<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250616.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "PF-LHM: 3D Animatable Avatar Reconstruction from Pose-free Articulated\n  Human Images", "author": "Lingteng Qiu and Peihao Li and Qi Zuo and Xiaodong Gu and Yuan Dong and Weihao Yuan and Siyu Zhu and Xiaoguang Han and Guanying Chen and Zilong Dong", "abstract": "  Reconstructing an animatable 3D human from casually captured images of an\narticulated subject without camera or human pose information is a practical yet\nchallenging task due to view misalignment, occlusions, and the absence of\nstructural priors. While optimization-based methods can produce high-fidelity\nresults from monocular or multi-view videos, they require accurate pose\nestimation and slow iterative optimization, limiting scalability in\nunconstrained scenarios. Recent feed-forward approaches enable efficient\nsingle-image reconstruction but struggle to effectively leverage multiple input\nimages to reduce ambiguity and improve reconstruction accuracy. To address\nthese challenges, we propose PF-LHM, a large human reconstruction model that\ngenerates high-quality 3D avatars in seconds from one or multiple casually\ncaptured pose-free images. Our approach introduces an efficient Encoder-Decoder\nPoint-Image Transformer architecture, which fuses hierarchical geometric point\nfeatures and multi-view image features through multimodal attention. The fused\nfeatures are decoded to recover detailed geometry and appearance, represented\nusing 3D Gaussian splats. Extensive experiments on both real and synthetic\ndatasets demonstrate that our method unifies single- and multi-image 3D human\nreconstruction, achieving high-fidelity and animatable 3D human avatars without\nrequiring camera and human pose annotations. Code and models will be released\nto the public.\n", "link": "http://arxiv.org/abs/2506.13766v1", "date": "2025-06-16", "relevancy": 3.414, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7056}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6864}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PF-LHM%3A%203D%20Animatable%20Avatar%20Reconstruction%20from%20Pose-free%20Articulated%0A%20%20Human%20Images&body=Title%3A%20PF-LHM%3A%203D%20Animatable%20Avatar%20Reconstruction%20from%20Pose-free%20Articulated%0A%20%20Human%20Images%0AAuthor%3A%20Lingteng%20Qiu%20and%20Peihao%20Li%20and%20Qi%20Zuo%20and%20Xiaodong%20Gu%20and%20Yuan%20Dong%20and%20Weihao%20Yuan%20and%20Siyu%20Zhu%20and%20Xiaoguang%20Han%20and%20Guanying%20Chen%20and%20Zilong%20Dong%0AAbstract%3A%20%20%20Reconstructing%20an%20animatable%203D%20human%20from%20casually%20captured%20images%20of%20an%0Aarticulated%20subject%20without%20camera%20or%20human%20pose%20information%20is%20a%20practical%20yet%0Achallenging%20task%20due%20to%20view%20misalignment%2C%20occlusions%2C%20and%20the%20absence%20of%0Astructural%20priors.%20While%20optimization-based%20methods%20can%20produce%20high-fidelity%0Aresults%20from%20monocular%20or%20multi-view%20videos%2C%20they%20require%20accurate%20pose%0Aestimation%20and%20slow%20iterative%20optimization%2C%20limiting%20scalability%20in%0Aunconstrained%20scenarios.%20Recent%20feed-forward%20approaches%20enable%20efficient%0Asingle-image%20reconstruction%20but%20struggle%20to%20effectively%20leverage%20multiple%20input%0Aimages%20to%20reduce%20ambiguity%20and%20improve%20reconstruction%20accuracy.%20To%20address%0Athese%20challenges%2C%20we%20propose%20PF-LHM%2C%20a%20large%20human%20reconstruction%20model%20that%0Agenerates%20high-quality%203D%20avatars%20in%20seconds%20from%20one%20or%20multiple%20casually%0Acaptured%20pose-free%20images.%20Our%20approach%20introduces%20an%20efficient%20Encoder-Decoder%0APoint-Image%20Transformer%20architecture%2C%20which%20fuses%20hierarchical%20geometric%20point%0Afeatures%20and%20multi-view%20image%20features%20through%20multimodal%20attention.%20The%20fused%0Afeatures%20are%20decoded%20to%20recover%20detailed%20geometry%20and%20appearance%2C%20represented%0Ausing%203D%20Gaussian%20splats.%20Extensive%20experiments%20on%20both%20real%20and%20synthetic%0Adatasets%20demonstrate%20that%20our%20method%20unifies%20single-%20and%20multi-image%203D%20human%0Areconstruction%2C%20achieving%20high-fidelity%20and%20animatable%203D%20human%20avatars%20without%0Arequiring%20camera%20and%20human%20pose%20annotations.%20Code%20and%20models%20will%20be%20released%0Ato%20the%20public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPF-LHM%253A%25203D%2520Animatable%2520Avatar%2520Reconstruction%2520from%2520Pose-free%2520Articulated%250A%2520%2520Human%2520Images%26entry.906535625%3DLingteng%2520Qiu%2520and%2520Peihao%2520Li%2520and%2520Qi%2520Zuo%2520and%2520Xiaodong%2520Gu%2520and%2520Yuan%2520Dong%2520and%2520Weihao%2520Yuan%2520and%2520Siyu%2520Zhu%2520and%2520Xiaoguang%2520Han%2520and%2520Guanying%2520Chen%2520and%2520Zilong%2520Dong%26entry.1292438233%3D%2520%2520Reconstructing%2520an%2520animatable%25203D%2520human%2520from%2520casually%2520captured%2520images%2520of%2520an%250Aarticulated%2520subject%2520without%2520camera%2520or%2520human%2520pose%2520information%2520is%2520a%2520practical%2520yet%250Achallenging%2520task%2520due%2520to%2520view%2520misalignment%252C%2520occlusions%252C%2520and%2520the%2520absence%2520of%250Astructural%2520priors.%2520While%2520optimization-based%2520methods%2520can%2520produce%2520high-fidelity%250Aresults%2520from%2520monocular%2520or%2520multi-view%2520videos%252C%2520they%2520require%2520accurate%2520pose%250Aestimation%2520and%2520slow%2520iterative%2520optimization%252C%2520limiting%2520scalability%2520in%250Aunconstrained%2520scenarios.%2520Recent%2520feed-forward%2520approaches%2520enable%2520efficient%250Asingle-image%2520reconstruction%2520but%2520struggle%2520to%2520effectively%2520leverage%2520multiple%2520input%250Aimages%2520to%2520reduce%2520ambiguity%2520and%2520improve%2520reconstruction%2520accuracy.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520PF-LHM%252C%2520a%2520large%2520human%2520reconstruction%2520model%2520that%250Agenerates%2520high-quality%25203D%2520avatars%2520in%2520seconds%2520from%2520one%2520or%2520multiple%2520casually%250Acaptured%2520pose-free%2520images.%2520Our%2520approach%2520introduces%2520an%2520efficient%2520Encoder-Decoder%250APoint-Image%2520Transformer%2520architecture%252C%2520which%2520fuses%2520hierarchical%2520geometric%2520point%250Afeatures%2520and%2520multi-view%2520image%2520features%2520through%2520multimodal%2520attention.%2520The%2520fused%250Afeatures%2520are%2520decoded%2520to%2520recover%2520detailed%2520geometry%2520and%2520appearance%252C%2520represented%250Ausing%25203D%2520Gaussian%2520splats.%2520Extensive%2520experiments%2520on%2520both%2520real%2520and%2520synthetic%250Adatasets%2520demonstrate%2520that%2520our%2520method%2520unifies%2520single-%2520and%2520multi-image%25203D%2520human%250Areconstruction%252C%2520achieving%2520high-fidelity%2520and%2520animatable%25203D%2520human%2520avatars%2520without%250Arequiring%2520camera%2520and%2520human%2520pose%2520annotations.%2520Code%2520and%2520models%2520will%2520be%2520released%250Ato%2520the%2520public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PF-LHM%3A%203D%20Animatable%20Avatar%20Reconstruction%20from%20Pose-free%20Articulated%0A%20%20Human%20Images&entry.906535625=Lingteng%20Qiu%20and%20Peihao%20Li%20and%20Qi%20Zuo%20and%20Xiaodong%20Gu%20and%20Yuan%20Dong%20and%20Weihao%20Yuan%20and%20Siyu%20Zhu%20and%20Xiaoguang%20Han%20and%20Guanying%20Chen%20and%20Zilong%20Dong&entry.1292438233=%20%20Reconstructing%20an%20animatable%203D%20human%20from%20casually%20captured%20images%20of%20an%0Aarticulated%20subject%20without%20camera%20or%20human%20pose%20information%20is%20a%20practical%20yet%0Achallenging%20task%20due%20to%20view%20misalignment%2C%20occlusions%2C%20and%20the%20absence%20of%0Astructural%20priors.%20While%20optimization-based%20methods%20can%20produce%20high-fidelity%0Aresults%20from%20monocular%20or%20multi-view%20videos%2C%20they%20require%20accurate%20pose%0Aestimation%20and%20slow%20iterative%20optimization%2C%20limiting%20scalability%20in%0Aunconstrained%20scenarios.%20Recent%20feed-forward%20approaches%20enable%20efficient%0Asingle-image%20reconstruction%20but%20struggle%20to%20effectively%20leverage%20multiple%20input%0Aimages%20to%20reduce%20ambiguity%20and%20improve%20reconstruction%20accuracy.%20To%20address%0Athese%20challenges%2C%20we%20propose%20PF-LHM%2C%20a%20large%20human%20reconstruction%20model%20that%0Agenerates%20high-quality%203D%20avatars%20in%20seconds%20from%20one%20or%20multiple%20casually%0Acaptured%20pose-free%20images.%20Our%20approach%20introduces%20an%20efficient%20Encoder-Decoder%0APoint-Image%20Transformer%20architecture%2C%20which%20fuses%20hierarchical%20geometric%20point%0Afeatures%20and%20multi-view%20image%20features%20through%20multimodal%20attention.%20The%20fused%0Afeatures%20are%20decoded%20to%20recover%20detailed%20geometry%20and%20appearance%2C%20represented%0Ausing%203D%20Gaussian%20splats.%20Extensive%20experiments%20on%20both%20real%20and%20synthetic%0Adatasets%20demonstrate%20that%20our%20method%20unifies%20single-%20and%20multi-image%203D%20human%0Areconstruction%2C%20achieving%20high-fidelity%20and%20animatable%203D%20human%20avatars%20without%0Arequiring%20camera%20and%20human%20pose%20annotations.%20Code%20and%20models%20will%20be%20released%0Ato%20the%20public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13766v1&entry.124074799=Read"},
{"title": "Micro-macro Gaussian Splatting with Enhanced Scalability for\n  Unconstrained Scene Reconstruction", "author": "Yihui Li and Chengxin Lv and Hongyu Yang and Di Huang", "abstract": "  Reconstructing 3D scenes from unconstrained image collections poses\nsignificant challenges due to variations in appearance. In this paper, we\npropose Scalable Micro-macro Wavelet-based Gaussian Splatting (SMW-GS), a novel\nmethod that enhances 3D reconstruction across diverse scales by decomposing\nscene representations into global, refined, and intrinsic components. SMW-GS\nincorporates the following innovations: Micro-macro Projection, which enables\nGaussian points to sample multi-scale details with improved diversity; and\nWavelet-based Sampling, which refines feature representations using\nfrequency-domain information to better capture complex scene appearances. To\nachieve scalability, we further propose a large-scale scene promotion strategy,\nwhich optimally assigns camera views to scene partitions by maximizing their\ncontributions to Gaussian points, achieving consistent and high-quality\nreconstructions even in expansive environments. Extensive experiments\ndemonstrate that SMW-GS significantly outperforms existing methods in both\nreconstruction quality and scalability, particularly excelling in large-scale\nurban environments with challenging illumination variations. Project is\navailable at https://github.com/Kidleyh/SMW-GS.\n", "link": "http://arxiv.org/abs/2506.13516v1", "date": "2025-06-16", "relevancy": 3.3744, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.712}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6598}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Micro-macro%20Gaussian%20Splatting%20with%20Enhanced%20Scalability%20for%0A%20%20Unconstrained%20Scene%20Reconstruction&body=Title%3A%20Micro-macro%20Gaussian%20Splatting%20with%20Enhanced%20Scalability%20for%0A%20%20Unconstrained%20Scene%20Reconstruction%0AAuthor%3A%20Yihui%20Li%20and%20Chengxin%20Lv%20and%20Hongyu%20Yang%20and%20Di%20Huang%0AAbstract%3A%20%20%20Reconstructing%203D%20scenes%20from%20unconstrained%20image%20collections%20poses%0Asignificant%20challenges%20due%20to%20variations%20in%20appearance.%20In%20this%20paper%2C%20we%0Apropose%20Scalable%20Micro-macro%20Wavelet-based%20Gaussian%20Splatting%20%28SMW-GS%29%2C%20a%20novel%0Amethod%20that%20enhances%203D%20reconstruction%20across%20diverse%20scales%20by%20decomposing%0Ascene%20representations%20into%20global%2C%20refined%2C%20and%20intrinsic%20components.%20SMW-GS%0Aincorporates%20the%20following%20innovations%3A%20Micro-macro%20Projection%2C%20which%20enables%0AGaussian%20points%20to%20sample%20multi-scale%20details%20with%20improved%20diversity%3B%20and%0AWavelet-based%20Sampling%2C%20which%20refines%20feature%20representations%20using%0Afrequency-domain%20information%20to%20better%20capture%20complex%20scene%20appearances.%20To%0Aachieve%20scalability%2C%20we%20further%20propose%20a%20large-scale%20scene%20promotion%20strategy%2C%0Awhich%20optimally%20assigns%20camera%20views%20to%20scene%20partitions%20by%20maximizing%20their%0Acontributions%20to%20Gaussian%20points%2C%20achieving%20consistent%20and%20high-quality%0Areconstructions%20even%20in%20expansive%20environments.%20Extensive%20experiments%0Ademonstrate%20that%20SMW-GS%20significantly%20outperforms%20existing%20methods%20in%20both%0Areconstruction%20quality%20and%20scalability%2C%20particularly%20excelling%20in%20large-scale%0Aurban%20environments%20with%20challenging%20illumination%20variations.%20Project%20is%0Aavailable%20at%20https%3A//github.com/Kidleyh/SMW-GS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMicro-macro%2520Gaussian%2520Splatting%2520with%2520Enhanced%2520Scalability%2520for%250A%2520%2520Unconstrained%2520Scene%2520Reconstruction%26entry.906535625%3DYihui%2520Li%2520and%2520Chengxin%2520Lv%2520and%2520Hongyu%2520Yang%2520and%2520Di%2520Huang%26entry.1292438233%3D%2520%2520Reconstructing%25203D%2520scenes%2520from%2520unconstrained%2520image%2520collections%2520poses%250Asignificant%2520challenges%2520due%2520to%2520variations%2520in%2520appearance.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520Scalable%2520Micro-macro%2520Wavelet-based%2520Gaussian%2520Splatting%2520%2528SMW-GS%2529%252C%2520a%2520novel%250Amethod%2520that%2520enhances%25203D%2520reconstruction%2520across%2520diverse%2520scales%2520by%2520decomposing%250Ascene%2520representations%2520into%2520global%252C%2520refined%252C%2520and%2520intrinsic%2520components.%2520SMW-GS%250Aincorporates%2520the%2520following%2520innovations%253A%2520Micro-macro%2520Projection%252C%2520which%2520enables%250AGaussian%2520points%2520to%2520sample%2520multi-scale%2520details%2520with%2520improved%2520diversity%253B%2520and%250AWavelet-based%2520Sampling%252C%2520which%2520refines%2520feature%2520representations%2520using%250Afrequency-domain%2520information%2520to%2520better%2520capture%2520complex%2520scene%2520appearances.%2520To%250Aachieve%2520scalability%252C%2520we%2520further%2520propose%2520a%2520large-scale%2520scene%2520promotion%2520strategy%252C%250Awhich%2520optimally%2520assigns%2520camera%2520views%2520to%2520scene%2520partitions%2520by%2520maximizing%2520their%250Acontributions%2520to%2520Gaussian%2520points%252C%2520achieving%2520consistent%2520and%2520high-quality%250Areconstructions%2520even%2520in%2520expansive%2520environments.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520SMW-GS%2520significantly%2520outperforms%2520existing%2520methods%2520in%2520both%250Areconstruction%2520quality%2520and%2520scalability%252C%2520particularly%2520excelling%2520in%2520large-scale%250Aurban%2520environments%2520with%2520challenging%2520illumination%2520variations.%2520Project%2520is%250Aavailable%2520at%2520https%253A//github.com/Kidleyh/SMW-GS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Micro-macro%20Gaussian%20Splatting%20with%20Enhanced%20Scalability%20for%0A%20%20Unconstrained%20Scene%20Reconstruction&entry.906535625=Yihui%20Li%20and%20Chengxin%20Lv%20and%20Hongyu%20Yang%20and%20Di%20Huang&entry.1292438233=%20%20Reconstructing%203D%20scenes%20from%20unconstrained%20image%20collections%20poses%0Asignificant%20challenges%20due%20to%20variations%20in%20appearance.%20In%20this%20paper%2C%20we%0Apropose%20Scalable%20Micro-macro%20Wavelet-based%20Gaussian%20Splatting%20%28SMW-GS%29%2C%20a%20novel%0Amethod%20that%20enhances%203D%20reconstruction%20across%20diverse%20scales%20by%20decomposing%0Ascene%20representations%20into%20global%2C%20refined%2C%20and%20intrinsic%20components.%20SMW-GS%0Aincorporates%20the%20following%20innovations%3A%20Micro-macro%20Projection%2C%20which%20enables%0AGaussian%20points%20to%20sample%20multi-scale%20details%20with%20improved%20diversity%3B%20and%0AWavelet-based%20Sampling%2C%20which%20refines%20feature%20representations%20using%0Afrequency-domain%20information%20to%20better%20capture%20complex%20scene%20appearances.%20To%0Aachieve%20scalability%2C%20we%20further%20propose%20a%20large-scale%20scene%20promotion%20strategy%2C%0Awhich%20optimally%20assigns%20camera%20views%20to%20scene%20partitions%20by%20maximizing%20their%0Acontributions%20to%20Gaussian%20points%2C%20achieving%20consistent%20and%20high-quality%0Areconstructions%20even%20in%20expansive%20environments.%20Extensive%20experiments%0Ademonstrate%20that%20SMW-GS%20significantly%20outperforms%20existing%20methods%20in%20both%0Areconstruction%20quality%20and%20scalability%2C%20particularly%20excelling%20in%20large-scale%0Aurban%20environments%20with%20challenging%20illumination%20variations.%20Project%20is%0Aavailable%20at%20https%3A//github.com/Kidleyh/SMW-GS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13516v1&entry.124074799=Read"},
{"title": "Multiview Geometric Regularization of Gaussian Splatting for Accurate\n  Radiance Fields", "author": "Jungeon Kim and Geonsoo Park and Seungyong Lee", "abstract": "  Recent methods, such as 2D Gaussian Splatting and Gaussian Opacity Fields,\nhave aimed to address the geometric inaccuracies of 3D Gaussian Splatting while\nretaining its superior rendering quality. However, these approaches still\nstruggle to reconstruct smooth and reliable geometry, particularly in scenes\nwith significant color variation across viewpoints, due to their per-point\nappearance modeling and single-view optimization constraints. In this paper, we\npropose an effective multiview geometric regularization strategy that\nintegrates multiview stereo (MVS) depth, RGB, and normal constraints into\nGaussian Splatting initialization and optimization. Our key insight is the\ncomplementary relationship between MVS-derived depth points and Gaussian\nSplatting-optimized positions: MVS robustly estimates geometry in regions of\nhigh color variation through local patch-based matching and epipolar\nconstraints, whereas Gaussian Splatting provides more reliable and less noisy\ndepth estimates near object boundaries and regions with lower color variation.\nTo leverage this insight, we introduce a median depth-based multiview relative\ndepth loss with uncertainty estimation, effectively integrating MVS depth\ninformation into Gaussian Splatting optimization. We also propose an MVS-guided\nGaussian Splatting initialization to avoid Gaussians falling into suboptimal\npositions. Extensive experiments validate that our approach successfully\ncombines these strengths, enhancing both geometric accuracy and rendering\nquality across diverse indoor and outdoor scenes.\n", "link": "http://arxiv.org/abs/2506.13508v1", "date": "2025-06-16", "relevancy": 3.2846, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6852}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6591}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiview%20Geometric%20Regularization%20of%20Gaussian%20Splatting%20for%20Accurate%0A%20%20Radiance%20Fields&body=Title%3A%20Multiview%20Geometric%20Regularization%20of%20Gaussian%20Splatting%20for%20Accurate%0A%20%20Radiance%20Fields%0AAuthor%3A%20Jungeon%20Kim%20and%20Geonsoo%20Park%20and%20Seungyong%20Lee%0AAbstract%3A%20%20%20Recent%20methods%2C%20such%20as%202D%20Gaussian%20Splatting%20and%20Gaussian%20Opacity%20Fields%2C%0Ahave%20aimed%20to%20address%20the%20geometric%20inaccuracies%20of%203D%20Gaussian%20Splatting%20while%0Aretaining%20its%20superior%20rendering%20quality.%20However%2C%20these%20approaches%20still%0Astruggle%20to%20reconstruct%20smooth%20and%20reliable%20geometry%2C%20particularly%20in%20scenes%0Awith%20significant%20color%20variation%20across%20viewpoints%2C%20due%20to%20their%20per-point%0Aappearance%20modeling%20and%20single-view%20optimization%20constraints.%20In%20this%20paper%2C%20we%0Apropose%20an%20effective%20multiview%20geometric%20regularization%20strategy%20that%0Aintegrates%20multiview%20stereo%20%28MVS%29%20depth%2C%20RGB%2C%20and%20normal%20constraints%20into%0AGaussian%20Splatting%20initialization%20and%20optimization.%20Our%20key%20insight%20is%20the%0Acomplementary%20relationship%20between%20MVS-derived%20depth%20points%20and%20Gaussian%0ASplatting-optimized%20positions%3A%20MVS%20robustly%20estimates%20geometry%20in%20regions%20of%0Ahigh%20color%20variation%20through%20local%20patch-based%20matching%20and%20epipolar%0Aconstraints%2C%20whereas%20Gaussian%20Splatting%20provides%20more%20reliable%20and%20less%20noisy%0Adepth%20estimates%20near%20object%20boundaries%20and%20regions%20with%20lower%20color%20variation.%0ATo%20leverage%20this%20insight%2C%20we%20introduce%20a%20median%20depth-based%20multiview%20relative%0Adepth%20loss%20with%20uncertainty%20estimation%2C%20effectively%20integrating%20MVS%20depth%0Ainformation%20into%20Gaussian%20Splatting%20optimization.%20We%20also%20propose%20an%20MVS-guided%0AGaussian%20Splatting%20initialization%20to%20avoid%20Gaussians%20falling%20into%20suboptimal%0Apositions.%20Extensive%20experiments%20validate%20that%20our%20approach%20successfully%0Acombines%20these%20strengths%2C%20enhancing%20both%20geometric%20accuracy%20and%20rendering%0Aquality%20across%20diverse%20indoor%20and%20outdoor%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13508v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiview%2520Geometric%2520Regularization%2520of%2520Gaussian%2520Splatting%2520for%2520Accurate%250A%2520%2520Radiance%2520Fields%26entry.906535625%3DJungeon%2520Kim%2520and%2520Geonsoo%2520Park%2520and%2520Seungyong%2520Lee%26entry.1292438233%3D%2520%2520Recent%2520methods%252C%2520such%2520as%25202D%2520Gaussian%2520Splatting%2520and%2520Gaussian%2520Opacity%2520Fields%252C%250Ahave%2520aimed%2520to%2520address%2520the%2520geometric%2520inaccuracies%2520of%25203D%2520Gaussian%2520Splatting%2520while%250Aretaining%2520its%2520superior%2520rendering%2520quality.%2520However%252C%2520these%2520approaches%2520still%250Astruggle%2520to%2520reconstruct%2520smooth%2520and%2520reliable%2520geometry%252C%2520particularly%2520in%2520scenes%250Awith%2520significant%2520color%2520variation%2520across%2520viewpoints%252C%2520due%2520to%2520their%2520per-point%250Aappearance%2520modeling%2520and%2520single-view%2520optimization%2520constraints.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520an%2520effective%2520multiview%2520geometric%2520regularization%2520strategy%2520that%250Aintegrates%2520multiview%2520stereo%2520%2528MVS%2529%2520depth%252C%2520RGB%252C%2520and%2520normal%2520constraints%2520into%250AGaussian%2520Splatting%2520initialization%2520and%2520optimization.%2520Our%2520key%2520insight%2520is%2520the%250Acomplementary%2520relationship%2520between%2520MVS-derived%2520depth%2520points%2520and%2520Gaussian%250ASplatting-optimized%2520positions%253A%2520MVS%2520robustly%2520estimates%2520geometry%2520in%2520regions%2520of%250Ahigh%2520color%2520variation%2520through%2520local%2520patch-based%2520matching%2520and%2520epipolar%250Aconstraints%252C%2520whereas%2520Gaussian%2520Splatting%2520provides%2520more%2520reliable%2520and%2520less%2520noisy%250Adepth%2520estimates%2520near%2520object%2520boundaries%2520and%2520regions%2520with%2520lower%2520color%2520variation.%250ATo%2520leverage%2520this%2520insight%252C%2520we%2520introduce%2520a%2520median%2520depth-based%2520multiview%2520relative%250Adepth%2520loss%2520with%2520uncertainty%2520estimation%252C%2520effectively%2520integrating%2520MVS%2520depth%250Ainformation%2520into%2520Gaussian%2520Splatting%2520optimization.%2520We%2520also%2520propose%2520an%2520MVS-guided%250AGaussian%2520Splatting%2520initialization%2520to%2520avoid%2520Gaussians%2520falling%2520into%2520suboptimal%250Apositions.%2520Extensive%2520experiments%2520validate%2520that%2520our%2520approach%2520successfully%250Acombines%2520these%2520strengths%252C%2520enhancing%2520both%2520geometric%2520accuracy%2520and%2520rendering%250Aquality%2520across%2520diverse%2520indoor%2520and%2520outdoor%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13508v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiview%20Geometric%20Regularization%20of%20Gaussian%20Splatting%20for%20Accurate%0A%20%20Radiance%20Fields&entry.906535625=Jungeon%20Kim%20and%20Geonsoo%20Park%20and%20Seungyong%20Lee&entry.1292438233=%20%20Recent%20methods%2C%20such%20as%202D%20Gaussian%20Splatting%20and%20Gaussian%20Opacity%20Fields%2C%0Ahave%20aimed%20to%20address%20the%20geometric%20inaccuracies%20of%203D%20Gaussian%20Splatting%20while%0Aretaining%20its%20superior%20rendering%20quality.%20However%2C%20these%20approaches%20still%0Astruggle%20to%20reconstruct%20smooth%20and%20reliable%20geometry%2C%20particularly%20in%20scenes%0Awith%20significant%20color%20variation%20across%20viewpoints%2C%20due%20to%20their%20per-point%0Aappearance%20modeling%20and%20single-view%20optimization%20constraints.%20In%20this%20paper%2C%20we%0Apropose%20an%20effective%20multiview%20geometric%20regularization%20strategy%20that%0Aintegrates%20multiview%20stereo%20%28MVS%29%20depth%2C%20RGB%2C%20and%20normal%20constraints%20into%0AGaussian%20Splatting%20initialization%20and%20optimization.%20Our%20key%20insight%20is%20the%0Acomplementary%20relationship%20between%20MVS-derived%20depth%20points%20and%20Gaussian%0ASplatting-optimized%20positions%3A%20MVS%20robustly%20estimates%20geometry%20in%20regions%20of%0Ahigh%20color%20variation%20through%20local%20patch-based%20matching%20and%20epipolar%0Aconstraints%2C%20whereas%20Gaussian%20Splatting%20provides%20more%20reliable%20and%20less%20noisy%0Adepth%20estimates%20near%20object%20boundaries%20and%20regions%20with%20lower%20color%20variation.%0ATo%20leverage%20this%20insight%2C%20we%20introduce%20a%20median%20depth-based%20multiview%20relative%0Adepth%20loss%20with%20uncertainty%20estimation%2C%20effectively%20integrating%20MVS%20depth%0Ainformation%20into%20Gaussian%20Splatting%20optimization.%20We%20also%20propose%20an%20MVS-guided%0AGaussian%20Splatting%20initialization%20to%20avoid%20Gaussians%20falling%20into%20suboptimal%0Apositions.%20Extensive%20experiments%20validate%20that%20our%20approach%20successfully%0Acombines%20these%20strengths%2C%20enhancing%20both%20geometric%20accuracy%20and%20rendering%0Aquality%20across%20diverse%20indoor%20and%20outdoor%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13508v1&entry.124074799=Read"},
{"title": "Unify3D: An Augmented Holistic End-to-end Monocular 3D Human\n  Reconstruction via Anatomy Shaping and Twins Negotiating", "author": "Nanjie Yao and Gangjian Zhang and Wenhao Shen and Jian Shu and Hao Wang", "abstract": "  Monocular 3D clothed human reconstruction aims to create a complete 3D avatar\nfrom a single image. To tackle the human geometry lacking in one RGB image,\ncurrent methods typically resort to a preceding model for an explicit geometric\nrepresentation. For the reconstruction itself, focus is on modeling both it and\nthe input image. This routine is constrained by the preceding model, and\noverlooks the integrity of the reconstruction task. To address this, this paper\nintroduces a novel paradigm that treats human reconstruction as a holistic\nprocess, utilizing an end-to-end network for direct prediction from 2D image to\n3D avatar, eliminating any explicit intermediate geometry display. Based on\nthis, we further propose a novel reconstruction framework consisting of two\ncore components: the Anatomy Shaping Extraction module, which captures implicit\nshape features taking into account the specialty of human anatomy, and the\nTwins Negotiating Reconstruction U-Net, which enhances reconstruction through\nfeature interaction between two U-Nets of different modalities. Moreover, we\npropose a Comic Data Augmentation strategy and construct 15k+ 3D human scans to\nbolster model performance in more complex case input. Extensive experiments on\ntwo test sets and many in-the-wild cases show the superiority of our method\nover SOTA methods. Our demos can be found in :\nhttps://e2e3dgsrecon.github.io/e2e3dgsrecon/.\n", "link": "http://arxiv.org/abs/2504.18215v2", "date": "2025-06-16", "relevancy": 3.24, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6591}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6574}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unify3D%3A%20An%20Augmented%20Holistic%20End-to-end%20Monocular%203D%20Human%0A%20%20Reconstruction%20via%20Anatomy%20Shaping%20and%20Twins%20Negotiating&body=Title%3A%20Unify3D%3A%20An%20Augmented%20Holistic%20End-to-end%20Monocular%203D%20Human%0A%20%20Reconstruction%20via%20Anatomy%20Shaping%20and%20Twins%20Negotiating%0AAuthor%3A%20Nanjie%20Yao%20and%20Gangjian%20Zhang%20and%20Wenhao%20Shen%20and%20Jian%20Shu%20and%20Hao%20Wang%0AAbstract%3A%20%20%20Monocular%203D%20clothed%20human%20reconstruction%20aims%20to%20create%20a%20complete%203D%20avatar%0Afrom%20a%20single%20image.%20To%20tackle%20the%20human%20geometry%20lacking%20in%20one%20RGB%20image%2C%0Acurrent%20methods%20typically%20resort%20to%20a%20preceding%20model%20for%20an%20explicit%20geometric%0Arepresentation.%20For%20the%20reconstruction%20itself%2C%20focus%20is%20on%20modeling%20both%20it%20and%0Athe%20input%20image.%20This%20routine%20is%20constrained%20by%20the%20preceding%20model%2C%20and%0Aoverlooks%20the%20integrity%20of%20the%20reconstruction%20task.%20To%20address%20this%2C%20this%20paper%0Aintroduces%20a%20novel%20paradigm%20that%20treats%20human%20reconstruction%20as%20a%20holistic%0Aprocess%2C%20utilizing%20an%20end-to-end%20network%20for%20direct%20prediction%20from%202D%20image%20to%0A3D%20avatar%2C%20eliminating%20any%20explicit%20intermediate%20geometry%20display.%20Based%20on%0Athis%2C%20we%20further%20propose%20a%20novel%20reconstruction%20framework%20consisting%20of%20two%0Acore%20components%3A%20the%20Anatomy%20Shaping%20Extraction%20module%2C%20which%20captures%20implicit%0Ashape%20features%20taking%20into%20account%20the%20specialty%20of%20human%20anatomy%2C%20and%20the%0ATwins%20Negotiating%20Reconstruction%20U-Net%2C%20which%20enhances%20reconstruction%20through%0Afeature%20interaction%20between%20two%20U-Nets%20of%20different%20modalities.%20Moreover%2C%20we%0Apropose%20a%20Comic%20Data%20Augmentation%20strategy%20and%20construct%2015k%2B%203D%20human%20scans%20to%0Abolster%20model%20performance%20in%20more%20complex%20case%20input.%20Extensive%20experiments%20on%0Atwo%20test%20sets%20and%20many%20in-the-wild%20cases%20show%20the%20superiority%20of%20our%20method%0Aover%20SOTA%20methods.%20Our%20demos%20can%20be%20found%20in%20%3A%0Ahttps%3A//e2e3dgsrecon.github.io/e2e3dgsrecon/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18215v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnify3D%253A%2520An%2520Augmented%2520Holistic%2520End-to-end%2520Monocular%25203D%2520Human%250A%2520%2520Reconstruction%2520via%2520Anatomy%2520Shaping%2520and%2520Twins%2520Negotiating%26entry.906535625%3DNanjie%2520Yao%2520and%2520Gangjian%2520Zhang%2520and%2520Wenhao%2520Shen%2520and%2520Jian%2520Shu%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%2520Monocular%25203D%2520clothed%2520human%2520reconstruction%2520aims%2520to%2520create%2520a%2520complete%25203D%2520avatar%250Afrom%2520a%2520single%2520image.%2520To%2520tackle%2520the%2520human%2520geometry%2520lacking%2520in%2520one%2520RGB%2520image%252C%250Acurrent%2520methods%2520typically%2520resort%2520to%2520a%2520preceding%2520model%2520for%2520an%2520explicit%2520geometric%250Arepresentation.%2520For%2520the%2520reconstruction%2520itself%252C%2520focus%2520is%2520on%2520modeling%2520both%2520it%2520and%250Athe%2520input%2520image.%2520This%2520routine%2520is%2520constrained%2520by%2520the%2520preceding%2520model%252C%2520and%250Aoverlooks%2520the%2520integrity%2520of%2520the%2520reconstruction%2520task.%2520To%2520address%2520this%252C%2520this%2520paper%250Aintroduces%2520a%2520novel%2520paradigm%2520that%2520treats%2520human%2520reconstruction%2520as%2520a%2520holistic%250Aprocess%252C%2520utilizing%2520an%2520end-to-end%2520network%2520for%2520direct%2520prediction%2520from%25202D%2520image%2520to%250A3D%2520avatar%252C%2520eliminating%2520any%2520explicit%2520intermediate%2520geometry%2520display.%2520Based%2520on%250Athis%252C%2520we%2520further%2520propose%2520a%2520novel%2520reconstruction%2520framework%2520consisting%2520of%2520two%250Acore%2520components%253A%2520the%2520Anatomy%2520Shaping%2520Extraction%2520module%252C%2520which%2520captures%2520implicit%250Ashape%2520features%2520taking%2520into%2520account%2520the%2520specialty%2520of%2520human%2520anatomy%252C%2520and%2520the%250ATwins%2520Negotiating%2520Reconstruction%2520U-Net%252C%2520which%2520enhances%2520reconstruction%2520through%250Afeature%2520interaction%2520between%2520two%2520U-Nets%2520of%2520different%2520modalities.%2520Moreover%252C%2520we%250Apropose%2520a%2520Comic%2520Data%2520Augmentation%2520strategy%2520and%2520construct%252015k%252B%25203D%2520human%2520scans%2520to%250Abolster%2520model%2520performance%2520in%2520more%2520complex%2520case%2520input.%2520Extensive%2520experiments%2520on%250Atwo%2520test%2520sets%2520and%2520many%2520in-the-wild%2520cases%2520show%2520the%2520superiority%2520of%2520our%2520method%250Aover%2520SOTA%2520methods.%2520Our%2520demos%2520can%2520be%2520found%2520in%2520%253A%250Ahttps%253A//e2e3dgsrecon.github.io/e2e3dgsrecon/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18215v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unify3D%3A%20An%20Augmented%20Holistic%20End-to-end%20Monocular%203D%20Human%0A%20%20Reconstruction%20via%20Anatomy%20Shaping%20and%20Twins%20Negotiating&entry.906535625=Nanjie%20Yao%20and%20Gangjian%20Zhang%20and%20Wenhao%20Shen%20and%20Jian%20Shu%20and%20Hao%20Wang&entry.1292438233=%20%20Monocular%203D%20clothed%20human%20reconstruction%20aims%20to%20create%20a%20complete%203D%20avatar%0Afrom%20a%20single%20image.%20To%20tackle%20the%20human%20geometry%20lacking%20in%20one%20RGB%20image%2C%0Acurrent%20methods%20typically%20resort%20to%20a%20preceding%20model%20for%20an%20explicit%20geometric%0Arepresentation.%20For%20the%20reconstruction%20itself%2C%20focus%20is%20on%20modeling%20both%20it%20and%0Athe%20input%20image.%20This%20routine%20is%20constrained%20by%20the%20preceding%20model%2C%20and%0Aoverlooks%20the%20integrity%20of%20the%20reconstruction%20task.%20To%20address%20this%2C%20this%20paper%0Aintroduces%20a%20novel%20paradigm%20that%20treats%20human%20reconstruction%20as%20a%20holistic%0Aprocess%2C%20utilizing%20an%20end-to-end%20network%20for%20direct%20prediction%20from%202D%20image%20to%0A3D%20avatar%2C%20eliminating%20any%20explicit%20intermediate%20geometry%20display.%20Based%20on%0Athis%2C%20we%20further%20propose%20a%20novel%20reconstruction%20framework%20consisting%20of%20two%0Acore%20components%3A%20the%20Anatomy%20Shaping%20Extraction%20module%2C%20which%20captures%20implicit%0Ashape%20features%20taking%20into%20account%20the%20specialty%20of%20human%20anatomy%2C%20and%20the%0ATwins%20Negotiating%20Reconstruction%20U-Net%2C%20which%20enhances%20reconstruction%20through%0Afeature%20interaction%20between%20two%20U-Nets%20of%20different%20modalities.%20Moreover%2C%20we%0Apropose%20a%20Comic%20Data%20Augmentation%20strategy%20and%20construct%2015k%2B%203D%20human%20scans%20to%0Abolster%20model%20performance%20in%20more%20complex%20case%20input.%20Extensive%20experiments%20on%0Atwo%20test%20sets%20and%20many%20in-the-wild%20cases%20show%20the%20superiority%20of%20our%20method%0Aover%20SOTA%20methods.%20Our%20demos%20can%20be%20found%20in%20%3A%0Ahttps%3A//e2e3dgsrecon.github.io/e2e3dgsrecon/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18215v2&entry.124074799=Read"},
{"title": "Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal\n  Instruction Tuning", "author": "Hanxun Yu and Wentong Li and Song Wang and Junbo Chen and Jianke Zhu", "abstract": "  Despite encouraging progress in 3D scene understanding, it remains\nchallenging to develop an effective Large Multi-modal Model (LMM) that is\ncapable of understanding and reasoning in complex 3D environments. Most\nprevious methods typically encode 3D point and 2D image features separately,\nneglecting interactions between 2D semantics and 3D object properties, as well\nas the spatial relationships within the 3D environment. This limitation not\nonly hinders comprehensive representations of 3D scene, but also compromises\ntraining and inference efficiency. To address these challenges, we propose a\nunified Instance-aware 3D Large Multi-modal Model (Inst3D-LMM) to deal with\nmultiple 3D scene understanding tasks simultaneously. To obtain the\nfine-grained instance-level visual tokens, we first introduce a novel\nMulti-view Cross-Modal Fusion (MCMF) module to inject the multi-view 2D\nsemantics into their corresponding 3D geometric features. For scene-level\nrelation-aware tokens, we further present a 3D Instance Spatial Relation\n(3D-ISR) module to capture the intricate pairwise spatial relationships among\nobjects. Additionally, we perform end-to-end multi-task instruction tuning\nsimultaneously without the subsequent task-specific fine-tuning. Extensive\nexperiments demonstrate that our approach outperforms the state-of-the-art\nmethods across 3D scene understanding, reasoning and grounding tasks. Source\ncode is available at https://github.com/hanxunyu/Inst3D-LMM\n", "link": "http://arxiv.org/abs/2503.00513v2", "date": "2025-06-16", "relevancy": 3.2373, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6509}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6457}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inst3D-LMM%3A%20Instance-Aware%203D%20Scene%20Understanding%20with%20Multi-modal%0A%20%20Instruction%20Tuning&body=Title%3A%20Inst3D-LMM%3A%20Instance-Aware%203D%20Scene%20Understanding%20with%20Multi-modal%0A%20%20Instruction%20Tuning%0AAuthor%3A%20Hanxun%20Yu%20and%20Wentong%20Li%20and%20Song%20Wang%20and%20Junbo%20Chen%20and%20Jianke%20Zhu%0AAbstract%3A%20%20%20Despite%20encouraging%20progress%20in%203D%20scene%20understanding%2C%20it%20remains%0Achallenging%20to%20develop%20an%20effective%20Large%20Multi-modal%20Model%20%28LMM%29%20that%20is%0Acapable%20of%20understanding%20and%20reasoning%20in%20complex%203D%20environments.%20Most%0Aprevious%20methods%20typically%20encode%203D%20point%20and%202D%20image%20features%20separately%2C%0Aneglecting%20interactions%20between%202D%20semantics%20and%203D%20object%20properties%2C%20as%20well%0Aas%20the%20spatial%20relationships%20within%20the%203D%20environment.%20This%20limitation%20not%0Aonly%20hinders%20comprehensive%20representations%20of%203D%20scene%2C%20but%20also%20compromises%0Atraining%20and%20inference%20efficiency.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Aunified%20Instance-aware%203D%20Large%20Multi-modal%20Model%20%28Inst3D-LMM%29%20to%20deal%20with%0Amultiple%203D%20scene%20understanding%20tasks%20simultaneously.%20To%20obtain%20the%0Afine-grained%20instance-level%20visual%20tokens%2C%20we%20first%20introduce%20a%20novel%0AMulti-view%20Cross-Modal%20Fusion%20%28MCMF%29%20module%20to%20inject%20the%20multi-view%202D%0Asemantics%20into%20their%20corresponding%203D%20geometric%20features.%20For%20scene-level%0Arelation-aware%20tokens%2C%20we%20further%20present%20a%203D%20Instance%20Spatial%20Relation%0A%283D-ISR%29%20module%20to%20capture%20the%20intricate%20pairwise%20spatial%20relationships%20among%0Aobjects.%20Additionally%2C%20we%20perform%20end-to-end%20multi-task%20instruction%20tuning%0Asimultaneously%20without%20the%20subsequent%20task-specific%20fine-tuning.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20approach%20outperforms%20the%20state-of-the-art%0Amethods%20across%203D%20scene%20understanding%2C%20reasoning%20and%20grounding%20tasks.%20Source%0Acode%20is%20available%20at%20https%3A//github.com/hanxunyu/Inst3D-LMM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.00513v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInst3D-LMM%253A%2520Instance-Aware%25203D%2520Scene%2520Understanding%2520with%2520Multi-modal%250A%2520%2520Instruction%2520Tuning%26entry.906535625%3DHanxun%2520Yu%2520and%2520Wentong%2520Li%2520and%2520Song%2520Wang%2520and%2520Junbo%2520Chen%2520and%2520Jianke%2520Zhu%26entry.1292438233%3D%2520%2520Despite%2520encouraging%2520progress%2520in%25203D%2520scene%2520understanding%252C%2520it%2520remains%250Achallenging%2520to%2520develop%2520an%2520effective%2520Large%2520Multi-modal%2520Model%2520%2528LMM%2529%2520that%2520is%250Acapable%2520of%2520understanding%2520and%2520reasoning%2520in%2520complex%25203D%2520environments.%2520Most%250Aprevious%2520methods%2520typically%2520encode%25203D%2520point%2520and%25202D%2520image%2520features%2520separately%252C%250Aneglecting%2520interactions%2520between%25202D%2520semantics%2520and%25203D%2520object%2520properties%252C%2520as%2520well%250Aas%2520the%2520spatial%2520relationships%2520within%2520the%25203D%2520environment.%2520This%2520limitation%2520not%250Aonly%2520hinders%2520comprehensive%2520representations%2520of%25203D%2520scene%252C%2520but%2520also%2520compromises%250Atraining%2520and%2520inference%2520efficiency.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%250Aunified%2520Instance-aware%25203D%2520Large%2520Multi-modal%2520Model%2520%2528Inst3D-LMM%2529%2520to%2520deal%2520with%250Amultiple%25203D%2520scene%2520understanding%2520tasks%2520simultaneously.%2520To%2520obtain%2520the%250Afine-grained%2520instance-level%2520visual%2520tokens%252C%2520we%2520first%2520introduce%2520a%2520novel%250AMulti-view%2520Cross-Modal%2520Fusion%2520%2528MCMF%2529%2520module%2520to%2520inject%2520the%2520multi-view%25202D%250Asemantics%2520into%2520their%2520corresponding%25203D%2520geometric%2520features.%2520For%2520scene-level%250Arelation-aware%2520tokens%252C%2520we%2520further%2520present%2520a%25203D%2520Instance%2520Spatial%2520Relation%250A%25283D-ISR%2529%2520module%2520to%2520capture%2520the%2520intricate%2520pairwise%2520spatial%2520relationships%2520among%250Aobjects.%2520Additionally%252C%2520we%2520perform%2520end-to-end%2520multi-task%2520instruction%2520tuning%250Asimultaneously%2520without%2520the%2520subsequent%2520task-specific%2520fine-tuning.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520the%2520state-of-the-art%250Amethods%2520across%25203D%2520scene%2520understanding%252C%2520reasoning%2520and%2520grounding%2520tasks.%2520Source%250Acode%2520is%2520available%2520at%2520https%253A//github.com/hanxunyu/Inst3D-LMM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.00513v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inst3D-LMM%3A%20Instance-Aware%203D%20Scene%20Understanding%20with%20Multi-modal%0A%20%20Instruction%20Tuning&entry.906535625=Hanxun%20Yu%20and%20Wentong%20Li%20and%20Song%20Wang%20and%20Junbo%20Chen%20and%20Jianke%20Zhu&entry.1292438233=%20%20Despite%20encouraging%20progress%20in%203D%20scene%20understanding%2C%20it%20remains%0Achallenging%20to%20develop%20an%20effective%20Large%20Multi-modal%20Model%20%28LMM%29%20that%20is%0Acapable%20of%20understanding%20and%20reasoning%20in%20complex%203D%20environments.%20Most%0Aprevious%20methods%20typically%20encode%203D%20point%20and%202D%20image%20features%20separately%2C%0Aneglecting%20interactions%20between%202D%20semantics%20and%203D%20object%20properties%2C%20as%20well%0Aas%20the%20spatial%20relationships%20within%20the%203D%20environment.%20This%20limitation%20not%0Aonly%20hinders%20comprehensive%20representations%20of%203D%20scene%2C%20but%20also%20compromises%0Atraining%20and%20inference%20efficiency.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Aunified%20Instance-aware%203D%20Large%20Multi-modal%20Model%20%28Inst3D-LMM%29%20to%20deal%20with%0Amultiple%203D%20scene%20understanding%20tasks%20simultaneously.%20To%20obtain%20the%0Afine-grained%20instance-level%20visual%20tokens%2C%20we%20first%20introduce%20a%20novel%0AMulti-view%20Cross-Modal%20Fusion%20%28MCMF%29%20module%20to%20inject%20the%20multi-view%202D%0Asemantics%20into%20their%20corresponding%203D%20geometric%20features.%20For%20scene-level%0Arelation-aware%20tokens%2C%20we%20further%20present%20a%203D%20Instance%20Spatial%20Relation%0A%283D-ISR%29%20module%20to%20capture%20the%20intricate%20pairwise%20spatial%20relationships%20among%0Aobjects.%20Additionally%2C%20we%20perform%20end-to-end%20multi-task%20instruction%20tuning%0Asimultaneously%20without%20the%20subsequent%20task-specific%20fine-tuning.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20approach%20outperforms%20the%20state-of-the-art%0Amethods%20across%203D%20scene%20understanding%2C%20reasoning%20and%20grounding%20tasks.%20Source%0Acode%20is%20available%20at%20https%3A//github.com/hanxunyu/Inst3D-LMM%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.00513v2&entry.124074799=Read"},
{"title": "Integrated Pipeline for Monocular 3D Reconstruction and Finite Element\n  Simulation in Industrial Applications", "author": "Bowen Zheng", "abstract": "  To address the challenges of 3D modeling and structural simulation in\nindustrial environment, such as the difficulty of equipment deployment, and the\ndifficulty of balancing accuracy and real-time performance, this paper proposes\nan integrated workflow, which integrates high-fidelity 3D reconstruction based\non monocular video, finite element simulation analysis, and mixed reality\nvisual display, aiming to build an interactive digital twin system for\nindustrial inspection, equipment maintenance and other scenes. Firstly, the\nNeuralangelo algorithm based on deep learning is used to reconstruct the 3D\nmesh model with rich details from the surround-shot video. Then, the QuadRemesh\ntool of Rhino is used to optimize the initial triangular mesh and generate a\nstructured mesh suitable for finite element analysis. The optimized mesh is\nfurther discretized by HyperMesh, and the material parameter setting and stress\nsimulation are carried out in Abaqus to obtain high-precision stress and\ndeformation results. Finally, combined with Unity and Vuforia engine, the\nreal-time superposition and interactive operation of simulation results in the\naugmented reality environment are realized, which improves users 'intuitive\nunderstanding of structural response. Experiments show that the method has good\nsimulation efficiency and visualization effect while maintaining high geometric\naccuracy. It provides a practical solution for digital modeling, mechanical\nanalysis and interactive display in complex industrial scenes, and lays a\nfoundation for the deep integration of digital twin and mixed reality\ntechnology in industrial applications.\n", "link": "http://arxiv.org/abs/2506.13573v1", "date": "2025-06-16", "relevancy": 3.1302, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6345}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6218}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrated%20Pipeline%20for%20Monocular%203D%20Reconstruction%20and%20Finite%20Element%0A%20%20Simulation%20in%20Industrial%20Applications&body=Title%3A%20Integrated%20Pipeline%20for%20Monocular%203D%20Reconstruction%20and%20Finite%20Element%0A%20%20Simulation%20in%20Industrial%20Applications%0AAuthor%3A%20Bowen%20Zheng%0AAbstract%3A%20%20%20To%20address%20the%20challenges%20of%203D%20modeling%20and%20structural%20simulation%20in%0Aindustrial%20environment%2C%20such%20as%20the%20difficulty%20of%20equipment%20deployment%2C%20and%20the%0Adifficulty%20of%20balancing%20accuracy%20and%20real-time%20performance%2C%20this%20paper%20proposes%0Aan%20integrated%20workflow%2C%20which%20integrates%20high-fidelity%203D%20reconstruction%20based%0Aon%20monocular%20video%2C%20finite%20element%20simulation%20analysis%2C%20and%20mixed%20reality%0Avisual%20display%2C%20aiming%20to%20build%20an%20interactive%20digital%20twin%20system%20for%0Aindustrial%20inspection%2C%20equipment%20maintenance%20and%20other%20scenes.%20Firstly%2C%20the%0ANeuralangelo%20algorithm%20based%20on%20deep%20learning%20is%20used%20to%20reconstruct%20the%203D%0Amesh%20model%20with%20rich%20details%20from%20the%20surround-shot%20video.%20Then%2C%20the%20QuadRemesh%0Atool%20of%20Rhino%20is%20used%20to%20optimize%20the%20initial%20triangular%20mesh%20and%20generate%20a%0Astructured%20mesh%20suitable%20for%20finite%20element%20analysis.%20The%20optimized%20mesh%20is%0Afurther%20discretized%20by%20HyperMesh%2C%20and%20the%20material%20parameter%20setting%20and%20stress%0Asimulation%20are%20carried%20out%20in%20Abaqus%20to%20obtain%20high-precision%20stress%20and%0Adeformation%20results.%20Finally%2C%20combined%20with%20Unity%20and%20Vuforia%20engine%2C%20the%0Areal-time%20superposition%20and%20interactive%20operation%20of%20simulation%20results%20in%20the%0Aaugmented%20reality%20environment%20are%20realized%2C%20which%20improves%20users%20%27intuitive%0Aunderstanding%20of%20structural%20response.%20Experiments%20show%20that%20the%20method%20has%20good%0Asimulation%20efficiency%20and%20visualization%20effect%20while%20maintaining%20high%20geometric%0Aaccuracy.%20It%20provides%20a%20practical%20solution%20for%20digital%20modeling%2C%20mechanical%0Aanalysis%20and%20interactive%20display%20in%20complex%20industrial%20scenes%2C%20and%20lays%20a%0Afoundation%20for%20the%20deep%20integration%20of%20digital%20twin%20and%20mixed%20reality%0Atechnology%20in%20industrial%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrated%2520Pipeline%2520for%2520Monocular%25203D%2520Reconstruction%2520and%2520Finite%2520Element%250A%2520%2520Simulation%2520in%2520Industrial%2520Applications%26entry.906535625%3DBowen%2520Zheng%26entry.1292438233%3D%2520%2520To%2520address%2520the%2520challenges%2520of%25203D%2520modeling%2520and%2520structural%2520simulation%2520in%250Aindustrial%2520environment%252C%2520such%2520as%2520the%2520difficulty%2520of%2520equipment%2520deployment%252C%2520and%2520the%250Adifficulty%2520of%2520balancing%2520accuracy%2520and%2520real-time%2520performance%252C%2520this%2520paper%2520proposes%250Aan%2520integrated%2520workflow%252C%2520which%2520integrates%2520high-fidelity%25203D%2520reconstruction%2520based%250Aon%2520monocular%2520video%252C%2520finite%2520element%2520simulation%2520analysis%252C%2520and%2520mixed%2520reality%250Avisual%2520display%252C%2520aiming%2520to%2520build%2520an%2520interactive%2520digital%2520twin%2520system%2520for%250Aindustrial%2520inspection%252C%2520equipment%2520maintenance%2520and%2520other%2520scenes.%2520Firstly%252C%2520the%250ANeuralangelo%2520algorithm%2520based%2520on%2520deep%2520learning%2520is%2520used%2520to%2520reconstruct%2520the%25203D%250Amesh%2520model%2520with%2520rich%2520details%2520from%2520the%2520surround-shot%2520video.%2520Then%252C%2520the%2520QuadRemesh%250Atool%2520of%2520Rhino%2520is%2520used%2520to%2520optimize%2520the%2520initial%2520triangular%2520mesh%2520and%2520generate%2520a%250Astructured%2520mesh%2520suitable%2520for%2520finite%2520element%2520analysis.%2520The%2520optimized%2520mesh%2520is%250Afurther%2520discretized%2520by%2520HyperMesh%252C%2520and%2520the%2520material%2520parameter%2520setting%2520and%2520stress%250Asimulation%2520are%2520carried%2520out%2520in%2520Abaqus%2520to%2520obtain%2520high-precision%2520stress%2520and%250Adeformation%2520results.%2520Finally%252C%2520combined%2520with%2520Unity%2520and%2520Vuforia%2520engine%252C%2520the%250Areal-time%2520superposition%2520and%2520interactive%2520operation%2520of%2520simulation%2520results%2520in%2520the%250Aaugmented%2520reality%2520environment%2520are%2520realized%252C%2520which%2520improves%2520users%2520%2527intuitive%250Aunderstanding%2520of%2520structural%2520response.%2520Experiments%2520show%2520that%2520the%2520method%2520has%2520good%250Asimulation%2520efficiency%2520and%2520visualization%2520effect%2520while%2520maintaining%2520high%2520geometric%250Aaccuracy.%2520It%2520provides%2520a%2520practical%2520solution%2520for%2520digital%2520modeling%252C%2520mechanical%250Aanalysis%2520and%2520interactive%2520display%2520in%2520complex%2520industrial%2520scenes%252C%2520and%2520lays%2520a%250Afoundation%2520for%2520the%2520deep%2520integration%2520of%2520digital%2520twin%2520and%2520mixed%2520reality%250Atechnology%2520in%2520industrial%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrated%20Pipeline%20for%20Monocular%203D%20Reconstruction%20and%20Finite%20Element%0A%20%20Simulation%20in%20Industrial%20Applications&entry.906535625=Bowen%20Zheng&entry.1292438233=%20%20To%20address%20the%20challenges%20of%203D%20modeling%20and%20structural%20simulation%20in%0Aindustrial%20environment%2C%20such%20as%20the%20difficulty%20of%20equipment%20deployment%2C%20and%20the%0Adifficulty%20of%20balancing%20accuracy%20and%20real-time%20performance%2C%20this%20paper%20proposes%0Aan%20integrated%20workflow%2C%20which%20integrates%20high-fidelity%203D%20reconstruction%20based%0Aon%20monocular%20video%2C%20finite%20element%20simulation%20analysis%2C%20and%20mixed%20reality%0Avisual%20display%2C%20aiming%20to%20build%20an%20interactive%20digital%20twin%20system%20for%0Aindustrial%20inspection%2C%20equipment%20maintenance%20and%20other%20scenes.%20Firstly%2C%20the%0ANeuralangelo%20algorithm%20based%20on%20deep%20learning%20is%20used%20to%20reconstruct%20the%203D%0Amesh%20model%20with%20rich%20details%20from%20the%20surround-shot%20video.%20Then%2C%20the%20QuadRemesh%0Atool%20of%20Rhino%20is%20used%20to%20optimize%20the%20initial%20triangular%20mesh%20and%20generate%20a%0Astructured%20mesh%20suitable%20for%20finite%20element%20analysis.%20The%20optimized%20mesh%20is%0Afurther%20discretized%20by%20HyperMesh%2C%20and%20the%20material%20parameter%20setting%20and%20stress%0Asimulation%20are%20carried%20out%20in%20Abaqus%20to%20obtain%20high-precision%20stress%20and%0Adeformation%20results.%20Finally%2C%20combined%20with%20Unity%20and%20Vuforia%20engine%2C%20the%0Areal-time%20superposition%20and%20interactive%20operation%20of%20simulation%20results%20in%20the%0Aaugmented%20reality%20environment%20are%20realized%2C%20which%20improves%20users%20%27intuitive%0Aunderstanding%20of%20structural%20response.%20Experiments%20show%20that%20the%20method%20has%20good%0Asimulation%20efficiency%20and%20visualization%20effect%20while%20maintaining%20high%20geometric%0Aaccuracy.%20It%20provides%20a%20practical%20solution%20for%20digital%20modeling%2C%20mechanical%0Aanalysis%20and%20interactive%20display%20in%20complex%20industrial%20scenes%2C%20and%20lays%20a%0Afoundation%20for%20the%20deep%20integration%20of%20digital%20twin%20and%20mixed%20reality%0Atechnology%20in%20industrial%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13573v1&entry.124074799=Read"},
{"title": "TextureSplat: Per-Primitive Texture Mapping for Reflective Gaussian\n  Splatting", "author": "Mae Younes and Adnane Boukhayma", "abstract": "  Gaussian Splatting have demonstrated remarkable novel view synthesis\nperformance at high rendering frame rates. Optimization-based inverse rendering\nwithin complex capture scenarios remains however a challenging problem. A\nparticular case is modelling complex surface light interactions for highly\nreflective scenes, which results in intricate high frequency specular radiance\ncomponents. We hypothesize that such challenging settings can benefit from\nincreased representation power. We hence propose a method that tackles this\nissue through a geometrically and physically grounded Gaussian Splatting borne\nradiance field, where normals and material properties are spatially variable in\nthe primitive's local space. Using per-primitive texture maps for this purpose,\nwe also propose to harness the GPU hardware to accelerate rendering at test\ntime via unified material texture atlas.\n", "link": "http://arxiv.org/abs/2506.13348v1", "date": "2025-06-16", "relevancy": 3.1248, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6679}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6136}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextureSplat%3A%20Per-Primitive%20Texture%20Mapping%20for%20Reflective%20Gaussian%0A%20%20Splatting&body=Title%3A%20TextureSplat%3A%20Per-Primitive%20Texture%20Mapping%20for%20Reflective%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Mae%20Younes%20and%20Adnane%20Boukhayma%0AAbstract%3A%20%20%20Gaussian%20Splatting%20have%20demonstrated%20remarkable%20novel%20view%20synthesis%0Aperformance%20at%20high%20rendering%20frame%20rates.%20Optimization-based%20inverse%20rendering%0Awithin%20complex%20capture%20scenarios%20remains%20however%20a%20challenging%20problem.%20A%0Aparticular%20case%20is%20modelling%20complex%20surface%20light%20interactions%20for%20highly%0Areflective%20scenes%2C%20which%20results%20in%20intricate%20high%20frequency%20specular%20radiance%0Acomponents.%20We%20hypothesize%20that%20such%20challenging%20settings%20can%20benefit%20from%0Aincreased%20representation%20power.%20We%20hence%20propose%20a%20method%20that%20tackles%20this%0Aissue%20through%20a%20geometrically%20and%20physically%20grounded%20Gaussian%20Splatting%20borne%0Aradiance%20field%2C%20where%20normals%20and%20material%20properties%20are%20spatially%20variable%20in%0Athe%20primitive%27s%20local%20space.%20Using%20per-primitive%20texture%20maps%20for%20this%20purpose%2C%0Awe%20also%20propose%20to%20harness%20the%20GPU%20hardware%20to%20accelerate%20rendering%20at%20test%0Atime%20via%20unified%20material%20texture%20atlas.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13348v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextureSplat%253A%2520Per-Primitive%2520Texture%2520Mapping%2520for%2520Reflective%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DMae%2520Younes%2520and%2520Adnane%2520Boukhayma%26entry.1292438233%3D%2520%2520Gaussian%2520Splatting%2520have%2520demonstrated%2520remarkable%2520novel%2520view%2520synthesis%250Aperformance%2520at%2520high%2520rendering%2520frame%2520rates.%2520Optimization-based%2520inverse%2520rendering%250Awithin%2520complex%2520capture%2520scenarios%2520remains%2520however%2520a%2520challenging%2520problem.%2520A%250Aparticular%2520case%2520is%2520modelling%2520complex%2520surface%2520light%2520interactions%2520for%2520highly%250Areflective%2520scenes%252C%2520which%2520results%2520in%2520intricate%2520high%2520frequency%2520specular%2520radiance%250Acomponents.%2520We%2520hypothesize%2520that%2520such%2520challenging%2520settings%2520can%2520benefit%2520from%250Aincreased%2520representation%2520power.%2520We%2520hence%2520propose%2520a%2520method%2520that%2520tackles%2520this%250Aissue%2520through%2520a%2520geometrically%2520and%2520physically%2520grounded%2520Gaussian%2520Splatting%2520borne%250Aradiance%2520field%252C%2520where%2520normals%2520and%2520material%2520properties%2520are%2520spatially%2520variable%2520in%250Athe%2520primitive%2527s%2520local%2520space.%2520Using%2520per-primitive%2520texture%2520maps%2520for%2520this%2520purpose%252C%250Awe%2520also%2520propose%2520to%2520harness%2520the%2520GPU%2520hardware%2520to%2520accelerate%2520rendering%2520at%2520test%250Atime%2520via%2520unified%2520material%2520texture%2520atlas.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13348v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextureSplat%3A%20Per-Primitive%20Texture%20Mapping%20for%20Reflective%20Gaussian%0A%20%20Splatting&entry.906535625=Mae%20Younes%20and%20Adnane%20Boukhayma&entry.1292438233=%20%20Gaussian%20Splatting%20have%20demonstrated%20remarkable%20novel%20view%20synthesis%0Aperformance%20at%20high%20rendering%20frame%20rates.%20Optimization-based%20inverse%20rendering%0Awithin%20complex%20capture%20scenarios%20remains%20however%20a%20challenging%20problem.%20A%0Aparticular%20case%20is%20modelling%20complex%20surface%20light%20interactions%20for%20highly%0Areflective%20scenes%2C%20which%20results%20in%20intricate%20high%20frequency%20specular%20radiance%0Acomponents.%20We%20hypothesize%20that%20such%20challenging%20settings%20can%20benefit%20from%0Aincreased%20representation%20power.%20We%20hence%20propose%20a%20method%20that%20tackles%20this%0Aissue%20through%20a%20geometrically%20and%20physically%20grounded%20Gaussian%20Splatting%20borne%0Aradiance%20field%2C%20where%20normals%20and%20material%20properties%20are%20spatially%20variable%20in%0Athe%20primitive%27s%20local%20space.%20Using%20per-primitive%20texture%20maps%20for%20this%20purpose%2C%0Awe%20also%20propose%20to%20harness%20the%20GPU%20hardware%20to%20accelerate%20rendering%20at%20test%0Atime%20via%20unified%20material%20texture%20atlas.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13348v1&entry.124074799=Read"},
{"title": "Test3R: Learning to Reconstruct 3D at Test Time", "author": "Yuheng Yuan and Qiuhong Shen and Shizun Wang and Xingyi Yang and Xinchao Wang", "abstract": "  Dense matching methods like DUSt3R regress pairwise pointmaps for 3D\nreconstruction. However, the reliance on pairwise prediction and the limited\ngeneralization capability inherently restrict the global geometric consistency.\nIn this work, we introduce Test3R, a surprisingly simple test-time learning\ntechnique that significantly boosts geometric accuracy. Using image triplets\n($I_1,I_2,I_3$), Test3R generates reconstructions from pairs ($I_1,I_2$) and\n($I_1,I_3$). The core idea is to optimize the network at test time via a\nself-supervised objective: maximizing the geometric consistency between these\ntwo reconstructions relative to the common image $I_1$. This ensures the model\nproduces cross-pair consistent outputs, regardless of the inputs. Extensive\nexperiments demonstrate that our technique significantly outperforms previous\nstate-of-the-art methods on the 3D reconstruction and multi-view depth\nestimation tasks. Moreover, it is universally applicable and nearly cost-free,\nmaking it easily applied to other models and implemented with minimal test-time\ntraining overhead and parameter footprint. Code is available at\nhttps://github.com/nopQAQ/Test3R.\n", "link": "http://arxiv.org/abs/2506.13750v1", "date": "2025-06-16", "relevancy": 3.0766, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6393}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6033}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test3R%3A%20Learning%20to%20Reconstruct%203D%20at%20Test%20Time&body=Title%3A%20Test3R%3A%20Learning%20to%20Reconstruct%203D%20at%20Test%20Time%0AAuthor%3A%20Yuheng%20Yuan%20and%20Qiuhong%20Shen%20and%20Shizun%20Wang%20and%20Xingyi%20Yang%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Dense%20matching%20methods%20like%20DUSt3R%20regress%20pairwise%20pointmaps%20for%203D%0Areconstruction.%20However%2C%20the%20reliance%20on%20pairwise%20prediction%20and%20the%20limited%0Ageneralization%20capability%20inherently%20restrict%20the%20global%20geometric%20consistency.%0AIn%20this%20work%2C%20we%20introduce%20Test3R%2C%20a%20surprisingly%20simple%20test-time%20learning%0Atechnique%20that%20significantly%20boosts%20geometric%20accuracy.%20Using%20image%20triplets%0A%28%24I_1%2CI_2%2CI_3%24%29%2C%20Test3R%20generates%20reconstructions%20from%20pairs%20%28%24I_1%2CI_2%24%29%20and%0A%28%24I_1%2CI_3%24%29.%20The%20core%20idea%20is%20to%20optimize%20the%20network%20at%20test%20time%20via%20a%0Aself-supervised%20objective%3A%20maximizing%20the%20geometric%20consistency%20between%20these%0Atwo%20reconstructions%20relative%20to%20the%20common%20image%20%24I_1%24.%20This%20ensures%20the%20model%0Aproduces%20cross-pair%20consistent%20outputs%2C%20regardless%20of%20the%20inputs.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20technique%20significantly%20outperforms%20previous%0Astate-of-the-art%20methods%20on%20the%203D%20reconstruction%20and%20multi-view%20depth%0Aestimation%20tasks.%20Moreover%2C%20it%20is%20universally%20applicable%20and%20nearly%20cost-free%2C%0Amaking%20it%20easily%20applied%20to%20other%20models%20and%20implemented%20with%20minimal%20test-time%0Atraining%20overhead%20and%20parameter%20footprint.%20Code%20is%20available%20at%0Ahttps%3A//github.com/nopQAQ/Test3R.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest3R%253A%2520Learning%2520to%2520Reconstruct%25203D%2520at%2520Test%2520Time%26entry.906535625%3DYuheng%2520Yuan%2520and%2520Qiuhong%2520Shen%2520and%2520Shizun%2520Wang%2520and%2520Xingyi%2520Yang%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Dense%2520matching%2520methods%2520like%2520DUSt3R%2520regress%2520pairwise%2520pointmaps%2520for%25203D%250Areconstruction.%2520However%252C%2520the%2520reliance%2520on%2520pairwise%2520prediction%2520and%2520the%2520limited%250Ageneralization%2520capability%2520inherently%2520restrict%2520the%2520global%2520geometric%2520consistency.%250AIn%2520this%2520work%252C%2520we%2520introduce%2520Test3R%252C%2520a%2520surprisingly%2520simple%2520test-time%2520learning%250Atechnique%2520that%2520significantly%2520boosts%2520geometric%2520accuracy.%2520Using%2520image%2520triplets%250A%2528%2524I_1%252CI_2%252CI_3%2524%2529%252C%2520Test3R%2520generates%2520reconstructions%2520from%2520pairs%2520%2528%2524I_1%252CI_2%2524%2529%2520and%250A%2528%2524I_1%252CI_3%2524%2529.%2520The%2520core%2520idea%2520is%2520to%2520optimize%2520the%2520network%2520at%2520test%2520time%2520via%2520a%250Aself-supervised%2520objective%253A%2520maximizing%2520the%2520geometric%2520consistency%2520between%2520these%250Atwo%2520reconstructions%2520relative%2520to%2520the%2520common%2520image%2520%2524I_1%2524.%2520This%2520ensures%2520the%2520model%250Aproduces%2520cross-pair%2520consistent%2520outputs%252C%2520regardless%2520of%2520the%2520inputs.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520technique%2520significantly%2520outperforms%2520previous%250Astate-of-the-art%2520methods%2520on%2520the%25203D%2520reconstruction%2520and%2520multi-view%2520depth%250Aestimation%2520tasks.%2520Moreover%252C%2520it%2520is%2520universally%2520applicable%2520and%2520nearly%2520cost-free%252C%250Amaking%2520it%2520easily%2520applied%2520to%2520other%2520models%2520and%2520implemented%2520with%2520minimal%2520test-time%250Atraining%2520overhead%2520and%2520parameter%2520footprint.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/nopQAQ/Test3R.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test3R%3A%20Learning%20to%20Reconstruct%203D%20at%20Test%20Time&entry.906535625=Yuheng%20Yuan%20and%20Qiuhong%20Shen%20and%20Shizun%20Wang%20and%20Xingyi%20Yang%20and%20Xinchao%20Wang&entry.1292438233=%20%20Dense%20matching%20methods%20like%20DUSt3R%20regress%20pairwise%20pointmaps%20for%203D%0Areconstruction.%20However%2C%20the%20reliance%20on%20pairwise%20prediction%20and%20the%20limited%0Ageneralization%20capability%20inherently%20restrict%20the%20global%20geometric%20consistency.%0AIn%20this%20work%2C%20we%20introduce%20Test3R%2C%20a%20surprisingly%20simple%20test-time%20learning%0Atechnique%20that%20significantly%20boosts%20geometric%20accuracy.%20Using%20image%20triplets%0A%28%24I_1%2CI_2%2CI_3%24%29%2C%20Test3R%20generates%20reconstructions%20from%20pairs%20%28%24I_1%2CI_2%24%29%20and%0A%28%24I_1%2CI_3%24%29.%20The%20core%20idea%20is%20to%20optimize%20the%20network%20at%20test%20time%20via%20a%0Aself-supervised%20objective%3A%20maximizing%20the%20geometric%20consistency%20between%20these%0Atwo%20reconstructions%20relative%20to%20the%20common%20image%20%24I_1%24.%20This%20ensures%20the%20model%0Aproduces%20cross-pair%20consistent%20outputs%2C%20regardless%20of%20the%20inputs.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20technique%20significantly%20outperforms%20previous%0Astate-of-the-art%20methods%20on%20the%203D%20reconstruction%20and%20multi-view%20depth%0Aestimation%20tasks.%20Moreover%2C%20it%20is%20universally%20applicable%20and%20nearly%20cost-free%2C%0Amaking%20it%20easily%20applied%20to%20other%20models%20and%20implemented%20with%20minimal%20test-time%0Atraining%20overhead%20and%20parameter%20footprint.%20Code%20is%20available%20at%0Ahttps%3A//github.com/nopQAQ/Test3R.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13750v1&entry.124074799=Read"},
{"title": "Dive3D: Diverse Distillation-based Text-to-3D Generation via Score\n  Implicit Matching", "author": "Weimin Bai and Yubo Li and Wenzheng Chen and Weijian Luo and He Sun", "abstract": "  Distilling pre-trained 2D diffusion models into 3D assets has driven\nremarkable advances in text-to-3D synthesis. However, existing methods\ntypically rely on Score Distillation Sampling (SDS) loss, which involves\nasymmetric KL divergence--a formulation that inherently favors mode-seeking\nbehavior and limits generation diversity. In this paper, we introduce Dive3D, a\nnovel text-to-3D generation framework that replaces KL-based objectives with\nScore Implicit Matching (SIM) loss, a score-based objective that effectively\nmitigates mode collapse. Furthermore, Dive3D integrates both diffusion\ndistillation and reward-guided optimization under a unified divergence\nperspective. Such reformulation, together with SIM loss, yields significantly\nmore diverse 3D outputs while improving text alignment, human preference, and\noverall visual fidelity. We validate Dive3D across various 2D-to-3D prompts and\nfind that it consistently outperforms prior methods in qualitative assessments,\nincluding diversity, photorealism, and aesthetic appeal. We further evaluate\nits performance on the GPTEval3D benchmark, comparing against nine\nstate-of-the-art baselines. Dive3D also achieves strong results on quantitative\nmetrics, including text-asset alignment, 3D plausibility, text-geometry\nconsistency, texture quality, and geometric detail.\n", "link": "http://arxiv.org/abs/2506.13594v1", "date": "2025-06-16", "relevancy": 3.0723, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6239}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6239}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dive3D%3A%20Diverse%20Distillation-based%20Text-to-3D%20Generation%20via%20Score%0A%20%20Implicit%20Matching&body=Title%3A%20Dive3D%3A%20Diverse%20Distillation-based%20Text-to-3D%20Generation%20via%20Score%0A%20%20Implicit%20Matching%0AAuthor%3A%20Weimin%20Bai%20and%20Yubo%20Li%20and%20Wenzheng%20Chen%20and%20Weijian%20Luo%20and%20He%20Sun%0AAbstract%3A%20%20%20Distilling%20pre-trained%202D%20diffusion%20models%20into%203D%20assets%20has%20driven%0Aremarkable%20advances%20in%20text-to-3D%20synthesis.%20However%2C%20existing%20methods%0Atypically%20rely%20on%20Score%20Distillation%20Sampling%20%28SDS%29%20loss%2C%20which%20involves%0Aasymmetric%20KL%20divergence--a%20formulation%20that%20inherently%20favors%20mode-seeking%0Abehavior%20and%20limits%20generation%20diversity.%20In%20this%20paper%2C%20we%20introduce%20Dive3D%2C%20a%0Anovel%20text-to-3D%20generation%20framework%20that%20replaces%20KL-based%20objectives%20with%0AScore%20Implicit%20Matching%20%28SIM%29%20loss%2C%20a%20score-based%20objective%20that%20effectively%0Amitigates%20mode%20collapse.%20Furthermore%2C%20Dive3D%20integrates%20both%20diffusion%0Adistillation%20and%20reward-guided%20optimization%20under%20a%20unified%20divergence%0Aperspective.%20Such%20reformulation%2C%20together%20with%20SIM%20loss%2C%20yields%20significantly%0Amore%20diverse%203D%20outputs%20while%20improving%20text%20alignment%2C%20human%20preference%2C%20and%0Aoverall%20visual%20fidelity.%20We%20validate%20Dive3D%20across%20various%202D-to-3D%20prompts%20and%0Afind%20that%20it%20consistently%20outperforms%20prior%20methods%20in%20qualitative%20assessments%2C%0Aincluding%20diversity%2C%20photorealism%2C%20and%20aesthetic%20appeal.%20We%20further%20evaluate%0Aits%20performance%20on%20the%20GPTEval3D%20benchmark%2C%20comparing%20against%20nine%0Astate-of-the-art%20baselines.%20Dive3D%20also%20achieves%20strong%20results%20on%20quantitative%0Ametrics%2C%20including%20text-asset%20alignment%2C%203D%20plausibility%2C%20text-geometry%0Aconsistency%2C%20texture%20quality%2C%20and%20geometric%20detail.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDive3D%253A%2520Diverse%2520Distillation-based%2520Text-to-3D%2520Generation%2520via%2520Score%250A%2520%2520Implicit%2520Matching%26entry.906535625%3DWeimin%2520Bai%2520and%2520Yubo%2520Li%2520and%2520Wenzheng%2520Chen%2520and%2520Weijian%2520Luo%2520and%2520He%2520Sun%26entry.1292438233%3D%2520%2520Distilling%2520pre-trained%25202D%2520diffusion%2520models%2520into%25203D%2520assets%2520has%2520driven%250Aremarkable%2520advances%2520in%2520text-to-3D%2520synthesis.%2520However%252C%2520existing%2520methods%250Atypically%2520rely%2520on%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529%2520loss%252C%2520which%2520involves%250Aasymmetric%2520KL%2520divergence--a%2520formulation%2520that%2520inherently%2520favors%2520mode-seeking%250Abehavior%2520and%2520limits%2520generation%2520diversity.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Dive3D%252C%2520a%250Anovel%2520text-to-3D%2520generation%2520framework%2520that%2520replaces%2520KL-based%2520objectives%2520with%250AScore%2520Implicit%2520Matching%2520%2528SIM%2529%2520loss%252C%2520a%2520score-based%2520objective%2520that%2520effectively%250Amitigates%2520mode%2520collapse.%2520Furthermore%252C%2520Dive3D%2520integrates%2520both%2520diffusion%250Adistillation%2520and%2520reward-guided%2520optimization%2520under%2520a%2520unified%2520divergence%250Aperspective.%2520Such%2520reformulation%252C%2520together%2520with%2520SIM%2520loss%252C%2520yields%2520significantly%250Amore%2520diverse%25203D%2520outputs%2520while%2520improving%2520text%2520alignment%252C%2520human%2520preference%252C%2520and%250Aoverall%2520visual%2520fidelity.%2520We%2520validate%2520Dive3D%2520across%2520various%25202D-to-3D%2520prompts%2520and%250Afind%2520that%2520it%2520consistently%2520outperforms%2520prior%2520methods%2520in%2520qualitative%2520assessments%252C%250Aincluding%2520diversity%252C%2520photorealism%252C%2520and%2520aesthetic%2520appeal.%2520We%2520further%2520evaluate%250Aits%2520performance%2520on%2520the%2520GPTEval3D%2520benchmark%252C%2520comparing%2520against%2520nine%250Astate-of-the-art%2520baselines.%2520Dive3D%2520also%2520achieves%2520strong%2520results%2520on%2520quantitative%250Ametrics%252C%2520including%2520text-asset%2520alignment%252C%25203D%2520plausibility%252C%2520text-geometry%250Aconsistency%252C%2520texture%2520quality%252C%2520and%2520geometric%2520detail.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dive3D%3A%20Diverse%20Distillation-based%20Text-to-3D%20Generation%20via%20Score%0A%20%20Implicit%20Matching&entry.906535625=Weimin%20Bai%20and%20Yubo%20Li%20and%20Wenzheng%20Chen%20and%20Weijian%20Luo%20and%20He%20Sun&entry.1292438233=%20%20Distilling%20pre-trained%202D%20diffusion%20models%20into%203D%20assets%20has%20driven%0Aremarkable%20advances%20in%20text-to-3D%20synthesis.%20However%2C%20existing%20methods%0Atypically%20rely%20on%20Score%20Distillation%20Sampling%20%28SDS%29%20loss%2C%20which%20involves%0Aasymmetric%20KL%20divergence--a%20formulation%20that%20inherently%20favors%20mode-seeking%0Abehavior%20and%20limits%20generation%20diversity.%20In%20this%20paper%2C%20we%20introduce%20Dive3D%2C%20a%0Anovel%20text-to-3D%20generation%20framework%20that%20replaces%20KL-based%20objectives%20with%0AScore%20Implicit%20Matching%20%28SIM%29%20loss%2C%20a%20score-based%20objective%20that%20effectively%0Amitigates%20mode%20collapse.%20Furthermore%2C%20Dive3D%20integrates%20both%20diffusion%0Adistillation%20and%20reward-guided%20optimization%20under%20a%20unified%20divergence%0Aperspective.%20Such%20reformulation%2C%20together%20with%20SIM%20loss%2C%20yields%20significantly%0Amore%20diverse%203D%20outputs%20while%20improving%20text%20alignment%2C%20human%20preference%2C%20and%0Aoverall%20visual%20fidelity.%20We%20validate%20Dive3D%20across%20various%202D-to-3D%20prompts%20and%0Afind%20that%20it%20consistently%20outperforms%20prior%20methods%20in%20qualitative%20assessments%2C%0Aincluding%20diversity%2C%20photorealism%2C%20and%20aesthetic%20appeal.%20We%20further%20evaluate%0Aits%20performance%20on%20the%20GPTEval3D%20benchmark%2C%20comparing%20against%20nine%0Astate-of-the-art%20baselines.%20Dive3D%20also%20achieves%20strong%20results%20on%20quantitative%0Ametrics%2C%20including%20text-asset%20alignment%2C%203D%20plausibility%2C%20text-geometry%0Aconsistency%2C%20texture%20quality%2C%20and%20geometric%20detail.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13594v1&entry.124074799=Read"},
{"title": "Unveiling the Learning Mind of Language Models: A Cognitive Framework\n  and Empirical Study", "author": "Zhengyu Hu and Jianxun Lian and Zheyuan Xiao and Seraphina Zhang and Tianfu Wang and Nicholas Jing Yuan and Xing Xie and Hui Xiong", "abstract": "  Large language models (LLMs) have shown impressive capabilities across tasks\nsuch as mathematics, coding, and reasoning, yet their learning ability, which\nis crucial for adapting to dynamic environments and acquiring new knowledge,\nremains underexplored. In this work, we address this gap by introducing a\nframework inspired by cognitive psychology and education. Specifically, we\ndecompose general learning ability into three distinct, complementary\ndimensions: Learning from Instructor (acquiring knowledge via explicit\nguidance), Learning from Concept (internalizing abstract structures and\ngeneralizing to new contexts), and Learning from Experience (adapting through\naccumulated exploration and feedback). We conduct a comprehensive empirical\nstudy across the three learning dimensions and identify several insightful\nfindings, such as (i) interaction improves learning; (ii) conceptual\nunderstanding is scale-emergent and benefits larger models; and (iii) LLMs are\neffective few-shot learners but not many-shot learners. Based on our framework\nand empirical findings, we introduce a benchmark that provides a unified and\nrealistic evaluation of LLMs' general learning abilities across three learning\ncognition dimensions. It enables diagnostic insights and supports evaluation\nand development of more adaptive and human-like models.\n", "link": "http://arxiv.org/abs/2506.13464v1", "date": "2025-06-16", "relevancy": 2.9901, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6143}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6143}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20the%20Learning%20Mind%20of%20Language%20Models%3A%20A%20Cognitive%20Framework%0A%20%20and%20Empirical%20Study&body=Title%3A%20Unveiling%20the%20Learning%20Mind%20of%20Language%20Models%3A%20A%20Cognitive%20Framework%0A%20%20and%20Empirical%20Study%0AAuthor%3A%20Zhengyu%20Hu%20and%20Jianxun%20Lian%20and%20Zheyuan%20Xiao%20and%20Seraphina%20Zhang%20and%20Tianfu%20Wang%20and%20Nicholas%20Jing%20Yuan%20and%20Xing%20Xie%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20impressive%20capabilities%20across%20tasks%0Asuch%20as%20mathematics%2C%20coding%2C%20and%20reasoning%2C%20yet%20their%20learning%20ability%2C%20which%0Ais%20crucial%20for%20adapting%20to%20dynamic%20environments%20and%20acquiring%20new%20knowledge%2C%0Aremains%20underexplored.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20introducing%20a%0Aframework%20inspired%20by%20cognitive%20psychology%20and%20education.%20Specifically%2C%20we%0Adecompose%20general%20learning%20ability%20into%20three%20distinct%2C%20complementary%0Adimensions%3A%20Learning%20from%20Instructor%20%28acquiring%20knowledge%20via%20explicit%0Aguidance%29%2C%20Learning%20from%20Concept%20%28internalizing%20abstract%20structures%20and%0Ageneralizing%20to%20new%20contexts%29%2C%20and%20Learning%20from%20Experience%20%28adapting%20through%0Aaccumulated%20exploration%20and%20feedback%29.%20We%20conduct%20a%20comprehensive%20empirical%0Astudy%20across%20the%20three%20learning%20dimensions%20and%20identify%20several%20insightful%0Afindings%2C%20such%20as%20%28i%29%20interaction%20improves%20learning%3B%20%28ii%29%20conceptual%0Aunderstanding%20is%20scale-emergent%20and%20benefits%20larger%20models%3B%20and%20%28iii%29%20LLMs%20are%0Aeffective%20few-shot%20learners%20but%20not%20many-shot%20learners.%20Based%20on%20our%20framework%0Aand%20empirical%20findings%2C%20we%20introduce%20a%20benchmark%20that%20provides%20a%20unified%20and%0Arealistic%20evaluation%20of%20LLMs%27%20general%20learning%20abilities%20across%20three%20learning%0Acognition%20dimensions.%20It%20enables%20diagnostic%20insights%20and%20supports%20evaluation%0Aand%20development%20of%20more%20adaptive%20and%20human-like%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520the%2520Learning%2520Mind%2520of%2520Language%2520Models%253A%2520A%2520Cognitive%2520Framework%250A%2520%2520and%2520Empirical%2520Study%26entry.906535625%3DZhengyu%2520Hu%2520and%2520Jianxun%2520Lian%2520and%2520Zheyuan%2520Xiao%2520and%2520Seraphina%2520Zhang%2520and%2520Tianfu%2520Wang%2520and%2520Nicholas%2520Jing%2520Yuan%2520and%2520Xing%2520Xie%2520and%2520Hui%2520Xiong%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520impressive%2520capabilities%2520across%2520tasks%250Asuch%2520as%2520mathematics%252C%2520coding%252C%2520and%2520reasoning%252C%2520yet%2520their%2520learning%2520ability%252C%2520which%250Ais%2520crucial%2520for%2520adapting%2520to%2520dynamic%2520environments%2520and%2520acquiring%2520new%2520knowledge%252C%250Aremains%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520gap%2520by%2520introducing%2520a%250Aframework%2520inspired%2520by%2520cognitive%2520psychology%2520and%2520education.%2520Specifically%252C%2520we%250Adecompose%2520general%2520learning%2520ability%2520into%2520three%2520distinct%252C%2520complementary%250Adimensions%253A%2520Learning%2520from%2520Instructor%2520%2528acquiring%2520knowledge%2520via%2520explicit%250Aguidance%2529%252C%2520Learning%2520from%2520Concept%2520%2528internalizing%2520abstract%2520structures%2520and%250Ageneralizing%2520to%2520new%2520contexts%2529%252C%2520and%2520Learning%2520from%2520Experience%2520%2528adapting%2520through%250Aaccumulated%2520exploration%2520and%2520feedback%2529.%2520We%2520conduct%2520a%2520comprehensive%2520empirical%250Astudy%2520across%2520the%2520three%2520learning%2520dimensions%2520and%2520identify%2520several%2520insightful%250Afindings%252C%2520such%2520as%2520%2528i%2529%2520interaction%2520improves%2520learning%253B%2520%2528ii%2529%2520conceptual%250Aunderstanding%2520is%2520scale-emergent%2520and%2520benefits%2520larger%2520models%253B%2520and%2520%2528iii%2529%2520LLMs%2520are%250Aeffective%2520few-shot%2520learners%2520but%2520not%2520many-shot%2520learners.%2520Based%2520on%2520our%2520framework%250Aand%2520empirical%2520findings%252C%2520we%2520introduce%2520a%2520benchmark%2520that%2520provides%2520a%2520unified%2520and%250Arealistic%2520evaluation%2520of%2520LLMs%2527%2520general%2520learning%2520abilities%2520across%2520three%2520learning%250Acognition%2520dimensions.%2520It%2520enables%2520diagnostic%2520insights%2520and%2520supports%2520evaluation%250Aand%2520development%2520of%2520more%2520adaptive%2520and%2520human-like%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20Learning%20Mind%20of%20Language%20Models%3A%20A%20Cognitive%20Framework%0A%20%20and%20Empirical%20Study&entry.906535625=Zhengyu%20Hu%20and%20Jianxun%20Lian%20and%20Zheyuan%20Xiao%20and%20Seraphina%20Zhang%20and%20Tianfu%20Wang%20and%20Nicholas%20Jing%20Yuan%20and%20Xing%20Xie%20and%20Hui%20Xiong&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20impressive%20capabilities%20across%20tasks%0Asuch%20as%20mathematics%2C%20coding%2C%20and%20reasoning%2C%20yet%20their%20learning%20ability%2C%20which%0Ais%20crucial%20for%20adapting%20to%20dynamic%20environments%20and%20acquiring%20new%20knowledge%2C%0Aremains%20underexplored.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20introducing%20a%0Aframework%20inspired%20by%20cognitive%20psychology%20and%20education.%20Specifically%2C%20we%0Adecompose%20general%20learning%20ability%20into%20three%20distinct%2C%20complementary%0Adimensions%3A%20Learning%20from%20Instructor%20%28acquiring%20knowledge%20via%20explicit%0Aguidance%29%2C%20Learning%20from%20Concept%20%28internalizing%20abstract%20structures%20and%0Ageneralizing%20to%20new%20contexts%29%2C%20and%20Learning%20from%20Experience%20%28adapting%20through%0Aaccumulated%20exploration%20and%20feedback%29.%20We%20conduct%20a%20comprehensive%20empirical%0Astudy%20across%20the%20three%20learning%20dimensions%20and%20identify%20several%20insightful%0Afindings%2C%20such%20as%20%28i%29%20interaction%20improves%20learning%3B%20%28ii%29%20conceptual%0Aunderstanding%20is%20scale-emergent%20and%20benefits%20larger%20models%3B%20and%20%28iii%29%20LLMs%20are%0Aeffective%20few-shot%20learners%20but%20not%20many-shot%20learners.%20Based%20on%20our%20framework%0Aand%20empirical%20findings%2C%20we%20introduce%20a%20benchmark%20that%20provides%20a%20unified%20and%0Arealistic%20evaluation%20of%20LLMs%27%20general%20learning%20abilities%20across%20three%20learning%0Acognition%20dimensions.%20It%20enables%20diagnostic%20insights%20and%20supports%20evaluation%0Aand%20development%20of%20more%20adaptive%20and%20human-like%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13464v1&entry.124074799=Read"},
{"title": "Limited-Angle CBCT Reconstruction via Geometry-Integrated Cycle-domain\n  Denoising Diffusion Probabilistic Models", "author": "Yuan Gao and Shaoyan Pan and Mingzhe Hu and Huiqiao Xie and Jill Remick and Chih-Wei Chang and Justin Roper and Zhen Tian and Xiaofeng Yang", "abstract": "  Cone-beam CT (CBCT) is widely used in clinical radiotherapy for image-guided\ntreatment, improving setup accuracy, adaptive planning, and motion management.\nHowever, slow gantry rotation limits performance by introducing motion\nartifacts, blurring, and increased dose. This work aims to develop a clinically\nfeasible method for reconstructing high-quality CBCT volumes from consecutive\nlimited-angle acquisitions, addressing imaging challenges in time- or\ndose-constrained settings. We propose a limited-angle (LA) geometry-integrated\ncycle-domain (LA-GICD) framework for CBCT reconstruction, comprising two\ndenoising diffusion probabilistic models (DDPMs) connected via analytic\ncone-beam forward and back projectors. A Projection-DDPM completes missing\nprojections, followed by back-projection, and an Image-DDPM refines the volume.\nThis dual-domain design leverages complementary priors from projection and\nimage spaces to achieve high-quality reconstructions from limited-angle (<= 90\ndegrees) scans. Performance was evaluated against full-angle reconstruction.\nFour board-certified medical physicists conducted assessments. A total of 78\nplanning CTs in common CBCT geometries were used for training and evaluation.\nThe method achieved a mean absolute error of 35.5 HU, SSIM of 0.84, and PSNR of\n29.8 dB, with visibly reduced artifacts and improved soft-tissue clarity.\nLA-GICD's geometry-aware dual-domain learning, embedded in analytic\nforward/backward operators, enabled artifact-free, high-contrast\nreconstructions from a single 90-degree scan, reducing acquisition time and\ndose four-fold. LA-GICD improves limited-angle CBCT reconstruction with strong\ndata fidelity and anatomical realism. It offers a practical solution for\nshort-arc acquisitions, enhancing CBCT use in radiotherapy by providing\nclinically applicable images with reduced scan time and dose for more accurate,\npersonalized treatments.\n", "link": "http://arxiv.org/abs/2506.13545v1", "date": "2025-06-16", "relevancy": 2.9732, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.611}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.611}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Limited-Angle%20CBCT%20Reconstruction%20via%20Geometry-Integrated%20Cycle-domain%0A%20%20Denoising%20Diffusion%20Probabilistic%20Models&body=Title%3A%20Limited-Angle%20CBCT%20Reconstruction%20via%20Geometry-Integrated%20Cycle-domain%0A%20%20Denoising%20Diffusion%20Probabilistic%20Models%0AAuthor%3A%20Yuan%20Gao%20and%20Shaoyan%20Pan%20and%20Mingzhe%20Hu%20and%20Huiqiao%20Xie%20and%20Jill%20Remick%20and%20Chih-Wei%20Chang%20and%20Justin%20Roper%20and%20Zhen%20Tian%20and%20Xiaofeng%20Yang%0AAbstract%3A%20%20%20Cone-beam%20CT%20%28CBCT%29%20is%20widely%20used%20in%20clinical%20radiotherapy%20for%20image-guided%0Atreatment%2C%20improving%20setup%20accuracy%2C%20adaptive%20planning%2C%20and%20motion%20management.%0AHowever%2C%20slow%20gantry%20rotation%20limits%20performance%20by%20introducing%20motion%0Aartifacts%2C%20blurring%2C%20and%20increased%20dose.%20This%20work%20aims%20to%20develop%20a%20clinically%0Afeasible%20method%20for%20reconstructing%20high-quality%20CBCT%20volumes%20from%20consecutive%0Alimited-angle%20acquisitions%2C%20addressing%20imaging%20challenges%20in%20time-%20or%0Adose-constrained%20settings.%20We%20propose%20a%20limited-angle%20%28LA%29%20geometry-integrated%0Acycle-domain%20%28LA-GICD%29%20framework%20for%20CBCT%20reconstruction%2C%20comprising%20two%0Adenoising%20diffusion%20probabilistic%20models%20%28DDPMs%29%20connected%20via%20analytic%0Acone-beam%20forward%20and%20back%20projectors.%20A%20Projection-DDPM%20completes%20missing%0Aprojections%2C%20followed%20by%20back-projection%2C%20and%20an%20Image-DDPM%20refines%20the%20volume.%0AThis%20dual-domain%20design%20leverages%20complementary%20priors%20from%20projection%20and%0Aimage%20spaces%20to%20achieve%20high-quality%20reconstructions%20from%20limited-angle%20%28%3C%3D%2090%0Adegrees%29%20scans.%20Performance%20was%20evaluated%20against%20full-angle%20reconstruction.%0AFour%20board-certified%20medical%20physicists%20conducted%20assessments.%20A%20total%20of%2078%0Aplanning%20CTs%20in%20common%20CBCT%20geometries%20were%20used%20for%20training%20and%20evaluation.%0AThe%20method%20achieved%20a%20mean%20absolute%20error%20of%2035.5%20HU%2C%20SSIM%20of%200.84%2C%20and%20PSNR%20of%0A29.8%20dB%2C%20with%20visibly%20reduced%20artifacts%20and%20improved%20soft-tissue%20clarity.%0ALA-GICD%27s%20geometry-aware%20dual-domain%20learning%2C%20embedded%20in%20analytic%0Aforward/backward%20operators%2C%20enabled%20artifact-free%2C%20high-contrast%0Areconstructions%20from%20a%20single%2090-degree%20scan%2C%20reducing%20acquisition%20time%20and%0Adose%20four-fold.%20LA-GICD%20improves%20limited-angle%20CBCT%20reconstruction%20with%20strong%0Adata%20fidelity%20and%20anatomical%20realism.%20It%20offers%20a%20practical%20solution%20for%0Ashort-arc%20acquisitions%2C%20enhancing%20CBCT%20use%20in%20radiotherapy%20by%20providing%0Aclinically%20applicable%20images%20with%20reduced%20scan%20time%20and%20dose%20for%20more%20accurate%2C%0Apersonalized%20treatments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLimited-Angle%2520CBCT%2520Reconstruction%2520via%2520Geometry-Integrated%2520Cycle-domain%250A%2520%2520Denoising%2520Diffusion%2520Probabilistic%2520Models%26entry.906535625%3DYuan%2520Gao%2520and%2520Shaoyan%2520Pan%2520and%2520Mingzhe%2520Hu%2520and%2520Huiqiao%2520Xie%2520and%2520Jill%2520Remick%2520and%2520Chih-Wei%2520Chang%2520and%2520Justin%2520Roper%2520and%2520Zhen%2520Tian%2520and%2520Xiaofeng%2520Yang%26entry.1292438233%3D%2520%2520Cone-beam%2520CT%2520%2528CBCT%2529%2520is%2520widely%2520used%2520in%2520clinical%2520radiotherapy%2520for%2520image-guided%250Atreatment%252C%2520improving%2520setup%2520accuracy%252C%2520adaptive%2520planning%252C%2520and%2520motion%2520management.%250AHowever%252C%2520slow%2520gantry%2520rotation%2520limits%2520performance%2520by%2520introducing%2520motion%250Aartifacts%252C%2520blurring%252C%2520and%2520increased%2520dose.%2520This%2520work%2520aims%2520to%2520develop%2520a%2520clinically%250Afeasible%2520method%2520for%2520reconstructing%2520high-quality%2520CBCT%2520volumes%2520from%2520consecutive%250Alimited-angle%2520acquisitions%252C%2520addressing%2520imaging%2520challenges%2520in%2520time-%2520or%250Adose-constrained%2520settings.%2520We%2520propose%2520a%2520limited-angle%2520%2528LA%2529%2520geometry-integrated%250Acycle-domain%2520%2528LA-GICD%2529%2520framework%2520for%2520CBCT%2520reconstruction%252C%2520comprising%2520two%250Adenoising%2520diffusion%2520probabilistic%2520models%2520%2528DDPMs%2529%2520connected%2520via%2520analytic%250Acone-beam%2520forward%2520and%2520back%2520projectors.%2520A%2520Projection-DDPM%2520completes%2520missing%250Aprojections%252C%2520followed%2520by%2520back-projection%252C%2520and%2520an%2520Image-DDPM%2520refines%2520the%2520volume.%250AThis%2520dual-domain%2520design%2520leverages%2520complementary%2520priors%2520from%2520projection%2520and%250Aimage%2520spaces%2520to%2520achieve%2520high-quality%2520reconstructions%2520from%2520limited-angle%2520%2528%253C%253D%252090%250Adegrees%2529%2520scans.%2520Performance%2520was%2520evaluated%2520against%2520full-angle%2520reconstruction.%250AFour%2520board-certified%2520medical%2520physicists%2520conducted%2520assessments.%2520A%2520total%2520of%252078%250Aplanning%2520CTs%2520in%2520common%2520CBCT%2520geometries%2520were%2520used%2520for%2520training%2520and%2520evaluation.%250AThe%2520method%2520achieved%2520a%2520mean%2520absolute%2520error%2520of%252035.5%2520HU%252C%2520SSIM%2520of%25200.84%252C%2520and%2520PSNR%2520of%250A29.8%2520dB%252C%2520with%2520visibly%2520reduced%2520artifacts%2520and%2520improved%2520soft-tissue%2520clarity.%250ALA-GICD%2527s%2520geometry-aware%2520dual-domain%2520learning%252C%2520embedded%2520in%2520analytic%250Aforward/backward%2520operators%252C%2520enabled%2520artifact-free%252C%2520high-contrast%250Areconstructions%2520from%2520a%2520single%252090-degree%2520scan%252C%2520reducing%2520acquisition%2520time%2520and%250Adose%2520four-fold.%2520LA-GICD%2520improves%2520limited-angle%2520CBCT%2520reconstruction%2520with%2520strong%250Adata%2520fidelity%2520and%2520anatomical%2520realism.%2520It%2520offers%2520a%2520practical%2520solution%2520for%250Ashort-arc%2520acquisitions%252C%2520enhancing%2520CBCT%2520use%2520in%2520radiotherapy%2520by%2520providing%250Aclinically%2520applicable%2520images%2520with%2520reduced%2520scan%2520time%2520and%2520dose%2520for%2520more%2520accurate%252C%250Apersonalized%2520treatments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Limited-Angle%20CBCT%20Reconstruction%20via%20Geometry-Integrated%20Cycle-domain%0A%20%20Denoising%20Diffusion%20Probabilistic%20Models&entry.906535625=Yuan%20Gao%20and%20Shaoyan%20Pan%20and%20Mingzhe%20Hu%20and%20Huiqiao%20Xie%20and%20Jill%20Remick%20and%20Chih-Wei%20Chang%20and%20Justin%20Roper%20and%20Zhen%20Tian%20and%20Xiaofeng%20Yang&entry.1292438233=%20%20Cone-beam%20CT%20%28CBCT%29%20is%20widely%20used%20in%20clinical%20radiotherapy%20for%20image-guided%0Atreatment%2C%20improving%20setup%20accuracy%2C%20adaptive%20planning%2C%20and%20motion%20management.%0AHowever%2C%20slow%20gantry%20rotation%20limits%20performance%20by%20introducing%20motion%0Aartifacts%2C%20blurring%2C%20and%20increased%20dose.%20This%20work%20aims%20to%20develop%20a%20clinically%0Afeasible%20method%20for%20reconstructing%20high-quality%20CBCT%20volumes%20from%20consecutive%0Alimited-angle%20acquisitions%2C%20addressing%20imaging%20challenges%20in%20time-%20or%0Adose-constrained%20settings.%20We%20propose%20a%20limited-angle%20%28LA%29%20geometry-integrated%0Acycle-domain%20%28LA-GICD%29%20framework%20for%20CBCT%20reconstruction%2C%20comprising%20two%0Adenoising%20diffusion%20probabilistic%20models%20%28DDPMs%29%20connected%20via%20analytic%0Acone-beam%20forward%20and%20back%20projectors.%20A%20Projection-DDPM%20completes%20missing%0Aprojections%2C%20followed%20by%20back-projection%2C%20and%20an%20Image-DDPM%20refines%20the%20volume.%0AThis%20dual-domain%20design%20leverages%20complementary%20priors%20from%20projection%20and%0Aimage%20spaces%20to%20achieve%20high-quality%20reconstructions%20from%20limited-angle%20%28%3C%3D%2090%0Adegrees%29%20scans.%20Performance%20was%20evaluated%20against%20full-angle%20reconstruction.%0AFour%20board-certified%20medical%20physicists%20conducted%20assessments.%20A%20total%20of%2078%0Aplanning%20CTs%20in%20common%20CBCT%20geometries%20were%20used%20for%20training%20and%20evaluation.%0AThe%20method%20achieved%20a%20mean%20absolute%20error%20of%2035.5%20HU%2C%20SSIM%20of%200.84%2C%20and%20PSNR%20of%0A29.8%20dB%2C%20with%20visibly%20reduced%20artifacts%20and%20improved%20soft-tissue%20clarity.%0ALA-GICD%27s%20geometry-aware%20dual-domain%20learning%2C%20embedded%20in%20analytic%0Aforward/backward%20operators%2C%20enabled%20artifact-free%2C%20high-contrast%0Areconstructions%20from%20a%20single%2090-degree%20scan%2C%20reducing%20acquisition%20time%20and%0Adose%20four-fold.%20LA-GICD%20improves%20limited-angle%20CBCT%20reconstruction%20with%20strong%0Adata%20fidelity%20and%20anatomical%20realism.%20It%20offers%20a%20practical%20solution%20for%0Ashort-arc%20acquisitions%2C%20enhancing%20CBCT%20use%20in%20radiotherapy%20by%20providing%0Aclinically%20applicable%20images%20with%20reduced%20scan%20time%20and%20dose%20for%20more%20accurate%2C%0Apersonalized%20treatments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13545v1&entry.124074799=Read"},
{"title": "Dissecting RGB-D Learning for Improved Multi-modal Fusion", "author": "Hao Chen and Haoran Zhou and Yunshu Zhang and Zheng Lin and Yongjian Deng", "abstract": "  In the RGB-D vision community, extensive research has been focused on\ndesigning multi-modal learning strategies and fusion structures. However, the\ncomplementary and fusion mechanisms in RGB-D models remain a black box. In this\npaper, we present an analytical framework and a novel score to dissect the\nRGB-D vision community. Our approach involves measuring proposed semantic\nvariance and feature similarity across modalities and levels, conducting visual\nand quantitative analyzes on multi-modal learning through comprehensive\nexperiments. Specifically, we investigate the consistency and specialty of\nfeatures across modalities, evolution rules within each modality, and the\ncollaboration logic used when optimizing a RGB-D model. Our studies\nreveal/verify several important findings, such as the discrepancy in\ncross-modal features and the hybrid multi-modal cooperation rule, which\nhighlights consistency and specialty simultaneously for complementary\ninference. We also showcase the versatility of the proposed RGB-D dissection\nmethod and introduce a straightforward fusion strategy based on our findings,\nwhich delivers significant enhancements across various tasks and even other\nmulti-modal data.\n", "link": "http://arxiv.org/abs/2308.10019v2", "date": "2025-06-16", "relevancy": 2.9427, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6017}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6017}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dissecting%20RGB-D%20Learning%20for%20Improved%20Multi-modal%20Fusion&body=Title%3A%20Dissecting%20RGB-D%20Learning%20for%20Improved%20Multi-modal%20Fusion%0AAuthor%3A%20Hao%20Chen%20and%20Haoran%20Zhou%20and%20Yunshu%20Zhang%20and%20Zheng%20Lin%20and%20Yongjian%20Deng%0AAbstract%3A%20%20%20In%20the%20RGB-D%20vision%20community%2C%20extensive%20research%20has%20been%20focused%20on%0Adesigning%20multi-modal%20learning%20strategies%20and%20fusion%20structures.%20However%2C%20the%0Acomplementary%20and%20fusion%20mechanisms%20in%20RGB-D%20models%20remain%20a%20black%20box.%20In%20this%0Apaper%2C%20we%20present%20an%20analytical%20framework%20and%20a%20novel%20score%20to%20dissect%20the%0ARGB-D%20vision%20community.%20Our%20approach%20involves%20measuring%20proposed%20semantic%0Avariance%20and%20feature%20similarity%20across%20modalities%20and%20levels%2C%20conducting%20visual%0Aand%20quantitative%20analyzes%20on%20multi-modal%20learning%20through%20comprehensive%0Aexperiments.%20Specifically%2C%20we%20investigate%20the%20consistency%20and%20specialty%20of%0Afeatures%20across%20modalities%2C%20evolution%20rules%20within%20each%20modality%2C%20and%20the%0Acollaboration%20logic%20used%20when%20optimizing%20a%20RGB-D%20model.%20Our%20studies%0Areveal/verify%20several%20important%20findings%2C%20such%20as%20the%20discrepancy%20in%0Across-modal%20features%20and%20the%20hybrid%20multi-modal%20cooperation%20rule%2C%20which%0Ahighlights%20consistency%20and%20specialty%20simultaneously%20for%20complementary%0Ainference.%20We%20also%20showcase%20the%20versatility%20of%20the%20proposed%20RGB-D%20dissection%0Amethod%20and%20introduce%20a%20straightforward%20fusion%20strategy%20based%20on%20our%20findings%2C%0Awhich%20delivers%20significant%20enhancements%20across%20various%20tasks%20and%20even%20other%0Amulti-modal%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.10019v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDissecting%2520RGB-D%2520Learning%2520for%2520Improved%2520Multi-modal%2520Fusion%26entry.906535625%3DHao%2520Chen%2520and%2520Haoran%2520Zhou%2520and%2520Yunshu%2520Zhang%2520and%2520Zheng%2520Lin%2520and%2520Yongjian%2520Deng%26entry.1292438233%3D%2520%2520In%2520the%2520RGB-D%2520vision%2520community%252C%2520extensive%2520research%2520has%2520been%2520focused%2520on%250Adesigning%2520multi-modal%2520learning%2520strategies%2520and%2520fusion%2520structures.%2520However%252C%2520the%250Acomplementary%2520and%2520fusion%2520mechanisms%2520in%2520RGB-D%2520models%2520remain%2520a%2520black%2520box.%2520In%2520this%250Apaper%252C%2520we%2520present%2520an%2520analytical%2520framework%2520and%2520a%2520novel%2520score%2520to%2520dissect%2520the%250ARGB-D%2520vision%2520community.%2520Our%2520approach%2520involves%2520measuring%2520proposed%2520semantic%250Avariance%2520and%2520feature%2520similarity%2520across%2520modalities%2520and%2520levels%252C%2520conducting%2520visual%250Aand%2520quantitative%2520analyzes%2520on%2520multi-modal%2520learning%2520through%2520comprehensive%250Aexperiments.%2520Specifically%252C%2520we%2520investigate%2520the%2520consistency%2520and%2520specialty%2520of%250Afeatures%2520across%2520modalities%252C%2520evolution%2520rules%2520within%2520each%2520modality%252C%2520and%2520the%250Acollaboration%2520logic%2520used%2520when%2520optimizing%2520a%2520RGB-D%2520model.%2520Our%2520studies%250Areveal/verify%2520several%2520important%2520findings%252C%2520such%2520as%2520the%2520discrepancy%2520in%250Across-modal%2520features%2520and%2520the%2520hybrid%2520multi-modal%2520cooperation%2520rule%252C%2520which%250Ahighlights%2520consistency%2520and%2520specialty%2520simultaneously%2520for%2520complementary%250Ainference.%2520We%2520also%2520showcase%2520the%2520versatility%2520of%2520the%2520proposed%2520RGB-D%2520dissection%250Amethod%2520and%2520introduce%2520a%2520straightforward%2520fusion%2520strategy%2520based%2520on%2520our%2520findings%252C%250Awhich%2520delivers%2520significant%2520enhancements%2520across%2520various%2520tasks%2520and%2520even%2520other%250Amulti-modal%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.10019v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dissecting%20RGB-D%20Learning%20for%20Improved%20Multi-modal%20Fusion&entry.906535625=Hao%20Chen%20and%20Haoran%20Zhou%20and%20Yunshu%20Zhang%20and%20Zheng%20Lin%20and%20Yongjian%20Deng&entry.1292438233=%20%20In%20the%20RGB-D%20vision%20community%2C%20extensive%20research%20has%20been%20focused%20on%0Adesigning%20multi-modal%20learning%20strategies%20and%20fusion%20structures.%20However%2C%20the%0Acomplementary%20and%20fusion%20mechanisms%20in%20RGB-D%20models%20remain%20a%20black%20box.%20In%20this%0Apaper%2C%20we%20present%20an%20analytical%20framework%20and%20a%20novel%20score%20to%20dissect%20the%0ARGB-D%20vision%20community.%20Our%20approach%20involves%20measuring%20proposed%20semantic%0Avariance%20and%20feature%20similarity%20across%20modalities%20and%20levels%2C%20conducting%20visual%0Aand%20quantitative%20analyzes%20on%20multi-modal%20learning%20through%20comprehensive%0Aexperiments.%20Specifically%2C%20we%20investigate%20the%20consistency%20and%20specialty%20of%0Afeatures%20across%20modalities%2C%20evolution%20rules%20within%20each%20modality%2C%20and%20the%0Acollaboration%20logic%20used%20when%20optimizing%20a%20RGB-D%20model.%20Our%20studies%0Areveal/verify%20several%20important%20findings%2C%20such%20as%20the%20discrepancy%20in%0Across-modal%20features%20and%20the%20hybrid%20multi-modal%20cooperation%20rule%2C%20which%0Ahighlights%20consistency%20and%20specialty%20simultaneously%20for%20complementary%0Ainference.%20We%20also%20showcase%20the%20versatility%20of%20the%20proposed%20RGB-D%20dissection%0Amethod%20and%20introduce%20a%20straightforward%20fusion%20strategy%20based%20on%20our%20findings%2C%0Awhich%20delivers%20significant%20enhancements%20across%20various%20tasks%20and%20even%20other%0Amulti-modal%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.10019v2&entry.124074799=Read"},
{"title": "VideoMat: Extracting PBR Materials from Video Diffusion Models", "author": "Jacob Munkberg and Zian Wang and Ruofan Liang and Tianchang Shen and Jon Hasselgren", "abstract": "  We leverage finetuned video diffusion models, intrinsic decomposition of\nvideos, and physically-based differentiable rendering to generate high quality\nmaterials for 3D models given a text prompt or a single image. We condition a\nvideo diffusion model to respect the input geometry and lighting condition.\nThis model produces multiple views of a given 3D model with coherent material\nproperties. Secondly, we use a recent model to extract intrinsics (base color,\nroughness, metallic) from the generated video. Finally, we use the intrinsics\nalongside the generated video in a differentiable path tracer to robustly\nextract PBR materials directly compatible with common content creation tools.\n", "link": "http://arxiv.org/abs/2506.09665v2", "date": "2025-06-16", "relevancy": 2.9319, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6031}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5799}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoMat%3A%20Extracting%20PBR%20Materials%20from%20Video%20Diffusion%20Models&body=Title%3A%20VideoMat%3A%20Extracting%20PBR%20Materials%20from%20Video%20Diffusion%20Models%0AAuthor%3A%20Jacob%20Munkberg%20and%20Zian%20Wang%20and%20Ruofan%20Liang%20and%20Tianchang%20Shen%20and%20Jon%20Hasselgren%0AAbstract%3A%20%20%20We%20leverage%20finetuned%20video%20diffusion%20models%2C%20intrinsic%20decomposition%20of%0Avideos%2C%20and%20physically-based%20differentiable%20rendering%20to%20generate%20high%20quality%0Amaterials%20for%203D%20models%20given%20a%20text%20prompt%20or%20a%20single%20image.%20We%20condition%20a%0Avideo%20diffusion%20model%20to%20respect%20the%20input%20geometry%20and%20lighting%20condition.%0AThis%20model%20produces%20multiple%20views%20of%20a%20given%203D%20model%20with%20coherent%20material%0Aproperties.%20Secondly%2C%20we%20use%20a%20recent%20model%20to%20extract%20intrinsics%20%28base%20color%2C%0Aroughness%2C%20metallic%29%20from%20the%20generated%20video.%20Finally%2C%20we%20use%20the%20intrinsics%0Aalongside%20the%20generated%20video%20in%20a%20differentiable%20path%20tracer%20to%20robustly%0Aextract%20PBR%20materials%20directly%20compatible%20with%20common%20content%20creation%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09665v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoMat%253A%2520Extracting%2520PBR%2520Materials%2520from%2520Video%2520Diffusion%2520Models%26entry.906535625%3DJacob%2520Munkberg%2520and%2520Zian%2520Wang%2520and%2520Ruofan%2520Liang%2520and%2520Tianchang%2520Shen%2520and%2520Jon%2520Hasselgren%26entry.1292438233%3D%2520%2520We%2520leverage%2520finetuned%2520video%2520diffusion%2520models%252C%2520intrinsic%2520decomposition%2520of%250Avideos%252C%2520and%2520physically-based%2520differentiable%2520rendering%2520to%2520generate%2520high%2520quality%250Amaterials%2520for%25203D%2520models%2520given%2520a%2520text%2520prompt%2520or%2520a%2520single%2520image.%2520We%2520condition%2520a%250Avideo%2520diffusion%2520model%2520to%2520respect%2520the%2520input%2520geometry%2520and%2520lighting%2520condition.%250AThis%2520model%2520produces%2520multiple%2520views%2520of%2520a%2520given%25203D%2520model%2520with%2520coherent%2520material%250Aproperties.%2520Secondly%252C%2520we%2520use%2520a%2520recent%2520model%2520to%2520extract%2520intrinsics%2520%2528base%2520color%252C%250Aroughness%252C%2520metallic%2529%2520from%2520the%2520generated%2520video.%2520Finally%252C%2520we%2520use%2520the%2520intrinsics%250Aalongside%2520the%2520generated%2520video%2520in%2520a%2520differentiable%2520path%2520tracer%2520to%2520robustly%250Aextract%2520PBR%2520materials%2520directly%2520compatible%2520with%2520common%2520content%2520creation%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09665v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoMat%3A%20Extracting%20PBR%20Materials%20from%20Video%20Diffusion%20Models&entry.906535625=Jacob%20Munkberg%20and%20Zian%20Wang%20and%20Ruofan%20Liang%20and%20Tianchang%20Shen%20and%20Jon%20Hasselgren&entry.1292438233=%20%20We%20leverage%20finetuned%20video%20diffusion%20models%2C%20intrinsic%20decomposition%20of%0Avideos%2C%20and%20physically-based%20differentiable%20rendering%20to%20generate%20high%20quality%0Amaterials%20for%203D%20models%20given%20a%20text%20prompt%20or%20a%20single%20image.%20We%20condition%20a%0Avideo%20diffusion%20model%20to%20respect%20the%20input%20geometry%20and%20lighting%20condition.%0AThis%20model%20produces%20multiple%20views%20of%20a%20given%203D%20model%20with%20coherent%20material%0Aproperties.%20Secondly%2C%20we%20use%20a%20recent%20model%20to%20extract%20intrinsics%20%28base%20color%2C%0Aroughness%2C%20metallic%29%20from%20the%20generated%20video.%20Finally%2C%20we%20use%20the%20intrinsics%0Aalongside%20the%20generated%20video%20in%20a%20differentiable%20path%20tracer%20to%20robustly%0Aextract%20PBR%20materials%20directly%20compatible%20with%20common%20content%20creation%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09665v2&entry.124074799=Read"},
{"title": "Towards Efficient Occupancy Mapping via Gaussian Process Latent Field\n  Shaping", "author": "Cedric Le Gentil and Cedric Pradalier and Timothy D. Barfoot", "abstract": "  Occupancy mapping has been a key enabler of mobile robotics. Originally based\non a discrete grid representation, occupancy mapping has evolved towards\ncontinuous representations that can predict the occupancy status at any\nlocation and account for occupancy correlations between neighbouring areas.\nGaussian Process (GP) approaches treat this task as a binary classification\nproblem using both observations of occupied and free space. Conceptually, a GP\nlatent field is passed through a logistic function to obtain the output class\nwithout actually manipulating the GP latent field. In this work, we propose to\nact directly on the latent function to efficiently integrate free space\ninformation as a prior based on the shape of the sensor's field-of-view. A\nmajor difference with existing methods is the change in the classification\nproblem, as we distinguish between free and unknown space. The `occupied' area\nis the infinitesimally thin location where the class transitions from free to\nunknown. We demonstrate in simulated environments that our approach is sound\nand leads to competitive reconstruction accuracy.\n", "link": "http://arxiv.org/abs/2506.13640v1", "date": "2025-06-16", "relevancy": 2.9148, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5916}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5802}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Efficient%20Occupancy%20Mapping%20via%20Gaussian%20Process%20Latent%20Field%0A%20%20Shaping&body=Title%3A%20Towards%20Efficient%20Occupancy%20Mapping%20via%20Gaussian%20Process%20Latent%20Field%0A%20%20Shaping%0AAuthor%3A%20Cedric%20Le%20Gentil%20and%20Cedric%20Pradalier%20and%20Timothy%20D.%20Barfoot%0AAbstract%3A%20%20%20Occupancy%20mapping%20has%20been%20a%20key%20enabler%20of%20mobile%20robotics.%20Originally%20based%0Aon%20a%20discrete%20grid%20representation%2C%20occupancy%20mapping%20has%20evolved%20towards%0Acontinuous%20representations%20that%20can%20predict%20the%20occupancy%20status%20at%20any%0Alocation%20and%20account%20for%20occupancy%20correlations%20between%20neighbouring%20areas.%0AGaussian%20Process%20%28GP%29%20approaches%20treat%20this%20task%20as%20a%20binary%20classification%0Aproblem%20using%20both%20observations%20of%20occupied%20and%20free%20space.%20Conceptually%2C%20a%20GP%0Alatent%20field%20is%20passed%20through%20a%20logistic%20function%20to%20obtain%20the%20output%20class%0Awithout%20actually%20manipulating%20the%20GP%20latent%20field.%20In%20this%20work%2C%20we%20propose%20to%0Aact%20directly%20on%20the%20latent%20function%20to%20efficiently%20integrate%20free%20space%0Ainformation%20as%20a%20prior%20based%20on%20the%20shape%20of%20the%20sensor%27s%20field-of-view.%20A%0Amajor%20difference%20with%20existing%20methods%20is%20the%20change%20in%20the%20classification%0Aproblem%2C%20as%20we%20distinguish%20between%20free%20and%20unknown%20space.%20The%20%60occupied%27%20area%0Ais%20the%20infinitesimally%20thin%20location%20where%20the%20class%20transitions%20from%20free%20to%0Aunknown.%20We%20demonstrate%20in%20simulated%20environments%20that%20our%20approach%20is%20sound%0Aand%20leads%20to%20competitive%20reconstruction%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Efficient%2520Occupancy%2520Mapping%2520via%2520Gaussian%2520Process%2520Latent%2520Field%250A%2520%2520Shaping%26entry.906535625%3DCedric%2520Le%2520Gentil%2520and%2520Cedric%2520Pradalier%2520and%2520Timothy%2520D.%2520Barfoot%26entry.1292438233%3D%2520%2520Occupancy%2520mapping%2520has%2520been%2520a%2520key%2520enabler%2520of%2520mobile%2520robotics.%2520Originally%2520based%250Aon%2520a%2520discrete%2520grid%2520representation%252C%2520occupancy%2520mapping%2520has%2520evolved%2520towards%250Acontinuous%2520representations%2520that%2520can%2520predict%2520the%2520occupancy%2520status%2520at%2520any%250Alocation%2520and%2520account%2520for%2520occupancy%2520correlations%2520between%2520neighbouring%2520areas.%250AGaussian%2520Process%2520%2528GP%2529%2520approaches%2520treat%2520this%2520task%2520as%2520a%2520binary%2520classification%250Aproblem%2520using%2520both%2520observations%2520of%2520occupied%2520and%2520free%2520space.%2520Conceptually%252C%2520a%2520GP%250Alatent%2520field%2520is%2520passed%2520through%2520a%2520logistic%2520function%2520to%2520obtain%2520the%2520output%2520class%250Awithout%2520actually%2520manipulating%2520the%2520GP%2520latent%2520field.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%250Aact%2520directly%2520on%2520the%2520latent%2520function%2520to%2520efficiently%2520integrate%2520free%2520space%250Ainformation%2520as%2520a%2520prior%2520based%2520on%2520the%2520shape%2520of%2520the%2520sensor%2527s%2520field-of-view.%2520A%250Amajor%2520difference%2520with%2520existing%2520methods%2520is%2520the%2520change%2520in%2520the%2520classification%250Aproblem%252C%2520as%2520we%2520distinguish%2520between%2520free%2520and%2520unknown%2520space.%2520The%2520%2560occupied%2527%2520area%250Ais%2520the%2520infinitesimally%2520thin%2520location%2520where%2520the%2520class%2520transitions%2520from%2520free%2520to%250Aunknown.%2520We%2520demonstrate%2520in%2520simulated%2520environments%2520that%2520our%2520approach%2520is%2520sound%250Aand%2520leads%2520to%2520competitive%2520reconstruction%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Efficient%20Occupancy%20Mapping%20via%20Gaussian%20Process%20Latent%20Field%0A%20%20Shaping&entry.906535625=Cedric%20Le%20Gentil%20and%20Cedric%20Pradalier%20and%20Timothy%20D.%20Barfoot&entry.1292438233=%20%20Occupancy%20mapping%20has%20been%20a%20key%20enabler%20of%20mobile%20robotics.%20Originally%20based%0Aon%20a%20discrete%20grid%20representation%2C%20occupancy%20mapping%20has%20evolved%20towards%0Acontinuous%20representations%20that%20can%20predict%20the%20occupancy%20status%20at%20any%0Alocation%20and%20account%20for%20occupancy%20correlations%20between%20neighbouring%20areas.%0AGaussian%20Process%20%28GP%29%20approaches%20treat%20this%20task%20as%20a%20binary%20classification%0Aproblem%20using%20both%20observations%20of%20occupied%20and%20free%20space.%20Conceptually%2C%20a%20GP%0Alatent%20field%20is%20passed%20through%20a%20logistic%20function%20to%20obtain%20the%20output%20class%0Awithout%20actually%20manipulating%20the%20GP%20latent%20field.%20In%20this%20work%2C%20we%20propose%20to%0Aact%20directly%20on%20the%20latent%20function%20to%20efficiently%20integrate%20free%20space%0Ainformation%20as%20a%20prior%20based%20on%20the%20shape%20of%20the%20sensor%27s%20field-of-view.%20A%0Amajor%20difference%20with%20existing%20methods%20is%20the%20change%20in%20the%20classification%0Aproblem%2C%20as%20we%20distinguish%20between%20free%20and%20unknown%20space.%20The%20%60occupied%27%20area%0Ais%20the%20infinitesimally%20thin%20location%20where%20the%20class%20transitions%20from%20free%20to%0Aunknown.%20We%20demonstrate%20in%20simulated%20environments%20that%20our%20approach%20is%20sound%0Aand%20leads%20to%20competitive%20reconstruction%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13640v1&entry.124074799=Read"},
{"title": "OTFusion: Bridging Vision-only and Vision-Language Models via Optimal\n  Transport for Transductive Zero-Shot Learning", "author": "Qiyu Xu and Wenyang Chen and Zhanxuan Hu and Huafeng Li and Yonghang Tai", "abstract": "  Transductive zero-shot learning (ZSL) aims to classify unseen categories by\nleveraging both semantic class descriptions and the distribution of unlabeled\ntest data. While Vision-Language Models (VLMs) such as CLIP excel at aligning\nvisual inputs with textual semantics, they often rely too heavily on\nclass-level priors and fail to capture fine-grained visual cues. In contrast,\nVision-only Foundation Models (VFMs) like DINOv2 provide rich perceptual\nfeatures but lack semantic alignment. To exploit the complementary strengths of\nthese models, we propose OTFusion, a simple yet effective training-free\nframework that bridges VLMs and VFMs via Optimal Transport. Specifically,\nOTFusion aims to learn a shared probabilistic representation that aligns visual\nand semantic information by minimizing the transport cost between their\nrespective distributions. This unified distribution enables coherent class\npredictions that are both semantically meaningful and visually grounded.\nExtensive experiments on 11 benchmark datasets demonstrate that OTFusion\nconsistently outperforms the original CLIP model, achieving an average accuracy\nimprovement of nearly $10\\%$, all without any fine-tuning or additional\nannotations. The code will be publicly released after the paper is accepted.\n", "link": "http://arxiv.org/abs/2506.13723v1", "date": "2025-06-16", "relevancy": 2.9101, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.585}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OTFusion%3A%20Bridging%20Vision-only%20and%20Vision-Language%20Models%20via%20Optimal%0A%20%20Transport%20for%20Transductive%20Zero-Shot%20Learning&body=Title%3A%20OTFusion%3A%20Bridging%20Vision-only%20and%20Vision-Language%20Models%20via%20Optimal%0A%20%20Transport%20for%20Transductive%20Zero-Shot%20Learning%0AAuthor%3A%20Qiyu%20Xu%20and%20Wenyang%20Chen%20and%20Zhanxuan%20Hu%20and%20Huafeng%20Li%20and%20Yonghang%20Tai%0AAbstract%3A%20%20%20Transductive%20zero-shot%20learning%20%28ZSL%29%20aims%20to%20classify%20unseen%20categories%20by%0Aleveraging%20both%20semantic%20class%20descriptions%20and%20the%20distribution%20of%20unlabeled%0Atest%20data.%20While%20Vision-Language%20Models%20%28VLMs%29%20such%20as%20CLIP%20excel%20at%20aligning%0Avisual%20inputs%20with%20textual%20semantics%2C%20they%20often%20rely%20too%20heavily%20on%0Aclass-level%20priors%20and%20fail%20to%20capture%20fine-grained%20visual%20cues.%20In%20contrast%2C%0AVision-only%20Foundation%20Models%20%28VFMs%29%20like%20DINOv2%20provide%20rich%20perceptual%0Afeatures%20but%20lack%20semantic%20alignment.%20To%20exploit%20the%20complementary%20strengths%20of%0Athese%20models%2C%20we%20propose%20OTFusion%2C%20a%20simple%20yet%20effective%20training-free%0Aframework%20that%20bridges%20VLMs%20and%20VFMs%20via%20Optimal%20Transport.%20Specifically%2C%0AOTFusion%20aims%20to%20learn%20a%20shared%20probabilistic%20representation%20that%20aligns%20visual%0Aand%20semantic%20information%20by%20minimizing%20the%20transport%20cost%20between%20their%0Arespective%20distributions.%20This%20unified%20distribution%20enables%20coherent%20class%0Apredictions%20that%20are%20both%20semantically%20meaningful%20and%20visually%20grounded.%0AExtensive%20experiments%20on%2011%20benchmark%20datasets%20demonstrate%20that%20OTFusion%0Aconsistently%20outperforms%20the%20original%20CLIP%20model%2C%20achieving%20an%20average%20accuracy%0Aimprovement%20of%20nearly%20%2410%5C%25%24%2C%20all%20without%20any%20fine-tuning%20or%20additional%0Aannotations.%20The%20code%20will%20be%20publicly%20released%20after%20the%20paper%20is%20accepted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOTFusion%253A%2520Bridging%2520Vision-only%2520and%2520Vision-Language%2520Models%2520via%2520Optimal%250A%2520%2520Transport%2520for%2520Transductive%2520Zero-Shot%2520Learning%26entry.906535625%3DQiyu%2520Xu%2520and%2520Wenyang%2520Chen%2520and%2520Zhanxuan%2520Hu%2520and%2520Huafeng%2520Li%2520and%2520Yonghang%2520Tai%26entry.1292438233%3D%2520%2520Transductive%2520zero-shot%2520learning%2520%2528ZSL%2529%2520aims%2520to%2520classify%2520unseen%2520categories%2520by%250Aleveraging%2520both%2520semantic%2520class%2520descriptions%2520and%2520the%2520distribution%2520of%2520unlabeled%250Atest%2520data.%2520While%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520such%2520as%2520CLIP%2520excel%2520at%2520aligning%250Avisual%2520inputs%2520with%2520textual%2520semantics%252C%2520they%2520often%2520rely%2520too%2520heavily%2520on%250Aclass-level%2520priors%2520and%2520fail%2520to%2520capture%2520fine-grained%2520visual%2520cues.%2520In%2520contrast%252C%250AVision-only%2520Foundation%2520Models%2520%2528VFMs%2529%2520like%2520DINOv2%2520provide%2520rich%2520perceptual%250Afeatures%2520but%2520lack%2520semantic%2520alignment.%2520To%2520exploit%2520the%2520complementary%2520strengths%2520of%250Athese%2520models%252C%2520we%2520propose%2520OTFusion%252C%2520a%2520simple%2520yet%2520effective%2520training-free%250Aframework%2520that%2520bridges%2520VLMs%2520and%2520VFMs%2520via%2520Optimal%2520Transport.%2520Specifically%252C%250AOTFusion%2520aims%2520to%2520learn%2520a%2520shared%2520probabilistic%2520representation%2520that%2520aligns%2520visual%250Aand%2520semantic%2520information%2520by%2520minimizing%2520the%2520transport%2520cost%2520between%2520their%250Arespective%2520distributions.%2520This%2520unified%2520distribution%2520enables%2520coherent%2520class%250Apredictions%2520that%2520are%2520both%2520semantically%2520meaningful%2520and%2520visually%2520grounded.%250AExtensive%2520experiments%2520on%252011%2520benchmark%2520datasets%2520demonstrate%2520that%2520OTFusion%250Aconsistently%2520outperforms%2520the%2520original%2520CLIP%2520model%252C%2520achieving%2520an%2520average%2520accuracy%250Aimprovement%2520of%2520nearly%2520%252410%255C%2525%2524%252C%2520all%2520without%2520any%2520fine-tuning%2520or%2520additional%250Aannotations.%2520The%2520code%2520will%2520be%2520publicly%2520released%2520after%2520the%2520paper%2520is%2520accepted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OTFusion%3A%20Bridging%20Vision-only%20and%20Vision-Language%20Models%20via%20Optimal%0A%20%20Transport%20for%20Transductive%20Zero-Shot%20Learning&entry.906535625=Qiyu%20Xu%20and%20Wenyang%20Chen%20and%20Zhanxuan%20Hu%20and%20Huafeng%20Li%20and%20Yonghang%20Tai&entry.1292438233=%20%20Transductive%20zero-shot%20learning%20%28ZSL%29%20aims%20to%20classify%20unseen%20categories%20by%0Aleveraging%20both%20semantic%20class%20descriptions%20and%20the%20distribution%20of%20unlabeled%0Atest%20data.%20While%20Vision-Language%20Models%20%28VLMs%29%20such%20as%20CLIP%20excel%20at%20aligning%0Avisual%20inputs%20with%20textual%20semantics%2C%20they%20often%20rely%20too%20heavily%20on%0Aclass-level%20priors%20and%20fail%20to%20capture%20fine-grained%20visual%20cues.%20In%20contrast%2C%0AVision-only%20Foundation%20Models%20%28VFMs%29%20like%20DINOv2%20provide%20rich%20perceptual%0Afeatures%20but%20lack%20semantic%20alignment.%20To%20exploit%20the%20complementary%20strengths%20of%0Athese%20models%2C%20we%20propose%20OTFusion%2C%20a%20simple%20yet%20effective%20training-free%0Aframework%20that%20bridges%20VLMs%20and%20VFMs%20via%20Optimal%20Transport.%20Specifically%2C%0AOTFusion%20aims%20to%20learn%20a%20shared%20probabilistic%20representation%20that%20aligns%20visual%0Aand%20semantic%20information%20by%20minimizing%20the%20transport%20cost%20between%20their%0Arespective%20distributions.%20This%20unified%20distribution%20enables%20coherent%20class%0Apredictions%20that%20are%20both%20semantically%20meaningful%20and%20visually%20grounded.%0AExtensive%20experiments%20on%2011%20benchmark%20datasets%20demonstrate%20that%20OTFusion%0Aconsistently%20outperforms%20the%20original%20CLIP%20model%2C%20achieving%20an%20average%20accuracy%0Aimprovement%20of%20nearly%20%2410%5C%25%24%2C%20all%20without%20any%20fine-tuning%20or%20additional%0Aannotations.%20The%20code%20will%20be%20publicly%20released%20after%20the%20paper%20is%20accepted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13723v1&entry.124074799=Read"},
{"title": "From Flat to Feeling: A Feasibility and Impact Study on Dynamic Facial\n  Emotions in AI-Generated Avatars", "author": "Pegah Salehi and Sajad Amouei Sheshkal and Vajira Thambawita and P\u00e5l Halvorsen", "abstract": "  Dynamic facial emotion is essential for believable AI-generated avatars;\nhowever, most systems remain visually inert, limiting their utility in\nhigh-stakes simulations such as virtual training for investigative interviews\nwith abused children. We introduce and evaluate a real-time architecture fusing\nUnreal Engine 5 MetaHuman rendering with NVIDIA Omniverse Audio2Face to\ntranslate vocal prosody into high-fidelity facial expressions on photorealistic\nchild avatars. We implemented a distributed two-PC setup that decouples\nlanguage processing and speech synthesis from GPU-intensive rendering, designed\nto support low-latency interaction in desktop and VR environments. A\nbetween-subjects study ($N=70$) using audio+visual and visual-only conditions\nassessed perceptual impacts as participants rated emotional clarity, facial\nrealism, and empathy for two avatars expressing joy, sadness, and anger.\n  Results demonstrate that avatars could express emotions recognizably, with\nsadness and joy achieving high identification rates. However, anger recognition\nsignificantly dropped without audio, highlighting the importance of congruent\nvocal cues for high-arousal emotions. Interestingly, removing audio boosted\nperceived facial realism, suggesting that audiovisual desynchrony remains a key\ndesign challenge. These findings confirm the technical feasibility of\ngenerating emotionally expressive avatars and provide guidance for improving\nnon-verbal communication in sensitive training simulations.\n", "link": "http://arxiv.org/abs/2506.13477v1", "date": "2025-06-16", "relevancy": 2.9082, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6017}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6017}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Flat%20to%20Feeling%3A%20A%20Feasibility%20and%20Impact%20Study%20on%20Dynamic%20Facial%0A%20%20Emotions%20in%20AI-Generated%20Avatars&body=Title%3A%20From%20Flat%20to%20Feeling%3A%20A%20Feasibility%20and%20Impact%20Study%20on%20Dynamic%20Facial%0A%20%20Emotions%20in%20AI-Generated%20Avatars%0AAuthor%3A%20Pegah%20Salehi%20and%20Sajad%20Amouei%20Sheshkal%20and%20Vajira%20Thambawita%20and%20P%C3%A5l%20Halvorsen%0AAbstract%3A%20%20%20Dynamic%20facial%20emotion%20is%20essential%20for%20believable%20AI-generated%20avatars%3B%0Ahowever%2C%20most%20systems%20remain%20visually%20inert%2C%20limiting%20their%20utility%20in%0Ahigh-stakes%20simulations%20such%20as%20virtual%20training%20for%20investigative%20interviews%0Awith%20abused%20children.%20We%20introduce%20and%20evaluate%20a%20real-time%20architecture%20fusing%0AUnreal%20Engine%205%20MetaHuman%20rendering%20with%20NVIDIA%20Omniverse%20Audio2Face%20to%0Atranslate%20vocal%20prosody%20into%20high-fidelity%20facial%20expressions%20on%20photorealistic%0Achild%20avatars.%20We%20implemented%20a%20distributed%20two-PC%20setup%20that%20decouples%0Alanguage%20processing%20and%20speech%20synthesis%20from%20GPU-intensive%20rendering%2C%20designed%0Ato%20support%20low-latency%20interaction%20in%20desktop%20and%20VR%20environments.%20A%0Abetween-subjects%20study%20%28%24N%3D70%24%29%20using%20audio%2Bvisual%20and%20visual-only%20conditions%0Aassessed%20perceptual%20impacts%20as%20participants%20rated%20emotional%20clarity%2C%20facial%0Arealism%2C%20and%20empathy%20for%20two%20avatars%20expressing%20joy%2C%20sadness%2C%20and%20anger.%0A%20%20Results%20demonstrate%20that%20avatars%20could%20express%20emotions%20recognizably%2C%20with%0Asadness%20and%20joy%20achieving%20high%20identification%20rates.%20However%2C%20anger%20recognition%0Asignificantly%20dropped%20without%20audio%2C%20highlighting%20the%20importance%20of%20congruent%0Avocal%20cues%20for%20high-arousal%20emotions.%20Interestingly%2C%20removing%20audio%20boosted%0Aperceived%20facial%20realism%2C%20suggesting%20that%20audiovisual%20desynchrony%20remains%20a%20key%0Adesign%20challenge.%20These%20findings%20confirm%20the%20technical%20feasibility%20of%0Agenerating%20emotionally%20expressive%20avatars%20and%20provide%20guidance%20for%20improving%0Anon-verbal%20communication%20in%20sensitive%20training%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Flat%2520to%2520Feeling%253A%2520A%2520Feasibility%2520and%2520Impact%2520Study%2520on%2520Dynamic%2520Facial%250A%2520%2520Emotions%2520in%2520AI-Generated%2520Avatars%26entry.906535625%3DPegah%2520Salehi%2520and%2520Sajad%2520Amouei%2520Sheshkal%2520and%2520Vajira%2520Thambawita%2520and%2520P%25C3%25A5l%2520Halvorsen%26entry.1292438233%3D%2520%2520Dynamic%2520facial%2520emotion%2520is%2520essential%2520for%2520believable%2520AI-generated%2520avatars%253B%250Ahowever%252C%2520most%2520systems%2520remain%2520visually%2520inert%252C%2520limiting%2520their%2520utility%2520in%250Ahigh-stakes%2520simulations%2520such%2520as%2520virtual%2520training%2520for%2520investigative%2520interviews%250Awith%2520abused%2520children.%2520We%2520introduce%2520and%2520evaluate%2520a%2520real-time%2520architecture%2520fusing%250AUnreal%2520Engine%25205%2520MetaHuman%2520rendering%2520with%2520NVIDIA%2520Omniverse%2520Audio2Face%2520to%250Atranslate%2520vocal%2520prosody%2520into%2520high-fidelity%2520facial%2520expressions%2520on%2520photorealistic%250Achild%2520avatars.%2520We%2520implemented%2520a%2520distributed%2520two-PC%2520setup%2520that%2520decouples%250Alanguage%2520processing%2520and%2520speech%2520synthesis%2520from%2520GPU-intensive%2520rendering%252C%2520designed%250Ato%2520support%2520low-latency%2520interaction%2520in%2520desktop%2520and%2520VR%2520environments.%2520A%250Abetween-subjects%2520study%2520%2528%2524N%253D70%2524%2529%2520using%2520audio%252Bvisual%2520and%2520visual-only%2520conditions%250Aassessed%2520perceptual%2520impacts%2520as%2520participants%2520rated%2520emotional%2520clarity%252C%2520facial%250Arealism%252C%2520and%2520empathy%2520for%2520two%2520avatars%2520expressing%2520joy%252C%2520sadness%252C%2520and%2520anger.%250A%2520%2520Results%2520demonstrate%2520that%2520avatars%2520could%2520express%2520emotions%2520recognizably%252C%2520with%250Asadness%2520and%2520joy%2520achieving%2520high%2520identification%2520rates.%2520However%252C%2520anger%2520recognition%250Asignificantly%2520dropped%2520without%2520audio%252C%2520highlighting%2520the%2520importance%2520of%2520congruent%250Avocal%2520cues%2520for%2520high-arousal%2520emotions.%2520Interestingly%252C%2520removing%2520audio%2520boosted%250Aperceived%2520facial%2520realism%252C%2520suggesting%2520that%2520audiovisual%2520desynchrony%2520remains%2520a%2520key%250Adesign%2520challenge.%2520These%2520findings%2520confirm%2520the%2520technical%2520feasibility%2520of%250Agenerating%2520emotionally%2520expressive%2520avatars%2520and%2520provide%2520guidance%2520for%2520improving%250Anon-verbal%2520communication%2520in%2520sensitive%2520training%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Flat%20to%20Feeling%3A%20A%20Feasibility%20and%20Impact%20Study%20on%20Dynamic%20Facial%0A%20%20Emotions%20in%20AI-Generated%20Avatars&entry.906535625=Pegah%20Salehi%20and%20Sajad%20Amouei%20Sheshkal%20and%20Vajira%20Thambawita%20and%20P%C3%A5l%20Halvorsen&entry.1292438233=%20%20Dynamic%20facial%20emotion%20is%20essential%20for%20believable%20AI-generated%20avatars%3B%0Ahowever%2C%20most%20systems%20remain%20visually%20inert%2C%20limiting%20their%20utility%20in%0Ahigh-stakes%20simulations%20such%20as%20virtual%20training%20for%20investigative%20interviews%0Awith%20abused%20children.%20We%20introduce%20and%20evaluate%20a%20real-time%20architecture%20fusing%0AUnreal%20Engine%205%20MetaHuman%20rendering%20with%20NVIDIA%20Omniverse%20Audio2Face%20to%0Atranslate%20vocal%20prosody%20into%20high-fidelity%20facial%20expressions%20on%20photorealistic%0Achild%20avatars.%20We%20implemented%20a%20distributed%20two-PC%20setup%20that%20decouples%0Alanguage%20processing%20and%20speech%20synthesis%20from%20GPU-intensive%20rendering%2C%20designed%0Ato%20support%20low-latency%20interaction%20in%20desktop%20and%20VR%20environments.%20A%0Abetween-subjects%20study%20%28%24N%3D70%24%29%20using%20audio%2Bvisual%20and%20visual-only%20conditions%0Aassessed%20perceptual%20impacts%20as%20participants%20rated%20emotional%20clarity%2C%20facial%0Arealism%2C%20and%20empathy%20for%20two%20avatars%20expressing%20joy%2C%20sadness%2C%20and%20anger.%0A%20%20Results%20demonstrate%20that%20avatars%20could%20express%20emotions%20recognizably%2C%20with%0Asadness%20and%20joy%20achieving%20high%20identification%20rates.%20However%2C%20anger%20recognition%0Asignificantly%20dropped%20without%20audio%2C%20highlighting%20the%20importance%20of%20congruent%0Avocal%20cues%20for%20high-arousal%20emotions.%20Interestingly%2C%20removing%20audio%20boosted%0Aperceived%20facial%20realism%2C%20suggesting%20that%20audiovisual%20desynchrony%20remains%20a%20key%0Adesign%20challenge.%20These%20findings%20confirm%20the%20technical%20feasibility%20of%0Agenerating%20emotionally%20expressive%20avatars%20and%20provide%20guidance%20for%20improving%0Anon-verbal%20communication%20in%20sensitive%20training%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13477v1&entry.124074799=Read"},
{"title": "Test-time Contrastive Concepts for Open-world Semantic Segmentation with\n  Vision-Language Models", "author": "Monika Wysocza\u0144ska and Antonin Vobecky and Amaia Cardiel and Tomasz Trzci\u0144ski and Renaud Marlet and Andrei Bursuc and Oriane Sim\u00e9oni", "abstract": "  Recent CLIP-like Vision-Language Models (VLMs), pre-trained on large amounts\nof image-text pairs to align both modalities with a simple contrastive\nobjective, have paved the way to open-vocabulary semantic segmentation. Given\nan arbitrary set of textual queries, image pixels are assigned the closest\nquery in feature space. However, this works well when a user exhaustively lists\nall possible visual concepts in an image that contrast against each other for\nthe assignment. This corresponds to the current evaluation setup in the\nliterature, which relies on having access to a list of in-domain relevant\nconcepts, typically classes of a benchmark dataset. Here, we consider the more\nchallenging (and realistic) scenario of segmenting a single concept, given a\ntextual prompt and nothing else. To achieve good results, besides contrasting\nwith the generic 'background' text, we propose two different approaches to\nautomatically generate, at test time, query-specific textual contrastive\nconcepts. We do so by leveraging the distribution of text in the VLM's training\nset or crafted LLM prompts. We also propose a metric designed to evaluate this\nscenario and show the relevance of our approach on commonly used datasets.\n", "link": "http://arxiv.org/abs/2407.05061v3", "date": "2025-06-16", "relevancy": 2.8879, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5991}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5991}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-time%20Contrastive%20Concepts%20for%20Open-world%20Semantic%20Segmentation%20with%0A%20%20Vision-Language%20Models&body=Title%3A%20Test-time%20Contrastive%20Concepts%20for%20Open-world%20Semantic%20Segmentation%20with%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Monika%20Wysocza%C5%84ska%20and%20Antonin%20Vobecky%20and%20Amaia%20Cardiel%20and%20Tomasz%20Trzci%C5%84ski%20and%20Renaud%20Marlet%20and%20Andrei%20Bursuc%20and%20Oriane%20Sim%C3%A9oni%0AAbstract%3A%20%20%20Recent%20CLIP-like%20Vision-Language%20Models%20%28VLMs%29%2C%20pre-trained%20on%20large%20amounts%0Aof%20image-text%20pairs%20to%20align%20both%20modalities%20with%20a%20simple%20contrastive%0Aobjective%2C%20have%20paved%20the%20way%20to%20open-vocabulary%20semantic%20segmentation.%20Given%0Aan%20arbitrary%20set%20of%20textual%20queries%2C%20image%20pixels%20are%20assigned%20the%20closest%0Aquery%20in%20feature%20space.%20However%2C%20this%20works%20well%20when%20a%20user%20exhaustively%20lists%0Aall%20possible%20visual%20concepts%20in%20an%20image%20that%20contrast%20against%20each%20other%20for%0Athe%20assignment.%20This%20corresponds%20to%20the%20current%20evaluation%20setup%20in%20the%0Aliterature%2C%20which%20relies%20on%20having%20access%20to%20a%20list%20of%20in-domain%20relevant%0Aconcepts%2C%20typically%20classes%20of%20a%20benchmark%20dataset.%20Here%2C%20we%20consider%20the%20more%0Achallenging%20%28and%20realistic%29%20scenario%20of%20segmenting%20a%20single%20concept%2C%20given%20a%0Atextual%20prompt%20and%20nothing%20else.%20To%20achieve%20good%20results%2C%20besides%20contrasting%0Awith%20the%20generic%20%27background%27%20text%2C%20we%20propose%20two%20different%20approaches%20to%0Aautomatically%20generate%2C%20at%20test%20time%2C%20query-specific%20textual%20contrastive%0Aconcepts.%20We%20do%20so%20by%20leveraging%20the%20distribution%20of%20text%20in%20the%20VLM%27s%20training%0Aset%20or%20crafted%20LLM%20prompts.%20We%20also%20propose%20a%20metric%20designed%20to%20evaluate%20this%0Ascenario%20and%20show%20the%20relevance%20of%20our%20approach%20on%20commonly%20used%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05061v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-time%2520Contrastive%2520Concepts%2520for%2520Open-world%2520Semantic%2520Segmentation%2520with%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DMonika%2520Wysocza%25C5%2584ska%2520and%2520Antonin%2520Vobecky%2520and%2520Amaia%2520Cardiel%2520and%2520Tomasz%2520Trzci%25C5%2584ski%2520and%2520Renaud%2520Marlet%2520and%2520Andrei%2520Bursuc%2520and%2520Oriane%2520Sim%25C3%25A9oni%26entry.1292438233%3D%2520%2520Recent%2520CLIP-like%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520pre-trained%2520on%2520large%2520amounts%250Aof%2520image-text%2520pairs%2520to%2520align%2520both%2520modalities%2520with%2520a%2520simple%2520contrastive%250Aobjective%252C%2520have%2520paved%2520the%2520way%2520to%2520open-vocabulary%2520semantic%2520segmentation.%2520Given%250Aan%2520arbitrary%2520set%2520of%2520textual%2520queries%252C%2520image%2520pixels%2520are%2520assigned%2520the%2520closest%250Aquery%2520in%2520feature%2520space.%2520However%252C%2520this%2520works%2520well%2520when%2520a%2520user%2520exhaustively%2520lists%250Aall%2520possible%2520visual%2520concepts%2520in%2520an%2520image%2520that%2520contrast%2520against%2520each%2520other%2520for%250Athe%2520assignment.%2520This%2520corresponds%2520to%2520the%2520current%2520evaluation%2520setup%2520in%2520the%250Aliterature%252C%2520which%2520relies%2520on%2520having%2520access%2520to%2520a%2520list%2520of%2520in-domain%2520relevant%250Aconcepts%252C%2520typically%2520classes%2520of%2520a%2520benchmark%2520dataset.%2520Here%252C%2520we%2520consider%2520the%2520more%250Achallenging%2520%2528and%2520realistic%2529%2520scenario%2520of%2520segmenting%2520a%2520single%2520concept%252C%2520given%2520a%250Atextual%2520prompt%2520and%2520nothing%2520else.%2520To%2520achieve%2520good%2520results%252C%2520besides%2520contrasting%250Awith%2520the%2520generic%2520%2527background%2527%2520text%252C%2520we%2520propose%2520two%2520different%2520approaches%2520to%250Aautomatically%2520generate%252C%2520at%2520test%2520time%252C%2520query-specific%2520textual%2520contrastive%250Aconcepts.%2520We%2520do%2520so%2520by%2520leveraging%2520the%2520distribution%2520of%2520text%2520in%2520the%2520VLM%2527s%2520training%250Aset%2520or%2520crafted%2520LLM%2520prompts.%2520We%2520also%2520propose%2520a%2520metric%2520designed%2520to%2520evaluate%2520this%250Ascenario%2520and%2520show%2520the%2520relevance%2520of%2520our%2520approach%2520on%2520commonly%2520used%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05061v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-time%20Contrastive%20Concepts%20for%20Open-world%20Semantic%20Segmentation%20with%0A%20%20Vision-Language%20Models&entry.906535625=Monika%20Wysocza%C5%84ska%20and%20Antonin%20Vobecky%20and%20Amaia%20Cardiel%20and%20Tomasz%20Trzci%C5%84ski%20and%20Renaud%20Marlet%20and%20Andrei%20Bursuc%20and%20Oriane%20Sim%C3%A9oni&entry.1292438233=%20%20Recent%20CLIP-like%20Vision-Language%20Models%20%28VLMs%29%2C%20pre-trained%20on%20large%20amounts%0Aof%20image-text%20pairs%20to%20align%20both%20modalities%20with%20a%20simple%20contrastive%0Aobjective%2C%20have%20paved%20the%20way%20to%20open-vocabulary%20semantic%20segmentation.%20Given%0Aan%20arbitrary%20set%20of%20textual%20queries%2C%20image%20pixels%20are%20assigned%20the%20closest%0Aquery%20in%20feature%20space.%20However%2C%20this%20works%20well%20when%20a%20user%20exhaustively%20lists%0Aall%20possible%20visual%20concepts%20in%20an%20image%20that%20contrast%20against%20each%20other%20for%0Athe%20assignment.%20This%20corresponds%20to%20the%20current%20evaluation%20setup%20in%20the%0Aliterature%2C%20which%20relies%20on%20having%20access%20to%20a%20list%20of%20in-domain%20relevant%0Aconcepts%2C%20typically%20classes%20of%20a%20benchmark%20dataset.%20Here%2C%20we%20consider%20the%20more%0Achallenging%20%28and%20realistic%29%20scenario%20of%20segmenting%20a%20single%20concept%2C%20given%20a%0Atextual%20prompt%20and%20nothing%20else.%20To%20achieve%20good%20results%2C%20besides%20contrasting%0Awith%20the%20generic%20%27background%27%20text%2C%20we%20propose%20two%20different%20approaches%20to%0Aautomatically%20generate%2C%20at%20test%20time%2C%20query-specific%20textual%20contrastive%0Aconcepts.%20We%20do%20so%20by%20leveraging%20the%20distribution%20of%20text%20in%20the%20VLM%27s%20training%0Aset%20or%20crafted%20LLM%20prompts.%20We%20also%20propose%20a%20metric%20designed%20to%20evaluate%20this%0Ascenario%20and%20show%20the%20relevance%20of%20our%20approach%20on%20commonly%20used%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05061v3&entry.124074799=Read"},
{"title": "Comparative Evaluation of 3D Reconstruction Methods for Object Pose\n  Estimation", "author": "Varun Burde and Assia Benbihi and Pavel Burget and Torsten Sattler", "abstract": "  Object pose estimation is essential to many industrial applications involving\nrobotic manipulation, navigation, and augmented reality. Current generalizable\nobject pose estimators, i.e., approaches that do not need to be trained per\nobject, rely on accurate 3D models. Predominantly, CAD models are used, which\ncan be hard to obtain in practice. At the same time, it is often possible to\nacquire images of an object. Naturally, this leads to the question whether 3D\nmodels reconstructed from images are sufficient to facilitate accurate object\npose estimation. We aim to answer this question by proposing a novel benchmark\nfor measuring the impact of 3D reconstruction quality on pose estimation\naccuracy. Our benchmark provides calibrated images for object reconstruction\nregistered with the test images of the YCB-V dataset for pose evaluation under\nthe BOP benchmark format. Detailed experiments with multiple state-of-the-art\n3D reconstruction and object pose estimation approaches show that the geometry\nproduced by modern reconstruction methods is often sufficient for accurate pose\nestimation. Our experiments lead to interesting observations: (1) Standard\nmetrics for measuring 3D reconstruction quality are not necessarily indicative\nof pose estimation accuracy, which shows the need for dedicated benchmarks such\nas ours. (2) Classical, non-learning-based approaches can perform on par with\nmodern learning-based reconstruction techniques and can even offer a better\nreconstruction time-pose accuracy tradeoff. (3) There is still a sizable gap\nbetween performance with reconstructed and with CAD models. To foster research\non closing this gap, our benchmark is publicly available at\nhttps://github.com/VarunBurde/reconstruction_pose_benchmark}.\n", "link": "http://arxiv.org/abs/2408.08234v2", "date": "2025-06-16", "relevancy": 2.8551, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5721}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5721}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Evaluation%20of%203D%20Reconstruction%20Methods%20for%20Object%20Pose%0A%20%20Estimation&body=Title%3A%20Comparative%20Evaluation%20of%203D%20Reconstruction%20Methods%20for%20Object%20Pose%0A%20%20Estimation%0AAuthor%3A%20Varun%20Burde%20and%20Assia%20Benbihi%20and%20Pavel%20Burget%20and%20Torsten%20Sattler%0AAbstract%3A%20%20%20Object%20pose%20estimation%20is%20essential%20to%20many%20industrial%20applications%20involving%0Arobotic%20manipulation%2C%20navigation%2C%20and%20augmented%20reality.%20Current%20generalizable%0Aobject%20pose%20estimators%2C%20i.e.%2C%20approaches%20that%20do%20not%20need%20to%20be%20trained%20per%0Aobject%2C%20rely%20on%20accurate%203D%20models.%20Predominantly%2C%20CAD%20models%20are%20used%2C%20which%0Acan%20be%20hard%20to%20obtain%20in%20practice.%20At%20the%20same%20time%2C%20it%20is%20often%20possible%20to%0Aacquire%20images%20of%20an%20object.%20Naturally%2C%20this%20leads%20to%20the%20question%20whether%203D%0Amodels%20reconstructed%20from%20images%20are%20sufficient%20to%20facilitate%20accurate%20object%0Apose%20estimation.%20We%20aim%20to%20answer%20this%20question%20by%20proposing%20a%20novel%20benchmark%0Afor%20measuring%20the%20impact%20of%203D%20reconstruction%20quality%20on%20pose%20estimation%0Aaccuracy.%20Our%20benchmark%20provides%20calibrated%20images%20for%20object%20reconstruction%0Aregistered%20with%20the%20test%20images%20of%20the%20YCB-V%20dataset%20for%20pose%20evaluation%20under%0Athe%20BOP%20benchmark%20format.%20Detailed%20experiments%20with%20multiple%20state-of-the-art%0A3D%20reconstruction%20and%20object%20pose%20estimation%20approaches%20show%20that%20the%20geometry%0Aproduced%20by%20modern%20reconstruction%20methods%20is%20often%20sufficient%20for%20accurate%20pose%0Aestimation.%20Our%20experiments%20lead%20to%20interesting%20observations%3A%20%281%29%20Standard%0Ametrics%20for%20measuring%203D%20reconstruction%20quality%20are%20not%20necessarily%20indicative%0Aof%20pose%20estimation%20accuracy%2C%20which%20shows%20the%20need%20for%20dedicated%20benchmarks%20such%0Aas%20ours.%20%282%29%20Classical%2C%20non-learning-based%20approaches%20can%20perform%20on%20par%20with%0Amodern%20learning-based%20reconstruction%20techniques%20and%20can%20even%20offer%20a%20better%0Areconstruction%20time-pose%20accuracy%20tradeoff.%20%283%29%20There%20is%20still%20a%20sizable%20gap%0Abetween%20performance%20with%20reconstructed%20and%20with%20CAD%20models.%20To%20foster%20research%0Aon%20closing%20this%20gap%2C%20our%20benchmark%20is%20publicly%20available%20at%0Ahttps%3A//github.com/VarunBurde/reconstruction_pose_benchmark%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08234v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Evaluation%2520of%25203D%2520Reconstruction%2520Methods%2520for%2520Object%2520Pose%250A%2520%2520Estimation%26entry.906535625%3DVarun%2520Burde%2520and%2520Assia%2520Benbihi%2520and%2520Pavel%2520Burget%2520and%2520Torsten%2520Sattler%26entry.1292438233%3D%2520%2520Object%2520pose%2520estimation%2520is%2520essential%2520to%2520many%2520industrial%2520applications%2520involving%250Arobotic%2520manipulation%252C%2520navigation%252C%2520and%2520augmented%2520reality.%2520Current%2520generalizable%250Aobject%2520pose%2520estimators%252C%2520i.e.%252C%2520approaches%2520that%2520do%2520not%2520need%2520to%2520be%2520trained%2520per%250Aobject%252C%2520rely%2520on%2520accurate%25203D%2520models.%2520Predominantly%252C%2520CAD%2520models%2520are%2520used%252C%2520which%250Acan%2520be%2520hard%2520to%2520obtain%2520in%2520practice.%2520At%2520the%2520same%2520time%252C%2520it%2520is%2520often%2520possible%2520to%250Aacquire%2520images%2520of%2520an%2520object.%2520Naturally%252C%2520this%2520leads%2520to%2520the%2520question%2520whether%25203D%250Amodels%2520reconstructed%2520from%2520images%2520are%2520sufficient%2520to%2520facilitate%2520accurate%2520object%250Apose%2520estimation.%2520We%2520aim%2520to%2520answer%2520this%2520question%2520by%2520proposing%2520a%2520novel%2520benchmark%250Afor%2520measuring%2520the%2520impact%2520of%25203D%2520reconstruction%2520quality%2520on%2520pose%2520estimation%250Aaccuracy.%2520Our%2520benchmark%2520provides%2520calibrated%2520images%2520for%2520object%2520reconstruction%250Aregistered%2520with%2520the%2520test%2520images%2520of%2520the%2520YCB-V%2520dataset%2520for%2520pose%2520evaluation%2520under%250Athe%2520BOP%2520benchmark%2520format.%2520Detailed%2520experiments%2520with%2520multiple%2520state-of-the-art%250A3D%2520reconstruction%2520and%2520object%2520pose%2520estimation%2520approaches%2520show%2520that%2520the%2520geometry%250Aproduced%2520by%2520modern%2520reconstruction%2520methods%2520is%2520often%2520sufficient%2520for%2520accurate%2520pose%250Aestimation.%2520Our%2520experiments%2520lead%2520to%2520interesting%2520observations%253A%2520%25281%2529%2520Standard%250Ametrics%2520for%2520measuring%25203D%2520reconstruction%2520quality%2520are%2520not%2520necessarily%2520indicative%250Aof%2520pose%2520estimation%2520accuracy%252C%2520which%2520shows%2520the%2520need%2520for%2520dedicated%2520benchmarks%2520such%250Aas%2520ours.%2520%25282%2529%2520Classical%252C%2520non-learning-based%2520approaches%2520can%2520perform%2520on%2520par%2520with%250Amodern%2520learning-based%2520reconstruction%2520techniques%2520and%2520can%2520even%2520offer%2520a%2520better%250Areconstruction%2520time-pose%2520accuracy%2520tradeoff.%2520%25283%2529%2520There%2520is%2520still%2520a%2520sizable%2520gap%250Abetween%2520performance%2520with%2520reconstructed%2520and%2520with%2520CAD%2520models.%2520To%2520foster%2520research%250Aon%2520closing%2520this%2520gap%252C%2520our%2520benchmark%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/VarunBurde/reconstruction_pose_benchmark%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08234v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Evaluation%20of%203D%20Reconstruction%20Methods%20for%20Object%20Pose%0A%20%20Estimation&entry.906535625=Varun%20Burde%20and%20Assia%20Benbihi%20and%20Pavel%20Burget%20and%20Torsten%20Sattler&entry.1292438233=%20%20Object%20pose%20estimation%20is%20essential%20to%20many%20industrial%20applications%20involving%0Arobotic%20manipulation%2C%20navigation%2C%20and%20augmented%20reality.%20Current%20generalizable%0Aobject%20pose%20estimators%2C%20i.e.%2C%20approaches%20that%20do%20not%20need%20to%20be%20trained%20per%0Aobject%2C%20rely%20on%20accurate%203D%20models.%20Predominantly%2C%20CAD%20models%20are%20used%2C%20which%0Acan%20be%20hard%20to%20obtain%20in%20practice.%20At%20the%20same%20time%2C%20it%20is%20often%20possible%20to%0Aacquire%20images%20of%20an%20object.%20Naturally%2C%20this%20leads%20to%20the%20question%20whether%203D%0Amodels%20reconstructed%20from%20images%20are%20sufficient%20to%20facilitate%20accurate%20object%0Apose%20estimation.%20We%20aim%20to%20answer%20this%20question%20by%20proposing%20a%20novel%20benchmark%0Afor%20measuring%20the%20impact%20of%203D%20reconstruction%20quality%20on%20pose%20estimation%0Aaccuracy.%20Our%20benchmark%20provides%20calibrated%20images%20for%20object%20reconstruction%0Aregistered%20with%20the%20test%20images%20of%20the%20YCB-V%20dataset%20for%20pose%20evaluation%20under%0Athe%20BOP%20benchmark%20format.%20Detailed%20experiments%20with%20multiple%20state-of-the-art%0A3D%20reconstruction%20and%20object%20pose%20estimation%20approaches%20show%20that%20the%20geometry%0Aproduced%20by%20modern%20reconstruction%20methods%20is%20often%20sufficient%20for%20accurate%20pose%0Aestimation.%20Our%20experiments%20lead%20to%20interesting%20observations%3A%20%281%29%20Standard%0Ametrics%20for%20measuring%203D%20reconstruction%20quality%20are%20not%20necessarily%20indicative%0Aof%20pose%20estimation%20accuracy%2C%20which%20shows%20the%20need%20for%20dedicated%20benchmarks%20such%0Aas%20ours.%20%282%29%20Classical%2C%20non-learning-based%20approaches%20can%20perform%20on%20par%20with%0Amodern%20learning-based%20reconstruction%20techniques%20and%20can%20even%20offer%20a%20better%0Areconstruction%20time-pose%20accuracy%20tradeoff.%20%283%29%20There%20is%20still%20a%20sizable%20gap%0Abetween%20performance%20with%20reconstructed%20and%20with%20CAD%20models.%20To%20foster%20research%0Aon%20closing%20this%20gap%2C%20our%20benchmark%20is%20publicly%20available%20at%0Ahttps%3A//github.com/VarunBurde/reconstruction_pose_benchmark%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08234v2&entry.124074799=Read"},
{"title": "Contrastive Self-Supervised Learning As Neural Manifold Packing", "author": "Guanming Zhang and David J. Heeger and Stefano Martiniani", "abstract": "  Contrastive self-supervised learning based on point-wise comparisons has been\nwidely studied for vision tasks. In the visual cortex of the brain, neuronal\nresponses to distinct stimulus classes are organized into geometric structures\nknown as neural manifolds. Accurate classification of stimuli can be achieved\nby effectively separating these manifolds, akin to solving a packing problem.\nWe introduce Contrastive Learning As Manifold Packing (CLAMP), a\nself-supervised framework that recasts representation learning as a manifold\npacking problem. CLAMP introduces a loss function inspired by the potential\nenergy of short-range repulsive particle systems, such as those encountered in\nthe physics of simple liquids and jammed packings. In this framework, each\nclass consists of sub-manifolds embedding multiple augmented views of a single\nimage. The sizes and positions of the sub-manifolds are dynamically optimized\nby following the gradient of a packing loss. This approach yields interpretable\ndynamics in the embedding space that parallel jamming physics, and introduces\ngeometrically meaningful hyperparameters within the loss function. Under the\nstandard linear evaluation protocol, which freezes the backbone and trains only\na linear classifier, CLAMP achieves competitive performance with\nstate-of-the-art self-supervised models. Furthermore, our analysis reveals that\nneural manifolds corresponding to different categories emerge naturally and are\neffectively separated in the learned representation space, highlighting the\npotential of CLAMP to bridge insights from physics, neural science, and machine\nlearning.\n", "link": "http://arxiv.org/abs/2506.13717v1", "date": "2025-06-16", "relevancy": 2.846, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6185}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5466}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Self-Supervised%20Learning%20As%20Neural%20Manifold%20Packing&body=Title%3A%20Contrastive%20Self-Supervised%20Learning%20As%20Neural%20Manifold%20Packing%0AAuthor%3A%20Guanming%20Zhang%20and%20David%20J.%20Heeger%20and%20Stefano%20Martiniani%0AAbstract%3A%20%20%20Contrastive%20self-supervised%20learning%20based%20on%20point-wise%20comparisons%20has%20been%0Awidely%20studied%20for%20vision%20tasks.%20In%20the%20visual%20cortex%20of%20the%20brain%2C%20neuronal%0Aresponses%20to%20distinct%20stimulus%20classes%20are%20organized%20into%20geometric%20structures%0Aknown%20as%20neural%20manifolds.%20Accurate%20classification%20of%20stimuli%20can%20be%20achieved%0Aby%20effectively%20separating%20these%20manifolds%2C%20akin%20to%20solving%20a%20packing%20problem.%0AWe%20introduce%20Contrastive%20Learning%20As%20Manifold%20Packing%20%28CLAMP%29%2C%20a%0Aself-supervised%20framework%20that%20recasts%20representation%20learning%20as%20a%20manifold%0Apacking%20problem.%20CLAMP%20introduces%20a%20loss%20function%20inspired%20by%20the%20potential%0Aenergy%20of%20short-range%20repulsive%20particle%20systems%2C%20such%20as%20those%20encountered%20in%0Athe%20physics%20of%20simple%20liquids%20and%20jammed%20packings.%20In%20this%20framework%2C%20each%0Aclass%20consists%20of%20sub-manifolds%20embedding%20multiple%20augmented%20views%20of%20a%20single%0Aimage.%20The%20sizes%20and%20positions%20of%20the%20sub-manifolds%20are%20dynamically%20optimized%0Aby%20following%20the%20gradient%20of%20a%20packing%20loss.%20This%20approach%20yields%20interpretable%0Adynamics%20in%20the%20embedding%20space%20that%20parallel%20jamming%20physics%2C%20and%20introduces%0Ageometrically%20meaningful%20hyperparameters%20within%20the%20loss%20function.%20Under%20the%0Astandard%20linear%20evaluation%20protocol%2C%20which%20freezes%20the%20backbone%20and%20trains%20only%0Aa%20linear%20classifier%2C%20CLAMP%20achieves%20competitive%20performance%20with%0Astate-of-the-art%20self-supervised%20models.%20Furthermore%2C%20our%20analysis%20reveals%20that%0Aneural%20manifolds%20corresponding%20to%20different%20categories%20emerge%20naturally%20and%20are%0Aeffectively%20separated%20in%20the%20learned%20representation%20space%2C%20highlighting%20the%0Apotential%20of%20CLAMP%20to%20bridge%20insights%20from%20physics%2C%20neural%20science%2C%20and%20machine%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Self-Supervised%2520Learning%2520As%2520Neural%2520Manifold%2520Packing%26entry.906535625%3DGuanming%2520Zhang%2520and%2520David%2520J.%2520Heeger%2520and%2520Stefano%2520Martiniani%26entry.1292438233%3D%2520%2520Contrastive%2520self-supervised%2520learning%2520based%2520on%2520point-wise%2520comparisons%2520has%2520been%250Awidely%2520studied%2520for%2520vision%2520tasks.%2520In%2520the%2520visual%2520cortex%2520of%2520the%2520brain%252C%2520neuronal%250Aresponses%2520to%2520distinct%2520stimulus%2520classes%2520are%2520organized%2520into%2520geometric%2520structures%250Aknown%2520as%2520neural%2520manifolds.%2520Accurate%2520classification%2520of%2520stimuli%2520can%2520be%2520achieved%250Aby%2520effectively%2520separating%2520these%2520manifolds%252C%2520akin%2520to%2520solving%2520a%2520packing%2520problem.%250AWe%2520introduce%2520Contrastive%2520Learning%2520As%2520Manifold%2520Packing%2520%2528CLAMP%2529%252C%2520a%250Aself-supervised%2520framework%2520that%2520recasts%2520representation%2520learning%2520as%2520a%2520manifold%250Apacking%2520problem.%2520CLAMP%2520introduces%2520a%2520loss%2520function%2520inspired%2520by%2520the%2520potential%250Aenergy%2520of%2520short-range%2520repulsive%2520particle%2520systems%252C%2520such%2520as%2520those%2520encountered%2520in%250Athe%2520physics%2520of%2520simple%2520liquids%2520and%2520jammed%2520packings.%2520In%2520this%2520framework%252C%2520each%250Aclass%2520consists%2520of%2520sub-manifolds%2520embedding%2520multiple%2520augmented%2520views%2520of%2520a%2520single%250Aimage.%2520The%2520sizes%2520and%2520positions%2520of%2520the%2520sub-manifolds%2520are%2520dynamically%2520optimized%250Aby%2520following%2520the%2520gradient%2520of%2520a%2520packing%2520loss.%2520This%2520approach%2520yields%2520interpretable%250Adynamics%2520in%2520the%2520embedding%2520space%2520that%2520parallel%2520jamming%2520physics%252C%2520and%2520introduces%250Ageometrically%2520meaningful%2520hyperparameters%2520within%2520the%2520loss%2520function.%2520Under%2520the%250Astandard%2520linear%2520evaluation%2520protocol%252C%2520which%2520freezes%2520the%2520backbone%2520and%2520trains%2520only%250Aa%2520linear%2520classifier%252C%2520CLAMP%2520achieves%2520competitive%2520performance%2520with%250Astate-of-the-art%2520self-supervised%2520models.%2520Furthermore%252C%2520our%2520analysis%2520reveals%2520that%250Aneural%2520manifolds%2520corresponding%2520to%2520different%2520categories%2520emerge%2520naturally%2520and%2520are%250Aeffectively%2520separated%2520in%2520the%2520learned%2520representation%2520space%252C%2520highlighting%2520the%250Apotential%2520of%2520CLAMP%2520to%2520bridge%2520insights%2520from%2520physics%252C%2520neural%2520science%252C%2520and%2520machine%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Self-Supervised%20Learning%20As%20Neural%20Manifold%20Packing&entry.906535625=Guanming%20Zhang%20and%20David%20J.%20Heeger%20and%20Stefano%20Martiniani&entry.1292438233=%20%20Contrastive%20self-supervised%20learning%20based%20on%20point-wise%20comparisons%20has%20been%0Awidely%20studied%20for%20vision%20tasks.%20In%20the%20visual%20cortex%20of%20the%20brain%2C%20neuronal%0Aresponses%20to%20distinct%20stimulus%20classes%20are%20organized%20into%20geometric%20structures%0Aknown%20as%20neural%20manifolds.%20Accurate%20classification%20of%20stimuli%20can%20be%20achieved%0Aby%20effectively%20separating%20these%20manifolds%2C%20akin%20to%20solving%20a%20packing%20problem.%0AWe%20introduce%20Contrastive%20Learning%20As%20Manifold%20Packing%20%28CLAMP%29%2C%20a%0Aself-supervised%20framework%20that%20recasts%20representation%20learning%20as%20a%20manifold%0Apacking%20problem.%20CLAMP%20introduces%20a%20loss%20function%20inspired%20by%20the%20potential%0Aenergy%20of%20short-range%20repulsive%20particle%20systems%2C%20such%20as%20those%20encountered%20in%0Athe%20physics%20of%20simple%20liquids%20and%20jammed%20packings.%20In%20this%20framework%2C%20each%0Aclass%20consists%20of%20sub-manifolds%20embedding%20multiple%20augmented%20views%20of%20a%20single%0Aimage.%20The%20sizes%20and%20positions%20of%20the%20sub-manifolds%20are%20dynamically%20optimized%0Aby%20following%20the%20gradient%20of%20a%20packing%20loss.%20This%20approach%20yields%20interpretable%0Adynamics%20in%20the%20embedding%20space%20that%20parallel%20jamming%20physics%2C%20and%20introduces%0Ageometrically%20meaningful%20hyperparameters%20within%20the%20loss%20function.%20Under%20the%0Astandard%20linear%20evaluation%20protocol%2C%20which%20freezes%20the%20backbone%20and%20trains%20only%0Aa%20linear%20classifier%2C%20CLAMP%20achieves%20competitive%20performance%20with%0Astate-of-the-art%20self-supervised%20models.%20Furthermore%2C%20our%20analysis%20reveals%20that%0Aneural%20manifolds%20corresponding%20to%20different%20categories%20emerge%20naturally%20and%20are%0Aeffectively%20separated%20in%20the%20learned%20representation%20space%2C%20highlighting%20the%0Apotential%20of%20CLAMP%20to%20bridge%20insights%20from%20physics%2C%20neural%20science%2C%20and%20machine%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13717v1&entry.124074799=Read"},
{"title": "Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model", "author": "Shaolei Zhang and Shoutao Guo and Qingkai Fang and Yan Zhou and Yang Feng", "abstract": "  The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\nexploration of integrating text, vision, and speech modalities to support more\nflexible multimodal interaction. Existing LMMs typically concatenate\nrepresentation of modalities along the sequence dimension and feed them into a\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\nstraightforward for modality integration, it often relies heavily on\nlarge-scale data to learn modality alignments. In this paper, we aim to model\nthe relationships between modalities more purposefully, thereby achieving more\nefficient and flexible modality alignments. To this end, we propose\nStream-Omni, a large language-vision-speech model with efficient modality\nalignments, which can simultaneously support interactions under various\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\nvision and speech to the text based on their relationships. For vision that is\nsemantically complementary to text, Stream-Omni uses sequence-dimension\nconcatenation to achieve vision-text alignment. For speech that is semantically\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\nmodality alignments with less data (especially speech), enabling the transfer\nof text capabilities to other modalities. Experiments on various benchmarks\ndemonstrate that Stream-Omni achieves strong performance on visual\nunderstanding, speech interaction, and vision-grounded speech interaction\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\nprovide intermediate text outputs (such as ASR transcriptions and model\nresponses) during speech interaction, offering users a comprehensive multimodal\nexperience.\n", "link": "http://arxiv.org/abs/2506.13642v1", "date": "2025-06-16", "relevancy": 2.8126, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5757}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stream-Omni%3A%20Simultaneous%20Multimodal%20Interactions%20with%20Large%0A%20%20Language-Vision-Speech%20Model&body=Title%3A%20Stream-Omni%3A%20Simultaneous%20Multimodal%20Interactions%20with%20Large%0A%20%20Language-Vision-Speech%20Model%0AAuthor%3A%20Shaolei%20Zhang%20and%20Shoutao%20Guo%20and%20Qingkai%20Fang%20and%20Yan%20Zhou%20and%20Yang%20Feng%0AAbstract%3A%20%20%20The%20emergence%20of%20GPT-4o-like%20large%20multimodal%20models%20%28LMMs%29%20has%20raised%20the%0Aexploration%20of%20integrating%20text%2C%20vision%2C%20and%20speech%20modalities%20to%20support%20more%0Aflexible%20multimodal%20interaction.%20Existing%20LMMs%20typically%20concatenate%0Arepresentation%20of%20modalities%20along%20the%20sequence%20dimension%20and%20feed%20them%20into%20a%0Alarge%20language%20model%20%28LLM%29%20backbone.%20While%20sequence-dimension%20concatenation%20is%0Astraightforward%20for%20modality%20integration%2C%20it%20often%20relies%20heavily%20on%0Alarge-scale%20data%20to%20learn%20modality%20alignments.%20In%20this%20paper%2C%20we%20aim%20to%20model%0Athe%20relationships%20between%20modalities%20more%20purposefully%2C%20thereby%20achieving%20more%0Aefficient%20and%20flexible%20modality%20alignments.%20To%20this%20end%2C%20we%20propose%0AStream-Omni%2C%20a%20large%20language-vision-speech%20model%20with%20efficient%20modality%0Aalignments%2C%20which%20can%20simultaneously%20support%20interactions%20under%20various%0Amodality%20combinations.%20Stream-Omni%20employs%20LLM%20as%20the%20backbone%20and%20aligns%20the%0Avision%20and%20speech%20to%20the%20text%20based%20on%20their%20relationships.%20For%20vision%20that%20is%0Asemantically%20complementary%20to%20text%2C%20Stream-Omni%20uses%20sequence-dimension%0Aconcatenation%20to%20achieve%20vision-text%20alignment.%20For%20speech%20that%20is%20semantically%0Aconsistent%20with%20text%2C%20Stream-Omni%20introduces%20a%20CTC-based%20layer-dimension%0Amapping%20to%20achieve%20speech-text%20alignment.%20In%20this%20way%2C%20Stream-Omni%20can%20achieve%0Amodality%20alignments%20with%20less%20data%20%28especially%20speech%29%2C%20enabling%20the%20transfer%0Aof%20text%20capabilities%20to%20other%20modalities.%20Experiments%20on%20various%20benchmarks%0Ademonstrate%20that%20Stream-Omni%20achieves%20strong%20performance%20on%20visual%0Aunderstanding%2C%20speech%20interaction%2C%20and%20vision-grounded%20speech%20interaction%0Atasks.%20Owing%20to%20the%20layer-dimensional%20mapping%2C%20Stream-Omni%20can%20simultaneously%0Aprovide%20intermediate%20text%20outputs%20%28such%20as%20ASR%20transcriptions%20and%20model%0Aresponses%29%20during%20speech%20interaction%2C%20offering%20users%20a%20comprehensive%20multimodal%0Aexperience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStream-Omni%253A%2520Simultaneous%2520Multimodal%2520Interactions%2520with%2520Large%250A%2520%2520Language-Vision-Speech%2520Model%26entry.906535625%3DShaolei%2520Zhang%2520and%2520Shoutao%2520Guo%2520and%2520Qingkai%2520Fang%2520and%2520Yan%2520Zhou%2520and%2520Yang%2520Feng%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520GPT-4o-like%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520has%2520raised%2520the%250Aexploration%2520of%2520integrating%2520text%252C%2520vision%252C%2520and%2520speech%2520modalities%2520to%2520support%2520more%250Aflexible%2520multimodal%2520interaction.%2520Existing%2520LMMs%2520typically%2520concatenate%250Arepresentation%2520of%2520modalities%2520along%2520the%2520sequence%2520dimension%2520and%2520feed%2520them%2520into%2520a%250Alarge%2520language%2520model%2520%2528LLM%2529%2520backbone.%2520While%2520sequence-dimension%2520concatenation%2520is%250Astraightforward%2520for%2520modality%2520integration%252C%2520it%2520often%2520relies%2520heavily%2520on%250Alarge-scale%2520data%2520to%2520learn%2520modality%2520alignments.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520model%250Athe%2520relationships%2520between%2520modalities%2520more%2520purposefully%252C%2520thereby%2520achieving%2520more%250Aefficient%2520and%2520flexible%2520modality%2520alignments.%2520To%2520this%2520end%252C%2520we%2520propose%250AStream-Omni%252C%2520a%2520large%2520language-vision-speech%2520model%2520with%2520efficient%2520modality%250Aalignments%252C%2520which%2520can%2520simultaneously%2520support%2520interactions%2520under%2520various%250Amodality%2520combinations.%2520Stream-Omni%2520employs%2520LLM%2520as%2520the%2520backbone%2520and%2520aligns%2520the%250Avision%2520and%2520speech%2520to%2520the%2520text%2520based%2520on%2520their%2520relationships.%2520For%2520vision%2520that%2520is%250Asemantically%2520complementary%2520to%2520text%252C%2520Stream-Omni%2520uses%2520sequence-dimension%250Aconcatenation%2520to%2520achieve%2520vision-text%2520alignment.%2520For%2520speech%2520that%2520is%2520semantically%250Aconsistent%2520with%2520text%252C%2520Stream-Omni%2520introduces%2520a%2520CTC-based%2520layer-dimension%250Amapping%2520to%2520achieve%2520speech-text%2520alignment.%2520In%2520this%2520way%252C%2520Stream-Omni%2520can%2520achieve%250Amodality%2520alignments%2520with%2520less%2520data%2520%2528especially%2520speech%2529%252C%2520enabling%2520the%2520transfer%250Aof%2520text%2520capabilities%2520to%2520other%2520modalities.%2520Experiments%2520on%2520various%2520benchmarks%250Ademonstrate%2520that%2520Stream-Omni%2520achieves%2520strong%2520performance%2520on%2520visual%250Aunderstanding%252C%2520speech%2520interaction%252C%2520and%2520vision-grounded%2520speech%2520interaction%250Atasks.%2520Owing%2520to%2520the%2520layer-dimensional%2520mapping%252C%2520Stream-Omni%2520can%2520simultaneously%250Aprovide%2520intermediate%2520text%2520outputs%2520%2528such%2520as%2520ASR%2520transcriptions%2520and%2520model%250Aresponses%2529%2520during%2520speech%2520interaction%252C%2520offering%2520users%2520a%2520comprehensive%2520multimodal%250Aexperience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stream-Omni%3A%20Simultaneous%20Multimodal%20Interactions%20with%20Large%0A%20%20Language-Vision-Speech%20Model&entry.906535625=Shaolei%20Zhang%20and%20Shoutao%20Guo%20and%20Qingkai%20Fang%20and%20Yan%20Zhou%20and%20Yang%20Feng&entry.1292438233=%20%20The%20emergence%20of%20GPT-4o-like%20large%20multimodal%20models%20%28LMMs%29%20has%20raised%20the%0Aexploration%20of%20integrating%20text%2C%20vision%2C%20and%20speech%20modalities%20to%20support%20more%0Aflexible%20multimodal%20interaction.%20Existing%20LMMs%20typically%20concatenate%0Arepresentation%20of%20modalities%20along%20the%20sequence%20dimension%20and%20feed%20them%20into%20a%0Alarge%20language%20model%20%28LLM%29%20backbone.%20While%20sequence-dimension%20concatenation%20is%0Astraightforward%20for%20modality%20integration%2C%20it%20often%20relies%20heavily%20on%0Alarge-scale%20data%20to%20learn%20modality%20alignments.%20In%20this%20paper%2C%20we%20aim%20to%20model%0Athe%20relationships%20between%20modalities%20more%20purposefully%2C%20thereby%20achieving%20more%0Aefficient%20and%20flexible%20modality%20alignments.%20To%20this%20end%2C%20we%20propose%0AStream-Omni%2C%20a%20large%20language-vision-speech%20model%20with%20efficient%20modality%0Aalignments%2C%20which%20can%20simultaneously%20support%20interactions%20under%20various%0Amodality%20combinations.%20Stream-Omni%20employs%20LLM%20as%20the%20backbone%20and%20aligns%20the%0Avision%20and%20speech%20to%20the%20text%20based%20on%20their%20relationships.%20For%20vision%20that%20is%0Asemantically%20complementary%20to%20text%2C%20Stream-Omni%20uses%20sequence-dimension%0Aconcatenation%20to%20achieve%20vision-text%20alignment.%20For%20speech%20that%20is%20semantically%0Aconsistent%20with%20text%2C%20Stream-Omni%20introduces%20a%20CTC-based%20layer-dimension%0Amapping%20to%20achieve%20speech-text%20alignment.%20In%20this%20way%2C%20Stream-Omni%20can%20achieve%0Amodality%20alignments%20with%20less%20data%20%28especially%20speech%29%2C%20enabling%20the%20transfer%0Aof%20text%20capabilities%20to%20other%20modalities.%20Experiments%20on%20various%20benchmarks%0Ademonstrate%20that%20Stream-Omni%20achieves%20strong%20performance%20on%20visual%0Aunderstanding%2C%20speech%20interaction%2C%20and%20vision-grounded%20speech%20interaction%0Atasks.%20Owing%20to%20the%20layer-dimensional%20mapping%2C%20Stream-Omni%20can%20simultaneously%0Aprovide%20intermediate%20text%20outputs%20%28such%20as%20ASR%20transcriptions%20and%20model%0Aresponses%29%20during%20speech%20interaction%2C%20offering%20users%20a%20comprehensive%20multimodal%0Aexperience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13642v1&entry.124074799=Read"},
{"title": "Foundation Models in Medical Imaging -- A Review and Outlook", "author": "Vivien van Veldhuizen and Vanessa Botha and Chunyao Lu and Melis Erdal Cesur and Kevin Groot Lipman and Edwin D. de Jong and Hugo Horlings and Cl\u00e1risa I. Sanchez and Cees G. M. Snoek and Lodewyk Wessels and Ritse Mann and Eric Marcus and Jonas Teuwen", "abstract": "  Foundation models (FMs) are changing the way medical images are analyzed by\nlearning from large collections of unlabeled data. Instead of relying on\nmanually annotated examples, FMs are pre-trained to learn general-purpose\nvisual features that can later be adapted to specific clinical tasks with\nlittle additional supervision. In this review, we examine how FMs are being\ndeveloped and applied in pathology, radiology, and ophthalmology, drawing on\nevidence from over 150 studies. We explain the core components of FM pipelines,\nincluding model architectures, self-supervised learning methods, and strategies\nfor downstream adaptation. We also review how FMs are being used in each\nimaging domain and compare design choices across applications. Finally, we\ndiscuss key challenges and open questions to guide future research.\n", "link": "http://arxiv.org/abs/2506.09095v3", "date": "2025-06-16", "relevancy": 2.7226, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Models%20in%20Medical%20Imaging%20--%20A%20Review%20and%20Outlook&body=Title%3A%20Foundation%20Models%20in%20Medical%20Imaging%20--%20A%20Review%20and%20Outlook%0AAuthor%3A%20Vivien%20van%20Veldhuizen%20and%20Vanessa%20Botha%20and%20Chunyao%20Lu%20and%20Melis%20Erdal%20Cesur%20and%20Kevin%20Groot%20Lipman%20and%20Edwin%20D.%20de%20Jong%20and%20Hugo%20Horlings%20and%20Cl%C3%A1risa%20I.%20Sanchez%20and%20Cees%20G.%20M.%20Snoek%20and%20Lodewyk%20Wessels%20and%20Ritse%20Mann%20and%20Eric%20Marcus%20and%20Jonas%20Teuwen%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20are%20changing%20the%20way%20medical%20images%20are%20analyzed%20by%0Alearning%20from%20large%20collections%20of%20unlabeled%20data.%20Instead%20of%20relying%20on%0Amanually%20annotated%20examples%2C%20FMs%20are%20pre-trained%20to%20learn%20general-purpose%0Avisual%20features%20that%20can%20later%20be%20adapted%20to%20specific%20clinical%20tasks%20with%0Alittle%20additional%20supervision.%20In%20this%20review%2C%20we%20examine%20how%20FMs%20are%20being%0Adeveloped%20and%20applied%20in%20pathology%2C%20radiology%2C%20and%20ophthalmology%2C%20drawing%20on%0Aevidence%20from%20over%20150%20studies.%20We%20explain%20the%20core%20components%20of%20FM%20pipelines%2C%0Aincluding%20model%20architectures%2C%20self-supervised%20learning%20methods%2C%20and%20strategies%0Afor%20downstream%20adaptation.%20We%20also%20review%20how%20FMs%20are%20being%20used%20in%20each%0Aimaging%20domain%20and%20compare%20design%20choices%20across%20applications.%20Finally%2C%20we%0Adiscuss%20key%20challenges%20and%20open%20questions%20to%20guide%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09095v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Models%2520in%2520Medical%2520Imaging%2520--%2520A%2520Review%2520and%2520Outlook%26entry.906535625%3DVivien%2520van%2520Veldhuizen%2520and%2520Vanessa%2520Botha%2520and%2520Chunyao%2520Lu%2520and%2520Melis%2520Erdal%2520Cesur%2520and%2520Kevin%2520Groot%2520Lipman%2520and%2520Edwin%2520D.%2520de%2520Jong%2520and%2520Hugo%2520Horlings%2520and%2520Cl%25C3%25A1risa%2520I.%2520Sanchez%2520and%2520Cees%2520G.%2520M.%2520Snoek%2520and%2520Lodewyk%2520Wessels%2520and%2520Ritse%2520Mann%2520and%2520Eric%2520Marcus%2520and%2520Jonas%2520Teuwen%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FMs%2529%2520are%2520changing%2520the%2520way%2520medical%2520images%2520are%2520analyzed%2520by%250Alearning%2520from%2520large%2520collections%2520of%2520unlabeled%2520data.%2520Instead%2520of%2520relying%2520on%250Amanually%2520annotated%2520examples%252C%2520FMs%2520are%2520pre-trained%2520to%2520learn%2520general-purpose%250Avisual%2520features%2520that%2520can%2520later%2520be%2520adapted%2520to%2520specific%2520clinical%2520tasks%2520with%250Alittle%2520additional%2520supervision.%2520In%2520this%2520review%252C%2520we%2520examine%2520how%2520FMs%2520are%2520being%250Adeveloped%2520and%2520applied%2520in%2520pathology%252C%2520radiology%252C%2520and%2520ophthalmology%252C%2520drawing%2520on%250Aevidence%2520from%2520over%2520150%2520studies.%2520We%2520explain%2520the%2520core%2520components%2520of%2520FM%2520pipelines%252C%250Aincluding%2520model%2520architectures%252C%2520self-supervised%2520learning%2520methods%252C%2520and%2520strategies%250Afor%2520downstream%2520adaptation.%2520We%2520also%2520review%2520how%2520FMs%2520are%2520being%2520used%2520in%2520each%250Aimaging%2520domain%2520and%2520compare%2520design%2520choices%2520across%2520applications.%2520Finally%252C%2520we%250Adiscuss%2520key%2520challenges%2520and%2520open%2520questions%2520to%2520guide%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09095v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Models%20in%20Medical%20Imaging%20--%20A%20Review%20and%20Outlook&entry.906535625=Vivien%20van%20Veldhuizen%20and%20Vanessa%20Botha%20and%20Chunyao%20Lu%20and%20Melis%20Erdal%20Cesur%20and%20Kevin%20Groot%20Lipman%20and%20Edwin%20D.%20de%20Jong%20and%20Hugo%20Horlings%20and%20Cl%C3%A1risa%20I.%20Sanchez%20and%20Cees%20G.%20M.%20Snoek%20and%20Lodewyk%20Wessels%20and%20Ritse%20Mann%20and%20Eric%20Marcus%20and%20Jonas%20Teuwen&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20are%20changing%20the%20way%20medical%20images%20are%20analyzed%20by%0Alearning%20from%20large%20collections%20of%20unlabeled%20data.%20Instead%20of%20relying%20on%0Amanually%20annotated%20examples%2C%20FMs%20are%20pre-trained%20to%20learn%20general-purpose%0Avisual%20features%20that%20can%20later%20be%20adapted%20to%20specific%20clinical%20tasks%20with%0Alittle%20additional%20supervision.%20In%20this%20review%2C%20we%20examine%20how%20FMs%20are%20being%0Adeveloped%20and%20applied%20in%20pathology%2C%20radiology%2C%20and%20ophthalmology%2C%20drawing%20on%0Aevidence%20from%20over%20150%20studies.%20We%20explain%20the%20core%20components%20of%20FM%20pipelines%2C%0Aincluding%20model%20architectures%2C%20self-supervised%20learning%20methods%2C%20and%20strategies%0Afor%20downstream%20adaptation.%20We%20also%20review%20how%20FMs%20are%20being%20used%20in%20each%0Aimaging%20domain%20and%20compare%20design%20choices%20across%20applications.%20Finally%2C%20we%0Adiscuss%20key%20challenges%20and%20open%20questions%20to%20guide%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09095v3&entry.124074799=Read"},
{"title": "SA-LUT: Spatial Adaptive 4D Look-Up Table for Photorealistic Style\n  Transfer", "author": "Zerui Gong and Zhonghua Wu and Qingyi Tao and Qinyue Li and Chen Change Loy", "abstract": "  Photorealistic style transfer (PST) enables real-world color grading by\nadapting reference image colors while preserving content structure. Existing\nmethods mainly follow either approaches: generation-based methods that\nprioritize stylistic fidelity at the cost of content integrity and efficiency,\nor global color transformation methods such as LUT, which preserve structure\nbut lack local adaptability. To bridge this gap, we propose Spatial Adaptive 4D\nLook-Up Table (SA-LUT), combining LUT efficiency with neural network\nadaptability. SA-LUT features: (1) a Style-guided 4D LUT Generator that\nextracts multi-scale features from the style image to predict a 4D LUT, and (2)\na Context Generator using content-style cross-attention to produce a context\nmap. This context map enables spatially-adaptive adjustments, allowing our 4D\nLUT to apply precise color transformations while preserving structural\nintegrity. To establish a rigorous evaluation framework for photorealistic\nstyle transfer, we introduce PST50, the first benchmark specifically designed\nfor PST assessment. Experiments demonstrate that SA-LUT substantially\noutperforms state-of-the-art methods, achieving a 66.7% reduction in LPIPS\nscore compared to 3D LUT approaches, while maintaining real-time performance at\n16 FPS for video stylization. Our code and benchmark are available at\nhttps://github.com/Ry3nG/SA-LUT\n", "link": "http://arxiv.org/abs/2506.13465v1", "date": "2025-06-16", "relevancy": 2.7129, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5552}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5461}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SA-LUT%3A%20Spatial%20Adaptive%204D%20Look-Up%20Table%20for%20Photorealistic%20Style%0A%20%20Transfer&body=Title%3A%20SA-LUT%3A%20Spatial%20Adaptive%204D%20Look-Up%20Table%20for%20Photorealistic%20Style%0A%20%20Transfer%0AAuthor%3A%20Zerui%20Gong%20and%20Zhonghua%20Wu%20and%20Qingyi%20Tao%20and%20Qinyue%20Li%20and%20Chen%20Change%20Loy%0AAbstract%3A%20%20%20Photorealistic%20style%20transfer%20%28PST%29%20enables%20real-world%20color%20grading%20by%0Aadapting%20reference%20image%20colors%20while%20preserving%20content%20structure.%20Existing%0Amethods%20mainly%20follow%20either%20approaches%3A%20generation-based%20methods%20that%0Aprioritize%20stylistic%20fidelity%20at%20the%20cost%20of%20content%20integrity%20and%20efficiency%2C%0Aor%20global%20color%20transformation%20methods%20such%20as%20LUT%2C%20which%20preserve%20structure%0Abut%20lack%20local%20adaptability.%20To%20bridge%20this%20gap%2C%20we%20propose%20Spatial%20Adaptive%204D%0ALook-Up%20Table%20%28SA-LUT%29%2C%20combining%20LUT%20efficiency%20with%20neural%20network%0Aadaptability.%20SA-LUT%20features%3A%20%281%29%20a%20Style-guided%204D%20LUT%20Generator%20that%0Aextracts%20multi-scale%20features%20from%20the%20style%20image%20to%20predict%20a%204D%20LUT%2C%20and%20%282%29%0Aa%20Context%20Generator%20using%20content-style%20cross-attention%20to%20produce%20a%20context%0Amap.%20This%20context%20map%20enables%20spatially-adaptive%20adjustments%2C%20allowing%20our%204D%0ALUT%20to%20apply%20precise%20color%20transformations%20while%20preserving%20structural%0Aintegrity.%20To%20establish%20a%20rigorous%20evaluation%20framework%20for%20photorealistic%0Astyle%20transfer%2C%20we%20introduce%20PST50%2C%20the%20first%20benchmark%20specifically%20designed%0Afor%20PST%20assessment.%20Experiments%20demonstrate%20that%20SA-LUT%20substantially%0Aoutperforms%20state-of-the-art%20methods%2C%20achieving%20a%2066.7%25%20reduction%20in%20LPIPS%0Ascore%20compared%20to%203D%20LUT%20approaches%2C%20while%20maintaining%20real-time%20performance%20at%0A16%20FPS%20for%20video%20stylization.%20Our%20code%20and%20benchmark%20are%20available%20at%0Ahttps%3A//github.com/Ry3nG/SA-LUT%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSA-LUT%253A%2520Spatial%2520Adaptive%25204D%2520Look-Up%2520Table%2520for%2520Photorealistic%2520Style%250A%2520%2520Transfer%26entry.906535625%3DZerui%2520Gong%2520and%2520Zhonghua%2520Wu%2520and%2520Qingyi%2520Tao%2520and%2520Qinyue%2520Li%2520and%2520Chen%2520Change%2520Loy%26entry.1292438233%3D%2520%2520Photorealistic%2520style%2520transfer%2520%2528PST%2529%2520enables%2520real-world%2520color%2520grading%2520by%250Aadapting%2520reference%2520image%2520colors%2520while%2520preserving%2520content%2520structure.%2520Existing%250Amethods%2520mainly%2520follow%2520either%2520approaches%253A%2520generation-based%2520methods%2520that%250Aprioritize%2520stylistic%2520fidelity%2520at%2520the%2520cost%2520of%2520content%2520integrity%2520and%2520efficiency%252C%250Aor%2520global%2520color%2520transformation%2520methods%2520such%2520as%2520LUT%252C%2520which%2520preserve%2520structure%250Abut%2520lack%2520local%2520adaptability.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520Spatial%2520Adaptive%25204D%250ALook-Up%2520Table%2520%2528SA-LUT%2529%252C%2520combining%2520LUT%2520efficiency%2520with%2520neural%2520network%250Aadaptability.%2520SA-LUT%2520features%253A%2520%25281%2529%2520a%2520Style-guided%25204D%2520LUT%2520Generator%2520that%250Aextracts%2520multi-scale%2520features%2520from%2520the%2520style%2520image%2520to%2520predict%2520a%25204D%2520LUT%252C%2520and%2520%25282%2529%250Aa%2520Context%2520Generator%2520using%2520content-style%2520cross-attention%2520to%2520produce%2520a%2520context%250Amap.%2520This%2520context%2520map%2520enables%2520spatially-adaptive%2520adjustments%252C%2520allowing%2520our%25204D%250ALUT%2520to%2520apply%2520precise%2520color%2520transformations%2520while%2520preserving%2520structural%250Aintegrity.%2520To%2520establish%2520a%2520rigorous%2520evaluation%2520framework%2520for%2520photorealistic%250Astyle%2520transfer%252C%2520we%2520introduce%2520PST50%252C%2520the%2520first%2520benchmark%2520specifically%2520designed%250Afor%2520PST%2520assessment.%2520Experiments%2520demonstrate%2520that%2520SA-LUT%2520substantially%250Aoutperforms%2520state-of-the-art%2520methods%252C%2520achieving%2520a%252066.7%2525%2520reduction%2520in%2520LPIPS%250Ascore%2520compared%2520to%25203D%2520LUT%2520approaches%252C%2520while%2520maintaining%2520real-time%2520performance%2520at%250A16%2520FPS%2520for%2520video%2520stylization.%2520Our%2520code%2520and%2520benchmark%2520are%2520available%2520at%250Ahttps%253A//github.com/Ry3nG/SA-LUT%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SA-LUT%3A%20Spatial%20Adaptive%204D%20Look-Up%20Table%20for%20Photorealistic%20Style%0A%20%20Transfer&entry.906535625=Zerui%20Gong%20and%20Zhonghua%20Wu%20and%20Qingyi%20Tao%20and%20Qinyue%20Li%20and%20Chen%20Change%20Loy&entry.1292438233=%20%20Photorealistic%20style%20transfer%20%28PST%29%20enables%20real-world%20color%20grading%20by%0Aadapting%20reference%20image%20colors%20while%20preserving%20content%20structure.%20Existing%0Amethods%20mainly%20follow%20either%20approaches%3A%20generation-based%20methods%20that%0Aprioritize%20stylistic%20fidelity%20at%20the%20cost%20of%20content%20integrity%20and%20efficiency%2C%0Aor%20global%20color%20transformation%20methods%20such%20as%20LUT%2C%20which%20preserve%20structure%0Abut%20lack%20local%20adaptability.%20To%20bridge%20this%20gap%2C%20we%20propose%20Spatial%20Adaptive%204D%0ALook-Up%20Table%20%28SA-LUT%29%2C%20combining%20LUT%20efficiency%20with%20neural%20network%0Aadaptability.%20SA-LUT%20features%3A%20%281%29%20a%20Style-guided%204D%20LUT%20Generator%20that%0Aextracts%20multi-scale%20features%20from%20the%20style%20image%20to%20predict%20a%204D%20LUT%2C%20and%20%282%29%0Aa%20Context%20Generator%20using%20content-style%20cross-attention%20to%20produce%20a%20context%0Amap.%20This%20context%20map%20enables%20spatially-adaptive%20adjustments%2C%20allowing%20our%204D%0ALUT%20to%20apply%20precise%20color%20transformations%20while%20preserving%20structural%0Aintegrity.%20To%20establish%20a%20rigorous%20evaluation%20framework%20for%20photorealistic%0Astyle%20transfer%2C%20we%20introduce%20PST50%2C%20the%20first%20benchmark%20specifically%20designed%0Afor%20PST%20assessment.%20Experiments%20demonstrate%20that%20SA-LUT%20substantially%0Aoutperforms%20state-of-the-art%20methods%2C%20achieving%20a%2066.7%25%20reduction%20in%20LPIPS%0Ascore%20compared%20to%203D%20LUT%20approaches%2C%20while%20maintaining%20real-time%20performance%20at%0A16%20FPS%20for%20video%20stylization.%20Our%20code%20and%20benchmark%20are%20available%20at%0Ahttps%3A//github.com/Ry3nG/SA-LUT%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13465v1&entry.124074799=Read"},
{"title": "Attribution-guided Pruning for Compression, Circuit Discovery, and\n  Targeted Correction in LLMs", "author": "Sayed Mohammad Vakilzadeh Hatefi and Maximilian Dreyer and Reduan Achtibat and Patrick Kahardipraja and Thomas Wiegand and Wojciech Samek and Sebastian Lapuschkin", "abstract": "  Large Language Models (LLMs) are central to many contemporary AI\napplications, yet their extensive parameter counts pose significant challenges\nfor deployment in memory- and compute-constrained environments. Recent works in\neXplainable AI (XAI), particularly on attribution methods, suggest that\ninterpretability can also enable model compression by identifying and removing\ncomponents irrelevant to inference. In this paper, we leverage Layer-wise\nRelevance Propagation (LRP) to perform attribution-guided pruning of LLMs.\nWhile LRP has shown promise in structured pruning for vision models, we extend\nit to unstructured pruning in LLMs and demonstrate that it can substantially\nreduce model size with minimal performance loss. Our method is especially\neffective in extracting task-relevant subgraphs -- so-called ``circuits'' --\nwhich can represent core functions (e.g., indirect object identification).\nBuilding on this, we introduce a technique for model correction, by selectively\nremoving circuits responsible for spurious behaviors (e.g., toxic outputs). All\nin all, we gather these techniques as a uniform holistic framework and showcase\nits effectiveness and limitations through extensive experiments for\ncompression, circuit discovery and model correction on Llama and OPT models,\nhighlighting its potential for improving both model efficiency and safety. Our\ncode is publicly available at https://github.com/erfanhatefi/SparC3.\n", "link": "http://arxiv.org/abs/2506.13727v1", "date": "2025-06-16", "relevancy": 2.7123, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5465}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5465}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attribution-guided%20Pruning%20for%20Compression%2C%20Circuit%20Discovery%2C%20and%0A%20%20Targeted%20Correction%20in%20LLMs&body=Title%3A%20Attribution-guided%20Pruning%20for%20Compression%2C%20Circuit%20Discovery%2C%20and%0A%20%20Targeted%20Correction%20in%20LLMs%0AAuthor%3A%20Sayed%20Mohammad%20Vakilzadeh%20Hatefi%20and%20Maximilian%20Dreyer%20and%20Reduan%20Achtibat%20and%20Patrick%20Kahardipraja%20and%20Thomas%20Wiegand%20and%20Wojciech%20Samek%20and%20Sebastian%20Lapuschkin%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20central%20to%20many%20contemporary%20AI%0Aapplications%2C%20yet%20their%20extensive%20parameter%20counts%20pose%20significant%20challenges%0Afor%20deployment%20in%20memory-%20and%20compute-constrained%20environments.%20Recent%20works%20in%0AeXplainable%20AI%20%28XAI%29%2C%20particularly%20on%20attribution%20methods%2C%20suggest%20that%0Ainterpretability%20can%20also%20enable%20model%20compression%20by%20identifying%20and%20removing%0Acomponents%20irrelevant%20to%20inference.%20In%20this%20paper%2C%20we%20leverage%20Layer-wise%0ARelevance%20Propagation%20%28LRP%29%20to%20perform%20attribution-guided%20pruning%20of%20LLMs.%0AWhile%20LRP%20has%20shown%20promise%20in%20structured%20pruning%20for%20vision%20models%2C%20we%20extend%0Ait%20to%20unstructured%20pruning%20in%20LLMs%20and%20demonstrate%20that%20it%20can%20substantially%0Areduce%20model%20size%20with%20minimal%20performance%20loss.%20Our%20method%20is%20especially%0Aeffective%20in%20extracting%20task-relevant%20subgraphs%20--%20so-called%20%60%60circuits%27%27%20--%0Awhich%20can%20represent%20core%20functions%20%28e.g.%2C%20indirect%20object%20identification%29.%0ABuilding%20on%20this%2C%20we%20introduce%20a%20technique%20for%20model%20correction%2C%20by%20selectively%0Aremoving%20circuits%20responsible%20for%20spurious%20behaviors%20%28e.g.%2C%20toxic%20outputs%29.%20All%0Ain%20all%2C%20we%20gather%20these%20techniques%20as%20a%20uniform%20holistic%20framework%20and%20showcase%0Aits%20effectiveness%20and%20limitations%20through%20extensive%20experiments%20for%0Acompression%2C%20circuit%20discovery%20and%20model%20correction%20on%20Llama%20and%20OPT%20models%2C%0Ahighlighting%20its%20potential%20for%20improving%20both%20model%20efficiency%20and%20safety.%20Our%0Acode%20is%20publicly%20available%20at%20https%3A//github.com/erfanhatefi/SparC3.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttribution-guided%2520Pruning%2520for%2520Compression%252C%2520Circuit%2520Discovery%252C%2520and%250A%2520%2520Targeted%2520Correction%2520in%2520LLMs%26entry.906535625%3DSayed%2520Mohammad%2520Vakilzadeh%2520Hatefi%2520and%2520Maximilian%2520Dreyer%2520and%2520Reduan%2520Achtibat%2520and%2520Patrick%2520Kahardipraja%2520and%2520Thomas%2520Wiegand%2520and%2520Wojciech%2520Samek%2520and%2520Sebastian%2520Lapuschkin%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520central%2520to%2520many%2520contemporary%2520AI%250Aapplications%252C%2520yet%2520their%2520extensive%2520parameter%2520counts%2520pose%2520significant%2520challenges%250Afor%2520deployment%2520in%2520memory-%2520and%2520compute-constrained%2520environments.%2520Recent%2520works%2520in%250AeXplainable%2520AI%2520%2528XAI%2529%252C%2520particularly%2520on%2520attribution%2520methods%252C%2520suggest%2520that%250Ainterpretability%2520can%2520also%2520enable%2520model%2520compression%2520by%2520identifying%2520and%2520removing%250Acomponents%2520irrelevant%2520to%2520inference.%2520In%2520this%2520paper%252C%2520we%2520leverage%2520Layer-wise%250ARelevance%2520Propagation%2520%2528LRP%2529%2520to%2520perform%2520attribution-guided%2520pruning%2520of%2520LLMs.%250AWhile%2520LRP%2520has%2520shown%2520promise%2520in%2520structured%2520pruning%2520for%2520vision%2520models%252C%2520we%2520extend%250Ait%2520to%2520unstructured%2520pruning%2520in%2520LLMs%2520and%2520demonstrate%2520that%2520it%2520can%2520substantially%250Areduce%2520model%2520size%2520with%2520minimal%2520performance%2520loss.%2520Our%2520method%2520is%2520especially%250Aeffective%2520in%2520extracting%2520task-relevant%2520subgraphs%2520--%2520so-called%2520%2560%2560circuits%2527%2527%2520--%250Awhich%2520can%2520represent%2520core%2520functions%2520%2528e.g.%252C%2520indirect%2520object%2520identification%2529.%250ABuilding%2520on%2520this%252C%2520we%2520introduce%2520a%2520technique%2520for%2520model%2520correction%252C%2520by%2520selectively%250Aremoving%2520circuits%2520responsible%2520for%2520spurious%2520behaviors%2520%2528e.g.%252C%2520toxic%2520outputs%2529.%2520All%250Ain%2520all%252C%2520we%2520gather%2520these%2520techniques%2520as%2520a%2520uniform%2520holistic%2520framework%2520and%2520showcase%250Aits%2520effectiveness%2520and%2520limitations%2520through%2520extensive%2520experiments%2520for%250Acompression%252C%2520circuit%2520discovery%2520and%2520model%2520correction%2520on%2520Llama%2520and%2520OPT%2520models%252C%250Ahighlighting%2520its%2520potential%2520for%2520improving%2520both%2520model%2520efficiency%2520and%2520safety.%2520Our%250Acode%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/erfanhatefi/SparC3.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attribution-guided%20Pruning%20for%20Compression%2C%20Circuit%20Discovery%2C%20and%0A%20%20Targeted%20Correction%20in%20LLMs&entry.906535625=Sayed%20Mohammad%20Vakilzadeh%20Hatefi%20and%20Maximilian%20Dreyer%20and%20Reduan%20Achtibat%20and%20Patrick%20Kahardipraja%20and%20Thomas%20Wiegand%20and%20Wojciech%20Samek%20and%20Sebastian%20Lapuschkin&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20central%20to%20many%20contemporary%20AI%0Aapplications%2C%20yet%20their%20extensive%20parameter%20counts%20pose%20significant%20challenges%0Afor%20deployment%20in%20memory-%20and%20compute-constrained%20environments.%20Recent%20works%20in%0AeXplainable%20AI%20%28XAI%29%2C%20particularly%20on%20attribution%20methods%2C%20suggest%20that%0Ainterpretability%20can%20also%20enable%20model%20compression%20by%20identifying%20and%20removing%0Acomponents%20irrelevant%20to%20inference.%20In%20this%20paper%2C%20we%20leverage%20Layer-wise%0ARelevance%20Propagation%20%28LRP%29%20to%20perform%20attribution-guided%20pruning%20of%20LLMs.%0AWhile%20LRP%20has%20shown%20promise%20in%20structured%20pruning%20for%20vision%20models%2C%20we%20extend%0Ait%20to%20unstructured%20pruning%20in%20LLMs%20and%20demonstrate%20that%20it%20can%20substantially%0Areduce%20model%20size%20with%20minimal%20performance%20loss.%20Our%20method%20is%20especially%0Aeffective%20in%20extracting%20task-relevant%20subgraphs%20--%20so-called%20%60%60circuits%27%27%20--%0Awhich%20can%20represent%20core%20functions%20%28e.g.%2C%20indirect%20object%20identification%29.%0ABuilding%20on%20this%2C%20we%20introduce%20a%20technique%20for%20model%20correction%2C%20by%20selectively%0Aremoving%20circuits%20responsible%20for%20spurious%20behaviors%20%28e.g.%2C%20toxic%20outputs%29.%20All%0Ain%20all%2C%20we%20gather%20these%20techniques%20as%20a%20uniform%20holistic%20framework%20and%20showcase%0Aits%20effectiveness%20and%20limitations%20through%20extensive%20experiments%20for%0Acompression%2C%20circuit%20discovery%20and%20model%20correction%20on%20Llama%20and%20OPT%20models%2C%0Ahighlighting%20its%20potential%20for%20improving%20both%20model%20efficiency%20and%20safety.%20Our%0Acode%20is%20publicly%20available%20at%20https%3A//github.com/erfanhatefi/SparC3.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13727v1&entry.124074799=Read"},
{"title": "RelTopo: Enhancing Relational Modeling for Driving Scene Topology\n  Reasoning", "author": "Yueru Luo and Changqing Zhou and Yiming Yang and Erlong Li and Chao Zheng and Shuqi Mei and Shuguang Cui and Zhen Li", "abstract": "  Accurate road topology reasoning is critical for autonomous driving, enabling\neffective navigation and adherence to traffic regulations. Central to this task\nare lane perception and topology reasoning. However, existing methods typically\nfocus on either lane detection or Lane-to-Lane (L2L) topology reasoning, often\n\\textit{neglecting} Lane-to-Traffic-element (L2T) relationships or\n\\textit{failing} to optimize these tasks jointly. Furthermore, most approaches\neither overlook relational modeling or apply it in a limited scope, despite the\ninherent spatial relationships among road elements. We argue that relational\nmodeling is beneficial for both perception and reasoning, as humans naturally\nleverage contextual relationships for road element recognition and their\nconnectivity inference. To this end, we introduce relational modeling into both\nperception and reasoning, \\textit{jointly} enhancing structural understanding.\nSpecifically, we propose: 1) a relation-aware lane detector, where our\ngeometry-biased self-attention and \\curve\\ cross-attention refine lane\nrepresentations by capturing relational dependencies; 2) relation-enhanced\ntopology heads, including a geometry-enhanced L2L head and a cross-view L2T\nhead, boosting reasoning with relational cues; and 3) a contrastive learning\nstrategy with InfoNCE loss to regularize relationship embeddings. Extensive\nexperiments on OpenLane-V2 demonstrate that our approach significantly improves\nboth detection and topology reasoning metrics, achieving +3.1 in DET$_l$, +5.3\nin TOP$_{ll}$, +4.9 in TOP$_{lt}$, and an overall +4.4 in OLS, setting a new\nstate-of-the-art. Code will be released.\n", "link": "http://arxiv.org/abs/2506.13553v1", "date": "2025-06-16", "relevancy": 2.6829, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5367}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5367}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RelTopo%3A%20Enhancing%20Relational%20Modeling%20for%20Driving%20Scene%20Topology%0A%20%20Reasoning&body=Title%3A%20RelTopo%3A%20Enhancing%20Relational%20Modeling%20for%20Driving%20Scene%20Topology%0A%20%20Reasoning%0AAuthor%3A%20Yueru%20Luo%20and%20Changqing%20Zhou%20and%20Yiming%20Yang%20and%20Erlong%20Li%20and%20Chao%20Zheng%20and%20Shuqi%20Mei%20and%20Shuguang%20Cui%20and%20Zhen%20Li%0AAbstract%3A%20%20%20Accurate%20road%20topology%20reasoning%20is%20critical%20for%20autonomous%20driving%2C%20enabling%0Aeffective%20navigation%20and%20adherence%20to%20traffic%20regulations.%20Central%20to%20this%20task%0Aare%20lane%20perception%20and%20topology%20reasoning.%20However%2C%20existing%20methods%20typically%0Afocus%20on%20either%20lane%20detection%20or%20Lane-to-Lane%20%28L2L%29%20topology%20reasoning%2C%20often%0A%5Ctextit%7Bneglecting%7D%20Lane-to-Traffic-element%20%28L2T%29%20relationships%20or%0A%5Ctextit%7Bfailing%7D%20to%20optimize%20these%20tasks%20jointly.%20Furthermore%2C%20most%20approaches%0Aeither%20overlook%20relational%20modeling%20or%20apply%20it%20in%20a%20limited%20scope%2C%20despite%20the%0Ainherent%20spatial%20relationships%20among%20road%20elements.%20We%20argue%20that%20relational%0Amodeling%20is%20beneficial%20for%20both%20perception%20and%20reasoning%2C%20as%20humans%20naturally%0Aleverage%20contextual%20relationships%20for%20road%20element%20recognition%20and%20their%0Aconnectivity%20inference.%20To%20this%20end%2C%20we%20introduce%20relational%20modeling%20into%20both%0Aperception%20and%20reasoning%2C%20%5Ctextit%7Bjointly%7D%20enhancing%20structural%20understanding.%0ASpecifically%2C%20we%20propose%3A%201%29%20a%20relation-aware%20lane%20detector%2C%20where%20our%0Ageometry-biased%20self-attention%20and%20%5Ccurve%5C%20cross-attention%20refine%20lane%0Arepresentations%20by%20capturing%20relational%20dependencies%3B%202%29%20relation-enhanced%0Atopology%20heads%2C%20including%20a%20geometry-enhanced%20L2L%20head%20and%20a%20cross-view%20L2T%0Ahead%2C%20boosting%20reasoning%20with%20relational%20cues%3B%20and%203%29%20a%20contrastive%20learning%0Astrategy%20with%20InfoNCE%20loss%20to%20regularize%20relationship%20embeddings.%20Extensive%0Aexperiments%20on%20OpenLane-V2%20demonstrate%20that%20our%20approach%20significantly%20improves%0Aboth%20detection%20and%20topology%20reasoning%20metrics%2C%20achieving%20%2B3.1%20in%20DET%24_l%24%2C%20%2B5.3%0Ain%20TOP%24_%7Bll%7D%24%2C%20%2B4.9%20in%20TOP%24_%7Blt%7D%24%2C%20and%20an%20overall%20%2B4.4%20in%20OLS%2C%20setting%20a%20new%0Astate-of-the-art.%20Code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelTopo%253A%2520Enhancing%2520Relational%2520Modeling%2520for%2520Driving%2520Scene%2520Topology%250A%2520%2520Reasoning%26entry.906535625%3DYueru%2520Luo%2520and%2520Changqing%2520Zhou%2520and%2520Yiming%2520Yang%2520and%2520Erlong%2520Li%2520and%2520Chao%2520Zheng%2520and%2520Shuqi%2520Mei%2520and%2520Shuguang%2520Cui%2520and%2520Zhen%2520Li%26entry.1292438233%3D%2520%2520Accurate%2520road%2520topology%2520reasoning%2520is%2520critical%2520for%2520autonomous%2520driving%252C%2520enabling%250Aeffective%2520navigation%2520and%2520adherence%2520to%2520traffic%2520regulations.%2520Central%2520to%2520this%2520task%250Aare%2520lane%2520perception%2520and%2520topology%2520reasoning.%2520However%252C%2520existing%2520methods%2520typically%250Afocus%2520on%2520either%2520lane%2520detection%2520or%2520Lane-to-Lane%2520%2528L2L%2529%2520topology%2520reasoning%252C%2520often%250A%255Ctextit%257Bneglecting%257D%2520Lane-to-Traffic-element%2520%2528L2T%2529%2520relationships%2520or%250A%255Ctextit%257Bfailing%257D%2520to%2520optimize%2520these%2520tasks%2520jointly.%2520Furthermore%252C%2520most%2520approaches%250Aeither%2520overlook%2520relational%2520modeling%2520or%2520apply%2520it%2520in%2520a%2520limited%2520scope%252C%2520despite%2520the%250Ainherent%2520spatial%2520relationships%2520among%2520road%2520elements.%2520We%2520argue%2520that%2520relational%250Amodeling%2520is%2520beneficial%2520for%2520both%2520perception%2520and%2520reasoning%252C%2520as%2520humans%2520naturally%250Aleverage%2520contextual%2520relationships%2520for%2520road%2520element%2520recognition%2520and%2520their%250Aconnectivity%2520inference.%2520To%2520this%2520end%252C%2520we%2520introduce%2520relational%2520modeling%2520into%2520both%250Aperception%2520and%2520reasoning%252C%2520%255Ctextit%257Bjointly%257D%2520enhancing%2520structural%2520understanding.%250ASpecifically%252C%2520we%2520propose%253A%25201%2529%2520a%2520relation-aware%2520lane%2520detector%252C%2520where%2520our%250Ageometry-biased%2520self-attention%2520and%2520%255Ccurve%255C%2520cross-attention%2520refine%2520lane%250Arepresentations%2520by%2520capturing%2520relational%2520dependencies%253B%25202%2529%2520relation-enhanced%250Atopology%2520heads%252C%2520including%2520a%2520geometry-enhanced%2520L2L%2520head%2520and%2520a%2520cross-view%2520L2T%250Ahead%252C%2520boosting%2520reasoning%2520with%2520relational%2520cues%253B%2520and%25203%2529%2520a%2520contrastive%2520learning%250Astrategy%2520with%2520InfoNCE%2520loss%2520to%2520regularize%2520relationship%2520embeddings.%2520Extensive%250Aexperiments%2520on%2520OpenLane-V2%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520improves%250Aboth%2520detection%2520and%2520topology%2520reasoning%2520metrics%252C%2520achieving%2520%252B3.1%2520in%2520DET%2524_l%2524%252C%2520%252B5.3%250Ain%2520TOP%2524_%257Bll%257D%2524%252C%2520%252B4.9%2520in%2520TOP%2524_%257Blt%257D%2524%252C%2520and%2520an%2520overall%2520%252B4.4%2520in%2520OLS%252C%2520setting%2520a%2520new%250Astate-of-the-art.%2520Code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RelTopo%3A%20Enhancing%20Relational%20Modeling%20for%20Driving%20Scene%20Topology%0A%20%20Reasoning&entry.906535625=Yueru%20Luo%20and%20Changqing%20Zhou%20and%20Yiming%20Yang%20and%20Erlong%20Li%20and%20Chao%20Zheng%20and%20Shuqi%20Mei%20and%20Shuguang%20Cui%20and%20Zhen%20Li&entry.1292438233=%20%20Accurate%20road%20topology%20reasoning%20is%20critical%20for%20autonomous%20driving%2C%20enabling%0Aeffective%20navigation%20and%20adherence%20to%20traffic%20regulations.%20Central%20to%20this%20task%0Aare%20lane%20perception%20and%20topology%20reasoning.%20However%2C%20existing%20methods%20typically%0Afocus%20on%20either%20lane%20detection%20or%20Lane-to-Lane%20%28L2L%29%20topology%20reasoning%2C%20often%0A%5Ctextit%7Bneglecting%7D%20Lane-to-Traffic-element%20%28L2T%29%20relationships%20or%0A%5Ctextit%7Bfailing%7D%20to%20optimize%20these%20tasks%20jointly.%20Furthermore%2C%20most%20approaches%0Aeither%20overlook%20relational%20modeling%20or%20apply%20it%20in%20a%20limited%20scope%2C%20despite%20the%0Ainherent%20spatial%20relationships%20among%20road%20elements.%20We%20argue%20that%20relational%0Amodeling%20is%20beneficial%20for%20both%20perception%20and%20reasoning%2C%20as%20humans%20naturally%0Aleverage%20contextual%20relationships%20for%20road%20element%20recognition%20and%20their%0Aconnectivity%20inference.%20To%20this%20end%2C%20we%20introduce%20relational%20modeling%20into%20both%0Aperception%20and%20reasoning%2C%20%5Ctextit%7Bjointly%7D%20enhancing%20structural%20understanding.%0ASpecifically%2C%20we%20propose%3A%201%29%20a%20relation-aware%20lane%20detector%2C%20where%20our%0Ageometry-biased%20self-attention%20and%20%5Ccurve%5C%20cross-attention%20refine%20lane%0Arepresentations%20by%20capturing%20relational%20dependencies%3B%202%29%20relation-enhanced%0Atopology%20heads%2C%20including%20a%20geometry-enhanced%20L2L%20head%20and%20a%20cross-view%20L2T%0Ahead%2C%20boosting%20reasoning%20with%20relational%20cues%3B%20and%203%29%20a%20contrastive%20learning%0Astrategy%20with%20InfoNCE%20loss%20to%20regularize%20relationship%20embeddings.%20Extensive%0Aexperiments%20on%20OpenLane-V2%20demonstrate%20that%20our%20approach%20significantly%20improves%0Aboth%20detection%20and%20topology%20reasoning%20metrics%2C%20achieving%20%2B3.1%20in%20DET%24_l%24%2C%20%2B5.3%0Ain%20TOP%24_%7Bll%7D%24%2C%20%2B4.9%20in%20TOP%24_%7Blt%7D%24%2C%20and%20an%20overall%20%2B4.4%20in%20OLS%2C%20setting%20a%20new%0Astate-of-the-art.%20Code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13553v1&entry.124074799=Read"},
{"title": "Atomizer: Generalizing to new modalities by breaking satellite images\n  down to a set of scalars", "author": "Hugo Riffaud de Turckheim and Sylvain Lobry and Roberto Interdonato and Diego Marcos", "abstract": "  The growing number of Earth observation satellites has led to increasingly\ndiverse remote sensing data, with varying spatial, spectral, and temporal\nconfigurations. Most existing models rely on fixed input formats and\nmodality-specific encoders, which require retraining when new configurations\nare introduced, limiting their ability to generalize across modalities. We\nintroduce Atomizer, a flexible architecture that represents remote sensing\nimages as sets of scalars, each corresponding to a spectral band value of a\npixel. Each scalar is enriched with contextual metadata (acquisition time,\nspatial resolution, wavelength, and bandwidth), producing an atomic\nrepresentation that allows a single encoder to process arbitrary modalities\nwithout interpolation or resampling. Atomizer uses structured tokenization with\nFourier features and non-uniform radial basis functions to encode content and\ncontext, and maps tokens into a latent space via cross-attention. Under\nmodality-disjoint evaluations, Atomizer outperforms standard models and\ndemonstrates robust performance across varying resolutions and spatial sizes.\n", "link": "http://arxiv.org/abs/2506.13542v1", "date": "2025-06-16", "relevancy": 2.6721, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5446}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5411}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Atomizer%3A%20Generalizing%20to%20new%20modalities%20by%20breaking%20satellite%20images%0A%20%20down%20to%20a%20set%20of%20scalars&body=Title%3A%20Atomizer%3A%20Generalizing%20to%20new%20modalities%20by%20breaking%20satellite%20images%0A%20%20down%20to%20a%20set%20of%20scalars%0AAuthor%3A%20Hugo%20Riffaud%20de%20Turckheim%20and%20Sylvain%20Lobry%20and%20Roberto%20Interdonato%20and%20Diego%20Marcos%0AAbstract%3A%20%20%20The%20growing%20number%20of%20Earth%20observation%20satellites%20has%20led%20to%20increasingly%0Adiverse%20remote%20sensing%20data%2C%20with%20varying%20spatial%2C%20spectral%2C%20and%20temporal%0Aconfigurations.%20Most%20existing%20models%20rely%20on%20fixed%20input%20formats%20and%0Amodality-specific%20encoders%2C%20which%20require%20retraining%20when%20new%20configurations%0Aare%20introduced%2C%20limiting%20their%20ability%20to%20generalize%20across%20modalities.%20We%0Aintroduce%20Atomizer%2C%20a%20flexible%20architecture%20that%20represents%20remote%20sensing%0Aimages%20as%20sets%20of%20scalars%2C%20each%20corresponding%20to%20a%20spectral%20band%20value%20of%20a%0Apixel.%20Each%20scalar%20is%20enriched%20with%20contextual%20metadata%20%28acquisition%20time%2C%0Aspatial%20resolution%2C%20wavelength%2C%20and%20bandwidth%29%2C%20producing%20an%20atomic%0Arepresentation%20that%20allows%20a%20single%20encoder%20to%20process%20arbitrary%20modalities%0Awithout%20interpolation%20or%20resampling.%20Atomizer%20uses%20structured%20tokenization%20with%0AFourier%20features%20and%20non-uniform%20radial%20basis%20functions%20to%20encode%20content%20and%0Acontext%2C%20and%20maps%20tokens%20into%20a%20latent%20space%20via%20cross-attention.%20Under%0Amodality-disjoint%20evaluations%2C%20Atomizer%20outperforms%20standard%20models%20and%0Ademonstrates%20robust%20performance%20across%20varying%20resolutions%20and%20spatial%20sizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAtomizer%253A%2520Generalizing%2520to%2520new%2520modalities%2520by%2520breaking%2520satellite%2520images%250A%2520%2520down%2520to%2520a%2520set%2520of%2520scalars%26entry.906535625%3DHugo%2520Riffaud%2520de%2520Turckheim%2520and%2520Sylvain%2520Lobry%2520and%2520Roberto%2520Interdonato%2520and%2520Diego%2520Marcos%26entry.1292438233%3D%2520%2520The%2520growing%2520number%2520of%2520Earth%2520observation%2520satellites%2520has%2520led%2520to%2520increasingly%250Adiverse%2520remote%2520sensing%2520data%252C%2520with%2520varying%2520spatial%252C%2520spectral%252C%2520and%2520temporal%250Aconfigurations.%2520Most%2520existing%2520models%2520rely%2520on%2520fixed%2520input%2520formats%2520and%250Amodality-specific%2520encoders%252C%2520which%2520require%2520retraining%2520when%2520new%2520configurations%250Aare%2520introduced%252C%2520limiting%2520their%2520ability%2520to%2520generalize%2520across%2520modalities.%2520We%250Aintroduce%2520Atomizer%252C%2520a%2520flexible%2520architecture%2520that%2520represents%2520remote%2520sensing%250Aimages%2520as%2520sets%2520of%2520scalars%252C%2520each%2520corresponding%2520to%2520a%2520spectral%2520band%2520value%2520of%2520a%250Apixel.%2520Each%2520scalar%2520is%2520enriched%2520with%2520contextual%2520metadata%2520%2528acquisition%2520time%252C%250Aspatial%2520resolution%252C%2520wavelength%252C%2520and%2520bandwidth%2529%252C%2520producing%2520an%2520atomic%250Arepresentation%2520that%2520allows%2520a%2520single%2520encoder%2520to%2520process%2520arbitrary%2520modalities%250Awithout%2520interpolation%2520or%2520resampling.%2520Atomizer%2520uses%2520structured%2520tokenization%2520with%250AFourier%2520features%2520and%2520non-uniform%2520radial%2520basis%2520functions%2520to%2520encode%2520content%2520and%250Acontext%252C%2520and%2520maps%2520tokens%2520into%2520a%2520latent%2520space%2520via%2520cross-attention.%2520Under%250Amodality-disjoint%2520evaluations%252C%2520Atomizer%2520outperforms%2520standard%2520models%2520and%250Ademonstrates%2520robust%2520performance%2520across%2520varying%2520resolutions%2520and%2520spatial%2520sizes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Atomizer%3A%20Generalizing%20to%20new%20modalities%20by%20breaking%20satellite%20images%0A%20%20down%20to%20a%20set%20of%20scalars&entry.906535625=Hugo%20Riffaud%20de%20Turckheim%20and%20Sylvain%20Lobry%20and%20Roberto%20Interdonato%20and%20Diego%20Marcos&entry.1292438233=%20%20The%20growing%20number%20of%20Earth%20observation%20satellites%20has%20led%20to%20increasingly%0Adiverse%20remote%20sensing%20data%2C%20with%20varying%20spatial%2C%20spectral%2C%20and%20temporal%0Aconfigurations.%20Most%20existing%20models%20rely%20on%20fixed%20input%20formats%20and%0Amodality-specific%20encoders%2C%20which%20require%20retraining%20when%20new%20configurations%0Aare%20introduced%2C%20limiting%20their%20ability%20to%20generalize%20across%20modalities.%20We%0Aintroduce%20Atomizer%2C%20a%20flexible%20architecture%20that%20represents%20remote%20sensing%0Aimages%20as%20sets%20of%20scalars%2C%20each%20corresponding%20to%20a%20spectral%20band%20value%20of%20a%0Apixel.%20Each%20scalar%20is%20enriched%20with%20contextual%20metadata%20%28acquisition%20time%2C%0Aspatial%20resolution%2C%20wavelength%2C%20and%20bandwidth%29%2C%20producing%20an%20atomic%0Arepresentation%20that%20allows%20a%20single%20encoder%20to%20process%20arbitrary%20modalities%0Awithout%20interpolation%20or%20resampling.%20Atomizer%20uses%20structured%20tokenization%20with%0AFourier%20features%20and%20non-uniform%20radial%20basis%20functions%20to%20encode%20content%20and%0Acontext%2C%20and%20maps%20tokens%20into%20a%20latent%20space%20via%20cross-attention.%20Under%0Amodality-disjoint%20evaluations%2C%20Atomizer%20outperforms%20standard%20models%20and%0Ademonstrates%20robust%20performance%20across%20varying%20resolutions%20and%20spatial%20sizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13542v1&entry.124074799=Read"},
{"title": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization", "author": "Guanghui Song and Dongping Liao and Yiren Zhao and Kejiang Ye and Cheng-zhong Xu and Xitong Gao", "abstract": "  Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets.\n", "link": "http://arxiv.org/abs/2506.13541v1", "date": "2025-06-16", "relevancy": 2.6678, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5989}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5033}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixture%20of%20Weight-shared%20Heterogeneous%20Group%20Attention%20Experts%20for%0A%20%20Dynamic%20Token-wise%20KV%20Optimization&body=Title%3A%20Mixture%20of%20Weight-shared%20Heterogeneous%20Group%20Attention%20Experts%20for%0A%20%20Dynamic%20Token-wise%20KV%20Optimization%0AAuthor%3A%20Guanghui%20Song%20and%20Dongping%20Liao%20and%20Yiren%20Zhao%20and%20Kejiang%20Ye%20and%20Cheng-zhong%20Xu%20and%20Xitong%20Gao%0AAbstract%3A%20%20%20Transformer%20models%20face%20scalability%20challenges%20in%20causal%20language%20modeling%0A%28CLM%29%20due%20to%20inefficient%20memory%20allocation%20for%20growing%20key-value%20%28KV%29%20caches%2C%0Awhich%20strains%20compute%20and%20storage%20resources.%20Existing%20methods%20like%20Grouped%0AQuery%20Attention%20%28GQA%29%20and%20token-level%20KV%20optimization%20improve%20efficiency%20but%0Arely%20on%20rigid%20resource%20allocation%2C%20often%20discarding%20%22low-priority%22%20tokens%20or%0Astatically%20grouping%20them%2C%20failing%20to%20address%20the%20dynamic%20spectrum%20of%20token%0Aimportance.%20We%20propose%20mixSGA%2C%20a%20novel%20mixture-of-expert%20%28MoE%29%20approach%20that%0Adynamically%20optimizes%20token-wise%20computation%20and%20memory%20allocation.%20Unlike%0Aprior%20approaches%2C%20mixSGA%20retains%20all%20tokens%20while%20adaptively%20routing%20them%20to%0Aspecialized%20experts%20with%20varying%20KV%20group%20sizes%2C%20balancing%20granularity%20and%0Aefficiency.%20Our%20key%20novelties%20include%3A%20%281%29%20a%20token-wise%20expert-choice%20routing%0Amechanism%20guided%20by%20learned%20importance%20scores%2C%20enabling%20proportional%20resource%0Aallocation%20without%20token%20discard%3B%20%282%29%20weight-sharing%20across%20grouped%20attention%0Aprojections%20to%20minimize%20parameter%20overhead%3B%20and%20%283%29%20an%20auxiliary%20loss%20to%20ensure%0Aone-hot%20routing%20decisions%20for%20training-inference%20consistency%20in%20CLMs.%20Extensive%0Aevaluations%20across%20Llama3%2C%20TinyLlama%2C%20OPT%2C%20and%20Gemma2%20model%20families%20show%0AmixSGA%27s%20superiority%20over%20static%20baselines.%20On%20instruction-following%20and%0Acontinued%20pretraining%20tasks%2C%20mixSGA%20achieves%20higher%20ROUGE-L%20and%20lower%0Aperplexity%20under%20the%20same%20KV%20budgets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixture%2520of%2520Weight-shared%2520Heterogeneous%2520Group%2520Attention%2520Experts%2520for%250A%2520%2520Dynamic%2520Token-wise%2520KV%2520Optimization%26entry.906535625%3DGuanghui%2520Song%2520and%2520Dongping%2520Liao%2520and%2520Yiren%2520Zhao%2520and%2520Kejiang%2520Ye%2520and%2520Cheng-zhong%2520Xu%2520and%2520Xitong%2520Gao%26entry.1292438233%3D%2520%2520Transformer%2520models%2520face%2520scalability%2520challenges%2520in%2520causal%2520language%2520modeling%250A%2528CLM%2529%2520due%2520to%2520inefficient%2520memory%2520allocation%2520for%2520growing%2520key-value%2520%2528KV%2529%2520caches%252C%250Awhich%2520strains%2520compute%2520and%2520storage%2520resources.%2520Existing%2520methods%2520like%2520Grouped%250AQuery%2520Attention%2520%2528GQA%2529%2520and%2520token-level%2520KV%2520optimization%2520improve%2520efficiency%2520but%250Arely%2520on%2520rigid%2520resource%2520allocation%252C%2520often%2520discarding%2520%2522low-priority%2522%2520tokens%2520or%250Astatically%2520grouping%2520them%252C%2520failing%2520to%2520address%2520the%2520dynamic%2520spectrum%2520of%2520token%250Aimportance.%2520We%2520propose%2520mixSGA%252C%2520a%2520novel%2520mixture-of-expert%2520%2528MoE%2529%2520approach%2520that%250Adynamically%2520optimizes%2520token-wise%2520computation%2520and%2520memory%2520allocation.%2520Unlike%250Aprior%2520approaches%252C%2520mixSGA%2520retains%2520all%2520tokens%2520while%2520adaptively%2520routing%2520them%2520to%250Aspecialized%2520experts%2520with%2520varying%2520KV%2520group%2520sizes%252C%2520balancing%2520granularity%2520and%250Aefficiency.%2520Our%2520key%2520novelties%2520include%253A%2520%25281%2529%2520a%2520token-wise%2520expert-choice%2520routing%250Amechanism%2520guided%2520by%2520learned%2520importance%2520scores%252C%2520enabling%2520proportional%2520resource%250Aallocation%2520without%2520token%2520discard%253B%2520%25282%2529%2520weight-sharing%2520across%2520grouped%2520attention%250Aprojections%2520to%2520minimize%2520parameter%2520overhead%253B%2520and%2520%25283%2529%2520an%2520auxiliary%2520loss%2520to%2520ensure%250Aone-hot%2520routing%2520decisions%2520for%2520training-inference%2520consistency%2520in%2520CLMs.%2520Extensive%250Aevaluations%2520across%2520Llama3%252C%2520TinyLlama%252C%2520OPT%252C%2520and%2520Gemma2%2520model%2520families%2520show%250AmixSGA%2527s%2520superiority%2520over%2520static%2520baselines.%2520On%2520instruction-following%2520and%250Acontinued%2520pretraining%2520tasks%252C%2520mixSGA%2520achieves%2520higher%2520ROUGE-L%2520and%2520lower%250Aperplexity%2520under%2520the%2520same%2520KV%2520budgets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixture%20of%20Weight-shared%20Heterogeneous%20Group%20Attention%20Experts%20for%0A%20%20Dynamic%20Token-wise%20KV%20Optimization&entry.906535625=Guanghui%20Song%20and%20Dongping%20Liao%20and%20Yiren%20Zhao%20and%20Kejiang%20Ye%20and%20Cheng-zhong%20Xu%20and%20Xitong%20Gao&entry.1292438233=%20%20Transformer%20models%20face%20scalability%20challenges%20in%20causal%20language%20modeling%0A%28CLM%29%20due%20to%20inefficient%20memory%20allocation%20for%20growing%20key-value%20%28KV%29%20caches%2C%0Awhich%20strains%20compute%20and%20storage%20resources.%20Existing%20methods%20like%20Grouped%0AQuery%20Attention%20%28GQA%29%20and%20token-level%20KV%20optimization%20improve%20efficiency%20but%0Arely%20on%20rigid%20resource%20allocation%2C%20often%20discarding%20%22low-priority%22%20tokens%20or%0Astatically%20grouping%20them%2C%20failing%20to%20address%20the%20dynamic%20spectrum%20of%20token%0Aimportance.%20We%20propose%20mixSGA%2C%20a%20novel%20mixture-of-expert%20%28MoE%29%20approach%20that%0Adynamically%20optimizes%20token-wise%20computation%20and%20memory%20allocation.%20Unlike%0Aprior%20approaches%2C%20mixSGA%20retains%20all%20tokens%20while%20adaptively%20routing%20them%20to%0Aspecialized%20experts%20with%20varying%20KV%20group%20sizes%2C%20balancing%20granularity%20and%0Aefficiency.%20Our%20key%20novelties%20include%3A%20%281%29%20a%20token-wise%20expert-choice%20routing%0Amechanism%20guided%20by%20learned%20importance%20scores%2C%20enabling%20proportional%20resource%0Aallocation%20without%20token%20discard%3B%20%282%29%20weight-sharing%20across%20grouped%20attention%0Aprojections%20to%20minimize%20parameter%20overhead%3B%20and%20%283%29%20an%20auxiliary%20loss%20to%20ensure%0Aone-hot%20routing%20decisions%20for%20training-inference%20consistency%20in%20CLMs.%20Extensive%0Aevaluations%20across%20Llama3%2C%20TinyLlama%2C%20OPT%2C%20and%20Gemma2%20model%20families%20show%0AmixSGA%27s%20superiority%20over%20static%20baselines.%20On%20instruction-following%20and%0Acontinued%20pretraining%20tasks%2C%20mixSGA%20achieves%20higher%20ROUGE-L%20and%20lower%0Aperplexity%20under%20the%20same%20KV%20budgets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13541v1&entry.124074799=Read"},
{"title": "Lecture Video Visual Objects (LVVO) Dataset: A Benchmark for Visual\n  Object Detection in Educational Videos", "author": "Dipayan Biswas and Shishir Shah and Jaspal Subhlok", "abstract": "  We introduce the Lecture Video Visual Objects (LVVO) dataset, a new benchmark\nfor visual object detection in educational video content. The dataset consists\nof 4,000 frames extracted from 245 lecture videos spanning biology, computer\nscience, and geosciences. A subset of 1,000 frames, referred to as LVVO_1k, has\nbeen manually annotated with bounding boxes for four visual categories: Table,\nChart-Graph, Photographic-image, and Visual-illustration. Each frame was\nlabeled independently by two annotators, resulting in an inter-annotator F1\nscore of 83.41%, indicating strong agreement. To ensure high-quality consensus\nannotations, a third expert reviewed and resolved all cases of disagreement\nthrough a conflict resolution process. To expand the dataset, a semi-supervised\napproach was employed to automatically annotate the remaining 3,000 frames,\nforming LVVO_3k. The complete dataset offers a valuable resource for developing\nand evaluating both supervised and semi-supervised methods for visual content\ndetection in educational videos. The LVVO dataset is publicly available to\nsupport further research in this domain.\n", "link": "http://arxiv.org/abs/2506.13657v1", "date": "2025-06-16", "relevancy": 2.6457, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lecture%20Video%20Visual%20Objects%20%28LVVO%29%20Dataset%3A%20A%20Benchmark%20for%20Visual%0A%20%20Object%20Detection%20in%20Educational%20Videos&body=Title%3A%20Lecture%20Video%20Visual%20Objects%20%28LVVO%29%20Dataset%3A%20A%20Benchmark%20for%20Visual%0A%20%20Object%20Detection%20in%20Educational%20Videos%0AAuthor%3A%20Dipayan%20Biswas%20and%20Shishir%20Shah%20and%20Jaspal%20Subhlok%0AAbstract%3A%20%20%20We%20introduce%20the%20Lecture%20Video%20Visual%20Objects%20%28LVVO%29%20dataset%2C%20a%20new%20benchmark%0Afor%20visual%20object%20detection%20in%20educational%20video%20content.%20The%20dataset%20consists%0Aof%204%2C000%20frames%20extracted%20from%20245%20lecture%20videos%20spanning%20biology%2C%20computer%0Ascience%2C%20and%20geosciences.%20A%20subset%20of%201%2C000%20frames%2C%20referred%20to%20as%20LVVO_1k%2C%20has%0Abeen%20manually%20annotated%20with%20bounding%20boxes%20for%20four%20visual%20categories%3A%20Table%2C%0AChart-Graph%2C%20Photographic-image%2C%20and%20Visual-illustration.%20Each%20frame%20was%0Alabeled%20independently%20by%20two%20annotators%2C%20resulting%20in%20an%20inter-annotator%20F1%0Ascore%20of%2083.41%25%2C%20indicating%20strong%20agreement.%20To%20ensure%20high-quality%20consensus%0Aannotations%2C%20a%20third%20expert%20reviewed%20and%20resolved%20all%20cases%20of%20disagreement%0Athrough%20a%20conflict%20resolution%20process.%20To%20expand%20the%20dataset%2C%20a%20semi-supervised%0Aapproach%20was%20employed%20to%20automatically%20annotate%20the%20remaining%203%2C000%20frames%2C%0Aforming%20LVVO_3k.%20The%20complete%20dataset%20offers%20a%20valuable%20resource%20for%20developing%0Aand%20evaluating%20both%20supervised%20and%20semi-supervised%20methods%20for%20visual%20content%0Adetection%20in%20educational%20videos.%20The%20LVVO%20dataset%20is%20publicly%20available%20to%0Asupport%20further%20research%20in%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLecture%2520Video%2520Visual%2520Objects%2520%2528LVVO%2529%2520Dataset%253A%2520A%2520Benchmark%2520for%2520Visual%250A%2520%2520Object%2520Detection%2520in%2520Educational%2520Videos%26entry.906535625%3DDipayan%2520Biswas%2520and%2520Shishir%2520Shah%2520and%2520Jaspal%2520Subhlok%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Lecture%2520Video%2520Visual%2520Objects%2520%2528LVVO%2529%2520dataset%252C%2520a%2520new%2520benchmark%250Afor%2520visual%2520object%2520detection%2520in%2520educational%2520video%2520content.%2520The%2520dataset%2520consists%250Aof%25204%252C000%2520frames%2520extracted%2520from%2520245%2520lecture%2520videos%2520spanning%2520biology%252C%2520computer%250Ascience%252C%2520and%2520geosciences.%2520A%2520subset%2520of%25201%252C000%2520frames%252C%2520referred%2520to%2520as%2520LVVO_1k%252C%2520has%250Abeen%2520manually%2520annotated%2520with%2520bounding%2520boxes%2520for%2520four%2520visual%2520categories%253A%2520Table%252C%250AChart-Graph%252C%2520Photographic-image%252C%2520and%2520Visual-illustration.%2520Each%2520frame%2520was%250Alabeled%2520independently%2520by%2520two%2520annotators%252C%2520resulting%2520in%2520an%2520inter-annotator%2520F1%250Ascore%2520of%252083.41%2525%252C%2520indicating%2520strong%2520agreement.%2520To%2520ensure%2520high-quality%2520consensus%250Aannotations%252C%2520a%2520third%2520expert%2520reviewed%2520and%2520resolved%2520all%2520cases%2520of%2520disagreement%250Athrough%2520a%2520conflict%2520resolution%2520process.%2520To%2520expand%2520the%2520dataset%252C%2520a%2520semi-supervised%250Aapproach%2520was%2520employed%2520to%2520automatically%2520annotate%2520the%2520remaining%25203%252C000%2520frames%252C%250Aforming%2520LVVO_3k.%2520The%2520complete%2520dataset%2520offers%2520a%2520valuable%2520resource%2520for%2520developing%250Aand%2520evaluating%2520both%2520supervised%2520and%2520semi-supervised%2520methods%2520for%2520visual%2520content%250Adetection%2520in%2520educational%2520videos.%2520The%2520LVVO%2520dataset%2520is%2520publicly%2520available%2520to%250Asupport%2520further%2520research%2520in%2520this%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lecture%20Video%20Visual%20Objects%20%28LVVO%29%20Dataset%3A%20A%20Benchmark%20for%20Visual%0A%20%20Object%20Detection%20in%20Educational%20Videos&entry.906535625=Dipayan%20Biswas%20and%20Shishir%20Shah%20and%20Jaspal%20Subhlok&entry.1292438233=%20%20We%20introduce%20the%20Lecture%20Video%20Visual%20Objects%20%28LVVO%29%20dataset%2C%20a%20new%20benchmark%0Afor%20visual%20object%20detection%20in%20educational%20video%20content.%20The%20dataset%20consists%0Aof%204%2C000%20frames%20extracted%20from%20245%20lecture%20videos%20spanning%20biology%2C%20computer%0Ascience%2C%20and%20geosciences.%20A%20subset%20of%201%2C000%20frames%2C%20referred%20to%20as%20LVVO_1k%2C%20has%0Abeen%20manually%20annotated%20with%20bounding%20boxes%20for%20four%20visual%20categories%3A%20Table%2C%0AChart-Graph%2C%20Photographic-image%2C%20and%20Visual-illustration.%20Each%20frame%20was%0Alabeled%20independently%20by%20two%20annotators%2C%20resulting%20in%20an%20inter-annotator%20F1%0Ascore%20of%2083.41%25%2C%20indicating%20strong%20agreement.%20To%20ensure%20high-quality%20consensus%0Aannotations%2C%20a%20third%20expert%20reviewed%20and%20resolved%20all%20cases%20of%20disagreement%0Athrough%20a%20conflict%20resolution%20process.%20To%20expand%20the%20dataset%2C%20a%20semi-supervised%0Aapproach%20was%20employed%20to%20automatically%20annotate%20the%20remaining%203%2C000%20frames%2C%0Aforming%20LVVO_3k.%20The%20complete%20dataset%20offers%20a%20valuable%20resource%20for%20developing%0Aand%20evaluating%20both%20supervised%20and%20semi-supervised%20methods%20for%20visual%20content%0Adetection%20in%20educational%20videos.%20The%20LVVO%20dataset%20is%20publicly%20available%20to%0Asupport%20further%20research%20in%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13657v1&entry.124074799=Read"},
{"title": "VIS-Shepherd: Constructing Critic for LLM-based Data Visualization\n  Generation", "author": "Bo Pan and Yixiao Fu and Ke Wang and Junyu Lu and Lunke Pan and Ziyang Qian and Yuhan Chen and Guoliang Wang and Yitao Zhou and Li Zheng and Yinghao Tang and Zhen Wen and Yuchen Wu and Junhua Lu and Biao Zhu and Minfeng Zhu and Bo Zhang and Wei Chen", "abstract": "  Data visualization generation using Large Language Models (LLMs) has shown\npromising results but often produces suboptimal visualizations that require\nhuman intervention for improvement. In this work, we introduce VIS-Shepherd, a\nspecialized Multimodal Large Language Model (MLLM)-based critic to evaluate and\nprovide feedback for LLM-generated data visualizations. At the core of our\napproach is a framework to construct a high-quality visualization critique\ndataset, where we collect human-created visualization instances, synthesize\ncorresponding LLM-generated instances, and construct high-quality critiques. We\nconduct both model-based automatic evaluation and human preference studies to\nevaluate the effectiveness of our approach. Our experiments show that even\nsmall (7B parameters) open-source MLLM models achieve substantial performance\ngains by leveraging our high-quality visualization critique dataset, reaching\nlevels comparable to much larger open-source or even proprietary models. Our\nwork demonstrates significant potential for MLLM-based automated visualization\ncritique and indicates promising directions for enhancing LLM-based data\nvisualization generation. Our project page:\nhttps://github.com/bopan3/VIS-Shepherd.\n", "link": "http://arxiv.org/abs/2506.13326v1", "date": "2025-06-16", "relevancy": 2.6376, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5286}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.527}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIS-Shepherd%3A%20Constructing%20Critic%20for%20LLM-based%20Data%20Visualization%0A%20%20Generation&body=Title%3A%20VIS-Shepherd%3A%20Constructing%20Critic%20for%20LLM-based%20Data%20Visualization%0A%20%20Generation%0AAuthor%3A%20Bo%20Pan%20and%20Yixiao%20Fu%20and%20Ke%20Wang%20and%20Junyu%20Lu%20and%20Lunke%20Pan%20and%20Ziyang%20Qian%20and%20Yuhan%20Chen%20and%20Guoliang%20Wang%20and%20Yitao%20Zhou%20and%20Li%20Zheng%20and%20Yinghao%20Tang%20and%20Zhen%20Wen%20and%20Yuchen%20Wu%20and%20Junhua%20Lu%20and%20Biao%20Zhu%20and%20Minfeng%20Zhu%20and%20Bo%20Zhang%20and%20Wei%20Chen%0AAbstract%3A%20%20%20Data%20visualization%20generation%20using%20Large%20Language%20Models%20%28LLMs%29%20has%20shown%0Apromising%20results%20but%20often%20produces%20suboptimal%20visualizations%20that%20require%0Ahuman%20intervention%20for%20improvement.%20In%20this%20work%2C%20we%20introduce%20VIS-Shepherd%2C%20a%0Aspecialized%20Multimodal%20Large%20Language%20Model%20%28MLLM%29-based%20critic%20to%20evaluate%20and%0Aprovide%20feedback%20for%20LLM-generated%20data%20visualizations.%20At%20the%20core%20of%20our%0Aapproach%20is%20a%20framework%20to%20construct%20a%20high-quality%20visualization%20critique%0Adataset%2C%20where%20we%20collect%20human-created%20visualization%20instances%2C%20synthesize%0Acorresponding%20LLM-generated%20instances%2C%20and%20construct%20high-quality%20critiques.%20We%0Aconduct%20both%20model-based%20automatic%20evaluation%20and%20human%20preference%20studies%20to%0Aevaluate%20the%20effectiveness%20of%20our%20approach.%20Our%20experiments%20show%20that%20even%0Asmall%20%287B%20parameters%29%20open-source%20MLLM%20models%20achieve%20substantial%20performance%0Agains%20by%20leveraging%20our%20high-quality%20visualization%20critique%20dataset%2C%20reaching%0Alevels%20comparable%20to%20much%20larger%20open-source%20or%20even%20proprietary%20models.%20Our%0Awork%20demonstrates%20significant%20potential%20for%20MLLM-based%20automated%20visualization%0Acritique%20and%20indicates%20promising%20directions%20for%20enhancing%20LLM-based%20data%0Avisualization%20generation.%20Our%20project%20page%3A%0Ahttps%3A//github.com/bopan3/VIS-Shepherd.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13326v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIS-Shepherd%253A%2520Constructing%2520Critic%2520for%2520LLM-based%2520Data%2520Visualization%250A%2520%2520Generation%26entry.906535625%3DBo%2520Pan%2520and%2520Yixiao%2520Fu%2520and%2520Ke%2520Wang%2520and%2520Junyu%2520Lu%2520and%2520Lunke%2520Pan%2520and%2520Ziyang%2520Qian%2520and%2520Yuhan%2520Chen%2520and%2520Guoliang%2520Wang%2520and%2520Yitao%2520Zhou%2520and%2520Li%2520Zheng%2520and%2520Yinghao%2520Tang%2520and%2520Zhen%2520Wen%2520and%2520Yuchen%2520Wu%2520and%2520Junhua%2520Lu%2520and%2520Biao%2520Zhu%2520and%2520Minfeng%2520Zhu%2520and%2520Bo%2520Zhang%2520and%2520Wei%2520Chen%26entry.1292438233%3D%2520%2520Data%2520visualization%2520generation%2520using%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520shown%250Apromising%2520results%2520but%2520often%2520produces%2520suboptimal%2520visualizations%2520that%2520require%250Ahuman%2520intervention%2520for%2520improvement.%2520In%2520this%2520work%252C%2520we%2520introduce%2520VIS-Shepherd%252C%2520a%250Aspecialized%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529-based%2520critic%2520to%2520evaluate%2520and%250Aprovide%2520feedback%2520for%2520LLM-generated%2520data%2520visualizations.%2520At%2520the%2520core%2520of%2520our%250Aapproach%2520is%2520a%2520framework%2520to%2520construct%2520a%2520high-quality%2520visualization%2520critique%250Adataset%252C%2520where%2520we%2520collect%2520human-created%2520visualization%2520instances%252C%2520synthesize%250Acorresponding%2520LLM-generated%2520instances%252C%2520and%2520construct%2520high-quality%2520critiques.%2520We%250Aconduct%2520both%2520model-based%2520automatic%2520evaluation%2520and%2520human%2520preference%2520studies%2520to%250Aevaluate%2520the%2520effectiveness%2520of%2520our%2520approach.%2520Our%2520experiments%2520show%2520that%2520even%250Asmall%2520%25287B%2520parameters%2529%2520open-source%2520MLLM%2520models%2520achieve%2520substantial%2520performance%250Agains%2520by%2520leveraging%2520our%2520high-quality%2520visualization%2520critique%2520dataset%252C%2520reaching%250Alevels%2520comparable%2520to%2520much%2520larger%2520open-source%2520or%2520even%2520proprietary%2520models.%2520Our%250Awork%2520demonstrates%2520significant%2520potential%2520for%2520MLLM-based%2520automated%2520visualization%250Acritique%2520and%2520indicates%2520promising%2520directions%2520for%2520enhancing%2520LLM-based%2520data%250Avisualization%2520generation.%2520Our%2520project%2520page%253A%250Ahttps%253A//github.com/bopan3/VIS-Shepherd.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13326v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIS-Shepherd%3A%20Constructing%20Critic%20for%20LLM-based%20Data%20Visualization%0A%20%20Generation&entry.906535625=Bo%20Pan%20and%20Yixiao%20Fu%20and%20Ke%20Wang%20and%20Junyu%20Lu%20and%20Lunke%20Pan%20and%20Ziyang%20Qian%20and%20Yuhan%20Chen%20and%20Guoliang%20Wang%20and%20Yitao%20Zhou%20and%20Li%20Zheng%20and%20Yinghao%20Tang%20and%20Zhen%20Wen%20and%20Yuchen%20Wu%20and%20Junhua%20Lu%20and%20Biao%20Zhu%20and%20Minfeng%20Zhu%20and%20Bo%20Zhang%20and%20Wei%20Chen&entry.1292438233=%20%20Data%20visualization%20generation%20using%20Large%20Language%20Models%20%28LLMs%29%20has%20shown%0Apromising%20results%20but%20often%20produces%20suboptimal%20visualizations%20that%20require%0Ahuman%20intervention%20for%20improvement.%20In%20this%20work%2C%20we%20introduce%20VIS-Shepherd%2C%20a%0Aspecialized%20Multimodal%20Large%20Language%20Model%20%28MLLM%29-based%20critic%20to%20evaluate%20and%0Aprovide%20feedback%20for%20LLM-generated%20data%20visualizations.%20At%20the%20core%20of%20our%0Aapproach%20is%20a%20framework%20to%20construct%20a%20high-quality%20visualization%20critique%0Adataset%2C%20where%20we%20collect%20human-created%20visualization%20instances%2C%20synthesize%0Acorresponding%20LLM-generated%20instances%2C%20and%20construct%20high-quality%20critiques.%20We%0Aconduct%20both%20model-based%20automatic%20evaluation%20and%20human%20preference%20studies%20to%0Aevaluate%20the%20effectiveness%20of%20our%20approach.%20Our%20experiments%20show%20that%20even%0Asmall%20%287B%20parameters%29%20open-source%20MLLM%20models%20achieve%20substantial%20performance%0Agains%20by%20leveraging%20our%20high-quality%20visualization%20critique%20dataset%2C%20reaching%0Alevels%20comparable%20to%20much%20larger%20open-source%20or%20even%20proprietary%20models.%20Our%0Awork%20demonstrates%20significant%20potential%20for%20MLLM-based%20automated%20visualization%0Acritique%20and%20indicates%20promising%20directions%20for%20enhancing%20LLM-based%20data%0Avisualization%20generation.%20Our%20project%20page%3A%0Ahttps%3A//github.com/bopan3/VIS-Shepherd.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13326v1&entry.124074799=Read"},
{"title": "Training Neural Networks by Optimizing Neuron Positions", "author": "Laura Erb and Tommaso Boccato and Alexandru Vasilache and Juergen Becker and Nicola Toschi", "abstract": "  The high computational complexity and increasing parameter counts of deep\nneural networks pose significant challenges for deployment in\nresource-constrained environments, such as edge devices or real-time systems.\nTo address this, we propose a parameter-efficient neural architecture where\nneurons are embedded in Euclidean space. During training, their positions are\noptimized and synaptic weights are determined as the inverse of the spatial\ndistance between connected neurons. These distance-dependent wiring rules\nreplace traditional learnable weight matrices and significantly reduce the\nnumber of parameters while introducing a biologically inspired inductive bias:\nconnection strength decreases with spatial distance, reflecting the brain's\nembedding in three-dimensional space where connections tend to minimize wiring\nlength. We validate this approach for both multi-layer perceptrons and spiking\nneural networks. Through a series of experiments, we demonstrate that these\nspatially embedded neural networks achieve a performance competitive with\nconventional architectures on the MNIST dataset. Additionally, the models\nmaintain performance even at pruning rates exceeding 80% sparsity,\noutperforming traditional networks with the same number of parameters under\nsimilar conditions. Finally, the spatial embedding framework offers an\nintuitive visualization of the network structure.\n", "link": "http://arxiv.org/abs/2506.13410v1", "date": "2025-06-16", "relevancy": 2.6128, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5458}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.515}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Neural%20Networks%20by%20Optimizing%20Neuron%20Positions&body=Title%3A%20Training%20Neural%20Networks%20by%20Optimizing%20Neuron%20Positions%0AAuthor%3A%20Laura%20Erb%20and%20Tommaso%20Boccato%20and%20Alexandru%20Vasilache%20and%20Juergen%20Becker%20and%20Nicola%20Toschi%0AAbstract%3A%20%20%20The%20high%20computational%20complexity%20and%20increasing%20parameter%20counts%20of%20deep%0Aneural%20networks%20pose%20significant%20challenges%20for%20deployment%20in%0Aresource-constrained%20environments%2C%20such%20as%20edge%20devices%20or%20real-time%20systems.%0ATo%20address%20this%2C%20we%20propose%20a%20parameter-efficient%20neural%20architecture%20where%0Aneurons%20are%20embedded%20in%20Euclidean%20space.%20During%20training%2C%20their%20positions%20are%0Aoptimized%20and%20synaptic%20weights%20are%20determined%20as%20the%20inverse%20of%20the%20spatial%0Adistance%20between%20connected%20neurons.%20These%20distance-dependent%20wiring%20rules%0Areplace%20traditional%20learnable%20weight%20matrices%20and%20significantly%20reduce%20the%0Anumber%20of%20parameters%20while%20introducing%20a%20biologically%20inspired%20inductive%20bias%3A%0Aconnection%20strength%20decreases%20with%20spatial%20distance%2C%20reflecting%20the%20brain%27s%0Aembedding%20in%20three-dimensional%20space%20where%20connections%20tend%20to%20minimize%20wiring%0Alength.%20We%20validate%20this%20approach%20for%20both%20multi-layer%20perceptrons%20and%20spiking%0Aneural%20networks.%20Through%20a%20series%20of%20experiments%2C%20we%20demonstrate%20that%20these%0Aspatially%20embedded%20neural%20networks%20achieve%20a%20performance%20competitive%20with%0Aconventional%20architectures%20on%20the%20MNIST%20dataset.%20Additionally%2C%20the%20models%0Amaintain%20performance%20even%20at%20pruning%20rates%20exceeding%2080%25%20sparsity%2C%0Aoutperforming%20traditional%20networks%20with%20the%20same%20number%20of%20parameters%20under%0Asimilar%20conditions.%20Finally%2C%20the%20spatial%20embedding%20framework%20offers%20an%0Aintuitive%20visualization%20of%20the%20network%20structure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13410v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Neural%2520Networks%2520by%2520Optimizing%2520Neuron%2520Positions%26entry.906535625%3DLaura%2520Erb%2520and%2520Tommaso%2520Boccato%2520and%2520Alexandru%2520Vasilache%2520and%2520Juergen%2520Becker%2520and%2520Nicola%2520Toschi%26entry.1292438233%3D%2520%2520The%2520high%2520computational%2520complexity%2520and%2520increasing%2520parameter%2520counts%2520of%2520deep%250Aneural%2520networks%2520pose%2520significant%2520challenges%2520for%2520deployment%2520in%250Aresource-constrained%2520environments%252C%2520such%2520as%2520edge%2520devices%2520or%2520real-time%2520systems.%250ATo%2520address%2520this%252C%2520we%2520propose%2520a%2520parameter-efficient%2520neural%2520architecture%2520where%250Aneurons%2520are%2520embedded%2520in%2520Euclidean%2520space.%2520During%2520training%252C%2520their%2520positions%2520are%250Aoptimized%2520and%2520synaptic%2520weights%2520are%2520determined%2520as%2520the%2520inverse%2520of%2520the%2520spatial%250Adistance%2520between%2520connected%2520neurons.%2520These%2520distance-dependent%2520wiring%2520rules%250Areplace%2520traditional%2520learnable%2520weight%2520matrices%2520and%2520significantly%2520reduce%2520the%250Anumber%2520of%2520parameters%2520while%2520introducing%2520a%2520biologically%2520inspired%2520inductive%2520bias%253A%250Aconnection%2520strength%2520decreases%2520with%2520spatial%2520distance%252C%2520reflecting%2520the%2520brain%2527s%250Aembedding%2520in%2520three-dimensional%2520space%2520where%2520connections%2520tend%2520to%2520minimize%2520wiring%250Alength.%2520We%2520validate%2520this%2520approach%2520for%2520both%2520multi-layer%2520perceptrons%2520and%2520spiking%250Aneural%2520networks.%2520Through%2520a%2520series%2520of%2520experiments%252C%2520we%2520demonstrate%2520that%2520these%250Aspatially%2520embedded%2520neural%2520networks%2520achieve%2520a%2520performance%2520competitive%2520with%250Aconventional%2520architectures%2520on%2520the%2520MNIST%2520dataset.%2520Additionally%252C%2520the%2520models%250Amaintain%2520performance%2520even%2520at%2520pruning%2520rates%2520exceeding%252080%2525%2520sparsity%252C%250Aoutperforming%2520traditional%2520networks%2520with%2520the%2520same%2520number%2520of%2520parameters%2520under%250Asimilar%2520conditions.%2520Finally%252C%2520the%2520spatial%2520embedding%2520framework%2520offers%2520an%250Aintuitive%2520visualization%2520of%2520the%2520network%2520structure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13410v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Neural%20Networks%20by%20Optimizing%20Neuron%20Positions&entry.906535625=Laura%20Erb%20and%20Tommaso%20Boccato%20and%20Alexandru%20Vasilache%20and%20Juergen%20Becker%20and%20Nicola%20Toschi&entry.1292438233=%20%20The%20high%20computational%20complexity%20and%20increasing%20parameter%20counts%20of%20deep%0Aneural%20networks%20pose%20significant%20challenges%20for%20deployment%20in%0Aresource-constrained%20environments%2C%20such%20as%20edge%20devices%20or%20real-time%20systems.%0ATo%20address%20this%2C%20we%20propose%20a%20parameter-efficient%20neural%20architecture%20where%0Aneurons%20are%20embedded%20in%20Euclidean%20space.%20During%20training%2C%20their%20positions%20are%0Aoptimized%20and%20synaptic%20weights%20are%20determined%20as%20the%20inverse%20of%20the%20spatial%0Adistance%20between%20connected%20neurons.%20These%20distance-dependent%20wiring%20rules%0Areplace%20traditional%20learnable%20weight%20matrices%20and%20significantly%20reduce%20the%0Anumber%20of%20parameters%20while%20introducing%20a%20biologically%20inspired%20inductive%20bias%3A%0Aconnection%20strength%20decreases%20with%20spatial%20distance%2C%20reflecting%20the%20brain%27s%0Aembedding%20in%20three-dimensional%20space%20where%20connections%20tend%20to%20minimize%20wiring%0Alength.%20We%20validate%20this%20approach%20for%20both%20multi-layer%20perceptrons%20and%20spiking%0Aneural%20networks.%20Through%20a%20series%20of%20experiments%2C%20we%20demonstrate%20that%20these%0Aspatially%20embedded%20neural%20networks%20achieve%20a%20performance%20competitive%20with%0Aconventional%20architectures%20on%20the%20MNIST%20dataset.%20Additionally%2C%20the%20models%0Amaintain%20performance%20even%20at%20pruning%20rates%20exceeding%2080%25%20sparsity%2C%0Aoutperforming%20traditional%20networks%20with%20the%20same%20number%20of%20parameters%20under%0Asimilar%20conditions.%20Finally%2C%20the%20spatial%20embedding%20framework%20offers%20an%0Aintuitive%20visualization%20of%20the%20network%20structure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13410v1&entry.124074799=Read"},
{"title": "Uncertainty-Informed Active Perception for Open Vocabulary Object Goal\n  Navigation", "author": "Utkarsh Bajpai and Julius R\u00fcckin and Cyrill Stachniss and Marija Popovi\u0107", "abstract": "  Mobile robots exploring indoor environments increasingly rely on\nvision-language models to perceive high-level semantic cues in camera images,\nsuch as object categories. Such models offer the potential to substantially\nadvance robot behaviour for tasks such as object-goal navigation (ObjectNav),\nwhere the robot must locate objects specified in natural language by exploring\nthe environment. Current ObjectNav methods heavily depend on prompt engineering\nfor perception and do not address the semantic uncertainty induced by\nvariations in prompt phrasing. Ignoring semantic uncertainty can lead to\nsuboptimal exploration, which in turn limits performance. Hence, we propose a\nsemantic uncertainty-informed active perception pipeline for ObjectNav in\nindoor environments. We introduce a novel probabilistic sensor model for\nquantifying semantic uncertainty in vision-language models and incorporate it\ninto a probabilistic geometric-semantic map to enhance spatial understanding.\nBased on this map, we develop a frontier exploration planner with an\nuncertainty-informed multi-armed bandit objective to guide efficient object\nsearch. Experimental results demonstrate that our method achieves ObjectNav\nsuccess rates comparable to those of state-of-the-art approaches, without\nrequiring extensive prompt engineering.\n", "link": "http://arxiv.org/abs/2506.13367v1", "date": "2025-06-16", "relevancy": 2.6106, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6849}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6694}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-Informed%20Active%20Perception%20for%20Open%20Vocabulary%20Object%20Goal%0A%20%20Navigation&body=Title%3A%20Uncertainty-Informed%20Active%20Perception%20for%20Open%20Vocabulary%20Object%20Goal%0A%20%20Navigation%0AAuthor%3A%20Utkarsh%20Bajpai%20and%20Julius%20R%C3%BCckin%20and%20Cyrill%20Stachniss%20and%20Marija%20Popovi%C4%87%0AAbstract%3A%20%20%20Mobile%20robots%20exploring%20indoor%20environments%20increasingly%20rely%20on%0Avision-language%20models%20to%20perceive%20high-level%20semantic%20cues%20in%20camera%20images%2C%0Asuch%20as%20object%20categories.%20Such%20models%20offer%20the%20potential%20to%20substantially%0Aadvance%20robot%20behaviour%20for%20tasks%20such%20as%20object-goal%20navigation%20%28ObjectNav%29%2C%0Awhere%20the%20robot%20must%20locate%20objects%20specified%20in%20natural%20language%20by%20exploring%0Athe%20environment.%20Current%20ObjectNav%20methods%20heavily%20depend%20on%20prompt%20engineering%0Afor%20perception%20and%20do%20not%20address%20the%20semantic%20uncertainty%20induced%20by%0Avariations%20in%20prompt%20phrasing.%20Ignoring%20semantic%20uncertainty%20can%20lead%20to%0Asuboptimal%20exploration%2C%20which%20in%20turn%20limits%20performance.%20Hence%2C%20we%20propose%20a%0Asemantic%20uncertainty-informed%20active%20perception%20pipeline%20for%20ObjectNav%20in%0Aindoor%20environments.%20We%20introduce%20a%20novel%20probabilistic%20sensor%20model%20for%0Aquantifying%20semantic%20uncertainty%20in%20vision-language%20models%20and%20incorporate%20it%0Ainto%20a%20probabilistic%20geometric-semantic%20map%20to%20enhance%20spatial%20understanding.%0ABased%20on%20this%20map%2C%20we%20develop%20a%20frontier%20exploration%20planner%20with%20an%0Auncertainty-informed%20multi-armed%20bandit%20objective%20to%20guide%20efficient%20object%0Asearch.%20Experimental%20results%20demonstrate%20that%20our%20method%20achieves%20ObjectNav%0Asuccess%20rates%20comparable%20to%20those%20of%20state-of-the-art%20approaches%2C%20without%0Arequiring%20extensive%20prompt%20engineering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-Informed%2520Active%2520Perception%2520for%2520Open%2520Vocabulary%2520Object%2520Goal%250A%2520%2520Navigation%26entry.906535625%3DUtkarsh%2520Bajpai%2520and%2520Julius%2520R%25C3%25BCckin%2520and%2520Cyrill%2520Stachniss%2520and%2520Marija%2520Popovi%25C4%2587%26entry.1292438233%3D%2520%2520Mobile%2520robots%2520exploring%2520indoor%2520environments%2520increasingly%2520rely%2520on%250Avision-language%2520models%2520to%2520perceive%2520high-level%2520semantic%2520cues%2520in%2520camera%2520images%252C%250Asuch%2520as%2520object%2520categories.%2520Such%2520models%2520offer%2520the%2520potential%2520to%2520substantially%250Aadvance%2520robot%2520behaviour%2520for%2520tasks%2520such%2520as%2520object-goal%2520navigation%2520%2528ObjectNav%2529%252C%250Awhere%2520the%2520robot%2520must%2520locate%2520objects%2520specified%2520in%2520natural%2520language%2520by%2520exploring%250Athe%2520environment.%2520Current%2520ObjectNav%2520methods%2520heavily%2520depend%2520on%2520prompt%2520engineering%250Afor%2520perception%2520and%2520do%2520not%2520address%2520the%2520semantic%2520uncertainty%2520induced%2520by%250Avariations%2520in%2520prompt%2520phrasing.%2520Ignoring%2520semantic%2520uncertainty%2520can%2520lead%2520to%250Asuboptimal%2520exploration%252C%2520which%2520in%2520turn%2520limits%2520performance.%2520Hence%252C%2520we%2520propose%2520a%250Asemantic%2520uncertainty-informed%2520active%2520perception%2520pipeline%2520for%2520ObjectNav%2520in%250Aindoor%2520environments.%2520We%2520introduce%2520a%2520novel%2520probabilistic%2520sensor%2520model%2520for%250Aquantifying%2520semantic%2520uncertainty%2520in%2520vision-language%2520models%2520and%2520incorporate%2520it%250Ainto%2520a%2520probabilistic%2520geometric-semantic%2520map%2520to%2520enhance%2520spatial%2520understanding.%250ABased%2520on%2520this%2520map%252C%2520we%2520develop%2520a%2520frontier%2520exploration%2520planner%2520with%2520an%250Auncertainty-informed%2520multi-armed%2520bandit%2520objective%2520to%2520guide%2520efficient%2520object%250Asearch.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520achieves%2520ObjectNav%250Asuccess%2520rates%2520comparable%2520to%2520those%2520of%2520state-of-the-art%2520approaches%252C%2520without%250Arequiring%2520extensive%2520prompt%2520engineering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-Informed%20Active%20Perception%20for%20Open%20Vocabulary%20Object%20Goal%0A%20%20Navigation&entry.906535625=Utkarsh%20Bajpai%20and%20Julius%20R%C3%BCckin%20and%20Cyrill%20Stachniss%20and%20Marija%20Popovi%C4%87&entry.1292438233=%20%20Mobile%20robots%20exploring%20indoor%20environments%20increasingly%20rely%20on%0Avision-language%20models%20to%20perceive%20high-level%20semantic%20cues%20in%20camera%20images%2C%0Asuch%20as%20object%20categories.%20Such%20models%20offer%20the%20potential%20to%20substantially%0Aadvance%20robot%20behaviour%20for%20tasks%20such%20as%20object-goal%20navigation%20%28ObjectNav%29%2C%0Awhere%20the%20robot%20must%20locate%20objects%20specified%20in%20natural%20language%20by%20exploring%0Athe%20environment.%20Current%20ObjectNav%20methods%20heavily%20depend%20on%20prompt%20engineering%0Afor%20perception%20and%20do%20not%20address%20the%20semantic%20uncertainty%20induced%20by%0Avariations%20in%20prompt%20phrasing.%20Ignoring%20semantic%20uncertainty%20can%20lead%20to%0Asuboptimal%20exploration%2C%20which%20in%20turn%20limits%20performance.%20Hence%2C%20we%20propose%20a%0Asemantic%20uncertainty-informed%20active%20perception%20pipeline%20for%20ObjectNav%20in%0Aindoor%20environments.%20We%20introduce%20a%20novel%20probabilistic%20sensor%20model%20for%0Aquantifying%20semantic%20uncertainty%20in%20vision-language%20models%20and%20incorporate%20it%0Ainto%20a%20probabilistic%20geometric-semantic%20map%20to%20enhance%20spatial%20understanding.%0ABased%20on%20this%20map%2C%20we%20develop%20a%20frontier%20exploration%20planner%20with%20an%0Auncertainty-informed%20multi-armed%20bandit%20objective%20to%20guide%20efficient%20object%0Asearch.%20Experimental%20results%20demonstrate%20that%20our%20method%20achieves%20ObjectNav%0Asuccess%20rates%20comparable%20to%20those%20of%20state-of-the-art%20approaches%2C%20without%0Arequiring%20extensive%20prompt%20engineering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13367v1&entry.124074799=Read"},
{"title": "Understand the Implication: Learning to Think for Pragmatic\n  Understanding", "author": "Settaluri Lakshmi Sravanthi and Kishan Maharaj and Sravani Gunnu and Abhijit Mishra and Pushpak Bhattacharyya", "abstract": "  Pragmatics, the ability to infer meaning beyond literal interpretation, is\ncrucial for social cognition and communication. While LLMs have been\nbenchmarked for their pragmatic understanding, improving their performance\nremains underexplored. Existing methods rely on annotated labels but overlook\nthe reasoning process humans naturally use to interpret implicit meaning. To\nbridge this gap, we introduce a novel pragmatic dataset,\nImpliedMeaningPreference, that includes explicit reasoning (thoughts) for both\ncorrect and incorrect interpretations. Through preference-tuning and supervised\nfine-tuning, we demonstrate that thought-based learning significantly enhances\nLLMs' pragmatic understanding, improving accuracy by 11.12% across model\nfamilies. We further discuss a transfer-learning study where we evaluate the\nperformance of thought-based training for the other tasks of pragmatics\n(presupposition, deixis) that are not seen during the training time and observe\nan improvement of 16.10% compared to label-trained models.\n", "link": "http://arxiv.org/abs/2506.13559v1", "date": "2025-06-16", "relevancy": 2.6101, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5362}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5362}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understand%20the%20Implication%3A%20Learning%20to%20Think%20for%20Pragmatic%0A%20%20Understanding&body=Title%3A%20Understand%20the%20Implication%3A%20Learning%20to%20Think%20for%20Pragmatic%0A%20%20Understanding%0AAuthor%3A%20Settaluri%20Lakshmi%20Sravanthi%20and%20Kishan%20Maharaj%20and%20Sravani%20Gunnu%20and%20Abhijit%20Mishra%20and%20Pushpak%20Bhattacharyya%0AAbstract%3A%20%20%20Pragmatics%2C%20the%20ability%20to%20infer%20meaning%20beyond%20literal%20interpretation%2C%20is%0Acrucial%20for%20social%20cognition%20and%20communication.%20While%20LLMs%20have%20been%0Abenchmarked%20for%20their%20pragmatic%20understanding%2C%20improving%20their%20performance%0Aremains%20underexplored.%20Existing%20methods%20rely%20on%20annotated%20labels%20but%20overlook%0Athe%20reasoning%20process%20humans%20naturally%20use%20to%20interpret%20implicit%20meaning.%20To%0Abridge%20this%20gap%2C%20we%20introduce%20a%20novel%20pragmatic%20dataset%2C%0AImpliedMeaningPreference%2C%20that%20includes%20explicit%20reasoning%20%28thoughts%29%20for%20both%0Acorrect%20and%20incorrect%20interpretations.%20Through%20preference-tuning%20and%20supervised%0Afine-tuning%2C%20we%20demonstrate%20that%20thought-based%20learning%20significantly%20enhances%0ALLMs%27%20pragmatic%20understanding%2C%20improving%20accuracy%20by%2011.12%25%20across%20model%0Afamilies.%20We%20further%20discuss%20a%20transfer-learning%20study%20where%20we%20evaluate%20the%0Aperformance%20of%20thought-based%20training%20for%20the%20other%20tasks%20of%20pragmatics%0A%28presupposition%2C%20deixis%29%20that%20are%20not%20seen%20during%20the%20training%20time%20and%20observe%0Aan%20improvement%20of%2016.10%25%20compared%20to%20label-trained%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13559v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstand%2520the%2520Implication%253A%2520Learning%2520to%2520Think%2520for%2520Pragmatic%250A%2520%2520Understanding%26entry.906535625%3DSettaluri%2520Lakshmi%2520Sravanthi%2520and%2520Kishan%2520Maharaj%2520and%2520Sravani%2520Gunnu%2520and%2520Abhijit%2520Mishra%2520and%2520Pushpak%2520Bhattacharyya%26entry.1292438233%3D%2520%2520Pragmatics%252C%2520the%2520ability%2520to%2520infer%2520meaning%2520beyond%2520literal%2520interpretation%252C%2520is%250Acrucial%2520for%2520social%2520cognition%2520and%2520communication.%2520While%2520LLMs%2520have%2520been%250Abenchmarked%2520for%2520their%2520pragmatic%2520understanding%252C%2520improving%2520their%2520performance%250Aremains%2520underexplored.%2520Existing%2520methods%2520rely%2520on%2520annotated%2520labels%2520but%2520overlook%250Athe%2520reasoning%2520process%2520humans%2520naturally%2520use%2520to%2520interpret%2520implicit%2520meaning.%2520To%250Abridge%2520this%2520gap%252C%2520we%2520introduce%2520a%2520novel%2520pragmatic%2520dataset%252C%250AImpliedMeaningPreference%252C%2520that%2520includes%2520explicit%2520reasoning%2520%2528thoughts%2529%2520for%2520both%250Acorrect%2520and%2520incorrect%2520interpretations.%2520Through%2520preference-tuning%2520and%2520supervised%250Afine-tuning%252C%2520we%2520demonstrate%2520that%2520thought-based%2520learning%2520significantly%2520enhances%250ALLMs%2527%2520pragmatic%2520understanding%252C%2520improving%2520accuracy%2520by%252011.12%2525%2520across%2520model%250Afamilies.%2520We%2520further%2520discuss%2520a%2520transfer-learning%2520study%2520where%2520we%2520evaluate%2520the%250Aperformance%2520of%2520thought-based%2520training%2520for%2520the%2520other%2520tasks%2520of%2520pragmatics%250A%2528presupposition%252C%2520deixis%2529%2520that%2520are%2520not%2520seen%2520during%2520the%2520training%2520time%2520and%2520observe%250Aan%2520improvement%2520of%252016.10%2525%2520compared%2520to%2520label-trained%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13559v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understand%20the%20Implication%3A%20Learning%20to%20Think%20for%20Pragmatic%0A%20%20Understanding&entry.906535625=Settaluri%20Lakshmi%20Sravanthi%20and%20Kishan%20Maharaj%20and%20Sravani%20Gunnu%20and%20Abhijit%20Mishra%20and%20Pushpak%20Bhattacharyya&entry.1292438233=%20%20Pragmatics%2C%20the%20ability%20to%20infer%20meaning%20beyond%20literal%20interpretation%2C%20is%0Acrucial%20for%20social%20cognition%20and%20communication.%20While%20LLMs%20have%20been%0Abenchmarked%20for%20their%20pragmatic%20understanding%2C%20improving%20their%20performance%0Aremains%20underexplored.%20Existing%20methods%20rely%20on%20annotated%20labels%20but%20overlook%0Athe%20reasoning%20process%20humans%20naturally%20use%20to%20interpret%20implicit%20meaning.%20To%0Abridge%20this%20gap%2C%20we%20introduce%20a%20novel%20pragmatic%20dataset%2C%0AImpliedMeaningPreference%2C%20that%20includes%20explicit%20reasoning%20%28thoughts%29%20for%20both%0Acorrect%20and%20incorrect%20interpretations.%20Through%20preference-tuning%20and%20supervised%0Afine-tuning%2C%20we%20demonstrate%20that%20thought-based%20learning%20significantly%20enhances%0ALLMs%27%20pragmatic%20understanding%2C%20improving%20accuracy%20by%2011.12%25%20across%20model%0Afamilies.%20We%20further%20discuss%20a%20transfer-learning%20study%20where%20we%20evaluate%20the%0Aperformance%20of%20thought-based%20training%20for%20the%20other%20tasks%20of%20pragmatics%0A%28presupposition%2C%20deixis%29%20that%20are%20not%20seen%20during%20the%20training%20time%20and%20observe%0Aan%20improvement%20of%2016.10%25%20compared%20to%20label-trained%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13559v1&entry.124074799=Read"},
{"title": "Evaluation of Vision Transformers for Multimodal Image Classification: A\n  Case Study on Brain, Lung, and Kidney Tumors", "author": "\u00d3scar A. Mart\u00edn and Javier S\u00e1nchez", "abstract": "  Neural networks have become the standard technique for medical diagnostics,\nespecially in cancer detection and classification. This work evaluates the\nperformance of Vision Transformers architectures, including Swin Transformer\nand MaxViT, in several datasets of magnetic resonance imaging (MRI) and\ncomputed tomography (CT) scans. We used three training sets of images with\nbrain, lung, and kidney tumors. Each dataset includes different classification\nlabels, from brain gliomas and meningiomas to benign and malignant lung\nconditions and kidney anomalies such as cysts and cancers. This work aims to\nanalyze the behavior of the neural networks in each dataset and the benefits of\ncombining different image modalities and tumor classes. We designed several\nexperiments by fine-tuning the models on combined and individual datasets. The\nresults revealed that the Swin Transformer provided high accuracy, achieving up\nto 99\\% on average for individual datasets and 99.4\\% accuracy for the combined\ndataset. This research highlights the adaptability of Transformer-based models\nto various image modalities and features. However, challenges persist,\nincluding limited annotated data and interpretability issues. Future work will\nexpand this study by incorporating other image modalities and enhancing\ndiagnostic capabilities. Integrating these models across diverse datasets could\nmark a significant advance in precision medicine, paving the way for more\nefficient and comprehensive healthcare solutions.\n", "link": "http://arxiv.org/abs/2502.05517v2", "date": "2025-06-16", "relevancy": 2.5938, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5363}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.51}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20of%20Vision%20Transformers%20for%20Multimodal%20Image%20Classification%3A%20A%0A%20%20Case%20Study%20on%20Brain%2C%20Lung%2C%20and%20Kidney%20Tumors&body=Title%3A%20Evaluation%20of%20Vision%20Transformers%20for%20Multimodal%20Image%20Classification%3A%20A%0A%20%20Case%20Study%20on%20Brain%2C%20Lung%2C%20and%20Kidney%20Tumors%0AAuthor%3A%20%C3%93scar%20A.%20Mart%C3%ADn%20and%20Javier%20S%C3%A1nchez%0AAbstract%3A%20%20%20Neural%20networks%20have%20become%20the%20standard%20technique%20for%20medical%20diagnostics%2C%0Aespecially%20in%20cancer%20detection%20and%20classification.%20This%20work%20evaluates%20the%0Aperformance%20of%20Vision%20Transformers%20architectures%2C%20including%20Swin%20Transformer%0Aand%20MaxViT%2C%20in%20several%20datasets%20of%20magnetic%20resonance%20imaging%20%28MRI%29%20and%0Acomputed%20tomography%20%28CT%29%20scans.%20We%20used%20three%20training%20sets%20of%20images%20with%0Abrain%2C%20lung%2C%20and%20kidney%20tumors.%20Each%20dataset%20includes%20different%20classification%0Alabels%2C%20from%20brain%20gliomas%20and%20meningiomas%20to%20benign%20and%20malignant%20lung%0Aconditions%20and%20kidney%20anomalies%20such%20as%20cysts%20and%20cancers.%20This%20work%20aims%20to%0Aanalyze%20the%20behavior%20of%20the%20neural%20networks%20in%20each%20dataset%20and%20the%20benefits%20of%0Acombining%20different%20image%20modalities%20and%20tumor%20classes.%20We%20designed%20several%0Aexperiments%20by%20fine-tuning%20the%20models%20on%20combined%20and%20individual%20datasets.%20The%0Aresults%20revealed%20that%20the%20Swin%20Transformer%20provided%20high%20accuracy%2C%20achieving%20up%0Ato%2099%5C%25%20on%20average%20for%20individual%20datasets%20and%2099.4%5C%25%20accuracy%20for%20the%20combined%0Adataset.%20This%20research%20highlights%20the%20adaptability%20of%20Transformer-based%20models%0Ato%20various%20image%20modalities%20and%20features.%20However%2C%20challenges%20persist%2C%0Aincluding%20limited%20annotated%20data%20and%20interpretability%20issues.%20Future%20work%20will%0Aexpand%20this%20study%20by%20incorporating%20other%20image%20modalities%20and%20enhancing%0Adiagnostic%20capabilities.%20Integrating%20these%20models%20across%20diverse%20datasets%20could%0Amark%20a%20significant%20advance%20in%20precision%20medicine%2C%20paving%20the%20way%20for%20more%0Aefficient%20and%20comprehensive%20healthcare%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05517v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520of%2520Vision%2520Transformers%2520for%2520Multimodal%2520Image%2520Classification%253A%2520A%250A%2520%2520Case%2520Study%2520on%2520Brain%252C%2520Lung%252C%2520and%2520Kidney%2520Tumors%26entry.906535625%3D%25C3%2593scar%2520A.%2520Mart%25C3%25ADn%2520and%2520Javier%2520S%25C3%25A1nchez%26entry.1292438233%3D%2520%2520Neural%2520networks%2520have%2520become%2520the%2520standard%2520technique%2520for%2520medical%2520diagnostics%252C%250Aespecially%2520in%2520cancer%2520detection%2520and%2520classification.%2520This%2520work%2520evaluates%2520the%250Aperformance%2520of%2520Vision%2520Transformers%2520architectures%252C%2520including%2520Swin%2520Transformer%250Aand%2520MaxViT%252C%2520in%2520several%2520datasets%2520of%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520and%250Acomputed%2520tomography%2520%2528CT%2529%2520scans.%2520We%2520used%2520three%2520training%2520sets%2520of%2520images%2520with%250Abrain%252C%2520lung%252C%2520and%2520kidney%2520tumors.%2520Each%2520dataset%2520includes%2520different%2520classification%250Alabels%252C%2520from%2520brain%2520gliomas%2520and%2520meningiomas%2520to%2520benign%2520and%2520malignant%2520lung%250Aconditions%2520and%2520kidney%2520anomalies%2520such%2520as%2520cysts%2520and%2520cancers.%2520This%2520work%2520aims%2520to%250Aanalyze%2520the%2520behavior%2520of%2520the%2520neural%2520networks%2520in%2520each%2520dataset%2520and%2520the%2520benefits%2520of%250Acombining%2520different%2520image%2520modalities%2520and%2520tumor%2520classes.%2520We%2520designed%2520several%250Aexperiments%2520by%2520fine-tuning%2520the%2520models%2520on%2520combined%2520and%2520individual%2520datasets.%2520The%250Aresults%2520revealed%2520that%2520the%2520Swin%2520Transformer%2520provided%2520high%2520accuracy%252C%2520achieving%2520up%250Ato%252099%255C%2525%2520on%2520average%2520for%2520individual%2520datasets%2520and%252099.4%255C%2525%2520accuracy%2520for%2520the%2520combined%250Adataset.%2520This%2520research%2520highlights%2520the%2520adaptability%2520of%2520Transformer-based%2520models%250Ato%2520various%2520image%2520modalities%2520and%2520features.%2520However%252C%2520challenges%2520persist%252C%250Aincluding%2520limited%2520annotated%2520data%2520and%2520interpretability%2520issues.%2520Future%2520work%2520will%250Aexpand%2520this%2520study%2520by%2520incorporating%2520other%2520image%2520modalities%2520and%2520enhancing%250Adiagnostic%2520capabilities.%2520Integrating%2520these%2520models%2520across%2520diverse%2520datasets%2520could%250Amark%2520a%2520significant%2520advance%2520in%2520precision%2520medicine%252C%2520paving%2520the%2520way%2520for%2520more%250Aefficient%2520and%2520comprehensive%2520healthcare%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05517v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20of%20Vision%20Transformers%20for%20Multimodal%20Image%20Classification%3A%20A%0A%20%20Case%20Study%20on%20Brain%2C%20Lung%2C%20and%20Kidney%20Tumors&entry.906535625=%C3%93scar%20A.%20Mart%C3%ADn%20and%20Javier%20S%C3%A1nchez&entry.1292438233=%20%20Neural%20networks%20have%20become%20the%20standard%20technique%20for%20medical%20diagnostics%2C%0Aespecially%20in%20cancer%20detection%20and%20classification.%20This%20work%20evaluates%20the%0Aperformance%20of%20Vision%20Transformers%20architectures%2C%20including%20Swin%20Transformer%0Aand%20MaxViT%2C%20in%20several%20datasets%20of%20magnetic%20resonance%20imaging%20%28MRI%29%20and%0Acomputed%20tomography%20%28CT%29%20scans.%20We%20used%20three%20training%20sets%20of%20images%20with%0Abrain%2C%20lung%2C%20and%20kidney%20tumors.%20Each%20dataset%20includes%20different%20classification%0Alabels%2C%20from%20brain%20gliomas%20and%20meningiomas%20to%20benign%20and%20malignant%20lung%0Aconditions%20and%20kidney%20anomalies%20such%20as%20cysts%20and%20cancers.%20This%20work%20aims%20to%0Aanalyze%20the%20behavior%20of%20the%20neural%20networks%20in%20each%20dataset%20and%20the%20benefits%20of%0Acombining%20different%20image%20modalities%20and%20tumor%20classes.%20We%20designed%20several%0Aexperiments%20by%20fine-tuning%20the%20models%20on%20combined%20and%20individual%20datasets.%20The%0Aresults%20revealed%20that%20the%20Swin%20Transformer%20provided%20high%20accuracy%2C%20achieving%20up%0Ato%2099%5C%25%20on%20average%20for%20individual%20datasets%20and%2099.4%5C%25%20accuracy%20for%20the%20combined%0Adataset.%20This%20research%20highlights%20the%20adaptability%20of%20Transformer-based%20models%0Ato%20various%20image%20modalities%20and%20features.%20However%2C%20challenges%20persist%2C%0Aincluding%20limited%20annotated%20data%20and%20interpretability%20issues.%20Future%20work%20will%0Aexpand%20this%20study%20by%20incorporating%20other%20image%20modalities%20and%20enhancing%0Adiagnostic%20capabilities.%20Integrating%20these%20models%20across%20diverse%20datasets%20could%0Amark%20a%20significant%20advance%20in%20precision%20medicine%2C%20paving%20the%20way%20for%20more%0Aefficient%20and%20comprehensive%20healthcare%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05517v2&entry.124074799=Read"},
{"title": "Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own\n  Reasoning for Open-Ended Tasks", "author": "Yifei Xu and Tusher Chakraborty and Srinagesh Sharma and Leonardo Nunes and Emre K\u0131c\u0131man and Songwu Lu and Ranveer Chandra", "abstract": "  Recent advances in Large Language Models (LLMs) have showcased impressive\nreasoning abilities in structured tasks like mathematics and programming,\nlargely driven by Reinforcement Learning with Verifiable Rewards (RLVR), which\nuses outcome-based signals that are scalable, effective, and robust against\nreward hacking. However, applying similar techniques to open-ended long-form\nreasoning tasks remains challenging due to the absence of generic, verifiable\nreward signals. To address this, we propose Direct Reasoning Optimization\n(DRO), a reinforcement learning framework for fine-tuning LLMs on open-ended,\nparticularly long-form, reasoning tasks, guided by a new reward signal: the\nReasoning Reflection Reward (R3). At its core, R3 selectively identifies and\nemphasizes key tokens in the reference outcome that reflect the influence of\nthe model's preceding chain-of-thought reasoning, thereby capturing the\nconsistency between reasoning and reference outcome at a fine-grained level.\nCrucially, R3 is computed internally using the same model being optimized,\nenabling a fully self-contained training setup. Additionally, we introduce a\ndynamic data filtering strategy based on R3 for open-ended reasoning tasks,\nreducing cost while improving downstream performance. We evaluate DRO on two\ndiverse datasets -- ParaRev, a long-form paragraph revision task, and FinQA, a\nmath-oriented QA benchmark -- and show that it consistently outperforms strong\nbaselines while remaining broadly applicable across both open-ended and\nstructured domains.\n", "link": "http://arxiv.org/abs/2506.13351v1", "date": "2025-06-16", "relevancy": 2.5766, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5307}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5307}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Direct%20Reasoning%20Optimization%3A%20LLMs%20Can%20Reward%20And%20Refine%20Their%20Own%0A%20%20Reasoning%20for%20Open-Ended%20Tasks&body=Title%3A%20Direct%20Reasoning%20Optimization%3A%20LLMs%20Can%20Reward%20And%20Refine%20Their%20Own%0A%20%20Reasoning%20for%20Open-Ended%20Tasks%0AAuthor%3A%20Yifei%20Xu%20and%20Tusher%20Chakraborty%20and%20Srinagesh%20Sharma%20and%20Leonardo%20Nunes%20and%20Emre%20K%C4%B1c%C4%B1man%20and%20Songwu%20Lu%20and%20Ranveer%20Chandra%0AAbstract%3A%20%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20showcased%20impressive%0Areasoning%20abilities%20in%20structured%20tasks%20like%20mathematics%20and%20programming%2C%0Alargely%20driven%20by%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%2C%20which%0Auses%20outcome-based%20signals%20that%20are%20scalable%2C%20effective%2C%20and%20robust%20against%0Areward%20hacking.%20However%2C%20applying%20similar%20techniques%20to%20open-ended%20long-form%0Areasoning%20tasks%20remains%20challenging%20due%20to%20the%20absence%20of%20generic%2C%20verifiable%0Areward%20signals.%20To%20address%20this%2C%20we%20propose%20Direct%20Reasoning%20Optimization%0A%28DRO%29%2C%20a%20reinforcement%20learning%20framework%20for%20fine-tuning%20LLMs%20on%20open-ended%2C%0Aparticularly%20long-form%2C%20reasoning%20tasks%2C%20guided%20by%20a%20new%20reward%20signal%3A%20the%0AReasoning%20Reflection%20Reward%20%28R3%29.%20At%20its%20core%2C%20R3%20selectively%20identifies%20and%0Aemphasizes%20key%20tokens%20in%20the%20reference%20outcome%20that%20reflect%20the%20influence%20of%0Athe%20model%27s%20preceding%20chain-of-thought%20reasoning%2C%20thereby%20capturing%20the%0Aconsistency%20between%20reasoning%20and%20reference%20outcome%20at%20a%20fine-grained%20level.%0ACrucially%2C%20R3%20is%20computed%20internally%20using%20the%20same%20model%20being%20optimized%2C%0Aenabling%20a%20fully%20self-contained%20training%20setup.%20Additionally%2C%20we%20introduce%20a%0Adynamic%20data%20filtering%20strategy%20based%20on%20R3%20for%20open-ended%20reasoning%20tasks%2C%0Areducing%20cost%20while%20improving%20downstream%20performance.%20We%20evaluate%20DRO%20on%20two%0Adiverse%20datasets%20--%20ParaRev%2C%20a%20long-form%20paragraph%20revision%20task%2C%20and%20FinQA%2C%20a%0Amath-oriented%20QA%20benchmark%20--%20and%20show%20that%20it%20consistently%20outperforms%20strong%0Abaselines%20while%20remaining%20broadly%20applicable%20across%20both%20open-ended%20and%0Astructured%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13351v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirect%2520Reasoning%2520Optimization%253A%2520LLMs%2520Can%2520Reward%2520And%2520Refine%2520Their%2520Own%250A%2520%2520Reasoning%2520for%2520Open-Ended%2520Tasks%26entry.906535625%3DYifei%2520Xu%2520and%2520Tusher%2520Chakraborty%2520and%2520Srinagesh%2520Sharma%2520and%2520Leonardo%2520Nunes%2520and%2520Emre%2520K%25C4%25B1c%25C4%25B1man%2520and%2520Songwu%2520Lu%2520and%2520Ranveer%2520Chandra%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520showcased%2520impressive%250Areasoning%2520abilities%2520in%2520structured%2520tasks%2520like%2520mathematics%2520and%2520programming%252C%250Alargely%2520driven%2520by%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%252C%2520which%250Auses%2520outcome-based%2520signals%2520that%2520are%2520scalable%252C%2520effective%252C%2520and%2520robust%2520against%250Areward%2520hacking.%2520However%252C%2520applying%2520similar%2520techniques%2520to%2520open-ended%2520long-form%250Areasoning%2520tasks%2520remains%2520challenging%2520due%2520to%2520the%2520absence%2520of%2520generic%252C%2520verifiable%250Areward%2520signals.%2520To%2520address%2520this%252C%2520we%2520propose%2520Direct%2520Reasoning%2520Optimization%250A%2528DRO%2529%252C%2520a%2520reinforcement%2520learning%2520framework%2520for%2520fine-tuning%2520LLMs%2520on%2520open-ended%252C%250Aparticularly%2520long-form%252C%2520reasoning%2520tasks%252C%2520guided%2520by%2520a%2520new%2520reward%2520signal%253A%2520the%250AReasoning%2520Reflection%2520Reward%2520%2528R3%2529.%2520At%2520its%2520core%252C%2520R3%2520selectively%2520identifies%2520and%250Aemphasizes%2520key%2520tokens%2520in%2520the%2520reference%2520outcome%2520that%2520reflect%2520the%2520influence%2520of%250Athe%2520model%2527s%2520preceding%2520chain-of-thought%2520reasoning%252C%2520thereby%2520capturing%2520the%250Aconsistency%2520between%2520reasoning%2520and%2520reference%2520outcome%2520at%2520a%2520fine-grained%2520level.%250ACrucially%252C%2520R3%2520is%2520computed%2520internally%2520using%2520the%2520same%2520model%2520being%2520optimized%252C%250Aenabling%2520a%2520fully%2520self-contained%2520training%2520setup.%2520Additionally%252C%2520we%2520introduce%2520a%250Adynamic%2520data%2520filtering%2520strategy%2520based%2520on%2520R3%2520for%2520open-ended%2520reasoning%2520tasks%252C%250Areducing%2520cost%2520while%2520improving%2520downstream%2520performance.%2520We%2520evaluate%2520DRO%2520on%2520two%250Adiverse%2520datasets%2520--%2520ParaRev%252C%2520a%2520long-form%2520paragraph%2520revision%2520task%252C%2520and%2520FinQA%252C%2520a%250Amath-oriented%2520QA%2520benchmark%2520--%2520and%2520show%2520that%2520it%2520consistently%2520outperforms%2520strong%250Abaselines%2520while%2520remaining%2520broadly%2520applicable%2520across%2520both%2520open-ended%2520and%250Astructured%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13351v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Direct%20Reasoning%20Optimization%3A%20LLMs%20Can%20Reward%20And%20Refine%20Their%20Own%0A%20%20Reasoning%20for%20Open-Ended%20Tasks&entry.906535625=Yifei%20Xu%20and%20Tusher%20Chakraborty%20and%20Srinagesh%20Sharma%20and%20Leonardo%20Nunes%20and%20Emre%20K%C4%B1c%C4%B1man%20and%20Songwu%20Lu%20and%20Ranveer%20Chandra&entry.1292438233=%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20showcased%20impressive%0Areasoning%20abilities%20in%20structured%20tasks%20like%20mathematics%20and%20programming%2C%0Alargely%20driven%20by%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%2C%20which%0Auses%20outcome-based%20signals%20that%20are%20scalable%2C%20effective%2C%20and%20robust%20against%0Areward%20hacking.%20However%2C%20applying%20similar%20techniques%20to%20open-ended%20long-form%0Areasoning%20tasks%20remains%20challenging%20due%20to%20the%20absence%20of%20generic%2C%20verifiable%0Areward%20signals.%20To%20address%20this%2C%20we%20propose%20Direct%20Reasoning%20Optimization%0A%28DRO%29%2C%20a%20reinforcement%20learning%20framework%20for%20fine-tuning%20LLMs%20on%20open-ended%2C%0Aparticularly%20long-form%2C%20reasoning%20tasks%2C%20guided%20by%20a%20new%20reward%20signal%3A%20the%0AReasoning%20Reflection%20Reward%20%28R3%29.%20At%20its%20core%2C%20R3%20selectively%20identifies%20and%0Aemphasizes%20key%20tokens%20in%20the%20reference%20outcome%20that%20reflect%20the%20influence%20of%0Athe%20model%27s%20preceding%20chain-of-thought%20reasoning%2C%20thereby%20capturing%20the%0Aconsistency%20between%20reasoning%20and%20reference%20outcome%20at%20a%20fine-grained%20level.%0ACrucially%2C%20R3%20is%20computed%20internally%20using%20the%20same%20model%20being%20optimized%2C%0Aenabling%20a%20fully%20self-contained%20training%20setup.%20Additionally%2C%20we%20introduce%20a%0Adynamic%20data%20filtering%20strategy%20based%20on%20R3%20for%20open-ended%20reasoning%20tasks%2C%0Areducing%20cost%20while%20improving%20downstream%20performance.%20We%20evaluate%20DRO%20on%20two%0Adiverse%20datasets%20--%20ParaRev%2C%20a%20long-form%20paragraph%20revision%20task%2C%20and%20FinQA%2C%20a%0Amath-oriented%20QA%20benchmark%20--%20and%20show%20that%20it%20consistently%20outperforms%20strong%0Abaselines%20while%20remaining%20broadly%20applicable%20across%20both%20open-ended%20and%0Astructured%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13351v1&entry.124074799=Read"},
{"title": "FreeQ-Graph: Free-form Querying with Semantic Consistent Scene Graph for\n  3D Scene Understanding", "author": "Chenlu Zhan and Gaoang Wang and Hongwei Wang", "abstract": "  Semantic querying in complex 3D scenes through free-form language presents a\nsignificant challenge. Existing 3D scene understanding methods use large-scale\ntraining data and CLIP to align text queries with 3D semantic features.\nHowever, their reliance on predefined vocabulary priors from training data\nhinders free-form semantic querying. Besides, recent advanced methods rely on\nLLMs for scene understanding but lack comprehensive 3D scene-level information\nand often overlook the potential inconsistencies in LLM-generated outputs. In\nour paper, we propose FreeQ-Graph, which enables Free-form Querying with a\nsemantic consistent scene Graph for 3D scene understanding. The core idea is to\nencode free-form queries from a complete and accurate 3D scene graph without\npredefined vocabularies, and to align them with 3D consistent semantic labels,\nwhich accomplished through three key steps. We initiate by constructing a\ncomplete and accurate 3D scene graph that maps free-form objects and their\nrelations through LLM and LVLM guidance, entirely free from training data or\npredefined priors. Most importantly, we align graph nodes with accurate\nsemantic labels by leveraging 3D semantic aligned features from merged\nsuperpoints, enhancing 3D semantic consistency. To enable free-form semantic\nquerying, we then design an LLM-based reasoning algorithm that combines\nscene-level and object-level information to intricate reasoning. We conducted\nextensive experiments on 3D semantic grounding, segmentation, and complex\nquerying tasks, while also validating the accuracy of graph generation.\nExperiments on 6 datasets show that our model excels in both complex free-form\nsemantic queries and intricate relational reasoning.\n", "link": "http://arxiv.org/abs/2506.13629v1", "date": "2025-06-16", "relevancy": 2.5468, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6458}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6458}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeQ-Graph%3A%20Free-form%20Querying%20with%20Semantic%20Consistent%20Scene%20Graph%20for%0A%20%203D%20Scene%20Understanding&body=Title%3A%20FreeQ-Graph%3A%20Free-form%20Querying%20with%20Semantic%20Consistent%20Scene%20Graph%20for%0A%20%203D%20Scene%20Understanding%0AAuthor%3A%20Chenlu%20Zhan%20and%20Gaoang%20Wang%20and%20Hongwei%20Wang%0AAbstract%3A%20%20%20Semantic%20querying%20in%20complex%203D%20scenes%20through%20free-form%20language%20presents%20a%0Asignificant%20challenge.%20Existing%203D%20scene%20understanding%20methods%20use%20large-scale%0Atraining%20data%20and%20CLIP%20to%20align%20text%20queries%20with%203D%20semantic%20features.%0AHowever%2C%20their%20reliance%20on%20predefined%20vocabulary%20priors%20from%20training%20data%0Ahinders%20free-form%20semantic%20querying.%20Besides%2C%20recent%20advanced%20methods%20rely%20on%0ALLMs%20for%20scene%20understanding%20but%20lack%20comprehensive%203D%20scene-level%20information%0Aand%20often%20overlook%20the%20potential%20inconsistencies%20in%20LLM-generated%20outputs.%20In%0Aour%20paper%2C%20we%20propose%20FreeQ-Graph%2C%20which%20enables%20Free-form%20Querying%20with%20a%0Asemantic%20consistent%20scene%20Graph%20for%203D%20scene%20understanding.%20The%20core%20idea%20is%20to%0Aencode%20free-form%20queries%20from%20a%20complete%20and%20accurate%203D%20scene%20graph%20without%0Apredefined%20vocabularies%2C%20and%20to%20align%20them%20with%203D%20consistent%20semantic%20labels%2C%0Awhich%20accomplished%20through%20three%20key%20steps.%20We%20initiate%20by%20constructing%20a%0Acomplete%20and%20accurate%203D%20scene%20graph%20that%20maps%20free-form%20objects%20and%20their%0Arelations%20through%20LLM%20and%20LVLM%20guidance%2C%20entirely%20free%20from%20training%20data%20or%0Apredefined%20priors.%20Most%20importantly%2C%20we%20align%20graph%20nodes%20with%20accurate%0Asemantic%20labels%20by%20leveraging%203D%20semantic%20aligned%20features%20from%20merged%0Asuperpoints%2C%20enhancing%203D%20semantic%20consistency.%20To%20enable%20free-form%20semantic%0Aquerying%2C%20we%20then%20design%20an%20LLM-based%20reasoning%20algorithm%20that%20combines%0Ascene-level%20and%20object-level%20information%20to%20intricate%20reasoning.%20We%20conducted%0Aextensive%20experiments%20on%203D%20semantic%20grounding%2C%20segmentation%2C%20and%20complex%0Aquerying%20tasks%2C%20while%20also%20validating%20the%20accuracy%20of%20graph%20generation.%0AExperiments%20on%206%20datasets%20show%20that%20our%20model%20excels%20in%20both%20complex%20free-form%0Asemantic%20queries%20and%20intricate%20relational%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeQ-Graph%253A%2520Free-form%2520Querying%2520with%2520Semantic%2520Consistent%2520Scene%2520Graph%2520for%250A%2520%25203D%2520Scene%2520Understanding%26entry.906535625%3DChenlu%2520Zhan%2520and%2520Gaoang%2520Wang%2520and%2520Hongwei%2520Wang%26entry.1292438233%3D%2520%2520Semantic%2520querying%2520in%2520complex%25203D%2520scenes%2520through%2520free-form%2520language%2520presents%2520a%250Asignificant%2520challenge.%2520Existing%25203D%2520scene%2520understanding%2520methods%2520use%2520large-scale%250Atraining%2520data%2520and%2520CLIP%2520to%2520align%2520text%2520queries%2520with%25203D%2520semantic%2520features.%250AHowever%252C%2520their%2520reliance%2520on%2520predefined%2520vocabulary%2520priors%2520from%2520training%2520data%250Ahinders%2520free-form%2520semantic%2520querying.%2520Besides%252C%2520recent%2520advanced%2520methods%2520rely%2520on%250ALLMs%2520for%2520scene%2520understanding%2520but%2520lack%2520comprehensive%25203D%2520scene-level%2520information%250Aand%2520often%2520overlook%2520the%2520potential%2520inconsistencies%2520in%2520LLM-generated%2520outputs.%2520In%250Aour%2520paper%252C%2520we%2520propose%2520FreeQ-Graph%252C%2520which%2520enables%2520Free-form%2520Querying%2520with%2520a%250Asemantic%2520consistent%2520scene%2520Graph%2520for%25203D%2520scene%2520understanding.%2520The%2520core%2520idea%2520is%2520to%250Aencode%2520free-form%2520queries%2520from%2520a%2520complete%2520and%2520accurate%25203D%2520scene%2520graph%2520without%250Apredefined%2520vocabularies%252C%2520and%2520to%2520align%2520them%2520with%25203D%2520consistent%2520semantic%2520labels%252C%250Awhich%2520accomplished%2520through%2520three%2520key%2520steps.%2520We%2520initiate%2520by%2520constructing%2520a%250Acomplete%2520and%2520accurate%25203D%2520scene%2520graph%2520that%2520maps%2520free-form%2520objects%2520and%2520their%250Arelations%2520through%2520LLM%2520and%2520LVLM%2520guidance%252C%2520entirely%2520free%2520from%2520training%2520data%2520or%250Apredefined%2520priors.%2520Most%2520importantly%252C%2520we%2520align%2520graph%2520nodes%2520with%2520accurate%250Asemantic%2520labels%2520by%2520leveraging%25203D%2520semantic%2520aligned%2520features%2520from%2520merged%250Asuperpoints%252C%2520enhancing%25203D%2520semantic%2520consistency.%2520To%2520enable%2520free-form%2520semantic%250Aquerying%252C%2520we%2520then%2520design%2520an%2520LLM-based%2520reasoning%2520algorithm%2520that%2520combines%250Ascene-level%2520and%2520object-level%2520information%2520to%2520intricate%2520reasoning.%2520We%2520conducted%250Aextensive%2520experiments%2520on%25203D%2520semantic%2520grounding%252C%2520segmentation%252C%2520and%2520complex%250Aquerying%2520tasks%252C%2520while%2520also%2520validating%2520the%2520accuracy%2520of%2520graph%2520generation.%250AExperiments%2520on%25206%2520datasets%2520show%2520that%2520our%2520model%2520excels%2520in%2520both%2520complex%2520free-form%250Asemantic%2520queries%2520and%2520intricate%2520relational%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeQ-Graph%3A%20Free-form%20Querying%20with%20Semantic%20Consistent%20Scene%20Graph%20for%0A%20%203D%20Scene%20Understanding&entry.906535625=Chenlu%20Zhan%20and%20Gaoang%20Wang%20and%20Hongwei%20Wang&entry.1292438233=%20%20Semantic%20querying%20in%20complex%203D%20scenes%20through%20free-form%20language%20presents%20a%0Asignificant%20challenge.%20Existing%203D%20scene%20understanding%20methods%20use%20large-scale%0Atraining%20data%20and%20CLIP%20to%20align%20text%20queries%20with%203D%20semantic%20features.%0AHowever%2C%20their%20reliance%20on%20predefined%20vocabulary%20priors%20from%20training%20data%0Ahinders%20free-form%20semantic%20querying.%20Besides%2C%20recent%20advanced%20methods%20rely%20on%0ALLMs%20for%20scene%20understanding%20but%20lack%20comprehensive%203D%20scene-level%20information%0Aand%20often%20overlook%20the%20potential%20inconsistencies%20in%20LLM-generated%20outputs.%20In%0Aour%20paper%2C%20we%20propose%20FreeQ-Graph%2C%20which%20enables%20Free-form%20Querying%20with%20a%0Asemantic%20consistent%20scene%20Graph%20for%203D%20scene%20understanding.%20The%20core%20idea%20is%20to%0Aencode%20free-form%20queries%20from%20a%20complete%20and%20accurate%203D%20scene%20graph%20without%0Apredefined%20vocabularies%2C%20and%20to%20align%20them%20with%203D%20consistent%20semantic%20labels%2C%0Awhich%20accomplished%20through%20three%20key%20steps.%20We%20initiate%20by%20constructing%20a%0Acomplete%20and%20accurate%203D%20scene%20graph%20that%20maps%20free-form%20objects%20and%20their%0Arelations%20through%20LLM%20and%20LVLM%20guidance%2C%20entirely%20free%20from%20training%20data%20or%0Apredefined%20priors.%20Most%20importantly%2C%20we%20align%20graph%20nodes%20with%20accurate%0Asemantic%20labels%20by%20leveraging%203D%20semantic%20aligned%20features%20from%20merged%0Asuperpoints%2C%20enhancing%203D%20semantic%20consistency.%20To%20enable%20free-form%20semantic%0Aquerying%2C%20we%20then%20design%20an%20LLM-based%20reasoning%20algorithm%20that%20combines%0Ascene-level%20and%20object-level%20information%20to%20intricate%20reasoning.%20We%20conducted%0Aextensive%20experiments%20on%203D%20semantic%20grounding%2C%20segmentation%2C%20and%20complex%0Aquerying%20tasks%2C%20while%20also%20validating%20the%20accuracy%20of%20graph%20generation.%0AExperiments%20on%206%20datasets%20show%20that%20our%20model%20excels%20in%20both%20complex%20free-form%0Asemantic%20queries%20and%20intricate%20relational%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13629v1&entry.124074799=Read"},
{"title": "Agentic 3D Scene Generation with Spatially Contextualized VLMs", "author": "Xinhang Liu and Yu-Wing Tai and Chi-Keung Tang", "abstract": "  Despite recent advances in multimodal content generation enabled by\nvision-language models (VLMs), their ability to reason about and generate\nstructured 3D scenes remains largely underexplored. This limitation constrains\ntheir utility in spatially grounded tasks such as embodied AI, immersive\nsimulations, and interactive 3D applications. We introduce a new paradigm that\nenables VLMs to generate, understand, and edit complex 3D environments by\ninjecting a continually evolving spatial context. Constructed from multimodal\ninput, this context consists of three components: a scene portrait that\nprovides a high-level semantic blueprint, a semantically labeled point cloud\ncapturing object-level geometry, and a scene hypergraph that encodes rich\nspatial relationships, including unary, binary, and higher-order constraints.\nTogether, these components provide the VLM with a structured, geometry-aware\nworking memory that integrates its inherent multimodal reasoning capabilities\nwith structured 3D understanding for effective spatial reasoning. Building on\nthis foundation, we develop an agentic 3D scene generation pipeline in which\nthe VLM iteratively reads from and updates the spatial context. The pipeline\nfeatures high-quality asset generation with geometric restoration, environment\nsetup with automatic verification, and ergonomic adjustment guided by the scene\nhypergraph. Experiments show that our framework can handle diverse and\nchallenging inputs, achieving a level of generalization not observed in prior\nwork. Further results demonstrate that injecting spatial context enables VLMs\nto perform downstream tasks such as interactive scene editing and path\nplanning, suggesting strong potential for spatially intelligent systems in\ncomputer graphics, 3D vision, and embodied applications. Project page:\nhttps://spatctxvlm.github.io/project_page/.\n", "link": "http://arxiv.org/abs/2505.20129v2", "date": "2025-06-16", "relevancy": 2.5427, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6404}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6404}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agentic%203D%20Scene%20Generation%20with%20Spatially%20Contextualized%20VLMs&body=Title%3A%20Agentic%203D%20Scene%20Generation%20with%20Spatially%20Contextualized%20VLMs%0AAuthor%3A%20Xinhang%20Liu%20and%20Yu-Wing%20Tai%20and%20Chi-Keung%20Tang%0AAbstract%3A%20%20%20Despite%20recent%20advances%20in%20multimodal%20content%20generation%20enabled%20by%0Avision-language%20models%20%28VLMs%29%2C%20their%20ability%20to%20reason%20about%20and%20generate%0Astructured%203D%20scenes%20remains%20largely%20underexplored.%20This%20limitation%20constrains%0Atheir%20utility%20in%20spatially%20grounded%20tasks%20such%20as%20embodied%20AI%2C%20immersive%0Asimulations%2C%20and%20interactive%203D%20applications.%20We%20introduce%20a%20new%20paradigm%20that%0Aenables%20VLMs%20to%20generate%2C%20understand%2C%20and%20edit%20complex%203D%20environments%20by%0Ainjecting%20a%20continually%20evolving%20spatial%20context.%20Constructed%20from%20multimodal%0Ainput%2C%20this%20context%20consists%20of%20three%20components%3A%20a%20scene%20portrait%20that%0Aprovides%20a%20high-level%20semantic%20blueprint%2C%20a%20semantically%20labeled%20point%20cloud%0Acapturing%20object-level%20geometry%2C%20and%20a%20scene%20hypergraph%20that%20encodes%20rich%0Aspatial%20relationships%2C%20including%20unary%2C%20binary%2C%20and%20higher-order%20constraints.%0ATogether%2C%20these%20components%20provide%20the%20VLM%20with%20a%20structured%2C%20geometry-aware%0Aworking%20memory%20that%20integrates%20its%20inherent%20multimodal%20reasoning%20capabilities%0Awith%20structured%203D%20understanding%20for%20effective%20spatial%20reasoning.%20Building%20on%0Athis%20foundation%2C%20we%20develop%20an%20agentic%203D%20scene%20generation%20pipeline%20in%20which%0Athe%20VLM%20iteratively%20reads%20from%20and%20updates%20the%20spatial%20context.%20The%20pipeline%0Afeatures%20high-quality%20asset%20generation%20with%20geometric%20restoration%2C%20environment%0Asetup%20with%20automatic%20verification%2C%20and%20ergonomic%20adjustment%20guided%20by%20the%20scene%0Ahypergraph.%20Experiments%20show%20that%20our%20framework%20can%20handle%20diverse%20and%0Achallenging%20inputs%2C%20achieving%20a%20level%20of%20generalization%20not%20observed%20in%20prior%0Awork.%20Further%20results%20demonstrate%20that%20injecting%20spatial%20context%20enables%20VLMs%0Ato%20perform%20downstream%20tasks%20such%20as%20interactive%20scene%20editing%20and%20path%0Aplanning%2C%20suggesting%20strong%20potential%20for%20spatially%20intelligent%20systems%20in%0Acomputer%20graphics%2C%203D%20vision%2C%20and%20embodied%20applications.%20Project%20page%3A%0Ahttps%3A//spatctxvlm.github.io/project_page/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20129v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentic%25203D%2520Scene%2520Generation%2520with%2520Spatially%2520Contextualized%2520VLMs%26entry.906535625%3DXinhang%2520Liu%2520and%2520Yu-Wing%2520Tai%2520and%2520Chi-Keung%2520Tang%26entry.1292438233%3D%2520%2520Despite%2520recent%2520advances%2520in%2520multimodal%2520content%2520generation%2520enabled%2520by%250Avision-language%2520models%2520%2528VLMs%2529%252C%2520their%2520ability%2520to%2520reason%2520about%2520and%2520generate%250Astructured%25203D%2520scenes%2520remains%2520largely%2520underexplored.%2520This%2520limitation%2520constrains%250Atheir%2520utility%2520in%2520spatially%2520grounded%2520tasks%2520such%2520as%2520embodied%2520AI%252C%2520immersive%250Asimulations%252C%2520and%2520interactive%25203D%2520applications.%2520We%2520introduce%2520a%2520new%2520paradigm%2520that%250Aenables%2520VLMs%2520to%2520generate%252C%2520understand%252C%2520and%2520edit%2520complex%25203D%2520environments%2520by%250Ainjecting%2520a%2520continually%2520evolving%2520spatial%2520context.%2520Constructed%2520from%2520multimodal%250Ainput%252C%2520this%2520context%2520consists%2520of%2520three%2520components%253A%2520a%2520scene%2520portrait%2520that%250Aprovides%2520a%2520high-level%2520semantic%2520blueprint%252C%2520a%2520semantically%2520labeled%2520point%2520cloud%250Acapturing%2520object-level%2520geometry%252C%2520and%2520a%2520scene%2520hypergraph%2520that%2520encodes%2520rich%250Aspatial%2520relationships%252C%2520including%2520unary%252C%2520binary%252C%2520and%2520higher-order%2520constraints.%250ATogether%252C%2520these%2520components%2520provide%2520the%2520VLM%2520with%2520a%2520structured%252C%2520geometry-aware%250Aworking%2520memory%2520that%2520integrates%2520its%2520inherent%2520multimodal%2520reasoning%2520capabilities%250Awith%2520structured%25203D%2520understanding%2520for%2520effective%2520spatial%2520reasoning.%2520Building%2520on%250Athis%2520foundation%252C%2520we%2520develop%2520an%2520agentic%25203D%2520scene%2520generation%2520pipeline%2520in%2520which%250Athe%2520VLM%2520iteratively%2520reads%2520from%2520and%2520updates%2520the%2520spatial%2520context.%2520The%2520pipeline%250Afeatures%2520high-quality%2520asset%2520generation%2520with%2520geometric%2520restoration%252C%2520environment%250Asetup%2520with%2520automatic%2520verification%252C%2520and%2520ergonomic%2520adjustment%2520guided%2520by%2520the%2520scene%250Ahypergraph.%2520Experiments%2520show%2520that%2520our%2520framework%2520can%2520handle%2520diverse%2520and%250Achallenging%2520inputs%252C%2520achieving%2520a%2520level%2520of%2520generalization%2520not%2520observed%2520in%2520prior%250Awork.%2520Further%2520results%2520demonstrate%2520that%2520injecting%2520spatial%2520context%2520enables%2520VLMs%250Ato%2520perform%2520downstream%2520tasks%2520such%2520as%2520interactive%2520scene%2520editing%2520and%2520path%250Aplanning%252C%2520suggesting%2520strong%2520potential%2520for%2520spatially%2520intelligent%2520systems%2520in%250Acomputer%2520graphics%252C%25203D%2520vision%252C%2520and%2520embodied%2520applications.%2520Project%2520page%253A%250Ahttps%253A//spatctxvlm.github.io/project_page/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20129v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agentic%203D%20Scene%20Generation%20with%20Spatially%20Contextualized%20VLMs&entry.906535625=Xinhang%20Liu%20and%20Yu-Wing%20Tai%20and%20Chi-Keung%20Tang&entry.1292438233=%20%20Despite%20recent%20advances%20in%20multimodal%20content%20generation%20enabled%20by%0Avision-language%20models%20%28VLMs%29%2C%20their%20ability%20to%20reason%20about%20and%20generate%0Astructured%203D%20scenes%20remains%20largely%20underexplored.%20This%20limitation%20constrains%0Atheir%20utility%20in%20spatially%20grounded%20tasks%20such%20as%20embodied%20AI%2C%20immersive%0Asimulations%2C%20and%20interactive%203D%20applications.%20We%20introduce%20a%20new%20paradigm%20that%0Aenables%20VLMs%20to%20generate%2C%20understand%2C%20and%20edit%20complex%203D%20environments%20by%0Ainjecting%20a%20continually%20evolving%20spatial%20context.%20Constructed%20from%20multimodal%0Ainput%2C%20this%20context%20consists%20of%20three%20components%3A%20a%20scene%20portrait%20that%0Aprovides%20a%20high-level%20semantic%20blueprint%2C%20a%20semantically%20labeled%20point%20cloud%0Acapturing%20object-level%20geometry%2C%20and%20a%20scene%20hypergraph%20that%20encodes%20rich%0Aspatial%20relationships%2C%20including%20unary%2C%20binary%2C%20and%20higher-order%20constraints.%0ATogether%2C%20these%20components%20provide%20the%20VLM%20with%20a%20structured%2C%20geometry-aware%0Aworking%20memory%20that%20integrates%20its%20inherent%20multimodal%20reasoning%20capabilities%0Awith%20structured%203D%20understanding%20for%20effective%20spatial%20reasoning.%20Building%20on%0Athis%20foundation%2C%20we%20develop%20an%20agentic%203D%20scene%20generation%20pipeline%20in%20which%0Athe%20VLM%20iteratively%20reads%20from%20and%20updates%20the%20spatial%20context.%20The%20pipeline%0Afeatures%20high-quality%20asset%20generation%20with%20geometric%20restoration%2C%20environment%0Asetup%20with%20automatic%20verification%2C%20and%20ergonomic%20adjustment%20guided%20by%20the%20scene%0Ahypergraph.%20Experiments%20show%20that%20our%20framework%20can%20handle%20diverse%20and%0Achallenging%20inputs%2C%20achieving%20a%20level%20of%20generalization%20not%20observed%20in%20prior%0Awork.%20Further%20results%20demonstrate%20that%20injecting%20spatial%20context%20enables%20VLMs%0Ato%20perform%20downstream%20tasks%20such%20as%20interactive%20scene%20editing%20and%20path%0Aplanning%2C%20suggesting%20strong%20potential%20for%20spatially%20intelligent%20systems%20in%0Acomputer%20graphics%2C%203D%20vision%2C%20and%20embodied%20applications.%20Project%20page%3A%0Ahttps%3A//spatctxvlm.github.io/project_page/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20129v2&entry.124074799=Read"},
{"title": "SAFE: Finding Sparse and Flat Minima to Improve Pruning", "author": "Dongyeop Lee and Kwanhee Lee and Jinseok Chung and Namhoon Lee", "abstract": "  Sparsifying neural networks often suffers from seemingly inevitable\nperformance degradation, and it remains challenging to restore the original\nperformance despite much recent progress. Motivated by recent studies in robust\noptimization, we aim to tackle this problem by finding subnetworks that are\nboth sparse and flat at the same time. Specifically, we formulate pruning as a\nsparsity-constrained optimization problem where flatness is encouraged as an\nobjective. We solve it explicitly via an augmented Lagrange dual approach and\nextend it further by proposing a generalized projection operation, resulting in\nnovel pruning methods called SAFE and its extension, SAFE$^+$. Extensive\nevaluations on standard image classification and language modeling tasks reveal\nthat SAFE consistently yields sparse networks with improved generalization\nperformance, which compares competitively to well-established baselines. In\naddition, SAFE demonstrates resilience to noisy data, making it well-suited for\nreal-world conditions.\n", "link": "http://arxiv.org/abs/2506.06866v2", "date": "2025-06-16", "relevancy": 2.5368, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5148}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5066}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAFE%3A%20Finding%20Sparse%20and%20Flat%20Minima%20to%20Improve%20Pruning&body=Title%3A%20SAFE%3A%20Finding%20Sparse%20and%20Flat%20Minima%20to%20Improve%20Pruning%0AAuthor%3A%20Dongyeop%20Lee%20and%20Kwanhee%20Lee%20and%20Jinseok%20Chung%20and%20Namhoon%20Lee%0AAbstract%3A%20%20%20Sparsifying%20neural%20networks%20often%20suffers%20from%20seemingly%20inevitable%0Aperformance%20degradation%2C%20and%20it%20remains%20challenging%20to%20restore%20the%20original%0Aperformance%20despite%20much%20recent%20progress.%20Motivated%20by%20recent%20studies%20in%20robust%0Aoptimization%2C%20we%20aim%20to%20tackle%20this%20problem%20by%20finding%20subnetworks%20that%20are%0Aboth%20sparse%20and%20flat%20at%20the%20same%20time.%20Specifically%2C%20we%20formulate%20pruning%20as%20a%0Asparsity-constrained%20optimization%20problem%20where%20flatness%20is%20encouraged%20as%20an%0Aobjective.%20We%20solve%20it%20explicitly%20via%20an%20augmented%20Lagrange%20dual%20approach%20and%0Aextend%20it%20further%20by%20proposing%20a%20generalized%20projection%20operation%2C%20resulting%20in%0Anovel%20pruning%20methods%20called%20SAFE%20and%20its%20extension%2C%20SAFE%24%5E%2B%24.%20Extensive%0Aevaluations%20on%20standard%20image%20classification%20and%20language%20modeling%20tasks%20reveal%0Athat%20SAFE%20consistently%20yields%20sparse%20networks%20with%20improved%20generalization%0Aperformance%2C%20which%20compares%20competitively%20to%20well-established%20baselines.%20In%0Aaddition%2C%20SAFE%20demonstrates%20resilience%20to%20noisy%20data%2C%20making%20it%20well-suited%20for%0Areal-world%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06866v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAFE%253A%2520Finding%2520Sparse%2520and%2520Flat%2520Minima%2520to%2520Improve%2520Pruning%26entry.906535625%3DDongyeop%2520Lee%2520and%2520Kwanhee%2520Lee%2520and%2520Jinseok%2520Chung%2520and%2520Namhoon%2520Lee%26entry.1292438233%3D%2520%2520Sparsifying%2520neural%2520networks%2520often%2520suffers%2520from%2520seemingly%2520inevitable%250Aperformance%2520degradation%252C%2520and%2520it%2520remains%2520challenging%2520to%2520restore%2520the%2520original%250Aperformance%2520despite%2520much%2520recent%2520progress.%2520Motivated%2520by%2520recent%2520studies%2520in%2520robust%250Aoptimization%252C%2520we%2520aim%2520to%2520tackle%2520this%2520problem%2520by%2520finding%2520subnetworks%2520that%2520are%250Aboth%2520sparse%2520and%2520flat%2520at%2520the%2520same%2520time.%2520Specifically%252C%2520we%2520formulate%2520pruning%2520as%2520a%250Asparsity-constrained%2520optimization%2520problem%2520where%2520flatness%2520is%2520encouraged%2520as%2520an%250Aobjective.%2520We%2520solve%2520it%2520explicitly%2520via%2520an%2520augmented%2520Lagrange%2520dual%2520approach%2520and%250Aextend%2520it%2520further%2520by%2520proposing%2520a%2520generalized%2520projection%2520operation%252C%2520resulting%2520in%250Anovel%2520pruning%2520methods%2520called%2520SAFE%2520and%2520its%2520extension%252C%2520SAFE%2524%255E%252B%2524.%2520Extensive%250Aevaluations%2520on%2520standard%2520image%2520classification%2520and%2520language%2520modeling%2520tasks%2520reveal%250Athat%2520SAFE%2520consistently%2520yields%2520sparse%2520networks%2520with%2520improved%2520generalization%250Aperformance%252C%2520which%2520compares%2520competitively%2520to%2520well-established%2520baselines.%2520In%250Aaddition%252C%2520SAFE%2520demonstrates%2520resilience%2520to%2520noisy%2520data%252C%2520making%2520it%2520well-suited%2520for%250Areal-world%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06866v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAFE%3A%20Finding%20Sparse%20and%20Flat%20Minima%20to%20Improve%20Pruning&entry.906535625=Dongyeop%20Lee%20and%20Kwanhee%20Lee%20and%20Jinseok%20Chung%20and%20Namhoon%20Lee&entry.1292438233=%20%20Sparsifying%20neural%20networks%20often%20suffers%20from%20seemingly%20inevitable%0Aperformance%20degradation%2C%20and%20it%20remains%20challenging%20to%20restore%20the%20original%0Aperformance%20despite%20much%20recent%20progress.%20Motivated%20by%20recent%20studies%20in%20robust%0Aoptimization%2C%20we%20aim%20to%20tackle%20this%20problem%20by%20finding%20subnetworks%20that%20are%0Aboth%20sparse%20and%20flat%20at%20the%20same%20time.%20Specifically%2C%20we%20formulate%20pruning%20as%20a%0Asparsity-constrained%20optimization%20problem%20where%20flatness%20is%20encouraged%20as%20an%0Aobjective.%20We%20solve%20it%20explicitly%20via%20an%20augmented%20Lagrange%20dual%20approach%20and%0Aextend%20it%20further%20by%20proposing%20a%20generalized%20projection%20operation%2C%20resulting%20in%0Anovel%20pruning%20methods%20called%20SAFE%20and%20its%20extension%2C%20SAFE%24%5E%2B%24.%20Extensive%0Aevaluations%20on%20standard%20image%20classification%20and%20language%20modeling%20tasks%20reveal%0Athat%20SAFE%20consistently%20yields%20sparse%20networks%20with%20improved%20generalization%0Aperformance%2C%20which%20compares%20competitively%20to%20well-established%20baselines.%20In%0Aaddition%2C%20SAFE%20demonstrates%20resilience%20to%20noisy%20data%2C%20making%20it%20well-suited%20for%0Areal-world%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06866v2&entry.124074799=Read"},
{"title": "Position: Pause Recycling LoRAs and Prioritize Mechanisms to Uncover\n  Limits and Effectiveness", "author": "Mei-Yen Chen and Thi Thu Uyen Hoang and Michael Hahn and M. Saquib Sarfraz", "abstract": "  Merging or routing low-rank adapters (LoRAs) has emerged as a popular\nsolution for enhancing large language models, particularly when data access is\nrestricted by regulatory or domain-specific constraints. This position paper\nargues that the research community should shift its focus from developing new\nmerging or routing algorithms to understanding the conditions under which\nreusing LoRAs is truly effective. Through theoretical analysis and synthetic\ntwo-hop reasoning and math word-problem tasks, we examine whether reusing LoRAs\nenables genuine compositional generalization or merely reflects shallow pattern\nmatching. Evaluating two data-agnostic methods--parameter averaging and dynamic\nadapter selection--we found that reusing LoRAs often fails to logically\nintegrate knowledge across disjoint fine-tuning datasets, especially when such\nknowledge is underrepresented during pretraining. Our empirical results,\nsupported by theoretical insights into LoRA's limited expressiveness, highlight\nthe preconditions and constraints of reusing them for unseen tasks and cast\ndoubt on its feasibility as a truly data-free approach. We advocate for pausing\nthe pursuit of novel methods for recycling LoRAs and emphasize the need for\nrigorous mechanisms to guide future academic research in adapter-based model\nmerging and practical system designs for practitioners.\n", "link": "http://arxiv.org/abs/2506.13479v1", "date": "2025-06-16", "relevancy": 2.5268, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5109}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5026}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Position%3A%20Pause%20Recycling%20LoRAs%20and%20Prioritize%20Mechanisms%20to%20Uncover%0A%20%20Limits%20and%20Effectiveness&body=Title%3A%20Position%3A%20Pause%20Recycling%20LoRAs%20and%20Prioritize%20Mechanisms%20to%20Uncover%0A%20%20Limits%20and%20Effectiveness%0AAuthor%3A%20Mei-Yen%20Chen%20and%20Thi%20Thu%20Uyen%20Hoang%20and%20Michael%20Hahn%20and%20M.%20Saquib%20Sarfraz%0AAbstract%3A%20%20%20Merging%20or%20routing%20low-rank%20adapters%20%28LoRAs%29%20has%20emerged%20as%20a%20popular%0Asolution%20for%20enhancing%20large%20language%20models%2C%20particularly%20when%20data%20access%20is%0Arestricted%20by%20regulatory%20or%20domain-specific%20constraints.%20This%20position%20paper%0Aargues%20that%20the%20research%20community%20should%20shift%20its%20focus%20from%20developing%20new%0Amerging%20or%20routing%20algorithms%20to%20understanding%20the%20conditions%20under%20which%0Areusing%20LoRAs%20is%20truly%20effective.%20Through%20theoretical%20analysis%20and%20synthetic%0Atwo-hop%20reasoning%20and%20math%20word-problem%20tasks%2C%20we%20examine%20whether%20reusing%20LoRAs%0Aenables%20genuine%20compositional%20generalization%20or%20merely%20reflects%20shallow%20pattern%0Amatching.%20Evaluating%20two%20data-agnostic%20methods--parameter%20averaging%20and%20dynamic%0Aadapter%20selection--we%20found%20that%20reusing%20LoRAs%20often%20fails%20to%20logically%0Aintegrate%20knowledge%20across%20disjoint%20fine-tuning%20datasets%2C%20especially%20when%20such%0Aknowledge%20is%20underrepresented%20during%20pretraining.%20Our%20empirical%20results%2C%0Asupported%20by%20theoretical%20insights%20into%20LoRA%27s%20limited%20expressiveness%2C%20highlight%0Athe%20preconditions%20and%20constraints%20of%20reusing%20them%20for%20unseen%20tasks%20and%20cast%0Adoubt%20on%20its%20feasibility%20as%20a%20truly%20data-free%20approach.%20We%20advocate%20for%20pausing%0Athe%20pursuit%20of%20novel%20methods%20for%20recycling%20LoRAs%20and%20emphasize%20the%20need%20for%0Arigorous%20mechanisms%20to%20guide%20future%20academic%20research%20in%20adapter-based%20model%0Amerging%20and%20practical%20system%20designs%20for%20practitioners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13479v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosition%253A%2520Pause%2520Recycling%2520LoRAs%2520and%2520Prioritize%2520Mechanisms%2520to%2520Uncover%250A%2520%2520Limits%2520and%2520Effectiveness%26entry.906535625%3DMei-Yen%2520Chen%2520and%2520Thi%2520Thu%2520Uyen%2520Hoang%2520and%2520Michael%2520Hahn%2520and%2520M.%2520Saquib%2520Sarfraz%26entry.1292438233%3D%2520%2520Merging%2520or%2520routing%2520low-rank%2520adapters%2520%2528LoRAs%2529%2520has%2520emerged%2520as%2520a%2520popular%250Asolution%2520for%2520enhancing%2520large%2520language%2520models%252C%2520particularly%2520when%2520data%2520access%2520is%250Arestricted%2520by%2520regulatory%2520or%2520domain-specific%2520constraints.%2520This%2520position%2520paper%250Aargues%2520that%2520the%2520research%2520community%2520should%2520shift%2520its%2520focus%2520from%2520developing%2520new%250Amerging%2520or%2520routing%2520algorithms%2520to%2520understanding%2520the%2520conditions%2520under%2520which%250Areusing%2520LoRAs%2520is%2520truly%2520effective.%2520Through%2520theoretical%2520analysis%2520and%2520synthetic%250Atwo-hop%2520reasoning%2520and%2520math%2520word-problem%2520tasks%252C%2520we%2520examine%2520whether%2520reusing%2520LoRAs%250Aenables%2520genuine%2520compositional%2520generalization%2520or%2520merely%2520reflects%2520shallow%2520pattern%250Amatching.%2520Evaluating%2520two%2520data-agnostic%2520methods--parameter%2520averaging%2520and%2520dynamic%250Aadapter%2520selection--we%2520found%2520that%2520reusing%2520LoRAs%2520often%2520fails%2520to%2520logically%250Aintegrate%2520knowledge%2520across%2520disjoint%2520fine-tuning%2520datasets%252C%2520especially%2520when%2520such%250Aknowledge%2520is%2520underrepresented%2520during%2520pretraining.%2520Our%2520empirical%2520results%252C%250Asupported%2520by%2520theoretical%2520insights%2520into%2520LoRA%2527s%2520limited%2520expressiveness%252C%2520highlight%250Athe%2520preconditions%2520and%2520constraints%2520of%2520reusing%2520them%2520for%2520unseen%2520tasks%2520and%2520cast%250Adoubt%2520on%2520its%2520feasibility%2520as%2520a%2520truly%2520data-free%2520approach.%2520We%2520advocate%2520for%2520pausing%250Athe%2520pursuit%2520of%2520novel%2520methods%2520for%2520recycling%2520LoRAs%2520and%2520emphasize%2520the%2520need%2520for%250Arigorous%2520mechanisms%2520to%2520guide%2520future%2520academic%2520research%2520in%2520adapter-based%2520model%250Amerging%2520and%2520practical%2520system%2520designs%2520for%2520practitioners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13479v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Position%3A%20Pause%20Recycling%20LoRAs%20and%20Prioritize%20Mechanisms%20to%20Uncover%0A%20%20Limits%20and%20Effectiveness&entry.906535625=Mei-Yen%20Chen%20and%20Thi%20Thu%20Uyen%20Hoang%20and%20Michael%20Hahn%20and%20M.%20Saquib%20Sarfraz&entry.1292438233=%20%20Merging%20or%20routing%20low-rank%20adapters%20%28LoRAs%29%20has%20emerged%20as%20a%20popular%0Asolution%20for%20enhancing%20large%20language%20models%2C%20particularly%20when%20data%20access%20is%0Arestricted%20by%20regulatory%20or%20domain-specific%20constraints.%20This%20position%20paper%0Aargues%20that%20the%20research%20community%20should%20shift%20its%20focus%20from%20developing%20new%0Amerging%20or%20routing%20algorithms%20to%20understanding%20the%20conditions%20under%20which%0Areusing%20LoRAs%20is%20truly%20effective.%20Through%20theoretical%20analysis%20and%20synthetic%0Atwo-hop%20reasoning%20and%20math%20word-problem%20tasks%2C%20we%20examine%20whether%20reusing%20LoRAs%0Aenables%20genuine%20compositional%20generalization%20or%20merely%20reflects%20shallow%20pattern%0Amatching.%20Evaluating%20two%20data-agnostic%20methods--parameter%20averaging%20and%20dynamic%0Aadapter%20selection--we%20found%20that%20reusing%20LoRAs%20often%20fails%20to%20logically%0Aintegrate%20knowledge%20across%20disjoint%20fine-tuning%20datasets%2C%20especially%20when%20such%0Aknowledge%20is%20underrepresented%20during%20pretraining.%20Our%20empirical%20results%2C%0Asupported%20by%20theoretical%20insights%20into%20LoRA%27s%20limited%20expressiveness%2C%20highlight%0Athe%20preconditions%20and%20constraints%20of%20reusing%20them%20for%20unseen%20tasks%20and%20cast%0Adoubt%20on%20its%20feasibility%20as%20a%20truly%20data-free%20approach.%20We%20advocate%20for%20pausing%0Athe%20pursuit%20of%20novel%20methods%20for%20recycling%20LoRAs%20and%20emphasize%20the%20need%20for%0Arigorous%20mechanisms%20to%20guide%20future%20academic%20research%20in%20adapter-based%20model%0Amerging%20and%20practical%20system%20designs%20for%20practitioners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13479v1&entry.124074799=Read"},
{"title": "PRO: Projection Domain Synthesis for CT Imaging", "author": "Kang Chen and Bin Huang and Xuebin Yang and Junyan Zhang and Qiegen Liu", "abstract": "  Synthesizing high quality CT images remains a signifi-cant challenge due to\nthe limited availability of annotat-ed data and the complex nature of CT\nimaging. In this work, we present PRO, a novel framework that, to the best of\nour knowledge, is the first to perform CT image synthesis in the projection\ndomain using latent diffusion models. Unlike previous approaches that operate\nin the image domain, PRO learns rich structural representa-tions from raw\nprojection data and leverages anatomi-cal text prompts for controllable\nsynthesis. This projec-tion domain strategy enables more faithful modeling of\nunderlying imaging physics and anatomical structures. Moreover, PRO functions\nas a foundation model, capa-ble of generalizing across diverse downstream tasks\nby adjusting its generative behavior via prompt inputs. Experimental results\ndemonstrated that incorporating our synthesized data significantly improves\nperfor-mance across multiple downstream tasks, including low-dose and\nsparse-view reconstruction, even with limited training data. These findings\nunderscore the versatility and scalability of PRO in data generation for\nvarious CT applications. These results highlight the potential of projection\ndomain synthesis as a powerful tool for data augmentation and robust CT\nimaging. Our source code is publicly available at:\nhttps://github.com/yqx7150/PRO.\n", "link": "http://arxiv.org/abs/2506.13443v1", "date": "2025-06-16", "relevancy": 2.5237, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6366}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6366}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRO%3A%20Projection%20Domain%20Synthesis%20for%20CT%20Imaging&body=Title%3A%20PRO%3A%20Projection%20Domain%20Synthesis%20for%20CT%20Imaging%0AAuthor%3A%20Kang%20Chen%20and%20Bin%20Huang%20and%20Xuebin%20Yang%20and%20Junyan%20Zhang%20and%20Qiegen%20Liu%0AAbstract%3A%20%20%20Synthesizing%20high%20quality%20CT%20images%20remains%20a%20signifi-cant%20challenge%20due%20to%0Athe%20limited%20availability%20of%20annotat-ed%20data%20and%20the%20complex%20nature%20of%20CT%0Aimaging.%20In%20this%20work%2C%20we%20present%20PRO%2C%20a%20novel%20framework%20that%2C%20to%20the%20best%20of%0Aour%20knowledge%2C%20is%20the%20first%20to%20perform%20CT%20image%20synthesis%20in%20the%20projection%0Adomain%20using%20latent%20diffusion%20models.%20Unlike%20previous%20approaches%20that%20operate%0Ain%20the%20image%20domain%2C%20PRO%20learns%20rich%20structural%20representa-tions%20from%20raw%0Aprojection%20data%20and%20leverages%20anatomi-cal%20text%20prompts%20for%20controllable%0Asynthesis.%20This%20projec-tion%20domain%20strategy%20enables%20more%20faithful%20modeling%20of%0Aunderlying%20imaging%20physics%20and%20anatomical%20structures.%20Moreover%2C%20PRO%20functions%0Aas%20a%20foundation%20model%2C%20capa-ble%20of%20generalizing%20across%20diverse%20downstream%20tasks%0Aby%20adjusting%20its%20generative%20behavior%20via%20prompt%20inputs.%20Experimental%20results%0Ademonstrated%20that%20incorporating%20our%20synthesized%20data%20significantly%20improves%0Aperfor-mance%20across%20multiple%20downstream%20tasks%2C%20including%20low-dose%20and%0Asparse-view%20reconstruction%2C%20even%20with%20limited%20training%20data.%20These%20findings%0Aunderscore%20the%20versatility%20and%20scalability%20of%20PRO%20in%20data%20generation%20for%0Avarious%20CT%20applications.%20These%20results%20highlight%20the%20potential%20of%20projection%0Adomain%20synthesis%20as%20a%20powerful%20tool%20for%20data%20augmentation%20and%20robust%20CT%0Aimaging.%20Our%20source%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/yqx7150/PRO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRO%253A%2520Projection%2520Domain%2520Synthesis%2520for%2520CT%2520Imaging%26entry.906535625%3DKang%2520Chen%2520and%2520Bin%2520Huang%2520and%2520Xuebin%2520Yang%2520and%2520Junyan%2520Zhang%2520and%2520Qiegen%2520Liu%26entry.1292438233%3D%2520%2520Synthesizing%2520high%2520quality%2520CT%2520images%2520remains%2520a%2520signifi-cant%2520challenge%2520due%2520to%250Athe%2520limited%2520availability%2520of%2520annotat-ed%2520data%2520and%2520the%2520complex%2520nature%2520of%2520CT%250Aimaging.%2520In%2520this%2520work%252C%2520we%2520present%2520PRO%252C%2520a%2520novel%2520framework%2520that%252C%2520to%2520the%2520best%2520of%250Aour%2520knowledge%252C%2520is%2520the%2520first%2520to%2520perform%2520CT%2520image%2520synthesis%2520in%2520the%2520projection%250Adomain%2520using%2520latent%2520diffusion%2520models.%2520Unlike%2520previous%2520approaches%2520that%2520operate%250Ain%2520the%2520image%2520domain%252C%2520PRO%2520learns%2520rich%2520structural%2520representa-tions%2520from%2520raw%250Aprojection%2520data%2520and%2520leverages%2520anatomi-cal%2520text%2520prompts%2520for%2520controllable%250Asynthesis.%2520This%2520projec-tion%2520domain%2520strategy%2520enables%2520more%2520faithful%2520modeling%2520of%250Aunderlying%2520imaging%2520physics%2520and%2520anatomical%2520structures.%2520Moreover%252C%2520PRO%2520functions%250Aas%2520a%2520foundation%2520model%252C%2520capa-ble%2520of%2520generalizing%2520across%2520diverse%2520downstream%2520tasks%250Aby%2520adjusting%2520its%2520generative%2520behavior%2520via%2520prompt%2520inputs.%2520Experimental%2520results%250Ademonstrated%2520that%2520incorporating%2520our%2520synthesized%2520data%2520significantly%2520improves%250Aperfor-mance%2520across%2520multiple%2520downstream%2520tasks%252C%2520including%2520low-dose%2520and%250Asparse-view%2520reconstruction%252C%2520even%2520with%2520limited%2520training%2520data.%2520These%2520findings%250Aunderscore%2520the%2520versatility%2520and%2520scalability%2520of%2520PRO%2520in%2520data%2520generation%2520for%250Avarious%2520CT%2520applications.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520projection%250Adomain%2520synthesis%2520as%2520a%2520powerful%2520tool%2520for%2520data%2520augmentation%2520and%2520robust%2520CT%250Aimaging.%2520Our%2520source%2520code%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/yqx7150/PRO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRO%3A%20Projection%20Domain%20Synthesis%20for%20CT%20Imaging&entry.906535625=Kang%20Chen%20and%20Bin%20Huang%20and%20Xuebin%20Yang%20and%20Junyan%20Zhang%20and%20Qiegen%20Liu&entry.1292438233=%20%20Synthesizing%20high%20quality%20CT%20images%20remains%20a%20signifi-cant%20challenge%20due%20to%0Athe%20limited%20availability%20of%20annotat-ed%20data%20and%20the%20complex%20nature%20of%20CT%0Aimaging.%20In%20this%20work%2C%20we%20present%20PRO%2C%20a%20novel%20framework%20that%2C%20to%20the%20best%20of%0Aour%20knowledge%2C%20is%20the%20first%20to%20perform%20CT%20image%20synthesis%20in%20the%20projection%0Adomain%20using%20latent%20diffusion%20models.%20Unlike%20previous%20approaches%20that%20operate%0Ain%20the%20image%20domain%2C%20PRO%20learns%20rich%20structural%20representa-tions%20from%20raw%0Aprojection%20data%20and%20leverages%20anatomi-cal%20text%20prompts%20for%20controllable%0Asynthesis.%20This%20projec-tion%20domain%20strategy%20enables%20more%20faithful%20modeling%20of%0Aunderlying%20imaging%20physics%20and%20anatomical%20structures.%20Moreover%2C%20PRO%20functions%0Aas%20a%20foundation%20model%2C%20capa-ble%20of%20generalizing%20across%20diverse%20downstream%20tasks%0Aby%20adjusting%20its%20generative%20behavior%20via%20prompt%20inputs.%20Experimental%20results%0Ademonstrated%20that%20incorporating%20our%20synthesized%20data%20significantly%20improves%0Aperfor-mance%20across%20multiple%20downstream%20tasks%2C%20including%20low-dose%20and%0Asparse-view%20reconstruction%2C%20even%20with%20limited%20training%20data.%20These%20findings%0Aunderscore%20the%20versatility%20and%20scalability%20of%20PRO%20in%20data%20generation%20for%0Avarious%20CT%20applications.%20These%20results%20highlight%20the%20potential%20of%20projection%0Adomain%20synthesis%20as%20a%20powerful%20tool%20for%20data%20augmentation%20and%20robust%20CT%0Aimaging.%20Our%20source%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/yqx7150/PRO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13443v1&entry.124074799=Read"},
{"title": "Structureless VIO", "author": "Junlin Song and Miguel Olivares-Mendez", "abstract": "  Visual odometry (VO) is typically considered as a chicken-and-egg problem, as\nthe localization and mapping modules are tightly-coupled. The estimation of a\nvisual map relies on accurate localization information. Meanwhile, localization\nrequires precise map points to provide motion constraints. This classical\ndesign principle is naturally inherited by visual-inertial odometry (VIO).\nEfficient localization solutions that do not require a map have not been fully\ninvestigated. To this end, we propose a novel structureless VIO, where the\nvisual map is removed from the odometry framework. Experimental results\ndemonstrated that, compared to the structure-based VIO baseline, our\nstructureless VIO not only substantially improves computational efficiency but\nalso has advantages in accuracy.\n", "link": "http://arxiv.org/abs/2505.12337v2", "date": "2025-06-16", "relevancy": 2.517, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5143}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4993}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structureless%20VIO&body=Title%3A%20Structureless%20VIO%0AAuthor%3A%20Junlin%20Song%20and%20Miguel%20Olivares-Mendez%0AAbstract%3A%20%20%20Visual%20odometry%20%28VO%29%20is%20typically%20considered%20as%20a%20chicken-and-egg%20problem%2C%20as%0Athe%20localization%20and%20mapping%20modules%20are%20tightly-coupled.%20The%20estimation%20of%20a%0Avisual%20map%20relies%20on%20accurate%20localization%20information.%20Meanwhile%2C%20localization%0Arequires%20precise%20map%20points%20to%20provide%20motion%20constraints.%20This%20classical%0Adesign%20principle%20is%20naturally%20inherited%20by%20visual-inertial%20odometry%20%28VIO%29.%0AEfficient%20localization%20solutions%20that%20do%20not%20require%20a%20map%20have%20not%20been%20fully%0Ainvestigated.%20To%20this%20end%2C%20we%20propose%20a%20novel%20structureless%20VIO%2C%20where%20the%0Avisual%20map%20is%20removed%20from%20the%20odometry%20framework.%20Experimental%20results%0Ademonstrated%20that%2C%20compared%20to%20the%20structure-based%20VIO%20baseline%2C%20our%0Astructureless%20VIO%20not%20only%20substantially%20improves%20computational%20efficiency%20but%0Aalso%20has%20advantages%20in%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12337v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructureless%2520VIO%26entry.906535625%3DJunlin%2520Song%2520and%2520Miguel%2520Olivares-Mendez%26entry.1292438233%3D%2520%2520Visual%2520odometry%2520%2528VO%2529%2520is%2520typically%2520considered%2520as%2520a%2520chicken-and-egg%2520problem%252C%2520as%250Athe%2520localization%2520and%2520mapping%2520modules%2520are%2520tightly-coupled.%2520The%2520estimation%2520of%2520a%250Avisual%2520map%2520relies%2520on%2520accurate%2520localization%2520information.%2520Meanwhile%252C%2520localization%250Arequires%2520precise%2520map%2520points%2520to%2520provide%2520motion%2520constraints.%2520This%2520classical%250Adesign%2520principle%2520is%2520naturally%2520inherited%2520by%2520visual-inertial%2520odometry%2520%2528VIO%2529.%250AEfficient%2520localization%2520solutions%2520that%2520do%2520not%2520require%2520a%2520map%2520have%2520not%2520been%2520fully%250Ainvestigated.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520structureless%2520VIO%252C%2520where%2520the%250Avisual%2520map%2520is%2520removed%2520from%2520the%2520odometry%2520framework.%2520Experimental%2520results%250Ademonstrated%2520that%252C%2520compared%2520to%2520the%2520structure-based%2520VIO%2520baseline%252C%2520our%250Astructureless%2520VIO%2520not%2520only%2520substantially%2520improves%2520computational%2520efficiency%2520but%250Aalso%2520has%2520advantages%2520in%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12337v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structureless%20VIO&entry.906535625=Junlin%20Song%20and%20Miguel%20Olivares-Mendez&entry.1292438233=%20%20Visual%20odometry%20%28VO%29%20is%20typically%20considered%20as%20a%20chicken-and-egg%20problem%2C%20as%0Athe%20localization%20and%20mapping%20modules%20are%20tightly-coupled.%20The%20estimation%20of%20a%0Avisual%20map%20relies%20on%20accurate%20localization%20information.%20Meanwhile%2C%20localization%0Arequires%20precise%20map%20points%20to%20provide%20motion%20constraints.%20This%20classical%0Adesign%20principle%20is%20naturally%20inherited%20by%20visual-inertial%20odometry%20%28VIO%29.%0AEfficient%20localization%20solutions%20that%20do%20not%20require%20a%20map%20have%20not%20been%20fully%0Ainvestigated.%20To%20this%20end%2C%20we%20propose%20a%20novel%20structureless%20VIO%2C%20where%20the%0Avisual%20map%20is%20removed%20from%20the%20odometry%20framework.%20Experimental%20results%0Ademonstrated%20that%2C%20compared%20to%20the%20structure-based%20VIO%20baseline%2C%20our%0Astructureless%20VIO%20not%20only%20substantially%20improves%20computational%20efficiency%20but%0Aalso%20has%20advantages%20in%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12337v2&entry.124074799=Read"},
{"title": "Decompositional Reasoning for Graph Retrieval with Large Language Models", "author": "Valentin Six and Evan Dufraisse and Ga\u00ebl de Chalendar", "abstract": "  Large Language Models (LLMs) excel at many NLP tasks, but struggle with\nmulti-hop reasoning and factual consistency, limiting their effectiveness on\nknowledge-intensive tasks like complex question answering (QA). Linking\nKnowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally\nlack the ability to reason efficiently over graph-structured information. To\ntackle this problem, we propose a novel retrieval approach that integrates\ntextual knowledge graphs into the LLM reasoning process via query\ndecomposition. Our method decomposes complex questions into sub-questions,\nretrieves relevant textual subgraphs, and composes a question-specific\nknowledge graph to guide answer generation. For that, we use a weighted\nsimilarity function that focuses on both the complex question and the generated\nsubquestions to extract a relevant subgraph, which allows efficient and precise\nretrieval for complex questions and improves the performance of LLMs on\nmulti-hop QA tasks. This structured reasoning pipeline enhances factual\ngrounding and interpretability while leveraging the generative strengths of\nLLMs. We evaluate our method on standard multi-hop QA benchmarks and show that\nit achieves comparable or superior performance to competitive existing methods,\nusing smaller models and fewer LLM calls.\n", "link": "http://arxiv.org/abs/2506.13380v1", "date": "2025-06-16", "relevancy": 2.5027, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decompositional%20Reasoning%20for%20Graph%20Retrieval%20with%20Large%20Language%20Models&body=Title%3A%20Decompositional%20Reasoning%20for%20Graph%20Retrieval%20with%20Large%20Language%20Models%0AAuthor%3A%20Valentin%20Six%20and%20Evan%20Dufraisse%20and%20Ga%C3%ABl%20de%20Chalendar%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20at%20many%20NLP%20tasks%2C%20but%20struggle%20with%0Amulti-hop%20reasoning%20and%20factual%20consistency%2C%20limiting%20their%20effectiveness%20on%0Aknowledge-intensive%20tasks%20like%20complex%20question%20answering%20%28QA%29.%20Linking%0AKnowledge%20Graphs%20%28KG%29%20and%20LLMs%20has%20shown%20promising%20results%2C%20but%20LLMs%20generally%0Alack%20the%20ability%20to%20reason%20efficiently%20over%20graph-structured%20information.%20To%0Atackle%20this%20problem%2C%20we%20propose%20a%20novel%20retrieval%20approach%20that%20integrates%0Atextual%20knowledge%20graphs%20into%20the%20LLM%20reasoning%20process%20via%20query%0Adecomposition.%20Our%20method%20decomposes%20complex%20questions%20into%20sub-questions%2C%0Aretrieves%20relevant%20textual%20subgraphs%2C%20and%20composes%20a%20question-specific%0Aknowledge%20graph%20to%20guide%20answer%20generation.%20For%20that%2C%20we%20use%20a%20weighted%0Asimilarity%20function%20that%20focuses%20on%20both%20the%20complex%20question%20and%20the%20generated%0Asubquestions%20to%20extract%20a%20relevant%20subgraph%2C%20which%20allows%20efficient%20and%20precise%0Aretrieval%20for%20complex%20questions%20and%20improves%20the%20performance%20of%20LLMs%20on%0Amulti-hop%20QA%20tasks.%20This%20structured%20reasoning%20pipeline%20enhances%20factual%0Agrounding%20and%20interpretability%20while%20leveraging%20the%20generative%20strengths%20of%0ALLMs.%20We%20evaluate%20our%20method%20on%20standard%20multi-hop%20QA%20benchmarks%20and%20show%20that%0Ait%20achieves%20comparable%20or%20superior%20performance%20to%20competitive%20existing%20methods%2C%0Ausing%20smaller%20models%20and%20fewer%20LLM%20calls.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecompositional%2520Reasoning%2520for%2520Graph%2520Retrieval%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DValentin%2520Six%2520and%2520Evan%2520Dufraisse%2520and%2520Ga%25C3%25ABl%2520de%2520Chalendar%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520at%2520many%2520NLP%2520tasks%252C%2520but%2520struggle%2520with%250Amulti-hop%2520reasoning%2520and%2520factual%2520consistency%252C%2520limiting%2520their%2520effectiveness%2520on%250Aknowledge-intensive%2520tasks%2520like%2520complex%2520question%2520answering%2520%2528QA%2529.%2520Linking%250AKnowledge%2520Graphs%2520%2528KG%2529%2520and%2520LLMs%2520has%2520shown%2520promising%2520results%252C%2520but%2520LLMs%2520generally%250Alack%2520the%2520ability%2520to%2520reason%2520efficiently%2520over%2520graph-structured%2520information.%2520To%250Atackle%2520this%2520problem%252C%2520we%2520propose%2520a%2520novel%2520retrieval%2520approach%2520that%2520integrates%250Atextual%2520knowledge%2520graphs%2520into%2520the%2520LLM%2520reasoning%2520process%2520via%2520query%250Adecomposition.%2520Our%2520method%2520decomposes%2520complex%2520questions%2520into%2520sub-questions%252C%250Aretrieves%2520relevant%2520textual%2520subgraphs%252C%2520and%2520composes%2520a%2520question-specific%250Aknowledge%2520graph%2520to%2520guide%2520answer%2520generation.%2520For%2520that%252C%2520we%2520use%2520a%2520weighted%250Asimilarity%2520function%2520that%2520focuses%2520on%2520both%2520the%2520complex%2520question%2520and%2520the%2520generated%250Asubquestions%2520to%2520extract%2520a%2520relevant%2520subgraph%252C%2520which%2520allows%2520efficient%2520and%2520precise%250Aretrieval%2520for%2520complex%2520questions%2520and%2520improves%2520the%2520performance%2520of%2520LLMs%2520on%250Amulti-hop%2520QA%2520tasks.%2520This%2520structured%2520reasoning%2520pipeline%2520enhances%2520factual%250Agrounding%2520and%2520interpretability%2520while%2520leveraging%2520the%2520generative%2520strengths%2520of%250ALLMs.%2520We%2520evaluate%2520our%2520method%2520on%2520standard%2520multi-hop%2520QA%2520benchmarks%2520and%2520show%2520that%250Ait%2520achieves%2520comparable%2520or%2520superior%2520performance%2520to%2520competitive%2520existing%2520methods%252C%250Ausing%2520smaller%2520models%2520and%2520fewer%2520LLM%2520calls.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decompositional%20Reasoning%20for%20Graph%20Retrieval%20with%20Large%20Language%20Models&entry.906535625=Valentin%20Six%20and%20Evan%20Dufraisse%20and%20Ga%C3%ABl%20de%20Chalendar&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20at%20many%20NLP%20tasks%2C%20but%20struggle%20with%0Amulti-hop%20reasoning%20and%20factual%20consistency%2C%20limiting%20their%20effectiveness%20on%0Aknowledge-intensive%20tasks%20like%20complex%20question%20answering%20%28QA%29.%20Linking%0AKnowledge%20Graphs%20%28KG%29%20and%20LLMs%20has%20shown%20promising%20results%2C%20but%20LLMs%20generally%0Alack%20the%20ability%20to%20reason%20efficiently%20over%20graph-structured%20information.%20To%0Atackle%20this%20problem%2C%20we%20propose%20a%20novel%20retrieval%20approach%20that%20integrates%0Atextual%20knowledge%20graphs%20into%20the%20LLM%20reasoning%20process%20via%20query%0Adecomposition.%20Our%20method%20decomposes%20complex%20questions%20into%20sub-questions%2C%0Aretrieves%20relevant%20textual%20subgraphs%2C%20and%20composes%20a%20question-specific%0Aknowledge%20graph%20to%20guide%20answer%20generation.%20For%20that%2C%20we%20use%20a%20weighted%0Asimilarity%20function%20that%20focuses%20on%20both%20the%20complex%20question%20and%20the%20generated%0Asubquestions%20to%20extract%20a%20relevant%20subgraph%2C%20which%20allows%20efficient%20and%20precise%0Aretrieval%20for%20complex%20questions%20and%20improves%20the%20performance%20of%20LLMs%20on%0Amulti-hop%20QA%20tasks.%20This%20structured%20reasoning%20pipeline%20enhances%20factual%0Agrounding%20and%20interpretability%20while%20leveraging%20the%20generative%20strengths%20of%0ALLMs.%20We%20evaluate%20our%20method%20on%20standard%20multi-hop%20QA%20benchmarks%20and%20show%20that%0Ait%20achieves%20comparable%20or%20superior%20performance%20to%20competitive%20existing%20methods%2C%0Ausing%20smaller%20models%20and%20fewer%20LLM%20calls.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13380v1&entry.124074799=Read"},
{"title": "DicFace: Dirichlet-Constrained Variational Codebook Learning for\n  Temporally Coherent Video Face Restoration", "author": "Yan Chen and Hanlin Shang and Ce Liu and Yuxuan Chen and Hui Li and Weihao Yuan and Hao Zhu and Zilong Dong and Siyu Zhu", "abstract": "  Video face restoration faces a critical challenge in maintaining temporal\nconsistency while recovering fine facial details from degraded inputs. This\npaper presents a novel approach that extends Vector-Quantized Variational\nAutoencoders (VQ-VAEs), pretrained on static high-quality portraits, into a\nvideo restoration framework through variational latent space modeling. Our key\ninnovation lies in reformulating discrete codebook representations as\nDirichlet-distributed continuous variables, enabling probabilistic transitions\nbetween facial features across frames. A spatio-temporal Transformer\narchitecture jointly models inter-frame dependencies and predicts latent\ndistributions, while a Laplacian-constrained reconstruction loss combined with\nperceptual (LPIPS) regularization enhances both pixel accuracy and visual\nquality. Comprehensive evaluations on blind face restoration, video inpainting,\nand facial colorization tasks demonstrate state-of-the-art performance. This\nwork establishes an effective paradigm for adapting intensive image priors,\npretrained on high-quality images, to video restoration while addressing the\ncritical challenge of flicker artifacts. The source code has been open-sourced\nand is available at https://github.com/fudan-generative-vision/DicFace.\n", "link": "http://arxiv.org/abs/2506.13355v1", "date": "2025-06-16", "relevancy": 2.4729, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6328}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6235}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DicFace%3A%20Dirichlet-Constrained%20Variational%20Codebook%20Learning%20for%0A%20%20Temporally%20Coherent%20Video%20Face%20Restoration&body=Title%3A%20DicFace%3A%20Dirichlet-Constrained%20Variational%20Codebook%20Learning%20for%0A%20%20Temporally%20Coherent%20Video%20Face%20Restoration%0AAuthor%3A%20Yan%20Chen%20and%20Hanlin%20Shang%20and%20Ce%20Liu%20and%20Yuxuan%20Chen%20and%20Hui%20Li%20and%20Weihao%20Yuan%20and%20Hao%20Zhu%20and%20Zilong%20Dong%20and%20Siyu%20Zhu%0AAbstract%3A%20%20%20Video%20face%20restoration%20faces%20a%20critical%20challenge%20in%20maintaining%20temporal%0Aconsistency%20while%20recovering%20fine%20facial%20details%20from%20degraded%20inputs.%20This%0Apaper%20presents%20a%20novel%20approach%20that%20extends%20Vector-Quantized%20Variational%0AAutoencoders%20%28VQ-VAEs%29%2C%20pretrained%20on%20static%20high-quality%20portraits%2C%20into%20a%0Avideo%20restoration%20framework%20through%20variational%20latent%20space%20modeling.%20Our%20key%0Ainnovation%20lies%20in%20reformulating%20discrete%20codebook%20representations%20as%0ADirichlet-distributed%20continuous%20variables%2C%20enabling%20probabilistic%20transitions%0Abetween%20facial%20features%20across%20frames.%20A%20spatio-temporal%20Transformer%0Aarchitecture%20jointly%20models%20inter-frame%20dependencies%20and%20predicts%20latent%0Adistributions%2C%20while%20a%20Laplacian-constrained%20reconstruction%20loss%20combined%20with%0Aperceptual%20%28LPIPS%29%20regularization%20enhances%20both%20pixel%20accuracy%20and%20visual%0Aquality.%20Comprehensive%20evaluations%20on%20blind%20face%20restoration%2C%20video%20inpainting%2C%0Aand%20facial%20colorization%20tasks%20demonstrate%20state-of-the-art%20performance.%20This%0Awork%20establishes%20an%20effective%20paradigm%20for%20adapting%20intensive%20image%20priors%2C%0Apretrained%20on%20high-quality%20images%2C%20to%20video%20restoration%20while%20addressing%20the%0Acritical%20challenge%20of%20flicker%20artifacts.%20The%20source%20code%20has%20been%20open-sourced%0Aand%20is%20available%20at%20https%3A//github.com/fudan-generative-vision/DicFace.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13355v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDicFace%253A%2520Dirichlet-Constrained%2520Variational%2520Codebook%2520Learning%2520for%250A%2520%2520Temporally%2520Coherent%2520Video%2520Face%2520Restoration%26entry.906535625%3DYan%2520Chen%2520and%2520Hanlin%2520Shang%2520and%2520Ce%2520Liu%2520and%2520Yuxuan%2520Chen%2520and%2520Hui%2520Li%2520and%2520Weihao%2520Yuan%2520and%2520Hao%2520Zhu%2520and%2520Zilong%2520Dong%2520and%2520Siyu%2520Zhu%26entry.1292438233%3D%2520%2520Video%2520face%2520restoration%2520faces%2520a%2520critical%2520challenge%2520in%2520maintaining%2520temporal%250Aconsistency%2520while%2520recovering%2520fine%2520facial%2520details%2520from%2520degraded%2520inputs.%2520This%250Apaper%2520presents%2520a%2520novel%2520approach%2520that%2520extends%2520Vector-Quantized%2520Variational%250AAutoencoders%2520%2528VQ-VAEs%2529%252C%2520pretrained%2520on%2520static%2520high-quality%2520portraits%252C%2520into%2520a%250Avideo%2520restoration%2520framework%2520through%2520variational%2520latent%2520space%2520modeling.%2520Our%2520key%250Ainnovation%2520lies%2520in%2520reformulating%2520discrete%2520codebook%2520representations%2520as%250ADirichlet-distributed%2520continuous%2520variables%252C%2520enabling%2520probabilistic%2520transitions%250Abetween%2520facial%2520features%2520across%2520frames.%2520A%2520spatio-temporal%2520Transformer%250Aarchitecture%2520jointly%2520models%2520inter-frame%2520dependencies%2520and%2520predicts%2520latent%250Adistributions%252C%2520while%2520a%2520Laplacian-constrained%2520reconstruction%2520loss%2520combined%2520with%250Aperceptual%2520%2528LPIPS%2529%2520regularization%2520enhances%2520both%2520pixel%2520accuracy%2520and%2520visual%250Aquality.%2520Comprehensive%2520evaluations%2520on%2520blind%2520face%2520restoration%252C%2520video%2520inpainting%252C%250Aand%2520facial%2520colorization%2520tasks%2520demonstrate%2520state-of-the-art%2520performance.%2520This%250Awork%2520establishes%2520an%2520effective%2520paradigm%2520for%2520adapting%2520intensive%2520image%2520priors%252C%250Apretrained%2520on%2520high-quality%2520images%252C%2520to%2520video%2520restoration%2520while%2520addressing%2520the%250Acritical%2520challenge%2520of%2520flicker%2520artifacts.%2520The%2520source%2520code%2520has%2520been%2520open-sourced%250Aand%2520is%2520available%2520at%2520https%253A//github.com/fudan-generative-vision/DicFace.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13355v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DicFace%3A%20Dirichlet-Constrained%20Variational%20Codebook%20Learning%20for%0A%20%20Temporally%20Coherent%20Video%20Face%20Restoration&entry.906535625=Yan%20Chen%20and%20Hanlin%20Shang%20and%20Ce%20Liu%20and%20Yuxuan%20Chen%20and%20Hui%20Li%20and%20Weihao%20Yuan%20and%20Hao%20Zhu%20and%20Zilong%20Dong%20and%20Siyu%20Zhu&entry.1292438233=%20%20Video%20face%20restoration%20faces%20a%20critical%20challenge%20in%20maintaining%20temporal%0Aconsistency%20while%20recovering%20fine%20facial%20details%20from%20degraded%20inputs.%20This%0Apaper%20presents%20a%20novel%20approach%20that%20extends%20Vector-Quantized%20Variational%0AAutoencoders%20%28VQ-VAEs%29%2C%20pretrained%20on%20static%20high-quality%20portraits%2C%20into%20a%0Avideo%20restoration%20framework%20through%20variational%20latent%20space%20modeling.%20Our%20key%0Ainnovation%20lies%20in%20reformulating%20discrete%20codebook%20representations%20as%0ADirichlet-distributed%20continuous%20variables%2C%20enabling%20probabilistic%20transitions%0Abetween%20facial%20features%20across%20frames.%20A%20spatio-temporal%20Transformer%0Aarchitecture%20jointly%20models%20inter-frame%20dependencies%20and%20predicts%20latent%0Adistributions%2C%20while%20a%20Laplacian-constrained%20reconstruction%20loss%20combined%20with%0Aperceptual%20%28LPIPS%29%20regularization%20enhances%20both%20pixel%20accuracy%20and%20visual%0Aquality.%20Comprehensive%20evaluations%20on%20blind%20face%20restoration%2C%20video%20inpainting%2C%0Aand%20facial%20colorization%20tasks%20demonstrate%20state-of-the-art%20performance.%20This%0Awork%20establishes%20an%20effective%20paradigm%20for%20adapting%20intensive%20image%20priors%2C%0Apretrained%20on%20high-quality%20images%2C%20to%20video%20restoration%20while%20addressing%20the%0Acritical%20challenge%20of%20flicker%20artifacts.%20The%20source%20code%20has%20been%20open-sourced%0Aand%20is%20available%20at%20https%3A//github.com/fudan-generative-vision/DicFace.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13355v1&entry.124074799=Read"},
{"title": "Learned radio interferometric imaging for varying visibility coverage", "author": "Matthijs Mars and Marta M. Betcke and Jason D. McEwen", "abstract": "  With the next generation of interferometric telescopes, such as the Square\nKilometre Array (SKA), the need for highly computationally efficient\nreconstruction techniques is particularly acute. The challenge in designing\nlearned, data-driven reconstruction techniques for radio interferometry is that\nthey need to be agnostic to the varying visibility coverages of the telescope,\nsince these are different for each observation. Because of this, learned\npost-processing or learned unrolled iterative reconstruction methods must\ntypically be retrained for each specific observation, amounting to a large\ncomputational overhead. In this work we develop learned post-processing and\nunrolled iterative methods for varying visibility coverages, proposing training\nstrategies to make these methods agnostic to variations in visibility coverage\nwith minimal to no fine-tuning. Learned post-processing techniques are heavily\ndependent on the prior information encoded in training data and generalise\npoorly to other visibility coverages. In contrast, unrolled iterative methods,\nwhich include the telescope measurement operator inside the network, achieve\ngood reconstruction quality and computation time, generalising well to other\ncoverages and require little to no fine-tuning. Furthermore, they generalise\nwell to more realistic radio observations and are able to reconstruct images\nwith with a larger dynamic range than the training set.\n", "link": "http://arxiv.org/abs/2405.08958v2", "date": "2025-06-16", "relevancy": 2.4668, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4952}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4952}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learned%20radio%20interferometric%20imaging%20for%20varying%20visibility%20coverage&body=Title%3A%20Learned%20radio%20interferometric%20imaging%20for%20varying%20visibility%20coverage%0AAuthor%3A%20Matthijs%20Mars%20and%20Marta%20M.%20Betcke%20and%20Jason%20D.%20McEwen%0AAbstract%3A%20%20%20With%20the%20next%20generation%20of%20interferometric%20telescopes%2C%20such%20as%20the%20Square%0AKilometre%20Array%20%28SKA%29%2C%20the%20need%20for%20highly%20computationally%20efficient%0Areconstruction%20techniques%20is%20particularly%20acute.%20The%20challenge%20in%20designing%0Alearned%2C%20data-driven%20reconstruction%20techniques%20for%20radio%20interferometry%20is%20that%0Athey%20need%20to%20be%20agnostic%20to%20the%20varying%20visibility%20coverages%20of%20the%20telescope%2C%0Asince%20these%20are%20different%20for%20each%20observation.%20Because%20of%20this%2C%20learned%0Apost-processing%20or%20learned%20unrolled%20iterative%20reconstruction%20methods%20must%0Atypically%20be%20retrained%20for%20each%20specific%20observation%2C%20amounting%20to%20a%20large%0Acomputational%20overhead.%20In%20this%20work%20we%20develop%20learned%20post-processing%20and%0Aunrolled%20iterative%20methods%20for%20varying%20visibility%20coverages%2C%20proposing%20training%0Astrategies%20to%20make%20these%20methods%20agnostic%20to%20variations%20in%20visibility%20coverage%0Awith%20minimal%20to%20no%20fine-tuning.%20Learned%20post-processing%20techniques%20are%20heavily%0Adependent%20on%20the%20prior%20information%20encoded%20in%20training%20data%20and%20generalise%0Apoorly%20to%20other%20visibility%20coverages.%20In%20contrast%2C%20unrolled%20iterative%20methods%2C%0Awhich%20include%20the%20telescope%20measurement%20operator%20inside%20the%20network%2C%20achieve%0Agood%20reconstruction%20quality%20and%20computation%20time%2C%20generalising%20well%20to%20other%0Acoverages%20and%20require%20little%20to%20no%20fine-tuning.%20Furthermore%2C%20they%20generalise%0Awell%20to%20more%20realistic%20radio%20observations%20and%20are%20able%20to%20reconstruct%20images%0Awith%20with%20a%20larger%20dynamic%20range%20than%20the%20training%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08958v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearned%2520radio%2520interferometric%2520imaging%2520for%2520varying%2520visibility%2520coverage%26entry.906535625%3DMatthijs%2520Mars%2520and%2520Marta%2520M.%2520Betcke%2520and%2520Jason%2520D.%2520McEwen%26entry.1292438233%3D%2520%2520With%2520the%2520next%2520generation%2520of%2520interferometric%2520telescopes%252C%2520such%2520as%2520the%2520Square%250AKilometre%2520Array%2520%2528SKA%2529%252C%2520the%2520need%2520for%2520highly%2520computationally%2520efficient%250Areconstruction%2520techniques%2520is%2520particularly%2520acute.%2520The%2520challenge%2520in%2520designing%250Alearned%252C%2520data-driven%2520reconstruction%2520techniques%2520for%2520radio%2520interferometry%2520is%2520that%250Athey%2520need%2520to%2520be%2520agnostic%2520to%2520the%2520varying%2520visibility%2520coverages%2520of%2520the%2520telescope%252C%250Asince%2520these%2520are%2520different%2520for%2520each%2520observation.%2520Because%2520of%2520this%252C%2520learned%250Apost-processing%2520or%2520learned%2520unrolled%2520iterative%2520reconstruction%2520methods%2520must%250Atypically%2520be%2520retrained%2520for%2520each%2520specific%2520observation%252C%2520amounting%2520to%2520a%2520large%250Acomputational%2520overhead.%2520In%2520this%2520work%2520we%2520develop%2520learned%2520post-processing%2520and%250Aunrolled%2520iterative%2520methods%2520for%2520varying%2520visibility%2520coverages%252C%2520proposing%2520training%250Astrategies%2520to%2520make%2520these%2520methods%2520agnostic%2520to%2520variations%2520in%2520visibility%2520coverage%250Awith%2520minimal%2520to%2520no%2520fine-tuning.%2520Learned%2520post-processing%2520techniques%2520are%2520heavily%250Adependent%2520on%2520the%2520prior%2520information%2520encoded%2520in%2520training%2520data%2520and%2520generalise%250Apoorly%2520to%2520other%2520visibility%2520coverages.%2520In%2520contrast%252C%2520unrolled%2520iterative%2520methods%252C%250Awhich%2520include%2520the%2520telescope%2520measurement%2520operator%2520inside%2520the%2520network%252C%2520achieve%250Agood%2520reconstruction%2520quality%2520and%2520computation%2520time%252C%2520generalising%2520well%2520to%2520other%250Acoverages%2520and%2520require%2520little%2520to%2520no%2520fine-tuning.%2520Furthermore%252C%2520they%2520generalise%250Awell%2520to%2520more%2520realistic%2520radio%2520observations%2520and%2520are%2520able%2520to%2520reconstruct%2520images%250Awith%2520with%2520a%2520larger%2520dynamic%2520range%2520than%2520the%2520training%2520set.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08958v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learned%20radio%20interferometric%20imaging%20for%20varying%20visibility%20coverage&entry.906535625=Matthijs%20Mars%20and%20Marta%20M.%20Betcke%20and%20Jason%20D.%20McEwen&entry.1292438233=%20%20With%20the%20next%20generation%20of%20interferometric%20telescopes%2C%20such%20as%20the%20Square%0AKilometre%20Array%20%28SKA%29%2C%20the%20need%20for%20highly%20computationally%20efficient%0Areconstruction%20techniques%20is%20particularly%20acute.%20The%20challenge%20in%20designing%0Alearned%2C%20data-driven%20reconstruction%20techniques%20for%20radio%20interferometry%20is%20that%0Athey%20need%20to%20be%20agnostic%20to%20the%20varying%20visibility%20coverages%20of%20the%20telescope%2C%0Asince%20these%20are%20different%20for%20each%20observation.%20Because%20of%20this%2C%20learned%0Apost-processing%20or%20learned%20unrolled%20iterative%20reconstruction%20methods%20must%0Atypically%20be%20retrained%20for%20each%20specific%20observation%2C%20amounting%20to%20a%20large%0Acomputational%20overhead.%20In%20this%20work%20we%20develop%20learned%20post-processing%20and%0Aunrolled%20iterative%20methods%20for%20varying%20visibility%20coverages%2C%20proposing%20training%0Astrategies%20to%20make%20these%20methods%20agnostic%20to%20variations%20in%20visibility%20coverage%0Awith%20minimal%20to%20no%20fine-tuning.%20Learned%20post-processing%20techniques%20are%20heavily%0Adependent%20on%20the%20prior%20information%20encoded%20in%20training%20data%20and%20generalise%0Apoorly%20to%20other%20visibility%20coverages.%20In%20contrast%2C%20unrolled%20iterative%20methods%2C%0Awhich%20include%20the%20telescope%20measurement%20operator%20inside%20the%20network%2C%20achieve%0Agood%20reconstruction%20quality%20and%20computation%20time%2C%20generalising%20well%20to%20other%0Acoverages%20and%20require%20little%20to%20no%20fine-tuning.%20Furthermore%2C%20they%20generalise%0Awell%20to%20more%20realistic%20radio%20observations%20and%20are%20able%20to%20reconstruct%20images%0Awith%20with%20a%20larger%20dynamic%20range%20than%20the%20training%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08958v2&entry.124074799=Read"},
{"title": "Delving Into the Psychology of Machines: Exploring the Structure of\n  Self-Regulated Learning via LLM-Generated Survey Responses", "author": "Leonie V. D. E. Vogelsmeier and Eduardo Oliveira and Kamila Misiejuk and Sonsoles L\u00f3pez-Pernas and Mohammed Saqr", "abstract": "  Large language models (LLMs) offer the potential to simulate human-like\nresponses and behaviors, creating new opportunities for psychological science.\nIn the context of self-regulated learning (SRL), if LLMs can reliably simulate\nsurvey responses at scale and speed, they could be used to test intervention\nscenarios, refine theoretical models, augment sparse datasets, and represent\nhard-to-reach populations. However, the validity of LLM-generated survey\nresponses remains uncertain, with limited research focused on SRL and existing\nstudies beyond SRL yielding mixed results. Therefore, in this study, we\nexamined LLM-generated responses to the 44-item Motivated Strategies for\nLearning Questionnaire (MSLQ; Pintrich \\& De Groot, 1990), a widely used\ninstrument assessing students' learning strategies and academic motivation.\nParticularly, we used the LLMs GPT-4o, Claude 3.7 Sonnet, Gemini 2 Flash, LLaMA\n3.1-8B, and Mistral Large. We analyzed item distributions, the psychological\nnetwork of the theoretical SRL dimensions, and psychometric validity based on\nthe latent factor structure. Our results suggest that Gemini 2 Flash was the\nmost promising LLM, showing considerable sampling variability and producing\nunderlying dimensions and theoretical relationships that align with prior\ntheory and empirical findings. At the same time, we observed discrepancies and\nlimitations, underscoring both the potential and current constraints of using\nLLMs for simulating psychological survey data and applying it in educational\ncontexts.\n", "link": "http://arxiv.org/abs/2506.13384v1", "date": "2025-06-16", "relevancy": 2.4574, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4917}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4914}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Delving%20Into%20the%20Psychology%20of%20Machines%3A%20Exploring%20the%20Structure%20of%0A%20%20Self-Regulated%20Learning%20via%20LLM-Generated%20Survey%20Responses&body=Title%3A%20Delving%20Into%20the%20Psychology%20of%20Machines%3A%20Exploring%20the%20Structure%20of%0A%20%20Self-Regulated%20Learning%20via%20LLM-Generated%20Survey%20Responses%0AAuthor%3A%20Leonie%20V.%20D.%20E.%20Vogelsmeier%20and%20Eduardo%20Oliveira%20and%20Kamila%20Misiejuk%20and%20Sonsoles%20L%C3%B3pez-Pernas%20and%20Mohammed%20Saqr%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20offer%20the%20potential%20to%20simulate%20human-like%0Aresponses%20and%20behaviors%2C%20creating%20new%20opportunities%20for%20psychological%20science.%0AIn%20the%20context%20of%20self-regulated%20learning%20%28SRL%29%2C%20if%20LLMs%20can%20reliably%20simulate%0Asurvey%20responses%20at%20scale%20and%20speed%2C%20they%20could%20be%20used%20to%20test%20intervention%0Ascenarios%2C%20refine%20theoretical%20models%2C%20augment%20sparse%20datasets%2C%20and%20represent%0Ahard-to-reach%20populations.%20However%2C%20the%20validity%20of%20LLM-generated%20survey%0Aresponses%20remains%20uncertain%2C%20with%20limited%20research%20focused%20on%20SRL%20and%20existing%0Astudies%20beyond%20SRL%20yielding%20mixed%20results.%20Therefore%2C%20in%20this%20study%2C%20we%0Aexamined%20LLM-generated%20responses%20to%20the%2044-item%20Motivated%20Strategies%20for%0ALearning%20Questionnaire%20%28MSLQ%3B%20Pintrich%20%5C%26%20De%20Groot%2C%201990%29%2C%20a%20widely%20used%0Ainstrument%20assessing%20students%27%20learning%20strategies%20and%20academic%20motivation.%0AParticularly%2C%20we%20used%20the%20LLMs%20GPT-4o%2C%20Claude%203.7%20Sonnet%2C%20Gemini%202%20Flash%2C%20LLaMA%0A3.1-8B%2C%20and%20Mistral%20Large.%20We%20analyzed%20item%20distributions%2C%20the%20psychological%0Anetwork%20of%20the%20theoretical%20SRL%20dimensions%2C%20and%20psychometric%20validity%20based%20on%0Athe%20latent%20factor%20structure.%20Our%20results%20suggest%20that%20Gemini%202%20Flash%20was%20the%0Amost%20promising%20LLM%2C%20showing%20considerable%20sampling%20variability%20and%20producing%0Aunderlying%20dimensions%20and%20theoretical%20relationships%20that%20align%20with%20prior%0Atheory%20and%20empirical%20findings.%20At%20the%20same%20time%2C%20we%20observed%20discrepancies%20and%0Alimitations%2C%20underscoring%20both%20the%20potential%20and%20current%20constraints%20of%20using%0ALLMs%20for%20simulating%20psychological%20survey%20data%20and%20applying%20it%20in%20educational%0Acontexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13384v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDelving%2520Into%2520the%2520Psychology%2520of%2520Machines%253A%2520Exploring%2520the%2520Structure%2520of%250A%2520%2520Self-Regulated%2520Learning%2520via%2520LLM-Generated%2520Survey%2520Responses%26entry.906535625%3DLeonie%2520V.%2520D.%2520E.%2520Vogelsmeier%2520and%2520Eduardo%2520Oliveira%2520and%2520Kamila%2520Misiejuk%2520and%2520Sonsoles%2520L%25C3%25B3pez-Pernas%2520and%2520Mohammed%2520Saqr%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520offer%2520the%2520potential%2520to%2520simulate%2520human-like%250Aresponses%2520and%2520behaviors%252C%2520creating%2520new%2520opportunities%2520for%2520psychological%2520science.%250AIn%2520the%2520context%2520of%2520self-regulated%2520learning%2520%2528SRL%2529%252C%2520if%2520LLMs%2520can%2520reliably%2520simulate%250Asurvey%2520responses%2520at%2520scale%2520and%2520speed%252C%2520they%2520could%2520be%2520used%2520to%2520test%2520intervention%250Ascenarios%252C%2520refine%2520theoretical%2520models%252C%2520augment%2520sparse%2520datasets%252C%2520and%2520represent%250Ahard-to-reach%2520populations.%2520However%252C%2520the%2520validity%2520of%2520LLM-generated%2520survey%250Aresponses%2520remains%2520uncertain%252C%2520with%2520limited%2520research%2520focused%2520on%2520SRL%2520and%2520existing%250Astudies%2520beyond%2520SRL%2520yielding%2520mixed%2520results.%2520Therefore%252C%2520in%2520this%2520study%252C%2520we%250Aexamined%2520LLM-generated%2520responses%2520to%2520the%252044-item%2520Motivated%2520Strategies%2520for%250ALearning%2520Questionnaire%2520%2528MSLQ%253B%2520Pintrich%2520%255C%2526%2520De%2520Groot%252C%25201990%2529%252C%2520a%2520widely%2520used%250Ainstrument%2520assessing%2520students%2527%2520learning%2520strategies%2520and%2520academic%2520motivation.%250AParticularly%252C%2520we%2520used%2520the%2520LLMs%2520GPT-4o%252C%2520Claude%25203.7%2520Sonnet%252C%2520Gemini%25202%2520Flash%252C%2520LLaMA%250A3.1-8B%252C%2520and%2520Mistral%2520Large.%2520We%2520analyzed%2520item%2520distributions%252C%2520the%2520psychological%250Anetwork%2520of%2520the%2520theoretical%2520SRL%2520dimensions%252C%2520and%2520psychometric%2520validity%2520based%2520on%250Athe%2520latent%2520factor%2520structure.%2520Our%2520results%2520suggest%2520that%2520Gemini%25202%2520Flash%2520was%2520the%250Amost%2520promising%2520LLM%252C%2520showing%2520considerable%2520sampling%2520variability%2520and%2520producing%250Aunderlying%2520dimensions%2520and%2520theoretical%2520relationships%2520that%2520align%2520with%2520prior%250Atheory%2520and%2520empirical%2520findings.%2520At%2520the%2520same%2520time%252C%2520we%2520observed%2520discrepancies%2520and%250Alimitations%252C%2520underscoring%2520both%2520the%2520potential%2520and%2520current%2520constraints%2520of%2520using%250ALLMs%2520for%2520simulating%2520psychological%2520survey%2520data%2520and%2520applying%2520it%2520in%2520educational%250Acontexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13384v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Delving%20Into%20the%20Psychology%20of%20Machines%3A%20Exploring%20the%20Structure%20of%0A%20%20Self-Regulated%20Learning%20via%20LLM-Generated%20Survey%20Responses&entry.906535625=Leonie%20V.%20D.%20E.%20Vogelsmeier%20and%20Eduardo%20Oliveira%20and%20Kamila%20Misiejuk%20and%20Sonsoles%20L%C3%B3pez-Pernas%20and%20Mohammed%20Saqr&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20offer%20the%20potential%20to%20simulate%20human-like%0Aresponses%20and%20behaviors%2C%20creating%20new%20opportunities%20for%20psychological%20science.%0AIn%20the%20context%20of%20self-regulated%20learning%20%28SRL%29%2C%20if%20LLMs%20can%20reliably%20simulate%0Asurvey%20responses%20at%20scale%20and%20speed%2C%20they%20could%20be%20used%20to%20test%20intervention%0Ascenarios%2C%20refine%20theoretical%20models%2C%20augment%20sparse%20datasets%2C%20and%20represent%0Ahard-to-reach%20populations.%20However%2C%20the%20validity%20of%20LLM-generated%20survey%0Aresponses%20remains%20uncertain%2C%20with%20limited%20research%20focused%20on%20SRL%20and%20existing%0Astudies%20beyond%20SRL%20yielding%20mixed%20results.%20Therefore%2C%20in%20this%20study%2C%20we%0Aexamined%20LLM-generated%20responses%20to%20the%2044-item%20Motivated%20Strategies%20for%0ALearning%20Questionnaire%20%28MSLQ%3B%20Pintrich%20%5C%26%20De%20Groot%2C%201990%29%2C%20a%20widely%20used%0Ainstrument%20assessing%20students%27%20learning%20strategies%20and%20academic%20motivation.%0AParticularly%2C%20we%20used%20the%20LLMs%20GPT-4o%2C%20Claude%203.7%20Sonnet%2C%20Gemini%202%20Flash%2C%20LLaMA%0A3.1-8B%2C%20and%20Mistral%20Large.%20We%20analyzed%20item%20distributions%2C%20the%20psychological%0Anetwork%20of%20the%20theoretical%20SRL%20dimensions%2C%20and%20psychometric%20validity%20based%20on%0Athe%20latent%20factor%20structure.%20Our%20results%20suggest%20that%20Gemini%202%20Flash%20was%20the%0Amost%20promising%20LLM%2C%20showing%20considerable%20sampling%20variability%20and%20producing%0Aunderlying%20dimensions%20and%20theoretical%20relationships%20that%20align%20with%20prior%0Atheory%20and%20empirical%20findings.%20At%20the%20same%20time%2C%20we%20observed%20discrepancies%20and%0Alimitations%2C%20underscoring%20both%20the%20potential%20and%20current%20constraints%20of%20using%0ALLMs%20for%20simulating%20psychological%20survey%20data%20and%20applying%20it%20in%20educational%0Acontexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13384v1&entry.124074799=Read"},
{"title": "Vid-CamEdit: Video Camera Trajectory Editing with Generative Rendering\n  from Estimated Geometry", "author": "Junyoung Seo and Jisang Han and Jaewoo Jung and Siyoon Jin and Joungbin Lee and Takuya Narihira and Kazumi Fukuda and Takashi Shibuya and Donghoon Ahn and Shoukang Hu and Seungryong Kim and Yuki Mitsufuji", "abstract": "  We introduce Vid-CamEdit, a novel framework for video camera trajectory\nediting, enabling the re-synthesis of monocular videos along user-defined\ncamera paths. This task is challenging due to its ill-posed nature and the\nlimited multi-view video data for training. Traditional reconstruction methods\nstruggle with extreme trajectory changes, and existing generative models for\ndynamic novel view synthesis cannot handle in-the-wild videos. Our approach\nconsists of two steps: estimating temporally consistent geometry, and\ngenerative rendering guided by this geometry. By integrating geometric priors,\nthe generative model focuses on synthesizing realistic details where the\nestimated geometry is uncertain. We eliminate the need for extensive 4D\ntraining data through a factorized fine-tuning framework that separately trains\nspatial and temporal components using multi-view image and video data. Our\nmethod outperforms baselines in producing plausible videos from novel camera\ntrajectories, especially in extreme extrapolation scenarios on real-world\nfootage.\n", "link": "http://arxiv.org/abs/2506.13697v1", "date": "2025-06-16", "relevancy": 2.4564, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6594}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6192}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vid-CamEdit%3A%20Video%20Camera%20Trajectory%20Editing%20with%20Generative%20Rendering%0A%20%20from%20Estimated%20Geometry&body=Title%3A%20Vid-CamEdit%3A%20Video%20Camera%20Trajectory%20Editing%20with%20Generative%20Rendering%0A%20%20from%20Estimated%20Geometry%0AAuthor%3A%20Junyoung%20Seo%20and%20Jisang%20Han%20and%20Jaewoo%20Jung%20and%20Siyoon%20Jin%20and%20Joungbin%20Lee%20and%20Takuya%20Narihira%20and%20Kazumi%20Fukuda%20and%20Takashi%20Shibuya%20and%20Donghoon%20Ahn%20and%20Shoukang%20Hu%20and%20Seungryong%20Kim%20and%20Yuki%20Mitsufuji%0AAbstract%3A%20%20%20We%20introduce%20Vid-CamEdit%2C%20a%20novel%20framework%20for%20video%20camera%20trajectory%0Aediting%2C%20enabling%20the%20re-synthesis%20of%20monocular%20videos%20along%20user-defined%0Acamera%20paths.%20This%20task%20is%20challenging%20due%20to%20its%20ill-posed%20nature%20and%20the%0Alimited%20multi-view%20video%20data%20for%20training.%20Traditional%20reconstruction%20methods%0Astruggle%20with%20extreme%20trajectory%20changes%2C%20and%20existing%20generative%20models%20for%0Adynamic%20novel%20view%20synthesis%20cannot%20handle%20in-the-wild%20videos.%20Our%20approach%0Aconsists%20of%20two%20steps%3A%20estimating%20temporally%20consistent%20geometry%2C%20and%0Agenerative%20rendering%20guided%20by%20this%20geometry.%20By%20integrating%20geometric%20priors%2C%0Athe%20generative%20model%20focuses%20on%20synthesizing%20realistic%20details%20where%20the%0Aestimated%20geometry%20is%20uncertain.%20We%20eliminate%20the%20need%20for%20extensive%204D%0Atraining%20data%20through%20a%20factorized%20fine-tuning%20framework%20that%20separately%20trains%0Aspatial%20and%20temporal%20components%20using%20multi-view%20image%20and%20video%20data.%20Our%0Amethod%20outperforms%20baselines%20in%20producing%20plausible%20videos%20from%20novel%20camera%0Atrajectories%2C%20especially%20in%20extreme%20extrapolation%20scenarios%20on%20real-world%0Afootage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVid-CamEdit%253A%2520Video%2520Camera%2520Trajectory%2520Editing%2520with%2520Generative%2520Rendering%250A%2520%2520from%2520Estimated%2520Geometry%26entry.906535625%3DJunyoung%2520Seo%2520and%2520Jisang%2520Han%2520and%2520Jaewoo%2520Jung%2520and%2520Siyoon%2520Jin%2520and%2520Joungbin%2520Lee%2520and%2520Takuya%2520Narihira%2520and%2520Kazumi%2520Fukuda%2520and%2520Takashi%2520Shibuya%2520and%2520Donghoon%2520Ahn%2520and%2520Shoukang%2520Hu%2520and%2520Seungryong%2520Kim%2520and%2520Yuki%2520Mitsufuji%26entry.1292438233%3D%2520%2520We%2520introduce%2520Vid-CamEdit%252C%2520a%2520novel%2520framework%2520for%2520video%2520camera%2520trajectory%250Aediting%252C%2520enabling%2520the%2520re-synthesis%2520of%2520monocular%2520videos%2520along%2520user-defined%250Acamera%2520paths.%2520This%2520task%2520is%2520challenging%2520due%2520to%2520its%2520ill-posed%2520nature%2520and%2520the%250Alimited%2520multi-view%2520video%2520data%2520for%2520training.%2520Traditional%2520reconstruction%2520methods%250Astruggle%2520with%2520extreme%2520trajectory%2520changes%252C%2520and%2520existing%2520generative%2520models%2520for%250Adynamic%2520novel%2520view%2520synthesis%2520cannot%2520handle%2520in-the-wild%2520videos.%2520Our%2520approach%250Aconsists%2520of%2520two%2520steps%253A%2520estimating%2520temporally%2520consistent%2520geometry%252C%2520and%250Agenerative%2520rendering%2520guided%2520by%2520this%2520geometry.%2520By%2520integrating%2520geometric%2520priors%252C%250Athe%2520generative%2520model%2520focuses%2520on%2520synthesizing%2520realistic%2520details%2520where%2520the%250Aestimated%2520geometry%2520is%2520uncertain.%2520We%2520eliminate%2520the%2520need%2520for%2520extensive%25204D%250Atraining%2520data%2520through%2520a%2520factorized%2520fine-tuning%2520framework%2520that%2520separately%2520trains%250Aspatial%2520and%2520temporal%2520components%2520using%2520multi-view%2520image%2520and%2520video%2520data.%2520Our%250Amethod%2520outperforms%2520baselines%2520in%2520producing%2520plausible%2520videos%2520from%2520novel%2520camera%250Atrajectories%252C%2520especially%2520in%2520extreme%2520extrapolation%2520scenarios%2520on%2520real-world%250Afootage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vid-CamEdit%3A%20Video%20Camera%20Trajectory%20Editing%20with%20Generative%20Rendering%0A%20%20from%20Estimated%20Geometry&entry.906535625=Junyoung%20Seo%20and%20Jisang%20Han%20and%20Jaewoo%20Jung%20and%20Siyoon%20Jin%20and%20Joungbin%20Lee%20and%20Takuya%20Narihira%20and%20Kazumi%20Fukuda%20and%20Takashi%20Shibuya%20and%20Donghoon%20Ahn%20and%20Shoukang%20Hu%20and%20Seungryong%20Kim%20and%20Yuki%20Mitsufuji&entry.1292438233=%20%20We%20introduce%20Vid-CamEdit%2C%20a%20novel%20framework%20for%20video%20camera%20trajectory%0Aediting%2C%20enabling%20the%20re-synthesis%20of%20monocular%20videos%20along%20user-defined%0Acamera%20paths.%20This%20task%20is%20challenging%20due%20to%20its%20ill-posed%20nature%20and%20the%0Alimited%20multi-view%20video%20data%20for%20training.%20Traditional%20reconstruction%20methods%0Astruggle%20with%20extreme%20trajectory%20changes%2C%20and%20existing%20generative%20models%20for%0Adynamic%20novel%20view%20synthesis%20cannot%20handle%20in-the-wild%20videos.%20Our%20approach%0Aconsists%20of%20two%20steps%3A%20estimating%20temporally%20consistent%20geometry%2C%20and%0Agenerative%20rendering%20guided%20by%20this%20geometry.%20By%20integrating%20geometric%20priors%2C%0Athe%20generative%20model%20focuses%20on%20synthesizing%20realistic%20details%20where%20the%0Aestimated%20geometry%20is%20uncertain.%20We%20eliminate%20the%20need%20for%20extensive%204D%0Atraining%20data%20through%20a%20factorized%20fine-tuning%20framework%20that%20separately%20trains%0Aspatial%20and%20temporal%20components%20using%20multi-view%20image%20and%20video%20data.%20Our%0Amethod%20outperforms%20baselines%20in%20producing%20plausible%20videos%20from%20novel%20camera%0Atrajectories%2C%20especially%20in%20extreme%20extrapolation%20scenarios%20on%20real-world%0Afootage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13697v1&entry.124074799=Read"},
{"title": "Better Think with Tables: Tabular Structures Enhance LLM Comprehension\n  for Data-Analytics Requests", "author": "Jio Oh and Geon Heo and Seungjun Oh and Hyunjin Kim and JinYeong Bak and Jindong Wang and Xing Xie and Steven Euijong Whang", "abstract": "  Large Language Models (LLMs) often struggle with data-analytics requests\nrelated to information retrieval and data manipulation that frequently arise in\nreal-world scenarios under multiple conditions. In this paper, we introduce\nThinking with Tables, where we inject tabular structures into LLMs for\ndata-analytics requests. Through comprehensive evaluations across various\nrequest types, we show that providing tabular structures yields a 40.29 percent\naverage performance gain along with better robustness and token efficiency.\nThrough attention-value analysis, we uncover that tables help LLMs better\nattend to relevant information, explaining these improvements. Beyond tables\nand text, we evaluate whether (1) blending structuredness within text, such as\nproviding templates or fixing the order of attributes, and (2) other\nrepresentative structures, such as knowledge graphs and JSON, are helpful. We\nobserve that utilizing tables offers the best balance between efficiency and\neffectiveness. These advantages remain consistent under increased task\ncomplexity and even when all input data cannot be structured. Finally, as data\nanalytics typically relies on structured factual inputs, our text-to-table\nconversion demonstrates the method's applicability to text-compatible data\nsources.\n", "link": "http://arxiv.org/abs/2412.17189v3", "date": "2025-06-16", "relevancy": 2.4539, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5021}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4851}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Better%20Think%20with%20Tables%3A%20Tabular%20Structures%20Enhance%20LLM%20Comprehension%0A%20%20for%20Data-Analytics%20Requests&body=Title%3A%20Better%20Think%20with%20Tables%3A%20Tabular%20Structures%20Enhance%20LLM%20Comprehension%0A%20%20for%20Data-Analytics%20Requests%0AAuthor%3A%20Jio%20Oh%20and%20Geon%20Heo%20and%20Seungjun%20Oh%20and%20Hyunjin%20Kim%20and%20JinYeong%20Bak%20and%20Jindong%20Wang%20and%20Xing%20Xie%20and%20Steven%20Euijong%20Whang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20often%20struggle%20with%20data-analytics%20requests%0Arelated%20to%20information%20retrieval%20and%20data%20manipulation%20that%20frequently%20arise%20in%0Areal-world%20scenarios%20under%20multiple%20conditions.%20In%20this%20paper%2C%20we%20introduce%0AThinking%20with%20Tables%2C%20where%20we%20inject%20tabular%20structures%20into%20LLMs%20for%0Adata-analytics%20requests.%20Through%20comprehensive%20evaluations%20across%20various%0Arequest%20types%2C%20we%20show%20that%20providing%20tabular%20structures%20yields%20a%2040.29%20percent%0Aaverage%20performance%20gain%20along%20with%20better%20robustness%20and%20token%20efficiency.%0AThrough%20attention-value%20analysis%2C%20we%20uncover%20that%20tables%20help%20LLMs%20better%0Aattend%20to%20relevant%20information%2C%20explaining%20these%20improvements.%20Beyond%20tables%0Aand%20text%2C%20we%20evaluate%20whether%20%281%29%20blending%20structuredness%20within%20text%2C%20such%20as%0Aproviding%20templates%20or%20fixing%20the%20order%20of%20attributes%2C%20and%20%282%29%20other%0Arepresentative%20structures%2C%20such%20as%20knowledge%20graphs%20and%20JSON%2C%20are%20helpful.%20We%0Aobserve%20that%20utilizing%20tables%20offers%20the%20best%20balance%20between%20efficiency%20and%0Aeffectiveness.%20These%20advantages%20remain%20consistent%20under%20increased%20task%0Acomplexity%20and%20even%20when%20all%20input%20data%20cannot%20be%20structured.%20Finally%2C%20as%20data%0Aanalytics%20typically%20relies%20on%20structured%20factual%20inputs%2C%20our%20text-to-table%0Aconversion%20demonstrates%20the%20method%27s%20applicability%20to%20text-compatible%20data%0Asources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17189v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBetter%2520Think%2520with%2520Tables%253A%2520Tabular%2520Structures%2520Enhance%2520LLM%2520Comprehension%250A%2520%2520for%2520Data-Analytics%2520Requests%26entry.906535625%3DJio%2520Oh%2520and%2520Geon%2520Heo%2520and%2520Seungjun%2520Oh%2520and%2520Hyunjin%2520Kim%2520and%2520JinYeong%2520Bak%2520and%2520Jindong%2520Wang%2520and%2520Xing%2520Xie%2520and%2520Steven%2520Euijong%2520Whang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520often%2520struggle%2520with%2520data-analytics%2520requests%250Arelated%2520to%2520information%2520retrieval%2520and%2520data%2520manipulation%2520that%2520frequently%2520arise%2520in%250Areal-world%2520scenarios%2520under%2520multiple%2520conditions.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AThinking%2520with%2520Tables%252C%2520where%2520we%2520inject%2520tabular%2520structures%2520into%2520LLMs%2520for%250Adata-analytics%2520requests.%2520Through%2520comprehensive%2520evaluations%2520across%2520various%250Arequest%2520types%252C%2520we%2520show%2520that%2520providing%2520tabular%2520structures%2520yields%2520a%252040.29%2520percent%250Aaverage%2520performance%2520gain%2520along%2520with%2520better%2520robustness%2520and%2520token%2520efficiency.%250AThrough%2520attention-value%2520analysis%252C%2520we%2520uncover%2520that%2520tables%2520help%2520LLMs%2520better%250Aattend%2520to%2520relevant%2520information%252C%2520explaining%2520these%2520improvements.%2520Beyond%2520tables%250Aand%2520text%252C%2520we%2520evaluate%2520whether%2520%25281%2529%2520blending%2520structuredness%2520within%2520text%252C%2520such%2520as%250Aproviding%2520templates%2520or%2520fixing%2520the%2520order%2520of%2520attributes%252C%2520and%2520%25282%2529%2520other%250Arepresentative%2520structures%252C%2520such%2520as%2520knowledge%2520graphs%2520and%2520JSON%252C%2520are%2520helpful.%2520We%250Aobserve%2520that%2520utilizing%2520tables%2520offers%2520the%2520best%2520balance%2520between%2520efficiency%2520and%250Aeffectiveness.%2520These%2520advantages%2520remain%2520consistent%2520under%2520increased%2520task%250Acomplexity%2520and%2520even%2520when%2520all%2520input%2520data%2520cannot%2520be%2520structured.%2520Finally%252C%2520as%2520data%250Aanalytics%2520typically%2520relies%2520on%2520structured%2520factual%2520inputs%252C%2520our%2520text-to-table%250Aconversion%2520demonstrates%2520the%2520method%2527s%2520applicability%2520to%2520text-compatible%2520data%250Asources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17189v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Better%20Think%20with%20Tables%3A%20Tabular%20Structures%20Enhance%20LLM%20Comprehension%0A%20%20for%20Data-Analytics%20Requests&entry.906535625=Jio%20Oh%20and%20Geon%20Heo%20and%20Seungjun%20Oh%20and%20Hyunjin%20Kim%20and%20JinYeong%20Bak%20and%20Jindong%20Wang%20and%20Xing%20Xie%20and%20Steven%20Euijong%20Whang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20often%20struggle%20with%20data-analytics%20requests%0Arelated%20to%20information%20retrieval%20and%20data%20manipulation%20that%20frequently%20arise%20in%0Areal-world%20scenarios%20under%20multiple%20conditions.%20In%20this%20paper%2C%20we%20introduce%0AThinking%20with%20Tables%2C%20where%20we%20inject%20tabular%20structures%20into%20LLMs%20for%0Adata-analytics%20requests.%20Through%20comprehensive%20evaluations%20across%20various%0Arequest%20types%2C%20we%20show%20that%20providing%20tabular%20structures%20yields%20a%2040.29%20percent%0Aaverage%20performance%20gain%20along%20with%20better%20robustness%20and%20token%20efficiency.%0AThrough%20attention-value%20analysis%2C%20we%20uncover%20that%20tables%20help%20LLMs%20better%0Aattend%20to%20relevant%20information%2C%20explaining%20these%20improvements.%20Beyond%20tables%0Aand%20text%2C%20we%20evaluate%20whether%20%281%29%20blending%20structuredness%20within%20text%2C%20such%20as%0Aproviding%20templates%20or%20fixing%20the%20order%20of%20attributes%2C%20and%20%282%29%20other%0Arepresentative%20structures%2C%20such%20as%20knowledge%20graphs%20and%20JSON%2C%20are%20helpful.%20We%0Aobserve%20that%20utilizing%20tables%20offers%20the%20best%20balance%20between%20efficiency%20and%0Aeffectiveness.%20These%20advantages%20remain%20consistent%20under%20increased%20task%0Acomplexity%20and%20even%20when%20all%20input%20data%20cannot%20be%20structured.%20Finally%2C%20as%20data%0Aanalytics%20typically%20relies%20on%20structured%20factual%20inputs%2C%20our%20text-to-table%0Aconversion%20demonstrates%20the%20method%27s%20applicability%20to%20text-compatible%20data%0Asources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17189v3&entry.124074799=Read"},
{"title": "A Comprehensive Survey on Video Scene Parsing:Advances, Challenges, and\n  Prospects", "author": "Guohuan Xie and Syed Ariff Syed Hesham and Wenya Guo and Bing Li and Ming-Ming Cheng and Guolei Sun and Yun Liu", "abstract": "  Video Scene Parsing (VSP) has emerged as a cornerstone in computer vision,\nfacilitating the simultaneous segmentation, recognition, and tracking of\ndiverse visual entities in dynamic scenes. In this survey, we present a\nholistic review of recent advances in VSP, covering a wide array of vision\ntasks, including Video Semantic Segmentation (VSS), Video Instance Segmentation\n(VIS), Video Panoptic Segmentation (VPS), as well as Video Tracking and\nSegmentation (VTS), and Open-Vocabulary Video Segmentation (OVVS). We\nsystematically analyze the evolution from traditional hand-crafted features to\nmodern deep learning paradigms -- spanning from fully convolutional networks to\nthe latest transformer-based architectures -- and assess their effectiveness in\ncapturing both local and global temporal contexts. Furthermore, our review\ncritically discusses the technical challenges, ranging from maintaining\ntemporal consistency to handling complex scene dynamics, and offers a\ncomprehensive comparative study of datasets and evaluation metrics that have\nshaped current benchmarking standards. By distilling the key contributions and\nshortcomings of state-of-the-art methodologies, this survey highlights emerging\ntrends and prospective research directions that promise to further elevate the\nrobustness and adaptability of VSP in real-world applications.\n", "link": "http://arxiv.org/abs/2506.13552v1", "date": "2025-06-16", "relevancy": 2.4506, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6258}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6258}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Survey%20on%20Video%20Scene%20Parsing%3AAdvances%2C%20Challenges%2C%20and%0A%20%20Prospects&body=Title%3A%20A%20Comprehensive%20Survey%20on%20Video%20Scene%20Parsing%3AAdvances%2C%20Challenges%2C%20and%0A%20%20Prospects%0AAuthor%3A%20Guohuan%20Xie%20and%20Syed%20Ariff%20Syed%20Hesham%20and%20Wenya%20Guo%20and%20Bing%20Li%20and%20Ming-Ming%20Cheng%20and%20Guolei%20Sun%20and%20Yun%20Liu%0AAbstract%3A%20%20%20Video%20Scene%20Parsing%20%28VSP%29%20has%20emerged%20as%20a%20cornerstone%20in%20computer%20vision%2C%0Afacilitating%20the%20simultaneous%20segmentation%2C%20recognition%2C%20and%20tracking%20of%0Adiverse%20visual%20entities%20in%20dynamic%20scenes.%20In%20this%20survey%2C%20we%20present%20a%0Aholistic%20review%20of%20recent%20advances%20in%20VSP%2C%20covering%20a%20wide%20array%20of%20vision%0Atasks%2C%20including%20Video%20Semantic%20Segmentation%20%28VSS%29%2C%20Video%20Instance%20Segmentation%0A%28VIS%29%2C%20Video%20Panoptic%20Segmentation%20%28VPS%29%2C%20as%20well%20as%20Video%20Tracking%20and%0ASegmentation%20%28VTS%29%2C%20and%20Open-Vocabulary%20Video%20Segmentation%20%28OVVS%29.%20We%0Asystematically%20analyze%20the%20evolution%20from%20traditional%20hand-crafted%20features%20to%0Amodern%20deep%20learning%20paradigms%20--%20spanning%20from%20fully%20convolutional%20networks%20to%0Athe%20latest%20transformer-based%20architectures%20--%20and%20assess%20their%20effectiveness%20in%0Acapturing%20both%20local%20and%20global%20temporal%20contexts.%20Furthermore%2C%20our%20review%0Acritically%20discusses%20the%20technical%20challenges%2C%20ranging%20from%20maintaining%0Atemporal%20consistency%20to%20handling%20complex%20scene%20dynamics%2C%20and%20offers%20a%0Acomprehensive%20comparative%20study%20of%20datasets%20and%20evaluation%20metrics%20that%20have%0Ashaped%20current%20benchmarking%20standards.%20By%20distilling%20the%20key%20contributions%20and%0Ashortcomings%20of%20state-of-the-art%20methodologies%2C%20this%20survey%20highlights%20emerging%0Atrends%20and%20prospective%20research%20directions%20that%20promise%20to%20further%20elevate%20the%0Arobustness%20and%20adaptability%20of%20VSP%20in%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Survey%2520on%2520Video%2520Scene%2520Parsing%253AAdvances%252C%2520Challenges%252C%2520and%250A%2520%2520Prospects%26entry.906535625%3DGuohuan%2520Xie%2520and%2520Syed%2520Ariff%2520Syed%2520Hesham%2520and%2520Wenya%2520Guo%2520and%2520Bing%2520Li%2520and%2520Ming-Ming%2520Cheng%2520and%2520Guolei%2520Sun%2520and%2520Yun%2520Liu%26entry.1292438233%3D%2520%2520Video%2520Scene%2520Parsing%2520%2528VSP%2529%2520has%2520emerged%2520as%2520a%2520cornerstone%2520in%2520computer%2520vision%252C%250Afacilitating%2520the%2520simultaneous%2520segmentation%252C%2520recognition%252C%2520and%2520tracking%2520of%250Adiverse%2520visual%2520entities%2520in%2520dynamic%2520scenes.%2520In%2520this%2520survey%252C%2520we%2520present%2520a%250Aholistic%2520review%2520of%2520recent%2520advances%2520in%2520VSP%252C%2520covering%2520a%2520wide%2520array%2520of%2520vision%250Atasks%252C%2520including%2520Video%2520Semantic%2520Segmentation%2520%2528VSS%2529%252C%2520Video%2520Instance%2520Segmentation%250A%2528VIS%2529%252C%2520Video%2520Panoptic%2520Segmentation%2520%2528VPS%2529%252C%2520as%2520well%2520as%2520Video%2520Tracking%2520and%250ASegmentation%2520%2528VTS%2529%252C%2520and%2520Open-Vocabulary%2520Video%2520Segmentation%2520%2528OVVS%2529.%2520We%250Asystematically%2520analyze%2520the%2520evolution%2520from%2520traditional%2520hand-crafted%2520features%2520to%250Amodern%2520deep%2520learning%2520paradigms%2520--%2520spanning%2520from%2520fully%2520convolutional%2520networks%2520to%250Athe%2520latest%2520transformer-based%2520architectures%2520--%2520and%2520assess%2520their%2520effectiveness%2520in%250Acapturing%2520both%2520local%2520and%2520global%2520temporal%2520contexts.%2520Furthermore%252C%2520our%2520review%250Acritically%2520discusses%2520the%2520technical%2520challenges%252C%2520ranging%2520from%2520maintaining%250Atemporal%2520consistency%2520to%2520handling%2520complex%2520scene%2520dynamics%252C%2520and%2520offers%2520a%250Acomprehensive%2520comparative%2520study%2520of%2520datasets%2520and%2520evaluation%2520metrics%2520that%2520have%250Ashaped%2520current%2520benchmarking%2520standards.%2520By%2520distilling%2520the%2520key%2520contributions%2520and%250Ashortcomings%2520of%2520state-of-the-art%2520methodologies%252C%2520this%2520survey%2520highlights%2520emerging%250Atrends%2520and%2520prospective%2520research%2520directions%2520that%2520promise%2520to%2520further%2520elevate%2520the%250Arobustness%2520and%2520adaptability%2520of%2520VSP%2520in%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Survey%20on%20Video%20Scene%20Parsing%3AAdvances%2C%20Challenges%2C%20and%0A%20%20Prospects&entry.906535625=Guohuan%20Xie%20and%20Syed%20Ariff%20Syed%20Hesham%20and%20Wenya%20Guo%20and%20Bing%20Li%20and%20Ming-Ming%20Cheng%20and%20Guolei%20Sun%20and%20Yun%20Liu&entry.1292438233=%20%20Video%20Scene%20Parsing%20%28VSP%29%20has%20emerged%20as%20a%20cornerstone%20in%20computer%20vision%2C%0Afacilitating%20the%20simultaneous%20segmentation%2C%20recognition%2C%20and%20tracking%20of%0Adiverse%20visual%20entities%20in%20dynamic%20scenes.%20In%20this%20survey%2C%20we%20present%20a%0Aholistic%20review%20of%20recent%20advances%20in%20VSP%2C%20covering%20a%20wide%20array%20of%20vision%0Atasks%2C%20including%20Video%20Semantic%20Segmentation%20%28VSS%29%2C%20Video%20Instance%20Segmentation%0A%28VIS%29%2C%20Video%20Panoptic%20Segmentation%20%28VPS%29%2C%20as%20well%20as%20Video%20Tracking%20and%0ASegmentation%20%28VTS%29%2C%20and%20Open-Vocabulary%20Video%20Segmentation%20%28OVVS%29.%20We%0Asystematically%20analyze%20the%20evolution%20from%20traditional%20hand-crafted%20features%20to%0Amodern%20deep%20learning%20paradigms%20--%20spanning%20from%20fully%20convolutional%20networks%20to%0Athe%20latest%20transformer-based%20architectures%20--%20and%20assess%20their%20effectiveness%20in%0Acapturing%20both%20local%20and%20global%20temporal%20contexts.%20Furthermore%2C%20our%20review%0Acritically%20discusses%20the%20technical%20challenges%2C%20ranging%20from%20maintaining%0Atemporal%20consistency%20to%20handling%20complex%20scene%20dynamics%2C%20and%20offers%20a%0Acomprehensive%20comparative%20study%20of%20datasets%20and%20evaluation%20metrics%20that%20have%0Ashaped%20current%20benchmarking%20standards.%20By%20distilling%20the%20key%20contributions%20and%0Ashortcomings%20of%20state-of-the-art%20methodologies%2C%20this%20survey%20highlights%20emerging%0Atrends%20and%20prospective%20research%20directions%20that%20promise%20to%20further%20elevate%20the%0Arobustness%20and%20adaptability%20of%20VSP%20in%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13552v1&entry.124074799=Read"},
{"title": "VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion\n  Models", "author": "Edward Li and Zichen Wang and Jiahe Huang and Jeong Joon Park", "abstract": "  We present a unified framework for solving partial differential equations\n(PDEs) using video-inpainting diffusion transformer models. Unlike existing\nmethods that devise specialized strategies for either forward or inverse\nproblems under full or partial observation, our approach unifies these tasks\nunder a single, flexible generative framework. Specifically, we recast\nPDE-solving as a generalized inpainting problem, e.g., treating forward\nprediction as inferring missing spatiotemporal information of future states\nfrom initial conditions. To this end, we design a transformer-based\narchitecture that conditions on arbitrary patterns of known data to infer\nmissing values across time and space. Our method proposes pixel-space video\ndiffusion models for fine-grained, high-fidelity inpainting and conditioning,\nwhile enhancing computational efficiency through hierarchical modeling.\nExtensive experiments show that our video inpainting-based diffusion model\noffers an accurate and versatile solution across a wide range of PDEs and\nproblem setups, outperforming state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2506.13754v1", "date": "2025-06-16", "relevancy": 2.4267, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6137}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6057}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoPDE%3A%20Unified%20Generative%20PDE%20Solving%20via%20Video%20Inpainting%20Diffusion%0A%20%20Models&body=Title%3A%20VideoPDE%3A%20Unified%20Generative%20PDE%20Solving%20via%20Video%20Inpainting%20Diffusion%0A%20%20Models%0AAuthor%3A%20Edward%20Li%20and%20Zichen%20Wang%20and%20Jiahe%20Huang%20and%20Jeong%20Joon%20Park%0AAbstract%3A%20%20%20We%20present%20a%20unified%20framework%20for%20solving%20partial%20differential%20equations%0A%28PDEs%29%20using%20video-inpainting%20diffusion%20transformer%20models.%20Unlike%20existing%0Amethods%20that%20devise%20specialized%20strategies%20for%20either%20forward%20or%20inverse%0Aproblems%20under%20full%20or%20partial%20observation%2C%20our%20approach%20unifies%20these%20tasks%0Aunder%20a%20single%2C%20flexible%20generative%20framework.%20Specifically%2C%20we%20recast%0APDE-solving%20as%20a%20generalized%20inpainting%20problem%2C%20e.g.%2C%20treating%20forward%0Aprediction%20as%20inferring%20missing%20spatiotemporal%20information%20of%20future%20states%0Afrom%20initial%20conditions.%20To%20this%20end%2C%20we%20design%20a%20transformer-based%0Aarchitecture%20that%20conditions%20on%20arbitrary%20patterns%20of%20known%20data%20to%20infer%0Amissing%20values%20across%20time%20and%20space.%20Our%20method%20proposes%20pixel-space%20video%0Adiffusion%20models%20for%20fine-grained%2C%20high-fidelity%20inpainting%20and%20conditioning%2C%0Awhile%20enhancing%20computational%20efficiency%20through%20hierarchical%20modeling.%0AExtensive%20experiments%20show%20that%20our%20video%20inpainting-based%20diffusion%20model%0Aoffers%20an%20accurate%20and%20versatile%20solution%20across%20a%20wide%20range%20of%20PDEs%20and%0Aproblem%20setups%2C%20outperforming%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoPDE%253A%2520Unified%2520Generative%2520PDE%2520Solving%2520via%2520Video%2520Inpainting%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DEdward%2520Li%2520and%2520Zichen%2520Wang%2520and%2520Jiahe%2520Huang%2520and%2520Jeong%2520Joon%2520Park%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520unified%2520framework%2520for%2520solving%2520partial%2520differential%2520equations%250A%2528PDEs%2529%2520using%2520video-inpainting%2520diffusion%2520transformer%2520models.%2520Unlike%2520existing%250Amethods%2520that%2520devise%2520specialized%2520strategies%2520for%2520either%2520forward%2520or%2520inverse%250Aproblems%2520under%2520full%2520or%2520partial%2520observation%252C%2520our%2520approach%2520unifies%2520these%2520tasks%250Aunder%2520a%2520single%252C%2520flexible%2520generative%2520framework.%2520Specifically%252C%2520we%2520recast%250APDE-solving%2520as%2520a%2520generalized%2520inpainting%2520problem%252C%2520e.g.%252C%2520treating%2520forward%250Aprediction%2520as%2520inferring%2520missing%2520spatiotemporal%2520information%2520of%2520future%2520states%250Afrom%2520initial%2520conditions.%2520To%2520this%2520end%252C%2520we%2520design%2520a%2520transformer-based%250Aarchitecture%2520that%2520conditions%2520on%2520arbitrary%2520patterns%2520of%2520known%2520data%2520to%2520infer%250Amissing%2520values%2520across%2520time%2520and%2520space.%2520Our%2520method%2520proposes%2520pixel-space%2520video%250Adiffusion%2520models%2520for%2520fine-grained%252C%2520high-fidelity%2520inpainting%2520and%2520conditioning%252C%250Awhile%2520enhancing%2520computational%2520efficiency%2520through%2520hierarchical%2520modeling.%250AExtensive%2520experiments%2520show%2520that%2520our%2520video%2520inpainting-based%2520diffusion%2520model%250Aoffers%2520an%2520accurate%2520and%2520versatile%2520solution%2520across%2520a%2520wide%2520range%2520of%2520PDEs%2520and%250Aproblem%2520setups%252C%2520outperforming%2520state-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoPDE%3A%20Unified%20Generative%20PDE%20Solving%20via%20Video%20Inpainting%20Diffusion%0A%20%20Models&entry.906535625=Edward%20Li%20and%20Zichen%20Wang%20and%20Jiahe%20Huang%20and%20Jeong%20Joon%20Park&entry.1292438233=%20%20We%20present%20a%20unified%20framework%20for%20solving%20partial%20differential%20equations%0A%28PDEs%29%20using%20video-inpainting%20diffusion%20transformer%20models.%20Unlike%20existing%0Amethods%20that%20devise%20specialized%20strategies%20for%20either%20forward%20or%20inverse%0Aproblems%20under%20full%20or%20partial%20observation%2C%20our%20approach%20unifies%20these%20tasks%0Aunder%20a%20single%2C%20flexible%20generative%20framework.%20Specifically%2C%20we%20recast%0APDE-solving%20as%20a%20generalized%20inpainting%20problem%2C%20e.g.%2C%20treating%20forward%0Aprediction%20as%20inferring%20missing%20spatiotemporal%20information%20of%20future%20states%0Afrom%20initial%20conditions.%20To%20this%20end%2C%20we%20design%20a%20transformer-based%0Aarchitecture%20that%20conditions%20on%20arbitrary%20patterns%20of%20known%20data%20to%20infer%0Amissing%20values%20across%20time%20and%20space.%20Our%20method%20proposes%20pixel-space%20video%0Adiffusion%20models%20for%20fine-grained%2C%20high-fidelity%20inpainting%20and%20conditioning%2C%0Awhile%20enhancing%20computational%20efficiency%20through%20hierarchical%20modeling.%0AExtensive%20experiments%20show%20that%20our%20video%20inpainting-based%20diffusion%20model%0Aoffers%20an%20accurate%20and%20versatile%20solution%20across%20a%20wide%20range%20of%20PDEs%20and%0Aproblem%20setups%2C%20outperforming%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13754v1&entry.124074799=Read"},
{"title": "Tady: A Neural Disassembler without Structural Constraint Violations", "author": "Siliang Qin and Fengrui Yang and Hao Wang and Bolun Zhang and Zeyu Gao and Chao Zhang and Kai Chen", "abstract": "  Disassembly is a crucial yet challenging step in binary analysis. While\nemerging neural disassemblers show promise for efficiency and accuracy, they\nfrequently generate outputs violating fundamental structural constraints, which\nsignificantly compromise their practical usability. To address this critical\nproblem, we regularize the disassembly solution space by formalizing and\napplying key structural constraints based on post-dominance relations. This\napproach systematically detects widespread errors in existing neural\ndisassemblers' outputs. These errors often originate from models' limited\ncontext modeling and instruction-level decoding that neglect global structural\nintegrity. We introduce Tady, a novel neural disassembler featuring an improved\nmodel architecture and a dedicated post-processing algorithm, specifically\nengineered to address these deficiencies. Comprehensive evaluations on diverse\nbinaries demonstrate that Tady effectively eliminates structural constraint\nviolations and functions with high efficiency, while maintaining\ninstruction-level accuracy.\n", "link": "http://arxiv.org/abs/2506.13323v1", "date": "2025-06-16", "relevancy": 2.3986, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4831}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.478}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tady%3A%20A%20Neural%20Disassembler%20without%20Structural%20Constraint%20Violations&body=Title%3A%20Tady%3A%20A%20Neural%20Disassembler%20without%20Structural%20Constraint%20Violations%0AAuthor%3A%20Siliang%20Qin%20and%20Fengrui%20Yang%20and%20Hao%20Wang%20and%20Bolun%20Zhang%20and%20Zeyu%20Gao%20and%20Chao%20Zhang%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Disassembly%20is%20a%20crucial%20yet%20challenging%20step%20in%20binary%20analysis.%20While%0Aemerging%20neural%20disassemblers%20show%20promise%20for%20efficiency%20and%20accuracy%2C%20they%0Afrequently%20generate%20outputs%20violating%20fundamental%20structural%20constraints%2C%20which%0Asignificantly%20compromise%20their%20practical%20usability.%20To%20address%20this%20critical%0Aproblem%2C%20we%20regularize%20the%20disassembly%20solution%20space%20by%20formalizing%20and%0Aapplying%20key%20structural%20constraints%20based%20on%20post-dominance%20relations.%20This%0Aapproach%20systematically%20detects%20widespread%20errors%20in%20existing%20neural%0Adisassemblers%27%20outputs.%20These%20errors%20often%20originate%20from%20models%27%20limited%0Acontext%20modeling%20and%20instruction-level%20decoding%20that%20neglect%20global%20structural%0Aintegrity.%20We%20introduce%20Tady%2C%20a%20novel%20neural%20disassembler%20featuring%20an%20improved%0Amodel%20architecture%20and%20a%20dedicated%20post-processing%20algorithm%2C%20specifically%0Aengineered%20to%20address%20these%20deficiencies.%20Comprehensive%20evaluations%20on%20diverse%0Abinaries%20demonstrate%20that%20Tady%20effectively%20eliminates%20structural%20constraint%0Aviolations%20and%20functions%20with%20high%20efficiency%2C%20while%20maintaining%0Ainstruction-level%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTady%253A%2520A%2520Neural%2520Disassembler%2520without%2520Structural%2520Constraint%2520Violations%26entry.906535625%3DSiliang%2520Qin%2520and%2520Fengrui%2520Yang%2520and%2520Hao%2520Wang%2520and%2520Bolun%2520Zhang%2520and%2520Zeyu%2520Gao%2520and%2520Chao%2520Zhang%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520Disassembly%2520is%2520a%2520crucial%2520yet%2520challenging%2520step%2520in%2520binary%2520analysis.%2520While%250Aemerging%2520neural%2520disassemblers%2520show%2520promise%2520for%2520efficiency%2520and%2520accuracy%252C%2520they%250Afrequently%2520generate%2520outputs%2520violating%2520fundamental%2520structural%2520constraints%252C%2520which%250Asignificantly%2520compromise%2520their%2520practical%2520usability.%2520To%2520address%2520this%2520critical%250Aproblem%252C%2520we%2520regularize%2520the%2520disassembly%2520solution%2520space%2520by%2520formalizing%2520and%250Aapplying%2520key%2520structural%2520constraints%2520based%2520on%2520post-dominance%2520relations.%2520This%250Aapproach%2520systematically%2520detects%2520widespread%2520errors%2520in%2520existing%2520neural%250Adisassemblers%2527%2520outputs.%2520These%2520errors%2520often%2520originate%2520from%2520models%2527%2520limited%250Acontext%2520modeling%2520and%2520instruction-level%2520decoding%2520that%2520neglect%2520global%2520structural%250Aintegrity.%2520We%2520introduce%2520Tady%252C%2520a%2520novel%2520neural%2520disassembler%2520featuring%2520an%2520improved%250Amodel%2520architecture%2520and%2520a%2520dedicated%2520post-processing%2520algorithm%252C%2520specifically%250Aengineered%2520to%2520address%2520these%2520deficiencies.%2520Comprehensive%2520evaluations%2520on%2520diverse%250Abinaries%2520demonstrate%2520that%2520Tady%2520effectively%2520eliminates%2520structural%2520constraint%250Aviolations%2520and%2520functions%2520with%2520high%2520efficiency%252C%2520while%2520maintaining%250Ainstruction-level%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tady%3A%20A%20Neural%20Disassembler%20without%20Structural%20Constraint%20Violations&entry.906535625=Siliang%20Qin%20and%20Fengrui%20Yang%20and%20Hao%20Wang%20and%20Bolun%20Zhang%20and%20Zeyu%20Gao%20and%20Chao%20Zhang%20and%20Kai%20Chen&entry.1292438233=%20%20Disassembly%20is%20a%20crucial%20yet%20challenging%20step%20in%20binary%20analysis.%20While%0Aemerging%20neural%20disassemblers%20show%20promise%20for%20efficiency%20and%20accuracy%2C%20they%0Afrequently%20generate%20outputs%20violating%20fundamental%20structural%20constraints%2C%20which%0Asignificantly%20compromise%20their%20practical%20usability.%20To%20address%20this%20critical%0Aproblem%2C%20we%20regularize%20the%20disassembly%20solution%20space%20by%20formalizing%20and%0Aapplying%20key%20structural%20constraints%20based%20on%20post-dominance%20relations.%20This%0Aapproach%20systematically%20detects%20widespread%20errors%20in%20existing%20neural%0Adisassemblers%27%20outputs.%20These%20errors%20often%20originate%20from%20models%27%20limited%0Acontext%20modeling%20and%20instruction-level%20decoding%20that%20neglect%20global%20structural%0Aintegrity.%20We%20introduce%20Tady%2C%20a%20novel%20neural%20disassembler%20featuring%20an%20improved%0Amodel%20architecture%20and%20a%20dedicated%20post-processing%20algorithm%2C%20specifically%0Aengineered%20to%20address%20these%20deficiencies.%20Comprehensive%20evaluations%20on%20diverse%0Abinaries%20demonstrate%20that%20Tady%20effectively%20eliminates%20structural%20constraint%0Aviolations%20and%20functions%20with%20high%20efficiency%2C%20while%20maintaining%0Ainstruction-level%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13323v1&entry.124074799=Read"},
{"title": "Prefix-Tuning+: Modernizing Prefix-Tuning through Attention Independent\n  Prefix Data", "author": "Haonan Wang and Brian Chen and Li Siquan and Liang Xinhe and Tianyang Hu and Hwee Kuan Lee and Kenji Kawaguchi", "abstract": "  Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for\nrapidly adapting large language models (LLMs) to downstream tasks.\nPrefix-Tuning, an early and effective PEFT technique, demonstrated the ability\nto achieve performance comparable to full fine-tuning with significantly\nreduced computational and memory overhead. However, despite its earlier\nsuccess, its effectiveness in training modern state-of-the-art LLMs has been\nvery limited. In this work, we demonstrate empirically that Prefix-Tuning\nunderperforms on LLMs because of an inherent tradeoff between input and prefix\nsignificance within the attention head. This motivates us to introduce\nPrefix-Tuning+, a novel architecture that generalizes the principles of\nPrefix-Tuning while addressing its shortcomings by shifting the prefix module\nout of the attention head itself. We further provide an overview of our\nconstruction process to guide future users when constructing their own\ncontext-based methods. Our experiments show that, across a diverse set of\nbenchmarks, Prefix-Tuning+ consistently outperforms existing Prefix-Tuning\nmethods. Notably, it achieves performance on par with the widely adopted LoRA\nmethod on several general benchmarks, highlighting the potential modern\nextension of Prefix-Tuning approaches. Our findings suggest that by overcoming\nits inherent limitations, Prefix-Tuning can remain a competitive and relevant\nresearch direction in the landscape of parameter-efficient LLM adaptation.\n", "link": "http://arxiv.org/abs/2506.13674v1", "date": "2025-06-16", "relevancy": 2.3975, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4837}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4779}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prefix-Tuning%2B%3A%20Modernizing%20Prefix-Tuning%20through%20Attention%20Independent%0A%20%20Prefix%20Data&body=Title%3A%20Prefix-Tuning%2B%3A%20Modernizing%20Prefix-Tuning%20through%20Attention%20Independent%0A%20%20Prefix%20Data%0AAuthor%3A%20Haonan%20Wang%20and%20Brian%20Chen%20and%20Li%20Siquan%20and%20Liang%20Xinhe%20and%20Tianyang%20Hu%20and%20Hwee%20Kuan%20Lee%20and%20Kenji%20Kawaguchi%0AAbstract%3A%20%20%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20have%20become%20crucial%20for%0Arapidly%20adapting%20large%20language%20models%20%28LLMs%29%20to%20downstream%20tasks.%0APrefix-Tuning%2C%20an%20early%20and%20effective%20PEFT%20technique%2C%20demonstrated%20the%20ability%0Ato%20achieve%20performance%20comparable%20to%20full%20fine-tuning%20with%20significantly%0Areduced%20computational%20and%20memory%20overhead.%20However%2C%20despite%20its%20earlier%0Asuccess%2C%20its%20effectiveness%20in%20training%20modern%20state-of-the-art%20LLMs%20has%20been%0Avery%20limited.%20In%20this%20work%2C%20we%20demonstrate%20empirically%20that%20Prefix-Tuning%0Aunderperforms%20on%20LLMs%20because%20of%20an%20inherent%20tradeoff%20between%20input%20and%20prefix%0Asignificance%20within%20the%20attention%20head.%20This%20motivates%20us%20to%20introduce%0APrefix-Tuning%2B%2C%20a%20novel%20architecture%20that%20generalizes%20the%20principles%20of%0APrefix-Tuning%20while%20addressing%20its%20shortcomings%20by%20shifting%20the%20prefix%20module%0Aout%20of%20the%20attention%20head%20itself.%20We%20further%20provide%20an%20overview%20of%20our%0Aconstruction%20process%20to%20guide%20future%20users%20when%20constructing%20their%20own%0Acontext-based%20methods.%20Our%20experiments%20show%20that%2C%20across%20a%20diverse%20set%20of%0Abenchmarks%2C%20Prefix-Tuning%2B%20consistently%20outperforms%20existing%20Prefix-Tuning%0Amethods.%20Notably%2C%20it%20achieves%20performance%20on%20par%20with%20the%20widely%20adopted%20LoRA%0Amethod%20on%20several%20general%20benchmarks%2C%20highlighting%20the%20potential%20modern%0Aextension%20of%20Prefix-Tuning%20approaches.%20Our%20findings%20suggest%20that%20by%20overcoming%0Aits%20inherent%20limitations%2C%20Prefix-Tuning%20can%20remain%20a%20competitive%20and%20relevant%0Aresearch%20direction%20in%20the%20landscape%20of%20parameter-efficient%20LLM%20adaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrefix-Tuning%252B%253A%2520Modernizing%2520Prefix-Tuning%2520through%2520Attention%2520Independent%250A%2520%2520Prefix%2520Data%26entry.906535625%3DHaonan%2520Wang%2520and%2520Brian%2520Chen%2520and%2520Li%2520Siquan%2520and%2520Liang%2520Xinhe%2520and%2520Tianyang%2520Hu%2520and%2520Hwee%2520Kuan%2520Lee%2520and%2520Kenji%2520Kawaguchi%26entry.1292438233%3D%2520%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520methods%2520have%2520become%2520crucial%2520for%250Arapidly%2520adapting%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520downstream%2520tasks.%250APrefix-Tuning%252C%2520an%2520early%2520and%2520effective%2520PEFT%2520technique%252C%2520demonstrated%2520the%2520ability%250Ato%2520achieve%2520performance%2520comparable%2520to%2520full%2520fine-tuning%2520with%2520significantly%250Areduced%2520computational%2520and%2520memory%2520overhead.%2520However%252C%2520despite%2520its%2520earlier%250Asuccess%252C%2520its%2520effectiveness%2520in%2520training%2520modern%2520state-of-the-art%2520LLMs%2520has%2520been%250Avery%2520limited.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520empirically%2520that%2520Prefix-Tuning%250Aunderperforms%2520on%2520LLMs%2520because%2520of%2520an%2520inherent%2520tradeoff%2520between%2520input%2520and%2520prefix%250Asignificance%2520within%2520the%2520attention%2520head.%2520This%2520motivates%2520us%2520to%2520introduce%250APrefix-Tuning%252B%252C%2520a%2520novel%2520architecture%2520that%2520generalizes%2520the%2520principles%2520of%250APrefix-Tuning%2520while%2520addressing%2520its%2520shortcomings%2520by%2520shifting%2520the%2520prefix%2520module%250Aout%2520of%2520the%2520attention%2520head%2520itself.%2520We%2520further%2520provide%2520an%2520overview%2520of%2520our%250Aconstruction%2520process%2520to%2520guide%2520future%2520users%2520when%2520constructing%2520their%2520own%250Acontext-based%2520methods.%2520Our%2520experiments%2520show%2520that%252C%2520across%2520a%2520diverse%2520set%2520of%250Abenchmarks%252C%2520Prefix-Tuning%252B%2520consistently%2520outperforms%2520existing%2520Prefix-Tuning%250Amethods.%2520Notably%252C%2520it%2520achieves%2520performance%2520on%2520par%2520with%2520the%2520widely%2520adopted%2520LoRA%250Amethod%2520on%2520several%2520general%2520benchmarks%252C%2520highlighting%2520the%2520potential%2520modern%250Aextension%2520of%2520Prefix-Tuning%2520approaches.%2520Our%2520findings%2520suggest%2520that%2520by%2520overcoming%250Aits%2520inherent%2520limitations%252C%2520Prefix-Tuning%2520can%2520remain%2520a%2520competitive%2520and%2520relevant%250Aresearch%2520direction%2520in%2520the%2520landscape%2520of%2520parameter-efficient%2520LLM%2520adaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prefix-Tuning%2B%3A%20Modernizing%20Prefix-Tuning%20through%20Attention%20Independent%0A%20%20Prefix%20Data&entry.906535625=Haonan%20Wang%20and%20Brian%20Chen%20and%20Li%20Siquan%20and%20Liang%20Xinhe%20and%20Tianyang%20Hu%20and%20Hwee%20Kuan%20Lee%20and%20Kenji%20Kawaguchi&entry.1292438233=%20%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20have%20become%20crucial%20for%0Arapidly%20adapting%20large%20language%20models%20%28LLMs%29%20to%20downstream%20tasks.%0APrefix-Tuning%2C%20an%20early%20and%20effective%20PEFT%20technique%2C%20demonstrated%20the%20ability%0Ato%20achieve%20performance%20comparable%20to%20full%20fine-tuning%20with%20significantly%0Areduced%20computational%20and%20memory%20overhead.%20However%2C%20despite%20its%20earlier%0Asuccess%2C%20its%20effectiveness%20in%20training%20modern%20state-of-the-art%20LLMs%20has%20been%0Avery%20limited.%20In%20this%20work%2C%20we%20demonstrate%20empirically%20that%20Prefix-Tuning%0Aunderperforms%20on%20LLMs%20because%20of%20an%20inherent%20tradeoff%20between%20input%20and%20prefix%0Asignificance%20within%20the%20attention%20head.%20This%20motivates%20us%20to%20introduce%0APrefix-Tuning%2B%2C%20a%20novel%20architecture%20that%20generalizes%20the%20principles%20of%0APrefix-Tuning%20while%20addressing%20its%20shortcomings%20by%20shifting%20the%20prefix%20module%0Aout%20of%20the%20attention%20head%20itself.%20We%20further%20provide%20an%20overview%20of%20our%0Aconstruction%20process%20to%20guide%20future%20users%20when%20constructing%20their%20own%0Acontext-based%20methods.%20Our%20experiments%20show%20that%2C%20across%20a%20diverse%20set%20of%0Abenchmarks%2C%20Prefix-Tuning%2B%20consistently%20outperforms%20existing%20Prefix-Tuning%0Amethods.%20Notably%2C%20it%20achieves%20performance%20on%20par%20with%20the%20widely%20adopted%20LoRA%0Amethod%20on%20several%20general%20benchmarks%2C%20highlighting%20the%20potential%20modern%0Aextension%20of%20Prefix-Tuning%20approaches.%20Our%20findings%20suggest%20that%20by%20overcoming%0Aits%20inherent%20limitations%2C%20Prefix-Tuning%20can%20remain%20a%20competitive%20and%20relevant%0Aresearch%20direction%20in%20the%20landscape%20of%20parameter-efficient%20LLM%20adaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13674v1&entry.124074799=Read"},
{"title": "Online Optimization for Learning to Communicate over Time-Correlated\n  Channels", "author": "Zheshun Wu and Junfan Li and Zenglin Xu and Sumei Sun and Jie Liu", "abstract": "  Machine learning techniques have garnered great interest in designing\ncommunication systems owing to their capacity in tackling with channel\nuncertainty. To provide theoretical guarantees for learning-based communication\nsystems, some recent works analyze generalization bounds for devised methods\nbased on the assumption of Independently and Identically Distributed (I.I.D.)\nchannels, a condition rarely met in practical scenarios. In this paper, we drop\nthe I.I.D. channel assumption and study an online optimization problem of\nlearning to communicate over time-correlated channels. To address this issue,\nwe further focus on two specific tasks: optimizing channel decoders for\ntime-correlated fading channels and selecting optimal codebooks for\ntime-correlated additive noise channels. For utilizing temporal dependence of\nconsidered channels to better learn communication systems, we develop two\nonline optimization algorithms based on the optimistic online mirror descent\nframework. Furthermore, we provide theoretical guarantees for proposed\nalgorithms via deriving sub-linear regret bound on the expected error\nprobability of learned systems. Extensive simulation experiments have been\nconducted to validate that our presented approaches can leverage the channel\ncorrelation to achieve a lower average symbol error rate compared to baseline\nmethods, consistent with our theoretical findings.\n", "link": "http://arxiv.org/abs/2409.00575v5", "date": "2025-06-16", "relevancy": 2.3792, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4808}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4779}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Optimization%20for%20Learning%20to%20Communicate%20over%20Time-Correlated%0A%20%20Channels&body=Title%3A%20Online%20Optimization%20for%20Learning%20to%20Communicate%20over%20Time-Correlated%0A%20%20Channels%0AAuthor%3A%20Zheshun%20Wu%20and%20Junfan%20Li%20and%20Zenglin%20Xu%20and%20Sumei%20Sun%20and%20Jie%20Liu%0AAbstract%3A%20%20%20Machine%20learning%20techniques%20have%20garnered%20great%20interest%20in%20designing%0Acommunication%20systems%20owing%20to%20their%20capacity%20in%20tackling%20with%20channel%0Auncertainty.%20To%20provide%20theoretical%20guarantees%20for%20learning-based%20communication%0Asystems%2C%20some%20recent%20works%20analyze%20generalization%20bounds%20for%20devised%20methods%0Abased%20on%20the%20assumption%20of%20Independently%20and%20Identically%20Distributed%20%28I.I.D.%29%0Achannels%2C%20a%20condition%20rarely%20met%20in%20practical%20scenarios.%20In%20this%20paper%2C%20we%20drop%0Athe%20I.I.D.%20channel%20assumption%20and%20study%20an%20online%20optimization%20problem%20of%0Alearning%20to%20communicate%20over%20time-correlated%20channels.%20To%20address%20this%20issue%2C%0Awe%20further%20focus%20on%20two%20specific%20tasks%3A%20optimizing%20channel%20decoders%20for%0Atime-correlated%20fading%20channels%20and%20selecting%20optimal%20codebooks%20for%0Atime-correlated%20additive%20noise%20channels.%20For%20utilizing%20temporal%20dependence%20of%0Aconsidered%20channels%20to%20better%20learn%20communication%20systems%2C%20we%20develop%20two%0Aonline%20optimization%20algorithms%20based%20on%20the%20optimistic%20online%20mirror%20descent%0Aframework.%20Furthermore%2C%20we%20provide%20theoretical%20guarantees%20for%20proposed%0Aalgorithms%20via%20deriving%20sub-linear%20regret%20bound%20on%20the%20expected%20error%0Aprobability%20of%20learned%20systems.%20Extensive%20simulation%20experiments%20have%20been%0Aconducted%20to%20validate%20that%20our%20presented%20approaches%20can%20leverage%20the%20channel%0Acorrelation%20to%20achieve%20a%20lower%20average%20symbol%20error%20rate%20compared%20to%20baseline%0Amethods%2C%20consistent%20with%20our%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00575v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Optimization%2520for%2520Learning%2520to%2520Communicate%2520over%2520Time-Correlated%250A%2520%2520Channels%26entry.906535625%3DZheshun%2520Wu%2520and%2520Junfan%2520Li%2520and%2520Zenglin%2520Xu%2520and%2520Sumei%2520Sun%2520and%2520Jie%2520Liu%26entry.1292438233%3D%2520%2520Machine%2520learning%2520techniques%2520have%2520garnered%2520great%2520interest%2520in%2520designing%250Acommunication%2520systems%2520owing%2520to%2520their%2520capacity%2520in%2520tackling%2520with%2520channel%250Auncertainty.%2520To%2520provide%2520theoretical%2520guarantees%2520for%2520learning-based%2520communication%250Asystems%252C%2520some%2520recent%2520works%2520analyze%2520generalization%2520bounds%2520for%2520devised%2520methods%250Abased%2520on%2520the%2520assumption%2520of%2520Independently%2520and%2520Identically%2520Distributed%2520%2528I.I.D.%2529%250Achannels%252C%2520a%2520condition%2520rarely%2520met%2520in%2520practical%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520drop%250Athe%2520I.I.D.%2520channel%2520assumption%2520and%2520study%2520an%2520online%2520optimization%2520problem%2520of%250Alearning%2520to%2520communicate%2520over%2520time-correlated%2520channels.%2520To%2520address%2520this%2520issue%252C%250Awe%2520further%2520focus%2520on%2520two%2520specific%2520tasks%253A%2520optimizing%2520channel%2520decoders%2520for%250Atime-correlated%2520fading%2520channels%2520and%2520selecting%2520optimal%2520codebooks%2520for%250Atime-correlated%2520additive%2520noise%2520channels.%2520For%2520utilizing%2520temporal%2520dependence%2520of%250Aconsidered%2520channels%2520to%2520better%2520learn%2520communication%2520systems%252C%2520we%2520develop%2520two%250Aonline%2520optimization%2520algorithms%2520based%2520on%2520the%2520optimistic%2520online%2520mirror%2520descent%250Aframework.%2520Furthermore%252C%2520we%2520provide%2520theoretical%2520guarantees%2520for%2520proposed%250Aalgorithms%2520via%2520deriving%2520sub-linear%2520regret%2520bound%2520on%2520the%2520expected%2520error%250Aprobability%2520of%2520learned%2520systems.%2520Extensive%2520simulation%2520experiments%2520have%2520been%250Aconducted%2520to%2520validate%2520that%2520our%2520presented%2520approaches%2520can%2520leverage%2520the%2520channel%250Acorrelation%2520to%2520achieve%2520a%2520lower%2520average%2520symbol%2520error%2520rate%2520compared%2520to%2520baseline%250Amethods%252C%2520consistent%2520with%2520our%2520theoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00575v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Optimization%20for%20Learning%20to%20Communicate%20over%20Time-Correlated%0A%20%20Channels&entry.906535625=Zheshun%20Wu%20and%20Junfan%20Li%20and%20Zenglin%20Xu%20and%20Sumei%20Sun%20and%20Jie%20Liu&entry.1292438233=%20%20Machine%20learning%20techniques%20have%20garnered%20great%20interest%20in%20designing%0Acommunication%20systems%20owing%20to%20their%20capacity%20in%20tackling%20with%20channel%0Auncertainty.%20To%20provide%20theoretical%20guarantees%20for%20learning-based%20communication%0Asystems%2C%20some%20recent%20works%20analyze%20generalization%20bounds%20for%20devised%20methods%0Abased%20on%20the%20assumption%20of%20Independently%20and%20Identically%20Distributed%20%28I.I.D.%29%0Achannels%2C%20a%20condition%20rarely%20met%20in%20practical%20scenarios.%20In%20this%20paper%2C%20we%20drop%0Athe%20I.I.D.%20channel%20assumption%20and%20study%20an%20online%20optimization%20problem%20of%0Alearning%20to%20communicate%20over%20time-correlated%20channels.%20To%20address%20this%20issue%2C%0Awe%20further%20focus%20on%20two%20specific%20tasks%3A%20optimizing%20channel%20decoders%20for%0Atime-correlated%20fading%20channels%20and%20selecting%20optimal%20codebooks%20for%0Atime-correlated%20additive%20noise%20channels.%20For%20utilizing%20temporal%20dependence%20of%0Aconsidered%20channels%20to%20better%20learn%20communication%20systems%2C%20we%20develop%20two%0Aonline%20optimization%20algorithms%20based%20on%20the%20optimistic%20online%20mirror%20descent%0Aframework.%20Furthermore%2C%20we%20provide%20theoretical%20guarantees%20for%20proposed%0Aalgorithms%20via%20deriving%20sub-linear%20regret%20bound%20on%20the%20expected%20error%0Aprobability%20of%20learned%20systems.%20Extensive%20simulation%20experiments%20have%20been%0Aconducted%20to%20validate%20that%20our%20presented%20approaches%20can%20leverage%20the%20channel%0Acorrelation%20to%20achieve%20a%20lower%20average%20symbol%20error%20rate%20compared%20to%20baseline%0Amethods%2C%20consistent%20with%20our%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00575v5&entry.124074799=Read"},
{"title": "Geometric Kolmogorov-Arnold Superposition Theorem", "author": "Francesco Alesiani and Takashi Maruyama and Henrik Christiansen and Viktor Zaverkin", "abstract": "  The Kolmogorov-Arnold Theorem (KAT), or more generally, the Kolmogorov\nSuperposition Theorem (KST), establishes that any non-linear multivariate\nfunction can be exactly represented as a finite superposition of non-linear\nunivariate functions. Unlike the universal approximation theorem, which\nprovides only an approximate representation without guaranteeing a fixed\nnetwork size, KST offers a theoretically exact decomposition. The\nKolmogorov-Arnold Network (KAN) was introduced as a trainable model to\nimplement KAT, and recent advancements have adapted KAN using concepts from\nmodern neural networks. However, KAN struggles to effectively model physical\nsystems that require inherent equivariance or invariance geometric symmetries\nas $E(3)$ transformations, a key property for many scientific and engineering\napplications. In this work, we propose a novel extension of KAT and KAN to\nincorporate equivariance and invariance over various group actions, including\n$O(n)$, $O(1,n)$, $S_n$, and general $GL$, enabling accurate and efficient\nmodeling of these systems. Our approach provides a unified approach that\nbridges the gap between mathematical theory and practical architectures for\nphysical systems, expanding the applicability of KAN to a broader class of\nproblems. We provide experimental validation on molecular dynamical systems and\nparticle physics.\n", "link": "http://arxiv.org/abs/2502.16664v2", "date": "2025-06-16", "relevancy": 2.3539, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4797}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4727}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Kolmogorov-Arnold%20Superposition%20Theorem&body=Title%3A%20Geometric%20Kolmogorov-Arnold%20Superposition%20Theorem%0AAuthor%3A%20Francesco%20Alesiani%20and%20Takashi%20Maruyama%20and%20Henrik%20Christiansen%20and%20Viktor%20Zaverkin%0AAbstract%3A%20%20%20The%20Kolmogorov-Arnold%20Theorem%20%28KAT%29%2C%20or%20more%20generally%2C%20the%20Kolmogorov%0ASuperposition%20Theorem%20%28KST%29%2C%20establishes%20that%20any%20non-linear%20multivariate%0Afunction%20can%20be%20exactly%20represented%20as%20a%20finite%20superposition%20of%20non-linear%0Aunivariate%20functions.%20Unlike%20the%20universal%20approximation%20theorem%2C%20which%0Aprovides%20only%20an%20approximate%20representation%20without%20guaranteeing%20a%20fixed%0Anetwork%20size%2C%20KST%20offers%20a%20theoretically%20exact%20decomposition.%20The%0AKolmogorov-Arnold%20Network%20%28KAN%29%20was%20introduced%20as%20a%20trainable%20model%20to%0Aimplement%20KAT%2C%20and%20recent%20advancements%20have%20adapted%20KAN%20using%20concepts%20from%0Amodern%20neural%20networks.%20However%2C%20KAN%20struggles%20to%20effectively%20model%20physical%0Asystems%20that%20require%20inherent%20equivariance%20or%20invariance%20geometric%20symmetries%0Aas%20%24E%283%29%24%20transformations%2C%20a%20key%20property%20for%20many%20scientific%20and%20engineering%0Aapplications.%20In%20this%20work%2C%20we%20propose%20a%20novel%20extension%20of%20KAT%20and%20KAN%20to%0Aincorporate%20equivariance%20and%20invariance%20over%20various%20group%20actions%2C%20including%0A%24O%28n%29%24%2C%20%24O%281%2Cn%29%24%2C%20%24S_n%24%2C%20and%20general%20%24GL%24%2C%20enabling%20accurate%20and%20efficient%0Amodeling%20of%20these%20systems.%20Our%20approach%20provides%20a%20unified%20approach%20that%0Abridges%20the%20gap%20between%20mathematical%20theory%20and%20practical%20architectures%20for%0Aphysical%20systems%2C%20expanding%20the%20applicability%20of%20KAN%20to%20a%20broader%20class%20of%0Aproblems.%20We%20provide%20experimental%20validation%20on%20molecular%20dynamical%20systems%20and%0Aparticle%20physics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.16664v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Kolmogorov-Arnold%2520Superposition%2520Theorem%26entry.906535625%3DFrancesco%2520Alesiani%2520and%2520Takashi%2520Maruyama%2520and%2520Henrik%2520Christiansen%2520and%2520Viktor%2520Zaverkin%26entry.1292438233%3D%2520%2520The%2520Kolmogorov-Arnold%2520Theorem%2520%2528KAT%2529%252C%2520or%2520more%2520generally%252C%2520the%2520Kolmogorov%250ASuperposition%2520Theorem%2520%2528KST%2529%252C%2520establishes%2520that%2520any%2520non-linear%2520multivariate%250Afunction%2520can%2520be%2520exactly%2520represented%2520as%2520a%2520finite%2520superposition%2520of%2520non-linear%250Aunivariate%2520functions.%2520Unlike%2520the%2520universal%2520approximation%2520theorem%252C%2520which%250Aprovides%2520only%2520an%2520approximate%2520representation%2520without%2520guaranteeing%2520a%2520fixed%250Anetwork%2520size%252C%2520KST%2520offers%2520a%2520theoretically%2520exact%2520decomposition.%2520The%250AKolmogorov-Arnold%2520Network%2520%2528KAN%2529%2520was%2520introduced%2520as%2520a%2520trainable%2520model%2520to%250Aimplement%2520KAT%252C%2520and%2520recent%2520advancements%2520have%2520adapted%2520KAN%2520using%2520concepts%2520from%250Amodern%2520neural%2520networks.%2520However%252C%2520KAN%2520struggles%2520to%2520effectively%2520model%2520physical%250Asystems%2520that%2520require%2520inherent%2520equivariance%2520or%2520invariance%2520geometric%2520symmetries%250Aas%2520%2524E%25283%2529%2524%2520transformations%252C%2520a%2520key%2520property%2520for%2520many%2520scientific%2520and%2520engineering%250Aapplications.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520extension%2520of%2520KAT%2520and%2520KAN%2520to%250Aincorporate%2520equivariance%2520and%2520invariance%2520over%2520various%2520group%2520actions%252C%2520including%250A%2524O%2528n%2529%2524%252C%2520%2524O%25281%252Cn%2529%2524%252C%2520%2524S_n%2524%252C%2520and%2520general%2520%2524GL%2524%252C%2520enabling%2520accurate%2520and%2520efficient%250Amodeling%2520of%2520these%2520systems.%2520Our%2520approach%2520provides%2520a%2520unified%2520approach%2520that%250Abridges%2520the%2520gap%2520between%2520mathematical%2520theory%2520and%2520practical%2520architectures%2520for%250Aphysical%2520systems%252C%2520expanding%2520the%2520applicability%2520of%2520KAN%2520to%2520a%2520broader%2520class%2520of%250Aproblems.%2520We%2520provide%2520experimental%2520validation%2520on%2520molecular%2520dynamical%2520systems%2520and%250Aparticle%2520physics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.16664v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Kolmogorov-Arnold%20Superposition%20Theorem&entry.906535625=Francesco%20Alesiani%20and%20Takashi%20Maruyama%20and%20Henrik%20Christiansen%20and%20Viktor%20Zaverkin&entry.1292438233=%20%20The%20Kolmogorov-Arnold%20Theorem%20%28KAT%29%2C%20or%20more%20generally%2C%20the%20Kolmogorov%0ASuperposition%20Theorem%20%28KST%29%2C%20establishes%20that%20any%20non-linear%20multivariate%0Afunction%20can%20be%20exactly%20represented%20as%20a%20finite%20superposition%20of%20non-linear%0Aunivariate%20functions.%20Unlike%20the%20universal%20approximation%20theorem%2C%20which%0Aprovides%20only%20an%20approximate%20representation%20without%20guaranteeing%20a%20fixed%0Anetwork%20size%2C%20KST%20offers%20a%20theoretically%20exact%20decomposition.%20The%0AKolmogorov-Arnold%20Network%20%28KAN%29%20was%20introduced%20as%20a%20trainable%20model%20to%0Aimplement%20KAT%2C%20and%20recent%20advancements%20have%20adapted%20KAN%20using%20concepts%20from%0Amodern%20neural%20networks.%20However%2C%20KAN%20struggles%20to%20effectively%20model%20physical%0Asystems%20that%20require%20inherent%20equivariance%20or%20invariance%20geometric%20symmetries%0Aas%20%24E%283%29%24%20transformations%2C%20a%20key%20property%20for%20many%20scientific%20and%20engineering%0Aapplications.%20In%20this%20work%2C%20we%20propose%20a%20novel%20extension%20of%20KAT%20and%20KAN%20to%0Aincorporate%20equivariance%20and%20invariance%20over%20various%20group%20actions%2C%20including%0A%24O%28n%29%24%2C%20%24O%281%2Cn%29%24%2C%20%24S_n%24%2C%20and%20general%20%24GL%24%2C%20enabling%20accurate%20and%20efficient%0Amodeling%20of%20these%20systems.%20Our%20approach%20provides%20a%20unified%20approach%20that%0Abridges%20the%20gap%20between%20mathematical%20theory%20and%20practical%20architectures%20for%0Aphysical%20systems%2C%20expanding%20the%20applicability%20of%20KAN%20to%20a%20broader%20class%20of%0Aproblems.%20We%20provide%20experimental%20validation%20on%20molecular%20dynamical%20systems%20and%0Aparticle%20physics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.16664v2&entry.124074799=Read"},
{"title": "LeVERB: Humanoid Whole-Body Control with Latent Vision-Language\n  Instruction", "author": "Haoru Xue and Xiaoyu Huang and Dantong Niu and Qiayuan Liao and Thomas Kragerud and Jan Tommy Gravdahl and Xue Bin Peng and Guanya Shi and Trevor Darrell and Koushil Screenath and Shankar Sastry", "abstract": "  Vision-language-action (VLA) models have demonstrated strong semantic\nunderstanding and zero-shot generalization, yet most existing systems assume an\naccurate low-level controller with hand-crafted action \"vocabulary\" such as\nend-effector pose or root velocity. This assumption confines prior work to\nquasi-static tasks and precludes the agile, whole-body behaviors required by\nhumanoid whole-body control (WBC) tasks. To capture this gap in the literature,\nwe start by introducing the first sim-to-real-ready, vision-language,\nclosed-loop benchmark for humanoid WBC, comprising over 150 tasks from 10\ncategories. We then propose LeVERB: Latent Vision-Language-Encoded Robot\nBehavior, a hierarchical latent instruction-following framework for humanoid\nvision-language WBC, the first of its kind. At the top level, a vision-language\npolicy learns a latent action vocabulary from synthetically rendered kinematic\ndemonstrations; at the low level, a reinforcement-learned WBC policy consumes\nthese latent verbs to generate dynamics-level commands. In our benchmark,\nLeVERB can zero-shot attain a 80% success rate on simple visual navigation\ntasks, and 58.5% success rate overall, outperforming naive hierarchical\nwhole-body VLA implementation by 7.8 times.\n", "link": "http://arxiv.org/abs/2506.13751v1", "date": "2025-06-16", "relevancy": 2.3472, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5929}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5856}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LeVERB%3A%20Humanoid%20Whole-Body%20Control%20with%20Latent%20Vision-Language%0A%20%20Instruction&body=Title%3A%20LeVERB%3A%20Humanoid%20Whole-Body%20Control%20with%20Latent%20Vision-Language%0A%20%20Instruction%0AAuthor%3A%20Haoru%20Xue%20and%20Xiaoyu%20Huang%20and%20Dantong%20Niu%20and%20Qiayuan%20Liao%20and%20Thomas%20Kragerud%20and%20Jan%20Tommy%20Gravdahl%20and%20Xue%20Bin%20Peng%20and%20Guanya%20Shi%20and%20Trevor%20Darrell%20and%20Koushil%20Screenath%20and%20Shankar%20Sastry%0AAbstract%3A%20%20%20Vision-language-action%20%28VLA%29%20models%20have%20demonstrated%20strong%20semantic%0Aunderstanding%20and%20zero-shot%20generalization%2C%20yet%20most%20existing%20systems%20assume%20an%0Aaccurate%20low-level%20controller%20with%20hand-crafted%20action%20%22vocabulary%22%20such%20as%0Aend-effector%20pose%20or%20root%20velocity.%20This%20assumption%20confines%20prior%20work%20to%0Aquasi-static%20tasks%20and%20precludes%20the%20agile%2C%20whole-body%20behaviors%20required%20by%0Ahumanoid%20whole-body%20control%20%28WBC%29%20tasks.%20To%20capture%20this%20gap%20in%20the%20literature%2C%0Awe%20start%20by%20introducing%20the%20first%20sim-to-real-ready%2C%20vision-language%2C%0Aclosed-loop%20benchmark%20for%20humanoid%20WBC%2C%20comprising%20over%20150%20tasks%20from%2010%0Acategories.%20We%20then%20propose%20LeVERB%3A%20Latent%20Vision-Language-Encoded%20Robot%0ABehavior%2C%20a%20hierarchical%20latent%20instruction-following%20framework%20for%20humanoid%0Avision-language%20WBC%2C%20the%20first%20of%20its%20kind.%20At%20the%20top%20level%2C%20a%20vision-language%0Apolicy%20learns%20a%20latent%20action%20vocabulary%20from%20synthetically%20rendered%20kinematic%0Ademonstrations%3B%20at%20the%20low%20level%2C%20a%20reinforcement-learned%20WBC%20policy%20consumes%0Athese%20latent%20verbs%20to%20generate%20dynamics-level%20commands.%20In%20our%20benchmark%2C%0ALeVERB%20can%20zero-shot%20attain%20a%2080%25%20success%20rate%20on%20simple%20visual%20navigation%0Atasks%2C%20and%2058.5%25%20success%20rate%20overall%2C%20outperforming%20naive%20hierarchical%0Awhole-body%20VLA%20implementation%20by%207.8%20times.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeVERB%253A%2520Humanoid%2520Whole-Body%2520Control%2520with%2520Latent%2520Vision-Language%250A%2520%2520Instruction%26entry.906535625%3DHaoru%2520Xue%2520and%2520Xiaoyu%2520Huang%2520and%2520Dantong%2520Niu%2520and%2520Qiayuan%2520Liao%2520and%2520Thomas%2520Kragerud%2520and%2520Jan%2520Tommy%2520Gravdahl%2520and%2520Xue%2520Bin%2520Peng%2520and%2520Guanya%2520Shi%2520and%2520Trevor%2520Darrell%2520and%2520Koushil%2520Screenath%2520and%2520Shankar%2520Sastry%26entry.1292438233%3D%2520%2520Vision-language-action%2520%2528VLA%2529%2520models%2520have%2520demonstrated%2520strong%2520semantic%250Aunderstanding%2520and%2520zero-shot%2520generalization%252C%2520yet%2520most%2520existing%2520systems%2520assume%2520an%250Aaccurate%2520low-level%2520controller%2520with%2520hand-crafted%2520action%2520%2522vocabulary%2522%2520such%2520as%250Aend-effector%2520pose%2520or%2520root%2520velocity.%2520This%2520assumption%2520confines%2520prior%2520work%2520to%250Aquasi-static%2520tasks%2520and%2520precludes%2520the%2520agile%252C%2520whole-body%2520behaviors%2520required%2520by%250Ahumanoid%2520whole-body%2520control%2520%2528WBC%2529%2520tasks.%2520To%2520capture%2520this%2520gap%2520in%2520the%2520literature%252C%250Awe%2520start%2520by%2520introducing%2520the%2520first%2520sim-to-real-ready%252C%2520vision-language%252C%250Aclosed-loop%2520benchmark%2520for%2520humanoid%2520WBC%252C%2520comprising%2520over%2520150%2520tasks%2520from%252010%250Acategories.%2520We%2520then%2520propose%2520LeVERB%253A%2520Latent%2520Vision-Language-Encoded%2520Robot%250ABehavior%252C%2520a%2520hierarchical%2520latent%2520instruction-following%2520framework%2520for%2520humanoid%250Avision-language%2520WBC%252C%2520the%2520first%2520of%2520its%2520kind.%2520At%2520the%2520top%2520level%252C%2520a%2520vision-language%250Apolicy%2520learns%2520a%2520latent%2520action%2520vocabulary%2520from%2520synthetically%2520rendered%2520kinematic%250Ademonstrations%253B%2520at%2520the%2520low%2520level%252C%2520a%2520reinforcement-learned%2520WBC%2520policy%2520consumes%250Athese%2520latent%2520verbs%2520to%2520generate%2520dynamics-level%2520commands.%2520In%2520our%2520benchmark%252C%250ALeVERB%2520can%2520zero-shot%2520attain%2520a%252080%2525%2520success%2520rate%2520on%2520simple%2520visual%2520navigation%250Atasks%252C%2520and%252058.5%2525%2520success%2520rate%2520overall%252C%2520outperforming%2520naive%2520hierarchical%250Awhole-body%2520VLA%2520implementation%2520by%25207.8%2520times.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeVERB%3A%20Humanoid%20Whole-Body%20Control%20with%20Latent%20Vision-Language%0A%20%20Instruction&entry.906535625=Haoru%20Xue%20and%20Xiaoyu%20Huang%20and%20Dantong%20Niu%20and%20Qiayuan%20Liao%20and%20Thomas%20Kragerud%20and%20Jan%20Tommy%20Gravdahl%20and%20Xue%20Bin%20Peng%20and%20Guanya%20Shi%20and%20Trevor%20Darrell%20and%20Koushil%20Screenath%20and%20Shankar%20Sastry&entry.1292438233=%20%20Vision-language-action%20%28VLA%29%20models%20have%20demonstrated%20strong%20semantic%0Aunderstanding%20and%20zero-shot%20generalization%2C%20yet%20most%20existing%20systems%20assume%20an%0Aaccurate%20low-level%20controller%20with%20hand-crafted%20action%20%22vocabulary%22%20such%20as%0Aend-effector%20pose%20or%20root%20velocity.%20This%20assumption%20confines%20prior%20work%20to%0Aquasi-static%20tasks%20and%20precludes%20the%20agile%2C%20whole-body%20behaviors%20required%20by%0Ahumanoid%20whole-body%20control%20%28WBC%29%20tasks.%20To%20capture%20this%20gap%20in%20the%20literature%2C%0Awe%20start%20by%20introducing%20the%20first%20sim-to-real-ready%2C%20vision-language%2C%0Aclosed-loop%20benchmark%20for%20humanoid%20WBC%2C%20comprising%20over%20150%20tasks%20from%2010%0Acategories.%20We%20then%20propose%20LeVERB%3A%20Latent%20Vision-Language-Encoded%20Robot%0ABehavior%2C%20a%20hierarchical%20latent%20instruction-following%20framework%20for%20humanoid%0Avision-language%20WBC%2C%20the%20first%20of%20its%20kind.%20At%20the%20top%20level%2C%20a%20vision-language%0Apolicy%20learns%20a%20latent%20action%20vocabulary%20from%20synthetically%20rendered%20kinematic%0Ademonstrations%3B%20at%20the%20low%20level%2C%20a%20reinforcement-learned%20WBC%20policy%20consumes%0Athese%20latent%20verbs%20to%20generate%20dynamics-level%20commands.%20In%20our%20benchmark%2C%0ALeVERB%20can%20zero-shot%20attain%20a%2080%25%20success%20rate%20on%20simple%20visual%20navigation%0Atasks%2C%20and%2058.5%25%20success%20rate%20overall%2C%20outperforming%20naive%20hierarchical%0Awhole-body%20VLA%20implementation%20by%207.8%20times.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13751v1&entry.124074799=Read"},
{"title": "GeoSDF: Plane Geometry Diagram Synthesis via Signed Distance Field", "author": "Chengrui Zhang and Maizhen Ning and Zihao Zhou and Jie Sun and Kaizhu Huang and Qiufeng Wang", "abstract": "  Plane Geometry Diagram Synthesis has been a crucial task in computer\ngraphics, with applications ranging from educational tools to AI-driven\nmathematical reasoning. Traditionally, we rely on computer tools (e.g.,\nMatplotlib and GeoGebra) to manually generate precise diagrams, but it usually\nrequires huge, complicated calculations cost. Recently, researchers start to\nwork on learning-based methods (e.g., Stable Diffusion and GPT4) to\nautomatically generate diagrams, saving operational cost but usually suffering\nfrom limited realism and insufficient accuracy. In this paper, we propose a\nnovel framework GeoSDF to automatically generate diagrams efficiently and\naccurately with Signed Distance Field (SDF). Specifically, we first represent\ngeometric elements in the SDF, then construct a series of constraint functions\nto represent geometric relationships, next we optimize such constraint\nfunctions to get an optimized field of both elements and constraints, finally\nby rendering the optimized field, we can obtain the synthesized diagram. In our\nGeoSDF, we define a symbolic language to easily represent geometric elements\nand those constraints, and our synthesized geometry diagrams can be\nself-verified in the SDF, ensuring both mathematical accuracy and visual\nplausibility. In experiments, our GeoSDF synthesized both normal high-school\nlevel and IMO-level geometry diagrams. Through both qualitative and\nquantitative analysis, we can see that synthesized diagrams are realistic and\naccurate, and our synthesizing process is simple and efficient. Furthermore, we\nobtain a very high accuracy of solving geometry problems (over 95\\% while the\ncurrent SOTA accuracy is around 75%) by leveraging our self-verification\nproperty. All of these demonstrate the advantage of GeoSDF, paving the way for\nmore sophisticated, accurate, and flexible generation of geometric diagrams for\na wide array of applications.\n", "link": "http://arxiv.org/abs/2506.13492v1", "date": "2025-06-16", "relevancy": 2.3409, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4756}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4661}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoSDF%3A%20Plane%20Geometry%20Diagram%20Synthesis%20via%20Signed%20Distance%20Field&body=Title%3A%20GeoSDF%3A%20Plane%20Geometry%20Diagram%20Synthesis%20via%20Signed%20Distance%20Field%0AAuthor%3A%20Chengrui%20Zhang%20and%20Maizhen%20Ning%20and%20Zihao%20Zhou%20and%20Jie%20Sun%20and%20Kaizhu%20Huang%20and%20Qiufeng%20Wang%0AAbstract%3A%20%20%20Plane%20Geometry%20Diagram%20Synthesis%20has%20been%20a%20crucial%20task%20in%20computer%0Agraphics%2C%20with%20applications%20ranging%20from%20educational%20tools%20to%20AI-driven%0Amathematical%20reasoning.%20Traditionally%2C%20we%20rely%20on%20computer%20tools%20%28e.g.%2C%0AMatplotlib%20and%20GeoGebra%29%20to%20manually%20generate%20precise%20diagrams%2C%20but%20it%20usually%0Arequires%20huge%2C%20complicated%20calculations%20cost.%20Recently%2C%20researchers%20start%20to%0Awork%20on%20learning-based%20methods%20%28e.g.%2C%20Stable%20Diffusion%20and%20GPT4%29%20to%0Aautomatically%20generate%20diagrams%2C%20saving%20operational%20cost%20but%20usually%20suffering%0Afrom%20limited%20realism%20and%20insufficient%20accuracy.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20framework%20GeoSDF%20to%20automatically%20generate%20diagrams%20efficiently%20and%0Aaccurately%20with%20Signed%20Distance%20Field%20%28SDF%29.%20Specifically%2C%20we%20first%20represent%0Ageometric%20elements%20in%20the%20SDF%2C%20then%20construct%20a%20series%20of%20constraint%20functions%0Ato%20represent%20geometric%20relationships%2C%20next%20we%20optimize%20such%20constraint%0Afunctions%20to%20get%20an%20optimized%20field%20of%20both%20elements%20and%20constraints%2C%20finally%0Aby%20rendering%20the%20optimized%20field%2C%20we%20can%20obtain%20the%20synthesized%20diagram.%20In%20our%0AGeoSDF%2C%20we%20define%20a%20symbolic%20language%20to%20easily%20represent%20geometric%20elements%0Aand%20those%20constraints%2C%20and%20our%20synthesized%20geometry%20diagrams%20can%20be%0Aself-verified%20in%20the%20SDF%2C%20ensuring%20both%20mathematical%20accuracy%20and%20visual%0Aplausibility.%20In%20experiments%2C%20our%20GeoSDF%20synthesized%20both%20normal%20high-school%0Alevel%20and%20IMO-level%20geometry%20diagrams.%20Through%20both%20qualitative%20and%0Aquantitative%20analysis%2C%20we%20can%20see%20that%20synthesized%20diagrams%20are%20realistic%20and%0Aaccurate%2C%20and%20our%20synthesizing%20process%20is%20simple%20and%20efficient.%20Furthermore%2C%20we%0Aobtain%20a%20very%20high%20accuracy%20of%20solving%20geometry%20problems%20%28over%2095%5C%25%20while%20the%0Acurrent%20SOTA%20accuracy%20is%20around%2075%25%29%20by%20leveraging%20our%20self-verification%0Aproperty.%20All%20of%20these%20demonstrate%20the%20advantage%20of%20GeoSDF%2C%20paving%20the%20way%20for%0Amore%20sophisticated%2C%20accurate%2C%20and%20flexible%20generation%20of%20geometric%20diagrams%20for%0Aa%20wide%20array%20of%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoSDF%253A%2520Plane%2520Geometry%2520Diagram%2520Synthesis%2520via%2520Signed%2520Distance%2520Field%26entry.906535625%3DChengrui%2520Zhang%2520and%2520Maizhen%2520Ning%2520and%2520Zihao%2520Zhou%2520and%2520Jie%2520Sun%2520and%2520Kaizhu%2520Huang%2520and%2520Qiufeng%2520Wang%26entry.1292438233%3D%2520%2520Plane%2520Geometry%2520Diagram%2520Synthesis%2520has%2520been%2520a%2520crucial%2520task%2520in%2520computer%250Agraphics%252C%2520with%2520applications%2520ranging%2520from%2520educational%2520tools%2520to%2520AI-driven%250Amathematical%2520reasoning.%2520Traditionally%252C%2520we%2520rely%2520on%2520computer%2520tools%2520%2528e.g.%252C%250AMatplotlib%2520and%2520GeoGebra%2529%2520to%2520manually%2520generate%2520precise%2520diagrams%252C%2520but%2520it%2520usually%250Arequires%2520huge%252C%2520complicated%2520calculations%2520cost.%2520Recently%252C%2520researchers%2520start%2520to%250Awork%2520on%2520learning-based%2520methods%2520%2528e.g.%252C%2520Stable%2520Diffusion%2520and%2520GPT4%2529%2520to%250Aautomatically%2520generate%2520diagrams%252C%2520saving%2520operational%2520cost%2520but%2520usually%2520suffering%250Afrom%2520limited%2520realism%2520and%2520insufficient%2520accuracy.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520framework%2520GeoSDF%2520to%2520automatically%2520generate%2520diagrams%2520efficiently%2520and%250Aaccurately%2520with%2520Signed%2520Distance%2520Field%2520%2528SDF%2529.%2520Specifically%252C%2520we%2520first%2520represent%250Ageometric%2520elements%2520in%2520the%2520SDF%252C%2520then%2520construct%2520a%2520series%2520of%2520constraint%2520functions%250Ato%2520represent%2520geometric%2520relationships%252C%2520next%2520we%2520optimize%2520such%2520constraint%250Afunctions%2520to%2520get%2520an%2520optimized%2520field%2520of%2520both%2520elements%2520and%2520constraints%252C%2520finally%250Aby%2520rendering%2520the%2520optimized%2520field%252C%2520we%2520can%2520obtain%2520the%2520synthesized%2520diagram.%2520In%2520our%250AGeoSDF%252C%2520we%2520define%2520a%2520symbolic%2520language%2520to%2520easily%2520represent%2520geometric%2520elements%250Aand%2520those%2520constraints%252C%2520and%2520our%2520synthesized%2520geometry%2520diagrams%2520can%2520be%250Aself-verified%2520in%2520the%2520SDF%252C%2520ensuring%2520both%2520mathematical%2520accuracy%2520and%2520visual%250Aplausibility.%2520In%2520experiments%252C%2520our%2520GeoSDF%2520synthesized%2520both%2520normal%2520high-school%250Alevel%2520and%2520IMO-level%2520geometry%2520diagrams.%2520Through%2520both%2520qualitative%2520and%250Aquantitative%2520analysis%252C%2520we%2520can%2520see%2520that%2520synthesized%2520diagrams%2520are%2520realistic%2520and%250Aaccurate%252C%2520and%2520our%2520synthesizing%2520process%2520is%2520simple%2520and%2520efficient.%2520Furthermore%252C%2520we%250Aobtain%2520a%2520very%2520high%2520accuracy%2520of%2520solving%2520geometry%2520problems%2520%2528over%252095%255C%2525%2520while%2520the%250Acurrent%2520SOTA%2520accuracy%2520is%2520around%252075%2525%2529%2520by%2520leveraging%2520our%2520self-verification%250Aproperty.%2520All%2520of%2520these%2520demonstrate%2520the%2520advantage%2520of%2520GeoSDF%252C%2520paving%2520the%2520way%2520for%250Amore%2520sophisticated%252C%2520accurate%252C%2520and%2520flexible%2520generation%2520of%2520geometric%2520diagrams%2520for%250Aa%2520wide%2520array%2520of%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoSDF%3A%20Plane%20Geometry%20Diagram%20Synthesis%20via%20Signed%20Distance%20Field&entry.906535625=Chengrui%20Zhang%20and%20Maizhen%20Ning%20and%20Zihao%20Zhou%20and%20Jie%20Sun%20and%20Kaizhu%20Huang%20and%20Qiufeng%20Wang&entry.1292438233=%20%20Plane%20Geometry%20Diagram%20Synthesis%20has%20been%20a%20crucial%20task%20in%20computer%0Agraphics%2C%20with%20applications%20ranging%20from%20educational%20tools%20to%20AI-driven%0Amathematical%20reasoning.%20Traditionally%2C%20we%20rely%20on%20computer%20tools%20%28e.g.%2C%0AMatplotlib%20and%20GeoGebra%29%20to%20manually%20generate%20precise%20diagrams%2C%20but%20it%20usually%0Arequires%20huge%2C%20complicated%20calculations%20cost.%20Recently%2C%20researchers%20start%20to%0Awork%20on%20learning-based%20methods%20%28e.g.%2C%20Stable%20Diffusion%20and%20GPT4%29%20to%0Aautomatically%20generate%20diagrams%2C%20saving%20operational%20cost%20but%20usually%20suffering%0Afrom%20limited%20realism%20and%20insufficient%20accuracy.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20framework%20GeoSDF%20to%20automatically%20generate%20diagrams%20efficiently%20and%0Aaccurately%20with%20Signed%20Distance%20Field%20%28SDF%29.%20Specifically%2C%20we%20first%20represent%0Ageometric%20elements%20in%20the%20SDF%2C%20then%20construct%20a%20series%20of%20constraint%20functions%0Ato%20represent%20geometric%20relationships%2C%20next%20we%20optimize%20such%20constraint%0Afunctions%20to%20get%20an%20optimized%20field%20of%20both%20elements%20and%20constraints%2C%20finally%0Aby%20rendering%20the%20optimized%20field%2C%20we%20can%20obtain%20the%20synthesized%20diagram.%20In%20our%0AGeoSDF%2C%20we%20define%20a%20symbolic%20language%20to%20easily%20represent%20geometric%20elements%0Aand%20those%20constraints%2C%20and%20our%20synthesized%20geometry%20diagrams%20can%20be%0Aself-verified%20in%20the%20SDF%2C%20ensuring%20both%20mathematical%20accuracy%20and%20visual%0Aplausibility.%20In%20experiments%2C%20our%20GeoSDF%20synthesized%20both%20normal%20high-school%0Alevel%20and%20IMO-level%20geometry%20diagrams.%20Through%20both%20qualitative%20and%0Aquantitative%20analysis%2C%20we%20can%20see%20that%20synthesized%20diagrams%20are%20realistic%20and%0Aaccurate%2C%20and%20our%20synthesizing%20process%20is%20simple%20and%20efficient.%20Furthermore%2C%20we%0Aobtain%20a%20very%20high%20accuracy%20of%20solving%20geometry%20problems%20%28over%2095%5C%25%20while%20the%0Acurrent%20SOTA%20accuracy%20is%20around%2075%25%29%20by%20leveraging%20our%20self-verification%0Aproperty.%20All%20of%20these%20demonstrate%20the%20advantage%20of%20GeoSDF%2C%20paving%20the%20way%20for%0Amore%20sophisticated%2C%20accurate%2C%20and%20flexible%20generation%20of%20geometric%20diagrams%20for%0Aa%20wide%20array%20of%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13492v1&entry.124074799=Read"},
{"title": "Turning Down the Heat: A Critical Analysis of Min-p Sampling in Language\n  Models", "author": "Rylan Schaeffer and Joshua Kazdan and Yegor Denisov-Blanch", "abstract": "  Sampling from language models impacts the quality and diversity of outputs,\naffecting both research and real-world applications. Recently, Nguyen et al.\n2024's \"Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM\nOutputs\" introduced a new sampler called min-p, claiming it achieves superior\nquality and diversity over established samplers such as basic, top-k, and top-p\nsampling. The significance of these claims was underscored by the paper's\nrecognition as the 18th highest-scoring submission to ICLR 2025 and selection\nfor an Oral presentation. This paper conducts a comprehensive re-examination of\nthe evidence supporting min-p and reaches different conclusions from the\noriginal paper's four lines of evidence. First, the original paper's human\nevaluations omitted data, conducted statistical tests incorrectly, and\ndescribed qualitative feedback inaccurately; our reanalysis demonstrates min-p\ndid not outperform baselines in quality, diversity, or a trade-off between\nquality and diversity; in response to our findings, the authors of the original\npaper conducted a new human evaluation using a different implementation, task,\nand rubric that nevertheless provides further evidence min-p does not improve\nover baselines. Second, comprehensively sweeping the original paper's NLP\nbenchmarks reveals min-p does not surpass baselines when controlling for the\nnumber of hyperparameters. Third, the original paper's LLM-as-a-Judge\nevaluations lack methodological clarity and appear inconsistently reported.\nFourth, community adoption claims (49k GitHub repositories, 1.1M GitHub stars)\nwere found to be unsubstantiated, leading to their removal; the revised\nadoption claim remains misleading. We conclude that evidence presented in the\noriginal paper fails to support claims that min-p improves quality, diversity,\nor a trade-off between quality and diversity.\n", "link": "http://arxiv.org/abs/2506.13681v1", "date": "2025-06-16", "relevancy": 2.3407, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4925}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Turning%20Down%20the%20Heat%3A%20A%20Critical%20Analysis%20of%20Min-p%20Sampling%20in%20Language%0A%20%20Models&body=Title%3A%20Turning%20Down%20the%20Heat%3A%20A%20Critical%20Analysis%20of%20Min-p%20Sampling%20in%20Language%0A%20%20Models%0AAuthor%3A%20Rylan%20Schaeffer%20and%20Joshua%20Kazdan%20and%20Yegor%20Denisov-Blanch%0AAbstract%3A%20%20%20Sampling%20from%20language%20models%20impacts%20the%20quality%20and%20diversity%20of%20outputs%2C%0Aaffecting%20both%20research%20and%20real-world%20applications.%20Recently%2C%20Nguyen%20et%20al.%0A2024%27s%20%22Turning%20Up%20the%20Heat%3A%20Min-p%20Sampling%20for%20Creative%20and%20Coherent%20LLM%0AOutputs%22%20introduced%20a%20new%20sampler%20called%20min-p%2C%20claiming%20it%20achieves%20superior%0Aquality%20and%20diversity%20over%20established%20samplers%20such%20as%20basic%2C%20top-k%2C%20and%20top-p%0Asampling.%20The%20significance%20of%20these%20claims%20was%20underscored%20by%20the%20paper%27s%0Arecognition%20as%20the%2018th%20highest-scoring%20submission%20to%20ICLR%202025%20and%20selection%0Afor%20an%20Oral%20presentation.%20This%20paper%20conducts%20a%20comprehensive%20re-examination%20of%0Athe%20evidence%20supporting%20min-p%20and%20reaches%20different%20conclusions%20from%20the%0Aoriginal%20paper%27s%20four%20lines%20of%20evidence.%20First%2C%20the%20original%20paper%27s%20human%0Aevaluations%20omitted%20data%2C%20conducted%20statistical%20tests%20incorrectly%2C%20and%0Adescribed%20qualitative%20feedback%20inaccurately%3B%20our%20reanalysis%20demonstrates%20min-p%0Adid%20not%20outperform%20baselines%20in%20quality%2C%20diversity%2C%20or%20a%20trade-off%20between%0Aquality%20and%20diversity%3B%20in%20response%20to%20our%20findings%2C%20the%20authors%20of%20the%20original%0Apaper%20conducted%20a%20new%20human%20evaluation%20using%20a%20different%20implementation%2C%20task%2C%0Aand%20rubric%20that%20nevertheless%20provides%20further%20evidence%20min-p%20does%20not%20improve%0Aover%20baselines.%20Second%2C%20comprehensively%20sweeping%20the%20original%20paper%27s%20NLP%0Abenchmarks%20reveals%20min-p%20does%20not%20surpass%20baselines%20when%20controlling%20for%20the%0Anumber%20of%20hyperparameters.%20Third%2C%20the%20original%20paper%27s%20LLM-as-a-Judge%0Aevaluations%20lack%20methodological%20clarity%20and%20appear%20inconsistently%20reported.%0AFourth%2C%20community%20adoption%20claims%20%2849k%20GitHub%20repositories%2C%201.1M%20GitHub%20stars%29%0Awere%20found%20to%20be%20unsubstantiated%2C%20leading%20to%20their%20removal%3B%20the%20revised%0Aadoption%20claim%20remains%20misleading.%20We%20conclude%20that%20evidence%20presented%20in%20the%0Aoriginal%20paper%20fails%20to%20support%20claims%20that%20min-p%20improves%20quality%2C%20diversity%2C%0Aor%20a%20trade-off%20between%20quality%20and%20diversity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTurning%2520Down%2520the%2520Heat%253A%2520A%2520Critical%2520Analysis%2520of%2520Min-p%2520Sampling%2520in%2520Language%250A%2520%2520Models%26entry.906535625%3DRylan%2520Schaeffer%2520and%2520Joshua%2520Kazdan%2520and%2520Yegor%2520Denisov-Blanch%26entry.1292438233%3D%2520%2520Sampling%2520from%2520language%2520models%2520impacts%2520the%2520quality%2520and%2520diversity%2520of%2520outputs%252C%250Aaffecting%2520both%2520research%2520and%2520real-world%2520applications.%2520Recently%252C%2520Nguyen%2520et%2520al.%250A2024%2527s%2520%2522Turning%2520Up%2520the%2520Heat%253A%2520Min-p%2520Sampling%2520for%2520Creative%2520and%2520Coherent%2520LLM%250AOutputs%2522%2520introduced%2520a%2520new%2520sampler%2520called%2520min-p%252C%2520claiming%2520it%2520achieves%2520superior%250Aquality%2520and%2520diversity%2520over%2520established%2520samplers%2520such%2520as%2520basic%252C%2520top-k%252C%2520and%2520top-p%250Asampling.%2520The%2520significance%2520of%2520these%2520claims%2520was%2520underscored%2520by%2520the%2520paper%2527s%250Arecognition%2520as%2520the%252018th%2520highest-scoring%2520submission%2520to%2520ICLR%25202025%2520and%2520selection%250Afor%2520an%2520Oral%2520presentation.%2520This%2520paper%2520conducts%2520a%2520comprehensive%2520re-examination%2520of%250Athe%2520evidence%2520supporting%2520min-p%2520and%2520reaches%2520different%2520conclusions%2520from%2520the%250Aoriginal%2520paper%2527s%2520four%2520lines%2520of%2520evidence.%2520First%252C%2520the%2520original%2520paper%2527s%2520human%250Aevaluations%2520omitted%2520data%252C%2520conducted%2520statistical%2520tests%2520incorrectly%252C%2520and%250Adescribed%2520qualitative%2520feedback%2520inaccurately%253B%2520our%2520reanalysis%2520demonstrates%2520min-p%250Adid%2520not%2520outperform%2520baselines%2520in%2520quality%252C%2520diversity%252C%2520or%2520a%2520trade-off%2520between%250Aquality%2520and%2520diversity%253B%2520in%2520response%2520to%2520our%2520findings%252C%2520the%2520authors%2520of%2520the%2520original%250Apaper%2520conducted%2520a%2520new%2520human%2520evaluation%2520using%2520a%2520different%2520implementation%252C%2520task%252C%250Aand%2520rubric%2520that%2520nevertheless%2520provides%2520further%2520evidence%2520min-p%2520does%2520not%2520improve%250Aover%2520baselines.%2520Second%252C%2520comprehensively%2520sweeping%2520the%2520original%2520paper%2527s%2520NLP%250Abenchmarks%2520reveals%2520min-p%2520does%2520not%2520surpass%2520baselines%2520when%2520controlling%2520for%2520the%250Anumber%2520of%2520hyperparameters.%2520Third%252C%2520the%2520original%2520paper%2527s%2520LLM-as-a-Judge%250Aevaluations%2520lack%2520methodological%2520clarity%2520and%2520appear%2520inconsistently%2520reported.%250AFourth%252C%2520community%2520adoption%2520claims%2520%252849k%2520GitHub%2520repositories%252C%25201.1M%2520GitHub%2520stars%2529%250Awere%2520found%2520to%2520be%2520unsubstantiated%252C%2520leading%2520to%2520their%2520removal%253B%2520the%2520revised%250Aadoption%2520claim%2520remains%2520misleading.%2520We%2520conclude%2520that%2520evidence%2520presented%2520in%2520the%250Aoriginal%2520paper%2520fails%2520to%2520support%2520claims%2520that%2520min-p%2520improves%2520quality%252C%2520diversity%252C%250Aor%2520a%2520trade-off%2520between%2520quality%2520and%2520diversity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Turning%20Down%20the%20Heat%3A%20A%20Critical%20Analysis%20of%20Min-p%20Sampling%20in%20Language%0A%20%20Models&entry.906535625=Rylan%20Schaeffer%20and%20Joshua%20Kazdan%20and%20Yegor%20Denisov-Blanch&entry.1292438233=%20%20Sampling%20from%20language%20models%20impacts%20the%20quality%20and%20diversity%20of%20outputs%2C%0Aaffecting%20both%20research%20and%20real-world%20applications.%20Recently%2C%20Nguyen%20et%20al.%0A2024%27s%20%22Turning%20Up%20the%20Heat%3A%20Min-p%20Sampling%20for%20Creative%20and%20Coherent%20LLM%0AOutputs%22%20introduced%20a%20new%20sampler%20called%20min-p%2C%20claiming%20it%20achieves%20superior%0Aquality%20and%20diversity%20over%20established%20samplers%20such%20as%20basic%2C%20top-k%2C%20and%20top-p%0Asampling.%20The%20significance%20of%20these%20claims%20was%20underscored%20by%20the%20paper%27s%0Arecognition%20as%20the%2018th%20highest-scoring%20submission%20to%20ICLR%202025%20and%20selection%0Afor%20an%20Oral%20presentation.%20This%20paper%20conducts%20a%20comprehensive%20re-examination%20of%0Athe%20evidence%20supporting%20min-p%20and%20reaches%20different%20conclusions%20from%20the%0Aoriginal%20paper%27s%20four%20lines%20of%20evidence.%20First%2C%20the%20original%20paper%27s%20human%0Aevaluations%20omitted%20data%2C%20conducted%20statistical%20tests%20incorrectly%2C%20and%0Adescribed%20qualitative%20feedback%20inaccurately%3B%20our%20reanalysis%20demonstrates%20min-p%0Adid%20not%20outperform%20baselines%20in%20quality%2C%20diversity%2C%20or%20a%20trade-off%20between%0Aquality%20and%20diversity%3B%20in%20response%20to%20our%20findings%2C%20the%20authors%20of%20the%20original%0Apaper%20conducted%20a%20new%20human%20evaluation%20using%20a%20different%20implementation%2C%20task%2C%0Aand%20rubric%20that%20nevertheless%20provides%20further%20evidence%20min-p%20does%20not%20improve%0Aover%20baselines.%20Second%2C%20comprehensively%20sweeping%20the%20original%20paper%27s%20NLP%0Abenchmarks%20reveals%20min-p%20does%20not%20surpass%20baselines%20when%20controlling%20for%20the%0Anumber%20of%20hyperparameters.%20Third%2C%20the%20original%20paper%27s%20LLM-as-a-Judge%0Aevaluations%20lack%20methodological%20clarity%20and%20appear%20inconsistently%20reported.%0AFourth%2C%20community%20adoption%20claims%20%2849k%20GitHub%20repositories%2C%201.1M%20GitHub%20stars%29%0Awere%20found%20to%20be%20unsubstantiated%2C%20leading%20to%20their%20removal%3B%20the%20revised%0Aadoption%20claim%20remains%20misleading.%20We%20conclude%20that%20evidence%20presented%20in%20the%0Aoriginal%20paper%20fails%20to%20support%20claims%20that%20min-p%20improves%20quality%2C%20diversity%2C%0Aor%20a%20trade-off%20between%20quality%20and%20diversity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13681v1&entry.124074799=Read"},
{"title": "A Gravity-informed Spatiotemporal Transformer for Human Activity\n  Intensity Prediction", "author": "Yi Wang and Zhenghong Wang and Fan Zhang and Chengling Tang and Chaogui Kang and Di Zhu and Zhongfu Ma and Sijie Ruan and Weiyu Zhang and Yu Zheng and Philip S. Yu and Yu Liu", "abstract": "  Human activity intensity prediction is a crucial to many location-based\nservices. Although tremendous progress has been made to model dynamic\nspatiotemporal patterns of human activity, most existing methods, including\nspatiotemporal graph neural networks (ST-GNNs), overlook physical constraints\nof spatial interactions and the over-smoothing phenomenon in spatial\ncorrelation modeling. To address these limitations, this work proposes a\nphysics-informed deep learning framework, namely Gravity-informed\nSpatiotemporal Transformer (Gravityformer) by refining transformer attention to\nintegrate the universal law of gravitation and explicitly incorporating\nconstraints from spatial interactions. Specifically, it (1) estimates two\nspatially explicit mass parameters based on inflow and outflow, (2) models the\nlikelihood of cross-unit interaction using closed-form solutions of spatial\ninteractions to constrain spatial modeling randomness, and (3) utilizes the\nlearned spatial interaction to guide and mitigate the over-smoothing phenomenon\nin transformer attention matrices. The underlying law of human activity can be\nexplicitly modeled by the proposed adaptive gravity model. Moreover, a parallel\nspatiotemporal graph convolution transformer structure is proposed for\nachieving a balance between coupled spatial and temporal learning. Systematic\nexperiments on six real-world large-scale activity datasets demonstrate the\nquantitative and qualitative superiority of our approach over state-of-the-art\nbenchmarks. Additionally, the learned gravity attention matrix can be\ndisentangled and interpreted based on geographical laws. This work provides a\nnovel insight into integrating physical laws with deep learning for\nspatiotemporal predictive learning.\n", "link": "http://arxiv.org/abs/2506.13678v1", "date": "2025-06-16", "relevancy": 2.3366, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6087}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5777}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Gravity-informed%20Spatiotemporal%20Transformer%20for%20Human%20Activity%0A%20%20Intensity%20Prediction&body=Title%3A%20A%20Gravity-informed%20Spatiotemporal%20Transformer%20for%20Human%20Activity%0A%20%20Intensity%20Prediction%0AAuthor%3A%20Yi%20Wang%20and%20Zhenghong%20Wang%20and%20Fan%20Zhang%20and%20Chengling%20Tang%20and%20Chaogui%20Kang%20and%20Di%20Zhu%20and%20Zhongfu%20Ma%20and%20Sijie%20Ruan%20and%20Weiyu%20Zhang%20and%20Yu%20Zheng%20and%20Philip%20S.%20Yu%20and%20Yu%20Liu%0AAbstract%3A%20%20%20Human%20activity%20intensity%20prediction%20is%20a%20crucial%20to%20many%20location-based%0Aservices.%20Although%20tremendous%20progress%20has%20been%20made%20to%20model%20dynamic%0Aspatiotemporal%20patterns%20of%20human%20activity%2C%20most%20existing%20methods%2C%20including%0Aspatiotemporal%20graph%20neural%20networks%20%28ST-GNNs%29%2C%20overlook%20physical%20constraints%0Aof%20spatial%20interactions%20and%20the%20over-smoothing%20phenomenon%20in%20spatial%0Acorrelation%20modeling.%20To%20address%20these%20limitations%2C%20this%20work%20proposes%20a%0Aphysics-informed%20deep%20learning%20framework%2C%20namely%20Gravity-informed%0ASpatiotemporal%20Transformer%20%28Gravityformer%29%20by%20refining%20transformer%20attention%20to%0Aintegrate%20the%20universal%20law%20of%20gravitation%20and%20explicitly%20incorporating%0Aconstraints%20from%20spatial%20interactions.%20Specifically%2C%20it%20%281%29%20estimates%20two%0Aspatially%20explicit%20mass%20parameters%20based%20on%20inflow%20and%20outflow%2C%20%282%29%20models%20the%0Alikelihood%20of%20cross-unit%20interaction%20using%20closed-form%20solutions%20of%20spatial%0Ainteractions%20to%20constrain%20spatial%20modeling%20randomness%2C%20and%20%283%29%20utilizes%20the%0Alearned%20spatial%20interaction%20to%20guide%20and%20mitigate%20the%20over-smoothing%20phenomenon%0Ain%20transformer%20attention%20matrices.%20The%20underlying%20law%20of%20human%20activity%20can%20be%0Aexplicitly%20modeled%20by%20the%20proposed%20adaptive%20gravity%20model.%20Moreover%2C%20a%20parallel%0Aspatiotemporal%20graph%20convolution%20transformer%20structure%20is%20proposed%20for%0Aachieving%20a%20balance%20between%20coupled%20spatial%20and%20temporal%20learning.%20Systematic%0Aexperiments%20on%20six%20real-world%20large-scale%20activity%20datasets%20demonstrate%20the%0Aquantitative%20and%20qualitative%20superiority%20of%20our%20approach%20over%20state-of-the-art%0Abenchmarks.%20Additionally%2C%20the%20learned%20gravity%20attention%20matrix%20can%20be%0Adisentangled%20and%20interpreted%20based%20on%20geographical%20laws.%20This%20work%20provides%20a%0Anovel%20insight%20into%20integrating%20physical%20laws%20with%20deep%20learning%20for%0Aspatiotemporal%20predictive%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Gravity-informed%2520Spatiotemporal%2520Transformer%2520for%2520Human%2520Activity%250A%2520%2520Intensity%2520Prediction%26entry.906535625%3DYi%2520Wang%2520and%2520Zhenghong%2520Wang%2520and%2520Fan%2520Zhang%2520and%2520Chengling%2520Tang%2520and%2520Chaogui%2520Kang%2520and%2520Di%2520Zhu%2520and%2520Zhongfu%2520Ma%2520and%2520Sijie%2520Ruan%2520and%2520Weiyu%2520Zhang%2520and%2520Yu%2520Zheng%2520and%2520Philip%2520S.%2520Yu%2520and%2520Yu%2520Liu%26entry.1292438233%3D%2520%2520Human%2520activity%2520intensity%2520prediction%2520is%2520a%2520crucial%2520to%2520many%2520location-based%250Aservices.%2520Although%2520tremendous%2520progress%2520has%2520been%2520made%2520to%2520model%2520dynamic%250Aspatiotemporal%2520patterns%2520of%2520human%2520activity%252C%2520most%2520existing%2520methods%252C%2520including%250Aspatiotemporal%2520graph%2520neural%2520networks%2520%2528ST-GNNs%2529%252C%2520overlook%2520physical%2520constraints%250Aof%2520spatial%2520interactions%2520and%2520the%2520over-smoothing%2520phenomenon%2520in%2520spatial%250Acorrelation%2520modeling.%2520To%2520address%2520these%2520limitations%252C%2520this%2520work%2520proposes%2520a%250Aphysics-informed%2520deep%2520learning%2520framework%252C%2520namely%2520Gravity-informed%250ASpatiotemporal%2520Transformer%2520%2528Gravityformer%2529%2520by%2520refining%2520transformer%2520attention%2520to%250Aintegrate%2520the%2520universal%2520law%2520of%2520gravitation%2520and%2520explicitly%2520incorporating%250Aconstraints%2520from%2520spatial%2520interactions.%2520Specifically%252C%2520it%2520%25281%2529%2520estimates%2520two%250Aspatially%2520explicit%2520mass%2520parameters%2520based%2520on%2520inflow%2520and%2520outflow%252C%2520%25282%2529%2520models%2520the%250Alikelihood%2520of%2520cross-unit%2520interaction%2520using%2520closed-form%2520solutions%2520of%2520spatial%250Ainteractions%2520to%2520constrain%2520spatial%2520modeling%2520randomness%252C%2520and%2520%25283%2529%2520utilizes%2520the%250Alearned%2520spatial%2520interaction%2520to%2520guide%2520and%2520mitigate%2520the%2520over-smoothing%2520phenomenon%250Ain%2520transformer%2520attention%2520matrices.%2520The%2520underlying%2520law%2520of%2520human%2520activity%2520can%2520be%250Aexplicitly%2520modeled%2520by%2520the%2520proposed%2520adaptive%2520gravity%2520model.%2520Moreover%252C%2520a%2520parallel%250Aspatiotemporal%2520graph%2520convolution%2520transformer%2520structure%2520is%2520proposed%2520for%250Aachieving%2520a%2520balance%2520between%2520coupled%2520spatial%2520and%2520temporal%2520learning.%2520Systematic%250Aexperiments%2520on%2520six%2520real-world%2520large-scale%2520activity%2520datasets%2520demonstrate%2520the%250Aquantitative%2520and%2520qualitative%2520superiority%2520of%2520our%2520approach%2520over%2520state-of-the-art%250Abenchmarks.%2520Additionally%252C%2520the%2520learned%2520gravity%2520attention%2520matrix%2520can%2520be%250Adisentangled%2520and%2520interpreted%2520based%2520on%2520geographical%2520laws.%2520This%2520work%2520provides%2520a%250Anovel%2520insight%2520into%2520integrating%2520physical%2520laws%2520with%2520deep%2520learning%2520for%250Aspatiotemporal%2520predictive%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Gravity-informed%20Spatiotemporal%20Transformer%20for%20Human%20Activity%0A%20%20Intensity%20Prediction&entry.906535625=Yi%20Wang%20and%20Zhenghong%20Wang%20and%20Fan%20Zhang%20and%20Chengling%20Tang%20and%20Chaogui%20Kang%20and%20Di%20Zhu%20and%20Zhongfu%20Ma%20and%20Sijie%20Ruan%20and%20Weiyu%20Zhang%20and%20Yu%20Zheng%20and%20Philip%20S.%20Yu%20and%20Yu%20Liu&entry.1292438233=%20%20Human%20activity%20intensity%20prediction%20is%20a%20crucial%20to%20many%20location-based%0Aservices.%20Although%20tremendous%20progress%20has%20been%20made%20to%20model%20dynamic%0Aspatiotemporal%20patterns%20of%20human%20activity%2C%20most%20existing%20methods%2C%20including%0Aspatiotemporal%20graph%20neural%20networks%20%28ST-GNNs%29%2C%20overlook%20physical%20constraints%0Aof%20spatial%20interactions%20and%20the%20over-smoothing%20phenomenon%20in%20spatial%0Acorrelation%20modeling.%20To%20address%20these%20limitations%2C%20this%20work%20proposes%20a%0Aphysics-informed%20deep%20learning%20framework%2C%20namely%20Gravity-informed%0ASpatiotemporal%20Transformer%20%28Gravityformer%29%20by%20refining%20transformer%20attention%20to%0Aintegrate%20the%20universal%20law%20of%20gravitation%20and%20explicitly%20incorporating%0Aconstraints%20from%20spatial%20interactions.%20Specifically%2C%20it%20%281%29%20estimates%20two%0Aspatially%20explicit%20mass%20parameters%20based%20on%20inflow%20and%20outflow%2C%20%282%29%20models%20the%0Alikelihood%20of%20cross-unit%20interaction%20using%20closed-form%20solutions%20of%20spatial%0Ainteractions%20to%20constrain%20spatial%20modeling%20randomness%2C%20and%20%283%29%20utilizes%20the%0Alearned%20spatial%20interaction%20to%20guide%20and%20mitigate%20the%20over-smoothing%20phenomenon%0Ain%20transformer%20attention%20matrices.%20The%20underlying%20law%20of%20human%20activity%20can%20be%0Aexplicitly%20modeled%20by%20the%20proposed%20adaptive%20gravity%20model.%20Moreover%2C%20a%20parallel%0Aspatiotemporal%20graph%20convolution%20transformer%20structure%20is%20proposed%20for%0Aachieving%20a%20balance%20between%20coupled%20spatial%20and%20temporal%20learning.%20Systematic%0Aexperiments%20on%20six%20real-world%20large-scale%20activity%20datasets%20demonstrate%20the%0Aquantitative%20and%20qualitative%20superiority%20of%20our%20approach%20over%20state-of-the-art%0Abenchmarks.%20Additionally%2C%20the%20learned%20gravity%20attention%20matrix%20can%20be%0Adisentangled%20and%20interpreted%20based%20on%20geographical%20laws.%20This%20work%20provides%20a%0Anovel%20insight%20into%20integrating%20physical%20laws%20with%20deep%20learning%20for%0Aspatiotemporal%20predictive%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13678v1&entry.124074799=Read"},
{"title": "Heart Rate Classification in ECG Signals Using Machine Learning and Deep\n  Learning", "author": "Thien Nhan Vo", "abstract": "  This study addresses the classification of heartbeats from ECG signals\nthrough two distinct approaches: traditional machine learning utilizing\nhand-crafted features and deep learning via transformed images of ECG beats.\nThe dataset underwent preprocessing steps, including downsampling, filtering,\nand normalization, to ensure consistency and relevance for subsequent analysis.\nIn the first approach, features such as heart rate variability (HRV), mean,\nvariance, and RR intervals were extracted to train various classifiers,\nincluding SVM, Random Forest, AdaBoost, LSTM, Bi-directional LSTM, and\nLightGBM. The second approach involved transforming ECG signals into images\nusing Gramian Angular Field (GAF), Markov Transition Field (MTF), and\nRecurrence Plots (RP), with these images subsequently classified using CNN\narchitectures like VGG and Inception.\n  Experimental results demonstrate that the LightGBM model achieved the highest\nperformance, with an accuracy of 99% and an F1 score of 0.94, outperforming the\nimage-based CNN approach (F1 score of 0.85). Models such as SVM and AdaBoost\nyielded significantly lower scores, indicating limited suitability for this\ntask. The findings underscore the superior ability of hand-crafted features to\ncapture temporal and morphological variations in ECG signals compared to\nimage-based representations of individual beats. Future investigations may\nbenefit from incorporating multi-lead ECG signals and temporal dependencies\nacross successive beats to enhance classification accuracy further.\n", "link": "http://arxiv.org/abs/2506.06349v2", "date": "2025-06-16", "relevancy": 2.2905, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4639}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4592}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heart%20Rate%20Classification%20in%20ECG%20Signals%20Using%20Machine%20Learning%20and%20Deep%0A%20%20Learning&body=Title%3A%20Heart%20Rate%20Classification%20in%20ECG%20Signals%20Using%20Machine%20Learning%20and%20Deep%0A%20%20Learning%0AAuthor%3A%20Thien%20Nhan%20Vo%0AAbstract%3A%20%20%20This%20study%20addresses%20the%20classification%20of%20heartbeats%20from%20ECG%20signals%0Athrough%20two%20distinct%20approaches%3A%20traditional%20machine%20learning%20utilizing%0Ahand-crafted%20features%20and%20deep%20learning%20via%20transformed%20images%20of%20ECG%20beats.%0AThe%20dataset%20underwent%20preprocessing%20steps%2C%20including%20downsampling%2C%20filtering%2C%0Aand%20normalization%2C%20to%20ensure%20consistency%20and%20relevance%20for%20subsequent%20analysis.%0AIn%20the%20first%20approach%2C%20features%20such%20as%20heart%20rate%20variability%20%28HRV%29%2C%20mean%2C%0Avariance%2C%20and%20RR%20intervals%20were%20extracted%20to%20train%20various%20classifiers%2C%0Aincluding%20SVM%2C%20Random%20Forest%2C%20AdaBoost%2C%20LSTM%2C%20Bi-directional%20LSTM%2C%20and%0ALightGBM.%20The%20second%20approach%20involved%20transforming%20ECG%20signals%20into%20images%0Ausing%20Gramian%20Angular%20Field%20%28GAF%29%2C%20Markov%20Transition%20Field%20%28MTF%29%2C%20and%0ARecurrence%20Plots%20%28RP%29%2C%20with%20these%20images%20subsequently%20classified%20using%20CNN%0Aarchitectures%20like%20VGG%20and%20Inception.%0A%20%20Experimental%20results%20demonstrate%20that%20the%20LightGBM%20model%20achieved%20the%20highest%0Aperformance%2C%20with%20an%20accuracy%20of%2099%25%20and%20an%20F1%20score%20of%200.94%2C%20outperforming%20the%0Aimage-based%20CNN%20approach%20%28F1%20score%20of%200.85%29.%20Models%20such%20as%20SVM%20and%20AdaBoost%0Ayielded%20significantly%20lower%20scores%2C%20indicating%20limited%20suitability%20for%20this%0Atask.%20The%20findings%20underscore%20the%20superior%20ability%20of%20hand-crafted%20features%20to%0Acapture%20temporal%20and%20morphological%20variations%20in%20ECG%20signals%20compared%20to%0Aimage-based%20representations%20of%20individual%20beats.%20Future%20investigations%20may%0Abenefit%20from%20incorporating%20multi-lead%20ECG%20signals%20and%20temporal%20dependencies%0Aacross%20successive%20beats%20to%20enhance%20classification%20accuracy%20further.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06349v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeart%2520Rate%2520Classification%2520in%2520ECG%2520Signals%2520Using%2520Machine%2520Learning%2520and%2520Deep%250A%2520%2520Learning%26entry.906535625%3DThien%2520Nhan%2520Vo%26entry.1292438233%3D%2520%2520This%2520study%2520addresses%2520the%2520classification%2520of%2520heartbeats%2520from%2520ECG%2520signals%250Athrough%2520two%2520distinct%2520approaches%253A%2520traditional%2520machine%2520learning%2520utilizing%250Ahand-crafted%2520features%2520and%2520deep%2520learning%2520via%2520transformed%2520images%2520of%2520ECG%2520beats.%250AThe%2520dataset%2520underwent%2520preprocessing%2520steps%252C%2520including%2520downsampling%252C%2520filtering%252C%250Aand%2520normalization%252C%2520to%2520ensure%2520consistency%2520and%2520relevance%2520for%2520subsequent%2520analysis.%250AIn%2520the%2520first%2520approach%252C%2520features%2520such%2520as%2520heart%2520rate%2520variability%2520%2528HRV%2529%252C%2520mean%252C%250Avariance%252C%2520and%2520RR%2520intervals%2520were%2520extracted%2520to%2520train%2520various%2520classifiers%252C%250Aincluding%2520SVM%252C%2520Random%2520Forest%252C%2520AdaBoost%252C%2520LSTM%252C%2520Bi-directional%2520LSTM%252C%2520and%250ALightGBM.%2520The%2520second%2520approach%2520involved%2520transforming%2520ECG%2520signals%2520into%2520images%250Ausing%2520Gramian%2520Angular%2520Field%2520%2528GAF%2529%252C%2520Markov%2520Transition%2520Field%2520%2528MTF%2529%252C%2520and%250ARecurrence%2520Plots%2520%2528RP%2529%252C%2520with%2520these%2520images%2520subsequently%2520classified%2520using%2520CNN%250Aarchitectures%2520like%2520VGG%2520and%2520Inception.%250A%2520%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520LightGBM%2520model%2520achieved%2520the%2520highest%250Aperformance%252C%2520with%2520an%2520accuracy%2520of%252099%2525%2520and%2520an%2520F1%2520score%2520of%25200.94%252C%2520outperforming%2520the%250Aimage-based%2520CNN%2520approach%2520%2528F1%2520score%2520of%25200.85%2529.%2520Models%2520such%2520as%2520SVM%2520and%2520AdaBoost%250Ayielded%2520significantly%2520lower%2520scores%252C%2520indicating%2520limited%2520suitability%2520for%2520this%250Atask.%2520The%2520findings%2520underscore%2520the%2520superior%2520ability%2520of%2520hand-crafted%2520features%2520to%250Acapture%2520temporal%2520and%2520morphological%2520variations%2520in%2520ECG%2520signals%2520compared%2520to%250Aimage-based%2520representations%2520of%2520individual%2520beats.%2520Future%2520investigations%2520may%250Abenefit%2520from%2520incorporating%2520multi-lead%2520ECG%2520signals%2520and%2520temporal%2520dependencies%250Aacross%2520successive%2520beats%2520to%2520enhance%2520classification%2520accuracy%2520further.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06349v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heart%20Rate%20Classification%20in%20ECG%20Signals%20Using%20Machine%20Learning%20and%20Deep%0A%20%20Learning&entry.906535625=Thien%20Nhan%20Vo&entry.1292438233=%20%20This%20study%20addresses%20the%20classification%20of%20heartbeats%20from%20ECG%20signals%0Athrough%20two%20distinct%20approaches%3A%20traditional%20machine%20learning%20utilizing%0Ahand-crafted%20features%20and%20deep%20learning%20via%20transformed%20images%20of%20ECG%20beats.%0AThe%20dataset%20underwent%20preprocessing%20steps%2C%20including%20downsampling%2C%20filtering%2C%0Aand%20normalization%2C%20to%20ensure%20consistency%20and%20relevance%20for%20subsequent%20analysis.%0AIn%20the%20first%20approach%2C%20features%20such%20as%20heart%20rate%20variability%20%28HRV%29%2C%20mean%2C%0Avariance%2C%20and%20RR%20intervals%20were%20extracted%20to%20train%20various%20classifiers%2C%0Aincluding%20SVM%2C%20Random%20Forest%2C%20AdaBoost%2C%20LSTM%2C%20Bi-directional%20LSTM%2C%20and%0ALightGBM.%20The%20second%20approach%20involved%20transforming%20ECG%20signals%20into%20images%0Ausing%20Gramian%20Angular%20Field%20%28GAF%29%2C%20Markov%20Transition%20Field%20%28MTF%29%2C%20and%0ARecurrence%20Plots%20%28RP%29%2C%20with%20these%20images%20subsequently%20classified%20using%20CNN%0Aarchitectures%20like%20VGG%20and%20Inception.%0A%20%20Experimental%20results%20demonstrate%20that%20the%20LightGBM%20model%20achieved%20the%20highest%0Aperformance%2C%20with%20an%20accuracy%20of%2099%25%20and%20an%20F1%20score%20of%200.94%2C%20outperforming%20the%0Aimage-based%20CNN%20approach%20%28F1%20score%20of%200.85%29.%20Models%20such%20as%20SVM%20and%20AdaBoost%0Ayielded%20significantly%20lower%20scores%2C%20indicating%20limited%20suitability%20for%20this%0Atask.%20The%20findings%20underscore%20the%20superior%20ability%20of%20hand-crafted%20features%20to%0Acapture%20temporal%20and%20morphological%20variations%20in%20ECG%20signals%20compared%20to%0Aimage-based%20representations%20of%20individual%20beats.%20Future%20investigations%20may%0Abenefit%20from%20incorporating%20multi-lead%20ECG%20signals%20and%20temporal%20dependencies%0Aacross%20successive%20beats%20to%20enhance%20classification%20accuracy%20further.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06349v2&entry.124074799=Read"},
{"title": "Observability-Aware Active Calibration of Multi-Sensor Extrinsics for\n  Ground Robots via Online Trajectory Optimization", "author": "Jiang Wang and Yaozhong Kang and Linya Fu and Kazuhiro Nakadai and He Kong", "abstract": "  Accurate calibration of sensor extrinsic parameters for ground robotic\nsystems (i.e., relative poses) is crucial for ensuring spatial alignment and\nachieving high-performance perception. However, existing calibration methods\ntypically require complex and often human-operated processes to collect data.\nMoreover, most frameworks neglect acoustic sensors, thereby limiting the\nassociated systems' auditory perception capabilities. To alleviate these\nissues, we propose an observability-aware active calibration method for ground\nrobots with multimodal sensors, including a microphone array, a LiDAR\n(exteroceptive sensors), and wheel encoders (proprioceptive sensors). Unlike\ntraditional approaches, our method enables active trajectory optimization for\nonline data collection and calibration, contributing to the development of more\nintelligent robotic systems. Specifically, we leverage the Fisher information\nmatrix (FIM) to quantify parameter observability and adopt its minimum\neigenvalue as an optimization metric for trajectory generation via B-spline\ncurves. Through planning and replanning of robot trajectory online, the method\nenhances the observability of multi-sensor extrinsic parameters. The\neffectiveness and advantages of our method have been demonstrated through\nnumerical simulations and real-world experiments. For the benefit of the\ncommunity, we have also open-sourced our code and data at\nhttps://github.com/AISLAB-sustech/Multisensor-Calibration.\n", "link": "http://arxiv.org/abs/2506.13420v1", "date": "2025-06-16", "relevancy": 2.2823, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6271}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5614}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Observability-Aware%20Active%20Calibration%20of%20Multi-Sensor%20Extrinsics%20for%0A%20%20Ground%20Robots%20via%20Online%20Trajectory%20Optimization&body=Title%3A%20Observability-Aware%20Active%20Calibration%20of%20Multi-Sensor%20Extrinsics%20for%0A%20%20Ground%20Robots%20via%20Online%20Trajectory%20Optimization%0AAuthor%3A%20Jiang%20Wang%20and%20Yaozhong%20Kang%20and%20Linya%20Fu%20and%20Kazuhiro%20Nakadai%20and%20He%20Kong%0AAbstract%3A%20%20%20Accurate%20calibration%20of%20sensor%20extrinsic%20parameters%20for%20ground%20robotic%0Asystems%20%28i.e.%2C%20relative%20poses%29%20is%20crucial%20for%20ensuring%20spatial%20alignment%20and%0Aachieving%20high-performance%20perception.%20However%2C%20existing%20calibration%20methods%0Atypically%20require%20complex%20and%20often%20human-operated%20processes%20to%20collect%20data.%0AMoreover%2C%20most%20frameworks%20neglect%20acoustic%20sensors%2C%20thereby%20limiting%20the%0Aassociated%20systems%27%20auditory%20perception%20capabilities.%20To%20alleviate%20these%0Aissues%2C%20we%20propose%20an%20observability-aware%20active%20calibration%20method%20for%20ground%0Arobots%20with%20multimodal%20sensors%2C%20including%20a%20microphone%20array%2C%20a%20LiDAR%0A%28exteroceptive%20sensors%29%2C%20and%20wheel%20encoders%20%28proprioceptive%20sensors%29.%20Unlike%0Atraditional%20approaches%2C%20our%20method%20enables%20active%20trajectory%20optimization%20for%0Aonline%20data%20collection%20and%20calibration%2C%20contributing%20to%20the%20development%20of%20more%0Aintelligent%20robotic%20systems.%20Specifically%2C%20we%20leverage%20the%20Fisher%20information%0Amatrix%20%28FIM%29%20to%20quantify%20parameter%20observability%20and%20adopt%20its%20minimum%0Aeigenvalue%20as%20an%20optimization%20metric%20for%20trajectory%20generation%20via%20B-spline%0Acurves.%20Through%20planning%20and%20replanning%20of%20robot%20trajectory%20online%2C%20the%20method%0Aenhances%20the%20observability%20of%20multi-sensor%20extrinsic%20parameters.%20The%0Aeffectiveness%20and%20advantages%20of%20our%20method%20have%20been%20demonstrated%20through%0Anumerical%20simulations%20and%20real-world%20experiments.%20For%20the%20benefit%20of%20the%0Acommunity%2C%20we%20have%20also%20open-sourced%20our%20code%20and%20data%20at%0Ahttps%3A//github.com/AISLAB-sustech/Multisensor-Calibration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObservability-Aware%2520Active%2520Calibration%2520of%2520Multi-Sensor%2520Extrinsics%2520for%250A%2520%2520Ground%2520Robots%2520via%2520Online%2520Trajectory%2520Optimization%26entry.906535625%3DJiang%2520Wang%2520and%2520Yaozhong%2520Kang%2520and%2520Linya%2520Fu%2520and%2520Kazuhiro%2520Nakadai%2520and%2520He%2520Kong%26entry.1292438233%3D%2520%2520Accurate%2520calibration%2520of%2520sensor%2520extrinsic%2520parameters%2520for%2520ground%2520robotic%250Asystems%2520%2528i.e.%252C%2520relative%2520poses%2529%2520is%2520crucial%2520for%2520ensuring%2520spatial%2520alignment%2520and%250Aachieving%2520high-performance%2520perception.%2520However%252C%2520existing%2520calibration%2520methods%250Atypically%2520require%2520complex%2520and%2520often%2520human-operated%2520processes%2520to%2520collect%2520data.%250AMoreover%252C%2520most%2520frameworks%2520neglect%2520acoustic%2520sensors%252C%2520thereby%2520limiting%2520the%250Aassociated%2520systems%2527%2520auditory%2520perception%2520capabilities.%2520To%2520alleviate%2520these%250Aissues%252C%2520we%2520propose%2520an%2520observability-aware%2520active%2520calibration%2520method%2520for%2520ground%250Arobots%2520with%2520multimodal%2520sensors%252C%2520including%2520a%2520microphone%2520array%252C%2520a%2520LiDAR%250A%2528exteroceptive%2520sensors%2529%252C%2520and%2520wheel%2520encoders%2520%2528proprioceptive%2520sensors%2529.%2520Unlike%250Atraditional%2520approaches%252C%2520our%2520method%2520enables%2520active%2520trajectory%2520optimization%2520for%250Aonline%2520data%2520collection%2520and%2520calibration%252C%2520contributing%2520to%2520the%2520development%2520of%2520more%250Aintelligent%2520robotic%2520systems.%2520Specifically%252C%2520we%2520leverage%2520the%2520Fisher%2520information%250Amatrix%2520%2528FIM%2529%2520to%2520quantify%2520parameter%2520observability%2520and%2520adopt%2520its%2520minimum%250Aeigenvalue%2520as%2520an%2520optimization%2520metric%2520for%2520trajectory%2520generation%2520via%2520B-spline%250Acurves.%2520Through%2520planning%2520and%2520replanning%2520of%2520robot%2520trajectory%2520online%252C%2520the%2520method%250Aenhances%2520the%2520observability%2520of%2520multi-sensor%2520extrinsic%2520parameters.%2520The%250Aeffectiveness%2520and%2520advantages%2520of%2520our%2520method%2520have%2520been%2520demonstrated%2520through%250Anumerical%2520simulations%2520and%2520real-world%2520experiments.%2520For%2520the%2520benefit%2520of%2520the%250Acommunity%252C%2520we%2520have%2520also%2520open-sourced%2520our%2520code%2520and%2520data%2520at%250Ahttps%253A//github.com/AISLAB-sustech/Multisensor-Calibration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Observability-Aware%20Active%20Calibration%20of%20Multi-Sensor%20Extrinsics%20for%0A%20%20Ground%20Robots%20via%20Online%20Trajectory%20Optimization&entry.906535625=Jiang%20Wang%20and%20Yaozhong%20Kang%20and%20Linya%20Fu%20and%20Kazuhiro%20Nakadai%20and%20He%20Kong&entry.1292438233=%20%20Accurate%20calibration%20of%20sensor%20extrinsic%20parameters%20for%20ground%20robotic%0Asystems%20%28i.e.%2C%20relative%20poses%29%20is%20crucial%20for%20ensuring%20spatial%20alignment%20and%0Aachieving%20high-performance%20perception.%20However%2C%20existing%20calibration%20methods%0Atypically%20require%20complex%20and%20often%20human-operated%20processes%20to%20collect%20data.%0AMoreover%2C%20most%20frameworks%20neglect%20acoustic%20sensors%2C%20thereby%20limiting%20the%0Aassociated%20systems%27%20auditory%20perception%20capabilities.%20To%20alleviate%20these%0Aissues%2C%20we%20propose%20an%20observability-aware%20active%20calibration%20method%20for%20ground%0Arobots%20with%20multimodal%20sensors%2C%20including%20a%20microphone%20array%2C%20a%20LiDAR%0A%28exteroceptive%20sensors%29%2C%20and%20wheel%20encoders%20%28proprioceptive%20sensors%29.%20Unlike%0Atraditional%20approaches%2C%20our%20method%20enables%20active%20trajectory%20optimization%20for%0Aonline%20data%20collection%20and%20calibration%2C%20contributing%20to%20the%20development%20of%20more%0Aintelligent%20robotic%20systems.%20Specifically%2C%20we%20leverage%20the%20Fisher%20information%0Amatrix%20%28FIM%29%20to%20quantify%20parameter%20observability%20and%20adopt%20its%20minimum%0Aeigenvalue%20as%20an%20optimization%20metric%20for%20trajectory%20generation%20via%20B-spline%0Acurves.%20Through%20planning%20and%20replanning%20of%20robot%20trajectory%20online%2C%20the%20method%0Aenhances%20the%20observability%20of%20multi-sensor%20extrinsic%20parameters.%20The%0Aeffectiveness%20and%20advantages%20of%20our%20method%20have%20been%20demonstrated%20through%0Anumerical%20simulations%20and%20real-world%20experiments.%20For%20the%20benefit%20of%20the%0Acommunity%2C%20we%20have%20also%20open-sourced%20our%20code%20and%20data%20at%0Ahttps%3A//github.com/AISLAB-sustech/Multisensor-Calibration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13420v1&entry.124074799=Read"},
{"title": "Audio-Visual Driven Compression for Low-Bitrate Talking Head Videos", "author": "Riku Takahashi and Ryugo Morita and Jinjia Zhou", "abstract": "  Talking head video compression has advanced with neural rendering and\nkeypoint-based methods, but challenges remain, especially at low bit rates,\nincluding handling large head movements, suboptimal lip synchronization, and\ndistorted facial reconstructions. To address these problems, we propose a novel\naudio-visual driven video codec that integrates compact 3D motion features and\naudio signals. This approach robustly models significant head rotations and\naligns lip movements with speech, improving both compression efficiency and\nreconstruction quality. Experiments on the CelebV-HQ dataset show that our\nmethod reduces bitrate by 22% compared to VVC and by 8.5% over state-of-the-art\nlearning-based codec. Furthermore, it provides superior lip-sync accuracy and\nvisual fidelity at comparable bitrates, highlighting its effectiveness in\nbandwidth-constrained scenarios.\n", "link": "http://arxiv.org/abs/2506.13419v1", "date": "2025-06-16", "relevancy": 2.2782, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5812}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5782}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio-Visual%20Driven%20Compression%20for%20Low-Bitrate%20Talking%20Head%20Videos&body=Title%3A%20Audio-Visual%20Driven%20Compression%20for%20Low-Bitrate%20Talking%20Head%20Videos%0AAuthor%3A%20Riku%20Takahashi%20and%20Ryugo%20Morita%20and%20Jinjia%20Zhou%0AAbstract%3A%20%20%20Talking%20head%20video%20compression%20has%20advanced%20with%20neural%20rendering%20and%0Akeypoint-based%20methods%2C%20but%20challenges%20remain%2C%20especially%20at%20low%20bit%20rates%2C%0Aincluding%20handling%20large%20head%20movements%2C%20suboptimal%20lip%20synchronization%2C%20and%0Adistorted%20facial%20reconstructions.%20To%20address%20these%20problems%2C%20we%20propose%20a%20novel%0Aaudio-visual%20driven%20video%20codec%20that%20integrates%20compact%203D%20motion%20features%20and%0Aaudio%20signals.%20This%20approach%20robustly%20models%20significant%20head%20rotations%20and%0Aaligns%20lip%20movements%20with%20speech%2C%20improving%20both%20compression%20efficiency%20and%0Areconstruction%20quality.%20Experiments%20on%20the%20CelebV-HQ%20dataset%20show%20that%20our%0Amethod%20reduces%20bitrate%20by%2022%25%20compared%20to%20VVC%20and%20by%208.5%25%20over%20state-of-the-art%0Alearning-based%20codec.%20Furthermore%2C%20it%20provides%20superior%20lip-sync%20accuracy%20and%0Avisual%20fidelity%20at%20comparable%20bitrates%2C%20highlighting%20its%20effectiveness%20in%0Abandwidth-constrained%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio-Visual%2520Driven%2520Compression%2520for%2520Low-Bitrate%2520Talking%2520Head%2520Videos%26entry.906535625%3DRiku%2520Takahashi%2520and%2520Ryugo%2520Morita%2520and%2520Jinjia%2520Zhou%26entry.1292438233%3D%2520%2520Talking%2520head%2520video%2520compression%2520has%2520advanced%2520with%2520neural%2520rendering%2520and%250Akeypoint-based%2520methods%252C%2520but%2520challenges%2520remain%252C%2520especially%2520at%2520low%2520bit%2520rates%252C%250Aincluding%2520handling%2520large%2520head%2520movements%252C%2520suboptimal%2520lip%2520synchronization%252C%2520and%250Adistorted%2520facial%2520reconstructions.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520a%2520novel%250Aaudio-visual%2520driven%2520video%2520codec%2520that%2520integrates%2520compact%25203D%2520motion%2520features%2520and%250Aaudio%2520signals.%2520This%2520approach%2520robustly%2520models%2520significant%2520head%2520rotations%2520and%250Aaligns%2520lip%2520movements%2520with%2520speech%252C%2520improving%2520both%2520compression%2520efficiency%2520and%250Areconstruction%2520quality.%2520Experiments%2520on%2520the%2520CelebV-HQ%2520dataset%2520show%2520that%2520our%250Amethod%2520reduces%2520bitrate%2520by%252022%2525%2520compared%2520to%2520VVC%2520and%2520by%25208.5%2525%2520over%2520state-of-the-art%250Alearning-based%2520codec.%2520Furthermore%252C%2520it%2520provides%2520superior%2520lip-sync%2520accuracy%2520and%250Avisual%2520fidelity%2520at%2520comparable%2520bitrates%252C%2520highlighting%2520its%2520effectiveness%2520in%250Abandwidth-constrained%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio-Visual%20Driven%20Compression%20for%20Low-Bitrate%20Talking%20Head%20Videos&entry.906535625=Riku%20Takahashi%20and%20Ryugo%20Morita%20and%20Jinjia%20Zhou&entry.1292438233=%20%20Talking%20head%20video%20compression%20has%20advanced%20with%20neural%20rendering%20and%0Akeypoint-based%20methods%2C%20but%20challenges%20remain%2C%20especially%20at%20low%20bit%20rates%2C%0Aincluding%20handling%20large%20head%20movements%2C%20suboptimal%20lip%20synchronization%2C%20and%0Adistorted%20facial%20reconstructions.%20To%20address%20these%20problems%2C%20we%20propose%20a%20novel%0Aaudio-visual%20driven%20video%20codec%20that%20integrates%20compact%203D%20motion%20features%20and%0Aaudio%20signals.%20This%20approach%20robustly%20models%20significant%20head%20rotations%20and%0Aaligns%20lip%20movements%20with%20speech%2C%20improving%20both%20compression%20efficiency%20and%0Areconstruction%20quality.%20Experiments%20on%20the%20CelebV-HQ%20dataset%20show%20that%20our%0Amethod%20reduces%20bitrate%20by%2022%25%20compared%20to%20VVC%20and%20by%208.5%25%20over%20state-of-the-art%0Alearning-based%20codec.%20Furthermore%2C%20it%20provides%20superior%20lip-sync%20accuracy%20and%0Avisual%20fidelity%20at%20comparable%20bitrates%2C%20highlighting%20its%20effectiveness%20in%0Abandwidth-constrained%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13419v1&entry.124074799=Read"},
{"title": "CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit\n  Decoding", "author": "Wenxuan Song and Jiayi Chen and Pengxiang Ding and Yuxin Huang and Han Zhao and Donglin Wang and Haoang Li", "abstract": "  In recent years, Vision-Language-Action (VLA) models have become a vital\nresearch direction in robotics due to their impressive multimodal understanding\nand generalization capabilities. Despite the progress, their practical\ndeployment is severely constrained by inference speed bottlenecks, particularly\nin high-frequency and dexterous manipulation tasks. While recent studies have\nexplored Jacobi decoding as a more efficient alternative to traditional\nautoregressive decoding, its practical benefits are marginal due to the lengthy\niterations. To address it, we introduce consistency distillation training to\npredict multiple correct action tokens in each iteration, thereby achieving\nacceleration. Besides, we design mixed-label supervision to mitigate the error\naccumulation during distillation. Although distillation brings acceptable\nspeedup, we identify that certain inefficient iterations remain a critical\nbottleneck. To tackle this, we propose an early-exit decoding strategy that\nmoderately relaxes convergence conditions, which further improves average\ninference efficiency. Experimental results show that the proposed method\nachieves more than 4 times inference acceleration across different baselines\nwhile maintaining high task success rates in both simulated and real-world\nrobot tasks. These experiments validate that our approach provides an efficient\nand general paradigm for accelerating multimodal decision-making in robotics.\nOur project page is available at https://irpn-eai.github.io/CEED-VLA/.\n", "link": "http://arxiv.org/abs/2506.13725v1", "date": "2025-06-16", "relevancy": 2.2776, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5715}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5715}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CEED-VLA%3A%20Consistency%20Vision-Language-Action%20Model%20with%20Early-Exit%0A%20%20Decoding&body=Title%3A%20CEED-VLA%3A%20Consistency%20Vision-Language-Action%20Model%20with%20Early-Exit%0A%20%20Decoding%0AAuthor%3A%20Wenxuan%20Song%20and%20Jiayi%20Chen%20and%20Pengxiang%20Ding%20and%20Yuxin%20Huang%20and%20Han%20Zhao%20and%20Donglin%20Wang%20and%20Haoang%20Li%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Vision-Language-Action%20%28VLA%29%20models%20have%20become%20a%20vital%0Aresearch%20direction%20in%20robotics%20due%20to%20their%20impressive%20multimodal%20understanding%0Aand%20generalization%20capabilities.%20Despite%20the%20progress%2C%20their%20practical%0Adeployment%20is%20severely%20constrained%20by%20inference%20speed%20bottlenecks%2C%20particularly%0Ain%20high-frequency%20and%20dexterous%20manipulation%20tasks.%20While%20recent%20studies%20have%0Aexplored%20Jacobi%20decoding%20as%20a%20more%20efficient%20alternative%20to%20traditional%0Aautoregressive%20decoding%2C%20its%20practical%20benefits%20are%20marginal%20due%20to%20the%20lengthy%0Aiterations.%20To%20address%20it%2C%20we%20introduce%20consistency%20distillation%20training%20to%0Apredict%20multiple%20correct%20action%20tokens%20in%20each%20iteration%2C%20thereby%20achieving%0Aacceleration.%20Besides%2C%20we%20design%20mixed-label%20supervision%20to%20mitigate%20the%20error%0Aaccumulation%20during%20distillation.%20Although%20distillation%20brings%20acceptable%0Aspeedup%2C%20we%20identify%20that%20certain%20inefficient%20iterations%20remain%20a%20critical%0Abottleneck.%20To%20tackle%20this%2C%20we%20propose%20an%20early-exit%20decoding%20strategy%20that%0Amoderately%20relaxes%20convergence%20conditions%2C%20which%20further%20improves%20average%0Ainference%20efficiency.%20Experimental%20results%20show%20that%20the%20proposed%20method%0Aachieves%20more%20than%204%20times%20inference%20acceleration%20across%20different%20baselines%0Awhile%20maintaining%20high%20task%20success%20rates%20in%20both%20simulated%20and%20real-world%0Arobot%20tasks.%20These%20experiments%20validate%20that%20our%20approach%20provides%20an%20efficient%0Aand%20general%20paradigm%20for%20accelerating%20multimodal%20decision-making%20in%20robotics.%0AOur%20project%20page%20is%20available%20at%20https%3A//irpn-eai.github.io/CEED-VLA/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13725v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCEED-VLA%253A%2520Consistency%2520Vision-Language-Action%2520Model%2520with%2520Early-Exit%250A%2520%2520Decoding%26entry.906535625%3DWenxuan%2520Song%2520and%2520Jiayi%2520Chen%2520and%2520Pengxiang%2520Ding%2520and%2520Yuxin%2520Huang%2520and%2520Han%2520Zhao%2520and%2520Donglin%2520Wang%2520and%2520Haoang%2520Li%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520become%2520a%2520vital%250Aresearch%2520direction%2520in%2520robotics%2520due%2520to%2520their%2520impressive%2520multimodal%2520understanding%250Aand%2520generalization%2520capabilities.%2520Despite%2520the%2520progress%252C%2520their%2520practical%250Adeployment%2520is%2520severely%2520constrained%2520by%2520inference%2520speed%2520bottlenecks%252C%2520particularly%250Ain%2520high-frequency%2520and%2520dexterous%2520manipulation%2520tasks.%2520While%2520recent%2520studies%2520have%250Aexplored%2520Jacobi%2520decoding%2520as%2520a%2520more%2520efficient%2520alternative%2520to%2520traditional%250Aautoregressive%2520decoding%252C%2520its%2520practical%2520benefits%2520are%2520marginal%2520due%2520to%2520the%2520lengthy%250Aiterations.%2520To%2520address%2520it%252C%2520we%2520introduce%2520consistency%2520distillation%2520training%2520to%250Apredict%2520multiple%2520correct%2520action%2520tokens%2520in%2520each%2520iteration%252C%2520thereby%2520achieving%250Aacceleration.%2520Besides%252C%2520we%2520design%2520mixed-label%2520supervision%2520to%2520mitigate%2520the%2520error%250Aaccumulation%2520during%2520distillation.%2520Although%2520distillation%2520brings%2520acceptable%250Aspeedup%252C%2520we%2520identify%2520that%2520certain%2520inefficient%2520iterations%2520remain%2520a%2520critical%250Abottleneck.%2520To%2520tackle%2520this%252C%2520we%2520propose%2520an%2520early-exit%2520decoding%2520strategy%2520that%250Amoderately%2520relaxes%2520convergence%2520conditions%252C%2520which%2520further%2520improves%2520average%250Ainference%2520efficiency.%2520Experimental%2520results%2520show%2520that%2520the%2520proposed%2520method%250Aachieves%2520more%2520than%25204%2520times%2520inference%2520acceleration%2520across%2520different%2520baselines%250Awhile%2520maintaining%2520high%2520task%2520success%2520rates%2520in%2520both%2520simulated%2520and%2520real-world%250Arobot%2520tasks.%2520These%2520experiments%2520validate%2520that%2520our%2520approach%2520provides%2520an%2520efficient%250Aand%2520general%2520paradigm%2520for%2520accelerating%2520multimodal%2520decision-making%2520in%2520robotics.%250AOur%2520project%2520page%2520is%2520available%2520at%2520https%253A//irpn-eai.github.io/CEED-VLA/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13725v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CEED-VLA%3A%20Consistency%20Vision-Language-Action%20Model%20with%20Early-Exit%0A%20%20Decoding&entry.906535625=Wenxuan%20Song%20and%20Jiayi%20Chen%20and%20Pengxiang%20Ding%20and%20Yuxin%20Huang%20and%20Han%20Zhao%20and%20Donglin%20Wang%20and%20Haoang%20Li&entry.1292438233=%20%20In%20recent%20years%2C%20Vision-Language-Action%20%28VLA%29%20models%20have%20become%20a%20vital%0Aresearch%20direction%20in%20robotics%20due%20to%20their%20impressive%20multimodal%20understanding%0Aand%20generalization%20capabilities.%20Despite%20the%20progress%2C%20their%20practical%0Adeployment%20is%20severely%20constrained%20by%20inference%20speed%20bottlenecks%2C%20particularly%0Ain%20high-frequency%20and%20dexterous%20manipulation%20tasks.%20While%20recent%20studies%20have%0Aexplored%20Jacobi%20decoding%20as%20a%20more%20efficient%20alternative%20to%20traditional%0Aautoregressive%20decoding%2C%20its%20practical%20benefits%20are%20marginal%20due%20to%20the%20lengthy%0Aiterations.%20To%20address%20it%2C%20we%20introduce%20consistency%20distillation%20training%20to%0Apredict%20multiple%20correct%20action%20tokens%20in%20each%20iteration%2C%20thereby%20achieving%0Aacceleration.%20Besides%2C%20we%20design%20mixed-label%20supervision%20to%20mitigate%20the%20error%0Aaccumulation%20during%20distillation.%20Although%20distillation%20brings%20acceptable%0Aspeedup%2C%20we%20identify%20that%20certain%20inefficient%20iterations%20remain%20a%20critical%0Abottleneck.%20To%20tackle%20this%2C%20we%20propose%20an%20early-exit%20decoding%20strategy%20that%0Amoderately%20relaxes%20convergence%20conditions%2C%20which%20further%20improves%20average%0Ainference%20efficiency.%20Experimental%20results%20show%20that%20the%20proposed%20method%0Aachieves%20more%20than%204%20times%20inference%20acceleration%20across%20different%20baselines%0Awhile%20maintaining%20high%20task%20success%20rates%20in%20both%20simulated%20and%20real-world%0Arobot%20tasks.%20These%20experiments%20validate%20that%20our%20approach%20provides%20an%20efficient%0Aand%20general%20paradigm%20for%20accelerating%20multimodal%20decision-making%20in%20robotics.%0AOur%20project%20page%20is%20available%20at%20https%3A//irpn-eai.github.io/CEED-VLA/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13725v1&entry.124074799=Read"},
{"title": "Understanding Learning Invariance in Deep Linear Networks", "author": "Hao Duan and Guido Mont\u00fafar", "abstract": "  Equivariant and invariant machine learning models exploit symmetries and\nstructural patterns in data to improve sample efficiency. While empirical\nstudies suggest that data-driven methods such as regularization and data\naugmentation can perform comparably to explicitly invariant models, theoretical\ninsights remain scarce. In this paper, we provide a theoretical comparison of\nthree approaches for achieving invariance: data augmentation, regularization,\nand hard-wiring. We focus on mean squared error regression with deep linear\nnetworks, which parametrize rank-bounded linear maps and can be hard-wired to\nbe invariant to specific group actions. We show that the critical points of the\noptimization problems for hard-wiring and data augmentation are identical,\nconsisting solely of saddles and the global optimum. By contrast,\nregularization introduces additional critical points, though they remain\nsaddles except for the global optimum. Moreover, we demonstrate that the\nregularization path is continuous and converges to the hard-wired solution.\n", "link": "http://arxiv.org/abs/2506.13714v1", "date": "2025-06-16", "relevancy": 2.2631, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4624}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4485}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Learning%20Invariance%20in%20Deep%20Linear%20Networks&body=Title%3A%20Understanding%20Learning%20Invariance%20in%20Deep%20Linear%20Networks%0AAuthor%3A%20Hao%20Duan%20and%20Guido%20Mont%C3%BAfar%0AAbstract%3A%20%20%20Equivariant%20and%20invariant%20machine%20learning%20models%20exploit%20symmetries%20and%0Astructural%20patterns%20in%20data%20to%20improve%20sample%20efficiency.%20While%20empirical%0Astudies%20suggest%20that%20data-driven%20methods%20such%20as%20regularization%20and%20data%0Aaugmentation%20can%20perform%20comparably%20to%20explicitly%20invariant%20models%2C%20theoretical%0Ainsights%20remain%20scarce.%20In%20this%20paper%2C%20we%20provide%20a%20theoretical%20comparison%20of%0Athree%20approaches%20for%20achieving%20invariance%3A%20data%20augmentation%2C%20regularization%2C%0Aand%20hard-wiring.%20We%20focus%20on%20mean%20squared%20error%20regression%20with%20deep%20linear%0Anetworks%2C%20which%20parametrize%20rank-bounded%20linear%20maps%20and%20can%20be%20hard-wired%20to%0Abe%20invariant%20to%20specific%20group%20actions.%20We%20show%20that%20the%20critical%20points%20of%20the%0Aoptimization%20problems%20for%20hard-wiring%20and%20data%20augmentation%20are%20identical%2C%0Aconsisting%20solely%20of%20saddles%20and%20the%20global%20optimum.%20By%20contrast%2C%0Aregularization%20introduces%20additional%20critical%20points%2C%20though%20they%20remain%0Asaddles%20except%20for%20the%20global%20optimum.%20Moreover%2C%20we%20demonstrate%20that%20the%0Aregularization%20path%20is%20continuous%20and%20converges%20to%20the%20hard-wired%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Learning%2520Invariance%2520in%2520Deep%2520Linear%2520Networks%26entry.906535625%3DHao%2520Duan%2520and%2520Guido%2520Mont%25C3%25BAfar%26entry.1292438233%3D%2520%2520Equivariant%2520and%2520invariant%2520machine%2520learning%2520models%2520exploit%2520symmetries%2520and%250Astructural%2520patterns%2520in%2520data%2520to%2520improve%2520sample%2520efficiency.%2520While%2520empirical%250Astudies%2520suggest%2520that%2520data-driven%2520methods%2520such%2520as%2520regularization%2520and%2520data%250Aaugmentation%2520can%2520perform%2520comparably%2520to%2520explicitly%2520invariant%2520models%252C%2520theoretical%250Ainsights%2520remain%2520scarce.%2520In%2520this%2520paper%252C%2520we%2520provide%2520a%2520theoretical%2520comparison%2520of%250Athree%2520approaches%2520for%2520achieving%2520invariance%253A%2520data%2520augmentation%252C%2520regularization%252C%250Aand%2520hard-wiring.%2520We%2520focus%2520on%2520mean%2520squared%2520error%2520regression%2520with%2520deep%2520linear%250Anetworks%252C%2520which%2520parametrize%2520rank-bounded%2520linear%2520maps%2520and%2520can%2520be%2520hard-wired%2520to%250Abe%2520invariant%2520to%2520specific%2520group%2520actions.%2520We%2520show%2520that%2520the%2520critical%2520points%2520of%2520the%250Aoptimization%2520problems%2520for%2520hard-wiring%2520and%2520data%2520augmentation%2520are%2520identical%252C%250Aconsisting%2520solely%2520of%2520saddles%2520and%2520the%2520global%2520optimum.%2520By%2520contrast%252C%250Aregularization%2520introduces%2520additional%2520critical%2520points%252C%2520though%2520they%2520remain%250Asaddles%2520except%2520for%2520the%2520global%2520optimum.%2520Moreover%252C%2520we%2520demonstrate%2520that%2520the%250Aregularization%2520path%2520is%2520continuous%2520and%2520converges%2520to%2520the%2520hard-wired%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Learning%20Invariance%20in%20Deep%20Linear%20Networks&entry.906535625=Hao%20Duan%20and%20Guido%20Mont%C3%BAfar&entry.1292438233=%20%20Equivariant%20and%20invariant%20machine%20learning%20models%20exploit%20symmetries%20and%0Astructural%20patterns%20in%20data%20to%20improve%20sample%20efficiency.%20While%20empirical%0Astudies%20suggest%20that%20data-driven%20methods%20such%20as%20regularization%20and%20data%0Aaugmentation%20can%20perform%20comparably%20to%20explicitly%20invariant%20models%2C%20theoretical%0Ainsights%20remain%20scarce.%20In%20this%20paper%2C%20we%20provide%20a%20theoretical%20comparison%20of%0Athree%20approaches%20for%20achieving%20invariance%3A%20data%20augmentation%2C%20regularization%2C%0Aand%20hard-wiring.%20We%20focus%20on%20mean%20squared%20error%20regression%20with%20deep%20linear%0Anetworks%2C%20which%20parametrize%20rank-bounded%20linear%20maps%20and%20can%20be%20hard-wired%20to%0Abe%20invariant%20to%20specific%20group%20actions.%20We%20show%20that%20the%20critical%20points%20of%20the%0Aoptimization%20problems%20for%20hard-wiring%20and%20data%20augmentation%20are%20identical%2C%0Aconsisting%20solely%20of%20saddles%20and%20the%20global%20optimum.%20By%20contrast%2C%0Aregularization%20introduces%20additional%20critical%20points%2C%20though%20they%20remain%0Asaddles%20except%20for%20the%20global%20optimum.%20Moreover%2C%20we%20demonstrate%20that%20the%0Aregularization%20path%20is%20continuous%20and%20converges%20to%20the%20hard-wired%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13714v1&entry.124074799=Read"},
{"title": "MambaTalk: Efficient Holistic Gesture Synthesis with Selective State\n  Space Models", "author": "Zunnan Xu and Yukang Lin and Haonan Han and Sicheng Yang and Ronghui Li and Yachao Zhang and Xiu Li", "abstract": "  Gesture synthesis is a vital realm of human-computer interaction, with\nwide-ranging applications across various fields like film, robotics, and\nvirtual reality. Recent advancements have utilized the diffusion model and\nattention mechanisms to improve gesture synthesis. However, due to the high\ncomputational complexity of these techniques, generating long and diverse\nsequences with low latency remains a challenge. We explore the potential of\nstate space models (SSMs) to address the challenge, implementing a two-stage\nmodeling strategy with discrete motion priors to enhance the quality of\ngestures. Leveraging the foundational Mamba block, we introduce MambaTalk,\nenhancing gesture diversity and rhythm through multimodal integration.\nExtensive experiments demonstrate that our method matches or exceeds the\nperformance of state-of-the-art models. Our project is publicly available at\nhttps://kkakkkka.github.io/MambaTalk\n", "link": "http://arxiv.org/abs/2403.09471v6", "date": "2025-06-16", "relevancy": 2.2476, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5849}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5483}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambaTalk%3A%20Efficient%20Holistic%20Gesture%20Synthesis%20with%20Selective%20State%0A%20%20Space%20Models&body=Title%3A%20MambaTalk%3A%20Efficient%20Holistic%20Gesture%20Synthesis%20with%20Selective%20State%0A%20%20Space%20Models%0AAuthor%3A%20Zunnan%20Xu%20and%20Yukang%20Lin%20and%20Haonan%20Han%20and%20Sicheng%20Yang%20and%20Ronghui%20Li%20and%20Yachao%20Zhang%20and%20Xiu%20Li%0AAbstract%3A%20%20%20Gesture%20synthesis%20is%20a%20vital%20realm%20of%20human-computer%20interaction%2C%20with%0Awide-ranging%20applications%20across%20various%20fields%20like%20film%2C%20robotics%2C%20and%0Avirtual%20reality.%20Recent%20advancements%20have%20utilized%20the%20diffusion%20model%20and%0Aattention%20mechanisms%20to%20improve%20gesture%20synthesis.%20However%2C%20due%20to%20the%20high%0Acomputational%20complexity%20of%20these%20techniques%2C%20generating%20long%20and%20diverse%0Asequences%20with%20low%20latency%20remains%20a%20challenge.%20We%20explore%20the%20potential%20of%0Astate%20space%20models%20%28SSMs%29%20to%20address%20the%20challenge%2C%20implementing%20a%20two-stage%0Amodeling%20strategy%20with%20discrete%20motion%20priors%20to%20enhance%20the%20quality%20of%0Agestures.%20Leveraging%20the%20foundational%20Mamba%20block%2C%20we%20introduce%20MambaTalk%2C%0Aenhancing%20gesture%20diversity%20and%20rhythm%20through%20multimodal%20integration.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20matches%20or%20exceeds%20the%0Aperformance%20of%20state-of-the-art%20models.%20Our%20project%20is%20publicly%20available%20at%0Ahttps%3A//kkakkkka.github.io/MambaTalk%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09471v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambaTalk%253A%2520Efficient%2520Holistic%2520Gesture%2520Synthesis%2520with%2520Selective%2520State%250A%2520%2520Space%2520Models%26entry.906535625%3DZunnan%2520Xu%2520and%2520Yukang%2520Lin%2520and%2520Haonan%2520Han%2520and%2520Sicheng%2520Yang%2520and%2520Ronghui%2520Li%2520and%2520Yachao%2520Zhang%2520and%2520Xiu%2520Li%26entry.1292438233%3D%2520%2520Gesture%2520synthesis%2520is%2520a%2520vital%2520realm%2520of%2520human-computer%2520interaction%252C%2520with%250Awide-ranging%2520applications%2520across%2520various%2520fields%2520like%2520film%252C%2520robotics%252C%2520and%250Avirtual%2520reality.%2520Recent%2520advancements%2520have%2520utilized%2520the%2520diffusion%2520model%2520and%250Aattention%2520mechanisms%2520to%2520improve%2520gesture%2520synthesis.%2520However%252C%2520due%2520to%2520the%2520high%250Acomputational%2520complexity%2520of%2520these%2520techniques%252C%2520generating%2520long%2520and%2520diverse%250Asequences%2520with%2520low%2520latency%2520remains%2520a%2520challenge.%2520We%2520explore%2520the%2520potential%2520of%250Astate%2520space%2520models%2520%2528SSMs%2529%2520to%2520address%2520the%2520challenge%252C%2520implementing%2520a%2520two-stage%250Amodeling%2520strategy%2520with%2520discrete%2520motion%2520priors%2520to%2520enhance%2520the%2520quality%2520of%250Agestures.%2520Leveraging%2520the%2520foundational%2520Mamba%2520block%252C%2520we%2520introduce%2520MambaTalk%252C%250Aenhancing%2520gesture%2520diversity%2520and%2520rhythm%2520through%2520multimodal%2520integration.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520matches%2520or%2520exceeds%2520the%250Aperformance%2520of%2520state-of-the-art%2520models.%2520Our%2520project%2520is%2520publicly%2520available%2520at%250Ahttps%253A//kkakkkka.github.io/MambaTalk%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09471v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaTalk%3A%20Efficient%20Holistic%20Gesture%20Synthesis%20with%20Selective%20State%0A%20%20Space%20Models&entry.906535625=Zunnan%20Xu%20and%20Yukang%20Lin%20and%20Haonan%20Han%20and%20Sicheng%20Yang%20and%20Ronghui%20Li%20and%20Yachao%20Zhang%20and%20Xiu%20Li&entry.1292438233=%20%20Gesture%20synthesis%20is%20a%20vital%20realm%20of%20human-computer%20interaction%2C%20with%0Awide-ranging%20applications%20across%20various%20fields%20like%20film%2C%20robotics%2C%20and%0Avirtual%20reality.%20Recent%20advancements%20have%20utilized%20the%20diffusion%20model%20and%0Aattention%20mechanisms%20to%20improve%20gesture%20synthesis.%20However%2C%20due%20to%20the%20high%0Acomputational%20complexity%20of%20these%20techniques%2C%20generating%20long%20and%20diverse%0Asequences%20with%20low%20latency%20remains%20a%20challenge.%20We%20explore%20the%20potential%20of%0Astate%20space%20models%20%28SSMs%29%20to%20address%20the%20challenge%2C%20implementing%20a%20two-stage%0Amodeling%20strategy%20with%20discrete%20motion%20priors%20to%20enhance%20the%20quality%20of%0Agestures.%20Leveraging%20the%20foundational%20Mamba%20block%2C%20we%20introduce%20MambaTalk%2C%0Aenhancing%20gesture%20diversity%20and%20rhythm%20through%20multimodal%20integration.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20matches%20or%20exceeds%20the%0Aperformance%20of%20state-of-the-art%20models.%20Our%20project%20is%20publicly%20available%20at%0Ahttps%3A//kkakkkka.github.io/MambaTalk%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09471v6&entry.124074799=Read"},
{"title": "Gradient-Normalized Smoothness for Optimization with Approximate\n  Hessians", "author": "Andrei Semenov and Martin Jaggi and Nikita Doikov", "abstract": "  In this work, we develop new optimization algorithms that use approximate\nsecond-order information combined with the gradient regularization technique to\nachieve fast global convergence rates for both convex and non-convex\nobjectives. The key innovation of our analysis is a novel notion called\nGradient-Normalized Smoothness, which characterizes the maximum radius of a\nball around the current point that yields a good relative approximation of the\ngradient field. Our theory establishes a natural intrinsic connection between\nHessian approximation and the linearization of the gradient. Importantly,\nGradient-Normalized Smoothness does not depend on the specific problem class of\nthe objective functions, while effectively translating local information about\nthe gradient field and Hessian approximation into the global behavior of the\nmethod. This new concept equips approximate second-order algorithms with\nuniversal global convergence guarantees, recovering state-of-the-art rates for\nfunctions with H\\\"older-continuous Hessians and third derivatives,\nquasi-self-concordant functions, as well as smooth classes in first-order\noptimization. These rates are achieved automatically and extend to broader\nclasses, such as generalized self-concordant functions. We demonstrate direct\napplications of our results for global linear rates in logistic regression and\nsoftmax problems with approximate Hessians, as well as in non-convex\noptimization using Fisher and Gauss-Newton approximations.\n", "link": "http://arxiv.org/abs/2506.13710v1", "date": "2025-06-16", "relevancy": 2.2378, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4599}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.444}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient-Normalized%20Smoothness%20for%20Optimization%20with%20Approximate%0A%20%20Hessians&body=Title%3A%20Gradient-Normalized%20Smoothness%20for%20Optimization%20with%20Approximate%0A%20%20Hessians%0AAuthor%3A%20Andrei%20Semenov%20and%20Martin%20Jaggi%20and%20Nikita%20Doikov%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20develop%20new%20optimization%20algorithms%20that%20use%20approximate%0Asecond-order%20information%20combined%20with%20the%20gradient%20regularization%20technique%20to%0Aachieve%20fast%20global%20convergence%20rates%20for%20both%20convex%20and%20non-convex%0Aobjectives.%20The%20key%20innovation%20of%20our%20analysis%20is%20a%20novel%20notion%20called%0AGradient-Normalized%20Smoothness%2C%20which%20characterizes%20the%20maximum%20radius%20of%20a%0Aball%20around%20the%20current%20point%20that%20yields%20a%20good%20relative%20approximation%20of%20the%0Agradient%20field.%20Our%20theory%20establishes%20a%20natural%20intrinsic%20connection%20between%0AHessian%20approximation%20and%20the%20linearization%20of%20the%20gradient.%20Importantly%2C%0AGradient-Normalized%20Smoothness%20does%20not%20depend%20on%20the%20specific%20problem%20class%20of%0Athe%20objective%20functions%2C%20while%20effectively%20translating%20local%20information%20about%0Athe%20gradient%20field%20and%20Hessian%20approximation%20into%20the%20global%20behavior%20of%20the%0Amethod.%20This%20new%20concept%20equips%20approximate%20second-order%20algorithms%20with%0Auniversal%20global%20convergence%20guarantees%2C%20recovering%20state-of-the-art%20rates%20for%0Afunctions%20with%20H%5C%22older-continuous%20Hessians%20and%20third%20derivatives%2C%0Aquasi-self-concordant%20functions%2C%20as%20well%20as%20smooth%20classes%20in%20first-order%0Aoptimization.%20These%20rates%20are%20achieved%20automatically%20and%20extend%20to%20broader%0Aclasses%2C%20such%20as%20generalized%20self-concordant%20functions.%20We%20demonstrate%20direct%0Aapplications%20of%20our%20results%20for%20global%20linear%20rates%20in%20logistic%20regression%20and%0Asoftmax%20problems%20with%20approximate%20Hessians%2C%20as%20well%20as%20in%20non-convex%0Aoptimization%20using%20Fisher%20and%20Gauss-Newton%20approximations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient-Normalized%2520Smoothness%2520for%2520Optimization%2520with%2520Approximate%250A%2520%2520Hessians%26entry.906535625%3DAndrei%2520Semenov%2520and%2520Martin%2520Jaggi%2520and%2520Nikita%2520Doikov%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520develop%2520new%2520optimization%2520algorithms%2520that%2520use%2520approximate%250Asecond-order%2520information%2520combined%2520with%2520the%2520gradient%2520regularization%2520technique%2520to%250Aachieve%2520fast%2520global%2520convergence%2520rates%2520for%2520both%2520convex%2520and%2520non-convex%250Aobjectives.%2520The%2520key%2520innovation%2520of%2520our%2520analysis%2520is%2520a%2520novel%2520notion%2520called%250AGradient-Normalized%2520Smoothness%252C%2520which%2520characterizes%2520the%2520maximum%2520radius%2520of%2520a%250Aball%2520around%2520the%2520current%2520point%2520that%2520yields%2520a%2520good%2520relative%2520approximation%2520of%2520the%250Agradient%2520field.%2520Our%2520theory%2520establishes%2520a%2520natural%2520intrinsic%2520connection%2520between%250AHessian%2520approximation%2520and%2520the%2520linearization%2520of%2520the%2520gradient.%2520Importantly%252C%250AGradient-Normalized%2520Smoothness%2520does%2520not%2520depend%2520on%2520the%2520specific%2520problem%2520class%2520of%250Athe%2520objective%2520functions%252C%2520while%2520effectively%2520translating%2520local%2520information%2520about%250Athe%2520gradient%2520field%2520and%2520Hessian%2520approximation%2520into%2520the%2520global%2520behavior%2520of%2520the%250Amethod.%2520This%2520new%2520concept%2520equips%2520approximate%2520second-order%2520algorithms%2520with%250Auniversal%2520global%2520convergence%2520guarantees%252C%2520recovering%2520state-of-the-art%2520rates%2520for%250Afunctions%2520with%2520H%255C%2522older-continuous%2520Hessians%2520and%2520third%2520derivatives%252C%250Aquasi-self-concordant%2520functions%252C%2520as%2520well%2520as%2520smooth%2520classes%2520in%2520first-order%250Aoptimization.%2520These%2520rates%2520are%2520achieved%2520automatically%2520and%2520extend%2520to%2520broader%250Aclasses%252C%2520such%2520as%2520generalized%2520self-concordant%2520functions.%2520We%2520demonstrate%2520direct%250Aapplications%2520of%2520our%2520results%2520for%2520global%2520linear%2520rates%2520in%2520logistic%2520regression%2520and%250Asoftmax%2520problems%2520with%2520approximate%2520Hessians%252C%2520as%2520well%2520as%2520in%2520non-convex%250Aoptimization%2520using%2520Fisher%2520and%2520Gauss-Newton%2520approximations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-Normalized%20Smoothness%20for%20Optimization%20with%20Approximate%0A%20%20Hessians&entry.906535625=Andrei%20Semenov%20and%20Martin%20Jaggi%20and%20Nikita%20Doikov&entry.1292438233=%20%20In%20this%20work%2C%20we%20develop%20new%20optimization%20algorithms%20that%20use%20approximate%0Asecond-order%20information%20combined%20with%20the%20gradient%20regularization%20technique%20to%0Aachieve%20fast%20global%20convergence%20rates%20for%20both%20convex%20and%20non-convex%0Aobjectives.%20The%20key%20innovation%20of%20our%20analysis%20is%20a%20novel%20notion%20called%0AGradient-Normalized%20Smoothness%2C%20which%20characterizes%20the%20maximum%20radius%20of%20a%0Aball%20around%20the%20current%20point%20that%20yields%20a%20good%20relative%20approximation%20of%20the%0Agradient%20field.%20Our%20theory%20establishes%20a%20natural%20intrinsic%20connection%20between%0AHessian%20approximation%20and%20the%20linearization%20of%20the%20gradient.%20Importantly%2C%0AGradient-Normalized%20Smoothness%20does%20not%20depend%20on%20the%20specific%20problem%20class%20of%0Athe%20objective%20functions%2C%20while%20effectively%20translating%20local%20information%20about%0Athe%20gradient%20field%20and%20Hessian%20approximation%20into%20the%20global%20behavior%20of%20the%0Amethod.%20This%20new%20concept%20equips%20approximate%20second-order%20algorithms%20with%0Auniversal%20global%20convergence%20guarantees%2C%20recovering%20state-of-the-art%20rates%20for%0Afunctions%20with%20H%5C%22older-continuous%20Hessians%20and%20third%20derivatives%2C%0Aquasi-self-concordant%20functions%2C%20as%20well%20as%20smooth%20classes%20in%20first-order%0Aoptimization.%20These%20rates%20are%20achieved%20automatically%20and%20extend%20to%20broader%0Aclasses%2C%20such%20as%20generalized%20self-concordant%20functions.%20We%20demonstrate%20direct%0Aapplications%20of%20our%20results%20for%20global%20linear%20rates%20in%20logistic%20regression%20and%0Asoftmax%20problems%20with%20approximate%20Hessians%2C%20as%20well%20as%20in%20non-convex%0Aoptimization%20using%20Fisher%20and%20Gauss-Newton%20approximations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13710v1&entry.124074799=Read"},
{"title": "JAEGER: Dual-Level Humanoid Whole-Body Controller", "author": "Ziluo Ding and Haobin Jiang and Yuxuan Wang and Zhenguo Sun and Yu Zhang and Xiaojie Niu and Ming Yang and Weishuai Zeng and Xinrun Xu and Zongqing Lu", "abstract": "  This paper presents JAEGER, a dual-level whole-body controller for humanoid\nrobots that addresses the challenges of training a more robust and versatile\npolicy. Unlike traditional single-controller approaches, JAEGER separates the\ncontrol of the upper and lower bodies into two independent controllers, so that\nthey can better focus on their distinct tasks. This separation alleviates the\ndimensionality curse and improves fault tolerance. JAEGER supports both root\nvelocity tracking (coarse-grained control) and local joint angle tracking\n(fine-grained control), enabling versatile and stable movements. To train the\ncontroller, we utilize a human motion dataset (AMASS), retargeting human poses\nto humanoid poses through an efficient retargeting network, and employ a\ncurriculum learning approach. This method performs supervised learning for\ninitialization, followed by reinforcement learning for further exploration. We\nconduct our experiments on two humanoid platforms and demonstrate the\nsuperiority of our approach against state-of-the-art methods in both simulation\nand real environments.\n", "link": "http://arxiv.org/abs/2505.06584v2", "date": "2025-06-16", "relevancy": 2.2337, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5852}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5614}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JAEGER%3A%20Dual-Level%20Humanoid%20Whole-Body%20Controller&body=Title%3A%20JAEGER%3A%20Dual-Level%20Humanoid%20Whole-Body%20Controller%0AAuthor%3A%20Ziluo%20Ding%20and%20Haobin%20Jiang%20and%20Yuxuan%20Wang%20and%20Zhenguo%20Sun%20and%20Yu%20Zhang%20and%20Xiaojie%20Niu%20and%20Ming%20Yang%20and%20Weishuai%20Zeng%20and%20Xinrun%20Xu%20and%20Zongqing%20Lu%0AAbstract%3A%20%20%20This%20paper%20presents%20JAEGER%2C%20a%20dual-level%20whole-body%20controller%20for%20humanoid%0Arobots%20that%20addresses%20the%20challenges%20of%20training%20a%20more%20robust%20and%20versatile%0Apolicy.%20Unlike%20traditional%20single-controller%20approaches%2C%20JAEGER%20separates%20the%0Acontrol%20of%20the%20upper%20and%20lower%20bodies%20into%20two%20independent%20controllers%2C%20so%20that%0Athey%20can%20better%20focus%20on%20their%20distinct%20tasks.%20This%20separation%20alleviates%20the%0Adimensionality%20curse%20and%20improves%20fault%20tolerance.%20JAEGER%20supports%20both%20root%0Avelocity%20tracking%20%28coarse-grained%20control%29%20and%20local%20joint%20angle%20tracking%0A%28fine-grained%20control%29%2C%20enabling%20versatile%20and%20stable%20movements.%20To%20train%20the%0Acontroller%2C%20we%20utilize%20a%20human%20motion%20dataset%20%28AMASS%29%2C%20retargeting%20human%20poses%0Ato%20humanoid%20poses%20through%20an%20efficient%20retargeting%20network%2C%20and%20employ%20a%0Acurriculum%20learning%20approach.%20This%20method%20performs%20supervised%20learning%20for%0Ainitialization%2C%20followed%20by%20reinforcement%20learning%20for%20further%20exploration.%20We%0Aconduct%20our%20experiments%20on%20two%20humanoid%20platforms%20and%20demonstrate%20the%0Asuperiority%20of%20our%20approach%20against%20state-of-the-art%20methods%20in%20both%20simulation%0Aand%20real%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06584v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJAEGER%253A%2520Dual-Level%2520Humanoid%2520Whole-Body%2520Controller%26entry.906535625%3DZiluo%2520Ding%2520and%2520Haobin%2520Jiang%2520and%2520Yuxuan%2520Wang%2520and%2520Zhenguo%2520Sun%2520and%2520Yu%2520Zhang%2520and%2520Xiaojie%2520Niu%2520and%2520Ming%2520Yang%2520and%2520Weishuai%2520Zeng%2520and%2520Xinrun%2520Xu%2520and%2520Zongqing%2520Lu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520JAEGER%252C%2520a%2520dual-level%2520whole-body%2520controller%2520for%2520humanoid%250Arobots%2520that%2520addresses%2520the%2520challenges%2520of%2520training%2520a%2520more%2520robust%2520and%2520versatile%250Apolicy.%2520Unlike%2520traditional%2520single-controller%2520approaches%252C%2520JAEGER%2520separates%2520the%250Acontrol%2520of%2520the%2520upper%2520and%2520lower%2520bodies%2520into%2520two%2520independent%2520controllers%252C%2520so%2520that%250Athey%2520can%2520better%2520focus%2520on%2520their%2520distinct%2520tasks.%2520This%2520separation%2520alleviates%2520the%250Adimensionality%2520curse%2520and%2520improves%2520fault%2520tolerance.%2520JAEGER%2520supports%2520both%2520root%250Avelocity%2520tracking%2520%2528coarse-grained%2520control%2529%2520and%2520local%2520joint%2520angle%2520tracking%250A%2528fine-grained%2520control%2529%252C%2520enabling%2520versatile%2520and%2520stable%2520movements.%2520To%2520train%2520the%250Acontroller%252C%2520we%2520utilize%2520a%2520human%2520motion%2520dataset%2520%2528AMASS%2529%252C%2520retargeting%2520human%2520poses%250Ato%2520humanoid%2520poses%2520through%2520an%2520efficient%2520retargeting%2520network%252C%2520and%2520employ%2520a%250Acurriculum%2520learning%2520approach.%2520This%2520method%2520performs%2520supervised%2520learning%2520for%250Ainitialization%252C%2520followed%2520by%2520reinforcement%2520learning%2520for%2520further%2520exploration.%2520We%250Aconduct%2520our%2520experiments%2520on%2520two%2520humanoid%2520platforms%2520and%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520approach%2520against%2520state-of-the-art%2520methods%2520in%2520both%2520simulation%250Aand%2520real%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06584v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JAEGER%3A%20Dual-Level%20Humanoid%20Whole-Body%20Controller&entry.906535625=Ziluo%20Ding%20and%20Haobin%20Jiang%20and%20Yuxuan%20Wang%20and%20Zhenguo%20Sun%20and%20Yu%20Zhang%20and%20Xiaojie%20Niu%20and%20Ming%20Yang%20and%20Weishuai%20Zeng%20and%20Xinrun%20Xu%20and%20Zongqing%20Lu&entry.1292438233=%20%20This%20paper%20presents%20JAEGER%2C%20a%20dual-level%20whole-body%20controller%20for%20humanoid%0Arobots%20that%20addresses%20the%20challenges%20of%20training%20a%20more%20robust%20and%20versatile%0Apolicy.%20Unlike%20traditional%20single-controller%20approaches%2C%20JAEGER%20separates%20the%0Acontrol%20of%20the%20upper%20and%20lower%20bodies%20into%20two%20independent%20controllers%2C%20so%20that%0Athey%20can%20better%20focus%20on%20their%20distinct%20tasks.%20This%20separation%20alleviates%20the%0Adimensionality%20curse%20and%20improves%20fault%20tolerance.%20JAEGER%20supports%20both%20root%0Avelocity%20tracking%20%28coarse-grained%20control%29%20and%20local%20joint%20angle%20tracking%0A%28fine-grained%20control%29%2C%20enabling%20versatile%20and%20stable%20movements.%20To%20train%20the%0Acontroller%2C%20we%20utilize%20a%20human%20motion%20dataset%20%28AMASS%29%2C%20retargeting%20human%20poses%0Ato%20humanoid%20poses%20through%20an%20efficient%20retargeting%20network%2C%20and%20employ%20a%0Acurriculum%20learning%20approach.%20This%20method%20performs%20supervised%20learning%20for%0Ainitialization%2C%20followed%20by%20reinforcement%20learning%20for%20further%20exploration.%20We%0Aconduct%20our%20experiments%20on%20two%20humanoid%20platforms%20and%20demonstrate%20the%0Asuperiority%20of%20our%20approach%20against%20state-of-the-art%20methods%20in%20both%20simulation%0Aand%20real%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06584v2&entry.124074799=Read"},
{"title": "DualEdit: Dual Editing for Knowledge Updating in Vision-Language Models", "author": "Zhiyi Shi and Binjie Wang and Chongjie Si and Yichen Wu and Junsik Kim and Hanspeter Pfister", "abstract": "  Model editing aims to efficiently update a pre-trained model's knowledge\nwithout the need for time-consuming full retraining. While existing pioneering\nediting methods achieve promising results, they primarily focus on editing\nsingle-modal language models (LLMs). However, for vision-language models\n(VLMs), which involve multiple modalities, the role and impact of each modality\non editing performance remain largely unexplored. To address this gap, we\nexplore the impact of textual and visual modalities on model editing and find\nthat: (1) textual and visual representations reach peak sensitivity at\ndifferent layers, reflecting their varying importance; and (2) editing both\nmodalities can efficiently update knowledge, but this comes at the cost of\ncompromising the model's original capabilities. Based on our findings, we\npropose DualEdit, an editor that modifies both textual and visual modalities at\ntheir respective key layers. Additionally, we introduce a gating module within\nthe more sensitive textual modality, allowing DualEdit to efficiently update\nnew knowledge while preserving the model's original information. We evaluate\nDualEdit across multiple VLM backbones and benchmark datasets, demonstrating\nits superiority over state-of-the-art VLM editing baselines as well as adapted\nLLM editing methods on different evaluation metrics.\n", "link": "http://arxiv.org/abs/2506.13638v1", "date": "2025-06-16", "relevancy": 2.2315, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5524}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DualEdit%3A%20Dual%20Editing%20for%20Knowledge%20Updating%20in%20Vision-Language%20Models&body=Title%3A%20DualEdit%3A%20Dual%20Editing%20for%20Knowledge%20Updating%20in%20Vision-Language%20Models%0AAuthor%3A%20Zhiyi%20Shi%20and%20Binjie%20Wang%20and%20Chongjie%20Si%20and%20Yichen%20Wu%20and%20Junsik%20Kim%20and%20Hanspeter%20Pfister%0AAbstract%3A%20%20%20Model%20editing%20aims%20to%20efficiently%20update%20a%20pre-trained%20model%27s%20knowledge%0Awithout%20the%20need%20for%20time-consuming%20full%20retraining.%20While%20existing%20pioneering%0Aediting%20methods%20achieve%20promising%20results%2C%20they%20primarily%20focus%20on%20editing%0Asingle-modal%20language%20models%20%28LLMs%29.%20However%2C%20for%20vision-language%20models%0A%28VLMs%29%2C%20which%20involve%20multiple%20modalities%2C%20the%20role%20and%20impact%20of%20each%20modality%0Aon%20editing%20performance%20remain%20largely%20unexplored.%20To%20address%20this%20gap%2C%20we%0Aexplore%20the%20impact%20of%20textual%20and%20visual%20modalities%20on%20model%20editing%20and%20find%0Athat%3A%20%281%29%20textual%20and%20visual%20representations%20reach%20peak%20sensitivity%20at%0Adifferent%20layers%2C%20reflecting%20their%20varying%20importance%3B%20and%20%282%29%20editing%20both%0Amodalities%20can%20efficiently%20update%20knowledge%2C%20but%20this%20comes%20at%20the%20cost%20of%0Acompromising%20the%20model%27s%20original%20capabilities.%20Based%20on%20our%20findings%2C%20we%0Apropose%20DualEdit%2C%20an%20editor%20that%20modifies%20both%20textual%20and%20visual%20modalities%20at%0Atheir%20respective%20key%20layers.%20Additionally%2C%20we%20introduce%20a%20gating%20module%20within%0Athe%20more%20sensitive%20textual%20modality%2C%20allowing%20DualEdit%20to%20efficiently%20update%0Anew%20knowledge%20while%20preserving%20the%20model%27s%20original%20information.%20We%20evaluate%0ADualEdit%20across%20multiple%20VLM%20backbones%20and%20benchmark%20datasets%2C%20demonstrating%0Aits%20superiority%20over%20state-of-the-art%20VLM%20editing%20baselines%20as%20well%20as%20adapted%0ALLM%20editing%20methods%20on%20different%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDualEdit%253A%2520Dual%2520Editing%2520for%2520Knowledge%2520Updating%2520in%2520Vision-Language%2520Models%26entry.906535625%3DZhiyi%2520Shi%2520and%2520Binjie%2520Wang%2520and%2520Chongjie%2520Si%2520and%2520Yichen%2520Wu%2520and%2520Junsik%2520Kim%2520and%2520Hanspeter%2520Pfister%26entry.1292438233%3D%2520%2520Model%2520editing%2520aims%2520to%2520efficiently%2520update%2520a%2520pre-trained%2520model%2527s%2520knowledge%250Awithout%2520the%2520need%2520for%2520time-consuming%2520full%2520retraining.%2520While%2520existing%2520pioneering%250Aediting%2520methods%2520achieve%2520promising%2520results%252C%2520they%2520primarily%2520focus%2520on%2520editing%250Asingle-modal%2520language%2520models%2520%2528LLMs%2529.%2520However%252C%2520for%2520vision-language%2520models%250A%2528VLMs%2529%252C%2520which%2520involve%2520multiple%2520modalities%252C%2520the%2520role%2520and%2520impact%2520of%2520each%2520modality%250Aon%2520editing%2520performance%2520remain%2520largely%2520unexplored.%2520To%2520address%2520this%2520gap%252C%2520we%250Aexplore%2520the%2520impact%2520of%2520textual%2520and%2520visual%2520modalities%2520on%2520model%2520editing%2520and%2520find%250Athat%253A%2520%25281%2529%2520textual%2520and%2520visual%2520representations%2520reach%2520peak%2520sensitivity%2520at%250Adifferent%2520layers%252C%2520reflecting%2520their%2520varying%2520importance%253B%2520and%2520%25282%2529%2520editing%2520both%250Amodalities%2520can%2520efficiently%2520update%2520knowledge%252C%2520but%2520this%2520comes%2520at%2520the%2520cost%2520of%250Acompromising%2520the%2520model%2527s%2520original%2520capabilities.%2520Based%2520on%2520our%2520findings%252C%2520we%250Apropose%2520DualEdit%252C%2520an%2520editor%2520that%2520modifies%2520both%2520textual%2520and%2520visual%2520modalities%2520at%250Atheir%2520respective%2520key%2520layers.%2520Additionally%252C%2520we%2520introduce%2520a%2520gating%2520module%2520within%250Athe%2520more%2520sensitive%2520textual%2520modality%252C%2520allowing%2520DualEdit%2520to%2520efficiently%2520update%250Anew%2520knowledge%2520while%2520preserving%2520the%2520model%2527s%2520original%2520information.%2520We%2520evaluate%250ADualEdit%2520across%2520multiple%2520VLM%2520backbones%2520and%2520benchmark%2520datasets%252C%2520demonstrating%250Aits%2520superiority%2520over%2520state-of-the-art%2520VLM%2520editing%2520baselines%2520as%2520well%2520as%2520adapted%250ALLM%2520editing%2520methods%2520on%2520different%2520evaluation%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DualEdit%3A%20Dual%20Editing%20for%20Knowledge%20Updating%20in%20Vision-Language%20Models&entry.906535625=Zhiyi%20Shi%20and%20Binjie%20Wang%20and%20Chongjie%20Si%20and%20Yichen%20Wu%20and%20Junsik%20Kim%20and%20Hanspeter%20Pfister&entry.1292438233=%20%20Model%20editing%20aims%20to%20efficiently%20update%20a%20pre-trained%20model%27s%20knowledge%0Awithout%20the%20need%20for%20time-consuming%20full%20retraining.%20While%20existing%20pioneering%0Aediting%20methods%20achieve%20promising%20results%2C%20they%20primarily%20focus%20on%20editing%0Asingle-modal%20language%20models%20%28LLMs%29.%20However%2C%20for%20vision-language%20models%0A%28VLMs%29%2C%20which%20involve%20multiple%20modalities%2C%20the%20role%20and%20impact%20of%20each%20modality%0Aon%20editing%20performance%20remain%20largely%20unexplored.%20To%20address%20this%20gap%2C%20we%0Aexplore%20the%20impact%20of%20textual%20and%20visual%20modalities%20on%20model%20editing%20and%20find%0Athat%3A%20%281%29%20textual%20and%20visual%20representations%20reach%20peak%20sensitivity%20at%0Adifferent%20layers%2C%20reflecting%20their%20varying%20importance%3B%20and%20%282%29%20editing%20both%0Amodalities%20can%20efficiently%20update%20knowledge%2C%20but%20this%20comes%20at%20the%20cost%20of%0Acompromising%20the%20model%27s%20original%20capabilities.%20Based%20on%20our%20findings%2C%20we%0Apropose%20DualEdit%2C%20an%20editor%20that%20modifies%20both%20textual%20and%20visual%20modalities%20at%0Atheir%20respective%20key%20layers.%20Additionally%2C%20we%20introduce%20a%20gating%20module%20within%0Athe%20more%20sensitive%20textual%20modality%2C%20allowing%20DualEdit%20to%20efficiently%20update%0Anew%20knowledge%20while%20preserving%20the%20model%27s%20original%20information.%20We%20evaluate%0ADualEdit%20across%20multiple%20VLM%20backbones%20and%20benchmark%20datasets%2C%20demonstrating%0Aits%20superiority%20over%20state-of-the-art%20VLM%20editing%20baselines%20as%20well%20as%20adapted%0ALLM%20editing%20methods%20on%20different%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13638v1&entry.124074799=Read"},
{"title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\n  Simulation", "author": "Yuwei Du and Jie Feng and Jian Yuan and Yong Li", "abstract": "  Human mobility simulation plays a crucial role in various real-world\napplications. Recently, to address the limitations of traditional data-driven\napproaches, researchers have explored leveraging the commonsense knowledge and\nreasoning capabilities of large language models (LLMs) to accelerate human\nmobility simulation. However, these methods suffer from several critical\nshortcomings, including inadequate modeling of urban spaces and poor\nintegration with both individual mobility patterns and collective mobility\ndistributions. To address these challenges, we propose \\textbf{C}ityGPT-Powered\n\\textbf{A}gentic framework for \\textbf{M}obility \\textbf{S}imulation\n(\\textbf{CAMS}), an agentic framework that leverages the language based urban\nfoundation model to simulate human mobility in urban space. \\textbf{CAMS}\ncomprises three core modules, including MobExtractor to extract template\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\nto generate anchor points considering collective knowledge and generate\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\ngenerate trajectories with real trajectory preference alignment via DPO.\nExperiments on real-world datasets show that \\textbf{CAMS} achieves superior\nperformance without relying on externally provided geospatial information.\nMoreover, by holistically modeling both individual mobility patterns and\ncollective mobility constraints, \\textbf{CAMS} generates more realistic and\nplausible trajectories. In general, \\textbf{CAMS} establishes a new paradigm\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\nmobility simulation.\n", "link": "http://arxiv.org/abs/2506.13599v1", "date": "2025-06-16", "relevancy": 2.2273, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5725}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.563}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAMS%3A%20A%20CityGPT-Powered%20Agentic%20Framework%20for%20Urban%20Human%20Mobility%0A%20%20Simulation&body=Title%3A%20CAMS%3A%20A%20CityGPT-Powered%20Agentic%20Framework%20for%20Urban%20Human%20Mobility%0A%20%20Simulation%0AAuthor%3A%20Yuwei%20Du%20and%20Jie%20Feng%20and%20Jian%20Yuan%20and%20Yong%20Li%0AAbstract%3A%20%20%20Human%20mobility%20simulation%20plays%20a%20crucial%20role%20in%20various%20real-world%0Aapplications.%20Recently%2C%20to%20address%20the%20limitations%20of%20traditional%20data-driven%0Aapproaches%2C%20researchers%20have%20explored%20leveraging%20the%20commonsense%20knowledge%20and%0Areasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20to%20accelerate%20human%0Amobility%20simulation.%20However%2C%20these%20methods%20suffer%20from%20several%20critical%0Ashortcomings%2C%20including%20inadequate%20modeling%20of%20urban%20spaces%20and%20poor%0Aintegration%20with%20both%20individual%20mobility%20patterns%20and%20collective%20mobility%0Adistributions.%20To%20address%20these%20challenges%2C%20we%20propose%20%5Ctextbf%7BC%7DityGPT-Powered%0A%5Ctextbf%7BA%7Dgentic%20framework%20for%20%5Ctextbf%7BM%7Dobility%20%5Ctextbf%7BS%7Dimulation%0A%28%5Ctextbf%7BCAMS%7D%29%2C%20an%20agentic%20framework%20that%20leverages%20the%20language%20based%20urban%0Afoundation%20model%20to%20simulate%20human%20mobility%20in%20urban%20space.%20%5Ctextbf%7BCAMS%7D%0Acomprises%20three%20core%20modules%2C%20including%20MobExtractor%20to%20extract%20template%0Amobility%20patterns%20and%20synthesize%20new%20ones%20based%20on%20user%20profiles%2C%20GeoGenerator%0Ato%20generate%20anchor%20points%20considering%20collective%20knowledge%20and%20generate%0Acandidate%20urban%20geospatial%20knowledge%20using%20an%20enhanced%20version%20of%20CityGPT%2C%0ATrajEnhancer%20to%20retrieve%20spatial%20knowledge%20based%20on%20mobility%20patterns%20and%0Agenerate%20trajectories%20with%20real%20trajectory%20preference%20alignment%20via%20DPO.%0AExperiments%20on%20real-world%20datasets%20show%20that%20%5Ctextbf%7BCAMS%7D%20achieves%20superior%0Aperformance%20without%20relying%20on%20externally%20provided%20geospatial%20information.%0AMoreover%2C%20by%20holistically%20modeling%20both%20individual%20mobility%20patterns%20and%0Acollective%20mobility%20constraints%2C%20%5Ctextbf%7BCAMS%7D%20generates%20more%20realistic%20and%0Aplausible%20trajectories.%20In%20general%2C%20%5Ctextbf%7BCAMS%7D%20establishes%20a%20new%20paradigm%0Athat%20integrates%20the%20agentic%20framework%20with%20urban-knowledgeable%20LLMs%20for%20human%0Amobility%20simulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAMS%253A%2520A%2520CityGPT-Powered%2520Agentic%2520Framework%2520for%2520Urban%2520Human%2520Mobility%250A%2520%2520Simulation%26entry.906535625%3DYuwei%2520Du%2520and%2520Jie%2520Feng%2520and%2520Jian%2520Yuan%2520and%2520Yong%2520Li%26entry.1292438233%3D%2520%2520Human%2520mobility%2520simulation%2520plays%2520a%2520crucial%2520role%2520in%2520various%2520real-world%250Aapplications.%2520Recently%252C%2520to%2520address%2520the%2520limitations%2520of%2520traditional%2520data-driven%250Aapproaches%252C%2520researchers%2520have%2520explored%2520leveraging%2520the%2520commonsense%2520knowledge%2520and%250Areasoning%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520accelerate%2520human%250Amobility%2520simulation.%2520However%252C%2520these%2520methods%2520suffer%2520from%2520several%2520critical%250Ashortcomings%252C%2520including%2520inadequate%2520modeling%2520of%2520urban%2520spaces%2520and%2520poor%250Aintegration%2520with%2520both%2520individual%2520mobility%2520patterns%2520and%2520collective%2520mobility%250Adistributions.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520%255Ctextbf%257BC%257DityGPT-Powered%250A%255Ctextbf%257BA%257Dgentic%2520framework%2520for%2520%255Ctextbf%257BM%257Dobility%2520%255Ctextbf%257BS%257Dimulation%250A%2528%255Ctextbf%257BCAMS%257D%2529%252C%2520an%2520agentic%2520framework%2520that%2520leverages%2520the%2520language%2520based%2520urban%250Afoundation%2520model%2520to%2520simulate%2520human%2520mobility%2520in%2520urban%2520space.%2520%255Ctextbf%257BCAMS%257D%250Acomprises%2520three%2520core%2520modules%252C%2520including%2520MobExtractor%2520to%2520extract%2520template%250Amobility%2520patterns%2520and%2520synthesize%2520new%2520ones%2520based%2520on%2520user%2520profiles%252C%2520GeoGenerator%250Ato%2520generate%2520anchor%2520points%2520considering%2520collective%2520knowledge%2520and%2520generate%250Acandidate%2520urban%2520geospatial%2520knowledge%2520using%2520an%2520enhanced%2520version%2520of%2520CityGPT%252C%250ATrajEnhancer%2520to%2520retrieve%2520spatial%2520knowledge%2520based%2520on%2520mobility%2520patterns%2520and%250Agenerate%2520trajectories%2520with%2520real%2520trajectory%2520preference%2520alignment%2520via%2520DPO.%250AExperiments%2520on%2520real-world%2520datasets%2520show%2520that%2520%255Ctextbf%257BCAMS%257D%2520achieves%2520superior%250Aperformance%2520without%2520relying%2520on%2520externally%2520provided%2520geospatial%2520information.%250AMoreover%252C%2520by%2520holistically%2520modeling%2520both%2520individual%2520mobility%2520patterns%2520and%250Acollective%2520mobility%2520constraints%252C%2520%255Ctextbf%257BCAMS%257D%2520generates%2520more%2520realistic%2520and%250Aplausible%2520trajectories.%2520In%2520general%252C%2520%255Ctextbf%257BCAMS%257D%2520establishes%2520a%2520new%2520paradigm%250Athat%2520integrates%2520the%2520agentic%2520framework%2520with%2520urban-knowledgeable%2520LLMs%2520for%2520human%250Amobility%2520simulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAMS%3A%20A%20CityGPT-Powered%20Agentic%20Framework%20for%20Urban%20Human%20Mobility%0A%20%20Simulation&entry.906535625=Yuwei%20Du%20and%20Jie%20Feng%20and%20Jian%20Yuan%20and%20Yong%20Li&entry.1292438233=%20%20Human%20mobility%20simulation%20plays%20a%20crucial%20role%20in%20various%20real-world%0Aapplications.%20Recently%2C%20to%20address%20the%20limitations%20of%20traditional%20data-driven%0Aapproaches%2C%20researchers%20have%20explored%20leveraging%20the%20commonsense%20knowledge%20and%0Areasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20to%20accelerate%20human%0Amobility%20simulation.%20However%2C%20these%20methods%20suffer%20from%20several%20critical%0Ashortcomings%2C%20including%20inadequate%20modeling%20of%20urban%20spaces%20and%20poor%0Aintegration%20with%20both%20individual%20mobility%20patterns%20and%20collective%20mobility%0Adistributions.%20To%20address%20these%20challenges%2C%20we%20propose%20%5Ctextbf%7BC%7DityGPT-Powered%0A%5Ctextbf%7BA%7Dgentic%20framework%20for%20%5Ctextbf%7BM%7Dobility%20%5Ctextbf%7BS%7Dimulation%0A%28%5Ctextbf%7BCAMS%7D%29%2C%20an%20agentic%20framework%20that%20leverages%20the%20language%20based%20urban%0Afoundation%20model%20to%20simulate%20human%20mobility%20in%20urban%20space.%20%5Ctextbf%7BCAMS%7D%0Acomprises%20three%20core%20modules%2C%20including%20MobExtractor%20to%20extract%20template%0Amobility%20patterns%20and%20synthesize%20new%20ones%20based%20on%20user%20profiles%2C%20GeoGenerator%0Ato%20generate%20anchor%20points%20considering%20collective%20knowledge%20and%20generate%0Acandidate%20urban%20geospatial%20knowledge%20using%20an%20enhanced%20version%20of%20CityGPT%2C%0ATrajEnhancer%20to%20retrieve%20spatial%20knowledge%20based%20on%20mobility%20patterns%20and%0Agenerate%20trajectories%20with%20real%20trajectory%20preference%20alignment%20via%20DPO.%0AExperiments%20on%20real-world%20datasets%20show%20that%20%5Ctextbf%7BCAMS%7D%20achieves%20superior%0Aperformance%20without%20relying%20on%20externally%20provided%20geospatial%20information.%0AMoreover%2C%20by%20holistically%20modeling%20both%20individual%20mobility%20patterns%20and%0Acollective%20mobility%20constraints%2C%20%5Ctextbf%7BCAMS%7D%20generates%20more%20realistic%20and%0Aplausible%20trajectories.%20In%20general%2C%20%5Ctextbf%7BCAMS%7D%20establishes%20a%20new%20paradigm%0Athat%20integrates%20the%20agentic%20framework%20with%20urban-knowledgeable%20LLMs%20for%20human%0Amobility%20simulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13599v1&entry.124074799=Read"},
{"title": "Omni-AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented for\n  Efficient Long Video Understanding", "author": "Zhucun Xue and Jiangning Zhang and Xurong Xie and Yuxuan Cai and Yong Liu and Xiangtai Li and Dacheng Tao", "abstract": "  Multimodal Large Language Models (MLLMs) struggle with long videos due to\nfixed context windows and weak long-term dependency modeling. Existing\nRetrieval-Augmented Generation (RAG) methods for videos use static retrieval\nstrategies, leading to inefficiencies for simple queries and information loss\nfor complex tasks. To address this, we propose AdaVideoRAG, a novel framework\nthat dynamically adapts retrieval granularity based on query complexity using a\nlightweight intent classifier. Our framework employs an Omni-Knowledge Indexing\nmodule to build hierarchical databases from text (captions, ASR, OCR), visual\nfeatures, and semantic graphs, enabling optimal resource allocation across\ntasks. We also introduce the HiVU benchmark for comprehensive evaluation.\nExperiments demonstrate improved efficiency and accuracy for long-video\nunderstanding, with seamless integration into existing MLLMs. AdaVideoRAG\nestablishes a new paradigm for adaptive retrieval in video analysis. Codes will\nbe open-sourced at https://github.com/xzc-zju/AdaVideoRAG.\n", "link": "http://arxiv.org/abs/2506.13589v1", "date": "2025-06-16", "relevancy": 2.2273, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omni-AdaVideoRAG%3A%20Omni-Contextual%20Adaptive%20Retrieval-Augmented%20for%0A%20%20Efficient%20Long%20Video%20Understanding&body=Title%3A%20Omni-AdaVideoRAG%3A%20Omni-Contextual%20Adaptive%20Retrieval-Augmented%20for%0A%20%20Efficient%20Long%20Video%20Understanding%0AAuthor%3A%20Zhucun%20Xue%20and%20Jiangning%20Zhang%20and%20Xurong%20Xie%20and%20Yuxuan%20Cai%20and%20Yong%20Liu%20and%20Xiangtai%20Li%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20struggle%20with%20long%20videos%20due%20to%0Afixed%20context%20windows%20and%20weak%20long-term%20dependency%20modeling.%20Existing%0ARetrieval-Augmented%20Generation%20%28RAG%29%20methods%20for%20videos%20use%20static%20retrieval%0Astrategies%2C%20leading%20to%20inefficiencies%20for%20simple%20queries%20and%20information%20loss%0Afor%20complex%20tasks.%20To%20address%20this%2C%20we%20propose%20AdaVideoRAG%2C%20a%20novel%20framework%0Athat%20dynamically%20adapts%20retrieval%20granularity%20based%20on%20query%20complexity%20using%20a%0Alightweight%20intent%20classifier.%20Our%20framework%20employs%20an%20Omni-Knowledge%20Indexing%0Amodule%20to%20build%20hierarchical%20databases%20from%20text%20%28captions%2C%20ASR%2C%20OCR%29%2C%20visual%0Afeatures%2C%20and%20semantic%20graphs%2C%20enabling%20optimal%20resource%20allocation%20across%0Atasks.%20We%20also%20introduce%20the%20HiVU%20benchmark%20for%20comprehensive%20evaluation.%0AExperiments%20demonstrate%20improved%20efficiency%20and%20accuracy%20for%20long-video%0Aunderstanding%2C%20with%20seamless%20integration%20into%20existing%20MLLMs.%20AdaVideoRAG%0Aestablishes%20a%20new%20paradigm%20for%20adaptive%20retrieval%20in%20video%20analysis.%20Codes%20will%0Abe%20open-sourced%20at%20https%3A//github.com/xzc-zju/AdaVideoRAG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmni-AdaVideoRAG%253A%2520Omni-Contextual%2520Adaptive%2520Retrieval-Augmented%2520for%250A%2520%2520Efficient%2520Long%2520Video%2520Understanding%26entry.906535625%3DZhucun%2520Xue%2520and%2520Jiangning%2520Zhang%2520and%2520Xurong%2520Xie%2520and%2520Yuxuan%2520Cai%2520and%2520Yong%2520Liu%2520and%2520Xiangtai%2520Li%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520struggle%2520with%2520long%2520videos%2520due%2520to%250Afixed%2520context%2520windows%2520and%2520weak%2520long-term%2520dependency%2520modeling.%2520Existing%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520methods%2520for%2520videos%2520use%2520static%2520retrieval%250Astrategies%252C%2520leading%2520to%2520inefficiencies%2520for%2520simple%2520queries%2520and%2520information%2520loss%250Afor%2520complex%2520tasks.%2520To%2520address%2520this%252C%2520we%2520propose%2520AdaVideoRAG%252C%2520a%2520novel%2520framework%250Athat%2520dynamically%2520adapts%2520retrieval%2520granularity%2520based%2520on%2520query%2520complexity%2520using%2520a%250Alightweight%2520intent%2520classifier.%2520Our%2520framework%2520employs%2520an%2520Omni-Knowledge%2520Indexing%250Amodule%2520to%2520build%2520hierarchical%2520databases%2520from%2520text%2520%2528captions%252C%2520ASR%252C%2520OCR%2529%252C%2520visual%250Afeatures%252C%2520and%2520semantic%2520graphs%252C%2520enabling%2520optimal%2520resource%2520allocation%2520across%250Atasks.%2520We%2520also%2520introduce%2520the%2520HiVU%2520benchmark%2520for%2520comprehensive%2520evaluation.%250AExperiments%2520demonstrate%2520improved%2520efficiency%2520and%2520accuracy%2520for%2520long-video%250Aunderstanding%252C%2520with%2520seamless%2520integration%2520into%2520existing%2520MLLMs.%2520AdaVideoRAG%250Aestablishes%2520a%2520new%2520paradigm%2520for%2520adaptive%2520retrieval%2520in%2520video%2520analysis.%2520Codes%2520will%250Abe%2520open-sourced%2520at%2520https%253A//github.com/xzc-zju/AdaVideoRAG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omni-AdaVideoRAG%3A%20Omni-Contextual%20Adaptive%20Retrieval-Augmented%20for%0A%20%20Efficient%20Long%20Video%20Understanding&entry.906535625=Zhucun%20Xue%20and%20Jiangning%20Zhang%20and%20Xurong%20Xie%20and%20Yuxuan%20Cai%20and%20Yong%20Liu%20and%20Xiangtai%20Li%20and%20Dacheng%20Tao&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20struggle%20with%20long%20videos%20due%20to%0Afixed%20context%20windows%20and%20weak%20long-term%20dependency%20modeling.%20Existing%0ARetrieval-Augmented%20Generation%20%28RAG%29%20methods%20for%20videos%20use%20static%20retrieval%0Astrategies%2C%20leading%20to%20inefficiencies%20for%20simple%20queries%20and%20information%20loss%0Afor%20complex%20tasks.%20To%20address%20this%2C%20we%20propose%20AdaVideoRAG%2C%20a%20novel%20framework%0Athat%20dynamically%20adapts%20retrieval%20granularity%20based%20on%20query%20complexity%20using%20a%0Alightweight%20intent%20classifier.%20Our%20framework%20employs%20an%20Omni-Knowledge%20Indexing%0Amodule%20to%20build%20hierarchical%20databases%20from%20text%20%28captions%2C%20ASR%2C%20OCR%29%2C%20visual%0Afeatures%2C%20and%20semantic%20graphs%2C%20enabling%20optimal%20resource%20allocation%20across%0Atasks.%20We%20also%20introduce%20the%20HiVU%20benchmark%20for%20comprehensive%20evaluation.%0AExperiments%20demonstrate%20improved%20efficiency%20and%20accuracy%20for%20long-video%0Aunderstanding%2C%20with%20seamless%20integration%20into%20existing%20MLLMs.%20AdaVideoRAG%0Aestablishes%20a%20new%20paradigm%20for%20adaptive%20retrieval%20in%20video%20analysis.%20Codes%20will%0Abe%20open-sourced%20at%20https%3A//github.com/xzc-zju/AdaVideoRAG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13589v1&entry.124074799=Read"},
{"title": "Multiverse Through Deepfakes: The MultiFakeVerse Dataset of\n  Person-Centric Visual and Conceptual Manipulations", "author": "Parul Gupta and Shreya Ghosh and Tom Gedeon and Thanh-Toan Do and Abhinav Dhall", "abstract": "  The rapid advancement of GenAI technology over the past few years has\nsignificantly contributed towards highly realistic deepfake content generation.\nDespite ongoing efforts, the research community still lacks a large-scale and\nreasoning capability driven deepfake benchmark dataset specifically tailored\nfor person-centric object, context and scene manipulations. In this paper, we\naddress this gap by introducing MultiFakeVerse, a large scale person-centric\ndeepfake dataset, comprising 845,286 images generated through manipulation\nsuggestions and image manipulations both derived from vision-language models\n(VLM). The VLM instructions were specifically targeted towards modifications to\nindividuals or contextual elements of a scene that influence human perception\nof importance, intent, or narrative. This VLM-driven approach enables semantic,\ncontext-aware alterations such as modifying actions, scenes, and human-object\ninteractions rather than synthetic or low-level identity swaps and\nregion-specific edits that are common in existing datasets. Our experiments\nreveal that current state-of-the-art deepfake detection models and human\nobservers struggle to detect these subtle yet meaningful manipulations. The\ncode and dataset are available on\n\\href{https://github.com/Parul-Gupta/MultiFakeVerse}{GitHub}.\n", "link": "http://arxiv.org/abs/2506.00868v2", "date": "2025-06-16", "relevancy": 2.2249, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5689}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5605}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiverse%20Through%20Deepfakes%3A%20The%20MultiFakeVerse%20Dataset%20of%0A%20%20Person-Centric%20Visual%20and%20Conceptual%20Manipulations&body=Title%3A%20Multiverse%20Through%20Deepfakes%3A%20The%20MultiFakeVerse%20Dataset%20of%0A%20%20Person-Centric%20Visual%20and%20Conceptual%20Manipulations%0AAuthor%3A%20Parul%20Gupta%20and%20Shreya%20Ghosh%20and%20Tom%20Gedeon%20and%20Thanh-Toan%20Do%20and%20Abhinav%20Dhall%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20GenAI%20technology%20over%20the%20past%20few%20years%20has%0Asignificantly%20contributed%20towards%20highly%20realistic%20deepfake%20content%20generation.%0ADespite%20ongoing%20efforts%2C%20the%20research%20community%20still%20lacks%20a%20large-scale%20and%0Areasoning%20capability%20driven%20deepfake%20benchmark%20dataset%20specifically%20tailored%0Afor%20person-centric%20object%2C%20context%20and%20scene%20manipulations.%20In%20this%20paper%2C%20we%0Aaddress%20this%20gap%20by%20introducing%20MultiFakeVerse%2C%20a%20large%20scale%20person-centric%0Adeepfake%20dataset%2C%20comprising%20845%2C286%20images%20generated%20through%20manipulation%0Asuggestions%20and%20image%20manipulations%20both%20derived%20from%20vision-language%20models%0A%28VLM%29.%20The%20VLM%20instructions%20were%20specifically%20targeted%20towards%20modifications%20to%0Aindividuals%20or%20contextual%20elements%20of%20a%20scene%20that%20influence%20human%20perception%0Aof%20importance%2C%20intent%2C%20or%20narrative.%20This%20VLM-driven%20approach%20enables%20semantic%2C%0Acontext-aware%20alterations%20such%20as%20modifying%20actions%2C%20scenes%2C%20and%20human-object%0Ainteractions%20rather%20than%20synthetic%20or%20low-level%20identity%20swaps%20and%0Aregion-specific%20edits%20that%20are%20common%20in%20existing%20datasets.%20Our%20experiments%0Areveal%20that%20current%20state-of-the-art%20deepfake%20detection%20models%20and%20human%0Aobservers%20struggle%20to%20detect%20these%20subtle%20yet%20meaningful%20manipulations.%20The%0Acode%20and%20dataset%20are%20available%20on%0A%5Chref%7Bhttps%3A//github.com/Parul-Gupta/MultiFakeVerse%7D%7BGitHub%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00868v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiverse%2520Through%2520Deepfakes%253A%2520The%2520MultiFakeVerse%2520Dataset%2520of%250A%2520%2520Person-Centric%2520Visual%2520and%2520Conceptual%2520Manipulations%26entry.906535625%3DParul%2520Gupta%2520and%2520Shreya%2520Ghosh%2520and%2520Tom%2520Gedeon%2520and%2520Thanh-Toan%2520Do%2520and%2520Abhinav%2520Dhall%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520GenAI%2520technology%2520over%2520the%2520past%2520few%2520years%2520has%250Asignificantly%2520contributed%2520towards%2520highly%2520realistic%2520deepfake%2520content%2520generation.%250ADespite%2520ongoing%2520efforts%252C%2520the%2520research%2520community%2520still%2520lacks%2520a%2520large-scale%2520and%250Areasoning%2520capability%2520driven%2520deepfake%2520benchmark%2520dataset%2520specifically%2520tailored%250Afor%2520person-centric%2520object%252C%2520context%2520and%2520scene%2520manipulations.%2520In%2520this%2520paper%252C%2520we%250Aaddress%2520this%2520gap%2520by%2520introducing%2520MultiFakeVerse%252C%2520a%2520large%2520scale%2520person-centric%250Adeepfake%2520dataset%252C%2520comprising%2520845%252C286%2520images%2520generated%2520through%2520manipulation%250Asuggestions%2520and%2520image%2520manipulations%2520both%2520derived%2520from%2520vision-language%2520models%250A%2528VLM%2529.%2520The%2520VLM%2520instructions%2520were%2520specifically%2520targeted%2520towards%2520modifications%2520to%250Aindividuals%2520or%2520contextual%2520elements%2520of%2520a%2520scene%2520that%2520influence%2520human%2520perception%250Aof%2520importance%252C%2520intent%252C%2520or%2520narrative.%2520This%2520VLM-driven%2520approach%2520enables%2520semantic%252C%250Acontext-aware%2520alterations%2520such%2520as%2520modifying%2520actions%252C%2520scenes%252C%2520and%2520human-object%250Ainteractions%2520rather%2520than%2520synthetic%2520or%2520low-level%2520identity%2520swaps%2520and%250Aregion-specific%2520edits%2520that%2520are%2520common%2520in%2520existing%2520datasets.%2520Our%2520experiments%250Areveal%2520that%2520current%2520state-of-the-art%2520deepfake%2520detection%2520models%2520and%2520human%250Aobservers%2520struggle%2520to%2520detect%2520these%2520subtle%2520yet%2520meaningful%2520manipulations.%2520The%250Acode%2520and%2520dataset%2520are%2520available%2520on%250A%255Chref%257Bhttps%253A//github.com/Parul-Gupta/MultiFakeVerse%257D%257BGitHub%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00868v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiverse%20Through%20Deepfakes%3A%20The%20MultiFakeVerse%20Dataset%20of%0A%20%20Person-Centric%20Visual%20and%20Conceptual%20Manipulations&entry.906535625=Parul%20Gupta%20and%20Shreya%20Ghosh%20and%20Tom%20Gedeon%20and%20Thanh-Toan%20Do%20and%20Abhinav%20Dhall&entry.1292438233=%20%20The%20rapid%20advancement%20of%20GenAI%20technology%20over%20the%20past%20few%20years%20has%0Asignificantly%20contributed%20towards%20highly%20realistic%20deepfake%20content%20generation.%0ADespite%20ongoing%20efforts%2C%20the%20research%20community%20still%20lacks%20a%20large-scale%20and%0Areasoning%20capability%20driven%20deepfake%20benchmark%20dataset%20specifically%20tailored%0Afor%20person-centric%20object%2C%20context%20and%20scene%20manipulations.%20In%20this%20paper%2C%20we%0Aaddress%20this%20gap%20by%20introducing%20MultiFakeVerse%2C%20a%20large%20scale%20person-centric%0Adeepfake%20dataset%2C%20comprising%20845%2C286%20images%20generated%20through%20manipulation%0Asuggestions%20and%20image%20manipulations%20both%20derived%20from%20vision-language%20models%0A%28VLM%29.%20The%20VLM%20instructions%20were%20specifically%20targeted%20towards%20modifications%20to%0Aindividuals%20or%20contextual%20elements%20of%20a%20scene%20that%20influence%20human%20perception%0Aof%20importance%2C%20intent%2C%20or%20narrative.%20This%20VLM-driven%20approach%20enables%20semantic%2C%0Acontext-aware%20alterations%20such%20as%20modifying%20actions%2C%20scenes%2C%20and%20human-object%0Ainteractions%20rather%20than%20synthetic%20or%20low-level%20identity%20swaps%20and%0Aregion-specific%20edits%20that%20are%20common%20in%20existing%20datasets.%20Our%20experiments%0Areveal%20that%20current%20state-of-the-art%20deepfake%20detection%20models%20and%20human%0Aobservers%20struggle%20to%20detect%20these%20subtle%20yet%20meaningful%20manipulations.%20The%0Acode%20and%20dataset%20are%20available%20on%0A%5Chref%7Bhttps%3A//github.com/Parul-Gupta/MultiFakeVerse%7D%7BGitHub%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00868v2&entry.124074799=Read"},
{"title": "Self-Supervised Enhancement for Depth from a Lightweight ToF Sensor with\n  Monocular Images", "author": "Laiyan Ding and Hualie Jiang and Jiwei Chen and Rui Huang", "abstract": "  Depth map enhancement using paired high-resolution RGB images offers a\ncost-effective solution for improving low-resolution depth data from\nlightweight ToF sensors. Nevertheless, naively adopting a depth estimation\npipeline to fuse the two modalities requires groundtruth depth maps for\nsupervision. To address this, we propose a self-supervised learning framework,\nSelfToF, which generates detailed and scale-aware depth maps. Starting from an\nimage-based self-supervised depth estimation pipeline, we add low-resolution\ndepth as inputs, design a new depth consistency loss, propose a scale-recovery\nmodule, and finally obtain a large performance boost. Furthermore, since the\nToF signal sparsity varies in real-world applications, we upgrade SelfToF to\nSelfToF* with submanifold convolution and guided feature fusion. Consequently,\nSelfToF* maintain robust performance across varying sparsity levels in ToF\ndata. Overall, our proposed method is both efficient and effective, as verified\nby extensive experiments on the NYU and ScanNet datasets. The code will be made\npublic.\n", "link": "http://arxiv.org/abs/2506.13444v1", "date": "2025-06-16", "relevancy": 2.2224, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5663}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5638}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Enhancement%20for%20Depth%20from%20a%20Lightweight%20ToF%20Sensor%20with%0A%20%20Monocular%20Images&body=Title%3A%20Self-Supervised%20Enhancement%20for%20Depth%20from%20a%20Lightweight%20ToF%20Sensor%20with%0A%20%20Monocular%20Images%0AAuthor%3A%20Laiyan%20Ding%20and%20Hualie%20Jiang%20and%20Jiwei%20Chen%20and%20Rui%20Huang%0AAbstract%3A%20%20%20Depth%20map%20enhancement%20using%20paired%20high-resolution%20RGB%20images%20offers%20a%0Acost-effective%20solution%20for%20improving%20low-resolution%20depth%20data%20from%0Alightweight%20ToF%20sensors.%20Nevertheless%2C%20naively%20adopting%20a%20depth%20estimation%0Apipeline%20to%20fuse%20the%20two%20modalities%20requires%20groundtruth%20depth%20maps%20for%0Asupervision.%20To%20address%20this%2C%20we%20propose%20a%20self-supervised%20learning%20framework%2C%0ASelfToF%2C%20which%20generates%20detailed%20and%20scale-aware%20depth%20maps.%20Starting%20from%20an%0Aimage-based%20self-supervised%20depth%20estimation%20pipeline%2C%20we%20add%20low-resolution%0Adepth%20as%20inputs%2C%20design%20a%20new%20depth%20consistency%20loss%2C%20propose%20a%20scale-recovery%0Amodule%2C%20and%20finally%20obtain%20a%20large%20performance%20boost.%20Furthermore%2C%20since%20the%0AToF%20signal%20sparsity%20varies%20in%20real-world%20applications%2C%20we%20upgrade%20SelfToF%20to%0ASelfToF%2A%20with%20submanifold%20convolution%20and%20guided%20feature%20fusion.%20Consequently%2C%0ASelfToF%2A%20maintain%20robust%20performance%20across%20varying%20sparsity%20levels%20in%20ToF%0Adata.%20Overall%2C%20our%20proposed%20method%20is%20both%20efficient%20and%20effective%2C%20as%20verified%0Aby%20extensive%20experiments%20on%20the%20NYU%20and%20ScanNet%20datasets.%20The%20code%20will%20be%20made%0Apublic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Enhancement%2520for%2520Depth%2520from%2520a%2520Lightweight%2520ToF%2520Sensor%2520with%250A%2520%2520Monocular%2520Images%26entry.906535625%3DLaiyan%2520Ding%2520and%2520Hualie%2520Jiang%2520and%2520Jiwei%2520Chen%2520and%2520Rui%2520Huang%26entry.1292438233%3D%2520%2520Depth%2520map%2520enhancement%2520using%2520paired%2520high-resolution%2520RGB%2520images%2520offers%2520a%250Acost-effective%2520solution%2520for%2520improving%2520low-resolution%2520depth%2520data%2520from%250Alightweight%2520ToF%2520sensors.%2520Nevertheless%252C%2520naively%2520adopting%2520a%2520depth%2520estimation%250Apipeline%2520to%2520fuse%2520the%2520two%2520modalities%2520requires%2520groundtruth%2520depth%2520maps%2520for%250Asupervision.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520self-supervised%2520learning%2520framework%252C%250ASelfToF%252C%2520which%2520generates%2520detailed%2520and%2520scale-aware%2520depth%2520maps.%2520Starting%2520from%2520an%250Aimage-based%2520self-supervised%2520depth%2520estimation%2520pipeline%252C%2520we%2520add%2520low-resolution%250Adepth%2520as%2520inputs%252C%2520design%2520a%2520new%2520depth%2520consistency%2520loss%252C%2520propose%2520a%2520scale-recovery%250Amodule%252C%2520and%2520finally%2520obtain%2520a%2520large%2520performance%2520boost.%2520Furthermore%252C%2520since%2520the%250AToF%2520signal%2520sparsity%2520varies%2520in%2520real-world%2520applications%252C%2520we%2520upgrade%2520SelfToF%2520to%250ASelfToF%252A%2520with%2520submanifold%2520convolution%2520and%2520guided%2520feature%2520fusion.%2520Consequently%252C%250ASelfToF%252A%2520maintain%2520robust%2520performance%2520across%2520varying%2520sparsity%2520levels%2520in%2520ToF%250Adata.%2520Overall%252C%2520our%2520proposed%2520method%2520is%2520both%2520efficient%2520and%2520effective%252C%2520as%2520verified%250Aby%2520extensive%2520experiments%2520on%2520the%2520NYU%2520and%2520ScanNet%2520datasets.%2520The%2520code%2520will%2520be%2520made%250Apublic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Enhancement%20for%20Depth%20from%20a%20Lightweight%20ToF%20Sensor%20with%0A%20%20Monocular%20Images&entry.906535625=Laiyan%20Ding%20and%20Hualie%20Jiang%20and%20Jiwei%20Chen%20and%20Rui%20Huang&entry.1292438233=%20%20Depth%20map%20enhancement%20using%20paired%20high-resolution%20RGB%20images%20offers%20a%0Acost-effective%20solution%20for%20improving%20low-resolution%20depth%20data%20from%0Alightweight%20ToF%20sensors.%20Nevertheless%2C%20naively%20adopting%20a%20depth%20estimation%0Apipeline%20to%20fuse%20the%20two%20modalities%20requires%20groundtruth%20depth%20maps%20for%0Asupervision.%20To%20address%20this%2C%20we%20propose%20a%20self-supervised%20learning%20framework%2C%0ASelfToF%2C%20which%20generates%20detailed%20and%20scale-aware%20depth%20maps.%20Starting%20from%20an%0Aimage-based%20self-supervised%20depth%20estimation%20pipeline%2C%20we%20add%20low-resolution%0Adepth%20as%20inputs%2C%20design%20a%20new%20depth%20consistency%20loss%2C%20propose%20a%20scale-recovery%0Amodule%2C%20and%20finally%20obtain%20a%20large%20performance%20boost.%20Furthermore%2C%20since%20the%0AToF%20signal%20sparsity%20varies%20in%20real-world%20applications%2C%20we%20upgrade%20SelfToF%20to%0ASelfToF%2A%20with%20submanifold%20convolution%20and%20guided%20feature%20fusion.%20Consequently%2C%0ASelfToF%2A%20maintain%20robust%20performance%20across%20varying%20sparsity%20levels%20in%20ToF%0Adata.%20Overall%2C%20our%20proposed%20method%20is%20both%20efficient%20and%20effective%2C%20as%20verified%0Aby%20extensive%20experiments%20on%20the%20NYU%20and%20ScanNet%20datasets.%20The%20code%20will%20be%20made%0Apublic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13444v1&entry.124074799=Read"},
{"title": "Fast Second-Order Online Kernel Learning through Incremental Matrix\n  Sketching and Decomposition", "author": "Dongxie Wen and Xiao Zhang and Zhewei Wei and Chenping Hou and Shuai Li and Weinan Zhang", "abstract": "  Online Kernel Learning (OKL) has attracted considerable research interest due\nto its promising predictive performance in streaming environments. Second-order\napproaches are particularly appealing for OKL as they often offer substantial\nimprovements in regret guarantees. However, existing second-order OKL\napproaches suffer from at least quadratic time complexity with respect to the\npre-set budget, rendering them unsuitable for meeting the real-time demands of\nlarge-scale streaming recommender systems. The singular value decomposition\nrequired to obtain explicit feature mapping is also computationally expensive\ndue to the complete decomposition process. Moreover, the absence of incremental\nupdates to manage approximate kernel space causes these algorithms to perform\npoorly in adversarial environments and real-world streaming recommendation\ndatasets. To address these issues, we propose FORKS, a fast incremental matrix\nsketching and decomposition approach tailored for second-order OKL. FORKS\nconstructs an incremental maintenance paradigm for second-order kernelized\ngradient descent, which includes incremental matrix sketching for kernel\napproximation and incremental matrix decomposition for explicit feature mapping\nconstruction. Theoretical analysis demonstrates that FORKS achieves a\nlogarithmic regret guarantee on par with other second-order approaches while\nmaintaining a linear time complexity w.r.t. the budget, significantly enhancing\nefficiency over existing approaches. We validate the performance of FORKS\nthrough extensive experiments conducted on real-world streaming recommendation\ndatasets, demonstrating its superior scalability and robustness against\nadversarial attacks.\n", "link": "http://arxiv.org/abs/2410.11188v2", "date": "2025-06-16", "relevancy": 2.2182, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4481}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4434}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Second-Order%20Online%20Kernel%20Learning%20through%20Incremental%20Matrix%0A%20%20Sketching%20and%20Decomposition&body=Title%3A%20Fast%20Second-Order%20Online%20Kernel%20Learning%20through%20Incremental%20Matrix%0A%20%20Sketching%20and%20Decomposition%0AAuthor%3A%20Dongxie%20Wen%20and%20Xiao%20Zhang%20and%20Zhewei%20Wei%20and%20Chenping%20Hou%20and%20Shuai%20Li%20and%20Weinan%20Zhang%0AAbstract%3A%20%20%20Online%20Kernel%20Learning%20%28OKL%29%20has%20attracted%20considerable%20research%20interest%20due%0Ato%20its%20promising%20predictive%20performance%20in%20streaming%20environments.%20Second-order%0Aapproaches%20are%20particularly%20appealing%20for%20OKL%20as%20they%20often%20offer%20substantial%0Aimprovements%20in%20regret%20guarantees.%20However%2C%20existing%20second-order%20OKL%0Aapproaches%20suffer%20from%20at%20least%20quadratic%20time%20complexity%20with%20respect%20to%20the%0Apre-set%20budget%2C%20rendering%20them%20unsuitable%20for%20meeting%20the%20real-time%20demands%20of%0Alarge-scale%20streaming%20recommender%20systems.%20The%20singular%20value%20decomposition%0Arequired%20to%20obtain%20explicit%20feature%20mapping%20is%20also%20computationally%20expensive%0Adue%20to%20the%20complete%20decomposition%20process.%20Moreover%2C%20the%20absence%20of%20incremental%0Aupdates%20to%20manage%20approximate%20kernel%20space%20causes%20these%20algorithms%20to%20perform%0Apoorly%20in%20adversarial%20environments%20and%20real-world%20streaming%20recommendation%0Adatasets.%20To%20address%20these%20issues%2C%20we%20propose%20FORKS%2C%20a%20fast%20incremental%20matrix%0Asketching%20and%20decomposition%20approach%20tailored%20for%20second-order%20OKL.%20FORKS%0Aconstructs%20an%20incremental%20maintenance%20paradigm%20for%20second-order%20kernelized%0Agradient%20descent%2C%20which%20includes%20incremental%20matrix%20sketching%20for%20kernel%0Aapproximation%20and%20incremental%20matrix%20decomposition%20for%20explicit%20feature%20mapping%0Aconstruction.%20Theoretical%20analysis%20demonstrates%20that%20FORKS%20achieves%20a%0Alogarithmic%20regret%20guarantee%20on%20par%20with%20other%20second-order%20approaches%20while%0Amaintaining%20a%20linear%20time%20complexity%20w.r.t.%20the%20budget%2C%20significantly%20enhancing%0Aefficiency%20over%20existing%20approaches.%20We%20validate%20the%20performance%20of%20FORKS%0Athrough%20extensive%20experiments%20conducted%20on%20real-world%20streaming%20recommendation%0Adatasets%2C%20demonstrating%20its%20superior%20scalability%20and%20robustness%20against%0Aadversarial%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11188v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Second-Order%2520Online%2520Kernel%2520Learning%2520through%2520Incremental%2520Matrix%250A%2520%2520Sketching%2520and%2520Decomposition%26entry.906535625%3DDongxie%2520Wen%2520and%2520Xiao%2520Zhang%2520and%2520Zhewei%2520Wei%2520and%2520Chenping%2520Hou%2520and%2520Shuai%2520Li%2520and%2520Weinan%2520Zhang%26entry.1292438233%3D%2520%2520Online%2520Kernel%2520Learning%2520%2528OKL%2529%2520has%2520attracted%2520considerable%2520research%2520interest%2520due%250Ato%2520its%2520promising%2520predictive%2520performance%2520in%2520streaming%2520environments.%2520Second-order%250Aapproaches%2520are%2520particularly%2520appealing%2520for%2520OKL%2520as%2520they%2520often%2520offer%2520substantial%250Aimprovements%2520in%2520regret%2520guarantees.%2520However%252C%2520existing%2520second-order%2520OKL%250Aapproaches%2520suffer%2520from%2520at%2520least%2520quadratic%2520time%2520complexity%2520with%2520respect%2520to%2520the%250Apre-set%2520budget%252C%2520rendering%2520them%2520unsuitable%2520for%2520meeting%2520the%2520real-time%2520demands%2520of%250Alarge-scale%2520streaming%2520recommender%2520systems.%2520The%2520singular%2520value%2520decomposition%250Arequired%2520to%2520obtain%2520explicit%2520feature%2520mapping%2520is%2520also%2520computationally%2520expensive%250Adue%2520to%2520the%2520complete%2520decomposition%2520process.%2520Moreover%252C%2520the%2520absence%2520of%2520incremental%250Aupdates%2520to%2520manage%2520approximate%2520kernel%2520space%2520causes%2520these%2520algorithms%2520to%2520perform%250Apoorly%2520in%2520adversarial%2520environments%2520and%2520real-world%2520streaming%2520recommendation%250Adatasets.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520FORKS%252C%2520a%2520fast%2520incremental%2520matrix%250Asketching%2520and%2520decomposition%2520approach%2520tailored%2520for%2520second-order%2520OKL.%2520FORKS%250Aconstructs%2520an%2520incremental%2520maintenance%2520paradigm%2520for%2520second-order%2520kernelized%250Agradient%2520descent%252C%2520which%2520includes%2520incremental%2520matrix%2520sketching%2520for%2520kernel%250Aapproximation%2520and%2520incremental%2520matrix%2520decomposition%2520for%2520explicit%2520feature%2520mapping%250Aconstruction.%2520Theoretical%2520analysis%2520demonstrates%2520that%2520FORKS%2520achieves%2520a%250Alogarithmic%2520regret%2520guarantee%2520on%2520par%2520with%2520other%2520second-order%2520approaches%2520while%250Amaintaining%2520a%2520linear%2520time%2520complexity%2520w.r.t.%2520the%2520budget%252C%2520significantly%2520enhancing%250Aefficiency%2520over%2520existing%2520approaches.%2520We%2520validate%2520the%2520performance%2520of%2520FORKS%250Athrough%2520extensive%2520experiments%2520conducted%2520on%2520real-world%2520streaming%2520recommendation%250Adatasets%252C%2520demonstrating%2520its%2520superior%2520scalability%2520and%2520robustness%2520against%250Aadversarial%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11188v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Second-Order%20Online%20Kernel%20Learning%20through%20Incremental%20Matrix%0A%20%20Sketching%20and%20Decomposition&entry.906535625=Dongxie%20Wen%20and%20Xiao%20Zhang%20and%20Zhewei%20Wei%20and%20Chenping%20Hou%20and%20Shuai%20Li%20and%20Weinan%20Zhang&entry.1292438233=%20%20Online%20Kernel%20Learning%20%28OKL%29%20has%20attracted%20considerable%20research%20interest%20due%0Ato%20its%20promising%20predictive%20performance%20in%20streaming%20environments.%20Second-order%0Aapproaches%20are%20particularly%20appealing%20for%20OKL%20as%20they%20often%20offer%20substantial%0Aimprovements%20in%20regret%20guarantees.%20However%2C%20existing%20second-order%20OKL%0Aapproaches%20suffer%20from%20at%20least%20quadratic%20time%20complexity%20with%20respect%20to%20the%0Apre-set%20budget%2C%20rendering%20them%20unsuitable%20for%20meeting%20the%20real-time%20demands%20of%0Alarge-scale%20streaming%20recommender%20systems.%20The%20singular%20value%20decomposition%0Arequired%20to%20obtain%20explicit%20feature%20mapping%20is%20also%20computationally%20expensive%0Adue%20to%20the%20complete%20decomposition%20process.%20Moreover%2C%20the%20absence%20of%20incremental%0Aupdates%20to%20manage%20approximate%20kernel%20space%20causes%20these%20algorithms%20to%20perform%0Apoorly%20in%20adversarial%20environments%20and%20real-world%20streaming%20recommendation%0Adatasets.%20To%20address%20these%20issues%2C%20we%20propose%20FORKS%2C%20a%20fast%20incremental%20matrix%0Asketching%20and%20decomposition%20approach%20tailored%20for%20second-order%20OKL.%20FORKS%0Aconstructs%20an%20incremental%20maintenance%20paradigm%20for%20second-order%20kernelized%0Agradient%20descent%2C%20which%20includes%20incremental%20matrix%20sketching%20for%20kernel%0Aapproximation%20and%20incremental%20matrix%20decomposition%20for%20explicit%20feature%20mapping%0Aconstruction.%20Theoretical%20analysis%20demonstrates%20that%20FORKS%20achieves%20a%0Alogarithmic%20regret%20guarantee%20on%20par%20with%20other%20second-order%20approaches%20while%0Amaintaining%20a%20linear%20time%20complexity%20w.r.t.%20the%20budget%2C%20significantly%20enhancing%0Aefficiency%20over%20existing%20approaches.%20We%20validate%20the%20performance%20of%20FORKS%0Athrough%20extensive%20experiments%20conducted%20on%20real-world%20streaming%20recommendation%0Adatasets%2C%20demonstrating%20its%20superior%20scalability%20and%20robustness%20against%0Aadversarial%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11188v2&entry.124074799=Read"},
{"title": "ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently\n  Compressing Large Language Models", "author": "Junho Yoon and Geom Lee and Donghyeon Jeon and Inho Kang and Seung-Hoon Na", "abstract": "  Quantization has been widely studied as an effective technique for reducing\nthe memory requirement of large language models (LLMs), potentially improving\nthe latency time as well. Utilizing the characteristic of rotational invariance\nof transformer, we propose the rotation-based saliency-aware weight\nquantization (ROSAQ), which identifies salient channels in the projection\nfeature space, not in the original feature space, where the projected\n\"principal\" dimensions are naturally considered as \"salient\" features. The\nproposed ROSAQ consists of 1) PCA-based projection, which first performs\nprincipal component analysis (PCA) on a calibration set and transforms via the\nPCA projection, 2) Salient channel dentification, which selects dimensions\ncorresponding to the K-largest eigenvalues as salient channels, and 3)\nSaliency-aware quantization with mixed-precision, which uses FP16 for salient\ndimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ\nshows improvements over the baseline saliency-aware quantization on the\noriginal feature space and other existing quantization methods. With kernel\nfusion, ROSAQ presents about 2.3x speed up over FP16 implementation in\ngenerating 256 tokens with a batch size of 64.\n", "link": "http://arxiv.org/abs/2506.13472v1", "date": "2025-06-16", "relevancy": 2.2108, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5655}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5574}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ROSAQ%3A%20Rotation-based%20Saliency-Aware%20Weight%20Quantization%20for%20Efficiently%0A%20%20Compressing%20Large%20Language%20Models&body=Title%3A%20ROSAQ%3A%20Rotation-based%20Saliency-Aware%20Weight%20Quantization%20for%20Efficiently%0A%20%20Compressing%20Large%20Language%20Models%0AAuthor%3A%20Junho%20Yoon%20and%20Geom%20Lee%20and%20Donghyeon%20Jeon%20and%20Inho%20Kang%20and%20Seung-Hoon%20Na%0AAbstract%3A%20%20%20Quantization%20has%20been%20widely%20studied%20as%20an%20effective%20technique%20for%20reducing%0Athe%20memory%20requirement%20of%20large%20language%20models%20%28LLMs%29%2C%20potentially%20improving%0Athe%20latency%20time%20as%20well.%20Utilizing%20the%20characteristic%20of%20rotational%20invariance%0Aof%20transformer%2C%20we%20propose%20the%20rotation-based%20saliency-aware%20weight%0Aquantization%20%28ROSAQ%29%2C%20which%20identifies%20salient%20channels%20in%20the%20projection%0Afeature%20space%2C%20not%20in%20the%20original%20feature%20space%2C%20where%20the%20projected%0A%22principal%22%20dimensions%20are%20naturally%20considered%20as%20%22salient%22%20features.%20The%0Aproposed%20ROSAQ%20consists%20of%201%29%20PCA-based%20projection%2C%20which%20first%20performs%0Aprincipal%20component%20analysis%20%28PCA%29%20on%20a%20calibration%20set%20and%20transforms%20via%20the%0APCA%20projection%2C%202%29%20Salient%20channel%20dentification%2C%20which%20selects%20dimensions%0Acorresponding%20to%20the%20K-largest%20eigenvalues%20as%20salient%20channels%2C%20and%203%29%0ASaliency-aware%20quantization%20with%20mixed-precision%2C%20which%20uses%20FP16%20for%20salient%0Adimensions%20and%20INT3/4%20for%20other%20dimensions.%20Experiment%20results%20show%20that%20ROSAQ%0Ashows%20improvements%20over%20the%20baseline%20saliency-aware%20quantization%20on%20the%0Aoriginal%20feature%20space%20and%20other%20existing%20quantization%20methods.%20With%20kernel%0Afusion%2C%20ROSAQ%20presents%20about%202.3x%20speed%20up%20over%20FP16%20implementation%20in%0Agenerating%20256%20tokens%20with%20a%20batch%20size%20of%2064.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DROSAQ%253A%2520Rotation-based%2520Saliency-Aware%2520Weight%2520Quantization%2520for%2520Efficiently%250A%2520%2520Compressing%2520Large%2520Language%2520Models%26entry.906535625%3DJunho%2520Yoon%2520and%2520Geom%2520Lee%2520and%2520Donghyeon%2520Jeon%2520and%2520Inho%2520Kang%2520and%2520Seung-Hoon%2520Na%26entry.1292438233%3D%2520%2520Quantization%2520has%2520been%2520widely%2520studied%2520as%2520an%2520effective%2520technique%2520for%2520reducing%250Athe%2520memory%2520requirement%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520potentially%2520improving%250Athe%2520latency%2520time%2520as%2520well.%2520Utilizing%2520the%2520characteristic%2520of%2520rotational%2520invariance%250Aof%2520transformer%252C%2520we%2520propose%2520the%2520rotation-based%2520saliency-aware%2520weight%250Aquantization%2520%2528ROSAQ%2529%252C%2520which%2520identifies%2520salient%2520channels%2520in%2520the%2520projection%250Afeature%2520space%252C%2520not%2520in%2520the%2520original%2520feature%2520space%252C%2520where%2520the%2520projected%250A%2522principal%2522%2520dimensions%2520are%2520naturally%2520considered%2520as%2520%2522salient%2522%2520features.%2520The%250Aproposed%2520ROSAQ%2520consists%2520of%25201%2529%2520PCA-based%2520projection%252C%2520which%2520first%2520performs%250Aprincipal%2520component%2520analysis%2520%2528PCA%2529%2520on%2520a%2520calibration%2520set%2520and%2520transforms%2520via%2520the%250APCA%2520projection%252C%25202%2529%2520Salient%2520channel%2520dentification%252C%2520which%2520selects%2520dimensions%250Acorresponding%2520to%2520the%2520K-largest%2520eigenvalues%2520as%2520salient%2520channels%252C%2520and%25203%2529%250ASaliency-aware%2520quantization%2520with%2520mixed-precision%252C%2520which%2520uses%2520FP16%2520for%2520salient%250Adimensions%2520and%2520INT3/4%2520for%2520other%2520dimensions.%2520Experiment%2520results%2520show%2520that%2520ROSAQ%250Ashows%2520improvements%2520over%2520the%2520baseline%2520saliency-aware%2520quantization%2520on%2520the%250Aoriginal%2520feature%2520space%2520and%2520other%2520existing%2520quantization%2520methods.%2520With%2520kernel%250Afusion%252C%2520ROSAQ%2520presents%2520about%25202.3x%2520speed%2520up%2520over%2520FP16%2520implementation%2520in%250Agenerating%2520256%2520tokens%2520with%2520a%2520batch%2520size%2520of%252064.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ROSAQ%3A%20Rotation-based%20Saliency-Aware%20Weight%20Quantization%20for%20Efficiently%0A%20%20Compressing%20Large%20Language%20Models&entry.906535625=Junho%20Yoon%20and%20Geom%20Lee%20and%20Donghyeon%20Jeon%20and%20Inho%20Kang%20and%20Seung-Hoon%20Na&entry.1292438233=%20%20Quantization%20has%20been%20widely%20studied%20as%20an%20effective%20technique%20for%20reducing%0Athe%20memory%20requirement%20of%20large%20language%20models%20%28LLMs%29%2C%20potentially%20improving%0Athe%20latency%20time%20as%20well.%20Utilizing%20the%20characteristic%20of%20rotational%20invariance%0Aof%20transformer%2C%20we%20propose%20the%20rotation-based%20saliency-aware%20weight%0Aquantization%20%28ROSAQ%29%2C%20which%20identifies%20salient%20channels%20in%20the%20projection%0Afeature%20space%2C%20not%20in%20the%20original%20feature%20space%2C%20where%20the%20projected%0A%22principal%22%20dimensions%20are%20naturally%20considered%20as%20%22salient%22%20features.%20The%0Aproposed%20ROSAQ%20consists%20of%201%29%20PCA-based%20projection%2C%20which%20first%20performs%0Aprincipal%20component%20analysis%20%28PCA%29%20on%20a%20calibration%20set%20and%20transforms%20via%20the%0APCA%20projection%2C%202%29%20Salient%20channel%20dentification%2C%20which%20selects%20dimensions%0Acorresponding%20to%20the%20K-largest%20eigenvalues%20as%20salient%20channels%2C%20and%203%29%0ASaliency-aware%20quantization%20with%20mixed-precision%2C%20which%20uses%20FP16%20for%20salient%0Adimensions%20and%20INT3/4%20for%20other%20dimensions.%20Experiment%20results%20show%20that%20ROSAQ%0Ashows%20improvements%20over%20the%20baseline%20saliency-aware%20quantization%20on%20the%0Aoriginal%20feature%20space%20and%20other%20existing%20quantization%20methods.%20With%20kernel%0Afusion%2C%20ROSAQ%20presents%20about%202.3x%20speed%20up%20over%20FP16%20implementation%20in%0Agenerating%20256%20tokens%20with%20a%20batch%20size%20of%2064.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13472v1&entry.124074799=Read"},
{"title": "EBS-CFL: Efficient and Byzantine-robust Secure Clustered Federated\n  Learning", "author": "Zhiqiang Li and Haiyong Bao and Menghong Guan and Hao Pan and Cheng Huang and Hong-Ning Dai", "abstract": "  Despite federated learning (FL)'s potential in collaborative learning, its\nperformance has deteriorated due to the data heterogeneity of distributed\nusers. Recently, clustered federated learning (CFL) has emerged to address this\nchallenge by partitioning users into clusters according to their similarity.\nHowever, CFL faces difficulties in training when users are unwilling to share\ntheir cluster identities due to privacy concerns. To address these issues, we\npresent an innovative Efficient and Robust Secure Aggregation scheme for CFL,\ndubbed EBS-CFL. The proposed EBS-CFL supports effectively training CFL while\nmaintaining users' cluster identity confidentially. Moreover, it detects\npotential poisonous attacks without compromising individual client gradients by\ndiscarding negatively correlated gradients and aggregating positively\ncorrelated ones using a weighted approach. The server also authenticates\ncorrect gradient encoding by clients. EBS-CFL has high efficiency with\nclient-side overhead O(ml + m^2) for communication and O(m^2l) for computation,\nwhere m is the number of cluster identities, and l is the gradient size. When m\n= 1, EBS-CFL's computational efficiency of client is at least O(log n) times\nbetter than comparison schemes, where n is the number of clients.In addition,\nwe validate the scheme through extensive experiments. Finally, we theoretically\nprove the scheme's security.\n", "link": "http://arxiv.org/abs/2506.13612v1", "date": "2025-06-16", "relevancy": 2.2087, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4509}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4382}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EBS-CFL%3A%20Efficient%20and%20Byzantine-robust%20Secure%20Clustered%20Federated%0A%20%20Learning&body=Title%3A%20EBS-CFL%3A%20Efficient%20and%20Byzantine-robust%20Secure%20Clustered%20Federated%0A%20%20Learning%0AAuthor%3A%20Zhiqiang%20Li%20and%20Haiyong%20Bao%20and%20Menghong%20Guan%20and%20Hao%20Pan%20and%20Cheng%20Huang%20and%20Hong-Ning%20Dai%0AAbstract%3A%20%20%20Despite%20federated%20learning%20%28FL%29%27s%20potential%20in%20collaborative%20learning%2C%20its%0Aperformance%20has%20deteriorated%20due%20to%20the%20data%20heterogeneity%20of%20distributed%0Ausers.%20Recently%2C%20clustered%20federated%20learning%20%28CFL%29%20has%20emerged%20to%20address%20this%0Achallenge%20by%20partitioning%20users%20into%20clusters%20according%20to%20their%20similarity.%0AHowever%2C%20CFL%20faces%20difficulties%20in%20training%20when%20users%20are%20unwilling%20to%20share%0Atheir%20cluster%20identities%20due%20to%20privacy%20concerns.%20To%20address%20these%20issues%2C%20we%0Apresent%20an%20innovative%20Efficient%20and%20Robust%20Secure%20Aggregation%20scheme%20for%20CFL%2C%0Adubbed%20EBS-CFL.%20The%20proposed%20EBS-CFL%20supports%20effectively%20training%20CFL%20while%0Amaintaining%20users%27%20cluster%20identity%20confidentially.%20Moreover%2C%20it%20detects%0Apotential%20poisonous%20attacks%20without%20compromising%20individual%20client%20gradients%20by%0Adiscarding%20negatively%20correlated%20gradients%20and%20aggregating%20positively%0Acorrelated%20ones%20using%20a%20weighted%20approach.%20The%20server%20also%20authenticates%0Acorrect%20gradient%20encoding%20by%20clients.%20EBS-CFL%20has%20high%20efficiency%20with%0Aclient-side%20overhead%20O%28ml%20%2B%20m%5E2%29%20for%20communication%20and%20O%28m%5E2l%29%20for%20computation%2C%0Awhere%20m%20is%20the%20number%20of%20cluster%20identities%2C%20and%20l%20is%20the%20gradient%20size.%20When%20m%0A%3D%201%2C%20EBS-CFL%27s%20computational%20efficiency%20of%20client%20is%20at%20least%20O%28log%20n%29%20times%0Abetter%20than%20comparison%20schemes%2C%20where%20n%20is%20the%20number%20of%20clients.In%20addition%2C%0Awe%20validate%20the%20scheme%20through%20extensive%20experiments.%20Finally%2C%20we%20theoretically%0Aprove%20the%20scheme%27s%20security.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEBS-CFL%253A%2520Efficient%2520and%2520Byzantine-robust%2520Secure%2520Clustered%2520Federated%250A%2520%2520Learning%26entry.906535625%3DZhiqiang%2520Li%2520and%2520Haiyong%2520Bao%2520and%2520Menghong%2520Guan%2520and%2520Hao%2520Pan%2520and%2520Cheng%2520Huang%2520and%2520Hong-Ning%2520Dai%26entry.1292438233%3D%2520%2520Despite%2520federated%2520learning%2520%2528FL%2529%2527s%2520potential%2520in%2520collaborative%2520learning%252C%2520its%250Aperformance%2520has%2520deteriorated%2520due%2520to%2520the%2520data%2520heterogeneity%2520of%2520distributed%250Ausers.%2520Recently%252C%2520clustered%2520federated%2520learning%2520%2528CFL%2529%2520has%2520emerged%2520to%2520address%2520this%250Achallenge%2520by%2520partitioning%2520users%2520into%2520clusters%2520according%2520to%2520their%2520similarity.%250AHowever%252C%2520CFL%2520faces%2520difficulties%2520in%2520training%2520when%2520users%2520are%2520unwilling%2520to%2520share%250Atheir%2520cluster%2520identities%2520due%2520to%2520privacy%2520concerns.%2520To%2520address%2520these%2520issues%252C%2520we%250Apresent%2520an%2520innovative%2520Efficient%2520and%2520Robust%2520Secure%2520Aggregation%2520scheme%2520for%2520CFL%252C%250Adubbed%2520EBS-CFL.%2520The%2520proposed%2520EBS-CFL%2520supports%2520effectively%2520training%2520CFL%2520while%250Amaintaining%2520users%2527%2520cluster%2520identity%2520confidentially.%2520Moreover%252C%2520it%2520detects%250Apotential%2520poisonous%2520attacks%2520without%2520compromising%2520individual%2520client%2520gradients%2520by%250Adiscarding%2520negatively%2520correlated%2520gradients%2520and%2520aggregating%2520positively%250Acorrelated%2520ones%2520using%2520a%2520weighted%2520approach.%2520The%2520server%2520also%2520authenticates%250Acorrect%2520gradient%2520encoding%2520by%2520clients.%2520EBS-CFL%2520has%2520high%2520efficiency%2520with%250Aclient-side%2520overhead%2520O%2528ml%2520%252B%2520m%255E2%2529%2520for%2520communication%2520and%2520O%2528m%255E2l%2529%2520for%2520computation%252C%250Awhere%2520m%2520is%2520the%2520number%2520of%2520cluster%2520identities%252C%2520and%2520l%2520is%2520the%2520gradient%2520size.%2520When%2520m%250A%253D%25201%252C%2520EBS-CFL%2527s%2520computational%2520efficiency%2520of%2520client%2520is%2520at%2520least%2520O%2528log%2520n%2529%2520times%250Abetter%2520than%2520comparison%2520schemes%252C%2520where%2520n%2520is%2520the%2520number%2520of%2520clients.In%2520addition%252C%250Awe%2520validate%2520the%2520scheme%2520through%2520extensive%2520experiments.%2520Finally%252C%2520we%2520theoretically%250Aprove%2520the%2520scheme%2527s%2520security.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EBS-CFL%3A%20Efficient%20and%20Byzantine-robust%20Secure%20Clustered%20Federated%0A%20%20Learning&entry.906535625=Zhiqiang%20Li%20and%20Haiyong%20Bao%20and%20Menghong%20Guan%20and%20Hao%20Pan%20and%20Cheng%20Huang%20and%20Hong-Ning%20Dai&entry.1292438233=%20%20Despite%20federated%20learning%20%28FL%29%27s%20potential%20in%20collaborative%20learning%2C%20its%0Aperformance%20has%20deteriorated%20due%20to%20the%20data%20heterogeneity%20of%20distributed%0Ausers.%20Recently%2C%20clustered%20federated%20learning%20%28CFL%29%20has%20emerged%20to%20address%20this%0Achallenge%20by%20partitioning%20users%20into%20clusters%20according%20to%20their%20similarity.%0AHowever%2C%20CFL%20faces%20difficulties%20in%20training%20when%20users%20are%20unwilling%20to%20share%0Atheir%20cluster%20identities%20due%20to%20privacy%20concerns.%20To%20address%20these%20issues%2C%20we%0Apresent%20an%20innovative%20Efficient%20and%20Robust%20Secure%20Aggregation%20scheme%20for%20CFL%2C%0Adubbed%20EBS-CFL.%20The%20proposed%20EBS-CFL%20supports%20effectively%20training%20CFL%20while%0Amaintaining%20users%27%20cluster%20identity%20confidentially.%20Moreover%2C%20it%20detects%0Apotential%20poisonous%20attacks%20without%20compromising%20individual%20client%20gradients%20by%0Adiscarding%20negatively%20correlated%20gradients%20and%20aggregating%20positively%0Acorrelated%20ones%20using%20a%20weighted%20approach.%20The%20server%20also%20authenticates%0Acorrect%20gradient%20encoding%20by%20clients.%20EBS-CFL%20has%20high%20efficiency%20with%0Aclient-side%20overhead%20O%28ml%20%2B%20m%5E2%29%20for%20communication%20and%20O%28m%5E2l%29%20for%20computation%2C%0Awhere%20m%20is%20the%20number%20of%20cluster%20identities%2C%20and%20l%20is%20the%20gradient%20size.%20When%20m%0A%3D%201%2C%20EBS-CFL%27s%20computational%20efficiency%20of%20client%20is%20at%20least%20O%28log%20n%29%20times%0Abetter%20than%20comparison%20schemes%2C%20where%20n%20is%20the%20number%20of%20clients.In%20addition%2C%0Awe%20validate%20the%20scheme%20through%20extensive%20experiments.%20Finally%2C%20we%20theoretically%0Aprove%20the%20scheme%27s%20security.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13612v1&entry.124074799=Read"},
{"title": "HARMONI: Haptic-Guided Assistance for Unified Robotic Tele-Manipulation\n  and Tele-Navigation", "author": "V. Sripada and A. Khan and J. F\u00f6cker and S. Parsa and Susmitha P and H Maior and A. Ghalamzan-E", "abstract": "  Shared control, which combines human expertise with autonomous assistance, is\ncritical for effective teleoperation in complex environments. While recent\nadvances in haptic-guided teleoperation have shown promise, they are often\nlimited to simplified tasks involving 6- or 7-DoF manipulators and rely on\nseparate control strategies for navigation and manipulation. This increases\nboth cognitive load and operational overhead. In this paper, we present a\nunified tele-mobile manipulation framework that leverages haptic-guided shared\ncontrol. The system integrates a 9-DoF follower mobile manipulator and a 7-DoF\nleader robotic arm, enabling seamless transitions between tele-navigation and\ntele-manipulation through real-time haptic feedback. A user study with 20\nparticipants under real-world conditions demonstrates that our framework\nsignificantly improves task accuracy and efficiency without increasing\ncognitive load. These findings highlight the potential of haptic-guided shared\ncontrol for enhancing operator performance in demanding teleoperation\nscenarios.\n", "link": "http://arxiv.org/abs/2506.13704v1", "date": "2025-06-16", "relevancy": 2.2036, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5865}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5279}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HARMONI%3A%20Haptic-Guided%20Assistance%20for%20Unified%20Robotic%20Tele-Manipulation%0A%20%20and%20Tele-Navigation&body=Title%3A%20HARMONI%3A%20Haptic-Guided%20Assistance%20for%20Unified%20Robotic%20Tele-Manipulation%0A%20%20and%20Tele-Navigation%0AAuthor%3A%20V.%20Sripada%20and%20A.%20Khan%20and%20J.%20F%C3%B6cker%20and%20S.%20Parsa%20and%20Susmitha%20P%20and%20H%20Maior%20and%20A.%20Ghalamzan-E%0AAbstract%3A%20%20%20Shared%20control%2C%20which%20combines%20human%20expertise%20with%20autonomous%20assistance%2C%20is%0Acritical%20for%20effective%20teleoperation%20in%20complex%20environments.%20While%20recent%0Aadvances%20in%20haptic-guided%20teleoperation%20have%20shown%20promise%2C%20they%20are%20often%0Alimited%20to%20simplified%20tasks%20involving%206-%20or%207-DoF%20manipulators%20and%20rely%20on%0Aseparate%20control%20strategies%20for%20navigation%20and%20manipulation.%20This%20increases%0Aboth%20cognitive%20load%20and%20operational%20overhead.%20In%20this%20paper%2C%20we%20present%20a%0Aunified%20tele-mobile%20manipulation%20framework%20that%20leverages%20haptic-guided%20shared%0Acontrol.%20The%20system%20integrates%20a%209-DoF%20follower%20mobile%20manipulator%20and%20a%207-DoF%0Aleader%20robotic%20arm%2C%20enabling%20seamless%20transitions%20between%20tele-navigation%20and%0Atele-manipulation%20through%20real-time%20haptic%20feedback.%20A%20user%20study%20with%2020%0Aparticipants%20under%20real-world%20conditions%20demonstrates%20that%20our%20framework%0Asignificantly%20improves%20task%20accuracy%20and%20efficiency%20without%20increasing%0Acognitive%20load.%20These%20findings%20highlight%20the%20potential%20of%20haptic-guided%20shared%0Acontrol%20for%20enhancing%20operator%20performance%20in%20demanding%20teleoperation%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHARMONI%253A%2520Haptic-Guided%2520Assistance%2520for%2520Unified%2520Robotic%2520Tele-Manipulation%250A%2520%2520and%2520Tele-Navigation%26entry.906535625%3DV.%2520Sripada%2520and%2520A.%2520Khan%2520and%2520J.%2520F%25C3%25B6cker%2520and%2520S.%2520Parsa%2520and%2520Susmitha%2520P%2520and%2520H%2520Maior%2520and%2520A.%2520Ghalamzan-E%26entry.1292438233%3D%2520%2520Shared%2520control%252C%2520which%2520combines%2520human%2520expertise%2520with%2520autonomous%2520assistance%252C%2520is%250Acritical%2520for%2520effective%2520teleoperation%2520in%2520complex%2520environments.%2520While%2520recent%250Aadvances%2520in%2520haptic-guided%2520teleoperation%2520have%2520shown%2520promise%252C%2520they%2520are%2520often%250Alimited%2520to%2520simplified%2520tasks%2520involving%25206-%2520or%25207-DoF%2520manipulators%2520and%2520rely%2520on%250Aseparate%2520control%2520strategies%2520for%2520navigation%2520and%2520manipulation.%2520This%2520increases%250Aboth%2520cognitive%2520load%2520and%2520operational%2520overhead.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%250Aunified%2520tele-mobile%2520manipulation%2520framework%2520that%2520leverages%2520haptic-guided%2520shared%250Acontrol.%2520The%2520system%2520integrates%2520a%25209-DoF%2520follower%2520mobile%2520manipulator%2520and%2520a%25207-DoF%250Aleader%2520robotic%2520arm%252C%2520enabling%2520seamless%2520transitions%2520between%2520tele-navigation%2520and%250Atele-manipulation%2520through%2520real-time%2520haptic%2520feedback.%2520A%2520user%2520study%2520with%252020%250Aparticipants%2520under%2520real-world%2520conditions%2520demonstrates%2520that%2520our%2520framework%250Asignificantly%2520improves%2520task%2520accuracy%2520and%2520efficiency%2520without%2520increasing%250Acognitive%2520load.%2520These%2520findings%2520highlight%2520the%2520potential%2520of%2520haptic-guided%2520shared%250Acontrol%2520for%2520enhancing%2520operator%2520performance%2520in%2520demanding%2520teleoperation%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HARMONI%3A%20Haptic-Guided%20Assistance%20for%20Unified%20Robotic%20Tele-Manipulation%0A%20%20and%20Tele-Navigation&entry.906535625=V.%20Sripada%20and%20A.%20Khan%20and%20J.%20F%C3%B6cker%20and%20S.%20Parsa%20and%20Susmitha%20P%20and%20H%20Maior%20and%20A.%20Ghalamzan-E&entry.1292438233=%20%20Shared%20control%2C%20which%20combines%20human%20expertise%20with%20autonomous%20assistance%2C%20is%0Acritical%20for%20effective%20teleoperation%20in%20complex%20environments.%20While%20recent%0Aadvances%20in%20haptic-guided%20teleoperation%20have%20shown%20promise%2C%20they%20are%20often%0Alimited%20to%20simplified%20tasks%20involving%206-%20or%207-DoF%20manipulators%20and%20rely%20on%0Aseparate%20control%20strategies%20for%20navigation%20and%20manipulation.%20This%20increases%0Aboth%20cognitive%20load%20and%20operational%20overhead.%20In%20this%20paper%2C%20we%20present%20a%0Aunified%20tele-mobile%20manipulation%20framework%20that%20leverages%20haptic-guided%20shared%0Acontrol.%20The%20system%20integrates%20a%209-DoF%20follower%20mobile%20manipulator%20and%20a%207-DoF%0Aleader%20robotic%20arm%2C%20enabling%20seamless%20transitions%20between%20tele-navigation%20and%0Atele-manipulation%20through%20real-time%20haptic%20feedback.%20A%20user%20study%20with%2020%0Aparticipants%20under%20real-world%20conditions%20demonstrates%20that%20our%20framework%0Asignificantly%20improves%20task%20accuracy%20and%20efficiency%20without%20increasing%0Acognitive%20load.%20These%20findings%20highlight%20the%20potential%20of%20haptic-guided%20shared%0Acontrol%20for%20enhancing%20operator%20performance%20in%20demanding%20teleoperation%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13704v1&entry.124074799=Read"},
{"title": "Overcoming Occlusions in the Wild: A Multi-Task Age Head Approach to Age\n  Estimation", "author": "Waqar Tanveer and Laura Fern\u00e1ndez-Robles and Eduardo Fidalgo and V\u00edctor Gonz\u00e1lez-Castro and Enrique Alegre", "abstract": "  Facial age estimation has achieved considerable success under controlled\nconditions. However, in unconstrained real-world scenarios, which are often\nreferred to as 'in the wild', age estimation remains challenging, especially\nwhen faces are partially occluded, which may obscure their visibility. To\naddress this limitation, we propose a new approach integrating generative\nadversarial networks (GANs) and transformer architectures to enable robust age\nestimation from occluded faces. We employ an SN-Patch GAN to effectively remove\nocclusions, while an Attentive Residual Convolution Module (ARCM), paired with\na Swin Transformer, enhances feature representation. Additionally, we introduce\na Multi-Task Age Head (MTAH) that combines regression and distribution\nlearning, further improving age estimation under occlusion. Experimental\nresults on the FG-NET, UTKFace, and MORPH datasets demonstrate that our\nproposed approach surpasses existing state-of-the-art techniques for occluded\nfacial age estimation by achieving an MAE of $3.00$, $4.54$, and $2.53$ years,\nrespectively.\n", "link": "http://arxiv.org/abs/2506.13445v1", "date": "2025-06-16", "relevancy": 2.2023, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5658}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5484}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overcoming%20Occlusions%20in%20the%20Wild%3A%20A%20Multi-Task%20Age%20Head%20Approach%20to%20Age%0A%20%20Estimation&body=Title%3A%20Overcoming%20Occlusions%20in%20the%20Wild%3A%20A%20Multi-Task%20Age%20Head%20Approach%20to%20Age%0A%20%20Estimation%0AAuthor%3A%20Waqar%20Tanveer%20and%20Laura%20Fern%C3%A1ndez-Robles%20and%20Eduardo%20Fidalgo%20and%20V%C3%ADctor%20Gonz%C3%A1lez-Castro%20and%20Enrique%20Alegre%0AAbstract%3A%20%20%20Facial%20age%20estimation%20has%20achieved%20considerable%20success%20under%20controlled%0Aconditions.%20However%2C%20in%20unconstrained%20real-world%20scenarios%2C%20which%20are%20often%0Areferred%20to%20as%20%27in%20the%20wild%27%2C%20age%20estimation%20remains%20challenging%2C%20especially%0Awhen%20faces%20are%20partially%20occluded%2C%20which%20may%20obscure%20their%20visibility.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20a%20new%20approach%20integrating%20generative%0Aadversarial%20networks%20%28GANs%29%20and%20transformer%20architectures%20to%20enable%20robust%20age%0Aestimation%20from%20occluded%20faces.%20We%20employ%20an%20SN-Patch%20GAN%20to%20effectively%20remove%0Aocclusions%2C%20while%20an%20Attentive%20Residual%20Convolution%20Module%20%28ARCM%29%2C%20paired%20with%0Aa%20Swin%20Transformer%2C%20enhances%20feature%20representation.%20Additionally%2C%20we%20introduce%0Aa%20Multi-Task%20Age%20Head%20%28MTAH%29%20that%20combines%20regression%20and%20distribution%0Alearning%2C%20further%20improving%20age%20estimation%20under%20occlusion.%20Experimental%0Aresults%20on%20the%20FG-NET%2C%20UTKFace%2C%20and%20MORPH%20datasets%20demonstrate%20that%20our%0Aproposed%20approach%20surpasses%20existing%20state-of-the-art%20techniques%20for%20occluded%0Afacial%20age%20estimation%20by%20achieving%20an%20MAE%20of%20%243.00%24%2C%20%244.54%24%2C%20and%20%242.53%24%20years%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOvercoming%2520Occlusions%2520in%2520the%2520Wild%253A%2520A%2520Multi-Task%2520Age%2520Head%2520Approach%2520to%2520Age%250A%2520%2520Estimation%26entry.906535625%3DWaqar%2520Tanveer%2520and%2520Laura%2520Fern%25C3%25A1ndez-Robles%2520and%2520Eduardo%2520Fidalgo%2520and%2520V%25C3%25ADctor%2520Gonz%25C3%25A1lez-Castro%2520and%2520Enrique%2520Alegre%26entry.1292438233%3D%2520%2520Facial%2520age%2520estimation%2520has%2520achieved%2520considerable%2520success%2520under%2520controlled%250Aconditions.%2520However%252C%2520in%2520unconstrained%2520real-world%2520scenarios%252C%2520which%2520are%2520often%250Areferred%2520to%2520as%2520%2527in%2520the%2520wild%2527%252C%2520age%2520estimation%2520remains%2520challenging%252C%2520especially%250Awhen%2520faces%2520are%2520partially%2520occluded%252C%2520which%2520may%2520obscure%2520their%2520visibility.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520propose%2520a%2520new%2520approach%2520integrating%2520generative%250Aadversarial%2520networks%2520%2528GANs%2529%2520and%2520transformer%2520architectures%2520to%2520enable%2520robust%2520age%250Aestimation%2520from%2520occluded%2520faces.%2520We%2520employ%2520an%2520SN-Patch%2520GAN%2520to%2520effectively%2520remove%250Aocclusions%252C%2520while%2520an%2520Attentive%2520Residual%2520Convolution%2520Module%2520%2528ARCM%2529%252C%2520paired%2520with%250Aa%2520Swin%2520Transformer%252C%2520enhances%2520feature%2520representation.%2520Additionally%252C%2520we%2520introduce%250Aa%2520Multi-Task%2520Age%2520Head%2520%2528MTAH%2529%2520that%2520combines%2520regression%2520and%2520distribution%250Alearning%252C%2520further%2520improving%2520age%2520estimation%2520under%2520occlusion.%2520Experimental%250Aresults%2520on%2520the%2520FG-NET%252C%2520UTKFace%252C%2520and%2520MORPH%2520datasets%2520demonstrate%2520that%2520our%250Aproposed%2520approach%2520surpasses%2520existing%2520state-of-the-art%2520techniques%2520for%2520occluded%250Afacial%2520age%2520estimation%2520by%2520achieving%2520an%2520MAE%2520of%2520%25243.00%2524%252C%2520%25244.54%2524%252C%2520and%2520%25242.53%2524%2520years%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overcoming%20Occlusions%20in%20the%20Wild%3A%20A%20Multi-Task%20Age%20Head%20Approach%20to%20Age%0A%20%20Estimation&entry.906535625=Waqar%20Tanveer%20and%20Laura%20Fern%C3%A1ndez-Robles%20and%20Eduardo%20Fidalgo%20and%20V%C3%ADctor%20Gonz%C3%A1lez-Castro%20and%20Enrique%20Alegre&entry.1292438233=%20%20Facial%20age%20estimation%20has%20achieved%20considerable%20success%20under%20controlled%0Aconditions.%20However%2C%20in%20unconstrained%20real-world%20scenarios%2C%20which%20are%20often%0Areferred%20to%20as%20%27in%20the%20wild%27%2C%20age%20estimation%20remains%20challenging%2C%20especially%0Awhen%20faces%20are%20partially%20occluded%2C%20which%20may%20obscure%20their%20visibility.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20a%20new%20approach%20integrating%20generative%0Aadversarial%20networks%20%28GANs%29%20and%20transformer%20architectures%20to%20enable%20robust%20age%0Aestimation%20from%20occluded%20faces.%20We%20employ%20an%20SN-Patch%20GAN%20to%20effectively%20remove%0Aocclusions%2C%20while%20an%20Attentive%20Residual%20Convolution%20Module%20%28ARCM%29%2C%20paired%20with%0Aa%20Swin%20Transformer%2C%20enhances%20feature%20representation.%20Additionally%2C%20we%20introduce%0Aa%20Multi-Task%20Age%20Head%20%28MTAH%29%20that%20combines%20regression%20and%20distribution%0Alearning%2C%20further%20improving%20age%20estimation%20under%20occlusion.%20Experimental%0Aresults%20on%20the%20FG-NET%2C%20UTKFace%2C%20and%20MORPH%20datasets%20demonstrate%20that%20our%0Aproposed%20approach%20surpasses%20existing%20state-of-the-art%20techniques%20for%20occluded%0Afacial%20age%20estimation%20by%20achieving%20an%20MAE%20of%20%243.00%24%2C%20%244.54%24%2C%20and%20%242.53%24%20years%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13445v1&entry.124074799=Read"},
{"title": "Benchmarking Practices in LLM-driven Offensive Security: Testbeds,\n  Metrics, and Experiment Design", "author": "Andreas Happe and J\u00fcrgen Cito", "abstract": "  Large Language Models (LLMs) have emerged as a powerful approach for driving\noffensive penetration-testing tooling. Due to the opaque nature of LLMs,\nempirical methods are typically used to analyze their efficacy. The quality of\nthis analysis is highly dependent on the chosen testbed, captured metrics and\nanalysis methods employed.\n  This paper analyzes the methodology and benchmarking practices used for\nevaluating Large Language Model (LLM)-driven attacks, focusing on offensive\nuses of LLMs in cybersecurity. We review 19 research papers detailing 18\nprototypes and their respective testbeds.\n  We detail our findings and provide actionable recommendations for future\nresearch, emphasizing the importance of extending existing testbeds, creating\nbaselines, and including comprehensive metrics and qualitative analysis. We\nalso note the distinction between security research and practice, suggesting\nthat CTF-based challenges may not fully represent real-world penetration\ntesting scenarios.\n", "link": "http://arxiv.org/abs/2504.10112v2", "date": "2025-06-16", "relevancy": 2.191, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4437}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4437}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Practices%20in%20LLM-driven%20Offensive%20Security%3A%20Testbeds%2C%0A%20%20Metrics%2C%20and%20Experiment%20Design&body=Title%3A%20Benchmarking%20Practices%20in%20LLM-driven%20Offensive%20Security%3A%20Testbeds%2C%0A%20%20Metrics%2C%20and%20Experiment%20Design%0AAuthor%3A%20Andreas%20Happe%20and%20J%C3%BCrgen%20Cito%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20a%20powerful%20approach%20for%20driving%0Aoffensive%20penetration-testing%20tooling.%20Due%20to%20the%20opaque%20nature%20of%20LLMs%2C%0Aempirical%20methods%20are%20typically%20used%20to%20analyze%20their%20efficacy.%20The%20quality%20of%0Athis%20analysis%20is%20highly%20dependent%20on%20the%20chosen%20testbed%2C%20captured%20metrics%20and%0Aanalysis%20methods%20employed.%0A%20%20This%20paper%20analyzes%20the%20methodology%20and%20benchmarking%20practices%20used%20for%0Aevaluating%20Large%20Language%20Model%20%28LLM%29-driven%20attacks%2C%20focusing%20on%20offensive%0Auses%20of%20LLMs%20in%20cybersecurity.%20We%20review%2019%20research%20papers%20detailing%2018%0Aprototypes%20and%20their%20respective%20testbeds.%0A%20%20We%20detail%20our%20findings%20and%20provide%20actionable%20recommendations%20for%20future%0Aresearch%2C%20emphasizing%20the%20importance%20of%20extending%20existing%20testbeds%2C%20creating%0Abaselines%2C%20and%20including%20comprehensive%20metrics%20and%20qualitative%20analysis.%20We%0Aalso%20note%20the%20distinction%20between%20security%20research%20and%20practice%2C%20suggesting%0Athat%20CTF-based%20challenges%20may%20not%20fully%20represent%20real-world%20penetration%0Atesting%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10112v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Practices%2520in%2520LLM-driven%2520Offensive%2520Security%253A%2520Testbeds%252C%250A%2520%2520Metrics%252C%2520and%2520Experiment%2520Design%26entry.906535625%3DAndreas%2520Happe%2520and%2520J%25C3%25BCrgen%2520Cito%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520approach%2520for%2520driving%250Aoffensive%2520penetration-testing%2520tooling.%2520Due%2520to%2520the%2520opaque%2520nature%2520of%2520LLMs%252C%250Aempirical%2520methods%2520are%2520typically%2520used%2520to%2520analyze%2520their%2520efficacy.%2520The%2520quality%2520of%250Athis%2520analysis%2520is%2520highly%2520dependent%2520on%2520the%2520chosen%2520testbed%252C%2520captured%2520metrics%2520and%250Aanalysis%2520methods%2520employed.%250A%2520%2520This%2520paper%2520analyzes%2520the%2520methodology%2520and%2520benchmarking%2520practices%2520used%2520for%250Aevaluating%2520Large%2520Language%2520Model%2520%2528LLM%2529-driven%2520attacks%252C%2520focusing%2520on%2520offensive%250Auses%2520of%2520LLMs%2520in%2520cybersecurity.%2520We%2520review%252019%2520research%2520papers%2520detailing%252018%250Aprototypes%2520and%2520their%2520respective%2520testbeds.%250A%2520%2520We%2520detail%2520our%2520findings%2520and%2520provide%2520actionable%2520recommendations%2520for%2520future%250Aresearch%252C%2520emphasizing%2520the%2520importance%2520of%2520extending%2520existing%2520testbeds%252C%2520creating%250Abaselines%252C%2520and%2520including%2520comprehensive%2520metrics%2520and%2520qualitative%2520analysis.%2520We%250Aalso%2520note%2520the%2520distinction%2520between%2520security%2520research%2520and%2520practice%252C%2520suggesting%250Athat%2520CTF-based%2520challenges%2520may%2520not%2520fully%2520represent%2520real-world%2520penetration%250Atesting%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10112v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Practices%20in%20LLM-driven%20Offensive%20Security%3A%20Testbeds%2C%0A%20%20Metrics%2C%20and%20Experiment%20Design&entry.906535625=Andreas%20Happe%20and%20J%C3%BCrgen%20Cito&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20a%20powerful%20approach%20for%20driving%0Aoffensive%20penetration-testing%20tooling.%20Due%20to%20the%20opaque%20nature%20of%20LLMs%2C%0Aempirical%20methods%20are%20typically%20used%20to%20analyze%20their%20efficacy.%20The%20quality%20of%0Athis%20analysis%20is%20highly%20dependent%20on%20the%20chosen%20testbed%2C%20captured%20metrics%20and%0Aanalysis%20methods%20employed.%0A%20%20This%20paper%20analyzes%20the%20methodology%20and%20benchmarking%20practices%20used%20for%0Aevaluating%20Large%20Language%20Model%20%28LLM%29-driven%20attacks%2C%20focusing%20on%20offensive%0Auses%20of%20LLMs%20in%20cybersecurity.%20We%20review%2019%20research%20papers%20detailing%2018%0Aprototypes%20and%20their%20respective%20testbeds.%0A%20%20We%20detail%20our%20findings%20and%20provide%20actionable%20recommendations%20for%20future%0Aresearch%2C%20emphasizing%20the%20importance%20of%20extending%20existing%20testbeds%2C%20creating%0Abaselines%2C%20and%20including%20comprehensive%20metrics%20and%20qualitative%20analysis.%20We%0Aalso%20note%20the%20distinction%20between%20security%20research%20and%20practice%2C%20suggesting%0Athat%20CTF-based%20challenges%20may%20not%20fully%20represent%20real-world%20penetration%0Atesting%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10112v2&entry.124074799=Read"},
{"title": "Active Multimodal Distillation for Few-shot Action Recognition", "author": "Weijia Feng and Yichen Zhu and Ruojia Zhang and Chenyang Wang and Fei Ma and Xiaobao Wang and Xiaobai Li", "abstract": "  Owing to its rapid progress and broad application prospects, few-shot action\nrecognition has attracted considerable interest. However, current methods are\npredominantly based on limited single-modal data, which does not fully exploit\nthe potential of multimodal information. This paper presents a novel framework\nthat actively identifies reliable modalities for each sample using\ntask-specific contextual cues, thus significantly improving recognition\nperformance. Our framework integrates an Active Sample Inference (ASI) module,\nwhich utilizes active inference to predict reliable modalities based on\nposterior distributions and subsequently organizes them accordingly. Unlike\nreinforcement learning, active inference replaces rewards with evidence-based\npreferences, making more stable predictions. Additionally, we introduce an\nactive mutual distillation module that enhances the representation learning of\nless reliable modalities by transferring knowledge from more reliable ones.\nAdaptive multimodal inference is employed during the meta-test to assign higher\nweights to reliable modalities. Extensive experiments across multiple\nbenchmarks demonstrate that our method significantly outperforms existing\napproaches.\n", "link": "http://arxiv.org/abs/2506.13322v1", "date": "2025-06-16", "relevancy": 2.1714, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5624}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5322}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Multimodal%20Distillation%20for%20Few-shot%20Action%20Recognition&body=Title%3A%20Active%20Multimodal%20Distillation%20for%20Few-shot%20Action%20Recognition%0AAuthor%3A%20Weijia%20Feng%20and%20Yichen%20Zhu%20and%20Ruojia%20Zhang%20and%20Chenyang%20Wang%20and%20Fei%20Ma%20and%20Xiaobao%20Wang%20and%20Xiaobai%20Li%0AAbstract%3A%20%20%20Owing%20to%20its%20rapid%20progress%20and%20broad%20application%20prospects%2C%20few-shot%20action%0Arecognition%20has%20attracted%20considerable%20interest.%20However%2C%20current%20methods%20are%0Apredominantly%20based%20on%20limited%20single-modal%20data%2C%20which%20does%20not%20fully%20exploit%0Athe%20potential%20of%20multimodal%20information.%20This%20paper%20presents%20a%20novel%20framework%0Athat%20actively%20identifies%20reliable%20modalities%20for%20each%20sample%20using%0Atask-specific%20contextual%20cues%2C%20thus%20significantly%20improving%20recognition%0Aperformance.%20Our%20framework%20integrates%20an%20Active%20Sample%20Inference%20%28ASI%29%20module%2C%0Awhich%20utilizes%20active%20inference%20to%20predict%20reliable%20modalities%20based%20on%0Aposterior%20distributions%20and%20subsequently%20organizes%20them%20accordingly.%20Unlike%0Areinforcement%20learning%2C%20active%20inference%20replaces%20rewards%20with%20evidence-based%0Apreferences%2C%20making%20more%20stable%20predictions.%20Additionally%2C%20we%20introduce%20an%0Aactive%20mutual%20distillation%20module%20that%20enhances%20the%20representation%20learning%20of%0Aless%20reliable%20modalities%20by%20transferring%20knowledge%20from%20more%20reliable%20ones.%0AAdaptive%20multimodal%20inference%20is%20employed%20during%20the%20meta-test%20to%20assign%20higher%0Aweights%20to%20reliable%20modalities.%20Extensive%20experiments%20across%20multiple%0Abenchmarks%20demonstrate%20that%20our%20method%20significantly%20outperforms%20existing%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Multimodal%2520Distillation%2520for%2520Few-shot%2520Action%2520Recognition%26entry.906535625%3DWeijia%2520Feng%2520and%2520Yichen%2520Zhu%2520and%2520Ruojia%2520Zhang%2520and%2520Chenyang%2520Wang%2520and%2520Fei%2520Ma%2520and%2520Xiaobao%2520Wang%2520and%2520Xiaobai%2520Li%26entry.1292438233%3D%2520%2520Owing%2520to%2520its%2520rapid%2520progress%2520and%2520broad%2520application%2520prospects%252C%2520few-shot%2520action%250Arecognition%2520has%2520attracted%2520considerable%2520interest.%2520However%252C%2520current%2520methods%2520are%250Apredominantly%2520based%2520on%2520limited%2520single-modal%2520data%252C%2520which%2520does%2520not%2520fully%2520exploit%250Athe%2520potential%2520of%2520multimodal%2520information.%2520This%2520paper%2520presents%2520a%2520novel%2520framework%250Athat%2520actively%2520identifies%2520reliable%2520modalities%2520for%2520each%2520sample%2520using%250Atask-specific%2520contextual%2520cues%252C%2520thus%2520significantly%2520improving%2520recognition%250Aperformance.%2520Our%2520framework%2520integrates%2520an%2520Active%2520Sample%2520Inference%2520%2528ASI%2529%2520module%252C%250Awhich%2520utilizes%2520active%2520inference%2520to%2520predict%2520reliable%2520modalities%2520based%2520on%250Aposterior%2520distributions%2520and%2520subsequently%2520organizes%2520them%2520accordingly.%2520Unlike%250Areinforcement%2520learning%252C%2520active%2520inference%2520replaces%2520rewards%2520with%2520evidence-based%250Apreferences%252C%2520making%2520more%2520stable%2520predictions.%2520Additionally%252C%2520we%2520introduce%2520an%250Aactive%2520mutual%2520distillation%2520module%2520that%2520enhances%2520the%2520representation%2520learning%2520of%250Aless%2520reliable%2520modalities%2520by%2520transferring%2520knowledge%2520from%2520more%2520reliable%2520ones.%250AAdaptive%2520multimodal%2520inference%2520is%2520employed%2520during%2520the%2520meta-test%2520to%2520assign%2520higher%250Aweights%2520to%2520reliable%2520modalities.%2520Extensive%2520experiments%2520across%2520multiple%250Abenchmarks%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520existing%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Multimodal%20Distillation%20for%20Few-shot%20Action%20Recognition&entry.906535625=Weijia%20Feng%20and%20Yichen%20Zhu%20and%20Ruojia%20Zhang%20and%20Chenyang%20Wang%20and%20Fei%20Ma%20and%20Xiaobao%20Wang%20and%20Xiaobai%20Li&entry.1292438233=%20%20Owing%20to%20its%20rapid%20progress%20and%20broad%20application%20prospects%2C%20few-shot%20action%0Arecognition%20has%20attracted%20considerable%20interest.%20However%2C%20current%20methods%20are%0Apredominantly%20based%20on%20limited%20single-modal%20data%2C%20which%20does%20not%20fully%20exploit%0Athe%20potential%20of%20multimodal%20information.%20This%20paper%20presents%20a%20novel%20framework%0Athat%20actively%20identifies%20reliable%20modalities%20for%20each%20sample%20using%0Atask-specific%20contextual%20cues%2C%20thus%20significantly%20improving%20recognition%0Aperformance.%20Our%20framework%20integrates%20an%20Active%20Sample%20Inference%20%28ASI%29%20module%2C%0Awhich%20utilizes%20active%20inference%20to%20predict%20reliable%20modalities%20based%20on%0Aposterior%20distributions%20and%20subsequently%20organizes%20them%20accordingly.%20Unlike%0Areinforcement%20learning%2C%20active%20inference%20replaces%20rewards%20with%20evidence-based%0Apreferences%2C%20making%20more%20stable%20predictions.%20Additionally%2C%20we%20introduce%20an%0Aactive%20mutual%20distillation%20module%20that%20enhances%20the%20representation%20learning%20of%0Aless%20reliable%20modalities%20by%20transferring%20knowledge%20from%20more%20reliable%20ones.%0AAdaptive%20multimodal%20inference%20is%20employed%20during%20the%20meta-test%20to%20assign%20higher%0Aweights%20to%20reliable%20modalities.%20Extensive%20experiments%20across%20multiple%0Abenchmarks%20demonstrate%20that%20our%20method%20significantly%20outperforms%20existing%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13322v1&entry.124074799=Read"},
{"title": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous\n  Driving with Adaptive Reasoning and Reinforcement Fine-Tuning", "author": "Zewei Zhou and Tianhui Cai and Seth Z. Zhao and Yun Zhang and Zhiyu Huang and Bolei Zhou and Jiaqi Ma", "abstract": "  Recent advancements in Vision-Language-Action (VLA) models have shown promise\nfor end-to-end autonomous driving by leveraging world knowledge and reasoning\ncapabilities. However, current VLA models often struggle with physically\ninfeasible action outputs, complex model structures, or unnecessarily long\nreasoning. In this paper, we propose AutoVLA, a novel VLA model that unifies\nreasoning and action generation within a single autoregressive generation model\nfor end-to-end autonomous driving. AutoVLA performs semantic reasoning and\ntrajectory planning directly from raw visual inputs and language instructions.\nWe tokenize continuous trajectories into discrete, feasible actions, enabling\ndirect integration into the language model. For training, we employ supervised\nfine-tuning to equip the model with dual thinking modes: fast thinking\n(trajectory-only) and slow thinking (enhanced with chain-of-thought reasoning).\nTo further enhance planning performance and efficiency, we introduce a\nreinforcement fine-tuning method based on Group Relative Policy Optimization\n(GRPO), reducing unnecessary reasoning in straightforward scenarios. Extensive\nexperiments across real-world and simulated datasets and benchmarks, including\nnuPlan, nuScenes, Waymo, and CARLA, demonstrate the competitive performance of\nAutoVLA in both open-loop and closed-loop settings. Qualitative results\nshowcase the adaptive reasoning and accurate planning capabilities of AutoVLA\nin diverse scenarios.\n", "link": "http://arxiv.org/abs/2506.13757v1", "date": "2025-06-16", "relevancy": 2.1609, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoVLA%3A%20A%20Vision-Language-Action%20Model%20for%20End-to-End%20Autonomous%0A%20%20Driving%20with%20Adaptive%20Reasoning%20and%20Reinforcement%20Fine-Tuning&body=Title%3A%20AutoVLA%3A%20A%20Vision-Language-Action%20Model%20for%20End-to-End%20Autonomous%0A%20%20Driving%20with%20Adaptive%20Reasoning%20and%20Reinforcement%20Fine-Tuning%0AAuthor%3A%20Zewei%20Zhou%20and%20Tianhui%20Cai%20and%20Seth%20Z.%20Zhao%20and%20Yun%20Zhang%20and%20Zhiyu%20Huang%20and%20Bolei%20Zhou%20and%20Jiaqi%20Ma%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Vision-Language-Action%20%28VLA%29%20models%20have%20shown%20promise%0Afor%20end-to-end%20autonomous%20driving%20by%20leveraging%20world%20knowledge%20and%20reasoning%0Acapabilities.%20However%2C%20current%20VLA%20models%20often%20struggle%20with%20physically%0Ainfeasible%20action%20outputs%2C%20complex%20model%20structures%2C%20or%20unnecessarily%20long%0Areasoning.%20In%20this%20paper%2C%20we%20propose%20AutoVLA%2C%20a%20novel%20VLA%20model%20that%20unifies%0Areasoning%20and%20action%20generation%20within%20a%20single%20autoregressive%20generation%20model%0Afor%20end-to-end%20autonomous%20driving.%20AutoVLA%20performs%20semantic%20reasoning%20and%0Atrajectory%20planning%20directly%20from%20raw%20visual%20inputs%20and%20language%20instructions.%0AWe%20tokenize%20continuous%20trajectories%20into%20discrete%2C%20feasible%20actions%2C%20enabling%0Adirect%20integration%20into%20the%20language%20model.%20For%20training%2C%20we%20employ%20supervised%0Afine-tuning%20to%20equip%20the%20model%20with%20dual%20thinking%20modes%3A%20fast%20thinking%0A%28trajectory-only%29%20and%20slow%20thinking%20%28enhanced%20with%20chain-of-thought%20reasoning%29.%0ATo%20further%20enhance%20planning%20performance%20and%20efficiency%2C%20we%20introduce%20a%0Areinforcement%20fine-tuning%20method%20based%20on%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%2C%20reducing%20unnecessary%20reasoning%20in%20straightforward%20scenarios.%20Extensive%0Aexperiments%20across%20real-world%20and%20simulated%20datasets%20and%20benchmarks%2C%20including%0AnuPlan%2C%20nuScenes%2C%20Waymo%2C%20and%20CARLA%2C%20demonstrate%20the%20competitive%20performance%20of%0AAutoVLA%20in%20both%20open-loop%20and%20closed-loop%20settings.%20Qualitative%20results%0Ashowcase%20the%20adaptive%20reasoning%20and%20accurate%20planning%20capabilities%20of%20AutoVLA%0Ain%20diverse%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13757v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoVLA%253A%2520A%2520Vision-Language-Action%2520Model%2520for%2520End-to-End%2520Autonomous%250A%2520%2520Driving%2520with%2520Adaptive%2520Reasoning%2520and%2520Reinforcement%2520Fine-Tuning%26entry.906535625%3DZewei%2520Zhou%2520and%2520Tianhui%2520Cai%2520and%2520Seth%2520Z.%2520Zhao%2520and%2520Yun%2520Zhang%2520and%2520Zhiyu%2520Huang%2520and%2520Bolei%2520Zhou%2520and%2520Jiaqi%2520Ma%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520shown%2520promise%250Afor%2520end-to-end%2520autonomous%2520driving%2520by%2520leveraging%2520world%2520knowledge%2520and%2520reasoning%250Acapabilities.%2520However%252C%2520current%2520VLA%2520models%2520often%2520struggle%2520with%2520physically%250Ainfeasible%2520action%2520outputs%252C%2520complex%2520model%2520structures%252C%2520or%2520unnecessarily%2520long%250Areasoning.%2520In%2520this%2520paper%252C%2520we%2520propose%2520AutoVLA%252C%2520a%2520novel%2520VLA%2520model%2520that%2520unifies%250Areasoning%2520and%2520action%2520generation%2520within%2520a%2520single%2520autoregressive%2520generation%2520model%250Afor%2520end-to-end%2520autonomous%2520driving.%2520AutoVLA%2520performs%2520semantic%2520reasoning%2520and%250Atrajectory%2520planning%2520directly%2520from%2520raw%2520visual%2520inputs%2520and%2520language%2520instructions.%250AWe%2520tokenize%2520continuous%2520trajectories%2520into%2520discrete%252C%2520feasible%2520actions%252C%2520enabling%250Adirect%2520integration%2520into%2520the%2520language%2520model.%2520For%2520training%252C%2520we%2520employ%2520supervised%250Afine-tuning%2520to%2520equip%2520the%2520model%2520with%2520dual%2520thinking%2520modes%253A%2520fast%2520thinking%250A%2528trajectory-only%2529%2520and%2520slow%2520thinking%2520%2528enhanced%2520with%2520chain-of-thought%2520reasoning%2529.%250ATo%2520further%2520enhance%2520planning%2520performance%2520and%2520efficiency%252C%2520we%2520introduce%2520a%250Areinforcement%2520fine-tuning%2520method%2520based%2520on%2520Group%2520Relative%2520Policy%2520Optimization%250A%2528GRPO%2529%252C%2520reducing%2520unnecessary%2520reasoning%2520in%2520straightforward%2520scenarios.%2520Extensive%250Aexperiments%2520across%2520real-world%2520and%2520simulated%2520datasets%2520and%2520benchmarks%252C%2520including%250AnuPlan%252C%2520nuScenes%252C%2520Waymo%252C%2520and%2520CARLA%252C%2520demonstrate%2520the%2520competitive%2520performance%2520of%250AAutoVLA%2520in%2520both%2520open-loop%2520and%2520closed-loop%2520settings.%2520Qualitative%2520results%250Ashowcase%2520the%2520adaptive%2520reasoning%2520and%2520accurate%2520planning%2520capabilities%2520of%2520AutoVLA%250Ain%2520diverse%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13757v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoVLA%3A%20A%20Vision-Language-Action%20Model%20for%20End-to-End%20Autonomous%0A%20%20Driving%20with%20Adaptive%20Reasoning%20and%20Reinforcement%20Fine-Tuning&entry.906535625=Zewei%20Zhou%20and%20Tianhui%20Cai%20and%20Seth%20Z.%20Zhao%20and%20Yun%20Zhang%20and%20Zhiyu%20Huang%20and%20Bolei%20Zhou%20and%20Jiaqi%20Ma&entry.1292438233=%20%20Recent%20advancements%20in%20Vision-Language-Action%20%28VLA%29%20models%20have%20shown%20promise%0Afor%20end-to-end%20autonomous%20driving%20by%20leveraging%20world%20knowledge%20and%20reasoning%0Acapabilities.%20However%2C%20current%20VLA%20models%20often%20struggle%20with%20physically%0Ainfeasible%20action%20outputs%2C%20complex%20model%20structures%2C%20or%20unnecessarily%20long%0Areasoning.%20In%20this%20paper%2C%20we%20propose%20AutoVLA%2C%20a%20novel%20VLA%20model%20that%20unifies%0Areasoning%20and%20action%20generation%20within%20a%20single%20autoregressive%20generation%20model%0Afor%20end-to-end%20autonomous%20driving.%20AutoVLA%20performs%20semantic%20reasoning%20and%0Atrajectory%20planning%20directly%20from%20raw%20visual%20inputs%20and%20language%20instructions.%0AWe%20tokenize%20continuous%20trajectories%20into%20discrete%2C%20feasible%20actions%2C%20enabling%0Adirect%20integration%20into%20the%20language%20model.%20For%20training%2C%20we%20employ%20supervised%0Afine-tuning%20to%20equip%20the%20model%20with%20dual%20thinking%20modes%3A%20fast%20thinking%0A%28trajectory-only%29%20and%20slow%20thinking%20%28enhanced%20with%20chain-of-thought%20reasoning%29.%0ATo%20further%20enhance%20planning%20performance%20and%20efficiency%2C%20we%20introduce%20a%0Areinforcement%20fine-tuning%20method%20based%20on%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%2C%20reducing%20unnecessary%20reasoning%20in%20straightforward%20scenarios.%20Extensive%0Aexperiments%20across%20real-world%20and%20simulated%20datasets%20and%20benchmarks%2C%20including%0AnuPlan%2C%20nuScenes%2C%20Waymo%2C%20and%20CARLA%2C%20demonstrate%20the%20competitive%20performance%20of%0AAutoVLA%20in%20both%20open-loop%20and%20closed-loop%20settings.%20Qualitative%20results%0Ashowcase%20the%20adaptive%20reasoning%20and%20accurate%20planning%20capabilities%20of%20AutoVLA%0Ain%20diverse%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13757v1&entry.124074799=Read"},
{"title": "Computing the Distance between unbalanced Distributions -- The flat\n  Metric", "author": "Henri Schmidt and Christian D\u00fcll", "abstract": "  We provide an implementation to compute the flat metric in any dimension. The\nflat metric, also called dual bounded Lipschitz distance, generalizes the\nwell-known Wasserstein distance $W_1$ to the case that the distributions are of\nunequal total mass. Thus, our implementation adapts very well to mass\ndifferences and uses them to distinguish between different distributions. This\nis of particular interest for unbalanced optimal transport tasks and for the\nanalysis of data distributions where the sample size is important or\nnormalization is not possible. The core of the method is based on a neural\nnetwork to determine an optimal test function realizing the distance between\ntwo given measures. Special focus was put on achieving comparability of\npairwise computed distances from independently trained networks. We tested the\nquality of the output in several experiments where ground truth was available\nas well as with simulated data.\n", "link": "http://arxiv.org/abs/2308.01039v2", "date": "2025-06-16", "relevancy": 2.1583, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4419}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4286}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computing%20the%20Distance%20between%20unbalanced%20Distributions%20--%20The%20flat%0A%20%20Metric&body=Title%3A%20Computing%20the%20Distance%20between%20unbalanced%20Distributions%20--%20The%20flat%0A%20%20Metric%0AAuthor%3A%20Henri%20Schmidt%20and%20Christian%20D%C3%BCll%0AAbstract%3A%20%20%20We%20provide%20an%20implementation%20to%20compute%20the%20flat%20metric%20in%20any%20dimension.%20The%0Aflat%20metric%2C%20also%20called%20dual%20bounded%20Lipschitz%20distance%2C%20generalizes%20the%0Awell-known%20Wasserstein%20distance%20%24W_1%24%20to%20the%20case%20that%20the%20distributions%20are%20of%0Aunequal%20total%20mass.%20Thus%2C%20our%20implementation%20adapts%20very%20well%20to%20mass%0Adifferences%20and%20uses%20them%20to%20distinguish%20between%20different%20distributions.%20This%0Ais%20of%20particular%20interest%20for%20unbalanced%20optimal%20transport%20tasks%20and%20for%20the%0Aanalysis%20of%20data%20distributions%20where%20the%20sample%20size%20is%20important%20or%0Anormalization%20is%20not%20possible.%20The%20core%20of%20the%20method%20is%20based%20on%20a%20neural%0Anetwork%20to%20determine%20an%20optimal%20test%20function%20realizing%20the%20distance%20between%0Atwo%20given%20measures.%20Special%20focus%20was%20put%20on%20achieving%20comparability%20of%0Apairwise%20computed%20distances%20from%20independently%20trained%20networks.%20We%20tested%20the%0Aquality%20of%20the%20output%20in%20several%20experiments%20where%20ground%20truth%20was%20available%0Aas%20well%20as%20with%20simulated%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.01039v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputing%2520the%2520Distance%2520between%2520unbalanced%2520Distributions%2520--%2520The%2520flat%250A%2520%2520Metric%26entry.906535625%3DHenri%2520Schmidt%2520and%2520Christian%2520D%25C3%25BCll%26entry.1292438233%3D%2520%2520We%2520provide%2520an%2520implementation%2520to%2520compute%2520the%2520flat%2520metric%2520in%2520any%2520dimension.%2520The%250Aflat%2520metric%252C%2520also%2520called%2520dual%2520bounded%2520Lipschitz%2520distance%252C%2520generalizes%2520the%250Awell-known%2520Wasserstein%2520distance%2520%2524W_1%2524%2520to%2520the%2520case%2520that%2520the%2520distributions%2520are%2520of%250Aunequal%2520total%2520mass.%2520Thus%252C%2520our%2520implementation%2520adapts%2520very%2520well%2520to%2520mass%250Adifferences%2520and%2520uses%2520them%2520to%2520distinguish%2520between%2520different%2520distributions.%2520This%250Ais%2520of%2520particular%2520interest%2520for%2520unbalanced%2520optimal%2520transport%2520tasks%2520and%2520for%2520the%250Aanalysis%2520of%2520data%2520distributions%2520where%2520the%2520sample%2520size%2520is%2520important%2520or%250Anormalization%2520is%2520not%2520possible.%2520The%2520core%2520of%2520the%2520method%2520is%2520based%2520on%2520a%2520neural%250Anetwork%2520to%2520determine%2520an%2520optimal%2520test%2520function%2520realizing%2520the%2520distance%2520between%250Atwo%2520given%2520measures.%2520Special%2520focus%2520was%2520put%2520on%2520achieving%2520comparability%2520of%250Apairwise%2520computed%2520distances%2520from%2520independently%2520trained%2520networks.%2520We%2520tested%2520the%250Aquality%2520of%2520the%2520output%2520in%2520several%2520experiments%2520where%2520ground%2520truth%2520was%2520available%250Aas%2520well%2520as%2520with%2520simulated%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.01039v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computing%20the%20Distance%20between%20unbalanced%20Distributions%20--%20The%20flat%0A%20%20Metric&entry.906535625=Henri%20Schmidt%20and%20Christian%20D%C3%BCll&entry.1292438233=%20%20We%20provide%20an%20implementation%20to%20compute%20the%20flat%20metric%20in%20any%20dimension.%20The%0Aflat%20metric%2C%20also%20called%20dual%20bounded%20Lipschitz%20distance%2C%20generalizes%20the%0Awell-known%20Wasserstein%20distance%20%24W_1%24%20to%20the%20case%20that%20the%20distributions%20are%20of%0Aunequal%20total%20mass.%20Thus%2C%20our%20implementation%20adapts%20very%20well%20to%20mass%0Adifferences%20and%20uses%20them%20to%20distinguish%20between%20different%20distributions.%20This%0Ais%20of%20particular%20interest%20for%20unbalanced%20optimal%20transport%20tasks%20and%20for%20the%0Aanalysis%20of%20data%20distributions%20where%20the%20sample%20size%20is%20important%20or%0Anormalization%20is%20not%20possible.%20The%20core%20of%20the%20method%20is%20based%20on%20a%20neural%0Anetwork%20to%20determine%20an%20optimal%20test%20function%20realizing%20the%20distance%20between%0Atwo%20given%20measures.%20Special%20focus%20was%20put%20on%20achieving%20comparability%20of%0Apairwise%20computed%20distances%20from%20independently%20trained%20networks.%20We%20tested%20the%0Aquality%20of%20the%20output%20in%20several%20experiments%20where%20ground%20truth%20was%20available%0Aas%20well%20as%20with%20simulated%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.01039v2&entry.124074799=Read"},
{"title": "Real-time Seafloor Segmentation and Mapping", "author": "Michele Grimaldi and Nouf Alkaabi and Francesco Ruscio and Sebastian Realpe Rua and Rafael Garcia and Nuno Gracias", "abstract": "  Posidonia oceanica meadows are a species of seagrass highly dependent on\nrocks for their survival and conservation. In recent years, there has been a\nconcerning global decline in this species, emphasizing the critical need for\nefficient monitoring and assessment tools. While deep learning-based semantic\nsegmentation and visual automated monitoring systems have shown promise in a\nvariety of applications, their performance in underwater environments remains\nchallenging due to complex water conditions and limited datasets. This paper\nintroduces a framework that combines machine learning and computer vision\ntechniques to enable an autonomous underwater vehicle (AUV) to inspect the\nboundaries of Posidonia oceanica meadows autonomously. The framework\nincorporates an image segmentation module using an existing Mask R-CNN model\nand a strategy for Posidonia oceanica meadow boundary tracking. Furthermore, a\nnew class dedicated to rocks is introduced to enhance the existing model,\naiming to contribute to a comprehensive monitoring approach and provide a\ndeeper understanding of the intricate interactions between the meadow and its\nsurrounding environment. The image segmentation model is validated using real\nunderwater images, while the overall inspection framework is evaluated in a\nrealistic simulation environment, replicating actual monitoring scenarios with\nreal underwater images. The results demonstrate that the proposed framework\nenables the AUV to autonomously accomplish the main tasks of underwater\ninspection and segmentation of rocks. Consequently, this work holds significant\npotential for the conservation and protection of marine environments, providing\nvaluable insights into the status of Posidonia oceanica meadows and supporting\ntargeted preservation efforts\n", "link": "http://arxiv.org/abs/2504.10750v3", "date": "2025-06-16", "relevancy": 2.155, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5548}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-time%20Seafloor%20Segmentation%20and%20Mapping&body=Title%3A%20Real-time%20Seafloor%20Segmentation%20and%20Mapping%0AAuthor%3A%20Michele%20Grimaldi%20and%20Nouf%20Alkaabi%20and%20Francesco%20Ruscio%20and%20Sebastian%20Realpe%20Rua%20and%20Rafael%20Garcia%20and%20Nuno%20Gracias%0AAbstract%3A%20%20%20Posidonia%20oceanica%20meadows%20are%20a%20species%20of%20seagrass%20highly%20dependent%20on%0Arocks%20for%20their%20survival%20and%20conservation.%20In%20recent%20years%2C%20there%20has%20been%20a%0Aconcerning%20global%20decline%20in%20this%20species%2C%20emphasizing%20the%20critical%20need%20for%0Aefficient%20monitoring%20and%20assessment%20tools.%20While%20deep%20learning-based%20semantic%0Asegmentation%20and%20visual%20automated%20monitoring%20systems%20have%20shown%20promise%20in%20a%0Avariety%20of%20applications%2C%20their%20performance%20in%20underwater%20environments%20remains%0Achallenging%20due%20to%20complex%20water%20conditions%20and%20limited%20datasets.%20This%20paper%0Aintroduces%20a%20framework%20that%20combines%20machine%20learning%20and%20computer%20vision%0Atechniques%20to%20enable%20an%20autonomous%20underwater%20vehicle%20%28AUV%29%20to%20inspect%20the%0Aboundaries%20of%20Posidonia%20oceanica%20meadows%20autonomously.%20The%20framework%0Aincorporates%20an%20image%20segmentation%20module%20using%20an%20existing%20Mask%20R-CNN%20model%0Aand%20a%20strategy%20for%20Posidonia%20oceanica%20meadow%20boundary%20tracking.%20Furthermore%2C%20a%0Anew%20class%20dedicated%20to%20rocks%20is%20introduced%20to%20enhance%20the%20existing%20model%2C%0Aaiming%20to%20contribute%20to%20a%20comprehensive%20monitoring%20approach%20and%20provide%20a%0Adeeper%20understanding%20of%20the%20intricate%20interactions%20between%20the%20meadow%20and%20its%0Asurrounding%20environment.%20The%20image%20segmentation%20model%20is%20validated%20using%20real%0Aunderwater%20images%2C%20while%20the%20overall%20inspection%20framework%20is%20evaluated%20in%20a%0Arealistic%20simulation%20environment%2C%20replicating%20actual%20monitoring%20scenarios%20with%0Areal%20underwater%20images.%20The%20results%20demonstrate%20that%20the%20proposed%20framework%0Aenables%20the%20AUV%20to%20autonomously%20accomplish%20the%20main%20tasks%20of%20underwater%0Ainspection%20and%20segmentation%20of%20rocks.%20Consequently%2C%20this%20work%20holds%20significant%0Apotential%20for%20the%20conservation%20and%20protection%20of%20marine%20environments%2C%20providing%0Avaluable%20insights%20into%20the%20status%20of%20Posidonia%20oceanica%20meadows%20and%20supporting%0Atargeted%20preservation%20efforts%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10750v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-time%2520Seafloor%2520Segmentation%2520and%2520Mapping%26entry.906535625%3DMichele%2520Grimaldi%2520and%2520Nouf%2520Alkaabi%2520and%2520Francesco%2520Ruscio%2520and%2520Sebastian%2520Realpe%2520Rua%2520and%2520Rafael%2520Garcia%2520and%2520Nuno%2520Gracias%26entry.1292438233%3D%2520%2520Posidonia%2520oceanica%2520meadows%2520are%2520a%2520species%2520of%2520seagrass%2520highly%2520dependent%2520on%250Arocks%2520for%2520their%2520survival%2520and%2520conservation.%2520In%2520recent%2520years%252C%2520there%2520has%2520been%2520a%250Aconcerning%2520global%2520decline%2520in%2520this%2520species%252C%2520emphasizing%2520the%2520critical%2520need%2520for%250Aefficient%2520monitoring%2520and%2520assessment%2520tools.%2520While%2520deep%2520learning-based%2520semantic%250Asegmentation%2520and%2520visual%2520automated%2520monitoring%2520systems%2520have%2520shown%2520promise%2520in%2520a%250Avariety%2520of%2520applications%252C%2520their%2520performance%2520in%2520underwater%2520environments%2520remains%250Achallenging%2520due%2520to%2520complex%2520water%2520conditions%2520and%2520limited%2520datasets.%2520This%2520paper%250Aintroduces%2520a%2520framework%2520that%2520combines%2520machine%2520learning%2520and%2520computer%2520vision%250Atechniques%2520to%2520enable%2520an%2520autonomous%2520underwater%2520vehicle%2520%2528AUV%2529%2520to%2520inspect%2520the%250Aboundaries%2520of%2520Posidonia%2520oceanica%2520meadows%2520autonomously.%2520The%2520framework%250Aincorporates%2520an%2520image%2520segmentation%2520module%2520using%2520an%2520existing%2520Mask%2520R-CNN%2520model%250Aand%2520a%2520strategy%2520for%2520Posidonia%2520oceanica%2520meadow%2520boundary%2520tracking.%2520Furthermore%252C%2520a%250Anew%2520class%2520dedicated%2520to%2520rocks%2520is%2520introduced%2520to%2520enhance%2520the%2520existing%2520model%252C%250Aaiming%2520to%2520contribute%2520to%2520a%2520comprehensive%2520monitoring%2520approach%2520and%2520provide%2520a%250Adeeper%2520understanding%2520of%2520the%2520intricate%2520interactions%2520between%2520the%2520meadow%2520and%2520its%250Asurrounding%2520environment.%2520The%2520image%2520segmentation%2520model%2520is%2520validated%2520using%2520real%250Aunderwater%2520images%252C%2520while%2520the%2520overall%2520inspection%2520framework%2520is%2520evaluated%2520in%2520a%250Arealistic%2520simulation%2520environment%252C%2520replicating%2520actual%2520monitoring%2520scenarios%2520with%250Areal%2520underwater%2520images.%2520The%2520results%2520demonstrate%2520that%2520the%2520proposed%2520framework%250Aenables%2520the%2520AUV%2520to%2520autonomously%2520accomplish%2520the%2520main%2520tasks%2520of%2520underwater%250Ainspection%2520and%2520segmentation%2520of%2520rocks.%2520Consequently%252C%2520this%2520work%2520holds%2520significant%250Apotential%2520for%2520the%2520conservation%2520and%2520protection%2520of%2520marine%2520environments%252C%2520providing%250Avaluable%2520insights%2520into%2520the%2520status%2520of%2520Posidonia%2520oceanica%2520meadows%2520and%2520supporting%250Atargeted%2520preservation%2520efforts%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10750v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%20Seafloor%20Segmentation%20and%20Mapping&entry.906535625=Michele%20Grimaldi%20and%20Nouf%20Alkaabi%20and%20Francesco%20Ruscio%20and%20Sebastian%20Realpe%20Rua%20and%20Rafael%20Garcia%20and%20Nuno%20Gracias&entry.1292438233=%20%20Posidonia%20oceanica%20meadows%20are%20a%20species%20of%20seagrass%20highly%20dependent%20on%0Arocks%20for%20their%20survival%20and%20conservation.%20In%20recent%20years%2C%20there%20has%20been%20a%0Aconcerning%20global%20decline%20in%20this%20species%2C%20emphasizing%20the%20critical%20need%20for%0Aefficient%20monitoring%20and%20assessment%20tools.%20While%20deep%20learning-based%20semantic%0Asegmentation%20and%20visual%20automated%20monitoring%20systems%20have%20shown%20promise%20in%20a%0Avariety%20of%20applications%2C%20their%20performance%20in%20underwater%20environments%20remains%0Achallenging%20due%20to%20complex%20water%20conditions%20and%20limited%20datasets.%20This%20paper%0Aintroduces%20a%20framework%20that%20combines%20machine%20learning%20and%20computer%20vision%0Atechniques%20to%20enable%20an%20autonomous%20underwater%20vehicle%20%28AUV%29%20to%20inspect%20the%0Aboundaries%20of%20Posidonia%20oceanica%20meadows%20autonomously.%20The%20framework%0Aincorporates%20an%20image%20segmentation%20module%20using%20an%20existing%20Mask%20R-CNN%20model%0Aand%20a%20strategy%20for%20Posidonia%20oceanica%20meadow%20boundary%20tracking.%20Furthermore%2C%20a%0Anew%20class%20dedicated%20to%20rocks%20is%20introduced%20to%20enhance%20the%20existing%20model%2C%0Aaiming%20to%20contribute%20to%20a%20comprehensive%20monitoring%20approach%20and%20provide%20a%0Adeeper%20understanding%20of%20the%20intricate%20interactions%20between%20the%20meadow%20and%20its%0Asurrounding%20environment.%20The%20image%20segmentation%20model%20is%20validated%20using%20real%0Aunderwater%20images%2C%20while%20the%20overall%20inspection%20framework%20is%20evaluated%20in%20a%0Arealistic%20simulation%20environment%2C%20replicating%20actual%20monitoring%20scenarios%20with%0Areal%20underwater%20images.%20The%20results%20demonstrate%20that%20the%20proposed%20framework%0Aenables%20the%20AUV%20to%20autonomously%20accomplish%20the%20main%20tasks%20of%20underwater%0Ainspection%20and%20segmentation%20of%20rocks.%20Consequently%2C%20this%20work%20holds%20significant%0Apotential%20for%20the%20conservation%20and%20protection%20of%20marine%20environments%2C%20providing%0Avaluable%20insights%20into%20the%20status%20of%20Posidonia%20oceanica%20meadows%20and%20supporting%0Atargeted%20preservation%20efforts%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10750v3&entry.124074799=Read"},
{"title": "Can you see how I learn? Human observers' inferences about Reinforcement\n  Learning agents' learning processes", "author": "Bernhard Hilpert and Muhan Hou and Kim Baraka and Joost Broekens", "abstract": "  Reinforcement Learning (RL) agents often exhibit learning behaviors that are\nnot intuitively interpretable by human observers, which can result in\nsuboptimal feedback in collaborative teaching settings. Yet, how humans\nperceive and interpret RL agent's learning behavior is largely unknown. In a\nbottom-up approach with two experiments, this work provides a data-driven\nunderstanding of the factors of human observers' understanding of the agent's\nlearning process. A novel, observation-based paradigm to directly assess human\ninferences about agent learning was developed. In an exploratory interview\nstudy (\\textit{N}=9), we identify four core themes in human interpretations:\nAgent Goals, Knowledge, Decision Making, and Learning Mechanisms. A second\nconfirmatory study (\\textit{N}=34) applied an expanded version of the paradigm\nacross two tasks (navigation/manipulation) and two RL algorithms\n(tabular/function approximation). Analyses of 816 responses confirmed the\nreliability of the paradigm and refined the thematic framework, revealing how\nthese themes evolve over time and interrelate. Our findings provide a\nhuman-centered understanding of how people make sense of agent learning,\noffering actionable insights for designing interpretable RL systems and\nimproving transparency in Human-Robot Interaction.\n", "link": "http://arxiv.org/abs/2506.13583v1", "date": "2025-06-16", "relevancy": 2.1366, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5406}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5329}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20you%20see%20how%20I%20learn%3F%20Human%20observers%27%20inferences%20about%20Reinforcement%0A%20%20Learning%20agents%27%20learning%20processes&body=Title%3A%20Can%20you%20see%20how%20I%20learn%3F%20Human%20observers%27%20inferences%20about%20Reinforcement%0A%20%20Learning%20agents%27%20learning%20processes%0AAuthor%3A%20Bernhard%20Hilpert%20and%20Muhan%20Hou%20and%20Kim%20Baraka%20and%20Joost%20Broekens%0AAbstract%3A%20%20%20Reinforcement%20Learning%20%28RL%29%20agents%20often%20exhibit%20learning%20behaviors%20that%20are%0Anot%20intuitively%20interpretable%20by%20human%20observers%2C%20which%20can%20result%20in%0Asuboptimal%20feedback%20in%20collaborative%20teaching%20settings.%20Yet%2C%20how%20humans%0Aperceive%20and%20interpret%20RL%20agent%27s%20learning%20behavior%20is%20largely%20unknown.%20In%20a%0Abottom-up%20approach%20with%20two%20experiments%2C%20this%20work%20provides%20a%20data-driven%0Aunderstanding%20of%20the%20factors%20of%20human%20observers%27%20understanding%20of%20the%20agent%27s%0Alearning%20process.%20A%20novel%2C%20observation-based%20paradigm%20to%20directly%20assess%20human%0Ainferences%20about%20agent%20learning%20was%20developed.%20In%20an%20exploratory%20interview%0Astudy%20%28%5Ctextit%7BN%7D%3D9%29%2C%20we%20identify%20four%20core%20themes%20in%20human%20interpretations%3A%0AAgent%20Goals%2C%20Knowledge%2C%20Decision%20Making%2C%20and%20Learning%20Mechanisms.%20A%20second%0Aconfirmatory%20study%20%28%5Ctextit%7BN%7D%3D34%29%20applied%20an%20expanded%20version%20of%20the%20paradigm%0Aacross%20two%20tasks%20%28navigation/manipulation%29%20and%20two%20RL%20algorithms%0A%28tabular/function%20approximation%29.%20Analyses%20of%20816%20responses%20confirmed%20the%0Areliability%20of%20the%20paradigm%20and%20refined%20the%20thematic%20framework%2C%20revealing%20how%0Athese%20themes%20evolve%20over%20time%20and%20interrelate.%20Our%20findings%20provide%20a%0Ahuman-centered%20understanding%20of%20how%20people%20make%20sense%20of%20agent%20learning%2C%0Aoffering%20actionable%20insights%20for%20designing%20interpretable%20RL%20systems%20and%0Aimproving%20transparency%20in%20Human-Robot%20Interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520you%2520see%2520how%2520I%2520learn%253F%2520Human%2520observers%2527%2520inferences%2520about%2520Reinforcement%250A%2520%2520Learning%2520agents%2527%2520learning%2520processes%26entry.906535625%3DBernhard%2520Hilpert%2520and%2520Muhan%2520Hou%2520and%2520Kim%2520Baraka%2520and%2520Joost%2520Broekens%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520%2528RL%2529%2520agents%2520often%2520exhibit%2520learning%2520behaviors%2520that%2520are%250Anot%2520intuitively%2520interpretable%2520by%2520human%2520observers%252C%2520which%2520can%2520result%2520in%250Asuboptimal%2520feedback%2520in%2520collaborative%2520teaching%2520settings.%2520Yet%252C%2520how%2520humans%250Aperceive%2520and%2520interpret%2520RL%2520agent%2527s%2520learning%2520behavior%2520is%2520largely%2520unknown.%2520In%2520a%250Abottom-up%2520approach%2520with%2520two%2520experiments%252C%2520this%2520work%2520provides%2520a%2520data-driven%250Aunderstanding%2520of%2520the%2520factors%2520of%2520human%2520observers%2527%2520understanding%2520of%2520the%2520agent%2527s%250Alearning%2520process.%2520A%2520novel%252C%2520observation-based%2520paradigm%2520to%2520directly%2520assess%2520human%250Ainferences%2520about%2520agent%2520learning%2520was%2520developed.%2520In%2520an%2520exploratory%2520interview%250Astudy%2520%2528%255Ctextit%257BN%257D%253D9%2529%252C%2520we%2520identify%2520four%2520core%2520themes%2520in%2520human%2520interpretations%253A%250AAgent%2520Goals%252C%2520Knowledge%252C%2520Decision%2520Making%252C%2520and%2520Learning%2520Mechanisms.%2520A%2520second%250Aconfirmatory%2520study%2520%2528%255Ctextit%257BN%257D%253D34%2529%2520applied%2520an%2520expanded%2520version%2520of%2520the%2520paradigm%250Aacross%2520two%2520tasks%2520%2528navigation/manipulation%2529%2520and%2520two%2520RL%2520algorithms%250A%2528tabular/function%2520approximation%2529.%2520Analyses%2520of%2520816%2520responses%2520confirmed%2520the%250Areliability%2520of%2520the%2520paradigm%2520and%2520refined%2520the%2520thematic%2520framework%252C%2520revealing%2520how%250Athese%2520themes%2520evolve%2520over%2520time%2520and%2520interrelate.%2520Our%2520findings%2520provide%2520a%250Ahuman-centered%2520understanding%2520of%2520how%2520people%2520make%2520sense%2520of%2520agent%2520learning%252C%250Aoffering%2520actionable%2520insights%2520for%2520designing%2520interpretable%2520RL%2520systems%2520and%250Aimproving%2520transparency%2520in%2520Human-Robot%2520Interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20you%20see%20how%20I%20learn%3F%20Human%20observers%27%20inferences%20about%20Reinforcement%0A%20%20Learning%20agents%27%20learning%20processes&entry.906535625=Bernhard%20Hilpert%20and%20Muhan%20Hou%20and%20Kim%20Baraka%20and%20Joost%20Broekens&entry.1292438233=%20%20Reinforcement%20Learning%20%28RL%29%20agents%20often%20exhibit%20learning%20behaviors%20that%20are%0Anot%20intuitively%20interpretable%20by%20human%20observers%2C%20which%20can%20result%20in%0Asuboptimal%20feedback%20in%20collaborative%20teaching%20settings.%20Yet%2C%20how%20humans%0Aperceive%20and%20interpret%20RL%20agent%27s%20learning%20behavior%20is%20largely%20unknown.%20In%20a%0Abottom-up%20approach%20with%20two%20experiments%2C%20this%20work%20provides%20a%20data-driven%0Aunderstanding%20of%20the%20factors%20of%20human%20observers%27%20understanding%20of%20the%20agent%27s%0Alearning%20process.%20A%20novel%2C%20observation-based%20paradigm%20to%20directly%20assess%20human%0Ainferences%20about%20agent%20learning%20was%20developed.%20In%20an%20exploratory%20interview%0Astudy%20%28%5Ctextit%7BN%7D%3D9%29%2C%20we%20identify%20four%20core%20themes%20in%20human%20interpretations%3A%0AAgent%20Goals%2C%20Knowledge%2C%20Decision%20Making%2C%20and%20Learning%20Mechanisms.%20A%20second%0Aconfirmatory%20study%20%28%5Ctextit%7BN%7D%3D34%29%20applied%20an%20expanded%20version%20of%20the%20paradigm%0Aacross%20two%20tasks%20%28navigation/manipulation%29%20and%20two%20RL%20algorithms%0A%28tabular/function%20approximation%29.%20Analyses%20of%20816%20responses%20confirmed%20the%0Areliability%20of%20the%20paradigm%20and%20refined%20the%20thematic%20framework%2C%20revealing%20how%0Athese%20themes%20evolve%20over%20time%20and%20interrelate.%20Our%20findings%20provide%20a%0Ahuman-centered%20understanding%20of%20how%20people%20make%20sense%20of%20agent%20learning%2C%0Aoffering%20actionable%20insights%20for%20designing%20interpretable%20RL%20systems%20and%0Aimproving%20transparency%20in%20Human-Robot%20Interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13583v1&entry.124074799=Read"},
{"title": "UltraVideo: High-Quality UHD Video Dataset with Comprehensive Captions", "author": "Zhucun Xue and Jiangning Zhang and Teng Hu and Haoyang He and Yinan Chen and Yuxuan Cai and Yabiao Wang and Chengjie Wang and Yong Liu and Xiangtai Li and Dacheng Tao", "abstract": "  The quality of the video dataset (image quality, resolution, and fine-grained\ncaption) greatly influences the performance of the video generation model. The\ngrowing demand for video applications sets higher requirements for high-quality\nvideo generation models. For example, the generation of movie-level Ultra-High\nDefinition (UHD) videos and the creation of 4K short video content. However,\nthe existing public datasets cannot support related research and applications.\nIn this paper, we first propose a high-quality open-sourced UHD-4K (22.4\\% of\nwhich are 8K) text-to-video dataset named UltraVideo, which contains a wide\nrange of topics (more than 100 kinds), and each video has 9 structured captions\nwith one summarized caption (average of 824 words). Specifically, we carefully\ndesign a highly automated curation process with four stages to obtain the final\nhigh-quality dataset: \\textit{i)} collection of diverse and high-quality video\nclips. \\textit{ii)} statistical data filtering. \\textit{iii)} model-based data\npurification. \\textit{iv)} generation of comprehensive, structured captions. In\naddition, we expand Wan to UltraWan-1K/-4K, which can natively generate\nhigh-quality 1K/4K videos with more consistent text controllability,\ndemonstrating the effectiveness of our data curation.We believe that this work\ncan make a significant contribution to future research on UHD video generation.\nUltraVideo dataset and UltraWan models are available at\nhttps://xzc-zju.github.io/projects/UltraVideo.\n", "link": "http://arxiv.org/abs/2506.13691v1", "date": "2025-06-16", "relevancy": 2.1351, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5384}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5364}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UltraVideo%3A%20High-Quality%20UHD%20Video%20Dataset%20with%20Comprehensive%20Captions&body=Title%3A%20UltraVideo%3A%20High-Quality%20UHD%20Video%20Dataset%20with%20Comprehensive%20Captions%0AAuthor%3A%20Zhucun%20Xue%20and%20Jiangning%20Zhang%20and%20Teng%20Hu%20and%20Haoyang%20He%20and%20Yinan%20Chen%20and%20Yuxuan%20Cai%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Yong%20Liu%20and%20Xiangtai%20Li%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20The%20quality%20of%20the%20video%20dataset%20%28image%20quality%2C%20resolution%2C%20and%20fine-grained%0Acaption%29%20greatly%20influences%20the%20performance%20of%20the%20video%20generation%20model.%20The%0Agrowing%20demand%20for%20video%20applications%20sets%20higher%20requirements%20for%20high-quality%0Avideo%20generation%20models.%20For%20example%2C%20the%20generation%20of%20movie-level%20Ultra-High%0ADefinition%20%28UHD%29%20videos%20and%20the%20creation%20of%204K%20short%20video%20content.%20However%2C%0Athe%20existing%20public%20datasets%20cannot%20support%20related%20research%20and%20applications.%0AIn%20this%20paper%2C%20we%20first%20propose%20a%20high-quality%20open-sourced%20UHD-4K%20%2822.4%5C%25%20of%0Awhich%20are%208K%29%20text-to-video%20dataset%20named%20UltraVideo%2C%20which%20contains%20a%20wide%0Arange%20of%20topics%20%28more%20than%20100%20kinds%29%2C%20and%20each%20video%20has%209%20structured%20captions%0Awith%20one%20summarized%20caption%20%28average%20of%20824%20words%29.%20Specifically%2C%20we%20carefully%0Adesign%20a%20highly%20automated%20curation%20process%20with%20four%20stages%20to%20obtain%20the%20final%0Ahigh-quality%20dataset%3A%20%5Ctextit%7Bi%29%7D%20collection%20of%20diverse%20and%20high-quality%20video%0Aclips.%20%5Ctextit%7Bii%29%7D%20statistical%20data%20filtering.%20%5Ctextit%7Biii%29%7D%20model-based%20data%0Apurification.%20%5Ctextit%7Biv%29%7D%20generation%20of%20comprehensive%2C%20structured%20captions.%20In%0Aaddition%2C%20we%20expand%20Wan%20to%20UltraWan-1K/-4K%2C%20which%20can%20natively%20generate%0Ahigh-quality%201K/4K%20videos%20with%20more%20consistent%20text%20controllability%2C%0Ademonstrating%20the%20effectiveness%20of%20our%20data%20curation.We%20believe%20that%20this%20work%0Acan%20make%20a%20significant%20contribution%20to%20future%20research%20on%20UHD%20video%20generation.%0AUltraVideo%20dataset%20and%20UltraWan%20models%20are%20available%20at%0Ahttps%3A//xzc-zju.github.io/projects/UltraVideo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltraVideo%253A%2520High-Quality%2520UHD%2520Video%2520Dataset%2520with%2520Comprehensive%2520Captions%26entry.906535625%3DZhucun%2520Xue%2520and%2520Jiangning%2520Zhang%2520and%2520Teng%2520Hu%2520and%2520Haoyang%2520He%2520and%2520Yinan%2520Chen%2520and%2520Yuxuan%2520Cai%2520and%2520Yabiao%2520Wang%2520and%2520Chengjie%2520Wang%2520and%2520Yong%2520Liu%2520and%2520Xiangtai%2520Li%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520The%2520quality%2520of%2520the%2520video%2520dataset%2520%2528image%2520quality%252C%2520resolution%252C%2520and%2520fine-grained%250Acaption%2529%2520greatly%2520influences%2520the%2520performance%2520of%2520the%2520video%2520generation%2520model.%2520The%250Agrowing%2520demand%2520for%2520video%2520applications%2520sets%2520higher%2520requirements%2520for%2520high-quality%250Avideo%2520generation%2520models.%2520For%2520example%252C%2520the%2520generation%2520of%2520movie-level%2520Ultra-High%250ADefinition%2520%2528UHD%2529%2520videos%2520and%2520the%2520creation%2520of%25204K%2520short%2520video%2520content.%2520However%252C%250Athe%2520existing%2520public%2520datasets%2520cannot%2520support%2520related%2520research%2520and%2520applications.%250AIn%2520this%2520paper%252C%2520we%2520first%2520propose%2520a%2520high-quality%2520open-sourced%2520UHD-4K%2520%252822.4%255C%2525%2520of%250Awhich%2520are%25208K%2529%2520text-to-video%2520dataset%2520named%2520UltraVideo%252C%2520which%2520contains%2520a%2520wide%250Arange%2520of%2520topics%2520%2528more%2520than%2520100%2520kinds%2529%252C%2520and%2520each%2520video%2520has%25209%2520structured%2520captions%250Awith%2520one%2520summarized%2520caption%2520%2528average%2520of%2520824%2520words%2529.%2520Specifically%252C%2520we%2520carefully%250Adesign%2520a%2520highly%2520automated%2520curation%2520process%2520with%2520four%2520stages%2520to%2520obtain%2520the%2520final%250Ahigh-quality%2520dataset%253A%2520%255Ctextit%257Bi%2529%257D%2520collection%2520of%2520diverse%2520and%2520high-quality%2520video%250Aclips.%2520%255Ctextit%257Bii%2529%257D%2520statistical%2520data%2520filtering.%2520%255Ctextit%257Biii%2529%257D%2520model-based%2520data%250Apurification.%2520%255Ctextit%257Biv%2529%257D%2520generation%2520of%2520comprehensive%252C%2520structured%2520captions.%2520In%250Aaddition%252C%2520we%2520expand%2520Wan%2520to%2520UltraWan-1K/-4K%252C%2520which%2520can%2520natively%2520generate%250Ahigh-quality%25201K/4K%2520videos%2520with%2520more%2520consistent%2520text%2520controllability%252C%250Ademonstrating%2520the%2520effectiveness%2520of%2520our%2520data%2520curation.We%2520believe%2520that%2520this%2520work%250Acan%2520make%2520a%2520significant%2520contribution%2520to%2520future%2520research%2520on%2520UHD%2520video%2520generation.%250AUltraVideo%2520dataset%2520and%2520UltraWan%2520models%2520are%2520available%2520at%250Ahttps%253A//xzc-zju.github.io/projects/UltraVideo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UltraVideo%3A%20High-Quality%20UHD%20Video%20Dataset%20with%20Comprehensive%20Captions&entry.906535625=Zhucun%20Xue%20and%20Jiangning%20Zhang%20and%20Teng%20Hu%20and%20Haoyang%20He%20and%20Yinan%20Chen%20and%20Yuxuan%20Cai%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Yong%20Liu%20and%20Xiangtai%20Li%20and%20Dacheng%20Tao&entry.1292438233=%20%20The%20quality%20of%20the%20video%20dataset%20%28image%20quality%2C%20resolution%2C%20and%20fine-grained%0Acaption%29%20greatly%20influences%20the%20performance%20of%20the%20video%20generation%20model.%20The%0Agrowing%20demand%20for%20video%20applications%20sets%20higher%20requirements%20for%20high-quality%0Avideo%20generation%20models.%20For%20example%2C%20the%20generation%20of%20movie-level%20Ultra-High%0ADefinition%20%28UHD%29%20videos%20and%20the%20creation%20of%204K%20short%20video%20content.%20However%2C%0Athe%20existing%20public%20datasets%20cannot%20support%20related%20research%20and%20applications.%0AIn%20this%20paper%2C%20we%20first%20propose%20a%20high-quality%20open-sourced%20UHD-4K%20%2822.4%5C%25%20of%0Awhich%20are%208K%29%20text-to-video%20dataset%20named%20UltraVideo%2C%20which%20contains%20a%20wide%0Arange%20of%20topics%20%28more%20than%20100%20kinds%29%2C%20and%20each%20video%20has%209%20structured%20captions%0Awith%20one%20summarized%20caption%20%28average%20of%20824%20words%29.%20Specifically%2C%20we%20carefully%0Adesign%20a%20highly%20automated%20curation%20process%20with%20four%20stages%20to%20obtain%20the%20final%0Ahigh-quality%20dataset%3A%20%5Ctextit%7Bi%29%7D%20collection%20of%20diverse%20and%20high-quality%20video%0Aclips.%20%5Ctextit%7Bii%29%7D%20statistical%20data%20filtering.%20%5Ctextit%7Biii%29%7D%20model-based%20data%0Apurification.%20%5Ctextit%7Biv%29%7D%20generation%20of%20comprehensive%2C%20structured%20captions.%20In%0Aaddition%2C%20we%20expand%20Wan%20to%20UltraWan-1K/-4K%2C%20which%20can%20natively%20generate%0Ahigh-quality%201K/4K%20videos%20with%20more%20consistent%20text%20controllability%2C%0Ademonstrating%20the%20effectiveness%20of%20our%20data%20curation.We%20believe%20that%20this%20work%0Acan%20make%20a%20significant%20contribution%20to%20future%20research%20on%20UHD%20video%20generation.%0AUltraVideo%20dataset%20and%20UltraWan%20models%20are%20available%20at%0Ahttps%3A//xzc-zju.github.io/projects/UltraVideo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13691v1&entry.124074799=Read"},
{"title": "A Technical Study into Small Reasoning Language Models", "author": "Xialie Zhuang and Peixian Ma and Zhikai Jia and Zheng Cao and Shiwei Liu", "abstract": "  The ongoing evolution of language models has led to the development of\nlarge-scale architectures that demonstrate exceptional performance across a\nwide range of tasks. However, these models come with significant computational\nand energy demands, as well as potential privacy implications. In this context,\nSmall Reasoning Language Models (SRLMs) with approximately 0.5 billion\nparameters present a compelling alternative due to their remarkable\ncomputational efficiency and cost effectiveness, particularly in\nresource-constrained environments. Despite these advantages, the limited\ncapacity of 0.5 billion parameter models poses challenges in handling complex\ntasks such as mathematical reasoning and code generation. This research\ninvestigates various training strategies, including supervised fine-tuning\n(SFT), knowledge distillation (KD), and reinforcement learning (RL), as well as\ntheir hybrid implementations, to enhance the performance of 0.5B SRLMs. We\nanalyze effective methodologies to bridge the performance gap between SRLMS and\nlarger models and present insights into optimal training pipelines tailored for\nthese smaller architectures. Through extensive experimental validation and\nanalysis, our work aims to provide actionable recommendations for maximizing\nthe reasoning capabilities of 0.5B models.\n", "link": "http://arxiv.org/abs/2506.13404v1", "date": "2025-06-16", "relevancy": 2.1318, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5376}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5376}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Technical%20Study%20into%20Small%20Reasoning%20Language%20Models&body=Title%3A%20A%20Technical%20Study%20into%20Small%20Reasoning%20Language%20Models%0AAuthor%3A%20Xialie%20Zhuang%20and%20Peixian%20Ma%20and%20Zhikai%20Jia%20and%20Zheng%20Cao%20and%20Shiwei%20Liu%0AAbstract%3A%20%20%20The%20ongoing%20evolution%20of%20language%20models%20has%20led%20to%20the%20development%20of%0Alarge-scale%20architectures%20that%20demonstrate%20exceptional%20performance%20across%20a%0Awide%20range%20of%20tasks.%20However%2C%20these%20models%20come%20with%20significant%20computational%0Aand%20energy%20demands%2C%20as%20well%20as%20potential%20privacy%20implications.%20In%20this%20context%2C%0ASmall%20Reasoning%20Language%20Models%20%28SRLMs%29%20with%20approximately%200.5%20billion%0Aparameters%20present%20a%20compelling%20alternative%20due%20to%20their%20remarkable%0Acomputational%20efficiency%20and%20cost%20effectiveness%2C%20particularly%20in%0Aresource-constrained%20environments.%20Despite%20these%20advantages%2C%20the%20limited%0Acapacity%20of%200.5%20billion%20parameter%20models%20poses%20challenges%20in%20handling%20complex%0Atasks%20such%20as%20mathematical%20reasoning%20and%20code%20generation.%20This%20research%0Ainvestigates%20various%20training%20strategies%2C%20including%20supervised%20fine-tuning%0A%28SFT%29%2C%20knowledge%20distillation%20%28KD%29%2C%20and%20reinforcement%20learning%20%28RL%29%2C%20as%20well%20as%0Atheir%20hybrid%20implementations%2C%20to%20enhance%20the%20performance%20of%200.5B%20SRLMs.%20We%0Aanalyze%20effective%20methodologies%20to%20bridge%20the%20performance%20gap%20between%20SRLMS%20and%0Alarger%20models%20and%20present%20insights%20into%20optimal%20training%20pipelines%20tailored%20for%0Athese%20smaller%20architectures.%20Through%20extensive%20experimental%20validation%20and%0Aanalysis%2C%20our%20work%20aims%20to%20provide%20actionable%20recommendations%20for%20maximizing%0Athe%20reasoning%20capabilities%20of%200.5B%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13404v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Technical%2520Study%2520into%2520Small%2520Reasoning%2520Language%2520Models%26entry.906535625%3DXialie%2520Zhuang%2520and%2520Peixian%2520Ma%2520and%2520Zhikai%2520Jia%2520and%2520Zheng%2520Cao%2520and%2520Shiwei%2520Liu%26entry.1292438233%3D%2520%2520The%2520ongoing%2520evolution%2520of%2520language%2520models%2520has%2520led%2520to%2520the%2520development%2520of%250Alarge-scale%2520architectures%2520that%2520demonstrate%2520exceptional%2520performance%2520across%2520a%250Awide%2520range%2520of%2520tasks.%2520However%252C%2520these%2520models%2520come%2520with%2520significant%2520computational%250Aand%2520energy%2520demands%252C%2520as%2520well%2520as%2520potential%2520privacy%2520implications.%2520In%2520this%2520context%252C%250ASmall%2520Reasoning%2520Language%2520Models%2520%2528SRLMs%2529%2520with%2520approximately%25200.5%2520billion%250Aparameters%2520present%2520a%2520compelling%2520alternative%2520due%2520to%2520their%2520remarkable%250Acomputational%2520efficiency%2520and%2520cost%2520effectiveness%252C%2520particularly%2520in%250Aresource-constrained%2520environments.%2520Despite%2520these%2520advantages%252C%2520the%2520limited%250Acapacity%2520of%25200.5%2520billion%2520parameter%2520models%2520poses%2520challenges%2520in%2520handling%2520complex%250Atasks%2520such%2520as%2520mathematical%2520reasoning%2520and%2520code%2520generation.%2520This%2520research%250Ainvestigates%2520various%2520training%2520strategies%252C%2520including%2520supervised%2520fine-tuning%250A%2528SFT%2529%252C%2520knowledge%2520distillation%2520%2528KD%2529%252C%2520and%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520as%2520well%2520as%250Atheir%2520hybrid%2520implementations%252C%2520to%2520enhance%2520the%2520performance%2520of%25200.5B%2520SRLMs.%2520We%250Aanalyze%2520effective%2520methodologies%2520to%2520bridge%2520the%2520performance%2520gap%2520between%2520SRLMS%2520and%250Alarger%2520models%2520and%2520present%2520insights%2520into%2520optimal%2520training%2520pipelines%2520tailored%2520for%250Athese%2520smaller%2520architectures.%2520Through%2520extensive%2520experimental%2520validation%2520and%250Aanalysis%252C%2520our%2520work%2520aims%2520to%2520provide%2520actionable%2520recommendations%2520for%2520maximizing%250Athe%2520reasoning%2520capabilities%2520of%25200.5B%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13404v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Technical%20Study%20into%20Small%20Reasoning%20Language%20Models&entry.906535625=Xialie%20Zhuang%20and%20Peixian%20Ma%20and%20Zhikai%20Jia%20and%20Zheng%20Cao%20and%20Shiwei%20Liu&entry.1292438233=%20%20The%20ongoing%20evolution%20of%20language%20models%20has%20led%20to%20the%20development%20of%0Alarge-scale%20architectures%20that%20demonstrate%20exceptional%20performance%20across%20a%0Awide%20range%20of%20tasks.%20However%2C%20these%20models%20come%20with%20significant%20computational%0Aand%20energy%20demands%2C%20as%20well%20as%20potential%20privacy%20implications.%20In%20this%20context%2C%0ASmall%20Reasoning%20Language%20Models%20%28SRLMs%29%20with%20approximately%200.5%20billion%0Aparameters%20present%20a%20compelling%20alternative%20due%20to%20their%20remarkable%0Acomputational%20efficiency%20and%20cost%20effectiveness%2C%20particularly%20in%0Aresource-constrained%20environments.%20Despite%20these%20advantages%2C%20the%20limited%0Acapacity%20of%200.5%20billion%20parameter%20models%20poses%20challenges%20in%20handling%20complex%0Atasks%20such%20as%20mathematical%20reasoning%20and%20code%20generation.%20This%20research%0Ainvestigates%20various%20training%20strategies%2C%20including%20supervised%20fine-tuning%0A%28SFT%29%2C%20knowledge%20distillation%20%28KD%29%2C%20and%20reinforcement%20learning%20%28RL%29%2C%20as%20well%20as%0Atheir%20hybrid%20implementations%2C%20to%20enhance%20the%20performance%20of%200.5B%20SRLMs.%20We%0Aanalyze%20effective%20methodologies%20to%20bridge%20the%20performance%20gap%20between%20SRLMS%20and%0Alarger%20models%20and%20present%20insights%20into%20optimal%20training%20pipelines%20tailored%20for%0Athese%20smaller%20architectures.%20Through%20extensive%20experimental%20validation%20and%0Aanalysis%2C%20our%20work%20aims%20to%20provide%20actionable%20recommendations%20for%20maximizing%0Athe%20reasoning%20capabilities%20of%200.5B%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13404v1&entry.124074799=Read"},
{"title": "Mitigating loss of variance in ensemble data assimilation: machine\n  learning-based and distance-free localizations for better covariance\n  estimation", "author": "Vinicius L. S. Silva and Gabriel S. Seabra and Alexandre A. Emerick", "abstract": "  We propose two new methods based/inspired by machine learning for tabular\ndata and distance-free localization to enhance the covariance estimations in an\nensemble data assimilation. The main goal is to enhance the data assimilation\nresults by mitigating loss of variance due to sampling errors. We also analyze\nthe suitability of several machine learning models and the balance between\naccuracy and computational cost of the covariance estimations. We introduce two\ndistance-free localization techniques leveraging machine learning methods\nspecifically tailored for tabular data. The methods are integrated into the\nEnsemble Smoother with Multiple Data Assimilation (ES-MDA) framework. The\nresults show that the proposed localizations improve covariance accuracy and\nenhance data assimilation and uncertainty quantification results. We observe\nreduced variance loss for the input variables using the proposed methods.\nFurthermore, we compare several machine learning models, assessing their\nsuitability for the problem in terms of computational cost, and quality of the\ncovariance estimation and data match. The influence of ensemble size is also\ninvestigated, providing insights into balancing accuracy and computational\nefficiency. Our findings demonstrate that certain machine learning models are\nmore suitable for this problem. This study introduces two novel methods that\nmitigate variance loss for model parameters in ensemble-based data\nassimilation, offering practical solutions that are easy to implement and do\nnot require any additional numerical simulation or hyperparameter tuning.\n", "link": "http://arxiv.org/abs/2506.13362v1", "date": "2025-06-16", "relevancy": 2.1306, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5434}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5256}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20loss%20of%20variance%20in%20ensemble%20data%20assimilation%3A%20machine%0A%20%20learning-based%20and%20distance-free%20localizations%20for%20better%20covariance%0A%20%20estimation&body=Title%3A%20Mitigating%20loss%20of%20variance%20in%20ensemble%20data%20assimilation%3A%20machine%0A%20%20learning-based%20and%20distance-free%20localizations%20for%20better%20covariance%0A%20%20estimation%0AAuthor%3A%20Vinicius%20L.%20S.%20Silva%20and%20Gabriel%20S.%20Seabra%20and%20Alexandre%20A.%20Emerick%0AAbstract%3A%20%20%20We%20propose%20two%20new%20methods%20based/inspired%20by%20machine%20learning%20for%20tabular%0Adata%20and%20distance-free%20localization%20to%20enhance%20the%20covariance%20estimations%20in%20an%0Aensemble%20data%20assimilation.%20The%20main%20goal%20is%20to%20enhance%20the%20data%20assimilation%0Aresults%20by%20mitigating%20loss%20of%20variance%20due%20to%20sampling%20errors.%20We%20also%20analyze%0Athe%20suitability%20of%20several%20machine%20learning%20models%20and%20the%20balance%20between%0Aaccuracy%20and%20computational%20cost%20of%20the%20covariance%20estimations.%20We%20introduce%20two%0Adistance-free%20localization%20techniques%20leveraging%20machine%20learning%20methods%0Aspecifically%20tailored%20for%20tabular%20data.%20The%20methods%20are%20integrated%20into%20the%0AEnsemble%20Smoother%20with%20Multiple%20Data%20Assimilation%20%28ES-MDA%29%20framework.%20The%0Aresults%20show%20that%20the%20proposed%20localizations%20improve%20covariance%20accuracy%20and%0Aenhance%20data%20assimilation%20and%20uncertainty%20quantification%20results.%20We%20observe%0Areduced%20variance%20loss%20for%20the%20input%20variables%20using%20the%20proposed%20methods.%0AFurthermore%2C%20we%20compare%20several%20machine%20learning%20models%2C%20assessing%20their%0Asuitability%20for%20the%20problem%20in%20terms%20of%20computational%20cost%2C%20and%20quality%20of%20the%0Acovariance%20estimation%20and%20data%20match.%20The%20influence%20of%20ensemble%20size%20is%20also%0Ainvestigated%2C%20providing%20insights%20into%20balancing%20accuracy%20and%20computational%0Aefficiency.%20Our%20findings%20demonstrate%20that%20certain%20machine%20learning%20models%20are%0Amore%20suitable%20for%20this%20problem.%20This%20study%20introduces%20two%20novel%20methods%20that%0Amitigate%20variance%20loss%20for%20model%20parameters%20in%20ensemble-based%20data%0Aassimilation%2C%20offering%20practical%20solutions%20that%20are%20easy%20to%20implement%20and%20do%0Anot%20require%20any%20additional%20numerical%20simulation%20or%20hyperparameter%20tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13362v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520loss%2520of%2520variance%2520in%2520ensemble%2520data%2520assimilation%253A%2520machine%250A%2520%2520learning-based%2520and%2520distance-free%2520localizations%2520for%2520better%2520covariance%250A%2520%2520estimation%26entry.906535625%3DVinicius%2520L.%2520S.%2520Silva%2520and%2520Gabriel%2520S.%2520Seabra%2520and%2520Alexandre%2520A.%2520Emerick%26entry.1292438233%3D%2520%2520We%2520propose%2520two%2520new%2520methods%2520based/inspired%2520by%2520machine%2520learning%2520for%2520tabular%250Adata%2520and%2520distance-free%2520localization%2520to%2520enhance%2520the%2520covariance%2520estimations%2520in%2520an%250Aensemble%2520data%2520assimilation.%2520The%2520main%2520goal%2520is%2520to%2520enhance%2520the%2520data%2520assimilation%250Aresults%2520by%2520mitigating%2520loss%2520of%2520variance%2520due%2520to%2520sampling%2520errors.%2520We%2520also%2520analyze%250Athe%2520suitability%2520of%2520several%2520machine%2520learning%2520models%2520and%2520the%2520balance%2520between%250Aaccuracy%2520and%2520computational%2520cost%2520of%2520the%2520covariance%2520estimations.%2520We%2520introduce%2520two%250Adistance-free%2520localization%2520techniques%2520leveraging%2520machine%2520learning%2520methods%250Aspecifically%2520tailored%2520for%2520tabular%2520data.%2520The%2520methods%2520are%2520integrated%2520into%2520the%250AEnsemble%2520Smoother%2520with%2520Multiple%2520Data%2520Assimilation%2520%2528ES-MDA%2529%2520framework.%2520The%250Aresults%2520show%2520that%2520the%2520proposed%2520localizations%2520improve%2520covariance%2520accuracy%2520and%250Aenhance%2520data%2520assimilation%2520and%2520uncertainty%2520quantification%2520results.%2520We%2520observe%250Areduced%2520variance%2520loss%2520for%2520the%2520input%2520variables%2520using%2520the%2520proposed%2520methods.%250AFurthermore%252C%2520we%2520compare%2520several%2520machine%2520learning%2520models%252C%2520assessing%2520their%250Asuitability%2520for%2520the%2520problem%2520in%2520terms%2520of%2520computational%2520cost%252C%2520and%2520quality%2520of%2520the%250Acovariance%2520estimation%2520and%2520data%2520match.%2520The%2520influence%2520of%2520ensemble%2520size%2520is%2520also%250Ainvestigated%252C%2520providing%2520insights%2520into%2520balancing%2520accuracy%2520and%2520computational%250Aefficiency.%2520Our%2520findings%2520demonstrate%2520that%2520certain%2520machine%2520learning%2520models%2520are%250Amore%2520suitable%2520for%2520this%2520problem.%2520This%2520study%2520introduces%2520two%2520novel%2520methods%2520that%250Amitigate%2520variance%2520loss%2520for%2520model%2520parameters%2520in%2520ensemble-based%2520data%250Aassimilation%252C%2520offering%2520practical%2520solutions%2520that%2520are%2520easy%2520to%2520implement%2520and%2520do%250Anot%2520require%2520any%2520additional%2520numerical%2520simulation%2520or%2520hyperparameter%2520tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13362v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20loss%20of%20variance%20in%20ensemble%20data%20assimilation%3A%20machine%0A%20%20learning-based%20and%20distance-free%20localizations%20for%20better%20covariance%0A%20%20estimation&entry.906535625=Vinicius%20L.%20S.%20Silva%20and%20Gabriel%20S.%20Seabra%20and%20Alexandre%20A.%20Emerick&entry.1292438233=%20%20We%20propose%20two%20new%20methods%20based/inspired%20by%20machine%20learning%20for%20tabular%0Adata%20and%20distance-free%20localization%20to%20enhance%20the%20covariance%20estimations%20in%20an%0Aensemble%20data%20assimilation.%20The%20main%20goal%20is%20to%20enhance%20the%20data%20assimilation%0Aresults%20by%20mitigating%20loss%20of%20variance%20due%20to%20sampling%20errors.%20We%20also%20analyze%0Athe%20suitability%20of%20several%20machine%20learning%20models%20and%20the%20balance%20between%0Aaccuracy%20and%20computational%20cost%20of%20the%20covariance%20estimations.%20We%20introduce%20two%0Adistance-free%20localization%20techniques%20leveraging%20machine%20learning%20methods%0Aspecifically%20tailored%20for%20tabular%20data.%20The%20methods%20are%20integrated%20into%20the%0AEnsemble%20Smoother%20with%20Multiple%20Data%20Assimilation%20%28ES-MDA%29%20framework.%20The%0Aresults%20show%20that%20the%20proposed%20localizations%20improve%20covariance%20accuracy%20and%0Aenhance%20data%20assimilation%20and%20uncertainty%20quantification%20results.%20We%20observe%0Areduced%20variance%20loss%20for%20the%20input%20variables%20using%20the%20proposed%20methods.%0AFurthermore%2C%20we%20compare%20several%20machine%20learning%20models%2C%20assessing%20their%0Asuitability%20for%20the%20problem%20in%20terms%20of%20computational%20cost%2C%20and%20quality%20of%20the%0Acovariance%20estimation%20and%20data%20match.%20The%20influence%20of%20ensemble%20size%20is%20also%0Ainvestigated%2C%20providing%20insights%20into%20balancing%20accuracy%20and%20computational%0Aefficiency.%20Our%20findings%20demonstrate%20that%20certain%20machine%20learning%20models%20are%0Amore%20suitable%20for%20this%20problem.%20This%20study%20introduces%20two%20novel%20methods%20that%0Amitigate%20variance%20loss%20for%20model%20parameters%20in%20ensemble-based%20data%0Aassimilation%2C%20offering%20practical%20solutions%20that%20are%20easy%20to%20implement%20and%20do%0Anot%20require%20any%20additional%20numerical%20simulation%20or%20hyperparameter%20tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13362v1&entry.124074799=Read"},
{"title": "CALM: Consensus-Aware Localized Merging for Multi-Task Learning", "author": "Kunda Yan and Min Zhang and Sen Cui and Zikun Qu and Bo Jiang and Feng Liu and Changshui Zhang", "abstract": "  Model merging aims to integrate the strengths of multiple fine-tuned models\ninto a unified model while preserving task-specific capabilities. Existing\nmethods, represented by task arithmetic, are typically classified into global-\nand local-aware methods. However, global-aware methods inevitably cause\nparameter interference, while local-aware methods struggle to maintain the\neffectiveness of task-specific details in the merged model. To address these\nlimitations, we propose a Consensus-Aware Localized Merging (CALM) method which\nincorporates localized information aligned with global task consensus, ensuring\nits effectiveness post-merging. CALM consists of three key components: (1)\nclass-balanced entropy minimization sampling, providing a more flexible and\nreliable way to leverage unsupervised data; (2) an efficient-aware framework,\nselecting a small set of tasks for sequential merging with high scalability;\n(3) a consensus-aware mask optimization, aligning localized binary masks with\nglobal task consensus and merging them conflict-free. Experiments demonstrate\nthe superiority and robustness of our CALM, significantly outperforming\nexisting methods and achieving performance close to traditional MTL.\n", "link": "http://arxiv.org/abs/2506.13406v1", "date": "2025-06-16", "relevancy": 2.1272, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5545}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5185}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CALM%3A%20Consensus-Aware%20Localized%20Merging%20for%20Multi-Task%20Learning&body=Title%3A%20CALM%3A%20Consensus-Aware%20Localized%20Merging%20for%20Multi-Task%20Learning%0AAuthor%3A%20Kunda%20Yan%20and%20Min%20Zhang%20and%20Sen%20Cui%20and%20Zikun%20Qu%20and%20Bo%20Jiang%20and%20Feng%20Liu%20and%20Changshui%20Zhang%0AAbstract%3A%20%20%20Model%20merging%20aims%20to%20integrate%20the%20strengths%20of%20multiple%20fine-tuned%20models%0Ainto%20a%20unified%20model%20while%20preserving%20task-specific%20capabilities.%20Existing%0Amethods%2C%20represented%20by%20task%20arithmetic%2C%20are%20typically%20classified%20into%20global-%0Aand%20local-aware%20methods.%20However%2C%20global-aware%20methods%20inevitably%20cause%0Aparameter%20interference%2C%20while%20local-aware%20methods%20struggle%20to%20maintain%20the%0Aeffectiveness%20of%20task-specific%20details%20in%20the%20merged%20model.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20Consensus-Aware%20Localized%20Merging%20%28CALM%29%20method%20which%0Aincorporates%20localized%20information%20aligned%20with%20global%20task%20consensus%2C%20ensuring%0Aits%20effectiveness%20post-merging.%20CALM%20consists%20of%20three%20key%20components%3A%20%281%29%0Aclass-balanced%20entropy%20minimization%20sampling%2C%20providing%20a%20more%20flexible%20and%0Areliable%20way%20to%20leverage%20unsupervised%20data%3B%20%282%29%20an%20efficient-aware%20framework%2C%0Aselecting%20a%20small%20set%20of%20tasks%20for%20sequential%20merging%20with%20high%20scalability%3B%0A%283%29%20a%20consensus-aware%20mask%20optimization%2C%20aligning%20localized%20binary%20masks%20with%0Aglobal%20task%20consensus%20and%20merging%20them%20conflict-free.%20Experiments%20demonstrate%0Athe%20superiority%20and%20robustness%20of%20our%20CALM%2C%20significantly%20outperforming%0Aexisting%20methods%20and%20achieving%20performance%20close%20to%20traditional%20MTL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCALM%253A%2520Consensus-Aware%2520Localized%2520Merging%2520for%2520Multi-Task%2520Learning%26entry.906535625%3DKunda%2520Yan%2520and%2520Min%2520Zhang%2520and%2520Sen%2520Cui%2520and%2520Zikun%2520Qu%2520and%2520Bo%2520Jiang%2520and%2520Feng%2520Liu%2520and%2520Changshui%2520Zhang%26entry.1292438233%3D%2520%2520Model%2520merging%2520aims%2520to%2520integrate%2520the%2520strengths%2520of%2520multiple%2520fine-tuned%2520models%250Ainto%2520a%2520unified%2520model%2520while%2520preserving%2520task-specific%2520capabilities.%2520Existing%250Amethods%252C%2520represented%2520by%2520task%2520arithmetic%252C%2520are%2520typically%2520classified%2520into%2520global-%250Aand%2520local-aware%2520methods.%2520However%252C%2520global-aware%2520methods%2520inevitably%2520cause%250Aparameter%2520interference%252C%2520while%2520local-aware%2520methods%2520struggle%2520to%2520maintain%2520the%250Aeffectiveness%2520of%2520task-specific%2520details%2520in%2520the%2520merged%2520model.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520Consensus-Aware%2520Localized%2520Merging%2520%2528CALM%2529%2520method%2520which%250Aincorporates%2520localized%2520information%2520aligned%2520with%2520global%2520task%2520consensus%252C%2520ensuring%250Aits%2520effectiveness%2520post-merging.%2520CALM%2520consists%2520of%2520three%2520key%2520components%253A%2520%25281%2529%250Aclass-balanced%2520entropy%2520minimization%2520sampling%252C%2520providing%2520a%2520more%2520flexible%2520and%250Areliable%2520way%2520to%2520leverage%2520unsupervised%2520data%253B%2520%25282%2529%2520an%2520efficient-aware%2520framework%252C%250Aselecting%2520a%2520small%2520set%2520of%2520tasks%2520for%2520sequential%2520merging%2520with%2520high%2520scalability%253B%250A%25283%2529%2520a%2520consensus-aware%2520mask%2520optimization%252C%2520aligning%2520localized%2520binary%2520masks%2520with%250Aglobal%2520task%2520consensus%2520and%2520merging%2520them%2520conflict-free.%2520Experiments%2520demonstrate%250Athe%2520superiority%2520and%2520robustness%2520of%2520our%2520CALM%252C%2520significantly%2520outperforming%250Aexisting%2520methods%2520and%2520achieving%2520performance%2520close%2520to%2520traditional%2520MTL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CALM%3A%20Consensus-Aware%20Localized%20Merging%20for%20Multi-Task%20Learning&entry.906535625=Kunda%20Yan%20and%20Min%20Zhang%20and%20Sen%20Cui%20and%20Zikun%20Qu%20and%20Bo%20Jiang%20and%20Feng%20Liu%20and%20Changshui%20Zhang&entry.1292438233=%20%20Model%20merging%20aims%20to%20integrate%20the%20strengths%20of%20multiple%20fine-tuned%20models%0Ainto%20a%20unified%20model%20while%20preserving%20task-specific%20capabilities.%20Existing%0Amethods%2C%20represented%20by%20task%20arithmetic%2C%20are%20typically%20classified%20into%20global-%0Aand%20local-aware%20methods.%20However%2C%20global-aware%20methods%20inevitably%20cause%0Aparameter%20interference%2C%20while%20local-aware%20methods%20struggle%20to%20maintain%20the%0Aeffectiveness%20of%20task-specific%20details%20in%20the%20merged%20model.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20Consensus-Aware%20Localized%20Merging%20%28CALM%29%20method%20which%0Aincorporates%20localized%20information%20aligned%20with%20global%20task%20consensus%2C%20ensuring%0Aits%20effectiveness%20post-merging.%20CALM%20consists%20of%20three%20key%20components%3A%20%281%29%0Aclass-balanced%20entropy%20minimization%20sampling%2C%20providing%20a%20more%20flexible%20and%0Areliable%20way%20to%20leverage%20unsupervised%20data%3B%20%282%29%20an%20efficient-aware%20framework%2C%0Aselecting%20a%20small%20set%20of%20tasks%20for%20sequential%20merging%20with%20high%20scalability%3B%0A%283%29%20a%20consensus-aware%20mask%20optimization%2C%20aligning%20localized%20binary%20masks%20with%0Aglobal%20task%20consensus%20and%20merging%20them%20conflict-free.%20Experiments%20demonstrate%0Athe%20superiority%20and%20robustness%20of%20our%20CALM%2C%20significantly%20outperforming%0Aexisting%20methods%20and%20achieving%20performance%20close%20to%20traditional%20MTL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13406v1&entry.124074799=Read"},
{"title": "Uncertainty-Aware Remaining Lifespan Prediction from Images", "author": "Tristan Kenneweg and Philip Kenneweg and Barbara Hammer", "abstract": "  Predicting mortality-related outcomes from images offers the prospect of\naccessible, noninvasive, and scalable health screening. We present a method\nthat leverages pretrained vision transformer foundation models to estimate\nremaining lifespan from facial and whole-body images, alongside robust\nuncertainty quantification. We show that predictive uncertainty varies\nsystematically with the true remaining lifespan, and that this uncertainty can\nbe effectively modeled by learning a Gaussian distribution for each sample. Our\napproach achieves state-of-the-art mean absolute error (MAE) of 7.48 years on\nan established Dataset, and further improves to 4.79 and 5.07 years MAE on two\nnew, higher-quality datasets curated and published in this work. Importantly,\nour models provide well-calibrated uncertainty estimates, as demonstrated by a\nbucketed expected calibration error of 0.62 years. While not intended for\nclinical deployment, these results highlight the potential of extracting\nmedically relevant signals from images. We make all code and datasets available\nto facilitate further research.\n", "link": "http://arxiv.org/abs/2506.13430v1", "date": "2025-06-16", "relevancy": 2.1221, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5466}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5411}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-Aware%20Remaining%20Lifespan%20Prediction%20from%20Images&body=Title%3A%20Uncertainty-Aware%20Remaining%20Lifespan%20Prediction%20from%20Images%0AAuthor%3A%20Tristan%20Kenneweg%20and%20Philip%20Kenneweg%20and%20Barbara%20Hammer%0AAbstract%3A%20%20%20Predicting%20mortality-related%20outcomes%20from%20images%20offers%20the%20prospect%20of%0Aaccessible%2C%20noninvasive%2C%20and%20scalable%20health%20screening.%20We%20present%20a%20method%0Athat%20leverages%20pretrained%20vision%20transformer%20foundation%20models%20to%20estimate%0Aremaining%20lifespan%20from%20facial%20and%20whole-body%20images%2C%20alongside%20robust%0Auncertainty%20quantification.%20We%20show%20that%20predictive%20uncertainty%20varies%0Asystematically%20with%20the%20true%20remaining%20lifespan%2C%20and%20that%20this%20uncertainty%20can%0Abe%20effectively%20modeled%20by%20learning%20a%20Gaussian%20distribution%20for%20each%20sample.%20Our%0Aapproach%20achieves%20state-of-the-art%20mean%20absolute%20error%20%28MAE%29%20of%207.48%20years%20on%0Aan%20established%20Dataset%2C%20and%20further%20improves%20to%204.79%20and%205.07%20years%20MAE%20on%20two%0Anew%2C%20higher-quality%20datasets%20curated%20and%20published%20in%20this%20work.%20Importantly%2C%0Aour%20models%20provide%20well-calibrated%20uncertainty%20estimates%2C%20as%20demonstrated%20by%20a%0Abucketed%20expected%20calibration%20error%20of%200.62%20years.%20While%20not%20intended%20for%0Aclinical%20deployment%2C%20these%20results%20highlight%20the%20potential%20of%20extracting%0Amedically%20relevant%20signals%20from%20images.%20We%20make%20all%20code%20and%20datasets%20available%0Ato%20facilitate%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-Aware%2520Remaining%2520Lifespan%2520Prediction%2520from%2520Images%26entry.906535625%3DTristan%2520Kenneweg%2520and%2520Philip%2520Kenneweg%2520and%2520Barbara%2520Hammer%26entry.1292438233%3D%2520%2520Predicting%2520mortality-related%2520outcomes%2520from%2520images%2520offers%2520the%2520prospect%2520of%250Aaccessible%252C%2520noninvasive%252C%2520and%2520scalable%2520health%2520screening.%2520We%2520present%2520a%2520method%250Athat%2520leverages%2520pretrained%2520vision%2520transformer%2520foundation%2520models%2520to%2520estimate%250Aremaining%2520lifespan%2520from%2520facial%2520and%2520whole-body%2520images%252C%2520alongside%2520robust%250Auncertainty%2520quantification.%2520We%2520show%2520that%2520predictive%2520uncertainty%2520varies%250Asystematically%2520with%2520the%2520true%2520remaining%2520lifespan%252C%2520and%2520that%2520this%2520uncertainty%2520can%250Abe%2520effectively%2520modeled%2520by%2520learning%2520a%2520Gaussian%2520distribution%2520for%2520each%2520sample.%2520Our%250Aapproach%2520achieves%2520state-of-the-art%2520mean%2520absolute%2520error%2520%2528MAE%2529%2520of%25207.48%2520years%2520on%250Aan%2520established%2520Dataset%252C%2520and%2520further%2520improves%2520to%25204.79%2520and%25205.07%2520years%2520MAE%2520on%2520two%250Anew%252C%2520higher-quality%2520datasets%2520curated%2520and%2520published%2520in%2520this%2520work.%2520Importantly%252C%250Aour%2520models%2520provide%2520well-calibrated%2520uncertainty%2520estimates%252C%2520as%2520demonstrated%2520by%2520a%250Abucketed%2520expected%2520calibration%2520error%2520of%25200.62%2520years.%2520While%2520not%2520intended%2520for%250Aclinical%2520deployment%252C%2520these%2520results%2520highlight%2520the%2520potential%2520of%2520extracting%250Amedically%2520relevant%2520signals%2520from%2520images.%2520We%2520make%2520all%2520code%2520and%2520datasets%2520available%250Ato%2520facilitate%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-Aware%20Remaining%20Lifespan%20Prediction%20from%20Images&entry.906535625=Tristan%20Kenneweg%20and%20Philip%20Kenneweg%20and%20Barbara%20Hammer&entry.1292438233=%20%20Predicting%20mortality-related%20outcomes%20from%20images%20offers%20the%20prospect%20of%0Aaccessible%2C%20noninvasive%2C%20and%20scalable%20health%20screening.%20We%20present%20a%20method%0Athat%20leverages%20pretrained%20vision%20transformer%20foundation%20models%20to%20estimate%0Aremaining%20lifespan%20from%20facial%20and%20whole-body%20images%2C%20alongside%20robust%0Auncertainty%20quantification.%20We%20show%20that%20predictive%20uncertainty%20varies%0Asystematically%20with%20the%20true%20remaining%20lifespan%2C%20and%20that%20this%20uncertainty%20can%0Abe%20effectively%20modeled%20by%20learning%20a%20Gaussian%20distribution%20for%20each%20sample.%20Our%0Aapproach%20achieves%20state-of-the-art%20mean%20absolute%20error%20%28MAE%29%20of%207.48%20years%20on%0Aan%20established%20Dataset%2C%20and%20further%20improves%20to%204.79%20and%205.07%20years%20MAE%20on%20two%0Anew%2C%20higher-quality%20datasets%20curated%20and%20published%20in%20this%20work.%20Importantly%2C%0Aour%20models%20provide%20well-calibrated%20uncertainty%20estimates%2C%20as%20demonstrated%20by%20a%0Abucketed%20expected%20calibration%20error%20of%200.62%20years.%20While%20not%20intended%20for%0Aclinical%20deployment%2C%20these%20results%20highlight%20the%20potential%20of%20extracting%0Amedically%20relevant%20signals%20from%20images.%20We%20make%20all%20code%20and%20datasets%20available%0Ato%20facilitate%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13430v1&entry.124074799=Read"},
{"title": "UAV Object Detection and Positioning in a Mining Industrial Metaverse\n  with Custom Geo-Referenced Data", "author": "Vasiliki Balaska and Ioannis Tsampikos Papapetros and Katerina Maria Oikonomou and Loukas Bampis and Antonios Gasteratos", "abstract": "  The mining sector increasingly adopts digital tools to improve operational\nefficiency, safety, and data-driven decision-making. One of the key challenges\nremains the reliable acquisition of high-resolution, geo-referenced spatial\ninformation to support core activities such as extraction planning and on-site\nmonitoring. This work presents an integrated system architecture that combines\nUAV-based sensing, LiDAR terrain modeling, and deep learning-based object\ndetection to generate spatially accurate information for open-pit mining\nenvironments. The proposed pipeline includes geo-referencing, 3D\nreconstruction, and object localization, enabling structured spatial outputs to\nbe integrated into an industrial digital twin platform. Unlike traditional\nstatic surveying methods, the system offers higher coverage and automation\npotential, with modular components suitable for deployment in real-world\nindustrial contexts. While the current implementation operates in post-flight\nbatch mode, it lays the foundation for real-time extensions. The system\ncontributes to the development of AI-enhanced remote sensing in mining by\ndemonstrating a scalable and field-validated geospatial data workflow that\nsupports situational awareness and infrastructure safety.\n", "link": "http://arxiv.org/abs/2506.13505v1", "date": "2025-06-16", "relevancy": 2.1161, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.555}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5176}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UAV%20Object%20Detection%20and%20Positioning%20in%20a%20Mining%20Industrial%20Metaverse%0A%20%20with%20Custom%20Geo-Referenced%20Data&body=Title%3A%20UAV%20Object%20Detection%20and%20Positioning%20in%20a%20Mining%20Industrial%20Metaverse%0A%20%20with%20Custom%20Geo-Referenced%20Data%0AAuthor%3A%20Vasiliki%20Balaska%20and%20Ioannis%20Tsampikos%20Papapetros%20and%20Katerina%20Maria%20Oikonomou%20and%20Loukas%20Bampis%20and%20Antonios%20Gasteratos%0AAbstract%3A%20%20%20The%20mining%20sector%20increasingly%20adopts%20digital%20tools%20to%20improve%20operational%0Aefficiency%2C%20safety%2C%20and%20data-driven%20decision-making.%20One%20of%20the%20key%20challenges%0Aremains%20the%20reliable%20acquisition%20of%20high-resolution%2C%20geo-referenced%20spatial%0Ainformation%20to%20support%20core%20activities%20such%20as%20extraction%20planning%20and%20on-site%0Amonitoring.%20This%20work%20presents%20an%20integrated%20system%20architecture%20that%20combines%0AUAV-based%20sensing%2C%20LiDAR%20terrain%20modeling%2C%20and%20deep%20learning-based%20object%0Adetection%20to%20generate%20spatially%20accurate%20information%20for%20open-pit%20mining%0Aenvironments.%20The%20proposed%20pipeline%20includes%20geo-referencing%2C%203D%0Areconstruction%2C%20and%20object%20localization%2C%20enabling%20structured%20spatial%20outputs%20to%0Abe%20integrated%20into%20an%20industrial%20digital%20twin%20platform.%20Unlike%20traditional%0Astatic%20surveying%20methods%2C%20the%20system%20offers%20higher%20coverage%20and%20automation%0Apotential%2C%20with%20modular%20components%20suitable%20for%20deployment%20in%20real-world%0Aindustrial%20contexts.%20While%20the%20current%20implementation%20operates%20in%20post-flight%0Abatch%20mode%2C%20it%20lays%20the%20foundation%20for%20real-time%20extensions.%20The%20system%0Acontributes%20to%20the%20development%20of%20AI-enhanced%20remote%20sensing%20in%20mining%20by%0Ademonstrating%20a%20scalable%20and%20field-validated%20geospatial%20data%20workflow%20that%0Asupports%20situational%20awareness%20and%20infrastructure%20safety.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUAV%2520Object%2520Detection%2520and%2520Positioning%2520in%2520a%2520Mining%2520Industrial%2520Metaverse%250A%2520%2520with%2520Custom%2520Geo-Referenced%2520Data%26entry.906535625%3DVasiliki%2520Balaska%2520and%2520Ioannis%2520Tsampikos%2520Papapetros%2520and%2520Katerina%2520Maria%2520Oikonomou%2520and%2520Loukas%2520Bampis%2520and%2520Antonios%2520Gasteratos%26entry.1292438233%3D%2520%2520The%2520mining%2520sector%2520increasingly%2520adopts%2520digital%2520tools%2520to%2520improve%2520operational%250Aefficiency%252C%2520safety%252C%2520and%2520data-driven%2520decision-making.%2520One%2520of%2520the%2520key%2520challenges%250Aremains%2520the%2520reliable%2520acquisition%2520of%2520high-resolution%252C%2520geo-referenced%2520spatial%250Ainformation%2520to%2520support%2520core%2520activities%2520such%2520as%2520extraction%2520planning%2520and%2520on-site%250Amonitoring.%2520This%2520work%2520presents%2520an%2520integrated%2520system%2520architecture%2520that%2520combines%250AUAV-based%2520sensing%252C%2520LiDAR%2520terrain%2520modeling%252C%2520and%2520deep%2520learning-based%2520object%250Adetection%2520to%2520generate%2520spatially%2520accurate%2520information%2520for%2520open-pit%2520mining%250Aenvironments.%2520The%2520proposed%2520pipeline%2520includes%2520geo-referencing%252C%25203D%250Areconstruction%252C%2520and%2520object%2520localization%252C%2520enabling%2520structured%2520spatial%2520outputs%2520to%250Abe%2520integrated%2520into%2520an%2520industrial%2520digital%2520twin%2520platform.%2520Unlike%2520traditional%250Astatic%2520surveying%2520methods%252C%2520the%2520system%2520offers%2520higher%2520coverage%2520and%2520automation%250Apotential%252C%2520with%2520modular%2520components%2520suitable%2520for%2520deployment%2520in%2520real-world%250Aindustrial%2520contexts.%2520While%2520the%2520current%2520implementation%2520operates%2520in%2520post-flight%250Abatch%2520mode%252C%2520it%2520lays%2520the%2520foundation%2520for%2520real-time%2520extensions.%2520The%2520system%250Acontributes%2520to%2520the%2520development%2520of%2520AI-enhanced%2520remote%2520sensing%2520in%2520mining%2520by%250Ademonstrating%2520a%2520scalable%2520and%2520field-validated%2520geospatial%2520data%2520workflow%2520that%250Asupports%2520situational%2520awareness%2520and%2520infrastructure%2520safety.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UAV%20Object%20Detection%20and%20Positioning%20in%20a%20Mining%20Industrial%20Metaverse%0A%20%20with%20Custom%20Geo-Referenced%20Data&entry.906535625=Vasiliki%20Balaska%20and%20Ioannis%20Tsampikos%20Papapetros%20and%20Katerina%20Maria%20Oikonomou%20and%20Loukas%20Bampis%20and%20Antonios%20Gasteratos&entry.1292438233=%20%20The%20mining%20sector%20increasingly%20adopts%20digital%20tools%20to%20improve%20operational%0Aefficiency%2C%20safety%2C%20and%20data-driven%20decision-making.%20One%20of%20the%20key%20challenges%0Aremains%20the%20reliable%20acquisition%20of%20high-resolution%2C%20geo-referenced%20spatial%0Ainformation%20to%20support%20core%20activities%20such%20as%20extraction%20planning%20and%20on-site%0Amonitoring.%20This%20work%20presents%20an%20integrated%20system%20architecture%20that%20combines%0AUAV-based%20sensing%2C%20LiDAR%20terrain%20modeling%2C%20and%20deep%20learning-based%20object%0Adetection%20to%20generate%20spatially%20accurate%20information%20for%20open-pit%20mining%0Aenvironments.%20The%20proposed%20pipeline%20includes%20geo-referencing%2C%203D%0Areconstruction%2C%20and%20object%20localization%2C%20enabling%20structured%20spatial%20outputs%20to%0Abe%20integrated%20into%20an%20industrial%20digital%20twin%20platform.%20Unlike%20traditional%0Astatic%20surveying%20methods%2C%20the%20system%20offers%20higher%20coverage%20and%20automation%0Apotential%2C%20with%20modular%20components%20suitable%20for%20deployment%20in%20real-world%0Aindustrial%20contexts.%20While%20the%20current%20implementation%20operates%20in%20post-flight%0Abatch%20mode%2C%20it%20lays%20the%20foundation%20for%20real-time%20extensions.%20The%20system%0Acontributes%20to%20the%20development%20of%20AI-enhanced%20remote%20sensing%20in%20mining%20by%0Ademonstrating%20a%20scalable%20and%20field-validated%20geospatial%20data%20workflow%20that%0Asupports%20situational%20awareness%20and%20infrastructure%20safety.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13505v1&entry.124074799=Read"},
{"title": "MLOmics: Cancer Multi-Omics Database for Machine Learning", "author": "Ziwei Yang and Rikuto Kotoge and Xihao Piao and Zheng Chen and Lingwei Zhu and Peng Gao and Yasuko Matsubara and Yasushi Sakurai and Jimeng Sun", "abstract": "  Framing the investigation of diverse cancers as a machine learning problem\nhas recently shown significant potential in multi-omics analysis and cancer\nresearch. Empowering these successful machine learning models are the\nhigh-quality training datasets with sufficient data volume and adequate\npreprocessing. However, while there exist several public data portals,\nincluding The Cancer Genome Atlas (TCGA) multi-omics initiative or open-bases\nsuch as the LinkedOmics, these databases are not off-the-shelf for existing\nmachine learning models. In this paper, we introduce MLOmics, an open cancer\nmulti-omics database aiming at serving better the development and evaluation of\nbioinformatics and machine learning models. MLOmics contains 8,314 patient\nsamples covering all 32 cancer types with four omics types, stratified\nfeatures, and extensive baselines. Complementary support for downstream\nanalysis and bio-knowledge linking are also included to support\ninterdisciplinary analysis.\n", "link": "http://arxiv.org/abs/2409.02143v3", "date": "2025-06-16", "relevancy": 2.11, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4421}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4119}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLOmics%3A%20Cancer%20Multi-Omics%20Database%20for%20Machine%20Learning&body=Title%3A%20MLOmics%3A%20Cancer%20Multi-Omics%20Database%20for%20Machine%20Learning%0AAuthor%3A%20Ziwei%20Yang%20and%20Rikuto%20Kotoge%20and%20Xihao%20Piao%20and%20Zheng%20Chen%20and%20Lingwei%20Zhu%20and%20Peng%20Gao%20and%20Yasuko%20Matsubara%20and%20Yasushi%20Sakurai%20and%20Jimeng%20Sun%0AAbstract%3A%20%20%20Framing%20the%20investigation%20of%20diverse%20cancers%20as%20a%20machine%20learning%20problem%0Ahas%20recently%20shown%20significant%20potential%20in%20multi-omics%20analysis%20and%20cancer%0Aresearch.%20Empowering%20these%20successful%20machine%20learning%20models%20are%20the%0Ahigh-quality%20training%20datasets%20with%20sufficient%20data%20volume%20and%20adequate%0Apreprocessing.%20However%2C%20while%20there%20exist%20several%20public%20data%20portals%2C%0Aincluding%20The%20Cancer%20Genome%20Atlas%20%28TCGA%29%20multi-omics%20initiative%20or%20open-bases%0Asuch%20as%20the%20LinkedOmics%2C%20these%20databases%20are%20not%20off-the-shelf%20for%20existing%0Amachine%20learning%20models.%20In%20this%20paper%2C%20we%20introduce%20MLOmics%2C%20an%20open%20cancer%0Amulti-omics%20database%20aiming%20at%20serving%20better%20the%20development%20and%20evaluation%20of%0Abioinformatics%20and%20machine%20learning%20models.%20MLOmics%20contains%208%2C314%20patient%0Asamples%20covering%20all%2032%20cancer%20types%20with%20four%20omics%20types%2C%20stratified%0Afeatures%2C%20and%20extensive%20baselines.%20Complementary%20support%20for%20downstream%0Aanalysis%20and%20bio-knowledge%20linking%20are%20also%20included%20to%20support%0Ainterdisciplinary%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02143v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLOmics%253A%2520Cancer%2520Multi-Omics%2520Database%2520for%2520Machine%2520Learning%26entry.906535625%3DZiwei%2520Yang%2520and%2520Rikuto%2520Kotoge%2520and%2520Xihao%2520Piao%2520and%2520Zheng%2520Chen%2520and%2520Lingwei%2520Zhu%2520and%2520Peng%2520Gao%2520and%2520Yasuko%2520Matsubara%2520and%2520Yasushi%2520Sakurai%2520and%2520Jimeng%2520Sun%26entry.1292438233%3D%2520%2520Framing%2520the%2520investigation%2520of%2520diverse%2520cancers%2520as%2520a%2520machine%2520learning%2520problem%250Ahas%2520recently%2520shown%2520significant%2520potential%2520in%2520multi-omics%2520analysis%2520and%2520cancer%250Aresearch.%2520Empowering%2520these%2520successful%2520machine%2520learning%2520models%2520are%2520the%250Ahigh-quality%2520training%2520datasets%2520with%2520sufficient%2520data%2520volume%2520and%2520adequate%250Apreprocessing.%2520However%252C%2520while%2520there%2520exist%2520several%2520public%2520data%2520portals%252C%250Aincluding%2520The%2520Cancer%2520Genome%2520Atlas%2520%2528TCGA%2529%2520multi-omics%2520initiative%2520or%2520open-bases%250Asuch%2520as%2520the%2520LinkedOmics%252C%2520these%2520databases%2520are%2520not%2520off-the-shelf%2520for%2520existing%250Amachine%2520learning%2520models.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520MLOmics%252C%2520an%2520open%2520cancer%250Amulti-omics%2520database%2520aiming%2520at%2520serving%2520better%2520the%2520development%2520and%2520evaluation%2520of%250Abioinformatics%2520and%2520machine%2520learning%2520models.%2520MLOmics%2520contains%25208%252C314%2520patient%250Asamples%2520covering%2520all%252032%2520cancer%2520types%2520with%2520four%2520omics%2520types%252C%2520stratified%250Afeatures%252C%2520and%2520extensive%2520baselines.%2520Complementary%2520support%2520for%2520downstream%250Aanalysis%2520and%2520bio-knowledge%2520linking%2520are%2520also%2520included%2520to%2520support%250Ainterdisciplinary%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02143v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLOmics%3A%20Cancer%20Multi-Omics%20Database%20for%20Machine%20Learning&entry.906535625=Ziwei%20Yang%20and%20Rikuto%20Kotoge%20and%20Xihao%20Piao%20and%20Zheng%20Chen%20and%20Lingwei%20Zhu%20and%20Peng%20Gao%20and%20Yasuko%20Matsubara%20and%20Yasushi%20Sakurai%20and%20Jimeng%20Sun&entry.1292438233=%20%20Framing%20the%20investigation%20of%20diverse%20cancers%20as%20a%20machine%20learning%20problem%0Ahas%20recently%20shown%20significant%20potential%20in%20multi-omics%20analysis%20and%20cancer%0Aresearch.%20Empowering%20these%20successful%20machine%20learning%20models%20are%20the%0Ahigh-quality%20training%20datasets%20with%20sufficient%20data%20volume%20and%20adequate%0Apreprocessing.%20However%2C%20while%20there%20exist%20several%20public%20data%20portals%2C%0Aincluding%20The%20Cancer%20Genome%20Atlas%20%28TCGA%29%20multi-omics%20initiative%20or%20open-bases%0Asuch%20as%20the%20LinkedOmics%2C%20these%20databases%20are%20not%20off-the-shelf%20for%20existing%0Amachine%20learning%20models.%20In%20this%20paper%2C%20we%20introduce%20MLOmics%2C%20an%20open%20cancer%0Amulti-omics%20database%20aiming%20at%20serving%20better%20the%20development%20and%20evaluation%20of%0Abioinformatics%20and%20machine%20learning%20models.%20MLOmics%20contains%208%2C314%20patient%0Asamples%20covering%20all%2032%20cancer%20types%20with%20four%20omics%20types%2C%20stratified%0Afeatures%2C%20and%20extensive%20baselines.%20Complementary%20support%20for%20downstream%0Aanalysis%20and%20bio-knowledge%20linking%20are%20also%20included%20to%20support%0Ainterdisciplinary%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02143v3&entry.124074799=Read"},
{"title": "Train with Perturbation, Infer after Merging: A Two-Stage Framework for\n  Continual Learning", "author": "Haomiao Qiu and Miao Zhang and Ziyue Qiao and Liqiang Nie", "abstract": "  Continual Learning (CL) aims to enable models to continuously acquire new\nknowledge from a sequence of tasks with avoiding the forgetting of learned\ninformation. However, existing CL methods only rely on the parameters of the\nmost recent task for inference, which makes them susceptible to catastrophic\nforgetting. Inspired by the recent success of model merging techniques, we\npropose \\textbf{Perturb-and-Merge (P\\&M)}, a novel continual learning framework\nthat integrates model merging into the CL paradigm to mitigate forgetting.\nSpecifically, after training on each task, P\\&M constructs a new model by\nforming a convex combination of the previous model and the newly trained\ntask-specific model. Through theoretical analysis, we minimize the total loss\nincrease across all tasks and derive an analytical solution for the optimal\nmerging coefficient. To further improve the performance of the merged model, we\nobserve that the degradation introduced during merging can be alleviated by a\nregularization term composed of the task vector and the Hessian matrix of the\nloss function. Interestingly, we show that this term can be efficiently\napproximated using second-order symmetric finite differences, and a stochastic\nperturbation strategy along the task vector direction is accordingly devised\nwhich incurs no additional forward or backward passes while providing an\neffective approximation of the regularization term. Finally, we combine P\\&M\nwith LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead.\nOur proposed approach achieves state-of-the-art performance on several\ncontinual learning benchmark datasets.\n", "link": "http://arxiv.org/abs/2505.22389v3", "date": "2025-06-16", "relevancy": 2.106, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5529}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5203}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Train%20with%20Perturbation%2C%20Infer%20after%20Merging%3A%20A%20Two-Stage%20Framework%20for%0A%20%20Continual%20Learning&body=Title%3A%20Train%20with%20Perturbation%2C%20Infer%20after%20Merging%3A%20A%20Two-Stage%20Framework%20for%0A%20%20Continual%20Learning%0AAuthor%3A%20Haomiao%20Qiu%20and%20Miao%20Zhang%20and%20Ziyue%20Qiao%20and%20Liqiang%20Nie%0AAbstract%3A%20%20%20Continual%20Learning%20%28CL%29%20aims%20to%20enable%20models%20to%20continuously%20acquire%20new%0Aknowledge%20from%20a%20sequence%20of%20tasks%20with%20avoiding%20the%20forgetting%20of%20learned%0Ainformation.%20However%2C%20existing%20CL%20methods%20only%20rely%20on%20the%20parameters%20of%20the%0Amost%20recent%20task%20for%20inference%2C%20which%20makes%20them%20susceptible%20to%20catastrophic%0Aforgetting.%20Inspired%20by%20the%20recent%20success%20of%20model%20merging%20techniques%2C%20we%0Apropose%20%5Ctextbf%7BPerturb-and-Merge%20%28P%5C%26M%29%7D%2C%20a%20novel%20continual%20learning%20framework%0Athat%20integrates%20model%20merging%20into%20the%20CL%20paradigm%20to%20mitigate%20forgetting.%0ASpecifically%2C%20after%20training%20on%20each%20task%2C%20P%5C%26M%20constructs%20a%20new%20model%20by%0Aforming%20a%20convex%20combination%20of%20the%20previous%20model%20and%20the%20newly%20trained%0Atask-specific%20model.%20Through%20theoretical%20analysis%2C%20we%20minimize%20the%20total%20loss%0Aincrease%20across%20all%20tasks%20and%20derive%20an%20analytical%20solution%20for%20the%20optimal%0Amerging%20coefficient.%20To%20further%20improve%20the%20performance%20of%20the%20merged%20model%2C%20we%0Aobserve%20that%20the%20degradation%20introduced%20during%20merging%20can%20be%20alleviated%20by%20a%0Aregularization%20term%20composed%20of%20the%20task%20vector%20and%20the%20Hessian%20matrix%20of%20the%0Aloss%20function.%20Interestingly%2C%20we%20show%20that%20this%20term%20can%20be%20efficiently%0Aapproximated%20using%20second-order%20symmetric%20finite%20differences%2C%20and%20a%20stochastic%0Aperturbation%20strategy%20along%20the%20task%20vector%20direction%20is%20accordingly%20devised%0Awhich%20incurs%20no%20additional%20forward%20or%20backward%20passes%20while%20providing%20an%0Aeffective%20approximation%20of%20the%20regularization%20term.%20Finally%2C%20we%20combine%20P%5C%26M%0Awith%20LoRA%2C%20a%20parameter-efficient%20fine-tuning%20method%2C%20to%20reduce%20memory%20overhead.%0AOur%20proposed%20approach%20achieves%20state-of-the-art%20performance%20on%20several%0Acontinual%20learning%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22389v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrain%2520with%2520Perturbation%252C%2520Infer%2520after%2520Merging%253A%2520A%2520Two-Stage%2520Framework%2520for%250A%2520%2520Continual%2520Learning%26entry.906535625%3DHaomiao%2520Qiu%2520and%2520Miao%2520Zhang%2520and%2520Ziyue%2520Qiao%2520and%2520Liqiang%2520Nie%26entry.1292438233%3D%2520%2520Continual%2520Learning%2520%2528CL%2529%2520aims%2520to%2520enable%2520models%2520to%2520continuously%2520acquire%2520new%250Aknowledge%2520from%2520a%2520sequence%2520of%2520tasks%2520with%2520avoiding%2520the%2520forgetting%2520of%2520learned%250Ainformation.%2520However%252C%2520existing%2520CL%2520methods%2520only%2520rely%2520on%2520the%2520parameters%2520of%2520the%250Amost%2520recent%2520task%2520for%2520inference%252C%2520which%2520makes%2520them%2520susceptible%2520to%2520catastrophic%250Aforgetting.%2520Inspired%2520by%2520the%2520recent%2520success%2520of%2520model%2520merging%2520techniques%252C%2520we%250Apropose%2520%255Ctextbf%257BPerturb-and-Merge%2520%2528P%255C%2526M%2529%257D%252C%2520a%2520novel%2520continual%2520learning%2520framework%250Athat%2520integrates%2520model%2520merging%2520into%2520the%2520CL%2520paradigm%2520to%2520mitigate%2520forgetting.%250ASpecifically%252C%2520after%2520training%2520on%2520each%2520task%252C%2520P%255C%2526M%2520constructs%2520a%2520new%2520model%2520by%250Aforming%2520a%2520convex%2520combination%2520of%2520the%2520previous%2520model%2520and%2520the%2520newly%2520trained%250Atask-specific%2520model.%2520Through%2520theoretical%2520analysis%252C%2520we%2520minimize%2520the%2520total%2520loss%250Aincrease%2520across%2520all%2520tasks%2520and%2520derive%2520an%2520analytical%2520solution%2520for%2520the%2520optimal%250Amerging%2520coefficient.%2520To%2520further%2520improve%2520the%2520performance%2520of%2520the%2520merged%2520model%252C%2520we%250Aobserve%2520that%2520the%2520degradation%2520introduced%2520during%2520merging%2520can%2520be%2520alleviated%2520by%2520a%250Aregularization%2520term%2520composed%2520of%2520the%2520task%2520vector%2520and%2520the%2520Hessian%2520matrix%2520of%2520the%250Aloss%2520function.%2520Interestingly%252C%2520we%2520show%2520that%2520this%2520term%2520can%2520be%2520efficiently%250Aapproximated%2520using%2520second-order%2520symmetric%2520finite%2520differences%252C%2520and%2520a%2520stochastic%250Aperturbation%2520strategy%2520along%2520the%2520task%2520vector%2520direction%2520is%2520accordingly%2520devised%250Awhich%2520incurs%2520no%2520additional%2520forward%2520or%2520backward%2520passes%2520while%2520providing%2520an%250Aeffective%2520approximation%2520of%2520the%2520regularization%2520term.%2520Finally%252C%2520we%2520combine%2520P%255C%2526M%250Awith%2520LoRA%252C%2520a%2520parameter-efficient%2520fine-tuning%2520method%252C%2520to%2520reduce%2520memory%2520overhead.%250AOur%2520proposed%2520approach%2520achieves%2520state-of-the-art%2520performance%2520on%2520several%250Acontinual%2520learning%2520benchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22389v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Train%20with%20Perturbation%2C%20Infer%20after%20Merging%3A%20A%20Two-Stage%20Framework%20for%0A%20%20Continual%20Learning&entry.906535625=Haomiao%20Qiu%20and%20Miao%20Zhang%20and%20Ziyue%20Qiao%20and%20Liqiang%20Nie&entry.1292438233=%20%20Continual%20Learning%20%28CL%29%20aims%20to%20enable%20models%20to%20continuously%20acquire%20new%0Aknowledge%20from%20a%20sequence%20of%20tasks%20with%20avoiding%20the%20forgetting%20of%20learned%0Ainformation.%20However%2C%20existing%20CL%20methods%20only%20rely%20on%20the%20parameters%20of%20the%0Amost%20recent%20task%20for%20inference%2C%20which%20makes%20them%20susceptible%20to%20catastrophic%0Aforgetting.%20Inspired%20by%20the%20recent%20success%20of%20model%20merging%20techniques%2C%20we%0Apropose%20%5Ctextbf%7BPerturb-and-Merge%20%28P%5C%26M%29%7D%2C%20a%20novel%20continual%20learning%20framework%0Athat%20integrates%20model%20merging%20into%20the%20CL%20paradigm%20to%20mitigate%20forgetting.%0ASpecifically%2C%20after%20training%20on%20each%20task%2C%20P%5C%26M%20constructs%20a%20new%20model%20by%0Aforming%20a%20convex%20combination%20of%20the%20previous%20model%20and%20the%20newly%20trained%0Atask-specific%20model.%20Through%20theoretical%20analysis%2C%20we%20minimize%20the%20total%20loss%0Aincrease%20across%20all%20tasks%20and%20derive%20an%20analytical%20solution%20for%20the%20optimal%0Amerging%20coefficient.%20To%20further%20improve%20the%20performance%20of%20the%20merged%20model%2C%20we%0Aobserve%20that%20the%20degradation%20introduced%20during%20merging%20can%20be%20alleviated%20by%20a%0Aregularization%20term%20composed%20of%20the%20task%20vector%20and%20the%20Hessian%20matrix%20of%20the%0Aloss%20function.%20Interestingly%2C%20we%20show%20that%20this%20term%20can%20be%20efficiently%0Aapproximated%20using%20second-order%20symmetric%20finite%20differences%2C%20and%20a%20stochastic%0Aperturbation%20strategy%20along%20the%20task%20vector%20direction%20is%20accordingly%20devised%0Awhich%20incurs%20no%20additional%20forward%20or%20backward%20passes%20while%20providing%20an%0Aeffective%20approximation%20of%20the%20regularization%20term.%20Finally%2C%20we%20combine%20P%5C%26M%0Awith%20LoRA%2C%20a%20parameter-efficient%20fine-tuning%20method%2C%20to%20reduce%20memory%20overhead.%0AOur%20proposed%20approach%20achieves%20state-of-the-art%20performance%20on%20several%0Acontinual%20learning%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22389v3&entry.124074799=Read"},
{"title": "Action Dubber: Timing Audible Actions via Inflectional Flow", "author": "Wenlong Wan and Weiying Zheng and Tianyi Xiang and Guiqing Li and Shengfeng He", "abstract": "  We introduce the task of Audible Action Temporal Localization, which aims to\nidentify the spatio-temporal coordinates of audible movements. Unlike\nconventional tasks such as action recognition and temporal action localization,\nwhich broadly analyze video content, our task focuses on the distinct kinematic\ndynamics of audible actions. It is based on the premise that key actions are\ndriven by inflectional movements; for example, collisions that produce sound\noften involve abrupt changes in motion. To capture this, we propose\n$TA^{2}Net$, a novel architecture that estimates inflectional flow using the\nsecond derivative of motion to determine collision timings without relying on\naudio input. $TA^{2}Net$ also integrates a self-supervised spatial localization\nstrategy during training, combining contrastive learning with spatial analysis.\nThis dual design improves temporal localization accuracy and simultaneously\nidentifies sound sources within video frames. To support this task, we\nintroduce a new benchmark dataset, $Audible623$, derived from Kinetics and\nUCF101 by removing non-essential vocalization subsets. Extensive experiments\nconfirm the effectiveness of our approach on $Audible623$ and show strong\ngeneralizability to other domains, such as repetitive counting and sound source\nlocalization. Code and dataset are available at\nhttps://github.com/WenlongWan/Audible623.\n", "link": "http://arxiv.org/abs/2506.13320v1", "date": "2025-06-16", "relevancy": 2.1055, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5533}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5195}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Action%20Dubber%3A%20Timing%20Audible%20Actions%20via%20Inflectional%20Flow&body=Title%3A%20Action%20Dubber%3A%20Timing%20Audible%20Actions%20via%20Inflectional%20Flow%0AAuthor%3A%20Wenlong%20Wan%20and%20Weiying%20Zheng%20and%20Tianyi%20Xiang%20and%20Guiqing%20Li%20and%20Shengfeng%20He%0AAbstract%3A%20%20%20We%20introduce%20the%20task%20of%20Audible%20Action%20Temporal%20Localization%2C%20which%20aims%20to%0Aidentify%20the%20spatio-temporal%20coordinates%20of%20audible%20movements.%20Unlike%0Aconventional%20tasks%20such%20as%20action%20recognition%20and%20temporal%20action%20localization%2C%0Awhich%20broadly%20analyze%20video%20content%2C%20our%20task%20focuses%20on%20the%20distinct%20kinematic%0Adynamics%20of%20audible%20actions.%20It%20is%20based%20on%20the%20premise%20that%20key%20actions%20are%0Adriven%20by%20inflectional%20movements%3B%20for%20example%2C%20collisions%20that%20produce%20sound%0Aoften%20involve%20abrupt%20changes%20in%20motion.%20To%20capture%20this%2C%20we%20propose%0A%24TA%5E%7B2%7DNet%24%2C%20a%20novel%20architecture%20that%20estimates%20inflectional%20flow%20using%20the%0Asecond%20derivative%20of%20motion%20to%20determine%20collision%20timings%20without%20relying%20on%0Aaudio%20input.%20%24TA%5E%7B2%7DNet%24%20also%20integrates%20a%20self-supervised%20spatial%20localization%0Astrategy%20during%20training%2C%20combining%20contrastive%20learning%20with%20spatial%20analysis.%0AThis%20dual%20design%20improves%20temporal%20localization%20accuracy%20and%20simultaneously%0Aidentifies%20sound%20sources%20within%20video%20frames.%20To%20support%20this%20task%2C%20we%0Aintroduce%20a%20new%20benchmark%20dataset%2C%20%24Audible623%24%2C%20derived%20from%20Kinetics%20and%0AUCF101%20by%20removing%20non-essential%20vocalization%20subsets.%20Extensive%20experiments%0Aconfirm%20the%20effectiveness%20of%20our%20approach%20on%20%24Audible623%24%20and%20show%20strong%0Ageneralizability%20to%20other%20domains%2C%20such%20as%20repetitive%20counting%20and%20sound%20source%0Alocalization.%20Code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/WenlongWan/Audible623.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAction%2520Dubber%253A%2520Timing%2520Audible%2520Actions%2520via%2520Inflectional%2520Flow%26entry.906535625%3DWenlong%2520Wan%2520and%2520Weiying%2520Zheng%2520and%2520Tianyi%2520Xiang%2520and%2520Guiqing%2520Li%2520and%2520Shengfeng%2520He%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520task%2520of%2520Audible%2520Action%2520Temporal%2520Localization%252C%2520which%2520aims%2520to%250Aidentify%2520the%2520spatio-temporal%2520coordinates%2520of%2520audible%2520movements.%2520Unlike%250Aconventional%2520tasks%2520such%2520as%2520action%2520recognition%2520and%2520temporal%2520action%2520localization%252C%250Awhich%2520broadly%2520analyze%2520video%2520content%252C%2520our%2520task%2520focuses%2520on%2520the%2520distinct%2520kinematic%250Adynamics%2520of%2520audible%2520actions.%2520It%2520is%2520based%2520on%2520the%2520premise%2520that%2520key%2520actions%2520are%250Adriven%2520by%2520inflectional%2520movements%253B%2520for%2520example%252C%2520collisions%2520that%2520produce%2520sound%250Aoften%2520involve%2520abrupt%2520changes%2520in%2520motion.%2520To%2520capture%2520this%252C%2520we%2520propose%250A%2524TA%255E%257B2%257DNet%2524%252C%2520a%2520novel%2520architecture%2520that%2520estimates%2520inflectional%2520flow%2520using%2520the%250Asecond%2520derivative%2520of%2520motion%2520to%2520determine%2520collision%2520timings%2520without%2520relying%2520on%250Aaudio%2520input.%2520%2524TA%255E%257B2%257DNet%2524%2520also%2520integrates%2520a%2520self-supervised%2520spatial%2520localization%250Astrategy%2520during%2520training%252C%2520combining%2520contrastive%2520learning%2520with%2520spatial%2520analysis.%250AThis%2520dual%2520design%2520improves%2520temporal%2520localization%2520accuracy%2520and%2520simultaneously%250Aidentifies%2520sound%2520sources%2520within%2520video%2520frames.%2520To%2520support%2520this%2520task%252C%2520we%250Aintroduce%2520a%2520new%2520benchmark%2520dataset%252C%2520%2524Audible623%2524%252C%2520derived%2520from%2520Kinetics%2520and%250AUCF101%2520by%2520removing%2520non-essential%2520vocalization%2520subsets.%2520Extensive%2520experiments%250Aconfirm%2520the%2520effectiveness%2520of%2520our%2520approach%2520on%2520%2524Audible623%2524%2520and%2520show%2520strong%250Ageneralizability%2520to%2520other%2520domains%252C%2520such%2520as%2520repetitive%2520counting%2520and%2520sound%2520source%250Alocalization.%2520Code%2520and%2520dataset%2520are%2520available%2520at%250Ahttps%253A//github.com/WenlongWan/Audible623.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Action%20Dubber%3A%20Timing%20Audible%20Actions%20via%20Inflectional%20Flow&entry.906535625=Wenlong%20Wan%20and%20Weiying%20Zheng%20and%20Tianyi%20Xiang%20and%20Guiqing%20Li%20and%20Shengfeng%20He&entry.1292438233=%20%20We%20introduce%20the%20task%20of%20Audible%20Action%20Temporal%20Localization%2C%20which%20aims%20to%0Aidentify%20the%20spatio-temporal%20coordinates%20of%20audible%20movements.%20Unlike%0Aconventional%20tasks%20such%20as%20action%20recognition%20and%20temporal%20action%20localization%2C%0Awhich%20broadly%20analyze%20video%20content%2C%20our%20task%20focuses%20on%20the%20distinct%20kinematic%0Adynamics%20of%20audible%20actions.%20It%20is%20based%20on%20the%20premise%20that%20key%20actions%20are%0Adriven%20by%20inflectional%20movements%3B%20for%20example%2C%20collisions%20that%20produce%20sound%0Aoften%20involve%20abrupt%20changes%20in%20motion.%20To%20capture%20this%2C%20we%20propose%0A%24TA%5E%7B2%7DNet%24%2C%20a%20novel%20architecture%20that%20estimates%20inflectional%20flow%20using%20the%0Asecond%20derivative%20of%20motion%20to%20determine%20collision%20timings%20without%20relying%20on%0Aaudio%20input.%20%24TA%5E%7B2%7DNet%24%20also%20integrates%20a%20self-supervised%20spatial%20localization%0Astrategy%20during%20training%2C%20combining%20contrastive%20learning%20with%20spatial%20analysis.%0AThis%20dual%20design%20improves%20temporal%20localization%20accuracy%20and%20simultaneously%0Aidentifies%20sound%20sources%20within%20video%20frames.%20To%20support%20this%20task%2C%20we%0Aintroduce%20a%20new%20benchmark%20dataset%2C%20%24Audible623%24%2C%20derived%20from%20Kinetics%20and%0AUCF101%20by%20removing%20non-essential%20vocalization%20subsets.%20Extensive%20experiments%0Aconfirm%20the%20effectiveness%20of%20our%20approach%20on%20%24Audible623%24%20and%20show%20strong%0Ageneralizability%20to%20other%20domains%2C%20such%20as%20repetitive%20counting%20and%20sound%20source%0Alocalization.%20Code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/WenlongWan/Audible623.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13320v1&entry.124074799=Read"},
{"title": "Looking around you: external information enhances representations for\n  event sequences", "author": "Maria Kovaleva and Petr Sokerin and Pavel Tikhomirov and Alexey Zaytsev", "abstract": "  Representation learning produces models in different domains, such as store\npurchases, client transactions, and general people's behaviour. However, such\nmodels for event sequences usually process each sequence in isolation, ignoring\ncontext from ones that co-occur in time. This limitation is particularly\nproblematic in domains with fast-evolving conditions, like finance and\ne-commerce, or when certain sequences lack recent events.\n  We develop a method that aggregates information from multiple user\nrepresentations, augmenting a specific user for a scenario of multiple\nco-occurring event sequences, achieving better quality than processing each\nsequence independently. Our study considers diverse aggregation approaches,\nranging from simple pooling techniques to trainable attention-based Kernel\nattention aggregation, that can highlight more complex information flow from\nother users. The proposed methods operate on top of an existing encoder and\nsupport its efficient fine-tuning. Across six diverse event sequence datasets\n(finance, e-commerce, education, etc.) and downstream tasks, Kernel attention\nimproves ROC-AUC scores, both with and without fine-tuning, while mean pooling\nyields a smaller but still significant gain.\n", "link": "http://arxiv.org/abs/2502.10205v2", "date": "2025-06-16", "relevancy": 2.0935, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5249}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5249}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Looking%20around%20you%3A%20external%20information%20enhances%20representations%20for%0A%20%20event%20sequences&body=Title%3A%20Looking%20around%20you%3A%20external%20information%20enhances%20representations%20for%0A%20%20event%20sequences%0AAuthor%3A%20Maria%20Kovaleva%20and%20Petr%20Sokerin%20and%20Pavel%20Tikhomirov%20and%20Alexey%20Zaytsev%0AAbstract%3A%20%20%20Representation%20learning%20produces%20models%20in%20different%20domains%2C%20such%20as%20store%0Apurchases%2C%20client%20transactions%2C%20and%20general%20people%27s%20behaviour.%20However%2C%20such%0Amodels%20for%20event%20sequences%20usually%20process%20each%20sequence%20in%20isolation%2C%20ignoring%0Acontext%20from%20ones%20that%20co-occur%20in%20time.%20This%20limitation%20is%20particularly%0Aproblematic%20in%20domains%20with%20fast-evolving%20conditions%2C%20like%20finance%20and%0Ae-commerce%2C%20or%20when%20certain%20sequences%20lack%20recent%20events.%0A%20%20We%20develop%20a%20method%20that%20aggregates%20information%20from%20multiple%20user%0Arepresentations%2C%20augmenting%20a%20specific%20user%20for%20a%20scenario%20of%20multiple%0Aco-occurring%20event%20sequences%2C%20achieving%20better%20quality%20than%20processing%20each%0Asequence%20independently.%20Our%20study%20considers%20diverse%20aggregation%20approaches%2C%0Aranging%20from%20simple%20pooling%20techniques%20to%20trainable%20attention-based%20Kernel%0Aattention%20aggregation%2C%20that%20can%20highlight%20more%20complex%20information%20flow%20from%0Aother%20users.%20The%20proposed%20methods%20operate%20on%20top%20of%20an%20existing%20encoder%20and%0Asupport%20its%20efficient%20fine-tuning.%20Across%20six%20diverse%20event%20sequence%20datasets%0A%28finance%2C%20e-commerce%2C%20education%2C%20etc.%29%20and%20downstream%20tasks%2C%20Kernel%20attention%0Aimproves%20ROC-AUC%20scores%2C%20both%20with%20and%20without%20fine-tuning%2C%20while%20mean%20pooling%0Ayields%20a%20smaller%20but%20still%20significant%20gain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10205v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLooking%2520around%2520you%253A%2520external%2520information%2520enhances%2520representations%2520for%250A%2520%2520event%2520sequences%26entry.906535625%3DMaria%2520Kovaleva%2520and%2520Petr%2520Sokerin%2520and%2520Pavel%2520Tikhomirov%2520and%2520Alexey%2520Zaytsev%26entry.1292438233%3D%2520%2520Representation%2520learning%2520produces%2520models%2520in%2520different%2520domains%252C%2520such%2520as%2520store%250Apurchases%252C%2520client%2520transactions%252C%2520and%2520general%2520people%2527s%2520behaviour.%2520However%252C%2520such%250Amodels%2520for%2520event%2520sequences%2520usually%2520process%2520each%2520sequence%2520in%2520isolation%252C%2520ignoring%250Acontext%2520from%2520ones%2520that%2520co-occur%2520in%2520time.%2520This%2520limitation%2520is%2520particularly%250Aproblematic%2520in%2520domains%2520with%2520fast-evolving%2520conditions%252C%2520like%2520finance%2520and%250Ae-commerce%252C%2520or%2520when%2520certain%2520sequences%2520lack%2520recent%2520events.%250A%2520%2520We%2520develop%2520a%2520method%2520that%2520aggregates%2520information%2520from%2520multiple%2520user%250Arepresentations%252C%2520augmenting%2520a%2520specific%2520user%2520for%2520a%2520scenario%2520of%2520multiple%250Aco-occurring%2520event%2520sequences%252C%2520achieving%2520better%2520quality%2520than%2520processing%2520each%250Asequence%2520independently.%2520Our%2520study%2520considers%2520diverse%2520aggregation%2520approaches%252C%250Aranging%2520from%2520simple%2520pooling%2520techniques%2520to%2520trainable%2520attention-based%2520Kernel%250Aattention%2520aggregation%252C%2520that%2520can%2520highlight%2520more%2520complex%2520information%2520flow%2520from%250Aother%2520users.%2520The%2520proposed%2520methods%2520operate%2520on%2520top%2520of%2520an%2520existing%2520encoder%2520and%250Asupport%2520its%2520efficient%2520fine-tuning.%2520Across%2520six%2520diverse%2520event%2520sequence%2520datasets%250A%2528finance%252C%2520e-commerce%252C%2520education%252C%2520etc.%2529%2520and%2520downstream%2520tasks%252C%2520Kernel%2520attention%250Aimproves%2520ROC-AUC%2520scores%252C%2520both%2520with%2520and%2520without%2520fine-tuning%252C%2520while%2520mean%2520pooling%250Ayields%2520a%2520smaller%2520but%2520still%2520significant%2520gain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10205v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Looking%20around%20you%3A%20external%20information%20enhances%20representations%20for%0A%20%20event%20sequences&entry.906535625=Maria%20Kovaleva%20and%20Petr%20Sokerin%20and%20Pavel%20Tikhomirov%20and%20Alexey%20Zaytsev&entry.1292438233=%20%20Representation%20learning%20produces%20models%20in%20different%20domains%2C%20such%20as%20store%0Apurchases%2C%20client%20transactions%2C%20and%20general%20people%27s%20behaviour.%20However%2C%20such%0Amodels%20for%20event%20sequences%20usually%20process%20each%20sequence%20in%20isolation%2C%20ignoring%0Acontext%20from%20ones%20that%20co-occur%20in%20time.%20This%20limitation%20is%20particularly%0Aproblematic%20in%20domains%20with%20fast-evolving%20conditions%2C%20like%20finance%20and%0Ae-commerce%2C%20or%20when%20certain%20sequences%20lack%20recent%20events.%0A%20%20We%20develop%20a%20method%20that%20aggregates%20information%20from%20multiple%20user%0Arepresentations%2C%20augmenting%20a%20specific%20user%20for%20a%20scenario%20of%20multiple%0Aco-occurring%20event%20sequences%2C%20achieving%20better%20quality%20than%20processing%20each%0Asequence%20independently.%20Our%20study%20considers%20diverse%20aggregation%20approaches%2C%0Aranging%20from%20simple%20pooling%20techniques%20to%20trainable%20attention-based%20Kernel%0Aattention%20aggregation%2C%20that%20can%20highlight%20more%20complex%20information%20flow%20from%0Aother%20users.%20The%20proposed%20methods%20operate%20on%20top%20of%20an%20existing%20encoder%20and%0Asupport%20its%20efficient%20fine-tuning.%20Across%20six%20diverse%20event%20sequence%20datasets%0A%28finance%2C%20e-commerce%2C%20education%2C%20etc.%29%20and%20downstream%20tasks%2C%20Kernel%20attention%0Aimproves%20ROC-AUC%20scores%2C%20both%20with%20and%20without%20fine-tuning%2C%20while%20mean%20pooling%0Ayields%20a%20smaller%20but%20still%20significant%20gain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10205v2&entry.124074799=Read"},
{"title": "Adversarial Disentanglement by Backpropagation with Physics-Informed\n  Variational Autoencoder", "author": "Ioannis Christoforos Koune and Alice Cicirello", "abstract": "  Inference and prediction under partial knowledge of a physical system is\nchallenging, particularly when multiple confounding sources influence the\nmeasured response. Explicitly accounting for these influences in physics-based\nmodels is often infeasible due to epistemic uncertainty, cost, or time\nconstraints, resulting in models that fail to accurately describe the behavior\nof the system. On the other hand, data-driven machine learning models such as\nvariational autoencoders are not guaranteed to identify a parsimonious\nrepresentation. As a result, they can suffer from poor generalization\nperformance and reconstruction accuracy in the regime of limited and noisy\ndata. We propose a physics-informed variational autoencoder architecture that\ncombines the interpretability of physics-based models with the flexibility of\ndata-driven models. To promote disentanglement of the known physics and\nconfounding influences, the latent space is partitioned into physically\nmeaningful variables that parametrize a physics-based model, and data-driven\nvariables that capture variability in the domain and class of the physical\nsystem. The encoder is coupled with a decoder that integrates physics-based and\ndata-driven components, and constrained by an adversarial training objective\nthat prevents the data-driven components from overriding the known physics,\nensuring that the physics-grounded latent variables remain interpretable. We\ndemonstrate that the model is able to disentangle features of the input signal\nand separate the known physics from confounding influences using supervision in\nthe form of class and domain observables. The model is evaluated on a series of\nsynthetic case studies relevant to engineering structures, demonstrating the\nfeasibility of the proposed approach.\n", "link": "http://arxiv.org/abs/2506.13658v1", "date": "2025-06-16", "relevancy": 2.0921, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5622}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.519}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Disentanglement%20by%20Backpropagation%20with%20Physics-Informed%0A%20%20Variational%20Autoencoder&body=Title%3A%20Adversarial%20Disentanglement%20by%20Backpropagation%20with%20Physics-Informed%0A%20%20Variational%20Autoencoder%0AAuthor%3A%20Ioannis%20Christoforos%20Koune%20and%20Alice%20Cicirello%0AAbstract%3A%20%20%20Inference%20and%20prediction%20under%20partial%20knowledge%20of%20a%20physical%20system%20is%0Achallenging%2C%20particularly%20when%20multiple%20confounding%20sources%20influence%20the%0Ameasured%20response.%20Explicitly%20accounting%20for%20these%20influences%20in%20physics-based%0Amodels%20is%20often%20infeasible%20due%20to%20epistemic%20uncertainty%2C%20cost%2C%20or%20time%0Aconstraints%2C%20resulting%20in%20models%20that%20fail%20to%20accurately%20describe%20the%20behavior%0Aof%20the%20system.%20On%20the%20other%20hand%2C%20data-driven%20machine%20learning%20models%20such%20as%0Avariational%20autoencoders%20are%20not%20guaranteed%20to%20identify%20a%20parsimonious%0Arepresentation.%20As%20a%20result%2C%20they%20can%20suffer%20from%20poor%20generalization%0Aperformance%20and%20reconstruction%20accuracy%20in%20the%20regime%20of%20limited%20and%20noisy%0Adata.%20We%20propose%20a%20physics-informed%20variational%20autoencoder%20architecture%20that%0Acombines%20the%20interpretability%20of%20physics-based%20models%20with%20the%20flexibility%20of%0Adata-driven%20models.%20To%20promote%20disentanglement%20of%20the%20known%20physics%20and%0Aconfounding%20influences%2C%20the%20latent%20space%20is%20partitioned%20into%20physically%0Ameaningful%20variables%20that%20parametrize%20a%20physics-based%20model%2C%20and%20data-driven%0Avariables%20that%20capture%20variability%20in%20the%20domain%20and%20class%20of%20the%20physical%0Asystem.%20The%20encoder%20is%20coupled%20with%20a%20decoder%20that%20integrates%20physics-based%20and%0Adata-driven%20components%2C%20and%20constrained%20by%20an%20adversarial%20training%20objective%0Athat%20prevents%20the%20data-driven%20components%20from%20overriding%20the%20known%20physics%2C%0Aensuring%20that%20the%20physics-grounded%20latent%20variables%20remain%20interpretable.%20We%0Ademonstrate%20that%20the%20model%20is%20able%20to%20disentangle%20features%20of%20the%20input%20signal%0Aand%20separate%20the%20known%20physics%20from%20confounding%20influences%20using%20supervision%20in%0Athe%20form%20of%20class%20and%20domain%20observables.%20The%20model%20is%20evaluated%20on%20a%20series%20of%0Asynthetic%20case%20studies%20relevant%20to%20engineering%20structures%2C%20demonstrating%20the%0Afeasibility%20of%20the%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13658v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Disentanglement%2520by%2520Backpropagation%2520with%2520Physics-Informed%250A%2520%2520Variational%2520Autoencoder%26entry.906535625%3DIoannis%2520Christoforos%2520Koune%2520and%2520Alice%2520Cicirello%26entry.1292438233%3D%2520%2520Inference%2520and%2520prediction%2520under%2520partial%2520knowledge%2520of%2520a%2520physical%2520system%2520is%250Achallenging%252C%2520particularly%2520when%2520multiple%2520confounding%2520sources%2520influence%2520the%250Ameasured%2520response.%2520Explicitly%2520accounting%2520for%2520these%2520influences%2520in%2520physics-based%250Amodels%2520is%2520often%2520infeasible%2520due%2520to%2520epistemic%2520uncertainty%252C%2520cost%252C%2520or%2520time%250Aconstraints%252C%2520resulting%2520in%2520models%2520that%2520fail%2520to%2520accurately%2520describe%2520the%2520behavior%250Aof%2520the%2520system.%2520On%2520the%2520other%2520hand%252C%2520data-driven%2520machine%2520learning%2520models%2520such%2520as%250Avariational%2520autoencoders%2520are%2520not%2520guaranteed%2520to%2520identify%2520a%2520parsimonious%250Arepresentation.%2520As%2520a%2520result%252C%2520they%2520can%2520suffer%2520from%2520poor%2520generalization%250Aperformance%2520and%2520reconstruction%2520accuracy%2520in%2520the%2520regime%2520of%2520limited%2520and%2520noisy%250Adata.%2520We%2520propose%2520a%2520physics-informed%2520variational%2520autoencoder%2520architecture%2520that%250Acombines%2520the%2520interpretability%2520of%2520physics-based%2520models%2520with%2520the%2520flexibility%2520of%250Adata-driven%2520models.%2520To%2520promote%2520disentanglement%2520of%2520the%2520known%2520physics%2520and%250Aconfounding%2520influences%252C%2520the%2520latent%2520space%2520is%2520partitioned%2520into%2520physically%250Ameaningful%2520variables%2520that%2520parametrize%2520a%2520physics-based%2520model%252C%2520and%2520data-driven%250Avariables%2520that%2520capture%2520variability%2520in%2520the%2520domain%2520and%2520class%2520of%2520the%2520physical%250Asystem.%2520The%2520encoder%2520is%2520coupled%2520with%2520a%2520decoder%2520that%2520integrates%2520physics-based%2520and%250Adata-driven%2520components%252C%2520and%2520constrained%2520by%2520an%2520adversarial%2520training%2520objective%250Athat%2520prevents%2520the%2520data-driven%2520components%2520from%2520overriding%2520the%2520known%2520physics%252C%250Aensuring%2520that%2520the%2520physics-grounded%2520latent%2520variables%2520remain%2520interpretable.%2520We%250Ademonstrate%2520that%2520the%2520model%2520is%2520able%2520to%2520disentangle%2520features%2520of%2520the%2520input%2520signal%250Aand%2520separate%2520the%2520known%2520physics%2520from%2520confounding%2520influences%2520using%2520supervision%2520in%250Athe%2520form%2520of%2520class%2520and%2520domain%2520observables.%2520The%2520model%2520is%2520evaluated%2520on%2520a%2520series%2520of%250Asynthetic%2520case%2520studies%2520relevant%2520to%2520engineering%2520structures%252C%2520demonstrating%2520the%250Afeasibility%2520of%2520the%2520proposed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13658v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Disentanglement%20by%20Backpropagation%20with%20Physics-Informed%0A%20%20Variational%20Autoencoder&entry.906535625=Ioannis%20Christoforos%20Koune%20and%20Alice%20Cicirello&entry.1292438233=%20%20Inference%20and%20prediction%20under%20partial%20knowledge%20of%20a%20physical%20system%20is%0Achallenging%2C%20particularly%20when%20multiple%20confounding%20sources%20influence%20the%0Ameasured%20response.%20Explicitly%20accounting%20for%20these%20influences%20in%20physics-based%0Amodels%20is%20often%20infeasible%20due%20to%20epistemic%20uncertainty%2C%20cost%2C%20or%20time%0Aconstraints%2C%20resulting%20in%20models%20that%20fail%20to%20accurately%20describe%20the%20behavior%0Aof%20the%20system.%20On%20the%20other%20hand%2C%20data-driven%20machine%20learning%20models%20such%20as%0Avariational%20autoencoders%20are%20not%20guaranteed%20to%20identify%20a%20parsimonious%0Arepresentation.%20As%20a%20result%2C%20they%20can%20suffer%20from%20poor%20generalization%0Aperformance%20and%20reconstruction%20accuracy%20in%20the%20regime%20of%20limited%20and%20noisy%0Adata.%20We%20propose%20a%20physics-informed%20variational%20autoencoder%20architecture%20that%0Acombines%20the%20interpretability%20of%20physics-based%20models%20with%20the%20flexibility%20of%0Adata-driven%20models.%20To%20promote%20disentanglement%20of%20the%20known%20physics%20and%0Aconfounding%20influences%2C%20the%20latent%20space%20is%20partitioned%20into%20physically%0Ameaningful%20variables%20that%20parametrize%20a%20physics-based%20model%2C%20and%20data-driven%0Avariables%20that%20capture%20variability%20in%20the%20domain%20and%20class%20of%20the%20physical%0Asystem.%20The%20encoder%20is%20coupled%20with%20a%20decoder%20that%20integrates%20physics-based%20and%0Adata-driven%20components%2C%20and%20constrained%20by%20an%20adversarial%20training%20objective%0Athat%20prevents%20the%20data-driven%20components%20from%20overriding%20the%20known%20physics%2C%0Aensuring%20that%20the%20physics-grounded%20latent%20variables%20remain%20interpretable.%20We%0Ademonstrate%20that%20the%20model%20is%20able%20to%20disentangle%20features%20of%20the%20input%20signal%0Aand%20separate%20the%20known%20physics%20from%20confounding%20influences%20using%20supervision%20in%0Athe%20form%20of%20class%20and%20domain%20observables.%20The%20model%20is%20evaluated%20on%20a%20series%20of%0Asynthetic%20case%20studies%20relevant%20to%20engineering%20structures%2C%20demonstrating%20the%0Afeasibility%20of%20the%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13658v1&entry.124074799=Read"},
{"title": "TR2M: Transferring Monocular Relative Depth to Metric Depth with\n  Language Descriptions and Scale-Oriented Contrast", "author": "Beilei Cui and Yiming Huang and Long Bai and Hongliang Ren", "abstract": "  This work presents a generalizable framework to transfer relative depth to\nmetric depth. Current monocular depth estimation methods are mainly divided\ninto metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs\nestimate depth in metric scale but are often limited to a specific domain.\nMRDEs generalize well across different domains, but with uncertain scales which\nhinders downstream applications. To this end, we aim to build up a framework to\nsolve scale uncertainty and transfer relative depth to metric depth. Previous\nmethods used language as input and estimated two factors for conducting\nrescaling. Our approach, TR2M, utilizes both text description and image as\ninputs and estimates two rescale maps to transfer relative depth to metric\ndepth at pixel level. Features from two modalities are fused with a\ncross-modality attention module to better capture scale information. A strategy\nis designed to construct and filter confident pseudo metric depth for more\ncomprehensive supervision. We also develop scale-oriented contrastive learning\nto utilize depth distribution as guidance to enforce the model learning about\nintrinsic knowledge aligning with the scale distribution. TR2M only exploits a\nsmall number of trainable parameters to train on datasets in various domains\nand experiments not only demonstrate TR2M's great performance in seen datasets\nbut also reveal superior zero-shot capabilities on five unseen datasets. We\nshow the huge potential in pixel-wise transferring relative depth to metric\ndepth with language assistance. (Code is available at:\nhttps://github.com/BeileiCui/TR2M)\n", "link": "http://arxiv.org/abs/2506.13387v1", "date": "2025-06-16", "relevancy": 1.8284, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6359}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5855}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TR2M%3A%20Transferring%20Monocular%20Relative%20Depth%20to%20Metric%20Depth%20with%0A%20%20Language%20Descriptions%20and%20Scale-Oriented%20Contrast&body=Title%3A%20TR2M%3A%20Transferring%20Monocular%20Relative%20Depth%20to%20Metric%20Depth%20with%0A%20%20Language%20Descriptions%20and%20Scale-Oriented%20Contrast%0AAuthor%3A%20Beilei%20Cui%20and%20Yiming%20Huang%20and%20Long%20Bai%20and%20Hongliang%20Ren%0AAbstract%3A%20%20%20This%20work%20presents%20a%20generalizable%20framework%20to%20transfer%20relative%20depth%20to%0Ametric%20depth.%20Current%20monocular%20depth%20estimation%20methods%20are%20mainly%20divided%0Ainto%20metric%20depth%20estimation%20%28MMDE%29%20and%20relative%20depth%20estimation%20%28MRDE%29.%20MMDEs%0Aestimate%20depth%20in%20metric%20scale%20but%20are%20often%20limited%20to%20a%20specific%20domain.%0AMRDEs%20generalize%20well%20across%20different%20domains%2C%20but%20with%20uncertain%20scales%20which%0Ahinders%20downstream%20applications.%20To%20this%20end%2C%20we%20aim%20to%20build%20up%20a%20framework%20to%0Asolve%20scale%20uncertainty%20and%20transfer%20relative%20depth%20to%20metric%20depth.%20Previous%0Amethods%20used%20language%20as%20input%20and%20estimated%20two%20factors%20for%20conducting%0Arescaling.%20Our%20approach%2C%20TR2M%2C%20utilizes%20both%20text%20description%20and%20image%20as%0Ainputs%20and%20estimates%20two%20rescale%20maps%20to%20transfer%20relative%20depth%20to%20metric%0Adepth%20at%20pixel%20level.%20Features%20from%20two%20modalities%20are%20fused%20with%20a%0Across-modality%20attention%20module%20to%20better%20capture%20scale%20information.%20A%20strategy%0Ais%20designed%20to%20construct%20and%20filter%20confident%20pseudo%20metric%20depth%20for%20more%0Acomprehensive%20supervision.%20We%20also%20develop%20scale-oriented%20contrastive%20learning%0Ato%20utilize%20depth%20distribution%20as%20guidance%20to%20enforce%20the%20model%20learning%20about%0Aintrinsic%20knowledge%20aligning%20with%20the%20scale%20distribution.%20TR2M%20only%20exploits%20a%0Asmall%20number%20of%20trainable%20parameters%20to%20train%20on%20datasets%20in%20various%20domains%0Aand%20experiments%20not%20only%20demonstrate%20TR2M%27s%20great%20performance%20in%20seen%20datasets%0Abut%20also%20reveal%20superior%20zero-shot%20capabilities%20on%20five%20unseen%20datasets.%20We%0Ashow%20the%20huge%20potential%20in%20pixel-wise%20transferring%20relative%20depth%20to%20metric%0Adepth%20with%20language%20assistance.%20%28Code%20is%20available%20at%3A%0Ahttps%3A//github.com/BeileiCui/TR2M%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTR2M%253A%2520Transferring%2520Monocular%2520Relative%2520Depth%2520to%2520Metric%2520Depth%2520with%250A%2520%2520Language%2520Descriptions%2520and%2520Scale-Oriented%2520Contrast%26entry.906535625%3DBeilei%2520Cui%2520and%2520Yiming%2520Huang%2520and%2520Long%2520Bai%2520and%2520Hongliang%2520Ren%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520a%2520generalizable%2520framework%2520to%2520transfer%2520relative%2520depth%2520to%250Ametric%2520depth.%2520Current%2520monocular%2520depth%2520estimation%2520methods%2520are%2520mainly%2520divided%250Ainto%2520metric%2520depth%2520estimation%2520%2528MMDE%2529%2520and%2520relative%2520depth%2520estimation%2520%2528MRDE%2529.%2520MMDEs%250Aestimate%2520depth%2520in%2520metric%2520scale%2520but%2520are%2520often%2520limited%2520to%2520a%2520specific%2520domain.%250AMRDEs%2520generalize%2520well%2520across%2520different%2520domains%252C%2520but%2520with%2520uncertain%2520scales%2520which%250Ahinders%2520downstream%2520applications.%2520To%2520this%2520end%252C%2520we%2520aim%2520to%2520build%2520up%2520a%2520framework%2520to%250Asolve%2520scale%2520uncertainty%2520and%2520transfer%2520relative%2520depth%2520to%2520metric%2520depth.%2520Previous%250Amethods%2520used%2520language%2520as%2520input%2520and%2520estimated%2520two%2520factors%2520for%2520conducting%250Arescaling.%2520Our%2520approach%252C%2520TR2M%252C%2520utilizes%2520both%2520text%2520description%2520and%2520image%2520as%250Ainputs%2520and%2520estimates%2520two%2520rescale%2520maps%2520to%2520transfer%2520relative%2520depth%2520to%2520metric%250Adepth%2520at%2520pixel%2520level.%2520Features%2520from%2520two%2520modalities%2520are%2520fused%2520with%2520a%250Across-modality%2520attention%2520module%2520to%2520better%2520capture%2520scale%2520information.%2520A%2520strategy%250Ais%2520designed%2520to%2520construct%2520and%2520filter%2520confident%2520pseudo%2520metric%2520depth%2520for%2520more%250Acomprehensive%2520supervision.%2520We%2520also%2520develop%2520scale-oriented%2520contrastive%2520learning%250Ato%2520utilize%2520depth%2520distribution%2520as%2520guidance%2520to%2520enforce%2520the%2520model%2520learning%2520about%250Aintrinsic%2520knowledge%2520aligning%2520with%2520the%2520scale%2520distribution.%2520TR2M%2520only%2520exploits%2520a%250Asmall%2520number%2520of%2520trainable%2520parameters%2520to%2520train%2520on%2520datasets%2520in%2520various%2520domains%250Aand%2520experiments%2520not%2520only%2520demonstrate%2520TR2M%2527s%2520great%2520performance%2520in%2520seen%2520datasets%250Abut%2520also%2520reveal%2520superior%2520zero-shot%2520capabilities%2520on%2520five%2520unseen%2520datasets.%2520We%250Ashow%2520the%2520huge%2520potential%2520in%2520pixel-wise%2520transferring%2520relative%2520depth%2520to%2520metric%250Adepth%2520with%2520language%2520assistance.%2520%2528Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/BeileiCui/TR2M%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TR2M%3A%20Transferring%20Monocular%20Relative%20Depth%20to%20Metric%20Depth%20with%0A%20%20Language%20Descriptions%20and%20Scale-Oriented%20Contrast&entry.906535625=Beilei%20Cui%20and%20Yiming%20Huang%20and%20Long%20Bai%20and%20Hongliang%20Ren&entry.1292438233=%20%20This%20work%20presents%20a%20generalizable%20framework%20to%20transfer%20relative%20depth%20to%0Ametric%20depth.%20Current%20monocular%20depth%20estimation%20methods%20are%20mainly%20divided%0Ainto%20metric%20depth%20estimation%20%28MMDE%29%20and%20relative%20depth%20estimation%20%28MRDE%29.%20MMDEs%0Aestimate%20depth%20in%20metric%20scale%20but%20are%20often%20limited%20to%20a%20specific%20domain.%0AMRDEs%20generalize%20well%20across%20different%20domains%2C%20but%20with%20uncertain%20scales%20which%0Ahinders%20downstream%20applications.%20To%20this%20end%2C%20we%20aim%20to%20build%20up%20a%20framework%20to%0Asolve%20scale%20uncertainty%20and%20transfer%20relative%20depth%20to%20metric%20depth.%20Previous%0Amethods%20used%20language%20as%20input%20and%20estimated%20two%20factors%20for%20conducting%0Arescaling.%20Our%20approach%2C%20TR2M%2C%20utilizes%20both%20text%20description%20and%20image%20as%0Ainputs%20and%20estimates%20two%20rescale%20maps%20to%20transfer%20relative%20depth%20to%20metric%0Adepth%20at%20pixel%20level.%20Features%20from%20two%20modalities%20are%20fused%20with%20a%0Across-modality%20attention%20module%20to%20better%20capture%20scale%20information.%20A%20strategy%0Ais%20designed%20to%20construct%20and%20filter%20confident%20pseudo%20metric%20depth%20for%20more%0Acomprehensive%20supervision.%20We%20also%20develop%20scale-oriented%20contrastive%20learning%0Ato%20utilize%20depth%20distribution%20as%20guidance%20to%20enforce%20the%20model%20learning%20about%0Aintrinsic%20knowledge%20aligning%20with%20the%20scale%20distribution.%20TR2M%20only%20exploits%20a%0Asmall%20number%20of%20trainable%20parameters%20to%20train%20on%20datasets%20in%20various%20domains%0Aand%20experiments%20not%20only%20demonstrate%20TR2M%27s%20great%20performance%20in%20seen%20datasets%0Abut%20also%20reveal%20superior%20zero-shot%20capabilities%20on%20five%20unseen%20datasets.%20We%0Ashow%20the%20huge%20potential%20in%20pixel-wise%20transferring%20relative%20depth%20to%20metric%0Adepth%20with%20language%20assistance.%20%28Code%20is%20available%20at%3A%0Ahttps%3A//github.com/BeileiCui/TR2M%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13387v1&entry.124074799=Read"},
{"title": "HELENA: High-Efficiency Learning-based channel Estimation using dual\n  Neural Attention", "author": "Miguel Camelo Botero and Esra Aycan Beyazit and Nina Slamnik-Krije\u0161torac and Johann M. Marquez-Barja", "abstract": "  Accurate channel estimation is critical for high-performance Orthogonal\nFrequency-Division Multiplexing systems such as 5G New Radio, particularly\nunder low signal-to-noise ratio and stringent latency constraints. This letter\npresents HELENA, a compact deep learning model that combines a lightweight\nconvolutional backbone with two efficient attention mechanisms: patch-wise\nmulti-head self-attention for capturing global dependencies and a\nsqueeze-and-excitation block for local feature refinement. Compared to CEViT, a\nstate-of-the-art vision transformer-based estimator, HELENA reduces inference\ntime by 45.0\\% (0.175\\,ms vs.\\ 0.318\\,ms), achieves comparable accuracy\n($-16.78$\\,dB vs.\\ $-17.30$\\,dB), and requires $8\\times$ fewer parameters\n(0.11M vs.\\ 0.88M), demonstrating its suitability for low-latency, real-time\ndeployment.\n", "link": "http://arxiv.org/abs/2506.13408v1", "date": "2025-06-16", "relevancy": 1.8468, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4701}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4564}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HELENA%3A%20High-Efficiency%20Learning-based%20channel%20Estimation%20using%20dual%0A%20%20Neural%20Attention&body=Title%3A%20HELENA%3A%20High-Efficiency%20Learning-based%20channel%20Estimation%20using%20dual%0A%20%20Neural%20Attention%0AAuthor%3A%20Miguel%20Camelo%20Botero%20and%20Esra%20Aycan%20Beyazit%20and%20Nina%20Slamnik-Krije%C5%A1torac%20and%20Johann%20M.%20Marquez-Barja%0AAbstract%3A%20%20%20Accurate%20channel%20estimation%20is%20critical%20for%20high-performance%20Orthogonal%0AFrequency-Division%20Multiplexing%20systems%20such%20as%205G%20New%20Radio%2C%20particularly%0Aunder%20low%20signal-to-noise%20ratio%20and%20stringent%20latency%20constraints.%20This%20letter%0Apresents%20HELENA%2C%20a%20compact%20deep%20learning%20model%20that%20combines%20a%20lightweight%0Aconvolutional%20backbone%20with%20two%20efficient%20attention%20mechanisms%3A%20patch-wise%0Amulti-head%20self-attention%20for%20capturing%20global%20dependencies%20and%20a%0Asqueeze-and-excitation%20block%20for%20local%20feature%20refinement.%20Compared%20to%20CEViT%2C%20a%0Astate-of-the-art%20vision%20transformer-based%20estimator%2C%20HELENA%20reduces%20inference%0Atime%20by%2045.0%5C%25%20%280.175%5C%2Cms%20vs.%5C%200.318%5C%2Cms%29%2C%20achieves%20comparable%20accuracy%0A%28%24-16.78%24%5C%2CdB%20vs.%5C%20%24-17.30%24%5C%2CdB%29%2C%20and%20requires%20%248%5Ctimes%24%20fewer%20parameters%0A%280.11M%20vs.%5C%200.88M%29%2C%20demonstrating%20its%20suitability%20for%20low-latency%2C%20real-time%0Adeployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHELENA%253A%2520High-Efficiency%2520Learning-based%2520channel%2520Estimation%2520using%2520dual%250A%2520%2520Neural%2520Attention%26entry.906535625%3DMiguel%2520Camelo%2520Botero%2520and%2520Esra%2520Aycan%2520Beyazit%2520and%2520Nina%2520Slamnik-Krije%25C5%25A1torac%2520and%2520Johann%2520M.%2520Marquez-Barja%26entry.1292438233%3D%2520%2520Accurate%2520channel%2520estimation%2520is%2520critical%2520for%2520high-performance%2520Orthogonal%250AFrequency-Division%2520Multiplexing%2520systems%2520such%2520as%25205G%2520New%2520Radio%252C%2520particularly%250Aunder%2520low%2520signal-to-noise%2520ratio%2520and%2520stringent%2520latency%2520constraints.%2520This%2520letter%250Apresents%2520HELENA%252C%2520a%2520compact%2520deep%2520learning%2520model%2520that%2520combines%2520a%2520lightweight%250Aconvolutional%2520backbone%2520with%2520two%2520efficient%2520attention%2520mechanisms%253A%2520patch-wise%250Amulti-head%2520self-attention%2520for%2520capturing%2520global%2520dependencies%2520and%2520a%250Asqueeze-and-excitation%2520block%2520for%2520local%2520feature%2520refinement.%2520Compared%2520to%2520CEViT%252C%2520a%250Astate-of-the-art%2520vision%2520transformer-based%2520estimator%252C%2520HELENA%2520reduces%2520inference%250Atime%2520by%252045.0%255C%2525%2520%25280.175%255C%252Cms%2520vs.%255C%25200.318%255C%252Cms%2529%252C%2520achieves%2520comparable%2520accuracy%250A%2528%2524-16.78%2524%255C%252CdB%2520vs.%255C%2520%2524-17.30%2524%255C%252CdB%2529%252C%2520and%2520requires%2520%25248%255Ctimes%2524%2520fewer%2520parameters%250A%25280.11M%2520vs.%255C%25200.88M%2529%252C%2520demonstrating%2520its%2520suitability%2520for%2520low-latency%252C%2520real-time%250Adeployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HELENA%3A%20High-Efficiency%20Learning-based%20channel%20Estimation%20using%20dual%0A%20%20Neural%20Attention&entry.906535625=Miguel%20Camelo%20Botero%20and%20Esra%20Aycan%20Beyazit%20and%20Nina%20Slamnik-Krije%C5%A1torac%20and%20Johann%20M.%20Marquez-Barja&entry.1292438233=%20%20Accurate%20channel%20estimation%20is%20critical%20for%20high-performance%20Orthogonal%0AFrequency-Division%20Multiplexing%20systems%20such%20as%205G%20New%20Radio%2C%20particularly%0Aunder%20low%20signal-to-noise%20ratio%20and%20stringent%20latency%20constraints.%20This%20letter%0Apresents%20HELENA%2C%20a%20compact%20deep%20learning%20model%20that%20combines%20a%20lightweight%0Aconvolutional%20backbone%20with%20two%20efficient%20attention%20mechanisms%3A%20patch-wise%0Amulti-head%20self-attention%20for%20capturing%20global%20dependencies%20and%20a%0Asqueeze-and-excitation%20block%20for%20local%20feature%20refinement.%20Compared%20to%20CEViT%2C%20a%0Astate-of-the-art%20vision%20transformer-based%20estimator%2C%20HELENA%20reduces%20inference%0Atime%20by%2045.0%5C%25%20%280.175%5C%2Cms%20vs.%5C%200.318%5C%2Cms%29%2C%20achieves%20comparable%20accuracy%0A%28%24-16.78%24%5C%2CdB%20vs.%5C%20%24-17.30%24%5C%2CdB%29%2C%20and%20requires%20%248%5Ctimes%24%20fewer%20parameters%0A%280.11M%20vs.%5C%200.88M%29%2C%20demonstrating%20its%20suitability%20for%20low-latency%2C%20real-time%0Adeployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13408v1&entry.124074799=Read"},
{"title": "Optimistic Q-learning for average reward and episodic reinforcement\n  learning", "author": "Priyank Agrawal and Shipra Agrawal", "abstract": "  We present an optimistic Q-learning algorithm for regret minimization in\naverage reward reinforcement learning under an additional assumption on the\nunderlying MDP that for all policies, the time to visit some frequent state\n$s_0$ is finite and upper bounded by $H$, either in expectation or with\nconstant probability. Our setting strictly generalizes the episodic setting and\nis significantly less restrictive than the assumption of bounded hitting time\n\\textit{for all states} made by most previous literature on model-free\nalgorithms in average reward settings. We demonstrate a regret bound of\n$\\tilde{O}(H^5 S\\sqrt{AT})$, where $S$ and $A$ are the numbers of states and\nactions, and $T$ is the horizon. A key technical novelty of our work is the\nintroduction of an $\\overline{L}$ operator defined as $\\overline{L} v =\n\\frac{1}{H} \\sum_{h=1}^H L^h v$ where $L$ denotes the Bellman operator. Under\nthe given assumption, we show that the $\\overline{L}$ operator has a strict\ncontraction (in span) even in the average-reward setting where the discount\nfactor is $1$. Our algorithm design uses ideas from episodic Q-learning to\nestimate and apply this operator iteratively. Thus, we provide a unified view\nof regret minimization in episodic and non-episodic settings, which may be of\nindependent interest.\n", "link": "http://arxiv.org/abs/2407.13743v3", "date": "2025-06-16", "relevancy": 1.7425, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4392}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4376}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimistic%20Q-learning%20for%20average%20reward%20and%20episodic%20reinforcement%0A%20%20learning&body=Title%3A%20Optimistic%20Q-learning%20for%20average%20reward%20and%20episodic%20reinforcement%0A%20%20learning%0AAuthor%3A%20Priyank%20Agrawal%20and%20Shipra%20Agrawal%0AAbstract%3A%20%20%20We%20present%20an%20optimistic%20Q-learning%20algorithm%20for%20regret%20minimization%20in%0Aaverage%20reward%20reinforcement%20learning%20under%20an%20additional%20assumption%20on%20the%0Aunderlying%20MDP%20that%20for%20all%20policies%2C%20the%20time%20to%20visit%20some%20frequent%20state%0A%24s_0%24%20is%20finite%20and%20upper%20bounded%20by%20%24H%24%2C%20either%20in%20expectation%20or%20with%0Aconstant%20probability.%20Our%20setting%20strictly%20generalizes%20the%20episodic%20setting%20and%0Ais%20significantly%20less%20restrictive%20than%20the%20assumption%20of%20bounded%20hitting%20time%0A%5Ctextit%7Bfor%20all%20states%7D%20made%20by%20most%20previous%20literature%20on%20model-free%0Aalgorithms%20in%20average%20reward%20settings.%20We%20demonstrate%20a%20regret%20bound%20of%0A%24%5Ctilde%7BO%7D%28H%5E5%20S%5Csqrt%7BAT%7D%29%24%2C%20where%20%24S%24%20and%20%24A%24%20are%20the%20numbers%20of%20states%20and%0Aactions%2C%20and%20%24T%24%20is%20the%20horizon.%20A%20key%20technical%20novelty%20of%20our%20work%20is%20the%0Aintroduction%20of%20an%20%24%5Coverline%7BL%7D%24%20operator%20defined%20as%20%24%5Coverline%7BL%7D%20v%20%3D%0A%5Cfrac%7B1%7D%7BH%7D%20%5Csum_%7Bh%3D1%7D%5EH%20L%5Eh%20v%24%20where%20%24L%24%20denotes%20the%20Bellman%20operator.%20Under%0Athe%20given%20assumption%2C%20we%20show%20that%20the%20%24%5Coverline%7BL%7D%24%20operator%20has%20a%20strict%0Acontraction%20%28in%20span%29%20even%20in%20the%20average-reward%20setting%20where%20the%20discount%0Afactor%20is%20%241%24.%20Our%20algorithm%20design%20uses%20ideas%20from%20episodic%20Q-learning%20to%0Aestimate%20and%20apply%20this%20operator%20iteratively.%20Thus%2C%20we%20provide%20a%20unified%20view%0Aof%20regret%20minimization%20in%20episodic%20and%20non-episodic%20settings%2C%20which%20may%20be%20of%0Aindependent%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13743v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimistic%2520Q-learning%2520for%2520average%2520reward%2520and%2520episodic%2520reinforcement%250A%2520%2520learning%26entry.906535625%3DPriyank%2520Agrawal%2520and%2520Shipra%2520Agrawal%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520optimistic%2520Q-learning%2520algorithm%2520for%2520regret%2520minimization%2520in%250Aaverage%2520reward%2520reinforcement%2520learning%2520under%2520an%2520additional%2520assumption%2520on%2520the%250Aunderlying%2520MDP%2520that%2520for%2520all%2520policies%252C%2520the%2520time%2520to%2520visit%2520some%2520frequent%2520state%250A%2524s_0%2524%2520is%2520finite%2520and%2520upper%2520bounded%2520by%2520%2524H%2524%252C%2520either%2520in%2520expectation%2520or%2520with%250Aconstant%2520probability.%2520Our%2520setting%2520strictly%2520generalizes%2520the%2520episodic%2520setting%2520and%250Ais%2520significantly%2520less%2520restrictive%2520than%2520the%2520assumption%2520of%2520bounded%2520hitting%2520time%250A%255Ctextit%257Bfor%2520all%2520states%257D%2520made%2520by%2520most%2520previous%2520literature%2520on%2520model-free%250Aalgorithms%2520in%2520average%2520reward%2520settings.%2520We%2520demonstrate%2520a%2520regret%2520bound%2520of%250A%2524%255Ctilde%257BO%257D%2528H%255E5%2520S%255Csqrt%257BAT%257D%2529%2524%252C%2520where%2520%2524S%2524%2520and%2520%2524A%2524%2520are%2520the%2520numbers%2520of%2520states%2520and%250Aactions%252C%2520and%2520%2524T%2524%2520is%2520the%2520horizon.%2520A%2520key%2520technical%2520novelty%2520of%2520our%2520work%2520is%2520the%250Aintroduction%2520of%2520an%2520%2524%255Coverline%257BL%257D%2524%2520operator%2520defined%2520as%2520%2524%255Coverline%257BL%257D%2520v%2520%253D%250A%255Cfrac%257B1%257D%257BH%257D%2520%255Csum_%257Bh%253D1%257D%255EH%2520L%255Eh%2520v%2524%2520where%2520%2524L%2524%2520denotes%2520the%2520Bellman%2520operator.%2520Under%250Athe%2520given%2520assumption%252C%2520we%2520show%2520that%2520the%2520%2524%255Coverline%257BL%257D%2524%2520operator%2520has%2520a%2520strict%250Acontraction%2520%2528in%2520span%2529%2520even%2520in%2520the%2520average-reward%2520setting%2520where%2520the%2520discount%250Afactor%2520is%2520%25241%2524.%2520Our%2520algorithm%2520design%2520uses%2520ideas%2520from%2520episodic%2520Q-learning%2520to%250Aestimate%2520and%2520apply%2520this%2520operator%2520iteratively.%2520Thus%252C%2520we%2520provide%2520a%2520unified%2520view%250Aof%2520regret%2520minimization%2520in%2520episodic%2520and%2520non-episodic%2520settings%252C%2520which%2520may%2520be%2520of%250Aindependent%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13743v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimistic%20Q-learning%20for%20average%20reward%20and%20episodic%20reinforcement%0A%20%20learning&entry.906535625=Priyank%20Agrawal%20and%20Shipra%20Agrawal&entry.1292438233=%20%20We%20present%20an%20optimistic%20Q-learning%20algorithm%20for%20regret%20minimization%20in%0Aaverage%20reward%20reinforcement%20learning%20under%20an%20additional%20assumption%20on%20the%0Aunderlying%20MDP%20that%20for%20all%20policies%2C%20the%20time%20to%20visit%20some%20frequent%20state%0A%24s_0%24%20is%20finite%20and%20upper%20bounded%20by%20%24H%24%2C%20either%20in%20expectation%20or%20with%0Aconstant%20probability.%20Our%20setting%20strictly%20generalizes%20the%20episodic%20setting%20and%0Ais%20significantly%20less%20restrictive%20than%20the%20assumption%20of%20bounded%20hitting%20time%0A%5Ctextit%7Bfor%20all%20states%7D%20made%20by%20most%20previous%20literature%20on%20model-free%0Aalgorithms%20in%20average%20reward%20settings.%20We%20demonstrate%20a%20regret%20bound%20of%0A%24%5Ctilde%7BO%7D%28H%5E5%20S%5Csqrt%7BAT%7D%29%24%2C%20where%20%24S%24%20and%20%24A%24%20are%20the%20numbers%20of%20states%20and%0Aactions%2C%20and%20%24T%24%20is%20the%20horizon.%20A%20key%20technical%20novelty%20of%20our%20work%20is%20the%0Aintroduction%20of%20an%20%24%5Coverline%7BL%7D%24%20operator%20defined%20as%20%24%5Coverline%7BL%7D%20v%20%3D%0A%5Cfrac%7B1%7D%7BH%7D%20%5Csum_%7Bh%3D1%7D%5EH%20L%5Eh%20v%24%20where%20%24L%24%20denotes%20the%20Bellman%20operator.%20Under%0Athe%20given%20assumption%2C%20we%20show%20that%20the%20%24%5Coverline%7BL%7D%24%20operator%20has%20a%20strict%0Acontraction%20%28in%20span%29%20even%20in%20the%20average-reward%20setting%20where%20the%20discount%0Afactor%20is%20%241%24.%20Our%20algorithm%20design%20uses%20ideas%20from%20episodic%20Q-learning%20to%0Aestimate%20and%20apply%20this%20operator%20iteratively.%20Thus%2C%20we%20provide%20a%20unified%20view%0Aof%20regret%20minimization%20in%20episodic%20and%20non-episodic%20settings%2C%20which%20may%20be%20of%0Aindependent%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13743v3&entry.124074799=Read"},
{"title": "Riemann Tensor Neural Networks: Learning Conservative Systems with\n  Physics-Constrained Networks", "author": "Anas Jnini and Lorenzo Breschi and Flavio Vella", "abstract": "  Divergence-free symmetric tensors (DFSTs) are fundamental in continuum\nmechanics, encoding conservation laws such as mass and momentum conservation.\nWe introduce Riemann Tensor Neural Networks (RTNNs), a novel neural\narchitecture that inherently satisfies the DFST condition to machine precision,\nproviding a strong inductive bias for enforcing these conservation laws. We\nprove that RTNNs can approximate any sufficiently smooth DFST with arbitrary\nprecision and demonstrate their effectiveness as surrogates for conservative\nPDEs, achieving improved accuracy across benchmarks. This work is the first to\nuse DFSTs as an inductive bias in neural PDE surrogates and to explicitly\nenforce the conservation of both mass and momentum within a physics-constrained\nneural architecture.\n", "link": "http://arxiv.org/abs/2503.00755v2", "date": "2025-06-16", "relevancy": 1.9166, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5159}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4731}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Riemann%20Tensor%20Neural%20Networks%3A%20Learning%20Conservative%20Systems%20with%0A%20%20Physics-Constrained%20Networks&body=Title%3A%20Riemann%20Tensor%20Neural%20Networks%3A%20Learning%20Conservative%20Systems%20with%0A%20%20Physics-Constrained%20Networks%0AAuthor%3A%20Anas%20Jnini%20and%20Lorenzo%20Breschi%20and%20Flavio%20Vella%0AAbstract%3A%20%20%20Divergence-free%20symmetric%20tensors%20%28DFSTs%29%20are%20fundamental%20in%20continuum%0Amechanics%2C%20encoding%20conservation%20laws%20such%20as%20mass%20and%20momentum%20conservation.%0AWe%20introduce%20Riemann%20Tensor%20Neural%20Networks%20%28RTNNs%29%2C%20a%20novel%20neural%0Aarchitecture%20that%20inherently%20satisfies%20the%20DFST%20condition%20to%20machine%20precision%2C%0Aproviding%20a%20strong%20inductive%20bias%20for%20enforcing%20these%20conservation%20laws.%20We%0Aprove%20that%20RTNNs%20can%20approximate%20any%20sufficiently%20smooth%20DFST%20with%20arbitrary%0Aprecision%20and%20demonstrate%20their%20effectiveness%20as%20surrogates%20for%20conservative%0APDEs%2C%20achieving%20improved%20accuracy%20across%20benchmarks.%20This%20work%20is%20the%20first%20to%0Ause%20DFSTs%20as%20an%20inductive%20bias%20in%20neural%20PDE%20surrogates%20and%20to%20explicitly%0Aenforce%20the%20conservation%20of%20both%20mass%20and%20momentum%20within%20a%20physics-constrained%0Aneural%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.00755v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRiemann%2520Tensor%2520Neural%2520Networks%253A%2520Learning%2520Conservative%2520Systems%2520with%250A%2520%2520Physics-Constrained%2520Networks%26entry.906535625%3DAnas%2520Jnini%2520and%2520Lorenzo%2520Breschi%2520and%2520Flavio%2520Vella%26entry.1292438233%3D%2520%2520Divergence-free%2520symmetric%2520tensors%2520%2528DFSTs%2529%2520are%2520fundamental%2520in%2520continuum%250Amechanics%252C%2520encoding%2520conservation%2520laws%2520such%2520as%2520mass%2520and%2520momentum%2520conservation.%250AWe%2520introduce%2520Riemann%2520Tensor%2520Neural%2520Networks%2520%2528RTNNs%2529%252C%2520a%2520novel%2520neural%250Aarchitecture%2520that%2520inherently%2520satisfies%2520the%2520DFST%2520condition%2520to%2520machine%2520precision%252C%250Aproviding%2520a%2520strong%2520inductive%2520bias%2520for%2520enforcing%2520these%2520conservation%2520laws.%2520We%250Aprove%2520that%2520RTNNs%2520can%2520approximate%2520any%2520sufficiently%2520smooth%2520DFST%2520with%2520arbitrary%250Aprecision%2520and%2520demonstrate%2520their%2520effectiveness%2520as%2520surrogates%2520for%2520conservative%250APDEs%252C%2520achieving%2520improved%2520accuracy%2520across%2520benchmarks.%2520This%2520work%2520is%2520the%2520first%2520to%250Ause%2520DFSTs%2520as%2520an%2520inductive%2520bias%2520in%2520neural%2520PDE%2520surrogates%2520and%2520to%2520explicitly%250Aenforce%2520the%2520conservation%2520of%2520both%2520mass%2520and%2520momentum%2520within%2520a%2520physics-constrained%250Aneural%2520architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.00755v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Riemann%20Tensor%20Neural%20Networks%3A%20Learning%20Conservative%20Systems%20with%0A%20%20Physics-Constrained%20Networks&entry.906535625=Anas%20Jnini%20and%20Lorenzo%20Breschi%20and%20Flavio%20Vella&entry.1292438233=%20%20Divergence-free%20symmetric%20tensors%20%28DFSTs%29%20are%20fundamental%20in%20continuum%0Amechanics%2C%20encoding%20conservation%20laws%20such%20as%20mass%20and%20momentum%20conservation.%0AWe%20introduce%20Riemann%20Tensor%20Neural%20Networks%20%28RTNNs%29%2C%20a%20novel%20neural%0Aarchitecture%20that%20inherently%20satisfies%20the%20DFST%20condition%20to%20machine%20precision%2C%0Aproviding%20a%20strong%20inductive%20bias%20for%20enforcing%20these%20conservation%20laws.%20We%0Aprove%20that%20RTNNs%20can%20approximate%20any%20sufficiently%20smooth%20DFST%20with%20arbitrary%0Aprecision%20and%20demonstrate%20their%20effectiveness%20as%20surrogates%20for%20conservative%0APDEs%2C%20achieving%20improved%20accuracy%20across%20benchmarks.%20This%20work%20is%20the%20first%20to%0Ause%20DFSTs%20as%20an%20inductive%20bias%20in%20neural%20PDE%20surrogates%20and%20to%20explicitly%0Aenforce%20the%20conservation%20of%20both%20mass%20and%20momentum%20within%20a%20physics-constrained%0Aneural%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.00755v2&entry.124074799=Read"},
{"title": "A Semantically-Aware Relevance Measure for Content-Based Medical Image\n  Retrieval Evaluation", "author": "Xiaoyang Wei and Camille Kurtz and Florence Cloppet", "abstract": "  Performance evaluation for Content-Based Image Retrieval (CBIR) remains a\ncrucial but unsolved problem today especially in the medical domain. Various\nevaluation metrics have been discussed in the literature to solve this problem.\nMost of the existing metrics (e.g., precision, recall) are adapted from\nclassification tasks which require manual labels as ground truth. However, such\nlabels are often expensive and unavailable in specific thematic domains.\nFurthermore, medical images are usually associated with (radiological) case\nreports or annotated with descriptive captions in literature figures, such text\ncontains information that can help to assess CBIR.Several researchers have\nargued that the medical concepts hidden in the text can serve as the basis for\nCBIR evaluation purpose. However, these works often consider these medical\nconcepts as independent and isolated labels while in fact the subtle\nrelationships between various concepts are neglected. In this work, we\nintroduce the use of knowledge graphs to measure the distance between various\nmedical concepts and propose a novel relevance measure for the evaluation of\nCBIR by defining an approximate matching-based relevance score between two sets\nof medical concepts which allows us to indirectly measure the similarity\nbetween medical images.We quantitatively demonstrate the effectiveness and\nfeasibility of our relevance measure using a public dataset.\n", "link": "http://arxiv.org/abs/2506.13509v1", "date": "2025-06-16", "relevancy": 1.3553, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4747}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Semantically-Aware%20Relevance%20Measure%20for%20Content-Based%20Medical%20Image%0A%20%20Retrieval%20Evaluation&body=Title%3A%20A%20Semantically-Aware%20Relevance%20Measure%20for%20Content-Based%20Medical%20Image%0A%20%20Retrieval%20Evaluation%0AAuthor%3A%20Xiaoyang%20Wei%20and%20Camille%20Kurtz%20and%20Florence%20Cloppet%0AAbstract%3A%20%20%20Performance%20evaluation%20for%20Content-Based%20Image%20Retrieval%20%28CBIR%29%20remains%20a%0Acrucial%20but%20unsolved%20problem%20today%20especially%20in%20the%20medical%20domain.%20Various%0Aevaluation%20metrics%20have%20been%20discussed%20in%20the%20literature%20to%20solve%20this%20problem.%0AMost%20of%20the%20existing%20metrics%20%28e.g.%2C%20precision%2C%20recall%29%20are%20adapted%20from%0Aclassification%20tasks%20which%20require%20manual%20labels%20as%20ground%20truth.%20However%2C%20such%0Alabels%20are%20often%20expensive%20and%20unavailable%20in%20specific%20thematic%20domains.%0AFurthermore%2C%20medical%20images%20are%20usually%20associated%20with%20%28radiological%29%20case%0Areports%20or%20annotated%20with%20descriptive%20captions%20in%20literature%20figures%2C%20such%20text%0Acontains%20information%20that%20can%20help%20to%20assess%20CBIR.Several%20researchers%20have%0Aargued%20that%20the%20medical%20concepts%20hidden%20in%20the%20text%20can%20serve%20as%20the%20basis%20for%0ACBIR%20evaluation%20purpose.%20However%2C%20these%20works%20often%20consider%20these%20medical%0Aconcepts%20as%20independent%20and%20isolated%20labels%20while%20in%20fact%20the%20subtle%0Arelationships%20between%20various%20concepts%20are%20neglected.%20In%20this%20work%2C%20we%0Aintroduce%20the%20use%20of%20knowledge%20graphs%20to%20measure%20the%20distance%20between%20various%0Amedical%20concepts%20and%20propose%20a%20novel%20relevance%20measure%20for%20the%20evaluation%20of%0ACBIR%20by%20defining%20an%20approximate%20matching-based%20relevance%20score%20between%20two%20sets%0Aof%20medical%20concepts%20which%20allows%20us%20to%20indirectly%20measure%20the%20similarity%0Abetween%20medical%20images.We%20quantitatively%20demonstrate%20the%20effectiveness%20and%0Afeasibility%20of%20our%20relevance%20measure%20using%20a%20public%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Semantically-Aware%2520Relevance%2520Measure%2520for%2520Content-Based%2520Medical%2520Image%250A%2520%2520Retrieval%2520Evaluation%26entry.906535625%3DXiaoyang%2520Wei%2520and%2520Camille%2520Kurtz%2520and%2520Florence%2520Cloppet%26entry.1292438233%3D%2520%2520Performance%2520evaluation%2520for%2520Content-Based%2520Image%2520Retrieval%2520%2528CBIR%2529%2520remains%2520a%250Acrucial%2520but%2520unsolved%2520problem%2520today%2520especially%2520in%2520the%2520medical%2520domain.%2520Various%250Aevaluation%2520metrics%2520have%2520been%2520discussed%2520in%2520the%2520literature%2520to%2520solve%2520this%2520problem.%250AMost%2520of%2520the%2520existing%2520metrics%2520%2528e.g.%252C%2520precision%252C%2520recall%2529%2520are%2520adapted%2520from%250Aclassification%2520tasks%2520which%2520require%2520manual%2520labels%2520as%2520ground%2520truth.%2520However%252C%2520such%250Alabels%2520are%2520often%2520expensive%2520and%2520unavailable%2520in%2520specific%2520thematic%2520domains.%250AFurthermore%252C%2520medical%2520images%2520are%2520usually%2520associated%2520with%2520%2528radiological%2529%2520case%250Areports%2520or%2520annotated%2520with%2520descriptive%2520captions%2520in%2520literature%2520figures%252C%2520such%2520text%250Acontains%2520information%2520that%2520can%2520help%2520to%2520assess%2520CBIR.Several%2520researchers%2520have%250Aargued%2520that%2520the%2520medical%2520concepts%2520hidden%2520in%2520the%2520text%2520can%2520serve%2520as%2520the%2520basis%2520for%250ACBIR%2520evaluation%2520purpose.%2520However%252C%2520these%2520works%2520often%2520consider%2520these%2520medical%250Aconcepts%2520as%2520independent%2520and%2520isolated%2520labels%2520while%2520in%2520fact%2520the%2520subtle%250Arelationships%2520between%2520various%2520concepts%2520are%2520neglected.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520the%2520use%2520of%2520knowledge%2520graphs%2520to%2520measure%2520the%2520distance%2520between%2520various%250Amedical%2520concepts%2520and%2520propose%2520a%2520novel%2520relevance%2520measure%2520for%2520the%2520evaluation%2520of%250ACBIR%2520by%2520defining%2520an%2520approximate%2520matching-based%2520relevance%2520score%2520between%2520two%2520sets%250Aof%2520medical%2520concepts%2520which%2520allows%2520us%2520to%2520indirectly%2520measure%2520the%2520similarity%250Abetween%2520medical%2520images.We%2520quantitatively%2520demonstrate%2520the%2520effectiveness%2520and%250Afeasibility%2520of%2520our%2520relevance%2520measure%2520using%2520a%2520public%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Semantically-Aware%20Relevance%20Measure%20for%20Content-Based%20Medical%20Image%0A%20%20Retrieval%20Evaluation&entry.906535625=Xiaoyang%20Wei%20and%20Camille%20Kurtz%20and%20Florence%20Cloppet&entry.1292438233=%20%20Performance%20evaluation%20for%20Content-Based%20Image%20Retrieval%20%28CBIR%29%20remains%20a%0Acrucial%20but%20unsolved%20problem%20today%20especially%20in%20the%20medical%20domain.%20Various%0Aevaluation%20metrics%20have%20been%20discussed%20in%20the%20literature%20to%20solve%20this%20problem.%0AMost%20of%20the%20existing%20metrics%20%28e.g.%2C%20precision%2C%20recall%29%20are%20adapted%20from%0Aclassification%20tasks%20which%20require%20manual%20labels%20as%20ground%20truth.%20However%2C%20such%0Alabels%20are%20often%20expensive%20and%20unavailable%20in%20specific%20thematic%20domains.%0AFurthermore%2C%20medical%20images%20are%20usually%20associated%20with%20%28radiological%29%20case%0Areports%20or%20annotated%20with%20descriptive%20captions%20in%20literature%20figures%2C%20such%20text%0Acontains%20information%20that%20can%20help%20to%20assess%20CBIR.Several%20researchers%20have%0Aargued%20that%20the%20medical%20concepts%20hidden%20in%20the%20text%20can%20serve%20as%20the%20basis%20for%0ACBIR%20evaluation%20purpose.%20However%2C%20these%20works%20often%20consider%20these%20medical%0Aconcepts%20as%20independent%20and%20isolated%20labels%20while%20in%20fact%20the%20subtle%0Arelationships%20between%20various%20concepts%20are%20neglected.%20In%20this%20work%2C%20we%0Aintroduce%20the%20use%20of%20knowledge%20graphs%20to%20measure%20the%20distance%20between%20various%0Amedical%20concepts%20and%20propose%20a%20novel%20relevance%20measure%20for%20the%20evaluation%20of%0ACBIR%20by%20defining%20an%20approximate%20matching-based%20relevance%20score%20between%20two%20sets%0Aof%20medical%20concepts%20which%20allows%20us%20to%20indirectly%20measure%20the%20similarity%0Abetween%20medical%20images.We%20quantitatively%20demonstrate%20the%20effectiveness%20and%0Afeasibility%20of%20our%20relevance%20measure%20using%20a%20public%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13509v1&entry.124074799=Read"},
{"title": "Edge Nearest Neighbor in Sampling-Based Motion Planning", "author": "Stav Ashur and Nancy M. Amato and Sariel Har-Peled", "abstract": "  Neighborhood finders and nearest neighbor queries are fundamental parts of\nsampling based motion planning algorithms. Using different distance metrics or\notherwise changing the definition of a neighborhood produces different\nalgorithms with unique empiric and theoretical properties. In \\cite{l-pa-06}\nLaValle suggests a neighborhood finder for the Rapidly-exploring Random Tree\nRRT\n  algorithm \\cite{l-rrtnt-98} which finds the nearest neighbor of the sampled\npoint on the swath of the tree, that is on the set of all of the points on the\ntree edges, using a hierarchical data structure. In this paper we implement\nsuch a neighborhood finder and show, theoretically and experimentally, that\nthis results in more efficient algorithms, and suggest a variant of the\nRapidly-exploring Random Graph RRG algorithm \\cite{f-isaom-10} that better\nexploits the exploration properties of the newly described subroutine for\nfinding narrow passages.\n", "link": "http://arxiv.org/abs/2506.13753v1", "date": "2025-06-16", "relevancy": 1.8169, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4697}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4447}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge%20Nearest%20Neighbor%20in%20Sampling-Based%20Motion%20Planning&body=Title%3A%20Edge%20Nearest%20Neighbor%20in%20Sampling-Based%20Motion%20Planning%0AAuthor%3A%20Stav%20Ashur%20and%20Nancy%20M.%20Amato%20and%20Sariel%20Har-Peled%0AAbstract%3A%20%20%20Neighborhood%20finders%20and%20nearest%20neighbor%20queries%20are%20fundamental%20parts%20of%0Asampling%20based%20motion%20planning%20algorithms.%20Using%20different%20distance%20metrics%20or%0Aotherwise%20changing%20the%20definition%20of%20a%20neighborhood%20produces%20different%0Aalgorithms%20with%20unique%20empiric%20and%20theoretical%20properties.%20In%20%5Ccite%7Bl-pa-06%7D%0ALaValle%20suggests%20a%20neighborhood%20finder%20for%20the%20Rapidly-exploring%20Random%20Tree%0ARRT%0A%20%20algorithm%20%5Ccite%7Bl-rrtnt-98%7D%20which%20finds%20the%20nearest%20neighbor%20of%20the%20sampled%0Apoint%20on%20the%20swath%20of%20the%20tree%2C%20that%20is%20on%20the%20set%20of%20all%20of%20the%20points%20on%20the%0Atree%20edges%2C%20using%20a%20hierarchical%20data%20structure.%20In%20this%20paper%20we%20implement%0Asuch%20a%20neighborhood%20finder%20and%20show%2C%20theoretically%20and%20experimentally%2C%20that%0Athis%20results%20in%20more%20efficient%20algorithms%2C%20and%20suggest%20a%20variant%20of%20the%0ARapidly-exploring%20Random%20Graph%20RRG%20algorithm%20%5Ccite%7Bf-isaom-10%7D%20that%20better%0Aexploits%20the%20exploration%20properties%20of%20the%20newly%20described%20subroutine%20for%0Afinding%20narrow%20passages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge%2520Nearest%2520Neighbor%2520in%2520Sampling-Based%2520Motion%2520Planning%26entry.906535625%3DStav%2520Ashur%2520and%2520Nancy%2520M.%2520Amato%2520and%2520Sariel%2520Har-Peled%26entry.1292438233%3D%2520%2520Neighborhood%2520finders%2520and%2520nearest%2520neighbor%2520queries%2520are%2520fundamental%2520parts%2520of%250Asampling%2520based%2520motion%2520planning%2520algorithms.%2520Using%2520different%2520distance%2520metrics%2520or%250Aotherwise%2520changing%2520the%2520definition%2520of%2520a%2520neighborhood%2520produces%2520different%250Aalgorithms%2520with%2520unique%2520empiric%2520and%2520theoretical%2520properties.%2520In%2520%255Ccite%257Bl-pa-06%257D%250ALaValle%2520suggests%2520a%2520neighborhood%2520finder%2520for%2520the%2520Rapidly-exploring%2520Random%2520Tree%250ARRT%250A%2520%2520algorithm%2520%255Ccite%257Bl-rrtnt-98%257D%2520which%2520finds%2520the%2520nearest%2520neighbor%2520of%2520the%2520sampled%250Apoint%2520on%2520the%2520swath%2520of%2520the%2520tree%252C%2520that%2520is%2520on%2520the%2520set%2520of%2520all%2520of%2520the%2520points%2520on%2520the%250Atree%2520edges%252C%2520using%2520a%2520hierarchical%2520data%2520structure.%2520In%2520this%2520paper%2520we%2520implement%250Asuch%2520a%2520neighborhood%2520finder%2520and%2520show%252C%2520theoretically%2520and%2520experimentally%252C%2520that%250Athis%2520results%2520in%2520more%2520efficient%2520algorithms%252C%2520and%2520suggest%2520a%2520variant%2520of%2520the%250ARapidly-exploring%2520Random%2520Graph%2520RRG%2520algorithm%2520%255Ccite%257Bf-isaom-10%257D%2520that%2520better%250Aexploits%2520the%2520exploration%2520properties%2520of%2520the%2520newly%2520described%2520subroutine%2520for%250Afinding%2520narrow%2520passages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge%20Nearest%20Neighbor%20in%20Sampling-Based%20Motion%20Planning&entry.906535625=Stav%20Ashur%20and%20Nancy%20M.%20Amato%20and%20Sariel%20Har-Peled&entry.1292438233=%20%20Neighborhood%20finders%20and%20nearest%20neighbor%20queries%20are%20fundamental%20parts%20of%0Asampling%20based%20motion%20planning%20algorithms.%20Using%20different%20distance%20metrics%20or%0Aotherwise%20changing%20the%20definition%20of%20a%20neighborhood%20produces%20different%0Aalgorithms%20with%20unique%20empiric%20and%20theoretical%20properties.%20In%20%5Ccite%7Bl-pa-06%7D%0ALaValle%20suggests%20a%20neighborhood%20finder%20for%20the%20Rapidly-exploring%20Random%20Tree%0ARRT%0A%20%20algorithm%20%5Ccite%7Bl-rrtnt-98%7D%20which%20finds%20the%20nearest%20neighbor%20of%20the%20sampled%0Apoint%20on%20the%20swath%20of%20the%20tree%2C%20that%20is%20on%20the%20set%20of%20all%20of%20the%20points%20on%20the%0Atree%20edges%2C%20using%20a%20hierarchical%20data%20structure.%20In%20this%20paper%20we%20implement%0Asuch%20a%20neighborhood%20finder%20and%20show%2C%20theoretically%20and%20experimentally%2C%20that%0Athis%20results%20in%20more%20efficient%20algorithms%2C%20and%20suggest%20a%20variant%20of%20the%0ARapidly-exploring%20Random%20Graph%20RRG%20algorithm%20%5Ccite%7Bf-isaom-10%7D%20that%20better%0Aexploits%20the%20exploration%20properties%20of%20the%20newly%20described%20subroutine%20for%0Afinding%20narrow%20passages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13753v1&entry.124074799=Read"},
{"title": "Stimulus Motion Perception Studies Imply Specific Neural Computations in\n  Human Visual Stabilization", "author": "David W Arathorn and Josephine C. D'Angelo and Austin Roorda", "abstract": "  Even during fixation the human eye is constantly in low amplitude motion,\njittering over small angles in random directions at up to 100Hz. This motion\nresults in all features of the image on the retina constantly traversing a\nnumber of cones, yet objects which are stable in the world are perceived to be\nstable, and any object which is moving in the world is perceived to be moving.\nA series of experiments carried out over a dozen years revealed the\npsychophysics of visual stabilization to be more nuanced than might be assumed,\nsay, from the mechanics of stabilization of camera images, or what might be\nassumed to be the simplest solution from an evolutionary perspective. The\npsychophysics revealed by the experiments strongly implies a specific set of\noperations on retinal signals resulting in the observed stabilization behavior.\nThe presentation is in two levels. First is a functional description of the\naction of the mechanism that is very likely responsible for the experimentally\nobserved behavior. Second is a more speculative proposal of circuit-level\nneural elements that might implement the functional behavior.\n", "link": "http://arxiv.org/abs/2506.13506v1", "date": "2025-06-16", "relevancy": 1.9999, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5167}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5069}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stimulus%20Motion%20Perception%20Studies%20Imply%20Specific%20Neural%20Computations%20in%0A%20%20Human%20Visual%20Stabilization&body=Title%3A%20Stimulus%20Motion%20Perception%20Studies%20Imply%20Specific%20Neural%20Computations%20in%0A%20%20Human%20Visual%20Stabilization%0AAuthor%3A%20David%20W%20Arathorn%20and%20Josephine%20C.%20D%27Angelo%20and%20Austin%20Roorda%0AAbstract%3A%20%20%20Even%20during%20fixation%20the%20human%20eye%20is%20constantly%20in%20low%20amplitude%20motion%2C%0Ajittering%20over%20small%20angles%20in%20random%20directions%20at%20up%20to%20100Hz.%20This%20motion%0Aresults%20in%20all%20features%20of%20the%20image%20on%20the%20retina%20constantly%20traversing%20a%0Anumber%20of%20cones%2C%20yet%20objects%20which%20are%20stable%20in%20the%20world%20are%20perceived%20to%20be%0Astable%2C%20and%20any%20object%20which%20is%20moving%20in%20the%20world%20is%20perceived%20to%20be%20moving.%0AA%20series%20of%20experiments%20carried%20out%20over%20a%20dozen%20years%20revealed%20the%0Apsychophysics%20of%20visual%20stabilization%20to%20be%20more%20nuanced%20than%20might%20be%20assumed%2C%0Asay%2C%20from%20the%20mechanics%20of%20stabilization%20of%20camera%20images%2C%20or%20what%20might%20be%0Aassumed%20to%20be%20the%20simplest%20solution%20from%20an%20evolutionary%20perspective.%20The%0Apsychophysics%20revealed%20by%20the%20experiments%20strongly%20implies%20a%20specific%20set%20of%0Aoperations%20on%20retinal%20signals%20resulting%20in%20the%20observed%20stabilization%20behavior.%0AThe%20presentation%20is%20in%20two%20levels.%20First%20is%20a%20functional%20description%20of%20the%0Aaction%20of%20the%20mechanism%20that%20is%20very%20likely%20responsible%20for%20the%20experimentally%0Aobserved%20behavior.%20Second%20is%20a%20more%20speculative%20proposal%20of%20circuit-level%0Aneural%20elements%20that%20might%20implement%20the%20functional%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStimulus%2520Motion%2520Perception%2520Studies%2520Imply%2520Specific%2520Neural%2520Computations%2520in%250A%2520%2520Human%2520Visual%2520Stabilization%26entry.906535625%3DDavid%2520W%2520Arathorn%2520and%2520Josephine%2520C.%2520D%2527Angelo%2520and%2520Austin%2520Roorda%26entry.1292438233%3D%2520%2520Even%2520during%2520fixation%2520the%2520human%2520eye%2520is%2520constantly%2520in%2520low%2520amplitude%2520motion%252C%250Ajittering%2520over%2520small%2520angles%2520in%2520random%2520directions%2520at%2520up%2520to%2520100Hz.%2520This%2520motion%250Aresults%2520in%2520all%2520features%2520of%2520the%2520image%2520on%2520the%2520retina%2520constantly%2520traversing%2520a%250Anumber%2520of%2520cones%252C%2520yet%2520objects%2520which%2520are%2520stable%2520in%2520the%2520world%2520are%2520perceived%2520to%2520be%250Astable%252C%2520and%2520any%2520object%2520which%2520is%2520moving%2520in%2520the%2520world%2520is%2520perceived%2520to%2520be%2520moving.%250AA%2520series%2520of%2520experiments%2520carried%2520out%2520over%2520a%2520dozen%2520years%2520revealed%2520the%250Apsychophysics%2520of%2520visual%2520stabilization%2520to%2520be%2520more%2520nuanced%2520than%2520might%2520be%2520assumed%252C%250Asay%252C%2520from%2520the%2520mechanics%2520of%2520stabilization%2520of%2520camera%2520images%252C%2520or%2520what%2520might%2520be%250Aassumed%2520to%2520be%2520the%2520simplest%2520solution%2520from%2520an%2520evolutionary%2520perspective.%2520The%250Apsychophysics%2520revealed%2520by%2520the%2520experiments%2520strongly%2520implies%2520a%2520specific%2520set%2520of%250Aoperations%2520on%2520retinal%2520signals%2520resulting%2520in%2520the%2520observed%2520stabilization%2520behavior.%250AThe%2520presentation%2520is%2520in%2520two%2520levels.%2520First%2520is%2520a%2520functional%2520description%2520of%2520the%250Aaction%2520of%2520the%2520mechanism%2520that%2520is%2520very%2520likely%2520responsible%2520for%2520the%2520experimentally%250Aobserved%2520behavior.%2520Second%2520is%2520a%2520more%2520speculative%2520proposal%2520of%2520circuit-level%250Aneural%2520elements%2520that%2520might%2520implement%2520the%2520functional%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stimulus%20Motion%20Perception%20Studies%20Imply%20Specific%20Neural%20Computations%20in%0A%20%20Human%20Visual%20Stabilization&entry.906535625=David%20W%20Arathorn%20and%20Josephine%20C.%20D%27Angelo%20and%20Austin%20Roorda&entry.1292438233=%20%20Even%20during%20fixation%20the%20human%20eye%20is%20constantly%20in%20low%20amplitude%20motion%2C%0Ajittering%20over%20small%20angles%20in%20random%20directions%20at%20up%20to%20100Hz.%20This%20motion%0Aresults%20in%20all%20features%20of%20the%20image%20on%20the%20retina%20constantly%20traversing%20a%0Anumber%20of%20cones%2C%20yet%20objects%20which%20are%20stable%20in%20the%20world%20are%20perceived%20to%20be%0Astable%2C%20and%20any%20object%20which%20is%20moving%20in%20the%20world%20is%20perceived%20to%20be%20moving.%0AA%20series%20of%20experiments%20carried%20out%20over%20a%20dozen%20years%20revealed%20the%0Apsychophysics%20of%20visual%20stabilization%20to%20be%20more%20nuanced%20than%20might%20be%20assumed%2C%0Asay%2C%20from%20the%20mechanics%20of%20stabilization%20of%20camera%20images%2C%20or%20what%20might%20be%0Aassumed%20to%20be%20the%20simplest%20solution%20from%20an%20evolutionary%20perspective.%20The%0Apsychophysics%20revealed%20by%20the%20experiments%20strongly%20implies%20a%20specific%20set%20of%0Aoperations%20on%20retinal%20signals%20resulting%20in%20the%20observed%20stabilization%20behavior.%0AThe%20presentation%20is%20in%20two%20levels.%20First%20is%20a%20functional%20description%20of%20the%0Aaction%20of%20the%20mechanism%20that%20is%20very%20likely%20responsible%20for%20the%20experimentally%0Aobserved%20behavior.%20Second%20is%20a%20more%20speculative%20proposal%20of%20circuit-level%0Aneural%20elements%20that%20might%20implement%20the%20functional%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13506v1&entry.124074799=Read"},
{"title": "Pursuit-Evasion for Car-like Robots with Sensor Constraints", "author": "Burak M. Gonultas and Volkan Isler", "abstract": "  We study a pursuit-evasion game between two players with car-like dynamics\nand sensing limitations by formalizing it as a partially observable stochastic\nzero-sum game. The partial observability caused by the sensing constraints is\nparticularly challenging. As an example, in a situation where the agents have\nno visibility of each other, they would need to extract information from their\nsensor coverage history to reason about potential locations of their opponents.\nHowever, keeping historical information greatly increases the size of the state\nspace. To mitigate the challenges encountered with such partially observable\nproblems, we develop a new learning-based method that encodes historical\ninformation to a belief state and uses it to generate agent actions. Through\nexperiments we show that the learned strategies improve over existing\nmulti-agent RL baselines by up to 16 % in terms of capture rate for the\npursuer. Additionally, we present experimental results showing that learned\nbelief states are strong state estimators for extending existing game theory\nsolvers and demonstrate our method's competitiveness for problems where\nexisting fully observable game theory solvers are computationally feasible.\nFinally, we deploy the learned policies on physical robots for a game between\nthe F1TENTH and JetRacer platforms moving as fast as $\\textbf{2 m/s}$ in indoor\nenvironments, showing that they can be executed on real-robots.\n", "link": "http://arxiv.org/abs/2405.05372v2", "date": "2025-06-16", "relevancy": 1.565, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5847}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5285}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pursuit-Evasion%20for%20Car-like%20Robots%20with%20Sensor%20Constraints&body=Title%3A%20Pursuit-Evasion%20for%20Car-like%20Robots%20with%20Sensor%20Constraints%0AAuthor%3A%20Burak%20M.%20Gonultas%20and%20Volkan%20Isler%0AAbstract%3A%20%20%20We%20study%20a%20pursuit-evasion%20game%20between%20two%20players%20with%20car-like%20dynamics%0Aand%20sensing%20limitations%20by%20formalizing%20it%20as%20a%20partially%20observable%20stochastic%0Azero-sum%20game.%20The%20partial%20observability%20caused%20by%20the%20sensing%20constraints%20is%0Aparticularly%20challenging.%20As%20an%20example%2C%20in%20a%20situation%20where%20the%20agents%20have%0Ano%20visibility%20of%20each%20other%2C%20they%20would%20need%20to%20extract%20information%20from%20their%0Asensor%20coverage%20history%20to%20reason%20about%20potential%20locations%20of%20their%20opponents.%0AHowever%2C%20keeping%20historical%20information%20greatly%20increases%20the%20size%20of%20the%20state%0Aspace.%20To%20mitigate%20the%20challenges%20encountered%20with%20such%20partially%20observable%0Aproblems%2C%20we%20develop%20a%20new%20learning-based%20method%20that%20encodes%20historical%0Ainformation%20to%20a%20belief%20state%20and%20uses%20it%20to%20generate%20agent%20actions.%20Through%0Aexperiments%20we%20show%20that%20the%20learned%20strategies%20improve%20over%20existing%0Amulti-agent%20RL%20baselines%20by%20up%20to%2016%20%25%20in%20terms%20of%20capture%20rate%20for%20the%0Apursuer.%20Additionally%2C%20we%20present%20experimental%20results%20showing%20that%20learned%0Abelief%20states%20are%20strong%20state%20estimators%20for%20extending%20existing%20game%20theory%0Asolvers%20and%20demonstrate%20our%20method%27s%20competitiveness%20for%20problems%20where%0Aexisting%20fully%20observable%20game%20theory%20solvers%20are%20computationally%20feasible.%0AFinally%2C%20we%20deploy%20the%20learned%20policies%20on%20physical%20robots%20for%20a%20game%20between%0Athe%20F1TENTH%20and%20JetRacer%20platforms%20moving%20as%20fast%20as%20%24%5Ctextbf%7B2%20m/s%7D%24%20in%20indoor%0Aenvironments%2C%20showing%20that%20they%20can%20be%20executed%20on%20real-robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05372v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPursuit-Evasion%2520for%2520Car-like%2520Robots%2520with%2520Sensor%2520Constraints%26entry.906535625%3DBurak%2520M.%2520Gonultas%2520and%2520Volkan%2520Isler%26entry.1292438233%3D%2520%2520We%2520study%2520a%2520pursuit-evasion%2520game%2520between%2520two%2520players%2520with%2520car-like%2520dynamics%250Aand%2520sensing%2520limitations%2520by%2520formalizing%2520it%2520as%2520a%2520partially%2520observable%2520stochastic%250Azero-sum%2520game.%2520The%2520partial%2520observability%2520caused%2520by%2520the%2520sensing%2520constraints%2520is%250Aparticularly%2520challenging.%2520As%2520an%2520example%252C%2520in%2520a%2520situation%2520where%2520the%2520agents%2520have%250Ano%2520visibility%2520of%2520each%2520other%252C%2520they%2520would%2520need%2520to%2520extract%2520information%2520from%2520their%250Asensor%2520coverage%2520history%2520to%2520reason%2520about%2520potential%2520locations%2520of%2520their%2520opponents.%250AHowever%252C%2520keeping%2520historical%2520information%2520greatly%2520increases%2520the%2520size%2520of%2520the%2520state%250Aspace.%2520To%2520mitigate%2520the%2520challenges%2520encountered%2520with%2520such%2520partially%2520observable%250Aproblems%252C%2520we%2520develop%2520a%2520new%2520learning-based%2520method%2520that%2520encodes%2520historical%250Ainformation%2520to%2520a%2520belief%2520state%2520and%2520uses%2520it%2520to%2520generate%2520agent%2520actions.%2520Through%250Aexperiments%2520we%2520show%2520that%2520the%2520learned%2520strategies%2520improve%2520over%2520existing%250Amulti-agent%2520RL%2520baselines%2520by%2520up%2520to%252016%2520%2525%2520in%2520terms%2520of%2520capture%2520rate%2520for%2520the%250Apursuer.%2520Additionally%252C%2520we%2520present%2520experimental%2520results%2520showing%2520that%2520learned%250Abelief%2520states%2520are%2520strong%2520state%2520estimators%2520for%2520extending%2520existing%2520game%2520theory%250Asolvers%2520and%2520demonstrate%2520our%2520method%2527s%2520competitiveness%2520for%2520problems%2520where%250Aexisting%2520fully%2520observable%2520game%2520theory%2520solvers%2520are%2520computationally%2520feasible.%250AFinally%252C%2520we%2520deploy%2520the%2520learned%2520policies%2520on%2520physical%2520robots%2520for%2520a%2520game%2520between%250Athe%2520F1TENTH%2520and%2520JetRacer%2520platforms%2520moving%2520as%2520fast%2520as%2520%2524%255Ctextbf%257B2%2520m/s%257D%2524%2520in%2520indoor%250Aenvironments%252C%2520showing%2520that%2520they%2520can%2520be%2520executed%2520on%2520real-robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05372v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pursuit-Evasion%20for%20Car-like%20Robots%20with%20Sensor%20Constraints&entry.906535625=Burak%20M.%20Gonultas%20and%20Volkan%20Isler&entry.1292438233=%20%20We%20study%20a%20pursuit-evasion%20game%20between%20two%20players%20with%20car-like%20dynamics%0Aand%20sensing%20limitations%20by%20formalizing%20it%20as%20a%20partially%20observable%20stochastic%0Azero-sum%20game.%20The%20partial%20observability%20caused%20by%20the%20sensing%20constraints%20is%0Aparticularly%20challenging.%20As%20an%20example%2C%20in%20a%20situation%20where%20the%20agents%20have%0Ano%20visibility%20of%20each%20other%2C%20they%20would%20need%20to%20extract%20information%20from%20their%0Asensor%20coverage%20history%20to%20reason%20about%20potential%20locations%20of%20their%20opponents.%0AHowever%2C%20keeping%20historical%20information%20greatly%20increases%20the%20size%20of%20the%20state%0Aspace.%20To%20mitigate%20the%20challenges%20encountered%20with%20such%20partially%20observable%0Aproblems%2C%20we%20develop%20a%20new%20learning-based%20method%20that%20encodes%20historical%0Ainformation%20to%20a%20belief%20state%20and%20uses%20it%20to%20generate%20agent%20actions.%20Through%0Aexperiments%20we%20show%20that%20the%20learned%20strategies%20improve%20over%20existing%0Amulti-agent%20RL%20baselines%20by%20up%20to%2016%20%25%20in%20terms%20of%20capture%20rate%20for%20the%0Apursuer.%20Additionally%2C%20we%20present%20experimental%20results%20showing%20that%20learned%0Abelief%20states%20are%20strong%20state%20estimators%20for%20extending%20existing%20game%20theory%0Asolvers%20and%20demonstrate%20our%20method%27s%20competitiveness%20for%20problems%20where%0Aexisting%20fully%20observable%20game%20theory%20solvers%20are%20computationally%20feasible.%0AFinally%2C%20we%20deploy%20the%20learned%20policies%20on%20physical%20robots%20for%20a%20game%20between%0Athe%20F1TENTH%20and%20JetRacer%20platforms%20moving%20as%20fast%20as%20%24%5Ctextbf%7B2%20m/s%7D%24%20in%20indoor%0Aenvironments%2C%20showing%20that%20they%20can%20be%20executed%20on%20real-robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05372v2&entry.124074799=Read"},
{"title": "Hybrid Meta-learners for Estimating Heterogeneous Treatment Effects", "author": "Zhongyuan Liang and Lars van der Laan and Ahmed Alaa", "abstract": "  Estimating conditional average treatment effects (CATE) from observational\ndata involves modeling decisions that differ from supervised learning,\nparticularly concerning how to regularize model complexity. Previous approaches\ncan be grouped into two primary \"meta-learner\" paradigms that impose distinct\ninductive biases. Indirect meta-learners first fit and regularize separate\npotential outcome (PO) models and then estimate CATE by taking their\ndifference, whereas direct meta-learners construct and directly regularize\nestimators for the CATE function itself. Neither approach consistently\noutperforms the other across all scenarios: indirect learners perform well when\nthe PO functions are simple, while direct learners outperform when the CATE is\nsimpler than individual PO functions. In this paper, we introduce the Hybrid\nLearner (H-learner), a novel regularization strategy that interpolates between\nthe direct and indirect regularizations depending on the dataset at hand. The\nH-learner achieves this by learning intermediate functions whose difference\nclosely approximates the CATE without necessarily requiring accurate individual\napproximations of the POs themselves. We demonstrate empirically that\nintentionally allowing suboptimal fits to the POs improves the bias-variance\ntradeoff in estimating CATE. Experiments conducted on semi-synthetic and\nreal-world benchmark datasets illustrate that the H-learner consistently\noperates at the Pareto frontier, effectively combining the strengths of both\ndirect and indirect meta-learners.\n", "link": "http://arxiv.org/abs/2506.13680v1", "date": "2025-06-16", "relevancy": 1.3729, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4635}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4582}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Meta-learners%20for%20Estimating%20Heterogeneous%20Treatment%20Effects&body=Title%3A%20Hybrid%20Meta-learners%20for%20Estimating%20Heterogeneous%20Treatment%20Effects%0AAuthor%3A%20Zhongyuan%20Liang%20and%20Lars%20van%20der%20Laan%20and%20Ahmed%20Alaa%0AAbstract%3A%20%20%20Estimating%20conditional%20average%20treatment%20effects%20%28CATE%29%20from%20observational%0Adata%20involves%20modeling%20decisions%20that%20differ%20from%20supervised%20learning%2C%0Aparticularly%20concerning%20how%20to%20regularize%20model%20complexity.%20Previous%20approaches%0Acan%20be%20grouped%20into%20two%20primary%20%22meta-learner%22%20paradigms%20that%20impose%20distinct%0Ainductive%20biases.%20Indirect%20meta-learners%20first%20fit%20and%20regularize%20separate%0Apotential%20outcome%20%28PO%29%20models%20and%20then%20estimate%20CATE%20by%20taking%20their%0Adifference%2C%20whereas%20direct%20meta-learners%20construct%20and%20directly%20regularize%0Aestimators%20for%20the%20CATE%20function%20itself.%20Neither%20approach%20consistently%0Aoutperforms%20the%20other%20across%20all%20scenarios%3A%20indirect%20learners%20perform%20well%20when%0Athe%20PO%20functions%20are%20simple%2C%20while%20direct%20learners%20outperform%20when%20the%20CATE%20is%0Asimpler%20than%20individual%20PO%20functions.%20In%20this%20paper%2C%20we%20introduce%20the%20Hybrid%0ALearner%20%28H-learner%29%2C%20a%20novel%20regularization%20strategy%20that%20interpolates%20between%0Athe%20direct%20and%20indirect%20regularizations%20depending%20on%20the%20dataset%20at%20hand.%20The%0AH-learner%20achieves%20this%20by%20learning%20intermediate%20functions%20whose%20difference%0Aclosely%20approximates%20the%20CATE%20without%20necessarily%20requiring%20accurate%20individual%0Aapproximations%20of%20the%20POs%20themselves.%20We%20demonstrate%20empirically%20that%0Aintentionally%20allowing%20suboptimal%20fits%20to%20the%20POs%20improves%20the%20bias-variance%0Atradeoff%20in%20estimating%20CATE.%20Experiments%20conducted%20on%20semi-synthetic%20and%0Areal-world%20benchmark%20datasets%20illustrate%20that%20the%20H-learner%20consistently%0Aoperates%20at%20the%20Pareto%20frontier%2C%20effectively%20combining%20the%20strengths%20of%20both%0Adirect%20and%20indirect%20meta-learners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Meta-learners%2520for%2520Estimating%2520Heterogeneous%2520Treatment%2520Effects%26entry.906535625%3DZhongyuan%2520Liang%2520and%2520Lars%2520van%2520der%2520Laan%2520and%2520Ahmed%2520Alaa%26entry.1292438233%3D%2520%2520Estimating%2520conditional%2520average%2520treatment%2520effects%2520%2528CATE%2529%2520from%2520observational%250Adata%2520involves%2520modeling%2520decisions%2520that%2520differ%2520from%2520supervised%2520learning%252C%250Aparticularly%2520concerning%2520how%2520to%2520regularize%2520model%2520complexity.%2520Previous%2520approaches%250Acan%2520be%2520grouped%2520into%2520two%2520primary%2520%2522meta-learner%2522%2520paradigms%2520that%2520impose%2520distinct%250Ainductive%2520biases.%2520Indirect%2520meta-learners%2520first%2520fit%2520and%2520regularize%2520separate%250Apotential%2520outcome%2520%2528PO%2529%2520models%2520and%2520then%2520estimate%2520CATE%2520by%2520taking%2520their%250Adifference%252C%2520whereas%2520direct%2520meta-learners%2520construct%2520and%2520directly%2520regularize%250Aestimators%2520for%2520the%2520CATE%2520function%2520itself.%2520Neither%2520approach%2520consistently%250Aoutperforms%2520the%2520other%2520across%2520all%2520scenarios%253A%2520indirect%2520learners%2520perform%2520well%2520when%250Athe%2520PO%2520functions%2520are%2520simple%252C%2520while%2520direct%2520learners%2520outperform%2520when%2520the%2520CATE%2520is%250Asimpler%2520than%2520individual%2520PO%2520functions.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Hybrid%250ALearner%2520%2528H-learner%2529%252C%2520a%2520novel%2520regularization%2520strategy%2520that%2520interpolates%2520between%250Athe%2520direct%2520and%2520indirect%2520regularizations%2520depending%2520on%2520the%2520dataset%2520at%2520hand.%2520The%250AH-learner%2520achieves%2520this%2520by%2520learning%2520intermediate%2520functions%2520whose%2520difference%250Aclosely%2520approximates%2520the%2520CATE%2520without%2520necessarily%2520requiring%2520accurate%2520individual%250Aapproximations%2520of%2520the%2520POs%2520themselves.%2520We%2520demonstrate%2520empirically%2520that%250Aintentionally%2520allowing%2520suboptimal%2520fits%2520to%2520the%2520POs%2520improves%2520the%2520bias-variance%250Atradeoff%2520in%2520estimating%2520CATE.%2520Experiments%2520conducted%2520on%2520semi-synthetic%2520and%250Areal-world%2520benchmark%2520datasets%2520illustrate%2520that%2520the%2520H-learner%2520consistently%250Aoperates%2520at%2520the%2520Pareto%2520frontier%252C%2520effectively%2520combining%2520the%2520strengths%2520of%2520both%250Adirect%2520and%2520indirect%2520meta-learners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Meta-learners%20for%20Estimating%20Heterogeneous%20Treatment%20Effects&entry.906535625=Zhongyuan%20Liang%20and%20Lars%20van%20der%20Laan%20and%20Ahmed%20Alaa&entry.1292438233=%20%20Estimating%20conditional%20average%20treatment%20effects%20%28CATE%29%20from%20observational%0Adata%20involves%20modeling%20decisions%20that%20differ%20from%20supervised%20learning%2C%0Aparticularly%20concerning%20how%20to%20regularize%20model%20complexity.%20Previous%20approaches%0Acan%20be%20grouped%20into%20two%20primary%20%22meta-learner%22%20paradigms%20that%20impose%20distinct%0Ainductive%20biases.%20Indirect%20meta-learners%20first%20fit%20and%20regularize%20separate%0Apotential%20outcome%20%28PO%29%20models%20and%20then%20estimate%20CATE%20by%20taking%20their%0Adifference%2C%20whereas%20direct%20meta-learners%20construct%20and%20directly%20regularize%0Aestimators%20for%20the%20CATE%20function%20itself.%20Neither%20approach%20consistently%0Aoutperforms%20the%20other%20across%20all%20scenarios%3A%20indirect%20learners%20perform%20well%20when%0Athe%20PO%20functions%20are%20simple%2C%20while%20direct%20learners%20outperform%20when%20the%20CATE%20is%0Asimpler%20than%20individual%20PO%20functions.%20In%20this%20paper%2C%20we%20introduce%20the%20Hybrid%0ALearner%20%28H-learner%29%2C%20a%20novel%20regularization%20strategy%20that%20interpolates%20between%0Athe%20direct%20and%20indirect%20regularizations%20depending%20on%20the%20dataset%20at%20hand.%20The%0AH-learner%20achieves%20this%20by%20learning%20intermediate%20functions%20whose%20difference%0Aclosely%20approximates%20the%20CATE%20without%20necessarily%20requiring%20accurate%20individual%0Aapproximations%20of%20the%20POs%20themselves.%20We%20demonstrate%20empirically%20that%0Aintentionally%20allowing%20suboptimal%20fits%20to%20the%20POs%20improves%20the%20bias-variance%0Atradeoff%20in%20estimating%20CATE.%20Experiments%20conducted%20on%20semi-synthetic%20and%0Areal-world%20benchmark%20datasets%20illustrate%20that%20the%20H-learner%20consistently%0Aoperates%20at%20the%20Pareto%20frontier%2C%20effectively%20combining%20the%20strengths%20of%20both%0Adirect%20and%20indirect%20meta-learners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13680v1&entry.124074799=Read"},
{"title": "Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact\n  Verifiers", "author": "Wooseok Seo and Seungju Han and Jaehun Jung and Benjamin Newman and Seungwon Lim and Seungbeen Lee and Ximing Lu and Yejin Choi and Youngjae Yu", "abstract": "  Fact verification is essential for ensuring the reliability of LLM\napplications. In this study, we evaluate 12 pre-trained LLMs and one\nspecialized fact-verifier, including frontier LLMs and open-weight reasoning\nLLMs, using a collection of examples from 14 fact-checking benchmarks. We share\nthree findings intended to guide future development of more robust fact\nverifiers. First, we highlight the importance of addressing annotation errors\nand ambiguity in datasets, demonstrating that approximately 16\\% of ambiguous\nor incorrectly labeled data substantially influences model rankings. Neglecting\nthis issue may result in misleading conclusions during comparative evaluations,\nand we suggest using a systematic pipeline utilizing LLM-as-a-judge to help\nidentify these issues at scale. Second, we discover that frontier LLMs with\nfew-shot in-context examples, often overlooked in previous works, achieve\ntop-tier performance. We therefore recommend future studies include comparisons\nwith these simple yet highly effective baselines. Lastly, despite their\neffectiveness, frontier LLMs incur substantial costs, motivating the\ndevelopment of small, fine-tuned fact verifiers. We show that these small\nmodels still have room for improvement, particularly on instances that require\ncomplex reasoning. Encouragingly, we demonstrate that augmenting training with\nsynthetic multi-hop reasoning data significantly enhances their capabilities in\nsuch instances. We release our code, model, and dataset at\nhttps://github.com/just1nseo/verifying-the-verifiers\n", "link": "http://arxiv.org/abs/2506.13342v1", "date": "2025-06-16", "relevancy": 2.0053, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5029}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.501}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Verifying%20the%20Verifiers%3A%20Unveiling%20Pitfalls%20and%20Potentials%20in%20Fact%0A%20%20Verifiers&body=Title%3A%20Verifying%20the%20Verifiers%3A%20Unveiling%20Pitfalls%20and%20Potentials%20in%20Fact%0A%20%20Verifiers%0AAuthor%3A%20Wooseok%20Seo%20and%20Seungju%20Han%20and%20Jaehun%20Jung%20and%20Benjamin%20Newman%20and%20Seungwon%20Lim%20and%20Seungbeen%20Lee%20and%20Ximing%20Lu%20and%20Yejin%20Choi%20and%20Youngjae%20Yu%0AAbstract%3A%20%20%20Fact%20verification%20is%20essential%20for%20ensuring%20the%20reliability%20of%20LLM%0Aapplications.%20In%20this%20study%2C%20we%20evaluate%2012%20pre-trained%20LLMs%20and%20one%0Aspecialized%20fact-verifier%2C%20including%20frontier%20LLMs%20and%20open-weight%20reasoning%0ALLMs%2C%20using%20a%20collection%20of%20examples%20from%2014%20fact-checking%20benchmarks.%20We%20share%0Athree%20findings%20intended%20to%20guide%20future%20development%20of%20more%20robust%20fact%0Averifiers.%20First%2C%20we%20highlight%20the%20importance%20of%20addressing%20annotation%20errors%0Aand%20ambiguity%20in%20datasets%2C%20demonstrating%20that%20approximately%2016%5C%25%20of%20ambiguous%0Aor%20incorrectly%20labeled%20data%20substantially%20influences%20model%20rankings.%20Neglecting%0Athis%20issue%20may%20result%20in%20misleading%20conclusions%20during%20comparative%20evaluations%2C%0Aand%20we%20suggest%20using%20a%20systematic%20pipeline%20utilizing%20LLM-as-a-judge%20to%20help%0Aidentify%20these%20issues%20at%20scale.%20Second%2C%20we%20discover%20that%20frontier%20LLMs%20with%0Afew-shot%20in-context%20examples%2C%20often%20overlooked%20in%20previous%20works%2C%20achieve%0Atop-tier%20performance.%20We%20therefore%20recommend%20future%20studies%20include%20comparisons%0Awith%20these%20simple%20yet%20highly%20effective%20baselines.%20Lastly%2C%20despite%20their%0Aeffectiveness%2C%20frontier%20LLMs%20incur%20substantial%20costs%2C%20motivating%20the%0Adevelopment%20of%20small%2C%20fine-tuned%20fact%20verifiers.%20We%20show%20that%20these%20small%0Amodels%20still%20have%20room%20for%20improvement%2C%20particularly%20on%20instances%20that%20require%0Acomplex%20reasoning.%20Encouragingly%2C%20we%20demonstrate%20that%20augmenting%20training%20with%0Asynthetic%20multi-hop%20reasoning%20data%20significantly%20enhances%20their%20capabilities%20in%0Asuch%20instances.%20We%20release%20our%20code%2C%20model%2C%20and%20dataset%20at%0Ahttps%3A//github.com/just1nseo/verifying-the-verifiers%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13342v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVerifying%2520the%2520Verifiers%253A%2520Unveiling%2520Pitfalls%2520and%2520Potentials%2520in%2520Fact%250A%2520%2520Verifiers%26entry.906535625%3DWooseok%2520Seo%2520and%2520Seungju%2520Han%2520and%2520Jaehun%2520Jung%2520and%2520Benjamin%2520Newman%2520and%2520Seungwon%2520Lim%2520and%2520Seungbeen%2520Lee%2520and%2520Ximing%2520Lu%2520and%2520Yejin%2520Choi%2520and%2520Youngjae%2520Yu%26entry.1292438233%3D%2520%2520Fact%2520verification%2520is%2520essential%2520for%2520ensuring%2520the%2520reliability%2520of%2520LLM%250Aapplications.%2520In%2520this%2520study%252C%2520we%2520evaluate%252012%2520pre-trained%2520LLMs%2520and%2520one%250Aspecialized%2520fact-verifier%252C%2520including%2520frontier%2520LLMs%2520and%2520open-weight%2520reasoning%250ALLMs%252C%2520using%2520a%2520collection%2520of%2520examples%2520from%252014%2520fact-checking%2520benchmarks.%2520We%2520share%250Athree%2520findings%2520intended%2520to%2520guide%2520future%2520development%2520of%2520more%2520robust%2520fact%250Averifiers.%2520First%252C%2520we%2520highlight%2520the%2520importance%2520of%2520addressing%2520annotation%2520errors%250Aand%2520ambiguity%2520in%2520datasets%252C%2520demonstrating%2520that%2520approximately%252016%255C%2525%2520of%2520ambiguous%250Aor%2520incorrectly%2520labeled%2520data%2520substantially%2520influences%2520model%2520rankings.%2520Neglecting%250Athis%2520issue%2520may%2520result%2520in%2520misleading%2520conclusions%2520during%2520comparative%2520evaluations%252C%250Aand%2520we%2520suggest%2520using%2520a%2520systematic%2520pipeline%2520utilizing%2520LLM-as-a-judge%2520to%2520help%250Aidentify%2520these%2520issues%2520at%2520scale.%2520Second%252C%2520we%2520discover%2520that%2520frontier%2520LLMs%2520with%250Afew-shot%2520in-context%2520examples%252C%2520often%2520overlooked%2520in%2520previous%2520works%252C%2520achieve%250Atop-tier%2520performance.%2520We%2520therefore%2520recommend%2520future%2520studies%2520include%2520comparisons%250Awith%2520these%2520simple%2520yet%2520highly%2520effective%2520baselines.%2520Lastly%252C%2520despite%2520their%250Aeffectiveness%252C%2520frontier%2520LLMs%2520incur%2520substantial%2520costs%252C%2520motivating%2520the%250Adevelopment%2520of%2520small%252C%2520fine-tuned%2520fact%2520verifiers.%2520We%2520show%2520that%2520these%2520small%250Amodels%2520still%2520have%2520room%2520for%2520improvement%252C%2520particularly%2520on%2520instances%2520that%2520require%250Acomplex%2520reasoning.%2520Encouragingly%252C%2520we%2520demonstrate%2520that%2520augmenting%2520training%2520with%250Asynthetic%2520multi-hop%2520reasoning%2520data%2520significantly%2520enhances%2520their%2520capabilities%2520in%250Asuch%2520instances.%2520We%2520release%2520our%2520code%252C%2520model%252C%2520and%2520dataset%2520at%250Ahttps%253A//github.com/just1nseo/verifying-the-verifiers%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13342v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Verifying%20the%20Verifiers%3A%20Unveiling%20Pitfalls%20and%20Potentials%20in%20Fact%0A%20%20Verifiers&entry.906535625=Wooseok%20Seo%20and%20Seungju%20Han%20and%20Jaehun%20Jung%20and%20Benjamin%20Newman%20and%20Seungwon%20Lim%20and%20Seungbeen%20Lee%20and%20Ximing%20Lu%20and%20Yejin%20Choi%20and%20Youngjae%20Yu&entry.1292438233=%20%20Fact%20verification%20is%20essential%20for%20ensuring%20the%20reliability%20of%20LLM%0Aapplications.%20In%20this%20study%2C%20we%20evaluate%2012%20pre-trained%20LLMs%20and%20one%0Aspecialized%20fact-verifier%2C%20including%20frontier%20LLMs%20and%20open-weight%20reasoning%0ALLMs%2C%20using%20a%20collection%20of%20examples%20from%2014%20fact-checking%20benchmarks.%20We%20share%0Athree%20findings%20intended%20to%20guide%20future%20development%20of%20more%20robust%20fact%0Averifiers.%20First%2C%20we%20highlight%20the%20importance%20of%20addressing%20annotation%20errors%0Aand%20ambiguity%20in%20datasets%2C%20demonstrating%20that%20approximately%2016%5C%25%20of%20ambiguous%0Aor%20incorrectly%20labeled%20data%20substantially%20influences%20model%20rankings.%20Neglecting%0Athis%20issue%20may%20result%20in%20misleading%20conclusions%20during%20comparative%20evaluations%2C%0Aand%20we%20suggest%20using%20a%20systematic%20pipeline%20utilizing%20LLM-as-a-judge%20to%20help%0Aidentify%20these%20issues%20at%20scale.%20Second%2C%20we%20discover%20that%20frontier%20LLMs%20with%0Afew-shot%20in-context%20examples%2C%20often%20overlooked%20in%20previous%20works%2C%20achieve%0Atop-tier%20performance.%20We%20therefore%20recommend%20future%20studies%20include%20comparisons%0Awith%20these%20simple%20yet%20highly%20effective%20baselines.%20Lastly%2C%20despite%20their%0Aeffectiveness%2C%20frontier%20LLMs%20incur%20substantial%20costs%2C%20motivating%20the%0Adevelopment%20of%20small%2C%20fine-tuned%20fact%20verifiers.%20We%20show%20that%20these%20small%0Amodels%20still%20have%20room%20for%20improvement%2C%20particularly%20on%20instances%20that%20require%0Acomplex%20reasoning.%20Encouragingly%2C%20we%20demonstrate%20that%20augmenting%20training%20with%0Asynthetic%20multi-hop%20reasoning%20data%20significantly%20enhances%20their%20capabilities%20in%0Asuch%20instances.%20We%20release%20our%20code%2C%20model%2C%20and%20dataset%20at%0Ahttps%3A//github.com/just1nseo/verifying-the-verifiers%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13342v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


