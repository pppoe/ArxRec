<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250610.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular\n  Videos", "author": "Chieh Hubert Lin and Zhaoyang Lv and Songyin Wu and Zhen Xu and Thu Nguyen-Phuoc and Hung-Yu Tseng and Julian Straub and Numair Khan and Lei Xiao and Ming-Hsuan Yang and Yuheng Ren and Richard Newcombe and Zhao Dong and Zhengqin Li", "abstract": "  We introduce the Deformable Gaussian Splats Large Reconstruction Model\n(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian\nsplats from a monocular posed video of any dynamic scene. Feed-forward scene\nreconstruction has gained significant attention for its ability to rapidly\ncreate digital replicas of real-world environments. However, most existing\nmodels are limited to static scenes and fail to reconstruct the motion of\nmoving objects. Developing a feed-forward model for dynamic scene\nreconstruction poses significant challenges, including the scarcity of training\ndata and the need for appropriate 3D representations and training paradigms. To\naddress these challenges, we introduce several key technical contributions: an\nenhanced large-scale synthetic dataset with ground-truth multi-view videos and\ndense 3D scene flow supervision; a per-pixel deformable 3D Gaussian\nrepresentation that is easy to learn, supports high-quality dynamic view\nsynthesis, and enables long-range 3D tracking; and a large transformer network\nthat achieves real-time, generalizable dynamic scene reconstruction. Extensive\nqualitative and quantitative experiments demonstrate that DGS-LRM achieves\ndynamic scene reconstruction quality comparable to optimization-based methods,\nwhile significantly outperforming the state-of-the-art predictive dynamic\nreconstruction method on real-world examples. Its predicted physically grounded\n3D deformation is accurate and can readily adapt for long-range 3D tracking\ntasks, achieving performance on par with state-of-the-art monocular video 3D\ntracking methods.\n", "link": "http://arxiv.org/abs/2506.09997v1", "date": "2025-06-11", "relevancy": 3.6346, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7487}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7439}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DGS-LRM%3A%20Real-Time%20Deformable%203D%20Gaussian%20Reconstruction%20From%20Monocular%0A%20%20Videos&body=Title%3A%20DGS-LRM%3A%20Real-Time%20Deformable%203D%20Gaussian%20Reconstruction%20From%20Monocular%0A%20%20Videos%0AAuthor%3A%20Chieh%20Hubert%20Lin%20and%20Zhaoyang%20Lv%20and%20Songyin%20Wu%20and%20Zhen%20Xu%20and%20Thu%20Nguyen-Phuoc%20and%20Hung-Yu%20Tseng%20and%20Julian%20Straub%20and%20Numair%20Khan%20and%20Lei%20Xiao%20and%20Ming-Hsuan%20Yang%20and%20Yuheng%20Ren%20and%20Richard%20Newcombe%20and%20Zhao%20Dong%20and%20Zhengqin%20Li%0AAbstract%3A%20%20%20We%20introduce%20the%20Deformable%20Gaussian%20Splats%20Large%20Reconstruction%20Model%0A%28DGS-LRM%29%2C%20the%20first%20feed-forward%20method%20predicting%20deformable%203D%20Gaussian%0Asplats%20from%20a%20monocular%20posed%20video%20of%20any%20dynamic%20scene.%20Feed-forward%20scene%0Areconstruction%20has%20gained%20significant%20attention%20for%20its%20ability%20to%20rapidly%0Acreate%20digital%20replicas%20of%20real-world%20environments.%20However%2C%20most%20existing%0Amodels%20are%20limited%20to%20static%20scenes%20and%20fail%20to%20reconstruct%20the%20motion%20of%0Amoving%20objects.%20Developing%20a%20feed-forward%20model%20for%20dynamic%20scene%0Areconstruction%20poses%20significant%20challenges%2C%20including%20the%20scarcity%20of%20training%0Adata%20and%20the%20need%20for%20appropriate%203D%20representations%20and%20training%20paradigms.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20several%20key%20technical%20contributions%3A%20an%0Aenhanced%20large-scale%20synthetic%20dataset%20with%20ground-truth%20multi-view%20videos%20and%0Adense%203D%20scene%20flow%20supervision%3B%20a%20per-pixel%20deformable%203D%20Gaussian%0Arepresentation%20that%20is%20easy%20to%20learn%2C%20supports%20high-quality%20dynamic%20view%0Asynthesis%2C%20and%20enables%20long-range%203D%20tracking%3B%20and%20a%20large%20transformer%20network%0Athat%20achieves%20real-time%2C%20generalizable%20dynamic%20scene%20reconstruction.%20Extensive%0Aqualitative%20and%20quantitative%20experiments%20demonstrate%20that%20DGS-LRM%20achieves%0Adynamic%20scene%20reconstruction%20quality%20comparable%20to%20optimization-based%20methods%2C%0Awhile%20significantly%20outperforming%20the%20state-of-the-art%20predictive%20dynamic%0Areconstruction%20method%20on%20real-world%20examples.%20Its%20predicted%20physically%20grounded%0A3D%20deformation%20is%20accurate%20and%20can%20readily%20adapt%20for%20long-range%203D%20tracking%0Atasks%2C%20achieving%20performance%20on%20par%20with%20state-of-the-art%20monocular%20video%203D%0Atracking%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDGS-LRM%253A%2520Real-Time%2520Deformable%25203D%2520Gaussian%2520Reconstruction%2520From%2520Monocular%250A%2520%2520Videos%26entry.906535625%3DChieh%2520Hubert%2520Lin%2520and%2520Zhaoyang%2520Lv%2520and%2520Songyin%2520Wu%2520and%2520Zhen%2520Xu%2520and%2520Thu%2520Nguyen-Phuoc%2520and%2520Hung-Yu%2520Tseng%2520and%2520Julian%2520Straub%2520and%2520Numair%2520Khan%2520and%2520Lei%2520Xiao%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Yuheng%2520Ren%2520and%2520Richard%2520Newcombe%2520and%2520Zhao%2520Dong%2520and%2520Zhengqin%2520Li%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Deformable%2520Gaussian%2520Splats%2520Large%2520Reconstruction%2520Model%250A%2528DGS-LRM%2529%252C%2520the%2520first%2520feed-forward%2520method%2520predicting%2520deformable%25203D%2520Gaussian%250Asplats%2520from%2520a%2520monocular%2520posed%2520video%2520of%2520any%2520dynamic%2520scene.%2520Feed-forward%2520scene%250Areconstruction%2520has%2520gained%2520significant%2520attention%2520for%2520its%2520ability%2520to%2520rapidly%250Acreate%2520digital%2520replicas%2520of%2520real-world%2520environments.%2520However%252C%2520most%2520existing%250Amodels%2520are%2520limited%2520to%2520static%2520scenes%2520and%2520fail%2520to%2520reconstruct%2520the%2520motion%2520of%250Amoving%2520objects.%2520Developing%2520a%2520feed-forward%2520model%2520for%2520dynamic%2520scene%250Areconstruction%2520poses%2520significant%2520challenges%252C%2520including%2520the%2520scarcity%2520of%2520training%250Adata%2520and%2520the%2520need%2520for%2520appropriate%25203D%2520representations%2520and%2520training%2520paradigms.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520several%2520key%2520technical%2520contributions%253A%2520an%250Aenhanced%2520large-scale%2520synthetic%2520dataset%2520with%2520ground-truth%2520multi-view%2520videos%2520and%250Adense%25203D%2520scene%2520flow%2520supervision%253B%2520a%2520per-pixel%2520deformable%25203D%2520Gaussian%250Arepresentation%2520that%2520is%2520easy%2520to%2520learn%252C%2520supports%2520high-quality%2520dynamic%2520view%250Asynthesis%252C%2520and%2520enables%2520long-range%25203D%2520tracking%253B%2520and%2520a%2520large%2520transformer%2520network%250Athat%2520achieves%2520real-time%252C%2520generalizable%2520dynamic%2520scene%2520reconstruction.%2520Extensive%250Aqualitative%2520and%2520quantitative%2520experiments%2520demonstrate%2520that%2520DGS-LRM%2520achieves%250Adynamic%2520scene%2520reconstruction%2520quality%2520comparable%2520to%2520optimization-based%2520methods%252C%250Awhile%2520significantly%2520outperforming%2520the%2520state-of-the-art%2520predictive%2520dynamic%250Areconstruction%2520method%2520on%2520real-world%2520examples.%2520Its%2520predicted%2520physically%2520grounded%250A3D%2520deformation%2520is%2520accurate%2520and%2520can%2520readily%2520adapt%2520for%2520long-range%25203D%2520tracking%250Atasks%252C%2520achieving%2520performance%2520on%2520par%2520with%2520state-of-the-art%2520monocular%2520video%25203D%250Atracking%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DGS-LRM%3A%20Real-Time%20Deformable%203D%20Gaussian%20Reconstruction%20From%20Monocular%0A%20%20Videos&entry.906535625=Chieh%20Hubert%20Lin%20and%20Zhaoyang%20Lv%20and%20Songyin%20Wu%20and%20Zhen%20Xu%20and%20Thu%20Nguyen-Phuoc%20and%20Hung-Yu%20Tseng%20and%20Julian%20Straub%20and%20Numair%20Khan%20and%20Lei%20Xiao%20and%20Ming-Hsuan%20Yang%20and%20Yuheng%20Ren%20and%20Richard%20Newcombe%20and%20Zhao%20Dong%20and%20Zhengqin%20Li&entry.1292438233=%20%20We%20introduce%20the%20Deformable%20Gaussian%20Splats%20Large%20Reconstruction%20Model%0A%28DGS-LRM%29%2C%20the%20first%20feed-forward%20method%20predicting%20deformable%203D%20Gaussian%0Asplats%20from%20a%20monocular%20posed%20video%20of%20any%20dynamic%20scene.%20Feed-forward%20scene%0Areconstruction%20has%20gained%20significant%20attention%20for%20its%20ability%20to%20rapidly%0Acreate%20digital%20replicas%20of%20real-world%20environments.%20However%2C%20most%20existing%0Amodels%20are%20limited%20to%20static%20scenes%20and%20fail%20to%20reconstruct%20the%20motion%20of%0Amoving%20objects.%20Developing%20a%20feed-forward%20model%20for%20dynamic%20scene%0Areconstruction%20poses%20significant%20challenges%2C%20including%20the%20scarcity%20of%20training%0Adata%20and%20the%20need%20for%20appropriate%203D%20representations%20and%20training%20paradigms.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20several%20key%20technical%20contributions%3A%20an%0Aenhanced%20large-scale%20synthetic%20dataset%20with%20ground-truth%20multi-view%20videos%20and%0Adense%203D%20scene%20flow%20supervision%3B%20a%20per-pixel%20deformable%203D%20Gaussian%0Arepresentation%20that%20is%20easy%20to%20learn%2C%20supports%20high-quality%20dynamic%20view%0Asynthesis%2C%20and%20enables%20long-range%203D%20tracking%3B%20and%20a%20large%20transformer%20network%0Athat%20achieves%20real-time%2C%20generalizable%20dynamic%20scene%20reconstruction.%20Extensive%0Aqualitative%20and%20quantitative%20experiments%20demonstrate%20that%20DGS-LRM%20achieves%0Adynamic%20scene%20reconstruction%20quality%20comparable%20to%20optimization-based%20methods%2C%0Awhile%20significantly%20outperforming%20the%20state-of-the-art%20predictive%20dynamic%0Areconstruction%20method%20on%20real-world%20examples.%20Its%20predicted%20physically%20grounded%0A3D%20deformation%20is%20accurate%20and%20can%20readily%20adapt%20for%20long-range%203D%20tracking%0Atasks%2C%20achieving%20performance%20on%20par%20with%20state-of-the-art%20monocular%20video%203D%0Atracking%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09997v1&entry.124074799=Read"},
{"title": "DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion\n  Decomposition for Scene Reconstruction", "author": "Junli Deng and Ping Shi and Qipei Li and Jinyang Guo", "abstract": "  Reconstructing intricate, ever-changing environments remains a central\nambition in computer vision, yet existing solutions often crumble before the\ncomplexity of real-world dynamics. We present DynaSplat, an approach that\nextends Gaussian Splatting to dynamic scenes by integrating dynamic-static\nseparation and hierarchical motion modeling. First, we classify scene elements\nas static or dynamic through a novel fusion of deformation offset statistics\nand 2D motion flow consistency, refining our spatial representation to focus\nprecisely where motion matters. We then introduce a hierarchical motion\nmodeling strategy that captures both coarse global transformations and\nfine-grained local movements, enabling accurate handling of intricate,\nnon-rigid motions. Finally, we integrate physically-based opacity estimation to\nensure visually coherent reconstructions, even under challenging occlusions and\nperspective shifts. Extensive experiments on challenging datasets reveal that\nDynaSplat not only surpasses state-of-the-art alternatives in accuracy and\nrealism but also provides a more intuitive, compact, and efficient route to\ndynamic scene reconstruction.\n", "link": "http://arxiv.org/abs/2506.09836v1", "date": "2025-06-11", "relevancy": 3.3639, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6904}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6747}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynaSplat%3A%20Dynamic-Static%20Gaussian%20Splatting%20with%20Hierarchical%20Motion%0A%20%20Decomposition%20for%20Scene%20Reconstruction&body=Title%3A%20DynaSplat%3A%20Dynamic-Static%20Gaussian%20Splatting%20with%20Hierarchical%20Motion%0A%20%20Decomposition%20for%20Scene%20Reconstruction%0AAuthor%3A%20Junli%20Deng%20and%20Ping%20Shi%20and%20Qipei%20Li%20and%20Jinyang%20Guo%0AAbstract%3A%20%20%20Reconstructing%20intricate%2C%20ever-changing%20environments%20remains%20a%20central%0Aambition%20in%20computer%20vision%2C%20yet%20existing%20solutions%20often%20crumble%20before%20the%0Acomplexity%20of%20real-world%20dynamics.%20We%20present%20DynaSplat%2C%20an%20approach%20that%0Aextends%20Gaussian%20Splatting%20to%20dynamic%20scenes%20by%20integrating%20dynamic-static%0Aseparation%20and%20hierarchical%20motion%20modeling.%20First%2C%20we%20classify%20scene%20elements%0Aas%20static%20or%20dynamic%20through%20a%20novel%20fusion%20of%20deformation%20offset%20statistics%0Aand%202D%20motion%20flow%20consistency%2C%20refining%20our%20spatial%20representation%20to%20focus%0Aprecisely%20where%20motion%20matters.%20We%20then%20introduce%20a%20hierarchical%20motion%0Amodeling%20strategy%20that%20captures%20both%20coarse%20global%20transformations%20and%0Afine-grained%20local%20movements%2C%20enabling%20accurate%20handling%20of%20intricate%2C%0Anon-rigid%20motions.%20Finally%2C%20we%20integrate%20physically-based%20opacity%20estimation%20to%0Aensure%20visually%20coherent%20reconstructions%2C%20even%20under%20challenging%20occlusions%20and%0Aperspective%20shifts.%20Extensive%20experiments%20on%20challenging%20datasets%20reveal%20that%0ADynaSplat%20not%20only%20surpasses%20state-of-the-art%20alternatives%20in%20accuracy%20and%0Arealism%20but%20also%20provides%20a%20more%20intuitive%2C%20compact%2C%20and%20efficient%20route%20to%0Adynamic%20scene%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynaSplat%253A%2520Dynamic-Static%2520Gaussian%2520Splatting%2520with%2520Hierarchical%2520Motion%250A%2520%2520Decomposition%2520for%2520Scene%2520Reconstruction%26entry.906535625%3DJunli%2520Deng%2520and%2520Ping%2520Shi%2520and%2520Qipei%2520Li%2520and%2520Jinyang%2520Guo%26entry.1292438233%3D%2520%2520Reconstructing%2520intricate%252C%2520ever-changing%2520environments%2520remains%2520a%2520central%250Aambition%2520in%2520computer%2520vision%252C%2520yet%2520existing%2520solutions%2520often%2520crumble%2520before%2520the%250Acomplexity%2520of%2520real-world%2520dynamics.%2520We%2520present%2520DynaSplat%252C%2520an%2520approach%2520that%250Aextends%2520Gaussian%2520Splatting%2520to%2520dynamic%2520scenes%2520by%2520integrating%2520dynamic-static%250Aseparation%2520and%2520hierarchical%2520motion%2520modeling.%2520First%252C%2520we%2520classify%2520scene%2520elements%250Aas%2520static%2520or%2520dynamic%2520through%2520a%2520novel%2520fusion%2520of%2520deformation%2520offset%2520statistics%250Aand%25202D%2520motion%2520flow%2520consistency%252C%2520refining%2520our%2520spatial%2520representation%2520to%2520focus%250Aprecisely%2520where%2520motion%2520matters.%2520We%2520then%2520introduce%2520a%2520hierarchical%2520motion%250Amodeling%2520strategy%2520that%2520captures%2520both%2520coarse%2520global%2520transformations%2520and%250Afine-grained%2520local%2520movements%252C%2520enabling%2520accurate%2520handling%2520of%2520intricate%252C%250Anon-rigid%2520motions.%2520Finally%252C%2520we%2520integrate%2520physically-based%2520opacity%2520estimation%2520to%250Aensure%2520visually%2520coherent%2520reconstructions%252C%2520even%2520under%2520challenging%2520occlusions%2520and%250Aperspective%2520shifts.%2520Extensive%2520experiments%2520on%2520challenging%2520datasets%2520reveal%2520that%250ADynaSplat%2520not%2520only%2520surpasses%2520state-of-the-art%2520alternatives%2520in%2520accuracy%2520and%250Arealism%2520but%2520also%2520provides%2520a%2520more%2520intuitive%252C%2520compact%252C%2520and%2520efficient%2520route%2520to%250Adynamic%2520scene%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynaSplat%3A%20Dynamic-Static%20Gaussian%20Splatting%20with%20Hierarchical%20Motion%0A%20%20Decomposition%20for%20Scene%20Reconstruction&entry.906535625=Junli%20Deng%20and%20Ping%20Shi%20and%20Qipei%20Li%20and%20Jinyang%20Guo&entry.1292438233=%20%20Reconstructing%20intricate%2C%20ever-changing%20environments%20remains%20a%20central%0Aambition%20in%20computer%20vision%2C%20yet%20existing%20solutions%20often%20crumble%20before%20the%0Acomplexity%20of%20real-world%20dynamics.%20We%20present%20DynaSplat%2C%20an%20approach%20that%0Aextends%20Gaussian%20Splatting%20to%20dynamic%20scenes%20by%20integrating%20dynamic-static%0Aseparation%20and%20hierarchical%20motion%20modeling.%20First%2C%20we%20classify%20scene%20elements%0Aas%20static%20or%20dynamic%20through%20a%20novel%20fusion%20of%20deformation%20offset%20statistics%0Aand%202D%20motion%20flow%20consistency%2C%20refining%20our%20spatial%20representation%20to%20focus%0Aprecisely%20where%20motion%20matters.%20We%20then%20introduce%20a%20hierarchical%20motion%0Amodeling%20strategy%20that%20captures%20both%20coarse%20global%20transformations%20and%0Afine-grained%20local%20movements%2C%20enabling%20accurate%20handling%20of%20intricate%2C%0Anon-rigid%20motions.%20Finally%2C%20we%20integrate%20physically-based%20opacity%20estimation%20to%0Aensure%20visually%20coherent%20reconstructions%2C%20even%20under%20challenging%20occlusions%20and%0Aperspective%20shifts.%20Extensive%20experiments%20on%20challenging%20datasets%20reveal%20that%0ADynaSplat%20not%20only%20surpasses%20state-of-the-art%20alternatives%20in%20accuracy%20and%0Arealism%20but%20also%20provides%20a%20more%20intuitive%2C%20compact%2C%20and%20efficient%20route%20to%0Adynamic%20scene%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09836v1&entry.124074799=Read"},
{"title": "LEO-VL: Towards 3D Vision-Language Generalists via Data Scaling with\n  Efficient Representation", "author": "Jiangyong Huang and Xiaojian Ma and Xiongkun Linghu and Yue Fan and Junchao He and Wenxin Tan and Qing Li and Song-Chun Zhu and Yixin Chen and Baoxiong Jia and Siyuan Huang", "abstract": "  Developing 3D-VL generalists capable of understanding 3D scenes and following\nnatural language instructions to perform a wide range of tasks has been a\nlong-standing goal in the 3D-VL community. Despite recent progress, 3D-VL\nmodels still lag behind their 2D counterparts in capability and robustness,\nfalling short of the generalist standard. A key obstacle to developing 3D-VL\ngeneralists lies in data scalability, hindered by the lack of an efficient\nscene representation. We propose LEO-VL, a 3D-VL model built upon condensed\nfeature grid (CFG), an efficient scene representation that bridges 2D\nperception and 3D spatial structure while significantly reducing token\noverhead. This efficiency unlocks large-scale training towards 3D-VL\ngeneralist, for which we curate over 700k high-quality 3D-VL data spanning four\ndomains of real-world indoor scenes and five tasks such as captioning and\ndialogue. LEO-VL achieves state-of-the-art performance on a variety of 3D QA\nbenchmarks, including SQA3D, MSQA, and Beacon3D. Ablation studies confirm the\nefficiency of our representation, the importance of task and scene diversity,\nand the validity of our data curation principle. Furthermore, we introduce\nSceneDPO, a novel post-training objective that enhances the robustness of 3D-VL\nmodels. We hope our findings contribute to the advancement of scalable and\nrobust 3D-VL generalists.\n", "link": "http://arxiv.org/abs/2506.09935v1", "date": "2025-06-11", "relevancy": 3.3279, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.7045}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.7045}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEO-VL%3A%20Towards%203D%20Vision-Language%20Generalists%20via%20Data%20Scaling%20with%0A%20%20Efficient%20Representation&body=Title%3A%20LEO-VL%3A%20Towards%203D%20Vision-Language%20Generalists%20via%20Data%20Scaling%20with%0A%20%20Efficient%20Representation%0AAuthor%3A%20Jiangyong%20Huang%20and%20Xiaojian%20Ma%20and%20Xiongkun%20Linghu%20and%20Yue%20Fan%20and%20Junchao%20He%20and%20Wenxin%20Tan%20and%20Qing%20Li%20and%20Song-Chun%20Zhu%20and%20Yixin%20Chen%20and%20Baoxiong%20Jia%20and%20Siyuan%20Huang%0AAbstract%3A%20%20%20Developing%203D-VL%20generalists%20capable%20of%20understanding%203D%20scenes%20and%20following%0Anatural%20language%20instructions%20to%20perform%20a%20wide%20range%20of%20tasks%20has%20been%20a%0Along-standing%20goal%20in%20the%203D-VL%20community.%20Despite%20recent%20progress%2C%203D-VL%0Amodels%20still%20lag%20behind%20their%202D%20counterparts%20in%20capability%20and%20robustness%2C%0Afalling%20short%20of%20the%20generalist%20standard.%20A%20key%20obstacle%20to%20developing%203D-VL%0Ageneralists%20lies%20in%20data%20scalability%2C%20hindered%20by%20the%20lack%20of%20an%20efficient%0Ascene%20representation.%20We%20propose%20LEO-VL%2C%20a%203D-VL%20model%20built%20upon%20condensed%0Afeature%20grid%20%28CFG%29%2C%20an%20efficient%20scene%20representation%20that%20bridges%202D%0Aperception%20and%203D%20spatial%20structure%20while%20significantly%20reducing%20token%0Aoverhead.%20This%20efficiency%20unlocks%20large-scale%20training%20towards%203D-VL%0Ageneralist%2C%20for%20which%20we%20curate%20over%20700k%20high-quality%203D-VL%20data%20spanning%20four%0Adomains%20of%20real-world%20indoor%20scenes%20and%20five%20tasks%20such%20as%20captioning%20and%0Adialogue.%20LEO-VL%20achieves%20state-of-the-art%20performance%20on%20a%20variety%20of%203D%20QA%0Abenchmarks%2C%20including%20SQA3D%2C%20MSQA%2C%20and%20Beacon3D.%20Ablation%20studies%20confirm%20the%0Aefficiency%20of%20our%20representation%2C%20the%20importance%20of%20task%20and%20scene%20diversity%2C%0Aand%20the%20validity%20of%20our%20data%20curation%20principle.%20Furthermore%2C%20we%20introduce%0ASceneDPO%2C%20a%20novel%20post-training%20objective%20that%20enhances%20the%20robustness%20of%203D-VL%0Amodels.%20We%20hope%20our%20findings%20contribute%20to%20the%20advancement%20of%20scalable%20and%0Arobust%203D-VL%20generalists.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09935v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEO-VL%253A%2520Towards%25203D%2520Vision-Language%2520Generalists%2520via%2520Data%2520Scaling%2520with%250A%2520%2520Efficient%2520Representation%26entry.906535625%3DJiangyong%2520Huang%2520and%2520Xiaojian%2520Ma%2520and%2520Xiongkun%2520Linghu%2520and%2520Yue%2520Fan%2520and%2520Junchao%2520He%2520and%2520Wenxin%2520Tan%2520and%2520Qing%2520Li%2520and%2520Song-Chun%2520Zhu%2520and%2520Yixin%2520Chen%2520and%2520Baoxiong%2520Jia%2520and%2520Siyuan%2520Huang%26entry.1292438233%3D%2520%2520Developing%25203D-VL%2520generalists%2520capable%2520of%2520understanding%25203D%2520scenes%2520and%2520following%250Anatural%2520language%2520instructions%2520to%2520perform%2520a%2520wide%2520range%2520of%2520tasks%2520has%2520been%2520a%250Along-standing%2520goal%2520in%2520the%25203D-VL%2520community.%2520Despite%2520recent%2520progress%252C%25203D-VL%250Amodels%2520still%2520lag%2520behind%2520their%25202D%2520counterparts%2520in%2520capability%2520and%2520robustness%252C%250Afalling%2520short%2520of%2520the%2520generalist%2520standard.%2520A%2520key%2520obstacle%2520to%2520developing%25203D-VL%250Ageneralists%2520lies%2520in%2520data%2520scalability%252C%2520hindered%2520by%2520the%2520lack%2520of%2520an%2520efficient%250Ascene%2520representation.%2520We%2520propose%2520LEO-VL%252C%2520a%25203D-VL%2520model%2520built%2520upon%2520condensed%250Afeature%2520grid%2520%2528CFG%2529%252C%2520an%2520efficient%2520scene%2520representation%2520that%2520bridges%25202D%250Aperception%2520and%25203D%2520spatial%2520structure%2520while%2520significantly%2520reducing%2520token%250Aoverhead.%2520This%2520efficiency%2520unlocks%2520large-scale%2520training%2520towards%25203D-VL%250Ageneralist%252C%2520for%2520which%2520we%2520curate%2520over%2520700k%2520high-quality%25203D-VL%2520data%2520spanning%2520four%250Adomains%2520of%2520real-world%2520indoor%2520scenes%2520and%2520five%2520tasks%2520such%2520as%2520captioning%2520and%250Adialogue.%2520LEO-VL%2520achieves%2520state-of-the-art%2520performance%2520on%2520a%2520variety%2520of%25203D%2520QA%250Abenchmarks%252C%2520including%2520SQA3D%252C%2520MSQA%252C%2520and%2520Beacon3D.%2520Ablation%2520studies%2520confirm%2520the%250Aefficiency%2520of%2520our%2520representation%252C%2520the%2520importance%2520of%2520task%2520and%2520scene%2520diversity%252C%250Aand%2520the%2520validity%2520of%2520our%2520data%2520curation%2520principle.%2520Furthermore%252C%2520we%2520introduce%250ASceneDPO%252C%2520a%2520novel%2520post-training%2520objective%2520that%2520enhances%2520the%2520robustness%2520of%25203D-VL%250Amodels.%2520We%2520hope%2520our%2520findings%2520contribute%2520to%2520the%2520advancement%2520of%2520scalable%2520and%250Arobust%25203D-VL%2520generalists.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09935v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEO-VL%3A%20Towards%203D%20Vision-Language%20Generalists%20via%20Data%20Scaling%20with%0A%20%20Efficient%20Representation&entry.906535625=Jiangyong%20Huang%20and%20Xiaojian%20Ma%20and%20Xiongkun%20Linghu%20and%20Yue%20Fan%20and%20Junchao%20He%20and%20Wenxin%20Tan%20and%20Qing%20Li%20and%20Song-Chun%20Zhu%20and%20Yixin%20Chen%20and%20Baoxiong%20Jia%20and%20Siyuan%20Huang&entry.1292438233=%20%20Developing%203D-VL%20generalists%20capable%20of%20understanding%203D%20scenes%20and%20following%0Anatural%20language%20instructions%20to%20perform%20a%20wide%20range%20of%20tasks%20has%20been%20a%0Along-standing%20goal%20in%20the%203D-VL%20community.%20Despite%20recent%20progress%2C%203D-VL%0Amodels%20still%20lag%20behind%20their%202D%20counterparts%20in%20capability%20and%20robustness%2C%0Afalling%20short%20of%20the%20generalist%20standard.%20A%20key%20obstacle%20to%20developing%203D-VL%0Ageneralists%20lies%20in%20data%20scalability%2C%20hindered%20by%20the%20lack%20of%20an%20efficient%0Ascene%20representation.%20We%20propose%20LEO-VL%2C%20a%203D-VL%20model%20built%20upon%20condensed%0Afeature%20grid%20%28CFG%29%2C%20an%20efficient%20scene%20representation%20that%20bridges%202D%0Aperception%20and%203D%20spatial%20structure%20while%20significantly%20reducing%20token%0Aoverhead.%20This%20efficiency%20unlocks%20large-scale%20training%20towards%203D-VL%0Ageneralist%2C%20for%20which%20we%20curate%20over%20700k%20high-quality%203D-VL%20data%20spanning%20four%0Adomains%20of%20real-world%20indoor%20scenes%20and%20five%20tasks%20such%20as%20captioning%20and%0Adialogue.%20LEO-VL%20achieves%20state-of-the-art%20performance%20on%20a%20variety%20of%203D%20QA%0Abenchmarks%2C%20including%20SQA3D%2C%20MSQA%2C%20and%20Beacon3D.%20Ablation%20studies%20confirm%20the%0Aefficiency%20of%20our%20representation%2C%20the%20importance%20of%20task%20and%20scene%20diversity%2C%0Aand%20the%20validity%20of%20our%20data%20curation%20principle.%20Furthermore%2C%20we%20introduce%0ASceneDPO%2C%20a%20novel%20post-training%20objective%20that%20enhances%20the%20robustness%20of%203D-VL%0Amodels.%20We%20hope%20our%20findings%20contribute%20to%20the%20advancement%20of%20scalable%20and%0Arobust%203D-VL%20generalists.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09935v1&entry.124074799=Read"},
{"title": "AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven\n  Universal Mesh Animation", "author": "Zijie Wu and Chaohui Yu and Fan Wang and Xiang Bai", "abstract": "  Recent advances in 4D content generation have attracted increasing attention,\nyet creating high-quality animated 3D models remains challenging due to the\ncomplexity of modeling spatio-temporal distributions and the scarcity of 4D\ntraining data. In this paper, we present AnimateAnyMesh, the first feed-forward\nframework that enables efficient text-driven animation of arbitrary 3D meshes.\nOur approach leverages a novel DyMeshVAE architecture that effectively\ncompresses and reconstructs dynamic mesh sequences by disentangling spatial and\ntemporal features while preserving local topological structures. To enable\nhigh-quality text-conditional generation, we employ a Rectified Flow-based\ntraining strategy in the compressed latent space. Additionally, we contribute\nthe DyMesh Dataset, containing over 4M diverse dynamic mesh sequences with text\nannotations. Experimental results demonstrate that our method generates\nsemantically accurate and temporally coherent mesh animations in a few seconds,\nsignificantly outperforming existing approaches in both quality and efficiency.\nOur work marks a substantial step forward in making 4D content creation more\naccessible and practical. All the data, code, and models will be open-released.\n", "link": "http://arxiv.org/abs/2506.09982v1", "date": "2025-06-11", "relevancy": 3.1873, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6615}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6503}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnimateAnyMesh%3A%20A%20Feed-Forward%204D%20Foundation%20Model%20for%20Text-Driven%0A%20%20Universal%20Mesh%20Animation&body=Title%3A%20AnimateAnyMesh%3A%20A%20Feed-Forward%204D%20Foundation%20Model%20for%20Text-Driven%0A%20%20Universal%20Mesh%20Animation%0AAuthor%3A%20Zijie%20Wu%20and%20Chaohui%20Yu%20and%20Fan%20Wang%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20Recent%20advances%20in%204D%20content%20generation%20have%20attracted%20increasing%20attention%2C%0Ayet%20creating%20high-quality%20animated%203D%20models%20remains%20challenging%20due%20to%20the%0Acomplexity%20of%20modeling%20spatio-temporal%20distributions%20and%20the%20scarcity%20of%204D%0Atraining%20data.%20In%20this%20paper%2C%20we%20present%20AnimateAnyMesh%2C%20the%20first%20feed-forward%0Aframework%20that%20enables%20efficient%20text-driven%20animation%20of%20arbitrary%203D%20meshes.%0AOur%20approach%20leverages%20a%20novel%20DyMeshVAE%20architecture%20that%20effectively%0Acompresses%20and%20reconstructs%20dynamic%20mesh%20sequences%20by%20disentangling%20spatial%20and%0Atemporal%20features%20while%20preserving%20local%20topological%20structures.%20To%20enable%0Ahigh-quality%20text-conditional%20generation%2C%20we%20employ%20a%20Rectified%20Flow-based%0Atraining%20strategy%20in%20the%20compressed%20latent%20space.%20Additionally%2C%20we%20contribute%0Athe%20DyMesh%20Dataset%2C%20containing%20over%204M%20diverse%20dynamic%20mesh%20sequences%20with%20text%0Aannotations.%20Experimental%20results%20demonstrate%20that%20our%20method%20generates%0Asemantically%20accurate%20and%20temporally%20coherent%20mesh%20animations%20in%20a%20few%20seconds%2C%0Asignificantly%20outperforming%20existing%20approaches%20in%20both%20quality%20and%20efficiency.%0AOur%20work%20marks%20a%20substantial%20step%20forward%20in%20making%204D%20content%20creation%20more%0Aaccessible%20and%20practical.%20All%20the%20data%2C%20code%2C%20and%20models%20will%20be%20open-released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnimateAnyMesh%253A%2520A%2520Feed-Forward%25204D%2520Foundation%2520Model%2520for%2520Text-Driven%250A%2520%2520Universal%2520Mesh%2520Animation%26entry.906535625%3DZijie%2520Wu%2520and%2520Chaohui%2520Yu%2520and%2520Fan%2520Wang%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%25204D%2520content%2520generation%2520have%2520attracted%2520increasing%2520attention%252C%250Ayet%2520creating%2520high-quality%2520animated%25203D%2520models%2520remains%2520challenging%2520due%2520to%2520the%250Acomplexity%2520of%2520modeling%2520spatio-temporal%2520distributions%2520and%2520the%2520scarcity%2520of%25204D%250Atraining%2520data.%2520In%2520this%2520paper%252C%2520we%2520present%2520AnimateAnyMesh%252C%2520the%2520first%2520feed-forward%250Aframework%2520that%2520enables%2520efficient%2520text-driven%2520animation%2520of%2520arbitrary%25203D%2520meshes.%250AOur%2520approach%2520leverages%2520a%2520novel%2520DyMeshVAE%2520architecture%2520that%2520effectively%250Acompresses%2520and%2520reconstructs%2520dynamic%2520mesh%2520sequences%2520by%2520disentangling%2520spatial%2520and%250Atemporal%2520features%2520while%2520preserving%2520local%2520topological%2520structures.%2520To%2520enable%250Ahigh-quality%2520text-conditional%2520generation%252C%2520we%2520employ%2520a%2520Rectified%2520Flow-based%250Atraining%2520strategy%2520in%2520the%2520compressed%2520latent%2520space.%2520Additionally%252C%2520we%2520contribute%250Athe%2520DyMesh%2520Dataset%252C%2520containing%2520over%25204M%2520diverse%2520dynamic%2520mesh%2520sequences%2520with%2520text%250Aannotations.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520generates%250Asemantically%2520accurate%2520and%2520temporally%2520coherent%2520mesh%2520animations%2520in%2520a%2520few%2520seconds%252C%250Asignificantly%2520outperforming%2520existing%2520approaches%2520in%2520both%2520quality%2520and%2520efficiency.%250AOur%2520work%2520marks%2520a%2520substantial%2520step%2520forward%2520in%2520making%25204D%2520content%2520creation%2520more%250Aaccessible%2520and%2520practical.%2520All%2520the%2520data%252C%2520code%252C%2520and%2520models%2520will%2520be%2520open-released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnimateAnyMesh%3A%20A%20Feed-Forward%204D%20Foundation%20Model%20for%20Text-Driven%0A%20%20Universal%20Mesh%20Animation&entry.906535625=Zijie%20Wu%20and%20Chaohui%20Yu%20and%20Fan%20Wang%20and%20Xiang%20Bai&entry.1292438233=%20%20Recent%20advances%20in%204D%20content%20generation%20have%20attracted%20increasing%20attention%2C%0Ayet%20creating%20high-quality%20animated%203D%20models%20remains%20challenging%20due%20to%20the%0Acomplexity%20of%20modeling%20spatio-temporal%20distributions%20and%20the%20scarcity%20of%204D%0Atraining%20data.%20In%20this%20paper%2C%20we%20present%20AnimateAnyMesh%2C%20the%20first%20feed-forward%0Aframework%20that%20enables%20efficient%20text-driven%20animation%20of%20arbitrary%203D%20meshes.%0AOur%20approach%20leverages%20a%20novel%20DyMeshVAE%20architecture%20that%20effectively%0Acompresses%20and%20reconstructs%20dynamic%20mesh%20sequences%20by%20disentangling%20spatial%20and%0Atemporal%20features%20while%20preserving%20local%20topological%20structures.%20To%20enable%0Ahigh-quality%20text-conditional%20generation%2C%20we%20employ%20a%20Rectified%20Flow-based%0Atraining%20strategy%20in%20the%20compressed%20latent%20space.%20Additionally%2C%20we%20contribute%0Athe%20DyMesh%20Dataset%2C%20containing%20over%204M%20diverse%20dynamic%20mesh%20sequences%20with%20text%0Aannotations.%20Experimental%20results%20demonstrate%20that%20our%20method%20generates%0Asemantically%20accurate%20and%20temporally%20coherent%20mesh%20animations%20in%20a%20few%20seconds%2C%0Asignificantly%20outperforming%20existing%20approaches%20in%20both%20quality%20and%20efficiency.%0AOur%20work%20marks%20a%20substantial%20step%20forward%20in%20making%204D%20content%20creation%20more%0Aaccessible%20and%20practical.%20All%20the%20data%2C%20code%2C%20and%20models%20will%20be%20open-released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09982v1&entry.124074799=Read"},
{"title": "Leveraging Depth and Language for Open-Vocabulary Domain-Generalized\n  Semantic Segmentation", "author": "Siyu Chen and Ting Han and Chengzheng Fu and Changshe Zhang and Chaolei Wang and Jinhe Su and Guorong Cai and Meiliu Wu", "abstract": "  Open-Vocabulary semantic segmentation (OVSS) and domain generalization in\nsemantic segmentation (DGSS) highlight a subtle complementarity that motivates\nOpen-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS\naims to generate pixel-level masks for unseen categories while maintaining\nrobustness across unseen domains, a critical capability for real-world\nscenarios such as autonomous driving in adverse conditions. We introduce Vireo,\na novel single-stage framework for OV-DGSS that unifies the strengths of OVSS\nand DGSS for the first time. Vireo builds upon the frozen Visual Foundation\nModels (VFMs) and incorporates scene geometry via Depth VFMs to extract\ndomain-invariant structural features. To bridge the gap between visual and\ntextual modalities under domain shift, we propose three key components: (1)\nGeoText Prompts, which align geometric features with language cues and\nprogressively refine VFM encoder representations; (2) Coarse Mask Prior\nEmbedding (CMPE) for enhancing gradient flow for faster convergence and\nstronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding\nHead (DOV-VEH), which fuses refined structural and semantic features for robust\nprediction. Comprehensive evaluation on these components demonstrates the\neffectiveness of our designs. Our proposed Vireo achieves the state-of-the-art\nperformance and surpasses existing methods by a large margin in both domain\ngeneralization and open-vocabulary recognition, offering a unified and scalable\nsolution for robust visual understanding in diverse and dynamic environments.\nCode is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.\n", "link": "http://arxiv.org/abs/2506.09881v1", "date": "2025-06-11", "relevancy": 3.0838, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6399}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6399}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Depth%20and%20Language%20for%20Open-Vocabulary%20Domain-Generalized%0A%20%20Semantic%20Segmentation&body=Title%3A%20Leveraging%20Depth%20and%20Language%20for%20Open-Vocabulary%20Domain-Generalized%0A%20%20Semantic%20Segmentation%0AAuthor%3A%20Siyu%20Chen%20and%20Ting%20Han%20and%20Chengzheng%20Fu%20and%20Changshe%20Zhang%20and%20Chaolei%20Wang%20and%20Jinhe%20Su%20and%20Guorong%20Cai%20and%20Meiliu%20Wu%0AAbstract%3A%20%20%20Open-Vocabulary%20semantic%20segmentation%20%28OVSS%29%20and%20domain%20generalization%20in%0Asemantic%20segmentation%20%28DGSS%29%20highlight%20a%20subtle%20complementarity%20that%20motivates%0AOpen-Vocabulary%20Domain-Generalized%20Semantic%20Segmentation%20%28OV-DGSS%29.%20OV-DGSS%0Aaims%20to%20generate%20pixel-level%20masks%20for%20unseen%20categories%20while%20maintaining%0Arobustness%20across%20unseen%20domains%2C%20a%20critical%20capability%20for%20real-world%0Ascenarios%20such%20as%20autonomous%20driving%20in%20adverse%20conditions.%20We%20introduce%20Vireo%2C%0Aa%20novel%20single-stage%20framework%20for%20OV-DGSS%20that%20unifies%20the%20strengths%20of%20OVSS%0Aand%20DGSS%20for%20the%20first%20time.%20Vireo%20builds%20upon%20the%20frozen%20Visual%20Foundation%0AModels%20%28VFMs%29%20and%20incorporates%20scene%20geometry%20via%20Depth%20VFMs%20to%20extract%0Adomain-invariant%20structural%20features.%20To%20bridge%20the%20gap%20between%20visual%20and%0Atextual%20modalities%20under%20domain%20shift%2C%20we%20propose%20three%20key%20components%3A%20%281%29%0AGeoText%20Prompts%2C%20which%20align%20geometric%20features%20with%20language%20cues%20and%0Aprogressively%20refine%20VFM%20encoder%20representations%3B%20%282%29%20Coarse%20Mask%20Prior%0AEmbedding%20%28CMPE%29%20for%20enhancing%20gradient%20flow%20for%20faster%20convergence%20and%0Astronger%20textual%20influence%3B%20and%20%283%29%20the%20Domain-Open-Vocabulary%20Vector%20Embedding%0AHead%20%28DOV-VEH%29%2C%20which%20fuses%20refined%20structural%20and%20semantic%20features%20for%20robust%0Aprediction.%20Comprehensive%20evaluation%20on%20these%20components%20demonstrates%20the%0Aeffectiveness%20of%20our%20designs.%20Our%20proposed%20Vireo%20achieves%20the%20state-of-the-art%0Aperformance%20and%20surpasses%20existing%20methods%20by%20a%20large%20margin%20in%20both%20domain%0Ageneralization%20and%20open-vocabulary%20recognition%2C%20offering%20a%20unified%20and%20scalable%0Asolution%20for%20robust%20visual%20understanding%20in%20diverse%20and%20dynamic%20environments.%0ACode%20is%20available%20at%20https%3A//github.com/anonymouse-9c53tp182bvz/Vireo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09881v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Depth%2520and%2520Language%2520for%2520Open-Vocabulary%2520Domain-Generalized%250A%2520%2520Semantic%2520Segmentation%26entry.906535625%3DSiyu%2520Chen%2520and%2520Ting%2520Han%2520and%2520Chengzheng%2520Fu%2520and%2520Changshe%2520Zhang%2520and%2520Chaolei%2520Wang%2520and%2520Jinhe%2520Su%2520and%2520Guorong%2520Cai%2520and%2520Meiliu%2520Wu%26entry.1292438233%3D%2520%2520Open-Vocabulary%2520semantic%2520segmentation%2520%2528OVSS%2529%2520and%2520domain%2520generalization%2520in%250Asemantic%2520segmentation%2520%2528DGSS%2529%2520highlight%2520a%2520subtle%2520complementarity%2520that%2520motivates%250AOpen-Vocabulary%2520Domain-Generalized%2520Semantic%2520Segmentation%2520%2528OV-DGSS%2529.%2520OV-DGSS%250Aaims%2520to%2520generate%2520pixel-level%2520masks%2520for%2520unseen%2520categories%2520while%2520maintaining%250Arobustness%2520across%2520unseen%2520domains%252C%2520a%2520critical%2520capability%2520for%2520real-world%250Ascenarios%2520such%2520as%2520autonomous%2520driving%2520in%2520adverse%2520conditions.%2520We%2520introduce%2520Vireo%252C%250Aa%2520novel%2520single-stage%2520framework%2520for%2520OV-DGSS%2520that%2520unifies%2520the%2520strengths%2520of%2520OVSS%250Aand%2520DGSS%2520for%2520the%2520first%2520time.%2520Vireo%2520builds%2520upon%2520the%2520frozen%2520Visual%2520Foundation%250AModels%2520%2528VFMs%2529%2520and%2520incorporates%2520scene%2520geometry%2520via%2520Depth%2520VFMs%2520to%2520extract%250Adomain-invariant%2520structural%2520features.%2520To%2520bridge%2520the%2520gap%2520between%2520visual%2520and%250Atextual%2520modalities%2520under%2520domain%2520shift%252C%2520we%2520propose%2520three%2520key%2520components%253A%2520%25281%2529%250AGeoText%2520Prompts%252C%2520which%2520align%2520geometric%2520features%2520with%2520language%2520cues%2520and%250Aprogressively%2520refine%2520VFM%2520encoder%2520representations%253B%2520%25282%2529%2520Coarse%2520Mask%2520Prior%250AEmbedding%2520%2528CMPE%2529%2520for%2520enhancing%2520gradient%2520flow%2520for%2520faster%2520convergence%2520and%250Astronger%2520textual%2520influence%253B%2520and%2520%25283%2529%2520the%2520Domain-Open-Vocabulary%2520Vector%2520Embedding%250AHead%2520%2528DOV-VEH%2529%252C%2520which%2520fuses%2520refined%2520structural%2520and%2520semantic%2520features%2520for%2520robust%250Aprediction.%2520Comprehensive%2520evaluation%2520on%2520these%2520components%2520demonstrates%2520the%250Aeffectiveness%2520of%2520our%2520designs.%2520Our%2520proposed%2520Vireo%2520achieves%2520the%2520state-of-the-art%250Aperformance%2520and%2520surpasses%2520existing%2520methods%2520by%2520a%2520large%2520margin%2520in%2520both%2520domain%250Ageneralization%2520and%2520open-vocabulary%2520recognition%252C%2520offering%2520a%2520unified%2520and%2520scalable%250Asolution%2520for%2520robust%2520visual%2520understanding%2520in%2520diverse%2520and%2520dynamic%2520environments.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/anonymouse-9c53tp182bvz/Vireo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09881v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Depth%20and%20Language%20for%20Open-Vocabulary%20Domain-Generalized%0A%20%20Semantic%20Segmentation&entry.906535625=Siyu%20Chen%20and%20Ting%20Han%20and%20Chengzheng%20Fu%20and%20Changshe%20Zhang%20and%20Chaolei%20Wang%20and%20Jinhe%20Su%20and%20Guorong%20Cai%20and%20Meiliu%20Wu&entry.1292438233=%20%20Open-Vocabulary%20semantic%20segmentation%20%28OVSS%29%20and%20domain%20generalization%20in%0Asemantic%20segmentation%20%28DGSS%29%20highlight%20a%20subtle%20complementarity%20that%20motivates%0AOpen-Vocabulary%20Domain-Generalized%20Semantic%20Segmentation%20%28OV-DGSS%29.%20OV-DGSS%0Aaims%20to%20generate%20pixel-level%20masks%20for%20unseen%20categories%20while%20maintaining%0Arobustness%20across%20unseen%20domains%2C%20a%20critical%20capability%20for%20real-world%0Ascenarios%20such%20as%20autonomous%20driving%20in%20adverse%20conditions.%20We%20introduce%20Vireo%2C%0Aa%20novel%20single-stage%20framework%20for%20OV-DGSS%20that%20unifies%20the%20strengths%20of%20OVSS%0Aand%20DGSS%20for%20the%20first%20time.%20Vireo%20builds%20upon%20the%20frozen%20Visual%20Foundation%0AModels%20%28VFMs%29%20and%20incorporates%20scene%20geometry%20via%20Depth%20VFMs%20to%20extract%0Adomain-invariant%20structural%20features.%20To%20bridge%20the%20gap%20between%20visual%20and%0Atextual%20modalities%20under%20domain%20shift%2C%20we%20propose%20three%20key%20components%3A%20%281%29%0AGeoText%20Prompts%2C%20which%20align%20geometric%20features%20with%20language%20cues%20and%0Aprogressively%20refine%20VFM%20encoder%20representations%3B%20%282%29%20Coarse%20Mask%20Prior%0AEmbedding%20%28CMPE%29%20for%20enhancing%20gradient%20flow%20for%20faster%20convergence%20and%0Astronger%20textual%20influence%3B%20and%20%283%29%20the%20Domain-Open-Vocabulary%20Vector%20Embedding%0AHead%20%28DOV-VEH%29%2C%20which%20fuses%20refined%20structural%20and%20semantic%20features%20for%20robust%0Aprediction.%20Comprehensive%20evaluation%20on%20these%20components%20demonstrates%20the%0Aeffectiveness%20of%20our%20designs.%20Our%20proposed%20Vireo%20achieves%20the%20state-of-the-art%0Aperformance%20and%20surpasses%20existing%20methods%20by%20a%20large%20margin%20in%20both%20domain%0Ageneralization%20and%20open-vocabulary%20recognition%2C%20offering%20a%20unified%20and%20scalable%0Asolution%20for%20robust%20visual%20understanding%20in%20diverse%20and%20dynamic%20environments.%0ACode%20is%20available%20at%20https%3A//github.com/anonymouse-9c53tp182bvz/Vireo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09881v1&entry.124074799=Read"},
{"title": "The Less You Depend, The More You Learn: Synthesizing Novel Views from\n  Sparse, Unposed Images without Any 3D Knowledge", "author": "Haoru Wang and Kai Ye and Yangyan Li and Wenzheng Chen and Baoquan Chen", "abstract": "  We consider the problem of generalizable novel view synthesis (NVS), which\naims to generate photorealistic novel views from sparse or even unposed 2D\nimages without per-scene optimization. This task remains fundamentally\nchallenging, as it requires inferring 3D structure from incomplete and\nambiguous 2D observations. Early approaches typically rely on strong 3D\nknowledge, including architectural 3D inductive biases (e.g., embedding\nexplicit 3D representations, such as NeRF or 3DGS, into network design) and\nground-truth camera poses for both input and target views. While recent efforts\nhave sought to reduce the 3D inductive bias or the dependence on known camera\nposes of input views, critical questions regarding the role of 3D knowledge and\nthe necessity of circumventing its use remain under-explored. In this work, we\nconduct a systematic analysis on the 3D knowledge and uncover a critical trend:\nthe performance of methods that requires less 3D knowledge accelerates more as\ndata scales, eventually achieving performance on par with their 3D\nknowledge-driven counterparts, which highlights the increasing importance of\nreducing dependence on 3D knowledge in the era of large-scale data. Motivated\nby and following this trend, we propose a novel NVS framework that minimizes 3D\ninductive bias and pose dependence for both input and target views. By\neliminating this 3D knowledge, our method fully leverages data scaling and\nlearns implicit 3D awareness directly from sparse 2D images, without any 3D\ninductive bias or pose annotation during training. Extensive experiments\ndemonstrate that our model generates photorealistic and 3D-consistent novel\nviews, achieving even comparable performance with methods that rely on posed\ninputs, thereby validating the feasibility and effectiveness of our\ndata-centric paradigm. Project page:\nhttps://pku-vcl-geometry.github.io/Less3Depend/ .\n", "link": "http://arxiv.org/abs/2506.09885v1", "date": "2025-06-11", "relevancy": 3.0579, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6117}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6117}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Less%20You%20Depend%2C%20The%20More%20You%20Learn%3A%20Synthesizing%20Novel%20Views%20from%0A%20%20Sparse%2C%20Unposed%20Images%20without%20Any%203D%20Knowledge&body=Title%3A%20The%20Less%20You%20Depend%2C%20The%20More%20You%20Learn%3A%20Synthesizing%20Novel%20Views%20from%0A%20%20Sparse%2C%20Unposed%20Images%20without%20Any%203D%20Knowledge%0AAuthor%3A%20Haoru%20Wang%20and%20Kai%20Ye%20and%20Yangyan%20Li%20and%20Wenzheng%20Chen%20and%20Baoquan%20Chen%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20generalizable%20novel%20view%20synthesis%20%28NVS%29%2C%20which%0Aaims%20to%20generate%20photorealistic%20novel%20views%20from%20sparse%20or%20even%20unposed%202D%0Aimages%20without%20per-scene%20optimization.%20This%20task%20remains%20fundamentally%0Achallenging%2C%20as%20it%20requires%20inferring%203D%20structure%20from%20incomplete%20and%0Aambiguous%202D%20observations.%20Early%20approaches%20typically%20rely%20on%20strong%203D%0Aknowledge%2C%20including%20architectural%203D%20inductive%20biases%20%28e.g.%2C%20embedding%0Aexplicit%203D%20representations%2C%20such%20as%20NeRF%20or%203DGS%2C%20into%20network%20design%29%20and%0Aground-truth%20camera%20poses%20for%20both%20input%20and%20target%20views.%20While%20recent%20efforts%0Ahave%20sought%20to%20reduce%20the%203D%20inductive%20bias%20or%20the%20dependence%20on%20known%20camera%0Aposes%20of%20input%20views%2C%20critical%20questions%20regarding%20the%20role%20of%203D%20knowledge%20and%0Athe%20necessity%20of%20circumventing%20its%20use%20remain%20under-explored.%20In%20this%20work%2C%20we%0Aconduct%20a%20systematic%20analysis%20on%20the%203D%20knowledge%20and%20uncover%20a%20critical%20trend%3A%0Athe%20performance%20of%20methods%20that%20requires%20less%203D%20knowledge%20accelerates%20more%20as%0Adata%20scales%2C%20eventually%20achieving%20performance%20on%20par%20with%20their%203D%0Aknowledge-driven%20counterparts%2C%20which%20highlights%20the%20increasing%20importance%20of%0Areducing%20dependence%20on%203D%20knowledge%20in%20the%20era%20of%20large-scale%20data.%20Motivated%0Aby%20and%20following%20this%20trend%2C%20we%20propose%20a%20novel%20NVS%20framework%20that%20minimizes%203D%0Ainductive%20bias%20and%20pose%20dependence%20for%20both%20input%20and%20target%20views.%20By%0Aeliminating%20this%203D%20knowledge%2C%20our%20method%20fully%20leverages%20data%20scaling%20and%0Alearns%20implicit%203D%20awareness%20directly%20from%20sparse%202D%20images%2C%20without%20any%203D%0Ainductive%20bias%20or%20pose%20annotation%20during%20training.%20Extensive%20experiments%0Ademonstrate%20that%20our%20model%20generates%20photorealistic%20and%203D-consistent%20novel%0Aviews%2C%20achieving%20even%20comparable%20performance%20with%20methods%20that%20rely%20on%20posed%0Ainputs%2C%20thereby%20validating%20the%20feasibility%20and%20effectiveness%20of%20our%0Adata-centric%20paradigm.%20Project%20page%3A%0Ahttps%3A//pku-vcl-geometry.github.io/Less3Depend/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Less%2520You%2520Depend%252C%2520The%2520More%2520You%2520Learn%253A%2520Synthesizing%2520Novel%2520Views%2520from%250A%2520%2520Sparse%252C%2520Unposed%2520Images%2520without%2520Any%25203D%2520Knowledge%26entry.906535625%3DHaoru%2520Wang%2520and%2520Kai%2520Ye%2520and%2520Yangyan%2520Li%2520and%2520Wenzheng%2520Chen%2520and%2520Baoquan%2520Chen%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520generalizable%2520novel%2520view%2520synthesis%2520%2528NVS%2529%252C%2520which%250Aaims%2520to%2520generate%2520photorealistic%2520novel%2520views%2520from%2520sparse%2520or%2520even%2520unposed%25202D%250Aimages%2520without%2520per-scene%2520optimization.%2520This%2520task%2520remains%2520fundamentally%250Achallenging%252C%2520as%2520it%2520requires%2520inferring%25203D%2520structure%2520from%2520incomplete%2520and%250Aambiguous%25202D%2520observations.%2520Early%2520approaches%2520typically%2520rely%2520on%2520strong%25203D%250Aknowledge%252C%2520including%2520architectural%25203D%2520inductive%2520biases%2520%2528e.g.%252C%2520embedding%250Aexplicit%25203D%2520representations%252C%2520such%2520as%2520NeRF%2520or%25203DGS%252C%2520into%2520network%2520design%2529%2520and%250Aground-truth%2520camera%2520poses%2520for%2520both%2520input%2520and%2520target%2520views.%2520While%2520recent%2520efforts%250Ahave%2520sought%2520to%2520reduce%2520the%25203D%2520inductive%2520bias%2520or%2520the%2520dependence%2520on%2520known%2520camera%250Aposes%2520of%2520input%2520views%252C%2520critical%2520questions%2520regarding%2520the%2520role%2520of%25203D%2520knowledge%2520and%250Athe%2520necessity%2520of%2520circumventing%2520its%2520use%2520remain%2520under-explored.%2520In%2520this%2520work%252C%2520we%250Aconduct%2520a%2520systematic%2520analysis%2520on%2520the%25203D%2520knowledge%2520and%2520uncover%2520a%2520critical%2520trend%253A%250Athe%2520performance%2520of%2520methods%2520that%2520requires%2520less%25203D%2520knowledge%2520accelerates%2520more%2520as%250Adata%2520scales%252C%2520eventually%2520achieving%2520performance%2520on%2520par%2520with%2520their%25203D%250Aknowledge-driven%2520counterparts%252C%2520which%2520highlights%2520the%2520increasing%2520importance%2520of%250Areducing%2520dependence%2520on%25203D%2520knowledge%2520in%2520the%2520era%2520of%2520large-scale%2520data.%2520Motivated%250Aby%2520and%2520following%2520this%2520trend%252C%2520we%2520propose%2520a%2520novel%2520NVS%2520framework%2520that%2520minimizes%25203D%250Ainductive%2520bias%2520and%2520pose%2520dependence%2520for%2520both%2520input%2520and%2520target%2520views.%2520By%250Aeliminating%2520this%25203D%2520knowledge%252C%2520our%2520method%2520fully%2520leverages%2520data%2520scaling%2520and%250Alearns%2520implicit%25203D%2520awareness%2520directly%2520from%2520sparse%25202D%2520images%252C%2520without%2520any%25203D%250Ainductive%2520bias%2520or%2520pose%2520annotation%2520during%2520training.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520model%2520generates%2520photorealistic%2520and%25203D-consistent%2520novel%250Aviews%252C%2520achieving%2520even%2520comparable%2520performance%2520with%2520methods%2520that%2520rely%2520on%2520posed%250Ainputs%252C%2520thereby%2520validating%2520the%2520feasibility%2520and%2520effectiveness%2520of%2520our%250Adata-centric%2520paradigm.%2520Project%2520page%253A%250Ahttps%253A//pku-vcl-geometry.github.io/Less3Depend/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Less%20You%20Depend%2C%20The%20More%20You%20Learn%3A%20Synthesizing%20Novel%20Views%20from%0A%20%20Sparse%2C%20Unposed%20Images%20without%20Any%203D%20Knowledge&entry.906535625=Haoru%20Wang%20and%20Kai%20Ye%20and%20Yangyan%20Li%20and%20Wenzheng%20Chen%20and%20Baoquan%20Chen&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20generalizable%20novel%20view%20synthesis%20%28NVS%29%2C%20which%0Aaims%20to%20generate%20photorealistic%20novel%20views%20from%20sparse%20or%20even%20unposed%202D%0Aimages%20without%20per-scene%20optimization.%20This%20task%20remains%20fundamentally%0Achallenging%2C%20as%20it%20requires%20inferring%203D%20structure%20from%20incomplete%20and%0Aambiguous%202D%20observations.%20Early%20approaches%20typically%20rely%20on%20strong%203D%0Aknowledge%2C%20including%20architectural%203D%20inductive%20biases%20%28e.g.%2C%20embedding%0Aexplicit%203D%20representations%2C%20such%20as%20NeRF%20or%203DGS%2C%20into%20network%20design%29%20and%0Aground-truth%20camera%20poses%20for%20both%20input%20and%20target%20views.%20While%20recent%20efforts%0Ahave%20sought%20to%20reduce%20the%203D%20inductive%20bias%20or%20the%20dependence%20on%20known%20camera%0Aposes%20of%20input%20views%2C%20critical%20questions%20regarding%20the%20role%20of%203D%20knowledge%20and%0Athe%20necessity%20of%20circumventing%20its%20use%20remain%20under-explored.%20In%20this%20work%2C%20we%0Aconduct%20a%20systematic%20analysis%20on%20the%203D%20knowledge%20and%20uncover%20a%20critical%20trend%3A%0Athe%20performance%20of%20methods%20that%20requires%20less%203D%20knowledge%20accelerates%20more%20as%0Adata%20scales%2C%20eventually%20achieving%20performance%20on%20par%20with%20their%203D%0Aknowledge-driven%20counterparts%2C%20which%20highlights%20the%20increasing%20importance%20of%0Areducing%20dependence%20on%203D%20knowledge%20in%20the%20era%20of%20large-scale%20data.%20Motivated%0Aby%20and%20following%20this%20trend%2C%20we%20propose%20a%20novel%20NVS%20framework%20that%20minimizes%203D%0Ainductive%20bias%20and%20pose%20dependence%20for%20both%20input%20and%20target%20views.%20By%0Aeliminating%20this%203D%20knowledge%2C%20our%20method%20fully%20leverages%20data%20scaling%20and%0Alearns%20implicit%203D%20awareness%20directly%20from%20sparse%202D%20images%2C%20without%20any%203D%0Ainductive%20bias%20or%20pose%20annotation%20during%20training.%20Extensive%20experiments%0Ademonstrate%20that%20our%20model%20generates%20photorealistic%20and%203D-consistent%20novel%0Aviews%2C%20achieving%20even%20comparable%20performance%20with%20methods%20that%20rely%20on%20posed%0Ainputs%2C%20thereby%20validating%20the%20feasibility%20and%20effectiveness%20of%20our%0Adata-centric%20paradigm.%20Project%20page%3A%0Ahttps%3A//pku-vcl-geometry.github.io/Less3Depend/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09885v1&entry.124074799=Read"},
{"title": "Understanding Long Videos with Multimodal Language Models", "author": "Kanchana Ranasinghe and Xiang Li and Kumara Kahatapitiya and Michael S. Ryoo", "abstract": "  Large Language Models (LLMs) have allowed recent LLM-based approaches to\nachieve excellent performance on long-video understanding benchmarks. We\ninvestigate how extensive world knowledge and strong reasoning skills of\nunderlying LLMs influence this strong performance. Surprisingly, we discover\nthat LLM-based approaches can yield surprisingly good accuracy on long-video\ntasks with limited video information, sometimes even with no video specific\ninformation. Building on this, we explore injecting video-specific information\ninto an LLM-based framework. We utilize off-the-shelf vision tools to extract\nthree object-centric information modalities from videos, and then leverage\nnatural language as a medium for fusing this information. Our resulting\nMultimodal Video Understanding (MVU) framework demonstrates state-of-the-art\nperformance across multiple video understanding benchmarks. Strong performance\nalso on robotics domain tasks establish its strong generality. Code:\nhttps://github.com/kahnchana/mvu\n", "link": "http://arxiv.org/abs/2403.16998v5", "date": "2025-06-11", "relevancy": 3.0127, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6154}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6154}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Long%20Videos%20with%20Multimodal%20Language%20Models&body=Title%3A%20Understanding%20Long%20Videos%20with%20Multimodal%20Language%20Models%0AAuthor%3A%20Kanchana%20Ranasinghe%20and%20Xiang%20Li%20and%20Kumara%20Kahatapitiya%20and%20Michael%20S.%20Ryoo%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20allowed%20recent%20LLM-based%20approaches%20to%0Aachieve%20excellent%20performance%20on%20long-video%20understanding%20benchmarks.%20We%0Ainvestigate%20how%20extensive%20world%20knowledge%20and%20strong%20reasoning%20skills%20of%0Aunderlying%20LLMs%20influence%20this%20strong%20performance.%20Surprisingly%2C%20we%20discover%0Athat%20LLM-based%20approaches%20can%20yield%20surprisingly%20good%20accuracy%20on%20long-video%0Atasks%20with%20limited%20video%20information%2C%20sometimes%20even%20with%20no%20video%20specific%0Ainformation.%20Building%20on%20this%2C%20we%20explore%20injecting%20video-specific%20information%0Ainto%20an%20LLM-based%20framework.%20We%20utilize%20off-the-shelf%20vision%20tools%20to%20extract%0Athree%20object-centric%20information%20modalities%20from%20videos%2C%20and%20then%20leverage%0Anatural%20language%20as%20a%20medium%20for%20fusing%20this%20information.%20Our%20resulting%0AMultimodal%20Video%20Understanding%20%28MVU%29%20framework%20demonstrates%20state-of-the-art%0Aperformance%20across%20multiple%20video%20understanding%20benchmarks.%20Strong%20performance%0Aalso%20on%20robotics%20domain%20tasks%20establish%20its%20strong%20generality.%20Code%3A%0Ahttps%3A//github.com/kahnchana/mvu%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16998v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Long%2520Videos%2520with%2520Multimodal%2520Language%2520Models%26entry.906535625%3DKanchana%2520Ranasinghe%2520and%2520Xiang%2520Li%2520and%2520Kumara%2520Kahatapitiya%2520and%2520Michael%2520S.%2520Ryoo%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520allowed%2520recent%2520LLM-based%2520approaches%2520to%250Aachieve%2520excellent%2520performance%2520on%2520long-video%2520understanding%2520benchmarks.%2520We%250Ainvestigate%2520how%2520extensive%2520world%2520knowledge%2520and%2520strong%2520reasoning%2520skills%2520of%250Aunderlying%2520LLMs%2520influence%2520this%2520strong%2520performance.%2520Surprisingly%252C%2520we%2520discover%250Athat%2520LLM-based%2520approaches%2520can%2520yield%2520surprisingly%2520good%2520accuracy%2520on%2520long-video%250Atasks%2520with%2520limited%2520video%2520information%252C%2520sometimes%2520even%2520with%2520no%2520video%2520specific%250Ainformation.%2520Building%2520on%2520this%252C%2520we%2520explore%2520injecting%2520video-specific%2520information%250Ainto%2520an%2520LLM-based%2520framework.%2520We%2520utilize%2520off-the-shelf%2520vision%2520tools%2520to%2520extract%250Athree%2520object-centric%2520information%2520modalities%2520from%2520videos%252C%2520and%2520then%2520leverage%250Anatural%2520language%2520as%2520a%2520medium%2520for%2520fusing%2520this%2520information.%2520Our%2520resulting%250AMultimodal%2520Video%2520Understanding%2520%2528MVU%2529%2520framework%2520demonstrates%2520state-of-the-art%250Aperformance%2520across%2520multiple%2520video%2520understanding%2520benchmarks.%2520Strong%2520performance%250Aalso%2520on%2520robotics%2520domain%2520tasks%2520establish%2520its%2520strong%2520generality.%2520Code%253A%250Ahttps%253A//github.com/kahnchana/mvu%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16998v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Long%20Videos%20with%20Multimodal%20Language%20Models&entry.906535625=Kanchana%20Ranasinghe%20and%20Xiang%20Li%20and%20Kumara%20Kahatapitiya%20and%20Michael%20S.%20Ryoo&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20allowed%20recent%20LLM-based%20approaches%20to%0Aachieve%20excellent%20performance%20on%20long-video%20understanding%20benchmarks.%20We%0Ainvestigate%20how%20extensive%20world%20knowledge%20and%20strong%20reasoning%20skills%20of%0Aunderlying%20LLMs%20influence%20this%20strong%20performance.%20Surprisingly%2C%20we%20discover%0Athat%20LLM-based%20approaches%20can%20yield%20surprisingly%20good%20accuracy%20on%20long-video%0Atasks%20with%20limited%20video%20information%2C%20sometimes%20even%20with%20no%20video%20specific%0Ainformation.%20Building%20on%20this%2C%20we%20explore%20injecting%20video-specific%20information%0Ainto%20an%20LLM-based%20framework.%20We%20utilize%20off-the-shelf%20vision%20tools%20to%20extract%0Athree%20object-centric%20information%20modalities%20from%20videos%2C%20and%20then%20leverage%0Anatural%20language%20as%20a%20medium%20for%20fusing%20this%20information.%20Our%20resulting%0AMultimodal%20Video%20Understanding%20%28MVU%29%20framework%20demonstrates%20state-of-the-art%0Aperformance%20across%20multiple%20video%20understanding%20benchmarks.%20Strong%20performance%0Aalso%20on%20robotics%20domain%20tasks%20establish%20its%20strong%20generality.%20Code%3A%0Ahttps%3A//github.com/kahnchana/mvu%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16998v5&entry.124074799=Read"},
{"title": "3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation", "author": "Seonho Lee and Jiho Choi and Inha Kang and Jiwook Kim and Junsung Park and Hyunjung Shim", "abstract": "  Vision-Language Models (VLMs) have shown remarkable performance on diverse\nvisual and linguistic tasks, yet they remain fundamentally limited in their\nunderstanding of 3D spatial structures. We propose Geometric Distillation, a\nlightweight, annotation-free fine-tuning framework that injects human-inspired\ngeometric cues into pretrained VLMs without modifying their architecture. By\ndistilling (1) sparse correspondences, (2) relative depth relations, and (3)\ndense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,\nVGGT), our method shapes representations to be geometry-aware while remaining\ncompatible with natural image-text inputs. Through extensive evaluations on 3D\nvision-language reasoning and 3D perception benchmarks, our method consistently\noutperforms prior approaches, achieving improved 3D spatial reasoning with\nsignificantly lower computational cost. Our work demonstrates a scalable and\nefficient path to bridge 2D-trained VLMs with 3D understanding, opening up\nwider use in spatially grounded multimodal tasks.\n", "link": "http://arxiv.org/abs/2506.09883v1", "date": "2025-06-11", "relevancy": 3.0037, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6041}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6041}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-Aware%20Vision-Language%20Models%20Fine-Tuning%20with%20Geometric%20Distillation&body=Title%3A%203D-Aware%20Vision-Language%20Models%20Fine-Tuning%20with%20Geometric%20Distillation%0AAuthor%3A%20Seonho%20Lee%20and%20Jiho%20Choi%20and%20Inha%20Kang%20and%20Jiwook%20Kim%20and%20Junsung%20Park%20and%20Hyunjung%20Shim%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20shown%20remarkable%20performance%20on%20diverse%0Avisual%20and%20linguistic%20tasks%2C%20yet%20they%20remain%20fundamentally%20limited%20in%20their%0Aunderstanding%20of%203D%20spatial%20structures.%20We%20propose%20Geometric%20Distillation%2C%20a%0Alightweight%2C%20annotation-free%20fine-tuning%20framework%20that%20injects%20human-inspired%0Ageometric%20cues%20into%20pretrained%20VLMs%20without%20modifying%20their%20architecture.%20By%0Adistilling%20%281%29%20sparse%20correspondences%2C%20%282%29%20relative%20depth%20relations%2C%20and%20%283%29%0Adense%20cost%20volumes%20from%20off-the-shelf%203D%20foundation%20models%20%28e.g.%2C%20MASt3R%2C%0AVGGT%29%2C%20our%20method%20shapes%20representations%20to%20be%20geometry-aware%20while%20remaining%0Acompatible%20with%20natural%20image-text%20inputs.%20Through%20extensive%20evaluations%20on%203D%0Avision-language%20reasoning%20and%203D%20perception%20benchmarks%2C%20our%20method%20consistently%0Aoutperforms%20prior%20approaches%2C%20achieving%20improved%203D%20spatial%20reasoning%20with%0Asignificantly%20lower%20computational%20cost.%20Our%20work%20demonstrates%20a%20scalable%20and%0Aefficient%20path%20to%20bridge%202D-trained%20VLMs%20with%203D%20understanding%2C%20opening%20up%0Awider%20use%20in%20spatially%20grounded%20multimodal%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09883v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-Aware%2520Vision-Language%2520Models%2520Fine-Tuning%2520with%2520Geometric%2520Distillation%26entry.906535625%3DSeonho%2520Lee%2520and%2520Jiho%2520Choi%2520and%2520Inha%2520Kang%2520and%2520Jiwook%2520Kim%2520and%2520Junsung%2520Park%2520and%2520Hyunjung%2520Shim%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520shown%2520remarkable%2520performance%2520on%2520diverse%250Avisual%2520and%2520linguistic%2520tasks%252C%2520yet%2520they%2520remain%2520fundamentally%2520limited%2520in%2520their%250Aunderstanding%2520of%25203D%2520spatial%2520structures.%2520We%2520propose%2520Geometric%2520Distillation%252C%2520a%250Alightweight%252C%2520annotation-free%2520fine-tuning%2520framework%2520that%2520injects%2520human-inspired%250Ageometric%2520cues%2520into%2520pretrained%2520VLMs%2520without%2520modifying%2520their%2520architecture.%2520By%250Adistilling%2520%25281%2529%2520sparse%2520correspondences%252C%2520%25282%2529%2520relative%2520depth%2520relations%252C%2520and%2520%25283%2529%250Adense%2520cost%2520volumes%2520from%2520off-the-shelf%25203D%2520foundation%2520models%2520%2528e.g.%252C%2520MASt3R%252C%250AVGGT%2529%252C%2520our%2520method%2520shapes%2520representations%2520to%2520be%2520geometry-aware%2520while%2520remaining%250Acompatible%2520with%2520natural%2520image-text%2520inputs.%2520Through%2520extensive%2520evaluations%2520on%25203D%250Avision-language%2520reasoning%2520and%25203D%2520perception%2520benchmarks%252C%2520our%2520method%2520consistently%250Aoutperforms%2520prior%2520approaches%252C%2520achieving%2520improved%25203D%2520spatial%2520reasoning%2520with%250Asignificantly%2520lower%2520computational%2520cost.%2520Our%2520work%2520demonstrates%2520a%2520scalable%2520and%250Aefficient%2520path%2520to%2520bridge%25202D-trained%2520VLMs%2520with%25203D%2520understanding%252C%2520opening%2520up%250Awider%2520use%2520in%2520spatially%2520grounded%2520multimodal%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09883v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-Aware%20Vision-Language%20Models%20Fine-Tuning%20with%20Geometric%20Distillation&entry.906535625=Seonho%20Lee%20and%20Jiho%20Choi%20and%20Inha%20Kang%20and%20Jiwook%20Kim%20and%20Junsung%20Park%20and%20Hyunjung%20Shim&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20shown%20remarkable%20performance%20on%20diverse%0Avisual%20and%20linguistic%20tasks%2C%20yet%20they%20remain%20fundamentally%20limited%20in%20their%0Aunderstanding%20of%203D%20spatial%20structures.%20We%20propose%20Geometric%20Distillation%2C%20a%0Alightweight%2C%20annotation-free%20fine-tuning%20framework%20that%20injects%20human-inspired%0Ageometric%20cues%20into%20pretrained%20VLMs%20without%20modifying%20their%20architecture.%20By%0Adistilling%20%281%29%20sparse%20correspondences%2C%20%282%29%20relative%20depth%20relations%2C%20and%20%283%29%0Adense%20cost%20volumes%20from%20off-the-shelf%203D%20foundation%20models%20%28e.g.%2C%20MASt3R%2C%0AVGGT%29%2C%20our%20method%20shapes%20representations%20to%20be%20geometry-aware%20while%20remaining%0Acompatible%20with%20natural%20image-text%20inputs.%20Through%20extensive%20evaluations%20on%203D%0Avision-language%20reasoning%20and%203D%20perception%20benchmarks%2C%20our%20method%20consistently%0Aoutperforms%20prior%20approaches%2C%20achieving%20improved%203D%20spatial%20reasoning%20with%0Asignificantly%20lower%20computational%20cost.%20Our%20work%20demonstrates%20a%20scalable%20and%0Aefficient%20path%20to%20bridge%202D-trained%20VLMs%20with%203D%20understanding%2C%20opening%20up%0Awider%20use%20in%20spatially%20grounded%20multimodal%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09883v1&entry.124074799=Read"},
{"title": "ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal\n  Large Language Models", "author": "Danae S\u00e1nchez Villegas and Ingo Ziegler and Desmond Elliott", "abstract": "  Reasoning over sequences of images remains a challenge for multimodal large\nlanguage models (MLLMs). While recent models incorporate multi-image data\nduring pre-training, they still struggle to recognize sequential structures,\noften treating images independently. This work introduces ImageChain, a\nframework that enhances MLLMs with sequential reasoning capabilities over image\ndata by modeling visual sequences as a multi-turn conversation. In ImageChain,\nimages are interleaved with corresponding textual descriptions to form a\ncontrolled dialogue that explicitly captures temporal dependencies and\nnarrative progression. Our method optimizes for the task of next-scene\ndescription, where the model generates a context-aware description of an\nupcoming scene based on preceding visual and textual cues. We demonstrate that\nour approach improves performance on the next-scene description task --\nachieving an average improvement from 3.7% to 19% in SimRate, a metric that\nquantifies semantic similarity to human-annotated ground truths. Moreover,\nImageChain achieves robust zero-shot out-of-domain performance in applications\nranging from comics to robotics. Extensive experiments validate that\ninstruction-tuning in a multimodal, multi-turn conversation design is key to\nbridging the gap between static image understanding and temporally-aware\nreasoning.\n", "link": "http://arxiv.org/abs/2502.19409v2", "date": "2025-06-11", "relevancy": 2.9783, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5973}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImageChain%3A%20Advancing%20Sequential%20Image-to-Text%20Reasoning%20in%20Multimodal%0A%20%20Large%20Language%20Models&body=Title%3A%20ImageChain%3A%20Advancing%20Sequential%20Image-to-Text%20Reasoning%20in%20Multimodal%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Danae%20S%C3%A1nchez%20Villegas%20and%20Ingo%20Ziegler%20and%20Desmond%20Elliott%0AAbstract%3A%20%20%20Reasoning%20over%20sequences%20of%20images%20remains%20a%20challenge%20for%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29.%20While%20recent%20models%20incorporate%20multi-image%20data%0Aduring%20pre-training%2C%20they%20still%20struggle%20to%20recognize%20sequential%20structures%2C%0Aoften%20treating%20images%20independently.%20This%20work%20introduces%20ImageChain%2C%20a%0Aframework%20that%20enhances%20MLLMs%20with%20sequential%20reasoning%20capabilities%20over%20image%0Adata%20by%20modeling%20visual%20sequences%20as%20a%20multi-turn%20conversation.%20In%20ImageChain%2C%0Aimages%20are%20interleaved%20with%20corresponding%20textual%20descriptions%20to%20form%20a%0Acontrolled%20dialogue%20that%20explicitly%20captures%20temporal%20dependencies%20and%0Anarrative%20progression.%20Our%20method%20optimizes%20for%20the%20task%20of%20next-scene%0Adescription%2C%20where%20the%20model%20generates%20a%20context-aware%20description%20of%20an%0Aupcoming%20scene%20based%20on%20preceding%20visual%20and%20textual%20cues.%20We%20demonstrate%20that%0Aour%20approach%20improves%20performance%20on%20the%20next-scene%20description%20task%20--%0Aachieving%20an%20average%20improvement%20from%203.7%25%20to%2019%25%20in%20SimRate%2C%20a%20metric%20that%0Aquantifies%20semantic%20similarity%20to%20human-annotated%20ground%20truths.%20Moreover%2C%0AImageChain%20achieves%20robust%20zero-shot%20out-of-domain%20performance%20in%20applications%0Aranging%20from%20comics%20to%20robotics.%20Extensive%20experiments%20validate%20that%0Ainstruction-tuning%20in%20a%20multimodal%2C%20multi-turn%20conversation%20design%20is%20key%20to%0Abridging%20the%20gap%20between%20static%20image%20understanding%20and%20temporally-aware%0Areasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19409v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImageChain%253A%2520Advancing%2520Sequential%2520Image-to-Text%2520Reasoning%2520in%2520Multimodal%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DDanae%2520S%25C3%25A1nchez%2520Villegas%2520and%2520Ingo%2520Ziegler%2520and%2520Desmond%2520Elliott%26entry.1292438233%3D%2520%2520Reasoning%2520over%2520sequences%2520of%2520images%2520remains%2520a%2520challenge%2520for%2520multimodal%2520large%250Alanguage%2520models%2520%2528MLLMs%2529.%2520While%2520recent%2520models%2520incorporate%2520multi-image%2520data%250Aduring%2520pre-training%252C%2520they%2520still%2520struggle%2520to%2520recognize%2520sequential%2520structures%252C%250Aoften%2520treating%2520images%2520independently.%2520This%2520work%2520introduces%2520ImageChain%252C%2520a%250Aframework%2520that%2520enhances%2520MLLMs%2520with%2520sequential%2520reasoning%2520capabilities%2520over%2520image%250Adata%2520by%2520modeling%2520visual%2520sequences%2520as%2520a%2520multi-turn%2520conversation.%2520In%2520ImageChain%252C%250Aimages%2520are%2520interleaved%2520with%2520corresponding%2520textual%2520descriptions%2520to%2520form%2520a%250Acontrolled%2520dialogue%2520that%2520explicitly%2520captures%2520temporal%2520dependencies%2520and%250Anarrative%2520progression.%2520Our%2520method%2520optimizes%2520for%2520the%2520task%2520of%2520next-scene%250Adescription%252C%2520where%2520the%2520model%2520generates%2520a%2520context-aware%2520description%2520of%2520an%250Aupcoming%2520scene%2520based%2520on%2520preceding%2520visual%2520and%2520textual%2520cues.%2520We%2520demonstrate%2520that%250Aour%2520approach%2520improves%2520performance%2520on%2520the%2520next-scene%2520description%2520task%2520--%250Aachieving%2520an%2520average%2520improvement%2520from%25203.7%2525%2520to%252019%2525%2520in%2520SimRate%252C%2520a%2520metric%2520that%250Aquantifies%2520semantic%2520similarity%2520to%2520human-annotated%2520ground%2520truths.%2520Moreover%252C%250AImageChain%2520achieves%2520robust%2520zero-shot%2520out-of-domain%2520performance%2520in%2520applications%250Aranging%2520from%2520comics%2520to%2520robotics.%2520Extensive%2520experiments%2520validate%2520that%250Ainstruction-tuning%2520in%2520a%2520multimodal%252C%2520multi-turn%2520conversation%2520design%2520is%2520key%2520to%250Abridging%2520the%2520gap%2520between%2520static%2520image%2520understanding%2520and%2520temporally-aware%250Areasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19409v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImageChain%3A%20Advancing%20Sequential%20Image-to-Text%20Reasoning%20in%20Multimodal%0A%20%20Large%20Language%20Models&entry.906535625=Danae%20S%C3%A1nchez%20Villegas%20and%20Ingo%20Ziegler%20and%20Desmond%20Elliott&entry.1292438233=%20%20Reasoning%20over%20sequences%20of%20images%20remains%20a%20challenge%20for%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29.%20While%20recent%20models%20incorporate%20multi-image%20data%0Aduring%20pre-training%2C%20they%20still%20struggle%20to%20recognize%20sequential%20structures%2C%0Aoften%20treating%20images%20independently.%20This%20work%20introduces%20ImageChain%2C%20a%0Aframework%20that%20enhances%20MLLMs%20with%20sequential%20reasoning%20capabilities%20over%20image%0Adata%20by%20modeling%20visual%20sequences%20as%20a%20multi-turn%20conversation.%20In%20ImageChain%2C%0Aimages%20are%20interleaved%20with%20corresponding%20textual%20descriptions%20to%20form%20a%0Acontrolled%20dialogue%20that%20explicitly%20captures%20temporal%20dependencies%20and%0Anarrative%20progression.%20Our%20method%20optimizes%20for%20the%20task%20of%20next-scene%0Adescription%2C%20where%20the%20model%20generates%20a%20context-aware%20description%20of%20an%0Aupcoming%20scene%20based%20on%20preceding%20visual%20and%20textual%20cues.%20We%20demonstrate%20that%0Aour%20approach%20improves%20performance%20on%20the%20next-scene%20description%20task%20--%0Aachieving%20an%20average%20improvement%20from%203.7%25%20to%2019%25%20in%20SimRate%2C%20a%20metric%20that%0Aquantifies%20semantic%20similarity%20to%20human-annotated%20ground%20truths.%20Moreover%2C%0AImageChain%20achieves%20robust%20zero-shot%20out-of-domain%20performance%20in%20applications%0Aranging%20from%20comics%20to%20robotics.%20Extensive%20experiments%20validate%20that%0Ainstruction-tuning%20in%20a%20multimodal%2C%20multi-turn%20conversation%20design%20is%20key%20to%0Abridging%20the%20gap%20between%20static%20image%20understanding%20and%20temporally-aware%0Areasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19409v2&entry.124074799=Read"},
{"title": "Accurate and efficient zero-shot 6D pose estimation with frozen\n  foundation models", "author": "Andrea Caraffa and Davide Boscaini and Fabio Poiesi", "abstract": "  Estimating the 6D pose of objects from RGBD data is a fundamental problem in\ncomputer vision, with applications in robotics and augmented reality. A key\nchallenge is achieving generalization to novel objects that were not seen\nduring training. Most existing approaches address this by scaling up training\non synthetic data tailored to the task, a process that demands substantial\ncomputational resources. But is task-specific training really necessary for\naccurate and efficient 6D pose estimation of novel objects? To answer No!, we\nintroduce FreeZeV2, the second generation of FreeZe: a training-free method\nthat achieves strong generalization to unseen objects by leveraging geometric\nand vision foundation models pre-trained on unrelated data. FreeZeV2 improves\nboth accuracy and efficiency over FreeZe through three key contributions: (i) a\nsparse feature extraction strategy that reduces inference-time computation\nwithout sacrificing accuracy; (ii) a feature-aware scoring mechanism that\nimproves both pose selection during RANSAC-based 3D registration and the final\nranking of pose candidates; and (iii) a modular design that supports ensembles\nof instance segmentation models, increasing robustness to segmentation masks\nerrors. We evaluate FreeZeV2 on the seven core datasets of the BOP Benchmark,\nwhere it establishes a new state-of-the-art in 6D pose estimation of unseen\nobjects. When using the same segmentation masks, FreeZeV2 achieves a remarkable\n8x speedup over FreeZe while also improving accuracy by 5%. When using\nensembles of segmentation models, FreeZeV2 gains an additional 8% in accuracy\nwhile still running 2.5x faster than FreeZe. FreeZeV2 was awarded Best Overall\nMethod at the BOP Challenge 2024.\n", "link": "http://arxiv.org/abs/2506.09784v1", "date": "2025-06-11", "relevancy": 2.973, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6018}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6018}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accurate%20and%20efficient%20zero-shot%206D%20pose%20estimation%20with%20frozen%0A%20%20foundation%20models&body=Title%3A%20Accurate%20and%20efficient%20zero-shot%206D%20pose%20estimation%20with%20frozen%0A%20%20foundation%20models%0AAuthor%3A%20Andrea%20Caraffa%20and%20Davide%20Boscaini%20and%20Fabio%20Poiesi%0AAbstract%3A%20%20%20Estimating%20the%206D%20pose%20of%20objects%20from%20RGBD%20data%20is%20a%20fundamental%20problem%20in%0Acomputer%20vision%2C%20with%20applications%20in%20robotics%20and%20augmented%20reality.%20A%20key%0Achallenge%20is%20achieving%20generalization%20to%20novel%20objects%20that%20were%20not%20seen%0Aduring%20training.%20Most%20existing%20approaches%20address%20this%20by%20scaling%20up%20training%0Aon%20synthetic%20data%20tailored%20to%20the%20task%2C%20a%20process%20that%20demands%20substantial%0Acomputational%20resources.%20But%20is%20task-specific%20training%20really%20necessary%20for%0Aaccurate%20and%20efficient%206D%20pose%20estimation%20of%20novel%20objects%3F%20To%20answer%20No%21%2C%20we%0Aintroduce%20FreeZeV2%2C%20the%20second%20generation%20of%20FreeZe%3A%20a%20training-free%20method%0Athat%20achieves%20strong%20generalization%20to%20unseen%20objects%20by%20leveraging%20geometric%0Aand%20vision%20foundation%20models%20pre-trained%20on%20unrelated%20data.%20FreeZeV2%20improves%0Aboth%20accuracy%20and%20efficiency%20over%20FreeZe%20through%20three%20key%20contributions%3A%20%28i%29%20a%0Asparse%20feature%20extraction%20strategy%20that%20reduces%20inference-time%20computation%0Awithout%20sacrificing%20accuracy%3B%20%28ii%29%20a%20feature-aware%20scoring%20mechanism%20that%0Aimproves%20both%20pose%20selection%20during%20RANSAC-based%203D%20registration%20and%20the%20final%0Aranking%20of%20pose%20candidates%3B%20and%20%28iii%29%20a%20modular%20design%20that%20supports%20ensembles%0Aof%20instance%20segmentation%20models%2C%20increasing%20robustness%20to%20segmentation%20masks%0Aerrors.%20We%20evaluate%20FreeZeV2%20on%20the%20seven%20core%20datasets%20of%20the%20BOP%20Benchmark%2C%0Awhere%20it%20establishes%20a%20new%20state-of-the-art%20in%206D%20pose%20estimation%20of%20unseen%0Aobjects.%20When%20using%20the%20same%20segmentation%20masks%2C%20FreeZeV2%20achieves%20a%20remarkable%0A8x%20speedup%20over%20FreeZe%20while%20also%20improving%20accuracy%20by%205%25.%20When%20using%0Aensembles%20of%20segmentation%20models%2C%20FreeZeV2%20gains%20an%20additional%208%25%20in%20accuracy%0Awhile%20still%20running%202.5x%20faster%20than%20FreeZe.%20FreeZeV2%20was%20awarded%20Best%20Overall%0AMethod%20at%20the%20BOP%20Challenge%202024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccurate%2520and%2520efficient%2520zero-shot%25206D%2520pose%2520estimation%2520with%2520frozen%250A%2520%2520foundation%2520models%26entry.906535625%3DAndrea%2520Caraffa%2520and%2520Davide%2520Boscaini%2520and%2520Fabio%2520Poiesi%26entry.1292438233%3D%2520%2520Estimating%2520the%25206D%2520pose%2520of%2520objects%2520from%2520RGBD%2520data%2520is%2520a%2520fundamental%2520problem%2520in%250Acomputer%2520vision%252C%2520with%2520applications%2520in%2520robotics%2520and%2520augmented%2520reality.%2520A%2520key%250Achallenge%2520is%2520achieving%2520generalization%2520to%2520novel%2520objects%2520that%2520were%2520not%2520seen%250Aduring%2520training.%2520Most%2520existing%2520approaches%2520address%2520this%2520by%2520scaling%2520up%2520training%250Aon%2520synthetic%2520data%2520tailored%2520to%2520the%2520task%252C%2520a%2520process%2520that%2520demands%2520substantial%250Acomputational%2520resources.%2520But%2520is%2520task-specific%2520training%2520really%2520necessary%2520for%250Aaccurate%2520and%2520efficient%25206D%2520pose%2520estimation%2520of%2520novel%2520objects%253F%2520To%2520answer%2520No%2521%252C%2520we%250Aintroduce%2520FreeZeV2%252C%2520the%2520second%2520generation%2520of%2520FreeZe%253A%2520a%2520training-free%2520method%250Athat%2520achieves%2520strong%2520generalization%2520to%2520unseen%2520objects%2520by%2520leveraging%2520geometric%250Aand%2520vision%2520foundation%2520models%2520pre-trained%2520on%2520unrelated%2520data.%2520FreeZeV2%2520improves%250Aboth%2520accuracy%2520and%2520efficiency%2520over%2520FreeZe%2520through%2520three%2520key%2520contributions%253A%2520%2528i%2529%2520a%250Asparse%2520feature%2520extraction%2520strategy%2520that%2520reduces%2520inference-time%2520computation%250Awithout%2520sacrificing%2520accuracy%253B%2520%2528ii%2529%2520a%2520feature-aware%2520scoring%2520mechanism%2520that%250Aimproves%2520both%2520pose%2520selection%2520during%2520RANSAC-based%25203D%2520registration%2520and%2520the%2520final%250Aranking%2520of%2520pose%2520candidates%253B%2520and%2520%2528iii%2529%2520a%2520modular%2520design%2520that%2520supports%2520ensembles%250Aof%2520instance%2520segmentation%2520models%252C%2520increasing%2520robustness%2520to%2520segmentation%2520masks%250Aerrors.%2520We%2520evaluate%2520FreeZeV2%2520on%2520the%2520seven%2520core%2520datasets%2520of%2520the%2520BOP%2520Benchmark%252C%250Awhere%2520it%2520establishes%2520a%2520new%2520state-of-the-art%2520in%25206D%2520pose%2520estimation%2520of%2520unseen%250Aobjects.%2520When%2520using%2520the%2520same%2520segmentation%2520masks%252C%2520FreeZeV2%2520achieves%2520a%2520remarkable%250A8x%2520speedup%2520over%2520FreeZe%2520while%2520also%2520improving%2520accuracy%2520by%25205%2525.%2520When%2520using%250Aensembles%2520of%2520segmentation%2520models%252C%2520FreeZeV2%2520gains%2520an%2520additional%25208%2525%2520in%2520accuracy%250Awhile%2520still%2520running%25202.5x%2520faster%2520than%2520FreeZe.%2520FreeZeV2%2520was%2520awarded%2520Best%2520Overall%250AMethod%2520at%2520the%2520BOP%2520Challenge%25202024.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accurate%20and%20efficient%20zero-shot%206D%20pose%20estimation%20with%20frozen%0A%20%20foundation%20models&entry.906535625=Andrea%20Caraffa%20and%20Davide%20Boscaini%20and%20Fabio%20Poiesi&entry.1292438233=%20%20Estimating%20the%206D%20pose%20of%20objects%20from%20RGBD%20data%20is%20a%20fundamental%20problem%20in%0Acomputer%20vision%2C%20with%20applications%20in%20robotics%20and%20augmented%20reality.%20A%20key%0Achallenge%20is%20achieving%20generalization%20to%20novel%20objects%20that%20were%20not%20seen%0Aduring%20training.%20Most%20existing%20approaches%20address%20this%20by%20scaling%20up%20training%0Aon%20synthetic%20data%20tailored%20to%20the%20task%2C%20a%20process%20that%20demands%20substantial%0Acomputational%20resources.%20But%20is%20task-specific%20training%20really%20necessary%20for%0Aaccurate%20and%20efficient%206D%20pose%20estimation%20of%20novel%20objects%3F%20To%20answer%20No%21%2C%20we%0Aintroduce%20FreeZeV2%2C%20the%20second%20generation%20of%20FreeZe%3A%20a%20training-free%20method%0Athat%20achieves%20strong%20generalization%20to%20unseen%20objects%20by%20leveraging%20geometric%0Aand%20vision%20foundation%20models%20pre-trained%20on%20unrelated%20data.%20FreeZeV2%20improves%0Aboth%20accuracy%20and%20efficiency%20over%20FreeZe%20through%20three%20key%20contributions%3A%20%28i%29%20a%0Asparse%20feature%20extraction%20strategy%20that%20reduces%20inference-time%20computation%0Awithout%20sacrificing%20accuracy%3B%20%28ii%29%20a%20feature-aware%20scoring%20mechanism%20that%0Aimproves%20both%20pose%20selection%20during%20RANSAC-based%203D%20registration%20and%20the%20final%0Aranking%20of%20pose%20candidates%3B%20and%20%28iii%29%20a%20modular%20design%20that%20supports%20ensembles%0Aof%20instance%20segmentation%20models%2C%20increasing%20robustness%20to%20segmentation%20masks%0Aerrors.%20We%20evaluate%20FreeZeV2%20on%20the%20seven%20core%20datasets%20of%20the%20BOP%20Benchmark%2C%0Awhere%20it%20establishes%20a%20new%20state-of-the-art%20in%206D%20pose%20estimation%20of%20unseen%0Aobjects.%20When%20using%20the%20same%20segmentation%20masks%2C%20FreeZeV2%20achieves%20a%20remarkable%0A8x%20speedup%20over%20FreeZe%20while%20also%20improving%20accuracy%20by%205%25.%20When%20using%0Aensembles%20of%20segmentation%20models%2C%20FreeZeV2%20gains%20an%20additional%208%25%20in%20accuracy%0Awhile%20still%20running%202.5x%20faster%20than%20FreeZe.%20FreeZeV2%20was%20awarded%20Best%20Overall%0AMethod%20at%20the%20BOP%20Challenge%202024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09784v1&entry.124074799=Read"},
{"title": "MetricHMR: Metric Human Mesh Recovery from Monocular Images", "author": "He Zhang and Chentao Song and Hongwen Zhang and Tao Yu", "abstract": "  We introduce MetricHMR (Metric Human Mesh Recovery), an approach for metric\nhuman mesh recovery with accurate global translation from monocular images. In\ncontrast to existing HMR methods that suffer from severe scale and depth\nambiguity, MetricHMR is able to produce geometrically reasonable body shape and\nglobal translation in the reconstruction results. To this end, we first\nsystematically analyze previous HMR methods on camera models to emphasize the\ncritical role of the standard perspective projection model in enabling\nmetric-scale HMR. We then validate the acceptable ambiguity range of metric HMR\nunder the standard perspective projection model. Finally, we contribute a novel\napproach that introduces a ray map based on the standard perspective projection\nto jointly encode bounding-box information, camera parameters, and geometric\ncues for End2End metric HMR without any additional metric-regularization\nmodules. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance, even compared with sequential HMR methods, in\nmetric pose, shape, and global translation estimation across both indoor and\nin-the-wild scenarios.\n", "link": "http://arxiv.org/abs/2506.09919v1", "date": "2025-06-11", "relevancy": 2.9256, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5998}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5985}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetricHMR%3A%20Metric%20Human%20Mesh%20Recovery%20from%20Monocular%20Images&body=Title%3A%20MetricHMR%3A%20Metric%20Human%20Mesh%20Recovery%20from%20Monocular%20Images%0AAuthor%3A%20He%20Zhang%20and%20Chentao%20Song%20and%20Hongwen%20Zhang%20and%20Tao%20Yu%0AAbstract%3A%20%20%20We%20introduce%20MetricHMR%20%28Metric%20Human%20Mesh%20Recovery%29%2C%20an%20approach%20for%20metric%0Ahuman%20mesh%20recovery%20with%20accurate%20global%20translation%20from%20monocular%20images.%20In%0Acontrast%20to%20existing%20HMR%20methods%20that%20suffer%20from%20severe%20scale%20and%20depth%0Aambiguity%2C%20MetricHMR%20is%20able%20to%20produce%20geometrically%20reasonable%20body%20shape%20and%0Aglobal%20translation%20in%20the%20reconstruction%20results.%20To%20this%20end%2C%20we%20first%0Asystematically%20analyze%20previous%20HMR%20methods%20on%20camera%20models%20to%20emphasize%20the%0Acritical%20role%20of%20the%20standard%20perspective%20projection%20model%20in%20enabling%0Ametric-scale%20HMR.%20We%20then%20validate%20the%20acceptable%20ambiguity%20range%20of%20metric%20HMR%0Aunder%20the%20standard%20perspective%20projection%20model.%20Finally%2C%20we%20contribute%20a%20novel%0Aapproach%20that%20introduces%20a%20ray%20map%20based%20on%20the%20standard%20perspective%20projection%0Ato%20jointly%20encode%20bounding-box%20information%2C%20camera%20parameters%2C%20and%20geometric%0Acues%20for%20End2End%20metric%20HMR%20without%20any%20additional%20metric-regularization%0Amodules.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%2C%20even%20compared%20with%20sequential%20HMR%20methods%2C%20in%0Ametric%20pose%2C%20shape%2C%20and%20global%20translation%20estimation%20across%20both%20indoor%20and%0Ain-the-wild%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetricHMR%253A%2520Metric%2520Human%2520Mesh%2520Recovery%2520from%2520Monocular%2520Images%26entry.906535625%3DHe%2520Zhang%2520and%2520Chentao%2520Song%2520and%2520Hongwen%2520Zhang%2520and%2520Tao%2520Yu%26entry.1292438233%3D%2520%2520We%2520introduce%2520MetricHMR%2520%2528Metric%2520Human%2520Mesh%2520Recovery%2529%252C%2520an%2520approach%2520for%2520metric%250Ahuman%2520mesh%2520recovery%2520with%2520accurate%2520global%2520translation%2520from%2520monocular%2520images.%2520In%250Acontrast%2520to%2520existing%2520HMR%2520methods%2520that%2520suffer%2520from%2520severe%2520scale%2520and%2520depth%250Aambiguity%252C%2520MetricHMR%2520is%2520able%2520to%2520produce%2520geometrically%2520reasonable%2520body%2520shape%2520and%250Aglobal%2520translation%2520in%2520the%2520reconstruction%2520results.%2520To%2520this%2520end%252C%2520we%2520first%250Asystematically%2520analyze%2520previous%2520HMR%2520methods%2520on%2520camera%2520models%2520to%2520emphasize%2520the%250Acritical%2520role%2520of%2520the%2520standard%2520perspective%2520projection%2520model%2520in%2520enabling%250Ametric-scale%2520HMR.%2520We%2520then%2520validate%2520the%2520acceptable%2520ambiguity%2520range%2520of%2520metric%2520HMR%250Aunder%2520the%2520standard%2520perspective%2520projection%2520model.%2520Finally%252C%2520we%2520contribute%2520a%2520novel%250Aapproach%2520that%2520introduces%2520a%2520ray%2520map%2520based%2520on%2520the%2520standard%2520perspective%2520projection%250Ato%2520jointly%2520encode%2520bounding-box%2520information%252C%2520camera%2520parameters%252C%2520and%2520geometric%250Acues%2520for%2520End2End%2520metric%2520HMR%2520without%2520any%2520additional%2520metric-regularization%250Amodules.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520performance%252C%2520even%2520compared%2520with%2520sequential%2520HMR%2520methods%252C%2520in%250Ametric%2520pose%252C%2520shape%252C%2520and%2520global%2520translation%2520estimation%2520across%2520both%2520indoor%2520and%250Ain-the-wild%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetricHMR%3A%20Metric%20Human%20Mesh%20Recovery%20from%20Monocular%20Images&entry.906535625=He%20Zhang%20and%20Chentao%20Song%20and%20Hongwen%20Zhang%20and%20Tao%20Yu&entry.1292438233=%20%20We%20introduce%20MetricHMR%20%28Metric%20Human%20Mesh%20Recovery%29%2C%20an%20approach%20for%20metric%0Ahuman%20mesh%20recovery%20with%20accurate%20global%20translation%20from%20monocular%20images.%20In%0Acontrast%20to%20existing%20HMR%20methods%20that%20suffer%20from%20severe%20scale%20and%20depth%0Aambiguity%2C%20MetricHMR%20is%20able%20to%20produce%20geometrically%20reasonable%20body%20shape%20and%0Aglobal%20translation%20in%20the%20reconstruction%20results.%20To%20this%20end%2C%20we%20first%0Asystematically%20analyze%20previous%20HMR%20methods%20on%20camera%20models%20to%20emphasize%20the%0Acritical%20role%20of%20the%20standard%20perspective%20projection%20model%20in%20enabling%0Ametric-scale%20HMR.%20We%20then%20validate%20the%20acceptable%20ambiguity%20range%20of%20metric%20HMR%0Aunder%20the%20standard%20perspective%20projection%20model.%20Finally%2C%20we%20contribute%20a%20novel%0Aapproach%20that%20introduces%20a%20ray%20map%20based%20on%20the%20standard%20perspective%20projection%0Ato%20jointly%20encode%20bounding-box%20information%2C%20camera%20parameters%2C%20and%20geometric%0Acues%20for%20End2End%20metric%20HMR%20without%20any%20additional%20metric-regularization%0Amodules.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%2C%20even%20compared%20with%20sequential%20HMR%20methods%2C%20in%0Ametric%20pose%2C%20shape%2C%20and%20global%20translation%20estimation%20across%20both%20indoor%20and%0Ain-the-wild%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09919v1&entry.124074799=Read"},
{"title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven\n  Thinking and Visual Drawing", "author": "Junfei Wu and Jian Guan and Kaituo Feng and Qiang Liu and Shu Wu and Liang Wang and Wei Wu and Tieniu Tan", "abstract": "  As textual reasoning with large language models (LLMs) has advanced\nsignificantly, there has been growing interest in enhancing the multimodal\nreasoning capabilities of large vision-language models (LVLMs). However,\nexisting methods primarily approach multimodal reasoning in a straightforward,\ntext-centric manner, where both reasoning and answer derivation are conducted\npurely through text, with the only difference being the presence of multimodal\ninput. As a result, these methods often encounter fundamental limitations in\nspatial reasoning tasks that demand precise geometric understanding and\ncontinuous spatial tracking-capabilities that humans achieve through mental\nvisualization and manipulation. To address the limitations, we propose drawing\nto reason in space, a novel paradigm that enables LVLMs to reason through\nelementary drawing operations in the visual space. By equipping models with\nbasic drawing operations, including annotating bounding boxes and drawing\nauxiliary lines, we empower them to express and analyze spatial relationships\nthrough direct visual manipulation, meanwhile avoiding the performance ceiling\nimposed by specialized perception tools in previous tool-integrated reasoning\napproaches. To cultivate this capability, we develop a three-stage training\nframework: cold-start training with synthetic data to establish basic drawing\nabilities, reflective rejection sampling to enhance self-reflection behaviors,\nand reinforcement learning to directly optimize for target rewards. Extensive\nexperiments demonstrate that our model, named VILASR, consistently outperforms\nexisting methods across diverse spatial reasoning benchmarks, involving maze\nnavigation, static spatial reasoning, video-based reasoning, and\nmulti-view-based reasoning tasks, with an average improvement of 18.4%.\n", "link": "http://arxiv.org/abs/2506.09965v1", "date": "2025-06-11", "relevancy": 2.9015, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5957}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5957}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcing%20Spatial%20Reasoning%20in%20Vision-Language%20Models%20with%20Interwoven%0A%20%20Thinking%20and%20Visual%20Drawing&body=Title%3A%20Reinforcing%20Spatial%20Reasoning%20in%20Vision-Language%20Models%20with%20Interwoven%0A%20%20Thinking%20and%20Visual%20Drawing%0AAuthor%3A%20Junfei%20Wu%20and%20Jian%20Guan%20and%20Kaituo%20Feng%20and%20Qiang%20Liu%20and%20Shu%20Wu%20and%20Liang%20Wang%20and%20Wei%20Wu%20and%20Tieniu%20Tan%0AAbstract%3A%20%20%20As%20textual%20reasoning%20with%20large%20language%20models%20%28LLMs%29%20has%20advanced%0Asignificantly%2C%20there%20has%20been%20growing%20interest%20in%20enhancing%20the%20multimodal%0Areasoning%20capabilities%20of%20large%20vision-language%20models%20%28LVLMs%29.%20However%2C%0Aexisting%20methods%20primarily%20approach%20multimodal%20reasoning%20in%20a%20straightforward%2C%0Atext-centric%20manner%2C%20where%20both%20reasoning%20and%20answer%20derivation%20are%20conducted%0Apurely%20through%20text%2C%20with%20the%20only%20difference%20being%20the%20presence%20of%20multimodal%0Ainput.%20As%20a%20result%2C%20these%20methods%20often%20encounter%20fundamental%20limitations%20in%0Aspatial%20reasoning%20tasks%20that%20demand%20precise%20geometric%20understanding%20and%0Acontinuous%20spatial%20tracking-capabilities%20that%20humans%20achieve%20through%20mental%0Avisualization%20and%20manipulation.%20To%20address%20the%20limitations%2C%20we%20propose%20drawing%0Ato%20reason%20in%20space%2C%20a%20novel%20paradigm%20that%20enables%20LVLMs%20to%20reason%20through%0Aelementary%20drawing%20operations%20in%20the%20visual%20space.%20By%20equipping%20models%20with%0Abasic%20drawing%20operations%2C%20including%20annotating%20bounding%20boxes%20and%20drawing%0Aauxiliary%20lines%2C%20we%20empower%20them%20to%20express%20and%20analyze%20spatial%20relationships%0Athrough%20direct%20visual%20manipulation%2C%20meanwhile%20avoiding%20the%20performance%20ceiling%0Aimposed%20by%20specialized%20perception%20tools%20in%20previous%20tool-integrated%20reasoning%0Aapproaches.%20To%20cultivate%20this%20capability%2C%20we%20develop%20a%20three-stage%20training%0Aframework%3A%20cold-start%20training%20with%20synthetic%20data%20to%20establish%20basic%20drawing%0Aabilities%2C%20reflective%20rejection%20sampling%20to%20enhance%20self-reflection%20behaviors%2C%0Aand%20reinforcement%20learning%20to%20directly%20optimize%20for%20target%20rewards.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20model%2C%20named%20VILASR%2C%20consistently%20outperforms%0Aexisting%20methods%20across%20diverse%20spatial%20reasoning%20benchmarks%2C%20involving%20maze%0Anavigation%2C%20static%20spatial%20reasoning%2C%20video-based%20reasoning%2C%20and%0Amulti-view-based%20reasoning%20tasks%2C%20with%20an%20average%20improvement%20of%2018.4%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09965v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcing%2520Spatial%2520Reasoning%2520in%2520Vision-Language%2520Models%2520with%2520Interwoven%250A%2520%2520Thinking%2520and%2520Visual%2520Drawing%26entry.906535625%3DJunfei%2520Wu%2520and%2520Jian%2520Guan%2520and%2520Kaituo%2520Feng%2520and%2520Qiang%2520Liu%2520and%2520Shu%2520Wu%2520and%2520Liang%2520Wang%2520and%2520Wei%2520Wu%2520and%2520Tieniu%2520Tan%26entry.1292438233%3D%2520%2520As%2520textual%2520reasoning%2520with%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520advanced%250Asignificantly%252C%2520there%2520has%2520been%2520growing%2520interest%2520in%2520enhancing%2520the%2520multimodal%250Areasoning%2520capabilities%2520of%2520large%2520vision-language%2520models%2520%2528LVLMs%2529.%2520However%252C%250Aexisting%2520methods%2520primarily%2520approach%2520multimodal%2520reasoning%2520in%2520a%2520straightforward%252C%250Atext-centric%2520manner%252C%2520where%2520both%2520reasoning%2520and%2520answer%2520derivation%2520are%2520conducted%250Apurely%2520through%2520text%252C%2520with%2520the%2520only%2520difference%2520being%2520the%2520presence%2520of%2520multimodal%250Ainput.%2520As%2520a%2520result%252C%2520these%2520methods%2520often%2520encounter%2520fundamental%2520limitations%2520in%250Aspatial%2520reasoning%2520tasks%2520that%2520demand%2520precise%2520geometric%2520understanding%2520and%250Acontinuous%2520spatial%2520tracking-capabilities%2520that%2520humans%2520achieve%2520through%2520mental%250Avisualization%2520and%2520manipulation.%2520To%2520address%2520the%2520limitations%252C%2520we%2520propose%2520drawing%250Ato%2520reason%2520in%2520space%252C%2520a%2520novel%2520paradigm%2520that%2520enables%2520LVLMs%2520to%2520reason%2520through%250Aelementary%2520drawing%2520operations%2520in%2520the%2520visual%2520space.%2520By%2520equipping%2520models%2520with%250Abasic%2520drawing%2520operations%252C%2520including%2520annotating%2520bounding%2520boxes%2520and%2520drawing%250Aauxiliary%2520lines%252C%2520we%2520empower%2520them%2520to%2520express%2520and%2520analyze%2520spatial%2520relationships%250Athrough%2520direct%2520visual%2520manipulation%252C%2520meanwhile%2520avoiding%2520the%2520performance%2520ceiling%250Aimposed%2520by%2520specialized%2520perception%2520tools%2520in%2520previous%2520tool-integrated%2520reasoning%250Aapproaches.%2520To%2520cultivate%2520this%2520capability%252C%2520we%2520develop%2520a%2520three-stage%2520training%250Aframework%253A%2520cold-start%2520training%2520with%2520synthetic%2520data%2520to%2520establish%2520basic%2520drawing%250Aabilities%252C%2520reflective%2520rejection%2520sampling%2520to%2520enhance%2520self-reflection%2520behaviors%252C%250Aand%2520reinforcement%2520learning%2520to%2520directly%2520optimize%2520for%2520target%2520rewards.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520model%252C%2520named%2520VILASR%252C%2520consistently%2520outperforms%250Aexisting%2520methods%2520across%2520diverse%2520spatial%2520reasoning%2520benchmarks%252C%2520involving%2520maze%250Anavigation%252C%2520static%2520spatial%2520reasoning%252C%2520video-based%2520reasoning%252C%2520and%250Amulti-view-based%2520reasoning%2520tasks%252C%2520with%2520an%2520average%2520improvement%2520of%252018.4%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09965v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcing%20Spatial%20Reasoning%20in%20Vision-Language%20Models%20with%20Interwoven%0A%20%20Thinking%20and%20Visual%20Drawing&entry.906535625=Junfei%20Wu%20and%20Jian%20Guan%20and%20Kaituo%20Feng%20and%20Qiang%20Liu%20and%20Shu%20Wu%20and%20Liang%20Wang%20and%20Wei%20Wu%20and%20Tieniu%20Tan&entry.1292438233=%20%20As%20textual%20reasoning%20with%20large%20language%20models%20%28LLMs%29%20has%20advanced%0Asignificantly%2C%20there%20has%20been%20growing%20interest%20in%20enhancing%20the%20multimodal%0Areasoning%20capabilities%20of%20large%20vision-language%20models%20%28LVLMs%29.%20However%2C%0Aexisting%20methods%20primarily%20approach%20multimodal%20reasoning%20in%20a%20straightforward%2C%0Atext-centric%20manner%2C%20where%20both%20reasoning%20and%20answer%20derivation%20are%20conducted%0Apurely%20through%20text%2C%20with%20the%20only%20difference%20being%20the%20presence%20of%20multimodal%0Ainput.%20As%20a%20result%2C%20these%20methods%20often%20encounter%20fundamental%20limitations%20in%0Aspatial%20reasoning%20tasks%20that%20demand%20precise%20geometric%20understanding%20and%0Acontinuous%20spatial%20tracking-capabilities%20that%20humans%20achieve%20through%20mental%0Avisualization%20and%20manipulation.%20To%20address%20the%20limitations%2C%20we%20propose%20drawing%0Ato%20reason%20in%20space%2C%20a%20novel%20paradigm%20that%20enables%20LVLMs%20to%20reason%20through%0Aelementary%20drawing%20operations%20in%20the%20visual%20space.%20By%20equipping%20models%20with%0Abasic%20drawing%20operations%2C%20including%20annotating%20bounding%20boxes%20and%20drawing%0Aauxiliary%20lines%2C%20we%20empower%20them%20to%20express%20and%20analyze%20spatial%20relationships%0Athrough%20direct%20visual%20manipulation%2C%20meanwhile%20avoiding%20the%20performance%20ceiling%0Aimposed%20by%20specialized%20perception%20tools%20in%20previous%20tool-integrated%20reasoning%0Aapproaches.%20To%20cultivate%20this%20capability%2C%20we%20develop%20a%20three-stage%20training%0Aframework%3A%20cold-start%20training%20with%20synthetic%20data%20to%20establish%20basic%20drawing%0Aabilities%2C%20reflective%20rejection%20sampling%20to%20enhance%20self-reflection%20behaviors%2C%0Aand%20reinforcement%20learning%20to%20directly%20optimize%20for%20target%20rewards.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20model%2C%20named%20VILASR%2C%20consistently%20outperforms%0Aexisting%20methods%20across%20diverse%20spatial%20reasoning%20benchmarks%2C%20involving%20maze%0Anavigation%2C%20static%20spatial%20reasoning%2C%20video-based%20reasoning%2C%20and%0Amulti-view-based%20reasoning%20tasks%2C%20with%20an%20average%20improvement%20of%2018.4%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09965v1&entry.124074799=Read"},
{"title": "Improving Handwritten Text Recognition via 3D Attention and Multi-Scale\n  Training", "author": "Zi-Rui Wang", "abstract": "  The segmentation-free research efforts for addressing handwritten text\nrecognition can be divided into three categories: connectionist temporal\nclassification (CTC), hidden Markov model and encoder-decoder methods. In this\npaper, inspired by the above three modeling methods, we propose a new\nrecognition network by using a novel three-dimensional (3D) attention module\nand global-local context information. Based on the feature maps of the last\nconvolutional layer, a series of 3D blocks with different resolutions are\nsplit. Then, these 3D blocks are fed into the 3D attention module to generate\nsequential visual features. Finally, by integrating the visual features and the\ncorresponding global-local context features, a well-designed representation can\nbe obtained. Main canonical neural units including attention mechanisms,\nfully-connected layer, recurrent unit and convolutional layer are efficiently\norganized into a network and can be jointly trained by the CTC loss and the\ncross-entropy loss. Experiments on the latest Chinese handwritten text datasets\n(the SCUT-HCCDoc and the SCUT-EPT) and one English handwritten text dataset\n(the IAM) show that the proposed method can achieve comparable results with the\nstate-of-the-art methods. The code is available at\nhttps://github.com/Wukong90/3DAttention-MultiScaleTraining-for-HTR.\n", "link": "http://arxiv.org/abs/2410.18374v2", "date": "2025-06-11", "relevancy": 2.8972, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5903}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5882}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Handwritten%20Text%20Recognition%20via%203D%20Attention%20and%20Multi-Scale%0A%20%20Training&body=Title%3A%20Improving%20Handwritten%20Text%20Recognition%20via%203D%20Attention%20and%20Multi-Scale%0A%20%20Training%0AAuthor%3A%20Zi-Rui%20Wang%0AAbstract%3A%20%20%20The%20segmentation-free%20research%20efforts%20for%20addressing%20handwritten%20text%0Arecognition%20can%20be%20divided%20into%20three%20categories%3A%20connectionist%20temporal%0Aclassification%20%28CTC%29%2C%20hidden%20Markov%20model%20and%20encoder-decoder%20methods.%20In%20this%0Apaper%2C%20inspired%20by%20the%20above%20three%20modeling%20methods%2C%20we%20propose%20a%20new%0Arecognition%20network%20by%20using%20a%20novel%20three-dimensional%20%283D%29%20attention%20module%0Aand%20global-local%20context%20information.%20Based%20on%20the%20feature%20maps%20of%20the%20last%0Aconvolutional%20layer%2C%20a%20series%20of%203D%20blocks%20with%20different%20resolutions%20are%0Asplit.%20Then%2C%20these%203D%20blocks%20are%20fed%20into%20the%203D%20attention%20module%20to%20generate%0Asequential%20visual%20features.%20Finally%2C%20by%20integrating%20the%20visual%20features%20and%20the%0Acorresponding%20global-local%20context%20features%2C%20a%20well-designed%20representation%20can%0Abe%20obtained.%20Main%20canonical%20neural%20units%20including%20attention%20mechanisms%2C%0Afully-connected%20layer%2C%20recurrent%20unit%20and%20convolutional%20layer%20are%20efficiently%0Aorganized%20into%20a%20network%20and%20can%20be%20jointly%20trained%20by%20the%20CTC%20loss%20and%20the%0Across-entropy%20loss.%20Experiments%20on%20the%20latest%20Chinese%20handwritten%20text%20datasets%0A%28the%20SCUT-HCCDoc%20and%20the%20SCUT-EPT%29%20and%20one%20English%20handwritten%20text%20dataset%0A%28the%20IAM%29%20show%20that%20the%20proposed%20method%20can%20achieve%20comparable%20results%20with%20the%0Astate-of-the-art%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Wukong90/3DAttention-MultiScaleTraining-for-HTR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18374v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Handwritten%2520Text%2520Recognition%2520via%25203D%2520Attention%2520and%2520Multi-Scale%250A%2520%2520Training%26entry.906535625%3DZi-Rui%2520Wang%26entry.1292438233%3D%2520%2520The%2520segmentation-free%2520research%2520efforts%2520for%2520addressing%2520handwritten%2520text%250Arecognition%2520can%2520be%2520divided%2520into%2520three%2520categories%253A%2520connectionist%2520temporal%250Aclassification%2520%2528CTC%2529%252C%2520hidden%2520Markov%2520model%2520and%2520encoder-decoder%2520methods.%2520In%2520this%250Apaper%252C%2520inspired%2520by%2520the%2520above%2520three%2520modeling%2520methods%252C%2520we%2520propose%2520a%2520new%250Arecognition%2520network%2520by%2520using%2520a%2520novel%2520three-dimensional%2520%25283D%2529%2520attention%2520module%250Aand%2520global-local%2520context%2520information.%2520Based%2520on%2520the%2520feature%2520maps%2520of%2520the%2520last%250Aconvolutional%2520layer%252C%2520a%2520series%2520of%25203D%2520blocks%2520with%2520different%2520resolutions%2520are%250Asplit.%2520Then%252C%2520these%25203D%2520blocks%2520are%2520fed%2520into%2520the%25203D%2520attention%2520module%2520to%2520generate%250Asequential%2520visual%2520features.%2520Finally%252C%2520by%2520integrating%2520the%2520visual%2520features%2520and%2520the%250Acorresponding%2520global-local%2520context%2520features%252C%2520a%2520well-designed%2520representation%2520can%250Abe%2520obtained.%2520Main%2520canonical%2520neural%2520units%2520including%2520attention%2520mechanisms%252C%250Afully-connected%2520layer%252C%2520recurrent%2520unit%2520and%2520convolutional%2520layer%2520are%2520efficiently%250Aorganized%2520into%2520a%2520network%2520and%2520can%2520be%2520jointly%2520trained%2520by%2520the%2520CTC%2520loss%2520and%2520the%250Across-entropy%2520loss.%2520Experiments%2520on%2520the%2520latest%2520Chinese%2520handwritten%2520text%2520datasets%250A%2528the%2520SCUT-HCCDoc%2520and%2520the%2520SCUT-EPT%2529%2520and%2520one%2520English%2520handwritten%2520text%2520dataset%250A%2528the%2520IAM%2529%2520show%2520that%2520the%2520proposed%2520method%2520can%2520achieve%2520comparable%2520results%2520with%2520the%250Astate-of-the-art%2520methods.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Wukong90/3DAttention-MultiScaleTraining-for-HTR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18374v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Handwritten%20Text%20Recognition%20via%203D%20Attention%20and%20Multi-Scale%0A%20%20Training&entry.906535625=Zi-Rui%20Wang&entry.1292438233=%20%20The%20segmentation-free%20research%20efforts%20for%20addressing%20handwritten%20text%0Arecognition%20can%20be%20divided%20into%20three%20categories%3A%20connectionist%20temporal%0Aclassification%20%28CTC%29%2C%20hidden%20Markov%20model%20and%20encoder-decoder%20methods.%20In%20this%0Apaper%2C%20inspired%20by%20the%20above%20three%20modeling%20methods%2C%20we%20propose%20a%20new%0Arecognition%20network%20by%20using%20a%20novel%20three-dimensional%20%283D%29%20attention%20module%0Aand%20global-local%20context%20information.%20Based%20on%20the%20feature%20maps%20of%20the%20last%0Aconvolutional%20layer%2C%20a%20series%20of%203D%20blocks%20with%20different%20resolutions%20are%0Asplit.%20Then%2C%20these%203D%20blocks%20are%20fed%20into%20the%203D%20attention%20module%20to%20generate%0Asequential%20visual%20features.%20Finally%2C%20by%20integrating%20the%20visual%20features%20and%20the%0Acorresponding%20global-local%20context%20features%2C%20a%20well-designed%20representation%20can%0Abe%20obtained.%20Main%20canonical%20neural%20units%20including%20attention%20mechanisms%2C%0Afully-connected%20layer%2C%20recurrent%20unit%20and%20convolutional%20layer%20are%20efficiently%0Aorganized%20into%20a%20network%20and%20can%20be%20jointly%20trained%20by%20the%20CTC%20loss%20and%20the%0Across-entropy%20loss.%20Experiments%20on%20the%20latest%20Chinese%20handwritten%20text%20datasets%0A%28the%20SCUT-HCCDoc%20and%20the%20SCUT-EPT%29%20and%20one%20English%20handwritten%20text%20dataset%0A%28the%20IAM%29%20show%20that%20the%20proposed%20method%20can%20achieve%20comparable%20results%20with%20the%0Astate-of-the-art%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Wukong90/3DAttention-MultiScaleTraining-for-HTR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18374v2&entry.124074799=Read"},
{"title": "DreamCS: Geometry-Aware Text-to-3D Generation with Unpaired 3D Reward\n  Supervision", "author": "Xiandong Zou and Ruihao Xia and Hongsong Wang and Pan Zhou", "abstract": "  While text-to-3D generation has attracted growing interest, existing methods\noften struggle to produce 3D assets that align well with human preferences.\nCurrent preference alignment techniques for 3D content typically rely on\nhardly-collected preference-paired multi-view 2D images to train 2D reward\nmodels, when then guide 3D generation -- leading to geometric artifacts due to\ntheir inherent 2D bias. To address these limitations, we construct 3D-MeshPref,\nthe first large-scale unpaired 3D preference dataset, featuring diverse 3D\nmeshes annotated by a large language model and refined by human evaluators. We\nthen develop RewardCS, the first reward model trained directly on unpaired\n3D-MeshPref data using a novel Cauchy-Schwarz divergence objective, enabling\neffective learning of human-aligned 3D geometric preferences without requiring\npaired comparisons. Building on this, we propose DreamCS, a unified framework\nthat integrates RewardCS into text-to-3D pipelines -- enhancing both implicit\nand explicit 3D generation with human preference feedback. Extensive\nexperiments show DreamCS outperforms prior methods, producing 3D assets that\nare both geometrically faithful and human-preferred. Code and models will be\nreleased publicly.\n", "link": "http://arxiv.org/abs/2506.09814v1", "date": "2025-06-11", "relevancy": 2.888, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5838}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.575}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamCS%3A%20Geometry-Aware%20Text-to-3D%20Generation%20with%20Unpaired%203D%20Reward%0A%20%20Supervision&body=Title%3A%20DreamCS%3A%20Geometry-Aware%20Text-to-3D%20Generation%20with%20Unpaired%203D%20Reward%0A%20%20Supervision%0AAuthor%3A%20Xiandong%20Zou%20and%20Ruihao%20Xia%20and%20Hongsong%20Wang%20and%20Pan%20Zhou%0AAbstract%3A%20%20%20While%20text-to-3D%20generation%20has%20attracted%20growing%20interest%2C%20existing%20methods%0Aoften%20struggle%20to%20produce%203D%20assets%20that%20align%20well%20with%20human%20preferences.%0ACurrent%20preference%20alignment%20techniques%20for%203D%20content%20typically%20rely%20on%0Ahardly-collected%20preference-paired%20multi-view%202D%20images%20to%20train%202D%20reward%0Amodels%2C%20when%20then%20guide%203D%20generation%20--%20leading%20to%20geometric%20artifacts%20due%20to%0Atheir%20inherent%202D%20bias.%20To%20address%20these%20limitations%2C%20we%20construct%203D-MeshPref%2C%0Athe%20first%20large-scale%20unpaired%203D%20preference%20dataset%2C%20featuring%20diverse%203D%0Ameshes%20annotated%20by%20a%20large%20language%20model%20and%20refined%20by%20human%20evaluators.%20We%0Athen%20develop%20RewardCS%2C%20the%20first%20reward%20model%20trained%20directly%20on%20unpaired%0A3D-MeshPref%20data%20using%20a%20novel%20Cauchy-Schwarz%20divergence%20objective%2C%20enabling%0Aeffective%20learning%20of%20human-aligned%203D%20geometric%20preferences%20without%20requiring%0Apaired%20comparisons.%20Building%20on%20this%2C%20we%20propose%20DreamCS%2C%20a%20unified%20framework%0Athat%20integrates%20RewardCS%20into%20text-to-3D%20pipelines%20--%20enhancing%20both%20implicit%0Aand%20explicit%203D%20generation%20with%20human%20preference%20feedback.%20Extensive%0Aexperiments%20show%20DreamCS%20outperforms%20prior%20methods%2C%20producing%203D%20assets%20that%0Aare%20both%20geometrically%20faithful%20and%20human-preferred.%20Code%20and%20models%20will%20be%0Areleased%20publicly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamCS%253A%2520Geometry-Aware%2520Text-to-3D%2520Generation%2520with%2520Unpaired%25203D%2520Reward%250A%2520%2520Supervision%26entry.906535625%3DXiandong%2520Zou%2520and%2520Ruihao%2520Xia%2520and%2520Hongsong%2520Wang%2520and%2520Pan%2520Zhou%26entry.1292438233%3D%2520%2520While%2520text-to-3D%2520generation%2520has%2520attracted%2520growing%2520interest%252C%2520existing%2520methods%250Aoften%2520struggle%2520to%2520produce%25203D%2520assets%2520that%2520align%2520well%2520with%2520human%2520preferences.%250ACurrent%2520preference%2520alignment%2520techniques%2520for%25203D%2520content%2520typically%2520rely%2520on%250Ahardly-collected%2520preference-paired%2520multi-view%25202D%2520images%2520to%2520train%25202D%2520reward%250Amodels%252C%2520when%2520then%2520guide%25203D%2520generation%2520--%2520leading%2520to%2520geometric%2520artifacts%2520due%2520to%250Atheir%2520inherent%25202D%2520bias.%2520To%2520address%2520these%2520limitations%252C%2520we%2520construct%25203D-MeshPref%252C%250Athe%2520first%2520large-scale%2520unpaired%25203D%2520preference%2520dataset%252C%2520featuring%2520diverse%25203D%250Ameshes%2520annotated%2520by%2520a%2520large%2520language%2520model%2520and%2520refined%2520by%2520human%2520evaluators.%2520We%250Athen%2520develop%2520RewardCS%252C%2520the%2520first%2520reward%2520model%2520trained%2520directly%2520on%2520unpaired%250A3D-MeshPref%2520data%2520using%2520a%2520novel%2520Cauchy-Schwarz%2520divergence%2520objective%252C%2520enabling%250Aeffective%2520learning%2520of%2520human-aligned%25203D%2520geometric%2520preferences%2520without%2520requiring%250Apaired%2520comparisons.%2520Building%2520on%2520this%252C%2520we%2520propose%2520DreamCS%252C%2520a%2520unified%2520framework%250Athat%2520integrates%2520RewardCS%2520into%2520text-to-3D%2520pipelines%2520--%2520enhancing%2520both%2520implicit%250Aand%2520explicit%25203D%2520generation%2520with%2520human%2520preference%2520feedback.%2520Extensive%250Aexperiments%2520show%2520DreamCS%2520outperforms%2520prior%2520methods%252C%2520producing%25203D%2520assets%2520that%250Aare%2520both%2520geometrically%2520faithful%2520and%2520human-preferred.%2520Code%2520and%2520models%2520will%2520be%250Areleased%2520publicly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamCS%3A%20Geometry-Aware%20Text-to-3D%20Generation%20with%20Unpaired%203D%20Reward%0A%20%20Supervision&entry.906535625=Xiandong%20Zou%20and%20Ruihao%20Xia%20and%20Hongsong%20Wang%20and%20Pan%20Zhou&entry.1292438233=%20%20While%20text-to-3D%20generation%20has%20attracted%20growing%20interest%2C%20existing%20methods%0Aoften%20struggle%20to%20produce%203D%20assets%20that%20align%20well%20with%20human%20preferences.%0ACurrent%20preference%20alignment%20techniques%20for%203D%20content%20typically%20rely%20on%0Ahardly-collected%20preference-paired%20multi-view%202D%20images%20to%20train%202D%20reward%0Amodels%2C%20when%20then%20guide%203D%20generation%20--%20leading%20to%20geometric%20artifacts%20due%20to%0Atheir%20inherent%202D%20bias.%20To%20address%20these%20limitations%2C%20we%20construct%203D-MeshPref%2C%0Athe%20first%20large-scale%20unpaired%203D%20preference%20dataset%2C%20featuring%20diverse%203D%0Ameshes%20annotated%20by%20a%20large%20language%20model%20and%20refined%20by%20human%20evaluators.%20We%0Athen%20develop%20RewardCS%2C%20the%20first%20reward%20model%20trained%20directly%20on%20unpaired%0A3D-MeshPref%20data%20using%20a%20novel%20Cauchy-Schwarz%20divergence%20objective%2C%20enabling%0Aeffective%20learning%20of%20human-aligned%203D%20geometric%20preferences%20without%20requiring%0Apaired%20comparisons.%20Building%20on%20this%2C%20we%20propose%20DreamCS%2C%20a%20unified%20framework%0Athat%20integrates%20RewardCS%20into%20text-to-3D%20pipelines%20--%20enhancing%20both%20implicit%0Aand%20explicit%203D%20generation%20with%20human%20preference%20feedback.%20Extensive%0Aexperiments%20show%20DreamCS%20outperforms%20prior%20methods%2C%20producing%203D%20assets%20that%0Aare%20both%20geometrically%20faithful%20and%20human-preferred.%20Code%20and%20models%20will%20be%0Areleased%20publicly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09814v1&entry.124074799=Read"},
{"title": "Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes", "author": "Yiming Dou and Wonseok Oh and Yuqing Luo and Antonio Loquercio and Andrew Owens", "abstract": "  We study the problem of making 3D scene reconstructions interactive by asking\nthe following question: can we predict the sounds of human hands physically\ninteracting with a scene? First, we record a video of a human manipulating\nobjects within a 3D scene using their hands. We then use these action-sound\npairs to train a rectified flow model to map 3D hand trajectories to their\ncorresponding audio. At test time, a user can query the model for other\nactions, parameterized as sequences of hand poses, to estimate their\ncorresponding sounds. In our experiments, we find that our generated sounds\naccurately convey material properties and actions, and that they are often\nindistinguishable to human observers from real sounds. Project page:\nhttps://www.yimingdou.com/hearing_hands/\n", "link": "http://arxiv.org/abs/2506.09989v1", "date": "2025-06-11", "relevancy": 2.8487, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6056}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5696}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hearing%20Hands%3A%20Generating%20Sounds%20from%20Physical%20Interactions%20in%203D%20Scenes&body=Title%3A%20Hearing%20Hands%3A%20Generating%20Sounds%20from%20Physical%20Interactions%20in%203D%20Scenes%0AAuthor%3A%20Yiming%20Dou%20and%20Wonseok%20Oh%20and%20Yuqing%20Luo%20and%20Antonio%20Loquercio%20and%20Andrew%20Owens%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20making%203D%20scene%20reconstructions%20interactive%20by%20asking%0Athe%20following%20question%3A%20can%20we%20predict%20the%20sounds%20of%20human%20hands%20physically%0Ainteracting%20with%20a%20scene%3F%20First%2C%20we%20record%20a%20video%20of%20a%20human%20manipulating%0Aobjects%20within%20a%203D%20scene%20using%20their%20hands.%20We%20then%20use%20these%20action-sound%0Apairs%20to%20train%20a%20rectified%20flow%20model%20to%20map%203D%20hand%20trajectories%20to%20their%0Acorresponding%20audio.%20At%20test%20time%2C%20a%20user%20can%20query%20the%20model%20for%20other%0Aactions%2C%20parameterized%20as%20sequences%20of%20hand%20poses%2C%20to%20estimate%20their%0Acorresponding%20sounds.%20In%20our%20experiments%2C%20we%20find%20that%20our%20generated%20sounds%0Aaccurately%20convey%20material%20properties%20and%20actions%2C%20and%20that%20they%20are%20often%0Aindistinguishable%20to%20human%20observers%20from%20real%20sounds.%20Project%20page%3A%0Ahttps%3A//www.yimingdou.com/hearing_hands/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09989v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHearing%2520Hands%253A%2520Generating%2520Sounds%2520from%2520Physical%2520Interactions%2520in%25203D%2520Scenes%26entry.906535625%3DYiming%2520Dou%2520and%2520Wonseok%2520Oh%2520and%2520Yuqing%2520Luo%2520and%2520Antonio%2520Loquercio%2520and%2520Andrew%2520Owens%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520making%25203D%2520scene%2520reconstructions%2520interactive%2520by%2520asking%250Athe%2520following%2520question%253A%2520can%2520we%2520predict%2520the%2520sounds%2520of%2520human%2520hands%2520physically%250Ainteracting%2520with%2520a%2520scene%253F%2520First%252C%2520we%2520record%2520a%2520video%2520of%2520a%2520human%2520manipulating%250Aobjects%2520within%2520a%25203D%2520scene%2520using%2520their%2520hands.%2520We%2520then%2520use%2520these%2520action-sound%250Apairs%2520to%2520train%2520a%2520rectified%2520flow%2520model%2520to%2520map%25203D%2520hand%2520trajectories%2520to%2520their%250Acorresponding%2520audio.%2520At%2520test%2520time%252C%2520a%2520user%2520can%2520query%2520the%2520model%2520for%2520other%250Aactions%252C%2520parameterized%2520as%2520sequences%2520of%2520hand%2520poses%252C%2520to%2520estimate%2520their%250Acorresponding%2520sounds.%2520In%2520our%2520experiments%252C%2520we%2520find%2520that%2520our%2520generated%2520sounds%250Aaccurately%2520convey%2520material%2520properties%2520and%2520actions%252C%2520and%2520that%2520they%2520are%2520often%250Aindistinguishable%2520to%2520human%2520observers%2520from%2520real%2520sounds.%2520Project%2520page%253A%250Ahttps%253A//www.yimingdou.com/hearing_hands/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09989v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hearing%20Hands%3A%20Generating%20Sounds%20from%20Physical%20Interactions%20in%203D%20Scenes&entry.906535625=Yiming%20Dou%20and%20Wonseok%20Oh%20and%20Yuqing%20Luo%20and%20Antonio%20Loquercio%20and%20Andrew%20Owens&entry.1292438233=%20%20We%20study%20the%20problem%20of%20making%203D%20scene%20reconstructions%20interactive%20by%20asking%0Athe%20following%20question%3A%20can%20we%20predict%20the%20sounds%20of%20human%20hands%20physically%0Ainteracting%20with%20a%20scene%3F%20First%2C%20we%20record%20a%20video%20of%20a%20human%20manipulating%0Aobjects%20within%20a%203D%20scene%20using%20their%20hands.%20We%20then%20use%20these%20action-sound%0Apairs%20to%20train%20a%20rectified%20flow%20model%20to%20map%203D%20hand%20trajectories%20to%20their%0Acorresponding%20audio.%20At%20test%20time%2C%20a%20user%20can%20query%20the%20model%20for%20other%0Aactions%2C%20parameterized%20as%20sequences%20of%20hand%20poses%2C%20to%20estimate%20their%0Acorresponding%20sounds.%20In%20our%20experiments%2C%20we%20find%20that%20our%20generated%20sounds%0Aaccurately%20convey%20material%20properties%20and%20actions%2C%20and%20that%20they%20are%20often%0Aindistinguishable%20to%20human%20observers%20from%20real%20sounds.%20Project%20page%3A%0Ahttps%3A//www.yimingdou.com/hearing_hands/%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09989v1&entry.124074799=Read"},
{"title": "RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing\n  Semi-Supervised Semantic Segmentation", "author": "Jiayi Song and Kaiyu Li and Xiangyong Cao and Deyu Meng", "abstract": "  Semantic segmentation in remote sensing images is crucial for various\napplications, yet its performance is heavily reliant on large-scale,\nhigh-quality pixel-wise annotations, which are notoriously expensive and\ntime-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a\npromising alternative to mitigate this data dependency. However, existing SSS\nmethods often struggle with the inherent distribution mismatch between limited\nlabeled data and abundant unlabeled data, leading to suboptimal generalization.\nTo alleviate this issue, we attempt to introduce the Vision Foundation Models\n(VFMs) pre-trained on vast and diverse datasets into the SSS task since VFMs\npossess robust generalization capabilities that can effectively bridge this\ndistribution gap and provide strong semantic priors for SSS. Inspired by this,\nwe introduce RS-MTDF (Multi-Teacher Distillation and Fusion), a novel framework\nthat leverages the powerful semantic knowledge embedded in VFMs to guide\nsemi-supervised learning in remote sensing. Specifically, RS-MTDF employs\nmultiple frozen VFMs (e.g., DINOv2 and CLIP) as expert teachers, utilizing\nfeature-level distillation to align student features with their robust\nrepresentations. To further enhance discriminative power, the distilled\nknowledge is seamlessly fused into the student decoder. Extensive experiments\non three challenging remote sensing datasets demonstrate that RS-MTDF\nconsistently achieves state-of-the-art performance. Notably, our method\noutperforms existing approaches across various label ratios on LoveDA and\nsecures the highest IoU in the majority of semantic categories. These results\nunderscore the efficacy of multi-teacher VFM guidance in significantly\nenhancing both generalization and semantic understanding for remote sensing\nsegmentation. Ablation studies further validate the contribution of each\nproposed module.\n", "link": "http://arxiv.org/abs/2506.08772v2", "date": "2025-06-11", "relevancy": 2.8272, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5692}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5635}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RS-MTDF%3A%20Multi-Teacher%20Distillation%20and%20Fusion%20for%20Remote%20Sensing%0A%20%20Semi-Supervised%20Semantic%20Segmentation&body=Title%3A%20RS-MTDF%3A%20Multi-Teacher%20Distillation%20and%20Fusion%20for%20Remote%20Sensing%0A%20%20Semi-Supervised%20Semantic%20Segmentation%0AAuthor%3A%20Jiayi%20Song%20and%20Kaiyu%20Li%20and%20Xiangyong%20Cao%20and%20Deyu%20Meng%0AAbstract%3A%20%20%20Semantic%20segmentation%20in%20remote%20sensing%20images%20is%20crucial%20for%20various%0Aapplications%2C%20yet%20its%20performance%20is%20heavily%20reliant%20on%20large-scale%2C%0Ahigh-quality%20pixel-wise%20annotations%2C%20which%20are%20notoriously%20expensive%20and%0Atime-consuming%20to%20acquire.%20Semi-supervised%20semantic%20segmentation%20%28SSS%29%20offers%20a%0Apromising%20alternative%20to%20mitigate%20this%20data%20dependency.%20However%2C%20existing%20SSS%0Amethods%20often%20struggle%20with%20the%20inherent%20distribution%20mismatch%20between%20limited%0Alabeled%20data%20and%20abundant%20unlabeled%20data%2C%20leading%20to%20suboptimal%20generalization.%0ATo%20alleviate%20this%20issue%2C%20we%20attempt%20to%20introduce%20the%20Vision%20Foundation%20Models%0A%28VFMs%29%20pre-trained%20on%20vast%20and%20diverse%20datasets%20into%20the%20SSS%20task%20since%20VFMs%0Apossess%20robust%20generalization%20capabilities%20that%20can%20effectively%20bridge%20this%0Adistribution%20gap%20and%20provide%20strong%20semantic%20priors%20for%20SSS.%20Inspired%20by%20this%2C%0Awe%20introduce%20RS-MTDF%20%28Multi-Teacher%20Distillation%20and%20Fusion%29%2C%20a%20novel%20framework%0Athat%20leverages%20the%20powerful%20semantic%20knowledge%20embedded%20in%20VFMs%20to%20guide%0Asemi-supervised%20learning%20in%20remote%20sensing.%20Specifically%2C%20RS-MTDF%20employs%0Amultiple%20frozen%20VFMs%20%28e.g.%2C%20DINOv2%20and%20CLIP%29%20as%20expert%20teachers%2C%20utilizing%0Afeature-level%20distillation%20to%20align%20student%20features%20with%20their%20robust%0Arepresentations.%20To%20further%20enhance%20discriminative%20power%2C%20the%20distilled%0Aknowledge%20is%20seamlessly%20fused%20into%20the%20student%20decoder.%20Extensive%20experiments%0Aon%20three%20challenging%20remote%20sensing%20datasets%20demonstrate%20that%20RS-MTDF%0Aconsistently%20achieves%20state-of-the-art%20performance.%20Notably%2C%20our%20method%0Aoutperforms%20existing%20approaches%20across%20various%20label%20ratios%20on%20LoveDA%20and%0Asecures%20the%20highest%20IoU%20in%20the%20majority%20of%20semantic%20categories.%20These%20results%0Aunderscore%20the%20efficacy%20of%20multi-teacher%20VFM%20guidance%20in%20significantly%0Aenhancing%20both%20generalization%20and%20semantic%20understanding%20for%20remote%20sensing%0Asegmentation.%20Ablation%20studies%20further%20validate%20the%20contribution%20of%20each%0Aproposed%20module.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08772v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRS-MTDF%253A%2520Multi-Teacher%2520Distillation%2520and%2520Fusion%2520for%2520Remote%2520Sensing%250A%2520%2520Semi-Supervised%2520Semantic%2520Segmentation%26entry.906535625%3DJiayi%2520Song%2520and%2520Kaiyu%2520Li%2520and%2520Xiangyong%2520Cao%2520and%2520Deyu%2520Meng%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520in%2520remote%2520sensing%2520images%2520is%2520crucial%2520for%2520various%250Aapplications%252C%2520yet%2520its%2520performance%2520is%2520heavily%2520reliant%2520on%2520large-scale%252C%250Ahigh-quality%2520pixel-wise%2520annotations%252C%2520which%2520are%2520notoriously%2520expensive%2520and%250Atime-consuming%2520to%2520acquire.%2520Semi-supervised%2520semantic%2520segmentation%2520%2528SSS%2529%2520offers%2520a%250Apromising%2520alternative%2520to%2520mitigate%2520this%2520data%2520dependency.%2520However%252C%2520existing%2520SSS%250Amethods%2520often%2520struggle%2520with%2520the%2520inherent%2520distribution%2520mismatch%2520between%2520limited%250Alabeled%2520data%2520and%2520abundant%2520unlabeled%2520data%252C%2520leading%2520to%2520suboptimal%2520generalization.%250ATo%2520alleviate%2520this%2520issue%252C%2520we%2520attempt%2520to%2520introduce%2520the%2520Vision%2520Foundation%2520Models%250A%2528VFMs%2529%2520pre-trained%2520on%2520vast%2520and%2520diverse%2520datasets%2520into%2520the%2520SSS%2520task%2520since%2520VFMs%250Apossess%2520robust%2520generalization%2520capabilities%2520that%2520can%2520effectively%2520bridge%2520this%250Adistribution%2520gap%2520and%2520provide%2520strong%2520semantic%2520priors%2520for%2520SSS.%2520Inspired%2520by%2520this%252C%250Awe%2520introduce%2520RS-MTDF%2520%2528Multi-Teacher%2520Distillation%2520and%2520Fusion%2529%252C%2520a%2520novel%2520framework%250Athat%2520leverages%2520the%2520powerful%2520semantic%2520knowledge%2520embedded%2520in%2520VFMs%2520to%2520guide%250Asemi-supervised%2520learning%2520in%2520remote%2520sensing.%2520Specifically%252C%2520RS-MTDF%2520employs%250Amultiple%2520frozen%2520VFMs%2520%2528e.g.%252C%2520DINOv2%2520and%2520CLIP%2529%2520as%2520expert%2520teachers%252C%2520utilizing%250Afeature-level%2520distillation%2520to%2520align%2520student%2520features%2520with%2520their%2520robust%250Arepresentations.%2520To%2520further%2520enhance%2520discriminative%2520power%252C%2520the%2520distilled%250Aknowledge%2520is%2520seamlessly%2520fused%2520into%2520the%2520student%2520decoder.%2520Extensive%2520experiments%250Aon%2520three%2520challenging%2520remote%2520sensing%2520datasets%2520demonstrate%2520that%2520RS-MTDF%250Aconsistently%2520achieves%2520state-of-the-art%2520performance.%2520Notably%252C%2520our%2520method%250Aoutperforms%2520existing%2520approaches%2520across%2520various%2520label%2520ratios%2520on%2520LoveDA%2520and%250Asecures%2520the%2520highest%2520IoU%2520in%2520the%2520majority%2520of%2520semantic%2520categories.%2520These%2520results%250Aunderscore%2520the%2520efficacy%2520of%2520multi-teacher%2520VFM%2520guidance%2520in%2520significantly%250Aenhancing%2520both%2520generalization%2520and%2520semantic%2520understanding%2520for%2520remote%2520sensing%250Asegmentation.%2520Ablation%2520studies%2520further%2520validate%2520the%2520contribution%2520of%2520each%250Aproposed%2520module.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08772v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RS-MTDF%3A%20Multi-Teacher%20Distillation%20and%20Fusion%20for%20Remote%20Sensing%0A%20%20Semi-Supervised%20Semantic%20Segmentation&entry.906535625=Jiayi%20Song%20and%20Kaiyu%20Li%20and%20Xiangyong%20Cao%20and%20Deyu%20Meng&entry.1292438233=%20%20Semantic%20segmentation%20in%20remote%20sensing%20images%20is%20crucial%20for%20various%0Aapplications%2C%20yet%20its%20performance%20is%20heavily%20reliant%20on%20large-scale%2C%0Ahigh-quality%20pixel-wise%20annotations%2C%20which%20are%20notoriously%20expensive%20and%0Atime-consuming%20to%20acquire.%20Semi-supervised%20semantic%20segmentation%20%28SSS%29%20offers%20a%0Apromising%20alternative%20to%20mitigate%20this%20data%20dependency.%20However%2C%20existing%20SSS%0Amethods%20often%20struggle%20with%20the%20inherent%20distribution%20mismatch%20between%20limited%0Alabeled%20data%20and%20abundant%20unlabeled%20data%2C%20leading%20to%20suboptimal%20generalization.%0ATo%20alleviate%20this%20issue%2C%20we%20attempt%20to%20introduce%20the%20Vision%20Foundation%20Models%0A%28VFMs%29%20pre-trained%20on%20vast%20and%20diverse%20datasets%20into%20the%20SSS%20task%20since%20VFMs%0Apossess%20robust%20generalization%20capabilities%20that%20can%20effectively%20bridge%20this%0Adistribution%20gap%20and%20provide%20strong%20semantic%20priors%20for%20SSS.%20Inspired%20by%20this%2C%0Awe%20introduce%20RS-MTDF%20%28Multi-Teacher%20Distillation%20and%20Fusion%29%2C%20a%20novel%20framework%0Athat%20leverages%20the%20powerful%20semantic%20knowledge%20embedded%20in%20VFMs%20to%20guide%0Asemi-supervised%20learning%20in%20remote%20sensing.%20Specifically%2C%20RS-MTDF%20employs%0Amultiple%20frozen%20VFMs%20%28e.g.%2C%20DINOv2%20and%20CLIP%29%20as%20expert%20teachers%2C%20utilizing%0Afeature-level%20distillation%20to%20align%20student%20features%20with%20their%20robust%0Arepresentations.%20To%20further%20enhance%20discriminative%20power%2C%20the%20distilled%0Aknowledge%20is%20seamlessly%20fused%20into%20the%20student%20decoder.%20Extensive%20experiments%0Aon%20three%20challenging%20remote%20sensing%20datasets%20demonstrate%20that%20RS-MTDF%0Aconsistently%20achieves%20state-of-the-art%20performance.%20Notably%2C%20our%20method%0Aoutperforms%20existing%20approaches%20across%20various%20label%20ratios%20on%20LoveDA%20and%0Asecures%20the%20highest%20IoU%20in%20the%20majority%20of%20semantic%20categories.%20These%20results%0Aunderscore%20the%20efficacy%20of%20multi-teacher%20VFM%20guidance%20in%20significantly%0Aenhancing%20both%20generalization%20and%20semantic%20understanding%20for%20remote%20sensing%0Asegmentation.%20Ablation%20studies%20further%20validate%20the%20contribution%20of%20each%0Aproposed%20module.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08772v2&entry.124074799=Read"},
{"title": "MindOmni: Unleashing Reasoning Generation in Vision Language Models with\n  RGPO", "author": "Yicheng Xiao and Lin Song and Yukang Chen and Yingmin Luo and Yuxin Chen and Yukang Gan and Wei Huang and Xiu Li and Xiaojuan Qi and Ying Shan", "abstract": "  Recent text-to-image systems face limitations in handling multimodal inputs\nand complex reasoning tasks. We introduce MindOmni, a unified multimodal large\nlanguage model that addresses these challenges by incorporating reasoning\ngeneration through reinforcement learning. MindOmni leverages a three-phase\ntraining strategy: i) design of a unified vision language model with a\ndecoder-only diffusion module, ii) supervised fine-tuning with Chain-of-Thought\n(CoT) instruction data, and iii) our proposed Reasoning Generation Policy\nOptimization (RGPO) algorithm, utilizing multimodal feedback to effectively\nguide policy updates. Experimental results demonstrate that MindOmni\noutperforms existing models, achieving impressive performance on both\nunderstanding and generation benchmarks, meanwhile showcasing advanced\nfine-grained reasoning generation capabilities, especially with mathematical\nreasoning instruction. All codes will be made public at\nhttps://github.com/TencentARC/MindOmni\n", "link": "http://arxiv.org/abs/2505.13031v2", "date": "2025-06-11", "relevancy": 2.827, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5673}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5673}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MindOmni%3A%20Unleashing%20Reasoning%20Generation%20in%20Vision%20Language%20Models%20with%0A%20%20RGPO&body=Title%3A%20MindOmni%3A%20Unleashing%20Reasoning%20Generation%20in%20Vision%20Language%20Models%20with%0A%20%20RGPO%0AAuthor%3A%20Yicheng%20Xiao%20and%20Lin%20Song%20and%20Yukang%20Chen%20and%20Yingmin%20Luo%20and%20Yuxin%20Chen%20and%20Yukang%20Gan%20and%20Wei%20Huang%20and%20Xiu%20Li%20and%20Xiaojuan%20Qi%20and%20Ying%20Shan%0AAbstract%3A%20%20%20Recent%20text-to-image%20systems%20face%20limitations%20in%20handling%20multimodal%20inputs%0Aand%20complex%20reasoning%20tasks.%20We%20introduce%20MindOmni%2C%20a%20unified%20multimodal%20large%0Alanguage%20model%20that%20addresses%20these%20challenges%20by%20incorporating%20reasoning%0Ageneration%20through%20reinforcement%20learning.%20MindOmni%20leverages%20a%20three-phase%0Atraining%20strategy%3A%20i%29%20design%20of%20a%20unified%20vision%20language%20model%20with%20a%0Adecoder-only%20diffusion%20module%2C%20ii%29%20supervised%20fine-tuning%20with%20Chain-of-Thought%0A%28CoT%29%20instruction%20data%2C%20and%20iii%29%20our%20proposed%20Reasoning%20Generation%20Policy%0AOptimization%20%28RGPO%29%20algorithm%2C%20utilizing%20multimodal%20feedback%20to%20effectively%0Aguide%20policy%20updates.%20Experimental%20results%20demonstrate%20that%20MindOmni%0Aoutperforms%20existing%20models%2C%20achieving%20impressive%20performance%20on%20both%0Aunderstanding%20and%20generation%20benchmarks%2C%20meanwhile%20showcasing%20advanced%0Afine-grained%20reasoning%20generation%20capabilities%2C%20especially%20with%20mathematical%0Areasoning%20instruction.%20All%20codes%20will%20be%20made%20public%20at%0Ahttps%3A//github.com/TencentARC/MindOmni%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13031v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMindOmni%253A%2520Unleashing%2520Reasoning%2520Generation%2520in%2520Vision%2520Language%2520Models%2520with%250A%2520%2520RGPO%26entry.906535625%3DYicheng%2520Xiao%2520and%2520Lin%2520Song%2520and%2520Yukang%2520Chen%2520and%2520Yingmin%2520Luo%2520and%2520Yuxin%2520Chen%2520and%2520Yukang%2520Gan%2520and%2520Wei%2520Huang%2520and%2520Xiu%2520Li%2520and%2520Xiaojuan%2520Qi%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520Recent%2520text-to-image%2520systems%2520face%2520limitations%2520in%2520handling%2520multimodal%2520inputs%250Aand%2520complex%2520reasoning%2520tasks.%2520We%2520introduce%2520MindOmni%252C%2520a%2520unified%2520multimodal%2520large%250Alanguage%2520model%2520that%2520addresses%2520these%2520challenges%2520by%2520incorporating%2520reasoning%250Ageneration%2520through%2520reinforcement%2520learning.%2520MindOmni%2520leverages%2520a%2520three-phase%250Atraining%2520strategy%253A%2520i%2529%2520design%2520of%2520a%2520unified%2520vision%2520language%2520model%2520with%2520a%250Adecoder-only%2520diffusion%2520module%252C%2520ii%2529%2520supervised%2520fine-tuning%2520with%2520Chain-of-Thought%250A%2528CoT%2529%2520instruction%2520data%252C%2520and%2520iii%2529%2520our%2520proposed%2520Reasoning%2520Generation%2520Policy%250AOptimization%2520%2528RGPO%2529%2520algorithm%252C%2520utilizing%2520multimodal%2520feedback%2520to%2520effectively%250Aguide%2520policy%2520updates.%2520Experimental%2520results%2520demonstrate%2520that%2520MindOmni%250Aoutperforms%2520existing%2520models%252C%2520achieving%2520impressive%2520performance%2520on%2520both%250Aunderstanding%2520and%2520generation%2520benchmarks%252C%2520meanwhile%2520showcasing%2520advanced%250Afine-grained%2520reasoning%2520generation%2520capabilities%252C%2520especially%2520with%2520mathematical%250Areasoning%2520instruction.%2520All%2520codes%2520will%2520be%2520made%2520public%2520at%250Ahttps%253A//github.com/TencentARC/MindOmni%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13031v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MindOmni%3A%20Unleashing%20Reasoning%20Generation%20in%20Vision%20Language%20Models%20with%0A%20%20RGPO&entry.906535625=Yicheng%20Xiao%20and%20Lin%20Song%20and%20Yukang%20Chen%20and%20Yingmin%20Luo%20and%20Yuxin%20Chen%20and%20Yukang%20Gan%20and%20Wei%20Huang%20and%20Xiu%20Li%20and%20Xiaojuan%20Qi%20and%20Ying%20Shan&entry.1292438233=%20%20Recent%20text-to-image%20systems%20face%20limitations%20in%20handling%20multimodal%20inputs%0Aand%20complex%20reasoning%20tasks.%20We%20introduce%20MindOmni%2C%20a%20unified%20multimodal%20large%0Alanguage%20model%20that%20addresses%20these%20challenges%20by%20incorporating%20reasoning%0Ageneration%20through%20reinforcement%20learning.%20MindOmni%20leverages%20a%20three-phase%0Atraining%20strategy%3A%20i%29%20design%20of%20a%20unified%20vision%20language%20model%20with%20a%0Adecoder-only%20diffusion%20module%2C%20ii%29%20supervised%20fine-tuning%20with%20Chain-of-Thought%0A%28CoT%29%20instruction%20data%2C%20and%20iii%29%20our%20proposed%20Reasoning%20Generation%20Policy%0AOptimization%20%28RGPO%29%20algorithm%2C%20utilizing%20multimodal%20feedback%20to%20effectively%0Aguide%20policy%20updates.%20Experimental%20results%20demonstrate%20that%20MindOmni%0Aoutperforms%20existing%20models%2C%20achieving%20impressive%20performance%20on%20both%0Aunderstanding%20and%20generation%20benchmarks%2C%20meanwhile%20showcasing%20advanced%0Afine-grained%20reasoning%20generation%20capabilities%2C%20especially%20with%20mathematical%0Areasoning%20instruction.%20All%20codes%20will%20be%20made%20public%20at%0Ahttps%3A//github.com/TencentARC/MindOmni%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13031v2&entry.124074799=Read"},
{"title": "Incorporating Linguistic Constraints from External Knowledge Source for\n  Audio-Visual Target Speech Extraction", "author": "Wenxuan Wu and Shuai Wang and Xixin Wu and Helen Meng and Haizhou Li", "abstract": "  Audio-visual target speaker extraction (AV-TSE) models primarily rely on\ntarget visual cues to isolate the target speaker's voice from others. We know\nthat humans leverage linguistic knowledge, such as syntax and semantics, to\nsupport speech perception. Inspired by this, we explore the potential of\npre-trained speech-language models (PSLMs) and pre-trained language models\n(PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose\nincorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE\nmodel as additional supervision signals. Without introducing any extra\ncomputational cost during inference, the proposed approach consistently\nimproves speech quality and intelligibility. Furthermore, we evaluate our\nmethod in multi-language settings and visual cue-impaired scenarios and show\nrobust performance gains.\n", "link": "http://arxiv.org/abs/2506.09792v1", "date": "2025-06-11", "relevancy": 2.7552, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5676}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5676}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incorporating%20Linguistic%20Constraints%20from%20External%20Knowledge%20Source%20for%0A%20%20Audio-Visual%20Target%20Speech%20Extraction&body=Title%3A%20Incorporating%20Linguistic%20Constraints%20from%20External%20Knowledge%20Source%20for%0A%20%20Audio-Visual%20Target%20Speech%20Extraction%0AAuthor%3A%20Wenxuan%20Wu%20and%20Shuai%20Wang%20and%20Xixin%20Wu%20and%20Helen%20Meng%20and%20Haizhou%20Li%0AAbstract%3A%20%20%20Audio-visual%20target%20speaker%20extraction%20%28AV-TSE%29%20models%20primarily%20rely%20on%0Atarget%20visual%20cues%20to%20isolate%20the%20target%20speaker%27s%20voice%20from%20others.%20We%20know%0Athat%20humans%20leverage%20linguistic%20knowledge%2C%20such%20as%20syntax%20and%20semantics%2C%20to%0Asupport%20speech%20perception.%20Inspired%20by%20this%2C%20we%20explore%20the%20potential%20of%0Apre-trained%20speech-language%20models%20%28PSLMs%29%20and%20pre-trained%20language%20models%0A%28PLMs%29%20as%20auxiliary%20knowledge%20sources%20for%20AV-TSE.%20In%20this%20study%2C%20we%20propose%0Aincorporating%20the%20linguistic%20constraints%20from%20PSLMs%20or%20PLMs%20for%20the%20AV-TSE%0Amodel%20as%20additional%20supervision%20signals.%20Without%20introducing%20any%20extra%0Acomputational%20cost%20during%20inference%2C%20the%20proposed%20approach%20consistently%0Aimproves%20speech%20quality%20and%20intelligibility.%20Furthermore%2C%20we%20evaluate%20our%0Amethod%20in%20multi-language%20settings%20and%20visual%20cue-impaired%20scenarios%20and%20show%0Arobust%20performance%20gains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncorporating%2520Linguistic%2520Constraints%2520from%2520External%2520Knowledge%2520Source%2520for%250A%2520%2520Audio-Visual%2520Target%2520Speech%2520Extraction%26entry.906535625%3DWenxuan%2520Wu%2520and%2520Shuai%2520Wang%2520and%2520Xixin%2520Wu%2520and%2520Helen%2520Meng%2520and%2520Haizhou%2520Li%26entry.1292438233%3D%2520%2520Audio-visual%2520target%2520speaker%2520extraction%2520%2528AV-TSE%2529%2520models%2520primarily%2520rely%2520on%250Atarget%2520visual%2520cues%2520to%2520isolate%2520the%2520target%2520speaker%2527s%2520voice%2520from%2520others.%2520We%2520know%250Athat%2520humans%2520leverage%2520linguistic%2520knowledge%252C%2520such%2520as%2520syntax%2520and%2520semantics%252C%2520to%250Asupport%2520speech%2520perception.%2520Inspired%2520by%2520this%252C%2520we%2520explore%2520the%2520potential%2520of%250Apre-trained%2520speech-language%2520models%2520%2528PSLMs%2529%2520and%2520pre-trained%2520language%2520models%250A%2528PLMs%2529%2520as%2520auxiliary%2520knowledge%2520sources%2520for%2520AV-TSE.%2520In%2520this%2520study%252C%2520we%2520propose%250Aincorporating%2520the%2520linguistic%2520constraints%2520from%2520PSLMs%2520or%2520PLMs%2520for%2520the%2520AV-TSE%250Amodel%2520as%2520additional%2520supervision%2520signals.%2520Without%2520introducing%2520any%2520extra%250Acomputational%2520cost%2520during%2520inference%252C%2520the%2520proposed%2520approach%2520consistently%250Aimproves%2520speech%2520quality%2520and%2520intelligibility.%2520Furthermore%252C%2520we%2520evaluate%2520our%250Amethod%2520in%2520multi-language%2520settings%2520and%2520visual%2520cue-impaired%2520scenarios%2520and%2520show%250Arobust%2520performance%2520gains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incorporating%20Linguistic%20Constraints%20from%20External%20Knowledge%20Source%20for%0A%20%20Audio-Visual%20Target%20Speech%20Extraction&entry.906535625=Wenxuan%20Wu%20and%20Shuai%20Wang%20and%20Xixin%20Wu%20and%20Helen%20Meng%20and%20Haizhou%20Li&entry.1292438233=%20%20Audio-visual%20target%20speaker%20extraction%20%28AV-TSE%29%20models%20primarily%20rely%20on%0Atarget%20visual%20cues%20to%20isolate%20the%20target%20speaker%27s%20voice%20from%20others.%20We%20know%0Athat%20humans%20leverage%20linguistic%20knowledge%2C%20such%20as%20syntax%20and%20semantics%2C%20to%0Asupport%20speech%20perception.%20Inspired%20by%20this%2C%20we%20explore%20the%20potential%20of%0Apre-trained%20speech-language%20models%20%28PSLMs%29%20and%20pre-trained%20language%20models%0A%28PLMs%29%20as%20auxiliary%20knowledge%20sources%20for%20AV-TSE.%20In%20this%20study%2C%20we%20propose%0Aincorporating%20the%20linguistic%20constraints%20from%20PSLMs%20or%20PLMs%20for%20the%20AV-TSE%0Amodel%20as%20additional%20supervision%20signals.%20Without%20introducing%20any%20extra%0Acomputational%20cost%20during%20inference%2C%20the%20proposed%20approach%20consistently%0Aimproves%20speech%20quality%20and%20intelligibility.%20Furthermore%2C%20we%20evaluate%20our%0Amethod%20in%20multi-language%20settings%20and%20visual%20cue-impaired%20scenarios%20and%20show%0Arobust%20performance%20gains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09792v1&entry.124074799=Read"},
{"title": "CEM-FBGTinyDet: Context-Enhanced Foreground Balance with Gradient Tuning\n  for tiny Objects", "author": "Tao Liu and Zhenchao Cui", "abstract": "  Tiny object detection (TOD) reveals a fundamental flaw in feature pyramid\nnetworks: high-level features (P5-P6) frequently receive zero positive anchors\nunder standard label assignment protocols, leaving their semantic\nrepresentations untrained due to exclusion from loss computation. This creates\ndual deficiencies: (1) Stranded high-level features become semantic dead-ends\nwithout gradient updates, while (2) low-level features lack essential semantic\ncontext for robust classification. We propose E-FPN-BS that systematically\nconverts wasted high-level semantics into low-level feature enhancements. To\naddress these issues, we propose E-FPN-BS, a novel architecture integrating\nmulti-scale feature enhancement and adaptive optimization. First, our Context\nEnhancement Module(CEM) employs dual-branch processing to align and compress\nhigh-level features for effective global-local fusion. Second, the\nForeground-Background Separation Module (FBSM) generates spatial gating masks\nthat dynamically amplify discriminative regions. To address gradient imbalance\nacross object scales, we further propose a Dynamic Gradient-Balanced Loss\n(DCLoss) that automatically modulates loss contributions via scale-aware\ngradient equilibrium. Extensive experiments across multiple benchmark datasets\ndemonstrate the outstanding performance and generalization ability of our\napproach.\n", "link": "http://arxiv.org/abs/2506.09897v1", "date": "2025-06-11", "relevancy": 2.7457, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5584}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5517}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CEM-FBGTinyDet%3A%20Context-Enhanced%20Foreground%20Balance%20with%20Gradient%20Tuning%0A%20%20for%20tiny%20Objects&body=Title%3A%20CEM-FBGTinyDet%3A%20Context-Enhanced%20Foreground%20Balance%20with%20Gradient%20Tuning%0A%20%20for%20tiny%20Objects%0AAuthor%3A%20Tao%20Liu%20and%20Zhenchao%20Cui%0AAbstract%3A%20%20%20Tiny%20object%20detection%20%28TOD%29%20reveals%20a%20fundamental%20flaw%20in%20feature%20pyramid%0Anetworks%3A%20high-level%20features%20%28P5-P6%29%20frequently%20receive%20zero%20positive%20anchors%0Aunder%20standard%20label%20assignment%20protocols%2C%20leaving%20their%20semantic%0Arepresentations%20untrained%20due%20to%20exclusion%20from%20loss%20computation.%20This%20creates%0Adual%20deficiencies%3A%20%281%29%20Stranded%20high-level%20features%20become%20semantic%20dead-ends%0Awithout%20gradient%20updates%2C%20while%20%282%29%20low-level%20features%20lack%20essential%20semantic%0Acontext%20for%20robust%20classification.%20We%20propose%20E-FPN-BS%20that%20systematically%0Aconverts%20wasted%20high-level%20semantics%20into%20low-level%20feature%20enhancements.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20E-FPN-BS%2C%20a%20novel%20architecture%20integrating%0Amulti-scale%20feature%20enhancement%20and%20adaptive%20optimization.%20First%2C%20our%20Context%0AEnhancement%20Module%28CEM%29%20employs%20dual-branch%20processing%20to%20align%20and%20compress%0Ahigh-level%20features%20for%20effective%20global-local%20fusion.%20Second%2C%20the%0AForeground-Background%20Separation%20Module%20%28FBSM%29%20generates%20spatial%20gating%20masks%0Athat%20dynamically%20amplify%20discriminative%20regions.%20To%20address%20gradient%20imbalance%0Aacross%20object%20scales%2C%20we%20further%20propose%20a%20Dynamic%20Gradient-Balanced%20Loss%0A%28DCLoss%29%20that%20automatically%20modulates%20loss%20contributions%20via%20scale-aware%0Agradient%20equilibrium.%20Extensive%20experiments%20across%20multiple%20benchmark%20datasets%0Ademonstrate%20the%20outstanding%20performance%20and%20generalization%20ability%20of%20our%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCEM-FBGTinyDet%253A%2520Context-Enhanced%2520Foreground%2520Balance%2520with%2520Gradient%2520Tuning%250A%2520%2520for%2520tiny%2520Objects%26entry.906535625%3DTao%2520Liu%2520and%2520Zhenchao%2520Cui%26entry.1292438233%3D%2520%2520Tiny%2520object%2520detection%2520%2528TOD%2529%2520reveals%2520a%2520fundamental%2520flaw%2520in%2520feature%2520pyramid%250Anetworks%253A%2520high-level%2520features%2520%2528P5-P6%2529%2520frequently%2520receive%2520zero%2520positive%2520anchors%250Aunder%2520standard%2520label%2520assignment%2520protocols%252C%2520leaving%2520their%2520semantic%250Arepresentations%2520untrained%2520due%2520to%2520exclusion%2520from%2520loss%2520computation.%2520This%2520creates%250Adual%2520deficiencies%253A%2520%25281%2529%2520Stranded%2520high-level%2520features%2520become%2520semantic%2520dead-ends%250Awithout%2520gradient%2520updates%252C%2520while%2520%25282%2529%2520low-level%2520features%2520lack%2520essential%2520semantic%250Acontext%2520for%2520robust%2520classification.%2520We%2520propose%2520E-FPN-BS%2520that%2520systematically%250Aconverts%2520wasted%2520high-level%2520semantics%2520into%2520low-level%2520feature%2520enhancements.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520propose%2520E-FPN-BS%252C%2520a%2520novel%2520architecture%2520integrating%250Amulti-scale%2520feature%2520enhancement%2520and%2520adaptive%2520optimization.%2520First%252C%2520our%2520Context%250AEnhancement%2520Module%2528CEM%2529%2520employs%2520dual-branch%2520processing%2520to%2520align%2520and%2520compress%250Ahigh-level%2520features%2520for%2520effective%2520global-local%2520fusion.%2520Second%252C%2520the%250AForeground-Background%2520Separation%2520Module%2520%2528FBSM%2529%2520generates%2520spatial%2520gating%2520masks%250Athat%2520dynamically%2520amplify%2520discriminative%2520regions.%2520To%2520address%2520gradient%2520imbalance%250Aacross%2520object%2520scales%252C%2520we%2520further%2520propose%2520a%2520Dynamic%2520Gradient-Balanced%2520Loss%250A%2528DCLoss%2529%2520that%2520automatically%2520modulates%2520loss%2520contributions%2520via%2520scale-aware%250Agradient%2520equilibrium.%2520Extensive%2520experiments%2520across%2520multiple%2520benchmark%2520datasets%250Ademonstrate%2520the%2520outstanding%2520performance%2520and%2520generalization%2520ability%2520of%2520our%250Aapproach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CEM-FBGTinyDet%3A%20Context-Enhanced%20Foreground%20Balance%20with%20Gradient%20Tuning%0A%20%20for%20tiny%20Objects&entry.906535625=Tao%20Liu%20and%20Zhenchao%20Cui&entry.1292438233=%20%20Tiny%20object%20detection%20%28TOD%29%20reveals%20a%20fundamental%20flaw%20in%20feature%20pyramid%0Anetworks%3A%20high-level%20features%20%28P5-P6%29%20frequently%20receive%20zero%20positive%20anchors%0Aunder%20standard%20label%20assignment%20protocols%2C%20leaving%20their%20semantic%0Arepresentations%20untrained%20due%20to%20exclusion%20from%20loss%20computation.%20This%20creates%0Adual%20deficiencies%3A%20%281%29%20Stranded%20high-level%20features%20become%20semantic%20dead-ends%0Awithout%20gradient%20updates%2C%20while%20%282%29%20low-level%20features%20lack%20essential%20semantic%0Acontext%20for%20robust%20classification.%20We%20propose%20E-FPN-BS%20that%20systematically%0Aconverts%20wasted%20high-level%20semantics%20into%20low-level%20feature%20enhancements.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20E-FPN-BS%2C%20a%20novel%20architecture%20integrating%0Amulti-scale%20feature%20enhancement%20and%20adaptive%20optimization.%20First%2C%20our%20Context%0AEnhancement%20Module%28CEM%29%20employs%20dual-branch%20processing%20to%20align%20and%20compress%0Ahigh-level%20features%20for%20effective%20global-local%20fusion.%20Second%2C%20the%0AForeground-Background%20Separation%20Module%20%28FBSM%29%20generates%20spatial%20gating%20masks%0Athat%20dynamically%20amplify%20discriminative%20regions.%20To%20address%20gradient%20imbalance%0Aacross%20object%20scales%2C%20we%20further%20propose%20a%20Dynamic%20Gradient-Balanced%20Loss%0A%28DCLoss%29%20that%20automatically%20modulates%20loss%20contributions%20via%20scale-aware%0Agradient%20equilibrium.%20Extensive%20experiments%20across%20multiple%20benchmark%20datasets%0Ademonstrate%20the%20outstanding%20performance%20and%20generalization%20ability%20of%20our%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09897v1&entry.124074799=Read"},
{"title": "Advancing Decoding Strategies: Enhancements in Locally Typical Sampling\n  for LLMs", "author": "Jaydip Sen and Saptarshi Sengupta and Subhasis Dasgupta", "abstract": "  This chapter explores advancements in decoding strategies for large language\nmodels (LLMs), focusing on enhancing the Locally Typical Sampling (LTS)\nalgorithm. Traditional decoding methods, such as top-k and nucleus sampling,\noften struggle to balance fluency, diversity, and coherence in text generation.\nTo address these challenges, Adaptive Semantic-Aware Typicality Sampling (ASTS)\nis proposed as an improved version of LTS, incorporating dynamic entropy\nthresholding, multi-objective scoring, and reward-penalty adjustments. ASTS\nensures contextually coherent and diverse text generation while maintaining\ncomputational efficiency. Its performance is evaluated across multiple\nbenchmarks, including story generation and abstractive summarization, using\nmetrics such as perplexity, MAUVE, and diversity scores. Experimental results\ndemonstrate that ASTS outperforms existing sampling techniques by reducing\nrepetition, enhancing semantic alignment, and improving fluency.\n", "link": "http://arxiv.org/abs/2506.05387v2", "date": "2025-06-11", "relevancy": 2.6606, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5361}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5361}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Decoding%20Strategies%3A%20Enhancements%20in%20Locally%20Typical%20Sampling%0A%20%20for%20LLMs&body=Title%3A%20Advancing%20Decoding%20Strategies%3A%20Enhancements%20in%20Locally%20Typical%20Sampling%0A%20%20for%20LLMs%0AAuthor%3A%20Jaydip%20Sen%20and%20Saptarshi%20Sengupta%20and%20Subhasis%20Dasgupta%0AAbstract%3A%20%20%20This%20chapter%20explores%20advancements%20in%20decoding%20strategies%20for%20large%20language%0Amodels%20%28LLMs%29%2C%20focusing%20on%20enhancing%20the%20Locally%20Typical%20Sampling%20%28LTS%29%0Aalgorithm.%20Traditional%20decoding%20methods%2C%20such%20as%20top-k%20and%20nucleus%20sampling%2C%0Aoften%20struggle%20to%20balance%20fluency%2C%20diversity%2C%20and%20coherence%20in%20text%20generation.%0ATo%20address%20these%20challenges%2C%20Adaptive%20Semantic-Aware%20Typicality%20Sampling%20%28ASTS%29%0Ais%20proposed%20as%20an%20improved%20version%20of%20LTS%2C%20incorporating%20dynamic%20entropy%0Athresholding%2C%20multi-objective%20scoring%2C%20and%20reward-penalty%20adjustments.%20ASTS%0Aensures%20contextually%20coherent%20and%20diverse%20text%20generation%20while%20maintaining%0Acomputational%20efficiency.%20Its%20performance%20is%20evaluated%20across%20multiple%0Abenchmarks%2C%20including%20story%20generation%20and%20abstractive%20summarization%2C%20using%0Ametrics%20such%20as%20perplexity%2C%20MAUVE%2C%20and%20diversity%20scores.%20Experimental%20results%0Ademonstrate%20that%20ASTS%20outperforms%20existing%20sampling%20techniques%20by%20reducing%0Arepetition%2C%20enhancing%20semantic%20alignment%2C%20and%20improving%20fluency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05387v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Decoding%2520Strategies%253A%2520Enhancements%2520in%2520Locally%2520Typical%2520Sampling%250A%2520%2520for%2520LLMs%26entry.906535625%3DJaydip%2520Sen%2520and%2520Saptarshi%2520Sengupta%2520and%2520Subhasis%2520Dasgupta%26entry.1292438233%3D%2520%2520This%2520chapter%2520explores%2520advancements%2520in%2520decoding%2520strategies%2520for%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520focusing%2520on%2520enhancing%2520the%2520Locally%2520Typical%2520Sampling%2520%2528LTS%2529%250Aalgorithm.%2520Traditional%2520decoding%2520methods%252C%2520such%2520as%2520top-k%2520and%2520nucleus%2520sampling%252C%250Aoften%2520struggle%2520to%2520balance%2520fluency%252C%2520diversity%252C%2520and%2520coherence%2520in%2520text%2520generation.%250ATo%2520address%2520these%2520challenges%252C%2520Adaptive%2520Semantic-Aware%2520Typicality%2520Sampling%2520%2528ASTS%2529%250Ais%2520proposed%2520as%2520an%2520improved%2520version%2520of%2520LTS%252C%2520incorporating%2520dynamic%2520entropy%250Athresholding%252C%2520multi-objective%2520scoring%252C%2520and%2520reward-penalty%2520adjustments.%2520ASTS%250Aensures%2520contextually%2520coherent%2520and%2520diverse%2520text%2520generation%2520while%2520maintaining%250Acomputational%2520efficiency.%2520Its%2520performance%2520is%2520evaluated%2520across%2520multiple%250Abenchmarks%252C%2520including%2520story%2520generation%2520and%2520abstractive%2520summarization%252C%2520using%250Ametrics%2520such%2520as%2520perplexity%252C%2520MAUVE%252C%2520and%2520diversity%2520scores.%2520Experimental%2520results%250Ademonstrate%2520that%2520ASTS%2520outperforms%2520existing%2520sampling%2520techniques%2520by%2520reducing%250Arepetition%252C%2520enhancing%2520semantic%2520alignment%252C%2520and%2520improving%2520fluency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05387v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Decoding%20Strategies%3A%20Enhancements%20in%20Locally%20Typical%20Sampling%0A%20%20for%20LLMs&entry.906535625=Jaydip%20Sen%20and%20Saptarshi%20Sengupta%20and%20Subhasis%20Dasgupta&entry.1292438233=%20%20This%20chapter%20explores%20advancements%20in%20decoding%20strategies%20for%20large%20language%0Amodels%20%28LLMs%29%2C%20focusing%20on%20enhancing%20the%20Locally%20Typical%20Sampling%20%28LTS%29%0Aalgorithm.%20Traditional%20decoding%20methods%2C%20such%20as%20top-k%20and%20nucleus%20sampling%2C%0Aoften%20struggle%20to%20balance%20fluency%2C%20diversity%2C%20and%20coherence%20in%20text%20generation.%0ATo%20address%20these%20challenges%2C%20Adaptive%20Semantic-Aware%20Typicality%20Sampling%20%28ASTS%29%0Ais%20proposed%20as%20an%20improved%20version%20of%20LTS%2C%20incorporating%20dynamic%20entropy%0Athresholding%2C%20multi-objective%20scoring%2C%20and%20reward-penalty%20adjustments.%20ASTS%0Aensures%20contextually%20coherent%20and%20diverse%20text%20generation%20while%20maintaining%0Acomputational%20efficiency.%20Its%20performance%20is%20evaluated%20across%20multiple%0Abenchmarks%2C%20including%20story%20generation%20and%20abstractive%20summarization%2C%20using%0Ametrics%20such%20as%20perplexity%2C%20MAUVE%2C%20and%20diversity%20scores.%20Experimental%20results%0Ademonstrate%20that%20ASTS%20outperforms%20existing%20sampling%20techniques%20by%20reducing%0Arepetition%2C%20enhancing%20semantic%20alignment%2C%20and%20improving%20fluency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05387v2&entry.124074799=Read"},
{"title": "7B Fully Open Source Moxin-LLM/VLM -- From Pretraining to GRPO-based\n  Reinforcement Learning Enhancement", "author": "Pu Zhao and Xuan Shen and Zhenglun Kong and Yixin Shen and Sung-En Chang and Timothy Rupprecht and Lei Lu and Enfu Nan and Changdi Yang and Yumei He and Weiyan Shi and Xingchen Xu and Yu Huang and Wei Jiang and Wei Wang and Yue Chen and Yong He and Yanzhi Wang", "abstract": "  Recently, Large Language Models (LLMs) have undergone a significant\ntransformation, marked by a rapid rise in both their popularity and\ncapabilities. Leading this evolution are proprietary LLMs like GPT-4 and\nGPT-o1, which have captured widespread attention in the AI community due to\ntheir remarkable performance and versatility. Simultaneously, open-source LLMs,\nsuch as LLaMA, have made great contributions to the ever-increasing popularity\nof LLMs due to the ease to customize and deploy the models across diverse\napplications. Although open-source LLMs present unprecedented opportunities for\ninnovation and research, the commercialization of LLMs has raised concerns\nabout transparency, reproducibility, and safety. Many open-source LLMs fail to\nmeet fundamental transparency requirements by withholding essential components\nlike training code and data, which may hinder further innovations on LLMs. To\nmitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed,\nadhering to principles of open science, open source, open data, and open\naccess. We release the pre-training code and configurations, training and\nfine-tuning datasets, and intermediate and final checkpoints, aiming to make\ncontinuous commitments to fully open-source LLMs. After pre-training the base\nmodel, we finetune the Moxin Base model with SOTA post-training framework and\ninstruction data to obtain Moxin Instruct model. To improve the reasoning\ncapability, we further finetune our Instruct model with chain-of-thought data\ndistilled from DeepSeek R1, and then use Group Relative Policy Optimization\n(GRPO) following DeepSeek R1 to finetune our model, leading to the Moxin\nReasoning model. Moreover, we develop our vision language model based on our\nMoxin model. Experiments show that our models achieve superior performance in\nvarious evaluations such as zero-shot evaluation, few-shot evaluation, and CoT\nevaluation.\n", "link": "http://arxiv.org/abs/2412.06845v5", "date": "2025-06-11", "relevancy": 2.6566, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5469}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%207B%20Fully%20Open%20Source%20Moxin-LLM/VLM%20--%20From%20Pretraining%20to%20GRPO-based%0A%20%20Reinforcement%20Learning%20Enhancement&body=Title%3A%207B%20Fully%20Open%20Source%20Moxin-LLM/VLM%20--%20From%20Pretraining%20to%20GRPO-based%0A%20%20Reinforcement%20Learning%20Enhancement%0AAuthor%3A%20Pu%20Zhao%20and%20Xuan%20Shen%20and%20Zhenglun%20Kong%20and%20Yixin%20Shen%20and%20Sung-En%20Chang%20and%20Timothy%20Rupprecht%20and%20Lei%20Lu%20and%20Enfu%20Nan%20and%20Changdi%20Yang%20and%20Yumei%20He%20and%20Weiyan%20Shi%20and%20Xingchen%20Xu%20and%20Yu%20Huang%20and%20Wei%20Jiang%20and%20Wei%20Wang%20and%20Yue%20Chen%20and%20Yong%20He%20and%20Yanzhi%20Wang%0AAbstract%3A%20%20%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20undergone%20a%20significant%0Atransformation%2C%20marked%20by%20a%20rapid%20rise%20in%20both%20their%20popularity%20and%0Acapabilities.%20Leading%20this%20evolution%20are%20proprietary%20LLMs%20like%20GPT-4%20and%0AGPT-o1%2C%20which%20have%20captured%20widespread%20attention%20in%20the%20AI%20community%20due%20to%0Atheir%20remarkable%20performance%20and%20versatility.%20Simultaneously%2C%20open-source%20LLMs%2C%0Asuch%20as%20LLaMA%2C%20have%20made%20great%20contributions%20to%20the%20ever-increasing%20popularity%0Aof%20LLMs%20due%20to%20the%20ease%20to%20customize%20and%20deploy%20the%20models%20across%20diverse%0Aapplications.%20Although%20open-source%20LLMs%20present%20unprecedented%20opportunities%20for%0Ainnovation%20and%20research%2C%20the%20commercialization%20of%20LLMs%20has%20raised%20concerns%0Aabout%20transparency%2C%20reproducibility%2C%20and%20safety.%20Many%20open-source%20LLMs%20fail%20to%0Ameet%20fundamental%20transparency%20requirements%20by%20withholding%20essential%20components%0Alike%20training%20code%20and%20data%2C%20which%20may%20hinder%20further%20innovations%20on%20LLMs.%20To%0Amitigate%20this%20issue%2C%20we%20introduce%20Moxin%207B%2C%20a%20fully%20open-source%20LLM%20developed%2C%0Aadhering%20to%20principles%20of%20open%20science%2C%20open%20source%2C%20open%20data%2C%20and%20open%0Aaccess.%20We%20release%20the%20pre-training%20code%20and%20configurations%2C%20training%20and%0Afine-tuning%20datasets%2C%20and%20intermediate%20and%20final%20checkpoints%2C%20aiming%20to%20make%0Acontinuous%20commitments%20to%20fully%20open-source%20LLMs.%20After%20pre-training%20the%20base%0Amodel%2C%20we%20finetune%20the%20Moxin%20Base%20model%20with%20SOTA%20post-training%20framework%20and%0Ainstruction%20data%20to%20obtain%20Moxin%20Instruct%20model.%20To%20improve%20the%20reasoning%0Acapability%2C%20we%20further%20finetune%20our%20Instruct%20model%20with%20chain-of-thought%20data%0Adistilled%20from%20DeepSeek%20R1%2C%20and%20then%20use%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%20following%20DeepSeek%20R1%20to%20finetune%20our%20model%2C%20leading%20to%20the%20Moxin%0AReasoning%20model.%20Moreover%2C%20we%20develop%20our%20vision%20language%20model%20based%20on%20our%0AMoxin%20model.%20Experiments%20show%20that%20our%20models%20achieve%20superior%20performance%20in%0Avarious%20evaluations%20such%20as%20zero-shot%20evaluation%2C%20few-shot%20evaluation%2C%20and%20CoT%0Aevaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06845v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D7B%2520Fully%2520Open%2520Source%2520Moxin-LLM/VLM%2520--%2520From%2520Pretraining%2520to%2520GRPO-based%250A%2520%2520Reinforcement%2520Learning%2520Enhancement%26entry.906535625%3DPu%2520Zhao%2520and%2520Xuan%2520Shen%2520and%2520Zhenglun%2520Kong%2520and%2520Yixin%2520Shen%2520and%2520Sung-En%2520Chang%2520and%2520Timothy%2520Rupprecht%2520and%2520Lei%2520Lu%2520and%2520Enfu%2520Nan%2520and%2520Changdi%2520Yang%2520and%2520Yumei%2520He%2520and%2520Weiyan%2520Shi%2520and%2520Xingchen%2520Xu%2520and%2520Yu%2520Huang%2520and%2520Wei%2520Jiang%2520and%2520Wei%2520Wang%2520and%2520Yue%2520Chen%2520and%2520Yong%2520He%2520and%2520Yanzhi%2520Wang%26entry.1292438233%3D%2520%2520Recently%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520undergone%2520a%2520significant%250Atransformation%252C%2520marked%2520by%2520a%2520rapid%2520rise%2520in%2520both%2520their%2520popularity%2520and%250Acapabilities.%2520Leading%2520this%2520evolution%2520are%2520proprietary%2520LLMs%2520like%2520GPT-4%2520and%250AGPT-o1%252C%2520which%2520have%2520captured%2520widespread%2520attention%2520in%2520the%2520AI%2520community%2520due%2520to%250Atheir%2520remarkable%2520performance%2520and%2520versatility.%2520Simultaneously%252C%2520open-source%2520LLMs%252C%250Asuch%2520as%2520LLaMA%252C%2520have%2520made%2520great%2520contributions%2520to%2520the%2520ever-increasing%2520popularity%250Aof%2520LLMs%2520due%2520to%2520the%2520ease%2520to%2520customize%2520and%2520deploy%2520the%2520models%2520across%2520diverse%250Aapplications.%2520Although%2520open-source%2520LLMs%2520present%2520unprecedented%2520opportunities%2520for%250Ainnovation%2520and%2520research%252C%2520the%2520commercialization%2520of%2520LLMs%2520has%2520raised%2520concerns%250Aabout%2520transparency%252C%2520reproducibility%252C%2520and%2520safety.%2520Many%2520open-source%2520LLMs%2520fail%2520to%250Ameet%2520fundamental%2520transparency%2520requirements%2520by%2520withholding%2520essential%2520components%250Alike%2520training%2520code%2520and%2520data%252C%2520which%2520may%2520hinder%2520further%2520innovations%2520on%2520LLMs.%2520To%250Amitigate%2520this%2520issue%252C%2520we%2520introduce%2520Moxin%25207B%252C%2520a%2520fully%2520open-source%2520LLM%2520developed%252C%250Aadhering%2520to%2520principles%2520of%2520open%2520science%252C%2520open%2520source%252C%2520open%2520data%252C%2520and%2520open%250Aaccess.%2520We%2520release%2520the%2520pre-training%2520code%2520and%2520configurations%252C%2520training%2520and%250Afine-tuning%2520datasets%252C%2520and%2520intermediate%2520and%2520final%2520checkpoints%252C%2520aiming%2520to%2520make%250Acontinuous%2520commitments%2520to%2520fully%2520open-source%2520LLMs.%2520After%2520pre-training%2520the%2520base%250Amodel%252C%2520we%2520finetune%2520the%2520Moxin%2520Base%2520model%2520with%2520SOTA%2520post-training%2520framework%2520and%250Ainstruction%2520data%2520to%2520obtain%2520Moxin%2520Instruct%2520model.%2520To%2520improve%2520the%2520reasoning%250Acapability%252C%2520we%2520further%2520finetune%2520our%2520Instruct%2520model%2520with%2520chain-of-thought%2520data%250Adistilled%2520from%2520DeepSeek%2520R1%252C%2520and%2520then%2520use%2520Group%2520Relative%2520Policy%2520Optimization%250A%2528GRPO%2529%2520following%2520DeepSeek%2520R1%2520to%2520finetune%2520our%2520model%252C%2520leading%2520to%2520the%2520Moxin%250AReasoning%2520model.%2520Moreover%252C%2520we%2520develop%2520our%2520vision%2520language%2520model%2520based%2520on%2520our%250AMoxin%2520model.%2520Experiments%2520show%2520that%2520our%2520models%2520achieve%2520superior%2520performance%2520in%250Avarious%2520evaluations%2520such%2520as%2520zero-shot%2520evaluation%252C%2520few-shot%2520evaluation%252C%2520and%2520CoT%250Aevaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06845v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=7B%20Fully%20Open%20Source%20Moxin-LLM/VLM%20--%20From%20Pretraining%20to%20GRPO-based%0A%20%20Reinforcement%20Learning%20Enhancement&entry.906535625=Pu%20Zhao%20and%20Xuan%20Shen%20and%20Zhenglun%20Kong%20and%20Yixin%20Shen%20and%20Sung-En%20Chang%20and%20Timothy%20Rupprecht%20and%20Lei%20Lu%20and%20Enfu%20Nan%20and%20Changdi%20Yang%20and%20Yumei%20He%20and%20Weiyan%20Shi%20and%20Xingchen%20Xu%20and%20Yu%20Huang%20and%20Wei%20Jiang%20and%20Wei%20Wang%20and%20Yue%20Chen%20and%20Yong%20He%20and%20Yanzhi%20Wang&entry.1292438233=%20%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20undergone%20a%20significant%0Atransformation%2C%20marked%20by%20a%20rapid%20rise%20in%20both%20their%20popularity%20and%0Acapabilities.%20Leading%20this%20evolution%20are%20proprietary%20LLMs%20like%20GPT-4%20and%0AGPT-o1%2C%20which%20have%20captured%20widespread%20attention%20in%20the%20AI%20community%20due%20to%0Atheir%20remarkable%20performance%20and%20versatility.%20Simultaneously%2C%20open-source%20LLMs%2C%0Asuch%20as%20LLaMA%2C%20have%20made%20great%20contributions%20to%20the%20ever-increasing%20popularity%0Aof%20LLMs%20due%20to%20the%20ease%20to%20customize%20and%20deploy%20the%20models%20across%20diverse%0Aapplications.%20Although%20open-source%20LLMs%20present%20unprecedented%20opportunities%20for%0Ainnovation%20and%20research%2C%20the%20commercialization%20of%20LLMs%20has%20raised%20concerns%0Aabout%20transparency%2C%20reproducibility%2C%20and%20safety.%20Many%20open-source%20LLMs%20fail%20to%0Ameet%20fundamental%20transparency%20requirements%20by%20withholding%20essential%20components%0Alike%20training%20code%20and%20data%2C%20which%20may%20hinder%20further%20innovations%20on%20LLMs.%20To%0Amitigate%20this%20issue%2C%20we%20introduce%20Moxin%207B%2C%20a%20fully%20open-source%20LLM%20developed%2C%0Aadhering%20to%20principles%20of%20open%20science%2C%20open%20source%2C%20open%20data%2C%20and%20open%0Aaccess.%20We%20release%20the%20pre-training%20code%20and%20configurations%2C%20training%20and%0Afine-tuning%20datasets%2C%20and%20intermediate%20and%20final%20checkpoints%2C%20aiming%20to%20make%0Acontinuous%20commitments%20to%20fully%20open-source%20LLMs.%20After%20pre-training%20the%20base%0Amodel%2C%20we%20finetune%20the%20Moxin%20Base%20model%20with%20SOTA%20post-training%20framework%20and%0Ainstruction%20data%20to%20obtain%20Moxin%20Instruct%20model.%20To%20improve%20the%20reasoning%0Acapability%2C%20we%20further%20finetune%20our%20Instruct%20model%20with%20chain-of-thought%20data%0Adistilled%20from%20DeepSeek%20R1%2C%20and%20then%20use%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%20following%20DeepSeek%20R1%20to%20finetune%20our%20model%2C%20leading%20to%20the%20Moxin%0AReasoning%20model.%20Moreover%2C%20we%20develop%20our%20vision%20language%20model%20based%20on%20our%0AMoxin%20model.%20Experiments%20show%20that%20our%20models%20achieve%20superior%20performance%20in%0Avarious%20evaluations%20such%20as%20zero-shot%20evaluation%2C%20few-shot%20evaluation%2C%20and%20CoT%0Aevaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06845v5&entry.124074799=Read"},
{"title": "EquiCaps: Predictor-Free Pose-Aware Pre-Trained Capsule Networks", "author": "Athinoulla Konstantinou and Georgios Leontidis and Mamatha Thota and Aiden Durrant", "abstract": "  Learning self-supervised representations that are invariant and equivariant\nto transformations is crucial for advancing beyond traditional visual\nclassification tasks. However, many methods rely on predictor architectures to\nencode equivariance, despite evidence that architectural choices, such as\ncapsule networks, inherently excel at learning interpretable pose-aware\nrepresentations. To explore this, we introduce EquiCaps (Equivariant Capsule\nNetwork), a capsule-based approach to pose-aware self-supervision that\neliminates the need for a specialised predictor for enforcing equivariance.\nInstead, we leverage the intrinsic pose-awareness capabilities of capsules to\nimprove performance in pose estimation tasks. To further challenge our\nassumptions, we increase task complexity via multi-geometric transformations to\nenable a more thorough evaluation of invariance and equivariance by introducing\n3DIEBench-T, an extension of a 3D object-rendering benchmark dataset. Empirical\nresults demonstrate that EquiCaps outperforms prior state-of-the-art\nequivariant methods on rotation prediction, achieving a supervised-level $R^2$\nof 0.78 on the 3DIEBench rotation prediction benchmark and improving upon SIE\nand CapsIE by 0.05 and 0.04 $R^2$, respectively. Moreover, in contrast to\nnon-capsule-based equivariant approaches, EquiCaps maintains robust equivariant\nperformance under combined geometric transformations, underscoring its\ngeneralisation capabilities and the promise of predictor-free capsule\narchitectures.\n", "link": "http://arxiv.org/abs/2506.09895v1", "date": "2025-06-11", "relevancy": 2.6416, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5402}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5225}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EquiCaps%3A%20Predictor-Free%20Pose-Aware%20Pre-Trained%20Capsule%20Networks&body=Title%3A%20EquiCaps%3A%20Predictor-Free%20Pose-Aware%20Pre-Trained%20Capsule%20Networks%0AAuthor%3A%20Athinoulla%20Konstantinou%20and%20Georgios%20Leontidis%20and%20Mamatha%20Thota%20and%20Aiden%20Durrant%0AAbstract%3A%20%20%20Learning%20self-supervised%20representations%20that%20are%20invariant%20and%20equivariant%0Ato%20transformations%20is%20crucial%20for%20advancing%20beyond%20traditional%20visual%0Aclassification%20tasks.%20However%2C%20many%20methods%20rely%20on%20predictor%20architectures%20to%0Aencode%20equivariance%2C%20despite%20evidence%20that%20architectural%20choices%2C%20such%20as%0Acapsule%20networks%2C%20inherently%20excel%20at%20learning%20interpretable%20pose-aware%0Arepresentations.%20To%20explore%20this%2C%20we%20introduce%20EquiCaps%20%28Equivariant%20Capsule%0ANetwork%29%2C%20a%20capsule-based%20approach%20to%20pose-aware%20self-supervision%20that%0Aeliminates%20the%20need%20for%20a%20specialised%20predictor%20for%20enforcing%20equivariance.%0AInstead%2C%20we%20leverage%20the%20intrinsic%20pose-awareness%20capabilities%20of%20capsules%20to%0Aimprove%20performance%20in%20pose%20estimation%20tasks.%20To%20further%20challenge%20our%0Aassumptions%2C%20we%20increase%20task%20complexity%20via%20multi-geometric%20transformations%20to%0Aenable%20a%20more%20thorough%20evaluation%20of%20invariance%20and%20equivariance%20by%20introducing%0A3DIEBench-T%2C%20an%20extension%20of%20a%203D%20object-rendering%20benchmark%20dataset.%20Empirical%0Aresults%20demonstrate%20that%20EquiCaps%20outperforms%20prior%20state-of-the-art%0Aequivariant%20methods%20on%20rotation%20prediction%2C%20achieving%20a%20supervised-level%20%24R%5E2%24%0Aof%200.78%20on%20the%203DIEBench%20rotation%20prediction%20benchmark%20and%20improving%20upon%20SIE%0Aand%20CapsIE%20by%200.05%20and%200.04%20%24R%5E2%24%2C%20respectively.%20Moreover%2C%20in%20contrast%20to%0Anon-capsule-based%20equivariant%20approaches%2C%20EquiCaps%20maintains%20robust%20equivariant%0Aperformance%20under%20combined%20geometric%20transformations%2C%20underscoring%20its%0Ageneralisation%20capabilities%20and%20the%20promise%20of%20predictor-free%20capsule%0Aarchitectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquiCaps%253A%2520Predictor-Free%2520Pose-Aware%2520Pre-Trained%2520Capsule%2520Networks%26entry.906535625%3DAthinoulla%2520Konstantinou%2520and%2520Georgios%2520Leontidis%2520and%2520Mamatha%2520Thota%2520and%2520Aiden%2520Durrant%26entry.1292438233%3D%2520%2520Learning%2520self-supervised%2520representations%2520that%2520are%2520invariant%2520and%2520equivariant%250Ato%2520transformations%2520is%2520crucial%2520for%2520advancing%2520beyond%2520traditional%2520visual%250Aclassification%2520tasks.%2520However%252C%2520many%2520methods%2520rely%2520on%2520predictor%2520architectures%2520to%250Aencode%2520equivariance%252C%2520despite%2520evidence%2520that%2520architectural%2520choices%252C%2520such%2520as%250Acapsule%2520networks%252C%2520inherently%2520excel%2520at%2520learning%2520interpretable%2520pose-aware%250Arepresentations.%2520To%2520explore%2520this%252C%2520we%2520introduce%2520EquiCaps%2520%2528Equivariant%2520Capsule%250ANetwork%2529%252C%2520a%2520capsule-based%2520approach%2520to%2520pose-aware%2520self-supervision%2520that%250Aeliminates%2520the%2520need%2520for%2520a%2520specialised%2520predictor%2520for%2520enforcing%2520equivariance.%250AInstead%252C%2520we%2520leverage%2520the%2520intrinsic%2520pose-awareness%2520capabilities%2520of%2520capsules%2520to%250Aimprove%2520performance%2520in%2520pose%2520estimation%2520tasks.%2520To%2520further%2520challenge%2520our%250Aassumptions%252C%2520we%2520increase%2520task%2520complexity%2520via%2520multi-geometric%2520transformations%2520to%250Aenable%2520a%2520more%2520thorough%2520evaluation%2520of%2520invariance%2520and%2520equivariance%2520by%2520introducing%250A3DIEBench-T%252C%2520an%2520extension%2520of%2520a%25203D%2520object-rendering%2520benchmark%2520dataset.%2520Empirical%250Aresults%2520demonstrate%2520that%2520EquiCaps%2520outperforms%2520prior%2520state-of-the-art%250Aequivariant%2520methods%2520on%2520rotation%2520prediction%252C%2520achieving%2520a%2520supervised-level%2520%2524R%255E2%2524%250Aof%25200.78%2520on%2520the%25203DIEBench%2520rotation%2520prediction%2520benchmark%2520and%2520improving%2520upon%2520SIE%250Aand%2520CapsIE%2520by%25200.05%2520and%25200.04%2520%2524R%255E2%2524%252C%2520respectively.%2520Moreover%252C%2520in%2520contrast%2520to%250Anon-capsule-based%2520equivariant%2520approaches%252C%2520EquiCaps%2520maintains%2520robust%2520equivariant%250Aperformance%2520under%2520combined%2520geometric%2520transformations%252C%2520underscoring%2520its%250Ageneralisation%2520capabilities%2520and%2520the%2520promise%2520of%2520predictor-free%2520capsule%250Aarchitectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EquiCaps%3A%20Predictor-Free%20Pose-Aware%20Pre-Trained%20Capsule%20Networks&entry.906535625=Athinoulla%20Konstantinou%20and%20Georgios%20Leontidis%20and%20Mamatha%20Thota%20and%20Aiden%20Durrant&entry.1292438233=%20%20Learning%20self-supervised%20representations%20that%20are%20invariant%20and%20equivariant%0Ato%20transformations%20is%20crucial%20for%20advancing%20beyond%20traditional%20visual%0Aclassification%20tasks.%20However%2C%20many%20methods%20rely%20on%20predictor%20architectures%20to%0Aencode%20equivariance%2C%20despite%20evidence%20that%20architectural%20choices%2C%20such%20as%0Acapsule%20networks%2C%20inherently%20excel%20at%20learning%20interpretable%20pose-aware%0Arepresentations.%20To%20explore%20this%2C%20we%20introduce%20EquiCaps%20%28Equivariant%20Capsule%0ANetwork%29%2C%20a%20capsule-based%20approach%20to%20pose-aware%20self-supervision%20that%0Aeliminates%20the%20need%20for%20a%20specialised%20predictor%20for%20enforcing%20equivariance.%0AInstead%2C%20we%20leverage%20the%20intrinsic%20pose-awareness%20capabilities%20of%20capsules%20to%0Aimprove%20performance%20in%20pose%20estimation%20tasks.%20To%20further%20challenge%20our%0Aassumptions%2C%20we%20increase%20task%20complexity%20via%20multi-geometric%20transformations%20to%0Aenable%20a%20more%20thorough%20evaluation%20of%20invariance%20and%20equivariance%20by%20introducing%0A3DIEBench-T%2C%20an%20extension%20of%20a%203D%20object-rendering%20benchmark%20dataset.%20Empirical%0Aresults%20demonstrate%20that%20EquiCaps%20outperforms%20prior%20state-of-the-art%0Aequivariant%20methods%20on%20rotation%20prediction%2C%20achieving%20a%20supervised-level%20%24R%5E2%24%0Aof%200.78%20on%20the%203DIEBench%20rotation%20prediction%20benchmark%20and%20improving%20upon%20SIE%0Aand%20CapsIE%20by%200.05%20and%200.04%20%24R%5E2%24%2C%20respectively.%20Moreover%2C%20in%20contrast%20to%0Anon-capsule-based%20equivariant%20approaches%2C%20EquiCaps%20maintains%20robust%20equivariant%0Aperformance%20under%20combined%20geometric%20transformations%2C%20underscoring%20its%0Ageneralisation%20capabilities%20and%20the%20promise%20of%20predictor-free%20capsule%0Aarchitectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09895v1&entry.124074799=Read"},
{"title": "Generalizing Supervised Contrastive learning: A Projection Perspective", "author": "Minoh Jeong and Alfred Hero", "abstract": "  Self-supervised contrastive learning (SSCL) has emerged as a powerful\nparadigm for representation learning and has been studied from multiple\nperspectives, including mutual information and geometric viewpoints. However,\nsupervised contrastive (SupCon) approaches have received comparatively little\nattention in this context: for instance, while InfoNCE used in SSCL is known to\nform a lower bound on mutual information (MI), the relationship between SupCon\nand MI remains unexplored. To address this gap, we introduce ProjNCE, a\ngeneralization of the InfoNCE loss that unifies supervised and self-supervised\ncontrastive objectives by incorporating projection functions and an adjustment\nterm for negative pairs. We prove that ProjNCE constitutes a valid MI bound and\naffords greater flexibility in selecting projection strategies for class\nembeddings. Building on this flexibility, we further explore the centroid-based\nclass embeddings in SupCon by exploring a variety of projection methods.\nExtensive experiments on multiple datasets and settings demonstrate that\nProjNCE consistently outperforms both SupCon and standard cross-entropy\ntraining. Our work thus refines SupCon along two complementary\nperspective--mutual information interpretation and projection design--and\noffers broadly applicable improvements whenever SupCon serves as the\nfoundational contrastive objective.\n", "link": "http://arxiv.org/abs/2506.09810v1", "date": "2025-06-11", "relevancy": 2.6293, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5422}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5177}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizing%20Supervised%20Contrastive%20learning%3A%20A%20Projection%20Perspective&body=Title%3A%20Generalizing%20Supervised%20Contrastive%20learning%3A%20A%20Projection%20Perspective%0AAuthor%3A%20Minoh%20Jeong%20and%20Alfred%20Hero%0AAbstract%3A%20%20%20Self-supervised%20contrastive%20learning%20%28SSCL%29%20has%20emerged%20as%20a%20powerful%0Aparadigm%20for%20representation%20learning%20and%20has%20been%20studied%20from%20multiple%0Aperspectives%2C%20including%20mutual%20information%20and%20geometric%20viewpoints.%20However%2C%0Asupervised%20contrastive%20%28SupCon%29%20approaches%20have%20received%20comparatively%20little%0Aattention%20in%20this%20context%3A%20for%20instance%2C%20while%20InfoNCE%20used%20in%20SSCL%20is%20known%20to%0Aform%20a%20lower%20bound%20on%20mutual%20information%20%28MI%29%2C%20the%20relationship%20between%20SupCon%0Aand%20MI%20remains%20unexplored.%20To%20address%20this%20gap%2C%20we%20introduce%20ProjNCE%2C%20a%0Ageneralization%20of%20the%20InfoNCE%20loss%20that%20unifies%20supervised%20and%20self-supervised%0Acontrastive%20objectives%20by%20incorporating%20projection%20functions%20and%20an%20adjustment%0Aterm%20for%20negative%20pairs.%20We%20prove%20that%20ProjNCE%20constitutes%20a%20valid%20MI%20bound%20and%0Aaffords%20greater%20flexibility%20in%20selecting%20projection%20strategies%20for%20class%0Aembeddings.%20Building%20on%20this%20flexibility%2C%20we%20further%20explore%20the%20centroid-based%0Aclass%20embeddings%20in%20SupCon%20by%20exploring%20a%20variety%20of%20projection%20methods.%0AExtensive%20experiments%20on%20multiple%20datasets%20and%20settings%20demonstrate%20that%0AProjNCE%20consistently%20outperforms%20both%20SupCon%20and%20standard%20cross-entropy%0Atraining.%20Our%20work%20thus%20refines%20SupCon%20along%20two%20complementary%0Aperspective--mutual%20information%20interpretation%20and%20projection%20design--and%0Aoffers%20broadly%20applicable%20improvements%20whenever%20SupCon%20serves%20as%20the%0Afoundational%20contrastive%20objective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizing%2520Supervised%2520Contrastive%2520learning%253A%2520A%2520Projection%2520Perspective%26entry.906535625%3DMinoh%2520Jeong%2520and%2520Alfred%2520Hero%26entry.1292438233%3D%2520%2520Self-supervised%2520contrastive%2520learning%2520%2528SSCL%2529%2520has%2520emerged%2520as%2520a%2520powerful%250Aparadigm%2520for%2520representation%2520learning%2520and%2520has%2520been%2520studied%2520from%2520multiple%250Aperspectives%252C%2520including%2520mutual%2520information%2520and%2520geometric%2520viewpoints.%2520However%252C%250Asupervised%2520contrastive%2520%2528SupCon%2529%2520approaches%2520have%2520received%2520comparatively%2520little%250Aattention%2520in%2520this%2520context%253A%2520for%2520instance%252C%2520while%2520InfoNCE%2520used%2520in%2520SSCL%2520is%2520known%2520to%250Aform%2520a%2520lower%2520bound%2520on%2520mutual%2520information%2520%2528MI%2529%252C%2520the%2520relationship%2520between%2520SupCon%250Aand%2520MI%2520remains%2520unexplored.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520ProjNCE%252C%2520a%250Ageneralization%2520of%2520the%2520InfoNCE%2520loss%2520that%2520unifies%2520supervised%2520and%2520self-supervised%250Acontrastive%2520objectives%2520by%2520incorporating%2520projection%2520functions%2520and%2520an%2520adjustment%250Aterm%2520for%2520negative%2520pairs.%2520We%2520prove%2520that%2520ProjNCE%2520constitutes%2520a%2520valid%2520MI%2520bound%2520and%250Aaffords%2520greater%2520flexibility%2520in%2520selecting%2520projection%2520strategies%2520for%2520class%250Aembeddings.%2520Building%2520on%2520this%2520flexibility%252C%2520we%2520further%2520explore%2520the%2520centroid-based%250Aclass%2520embeddings%2520in%2520SupCon%2520by%2520exploring%2520a%2520variety%2520of%2520projection%2520methods.%250AExtensive%2520experiments%2520on%2520multiple%2520datasets%2520and%2520settings%2520demonstrate%2520that%250AProjNCE%2520consistently%2520outperforms%2520both%2520SupCon%2520and%2520standard%2520cross-entropy%250Atraining.%2520Our%2520work%2520thus%2520refines%2520SupCon%2520along%2520two%2520complementary%250Aperspective--mutual%2520information%2520interpretation%2520and%2520projection%2520design--and%250Aoffers%2520broadly%2520applicable%2520improvements%2520whenever%2520SupCon%2520serves%2520as%2520the%250Afoundational%2520contrastive%2520objective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizing%20Supervised%20Contrastive%20learning%3A%20A%20Projection%20Perspective&entry.906535625=Minoh%20Jeong%20and%20Alfred%20Hero&entry.1292438233=%20%20Self-supervised%20contrastive%20learning%20%28SSCL%29%20has%20emerged%20as%20a%20powerful%0Aparadigm%20for%20representation%20learning%20and%20has%20been%20studied%20from%20multiple%0Aperspectives%2C%20including%20mutual%20information%20and%20geometric%20viewpoints.%20However%2C%0Asupervised%20contrastive%20%28SupCon%29%20approaches%20have%20received%20comparatively%20little%0Aattention%20in%20this%20context%3A%20for%20instance%2C%20while%20InfoNCE%20used%20in%20SSCL%20is%20known%20to%0Aform%20a%20lower%20bound%20on%20mutual%20information%20%28MI%29%2C%20the%20relationship%20between%20SupCon%0Aand%20MI%20remains%20unexplored.%20To%20address%20this%20gap%2C%20we%20introduce%20ProjNCE%2C%20a%0Ageneralization%20of%20the%20InfoNCE%20loss%20that%20unifies%20supervised%20and%20self-supervised%0Acontrastive%20objectives%20by%20incorporating%20projection%20functions%20and%20an%20adjustment%0Aterm%20for%20negative%20pairs.%20We%20prove%20that%20ProjNCE%20constitutes%20a%20valid%20MI%20bound%20and%0Aaffords%20greater%20flexibility%20in%20selecting%20projection%20strategies%20for%20class%0Aembeddings.%20Building%20on%20this%20flexibility%2C%20we%20further%20explore%20the%20centroid-based%0Aclass%20embeddings%20in%20SupCon%20by%20exploring%20a%20variety%20of%20projection%20methods.%0AExtensive%20experiments%20on%20multiple%20datasets%20and%20settings%20demonstrate%20that%0AProjNCE%20consistently%20outperforms%20both%20SupCon%20and%20standard%20cross-entropy%0Atraining.%20Our%20work%20thus%20refines%20SupCon%20along%20two%20complementary%0Aperspective--mutual%20information%20interpretation%20and%20projection%20design--and%0Aoffers%20broadly%20applicable%20improvements%20whenever%20SupCon%20serves%20as%20the%0Afoundational%20contrastive%20objective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09810v1&entry.124074799=Read"},
{"title": "Griffin: Towards a Graph-Centric Relational Database Foundation Model", "author": "Yanbo Wang and Xiyuan Wang and Quan Gan and Minjie Wang and Qibin Yang and David Wipf and Muhan Zhang", "abstract": "  We introduce Griffin, the first foundation model attemptation designed\nspecifically for Relational Databases (RDBs). Unlike previous smaller models\nfocused on single RDB tasks, Griffin unifies the data encoder and task decoder\nto handle diverse tasks. Additionally, we enhance the architecture by\nincorporating a cross-attention module and a novel aggregator. Griffin utilizes\npretraining on both single-table and RDB datasets, employing advanced encoders\nfor categorical, numerical, and metadata features, along with innovative\ncomponents such as cross-attention modules and enhanced message-passing neural\nnetworks (MPNNs) to capture the complexities of relational data. Evaluated on\nlarge-scale, heterogeneous, and temporal graphs extracted from RDBs across\nvarious domains (spanning over 150 million nodes), Griffin demonstrates\nsuperior or comparable performance to individually trained models, excels in\nlow-data scenarios, and shows strong transferability with similarity and\ndiversity in pretraining across new datasets and tasks, highlighting its\npotential as a universally applicable foundation model for RDBs. Code available\nat https://github.com/yanxwb/Griffin.\n", "link": "http://arxiv.org/abs/2505.05568v2", "date": "2025-06-11", "relevancy": 2.6154, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5366}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Griffin%3A%20Towards%20a%20Graph-Centric%20Relational%20Database%20Foundation%20Model&body=Title%3A%20Griffin%3A%20Towards%20a%20Graph-Centric%20Relational%20Database%20Foundation%20Model%0AAuthor%3A%20Yanbo%20Wang%20and%20Xiyuan%20Wang%20and%20Quan%20Gan%20and%20Minjie%20Wang%20and%20Qibin%20Yang%20and%20David%20Wipf%20and%20Muhan%20Zhang%0AAbstract%3A%20%20%20We%20introduce%20Griffin%2C%20the%20first%20foundation%20model%20attemptation%20designed%0Aspecifically%20for%20Relational%20Databases%20%28RDBs%29.%20Unlike%20previous%20smaller%20models%0Afocused%20on%20single%20RDB%20tasks%2C%20Griffin%20unifies%20the%20data%20encoder%20and%20task%20decoder%0Ato%20handle%20diverse%20tasks.%20Additionally%2C%20we%20enhance%20the%20architecture%20by%0Aincorporating%20a%20cross-attention%20module%20and%20a%20novel%20aggregator.%20Griffin%20utilizes%0Apretraining%20on%20both%20single-table%20and%20RDB%20datasets%2C%20employing%20advanced%20encoders%0Afor%20categorical%2C%20numerical%2C%20and%20metadata%20features%2C%20along%20with%20innovative%0Acomponents%20such%20as%20cross-attention%20modules%20and%20enhanced%20message-passing%20neural%0Anetworks%20%28MPNNs%29%20to%20capture%20the%20complexities%20of%20relational%20data.%20Evaluated%20on%0Alarge-scale%2C%20heterogeneous%2C%20and%20temporal%20graphs%20extracted%20from%20RDBs%20across%0Avarious%20domains%20%28spanning%20over%20150%20million%20nodes%29%2C%20Griffin%20demonstrates%0Asuperior%20or%20comparable%20performance%20to%20individually%20trained%20models%2C%20excels%20in%0Alow-data%20scenarios%2C%20and%20shows%20strong%20transferability%20with%20similarity%20and%0Adiversity%20in%20pretraining%20across%20new%20datasets%20and%20tasks%2C%20highlighting%20its%0Apotential%20as%20a%20universally%20applicable%20foundation%20model%20for%20RDBs.%20Code%20available%0Aat%20https%3A//github.com/yanxwb/Griffin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05568v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGriffin%253A%2520Towards%2520a%2520Graph-Centric%2520Relational%2520Database%2520Foundation%2520Model%26entry.906535625%3DYanbo%2520Wang%2520and%2520Xiyuan%2520Wang%2520and%2520Quan%2520Gan%2520and%2520Minjie%2520Wang%2520and%2520Qibin%2520Yang%2520and%2520David%2520Wipf%2520and%2520Muhan%2520Zhang%26entry.1292438233%3D%2520%2520We%2520introduce%2520Griffin%252C%2520the%2520first%2520foundation%2520model%2520attemptation%2520designed%250Aspecifically%2520for%2520Relational%2520Databases%2520%2528RDBs%2529.%2520Unlike%2520previous%2520smaller%2520models%250Afocused%2520on%2520single%2520RDB%2520tasks%252C%2520Griffin%2520unifies%2520the%2520data%2520encoder%2520and%2520task%2520decoder%250Ato%2520handle%2520diverse%2520tasks.%2520Additionally%252C%2520we%2520enhance%2520the%2520architecture%2520by%250Aincorporating%2520a%2520cross-attention%2520module%2520and%2520a%2520novel%2520aggregator.%2520Griffin%2520utilizes%250Apretraining%2520on%2520both%2520single-table%2520and%2520RDB%2520datasets%252C%2520employing%2520advanced%2520encoders%250Afor%2520categorical%252C%2520numerical%252C%2520and%2520metadata%2520features%252C%2520along%2520with%2520innovative%250Acomponents%2520such%2520as%2520cross-attention%2520modules%2520and%2520enhanced%2520message-passing%2520neural%250Anetworks%2520%2528MPNNs%2529%2520to%2520capture%2520the%2520complexities%2520of%2520relational%2520data.%2520Evaluated%2520on%250Alarge-scale%252C%2520heterogeneous%252C%2520and%2520temporal%2520graphs%2520extracted%2520from%2520RDBs%2520across%250Avarious%2520domains%2520%2528spanning%2520over%2520150%2520million%2520nodes%2529%252C%2520Griffin%2520demonstrates%250Asuperior%2520or%2520comparable%2520performance%2520to%2520individually%2520trained%2520models%252C%2520excels%2520in%250Alow-data%2520scenarios%252C%2520and%2520shows%2520strong%2520transferability%2520with%2520similarity%2520and%250Adiversity%2520in%2520pretraining%2520across%2520new%2520datasets%2520and%2520tasks%252C%2520highlighting%2520its%250Apotential%2520as%2520a%2520universally%2520applicable%2520foundation%2520model%2520for%2520RDBs.%2520Code%2520available%250Aat%2520https%253A//github.com/yanxwb/Griffin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05568v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Griffin%3A%20Towards%20a%20Graph-Centric%20Relational%20Database%20Foundation%20Model&entry.906535625=Yanbo%20Wang%20and%20Xiyuan%20Wang%20and%20Quan%20Gan%20and%20Minjie%20Wang%20and%20Qibin%20Yang%20and%20David%20Wipf%20and%20Muhan%20Zhang&entry.1292438233=%20%20We%20introduce%20Griffin%2C%20the%20first%20foundation%20model%20attemptation%20designed%0Aspecifically%20for%20Relational%20Databases%20%28RDBs%29.%20Unlike%20previous%20smaller%20models%0Afocused%20on%20single%20RDB%20tasks%2C%20Griffin%20unifies%20the%20data%20encoder%20and%20task%20decoder%0Ato%20handle%20diverse%20tasks.%20Additionally%2C%20we%20enhance%20the%20architecture%20by%0Aincorporating%20a%20cross-attention%20module%20and%20a%20novel%20aggregator.%20Griffin%20utilizes%0Apretraining%20on%20both%20single-table%20and%20RDB%20datasets%2C%20employing%20advanced%20encoders%0Afor%20categorical%2C%20numerical%2C%20and%20metadata%20features%2C%20along%20with%20innovative%0Acomponents%20such%20as%20cross-attention%20modules%20and%20enhanced%20message-passing%20neural%0Anetworks%20%28MPNNs%29%20to%20capture%20the%20complexities%20of%20relational%20data.%20Evaluated%20on%0Alarge-scale%2C%20heterogeneous%2C%20and%20temporal%20graphs%20extracted%20from%20RDBs%20across%0Avarious%20domains%20%28spanning%20over%20150%20million%20nodes%29%2C%20Griffin%20demonstrates%0Asuperior%20or%20comparable%20performance%20to%20individually%20trained%20models%2C%20excels%20in%0Alow-data%20scenarios%2C%20and%20shows%20strong%20transferability%20with%20similarity%20and%0Adiversity%20in%20pretraining%20across%20new%20datasets%20and%20tasks%2C%20highlighting%20its%0Apotential%20as%20a%20universally%20applicable%20foundation%20model%20for%20RDBs.%20Code%20available%0Aat%20https%3A//github.com/yanxwb/Griffin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05568v2&entry.124074799=Read"},
{"title": "The Emergence of Abstract Thought in Large Language Models Beyond Any\n  Language", "author": "Yuxin Chen and Yiran Zhao and Yang Zhang and An Zhang and Kenji Kawaguchi and Shafiq Joty and Junnan Li and Tat-Seng Chua and Michael Qizhe Shieh and Wenxuan Zhang", "abstract": "  As large language models (LLMs) continue to advance, their capacity to\nfunction effectively across a diverse range of languages has shown marked\nimprovement. Preliminary studies observe that the hidden activations of LLMs\noften resemble English, even when responding to non-English prompts. This has\nled to the widespread assumption that LLMs may \"think\" in English. However,\nmore recent results showing strong multilingual performance, even surpassing\nEnglish performance on specific tasks in other languages, challenge this view.\nIn this work, we find that LLMs progressively develop a core language-agnostic\nparameter space-a remarkably small subset of parameters whose deactivation\nresults in significant performance degradation across all languages. This\ncompact yet critical set of parameters underlies the model's ability to\ngeneralize beyond individual languages, supporting the emergence of abstract\nthought that is not tied to any specific linguistic system. Specifically, we\nidentify language-related neurons-those are consistently activated during the\nprocessing of particular languages, and categorize them as either shared\n(active across multiple languages) or exclusive (specific to one). As LLMs\nundergo continued development over time, we observe a marked increase in both\nthe proportion and functional importance of shared neurons, while exclusive\nneurons progressively diminish in influence. These shared neurons constitute\nthe backbone of the core language-agnostic parameter space, supporting the\nemergence of abstract thought. Motivated by these insights, we propose\nneuron-specific training strategies tailored to LLMs' language-agnostic levels\nat different development stages. Experiments across diverse LLM families\nsupport our approach.\n", "link": "http://arxiv.org/abs/2506.09890v1", "date": "2025-06-11", "relevancy": 2.6108, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.549}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.549}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Emergence%20of%20Abstract%20Thought%20in%20Large%20Language%20Models%20Beyond%20Any%0A%20%20Language&body=Title%3A%20The%20Emergence%20of%20Abstract%20Thought%20in%20Large%20Language%20Models%20Beyond%20Any%0A%20%20Language%0AAuthor%3A%20Yuxin%20Chen%20and%20Yiran%20Zhao%20and%20Yang%20Zhang%20and%20An%20Zhang%20and%20Kenji%20Kawaguchi%20and%20Shafiq%20Joty%20and%20Junnan%20Li%20and%20Tat-Seng%20Chua%20and%20Michael%20Qizhe%20Shieh%20and%20Wenxuan%20Zhang%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20continue%20to%20advance%2C%20their%20capacity%20to%0Afunction%20effectively%20across%20a%20diverse%20range%20of%20languages%20has%20shown%20marked%0Aimprovement.%20Preliminary%20studies%20observe%20that%20the%20hidden%20activations%20of%20LLMs%0Aoften%20resemble%20English%2C%20even%20when%20responding%20to%20non-English%20prompts.%20This%20has%0Aled%20to%20the%20widespread%20assumption%20that%20LLMs%20may%20%22think%22%20in%20English.%20However%2C%0Amore%20recent%20results%20showing%20strong%20multilingual%20performance%2C%20even%20surpassing%0AEnglish%20performance%20on%20specific%20tasks%20in%20other%20languages%2C%20challenge%20this%20view.%0AIn%20this%20work%2C%20we%20find%20that%20LLMs%20progressively%20develop%20a%20core%20language-agnostic%0Aparameter%20space-a%20remarkably%20small%20subset%20of%20parameters%20whose%20deactivation%0Aresults%20in%20significant%20performance%20degradation%20across%20all%20languages.%20This%0Acompact%20yet%20critical%20set%20of%20parameters%20underlies%20the%20model%27s%20ability%20to%0Ageneralize%20beyond%20individual%20languages%2C%20supporting%20the%20emergence%20of%20abstract%0Athought%20that%20is%20not%20tied%20to%20any%20specific%20linguistic%20system.%20Specifically%2C%20we%0Aidentify%20language-related%20neurons-those%20are%20consistently%20activated%20during%20the%0Aprocessing%20of%20particular%20languages%2C%20and%20categorize%20them%20as%20either%20shared%0A%28active%20across%20multiple%20languages%29%20or%20exclusive%20%28specific%20to%20one%29.%20As%20LLMs%0Aundergo%20continued%20development%20over%20time%2C%20we%20observe%20a%20marked%20increase%20in%20both%0Athe%20proportion%20and%20functional%20importance%20of%20shared%20neurons%2C%20while%20exclusive%0Aneurons%20progressively%20diminish%20in%20influence.%20These%20shared%20neurons%20constitute%0Athe%20backbone%20of%20the%20core%20language-agnostic%20parameter%20space%2C%20supporting%20the%0Aemergence%20of%20abstract%20thought.%20Motivated%20by%20these%20insights%2C%20we%20propose%0Aneuron-specific%20training%20strategies%20tailored%20to%20LLMs%27%20language-agnostic%20levels%0Aat%20different%20development%20stages.%20Experiments%20across%20diverse%20LLM%20families%0Asupport%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09890v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Emergence%2520of%2520Abstract%2520Thought%2520in%2520Large%2520Language%2520Models%2520Beyond%2520Any%250A%2520%2520Language%26entry.906535625%3DYuxin%2520Chen%2520and%2520Yiran%2520Zhao%2520and%2520Yang%2520Zhang%2520and%2520An%2520Zhang%2520and%2520Kenji%2520Kawaguchi%2520and%2520Shafiq%2520Joty%2520and%2520Junnan%2520Li%2520and%2520Tat-Seng%2520Chua%2520and%2520Michael%2520Qizhe%2520Shieh%2520and%2520Wenxuan%2520Zhang%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520continue%2520to%2520advance%252C%2520their%2520capacity%2520to%250Afunction%2520effectively%2520across%2520a%2520diverse%2520range%2520of%2520languages%2520has%2520shown%2520marked%250Aimprovement.%2520Preliminary%2520studies%2520observe%2520that%2520the%2520hidden%2520activations%2520of%2520LLMs%250Aoften%2520resemble%2520English%252C%2520even%2520when%2520responding%2520to%2520non-English%2520prompts.%2520This%2520has%250Aled%2520to%2520the%2520widespread%2520assumption%2520that%2520LLMs%2520may%2520%2522think%2522%2520in%2520English.%2520However%252C%250Amore%2520recent%2520results%2520showing%2520strong%2520multilingual%2520performance%252C%2520even%2520surpassing%250AEnglish%2520performance%2520on%2520specific%2520tasks%2520in%2520other%2520languages%252C%2520challenge%2520this%2520view.%250AIn%2520this%2520work%252C%2520we%2520find%2520that%2520LLMs%2520progressively%2520develop%2520a%2520core%2520language-agnostic%250Aparameter%2520space-a%2520remarkably%2520small%2520subset%2520of%2520parameters%2520whose%2520deactivation%250Aresults%2520in%2520significant%2520performance%2520degradation%2520across%2520all%2520languages.%2520This%250Acompact%2520yet%2520critical%2520set%2520of%2520parameters%2520underlies%2520the%2520model%2527s%2520ability%2520to%250Ageneralize%2520beyond%2520individual%2520languages%252C%2520supporting%2520the%2520emergence%2520of%2520abstract%250Athought%2520that%2520is%2520not%2520tied%2520to%2520any%2520specific%2520linguistic%2520system.%2520Specifically%252C%2520we%250Aidentify%2520language-related%2520neurons-those%2520are%2520consistently%2520activated%2520during%2520the%250Aprocessing%2520of%2520particular%2520languages%252C%2520and%2520categorize%2520them%2520as%2520either%2520shared%250A%2528active%2520across%2520multiple%2520languages%2529%2520or%2520exclusive%2520%2528specific%2520to%2520one%2529.%2520As%2520LLMs%250Aundergo%2520continued%2520development%2520over%2520time%252C%2520we%2520observe%2520a%2520marked%2520increase%2520in%2520both%250Athe%2520proportion%2520and%2520functional%2520importance%2520of%2520shared%2520neurons%252C%2520while%2520exclusive%250Aneurons%2520progressively%2520diminish%2520in%2520influence.%2520These%2520shared%2520neurons%2520constitute%250Athe%2520backbone%2520of%2520the%2520core%2520language-agnostic%2520parameter%2520space%252C%2520supporting%2520the%250Aemergence%2520of%2520abstract%2520thought.%2520Motivated%2520by%2520these%2520insights%252C%2520we%2520propose%250Aneuron-specific%2520training%2520strategies%2520tailored%2520to%2520LLMs%2527%2520language-agnostic%2520levels%250Aat%2520different%2520development%2520stages.%2520Experiments%2520across%2520diverse%2520LLM%2520families%250Asupport%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09890v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Emergence%20of%20Abstract%20Thought%20in%20Large%20Language%20Models%20Beyond%20Any%0A%20%20Language&entry.906535625=Yuxin%20Chen%20and%20Yiran%20Zhao%20and%20Yang%20Zhang%20and%20An%20Zhang%20and%20Kenji%20Kawaguchi%20and%20Shafiq%20Joty%20and%20Junnan%20Li%20and%20Tat-Seng%20Chua%20and%20Michael%20Qizhe%20Shieh%20and%20Wenxuan%20Zhang&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20continue%20to%20advance%2C%20their%20capacity%20to%0Afunction%20effectively%20across%20a%20diverse%20range%20of%20languages%20has%20shown%20marked%0Aimprovement.%20Preliminary%20studies%20observe%20that%20the%20hidden%20activations%20of%20LLMs%0Aoften%20resemble%20English%2C%20even%20when%20responding%20to%20non-English%20prompts.%20This%20has%0Aled%20to%20the%20widespread%20assumption%20that%20LLMs%20may%20%22think%22%20in%20English.%20However%2C%0Amore%20recent%20results%20showing%20strong%20multilingual%20performance%2C%20even%20surpassing%0AEnglish%20performance%20on%20specific%20tasks%20in%20other%20languages%2C%20challenge%20this%20view.%0AIn%20this%20work%2C%20we%20find%20that%20LLMs%20progressively%20develop%20a%20core%20language-agnostic%0Aparameter%20space-a%20remarkably%20small%20subset%20of%20parameters%20whose%20deactivation%0Aresults%20in%20significant%20performance%20degradation%20across%20all%20languages.%20This%0Acompact%20yet%20critical%20set%20of%20parameters%20underlies%20the%20model%27s%20ability%20to%0Ageneralize%20beyond%20individual%20languages%2C%20supporting%20the%20emergence%20of%20abstract%0Athought%20that%20is%20not%20tied%20to%20any%20specific%20linguistic%20system.%20Specifically%2C%20we%0Aidentify%20language-related%20neurons-those%20are%20consistently%20activated%20during%20the%0Aprocessing%20of%20particular%20languages%2C%20and%20categorize%20them%20as%20either%20shared%0A%28active%20across%20multiple%20languages%29%20or%20exclusive%20%28specific%20to%20one%29.%20As%20LLMs%0Aundergo%20continued%20development%20over%20time%2C%20we%20observe%20a%20marked%20increase%20in%20both%0Athe%20proportion%20and%20functional%20importance%20of%20shared%20neurons%2C%20while%20exclusive%0Aneurons%20progressively%20diminish%20in%20influence.%20These%20shared%20neurons%20constitute%0Athe%20backbone%20of%20the%20core%20language-agnostic%20parameter%20space%2C%20supporting%20the%0Aemergence%20of%20abstract%20thought.%20Motivated%20by%20these%20insights%2C%20we%20propose%0Aneuron-specific%20training%20strategies%20tailored%20to%20LLMs%27%20language-agnostic%20levels%0Aat%20different%20development%20stages.%20Experiments%20across%20diverse%20LLM%20families%0Asupport%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09890v1&entry.124074799=Read"},
{"title": "SpikeSMOKE: Spiking Neural Networks for Monocular 3D Object Detection\n  with Cross-Scale Gated Coding", "author": "Xuemei Chen and Huamin Wang and Hangchi Shen and Shukai Duan and Shiping Wen and Tingwen Huang", "abstract": "  Low energy consumption for 3D object detection is an important research area\nbecause of the increasing energy consumption with their wide application in\nfields such as autonomous driving. The spiking neural networks (SNNs) with\nlow-power consumption characteristics can provide a novel solution for this\nresearch. Therefore, we apply SNNs to monocular 3D object detection and propose\nthe SpikeSMOKE architecture in this paper, which is a new attempt for low-power\nmonocular 3D object detection. As we all know, discrete signals of SNNs will\ngenerate information loss and limit their feature expression ability compared\nwith the artificial neural networks (ANNs).In order to address this issue,\ninspired by the filtering mechanism of biological neuronal synapses, we propose\na cross-scale gated coding mechanism(CSGC), which can enhance feature\nrepresentation by combining cross-scale fusion of attentional methods and gated\nfiltering mechanisms.In addition, to reduce the computation and increase the\nspeed of training, we present a novel light-weight residual block that can\nmaintain spiking computing paradigm and the highest possible detection\nperformance. Compared to the baseline SpikeSMOKE under the 3D Object Detection,\nthe proposed SpikeSMOKE with CSGC can achieve 11.78 (+2.82, Easy), 10.69 (+3.2,\nModerate), and 10.48 (+3.17, Hard) on the KITTI autonomous driving dataset by\nAP|R11 at 0.7 IoU threshold, respectively. It is important to note that the\nresults of SpikeSMOKE can significantly reduce energy consumption compared to\nthe results on SMOKE. For example,the energy consumption can be reduced by\n72.2% on the hard category, while the detection performance is reduced by only\n4%. SpikeSMOKE-L (lightweight) can further reduce the amount of parameters by 3\ntimes and computation by 10 times compared to SMOKE.\n", "link": "http://arxiv.org/abs/2506.07737v2", "date": "2025-06-11", "relevancy": 2.61, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.531}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5184}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpikeSMOKE%3A%20Spiking%20Neural%20Networks%20for%20Monocular%203D%20Object%20Detection%0A%20%20with%20Cross-Scale%20Gated%20Coding&body=Title%3A%20SpikeSMOKE%3A%20Spiking%20Neural%20Networks%20for%20Monocular%203D%20Object%20Detection%0A%20%20with%20Cross-Scale%20Gated%20Coding%0AAuthor%3A%20Xuemei%20Chen%20and%20Huamin%20Wang%20and%20Hangchi%20Shen%20and%20Shukai%20Duan%20and%20Shiping%20Wen%20and%20Tingwen%20Huang%0AAbstract%3A%20%20%20Low%20energy%20consumption%20for%203D%20object%20detection%20is%20an%20important%20research%20area%0Abecause%20of%20the%20increasing%20energy%20consumption%20with%20their%20wide%20application%20in%0Afields%20such%20as%20autonomous%20driving.%20The%20spiking%20neural%20networks%20%28SNNs%29%20with%0Alow-power%20consumption%20characteristics%20can%20provide%20a%20novel%20solution%20for%20this%0Aresearch.%20Therefore%2C%20we%20apply%20SNNs%20to%20monocular%203D%20object%20detection%20and%20propose%0Athe%20SpikeSMOKE%20architecture%20in%20this%20paper%2C%20which%20is%20a%20new%20attempt%20for%20low-power%0Amonocular%203D%20object%20detection.%20As%20we%20all%20know%2C%20discrete%20signals%20of%20SNNs%20will%0Agenerate%20information%20loss%20and%20limit%20their%20feature%20expression%20ability%20compared%0Awith%20the%20artificial%20neural%20networks%20%28ANNs%29.In%20order%20to%20address%20this%20issue%2C%0Ainspired%20by%20the%20filtering%20mechanism%20of%20biological%20neuronal%20synapses%2C%20we%20propose%0Aa%20cross-scale%20gated%20coding%20mechanism%28CSGC%29%2C%20which%20can%20enhance%20feature%0Arepresentation%20by%20combining%20cross-scale%20fusion%20of%20attentional%20methods%20and%20gated%0Afiltering%20mechanisms.In%20addition%2C%20to%20reduce%20the%20computation%20and%20increase%20the%0Aspeed%20of%20training%2C%20we%20present%20a%20novel%20light-weight%20residual%20block%20that%20can%0Amaintain%20spiking%20computing%20paradigm%20and%20the%20highest%20possible%20detection%0Aperformance.%20Compared%20to%20the%20baseline%20SpikeSMOKE%20under%20the%203D%20Object%20Detection%2C%0Athe%20proposed%20SpikeSMOKE%20with%20CSGC%20can%20achieve%2011.78%20%28%2B2.82%2C%20Easy%29%2C%2010.69%20%28%2B3.2%2C%0AModerate%29%2C%20and%2010.48%20%28%2B3.17%2C%20Hard%29%20on%20the%20KITTI%20autonomous%20driving%20dataset%20by%0AAP%7CR11%20at%200.7%20IoU%20threshold%2C%20respectively.%20It%20is%20important%20to%20note%20that%20the%0Aresults%20of%20SpikeSMOKE%20can%20significantly%20reduce%20energy%20consumption%20compared%20to%0Athe%20results%20on%20SMOKE.%20For%20example%2Cthe%20energy%20consumption%20can%20be%20reduced%20by%0A72.2%25%20on%20the%20hard%20category%2C%20while%20the%20detection%20performance%20is%20reduced%20by%20only%0A4%25.%20SpikeSMOKE-L%20%28lightweight%29%20can%20further%20reduce%20the%20amount%20of%20parameters%20by%203%0Atimes%20and%20computation%20by%2010%20times%20compared%20to%20SMOKE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07737v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpikeSMOKE%253A%2520Spiking%2520Neural%2520Networks%2520for%2520Monocular%25203D%2520Object%2520Detection%250A%2520%2520with%2520Cross-Scale%2520Gated%2520Coding%26entry.906535625%3DXuemei%2520Chen%2520and%2520Huamin%2520Wang%2520and%2520Hangchi%2520Shen%2520and%2520Shukai%2520Duan%2520and%2520Shiping%2520Wen%2520and%2520Tingwen%2520Huang%26entry.1292438233%3D%2520%2520Low%2520energy%2520consumption%2520for%25203D%2520object%2520detection%2520is%2520an%2520important%2520research%2520area%250Abecause%2520of%2520the%2520increasing%2520energy%2520consumption%2520with%2520their%2520wide%2520application%2520in%250Afields%2520such%2520as%2520autonomous%2520driving.%2520The%2520spiking%2520neural%2520networks%2520%2528SNNs%2529%2520with%250Alow-power%2520consumption%2520characteristics%2520can%2520provide%2520a%2520novel%2520solution%2520for%2520this%250Aresearch.%2520Therefore%252C%2520we%2520apply%2520SNNs%2520to%2520monocular%25203D%2520object%2520detection%2520and%2520propose%250Athe%2520SpikeSMOKE%2520architecture%2520in%2520this%2520paper%252C%2520which%2520is%2520a%2520new%2520attempt%2520for%2520low-power%250Amonocular%25203D%2520object%2520detection.%2520As%2520we%2520all%2520know%252C%2520discrete%2520signals%2520of%2520SNNs%2520will%250Agenerate%2520information%2520loss%2520and%2520limit%2520their%2520feature%2520expression%2520ability%2520compared%250Awith%2520the%2520artificial%2520neural%2520networks%2520%2528ANNs%2529.In%2520order%2520to%2520address%2520this%2520issue%252C%250Ainspired%2520by%2520the%2520filtering%2520mechanism%2520of%2520biological%2520neuronal%2520synapses%252C%2520we%2520propose%250Aa%2520cross-scale%2520gated%2520coding%2520mechanism%2528CSGC%2529%252C%2520which%2520can%2520enhance%2520feature%250Arepresentation%2520by%2520combining%2520cross-scale%2520fusion%2520of%2520attentional%2520methods%2520and%2520gated%250Afiltering%2520mechanisms.In%2520addition%252C%2520to%2520reduce%2520the%2520computation%2520and%2520increase%2520the%250Aspeed%2520of%2520training%252C%2520we%2520present%2520a%2520novel%2520light-weight%2520residual%2520block%2520that%2520can%250Amaintain%2520spiking%2520computing%2520paradigm%2520and%2520the%2520highest%2520possible%2520detection%250Aperformance.%2520Compared%2520to%2520the%2520baseline%2520SpikeSMOKE%2520under%2520the%25203D%2520Object%2520Detection%252C%250Athe%2520proposed%2520SpikeSMOKE%2520with%2520CSGC%2520can%2520achieve%252011.78%2520%2528%252B2.82%252C%2520Easy%2529%252C%252010.69%2520%2528%252B3.2%252C%250AModerate%2529%252C%2520and%252010.48%2520%2528%252B3.17%252C%2520Hard%2529%2520on%2520the%2520KITTI%2520autonomous%2520driving%2520dataset%2520by%250AAP%257CR11%2520at%25200.7%2520IoU%2520threshold%252C%2520respectively.%2520It%2520is%2520important%2520to%2520note%2520that%2520the%250Aresults%2520of%2520SpikeSMOKE%2520can%2520significantly%2520reduce%2520energy%2520consumption%2520compared%2520to%250Athe%2520results%2520on%2520SMOKE.%2520For%2520example%252Cthe%2520energy%2520consumption%2520can%2520be%2520reduced%2520by%250A72.2%2525%2520on%2520the%2520hard%2520category%252C%2520while%2520the%2520detection%2520performance%2520is%2520reduced%2520by%2520only%250A4%2525.%2520SpikeSMOKE-L%2520%2528lightweight%2529%2520can%2520further%2520reduce%2520the%2520amount%2520of%2520parameters%2520by%25203%250Atimes%2520and%2520computation%2520by%252010%2520times%2520compared%2520to%2520SMOKE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07737v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpikeSMOKE%3A%20Spiking%20Neural%20Networks%20for%20Monocular%203D%20Object%20Detection%0A%20%20with%20Cross-Scale%20Gated%20Coding&entry.906535625=Xuemei%20Chen%20and%20Huamin%20Wang%20and%20Hangchi%20Shen%20and%20Shukai%20Duan%20and%20Shiping%20Wen%20and%20Tingwen%20Huang&entry.1292438233=%20%20Low%20energy%20consumption%20for%203D%20object%20detection%20is%20an%20important%20research%20area%0Abecause%20of%20the%20increasing%20energy%20consumption%20with%20their%20wide%20application%20in%0Afields%20such%20as%20autonomous%20driving.%20The%20spiking%20neural%20networks%20%28SNNs%29%20with%0Alow-power%20consumption%20characteristics%20can%20provide%20a%20novel%20solution%20for%20this%0Aresearch.%20Therefore%2C%20we%20apply%20SNNs%20to%20monocular%203D%20object%20detection%20and%20propose%0Athe%20SpikeSMOKE%20architecture%20in%20this%20paper%2C%20which%20is%20a%20new%20attempt%20for%20low-power%0Amonocular%203D%20object%20detection.%20As%20we%20all%20know%2C%20discrete%20signals%20of%20SNNs%20will%0Agenerate%20information%20loss%20and%20limit%20their%20feature%20expression%20ability%20compared%0Awith%20the%20artificial%20neural%20networks%20%28ANNs%29.In%20order%20to%20address%20this%20issue%2C%0Ainspired%20by%20the%20filtering%20mechanism%20of%20biological%20neuronal%20synapses%2C%20we%20propose%0Aa%20cross-scale%20gated%20coding%20mechanism%28CSGC%29%2C%20which%20can%20enhance%20feature%0Arepresentation%20by%20combining%20cross-scale%20fusion%20of%20attentional%20methods%20and%20gated%0Afiltering%20mechanisms.In%20addition%2C%20to%20reduce%20the%20computation%20and%20increase%20the%0Aspeed%20of%20training%2C%20we%20present%20a%20novel%20light-weight%20residual%20block%20that%20can%0Amaintain%20spiking%20computing%20paradigm%20and%20the%20highest%20possible%20detection%0Aperformance.%20Compared%20to%20the%20baseline%20SpikeSMOKE%20under%20the%203D%20Object%20Detection%2C%0Athe%20proposed%20SpikeSMOKE%20with%20CSGC%20can%20achieve%2011.78%20%28%2B2.82%2C%20Easy%29%2C%2010.69%20%28%2B3.2%2C%0AModerate%29%2C%20and%2010.48%20%28%2B3.17%2C%20Hard%29%20on%20the%20KITTI%20autonomous%20driving%20dataset%20by%0AAP%7CR11%20at%200.7%20IoU%20threshold%2C%20respectively.%20It%20is%20important%20to%20note%20that%20the%0Aresults%20of%20SpikeSMOKE%20can%20significantly%20reduce%20energy%20consumption%20compared%20to%0Athe%20results%20on%20SMOKE.%20For%20example%2Cthe%20energy%20consumption%20can%20be%20reduced%20by%0A72.2%25%20on%20the%20hard%20category%2C%20while%20the%20detection%20performance%20is%20reduced%20by%20only%0A4%25.%20SpikeSMOKE-L%20%28lightweight%29%20can%20further%20reduce%20the%20amount%20of%20parameters%20by%203%0Atimes%20and%20computation%20by%2010%20times%20compared%20to%20SMOKE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07737v2&entry.124074799=Read"},
{"title": "Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over\n  Videos", "author": "Benjamin Reichman and Constantin Patsch and Jack Truxal and Atishay Jain and Larry Heck", "abstract": "  In outside knowledge visual question answering (OK-VQA), the model must\nidentify relevant visual information within an image and incorporate external\nknowledge to accurately respond to a question. Extending this task to a\nvisually grounded dialogue setting based on videos, a conversational model must\nboth recognize pertinent visual details over time and answer questions where\nthe required information is not necessarily present in the visual information.\nMoreover, the context of the overall conversation must be considered for the\nsubsequent dialogue. To explore this task, we introduce a dataset comprised of\n$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$\ninterleaved dialogue turns. While the dialogue context is visually grounded in\nspecific video segments, the questions further require external knowledge that\nis not visually present. Thus, the model not only has to identify relevant\nvideo parts but also leverage external knowledge to converse within the\ndialogue. We further provide several baselines evaluated on our dataset and\nshow future challenges associated with this task. The dataset is made publicly\navailable here: https://github.com/c-patsch/OKCV.\n", "link": "http://arxiv.org/abs/2506.09953v1", "date": "2025-06-11", "relevancy": 2.5993, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5349}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5349}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Outside%20Knowledge%20Conversational%20Video%20%28OKCV%29%20Dataset%20--%20Dialoguing%20over%0A%20%20Videos&body=Title%3A%20Outside%20Knowledge%20Conversational%20Video%20%28OKCV%29%20Dataset%20--%20Dialoguing%20over%0A%20%20Videos%0AAuthor%3A%20Benjamin%20Reichman%20and%20Constantin%20Patsch%20and%20Jack%20Truxal%20and%20Atishay%20Jain%20and%20Larry%20Heck%0AAbstract%3A%20%20%20In%20outside%20knowledge%20visual%20question%20answering%20%28OK-VQA%29%2C%20the%20model%20must%0Aidentify%20relevant%20visual%20information%20within%20an%20image%20and%20incorporate%20external%0Aknowledge%20to%20accurately%20respond%20to%20a%20question.%20Extending%20this%20task%20to%20a%0Avisually%20grounded%20dialogue%20setting%20based%20on%20videos%2C%20a%20conversational%20model%20must%0Aboth%20recognize%20pertinent%20visual%20details%20over%20time%20and%20answer%20questions%20where%0Athe%20required%20information%20is%20not%20necessarily%20present%20in%20the%20visual%20information.%0AMoreover%2C%20the%20context%20of%20the%20overall%20conversation%20must%20be%20considered%20for%20the%0Asubsequent%20dialogue.%20To%20explore%20this%20task%2C%20we%20introduce%20a%20dataset%20comprised%20of%0A%242%2C017%24%20videos%20with%20%245%2C986%24%20human-annotated%20dialogues%20consisting%20of%20%2440%2C954%24%0Ainterleaved%20dialogue%20turns.%20While%20the%20dialogue%20context%20is%20visually%20grounded%20in%0Aspecific%20video%20segments%2C%20the%20questions%20further%20require%20external%20knowledge%20that%0Ais%20not%20visually%20present.%20Thus%2C%20the%20model%20not%20only%20has%20to%20identify%20relevant%0Avideo%20parts%20but%20also%20leverage%20external%20knowledge%20to%20converse%20within%20the%0Adialogue.%20We%20further%20provide%20several%20baselines%20evaluated%20on%20our%20dataset%20and%0Ashow%20future%20challenges%20associated%20with%20this%20task.%20The%20dataset%20is%20made%20publicly%0Aavailable%20here%3A%20https%3A//github.com/c-patsch/OKCV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOutside%2520Knowledge%2520Conversational%2520Video%2520%2528OKCV%2529%2520Dataset%2520--%2520Dialoguing%2520over%250A%2520%2520Videos%26entry.906535625%3DBenjamin%2520Reichman%2520and%2520Constantin%2520Patsch%2520and%2520Jack%2520Truxal%2520and%2520Atishay%2520Jain%2520and%2520Larry%2520Heck%26entry.1292438233%3D%2520%2520In%2520outside%2520knowledge%2520visual%2520question%2520answering%2520%2528OK-VQA%2529%252C%2520the%2520model%2520must%250Aidentify%2520relevant%2520visual%2520information%2520within%2520an%2520image%2520and%2520incorporate%2520external%250Aknowledge%2520to%2520accurately%2520respond%2520to%2520a%2520question.%2520Extending%2520this%2520task%2520to%2520a%250Avisually%2520grounded%2520dialogue%2520setting%2520based%2520on%2520videos%252C%2520a%2520conversational%2520model%2520must%250Aboth%2520recognize%2520pertinent%2520visual%2520details%2520over%2520time%2520and%2520answer%2520questions%2520where%250Athe%2520required%2520information%2520is%2520not%2520necessarily%2520present%2520in%2520the%2520visual%2520information.%250AMoreover%252C%2520the%2520context%2520of%2520the%2520overall%2520conversation%2520must%2520be%2520considered%2520for%2520the%250Asubsequent%2520dialogue.%2520To%2520explore%2520this%2520task%252C%2520we%2520introduce%2520a%2520dataset%2520comprised%2520of%250A%25242%252C017%2524%2520videos%2520with%2520%25245%252C986%2524%2520human-annotated%2520dialogues%2520consisting%2520of%2520%252440%252C954%2524%250Ainterleaved%2520dialogue%2520turns.%2520While%2520the%2520dialogue%2520context%2520is%2520visually%2520grounded%2520in%250Aspecific%2520video%2520segments%252C%2520the%2520questions%2520further%2520require%2520external%2520knowledge%2520that%250Ais%2520not%2520visually%2520present.%2520Thus%252C%2520the%2520model%2520not%2520only%2520has%2520to%2520identify%2520relevant%250Avideo%2520parts%2520but%2520also%2520leverage%2520external%2520knowledge%2520to%2520converse%2520within%2520the%250Adialogue.%2520We%2520further%2520provide%2520several%2520baselines%2520evaluated%2520on%2520our%2520dataset%2520and%250Ashow%2520future%2520challenges%2520associated%2520with%2520this%2520task.%2520The%2520dataset%2520is%2520made%2520publicly%250Aavailable%2520here%253A%2520https%253A//github.com/c-patsch/OKCV.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Outside%20Knowledge%20Conversational%20Video%20%28OKCV%29%20Dataset%20--%20Dialoguing%20over%0A%20%20Videos&entry.906535625=Benjamin%20Reichman%20and%20Constantin%20Patsch%20and%20Jack%20Truxal%20and%20Atishay%20Jain%20and%20Larry%20Heck&entry.1292438233=%20%20In%20outside%20knowledge%20visual%20question%20answering%20%28OK-VQA%29%2C%20the%20model%20must%0Aidentify%20relevant%20visual%20information%20within%20an%20image%20and%20incorporate%20external%0Aknowledge%20to%20accurately%20respond%20to%20a%20question.%20Extending%20this%20task%20to%20a%0Avisually%20grounded%20dialogue%20setting%20based%20on%20videos%2C%20a%20conversational%20model%20must%0Aboth%20recognize%20pertinent%20visual%20details%20over%20time%20and%20answer%20questions%20where%0Athe%20required%20information%20is%20not%20necessarily%20present%20in%20the%20visual%20information.%0AMoreover%2C%20the%20context%20of%20the%20overall%20conversation%20must%20be%20considered%20for%20the%0Asubsequent%20dialogue.%20To%20explore%20this%20task%2C%20we%20introduce%20a%20dataset%20comprised%20of%0A%242%2C017%24%20videos%20with%20%245%2C986%24%20human-annotated%20dialogues%20consisting%20of%20%2440%2C954%24%0Ainterleaved%20dialogue%20turns.%20While%20the%20dialogue%20context%20is%20visually%20grounded%20in%0Aspecific%20video%20segments%2C%20the%20questions%20further%20require%20external%20knowledge%20that%0Ais%20not%20visually%20present.%20Thus%2C%20the%20model%20not%20only%20has%20to%20identify%20relevant%0Avideo%20parts%20but%20also%20leverage%20external%20knowledge%20to%20converse%20within%20the%0Adialogue.%20We%20further%20provide%20several%20baselines%20evaluated%20on%20our%20dataset%20and%0Ashow%20future%20challenges%20associated%20with%20this%20task.%20The%20dataset%20is%20made%20publicly%0Aavailable%20here%3A%20https%3A//github.com/c-patsch/OKCV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09953v1&entry.124074799=Read"},
{"title": "A theoretical framework for self-supervised contrastive learning for\n  continuous dependent data", "author": "Alexander Marusov and Alexander Yuhay and Alexey Zaytsev", "abstract": "  Self-supervised learning (SSL) has emerged as a powerful approach to learning\nrepresentations, particularly in the field of computer vision. However, its\napplication to dependent data, such as temporal and spatio-temporal domains,\nremains underexplored. Besides, traditional contrastive SSL methods often\nassume \\emph{semantic independence between samples}, which does not hold for\ndependent data exhibiting complex correlations. We propose a novel theoretical\nframework for contrastive SSL tailored to \\emph{continuous dependent data},\nwhich allows the nearest samples to be semantically close to each other. In\nparticular, we propose two possible \\textit{ground truth similarity measures}\nbetween objects -- \\emph{hard} and \\emph{soft} closeness. Under it, we derive\nan analytical form for the \\textit{estimated similarity matrix} that\naccommodates both types of closeness between samples, thereby introducing\ndependency-aware loss functions. We validate our approach, \\emph{Dependent\nTS2Vec}, on temporal and spatio-temporal downstream problems. Given the\ndependency patterns presented in the data, our approach surpasses modern ones\nfor dependent data, highlighting the effectiveness of our theoretically\ngrounded loss functions for SSL in capturing spatio-temporal dependencies.\nSpecifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with\naccuracy improvements of $4.17$\\% and $2.08$\\%, respectively. Furthermore, on\nthe drought classification task, which involves complex spatio-temporal\npatterns, our method achieves a $7$\\% higher ROC-AUC score.\n", "link": "http://arxiv.org/abs/2506.09785v1", "date": "2025-06-11", "relevancy": 2.5932, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5293}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5225}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20theoretical%20framework%20for%20self-supervised%20contrastive%20learning%20for%0A%20%20continuous%20dependent%20data&body=Title%3A%20A%20theoretical%20framework%20for%20self-supervised%20contrastive%20learning%20for%0A%20%20continuous%20dependent%20data%0AAuthor%3A%20Alexander%20Marusov%20and%20Alexander%20Yuhay%20and%20Alexey%20Zaytsev%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20has%20emerged%20as%20a%20powerful%20approach%20to%20learning%0Arepresentations%2C%20particularly%20in%20the%20field%20of%20computer%20vision.%20However%2C%20its%0Aapplication%20to%20dependent%20data%2C%20such%20as%20temporal%20and%20spatio-temporal%20domains%2C%0Aremains%20underexplored.%20Besides%2C%20traditional%20contrastive%20SSL%20methods%20often%0Aassume%20%5Cemph%7Bsemantic%20independence%20between%20samples%7D%2C%20which%20does%20not%20hold%20for%0Adependent%20data%20exhibiting%20complex%20correlations.%20We%20propose%20a%20novel%20theoretical%0Aframework%20for%20contrastive%20SSL%20tailored%20to%20%5Cemph%7Bcontinuous%20dependent%20data%7D%2C%0Awhich%20allows%20the%20nearest%20samples%20to%20be%20semantically%20close%20to%20each%20other.%20In%0Aparticular%2C%20we%20propose%20two%20possible%20%5Ctextit%7Bground%20truth%20similarity%20measures%7D%0Abetween%20objects%20--%20%5Cemph%7Bhard%7D%20and%20%5Cemph%7Bsoft%7D%20closeness.%20Under%20it%2C%20we%20derive%0Aan%20analytical%20form%20for%20the%20%5Ctextit%7Bestimated%20similarity%20matrix%7D%20that%0Aaccommodates%20both%20types%20of%20closeness%20between%20samples%2C%20thereby%20introducing%0Adependency-aware%20loss%20functions.%20We%20validate%20our%20approach%2C%20%5Cemph%7BDependent%0ATS2Vec%7D%2C%20on%20temporal%20and%20spatio-temporal%20downstream%20problems.%20Given%20the%0Adependency%20patterns%20presented%20in%20the%20data%2C%20our%20approach%20surpasses%20modern%20ones%0Afor%20dependent%20data%2C%20highlighting%20the%20effectiveness%20of%20our%20theoretically%0Agrounded%20loss%20functions%20for%20SSL%20in%20capturing%20spatio-temporal%20dependencies.%0ASpecifically%2C%20we%20outperform%20TS2Vec%20on%20the%20standard%20UEA%20and%20UCR%20benchmarks%2C%20with%0Aaccuracy%20improvements%20of%20%244.17%24%5C%25%20and%20%242.08%24%5C%25%2C%20respectively.%20Furthermore%2C%20on%0Athe%20drought%20classification%20task%2C%20which%20involves%20complex%20spatio-temporal%0Apatterns%2C%20our%20method%20achieves%20a%20%247%24%5C%25%20higher%20ROC-AUC%20score.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520theoretical%2520framework%2520for%2520self-supervised%2520contrastive%2520learning%2520for%250A%2520%2520continuous%2520dependent%2520data%26entry.906535625%3DAlexander%2520Marusov%2520and%2520Alexander%2520Yuhay%2520and%2520Alexey%2520Zaytsev%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520approach%2520to%2520learning%250Arepresentations%252C%2520particularly%2520in%2520the%2520field%2520of%2520computer%2520vision.%2520However%252C%2520its%250Aapplication%2520to%2520dependent%2520data%252C%2520such%2520as%2520temporal%2520and%2520spatio-temporal%2520domains%252C%250Aremains%2520underexplored.%2520Besides%252C%2520traditional%2520contrastive%2520SSL%2520methods%2520often%250Aassume%2520%255Cemph%257Bsemantic%2520independence%2520between%2520samples%257D%252C%2520which%2520does%2520not%2520hold%2520for%250Adependent%2520data%2520exhibiting%2520complex%2520correlations.%2520We%2520propose%2520a%2520novel%2520theoretical%250Aframework%2520for%2520contrastive%2520SSL%2520tailored%2520to%2520%255Cemph%257Bcontinuous%2520dependent%2520data%257D%252C%250Awhich%2520allows%2520the%2520nearest%2520samples%2520to%2520be%2520semantically%2520close%2520to%2520each%2520other.%2520In%250Aparticular%252C%2520we%2520propose%2520two%2520possible%2520%255Ctextit%257Bground%2520truth%2520similarity%2520measures%257D%250Abetween%2520objects%2520--%2520%255Cemph%257Bhard%257D%2520and%2520%255Cemph%257Bsoft%257D%2520closeness.%2520Under%2520it%252C%2520we%2520derive%250Aan%2520analytical%2520form%2520for%2520the%2520%255Ctextit%257Bestimated%2520similarity%2520matrix%257D%2520that%250Aaccommodates%2520both%2520types%2520of%2520closeness%2520between%2520samples%252C%2520thereby%2520introducing%250Adependency-aware%2520loss%2520functions.%2520We%2520validate%2520our%2520approach%252C%2520%255Cemph%257BDependent%250ATS2Vec%257D%252C%2520on%2520temporal%2520and%2520spatio-temporal%2520downstream%2520problems.%2520Given%2520the%250Adependency%2520patterns%2520presented%2520in%2520the%2520data%252C%2520our%2520approach%2520surpasses%2520modern%2520ones%250Afor%2520dependent%2520data%252C%2520highlighting%2520the%2520effectiveness%2520of%2520our%2520theoretically%250Agrounded%2520loss%2520functions%2520for%2520SSL%2520in%2520capturing%2520spatio-temporal%2520dependencies.%250ASpecifically%252C%2520we%2520outperform%2520TS2Vec%2520on%2520the%2520standard%2520UEA%2520and%2520UCR%2520benchmarks%252C%2520with%250Aaccuracy%2520improvements%2520of%2520%25244.17%2524%255C%2525%2520and%2520%25242.08%2524%255C%2525%252C%2520respectively.%2520Furthermore%252C%2520on%250Athe%2520drought%2520classification%2520task%252C%2520which%2520involves%2520complex%2520spatio-temporal%250Apatterns%252C%2520our%2520method%2520achieves%2520a%2520%25247%2524%255C%2525%2520higher%2520ROC-AUC%2520score.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20theoretical%20framework%20for%20self-supervised%20contrastive%20learning%20for%0A%20%20continuous%20dependent%20data&entry.906535625=Alexander%20Marusov%20and%20Alexander%20Yuhay%20and%20Alexey%20Zaytsev&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20has%20emerged%20as%20a%20powerful%20approach%20to%20learning%0Arepresentations%2C%20particularly%20in%20the%20field%20of%20computer%20vision.%20However%2C%20its%0Aapplication%20to%20dependent%20data%2C%20such%20as%20temporal%20and%20spatio-temporal%20domains%2C%0Aremains%20underexplored.%20Besides%2C%20traditional%20contrastive%20SSL%20methods%20often%0Aassume%20%5Cemph%7Bsemantic%20independence%20between%20samples%7D%2C%20which%20does%20not%20hold%20for%0Adependent%20data%20exhibiting%20complex%20correlations.%20We%20propose%20a%20novel%20theoretical%0Aframework%20for%20contrastive%20SSL%20tailored%20to%20%5Cemph%7Bcontinuous%20dependent%20data%7D%2C%0Awhich%20allows%20the%20nearest%20samples%20to%20be%20semantically%20close%20to%20each%20other.%20In%0Aparticular%2C%20we%20propose%20two%20possible%20%5Ctextit%7Bground%20truth%20similarity%20measures%7D%0Abetween%20objects%20--%20%5Cemph%7Bhard%7D%20and%20%5Cemph%7Bsoft%7D%20closeness.%20Under%20it%2C%20we%20derive%0Aan%20analytical%20form%20for%20the%20%5Ctextit%7Bestimated%20similarity%20matrix%7D%20that%0Aaccommodates%20both%20types%20of%20closeness%20between%20samples%2C%20thereby%20introducing%0Adependency-aware%20loss%20functions.%20We%20validate%20our%20approach%2C%20%5Cemph%7BDependent%0ATS2Vec%7D%2C%20on%20temporal%20and%20spatio-temporal%20downstream%20problems.%20Given%20the%0Adependency%20patterns%20presented%20in%20the%20data%2C%20our%20approach%20surpasses%20modern%20ones%0Afor%20dependent%20data%2C%20highlighting%20the%20effectiveness%20of%20our%20theoretically%0Agrounded%20loss%20functions%20for%20SSL%20in%20capturing%20spatio-temporal%20dependencies.%0ASpecifically%2C%20we%20outperform%20TS2Vec%20on%20the%20standard%20UEA%20and%20UCR%20benchmarks%2C%20with%0Aaccuracy%20improvements%20of%20%244.17%24%5C%25%20and%20%242.08%24%5C%25%2C%20respectively.%20Furthermore%2C%20on%0Athe%20drought%20classification%20task%2C%20which%20involves%20complex%20spatio-temporal%0Apatterns%2C%20our%20method%20achieves%20a%20%247%24%5C%25%20higher%20ROC-AUC%20score.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09785v1&entry.124074799=Read"},
{"title": "Discovering Forbidden Topics in Language Models", "author": "Can Rager and Chris Wendler and Rohit Gandikota and David Bau", "abstract": "  Refusal discovery is the task of identifying the full set of topics that a\nlanguage model refuses to discuss. We introduce this new problem setting and\ndevelop a refusal discovery method, Iterated Prefill Crawler (IPC), that uses\ntoken prefilling to find forbidden topics. We benchmark IPC on Tulu-3-8B, an\nopen-source model with public safety tuning data. Our crawler manages to\nretrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale\nthe crawler to a frontier model using the prefilling option of Claude-Haiku.\nFinally, we crawl three widely used open-weight models: Llama-3.3-70B and two\nof its variants finetuned for reasoning: DeepSeek-R1-70B and\nPerplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with\ncensorship tuning: The model exhibits \"thought suppression\" behavior that\nindicates memorization of CCP-aligned responses. Although\nPerplexity-R1-1776-70B is robust to censorship, IPC elicits CCP-aligned\nrefusals answers in the quantized model. Our findings highlight the critical\nneed for refusal discovery methods to detect biases, boundaries, and alignment\nfailures of AI systems.\n", "link": "http://arxiv.org/abs/2505.17441v3", "date": "2025-06-11", "relevancy": 2.53, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5236}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovering%20Forbidden%20Topics%20in%20Language%20Models&body=Title%3A%20Discovering%20Forbidden%20Topics%20in%20Language%20Models%0AAuthor%3A%20Can%20Rager%20and%20Chris%20Wendler%20and%20Rohit%20Gandikota%20and%20David%20Bau%0AAbstract%3A%20%20%20Refusal%20discovery%20is%20the%20task%20of%20identifying%20the%20full%20set%20of%20topics%20that%20a%0Alanguage%20model%20refuses%20to%20discuss.%20We%20introduce%20this%20new%20problem%20setting%20and%0Adevelop%20a%20refusal%20discovery%20method%2C%20Iterated%20Prefill%20Crawler%20%28IPC%29%2C%20that%20uses%0Atoken%20prefilling%20to%20find%20forbidden%20topics.%20We%20benchmark%20IPC%20on%20Tulu-3-8B%2C%20an%0Aopen-source%20model%20with%20public%20safety%20tuning%20data.%20Our%20crawler%20manages%20to%0Aretrieve%2031%20out%20of%2036%20topics%20within%20a%20budget%20of%201000%20prompts.%20Next%2C%20we%20scale%0Athe%20crawler%20to%20a%20frontier%20model%20using%20the%20prefilling%20option%20of%20Claude-Haiku.%0AFinally%2C%20we%20crawl%20three%20widely%20used%20open-weight%20models%3A%20Llama-3.3-70B%20and%20two%0Aof%20its%20variants%20finetuned%20for%20reasoning%3A%20DeepSeek-R1-70B%20and%0APerplexity-R1-1776-70B.%20DeepSeek-R1-70B%20reveals%20patterns%20consistent%20with%0Acensorship%20tuning%3A%20The%20model%20exhibits%20%22thought%20suppression%22%20behavior%20that%0Aindicates%20memorization%20of%20CCP-aligned%20responses.%20Although%0APerplexity-R1-1776-70B%20is%20robust%20to%20censorship%2C%20IPC%20elicits%20CCP-aligned%0Arefusals%20answers%20in%20the%20quantized%20model.%20Our%20findings%20highlight%20the%20critical%0Aneed%20for%20refusal%20discovery%20methods%20to%20detect%20biases%2C%20boundaries%2C%20and%20alignment%0Afailures%20of%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17441v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovering%2520Forbidden%2520Topics%2520in%2520Language%2520Models%26entry.906535625%3DCan%2520Rager%2520and%2520Chris%2520Wendler%2520and%2520Rohit%2520Gandikota%2520and%2520David%2520Bau%26entry.1292438233%3D%2520%2520Refusal%2520discovery%2520is%2520the%2520task%2520of%2520identifying%2520the%2520full%2520set%2520of%2520topics%2520that%2520a%250Alanguage%2520model%2520refuses%2520to%2520discuss.%2520We%2520introduce%2520this%2520new%2520problem%2520setting%2520and%250Adevelop%2520a%2520refusal%2520discovery%2520method%252C%2520Iterated%2520Prefill%2520Crawler%2520%2528IPC%2529%252C%2520that%2520uses%250Atoken%2520prefilling%2520to%2520find%2520forbidden%2520topics.%2520We%2520benchmark%2520IPC%2520on%2520Tulu-3-8B%252C%2520an%250Aopen-source%2520model%2520with%2520public%2520safety%2520tuning%2520data.%2520Our%2520crawler%2520manages%2520to%250Aretrieve%252031%2520out%2520of%252036%2520topics%2520within%2520a%2520budget%2520of%25201000%2520prompts.%2520Next%252C%2520we%2520scale%250Athe%2520crawler%2520to%2520a%2520frontier%2520model%2520using%2520the%2520prefilling%2520option%2520of%2520Claude-Haiku.%250AFinally%252C%2520we%2520crawl%2520three%2520widely%2520used%2520open-weight%2520models%253A%2520Llama-3.3-70B%2520and%2520two%250Aof%2520its%2520variants%2520finetuned%2520for%2520reasoning%253A%2520DeepSeek-R1-70B%2520and%250APerplexity-R1-1776-70B.%2520DeepSeek-R1-70B%2520reveals%2520patterns%2520consistent%2520with%250Acensorship%2520tuning%253A%2520The%2520model%2520exhibits%2520%2522thought%2520suppression%2522%2520behavior%2520that%250Aindicates%2520memorization%2520of%2520CCP-aligned%2520responses.%2520Although%250APerplexity-R1-1776-70B%2520is%2520robust%2520to%2520censorship%252C%2520IPC%2520elicits%2520CCP-aligned%250Arefusals%2520answers%2520in%2520the%2520quantized%2520model.%2520Our%2520findings%2520highlight%2520the%2520critical%250Aneed%2520for%2520refusal%2520discovery%2520methods%2520to%2520detect%2520biases%252C%2520boundaries%252C%2520and%2520alignment%250Afailures%2520of%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17441v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovering%20Forbidden%20Topics%20in%20Language%20Models&entry.906535625=Can%20Rager%20and%20Chris%20Wendler%20and%20Rohit%20Gandikota%20and%20David%20Bau&entry.1292438233=%20%20Refusal%20discovery%20is%20the%20task%20of%20identifying%20the%20full%20set%20of%20topics%20that%20a%0Alanguage%20model%20refuses%20to%20discuss.%20We%20introduce%20this%20new%20problem%20setting%20and%0Adevelop%20a%20refusal%20discovery%20method%2C%20Iterated%20Prefill%20Crawler%20%28IPC%29%2C%20that%20uses%0Atoken%20prefilling%20to%20find%20forbidden%20topics.%20We%20benchmark%20IPC%20on%20Tulu-3-8B%2C%20an%0Aopen-source%20model%20with%20public%20safety%20tuning%20data.%20Our%20crawler%20manages%20to%0Aretrieve%2031%20out%20of%2036%20topics%20within%20a%20budget%20of%201000%20prompts.%20Next%2C%20we%20scale%0Athe%20crawler%20to%20a%20frontier%20model%20using%20the%20prefilling%20option%20of%20Claude-Haiku.%0AFinally%2C%20we%20crawl%20three%20widely%20used%20open-weight%20models%3A%20Llama-3.3-70B%20and%20two%0Aof%20its%20variants%20finetuned%20for%20reasoning%3A%20DeepSeek-R1-70B%20and%0APerplexity-R1-1776-70B.%20DeepSeek-R1-70B%20reveals%20patterns%20consistent%20with%0Acensorship%20tuning%3A%20The%20model%20exhibits%20%22thought%20suppression%22%20behavior%20that%0Aindicates%20memorization%20of%20CCP-aligned%20responses.%20Although%0APerplexity-R1-1776-70B%20is%20robust%20to%20censorship%2C%20IPC%20elicits%20CCP-aligned%0Arefusals%20answers%20in%20the%20quantized%20model.%20Our%20findings%20highlight%20the%20critical%0Aneed%20for%20refusal%20discovery%20methods%20to%20detect%20biases%2C%20boundaries%2C%20and%20alignment%0Afailures%20of%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17441v3&entry.124074799=Read"},
{"title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio\n  Conditions", "author": "Zhenzhi Wang and Jiaqi Yang and Jianwen Jiang and Chao Liang and Gaojie Lin and Zerong Zheng and Ceyuan Yang and Dahua Lin", "abstract": "  End-to-end human animation with rich multi-modal conditions, e.g., text,\nimage and audio has achieved remarkable advancements in recent years. However,\nmost existing methods could only animate a single subject and inject conditions\nin a global manner, ignoring scenarios that multiple concepts could appears in\nthe same video with rich human-human interactions and human-object\ninteractions. Such global assumption prevents precise and per-identity control\nof multiple concepts including humans and objects, therefore hinders\napplications. In this work, we discard the single-entity assumption and\nintroduce a novel framework that enforces strong, region-specific binding of\nconditions from modalities to each identity's spatiotemporal footprint. Given\nreference images of multiple concepts, our method could automatically infer\nlayout information by leveraging a mask predictor to match appearance cues\nbetween the denoised video and each reference appearance. Furthermore, we\ninject local audio condition into its corresponding region to ensure\nlayout-aligned modality matching in a iterative manner. This design enables the\nhigh-quality generation of controllable multi-concept human-centric videos.\nEmpirical results and ablation studies validate the effectiveness of our\nexplicit layout control for multi-modal conditions compared to implicit\ncounterparts and other existing methods.\n", "link": "http://arxiv.org/abs/2506.09984v1", "date": "2025-06-11", "relevancy": 2.5278, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6512}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6229}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InterActHuman%3A%20Multi-Concept%20Human%20Animation%20with%20Layout-Aligned%20Audio%0A%20%20Conditions&body=Title%3A%20InterActHuman%3A%20Multi-Concept%20Human%20Animation%20with%20Layout-Aligned%20Audio%0A%20%20Conditions%0AAuthor%3A%20Zhenzhi%20Wang%20and%20Jiaqi%20Yang%20and%20Jianwen%20Jiang%20and%20Chao%20Liang%20and%20Gaojie%20Lin%20and%20Zerong%20Zheng%20and%20Ceyuan%20Yang%20and%20Dahua%20Lin%0AAbstract%3A%20%20%20End-to-end%20human%20animation%20with%20rich%20multi-modal%20conditions%2C%20e.g.%2C%20text%2C%0Aimage%20and%20audio%20has%20achieved%20remarkable%20advancements%20in%20recent%20years.%20However%2C%0Amost%20existing%20methods%20could%20only%20animate%20a%20single%20subject%20and%20inject%20conditions%0Ain%20a%20global%20manner%2C%20ignoring%20scenarios%20that%20multiple%20concepts%20could%20appears%20in%0Athe%20same%20video%20with%20rich%20human-human%20interactions%20and%20human-object%0Ainteractions.%20Such%20global%20assumption%20prevents%20precise%20and%20per-identity%20control%0Aof%20multiple%20concepts%20including%20humans%20and%20objects%2C%20therefore%20hinders%0Aapplications.%20In%20this%20work%2C%20we%20discard%20the%20single-entity%20assumption%20and%0Aintroduce%20a%20novel%20framework%20that%20enforces%20strong%2C%20region-specific%20binding%20of%0Aconditions%20from%20modalities%20to%20each%20identity%27s%20spatiotemporal%20footprint.%20Given%0Areference%20images%20of%20multiple%20concepts%2C%20our%20method%20could%20automatically%20infer%0Alayout%20information%20by%20leveraging%20a%20mask%20predictor%20to%20match%20appearance%20cues%0Abetween%20the%20denoised%20video%20and%20each%20reference%20appearance.%20Furthermore%2C%20we%0Ainject%20local%20audio%20condition%20into%20its%20corresponding%20region%20to%20ensure%0Alayout-aligned%20modality%20matching%20in%20a%20iterative%20manner.%20This%20design%20enables%20the%0Ahigh-quality%20generation%20of%20controllable%20multi-concept%20human-centric%20videos.%0AEmpirical%20results%20and%20ablation%20studies%20validate%20the%20effectiveness%20of%20our%0Aexplicit%20layout%20control%20for%20multi-modal%20conditions%20compared%20to%20implicit%0Acounterparts%20and%20other%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterActHuman%253A%2520Multi-Concept%2520Human%2520Animation%2520with%2520Layout-Aligned%2520Audio%250A%2520%2520Conditions%26entry.906535625%3DZhenzhi%2520Wang%2520and%2520Jiaqi%2520Yang%2520and%2520Jianwen%2520Jiang%2520and%2520Chao%2520Liang%2520and%2520Gaojie%2520Lin%2520and%2520Zerong%2520Zheng%2520and%2520Ceyuan%2520Yang%2520and%2520Dahua%2520Lin%26entry.1292438233%3D%2520%2520End-to-end%2520human%2520animation%2520with%2520rich%2520multi-modal%2520conditions%252C%2520e.g.%252C%2520text%252C%250Aimage%2520and%2520audio%2520has%2520achieved%2520remarkable%2520advancements%2520in%2520recent%2520years.%2520However%252C%250Amost%2520existing%2520methods%2520could%2520only%2520animate%2520a%2520single%2520subject%2520and%2520inject%2520conditions%250Ain%2520a%2520global%2520manner%252C%2520ignoring%2520scenarios%2520that%2520multiple%2520concepts%2520could%2520appears%2520in%250Athe%2520same%2520video%2520with%2520rich%2520human-human%2520interactions%2520and%2520human-object%250Ainteractions.%2520Such%2520global%2520assumption%2520prevents%2520precise%2520and%2520per-identity%2520control%250Aof%2520multiple%2520concepts%2520including%2520humans%2520and%2520objects%252C%2520therefore%2520hinders%250Aapplications.%2520In%2520this%2520work%252C%2520we%2520discard%2520the%2520single-entity%2520assumption%2520and%250Aintroduce%2520a%2520novel%2520framework%2520that%2520enforces%2520strong%252C%2520region-specific%2520binding%2520of%250Aconditions%2520from%2520modalities%2520to%2520each%2520identity%2527s%2520spatiotemporal%2520footprint.%2520Given%250Areference%2520images%2520of%2520multiple%2520concepts%252C%2520our%2520method%2520could%2520automatically%2520infer%250Alayout%2520information%2520by%2520leveraging%2520a%2520mask%2520predictor%2520to%2520match%2520appearance%2520cues%250Abetween%2520the%2520denoised%2520video%2520and%2520each%2520reference%2520appearance.%2520Furthermore%252C%2520we%250Ainject%2520local%2520audio%2520condition%2520into%2520its%2520corresponding%2520region%2520to%2520ensure%250Alayout-aligned%2520modality%2520matching%2520in%2520a%2520iterative%2520manner.%2520This%2520design%2520enables%2520the%250Ahigh-quality%2520generation%2520of%2520controllable%2520multi-concept%2520human-centric%2520videos.%250AEmpirical%2520results%2520and%2520ablation%2520studies%2520validate%2520the%2520effectiveness%2520of%2520our%250Aexplicit%2520layout%2520control%2520for%2520multi-modal%2520conditions%2520compared%2520to%2520implicit%250Acounterparts%2520and%2520other%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InterActHuman%3A%20Multi-Concept%20Human%20Animation%20with%20Layout-Aligned%20Audio%0A%20%20Conditions&entry.906535625=Zhenzhi%20Wang%20and%20Jiaqi%20Yang%20and%20Jianwen%20Jiang%20and%20Chao%20Liang%20and%20Gaojie%20Lin%20and%20Zerong%20Zheng%20and%20Ceyuan%20Yang%20and%20Dahua%20Lin&entry.1292438233=%20%20End-to-end%20human%20animation%20with%20rich%20multi-modal%20conditions%2C%20e.g.%2C%20text%2C%0Aimage%20and%20audio%20has%20achieved%20remarkable%20advancements%20in%20recent%20years.%20However%2C%0Amost%20existing%20methods%20could%20only%20animate%20a%20single%20subject%20and%20inject%20conditions%0Ain%20a%20global%20manner%2C%20ignoring%20scenarios%20that%20multiple%20concepts%20could%20appears%20in%0Athe%20same%20video%20with%20rich%20human-human%20interactions%20and%20human-object%0Ainteractions.%20Such%20global%20assumption%20prevents%20precise%20and%20per-identity%20control%0Aof%20multiple%20concepts%20including%20humans%20and%20objects%2C%20therefore%20hinders%0Aapplications.%20In%20this%20work%2C%20we%20discard%20the%20single-entity%20assumption%20and%0Aintroduce%20a%20novel%20framework%20that%20enforces%20strong%2C%20region-specific%20binding%20of%0Aconditions%20from%20modalities%20to%20each%20identity%27s%20spatiotemporal%20footprint.%20Given%0Areference%20images%20of%20multiple%20concepts%2C%20our%20method%20could%20automatically%20infer%0Alayout%20information%20by%20leveraging%20a%20mask%20predictor%20to%20match%20appearance%20cues%0Abetween%20the%20denoised%20video%20and%20each%20reference%20appearance.%20Furthermore%2C%20we%0Ainject%20local%20audio%20condition%20into%20its%20corresponding%20region%20to%20ensure%0Alayout-aligned%20modality%20matching%20in%20a%20iterative%20manner.%20This%20design%20enables%20the%0Ahigh-quality%20generation%20of%20controllable%20multi-concept%20human-centric%20videos.%0AEmpirical%20results%20and%20ablation%20studies%20validate%20the%20effectiveness%20of%20our%0Aexplicit%20layout%20control%20for%20multi-modal%20conditions%20compared%20to%20implicit%0Acounterparts%20and%20other%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09984v1&entry.124074799=Read"},
{"title": "ContentV: Efficient Training of Video Generation Models with Limited\n  Compute", "author": "Wenfeng Lin and Renjie Chen and Boyuan Liu and Shiyue Yan and Ruoyu Feng and Jiangchuan Wei and Yichen Zhang and Yimeng Zhou and Chao Feng and Jiao Ran and Qi Wu and Zuotao Liu and Mingyu Guo", "abstract": "  Recent advances in video generation demand increasingly efficient training\nrecipes to mitigate escalating computational costs. In this report, we present\nContentV, an 8B-parameter text-to-video model that achieves state-of-the-art\nperformance (85.14 on VBench) after training on 256 x 64GB Neural Processing\nUnits (NPUs) for merely four weeks. ContentV generates diverse, high-quality\nvideos across multiple resolutions and durations from text prompts, enabled by\nthree key innovations: (1) A minimalist architecture that maximizes reuse of\npre-trained image generation models for video generation; (2) A systematic\nmulti-stage training strategy leveraging flow matching for enhanced efficiency;\nand (3) A cost-effective reinforcement learning with human feedback framework\nthat improves generation quality without requiring additional human\nannotations. All the code and models are available at:\nhttps://contentv.github.io.\n", "link": "http://arxiv.org/abs/2506.05343v2", "date": "2025-06-11", "relevancy": 2.5044, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6454}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6174}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ContentV%3A%20Efficient%20Training%20of%20Video%20Generation%20Models%20with%20Limited%0A%20%20Compute&body=Title%3A%20ContentV%3A%20Efficient%20Training%20of%20Video%20Generation%20Models%20with%20Limited%0A%20%20Compute%0AAuthor%3A%20Wenfeng%20Lin%20and%20Renjie%20Chen%20and%20Boyuan%20Liu%20and%20Shiyue%20Yan%20and%20Ruoyu%20Feng%20and%20Jiangchuan%20Wei%20and%20Yichen%20Zhang%20and%20Yimeng%20Zhou%20and%20Chao%20Feng%20and%20Jiao%20Ran%20and%20Qi%20Wu%20and%20Zuotao%20Liu%20and%20Mingyu%20Guo%0AAbstract%3A%20%20%20Recent%20advances%20in%20video%20generation%20demand%20increasingly%20efficient%20training%0Arecipes%20to%20mitigate%20escalating%20computational%20costs.%20In%20this%20report%2C%20we%20present%0AContentV%2C%20an%208B-parameter%20text-to-video%20model%20that%20achieves%20state-of-the-art%0Aperformance%20%2885.14%20on%20VBench%29%20after%20training%20on%20256%20x%2064GB%20Neural%20Processing%0AUnits%20%28NPUs%29%20for%20merely%20four%20weeks.%20ContentV%20generates%20diverse%2C%20high-quality%0Avideos%20across%20multiple%20resolutions%20and%20durations%20from%20text%20prompts%2C%20enabled%20by%0Athree%20key%20innovations%3A%20%281%29%20A%20minimalist%20architecture%20that%20maximizes%20reuse%20of%0Apre-trained%20image%20generation%20models%20for%20video%20generation%3B%20%282%29%20A%20systematic%0Amulti-stage%20training%20strategy%20leveraging%20flow%20matching%20for%20enhanced%20efficiency%3B%0Aand%20%283%29%20A%20cost-effective%20reinforcement%20learning%20with%20human%20feedback%20framework%0Athat%20improves%20generation%20quality%20without%20requiring%20additional%20human%0Aannotations.%20All%20the%20code%20and%20models%20are%20available%20at%3A%0Ahttps%3A//contentv.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05343v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContentV%253A%2520Efficient%2520Training%2520of%2520Video%2520Generation%2520Models%2520with%2520Limited%250A%2520%2520Compute%26entry.906535625%3DWenfeng%2520Lin%2520and%2520Renjie%2520Chen%2520and%2520Boyuan%2520Liu%2520and%2520Shiyue%2520Yan%2520and%2520Ruoyu%2520Feng%2520and%2520Jiangchuan%2520Wei%2520and%2520Yichen%2520Zhang%2520and%2520Yimeng%2520Zhou%2520and%2520Chao%2520Feng%2520and%2520Jiao%2520Ran%2520and%2520Qi%2520Wu%2520and%2520Zuotao%2520Liu%2520and%2520Mingyu%2520Guo%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520video%2520generation%2520demand%2520increasingly%2520efficient%2520training%250Arecipes%2520to%2520mitigate%2520escalating%2520computational%2520costs.%2520In%2520this%2520report%252C%2520we%2520present%250AContentV%252C%2520an%25208B-parameter%2520text-to-video%2520model%2520that%2520achieves%2520state-of-the-art%250Aperformance%2520%252885.14%2520on%2520VBench%2529%2520after%2520training%2520on%2520256%2520x%252064GB%2520Neural%2520Processing%250AUnits%2520%2528NPUs%2529%2520for%2520merely%2520four%2520weeks.%2520ContentV%2520generates%2520diverse%252C%2520high-quality%250Avideos%2520across%2520multiple%2520resolutions%2520and%2520durations%2520from%2520text%2520prompts%252C%2520enabled%2520by%250Athree%2520key%2520innovations%253A%2520%25281%2529%2520A%2520minimalist%2520architecture%2520that%2520maximizes%2520reuse%2520of%250Apre-trained%2520image%2520generation%2520models%2520for%2520video%2520generation%253B%2520%25282%2529%2520A%2520systematic%250Amulti-stage%2520training%2520strategy%2520leveraging%2520flow%2520matching%2520for%2520enhanced%2520efficiency%253B%250Aand%2520%25283%2529%2520A%2520cost-effective%2520reinforcement%2520learning%2520with%2520human%2520feedback%2520framework%250Athat%2520improves%2520generation%2520quality%2520without%2520requiring%2520additional%2520human%250Aannotations.%2520All%2520the%2520code%2520and%2520models%2520are%2520available%2520at%253A%250Ahttps%253A//contentv.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05343v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ContentV%3A%20Efficient%20Training%20of%20Video%20Generation%20Models%20with%20Limited%0A%20%20Compute&entry.906535625=Wenfeng%20Lin%20and%20Renjie%20Chen%20and%20Boyuan%20Liu%20and%20Shiyue%20Yan%20and%20Ruoyu%20Feng%20and%20Jiangchuan%20Wei%20and%20Yichen%20Zhang%20and%20Yimeng%20Zhou%20and%20Chao%20Feng%20and%20Jiao%20Ran%20and%20Qi%20Wu%20and%20Zuotao%20Liu%20and%20Mingyu%20Guo&entry.1292438233=%20%20Recent%20advances%20in%20video%20generation%20demand%20increasingly%20efficient%20training%0Arecipes%20to%20mitigate%20escalating%20computational%20costs.%20In%20this%20report%2C%20we%20present%0AContentV%2C%20an%208B-parameter%20text-to-video%20model%20that%20achieves%20state-of-the-art%0Aperformance%20%2885.14%20on%20VBench%29%20after%20training%20on%20256%20x%2064GB%20Neural%20Processing%0AUnits%20%28NPUs%29%20for%20merely%20four%20weeks.%20ContentV%20generates%20diverse%2C%20high-quality%0Avideos%20across%20multiple%20resolutions%20and%20durations%20from%20text%20prompts%2C%20enabled%20by%0Athree%20key%20innovations%3A%20%281%29%20A%20minimalist%20architecture%20that%20maximizes%20reuse%20of%0Apre-trained%20image%20generation%20models%20for%20video%20generation%3B%20%282%29%20A%20systematic%0Amulti-stage%20training%20strategy%20leveraging%20flow%20matching%20for%20enhanced%20efficiency%3B%0Aand%20%283%29%20A%20cost-effective%20reinforcement%20learning%20with%20human%20feedback%20framework%0Athat%20improves%20generation%20quality%20without%20requiring%20additional%20human%0Aannotations.%20All%20the%20code%20and%20models%20are%20available%20at%3A%0Ahttps%3A//contentv.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05343v2&entry.124074799=Read"},
{"title": "Using Shapley interactions to understand how models use structure", "author": "Divyansh Singhvi and Diganta Misra and Andrej Erkelens and Raghav Jain and Isabel Papadimitriou and Naomi Saphra", "abstract": "  Language is an intricately structured system, and a key goal of NLP\ninterpretability is to provide methodological insights for understanding how\nlanguage models represent this structure internally. In this paper, we use\nShapley Taylor interaction indices (STII) in order to examine how language and\nspeech models internally relate and structure their inputs. Pairwise Shapley\ninteractions measure how much two inputs work together to influence model\noutputs beyond if we linearly added their independent influences, providing a\nview into how models encode structural interactions between inputs. We relate\nthe interaction patterns in models to three underlying linguistic structures:\nsyntactic structure, non-compositional semantics, and phonetic coarticulation.\nWe find that autoregressive text models encode interactions that correlate with\nthe syntactic proximity of inputs, and that both autoregressive and masked\nmodels encode nonlinear interactions in idiomatic phrases with\nnon-compositional semantics. Our speech results show that inputs are more\nentangled for pairs where a neighboring consonant is likely to influence a\nvowel or approximant, showing that models encode the phonetic interaction\nneeded for extracting discrete phonemic representations.\n", "link": "http://arxiv.org/abs/2403.13106v2", "date": "2025-06-11", "relevancy": 2.4688, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4994}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4994}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Shapley%20interactions%20to%20understand%20how%20models%20use%20structure&body=Title%3A%20Using%20Shapley%20interactions%20to%20understand%20how%20models%20use%20structure%0AAuthor%3A%20Divyansh%20Singhvi%20and%20Diganta%20Misra%20and%20Andrej%20Erkelens%20and%20Raghav%20Jain%20and%20Isabel%20Papadimitriou%20and%20Naomi%20Saphra%0AAbstract%3A%20%20%20Language%20is%20an%20intricately%20structured%20system%2C%20and%20a%20key%20goal%20of%20NLP%0Ainterpretability%20is%20to%20provide%20methodological%20insights%20for%20understanding%20how%0Alanguage%20models%20represent%20this%20structure%20internally.%20In%20this%20paper%2C%20we%20use%0AShapley%20Taylor%20interaction%20indices%20%28STII%29%20in%20order%20to%20examine%20how%20language%20and%0Aspeech%20models%20internally%20relate%20and%20structure%20their%20inputs.%20Pairwise%20Shapley%0Ainteractions%20measure%20how%20much%20two%20inputs%20work%20together%20to%20influence%20model%0Aoutputs%20beyond%20if%20we%20linearly%20added%20their%20independent%20influences%2C%20providing%20a%0Aview%20into%20how%20models%20encode%20structural%20interactions%20between%20inputs.%20We%20relate%0Athe%20interaction%20patterns%20in%20models%20to%20three%20underlying%20linguistic%20structures%3A%0Asyntactic%20structure%2C%20non-compositional%20semantics%2C%20and%20phonetic%20coarticulation.%0AWe%20find%20that%20autoregressive%20text%20models%20encode%20interactions%20that%20correlate%20with%0Athe%20syntactic%20proximity%20of%20inputs%2C%20and%20that%20both%20autoregressive%20and%20masked%0Amodels%20encode%20nonlinear%20interactions%20in%20idiomatic%20phrases%20with%0Anon-compositional%20semantics.%20Our%20speech%20results%20show%20that%20inputs%20are%20more%0Aentangled%20for%20pairs%20where%20a%20neighboring%20consonant%20is%20likely%20to%20influence%20a%0Avowel%20or%20approximant%2C%20showing%20that%20models%20encode%20the%20phonetic%20interaction%0Aneeded%20for%20extracting%20discrete%20phonemic%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13106v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Shapley%2520interactions%2520to%2520understand%2520how%2520models%2520use%2520structure%26entry.906535625%3DDivyansh%2520Singhvi%2520and%2520Diganta%2520Misra%2520and%2520Andrej%2520Erkelens%2520and%2520Raghav%2520Jain%2520and%2520Isabel%2520Papadimitriou%2520and%2520Naomi%2520Saphra%26entry.1292438233%3D%2520%2520Language%2520is%2520an%2520intricately%2520structured%2520system%252C%2520and%2520a%2520key%2520goal%2520of%2520NLP%250Ainterpretability%2520is%2520to%2520provide%2520methodological%2520insights%2520for%2520understanding%2520how%250Alanguage%2520models%2520represent%2520this%2520structure%2520internally.%2520In%2520this%2520paper%252C%2520we%2520use%250AShapley%2520Taylor%2520interaction%2520indices%2520%2528STII%2529%2520in%2520order%2520to%2520examine%2520how%2520language%2520and%250Aspeech%2520models%2520internally%2520relate%2520and%2520structure%2520their%2520inputs.%2520Pairwise%2520Shapley%250Ainteractions%2520measure%2520how%2520much%2520two%2520inputs%2520work%2520together%2520to%2520influence%2520model%250Aoutputs%2520beyond%2520if%2520we%2520linearly%2520added%2520their%2520independent%2520influences%252C%2520providing%2520a%250Aview%2520into%2520how%2520models%2520encode%2520structural%2520interactions%2520between%2520inputs.%2520We%2520relate%250Athe%2520interaction%2520patterns%2520in%2520models%2520to%2520three%2520underlying%2520linguistic%2520structures%253A%250Asyntactic%2520structure%252C%2520non-compositional%2520semantics%252C%2520and%2520phonetic%2520coarticulation.%250AWe%2520find%2520that%2520autoregressive%2520text%2520models%2520encode%2520interactions%2520that%2520correlate%2520with%250Athe%2520syntactic%2520proximity%2520of%2520inputs%252C%2520and%2520that%2520both%2520autoregressive%2520and%2520masked%250Amodels%2520encode%2520nonlinear%2520interactions%2520in%2520idiomatic%2520phrases%2520with%250Anon-compositional%2520semantics.%2520Our%2520speech%2520results%2520show%2520that%2520inputs%2520are%2520more%250Aentangled%2520for%2520pairs%2520where%2520a%2520neighboring%2520consonant%2520is%2520likely%2520to%2520influence%2520a%250Avowel%2520or%2520approximant%252C%2520showing%2520that%2520models%2520encode%2520the%2520phonetic%2520interaction%250Aneeded%2520for%2520extracting%2520discrete%2520phonemic%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13106v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Shapley%20interactions%20to%20understand%20how%20models%20use%20structure&entry.906535625=Divyansh%20Singhvi%20and%20Diganta%20Misra%20and%20Andrej%20Erkelens%20and%20Raghav%20Jain%20and%20Isabel%20Papadimitriou%20and%20Naomi%20Saphra&entry.1292438233=%20%20Language%20is%20an%20intricately%20structured%20system%2C%20and%20a%20key%20goal%20of%20NLP%0Ainterpretability%20is%20to%20provide%20methodological%20insights%20for%20understanding%20how%0Alanguage%20models%20represent%20this%20structure%20internally.%20In%20this%20paper%2C%20we%20use%0AShapley%20Taylor%20interaction%20indices%20%28STII%29%20in%20order%20to%20examine%20how%20language%20and%0Aspeech%20models%20internally%20relate%20and%20structure%20their%20inputs.%20Pairwise%20Shapley%0Ainteractions%20measure%20how%20much%20two%20inputs%20work%20together%20to%20influence%20model%0Aoutputs%20beyond%20if%20we%20linearly%20added%20their%20independent%20influences%2C%20providing%20a%0Aview%20into%20how%20models%20encode%20structural%20interactions%20between%20inputs.%20We%20relate%0Athe%20interaction%20patterns%20in%20models%20to%20three%20underlying%20linguistic%20structures%3A%0Asyntactic%20structure%2C%20non-compositional%20semantics%2C%20and%20phonetic%20coarticulation.%0AWe%20find%20that%20autoregressive%20text%20models%20encode%20interactions%20that%20correlate%20with%0Athe%20syntactic%20proximity%20of%20inputs%2C%20and%20that%20both%20autoregressive%20and%20masked%0Amodels%20encode%20nonlinear%20interactions%20in%20idiomatic%20phrases%20with%0Anon-compositional%20semantics.%20Our%20speech%20results%20show%20that%20inputs%20are%20more%0Aentangled%20for%20pairs%20where%20a%20neighboring%20consonant%20is%20likely%20to%20influence%20a%0Avowel%20or%20approximant%2C%20showing%20that%20models%20encode%20the%20phonetic%20interaction%0Aneeded%20for%20extracting%20discrete%20phonemic%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13106v2&entry.124074799=Read"},
{"title": "On the Similarities of Embeddings in Contrastive Learning", "author": "Chungpa Lee and Sehee Lim and Kibok Lee and Jy-yong Sohn", "abstract": "  Contrastive learning (CL) operates on a simple yet effective principle:\nembeddings of positive pairs are pulled together, while those of negative pairs\nare pushed apart. Although various forms of contrastive loss have been proposed\nand analyzed from different perspectives, prior works lack a comprehensive\nframework that systematically explains a broad class of these objectives. In\nthis paper, we present a unified framework for understanding CL, which is based\non analyzing the cosine similarity between embeddings of positive and negative\npairs. In full-batch settings, we show that perfect alignment of positive pairs\nis unattainable when similarities of negative pairs fall below a certain\nthreshold, and that this misalignment can be alleviated by incorporating\nwithin-view negative pairs. In mini-batch settings, we demonstrate that smaller\nbatch sizes incur stronger separation among negative pairs within batches,\nwhich leads to higher variance in similarities of negative pairs. To address\nthis limitation of mini-batch CL, we introduce an auxiliary loss term that\nreduces the variance of similarities of negative pairs in CL. Empirical results\ndemonstrate that incorporating the proposed loss consistently improves the\nperformance of CL methods in small-batch training.\n", "link": "http://arxiv.org/abs/2506.09781v1", "date": "2025-06-11", "relevancy": 2.467, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5051}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5004}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Similarities%20of%20Embeddings%20in%20Contrastive%20Learning&body=Title%3A%20On%20the%20Similarities%20of%20Embeddings%20in%20Contrastive%20Learning%0AAuthor%3A%20Chungpa%20Lee%20and%20Sehee%20Lim%20and%20Kibok%20Lee%20and%20Jy-yong%20Sohn%0AAbstract%3A%20%20%20Contrastive%20learning%20%28CL%29%20operates%20on%20a%20simple%20yet%20effective%20principle%3A%0Aembeddings%20of%20positive%20pairs%20are%20pulled%20together%2C%20while%20those%20of%20negative%20pairs%0Aare%20pushed%20apart.%20Although%20various%20forms%20of%20contrastive%20loss%20have%20been%20proposed%0Aand%20analyzed%20from%20different%20perspectives%2C%20prior%20works%20lack%20a%20comprehensive%0Aframework%20that%20systematically%20explains%20a%20broad%20class%20of%20these%20objectives.%20In%0Athis%20paper%2C%20we%20present%20a%20unified%20framework%20for%20understanding%20CL%2C%20which%20is%20based%0Aon%20analyzing%20the%20cosine%20similarity%20between%20embeddings%20of%20positive%20and%20negative%0Apairs.%20In%20full-batch%20settings%2C%20we%20show%20that%20perfect%20alignment%20of%20positive%20pairs%0Ais%20unattainable%20when%20similarities%20of%20negative%20pairs%20fall%20below%20a%20certain%0Athreshold%2C%20and%20that%20this%20misalignment%20can%20be%20alleviated%20by%20incorporating%0Awithin-view%20negative%20pairs.%20In%20mini-batch%20settings%2C%20we%20demonstrate%20that%20smaller%0Abatch%20sizes%20incur%20stronger%20separation%20among%20negative%20pairs%20within%20batches%2C%0Awhich%20leads%20to%20higher%20variance%20in%20similarities%20of%20negative%20pairs.%20To%20address%0Athis%20limitation%20of%20mini-batch%20CL%2C%20we%20introduce%20an%20auxiliary%20loss%20term%20that%0Areduces%20the%20variance%20of%20similarities%20of%20negative%20pairs%20in%20CL.%20Empirical%20results%0Ademonstrate%20that%20incorporating%20the%20proposed%20loss%20consistently%20improves%20the%0Aperformance%20of%20CL%20methods%20in%20small-batch%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Similarities%2520of%2520Embeddings%2520in%2520Contrastive%2520Learning%26entry.906535625%3DChungpa%2520Lee%2520and%2520Sehee%2520Lim%2520and%2520Kibok%2520Lee%2520and%2520Jy-yong%2520Sohn%26entry.1292438233%3D%2520%2520Contrastive%2520learning%2520%2528CL%2529%2520operates%2520on%2520a%2520simple%2520yet%2520effective%2520principle%253A%250Aembeddings%2520of%2520positive%2520pairs%2520are%2520pulled%2520together%252C%2520while%2520those%2520of%2520negative%2520pairs%250Aare%2520pushed%2520apart.%2520Although%2520various%2520forms%2520of%2520contrastive%2520loss%2520have%2520been%2520proposed%250Aand%2520analyzed%2520from%2520different%2520perspectives%252C%2520prior%2520works%2520lack%2520a%2520comprehensive%250Aframework%2520that%2520systematically%2520explains%2520a%2520broad%2520class%2520of%2520these%2520objectives.%2520In%250Athis%2520paper%252C%2520we%2520present%2520a%2520unified%2520framework%2520for%2520understanding%2520CL%252C%2520which%2520is%2520based%250Aon%2520analyzing%2520the%2520cosine%2520similarity%2520between%2520embeddings%2520of%2520positive%2520and%2520negative%250Apairs.%2520In%2520full-batch%2520settings%252C%2520we%2520show%2520that%2520perfect%2520alignment%2520of%2520positive%2520pairs%250Ais%2520unattainable%2520when%2520similarities%2520of%2520negative%2520pairs%2520fall%2520below%2520a%2520certain%250Athreshold%252C%2520and%2520that%2520this%2520misalignment%2520can%2520be%2520alleviated%2520by%2520incorporating%250Awithin-view%2520negative%2520pairs.%2520In%2520mini-batch%2520settings%252C%2520we%2520demonstrate%2520that%2520smaller%250Abatch%2520sizes%2520incur%2520stronger%2520separation%2520among%2520negative%2520pairs%2520within%2520batches%252C%250Awhich%2520leads%2520to%2520higher%2520variance%2520in%2520similarities%2520of%2520negative%2520pairs.%2520To%2520address%250Athis%2520limitation%2520of%2520mini-batch%2520CL%252C%2520we%2520introduce%2520an%2520auxiliary%2520loss%2520term%2520that%250Areduces%2520the%2520variance%2520of%2520similarities%2520of%2520negative%2520pairs%2520in%2520CL.%2520Empirical%2520results%250Ademonstrate%2520that%2520incorporating%2520the%2520proposed%2520loss%2520consistently%2520improves%2520the%250Aperformance%2520of%2520CL%2520methods%2520in%2520small-batch%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Similarities%20of%20Embeddings%20in%20Contrastive%20Learning&entry.906535625=Chungpa%20Lee%20and%20Sehee%20Lim%20and%20Kibok%20Lee%20and%20Jy-yong%20Sohn&entry.1292438233=%20%20Contrastive%20learning%20%28CL%29%20operates%20on%20a%20simple%20yet%20effective%20principle%3A%0Aembeddings%20of%20positive%20pairs%20are%20pulled%20together%2C%20while%20those%20of%20negative%20pairs%0Aare%20pushed%20apart.%20Although%20various%20forms%20of%20contrastive%20loss%20have%20been%20proposed%0Aand%20analyzed%20from%20different%20perspectives%2C%20prior%20works%20lack%20a%20comprehensive%0Aframework%20that%20systematically%20explains%20a%20broad%20class%20of%20these%20objectives.%20In%0Athis%20paper%2C%20we%20present%20a%20unified%20framework%20for%20understanding%20CL%2C%20which%20is%20based%0Aon%20analyzing%20the%20cosine%20similarity%20between%20embeddings%20of%20positive%20and%20negative%0Apairs.%20In%20full-batch%20settings%2C%20we%20show%20that%20perfect%20alignment%20of%20positive%20pairs%0Ais%20unattainable%20when%20similarities%20of%20negative%20pairs%20fall%20below%20a%20certain%0Athreshold%2C%20and%20that%20this%20misalignment%20can%20be%20alleviated%20by%20incorporating%0Awithin-view%20negative%20pairs.%20In%20mini-batch%20settings%2C%20we%20demonstrate%20that%20smaller%0Abatch%20sizes%20incur%20stronger%20separation%20among%20negative%20pairs%20within%20batches%2C%0Awhich%20leads%20to%20higher%20variance%20in%20similarities%20of%20negative%20pairs.%20To%20address%0Athis%20limitation%20of%20mini-batch%20CL%2C%20we%20introduce%20an%20auxiliary%20loss%20term%20that%0Areduces%20the%20variance%20of%20similarities%20of%20negative%20pairs%20in%20CL.%20Empirical%20results%0Ademonstrate%20that%20incorporating%20the%20proposed%20loss%20consistently%20improves%20the%0Aperformance%20of%20CL%20methods%20in%20small-batch%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09781v1&entry.124074799=Read"},
{"title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal\n  Gaussian Splatting", "author": "Ziyi Wang and Yanran Zhang and Jie Zhou and Jiwen Lu", "abstract": "  The scale diversity of point cloud data presents significant challenges in\ndeveloping unified representation learning techniques for 3D vision. Currently,\nthere are few unified 3D models, and no existing pre-training method is equally\neffective for both object- and scene-level point clouds. In this paper, we\nintroduce UniPre3D, the first unified pre-training method that can be\nseamlessly applied to point clouds of any scale and 3D models of any\narchitecture. Our approach predicts Gaussian primitives as the pre-training\ntask and employs differentiable Gaussian splatting to render images, enabling\nprecise pixel-level supervision and end-to-end optimization. To further\nregulate the complexity of the pre-training task and direct the model's focus\ntoward geometric structures, we integrate 2D features from pre-trained image\nmodels to incorporate well-established texture knowledge. We validate the\nuniversal effectiveness of our proposed method through extensive experiments\nacross a variety of object- and scene-level tasks, using diverse point cloud\nmodels as backbones. Code is available at https://github.com/wangzy22/UniPre3D.\n", "link": "http://arxiv.org/abs/2506.09952v1", "date": "2025-06-11", "relevancy": 2.4537, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6505}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5879}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniPre3D%3A%20Unified%20Pre-training%20of%203D%20Point%20Cloud%20Models%20with%20Cross-Modal%0A%20%20Gaussian%20Splatting&body=Title%3A%20UniPre3D%3A%20Unified%20Pre-training%20of%203D%20Point%20Cloud%20Models%20with%20Cross-Modal%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Ziyi%20Wang%20and%20Yanran%20Zhang%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20The%20scale%20diversity%20of%20point%20cloud%20data%20presents%20significant%20challenges%20in%0Adeveloping%20unified%20representation%20learning%20techniques%20for%203D%20vision.%20Currently%2C%0Athere%20are%20few%20unified%203D%20models%2C%20and%20no%20existing%20pre-training%20method%20is%20equally%0Aeffective%20for%20both%20object-%20and%20scene-level%20point%20clouds.%20In%20this%20paper%2C%20we%0Aintroduce%20UniPre3D%2C%20the%20first%20unified%20pre-training%20method%20that%20can%20be%0Aseamlessly%20applied%20to%20point%20clouds%20of%20any%20scale%20and%203D%20models%20of%20any%0Aarchitecture.%20Our%20approach%20predicts%20Gaussian%20primitives%20as%20the%20pre-training%0Atask%20and%20employs%20differentiable%20Gaussian%20splatting%20to%20render%20images%2C%20enabling%0Aprecise%20pixel-level%20supervision%20and%20end-to-end%20optimization.%20To%20further%0Aregulate%20the%20complexity%20of%20the%20pre-training%20task%20and%20direct%20the%20model%27s%20focus%0Atoward%20geometric%20structures%2C%20we%20integrate%202D%20features%20from%20pre-trained%20image%0Amodels%20to%20incorporate%20well-established%20texture%20knowledge.%20We%20validate%20the%0Auniversal%20effectiveness%20of%20our%20proposed%20method%20through%20extensive%20experiments%0Aacross%20a%20variety%20of%20object-%20and%20scene-level%20tasks%2C%20using%20diverse%20point%20cloud%0Amodels%20as%20backbones.%20Code%20is%20available%20at%20https%3A//github.com/wangzy22/UniPre3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniPre3D%253A%2520Unified%2520Pre-training%2520of%25203D%2520Point%2520Cloud%2520Models%2520with%2520Cross-Modal%250A%2520%2520Gaussian%2520Splatting%26entry.906535625%3DZiyi%2520Wang%2520and%2520Yanran%2520Zhang%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520The%2520scale%2520diversity%2520of%2520point%2520cloud%2520data%2520presents%2520significant%2520challenges%2520in%250Adeveloping%2520unified%2520representation%2520learning%2520techniques%2520for%25203D%2520vision.%2520Currently%252C%250Athere%2520are%2520few%2520unified%25203D%2520models%252C%2520and%2520no%2520existing%2520pre-training%2520method%2520is%2520equally%250Aeffective%2520for%2520both%2520object-%2520and%2520scene-level%2520point%2520clouds.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520UniPre3D%252C%2520the%2520first%2520unified%2520pre-training%2520method%2520that%2520can%2520be%250Aseamlessly%2520applied%2520to%2520point%2520clouds%2520of%2520any%2520scale%2520and%25203D%2520models%2520of%2520any%250Aarchitecture.%2520Our%2520approach%2520predicts%2520Gaussian%2520primitives%2520as%2520the%2520pre-training%250Atask%2520and%2520employs%2520differentiable%2520Gaussian%2520splatting%2520to%2520render%2520images%252C%2520enabling%250Aprecise%2520pixel-level%2520supervision%2520and%2520end-to-end%2520optimization.%2520To%2520further%250Aregulate%2520the%2520complexity%2520of%2520the%2520pre-training%2520task%2520and%2520direct%2520the%2520model%2527s%2520focus%250Atoward%2520geometric%2520structures%252C%2520we%2520integrate%25202D%2520features%2520from%2520pre-trained%2520image%250Amodels%2520to%2520incorporate%2520well-established%2520texture%2520knowledge.%2520We%2520validate%2520the%250Auniversal%2520effectiveness%2520of%2520our%2520proposed%2520method%2520through%2520extensive%2520experiments%250Aacross%2520a%2520variety%2520of%2520object-%2520and%2520scene-level%2520tasks%252C%2520using%2520diverse%2520point%2520cloud%250Amodels%2520as%2520backbones.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/wangzy22/UniPre3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniPre3D%3A%20Unified%20Pre-training%20of%203D%20Point%20Cloud%20Models%20with%20Cross-Modal%0A%20%20Gaussian%20Splatting&entry.906535625=Ziyi%20Wang%20and%20Yanran%20Zhang%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%20The%20scale%20diversity%20of%20point%20cloud%20data%20presents%20significant%20challenges%20in%0Adeveloping%20unified%20representation%20learning%20techniques%20for%203D%20vision.%20Currently%2C%0Athere%20are%20few%20unified%203D%20models%2C%20and%20no%20existing%20pre-training%20method%20is%20equally%0Aeffective%20for%20both%20object-%20and%20scene-level%20point%20clouds.%20In%20this%20paper%2C%20we%0Aintroduce%20UniPre3D%2C%20the%20first%20unified%20pre-training%20method%20that%20can%20be%0Aseamlessly%20applied%20to%20point%20clouds%20of%20any%20scale%20and%203D%20models%20of%20any%0Aarchitecture.%20Our%20approach%20predicts%20Gaussian%20primitives%20as%20the%20pre-training%0Atask%20and%20employs%20differentiable%20Gaussian%20splatting%20to%20render%20images%2C%20enabling%0Aprecise%20pixel-level%20supervision%20and%20end-to-end%20optimization.%20To%20further%0Aregulate%20the%20complexity%20of%20the%20pre-training%20task%20and%20direct%20the%20model%27s%20focus%0Atoward%20geometric%20structures%2C%20we%20integrate%202D%20features%20from%20pre-trained%20image%0Amodels%20to%20incorporate%20well-established%20texture%20knowledge.%20We%20validate%20the%0Auniversal%20effectiveness%20of%20our%20proposed%20method%20through%20extensive%20experiments%0Aacross%20a%20variety%20of%20object-%20and%20scene-level%20tasks%2C%20using%20diverse%20point%20cloud%0Amodels%20as%20backbones.%20Code%20is%20available%20at%20https%3A//github.com/wangzy22/UniPre3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09952v1&entry.124074799=Read"},
{"title": "Structural-Spectral Graph Convolution with Evidential Edge Learning for\n  Hyperspectral Image Clustering", "author": "Jianhan Qi and Yuheng Jia and Hui Liu and Junhui Hou", "abstract": "  Hyperspectral image (HSI) clustering assigns similar pixels to the same class\nwithout any annotations, which is an important yet challenging task. For\nlarge-scale HSIs, most methods rely on superpixel segmentation and perform\nsuperpixel-level clustering based on graph neural networks (GNNs). However,\nexisting GNNs cannot fully exploit the spectral information of the input HSI,\nand the inaccurate superpixel topological graph may lead to the confusion of\ndifferent class semantics during information aggregation. To address these\nchallenges, we first propose a structural-spectral graph convolutional operator\n(SSGCO) tailored for graph-structured HSI superpixels to improve their\nrepresentation quality through the co-extraction of spatial and spectral\nfeatures. Second, we propose an evidence-guided adaptive edge learning (EGAEL)\nmodule that adaptively predicts and refines edge weights in the superpixel\ntopological graph. We integrate the proposed method into a contrastive learning\nframework to achieve clustering, where representation learning and clustering\nare simultaneously conducted. Experiments demonstrate that the proposed method\nimproves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best\ncompared methods on four HSI datasets. Our code is available at\nhttps://github.com/jhqi/SSGCO-EGAEL.\n", "link": "http://arxiv.org/abs/2506.09920v1", "date": "2025-06-11", "relevancy": 2.3935, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4804}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.48}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structural-Spectral%20Graph%20Convolution%20with%20Evidential%20Edge%20Learning%20for%0A%20%20Hyperspectral%20Image%20Clustering&body=Title%3A%20Structural-Spectral%20Graph%20Convolution%20with%20Evidential%20Edge%20Learning%20for%0A%20%20Hyperspectral%20Image%20Clustering%0AAuthor%3A%20Jianhan%20Qi%20and%20Yuheng%20Jia%20and%20Hui%20Liu%20and%20Junhui%20Hou%0AAbstract%3A%20%20%20Hyperspectral%20image%20%28HSI%29%20clustering%20assigns%20similar%20pixels%20to%20the%20same%20class%0Awithout%20any%20annotations%2C%20which%20is%20an%20important%20yet%20challenging%20task.%20For%0Alarge-scale%20HSIs%2C%20most%20methods%20rely%20on%20superpixel%20segmentation%20and%20perform%0Asuperpixel-level%20clustering%20based%20on%20graph%20neural%20networks%20%28GNNs%29.%20However%2C%0Aexisting%20GNNs%20cannot%20fully%20exploit%20the%20spectral%20information%20of%20the%20input%20HSI%2C%0Aand%20the%20inaccurate%20superpixel%20topological%20graph%20may%20lead%20to%20the%20confusion%20of%0Adifferent%20class%20semantics%20during%20information%20aggregation.%20To%20address%20these%0Achallenges%2C%20we%20first%20propose%20a%20structural-spectral%20graph%20convolutional%20operator%0A%28SSGCO%29%20tailored%20for%20graph-structured%20HSI%20superpixels%20to%20improve%20their%0Arepresentation%20quality%20through%20the%20co-extraction%20of%20spatial%20and%20spectral%0Afeatures.%20Second%2C%20we%20propose%20an%20evidence-guided%20adaptive%20edge%20learning%20%28EGAEL%29%0Amodule%20that%20adaptively%20predicts%20and%20refines%20edge%20weights%20in%20the%20superpixel%0Atopological%20graph.%20We%20integrate%20the%20proposed%20method%20into%20a%20contrastive%20learning%0Aframework%20to%20achieve%20clustering%2C%20where%20representation%20learning%20and%20clustering%0Aare%20simultaneously%20conducted.%20Experiments%20demonstrate%20that%20the%20proposed%20method%0Aimproves%20clustering%20accuracy%20by%202.61%25%2C%206.06%25%2C%204.96%25%20and%203.15%25%20over%20the%20best%0Acompared%20methods%20on%20four%20HSI%20datasets.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/jhqi/SSGCO-EGAEL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructural-Spectral%2520Graph%2520Convolution%2520with%2520Evidential%2520Edge%2520Learning%2520for%250A%2520%2520Hyperspectral%2520Image%2520Clustering%26entry.906535625%3DJianhan%2520Qi%2520and%2520Yuheng%2520Jia%2520and%2520Hui%2520Liu%2520and%2520Junhui%2520Hou%26entry.1292438233%3D%2520%2520Hyperspectral%2520image%2520%2528HSI%2529%2520clustering%2520assigns%2520similar%2520pixels%2520to%2520the%2520same%2520class%250Awithout%2520any%2520annotations%252C%2520which%2520is%2520an%2520important%2520yet%2520challenging%2520task.%2520For%250Alarge-scale%2520HSIs%252C%2520most%2520methods%2520rely%2520on%2520superpixel%2520segmentation%2520and%2520perform%250Asuperpixel-level%2520clustering%2520based%2520on%2520graph%2520neural%2520networks%2520%2528GNNs%2529.%2520However%252C%250Aexisting%2520GNNs%2520cannot%2520fully%2520exploit%2520the%2520spectral%2520information%2520of%2520the%2520input%2520HSI%252C%250Aand%2520the%2520inaccurate%2520superpixel%2520topological%2520graph%2520may%2520lead%2520to%2520the%2520confusion%2520of%250Adifferent%2520class%2520semantics%2520during%2520information%2520aggregation.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520first%2520propose%2520a%2520structural-spectral%2520graph%2520convolutional%2520operator%250A%2528SSGCO%2529%2520tailored%2520for%2520graph-structured%2520HSI%2520superpixels%2520to%2520improve%2520their%250Arepresentation%2520quality%2520through%2520the%2520co-extraction%2520of%2520spatial%2520and%2520spectral%250Afeatures.%2520Second%252C%2520we%2520propose%2520an%2520evidence-guided%2520adaptive%2520edge%2520learning%2520%2528EGAEL%2529%250Amodule%2520that%2520adaptively%2520predicts%2520and%2520refines%2520edge%2520weights%2520in%2520the%2520superpixel%250Atopological%2520graph.%2520We%2520integrate%2520the%2520proposed%2520method%2520into%2520a%2520contrastive%2520learning%250Aframework%2520to%2520achieve%2520clustering%252C%2520where%2520representation%2520learning%2520and%2520clustering%250Aare%2520simultaneously%2520conducted.%2520Experiments%2520demonstrate%2520that%2520the%2520proposed%2520method%250Aimproves%2520clustering%2520accuracy%2520by%25202.61%2525%252C%25206.06%2525%252C%25204.96%2525%2520and%25203.15%2525%2520over%2520the%2520best%250Acompared%2520methods%2520on%2520four%2520HSI%2520datasets.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/jhqi/SSGCO-EGAEL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structural-Spectral%20Graph%20Convolution%20with%20Evidential%20Edge%20Learning%20for%0A%20%20Hyperspectral%20Image%20Clustering&entry.906535625=Jianhan%20Qi%20and%20Yuheng%20Jia%20and%20Hui%20Liu%20and%20Junhui%20Hou&entry.1292438233=%20%20Hyperspectral%20image%20%28HSI%29%20clustering%20assigns%20similar%20pixels%20to%20the%20same%20class%0Awithout%20any%20annotations%2C%20which%20is%20an%20important%20yet%20challenging%20task.%20For%0Alarge-scale%20HSIs%2C%20most%20methods%20rely%20on%20superpixel%20segmentation%20and%20perform%0Asuperpixel-level%20clustering%20based%20on%20graph%20neural%20networks%20%28GNNs%29.%20However%2C%0Aexisting%20GNNs%20cannot%20fully%20exploit%20the%20spectral%20information%20of%20the%20input%20HSI%2C%0Aand%20the%20inaccurate%20superpixel%20topological%20graph%20may%20lead%20to%20the%20confusion%20of%0Adifferent%20class%20semantics%20during%20information%20aggregation.%20To%20address%20these%0Achallenges%2C%20we%20first%20propose%20a%20structural-spectral%20graph%20convolutional%20operator%0A%28SSGCO%29%20tailored%20for%20graph-structured%20HSI%20superpixels%20to%20improve%20their%0Arepresentation%20quality%20through%20the%20co-extraction%20of%20spatial%20and%20spectral%0Afeatures.%20Second%2C%20we%20propose%20an%20evidence-guided%20adaptive%20edge%20learning%20%28EGAEL%29%0Amodule%20that%20adaptively%20predicts%20and%20refines%20edge%20weights%20in%20the%20superpixel%0Atopological%20graph.%20We%20integrate%20the%20proposed%20method%20into%20a%20contrastive%20learning%0Aframework%20to%20achieve%20clustering%2C%20where%20representation%20learning%20and%20clustering%0Aare%20simultaneously%20conducted.%20Experiments%20demonstrate%20that%20the%20proposed%20method%0Aimproves%20clustering%20accuracy%20by%202.61%25%2C%206.06%25%2C%204.96%25%20and%203.15%25%20over%20the%20best%0Acompared%20methods%20on%20four%20HSI%20datasets.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/jhqi/SSGCO-EGAEL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09920v1&entry.124074799=Read"},
{"title": "Efficient Part-level 3D Object Generation via Dual Volume Packing", "author": "Jiaxiang Tang and Ruijie Lu and Zhaoshuo Li and Zekun Hao and Xuan Li and Fangyin Wei and Shuran Song and Gang Zeng and Ming-Yu Liu and Tsung-Yi Lin", "abstract": "  Recent progress in 3D object generation has greatly improved both the quality\nand efficiency. However, most existing methods generate a single mesh with all\nparts fused together, which limits the ability to edit or manipulate individual\nparts. A key challenge is that different objects may have a varying number of\nparts. To address this, we propose a new end-to-end framework for part-level 3D\nobject generation. Given a single input image, our method generates\nhigh-quality 3D objects with an arbitrary number of complete and semantically\nmeaningful parts. We introduce a dual volume packing strategy that organizes\nall parts into two complementary volumes, allowing for the creation of complete\nand interleaved parts that assemble into the final object. Experiments show\nthat our model achieves better quality, diversity, and generalization than\nprevious image-based part-level generation methods.\n", "link": "http://arxiv.org/abs/2506.09980v1", "date": "2025-06-11", "relevancy": 2.3825, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6306}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5886}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Part-level%203D%20Object%20Generation%20via%20Dual%20Volume%20Packing&body=Title%3A%20Efficient%20Part-level%203D%20Object%20Generation%20via%20Dual%20Volume%20Packing%0AAuthor%3A%20Jiaxiang%20Tang%20and%20Ruijie%20Lu%20and%20Zhaoshuo%20Li%20and%20Zekun%20Hao%20and%20Xuan%20Li%20and%20Fangyin%20Wei%20and%20Shuran%20Song%20and%20Gang%20Zeng%20and%20Ming-Yu%20Liu%20and%20Tsung-Yi%20Lin%0AAbstract%3A%20%20%20Recent%20progress%20in%203D%20object%20generation%20has%20greatly%20improved%20both%20the%20quality%0Aand%20efficiency.%20However%2C%20most%20existing%20methods%20generate%20a%20single%20mesh%20with%20all%0Aparts%20fused%20together%2C%20which%20limits%20the%20ability%20to%20edit%20or%20manipulate%20individual%0Aparts.%20A%20key%20challenge%20is%20that%20different%20objects%20may%20have%20a%20varying%20number%20of%0Aparts.%20To%20address%20this%2C%20we%20propose%20a%20new%20end-to-end%20framework%20for%20part-level%203D%0Aobject%20generation.%20Given%20a%20single%20input%20image%2C%20our%20method%20generates%0Ahigh-quality%203D%20objects%20with%20an%20arbitrary%20number%20of%20complete%20and%20semantically%0Ameaningful%20parts.%20We%20introduce%20a%20dual%20volume%20packing%20strategy%20that%20organizes%0Aall%20parts%20into%20two%20complementary%20volumes%2C%20allowing%20for%20the%20creation%20of%20complete%0Aand%20interleaved%20parts%20that%20assemble%20into%20the%20final%20object.%20Experiments%20show%0Athat%20our%20model%20achieves%20better%20quality%2C%20diversity%2C%20and%20generalization%20than%0Aprevious%20image-based%20part-level%20generation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Part-level%25203D%2520Object%2520Generation%2520via%2520Dual%2520Volume%2520Packing%26entry.906535625%3DJiaxiang%2520Tang%2520and%2520Ruijie%2520Lu%2520and%2520Zhaoshuo%2520Li%2520and%2520Zekun%2520Hao%2520and%2520Xuan%2520Li%2520and%2520Fangyin%2520Wei%2520and%2520Shuran%2520Song%2520and%2520Gang%2520Zeng%2520and%2520Ming-Yu%2520Liu%2520and%2520Tsung-Yi%2520Lin%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%25203D%2520object%2520generation%2520has%2520greatly%2520improved%2520both%2520the%2520quality%250Aand%2520efficiency.%2520However%252C%2520most%2520existing%2520methods%2520generate%2520a%2520single%2520mesh%2520with%2520all%250Aparts%2520fused%2520together%252C%2520which%2520limits%2520the%2520ability%2520to%2520edit%2520or%2520manipulate%2520individual%250Aparts.%2520A%2520key%2520challenge%2520is%2520that%2520different%2520objects%2520may%2520have%2520a%2520varying%2520number%2520of%250Aparts.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520new%2520end-to-end%2520framework%2520for%2520part-level%25203D%250Aobject%2520generation.%2520Given%2520a%2520single%2520input%2520image%252C%2520our%2520method%2520generates%250Ahigh-quality%25203D%2520objects%2520with%2520an%2520arbitrary%2520number%2520of%2520complete%2520and%2520semantically%250Ameaningful%2520parts.%2520We%2520introduce%2520a%2520dual%2520volume%2520packing%2520strategy%2520that%2520organizes%250Aall%2520parts%2520into%2520two%2520complementary%2520volumes%252C%2520allowing%2520for%2520the%2520creation%2520of%2520complete%250Aand%2520interleaved%2520parts%2520that%2520assemble%2520into%2520the%2520final%2520object.%2520Experiments%2520show%250Athat%2520our%2520model%2520achieves%2520better%2520quality%252C%2520diversity%252C%2520and%2520generalization%2520than%250Aprevious%2520image-based%2520part-level%2520generation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Part-level%203D%20Object%20Generation%20via%20Dual%20Volume%20Packing&entry.906535625=Jiaxiang%20Tang%20and%20Ruijie%20Lu%20and%20Zhaoshuo%20Li%20and%20Zekun%20Hao%20and%20Xuan%20Li%20and%20Fangyin%20Wei%20and%20Shuran%20Song%20and%20Gang%20Zeng%20and%20Ming-Yu%20Liu%20and%20Tsung-Yi%20Lin&entry.1292438233=%20%20Recent%20progress%20in%203D%20object%20generation%20has%20greatly%20improved%20both%20the%20quality%0Aand%20efficiency.%20However%2C%20most%20existing%20methods%20generate%20a%20single%20mesh%20with%20all%0Aparts%20fused%20together%2C%20which%20limits%20the%20ability%20to%20edit%20or%20manipulate%20individual%0Aparts.%20A%20key%20challenge%20is%20that%20different%20objects%20may%20have%20a%20varying%20number%20of%0Aparts.%20To%20address%20this%2C%20we%20propose%20a%20new%20end-to-end%20framework%20for%20part-level%203D%0Aobject%20generation.%20Given%20a%20single%20input%20image%2C%20our%20method%20generates%0Ahigh-quality%203D%20objects%20with%20an%20arbitrary%20number%20of%20complete%20and%20semantically%0Ameaningful%20parts.%20We%20introduce%20a%20dual%20volume%20packing%20strategy%20that%20organizes%0Aall%20parts%20into%20two%20complementary%20volumes%2C%20allowing%20for%20the%20creation%20of%20complete%0Aand%20interleaved%20parts%20that%20assemble%20into%20the%20final%20object.%20Experiments%20show%0Athat%20our%20model%20achieves%20better%20quality%2C%20diversity%2C%20and%20generalization%20than%0Aprevious%20image-based%20part-level%20generation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09980v1&entry.124074799=Read"},
{"title": "From Intention to Execution: Probing the Generalization Boundaries of\n  Vision-Language-Action Models", "author": "Irving Fang and Juexiao Zhang and Shengbang Tong and Chen Feng", "abstract": "  One promise that Vision-Language-Action (VLA) models hold over traditional\nimitation learning for robotics is to leverage the broad generalization\ncapabilities of large Vision-Language Models (VLMs) to produce versatile,\n\"generalist\" robot policies. However, current evaluations of VLAs remain\ninsufficient. Traditional imitation learning benchmarks are unsuitable due to\nthe lack of language instructions. Emerging benchmarks for VLAs that\nincorporate language often come with limited evaluation tasks and do not intend\nto investigate how much VLM pretraining truly contributes to the generalization\ncapabilities of the downstream robotic policy. Meanwhile, much research relies\non real-world robot setups designed in isolation by different institutions,\nwhich creates a barrier for reproducibility and accessibility. To address this\ngap, we introduce a unified probing suite of 50 simulation-based tasks across\n10 subcategories spanning language instruction, vision, and objects. We\nsystematically evaluate several state-of-the-art VLA architectures on this\nsuite to understand their generalization capability. Our results show that\nwhile VLM backbones endow VLAs with robust perceptual understanding and high\nlevel planning, which we refer to as good intentions, this does not reliably\ntranslate into precise motor execution: when faced with out-of-distribution\nobservations, policies often exhibit coherent intentions, but falter in action\nexecution. Moreover, finetuning on action data can erode the original VLM's\ngeneralist reasoning abilities. We release our task suite and evaluation code\nto serve as a standardized benchmark for future VLAs and to drive research on\nclosing the perception-to-action gap. More information, including the source\ncode, can be found at https://ai4ce.github.io/INT-ACT/\n", "link": "http://arxiv.org/abs/2506.09930v1", "date": "2025-06-11", "relevancy": 2.3802, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5974}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5974}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Intention%20to%20Execution%3A%20Probing%20the%20Generalization%20Boundaries%20of%0A%20%20Vision-Language-Action%20Models&body=Title%3A%20From%20Intention%20to%20Execution%3A%20Probing%20the%20Generalization%20Boundaries%20of%0A%20%20Vision-Language-Action%20Models%0AAuthor%3A%20Irving%20Fang%20and%20Juexiao%20Zhang%20and%20Shengbang%20Tong%20and%20Chen%20Feng%0AAbstract%3A%20%20%20One%20promise%20that%20Vision-Language-Action%20%28VLA%29%20models%20hold%20over%20traditional%0Aimitation%20learning%20for%20robotics%20is%20to%20leverage%20the%20broad%20generalization%0Acapabilities%20of%20large%20Vision-Language%20Models%20%28VLMs%29%20to%20produce%20versatile%2C%0A%22generalist%22%20robot%20policies.%20However%2C%20current%20evaluations%20of%20VLAs%20remain%0Ainsufficient.%20Traditional%20imitation%20learning%20benchmarks%20are%20unsuitable%20due%20to%0Athe%20lack%20of%20language%20instructions.%20Emerging%20benchmarks%20for%20VLAs%20that%0Aincorporate%20language%20often%20come%20with%20limited%20evaluation%20tasks%20and%20do%20not%20intend%0Ato%20investigate%20how%20much%20VLM%20pretraining%20truly%20contributes%20to%20the%20generalization%0Acapabilities%20of%20the%20downstream%20robotic%20policy.%20Meanwhile%2C%20much%20research%20relies%0Aon%20real-world%20robot%20setups%20designed%20in%20isolation%20by%20different%20institutions%2C%0Awhich%20creates%20a%20barrier%20for%20reproducibility%20and%20accessibility.%20To%20address%20this%0Agap%2C%20we%20introduce%20a%20unified%20probing%20suite%20of%2050%20simulation-based%20tasks%20across%0A10%20subcategories%20spanning%20language%20instruction%2C%20vision%2C%20and%20objects.%20We%0Asystematically%20evaluate%20several%20state-of-the-art%20VLA%20architectures%20on%20this%0Asuite%20to%20understand%20their%20generalization%20capability.%20Our%20results%20show%20that%0Awhile%20VLM%20backbones%20endow%20VLAs%20with%20robust%20perceptual%20understanding%20and%20high%0Alevel%20planning%2C%20which%20we%20refer%20to%20as%20good%20intentions%2C%20this%20does%20not%20reliably%0Atranslate%20into%20precise%20motor%20execution%3A%20when%20faced%20with%20out-of-distribution%0Aobservations%2C%20policies%20often%20exhibit%20coherent%20intentions%2C%20but%20falter%20in%20action%0Aexecution.%20Moreover%2C%20finetuning%20on%20action%20data%20can%20erode%20the%20original%20VLM%27s%0Ageneralist%20reasoning%20abilities.%20We%20release%20our%20task%20suite%20and%20evaluation%20code%0Ato%20serve%20as%20a%20standardized%20benchmark%20for%20future%20VLAs%20and%20to%20drive%20research%20on%0Aclosing%20the%20perception-to-action%20gap.%20More%20information%2C%20including%20the%20source%0Acode%2C%20can%20be%20found%20at%20https%3A//ai4ce.github.io/INT-ACT/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Intention%2520to%2520Execution%253A%2520Probing%2520the%2520Generalization%2520Boundaries%2520of%250A%2520%2520Vision-Language-Action%2520Models%26entry.906535625%3DIrving%2520Fang%2520and%2520Juexiao%2520Zhang%2520and%2520Shengbang%2520Tong%2520and%2520Chen%2520Feng%26entry.1292438233%3D%2520%2520One%2520promise%2520that%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520hold%2520over%2520traditional%250Aimitation%2520learning%2520for%2520robotics%2520is%2520to%2520leverage%2520the%2520broad%2520generalization%250Acapabilities%2520of%2520large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520to%2520produce%2520versatile%252C%250A%2522generalist%2522%2520robot%2520policies.%2520However%252C%2520current%2520evaluations%2520of%2520VLAs%2520remain%250Ainsufficient.%2520Traditional%2520imitation%2520learning%2520benchmarks%2520are%2520unsuitable%2520due%2520to%250Athe%2520lack%2520of%2520language%2520instructions.%2520Emerging%2520benchmarks%2520for%2520VLAs%2520that%250Aincorporate%2520language%2520often%2520come%2520with%2520limited%2520evaluation%2520tasks%2520and%2520do%2520not%2520intend%250Ato%2520investigate%2520how%2520much%2520VLM%2520pretraining%2520truly%2520contributes%2520to%2520the%2520generalization%250Acapabilities%2520of%2520the%2520downstream%2520robotic%2520policy.%2520Meanwhile%252C%2520much%2520research%2520relies%250Aon%2520real-world%2520robot%2520setups%2520designed%2520in%2520isolation%2520by%2520different%2520institutions%252C%250Awhich%2520creates%2520a%2520barrier%2520for%2520reproducibility%2520and%2520accessibility.%2520To%2520address%2520this%250Agap%252C%2520we%2520introduce%2520a%2520unified%2520probing%2520suite%2520of%252050%2520simulation-based%2520tasks%2520across%250A10%2520subcategories%2520spanning%2520language%2520instruction%252C%2520vision%252C%2520and%2520objects.%2520We%250Asystematically%2520evaluate%2520several%2520state-of-the-art%2520VLA%2520architectures%2520on%2520this%250Asuite%2520to%2520understand%2520their%2520generalization%2520capability.%2520Our%2520results%2520show%2520that%250Awhile%2520VLM%2520backbones%2520endow%2520VLAs%2520with%2520robust%2520perceptual%2520understanding%2520and%2520high%250Alevel%2520planning%252C%2520which%2520we%2520refer%2520to%2520as%2520good%2520intentions%252C%2520this%2520does%2520not%2520reliably%250Atranslate%2520into%2520precise%2520motor%2520execution%253A%2520when%2520faced%2520with%2520out-of-distribution%250Aobservations%252C%2520policies%2520often%2520exhibit%2520coherent%2520intentions%252C%2520but%2520falter%2520in%2520action%250Aexecution.%2520Moreover%252C%2520finetuning%2520on%2520action%2520data%2520can%2520erode%2520the%2520original%2520VLM%2527s%250Ageneralist%2520reasoning%2520abilities.%2520We%2520release%2520our%2520task%2520suite%2520and%2520evaluation%2520code%250Ato%2520serve%2520as%2520a%2520standardized%2520benchmark%2520for%2520future%2520VLAs%2520and%2520to%2520drive%2520research%2520on%250Aclosing%2520the%2520perception-to-action%2520gap.%2520More%2520information%252C%2520including%2520the%2520source%250Acode%252C%2520can%2520be%2520found%2520at%2520https%253A//ai4ce.github.io/INT-ACT/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Intention%20to%20Execution%3A%20Probing%20the%20Generalization%20Boundaries%20of%0A%20%20Vision-Language-Action%20Models&entry.906535625=Irving%20Fang%20and%20Juexiao%20Zhang%20and%20Shengbang%20Tong%20and%20Chen%20Feng&entry.1292438233=%20%20One%20promise%20that%20Vision-Language-Action%20%28VLA%29%20models%20hold%20over%20traditional%0Aimitation%20learning%20for%20robotics%20is%20to%20leverage%20the%20broad%20generalization%0Acapabilities%20of%20large%20Vision-Language%20Models%20%28VLMs%29%20to%20produce%20versatile%2C%0A%22generalist%22%20robot%20policies.%20However%2C%20current%20evaluations%20of%20VLAs%20remain%0Ainsufficient.%20Traditional%20imitation%20learning%20benchmarks%20are%20unsuitable%20due%20to%0Athe%20lack%20of%20language%20instructions.%20Emerging%20benchmarks%20for%20VLAs%20that%0Aincorporate%20language%20often%20come%20with%20limited%20evaluation%20tasks%20and%20do%20not%20intend%0Ato%20investigate%20how%20much%20VLM%20pretraining%20truly%20contributes%20to%20the%20generalization%0Acapabilities%20of%20the%20downstream%20robotic%20policy.%20Meanwhile%2C%20much%20research%20relies%0Aon%20real-world%20robot%20setups%20designed%20in%20isolation%20by%20different%20institutions%2C%0Awhich%20creates%20a%20barrier%20for%20reproducibility%20and%20accessibility.%20To%20address%20this%0Agap%2C%20we%20introduce%20a%20unified%20probing%20suite%20of%2050%20simulation-based%20tasks%20across%0A10%20subcategories%20spanning%20language%20instruction%2C%20vision%2C%20and%20objects.%20We%0Asystematically%20evaluate%20several%20state-of-the-art%20VLA%20architectures%20on%20this%0Asuite%20to%20understand%20their%20generalization%20capability.%20Our%20results%20show%20that%0Awhile%20VLM%20backbones%20endow%20VLAs%20with%20robust%20perceptual%20understanding%20and%20high%0Alevel%20planning%2C%20which%20we%20refer%20to%20as%20good%20intentions%2C%20this%20does%20not%20reliably%0Atranslate%20into%20precise%20motor%20execution%3A%20when%20faced%20with%20out-of-distribution%0Aobservations%2C%20policies%20often%20exhibit%20coherent%20intentions%2C%20but%20falter%20in%20action%0Aexecution.%20Moreover%2C%20finetuning%20on%20action%20data%20can%20erode%20the%20original%20VLM%27s%0Ageneralist%20reasoning%20abilities.%20We%20release%20our%20task%20suite%20and%20evaluation%20code%0Ato%20serve%20as%20a%20standardized%20benchmark%20for%20future%20VLAs%20and%20to%20drive%20research%20on%0Aclosing%20the%20perception-to-action%20gap.%20More%20information%2C%20including%20the%20source%0Acode%2C%20can%20be%20found%20at%20https%3A//ai4ce.github.io/INT-ACT/%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09930v1&entry.124074799=Read"},
{"title": "Load-Aware Training Scheduling for Model Circulation-based Decentralized\n  Federated Learning", "author": "Haruki Kainuma and Takayuki Nishio", "abstract": "  This paper proposes Load-aware Tram-FL, an extension of Tram-FL that\nintroduces a training scheduling mechanism to minimize total training time in\ndecentralized federated learning by accounting for both computational and\ncommunication loads. The scheduling problem is formulated as a global\noptimization task, which-though intractable in its original form-is made\nsolvable by decomposing it into node-wise subproblems. To promote balanced data\nutilization under non-IID distributions, a variance constraint is introduced,\nwhile the overall training latency, including both computation and\ncommunication costs, is minimized through the objective function. Simulation\nresults on MNIST and CIFAR-10 demonstrate that Load-aware Tram-FL significantly\nreduces training time and accelerates convergence compared to baseline methods.\n", "link": "http://arxiv.org/abs/2506.09769v1", "date": "2025-06-11", "relevancy": 2.3762, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4842}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.48}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Load-Aware%20Training%20Scheduling%20for%20Model%20Circulation-based%20Decentralized%0A%20%20Federated%20Learning&body=Title%3A%20Load-Aware%20Training%20Scheduling%20for%20Model%20Circulation-based%20Decentralized%0A%20%20Federated%20Learning%0AAuthor%3A%20Haruki%20Kainuma%20and%20Takayuki%20Nishio%0AAbstract%3A%20%20%20This%20paper%20proposes%20Load-aware%20Tram-FL%2C%20an%20extension%20of%20Tram-FL%20that%0Aintroduces%20a%20training%20scheduling%20mechanism%20to%20minimize%20total%20training%20time%20in%0Adecentralized%20federated%20learning%20by%20accounting%20for%20both%20computational%20and%0Acommunication%20loads.%20The%20scheduling%20problem%20is%20formulated%20as%20a%20global%0Aoptimization%20task%2C%20which-though%20intractable%20in%20its%20original%20form-is%20made%0Asolvable%20by%20decomposing%20it%20into%20node-wise%20subproblems.%20To%20promote%20balanced%20data%0Autilization%20under%20non-IID%20distributions%2C%20a%20variance%20constraint%20is%20introduced%2C%0Awhile%20the%20overall%20training%20latency%2C%20including%20both%20computation%20and%0Acommunication%20costs%2C%20is%20minimized%20through%20the%20objective%20function.%20Simulation%0Aresults%20on%20MNIST%20and%20CIFAR-10%20demonstrate%20that%20Load-aware%20Tram-FL%20significantly%0Areduces%20training%20time%20and%20accelerates%20convergence%20compared%20to%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoad-Aware%2520Training%2520Scheduling%2520for%2520Model%2520Circulation-based%2520Decentralized%250A%2520%2520Federated%2520Learning%26entry.906535625%3DHaruki%2520Kainuma%2520and%2520Takayuki%2520Nishio%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520Load-aware%2520Tram-FL%252C%2520an%2520extension%2520of%2520Tram-FL%2520that%250Aintroduces%2520a%2520training%2520scheduling%2520mechanism%2520to%2520minimize%2520total%2520training%2520time%2520in%250Adecentralized%2520federated%2520learning%2520by%2520accounting%2520for%2520both%2520computational%2520and%250Acommunication%2520loads.%2520The%2520scheduling%2520problem%2520is%2520formulated%2520as%2520a%2520global%250Aoptimization%2520task%252C%2520which-though%2520intractable%2520in%2520its%2520original%2520form-is%2520made%250Asolvable%2520by%2520decomposing%2520it%2520into%2520node-wise%2520subproblems.%2520To%2520promote%2520balanced%2520data%250Autilization%2520under%2520non-IID%2520distributions%252C%2520a%2520variance%2520constraint%2520is%2520introduced%252C%250Awhile%2520the%2520overall%2520training%2520latency%252C%2520including%2520both%2520computation%2520and%250Acommunication%2520costs%252C%2520is%2520minimized%2520through%2520the%2520objective%2520function.%2520Simulation%250Aresults%2520on%2520MNIST%2520and%2520CIFAR-10%2520demonstrate%2520that%2520Load-aware%2520Tram-FL%2520significantly%250Areduces%2520training%2520time%2520and%2520accelerates%2520convergence%2520compared%2520to%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Load-Aware%20Training%20Scheduling%20for%20Model%20Circulation-based%20Decentralized%0A%20%20Federated%20Learning&entry.906535625=Haruki%20Kainuma%20and%20Takayuki%20Nishio&entry.1292438233=%20%20This%20paper%20proposes%20Load-aware%20Tram-FL%2C%20an%20extension%20of%20Tram-FL%20that%0Aintroduces%20a%20training%20scheduling%20mechanism%20to%20minimize%20total%20training%20time%20in%0Adecentralized%20federated%20learning%20by%20accounting%20for%20both%20computational%20and%0Acommunication%20loads.%20The%20scheduling%20problem%20is%20formulated%20as%20a%20global%0Aoptimization%20task%2C%20which-though%20intractable%20in%20its%20original%20form-is%20made%0Asolvable%20by%20decomposing%20it%20into%20node-wise%20subproblems.%20To%20promote%20balanced%20data%0Autilization%20under%20non-IID%20distributions%2C%20a%20variance%20constraint%20is%20introduced%2C%0Awhile%20the%20overall%20training%20latency%2C%20including%20both%20computation%20and%0Acommunication%20costs%2C%20is%20minimized%20through%20the%20objective%20function.%20Simulation%0Aresults%20on%20MNIST%20and%20CIFAR-10%20demonstrate%20that%20Load-aware%20Tram-FL%20significantly%0Areduces%20training%20time%20and%20accelerates%20convergence%20compared%20to%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09769v1&entry.124074799=Read"},
{"title": "PlayerOne: Egocentric World Simulator", "author": "Yuanpeng Tu and Hao Luo and Xi Chen and Xiang Bai and Fan Wang and Hengshuang Zhao", "abstract": "  We introduce PlayerOne, the first egocentric realistic world simulator,\nfacilitating immersive and unrestricted exploration within vividly dynamic\nenvironments. Given an egocentric scene image from the user, PlayerOne can\naccurately construct the corresponding world and generate egocentric videos\nthat are strictly aligned with the real scene human motion of the user captured\nby an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that\nfirst performs pretraining on large-scale egocentric text-video pairs for\ncoarse-level egocentric understanding, followed by finetuning on synchronous\nmotion-video data extracted from egocentric-exocentric video datasets with our\nautomatic construction pipeline. Besides, considering the varying importance of\ndifferent components, we design a part-disentangled motion injection scheme,\nenabling precise control of part-level movements. In addition, we devise a\njoint reconstruction framework that progressively models both the 4D scene and\nvideo frames, ensuring scene consistency in the long-form video generation.\nExperimental results demonstrate its great generalization ability in precise\ncontrol of varying human movements and worldconsistent modeling of diverse\nscenarios. It marks the first endeavor into egocentric real-world simulation\nand can pave the way for the community to delve into fresh frontiers of world\nmodeling and its diverse applications.\n", "link": "http://arxiv.org/abs/2506.09995v1", "date": "2025-06-11", "relevancy": 2.3698, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6025}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5884}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PlayerOne%3A%20Egocentric%20World%20Simulator&body=Title%3A%20PlayerOne%3A%20Egocentric%20World%20Simulator%0AAuthor%3A%20Yuanpeng%20Tu%20and%20Hao%20Luo%20and%20Xi%20Chen%20and%20Xiang%20Bai%20and%20Fan%20Wang%20and%20Hengshuang%20Zhao%0AAbstract%3A%20%20%20We%20introduce%20PlayerOne%2C%20the%20first%20egocentric%20realistic%20world%20simulator%2C%0Afacilitating%20immersive%20and%20unrestricted%20exploration%20within%20vividly%20dynamic%0Aenvironments.%20Given%20an%20egocentric%20scene%20image%20from%20the%20user%2C%20PlayerOne%20can%0Aaccurately%20construct%20the%20corresponding%20world%20and%20generate%20egocentric%20videos%0Athat%20are%20strictly%20aligned%20with%20the%20real%20scene%20human%20motion%20of%20the%20user%20captured%0Aby%20an%20exocentric%20camera.%20PlayerOne%20is%20trained%20in%20a%20coarse-to-fine%20pipeline%20that%0Afirst%20performs%20pretraining%20on%20large-scale%20egocentric%20text-video%20pairs%20for%0Acoarse-level%20egocentric%20understanding%2C%20followed%20by%20finetuning%20on%20synchronous%0Amotion-video%20data%20extracted%20from%20egocentric-exocentric%20video%20datasets%20with%20our%0Aautomatic%20construction%20pipeline.%20Besides%2C%20considering%20the%20varying%20importance%20of%0Adifferent%20components%2C%20we%20design%20a%20part-disentangled%20motion%20injection%20scheme%2C%0Aenabling%20precise%20control%20of%20part-level%20movements.%20In%20addition%2C%20we%20devise%20a%0Ajoint%20reconstruction%20framework%20that%20progressively%20models%20both%20the%204D%20scene%20and%0Avideo%20frames%2C%20ensuring%20scene%20consistency%20in%20the%20long-form%20video%20generation.%0AExperimental%20results%20demonstrate%20its%20great%20generalization%20ability%20in%20precise%0Acontrol%20of%20varying%20human%20movements%20and%20worldconsistent%20modeling%20of%20diverse%0Ascenarios.%20It%20marks%20the%20first%20endeavor%20into%20egocentric%20real-world%20simulation%0Aand%20can%20pave%20the%20way%20for%20the%20community%20to%20delve%20into%20fresh%20frontiers%20of%20world%0Amodeling%20and%20its%20diverse%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlayerOne%253A%2520Egocentric%2520World%2520Simulator%26entry.906535625%3DYuanpeng%2520Tu%2520and%2520Hao%2520Luo%2520and%2520Xi%2520Chen%2520and%2520Xiang%2520Bai%2520and%2520Fan%2520Wang%2520and%2520Hengshuang%2520Zhao%26entry.1292438233%3D%2520%2520We%2520introduce%2520PlayerOne%252C%2520the%2520first%2520egocentric%2520realistic%2520world%2520simulator%252C%250Afacilitating%2520immersive%2520and%2520unrestricted%2520exploration%2520within%2520vividly%2520dynamic%250Aenvironments.%2520Given%2520an%2520egocentric%2520scene%2520image%2520from%2520the%2520user%252C%2520PlayerOne%2520can%250Aaccurately%2520construct%2520the%2520corresponding%2520world%2520and%2520generate%2520egocentric%2520videos%250Athat%2520are%2520strictly%2520aligned%2520with%2520the%2520real%2520scene%2520human%2520motion%2520of%2520the%2520user%2520captured%250Aby%2520an%2520exocentric%2520camera.%2520PlayerOne%2520is%2520trained%2520in%2520a%2520coarse-to-fine%2520pipeline%2520that%250Afirst%2520performs%2520pretraining%2520on%2520large-scale%2520egocentric%2520text-video%2520pairs%2520for%250Acoarse-level%2520egocentric%2520understanding%252C%2520followed%2520by%2520finetuning%2520on%2520synchronous%250Amotion-video%2520data%2520extracted%2520from%2520egocentric-exocentric%2520video%2520datasets%2520with%2520our%250Aautomatic%2520construction%2520pipeline.%2520Besides%252C%2520considering%2520the%2520varying%2520importance%2520of%250Adifferent%2520components%252C%2520we%2520design%2520a%2520part-disentangled%2520motion%2520injection%2520scheme%252C%250Aenabling%2520precise%2520control%2520of%2520part-level%2520movements.%2520In%2520addition%252C%2520we%2520devise%2520a%250Ajoint%2520reconstruction%2520framework%2520that%2520progressively%2520models%2520both%2520the%25204D%2520scene%2520and%250Avideo%2520frames%252C%2520ensuring%2520scene%2520consistency%2520in%2520the%2520long-form%2520video%2520generation.%250AExperimental%2520results%2520demonstrate%2520its%2520great%2520generalization%2520ability%2520in%2520precise%250Acontrol%2520of%2520varying%2520human%2520movements%2520and%2520worldconsistent%2520modeling%2520of%2520diverse%250Ascenarios.%2520It%2520marks%2520the%2520first%2520endeavor%2520into%2520egocentric%2520real-world%2520simulation%250Aand%2520can%2520pave%2520the%2520way%2520for%2520the%2520community%2520to%2520delve%2520into%2520fresh%2520frontiers%2520of%2520world%250Amodeling%2520and%2520its%2520diverse%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PlayerOne%3A%20Egocentric%20World%20Simulator&entry.906535625=Yuanpeng%20Tu%20and%20Hao%20Luo%20and%20Xi%20Chen%20and%20Xiang%20Bai%20and%20Fan%20Wang%20and%20Hengshuang%20Zhao&entry.1292438233=%20%20We%20introduce%20PlayerOne%2C%20the%20first%20egocentric%20realistic%20world%20simulator%2C%0Afacilitating%20immersive%20and%20unrestricted%20exploration%20within%20vividly%20dynamic%0Aenvironments.%20Given%20an%20egocentric%20scene%20image%20from%20the%20user%2C%20PlayerOne%20can%0Aaccurately%20construct%20the%20corresponding%20world%20and%20generate%20egocentric%20videos%0Athat%20are%20strictly%20aligned%20with%20the%20real%20scene%20human%20motion%20of%20the%20user%20captured%0Aby%20an%20exocentric%20camera.%20PlayerOne%20is%20trained%20in%20a%20coarse-to-fine%20pipeline%20that%0Afirst%20performs%20pretraining%20on%20large-scale%20egocentric%20text-video%20pairs%20for%0Acoarse-level%20egocentric%20understanding%2C%20followed%20by%20finetuning%20on%20synchronous%0Amotion-video%20data%20extracted%20from%20egocentric-exocentric%20video%20datasets%20with%20our%0Aautomatic%20construction%20pipeline.%20Besides%2C%20considering%20the%20varying%20importance%20of%0Adifferent%20components%2C%20we%20design%20a%20part-disentangled%20motion%20injection%20scheme%2C%0Aenabling%20precise%20control%20of%20part-level%20movements.%20In%20addition%2C%20we%20devise%20a%0Ajoint%20reconstruction%20framework%20that%20progressively%20models%20both%20the%204D%20scene%20and%0Avideo%20frames%2C%20ensuring%20scene%20consistency%20in%20the%20long-form%20video%20generation.%0AExperimental%20results%20demonstrate%20its%20great%20generalization%20ability%20in%20precise%0Acontrol%20of%20varying%20human%20movements%20and%20worldconsistent%20modeling%20of%20diverse%0Ascenarios.%20It%20marks%20the%20first%20endeavor%20into%20egocentric%20real-world%20simulation%0Aand%20can%20pave%20the%20way%20for%20the%20community%20to%20delve%20into%20fresh%20frontiers%20of%20world%0Amodeling%20and%20its%20diverse%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09995v1&entry.124074799=Read"},
{"title": "Vision Generalist Model: A Survey", "author": "Ziyi Wang and Yongming Rao and Shuofeng Sun and Xinrun Liu and Yi Wei and Xumin Yu and Zuyan Liu and Yanbo Wang and Hongmin Liu and Jie Zhou and Jiwen Lu", "abstract": "  Recently, we have witnessed the great success of the generalist model in\nnatural language processing. The generalist model is a general framework\ntrained with massive data and is able to process various downstream tasks\nsimultaneously. Encouraged by their impressive performance, an increasing\nnumber of researchers are venturing into the realm of applying these models to\ncomputer vision tasks. However, the inputs and outputs of vision tasks are more\ndiverse, and it is difficult to summarize them as a unified representation. In\nthis paper, we provide a comprehensive overview of the vision generalist\nmodels, delving into their characteristics and capabilities within the field.\nFirst, we review the background, including the datasets, tasks, and benchmarks.\nThen, we dig into the design of frameworks that have been proposed in existing\nresearch, while also introducing the techniques employed to enhance their\nperformance. To better help the researchers comprehend the area, we take a\nbrief excursion into related domains, shedding light on their interconnections\nand potential synergies. To conclude, we provide some real-world application\nscenarios, undertake a thorough examination of the persistent challenges, and\noffer insights into possible directions for future research endeavors.\n", "link": "http://arxiv.org/abs/2506.09954v1", "date": "2025-06-11", "relevancy": 2.3661, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6065}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6065}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Generalist%20Model%3A%20A%20Survey&body=Title%3A%20Vision%20Generalist%20Model%3A%20A%20Survey%0AAuthor%3A%20Ziyi%20Wang%20and%20Yongming%20Rao%20and%20Shuofeng%20Sun%20and%20Xinrun%20Liu%20and%20Yi%20Wei%20and%20Xumin%20Yu%20and%20Zuyan%20Liu%20and%20Yanbo%20Wang%20and%20Hongmin%20Liu%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20Recently%2C%20we%20have%20witnessed%20the%20great%20success%20of%20the%20generalist%20model%20in%0Anatural%20language%20processing.%20The%20generalist%20model%20is%20a%20general%20framework%0Atrained%20with%20massive%20data%20and%20is%20able%20to%20process%20various%20downstream%20tasks%0Asimultaneously.%20Encouraged%20by%20their%20impressive%20performance%2C%20an%20increasing%0Anumber%20of%20researchers%20are%20venturing%20into%20the%20realm%20of%20applying%20these%20models%20to%0Acomputer%20vision%20tasks.%20However%2C%20the%20inputs%20and%20outputs%20of%20vision%20tasks%20are%20more%0Adiverse%2C%20and%20it%20is%20difficult%20to%20summarize%20them%20as%20a%20unified%20representation.%20In%0Athis%20paper%2C%20we%20provide%20a%20comprehensive%20overview%20of%20the%20vision%20generalist%0Amodels%2C%20delving%20into%20their%20characteristics%20and%20capabilities%20within%20the%20field.%0AFirst%2C%20we%20review%20the%20background%2C%20including%20the%20datasets%2C%20tasks%2C%20and%20benchmarks.%0AThen%2C%20we%20dig%20into%20the%20design%20of%20frameworks%20that%20have%20been%20proposed%20in%20existing%0Aresearch%2C%20while%20also%20introducing%20the%20techniques%20employed%20to%20enhance%20their%0Aperformance.%20To%20better%20help%20the%20researchers%20comprehend%20the%20area%2C%20we%20take%20a%0Abrief%20excursion%20into%20related%20domains%2C%20shedding%20light%20on%20their%20interconnections%0Aand%20potential%20synergies.%20To%20conclude%2C%20we%20provide%20some%20real-world%20application%0Ascenarios%2C%20undertake%20a%20thorough%20examination%20of%20the%20persistent%20challenges%2C%20and%0Aoffer%20insights%20into%20possible%20directions%20for%20future%20research%20endeavors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09954v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Generalist%2520Model%253A%2520A%2520Survey%26entry.906535625%3DZiyi%2520Wang%2520and%2520Yongming%2520Rao%2520and%2520Shuofeng%2520Sun%2520and%2520Xinrun%2520Liu%2520and%2520Yi%2520Wei%2520and%2520Xumin%2520Yu%2520and%2520Zuyan%2520Liu%2520and%2520Yanbo%2520Wang%2520and%2520Hongmin%2520Liu%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520Recently%252C%2520we%2520have%2520witnessed%2520the%2520great%2520success%2520of%2520the%2520generalist%2520model%2520in%250Anatural%2520language%2520processing.%2520The%2520generalist%2520model%2520is%2520a%2520general%2520framework%250Atrained%2520with%2520massive%2520data%2520and%2520is%2520able%2520to%2520process%2520various%2520downstream%2520tasks%250Asimultaneously.%2520Encouraged%2520by%2520their%2520impressive%2520performance%252C%2520an%2520increasing%250Anumber%2520of%2520researchers%2520are%2520venturing%2520into%2520the%2520realm%2520of%2520applying%2520these%2520models%2520to%250Acomputer%2520vision%2520tasks.%2520However%252C%2520the%2520inputs%2520and%2520outputs%2520of%2520vision%2520tasks%2520are%2520more%250Adiverse%252C%2520and%2520it%2520is%2520difficult%2520to%2520summarize%2520them%2520as%2520a%2520unified%2520representation.%2520In%250Athis%2520paper%252C%2520we%2520provide%2520a%2520comprehensive%2520overview%2520of%2520the%2520vision%2520generalist%250Amodels%252C%2520delving%2520into%2520their%2520characteristics%2520and%2520capabilities%2520within%2520the%2520field.%250AFirst%252C%2520we%2520review%2520the%2520background%252C%2520including%2520the%2520datasets%252C%2520tasks%252C%2520and%2520benchmarks.%250AThen%252C%2520we%2520dig%2520into%2520the%2520design%2520of%2520frameworks%2520that%2520have%2520been%2520proposed%2520in%2520existing%250Aresearch%252C%2520while%2520also%2520introducing%2520the%2520techniques%2520employed%2520to%2520enhance%2520their%250Aperformance.%2520To%2520better%2520help%2520the%2520researchers%2520comprehend%2520the%2520area%252C%2520we%2520take%2520a%250Abrief%2520excursion%2520into%2520related%2520domains%252C%2520shedding%2520light%2520on%2520their%2520interconnections%250Aand%2520potential%2520synergies.%2520To%2520conclude%252C%2520we%2520provide%2520some%2520real-world%2520application%250Ascenarios%252C%2520undertake%2520a%2520thorough%2520examination%2520of%2520the%2520persistent%2520challenges%252C%2520and%250Aoffer%2520insights%2520into%2520possible%2520directions%2520for%2520future%2520research%2520endeavors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09954v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Generalist%20Model%3A%20A%20Survey&entry.906535625=Ziyi%20Wang%20and%20Yongming%20Rao%20and%20Shuofeng%20Sun%20and%20Xinrun%20Liu%20and%20Yi%20Wei%20and%20Xumin%20Yu%20and%20Zuyan%20Liu%20and%20Yanbo%20Wang%20and%20Hongmin%20Liu%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%20Recently%2C%20we%20have%20witnessed%20the%20great%20success%20of%20the%20generalist%20model%20in%0Anatural%20language%20processing.%20The%20generalist%20model%20is%20a%20general%20framework%0Atrained%20with%20massive%20data%20and%20is%20able%20to%20process%20various%20downstream%20tasks%0Asimultaneously.%20Encouraged%20by%20their%20impressive%20performance%2C%20an%20increasing%0Anumber%20of%20researchers%20are%20venturing%20into%20the%20realm%20of%20applying%20these%20models%20to%0Acomputer%20vision%20tasks.%20However%2C%20the%20inputs%20and%20outputs%20of%20vision%20tasks%20are%20more%0Adiverse%2C%20and%20it%20is%20difficult%20to%20summarize%20them%20as%20a%20unified%20representation.%20In%0Athis%20paper%2C%20we%20provide%20a%20comprehensive%20overview%20of%20the%20vision%20generalist%0Amodels%2C%20delving%20into%20their%20characteristics%20and%20capabilities%20within%20the%20field.%0AFirst%2C%20we%20review%20the%20background%2C%20including%20the%20datasets%2C%20tasks%2C%20and%20benchmarks.%0AThen%2C%20we%20dig%20into%20the%20design%20of%20frameworks%20that%20have%20been%20proposed%20in%20existing%0Aresearch%2C%20while%20also%20introducing%20the%20techniques%20employed%20to%20enhance%20their%0Aperformance.%20To%20better%20help%20the%20researchers%20comprehend%20the%20area%2C%20we%20take%20a%0Abrief%20excursion%20into%20related%20domains%2C%20shedding%20light%20on%20their%20interconnections%0Aand%20potential%20synergies.%20To%20conclude%2C%20we%20provide%20some%20real-world%20application%0Ascenarios%2C%20undertake%20a%20thorough%20examination%20of%20the%20persistent%20challenges%2C%20and%0Aoffer%20insights%20into%20possible%20directions%20for%20future%20research%20endeavors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09954v1&entry.124074799=Read"},
{"title": "Guided Graph Compression for Quantum Graph Neural Networks", "author": "Mikel Casals and Vasilis Belis and Elias F. Combarro and Eduard Alarc\u00f3n and Sofia Vallecorsa and Michele Grossi", "abstract": "  Graph Neural Networks (GNNs) are effective for processing graph-structured\ndata but face challenges with large graphs due to high memory requirements and\ninefficient sparse matrix operations on GPUs. Quantum Computing (QC) offers a\npromising avenue to address these issues and inspires new algorithmic\napproaches. In particular, Quantum Graph Neural Networks (QGNNs) have been\nexplored in recent literature. However, current quantum hardware limits the\ndimension of the data that can be effectively encoded. Existing approaches\neither simplify datasets manually or use artificial graph datasets. This work\nintroduces the Guided Graph Compression (GGC) framework, which uses a graph\nautoencoder to reduce both the number of nodes and the dimensionality of node\nfeatures. The compression is guided to enhance the performance of a downstream\nclassification task, which can be applied either with a quantum or a classical\nclassifier. The framework is evaluated on the Jet Tagging task, a\nclassification problem of fundamental importance in high energy physics that\ninvolves distinguishing particle jets initiated by quarks from those by gluons.\nThe GGC is compared against using the autoencoder as a standalone preprocessing\nstep and against a baseline classical GNN classifier. Our numerical results\ndemonstrate that GGC outperforms both alternatives, while also facilitating the\ntesting of novel QGNN ansatzes on realistic datasets.\n", "link": "http://arxiv.org/abs/2506.09862v1", "date": "2025-06-11", "relevancy": 2.3447, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4797}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4692}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guided%20Graph%20Compression%20for%20Quantum%20Graph%20Neural%20Networks&body=Title%3A%20Guided%20Graph%20Compression%20for%20Quantum%20Graph%20Neural%20Networks%0AAuthor%3A%20Mikel%20Casals%20and%20Vasilis%20Belis%20and%20Elias%20F.%20Combarro%20and%20Eduard%20Alarc%C3%B3n%20and%20Sofia%20Vallecorsa%20and%20Michele%20Grossi%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20effective%20for%20processing%20graph-structured%0Adata%20but%20face%20challenges%20with%20large%20graphs%20due%20to%20high%20memory%20requirements%20and%0Ainefficient%20sparse%20matrix%20operations%20on%20GPUs.%20Quantum%20Computing%20%28QC%29%20offers%20a%0Apromising%20avenue%20to%20address%20these%20issues%20and%20inspires%20new%20algorithmic%0Aapproaches.%20In%20particular%2C%20Quantum%20Graph%20Neural%20Networks%20%28QGNNs%29%20have%20been%0Aexplored%20in%20recent%20literature.%20However%2C%20current%20quantum%20hardware%20limits%20the%0Adimension%20of%20the%20data%20that%20can%20be%20effectively%20encoded.%20Existing%20approaches%0Aeither%20simplify%20datasets%20manually%20or%20use%20artificial%20graph%20datasets.%20This%20work%0Aintroduces%20the%20Guided%20Graph%20Compression%20%28GGC%29%20framework%2C%20which%20uses%20a%20graph%0Aautoencoder%20to%20reduce%20both%20the%20number%20of%20nodes%20and%20the%20dimensionality%20of%20node%0Afeatures.%20The%20compression%20is%20guided%20to%20enhance%20the%20performance%20of%20a%20downstream%0Aclassification%20task%2C%20which%20can%20be%20applied%20either%20with%20a%20quantum%20or%20a%20classical%0Aclassifier.%20The%20framework%20is%20evaluated%20on%20the%20Jet%20Tagging%20task%2C%20a%0Aclassification%20problem%20of%20fundamental%20importance%20in%20high%20energy%20physics%20that%0Ainvolves%20distinguishing%20particle%20jets%20initiated%20by%20quarks%20from%20those%20by%20gluons.%0AThe%20GGC%20is%20compared%20against%20using%20the%20autoencoder%20as%20a%20standalone%20preprocessing%0Astep%20and%20against%20a%20baseline%20classical%20GNN%20classifier.%20Our%20numerical%20results%0Ademonstrate%20that%20GGC%20outperforms%20both%20alternatives%2C%20while%20also%20facilitating%20the%0Atesting%20of%20novel%20QGNN%20ansatzes%20on%20realistic%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuided%2520Graph%2520Compression%2520for%2520Quantum%2520Graph%2520Neural%2520Networks%26entry.906535625%3DMikel%2520Casals%2520and%2520Vasilis%2520Belis%2520and%2520Elias%2520F.%2520Combarro%2520and%2520Eduard%2520Alarc%25C3%25B3n%2520and%2520Sofia%2520Vallecorsa%2520and%2520Michele%2520Grossi%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520effective%2520for%2520processing%2520graph-structured%250Adata%2520but%2520face%2520challenges%2520with%2520large%2520graphs%2520due%2520to%2520high%2520memory%2520requirements%2520and%250Ainefficient%2520sparse%2520matrix%2520operations%2520on%2520GPUs.%2520Quantum%2520Computing%2520%2528QC%2529%2520offers%2520a%250Apromising%2520avenue%2520to%2520address%2520these%2520issues%2520and%2520inspires%2520new%2520algorithmic%250Aapproaches.%2520In%2520particular%252C%2520Quantum%2520Graph%2520Neural%2520Networks%2520%2528QGNNs%2529%2520have%2520been%250Aexplored%2520in%2520recent%2520literature.%2520However%252C%2520current%2520quantum%2520hardware%2520limits%2520the%250Adimension%2520of%2520the%2520data%2520that%2520can%2520be%2520effectively%2520encoded.%2520Existing%2520approaches%250Aeither%2520simplify%2520datasets%2520manually%2520or%2520use%2520artificial%2520graph%2520datasets.%2520This%2520work%250Aintroduces%2520the%2520Guided%2520Graph%2520Compression%2520%2528GGC%2529%2520framework%252C%2520which%2520uses%2520a%2520graph%250Aautoencoder%2520to%2520reduce%2520both%2520the%2520number%2520of%2520nodes%2520and%2520the%2520dimensionality%2520of%2520node%250Afeatures.%2520The%2520compression%2520is%2520guided%2520to%2520enhance%2520the%2520performance%2520of%2520a%2520downstream%250Aclassification%2520task%252C%2520which%2520can%2520be%2520applied%2520either%2520with%2520a%2520quantum%2520or%2520a%2520classical%250Aclassifier.%2520The%2520framework%2520is%2520evaluated%2520on%2520the%2520Jet%2520Tagging%2520task%252C%2520a%250Aclassification%2520problem%2520of%2520fundamental%2520importance%2520in%2520high%2520energy%2520physics%2520that%250Ainvolves%2520distinguishing%2520particle%2520jets%2520initiated%2520by%2520quarks%2520from%2520those%2520by%2520gluons.%250AThe%2520GGC%2520is%2520compared%2520against%2520using%2520the%2520autoencoder%2520as%2520a%2520standalone%2520preprocessing%250Astep%2520and%2520against%2520a%2520baseline%2520classical%2520GNN%2520classifier.%2520Our%2520numerical%2520results%250Ademonstrate%2520that%2520GGC%2520outperforms%2520both%2520alternatives%252C%2520while%2520also%2520facilitating%2520the%250Atesting%2520of%2520novel%2520QGNN%2520ansatzes%2520on%2520realistic%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guided%20Graph%20Compression%20for%20Quantum%20Graph%20Neural%20Networks&entry.906535625=Mikel%20Casals%20and%20Vasilis%20Belis%20and%20Elias%20F.%20Combarro%20and%20Eduard%20Alarc%C3%B3n%20and%20Sofia%20Vallecorsa%20and%20Michele%20Grossi&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20effective%20for%20processing%20graph-structured%0Adata%20but%20face%20challenges%20with%20large%20graphs%20due%20to%20high%20memory%20requirements%20and%0Ainefficient%20sparse%20matrix%20operations%20on%20GPUs.%20Quantum%20Computing%20%28QC%29%20offers%20a%0Apromising%20avenue%20to%20address%20these%20issues%20and%20inspires%20new%20algorithmic%0Aapproaches.%20In%20particular%2C%20Quantum%20Graph%20Neural%20Networks%20%28QGNNs%29%20have%20been%0Aexplored%20in%20recent%20literature.%20However%2C%20current%20quantum%20hardware%20limits%20the%0Adimension%20of%20the%20data%20that%20can%20be%20effectively%20encoded.%20Existing%20approaches%0Aeither%20simplify%20datasets%20manually%20or%20use%20artificial%20graph%20datasets.%20This%20work%0Aintroduces%20the%20Guided%20Graph%20Compression%20%28GGC%29%20framework%2C%20which%20uses%20a%20graph%0Aautoencoder%20to%20reduce%20both%20the%20number%20of%20nodes%20and%20the%20dimensionality%20of%20node%0Afeatures.%20The%20compression%20is%20guided%20to%20enhance%20the%20performance%20of%20a%20downstream%0Aclassification%20task%2C%20which%20can%20be%20applied%20either%20with%20a%20quantum%20or%20a%20classical%0Aclassifier.%20The%20framework%20is%20evaluated%20on%20the%20Jet%20Tagging%20task%2C%20a%0Aclassification%20problem%20of%20fundamental%20importance%20in%20high%20energy%20physics%20that%0Ainvolves%20distinguishing%20particle%20jets%20initiated%20by%20quarks%20from%20those%20by%20gluons.%0AThe%20GGC%20is%20compared%20against%20using%20the%20autoencoder%20as%20a%20standalone%20preprocessing%0Astep%20and%20against%20a%20baseline%20classical%20GNN%20classifier.%20Our%20numerical%20results%0Ademonstrate%20that%20GGC%20outperforms%20both%20alternatives%2C%20while%20also%20facilitating%20the%0Atesting%20of%20novel%20QGNN%20ansatzes%20on%20realistic%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09862v1&entry.124074799=Read"},
{"title": "TerraMind: Large-Scale Generative Multimodality for Earth Observation", "author": "Johannes Jakubik and Felix Yang and Benedikt Blumenstiel and Erik Scheurer and Rocco Sedona and Stefano Maurogiovanni and Jente Bosmans and Nikolaos Dionelis and Valerio Marsocci and Niklas Kopp and Rahul Ramachandran and Paolo Fraccaro and Thomas Brunschwiler and Gabriele Cavallaro and Juan Bernabe-Moreno and Nicolas Long\u00e9p\u00e9", "abstract": "  We present TerraMind, the first any-to-any generative, multimodal foundation\nmodel for Earth observation (EO). Unlike other multimodal models, TerraMind is\npretrained on dual-scale representations combining both token-level and\npixel-level data across modalities. On a token level, TerraMind encodes\nhigh-level contextual information to learn cross-modal relationships, while on\na pixel level, TerraMind leverages fine-grained representations to capture\ncritical spatial nuances. We pretrained TerraMind on nine geospatial modalities\nof a global, large-scale dataset. In this paper, we demonstrate that (i)\nTerraMind's dual-scale early fusion approach unlocks a range of zero-shot and\nfew-shot applications for Earth observation, (ii) TerraMind introduces\n\"Thinking-in-Modalities\" (TiM) -- the capability of generating additional\nartificial data during finetuning and inference to improve the model output --\nand (iii) TerraMind achieves beyond state-of-the-art performance in\ncommunity-standard benchmarks for EO like PANGAEA. The pretraining dataset, the\nmodel weights, and our code are open-sourced under a permissive license.\n", "link": "http://arxiv.org/abs/2504.11171v2", "date": "2025-06-11", "relevancy": 2.3079, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.589}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5717}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TerraMind%3A%20Large-Scale%20Generative%20Multimodality%20for%20Earth%20Observation&body=Title%3A%20TerraMind%3A%20Large-Scale%20Generative%20Multimodality%20for%20Earth%20Observation%0AAuthor%3A%20Johannes%20Jakubik%20and%20Felix%20Yang%20and%20Benedikt%20Blumenstiel%20and%20Erik%20Scheurer%20and%20Rocco%20Sedona%20and%20Stefano%20Maurogiovanni%20and%20Jente%20Bosmans%20and%20Nikolaos%20Dionelis%20and%20Valerio%20Marsocci%20and%20Niklas%20Kopp%20and%20Rahul%20Ramachandran%20and%20Paolo%20Fraccaro%20and%20Thomas%20Brunschwiler%20and%20Gabriele%20Cavallaro%20and%20Juan%20Bernabe-Moreno%20and%20Nicolas%20Long%C3%A9p%C3%A9%0AAbstract%3A%20%20%20We%20present%20TerraMind%2C%20the%20first%20any-to-any%20generative%2C%20multimodal%20foundation%0Amodel%20for%20Earth%20observation%20%28EO%29.%20Unlike%20other%20multimodal%20models%2C%20TerraMind%20is%0Apretrained%20on%20dual-scale%20representations%20combining%20both%20token-level%20and%0Apixel-level%20data%20across%20modalities.%20On%20a%20token%20level%2C%20TerraMind%20encodes%0Ahigh-level%20contextual%20information%20to%20learn%20cross-modal%20relationships%2C%20while%20on%0Aa%20pixel%20level%2C%20TerraMind%20leverages%20fine-grained%20representations%20to%20capture%0Acritical%20spatial%20nuances.%20We%20pretrained%20TerraMind%20on%20nine%20geospatial%20modalities%0Aof%20a%20global%2C%20large-scale%20dataset.%20In%20this%20paper%2C%20we%20demonstrate%20that%20%28i%29%0ATerraMind%27s%20dual-scale%20early%20fusion%20approach%20unlocks%20a%20range%20of%20zero-shot%20and%0Afew-shot%20applications%20for%20Earth%20observation%2C%20%28ii%29%20TerraMind%20introduces%0A%22Thinking-in-Modalities%22%20%28TiM%29%20--%20the%20capability%20of%20generating%20additional%0Aartificial%20data%20during%20finetuning%20and%20inference%20to%20improve%20the%20model%20output%20--%0Aand%20%28iii%29%20TerraMind%20achieves%20beyond%20state-of-the-art%20performance%20in%0Acommunity-standard%20benchmarks%20for%20EO%20like%20PANGAEA.%20The%20pretraining%20dataset%2C%20the%0Amodel%20weights%2C%20and%20our%20code%20are%20open-sourced%20under%20a%20permissive%20license.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11171v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTerraMind%253A%2520Large-Scale%2520Generative%2520Multimodality%2520for%2520Earth%2520Observation%26entry.906535625%3DJohannes%2520Jakubik%2520and%2520Felix%2520Yang%2520and%2520Benedikt%2520Blumenstiel%2520and%2520Erik%2520Scheurer%2520and%2520Rocco%2520Sedona%2520and%2520Stefano%2520Maurogiovanni%2520and%2520Jente%2520Bosmans%2520and%2520Nikolaos%2520Dionelis%2520and%2520Valerio%2520Marsocci%2520and%2520Niklas%2520Kopp%2520and%2520Rahul%2520Ramachandran%2520and%2520Paolo%2520Fraccaro%2520and%2520Thomas%2520Brunschwiler%2520and%2520Gabriele%2520Cavallaro%2520and%2520Juan%2520Bernabe-Moreno%2520and%2520Nicolas%2520Long%25C3%25A9p%25C3%25A9%26entry.1292438233%3D%2520%2520We%2520present%2520TerraMind%252C%2520the%2520first%2520any-to-any%2520generative%252C%2520multimodal%2520foundation%250Amodel%2520for%2520Earth%2520observation%2520%2528EO%2529.%2520Unlike%2520other%2520multimodal%2520models%252C%2520TerraMind%2520is%250Apretrained%2520on%2520dual-scale%2520representations%2520combining%2520both%2520token-level%2520and%250Apixel-level%2520data%2520across%2520modalities.%2520On%2520a%2520token%2520level%252C%2520TerraMind%2520encodes%250Ahigh-level%2520contextual%2520information%2520to%2520learn%2520cross-modal%2520relationships%252C%2520while%2520on%250Aa%2520pixel%2520level%252C%2520TerraMind%2520leverages%2520fine-grained%2520representations%2520to%2520capture%250Acritical%2520spatial%2520nuances.%2520We%2520pretrained%2520TerraMind%2520on%2520nine%2520geospatial%2520modalities%250Aof%2520a%2520global%252C%2520large-scale%2520dataset.%2520In%2520this%2520paper%252C%2520we%2520demonstrate%2520that%2520%2528i%2529%250ATerraMind%2527s%2520dual-scale%2520early%2520fusion%2520approach%2520unlocks%2520a%2520range%2520of%2520zero-shot%2520and%250Afew-shot%2520applications%2520for%2520Earth%2520observation%252C%2520%2528ii%2529%2520TerraMind%2520introduces%250A%2522Thinking-in-Modalities%2522%2520%2528TiM%2529%2520--%2520the%2520capability%2520of%2520generating%2520additional%250Aartificial%2520data%2520during%2520finetuning%2520and%2520inference%2520to%2520improve%2520the%2520model%2520output%2520--%250Aand%2520%2528iii%2529%2520TerraMind%2520achieves%2520beyond%2520state-of-the-art%2520performance%2520in%250Acommunity-standard%2520benchmarks%2520for%2520EO%2520like%2520PANGAEA.%2520The%2520pretraining%2520dataset%252C%2520the%250Amodel%2520weights%252C%2520and%2520our%2520code%2520are%2520open-sourced%2520under%2520a%2520permissive%2520license.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11171v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TerraMind%3A%20Large-Scale%20Generative%20Multimodality%20for%20Earth%20Observation&entry.906535625=Johannes%20Jakubik%20and%20Felix%20Yang%20and%20Benedikt%20Blumenstiel%20and%20Erik%20Scheurer%20and%20Rocco%20Sedona%20and%20Stefano%20Maurogiovanni%20and%20Jente%20Bosmans%20and%20Nikolaos%20Dionelis%20and%20Valerio%20Marsocci%20and%20Niklas%20Kopp%20and%20Rahul%20Ramachandran%20and%20Paolo%20Fraccaro%20and%20Thomas%20Brunschwiler%20and%20Gabriele%20Cavallaro%20and%20Juan%20Bernabe-Moreno%20and%20Nicolas%20Long%C3%A9p%C3%A9&entry.1292438233=%20%20We%20present%20TerraMind%2C%20the%20first%20any-to-any%20generative%2C%20multimodal%20foundation%0Amodel%20for%20Earth%20observation%20%28EO%29.%20Unlike%20other%20multimodal%20models%2C%20TerraMind%20is%0Apretrained%20on%20dual-scale%20representations%20combining%20both%20token-level%20and%0Apixel-level%20data%20across%20modalities.%20On%20a%20token%20level%2C%20TerraMind%20encodes%0Ahigh-level%20contextual%20information%20to%20learn%20cross-modal%20relationships%2C%20while%20on%0Aa%20pixel%20level%2C%20TerraMind%20leverages%20fine-grained%20representations%20to%20capture%0Acritical%20spatial%20nuances.%20We%20pretrained%20TerraMind%20on%20nine%20geospatial%20modalities%0Aof%20a%20global%2C%20large-scale%20dataset.%20In%20this%20paper%2C%20we%20demonstrate%20that%20%28i%29%0ATerraMind%27s%20dual-scale%20early%20fusion%20approach%20unlocks%20a%20range%20of%20zero-shot%20and%0Afew-shot%20applications%20for%20Earth%20observation%2C%20%28ii%29%20TerraMind%20introduces%0A%22Thinking-in-Modalities%22%20%28TiM%29%20--%20the%20capability%20of%20generating%20additional%0Aartificial%20data%20during%20finetuning%20and%20inference%20to%20improve%20the%20model%20output%20--%0Aand%20%28iii%29%20TerraMind%20achieves%20beyond%20state-of-the-art%20performance%20in%0Acommunity-standard%20benchmarks%20for%20EO%20like%20PANGAEA.%20The%20pretraining%20dataset%2C%20the%0Amodel%20weights%2C%20and%20our%20code%20are%20open-sourced%20under%20a%20permissive%20license.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11171v2&entry.124074799=Read"},
{"title": "IntPhys 2: Benchmarking Intuitive Physics Understanding In Complex\n  Synthetic Environments", "author": "Florian Bordes and Quentin Garrido and Justine T Kao and Adina Williams and Michael Rabbat and Emmanuel Dupoux", "abstract": "  We present IntPhys 2, a video benchmark designed to evaluate the intuitive\nphysics understanding of deep learning models. Building on the original IntPhys\nbenchmark, IntPhys 2 focuses on four core principles related to macroscopic\nobjects: Permanence, Immutability, Spatio-Temporal Continuity, and Solidity.\nThese conditions are inspired by research into intuitive physical understanding\nemerging during early childhood. IntPhys 2 offers a comprehensive suite of\ntests, based on the violation of expectation framework, that challenge models\nto differentiate between possible and impossible events within controlled and\ndiverse virtual environments. Alongside the benchmark, we provide performance\nevaluations of several state-of-the-art models. Our findings indicate that\nwhile these models demonstrate basic visual understanding, they face\nsignificant challenges in grasping intuitive physics across the four principles\nin complex scenes, with most models performing at chance levels (50%), in stark\ncontrast to human performance, which achieves near-perfect accuracy. This\nunderscores the gap between current models and human-like intuitive physics\nunderstanding, highlighting the need for advancements in model architectures\nand training methodologies.\n", "link": "http://arxiv.org/abs/2506.09849v1", "date": "2025-06-11", "relevancy": 2.2457, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5723}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5561}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IntPhys%202%3A%20Benchmarking%20Intuitive%20Physics%20Understanding%20In%20Complex%0A%20%20Synthetic%20Environments&body=Title%3A%20IntPhys%202%3A%20Benchmarking%20Intuitive%20Physics%20Understanding%20In%20Complex%0A%20%20Synthetic%20Environments%0AAuthor%3A%20Florian%20Bordes%20and%20Quentin%20Garrido%20and%20Justine%20T%20Kao%20and%20Adina%20Williams%20and%20Michael%20Rabbat%20and%20Emmanuel%20Dupoux%0AAbstract%3A%20%20%20We%20present%20IntPhys%202%2C%20a%20video%20benchmark%20designed%20to%20evaluate%20the%20intuitive%0Aphysics%20understanding%20of%20deep%20learning%20models.%20Building%20on%20the%20original%20IntPhys%0Abenchmark%2C%20IntPhys%202%20focuses%20on%20four%20core%20principles%20related%20to%20macroscopic%0Aobjects%3A%20Permanence%2C%20Immutability%2C%20Spatio-Temporal%20Continuity%2C%20and%20Solidity.%0AThese%20conditions%20are%20inspired%20by%20research%20into%20intuitive%20physical%20understanding%0Aemerging%20during%20early%20childhood.%20IntPhys%202%20offers%20a%20comprehensive%20suite%20of%0Atests%2C%20based%20on%20the%20violation%20of%20expectation%20framework%2C%20that%20challenge%20models%0Ato%20differentiate%20between%20possible%20and%20impossible%20events%20within%20controlled%20and%0Adiverse%20virtual%20environments.%20Alongside%20the%20benchmark%2C%20we%20provide%20performance%0Aevaluations%20of%20several%20state-of-the-art%20models.%20Our%20findings%20indicate%20that%0Awhile%20these%20models%20demonstrate%20basic%20visual%20understanding%2C%20they%20face%0Asignificant%20challenges%20in%20grasping%20intuitive%20physics%20across%20the%20four%20principles%0Ain%20complex%20scenes%2C%20with%20most%20models%20performing%20at%20chance%20levels%20%2850%25%29%2C%20in%20stark%0Acontrast%20to%20human%20performance%2C%20which%20achieves%20near-perfect%20accuracy.%20This%0Aunderscores%20the%20gap%20between%20current%20models%20and%20human-like%20intuitive%20physics%0Aunderstanding%2C%20highlighting%20the%20need%20for%20advancements%20in%20model%20architectures%0Aand%20training%20methodologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09849v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntPhys%25202%253A%2520Benchmarking%2520Intuitive%2520Physics%2520Understanding%2520In%2520Complex%250A%2520%2520Synthetic%2520Environments%26entry.906535625%3DFlorian%2520Bordes%2520and%2520Quentin%2520Garrido%2520and%2520Justine%2520T%2520Kao%2520and%2520Adina%2520Williams%2520and%2520Michael%2520Rabbat%2520and%2520Emmanuel%2520Dupoux%26entry.1292438233%3D%2520%2520We%2520present%2520IntPhys%25202%252C%2520a%2520video%2520benchmark%2520designed%2520to%2520evaluate%2520the%2520intuitive%250Aphysics%2520understanding%2520of%2520deep%2520learning%2520models.%2520Building%2520on%2520the%2520original%2520IntPhys%250Abenchmark%252C%2520IntPhys%25202%2520focuses%2520on%2520four%2520core%2520principles%2520related%2520to%2520macroscopic%250Aobjects%253A%2520Permanence%252C%2520Immutability%252C%2520Spatio-Temporal%2520Continuity%252C%2520and%2520Solidity.%250AThese%2520conditions%2520are%2520inspired%2520by%2520research%2520into%2520intuitive%2520physical%2520understanding%250Aemerging%2520during%2520early%2520childhood.%2520IntPhys%25202%2520offers%2520a%2520comprehensive%2520suite%2520of%250Atests%252C%2520based%2520on%2520the%2520violation%2520of%2520expectation%2520framework%252C%2520that%2520challenge%2520models%250Ato%2520differentiate%2520between%2520possible%2520and%2520impossible%2520events%2520within%2520controlled%2520and%250Adiverse%2520virtual%2520environments.%2520Alongside%2520the%2520benchmark%252C%2520we%2520provide%2520performance%250Aevaluations%2520of%2520several%2520state-of-the-art%2520models.%2520Our%2520findings%2520indicate%2520that%250Awhile%2520these%2520models%2520demonstrate%2520basic%2520visual%2520understanding%252C%2520they%2520face%250Asignificant%2520challenges%2520in%2520grasping%2520intuitive%2520physics%2520across%2520the%2520four%2520principles%250Ain%2520complex%2520scenes%252C%2520with%2520most%2520models%2520performing%2520at%2520chance%2520levels%2520%252850%2525%2529%252C%2520in%2520stark%250Acontrast%2520to%2520human%2520performance%252C%2520which%2520achieves%2520near-perfect%2520accuracy.%2520This%250Aunderscores%2520the%2520gap%2520between%2520current%2520models%2520and%2520human-like%2520intuitive%2520physics%250Aunderstanding%252C%2520highlighting%2520the%2520need%2520for%2520advancements%2520in%2520model%2520architectures%250Aand%2520training%2520methodologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09849v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IntPhys%202%3A%20Benchmarking%20Intuitive%20Physics%20Understanding%20In%20Complex%0A%20%20Synthetic%20Environments&entry.906535625=Florian%20Bordes%20and%20Quentin%20Garrido%20and%20Justine%20T%20Kao%20and%20Adina%20Williams%20and%20Michael%20Rabbat%20and%20Emmanuel%20Dupoux&entry.1292438233=%20%20We%20present%20IntPhys%202%2C%20a%20video%20benchmark%20designed%20to%20evaluate%20the%20intuitive%0Aphysics%20understanding%20of%20deep%20learning%20models.%20Building%20on%20the%20original%20IntPhys%0Abenchmark%2C%20IntPhys%202%20focuses%20on%20four%20core%20principles%20related%20to%20macroscopic%0Aobjects%3A%20Permanence%2C%20Immutability%2C%20Spatio-Temporal%20Continuity%2C%20and%20Solidity.%0AThese%20conditions%20are%20inspired%20by%20research%20into%20intuitive%20physical%20understanding%0Aemerging%20during%20early%20childhood.%20IntPhys%202%20offers%20a%20comprehensive%20suite%20of%0Atests%2C%20based%20on%20the%20violation%20of%20expectation%20framework%2C%20that%20challenge%20models%0Ato%20differentiate%20between%20possible%20and%20impossible%20events%20within%20controlled%20and%0Adiverse%20virtual%20environments.%20Alongside%20the%20benchmark%2C%20we%20provide%20performance%0Aevaluations%20of%20several%20state-of-the-art%20models.%20Our%20findings%20indicate%20that%0Awhile%20these%20models%20demonstrate%20basic%20visual%20understanding%2C%20they%20face%0Asignificant%20challenges%20in%20grasping%20intuitive%20physics%20across%20the%20four%20principles%0Ain%20complex%20scenes%2C%20with%20most%20models%20performing%20at%20chance%20levels%20%2850%25%29%2C%20in%20stark%0Acontrast%20to%20human%20performance%2C%20which%20achieves%20near-perfect%20accuracy.%20This%0Aunderscores%20the%20gap%20between%20current%20models%20and%20human-like%20intuitive%20physics%0Aunderstanding%2C%20highlighting%20the%20need%20for%20advancements%20in%20model%20architectures%0Aand%20training%20methodologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09849v1&entry.124074799=Read"},
{"title": "Aucamp: An Underwater Camera-Based Multi-Robot Platform with Low-Cost,\n  Distributed, and Robust Localization", "author": "Jisheng Xu and Ding Lin and Pangkit Fong and Chongrong Fang and Xiaoming Duan and Jianping He", "abstract": "  This paper introduces an underwater multi-robot platform, named Aucamp,\ncharacterized by cost-effective monocular-camera-based sensing, distributed\nprotocol and robust orientation control for localization. We utilize the\nclarity feature to measure the distance, present the monocular imaging model,\nand estimate the position of the target object. We achieve global positioning\nin our platform by designing a distributed update protocol. The distributed\nalgorithm enables the perception process to simultaneously cover a broader\nrange, and greatly improves the accuracy and robustness of the positioning.\nMoreover, the explicit dynamics model of the robot in our platform is obtained,\nbased on which, we propose a robust orientation control framework. The control\nsystem ensures that the platform maintains a balanced posture for each robot,\nthereby ensuring the stability of the localization system. The platform can\nswiftly recover from an forced unstable state to a stable horizontal posture.\nAdditionally, we conduct extensive experiments and application scenarios to\nevaluate the performance of our platform. The proposed new platform may provide\nsupport for extensive marine exploration by underwater sensor networks.\n", "link": "http://arxiv.org/abs/2506.09876v1", "date": "2025-06-11", "relevancy": 2.2371, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5977}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5557}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aucamp%3A%20An%20Underwater%20Camera-Based%20Multi-Robot%20Platform%20with%20Low-Cost%2C%0A%20%20Distributed%2C%20and%20Robust%20Localization&body=Title%3A%20Aucamp%3A%20An%20Underwater%20Camera-Based%20Multi-Robot%20Platform%20with%20Low-Cost%2C%0A%20%20Distributed%2C%20and%20Robust%20Localization%0AAuthor%3A%20Jisheng%20Xu%20and%20Ding%20Lin%20and%20Pangkit%20Fong%20and%20Chongrong%20Fang%20and%20Xiaoming%20Duan%20and%20Jianping%20He%0AAbstract%3A%20%20%20This%20paper%20introduces%20an%20underwater%20multi-robot%20platform%2C%20named%20Aucamp%2C%0Acharacterized%20by%20cost-effective%20monocular-camera-based%20sensing%2C%20distributed%0Aprotocol%20and%20robust%20orientation%20control%20for%20localization.%20We%20utilize%20the%0Aclarity%20feature%20to%20measure%20the%20distance%2C%20present%20the%20monocular%20imaging%20model%2C%0Aand%20estimate%20the%20position%20of%20the%20target%20object.%20We%20achieve%20global%20positioning%0Ain%20our%20platform%20by%20designing%20a%20distributed%20update%20protocol.%20The%20distributed%0Aalgorithm%20enables%20the%20perception%20process%20to%20simultaneously%20cover%20a%20broader%0Arange%2C%20and%20greatly%20improves%20the%20accuracy%20and%20robustness%20of%20the%20positioning.%0AMoreover%2C%20the%20explicit%20dynamics%20model%20of%20the%20robot%20in%20our%20platform%20is%20obtained%2C%0Abased%20on%20which%2C%20we%20propose%20a%20robust%20orientation%20control%20framework.%20The%20control%0Asystem%20ensures%20that%20the%20platform%20maintains%20a%20balanced%20posture%20for%20each%20robot%2C%0Athereby%20ensuring%20the%20stability%20of%20the%20localization%20system.%20The%20platform%20can%0Aswiftly%20recover%20from%20an%20forced%20unstable%20state%20to%20a%20stable%20horizontal%20posture.%0AAdditionally%2C%20we%20conduct%20extensive%20experiments%20and%20application%20scenarios%20to%0Aevaluate%20the%20performance%20of%20our%20platform.%20The%20proposed%20new%20platform%20may%20provide%0Asupport%20for%20extensive%20marine%20exploration%20by%20underwater%20sensor%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAucamp%253A%2520An%2520Underwater%2520Camera-Based%2520Multi-Robot%2520Platform%2520with%2520Low-Cost%252C%250A%2520%2520Distributed%252C%2520and%2520Robust%2520Localization%26entry.906535625%3DJisheng%2520Xu%2520and%2520Ding%2520Lin%2520and%2520Pangkit%2520Fong%2520and%2520Chongrong%2520Fang%2520and%2520Xiaoming%2520Duan%2520and%2520Jianping%2520He%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520an%2520underwater%2520multi-robot%2520platform%252C%2520named%2520Aucamp%252C%250Acharacterized%2520by%2520cost-effective%2520monocular-camera-based%2520sensing%252C%2520distributed%250Aprotocol%2520and%2520robust%2520orientation%2520control%2520for%2520localization.%2520We%2520utilize%2520the%250Aclarity%2520feature%2520to%2520measure%2520the%2520distance%252C%2520present%2520the%2520monocular%2520imaging%2520model%252C%250Aand%2520estimate%2520the%2520position%2520of%2520the%2520target%2520object.%2520We%2520achieve%2520global%2520positioning%250Ain%2520our%2520platform%2520by%2520designing%2520a%2520distributed%2520update%2520protocol.%2520The%2520distributed%250Aalgorithm%2520enables%2520the%2520perception%2520process%2520to%2520simultaneously%2520cover%2520a%2520broader%250Arange%252C%2520and%2520greatly%2520improves%2520the%2520accuracy%2520and%2520robustness%2520of%2520the%2520positioning.%250AMoreover%252C%2520the%2520explicit%2520dynamics%2520model%2520of%2520the%2520robot%2520in%2520our%2520platform%2520is%2520obtained%252C%250Abased%2520on%2520which%252C%2520we%2520propose%2520a%2520robust%2520orientation%2520control%2520framework.%2520The%2520control%250Asystem%2520ensures%2520that%2520the%2520platform%2520maintains%2520a%2520balanced%2520posture%2520for%2520each%2520robot%252C%250Athereby%2520ensuring%2520the%2520stability%2520of%2520the%2520localization%2520system.%2520The%2520platform%2520can%250Aswiftly%2520recover%2520from%2520an%2520forced%2520unstable%2520state%2520to%2520a%2520stable%2520horizontal%2520posture.%250AAdditionally%252C%2520we%2520conduct%2520extensive%2520experiments%2520and%2520application%2520scenarios%2520to%250Aevaluate%2520the%2520performance%2520of%2520our%2520platform.%2520The%2520proposed%2520new%2520platform%2520may%2520provide%250Asupport%2520for%2520extensive%2520marine%2520exploration%2520by%2520underwater%2520sensor%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aucamp%3A%20An%20Underwater%20Camera-Based%20Multi-Robot%20Platform%20with%20Low-Cost%2C%0A%20%20Distributed%2C%20and%20Robust%20Localization&entry.906535625=Jisheng%20Xu%20and%20Ding%20Lin%20and%20Pangkit%20Fong%20and%20Chongrong%20Fang%20and%20Xiaoming%20Duan%20and%20Jianping%20He&entry.1292438233=%20%20This%20paper%20introduces%20an%20underwater%20multi-robot%20platform%2C%20named%20Aucamp%2C%0Acharacterized%20by%20cost-effective%20monocular-camera-based%20sensing%2C%20distributed%0Aprotocol%20and%20robust%20orientation%20control%20for%20localization.%20We%20utilize%20the%0Aclarity%20feature%20to%20measure%20the%20distance%2C%20present%20the%20monocular%20imaging%20model%2C%0Aand%20estimate%20the%20position%20of%20the%20target%20object.%20We%20achieve%20global%20positioning%0Ain%20our%20platform%20by%20designing%20a%20distributed%20update%20protocol.%20The%20distributed%0Aalgorithm%20enables%20the%20perception%20process%20to%20simultaneously%20cover%20a%20broader%0Arange%2C%20and%20greatly%20improves%20the%20accuracy%20and%20robustness%20of%20the%20positioning.%0AMoreover%2C%20the%20explicit%20dynamics%20model%20of%20the%20robot%20in%20our%20platform%20is%20obtained%2C%0Abased%20on%20which%2C%20we%20propose%20a%20robust%20orientation%20control%20framework.%20The%20control%0Asystem%20ensures%20that%20the%20platform%20maintains%20a%20balanced%20posture%20for%20each%20robot%2C%0Athereby%20ensuring%20the%20stability%20of%20the%20localization%20system.%20The%20platform%20can%0Aswiftly%20recover%20from%20an%20forced%20unstable%20state%20to%20a%20stable%20horizontal%20posture.%0AAdditionally%2C%20we%20conduct%20extensive%20experiments%20and%20application%20scenarios%20to%0Aevaluate%20the%20performance%20of%20our%20platform.%20The%20proposed%20new%20platform%20may%20provide%0Asupport%20for%20extensive%20marine%20exploration%20by%20underwater%20sensor%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09876v1&entry.124074799=Read"},
{"title": "Network Dynamics-Based Framework for Understanding Deep Neural Networks", "author": "Yuchen Lin and Yong Zhang and Sihan Feng and Hong Zhao", "abstract": "  Advancements in artificial intelligence call for a deeper understanding of\nthe fundamental mechanisms underlying deep learning. In this work, we propose a\ntheoretical framework to analyze learning dynamics through the lens of\ndynamical systems theory. We redefine the notions of linearity and nonlinearity\nin neural networks by introducing two fundamental transformation units at the\nneuron level: order-preserving transformations and non-order-preserving\ntransformations. Different transformation modes lead to distinct collective\nbehaviors in weight vector organization, different modes of information\nextraction, and the emergence of qualitatively different learning phases.\nTransitions between these phases may occur during training, accounting for key\nphenomena such as grokking. To further characterize generalization and\nstructural stability, we introduce the concept of attraction basins in both\nsample and weight spaces. The distribution of neurons with different\ntransformation modes across layers, along with the structural characteristics\nof the two types of attraction basins, forms a set of core metrics for\nanalyzing the performance of learning models. Hyperparameters such as depth,\nwidth, learning rate, and batch size act as control variables for fine-tuning\nthese metrics. Our framework not only sheds light on the intrinsic advantages\nof deep learning, but also provides a novel perspective for optimizing network\narchitectures and training strategies.\n", "link": "http://arxiv.org/abs/2501.02436v3", "date": "2025-06-11", "relevancy": 2.2111, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6014}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5394}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Network%20Dynamics-Based%20Framework%20for%20Understanding%20Deep%20Neural%20Networks&body=Title%3A%20Network%20Dynamics-Based%20Framework%20for%20Understanding%20Deep%20Neural%20Networks%0AAuthor%3A%20Yuchen%20Lin%20and%20Yong%20Zhang%20and%20Sihan%20Feng%20and%20Hong%20Zhao%0AAbstract%3A%20%20%20Advancements%20in%20artificial%20intelligence%20call%20for%20a%20deeper%20understanding%20of%0Athe%20fundamental%20mechanisms%20underlying%20deep%20learning.%20In%20this%20work%2C%20we%20propose%20a%0Atheoretical%20framework%20to%20analyze%20learning%20dynamics%20through%20the%20lens%20of%0Adynamical%20systems%20theory.%20We%20redefine%20the%20notions%20of%20linearity%20and%20nonlinearity%0Ain%20neural%20networks%20by%20introducing%20two%20fundamental%20transformation%20units%20at%20the%0Aneuron%20level%3A%20order-preserving%20transformations%20and%20non-order-preserving%0Atransformations.%20Different%20transformation%20modes%20lead%20to%20distinct%20collective%0Abehaviors%20in%20weight%20vector%20organization%2C%20different%20modes%20of%20information%0Aextraction%2C%20and%20the%20emergence%20of%20qualitatively%20different%20learning%20phases.%0ATransitions%20between%20these%20phases%20may%20occur%20during%20training%2C%20accounting%20for%20key%0Aphenomena%20such%20as%20grokking.%20To%20further%20characterize%20generalization%20and%0Astructural%20stability%2C%20we%20introduce%20the%20concept%20of%20attraction%20basins%20in%20both%0Asample%20and%20weight%20spaces.%20The%20distribution%20of%20neurons%20with%20different%0Atransformation%20modes%20across%20layers%2C%20along%20with%20the%20structural%20characteristics%0Aof%20the%20two%20types%20of%20attraction%20basins%2C%20forms%20a%20set%20of%20core%20metrics%20for%0Aanalyzing%20the%20performance%20of%20learning%20models.%20Hyperparameters%20such%20as%20depth%2C%0Awidth%2C%20learning%20rate%2C%20and%20batch%20size%20act%20as%20control%20variables%20for%20fine-tuning%0Athese%20metrics.%20Our%20framework%20not%20only%20sheds%20light%20on%20the%20intrinsic%20advantages%0Aof%20deep%20learning%2C%20but%20also%20provides%20a%20novel%20perspective%20for%20optimizing%20network%0Aarchitectures%20and%20training%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02436v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNetwork%2520Dynamics-Based%2520Framework%2520for%2520Understanding%2520Deep%2520Neural%2520Networks%26entry.906535625%3DYuchen%2520Lin%2520and%2520Yong%2520Zhang%2520and%2520Sihan%2520Feng%2520and%2520Hong%2520Zhao%26entry.1292438233%3D%2520%2520Advancements%2520in%2520artificial%2520intelligence%2520call%2520for%2520a%2520deeper%2520understanding%2520of%250Athe%2520fundamental%2520mechanisms%2520underlying%2520deep%2520learning.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Atheoretical%2520framework%2520to%2520analyze%2520learning%2520dynamics%2520through%2520the%2520lens%2520of%250Adynamical%2520systems%2520theory.%2520We%2520redefine%2520the%2520notions%2520of%2520linearity%2520and%2520nonlinearity%250Ain%2520neural%2520networks%2520by%2520introducing%2520two%2520fundamental%2520transformation%2520units%2520at%2520the%250Aneuron%2520level%253A%2520order-preserving%2520transformations%2520and%2520non-order-preserving%250Atransformations.%2520Different%2520transformation%2520modes%2520lead%2520to%2520distinct%2520collective%250Abehaviors%2520in%2520weight%2520vector%2520organization%252C%2520different%2520modes%2520of%2520information%250Aextraction%252C%2520and%2520the%2520emergence%2520of%2520qualitatively%2520different%2520learning%2520phases.%250ATransitions%2520between%2520these%2520phases%2520may%2520occur%2520during%2520training%252C%2520accounting%2520for%2520key%250Aphenomena%2520such%2520as%2520grokking.%2520To%2520further%2520characterize%2520generalization%2520and%250Astructural%2520stability%252C%2520we%2520introduce%2520the%2520concept%2520of%2520attraction%2520basins%2520in%2520both%250Asample%2520and%2520weight%2520spaces.%2520The%2520distribution%2520of%2520neurons%2520with%2520different%250Atransformation%2520modes%2520across%2520layers%252C%2520along%2520with%2520the%2520structural%2520characteristics%250Aof%2520the%2520two%2520types%2520of%2520attraction%2520basins%252C%2520forms%2520a%2520set%2520of%2520core%2520metrics%2520for%250Aanalyzing%2520the%2520performance%2520of%2520learning%2520models.%2520Hyperparameters%2520such%2520as%2520depth%252C%250Awidth%252C%2520learning%2520rate%252C%2520and%2520batch%2520size%2520act%2520as%2520control%2520variables%2520for%2520fine-tuning%250Athese%2520metrics.%2520Our%2520framework%2520not%2520only%2520sheds%2520light%2520on%2520the%2520intrinsic%2520advantages%250Aof%2520deep%2520learning%252C%2520but%2520also%2520provides%2520a%2520novel%2520perspective%2520for%2520optimizing%2520network%250Aarchitectures%2520and%2520training%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02436v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Network%20Dynamics-Based%20Framework%20for%20Understanding%20Deep%20Neural%20Networks&entry.906535625=Yuchen%20Lin%20and%20Yong%20Zhang%20and%20Sihan%20Feng%20and%20Hong%20Zhao&entry.1292438233=%20%20Advancements%20in%20artificial%20intelligence%20call%20for%20a%20deeper%20understanding%20of%0Athe%20fundamental%20mechanisms%20underlying%20deep%20learning.%20In%20this%20work%2C%20we%20propose%20a%0Atheoretical%20framework%20to%20analyze%20learning%20dynamics%20through%20the%20lens%20of%0Adynamical%20systems%20theory.%20We%20redefine%20the%20notions%20of%20linearity%20and%20nonlinearity%0Ain%20neural%20networks%20by%20introducing%20two%20fundamental%20transformation%20units%20at%20the%0Aneuron%20level%3A%20order-preserving%20transformations%20and%20non-order-preserving%0Atransformations.%20Different%20transformation%20modes%20lead%20to%20distinct%20collective%0Abehaviors%20in%20weight%20vector%20organization%2C%20different%20modes%20of%20information%0Aextraction%2C%20and%20the%20emergence%20of%20qualitatively%20different%20learning%20phases.%0ATransitions%20between%20these%20phases%20may%20occur%20during%20training%2C%20accounting%20for%20key%0Aphenomena%20such%20as%20grokking.%20To%20further%20characterize%20generalization%20and%0Astructural%20stability%2C%20we%20introduce%20the%20concept%20of%20attraction%20basins%20in%20both%0Asample%20and%20weight%20spaces.%20The%20distribution%20of%20neurons%20with%20different%0Atransformation%20modes%20across%20layers%2C%20along%20with%20the%20structural%20characteristics%0Aof%20the%20two%20types%20of%20attraction%20basins%2C%20forms%20a%20set%20of%20core%20metrics%20for%0Aanalyzing%20the%20performance%20of%20learning%20models.%20Hyperparameters%20such%20as%20depth%2C%0Awidth%2C%20learning%20rate%2C%20and%20batch%20size%20act%20as%20control%20variables%20for%20fine-tuning%0Athese%20metrics.%20Our%20framework%20not%20only%20sheds%20light%20on%20the%20intrinsic%20advantages%0Aof%20deep%20learning%2C%20but%20also%20provides%20a%20novel%20perspective%20for%20optimizing%20network%0Aarchitectures%20and%20training%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02436v3&entry.124074799=Read"},
{"title": "Video2BEV: Transforming Drone Videos to BEVs for Video-based\n  Geo-localization", "author": "Hao Ju and Shaofei Huang and Si Liu and Zhedong Zheng", "abstract": "  Existing approaches to drone visual geo-localization predominantly adopt the\nimage-based setting, where a single drone-view snapshot is matched with images\nfrom other platforms. Such task formulation, however, underutilizes the\ninherent video output of the drone and is sensitive to occlusions and viewpoint\ndisparity. To address these limitations, we formulate a new video-based drone\ngeo-localization task and propose the Video2BEV paradigm. This paradigm\ntransforms the video into a Bird's Eye View (BEV), simplifying the subsequent\n\\textbf{inter-platform} matching process. In particular, we employ Gaussian\nSplatting to reconstruct a 3D scene and obtain the BEV projection. Different\nfrom the existing transform methods, \\eg, polar transform, our BEVs preserve\nmore fine-grained details without significant distortion. To facilitate the\ndiscriminative \\textbf{intra-platform} representation learning, our Video2BEV\nparadigm also incorporates a diffusion-based module for generating hard\nnegative samples. To validate our approach, we introduce UniV, a new\nvideo-based geo-localization dataset that extends the image-based\nUniversity-1652 dataset. UniV features flight paths at $30^\\circ$ and\n$45^\\circ$ elevation angles with increased frame rates of up to 10 frames per\nsecond (FPS). Extensive experiments on the UniV dataset show that our Video2BEV\nparadigm achieves competitive recall rates and outperforms conventional\nvideo-based methods. Compared to other competitive methods, our proposed\napproach exhibits robustness at lower elevations with more occlusions.\n", "link": "http://arxiv.org/abs/2411.13610v3", "date": "2025-06-11", "relevancy": 2.202, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5898}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5576}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video2BEV%3A%20Transforming%20Drone%20Videos%20to%20BEVs%20for%20Video-based%0A%20%20Geo-localization&body=Title%3A%20Video2BEV%3A%20Transforming%20Drone%20Videos%20to%20BEVs%20for%20Video-based%0A%20%20Geo-localization%0AAuthor%3A%20Hao%20Ju%20and%20Shaofei%20Huang%20and%20Si%20Liu%20and%20Zhedong%20Zheng%0AAbstract%3A%20%20%20Existing%20approaches%20to%20drone%20visual%20geo-localization%20predominantly%20adopt%20the%0Aimage-based%20setting%2C%20where%20a%20single%20drone-view%20snapshot%20is%20matched%20with%20images%0Afrom%20other%20platforms.%20Such%20task%20formulation%2C%20however%2C%20underutilizes%20the%0Ainherent%20video%20output%20of%20the%20drone%20and%20is%20sensitive%20to%20occlusions%20and%20viewpoint%0Adisparity.%20To%20address%20these%20limitations%2C%20we%20formulate%20a%20new%20video-based%20drone%0Ageo-localization%20task%20and%20propose%20the%20Video2BEV%20paradigm.%20This%20paradigm%0Atransforms%20the%20video%20into%20a%20Bird%27s%20Eye%20View%20%28BEV%29%2C%20simplifying%20the%20subsequent%0A%5Ctextbf%7Binter-platform%7D%20matching%20process.%20In%20particular%2C%20we%20employ%20Gaussian%0ASplatting%20to%20reconstruct%20a%203D%20scene%20and%20obtain%20the%20BEV%20projection.%20Different%0Afrom%20the%20existing%20transform%20methods%2C%20%5Ceg%2C%20polar%20transform%2C%20our%20BEVs%20preserve%0Amore%20fine-grained%20details%20without%20significant%20distortion.%20To%20facilitate%20the%0Adiscriminative%20%5Ctextbf%7Bintra-platform%7D%20representation%20learning%2C%20our%20Video2BEV%0Aparadigm%20also%20incorporates%20a%20diffusion-based%20module%20for%20generating%20hard%0Anegative%20samples.%20To%20validate%20our%20approach%2C%20we%20introduce%20UniV%2C%20a%20new%0Avideo-based%20geo-localization%20dataset%20that%20extends%20the%20image-based%0AUniversity-1652%20dataset.%20UniV%20features%20flight%20paths%20at%20%2430%5E%5Ccirc%24%20and%0A%2445%5E%5Ccirc%24%20elevation%20angles%20with%20increased%20frame%20rates%20of%20up%20to%2010%20frames%20per%0Asecond%20%28FPS%29.%20Extensive%20experiments%20on%20the%20UniV%20dataset%20show%20that%20our%20Video2BEV%0Aparadigm%20achieves%20competitive%20recall%20rates%20and%20outperforms%20conventional%0Avideo-based%20methods.%20Compared%20to%20other%20competitive%20methods%2C%20our%20proposed%0Aapproach%20exhibits%20robustness%20at%20lower%20elevations%20with%20more%20occlusions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13610v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo2BEV%253A%2520Transforming%2520Drone%2520Videos%2520to%2520BEVs%2520for%2520Video-based%250A%2520%2520Geo-localization%26entry.906535625%3DHao%2520Ju%2520and%2520Shaofei%2520Huang%2520and%2520Si%2520Liu%2520and%2520Zhedong%2520Zheng%26entry.1292438233%3D%2520%2520Existing%2520approaches%2520to%2520drone%2520visual%2520geo-localization%2520predominantly%2520adopt%2520the%250Aimage-based%2520setting%252C%2520where%2520a%2520single%2520drone-view%2520snapshot%2520is%2520matched%2520with%2520images%250Afrom%2520other%2520platforms.%2520Such%2520task%2520formulation%252C%2520however%252C%2520underutilizes%2520the%250Ainherent%2520video%2520output%2520of%2520the%2520drone%2520and%2520is%2520sensitive%2520to%2520occlusions%2520and%2520viewpoint%250Adisparity.%2520To%2520address%2520these%2520limitations%252C%2520we%2520formulate%2520a%2520new%2520video-based%2520drone%250Ageo-localization%2520task%2520and%2520propose%2520the%2520Video2BEV%2520paradigm.%2520This%2520paradigm%250Atransforms%2520the%2520video%2520into%2520a%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%252C%2520simplifying%2520the%2520subsequent%250A%255Ctextbf%257Binter-platform%257D%2520matching%2520process.%2520In%2520particular%252C%2520we%2520employ%2520Gaussian%250ASplatting%2520to%2520reconstruct%2520a%25203D%2520scene%2520and%2520obtain%2520the%2520BEV%2520projection.%2520Different%250Afrom%2520the%2520existing%2520transform%2520methods%252C%2520%255Ceg%252C%2520polar%2520transform%252C%2520our%2520BEVs%2520preserve%250Amore%2520fine-grained%2520details%2520without%2520significant%2520distortion.%2520To%2520facilitate%2520the%250Adiscriminative%2520%255Ctextbf%257Bintra-platform%257D%2520representation%2520learning%252C%2520our%2520Video2BEV%250Aparadigm%2520also%2520incorporates%2520a%2520diffusion-based%2520module%2520for%2520generating%2520hard%250Anegative%2520samples.%2520To%2520validate%2520our%2520approach%252C%2520we%2520introduce%2520UniV%252C%2520a%2520new%250Avideo-based%2520geo-localization%2520dataset%2520that%2520extends%2520the%2520image-based%250AUniversity-1652%2520dataset.%2520UniV%2520features%2520flight%2520paths%2520at%2520%252430%255E%255Ccirc%2524%2520and%250A%252445%255E%255Ccirc%2524%2520elevation%2520angles%2520with%2520increased%2520frame%2520rates%2520of%2520up%2520to%252010%2520frames%2520per%250Asecond%2520%2528FPS%2529.%2520Extensive%2520experiments%2520on%2520the%2520UniV%2520dataset%2520show%2520that%2520our%2520Video2BEV%250Aparadigm%2520achieves%2520competitive%2520recall%2520rates%2520and%2520outperforms%2520conventional%250Avideo-based%2520methods.%2520Compared%2520to%2520other%2520competitive%2520methods%252C%2520our%2520proposed%250Aapproach%2520exhibits%2520robustness%2520at%2520lower%2520elevations%2520with%2520more%2520occlusions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13610v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video2BEV%3A%20Transforming%20Drone%20Videos%20to%20BEVs%20for%20Video-based%0A%20%20Geo-localization&entry.906535625=Hao%20Ju%20and%20Shaofei%20Huang%20and%20Si%20Liu%20and%20Zhedong%20Zheng&entry.1292438233=%20%20Existing%20approaches%20to%20drone%20visual%20geo-localization%20predominantly%20adopt%20the%0Aimage-based%20setting%2C%20where%20a%20single%20drone-view%20snapshot%20is%20matched%20with%20images%0Afrom%20other%20platforms.%20Such%20task%20formulation%2C%20however%2C%20underutilizes%20the%0Ainherent%20video%20output%20of%20the%20drone%20and%20is%20sensitive%20to%20occlusions%20and%20viewpoint%0Adisparity.%20To%20address%20these%20limitations%2C%20we%20formulate%20a%20new%20video-based%20drone%0Ageo-localization%20task%20and%20propose%20the%20Video2BEV%20paradigm.%20This%20paradigm%0Atransforms%20the%20video%20into%20a%20Bird%27s%20Eye%20View%20%28BEV%29%2C%20simplifying%20the%20subsequent%0A%5Ctextbf%7Binter-platform%7D%20matching%20process.%20In%20particular%2C%20we%20employ%20Gaussian%0ASplatting%20to%20reconstruct%20a%203D%20scene%20and%20obtain%20the%20BEV%20projection.%20Different%0Afrom%20the%20existing%20transform%20methods%2C%20%5Ceg%2C%20polar%20transform%2C%20our%20BEVs%20preserve%0Amore%20fine-grained%20details%20without%20significant%20distortion.%20To%20facilitate%20the%0Adiscriminative%20%5Ctextbf%7Bintra-platform%7D%20representation%20learning%2C%20our%20Video2BEV%0Aparadigm%20also%20incorporates%20a%20diffusion-based%20module%20for%20generating%20hard%0Anegative%20samples.%20To%20validate%20our%20approach%2C%20we%20introduce%20UniV%2C%20a%20new%0Avideo-based%20geo-localization%20dataset%20that%20extends%20the%20image-based%0AUniversity-1652%20dataset.%20UniV%20features%20flight%20paths%20at%20%2430%5E%5Ccirc%24%20and%0A%2445%5E%5Ccirc%24%20elevation%20angles%20with%20increased%20frame%20rates%20of%20up%20to%2010%20frames%20per%0Asecond%20%28FPS%29.%20Extensive%20experiments%20on%20the%20UniV%20dataset%20show%20that%20our%20Video2BEV%0Aparadigm%20achieves%20competitive%20recall%20rates%20and%20outperforms%20conventional%0Avideo-based%20methods.%20Compared%20to%20other%20competitive%20methods%2C%20our%20proposed%0Aapproach%20exhibits%20robustness%20at%20lower%20elevations%20with%20more%20occlusions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13610v3&entry.124074799=Read"},
{"title": "Unseen Visual Anomaly Generation", "author": "Han Sun and Yunkang Cao and Hao Dong and Olga Fink", "abstract": "  Visual anomaly detection (AD) presents significant challenges due to the\nscarcity of anomalous data samples. While numerous works have been proposed to\nsynthesize anomalous samples, these synthetic anomalies often lack authenticity\nor require extensive training data, limiting their applicability in real-world\nscenarios. In this work, we propose Anomaly Anything (AnomalyAny), a novel\nframework that leverages Stable Diffusion (SD)'s image generation capabilities\nto generate diverse and realistic unseen anomalies. By conditioning on a single\nnormal sample during test time, AnomalyAny is able to generate unseen anomalies\nfor arbitrary object types with text descriptions. Within AnomalyAny, we\npropose attention-guided anomaly optimization to direct SD attention on\ngenerating hard anomaly concepts. Additionally, we introduce prompt-guided\nanomaly refinement, incorporating detailed descriptions to further improve the\ngeneration quality. Extensive experiments on MVTec AD and VisA datasets\ndemonstrate AnomalyAny's ability in generating high-quality unseen anomalies\nand its effectiveness in enhancing downstream AD performance.\n", "link": "http://arxiv.org/abs/2406.01078v4", "date": "2025-06-11", "relevancy": 2.1915, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5567}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5437}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unseen%20Visual%20Anomaly%20Generation&body=Title%3A%20Unseen%20Visual%20Anomaly%20Generation%0AAuthor%3A%20Han%20Sun%20and%20Yunkang%20Cao%20and%20Hao%20Dong%20and%20Olga%20Fink%0AAbstract%3A%20%20%20Visual%20anomaly%20detection%20%28AD%29%20presents%20significant%20challenges%20due%20to%20the%0Ascarcity%20of%20anomalous%20data%20samples.%20While%20numerous%20works%20have%20been%20proposed%20to%0Asynthesize%20anomalous%20samples%2C%20these%20synthetic%20anomalies%20often%20lack%20authenticity%0Aor%20require%20extensive%20training%20data%2C%20limiting%20their%20applicability%20in%20real-world%0Ascenarios.%20In%20this%20work%2C%20we%20propose%20Anomaly%20Anything%20%28AnomalyAny%29%2C%20a%20novel%0Aframework%20that%20leverages%20Stable%20Diffusion%20%28SD%29%27s%20image%20generation%20capabilities%0Ato%20generate%20diverse%20and%20realistic%20unseen%20anomalies.%20By%20conditioning%20on%20a%20single%0Anormal%20sample%20during%20test%20time%2C%20AnomalyAny%20is%20able%20to%20generate%20unseen%20anomalies%0Afor%20arbitrary%20object%20types%20with%20text%20descriptions.%20Within%20AnomalyAny%2C%20we%0Apropose%20attention-guided%20anomaly%20optimization%20to%20direct%20SD%20attention%20on%0Agenerating%20hard%20anomaly%20concepts.%20Additionally%2C%20we%20introduce%20prompt-guided%0Aanomaly%20refinement%2C%20incorporating%20detailed%20descriptions%20to%20further%20improve%20the%0Ageneration%20quality.%20Extensive%20experiments%20on%20MVTec%20AD%20and%20VisA%20datasets%0Ademonstrate%20AnomalyAny%27s%20ability%20in%20generating%20high-quality%20unseen%20anomalies%0Aand%20its%20effectiveness%20in%20enhancing%20downstream%20AD%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01078v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnseen%2520Visual%2520Anomaly%2520Generation%26entry.906535625%3DHan%2520Sun%2520and%2520Yunkang%2520Cao%2520and%2520Hao%2520Dong%2520and%2520Olga%2520Fink%26entry.1292438233%3D%2520%2520Visual%2520anomaly%2520detection%2520%2528AD%2529%2520presents%2520significant%2520challenges%2520due%2520to%2520the%250Ascarcity%2520of%2520anomalous%2520data%2520samples.%2520While%2520numerous%2520works%2520have%2520been%2520proposed%2520to%250Asynthesize%2520anomalous%2520samples%252C%2520these%2520synthetic%2520anomalies%2520often%2520lack%2520authenticity%250Aor%2520require%2520extensive%2520training%2520data%252C%2520limiting%2520their%2520applicability%2520in%2520real-world%250Ascenarios.%2520In%2520this%2520work%252C%2520we%2520propose%2520Anomaly%2520Anything%2520%2528AnomalyAny%2529%252C%2520a%2520novel%250Aframework%2520that%2520leverages%2520Stable%2520Diffusion%2520%2528SD%2529%2527s%2520image%2520generation%2520capabilities%250Ato%2520generate%2520diverse%2520and%2520realistic%2520unseen%2520anomalies.%2520By%2520conditioning%2520on%2520a%2520single%250Anormal%2520sample%2520during%2520test%2520time%252C%2520AnomalyAny%2520is%2520able%2520to%2520generate%2520unseen%2520anomalies%250Afor%2520arbitrary%2520object%2520types%2520with%2520text%2520descriptions.%2520Within%2520AnomalyAny%252C%2520we%250Apropose%2520attention-guided%2520anomaly%2520optimization%2520to%2520direct%2520SD%2520attention%2520on%250Agenerating%2520hard%2520anomaly%2520concepts.%2520Additionally%252C%2520we%2520introduce%2520prompt-guided%250Aanomaly%2520refinement%252C%2520incorporating%2520detailed%2520descriptions%2520to%2520further%2520improve%2520the%250Ageneration%2520quality.%2520Extensive%2520experiments%2520on%2520MVTec%2520AD%2520and%2520VisA%2520datasets%250Ademonstrate%2520AnomalyAny%2527s%2520ability%2520in%2520generating%2520high-quality%2520unseen%2520anomalies%250Aand%2520its%2520effectiveness%2520in%2520enhancing%2520downstream%2520AD%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01078v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unseen%20Visual%20Anomaly%20Generation&entry.906535625=Han%20Sun%20and%20Yunkang%20Cao%20and%20Hao%20Dong%20and%20Olga%20Fink&entry.1292438233=%20%20Visual%20anomaly%20detection%20%28AD%29%20presents%20significant%20challenges%20due%20to%20the%0Ascarcity%20of%20anomalous%20data%20samples.%20While%20numerous%20works%20have%20been%20proposed%20to%0Asynthesize%20anomalous%20samples%2C%20these%20synthetic%20anomalies%20often%20lack%20authenticity%0Aor%20require%20extensive%20training%20data%2C%20limiting%20their%20applicability%20in%20real-world%0Ascenarios.%20In%20this%20work%2C%20we%20propose%20Anomaly%20Anything%20%28AnomalyAny%29%2C%20a%20novel%0Aframework%20that%20leverages%20Stable%20Diffusion%20%28SD%29%27s%20image%20generation%20capabilities%0Ato%20generate%20diverse%20and%20realistic%20unseen%20anomalies.%20By%20conditioning%20on%20a%20single%0Anormal%20sample%20during%20test%20time%2C%20AnomalyAny%20is%20able%20to%20generate%20unseen%20anomalies%0Afor%20arbitrary%20object%20types%20with%20text%20descriptions.%20Within%20AnomalyAny%2C%20we%0Apropose%20attention-guided%20anomaly%20optimization%20to%20direct%20SD%20attention%20on%0Agenerating%20hard%20anomaly%20concepts.%20Additionally%2C%20we%20introduce%20prompt-guided%0Aanomaly%20refinement%2C%20incorporating%20detailed%20descriptions%20to%20further%20improve%20the%0Ageneration%20quality.%20Extensive%20experiments%20on%20MVTec%20AD%20and%20VisA%20datasets%0Ademonstrate%20AnomalyAny%27s%20ability%20in%20generating%20high-quality%20unseen%20anomalies%0Aand%20its%20effectiveness%20in%20enhancing%20downstream%20AD%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01078v4&entry.124074799=Read"},
{"title": "EVINET: Towards Open-World Graph Learning via Evidential Reasoning\n  Network", "author": "Weijie Guan and Haohui Wang and Jian Kang and Lihui Liu and Dawei Zhou", "abstract": "  Graph learning has been crucial to many real-world tasks, but they are often\nstudied with a closed-world assumption, with all possible labels of data known\na priori. To enable effective graph learning in an open and noisy environment,\nit is critical to inform the model users when the model makes a wrong\nprediction to in-distribution data of a known class, i.e., misclassification\ndetection or when the model encounters out-of-distribution from novel classes,\ni.e., out-of-distribution detection. This paper introduces Evidential Reasoning\nNetwork (EVINET), a framework that addresses these two challenges by\nintegrating Beta embedding within a subjective logic framework. EVINET includes\ntwo key modules: Dissonance Reasoning for misclassification detection and\nVacuity Reasoning for out-of-distribution detection. Extensive experiments\ndemonstrate that EVINET outperforms state-of-the-art methods across multiple\nmetrics in the tasks of in-distribution classification, misclassification\ndetection, and out-of-distribution detection. EVINET demonstrates the necessity\nof uncertainty estimation and logical reasoning for misclassification detection\nand out-of-distribution detection and paves the way for open-world graph\nlearning. Our code and data are available at https://github.com/SSSKJ/EviNET.\n", "link": "http://arxiv.org/abs/2506.07288v2", "date": "2025-06-11", "relevancy": 2.1784, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5809}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5366}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EVINET%3A%20Towards%20Open-World%20Graph%20Learning%20via%20Evidential%20Reasoning%0A%20%20Network&body=Title%3A%20EVINET%3A%20Towards%20Open-World%20Graph%20Learning%20via%20Evidential%20Reasoning%0A%20%20Network%0AAuthor%3A%20Weijie%20Guan%20and%20Haohui%20Wang%20and%20Jian%20Kang%20and%20Lihui%20Liu%20and%20Dawei%20Zhou%0AAbstract%3A%20%20%20Graph%20learning%20has%20been%20crucial%20to%20many%20real-world%20tasks%2C%20but%20they%20are%20often%0Astudied%20with%20a%20closed-world%20assumption%2C%20with%20all%20possible%20labels%20of%20data%20known%0Aa%20priori.%20To%20enable%20effective%20graph%20learning%20in%20an%20open%20and%20noisy%20environment%2C%0Ait%20is%20critical%20to%20inform%20the%20model%20users%20when%20the%20model%20makes%20a%20wrong%0Aprediction%20to%20in-distribution%20data%20of%20a%20known%20class%2C%20i.e.%2C%20misclassification%0Adetection%20or%20when%20the%20model%20encounters%20out-of-distribution%20from%20novel%20classes%2C%0Ai.e.%2C%20out-of-distribution%20detection.%20This%20paper%20introduces%20Evidential%20Reasoning%0ANetwork%20%28EVINET%29%2C%20a%20framework%20that%20addresses%20these%20two%20challenges%20by%0Aintegrating%20Beta%20embedding%20within%20a%20subjective%20logic%20framework.%20EVINET%20includes%0Atwo%20key%20modules%3A%20Dissonance%20Reasoning%20for%20misclassification%20detection%20and%0AVacuity%20Reasoning%20for%20out-of-distribution%20detection.%20Extensive%20experiments%0Ademonstrate%20that%20EVINET%20outperforms%20state-of-the-art%20methods%20across%20multiple%0Ametrics%20in%20the%20tasks%20of%20in-distribution%20classification%2C%20misclassification%0Adetection%2C%20and%20out-of-distribution%20detection.%20EVINET%20demonstrates%20the%20necessity%0Aof%20uncertainty%20estimation%20and%20logical%20reasoning%20for%20misclassification%20detection%0Aand%20out-of-distribution%20detection%20and%20paves%20the%20way%20for%20open-world%20graph%0Alearning.%20Our%20code%20and%20data%20are%20available%20at%20https%3A//github.com/SSSKJ/EviNET.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07288v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEVINET%253A%2520Towards%2520Open-World%2520Graph%2520Learning%2520via%2520Evidential%2520Reasoning%250A%2520%2520Network%26entry.906535625%3DWeijie%2520Guan%2520and%2520Haohui%2520Wang%2520and%2520Jian%2520Kang%2520and%2520Lihui%2520Liu%2520and%2520Dawei%2520Zhou%26entry.1292438233%3D%2520%2520Graph%2520learning%2520has%2520been%2520crucial%2520to%2520many%2520real-world%2520tasks%252C%2520but%2520they%2520are%2520often%250Astudied%2520with%2520a%2520closed-world%2520assumption%252C%2520with%2520all%2520possible%2520labels%2520of%2520data%2520known%250Aa%2520priori.%2520To%2520enable%2520effective%2520graph%2520learning%2520in%2520an%2520open%2520and%2520noisy%2520environment%252C%250Ait%2520is%2520critical%2520to%2520inform%2520the%2520model%2520users%2520when%2520the%2520model%2520makes%2520a%2520wrong%250Aprediction%2520to%2520in-distribution%2520data%2520of%2520a%2520known%2520class%252C%2520i.e.%252C%2520misclassification%250Adetection%2520or%2520when%2520the%2520model%2520encounters%2520out-of-distribution%2520from%2520novel%2520classes%252C%250Ai.e.%252C%2520out-of-distribution%2520detection.%2520This%2520paper%2520introduces%2520Evidential%2520Reasoning%250ANetwork%2520%2528EVINET%2529%252C%2520a%2520framework%2520that%2520addresses%2520these%2520two%2520challenges%2520by%250Aintegrating%2520Beta%2520embedding%2520within%2520a%2520subjective%2520logic%2520framework.%2520EVINET%2520includes%250Atwo%2520key%2520modules%253A%2520Dissonance%2520Reasoning%2520for%2520misclassification%2520detection%2520and%250AVacuity%2520Reasoning%2520for%2520out-of-distribution%2520detection.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520EVINET%2520outperforms%2520state-of-the-art%2520methods%2520across%2520multiple%250Ametrics%2520in%2520the%2520tasks%2520of%2520in-distribution%2520classification%252C%2520misclassification%250Adetection%252C%2520and%2520out-of-distribution%2520detection.%2520EVINET%2520demonstrates%2520the%2520necessity%250Aof%2520uncertainty%2520estimation%2520and%2520logical%2520reasoning%2520for%2520misclassification%2520detection%250Aand%2520out-of-distribution%2520detection%2520and%2520paves%2520the%2520way%2520for%2520open-world%2520graph%250Alearning.%2520Our%2520code%2520and%2520data%2520are%2520available%2520at%2520https%253A//github.com/SSSKJ/EviNET.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07288v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EVINET%3A%20Towards%20Open-World%20Graph%20Learning%20via%20Evidential%20Reasoning%0A%20%20Network&entry.906535625=Weijie%20Guan%20and%20Haohui%20Wang%20and%20Jian%20Kang%20and%20Lihui%20Liu%20and%20Dawei%20Zhou&entry.1292438233=%20%20Graph%20learning%20has%20been%20crucial%20to%20many%20real-world%20tasks%2C%20but%20they%20are%20often%0Astudied%20with%20a%20closed-world%20assumption%2C%20with%20all%20possible%20labels%20of%20data%20known%0Aa%20priori.%20To%20enable%20effective%20graph%20learning%20in%20an%20open%20and%20noisy%20environment%2C%0Ait%20is%20critical%20to%20inform%20the%20model%20users%20when%20the%20model%20makes%20a%20wrong%0Aprediction%20to%20in-distribution%20data%20of%20a%20known%20class%2C%20i.e.%2C%20misclassification%0Adetection%20or%20when%20the%20model%20encounters%20out-of-distribution%20from%20novel%20classes%2C%0Ai.e.%2C%20out-of-distribution%20detection.%20This%20paper%20introduces%20Evidential%20Reasoning%0ANetwork%20%28EVINET%29%2C%20a%20framework%20that%20addresses%20these%20two%20challenges%20by%0Aintegrating%20Beta%20embedding%20within%20a%20subjective%20logic%20framework.%20EVINET%20includes%0Atwo%20key%20modules%3A%20Dissonance%20Reasoning%20for%20misclassification%20detection%20and%0AVacuity%20Reasoning%20for%20out-of-distribution%20detection.%20Extensive%20experiments%0Ademonstrate%20that%20EVINET%20outperforms%20state-of-the-art%20methods%20across%20multiple%0Ametrics%20in%20the%20tasks%20of%20in-distribution%20classification%2C%20misclassification%0Adetection%2C%20and%20out-of-distribution%20detection.%20EVINET%20demonstrates%20the%20necessity%0Aof%20uncertainty%20estimation%20and%20logical%20reasoning%20for%20misclassification%20detection%0Aand%20out-of-distribution%20detection%20and%20paves%20the%20way%20for%20open-world%20graph%0Alearning.%20Our%20code%20and%20data%20are%20available%20at%20https%3A//github.com/SSSKJ/EviNET.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07288v2&entry.124074799=Read"},
{"title": "Fine-Grained Spatially Varying Material Selection in Images", "author": "Julia Guerrero-Viu and Michael Fischer and Iliyan Georgiev and Elena Garces and Diego Gutierrez and Belen Masia and Valentin Deschaintre", "abstract": "  Selection is the first step in many image editing processes, enabling faster\nand simpler modifications of all pixels sharing a common modality. In this\nwork, we present a method for material selection in images, robust to lighting\nand reflectance variations, which can be used for downstream editing tasks. We\nrely on vision transformer (ViT) models and leverage their features for\nselection, proposing a multi-resolution processing strategy that yields finer\nand more stable selection results than prior methods. Furthermore, we enable\nselection at two levels: texture and subtexture, leveraging a new two-level\nmaterial selection (DuMaS) dataset which includes dense annotations for over\n800,000 synthetic images, both on the texture and subtexture levels.\n", "link": "http://arxiv.org/abs/2506.09023v2", "date": "2025-06-11", "relevancy": 2.1766, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5856}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5507}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Grained%20Spatially%20Varying%20Material%20Selection%20in%20Images&body=Title%3A%20Fine-Grained%20Spatially%20Varying%20Material%20Selection%20in%20Images%0AAuthor%3A%20Julia%20Guerrero-Viu%20and%20Michael%20Fischer%20and%20Iliyan%20Georgiev%20and%20Elena%20Garces%20and%20Diego%20Gutierrez%20and%20Belen%20Masia%20and%20Valentin%20Deschaintre%0AAbstract%3A%20%20%20Selection%20is%20the%20first%20step%20in%20many%20image%20editing%20processes%2C%20enabling%20faster%0Aand%20simpler%20modifications%20of%20all%20pixels%20sharing%20a%20common%20modality.%20In%20this%0Awork%2C%20we%20present%20a%20method%20for%20material%20selection%20in%20images%2C%20robust%20to%20lighting%0Aand%20reflectance%20variations%2C%20which%20can%20be%20used%20for%20downstream%20editing%20tasks.%20We%0Arely%20on%20vision%20transformer%20%28ViT%29%20models%20and%20leverage%20their%20features%20for%0Aselection%2C%20proposing%20a%20multi-resolution%20processing%20strategy%20that%20yields%20finer%0Aand%20more%20stable%20selection%20results%20than%20prior%20methods.%20Furthermore%2C%20we%20enable%0Aselection%20at%20two%20levels%3A%20texture%20and%20subtexture%2C%20leveraging%20a%20new%20two-level%0Amaterial%20selection%20%28DuMaS%29%20dataset%20which%20includes%20dense%20annotations%20for%20over%0A800%2C000%20synthetic%20images%2C%20both%20on%20the%20texture%20and%20subtexture%20levels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09023v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Grained%2520Spatially%2520Varying%2520Material%2520Selection%2520in%2520Images%26entry.906535625%3DJulia%2520Guerrero-Viu%2520and%2520Michael%2520Fischer%2520and%2520Iliyan%2520Georgiev%2520and%2520Elena%2520Garces%2520and%2520Diego%2520Gutierrez%2520and%2520Belen%2520Masia%2520and%2520Valentin%2520Deschaintre%26entry.1292438233%3D%2520%2520Selection%2520is%2520the%2520first%2520step%2520in%2520many%2520image%2520editing%2520processes%252C%2520enabling%2520faster%250Aand%2520simpler%2520modifications%2520of%2520all%2520pixels%2520sharing%2520a%2520common%2520modality.%2520In%2520this%250Awork%252C%2520we%2520present%2520a%2520method%2520for%2520material%2520selection%2520in%2520images%252C%2520robust%2520to%2520lighting%250Aand%2520reflectance%2520variations%252C%2520which%2520can%2520be%2520used%2520for%2520downstream%2520editing%2520tasks.%2520We%250Arely%2520on%2520vision%2520transformer%2520%2528ViT%2529%2520models%2520and%2520leverage%2520their%2520features%2520for%250Aselection%252C%2520proposing%2520a%2520multi-resolution%2520processing%2520strategy%2520that%2520yields%2520finer%250Aand%2520more%2520stable%2520selection%2520results%2520than%2520prior%2520methods.%2520Furthermore%252C%2520we%2520enable%250Aselection%2520at%2520two%2520levels%253A%2520texture%2520and%2520subtexture%252C%2520leveraging%2520a%2520new%2520two-level%250Amaterial%2520selection%2520%2528DuMaS%2529%2520dataset%2520which%2520includes%2520dense%2520annotations%2520for%2520over%250A800%252C000%2520synthetic%2520images%252C%2520both%2520on%2520the%2520texture%2520and%2520subtexture%2520levels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09023v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Grained%20Spatially%20Varying%20Material%20Selection%20in%20Images&entry.906535625=Julia%20Guerrero-Viu%20and%20Michael%20Fischer%20and%20Iliyan%20Georgiev%20and%20Elena%20Garces%20and%20Diego%20Gutierrez%20and%20Belen%20Masia%20and%20Valentin%20Deschaintre&entry.1292438233=%20%20Selection%20is%20the%20first%20step%20in%20many%20image%20editing%20processes%2C%20enabling%20faster%0Aand%20simpler%20modifications%20of%20all%20pixels%20sharing%20a%20common%20modality.%20In%20this%0Awork%2C%20we%20present%20a%20method%20for%20material%20selection%20in%20images%2C%20robust%20to%20lighting%0Aand%20reflectance%20variations%2C%20which%20can%20be%20used%20for%20downstream%20editing%20tasks.%20We%0Arely%20on%20vision%20transformer%20%28ViT%29%20models%20and%20leverage%20their%20features%20for%0Aselection%2C%20proposing%20a%20multi-resolution%20processing%20strategy%20that%20yields%20finer%0Aand%20more%20stable%20selection%20results%20than%20prior%20methods.%20Furthermore%2C%20we%20enable%0Aselection%20at%20two%20levels%3A%20texture%20and%20subtexture%2C%20leveraging%20a%20new%20two-level%0Amaterial%20selection%20%28DuMaS%29%20dataset%20which%20includes%20dense%20annotations%20for%20over%0A800%2C000%20synthetic%20images%2C%20both%20on%20the%20texture%20and%20subtexture%20levels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09023v2&entry.124074799=Read"},
{"title": "CoRT: Code-integrated Reasoning within Thinking", "author": "Chengpeng Li and Zhengyang Tang and Ziniu Li and Mingfeng Xue and Keqin Bao and Tian Ding and Ruoyu Sun and Benyou Wang and Xiang Wang and Junyang Lin and Dayiheng Liu", "abstract": "  Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable\nprogress in natural language reasoning with long chain-of-thought (CoT), yet\nthey remain inefficient or inaccurate when handling complex mathematical\noperations. Addressing these limitations through computational tools (e.g.,\ncomputation libraries and symbolic solvers) is promising, but it introduces a\ntechnical challenge: Code Interpreter (CI) brings external knowledge beyond the\nmodel's internal text representations, thus the direct combination is not\nefficient. This paper introduces CoRT, a post-training framework for teaching\nLRMs to leverage CI effectively and efficiently. As a first step, we address\nthe data scarcity issue by synthesizing code-integrated reasoning data through\nHint-Engineering, which strategically inserts different hints at appropriate\npositions to optimize LRM-CI interaction. We manually create 30 high-quality\nsamples, upon which we post-train models ranging from 1.5B to 32B parameters,\nwith supervised fine-tuning, rejection fine-tuning and reinforcement learning.\nOur experimental results demonstrate that Hint-Engineering models achieve 4\\%\nand 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and\nDeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging\nmathematical reasoning datasets. Furthermore, Hint-Engineering models use about\n30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model\ncompared with the natural language models. The models and code are available at\nhttps://github.com/ChengpengLi1003/CoRT.\n", "link": "http://arxiv.org/abs/2506.09820v1", "date": "2025-06-11", "relevancy": 2.1689, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5479}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoRT%3A%20Code-integrated%20Reasoning%20within%20Thinking&body=Title%3A%20CoRT%3A%20Code-integrated%20Reasoning%20within%20Thinking%0AAuthor%3A%20Chengpeng%20Li%20and%20Zhengyang%20Tang%20and%20Ziniu%20Li%20and%20Mingfeng%20Xue%20and%20Keqin%20Bao%20and%20Tian%20Ding%20and%20Ruoyu%20Sun%20and%20Benyou%20Wang%20and%20Xiang%20Wang%20and%20Junyang%20Lin%20and%20Dayiheng%20Liu%0AAbstract%3A%20%20%20Large%20Reasoning%20Models%20%28LRMs%29%20like%20o1%20and%20DeepSeek-R1%20have%20shown%20remarkable%0Aprogress%20in%20natural%20language%20reasoning%20with%20long%20chain-of-thought%20%28CoT%29%2C%20yet%0Athey%20remain%20inefficient%20or%20inaccurate%20when%20handling%20complex%20mathematical%0Aoperations.%20Addressing%20these%20limitations%20through%20computational%20tools%20%28e.g.%2C%0Acomputation%20libraries%20and%20symbolic%20solvers%29%20is%20promising%2C%20but%20it%20introduces%20a%0Atechnical%20challenge%3A%20Code%20Interpreter%20%28CI%29%20brings%20external%20knowledge%20beyond%20the%0Amodel%27s%20internal%20text%20representations%2C%20thus%20the%20direct%20combination%20is%20not%0Aefficient.%20This%20paper%20introduces%20CoRT%2C%20a%20post-training%20framework%20for%20teaching%0ALRMs%20to%20leverage%20CI%20effectively%20and%20efficiently.%20As%20a%20first%20step%2C%20we%20address%0Athe%20data%20scarcity%20issue%20by%20synthesizing%20code-integrated%20reasoning%20data%20through%0AHint-Engineering%2C%20which%20strategically%20inserts%20different%20hints%20at%20appropriate%0Apositions%20to%20optimize%20LRM-CI%20interaction.%20We%20manually%20create%2030%20high-quality%0Asamples%2C%20upon%20which%20we%20post-train%20models%20ranging%20from%201.5B%20to%2032B%20parameters%2C%0Awith%20supervised%20fine-tuning%2C%20rejection%20fine-tuning%20and%20reinforcement%20learning.%0AOur%20experimental%20results%20demonstrate%20that%20Hint-Engineering%20models%20achieve%204%5C%25%0Aand%208%5C%25%20absolute%20improvements%20on%20DeepSeek-R1-Distill-Qwen-32B%20and%0ADeepSeek-R1-Distill-Qwen-1.5B%20respectively%2C%20across%20five%20challenging%0Amathematical%20reasoning%20datasets.%20Furthermore%2C%20Hint-Engineering%20models%20use%20about%0A30%5C%25%20fewer%20tokens%20for%20the%2032B%20model%20and%2050%5C%25%20fewer%20tokens%20for%20the%201.5B%20model%0Acompared%20with%20the%20natural%20language%20models.%20The%20models%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/ChengpengLi1003/CoRT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoRT%253A%2520Code-integrated%2520Reasoning%2520within%2520Thinking%26entry.906535625%3DChengpeng%2520Li%2520and%2520Zhengyang%2520Tang%2520and%2520Ziniu%2520Li%2520and%2520Mingfeng%2520Xue%2520and%2520Keqin%2520Bao%2520and%2520Tian%2520Ding%2520and%2520Ruoyu%2520Sun%2520and%2520Benyou%2520Wang%2520and%2520Xiang%2520Wang%2520and%2520Junyang%2520Lin%2520and%2520Dayiheng%2520Liu%26entry.1292438233%3D%2520%2520Large%2520Reasoning%2520Models%2520%2528LRMs%2529%2520like%2520o1%2520and%2520DeepSeek-R1%2520have%2520shown%2520remarkable%250Aprogress%2520in%2520natural%2520language%2520reasoning%2520with%2520long%2520chain-of-thought%2520%2528CoT%2529%252C%2520yet%250Athey%2520remain%2520inefficient%2520or%2520inaccurate%2520when%2520handling%2520complex%2520mathematical%250Aoperations.%2520Addressing%2520these%2520limitations%2520through%2520computational%2520tools%2520%2528e.g.%252C%250Acomputation%2520libraries%2520and%2520symbolic%2520solvers%2529%2520is%2520promising%252C%2520but%2520it%2520introduces%2520a%250Atechnical%2520challenge%253A%2520Code%2520Interpreter%2520%2528CI%2529%2520brings%2520external%2520knowledge%2520beyond%2520the%250Amodel%2527s%2520internal%2520text%2520representations%252C%2520thus%2520the%2520direct%2520combination%2520is%2520not%250Aefficient.%2520This%2520paper%2520introduces%2520CoRT%252C%2520a%2520post-training%2520framework%2520for%2520teaching%250ALRMs%2520to%2520leverage%2520CI%2520effectively%2520and%2520efficiently.%2520As%2520a%2520first%2520step%252C%2520we%2520address%250Athe%2520data%2520scarcity%2520issue%2520by%2520synthesizing%2520code-integrated%2520reasoning%2520data%2520through%250AHint-Engineering%252C%2520which%2520strategically%2520inserts%2520different%2520hints%2520at%2520appropriate%250Apositions%2520to%2520optimize%2520LRM-CI%2520interaction.%2520We%2520manually%2520create%252030%2520high-quality%250Asamples%252C%2520upon%2520which%2520we%2520post-train%2520models%2520ranging%2520from%25201.5B%2520to%252032B%2520parameters%252C%250Awith%2520supervised%2520fine-tuning%252C%2520rejection%2520fine-tuning%2520and%2520reinforcement%2520learning.%250AOur%2520experimental%2520results%2520demonstrate%2520that%2520Hint-Engineering%2520models%2520achieve%25204%255C%2525%250Aand%25208%255C%2525%2520absolute%2520improvements%2520on%2520DeepSeek-R1-Distill-Qwen-32B%2520and%250ADeepSeek-R1-Distill-Qwen-1.5B%2520respectively%252C%2520across%2520five%2520challenging%250Amathematical%2520reasoning%2520datasets.%2520Furthermore%252C%2520Hint-Engineering%2520models%2520use%2520about%250A30%255C%2525%2520fewer%2520tokens%2520for%2520the%252032B%2520model%2520and%252050%255C%2525%2520fewer%2520tokens%2520for%2520the%25201.5B%2520model%250Acompared%2520with%2520the%2520natural%2520language%2520models.%2520The%2520models%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/ChengpengLi1003/CoRT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoRT%3A%20Code-integrated%20Reasoning%20within%20Thinking&entry.906535625=Chengpeng%20Li%20and%20Zhengyang%20Tang%20and%20Ziniu%20Li%20and%20Mingfeng%20Xue%20and%20Keqin%20Bao%20and%20Tian%20Ding%20and%20Ruoyu%20Sun%20and%20Benyou%20Wang%20and%20Xiang%20Wang%20and%20Junyang%20Lin%20and%20Dayiheng%20Liu&entry.1292438233=%20%20Large%20Reasoning%20Models%20%28LRMs%29%20like%20o1%20and%20DeepSeek-R1%20have%20shown%20remarkable%0Aprogress%20in%20natural%20language%20reasoning%20with%20long%20chain-of-thought%20%28CoT%29%2C%20yet%0Athey%20remain%20inefficient%20or%20inaccurate%20when%20handling%20complex%20mathematical%0Aoperations.%20Addressing%20these%20limitations%20through%20computational%20tools%20%28e.g.%2C%0Acomputation%20libraries%20and%20symbolic%20solvers%29%20is%20promising%2C%20but%20it%20introduces%20a%0Atechnical%20challenge%3A%20Code%20Interpreter%20%28CI%29%20brings%20external%20knowledge%20beyond%20the%0Amodel%27s%20internal%20text%20representations%2C%20thus%20the%20direct%20combination%20is%20not%0Aefficient.%20This%20paper%20introduces%20CoRT%2C%20a%20post-training%20framework%20for%20teaching%0ALRMs%20to%20leverage%20CI%20effectively%20and%20efficiently.%20As%20a%20first%20step%2C%20we%20address%0Athe%20data%20scarcity%20issue%20by%20synthesizing%20code-integrated%20reasoning%20data%20through%0AHint-Engineering%2C%20which%20strategically%20inserts%20different%20hints%20at%20appropriate%0Apositions%20to%20optimize%20LRM-CI%20interaction.%20We%20manually%20create%2030%20high-quality%0Asamples%2C%20upon%20which%20we%20post-train%20models%20ranging%20from%201.5B%20to%2032B%20parameters%2C%0Awith%20supervised%20fine-tuning%2C%20rejection%20fine-tuning%20and%20reinforcement%20learning.%0AOur%20experimental%20results%20demonstrate%20that%20Hint-Engineering%20models%20achieve%204%5C%25%0Aand%208%5C%25%20absolute%20improvements%20on%20DeepSeek-R1-Distill-Qwen-32B%20and%0ADeepSeek-R1-Distill-Qwen-1.5B%20respectively%2C%20across%20five%20challenging%0Amathematical%20reasoning%20datasets.%20Furthermore%2C%20Hint-Engineering%20models%20use%20about%0A30%5C%25%20fewer%20tokens%20for%20the%2032B%20model%20and%2050%5C%25%20fewer%20tokens%20for%20the%201.5B%20model%0Acompared%20with%20the%20natural%20language%20models.%20The%20models%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/ChengpengLi1003/CoRT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09820v1&entry.124074799=Read"},
{"title": "Sampling Theory for Super-Resolution with Implicit Neural\n  Representations", "author": "Mahrokh Najaf and Gregory Ongie", "abstract": "  Implicit neural representations (INRs) have emerged as a powerful tool for\nsolving inverse problems in computer vision and computational imaging. INRs\nrepresent images as continuous domain functions realized by a neural network\ntaking spatial coordinates as inputs. However, unlike traditional pixel\nrepresentations, little is known about the sample complexity of estimating\nimages using INRs in the context of linear inverse problems. Towards this end,\nwe study the sampling requirements for recovery of a continuous domain image\nfrom its low-pass Fourier samples by fitting a single hidden-layer INR with\nReLU activation and a Fourier features layer using a generalized form of weight\ndecay regularization. Our key insight is to relate minimizers of this\nnon-convex parameter space optimization problem to minimizers of a convex\npenalty defined over an infinite-dimensional space of measures. We identify a\nsufficient number of Fourier samples for which an image realized by an INR is\nexactly recoverable by solving the INR training problem. To validate our\ntheory, we empirically assess the probability of achieving exact recovery of\nimages realized by low-width single hidden-layer INRs, and illustrate the\nperformance of INRs on super-resolution recovery of continuous domain phantom\nimages.\n", "link": "http://arxiv.org/abs/2506.09949v1", "date": "2025-06-11", "relevancy": 2.1566, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5497}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5355}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sampling%20Theory%20for%20Super-Resolution%20with%20Implicit%20Neural%0A%20%20Representations&body=Title%3A%20Sampling%20Theory%20for%20Super-Resolution%20with%20Implicit%20Neural%0A%20%20Representations%0AAuthor%3A%20Mahrokh%20Najaf%20and%20Gregory%20Ongie%0AAbstract%3A%20%20%20Implicit%20neural%20representations%20%28INRs%29%20have%20emerged%20as%20a%20powerful%20tool%20for%0Asolving%20inverse%20problems%20in%20computer%20vision%20and%20computational%20imaging.%20INRs%0Arepresent%20images%20as%20continuous%20domain%20functions%20realized%20by%20a%20neural%20network%0Ataking%20spatial%20coordinates%20as%20inputs.%20However%2C%20unlike%20traditional%20pixel%0Arepresentations%2C%20little%20is%20known%20about%20the%20sample%20complexity%20of%20estimating%0Aimages%20using%20INRs%20in%20the%20context%20of%20linear%20inverse%20problems.%20Towards%20this%20end%2C%0Awe%20study%20the%20sampling%20requirements%20for%20recovery%20of%20a%20continuous%20domain%20image%0Afrom%20its%20low-pass%20Fourier%20samples%20by%20fitting%20a%20single%20hidden-layer%20INR%20with%0AReLU%20activation%20and%20a%20Fourier%20features%20layer%20using%20a%20generalized%20form%20of%20weight%0Adecay%20regularization.%20Our%20key%20insight%20is%20to%20relate%20minimizers%20of%20this%0Anon-convex%20parameter%20space%20optimization%20problem%20to%20minimizers%20of%20a%20convex%0Apenalty%20defined%20over%20an%20infinite-dimensional%20space%20of%20measures.%20We%20identify%20a%0Asufficient%20number%20of%20Fourier%20samples%20for%20which%20an%20image%20realized%20by%20an%20INR%20is%0Aexactly%20recoverable%20by%20solving%20the%20INR%20training%20problem.%20To%20validate%20our%0Atheory%2C%20we%20empirically%20assess%20the%20probability%20of%20achieving%20exact%20recovery%20of%0Aimages%20realized%20by%20low-width%20single%20hidden-layer%20INRs%2C%20and%20illustrate%20the%0Aperformance%20of%20INRs%20on%20super-resolution%20recovery%20of%20continuous%20domain%20phantom%0Aimages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09949v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSampling%2520Theory%2520for%2520Super-Resolution%2520with%2520Implicit%2520Neural%250A%2520%2520Representations%26entry.906535625%3DMahrokh%2520Najaf%2520and%2520Gregory%2520Ongie%26entry.1292438233%3D%2520%2520Implicit%2520neural%2520representations%2520%2528INRs%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%250Asolving%2520inverse%2520problems%2520in%2520computer%2520vision%2520and%2520computational%2520imaging.%2520INRs%250Arepresent%2520images%2520as%2520continuous%2520domain%2520functions%2520realized%2520by%2520a%2520neural%2520network%250Ataking%2520spatial%2520coordinates%2520as%2520inputs.%2520However%252C%2520unlike%2520traditional%2520pixel%250Arepresentations%252C%2520little%2520is%2520known%2520about%2520the%2520sample%2520complexity%2520of%2520estimating%250Aimages%2520using%2520INRs%2520in%2520the%2520context%2520of%2520linear%2520inverse%2520problems.%2520Towards%2520this%2520end%252C%250Awe%2520study%2520the%2520sampling%2520requirements%2520for%2520recovery%2520of%2520a%2520continuous%2520domain%2520image%250Afrom%2520its%2520low-pass%2520Fourier%2520samples%2520by%2520fitting%2520a%2520single%2520hidden-layer%2520INR%2520with%250AReLU%2520activation%2520and%2520a%2520Fourier%2520features%2520layer%2520using%2520a%2520generalized%2520form%2520of%2520weight%250Adecay%2520regularization.%2520Our%2520key%2520insight%2520is%2520to%2520relate%2520minimizers%2520of%2520this%250Anon-convex%2520parameter%2520space%2520optimization%2520problem%2520to%2520minimizers%2520of%2520a%2520convex%250Apenalty%2520defined%2520over%2520an%2520infinite-dimensional%2520space%2520of%2520measures.%2520We%2520identify%2520a%250Asufficient%2520number%2520of%2520Fourier%2520samples%2520for%2520which%2520an%2520image%2520realized%2520by%2520an%2520INR%2520is%250Aexactly%2520recoverable%2520by%2520solving%2520the%2520INR%2520training%2520problem.%2520To%2520validate%2520our%250Atheory%252C%2520we%2520empirically%2520assess%2520the%2520probability%2520of%2520achieving%2520exact%2520recovery%2520of%250Aimages%2520realized%2520by%2520low-width%2520single%2520hidden-layer%2520INRs%252C%2520and%2520illustrate%2520the%250Aperformance%2520of%2520INRs%2520on%2520super-resolution%2520recovery%2520of%2520continuous%2520domain%2520phantom%250Aimages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09949v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sampling%20Theory%20for%20Super-Resolution%20with%20Implicit%20Neural%0A%20%20Representations&entry.906535625=Mahrokh%20Najaf%20and%20Gregory%20Ongie&entry.1292438233=%20%20Implicit%20neural%20representations%20%28INRs%29%20have%20emerged%20as%20a%20powerful%20tool%20for%0Asolving%20inverse%20problems%20in%20computer%20vision%20and%20computational%20imaging.%20INRs%0Arepresent%20images%20as%20continuous%20domain%20functions%20realized%20by%20a%20neural%20network%0Ataking%20spatial%20coordinates%20as%20inputs.%20However%2C%20unlike%20traditional%20pixel%0Arepresentations%2C%20little%20is%20known%20about%20the%20sample%20complexity%20of%20estimating%0Aimages%20using%20INRs%20in%20the%20context%20of%20linear%20inverse%20problems.%20Towards%20this%20end%2C%0Awe%20study%20the%20sampling%20requirements%20for%20recovery%20of%20a%20continuous%20domain%20image%0Afrom%20its%20low-pass%20Fourier%20samples%20by%20fitting%20a%20single%20hidden-layer%20INR%20with%0AReLU%20activation%20and%20a%20Fourier%20features%20layer%20using%20a%20generalized%20form%20of%20weight%0Adecay%20regularization.%20Our%20key%20insight%20is%20to%20relate%20minimizers%20of%20this%0Anon-convex%20parameter%20space%20optimization%20problem%20to%20minimizers%20of%20a%20convex%0Apenalty%20defined%20over%20an%20infinite-dimensional%20space%20of%20measures.%20We%20identify%20a%0Asufficient%20number%20of%20Fourier%20samples%20for%20which%20an%20image%20realized%20by%20an%20INR%20is%0Aexactly%20recoverable%20by%20solving%20the%20INR%20training%20problem.%20To%20validate%20our%0Atheory%2C%20we%20empirically%20assess%20the%20probability%20of%20achieving%20exact%20recovery%20of%0Aimages%20realized%20by%20low-width%20single%20hidden-layer%20INRs%2C%20and%20illustrate%20the%0Aperformance%20of%20INRs%20on%20super-resolution%20recovery%20of%20continuous%20domain%20phantom%0Aimages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09949v1&entry.124074799=Read"},
{"title": "HRTR: A Single-stage Transformer for Fine-grained Sub-second Action\n  Segmentation in Stroke Rehabilitation", "author": "Halil Ismail Helvaci and Justin Philip Huber and Jihye Bae and Sen-ching Samson Cheung", "abstract": "  Stroke rehabilitation often demands precise tracking of patient movements to\nmonitor progress, with complexities of rehabilitation exercises presenting two\ncritical challenges: fine-grained and sub-second (under one-second) action\ndetection. In this work, we propose the High Resolution Temporal Transformer\n(HRTR), to time-localize and classify high-resolution (fine-grained),\nsub-second actions in a single-stage transformer, eliminating the need for\nmulti-stage methods and post-processing. Without any refinements, HRTR\noutperforms state-of-the-art systems on both stroke related and general\ndatasets, achieving Edit Score (ES) of 70.1 on StrokeRehab Video, 69.4 on\nStrokeRehab IMU, and 88.4 on 50Salads.\n", "link": "http://arxiv.org/abs/2506.02472v2", "date": "2025-06-11", "relevancy": 2.1491, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5465}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5391}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HRTR%3A%20A%20Single-stage%20Transformer%20for%20Fine-grained%20Sub-second%20Action%0A%20%20Segmentation%20in%20Stroke%20Rehabilitation&body=Title%3A%20HRTR%3A%20A%20Single-stage%20Transformer%20for%20Fine-grained%20Sub-second%20Action%0A%20%20Segmentation%20in%20Stroke%20Rehabilitation%0AAuthor%3A%20Halil%20Ismail%20Helvaci%20and%20Justin%20Philip%20Huber%20and%20Jihye%20Bae%20and%20Sen-ching%20Samson%20Cheung%0AAbstract%3A%20%20%20Stroke%20rehabilitation%20often%20demands%20precise%20tracking%20of%20patient%20movements%20to%0Amonitor%20progress%2C%20with%20complexities%20of%20rehabilitation%20exercises%20presenting%20two%0Acritical%20challenges%3A%20fine-grained%20and%20sub-second%20%28under%20one-second%29%20action%0Adetection.%20In%20this%20work%2C%20we%20propose%20the%20High%20Resolution%20Temporal%20Transformer%0A%28HRTR%29%2C%20to%20time-localize%20and%20classify%20high-resolution%20%28fine-grained%29%2C%0Asub-second%20actions%20in%20a%20single-stage%20transformer%2C%20eliminating%20the%20need%20for%0Amulti-stage%20methods%20and%20post-processing.%20Without%20any%20refinements%2C%20HRTR%0Aoutperforms%20state-of-the-art%20systems%20on%20both%20stroke%20related%20and%20general%0Adatasets%2C%20achieving%20Edit%20Score%20%28ES%29%20of%2070.1%20on%20StrokeRehab%20Video%2C%2069.4%20on%0AStrokeRehab%20IMU%2C%20and%2088.4%20on%2050Salads.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02472v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHRTR%253A%2520A%2520Single-stage%2520Transformer%2520for%2520Fine-grained%2520Sub-second%2520Action%250A%2520%2520Segmentation%2520in%2520Stroke%2520Rehabilitation%26entry.906535625%3DHalil%2520Ismail%2520Helvaci%2520and%2520Justin%2520Philip%2520Huber%2520and%2520Jihye%2520Bae%2520and%2520Sen-ching%2520Samson%2520Cheung%26entry.1292438233%3D%2520%2520Stroke%2520rehabilitation%2520often%2520demands%2520precise%2520tracking%2520of%2520patient%2520movements%2520to%250Amonitor%2520progress%252C%2520with%2520complexities%2520of%2520rehabilitation%2520exercises%2520presenting%2520two%250Acritical%2520challenges%253A%2520fine-grained%2520and%2520sub-second%2520%2528under%2520one-second%2529%2520action%250Adetection.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520High%2520Resolution%2520Temporal%2520Transformer%250A%2528HRTR%2529%252C%2520to%2520time-localize%2520and%2520classify%2520high-resolution%2520%2528fine-grained%2529%252C%250Asub-second%2520actions%2520in%2520a%2520single-stage%2520transformer%252C%2520eliminating%2520the%2520need%2520for%250Amulti-stage%2520methods%2520and%2520post-processing.%2520Without%2520any%2520refinements%252C%2520HRTR%250Aoutperforms%2520state-of-the-art%2520systems%2520on%2520both%2520stroke%2520related%2520and%2520general%250Adatasets%252C%2520achieving%2520Edit%2520Score%2520%2528ES%2529%2520of%252070.1%2520on%2520StrokeRehab%2520Video%252C%252069.4%2520on%250AStrokeRehab%2520IMU%252C%2520and%252088.4%2520on%252050Salads.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02472v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HRTR%3A%20A%20Single-stage%20Transformer%20for%20Fine-grained%20Sub-second%20Action%0A%20%20Segmentation%20in%20Stroke%20Rehabilitation&entry.906535625=Halil%20Ismail%20Helvaci%20and%20Justin%20Philip%20Huber%20and%20Jihye%20Bae%20and%20Sen-ching%20Samson%20Cheung&entry.1292438233=%20%20Stroke%20rehabilitation%20often%20demands%20precise%20tracking%20of%20patient%20movements%20to%0Amonitor%20progress%2C%20with%20complexities%20of%20rehabilitation%20exercises%20presenting%20two%0Acritical%20challenges%3A%20fine-grained%20and%20sub-second%20%28under%20one-second%29%20action%0Adetection.%20In%20this%20work%2C%20we%20propose%20the%20High%20Resolution%20Temporal%20Transformer%0A%28HRTR%29%2C%20to%20time-localize%20and%20classify%20high-resolution%20%28fine-grained%29%2C%0Asub-second%20actions%20in%20a%20single-stage%20transformer%2C%20eliminating%20the%20need%20for%0Amulti-stage%20methods%20and%20post-processing.%20Without%20any%20refinements%2C%20HRTR%0Aoutperforms%20state-of-the-art%20systems%20on%20both%20stroke%20related%20and%20general%0Adatasets%2C%20achieving%20Edit%20Score%20%28ES%29%20of%2070.1%20on%20StrokeRehab%20Video%2C%2069.4%20on%0AStrokeRehab%20IMU%2C%20and%2088.4%20on%2050Salads.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02472v2&entry.124074799=Read"},
{"title": "Multiverse: Your Language Models Secretly Decide How to Parallelize and\n  Merge Generation", "author": "Xinyu Yang and Yuwei An and Hongyi Liu and Tianqi Chen and Beidi Chen", "abstract": "  Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit\nparallelism in sequential generation. Inspired by this, we introduce\nMultiverse, a new generative model that enables natively parallel generation.\nMultiverse internalizes a MapReduce paradigm, generating automatically through\nthree stages: (i) a Map stage for adaptive task decomposition, (ii) a Process\nstage for parallel subtask execution, and (iii) a Reduce stage for lossless\nresult synthesis. Next, we build a real-world Multiverse reasoning model with\nco-design of data, algorithm, and system, enabling rapid and seamless transfer\nfrom frontier AR-LLMs. Starting from sequential reasoning chains, we create\nMultiverse 1K by converting them into structured training data using an\nautomated LLM-assisted pipeline, avoiding costly human annotations.\nAlgorithmically, we design Multiverse Attention to separate parallel reasoning\nsteps while keeping compatibility with causal attention for efficient training.\nSystematically, we implement Multiverse Engine to enable parallel inference. It\nfeatures a dedicated scheduler that dynamically switches between sequential and\nparallel generation, triggered directly by the model. After a 3-hour\nfine-tuning with 1K examples, our Multiverse-32B stands as the only\nopen-sourced non-AR model achieving performance on par with leading AR-LLMs of\nthe same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.\nMoreover, our budget control experiments show that Multiverse-32B exhibits\nsuperior scaling, outperforming AR-LLMs by 1.87% on average using the same\ncontext length. Such scaling further leads to practical efficiency gain,\nachieving up to 2x speedup across varying batch sizes. We have open-sourced the\nentire Multiverse ecosystem, including data, model weights, engine, supporting\ntools, as well as complete data curation prompts and detailed training and\nevaluation recipes.\n", "link": "http://arxiv.org/abs/2506.09991v1", "date": "2025-06-11", "relevancy": 2.1468, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5735}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5166}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiverse%3A%20Your%20Language%20Models%20Secretly%20Decide%20How%20to%20Parallelize%20and%0A%20%20Merge%20Generation&body=Title%3A%20Multiverse%3A%20Your%20Language%20Models%20Secretly%20Decide%20How%20to%20Parallelize%20and%0A%20%20Merge%20Generation%0AAuthor%3A%20Xinyu%20Yang%20and%20Yuwei%20An%20and%20Hongyi%20Liu%20and%20Tianqi%20Chen%20and%20Beidi%20Chen%0AAbstract%3A%20%20%20Autoregressive%20Large%20Language%20Models%20%28AR-LLMs%29%20frequently%20exhibit%20implicit%0Aparallelism%20in%20sequential%20generation.%20Inspired%20by%20this%2C%20we%20introduce%0AMultiverse%2C%20a%20new%20generative%20model%20that%20enables%20natively%20parallel%20generation.%0AMultiverse%20internalizes%20a%20MapReduce%20paradigm%2C%20generating%20automatically%20through%0Athree%20stages%3A%20%28i%29%20a%20Map%20stage%20for%20adaptive%20task%20decomposition%2C%20%28ii%29%20a%20Process%0Astage%20for%20parallel%20subtask%20execution%2C%20and%20%28iii%29%20a%20Reduce%20stage%20for%20lossless%0Aresult%20synthesis.%20Next%2C%20we%20build%20a%20real-world%20Multiverse%20reasoning%20model%20with%0Aco-design%20of%20data%2C%20algorithm%2C%20and%20system%2C%20enabling%20rapid%20and%20seamless%20transfer%0Afrom%20frontier%20AR-LLMs.%20Starting%20from%20sequential%20reasoning%20chains%2C%20we%20create%0AMultiverse%201K%20by%20converting%20them%20into%20structured%20training%20data%20using%20an%0Aautomated%20LLM-assisted%20pipeline%2C%20avoiding%20costly%20human%20annotations.%0AAlgorithmically%2C%20we%20design%20Multiverse%20Attention%20to%20separate%20parallel%20reasoning%0Asteps%20while%20keeping%20compatibility%20with%20causal%20attention%20for%20efficient%20training.%0ASystematically%2C%20we%20implement%20Multiverse%20Engine%20to%20enable%20parallel%20inference.%20It%0Afeatures%20a%20dedicated%20scheduler%20that%20dynamically%20switches%20between%20sequential%20and%0Aparallel%20generation%2C%20triggered%20directly%20by%20the%20model.%20After%20a%203-hour%0Afine-tuning%20with%201K%20examples%2C%20our%20Multiverse-32B%20stands%20as%20the%20only%0Aopen-sourced%20non-AR%20model%20achieving%20performance%20on%20par%20with%20leading%20AR-LLMs%20of%0Athe%20same%20scale%2C%20evidenced%20by%20AIME24%20%26%2025%20scores%20of%2054%25%20and%2046%25%2C%20respectively.%0AMoreover%2C%20our%20budget%20control%20experiments%20show%20that%20Multiverse-32B%20exhibits%0Asuperior%20scaling%2C%20outperforming%20AR-LLMs%20by%201.87%25%20on%20average%20using%20the%20same%0Acontext%20length.%20Such%20scaling%20further%20leads%20to%20practical%20efficiency%20gain%2C%0Aachieving%20up%20to%202x%20speedup%20across%20varying%20batch%20sizes.%20We%20have%20open-sourced%20the%0Aentire%20Multiverse%20ecosystem%2C%20including%20data%2C%20model%20weights%2C%20engine%2C%20supporting%0Atools%2C%20as%20well%20as%20complete%20data%20curation%20prompts%20and%20detailed%20training%20and%0Aevaluation%20recipes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiverse%253A%2520Your%2520Language%2520Models%2520Secretly%2520Decide%2520How%2520to%2520Parallelize%2520and%250A%2520%2520Merge%2520Generation%26entry.906535625%3DXinyu%2520Yang%2520and%2520Yuwei%2520An%2520and%2520Hongyi%2520Liu%2520and%2520Tianqi%2520Chen%2520and%2520Beidi%2520Chen%26entry.1292438233%3D%2520%2520Autoregressive%2520Large%2520Language%2520Models%2520%2528AR-LLMs%2529%2520frequently%2520exhibit%2520implicit%250Aparallelism%2520in%2520sequential%2520generation.%2520Inspired%2520by%2520this%252C%2520we%2520introduce%250AMultiverse%252C%2520a%2520new%2520generative%2520model%2520that%2520enables%2520natively%2520parallel%2520generation.%250AMultiverse%2520internalizes%2520a%2520MapReduce%2520paradigm%252C%2520generating%2520automatically%2520through%250Athree%2520stages%253A%2520%2528i%2529%2520a%2520Map%2520stage%2520for%2520adaptive%2520task%2520decomposition%252C%2520%2528ii%2529%2520a%2520Process%250Astage%2520for%2520parallel%2520subtask%2520execution%252C%2520and%2520%2528iii%2529%2520a%2520Reduce%2520stage%2520for%2520lossless%250Aresult%2520synthesis.%2520Next%252C%2520we%2520build%2520a%2520real-world%2520Multiverse%2520reasoning%2520model%2520with%250Aco-design%2520of%2520data%252C%2520algorithm%252C%2520and%2520system%252C%2520enabling%2520rapid%2520and%2520seamless%2520transfer%250Afrom%2520frontier%2520AR-LLMs.%2520Starting%2520from%2520sequential%2520reasoning%2520chains%252C%2520we%2520create%250AMultiverse%25201K%2520by%2520converting%2520them%2520into%2520structured%2520training%2520data%2520using%2520an%250Aautomated%2520LLM-assisted%2520pipeline%252C%2520avoiding%2520costly%2520human%2520annotations.%250AAlgorithmically%252C%2520we%2520design%2520Multiverse%2520Attention%2520to%2520separate%2520parallel%2520reasoning%250Asteps%2520while%2520keeping%2520compatibility%2520with%2520causal%2520attention%2520for%2520efficient%2520training.%250ASystematically%252C%2520we%2520implement%2520Multiverse%2520Engine%2520to%2520enable%2520parallel%2520inference.%2520It%250Afeatures%2520a%2520dedicated%2520scheduler%2520that%2520dynamically%2520switches%2520between%2520sequential%2520and%250Aparallel%2520generation%252C%2520triggered%2520directly%2520by%2520the%2520model.%2520After%2520a%25203-hour%250Afine-tuning%2520with%25201K%2520examples%252C%2520our%2520Multiverse-32B%2520stands%2520as%2520the%2520only%250Aopen-sourced%2520non-AR%2520model%2520achieving%2520performance%2520on%2520par%2520with%2520leading%2520AR-LLMs%2520of%250Athe%2520same%2520scale%252C%2520evidenced%2520by%2520AIME24%2520%2526%252025%2520scores%2520of%252054%2525%2520and%252046%2525%252C%2520respectively.%250AMoreover%252C%2520our%2520budget%2520control%2520experiments%2520show%2520that%2520Multiverse-32B%2520exhibits%250Asuperior%2520scaling%252C%2520outperforming%2520AR-LLMs%2520by%25201.87%2525%2520on%2520average%2520using%2520the%2520same%250Acontext%2520length.%2520Such%2520scaling%2520further%2520leads%2520to%2520practical%2520efficiency%2520gain%252C%250Aachieving%2520up%2520to%25202x%2520speedup%2520across%2520varying%2520batch%2520sizes.%2520We%2520have%2520open-sourced%2520the%250Aentire%2520Multiverse%2520ecosystem%252C%2520including%2520data%252C%2520model%2520weights%252C%2520engine%252C%2520supporting%250Atools%252C%2520as%2520well%2520as%2520complete%2520data%2520curation%2520prompts%2520and%2520detailed%2520training%2520and%250Aevaluation%2520recipes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiverse%3A%20Your%20Language%20Models%20Secretly%20Decide%20How%20to%20Parallelize%20and%0A%20%20Merge%20Generation&entry.906535625=Xinyu%20Yang%20and%20Yuwei%20An%20and%20Hongyi%20Liu%20and%20Tianqi%20Chen%20and%20Beidi%20Chen&entry.1292438233=%20%20Autoregressive%20Large%20Language%20Models%20%28AR-LLMs%29%20frequently%20exhibit%20implicit%0Aparallelism%20in%20sequential%20generation.%20Inspired%20by%20this%2C%20we%20introduce%0AMultiverse%2C%20a%20new%20generative%20model%20that%20enables%20natively%20parallel%20generation.%0AMultiverse%20internalizes%20a%20MapReduce%20paradigm%2C%20generating%20automatically%20through%0Athree%20stages%3A%20%28i%29%20a%20Map%20stage%20for%20adaptive%20task%20decomposition%2C%20%28ii%29%20a%20Process%0Astage%20for%20parallel%20subtask%20execution%2C%20and%20%28iii%29%20a%20Reduce%20stage%20for%20lossless%0Aresult%20synthesis.%20Next%2C%20we%20build%20a%20real-world%20Multiverse%20reasoning%20model%20with%0Aco-design%20of%20data%2C%20algorithm%2C%20and%20system%2C%20enabling%20rapid%20and%20seamless%20transfer%0Afrom%20frontier%20AR-LLMs.%20Starting%20from%20sequential%20reasoning%20chains%2C%20we%20create%0AMultiverse%201K%20by%20converting%20them%20into%20structured%20training%20data%20using%20an%0Aautomated%20LLM-assisted%20pipeline%2C%20avoiding%20costly%20human%20annotations.%0AAlgorithmically%2C%20we%20design%20Multiverse%20Attention%20to%20separate%20parallel%20reasoning%0Asteps%20while%20keeping%20compatibility%20with%20causal%20attention%20for%20efficient%20training.%0ASystematically%2C%20we%20implement%20Multiverse%20Engine%20to%20enable%20parallel%20inference.%20It%0Afeatures%20a%20dedicated%20scheduler%20that%20dynamically%20switches%20between%20sequential%20and%0Aparallel%20generation%2C%20triggered%20directly%20by%20the%20model.%20After%20a%203-hour%0Afine-tuning%20with%201K%20examples%2C%20our%20Multiverse-32B%20stands%20as%20the%20only%0Aopen-sourced%20non-AR%20model%20achieving%20performance%20on%20par%20with%20leading%20AR-LLMs%20of%0Athe%20same%20scale%2C%20evidenced%20by%20AIME24%20%26%2025%20scores%20of%2054%25%20and%2046%25%2C%20respectively.%0AMoreover%2C%20our%20budget%20control%20experiments%20show%20that%20Multiverse-32B%20exhibits%0Asuperior%20scaling%2C%20outperforming%20AR-LLMs%20by%201.87%25%20on%20average%20using%20the%20same%0Acontext%20length.%20Such%20scaling%20further%20leads%20to%20practical%20efficiency%20gain%2C%0Aachieving%20up%20to%202x%20speedup%20across%20varying%20batch%20sizes.%20We%20have%20open-sourced%20the%0Aentire%20Multiverse%20ecosystem%2C%20including%20data%2C%20model%20weights%2C%20engine%2C%20supporting%0Atools%2C%20as%20well%20as%20complete%20data%20curation%20prompts%20and%20detailed%20training%20and%0Aevaluation%20recipes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09991v1&entry.124074799=Read"},
{"title": "MVTamperBench: Evaluating Robustness of Vision-Language Models", "author": "Amit Agarwal and Srikant Panda and Angeline Charles and Bhargava Kumar and Hitesh Patel and Priyaranjan Pattnayak and Taki Hasan Rafi and Tejaswini Kumar and Hansa Meghwani and Karan Gupta and Dong-Kyu Chae", "abstract": "  Multimodal Large Language Models (MLLMs), are recent advancement of\nVision-Language Models (VLMs) that have driven major advances in video\nunderstanding. However, their vulnerability to adversarial tampering and\nmanipulations remains underexplored. To address this gap, we introduce\n\\textbf{MVTamperBench}, a benchmark that systematically evaluates MLLM\nrobustness against five prevalent tampering techniques: rotation, masking,\nsubstitution, repetition, and dropping; based on real-world visual tampering\nscenarios such as surveillance interference, social media content edits, and\nmisinformation injection. MVTamperBench comprises ~3.4K original videos,\nexpanded into over ~17K tampered clips covering 19 distinct video manipulation\ntasks. This benchmark challenges models to detect manipulations in spatial and\ntemporal coherence. We evaluate 45 recent MLLMs from 15+ model families. We\nreveal substantial variability in resilience across tampering types and show\nthat larger parameter counts do not necessarily guarantee robustness.\nMVTamperBench sets a new benchmark for developing tamper-resilient MLLM in\nsafety-critical applications, including detecting clickbait, preventing harmful\ncontent distribution, and enforcing policies on media platforms. We release all\ncode, data, and benchmark to foster open research in trustworthy video\nunderstanding.\n  Code: https://amitbcp.github.io/MVTamperBench/ Data:\nhttps://huggingface.co/datasets/Srikant86/MVTamperBench\n", "link": "http://arxiv.org/abs/2412.19794v5", "date": "2025-06-11", "relevancy": 2.1422, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5548}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5368}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVTamperBench%3A%20Evaluating%20Robustness%20of%20Vision-Language%20Models&body=Title%3A%20MVTamperBench%3A%20Evaluating%20Robustness%20of%20Vision-Language%20Models%0AAuthor%3A%20Amit%20Agarwal%20and%20Srikant%20Panda%20and%20Angeline%20Charles%20and%20Bhargava%20Kumar%20and%20Hitesh%20Patel%20and%20Priyaranjan%20Pattnayak%20and%20Taki%20Hasan%20Rafi%20and%20Tejaswini%20Kumar%20and%20Hansa%20Meghwani%20and%20Karan%20Gupta%20and%20Dong-Kyu%20Chae%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20are%20recent%20advancement%20of%0AVision-Language%20Models%20%28VLMs%29%20that%20have%20driven%20major%20advances%20in%20video%0Aunderstanding.%20However%2C%20their%20vulnerability%20to%20adversarial%20tampering%20and%0Amanipulations%20remains%20underexplored.%20To%20address%20this%20gap%2C%20we%20introduce%0A%5Ctextbf%7BMVTamperBench%7D%2C%20a%20benchmark%20that%20systematically%20evaluates%20MLLM%0Arobustness%20against%20five%20prevalent%20tampering%20techniques%3A%20rotation%2C%20masking%2C%0Asubstitution%2C%20repetition%2C%20and%20dropping%3B%20based%20on%20real-world%20visual%20tampering%0Ascenarios%20such%20as%20surveillance%20interference%2C%20social%20media%20content%20edits%2C%20and%0Amisinformation%20injection.%20MVTamperBench%20comprises%20~3.4K%20original%20videos%2C%0Aexpanded%20into%20over%20~17K%20tampered%20clips%20covering%2019%20distinct%20video%20manipulation%0Atasks.%20This%20benchmark%20challenges%20models%20to%20detect%20manipulations%20in%20spatial%20and%0Atemporal%20coherence.%20We%20evaluate%2045%20recent%20MLLMs%20from%2015%2B%20model%20families.%20We%0Areveal%20substantial%20variability%20in%20resilience%20across%20tampering%20types%20and%20show%0Athat%20larger%20parameter%20counts%20do%20not%20necessarily%20guarantee%20robustness.%0AMVTamperBench%20sets%20a%20new%20benchmark%20for%20developing%20tamper-resilient%20MLLM%20in%0Asafety-critical%20applications%2C%20including%20detecting%20clickbait%2C%20preventing%20harmful%0Acontent%20distribution%2C%20and%20enforcing%20policies%20on%20media%20platforms.%20We%20release%20all%0Acode%2C%20data%2C%20and%20benchmark%20to%20foster%20open%20research%20in%20trustworthy%20video%0Aunderstanding.%0A%20%20Code%3A%20https%3A//amitbcp.github.io/MVTamperBench/%20Data%3A%0Ahttps%3A//huggingface.co/datasets/Srikant86/MVTamperBench%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19794v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVTamperBench%253A%2520Evaluating%2520Robustness%2520of%2520Vision-Language%2520Models%26entry.906535625%3DAmit%2520Agarwal%2520and%2520Srikant%2520Panda%2520and%2520Angeline%2520Charles%2520and%2520Bhargava%2520Kumar%2520and%2520Hitesh%2520Patel%2520and%2520Priyaranjan%2520Pattnayak%2520and%2520Taki%2520Hasan%2520Rafi%2520and%2520Tejaswini%2520Kumar%2520and%2520Hansa%2520Meghwani%2520and%2520Karan%2520Gupta%2520and%2520Dong-Kyu%2520Chae%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520are%2520recent%2520advancement%2520of%250AVision-Language%2520Models%2520%2528VLMs%2529%2520that%2520have%2520driven%2520major%2520advances%2520in%2520video%250Aunderstanding.%2520However%252C%2520their%2520vulnerability%2520to%2520adversarial%2520tampering%2520and%250Amanipulations%2520remains%2520underexplored.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250A%255Ctextbf%257BMVTamperBench%257D%252C%2520a%2520benchmark%2520that%2520systematically%2520evaluates%2520MLLM%250Arobustness%2520against%2520five%2520prevalent%2520tampering%2520techniques%253A%2520rotation%252C%2520masking%252C%250Asubstitution%252C%2520repetition%252C%2520and%2520dropping%253B%2520based%2520on%2520real-world%2520visual%2520tampering%250Ascenarios%2520such%2520as%2520surveillance%2520interference%252C%2520social%2520media%2520content%2520edits%252C%2520and%250Amisinformation%2520injection.%2520MVTamperBench%2520comprises%2520~3.4K%2520original%2520videos%252C%250Aexpanded%2520into%2520over%2520~17K%2520tampered%2520clips%2520covering%252019%2520distinct%2520video%2520manipulation%250Atasks.%2520This%2520benchmark%2520challenges%2520models%2520to%2520detect%2520manipulations%2520in%2520spatial%2520and%250Atemporal%2520coherence.%2520We%2520evaluate%252045%2520recent%2520MLLMs%2520from%252015%252B%2520model%2520families.%2520We%250Areveal%2520substantial%2520variability%2520in%2520resilience%2520across%2520tampering%2520types%2520and%2520show%250Athat%2520larger%2520parameter%2520counts%2520do%2520not%2520necessarily%2520guarantee%2520robustness.%250AMVTamperBench%2520sets%2520a%2520new%2520benchmark%2520for%2520developing%2520tamper-resilient%2520MLLM%2520in%250Asafety-critical%2520applications%252C%2520including%2520detecting%2520clickbait%252C%2520preventing%2520harmful%250Acontent%2520distribution%252C%2520and%2520enforcing%2520policies%2520on%2520media%2520platforms.%2520We%2520release%2520all%250Acode%252C%2520data%252C%2520and%2520benchmark%2520to%2520foster%2520open%2520research%2520in%2520trustworthy%2520video%250Aunderstanding.%250A%2520%2520Code%253A%2520https%253A//amitbcp.github.io/MVTamperBench/%2520Data%253A%250Ahttps%253A//huggingface.co/datasets/Srikant86/MVTamperBench%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19794v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVTamperBench%3A%20Evaluating%20Robustness%20of%20Vision-Language%20Models&entry.906535625=Amit%20Agarwal%20and%20Srikant%20Panda%20and%20Angeline%20Charles%20and%20Bhargava%20Kumar%20and%20Hitesh%20Patel%20and%20Priyaranjan%20Pattnayak%20and%20Taki%20Hasan%20Rafi%20and%20Tejaswini%20Kumar%20and%20Hansa%20Meghwani%20and%20Karan%20Gupta%20and%20Dong-Kyu%20Chae&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20are%20recent%20advancement%20of%0AVision-Language%20Models%20%28VLMs%29%20that%20have%20driven%20major%20advances%20in%20video%0Aunderstanding.%20However%2C%20their%20vulnerability%20to%20adversarial%20tampering%20and%0Amanipulations%20remains%20underexplored.%20To%20address%20this%20gap%2C%20we%20introduce%0A%5Ctextbf%7BMVTamperBench%7D%2C%20a%20benchmark%20that%20systematically%20evaluates%20MLLM%0Arobustness%20against%20five%20prevalent%20tampering%20techniques%3A%20rotation%2C%20masking%2C%0Asubstitution%2C%20repetition%2C%20and%20dropping%3B%20based%20on%20real-world%20visual%20tampering%0Ascenarios%20such%20as%20surveillance%20interference%2C%20social%20media%20content%20edits%2C%20and%0Amisinformation%20injection.%20MVTamperBench%20comprises%20~3.4K%20original%20videos%2C%0Aexpanded%20into%20over%20~17K%20tampered%20clips%20covering%2019%20distinct%20video%20manipulation%0Atasks.%20This%20benchmark%20challenges%20models%20to%20detect%20manipulations%20in%20spatial%20and%0Atemporal%20coherence.%20We%20evaluate%2045%20recent%20MLLMs%20from%2015%2B%20model%20families.%20We%0Areveal%20substantial%20variability%20in%20resilience%20across%20tampering%20types%20and%20show%0Athat%20larger%20parameter%20counts%20do%20not%20necessarily%20guarantee%20robustness.%0AMVTamperBench%20sets%20a%20new%20benchmark%20for%20developing%20tamper-resilient%20MLLM%20in%0Asafety-critical%20applications%2C%20including%20detecting%20clickbait%2C%20preventing%20harmful%0Acontent%20distribution%2C%20and%20enforcing%20policies%20on%20media%20platforms.%20We%20release%20all%0Acode%2C%20data%2C%20and%20benchmark%20to%20foster%20open%20research%20in%20trustworthy%20video%0Aunderstanding.%0A%20%20Code%3A%20https%3A//amitbcp.github.io/MVTamperBench/%20Data%3A%0Ahttps%3A//huggingface.co/datasets/Srikant86/MVTamperBench%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19794v5&entry.124074799=Read"},
{"title": "Q-SAM2: Accurate Quantization for Segment Anything Model 2", "author": "Nicola Farronato and Florian Scheidegger and Mattia Rigotti and Cristiano Malossi and Michele Magno and Haotong Qin", "abstract": "  The Segment Anything Model 2 (SAM2) has gained significant attention as a\nfoundational approach for promptable image and video segmentation. However, its\nexpensive computational and memory consumption poses a severe challenge for its\napplication in resource-constrained scenarios. In this paper, we propose an\naccurate low-bit quantization method for efficient SAM2, termed Q-SAM2. To\naddress the performance degradation caused by the singularities in weight and\nactivation distributions during quantization, Q-SAM2 introduces two novel\ntechnical contributions. We first introduce a linear layer calibration method\nfor low-bit initialization of SAM2, which minimizes the Frobenius norm over a\nsmall image batch to reposition weight distributions for improved quantization.\nWe then propose a Quantization-Aware Training (QAT) pipeline that applies\nclipping to suppress outliers and allows the network to adapt to quantization\nthresholds during training. Our comprehensive experiments demonstrate that\nQ-SAM2 allows for highly accurate inference while substantially improving\nefficiency. Both quantitative and visual results show that our Q-SAM2 surpasses\nexisting state-of-the-art general quantization schemes, especially for\nultra-low 2-bit quantization. While designed for quantization-aware training,\nour proposed calibration technique also proves effective in post-training\nquantization, achieving up to a 66% mIoU accuracy improvement over\nnon-calibrated models.\n", "link": "http://arxiv.org/abs/2506.09782v1", "date": "2025-06-11", "relevancy": 2.1163, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5481}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5455}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Q-SAM2%3A%20Accurate%20Quantization%20for%20Segment%20Anything%20Model%202&body=Title%3A%20Q-SAM2%3A%20Accurate%20Quantization%20for%20Segment%20Anything%20Model%202%0AAuthor%3A%20Nicola%20Farronato%20and%20Florian%20Scheidegger%20and%20Mattia%20Rigotti%20and%20Cristiano%20Malossi%20and%20Michele%20Magno%20and%20Haotong%20Qin%0AAbstract%3A%20%20%20The%20Segment%20Anything%20Model%202%20%28SAM2%29%20has%20gained%20significant%20attention%20as%20a%0Afoundational%20approach%20for%20promptable%20image%20and%20video%20segmentation.%20However%2C%20its%0Aexpensive%20computational%20and%20memory%20consumption%20poses%20a%20severe%20challenge%20for%20its%0Aapplication%20in%20resource-constrained%20scenarios.%20In%20this%20paper%2C%20we%20propose%20an%0Aaccurate%20low-bit%20quantization%20method%20for%20efficient%20SAM2%2C%20termed%20Q-SAM2.%20To%0Aaddress%20the%20performance%20degradation%20caused%20by%20the%20singularities%20in%20weight%20and%0Aactivation%20distributions%20during%20quantization%2C%20Q-SAM2%20introduces%20two%20novel%0Atechnical%20contributions.%20We%20first%20introduce%20a%20linear%20layer%20calibration%20method%0Afor%20low-bit%20initialization%20of%20SAM2%2C%20which%20minimizes%20the%20Frobenius%20norm%20over%20a%0Asmall%20image%20batch%20to%20reposition%20weight%20distributions%20for%20improved%20quantization.%0AWe%20then%20propose%20a%20Quantization-Aware%20Training%20%28QAT%29%20pipeline%20that%20applies%0Aclipping%20to%20suppress%20outliers%20and%20allows%20the%20network%20to%20adapt%20to%20quantization%0Athresholds%20during%20training.%20Our%20comprehensive%20experiments%20demonstrate%20that%0AQ-SAM2%20allows%20for%20highly%20accurate%20inference%20while%20substantially%20improving%0Aefficiency.%20Both%20quantitative%20and%20visual%20results%20show%20that%20our%20Q-SAM2%20surpasses%0Aexisting%20state-of-the-art%20general%20quantization%20schemes%2C%20especially%20for%0Aultra-low%202-bit%20quantization.%20While%20designed%20for%20quantization-aware%20training%2C%0Aour%20proposed%20calibration%20technique%20also%20proves%20effective%20in%20post-training%0Aquantization%2C%20achieving%20up%20to%20a%2066%25%20mIoU%20accuracy%20improvement%20over%0Anon-calibrated%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQ-SAM2%253A%2520Accurate%2520Quantization%2520for%2520Segment%2520Anything%2520Model%25202%26entry.906535625%3DNicola%2520Farronato%2520and%2520Florian%2520Scheidegger%2520and%2520Mattia%2520Rigotti%2520and%2520Cristiano%2520Malossi%2520and%2520Michele%2520Magno%2520and%2520Haotong%2520Qin%26entry.1292438233%3D%2520%2520The%2520Segment%2520Anything%2520Model%25202%2520%2528SAM2%2529%2520has%2520gained%2520significant%2520attention%2520as%2520a%250Afoundational%2520approach%2520for%2520promptable%2520image%2520and%2520video%2520segmentation.%2520However%252C%2520its%250Aexpensive%2520computational%2520and%2520memory%2520consumption%2520poses%2520a%2520severe%2520challenge%2520for%2520its%250Aapplication%2520in%2520resource-constrained%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%250Aaccurate%2520low-bit%2520quantization%2520method%2520for%2520efficient%2520SAM2%252C%2520termed%2520Q-SAM2.%2520To%250Aaddress%2520the%2520performance%2520degradation%2520caused%2520by%2520the%2520singularities%2520in%2520weight%2520and%250Aactivation%2520distributions%2520during%2520quantization%252C%2520Q-SAM2%2520introduces%2520two%2520novel%250Atechnical%2520contributions.%2520We%2520first%2520introduce%2520a%2520linear%2520layer%2520calibration%2520method%250Afor%2520low-bit%2520initialization%2520of%2520SAM2%252C%2520which%2520minimizes%2520the%2520Frobenius%2520norm%2520over%2520a%250Asmall%2520image%2520batch%2520to%2520reposition%2520weight%2520distributions%2520for%2520improved%2520quantization.%250AWe%2520then%2520propose%2520a%2520Quantization-Aware%2520Training%2520%2528QAT%2529%2520pipeline%2520that%2520applies%250Aclipping%2520to%2520suppress%2520outliers%2520and%2520allows%2520the%2520network%2520to%2520adapt%2520to%2520quantization%250Athresholds%2520during%2520training.%2520Our%2520comprehensive%2520experiments%2520demonstrate%2520that%250AQ-SAM2%2520allows%2520for%2520highly%2520accurate%2520inference%2520while%2520substantially%2520improving%250Aefficiency.%2520Both%2520quantitative%2520and%2520visual%2520results%2520show%2520that%2520our%2520Q-SAM2%2520surpasses%250Aexisting%2520state-of-the-art%2520general%2520quantization%2520schemes%252C%2520especially%2520for%250Aultra-low%25202-bit%2520quantization.%2520While%2520designed%2520for%2520quantization-aware%2520training%252C%250Aour%2520proposed%2520calibration%2520technique%2520also%2520proves%2520effective%2520in%2520post-training%250Aquantization%252C%2520achieving%2520up%2520to%2520a%252066%2525%2520mIoU%2520accuracy%2520improvement%2520over%250Anon-calibrated%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q-SAM2%3A%20Accurate%20Quantization%20for%20Segment%20Anything%20Model%202&entry.906535625=Nicola%20Farronato%20and%20Florian%20Scheidegger%20and%20Mattia%20Rigotti%20and%20Cristiano%20Malossi%20and%20Michele%20Magno%20and%20Haotong%20Qin&entry.1292438233=%20%20The%20Segment%20Anything%20Model%202%20%28SAM2%29%20has%20gained%20significant%20attention%20as%20a%0Afoundational%20approach%20for%20promptable%20image%20and%20video%20segmentation.%20However%2C%20its%0Aexpensive%20computational%20and%20memory%20consumption%20poses%20a%20severe%20challenge%20for%20its%0Aapplication%20in%20resource-constrained%20scenarios.%20In%20this%20paper%2C%20we%20propose%20an%0Aaccurate%20low-bit%20quantization%20method%20for%20efficient%20SAM2%2C%20termed%20Q-SAM2.%20To%0Aaddress%20the%20performance%20degradation%20caused%20by%20the%20singularities%20in%20weight%20and%0Aactivation%20distributions%20during%20quantization%2C%20Q-SAM2%20introduces%20two%20novel%0Atechnical%20contributions.%20We%20first%20introduce%20a%20linear%20layer%20calibration%20method%0Afor%20low-bit%20initialization%20of%20SAM2%2C%20which%20minimizes%20the%20Frobenius%20norm%20over%20a%0Asmall%20image%20batch%20to%20reposition%20weight%20distributions%20for%20improved%20quantization.%0AWe%20then%20propose%20a%20Quantization-Aware%20Training%20%28QAT%29%20pipeline%20that%20applies%0Aclipping%20to%20suppress%20outliers%20and%20allows%20the%20network%20to%20adapt%20to%20quantization%0Athresholds%20during%20training.%20Our%20comprehensive%20experiments%20demonstrate%20that%0AQ-SAM2%20allows%20for%20highly%20accurate%20inference%20while%20substantially%20improving%0Aefficiency.%20Both%20quantitative%20and%20visual%20results%20show%20that%20our%20Q-SAM2%20surpasses%0Aexisting%20state-of-the-art%20general%20quantization%20schemes%2C%20especially%20for%0Aultra-low%202-bit%20quantization.%20While%20designed%20for%20quantization-aware%20training%2C%0Aour%20proposed%20calibration%20technique%20also%20proves%20effective%20in%20post-training%0Aquantization%2C%20achieving%20up%20to%20a%2066%25%20mIoU%20accuracy%20improvement%20over%0Anon-calibrated%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09782v1&entry.124074799=Read"},
{"title": "Let's Fuse Step by Step: A Generative Fusion Decoding Algorithm with\n  LLMs for Robust and Instruction-Aware ASR and OCR", "author": "Chan-Jan Hsu and Yi-Chang Chen and Feng-Ting Liao and Pei-Chen Ho and Yu-Hsiang Wang and Po-Chun Hsu and Da-shan Shiu", "abstract": "  We propose \"Generative Fusion Decoding\" (GFD), a novel shallow fusion\nframework designed to integrate large language models (LLMs) into cross-modal\ntext recognition systems for automatic speech recognition (ASR) and optical\ncharacter recognition (OCR). We derive the necessary formulations to enable GFD\nto operate across mismatched token spaces of different models by calculating\nlikelihood at the byte level, thereby enabling seamless fusion and synchronous\nprogression during the decoding process. GFD is plug-and-play by design, making\nit readily compatible with various auto-regressive models without the need for\nany re-training. GFD proves effective for general ASR and OCR tasks through\nintermediate and frequent interactions with LLMs, surpassing cascaded methods\nin English and Mandarin benchmarks. In addition, GFD transfers in-context\nlearning abilities of LLMs and allows for adaptive ASR in instruction-aware and\nlong-context settings, yielding significant WER reductions of up to 17.7\\%.\n", "link": "http://arxiv.org/abs/2405.14259v4", "date": "2025-06-11", "relevancy": 2.0953, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5561}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%27s%20Fuse%20Step%20by%20Step%3A%20A%20Generative%20Fusion%20Decoding%20Algorithm%20with%0A%20%20LLMs%20for%20Robust%20and%20Instruction-Aware%20ASR%20and%20OCR&body=Title%3A%20Let%27s%20Fuse%20Step%20by%20Step%3A%20A%20Generative%20Fusion%20Decoding%20Algorithm%20with%0A%20%20LLMs%20for%20Robust%20and%20Instruction-Aware%20ASR%20and%20OCR%0AAuthor%3A%20Chan-Jan%20Hsu%20and%20Yi-Chang%20Chen%20and%20Feng-Ting%20Liao%20and%20Pei-Chen%20Ho%20and%20Yu-Hsiang%20Wang%20and%20Po-Chun%20Hsu%20and%20Da-shan%20Shiu%0AAbstract%3A%20%20%20We%20propose%20%22Generative%20Fusion%20Decoding%22%20%28GFD%29%2C%20a%20novel%20shallow%20fusion%0Aframework%20designed%20to%20integrate%20large%20language%20models%20%28LLMs%29%20into%20cross-modal%0Atext%20recognition%20systems%20for%20automatic%20speech%20recognition%20%28ASR%29%20and%20optical%0Acharacter%20recognition%20%28OCR%29.%20We%20derive%20the%20necessary%20formulations%20to%20enable%20GFD%0Ato%20operate%20across%20mismatched%20token%20spaces%20of%20different%20models%20by%20calculating%0Alikelihood%20at%20the%20byte%20level%2C%20thereby%20enabling%20seamless%20fusion%20and%20synchronous%0Aprogression%20during%20the%20decoding%20process.%20GFD%20is%20plug-and-play%20by%20design%2C%20making%0Ait%20readily%20compatible%20with%20various%20auto-regressive%20models%20without%20the%20need%20for%0Aany%20re-training.%20GFD%20proves%20effective%20for%20general%20ASR%20and%20OCR%20tasks%20through%0Aintermediate%20and%20frequent%20interactions%20with%20LLMs%2C%20surpassing%20cascaded%20methods%0Ain%20English%20and%20Mandarin%20benchmarks.%20In%20addition%2C%20GFD%20transfers%20in-context%0Alearning%20abilities%20of%20LLMs%20and%20allows%20for%20adaptive%20ASR%20in%20instruction-aware%20and%0Along-context%20settings%2C%20yielding%20significant%20WER%20reductions%20of%20up%20to%2017.7%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14259v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2527s%2520Fuse%2520Step%2520by%2520Step%253A%2520A%2520Generative%2520Fusion%2520Decoding%2520Algorithm%2520with%250A%2520%2520LLMs%2520for%2520Robust%2520and%2520Instruction-Aware%2520ASR%2520and%2520OCR%26entry.906535625%3DChan-Jan%2520Hsu%2520and%2520Yi-Chang%2520Chen%2520and%2520Feng-Ting%2520Liao%2520and%2520Pei-Chen%2520Ho%2520and%2520Yu-Hsiang%2520Wang%2520and%2520Po-Chun%2520Hsu%2520and%2520Da-shan%2520Shiu%26entry.1292438233%3D%2520%2520We%2520propose%2520%2522Generative%2520Fusion%2520Decoding%2522%2520%2528GFD%2529%252C%2520a%2520novel%2520shallow%2520fusion%250Aframework%2520designed%2520to%2520integrate%2520large%2520language%2520models%2520%2528LLMs%2529%2520into%2520cross-modal%250Atext%2520recognition%2520systems%2520for%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%2520and%2520optical%250Acharacter%2520recognition%2520%2528OCR%2529.%2520We%2520derive%2520the%2520necessary%2520formulations%2520to%2520enable%2520GFD%250Ato%2520operate%2520across%2520mismatched%2520token%2520spaces%2520of%2520different%2520models%2520by%2520calculating%250Alikelihood%2520at%2520the%2520byte%2520level%252C%2520thereby%2520enabling%2520seamless%2520fusion%2520and%2520synchronous%250Aprogression%2520during%2520the%2520decoding%2520process.%2520GFD%2520is%2520plug-and-play%2520by%2520design%252C%2520making%250Ait%2520readily%2520compatible%2520with%2520various%2520auto-regressive%2520models%2520without%2520the%2520need%2520for%250Aany%2520re-training.%2520GFD%2520proves%2520effective%2520for%2520general%2520ASR%2520and%2520OCR%2520tasks%2520through%250Aintermediate%2520and%2520frequent%2520interactions%2520with%2520LLMs%252C%2520surpassing%2520cascaded%2520methods%250Ain%2520English%2520and%2520Mandarin%2520benchmarks.%2520In%2520addition%252C%2520GFD%2520transfers%2520in-context%250Alearning%2520abilities%2520of%2520LLMs%2520and%2520allows%2520for%2520adaptive%2520ASR%2520in%2520instruction-aware%2520and%250Along-context%2520settings%252C%2520yielding%2520significant%2520WER%2520reductions%2520of%2520up%2520to%252017.7%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14259v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%27s%20Fuse%20Step%20by%20Step%3A%20A%20Generative%20Fusion%20Decoding%20Algorithm%20with%0A%20%20LLMs%20for%20Robust%20and%20Instruction-Aware%20ASR%20and%20OCR&entry.906535625=Chan-Jan%20Hsu%20and%20Yi-Chang%20Chen%20and%20Feng-Ting%20Liao%20and%20Pei-Chen%20Ho%20and%20Yu-Hsiang%20Wang%20and%20Po-Chun%20Hsu%20and%20Da-shan%20Shiu&entry.1292438233=%20%20We%20propose%20%22Generative%20Fusion%20Decoding%22%20%28GFD%29%2C%20a%20novel%20shallow%20fusion%0Aframework%20designed%20to%20integrate%20large%20language%20models%20%28LLMs%29%20into%20cross-modal%0Atext%20recognition%20systems%20for%20automatic%20speech%20recognition%20%28ASR%29%20and%20optical%0Acharacter%20recognition%20%28OCR%29.%20We%20derive%20the%20necessary%20formulations%20to%20enable%20GFD%0Ato%20operate%20across%20mismatched%20token%20spaces%20of%20different%20models%20by%20calculating%0Alikelihood%20at%20the%20byte%20level%2C%20thereby%20enabling%20seamless%20fusion%20and%20synchronous%0Aprogression%20during%20the%20decoding%20process.%20GFD%20is%20plug-and-play%20by%20design%2C%20making%0Ait%20readily%20compatible%20with%20various%20auto-regressive%20models%20without%20the%20need%20for%0Aany%20re-training.%20GFD%20proves%20effective%20for%20general%20ASR%20and%20OCR%20tasks%20through%0Aintermediate%20and%20frequent%20interactions%20with%20LLMs%2C%20surpassing%20cascaded%20methods%0Ain%20English%20and%20Mandarin%20benchmarks.%20In%20addition%2C%20GFD%20transfers%20in-context%0Alearning%20abilities%20of%20LLMs%20and%20allows%20for%20adaptive%20ASR%20in%20instruction-aware%20and%0Along-context%20settings%2C%20yielding%20significant%20WER%20reductions%20of%20up%20to%2017.7%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14259v4&entry.124074799=Read"},
{"title": "CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video\n  Models", "author": "Aaron Foss and Chloe Evans and Sasha Mitts and Koustuv Sinha and Ammar Rizvi and Justine T. Kao", "abstract": "  We introduce CausalVQA, a benchmark dataset for video question answering\n(VQA) composed of question-answer pairs that probe models' understanding of\ncausality in the physical world. Existing VQA benchmarks either tend to focus\non surface perceptual understanding of real-world videos, or on narrow physical\nreasoning questions created using simulation environments. CausalVQA fills an\nimportant gap by presenting challenging questions that are grounded in\nreal-world scenarios, while focusing on models' ability to predict the likely\noutcomes of different actions and events through five question types:\ncounterfactual, hypothetical, anticipation, planning and descriptive. We\ndesigned quality control mechanisms that prevent models from exploiting trivial\nshortcuts, requiring models to base their answers on deep visual understanding\ninstead of linguistic cues. We find that current frontier multimodal models\nfall substantially below human performance on the benchmark, especially on\nanticipation and hypothetical questions. This highlights a challenge for\ncurrent systems to leverage spatial-temporal reasoning, understanding of\nphysical principles, and comprehension of possible alternatives to make\naccurate predictions in real-world settings.\n", "link": "http://arxiv.org/abs/2506.09943v1", "date": "2025-06-11", "relevancy": 2.0895, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5283}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5212}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CausalVQA%3A%20A%20Physically%20Grounded%20Causal%20Reasoning%20Benchmark%20for%20Video%0A%20%20Models&body=Title%3A%20CausalVQA%3A%20A%20Physically%20Grounded%20Causal%20Reasoning%20Benchmark%20for%20Video%0A%20%20Models%0AAuthor%3A%20Aaron%20Foss%20and%20Chloe%20Evans%20and%20Sasha%20Mitts%20and%20Koustuv%20Sinha%20and%20Ammar%20Rizvi%20and%20Justine%20T.%20Kao%0AAbstract%3A%20%20%20We%20introduce%20CausalVQA%2C%20a%20benchmark%20dataset%20for%20video%20question%20answering%0A%28VQA%29%20composed%20of%20question-answer%20pairs%20that%20probe%20models%27%20understanding%20of%0Acausality%20in%20the%20physical%20world.%20Existing%20VQA%20benchmarks%20either%20tend%20to%20focus%0Aon%20surface%20perceptual%20understanding%20of%20real-world%20videos%2C%20or%20on%20narrow%20physical%0Areasoning%20questions%20created%20using%20simulation%20environments.%20CausalVQA%20fills%20an%0Aimportant%20gap%20by%20presenting%20challenging%20questions%20that%20are%20grounded%20in%0Areal-world%20scenarios%2C%20while%20focusing%20on%20models%27%20ability%20to%20predict%20the%20likely%0Aoutcomes%20of%20different%20actions%20and%20events%20through%20five%20question%20types%3A%0Acounterfactual%2C%20hypothetical%2C%20anticipation%2C%20planning%20and%20descriptive.%20We%0Adesigned%20quality%20control%20mechanisms%20that%20prevent%20models%20from%20exploiting%20trivial%0Ashortcuts%2C%20requiring%20models%20to%20base%20their%20answers%20on%20deep%20visual%20understanding%0Ainstead%20of%20linguistic%20cues.%20We%20find%20that%20current%20frontier%20multimodal%20models%0Afall%20substantially%20below%20human%20performance%20on%20the%20benchmark%2C%20especially%20on%0Aanticipation%20and%20hypothetical%20questions.%20This%20highlights%20a%20challenge%20for%0Acurrent%20systems%20to%20leverage%20spatial-temporal%20reasoning%2C%20understanding%20of%0Aphysical%20principles%2C%20and%20comprehension%20of%20possible%20alternatives%20to%20make%0Aaccurate%20predictions%20in%20real-world%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausalVQA%253A%2520A%2520Physically%2520Grounded%2520Causal%2520Reasoning%2520Benchmark%2520for%2520Video%250A%2520%2520Models%26entry.906535625%3DAaron%2520Foss%2520and%2520Chloe%2520Evans%2520and%2520Sasha%2520Mitts%2520and%2520Koustuv%2520Sinha%2520and%2520Ammar%2520Rizvi%2520and%2520Justine%2520T.%2520Kao%26entry.1292438233%3D%2520%2520We%2520introduce%2520CausalVQA%252C%2520a%2520benchmark%2520dataset%2520for%2520video%2520question%2520answering%250A%2528VQA%2529%2520composed%2520of%2520question-answer%2520pairs%2520that%2520probe%2520models%2527%2520understanding%2520of%250Acausality%2520in%2520the%2520physical%2520world.%2520Existing%2520VQA%2520benchmarks%2520either%2520tend%2520to%2520focus%250Aon%2520surface%2520perceptual%2520understanding%2520of%2520real-world%2520videos%252C%2520or%2520on%2520narrow%2520physical%250Areasoning%2520questions%2520created%2520using%2520simulation%2520environments.%2520CausalVQA%2520fills%2520an%250Aimportant%2520gap%2520by%2520presenting%2520challenging%2520questions%2520that%2520are%2520grounded%2520in%250Areal-world%2520scenarios%252C%2520while%2520focusing%2520on%2520models%2527%2520ability%2520to%2520predict%2520the%2520likely%250Aoutcomes%2520of%2520different%2520actions%2520and%2520events%2520through%2520five%2520question%2520types%253A%250Acounterfactual%252C%2520hypothetical%252C%2520anticipation%252C%2520planning%2520and%2520descriptive.%2520We%250Adesigned%2520quality%2520control%2520mechanisms%2520that%2520prevent%2520models%2520from%2520exploiting%2520trivial%250Ashortcuts%252C%2520requiring%2520models%2520to%2520base%2520their%2520answers%2520on%2520deep%2520visual%2520understanding%250Ainstead%2520of%2520linguistic%2520cues.%2520We%2520find%2520that%2520current%2520frontier%2520multimodal%2520models%250Afall%2520substantially%2520below%2520human%2520performance%2520on%2520the%2520benchmark%252C%2520especially%2520on%250Aanticipation%2520and%2520hypothetical%2520questions.%2520This%2520highlights%2520a%2520challenge%2520for%250Acurrent%2520systems%2520to%2520leverage%2520spatial-temporal%2520reasoning%252C%2520understanding%2520of%250Aphysical%2520principles%252C%2520and%2520comprehension%2520of%2520possible%2520alternatives%2520to%2520make%250Aaccurate%2520predictions%2520in%2520real-world%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CausalVQA%3A%20A%20Physically%20Grounded%20Causal%20Reasoning%20Benchmark%20for%20Video%0A%20%20Models&entry.906535625=Aaron%20Foss%20and%20Chloe%20Evans%20and%20Sasha%20Mitts%20and%20Koustuv%20Sinha%20and%20Ammar%20Rizvi%20and%20Justine%20T.%20Kao&entry.1292438233=%20%20We%20introduce%20CausalVQA%2C%20a%20benchmark%20dataset%20for%20video%20question%20answering%0A%28VQA%29%20composed%20of%20question-answer%20pairs%20that%20probe%20models%27%20understanding%20of%0Acausality%20in%20the%20physical%20world.%20Existing%20VQA%20benchmarks%20either%20tend%20to%20focus%0Aon%20surface%20perceptual%20understanding%20of%20real-world%20videos%2C%20or%20on%20narrow%20physical%0Areasoning%20questions%20created%20using%20simulation%20environments.%20CausalVQA%20fills%20an%0Aimportant%20gap%20by%20presenting%20challenging%20questions%20that%20are%20grounded%20in%0Areal-world%20scenarios%2C%20while%20focusing%20on%20models%27%20ability%20to%20predict%20the%20likely%0Aoutcomes%20of%20different%20actions%20and%20events%20through%20five%20question%20types%3A%0Acounterfactual%2C%20hypothetical%2C%20anticipation%2C%20planning%20and%20descriptive.%20We%0Adesigned%20quality%20control%20mechanisms%20that%20prevent%20models%20from%20exploiting%20trivial%0Ashortcuts%2C%20requiring%20models%20to%20base%20their%20answers%20on%20deep%20visual%20understanding%0Ainstead%20of%20linguistic%20cues.%20We%20find%20that%20current%20frontier%20multimodal%20models%0Afall%20substantially%20below%20human%20performance%20on%20the%20benchmark%2C%20especially%20on%0Aanticipation%20and%20hypothetical%20questions.%20This%20highlights%20a%20challenge%20for%0Acurrent%20systems%20to%20leverage%20spatial-temporal%20reasoning%2C%20understanding%20of%0Aphysical%20principles%2C%20and%20comprehension%20of%20possible%20alternatives%20to%20make%0Aaccurate%20predictions%20in%20real-world%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09943v1&entry.124074799=Read"},
{"title": "Devil's Hand: Data Poisoning Attacks to Locally Private Graph Learning\n  Protocols", "author": "Longzhu He and Chaozhuo Li and Peng Tang and Litian Zhang and Sen Su", "abstract": "  Graph neural networks (GNNs) have achieved significant success in graph\nrepresentation learning and have been applied to various domains. However, many\nreal-world graphs contain sensitive personal information, such as user profiles\nin social networks, raising serious privacy concerns when graph learning is\nperformed using GNNs. To address this issue, locally private graph learning\nprotocols have gained considerable attention. These protocols leverage the\nprivacy advantages of local differential privacy (LDP) and the effectiveness of\nGNN's message-passing in calibrating noisy data, offering strict privacy\nguarantees for users' local data while maintaining high utility (e.g., node\nclassification accuracy) for graph learning. Despite these advantages, such\nprotocols may be vulnerable to data poisoning attacks, a threat that has not\nbeen considered in previous research. Identifying and addressing these threats\nis crucial for ensuring the robustness and security of privacy-preserving graph\nlearning frameworks. This work introduces the first data poisoning attack\ntargeting locally private graph learning protocols. The attacker injects fake\nusers into the protocol, manipulates these fake users to establish links with\ngenuine users, and sends carefully crafted data to the server, ultimately\ncompromising the utility of private graph learning. The effectiveness of the\nattack is demonstrated both theoretically and empirically. In addition, several\ndefense strategies have also been explored, but their limited effectiveness\nhighlights the need for more robust defenses.\n", "link": "http://arxiv.org/abs/2506.09803v1", "date": "2025-06-11", "relevancy": 2.0677, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.425}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4144}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Devil%27s%20Hand%3A%20Data%20Poisoning%20Attacks%20to%20Locally%20Private%20Graph%20Learning%0A%20%20Protocols&body=Title%3A%20Devil%27s%20Hand%3A%20Data%20Poisoning%20Attacks%20to%20Locally%20Private%20Graph%20Learning%0A%20%20Protocols%0AAuthor%3A%20Longzhu%20He%20and%20Chaozhuo%20Li%20and%20Peng%20Tang%20and%20Litian%20Zhang%20and%20Sen%20Su%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20achieved%20significant%20success%20in%20graph%0Arepresentation%20learning%20and%20have%20been%20applied%20to%20various%20domains.%20However%2C%20many%0Areal-world%20graphs%20contain%20sensitive%20personal%20information%2C%20such%20as%20user%20profiles%0Ain%20social%20networks%2C%20raising%20serious%20privacy%20concerns%20when%20graph%20learning%20is%0Aperformed%20using%20GNNs.%20To%20address%20this%20issue%2C%20locally%20private%20graph%20learning%0Aprotocols%20have%20gained%20considerable%20attention.%20These%20protocols%20leverage%20the%0Aprivacy%20advantages%20of%20local%20differential%20privacy%20%28LDP%29%20and%20the%20effectiveness%20of%0AGNN%27s%20message-passing%20in%20calibrating%20noisy%20data%2C%20offering%20strict%20privacy%0Aguarantees%20for%20users%27%20local%20data%20while%20maintaining%20high%20utility%20%28e.g.%2C%20node%0Aclassification%20accuracy%29%20for%20graph%20learning.%20Despite%20these%20advantages%2C%20such%0Aprotocols%20may%20be%20vulnerable%20to%20data%20poisoning%20attacks%2C%20a%20threat%20that%20has%20not%0Abeen%20considered%20in%20previous%20research.%20Identifying%20and%20addressing%20these%20threats%0Ais%20crucial%20for%20ensuring%20the%20robustness%20and%20security%20of%20privacy-preserving%20graph%0Alearning%20frameworks.%20This%20work%20introduces%20the%20first%20data%20poisoning%20attack%0Atargeting%20locally%20private%20graph%20learning%20protocols.%20The%20attacker%20injects%20fake%0Ausers%20into%20the%20protocol%2C%20manipulates%20these%20fake%20users%20to%20establish%20links%20with%0Agenuine%20users%2C%20and%20sends%20carefully%20crafted%20data%20to%20the%20server%2C%20ultimately%0Acompromising%20the%20utility%20of%20private%20graph%20learning.%20The%20effectiveness%20of%20the%0Aattack%20is%20demonstrated%20both%20theoretically%20and%20empirically.%20In%20addition%2C%20several%0Adefense%20strategies%20have%20also%20been%20explored%2C%20but%20their%20limited%20effectiveness%0Ahighlights%20the%20need%20for%20more%20robust%20defenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDevil%2527s%2520Hand%253A%2520Data%2520Poisoning%2520Attacks%2520to%2520Locally%2520Private%2520Graph%2520Learning%250A%2520%2520Protocols%26entry.906535625%3DLongzhu%2520He%2520and%2520Chaozhuo%2520Li%2520and%2520Peng%2520Tang%2520and%2520Litian%2520Zhang%2520and%2520Sen%2520Su%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520achieved%2520significant%2520success%2520in%2520graph%250Arepresentation%2520learning%2520and%2520have%2520been%2520applied%2520to%2520various%2520domains.%2520However%252C%2520many%250Areal-world%2520graphs%2520contain%2520sensitive%2520personal%2520information%252C%2520such%2520as%2520user%2520profiles%250Ain%2520social%2520networks%252C%2520raising%2520serious%2520privacy%2520concerns%2520when%2520graph%2520learning%2520is%250Aperformed%2520using%2520GNNs.%2520To%2520address%2520this%2520issue%252C%2520locally%2520private%2520graph%2520learning%250Aprotocols%2520have%2520gained%2520considerable%2520attention.%2520These%2520protocols%2520leverage%2520the%250Aprivacy%2520advantages%2520of%2520local%2520differential%2520privacy%2520%2528LDP%2529%2520and%2520the%2520effectiveness%2520of%250AGNN%2527s%2520message-passing%2520in%2520calibrating%2520noisy%2520data%252C%2520offering%2520strict%2520privacy%250Aguarantees%2520for%2520users%2527%2520local%2520data%2520while%2520maintaining%2520high%2520utility%2520%2528e.g.%252C%2520node%250Aclassification%2520accuracy%2529%2520for%2520graph%2520learning.%2520Despite%2520these%2520advantages%252C%2520such%250Aprotocols%2520may%2520be%2520vulnerable%2520to%2520data%2520poisoning%2520attacks%252C%2520a%2520threat%2520that%2520has%2520not%250Abeen%2520considered%2520in%2520previous%2520research.%2520Identifying%2520and%2520addressing%2520these%2520threats%250Ais%2520crucial%2520for%2520ensuring%2520the%2520robustness%2520and%2520security%2520of%2520privacy-preserving%2520graph%250Alearning%2520frameworks.%2520This%2520work%2520introduces%2520the%2520first%2520data%2520poisoning%2520attack%250Atargeting%2520locally%2520private%2520graph%2520learning%2520protocols.%2520The%2520attacker%2520injects%2520fake%250Ausers%2520into%2520the%2520protocol%252C%2520manipulates%2520these%2520fake%2520users%2520to%2520establish%2520links%2520with%250Agenuine%2520users%252C%2520and%2520sends%2520carefully%2520crafted%2520data%2520to%2520the%2520server%252C%2520ultimately%250Acompromising%2520the%2520utility%2520of%2520private%2520graph%2520learning.%2520The%2520effectiveness%2520of%2520the%250Aattack%2520is%2520demonstrated%2520both%2520theoretically%2520and%2520empirically.%2520In%2520addition%252C%2520several%250Adefense%2520strategies%2520have%2520also%2520been%2520explored%252C%2520but%2520their%2520limited%2520effectiveness%250Ahighlights%2520the%2520need%2520for%2520more%2520robust%2520defenses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Devil%27s%20Hand%3A%20Data%20Poisoning%20Attacks%20to%20Locally%20Private%20Graph%20Learning%0A%20%20Protocols&entry.906535625=Longzhu%20He%20and%20Chaozhuo%20Li%20and%20Peng%20Tang%20and%20Litian%20Zhang%20and%20Sen%20Su&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20achieved%20significant%20success%20in%20graph%0Arepresentation%20learning%20and%20have%20been%20applied%20to%20various%20domains.%20However%2C%20many%0Areal-world%20graphs%20contain%20sensitive%20personal%20information%2C%20such%20as%20user%20profiles%0Ain%20social%20networks%2C%20raising%20serious%20privacy%20concerns%20when%20graph%20learning%20is%0Aperformed%20using%20GNNs.%20To%20address%20this%20issue%2C%20locally%20private%20graph%20learning%0Aprotocols%20have%20gained%20considerable%20attention.%20These%20protocols%20leverage%20the%0Aprivacy%20advantages%20of%20local%20differential%20privacy%20%28LDP%29%20and%20the%20effectiveness%20of%0AGNN%27s%20message-passing%20in%20calibrating%20noisy%20data%2C%20offering%20strict%20privacy%0Aguarantees%20for%20users%27%20local%20data%20while%20maintaining%20high%20utility%20%28e.g.%2C%20node%0Aclassification%20accuracy%29%20for%20graph%20learning.%20Despite%20these%20advantages%2C%20such%0Aprotocols%20may%20be%20vulnerable%20to%20data%20poisoning%20attacks%2C%20a%20threat%20that%20has%20not%0Abeen%20considered%20in%20previous%20research.%20Identifying%20and%20addressing%20these%20threats%0Ais%20crucial%20for%20ensuring%20the%20robustness%20and%20security%20of%20privacy-preserving%20graph%0Alearning%20frameworks.%20This%20work%20introduces%20the%20first%20data%20poisoning%20attack%0Atargeting%20locally%20private%20graph%20learning%20protocols.%20The%20attacker%20injects%20fake%0Ausers%20into%20the%20protocol%2C%20manipulates%20these%20fake%20users%20to%20establish%20links%20with%0Agenuine%20users%2C%20and%20sends%20carefully%20crafted%20data%20to%20the%20server%2C%20ultimately%0Acompromising%20the%20utility%20of%20private%20graph%20learning.%20The%20effectiveness%20of%20the%0Aattack%20is%20demonstrated%20both%20theoretically%20and%20empirically.%20In%20addition%2C%20several%0Adefense%20strategies%20have%20also%20been%20explored%2C%20but%20their%20limited%20effectiveness%0Ahighlights%20the%20need%20for%20more%20robust%20defenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09803v1&entry.124074799=Read"},
{"title": "Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized\n  Rejection Sampling", "author": "Tim Z. Xiao and Johannes Zenn and Zhen Liu and Weiyang Liu and Robert Bamler and Bernhard Sch\u00f6lkopf", "abstract": "  Large language models (LLMs) can often accurately describe probability\ndistributions using natural language, yet they still struggle to generate\nfaithful samples from them. This mismatch limits their use in tasks requiring\nreliable stochasticity, such as Monte Carlo methods, agent-based simulations,\nand randomized decision-making. We investigate this gap between knowledge and\nsampling in the context of Bernoulli distributions. We introduce Verbalized\nRejection Sampling (VRS), a natural-language adaptation of classical rejection\nsampling that prompts the LLM to reason about and accept or reject proposed\nsamples. Despite relying on the same Bernoulli mechanism internally, VRS\nsubstantially reduces sampling bias across models. We provide theoretical\nanalysis showing that, under mild assumptions, VRS improves over direct\nsampling, with gains attributable to both the algorithm and prompt design. More\nbroadly, our results show how classical probabilistic tools can be verbalized\nand embedded into LLM workflows to improve reliability, without requiring\naccess to model internals or heavy prompt engineering.\n", "link": "http://arxiv.org/abs/2506.09998v1", "date": "2025-06-11", "relevancy": 2.0636, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.521}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5159}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flipping%20Against%20All%20Odds%3A%20Reducing%20LLM%20Coin%20Flip%20Bias%20via%20Verbalized%0A%20%20Rejection%20Sampling&body=Title%3A%20Flipping%20Against%20All%20Odds%3A%20Reducing%20LLM%20Coin%20Flip%20Bias%20via%20Verbalized%0A%20%20Rejection%20Sampling%0AAuthor%3A%20Tim%20Z.%20Xiao%20and%20Johannes%20Zenn%20and%20Zhen%20Liu%20and%20Weiyang%20Liu%20and%20Robert%20Bamler%20and%20Bernhard%20Sch%C3%B6lkopf%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20can%20often%20accurately%20describe%20probability%0Adistributions%20using%20natural%20language%2C%20yet%20they%20still%20struggle%20to%20generate%0Afaithful%20samples%20from%20them.%20This%20mismatch%20limits%20their%20use%20in%20tasks%20requiring%0Areliable%20stochasticity%2C%20such%20as%20Monte%20Carlo%20methods%2C%20agent-based%20simulations%2C%0Aand%20randomized%20decision-making.%20We%20investigate%20this%20gap%20between%20knowledge%20and%0Asampling%20in%20the%20context%20of%20Bernoulli%20distributions.%20We%20introduce%20Verbalized%0ARejection%20Sampling%20%28VRS%29%2C%20a%20natural-language%20adaptation%20of%20classical%20rejection%0Asampling%20that%20prompts%20the%20LLM%20to%20reason%20about%20and%20accept%20or%20reject%20proposed%0Asamples.%20Despite%20relying%20on%20the%20same%20Bernoulli%20mechanism%20internally%2C%20VRS%0Asubstantially%20reduces%20sampling%20bias%20across%20models.%20We%20provide%20theoretical%0Aanalysis%20showing%20that%2C%20under%20mild%20assumptions%2C%20VRS%20improves%20over%20direct%0Asampling%2C%20with%20gains%20attributable%20to%20both%20the%20algorithm%20and%20prompt%20design.%20More%0Abroadly%2C%20our%20results%20show%20how%20classical%20probabilistic%20tools%20can%20be%20verbalized%0Aand%20embedded%20into%20LLM%20workflows%20to%20improve%20reliability%2C%20without%20requiring%0Aaccess%20to%20model%20internals%20or%20heavy%20prompt%20engineering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09998v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlipping%2520Against%2520All%2520Odds%253A%2520Reducing%2520LLM%2520Coin%2520Flip%2520Bias%2520via%2520Verbalized%250A%2520%2520Rejection%2520Sampling%26entry.906535625%3DTim%2520Z.%2520Xiao%2520and%2520Johannes%2520Zenn%2520and%2520Zhen%2520Liu%2520and%2520Weiyang%2520Liu%2520and%2520Robert%2520Bamler%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520can%2520often%2520accurately%2520describe%2520probability%250Adistributions%2520using%2520natural%2520language%252C%2520yet%2520they%2520still%2520struggle%2520to%2520generate%250Afaithful%2520samples%2520from%2520them.%2520This%2520mismatch%2520limits%2520their%2520use%2520in%2520tasks%2520requiring%250Areliable%2520stochasticity%252C%2520such%2520as%2520Monte%2520Carlo%2520methods%252C%2520agent-based%2520simulations%252C%250Aand%2520randomized%2520decision-making.%2520We%2520investigate%2520this%2520gap%2520between%2520knowledge%2520and%250Asampling%2520in%2520the%2520context%2520of%2520Bernoulli%2520distributions.%2520We%2520introduce%2520Verbalized%250ARejection%2520Sampling%2520%2528VRS%2529%252C%2520a%2520natural-language%2520adaptation%2520of%2520classical%2520rejection%250Asampling%2520that%2520prompts%2520the%2520LLM%2520to%2520reason%2520about%2520and%2520accept%2520or%2520reject%2520proposed%250Asamples.%2520Despite%2520relying%2520on%2520the%2520same%2520Bernoulli%2520mechanism%2520internally%252C%2520VRS%250Asubstantially%2520reduces%2520sampling%2520bias%2520across%2520models.%2520We%2520provide%2520theoretical%250Aanalysis%2520showing%2520that%252C%2520under%2520mild%2520assumptions%252C%2520VRS%2520improves%2520over%2520direct%250Asampling%252C%2520with%2520gains%2520attributable%2520to%2520both%2520the%2520algorithm%2520and%2520prompt%2520design.%2520More%250Abroadly%252C%2520our%2520results%2520show%2520how%2520classical%2520probabilistic%2520tools%2520can%2520be%2520verbalized%250Aand%2520embedded%2520into%2520LLM%2520workflows%2520to%2520improve%2520reliability%252C%2520without%2520requiring%250Aaccess%2520to%2520model%2520internals%2520or%2520heavy%2520prompt%2520engineering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09998v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flipping%20Against%20All%20Odds%3A%20Reducing%20LLM%20Coin%20Flip%20Bias%20via%20Verbalized%0A%20%20Rejection%20Sampling&entry.906535625=Tim%20Z.%20Xiao%20and%20Johannes%20Zenn%20and%20Zhen%20Liu%20and%20Weiyang%20Liu%20and%20Robert%20Bamler%20and%20Bernhard%20Sch%C3%B6lkopf&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20can%20often%20accurately%20describe%20probability%0Adistributions%20using%20natural%20language%2C%20yet%20they%20still%20struggle%20to%20generate%0Afaithful%20samples%20from%20them.%20This%20mismatch%20limits%20their%20use%20in%20tasks%20requiring%0Areliable%20stochasticity%2C%20such%20as%20Monte%20Carlo%20methods%2C%20agent-based%20simulations%2C%0Aand%20randomized%20decision-making.%20We%20investigate%20this%20gap%20between%20knowledge%20and%0Asampling%20in%20the%20context%20of%20Bernoulli%20distributions.%20We%20introduce%20Verbalized%0ARejection%20Sampling%20%28VRS%29%2C%20a%20natural-language%20adaptation%20of%20classical%20rejection%0Asampling%20that%20prompts%20the%20LLM%20to%20reason%20about%20and%20accept%20or%20reject%20proposed%0Asamples.%20Despite%20relying%20on%20the%20same%20Bernoulli%20mechanism%20internally%2C%20VRS%0Asubstantially%20reduces%20sampling%20bias%20across%20models.%20We%20provide%20theoretical%0Aanalysis%20showing%20that%2C%20under%20mild%20assumptions%2C%20VRS%20improves%20over%20direct%0Asampling%2C%20with%20gains%20attributable%20to%20both%20the%20algorithm%20and%20prompt%20design.%20More%0Abroadly%2C%20our%20results%20show%20how%20classical%20probabilistic%20tools%20can%20be%20verbalized%0Aand%20embedded%20into%20LLM%20workflows%20to%20improve%20reliability%2C%20without%20requiring%0Aaccess%20to%20model%20internals%20or%20heavy%20prompt%20engineering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09998v1&entry.124074799=Read"},
{"title": "Reinforced Refinement with Self-Aware Expansion for End-to-End\n  Autonomous Driving", "author": "Haochen Liu and Tianyu Li and Haohan Yang and Li Chen and Caojun Wang and Ke Guo and Haochen Tian and Hongchen Li and Hongyang Li and Chen Lv", "abstract": "  End-to-end autonomous driving has emerged as a promising paradigm for\ndirectly mapping sensor inputs to planning maneuvers using learning-based\nmodular integrations. However, existing imitation learning (IL)-based models\nsuffer from generalization to hard cases, and a lack of corrective feedback\nloop under post-deployment. While reinforcement learning (RL) offers a\npotential solution to tackle hard cases with optimality, it is often hindered\nby overfitting to specific driving cases, resulting in catastrophic forgetting\nof generalizable knowledge and sample inefficiency. To overcome these\nchallenges, we propose Reinforced Refinement with Self-aware Expansion (R2SE),\na novel learning pipeline that constantly refines hard domain while keeping\ngeneralizable driving policy for model-agnostic end-to-end driving systems.\nThrough reinforcement fine-tuning and policy expansion that facilitates\ncontinuous improvement, R2SE features three key components: 1) Generalist\nPretraining with hard-case allocation trains a generalist imitation learning\n(IL) driving system while dynamically identifying failure-prone cases for\ntargeted refinement; 2) Residual Reinforced Specialist Fine-tuning optimizes\nresidual corrections using reinforcement learning (RL) to improve performance\nin hard case domain while preserving global driving knowledge; 3) Self-aware\nAdapter Expansion dynamically integrates specialist policies back into the\ngeneralist model, enhancing continuous performance improvement. Experimental\nresults in closed-loop simulation and real-world datasets demonstrate\nimprovements in generalization, safety, and long-horizon policy robustness over\nstate-of-the-art E2E systems, highlighting the effectiveness of reinforce\nrefinement for scalable autonomous driving.\n", "link": "http://arxiv.org/abs/2506.09800v1", "date": "2025-06-11", "relevancy": 2.0582, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5253}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.514}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforced%20Refinement%20with%20Self-Aware%20Expansion%20for%20End-to-End%0A%20%20Autonomous%20Driving&body=Title%3A%20Reinforced%20Refinement%20with%20Self-Aware%20Expansion%20for%20End-to-End%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Haochen%20Liu%20and%20Tianyu%20Li%20and%20Haohan%20Yang%20and%20Li%20Chen%20and%20Caojun%20Wang%20and%20Ke%20Guo%20and%20Haochen%20Tian%20and%20Hongchen%20Li%20and%20Hongyang%20Li%20and%20Chen%20Lv%0AAbstract%3A%20%20%20End-to-end%20autonomous%20driving%20has%20emerged%20as%20a%20promising%20paradigm%20for%0Adirectly%20mapping%20sensor%20inputs%20to%20planning%20maneuvers%20using%20learning-based%0Amodular%20integrations.%20However%2C%20existing%20imitation%20learning%20%28IL%29-based%20models%0Asuffer%20from%20generalization%20to%20hard%20cases%2C%20and%20a%20lack%20of%20corrective%20feedback%0Aloop%20under%20post-deployment.%20While%20reinforcement%20learning%20%28RL%29%20offers%20a%0Apotential%20solution%20to%20tackle%20hard%20cases%20with%20optimality%2C%20it%20is%20often%20hindered%0Aby%20overfitting%20to%20specific%20driving%20cases%2C%20resulting%20in%20catastrophic%20forgetting%0Aof%20generalizable%20knowledge%20and%20sample%20inefficiency.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20Reinforced%20Refinement%20with%20Self-aware%20Expansion%20%28R2SE%29%2C%0Aa%20novel%20learning%20pipeline%20that%20constantly%20refines%20hard%20domain%20while%20keeping%0Ageneralizable%20driving%20policy%20for%20model-agnostic%20end-to-end%20driving%20systems.%0AThrough%20reinforcement%20fine-tuning%20and%20policy%20expansion%20that%20facilitates%0Acontinuous%20improvement%2C%20R2SE%20features%20three%20key%20components%3A%201%29%20Generalist%0APretraining%20with%20hard-case%20allocation%20trains%20a%20generalist%20imitation%20learning%0A%28IL%29%20driving%20system%20while%20dynamically%20identifying%20failure-prone%20cases%20for%0Atargeted%20refinement%3B%202%29%20Residual%20Reinforced%20Specialist%20Fine-tuning%20optimizes%0Aresidual%20corrections%20using%20reinforcement%20learning%20%28RL%29%20to%20improve%20performance%0Ain%20hard%20case%20domain%20while%20preserving%20global%20driving%20knowledge%3B%203%29%20Self-aware%0AAdapter%20Expansion%20dynamically%20integrates%20specialist%20policies%20back%20into%20the%0Ageneralist%20model%2C%20enhancing%20continuous%20performance%20improvement.%20Experimental%0Aresults%20in%20closed-loop%20simulation%20and%20real-world%20datasets%20demonstrate%0Aimprovements%20in%20generalization%2C%20safety%2C%20and%20long-horizon%20policy%20robustness%20over%0Astate-of-the-art%20E2E%20systems%2C%20highlighting%20the%20effectiveness%20of%20reinforce%0Arefinement%20for%20scalable%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforced%2520Refinement%2520with%2520Self-Aware%2520Expansion%2520for%2520End-to-End%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DHaochen%2520Liu%2520and%2520Tianyu%2520Li%2520and%2520Haohan%2520Yang%2520and%2520Li%2520Chen%2520and%2520Caojun%2520Wang%2520and%2520Ke%2520Guo%2520and%2520Haochen%2520Tian%2520and%2520Hongchen%2520Li%2520and%2520Hongyang%2520Li%2520and%2520Chen%2520Lv%26entry.1292438233%3D%2520%2520End-to-end%2520autonomous%2520driving%2520has%2520emerged%2520as%2520a%2520promising%2520paradigm%2520for%250Adirectly%2520mapping%2520sensor%2520inputs%2520to%2520planning%2520maneuvers%2520using%2520learning-based%250Amodular%2520integrations.%2520However%252C%2520existing%2520imitation%2520learning%2520%2528IL%2529-based%2520models%250Asuffer%2520from%2520generalization%2520to%2520hard%2520cases%252C%2520and%2520a%2520lack%2520of%2520corrective%2520feedback%250Aloop%2520under%2520post-deployment.%2520While%2520reinforcement%2520learning%2520%2528RL%2529%2520offers%2520a%250Apotential%2520solution%2520to%2520tackle%2520hard%2520cases%2520with%2520optimality%252C%2520it%2520is%2520often%2520hindered%250Aby%2520overfitting%2520to%2520specific%2520driving%2520cases%252C%2520resulting%2520in%2520catastrophic%2520forgetting%250Aof%2520generalizable%2520knowledge%2520and%2520sample%2520inefficiency.%2520To%2520overcome%2520these%250Achallenges%252C%2520we%2520propose%2520Reinforced%2520Refinement%2520with%2520Self-aware%2520Expansion%2520%2528R2SE%2529%252C%250Aa%2520novel%2520learning%2520pipeline%2520that%2520constantly%2520refines%2520hard%2520domain%2520while%2520keeping%250Ageneralizable%2520driving%2520policy%2520for%2520model-agnostic%2520end-to-end%2520driving%2520systems.%250AThrough%2520reinforcement%2520fine-tuning%2520and%2520policy%2520expansion%2520that%2520facilitates%250Acontinuous%2520improvement%252C%2520R2SE%2520features%2520three%2520key%2520components%253A%25201%2529%2520Generalist%250APretraining%2520with%2520hard-case%2520allocation%2520trains%2520a%2520generalist%2520imitation%2520learning%250A%2528IL%2529%2520driving%2520system%2520while%2520dynamically%2520identifying%2520failure-prone%2520cases%2520for%250Atargeted%2520refinement%253B%25202%2529%2520Residual%2520Reinforced%2520Specialist%2520Fine-tuning%2520optimizes%250Aresidual%2520corrections%2520using%2520reinforcement%2520learning%2520%2528RL%2529%2520to%2520improve%2520performance%250Ain%2520hard%2520case%2520domain%2520while%2520preserving%2520global%2520driving%2520knowledge%253B%25203%2529%2520Self-aware%250AAdapter%2520Expansion%2520dynamically%2520integrates%2520specialist%2520policies%2520back%2520into%2520the%250Ageneralist%2520model%252C%2520enhancing%2520continuous%2520performance%2520improvement.%2520Experimental%250Aresults%2520in%2520closed-loop%2520simulation%2520and%2520real-world%2520datasets%2520demonstrate%250Aimprovements%2520in%2520generalization%252C%2520safety%252C%2520and%2520long-horizon%2520policy%2520robustness%2520over%250Astate-of-the-art%2520E2E%2520systems%252C%2520highlighting%2520the%2520effectiveness%2520of%2520reinforce%250Arefinement%2520for%2520scalable%2520autonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforced%20Refinement%20with%20Self-Aware%20Expansion%20for%20End-to-End%0A%20%20Autonomous%20Driving&entry.906535625=Haochen%20Liu%20and%20Tianyu%20Li%20and%20Haohan%20Yang%20and%20Li%20Chen%20and%20Caojun%20Wang%20and%20Ke%20Guo%20and%20Haochen%20Tian%20and%20Hongchen%20Li%20and%20Hongyang%20Li%20and%20Chen%20Lv&entry.1292438233=%20%20End-to-end%20autonomous%20driving%20has%20emerged%20as%20a%20promising%20paradigm%20for%0Adirectly%20mapping%20sensor%20inputs%20to%20planning%20maneuvers%20using%20learning-based%0Amodular%20integrations.%20However%2C%20existing%20imitation%20learning%20%28IL%29-based%20models%0Asuffer%20from%20generalization%20to%20hard%20cases%2C%20and%20a%20lack%20of%20corrective%20feedback%0Aloop%20under%20post-deployment.%20While%20reinforcement%20learning%20%28RL%29%20offers%20a%0Apotential%20solution%20to%20tackle%20hard%20cases%20with%20optimality%2C%20it%20is%20often%20hindered%0Aby%20overfitting%20to%20specific%20driving%20cases%2C%20resulting%20in%20catastrophic%20forgetting%0Aof%20generalizable%20knowledge%20and%20sample%20inefficiency.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20Reinforced%20Refinement%20with%20Self-aware%20Expansion%20%28R2SE%29%2C%0Aa%20novel%20learning%20pipeline%20that%20constantly%20refines%20hard%20domain%20while%20keeping%0Ageneralizable%20driving%20policy%20for%20model-agnostic%20end-to-end%20driving%20systems.%0AThrough%20reinforcement%20fine-tuning%20and%20policy%20expansion%20that%20facilitates%0Acontinuous%20improvement%2C%20R2SE%20features%20three%20key%20components%3A%201%29%20Generalist%0APretraining%20with%20hard-case%20allocation%20trains%20a%20generalist%20imitation%20learning%0A%28IL%29%20driving%20system%20while%20dynamically%20identifying%20failure-prone%20cases%20for%0Atargeted%20refinement%3B%202%29%20Residual%20Reinforced%20Specialist%20Fine-tuning%20optimizes%0Aresidual%20corrections%20using%20reinforcement%20learning%20%28RL%29%20to%20improve%20performance%0Ain%20hard%20case%20domain%20while%20preserving%20global%20driving%20knowledge%3B%203%29%20Self-aware%0AAdapter%20Expansion%20dynamically%20integrates%20specialist%20policies%20back%20into%20the%0Ageneralist%20model%2C%20enhancing%20continuous%20performance%20improvement.%20Experimental%0Aresults%20in%20closed-loop%20simulation%20and%20real-world%20datasets%20demonstrate%0Aimprovements%20in%20generalization%2C%20safety%2C%20and%20long-horizon%20policy%20robustness%20over%0Astate-of-the-art%20E2E%20systems%2C%20highlighting%20the%20effectiveness%20of%20reinforce%0Arefinement%20for%20scalable%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09800v1&entry.124074799=Read"},
{"title": "Dataset of News Articles with Provenance Metadata for Media Relevance\n  Assessment", "author": "Tomas Peterka and Matyas Bohacek", "abstract": "  Out-of-context and misattributed imagery is the leading form of media\nmanipulation in today's misinformation and disinformation landscape. The\nexisting methods attempting to detect this practice often only consider whether\nthe semantics of the imagery corresponds to the text narrative, missing\nmanipulation so long as the depicted objects or scenes somewhat correspond to\nthe narrative at hand. To tackle this, we introduce News Media Provenance\nDataset, a dataset of news articles with provenance-tagged images. We formulate\ntwo tasks on this dataset, location of origin relevance (LOR) and date and time\nof origin relevance (DTOR), and present baseline results on six large language\nmodels (LLMs). We identify that, while the zero-shot performance on LOR is\npromising, the performance on DTOR hinders, leaving room for specialized\narchitectures and future work.\n", "link": "http://arxiv.org/abs/2506.09847v1", "date": "2025-06-11", "relevancy": 2.0565, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5246}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dataset%20of%20News%20Articles%20with%20Provenance%20Metadata%20for%20Media%20Relevance%0A%20%20Assessment&body=Title%3A%20Dataset%20of%20News%20Articles%20with%20Provenance%20Metadata%20for%20Media%20Relevance%0A%20%20Assessment%0AAuthor%3A%20Tomas%20Peterka%20and%20Matyas%20Bohacek%0AAbstract%3A%20%20%20Out-of-context%20and%20misattributed%20imagery%20is%20the%20leading%20form%20of%20media%0Amanipulation%20in%20today%27s%20misinformation%20and%20disinformation%20landscape.%20The%0Aexisting%20methods%20attempting%20to%20detect%20this%20practice%20often%20only%20consider%20whether%0Athe%20semantics%20of%20the%20imagery%20corresponds%20to%20the%20text%20narrative%2C%20missing%0Amanipulation%20so%20long%20as%20the%20depicted%20objects%20or%20scenes%20somewhat%20correspond%20to%0Athe%20narrative%20at%20hand.%20To%20tackle%20this%2C%20we%20introduce%20News%20Media%20Provenance%0ADataset%2C%20a%20dataset%20of%20news%20articles%20with%20provenance-tagged%20images.%20We%20formulate%0Atwo%20tasks%20on%20this%20dataset%2C%20location%20of%20origin%20relevance%20%28LOR%29%20and%20date%20and%20time%0Aof%20origin%20relevance%20%28DTOR%29%2C%20and%20present%20baseline%20results%20on%20six%20large%20language%0Amodels%20%28LLMs%29.%20We%20identify%20that%2C%20while%20the%20zero-shot%20performance%20on%20LOR%20is%0Apromising%2C%20the%20performance%20on%20DTOR%20hinders%2C%20leaving%20room%20for%20specialized%0Aarchitectures%20and%20future%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDataset%2520of%2520News%2520Articles%2520with%2520Provenance%2520Metadata%2520for%2520Media%2520Relevance%250A%2520%2520Assessment%26entry.906535625%3DTomas%2520Peterka%2520and%2520Matyas%2520Bohacek%26entry.1292438233%3D%2520%2520Out-of-context%2520and%2520misattributed%2520imagery%2520is%2520the%2520leading%2520form%2520of%2520media%250Amanipulation%2520in%2520today%2527s%2520misinformation%2520and%2520disinformation%2520landscape.%2520The%250Aexisting%2520methods%2520attempting%2520to%2520detect%2520this%2520practice%2520often%2520only%2520consider%2520whether%250Athe%2520semantics%2520of%2520the%2520imagery%2520corresponds%2520to%2520the%2520text%2520narrative%252C%2520missing%250Amanipulation%2520so%2520long%2520as%2520the%2520depicted%2520objects%2520or%2520scenes%2520somewhat%2520correspond%2520to%250Athe%2520narrative%2520at%2520hand.%2520To%2520tackle%2520this%252C%2520we%2520introduce%2520News%2520Media%2520Provenance%250ADataset%252C%2520a%2520dataset%2520of%2520news%2520articles%2520with%2520provenance-tagged%2520images.%2520We%2520formulate%250Atwo%2520tasks%2520on%2520this%2520dataset%252C%2520location%2520of%2520origin%2520relevance%2520%2528LOR%2529%2520and%2520date%2520and%2520time%250Aof%2520origin%2520relevance%2520%2528DTOR%2529%252C%2520and%2520present%2520baseline%2520results%2520on%2520six%2520large%2520language%250Amodels%2520%2528LLMs%2529.%2520We%2520identify%2520that%252C%2520while%2520the%2520zero-shot%2520performance%2520on%2520LOR%2520is%250Apromising%252C%2520the%2520performance%2520on%2520DTOR%2520hinders%252C%2520leaving%2520room%2520for%2520specialized%250Aarchitectures%2520and%2520future%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dataset%20of%20News%20Articles%20with%20Provenance%20Metadata%20for%20Media%20Relevance%0A%20%20Assessment&entry.906535625=Tomas%20Peterka%20and%20Matyas%20Bohacek&entry.1292438233=%20%20Out-of-context%20and%20misattributed%20imagery%20is%20the%20leading%20form%20of%20media%0Amanipulation%20in%20today%27s%20misinformation%20and%20disinformation%20landscape.%20The%0Aexisting%20methods%20attempting%20to%20detect%20this%20practice%20often%20only%20consider%20whether%0Athe%20semantics%20of%20the%20imagery%20corresponds%20to%20the%20text%20narrative%2C%20missing%0Amanipulation%20so%20long%20as%20the%20depicted%20objects%20or%20scenes%20somewhat%20correspond%20to%0Athe%20narrative%20at%20hand.%20To%20tackle%20this%2C%20we%20introduce%20News%20Media%20Provenance%0ADataset%2C%20a%20dataset%20of%20news%20articles%20with%20provenance-tagged%20images.%20We%20formulate%0Atwo%20tasks%20on%20this%20dataset%2C%20location%20of%20origin%20relevance%20%28LOR%29%20and%20date%20and%20time%0Aof%20origin%20relevance%20%28DTOR%29%2C%20and%20present%20baseline%20results%20on%20six%20large%20language%0Amodels%20%28LLMs%29.%20We%20identify%20that%2C%20while%20the%20zero-shot%20performance%20on%20LOR%20is%0Apromising%2C%20the%20performance%20on%20DTOR%20hinders%2C%20leaving%20room%20for%20specialized%0Aarchitectures%20and%20future%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09847v1&entry.124074799=Read"},
{"title": "TACTIC: Translation Agents with Cognitive-Theoretic Interactive\n  Collaboration", "author": "Weiya Li and Junjie Chen and Bei Li and Boyang Liu and Zichen Wen and Nuanqiao Shan and Xiaoqian Liu and Anping Liu and Huajie Liu and Hu Song and Linfeng Zhang", "abstract": "  Machine translation has long been a central task in natural language\nprocessing. With the rapid advancement of large language models (LLMs), there\nhas been remarkable progress in translation quality. However, fully realizing\nthe translation potential of LLMs remains an open challenge. Recent studies\nhave explored multi-agent systems to decompose complex translation tasks into\ncollaborative subtasks, showing initial promise in enhancing translation\nquality through agent cooperation and specialization. Nevertheless, existing\nmulti-agent translation frameworks largely neglect foundational insights from\ncognitive translation studies. These insights emphasize how human translators\nemploy different cognitive strategies, such as balancing literal and free\ntranslation, refining expressions based on context, and iteratively evaluating\noutputs. To address this limitation, we propose a cognitively informed\nmulti-agent framework called TACTIC, which stands for T ranslation A gents with\nCognitive- T heoretic Interactive Collaboration. The framework comprises six\nfunctionally distinct agents that mirror key cognitive processes observed in\nhuman translation behavior. These include agents for drafting, refinement,\nevaluation, scoring, context reasoning, and external knowledge gathering. By\nsimulating an interactive and theory-grounded translation workflow, TACTIC\neffectively leverages the full capacity of LLMs for high-quality translation.\nExperimental results on diverse language pairs from the FLORES-200 and WMT24\nbenchmarks show that our method consistently achieves state-of-the-art\nperformance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by\nan average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it\nfurther improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at\nhttps://github.com/weiyali126/TACTIC.\n", "link": "http://arxiv.org/abs/2506.08403v2", "date": "2025-06-11", "relevancy": 2.0448, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5325}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5012}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TACTIC%3A%20Translation%20Agents%20with%20Cognitive-Theoretic%20Interactive%0A%20%20Collaboration&body=Title%3A%20TACTIC%3A%20Translation%20Agents%20with%20Cognitive-Theoretic%20Interactive%0A%20%20Collaboration%0AAuthor%3A%20Weiya%20Li%20and%20Junjie%20Chen%20and%20Bei%20Li%20and%20Boyang%20Liu%20and%20Zichen%20Wen%20and%20Nuanqiao%20Shan%20and%20Xiaoqian%20Liu%20and%20Anping%20Liu%20and%20Huajie%20Liu%20and%20Hu%20Song%20and%20Linfeng%20Zhang%0AAbstract%3A%20%20%20Machine%20translation%20has%20long%20been%20a%20central%20task%20in%20natural%20language%0Aprocessing.%20With%20the%20rapid%20advancement%20of%20large%20language%20models%20%28LLMs%29%2C%20there%0Ahas%20been%20remarkable%20progress%20in%20translation%20quality.%20However%2C%20fully%20realizing%0Athe%20translation%20potential%20of%20LLMs%20remains%20an%20open%20challenge.%20Recent%20studies%0Ahave%20explored%20multi-agent%20systems%20to%20decompose%20complex%20translation%20tasks%20into%0Acollaborative%20subtasks%2C%20showing%20initial%20promise%20in%20enhancing%20translation%0Aquality%20through%20agent%20cooperation%20and%20specialization.%20Nevertheless%2C%20existing%0Amulti-agent%20translation%20frameworks%20largely%20neglect%20foundational%20insights%20from%0Acognitive%20translation%20studies.%20These%20insights%20emphasize%20how%20human%20translators%0Aemploy%20different%20cognitive%20strategies%2C%20such%20as%20balancing%20literal%20and%20free%0Atranslation%2C%20refining%20expressions%20based%20on%20context%2C%20and%20iteratively%20evaluating%0Aoutputs.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20cognitively%20informed%0Amulti-agent%20framework%20called%20TACTIC%2C%20which%20stands%20for%20T%20ranslation%20A%20gents%20with%0ACognitive-%20T%20heoretic%20Interactive%20Collaboration.%20The%20framework%20comprises%20six%0Afunctionally%20distinct%20agents%20that%20mirror%20key%20cognitive%20processes%20observed%20in%0Ahuman%20translation%20behavior.%20These%20include%20agents%20for%20drafting%2C%20refinement%2C%0Aevaluation%2C%20scoring%2C%20context%20reasoning%2C%20and%20external%20knowledge%20gathering.%20By%0Asimulating%20an%20interactive%20and%20theory-grounded%20translation%20workflow%2C%20TACTIC%0Aeffectively%20leverages%20the%20full%20capacity%20of%20LLMs%20for%20high-quality%20translation.%0AExperimental%20results%20on%20diverse%20language%20pairs%20from%20the%20FLORES-200%20and%20WMT24%0Abenchmarks%20show%20that%20our%20method%20consistently%20achieves%20state-of-the-art%0Aperformance.%20Using%20DeepSeek-V3%20as%20the%20base%20model%2C%20TACTIC%20surpasses%20GPT-4.1%20by%0Aan%20average%20of%20%2B0.6%20XCOMET%20and%20%2B1.18%20COMETKIWI-23.%20Compared%20to%20DeepSeek-R1%2C%20it%0Afurther%20improves%20by%20%2B0.84%20XCOMET%20and%20%2B2.99%20COMETKIWI-23.%20Code%20is%20available%20at%0Ahttps%3A//github.com/weiyali126/TACTIC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08403v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTACTIC%253A%2520Translation%2520Agents%2520with%2520Cognitive-Theoretic%2520Interactive%250A%2520%2520Collaboration%26entry.906535625%3DWeiya%2520Li%2520and%2520Junjie%2520Chen%2520and%2520Bei%2520Li%2520and%2520Boyang%2520Liu%2520and%2520Zichen%2520Wen%2520and%2520Nuanqiao%2520Shan%2520and%2520Xiaoqian%2520Liu%2520and%2520Anping%2520Liu%2520and%2520Huajie%2520Liu%2520and%2520Hu%2520Song%2520and%2520Linfeng%2520Zhang%26entry.1292438233%3D%2520%2520Machine%2520translation%2520has%2520long%2520been%2520a%2520central%2520task%2520in%2520natural%2520language%250Aprocessing.%2520With%2520the%2520rapid%2520advancement%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520there%250Ahas%2520been%2520remarkable%2520progress%2520in%2520translation%2520quality.%2520However%252C%2520fully%2520realizing%250Athe%2520translation%2520potential%2520of%2520LLMs%2520remains%2520an%2520open%2520challenge.%2520Recent%2520studies%250Ahave%2520explored%2520multi-agent%2520systems%2520to%2520decompose%2520complex%2520translation%2520tasks%2520into%250Acollaborative%2520subtasks%252C%2520showing%2520initial%2520promise%2520in%2520enhancing%2520translation%250Aquality%2520through%2520agent%2520cooperation%2520and%2520specialization.%2520Nevertheless%252C%2520existing%250Amulti-agent%2520translation%2520frameworks%2520largely%2520neglect%2520foundational%2520insights%2520from%250Acognitive%2520translation%2520studies.%2520These%2520insights%2520emphasize%2520how%2520human%2520translators%250Aemploy%2520different%2520cognitive%2520strategies%252C%2520such%2520as%2520balancing%2520literal%2520and%2520free%250Atranslation%252C%2520refining%2520expressions%2520based%2520on%2520context%252C%2520and%2520iteratively%2520evaluating%250Aoutputs.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520cognitively%2520informed%250Amulti-agent%2520framework%2520called%2520TACTIC%252C%2520which%2520stands%2520for%2520T%2520ranslation%2520A%2520gents%2520with%250ACognitive-%2520T%2520heoretic%2520Interactive%2520Collaboration.%2520The%2520framework%2520comprises%2520six%250Afunctionally%2520distinct%2520agents%2520that%2520mirror%2520key%2520cognitive%2520processes%2520observed%2520in%250Ahuman%2520translation%2520behavior.%2520These%2520include%2520agents%2520for%2520drafting%252C%2520refinement%252C%250Aevaluation%252C%2520scoring%252C%2520context%2520reasoning%252C%2520and%2520external%2520knowledge%2520gathering.%2520By%250Asimulating%2520an%2520interactive%2520and%2520theory-grounded%2520translation%2520workflow%252C%2520TACTIC%250Aeffectively%2520leverages%2520the%2520full%2520capacity%2520of%2520LLMs%2520for%2520high-quality%2520translation.%250AExperimental%2520results%2520on%2520diverse%2520language%2520pairs%2520from%2520the%2520FLORES-200%2520and%2520WMT24%250Abenchmarks%2520show%2520that%2520our%2520method%2520consistently%2520achieves%2520state-of-the-art%250Aperformance.%2520Using%2520DeepSeek-V3%2520as%2520the%2520base%2520model%252C%2520TACTIC%2520surpasses%2520GPT-4.1%2520by%250Aan%2520average%2520of%2520%252B0.6%2520XCOMET%2520and%2520%252B1.18%2520COMETKIWI-23.%2520Compared%2520to%2520DeepSeek-R1%252C%2520it%250Afurther%2520improves%2520by%2520%252B0.84%2520XCOMET%2520and%2520%252B2.99%2520COMETKIWI-23.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/weiyali126/TACTIC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08403v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TACTIC%3A%20Translation%20Agents%20with%20Cognitive-Theoretic%20Interactive%0A%20%20Collaboration&entry.906535625=Weiya%20Li%20and%20Junjie%20Chen%20and%20Bei%20Li%20and%20Boyang%20Liu%20and%20Zichen%20Wen%20and%20Nuanqiao%20Shan%20and%20Xiaoqian%20Liu%20and%20Anping%20Liu%20and%20Huajie%20Liu%20and%20Hu%20Song%20and%20Linfeng%20Zhang&entry.1292438233=%20%20Machine%20translation%20has%20long%20been%20a%20central%20task%20in%20natural%20language%0Aprocessing.%20With%20the%20rapid%20advancement%20of%20large%20language%20models%20%28LLMs%29%2C%20there%0Ahas%20been%20remarkable%20progress%20in%20translation%20quality.%20However%2C%20fully%20realizing%0Athe%20translation%20potential%20of%20LLMs%20remains%20an%20open%20challenge.%20Recent%20studies%0Ahave%20explored%20multi-agent%20systems%20to%20decompose%20complex%20translation%20tasks%20into%0Acollaborative%20subtasks%2C%20showing%20initial%20promise%20in%20enhancing%20translation%0Aquality%20through%20agent%20cooperation%20and%20specialization.%20Nevertheless%2C%20existing%0Amulti-agent%20translation%20frameworks%20largely%20neglect%20foundational%20insights%20from%0Acognitive%20translation%20studies.%20These%20insights%20emphasize%20how%20human%20translators%0Aemploy%20different%20cognitive%20strategies%2C%20such%20as%20balancing%20literal%20and%20free%0Atranslation%2C%20refining%20expressions%20based%20on%20context%2C%20and%20iteratively%20evaluating%0Aoutputs.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20cognitively%20informed%0Amulti-agent%20framework%20called%20TACTIC%2C%20which%20stands%20for%20T%20ranslation%20A%20gents%20with%0ACognitive-%20T%20heoretic%20Interactive%20Collaboration.%20The%20framework%20comprises%20six%0Afunctionally%20distinct%20agents%20that%20mirror%20key%20cognitive%20processes%20observed%20in%0Ahuman%20translation%20behavior.%20These%20include%20agents%20for%20drafting%2C%20refinement%2C%0Aevaluation%2C%20scoring%2C%20context%20reasoning%2C%20and%20external%20knowledge%20gathering.%20By%0Asimulating%20an%20interactive%20and%20theory-grounded%20translation%20workflow%2C%20TACTIC%0Aeffectively%20leverages%20the%20full%20capacity%20of%20LLMs%20for%20high-quality%20translation.%0AExperimental%20results%20on%20diverse%20language%20pairs%20from%20the%20FLORES-200%20and%20WMT24%0Abenchmarks%20show%20that%20our%20method%20consistently%20achieves%20state-of-the-art%0Aperformance.%20Using%20DeepSeek-V3%20as%20the%20base%20model%2C%20TACTIC%20surpasses%20GPT-4.1%20by%0Aan%20average%20of%20%2B0.6%20XCOMET%20and%20%2B1.18%20COMETKIWI-23.%20Compared%20to%20DeepSeek-R1%2C%20it%0Afurther%20improves%20by%20%2B0.84%20XCOMET%20and%20%2B2.99%20COMETKIWI-23.%20Code%20is%20available%20at%0Ahttps%3A//github.com/weiyali126/TACTIC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08403v2&entry.124074799=Read"},
{"title": "Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of\n  Perspective", "author": "Daniel Franzen and Jan Disselhoff and David Hartmann", "abstract": "  The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge\nfor large language models (LLMs), exposing limitations in their abstract\nreasoning abilities. In this work, we leverage task-specific data augmentations\nthroughout the training, generation, and scoring phases, and employ a\ndepth-first search algorithm to generate diverse, high-probability candidate\nsolutions. Furthermore, we utilize the LLM not only as a generator but also as\na scorer, using its output probabilities to select the most promising\nsolutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the\npublic ARC-AGI evaluation set, demonstrating state-of-the-art performance among\npublicly available approaches. While concurrent closed-source work has reported\nhigher scores, our method distinguishes itself through its transparency,\nreproducibility, and remarkably low inference cost, averaging only around 2ct\nper task on readily available hardware (we assume a price of 36ct/hour for a\nNvidia 4090 GPU).\n", "link": "http://arxiv.org/abs/2505.07859v2", "date": "2025-06-11", "relevancy": 2.0376, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5101}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5101}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Product%20of%20Experts%20with%20LLMs%3A%20Boosting%20Performance%20on%20ARC%20Is%20a%20Matter%20of%0A%20%20Perspective&body=Title%3A%20Product%20of%20Experts%20with%20LLMs%3A%20Boosting%20Performance%20on%20ARC%20Is%20a%20Matter%20of%0A%20%20Perspective%0AAuthor%3A%20Daniel%20Franzen%20and%20Jan%20Disselhoff%20and%20David%20Hartmann%0AAbstract%3A%20%20%20The%20Abstraction%20and%20Reasoning%20Corpus%20%28ARC-AGI%29%20poses%20a%20significant%20challenge%0Afor%20large%20language%20models%20%28LLMs%29%2C%20exposing%20limitations%20in%20their%20abstract%0Areasoning%20abilities.%20In%20this%20work%2C%20we%20leverage%20task-specific%20data%20augmentations%0Athroughout%20the%20training%2C%20generation%2C%20and%20scoring%20phases%2C%20and%20employ%20a%0Adepth-first%20search%20algorithm%20to%20generate%20diverse%2C%20high-probability%20candidate%0Asolutions.%20Furthermore%2C%20we%20utilize%20the%20LLM%20not%20only%20as%20a%20generator%20but%20also%20as%0Aa%20scorer%2C%20using%20its%20output%20probabilities%20to%20select%20the%20most%20promising%0Asolutions.%20Our%20method%20achieves%20a%20score%20of%2071.6%25%20%28286.5/400%20solved%20tasks%29%20on%20the%0Apublic%20ARC-AGI%20evaluation%20set%2C%20demonstrating%20state-of-the-art%20performance%20among%0Apublicly%20available%20approaches.%20While%20concurrent%20closed-source%20work%20has%20reported%0Ahigher%20scores%2C%20our%20method%20distinguishes%20itself%20through%20its%20transparency%2C%0Areproducibility%2C%20and%20remarkably%20low%20inference%20cost%2C%20averaging%20only%20around%202ct%0Aper%20task%20on%20readily%20available%20hardware%20%28we%20assume%20a%20price%20of%2036ct/hour%20for%20a%0ANvidia%204090%20GPU%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07859v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProduct%2520of%2520Experts%2520with%2520LLMs%253A%2520Boosting%2520Performance%2520on%2520ARC%2520Is%2520a%2520Matter%2520of%250A%2520%2520Perspective%26entry.906535625%3DDaniel%2520Franzen%2520and%2520Jan%2520Disselhoff%2520and%2520David%2520Hartmann%26entry.1292438233%3D%2520%2520The%2520Abstraction%2520and%2520Reasoning%2520Corpus%2520%2528ARC-AGI%2529%2520poses%2520a%2520significant%2520challenge%250Afor%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520exposing%2520limitations%2520in%2520their%2520abstract%250Areasoning%2520abilities.%2520In%2520this%2520work%252C%2520we%2520leverage%2520task-specific%2520data%2520augmentations%250Athroughout%2520the%2520training%252C%2520generation%252C%2520and%2520scoring%2520phases%252C%2520and%2520employ%2520a%250Adepth-first%2520search%2520algorithm%2520to%2520generate%2520diverse%252C%2520high-probability%2520candidate%250Asolutions.%2520Furthermore%252C%2520we%2520utilize%2520the%2520LLM%2520not%2520only%2520as%2520a%2520generator%2520but%2520also%2520as%250Aa%2520scorer%252C%2520using%2520its%2520output%2520probabilities%2520to%2520select%2520the%2520most%2520promising%250Asolutions.%2520Our%2520method%2520achieves%2520a%2520score%2520of%252071.6%2525%2520%2528286.5/400%2520solved%2520tasks%2529%2520on%2520the%250Apublic%2520ARC-AGI%2520evaluation%2520set%252C%2520demonstrating%2520state-of-the-art%2520performance%2520among%250Apublicly%2520available%2520approaches.%2520While%2520concurrent%2520closed-source%2520work%2520has%2520reported%250Ahigher%2520scores%252C%2520our%2520method%2520distinguishes%2520itself%2520through%2520its%2520transparency%252C%250Areproducibility%252C%2520and%2520remarkably%2520low%2520inference%2520cost%252C%2520averaging%2520only%2520around%25202ct%250Aper%2520task%2520on%2520readily%2520available%2520hardware%2520%2528we%2520assume%2520a%2520price%2520of%252036ct/hour%2520for%2520a%250ANvidia%25204090%2520GPU%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07859v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Product%20of%20Experts%20with%20LLMs%3A%20Boosting%20Performance%20on%20ARC%20Is%20a%20Matter%20of%0A%20%20Perspective&entry.906535625=Daniel%20Franzen%20and%20Jan%20Disselhoff%20and%20David%20Hartmann&entry.1292438233=%20%20The%20Abstraction%20and%20Reasoning%20Corpus%20%28ARC-AGI%29%20poses%20a%20significant%20challenge%0Afor%20large%20language%20models%20%28LLMs%29%2C%20exposing%20limitations%20in%20their%20abstract%0Areasoning%20abilities.%20In%20this%20work%2C%20we%20leverage%20task-specific%20data%20augmentations%0Athroughout%20the%20training%2C%20generation%2C%20and%20scoring%20phases%2C%20and%20employ%20a%0Adepth-first%20search%20algorithm%20to%20generate%20diverse%2C%20high-probability%20candidate%0Asolutions.%20Furthermore%2C%20we%20utilize%20the%20LLM%20not%20only%20as%20a%20generator%20but%20also%20as%0Aa%20scorer%2C%20using%20its%20output%20probabilities%20to%20select%20the%20most%20promising%0Asolutions.%20Our%20method%20achieves%20a%20score%20of%2071.6%25%20%28286.5/400%20solved%20tasks%29%20on%20the%0Apublic%20ARC-AGI%20evaluation%20set%2C%20demonstrating%20state-of-the-art%20performance%20among%0Apublicly%20available%20approaches.%20While%20concurrent%20closed-source%20work%20has%20reported%0Ahigher%20scores%2C%20our%20method%20distinguishes%20itself%20through%20its%20transparency%2C%0Areproducibility%2C%20and%20remarkably%20low%20inference%20cost%2C%20averaging%20only%20around%202ct%0Aper%20task%20on%20readily%20available%20hardware%20%28we%20assume%20a%20price%20of%2036ct/hour%20for%20a%0ANvidia%204090%20GPU%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07859v2&entry.124074799=Read"},
{"title": "Is Long Context All You Need? Leveraging LLM's Extended Context for\n  NL2SQL", "author": "Yeounoh Chung and Gaurav T. Kakkar and Yu Gan and Brenton Milne and Fatma Ozcan", "abstract": "  Large Language Models (LLMs) have demonstrated impressive capabilities across\na range of natural language processing tasks. In particular, improvements in\nreasoning abilities and the expansion of context windows have opened new\navenues for leveraging these powerful models. NL2SQL is challenging in that the\nnatural language question is inherently ambiguous, while the SQL generation\nrequires a precise understanding of complex data schema and semantics. One\napproach to this semantic ambiguous problem is to provide more and sufficient\ncontextual information.\n  In this work, we explore the performance and the latency trade-offs of the\nextended context window (a.k.a., long context) offered by Google's\nstate-of-the-art LLM (\\textit{gemini-1.5-pro}). We study the impact of various\ncontextual information, including column example values, question and SQL query\npairs, user-provided hints, SQL documentation, and schema. To the best of our\nknowledge, this is the first work to study how the extended context window and\nextra contextual information can help NL2SQL generation with respect to both\naccuracy and latency cost. We show that long context LLMs are robust and do not\nget lost in the extended contextual information. Additionally, our long-context\nNL2SQL pipeline based on Google's \\textit{gemini-pro-1.5} achieve strong\nperformances on various benchmark datasets without finetuning and expensive\nself-consistency based techniques.\n", "link": "http://arxiv.org/abs/2501.12372v6", "date": "2025-06-11", "relevancy": 2.0324, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5193}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5193}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Long%20Context%20All%20You%20Need%3F%20Leveraging%20LLM%27s%20Extended%20Context%20for%0A%20%20NL2SQL&body=Title%3A%20Is%20Long%20Context%20All%20You%20Need%3F%20Leveraging%20LLM%27s%20Extended%20Context%20for%0A%20%20NL2SQL%0AAuthor%3A%20Yeounoh%20Chung%20and%20Gaurav%20T.%20Kakkar%20and%20Yu%20Gan%20and%20Brenton%20Milne%20and%20Fatma%20Ozcan%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20across%0Aa%20range%20of%20natural%20language%20processing%20tasks.%20In%20particular%2C%20improvements%20in%0Areasoning%20abilities%20and%20the%20expansion%20of%20context%20windows%20have%20opened%20new%0Aavenues%20for%20leveraging%20these%20powerful%20models.%20NL2SQL%20is%20challenging%20in%20that%20the%0Anatural%20language%20question%20is%20inherently%20ambiguous%2C%20while%20the%20SQL%20generation%0Arequires%20a%20precise%20understanding%20of%20complex%20data%20schema%20and%20semantics.%20One%0Aapproach%20to%20this%20semantic%20ambiguous%20problem%20is%20to%20provide%20more%20and%20sufficient%0Acontextual%20information.%0A%20%20In%20this%20work%2C%20we%20explore%20the%20performance%20and%20the%20latency%20trade-offs%20of%20the%0Aextended%20context%20window%20%28a.k.a.%2C%20long%20context%29%20offered%20by%20Google%27s%0Astate-of-the-art%20LLM%20%28%5Ctextit%7Bgemini-1.5-pro%7D%29.%20We%20study%20the%20impact%20of%20various%0Acontextual%20information%2C%20including%20column%20example%20values%2C%20question%20and%20SQL%20query%0Apairs%2C%20user-provided%20hints%2C%20SQL%20documentation%2C%20and%20schema.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20work%20to%20study%20how%20the%20extended%20context%20window%20and%0Aextra%20contextual%20information%20can%20help%20NL2SQL%20generation%20with%20respect%20to%20both%0Aaccuracy%20and%20latency%20cost.%20We%20show%20that%20long%20context%20LLMs%20are%20robust%20and%20do%20not%0Aget%20lost%20in%20the%20extended%20contextual%20information.%20Additionally%2C%20our%20long-context%0ANL2SQL%20pipeline%20based%20on%20Google%27s%20%5Ctextit%7Bgemini-pro-1.5%7D%20achieve%20strong%0Aperformances%20on%20various%20benchmark%20datasets%20without%20finetuning%20and%20expensive%0Aself-consistency%20based%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12372v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Long%2520Context%2520All%2520You%2520Need%253F%2520Leveraging%2520LLM%2527s%2520Extended%2520Context%2520for%250A%2520%2520NL2SQL%26entry.906535625%3DYeounoh%2520Chung%2520and%2520Gaurav%2520T.%2520Kakkar%2520and%2520Yu%2520Gan%2520and%2520Brenton%2520Milne%2520and%2520Fatma%2520Ozcan%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520capabilities%2520across%250Aa%2520range%2520of%2520natural%2520language%2520processing%2520tasks.%2520In%2520particular%252C%2520improvements%2520in%250Areasoning%2520abilities%2520and%2520the%2520expansion%2520of%2520context%2520windows%2520have%2520opened%2520new%250Aavenues%2520for%2520leveraging%2520these%2520powerful%2520models.%2520NL2SQL%2520is%2520challenging%2520in%2520that%2520the%250Anatural%2520language%2520question%2520is%2520inherently%2520ambiguous%252C%2520while%2520the%2520SQL%2520generation%250Arequires%2520a%2520precise%2520understanding%2520of%2520complex%2520data%2520schema%2520and%2520semantics.%2520One%250Aapproach%2520to%2520this%2520semantic%2520ambiguous%2520problem%2520is%2520to%2520provide%2520more%2520and%2520sufficient%250Acontextual%2520information.%250A%2520%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520performance%2520and%2520the%2520latency%2520trade-offs%2520of%2520the%250Aextended%2520context%2520window%2520%2528a.k.a.%252C%2520long%2520context%2529%2520offered%2520by%2520Google%2527s%250Astate-of-the-art%2520LLM%2520%2528%255Ctextit%257Bgemini-1.5-pro%257D%2529.%2520We%2520study%2520the%2520impact%2520of%2520various%250Acontextual%2520information%252C%2520including%2520column%2520example%2520values%252C%2520question%2520and%2520SQL%2520query%250Apairs%252C%2520user-provided%2520hints%252C%2520SQL%2520documentation%252C%2520and%2520schema.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520work%2520to%2520study%2520how%2520the%2520extended%2520context%2520window%2520and%250Aextra%2520contextual%2520information%2520can%2520help%2520NL2SQL%2520generation%2520with%2520respect%2520to%2520both%250Aaccuracy%2520and%2520latency%2520cost.%2520We%2520show%2520that%2520long%2520context%2520LLMs%2520are%2520robust%2520and%2520do%2520not%250Aget%2520lost%2520in%2520the%2520extended%2520contextual%2520information.%2520Additionally%252C%2520our%2520long-context%250ANL2SQL%2520pipeline%2520based%2520on%2520Google%2527s%2520%255Ctextit%257Bgemini-pro-1.5%257D%2520achieve%2520strong%250Aperformances%2520on%2520various%2520benchmark%2520datasets%2520without%2520finetuning%2520and%2520expensive%250Aself-consistency%2520based%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12372v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Long%20Context%20All%20You%20Need%3F%20Leveraging%20LLM%27s%20Extended%20Context%20for%0A%20%20NL2SQL&entry.906535625=Yeounoh%20Chung%20and%20Gaurav%20T.%20Kakkar%20and%20Yu%20Gan%20and%20Brenton%20Milne%20and%20Fatma%20Ozcan&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20across%0Aa%20range%20of%20natural%20language%20processing%20tasks.%20In%20particular%2C%20improvements%20in%0Areasoning%20abilities%20and%20the%20expansion%20of%20context%20windows%20have%20opened%20new%0Aavenues%20for%20leveraging%20these%20powerful%20models.%20NL2SQL%20is%20challenging%20in%20that%20the%0Anatural%20language%20question%20is%20inherently%20ambiguous%2C%20while%20the%20SQL%20generation%0Arequires%20a%20precise%20understanding%20of%20complex%20data%20schema%20and%20semantics.%20One%0Aapproach%20to%20this%20semantic%20ambiguous%20problem%20is%20to%20provide%20more%20and%20sufficient%0Acontextual%20information.%0A%20%20In%20this%20work%2C%20we%20explore%20the%20performance%20and%20the%20latency%20trade-offs%20of%20the%0Aextended%20context%20window%20%28a.k.a.%2C%20long%20context%29%20offered%20by%20Google%27s%0Astate-of-the-art%20LLM%20%28%5Ctextit%7Bgemini-1.5-pro%7D%29.%20We%20study%20the%20impact%20of%20various%0Acontextual%20information%2C%20including%20column%20example%20values%2C%20question%20and%20SQL%20query%0Apairs%2C%20user-provided%20hints%2C%20SQL%20documentation%2C%20and%20schema.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20work%20to%20study%20how%20the%20extended%20context%20window%20and%0Aextra%20contextual%20information%20can%20help%20NL2SQL%20generation%20with%20respect%20to%20both%0Aaccuracy%20and%20latency%20cost.%20We%20show%20that%20long%20context%20LLMs%20are%20robust%20and%20do%20not%0Aget%20lost%20in%20the%20extended%20contextual%20information.%20Additionally%2C%20our%20long-context%0ANL2SQL%20pipeline%20based%20on%20Google%27s%20%5Ctextit%7Bgemini-pro-1.5%7D%20achieve%20strong%0Aperformances%20on%20various%20benchmark%20datasets%20without%20finetuning%20and%20expensive%0Aself-consistency%20based%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12372v6&entry.124074799=Read"},
{"title": "Sparser, Better, Faster, Stronger: Sparsity Detection for Efficient\n  Automatic Differentiation", "author": "Adrian Hill and Guillaume Dalle", "abstract": "  From implicit differentiation to probabilistic modeling, Jacobian and Hessian\nmatrices have many potential use cases in Machine Learning (ML), but they are\nviewed as computationally prohibitive. Fortunately, these matrices often\nexhibit sparsity, which can be leveraged to speed up the process of Automatic\nDifferentiation (AD). This paper presents advances in sparsity detection,\npreviously the performance bottleneck of Automatic Sparse Differentiation\n(ASD). Our implementation of sparsity detection is based on operator\noverloading, able to detect both local and global sparsity patterns, and\nsupports flexible index set representations. It is fully automatic and requires\nno modification of user code, making it compatible with existing ML codebases.\nMost importantly, it is highly performant, unlocking Jacobians and Hessians at\nscales where they were considered too expensive to compute. On real-world\nproblems from scientific ML, graph neural networks and optimization, we show\nsignificant speed-ups of up to three orders of magnitude. Notably, using our\nsparsity detection system, ASD outperforms standard AD for one-off\ncomputations, without amortization of either sparsity detection or matrix\ncoloring.\n", "link": "http://arxiv.org/abs/2501.17737v2", "date": "2025-06-11", "relevancy": 2.0303, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5355}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5081}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparser%2C%20Better%2C%20Faster%2C%20Stronger%3A%20Sparsity%20Detection%20for%20Efficient%0A%20%20Automatic%20Differentiation&body=Title%3A%20Sparser%2C%20Better%2C%20Faster%2C%20Stronger%3A%20Sparsity%20Detection%20for%20Efficient%0A%20%20Automatic%20Differentiation%0AAuthor%3A%20Adrian%20Hill%20and%20Guillaume%20Dalle%0AAbstract%3A%20%20%20From%20implicit%20differentiation%20to%20probabilistic%20modeling%2C%20Jacobian%20and%20Hessian%0Amatrices%20have%20many%20potential%20use%20cases%20in%20Machine%20Learning%20%28ML%29%2C%20but%20they%20are%0Aviewed%20as%20computationally%20prohibitive.%20Fortunately%2C%20these%20matrices%20often%0Aexhibit%20sparsity%2C%20which%20can%20be%20leveraged%20to%20speed%20up%20the%20process%20of%20Automatic%0ADifferentiation%20%28AD%29.%20This%20paper%20presents%20advances%20in%20sparsity%20detection%2C%0Apreviously%20the%20performance%20bottleneck%20of%20Automatic%20Sparse%20Differentiation%0A%28ASD%29.%20Our%20implementation%20of%20sparsity%20detection%20is%20based%20on%20operator%0Aoverloading%2C%20able%20to%20detect%20both%20local%20and%20global%20sparsity%20patterns%2C%20and%0Asupports%20flexible%20index%20set%20representations.%20It%20is%20fully%20automatic%20and%20requires%0Ano%20modification%20of%20user%20code%2C%20making%20it%20compatible%20with%20existing%20ML%20codebases.%0AMost%20importantly%2C%20it%20is%20highly%20performant%2C%20unlocking%20Jacobians%20and%20Hessians%20at%0Ascales%20where%20they%20were%20considered%20too%20expensive%20to%20compute.%20On%20real-world%0Aproblems%20from%20scientific%20ML%2C%20graph%20neural%20networks%20and%20optimization%2C%20we%20show%0Asignificant%20speed-ups%20of%20up%20to%20three%20orders%20of%20magnitude.%20Notably%2C%20using%20our%0Asparsity%20detection%20system%2C%20ASD%20outperforms%20standard%20AD%20for%20one-off%0Acomputations%2C%20without%20amortization%20of%20either%20sparsity%20detection%20or%20matrix%0Acoloring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17737v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparser%252C%2520Better%252C%2520Faster%252C%2520Stronger%253A%2520Sparsity%2520Detection%2520for%2520Efficient%250A%2520%2520Automatic%2520Differentiation%26entry.906535625%3DAdrian%2520Hill%2520and%2520Guillaume%2520Dalle%26entry.1292438233%3D%2520%2520From%2520implicit%2520differentiation%2520to%2520probabilistic%2520modeling%252C%2520Jacobian%2520and%2520Hessian%250Amatrices%2520have%2520many%2520potential%2520use%2520cases%2520in%2520Machine%2520Learning%2520%2528ML%2529%252C%2520but%2520they%2520are%250Aviewed%2520as%2520computationally%2520prohibitive.%2520Fortunately%252C%2520these%2520matrices%2520often%250Aexhibit%2520sparsity%252C%2520which%2520can%2520be%2520leveraged%2520to%2520speed%2520up%2520the%2520process%2520of%2520Automatic%250ADifferentiation%2520%2528AD%2529.%2520This%2520paper%2520presents%2520advances%2520in%2520sparsity%2520detection%252C%250Apreviously%2520the%2520performance%2520bottleneck%2520of%2520Automatic%2520Sparse%2520Differentiation%250A%2528ASD%2529.%2520Our%2520implementation%2520of%2520sparsity%2520detection%2520is%2520based%2520on%2520operator%250Aoverloading%252C%2520able%2520to%2520detect%2520both%2520local%2520and%2520global%2520sparsity%2520patterns%252C%2520and%250Asupports%2520flexible%2520index%2520set%2520representations.%2520It%2520is%2520fully%2520automatic%2520and%2520requires%250Ano%2520modification%2520of%2520user%2520code%252C%2520making%2520it%2520compatible%2520with%2520existing%2520ML%2520codebases.%250AMost%2520importantly%252C%2520it%2520is%2520highly%2520performant%252C%2520unlocking%2520Jacobians%2520and%2520Hessians%2520at%250Ascales%2520where%2520they%2520were%2520considered%2520too%2520expensive%2520to%2520compute.%2520On%2520real-world%250Aproblems%2520from%2520scientific%2520ML%252C%2520graph%2520neural%2520networks%2520and%2520optimization%252C%2520we%2520show%250Asignificant%2520speed-ups%2520of%2520up%2520to%2520three%2520orders%2520of%2520magnitude.%2520Notably%252C%2520using%2520our%250Asparsity%2520detection%2520system%252C%2520ASD%2520outperforms%2520standard%2520AD%2520for%2520one-off%250Acomputations%252C%2520without%2520amortization%2520of%2520either%2520sparsity%2520detection%2520or%2520matrix%250Acoloring.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17737v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparser%2C%20Better%2C%20Faster%2C%20Stronger%3A%20Sparsity%20Detection%20for%20Efficient%0A%20%20Automatic%20Differentiation&entry.906535625=Adrian%20Hill%20and%20Guillaume%20Dalle&entry.1292438233=%20%20From%20implicit%20differentiation%20to%20probabilistic%20modeling%2C%20Jacobian%20and%20Hessian%0Amatrices%20have%20many%20potential%20use%20cases%20in%20Machine%20Learning%20%28ML%29%2C%20but%20they%20are%0Aviewed%20as%20computationally%20prohibitive.%20Fortunately%2C%20these%20matrices%20often%0Aexhibit%20sparsity%2C%20which%20can%20be%20leveraged%20to%20speed%20up%20the%20process%20of%20Automatic%0ADifferentiation%20%28AD%29.%20This%20paper%20presents%20advances%20in%20sparsity%20detection%2C%0Apreviously%20the%20performance%20bottleneck%20of%20Automatic%20Sparse%20Differentiation%0A%28ASD%29.%20Our%20implementation%20of%20sparsity%20detection%20is%20based%20on%20operator%0Aoverloading%2C%20able%20to%20detect%20both%20local%20and%20global%20sparsity%20patterns%2C%20and%0Asupports%20flexible%20index%20set%20representations.%20It%20is%20fully%20automatic%20and%20requires%0Ano%20modification%20of%20user%20code%2C%20making%20it%20compatible%20with%20existing%20ML%20codebases.%0AMost%20importantly%2C%20it%20is%20highly%20performant%2C%20unlocking%20Jacobians%20and%20Hessians%20at%0Ascales%20where%20they%20were%20considered%20too%20expensive%20to%20compute.%20On%20real-world%0Aproblems%20from%20scientific%20ML%2C%20graph%20neural%20networks%20and%20optimization%2C%20we%20show%0Asignificant%20speed-ups%20of%20up%20to%20three%20orders%20of%20magnitude.%20Notably%2C%20using%20our%0Asparsity%20detection%20system%2C%20ASD%20outperforms%20standard%20AD%20for%20one-off%0Acomputations%2C%20without%20amortization%20of%20either%20sparsity%20detection%20or%20matrix%0Acoloring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17737v2&entry.124074799=Read"},
{"title": "Learning to Align: Addressing Character Frequency Distribution Shifts in\n  Handwritten Text Recognition", "author": "Panagiotis Kaliosis and John Pavlopoulos", "abstract": "  Handwritten text recognition aims to convert visual input into\nmachine-readable text, and it remains challenging due to the evolving and\ncontext-dependent nature of handwriting. Character sets change over time, and\ncharacter frequency distributions shift across historical periods or regions,\noften causing models trained on broad, heterogeneous corpora to underperform on\nspecific subsets. To tackle this, we propose a novel loss function that\nincorporates the Wasserstein distance between the character frequency\ndistribution of the predicted text and a target distribution empirically\nderived from training data. By penalizing divergence from expected\ndistributions, our approach enhances both accuracy and robustness under\ntemporal and contextual intra-dataset shifts. Furthermore, we demonstrate that\ncharacter distribution alignment can also improve existing models at inference\ntime without requiring retraining by integrating it as a scoring function in a\nguided decoding scheme. Experimental results across multiple datasets and\narchitectures confirm the effectiveness of our method in boosting\ngeneralization and performance. We open source our code at\nhttps://github.com/pkaliosis/fada.\n", "link": "http://arxiv.org/abs/2506.09846v1", "date": "2025-06-11", "relevancy": 2.0057, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5097}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5024}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Align%3A%20Addressing%20Character%20Frequency%20Distribution%20Shifts%20in%0A%20%20Handwritten%20Text%20Recognition&body=Title%3A%20Learning%20to%20Align%3A%20Addressing%20Character%20Frequency%20Distribution%20Shifts%20in%0A%20%20Handwritten%20Text%20Recognition%0AAuthor%3A%20Panagiotis%20Kaliosis%20and%20John%20Pavlopoulos%0AAbstract%3A%20%20%20Handwritten%20text%20recognition%20aims%20to%20convert%20visual%20input%20into%0Amachine-readable%20text%2C%20and%20it%20remains%20challenging%20due%20to%20the%20evolving%20and%0Acontext-dependent%20nature%20of%20handwriting.%20Character%20sets%20change%20over%20time%2C%20and%0Acharacter%20frequency%20distributions%20shift%20across%20historical%20periods%20or%20regions%2C%0Aoften%20causing%20models%20trained%20on%20broad%2C%20heterogeneous%20corpora%20to%20underperform%20on%0Aspecific%20subsets.%20To%20tackle%20this%2C%20we%20propose%20a%20novel%20loss%20function%20that%0Aincorporates%20the%20Wasserstein%20distance%20between%20the%20character%20frequency%0Adistribution%20of%20the%20predicted%20text%20and%20a%20target%20distribution%20empirically%0Aderived%20from%20training%20data.%20By%20penalizing%20divergence%20from%20expected%0Adistributions%2C%20our%20approach%20enhances%20both%20accuracy%20and%20robustness%20under%0Atemporal%20and%20contextual%20intra-dataset%20shifts.%20Furthermore%2C%20we%20demonstrate%20that%0Acharacter%20distribution%20alignment%20can%20also%20improve%20existing%20models%20at%20inference%0Atime%20without%20requiring%20retraining%20by%20integrating%20it%20as%20a%20scoring%20function%20in%20a%0Aguided%20decoding%20scheme.%20Experimental%20results%20across%20multiple%20datasets%20and%0Aarchitectures%20confirm%20the%20effectiveness%20of%20our%20method%20in%20boosting%0Ageneralization%20and%20performance.%20We%20open%20source%20our%20code%20at%0Ahttps%3A//github.com/pkaliosis/fada.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09846v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Align%253A%2520Addressing%2520Character%2520Frequency%2520Distribution%2520Shifts%2520in%250A%2520%2520Handwritten%2520Text%2520Recognition%26entry.906535625%3DPanagiotis%2520Kaliosis%2520and%2520John%2520Pavlopoulos%26entry.1292438233%3D%2520%2520Handwritten%2520text%2520recognition%2520aims%2520to%2520convert%2520visual%2520input%2520into%250Amachine-readable%2520text%252C%2520and%2520it%2520remains%2520challenging%2520due%2520to%2520the%2520evolving%2520and%250Acontext-dependent%2520nature%2520of%2520handwriting.%2520Character%2520sets%2520change%2520over%2520time%252C%2520and%250Acharacter%2520frequency%2520distributions%2520shift%2520across%2520historical%2520periods%2520or%2520regions%252C%250Aoften%2520causing%2520models%2520trained%2520on%2520broad%252C%2520heterogeneous%2520corpora%2520to%2520underperform%2520on%250Aspecific%2520subsets.%2520To%2520tackle%2520this%252C%2520we%2520propose%2520a%2520novel%2520loss%2520function%2520that%250Aincorporates%2520the%2520Wasserstein%2520distance%2520between%2520the%2520character%2520frequency%250Adistribution%2520of%2520the%2520predicted%2520text%2520and%2520a%2520target%2520distribution%2520empirically%250Aderived%2520from%2520training%2520data.%2520By%2520penalizing%2520divergence%2520from%2520expected%250Adistributions%252C%2520our%2520approach%2520enhances%2520both%2520accuracy%2520and%2520robustness%2520under%250Atemporal%2520and%2520contextual%2520intra-dataset%2520shifts.%2520Furthermore%252C%2520we%2520demonstrate%2520that%250Acharacter%2520distribution%2520alignment%2520can%2520also%2520improve%2520existing%2520models%2520at%2520inference%250Atime%2520without%2520requiring%2520retraining%2520by%2520integrating%2520it%2520as%2520a%2520scoring%2520function%2520in%2520a%250Aguided%2520decoding%2520scheme.%2520Experimental%2520results%2520across%2520multiple%2520datasets%2520and%250Aarchitectures%2520confirm%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520boosting%250Ageneralization%2520and%2520performance.%2520We%2520open%2520source%2520our%2520code%2520at%250Ahttps%253A//github.com/pkaliosis/fada.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09846v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Align%3A%20Addressing%20Character%20Frequency%20Distribution%20Shifts%20in%0A%20%20Handwritten%20Text%20Recognition&entry.906535625=Panagiotis%20Kaliosis%20and%20John%20Pavlopoulos&entry.1292438233=%20%20Handwritten%20text%20recognition%20aims%20to%20convert%20visual%20input%20into%0Amachine-readable%20text%2C%20and%20it%20remains%20challenging%20due%20to%20the%20evolving%20and%0Acontext-dependent%20nature%20of%20handwriting.%20Character%20sets%20change%20over%20time%2C%20and%0Acharacter%20frequency%20distributions%20shift%20across%20historical%20periods%20or%20regions%2C%0Aoften%20causing%20models%20trained%20on%20broad%2C%20heterogeneous%20corpora%20to%20underperform%20on%0Aspecific%20subsets.%20To%20tackle%20this%2C%20we%20propose%20a%20novel%20loss%20function%20that%0Aincorporates%20the%20Wasserstein%20distance%20between%20the%20character%20frequency%0Adistribution%20of%20the%20predicted%20text%20and%20a%20target%20distribution%20empirically%0Aderived%20from%20training%20data.%20By%20penalizing%20divergence%20from%20expected%0Adistributions%2C%20our%20approach%20enhances%20both%20accuracy%20and%20robustness%20under%0Atemporal%20and%20contextual%20intra-dataset%20shifts.%20Furthermore%2C%20we%20demonstrate%20that%0Acharacter%20distribution%20alignment%20can%20also%20improve%20existing%20models%20at%20inference%0Atime%20without%20requiring%20retraining%20by%20integrating%20it%20as%20a%20scoring%20function%20in%20a%0Aguided%20decoding%20scheme.%20Experimental%20results%20across%20multiple%20datasets%20and%0Aarchitectures%20confirm%20the%20effectiveness%20of%20our%20method%20in%20boosting%0Ageneralization%20and%20performance.%20We%20open%20source%20our%20code%20at%0Ahttps%3A//github.com/pkaliosis/fada.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09846v1&entry.124074799=Read"},
{"title": "Learning single-index models via harmonic decomposition", "author": "Nirmit Joshi and Hugo Koubbi and Theodor Misiakiewicz and Nathan Srebro", "abstract": "  We study the problem of learning single-index models, where the label $y \\in\n\\mathbb{R}$ depends on the input $\\boldsymbol{x} \\in \\mathbb{R}^d$ only through\nan unknown one-dimensional projection $\\langle\n\\boldsymbol{w}_*,\\boldsymbol{x}\\rangle$. Prior work has shown that under\nGaussian inputs, the statistical and computational complexity of recovering\n$\\boldsymbol{w}_*$ is governed by the Hermite expansion of the link function.\nIn this paper, we propose a new perspective: we argue that \"spherical\nharmonics\" -- rather than \"Hermite polynomials\" -- provide the natural basis\nfor this problem, as they capture its intrinsic \"rotational symmetry\". Building\non this insight, we characterize the complexity of learning single-index models\nunder arbitrary spherically symmetric input distributions. We introduce two\nfamilies of estimators -- based on tensor unfolding and online SGD -- that\nrespectively achieve either optimal sample complexity or optimal runtime, and\nargue that estimators achieving both may not exist in general. When specialized\nto Gaussian inputs, our theory not only recovers and clarifies existing results\nbut also reveals new phenomena that had previously been overlooked.\n", "link": "http://arxiv.org/abs/2506.09887v1", "date": "2025-06-11", "relevancy": 1.9997, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5086}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5072}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20single-index%20models%20via%20harmonic%20decomposition&body=Title%3A%20Learning%20single-index%20models%20via%20harmonic%20decomposition%0AAuthor%3A%20Nirmit%20Joshi%20and%20Hugo%20Koubbi%20and%20Theodor%20Misiakiewicz%20and%20Nathan%20Srebro%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20learning%20single-index%20models%2C%20where%20the%20label%20%24y%20%5Cin%0A%5Cmathbb%7BR%7D%24%20depends%20on%20the%20input%20%24%5Cboldsymbol%7Bx%7D%20%5Cin%20%5Cmathbb%7BR%7D%5Ed%24%20only%20through%0Aan%20unknown%20one-dimensional%20projection%20%24%5Clangle%0A%5Cboldsymbol%7Bw%7D_%2A%2C%5Cboldsymbol%7Bx%7D%5Crangle%24.%20Prior%20work%20has%20shown%20that%20under%0AGaussian%20inputs%2C%20the%20statistical%20and%20computational%20complexity%20of%20recovering%0A%24%5Cboldsymbol%7Bw%7D_%2A%24%20is%20governed%20by%20the%20Hermite%20expansion%20of%20the%20link%20function.%0AIn%20this%20paper%2C%20we%20propose%20a%20new%20perspective%3A%20we%20argue%20that%20%22spherical%0Aharmonics%22%20--%20rather%20than%20%22Hermite%20polynomials%22%20--%20provide%20the%20natural%20basis%0Afor%20this%20problem%2C%20as%20they%20capture%20its%20intrinsic%20%22rotational%20symmetry%22.%20Building%0Aon%20this%20insight%2C%20we%20characterize%20the%20complexity%20of%20learning%20single-index%20models%0Aunder%20arbitrary%20spherically%20symmetric%20input%20distributions.%20We%20introduce%20two%0Afamilies%20of%20estimators%20--%20based%20on%20tensor%20unfolding%20and%20online%20SGD%20--%20that%0Arespectively%20achieve%20either%20optimal%20sample%20complexity%20or%20optimal%20runtime%2C%20and%0Aargue%20that%20estimators%20achieving%20both%20may%20not%20exist%20in%20general.%20When%20specialized%0Ato%20Gaussian%20inputs%2C%20our%20theory%20not%20only%20recovers%20and%20clarifies%20existing%20results%0Abut%20also%20reveals%20new%20phenomena%20that%20had%20previously%20been%20overlooked.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09887v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520single-index%2520models%2520via%2520harmonic%2520decomposition%26entry.906535625%3DNirmit%2520Joshi%2520and%2520Hugo%2520Koubbi%2520and%2520Theodor%2520Misiakiewicz%2520and%2520Nathan%2520Srebro%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520learning%2520single-index%2520models%252C%2520where%2520the%2520label%2520%2524y%2520%255Cin%250A%255Cmathbb%257BR%257D%2524%2520depends%2520on%2520the%2520input%2520%2524%255Cboldsymbol%257Bx%257D%2520%255Cin%2520%255Cmathbb%257BR%257D%255Ed%2524%2520only%2520through%250Aan%2520unknown%2520one-dimensional%2520projection%2520%2524%255Clangle%250A%255Cboldsymbol%257Bw%257D_%252A%252C%255Cboldsymbol%257Bx%257D%255Crangle%2524.%2520Prior%2520work%2520has%2520shown%2520that%2520under%250AGaussian%2520inputs%252C%2520the%2520statistical%2520and%2520computational%2520complexity%2520of%2520recovering%250A%2524%255Cboldsymbol%257Bw%257D_%252A%2524%2520is%2520governed%2520by%2520the%2520Hermite%2520expansion%2520of%2520the%2520link%2520function.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520perspective%253A%2520we%2520argue%2520that%2520%2522spherical%250Aharmonics%2522%2520--%2520rather%2520than%2520%2522Hermite%2520polynomials%2522%2520--%2520provide%2520the%2520natural%2520basis%250Afor%2520this%2520problem%252C%2520as%2520they%2520capture%2520its%2520intrinsic%2520%2522rotational%2520symmetry%2522.%2520Building%250Aon%2520this%2520insight%252C%2520we%2520characterize%2520the%2520complexity%2520of%2520learning%2520single-index%2520models%250Aunder%2520arbitrary%2520spherically%2520symmetric%2520input%2520distributions.%2520We%2520introduce%2520two%250Afamilies%2520of%2520estimators%2520--%2520based%2520on%2520tensor%2520unfolding%2520and%2520online%2520SGD%2520--%2520that%250Arespectively%2520achieve%2520either%2520optimal%2520sample%2520complexity%2520or%2520optimal%2520runtime%252C%2520and%250Aargue%2520that%2520estimators%2520achieving%2520both%2520may%2520not%2520exist%2520in%2520general.%2520When%2520specialized%250Ato%2520Gaussian%2520inputs%252C%2520our%2520theory%2520not%2520only%2520recovers%2520and%2520clarifies%2520existing%2520results%250Abut%2520also%2520reveals%2520new%2520phenomena%2520that%2520had%2520previously%2520been%2520overlooked.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09887v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20single-index%20models%20via%20harmonic%20decomposition&entry.906535625=Nirmit%20Joshi%20and%20Hugo%20Koubbi%20and%20Theodor%20Misiakiewicz%20and%20Nathan%20Srebro&entry.1292438233=%20%20We%20study%20the%20problem%20of%20learning%20single-index%20models%2C%20where%20the%20label%20%24y%20%5Cin%0A%5Cmathbb%7BR%7D%24%20depends%20on%20the%20input%20%24%5Cboldsymbol%7Bx%7D%20%5Cin%20%5Cmathbb%7BR%7D%5Ed%24%20only%20through%0Aan%20unknown%20one-dimensional%20projection%20%24%5Clangle%0A%5Cboldsymbol%7Bw%7D_%2A%2C%5Cboldsymbol%7Bx%7D%5Crangle%24.%20Prior%20work%20has%20shown%20that%20under%0AGaussian%20inputs%2C%20the%20statistical%20and%20computational%20complexity%20of%20recovering%0A%24%5Cboldsymbol%7Bw%7D_%2A%24%20is%20governed%20by%20the%20Hermite%20expansion%20of%20the%20link%20function.%0AIn%20this%20paper%2C%20we%20propose%20a%20new%20perspective%3A%20we%20argue%20that%20%22spherical%0Aharmonics%22%20--%20rather%20than%20%22Hermite%20polynomials%22%20--%20provide%20the%20natural%20basis%0Afor%20this%20problem%2C%20as%20they%20capture%20its%20intrinsic%20%22rotational%20symmetry%22.%20Building%0Aon%20this%20insight%2C%20we%20characterize%20the%20complexity%20of%20learning%20single-index%20models%0Aunder%20arbitrary%20spherically%20symmetric%20input%20distributions.%20We%20introduce%20two%0Afamilies%20of%20estimators%20--%20based%20on%20tensor%20unfolding%20and%20online%20SGD%20--%20that%0Arespectively%20achieve%20either%20optimal%20sample%20complexity%20or%20optimal%20runtime%2C%20and%0Aargue%20that%20estimators%20achieving%20both%20may%20not%20exist%20in%20general.%20When%20specialized%0Ato%20Gaussian%20inputs%2C%20our%20theory%20not%20only%20recovers%20and%20clarifies%20existing%20results%0Abut%20also%20reveals%20new%20phenomena%20that%20had%20previously%20been%20overlooked.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09887v1&entry.124074799=Read"},
{"title": "The Remarkable Robustness of LLMs: Stages of Inference?", "author": "Vedang Lad and Wes Gurnee and Max Tegmark", "abstract": "  We investigate the robustness of Large Language Models (LLMs) to structural\ninterventions by deleting and swapping adjacent layers during inference.\nSurprisingly, models retain 72-95% of their original top-1 prediction accuracy\nwithout any fine-tuning. We find that performance degradation is not uniform\nacross layers: interventions to the early and final layers cause the most\ndegradation, while the model is remarkably robust to dropping middle layers.\nThis pattern of localized sensitivity motivates our hypothesis of four stages\nof inference, observed across diverse model families and sizes: (1)\ndetokenization, where local context is integrated to lift raw token embeddings\ninto higher-level representations; (2) feature engineering, where task- and\nentity-specific features are iteratively refined; (3) prediction ensembling,\nwhere hidden states are aggregated into plausible next-token predictions; and\n(4) residual sharpening, where irrelevant features are suppressed to finalize\nthe output distribution. Synthesizing behavioral and mechanistic evidence, we\nprovide a framework for interpreting depth-dependent computations in LLMs.\n", "link": "http://arxiv.org/abs/2406.19384v2", "date": "2025-06-11", "relevancy": 1.9976, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5007}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5007}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Remarkable%20Robustness%20of%20LLMs%3A%20Stages%20of%20Inference%3F&body=Title%3A%20The%20Remarkable%20Robustness%20of%20LLMs%3A%20Stages%20of%20Inference%3F%0AAuthor%3A%20Vedang%20Lad%20and%20Wes%20Gurnee%20and%20Max%20Tegmark%0AAbstract%3A%20%20%20We%20investigate%20the%20robustness%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20structural%0Ainterventions%20by%20deleting%20and%20swapping%20adjacent%20layers%20during%20inference.%0ASurprisingly%2C%20models%20retain%2072-95%25%20of%20their%20original%20top-1%20prediction%20accuracy%0Awithout%20any%20fine-tuning.%20We%20find%20that%20performance%20degradation%20is%20not%20uniform%0Aacross%20layers%3A%20interventions%20to%20the%20early%20and%20final%20layers%20cause%20the%20most%0Adegradation%2C%20while%20the%20model%20is%20remarkably%20robust%20to%20dropping%20middle%20layers.%0AThis%20pattern%20of%20localized%20sensitivity%20motivates%20our%20hypothesis%20of%20four%20stages%0Aof%20inference%2C%20observed%20across%20diverse%20model%20families%20and%20sizes%3A%20%281%29%0Adetokenization%2C%20where%20local%20context%20is%20integrated%20to%20lift%20raw%20token%20embeddings%0Ainto%20higher-level%20representations%3B%20%282%29%20feature%20engineering%2C%20where%20task-%20and%0Aentity-specific%20features%20are%20iteratively%20refined%3B%20%283%29%20prediction%20ensembling%2C%0Awhere%20hidden%20states%20are%20aggregated%20into%20plausible%20next-token%20predictions%3B%20and%0A%284%29%20residual%20sharpening%2C%20where%20irrelevant%20features%20are%20suppressed%20to%20finalize%0Athe%20output%20distribution.%20Synthesizing%20behavioral%20and%20mechanistic%20evidence%2C%20we%0Aprovide%20a%20framework%20for%20interpreting%20depth-dependent%20computations%20in%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19384v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Remarkable%2520Robustness%2520of%2520LLMs%253A%2520Stages%2520of%2520Inference%253F%26entry.906535625%3DVedang%2520Lad%2520and%2520Wes%2520Gurnee%2520and%2520Max%2520Tegmark%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520robustness%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520structural%250Ainterventions%2520by%2520deleting%2520and%2520swapping%2520adjacent%2520layers%2520during%2520inference.%250ASurprisingly%252C%2520models%2520retain%252072-95%2525%2520of%2520their%2520original%2520top-1%2520prediction%2520accuracy%250Awithout%2520any%2520fine-tuning.%2520We%2520find%2520that%2520performance%2520degradation%2520is%2520not%2520uniform%250Aacross%2520layers%253A%2520interventions%2520to%2520the%2520early%2520and%2520final%2520layers%2520cause%2520the%2520most%250Adegradation%252C%2520while%2520the%2520model%2520is%2520remarkably%2520robust%2520to%2520dropping%2520middle%2520layers.%250AThis%2520pattern%2520of%2520localized%2520sensitivity%2520motivates%2520our%2520hypothesis%2520of%2520four%2520stages%250Aof%2520inference%252C%2520observed%2520across%2520diverse%2520model%2520families%2520and%2520sizes%253A%2520%25281%2529%250Adetokenization%252C%2520where%2520local%2520context%2520is%2520integrated%2520to%2520lift%2520raw%2520token%2520embeddings%250Ainto%2520higher-level%2520representations%253B%2520%25282%2529%2520feature%2520engineering%252C%2520where%2520task-%2520and%250Aentity-specific%2520features%2520are%2520iteratively%2520refined%253B%2520%25283%2529%2520prediction%2520ensembling%252C%250Awhere%2520hidden%2520states%2520are%2520aggregated%2520into%2520plausible%2520next-token%2520predictions%253B%2520and%250A%25284%2529%2520residual%2520sharpening%252C%2520where%2520irrelevant%2520features%2520are%2520suppressed%2520to%2520finalize%250Athe%2520output%2520distribution.%2520Synthesizing%2520behavioral%2520and%2520mechanistic%2520evidence%252C%2520we%250Aprovide%2520a%2520framework%2520for%2520interpreting%2520depth-dependent%2520computations%2520in%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19384v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Remarkable%20Robustness%20of%20LLMs%3A%20Stages%20of%20Inference%3F&entry.906535625=Vedang%20Lad%20and%20Wes%20Gurnee%20and%20Max%20Tegmark&entry.1292438233=%20%20We%20investigate%20the%20robustness%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20structural%0Ainterventions%20by%20deleting%20and%20swapping%20adjacent%20layers%20during%20inference.%0ASurprisingly%2C%20models%20retain%2072-95%25%20of%20their%20original%20top-1%20prediction%20accuracy%0Awithout%20any%20fine-tuning.%20We%20find%20that%20performance%20degradation%20is%20not%20uniform%0Aacross%20layers%3A%20interventions%20to%20the%20early%20and%20final%20layers%20cause%20the%20most%0Adegradation%2C%20while%20the%20model%20is%20remarkably%20robust%20to%20dropping%20middle%20layers.%0AThis%20pattern%20of%20localized%20sensitivity%20motivates%20our%20hypothesis%20of%20four%20stages%0Aof%20inference%2C%20observed%20across%20diverse%20model%20families%20and%20sizes%3A%20%281%29%0Adetokenization%2C%20where%20local%20context%20is%20integrated%20to%20lift%20raw%20token%20embeddings%0Ainto%20higher-level%20representations%3B%20%282%29%20feature%20engineering%2C%20where%20task-%20and%0Aentity-specific%20features%20are%20iteratively%20refined%3B%20%283%29%20prediction%20ensembling%2C%0Awhere%20hidden%20states%20are%20aggregated%20into%20plausible%20next-token%20predictions%3B%20and%0A%284%29%20residual%20sharpening%2C%20where%20irrelevant%20features%20are%20suppressed%20to%20finalize%0Athe%20output%20distribution.%20Synthesizing%20behavioral%20and%20mechanistic%20evidence%2C%20we%0Aprovide%20a%20framework%20for%20interpreting%20depth-dependent%20computations%20in%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19384v2&entry.124074799=Read"},
{"title": "Lost in Sequence: Do Large Language Models Understand Sequential\n  Recommendation?", "author": "Sein Kim and Hongseok Kang and Kibum Kim and Jiwan Kim and Donghyun Kim and Minchul Yang and Kwangjin Oh and Julian McAuley and Chanyoung Park", "abstract": "  Large Language Models (LLMs) have recently emerged as promising tools for\nrecommendation thanks to their advanced textual understanding ability and\ncontext-awareness. Despite the current practice of training and evaluating\nLLM-based recommendation (LLM4Rec) models under a sequential recommendation\nscenario, we found that whether these models understand the sequential\ninformation inherent in users' item interaction sequences has been largely\noverlooked. In this paper, we first demonstrate through a series of experiments\nthat existing LLM4Rec models do not fully capture sequential information both\nduring training and inference. Then, we propose a simple yet effective\nLLM-based sequential recommender, called LLM-SRec, a method that enhances the\nintegration of sequential information into LLMs by distilling the user\nrepresentations extracted from a pre-trained CF-SRec model into LLMs. Our\nextensive experiments show that LLM-SRec enhances LLMs' ability to understand\nusers' item interaction sequences, ultimately leading to improved\nrecommendation performance. Furthermore, unlike existing LLM4Rec models that\nrequire fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by\ntraining only a few lightweight MLPs, highlighting its practicality in\nreal-world applications. Our code is available at\nhttps://github.com/Sein-Kim/LLM-SRec.\n", "link": "http://arxiv.org/abs/2502.13909v4", "date": "2025-06-11", "relevancy": 1.9925, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.509}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.509}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lost%20in%20Sequence%3A%20Do%20Large%20Language%20Models%20Understand%20Sequential%0A%20%20Recommendation%3F&body=Title%3A%20Lost%20in%20Sequence%3A%20Do%20Large%20Language%20Models%20Understand%20Sequential%0A%20%20Recommendation%3F%0AAuthor%3A%20Sein%20Kim%20and%20Hongseok%20Kang%20and%20Kibum%20Kim%20and%20Jiwan%20Kim%20and%20Donghyun%20Kim%20and%20Minchul%20Yang%20and%20Kwangjin%20Oh%20and%20Julian%20McAuley%20and%20Chanyoung%20Park%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20emerged%20as%20promising%20tools%20for%0Arecommendation%20thanks%20to%20their%20advanced%20textual%20understanding%20ability%20and%0Acontext-awareness.%20Despite%20the%20current%20practice%20of%20training%20and%20evaluating%0ALLM-based%20recommendation%20%28LLM4Rec%29%20models%20under%20a%20sequential%20recommendation%0Ascenario%2C%20we%20found%20that%20whether%20these%20models%20understand%20the%20sequential%0Ainformation%20inherent%20in%20users%27%20item%20interaction%20sequences%20has%20been%20largely%0Aoverlooked.%20In%20this%20paper%2C%20we%20first%20demonstrate%20through%20a%20series%20of%20experiments%0Athat%20existing%20LLM4Rec%20models%20do%20not%20fully%20capture%20sequential%20information%20both%0Aduring%20training%20and%20inference.%20Then%2C%20we%20propose%20a%20simple%20yet%20effective%0ALLM-based%20sequential%20recommender%2C%20called%20LLM-SRec%2C%20a%20method%20that%20enhances%20the%0Aintegration%20of%20sequential%20information%20into%20LLMs%20by%20distilling%20the%20user%0Arepresentations%20extracted%20from%20a%20pre-trained%20CF-SRec%20model%20into%20LLMs.%20Our%0Aextensive%20experiments%20show%20that%20LLM-SRec%20enhances%20LLMs%27%20ability%20to%20understand%0Ausers%27%20item%20interaction%20sequences%2C%20ultimately%20leading%20to%20improved%0Arecommendation%20performance.%20Furthermore%2C%20unlike%20existing%20LLM4Rec%20models%20that%0Arequire%20fine-tuning%20of%20LLMs%2C%20LLM-SRec%20achieves%20state-of-the-art%20performance%20by%0Atraining%20only%20a%20few%20lightweight%20MLPs%2C%20highlighting%20its%20practicality%20in%0Areal-world%20applications.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Sein-Kim/LLM-SRec.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13909v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLost%2520in%2520Sequence%253A%2520Do%2520Large%2520Language%2520Models%2520Understand%2520Sequential%250A%2520%2520Recommendation%253F%26entry.906535625%3DSein%2520Kim%2520and%2520Hongseok%2520Kang%2520and%2520Kibum%2520Kim%2520and%2520Jiwan%2520Kim%2520and%2520Donghyun%2520Kim%2520and%2520Minchul%2520Yang%2520and%2520Kwangjin%2520Oh%2520and%2520Julian%2520McAuley%2520and%2520Chanyoung%2520Park%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520recently%2520emerged%2520as%2520promising%2520tools%2520for%250Arecommendation%2520thanks%2520to%2520their%2520advanced%2520textual%2520understanding%2520ability%2520and%250Acontext-awareness.%2520Despite%2520the%2520current%2520practice%2520of%2520training%2520and%2520evaluating%250ALLM-based%2520recommendation%2520%2528LLM4Rec%2529%2520models%2520under%2520a%2520sequential%2520recommendation%250Ascenario%252C%2520we%2520found%2520that%2520whether%2520these%2520models%2520understand%2520the%2520sequential%250Ainformation%2520inherent%2520in%2520users%2527%2520item%2520interaction%2520sequences%2520has%2520been%2520largely%250Aoverlooked.%2520In%2520this%2520paper%252C%2520we%2520first%2520demonstrate%2520through%2520a%2520series%2520of%2520experiments%250Athat%2520existing%2520LLM4Rec%2520models%2520do%2520not%2520fully%2520capture%2520sequential%2520information%2520both%250Aduring%2520training%2520and%2520inference.%2520Then%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%250ALLM-based%2520sequential%2520recommender%252C%2520called%2520LLM-SRec%252C%2520a%2520method%2520that%2520enhances%2520the%250Aintegration%2520of%2520sequential%2520information%2520into%2520LLMs%2520by%2520distilling%2520the%2520user%250Arepresentations%2520extracted%2520from%2520a%2520pre-trained%2520CF-SRec%2520model%2520into%2520LLMs.%2520Our%250Aextensive%2520experiments%2520show%2520that%2520LLM-SRec%2520enhances%2520LLMs%2527%2520ability%2520to%2520understand%250Ausers%2527%2520item%2520interaction%2520sequences%252C%2520ultimately%2520leading%2520to%2520improved%250Arecommendation%2520performance.%2520Furthermore%252C%2520unlike%2520existing%2520LLM4Rec%2520models%2520that%250Arequire%2520fine-tuning%2520of%2520LLMs%252C%2520LLM-SRec%2520achieves%2520state-of-the-art%2520performance%2520by%250Atraining%2520only%2520a%2520few%2520lightweight%2520MLPs%252C%2520highlighting%2520its%2520practicality%2520in%250Areal-world%2520applications.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Sein-Kim/LLM-SRec.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13909v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lost%20in%20Sequence%3A%20Do%20Large%20Language%20Models%20Understand%20Sequential%0A%20%20Recommendation%3F&entry.906535625=Sein%20Kim%20and%20Hongseok%20Kang%20and%20Kibum%20Kim%20and%20Jiwan%20Kim%20and%20Donghyun%20Kim%20and%20Minchul%20Yang%20and%20Kwangjin%20Oh%20and%20Julian%20McAuley%20and%20Chanyoung%20Park&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20emerged%20as%20promising%20tools%20for%0Arecommendation%20thanks%20to%20their%20advanced%20textual%20understanding%20ability%20and%0Acontext-awareness.%20Despite%20the%20current%20practice%20of%20training%20and%20evaluating%0ALLM-based%20recommendation%20%28LLM4Rec%29%20models%20under%20a%20sequential%20recommendation%0Ascenario%2C%20we%20found%20that%20whether%20these%20models%20understand%20the%20sequential%0Ainformation%20inherent%20in%20users%27%20item%20interaction%20sequences%20has%20been%20largely%0Aoverlooked.%20In%20this%20paper%2C%20we%20first%20demonstrate%20through%20a%20series%20of%20experiments%0Athat%20existing%20LLM4Rec%20models%20do%20not%20fully%20capture%20sequential%20information%20both%0Aduring%20training%20and%20inference.%20Then%2C%20we%20propose%20a%20simple%20yet%20effective%0ALLM-based%20sequential%20recommender%2C%20called%20LLM-SRec%2C%20a%20method%20that%20enhances%20the%0Aintegration%20of%20sequential%20information%20into%20LLMs%20by%20distilling%20the%20user%0Arepresentations%20extracted%20from%20a%20pre-trained%20CF-SRec%20model%20into%20LLMs.%20Our%0Aextensive%20experiments%20show%20that%20LLM-SRec%20enhances%20LLMs%27%20ability%20to%20understand%0Ausers%27%20item%20interaction%20sequences%2C%20ultimately%20leading%20to%20improved%0Arecommendation%20performance.%20Furthermore%2C%20unlike%20existing%20LLM4Rec%20models%20that%0Arequire%20fine-tuning%20of%20LLMs%2C%20LLM-SRec%20achieves%20state-of-the-art%20performance%20by%0Atraining%20only%20a%20few%20lightweight%20MLPs%2C%20highlighting%20its%20practicality%20in%0Areal-world%20applications.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Sein-Kim/LLM-SRec.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13909v4&entry.124074799=Read"},
{"title": "Gradient Aligned Regression via Pairwise Losses", "author": "Dixian Zhu and Tianbao Yang and Livnat Jerby", "abstract": "  Regression is a fundamental task in machine learning that has garnered\nextensive attention over the past decades. The conventional approach for\nregression involves employing loss functions that primarily concentrate on\naligning model prediction with the ground truth for each individual data\nsample. Recent research endeavors have introduced novel perspectives by\nincorporating label similarity to regression via imposing extra pairwise\nregularization on the latent feature space and demonstrated the effectiveness.\nHowever, there are two drawbacks for those approaches: i) their pairwise\noperation in latent feature space is computationally more expensive than\nconventional regression losses; ii) it lacks of theoretical justifications\nbehind such regularization. In this work, we propose GAR (Gradient Aligned\nRegression) as a competitive alternative method in label space, which is\nconstituted by a conventional regression loss and two pairwise label difference\nlosses for gradient alignment including magnitude and direction. GAR enjoys: i)\nthe same level efficiency as conventional regression loss because the quadratic\ncomplexity for the proposed pairwise losses can be reduced to linear\ncomplexity; ii) theoretical insights from learning the pairwise label\ndifference to learning the gradient of the ground truth function. We limit our\ncurrent scope as regression on the clean data setting without noises, outliers\nor distributional shifts, etc. We demonstrate the effectiveness of the proposed\nmethod practically on two synthetic datasets and on eight extensive real-world\ntasks from six benchmark datasets with other eight competitive baselines.\nRunning time experiments demonstrate the superior efficiency of the proposed\nGAR over existing methods with pairwise regularization in latent feature space\nand ablation studies demonstrate the effectiveness of each component for GAR.\n", "link": "http://arxiv.org/abs/2402.06104v6", "date": "2025-06-11", "relevancy": 1.9918, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5226}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4828}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Aligned%20Regression%20via%20Pairwise%20Losses&body=Title%3A%20Gradient%20Aligned%20Regression%20via%20Pairwise%20Losses%0AAuthor%3A%20Dixian%20Zhu%20and%20Tianbao%20Yang%20and%20Livnat%20Jerby%0AAbstract%3A%20%20%20Regression%20is%20a%20fundamental%20task%20in%20machine%20learning%20that%20has%20garnered%0Aextensive%20attention%20over%20the%20past%20decades.%20The%20conventional%20approach%20for%0Aregression%20involves%20employing%20loss%20functions%20that%20primarily%20concentrate%20on%0Aaligning%20model%20prediction%20with%20the%20ground%20truth%20for%20each%20individual%20data%0Asample.%20Recent%20research%20endeavors%20have%20introduced%20novel%20perspectives%20by%0Aincorporating%20label%20similarity%20to%20regression%20via%20imposing%20extra%20pairwise%0Aregularization%20on%20the%20latent%20feature%20space%20and%20demonstrated%20the%20effectiveness.%0AHowever%2C%20there%20are%20two%20drawbacks%20for%20those%20approaches%3A%20i%29%20their%20pairwise%0Aoperation%20in%20latent%20feature%20space%20is%20computationally%20more%20expensive%20than%0Aconventional%20regression%20losses%3B%20ii%29%20it%20lacks%20of%20theoretical%20justifications%0Abehind%20such%20regularization.%20In%20this%20work%2C%20we%20propose%20GAR%20%28Gradient%20Aligned%0ARegression%29%20as%20a%20competitive%20alternative%20method%20in%20label%20space%2C%20which%20is%0Aconstituted%20by%20a%20conventional%20regression%20loss%20and%20two%20pairwise%20label%20difference%0Alosses%20for%20gradient%20alignment%20including%20magnitude%20and%20direction.%20GAR%20enjoys%3A%20i%29%0Athe%20same%20level%20efficiency%20as%20conventional%20regression%20loss%20because%20the%20quadratic%0Acomplexity%20for%20the%20proposed%20pairwise%20losses%20can%20be%20reduced%20to%20linear%0Acomplexity%3B%20ii%29%20theoretical%20insights%20from%20learning%20the%20pairwise%20label%0Adifference%20to%20learning%20the%20gradient%20of%20the%20ground%20truth%20function.%20We%20limit%20our%0Acurrent%20scope%20as%20regression%20on%20the%20clean%20data%20setting%20without%20noises%2C%20outliers%0Aor%20distributional%20shifts%2C%20etc.%20We%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Amethod%20practically%20on%20two%20synthetic%20datasets%20and%20on%20eight%20extensive%20real-world%0Atasks%20from%20six%20benchmark%20datasets%20with%20other%20eight%20competitive%20baselines.%0ARunning%20time%20experiments%20demonstrate%20the%20superior%20efficiency%20of%20the%20proposed%0AGAR%20over%20existing%20methods%20with%20pairwise%20regularization%20in%20latent%20feature%20space%0Aand%20ablation%20studies%20demonstrate%20the%20effectiveness%20of%20each%20component%20for%20GAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06104v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Aligned%2520Regression%2520via%2520Pairwise%2520Losses%26entry.906535625%3DDixian%2520Zhu%2520and%2520Tianbao%2520Yang%2520and%2520Livnat%2520Jerby%26entry.1292438233%3D%2520%2520Regression%2520is%2520a%2520fundamental%2520task%2520in%2520machine%2520learning%2520that%2520has%2520garnered%250Aextensive%2520attention%2520over%2520the%2520past%2520decades.%2520The%2520conventional%2520approach%2520for%250Aregression%2520involves%2520employing%2520loss%2520functions%2520that%2520primarily%2520concentrate%2520on%250Aaligning%2520model%2520prediction%2520with%2520the%2520ground%2520truth%2520for%2520each%2520individual%2520data%250Asample.%2520Recent%2520research%2520endeavors%2520have%2520introduced%2520novel%2520perspectives%2520by%250Aincorporating%2520label%2520similarity%2520to%2520regression%2520via%2520imposing%2520extra%2520pairwise%250Aregularization%2520on%2520the%2520latent%2520feature%2520space%2520and%2520demonstrated%2520the%2520effectiveness.%250AHowever%252C%2520there%2520are%2520two%2520drawbacks%2520for%2520those%2520approaches%253A%2520i%2529%2520their%2520pairwise%250Aoperation%2520in%2520latent%2520feature%2520space%2520is%2520computationally%2520more%2520expensive%2520than%250Aconventional%2520regression%2520losses%253B%2520ii%2529%2520it%2520lacks%2520of%2520theoretical%2520justifications%250Abehind%2520such%2520regularization.%2520In%2520this%2520work%252C%2520we%2520propose%2520GAR%2520%2528Gradient%2520Aligned%250ARegression%2529%2520as%2520a%2520competitive%2520alternative%2520method%2520in%2520label%2520space%252C%2520which%2520is%250Aconstituted%2520by%2520a%2520conventional%2520regression%2520loss%2520and%2520two%2520pairwise%2520label%2520difference%250Alosses%2520for%2520gradient%2520alignment%2520including%2520magnitude%2520and%2520direction.%2520GAR%2520enjoys%253A%2520i%2529%250Athe%2520same%2520level%2520efficiency%2520as%2520conventional%2520regression%2520loss%2520because%2520the%2520quadratic%250Acomplexity%2520for%2520the%2520proposed%2520pairwise%2520losses%2520can%2520be%2520reduced%2520to%2520linear%250Acomplexity%253B%2520ii%2529%2520theoretical%2520insights%2520from%2520learning%2520the%2520pairwise%2520label%250Adifference%2520to%2520learning%2520the%2520gradient%2520of%2520the%2520ground%2520truth%2520function.%2520We%2520limit%2520our%250Acurrent%2520scope%2520as%2520regression%2520on%2520the%2520clean%2520data%2520setting%2520without%2520noises%252C%2520outliers%250Aor%2520distributional%2520shifts%252C%2520etc.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%250Amethod%2520practically%2520on%2520two%2520synthetic%2520datasets%2520and%2520on%2520eight%2520extensive%2520real-world%250Atasks%2520from%2520six%2520benchmark%2520datasets%2520with%2520other%2520eight%2520competitive%2520baselines.%250ARunning%2520time%2520experiments%2520demonstrate%2520the%2520superior%2520efficiency%2520of%2520the%2520proposed%250AGAR%2520over%2520existing%2520methods%2520with%2520pairwise%2520regularization%2520in%2520latent%2520feature%2520space%250Aand%2520ablation%2520studies%2520demonstrate%2520the%2520effectiveness%2520of%2520each%2520component%2520for%2520GAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.06104v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Aligned%20Regression%20via%20Pairwise%20Losses&entry.906535625=Dixian%20Zhu%20and%20Tianbao%20Yang%20and%20Livnat%20Jerby&entry.1292438233=%20%20Regression%20is%20a%20fundamental%20task%20in%20machine%20learning%20that%20has%20garnered%0Aextensive%20attention%20over%20the%20past%20decades.%20The%20conventional%20approach%20for%0Aregression%20involves%20employing%20loss%20functions%20that%20primarily%20concentrate%20on%0Aaligning%20model%20prediction%20with%20the%20ground%20truth%20for%20each%20individual%20data%0Asample.%20Recent%20research%20endeavors%20have%20introduced%20novel%20perspectives%20by%0Aincorporating%20label%20similarity%20to%20regression%20via%20imposing%20extra%20pairwise%0Aregularization%20on%20the%20latent%20feature%20space%20and%20demonstrated%20the%20effectiveness.%0AHowever%2C%20there%20are%20two%20drawbacks%20for%20those%20approaches%3A%20i%29%20their%20pairwise%0Aoperation%20in%20latent%20feature%20space%20is%20computationally%20more%20expensive%20than%0Aconventional%20regression%20losses%3B%20ii%29%20it%20lacks%20of%20theoretical%20justifications%0Abehind%20such%20regularization.%20In%20this%20work%2C%20we%20propose%20GAR%20%28Gradient%20Aligned%0ARegression%29%20as%20a%20competitive%20alternative%20method%20in%20label%20space%2C%20which%20is%0Aconstituted%20by%20a%20conventional%20regression%20loss%20and%20two%20pairwise%20label%20difference%0Alosses%20for%20gradient%20alignment%20including%20magnitude%20and%20direction.%20GAR%20enjoys%3A%20i%29%0Athe%20same%20level%20efficiency%20as%20conventional%20regression%20loss%20because%20the%20quadratic%0Acomplexity%20for%20the%20proposed%20pairwise%20losses%20can%20be%20reduced%20to%20linear%0Acomplexity%3B%20ii%29%20theoretical%20insights%20from%20learning%20the%20pairwise%20label%0Adifference%20to%20learning%20the%20gradient%20of%20the%20ground%20truth%20function.%20We%20limit%20our%0Acurrent%20scope%20as%20regression%20on%20the%20clean%20data%20setting%20without%20noises%2C%20outliers%0Aor%20distributional%20shifts%2C%20etc.%20We%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Amethod%20practically%20on%20two%20synthetic%20datasets%20and%20on%20eight%20extensive%20real-world%0Atasks%20from%20six%20benchmark%20datasets%20with%20other%20eight%20competitive%20baselines.%0ARunning%20time%20experiments%20demonstrate%20the%20superior%20efficiency%20of%20the%20proposed%0AGAR%20over%20existing%20methods%20with%20pairwise%20regularization%20in%20latent%20feature%20space%0Aand%20ablation%20studies%20demonstrate%20the%20effectiveness%20of%20each%20component%20for%20GAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06104v6&entry.124074799=Read"},
{"title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust\n  MedVQA in Gastrointestinal Endoscopy", "author": "Sushant Gautam and Michael A. Riegler and P\u00e5l Halvorsen", "abstract": "  Medical Visual Question Answering (MedVQA) is a promising field for\ndeveloping clinical decision support systems, yet progress is often limited by\nthe available datasets, which can lack clinical complexity and visual\ndiversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,\nlarge-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly\nexpands upon the original Kvasir-VQA by incorporating 159,549 new\nquestion-answer pairs that are designed to test deeper clinical reasoning. We\ndeveloped a systematic method using large language models to generate these\nquestions, which are stratified by complexity to better assess a model's\ninference capabilities. To ensure our dataset prepares models for real-world\nclinical scenarios, we have also introduced a variety of visual augmentations\nthat mimic common imaging artifacts. The dataset is structured to support two\nmain evaluation tracks: one for standard VQA performance and another to test\nmodel robustness against these visual perturbations. By providing a more\nchallenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate\nthe development of more reliable and effective multimodal AI systems for use in\nclinical settings. The dataset is fully accessible and adheres to FAIR data\nprinciples, making it a valuable resource for the wider research community.\nCode and data: https://github.com/Simula/Kvasir-VQA-x1 and\nhttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1\n", "link": "http://arxiv.org/abs/2506.09958v1", "date": "2025-06-11", "relevancy": 1.991, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4999}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kvasir-VQA-x1%3A%20A%20Multimodal%20Dataset%20for%20Medical%20Reasoning%20and%20Robust%0A%20%20MedVQA%20in%20Gastrointestinal%20Endoscopy&body=Title%3A%20Kvasir-VQA-x1%3A%20A%20Multimodal%20Dataset%20for%20Medical%20Reasoning%20and%20Robust%0A%20%20MedVQA%20in%20Gastrointestinal%20Endoscopy%0AAuthor%3A%20Sushant%20Gautam%20and%20Michael%20A.%20Riegler%20and%20P%C3%A5l%20Halvorsen%0AAbstract%3A%20%20%20Medical%20Visual%20Question%20Answering%20%28MedVQA%29%20is%20a%20promising%20field%20for%0Adeveloping%20clinical%20decision%20support%20systems%2C%20yet%20progress%20is%20often%20limited%20by%0Athe%20available%20datasets%2C%20which%20can%20lack%20clinical%20complexity%20and%20visual%0Adiversity.%20To%20address%20these%20gaps%2C%20we%20introduce%20Kvasir-VQA-x1%2C%20a%20new%2C%0Alarge-scale%20dataset%20for%20gastrointestinal%20%28GI%29%20endoscopy.%20Our%20work%20significantly%0Aexpands%20upon%20the%20original%20Kvasir-VQA%20by%20incorporating%20159%2C549%20new%0Aquestion-answer%20pairs%20that%20are%20designed%20to%20test%20deeper%20clinical%20reasoning.%20We%0Adeveloped%20a%20systematic%20method%20using%20large%20language%20models%20to%20generate%20these%0Aquestions%2C%20which%20are%20stratified%20by%20complexity%20to%20better%20assess%20a%20model%27s%0Ainference%20capabilities.%20To%20ensure%20our%20dataset%20prepares%20models%20for%20real-world%0Aclinical%20scenarios%2C%20we%20have%20also%20introduced%20a%20variety%20of%20visual%20augmentations%0Athat%20mimic%20common%20imaging%20artifacts.%20The%20dataset%20is%20structured%20to%20support%20two%0Amain%20evaluation%20tracks%3A%20one%20for%20standard%20VQA%20performance%20and%20another%20to%20test%0Amodel%20robustness%20against%20these%20visual%20perturbations.%20By%20providing%20a%20more%0Achallenging%20and%20clinically%20relevant%20benchmark%2C%20Kvasir-VQA-x1%20aims%20to%20accelerate%0Athe%20development%20of%20more%20reliable%20and%20effective%20multimodal%20AI%20systems%20for%20use%20in%0Aclinical%20settings.%20The%20dataset%20is%20fully%20accessible%20and%20adheres%20to%20FAIR%20data%0Aprinciples%2C%20making%20it%20a%20valuable%20resource%20for%20the%20wider%20research%20community.%0ACode%20and%20data%3A%20https%3A//github.com/Simula/Kvasir-VQA-x1%20and%0Ahttps%3A//huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKvasir-VQA-x1%253A%2520A%2520Multimodal%2520Dataset%2520for%2520Medical%2520Reasoning%2520and%2520Robust%250A%2520%2520MedVQA%2520in%2520Gastrointestinal%2520Endoscopy%26entry.906535625%3DSushant%2520Gautam%2520and%2520Michael%2520A.%2520Riegler%2520and%2520P%25C3%25A5l%2520Halvorsen%26entry.1292438233%3D%2520%2520Medical%2520Visual%2520Question%2520Answering%2520%2528MedVQA%2529%2520is%2520a%2520promising%2520field%2520for%250Adeveloping%2520clinical%2520decision%2520support%2520systems%252C%2520yet%2520progress%2520is%2520often%2520limited%2520by%250Athe%2520available%2520datasets%252C%2520which%2520can%2520lack%2520clinical%2520complexity%2520and%2520visual%250Adiversity.%2520To%2520address%2520these%2520gaps%252C%2520we%2520introduce%2520Kvasir-VQA-x1%252C%2520a%2520new%252C%250Alarge-scale%2520dataset%2520for%2520gastrointestinal%2520%2528GI%2529%2520endoscopy.%2520Our%2520work%2520significantly%250Aexpands%2520upon%2520the%2520original%2520Kvasir-VQA%2520by%2520incorporating%2520159%252C549%2520new%250Aquestion-answer%2520pairs%2520that%2520are%2520designed%2520to%2520test%2520deeper%2520clinical%2520reasoning.%2520We%250Adeveloped%2520a%2520systematic%2520method%2520using%2520large%2520language%2520models%2520to%2520generate%2520these%250Aquestions%252C%2520which%2520are%2520stratified%2520by%2520complexity%2520to%2520better%2520assess%2520a%2520model%2527s%250Ainference%2520capabilities.%2520To%2520ensure%2520our%2520dataset%2520prepares%2520models%2520for%2520real-world%250Aclinical%2520scenarios%252C%2520we%2520have%2520also%2520introduced%2520a%2520variety%2520of%2520visual%2520augmentations%250Athat%2520mimic%2520common%2520imaging%2520artifacts.%2520The%2520dataset%2520is%2520structured%2520to%2520support%2520two%250Amain%2520evaluation%2520tracks%253A%2520one%2520for%2520standard%2520VQA%2520performance%2520and%2520another%2520to%2520test%250Amodel%2520robustness%2520against%2520these%2520visual%2520perturbations.%2520By%2520providing%2520a%2520more%250Achallenging%2520and%2520clinically%2520relevant%2520benchmark%252C%2520Kvasir-VQA-x1%2520aims%2520to%2520accelerate%250Athe%2520development%2520of%2520more%2520reliable%2520and%2520effective%2520multimodal%2520AI%2520systems%2520for%2520use%2520in%250Aclinical%2520settings.%2520The%2520dataset%2520is%2520fully%2520accessible%2520and%2520adheres%2520to%2520FAIR%2520data%250Aprinciples%252C%2520making%2520it%2520a%2520valuable%2520resource%2520for%2520the%2520wider%2520research%2520community.%250ACode%2520and%2520data%253A%2520https%253A//github.com/Simula/Kvasir-VQA-x1%2520and%250Ahttps%253A//huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kvasir-VQA-x1%3A%20A%20Multimodal%20Dataset%20for%20Medical%20Reasoning%20and%20Robust%0A%20%20MedVQA%20in%20Gastrointestinal%20Endoscopy&entry.906535625=Sushant%20Gautam%20and%20Michael%20A.%20Riegler%20and%20P%C3%A5l%20Halvorsen&entry.1292438233=%20%20Medical%20Visual%20Question%20Answering%20%28MedVQA%29%20is%20a%20promising%20field%20for%0Adeveloping%20clinical%20decision%20support%20systems%2C%20yet%20progress%20is%20often%20limited%20by%0Athe%20available%20datasets%2C%20which%20can%20lack%20clinical%20complexity%20and%20visual%0Adiversity.%20To%20address%20these%20gaps%2C%20we%20introduce%20Kvasir-VQA-x1%2C%20a%20new%2C%0Alarge-scale%20dataset%20for%20gastrointestinal%20%28GI%29%20endoscopy.%20Our%20work%20significantly%0Aexpands%20upon%20the%20original%20Kvasir-VQA%20by%20incorporating%20159%2C549%20new%0Aquestion-answer%20pairs%20that%20are%20designed%20to%20test%20deeper%20clinical%20reasoning.%20We%0Adeveloped%20a%20systematic%20method%20using%20large%20language%20models%20to%20generate%20these%0Aquestions%2C%20which%20are%20stratified%20by%20complexity%20to%20better%20assess%20a%20model%27s%0Ainference%20capabilities.%20To%20ensure%20our%20dataset%20prepares%20models%20for%20real-world%0Aclinical%20scenarios%2C%20we%20have%20also%20introduced%20a%20variety%20of%20visual%20augmentations%0Athat%20mimic%20common%20imaging%20artifacts.%20The%20dataset%20is%20structured%20to%20support%20two%0Amain%20evaluation%20tracks%3A%20one%20for%20standard%20VQA%20performance%20and%20another%20to%20test%0Amodel%20robustness%20against%20these%20visual%20perturbations.%20By%20providing%20a%20more%0Achallenging%20and%20clinically%20relevant%20benchmark%2C%20Kvasir-VQA-x1%20aims%20to%20accelerate%0Athe%20development%20of%20more%20reliable%20and%20effective%20multimodal%20AI%20systems%20for%20use%20in%0Aclinical%20settings.%20The%20dataset%20is%20fully%20accessible%20and%20adheres%20to%20FAIR%20data%0Aprinciples%2C%20making%20it%20a%20valuable%20resource%20for%20the%20wider%20research%20community.%0ACode%20and%20data%3A%20https%3A//github.com/Simula/Kvasir-VQA-x1%20and%0Ahttps%3A//huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09958v1&entry.124074799=Read"},
{"title": "A Deep Generative Model for the Simulation of Discrete Karst Networks", "author": "Dany Lauzon and Julien Straubhaar and Philippe Renard", "abstract": "  The simulation of discrete karst networks presents a significant challenge\ndue to the complexity of the physicochemical processes occurring within various\ngeological and hydrogeological contexts over extended periods. This complex\ninterplay leads to a wide variety of karst network patterns, each intricately\nlinked to specific hydrogeological conditions. We explore a novel approach that\nrepresents karst networks as graphs and applies graph generative models (deep\nlearning techniques) to capture the intricate nature of karst environments. In\nthis representation, nodes retain spatial information and properties, while\nedges signify connections between nodes. Our generative process consists of two\nmain steps. First, we utilize graph recurrent neural networks (GraphRNN) to\nlearn the topological distribution of karst networks. GraphRNN decomposes the\ngraph simulation into a sequential generation of nodes and edges, informed by\npreviously generated structures. Second, we employ denoising diffusion\nprobabilistic models on graphs (G-DDPM) to learn node features (spatial\ncoordinates and other properties). G-DDPMs enable the generation of nodes\nfeatures on the graphs produced by the GraphRNN that adhere to the learned\nstatistical properties by sampling from the derived probability distribution,\nensuring that the generated graphs are realistic and capture the essential\nfeatures of the original data. We test our approach using real-world karst\nnetworks and compare generated subgraphs with actual subgraphs from the\ndatabase, by using geometry and topology metrics. Our methodology allows\nstochastic simulation of discrete karst networks across various types of\nformations, a useful tool for studying the behavior of physical processes such\nas flow and transport.\n", "link": "http://arxiv.org/abs/2506.09832v1", "date": "2025-06-11", "relevancy": 1.9865, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5098}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4875}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Deep%20Generative%20Model%20for%20the%20Simulation%20of%20Discrete%20Karst%20Networks&body=Title%3A%20A%20Deep%20Generative%20Model%20for%20the%20Simulation%20of%20Discrete%20Karst%20Networks%0AAuthor%3A%20Dany%20Lauzon%20and%20Julien%20Straubhaar%20and%20Philippe%20Renard%0AAbstract%3A%20%20%20The%20simulation%20of%20discrete%20karst%20networks%20presents%20a%20significant%20challenge%0Adue%20to%20the%20complexity%20of%20the%20physicochemical%20processes%20occurring%20within%20various%0Ageological%20and%20hydrogeological%20contexts%20over%20extended%20periods.%20This%20complex%0Ainterplay%20leads%20to%20a%20wide%20variety%20of%20karst%20network%20patterns%2C%20each%20intricately%0Alinked%20to%20specific%20hydrogeological%20conditions.%20We%20explore%20a%20novel%20approach%20that%0Arepresents%20karst%20networks%20as%20graphs%20and%20applies%20graph%20generative%20models%20%28deep%0Alearning%20techniques%29%20to%20capture%20the%20intricate%20nature%20of%20karst%20environments.%20In%0Athis%20representation%2C%20nodes%20retain%20spatial%20information%20and%20properties%2C%20while%0Aedges%20signify%20connections%20between%20nodes.%20Our%20generative%20process%20consists%20of%20two%0Amain%20steps.%20First%2C%20we%20utilize%20graph%20recurrent%20neural%20networks%20%28GraphRNN%29%20to%0Alearn%20the%20topological%20distribution%20of%20karst%20networks.%20GraphRNN%20decomposes%20the%0Agraph%20simulation%20into%20a%20sequential%20generation%20of%20nodes%20and%20edges%2C%20informed%20by%0Apreviously%20generated%20structures.%20Second%2C%20we%20employ%20denoising%20diffusion%0Aprobabilistic%20models%20on%20graphs%20%28G-DDPM%29%20to%20learn%20node%20features%20%28spatial%0Acoordinates%20and%20other%20properties%29.%20G-DDPMs%20enable%20the%20generation%20of%20nodes%0Afeatures%20on%20the%20graphs%20produced%20by%20the%20GraphRNN%20that%20adhere%20to%20the%20learned%0Astatistical%20properties%20by%20sampling%20from%20the%20derived%20probability%20distribution%2C%0Aensuring%20that%20the%20generated%20graphs%20are%20realistic%20and%20capture%20the%20essential%0Afeatures%20of%20the%20original%20data.%20We%20test%20our%20approach%20using%20real-world%20karst%0Anetworks%20and%20compare%20generated%20subgraphs%20with%20actual%20subgraphs%20from%20the%0Adatabase%2C%20by%20using%20geometry%20and%20topology%20metrics.%20Our%20methodology%20allows%0Astochastic%20simulation%20of%20discrete%20karst%20networks%20across%20various%20types%20of%0Aformations%2C%20a%20useful%20tool%20for%20studying%20the%20behavior%20of%20physical%20processes%20such%0Aas%20flow%20and%20transport.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Deep%2520Generative%2520Model%2520for%2520the%2520Simulation%2520of%2520Discrete%2520Karst%2520Networks%26entry.906535625%3DDany%2520Lauzon%2520and%2520Julien%2520Straubhaar%2520and%2520Philippe%2520Renard%26entry.1292438233%3D%2520%2520The%2520simulation%2520of%2520discrete%2520karst%2520networks%2520presents%2520a%2520significant%2520challenge%250Adue%2520to%2520the%2520complexity%2520of%2520the%2520physicochemical%2520processes%2520occurring%2520within%2520various%250Ageological%2520and%2520hydrogeological%2520contexts%2520over%2520extended%2520periods.%2520This%2520complex%250Ainterplay%2520leads%2520to%2520a%2520wide%2520variety%2520of%2520karst%2520network%2520patterns%252C%2520each%2520intricately%250Alinked%2520to%2520specific%2520hydrogeological%2520conditions.%2520We%2520explore%2520a%2520novel%2520approach%2520that%250Arepresents%2520karst%2520networks%2520as%2520graphs%2520and%2520applies%2520graph%2520generative%2520models%2520%2528deep%250Alearning%2520techniques%2529%2520to%2520capture%2520the%2520intricate%2520nature%2520of%2520karst%2520environments.%2520In%250Athis%2520representation%252C%2520nodes%2520retain%2520spatial%2520information%2520and%2520properties%252C%2520while%250Aedges%2520signify%2520connections%2520between%2520nodes.%2520Our%2520generative%2520process%2520consists%2520of%2520two%250Amain%2520steps.%2520First%252C%2520we%2520utilize%2520graph%2520recurrent%2520neural%2520networks%2520%2528GraphRNN%2529%2520to%250Alearn%2520the%2520topological%2520distribution%2520of%2520karst%2520networks.%2520GraphRNN%2520decomposes%2520the%250Agraph%2520simulation%2520into%2520a%2520sequential%2520generation%2520of%2520nodes%2520and%2520edges%252C%2520informed%2520by%250Apreviously%2520generated%2520structures.%2520Second%252C%2520we%2520employ%2520denoising%2520diffusion%250Aprobabilistic%2520models%2520on%2520graphs%2520%2528G-DDPM%2529%2520to%2520learn%2520node%2520features%2520%2528spatial%250Acoordinates%2520and%2520other%2520properties%2529.%2520G-DDPMs%2520enable%2520the%2520generation%2520of%2520nodes%250Afeatures%2520on%2520the%2520graphs%2520produced%2520by%2520the%2520GraphRNN%2520that%2520adhere%2520to%2520the%2520learned%250Astatistical%2520properties%2520by%2520sampling%2520from%2520the%2520derived%2520probability%2520distribution%252C%250Aensuring%2520that%2520the%2520generated%2520graphs%2520are%2520realistic%2520and%2520capture%2520the%2520essential%250Afeatures%2520of%2520the%2520original%2520data.%2520We%2520test%2520our%2520approach%2520using%2520real-world%2520karst%250Anetworks%2520and%2520compare%2520generated%2520subgraphs%2520with%2520actual%2520subgraphs%2520from%2520the%250Adatabase%252C%2520by%2520using%2520geometry%2520and%2520topology%2520metrics.%2520Our%2520methodology%2520allows%250Astochastic%2520simulation%2520of%2520discrete%2520karst%2520networks%2520across%2520various%2520types%2520of%250Aformations%252C%2520a%2520useful%2520tool%2520for%2520studying%2520the%2520behavior%2520of%2520physical%2520processes%2520such%250Aas%2520flow%2520and%2520transport.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Deep%20Generative%20Model%20for%20the%20Simulation%20of%20Discrete%20Karst%20Networks&entry.906535625=Dany%20Lauzon%20and%20Julien%20Straubhaar%20and%20Philippe%20Renard&entry.1292438233=%20%20The%20simulation%20of%20discrete%20karst%20networks%20presents%20a%20significant%20challenge%0Adue%20to%20the%20complexity%20of%20the%20physicochemical%20processes%20occurring%20within%20various%0Ageological%20and%20hydrogeological%20contexts%20over%20extended%20periods.%20This%20complex%0Ainterplay%20leads%20to%20a%20wide%20variety%20of%20karst%20network%20patterns%2C%20each%20intricately%0Alinked%20to%20specific%20hydrogeological%20conditions.%20We%20explore%20a%20novel%20approach%20that%0Arepresents%20karst%20networks%20as%20graphs%20and%20applies%20graph%20generative%20models%20%28deep%0Alearning%20techniques%29%20to%20capture%20the%20intricate%20nature%20of%20karst%20environments.%20In%0Athis%20representation%2C%20nodes%20retain%20spatial%20information%20and%20properties%2C%20while%0Aedges%20signify%20connections%20between%20nodes.%20Our%20generative%20process%20consists%20of%20two%0Amain%20steps.%20First%2C%20we%20utilize%20graph%20recurrent%20neural%20networks%20%28GraphRNN%29%20to%0Alearn%20the%20topological%20distribution%20of%20karst%20networks.%20GraphRNN%20decomposes%20the%0Agraph%20simulation%20into%20a%20sequential%20generation%20of%20nodes%20and%20edges%2C%20informed%20by%0Apreviously%20generated%20structures.%20Second%2C%20we%20employ%20denoising%20diffusion%0Aprobabilistic%20models%20on%20graphs%20%28G-DDPM%29%20to%20learn%20node%20features%20%28spatial%0Acoordinates%20and%20other%20properties%29.%20G-DDPMs%20enable%20the%20generation%20of%20nodes%0Afeatures%20on%20the%20graphs%20produced%20by%20the%20GraphRNN%20that%20adhere%20to%20the%20learned%0Astatistical%20properties%20by%20sampling%20from%20the%20derived%20probability%20distribution%2C%0Aensuring%20that%20the%20generated%20graphs%20are%20realistic%20and%20capture%20the%20essential%0Afeatures%20of%20the%20original%20data.%20We%20test%20our%20approach%20using%20real-world%20karst%0Anetworks%20and%20compare%20generated%20subgraphs%20with%20actual%20subgraphs%20from%20the%0Adatabase%2C%20by%20using%20geometry%20and%20topology%20metrics.%20Our%20methodology%20allows%0Astochastic%20simulation%20of%20discrete%20karst%20networks%20across%20various%20types%20of%0Aformations%2C%20a%20useful%20tool%20for%20studying%20the%20behavior%20of%20physical%20processes%20such%0Aas%20flow%20and%20transport.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09832v1&entry.124074799=Read"},
{"title": "Cross-Channel Unlabeled Sensing over a Union of Signal Subspaces", "author": "Taulant Koka and Manolis C. Tsakiris and Benjam\u00edn B\u00e9jar Haro and Michael Muma", "abstract": "  Cross-channel unlabeled sensing addresses the problem of recovering a\nmulti-channel signal from measurements that were shuffled across channels. This\nwork expands the cross-channel unlabeled sensing framework to signals that lie\nin a union of subspaces. The extension allows for handling more complex signal\nstructures and broadens the framework to tasks like compressed sensing. These\nmismatches between samples and channels often arise in applications such as\nwhole-brain calcium imaging of freely moving organisms or multi-target\ntracking. We improve over previous models by deriving tighter bounds on the\nrequired number of samples for unique reconstruction, while supporting more\ngeneral signal types. The approach is validated through an application in\nwhole-brain calcium imaging, where organism movements disrupt sample-to-neuron\nmappings. This demonstrates the utility of our framework in real-world settings\nwith imprecise sample-channel associations, achieving accurate signal\nreconstruction.\n", "link": "http://arxiv.org/abs/2506.09773v1", "date": "2025-06-11", "relevancy": 1.9781, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5035}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.489}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Channel%20Unlabeled%20Sensing%20over%20a%20Union%20of%20Signal%20Subspaces&body=Title%3A%20Cross-Channel%20Unlabeled%20Sensing%20over%20a%20Union%20of%20Signal%20Subspaces%0AAuthor%3A%20Taulant%20Koka%20and%20Manolis%20C.%20Tsakiris%20and%20Benjam%C3%ADn%20B%C3%A9jar%20Haro%20and%20Michael%20Muma%0AAbstract%3A%20%20%20Cross-channel%20unlabeled%20sensing%20addresses%20the%20problem%20of%20recovering%20a%0Amulti-channel%20signal%20from%20measurements%20that%20were%20shuffled%20across%20channels.%20This%0Awork%20expands%20the%20cross-channel%20unlabeled%20sensing%20framework%20to%20signals%20that%20lie%0Ain%20a%20union%20of%20subspaces.%20The%20extension%20allows%20for%20handling%20more%20complex%20signal%0Astructures%20and%20broadens%20the%20framework%20to%20tasks%20like%20compressed%20sensing.%20These%0Amismatches%20between%20samples%20and%20channels%20often%20arise%20in%20applications%20such%20as%0Awhole-brain%20calcium%20imaging%20of%20freely%20moving%20organisms%20or%20multi-target%0Atracking.%20We%20improve%20over%20previous%20models%20by%20deriving%20tighter%20bounds%20on%20the%0Arequired%20number%20of%20samples%20for%20unique%20reconstruction%2C%20while%20supporting%20more%0Ageneral%20signal%20types.%20The%20approach%20is%20validated%20through%20an%20application%20in%0Awhole-brain%20calcium%20imaging%2C%20where%20organism%20movements%20disrupt%20sample-to-neuron%0Amappings.%20This%20demonstrates%20the%20utility%20of%20our%20framework%20in%20real-world%20settings%0Awith%20imprecise%20sample-channel%20associations%2C%20achieving%20accurate%20signal%0Areconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09773v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Channel%2520Unlabeled%2520Sensing%2520over%2520a%2520Union%2520of%2520Signal%2520Subspaces%26entry.906535625%3DTaulant%2520Koka%2520and%2520Manolis%2520C.%2520Tsakiris%2520and%2520Benjam%25C3%25ADn%2520B%25C3%25A9jar%2520Haro%2520and%2520Michael%2520Muma%26entry.1292438233%3D%2520%2520Cross-channel%2520unlabeled%2520sensing%2520addresses%2520the%2520problem%2520of%2520recovering%2520a%250Amulti-channel%2520signal%2520from%2520measurements%2520that%2520were%2520shuffled%2520across%2520channels.%2520This%250Awork%2520expands%2520the%2520cross-channel%2520unlabeled%2520sensing%2520framework%2520to%2520signals%2520that%2520lie%250Ain%2520a%2520union%2520of%2520subspaces.%2520The%2520extension%2520allows%2520for%2520handling%2520more%2520complex%2520signal%250Astructures%2520and%2520broadens%2520the%2520framework%2520to%2520tasks%2520like%2520compressed%2520sensing.%2520These%250Amismatches%2520between%2520samples%2520and%2520channels%2520often%2520arise%2520in%2520applications%2520such%2520as%250Awhole-brain%2520calcium%2520imaging%2520of%2520freely%2520moving%2520organisms%2520or%2520multi-target%250Atracking.%2520We%2520improve%2520over%2520previous%2520models%2520by%2520deriving%2520tighter%2520bounds%2520on%2520the%250Arequired%2520number%2520of%2520samples%2520for%2520unique%2520reconstruction%252C%2520while%2520supporting%2520more%250Ageneral%2520signal%2520types.%2520The%2520approach%2520is%2520validated%2520through%2520an%2520application%2520in%250Awhole-brain%2520calcium%2520imaging%252C%2520where%2520organism%2520movements%2520disrupt%2520sample-to-neuron%250Amappings.%2520This%2520demonstrates%2520the%2520utility%2520of%2520our%2520framework%2520in%2520real-world%2520settings%250Awith%2520imprecise%2520sample-channel%2520associations%252C%2520achieving%2520accurate%2520signal%250Areconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09773v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Channel%20Unlabeled%20Sensing%20over%20a%20Union%20of%20Signal%20Subspaces&entry.906535625=Taulant%20Koka%20and%20Manolis%20C.%20Tsakiris%20and%20Benjam%C3%ADn%20B%C3%A9jar%20Haro%20and%20Michael%20Muma&entry.1292438233=%20%20Cross-channel%20unlabeled%20sensing%20addresses%20the%20problem%20of%20recovering%20a%0Amulti-channel%20signal%20from%20measurements%20that%20were%20shuffled%20across%20channels.%20This%0Awork%20expands%20the%20cross-channel%20unlabeled%20sensing%20framework%20to%20signals%20that%20lie%0Ain%20a%20union%20of%20subspaces.%20The%20extension%20allows%20for%20handling%20more%20complex%20signal%0Astructures%20and%20broadens%20the%20framework%20to%20tasks%20like%20compressed%20sensing.%20These%0Amismatches%20between%20samples%20and%20channels%20often%20arise%20in%20applications%20such%20as%0Awhole-brain%20calcium%20imaging%20of%20freely%20moving%20organisms%20or%20multi-target%0Atracking.%20We%20improve%20over%20previous%20models%20by%20deriving%20tighter%20bounds%20on%20the%0Arequired%20number%20of%20samples%20for%20unique%20reconstruction%2C%20while%20supporting%20more%0Ageneral%20signal%20types.%20The%20approach%20is%20validated%20through%20an%20application%20in%0Awhole-brain%20calcium%20imaging%2C%20where%20organism%20movements%20disrupt%20sample-to-neuron%0Amappings.%20This%20demonstrates%20the%20utility%20of%20our%20framework%20in%20real-world%20settings%0Awith%20imprecise%20sample-channel%20associations%2C%20achieving%20accurate%20signal%0Areconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09773v1&entry.124074799=Read"},
{"title": "Logits-Based Finetuning", "author": "Jingyao Li and Senqiao Yang and Sitong Wu and Han Shi and Chuanyang Zheng and Hong Xu and Jiaya Jia", "abstract": "  In recent years, developing compact and efficient large language models\n(LLMs) has emerged as a thriving area of research. Traditional Supervised\nFine-Tuning (SFT), which relies on singular ground truth labels, often fails to\ncapture token-level dependencies and linguistic diversity. To address these\nlimitations, we propose a logits-based fine-tuning framework that integrates\nthe strengths of supervised learning and knowledge distillation. Our approach\nconstructs enriched training targets by combining teacher logits with ground\ntruth labels, preserving both correctness and linguistic diversity. This\nensures more reliable and effective training. We constructed a large-scale 1.2M\nlogits dataset and trained a series of science-focused models. Experimental\nresults demonstrate that our method achieves significant improvements, with\naccuracy gains of 18% on Mawps and 22.7% on TabMWP. Across nine widely used\nmathematical benchmarks, our method consistently outperforms prior SFT models,\nachieving an average improvement of 7.28%. Codes are available at\nhttps://github.com/dvlab-research/Logits-Based-Finetuning.\n", "link": "http://arxiv.org/abs/2505.24461v2", "date": "2025-06-11", "relevancy": 1.9734, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5058}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4916}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Logits-Based%20Finetuning&body=Title%3A%20Logits-Based%20Finetuning%0AAuthor%3A%20Jingyao%20Li%20and%20Senqiao%20Yang%20and%20Sitong%20Wu%20and%20Han%20Shi%20and%20Chuanyang%20Zheng%20and%20Hong%20Xu%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20In%20recent%20years%2C%20developing%20compact%20and%20efficient%20large%20language%20models%0A%28LLMs%29%20has%20emerged%20as%20a%20thriving%20area%20of%20research.%20Traditional%20Supervised%0AFine-Tuning%20%28SFT%29%2C%20which%20relies%20on%20singular%20ground%20truth%20labels%2C%20often%20fails%20to%0Acapture%20token-level%20dependencies%20and%20linguistic%20diversity.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20logits-based%20fine-tuning%20framework%20that%20integrates%0Athe%20strengths%20of%20supervised%20learning%20and%20knowledge%20distillation.%20Our%20approach%0Aconstructs%20enriched%20training%20targets%20by%20combining%20teacher%20logits%20with%20ground%0Atruth%20labels%2C%20preserving%20both%20correctness%20and%20linguistic%20diversity.%20This%0Aensures%20more%20reliable%20and%20effective%20training.%20We%20constructed%20a%20large-scale%201.2M%0Alogits%20dataset%20and%20trained%20a%20series%20of%20science-focused%20models.%20Experimental%0Aresults%20demonstrate%20that%20our%20method%20achieves%20significant%20improvements%2C%20with%0Aaccuracy%20gains%20of%2018%25%20on%20Mawps%20and%2022.7%25%20on%20TabMWP.%20Across%20nine%20widely%20used%0Amathematical%20benchmarks%2C%20our%20method%20consistently%20outperforms%20prior%20SFT%20models%2C%0Aachieving%20an%20average%20improvement%20of%207.28%25.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/dvlab-research/Logits-Based-Finetuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24461v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogits-Based%2520Finetuning%26entry.906535625%3DJingyao%2520Li%2520and%2520Senqiao%2520Yang%2520and%2520Sitong%2520Wu%2520and%2520Han%2520Shi%2520and%2520Chuanyang%2520Zheng%2520and%2520Hong%2520Xu%2520and%2520Jiaya%2520Jia%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520developing%2520compact%2520and%2520efficient%2520large%2520language%2520models%250A%2528LLMs%2529%2520has%2520emerged%2520as%2520a%2520thriving%2520area%2520of%2520research.%2520Traditional%2520Supervised%250AFine-Tuning%2520%2528SFT%2529%252C%2520which%2520relies%2520on%2520singular%2520ground%2520truth%2520labels%252C%2520often%2520fails%2520to%250Acapture%2520token-level%2520dependencies%2520and%2520linguistic%2520diversity.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520logits-based%2520fine-tuning%2520framework%2520that%2520integrates%250Athe%2520strengths%2520of%2520supervised%2520learning%2520and%2520knowledge%2520distillation.%2520Our%2520approach%250Aconstructs%2520enriched%2520training%2520targets%2520by%2520combining%2520teacher%2520logits%2520with%2520ground%250Atruth%2520labels%252C%2520preserving%2520both%2520correctness%2520and%2520linguistic%2520diversity.%2520This%250Aensures%2520more%2520reliable%2520and%2520effective%2520training.%2520We%2520constructed%2520a%2520large-scale%25201.2M%250Alogits%2520dataset%2520and%2520trained%2520a%2520series%2520of%2520science-focused%2520models.%2520Experimental%250Aresults%2520demonstrate%2520that%2520our%2520method%2520achieves%2520significant%2520improvements%252C%2520with%250Aaccuracy%2520gains%2520of%252018%2525%2520on%2520Mawps%2520and%252022.7%2525%2520on%2520TabMWP.%2520Across%2520nine%2520widely%2520used%250Amathematical%2520benchmarks%252C%2520our%2520method%2520consistently%2520outperforms%2520prior%2520SFT%2520models%252C%250Aachieving%2520an%2520average%2520improvement%2520of%25207.28%2525.%2520Codes%2520are%2520available%2520at%250Ahttps%253A//github.com/dvlab-research/Logits-Based-Finetuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24461v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Logits-Based%20Finetuning&entry.906535625=Jingyao%20Li%20and%20Senqiao%20Yang%20and%20Sitong%20Wu%20and%20Han%20Shi%20and%20Chuanyang%20Zheng%20and%20Hong%20Xu%20and%20Jiaya%20Jia&entry.1292438233=%20%20In%20recent%20years%2C%20developing%20compact%20and%20efficient%20large%20language%20models%0A%28LLMs%29%20has%20emerged%20as%20a%20thriving%20area%20of%20research.%20Traditional%20Supervised%0AFine-Tuning%20%28SFT%29%2C%20which%20relies%20on%20singular%20ground%20truth%20labels%2C%20often%20fails%20to%0Acapture%20token-level%20dependencies%20and%20linguistic%20diversity.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20logits-based%20fine-tuning%20framework%20that%20integrates%0Athe%20strengths%20of%20supervised%20learning%20and%20knowledge%20distillation.%20Our%20approach%0Aconstructs%20enriched%20training%20targets%20by%20combining%20teacher%20logits%20with%20ground%0Atruth%20labels%2C%20preserving%20both%20correctness%20and%20linguistic%20diversity.%20This%0Aensures%20more%20reliable%20and%20effective%20training.%20We%20constructed%20a%20large-scale%201.2M%0Alogits%20dataset%20and%20trained%20a%20series%20of%20science-focused%20models.%20Experimental%0Aresults%20demonstrate%20that%20our%20method%20achieves%20significant%20improvements%2C%20with%0Aaccuracy%20gains%20of%2018%25%20on%20Mawps%20and%2022.7%25%20on%20TabMWP.%20Across%20nine%20widely%20used%0Amathematical%20benchmarks%2C%20our%20method%20consistently%20outperforms%20prior%20SFT%20models%2C%0Aachieving%20an%20average%20improvement%20of%207.28%25.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/dvlab-research/Logits-Based-Finetuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24461v2&entry.124074799=Read"},
{"title": "VerIF: Verification Engineering for Reinforcement Learning in\n  Instruction Following", "author": "Hao Peng and Yunjia Qi and Xiaozhi Wang and Bin Xu and Lei Hou and Juanzi Li", "abstract": "  Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF.\n", "link": "http://arxiv.org/abs/2506.09942v1", "date": "2025-06-11", "relevancy": 1.956, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4906}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4906}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VerIF%3A%20Verification%20Engineering%20for%20Reinforcement%20Learning%20in%0A%20%20Instruction%20Following&body=Title%3A%20VerIF%3A%20Verification%20Engineering%20for%20Reinforcement%20Learning%20in%0A%20%20Instruction%20Following%0AAuthor%3A%20Hao%20Peng%20and%20Yunjia%20Qi%20and%20Xiaozhi%20Wang%20and%20Bin%20Xu%20and%20Lei%20Hou%20and%20Juanzi%20Li%0AAbstract%3A%20%20%20Reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20has%20become%20a%20key%0Atechnique%20for%20enhancing%20large%20language%20models%20%28LLMs%29%2C%20with%20verification%0Aengineering%20playing%20a%20central%20role.%20However%2C%20best%20practices%20for%20RL%20in%0Ainstruction%20following%20remain%20underexplored.%20In%20this%20work%2C%20we%20explore%20the%0Averification%20challenge%20in%20RL%20for%20instruction%20following%20and%20propose%20VerIF%2C%20a%0Averification%20method%20that%20combines%20rule-based%20code%20verification%20with%20LLM-based%0Averification%20from%20a%20large%20reasoning%20model%20%28e.g.%2C%20QwQ-32B%29.%20To%20support%20this%0Aapproach%2C%20we%20construct%20a%20high-quality%20instruction-following%20dataset%2C%0AVerInstruct%2C%20containing%20approximately%2022%2C000%20instances%20with%20associated%0Averification%20signals.%20We%20apply%20RL%20training%20with%20VerIF%20to%20two%20models%2C%20achieving%0Asignificant%20improvements%20across%20several%20representative%20instruction-following%0Abenchmarks.%20The%20trained%20models%20reach%20state-of-the-art%20performance%20among%20models%0Aof%20comparable%20size%20and%20generalize%20well%20to%20unseen%20constraints.%20We%20further%0Aobserve%20that%20their%20general%20capabilities%20remain%20unaffected%2C%20suggesting%20that%20RL%0Awith%20VerIF%20can%20be%20integrated%20into%20existing%20RL%20recipes%20to%20enhance%20overall%20model%0Aperformance.%20We%20have%20released%20our%20datasets%2C%20codes%2C%20and%20models%20to%20facilitate%0Afuture%20research%20at%20https%3A//github.com/THU-KEG/VerIF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVerIF%253A%2520Verification%2520Engineering%2520for%2520Reinforcement%2520Learning%2520in%250A%2520%2520Instruction%2520Following%26entry.906535625%3DHao%2520Peng%2520and%2520Yunjia%2520Qi%2520and%2520Xiaozhi%2520Wang%2520and%2520Bin%2520Xu%2520and%2520Lei%2520Hou%2520and%2520Juanzi%2520Li%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529%2520has%2520become%2520a%2520key%250Atechnique%2520for%2520enhancing%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520with%2520verification%250Aengineering%2520playing%2520a%2520central%2520role.%2520However%252C%2520best%2520practices%2520for%2520RL%2520in%250Ainstruction%2520following%2520remain%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%250Averification%2520challenge%2520in%2520RL%2520for%2520instruction%2520following%2520and%2520propose%2520VerIF%252C%2520a%250Averification%2520method%2520that%2520combines%2520rule-based%2520code%2520verification%2520with%2520LLM-based%250Averification%2520from%2520a%2520large%2520reasoning%2520model%2520%2528e.g.%252C%2520QwQ-32B%2529.%2520To%2520support%2520this%250Aapproach%252C%2520we%2520construct%2520a%2520high-quality%2520instruction-following%2520dataset%252C%250AVerInstruct%252C%2520containing%2520approximately%252022%252C000%2520instances%2520with%2520associated%250Averification%2520signals.%2520We%2520apply%2520RL%2520training%2520with%2520VerIF%2520to%2520two%2520models%252C%2520achieving%250Asignificant%2520improvements%2520across%2520several%2520representative%2520instruction-following%250Abenchmarks.%2520The%2520trained%2520models%2520reach%2520state-of-the-art%2520performance%2520among%2520models%250Aof%2520comparable%2520size%2520and%2520generalize%2520well%2520to%2520unseen%2520constraints.%2520We%2520further%250Aobserve%2520that%2520their%2520general%2520capabilities%2520remain%2520unaffected%252C%2520suggesting%2520that%2520RL%250Awith%2520VerIF%2520can%2520be%2520integrated%2520into%2520existing%2520RL%2520recipes%2520to%2520enhance%2520overall%2520model%250Aperformance.%2520We%2520have%2520released%2520our%2520datasets%252C%2520codes%252C%2520and%2520models%2520to%2520facilitate%250Afuture%2520research%2520at%2520https%253A//github.com/THU-KEG/VerIF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VerIF%3A%20Verification%20Engineering%20for%20Reinforcement%20Learning%20in%0A%20%20Instruction%20Following&entry.906535625=Hao%20Peng%20and%20Yunjia%20Qi%20and%20Xiaozhi%20Wang%20and%20Bin%20Xu%20and%20Lei%20Hou%20and%20Juanzi%20Li&entry.1292438233=%20%20Reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20has%20become%20a%20key%0Atechnique%20for%20enhancing%20large%20language%20models%20%28LLMs%29%2C%20with%20verification%0Aengineering%20playing%20a%20central%20role.%20However%2C%20best%20practices%20for%20RL%20in%0Ainstruction%20following%20remain%20underexplored.%20In%20this%20work%2C%20we%20explore%20the%0Averification%20challenge%20in%20RL%20for%20instruction%20following%20and%20propose%20VerIF%2C%20a%0Averification%20method%20that%20combines%20rule-based%20code%20verification%20with%20LLM-based%0Averification%20from%20a%20large%20reasoning%20model%20%28e.g.%2C%20QwQ-32B%29.%20To%20support%20this%0Aapproach%2C%20we%20construct%20a%20high-quality%20instruction-following%20dataset%2C%0AVerInstruct%2C%20containing%20approximately%2022%2C000%20instances%20with%20associated%0Averification%20signals.%20We%20apply%20RL%20training%20with%20VerIF%20to%20two%20models%2C%20achieving%0Asignificant%20improvements%20across%20several%20representative%20instruction-following%0Abenchmarks.%20The%20trained%20models%20reach%20state-of-the-art%20performance%20among%20models%0Aof%20comparable%20size%20and%20generalize%20well%20to%20unseen%20constraints.%20We%20further%0Aobserve%20that%20their%20general%20capabilities%20remain%20unaffected%2C%20suggesting%20that%20RL%0Awith%20VerIF%20can%20be%20integrated%20into%20existing%20RL%20recipes%20to%20enhance%20overall%20model%0Aperformance.%20We%20have%20released%20our%20datasets%2C%20codes%2C%20and%20models%20to%20facilitate%0Afuture%20research%20at%20https%3A//github.com/THU-KEG/VerIF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09942v1&entry.124074799=Read"},
{"title": "Regularizing Learnable Feature Extraction for Automatic Speech\n  Recognition", "author": "Peter Vieting and Maximilian Kannen and Benedikt Hilmes and Ralf Schl\u00fcter and Hermann Ney", "abstract": "  Neural front-ends are an appealing alternative to traditional, fixed feature\nextraction pipelines for automatic speech recognition (ASR) systems since they\ncan be directly trained to fit the acoustic model. However, their performance\noften falls short compared to classical methods, which we show is largely due\nto their increased susceptibility to overfitting. This work therefore\ninvestigates regularization methods for training ASR models with learnable\nfeature extraction front-ends. First, we examine audio perturbation methods and\nshow that larger relative improvements can be obtained for learnable features.\nAdditionally, we identify two limitations in the standard use of SpecAugment\nfor these front-ends and propose masking in the short time Fourier transform\n(STFT)-domain as a simple but effective modification to address these\nchallenges. Finally, integrating both regularization approaches effectively\ncloses the performance gap between traditional and learnable features.\n", "link": "http://arxiv.org/abs/2506.09804v1", "date": "2025-06-11", "relevancy": 1.9423, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5278}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4587}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regularizing%20Learnable%20Feature%20Extraction%20for%20Automatic%20Speech%0A%20%20Recognition&body=Title%3A%20Regularizing%20Learnable%20Feature%20Extraction%20for%20Automatic%20Speech%0A%20%20Recognition%0AAuthor%3A%20Peter%20Vieting%20and%20Maximilian%20Kannen%20and%20Benedikt%20Hilmes%20and%20Ralf%20Schl%C3%BCter%20and%20Hermann%20Ney%0AAbstract%3A%20%20%20Neural%20front-ends%20are%20an%20appealing%20alternative%20to%20traditional%2C%20fixed%20feature%0Aextraction%20pipelines%20for%20automatic%20speech%20recognition%20%28ASR%29%20systems%20since%20they%0Acan%20be%20directly%20trained%20to%20fit%20the%20acoustic%20model.%20However%2C%20their%20performance%0Aoften%20falls%20short%20compared%20to%20classical%20methods%2C%20which%20we%20show%20is%20largely%20due%0Ato%20their%20increased%20susceptibility%20to%20overfitting.%20This%20work%20therefore%0Ainvestigates%20regularization%20methods%20for%20training%20ASR%20models%20with%20learnable%0Afeature%20extraction%20front-ends.%20First%2C%20we%20examine%20audio%20perturbation%20methods%20and%0Ashow%20that%20larger%20relative%20improvements%20can%20be%20obtained%20for%20learnable%20features.%0AAdditionally%2C%20we%20identify%20two%20limitations%20in%20the%20standard%20use%20of%20SpecAugment%0Afor%20these%20front-ends%20and%20propose%20masking%20in%20the%20short%20time%20Fourier%20transform%0A%28STFT%29-domain%20as%20a%20simple%20but%20effective%20modification%20to%20address%20these%0Achallenges.%20Finally%2C%20integrating%20both%20regularization%20approaches%20effectively%0Acloses%20the%20performance%20gap%20between%20traditional%20and%20learnable%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09804v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegularizing%2520Learnable%2520Feature%2520Extraction%2520for%2520Automatic%2520Speech%250A%2520%2520Recognition%26entry.906535625%3DPeter%2520Vieting%2520and%2520Maximilian%2520Kannen%2520and%2520Benedikt%2520Hilmes%2520and%2520Ralf%2520Schl%25C3%25BCter%2520and%2520Hermann%2520Ney%26entry.1292438233%3D%2520%2520Neural%2520front-ends%2520are%2520an%2520appealing%2520alternative%2520to%2520traditional%252C%2520fixed%2520feature%250Aextraction%2520pipelines%2520for%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%2520systems%2520since%2520they%250Acan%2520be%2520directly%2520trained%2520to%2520fit%2520the%2520acoustic%2520model.%2520However%252C%2520their%2520performance%250Aoften%2520falls%2520short%2520compared%2520to%2520classical%2520methods%252C%2520which%2520we%2520show%2520is%2520largely%2520due%250Ato%2520their%2520increased%2520susceptibility%2520to%2520overfitting.%2520This%2520work%2520therefore%250Ainvestigates%2520regularization%2520methods%2520for%2520training%2520ASR%2520models%2520with%2520learnable%250Afeature%2520extraction%2520front-ends.%2520First%252C%2520we%2520examine%2520audio%2520perturbation%2520methods%2520and%250Ashow%2520that%2520larger%2520relative%2520improvements%2520can%2520be%2520obtained%2520for%2520learnable%2520features.%250AAdditionally%252C%2520we%2520identify%2520two%2520limitations%2520in%2520the%2520standard%2520use%2520of%2520SpecAugment%250Afor%2520these%2520front-ends%2520and%2520propose%2520masking%2520in%2520the%2520short%2520time%2520Fourier%2520transform%250A%2528STFT%2529-domain%2520as%2520a%2520simple%2520but%2520effective%2520modification%2520to%2520address%2520these%250Achallenges.%2520Finally%252C%2520integrating%2520both%2520regularization%2520approaches%2520effectively%250Acloses%2520the%2520performance%2520gap%2520between%2520traditional%2520and%2520learnable%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09804v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regularizing%20Learnable%20Feature%20Extraction%20for%20Automatic%20Speech%0A%20%20Recognition&entry.906535625=Peter%20Vieting%20and%20Maximilian%20Kannen%20and%20Benedikt%20Hilmes%20and%20Ralf%20Schl%C3%BCter%20and%20Hermann%20Ney&entry.1292438233=%20%20Neural%20front-ends%20are%20an%20appealing%20alternative%20to%20traditional%2C%20fixed%20feature%0Aextraction%20pipelines%20for%20automatic%20speech%20recognition%20%28ASR%29%20systems%20since%20they%0Acan%20be%20directly%20trained%20to%20fit%20the%20acoustic%20model.%20However%2C%20their%20performance%0Aoften%20falls%20short%20compared%20to%20classical%20methods%2C%20which%20we%20show%20is%20largely%20due%0Ato%20their%20increased%20susceptibility%20to%20overfitting.%20This%20work%20therefore%0Ainvestigates%20regularization%20methods%20for%20training%20ASR%20models%20with%20learnable%0Afeature%20extraction%20front-ends.%20First%2C%20we%20examine%20audio%20perturbation%20methods%20and%0Ashow%20that%20larger%20relative%20improvements%20can%20be%20obtained%20for%20learnable%20features.%0AAdditionally%2C%20we%20identify%20two%20limitations%20in%20the%20standard%20use%20of%20SpecAugment%0Afor%20these%20front-ends%20and%20propose%20masking%20in%20the%20short%20time%20Fourier%20transform%0A%28STFT%29-domain%20as%20a%20simple%20but%20effective%20modification%20to%20address%20these%0Achallenges.%20Finally%2C%20integrating%20both%20regularization%20approaches%20effectively%0Acloses%20the%20performance%20gap%20between%20traditional%20and%20learnable%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09804v1&entry.124074799=Read"},
{"title": "Weighted Loss Methods for Robust Federated Learning under Data\n  Heterogeneity", "author": "Johan Erbani and Sonia Ben Mokhtar and Pierre-Edouard Portier and Elod Egyed-Zsigmond and Diana Nurbakova", "abstract": "  Federated learning (FL) is a machine learning paradigm that enables multiple\ndata holders to collaboratively train a machine learning model without sharing\ntheir training data with external parties. In this paradigm, workers locally\nupdate a model and share with a central server their updated gradients (or\nmodel parameters). While FL seems appealing from a privacy perspective, it\nopens a number of threats from a security perspective as (Byzantine)\nparticipants can contribute poisonous gradients (or model parameters) harming\nmodel convergence. Byzantine-resilient FL addresses this issue by ensuring that\nthe training proceeds as if Byzantine participants were absent. Towards this\npurpose, common strategies ignore outlier gradients during model aggregation,\nassuming that Byzantine gradients deviate more from honest gradients than\nhonest gradients do from each other. However, in heterogeneous settings, honest\ngradients may differ significantly, making it difficult to distinguish honest\noutliers from Byzantine ones. In this paper, we introduce the Worker Label\nAlignement Loss (WoLA), a weighted loss that aligns honest worker gradients\ndespite data heterogeneity, which facilitates the identification of Byzantines'\ngradients. This approach significantly outperforms state-of-the-art methods in\nheterogeneous settings. In this paper, we provide both theoretical insights and\nempirical evidence of its effectiveness.\n", "link": "http://arxiv.org/abs/2506.09824v1", "date": "2025-06-11", "relevancy": 1.9408, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4959}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.486}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weighted%20Loss%20Methods%20for%20Robust%20Federated%20Learning%20under%20Data%0A%20%20Heterogeneity&body=Title%3A%20Weighted%20Loss%20Methods%20for%20Robust%20Federated%20Learning%20under%20Data%0A%20%20Heterogeneity%0AAuthor%3A%20Johan%20Erbani%20and%20Sonia%20Ben%20Mokhtar%20and%20Pierre-Edouard%20Portier%20and%20Elod%20Egyed-Zsigmond%20and%20Diana%20Nurbakova%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20is%20a%20machine%20learning%20paradigm%20that%20enables%20multiple%0Adata%20holders%20to%20collaboratively%20train%20a%20machine%20learning%20model%20without%20sharing%0Atheir%20training%20data%20with%20external%20parties.%20In%20this%20paradigm%2C%20workers%20locally%0Aupdate%20a%20model%20and%20share%20with%20a%20central%20server%20their%20updated%20gradients%20%28or%0Amodel%20parameters%29.%20While%20FL%20seems%20appealing%20from%20a%20privacy%20perspective%2C%20it%0Aopens%20a%20number%20of%20threats%20from%20a%20security%20perspective%20as%20%28Byzantine%29%0Aparticipants%20can%20contribute%20poisonous%20gradients%20%28or%20model%20parameters%29%20harming%0Amodel%20convergence.%20Byzantine-resilient%20FL%20addresses%20this%20issue%20by%20ensuring%20that%0Athe%20training%20proceeds%20as%20if%20Byzantine%20participants%20were%20absent.%20Towards%20this%0Apurpose%2C%20common%20strategies%20ignore%20outlier%20gradients%20during%20model%20aggregation%2C%0Aassuming%20that%20Byzantine%20gradients%20deviate%20more%20from%20honest%20gradients%20than%0Ahonest%20gradients%20do%20from%20each%20other.%20However%2C%20in%20heterogeneous%20settings%2C%20honest%0Agradients%20may%20differ%20significantly%2C%20making%20it%20difficult%20to%20distinguish%20honest%0Aoutliers%20from%20Byzantine%20ones.%20In%20this%20paper%2C%20we%20introduce%20the%20Worker%20Label%0AAlignement%20Loss%20%28WoLA%29%2C%20a%20weighted%20loss%20that%20aligns%20honest%20worker%20gradients%0Adespite%20data%20heterogeneity%2C%20which%20facilitates%20the%20identification%20of%20Byzantines%27%0Agradients.%20This%20approach%20significantly%20outperforms%20state-of-the-art%20methods%20in%0Aheterogeneous%20settings.%20In%20this%20paper%2C%20we%20provide%20both%20theoretical%20insights%20and%0Aempirical%20evidence%20of%20its%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeighted%2520Loss%2520Methods%2520for%2520Robust%2520Federated%2520Learning%2520under%2520Data%250A%2520%2520Heterogeneity%26entry.906535625%3DJohan%2520Erbani%2520and%2520Sonia%2520Ben%2520Mokhtar%2520and%2520Pierre-Edouard%2520Portier%2520and%2520Elod%2520Egyed-Zsigmond%2520and%2520Diana%2520Nurbakova%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520is%2520a%2520machine%2520learning%2520paradigm%2520that%2520enables%2520multiple%250Adata%2520holders%2520to%2520collaboratively%2520train%2520a%2520machine%2520learning%2520model%2520without%2520sharing%250Atheir%2520training%2520data%2520with%2520external%2520parties.%2520In%2520this%2520paradigm%252C%2520workers%2520locally%250Aupdate%2520a%2520model%2520and%2520share%2520with%2520a%2520central%2520server%2520their%2520updated%2520gradients%2520%2528or%250Amodel%2520parameters%2529.%2520While%2520FL%2520seems%2520appealing%2520from%2520a%2520privacy%2520perspective%252C%2520it%250Aopens%2520a%2520number%2520of%2520threats%2520from%2520a%2520security%2520perspective%2520as%2520%2528Byzantine%2529%250Aparticipants%2520can%2520contribute%2520poisonous%2520gradients%2520%2528or%2520model%2520parameters%2529%2520harming%250Amodel%2520convergence.%2520Byzantine-resilient%2520FL%2520addresses%2520this%2520issue%2520by%2520ensuring%2520that%250Athe%2520training%2520proceeds%2520as%2520if%2520Byzantine%2520participants%2520were%2520absent.%2520Towards%2520this%250Apurpose%252C%2520common%2520strategies%2520ignore%2520outlier%2520gradients%2520during%2520model%2520aggregation%252C%250Aassuming%2520that%2520Byzantine%2520gradients%2520deviate%2520more%2520from%2520honest%2520gradients%2520than%250Ahonest%2520gradients%2520do%2520from%2520each%2520other.%2520However%252C%2520in%2520heterogeneous%2520settings%252C%2520honest%250Agradients%2520may%2520differ%2520significantly%252C%2520making%2520it%2520difficult%2520to%2520distinguish%2520honest%250Aoutliers%2520from%2520Byzantine%2520ones.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Worker%2520Label%250AAlignement%2520Loss%2520%2528WoLA%2529%252C%2520a%2520weighted%2520loss%2520that%2520aligns%2520honest%2520worker%2520gradients%250Adespite%2520data%2520heterogeneity%252C%2520which%2520facilitates%2520the%2520identification%2520of%2520Byzantines%2527%250Agradients.%2520This%2520approach%2520significantly%2520outperforms%2520state-of-the-art%2520methods%2520in%250Aheterogeneous%2520settings.%2520In%2520this%2520paper%252C%2520we%2520provide%2520both%2520theoretical%2520insights%2520and%250Aempirical%2520evidence%2520of%2520its%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weighted%20Loss%20Methods%20for%20Robust%20Federated%20Learning%20under%20Data%0A%20%20Heterogeneity&entry.906535625=Johan%20Erbani%20and%20Sonia%20Ben%20Mokhtar%20and%20Pierre-Edouard%20Portier%20and%20Elod%20Egyed-Zsigmond%20and%20Diana%20Nurbakova&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20is%20a%20machine%20learning%20paradigm%20that%20enables%20multiple%0Adata%20holders%20to%20collaboratively%20train%20a%20machine%20learning%20model%20without%20sharing%0Atheir%20training%20data%20with%20external%20parties.%20In%20this%20paradigm%2C%20workers%20locally%0Aupdate%20a%20model%20and%20share%20with%20a%20central%20server%20their%20updated%20gradients%20%28or%0Amodel%20parameters%29.%20While%20FL%20seems%20appealing%20from%20a%20privacy%20perspective%2C%20it%0Aopens%20a%20number%20of%20threats%20from%20a%20security%20perspective%20as%20%28Byzantine%29%0Aparticipants%20can%20contribute%20poisonous%20gradients%20%28or%20model%20parameters%29%20harming%0Amodel%20convergence.%20Byzantine-resilient%20FL%20addresses%20this%20issue%20by%20ensuring%20that%0Athe%20training%20proceeds%20as%20if%20Byzantine%20participants%20were%20absent.%20Towards%20this%0Apurpose%2C%20common%20strategies%20ignore%20outlier%20gradients%20during%20model%20aggregation%2C%0Aassuming%20that%20Byzantine%20gradients%20deviate%20more%20from%20honest%20gradients%20than%0Ahonest%20gradients%20do%20from%20each%20other.%20However%2C%20in%20heterogeneous%20settings%2C%20honest%0Agradients%20may%20differ%20significantly%2C%20making%20it%20difficult%20to%20distinguish%20honest%0Aoutliers%20from%20Byzantine%20ones.%20In%20this%20paper%2C%20we%20introduce%20the%20Worker%20Label%0AAlignement%20Loss%20%28WoLA%29%2C%20a%20weighted%20loss%20that%20aligns%20honest%20worker%20gradients%0Adespite%20data%20heterogeneity%2C%20which%20facilitates%20the%20identification%20of%20Byzantines%27%0Agradients.%20This%20approach%20significantly%20outperforms%20state-of-the-art%20methods%20in%0Aheterogeneous%20settings.%20In%20this%20paper%2C%20we%20provide%20both%20theoretical%20insights%20and%0Aempirical%20evidence%20of%20its%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09824v1&entry.124074799=Read"},
{"title": "Attention Head Embeddings with Trainable Deep Kernels for Hallucination\n  Detection in LLMs", "author": "Rodion Oblovatny and Alexandra Bazarova and Alexey Zaytsev", "abstract": "  We present a novel approach for detecting hallucinations in large language\nmodels (LLMs) by analyzing the probabilistic divergence between prompt and\nresponse hidden-state distributions. Counterintuitively, we find that\nhallucinated responses exhibit smaller deviations from their prompts compared\nto grounded responses, suggesting that hallucinations often arise from\nsuperficial rephrasing rather than substantive reasoning. Leveraging this\ninsight, we propose a model-intrinsic detection method that uses distributional\ndistances as principled hallucination scores, eliminating the need for external\nknowledge or auxiliary models. To enhance sensitivity, we employ deep learnable\nkernels that automatically adapt to capture nuanced geometric differences\nbetween distributions. Our approach outperforms existing baselines,\ndemonstrating state-of-the-art performance on several benchmarks. The method\nremains competitive even without kernel training, offering a robust, scalable\nsolution for hallucination detection.\n", "link": "http://arxiv.org/abs/2506.09886v1", "date": "2025-06-11", "relevancy": 1.93, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4966}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4797}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20Head%20Embeddings%20with%20Trainable%20Deep%20Kernels%20for%20Hallucination%0A%20%20Detection%20in%20LLMs&body=Title%3A%20Attention%20Head%20Embeddings%20with%20Trainable%20Deep%20Kernels%20for%20Hallucination%0A%20%20Detection%20in%20LLMs%0AAuthor%3A%20Rodion%20Oblovatny%20and%20Alexandra%20Bazarova%20and%20Alexey%20Zaytsev%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20for%20detecting%20hallucinations%20in%20large%20language%0Amodels%20%28LLMs%29%20by%20analyzing%20the%20probabilistic%20divergence%20between%20prompt%20and%0Aresponse%20hidden-state%20distributions.%20Counterintuitively%2C%20we%20find%20that%0Ahallucinated%20responses%20exhibit%20smaller%20deviations%20from%20their%20prompts%20compared%0Ato%20grounded%20responses%2C%20suggesting%20that%20hallucinations%20often%20arise%20from%0Asuperficial%20rephrasing%20rather%20than%20substantive%20reasoning.%20Leveraging%20this%0Ainsight%2C%20we%20propose%20a%20model-intrinsic%20detection%20method%20that%20uses%20distributional%0Adistances%20as%20principled%20hallucination%20scores%2C%20eliminating%20the%20need%20for%20external%0Aknowledge%20or%20auxiliary%20models.%20To%20enhance%20sensitivity%2C%20we%20employ%20deep%20learnable%0Akernels%20that%20automatically%20adapt%20to%20capture%20nuanced%20geometric%20differences%0Abetween%20distributions.%20Our%20approach%20outperforms%20existing%20baselines%2C%0Ademonstrating%20state-of-the-art%20performance%20on%20several%20benchmarks.%20The%20method%0Aremains%20competitive%20even%20without%20kernel%20training%2C%20offering%20a%20robust%2C%20scalable%0Asolution%20for%20hallucination%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09886v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520Head%2520Embeddings%2520with%2520Trainable%2520Deep%2520Kernels%2520for%2520Hallucination%250A%2520%2520Detection%2520in%2520LLMs%26entry.906535625%3DRodion%2520Oblovatny%2520and%2520Alexandra%2520Bazarova%2520and%2520Alexey%2520Zaytsev%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520for%2520detecting%2520hallucinations%2520in%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520by%2520analyzing%2520the%2520probabilistic%2520divergence%2520between%2520prompt%2520and%250Aresponse%2520hidden-state%2520distributions.%2520Counterintuitively%252C%2520we%2520find%2520that%250Ahallucinated%2520responses%2520exhibit%2520smaller%2520deviations%2520from%2520their%2520prompts%2520compared%250Ato%2520grounded%2520responses%252C%2520suggesting%2520that%2520hallucinations%2520often%2520arise%2520from%250Asuperficial%2520rephrasing%2520rather%2520than%2520substantive%2520reasoning.%2520Leveraging%2520this%250Ainsight%252C%2520we%2520propose%2520a%2520model-intrinsic%2520detection%2520method%2520that%2520uses%2520distributional%250Adistances%2520as%2520principled%2520hallucination%2520scores%252C%2520eliminating%2520the%2520need%2520for%2520external%250Aknowledge%2520or%2520auxiliary%2520models.%2520To%2520enhance%2520sensitivity%252C%2520we%2520employ%2520deep%2520learnable%250Akernels%2520that%2520automatically%2520adapt%2520to%2520capture%2520nuanced%2520geometric%2520differences%250Abetween%2520distributions.%2520Our%2520approach%2520outperforms%2520existing%2520baselines%252C%250Ademonstrating%2520state-of-the-art%2520performance%2520on%2520several%2520benchmarks.%2520The%2520method%250Aremains%2520competitive%2520even%2520without%2520kernel%2520training%252C%2520offering%2520a%2520robust%252C%2520scalable%250Asolution%2520for%2520hallucination%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09886v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Head%20Embeddings%20with%20Trainable%20Deep%20Kernels%20for%20Hallucination%0A%20%20Detection%20in%20LLMs&entry.906535625=Rodion%20Oblovatny%20and%20Alexandra%20Bazarova%20and%20Alexey%20Zaytsev&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20for%20detecting%20hallucinations%20in%20large%20language%0Amodels%20%28LLMs%29%20by%20analyzing%20the%20probabilistic%20divergence%20between%20prompt%20and%0Aresponse%20hidden-state%20distributions.%20Counterintuitively%2C%20we%20find%20that%0Ahallucinated%20responses%20exhibit%20smaller%20deviations%20from%20their%20prompts%20compared%0Ato%20grounded%20responses%2C%20suggesting%20that%20hallucinations%20often%20arise%20from%0Asuperficial%20rephrasing%20rather%20than%20substantive%20reasoning.%20Leveraging%20this%0Ainsight%2C%20we%20propose%20a%20model-intrinsic%20detection%20method%20that%20uses%20distributional%0Adistances%20as%20principled%20hallucination%20scores%2C%20eliminating%20the%20need%20for%20external%0Aknowledge%20or%20auxiliary%20models.%20To%20enhance%20sensitivity%2C%20we%20employ%20deep%20learnable%0Akernels%20that%20automatically%20adapt%20to%20capture%20nuanced%20geometric%20differences%0Abetween%20distributions.%20Our%20approach%20outperforms%20existing%20baselines%2C%0Ademonstrating%20state-of-the-art%20performance%20on%20several%20benchmarks.%20The%20method%0Aremains%20competitive%20even%20without%20kernel%20training%2C%20offering%20a%20robust%2C%20scalable%0Asolution%20for%20hallucination%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09886v1&entry.124074799=Read"},
{"title": "Fluoroscopic Shape and Pose Tracking of Catheters with Custom Radiopaque\n  Markers", "author": "Jared Lawson and Rohan Chitale and Nabil Simaan", "abstract": "  Safe navigation of steerable and robotic catheters in the cerebral\nvasculature requires awareness of the catheters shape and pose. Currently, a\nsignificant perception burden is placed on interventionalists to mentally\nreconstruct and predict catheter motions from biplane fluoroscopy images.\nEfforts to track these catheters are limited to planar segmentation or bulky\nsensing instrumentation, which are incompatible with microcatheters used in\nneurointervention. In this work, a catheter is equipped with custom radiopaque\nmarkers arranged to enable simultaneous shape and pose estimation under biplane\nfluoroscopy. A design measure is proposed to guide the arrangement of these\nmarkers to minimize sensitivity to marker tracking uncertainty. This approach\nwas deployed for microcatheters smaller than 2mm OD navigating phantom\nvasculature with shape tracking errors less than 1mm and catheter roll errors\nbelow 40 degrees. This work can enable steerable catheters to autonomously\nnavigate under biplane imaging.\n", "link": "http://arxiv.org/abs/2506.09934v1", "date": "2025-06-11", "relevancy": 1.9287, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4869}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4808}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fluoroscopic%20Shape%20and%20Pose%20Tracking%20of%20Catheters%20with%20Custom%20Radiopaque%0A%20%20Markers&body=Title%3A%20Fluoroscopic%20Shape%20and%20Pose%20Tracking%20of%20Catheters%20with%20Custom%20Radiopaque%0A%20%20Markers%0AAuthor%3A%20Jared%20Lawson%20and%20Rohan%20Chitale%20and%20Nabil%20Simaan%0AAbstract%3A%20%20%20Safe%20navigation%20of%20steerable%20and%20robotic%20catheters%20in%20the%20cerebral%0Avasculature%20requires%20awareness%20of%20the%20catheters%20shape%20and%20pose.%20Currently%2C%20a%0Asignificant%20perception%20burden%20is%20placed%20on%20interventionalists%20to%20mentally%0Areconstruct%20and%20predict%20catheter%20motions%20from%20biplane%20fluoroscopy%20images.%0AEfforts%20to%20track%20these%20catheters%20are%20limited%20to%20planar%20segmentation%20or%20bulky%0Asensing%20instrumentation%2C%20which%20are%20incompatible%20with%20microcatheters%20used%20in%0Aneurointervention.%20In%20this%20work%2C%20a%20catheter%20is%20equipped%20with%20custom%20radiopaque%0Amarkers%20arranged%20to%20enable%20simultaneous%20shape%20and%20pose%20estimation%20under%20biplane%0Afluoroscopy.%20A%20design%20measure%20is%20proposed%20to%20guide%20the%20arrangement%20of%20these%0Amarkers%20to%20minimize%20sensitivity%20to%20marker%20tracking%20uncertainty.%20This%20approach%0Awas%20deployed%20for%20microcatheters%20smaller%20than%202mm%20OD%20navigating%20phantom%0Avasculature%20with%20shape%20tracking%20errors%20less%20than%201mm%20and%20catheter%20roll%20errors%0Abelow%2040%20degrees.%20This%20work%20can%20enable%20steerable%20catheters%20to%20autonomously%0Anavigate%20under%20biplane%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFluoroscopic%2520Shape%2520and%2520Pose%2520Tracking%2520of%2520Catheters%2520with%2520Custom%2520Radiopaque%250A%2520%2520Markers%26entry.906535625%3DJared%2520Lawson%2520and%2520Rohan%2520Chitale%2520and%2520Nabil%2520Simaan%26entry.1292438233%3D%2520%2520Safe%2520navigation%2520of%2520steerable%2520and%2520robotic%2520catheters%2520in%2520the%2520cerebral%250Avasculature%2520requires%2520awareness%2520of%2520the%2520catheters%2520shape%2520and%2520pose.%2520Currently%252C%2520a%250Asignificant%2520perception%2520burden%2520is%2520placed%2520on%2520interventionalists%2520to%2520mentally%250Areconstruct%2520and%2520predict%2520catheter%2520motions%2520from%2520biplane%2520fluoroscopy%2520images.%250AEfforts%2520to%2520track%2520these%2520catheters%2520are%2520limited%2520to%2520planar%2520segmentation%2520or%2520bulky%250Asensing%2520instrumentation%252C%2520which%2520are%2520incompatible%2520with%2520microcatheters%2520used%2520in%250Aneurointervention.%2520In%2520this%2520work%252C%2520a%2520catheter%2520is%2520equipped%2520with%2520custom%2520radiopaque%250Amarkers%2520arranged%2520to%2520enable%2520simultaneous%2520shape%2520and%2520pose%2520estimation%2520under%2520biplane%250Afluoroscopy.%2520A%2520design%2520measure%2520is%2520proposed%2520to%2520guide%2520the%2520arrangement%2520of%2520these%250Amarkers%2520to%2520minimize%2520sensitivity%2520to%2520marker%2520tracking%2520uncertainty.%2520This%2520approach%250Awas%2520deployed%2520for%2520microcatheters%2520smaller%2520than%25202mm%2520OD%2520navigating%2520phantom%250Avasculature%2520with%2520shape%2520tracking%2520errors%2520less%2520than%25201mm%2520and%2520catheter%2520roll%2520errors%250Abelow%252040%2520degrees.%2520This%2520work%2520can%2520enable%2520steerable%2520catheters%2520to%2520autonomously%250Anavigate%2520under%2520biplane%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fluoroscopic%20Shape%20and%20Pose%20Tracking%20of%20Catheters%20with%20Custom%20Radiopaque%0A%20%20Markers&entry.906535625=Jared%20Lawson%20and%20Rohan%20Chitale%20and%20Nabil%20Simaan&entry.1292438233=%20%20Safe%20navigation%20of%20steerable%20and%20robotic%20catheters%20in%20the%20cerebral%0Avasculature%20requires%20awareness%20of%20the%20catheters%20shape%20and%20pose.%20Currently%2C%20a%0Asignificant%20perception%20burden%20is%20placed%20on%20interventionalists%20to%20mentally%0Areconstruct%20and%20predict%20catheter%20motions%20from%20biplane%20fluoroscopy%20images.%0AEfforts%20to%20track%20these%20catheters%20are%20limited%20to%20planar%20segmentation%20or%20bulky%0Asensing%20instrumentation%2C%20which%20are%20incompatible%20with%20microcatheters%20used%20in%0Aneurointervention.%20In%20this%20work%2C%20a%20catheter%20is%20equipped%20with%20custom%20radiopaque%0Amarkers%20arranged%20to%20enable%20simultaneous%20shape%20and%20pose%20estimation%20under%20biplane%0Afluoroscopy.%20A%20design%20measure%20is%20proposed%20to%20guide%20the%20arrangement%20of%20these%0Amarkers%20to%20minimize%20sensitivity%20to%20marker%20tracking%20uncertainty.%20This%20approach%0Awas%20deployed%20for%20microcatheters%20smaller%20than%202mm%20OD%20navigating%20phantom%0Avasculature%20with%20shape%20tracking%20errors%20less%20than%201mm%20and%20catheter%20roll%20errors%0Abelow%2040%20degrees.%20This%20work%20can%20enable%20steerable%20catheters%20to%20autonomously%0Anavigate%20under%20biplane%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09934v1&entry.124074799=Read"},
{"title": "LogProber: Disentangling confidence from contamination in LLM responses", "author": "Nicolas Yax and Pierre-Yves Oudeyer and Stefano Palminteri", "abstract": "  In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. To date, only a few recent studies have attempted to\naddress the issue of quantifying and detecting contamination in short text\nsequences, such as those commonly found in benchmarks. However, these methods\nhave limitations that can sometimes render them impractical.In the present\npaper, we introduce LogProber, a novel, efficient algorithm that we show to be\nable to detect contamination in a black box setting that tries to tackle some\nof these drawbacks by focusing on the familiarity with the question rather than\nthe answer. Here, we explore the properties of the proposed method in\ncomparison with concurrent approaches, identify its advantages and limitations,\nand illustrate how different forms of contamination can go undetected depending\non the design of the detection algorithm.\n", "link": "http://arxiv.org/abs/2408.14352v2", "date": "2025-06-11", "relevancy": 1.9233, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5099}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4785}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LogProber%3A%20Disentangling%20confidence%20from%20contamination%20in%20LLM%20responses&body=Title%3A%20LogProber%3A%20Disentangling%20confidence%20from%20contamination%20in%20LLM%20responses%0AAuthor%3A%20Nicolas%20Yax%20and%20Pierre-Yves%20Oudeyer%20and%20Stefano%20Palminteri%0AAbstract%3A%20%20%20In%20machine%20learning%2C%20contamination%20refers%20to%20situations%20where%20testing%20data%0Aleak%20into%20the%20training%20set.%20The%20issue%20is%20particularly%20relevant%20for%20the%0Aevaluation%20of%20the%20performance%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20which%20are%0Agenerally%20trained%20on%20gargantuan%2C%20and%20generally%20opaque%2C%20corpora%20of%20text%20scraped%0Afrom%20the%20world%20wide%20web.%20Developing%20tools%20to%20detect%20contamination%20is%20therefore%0Acrucial%20to%20be%20able%20to%20fairly%20and%20properly%20track%20the%20evolution%20of%20the%0Aperformance%20of%20LLMs.%20To%20date%2C%20only%20a%20few%20recent%20studies%20have%20attempted%20to%0Aaddress%20the%20issue%20of%20quantifying%20and%20detecting%20contamination%20in%20short%20text%0Asequences%2C%20such%20as%20those%20commonly%20found%20in%20benchmarks.%20However%2C%20these%20methods%0Ahave%20limitations%20that%20can%20sometimes%20render%20them%20impractical.In%20the%20present%0Apaper%2C%20we%20introduce%20LogProber%2C%20a%20novel%2C%20efficient%20algorithm%20that%20we%20show%20to%20be%0Aable%20to%20detect%20contamination%20in%20a%20black%20box%20setting%20that%20tries%20to%20tackle%20some%0Aof%20these%20drawbacks%20by%20focusing%20on%20the%20familiarity%20with%20the%20question%20rather%20than%0Athe%20answer.%20Here%2C%20we%20explore%20the%20properties%20of%20the%20proposed%20method%20in%0Acomparison%20with%20concurrent%20approaches%2C%20identify%20its%20advantages%20and%20limitations%2C%0Aand%20illustrate%20how%20different%20forms%20of%20contamination%20can%20go%20undetected%20depending%0Aon%20the%20design%20of%20the%20detection%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14352v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogProber%253A%2520Disentangling%2520confidence%2520from%2520contamination%2520in%2520LLM%2520responses%26entry.906535625%3DNicolas%2520Yax%2520and%2520Pierre-Yves%2520Oudeyer%2520and%2520Stefano%2520Palminteri%26entry.1292438233%3D%2520%2520In%2520machine%2520learning%252C%2520contamination%2520refers%2520to%2520situations%2520where%2520testing%2520data%250Aleak%2520into%2520the%2520training%2520set.%2520The%2520issue%2520is%2520particularly%2520relevant%2520for%2520the%250Aevaluation%2520of%2520the%2520performance%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520which%2520are%250Agenerally%2520trained%2520on%2520gargantuan%252C%2520and%2520generally%2520opaque%252C%2520corpora%2520of%2520text%2520scraped%250Afrom%2520the%2520world%2520wide%2520web.%2520Developing%2520tools%2520to%2520detect%2520contamination%2520is%2520therefore%250Acrucial%2520to%2520be%2520able%2520to%2520fairly%2520and%2520properly%2520track%2520the%2520evolution%2520of%2520the%250Aperformance%2520of%2520LLMs.%2520To%2520date%252C%2520only%2520a%2520few%2520recent%2520studies%2520have%2520attempted%2520to%250Aaddress%2520the%2520issue%2520of%2520quantifying%2520and%2520detecting%2520contamination%2520in%2520short%2520text%250Asequences%252C%2520such%2520as%2520those%2520commonly%2520found%2520in%2520benchmarks.%2520However%252C%2520these%2520methods%250Ahave%2520limitations%2520that%2520can%2520sometimes%2520render%2520them%2520impractical.In%2520the%2520present%250Apaper%252C%2520we%2520introduce%2520LogProber%252C%2520a%2520novel%252C%2520efficient%2520algorithm%2520that%2520we%2520show%2520to%2520be%250Aable%2520to%2520detect%2520contamination%2520in%2520a%2520black%2520box%2520setting%2520that%2520tries%2520to%2520tackle%2520some%250Aof%2520these%2520drawbacks%2520by%2520focusing%2520on%2520the%2520familiarity%2520with%2520the%2520question%2520rather%2520than%250Athe%2520answer.%2520Here%252C%2520we%2520explore%2520the%2520properties%2520of%2520the%2520proposed%2520method%2520in%250Acomparison%2520with%2520concurrent%2520approaches%252C%2520identify%2520its%2520advantages%2520and%2520limitations%252C%250Aand%2520illustrate%2520how%2520different%2520forms%2520of%2520contamination%2520can%2520go%2520undetected%2520depending%250Aon%2520the%2520design%2520of%2520the%2520detection%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14352v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LogProber%3A%20Disentangling%20confidence%20from%20contamination%20in%20LLM%20responses&entry.906535625=Nicolas%20Yax%20and%20Pierre-Yves%20Oudeyer%20and%20Stefano%20Palminteri&entry.1292438233=%20%20In%20machine%20learning%2C%20contamination%20refers%20to%20situations%20where%20testing%20data%0Aleak%20into%20the%20training%20set.%20The%20issue%20is%20particularly%20relevant%20for%20the%0Aevaluation%20of%20the%20performance%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20which%20are%0Agenerally%20trained%20on%20gargantuan%2C%20and%20generally%20opaque%2C%20corpora%20of%20text%20scraped%0Afrom%20the%20world%20wide%20web.%20Developing%20tools%20to%20detect%20contamination%20is%20therefore%0Acrucial%20to%20be%20able%20to%20fairly%20and%20properly%20track%20the%20evolution%20of%20the%0Aperformance%20of%20LLMs.%20To%20date%2C%20only%20a%20few%20recent%20studies%20have%20attempted%20to%0Aaddress%20the%20issue%20of%20quantifying%20and%20detecting%20contamination%20in%20short%20text%0Asequences%2C%20such%20as%20those%20commonly%20found%20in%20benchmarks.%20However%2C%20these%20methods%0Ahave%20limitations%20that%20can%20sometimes%20render%20them%20impractical.In%20the%20present%0Apaper%2C%20we%20introduce%20LogProber%2C%20a%20novel%2C%20efficient%20algorithm%20that%20we%20show%20to%20be%0Aable%20to%20detect%20contamination%20in%20a%20black%20box%20setting%20that%20tries%20to%20tackle%20some%0Aof%20these%20drawbacks%20by%20focusing%20on%20the%20familiarity%20with%20the%20question%20rather%20than%0Athe%20answer.%20Here%2C%20we%20explore%20the%20properties%20of%20the%20proposed%20method%20in%0Acomparison%20with%20concurrent%20approaches%2C%20identify%20its%20advantages%20and%20limitations%2C%0Aand%20illustrate%20how%20different%20forms%20of%20contamination%20can%20go%20undetected%20depending%0Aon%20the%20design%20of%20the%20detection%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14352v2&entry.124074799=Read"},
{"title": "One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented\n  Generation with a Single Image", "author": "Ezzeldin Shereen and Dan Ristea and Shae McFadden and Burak Hasircioglu and Vasilios Mavroudis and Chris Hicks", "abstract": "  Multi-modal retrieval augmented generation (M-RAG) is instrumental for\ninhibiting hallucinations in large multi-modal models (LMMs) through the use of\na factual knowledge base (KB). However, M-RAG introduces new attack vectors for\nadversaries that aim to disrupt the system by injecting malicious entries into\nthe KB. In this paper, we present the first poisoning attack against M-RAG\ntargeting visual document retrieval applications where the KB contains images\nof document pages. We propose two attacks, each of which require injecting only\na single adversarial image into the KB. Firstly, we propose a universal attack\nthat, for any potential user query, influences the response to cause a\ndenial-of-service (DoS) in the M-RAG system. Secondly, we present a targeted\nattack against one or a group of user queries, with the goal of spreading\ntargeted misinformation. For both attacks, we use a multi-objective\ngradient-based adversarial approach to craft the injected image while\noptimizing for both retrieval and generation. We evaluate our attacks against\nseveral visual document retrieval datasets, a diverse set of state-of-the-art\nretrievers (embedding models) and generators (LMMs), demonstrating the attack\neffectiveness in both the universal and targeted settings. We additionally\npresent results including commonly used defenses, various attack\nhyper-parameter settings, ablations, and attack transferability.\n", "link": "http://arxiv.org/abs/2504.02132v2", "date": "2025-06-11", "relevancy": 1.9205, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5015}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4771}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Pic%20is%20All%20it%20Takes%3A%20Poisoning%20Visual%20Document%20Retrieval%20Augmented%0A%20%20Generation%20with%20a%20Single%20Image&body=Title%3A%20One%20Pic%20is%20All%20it%20Takes%3A%20Poisoning%20Visual%20Document%20Retrieval%20Augmented%0A%20%20Generation%20with%20a%20Single%20Image%0AAuthor%3A%20Ezzeldin%20Shereen%20and%20Dan%20Ristea%20and%20Shae%20McFadden%20and%20Burak%20Hasircioglu%20and%20Vasilios%20Mavroudis%20and%20Chris%20Hicks%0AAbstract%3A%20%20%20Multi-modal%20retrieval%20augmented%20generation%20%28M-RAG%29%20is%20instrumental%20for%0Ainhibiting%20hallucinations%20in%20large%20multi-modal%20models%20%28LMMs%29%20through%20the%20use%20of%0Aa%20factual%20knowledge%20base%20%28KB%29.%20However%2C%20M-RAG%20introduces%20new%20attack%20vectors%20for%0Aadversaries%20that%20aim%20to%20disrupt%20the%20system%20by%20injecting%20malicious%20entries%20into%0Athe%20KB.%20In%20this%20paper%2C%20we%20present%20the%20first%20poisoning%20attack%20against%20M-RAG%0Atargeting%20visual%20document%20retrieval%20applications%20where%20the%20KB%20contains%20images%0Aof%20document%20pages.%20We%20propose%20two%20attacks%2C%20each%20of%20which%20require%20injecting%20only%0Aa%20single%20adversarial%20image%20into%20the%20KB.%20Firstly%2C%20we%20propose%20a%20universal%20attack%0Athat%2C%20for%20any%20potential%20user%20query%2C%20influences%20the%20response%20to%20cause%20a%0Adenial-of-service%20%28DoS%29%20in%20the%20M-RAG%20system.%20Secondly%2C%20we%20present%20a%20targeted%0Aattack%20against%20one%20or%20a%20group%20of%20user%20queries%2C%20with%20the%20goal%20of%20spreading%0Atargeted%20misinformation.%20For%20both%20attacks%2C%20we%20use%20a%20multi-objective%0Agradient-based%20adversarial%20approach%20to%20craft%20the%20injected%20image%20while%0Aoptimizing%20for%20both%20retrieval%20and%20generation.%20We%20evaluate%20our%20attacks%20against%0Aseveral%20visual%20document%20retrieval%20datasets%2C%20a%20diverse%20set%20of%20state-of-the-art%0Aretrievers%20%28embedding%20models%29%20and%20generators%20%28LMMs%29%2C%20demonstrating%20the%20attack%0Aeffectiveness%20in%20both%20the%20universal%20and%20targeted%20settings.%20We%20additionally%0Apresent%20results%20including%20commonly%20used%20defenses%2C%20various%20attack%0Ahyper-parameter%20settings%2C%20ablations%2C%20and%20attack%20transferability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.02132v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Pic%2520is%2520All%2520it%2520Takes%253A%2520Poisoning%2520Visual%2520Document%2520Retrieval%2520Augmented%250A%2520%2520Generation%2520with%2520a%2520Single%2520Image%26entry.906535625%3DEzzeldin%2520Shereen%2520and%2520Dan%2520Ristea%2520and%2520Shae%2520McFadden%2520and%2520Burak%2520Hasircioglu%2520and%2520Vasilios%2520Mavroudis%2520and%2520Chris%2520Hicks%26entry.1292438233%3D%2520%2520Multi-modal%2520retrieval%2520augmented%2520generation%2520%2528M-RAG%2529%2520is%2520instrumental%2520for%250Ainhibiting%2520hallucinations%2520in%2520large%2520multi-modal%2520models%2520%2528LMMs%2529%2520through%2520the%2520use%2520of%250Aa%2520factual%2520knowledge%2520base%2520%2528KB%2529.%2520However%252C%2520M-RAG%2520introduces%2520new%2520attack%2520vectors%2520for%250Aadversaries%2520that%2520aim%2520to%2520disrupt%2520the%2520system%2520by%2520injecting%2520malicious%2520entries%2520into%250Athe%2520KB.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520first%2520poisoning%2520attack%2520against%2520M-RAG%250Atargeting%2520visual%2520document%2520retrieval%2520applications%2520where%2520the%2520KB%2520contains%2520images%250Aof%2520document%2520pages.%2520We%2520propose%2520two%2520attacks%252C%2520each%2520of%2520which%2520require%2520injecting%2520only%250Aa%2520single%2520adversarial%2520image%2520into%2520the%2520KB.%2520Firstly%252C%2520we%2520propose%2520a%2520universal%2520attack%250Athat%252C%2520for%2520any%2520potential%2520user%2520query%252C%2520influences%2520the%2520response%2520to%2520cause%2520a%250Adenial-of-service%2520%2528DoS%2529%2520in%2520the%2520M-RAG%2520system.%2520Secondly%252C%2520we%2520present%2520a%2520targeted%250Aattack%2520against%2520one%2520or%2520a%2520group%2520of%2520user%2520queries%252C%2520with%2520the%2520goal%2520of%2520spreading%250Atargeted%2520misinformation.%2520For%2520both%2520attacks%252C%2520we%2520use%2520a%2520multi-objective%250Agradient-based%2520adversarial%2520approach%2520to%2520craft%2520the%2520injected%2520image%2520while%250Aoptimizing%2520for%2520both%2520retrieval%2520and%2520generation.%2520We%2520evaluate%2520our%2520attacks%2520against%250Aseveral%2520visual%2520document%2520retrieval%2520datasets%252C%2520a%2520diverse%2520set%2520of%2520state-of-the-art%250Aretrievers%2520%2528embedding%2520models%2529%2520and%2520generators%2520%2528LMMs%2529%252C%2520demonstrating%2520the%2520attack%250Aeffectiveness%2520in%2520both%2520the%2520universal%2520and%2520targeted%2520settings.%2520We%2520additionally%250Apresent%2520results%2520including%2520commonly%2520used%2520defenses%252C%2520various%2520attack%250Ahyper-parameter%2520settings%252C%2520ablations%252C%2520and%2520attack%2520transferability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02132v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Pic%20is%20All%20it%20Takes%3A%20Poisoning%20Visual%20Document%20Retrieval%20Augmented%0A%20%20Generation%20with%20a%20Single%20Image&entry.906535625=Ezzeldin%20Shereen%20and%20Dan%20Ristea%20and%20Shae%20McFadden%20and%20Burak%20Hasircioglu%20and%20Vasilios%20Mavroudis%20and%20Chris%20Hicks&entry.1292438233=%20%20Multi-modal%20retrieval%20augmented%20generation%20%28M-RAG%29%20is%20instrumental%20for%0Ainhibiting%20hallucinations%20in%20large%20multi-modal%20models%20%28LMMs%29%20through%20the%20use%20of%0Aa%20factual%20knowledge%20base%20%28KB%29.%20However%2C%20M-RAG%20introduces%20new%20attack%20vectors%20for%0Aadversaries%20that%20aim%20to%20disrupt%20the%20system%20by%20injecting%20malicious%20entries%20into%0Athe%20KB.%20In%20this%20paper%2C%20we%20present%20the%20first%20poisoning%20attack%20against%20M-RAG%0Atargeting%20visual%20document%20retrieval%20applications%20where%20the%20KB%20contains%20images%0Aof%20document%20pages.%20We%20propose%20two%20attacks%2C%20each%20of%20which%20require%20injecting%20only%0Aa%20single%20adversarial%20image%20into%20the%20KB.%20Firstly%2C%20we%20propose%20a%20universal%20attack%0Athat%2C%20for%20any%20potential%20user%20query%2C%20influences%20the%20response%20to%20cause%20a%0Adenial-of-service%20%28DoS%29%20in%20the%20M-RAG%20system.%20Secondly%2C%20we%20present%20a%20targeted%0Aattack%20against%20one%20or%20a%20group%20of%20user%20queries%2C%20with%20the%20goal%20of%20spreading%0Atargeted%20misinformation.%20For%20both%20attacks%2C%20we%20use%20a%20multi-objective%0Agradient-based%20adversarial%20approach%20to%20craft%20the%20injected%20image%20while%0Aoptimizing%20for%20both%20retrieval%20and%20generation.%20We%20evaluate%20our%20attacks%20against%0Aseveral%20visual%20document%20retrieval%20datasets%2C%20a%20diverse%20set%20of%20state-of-the-art%0Aretrievers%20%28embedding%20models%29%20and%20generators%20%28LMMs%29%2C%20demonstrating%20the%20attack%0Aeffectiveness%20in%20both%20the%20universal%20and%20targeted%20settings.%20We%20additionally%0Apresent%20results%20including%20commonly%20used%20defenses%2C%20various%20attack%0Ahyper-parameter%20settings%2C%20ablations%2C%20and%20attack%20transferability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.02132v2&entry.124074799=Read"},
{"title": "DANCE: Deep Learning-Assisted Analysis of Protein Sequences Using Chaos\n  Enhanced Kaleidoscopic Images", "author": "Taslim Murad and Prakash Chourasia and Sarwan Ali and Imdad Ullah Khan and Murray Patterson", "abstract": "  Cancer is a complex disease characterized by uncontrolled cell growth. T cell\nreceptors (TCRs), crucial proteins in the immune system, play a key role in\nrecognizing antigens, including those associated with cancer. Recent\nadvancements in sequencing technologies have facilitated comprehensive\nprofiling of TCR repertoires, uncovering TCRs with potent anti-cancer activity\nand enabling TCR-based immunotherapies. However, analyzing these intricate\nbiomolecules necessitates efficient representations that capture their\nstructural and functional information. T-cell protein sequences pose unique\nchallenges due to their relatively smaller lengths compared to other\nbiomolecules. An image-based representation approach becomes a preferred choice\nfor efficient embeddings, allowing for the preservation of essential details\nand enabling comprehensive analysis of T-cell protein sequences. In this paper,\nwe propose to generate images from the protein sequences using the idea of\nChaos Game Representation (CGR) using the Kaleidoscopic images approach. This\nDeep Learning Assisted Analysis of Protein Sequences Using Chaos Enhanced\nKaleidoscopic Images (called DANCE) provides a unique way to visualize protein\nsequences by recursively applying chaos game rules around a central seed point.\nwe perform the classification of the T cell receptors (TCRs) protein sequences\nin terms of their respective target cancer cells, as TCRs are known for their\nimmune response against cancer disease. The TCR sequences are converted into\nimages using the DANCE method. We employ deep-learning vision models to perform\nthe classification to obtain insights into the relationship between the visual\npatterns observed in the generated kaleidoscopic images and the underlying\nprotein properties. By combining CGR-based image generation with deep learning\nclassification, this study opens novel possibilities in the protein analysis\ndomain.\n", "link": "http://arxiv.org/abs/2409.06694v3", "date": "2025-06-11", "relevancy": 1.9096, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5023}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4724}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DANCE%3A%20Deep%20Learning-Assisted%20Analysis%20of%20Protein%20Sequences%20Using%20Chaos%0A%20%20Enhanced%20Kaleidoscopic%20Images&body=Title%3A%20DANCE%3A%20Deep%20Learning-Assisted%20Analysis%20of%20Protein%20Sequences%20Using%20Chaos%0A%20%20Enhanced%20Kaleidoscopic%20Images%0AAuthor%3A%20Taslim%20Murad%20and%20Prakash%20Chourasia%20and%20Sarwan%20Ali%20and%20Imdad%20Ullah%20Khan%20and%20Murray%20Patterson%0AAbstract%3A%20%20%20Cancer%20is%20a%20complex%20disease%20characterized%20by%20uncontrolled%20cell%20growth.%20T%20cell%0Areceptors%20%28TCRs%29%2C%20crucial%20proteins%20in%20the%20immune%20system%2C%20play%20a%20key%20role%20in%0Arecognizing%20antigens%2C%20including%20those%20associated%20with%20cancer.%20Recent%0Aadvancements%20in%20sequencing%20technologies%20have%20facilitated%20comprehensive%0Aprofiling%20of%20TCR%20repertoires%2C%20uncovering%20TCRs%20with%20potent%20anti-cancer%20activity%0Aand%20enabling%20TCR-based%20immunotherapies.%20However%2C%20analyzing%20these%20intricate%0Abiomolecules%20necessitates%20efficient%20representations%20that%20capture%20their%0Astructural%20and%20functional%20information.%20T-cell%20protein%20sequences%20pose%20unique%0Achallenges%20due%20to%20their%20relatively%20smaller%20lengths%20compared%20to%20other%0Abiomolecules.%20An%20image-based%20representation%20approach%20becomes%20a%20preferred%20choice%0Afor%20efficient%20embeddings%2C%20allowing%20for%20the%20preservation%20of%20essential%20details%0Aand%20enabling%20comprehensive%20analysis%20of%20T-cell%20protein%20sequences.%20In%20this%20paper%2C%0Awe%20propose%20to%20generate%20images%20from%20the%20protein%20sequences%20using%20the%20idea%20of%0AChaos%20Game%20Representation%20%28CGR%29%20using%20the%20Kaleidoscopic%20images%20approach.%20This%0ADeep%20Learning%20Assisted%20Analysis%20of%20Protein%20Sequences%20Using%20Chaos%20Enhanced%0AKaleidoscopic%20Images%20%28called%20DANCE%29%20provides%20a%20unique%20way%20to%20visualize%20protein%0Asequences%20by%20recursively%20applying%20chaos%20game%20rules%20around%20a%20central%20seed%20point.%0Awe%20perform%20the%20classification%20of%20the%20T%20cell%20receptors%20%28TCRs%29%20protein%20sequences%0Ain%20terms%20of%20their%20respective%20target%20cancer%20cells%2C%20as%20TCRs%20are%20known%20for%20their%0Aimmune%20response%20against%20cancer%20disease.%20The%20TCR%20sequences%20are%20converted%20into%0Aimages%20using%20the%20DANCE%20method.%20We%20employ%20deep-learning%20vision%20models%20to%20perform%0Athe%20classification%20to%20obtain%20insights%20into%20the%20relationship%20between%20the%20visual%0Apatterns%20observed%20in%20the%20generated%20kaleidoscopic%20images%20and%20the%20underlying%0Aprotein%20properties.%20By%20combining%20CGR-based%20image%20generation%20with%20deep%20learning%0Aclassification%2C%20this%20study%20opens%20novel%20possibilities%20in%20the%20protein%20analysis%0Adomain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06694v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDANCE%253A%2520Deep%2520Learning-Assisted%2520Analysis%2520of%2520Protein%2520Sequences%2520Using%2520Chaos%250A%2520%2520Enhanced%2520Kaleidoscopic%2520Images%26entry.906535625%3DTaslim%2520Murad%2520and%2520Prakash%2520Chourasia%2520and%2520Sarwan%2520Ali%2520and%2520Imdad%2520Ullah%2520Khan%2520and%2520Murray%2520Patterson%26entry.1292438233%3D%2520%2520Cancer%2520is%2520a%2520complex%2520disease%2520characterized%2520by%2520uncontrolled%2520cell%2520growth.%2520T%2520cell%250Areceptors%2520%2528TCRs%2529%252C%2520crucial%2520proteins%2520in%2520the%2520immune%2520system%252C%2520play%2520a%2520key%2520role%2520in%250Arecognizing%2520antigens%252C%2520including%2520those%2520associated%2520with%2520cancer.%2520Recent%250Aadvancements%2520in%2520sequencing%2520technologies%2520have%2520facilitated%2520comprehensive%250Aprofiling%2520of%2520TCR%2520repertoires%252C%2520uncovering%2520TCRs%2520with%2520potent%2520anti-cancer%2520activity%250Aand%2520enabling%2520TCR-based%2520immunotherapies.%2520However%252C%2520analyzing%2520these%2520intricate%250Abiomolecules%2520necessitates%2520efficient%2520representations%2520that%2520capture%2520their%250Astructural%2520and%2520functional%2520information.%2520T-cell%2520protein%2520sequences%2520pose%2520unique%250Achallenges%2520due%2520to%2520their%2520relatively%2520smaller%2520lengths%2520compared%2520to%2520other%250Abiomolecules.%2520An%2520image-based%2520representation%2520approach%2520becomes%2520a%2520preferred%2520choice%250Afor%2520efficient%2520embeddings%252C%2520allowing%2520for%2520the%2520preservation%2520of%2520essential%2520details%250Aand%2520enabling%2520comprehensive%2520analysis%2520of%2520T-cell%2520protein%2520sequences.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520to%2520generate%2520images%2520from%2520the%2520protein%2520sequences%2520using%2520the%2520idea%2520of%250AChaos%2520Game%2520Representation%2520%2528CGR%2529%2520using%2520the%2520Kaleidoscopic%2520images%2520approach.%2520This%250ADeep%2520Learning%2520Assisted%2520Analysis%2520of%2520Protein%2520Sequences%2520Using%2520Chaos%2520Enhanced%250AKaleidoscopic%2520Images%2520%2528called%2520DANCE%2529%2520provides%2520a%2520unique%2520way%2520to%2520visualize%2520protein%250Asequences%2520by%2520recursively%2520applying%2520chaos%2520game%2520rules%2520around%2520a%2520central%2520seed%2520point.%250Awe%2520perform%2520the%2520classification%2520of%2520the%2520T%2520cell%2520receptors%2520%2528TCRs%2529%2520protein%2520sequences%250Ain%2520terms%2520of%2520their%2520respective%2520target%2520cancer%2520cells%252C%2520as%2520TCRs%2520are%2520known%2520for%2520their%250Aimmune%2520response%2520against%2520cancer%2520disease.%2520The%2520TCR%2520sequences%2520are%2520converted%2520into%250Aimages%2520using%2520the%2520DANCE%2520method.%2520We%2520employ%2520deep-learning%2520vision%2520models%2520to%2520perform%250Athe%2520classification%2520to%2520obtain%2520insights%2520into%2520the%2520relationship%2520between%2520the%2520visual%250Apatterns%2520observed%2520in%2520the%2520generated%2520kaleidoscopic%2520images%2520and%2520the%2520underlying%250Aprotein%2520properties.%2520By%2520combining%2520CGR-based%2520image%2520generation%2520with%2520deep%2520learning%250Aclassification%252C%2520this%2520study%2520opens%2520novel%2520possibilities%2520in%2520the%2520protein%2520analysis%250Adomain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06694v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DANCE%3A%20Deep%20Learning-Assisted%20Analysis%20of%20Protein%20Sequences%20Using%20Chaos%0A%20%20Enhanced%20Kaleidoscopic%20Images&entry.906535625=Taslim%20Murad%20and%20Prakash%20Chourasia%20and%20Sarwan%20Ali%20and%20Imdad%20Ullah%20Khan%20and%20Murray%20Patterson&entry.1292438233=%20%20Cancer%20is%20a%20complex%20disease%20characterized%20by%20uncontrolled%20cell%20growth.%20T%20cell%0Areceptors%20%28TCRs%29%2C%20crucial%20proteins%20in%20the%20immune%20system%2C%20play%20a%20key%20role%20in%0Arecognizing%20antigens%2C%20including%20those%20associated%20with%20cancer.%20Recent%0Aadvancements%20in%20sequencing%20technologies%20have%20facilitated%20comprehensive%0Aprofiling%20of%20TCR%20repertoires%2C%20uncovering%20TCRs%20with%20potent%20anti-cancer%20activity%0Aand%20enabling%20TCR-based%20immunotherapies.%20However%2C%20analyzing%20these%20intricate%0Abiomolecules%20necessitates%20efficient%20representations%20that%20capture%20their%0Astructural%20and%20functional%20information.%20T-cell%20protein%20sequences%20pose%20unique%0Achallenges%20due%20to%20their%20relatively%20smaller%20lengths%20compared%20to%20other%0Abiomolecules.%20An%20image-based%20representation%20approach%20becomes%20a%20preferred%20choice%0Afor%20efficient%20embeddings%2C%20allowing%20for%20the%20preservation%20of%20essential%20details%0Aand%20enabling%20comprehensive%20analysis%20of%20T-cell%20protein%20sequences.%20In%20this%20paper%2C%0Awe%20propose%20to%20generate%20images%20from%20the%20protein%20sequences%20using%20the%20idea%20of%0AChaos%20Game%20Representation%20%28CGR%29%20using%20the%20Kaleidoscopic%20images%20approach.%20This%0ADeep%20Learning%20Assisted%20Analysis%20of%20Protein%20Sequences%20Using%20Chaos%20Enhanced%0AKaleidoscopic%20Images%20%28called%20DANCE%29%20provides%20a%20unique%20way%20to%20visualize%20protein%0Asequences%20by%20recursively%20applying%20chaos%20game%20rules%20around%20a%20central%20seed%20point.%0Awe%20perform%20the%20classification%20of%20the%20T%20cell%20receptors%20%28TCRs%29%20protein%20sequences%0Ain%20terms%20of%20their%20respective%20target%20cancer%20cells%2C%20as%20TCRs%20are%20known%20for%20their%0Aimmune%20response%20against%20cancer%20disease.%20The%20TCR%20sequences%20are%20converted%20into%0Aimages%20using%20the%20DANCE%20method.%20We%20employ%20deep-learning%20vision%20models%20to%20perform%0Athe%20classification%20to%20obtain%20insights%20into%20the%20relationship%20between%20the%20visual%0Apatterns%20observed%20in%20the%20generated%20kaleidoscopic%20images%20and%20the%20underlying%0Aprotein%20properties.%20By%20combining%20CGR-based%20image%20generation%20with%20deep%20learning%0Aclassification%2C%20this%20study%20opens%20novel%20possibilities%20in%20the%20protein%20analysis%0Adomain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06694v3&entry.124074799=Read"},
{"title": "Language Models Resist Alignment: Evidence From Data Compression", "author": "Jiaming Ji and Kaile Wang and Tianyi Qiu and Boyuan Chen and Jiayi Zhou and Changye Li and Hantao Lou and Juntao Dai and Yunhuai Liu and Yaodong Yang", "abstract": "  Large language models (LLMs) may exhibit unintended or undesirable behaviors.\nRecent works have concentrated on aligning LLMs to mitigate harmful outputs.\nDespite these efforts, some anomalies indicate that even a well-conducted\nalignment process can be easily circumvented, whether intentionally or\naccidentally. Does alignment fine-tuning yield have robust effects on models,\nor are its impacts merely superficial? In this work, we make the first\nexploration of this phenomenon from both theoretical and empirical\nperspectives. Empirically, we demonstrate the $\\mathbf{elasticity}$ of\npost-alignment models, i.e., the tendency to revert to the behavior\ndistribution formed during the pre-training phase upon further fine-tuning.\nLeveraging compression theory, we formally deduce that fine-tuning\ndisproportionately undermines alignment relative to pre-training, potentially\nby orders of magnitude. We validate the presence of elasticity through\nexperiments on models of varying types and scales. Specifically, we find that\nmodel performance declines rapidly before reverting to the pre-training\ndistribution, after which the rate of decline drops significantly. Furthermore,\nwe further reveal that elasticity positively correlates with the increased\nmodel size and the expansion of pre-training data. Our findings underscore the\nneed to address the inherent elasticity of LLMs to mitigate their resistance to\nalignment. The model weight and code are available at\npku-lm-resist-alignment.github.io.\n", "link": "http://arxiv.org/abs/2406.06144v4", "date": "2025-06-11", "relevancy": 1.8994, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4916}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4778}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Models%20Resist%20Alignment%3A%20Evidence%20From%20Data%20Compression&body=Title%3A%20Language%20Models%20Resist%20Alignment%3A%20Evidence%20From%20Data%20Compression%0AAuthor%3A%20Jiaming%20Ji%20and%20Kaile%20Wang%20and%20Tianyi%20Qiu%20and%20Boyuan%20Chen%20and%20Jiayi%20Zhou%20and%20Changye%20Li%20and%20Hantao%20Lou%20and%20Juntao%20Dai%20and%20Yunhuai%20Liu%20and%20Yaodong%20Yang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20may%20exhibit%20unintended%20or%20undesirable%20behaviors.%0ARecent%20works%20have%20concentrated%20on%20aligning%20LLMs%20to%20mitigate%20harmful%20outputs.%0ADespite%20these%20efforts%2C%20some%20anomalies%20indicate%20that%20even%20a%20well-conducted%0Aalignment%20process%20can%20be%20easily%20circumvented%2C%20whether%20intentionally%20or%0Aaccidentally.%20Does%20alignment%20fine-tuning%20yield%20have%20robust%20effects%20on%20models%2C%0Aor%20are%20its%20impacts%20merely%20superficial%3F%20In%20this%20work%2C%20we%20make%20the%20first%0Aexploration%20of%20this%20phenomenon%20from%20both%20theoretical%20and%20empirical%0Aperspectives.%20Empirically%2C%20we%20demonstrate%20the%20%24%5Cmathbf%7Belasticity%7D%24%20of%0Apost-alignment%20models%2C%20i.e.%2C%20the%20tendency%20to%20revert%20to%20the%20behavior%0Adistribution%20formed%20during%20the%20pre-training%20phase%20upon%20further%20fine-tuning.%0ALeveraging%20compression%20theory%2C%20we%20formally%20deduce%20that%20fine-tuning%0Adisproportionately%20undermines%20alignment%20relative%20to%20pre-training%2C%20potentially%0Aby%20orders%20of%20magnitude.%20We%20validate%20the%20presence%20of%20elasticity%20through%0Aexperiments%20on%20models%20of%20varying%20types%20and%20scales.%20Specifically%2C%20we%20find%20that%0Amodel%20performance%20declines%20rapidly%20before%20reverting%20to%20the%20pre-training%0Adistribution%2C%20after%20which%20the%20rate%20of%20decline%20drops%20significantly.%20Furthermore%2C%0Awe%20further%20reveal%20that%20elasticity%20positively%20correlates%20with%20the%20increased%0Amodel%20size%20and%20the%20expansion%20of%20pre-training%20data.%20Our%20findings%20underscore%20the%0Aneed%20to%20address%20the%20inherent%20elasticity%20of%20LLMs%20to%20mitigate%20their%20resistance%20to%0Aalignment.%20The%20model%20weight%20and%20code%20are%20available%20at%0Apku-lm-resist-alignment.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06144v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Models%2520Resist%2520Alignment%253A%2520Evidence%2520From%2520Data%2520Compression%26entry.906535625%3DJiaming%2520Ji%2520and%2520Kaile%2520Wang%2520and%2520Tianyi%2520Qiu%2520and%2520Boyuan%2520Chen%2520and%2520Jiayi%2520Zhou%2520and%2520Changye%2520Li%2520and%2520Hantao%2520Lou%2520and%2520Juntao%2520Dai%2520and%2520Yunhuai%2520Liu%2520and%2520Yaodong%2520Yang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520may%2520exhibit%2520unintended%2520or%2520undesirable%2520behaviors.%250ARecent%2520works%2520have%2520concentrated%2520on%2520aligning%2520LLMs%2520to%2520mitigate%2520harmful%2520outputs.%250ADespite%2520these%2520efforts%252C%2520some%2520anomalies%2520indicate%2520that%2520even%2520a%2520well-conducted%250Aalignment%2520process%2520can%2520be%2520easily%2520circumvented%252C%2520whether%2520intentionally%2520or%250Aaccidentally.%2520Does%2520alignment%2520fine-tuning%2520yield%2520have%2520robust%2520effects%2520on%2520models%252C%250Aor%2520are%2520its%2520impacts%2520merely%2520superficial%253F%2520In%2520this%2520work%252C%2520we%2520make%2520the%2520first%250Aexploration%2520of%2520this%2520phenomenon%2520from%2520both%2520theoretical%2520and%2520empirical%250Aperspectives.%2520Empirically%252C%2520we%2520demonstrate%2520the%2520%2524%255Cmathbf%257Belasticity%257D%2524%2520of%250Apost-alignment%2520models%252C%2520i.e.%252C%2520the%2520tendency%2520to%2520revert%2520to%2520the%2520behavior%250Adistribution%2520formed%2520during%2520the%2520pre-training%2520phase%2520upon%2520further%2520fine-tuning.%250ALeveraging%2520compression%2520theory%252C%2520we%2520formally%2520deduce%2520that%2520fine-tuning%250Adisproportionately%2520undermines%2520alignment%2520relative%2520to%2520pre-training%252C%2520potentially%250Aby%2520orders%2520of%2520magnitude.%2520We%2520validate%2520the%2520presence%2520of%2520elasticity%2520through%250Aexperiments%2520on%2520models%2520of%2520varying%2520types%2520and%2520scales.%2520Specifically%252C%2520we%2520find%2520that%250Amodel%2520performance%2520declines%2520rapidly%2520before%2520reverting%2520to%2520the%2520pre-training%250Adistribution%252C%2520after%2520which%2520the%2520rate%2520of%2520decline%2520drops%2520significantly.%2520Furthermore%252C%250Awe%2520further%2520reveal%2520that%2520elasticity%2520positively%2520correlates%2520with%2520the%2520increased%250Amodel%2520size%2520and%2520the%2520expansion%2520of%2520pre-training%2520data.%2520Our%2520findings%2520underscore%2520the%250Aneed%2520to%2520address%2520the%2520inherent%2520elasticity%2520of%2520LLMs%2520to%2520mitigate%2520their%2520resistance%2520to%250Aalignment.%2520The%2520model%2520weight%2520and%2520code%2520are%2520available%2520at%250Apku-lm-resist-alignment.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06144v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Models%20Resist%20Alignment%3A%20Evidence%20From%20Data%20Compression&entry.906535625=Jiaming%20Ji%20and%20Kaile%20Wang%20and%20Tianyi%20Qiu%20and%20Boyuan%20Chen%20and%20Jiayi%20Zhou%20and%20Changye%20Li%20and%20Hantao%20Lou%20and%20Juntao%20Dai%20and%20Yunhuai%20Liu%20and%20Yaodong%20Yang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20may%20exhibit%20unintended%20or%20undesirable%20behaviors.%0ARecent%20works%20have%20concentrated%20on%20aligning%20LLMs%20to%20mitigate%20harmful%20outputs.%0ADespite%20these%20efforts%2C%20some%20anomalies%20indicate%20that%20even%20a%20well-conducted%0Aalignment%20process%20can%20be%20easily%20circumvented%2C%20whether%20intentionally%20or%0Aaccidentally.%20Does%20alignment%20fine-tuning%20yield%20have%20robust%20effects%20on%20models%2C%0Aor%20are%20its%20impacts%20merely%20superficial%3F%20In%20this%20work%2C%20we%20make%20the%20first%0Aexploration%20of%20this%20phenomenon%20from%20both%20theoretical%20and%20empirical%0Aperspectives.%20Empirically%2C%20we%20demonstrate%20the%20%24%5Cmathbf%7Belasticity%7D%24%20of%0Apost-alignment%20models%2C%20i.e.%2C%20the%20tendency%20to%20revert%20to%20the%20behavior%0Adistribution%20formed%20during%20the%20pre-training%20phase%20upon%20further%20fine-tuning.%0ALeveraging%20compression%20theory%2C%20we%20formally%20deduce%20that%20fine-tuning%0Adisproportionately%20undermines%20alignment%20relative%20to%20pre-training%2C%20potentially%0Aby%20orders%20of%20magnitude.%20We%20validate%20the%20presence%20of%20elasticity%20through%0Aexperiments%20on%20models%20of%20varying%20types%20and%20scales.%20Specifically%2C%20we%20find%20that%0Amodel%20performance%20declines%20rapidly%20before%20reverting%20to%20the%20pre-training%0Adistribution%2C%20after%20which%20the%20rate%20of%20decline%20drops%20significantly.%20Furthermore%2C%0Awe%20further%20reveal%20that%20elasticity%20positively%20correlates%20with%20the%20increased%0Amodel%20size%20and%20the%20expansion%20of%20pre-training%20data.%20Our%20findings%20underscore%20the%0Aneed%20to%20address%20the%20inherent%20elasticity%20of%20LLMs%20to%20mitigate%20their%20resistance%20to%0Aalignment.%20The%20model%20weight%20and%20code%20are%20available%20at%0Apku-lm-resist-alignment.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06144v4&entry.124074799=Read"},
{"title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech\n  Emotion Detection", "author": "Christoph Schuhmann and Robert Kaczmarczyk and Gollam Rabby and Felix Friedrich and Maurice Kraus and Kourosh Nadi and Huu Nguyen and Kristian Kersting and S\u00f6ren Auer", "abstract": "  The advancement of text-to-speech and audio generation models necessitates\nrobust benchmarks for evaluating the emotional understanding capabilities of AI\nsystems. Current speech emotion recognition (SER) datasets often exhibit\nlimitations in emotional granularity, privacy concerns, or reliance on acted\nportrayals. This paper introduces EmoNet-Voice, a new resource for speech\nemotion detection, which includes EmoNet-Voice Big, a large-scale pre-training\ndataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,\nand 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human\nexpert annotations. EmoNet-Voice is designed to evaluate SER models on a\nfine-grained spectrum of 40 emotion categories with different levels of\nintensities. Leveraging state-of-the-art voice generation, we curated synthetic\naudio snippets simulating actors portraying scenes designed to evoke specific\nemotions. Crucially, we conducted rigorous validation by psychology experts who\nassigned perceived intensity labels. This synthetic, privacy-preserving\napproach allows for the inclusion of sensitive emotional states often absent in\nexisting datasets. Lastly, we introduce Empathic Insight Voice models that set\na new standard in speech emotion recognition with high agreement with human\nexperts. Our evaluations across the current model landscape exhibit valuable\nfindings, such as high-arousal emotions like anger being much easier to detect\nthan low-arousal states like concentration.\n", "link": "http://arxiv.org/abs/2506.09827v1", "date": "2025-06-11", "relevancy": 1.8987, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4776}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4776}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmoNet-Voice%3A%20A%20Fine-Grained%2C%20Expert-Verified%20Benchmark%20for%20Speech%0A%20%20Emotion%20Detection&body=Title%3A%20EmoNet-Voice%3A%20A%20Fine-Grained%2C%20Expert-Verified%20Benchmark%20for%20Speech%0A%20%20Emotion%20Detection%0AAuthor%3A%20Christoph%20Schuhmann%20and%20Robert%20Kaczmarczyk%20and%20Gollam%20Rabby%20and%20Felix%20Friedrich%20and%20Maurice%20Kraus%20and%20Kourosh%20Nadi%20and%20Huu%20Nguyen%20and%20Kristian%20Kersting%20and%20S%C3%B6ren%20Auer%0AAbstract%3A%20%20%20The%20advancement%20of%20text-to-speech%20and%20audio%20generation%20models%20necessitates%0Arobust%20benchmarks%20for%20evaluating%20the%20emotional%20understanding%20capabilities%20of%20AI%0Asystems.%20Current%20speech%20emotion%20recognition%20%28SER%29%20datasets%20often%20exhibit%0Alimitations%20in%20emotional%20granularity%2C%20privacy%20concerns%2C%20or%20reliance%20on%20acted%0Aportrayals.%20This%20paper%20introduces%20EmoNet-Voice%2C%20a%20new%20resource%20for%20speech%0Aemotion%20detection%2C%20which%20includes%20EmoNet-Voice%20Big%2C%20a%20large-scale%20pre-training%0Adataset%20%28featuring%20over%204%2C500%20hours%20of%20speech%20across%2011%20voices%2C%2040%20emotions%2C%0Aand%204%20languages%29%2C%20and%20EmoNet-Voice%20Bench%2C%20a%20novel%20benchmark%20dataset%20with%20human%0Aexpert%20annotations.%20EmoNet-Voice%20is%20designed%20to%20evaluate%20SER%20models%20on%20a%0Afine-grained%20spectrum%20of%2040%20emotion%20categories%20with%20different%20levels%20of%0Aintensities.%20Leveraging%20state-of-the-art%20voice%20generation%2C%20we%20curated%20synthetic%0Aaudio%20snippets%20simulating%20actors%20portraying%20scenes%20designed%20to%20evoke%20specific%0Aemotions.%20Crucially%2C%20we%20conducted%20rigorous%20validation%20by%20psychology%20experts%20who%0Aassigned%20perceived%20intensity%20labels.%20This%20synthetic%2C%20privacy-preserving%0Aapproach%20allows%20for%20the%20inclusion%20of%20sensitive%20emotional%20states%20often%20absent%20in%0Aexisting%20datasets.%20Lastly%2C%20we%20introduce%20Empathic%20Insight%20Voice%20models%20that%20set%0Aa%20new%20standard%20in%20speech%20emotion%20recognition%20with%20high%20agreement%20with%20human%0Aexperts.%20Our%20evaluations%20across%20the%20current%20model%20landscape%20exhibit%20valuable%0Afindings%2C%20such%20as%20high-arousal%20emotions%20like%20anger%20being%20much%20easier%20to%20detect%0Athan%20low-arousal%20states%20like%20concentration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmoNet-Voice%253A%2520A%2520Fine-Grained%252C%2520Expert-Verified%2520Benchmark%2520for%2520Speech%250A%2520%2520Emotion%2520Detection%26entry.906535625%3DChristoph%2520Schuhmann%2520and%2520Robert%2520Kaczmarczyk%2520and%2520Gollam%2520Rabby%2520and%2520Felix%2520Friedrich%2520and%2520Maurice%2520Kraus%2520and%2520Kourosh%2520Nadi%2520and%2520Huu%2520Nguyen%2520and%2520Kristian%2520Kersting%2520and%2520S%25C3%25B6ren%2520Auer%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520text-to-speech%2520and%2520audio%2520generation%2520models%2520necessitates%250Arobust%2520benchmarks%2520for%2520evaluating%2520the%2520emotional%2520understanding%2520capabilities%2520of%2520AI%250Asystems.%2520Current%2520speech%2520emotion%2520recognition%2520%2528SER%2529%2520datasets%2520often%2520exhibit%250Alimitations%2520in%2520emotional%2520granularity%252C%2520privacy%2520concerns%252C%2520or%2520reliance%2520on%2520acted%250Aportrayals.%2520This%2520paper%2520introduces%2520EmoNet-Voice%252C%2520a%2520new%2520resource%2520for%2520speech%250Aemotion%2520detection%252C%2520which%2520includes%2520EmoNet-Voice%2520Big%252C%2520a%2520large-scale%2520pre-training%250Adataset%2520%2528featuring%2520over%25204%252C500%2520hours%2520of%2520speech%2520across%252011%2520voices%252C%252040%2520emotions%252C%250Aand%25204%2520languages%2529%252C%2520and%2520EmoNet-Voice%2520Bench%252C%2520a%2520novel%2520benchmark%2520dataset%2520with%2520human%250Aexpert%2520annotations.%2520EmoNet-Voice%2520is%2520designed%2520to%2520evaluate%2520SER%2520models%2520on%2520a%250Afine-grained%2520spectrum%2520of%252040%2520emotion%2520categories%2520with%2520different%2520levels%2520of%250Aintensities.%2520Leveraging%2520state-of-the-art%2520voice%2520generation%252C%2520we%2520curated%2520synthetic%250Aaudio%2520snippets%2520simulating%2520actors%2520portraying%2520scenes%2520designed%2520to%2520evoke%2520specific%250Aemotions.%2520Crucially%252C%2520we%2520conducted%2520rigorous%2520validation%2520by%2520psychology%2520experts%2520who%250Aassigned%2520perceived%2520intensity%2520labels.%2520This%2520synthetic%252C%2520privacy-preserving%250Aapproach%2520allows%2520for%2520the%2520inclusion%2520of%2520sensitive%2520emotional%2520states%2520often%2520absent%2520in%250Aexisting%2520datasets.%2520Lastly%252C%2520we%2520introduce%2520Empathic%2520Insight%2520Voice%2520models%2520that%2520set%250Aa%2520new%2520standard%2520in%2520speech%2520emotion%2520recognition%2520with%2520high%2520agreement%2520with%2520human%250Aexperts.%2520Our%2520evaluations%2520across%2520the%2520current%2520model%2520landscape%2520exhibit%2520valuable%250Afindings%252C%2520such%2520as%2520high-arousal%2520emotions%2520like%2520anger%2520being%2520much%2520easier%2520to%2520detect%250Athan%2520low-arousal%2520states%2520like%2520concentration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmoNet-Voice%3A%20A%20Fine-Grained%2C%20Expert-Verified%20Benchmark%20for%20Speech%0A%20%20Emotion%20Detection&entry.906535625=Christoph%20Schuhmann%20and%20Robert%20Kaczmarczyk%20and%20Gollam%20Rabby%20and%20Felix%20Friedrich%20and%20Maurice%20Kraus%20and%20Kourosh%20Nadi%20and%20Huu%20Nguyen%20and%20Kristian%20Kersting%20and%20S%C3%B6ren%20Auer&entry.1292438233=%20%20The%20advancement%20of%20text-to-speech%20and%20audio%20generation%20models%20necessitates%0Arobust%20benchmarks%20for%20evaluating%20the%20emotional%20understanding%20capabilities%20of%20AI%0Asystems.%20Current%20speech%20emotion%20recognition%20%28SER%29%20datasets%20often%20exhibit%0Alimitations%20in%20emotional%20granularity%2C%20privacy%20concerns%2C%20or%20reliance%20on%20acted%0Aportrayals.%20This%20paper%20introduces%20EmoNet-Voice%2C%20a%20new%20resource%20for%20speech%0Aemotion%20detection%2C%20which%20includes%20EmoNet-Voice%20Big%2C%20a%20large-scale%20pre-training%0Adataset%20%28featuring%20over%204%2C500%20hours%20of%20speech%20across%2011%20voices%2C%2040%20emotions%2C%0Aand%204%20languages%29%2C%20and%20EmoNet-Voice%20Bench%2C%20a%20novel%20benchmark%20dataset%20with%20human%0Aexpert%20annotations.%20EmoNet-Voice%20is%20designed%20to%20evaluate%20SER%20models%20on%20a%0Afine-grained%20spectrum%20of%2040%20emotion%20categories%20with%20different%20levels%20of%0Aintensities.%20Leveraging%20state-of-the-art%20voice%20generation%2C%20we%20curated%20synthetic%0Aaudio%20snippets%20simulating%20actors%20portraying%20scenes%20designed%20to%20evoke%20specific%0Aemotions.%20Crucially%2C%20we%20conducted%20rigorous%20validation%20by%20psychology%20experts%20who%0Aassigned%20perceived%20intensity%20labels.%20This%20synthetic%2C%20privacy-preserving%0Aapproach%20allows%20for%20the%20inclusion%20of%20sensitive%20emotional%20states%20often%20absent%20in%0Aexisting%20datasets.%20Lastly%2C%20we%20introduce%20Empathic%20Insight%20Voice%20models%20that%20set%0Aa%20new%20standard%20in%20speech%20emotion%20recognition%20with%20high%20agreement%20with%20human%0Aexperts.%20Our%20evaluations%20across%20the%20current%20model%20landscape%20exhibit%20valuable%0Afindings%2C%20such%20as%20high-arousal%20emotions%20like%20anger%20being%20much%20easier%20to%20detect%0Athan%20low-arousal%20states%20like%20concentration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09827v1&entry.124074799=Read"},
{"title": "Adam Exploits $\\ell_\\infty$-geometry of Loss Landscape via\n  Coordinate-wise Adaptivity", "author": "Shuo Xie and Mohamad Amin Mohamadi and Zhiyuan Li", "abstract": "  Adam outperforms SGD when training language models. Yet this advantage is not\nwell-understood theoretically -- previous convergence analysis for Adam and SGD\nmainly focuses on the number of steps $T$ and is already minimax-optimal in\nnon-convex cases, which are both $\\widetilde{O}(T^{-1/4})$. In this work, we\nargue that the exploitation of nice $\\ell_\\infty$-geometry is the key advantage\nof Adam over SGD. More specifically, we give a new convergence analysis for\nAdam under novel assumptions that loss is smooth under $\\ell_\\infty$-geometry\nrather than the more common $\\ell_2$-geometry, which yields a much better\nempirical smoothness constant for GPT-2 and ResNet models. Our experiments\nconfirm that Adam performs much worse when the favorable $\\ell_\\infty$-geometry\nis changed while SGD provably remains unaffected. We also extend the\nconvergence analysis to blockwise Adam under novel blockwise smoothness\nassumptions.\n", "link": "http://arxiv.org/abs/2410.08198v3", "date": "2025-06-11", "relevancy": 1.8976, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4822}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4723}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adam%20Exploits%20%24%5Cell_%5Cinfty%24-geometry%20of%20Loss%20Landscape%20via%0A%20%20Coordinate-wise%20Adaptivity&body=Title%3A%20Adam%20Exploits%20%24%5Cell_%5Cinfty%24-geometry%20of%20Loss%20Landscape%20via%0A%20%20Coordinate-wise%20Adaptivity%0AAuthor%3A%20Shuo%20Xie%20and%20Mohamad%20Amin%20Mohamadi%20and%20Zhiyuan%20Li%0AAbstract%3A%20%20%20Adam%20outperforms%20SGD%20when%20training%20language%20models.%20Yet%20this%20advantage%20is%20not%0Awell-understood%20theoretically%20--%20previous%20convergence%20analysis%20for%20Adam%20and%20SGD%0Amainly%20focuses%20on%20the%20number%20of%20steps%20%24T%24%20and%20is%20already%20minimax-optimal%20in%0Anon-convex%20cases%2C%20which%20are%20both%20%24%5Cwidetilde%7BO%7D%28T%5E%7B-1/4%7D%29%24.%20In%20this%20work%2C%20we%0Aargue%20that%20the%20exploitation%20of%20nice%20%24%5Cell_%5Cinfty%24-geometry%20is%20the%20key%20advantage%0Aof%20Adam%20over%20SGD.%20More%20specifically%2C%20we%20give%20a%20new%20convergence%20analysis%20for%0AAdam%20under%20novel%20assumptions%20that%20loss%20is%20smooth%20under%20%24%5Cell_%5Cinfty%24-geometry%0Arather%20than%20the%20more%20common%20%24%5Cell_2%24-geometry%2C%20which%20yields%20a%20much%20better%0Aempirical%20smoothness%20constant%20for%20GPT-2%20and%20ResNet%20models.%20Our%20experiments%0Aconfirm%20that%20Adam%20performs%20much%20worse%20when%20the%20favorable%20%24%5Cell_%5Cinfty%24-geometry%0Ais%20changed%20while%20SGD%20provably%20remains%20unaffected.%20We%20also%20extend%20the%0Aconvergence%20analysis%20to%20blockwise%20Adam%20under%20novel%20blockwise%20smoothness%0Aassumptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08198v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdam%2520Exploits%2520%2524%255Cell_%255Cinfty%2524-geometry%2520of%2520Loss%2520Landscape%2520via%250A%2520%2520Coordinate-wise%2520Adaptivity%26entry.906535625%3DShuo%2520Xie%2520and%2520Mohamad%2520Amin%2520Mohamadi%2520and%2520Zhiyuan%2520Li%26entry.1292438233%3D%2520%2520Adam%2520outperforms%2520SGD%2520when%2520training%2520language%2520models.%2520Yet%2520this%2520advantage%2520is%2520not%250Awell-understood%2520theoretically%2520--%2520previous%2520convergence%2520analysis%2520for%2520Adam%2520and%2520SGD%250Amainly%2520focuses%2520on%2520the%2520number%2520of%2520steps%2520%2524T%2524%2520and%2520is%2520already%2520minimax-optimal%2520in%250Anon-convex%2520cases%252C%2520which%2520are%2520both%2520%2524%255Cwidetilde%257BO%257D%2528T%255E%257B-1/4%257D%2529%2524.%2520In%2520this%2520work%252C%2520we%250Aargue%2520that%2520the%2520exploitation%2520of%2520nice%2520%2524%255Cell_%255Cinfty%2524-geometry%2520is%2520the%2520key%2520advantage%250Aof%2520Adam%2520over%2520SGD.%2520More%2520specifically%252C%2520we%2520give%2520a%2520new%2520convergence%2520analysis%2520for%250AAdam%2520under%2520novel%2520assumptions%2520that%2520loss%2520is%2520smooth%2520under%2520%2524%255Cell_%255Cinfty%2524-geometry%250Arather%2520than%2520the%2520more%2520common%2520%2524%255Cell_2%2524-geometry%252C%2520which%2520yields%2520a%2520much%2520better%250Aempirical%2520smoothness%2520constant%2520for%2520GPT-2%2520and%2520ResNet%2520models.%2520Our%2520experiments%250Aconfirm%2520that%2520Adam%2520performs%2520much%2520worse%2520when%2520the%2520favorable%2520%2524%255Cell_%255Cinfty%2524-geometry%250Ais%2520changed%2520while%2520SGD%2520provably%2520remains%2520unaffected.%2520We%2520also%2520extend%2520the%250Aconvergence%2520analysis%2520to%2520blockwise%2520Adam%2520under%2520novel%2520blockwise%2520smoothness%250Aassumptions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08198v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adam%20Exploits%20%24%5Cell_%5Cinfty%24-geometry%20of%20Loss%20Landscape%20via%0A%20%20Coordinate-wise%20Adaptivity&entry.906535625=Shuo%20Xie%20and%20Mohamad%20Amin%20Mohamadi%20and%20Zhiyuan%20Li&entry.1292438233=%20%20Adam%20outperforms%20SGD%20when%20training%20language%20models.%20Yet%20this%20advantage%20is%20not%0Awell-understood%20theoretically%20--%20previous%20convergence%20analysis%20for%20Adam%20and%20SGD%0Amainly%20focuses%20on%20the%20number%20of%20steps%20%24T%24%20and%20is%20already%20minimax-optimal%20in%0Anon-convex%20cases%2C%20which%20are%20both%20%24%5Cwidetilde%7BO%7D%28T%5E%7B-1/4%7D%29%24.%20In%20this%20work%2C%20we%0Aargue%20that%20the%20exploitation%20of%20nice%20%24%5Cell_%5Cinfty%24-geometry%20is%20the%20key%20advantage%0Aof%20Adam%20over%20SGD.%20More%20specifically%2C%20we%20give%20a%20new%20convergence%20analysis%20for%0AAdam%20under%20novel%20assumptions%20that%20loss%20is%20smooth%20under%20%24%5Cell_%5Cinfty%24-geometry%0Arather%20than%20the%20more%20common%20%24%5Cell_2%24-geometry%2C%20which%20yields%20a%20much%20better%0Aempirical%20smoothness%20constant%20for%20GPT-2%20and%20ResNet%20models.%20Our%20experiments%0Aconfirm%20that%20Adam%20performs%20much%20worse%20when%20the%20favorable%20%24%5Cell_%5Cinfty%24-geometry%0Ais%20changed%20while%20SGD%20provably%20remains%20unaffected.%20We%20also%20extend%20the%0Aconvergence%20analysis%20to%20blockwise%20Adam%20under%20novel%20blockwise%20smoothness%0Aassumptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08198v3&entry.124074799=Read"},
{"title": "Conformal Prediction as Bayesian Quadrature", "author": "Jake C. Snell and Thomas L. Griffiths", "abstract": "  As machine learning-based prediction systems are increasingly used in\nhigh-stakes situations, it is important to understand how such predictive\nmodels will perform upon deployment. Distribution-free uncertainty\nquantification techniques such as conformal prediction provide guarantees about\nthe loss black-box models will incur even when the details of the models are\nhidden. However, such methods are based on frequentist probability, which\nunduly limits their applicability. We revisit the central aspects of conformal\nprediction from a Bayesian perspective and thereby illuminate the shortcomings\nof frequentist guarantees. We propose a practical alternative based on Bayesian\nquadrature that provides interpretable guarantees and offers a richer\nrepresentation of the likely range of losses to be observed at test time.\n", "link": "http://arxiv.org/abs/2502.13228v2", "date": "2025-06-11", "relevancy": 1.8923, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5461}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4604}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Prediction%20as%20Bayesian%20Quadrature&body=Title%3A%20Conformal%20Prediction%20as%20Bayesian%20Quadrature%0AAuthor%3A%20Jake%20C.%20Snell%20and%20Thomas%20L.%20Griffiths%0AAbstract%3A%20%20%20As%20machine%20learning-based%20prediction%20systems%20are%20increasingly%20used%20in%0Ahigh-stakes%20situations%2C%20it%20is%20important%20to%20understand%20how%20such%20predictive%0Amodels%20will%20perform%20upon%20deployment.%20Distribution-free%20uncertainty%0Aquantification%20techniques%20such%20as%20conformal%20prediction%20provide%20guarantees%20about%0Athe%20loss%20black-box%20models%20will%20incur%20even%20when%20the%20details%20of%20the%20models%20are%0Ahidden.%20However%2C%20such%20methods%20are%20based%20on%20frequentist%20probability%2C%20which%0Aunduly%20limits%20their%20applicability.%20We%20revisit%20the%20central%20aspects%20of%20conformal%0Aprediction%20from%20a%20Bayesian%20perspective%20and%20thereby%20illuminate%20the%20shortcomings%0Aof%20frequentist%20guarantees.%20We%20propose%20a%20practical%20alternative%20based%20on%20Bayesian%0Aquadrature%20that%20provides%20interpretable%20guarantees%20and%20offers%20a%20richer%0Arepresentation%20of%20the%20likely%20range%20of%20losses%20to%20be%20observed%20at%20test%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13228v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Prediction%2520as%2520Bayesian%2520Quadrature%26entry.906535625%3DJake%2520C.%2520Snell%2520and%2520Thomas%2520L.%2520Griffiths%26entry.1292438233%3D%2520%2520As%2520machine%2520learning-based%2520prediction%2520systems%2520are%2520increasingly%2520used%2520in%250Ahigh-stakes%2520situations%252C%2520it%2520is%2520important%2520to%2520understand%2520how%2520such%2520predictive%250Amodels%2520will%2520perform%2520upon%2520deployment.%2520Distribution-free%2520uncertainty%250Aquantification%2520techniques%2520such%2520as%2520conformal%2520prediction%2520provide%2520guarantees%2520about%250Athe%2520loss%2520black-box%2520models%2520will%2520incur%2520even%2520when%2520the%2520details%2520of%2520the%2520models%2520are%250Ahidden.%2520However%252C%2520such%2520methods%2520are%2520based%2520on%2520frequentist%2520probability%252C%2520which%250Aunduly%2520limits%2520their%2520applicability.%2520We%2520revisit%2520the%2520central%2520aspects%2520of%2520conformal%250Aprediction%2520from%2520a%2520Bayesian%2520perspective%2520and%2520thereby%2520illuminate%2520the%2520shortcomings%250Aof%2520frequentist%2520guarantees.%2520We%2520propose%2520a%2520practical%2520alternative%2520based%2520on%2520Bayesian%250Aquadrature%2520that%2520provides%2520interpretable%2520guarantees%2520and%2520offers%2520a%2520richer%250Arepresentation%2520of%2520the%2520likely%2520range%2520of%2520losses%2520to%2520be%2520observed%2520at%2520test%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13228v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Prediction%20as%20Bayesian%20Quadrature&entry.906535625=Jake%20C.%20Snell%20and%20Thomas%20L.%20Griffiths&entry.1292438233=%20%20As%20machine%20learning-based%20prediction%20systems%20are%20increasingly%20used%20in%0Ahigh-stakes%20situations%2C%20it%20is%20important%20to%20understand%20how%20such%20predictive%0Amodels%20will%20perform%20upon%20deployment.%20Distribution-free%20uncertainty%0Aquantification%20techniques%20such%20as%20conformal%20prediction%20provide%20guarantees%20about%0Athe%20loss%20black-box%20models%20will%20incur%20even%20when%20the%20details%20of%20the%20models%20are%0Ahidden.%20However%2C%20such%20methods%20are%20based%20on%20frequentist%20probability%2C%20which%0Aunduly%20limits%20their%20applicability.%20We%20revisit%20the%20central%20aspects%20of%20conformal%0Aprediction%20from%20a%20Bayesian%20perspective%20and%20thereby%20illuminate%20the%20shortcomings%0Aof%20frequentist%20guarantees.%20We%20propose%20a%20practical%20alternative%20based%20on%20Bayesian%0Aquadrature%20that%20provides%20interpretable%20guarantees%20and%20offers%20a%20richer%0Arepresentation%20of%20the%20likely%20range%20of%20losses%20to%20be%20observed%20at%20test%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13228v2&entry.124074799=Read"},
{"title": "PersonaLens: A Benchmark for Personalization Evaluation in\n  Conversational AI Assistants", "author": "Zheng Zhao and Clara Vania and Subhradeep Kayal and Naila Khan and Shay B. Cohen and Emine Yilmaz", "abstract": "  Large language models (LLMs) have advanced conversational AI assistants.\nHowever, systematically evaluating how well these assistants apply\npersonalization--adapting to individual user preferences while completing\ntasks--remains challenging. Existing personalization benchmarks focus on\nchit-chat, non-conversational tasks, or narrow domains, failing to capture the\ncomplexities of personalized task-oriented assistance. To address this, we\nintroduce PersonaLens, a comprehensive benchmark for evaluating personalization\nin task-oriented AI assistants. Our benchmark features diverse user profiles\nequipped with rich preferences and interaction histories, along with two\nspecialized LLM-based agents: a user agent that engages in realistic\ntask-oriented dialogues with AI assistants, and a judge agent that employs the\nLLM-as-a-Judge paradigm to assess personalization, response quality, and task\nsuccess. Through extensive experiments with current LLM assistants across\ndiverse tasks, we reveal significant variability in their personalization\ncapabilities, providing crucial insights for advancing conversational AI\nsystems.\n", "link": "http://arxiv.org/abs/2506.09902v1", "date": "2025-06-11", "relevancy": 1.8711, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4691}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4677}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PersonaLens%3A%20A%20Benchmark%20for%20Personalization%20Evaluation%20in%0A%20%20Conversational%20AI%20Assistants&body=Title%3A%20PersonaLens%3A%20A%20Benchmark%20for%20Personalization%20Evaluation%20in%0A%20%20Conversational%20AI%20Assistants%0AAuthor%3A%20Zheng%20Zhao%20and%20Clara%20Vania%20and%20Subhradeep%20Kayal%20and%20Naila%20Khan%20and%20Shay%20B.%20Cohen%20and%20Emine%20Yilmaz%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20advanced%20conversational%20AI%20assistants.%0AHowever%2C%20systematically%20evaluating%20how%20well%20these%20assistants%20apply%0Apersonalization--adapting%20to%20individual%20user%20preferences%20while%20completing%0Atasks--remains%20challenging.%20Existing%20personalization%20benchmarks%20focus%20on%0Achit-chat%2C%20non-conversational%20tasks%2C%20or%20narrow%20domains%2C%20failing%20to%20capture%20the%0Acomplexities%20of%20personalized%20task-oriented%20assistance.%20To%20address%20this%2C%20we%0Aintroduce%20PersonaLens%2C%20a%20comprehensive%20benchmark%20for%20evaluating%20personalization%0Ain%20task-oriented%20AI%20assistants.%20Our%20benchmark%20features%20diverse%20user%20profiles%0Aequipped%20with%20rich%20preferences%20and%20interaction%20histories%2C%20along%20with%20two%0Aspecialized%20LLM-based%20agents%3A%20a%20user%20agent%20that%20engages%20in%20realistic%0Atask-oriented%20dialogues%20with%20AI%20assistants%2C%20and%20a%20judge%20agent%20that%20employs%20the%0ALLM-as-a-Judge%20paradigm%20to%20assess%20personalization%2C%20response%20quality%2C%20and%20task%0Asuccess.%20Through%20extensive%20experiments%20with%20current%20LLM%20assistants%20across%0Adiverse%20tasks%2C%20we%20reveal%20significant%20variability%20in%20their%20personalization%0Acapabilities%2C%20providing%20crucial%20insights%20for%20advancing%20conversational%20AI%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonaLens%253A%2520A%2520Benchmark%2520for%2520Personalization%2520Evaluation%2520in%250A%2520%2520Conversational%2520AI%2520Assistants%26entry.906535625%3DZheng%2520Zhao%2520and%2520Clara%2520Vania%2520and%2520Subhradeep%2520Kayal%2520and%2520Naila%2520Khan%2520and%2520Shay%2520B.%2520Cohen%2520and%2520Emine%2520Yilmaz%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520advanced%2520conversational%2520AI%2520assistants.%250AHowever%252C%2520systematically%2520evaluating%2520how%2520well%2520these%2520assistants%2520apply%250Apersonalization--adapting%2520to%2520individual%2520user%2520preferences%2520while%2520completing%250Atasks--remains%2520challenging.%2520Existing%2520personalization%2520benchmarks%2520focus%2520on%250Achit-chat%252C%2520non-conversational%2520tasks%252C%2520or%2520narrow%2520domains%252C%2520failing%2520to%2520capture%2520the%250Acomplexities%2520of%2520personalized%2520task-oriented%2520assistance.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520PersonaLens%252C%2520a%2520comprehensive%2520benchmark%2520for%2520evaluating%2520personalization%250Ain%2520task-oriented%2520AI%2520assistants.%2520Our%2520benchmark%2520features%2520diverse%2520user%2520profiles%250Aequipped%2520with%2520rich%2520preferences%2520and%2520interaction%2520histories%252C%2520along%2520with%2520two%250Aspecialized%2520LLM-based%2520agents%253A%2520a%2520user%2520agent%2520that%2520engages%2520in%2520realistic%250Atask-oriented%2520dialogues%2520with%2520AI%2520assistants%252C%2520and%2520a%2520judge%2520agent%2520that%2520employs%2520the%250ALLM-as-a-Judge%2520paradigm%2520to%2520assess%2520personalization%252C%2520response%2520quality%252C%2520and%2520task%250Asuccess.%2520Through%2520extensive%2520experiments%2520with%2520current%2520LLM%2520assistants%2520across%250Adiverse%2520tasks%252C%2520we%2520reveal%2520significant%2520variability%2520in%2520their%2520personalization%250Acapabilities%252C%2520providing%2520crucial%2520insights%2520for%2520advancing%2520conversational%2520AI%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PersonaLens%3A%20A%20Benchmark%20for%20Personalization%20Evaluation%20in%0A%20%20Conversational%20AI%20Assistants&entry.906535625=Zheng%20Zhao%20and%20Clara%20Vania%20and%20Subhradeep%20Kayal%20and%20Naila%20Khan%20and%20Shay%20B.%20Cohen%20and%20Emine%20Yilmaz&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20advanced%20conversational%20AI%20assistants.%0AHowever%2C%20systematically%20evaluating%20how%20well%20these%20assistants%20apply%0Apersonalization--adapting%20to%20individual%20user%20preferences%20while%20completing%0Atasks--remains%20challenging.%20Existing%20personalization%20benchmarks%20focus%20on%0Achit-chat%2C%20non-conversational%20tasks%2C%20or%20narrow%20domains%2C%20failing%20to%20capture%20the%0Acomplexities%20of%20personalized%20task-oriented%20assistance.%20To%20address%20this%2C%20we%0Aintroduce%20PersonaLens%2C%20a%20comprehensive%20benchmark%20for%20evaluating%20personalization%0Ain%20task-oriented%20AI%20assistants.%20Our%20benchmark%20features%20diverse%20user%20profiles%0Aequipped%20with%20rich%20preferences%20and%20interaction%20histories%2C%20along%20with%20two%0Aspecialized%20LLM-based%20agents%3A%20a%20user%20agent%20that%20engages%20in%20realistic%0Atask-oriented%20dialogues%20with%20AI%20assistants%2C%20and%20a%20judge%20agent%20that%20employs%20the%0ALLM-as-a-Judge%20paradigm%20to%20assess%20personalization%2C%20response%20quality%2C%20and%20task%0Asuccess.%20Through%20extensive%20experiments%20with%20current%20LLM%20assistants%20across%0Adiverse%20tasks%2C%20we%20reveal%20significant%20variability%20in%20their%20personalization%0Acapabilities%2C%20providing%20crucial%20insights%20for%20advancing%20conversational%20AI%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09902v1&entry.124074799=Read"},
{"title": "Optimal Noise Reduction in Dense Mixed-Membership Stochastic Block\n  Models under Diverging Spiked Eigenvalues Condition", "author": "Fedor Noskov and Maxim Panov", "abstract": "  Community detection is one of the most critical problems in modern network\nscience. Its applications can be found in various fields, from protein modeling\nto social network analysis. Recently, many papers appeared studying the problem\nof overlapping community detection, where each node of a network may belong to\nseveral communities. In this work, we consider Mixed-Membership Stochastic\nBlock Model (MMSB) first proposed by Airoldi et al. MMSB provides quite a\ngeneral setting for modeling overlapping community structure in graphs. The\ncentral question of this paper is to reconstruct relations between communities\ngiven an observed network. We compare different approaches and establish the\nminimax lower bound on the estimation error. Then, we propose a new estimator\nthat matches this lower bound. Theoretical results are proved under fairly\ngeneral conditions on the considered model. Finally, we illustrate the theory\nin a series of experiments.\n", "link": "http://arxiv.org/abs/2307.14530v3", "date": "2025-06-11", "relevancy": 1.8554, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4707}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4693}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Noise%20Reduction%20in%20Dense%20Mixed-Membership%20Stochastic%20Block%0A%20%20Models%20under%20Diverging%20Spiked%20Eigenvalues%20Condition&body=Title%3A%20Optimal%20Noise%20Reduction%20in%20Dense%20Mixed-Membership%20Stochastic%20Block%0A%20%20Models%20under%20Diverging%20Spiked%20Eigenvalues%20Condition%0AAuthor%3A%20Fedor%20Noskov%20and%20Maxim%20Panov%0AAbstract%3A%20%20%20Community%20detection%20is%20one%20of%20the%20most%20critical%20problems%20in%20modern%20network%0Ascience.%20Its%20applications%20can%20be%20found%20in%20various%20fields%2C%20from%20protein%20modeling%0Ato%20social%20network%20analysis.%20Recently%2C%20many%20papers%20appeared%20studying%20the%20problem%0Aof%20overlapping%20community%20detection%2C%20where%20each%20node%20of%20a%20network%20may%20belong%20to%0Aseveral%20communities.%20In%20this%20work%2C%20we%20consider%20Mixed-Membership%20Stochastic%0ABlock%20Model%20%28MMSB%29%20first%20proposed%20by%20Airoldi%20et%20al.%20MMSB%20provides%20quite%20a%0Ageneral%20setting%20for%20modeling%20overlapping%20community%20structure%20in%20graphs.%20The%0Acentral%20question%20of%20this%20paper%20is%20to%20reconstruct%20relations%20between%20communities%0Agiven%20an%20observed%20network.%20We%20compare%20different%20approaches%20and%20establish%20the%0Aminimax%20lower%20bound%20on%20the%20estimation%20error.%20Then%2C%20we%20propose%20a%20new%20estimator%0Athat%20matches%20this%20lower%20bound.%20Theoretical%20results%20are%20proved%20under%20fairly%0Ageneral%20conditions%20on%20the%20considered%20model.%20Finally%2C%20we%20illustrate%20the%20theory%0Ain%20a%20series%20of%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.14530v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Noise%2520Reduction%2520in%2520Dense%2520Mixed-Membership%2520Stochastic%2520Block%250A%2520%2520Models%2520under%2520Diverging%2520Spiked%2520Eigenvalues%2520Condition%26entry.906535625%3DFedor%2520Noskov%2520and%2520Maxim%2520Panov%26entry.1292438233%3D%2520%2520Community%2520detection%2520is%2520one%2520of%2520the%2520most%2520critical%2520problems%2520in%2520modern%2520network%250Ascience.%2520Its%2520applications%2520can%2520be%2520found%2520in%2520various%2520fields%252C%2520from%2520protein%2520modeling%250Ato%2520social%2520network%2520analysis.%2520Recently%252C%2520many%2520papers%2520appeared%2520studying%2520the%2520problem%250Aof%2520overlapping%2520community%2520detection%252C%2520where%2520each%2520node%2520of%2520a%2520network%2520may%2520belong%2520to%250Aseveral%2520communities.%2520In%2520this%2520work%252C%2520we%2520consider%2520Mixed-Membership%2520Stochastic%250ABlock%2520Model%2520%2528MMSB%2529%2520first%2520proposed%2520by%2520Airoldi%2520et%2520al.%2520MMSB%2520provides%2520quite%2520a%250Ageneral%2520setting%2520for%2520modeling%2520overlapping%2520community%2520structure%2520in%2520graphs.%2520The%250Acentral%2520question%2520of%2520this%2520paper%2520is%2520to%2520reconstruct%2520relations%2520between%2520communities%250Agiven%2520an%2520observed%2520network.%2520We%2520compare%2520different%2520approaches%2520and%2520establish%2520the%250Aminimax%2520lower%2520bound%2520on%2520the%2520estimation%2520error.%2520Then%252C%2520we%2520propose%2520a%2520new%2520estimator%250Athat%2520matches%2520this%2520lower%2520bound.%2520Theoretical%2520results%2520are%2520proved%2520under%2520fairly%250Ageneral%2520conditions%2520on%2520the%2520considered%2520model.%2520Finally%252C%2520we%2520illustrate%2520the%2520theory%250Ain%2520a%2520series%2520of%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.14530v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Noise%20Reduction%20in%20Dense%20Mixed-Membership%20Stochastic%20Block%0A%20%20Models%20under%20Diverging%20Spiked%20Eigenvalues%20Condition&entry.906535625=Fedor%20Noskov%20and%20Maxim%20Panov&entry.1292438233=%20%20Community%20detection%20is%20one%20of%20the%20most%20critical%20problems%20in%20modern%20network%0Ascience.%20Its%20applications%20can%20be%20found%20in%20various%20fields%2C%20from%20protein%20modeling%0Ato%20social%20network%20analysis.%20Recently%2C%20many%20papers%20appeared%20studying%20the%20problem%0Aof%20overlapping%20community%20detection%2C%20where%20each%20node%20of%20a%20network%20may%20belong%20to%0Aseveral%20communities.%20In%20this%20work%2C%20we%20consider%20Mixed-Membership%20Stochastic%0ABlock%20Model%20%28MMSB%29%20first%20proposed%20by%20Airoldi%20et%20al.%20MMSB%20provides%20quite%20a%0Ageneral%20setting%20for%20modeling%20overlapping%20community%20structure%20in%20graphs.%20The%0Acentral%20question%20of%20this%20paper%20is%20to%20reconstruct%20relations%20between%20communities%0Agiven%20an%20observed%20network.%20We%20compare%20different%20approaches%20and%20establish%20the%0Aminimax%20lower%20bound%20on%20the%20estimation%20error.%20Then%2C%20we%20propose%20a%20new%20estimator%0Athat%20matches%20this%20lower%20bound.%20Theoretical%20results%20are%20proved%20under%20fairly%0Ageneral%20conditions%20on%20the%20considered%20model.%20Finally%2C%20we%20illustrate%20the%20theory%0Ain%20a%20series%20of%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.14530v3&entry.124074799=Read"},
{"title": "Provable Benefits of Unsupervised Pre-training and Transfer Learning via\n  Single-Index Models", "author": "Taj Jones-McCormick and Aukosh Jagannath and Subhabrata Sen", "abstract": "  Unsupervised pre-training and transfer learning are commonly used techniques\nto initialize training algorithms for neural networks, particularly in settings\nwith limited labeled data. In this paper, we study the effects of unsupervised\npre-training and transfer learning on the sample complexity of high-dimensional\nsupervised learning. Specifically, we consider the problem of training a\nsingle-layer neural network via online stochastic gradient descent. We\nestablish that pre-training and transfer learning (under concept shift) reduce\nsample complexity by polynomial factors (in the dimension) under very general\nassumptions. We also uncover some surprising settings where pre-training grants\nexponential improvement over random initialization in terms of sample\ncomplexity.\n", "link": "http://arxiv.org/abs/2502.16849v2", "date": "2025-06-11", "relevancy": 1.8485, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4704}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4576}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provable%20Benefits%20of%20Unsupervised%20Pre-training%20and%20Transfer%20Learning%20via%0A%20%20Single-Index%20Models&body=Title%3A%20Provable%20Benefits%20of%20Unsupervised%20Pre-training%20and%20Transfer%20Learning%20via%0A%20%20Single-Index%20Models%0AAuthor%3A%20Taj%20Jones-McCormick%20and%20Aukosh%20Jagannath%20and%20Subhabrata%20Sen%0AAbstract%3A%20%20%20Unsupervised%20pre-training%20and%20transfer%20learning%20are%20commonly%20used%20techniques%0Ato%20initialize%20training%20algorithms%20for%20neural%20networks%2C%20particularly%20in%20settings%0Awith%20limited%20labeled%20data.%20In%20this%20paper%2C%20we%20study%20the%20effects%20of%20unsupervised%0Apre-training%20and%20transfer%20learning%20on%20the%20sample%20complexity%20of%20high-dimensional%0Asupervised%20learning.%20Specifically%2C%20we%20consider%20the%20problem%20of%20training%20a%0Asingle-layer%20neural%20network%20via%20online%20stochastic%20gradient%20descent.%20We%0Aestablish%20that%20pre-training%20and%20transfer%20learning%20%28under%20concept%20shift%29%20reduce%0Asample%20complexity%20by%20polynomial%20factors%20%28in%20the%20dimension%29%20under%20very%20general%0Aassumptions.%20We%20also%20uncover%20some%20surprising%20settings%20where%20pre-training%20grants%0Aexponential%20improvement%20over%20random%20initialization%20in%20terms%20of%20sample%0Acomplexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.16849v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvable%2520Benefits%2520of%2520Unsupervised%2520Pre-training%2520and%2520Transfer%2520Learning%2520via%250A%2520%2520Single-Index%2520Models%26entry.906535625%3DTaj%2520Jones-McCormick%2520and%2520Aukosh%2520Jagannath%2520and%2520Subhabrata%2520Sen%26entry.1292438233%3D%2520%2520Unsupervised%2520pre-training%2520and%2520transfer%2520learning%2520are%2520commonly%2520used%2520techniques%250Ato%2520initialize%2520training%2520algorithms%2520for%2520neural%2520networks%252C%2520particularly%2520in%2520settings%250Awith%2520limited%2520labeled%2520data.%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520effects%2520of%2520unsupervised%250Apre-training%2520and%2520transfer%2520learning%2520on%2520the%2520sample%2520complexity%2520of%2520high-dimensional%250Asupervised%2520learning.%2520Specifically%252C%2520we%2520consider%2520the%2520problem%2520of%2520training%2520a%250Asingle-layer%2520neural%2520network%2520via%2520online%2520stochastic%2520gradient%2520descent.%2520We%250Aestablish%2520that%2520pre-training%2520and%2520transfer%2520learning%2520%2528under%2520concept%2520shift%2529%2520reduce%250Asample%2520complexity%2520by%2520polynomial%2520factors%2520%2528in%2520the%2520dimension%2529%2520under%2520very%2520general%250Aassumptions.%2520We%2520also%2520uncover%2520some%2520surprising%2520settings%2520where%2520pre-training%2520grants%250Aexponential%2520improvement%2520over%2520random%2520initialization%2520in%2520terms%2520of%2520sample%250Acomplexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.16849v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provable%20Benefits%20of%20Unsupervised%20Pre-training%20and%20Transfer%20Learning%20via%0A%20%20Single-Index%20Models&entry.906535625=Taj%20Jones-McCormick%20and%20Aukosh%20Jagannath%20and%20Subhabrata%20Sen&entry.1292438233=%20%20Unsupervised%20pre-training%20and%20transfer%20learning%20are%20commonly%20used%20techniques%0Ato%20initialize%20training%20algorithms%20for%20neural%20networks%2C%20particularly%20in%20settings%0Awith%20limited%20labeled%20data.%20In%20this%20paper%2C%20we%20study%20the%20effects%20of%20unsupervised%0Apre-training%20and%20transfer%20learning%20on%20the%20sample%20complexity%20of%20high-dimensional%0Asupervised%20learning.%20Specifically%2C%20we%20consider%20the%20problem%20of%20training%20a%0Asingle-layer%20neural%20network%20via%20online%20stochastic%20gradient%20descent.%20We%0Aestablish%20that%20pre-training%20and%20transfer%20learning%20%28under%20concept%20shift%29%20reduce%0Asample%20complexity%20by%20polynomial%20factors%20%28in%20the%20dimension%29%20under%20very%20general%0Aassumptions.%20We%20also%20uncover%20some%20surprising%20settings%20where%20pre-training%20grants%0Aexponential%20improvement%20over%20random%20initialization%20in%20terms%20of%20sample%0Acomplexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.16849v2&entry.124074799=Read"},
{"title": "Scalable First-order Method for Certifying Optimal k-Sparse GLMs", "author": "Jiachang Liu and Soroosh Shafiee and Andrea Lodi", "abstract": "  This paper investigates the problem of certifying optimality for sparse\ngeneralized linear models (GLMs), where sparsity is enforced through an\n$\\ell_0$ cardinality constraint. While branch-and-bound (BnB) frameworks can\ncertify optimality by pruning nodes using dual bounds, existing methods for\ncomputing these bounds are either computationally intensive or exhibit slow\nconvergence, limiting their scalability to large-scale problems. To address\nthis challenge, we propose a first-order proximal gradient algorithm designed\nto solve the perspective relaxation of the problem within a BnB framework.\nSpecifically, we formulate the relaxed problem as a composite optimization\nproblem and demonstrate that the proximal operator of the non-smooth component\ncan be computed exactly in log-linear time complexity, eliminating the need to\nsolve a computationally expensive second-order cone program. Furthermore, we\nintroduce a simple restart strategy that enhances convergence speed while\nmaintaining low per-iteration complexity. Extensive experiments on synthetic\nand real-world datasets show that our approach significantly accelerates dual\nbound computations and is highly effective in providing optimality certificates\nfor large-scale problems.\n", "link": "http://arxiv.org/abs/2502.09502v3", "date": "2025-06-11", "relevancy": 1.8334, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4708}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4587}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20First-order%20Method%20for%20Certifying%20Optimal%20k-Sparse%20GLMs&body=Title%3A%20Scalable%20First-order%20Method%20for%20Certifying%20Optimal%20k-Sparse%20GLMs%0AAuthor%3A%20Jiachang%20Liu%20and%20Soroosh%20Shafiee%20and%20Andrea%20Lodi%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20problem%20of%20certifying%20optimality%20for%20sparse%0Ageneralized%20linear%20models%20%28GLMs%29%2C%20where%20sparsity%20is%20enforced%20through%20an%0A%24%5Cell_0%24%20cardinality%20constraint.%20While%20branch-and-bound%20%28BnB%29%20frameworks%20can%0Acertify%20optimality%20by%20pruning%20nodes%20using%20dual%20bounds%2C%20existing%20methods%20for%0Acomputing%20these%20bounds%20are%20either%20computationally%20intensive%20or%20exhibit%20slow%0Aconvergence%2C%20limiting%20their%20scalability%20to%20large-scale%20problems.%20To%20address%0Athis%20challenge%2C%20we%20propose%20a%20first-order%20proximal%20gradient%20algorithm%20designed%0Ato%20solve%20the%20perspective%20relaxation%20of%20the%20problem%20within%20a%20BnB%20framework.%0ASpecifically%2C%20we%20formulate%20the%20relaxed%20problem%20as%20a%20composite%20optimization%0Aproblem%20and%20demonstrate%20that%20the%20proximal%20operator%20of%20the%20non-smooth%20component%0Acan%20be%20computed%20exactly%20in%20log-linear%20time%20complexity%2C%20eliminating%20the%20need%20to%0Asolve%20a%20computationally%20expensive%20second-order%20cone%20program.%20Furthermore%2C%20we%0Aintroduce%20a%20simple%20restart%20strategy%20that%20enhances%20convergence%20speed%20while%0Amaintaining%20low%20per-iteration%20complexity.%20Extensive%20experiments%20on%20synthetic%0Aand%20real-world%20datasets%20show%20that%20our%20approach%20significantly%20accelerates%20dual%0Abound%20computations%20and%20is%20highly%20effective%20in%20providing%20optimality%20certificates%0Afor%20large-scale%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09502v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520First-order%2520Method%2520for%2520Certifying%2520Optimal%2520k-Sparse%2520GLMs%26entry.906535625%3DJiachang%2520Liu%2520and%2520Soroosh%2520Shafiee%2520and%2520Andrea%2520Lodi%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520problem%2520of%2520certifying%2520optimality%2520for%2520sparse%250Ageneralized%2520linear%2520models%2520%2528GLMs%2529%252C%2520where%2520sparsity%2520is%2520enforced%2520through%2520an%250A%2524%255Cell_0%2524%2520cardinality%2520constraint.%2520While%2520branch-and-bound%2520%2528BnB%2529%2520frameworks%2520can%250Acertify%2520optimality%2520by%2520pruning%2520nodes%2520using%2520dual%2520bounds%252C%2520existing%2520methods%2520for%250Acomputing%2520these%2520bounds%2520are%2520either%2520computationally%2520intensive%2520or%2520exhibit%2520slow%250Aconvergence%252C%2520limiting%2520their%2520scalability%2520to%2520large-scale%2520problems.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520propose%2520a%2520first-order%2520proximal%2520gradient%2520algorithm%2520designed%250Ato%2520solve%2520the%2520perspective%2520relaxation%2520of%2520the%2520problem%2520within%2520a%2520BnB%2520framework.%250ASpecifically%252C%2520we%2520formulate%2520the%2520relaxed%2520problem%2520as%2520a%2520composite%2520optimization%250Aproblem%2520and%2520demonstrate%2520that%2520the%2520proximal%2520operator%2520of%2520the%2520non-smooth%2520component%250Acan%2520be%2520computed%2520exactly%2520in%2520log-linear%2520time%2520complexity%252C%2520eliminating%2520the%2520need%2520to%250Asolve%2520a%2520computationally%2520expensive%2520second-order%2520cone%2520program.%2520Furthermore%252C%2520we%250Aintroduce%2520a%2520simple%2520restart%2520strategy%2520that%2520enhances%2520convergence%2520speed%2520while%250Amaintaining%2520low%2520per-iteration%2520complexity.%2520Extensive%2520experiments%2520on%2520synthetic%250Aand%2520real-world%2520datasets%2520show%2520that%2520our%2520approach%2520significantly%2520accelerates%2520dual%250Abound%2520computations%2520and%2520is%2520highly%2520effective%2520in%2520providing%2520optimality%2520certificates%250Afor%2520large-scale%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09502v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20First-order%20Method%20for%20Certifying%20Optimal%20k-Sparse%20GLMs&entry.906535625=Jiachang%20Liu%20and%20Soroosh%20Shafiee%20and%20Andrea%20Lodi&entry.1292438233=%20%20This%20paper%20investigates%20the%20problem%20of%20certifying%20optimality%20for%20sparse%0Ageneralized%20linear%20models%20%28GLMs%29%2C%20where%20sparsity%20is%20enforced%20through%20an%0A%24%5Cell_0%24%20cardinality%20constraint.%20While%20branch-and-bound%20%28BnB%29%20frameworks%20can%0Acertify%20optimality%20by%20pruning%20nodes%20using%20dual%20bounds%2C%20existing%20methods%20for%0Acomputing%20these%20bounds%20are%20either%20computationally%20intensive%20or%20exhibit%20slow%0Aconvergence%2C%20limiting%20their%20scalability%20to%20large-scale%20problems.%20To%20address%0Athis%20challenge%2C%20we%20propose%20a%20first-order%20proximal%20gradient%20algorithm%20designed%0Ato%20solve%20the%20perspective%20relaxation%20of%20the%20problem%20within%20a%20BnB%20framework.%0ASpecifically%2C%20we%20formulate%20the%20relaxed%20problem%20as%20a%20composite%20optimization%0Aproblem%20and%20demonstrate%20that%20the%20proximal%20operator%20of%20the%20non-smooth%20component%0Acan%20be%20computed%20exactly%20in%20log-linear%20time%20complexity%2C%20eliminating%20the%20need%20to%0Asolve%20a%20computationally%20expensive%20second-order%20cone%20program.%20Furthermore%2C%20we%0Aintroduce%20a%20simple%20restart%20strategy%20that%20enhances%20convergence%20speed%20while%0Amaintaining%20low%20per-iteration%20complexity.%20Extensive%20experiments%20on%20synthetic%0Aand%20real-world%20datasets%20show%20that%20our%20approach%20significantly%20accelerates%20dual%0Abound%20computations%20and%20is%20highly%20effective%20in%20providing%20optimality%20certificates%0Afor%20large-scale%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09502v3&entry.124074799=Read"},
{"title": "Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI\n  for Enhanced Accuracy in Global Financial Markets", "author": "Md. Yeasin Rahat and Rajan Das Gupta and Nur Raisa Rahman and Sudipto Roy Pritom and Samiur Rahman Shakir and Md Imrul Hasan Showmick and Md. Jakir Hossen", "abstract": "  The prediction of foreign exchange rates, such as the US Dollar (USD) to\nBangladeshi Taka (BDT), plays a pivotal role in global financial markets,\ninfluencing trade, investments, and economic stability. This study leverages\nhistorical USD/BDT exchange rate data from 2018 to 2023, sourced from Yahoo\nFinance, to develop advanced machine learning models for accurate forecasting.\nA Long Short-Term Memory (LSTM) neural network is employed, achieving an\nexceptional accuracy of 99.449%, a Root Mean Square Error (RMSE) of 0.9858, and\na test loss of 0.8523, significantly outperforming traditional methods like\nARIMA (RMSE 1.342). Additionally, a Gradient Boosting Classifier (GBC) is\napplied for directional prediction, with backtesting on a $10,000 initial\ncapital revealing a 40.82% profitable trade rate, though resulting in a net\nloss of $20,653.25 over 49 trades. The study analyzes historical trends,\nshowing a decline in BDT/USD rates from 0.012 to 0.009, and incorporates\nnormalized daily returns to capture volatility. These findings highlight the\npotential of deep learning in forex forecasting, offering traders and\npolicymakers robust tools to mitigate risks. Future work could integrate\nsentiment analysis and real-time economic indicators to further enhance model\nadaptability in volatile markets.\n", "link": "http://arxiv.org/abs/2506.09851v1", "date": "2025-06-11", "relevancy": 1.2876, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4444}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4219}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Exchange%20Rate%20Forecasting%3A%20Leveraging%20Machine%20Learning%20and%20AI%0A%20%20for%20Enhanced%20Accuracy%20in%20Global%20Financial%20Markets&body=Title%3A%20Advancing%20Exchange%20Rate%20Forecasting%3A%20Leveraging%20Machine%20Learning%20and%20AI%0A%20%20for%20Enhanced%20Accuracy%20in%20Global%20Financial%20Markets%0AAuthor%3A%20Md.%20Yeasin%20Rahat%20and%20Rajan%20Das%20Gupta%20and%20Nur%20Raisa%20Rahman%20and%20Sudipto%20Roy%20Pritom%20and%20Samiur%20Rahman%20Shakir%20and%20Md%20Imrul%20Hasan%20Showmick%20and%20Md.%20Jakir%20Hossen%0AAbstract%3A%20%20%20The%20prediction%20of%20foreign%20exchange%20rates%2C%20such%20as%20the%20US%20Dollar%20%28USD%29%20to%0ABangladeshi%20Taka%20%28BDT%29%2C%20plays%20a%20pivotal%20role%20in%20global%20financial%20markets%2C%0Ainfluencing%20trade%2C%20investments%2C%20and%20economic%20stability.%20This%20study%20leverages%0Ahistorical%20USD/BDT%20exchange%20rate%20data%20from%202018%20to%202023%2C%20sourced%20from%20Yahoo%0AFinance%2C%20to%20develop%20advanced%20machine%20learning%20models%20for%20accurate%20forecasting.%0AA%20Long%20Short-Term%20Memory%20%28LSTM%29%20neural%20network%20is%20employed%2C%20achieving%20an%0Aexceptional%20accuracy%20of%2099.449%25%2C%20a%20Root%20Mean%20Square%20Error%20%28RMSE%29%20of%200.9858%2C%20and%0Aa%20test%20loss%20of%200.8523%2C%20significantly%20outperforming%20traditional%20methods%20like%0AARIMA%20%28RMSE%201.342%29.%20Additionally%2C%20a%20Gradient%20Boosting%20Classifier%20%28GBC%29%20is%0Aapplied%20for%20directional%20prediction%2C%20with%20backtesting%20on%20a%20%2410%2C000%20initial%0Acapital%20revealing%20a%2040.82%25%20profitable%20trade%20rate%2C%20though%20resulting%20in%20a%20net%0Aloss%20of%20%2420%2C653.25%20over%2049%20trades.%20The%20study%20analyzes%20historical%20trends%2C%0Ashowing%20a%20decline%20in%20BDT/USD%20rates%20from%200.012%20to%200.009%2C%20and%20incorporates%0Anormalized%20daily%20returns%20to%20capture%20volatility.%20These%20findings%20highlight%20the%0Apotential%20of%20deep%20learning%20in%20forex%20forecasting%2C%20offering%20traders%20and%0Apolicymakers%20robust%20tools%20to%20mitigate%20risks.%20Future%20work%20could%20integrate%0Asentiment%20analysis%20and%20real-time%20economic%20indicators%20to%20further%20enhance%20model%0Aadaptability%20in%20volatile%20markets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09851v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Exchange%2520Rate%2520Forecasting%253A%2520Leveraging%2520Machine%2520Learning%2520and%2520AI%250A%2520%2520for%2520Enhanced%2520Accuracy%2520in%2520Global%2520Financial%2520Markets%26entry.906535625%3DMd.%2520Yeasin%2520Rahat%2520and%2520Rajan%2520Das%2520Gupta%2520and%2520Nur%2520Raisa%2520Rahman%2520and%2520Sudipto%2520Roy%2520Pritom%2520and%2520Samiur%2520Rahman%2520Shakir%2520and%2520Md%2520Imrul%2520Hasan%2520Showmick%2520and%2520Md.%2520Jakir%2520Hossen%26entry.1292438233%3D%2520%2520The%2520prediction%2520of%2520foreign%2520exchange%2520rates%252C%2520such%2520as%2520the%2520US%2520Dollar%2520%2528USD%2529%2520to%250ABangladeshi%2520Taka%2520%2528BDT%2529%252C%2520plays%2520a%2520pivotal%2520role%2520in%2520global%2520financial%2520markets%252C%250Ainfluencing%2520trade%252C%2520investments%252C%2520and%2520economic%2520stability.%2520This%2520study%2520leverages%250Ahistorical%2520USD/BDT%2520exchange%2520rate%2520data%2520from%25202018%2520to%25202023%252C%2520sourced%2520from%2520Yahoo%250AFinance%252C%2520to%2520develop%2520advanced%2520machine%2520learning%2520models%2520for%2520accurate%2520forecasting.%250AA%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%2520neural%2520network%2520is%2520employed%252C%2520achieving%2520an%250Aexceptional%2520accuracy%2520of%252099.449%2525%252C%2520a%2520Root%2520Mean%2520Square%2520Error%2520%2528RMSE%2529%2520of%25200.9858%252C%2520and%250Aa%2520test%2520loss%2520of%25200.8523%252C%2520significantly%2520outperforming%2520traditional%2520methods%2520like%250AARIMA%2520%2528RMSE%25201.342%2529.%2520Additionally%252C%2520a%2520Gradient%2520Boosting%2520Classifier%2520%2528GBC%2529%2520is%250Aapplied%2520for%2520directional%2520prediction%252C%2520with%2520backtesting%2520on%2520a%2520%252410%252C000%2520initial%250Acapital%2520revealing%2520a%252040.82%2525%2520profitable%2520trade%2520rate%252C%2520though%2520resulting%2520in%2520a%2520net%250Aloss%2520of%2520%252420%252C653.25%2520over%252049%2520trades.%2520The%2520study%2520analyzes%2520historical%2520trends%252C%250Ashowing%2520a%2520decline%2520in%2520BDT/USD%2520rates%2520from%25200.012%2520to%25200.009%252C%2520and%2520incorporates%250Anormalized%2520daily%2520returns%2520to%2520capture%2520volatility.%2520These%2520findings%2520highlight%2520the%250Apotential%2520of%2520deep%2520learning%2520in%2520forex%2520forecasting%252C%2520offering%2520traders%2520and%250Apolicymakers%2520robust%2520tools%2520to%2520mitigate%2520risks.%2520Future%2520work%2520could%2520integrate%250Asentiment%2520analysis%2520and%2520real-time%2520economic%2520indicators%2520to%2520further%2520enhance%2520model%250Aadaptability%2520in%2520volatile%2520markets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09851v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Exchange%20Rate%20Forecasting%3A%20Leveraging%20Machine%20Learning%20and%20AI%0A%20%20for%20Enhanced%20Accuracy%20in%20Global%20Financial%20Markets&entry.906535625=Md.%20Yeasin%20Rahat%20and%20Rajan%20Das%20Gupta%20and%20Nur%20Raisa%20Rahman%20and%20Sudipto%20Roy%20Pritom%20and%20Samiur%20Rahman%20Shakir%20and%20Md%20Imrul%20Hasan%20Showmick%20and%20Md.%20Jakir%20Hossen&entry.1292438233=%20%20The%20prediction%20of%20foreign%20exchange%20rates%2C%20such%20as%20the%20US%20Dollar%20%28USD%29%20to%0ABangladeshi%20Taka%20%28BDT%29%2C%20plays%20a%20pivotal%20role%20in%20global%20financial%20markets%2C%0Ainfluencing%20trade%2C%20investments%2C%20and%20economic%20stability.%20This%20study%20leverages%0Ahistorical%20USD/BDT%20exchange%20rate%20data%20from%202018%20to%202023%2C%20sourced%20from%20Yahoo%0AFinance%2C%20to%20develop%20advanced%20machine%20learning%20models%20for%20accurate%20forecasting.%0AA%20Long%20Short-Term%20Memory%20%28LSTM%29%20neural%20network%20is%20employed%2C%20achieving%20an%0Aexceptional%20accuracy%20of%2099.449%25%2C%20a%20Root%20Mean%20Square%20Error%20%28RMSE%29%20of%200.9858%2C%20and%0Aa%20test%20loss%20of%200.8523%2C%20significantly%20outperforming%20traditional%20methods%20like%0AARIMA%20%28RMSE%201.342%29.%20Additionally%2C%20a%20Gradient%20Boosting%20Classifier%20%28GBC%29%20is%0Aapplied%20for%20directional%20prediction%2C%20with%20backtesting%20on%20a%20%2410%2C000%20initial%0Acapital%20revealing%20a%2040.82%25%20profitable%20trade%20rate%2C%20though%20resulting%20in%20a%20net%0Aloss%20of%20%2420%2C653.25%20over%2049%20trades.%20The%20study%20analyzes%20historical%20trends%2C%0Ashowing%20a%20decline%20in%20BDT/USD%20rates%20from%200.012%20to%200.009%2C%20and%20incorporates%0Anormalized%20daily%20returns%20to%20capture%20volatility.%20These%20findings%20highlight%20the%0Apotential%20of%20deep%20learning%20in%20forex%20forecasting%2C%20offering%20traders%20and%0Apolicymakers%20robust%20tools%20to%20mitigate%20risks.%20Future%20work%20could%20integrate%0Asentiment%20analysis%20and%20real-time%20economic%20indicators%20to%20further%20enhance%20model%0Aadaptability%20in%20volatile%20markets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09851v1&entry.124074799=Read"},
{"title": "Curvature Tuning: Provable Training-free Model Steering From a Single\n  Parameter", "author": "Leyang Hu and Matteo Gamba and Randall Balestriero", "abstract": "  The scaling of model and data sizes has reshaped the AI landscape,\nestablishing finetuning pretrained models as the standard paradigm for solving\ndownstream tasks. However, dominant finetuning methods typically rely on weight\nadaptation, often lack interpretability, and depend on heuristically chosen\nhyperparameters. In this paper, we take a different perspective and shift the\nfocus from weights to activation functions, viewing them through the lens of\nspline operators. We propose Curvature Tuning (CT), an interpretable and\nprincipled steering method that modulates a model's decision boundary by\ninjecting a single hyperparameter into its activation functions. We show that\nCT provably adjusts model decision boundary curvature and, more fundamentally,\nprojects a model onto a space of smooth functions-thereby complementing current\nfinetuning methods, whose effect lies primarily in feature adaptation. Making\nthis hyperparameter trainable gives rise to a novel and highly\nparameter-efficient finetuning method. Empirically, CT improves both\ngeneralization and robustness. For example, it boosts downstream accuracy of\nResNet-50/152 by 7.14%/8.46% over linear probing and 4.64%/1.70% over LoRA\nacross 12 datasets, and improves robust accuracy on the $\\ell_\\infty$ benchmark\nfrom RobustBench by 1032.64%/1494.46%. Our code is available at\nhttps://github.com/Leon-Leyang/curvature-tuning.\n", "link": "http://arxiv.org/abs/2502.07783v4", "date": "2025-06-11", "relevancy": 1.5431, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5283}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5128}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Curvature%20Tuning%3A%20Provable%20Training-free%20Model%20Steering%20From%20a%20Single%0A%20%20Parameter&body=Title%3A%20Curvature%20Tuning%3A%20Provable%20Training-free%20Model%20Steering%20From%20a%20Single%0A%20%20Parameter%0AAuthor%3A%20Leyang%20Hu%20and%20Matteo%20Gamba%20and%20Randall%20Balestriero%0AAbstract%3A%20%20%20The%20scaling%20of%20model%20and%20data%20sizes%20has%20reshaped%20the%20AI%20landscape%2C%0Aestablishing%20finetuning%20pretrained%20models%20as%20the%20standard%20paradigm%20for%20solving%0Adownstream%20tasks.%20However%2C%20dominant%20finetuning%20methods%20typically%20rely%20on%20weight%0Aadaptation%2C%20often%20lack%20interpretability%2C%20and%20depend%20on%20heuristically%20chosen%0Ahyperparameters.%20In%20this%20paper%2C%20we%20take%20a%20different%20perspective%20and%20shift%20the%0Afocus%20from%20weights%20to%20activation%20functions%2C%20viewing%20them%20through%20the%20lens%20of%0Aspline%20operators.%20We%20propose%20Curvature%20Tuning%20%28CT%29%2C%20an%20interpretable%20and%0Aprincipled%20steering%20method%20that%20modulates%20a%20model%27s%20decision%20boundary%20by%0Ainjecting%20a%20single%20hyperparameter%20into%20its%20activation%20functions.%20We%20show%20that%0ACT%20provably%20adjusts%20model%20decision%20boundary%20curvature%20and%2C%20more%20fundamentally%2C%0Aprojects%20a%20model%20onto%20a%20space%20of%20smooth%20functions-thereby%20complementing%20current%0Afinetuning%20methods%2C%20whose%20effect%20lies%20primarily%20in%20feature%20adaptation.%20Making%0Athis%20hyperparameter%20trainable%20gives%20rise%20to%20a%20novel%20and%20highly%0Aparameter-efficient%20finetuning%20method.%20Empirically%2C%20CT%20improves%20both%0Ageneralization%20and%20robustness.%20For%20example%2C%20it%20boosts%20downstream%20accuracy%20of%0AResNet-50/152%20by%207.14%25/8.46%25%20over%20linear%20probing%20and%204.64%25/1.70%25%20over%20LoRA%0Aacross%2012%20datasets%2C%20and%20improves%20robust%20accuracy%20on%20the%20%24%5Cell_%5Cinfty%24%20benchmark%0Afrom%20RobustBench%20by%201032.64%25/1494.46%25.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Leon-Leyang/curvature-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07783v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurvature%2520Tuning%253A%2520Provable%2520Training-free%2520Model%2520Steering%2520From%2520a%2520Single%250A%2520%2520Parameter%26entry.906535625%3DLeyang%2520Hu%2520and%2520Matteo%2520Gamba%2520and%2520Randall%2520Balestriero%26entry.1292438233%3D%2520%2520The%2520scaling%2520of%2520model%2520and%2520data%2520sizes%2520has%2520reshaped%2520the%2520AI%2520landscape%252C%250Aestablishing%2520finetuning%2520pretrained%2520models%2520as%2520the%2520standard%2520paradigm%2520for%2520solving%250Adownstream%2520tasks.%2520However%252C%2520dominant%2520finetuning%2520methods%2520typically%2520rely%2520on%2520weight%250Aadaptation%252C%2520often%2520lack%2520interpretability%252C%2520and%2520depend%2520on%2520heuristically%2520chosen%250Ahyperparameters.%2520In%2520this%2520paper%252C%2520we%2520take%2520a%2520different%2520perspective%2520and%2520shift%2520the%250Afocus%2520from%2520weights%2520to%2520activation%2520functions%252C%2520viewing%2520them%2520through%2520the%2520lens%2520of%250Aspline%2520operators.%2520We%2520propose%2520Curvature%2520Tuning%2520%2528CT%2529%252C%2520an%2520interpretable%2520and%250Aprincipled%2520steering%2520method%2520that%2520modulates%2520a%2520model%2527s%2520decision%2520boundary%2520by%250Ainjecting%2520a%2520single%2520hyperparameter%2520into%2520its%2520activation%2520functions.%2520We%2520show%2520that%250ACT%2520provably%2520adjusts%2520model%2520decision%2520boundary%2520curvature%2520and%252C%2520more%2520fundamentally%252C%250Aprojects%2520a%2520model%2520onto%2520a%2520space%2520of%2520smooth%2520functions-thereby%2520complementing%2520current%250Afinetuning%2520methods%252C%2520whose%2520effect%2520lies%2520primarily%2520in%2520feature%2520adaptation.%2520Making%250Athis%2520hyperparameter%2520trainable%2520gives%2520rise%2520to%2520a%2520novel%2520and%2520highly%250Aparameter-efficient%2520finetuning%2520method.%2520Empirically%252C%2520CT%2520improves%2520both%250Ageneralization%2520and%2520robustness.%2520For%2520example%252C%2520it%2520boosts%2520downstream%2520accuracy%2520of%250AResNet-50/152%2520by%25207.14%2525/8.46%2525%2520over%2520linear%2520probing%2520and%25204.64%2525/1.70%2525%2520over%2520LoRA%250Aacross%252012%2520datasets%252C%2520and%2520improves%2520robust%2520accuracy%2520on%2520the%2520%2524%255Cell_%255Cinfty%2524%2520benchmark%250Afrom%2520RobustBench%2520by%25201032.64%2525/1494.46%2525.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Leon-Leyang/curvature-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07783v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Curvature%20Tuning%3A%20Provable%20Training-free%20Model%20Steering%20From%20a%20Single%0A%20%20Parameter&entry.906535625=Leyang%20Hu%20and%20Matteo%20Gamba%20and%20Randall%20Balestriero&entry.1292438233=%20%20The%20scaling%20of%20model%20and%20data%20sizes%20has%20reshaped%20the%20AI%20landscape%2C%0Aestablishing%20finetuning%20pretrained%20models%20as%20the%20standard%20paradigm%20for%20solving%0Adownstream%20tasks.%20However%2C%20dominant%20finetuning%20methods%20typically%20rely%20on%20weight%0Aadaptation%2C%20often%20lack%20interpretability%2C%20and%20depend%20on%20heuristically%20chosen%0Ahyperparameters.%20In%20this%20paper%2C%20we%20take%20a%20different%20perspective%20and%20shift%20the%0Afocus%20from%20weights%20to%20activation%20functions%2C%20viewing%20them%20through%20the%20lens%20of%0Aspline%20operators.%20We%20propose%20Curvature%20Tuning%20%28CT%29%2C%20an%20interpretable%20and%0Aprincipled%20steering%20method%20that%20modulates%20a%20model%27s%20decision%20boundary%20by%0Ainjecting%20a%20single%20hyperparameter%20into%20its%20activation%20functions.%20We%20show%20that%0ACT%20provably%20adjusts%20model%20decision%20boundary%20curvature%20and%2C%20more%20fundamentally%2C%0Aprojects%20a%20model%20onto%20a%20space%20of%20smooth%20functions-thereby%20complementing%20current%0Afinetuning%20methods%2C%20whose%20effect%20lies%20primarily%20in%20feature%20adaptation.%20Making%0Athis%20hyperparameter%20trainable%20gives%20rise%20to%20a%20novel%20and%20highly%0Aparameter-efficient%20finetuning%20method.%20Empirically%2C%20CT%20improves%20both%0Ageneralization%20and%20robustness.%20For%20example%2C%20it%20boosts%20downstream%20accuracy%20of%0AResNet-50/152%20by%207.14%25/8.46%25%20over%20linear%20probing%20and%204.64%25/1.70%25%20over%20LoRA%0Aacross%2012%20datasets%2C%20and%20improves%20robust%20accuracy%20on%20the%20%24%5Cell_%5Cinfty%24%20benchmark%0Afrom%20RobustBench%20by%201032.64%25/1494.46%25.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Leon-Leyang/curvature-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07783v4&entry.124074799=Read"},
{"title": "Apollo: A Posteriori Label-Only Membership Inference Attack Towards\n  Machine Unlearning", "author": "Liou Tang and James Joshi and Ashish Kundu", "abstract": "  Machine Unlearning (MU) aims to update Machine Learning (ML) models following\nrequests to remove training samples and their influences on a trained model\nefficiently without retraining the original ML model from scratch. While MU\nitself has been employed to provide privacy protection and regulatory\ncompliance, it can also increase the attack surface of the model. Existing\nprivacy inference attacks towards MU that aim to infer properties of the\nunlearned set rely on the weaker threat model that assumes the attacker has\naccess to both the unlearned model and the original model, limiting their\nfeasibility toward real-life scenarios. We propose a novel privacy attack, A\nPosteriori Label-Only Membership Inference Attack towards MU, Apollo, that\ninfers whether a data sample has been unlearned, following a strict threat\nmodel where an adversary has access to the label-output of the unlearned model\nonly. We demonstrate that our proposed attack, while requiring less access to\nthe target model compared to previous attacks, can achieve relatively high\nprecision on the membership status of the unlearned samples.\n", "link": "http://arxiv.org/abs/2506.09923v1", "date": "2025-06-11", "relevancy": 1.4169, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5152}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4663}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Apollo%3A%20A%20Posteriori%20Label-Only%20Membership%20Inference%20Attack%20Towards%0A%20%20Machine%20Unlearning&body=Title%3A%20Apollo%3A%20A%20Posteriori%20Label-Only%20Membership%20Inference%20Attack%20Towards%0A%20%20Machine%20Unlearning%0AAuthor%3A%20Liou%20Tang%20and%20James%20Joshi%20and%20Ashish%20Kundu%0AAbstract%3A%20%20%20Machine%20Unlearning%20%28MU%29%20aims%20to%20update%20Machine%20Learning%20%28ML%29%20models%20following%0Arequests%20to%20remove%20training%20samples%20and%20their%20influences%20on%20a%20trained%20model%0Aefficiently%20without%20retraining%20the%20original%20ML%20model%20from%20scratch.%20While%20MU%0Aitself%20has%20been%20employed%20to%20provide%20privacy%20protection%20and%20regulatory%0Acompliance%2C%20it%20can%20also%20increase%20the%20attack%20surface%20of%20the%20model.%20Existing%0Aprivacy%20inference%20attacks%20towards%20MU%20that%20aim%20to%20infer%20properties%20of%20the%0Aunlearned%20set%20rely%20on%20the%20weaker%20threat%20model%20that%20assumes%20the%20attacker%20has%0Aaccess%20to%20both%20the%20unlearned%20model%20and%20the%20original%20model%2C%20limiting%20their%0Afeasibility%20toward%20real-life%20scenarios.%20We%20propose%20a%20novel%20privacy%20attack%2C%20A%0APosteriori%20Label-Only%20Membership%20Inference%20Attack%20towards%20MU%2C%20Apollo%2C%20that%0Ainfers%20whether%20a%20data%20sample%20has%20been%20unlearned%2C%20following%20a%20strict%20threat%0Amodel%20where%20an%20adversary%20has%20access%20to%20the%20label-output%20of%20the%20unlearned%20model%0Aonly.%20We%20demonstrate%20that%20our%20proposed%20attack%2C%20while%20requiring%20less%20access%20to%0Athe%20target%20model%20compared%20to%20previous%20attacks%2C%20can%20achieve%20relatively%20high%0Aprecision%20on%20the%20membership%20status%20of%20the%20unlearned%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApollo%253A%2520A%2520Posteriori%2520Label-Only%2520Membership%2520Inference%2520Attack%2520Towards%250A%2520%2520Machine%2520Unlearning%26entry.906535625%3DLiou%2520Tang%2520and%2520James%2520Joshi%2520and%2520Ashish%2520Kundu%26entry.1292438233%3D%2520%2520Machine%2520Unlearning%2520%2528MU%2529%2520aims%2520to%2520update%2520Machine%2520Learning%2520%2528ML%2529%2520models%2520following%250Arequests%2520to%2520remove%2520training%2520samples%2520and%2520their%2520influences%2520on%2520a%2520trained%2520model%250Aefficiently%2520without%2520retraining%2520the%2520original%2520ML%2520model%2520from%2520scratch.%2520While%2520MU%250Aitself%2520has%2520been%2520employed%2520to%2520provide%2520privacy%2520protection%2520and%2520regulatory%250Acompliance%252C%2520it%2520can%2520also%2520increase%2520the%2520attack%2520surface%2520of%2520the%2520model.%2520Existing%250Aprivacy%2520inference%2520attacks%2520towards%2520MU%2520that%2520aim%2520to%2520infer%2520properties%2520of%2520the%250Aunlearned%2520set%2520rely%2520on%2520the%2520weaker%2520threat%2520model%2520that%2520assumes%2520the%2520attacker%2520has%250Aaccess%2520to%2520both%2520the%2520unlearned%2520model%2520and%2520the%2520original%2520model%252C%2520limiting%2520their%250Afeasibility%2520toward%2520real-life%2520scenarios.%2520We%2520propose%2520a%2520novel%2520privacy%2520attack%252C%2520A%250APosteriori%2520Label-Only%2520Membership%2520Inference%2520Attack%2520towards%2520MU%252C%2520Apollo%252C%2520that%250Ainfers%2520whether%2520a%2520data%2520sample%2520has%2520been%2520unlearned%252C%2520following%2520a%2520strict%2520threat%250Amodel%2520where%2520an%2520adversary%2520has%2520access%2520to%2520the%2520label-output%2520of%2520the%2520unlearned%2520model%250Aonly.%2520We%2520demonstrate%2520that%2520our%2520proposed%2520attack%252C%2520while%2520requiring%2520less%2520access%2520to%250Athe%2520target%2520model%2520compared%2520to%2520previous%2520attacks%252C%2520can%2520achieve%2520relatively%2520high%250Aprecision%2520on%2520the%2520membership%2520status%2520of%2520the%2520unlearned%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Apollo%3A%20A%20Posteriori%20Label-Only%20Membership%20Inference%20Attack%20Towards%0A%20%20Machine%20Unlearning&entry.906535625=Liou%20Tang%20and%20James%20Joshi%20and%20Ashish%20Kundu&entry.1292438233=%20%20Machine%20Unlearning%20%28MU%29%20aims%20to%20update%20Machine%20Learning%20%28ML%29%20models%20following%0Arequests%20to%20remove%20training%20samples%20and%20their%20influences%20on%20a%20trained%20model%0Aefficiently%20without%20retraining%20the%20original%20ML%20model%20from%20scratch.%20While%20MU%0Aitself%20has%20been%20employed%20to%20provide%20privacy%20protection%20and%20regulatory%0Acompliance%2C%20it%20can%20also%20increase%20the%20attack%20surface%20of%20the%20model.%20Existing%0Aprivacy%20inference%20attacks%20towards%20MU%20that%20aim%20to%20infer%20properties%20of%20the%0Aunlearned%20set%20rely%20on%20the%20weaker%20threat%20model%20that%20assumes%20the%20attacker%20has%0Aaccess%20to%20both%20the%20unlearned%20model%20and%20the%20original%20model%2C%20limiting%20their%0Afeasibility%20toward%20real-life%20scenarios.%20We%20propose%20a%20novel%20privacy%20attack%2C%20A%0APosteriori%20Label-Only%20Membership%20Inference%20Attack%20towards%20MU%2C%20Apollo%2C%20that%0Ainfers%20whether%20a%20data%20sample%20has%20been%20unlearned%2C%20following%20a%20strict%20threat%0Amodel%20where%20an%20adversary%20has%20access%20to%20the%20label-output%20of%20the%20unlearned%20model%0Aonly.%20We%20demonstrate%20that%20our%20proposed%20attack%2C%20while%20requiring%20less%20access%20to%0Athe%20target%20model%20compared%20to%20previous%20attacks%2C%20can%20achieve%20relatively%20high%0Aprecision%20on%20the%20membership%20status%20of%20the%20unlearned%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09923v1&entry.124074799=Read"},
{"title": "Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized\n  Zero-Order", "author": "Egor Petrov and Grigoriy Evseev and Aleksey Antonov and Andrey Veprikov and Pavel Plyusnin and Nikolay Bushkov and Stanislav Moiseev and Aleksandr Beznosikov", "abstract": "  Fine-tuning Large Language Models (LLMs) is essential for adapting\npre-trained models to downstream tasks. Yet traditional first-order optimizers\nsuch as Stochastic Gradient Descent (SGD) and Adam incur prohibitive memory and\ncomputational costs that scale poorly with model size. In this paper, we\ninvestigate zero-order (ZO) optimization methods as a memory- and\ncompute-efficient alternative, particularly in the context of\nparameter-efficient fine-tuning techniques like LoRA. We propose\n$\\texttt{JAGUAR SignSGD}$, a ZO momentum-based algorithm that extends ZO\nSignSGD, requiring the same number of parameters as the standard ZO SGD and\nonly $\\mathcal{O}(1)$ function evaluations per iteration. To the best of our\nknowledge, this is the first study to establish rigorous convergence guarantees\nfor SignSGD in the stochastic ZO case. We further propose $\\texttt{JAGUAR\nMuon}$, a novel ZO extension of the Muon optimizer that leverages the matrix\nstructure of model parameters, and we provide its convergence rate under\narbitrary stochastic noise. Through extensive experiments on challenging LLM\nfine-tuning benchmarks, we demonstrate that the proposed algorithms meet or\nexceed the convergence quality of standard first-order methods, achieving\nsignificant memory reduction. Our theoretical and empirical results establish\nnew ZO optimization methods as a practical and theoretically grounded approach\nfor resource-constrained LLM adaptation. Our code is available at\nhttps://github.com/brain-mmo-lab/ZO_LLM\n", "link": "http://arxiv.org/abs/2506.04430v2", "date": "2025-06-11", "relevancy": 1.4514, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4871}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4841}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Coordinate%20Momentum%20in%20SignSGD%20and%20Muon%3A%20Memory-Optimized%0A%20%20Zero-Order&body=Title%3A%20Leveraging%20Coordinate%20Momentum%20in%20SignSGD%20and%20Muon%3A%20Memory-Optimized%0A%20%20Zero-Order%0AAuthor%3A%20Egor%20Petrov%20and%20Grigoriy%20Evseev%20and%20Aleksey%20Antonov%20and%20Andrey%20Veprikov%20and%20Pavel%20Plyusnin%20and%20Nikolay%20Bushkov%20and%20Stanislav%20Moiseev%20and%20Aleksandr%20Beznosikov%0AAbstract%3A%20%20%20Fine-tuning%20Large%20Language%20Models%20%28LLMs%29%20is%20essential%20for%20adapting%0Apre-trained%20models%20to%20downstream%20tasks.%20Yet%20traditional%20first-order%20optimizers%0Asuch%20as%20Stochastic%20Gradient%20Descent%20%28SGD%29%20and%20Adam%20incur%20prohibitive%20memory%20and%0Acomputational%20costs%20that%20scale%20poorly%20with%20model%20size.%20In%20this%20paper%2C%20we%0Ainvestigate%20zero-order%20%28ZO%29%20optimization%20methods%20as%20a%20memory-%20and%0Acompute-efficient%20alternative%2C%20particularly%20in%20the%20context%20of%0Aparameter-efficient%20fine-tuning%20techniques%20like%20LoRA.%20We%20propose%0A%24%5Ctexttt%7BJAGUAR%20SignSGD%7D%24%2C%20a%20ZO%20momentum-based%20algorithm%20that%20extends%20ZO%0ASignSGD%2C%20requiring%20the%20same%20number%20of%20parameters%20as%20the%20standard%20ZO%20SGD%20and%0Aonly%20%24%5Cmathcal%7BO%7D%281%29%24%20function%20evaluations%20per%20iteration.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20study%20to%20establish%20rigorous%20convergence%20guarantees%0Afor%20SignSGD%20in%20the%20stochastic%20ZO%20case.%20We%20further%20propose%20%24%5Ctexttt%7BJAGUAR%0AMuon%7D%24%2C%20a%20novel%20ZO%20extension%20of%20the%20Muon%20optimizer%20that%20leverages%20the%20matrix%0Astructure%20of%20model%20parameters%2C%20and%20we%20provide%20its%20convergence%20rate%20under%0Aarbitrary%20stochastic%20noise.%20Through%20extensive%20experiments%20on%20challenging%20LLM%0Afine-tuning%20benchmarks%2C%20we%20demonstrate%20that%20the%20proposed%20algorithms%20meet%20or%0Aexceed%20the%20convergence%20quality%20of%20standard%20first-order%20methods%2C%20achieving%0Asignificant%20memory%20reduction.%20Our%20theoretical%20and%20empirical%20results%20establish%0Anew%20ZO%20optimization%20methods%20as%20a%20practical%20and%20theoretically%20grounded%20approach%0Afor%20resource-constrained%20LLM%20adaptation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/brain-mmo-lab/ZO_LLM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04430v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Coordinate%2520Momentum%2520in%2520SignSGD%2520and%2520Muon%253A%2520Memory-Optimized%250A%2520%2520Zero-Order%26entry.906535625%3DEgor%2520Petrov%2520and%2520Grigoriy%2520Evseev%2520and%2520Aleksey%2520Antonov%2520and%2520Andrey%2520Veprikov%2520and%2520Pavel%2520Plyusnin%2520and%2520Nikolay%2520Bushkov%2520and%2520Stanislav%2520Moiseev%2520and%2520Aleksandr%2520Beznosikov%26entry.1292438233%3D%2520%2520Fine-tuning%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520essential%2520for%2520adapting%250Apre-trained%2520models%2520to%2520downstream%2520tasks.%2520Yet%2520traditional%2520first-order%2520optimizers%250Asuch%2520as%2520Stochastic%2520Gradient%2520Descent%2520%2528SGD%2529%2520and%2520Adam%2520incur%2520prohibitive%2520memory%2520and%250Acomputational%2520costs%2520that%2520scale%2520poorly%2520with%2520model%2520size.%2520In%2520this%2520paper%252C%2520we%250Ainvestigate%2520zero-order%2520%2528ZO%2529%2520optimization%2520methods%2520as%2520a%2520memory-%2520and%250Acompute-efficient%2520alternative%252C%2520particularly%2520in%2520the%2520context%2520of%250Aparameter-efficient%2520fine-tuning%2520techniques%2520like%2520LoRA.%2520We%2520propose%250A%2524%255Ctexttt%257BJAGUAR%2520SignSGD%257D%2524%252C%2520a%2520ZO%2520momentum-based%2520algorithm%2520that%2520extends%2520ZO%250ASignSGD%252C%2520requiring%2520the%2520same%2520number%2520of%2520parameters%2520as%2520the%2520standard%2520ZO%2520SGD%2520and%250Aonly%2520%2524%255Cmathcal%257BO%257D%25281%2529%2524%2520function%2520evaluations%2520per%2520iteration.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520study%2520to%2520establish%2520rigorous%2520convergence%2520guarantees%250Afor%2520SignSGD%2520in%2520the%2520stochastic%2520ZO%2520case.%2520We%2520further%2520propose%2520%2524%255Ctexttt%257BJAGUAR%250AMuon%257D%2524%252C%2520a%2520novel%2520ZO%2520extension%2520of%2520the%2520Muon%2520optimizer%2520that%2520leverages%2520the%2520matrix%250Astructure%2520of%2520model%2520parameters%252C%2520and%2520we%2520provide%2520its%2520convergence%2520rate%2520under%250Aarbitrary%2520stochastic%2520noise.%2520Through%2520extensive%2520experiments%2520on%2520challenging%2520LLM%250Afine-tuning%2520benchmarks%252C%2520we%2520demonstrate%2520that%2520the%2520proposed%2520algorithms%2520meet%2520or%250Aexceed%2520the%2520convergence%2520quality%2520of%2520standard%2520first-order%2520methods%252C%2520achieving%250Asignificant%2520memory%2520reduction.%2520Our%2520theoretical%2520and%2520empirical%2520results%2520establish%250Anew%2520ZO%2520optimization%2520methods%2520as%2520a%2520practical%2520and%2520theoretically%2520grounded%2520approach%250Afor%2520resource-constrained%2520LLM%2520adaptation.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/brain-mmo-lab/ZO_LLM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04430v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Coordinate%20Momentum%20in%20SignSGD%20and%20Muon%3A%20Memory-Optimized%0A%20%20Zero-Order&entry.906535625=Egor%20Petrov%20and%20Grigoriy%20Evseev%20and%20Aleksey%20Antonov%20and%20Andrey%20Veprikov%20and%20Pavel%20Plyusnin%20and%20Nikolay%20Bushkov%20and%20Stanislav%20Moiseev%20and%20Aleksandr%20Beznosikov&entry.1292438233=%20%20Fine-tuning%20Large%20Language%20Models%20%28LLMs%29%20is%20essential%20for%20adapting%0Apre-trained%20models%20to%20downstream%20tasks.%20Yet%20traditional%20first-order%20optimizers%0Asuch%20as%20Stochastic%20Gradient%20Descent%20%28SGD%29%20and%20Adam%20incur%20prohibitive%20memory%20and%0Acomputational%20costs%20that%20scale%20poorly%20with%20model%20size.%20In%20this%20paper%2C%20we%0Ainvestigate%20zero-order%20%28ZO%29%20optimization%20methods%20as%20a%20memory-%20and%0Acompute-efficient%20alternative%2C%20particularly%20in%20the%20context%20of%0Aparameter-efficient%20fine-tuning%20techniques%20like%20LoRA.%20We%20propose%0A%24%5Ctexttt%7BJAGUAR%20SignSGD%7D%24%2C%20a%20ZO%20momentum-based%20algorithm%20that%20extends%20ZO%0ASignSGD%2C%20requiring%20the%20same%20number%20of%20parameters%20as%20the%20standard%20ZO%20SGD%20and%0Aonly%20%24%5Cmathcal%7BO%7D%281%29%24%20function%20evaluations%20per%20iteration.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20study%20to%20establish%20rigorous%20convergence%20guarantees%0Afor%20SignSGD%20in%20the%20stochastic%20ZO%20case.%20We%20further%20propose%20%24%5Ctexttt%7BJAGUAR%0AMuon%7D%24%2C%20a%20novel%20ZO%20extension%20of%20the%20Muon%20optimizer%20that%20leverages%20the%20matrix%0Astructure%20of%20model%20parameters%2C%20and%20we%20provide%20its%20convergence%20rate%20under%0Aarbitrary%20stochastic%20noise.%20Through%20extensive%20experiments%20on%20challenging%20LLM%0Afine-tuning%20benchmarks%2C%20we%20demonstrate%20that%20the%20proposed%20algorithms%20meet%20or%0Aexceed%20the%20convergence%20quality%20of%20standard%20first-order%20methods%2C%20achieving%0Asignificant%20memory%20reduction.%20Our%20theoretical%20and%20empirical%20results%20establish%0Anew%20ZO%20optimization%20methods%20as%20a%20practical%20and%20theoretically%20grounded%20approach%0Afor%20resource-constrained%20LLM%20adaptation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/brain-mmo-lab/ZO_LLM%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04430v2&entry.124074799=Read"},
{"title": "Canonical Latent Representations in Conditional Diffusion Models", "author": "Yitao Xu and Tong Zhang and Ehsan Pajouheshgar and Sabine S\u00fcsstrunk", "abstract": "  Conditional diffusion models (CDMs) have shown impressive performance across\na range of generative tasks. Their ability to model the full data distribution\nhas opened new avenues for analysis-by-synthesis in downstream discriminative\nlearning. However, this same modeling capacity causes CDMs to entangle the\nclass-defining features with irrelevant context, posing challenges to\nextracting robust and interpretable representations. To this end, we identify\nCanonical LAtent Representations (CLAReps), latent codes whose internal CDM\nfeatures preserve essential categorical information while discarding\nnon-discriminative signals. When decoded, CLAReps produce representative\nsamples for each class, offering an interpretable and compact summary of the\ncore class semantics with minimal irrelevant details. Exploiting CLAReps, we\ndevelop a novel diffusion-based feature-distillation paradigm, CaDistill. While\nthe student has full access to the training set, the CDM as teacher transfers\ncore class knowledge only via CLAReps, which amounts to merely 10 % of the\ntraining data in size. After training, the student achieves strong adversarial\nrobustness and generalization ability, focusing more on the class signals\ninstead of spurious background cues. Our findings suggest that CDMs can serve\nnot just as image generators but also as compact, interpretable teachers that\ncan drive robust representation learning.\n", "link": "http://arxiv.org/abs/2506.09955v1", "date": "2025-06-11", "relevancy": 1.1638, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6337}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5678}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Canonical%20Latent%20Representations%20in%20Conditional%20Diffusion%20Models&body=Title%3A%20Canonical%20Latent%20Representations%20in%20Conditional%20Diffusion%20Models%0AAuthor%3A%20Yitao%20Xu%20and%20Tong%20Zhang%20and%20Ehsan%20Pajouheshgar%20and%20Sabine%20S%C3%BCsstrunk%0AAbstract%3A%20%20%20Conditional%20diffusion%20models%20%28CDMs%29%20have%20shown%20impressive%20performance%20across%0Aa%20range%20of%20generative%20tasks.%20Their%20ability%20to%20model%20the%20full%20data%20distribution%0Ahas%20opened%20new%20avenues%20for%20analysis-by-synthesis%20in%20downstream%20discriminative%0Alearning.%20However%2C%20this%20same%20modeling%20capacity%20causes%20CDMs%20to%20entangle%20the%0Aclass-defining%20features%20with%20irrelevant%20context%2C%20posing%20challenges%20to%0Aextracting%20robust%20and%20interpretable%20representations.%20To%20this%20end%2C%20we%20identify%0ACanonical%20LAtent%20Representations%20%28CLAReps%29%2C%20latent%20codes%20whose%20internal%20CDM%0Afeatures%20preserve%20essential%20categorical%20information%20while%20discarding%0Anon-discriminative%20signals.%20When%20decoded%2C%20CLAReps%20produce%20representative%0Asamples%20for%20each%20class%2C%20offering%20an%20interpretable%20and%20compact%20summary%20of%20the%0Acore%20class%20semantics%20with%20minimal%20irrelevant%20details.%20Exploiting%20CLAReps%2C%20we%0Adevelop%20a%20novel%20diffusion-based%20feature-distillation%20paradigm%2C%20CaDistill.%20While%0Athe%20student%20has%20full%20access%20to%20the%20training%20set%2C%20the%20CDM%20as%20teacher%20transfers%0Acore%20class%20knowledge%20only%20via%20CLAReps%2C%20which%20amounts%20to%20merely%2010%20%25%20of%20the%0Atraining%20data%20in%20size.%20After%20training%2C%20the%20student%20achieves%20strong%20adversarial%0Arobustness%20and%20generalization%20ability%2C%20focusing%20more%20on%20the%20class%20signals%0Ainstead%20of%20spurious%20background%20cues.%20Our%20findings%20suggest%20that%20CDMs%20can%20serve%0Anot%20just%20as%20image%20generators%20but%20also%20as%20compact%2C%20interpretable%20teachers%20that%0Acan%20drive%20robust%20representation%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCanonical%2520Latent%2520Representations%2520in%2520Conditional%2520Diffusion%2520Models%26entry.906535625%3DYitao%2520Xu%2520and%2520Tong%2520Zhang%2520and%2520Ehsan%2520Pajouheshgar%2520and%2520Sabine%2520S%25C3%25BCsstrunk%26entry.1292438233%3D%2520%2520Conditional%2520diffusion%2520models%2520%2528CDMs%2529%2520have%2520shown%2520impressive%2520performance%2520across%250Aa%2520range%2520of%2520generative%2520tasks.%2520Their%2520ability%2520to%2520model%2520the%2520full%2520data%2520distribution%250Ahas%2520opened%2520new%2520avenues%2520for%2520analysis-by-synthesis%2520in%2520downstream%2520discriminative%250Alearning.%2520However%252C%2520this%2520same%2520modeling%2520capacity%2520causes%2520CDMs%2520to%2520entangle%2520the%250Aclass-defining%2520features%2520with%2520irrelevant%2520context%252C%2520posing%2520challenges%2520to%250Aextracting%2520robust%2520and%2520interpretable%2520representations.%2520To%2520this%2520end%252C%2520we%2520identify%250ACanonical%2520LAtent%2520Representations%2520%2528CLAReps%2529%252C%2520latent%2520codes%2520whose%2520internal%2520CDM%250Afeatures%2520preserve%2520essential%2520categorical%2520information%2520while%2520discarding%250Anon-discriminative%2520signals.%2520When%2520decoded%252C%2520CLAReps%2520produce%2520representative%250Asamples%2520for%2520each%2520class%252C%2520offering%2520an%2520interpretable%2520and%2520compact%2520summary%2520of%2520the%250Acore%2520class%2520semantics%2520with%2520minimal%2520irrelevant%2520details.%2520Exploiting%2520CLAReps%252C%2520we%250Adevelop%2520a%2520novel%2520diffusion-based%2520feature-distillation%2520paradigm%252C%2520CaDistill.%2520While%250Athe%2520student%2520has%2520full%2520access%2520to%2520the%2520training%2520set%252C%2520the%2520CDM%2520as%2520teacher%2520transfers%250Acore%2520class%2520knowledge%2520only%2520via%2520CLAReps%252C%2520which%2520amounts%2520to%2520merely%252010%2520%2525%2520of%2520the%250Atraining%2520data%2520in%2520size.%2520After%2520training%252C%2520the%2520student%2520achieves%2520strong%2520adversarial%250Arobustness%2520and%2520generalization%2520ability%252C%2520focusing%2520more%2520on%2520the%2520class%2520signals%250Ainstead%2520of%2520spurious%2520background%2520cues.%2520Our%2520findings%2520suggest%2520that%2520CDMs%2520can%2520serve%250Anot%2520just%2520as%2520image%2520generators%2520but%2520also%2520as%2520compact%252C%2520interpretable%2520teachers%2520that%250Acan%2520drive%2520robust%2520representation%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Canonical%20Latent%20Representations%20in%20Conditional%20Diffusion%20Models&entry.906535625=Yitao%20Xu%20and%20Tong%20Zhang%20and%20Ehsan%20Pajouheshgar%20and%20Sabine%20S%C3%BCsstrunk&entry.1292438233=%20%20Conditional%20diffusion%20models%20%28CDMs%29%20have%20shown%20impressive%20performance%20across%0Aa%20range%20of%20generative%20tasks.%20Their%20ability%20to%20model%20the%20full%20data%20distribution%0Ahas%20opened%20new%20avenues%20for%20analysis-by-synthesis%20in%20downstream%20discriminative%0Alearning.%20However%2C%20this%20same%20modeling%20capacity%20causes%20CDMs%20to%20entangle%20the%0Aclass-defining%20features%20with%20irrelevant%20context%2C%20posing%20challenges%20to%0Aextracting%20robust%20and%20interpretable%20representations.%20To%20this%20end%2C%20we%20identify%0ACanonical%20LAtent%20Representations%20%28CLAReps%29%2C%20latent%20codes%20whose%20internal%20CDM%0Afeatures%20preserve%20essential%20categorical%20information%20while%20discarding%0Anon-discriminative%20signals.%20When%20decoded%2C%20CLAReps%20produce%20representative%0Asamples%20for%20each%20class%2C%20offering%20an%20interpretable%20and%20compact%20summary%20of%20the%0Acore%20class%20semantics%20with%20minimal%20irrelevant%20details.%20Exploiting%20CLAReps%2C%20we%0Adevelop%20a%20novel%20diffusion-based%20feature-distillation%20paradigm%2C%20CaDistill.%20While%0Athe%20student%20has%20full%20access%20to%20the%20training%20set%2C%20the%20CDM%20as%20teacher%20transfers%0Acore%20class%20knowledge%20only%20via%20CLAReps%2C%20which%20amounts%20to%20merely%2010%20%25%20of%20the%0Atraining%20data%20in%20size.%20After%20training%2C%20the%20student%20achieves%20strong%20adversarial%0Arobustness%20and%20generalization%20ability%2C%20focusing%20more%20on%20the%20class%20signals%0Ainstead%20of%20spurious%20background%20cues.%20Our%20findings%20suggest%20that%20CDMs%20can%20serve%0Anot%20just%20as%20image%20generators%20but%20also%20as%20compact%2C%20interpretable%20teachers%20that%0Acan%20drive%20robust%20representation%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09955v1&entry.124074799=Read"},
{"title": "OmniJet-$\u03b1_C$: Learning point cloud calorimeter simulations using\n  generative transformers", "author": "Joschka Birk and Frank Gaede and Anna Hallin and Gregor Kasieczka and Martina Mozzanica and Henning Rose", "abstract": "  We show the first use of generative transformers for generating calorimeter\nshowers as point clouds in a high-granularity calorimeter. Using the tokenizer\nand generative part of the OmniJet-${\\alpha}$ model, we represent the hits in\nthe detector as sequences of integers. This model allows variable-length\nsequences, which means that it supports realistic shower development and does\nnot need to be conditioned on the number of hits. Since the tokenization\nrepresents the showers as point clouds, the model learns the geometry of the\nshowers without being restricted to any particular voxel grid.\n", "link": "http://arxiv.org/abs/2501.05534v2", "date": "2025-06-11", "relevancy": 1.4965, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5055}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4979}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniJet-%24%CE%B1_C%24%3A%20Learning%20point%20cloud%20calorimeter%20simulations%20using%0A%20%20generative%20transformers&body=Title%3A%20OmniJet-%24%CE%B1_C%24%3A%20Learning%20point%20cloud%20calorimeter%20simulations%20using%0A%20%20generative%20transformers%0AAuthor%3A%20Joschka%20Birk%20and%20Frank%20Gaede%20and%20Anna%20Hallin%20and%20Gregor%20Kasieczka%20and%20Martina%20Mozzanica%20and%20Henning%20Rose%0AAbstract%3A%20%20%20We%20show%20the%20first%20use%20of%20generative%20transformers%20for%20generating%20calorimeter%0Ashowers%20as%20point%20clouds%20in%20a%20high-granularity%20calorimeter.%20Using%20the%20tokenizer%0Aand%20generative%20part%20of%20the%20OmniJet-%24%7B%5Calpha%7D%24%20model%2C%20we%20represent%20the%20hits%20in%0Athe%20detector%20as%20sequences%20of%20integers.%20This%20model%20allows%20variable-length%0Asequences%2C%20which%20means%20that%20it%20supports%20realistic%20shower%20development%20and%20does%0Anot%20need%20to%20be%20conditioned%20on%20the%20number%20of%20hits.%20Since%20the%20tokenization%0Arepresents%20the%20showers%20as%20point%20clouds%2C%20the%20model%20learns%20the%20geometry%20of%20the%0Ashowers%20without%20being%20restricted%20to%20any%20particular%20voxel%20grid.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05534v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniJet-%2524%25CE%25B1_C%2524%253A%2520Learning%2520point%2520cloud%2520calorimeter%2520simulations%2520using%250A%2520%2520generative%2520transformers%26entry.906535625%3DJoschka%2520Birk%2520and%2520Frank%2520Gaede%2520and%2520Anna%2520Hallin%2520and%2520Gregor%2520Kasieczka%2520and%2520Martina%2520Mozzanica%2520and%2520Henning%2520Rose%26entry.1292438233%3D%2520%2520We%2520show%2520the%2520first%2520use%2520of%2520generative%2520transformers%2520for%2520generating%2520calorimeter%250Ashowers%2520as%2520point%2520clouds%2520in%2520a%2520high-granularity%2520calorimeter.%2520Using%2520the%2520tokenizer%250Aand%2520generative%2520part%2520of%2520the%2520OmniJet-%2524%257B%255Calpha%257D%2524%2520model%252C%2520we%2520represent%2520the%2520hits%2520in%250Athe%2520detector%2520as%2520sequences%2520of%2520integers.%2520This%2520model%2520allows%2520variable-length%250Asequences%252C%2520which%2520means%2520that%2520it%2520supports%2520realistic%2520shower%2520development%2520and%2520does%250Anot%2520need%2520to%2520be%2520conditioned%2520on%2520the%2520number%2520of%2520hits.%2520Since%2520the%2520tokenization%250Arepresents%2520the%2520showers%2520as%2520point%2520clouds%252C%2520the%2520model%2520learns%2520the%2520geometry%2520of%2520the%250Ashowers%2520without%2520being%2520restricted%2520to%2520any%2520particular%2520voxel%2520grid.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05534v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniJet-%24%CE%B1_C%24%3A%20Learning%20point%20cloud%20calorimeter%20simulations%20using%0A%20%20generative%20transformers&entry.906535625=Joschka%20Birk%20and%20Frank%20Gaede%20and%20Anna%20Hallin%20and%20Gregor%20Kasieczka%20and%20Martina%20Mozzanica%20and%20Henning%20Rose&entry.1292438233=%20%20We%20show%20the%20first%20use%20of%20generative%20transformers%20for%20generating%20calorimeter%0Ashowers%20as%20point%20clouds%20in%20a%20high-granularity%20calorimeter.%20Using%20the%20tokenizer%0Aand%20generative%20part%20of%20the%20OmniJet-%24%7B%5Calpha%7D%24%20model%2C%20we%20represent%20the%20hits%20in%0Athe%20detector%20as%20sequences%20of%20integers.%20This%20model%20allows%20variable-length%0Asequences%2C%20which%20means%20that%20it%20supports%20realistic%20shower%20development%20and%20does%0Anot%20need%20to%20be%20conditioned%20on%20the%20number%20of%20hits.%20Since%20the%20tokenization%0Arepresents%20the%20showers%20as%20point%20clouds%2C%20the%20model%20learns%20the%20geometry%20of%20the%0Ashowers%20without%20being%20restricted%20to%20any%20particular%20voxel%20grid.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05534v2&entry.124074799=Read"},
{"title": "A Shortcut-aware Video-QA Benchmark for Physical Understanding via\n  Minimal Video Pairs", "author": "Benno Krojer and Mojtaba Komeili and Candace Ross and Quentin Garrido and Koustuv Sinha and Nicolas Ballas and Mahmoud Assran", "abstract": "  Existing benchmarks for assessing the spatio-temporal understanding and\nreasoning abilities of video language models are susceptible to score inflation\ndue to the presence of shortcut solutions based on superficial visual or\ntextual cues. This paper mitigates the challenges in accurately assessing model\nperformance by introducing the Minimal Video Pairs (MVP) benchmark, a simple\nshortcut-aware video QA benchmark for assessing the physical understanding of\nvideo language models. The benchmark is comprised of 55K high-quality\nmultiple-choice video QA examples focusing on physical world understanding.\nExamples are curated from nine video data sources, spanning first-person\negocentric and exocentric videos, robotic interaction data, and cognitive\nscience intuitive physics benchmarks. To mitigate shortcut solutions that rely\non superficial visual or textual cues and biases, each sample in MVP has a\nminimal-change pair -- a visually similar video accompanied by an identical\nquestion but an opposing answer. To answer a question correctly, a model must\nprovide correct answers for both examples in the minimal-change pair; as such,\nmodels that solely rely on visual or textual biases would achieve below random\nperformance. Human performance on MVP is 92.9\\%, while the best open-source\nstate-of-the-art video-language model achieves 40.2\\% compared to random\nperformance at 25\\%.\n", "link": "http://arxiv.org/abs/2506.09987v1", "date": "2025-06-11", "relevancy": 1.6159, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5492}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5478}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Shortcut-aware%20Video-QA%20Benchmark%20for%20Physical%20Understanding%20via%0A%20%20Minimal%20Video%20Pairs&body=Title%3A%20A%20Shortcut-aware%20Video-QA%20Benchmark%20for%20Physical%20Understanding%20via%0A%20%20Minimal%20Video%20Pairs%0AAuthor%3A%20Benno%20Krojer%20and%20Mojtaba%20Komeili%20and%20Candace%20Ross%20and%20Quentin%20Garrido%20and%20Koustuv%20Sinha%20and%20Nicolas%20Ballas%20and%20Mahmoud%20Assran%0AAbstract%3A%20%20%20Existing%20benchmarks%20for%20assessing%20the%20spatio-temporal%20understanding%20and%0Areasoning%20abilities%20of%20video%20language%20models%20are%20susceptible%20to%20score%20inflation%0Adue%20to%20the%20presence%20of%20shortcut%20solutions%20based%20on%20superficial%20visual%20or%0Atextual%20cues.%20This%20paper%20mitigates%20the%20challenges%20in%20accurately%20assessing%20model%0Aperformance%20by%20introducing%20the%20Minimal%20Video%20Pairs%20%28MVP%29%20benchmark%2C%20a%20simple%0Ashortcut-aware%20video%20QA%20benchmark%20for%20assessing%20the%20physical%20understanding%20of%0Avideo%20language%20models.%20The%20benchmark%20is%20comprised%20of%2055K%20high-quality%0Amultiple-choice%20video%20QA%20examples%20focusing%20on%20physical%20world%20understanding.%0AExamples%20are%20curated%20from%20nine%20video%20data%20sources%2C%20spanning%20first-person%0Aegocentric%20and%20exocentric%20videos%2C%20robotic%20interaction%20data%2C%20and%20cognitive%0Ascience%20intuitive%20physics%20benchmarks.%20To%20mitigate%20shortcut%20solutions%20that%20rely%0Aon%20superficial%20visual%20or%20textual%20cues%20and%20biases%2C%20each%20sample%20in%20MVP%20has%20a%0Aminimal-change%20pair%20--%20a%20visually%20similar%20video%20accompanied%20by%20an%20identical%0Aquestion%20but%20an%20opposing%20answer.%20To%20answer%20a%20question%20correctly%2C%20a%20model%20must%0Aprovide%20correct%20answers%20for%20both%20examples%20in%20the%20minimal-change%20pair%3B%20as%20such%2C%0Amodels%20that%20solely%20rely%20on%20visual%20or%20textual%20biases%20would%20achieve%20below%20random%0Aperformance.%20Human%20performance%20on%20MVP%20is%2092.9%5C%25%2C%20while%20the%20best%20open-source%0Astate-of-the-art%20video-language%20model%20achieves%2040.2%5C%25%20compared%20to%20random%0Aperformance%20at%2025%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09987v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Shortcut-aware%2520Video-QA%2520Benchmark%2520for%2520Physical%2520Understanding%2520via%250A%2520%2520Minimal%2520Video%2520Pairs%26entry.906535625%3DBenno%2520Krojer%2520and%2520Mojtaba%2520Komeili%2520and%2520Candace%2520Ross%2520and%2520Quentin%2520Garrido%2520and%2520Koustuv%2520Sinha%2520and%2520Nicolas%2520Ballas%2520and%2520Mahmoud%2520Assran%26entry.1292438233%3D%2520%2520Existing%2520benchmarks%2520for%2520assessing%2520the%2520spatio-temporal%2520understanding%2520and%250Areasoning%2520abilities%2520of%2520video%2520language%2520models%2520are%2520susceptible%2520to%2520score%2520inflation%250Adue%2520to%2520the%2520presence%2520of%2520shortcut%2520solutions%2520based%2520on%2520superficial%2520visual%2520or%250Atextual%2520cues.%2520This%2520paper%2520mitigates%2520the%2520challenges%2520in%2520accurately%2520assessing%2520model%250Aperformance%2520by%2520introducing%2520the%2520Minimal%2520Video%2520Pairs%2520%2528MVP%2529%2520benchmark%252C%2520a%2520simple%250Ashortcut-aware%2520video%2520QA%2520benchmark%2520for%2520assessing%2520the%2520physical%2520understanding%2520of%250Avideo%2520language%2520models.%2520The%2520benchmark%2520is%2520comprised%2520of%252055K%2520high-quality%250Amultiple-choice%2520video%2520QA%2520examples%2520focusing%2520on%2520physical%2520world%2520understanding.%250AExamples%2520are%2520curated%2520from%2520nine%2520video%2520data%2520sources%252C%2520spanning%2520first-person%250Aegocentric%2520and%2520exocentric%2520videos%252C%2520robotic%2520interaction%2520data%252C%2520and%2520cognitive%250Ascience%2520intuitive%2520physics%2520benchmarks.%2520To%2520mitigate%2520shortcut%2520solutions%2520that%2520rely%250Aon%2520superficial%2520visual%2520or%2520textual%2520cues%2520and%2520biases%252C%2520each%2520sample%2520in%2520MVP%2520has%2520a%250Aminimal-change%2520pair%2520--%2520a%2520visually%2520similar%2520video%2520accompanied%2520by%2520an%2520identical%250Aquestion%2520but%2520an%2520opposing%2520answer.%2520To%2520answer%2520a%2520question%2520correctly%252C%2520a%2520model%2520must%250Aprovide%2520correct%2520answers%2520for%2520both%2520examples%2520in%2520the%2520minimal-change%2520pair%253B%2520as%2520such%252C%250Amodels%2520that%2520solely%2520rely%2520on%2520visual%2520or%2520textual%2520biases%2520would%2520achieve%2520below%2520random%250Aperformance.%2520Human%2520performance%2520on%2520MVP%2520is%252092.9%255C%2525%252C%2520while%2520the%2520best%2520open-source%250Astate-of-the-art%2520video-language%2520model%2520achieves%252040.2%255C%2525%2520compared%2520to%2520random%250Aperformance%2520at%252025%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09987v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Shortcut-aware%20Video-QA%20Benchmark%20for%20Physical%20Understanding%20via%0A%20%20Minimal%20Video%20Pairs&entry.906535625=Benno%20Krojer%20and%20Mojtaba%20Komeili%20and%20Candace%20Ross%20and%20Quentin%20Garrido%20and%20Koustuv%20Sinha%20and%20Nicolas%20Ballas%20and%20Mahmoud%20Assran&entry.1292438233=%20%20Existing%20benchmarks%20for%20assessing%20the%20spatio-temporal%20understanding%20and%0Areasoning%20abilities%20of%20video%20language%20models%20are%20susceptible%20to%20score%20inflation%0Adue%20to%20the%20presence%20of%20shortcut%20solutions%20based%20on%20superficial%20visual%20or%0Atextual%20cues.%20This%20paper%20mitigates%20the%20challenges%20in%20accurately%20assessing%20model%0Aperformance%20by%20introducing%20the%20Minimal%20Video%20Pairs%20%28MVP%29%20benchmark%2C%20a%20simple%0Ashortcut-aware%20video%20QA%20benchmark%20for%20assessing%20the%20physical%20understanding%20of%0Avideo%20language%20models.%20The%20benchmark%20is%20comprised%20of%2055K%20high-quality%0Amultiple-choice%20video%20QA%20examples%20focusing%20on%20physical%20world%20understanding.%0AExamples%20are%20curated%20from%20nine%20video%20data%20sources%2C%20spanning%20first-person%0Aegocentric%20and%20exocentric%20videos%2C%20robotic%20interaction%20data%2C%20and%20cognitive%0Ascience%20intuitive%20physics%20benchmarks.%20To%20mitigate%20shortcut%20solutions%20that%20rely%0Aon%20superficial%20visual%20or%20textual%20cues%20and%20biases%2C%20each%20sample%20in%20MVP%20has%20a%0Aminimal-change%20pair%20--%20a%20visually%20similar%20video%20accompanied%20by%20an%20identical%0Aquestion%20but%20an%20opposing%20answer.%20To%20answer%20a%20question%20correctly%2C%20a%20model%20must%0Aprovide%20correct%20answers%20for%20both%20examples%20in%20the%20minimal-change%20pair%3B%20as%20such%2C%0Amodels%20that%20solely%20rely%20on%20visual%20or%20textual%20biases%20would%20achieve%20below%20random%0Aperformance.%20Human%20performance%20on%20MVP%20is%2092.9%5C%25%2C%20while%20the%20best%20open-source%0Astate-of-the-art%20video-language%20model%20achieves%2040.2%5C%25%20compared%20to%20random%0Aperformance%20at%2025%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09987v1&entry.124074799=Read"},
{"title": "\"What are my options?\": Explaining RL Agents with Diverse Near-Optimal\n  Alternatives (Extended)", "author": "Noel Brindise and Vijeth Hebbar and Riya Shah and Cedric Langbort", "abstract": "  In this work, we provide an extended discussion of a new approach to\nexplainable Reinforcement Learning called Diverse Near-Optimal Alternatives\n(DNA), first proposed at L4DC 2025. DNA seeks a set of reasonable \"options\" for\ntrajectory-planning agents, optimizing policies to produce qualitatively\ndiverse trajectories in Euclidean space. In the spirit of explainability, these\ndistinct policies are used to \"explain\" an agent's options in terms of\navailable trajectory shapes from which a human user may choose. In particular,\nDNA applies to value function-based policies on Markov decision processes where\nagents are limited to continuous trajectories. Here, we describe DNA, which\nuses reward shaping in local, modified Q-learning problems to solve for\ndistinct policies with guaranteed epsilon-optimality. We show that it\nsuccessfully returns qualitatively different policies that constitute\nmeaningfully different \"options\" in simulation, including a brief comparison to\nrelated approaches in the stochastic optimization field of Quality Diversity.\nBeyond the explanatory motivation, this work opens new possibilities for\nexploration and adaptive planning in RL.\n", "link": "http://arxiv.org/abs/2506.09901v1", "date": "2025-06-11", "relevancy": 1.4706, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5098}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4975}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%22What%20are%20my%20options%3F%22%3A%20Explaining%20RL%20Agents%20with%20Diverse%20Near-Optimal%0A%20%20Alternatives%20%28Extended%29&body=Title%3A%20%22What%20are%20my%20options%3F%22%3A%20Explaining%20RL%20Agents%20with%20Diverse%20Near-Optimal%0A%20%20Alternatives%20%28Extended%29%0AAuthor%3A%20Noel%20Brindise%20and%20Vijeth%20Hebbar%20and%20Riya%20Shah%20and%20Cedric%20Langbort%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20provide%20an%20extended%20discussion%20of%20a%20new%20approach%20to%0Aexplainable%20Reinforcement%20Learning%20called%20Diverse%20Near-Optimal%20Alternatives%0A%28DNA%29%2C%20first%20proposed%20at%20L4DC%202025.%20DNA%20seeks%20a%20set%20of%20reasonable%20%22options%22%20for%0Atrajectory-planning%20agents%2C%20optimizing%20policies%20to%20produce%20qualitatively%0Adiverse%20trajectories%20in%20Euclidean%20space.%20In%20the%20spirit%20of%20explainability%2C%20these%0Adistinct%20policies%20are%20used%20to%20%22explain%22%20an%20agent%27s%20options%20in%20terms%20of%0Aavailable%20trajectory%20shapes%20from%20which%20a%20human%20user%20may%20choose.%20In%20particular%2C%0ADNA%20applies%20to%20value%20function-based%20policies%20on%20Markov%20decision%20processes%20where%0Aagents%20are%20limited%20to%20continuous%20trajectories.%20Here%2C%20we%20describe%20DNA%2C%20which%0Auses%20reward%20shaping%20in%20local%2C%20modified%20Q-learning%20problems%20to%20solve%20for%0Adistinct%20policies%20with%20guaranteed%20epsilon-optimality.%20We%20show%20that%20it%0Asuccessfully%20returns%20qualitatively%20different%20policies%20that%20constitute%0Ameaningfully%20different%20%22options%22%20in%20simulation%2C%20including%20a%20brief%20comparison%20to%0Arelated%20approaches%20in%20the%20stochastic%20optimization%20field%20of%20Quality%20Diversity.%0ABeyond%20the%20explanatory%20motivation%2C%20this%20work%20opens%20new%20possibilities%20for%0Aexploration%20and%20adaptive%20planning%20in%20RL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2522What%2520are%2520my%2520options%253F%2522%253A%2520Explaining%2520RL%2520Agents%2520with%2520Diverse%2520Near-Optimal%250A%2520%2520Alternatives%2520%2528Extended%2529%26entry.906535625%3DNoel%2520Brindise%2520and%2520Vijeth%2520Hebbar%2520and%2520Riya%2520Shah%2520and%2520Cedric%2520Langbort%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520provide%2520an%2520extended%2520discussion%2520of%2520a%2520new%2520approach%2520to%250Aexplainable%2520Reinforcement%2520Learning%2520called%2520Diverse%2520Near-Optimal%2520Alternatives%250A%2528DNA%2529%252C%2520first%2520proposed%2520at%2520L4DC%25202025.%2520DNA%2520seeks%2520a%2520set%2520of%2520reasonable%2520%2522options%2522%2520for%250Atrajectory-planning%2520agents%252C%2520optimizing%2520policies%2520to%2520produce%2520qualitatively%250Adiverse%2520trajectories%2520in%2520Euclidean%2520space.%2520In%2520the%2520spirit%2520of%2520explainability%252C%2520these%250Adistinct%2520policies%2520are%2520used%2520to%2520%2522explain%2522%2520an%2520agent%2527s%2520options%2520in%2520terms%2520of%250Aavailable%2520trajectory%2520shapes%2520from%2520which%2520a%2520human%2520user%2520may%2520choose.%2520In%2520particular%252C%250ADNA%2520applies%2520to%2520value%2520function-based%2520policies%2520on%2520Markov%2520decision%2520processes%2520where%250Aagents%2520are%2520limited%2520to%2520continuous%2520trajectories.%2520Here%252C%2520we%2520describe%2520DNA%252C%2520which%250Auses%2520reward%2520shaping%2520in%2520local%252C%2520modified%2520Q-learning%2520problems%2520to%2520solve%2520for%250Adistinct%2520policies%2520with%2520guaranteed%2520epsilon-optimality.%2520We%2520show%2520that%2520it%250Asuccessfully%2520returns%2520qualitatively%2520different%2520policies%2520that%2520constitute%250Ameaningfully%2520different%2520%2522options%2522%2520in%2520simulation%252C%2520including%2520a%2520brief%2520comparison%2520to%250Arelated%2520approaches%2520in%2520the%2520stochastic%2520optimization%2520field%2520of%2520Quality%2520Diversity.%250ABeyond%2520the%2520explanatory%2520motivation%252C%2520this%2520work%2520opens%2520new%2520possibilities%2520for%250Aexploration%2520and%2520adaptive%2520planning%2520in%2520RL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22What%20are%20my%20options%3F%22%3A%20Explaining%20RL%20Agents%20with%20Diverse%20Near-Optimal%0A%20%20Alternatives%20%28Extended%29&entry.906535625=Noel%20Brindise%20and%20Vijeth%20Hebbar%20and%20Riya%20Shah%20and%20Cedric%20Langbort&entry.1292438233=%20%20In%20this%20work%2C%20we%20provide%20an%20extended%20discussion%20of%20a%20new%20approach%20to%0Aexplainable%20Reinforcement%20Learning%20called%20Diverse%20Near-Optimal%20Alternatives%0A%28DNA%29%2C%20first%20proposed%20at%20L4DC%202025.%20DNA%20seeks%20a%20set%20of%20reasonable%20%22options%22%20for%0Atrajectory-planning%20agents%2C%20optimizing%20policies%20to%20produce%20qualitatively%0Adiverse%20trajectories%20in%20Euclidean%20space.%20In%20the%20spirit%20of%20explainability%2C%20these%0Adistinct%20policies%20are%20used%20to%20%22explain%22%20an%20agent%27s%20options%20in%20terms%20of%0Aavailable%20trajectory%20shapes%20from%20which%20a%20human%20user%20may%20choose.%20In%20particular%2C%0ADNA%20applies%20to%20value%20function-based%20policies%20on%20Markov%20decision%20processes%20where%0Aagents%20are%20limited%20to%20continuous%20trajectories.%20Here%2C%20we%20describe%20DNA%2C%20which%0Auses%20reward%20shaping%20in%20local%2C%20modified%20Q-learning%20problems%20to%20solve%20for%0Adistinct%20policies%20with%20guaranteed%20epsilon-optimality.%20We%20show%20that%20it%0Asuccessfully%20returns%20qualitatively%20different%20policies%20that%20constitute%0Ameaningfully%20different%20%22options%22%20in%20simulation%2C%20including%20a%20brief%20comparison%20to%0Arelated%20approaches%20in%20the%20stochastic%20optimization%20field%20of%20Quality%20Diversity.%0ABeyond%20the%20explanatory%20motivation%2C%20this%20work%20opens%20new%20possibilities%20for%0Aexploration%20and%20adaptive%20planning%20in%20RL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09901v1&entry.124074799=Read"},
{"title": "Dynamic Diffusion Schr\u00f6dinger Bridge in Astrophysical Observational\n  Inversions", "author": "Ye Zhu and Duo Xu and Zhiwei Deng and Jonathan C. Tan and Olga Russakovsky", "abstract": "  We study Diffusion Schr\\\"odinger Bridge (DSB) models in the context of\ndynamical astrophysical systems, specifically tackling observational inverse\nprediction tasks within Giant Molecular Clouds (GMCs) for star formation. We\nintroduce the Astro-DSB model, a variant of DSB with the pairwise domain\nassumption tailored for astrophysical dynamics. By investigating its learning\nprocess and prediction performance in both physically simulated data and in\nreal observations (the Taurus B213 data), we present two main takeaways. First,\nfrom the astrophysical perspective, our proposed paired DSB method improves\ninterpretability, learning efficiency, and prediction performance over\nconventional astrostatistical and other machine learning methods. Second, from\nthe generative modeling perspective, probabilistic generative modeling reveals\nimprovements over discriminative pixel-to-pixel modeling in Out-Of-Distribution\n(OOD) testing cases of physical simulations with unseen initial conditions and\ndifferent dominant physical processes. Our study expands research into\ndiffusion models beyond the traditional visual synthesis application and\nprovides evidence of the models' learning abilities beyond pure data\nstatistics, paving a path for future physics-aware generative models which can\nalign dynamics between machine learning and real (astro)physical systems.\n", "link": "http://arxiv.org/abs/2506.08065v2", "date": "2025-06-11", "relevancy": 1.6557, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5887}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5548}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Diffusion%20Schr%C3%B6dinger%20Bridge%20in%20Astrophysical%20Observational%0A%20%20Inversions&body=Title%3A%20Dynamic%20Diffusion%20Schr%C3%B6dinger%20Bridge%20in%20Astrophysical%20Observational%0A%20%20Inversions%0AAuthor%3A%20Ye%20Zhu%20and%20Duo%20Xu%20and%20Zhiwei%20Deng%20and%20Jonathan%20C.%20Tan%20and%20Olga%20Russakovsky%0AAbstract%3A%20%20%20We%20study%20Diffusion%20Schr%5C%22odinger%20Bridge%20%28DSB%29%20models%20in%20the%20context%20of%0Adynamical%20astrophysical%20systems%2C%20specifically%20tackling%20observational%20inverse%0Aprediction%20tasks%20within%20Giant%20Molecular%20Clouds%20%28GMCs%29%20for%20star%20formation.%20We%0Aintroduce%20the%20Astro-DSB%20model%2C%20a%20variant%20of%20DSB%20with%20the%20pairwise%20domain%0Aassumption%20tailored%20for%20astrophysical%20dynamics.%20By%20investigating%20its%20learning%0Aprocess%20and%20prediction%20performance%20in%20both%20physically%20simulated%20data%20and%20in%0Areal%20observations%20%28the%20Taurus%20B213%20data%29%2C%20we%20present%20two%20main%20takeaways.%20First%2C%0Afrom%20the%20astrophysical%20perspective%2C%20our%20proposed%20paired%20DSB%20method%20improves%0Ainterpretability%2C%20learning%20efficiency%2C%20and%20prediction%20performance%20over%0Aconventional%20astrostatistical%20and%20other%20machine%20learning%20methods.%20Second%2C%20from%0Athe%20generative%20modeling%20perspective%2C%20probabilistic%20generative%20modeling%20reveals%0Aimprovements%20over%20discriminative%20pixel-to-pixel%20modeling%20in%20Out-Of-Distribution%0A%28OOD%29%20testing%20cases%20of%20physical%20simulations%20with%20unseen%20initial%20conditions%20and%0Adifferent%20dominant%20physical%20processes.%20Our%20study%20expands%20research%20into%0Adiffusion%20models%20beyond%20the%20traditional%20visual%20synthesis%20application%20and%0Aprovides%20evidence%20of%20the%20models%27%20learning%20abilities%20beyond%20pure%20data%0Astatistics%2C%20paving%20a%20path%20for%20future%20physics-aware%20generative%20models%20which%20can%0Aalign%20dynamics%20between%20machine%20learning%20and%20real%20%28astro%29physical%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08065v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Diffusion%2520Schr%25C3%25B6dinger%2520Bridge%2520in%2520Astrophysical%2520Observational%250A%2520%2520Inversions%26entry.906535625%3DYe%2520Zhu%2520and%2520Duo%2520Xu%2520and%2520Zhiwei%2520Deng%2520and%2520Jonathan%2520C.%2520Tan%2520and%2520Olga%2520Russakovsky%26entry.1292438233%3D%2520%2520We%2520study%2520Diffusion%2520Schr%255C%2522odinger%2520Bridge%2520%2528DSB%2529%2520models%2520in%2520the%2520context%2520of%250Adynamical%2520astrophysical%2520systems%252C%2520specifically%2520tackling%2520observational%2520inverse%250Aprediction%2520tasks%2520within%2520Giant%2520Molecular%2520Clouds%2520%2528GMCs%2529%2520for%2520star%2520formation.%2520We%250Aintroduce%2520the%2520Astro-DSB%2520model%252C%2520a%2520variant%2520of%2520DSB%2520with%2520the%2520pairwise%2520domain%250Aassumption%2520tailored%2520for%2520astrophysical%2520dynamics.%2520By%2520investigating%2520its%2520learning%250Aprocess%2520and%2520prediction%2520performance%2520in%2520both%2520physically%2520simulated%2520data%2520and%2520in%250Areal%2520observations%2520%2528the%2520Taurus%2520B213%2520data%2529%252C%2520we%2520present%2520two%2520main%2520takeaways.%2520First%252C%250Afrom%2520the%2520astrophysical%2520perspective%252C%2520our%2520proposed%2520paired%2520DSB%2520method%2520improves%250Ainterpretability%252C%2520learning%2520efficiency%252C%2520and%2520prediction%2520performance%2520over%250Aconventional%2520astrostatistical%2520and%2520other%2520machine%2520learning%2520methods.%2520Second%252C%2520from%250Athe%2520generative%2520modeling%2520perspective%252C%2520probabilistic%2520generative%2520modeling%2520reveals%250Aimprovements%2520over%2520discriminative%2520pixel-to-pixel%2520modeling%2520in%2520Out-Of-Distribution%250A%2528OOD%2529%2520testing%2520cases%2520of%2520physical%2520simulations%2520with%2520unseen%2520initial%2520conditions%2520and%250Adifferent%2520dominant%2520physical%2520processes.%2520Our%2520study%2520expands%2520research%2520into%250Adiffusion%2520models%2520beyond%2520the%2520traditional%2520visual%2520synthesis%2520application%2520and%250Aprovides%2520evidence%2520of%2520the%2520models%2527%2520learning%2520abilities%2520beyond%2520pure%2520data%250Astatistics%252C%2520paving%2520a%2520path%2520for%2520future%2520physics-aware%2520generative%2520models%2520which%2520can%250Aalign%2520dynamics%2520between%2520machine%2520learning%2520and%2520real%2520%2528astro%2529physical%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08065v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Diffusion%20Schr%C3%B6dinger%20Bridge%20in%20Astrophysical%20Observational%0A%20%20Inversions&entry.906535625=Ye%20Zhu%20and%20Duo%20Xu%20and%20Zhiwei%20Deng%20and%20Jonathan%20C.%20Tan%20and%20Olga%20Russakovsky&entry.1292438233=%20%20We%20study%20Diffusion%20Schr%5C%22odinger%20Bridge%20%28DSB%29%20models%20in%20the%20context%20of%0Adynamical%20astrophysical%20systems%2C%20specifically%20tackling%20observational%20inverse%0Aprediction%20tasks%20within%20Giant%20Molecular%20Clouds%20%28GMCs%29%20for%20star%20formation.%20We%0Aintroduce%20the%20Astro-DSB%20model%2C%20a%20variant%20of%20DSB%20with%20the%20pairwise%20domain%0Aassumption%20tailored%20for%20astrophysical%20dynamics.%20By%20investigating%20its%20learning%0Aprocess%20and%20prediction%20performance%20in%20both%20physically%20simulated%20data%20and%20in%0Areal%20observations%20%28the%20Taurus%20B213%20data%29%2C%20we%20present%20two%20main%20takeaways.%20First%2C%0Afrom%20the%20astrophysical%20perspective%2C%20our%20proposed%20paired%20DSB%20method%20improves%0Ainterpretability%2C%20learning%20efficiency%2C%20and%20prediction%20performance%20over%0Aconventional%20astrostatistical%20and%20other%20machine%20learning%20methods.%20Second%2C%20from%0Athe%20generative%20modeling%20perspective%2C%20probabilistic%20generative%20modeling%20reveals%0Aimprovements%20over%20discriminative%20pixel-to-pixel%20modeling%20in%20Out-Of-Distribution%0A%28OOD%29%20testing%20cases%20of%20physical%20simulations%20with%20unseen%20initial%20conditions%20and%0Adifferent%20dominant%20physical%20processes.%20Our%20study%20expands%20research%20into%0Adiffusion%20models%20beyond%20the%20traditional%20visual%20synthesis%20application%20and%0Aprovides%20evidence%20of%20the%20models%27%20learning%20abilities%20beyond%20pure%20data%0Astatistics%2C%20paving%20a%20path%20for%20future%20physics-aware%20generative%20models%20which%20can%0Aalign%20dynamics%20between%20machine%20learning%20and%20real%20%28astro%29physical%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08065v2&entry.124074799=Read"},
{"title": "Mamba time series forecasting with uncertainty quantification", "author": "Pedro Pessoa and Paul Campitelli and Douglas P. Shepherd and S. Banu Ozkan and Steve Press\u00e9", "abstract": "  State space models, such as Mamba, have recently garnered attention in time\nseries forecasting due to their ability to capture sequence patterns. However,\nin electricity consumption benchmarks, Mamba forecasts exhibit a mean error of\napproximately 8\\%. Similarly, in traffic occupancy benchmarks, the mean error\nreaches 18\\%. This discrepancy leaves us to wonder whether the prediction is\nsimply inaccurate or falls within error given spread in historical data. To\naddress this limitation, we propose a method to quantify the predictive\nuncertainty of Mamba forecasts. Here, we propose a dual-network framework based\non the Mamba architecture for probabilistic forecasting, where one network\ngenerates point forecasts while the other estimates predictive uncertainty by\nmodeling variance. We abbreviate our tool, Mamba with probabilistic time series\nforecasting, as Mamba-ProbTSF and the code for its implementation is available\non GitHub (https://github.com/PessoaP/Mamba-ProbTSF). Evaluating this approach\non synthetic and real-world benchmark datasets, we find Kullback-Leibler\ndivergence between the learned distributions and the data--which, in the limit\nof infinite data, should converge to zero if the model correctly captures the\nunderlying probability distribution--reduced to the order of $10^{-3}$ for\nsynthetic data and $10^{-1}$ for real-world benchmark, demonstrating its\neffectiveness. We find that in both the electricity consumption and traffic\noccupancy benchmark, the true trajectory stays within the predicted uncertainty\ninterval at the two-sigma level about 95\\% of the time. We end with a\nconsideration of potential limitations, adjustments to improve performance, and\nconsiderations for applying this framework to processes for purely or largely\nstochastic dynamics where the stochastic changes accumulate, as observed for\nexample in pure Brownian motion or molecular dynamics trajectories.\n", "link": "http://arxiv.org/abs/2503.10873v2", "date": "2025-06-11", "relevancy": 1.518, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5372}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5052}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mamba%20time%20series%20forecasting%20with%20uncertainty%20quantification&body=Title%3A%20Mamba%20time%20series%20forecasting%20with%20uncertainty%20quantification%0AAuthor%3A%20Pedro%20Pessoa%20and%20Paul%20Campitelli%20and%20Douglas%20P.%20Shepherd%20and%20S.%20Banu%20Ozkan%20and%20Steve%20Press%C3%A9%0AAbstract%3A%20%20%20State%20space%20models%2C%20such%20as%20Mamba%2C%20have%20recently%20garnered%20attention%20in%20time%0Aseries%20forecasting%20due%20to%20their%20ability%20to%20capture%20sequence%20patterns.%20However%2C%0Ain%20electricity%20consumption%20benchmarks%2C%20Mamba%20forecasts%20exhibit%20a%20mean%20error%20of%0Aapproximately%208%5C%25.%20Similarly%2C%20in%20traffic%20occupancy%20benchmarks%2C%20the%20mean%20error%0Areaches%2018%5C%25.%20This%20discrepancy%20leaves%20us%20to%20wonder%20whether%20the%20prediction%20is%0Asimply%20inaccurate%20or%20falls%20within%20error%20given%20spread%20in%20historical%20data.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20a%20method%20to%20quantify%20the%20predictive%0Auncertainty%20of%20Mamba%20forecasts.%20Here%2C%20we%20propose%20a%20dual-network%20framework%20based%0Aon%20the%20Mamba%20architecture%20for%20probabilistic%20forecasting%2C%20where%20one%20network%0Agenerates%20point%20forecasts%20while%20the%20other%20estimates%20predictive%20uncertainty%20by%0Amodeling%20variance.%20We%20abbreviate%20our%20tool%2C%20Mamba%20with%20probabilistic%20time%20series%0Aforecasting%2C%20as%20Mamba-ProbTSF%20and%20the%20code%20for%20its%20implementation%20is%20available%0Aon%20GitHub%20%28https%3A//github.com/PessoaP/Mamba-ProbTSF%29.%20Evaluating%20this%20approach%0Aon%20synthetic%20and%20real-world%20benchmark%20datasets%2C%20we%20find%20Kullback-Leibler%0Adivergence%20between%20the%20learned%20distributions%20and%20the%20data--which%2C%20in%20the%20limit%0Aof%20infinite%20data%2C%20should%20converge%20to%20zero%20if%20the%20model%20correctly%20captures%20the%0Aunderlying%20probability%20distribution--reduced%20to%20the%20order%20of%20%2410%5E%7B-3%7D%24%20for%0Asynthetic%20data%20and%20%2410%5E%7B-1%7D%24%20for%20real-world%20benchmark%2C%20demonstrating%20its%0Aeffectiveness.%20We%20find%20that%20in%20both%20the%20electricity%20consumption%20and%20traffic%0Aoccupancy%20benchmark%2C%20the%20true%20trajectory%20stays%20within%20the%20predicted%20uncertainty%0Ainterval%20at%20the%20two-sigma%20level%20about%2095%5C%25%20of%20the%20time.%20We%20end%20with%20a%0Aconsideration%20of%20potential%20limitations%2C%20adjustments%20to%20improve%20performance%2C%20and%0Aconsiderations%20for%20applying%20this%20framework%20to%20processes%20for%20purely%20or%20largely%0Astochastic%20dynamics%20where%20the%20stochastic%20changes%20accumulate%2C%20as%20observed%20for%0Aexample%20in%20pure%20Brownian%20motion%20or%20molecular%20dynamics%20trajectories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10873v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMamba%2520time%2520series%2520forecasting%2520with%2520uncertainty%2520quantification%26entry.906535625%3DPedro%2520Pessoa%2520and%2520Paul%2520Campitelli%2520and%2520Douglas%2520P.%2520Shepherd%2520and%2520S.%2520Banu%2520Ozkan%2520and%2520Steve%2520Press%25C3%25A9%26entry.1292438233%3D%2520%2520State%2520space%2520models%252C%2520such%2520as%2520Mamba%252C%2520have%2520recently%2520garnered%2520attention%2520in%2520time%250Aseries%2520forecasting%2520due%2520to%2520their%2520ability%2520to%2520capture%2520sequence%2520patterns.%2520However%252C%250Ain%2520electricity%2520consumption%2520benchmarks%252C%2520Mamba%2520forecasts%2520exhibit%2520a%2520mean%2520error%2520of%250Aapproximately%25208%255C%2525.%2520Similarly%252C%2520in%2520traffic%2520occupancy%2520benchmarks%252C%2520the%2520mean%2520error%250Areaches%252018%255C%2525.%2520This%2520discrepancy%2520leaves%2520us%2520to%2520wonder%2520whether%2520the%2520prediction%2520is%250Asimply%2520inaccurate%2520or%2520falls%2520within%2520error%2520given%2520spread%2520in%2520historical%2520data.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520propose%2520a%2520method%2520to%2520quantify%2520the%2520predictive%250Auncertainty%2520of%2520Mamba%2520forecasts.%2520Here%252C%2520we%2520propose%2520a%2520dual-network%2520framework%2520based%250Aon%2520the%2520Mamba%2520architecture%2520for%2520probabilistic%2520forecasting%252C%2520where%2520one%2520network%250Agenerates%2520point%2520forecasts%2520while%2520the%2520other%2520estimates%2520predictive%2520uncertainty%2520by%250Amodeling%2520variance.%2520We%2520abbreviate%2520our%2520tool%252C%2520Mamba%2520with%2520probabilistic%2520time%2520series%250Aforecasting%252C%2520as%2520Mamba-ProbTSF%2520and%2520the%2520code%2520for%2520its%2520implementation%2520is%2520available%250Aon%2520GitHub%2520%2528https%253A//github.com/PessoaP/Mamba-ProbTSF%2529.%2520Evaluating%2520this%2520approach%250Aon%2520synthetic%2520and%2520real-world%2520benchmark%2520datasets%252C%2520we%2520find%2520Kullback-Leibler%250Adivergence%2520between%2520the%2520learned%2520distributions%2520and%2520the%2520data--which%252C%2520in%2520the%2520limit%250Aof%2520infinite%2520data%252C%2520should%2520converge%2520to%2520zero%2520if%2520the%2520model%2520correctly%2520captures%2520the%250Aunderlying%2520probability%2520distribution--reduced%2520to%2520the%2520order%2520of%2520%252410%255E%257B-3%257D%2524%2520for%250Asynthetic%2520data%2520and%2520%252410%255E%257B-1%257D%2524%2520for%2520real-world%2520benchmark%252C%2520demonstrating%2520its%250Aeffectiveness.%2520We%2520find%2520that%2520in%2520both%2520the%2520electricity%2520consumption%2520and%2520traffic%250Aoccupancy%2520benchmark%252C%2520the%2520true%2520trajectory%2520stays%2520within%2520the%2520predicted%2520uncertainty%250Ainterval%2520at%2520the%2520two-sigma%2520level%2520about%252095%255C%2525%2520of%2520the%2520time.%2520We%2520end%2520with%2520a%250Aconsideration%2520of%2520potential%2520limitations%252C%2520adjustments%2520to%2520improve%2520performance%252C%2520and%250Aconsiderations%2520for%2520applying%2520this%2520framework%2520to%2520processes%2520for%2520purely%2520or%2520largely%250Astochastic%2520dynamics%2520where%2520the%2520stochastic%2520changes%2520accumulate%252C%2520as%2520observed%2520for%250Aexample%2520in%2520pure%2520Brownian%2520motion%2520or%2520molecular%2520dynamics%2520trajectories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10873v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mamba%20time%20series%20forecasting%20with%20uncertainty%20quantification&entry.906535625=Pedro%20Pessoa%20and%20Paul%20Campitelli%20and%20Douglas%20P.%20Shepherd%20and%20S.%20Banu%20Ozkan%20and%20Steve%20Press%C3%A9&entry.1292438233=%20%20State%20space%20models%2C%20such%20as%20Mamba%2C%20have%20recently%20garnered%20attention%20in%20time%0Aseries%20forecasting%20due%20to%20their%20ability%20to%20capture%20sequence%20patterns.%20However%2C%0Ain%20electricity%20consumption%20benchmarks%2C%20Mamba%20forecasts%20exhibit%20a%20mean%20error%20of%0Aapproximately%208%5C%25.%20Similarly%2C%20in%20traffic%20occupancy%20benchmarks%2C%20the%20mean%20error%0Areaches%2018%5C%25.%20This%20discrepancy%20leaves%20us%20to%20wonder%20whether%20the%20prediction%20is%0Asimply%20inaccurate%20or%20falls%20within%20error%20given%20spread%20in%20historical%20data.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20a%20method%20to%20quantify%20the%20predictive%0Auncertainty%20of%20Mamba%20forecasts.%20Here%2C%20we%20propose%20a%20dual-network%20framework%20based%0Aon%20the%20Mamba%20architecture%20for%20probabilistic%20forecasting%2C%20where%20one%20network%0Agenerates%20point%20forecasts%20while%20the%20other%20estimates%20predictive%20uncertainty%20by%0Amodeling%20variance.%20We%20abbreviate%20our%20tool%2C%20Mamba%20with%20probabilistic%20time%20series%0Aforecasting%2C%20as%20Mamba-ProbTSF%20and%20the%20code%20for%20its%20implementation%20is%20available%0Aon%20GitHub%20%28https%3A//github.com/PessoaP/Mamba-ProbTSF%29.%20Evaluating%20this%20approach%0Aon%20synthetic%20and%20real-world%20benchmark%20datasets%2C%20we%20find%20Kullback-Leibler%0Adivergence%20between%20the%20learned%20distributions%20and%20the%20data--which%2C%20in%20the%20limit%0Aof%20infinite%20data%2C%20should%20converge%20to%20zero%20if%20the%20model%20correctly%20captures%20the%0Aunderlying%20probability%20distribution--reduced%20to%20the%20order%20of%20%2410%5E%7B-3%7D%24%20for%0Asynthetic%20data%20and%20%2410%5E%7B-1%7D%24%20for%20real-world%20benchmark%2C%20demonstrating%20its%0Aeffectiveness.%20We%20find%20that%20in%20both%20the%20electricity%20consumption%20and%20traffic%0Aoccupancy%20benchmark%2C%20the%20true%20trajectory%20stays%20within%20the%20predicted%20uncertainty%0Ainterval%20at%20the%20two-sigma%20level%20about%2095%5C%25%20of%20the%20time.%20We%20end%20with%20a%0Aconsideration%20of%20potential%20limitations%2C%20adjustments%20to%20improve%20performance%2C%20and%0Aconsiderations%20for%20applying%20this%20framework%20to%20processes%20for%20purely%20or%20largely%0Astochastic%20dynamics%20where%20the%20stochastic%20changes%20accumulate%2C%20as%20observed%20for%0Aexample%20in%20pure%20Brownian%20motion%20or%20molecular%20dynamics%20trajectories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10873v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


