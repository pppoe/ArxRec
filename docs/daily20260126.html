<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260125.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "ReWeaver: Towards Simulation-Ready and Topology-Accurate Garment Reconstruction", "author": "Ming Li and Hui Shan and Kai Zheng and Chentao Shen and Siyu Liu and Yanwei Fu and Zhen Chen and Xiangru Huang", "abstract": "High-quality 3D garment reconstruction plays a crucial role in mitigating the sim-to-real gap in applications such as digital avatars, virtual try-on and robotic manipulation. However, existing garment reconstruction methods typically rely on unstructured representations, such as 3D Gaussian Splats, struggling to provide accurate reconstructions of garment topology and sewing structures. As a result, the reconstructed outputs are often unsuitable for high-fidelity physical simulation. We propose ReWeaver, a novel framework for topology-accurate 3D garment and sewing pattern reconstruction from sparse multi-view RGB images. Given as few as four input views, ReWeaver predicts seams and panels as well as their connectivities in both the 2D UV space and the 3D space. The predicted seams and panels align precisely with the multi-view images, yielding structured 2D--3D garment representations suitable for 3D perception, high-fidelity physical simulation, and robotic manipulation. To enable effective training, we construct a large-scale dataset GCD-TS, comprising multi-view RGB images, 3D garment geometries, textured human body meshes and annotated sewing patterns. The dataset contains over 100,000 synthetic samples covering a wide range of complex geometries and topologies. Extensive experiments show that ReWeaver consistently outperforms existing methods in terms of topology accuracy, geometry alignment and seam-panel consistency.", "link": "http://arxiv.org/abs/2601.16672v1", "date": "2026-01-23", "relevancy": 3.1652, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6634}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6347}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReWeaver%3A%20Towards%20Simulation-Ready%20and%20Topology-Accurate%20Garment%20Reconstruction&body=Title%3A%20ReWeaver%3A%20Towards%20Simulation-Ready%20and%20Topology-Accurate%20Garment%20Reconstruction%0AAuthor%3A%20Ming%20Li%20and%20Hui%20Shan%20and%20Kai%20Zheng%20and%20Chentao%20Shen%20and%20Siyu%20Liu%20and%20Yanwei%20Fu%20and%20Zhen%20Chen%20and%20Xiangru%20Huang%0AAbstract%3A%20High-quality%203D%20garment%20reconstruction%20plays%20a%20crucial%20role%20in%20mitigating%20the%20sim-to-real%20gap%20in%20applications%20such%20as%20digital%20avatars%2C%20virtual%20try-on%20and%20robotic%20manipulation.%20However%2C%20existing%20garment%20reconstruction%20methods%20typically%20rely%20on%20unstructured%20representations%2C%20such%20as%203D%20Gaussian%20Splats%2C%20struggling%20to%20provide%20accurate%20reconstructions%20of%20garment%20topology%20and%20sewing%20structures.%20As%20a%20result%2C%20the%20reconstructed%20outputs%20are%20often%20unsuitable%20for%20high-fidelity%20physical%20simulation.%20We%20propose%20ReWeaver%2C%20a%20novel%20framework%20for%20topology-accurate%203D%20garment%20and%20sewing%20pattern%20reconstruction%20from%20sparse%20multi-view%20RGB%20images.%20Given%20as%20few%20as%20four%20input%20views%2C%20ReWeaver%20predicts%20seams%20and%20panels%20as%20well%20as%20their%20connectivities%20in%20both%20the%202D%20UV%20space%20and%20the%203D%20space.%20The%20predicted%20seams%20and%20panels%20align%20precisely%20with%20the%20multi-view%20images%2C%20yielding%20structured%202D--3D%20garment%20representations%20suitable%20for%203D%20perception%2C%20high-fidelity%20physical%20simulation%2C%20and%20robotic%20manipulation.%20To%20enable%20effective%20training%2C%20we%20construct%20a%20large-scale%20dataset%20GCD-TS%2C%20comprising%20multi-view%20RGB%20images%2C%203D%20garment%20geometries%2C%20textured%20human%20body%20meshes%20and%20annotated%20sewing%20patterns.%20The%20dataset%20contains%20over%20100%2C000%20synthetic%20samples%20covering%20a%20wide%20range%20of%20complex%20geometries%20and%20topologies.%20Extensive%20experiments%20show%20that%20ReWeaver%20consistently%20outperforms%20existing%20methods%20in%20terms%20of%20topology%20accuracy%2C%20geometry%20alignment%20and%20seam-panel%20consistency.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReWeaver%253A%2520Towards%2520Simulation-Ready%2520and%2520Topology-Accurate%2520Garment%2520Reconstruction%26entry.906535625%3DMing%2520Li%2520and%2520Hui%2520Shan%2520and%2520Kai%2520Zheng%2520and%2520Chentao%2520Shen%2520and%2520Siyu%2520Liu%2520and%2520Yanwei%2520Fu%2520and%2520Zhen%2520Chen%2520and%2520Xiangru%2520Huang%26entry.1292438233%3DHigh-quality%25203D%2520garment%2520reconstruction%2520plays%2520a%2520crucial%2520role%2520in%2520mitigating%2520the%2520sim-to-real%2520gap%2520in%2520applications%2520such%2520as%2520digital%2520avatars%252C%2520virtual%2520try-on%2520and%2520robotic%2520manipulation.%2520However%252C%2520existing%2520garment%2520reconstruction%2520methods%2520typically%2520rely%2520on%2520unstructured%2520representations%252C%2520such%2520as%25203D%2520Gaussian%2520Splats%252C%2520struggling%2520to%2520provide%2520accurate%2520reconstructions%2520of%2520garment%2520topology%2520and%2520sewing%2520structures.%2520As%2520a%2520result%252C%2520the%2520reconstructed%2520outputs%2520are%2520often%2520unsuitable%2520for%2520high-fidelity%2520physical%2520simulation.%2520We%2520propose%2520ReWeaver%252C%2520a%2520novel%2520framework%2520for%2520topology-accurate%25203D%2520garment%2520and%2520sewing%2520pattern%2520reconstruction%2520from%2520sparse%2520multi-view%2520RGB%2520images.%2520Given%2520as%2520few%2520as%2520four%2520input%2520views%252C%2520ReWeaver%2520predicts%2520seams%2520and%2520panels%2520as%2520well%2520as%2520their%2520connectivities%2520in%2520both%2520the%25202D%2520UV%2520space%2520and%2520the%25203D%2520space.%2520The%2520predicted%2520seams%2520and%2520panels%2520align%2520precisely%2520with%2520the%2520multi-view%2520images%252C%2520yielding%2520structured%25202D--3D%2520garment%2520representations%2520suitable%2520for%25203D%2520perception%252C%2520high-fidelity%2520physical%2520simulation%252C%2520and%2520robotic%2520manipulation.%2520To%2520enable%2520effective%2520training%252C%2520we%2520construct%2520a%2520large-scale%2520dataset%2520GCD-TS%252C%2520comprising%2520multi-view%2520RGB%2520images%252C%25203D%2520garment%2520geometries%252C%2520textured%2520human%2520body%2520meshes%2520and%2520annotated%2520sewing%2520patterns.%2520The%2520dataset%2520contains%2520over%2520100%252C000%2520synthetic%2520samples%2520covering%2520a%2520wide%2520range%2520of%2520complex%2520geometries%2520and%2520topologies.%2520Extensive%2520experiments%2520show%2520that%2520ReWeaver%2520consistently%2520outperforms%2520existing%2520methods%2520in%2520terms%2520of%2520topology%2520accuracy%252C%2520geometry%2520alignment%2520and%2520seam-panel%2520consistency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReWeaver%3A%20Towards%20Simulation-Ready%20and%20Topology-Accurate%20Garment%20Reconstruction&entry.906535625=Ming%20Li%20and%20Hui%20Shan%20and%20Kai%20Zheng%20and%20Chentao%20Shen%20and%20Siyu%20Liu%20and%20Yanwei%20Fu%20and%20Zhen%20Chen%20and%20Xiangru%20Huang&entry.1292438233=High-quality%203D%20garment%20reconstruction%20plays%20a%20crucial%20role%20in%20mitigating%20the%20sim-to-real%20gap%20in%20applications%20such%20as%20digital%20avatars%2C%20virtual%20try-on%20and%20robotic%20manipulation.%20However%2C%20existing%20garment%20reconstruction%20methods%20typically%20rely%20on%20unstructured%20representations%2C%20such%20as%203D%20Gaussian%20Splats%2C%20struggling%20to%20provide%20accurate%20reconstructions%20of%20garment%20topology%20and%20sewing%20structures.%20As%20a%20result%2C%20the%20reconstructed%20outputs%20are%20often%20unsuitable%20for%20high-fidelity%20physical%20simulation.%20We%20propose%20ReWeaver%2C%20a%20novel%20framework%20for%20topology-accurate%203D%20garment%20and%20sewing%20pattern%20reconstruction%20from%20sparse%20multi-view%20RGB%20images.%20Given%20as%20few%20as%20four%20input%20views%2C%20ReWeaver%20predicts%20seams%20and%20panels%20as%20well%20as%20their%20connectivities%20in%20both%20the%202D%20UV%20space%20and%20the%203D%20space.%20The%20predicted%20seams%20and%20panels%20align%20precisely%20with%20the%20multi-view%20images%2C%20yielding%20structured%202D--3D%20garment%20representations%20suitable%20for%203D%20perception%2C%20high-fidelity%20physical%20simulation%2C%20and%20robotic%20manipulation.%20To%20enable%20effective%20training%2C%20we%20construct%20a%20large-scale%20dataset%20GCD-TS%2C%20comprising%20multi-view%20RGB%20images%2C%203D%20garment%20geometries%2C%20textured%20human%20body%20meshes%20and%20annotated%20sewing%20patterns.%20The%20dataset%20contains%20over%20100%2C000%20synthetic%20samples%20covering%20a%20wide%20range%20of%20complex%20geometries%20and%20topologies.%20Extensive%20experiments%20show%20that%20ReWeaver%20consistently%20outperforms%20existing%20methods%20in%20terms%20of%20topology%20accuracy%2C%20geometry%20alignment%20and%20seam-panel%20consistency.&entry.1838667208=http%3A//arxiv.org/abs/2601.16672v1&entry.124074799=Read"},
{"title": "GPA-VGGT:Adapting VGGT to Large scale Localization by self-Supervised learning with Geometry and Physics Aware loss", "author": "Yangfan Xu and Lilian Zhang and Xiaofeng He and Pengdong Wu and Wenqi Wu and Jun Mao", "abstract": "Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT.", "link": "http://arxiv.org/abs/2601.16885v1", "date": "2026-01-23", "relevancy": 3.0867, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6285}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6277}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPA-VGGT%3AAdapting%20VGGT%20to%20Large%20scale%20Localization%20by%20self-Supervised%20learning%20with%20Geometry%20and%20Physics%20Aware%20loss&body=Title%3A%20GPA-VGGT%3AAdapting%20VGGT%20to%20Large%20scale%20Localization%20by%20self-Supervised%20learning%20with%20Geometry%20and%20Physics%20Aware%20loss%0AAuthor%3A%20Yangfan%20Xu%20and%20Lilian%20Zhang%20and%20Xiaofeng%20He%20and%20Pengdong%20Wu%20and%20Wenqi%20Wu%20and%20Jun%20Mao%0AAbstract%3A%20Transformer-based%20general%20visual%20geometry%20frameworks%20have%20shown%20promising%20performance%20in%20camera%20pose%20estimation%20and%203D%20scene%20understanding.%20Recent%20advancements%20in%20Visual%20Geometry%20Grounded%20Transformer%20%28VGGT%29%20models%20have%20shown%20great%20promise%20in%20camera%20pose%20estimation%20and%203D%20reconstruction.%20However%2C%20these%20models%20typically%20rely%20on%20ground%20truth%20labels%20for%20training%2C%20posing%20challenges%20when%20adapting%20to%20unlabeled%20and%20unseen%20scenes.%20In%20this%20paper%2C%20we%20propose%20a%20self-supervised%20framework%20to%20train%20VGGT%20with%20unlabeled%20data%2C%20thereby%20enhancing%20its%20localization%20capability%20in%20large-scale%20environments.%20To%20achieve%20this%2C%20we%20extend%20conventional%20pair-wise%20relations%20to%20sequence-wise%20geometric%20constraints%20for%20self-supervised%20learning.%20Specifically%2C%20in%20each%20sequence%2C%20we%20sample%20multiple%20source%20frames%20and%20geometrically%20project%20them%20onto%20different%20target%20frames%2C%20which%20improves%20temporal%20feature%20consistency.%20We%20formulate%20physical%20photometric%20consistency%20and%20geometric%20constraints%20as%20a%20joint%20optimization%20loss%20to%20circumvent%20the%20requirement%20for%20hard%20labels.%20By%20training%20the%20model%20with%20this%20proposed%20method%2C%20not%20only%20the%20local%20and%20global%20cross-view%20attention%20layers%20but%20also%20the%20camera%20and%20depth%20heads%20can%20effectively%20capture%20the%20underlying%20multi-view%20geometry.%20Experiments%20demonstrate%20that%20the%20model%20converges%20within%20hundreds%20of%20iterations%20and%20achieves%20significant%20improvements%20in%20large-scale%20localization.%20Our%20code%20will%20be%20released%20at%20https%3A//github.com/X-yangfan/GPA-VGGT.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPA-VGGT%253AAdapting%2520VGGT%2520to%2520Large%2520scale%2520Localization%2520by%2520self-Supervised%2520learning%2520with%2520Geometry%2520and%2520Physics%2520Aware%2520loss%26entry.906535625%3DYangfan%2520Xu%2520and%2520Lilian%2520Zhang%2520and%2520Xiaofeng%2520He%2520and%2520Pengdong%2520Wu%2520and%2520Wenqi%2520Wu%2520and%2520Jun%2520Mao%26entry.1292438233%3DTransformer-based%2520general%2520visual%2520geometry%2520frameworks%2520have%2520shown%2520promising%2520performance%2520in%2520camera%2520pose%2520estimation%2520and%25203D%2520scene%2520understanding.%2520Recent%2520advancements%2520in%2520Visual%2520Geometry%2520Grounded%2520Transformer%2520%2528VGGT%2529%2520models%2520have%2520shown%2520great%2520promise%2520in%2520camera%2520pose%2520estimation%2520and%25203D%2520reconstruction.%2520However%252C%2520these%2520models%2520typically%2520rely%2520on%2520ground%2520truth%2520labels%2520for%2520training%252C%2520posing%2520challenges%2520when%2520adapting%2520to%2520unlabeled%2520and%2520unseen%2520scenes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520self-supervised%2520framework%2520to%2520train%2520VGGT%2520with%2520unlabeled%2520data%252C%2520thereby%2520enhancing%2520its%2520localization%2520capability%2520in%2520large-scale%2520environments.%2520To%2520achieve%2520this%252C%2520we%2520extend%2520conventional%2520pair-wise%2520relations%2520to%2520sequence-wise%2520geometric%2520constraints%2520for%2520self-supervised%2520learning.%2520Specifically%252C%2520in%2520each%2520sequence%252C%2520we%2520sample%2520multiple%2520source%2520frames%2520and%2520geometrically%2520project%2520them%2520onto%2520different%2520target%2520frames%252C%2520which%2520improves%2520temporal%2520feature%2520consistency.%2520We%2520formulate%2520physical%2520photometric%2520consistency%2520and%2520geometric%2520constraints%2520as%2520a%2520joint%2520optimization%2520loss%2520to%2520circumvent%2520the%2520requirement%2520for%2520hard%2520labels.%2520By%2520training%2520the%2520model%2520with%2520this%2520proposed%2520method%252C%2520not%2520only%2520the%2520local%2520and%2520global%2520cross-view%2520attention%2520layers%2520but%2520also%2520the%2520camera%2520and%2520depth%2520heads%2520can%2520effectively%2520capture%2520the%2520underlying%2520multi-view%2520geometry.%2520Experiments%2520demonstrate%2520that%2520the%2520model%2520converges%2520within%2520hundreds%2520of%2520iterations%2520and%2520achieves%2520significant%2520improvements%2520in%2520large-scale%2520localization.%2520Our%2520code%2520will%2520be%2520released%2520at%2520https%253A//github.com/X-yangfan/GPA-VGGT.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPA-VGGT%3AAdapting%20VGGT%20to%20Large%20scale%20Localization%20by%20self-Supervised%20learning%20with%20Geometry%20and%20Physics%20Aware%20loss&entry.906535625=Yangfan%20Xu%20and%20Lilian%20Zhang%20and%20Xiaofeng%20He%20and%20Pengdong%20Wu%20and%20Wenqi%20Wu%20and%20Jun%20Mao&entry.1292438233=Transformer-based%20general%20visual%20geometry%20frameworks%20have%20shown%20promising%20performance%20in%20camera%20pose%20estimation%20and%203D%20scene%20understanding.%20Recent%20advancements%20in%20Visual%20Geometry%20Grounded%20Transformer%20%28VGGT%29%20models%20have%20shown%20great%20promise%20in%20camera%20pose%20estimation%20and%203D%20reconstruction.%20However%2C%20these%20models%20typically%20rely%20on%20ground%20truth%20labels%20for%20training%2C%20posing%20challenges%20when%20adapting%20to%20unlabeled%20and%20unseen%20scenes.%20In%20this%20paper%2C%20we%20propose%20a%20self-supervised%20framework%20to%20train%20VGGT%20with%20unlabeled%20data%2C%20thereby%20enhancing%20its%20localization%20capability%20in%20large-scale%20environments.%20To%20achieve%20this%2C%20we%20extend%20conventional%20pair-wise%20relations%20to%20sequence-wise%20geometric%20constraints%20for%20self-supervised%20learning.%20Specifically%2C%20in%20each%20sequence%2C%20we%20sample%20multiple%20source%20frames%20and%20geometrically%20project%20them%20onto%20different%20target%20frames%2C%20which%20improves%20temporal%20feature%20consistency.%20We%20formulate%20physical%20photometric%20consistency%20and%20geometric%20constraints%20as%20a%20joint%20optimization%20loss%20to%20circumvent%20the%20requirement%20for%20hard%20labels.%20By%20training%20the%20model%20with%20this%20proposed%20method%2C%20not%20only%20the%20local%20and%20global%20cross-view%20attention%20layers%20but%20also%20the%20camera%20and%20depth%20heads%20can%20effectively%20capture%20the%20underlying%20multi-view%20geometry.%20Experiments%20demonstrate%20that%20the%20model%20converges%20within%20hundreds%20of%20iterations%20and%20achieves%20significant%20improvements%20in%20large-scale%20localization.%20Our%20code%20will%20be%20released%20at%20https%3A//github.com/X-yangfan/GPA-VGGT.&entry.1838667208=http%3A//arxiv.org/abs/2601.16885v1&entry.124074799=Read"},
{"title": "A Step to Decouple Optimization in 3DGS", "author": "Renjie Ding and Yaonan Wang and Min Liu and Jialin Zhu and Jiazheng Wang and Jiahao Zhao and Wenting Shen and Feixiang He and Xiang Che", "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time novel view synthesis. As an explicit representation optimized through gradient propagation among primitives, optimization widely accepted in deep neural networks (DNNs) is actually adopted in 3DGS, such as synchronous weight updating and Adam with the adaptive gradient. However, considering the physical significance and specific design in 3DGS, there are two overlooked details in the optimization of 3DGS: (i) update step coupling, which induces optimizer state rescaling and costly attribute updates outside the viewpoints, and (ii) gradient coupling in the moment, which may lead to under- or over-effective regularization. Nevertheless, such a complex coupling is under-explored. After revisiting the optimization of 3DGS, we take a step to decouple it and recompose the process into: Sparse Adam, Re-State Regularization and Decoupled Attribute Regularization. Taking a large number of experiments under the 3DGS and 3DGS-MCMC frameworks, our work provides a deeper understanding of these components. Finally, based on the empirical analysis, we re-design the optimization and propose AdamW-GS by re-coupling the beneficial components, under which better optimization efficiency and representation effectiveness are achieved simultaneously.", "link": "http://arxiv.org/abs/2601.16736v1", "date": "2026-01-23", "relevancy": 3.0164, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6104}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6069}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Step%20to%20Decouple%20Optimization%20in%203DGS&body=Title%3A%20A%20Step%20to%20Decouple%20Optimization%20in%203DGS%0AAuthor%3A%20Renjie%20Ding%20and%20Yaonan%20Wang%20and%20Min%20Liu%20and%20Jialin%20Zhu%20and%20Jiazheng%20Wang%20and%20Jiahao%20Zhao%20and%20Wenting%20Shen%20and%20Feixiang%20He%20and%20Xiang%20Che%0AAbstract%3A%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20powerful%20technique%20for%20real-time%20novel%20view%20synthesis.%20As%20an%20explicit%20representation%20optimized%20through%20gradient%20propagation%20among%20primitives%2C%20optimization%20widely%20accepted%20in%20deep%20neural%20networks%20%28DNNs%29%20is%20actually%20adopted%20in%203DGS%2C%20such%20as%20synchronous%20weight%20updating%20and%20Adam%20with%20the%20adaptive%20gradient.%20However%2C%20considering%20the%20physical%20significance%20and%20specific%20design%20in%203DGS%2C%20there%20are%20two%20overlooked%20details%20in%20the%20optimization%20of%203DGS%3A%20%28i%29%20update%20step%20coupling%2C%20which%20induces%20optimizer%20state%20rescaling%20and%20costly%20attribute%20updates%20outside%20the%20viewpoints%2C%20and%20%28ii%29%20gradient%20coupling%20in%20the%20moment%2C%20which%20may%20lead%20to%20under-%20or%20over-effective%20regularization.%20Nevertheless%2C%20such%20a%20complex%20coupling%20is%20under-explored.%20After%20revisiting%20the%20optimization%20of%203DGS%2C%20we%20take%20a%20step%20to%20decouple%20it%20and%20recompose%20the%20process%20into%3A%20Sparse%20Adam%2C%20Re-State%20Regularization%20and%20Decoupled%20Attribute%20Regularization.%20Taking%20a%20large%20number%20of%20experiments%20under%20the%203DGS%20and%203DGS-MCMC%20frameworks%2C%20our%20work%20provides%20a%20deeper%20understanding%20of%20these%20components.%20Finally%2C%20based%20on%20the%20empirical%20analysis%2C%20we%20re-design%20the%20optimization%20and%20propose%20AdamW-GS%20by%20re-coupling%20the%20beneficial%20components%2C%20under%20which%20better%20optimization%20efficiency%20and%20representation%20effectiveness%20are%20achieved%20simultaneously.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Step%2520to%2520Decouple%2520Optimization%2520in%25203DGS%26entry.906535625%3DRenjie%2520Ding%2520and%2520Yaonan%2520Wang%2520and%2520Min%2520Liu%2520and%2520Jialin%2520Zhu%2520and%2520Jiazheng%2520Wang%2520and%2520Jiahao%2520Zhao%2520and%2520Wenting%2520Shen%2520and%2520Feixiang%2520He%2520and%2520Xiang%2520Che%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520technique%2520for%2520real-time%2520novel%2520view%2520synthesis.%2520As%2520an%2520explicit%2520representation%2520optimized%2520through%2520gradient%2520propagation%2520among%2520primitives%252C%2520optimization%2520widely%2520accepted%2520in%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520is%2520actually%2520adopted%2520in%25203DGS%252C%2520such%2520as%2520synchronous%2520weight%2520updating%2520and%2520Adam%2520with%2520the%2520adaptive%2520gradient.%2520However%252C%2520considering%2520the%2520physical%2520significance%2520and%2520specific%2520design%2520in%25203DGS%252C%2520there%2520are%2520two%2520overlooked%2520details%2520in%2520the%2520optimization%2520of%25203DGS%253A%2520%2528i%2529%2520update%2520step%2520coupling%252C%2520which%2520induces%2520optimizer%2520state%2520rescaling%2520and%2520costly%2520attribute%2520updates%2520outside%2520the%2520viewpoints%252C%2520and%2520%2528ii%2529%2520gradient%2520coupling%2520in%2520the%2520moment%252C%2520which%2520may%2520lead%2520to%2520under-%2520or%2520over-effective%2520regularization.%2520Nevertheless%252C%2520such%2520a%2520complex%2520coupling%2520is%2520under-explored.%2520After%2520revisiting%2520the%2520optimization%2520of%25203DGS%252C%2520we%2520take%2520a%2520step%2520to%2520decouple%2520it%2520and%2520recompose%2520the%2520process%2520into%253A%2520Sparse%2520Adam%252C%2520Re-State%2520Regularization%2520and%2520Decoupled%2520Attribute%2520Regularization.%2520Taking%2520a%2520large%2520number%2520of%2520experiments%2520under%2520the%25203DGS%2520and%25203DGS-MCMC%2520frameworks%252C%2520our%2520work%2520provides%2520a%2520deeper%2520understanding%2520of%2520these%2520components.%2520Finally%252C%2520based%2520on%2520the%2520empirical%2520analysis%252C%2520we%2520re-design%2520the%2520optimization%2520and%2520propose%2520AdamW-GS%2520by%2520re-coupling%2520the%2520beneficial%2520components%252C%2520under%2520which%2520better%2520optimization%2520efficiency%2520and%2520representation%2520effectiveness%2520are%2520achieved%2520simultaneously.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Step%20to%20Decouple%20Optimization%20in%203DGS&entry.906535625=Renjie%20Ding%20and%20Yaonan%20Wang%20and%20Min%20Liu%20and%20Jialin%20Zhu%20and%20Jiazheng%20Wang%20and%20Jiahao%20Zhao%20and%20Wenting%20Shen%20and%20Feixiang%20He%20and%20Xiang%20Che&entry.1292438233=3D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20powerful%20technique%20for%20real-time%20novel%20view%20synthesis.%20As%20an%20explicit%20representation%20optimized%20through%20gradient%20propagation%20among%20primitives%2C%20optimization%20widely%20accepted%20in%20deep%20neural%20networks%20%28DNNs%29%20is%20actually%20adopted%20in%203DGS%2C%20such%20as%20synchronous%20weight%20updating%20and%20Adam%20with%20the%20adaptive%20gradient.%20However%2C%20considering%20the%20physical%20significance%20and%20specific%20design%20in%203DGS%2C%20there%20are%20two%20overlooked%20details%20in%20the%20optimization%20of%203DGS%3A%20%28i%29%20update%20step%20coupling%2C%20which%20induces%20optimizer%20state%20rescaling%20and%20costly%20attribute%20updates%20outside%20the%20viewpoints%2C%20and%20%28ii%29%20gradient%20coupling%20in%20the%20moment%2C%20which%20may%20lead%20to%20under-%20or%20over-effective%20regularization.%20Nevertheless%2C%20such%20a%20complex%20coupling%20is%20under-explored.%20After%20revisiting%20the%20optimization%20of%203DGS%2C%20we%20take%20a%20step%20to%20decouple%20it%20and%20recompose%20the%20process%20into%3A%20Sparse%20Adam%2C%20Re-State%20Regularization%20and%20Decoupled%20Attribute%20Regularization.%20Taking%20a%20large%20number%20of%20experiments%20under%20the%203DGS%20and%203DGS-MCMC%20frameworks%2C%20our%20work%20provides%20a%20deeper%20understanding%20of%20these%20components.%20Finally%2C%20based%20on%20the%20empirical%20analysis%2C%20we%20re-design%20the%20optimization%20and%20propose%20AdamW-GS%20by%20re-coupling%20the%20beneficial%20components%2C%20under%20which%20better%20optimization%20efficiency%20and%20representation%20effectiveness%20are%20achieved%20simultaneously.&entry.1838667208=http%3A//arxiv.org/abs/2601.16736v1&entry.124074799=Read"},
{"title": "Flow Matching for Probabilistic Monocular 3D Human Pose Estimation", "author": "Cuong Le and Pavl\u00f3 Melnyk and Bastian Wandt and M\u00e5rten Wadenb\u00e4ck", "abstract": "Recovering 3D human poses from a monocular camera view is a highly ill-posed problem due to the depth ambiguity. Earlier studies on 3D human pose lifting from 2D often contain incorrect-yet-overconfident 3D estimations. To mitigate the problem, emerging probabilistic approaches treat the 3D estimations as a distribution, taking into account the uncertainty measurement of the poses. Falling in a similar category, we proposed FMPose, a probabilistic 3D human pose estimation method based on the flow matching generative approach. Conditioned on the 2D cues, the flow matching scheme learns the optimal transport from a simple source distribution to the plausible 3D human pose distribution via continuous normalizing flows. The 2D lifting condition is modeled via graph convolutional networks, leveraging the learnable connections between human body joints as the graph structure for feature aggregation. Compared to diffusion-based methods, the FMPose with optimal transport produces faster and more accurate 3D pose generations. Experimental results show major improvements of our FMPose over current state-of-the-art methods on three common benchmarks for 3D human pose estimation, namely Human3.6M, MPI-INF-3DHP and 3DPW.", "link": "http://arxiv.org/abs/2601.16763v1", "date": "2026-01-23", "relevancy": 2.9614, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6416}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5812}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flow%20Matching%20for%20Probabilistic%20Monocular%203D%20Human%20Pose%20Estimation&body=Title%3A%20Flow%20Matching%20for%20Probabilistic%20Monocular%203D%20Human%20Pose%20Estimation%0AAuthor%3A%20Cuong%20Le%20and%20Pavl%C3%B3%20Melnyk%20and%20Bastian%20Wandt%20and%20M%C3%A5rten%20Wadenb%C3%A4ck%0AAbstract%3A%20Recovering%203D%20human%20poses%20from%20a%20monocular%20camera%20view%20is%20a%20highly%20ill-posed%20problem%20due%20to%20the%20depth%20ambiguity.%20Earlier%20studies%20on%203D%20human%20pose%20lifting%20from%202D%20often%20contain%20incorrect-yet-overconfident%203D%20estimations.%20To%20mitigate%20the%20problem%2C%20emerging%20probabilistic%20approaches%20treat%20the%203D%20estimations%20as%20a%20distribution%2C%20taking%20into%20account%20the%20uncertainty%20measurement%20of%20the%20poses.%20Falling%20in%20a%20similar%20category%2C%20we%20proposed%20FMPose%2C%20a%20probabilistic%203D%20human%20pose%20estimation%20method%20based%20on%20the%20flow%20matching%20generative%20approach.%20Conditioned%20on%20the%202D%20cues%2C%20the%20flow%20matching%20scheme%20learns%20the%20optimal%20transport%20from%20a%20simple%20source%20distribution%20to%20the%20plausible%203D%20human%20pose%20distribution%20via%20continuous%20normalizing%20flows.%20The%202D%20lifting%20condition%20is%20modeled%20via%20graph%20convolutional%20networks%2C%20leveraging%20the%20learnable%20connections%20between%20human%20body%20joints%20as%20the%20graph%20structure%20for%20feature%20aggregation.%20Compared%20to%20diffusion-based%20methods%2C%20the%20FMPose%20with%20optimal%20transport%20produces%20faster%20and%20more%20accurate%203D%20pose%20generations.%20Experimental%20results%20show%20major%20improvements%20of%20our%20FMPose%20over%20current%20state-of-the-art%20methods%20on%20three%20common%20benchmarks%20for%203D%20human%20pose%20estimation%2C%20namely%20Human3.6M%2C%20MPI-INF-3DHP%20and%203DPW.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlow%2520Matching%2520for%2520Probabilistic%2520Monocular%25203D%2520Human%2520Pose%2520Estimation%26entry.906535625%3DCuong%2520Le%2520and%2520Pavl%25C3%25B3%2520Melnyk%2520and%2520Bastian%2520Wandt%2520and%2520M%25C3%25A5rten%2520Wadenb%25C3%25A4ck%26entry.1292438233%3DRecovering%25203D%2520human%2520poses%2520from%2520a%2520monocular%2520camera%2520view%2520is%2520a%2520highly%2520ill-posed%2520problem%2520due%2520to%2520the%2520depth%2520ambiguity.%2520Earlier%2520studies%2520on%25203D%2520human%2520pose%2520lifting%2520from%25202D%2520often%2520contain%2520incorrect-yet-overconfident%25203D%2520estimations.%2520To%2520mitigate%2520the%2520problem%252C%2520emerging%2520probabilistic%2520approaches%2520treat%2520the%25203D%2520estimations%2520as%2520a%2520distribution%252C%2520taking%2520into%2520account%2520the%2520uncertainty%2520measurement%2520of%2520the%2520poses.%2520Falling%2520in%2520a%2520similar%2520category%252C%2520we%2520proposed%2520FMPose%252C%2520a%2520probabilistic%25203D%2520human%2520pose%2520estimation%2520method%2520based%2520on%2520the%2520flow%2520matching%2520generative%2520approach.%2520Conditioned%2520on%2520the%25202D%2520cues%252C%2520the%2520flow%2520matching%2520scheme%2520learns%2520the%2520optimal%2520transport%2520from%2520a%2520simple%2520source%2520distribution%2520to%2520the%2520plausible%25203D%2520human%2520pose%2520distribution%2520via%2520continuous%2520normalizing%2520flows.%2520The%25202D%2520lifting%2520condition%2520is%2520modeled%2520via%2520graph%2520convolutional%2520networks%252C%2520leveraging%2520the%2520learnable%2520connections%2520between%2520human%2520body%2520joints%2520as%2520the%2520graph%2520structure%2520for%2520feature%2520aggregation.%2520Compared%2520to%2520diffusion-based%2520methods%252C%2520the%2520FMPose%2520with%2520optimal%2520transport%2520produces%2520faster%2520and%2520more%2520accurate%25203D%2520pose%2520generations.%2520Experimental%2520results%2520show%2520major%2520improvements%2520of%2520our%2520FMPose%2520over%2520current%2520state-of-the-art%2520methods%2520on%2520three%2520common%2520benchmarks%2520for%25203D%2520human%2520pose%2520estimation%252C%2520namely%2520Human3.6M%252C%2520MPI-INF-3DHP%2520and%25203DPW.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flow%20Matching%20for%20Probabilistic%20Monocular%203D%20Human%20Pose%20Estimation&entry.906535625=Cuong%20Le%20and%20Pavl%C3%B3%20Melnyk%20and%20Bastian%20Wandt%20and%20M%C3%A5rten%20Wadenb%C3%A4ck&entry.1292438233=Recovering%203D%20human%20poses%20from%20a%20monocular%20camera%20view%20is%20a%20highly%20ill-posed%20problem%20due%20to%20the%20depth%20ambiguity.%20Earlier%20studies%20on%203D%20human%20pose%20lifting%20from%202D%20often%20contain%20incorrect-yet-overconfident%203D%20estimations.%20To%20mitigate%20the%20problem%2C%20emerging%20probabilistic%20approaches%20treat%20the%203D%20estimations%20as%20a%20distribution%2C%20taking%20into%20account%20the%20uncertainty%20measurement%20of%20the%20poses.%20Falling%20in%20a%20similar%20category%2C%20we%20proposed%20FMPose%2C%20a%20probabilistic%203D%20human%20pose%20estimation%20method%20based%20on%20the%20flow%20matching%20generative%20approach.%20Conditioned%20on%20the%202D%20cues%2C%20the%20flow%20matching%20scheme%20learns%20the%20optimal%20transport%20from%20a%20simple%20source%20distribution%20to%20the%20plausible%203D%20human%20pose%20distribution%20via%20continuous%20normalizing%20flows.%20The%202D%20lifting%20condition%20is%20modeled%20via%20graph%20convolutional%20networks%2C%20leveraging%20the%20learnable%20connections%20between%20human%20body%20joints%20as%20the%20graph%20structure%20for%20feature%20aggregation.%20Compared%20to%20diffusion-based%20methods%2C%20the%20FMPose%20with%20optimal%20transport%20produces%20faster%20and%20more%20accurate%203D%20pose%20generations.%20Experimental%20results%20show%20major%20improvements%20of%20our%20FMPose%20over%20current%20state-of-the-art%20methods%20on%20three%20common%20benchmarks%20for%203D%20human%20pose%20estimation%2C%20namely%20Human3.6M%2C%20MPI-INF-3DHP%20and%203DPW.&entry.1838667208=http%3A//arxiv.org/abs/2601.16763v1&entry.124074799=Read"},
{"title": "FUSAR-KLIP: Towards Multimodal Foundation Models for Remote Sensing", "author": "Yi Yang and Xiaokun Zhang and Qingchen Fang and Jing Liu and Ziqi Ye and Rui Li and Li Liu and Haipeng Wang", "abstract": "Cross-modal artificial intelligence, represented by visual language models, has achieved significant success in general image understanding. However, a fundamental cognitive inconsistency exists between general visual representation and remote sensing image interpretation: remote sensing images couple topography, terrain, and spatial structure, thereby inherently requiring models to possess deep geoscientific understanding. This cognitive difference is further amplified in synthetic aperture radar (SAR) imagery: while SAR possesses irreplaceable all-weather, all-day observation capabilities, it is constrained by coherent imaging mechanisms, exhibiting significant modal heterogeneity with general images. To address this inconsistency, we propose FUSAR-KLIP, the first knowledge-guided general multimodal foundational model for SAR, along with reusable data and evaluation baselines. Specifically: (1) FUSAR-GEOVL-1M (the first large-scale SAR dataset with complete geographic projection attributes) was constructed, covering multiple satellite platforms, 120,000 images, and 135 cities; (2) Aligned structured text was generated through hierarchical cognitive thought chains, accurately encoding more than 1 million multidimensional semantic information from geomorphological environment and regional attributes to spatial relationships; (3) A self-consistent iterative optimization mechanism was designed to guide cross-modal learning with this knowledge information consistent with human cognition and physical laws in a self-supervised closed loop consisting of contrast, matching, and reconstruction; (4) A unified evaluation benchmark was established in 11 typical downstream tasks in the two major categories of vision and language, and compared with 15 mainstream foundation models.", "link": "http://arxiv.org/abs/2509.23927v4", "date": "2026-01-23", "relevancy": 2.9476, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5942}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5942}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FUSAR-KLIP%3A%20Towards%20Multimodal%20Foundation%20Models%20for%20Remote%20Sensing&body=Title%3A%20FUSAR-KLIP%3A%20Towards%20Multimodal%20Foundation%20Models%20for%20Remote%20Sensing%0AAuthor%3A%20Yi%20Yang%20and%20Xiaokun%20Zhang%20and%20Qingchen%20Fang%20and%20Jing%20Liu%20and%20Ziqi%20Ye%20and%20Rui%20Li%20and%20Li%20Liu%20and%20Haipeng%20Wang%0AAbstract%3A%20Cross-modal%20artificial%20intelligence%2C%20represented%20by%20visual%20language%20models%2C%20has%20achieved%20significant%20success%20in%20general%20image%20understanding.%20However%2C%20a%20fundamental%20cognitive%20inconsistency%20exists%20between%20general%20visual%20representation%20and%20remote%20sensing%20image%20interpretation%3A%20remote%20sensing%20images%20couple%20topography%2C%20terrain%2C%20and%20spatial%20structure%2C%20thereby%20inherently%20requiring%20models%20to%20possess%20deep%20geoscientific%20understanding.%20This%20cognitive%20difference%20is%20further%20amplified%20in%20synthetic%20aperture%20radar%20%28SAR%29%20imagery%3A%20while%20SAR%20possesses%20irreplaceable%20all-weather%2C%20all-day%20observation%20capabilities%2C%20it%20is%20constrained%20by%20coherent%20imaging%20mechanisms%2C%20exhibiting%20significant%20modal%20heterogeneity%20with%20general%20images.%20To%20address%20this%20inconsistency%2C%20we%20propose%20FUSAR-KLIP%2C%20the%20first%20knowledge-guided%20general%20multimodal%20foundational%20model%20for%20SAR%2C%20along%20with%20reusable%20data%20and%20evaluation%20baselines.%20Specifically%3A%20%281%29%20FUSAR-GEOVL-1M%20%28the%20first%20large-scale%20SAR%20dataset%20with%20complete%20geographic%20projection%20attributes%29%20was%20constructed%2C%20covering%20multiple%20satellite%20platforms%2C%20120%2C000%20images%2C%20and%20135%20cities%3B%20%282%29%20Aligned%20structured%20text%20was%20generated%20through%20hierarchical%20cognitive%20thought%20chains%2C%20accurately%20encoding%20more%20than%201%20million%20multidimensional%20semantic%20information%20from%20geomorphological%20environment%20and%20regional%20attributes%20to%20spatial%20relationships%3B%20%283%29%20A%20self-consistent%20iterative%20optimization%20mechanism%20was%20designed%20to%20guide%20cross-modal%20learning%20with%20this%20knowledge%20information%20consistent%20with%20human%20cognition%20and%20physical%20laws%20in%20a%20self-supervised%20closed%20loop%20consisting%20of%20contrast%2C%20matching%2C%20and%20reconstruction%3B%20%284%29%20A%20unified%20evaluation%20benchmark%20was%20established%20in%2011%20typical%20downstream%20tasks%20in%20the%20two%20major%20categories%20of%20vision%20and%20language%2C%20and%20compared%20with%2015%20mainstream%20foundation%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2509.23927v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFUSAR-KLIP%253A%2520Towards%2520Multimodal%2520Foundation%2520Models%2520for%2520Remote%2520Sensing%26entry.906535625%3DYi%2520Yang%2520and%2520Xiaokun%2520Zhang%2520and%2520Qingchen%2520Fang%2520and%2520Jing%2520Liu%2520and%2520Ziqi%2520Ye%2520and%2520Rui%2520Li%2520and%2520Li%2520Liu%2520and%2520Haipeng%2520Wang%26entry.1292438233%3DCross-modal%2520artificial%2520intelligence%252C%2520represented%2520by%2520visual%2520language%2520models%252C%2520has%2520achieved%2520significant%2520success%2520in%2520general%2520image%2520understanding.%2520However%252C%2520a%2520fundamental%2520cognitive%2520inconsistency%2520exists%2520between%2520general%2520visual%2520representation%2520and%2520remote%2520sensing%2520image%2520interpretation%253A%2520remote%2520sensing%2520images%2520couple%2520topography%252C%2520terrain%252C%2520and%2520spatial%2520structure%252C%2520thereby%2520inherently%2520requiring%2520models%2520to%2520possess%2520deep%2520geoscientific%2520understanding.%2520This%2520cognitive%2520difference%2520is%2520further%2520amplified%2520in%2520synthetic%2520aperture%2520radar%2520%2528SAR%2529%2520imagery%253A%2520while%2520SAR%2520possesses%2520irreplaceable%2520all-weather%252C%2520all-day%2520observation%2520capabilities%252C%2520it%2520is%2520constrained%2520by%2520coherent%2520imaging%2520mechanisms%252C%2520exhibiting%2520significant%2520modal%2520heterogeneity%2520with%2520general%2520images.%2520To%2520address%2520this%2520inconsistency%252C%2520we%2520propose%2520FUSAR-KLIP%252C%2520the%2520first%2520knowledge-guided%2520general%2520multimodal%2520foundational%2520model%2520for%2520SAR%252C%2520along%2520with%2520reusable%2520data%2520and%2520evaluation%2520baselines.%2520Specifically%253A%2520%25281%2529%2520FUSAR-GEOVL-1M%2520%2528the%2520first%2520large-scale%2520SAR%2520dataset%2520with%2520complete%2520geographic%2520projection%2520attributes%2529%2520was%2520constructed%252C%2520covering%2520multiple%2520satellite%2520platforms%252C%2520120%252C000%2520images%252C%2520and%2520135%2520cities%253B%2520%25282%2529%2520Aligned%2520structured%2520text%2520was%2520generated%2520through%2520hierarchical%2520cognitive%2520thought%2520chains%252C%2520accurately%2520encoding%2520more%2520than%25201%2520million%2520multidimensional%2520semantic%2520information%2520from%2520geomorphological%2520environment%2520and%2520regional%2520attributes%2520to%2520spatial%2520relationships%253B%2520%25283%2529%2520A%2520self-consistent%2520iterative%2520optimization%2520mechanism%2520was%2520designed%2520to%2520guide%2520cross-modal%2520learning%2520with%2520this%2520knowledge%2520information%2520consistent%2520with%2520human%2520cognition%2520and%2520physical%2520laws%2520in%2520a%2520self-supervised%2520closed%2520loop%2520consisting%2520of%2520contrast%252C%2520matching%252C%2520and%2520reconstruction%253B%2520%25284%2529%2520A%2520unified%2520evaluation%2520benchmark%2520was%2520established%2520in%252011%2520typical%2520downstream%2520tasks%2520in%2520the%2520two%2520major%2520categories%2520of%2520vision%2520and%2520language%252C%2520and%2520compared%2520with%252015%2520mainstream%2520foundation%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23927v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FUSAR-KLIP%3A%20Towards%20Multimodal%20Foundation%20Models%20for%20Remote%20Sensing&entry.906535625=Yi%20Yang%20and%20Xiaokun%20Zhang%20and%20Qingchen%20Fang%20and%20Jing%20Liu%20and%20Ziqi%20Ye%20and%20Rui%20Li%20and%20Li%20Liu%20and%20Haipeng%20Wang&entry.1292438233=Cross-modal%20artificial%20intelligence%2C%20represented%20by%20visual%20language%20models%2C%20has%20achieved%20significant%20success%20in%20general%20image%20understanding.%20However%2C%20a%20fundamental%20cognitive%20inconsistency%20exists%20between%20general%20visual%20representation%20and%20remote%20sensing%20image%20interpretation%3A%20remote%20sensing%20images%20couple%20topography%2C%20terrain%2C%20and%20spatial%20structure%2C%20thereby%20inherently%20requiring%20models%20to%20possess%20deep%20geoscientific%20understanding.%20This%20cognitive%20difference%20is%20further%20amplified%20in%20synthetic%20aperture%20radar%20%28SAR%29%20imagery%3A%20while%20SAR%20possesses%20irreplaceable%20all-weather%2C%20all-day%20observation%20capabilities%2C%20it%20is%20constrained%20by%20coherent%20imaging%20mechanisms%2C%20exhibiting%20significant%20modal%20heterogeneity%20with%20general%20images.%20To%20address%20this%20inconsistency%2C%20we%20propose%20FUSAR-KLIP%2C%20the%20first%20knowledge-guided%20general%20multimodal%20foundational%20model%20for%20SAR%2C%20along%20with%20reusable%20data%20and%20evaluation%20baselines.%20Specifically%3A%20%281%29%20FUSAR-GEOVL-1M%20%28the%20first%20large-scale%20SAR%20dataset%20with%20complete%20geographic%20projection%20attributes%29%20was%20constructed%2C%20covering%20multiple%20satellite%20platforms%2C%20120%2C000%20images%2C%20and%20135%20cities%3B%20%282%29%20Aligned%20structured%20text%20was%20generated%20through%20hierarchical%20cognitive%20thought%20chains%2C%20accurately%20encoding%20more%20than%201%20million%20multidimensional%20semantic%20information%20from%20geomorphological%20environment%20and%20regional%20attributes%20to%20spatial%20relationships%3B%20%283%29%20A%20self-consistent%20iterative%20optimization%20mechanism%20was%20designed%20to%20guide%20cross-modal%20learning%20with%20this%20knowledge%20information%20consistent%20with%20human%20cognition%20and%20physical%20laws%20in%20a%20self-supervised%20closed%20loop%20consisting%20of%20contrast%2C%20matching%2C%20and%20reconstruction%3B%20%284%29%20A%20unified%20evaluation%20benchmark%20was%20established%20in%2011%20typical%20downstream%20tasks%20in%20the%20two%20major%20categories%20of%20vision%20and%20language%2C%20and%20compared%20with%2015%20mainstream%20foundation%20models.&entry.1838667208=http%3A//arxiv.org/abs/2509.23927v4&entry.124074799=Read"},
{"title": "Reward-Forcing: Autoregressive Video Generation with Reward Feedback", "author": "Jingran Zhang and Ning Li and Yuanhao Ban and Andrew Bai and Justin Cui", "abstract": "While most prior work in video generation relies on bidirectional architectures, recent efforts have sought to adapt these models into autoregressive variants to support near real-time generation. However, such adaptations often depend heavily on teacher models, which can limit performance, particularly in the absence of a strong autoregressive teacher, resulting in output quality that typically lags behind their bidirectional counterparts. In this paper, we explore an alternative approach that uses reward signals to guide the generation process, enabling more efficient and scalable autoregressive generation. By using reward signals to guide the model, our method simplifies training while preserving high visual fidelity and temporal consistency. Through extensive experiments on standard benchmarks, we find that our approach performs comparably to existing autoregressive models and, in some cases, surpasses similarly sized bidirectional models by avoiding constraints imposed by teacher architectures. For example, on VBench, our method achieves a total score of 84.92, closely matching state-of-the-art autoregressive methods that score 84.31 but require significant heterogeneous distillation.", "link": "http://arxiv.org/abs/2601.16933v1", "date": "2026-01-23", "relevancy": 2.914, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5927}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.581}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reward-Forcing%3A%20Autoregressive%20Video%20Generation%20with%20Reward%20Feedback&body=Title%3A%20Reward-Forcing%3A%20Autoregressive%20Video%20Generation%20with%20Reward%20Feedback%0AAuthor%3A%20Jingran%20Zhang%20and%20Ning%20Li%20and%20Yuanhao%20Ban%20and%20Andrew%20Bai%20and%20Justin%20Cui%0AAbstract%3A%20While%20most%20prior%20work%20in%20video%20generation%20relies%20on%20bidirectional%20architectures%2C%20recent%20efforts%20have%20sought%20to%20adapt%20these%20models%20into%20autoregressive%20variants%20to%20support%20near%20real-time%20generation.%20However%2C%20such%20adaptations%20often%20depend%20heavily%20on%20teacher%20models%2C%20which%20can%20limit%20performance%2C%20particularly%20in%20the%20absence%20of%20a%20strong%20autoregressive%20teacher%2C%20resulting%20in%20output%20quality%20that%20typically%20lags%20behind%20their%20bidirectional%20counterparts.%20In%20this%20paper%2C%20we%20explore%20an%20alternative%20approach%20that%20uses%20reward%20signals%20to%20guide%20the%20generation%20process%2C%20enabling%20more%20efficient%20and%20scalable%20autoregressive%20generation.%20By%20using%20reward%20signals%20to%20guide%20the%20model%2C%20our%20method%20simplifies%20training%20while%20preserving%20high%20visual%20fidelity%20and%20temporal%20consistency.%20Through%20extensive%20experiments%20on%20standard%20benchmarks%2C%20we%20find%20that%20our%20approach%20performs%20comparably%20to%20existing%20autoregressive%20models%20and%2C%20in%20some%20cases%2C%20surpasses%20similarly%20sized%20bidirectional%20models%20by%20avoiding%20constraints%20imposed%20by%20teacher%20architectures.%20For%20example%2C%20on%20VBench%2C%20our%20method%20achieves%20a%20total%20score%20of%2084.92%2C%20closely%20matching%20state-of-the-art%20autoregressive%20methods%20that%20score%2084.31%20but%20require%20significant%20heterogeneous%20distillation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16933v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReward-Forcing%253A%2520Autoregressive%2520Video%2520Generation%2520with%2520Reward%2520Feedback%26entry.906535625%3DJingran%2520Zhang%2520and%2520Ning%2520Li%2520and%2520Yuanhao%2520Ban%2520and%2520Andrew%2520Bai%2520and%2520Justin%2520Cui%26entry.1292438233%3DWhile%2520most%2520prior%2520work%2520in%2520video%2520generation%2520relies%2520on%2520bidirectional%2520architectures%252C%2520recent%2520efforts%2520have%2520sought%2520to%2520adapt%2520these%2520models%2520into%2520autoregressive%2520variants%2520to%2520support%2520near%2520real-time%2520generation.%2520However%252C%2520such%2520adaptations%2520often%2520depend%2520heavily%2520on%2520teacher%2520models%252C%2520which%2520can%2520limit%2520performance%252C%2520particularly%2520in%2520the%2520absence%2520of%2520a%2520strong%2520autoregressive%2520teacher%252C%2520resulting%2520in%2520output%2520quality%2520that%2520typically%2520lags%2520behind%2520their%2520bidirectional%2520counterparts.%2520In%2520this%2520paper%252C%2520we%2520explore%2520an%2520alternative%2520approach%2520that%2520uses%2520reward%2520signals%2520to%2520guide%2520the%2520generation%2520process%252C%2520enabling%2520more%2520efficient%2520and%2520scalable%2520autoregressive%2520generation.%2520By%2520using%2520reward%2520signals%2520to%2520guide%2520the%2520model%252C%2520our%2520method%2520simplifies%2520training%2520while%2520preserving%2520high%2520visual%2520fidelity%2520and%2520temporal%2520consistency.%2520Through%2520extensive%2520experiments%2520on%2520standard%2520benchmarks%252C%2520we%2520find%2520that%2520our%2520approach%2520performs%2520comparably%2520to%2520existing%2520autoregressive%2520models%2520and%252C%2520in%2520some%2520cases%252C%2520surpasses%2520similarly%2520sized%2520bidirectional%2520models%2520by%2520avoiding%2520constraints%2520imposed%2520by%2520teacher%2520architectures.%2520For%2520example%252C%2520on%2520VBench%252C%2520our%2520method%2520achieves%2520a%2520total%2520score%2520of%252084.92%252C%2520closely%2520matching%2520state-of-the-art%2520autoregressive%2520methods%2520that%2520score%252084.31%2520but%2520require%2520significant%2520heterogeneous%2520distillation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16933v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reward-Forcing%3A%20Autoregressive%20Video%20Generation%20with%20Reward%20Feedback&entry.906535625=Jingran%20Zhang%20and%20Ning%20Li%20and%20Yuanhao%20Ban%20and%20Andrew%20Bai%20and%20Justin%20Cui&entry.1292438233=While%20most%20prior%20work%20in%20video%20generation%20relies%20on%20bidirectional%20architectures%2C%20recent%20efforts%20have%20sought%20to%20adapt%20these%20models%20into%20autoregressive%20variants%20to%20support%20near%20real-time%20generation.%20However%2C%20such%20adaptations%20often%20depend%20heavily%20on%20teacher%20models%2C%20which%20can%20limit%20performance%2C%20particularly%20in%20the%20absence%20of%20a%20strong%20autoregressive%20teacher%2C%20resulting%20in%20output%20quality%20that%20typically%20lags%20behind%20their%20bidirectional%20counterparts.%20In%20this%20paper%2C%20we%20explore%20an%20alternative%20approach%20that%20uses%20reward%20signals%20to%20guide%20the%20generation%20process%2C%20enabling%20more%20efficient%20and%20scalable%20autoregressive%20generation.%20By%20using%20reward%20signals%20to%20guide%20the%20model%2C%20our%20method%20simplifies%20training%20while%20preserving%20high%20visual%20fidelity%20and%20temporal%20consistency.%20Through%20extensive%20experiments%20on%20standard%20benchmarks%2C%20we%20find%20that%20our%20approach%20performs%20comparably%20to%20existing%20autoregressive%20models%20and%2C%20in%20some%20cases%2C%20surpasses%20similarly%20sized%20bidirectional%20models%20by%20avoiding%20constraints%20imposed%20by%20teacher%20architectures.%20For%20example%2C%20on%20VBench%2C%20our%20method%20achieves%20a%20total%20score%20of%2084.92%2C%20closely%20matching%20state-of-the-art%20autoregressive%20methods%20that%20score%2084.31%20but%20require%20significant%20heterogeneous%20distillation.&entry.1838667208=http%3A//arxiv.org/abs/2601.16933v1&entry.124074799=Read"},
{"title": "Affinity Contrastive Learning for Skeleton-based Human Activity Understanding", "author": "Hongda Liu and Yunfan Liu and Min Ren and Lin Sui and Yunlong Wang and Zhenan Sun", "abstract": "In skeleton-based human activity understanding, existing methods often adopt the contrastive learning paradigm to construct a discriminative feature space. However, many of these approaches fail to exploit the structural inter-class similarities and overlook the impact of anomalous positive samples. In this study, we introduce ACLNet, an Affinity Contrastive Learning Network that explores the intricate clustering relationships among human activity classes to improve feature discrimination. Specifically, we propose an affinity metric to refine similarity measurements, thereby forming activity superclasses that provide more informative contrastive signals. A dynamic temperature schedule is also introduced to adaptively adjust the penalty strength for various superclasses. In addition, we employ a margin-based contrastive strategy to improve the separation of hard positive and negative samples within classes. Extensive experiments on NTU RGB+D 60, NTU RGB+D 120, Kinetics-Skeleton, PKU-MMD, FineGYM, and CASIA-B demonstrate the superiority of our method in skeleton-based action recognition, gait recognition, and person re-identification. The source code is available at https://github.com/firework8/ACLNet.", "link": "http://arxiv.org/abs/2601.16694v1", "date": "2026-01-23", "relevancy": 2.7683, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5822}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5434}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Affinity%20Contrastive%20Learning%20for%20Skeleton-based%20Human%20Activity%20Understanding&body=Title%3A%20Affinity%20Contrastive%20Learning%20for%20Skeleton-based%20Human%20Activity%20Understanding%0AAuthor%3A%20Hongda%20Liu%20and%20Yunfan%20Liu%20and%20Min%20Ren%20and%20Lin%20Sui%20and%20Yunlong%20Wang%20and%20Zhenan%20Sun%0AAbstract%3A%20In%20skeleton-based%20human%20activity%20understanding%2C%20existing%20methods%20often%20adopt%20the%20contrastive%20learning%20paradigm%20to%20construct%20a%20discriminative%20feature%20space.%20However%2C%20many%20of%20these%20approaches%20fail%20to%20exploit%20the%20structural%20inter-class%20similarities%20and%20overlook%20the%20impact%20of%20anomalous%20positive%20samples.%20In%20this%20study%2C%20we%20introduce%20ACLNet%2C%20an%20Affinity%20Contrastive%20Learning%20Network%20that%20explores%20the%20intricate%20clustering%20relationships%20among%20human%20activity%20classes%20to%20improve%20feature%20discrimination.%20Specifically%2C%20we%20propose%20an%20affinity%20metric%20to%20refine%20similarity%20measurements%2C%20thereby%20forming%20activity%20superclasses%20that%20provide%20more%20informative%20contrastive%20signals.%20A%20dynamic%20temperature%20schedule%20is%20also%20introduced%20to%20adaptively%20adjust%20the%20penalty%20strength%20for%20various%20superclasses.%20In%20addition%2C%20we%20employ%20a%20margin-based%20contrastive%20strategy%20to%20improve%20the%20separation%20of%20hard%20positive%20and%20negative%20samples%20within%20classes.%20Extensive%20experiments%20on%20NTU%20RGB%2BD%2060%2C%20NTU%20RGB%2BD%20120%2C%20Kinetics-Skeleton%2C%20PKU-MMD%2C%20FineGYM%2C%20and%20CASIA-B%20demonstrate%20the%20superiority%20of%20our%20method%20in%20skeleton-based%20action%20recognition%2C%20gait%20recognition%2C%20and%20person%20re-identification.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/firework8/ACLNet.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAffinity%2520Contrastive%2520Learning%2520for%2520Skeleton-based%2520Human%2520Activity%2520Understanding%26entry.906535625%3DHongda%2520Liu%2520and%2520Yunfan%2520Liu%2520and%2520Min%2520Ren%2520and%2520Lin%2520Sui%2520and%2520Yunlong%2520Wang%2520and%2520Zhenan%2520Sun%26entry.1292438233%3DIn%2520skeleton-based%2520human%2520activity%2520understanding%252C%2520existing%2520methods%2520often%2520adopt%2520the%2520contrastive%2520learning%2520paradigm%2520to%2520construct%2520a%2520discriminative%2520feature%2520space.%2520However%252C%2520many%2520of%2520these%2520approaches%2520fail%2520to%2520exploit%2520the%2520structural%2520inter-class%2520similarities%2520and%2520overlook%2520the%2520impact%2520of%2520anomalous%2520positive%2520samples.%2520In%2520this%2520study%252C%2520we%2520introduce%2520ACLNet%252C%2520an%2520Affinity%2520Contrastive%2520Learning%2520Network%2520that%2520explores%2520the%2520intricate%2520clustering%2520relationships%2520among%2520human%2520activity%2520classes%2520to%2520improve%2520feature%2520discrimination.%2520Specifically%252C%2520we%2520propose%2520an%2520affinity%2520metric%2520to%2520refine%2520similarity%2520measurements%252C%2520thereby%2520forming%2520activity%2520superclasses%2520that%2520provide%2520more%2520informative%2520contrastive%2520signals.%2520A%2520dynamic%2520temperature%2520schedule%2520is%2520also%2520introduced%2520to%2520adaptively%2520adjust%2520the%2520penalty%2520strength%2520for%2520various%2520superclasses.%2520In%2520addition%252C%2520we%2520employ%2520a%2520margin-based%2520contrastive%2520strategy%2520to%2520improve%2520the%2520separation%2520of%2520hard%2520positive%2520and%2520negative%2520samples%2520within%2520classes.%2520Extensive%2520experiments%2520on%2520NTU%2520RGB%252BD%252060%252C%2520NTU%2520RGB%252BD%2520120%252C%2520Kinetics-Skeleton%252C%2520PKU-MMD%252C%2520FineGYM%252C%2520and%2520CASIA-B%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method%2520in%2520skeleton-based%2520action%2520recognition%252C%2520gait%2520recognition%252C%2520and%2520person%2520re-identification.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/firework8/ACLNet.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Affinity%20Contrastive%20Learning%20for%20Skeleton-based%20Human%20Activity%20Understanding&entry.906535625=Hongda%20Liu%20and%20Yunfan%20Liu%20and%20Min%20Ren%20and%20Lin%20Sui%20and%20Yunlong%20Wang%20and%20Zhenan%20Sun&entry.1292438233=In%20skeleton-based%20human%20activity%20understanding%2C%20existing%20methods%20often%20adopt%20the%20contrastive%20learning%20paradigm%20to%20construct%20a%20discriminative%20feature%20space.%20However%2C%20many%20of%20these%20approaches%20fail%20to%20exploit%20the%20structural%20inter-class%20similarities%20and%20overlook%20the%20impact%20of%20anomalous%20positive%20samples.%20In%20this%20study%2C%20we%20introduce%20ACLNet%2C%20an%20Affinity%20Contrastive%20Learning%20Network%20that%20explores%20the%20intricate%20clustering%20relationships%20among%20human%20activity%20classes%20to%20improve%20feature%20discrimination.%20Specifically%2C%20we%20propose%20an%20affinity%20metric%20to%20refine%20similarity%20measurements%2C%20thereby%20forming%20activity%20superclasses%20that%20provide%20more%20informative%20contrastive%20signals.%20A%20dynamic%20temperature%20schedule%20is%20also%20introduced%20to%20adaptively%20adjust%20the%20penalty%20strength%20for%20various%20superclasses.%20In%20addition%2C%20we%20employ%20a%20margin-based%20contrastive%20strategy%20to%20improve%20the%20separation%20of%20hard%20positive%20and%20negative%20samples%20within%20classes.%20Extensive%20experiments%20on%20NTU%20RGB%2BD%2060%2C%20NTU%20RGB%2BD%20120%2C%20Kinetics-Skeleton%2C%20PKU-MMD%2C%20FineGYM%2C%20and%20CASIA-B%20demonstrate%20the%20superiority%20of%20our%20method%20in%20skeleton-based%20action%20recognition%2C%20gait%20recognition%2C%20and%20person%20re-identification.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/firework8/ACLNet.&entry.1838667208=http%3A//arxiv.org/abs/2601.16694v1&entry.124074799=Read"},
{"title": "PanopMamba: Vision State Space Modeling for Nuclei Panoptic Segmentation", "author": "Ming Kang and Fung Fung Ting and Rapha\u00ebl C. -W. Phan and Zongyuan Ge and Chee-Ming Ting", "abstract": "Nuclei panoptic segmentation supports cancer diagnostics by integrating both semantic and instance segmentation of different cell types to analyze overall tissue structure and individual nuclei in histopathology images. Major challenges include detecting small objects, handling ambiguous boundaries, and addressing class imbalance. To address these issues, we propose PanopMamba, a novel hybrid encoder-decoder architecture that integrates Mamba and Transformer with additional feature-enhanced fusion via state space modeling. We design a multiscale Mamba backbone and a State Space Model (SSM)-based fusion network to enable efficient long-range perception in pyramid features, thereby extending the pure encoder-decoder framework while facilitating information sharing across multiscale features of nuclei. The proposed SSM-based feature-enhanced fusion integrates pyramid feature networks and dynamic feature enhancement across different spatial scales, enhancing the feature representation of densely overlapping nuclei in both semantic and spatial dimensions. To the best of our knowledge, this is the first Mamba-based approach for panoptic segmentation. Additionally, we introduce alternative evaluation metrics, including image-level Panoptic Quality ($i$PQ), boundary-weighted PQ ($w$PQ), and frequency-weighted PQ ($fw$PQ), which are specifically designed to address the unique challenges of nuclei segmentation and thereby mitigate the potential bias inherent in vanilla PQ. Experimental evaluations on two multiclass nuclei segmentation benchmark datasets, MoNuSAC2020 and NuInsSeg, demonstrate the superiority of PanopMamba for nuclei panoptic segmentation over state-of-the-art methods. Consequently, the robustness of PanopMamba is validated across various metrics, while the distinctiveness of PQ variants is also demonstrated. Code is available at https://github.com/mkang315/PanopMamba.", "link": "http://arxiv.org/abs/2601.16631v1", "date": "2026-01-23", "relevancy": 2.75, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5901}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.53}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.53}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PanopMamba%3A%20Vision%20State%20Space%20Modeling%20for%20Nuclei%20Panoptic%20Segmentation&body=Title%3A%20PanopMamba%3A%20Vision%20State%20Space%20Modeling%20for%20Nuclei%20Panoptic%20Segmentation%0AAuthor%3A%20Ming%20Kang%20and%20Fung%20Fung%20Ting%20and%20Rapha%C3%ABl%20C.%20-W.%20Phan%20and%20Zongyuan%20Ge%20and%20Chee-Ming%20Ting%0AAbstract%3A%20Nuclei%20panoptic%20segmentation%20supports%20cancer%20diagnostics%20by%20integrating%20both%20semantic%20and%20instance%20segmentation%20of%20different%20cell%20types%20to%20analyze%20overall%20tissue%20structure%20and%20individual%20nuclei%20in%20histopathology%20images.%20Major%20challenges%20include%20detecting%20small%20objects%2C%20handling%20ambiguous%20boundaries%2C%20and%20addressing%20class%20imbalance.%20To%20address%20these%20issues%2C%20we%20propose%20PanopMamba%2C%20a%20novel%20hybrid%20encoder-decoder%20architecture%20that%20integrates%20Mamba%20and%20Transformer%20with%20additional%20feature-enhanced%20fusion%20via%20state%20space%20modeling.%20We%20design%20a%20multiscale%20Mamba%20backbone%20and%20a%20State%20Space%20Model%20%28SSM%29-based%20fusion%20network%20to%20enable%20efficient%20long-range%20perception%20in%20pyramid%20features%2C%20thereby%20extending%20the%20pure%20encoder-decoder%20framework%20while%20facilitating%20information%20sharing%20across%20multiscale%20features%20of%20nuclei.%20The%20proposed%20SSM-based%20feature-enhanced%20fusion%20integrates%20pyramid%20feature%20networks%20and%20dynamic%20feature%20enhancement%20across%20different%20spatial%20scales%2C%20enhancing%20the%20feature%20representation%20of%20densely%20overlapping%20nuclei%20in%20both%20semantic%20and%20spatial%20dimensions.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20Mamba-based%20approach%20for%20panoptic%20segmentation.%20Additionally%2C%20we%20introduce%20alternative%20evaluation%20metrics%2C%20including%20image-level%20Panoptic%20Quality%20%28%24i%24PQ%29%2C%20boundary-weighted%20PQ%20%28%24w%24PQ%29%2C%20and%20frequency-weighted%20PQ%20%28%24fw%24PQ%29%2C%20which%20are%20specifically%20designed%20to%20address%20the%20unique%20challenges%20of%20nuclei%20segmentation%20and%20thereby%20mitigate%20the%20potential%20bias%20inherent%20in%20vanilla%20PQ.%20Experimental%20evaluations%20on%20two%20multiclass%20nuclei%20segmentation%20benchmark%20datasets%2C%20MoNuSAC2020%20and%20NuInsSeg%2C%20demonstrate%20the%20superiority%20of%20PanopMamba%20for%20nuclei%20panoptic%20segmentation%20over%20state-of-the-art%20methods.%20Consequently%2C%20the%20robustness%20of%20PanopMamba%20is%20validated%20across%20various%20metrics%2C%20while%20the%20distinctiveness%20of%20PQ%20variants%20is%20also%20demonstrated.%20Code%20is%20available%20at%20https%3A//github.com/mkang315/PanopMamba.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanopMamba%253A%2520Vision%2520State%2520Space%2520Modeling%2520for%2520Nuclei%2520Panoptic%2520Segmentation%26entry.906535625%3DMing%2520Kang%2520and%2520Fung%2520Fung%2520Ting%2520and%2520Rapha%25C3%25ABl%2520C.%2520-W.%2520Phan%2520and%2520Zongyuan%2520Ge%2520and%2520Chee-Ming%2520Ting%26entry.1292438233%3DNuclei%2520panoptic%2520segmentation%2520supports%2520cancer%2520diagnostics%2520by%2520integrating%2520both%2520semantic%2520and%2520instance%2520segmentation%2520of%2520different%2520cell%2520types%2520to%2520analyze%2520overall%2520tissue%2520structure%2520and%2520individual%2520nuclei%2520in%2520histopathology%2520images.%2520Major%2520challenges%2520include%2520detecting%2520small%2520objects%252C%2520handling%2520ambiguous%2520boundaries%252C%2520and%2520addressing%2520class%2520imbalance.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520PanopMamba%252C%2520a%2520novel%2520hybrid%2520encoder-decoder%2520architecture%2520that%2520integrates%2520Mamba%2520and%2520Transformer%2520with%2520additional%2520feature-enhanced%2520fusion%2520via%2520state%2520space%2520modeling.%2520We%2520design%2520a%2520multiscale%2520Mamba%2520backbone%2520and%2520a%2520State%2520Space%2520Model%2520%2528SSM%2529-based%2520fusion%2520network%2520to%2520enable%2520efficient%2520long-range%2520perception%2520in%2520pyramid%2520features%252C%2520thereby%2520extending%2520the%2520pure%2520encoder-decoder%2520framework%2520while%2520facilitating%2520information%2520sharing%2520across%2520multiscale%2520features%2520of%2520nuclei.%2520The%2520proposed%2520SSM-based%2520feature-enhanced%2520fusion%2520integrates%2520pyramid%2520feature%2520networks%2520and%2520dynamic%2520feature%2520enhancement%2520across%2520different%2520spatial%2520scales%252C%2520enhancing%2520the%2520feature%2520representation%2520of%2520densely%2520overlapping%2520nuclei%2520in%2520both%2520semantic%2520and%2520spatial%2520dimensions.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520Mamba-based%2520approach%2520for%2520panoptic%2520segmentation.%2520Additionally%252C%2520we%2520introduce%2520alternative%2520evaluation%2520metrics%252C%2520including%2520image-level%2520Panoptic%2520Quality%2520%2528%2524i%2524PQ%2529%252C%2520boundary-weighted%2520PQ%2520%2528%2524w%2524PQ%2529%252C%2520and%2520frequency-weighted%2520PQ%2520%2528%2524fw%2524PQ%2529%252C%2520which%2520are%2520specifically%2520designed%2520to%2520address%2520the%2520unique%2520challenges%2520of%2520nuclei%2520segmentation%2520and%2520thereby%2520mitigate%2520the%2520potential%2520bias%2520inherent%2520in%2520vanilla%2520PQ.%2520Experimental%2520evaluations%2520on%2520two%2520multiclass%2520nuclei%2520segmentation%2520benchmark%2520datasets%252C%2520MoNuSAC2020%2520and%2520NuInsSeg%252C%2520demonstrate%2520the%2520superiority%2520of%2520PanopMamba%2520for%2520nuclei%2520panoptic%2520segmentation%2520over%2520state-of-the-art%2520methods.%2520Consequently%252C%2520the%2520robustness%2520of%2520PanopMamba%2520is%2520validated%2520across%2520various%2520metrics%252C%2520while%2520the%2520distinctiveness%2520of%2520PQ%2520variants%2520is%2520also%2520demonstrated.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/mkang315/PanopMamba.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PanopMamba%3A%20Vision%20State%20Space%20Modeling%20for%20Nuclei%20Panoptic%20Segmentation&entry.906535625=Ming%20Kang%20and%20Fung%20Fung%20Ting%20and%20Rapha%C3%ABl%20C.%20-W.%20Phan%20and%20Zongyuan%20Ge%20and%20Chee-Ming%20Ting&entry.1292438233=Nuclei%20panoptic%20segmentation%20supports%20cancer%20diagnostics%20by%20integrating%20both%20semantic%20and%20instance%20segmentation%20of%20different%20cell%20types%20to%20analyze%20overall%20tissue%20structure%20and%20individual%20nuclei%20in%20histopathology%20images.%20Major%20challenges%20include%20detecting%20small%20objects%2C%20handling%20ambiguous%20boundaries%2C%20and%20addressing%20class%20imbalance.%20To%20address%20these%20issues%2C%20we%20propose%20PanopMamba%2C%20a%20novel%20hybrid%20encoder-decoder%20architecture%20that%20integrates%20Mamba%20and%20Transformer%20with%20additional%20feature-enhanced%20fusion%20via%20state%20space%20modeling.%20We%20design%20a%20multiscale%20Mamba%20backbone%20and%20a%20State%20Space%20Model%20%28SSM%29-based%20fusion%20network%20to%20enable%20efficient%20long-range%20perception%20in%20pyramid%20features%2C%20thereby%20extending%20the%20pure%20encoder-decoder%20framework%20while%20facilitating%20information%20sharing%20across%20multiscale%20features%20of%20nuclei.%20The%20proposed%20SSM-based%20feature-enhanced%20fusion%20integrates%20pyramid%20feature%20networks%20and%20dynamic%20feature%20enhancement%20across%20different%20spatial%20scales%2C%20enhancing%20the%20feature%20representation%20of%20densely%20overlapping%20nuclei%20in%20both%20semantic%20and%20spatial%20dimensions.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20Mamba-based%20approach%20for%20panoptic%20segmentation.%20Additionally%2C%20we%20introduce%20alternative%20evaluation%20metrics%2C%20including%20image-level%20Panoptic%20Quality%20%28%24i%24PQ%29%2C%20boundary-weighted%20PQ%20%28%24w%24PQ%29%2C%20and%20frequency-weighted%20PQ%20%28%24fw%24PQ%29%2C%20which%20are%20specifically%20designed%20to%20address%20the%20unique%20challenges%20of%20nuclei%20segmentation%20and%20thereby%20mitigate%20the%20potential%20bias%20inherent%20in%20vanilla%20PQ.%20Experimental%20evaluations%20on%20two%20multiclass%20nuclei%20segmentation%20benchmark%20datasets%2C%20MoNuSAC2020%20and%20NuInsSeg%2C%20demonstrate%20the%20superiority%20of%20PanopMamba%20for%20nuclei%20panoptic%20segmentation%20over%20state-of-the-art%20methods.%20Consequently%2C%20the%20robustness%20of%20PanopMamba%20is%20validated%20across%20various%20metrics%2C%20while%20the%20distinctiveness%20of%20PQ%20variants%20is%20also%20demonstrated.%20Code%20is%20available%20at%20https%3A//github.com/mkang315/PanopMamba.&entry.1838667208=http%3A//arxiv.org/abs/2601.16631v1&entry.124074799=Read"},
{"title": "Boundary and Position Information Mining for Aerial Small Object Detection", "author": "Rongxin Huang and Guangfeng Lin and Wenbo Zhou and Zhirong Li and Wenhuan Wu", "abstract": "Unmanned Aerial Vehicle (UAV) applications have become increasingly prevalent in aerial photography and object recognition. However, there are major challenges to accurately capturing small targets in object detection due to the imbalanced scale and the blurred edges. To address these issues, boundary and position information mining (BPIM) framework is proposed for capturing object edge and location cues. The proposed BPIM includes position information guidance (PIG) module for obtaining location information, boundary information guidance (BIG) module for extracting object edge, cross scale fusion (CSF) module for gradually assembling the shallow layer image feature, three feature fusion (TFF) module for progressively combining position and boundary information, and adaptive weight fusion (AWF) module for flexibly merging the deep layer semantic feature. Therefore, BPIM can integrate boundary, position, and scale information in image for small object detection using attention mechanisms and cross-scale feature fusion strategies. Furthermore, BPIM not only improves the discrimination of the contextual feature by adaptive weight fusion with boundary, but also enhances small object perceptions by cross-scale position fusion. On the VisDrone2021, DOTA1.0, and WiderPerson datasets, experimental results show the better performances of BPIM compared to the baseline Yolov5-P2, and obtains the promising performance in the state-of-the-art methods with comparable computation load.", "link": "http://arxiv.org/abs/2601.16617v1", "date": "2026-01-23", "relevancy": 2.7082, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5547}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5413}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boundary%20and%20Position%20Information%20Mining%20for%20Aerial%20Small%20Object%20Detection&body=Title%3A%20Boundary%20and%20Position%20Information%20Mining%20for%20Aerial%20Small%20Object%20Detection%0AAuthor%3A%20Rongxin%20Huang%20and%20Guangfeng%20Lin%20and%20Wenbo%20Zhou%20and%20Zhirong%20Li%20and%20Wenhuan%20Wu%0AAbstract%3A%20Unmanned%20Aerial%20Vehicle%20%28UAV%29%20applications%20have%20become%20increasingly%20prevalent%20in%20aerial%20photography%20and%20object%20recognition.%20However%2C%20there%20are%20major%20challenges%20to%20accurately%20capturing%20small%20targets%20in%20object%20detection%20due%20to%20the%20imbalanced%20scale%20and%20the%20blurred%20edges.%20To%20address%20these%20issues%2C%20boundary%20and%20position%20information%20mining%20%28BPIM%29%20framework%20is%20proposed%20for%20capturing%20object%20edge%20and%20location%20cues.%20The%20proposed%20BPIM%20includes%20position%20information%20guidance%20%28PIG%29%20module%20for%20obtaining%20location%20information%2C%20boundary%20information%20guidance%20%28BIG%29%20module%20for%20extracting%20object%20edge%2C%20cross%20scale%20fusion%20%28CSF%29%20module%20for%20gradually%20assembling%20the%20shallow%20layer%20image%20feature%2C%20three%20feature%20fusion%20%28TFF%29%20module%20for%20progressively%20combining%20position%20and%20boundary%20information%2C%20and%20adaptive%20weight%20fusion%20%28AWF%29%20module%20for%20flexibly%20merging%20the%20deep%20layer%20semantic%20feature.%20Therefore%2C%20BPIM%20can%20integrate%20boundary%2C%20position%2C%20and%20scale%20information%20in%20image%20for%20small%20object%20detection%20using%20attention%20mechanisms%20and%20cross-scale%20feature%20fusion%20strategies.%20Furthermore%2C%20BPIM%20not%20only%20improves%20the%20discrimination%20of%20the%20contextual%20feature%20by%20adaptive%20weight%20fusion%20with%20boundary%2C%20but%20also%20enhances%20small%20object%20perceptions%20by%20cross-scale%20position%20fusion.%20On%20the%20VisDrone2021%2C%20DOTA1.0%2C%20and%20WiderPerson%20datasets%2C%20experimental%20results%20show%20the%20better%20performances%20of%20BPIM%20compared%20to%20the%20baseline%20Yolov5-P2%2C%20and%20obtains%20the%20promising%20performance%20in%20the%20state-of-the-art%20methods%20with%20comparable%20computation%20load.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoundary%2520and%2520Position%2520Information%2520Mining%2520for%2520Aerial%2520Small%2520Object%2520Detection%26entry.906535625%3DRongxin%2520Huang%2520and%2520Guangfeng%2520Lin%2520and%2520Wenbo%2520Zhou%2520and%2520Zhirong%2520Li%2520and%2520Wenhuan%2520Wu%26entry.1292438233%3DUnmanned%2520Aerial%2520Vehicle%2520%2528UAV%2529%2520applications%2520have%2520become%2520increasingly%2520prevalent%2520in%2520aerial%2520photography%2520and%2520object%2520recognition.%2520However%252C%2520there%2520are%2520major%2520challenges%2520to%2520accurately%2520capturing%2520small%2520targets%2520in%2520object%2520detection%2520due%2520to%2520the%2520imbalanced%2520scale%2520and%2520the%2520blurred%2520edges.%2520To%2520address%2520these%2520issues%252C%2520boundary%2520and%2520position%2520information%2520mining%2520%2528BPIM%2529%2520framework%2520is%2520proposed%2520for%2520capturing%2520object%2520edge%2520and%2520location%2520cues.%2520The%2520proposed%2520BPIM%2520includes%2520position%2520information%2520guidance%2520%2528PIG%2529%2520module%2520for%2520obtaining%2520location%2520information%252C%2520boundary%2520information%2520guidance%2520%2528BIG%2529%2520module%2520for%2520extracting%2520object%2520edge%252C%2520cross%2520scale%2520fusion%2520%2528CSF%2529%2520module%2520for%2520gradually%2520assembling%2520the%2520shallow%2520layer%2520image%2520feature%252C%2520three%2520feature%2520fusion%2520%2528TFF%2529%2520module%2520for%2520progressively%2520combining%2520position%2520and%2520boundary%2520information%252C%2520and%2520adaptive%2520weight%2520fusion%2520%2528AWF%2529%2520module%2520for%2520flexibly%2520merging%2520the%2520deep%2520layer%2520semantic%2520feature.%2520Therefore%252C%2520BPIM%2520can%2520integrate%2520boundary%252C%2520position%252C%2520and%2520scale%2520information%2520in%2520image%2520for%2520small%2520object%2520detection%2520using%2520attention%2520mechanisms%2520and%2520cross-scale%2520feature%2520fusion%2520strategies.%2520Furthermore%252C%2520BPIM%2520not%2520only%2520improves%2520the%2520discrimination%2520of%2520the%2520contextual%2520feature%2520by%2520adaptive%2520weight%2520fusion%2520with%2520boundary%252C%2520but%2520also%2520enhances%2520small%2520object%2520perceptions%2520by%2520cross-scale%2520position%2520fusion.%2520On%2520the%2520VisDrone2021%252C%2520DOTA1.0%252C%2520and%2520WiderPerson%2520datasets%252C%2520experimental%2520results%2520show%2520the%2520better%2520performances%2520of%2520BPIM%2520compared%2520to%2520the%2520baseline%2520Yolov5-P2%252C%2520and%2520obtains%2520the%2520promising%2520performance%2520in%2520the%2520state-of-the-art%2520methods%2520with%2520comparable%2520computation%2520load.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boundary%20and%20Position%20Information%20Mining%20for%20Aerial%20Small%20Object%20Detection&entry.906535625=Rongxin%20Huang%20and%20Guangfeng%20Lin%20and%20Wenbo%20Zhou%20and%20Zhirong%20Li%20and%20Wenhuan%20Wu&entry.1292438233=Unmanned%20Aerial%20Vehicle%20%28UAV%29%20applications%20have%20become%20increasingly%20prevalent%20in%20aerial%20photography%20and%20object%20recognition.%20However%2C%20there%20are%20major%20challenges%20to%20accurately%20capturing%20small%20targets%20in%20object%20detection%20due%20to%20the%20imbalanced%20scale%20and%20the%20blurred%20edges.%20To%20address%20these%20issues%2C%20boundary%20and%20position%20information%20mining%20%28BPIM%29%20framework%20is%20proposed%20for%20capturing%20object%20edge%20and%20location%20cues.%20The%20proposed%20BPIM%20includes%20position%20information%20guidance%20%28PIG%29%20module%20for%20obtaining%20location%20information%2C%20boundary%20information%20guidance%20%28BIG%29%20module%20for%20extracting%20object%20edge%2C%20cross%20scale%20fusion%20%28CSF%29%20module%20for%20gradually%20assembling%20the%20shallow%20layer%20image%20feature%2C%20three%20feature%20fusion%20%28TFF%29%20module%20for%20progressively%20combining%20position%20and%20boundary%20information%2C%20and%20adaptive%20weight%20fusion%20%28AWF%29%20module%20for%20flexibly%20merging%20the%20deep%20layer%20semantic%20feature.%20Therefore%2C%20BPIM%20can%20integrate%20boundary%2C%20position%2C%20and%20scale%20information%20in%20image%20for%20small%20object%20detection%20using%20attention%20mechanisms%20and%20cross-scale%20feature%20fusion%20strategies.%20Furthermore%2C%20BPIM%20not%20only%20improves%20the%20discrimination%20of%20the%20contextual%20feature%20by%20adaptive%20weight%20fusion%20with%20boundary%2C%20but%20also%20enhances%20small%20object%20perceptions%20by%20cross-scale%20position%20fusion.%20On%20the%20VisDrone2021%2C%20DOTA1.0%2C%20and%20WiderPerson%20datasets%2C%20experimental%20results%20show%20the%20better%20performances%20of%20BPIM%20compared%20to%20the%20baseline%20Yolov5-P2%2C%20and%20obtains%20the%20promising%20performance%20in%20the%20state-of-the-art%20methods%20with%20comparable%20computation%20load.&entry.1838667208=http%3A//arxiv.org/abs/2601.16617v1&entry.124074799=Read"},
{"title": "Data Matters Most: Auditing Social Bias in Contrastive Vision Language Models", "author": "Zahraa Al Sahili and Ioannis Patras and Matthew Purver", "abstract": "Vision-language models (VLMs) deliver strong zero-shot recognition but frequently inherit social biases from their training data. We systematically disentangle three design factors -- model size, training-data scale, and training-data source -- by comparing CLIP and OpenCLIP, two models that share an identical contrastive objective yet differ in encoder width and in the image-text corpora on which they are pre-trained (400M proprietary pairs vs. 400M/2B LAION). Across balanced face-analysis benchmarks, enlarging the encoder reduces gender skew in CLIP but amplifies both gender and racial skew in OpenCLIP; increasing the LAION corpus from 400M to 2B further increases OpenCLIP bias. At matched model and data budgets, substituting proprietary data with LAION improves gender fairness while increasing racial skew, underscoring data source as the primary driver of bias patterns. We also evaluate three post-hoc, test-time debiasing strategies -- Bias Prompts, Prompt Array, and SANER. Debiasing reduces but does not eliminate harm, and its effectiveness is source- and size-dependent: Bias Prompts most effectively reduce gender skew in CLIP at smaller model sizes, whereas Prompt Array and SANER more reliably reduce racial skew in OpenCLIP; scaling LAION reconfigures which method is most fair. Taken together, these findings challenge the assumption that bigger models or datasets are automatically fairer and foreground training data source as the key determinant of both bias and mitigation efficacy. We release code and evaluation scripts to enable transparent, reproducible auditing of future VLMs.", "link": "http://arxiv.org/abs/2501.13223v7", "date": "2026-01-23", "relevancy": 2.6997, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5449}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5449}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Matters%20Most%3A%20Auditing%20Social%20Bias%20in%20Contrastive%20Vision%20Language%20Models&body=Title%3A%20Data%20Matters%20Most%3A%20Auditing%20Social%20Bias%20in%20Contrastive%20Vision%20Language%20Models%0AAuthor%3A%20Zahraa%20Al%20Sahili%20and%20Ioannis%20Patras%20and%20Matthew%20Purver%0AAbstract%3A%20Vision-language%20models%20%28VLMs%29%20deliver%20strong%20zero-shot%20recognition%20but%20frequently%20inherit%20social%20biases%20from%20their%20training%20data.%20We%20systematically%20disentangle%20three%20design%20factors%20--%20model%20size%2C%20training-data%20scale%2C%20and%20training-data%20source%20--%20by%20comparing%20CLIP%20and%20OpenCLIP%2C%20two%20models%20that%20share%20an%20identical%20contrastive%20objective%20yet%20differ%20in%20encoder%20width%20and%20in%20the%20image-text%20corpora%20on%20which%20they%20are%20pre-trained%20%28400M%20proprietary%20pairs%20vs.%20400M/2B%20LAION%29.%20Across%20balanced%20face-analysis%20benchmarks%2C%20enlarging%20the%20encoder%20reduces%20gender%20skew%20in%20CLIP%20but%20amplifies%20both%20gender%20and%20racial%20skew%20in%20OpenCLIP%3B%20increasing%20the%20LAION%20corpus%20from%20400M%20to%202B%20further%20increases%20OpenCLIP%20bias.%20At%20matched%20model%20and%20data%20budgets%2C%20substituting%20proprietary%20data%20with%20LAION%20improves%20gender%20fairness%20while%20increasing%20racial%20skew%2C%20underscoring%20data%20source%20as%20the%20primary%20driver%20of%20bias%20patterns.%20We%20also%20evaluate%20three%20post-hoc%2C%20test-time%20debiasing%20strategies%20--%20Bias%20Prompts%2C%20Prompt%20Array%2C%20and%20SANER.%20Debiasing%20reduces%20but%20does%20not%20eliminate%20harm%2C%20and%20its%20effectiveness%20is%20source-%20and%20size-dependent%3A%20Bias%20Prompts%20most%20effectively%20reduce%20gender%20skew%20in%20CLIP%20at%20smaller%20model%20sizes%2C%20whereas%20Prompt%20Array%20and%20SANER%20more%20reliably%20reduce%20racial%20skew%20in%20OpenCLIP%3B%20scaling%20LAION%20reconfigures%20which%20method%20is%20most%20fair.%20Taken%20together%2C%20these%20findings%20challenge%20the%20assumption%20that%20bigger%20models%20or%20datasets%20are%20automatically%20fairer%20and%20foreground%20training%20data%20source%20as%20the%20key%20determinant%20of%20both%20bias%20and%20mitigation%20efficacy.%20We%20release%20code%20and%20evaluation%20scripts%20to%20enable%20transparent%2C%20reproducible%20auditing%20of%20future%20VLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2501.13223v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Matters%2520Most%253A%2520Auditing%2520Social%2520Bias%2520in%2520Contrastive%2520Vision%2520Language%2520Models%26entry.906535625%3DZahraa%2520Al%2520Sahili%2520and%2520Ioannis%2520Patras%2520and%2520Matthew%2520Purver%26entry.1292438233%3DVision-language%2520models%2520%2528VLMs%2529%2520deliver%2520strong%2520zero-shot%2520recognition%2520but%2520frequently%2520inherit%2520social%2520biases%2520from%2520their%2520training%2520data.%2520We%2520systematically%2520disentangle%2520three%2520design%2520factors%2520--%2520model%2520size%252C%2520training-data%2520scale%252C%2520and%2520training-data%2520source%2520--%2520by%2520comparing%2520CLIP%2520and%2520OpenCLIP%252C%2520two%2520models%2520that%2520share%2520an%2520identical%2520contrastive%2520objective%2520yet%2520differ%2520in%2520encoder%2520width%2520and%2520in%2520the%2520image-text%2520corpora%2520on%2520which%2520they%2520are%2520pre-trained%2520%2528400M%2520proprietary%2520pairs%2520vs.%2520400M/2B%2520LAION%2529.%2520Across%2520balanced%2520face-analysis%2520benchmarks%252C%2520enlarging%2520the%2520encoder%2520reduces%2520gender%2520skew%2520in%2520CLIP%2520but%2520amplifies%2520both%2520gender%2520and%2520racial%2520skew%2520in%2520OpenCLIP%253B%2520increasing%2520the%2520LAION%2520corpus%2520from%2520400M%2520to%25202B%2520further%2520increases%2520OpenCLIP%2520bias.%2520At%2520matched%2520model%2520and%2520data%2520budgets%252C%2520substituting%2520proprietary%2520data%2520with%2520LAION%2520improves%2520gender%2520fairness%2520while%2520increasing%2520racial%2520skew%252C%2520underscoring%2520data%2520source%2520as%2520the%2520primary%2520driver%2520of%2520bias%2520patterns.%2520We%2520also%2520evaluate%2520three%2520post-hoc%252C%2520test-time%2520debiasing%2520strategies%2520--%2520Bias%2520Prompts%252C%2520Prompt%2520Array%252C%2520and%2520SANER.%2520Debiasing%2520reduces%2520but%2520does%2520not%2520eliminate%2520harm%252C%2520and%2520its%2520effectiveness%2520is%2520source-%2520and%2520size-dependent%253A%2520Bias%2520Prompts%2520most%2520effectively%2520reduce%2520gender%2520skew%2520in%2520CLIP%2520at%2520smaller%2520model%2520sizes%252C%2520whereas%2520Prompt%2520Array%2520and%2520SANER%2520more%2520reliably%2520reduce%2520racial%2520skew%2520in%2520OpenCLIP%253B%2520scaling%2520LAION%2520reconfigures%2520which%2520method%2520is%2520most%2520fair.%2520Taken%2520together%252C%2520these%2520findings%2520challenge%2520the%2520assumption%2520that%2520bigger%2520models%2520or%2520datasets%2520are%2520automatically%2520fairer%2520and%2520foreground%2520training%2520data%2520source%2520as%2520the%2520key%2520determinant%2520of%2520both%2520bias%2520and%2520mitigation%2520efficacy.%2520We%2520release%2520code%2520and%2520evaluation%2520scripts%2520to%2520enable%2520transparent%252C%2520reproducible%2520auditing%2520of%2520future%2520VLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13223v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Matters%20Most%3A%20Auditing%20Social%20Bias%20in%20Contrastive%20Vision%20Language%20Models&entry.906535625=Zahraa%20Al%20Sahili%20and%20Ioannis%20Patras%20and%20Matthew%20Purver&entry.1292438233=Vision-language%20models%20%28VLMs%29%20deliver%20strong%20zero-shot%20recognition%20but%20frequently%20inherit%20social%20biases%20from%20their%20training%20data.%20We%20systematically%20disentangle%20three%20design%20factors%20--%20model%20size%2C%20training-data%20scale%2C%20and%20training-data%20source%20--%20by%20comparing%20CLIP%20and%20OpenCLIP%2C%20two%20models%20that%20share%20an%20identical%20contrastive%20objective%20yet%20differ%20in%20encoder%20width%20and%20in%20the%20image-text%20corpora%20on%20which%20they%20are%20pre-trained%20%28400M%20proprietary%20pairs%20vs.%20400M/2B%20LAION%29.%20Across%20balanced%20face-analysis%20benchmarks%2C%20enlarging%20the%20encoder%20reduces%20gender%20skew%20in%20CLIP%20but%20amplifies%20both%20gender%20and%20racial%20skew%20in%20OpenCLIP%3B%20increasing%20the%20LAION%20corpus%20from%20400M%20to%202B%20further%20increases%20OpenCLIP%20bias.%20At%20matched%20model%20and%20data%20budgets%2C%20substituting%20proprietary%20data%20with%20LAION%20improves%20gender%20fairness%20while%20increasing%20racial%20skew%2C%20underscoring%20data%20source%20as%20the%20primary%20driver%20of%20bias%20patterns.%20We%20also%20evaluate%20three%20post-hoc%2C%20test-time%20debiasing%20strategies%20--%20Bias%20Prompts%2C%20Prompt%20Array%2C%20and%20SANER.%20Debiasing%20reduces%20but%20does%20not%20eliminate%20harm%2C%20and%20its%20effectiveness%20is%20source-%20and%20size-dependent%3A%20Bias%20Prompts%20most%20effectively%20reduce%20gender%20skew%20in%20CLIP%20at%20smaller%20model%20sizes%2C%20whereas%20Prompt%20Array%20and%20SANER%20more%20reliably%20reduce%20racial%20skew%20in%20OpenCLIP%3B%20scaling%20LAION%20reconfigures%20which%20method%20is%20most%20fair.%20Taken%20together%2C%20these%20findings%20challenge%20the%20assumption%20that%20bigger%20models%20or%20datasets%20are%20automatically%20fairer%20and%20foreground%20training%20data%20source%20as%20the%20key%20determinant%20of%20both%20bias%20and%20mitigation%20efficacy.%20We%20release%20code%20and%20evaluation%20scripts%20to%20enable%20transparent%2C%20reproducible%20auditing%20of%20future%20VLMs.&entry.1838667208=http%3A//arxiv.org/abs/2501.13223v7&entry.124074799=Read"},
{"title": "REL-SF4PASS: Panoramic Semantic Segmentation with REL Depth Representation and Spherical Fusion", "author": "Xuewei Li and Xinghan Bao and Zhimin Chen and Xi Li", "abstract": "As an important and challenging problem in computer vision, Panoramic Semantic Segmentation (PASS) aims to give complete scene perception based on an ultra-wide angle of view. Most PASS methods often focus on spherical geometry with RGB input or using the depth information in original or HHA format, which does not make full use of panoramic image geometry. To address these shortcomings, we propose REL-SF4PASS with our REL depth representation based on cylindrical coordinate and Spherical-dynamic Multi-Modal Fusion SMMF. REL is made up of Rectified Depth, Elevation-Gained Vertical Inclination Angle, and Lateral Orientation Angle, which fully represents 3D space in cylindrical coordinate style and the surface normal direction. SMMF aims to ensure the diversity of fusion for different panoramic image regions and reduce the breakage of cylinder side surface expansion in ERP projection, which uses different fusion strategies to match the different regions in panoramic images. Experimental results show that REL-SF4PASS considerably improves performance and robustness on popular benchmark, Stanford2D3D Panoramic datasets. It gains 2.35% average mIoU improvement on all 3 folds and reduces the performance variance by approximately 70% when facing 3D disturbance.", "link": "http://arxiv.org/abs/2601.16788v1", "date": "2026-01-23", "relevancy": 2.6959, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5482}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5421}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REL-SF4PASS%3A%20Panoramic%20Semantic%20Segmentation%20with%20REL%20Depth%20Representation%20and%20Spherical%20Fusion&body=Title%3A%20REL-SF4PASS%3A%20Panoramic%20Semantic%20Segmentation%20with%20REL%20Depth%20Representation%20and%20Spherical%20Fusion%0AAuthor%3A%20Xuewei%20Li%20and%20Xinghan%20Bao%20and%20Zhimin%20Chen%20and%20Xi%20Li%0AAbstract%3A%20As%20an%20important%20and%20challenging%20problem%20in%20computer%20vision%2C%20Panoramic%20Semantic%20Segmentation%20%28PASS%29%20aims%20to%20give%20complete%20scene%20perception%20based%20on%20an%20ultra-wide%20angle%20of%20view.%20Most%20PASS%20methods%20often%20focus%20on%20spherical%20geometry%20with%20RGB%20input%20or%20using%20the%20depth%20information%20in%20original%20or%20HHA%20format%2C%20which%20does%20not%20make%20full%20use%20of%20panoramic%20image%20geometry.%20To%20address%20these%20shortcomings%2C%20we%20propose%20REL-SF4PASS%20with%20our%20REL%20depth%20representation%20based%20on%20cylindrical%20coordinate%20and%20Spherical-dynamic%20Multi-Modal%20Fusion%20SMMF.%20REL%20is%20made%20up%20of%20Rectified%20Depth%2C%20Elevation-Gained%20Vertical%20Inclination%20Angle%2C%20and%20Lateral%20Orientation%20Angle%2C%20which%20fully%20represents%203D%20space%20in%20cylindrical%20coordinate%20style%20and%20the%20surface%20normal%20direction.%20SMMF%20aims%20to%20ensure%20the%20diversity%20of%20fusion%20for%20different%20panoramic%20image%20regions%20and%20reduce%20the%20breakage%20of%20cylinder%20side%20surface%20expansion%20in%20ERP%20projection%2C%20which%20uses%20different%20fusion%20strategies%20to%20match%20the%20different%20regions%20in%20panoramic%20images.%20Experimental%20results%20show%20that%20REL-SF4PASS%20considerably%20improves%20performance%20and%20robustness%20on%20popular%20benchmark%2C%20Stanford2D3D%20Panoramic%20datasets.%20It%20gains%202.35%25%20average%20mIoU%20improvement%20on%20all%203%20folds%20and%20reduces%20the%20performance%20variance%20by%20approximately%2070%25%20when%20facing%203D%20disturbance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREL-SF4PASS%253A%2520Panoramic%2520Semantic%2520Segmentation%2520with%2520REL%2520Depth%2520Representation%2520and%2520Spherical%2520Fusion%26entry.906535625%3DXuewei%2520Li%2520and%2520Xinghan%2520Bao%2520and%2520Zhimin%2520Chen%2520and%2520Xi%2520Li%26entry.1292438233%3DAs%2520an%2520important%2520and%2520challenging%2520problem%2520in%2520computer%2520vision%252C%2520Panoramic%2520Semantic%2520Segmentation%2520%2528PASS%2529%2520aims%2520to%2520give%2520complete%2520scene%2520perception%2520based%2520on%2520an%2520ultra-wide%2520angle%2520of%2520view.%2520Most%2520PASS%2520methods%2520often%2520focus%2520on%2520spherical%2520geometry%2520with%2520RGB%2520input%2520or%2520using%2520the%2520depth%2520information%2520in%2520original%2520or%2520HHA%2520format%252C%2520which%2520does%2520not%2520make%2520full%2520use%2520of%2520panoramic%2520image%2520geometry.%2520To%2520address%2520these%2520shortcomings%252C%2520we%2520propose%2520REL-SF4PASS%2520with%2520our%2520REL%2520depth%2520representation%2520based%2520on%2520cylindrical%2520coordinate%2520and%2520Spherical-dynamic%2520Multi-Modal%2520Fusion%2520SMMF.%2520REL%2520is%2520made%2520up%2520of%2520Rectified%2520Depth%252C%2520Elevation-Gained%2520Vertical%2520Inclination%2520Angle%252C%2520and%2520Lateral%2520Orientation%2520Angle%252C%2520which%2520fully%2520represents%25203D%2520space%2520in%2520cylindrical%2520coordinate%2520style%2520and%2520the%2520surface%2520normal%2520direction.%2520SMMF%2520aims%2520to%2520ensure%2520the%2520diversity%2520of%2520fusion%2520for%2520different%2520panoramic%2520image%2520regions%2520and%2520reduce%2520the%2520breakage%2520of%2520cylinder%2520side%2520surface%2520expansion%2520in%2520ERP%2520projection%252C%2520which%2520uses%2520different%2520fusion%2520strategies%2520to%2520match%2520the%2520different%2520regions%2520in%2520panoramic%2520images.%2520Experimental%2520results%2520show%2520that%2520REL-SF4PASS%2520considerably%2520improves%2520performance%2520and%2520robustness%2520on%2520popular%2520benchmark%252C%2520Stanford2D3D%2520Panoramic%2520datasets.%2520It%2520gains%25202.35%2525%2520average%2520mIoU%2520improvement%2520on%2520all%25203%2520folds%2520and%2520reduces%2520the%2520performance%2520variance%2520by%2520approximately%252070%2525%2520when%2520facing%25203D%2520disturbance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REL-SF4PASS%3A%20Panoramic%20Semantic%20Segmentation%20with%20REL%20Depth%20Representation%20and%20Spherical%20Fusion&entry.906535625=Xuewei%20Li%20and%20Xinghan%20Bao%20and%20Zhimin%20Chen%20and%20Xi%20Li&entry.1292438233=As%20an%20important%20and%20challenging%20problem%20in%20computer%20vision%2C%20Panoramic%20Semantic%20Segmentation%20%28PASS%29%20aims%20to%20give%20complete%20scene%20perception%20based%20on%20an%20ultra-wide%20angle%20of%20view.%20Most%20PASS%20methods%20often%20focus%20on%20spherical%20geometry%20with%20RGB%20input%20or%20using%20the%20depth%20information%20in%20original%20or%20HHA%20format%2C%20which%20does%20not%20make%20full%20use%20of%20panoramic%20image%20geometry.%20To%20address%20these%20shortcomings%2C%20we%20propose%20REL-SF4PASS%20with%20our%20REL%20depth%20representation%20based%20on%20cylindrical%20coordinate%20and%20Spherical-dynamic%20Multi-Modal%20Fusion%20SMMF.%20REL%20is%20made%20up%20of%20Rectified%20Depth%2C%20Elevation-Gained%20Vertical%20Inclination%20Angle%2C%20and%20Lateral%20Orientation%20Angle%2C%20which%20fully%20represents%203D%20space%20in%20cylindrical%20coordinate%20style%20and%20the%20surface%20normal%20direction.%20SMMF%20aims%20to%20ensure%20the%20diversity%20of%20fusion%20for%20different%20panoramic%20image%20regions%20and%20reduce%20the%20breakage%20of%20cylinder%20side%20surface%20expansion%20in%20ERP%20projection%2C%20which%20uses%20different%20fusion%20strategies%20to%20match%20the%20different%20regions%20in%20panoramic%20images.%20Experimental%20results%20show%20that%20REL-SF4PASS%20considerably%20improves%20performance%20and%20robustness%20on%20popular%20benchmark%2C%20Stanford2D3D%20Panoramic%20datasets.%20It%20gains%202.35%25%20average%20mIoU%20improvement%20on%20all%203%20folds%20and%20reduces%20the%20performance%20variance%20by%20approximately%2070%25%20when%20facing%203D%20disturbance.&entry.1838667208=http%3A//arxiv.org/abs/2601.16788v1&entry.124074799=Read"},
{"title": "Prometheus Mind: Retrofitting Memory to Frozen Language Models", "author": "Mark Wind", "abstract": "Adding memory to pretrained language models typically requires architectural changes or weight modification. We present Prometheus Mind, which retrofits memory to a frozen Qwen3-4B using 11 modular adapters (530MB, 7% overhead) -- fully reversible by removing the adapters. Building this system required solving four problems: (1) Extraction -- we develop Contrastive Direction Discovery (CDD), which finds semantic directions via minimal pairs without labeled data. (2) Training -- end-to-end optimization collapses; stage-wise training of each adapter on simple proxy tasks succeeds. (3) Injection -- learned encoders fail to generalize; we find that lm_head-weight rows already provide the mapping we need, requiring no training. (4) Hidden state collapse -- transformers make ``wife'' and ``brother'' 0.98+ similar; we train projections to recover distinction (0.98 $\\rightarrow$ 0.09). On PrometheusExtract-132 (132 cases), the system achieves 94.4% retrieval on clean inputs (n=54, 95% CI: [84.9%, 98.1%]), degrading to 19.4% on informal inputs with ellipsis, filler words, or implicit subjects (n=36). The primary bottleneck is relation classification (47.3% accuracy), responsible for most extraction errors.", "link": "http://arxiv.org/abs/2601.15324v2", "date": "2026-01-23", "relevancy": 2.6617, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5336}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5317}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prometheus%20Mind%3A%20Retrofitting%20Memory%20to%20Frozen%20Language%20Models&body=Title%3A%20Prometheus%20Mind%3A%20Retrofitting%20Memory%20to%20Frozen%20Language%20Models%0AAuthor%3A%20Mark%20Wind%0AAbstract%3A%20Adding%20memory%20to%20pretrained%20language%20models%20typically%20requires%20architectural%20changes%20or%20weight%20modification.%20We%20present%20Prometheus%20Mind%2C%20which%20retrofits%20memory%20to%20a%20frozen%20Qwen3-4B%20using%2011%20modular%20adapters%20%28530MB%2C%207%25%20overhead%29%20--%20fully%20reversible%20by%20removing%20the%20adapters.%20Building%20this%20system%20required%20solving%20four%20problems%3A%20%281%29%20Extraction%20--%20we%20develop%20Contrastive%20Direction%20Discovery%20%28CDD%29%2C%20which%20finds%20semantic%20directions%20via%20minimal%20pairs%20without%20labeled%20data.%20%282%29%20Training%20--%20end-to-end%20optimization%20collapses%3B%20stage-wise%20training%20of%20each%20adapter%20on%20simple%20proxy%20tasks%20succeeds.%20%283%29%20Injection%20--%20learned%20encoders%20fail%20to%20generalize%3B%20we%20find%20that%20lm_head-weight%20rows%20already%20provide%20the%20mapping%20we%20need%2C%20requiring%20no%20training.%20%284%29%20Hidden%20state%20collapse%20--%20transformers%20make%20%60%60wife%27%27%20and%20%60%60brother%27%27%200.98%2B%20similar%3B%20we%20train%20projections%20to%20recover%20distinction%20%280.98%20%24%5Crightarrow%24%200.09%29.%20On%20PrometheusExtract-132%20%28132%20cases%29%2C%20the%20system%20achieves%2094.4%25%20retrieval%20on%20clean%20inputs%20%28n%3D54%2C%2095%25%20CI%3A%20%5B84.9%25%2C%2098.1%25%5D%29%2C%20degrading%20to%2019.4%25%20on%20informal%20inputs%20with%20ellipsis%2C%20filler%20words%2C%20or%20implicit%20subjects%20%28n%3D36%29.%20The%20primary%20bottleneck%20is%20relation%20classification%20%2847.3%25%20accuracy%29%2C%20responsible%20for%20most%20extraction%20errors.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15324v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrometheus%2520Mind%253A%2520Retrofitting%2520Memory%2520to%2520Frozen%2520Language%2520Models%26entry.906535625%3DMark%2520Wind%26entry.1292438233%3DAdding%2520memory%2520to%2520pretrained%2520language%2520models%2520typically%2520requires%2520architectural%2520changes%2520or%2520weight%2520modification.%2520We%2520present%2520Prometheus%2520Mind%252C%2520which%2520retrofits%2520memory%2520to%2520a%2520frozen%2520Qwen3-4B%2520using%252011%2520modular%2520adapters%2520%2528530MB%252C%25207%2525%2520overhead%2529%2520--%2520fully%2520reversible%2520by%2520removing%2520the%2520adapters.%2520Building%2520this%2520system%2520required%2520solving%2520four%2520problems%253A%2520%25281%2529%2520Extraction%2520--%2520we%2520develop%2520Contrastive%2520Direction%2520Discovery%2520%2528CDD%2529%252C%2520which%2520finds%2520semantic%2520directions%2520via%2520minimal%2520pairs%2520without%2520labeled%2520data.%2520%25282%2529%2520Training%2520--%2520end-to-end%2520optimization%2520collapses%253B%2520stage-wise%2520training%2520of%2520each%2520adapter%2520on%2520simple%2520proxy%2520tasks%2520succeeds.%2520%25283%2529%2520Injection%2520--%2520learned%2520encoders%2520fail%2520to%2520generalize%253B%2520we%2520find%2520that%2520lm_head-weight%2520rows%2520already%2520provide%2520the%2520mapping%2520we%2520need%252C%2520requiring%2520no%2520training.%2520%25284%2529%2520Hidden%2520state%2520collapse%2520--%2520transformers%2520make%2520%2560%2560wife%2527%2527%2520and%2520%2560%2560brother%2527%2527%25200.98%252B%2520similar%253B%2520we%2520train%2520projections%2520to%2520recover%2520distinction%2520%25280.98%2520%2524%255Crightarrow%2524%25200.09%2529.%2520On%2520PrometheusExtract-132%2520%2528132%2520cases%2529%252C%2520the%2520system%2520achieves%252094.4%2525%2520retrieval%2520on%2520clean%2520inputs%2520%2528n%253D54%252C%252095%2525%2520CI%253A%2520%255B84.9%2525%252C%252098.1%2525%255D%2529%252C%2520degrading%2520to%252019.4%2525%2520on%2520informal%2520inputs%2520with%2520ellipsis%252C%2520filler%2520words%252C%2520or%2520implicit%2520subjects%2520%2528n%253D36%2529.%2520The%2520primary%2520bottleneck%2520is%2520relation%2520classification%2520%252847.3%2525%2520accuracy%2529%252C%2520responsible%2520for%2520most%2520extraction%2520errors.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15324v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prometheus%20Mind%3A%20Retrofitting%20Memory%20to%20Frozen%20Language%20Models&entry.906535625=Mark%20Wind&entry.1292438233=Adding%20memory%20to%20pretrained%20language%20models%20typically%20requires%20architectural%20changes%20or%20weight%20modification.%20We%20present%20Prometheus%20Mind%2C%20which%20retrofits%20memory%20to%20a%20frozen%20Qwen3-4B%20using%2011%20modular%20adapters%20%28530MB%2C%207%25%20overhead%29%20--%20fully%20reversible%20by%20removing%20the%20adapters.%20Building%20this%20system%20required%20solving%20four%20problems%3A%20%281%29%20Extraction%20--%20we%20develop%20Contrastive%20Direction%20Discovery%20%28CDD%29%2C%20which%20finds%20semantic%20directions%20via%20minimal%20pairs%20without%20labeled%20data.%20%282%29%20Training%20--%20end-to-end%20optimization%20collapses%3B%20stage-wise%20training%20of%20each%20adapter%20on%20simple%20proxy%20tasks%20succeeds.%20%283%29%20Injection%20--%20learned%20encoders%20fail%20to%20generalize%3B%20we%20find%20that%20lm_head-weight%20rows%20already%20provide%20the%20mapping%20we%20need%2C%20requiring%20no%20training.%20%284%29%20Hidden%20state%20collapse%20--%20transformers%20make%20%60%60wife%27%27%20and%20%60%60brother%27%27%200.98%2B%20similar%3B%20we%20train%20projections%20to%20recover%20distinction%20%280.98%20%24%5Crightarrow%24%200.09%29.%20On%20PrometheusExtract-132%20%28132%20cases%29%2C%20the%20system%20achieves%2094.4%25%20retrieval%20on%20clean%20inputs%20%28n%3D54%2C%2095%25%20CI%3A%20%5B84.9%25%2C%2098.1%25%5D%29%2C%20degrading%20to%2019.4%25%20on%20informal%20inputs%20with%20ellipsis%2C%20filler%20words%2C%20or%20implicit%20subjects%20%28n%3D36%29.%20The%20primary%20bottleneck%20is%20relation%20classification%20%2847.3%25%20accuracy%29%2C%20responsible%20for%20most%20extraction%20errors.&entry.1838667208=http%3A//arxiv.org/abs/2601.15324v2&entry.124074799=Read"},
{"title": "UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval", "author": "Hongyu Guo and Xiangzhao Hao and Jiarui Guo and Haiyun Guo and Jinqiao Wang and Tat-Seng Chua", "abstract": "Few-shot fine-grained visual classification (FGVC) aims to leverage limited data to enable models to discriminate subtly distinct categories. Recent works mostly finetuned the pre-trained visual language models to achieve performance gain, yet suffering from overfitting and weak generalization. To deal with this, we introduce UniFGVC, a universal training-free framework that reformulates few-shot FGVC as multimodal retrieval. First, we propose the Category-Discriminative Visual Captioner (CDV-Captioner) to exploit the open-world knowledge of multimodal large language models (MLLMs) to generate a structured text description that captures the fine-grained attribute features distinguishing closely related classes. CDV-Captioner uses chain-of-thought prompting and visually similar reference images to reduce hallucination and enhance discrimination of generated captions. Using it we can convert each image into an image-description pair, enabling more comprehensive feature representation, and construct the multimodal category templates using few-shot samples for the subsequent retrieval pipeline. Then, off-the-shelf vision and text encoders embed query and template pairs, and FGVC is accomplished by retrieving the nearest template in the joint space. UniFGVC ensures broad compatibility with diverse MLLMs and encoders, offering reliable generalization and adaptability across few-shot FGVC scenarios. Extensive experiments on 12 FGVC benchmarks demonstrate its consistent superiority over prior few-shot CLIP-based methods and even several fully-supervised MLLMs-based approaches.", "link": "http://arxiv.org/abs/2508.04136v2", "date": "2026-01-23", "relevancy": 2.6595, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5355}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.535}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniFGVC%3A%20Universal%20Training-Free%20Few-Shot%20Fine-Grained%20Vision%20Classification%20via%20Attribute-Aware%20Multimodal%20Retrieval&body=Title%3A%20UniFGVC%3A%20Universal%20Training-Free%20Few-Shot%20Fine-Grained%20Vision%20Classification%20via%20Attribute-Aware%20Multimodal%20Retrieval%0AAuthor%3A%20Hongyu%20Guo%20and%20Xiangzhao%20Hao%20and%20Jiarui%20Guo%20and%20Haiyun%20Guo%20and%20Jinqiao%20Wang%20and%20Tat-Seng%20Chua%0AAbstract%3A%20Few-shot%20fine-grained%20visual%20classification%20%28FGVC%29%20aims%20to%20leverage%20limited%20data%20to%20enable%20models%20to%20discriminate%20subtly%20distinct%20categories.%20Recent%20works%20mostly%20finetuned%20the%20pre-trained%20visual%20language%20models%20to%20achieve%20performance%20gain%2C%20yet%20suffering%20from%20overfitting%20and%20weak%20generalization.%20To%20deal%20with%20this%2C%20we%20introduce%20UniFGVC%2C%20a%20universal%20training-free%20framework%20that%20reformulates%20few-shot%20FGVC%20as%20multimodal%20retrieval.%20First%2C%20we%20propose%20the%20Category-Discriminative%20Visual%20Captioner%20%28CDV-Captioner%29%20to%20exploit%20the%20open-world%20knowledge%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%20to%20generate%20a%20structured%20text%20description%20that%20captures%20the%20fine-grained%20attribute%20features%20distinguishing%20closely%20related%20classes.%20CDV-Captioner%20uses%20chain-of-thought%20prompting%20and%20visually%20similar%20reference%20images%20to%20reduce%20hallucination%20and%20enhance%20discrimination%20of%20generated%20captions.%20Using%20it%20we%20can%20convert%20each%20image%20into%20an%20image-description%20pair%2C%20enabling%20more%20comprehensive%20feature%20representation%2C%20and%20construct%20the%20multimodal%20category%20templates%20using%20few-shot%20samples%20for%20the%20subsequent%20retrieval%20pipeline.%20Then%2C%20off-the-shelf%20vision%20and%20text%20encoders%20embed%20query%20and%20template%20pairs%2C%20and%20FGVC%20is%20accomplished%20by%20retrieving%20the%20nearest%20template%20in%20the%20joint%20space.%20UniFGVC%20ensures%20broad%20compatibility%20with%20diverse%20MLLMs%20and%20encoders%2C%20offering%20reliable%20generalization%20and%20adaptability%20across%20few-shot%20FGVC%20scenarios.%20Extensive%20experiments%20on%2012%20FGVC%20benchmarks%20demonstrate%20its%20consistent%20superiority%20over%20prior%20few-shot%20CLIP-based%20methods%20and%20even%20several%20fully-supervised%20MLLMs-based%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2508.04136v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniFGVC%253A%2520Universal%2520Training-Free%2520Few-Shot%2520Fine-Grained%2520Vision%2520Classification%2520via%2520Attribute-Aware%2520Multimodal%2520Retrieval%26entry.906535625%3DHongyu%2520Guo%2520and%2520Xiangzhao%2520Hao%2520and%2520Jiarui%2520Guo%2520and%2520Haiyun%2520Guo%2520and%2520Jinqiao%2520Wang%2520and%2520Tat-Seng%2520Chua%26entry.1292438233%3DFew-shot%2520fine-grained%2520visual%2520classification%2520%2528FGVC%2529%2520aims%2520to%2520leverage%2520limited%2520data%2520to%2520enable%2520models%2520to%2520discriminate%2520subtly%2520distinct%2520categories.%2520Recent%2520works%2520mostly%2520finetuned%2520the%2520pre-trained%2520visual%2520language%2520models%2520to%2520achieve%2520performance%2520gain%252C%2520yet%2520suffering%2520from%2520overfitting%2520and%2520weak%2520generalization.%2520To%2520deal%2520with%2520this%252C%2520we%2520introduce%2520UniFGVC%252C%2520a%2520universal%2520training-free%2520framework%2520that%2520reformulates%2520few-shot%2520FGVC%2520as%2520multimodal%2520retrieval.%2520First%252C%2520we%2520propose%2520the%2520Category-Discriminative%2520Visual%2520Captioner%2520%2528CDV-Captioner%2529%2520to%2520exploit%2520the%2520open-world%2520knowledge%2520of%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520to%2520generate%2520a%2520structured%2520text%2520description%2520that%2520captures%2520the%2520fine-grained%2520attribute%2520features%2520distinguishing%2520closely%2520related%2520classes.%2520CDV-Captioner%2520uses%2520chain-of-thought%2520prompting%2520and%2520visually%2520similar%2520reference%2520images%2520to%2520reduce%2520hallucination%2520and%2520enhance%2520discrimination%2520of%2520generated%2520captions.%2520Using%2520it%2520we%2520can%2520convert%2520each%2520image%2520into%2520an%2520image-description%2520pair%252C%2520enabling%2520more%2520comprehensive%2520feature%2520representation%252C%2520and%2520construct%2520the%2520multimodal%2520category%2520templates%2520using%2520few-shot%2520samples%2520for%2520the%2520subsequent%2520retrieval%2520pipeline.%2520Then%252C%2520off-the-shelf%2520vision%2520and%2520text%2520encoders%2520embed%2520query%2520and%2520template%2520pairs%252C%2520and%2520FGVC%2520is%2520accomplished%2520by%2520retrieving%2520the%2520nearest%2520template%2520in%2520the%2520joint%2520space.%2520UniFGVC%2520ensures%2520broad%2520compatibility%2520with%2520diverse%2520MLLMs%2520and%2520encoders%252C%2520offering%2520reliable%2520generalization%2520and%2520adaptability%2520across%2520few-shot%2520FGVC%2520scenarios.%2520Extensive%2520experiments%2520on%252012%2520FGVC%2520benchmarks%2520demonstrate%2520its%2520consistent%2520superiority%2520over%2520prior%2520few-shot%2520CLIP-based%2520methods%2520and%2520even%2520several%2520fully-supervised%2520MLLMs-based%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04136v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniFGVC%3A%20Universal%20Training-Free%20Few-Shot%20Fine-Grained%20Vision%20Classification%20via%20Attribute-Aware%20Multimodal%20Retrieval&entry.906535625=Hongyu%20Guo%20and%20Xiangzhao%20Hao%20and%20Jiarui%20Guo%20and%20Haiyun%20Guo%20and%20Jinqiao%20Wang%20and%20Tat-Seng%20Chua&entry.1292438233=Few-shot%20fine-grained%20visual%20classification%20%28FGVC%29%20aims%20to%20leverage%20limited%20data%20to%20enable%20models%20to%20discriminate%20subtly%20distinct%20categories.%20Recent%20works%20mostly%20finetuned%20the%20pre-trained%20visual%20language%20models%20to%20achieve%20performance%20gain%2C%20yet%20suffering%20from%20overfitting%20and%20weak%20generalization.%20To%20deal%20with%20this%2C%20we%20introduce%20UniFGVC%2C%20a%20universal%20training-free%20framework%20that%20reformulates%20few-shot%20FGVC%20as%20multimodal%20retrieval.%20First%2C%20we%20propose%20the%20Category-Discriminative%20Visual%20Captioner%20%28CDV-Captioner%29%20to%20exploit%20the%20open-world%20knowledge%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%20to%20generate%20a%20structured%20text%20description%20that%20captures%20the%20fine-grained%20attribute%20features%20distinguishing%20closely%20related%20classes.%20CDV-Captioner%20uses%20chain-of-thought%20prompting%20and%20visually%20similar%20reference%20images%20to%20reduce%20hallucination%20and%20enhance%20discrimination%20of%20generated%20captions.%20Using%20it%20we%20can%20convert%20each%20image%20into%20an%20image-description%20pair%2C%20enabling%20more%20comprehensive%20feature%20representation%2C%20and%20construct%20the%20multimodal%20category%20templates%20using%20few-shot%20samples%20for%20the%20subsequent%20retrieval%20pipeline.%20Then%2C%20off-the-shelf%20vision%20and%20text%20encoders%20embed%20query%20and%20template%20pairs%2C%20and%20FGVC%20is%20accomplished%20by%20retrieving%20the%20nearest%20template%20in%20the%20joint%20space.%20UniFGVC%20ensures%20broad%20compatibility%20with%20diverse%20MLLMs%20and%20encoders%2C%20offering%20reliable%20generalization%20and%20adaptability%20across%20few-shot%20FGVC%20scenarios.%20Extensive%20experiments%20on%2012%20FGVC%20benchmarks%20demonstrate%20its%20consistent%20superiority%20over%20prior%20few-shot%20CLIP-based%20methods%20and%20even%20several%20fully-supervised%20MLLMs-based%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2508.04136v2&entry.124074799=Read"},
{"title": "SoS: Analysis of Surface over Semantics in Multilingual Text-To-Image Generation", "author": "Carolin Holtermann and Florian Schneider and Anne Lauscher", "abstract": "Text-to-image (T2I) models are increasingly employed by users worldwide. However, prior research has pointed to the high sensitivity of T2I towards particular input languages - when faced with languages other than English (i.e., different surface forms of the same prompt), T2I models often produce culturally stereotypical depictions, prioritizing the surface over the prompt's semantics. Yet a comprehensive analysis of this behavior, which we dub Surface-over-Semantics (SoS), is missing. We present the first analysis of T2I models' SoS tendencies. To this end, we create a set of prompts covering 171 cultural identities, translated into 14 languages, and use it to prompt seven T2I models. To quantify SoS tendencies across models, languages, and cultures, we introduce a novel measure and analyze how the tendencies we identify manifest visually. We show that all but one model exhibit strong surface-level tendency in at least two languages, with this effect intensifying across the layers of T2I text encoders. Moreover, these surface tendencies frequently correlate with stereotypical visual depictions.", "link": "http://arxiv.org/abs/2601.16803v1", "date": "2026-01-23", "relevancy": 2.6472, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5337}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5337}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoS%3A%20Analysis%20of%20Surface%20over%20Semantics%20in%20Multilingual%20Text-To-Image%20Generation&body=Title%3A%20SoS%3A%20Analysis%20of%20Surface%20over%20Semantics%20in%20Multilingual%20Text-To-Image%20Generation%0AAuthor%3A%20Carolin%20Holtermann%20and%20Florian%20Schneider%20and%20Anne%20Lauscher%0AAbstract%3A%20Text-to-image%20%28T2I%29%20models%20are%20increasingly%20employed%20by%20users%20worldwide.%20However%2C%20prior%20research%20has%20pointed%20to%20the%20high%20sensitivity%20of%20T2I%20towards%20particular%20input%20languages%20-%20when%20faced%20with%20languages%20other%20than%20English%20%28i.e.%2C%20different%20surface%20forms%20of%20the%20same%20prompt%29%2C%20T2I%20models%20often%20produce%20culturally%20stereotypical%20depictions%2C%20prioritizing%20the%20surface%20over%20the%20prompt%27s%20semantics.%20Yet%20a%20comprehensive%20analysis%20of%20this%20behavior%2C%20which%20we%20dub%20Surface-over-Semantics%20%28SoS%29%2C%20is%20missing.%20We%20present%20the%20first%20analysis%20of%20T2I%20models%27%20SoS%20tendencies.%20To%20this%20end%2C%20we%20create%20a%20set%20of%20prompts%20covering%20171%20cultural%20identities%2C%20translated%20into%2014%20languages%2C%20and%20use%20it%20to%20prompt%20seven%20T2I%20models.%20To%20quantify%20SoS%20tendencies%20across%20models%2C%20languages%2C%20and%20cultures%2C%20we%20introduce%20a%20novel%20measure%20and%20analyze%20how%20the%20tendencies%20we%20identify%20manifest%20visually.%20We%20show%20that%20all%20but%20one%20model%20exhibit%20strong%20surface-level%20tendency%20in%20at%20least%20two%20languages%2C%20with%20this%20effect%20intensifying%20across%20the%20layers%20of%20T2I%20text%20encoders.%20Moreover%2C%20these%20surface%20tendencies%20frequently%20correlate%20with%20stereotypical%20visual%20depictions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoS%253A%2520Analysis%2520of%2520Surface%2520over%2520Semantics%2520in%2520Multilingual%2520Text-To-Image%2520Generation%26entry.906535625%3DCarolin%2520Holtermann%2520and%2520Florian%2520Schneider%2520and%2520Anne%2520Lauscher%26entry.1292438233%3DText-to-image%2520%2528T2I%2529%2520models%2520are%2520increasingly%2520employed%2520by%2520users%2520worldwide.%2520However%252C%2520prior%2520research%2520has%2520pointed%2520to%2520the%2520high%2520sensitivity%2520of%2520T2I%2520towards%2520particular%2520input%2520languages%2520-%2520when%2520faced%2520with%2520languages%2520other%2520than%2520English%2520%2528i.e.%252C%2520different%2520surface%2520forms%2520of%2520the%2520same%2520prompt%2529%252C%2520T2I%2520models%2520often%2520produce%2520culturally%2520stereotypical%2520depictions%252C%2520prioritizing%2520the%2520surface%2520over%2520the%2520prompt%2527s%2520semantics.%2520Yet%2520a%2520comprehensive%2520analysis%2520of%2520this%2520behavior%252C%2520which%2520we%2520dub%2520Surface-over-Semantics%2520%2528SoS%2529%252C%2520is%2520missing.%2520We%2520present%2520the%2520first%2520analysis%2520of%2520T2I%2520models%2527%2520SoS%2520tendencies.%2520To%2520this%2520end%252C%2520we%2520create%2520a%2520set%2520of%2520prompts%2520covering%2520171%2520cultural%2520identities%252C%2520translated%2520into%252014%2520languages%252C%2520and%2520use%2520it%2520to%2520prompt%2520seven%2520T2I%2520models.%2520To%2520quantify%2520SoS%2520tendencies%2520across%2520models%252C%2520languages%252C%2520and%2520cultures%252C%2520we%2520introduce%2520a%2520novel%2520measure%2520and%2520analyze%2520how%2520the%2520tendencies%2520we%2520identify%2520manifest%2520visually.%2520We%2520show%2520that%2520all%2520but%2520one%2520model%2520exhibit%2520strong%2520surface-level%2520tendency%2520in%2520at%2520least%2520two%2520languages%252C%2520with%2520this%2520effect%2520intensifying%2520across%2520the%2520layers%2520of%2520T2I%2520text%2520encoders.%2520Moreover%252C%2520these%2520surface%2520tendencies%2520frequently%2520correlate%2520with%2520stereotypical%2520visual%2520depictions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoS%3A%20Analysis%20of%20Surface%20over%20Semantics%20in%20Multilingual%20Text-To-Image%20Generation&entry.906535625=Carolin%20Holtermann%20and%20Florian%20Schneider%20and%20Anne%20Lauscher&entry.1292438233=Text-to-image%20%28T2I%29%20models%20are%20increasingly%20employed%20by%20users%20worldwide.%20However%2C%20prior%20research%20has%20pointed%20to%20the%20high%20sensitivity%20of%20T2I%20towards%20particular%20input%20languages%20-%20when%20faced%20with%20languages%20other%20than%20English%20%28i.e.%2C%20different%20surface%20forms%20of%20the%20same%20prompt%29%2C%20T2I%20models%20often%20produce%20culturally%20stereotypical%20depictions%2C%20prioritizing%20the%20surface%20over%20the%20prompt%27s%20semantics.%20Yet%20a%20comprehensive%20analysis%20of%20this%20behavior%2C%20which%20we%20dub%20Surface-over-Semantics%20%28SoS%29%2C%20is%20missing.%20We%20present%20the%20first%20analysis%20of%20T2I%20models%27%20SoS%20tendencies.%20To%20this%20end%2C%20we%20create%20a%20set%20of%20prompts%20covering%20171%20cultural%20identities%2C%20translated%20into%2014%20languages%2C%20and%20use%20it%20to%20prompt%20seven%20T2I%20models.%20To%20quantify%20SoS%20tendencies%20across%20models%2C%20languages%2C%20and%20cultures%2C%20we%20introduce%20a%20novel%20measure%20and%20analyze%20how%20the%20tendencies%20we%20identify%20manifest%20visually.%20We%20show%20that%20all%20but%20one%20model%20exhibit%20strong%20surface-level%20tendency%20in%20at%20least%20two%20languages%2C%20with%20this%20effect%20intensifying%20across%20the%20layers%20of%20T2I%20text%20encoders.%20Moreover%2C%20these%20surface%20tendencies%20frequently%20correlate%20with%20stereotypical%20visual%20depictions.&entry.1838667208=http%3A//arxiv.org/abs/2601.16803v1&entry.124074799=Read"},
{"title": "E2Former-V2: On-the-Fly Equivariant Attention with Linear Activation Memory", "author": "Lin Huang and Chengxiang Huang and Ziang Wang and Yiyue Du and Chu Wang and Haocheng Lu and Yunyang Li and Xiaoli Liu and Arthur Jiang and Jia Zhang", "abstract": "Equivariant Graph Neural Networks (EGNNs) have become a widely used approach for modeling 3D atomistic systems. However, mainstream architectures face critical scalability bottlenecks due to the explicit construction of geometric features or dense tensor products on \\textit{every} edge. To overcome this, we introduce \\textbf{E2Former-V2}, a scalable architecture that integrates algebraic sparsity with hardware-aware execution. We first propose \\textbf{E}quivariant \\textbf{A}xis-\\textbf{A}ligned \\textbf{S}parsification (EAAS). EAAS builds on Wigner-$6j$ convolution by exploiting an $\\mathrm{SO}(3) \\rightarrow \\mathrm{SO}(2)$ change of basis to transform computationally expensive dense tensor contractions into efficient, sparse parity re-indexing operations. Building on this representation, we introduce \\textbf{On-the-Fly Equivariant Attention}, a fully node-centric mechanism implemented via a custom fused Triton kernel. By eliminating materialized edge tensors and maximizing SRAM utilization, our kernel achieves a \\textbf{20$\\times$ improvement in TFLOPS} compared to standard implementations. Extensive experiments on the SPICE and OMol25 datasets demonstrate that E2Former-V2 maintains comparable predictive performance while notably accelerating inference. This work demonstrates that large equivariant transformers can be trained efficiently using widely accessible GPU platforms. The code is avalible at https://github.com/IQuestLab/UBio-MolFM/tree/e2formerv2.", "link": "http://arxiv.org/abs/2601.16622v1", "date": "2026-01-23", "relevancy": 2.6439, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5374}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5348}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E2Former-V2%3A%20On-the-Fly%20Equivariant%20Attention%20with%20Linear%20Activation%20Memory&body=Title%3A%20E2Former-V2%3A%20On-the-Fly%20Equivariant%20Attention%20with%20Linear%20Activation%20Memory%0AAuthor%3A%20Lin%20Huang%20and%20Chengxiang%20Huang%20and%20Ziang%20Wang%20and%20Yiyue%20Du%20and%20Chu%20Wang%20and%20Haocheng%20Lu%20and%20Yunyang%20Li%20and%20Xiaoli%20Liu%20and%20Arthur%20Jiang%20and%20Jia%20Zhang%0AAbstract%3A%20Equivariant%20Graph%20Neural%20Networks%20%28EGNNs%29%20have%20become%20a%20widely%20used%20approach%20for%20modeling%203D%20atomistic%20systems.%20However%2C%20mainstream%20architectures%20face%20critical%20scalability%20bottlenecks%20due%20to%20the%20explicit%20construction%20of%20geometric%20features%20or%20dense%20tensor%20products%20on%20%5Ctextit%7Bevery%7D%20edge.%20To%20overcome%20this%2C%20we%20introduce%20%5Ctextbf%7BE2Former-V2%7D%2C%20a%20scalable%20architecture%20that%20integrates%20algebraic%20sparsity%20with%20hardware-aware%20execution.%20We%20first%20propose%20%5Ctextbf%7BE%7Dquivariant%20%5Ctextbf%7BA%7Dxis-%5Ctextbf%7BA%7Dligned%20%5Ctextbf%7BS%7Dparsification%20%28EAAS%29.%20EAAS%20builds%20on%20Wigner-%246j%24%20convolution%20by%20exploiting%20an%20%24%5Cmathrm%7BSO%7D%283%29%20%5Crightarrow%20%5Cmathrm%7BSO%7D%282%29%24%20change%20of%20basis%20to%20transform%20computationally%20expensive%20dense%20tensor%20contractions%20into%20efficient%2C%20sparse%20parity%20re-indexing%20operations.%20Building%20on%20this%20representation%2C%20we%20introduce%20%5Ctextbf%7BOn-the-Fly%20Equivariant%20Attention%7D%2C%20a%20fully%20node-centric%20mechanism%20implemented%20via%20a%20custom%20fused%20Triton%20kernel.%20By%20eliminating%20materialized%20edge%20tensors%20and%20maximizing%20SRAM%20utilization%2C%20our%20kernel%20achieves%20a%20%5Ctextbf%7B20%24%5Ctimes%24%20improvement%20in%20TFLOPS%7D%20compared%20to%20standard%20implementations.%20Extensive%20experiments%20on%20the%20SPICE%20and%20OMol25%20datasets%20demonstrate%20that%20E2Former-V2%20maintains%20comparable%20predictive%20performance%20while%20notably%20accelerating%20inference.%20This%20work%20demonstrates%20that%20large%20equivariant%20transformers%20can%20be%20trained%20efficiently%20using%20widely%20accessible%20GPU%20platforms.%20The%20code%20is%20avalible%20at%20https%3A//github.com/IQuestLab/UBio-MolFM/tree/e2formerv2.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE2Former-V2%253A%2520On-the-Fly%2520Equivariant%2520Attention%2520with%2520Linear%2520Activation%2520Memory%26entry.906535625%3DLin%2520Huang%2520and%2520Chengxiang%2520Huang%2520and%2520Ziang%2520Wang%2520and%2520Yiyue%2520Du%2520and%2520Chu%2520Wang%2520and%2520Haocheng%2520Lu%2520and%2520Yunyang%2520Li%2520and%2520Xiaoli%2520Liu%2520and%2520Arthur%2520Jiang%2520and%2520Jia%2520Zhang%26entry.1292438233%3DEquivariant%2520Graph%2520Neural%2520Networks%2520%2528EGNNs%2529%2520have%2520become%2520a%2520widely%2520used%2520approach%2520for%2520modeling%25203D%2520atomistic%2520systems.%2520However%252C%2520mainstream%2520architectures%2520face%2520critical%2520scalability%2520bottlenecks%2520due%2520to%2520the%2520explicit%2520construction%2520of%2520geometric%2520features%2520or%2520dense%2520tensor%2520products%2520on%2520%255Ctextit%257Bevery%257D%2520edge.%2520To%2520overcome%2520this%252C%2520we%2520introduce%2520%255Ctextbf%257BE2Former-V2%257D%252C%2520a%2520scalable%2520architecture%2520that%2520integrates%2520algebraic%2520sparsity%2520with%2520hardware-aware%2520execution.%2520We%2520first%2520propose%2520%255Ctextbf%257BE%257Dquivariant%2520%255Ctextbf%257BA%257Dxis-%255Ctextbf%257BA%257Dligned%2520%255Ctextbf%257BS%257Dparsification%2520%2528EAAS%2529.%2520EAAS%2520builds%2520on%2520Wigner-%25246j%2524%2520convolution%2520by%2520exploiting%2520an%2520%2524%255Cmathrm%257BSO%257D%25283%2529%2520%255Crightarrow%2520%255Cmathrm%257BSO%257D%25282%2529%2524%2520change%2520of%2520basis%2520to%2520transform%2520computationally%2520expensive%2520dense%2520tensor%2520contractions%2520into%2520efficient%252C%2520sparse%2520parity%2520re-indexing%2520operations.%2520Building%2520on%2520this%2520representation%252C%2520we%2520introduce%2520%255Ctextbf%257BOn-the-Fly%2520Equivariant%2520Attention%257D%252C%2520a%2520fully%2520node-centric%2520mechanism%2520implemented%2520via%2520a%2520custom%2520fused%2520Triton%2520kernel.%2520By%2520eliminating%2520materialized%2520edge%2520tensors%2520and%2520maximizing%2520SRAM%2520utilization%252C%2520our%2520kernel%2520achieves%2520a%2520%255Ctextbf%257B20%2524%255Ctimes%2524%2520improvement%2520in%2520TFLOPS%257D%2520compared%2520to%2520standard%2520implementations.%2520Extensive%2520experiments%2520on%2520the%2520SPICE%2520and%2520OMol25%2520datasets%2520demonstrate%2520that%2520E2Former-V2%2520maintains%2520comparable%2520predictive%2520performance%2520while%2520notably%2520accelerating%2520inference.%2520This%2520work%2520demonstrates%2520that%2520large%2520equivariant%2520transformers%2520can%2520be%2520trained%2520efficiently%2520using%2520widely%2520accessible%2520GPU%2520platforms.%2520The%2520code%2520is%2520avalible%2520at%2520https%253A//github.com/IQuestLab/UBio-MolFM/tree/e2formerv2.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E2Former-V2%3A%20On-the-Fly%20Equivariant%20Attention%20with%20Linear%20Activation%20Memory&entry.906535625=Lin%20Huang%20and%20Chengxiang%20Huang%20and%20Ziang%20Wang%20and%20Yiyue%20Du%20and%20Chu%20Wang%20and%20Haocheng%20Lu%20and%20Yunyang%20Li%20and%20Xiaoli%20Liu%20and%20Arthur%20Jiang%20and%20Jia%20Zhang&entry.1292438233=Equivariant%20Graph%20Neural%20Networks%20%28EGNNs%29%20have%20become%20a%20widely%20used%20approach%20for%20modeling%203D%20atomistic%20systems.%20However%2C%20mainstream%20architectures%20face%20critical%20scalability%20bottlenecks%20due%20to%20the%20explicit%20construction%20of%20geometric%20features%20or%20dense%20tensor%20products%20on%20%5Ctextit%7Bevery%7D%20edge.%20To%20overcome%20this%2C%20we%20introduce%20%5Ctextbf%7BE2Former-V2%7D%2C%20a%20scalable%20architecture%20that%20integrates%20algebraic%20sparsity%20with%20hardware-aware%20execution.%20We%20first%20propose%20%5Ctextbf%7BE%7Dquivariant%20%5Ctextbf%7BA%7Dxis-%5Ctextbf%7BA%7Dligned%20%5Ctextbf%7BS%7Dparsification%20%28EAAS%29.%20EAAS%20builds%20on%20Wigner-%246j%24%20convolution%20by%20exploiting%20an%20%24%5Cmathrm%7BSO%7D%283%29%20%5Crightarrow%20%5Cmathrm%7BSO%7D%282%29%24%20change%20of%20basis%20to%20transform%20computationally%20expensive%20dense%20tensor%20contractions%20into%20efficient%2C%20sparse%20parity%20re-indexing%20operations.%20Building%20on%20this%20representation%2C%20we%20introduce%20%5Ctextbf%7BOn-the-Fly%20Equivariant%20Attention%7D%2C%20a%20fully%20node-centric%20mechanism%20implemented%20via%20a%20custom%20fused%20Triton%20kernel.%20By%20eliminating%20materialized%20edge%20tensors%20and%20maximizing%20SRAM%20utilization%2C%20our%20kernel%20achieves%20a%20%5Ctextbf%7B20%24%5Ctimes%24%20improvement%20in%20TFLOPS%7D%20compared%20to%20standard%20implementations.%20Extensive%20experiments%20on%20the%20SPICE%20and%20OMol25%20datasets%20demonstrate%20that%20E2Former-V2%20maintains%20comparable%20predictive%20performance%20while%20notably%20accelerating%20inference.%20This%20work%20demonstrates%20that%20large%20equivariant%20transformers%20can%20be%20trained%20efficiently%20using%20widely%20accessible%20GPU%20platforms.%20The%20code%20is%20avalible%20at%20https%3A//github.com/IQuestLab/UBio-MolFM/tree/e2formerv2.&entry.1838667208=http%3A//arxiv.org/abs/2601.16622v1&entry.124074799=Read"},
{"title": "UrbanIng-V2X: A Large-Scale Multi-Vehicle, Multi-Infrastructure Dataset Across Multiple Intersections for Cooperative Perception", "author": "Karthikeyan Chandra Sekaran and Markus Geisler and Dominik R\u00f6\u00dfle and Adithya Mohan and Daniel Cremers and Wolfgang Utschick and Michael Botsch and Werner Huber and Torsten Sch\u00f6n", "abstract": "Recent cooperative perception datasets have played a crucial role in advancing smart mobility applications by enabling information exchange between intelligent agents, helping to overcome challenges such as occlusions and improving overall scene understanding. While some existing real-world datasets incorporate both vehicle-to-vehicle and vehicle-to-infrastructure interactions, they are typically limited to a single intersection or a single vehicle. A comprehensive perception dataset featuring multiple connected vehicles and infrastructure sensors across several intersections remains unavailable, limiting the benchmarking of algorithms in diverse traffic environments. Consequently, overfitting can occur, and models may demonstrate misleadingly high performance due to similar intersection layouts and traffic participant behavior. To address this gap, we introduce UrbanIng-V2X, the first large-scale, multi-modal dataset supporting cooperative perception involving vehicles and infrastructure sensors deployed across three urban intersections in Ingolstadt, Germany. UrbanIng-V2X consists of 34 temporally aligned and spatially calibrated sensor sequences, each lasting 20 seconds. All sequences contain recordings from one of three intersections, involving two vehicles and up to three infrastructure-mounted sensor poles operating in coordinated scenarios. In total, UrbanIng-V2X provides data from 12 vehicle-mounted RGB cameras, 2 vehicle LiDARs, 17 infrastructure thermal cameras, and 12 infrastructure LiDARs. All sequences are annotated at a frequency of 10 Hz with 3D bounding boxes spanning 13 object classes, resulting in approximately 712k annotated instances across the dataset. We provide comprehensive evaluations using state-of-the-art cooperative perception methods and publicly release the codebase, dataset, HD map, and a digital twin of the complete data collection environment.", "link": "http://arxiv.org/abs/2510.23478v2", "date": "2026-01-23", "relevancy": 2.6158, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5521}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5087}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UrbanIng-V2X%3A%20A%20Large-Scale%20Multi-Vehicle%2C%20Multi-Infrastructure%20Dataset%20Across%20Multiple%20Intersections%20for%20Cooperative%20Perception&body=Title%3A%20UrbanIng-V2X%3A%20A%20Large-Scale%20Multi-Vehicle%2C%20Multi-Infrastructure%20Dataset%20Across%20Multiple%20Intersections%20for%20Cooperative%20Perception%0AAuthor%3A%20Karthikeyan%20Chandra%20Sekaran%20and%20Markus%20Geisler%20and%20Dominik%20R%C3%B6%C3%9Fle%20and%20Adithya%20Mohan%20and%20Daniel%20Cremers%20and%20Wolfgang%20Utschick%20and%20Michael%20Botsch%20and%20Werner%20Huber%20and%20Torsten%20Sch%C3%B6n%0AAbstract%3A%20Recent%20cooperative%20perception%20datasets%20have%20played%20a%20crucial%20role%20in%20advancing%20smart%20mobility%20applications%20by%20enabling%20information%20exchange%20between%20intelligent%20agents%2C%20helping%20to%20overcome%20challenges%20such%20as%20occlusions%20and%20improving%20overall%20scene%20understanding.%20While%20some%20existing%20real-world%20datasets%20incorporate%20both%20vehicle-to-vehicle%20and%20vehicle-to-infrastructure%20interactions%2C%20they%20are%20typically%20limited%20to%20a%20single%20intersection%20or%20a%20single%20vehicle.%20A%20comprehensive%20perception%20dataset%20featuring%20multiple%20connected%20vehicles%20and%20infrastructure%20sensors%20across%20several%20intersections%20remains%20unavailable%2C%20limiting%20the%20benchmarking%20of%20algorithms%20in%20diverse%20traffic%20environments.%20Consequently%2C%20overfitting%20can%20occur%2C%20and%20models%20may%20demonstrate%20misleadingly%20high%20performance%20due%20to%20similar%20intersection%20layouts%20and%20traffic%20participant%20behavior.%20To%20address%20this%20gap%2C%20we%20introduce%20UrbanIng-V2X%2C%20the%20first%20large-scale%2C%20multi-modal%20dataset%20supporting%20cooperative%20perception%20involving%20vehicles%20and%20infrastructure%20sensors%20deployed%20across%20three%20urban%20intersections%20in%20Ingolstadt%2C%20Germany.%20UrbanIng-V2X%20consists%20of%2034%20temporally%20aligned%20and%20spatially%20calibrated%20sensor%20sequences%2C%20each%20lasting%2020%20seconds.%20All%20sequences%20contain%20recordings%20from%20one%20of%20three%20intersections%2C%20involving%20two%20vehicles%20and%20up%20to%20three%20infrastructure-mounted%20sensor%20poles%20operating%20in%20coordinated%20scenarios.%20In%20total%2C%20UrbanIng-V2X%20provides%20data%20from%2012%20vehicle-mounted%20RGB%20cameras%2C%202%20vehicle%20LiDARs%2C%2017%20infrastructure%20thermal%20cameras%2C%20and%2012%20infrastructure%20LiDARs.%20All%20sequences%20are%20annotated%20at%20a%20frequency%20of%2010%20Hz%20with%203D%20bounding%20boxes%20spanning%2013%20object%20classes%2C%20resulting%20in%20approximately%20712k%20annotated%20instances%20across%20the%20dataset.%20We%20provide%20comprehensive%20evaluations%20using%20state-of-the-art%20cooperative%20perception%20methods%20and%20publicly%20release%20the%20codebase%2C%20dataset%2C%20HD%20map%2C%20and%20a%20digital%20twin%20of%20the%20complete%20data%20collection%20environment.%0ALink%3A%20http%3A//arxiv.org/abs/2510.23478v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrbanIng-V2X%253A%2520A%2520Large-Scale%2520Multi-Vehicle%252C%2520Multi-Infrastructure%2520Dataset%2520Across%2520Multiple%2520Intersections%2520for%2520Cooperative%2520Perception%26entry.906535625%3DKarthikeyan%2520Chandra%2520Sekaran%2520and%2520Markus%2520Geisler%2520and%2520Dominik%2520R%25C3%25B6%25C3%259Fle%2520and%2520Adithya%2520Mohan%2520and%2520Daniel%2520Cremers%2520and%2520Wolfgang%2520Utschick%2520and%2520Michael%2520Botsch%2520and%2520Werner%2520Huber%2520and%2520Torsten%2520Sch%25C3%25B6n%26entry.1292438233%3DRecent%2520cooperative%2520perception%2520datasets%2520have%2520played%2520a%2520crucial%2520role%2520in%2520advancing%2520smart%2520mobility%2520applications%2520by%2520enabling%2520information%2520exchange%2520between%2520intelligent%2520agents%252C%2520helping%2520to%2520overcome%2520challenges%2520such%2520as%2520occlusions%2520and%2520improving%2520overall%2520scene%2520understanding.%2520While%2520some%2520existing%2520real-world%2520datasets%2520incorporate%2520both%2520vehicle-to-vehicle%2520and%2520vehicle-to-infrastructure%2520interactions%252C%2520they%2520are%2520typically%2520limited%2520to%2520a%2520single%2520intersection%2520or%2520a%2520single%2520vehicle.%2520A%2520comprehensive%2520perception%2520dataset%2520featuring%2520multiple%2520connected%2520vehicles%2520and%2520infrastructure%2520sensors%2520across%2520several%2520intersections%2520remains%2520unavailable%252C%2520limiting%2520the%2520benchmarking%2520of%2520algorithms%2520in%2520diverse%2520traffic%2520environments.%2520Consequently%252C%2520overfitting%2520can%2520occur%252C%2520and%2520models%2520may%2520demonstrate%2520misleadingly%2520high%2520performance%2520due%2520to%2520similar%2520intersection%2520layouts%2520and%2520traffic%2520participant%2520behavior.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520UrbanIng-V2X%252C%2520the%2520first%2520large-scale%252C%2520multi-modal%2520dataset%2520supporting%2520cooperative%2520perception%2520involving%2520vehicles%2520and%2520infrastructure%2520sensors%2520deployed%2520across%2520three%2520urban%2520intersections%2520in%2520Ingolstadt%252C%2520Germany.%2520UrbanIng-V2X%2520consists%2520of%252034%2520temporally%2520aligned%2520and%2520spatially%2520calibrated%2520sensor%2520sequences%252C%2520each%2520lasting%252020%2520seconds.%2520All%2520sequences%2520contain%2520recordings%2520from%2520one%2520of%2520three%2520intersections%252C%2520involving%2520two%2520vehicles%2520and%2520up%2520to%2520three%2520infrastructure-mounted%2520sensor%2520poles%2520operating%2520in%2520coordinated%2520scenarios.%2520In%2520total%252C%2520UrbanIng-V2X%2520provides%2520data%2520from%252012%2520vehicle-mounted%2520RGB%2520cameras%252C%25202%2520vehicle%2520LiDARs%252C%252017%2520infrastructure%2520thermal%2520cameras%252C%2520and%252012%2520infrastructure%2520LiDARs.%2520All%2520sequences%2520are%2520annotated%2520at%2520a%2520frequency%2520of%252010%2520Hz%2520with%25203D%2520bounding%2520boxes%2520spanning%252013%2520object%2520classes%252C%2520resulting%2520in%2520approximately%2520712k%2520annotated%2520instances%2520across%2520the%2520dataset.%2520We%2520provide%2520comprehensive%2520evaluations%2520using%2520state-of-the-art%2520cooperative%2520perception%2520methods%2520and%2520publicly%2520release%2520the%2520codebase%252C%2520dataset%252C%2520HD%2520map%252C%2520and%2520a%2520digital%2520twin%2520of%2520the%2520complete%2520data%2520collection%2520environment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.23478v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UrbanIng-V2X%3A%20A%20Large-Scale%20Multi-Vehicle%2C%20Multi-Infrastructure%20Dataset%20Across%20Multiple%20Intersections%20for%20Cooperative%20Perception&entry.906535625=Karthikeyan%20Chandra%20Sekaran%20and%20Markus%20Geisler%20and%20Dominik%20R%C3%B6%C3%9Fle%20and%20Adithya%20Mohan%20and%20Daniel%20Cremers%20and%20Wolfgang%20Utschick%20and%20Michael%20Botsch%20and%20Werner%20Huber%20and%20Torsten%20Sch%C3%B6n&entry.1292438233=Recent%20cooperative%20perception%20datasets%20have%20played%20a%20crucial%20role%20in%20advancing%20smart%20mobility%20applications%20by%20enabling%20information%20exchange%20between%20intelligent%20agents%2C%20helping%20to%20overcome%20challenges%20such%20as%20occlusions%20and%20improving%20overall%20scene%20understanding.%20While%20some%20existing%20real-world%20datasets%20incorporate%20both%20vehicle-to-vehicle%20and%20vehicle-to-infrastructure%20interactions%2C%20they%20are%20typically%20limited%20to%20a%20single%20intersection%20or%20a%20single%20vehicle.%20A%20comprehensive%20perception%20dataset%20featuring%20multiple%20connected%20vehicles%20and%20infrastructure%20sensors%20across%20several%20intersections%20remains%20unavailable%2C%20limiting%20the%20benchmarking%20of%20algorithms%20in%20diverse%20traffic%20environments.%20Consequently%2C%20overfitting%20can%20occur%2C%20and%20models%20may%20demonstrate%20misleadingly%20high%20performance%20due%20to%20similar%20intersection%20layouts%20and%20traffic%20participant%20behavior.%20To%20address%20this%20gap%2C%20we%20introduce%20UrbanIng-V2X%2C%20the%20first%20large-scale%2C%20multi-modal%20dataset%20supporting%20cooperative%20perception%20involving%20vehicles%20and%20infrastructure%20sensors%20deployed%20across%20three%20urban%20intersections%20in%20Ingolstadt%2C%20Germany.%20UrbanIng-V2X%20consists%20of%2034%20temporally%20aligned%20and%20spatially%20calibrated%20sensor%20sequences%2C%20each%20lasting%2020%20seconds.%20All%20sequences%20contain%20recordings%20from%20one%20of%20three%20intersections%2C%20involving%20two%20vehicles%20and%20up%20to%20three%20infrastructure-mounted%20sensor%20poles%20operating%20in%20coordinated%20scenarios.%20In%20total%2C%20UrbanIng-V2X%20provides%20data%20from%2012%20vehicle-mounted%20RGB%20cameras%2C%202%20vehicle%20LiDARs%2C%2017%20infrastructure%20thermal%20cameras%2C%20and%2012%20infrastructure%20LiDARs.%20All%20sequences%20are%20annotated%20at%20a%20frequency%20of%2010%20Hz%20with%203D%20bounding%20boxes%20spanning%2013%20object%20classes%2C%20resulting%20in%20approximately%20712k%20annotated%20instances%20across%20the%20dataset.%20We%20provide%20comprehensive%20evaluations%20using%20state-of-the-art%20cooperative%20perception%20methods%20and%20publicly%20release%20the%20codebase%2C%20dataset%2C%20HD%20map%2C%20and%20a%20digital%20twin%20of%20the%20complete%20data%20collection%20environment.&entry.1838667208=http%3A//arxiv.org/abs/2510.23478v2&entry.124074799=Read"},
{"title": "Multigrade Neural Network Approximation", "author": "Shijun Zhang and Zuowei Shen and Yuesheng Xu", "abstract": "We study multigrade deep learning (MGDL) as a principled framework for structured error refinement in deep neural networks. While the approximation power of neural networks is now relatively well understood, training very deep architectures remains challenging due to highly non-convex and often ill-conditioned optimization landscapes. In contrast, for relatively shallow networks, most notably one-hidden-layer $\\texttt{ReLU}$ models, training admits convex reformulations with global guarantees, motivating learning paradigms that improve stability while scaling to depth. MGDL builds upon this insight by training deep networks grade by grade: previously learned grades are frozen, and each new residual block is trained solely to reduce the remaining approximation error, yielding an interpretable and stable hierarchical refinement process. We develop an operator-theoretic foundation for MGDL and prove that, for any continuous target function, there exists a fixed-width multigrade $\\texttt{ReLU}$ scheme whose residuals decrease strictly across grades and converge uniformly to zero. To the best of our knowledge, this work provides the first rigorous theoretical guarantee that grade-wise training yields provable vanishing approximation error in deep networks. Numerical experiments further illustrate the theoretical results.", "link": "http://arxiv.org/abs/2601.16884v1", "date": "2026-01-23", "relevancy": 2.6035, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5534}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5116}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multigrade%20Neural%20Network%20Approximation&body=Title%3A%20Multigrade%20Neural%20Network%20Approximation%0AAuthor%3A%20Shijun%20Zhang%20and%20Zuowei%20Shen%20and%20Yuesheng%20Xu%0AAbstract%3A%20We%20study%20multigrade%20deep%20learning%20%28MGDL%29%20as%20a%20principled%20framework%20for%20structured%20error%20refinement%20in%20deep%20neural%20networks.%20While%20the%20approximation%20power%20of%20neural%20networks%20is%20now%20relatively%20well%20understood%2C%20training%20very%20deep%20architectures%20remains%20challenging%20due%20to%20highly%20non-convex%20and%20often%20ill-conditioned%20optimization%20landscapes.%20In%20contrast%2C%20for%20relatively%20shallow%20networks%2C%20most%20notably%20one-hidden-layer%20%24%5Ctexttt%7BReLU%7D%24%20models%2C%20training%20admits%20convex%20reformulations%20with%20global%20guarantees%2C%20motivating%20learning%20paradigms%20that%20improve%20stability%20while%20scaling%20to%20depth.%20MGDL%20builds%20upon%20this%20insight%20by%20training%20deep%20networks%20grade%20by%20grade%3A%20previously%20learned%20grades%20are%20frozen%2C%20and%20each%20new%20residual%20block%20is%20trained%20solely%20to%20reduce%20the%20remaining%20approximation%20error%2C%20yielding%20an%20interpretable%20and%20stable%20hierarchical%20refinement%20process.%20We%20develop%20an%20operator-theoretic%20foundation%20for%20MGDL%20and%20prove%20that%2C%20for%20any%20continuous%20target%20function%2C%20there%20exists%20a%20fixed-width%20multigrade%20%24%5Ctexttt%7BReLU%7D%24%20scheme%20whose%20residuals%20decrease%20strictly%20across%20grades%20and%20converge%20uniformly%20to%20zero.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20provides%20the%20first%20rigorous%20theoretical%20guarantee%20that%20grade-wise%20training%20yields%20provable%20vanishing%20approximation%20error%20in%20deep%20networks.%20Numerical%20experiments%20further%20illustrate%20the%20theoretical%20results.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16884v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultigrade%2520Neural%2520Network%2520Approximation%26entry.906535625%3DShijun%2520Zhang%2520and%2520Zuowei%2520Shen%2520and%2520Yuesheng%2520Xu%26entry.1292438233%3DWe%2520study%2520multigrade%2520deep%2520learning%2520%2528MGDL%2529%2520as%2520a%2520principled%2520framework%2520for%2520structured%2520error%2520refinement%2520in%2520deep%2520neural%2520networks.%2520While%2520the%2520approximation%2520power%2520of%2520neural%2520networks%2520is%2520now%2520relatively%2520well%2520understood%252C%2520training%2520very%2520deep%2520architectures%2520remains%2520challenging%2520due%2520to%2520highly%2520non-convex%2520and%2520often%2520ill-conditioned%2520optimization%2520landscapes.%2520In%2520contrast%252C%2520for%2520relatively%2520shallow%2520networks%252C%2520most%2520notably%2520one-hidden-layer%2520%2524%255Ctexttt%257BReLU%257D%2524%2520models%252C%2520training%2520admits%2520convex%2520reformulations%2520with%2520global%2520guarantees%252C%2520motivating%2520learning%2520paradigms%2520that%2520improve%2520stability%2520while%2520scaling%2520to%2520depth.%2520MGDL%2520builds%2520upon%2520this%2520insight%2520by%2520training%2520deep%2520networks%2520grade%2520by%2520grade%253A%2520previously%2520learned%2520grades%2520are%2520frozen%252C%2520and%2520each%2520new%2520residual%2520block%2520is%2520trained%2520solely%2520to%2520reduce%2520the%2520remaining%2520approximation%2520error%252C%2520yielding%2520an%2520interpretable%2520and%2520stable%2520hierarchical%2520refinement%2520process.%2520We%2520develop%2520an%2520operator-theoretic%2520foundation%2520for%2520MGDL%2520and%2520prove%2520that%252C%2520for%2520any%2520continuous%2520target%2520function%252C%2520there%2520exists%2520a%2520fixed-width%2520multigrade%2520%2524%255Ctexttt%257BReLU%257D%2524%2520scheme%2520whose%2520residuals%2520decrease%2520strictly%2520across%2520grades%2520and%2520converge%2520uniformly%2520to%2520zero.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520work%2520provides%2520the%2520first%2520rigorous%2520theoretical%2520guarantee%2520that%2520grade-wise%2520training%2520yields%2520provable%2520vanishing%2520approximation%2520error%2520in%2520deep%2520networks.%2520Numerical%2520experiments%2520further%2520illustrate%2520the%2520theoretical%2520results.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16884v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multigrade%20Neural%20Network%20Approximation&entry.906535625=Shijun%20Zhang%20and%20Zuowei%20Shen%20and%20Yuesheng%20Xu&entry.1292438233=We%20study%20multigrade%20deep%20learning%20%28MGDL%29%20as%20a%20principled%20framework%20for%20structured%20error%20refinement%20in%20deep%20neural%20networks.%20While%20the%20approximation%20power%20of%20neural%20networks%20is%20now%20relatively%20well%20understood%2C%20training%20very%20deep%20architectures%20remains%20challenging%20due%20to%20highly%20non-convex%20and%20often%20ill-conditioned%20optimization%20landscapes.%20In%20contrast%2C%20for%20relatively%20shallow%20networks%2C%20most%20notably%20one-hidden-layer%20%24%5Ctexttt%7BReLU%7D%24%20models%2C%20training%20admits%20convex%20reformulations%20with%20global%20guarantees%2C%20motivating%20learning%20paradigms%20that%20improve%20stability%20while%20scaling%20to%20depth.%20MGDL%20builds%20upon%20this%20insight%20by%20training%20deep%20networks%20grade%20by%20grade%3A%20previously%20learned%20grades%20are%20frozen%2C%20and%20each%20new%20residual%20block%20is%20trained%20solely%20to%20reduce%20the%20remaining%20approximation%20error%2C%20yielding%20an%20interpretable%20and%20stable%20hierarchical%20refinement%20process.%20We%20develop%20an%20operator-theoretic%20foundation%20for%20MGDL%20and%20prove%20that%2C%20for%20any%20continuous%20target%20function%2C%20there%20exists%20a%20fixed-width%20multigrade%20%24%5Ctexttt%7BReLU%7D%24%20scheme%20whose%20residuals%20decrease%20strictly%20across%20grades%20and%20converge%20uniformly%20to%20zero.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20provides%20the%20first%20rigorous%20theoretical%20guarantee%20that%20grade-wise%20training%20yields%20provable%20vanishing%20approximation%20error%20in%20deep%20networks.%20Numerical%20experiments%20further%20illustrate%20the%20theoretical%20results.&entry.1838667208=http%3A//arxiv.org/abs/2601.16884v1&entry.124074799=Read"},
{"title": "AnyView: Synthesizing Any Novel View in Dynamic Scenes", "author": "Basile Van Hoorick and Dian Chen and Shun Iwase and Pavel Tokmakov and Muhammad Zubair Irshad and Igor Vasiljevic and Swati Gupta and Fangzhou Cheng and Sergey Zakharov and Vitor Campagnolo Guizilini", "abstract": "Modern generative video models excel at producing convincing, high-quality outputs, but struggle to maintain multi-view and spatiotemporal consistency in highly dynamic real-world environments. In this work, we introduce \\textbf{AnyView}, a diffusion-based video generation framework for \\emph{dynamic view synthesis} with minimal inductive biases or geometric assumptions. We leverage multiple data sources with various levels of supervision, including monocular (2D), multi-view static (3D) and multi-view dynamic (4D) datasets, to train a generalist spatiotemporal implicit representation capable of producing zero-shot novel videos from arbitrary camera locations and trajectories. We evaluate AnyView on standard benchmarks, showing competitive results with the current state of the art, and propose \\textbf{AnyViewBench}, a challenging new benchmark tailored towards \\emph{extreme} dynamic view synthesis in diverse real-world scenarios. In this more dramatic setting, we find that most baselines drastically degrade in performance, as they require significant overlap between viewpoints, while AnyView maintains the ability to produce realistic, plausible, and spatiotemporally consistent videos when prompted from \\emph{any} viewpoint. Results, data, code, and models can be viewed at: https://tri-ml.github.io/AnyView/", "link": "http://arxiv.org/abs/2601.16982v1", "date": "2026-01-23", "relevancy": 2.5815, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6482}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6482}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnyView%3A%20Synthesizing%20Any%20Novel%20View%20in%20Dynamic%20Scenes&body=Title%3A%20AnyView%3A%20Synthesizing%20Any%20Novel%20View%20in%20Dynamic%20Scenes%0AAuthor%3A%20Basile%20Van%20Hoorick%20and%20Dian%20Chen%20and%20Shun%20Iwase%20and%20Pavel%20Tokmakov%20and%20Muhammad%20Zubair%20Irshad%20and%20Igor%20Vasiljevic%20and%20Swati%20Gupta%20and%20Fangzhou%20Cheng%20and%20Sergey%20Zakharov%20and%20Vitor%20Campagnolo%20Guizilini%0AAbstract%3A%20Modern%20generative%20video%20models%20excel%20at%20producing%20convincing%2C%20high-quality%20outputs%2C%20but%20struggle%20to%20maintain%20multi-view%20and%20spatiotemporal%20consistency%20in%20highly%20dynamic%20real-world%20environments.%20In%20this%20work%2C%20we%20introduce%20%5Ctextbf%7BAnyView%7D%2C%20a%20diffusion-based%20video%20generation%20framework%20for%20%5Cemph%7Bdynamic%20view%20synthesis%7D%20with%20minimal%20inductive%20biases%20or%20geometric%20assumptions.%20We%20leverage%20multiple%20data%20sources%20with%20various%20levels%20of%20supervision%2C%20including%20monocular%20%282D%29%2C%20multi-view%20static%20%283D%29%20and%20multi-view%20dynamic%20%284D%29%20datasets%2C%20to%20train%20a%20generalist%20spatiotemporal%20implicit%20representation%20capable%20of%20producing%20zero-shot%20novel%20videos%20from%20arbitrary%20camera%20locations%20and%20trajectories.%20We%20evaluate%20AnyView%20on%20standard%20benchmarks%2C%20showing%20competitive%20results%20with%20the%20current%20state%20of%20the%20art%2C%20and%20propose%20%5Ctextbf%7BAnyViewBench%7D%2C%20a%20challenging%20new%20benchmark%20tailored%20towards%20%5Cemph%7Bextreme%7D%20dynamic%20view%20synthesis%20in%20diverse%20real-world%20scenarios.%20In%20this%20more%20dramatic%20setting%2C%20we%20find%20that%20most%20baselines%20drastically%20degrade%20in%20performance%2C%20as%20they%20require%20significant%20overlap%20between%20viewpoints%2C%20while%20AnyView%20maintains%20the%20ability%20to%20produce%20realistic%2C%20plausible%2C%20and%20spatiotemporally%20consistent%20videos%20when%20prompted%20from%20%5Cemph%7Bany%7D%20viewpoint.%20Results%2C%20data%2C%20code%2C%20and%20models%20can%20be%20viewed%20at%3A%20https%3A//tri-ml.github.io/AnyView/%0ALink%3A%20http%3A//arxiv.org/abs/2601.16982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnyView%253A%2520Synthesizing%2520Any%2520Novel%2520View%2520in%2520Dynamic%2520Scenes%26entry.906535625%3DBasile%2520Van%2520Hoorick%2520and%2520Dian%2520Chen%2520and%2520Shun%2520Iwase%2520and%2520Pavel%2520Tokmakov%2520and%2520Muhammad%2520Zubair%2520Irshad%2520and%2520Igor%2520Vasiljevic%2520and%2520Swati%2520Gupta%2520and%2520Fangzhou%2520Cheng%2520and%2520Sergey%2520Zakharov%2520and%2520Vitor%2520Campagnolo%2520Guizilini%26entry.1292438233%3DModern%2520generative%2520video%2520models%2520excel%2520at%2520producing%2520convincing%252C%2520high-quality%2520outputs%252C%2520but%2520struggle%2520to%2520maintain%2520multi-view%2520and%2520spatiotemporal%2520consistency%2520in%2520highly%2520dynamic%2520real-world%2520environments.%2520In%2520this%2520work%252C%2520we%2520introduce%2520%255Ctextbf%257BAnyView%257D%252C%2520a%2520diffusion-based%2520video%2520generation%2520framework%2520for%2520%255Cemph%257Bdynamic%2520view%2520synthesis%257D%2520with%2520minimal%2520inductive%2520biases%2520or%2520geometric%2520assumptions.%2520We%2520leverage%2520multiple%2520data%2520sources%2520with%2520various%2520levels%2520of%2520supervision%252C%2520including%2520monocular%2520%25282D%2529%252C%2520multi-view%2520static%2520%25283D%2529%2520and%2520multi-view%2520dynamic%2520%25284D%2529%2520datasets%252C%2520to%2520train%2520a%2520generalist%2520spatiotemporal%2520implicit%2520representation%2520capable%2520of%2520producing%2520zero-shot%2520novel%2520videos%2520from%2520arbitrary%2520camera%2520locations%2520and%2520trajectories.%2520We%2520evaluate%2520AnyView%2520on%2520standard%2520benchmarks%252C%2520showing%2520competitive%2520results%2520with%2520the%2520current%2520state%2520of%2520the%2520art%252C%2520and%2520propose%2520%255Ctextbf%257BAnyViewBench%257D%252C%2520a%2520challenging%2520new%2520benchmark%2520tailored%2520towards%2520%255Cemph%257Bextreme%257D%2520dynamic%2520view%2520synthesis%2520in%2520diverse%2520real-world%2520scenarios.%2520In%2520this%2520more%2520dramatic%2520setting%252C%2520we%2520find%2520that%2520most%2520baselines%2520drastically%2520degrade%2520in%2520performance%252C%2520as%2520they%2520require%2520significant%2520overlap%2520between%2520viewpoints%252C%2520while%2520AnyView%2520maintains%2520the%2520ability%2520to%2520produce%2520realistic%252C%2520plausible%252C%2520and%2520spatiotemporally%2520consistent%2520videos%2520when%2520prompted%2520from%2520%255Cemph%257Bany%257D%2520viewpoint.%2520Results%252C%2520data%252C%2520code%252C%2520and%2520models%2520can%2520be%2520viewed%2520at%253A%2520https%253A//tri-ml.github.io/AnyView/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyView%3A%20Synthesizing%20Any%20Novel%20View%20in%20Dynamic%20Scenes&entry.906535625=Basile%20Van%20Hoorick%20and%20Dian%20Chen%20and%20Shun%20Iwase%20and%20Pavel%20Tokmakov%20and%20Muhammad%20Zubair%20Irshad%20and%20Igor%20Vasiljevic%20and%20Swati%20Gupta%20and%20Fangzhou%20Cheng%20and%20Sergey%20Zakharov%20and%20Vitor%20Campagnolo%20Guizilini&entry.1292438233=Modern%20generative%20video%20models%20excel%20at%20producing%20convincing%2C%20high-quality%20outputs%2C%20but%20struggle%20to%20maintain%20multi-view%20and%20spatiotemporal%20consistency%20in%20highly%20dynamic%20real-world%20environments.%20In%20this%20work%2C%20we%20introduce%20%5Ctextbf%7BAnyView%7D%2C%20a%20diffusion-based%20video%20generation%20framework%20for%20%5Cemph%7Bdynamic%20view%20synthesis%7D%20with%20minimal%20inductive%20biases%20or%20geometric%20assumptions.%20We%20leverage%20multiple%20data%20sources%20with%20various%20levels%20of%20supervision%2C%20including%20monocular%20%282D%29%2C%20multi-view%20static%20%283D%29%20and%20multi-view%20dynamic%20%284D%29%20datasets%2C%20to%20train%20a%20generalist%20spatiotemporal%20implicit%20representation%20capable%20of%20producing%20zero-shot%20novel%20videos%20from%20arbitrary%20camera%20locations%20and%20trajectories.%20We%20evaluate%20AnyView%20on%20standard%20benchmarks%2C%20showing%20competitive%20results%20with%20the%20current%20state%20of%20the%20art%2C%20and%20propose%20%5Ctextbf%7BAnyViewBench%7D%2C%20a%20challenging%20new%20benchmark%20tailored%20towards%20%5Cemph%7Bextreme%7D%20dynamic%20view%20synthesis%20in%20diverse%20real-world%20scenarios.%20In%20this%20more%20dramatic%20setting%2C%20we%20find%20that%20most%20baselines%20drastically%20degrade%20in%20performance%2C%20as%20they%20require%20significant%20overlap%20between%20viewpoints%2C%20while%20AnyView%20maintains%20the%20ability%20to%20produce%20realistic%2C%20plausible%2C%20and%20spatiotemporally%20consistent%20videos%20when%20prompted%20from%20%5Cemph%7Bany%7D%20viewpoint.%20Results%2C%20data%2C%20code%2C%20and%20models%20can%20be%20viewed%20at%3A%20https%3A//tri-ml.github.io/AnyView/&entry.1838667208=http%3A//arxiv.org/abs/2601.16982v1&entry.124074799=Read"},
{"title": "Gradient-Based Neuroplastic Adaptation for Concurrent Optimization of Neuro-Fuzzy Networks", "author": "John Wesley Hostetter and Min Chi", "abstract": "Neuro-fuzzy networks (NFNs) are transparent, symbolic, and universal function approximations that perform as well as conventional neural architectures, but their knowledge is expressed as linguistic IF-THEN rules. Despite these advantages, their systematic design process remains a challenge. Existing work will often sequentially build NFNs by inefficiently isolating parametric and structural identification, leading to a premature commitment to brittle and subpar architecture. We propose a novel application-independent approach called gradient-based neuroplastic adaptation for the concurrent optimization of NFNs' parameters and structure. By recognizing that NFNs' parameters and structure should be optimized simultaneously as they are deeply conjoined, settings previously unapproachable for NFNs are now accessible, such as the online reinforcement learning of NFNs for vision-based tasks. The effectiveness of concurrently optimizing NFNs is empirically shown as it is trained by online reinforcement learning to proficiently play challenging scenarios from a vision-based video game called DOOM.", "link": "http://arxiv.org/abs/2506.21771v2", "date": "2026-01-23", "relevancy": 2.5691, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5713}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4905}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient-Based%20Neuroplastic%20Adaptation%20for%20Concurrent%20Optimization%20of%20Neuro-Fuzzy%20Networks&body=Title%3A%20Gradient-Based%20Neuroplastic%20Adaptation%20for%20Concurrent%20Optimization%20of%20Neuro-Fuzzy%20Networks%0AAuthor%3A%20John%20Wesley%20Hostetter%20and%20Min%20Chi%0AAbstract%3A%20Neuro-fuzzy%20networks%20%28NFNs%29%20are%20transparent%2C%20symbolic%2C%20and%20universal%20function%20approximations%20that%20perform%20as%20well%20as%20conventional%20neural%20architectures%2C%20but%20their%20knowledge%20is%20expressed%20as%20linguistic%20IF-THEN%20rules.%20Despite%20these%20advantages%2C%20their%20systematic%20design%20process%20remains%20a%20challenge.%20Existing%20work%20will%20often%20sequentially%20build%20NFNs%20by%20inefficiently%20isolating%20parametric%20and%20structural%20identification%2C%20leading%20to%20a%20premature%20commitment%20to%20brittle%20and%20subpar%20architecture.%20We%20propose%20a%20novel%20application-independent%20approach%20called%20gradient-based%20neuroplastic%20adaptation%20for%20the%20concurrent%20optimization%20of%20NFNs%27%20parameters%20and%20structure.%20By%20recognizing%20that%20NFNs%27%20parameters%20and%20structure%20should%20be%20optimized%20simultaneously%20as%20they%20are%20deeply%20conjoined%2C%20settings%20previously%20unapproachable%20for%20NFNs%20are%20now%20accessible%2C%20such%20as%20the%20online%20reinforcement%20learning%20of%20NFNs%20for%20vision-based%20tasks.%20The%20effectiveness%20of%20concurrently%20optimizing%20NFNs%20is%20empirically%20shown%20as%20it%20is%20trained%20by%20online%20reinforcement%20learning%20to%20proficiently%20play%20challenging%20scenarios%20from%20a%20vision-based%20video%20game%20called%20DOOM.%0ALink%3A%20http%3A//arxiv.org/abs/2506.21771v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient-Based%2520Neuroplastic%2520Adaptation%2520for%2520Concurrent%2520Optimization%2520of%2520Neuro-Fuzzy%2520Networks%26entry.906535625%3DJohn%2520Wesley%2520Hostetter%2520and%2520Min%2520Chi%26entry.1292438233%3DNeuro-fuzzy%2520networks%2520%2528NFNs%2529%2520are%2520transparent%252C%2520symbolic%252C%2520and%2520universal%2520function%2520approximations%2520that%2520perform%2520as%2520well%2520as%2520conventional%2520neural%2520architectures%252C%2520but%2520their%2520knowledge%2520is%2520expressed%2520as%2520linguistic%2520IF-THEN%2520rules.%2520Despite%2520these%2520advantages%252C%2520their%2520systematic%2520design%2520process%2520remains%2520a%2520challenge.%2520Existing%2520work%2520will%2520often%2520sequentially%2520build%2520NFNs%2520by%2520inefficiently%2520isolating%2520parametric%2520and%2520structural%2520identification%252C%2520leading%2520to%2520a%2520premature%2520commitment%2520to%2520brittle%2520and%2520subpar%2520architecture.%2520We%2520propose%2520a%2520novel%2520application-independent%2520approach%2520called%2520gradient-based%2520neuroplastic%2520adaptation%2520for%2520the%2520concurrent%2520optimization%2520of%2520NFNs%2527%2520parameters%2520and%2520structure.%2520By%2520recognizing%2520that%2520NFNs%2527%2520parameters%2520and%2520structure%2520should%2520be%2520optimized%2520simultaneously%2520as%2520they%2520are%2520deeply%2520conjoined%252C%2520settings%2520previously%2520unapproachable%2520for%2520NFNs%2520are%2520now%2520accessible%252C%2520such%2520as%2520the%2520online%2520reinforcement%2520learning%2520of%2520NFNs%2520for%2520vision-based%2520tasks.%2520The%2520effectiveness%2520of%2520concurrently%2520optimizing%2520NFNs%2520is%2520empirically%2520shown%2520as%2520it%2520is%2520trained%2520by%2520online%2520reinforcement%2520learning%2520to%2520proficiently%2520play%2520challenging%2520scenarios%2520from%2520a%2520vision-based%2520video%2520game%2520called%2520DOOM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21771v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-Based%20Neuroplastic%20Adaptation%20for%20Concurrent%20Optimization%20of%20Neuro-Fuzzy%20Networks&entry.906535625=John%20Wesley%20Hostetter%20and%20Min%20Chi&entry.1292438233=Neuro-fuzzy%20networks%20%28NFNs%29%20are%20transparent%2C%20symbolic%2C%20and%20universal%20function%20approximations%20that%20perform%20as%20well%20as%20conventional%20neural%20architectures%2C%20but%20their%20knowledge%20is%20expressed%20as%20linguistic%20IF-THEN%20rules.%20Despite%20these%20advantages%2C%20their%20systematic%20design%20process%20remains%20a%20challenge.%20Existing%20work%20will%20often%20sequentially%20build%20NFNs%20by%20inefficiently%20isolating%20parametric%20and%20structural%20identification%2C%20leading%20to%20a%20premature%20commitment%20to%20brittle%20and%20subpar%20architecture.%20We%20propose%20a%20novel%20application-independent%20approach%20called%20gradient-based%20neuroplastic%20adaptation%20for%20the%20concurrent%20optimization%20of%20NFNs%27%20parameters%20and%20structure.%20By%20recognizing%20that%20NFNs%27%20parameters%20and%20structure%20should%20be%20optimized%20simultaneously%20as%20they%20are%20deeply%20conjoined%2C%20settings%20previously%20unapproachable%20for%20NFNs%20are%20now%20accessible%2C%20such%20as%20the%20online%20reinforcement%20learning%20of%20NFNs%20for%20vision-based%20tasks.%20The%20effectiveness%20of%20concurrently%20optimizing%20NFNs%20is%20empirically%20shown%20as%20it%20is%20trained%20by%20online%20reinforcement%20learning%20to%20proficiently%20play%20challenging%20scenarios%20from%20a%20vision-based%20video%20game%20called%20DOOM.&entry.1838667208=http%3A//arxiv.org/abs/2506.21771v2&entry.124074799=Read"},
{"title": "Task Aware Dreamer for Task Generalization in Reinforcement Learning", "author": "Chengyang Ying and Xinning Zhou and Zhongkai Hao and Hang Su and Songming Liu and Dong Yan and Jun Zhu", "abstract": "A long-standing goal of reinforcement learning is to acquire agents that can learn on training tasks and generalize well on unseen tasks that may share a similar dynamic but with different reward functions. The ability to generalize across tasks is important as it determines an agent's adaptability to real-world scenarios where reward mechanisms might vary. In this work, we first show that training a general world model can utilize similar structures in these tasks and help train more generalizable agents. Extending world models into the task generalization setting, we introduce a novel method named Task Aware Dreamer (TAD), which integrates reward-informed features to identify consistent latent characteristics across tasks. Within TAD, we compute the variational lower bound of sample data log-likelihood, which introduces a new term designed to differentiate tasks using their states, as the optimization objective of our reward-informed world models. To demonstrate the advantages of the reward-informed policy in TAD, we introduce a new metric called Task Distribution Relevance (TDR) which quantitatively measures the relevance of different tasks. For tasks exhibiting a high TDR, i.e., the tasks differ significantly, we illustrate that Markovian policies struggle to distinguish them, thus it is necessary to utilize reward-informed policies in TAD. Extensive experiments in both image-based and state-based tasks show that TAD can significantly improve the performance of handling different tasks simultaneously, especially for those with high TDR, and display a strong generalization ability to unseen tasks.", "link": "http://arxiv.org/abs/2303.05092v5", "date": "2026-01-23", "relevancy": 2.5537, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5292}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5055}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task%20Aware%20Dreamer%20for%20Task%20Generalization%20in%20Reinforcement%20Learning&body=Title%3A%20Task%20Aware%20Dreamer%20for%20Task%20Generalization%20in%20Reinforcement%20Learning%0AAuthor%3A%20Chengyang%20Ying%20and%20Xinning%20Zhou%20and%20Zhongkai%20Hao%20and%20Hang%20Su%20and%20Songming%20Liu%20and%20Dong%20Yan%20and%20Jun%20Zhu%0AAbstract%3A%20A%20long-standing%20goal%20of%20reinforcement%20learning%20is%20to%20acquire%20agents%20that%20can%20learn%20on%20training%20tasks%20and%20generalize%20well%20on%20unseen%20tasks%20that%20may%20share%20a%20similar%20dynamic%20but%20with%20different%20reward%20functions.%20The%20ability%20to%20generalize%20across%20tasks%20is%20important%20as%20it%20determines%20an%20agent%27s%20adaptability%20to%20real-world%20scenarios%20where%20reward%20mechanisms%20might%20vary.%20In%20this%20work%2C%20we%20first%20show%20that%20training%20a%20general%20world%20model%20can%20utilize%20similar%20structures%20in%20these%20tasks%20and%20help%20train%20more%20generalizable%20agents.%20Extending%20world%20models%20into%20the%20task%20generalization%20setting%2C%20we%20introduce%20a%20novel%20method%20named%20Task%20Aware%20Dreamer%20%28TAD%29%2C%20which%20integrates%20reward-informed%20features%20to%20identify%20consistent%20latent%20characteristics%20across%20tasks.%20Within%20TAD%2C%20we%20compute%20the%20variational%20lower%20bound%20of%20sample%20data%20log-likelihood%2C%20which%20introduces%20a%20new%20term%20designed%20to%20differentiate%20tasks%20using%20their%20states%2C%20as%20the%20optimization%20objective%20of%20our%20reward-informed%20world%20models.%20To%20demonstrate%20the%20advantages%20of%20the%20reward-informed%20policy%20in%20TAD%2C%20we%20introduce%20a%20new%20metric%20called%20Task%20Distribution%20Relevance%20%28TDR%29%20which%20quantitatively%20measures%20the%20relevance%20of%20different%20tasks.%20For%20tasks%20exhibiting%20a%20high%20TDR%2C%20i.e.%2C%20the%20tasks%20differ%20significantly%2C%20we%20illustrate%20that%20Markovian%20policies%20struggle%20to%20distinguish%20them%2C%20thus%20it%20is%20necessary%20to%20utilize%20reward-informed%20policies%20in%20TAD.%20Extensive%20experiments%20in%20both%20image-based%20and%20state-based%20tasks%20show%20that%20TAD%20can%20significantly%20improve%20the%20performance%20of%20handling%20different%20tasks%20simultaneously%2C%20especially%20for%20those%20with%20high%20TDR%2C%20and%20display%20a%20strong%20generalization%20ability%20to%20unseen%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2303.05092v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask%2520Aware%2520Dreamer%2520for%2520Task%2520Generalization%2520in%2520Reinforcement%2520Learning%26entry.906535625%3DChengyang%2520Ying%2520and%2520Xinning%2520Zhou%2520and%2520Zhongkai%2520Hao%2520and%2520Hang%2520Su%2520and%2520Songming%2520Liu%2520and%2520Dong%2520Yan%2520and%2520Jun%2520Zhu%26entry.1292438233%3DA%2520long-standing%2520goal%2520of%2520reinforcement%2520learning%2520is%2520to%2520acquire%2520agents%2520that%2520can%2520learn%2520on%2520training%2520tasks%2520and%2520generalize%2520well%2520on%2520unseen%2520tasks%2520that%2520may%2520share%2520a%2520similar%2520dynamic%2520but%2520with%2520different%2520reward%2520functions.%2520The%2520ability%2520to%2520generalize%2520across%2520tasks%2520is%2520important%2520as%2520it%2520determines%2520an%2520agent%2527s%2520adaptability%2520to%2520real-world%2520scenarios%2520where%2520reward%2520mechanisms%2520might%2520vary.%2520In%2520this%2520work%252C%2520we%2520first%2520show%2520that%2520training%2520a%2520general%2520world%2520model%2520can%2520utilize%2520similar%2520structures%2520in%2520these%2520tasks%2520and%2520help%2520train%2520more%2520generalizable%2520agents.%2520Extending%2520world%2520models%2520into%2520the%2520task%2520generalization%2520setting%252C%2520we%2520introduce%2520a%2520novel%2520method%2520named%2520Task%2520Aware%2520Dreamer%2520%2528TAD%2529%252C%2520which%2520integrates%2520reward-informed%2520features%2520to%2520identify%2520consistent%2520latent%2520characteristics%2520across%2520tasks.%2520Within%2520TAD%252C%2520we%2520compute%2520the%2520variational%2520lower%2520bound%2520of%2520sample%2520data%2520log-likelihood%252C%2520which%2520introduces%2520a%2520new%2520term%2520designed%2520to%2520differentiate%2520tasks%2520using%2520their%2520states%252C%2520as%2520the%2520optimization%2520objective%2520of%2520our%2520reward-informed%2520world%2520models.%2520To%2520demonstrate%2520the%2520advantages%2520of%2520the%2520reward-informed%2520policy%2520in%2520TAD%252C%2520we%2520introduce%2520a%2520new%2520metric%2520called%2520Task%2520Distribution%2520Relevance%2520%2528TDR%2529%2520which%2520quantitatively%2520measures%2520the%2520relevance%2520of%2520different%2520tasks.%2520For%2520tasks%2520exhibiting%2520a%2520high%2520TDR%252C%2520i.e.%252C%2520the%2520tasks%2520differ%2520significantly%252C%2520we%2520illustrate%2520that%2520Markovian%2520policies%2520struggle%2520to%2520distinguish%2520them%252C%2520thus%2520it%2520is%2520necessary%2520to%2520utilize%2520reward-informed%2520policies%2520in%2520TAD.%2520Extensive%2520experiments%2520in%2520both%2520image-based%2520and%2520state-based%2520tasks%2520show%2520that%2520TAD%2520can%2520significantly%2520improve%2520the%2520performance%2520of%2520handling%2520different%2520tasks%2520simultaneously%252C%2520especially%2520for%2520those%2520with%2520high%2520TDR%252C%2520and%2520display%2520a%2520strong%2520generalization%2520ability%2520to%2520unseen%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.05092v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task%20Aware%20Dreamer%20for%20Task%20Generalization%20in%20Reinforcement%20Learning&entry.906535625=Chengyang%20Ying%20and%20Xinning%20Zhou%20and%20Zhongkai%20Hao%20and%20Hang%20Su%20and%20Songming%20Liu%20and%20Dong%20Yan%20and%20Jun%20Zhu&entry.1292438233=A%20long-standing%20goal%20of%20reinforcement%20learning%20is%20to%20acquire%20agents%20that%20can%20learn%20on%20training%20tasks%20and%20generalize%20well%20on%20unseen%20tasks%20that%20may%20share%20a%20similar%20dynamic%20but%20with%20different%20reward%20functions.%20The%20ability%20to%20generalize%20across%20tasks%20is%20important%20as%20it%20determines%20an%20agent%27s%20adaptability%20to%20real-world%20scenarios%20where%20reward%20mechanisms%20might%20vary.%20In%20this%20work%2C%20we%20first%20show%20that%20training%20a%20general%20world%20model%20can%20utilize%20similar%20structures%20in%20these%20tasks%20and%20help%20train%20more%20generalizable%20agents.%20Extending%20world%20models%20into%20the%20task%20generalization%20setting%2C%20we%20introduce%20a%20novel%20method%20named%20Task%20Aware%20Dreamer%20%28TAD%29%2C%20which%20integrates%20reward-informed%20features%20to%20identify%20consistent%20latent%20characteristics%20across%20tasks.%20Within%20TAD%2C%20we%20compute%20the%20variational%20lower%20bound%20of%20sample%20data%20log-likelihood%2C%20which%20introduces%20a%20new%20term%20designed%20to%20differentiate%20tasks%20using%20their%20states%2C%20as%20the%20optimization%20objective%20of%20our%20reward-informed%20world%20models.%20To%20demonstrate%20the%20advantages%20of%20the%20reward-informed%20policy%20in%20TAD%2C%20we%20introduce%20a%20new%20metric%20called%20Task%20Distribution%20Relevance%20%28TDR%29%20which%20quantitatively%20measures%20the%20relevance%20of%20different%20tasks.%20For%20tasks%20exhibiting%20a%20high%20TDR%2C%20i.e.%2C%20the%20tasks%20differ%20significantly%2C%20we%20illustrate%20that%20Markovian%20policies%20struggle%20to%20distinguish%20them%2C%20thus%20it%20is%20necessary%20to%20utilize%20reward-informed%20policies%20in%20TAD.%20Extensive%20experiments%20in%20both%20image-based%20and%20state-based%20tasks%20show%20that%20TAD%20can%20significantly%20improve%20the%20performance%20of%20handling%20different%20tasks%20simultaneously%2C%20especially%20for%20those%20with%20high%20TDR%2C%20and%20display%20a%20strong%20generalization%20ability%20to%20unseen%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2303.05092v5&entry.124074799=Read"},
{"title": "From Neck to Head: Bio-Impedance Sensing for Head Pose Estimation", "author": "Mengxi Liu and Lala Shakti Swarup Ray and Sizhen Bian and Ko Watanabe and Ankur Bhatt and Joanna Sorysz and Russel Torah and Bo Zhou and Paul Lukowicz", "abstract": "We present NeckSense, a novel wearable system for head pose tracking that leverages multi-channel bio-impedance sensing with soft, dry electrodes embedded in a lightweight, necklace-style form factor. NeckSense captures dynamic changes in tissue impedance around the neck, which are modulated by head rotations and subtle muscle activations. To robustly estimate head pose, we propose a deep learning framework that integrates anatomical priors, including joint constraints and natural head rotation ranges, into the loss function design. We validate NeckSense on 7 participants using the current SOTA pose estimation model as ground truth. Our system achieves a mean per-vertex error of 25.9 mm across various head movements with a leave-one-person-out cross-validation method, demonstrating that a compact, line-of-sight-free bio-impedance wearable can deliver head-tracking performance comparable to SOTA vision-based methods.", "link": "http://arxiv.org/abs/2507.12884v2", "date": "2026-01-23", "relevancy": 2.5437, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5145}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5058}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Neck%20to%20Head%3A%20Bio-Impedance%20Sensing%20for%20Head%20Pose%20Estimation&body=Title%3A%20From%20Neck%20to%20Head%3A%20Bio-Impedance%20Sensing%20for%20Head%20Pose%20Estimation%0AAuthor%3A%20Mengxi%20Liu%20and%20Lala%20Shakti%20Swarup%20Ray%20and%20Sizhen%20Bian%20and%20Ko%20Watanabe%20and%20Ankur%20Bhatt%20and%20Joanna%20Sorysz%20and%20Russel%20Torah%20and%20Bo%20Zhou%20and%20Paul%20Lukowicz%0AAbstract%3A%20We%20present%20NeckSense%2C%20a%20novel%20wearable%20system%20for%20head%20pose%20tracking%20that%20leverages%20multi-channel%20bio-impedance%20sensing%20with%20soft%2C%20dry%20electrodes%20embedded%20in%20a%20lightweight%2C%20necklace-style%20form%20factor.%20NeckSense%20captures%20dynamic%20changes%20in%20tissue%20impedance%20around%20the%20neck%2C%20which%20are%20modulated%20by%20head%20rotations%20and%20subtle%20muscle%20activations.%20To%20robustly%20estimate%20head%20pose%2C%20we%20propose%20a%20deep%20learning%20framework%20that%20integrates%20anatomical%20priors%2C%20including%20joint%20constraints%20and%20natural%20head%20rotation%20ranges%2C%20into%20the%20loss%20function%20design.%20We%20validate%20NeckSense%20on%207%20participants%20using%20the%20current%20SOTA%20pose%20estimation%20model%20as%20ground%20truth.%20Our%20system%20achieves%20a%20mean%20per-vertex%20error%20of%2025.9%20mm%20across%20various%20head%20movements%20with%20a%20leave-one-person-out%20cross-validation%20method%2C%20demonstrating%20that%20a%20compact%2C%20line-of-sight-free%20bio-impedance%20wearable%20can%20deliver%20head-tracking%20performance%20comparable%20to%20SOTA%20vision-based%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2507.12884v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Neck%2520to%2520Head%253A%2520Bio-Impedance%2520Sensing%2520for%2520Head%2520Pose%2520Estimation%26entry.906535625%3DMengxi%2520Liu%2520and%2520Lala%2520Shakti%2520Swarup%2520Ray%2520and%2520Sizhen%2520Bian%2520and%2520Ko%2520Watanabe%2520and%2520Ankur%2520Bhatt%2520and%2520Joanna%2520Sorysz%2520and%2520Russel%2520Torah%2520and%2520Bo%2520Zhou%2520and%2520Paul%2520Lukowicz%26entry.1292438233%3DWe%2520present%2520NeckSense%252C%2520a%2520novel%2520wearable%2520system%2520for%2520head%2520pose%2520tracking%2520that%2520leverages%2520multi-channel%2520bio-impedance%2520sensing%2520with%2520soft%252C%2520dry%2520electrodes%2520embedded%2520in%2520a%2520lightweight%252C%2520necklace-style%2520form%2520factor.%2520NeckSense%2520captures%2520dynamic%2520changes%2520in%2520tissue%2520impedance%2520around%2520the%2520neck%252C%2520which%2520are%2520modulated%2520by%2520head%2520rotations%2520and%2520subtle%2520muscle%2520activations.%2520To%2520robustly%2520estimate%2520head%2520pose%252C%2520we%2520propose%2520a%2520deep%2520learning%2520framework%2520that%2520integrates%2520anatomical%2520priors%252C%2520including%2520joint%2520constraints%2520and%2520natural%2520head%2520rotation%2520ranges%252C%2520into%2520the%2520loss%2520function%2520design.%2520We%2520validate%2520NeckSense%2520on%25207%2520participants%2520using%2520the%2520current%2520SOTA%2520pose%2520estimation%2520model%2520as%2520ground%2520truth.%2520Our%2520system%2520achieves%2520a%2520mean%2520per-vertex%2520error%2520of%252025.9%2520mm%2520across%2520various%2520head%2520movements%2520with%2520a%2520leave-one-person-out%2520cross-validation%2520method%252C%2520demonstrating%2520that%2520a%2520compact%252C%2520line-of-sight-free%2520bio-impedance%2520wearable%2520can%2520deliver%2520head-tracking%2520performance%2520comparable%2520to%2520SOTA%2520vision-based%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12884v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Neck%20to%20Head%3A%20Bio-Impedance%20Sensing%20for%20Head%20Pose%20Estimation&entry.906535625=Mengxi%20Liu%20and%20Lala%20Shakti%20Swarup%20Ray%20and%20Sizhen%20Bian%20and%20Ko%20Watanabe%20and%20Ankur%20Bhatt%20and%20Joanna%20Sorysz%20and%20Russel%20Torah%20and%20Bo%20Zhou%20and%20Paul%20Lukowicz&entry.1292438233=We%20present%20NeckSense%2C%20a%20novel%20wearable%20system%20for%20head%20pose%20tracking%20that%20leverages%20multi-channel%20bio-impedance%20sensing%20with%20soft%2C%20dry%20electrodes%20embedded%20in%20a%20lightweight%2C%20necklace-style%20form%20factor.%20NeckSense%20captures%20dynamic%20changes%20in%20tissue%20impedance%20around%20the%20neck%2C%20which%20are%20modulated%20by%20head%20rotations%20and%20subtle%20muscle%20activations.%20To%20robustly%20estimate%20head%20pose%2C%20we%20propose%20a%20deep%20learning%20framework%20that%20integrates%20anatomical%20priors%2C%20including%20joint%20constraints%20and%20natural%20head%20rotation%20ranges%2C%20into%20the%20loss%20function%20design.%20We%20validate%20NeckSense%20on%207%20participants%20using%20the%20current%20SOTA%20pose%20estimation%20model%20as%20ground%20truth.%20Our%20system%20achieves%20a%20mean%20per-vertex%20error%20of%2025.9%20mm%20across%20various%20head%20movements%20with%20a%20leave-one-person-out%20cross-validation%20method%2C%20demonstrating%20that%20a%20compact%2C%20line-of-sight-free%20bio-impedance%20wearable%20can%20deliver%20head-tracking%20performance%20comparable%20to%20SOTA%20vision-based%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2507.12884v2&entry.124074799=Read"},
{"title": "GTR-Mamba: Geometry-to-Tangent Routing Mamba for Hyperbolic POI Recommendation", "author": "Zhuoxuan Li and Jieyuan Pei and Tangwei Ye and Zhongyuan Lai and Zihan Liu and Fengyuan Xu and Qi Zhang and Liang Hu", "abstract": "Next Point-of-Interest (POI) recommendation is a critical task in modern Location-Based Social Networks (LBSNs), aiming to model the complex decision-making process of human mobility to provide personalized recommendations for a user's next check-in location. Existing hyperbolic POI recommendation models, predominantly based on rotations and graph representations, have been extensively investigated. Although hyperbolic geometry has proven superior in representing hierarchical data with low distortion, current hyperbolic sequence models typically rely on performing recurrence via expensive M\u00f6bius operations directly on the manifold. This incurs prohibitive computational costs and numerical instability, rendering them ill-suited for trajectory modeling. To resolve this conflict between geometric representational power and sequential efficiency, we propose GTR-Mamba, a novel framework featuring Geometry-to-Tangent Routing. GTR-Mamba strategically routes complex state transitions to the computationally efficient Euclidean tangent space. Crucially, instead of a static approximation, we introduce a Parallel Transport (PT) mechanism that dynamically aligns tangent spaces along the trajectory. This ensures geometric consistency across recursive updates, effectively bridging the gap between the curved manifold and linear tangent operations. This process is orchestrated by an exogenous spatio-temporal channel, which explicitly modulates the SSM discretization parameters. Extensive experiments on three real-world datasets demonstrate that GTR-Mamba consistently outperforms state-of-the-art baselines in next POI recommendation.", "link": "http://arxiv.org/abs/2510.22942v2", "date": "2026-01-23", "relevancy": 2.5348, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5217}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5103}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GTR-Mamba%3A%20Geometry-to-Tangent%20Routing%20Mamba%20for%20Hyperbolic%20POI%20Recommendation&body=Title%3A%20GTR-Mamba%3A%20Geometry-to-Tangent%20Routing%20Mamba%20for%20Hyperbolic%20POI%20Recommendation%0AAuthor%3A%20Zhuoxuan%20Li%20and%20Jieyuan%20Pei%20and%20Tangwei%20Ye%20and%20Zhongyuan%20Lai%20and%20Zihan%20Liu%20and%20Fengyuan%20Xu%20and%20Qi%20Zhang%20and%20Liang%20Hu%0AAbstract%3A%20Next%20Point-of-Interest%20%28POI%29%20recommendation%20is%20a%20critical%20task%20in%20modern%20Location-Based%20Social%20Networks%20%28LBSNs%29%2C%20aiming%20to%20model%20the%20complex%20decision-making%20process%20of%20human%20mobility%20to%20provide%20personalized%20recommendations%20for%20a%20user%27s%20next%20check-in%20location.%20Existing%20hyperbolic%20POI%20recommendation%20models%2C%20predominantly%20based%20on%20rotations%20and%20graph%20representations%2C%20have%20been%20extensively%20investigated.%20Although%20hyperbolic%20geometry%20has%20proven%20superior%20in%20representing%20hierarchical%20data%20with%20low%20distortion%2C%20current%20hyperbolic%20sequence%20models%20typically%20rely%20on%20performing%20recurrence%20via%20expensive%20M%C3%B6bius%20operations%20directly%20on%20the%20manifold.%20This%20incurs%20prohibitive%20computational%20costs%20and%20numerical%20instability%2C%20rendering%20them%20ill-suited%20for%20trajectory%20modeling.%20To%20resolve%20this%20conflict%20between%20geometric%20representational%20power%20and%20sequential%20efficiency%2C%20we%20propose%20GTR-Mamba%2C%20a%20novel%20framework%20featuring%20Geometry-to-Tangent%20Routing.%20GTR-Mamba%20strategically%20routes%20complex%20state%20transitions%20to%20the%20computationally%20efficient%20Euclidean%20tangent%20space.%20Crucially%2C%20instead%20of%20a%20static%20approximation%2C%20we%20introduce%20a%20Parallel%20Transport%20%28PT%29%20mechanism%20that%20dynamically%20aligns%20tangent%20spaces%20along%20the%20trajectory.%20This%20ensures%20geometric%20consistency%20across%20recursive%20updates%2C%20effectively%20bridging%20the%20gap%20between%20the%20curved%20manifold%20and%20linear%20tangent%20operations.%20This%20process%20is%20orchestrated%20by%20an%20exogenous%20spatio-temporal%20channel%2C%20which%20explicitly%20modulates%20the%20SSM%20discretization%20parameters.%20Extensive%20experiments%20on%20three%20real-world%20datasets%20demonstrate%20that%20GTR-Mamba%20consistently%20outperforms%20state-of-the-art%20baselines%20in%20next%20POI%20recommendation.%0ALink%3A%20http%3A//arxiv.org/abs/2510.22942v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGTR-Mamba%253A%2520Geometry-to-Tangent%2520Routing%2520Mamba%2520for%2520Hyperbolic%2520POI%2520Recommendation%26entry.906535625%3DZhuoxuan%2520Li%2520and%2520Jieyuan%2520Pei%2520and%2520Tangwei%2520Ye%2520and%2520Zhongyuan%2520Lai%2520and%2520Zihan%2520Liu%2520and%2520Fengyuan%2520Xu%2520and%2520Qi%2520Zhang%2520and%2520Liang%2520Hu%26entry.1292438233%3DNext%2520Point-of-Interest%2520%2528POI%2529%2520recommendation%2520is%2520a%2520critical%2520task%2520in%2520modern%2520Location-Based%2520Social%2520Networks%2520%2528LBSNs%2529%252C%2520aiming%2520to%2520model%2520the%2520complex%2520decision-making%2520process%2520of%2520human%2520mobility%2520to%2520provide%2520personalized%2520recommendations%2520for%2520a%2520user%2527s%2520next%2520check-in%2520location.%2520Existing%2520hyperbolic%2520POI%2520recommendation%2520models%252C%2520predominantly%2520based%2520on%2520rotations%2520and%2520graph%2520representations%252C%2520have%2520been%2520extensively%2520investigated.%2520Although%2520hyperbolic%2520geometry%2520has%2520proven%2520superior%2520in%2520representing%2520hierarchical%2520data%2520with%2520low%2520distortion%252C%2520current%2520hyperbolic%2520sequence%2520models%2520typically%2520rely%2520on%2520performing%2520recurrence%2520via%2520expensive%2520M%25C3%25B6bius%2520operations%2520directly%2520on%2520the%2520manifold.%2520This%2520incurs%2520prohibitive%2520computational%2520costs%2520and%2520numerical%2520instability%252C%2520rendering%2520them%2520ill-suited%2520for%2520trajectory%2520modeling.%2520To%2520resolve%2520this%2520conflict%2520between%2520geometric%2520representational%2520power%2520and%2520sequential%2520efficiency%252C%2520we%2520propose%2520GTR-Mamba%252C%2520a%2520novel%2520framework%2520featuring%2520Geometry-to-Tangent%2520Routing.%2520GTR-Mamba%2520strategically%2520routes%2520complex%2520state%2520transitions%2520to%2520the%2520computationally%2520efficient%2520Euclidean%2520tangent%2520space.%2520Crucially%252C%2520instead%2520of%2520a%2520static%2520approximation%252C%2520we%2520introduce%2520a%2520Parallel%2520Transport%2520%2528PT%2529%2520mechanism%2520that%2520dynamically%2520aligns%2520tangent%2520spaces%2520along%2520the%2520trajectory.%2520This%2520ensures%2520geometric%2520consistency%2520across%2520recursive%2520updates%252C%2520effectively%2520bridging%2520the%2520gap%2520between%2520the%2520curved%2520manifold%2520and%2520linear%2520tangent%2520operations.%2520This%2520process%2520is%2520orchestrated%2520by%2520an%2520exogenous%2520spatio-temporal%2520channel%252C%2520which%2520explicitly%2520modulates%2520the%2520SSM%2520discretization%2520parameters.%2520Extensive%2520experiments%2520on%2520three%2520real-world%2520datasets%2520demonstrate%2520that%2520GTR-Mamba%2520consistently%2520outperforms%2520state-of-the-art%2520baselines%2520in%2520next%2520POI%2520recommendation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.22942v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GTR-Mamba%3A%20Geometry-to-Tangent%20Routing%20Mamba%20for%20Hyperbolic%20POI%20Recommendation&entry.906535625=Zhuoxuan%20Li%20and%20Jieyuan%20Pei%20and%20Tangwei%20Ye%20and%20Zhongyuan%20Lai%20and%20Zihan%20Liu%20and%20Fengyuan%20Xu%20and%20Qi%20Zhang%20and%20Liang%20Hu&entry.1292438233=Next%20Point-of-Interest%20%28POI%29%20recommendation%20is%20a%20critical%20task%20in%20modern%20Location-Based%20Social%20Networks%20%28LBSNs%29%2C%20aiming%20to%20model%20the%20complex%20decision-making%20process%20of%20human%20mobility%20to%20provide%20personalized%20recommendations%20for%20a%20user%27s%20next%20check-in%20location.%20Existing%20hyperbolic%20POI%20recommendation%20models%2C%20predominantly%20based%20on%20rotations%20and%20graph%20representations%2C%20have%20been%20extensively%20investigated.%20Although%20hyperbolic%20geometry%20has%20proven%20superior%20in%20representing%20hierarchical%20data%20with%20low%20distortion%2C%20current%20hyperbolic%20sequence%20models%20typically%20rely%20on%20performing%20recurrence%20via%20expensive%20M%C3%B6bius%20operations%20directly%20on%20the%20manifold.%20This%20incurs%20prohibitive%20computational%20costs%20and%20numerical%20instability%2C%20rendering%20them%20ill-suited%20for%20trajectory%20modeling.%20To%20resolve%20this%20conflict%20between%20geometric%20representational%20power%20and%20sequential%20efficiency%2C%20we%20propose%20GTR-Mamba%2C%20a%20novel%20framework%20featuring%20Geometry-to-Tangent%20Routing.%20GTR-Mamba%20strategically%20routes%20complex%20state%20transitions%20to%20the%20computationally%20efficient%20Euclidean%20tangent%20space.%20Crucially%2C%20instead%20of%20a%20static%20approximation%2C%20we%20introduce%20a%20Parallel%20Transport%20%28PT%29%20mechanism%20that%20dynamically%20aligns%20tangent%20spaces%20along%20the%20trajectory.%20This%20ensures%20geometric%20consistency%20across%20recursive%20updates%2C%20effectively%20bridging%20the%20gap%20between%20the%20curved%20manifold%20and%20linear%20tangent%20operations.%20This%20process%20is%20orchestrated%20by%20an%20exogenous%20spatio-temporal%20channel%2C%20which%20explicitly%20modulates%20the%20SSM%20discretization%20parameters.%20Extensive%20experiments%20on%20three%20real-world%20datasets%20demonstrate%20that%20GTR-Mamba%20consistently%20outperforms%20state-of-the-art%20baselines%20in%20next%20POI%20recommendation.&entry.1838667208=http%3A//arxiv.org/abs/2510.22942v2&entry.124074799=Read"},
{"title": "Detecting High-Stakes Interactions with Activation Probes", "author": "Alex McKenzie and Urja Pawar and Phil Blandfort and William Bankes and David Krueger and Ekdeep Singh Lubana and Dmitrii Krasheninnikov", "abstract": "Monitoring is an important aspect of safely deploying Large Language Models (LLMs). This paper examines activation probes for detecting ``high-stakes'' interactions -- where the text indicates that the interaction might lead to significant harm -- as a critical, yet underexplored, target for such monitoring. We evaluate several probe architectures trained on synthetic data, and find them to exhibit robust generalization to diverse, out-of-distribution, real-world data. Probes' performance is comparable to that of prompted or finetuned medium-sized LLM monitors, while offering computational savings of six orders-of-magnitude. These savings are enabled by reusing activations of the model that is being monitored. Our experiments also highlight the potential of building resource-aware hierarchical monitoring systems, where probes serve as an efficient initial filter and flag cases for more expensive downstream analysis. We release our novel synthetic dataset and the codebase at https://github.com/arrrlex/models-under-pressure.", "link": "http://arxiv.org/abs/2506.10805v4", "date": "2026-01-23", "relevancy": 2.5324, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5097}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5097}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20High-Stakes%20Interactions%20with%20Activation%20Probes&body=Title%3A%20Detecting%20High-Stakes%20Interactions%20with%20Activation%20Probes%0AAuthor%3A%20Alex%20McKenzie%20and%20Urja%20Pawar%20and%20Phil%20Blandfort%20and%20William%20Bankes%20and%20David%20Krueger%20and%20Ekdeep%20Singh%20Lubana%20and%20Dmitrii%20Krasheninnikov%0AAbstract%3A%20Monitoring%20is%20an%20important%20aspect%20of%20safely%20deploying%20Large%20Language%20Models%20%28LLMs%29.%20This%20paper%20examines%20activation%20probes%20for%20detecting%20%60%60high-stakes%27%27%20interactions%20--%20where%20the%20text%20indicates%20that%20the%20interaction%20might%20lead%20to%20significant%20harm%20--%20as%20a%20critical%2C%20yet%20underexplored%2C%20target%20for%20such%20monitoring.%20We%20evaluate%20several%20probe%20architectures%20trained%20on%20synthetic%20data%2C%20and%20find%20them%20to%20exhibit%20robust%20generalization%20to%20diverse%2C%20out-of-distribution%2C%20real-world%20data.%20Probes%27%20performance%20is%20comparable%20to%20that%20of%20prompted%20or%20finetuned%20medium-sized%20LLM%20monitors%2C%20while%20offering%20computational%20savings%20of%20six%20orders-of-magnitude.%20These%20savings%20are%20enabled%20by%20reusing%20activations%20of%20the%20model%20that%20is%20being%20monitored.%20Our%20experiments%20also%20highlight%20the%20potential%20of%20building%20resource-aware%20hierarchical%20monitoring%20systems%2C%20where%20probes%20serve%20as%20an%20efficient%20initial%20filter%20and%20flag%20cases%20for%20more%20expensive%20downstream%20analysis.%20We%20release%20our%20novel%20synthetic%20dataset%20and%20the%20codebase%20at%20https%3A//github.com/arrrlex/models-under-pressure.%0ALink%3A%20http%3A//arxiv.org/abs/2506.10805v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520High-Stakes%2520Interactions%2520with%2520Activation%2520Probes%26entry.906535625%3DAlex%2520McKenzie%2520and%2520Urja%2520Pawar%2520and%2520Phil%2520Blandfort%2520and%2520William%2520Bankes%2520and%2520David%2520Krueger%2520and%2520Ekdeep%2520Singh%2520Lubana%2520and%2520Dmitrii%2520Krasheninnikov%26entry.1292438233%3DMonitoring%2520is%2520an%2520important%2520aspect%2520of%2520safely%2520deploying%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520This%2520paper%2520examines%2520activation%2520probes%2520for%2520detecting%2520%2560%2560high-stakes%2527%2527%2520interactions%2520--%2520where%2520the%2520text%2520indicates%2520that%2520the%2520interaction%2520might%2520lead%2520to%2520significant%2520harm%2520--%2520as%2520a%2520critical%252C%2520yet%2520underexplored%252C%2520target%2520for%2520such%2520monitoring.%2520We%2520evaluate%2520several%2520probe%2520architectures%2520trained%2520on%2520synthetic%2520data%252C%2520and%2520find%2520them%2520to%2520exhibit%2520robust%2520generalization%2520to%2520diverse%252C%2520out-of-distribution%252C%2520real-world%2520data.%2520Probes%2527%2520performance%2520is%2520comparable%2520to%2520that%2520of%2520prompted%2520or%2520finetuned%2520medium-sized%2520LLM%2520monitors%252C%2520while%2520offering%2520computational%2520savings%2520of%2520six%2520orders-of-magnitude.%2520These%2520savings%2520are%2520enabled%2520by%2520reusing%2520activations%2520of%2520the%2520model%2520that%2520is%2520being%2520monitored.%2520Our%2520experiments%2520also%2520highlight%2520the%2520potential%2520of%2520building%2520resource-aware%2520hierarchical%2520monitoring%2520systems%252C%2520where%2520probes%2520serve%2520as%2520an%2520efficient%2520initial%2520filter%2520and%2520flag%2520cases%2520for%2520more%2520expensive%2520downstream%2520analysis.%2520We%2520release%2520our%2520novel%2520synthetic%2520dataset%2520and%2520the%2520codebase%2520at%2520https%253A//github.com/arrrlex/models-under-pressure.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10805v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20High-Stakes%20Interactions%20with%20Activation%20Probes&entry.906535625=Alex%20McKenzie%20and%20Urja%20Pawar%20and%20Phil%20Blandfort%20and%20William%20Bankes%20and%20David%20Krueger%20and%20Ekdeep%20Singh%20Lubana%20and%20Dmitrii%20Krasheninnikov&entry.1292438233=Monitoring%20is%20an%20important%20aspect%20of%20safely%20deploying%20Large%20Language%20Models%20%28LLMs%29.%20This%20paper%20examines%20activation%20probes%20for%20detecting%20%60%60high-stakes%27%27%20interactions%20--%20where%20the%20text%20indicates%20that%20the%20interaction%20might%20lead%20to%20significant%20harm%20--%20as%20a%20critical%2C%20yet%20underexplored%2C%20target%20for%20such%20monitoring.%20We%20evaluate%20several%20probe%20architectures%20trained%20on%20synthetic%20data%2C%20and%20find%20them%20to%20exhibit%20robust%20generalization%20to%20diverse%2C%20out-of-distribution%2C%20real-world%20data.%20Probes%27%20performance%20is%20comparable%20to%20that%20of%20prompted%20or%20finetuned%20medium-sized%20LLM%20monitors%2C%20while%20offering%20computational%20savings%20of%20six%20orders-of-magnitude.%20These%20savings%20are%20enabled%20by%20reusing%20activations%20of%20the%20model%20that%20is%20being%20monitored.%20Our%20experiments%20also%20highlight%20the%20potential%20of%20building%20resource-aware%20hierarchical%20monitoring%20systems%2C%20where%20probes%20serve%20as%20an%20efficient%20initial%20filter%20and%20flag%20cases%20for%20more%20expensive%20downstream%20analysis.%20We%20release%20our%20novel%20synthetic%20dataset%20and%20the%20codebase%20at%20https%3A//github.com/arrrlex/models-under-pressure.&entry.1838667208=http%3A//arxiv.org/abs/2506.10805v4&entry.124074799=Read"},
{"title": "3D Molecule Generation from Rigid Motifs via SE(3) Flows", "author": "Roman Poletukhin and Marcel Kollovieh and Eike Eberhard and Stephan G\u00fcnnemann", "abstract": "Three-dimensional molecular structure generation is typically performed at the level of individual atoms, yet molecular graph generation techniques often consider fragments as their structural units. Building on the advances in frame-based protein structure generation, we extend these fragmentation ideas to 3D, treating general molecules as sets of rigid-body motifs. Utilising this representation, we employ SE(3)-equivariant generative modelling for de novo 3D molecule generation from rigid motifs. In our evaluations, we observe comparable or superior results to state-of-the-art across benchmarks, surpassing it in atom stability on GEOM-Drugs, while yielding a 2x to 10x reduction in generation steps and offering 3.5x compression in molecular representations compared to the standard atom-based methods.", "link": "http://arxiv.org/abs/2601.16955v1", "date": "2026-01-23", "relevancy": 2.532, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5228}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4982}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Molecule%20Generation%20from%20Rigid%20Motifs%20via%20SE%283%29%20Flows&body=Title%3A%203D%20Molecule%20Generation%20from%20Rigid%20Motifs%20via%20SE%283%29%20Flows%0AAuthor%3A%20Roman%20Poletukhin%20and%20Marcel%20Kollovieh%20and%20Eike%20Eberhard%20and%20Stephan%20G%C3%BCnnemann%0AAbstract%3A%20Three-dimensional%20molecular%20structure%20generation%20is%20typically%20performed%20at%20the%20level%20of%20individual%20atoms%2C%20yet%20molecular%20graph%20generation%20techniques%20often%20consider%20fragments%20as%20their%20structural%20units.%20Building%20on%20the%20advances%20in%20frame-based%20protein%20structure%20generation%2C%20we%20extend%20these%20fragmentation%20ideas%20to%203D%2C%20treating%20general%20molecules%20as%20sets%20of%20rigid-body%20motifs.%20Utilising%20this%20representation%2C%20we%20employ%20SE%283%29-equivariant%20generative%20modelling%20for%20de%20novo%203D%20molecule%20generation%20from%20rigid%20motifs.%20In%20our%20evaluations%2C%20we%20observe%20comparable%20or%20superior%20results%20to%20state-of-the-art%20across%20benchmarks%2C%20surpassing%20it%20in%20atom%20stability%20on%20GEOM-Drugs%2C%20while%20yielding%20a%202x%20to%2010x%20reduction%20in%20generation%20steps%20and%20offering%203.5x%20compression%20in%20molecular%20representations%20compared%20to%20the%20standard%20atom-based%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Molecule%2520Generation%2520from%2520Rigid%2520Motifs%2520via%2520SE%25283%2529%2520Flows%26entry.906535625%3DRoman%2520Poletukhin%2520and%2520Marcel%2520Kollovieh%2520and%2520Eike%2520Eberhard%2520and%2520Stephan%2520G%25C3%25BCnnemann%26entry.1292438233%3DThree-dimensional%2520molecular%2520structure%2520generation%2520is%2520typically%2520performed%2520at%2520the%2520level%2520of%2520individual%2520atoms%252C%2520yet%2520molecular%2520graph%2520generation%2520techniques%2520often%2520consider%2520fragments%2520as%2520their%2520structural%2520units.%2520Building%2520on%2520the%2520advances%2520in%2520frame-based%2520protein%2520structure%2520generation%252C%2520we%2520extend%2520these%2520fragmentation%2520ideas%2520to%25203D%252C%2520treating%2520general%2520molecules%2520as%2520sets%2520of%2520rigid-body%2520motifs.%2520Utilising%2520this%2520representation%252C%2520we%2520employ%2520SE%25283%2529-equivariant%2520generative%2520modelling%2520for%2520de%2520novo%25203D%2520molecule%2520generation%2520from%2520rigid%2520motifs.%2520In%2520our%2520evaluations%252C%2520we%2520observe%2520comparable%2520or%2520superior%2520results%2520to%2520state-of-the-art%2520across%2520benchmarks%252C%2520surpassing%2520it%2520in%2520atom%2520stability%2520on%2520GEOM-Drugs%252C%2520while%2520yielding%2520a%25202x%2520to%252010x%2520reduction%2520in%2520generation%2520steps%2520and%2520offering%25203.5x%2520compression%2520in%2520molecular%2520representations%2520compared%2520to%2520the%2520standard%2520atom-based%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Molecule%20Generation%20from%20Rigid%20Motifs%20via%20SE%283%29%20Flows&entry.906535625=Roman%20Poletukhin%20and%20Marcel%20Kollovieh%20and%20Eike%20Eberhard%20and%20Stephan%20G%C3%BCnnemann&entry.1292438233=Three-dimensional%20molecular%20structure%20generation%20is%20typically%20performed%20at%20the%20level%20of%20individual%20atoms%2C%20yet%20molecular%20graph%20generation%20techniques%20often%20consider%20fragments%20as%20their%20structural%20units.%20Building%20on%20the%20advances%20in%20frame-based%20protein%20structure%20generation%2C%20we%20extend%20these%20fragmentation%20ideas%20to%203D%2C%20treating%20general%20molecules%20as%20sets%20of%20rigid-body%20motifs.%20Utilising%20this%20representation%2C%20we%20employ%20SE%283%29-equivariant%20generative%20modelling%20for%20de%20novo%203D%20molecule%20generation%20from%20rigid%20motifs.%20In%20our%20evaluations%2C%20we%20observe%20comparable%20or%20superior%20results%20to%20state-of-the-art%20across%20benchmarks%2C%20surpassing%20it%20in%20atom%20stability%20on%20GEOM-Drugs%2C%20while%20yielding%20a%202x%20to%2010x%20reduction%20in%20generation%20steps%20and%20offering%203.5x%20compression%20in%20molecular%20representations%20compared%20to%20the%20standard%20atom-based%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.16955v1&entry.124074799=Read"},
{"title": "Masked Modeling for Human Motion Recovery Under Occlusions", "author": "Zhiyin Qian and Siwei Zhang and Bharat Lal Bhatnagar and Federica Bogo and Siyu Tang", "abstract": "Human motion reconstruction from monocular videos is a fundamental challenge in computer vision, with broad applications in AR/VR, robotics, and digital content creation, but remains challenging under frequent occlusions in real-world settings. Existing regression-based methods are efficient but fragile to missing observations, while optimization- and diffusion-based approaches improve robustness at the cost of slow inference speed and heavy preprocessing steps. To address these limitations, we leverage recent advances in generative masked modeling and present MoRo: Masked Modeling for human motion Recovery under Occlusions. MoRo is an occlusion-robust, end-to-end generative framework that formulates motion reconstruction as a video-conditioned task, and efficiently recover human motion in a consistent global coordinate system from RGB videos. By masked modeling, MoRo naturally handles occlusions while enabling efficient, end-to-end inference. To overcome the scarcity of paired video-motion data, we design a cross-modality learning scheme that learns multi-modal priors from a set of heterogeneous datasets: (i) a trajectory-aware motion prior trained on MoCap datasets, (ii) an image-conditioned pose prior trained on image-pose datasets, capturing diverse per-frame poses, and (iii) a video-conditioned masked transformer that fuses motion and pose priors, finetuned on video-motion datasets to integrate visual cues with motion dynamics for robust inference. Extensive experiments on EgoBody and RICH demonstrate that MoRo substantially outperforms state-of-the-art methods in accuracy and motion realism under occlusions, while performing on-par in non-occluded scenarios. MoRo achieves real-time inference at 70 FPS on a single H200 GPU.", "link": "http://arxiv.org/abs/2601.16079v2", "date": "2026-01-23", "relevancy": 2.4986, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6346}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6283}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Masked%20Modeling%20for%20Human%20Motion%20Recovery%20Under%20Occlusions&body=Title%3A%20Masked%20Modeling%20for%20Human%20Motion%20Recovery%20Under%20Occlusions%0AAuthor%3A%20Zhiyin%20Qian%20and%20Siwei%20Zhang%20and%20Bharat%20Lal%20Bhatnagar%20and%20Federica%20Bogo%20and%20Siyu%20Tang%0AAbstract%3A%20Human%20motion%20reconstruction%20from%20monocular%20videos%20is%20a%20fundamental%20challenge%20in%20computer%20vision%2C%20with%20broad%20applications%20in%20AR/VR%2C%20robotics%2C%20and%20digital%20content%20creation%2C%20but%20remains%20challenging%20under%20frequent%20occlusions%20in%20real-world%20settings.%20Existing%20regression-based%20methods%20are%20efficient%20but%20fragile%20to%20missing%20observations%2C%20while%20optimization-%20and%20diffusion-based%20approaches%20improve%20robustness%20at%20the%20cost%20of%20slow%20inference%20speed%20and%20heavy%20preprocessing%20steps.%20To%20address%20these%20limitations%2C%20we%20leverage%20recent%20advances%20in%20generative%20masked%20modeling%20and%20present%20MoRo%3A%20Masked%20Modeling%20for%20human%20motion%20Recovery%20under%20Occlusions.%20MoRo%20is%20an%20occlusion-robust%2C%20end-to-end%20generative%20framework%20that%20formulates%20motion%20reconstruction%20as%20a%20video-conditioned%20task%2C%20and%20efficiently%20recover%20human%20motion%20in%20a%20consistent%20global%20coordinate%20system%20from%20RGB%20videos.%20By%20masked%20modeling%2C%20MoRo%20naturally%20handles%20occlusions%20while%20enabling%20efficient%2C%20end-to-end%20inference.%20To%20overcome%20the%20scarcity%20of%20paired%20video-motion%20data%2C%20we%20design%20a%20cross-modality%20learning%20scheme%20that%20learns%20multi-modal%20priors%20from%20a%20set%20of%20heterogeneous%20datasets%3A%20%28i%29%20a%20trajectory-aware%20motion%20prior%20trained%20on%20MoCap%20datasets%2C%20%28ii%29%20an%20image-conditioned%20pose%20prior%20trained%20on%20image-pose%20datasets%2C%20capturing%20diverse%20per-frame%20poses%2C%20and%20%28iii%29%20a%20video-conditioned%20masked%20transformer%20that%20fuses%20motion%20and%20pose%20priors%2C%20finetuned%20on%20video-motion%20datasets%20to%20integrate%20visual%20cues%20with%20motion%20dynamics%20for%20robust%20inference.%20Extensive%20experiments%20on%20EgoBody%20and%20RICH%20demonstrate%20that%20MoRo%20substantially%20outperforms%20state-of-the-art%20methods%20in%20accuracy%20and%20motion%20realism%20under%20occlusions%2C%20while%20performing%20on-par%20in%20non-occluded%20scenarios.%20MoRo%20achieves%20real-time%20inference%20at%2070%20FPS%20on%20a%20single%20H200%20GPU.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16079v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasked%2520Modeling%2520for%2520Human%2520Motion%2520Recovery%2520Under%2520Occlusions%26entry.906535625%3DZhiyin%2520Qian%2520and%2520Siwei%2520Zhang%2520and%2520Bharat%2520Lal%2520Bhatnagar%2520and%2520Federica%2520Bogo%2520and%2520Siyu%2520Tang%26entry.1292438233%3DHuman%2520motion%2520reconstruction%2520from%2520monocular%2520videos%2520is%2520a%2520fundamental%2520challenge%2520in%2520computer%2520vision%252C%2520with%2520broad%2520applications%2520in%2520AR/VR%252C%2520robotics%252C%2520and%2520digital%2520content%2520creation%252C%2520but%2520remains%2520challenging%2520under%2520frequent%2520occlusions%2520in%2520real-world%2520settings.%2520Existing%2520regression-based%2520methods%2520are%2520efficient%2520but%2520fragile%2520to%2520missing%2520observations%252C%2520while%2520optimization-%2520and%2520diffusion-based%2520approaches%2520improve%2520robustness%2520at%2520the%2520cost%2520of%2520slow%2520inference%2520speed%2520and%2520heavy%2520preprocessing%2520steps.%2520To%2520address%2520these%2520limitations%252C%2520we%2520leverage%2520recent%2520advances%2520in%2520generative%2520masked%2520modeling%2520and%2520present%2520MoRo%253A%2520Masked%2520Modeling%2520for%2520human%2520motion%2520Recovery%2520under%2520Occlusions.%2520MoRo%2520is%2520an%2520occlusion-robust%252C%2520end-to-end%2520generative%2520framework%2520that%2520formulates%2520motion%2520reconstruction%2520as%2520a%2520video-conditioned%2520task%252C%2520and%2520efficiently%2520recover%2520human%2520motion%2520in%2520a%2520consistent%2520global%2520coordinate%2520system%2520from%2520RGB%2520videos.%2520By%2520masked%2520modeling%252C%2520MoRo%2520naturally%2520handles%2520occlusions%2520while%2520enabling%2520efficient%252C%2520end-to-end%2520inference.%2520To%2520overcome%2520the%2520scarcity%2520of%2520paired%2520video-motion%2520data%252C%2520we%2520design%2520a%2520cross-modality%2520learning%2520scheme%2520that%2520learns%2520multi-modal%2520priors%2520from%2520a%2520set%2520of%2520heterogeneous%2520datasets%253A%2520%2528i%2529%2520a%2520trajectory-aware%2520motion%2520prior%2520trained%2520on%2520MoCap%2520datasets%252C%2520%2528ii%2529%2520an%2520image-conditioned%2520pose%2520prior%2520trained%2520on%2520image-pose%2520datasets%252C%2520capturing%2520diverse%2520per-frame%2520poses%252C%2520and%2520%2528iii%2529%2520a%2520video-conditioned%2520masked%2520transformer%2520that%2520fuses%2520motion%2520and%2520pose%2520priors%252C%2520finetuned%2520on%2520video-motion%2520datasets%2520to%2520integrate%2520visual%2520cues%2520with%2520motion%2520dynamics%2520for%2520robust%2520inference.%2520Extensive%2520experiments%2520on%2520EgoBody%2520and%2520RICH%2520demonstrate%2520that%2520MoRo%2520substantially%2520outperforms%2520state-of-the-art%2520methods%2520in%2520accuracy%2520and%2520motion%2520realism%2520under%2520occlusions%252C%2520while%2520performing%2520on-par%2520in%2520non-occluded%2520scenarios.%2520MoRo%2520achieves%2520real-time%2520inference%2520at%252070%2520FPS%2520on%2520a%2520single%2520H200%2520GPU.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16079v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Modeling%20for%20Human%20Motion%20Recovery%20Under%20Occlusions&entry.906535625=Zhiyin%20Qian%20and%20Siwei%20Zhang%20and%20Bharat%20Lal%20Bhatnagar%20and%20Federica%20Bogo%20and%20Siyu%20Tang&entry.1292438233=Human%20motion%20reconstruction%20from%20monocular%20videos%20is%20a%20fundamental%20challenge%20in%20computer%20vision%2C%20with%20broad%20applications%20in%20AR/VR%2C%20robotics%2C%20and%20digital%20content%20creation%2C%20but%20remains%20challenging%20under%20frequent%20occlusions%20in%20real-world%20settings.%20Existing%20regression-based%20methods%20are%20efficient%20but%20fragile%20to%20missing%20observations%2C%20while%20optimization-%20and%20diffusion-based%20approaches%20improve%20robustness%20at%20the%20cost%20of%20slow%20inference%20speed%20and%20heavy%20preprocessing%20steps.%20To%20address%20these%20limitations%2C%20we%20leverage%20recent%20advances%20in%20generative%20masked%20modeling%20and%20present%20MoRo%3A%20Masked%20Modeling%20for%20human%20motion%20Recovery%20under%20Occlusions.%20MoRo%20is%20an%20occlusion-robust%2C%20end-to-end%20generative%20framework%20that%20formulates%20motion%20reconstruction%20as%20a%20video-conditioned%20task%2C%20and%20efficiently%20recover%20human%20motion%20in%20a%20consistent%20global%20coordinate%20system%20from%20RGB%20videos.%20By%20masked%20modeling%2C%20MoRo%20naturally%20handles%20occlusions%20while%20enabling%20efficient%2C%20end-to-end%20inference.%20To%20overcome%20the%20scarcity%20of%20paired%20video-motion%20data%2C%20we%20design%20a%20cross-modality%20learning%20scheme%20that%20learns%20multi-modal%20priors%20from%20a%20set%20of%20heterogeneous%20datasets%3A%20%28i%29%20a%20trajectory-aware%20motion%20prior%20trained%20on%20MoCap%20datasets%2C%20%28ii%29%20an%20image-conditioned%20pose%20prior%20trained%20on%20image-pose%20datasets%2C%20capturing%20diverse%20per-frame%20poses%2C%20and%20%28iii%29%20a%20video-conditioned%20masked%20transformer%20that%20fuses%20motion%20and%20pose%20priors%2C%20finetuned%20on%20video-motion%20datasets%20to%20integrate%20visual%20cues%20with%20motion%20dynamics%20for%20robust%20inference.%20Extensive%20experiments%20on%20EgoBody%20and%20RICH%20demonstrate%20that%20MoRo%20substantially%20outperforms%20state-of-the-art%20methods%20in%20accuracy%20and%20motion%20realism%20under%20occlusions%2C%20while%20performing%20on-par%20in%20non-occluded%20scenarios.%20MoRo%20achieves%20real-time%20inference%20at%2070%20FPS%20on%20a%20single%20H200%20GPU.&entry.1838667208=http%3A//arxiv.org/abs/2601.16079v2&entry.124074799=Read"},
{"title": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "author": "Nikhil Keetha and Norman M\u00fcller and Johannes Sch\u00f6nberger and Lorenzo Porzi and Yuchen Zhang and Tobias Fischer and Arno Knapitsch and Duncan Zauss and Ethan Weber and Nelson Antunes and Jonathon Luiten and Manuel Lopez-Antequera and Samuel Rota Bul\u00f2 and Christian Richardt and Deva Ramanan and Sebastian Scherer and Peter Kontschieder", "abstract": "We introduce MapAnything, a unified transformer-based feed-forward model that ingests one or more images along with optional geometric inputs such as camera intrinsics, poses, depth, or partial reconstructions, and then directly regresses the metric 3D scene geometry and cameras. MapAnything leverages a factored representation of multi-view scene geometry, i.e., a collection of depth maps, local ray maps, camera poses, and a metric scale factor that effectively upgrades local reconstructions into a globally consistent metric frame. Standardizing the supervision and training across diverse datasets, along with flexible input augmentation, enables MapAnything to address a broad range of 3D vision tasks in a single feed-forward pass, including uncalibrated structure-from-motion, calibrated multi-view stereo, monocular depth estimation, camera localization, depth completion, and more. We provide extensive experimental analyses and model ablations demonstrating that MapAnything outperforms or matches specialist feed-forward models while offering more efficient joint training behavior, thus paving the way toward a universal 3D reconstruction backbone.", "link": "http://arxiv.org/abs/2509.13414v3", "date": "2026-01-23", "relevancy": 2.4904, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6297}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6212}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MapAnything%3A%20Universal%20Feed-Forward%20Metric%203D%20Reconstruction&body=Title%3A%20MapAnything%3A%20Universal%20Feed-Forward%20Metric%203D%20Reconstruction%0AAuthor%3A%20Nikhil%20Keetha%20and%20Norman%20M%C3%BCller%20and%20Johannes%20Sch%C3%B6nberger%20and%20Lorenzo%20Porzi%20and%20Yuchen%20Zhang%20and%20Tobias%20Fischer%20and%20Arno%20Knapitsch%20and%20Duncan%20Zauss%20and%20Ethan%20Weber%20and%20Nelson%20Antunes%20and%20Jonathon%20Luiten%20and%20Manuel%20Lopez-Antequera%20and%20Samuel%20Rota%20Bul%C3%B2%20and%20Christian%20Richardt%20and%20Deva%20Ramanan%20and%20Sebastian%20Scherer%20and%20Peter%20Kontschieder%0AAbstract%3A%20We%20introduce%20MapAnything%2C%20a%20unified%20transformer-based%20feed-forward%20model%20that%20ingests%20one%20or%20more%20images%20along%20with%20optional%20geometric%20inputs%20such%20as%20camera%20intrinsics%2C%20poses%2C%20depth%2C%20or%20partial%20reconstructions%2C%20and%20then%20directly%20regresses%20the%20metric%203D%20scene%20geometry%20and%20cameras.%20MapAnything%20leverages%20a%20factored%20representation%20of%20multi-view%20scene%20geometry%2C%20i.e.%2C%20a%20collection%20of%20depth%20maps%2C%20local%20ray%20maps%2C%20camera%20poses%2C%20and%20a%20metric%20scale%20factor%20that%20effectively%20upgrades%20local%20reconstructions%20into%20a%20globally%20consistent%20metric%20frame.%20Standardizing%20the%20supervision%20and%20training%20across%20diverse%20datasets%2C%20along%20with%20flexible%20input%20augmentation%2C%20enables%20MapAnything%20to%20address%20a%20broad%20range%20of%203D%20vision%20tasks%20in%20a%20single%20feed-forward%20pass%2C%20including%20uncalibrated%20structure-from-motion%2C%20calibrated%20multi-view%20stereo%2C%20monocular%20depth%20estimation%2C%20camera%20localization%2C%20depth%20completion%2C%20and%20more.%20We%20provide%20extensive%20experimental%20analyses%20and%20model%20ablations%20demonstrating%20that%20MapAnything%20outperforms%20or%20matches%20specialist%20feed-forward%20models%20while%20offering%20more%20efficient%20joint%20training%20behavior%2C%20thus%20paving%20the%20way%20toward%20a%20universal%203D%20reconstruction%20backbone.%0ALink%3A%20http%3A//arxiv.org/abs/2509.13414v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapAnything%253A%2520Universal%2520Feed-Forward%2520Metric%25203D%2520Reconstruction%26entry.906535625%3DNikhil%2520Keetha%2520and%2520Norman%2520M%25C3%25BCller%2520and%2520Johannes%2520Sch%25C3%25B6nberger%2520and%2520Lorenzo%2520Porzi%2520and%2520Yuchen%2520Zhang%2520and%2520Tobias%2520Fischer%2520and%2520Arno%2520Knapitsch%2520and%2520Duncan%2520Zauss%2520and%2520Ethan%2520Weber%2520and%2520Nelson%2520Antunes%2520and%2520Jonathon%2520Luiten%2520and%2520Manuel%2520Lopez-Antequera%2520and%2520Samuel%2520Rota%2520Bul%25C3%25B2%2520and%2520Christian%2520Richardt%2520and%2520Deva%2520Ramanan%2520and%2520Sebastian%2520Scherer%2520and%2520Peter%2520Kontschieder%26entry.1292438233%3DWe%2520introduce%2520MapAnything%252C%2520a%2520unified%2520transformer-based%2520feed-forward%2520model%2520that%2520ingests%2520one%2520or%2520more%2520images%2520along%2520with%2520optional%2520geometric%2520inputs%2520such%2520as%2520camera%2520intrinsics%252C%2520poses%252C%2520depth%252C%2520or%2520partial%2520reconstructions%252C%2520and%2520then%2520directly%2520regresses%2520the%2520metric%25203D%2520scene%2520geometry%2520and%2520cameras.%2520MapAnything%2520leverages%2520a%2520factored%2520representation%2520of%2520multi-view%2520scene%2520geometry%252C%2520i.e.%252C%2520a%2520collection%2520of%2520depth%2520maps%252C%2520local%2520ray%2520maps%252C%2520camera%2520poses%252C%2520and%2520a%2520metric%2520scale%2520factor%2520that%2520effectively%2520upgrades%2520local%2520reconstructions%2520into%2520a%2520globally%2520consistent%2520metric%2520frame.%2520Standardizing%2520the%2520supervision%2520and%2520training%2520across%2520diverse%2520datasets%252C%2520along%2520with%2520flexible%2520input%2520augmentation%252C%2520enables%2520MapAnything%2520to%2520address%2520a%2520broad%2520range%2520of%25203D%2520vision%2520tasks%2520in%2520a%2520single%2520feed-forward%2520pass%252C%2520including%2520uncalibrated%2520structure-from-motion%252C%2520calibrated%2520multi-view%2520stereo%252C%2520monocular%2520depth%2520estimation%252C%2520camera%2520localization%252C%2520depth%2520completion%252C%2520and%2520more.%2520We%2520provide%2520extensive%2520experimental%2520analyses%2520and%2520model%2520ablations%2520demonstrating%2520that%2520MapAnything%2520outperforms%2520or%2520matches%2520specialist%2520feed-forward%2520models%2520while%2520offering%2520more%2520efficient%2520joint%2520training%2520behavior%252C%2520thus%2520paving%2520the%2520way%2520toward%2520a%2520universal%25203D%2520reconstruction%2520backbone.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13414v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MapAnything%3A%20Universal%20Feed-Forward%20Metric%203D%20Reconstruction&entry.906535625=Nikhil%20Keetha%20and%20Norman%20M%C3%BCller%20and%20Johannes%20Sch%C3%B6nberger%20and%20Lorenzo%20Porzi%20and%20Yuchen%20Zhang%20and%20Tobias%20Fischer%20and%20Arno%20Knapitsch%20and%20Duncan%20Zauss%20and%20Ethan%20Weber%20and%20Nelson%20Antunes%20and%20Jonathon%20Luiten%20and%20Manuel%20Lopez-Antequera%20and%20Samuel%20Rota%20Bul%C3%B2%20and%20Christian%20Richardt%20and%20Deva%20Ramanan%20and%20Sebastian%20Scherer%20and%20Peter%20Kontschieder&entry.1292438233=We%20introduce%20MapAnything%2C%20a%20unified%20transformer-based%20feed-forward%20model%20that%20ingests%20one%20or%20more%20images%20along%20with%20optional%20geometric%20inputs%20such%20as%20camera%20intrinsics%2C%20poses%2C%20depth%2C%20or%20partial%20reconstructions%2C%20and%20then%20directly%20regresses%20the%20metric%203D%20scene%20geometry%20and%20cameras.%20MapAnything%20leverages%20a%20factored%20representation%20of%20multi-view%20scene%20geometry%2C%20i.e.%2C%20a%20collection%20of%20depth%20maps%2C%20local%20ray%20maps%2C%20camera%20poses%2C%20and%20a%20metric%20scale%20factor%20that%20effectively%20upgrades%20local%20reconstructions%20into%20a%20globally%20consistent%20metric%20frame.%20Standardizing%20the%20supervision%20and%20training%20across%20diverse%20datasets%2C%20along%20with%20flexible%20input%20augmentation%2C%20enables%20MapAnything%20to%20address%20a%20broad%20range%20of%203D%20vision%20tasks%20in%20a%20single%20feed-forward%20pass%2C%20including%20uncalibrated%20structure-from-motion%2C%20calibrated%20multi-view%20stereo%2C%20monocular%20depth%20estimation%2C%20camera%20localization%2C%20depth%20completion%2C%20and%20more.%20We%20provide%20extensive%20experimental%20analyses%20and%20model%20ablations%20demonstrating%20that%20MapAnything%20outperforms%20or%20matches%20specialist%20feed-forward%20models%20while%20offering%20more%20efficient%20joint%20training%20behavior%2C%20thus%20paving%20the%20way%20toward%20a%20universal%203D%20reconstruction%20backbone.&entry.1838667208=http%3A//arxiv.org/abs/2509.13414v3&entry.124074799=Read"},
{"title": "Using Shadows in Circular Synthetic Aperture Sonar Imaging for Target Analysis", "author": "Yann Le Gall and Nicolas Burlet and Mathieu Simon and Fabien Novella and Samantha Dugelay and Jean-Philippe Malkasse", "abstract": "Circular Synthetic Aperture Sonar (CSAS) provides a 360\u00b0 azimuth view of the seabed, surpassing the limited aperture and mono-view image of conventional side-scan SAS. This makes CSAS a valuable tool for target recognition in mine warfare where the diversity of point of view is essential for reducing false alarms. CSAS processing typically produces a very high-resolution two-dimensional image. However, the parallax introduced by the circular displacement of the illuminator fill-in the shadow regions, and the shadow cast by an object on the seafloor is lost in favor of azimuth coverage and resolution. Yet the shadows provide complementary information on target shape useful for target recognition. In this paper, we explore a way to retrieve shadow information from CSAS data to improve target analysis and carry 3D reconstruction. Sub-aperture filtering is used to get a collection of images at various points of view along the circular trajectory and fixed focus shadow enhancement (FFSE) is applied to obtain sharp shadows. An interactive interface is also proposed to allow human operators to visualize these shadows along the circular trajectory. A space-carving reconstruction method is applied to infer the 3D shape of the object from the segmented shadows. The results demonstrate the potential of shadows in circular SAS for improving target analysis and 3D reconstruction.", "link": "http://arxiv.org/abs/2601.16733v1", "date": "2026-01-23", "relevancy": 2.4894, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5061}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5061}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Shadows%20in%20Circular%20Synthetic%20Aperture%20Sonar%20Imaging%20for%20Target%20Analysis&body=Title%3A%20Using%20Shadows%20in%20Circular%20Synthetic%20Aperture%20Sonar%20Imaging%20for%20Target%20Analysis%0AAuthor%3A%20Yann%20Le%20Gall%20and%20Nicolas%20Burlet%20and%20Mathieu%20Simon%20and%20Fabien%20Novella%20and%20Samantha%20Dugelay%20and%20Jean-Philippe%20Malkasse%0AAbstract%3A%20Circular%20Synthetic%20Aperture%20Sonar%20%28CSAS%29%20provides%20a%20360%C2%B0%20azimuth%20view%20of%20the%20seabed%2C%20surpassing%20the%20limited%20aperture%20and%20mono-view%20image%20of%20conventional%20side-scan%20SAS.%20This%20makes%20CSAS%20a%20valuable%20tool%20for%20target%20recognition%20in%20mine%20warfare%20where%20the%20diversity%20of%20point%20of%20view%20is%20essential%20for%20reducing%20false%20alarms.%20CSAS%20processing%20typically%20produces%20a%20very%20high-resolution%20two-dimensional%20image.%20However%2C%20the%20parallax%20introduced%20by%20the%20circular%20displacement%20of%20the%20illuminator%20fill-in%20the%20shadow%20regions%2C%20and%20the%20shadow%20cast%20by%20an%20object%20on%20the%20seafloor%20is%20lost%20in%20favor%20of%20azimuth%20coverage%20and%20resolution.%20Yet%20the%20shadows%20provide%20complementary%20information%20on%20target%20shape%20useful%20for%20target%20recognition.%20In%20this%20paper%2C%20we%20explore%20a%20way%20to%20retrieve%20shadow%20information%20from%20CSAS%20data%20to%20improve%20target%20analysis%20and%20carry%203D%20reconstruction.%20Sub-aperture%20filtering%20is%20used%20to%20get%20a%20collection%20of%20images%20at%20various%20points%20of%20view%20along%20the%20circular%20trajectory%20and%20fixed%20focus%20shadow%20enhancement%20%28FFSE%29%20is%20applied%20to%20obtain%20sharp%20shadows.%20An%20interactive%20interface%20is%20also%20proposed%20to%20allow%20human%20operators%20to%20visualize%20these%20shadows%20along%20the%20circular%20trajectory.%20A%20space-carving%20reconstruction%20method%20is%20applied%20to%20infer%20the%203D%20shape%20of%20the%20object%20from%20the%20segmented%20shadows.%20The%20results%20demonstrate%20the%20potential%20of%20shadows%20in%20circular%20SAS%20for%20improving%20target%20analysis%20and%203D%20reconstruction.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Shadows%2520in%2520Circular%2520Synthetic%2520Aperture%2520Sonar%2520Imaging%2520for%2520Target%2520Analysis%26entry.906535625%3DYann%2520Le%2520Gall%2520and%2520Nicolas%2520Burlet%2520and%2520Mathieu%2520Simon%2520and%2520Fabien%2520Novella%2520and%2520Samantha%2520Dugelay%2520and%2520Jean-Philippe%2520Malkasse%26entry.1292438233%3DCircular%2520Synthetic%2520Aperture%2520Sonar%2520%2528CSAS%2529%2520provides%2520a%2520360%25C2%25B0%2520azimuth%2520view%2520of%2520the%2520seabed%252C%2520surpassing%2520the%2520limited%2520aperture%2520and%2520mono-view%2520image%2520of%2520conventional%2520side-scan%2520SAS.%2520This%2520makes%2520CSAS%2520a%2520valuable%2520tool%2520for%2520target%2520recognition%2520in%2520mine%2520warfare%2520where%2520the%2520diversity%2520of%2520point%2520of%2520view%2520is%2520essential%2520for%2520reducing%2520false%2520alarms.%2520CSAS%2520processing%2520typically%2520produces%2520a%2520very%2520high-resolution%2520two-dimensional%2520image.%2520However%252C%2520the%2520parallax%2520introduced%2520by%2520the%2520circular%2520displacement%2520of%2520the%2520illuminator%2520fill-in%2520the%2520shadow%2520regions%252C%2520and%2520the%2520shadow%2520cast%2520by%2520an%2520object%2520on%2520the%2520seafloor%2520is%2520lost%2520in%2520favor%2520of%2520azimuth%2520coverage%2520and%2520resolution.%2520Yet%2520the%2520shadows%2520provide%2520complementary%2520information%2520on%2520target%2520shape%2520useful%2520for%2520target%2520recognition.%2520In%2520this%2520paper%252C%2520we%2520explore%2520a%2520way%2520to%2520retrieve%2520shadow%2520information%2520from%2520CSAS%2520data%2520to%2520improve%2520target%2520analysis%2520and%2520carry%25203D%2520reconstruction.%2520Sub-aperture%2520filtering%2520is%2520used%2520to%2520get%2520a%2520collection%2520of%2520images%2520at%2520various%2520points%2520of%2520view%2520along%2520the%2520circular%2520trajectory%2520and%2520fixed%2520focus%2520shadow%2520enhancement%2520%2528FFSE%2529%2520is%2520applied%2520to%2520obtain%2520sharp%2520shadows.%2520An%2520interactive%2520interface%2520is%2520also%2520proposed%2520to%2520allow%2520human%2520operators%2520to%2520visualize%2520these%2520shadows%2520along%2520the%2520circular%2520trajectory.%2520A%2520space-carving%2520reconstruction%2520method%2520is%2520applied%2520to%2520infer%2520the%25203D%2520shape%2520of%2520the%2520object%2520from%2520the%2520segmented%2520shadows.%2520The%2520results%2520demonstrate%2520the%2520potential%2520of%2520shadows%2520in%2520circular%2520SAS%2520for%2520improving%2520target%2520analysis%2520and%25203D%2520reconstruction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Shadows%20in%20Circular%20Synthetic%20Aperture%20Sonar%20Imaging%20for%20Target%20Analysis&entry.906535625=Yann%20Le%20Gall%20and%20Nicolas%20Burlet%20and%20Mathieu%20Simon%20and%20Fabien%20Novella%20and%20Samantha%20Dugelay%20and%20Jean-Philippe%20Malkasse&entry.1292438233=Circular%20Synthetic%20Aperture%20Sonar%20%28CSAS%29%20provides%20a%20360%C2%B0%20azimuth%20view%20of%20the%20seabed%2C%20surpassing%20the%20limited%20aperture%20and%20mono-view%20image%20of%20conventional%20side-scan%20SAS.%20This%20makes%20CSAS%20a%20valuable%20tool%20for%20target%20recognition%20in%20mine%20warfare%20where%20the%20diversity%20of%20point%20of%20view%20is%20essential%20for%20reducing%20false%20alarms.%20CSAS%20processing%20typically%20produces%20a%20very%20high-resolution%20two-dimensional%20image.%20However%2C%20the%20parallax%20introduced%20by%20the%20circular%20displacement%20of%20the%20illuminator%20fill-in%20the%20shadow%20regions%2C%20and%20the%20shadow%20cast%20by%20an%20object%20on%20the%20seafloor%20is%20lost%20in%20favor%20of%20azimuth%20coverage%20and%20resolution.%20Yet%20the%20shadows%20provide%20complementary%20information%20on%20target%20shape%20useful%20for%20target%20recognition.%20In%20this%20paper%2C%20we%20explore%20a%20way%20to%20retrieve%20shadow%20information%20from%20CSAS%20data%20to%20improve%20target%20analysis%20and%20carry%203D%20reconstruction.%20Sub-aperture%20filtering%20is%20used%20to%20get%20a%20collection%20of%20images%20at%20various%20points%20of%20view%20along%20the%20circular%20trajectory%20and%20fixed%20focus%20shadow%20enhancement%20%28FFSE%29%20is%20applied%20to%20obtain%20sharp%20shadows.%20An%20interactive%20interface%20is%20also%20proposed%20to%20allow%20human%20operators%20to%20visualize%20these%20shadows%20along%20the%20circular%20trajectory.%20A%20space-carving%20reconstruction%20method%20is%20applied%20to%20infer%20the%203D%20shape%20of%20the%20object%20from%20the%20segmented%20shadows.%20The%20results%20demonstrate%20the%20potential%20of%20shadows%20in%20circular%20SAS%20for%20improving%20target%20analysis%20and%203D%20reconstruction.&entry.1838667208=http%3A//arxiv.org/abs/2601.16733v1&entry.124074799=Read"},
{"title": "Reliable Brain Tumor Segmentation Based on Spiking Neural Networks with Efficient Training", "author": "Aurora Pia Ghiardelli and Guangzhi Tang and Tao Sun", "abstract": "We propose a reliable and energy-efficient framework for 3D brain tumor segmentation using spiking neural networks (SNNs). A multi-view ensemble of sagittal, coronal, and axial SNN models provides voxel-wise uncertainty estimation and enhances segmentation robustness. To address the high computational cost in training SNN models for semantic image segmentation, we employ Forward Propagation Through Time (FPTT), which maintains temporal learning efficiency with significantly reduced computational cost. Experiments on the Multimodal Brain Tumor Segmentation Challenges (BraTS 2017 and BraTS 2023) demonstrate competitive accuracy, well-calibrated uncertainty, and an 87% reduction in FLOPs, underscoring the potential of SNNs for reliable, low-power medical IoT and Point-of-Care systems.", "link": "http://arxiv.org/abs/2601.16652v1", "date": "2026-01-23", "relevancy": 2.4842, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5096}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4974}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reliable%20Brain%20Tumor%20Segmentation%20Based%20on%20Spiking%20Neural%20Networks%20with%20Efficient%20Training&body=Title%3A%20Reliable%20Brain%20Tumor%20Segmentation%20Based%20on%20Spiking%20Neural%20Networks%20with%20Efficient%20Training%0AAuthor%3A%20Aurora%20Pia%20Ghiardelli%20and%20Guangzhi%20Tang%20and%20Tao%20Sun%0AAbstract%3A%20We%20propose%20a%20reliable%20and%20energy-efficient%20framework%20for%203D%20brain%20tumor%20segmentation%20using%20spiking%20neural%20networks%20%28SNNs%29.%20A%20multi-view%20ensemble%20of%20sagittal%2C%20coronal%2C%20and%20axial%20SNN%20models%20provides%20voxel-wise%20uncertainty%20estimation%20and%20enhances%20segmentation%20robustness.%20To%20address%20the%20high%20computational%20cost%20in%20training%20SNN%20models%20for%20semantic%20image%20segmentation%2C%20we%20employ%20Forward%20Propagation%20Through%20Time%20%28FPTT%29%2C%20which%20maintains%20temporal%20learning%20efficiency%20with%20significantly%20reduced%20computational%20cost.%20Experiments%20on%20the%20Multimodal%20Brain%20Tumor%20Segmentation%20Challenges%20%28BraTS%202017%20and%20BraTS%202023%29%20demonstrate%20competitive%20accuracy%2C%20well-calibrated%20uncertainty%2C%20and%20an%2087%25%20reduction%20in%20FLOPs%2C%20underscoring%20the%20potential%20of%20SNNs%20for%20reliable%2C%20low-power%20medical%20IoT%20and%20Point-of-Care%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16652v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReliable%2520Brain%2520Tumor%2520Segmentation%2520Based%2520on%2520Spiking%2520Neural%2520Networks%2520with%2520Efficient%2520Training%26entry.906535625%3DAurora%2520Pia%2520Ghiardelli%2520and%2520Guangzhi%2520Tang%2520and%2520Tao%2520Sun%26entry.1292438233%3DWe%2520propose%2520a%2520reliable%2520and%2520energy-efficient%2520framework%2520for%25203D%2520brain%2520tumor%2520segmentation%2520using%2520spiking%2520neural%2520networks%2520%2528SNNs%2529.%2520A%2520multi-view%2520ensemble%2520of%2520sagittal%252C%2520coronal%252C%2520and%2520axial%2520SNN%2520models%2520provides%2520voxel-wise%2520uncertainty%2520estimation%2520and%2520enhances%2520segmentation%2520robustness.%2520To%2520address%2520the%2520high%2520computational%2520cost%2520in%2520training%2520SNN%2520models%2520for%2520semantic%2520image%2520segmentation%252C%2520we%2520employ%2520Forward%2520Propagation%2520Through%2520Time%2520%2528FPTT%2529%252C%2520which%2520maintains%2520temporal%2520learning%2520efficiency%2520with%2520significantly%2520reduced%2520computational%2520cost.%2520Experiments%2520on%2520the%2520Multimodal%2520Brain%2520Tumor%2520Segmentation%2520Challenges%2520%2528BraTS%25202017%2520and%2520BraTS%25202023%2529%2520demonstrate%2520competitive%2520accuracy%252C%2520well-calibrated%2520uncertainty%252C%2520and%2520an%252087%2525%2520reduction%2520in%2520FLOPs%252C%2520underscoring%2520the%2520potential%2520of%2520SNNs%2520for%2520reliable%252C%2520low-power%2520medical%2520IoT%2520and%2520Point-of-Care%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16652v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reliable%20Brain%20Tumor%20Segmentation%20Based%20on%20Spiking%20Neural%20Networks%20with%20Efficient%20Training&entry.906535625=Aurora%20Pia%20Ghiardelli%20and%20Guangzhi%20Tang%20and%20Tao%20Sun&entry.1292438233=We%20propose%20a%20reliable%20and%20energy-efficient%20framework%20for%203D%20brain%20tumor%20segmentation%20using%20spiking%20neural%20networks%20%28SNNs%29.%20A%20multi-view%20ensemble%20of%20sagittal%2C%20coronal%2C%20and%20axial%20SNN%20models%20provides%20voxel-wise%20uncertainty%20estimation%20and%20enhances%20segmentation%20robustness.%20To%20address%20the%20high%20computational%20cost%20in%20training%20SNN%20models%20for%20semantic%20image%20segmentation%2C%20we%20employ%20Forward%20Propagation%20Through%20Time%20%28FPTT%29%2C%20which%20maintains%20temporal%20learning%20efficiency%20with%20significantly%20reduced%20computational%20cost.%20Experiments%20on%20the%20Multimodal%20Brain%20Tumor%20Segmentation%20Challenges%20%28BraTS%202017%20and%20BraTS%202023%29%20demonstrate%20competitive%20accuracy%2C%20well-calibrated%20uncertainty%2C%20and%20an%2087%25%20reduction%20in%20FLOPs%2C%20underscoring%20the%20potential%20of%20SNNs%20for%20reliable%2C%20low-power%20medical%20IoT%20and%20Point-of-Care%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.16652v1&entry.124074799=Read"},
{"title": "CASP: Few-Shot Class-Incremental Learning with CLS Token Attention Steering Prompts", "author": "Shuai Huang and Xuhan Lin and Yuwu Lu", "abstract": "Few-shot class-incremental learning (FSCIL) presents a core challenge in continual learning, requiring models to rapidly adapt to new classes with very limited samples while mitigating catastrophic forgetting. Recent prompt-based methods, which integrate pretrained backbones with task-specific prompts, have made notable progress. However, under extreme few-shot incremental settings, the model's ability to transfer and generalize becomes critical, and it is thus essential to leverage pretrained knowledge to learn feature representations that can be shared across future categories during the base session. Inspired by the mechanism of the CLS token, which is similar to human attention and progressively filters out task-irrelevant information, we propose the CLS Token Attention Steering Prompts (CASP). This approach introduces class-shared trainable bias parameters into the query, key, and value projections of the CLS token to explicitly modulate the self-attention weights. To further enhance generalization, we also design an attention perturbation strategy and perform Manifold Token Mixup in the shallow feature space, synthesizing potential new class features to improve generalization and reserve the representation capacity for upcoming tasks. Experiments on the CUB200, CIFAR100, and ImageNet-R datasets demonstrate that CASP outperforms state-of-the-art methods in both standard and fine-grained FSCIL settings without requiring fine-tuning during incremental phases and while significantly reducing the parameter overhead.", "link": "http://arxiv.org/abs/2601.16773v1", "date": "2026-01-23", "relevancy": 2.4797, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4992}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4953}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CASP%3A%20Few-Shot%20Class-Incremental%20Learning%20with%20CLS%20Token%20Attention%20Steering%20Prompts&body=Title%3A%20CASP%3A%20Few-Shot%20Class-Incremental%20Learning%20with%20CLS%20Token%20Attention%20Steering%20Prompts%0AAuthor%3A%20Shuai%20Huang%20and%20Xuhan%20Lin%20and%20Yuwu%20Lu%0AAbstract%3A%20Few-shot%20class-incremental%20learning%20%28FSCIL%29%20presents%20a%20core%20challenge%20in%20continual%20learning%2C%20requiring%20models%20to%20rapidly%20adapt%20to%20new%20classes%20with%20very%20limited%20samples%20while%20mitigating%20catastrophic%20forgetting.%20Recent%20prompt-based%20methods%2C%20which%20integrate%20pretrained%20backbones%20with%20task-specific%20prompts%2C%20have%20made%20notable%20progress.%20However%2C%20under%20extreme%20few-shot%20incremental%20settings%2C%20the%20model%27s%20ability%20to%20transfer%20and%20generalize%20becomes%20critical%2C%20and%20it%20is%20thus%20essential%20to%20leverage%20pretrained%20knowledge%20to%20learn%20feature%20representations%20that%20can%20be%20shared%20across%20future%20categories%20during%20the%20base%20session.%20Inspired%20by%20the%20mechanism%20of%20the%20CLS%20token%2C%20which%20is%20similar%20to%20human%20attention%20and%20progressively%20filters%20out%20task-irrelevant%20information%2C%20we%20propose%20the%20CLS%20Token%20Attention%20Steering%20Prompts%20%28CASP%29.%20This%20approach%20introduces%20class-shared%20trainable%20bias%20parameters%20into%20the%20query%2C%20key%2C%20and%20value%20projections%20of%20the%20CLS%20token%20to%20explicitly%20modulate%20the%20self-attention%20weights.%20To%20further%20enhance%20generalization%2C%20we%20also%20design%20an%20attention%20perturbation%20strategy%20and%20perform%20Manifold%20Token%20Mixup%20in%20the%20shallow%20feature%20space%2C%20synthesizing%20potential%20new%20class%20features%20to%20improve%20generalization%20and%20reserve%20the%20representation%20capacity%20for%20upcoming%20tasks.%20Experiments%20on%20the%20CUB200%2C%20CIFAR100%2C%20and%20ImageNet-R%20datasets%20demonstrate%20that%20CASP%20outperforms%20state-of-the-art%20methods%20in%20both%20standard%20and%20fine-grained%20FSCIL%20settings%20without%20requiring%20fine-tuning%20during%20incremental%20phases%20and%20while%20significantly%20reducing%20the%20parameter%20overhead.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16773v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCASP%253A%2520Few-Shot%2520Class-Incremental%2520Learning%2520with%2520CLS%2520Token%2520Attention%2520Steering%2520Prompts%26entry.906535625%3DShuai%2520Huang%2520and%2520Xuhan%2520Lin%2520and%2520Yuwu%2520Lu%26entry.1292438233%3DFew-shot%2520class-incremental%2520learning%2520%2528FSCIL%2529%2520presents%2520a%2520core%2520challenge%2520in%2520continual%2520learning%252C%2520requiring%2520models%2520to%2520rapidly%2520adapt%2520to%2520new%2520classes%2520with%2520very%2520limited%2520samples%2520while%2520mitigating%2520catastrophic%2520forgetting.%2520Recent%2520prompt-based%2520methods%252C%2520which%2520integrate%2520pretrained%2520backbones%2520with%2520task-specific%2520prompts%252C%2520have%2520made%2520notable%2520progress.%2520However%252C%2520under%2520extreme%2520few-shot%2520incremental%2520settings%252C%2520the%2520model%2527s%2520ability%2520to%2520transfer%2520and%2520generalize%2520becomes%2520critical%252C%2520and%2520it%2520is%2520thus%2520essential%2520to%2520leverage%2520pretrained%2520knowledge%2520to%2520learn%2520feature%2520representations%2520that%2520can%2520be%2520shared%2520across%2520future%2520categories%2520during%2520the%2520base%2520session.%2520Inspired%2520by%2520the%2520mechanism%2520of%2520the%2520CLS%2520token%252C%2520which%2520is%2520similar%2520to%2520human%2520attention%2520and%2520progressively%2520filters%2520out%2520task-irrelevant%2520information%252C%2520we%2520propose%2520the%2520CLS%2520Token%2520Attention%2520Steering%2520Prompts%2520%2528CASP%2529.%2520This%2520approach%2520introduces%2520class-shared%2520trainable%2520bias%2520parameters%2520into%2520the%2520query%252C%2520key%252C%2520and%2520value%2520projections%2520of%2520the%2520CLS%2520token%2520to%2520explicitly%2520modulate%2520the%2520self-attention%2520weights.%2520To%2520further%2520enhance%2520generalization%252C%2520we%2520also%2520design%2520an%2520attention%2520perturbation%2520strategy%2520and%2520perform%2520Manifold%2520Token%2520Mixup%2520in%2520the%2520shallow%2520feature%2520space%252C%2520synthesizing%2520potential%2520new%2520class%2520features%2520to%2520improve%2520generalization%2520and%2520reserve%2520the%2520representation%2520capacity%2520for%2520upcoming%2520tasks.%2520Experiments%2520on%2520the%2520CUB200%252C%2520CIFAR100%252C%2520and%2520ImageNet-R%2520datasets%2520demonstrate%2520that%2520CASP%2520outperforms%2520state-of-the-art%2520methods%2520in%2520both%2520standard%2520and%2520fine-grained%2520FSCIL%2520settings%2520without%2520requiring%2520fine-tuning%2520during%2520incremental%2520phases%2520and%2520while%2520significantly%2520reducing%2520the%2520parameter%2520overhead.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16773v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CASP%3A%20Few-Shot%20Class-Incremental%20Learning%20with%20CLS%20Token%20Attention%20Steering%20Prompts&entry.906535625=Shuai%20Huang%20and%20Xuhan%20Lin%20and%20Yuwu%20Lu&entry.1292438233=Few-shot%20class-incremental%20learning%20%28FSCIL%29%20presents%20a%20core%20challenge%20in%20continual%20learning%2C%20requiring%20models%20to%20rapidly%20adapt%20to%20new%20classes%20with%20very%20limited%20samples%20while%20mitigating%20catastrophic%20forgetting.%20Recent%20prompt-based%20methods%2C%20which%20integrate%20pretrained%20backbones%20with%20task-specific%20prompts%2C%20have%20made%20notable%20progress.%20However%2C%20under%20extreme%20few-shot%20incremental%20settings%2C%20the%20model%27s%20ability%20to%20transfer%20and%20generalize%20becomes%20critical%2C%20and%20it%20is%20thus%20essential%20to%20leverage%20pretrained%20knowledge%20to%20learn%20feature%20representations%20that%20can%20be%20shared%20across%20future%20categories%20during%20the%20base%20session.%20Inspired%20by%20the%20mechanism%20of%20the%20CLS%20token%2C%20which%20is%20similar%20to%20human%20attention%20and%20progressively%20filters%20out%20task-irrelevant%20information%2C%20we%20propose%20the%20CLS%20Token%20Attention%20Steering%20Prompts%20%28CASP%29.%20This%20approach%20introduces%20class-shared%20trainable%20bias%20parameters%20into%20the%20query%2C%20key%2C%20and%20value%20projections%20of%20the%20CLS%20token%20to%20explicitly%20modulate%20the%20self-attention%20weights.%20To%20further%20enhance%20generalization%2C%20we%20also%20design%20an%20attention%20perturbation%20strategy%20and%20perform%20Manifold%20Token%20Mixup%20in%20the%20shallow%20feature%20space%2C%20synthesizing%20potential%20new%20class%20features%20to%20improve%20generalization%20and%20reserve%20the%20representation%20capacity%20for%20upcoming%20tasks.%20Experiments%20on%20the%20CUB200%2C%20CIFAR100%2C%20and%20ImageNet-R%20datasets%20demonstrate%20that%20CASP%20outperforms%20state-of-the-art%20methods%20in%20both%20standard%20and%20fine-grained%20FSCIL%20settings%20without%20requiring%20fine-tuning%20during%20incremental%20phases%20and%20while%20significantly%20reducing%20the%20parameter%20overhead.&entry.1838667208=http%3A//arxiv.org/abs/2601.16773v1&entry.124074799=Read"},
{"title": "SAMRI: Segment Anything Model for MRI", "author": "Zhao Wang and Wei Dai and Thuy Thanh Dao and Steffen Bollmann and Hongfu Sun and Craig Engstrom and Shekhar S. Chandra", "abstract": "Accurate magnetic resonance imaging (MRI) segmentation is crucial for clinical decision-making, but remains labor-intensive when performed manually. Convolutional neural network (CNN) based methods can be accurate and efficient but often generalize poorly to MRI variable contrast, intensity inhomogeneity, and sequences. Although the transformer-based Segment Anything Model (SAM) has demonstrated remarkable generalizability in natural images, existing adaptations often treat MRI as another imaging modality, overlooking these modality-specific challenges. We present SAMRI, an MRI-specialized SAM trained and validated on 1.1 million labeled MR slices spanning whole-body organs and pathologies. We demonstrate that SAM can be effectively adapted to MRI by fine-tuning its mask decoder using a two-stage strategy, reducing training time by 94 percent and trainable parameters by 96 percent compared to full-model retraining. Across diverse MRI segmentation tasks, SAMRI achieves a mean Dice of 0.87, delivering state-of-the-art accuracy across anatomical regions and robust generalization on unseen structures, particularly small clinically important structures. In addition, we provide a complete training-to-inference pipeline and a user-friendly local graphical interface that enables interactive application of pretrained SAMRI models on standard machines, facilitating practical deployment for real-world MRI segmentation.", "link": "http://arxiv.org/abs/2510.26635v2", "date": "2026-01-23", "relevancy": 2.4726, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5232}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4961}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAMRI%3A%20Segment%20Anything%20Model%20for%20MRI&body=Title%3A%20SAMRI%3A%20Segment%20Anything%20Model%20for%20MRI%0AAuthor%3A%20Zhao%20Wang%20and%20Wei%20Dai%20and%20Thuy%20Thanh%20Dao%20and%20Steffen%20Bollmann%20and%20Hongfu%20Sun%20and%20Craig%20Engstrom%20and%20Shekhar%20S.%20Chandra%0AAbstract%3A%20Accurate%20magnetic%20resonance%20imaging%20%28MRI%29%20segmentation%20is%20crucial%20for%20clinical%20decision-making%2C%20but%20remains%20labor-intensive%20when%20performed%20manually.%20Convolutional%20neural%20network%20%28CNN%29%20based%20methods%20can%20be%20accurate%20and%20efficient%20but%20often%20generalize%20poorly%20to%20MRI%20variable%20contrast%2C%20intensity%20inhomogeneity%2C%20and%20sequences.%20Although%20the%20transformer-based%20Segment%20Anything%20Model%20%28SAM%29%20has%20demonstrated%20remarkable%20generalizability%20in%20natural%20images%2C%20existing%20adaptations%20often%20treat%20MRI%20as%20another%20imaging%20modality%2C%20overlooking%20these%20modality-specific%20challenges.%20We%20present%20SAMRI%2C%20an%20MRI-specialized%20SAM%20trained%20and%20validated%20on%201.1%20million%20labeled%20MR%20slices%20spanning%20whole-body%20organs%20and%20pathologies.%20We%20demonstrate%20that%20SAM%20can%20be%20effectively%20adapted%20to%20MRI%20by%20fine-tuning%20its%20mask%20decoder%20using%20a%20two-stage%20strategy%2C%20reducing%20training%20time%20by%2094%20percent%20and%20trainable%20parameters%20by%2096%20percent%20compared%20to%20full-model%20retraining.%20Across%20diverse%20MRI%20segmentation%20tasks%2C%20SAMRI%20achieves%20a%20mean%20Dice%20of%200.87%2C%20delivering%20state-of-the-art%20accuracy%20across%20anatomical%20regions%20and%20robust%20generalization%20on%20unseen%20structures%2C%20particularly%20small%20clinically%20important%20structures.%20In%20addition%2C%20we%20provide%20a%20complete%20training-to-inference%20pipeline%20and%20a%20user-friendly%20local%20graphical%20interface%20that%20enables%20interactive%20application%20of%20pretrained%20SAMRI%20models%20on%20standard%20machines%2C%20facilitating%20practical%20deployment%20for%20real-world%20MRI%20segmentation.%0ALink%3A%20http%3A//arxiv.org/abs/2510.26635v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAMRI%253A%2520Segment%2520Anything%2520Model%2520for%2520MRI%26entry.906535625%3DZhao%2520Wang%2520and%2520Wei%2520Dai%2520and%2520Thuy%2520Thanh%2520Dao%2520and%2520Steffen%2520Bollmann%2520and%2520Hongfu%2520Sun%2520and%2520Craig%2520Engstrom%2520and%2520Shekhar%2520S.%2520Chandra%26entry.1292438233%3DAccurate%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520segmentation%2520is%2520crucial%2520for%2520clinical%2520decision-making%252C%2520but%2520remains%2520labor-intensive%2520when%2520performed%2520manually.%2520Convolutional%2520neural%2520network%2520%2528CNN%2529%2520based%2520methods%2520can%2520be%2520accurate%2520and%2520efficient%2520but%2520often%2520generalize%2520poorly%2520to%2520MRI%2520variable%2520contrast%252C%2520intensity%2520inhomogeneity%252C%2520and%2520sequences.%2520Although%2520the%2520transformer-based%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520has%2520demonstrated%2520remarkable%2520generalizability%2520in%2520natural%2520images%252C%2520existing%2520adaptations%2520often%2520treat%2520MRI%2520as%2520another%2520imaging%2520modality%252C%2520overlooking%2520these%2520modality-specific%2520challenges.%2520We%2520present%2520SAMRI%252C%2520an%2520MRI-specialized%2520SAM%2520trained%2520and%2520validated%2520on%25201.1%2520million%2520labeled%2520MR%2520slices%2520spanning%2520whole-body%2520organs%2520and%2520pathologies.%2520We%2520demonstrate%2520that%2520SAM%2520can%2520be%2520effectively%2520adapted%2520to%2520MRI%2520by%2520fine-tuning%2520its%2520mask%2520decoder%2520using%2520a%2520two-stage%2520strategy%252C%2520reducing%2520training%2520time%2520by%252094%2520percent%2520and%2520trainable%2520parameters%2520by%252096%2520percent%2520compared%2520to%2520full-model%2520retraining.%2520Across%2520diverse%2520MRI%2520segmentation%2520tasks%252C%2520SAMRI%2520achieves%2520a%2520mean%2520Dice%2520of%25200.87%252C%2520delivering%2520state-of-the-art%2520accuracy%2520across%2520anatomical%2520regions%2520and%2520robust%2520generalization%2520on%2520unseen%2520structures%252C%2520particularly%2520small%2520clinically%2520important%2520structures.%2520In%2520addition%252C%2520we%2520provide%2520a%2520complete%2520training-to-inference%2520pipeline%2520and%2520a%2520user-friendly%2520local%2520graphical%2520interface%2520that%2520enables%2520interactive%2520application%2520of%2520pretrained%2520SAMRI%2520models%2520on%2520standard%2520machines%252C%2520facilitating%2520practical%2520deployment%2520for%2520real-world%2520MRI%2520segmentation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26635v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAMRI%3A%20Segment%20Anything%20Model%20for%20MRI&entry.906535625=Zhao%20Wang%20and%20Wei%20Dai%20and%20Thuy%20Thanh%20Dao%20and%20Steffen%20Bollmann%20and%20Hongfu%20Sun%20and%20Craig%20Engstrom%20and%20Shekhar%20S.%20Chandra&entry.1292438233=Accurate%20magnetic%20resonance%20imaging%20%28MRI%29%20segmentation%20is%20crucial%20for%20clinical%20decision-making%2C%20but%20remains%20labor-intensive%20when%20performed%20manually.%20Convolutional%20neural%20network%20%28CNN%29%20based%20methods%20can%20be%20accurate%20and%20efficient%20but%20often%20generalize%20poorly%20to%20MRI%20variable%20contrast%2C%20intensity%20inhomogeneity%2C%20and%20sequences.%20Although%20the%20transformer-based%20Segment%20Anything%20Model%20%28SAM%29%20has%20demonstrated%20remarkable%20generalizability%20in%20natural%20images%2C%20existing%20adaptations%20often%20treat%20MRI%20as%20another%20imaging%20modality%2C%20overlooking%20these%20modality-specific%20challenges.%20We%20present%20SAMRI%2C%20an%20MRI-specialized%20SAM%20trained%20and%20validated%20on%201.1%20million%20labeled%20MR%20slices%20spanning%20whole-body%20organs%20and%20pathologies.%20We%20demonstrate%20that%20SAM%20can%20be%20effectively%20adapted%20to%20MRI%20by%20fine-tuning%20its%20mask%20decoder%20using%20a%20two-stage%20strategy%2C%20reducing%20training%20time%20by%2094%20percent%20and%20trainable%20parameters%20by%2096%20percent%20compared%20to%20full-model%20retraining.%20Across%20diverse%20MRI%20segmentation%20tasks%2C%20SAMRI%20achieves%20a%20mean%20Dice%20of%200.87%2C%20delivering%20state-of-the-art%20accuracy%20across%20anatomical%20regions%20and%20robust%20generalization%20on%20unseen%20structures%2C%20particularly%20small%20clinically%20important%20structures.%20In%20addition%2C%20we%20provide%20a%20complete%20training-to-inference%20pipeline%20and%20a%20user-friendly%20local%20graphical%20interface%20that%20enables%20interactive%20application%20of%20pretrained%20SAMRI%20models%20on%20standard%20machines%2C%20facilitating%20practical%20deployment%20for%20real-world%20MRI%20segmentation.&entry.1838667208=http%3A//arxiv.org/abs/2510.26635v2&entry.124074799=Read"},
{"title": "Generative Confidants: How do People Experience Trust in Emotional Support from Generative AI?", "author": "Riccardo Volpato and Simone Stumpf and Lisa DeBruine", "abstract": "People are increasingly turning to generative AI (e.g., ChatGPT, Gemini, Copilot) for emotional support and companionship. While trust is likely to play a central role in enabling these informal and unsupervised interactions, we still lack an understanding of how people develop and experience it in this context. Seeking to fill this gap, we recruited 24 frequent users of generative AI for emotional support and conducted a qualitative study consisting of diary entries about interactions, transcripts of chats with AI, and in-depth interviews. Our results suggest important novel drivers of trust in this context: familiarity emerging from personalisation, nuanced mental models of generative AI, and awareness of people's control over conversations. Notably, generative AI's homogeneous use of personalised, positive, and persuasive language appears to promote some of these trust-building factors. However, this also seems to discourage other trust-related behaviours, such as remembering that generative AI is a machine trained to converse in human language. We present implications for future research that are likely to become critical as the use of generative AI for emotional support increasingly overlaps with therapeutic work.", "link": "http://arxiv.org/abs/2601.16656v1", "date": "2026-01-23", "relevancy": 2.4483, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5319}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.479}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Confidants%3A%20How%20do%20People%20Experience%20Trust%20in%20Emotional%20Support%20from%20Generative%20AI%3F&body=Title%3A%20Generative%20Confidants%3A%20How%20do%20People%20Experience%20Trust%20in%20Emotional%20Support%20from%20Generative%20AI%3F%0AAuthor%3A%20Riccardo%20Volpato%20and%20Simone%20Stumpf%20and%20Lisa%20DeBruine%0AAbstract%3A%20People%20are%20increasingly%20turning%20to%20generative%20AI%20%28e.g.%2C%20ChatGPT%2C%20Gemini%2C%20Copilot%29%20for%20emotional%20support%20and%20companionship.%20While%20trust%20is%20likely%20to%20play%20a%20central%20role%20in%20enabling%20these%20informal%20and%20unsupervised%20interactions%2C%20we%20still%20lack%20an%20understanding%20of%20how%20people%20develop%20and%20experience%20it%20in%20this%20context.%20Seeking%20to%20fill%20this%20gap%2C%20we%20recruited%2024%20frequent%20users%20of%20generative%20AI%20for%20emotional%20support%20and%20conducted%20a%20qualitative%20study%20consisting%20of%20diary%20entries%20about%20interactions%2C%20transcripts%20of%20chats%20with%20AI%2C%20and%20in-depth%20interviews.%20Our%20results%20suggest%20important%20novel%20drivers%20of%20trust%20in%20this%20context%3A%20familiarity%20emerging%20from%20personalisation%2C%20nuanced%20mental%20models%20of%20generative%20AI%2C%20and%20awareness%20of%20people%27s%20control%20over%20conversations.%20Notably%2C%20generative%20AI%27s%20homogeneous%20use%20of%20personalised%2C%20positive%2C%20and%20persuasive%20language%20appears%20to%20promote%20some%20of%20these%20trust-building%20factors.%20However%2C%20this%20also%20seems%20to%20discourage%20other%20trust-related%20behaviours%2C%20such%20as%20remembering%20that%20generative%20AI%20is%20a%20machine%20trained%20to%20converse%20in%20human%20language.%20We%20present%20implications%20for%20future%20research%20that%20are%20likely%20to%20become%20critical%20as%20the%20use%20of%20generative%20AI%20for%20emotional%20support%20increasingly%20overlaps%20with%20therapeutic%20work.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16656v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Confidants%253A%2520How%2520do%2520People%2520Experience%2520Trust%2520in%2520Emotional%2520Support%2520from%2520Generative%2520AI%253F%26entry.906535625%3DRiccardo%2520Volpato%2520and%2520Simone%2520Stumpf%2520and%2520Lisa%2520DeBruine%26entry.1292438233%3DPeople%2520are%2520increasingly%2520turning%2520to%2520generative%2520AI%2520%2528e.g.%252C%2520ChatGPT%252C%2520Gemini%252C%2520Copilot%2529%2520for%2520emotional%2520support%2520and%2520companionship.%2520While%2520trust%2520is%2520likely%2520to%2520play%2520a%2520central%2520role%2520in%2520enabling%2520these%2520informal%2520and%2520unsupervised%2520interactions%252C%2520we%2520still%2520lack%2520an%2520understanding%2520of%2520how%2520people%2520develop%2520and%2520experience%2520it%2520in%2520this%2520context.%2520Seeking%2520to%2520fill%2520this%2520gap%252C%2520we%2520recruited%252024%2520frequent%2520users%2520of%2520generative%2520AI%2520for%2520emotional%2520support%2520and%2520conducted%2520a%2520qualitative%2520study%2520consisting%2520of%2520diary%2520entries%2520about%2520interactions%252C%2520transcripts%2520of%2520chats%2520with%2520AI%252C%2520and%2520in-depth%2520interviews.%2520Our%2520results%2520suggest%2520important%2520novel%2520drivers%2520of%2520trust%2520in%2520this%2520context%253A%2520familiarity%2520emerging%2520from%2520personalisation%252C%2520nuanced%2520mental%2520models%2520of%2520generative%2520AI%252C%2520and%2520awareness%2520of%2520people%2527s%2520control%2520over%2520conversations.%2520Notably%252C%2520generative%2520AI%2527s%2520homogeneous%2520use%2520of%2520personalised%252C%2520positive%252C%2520and%2520persuasive%2520language%2520appears%2520to%2520promote%2520some%2520of%2520these%2520trust-building%2520factors.%2520However%252C%2520this%2520also%2520seems%2520to%2520discourage%2520other%2520trust-related%2520behaviours%252C%2520such%2520as%2520remembering%2520that%2520generative%2520AI%2520is%2520a%2520machine%2520trained%2520to%2520converse%2520in%2520human%2520language.%2520We%2520present%2520implications%2520for%2520future%2520research%2520that%2520are%2520likely%2520to%2520become%2520critical%2520as%2520the%2520use%2520of%2520generative%2520AI%2520for%2520emotional%2520support%2520increasingly%2520overlaps%2520with%2520therapeutic%2520work.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16656v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Confidants%3A%20How%20do%20People%20Experience%20Trust%20in%20Emotional%20Support%20from%20Generative%20AI%3F&entry.906535625=Riccardo%20Volpato%20and%20Simone%20Stumpf%20and%20Lisa%20DeBruine&entry.1292438233=People%20are%20increasingly%20turning%20to%20generative%20AI%20%28e.g.%2C%20ChatGPT%2C%20Gemini%2C%20Copilot%29%20for%20emotional%20support%20and%20companionship.%20While%20trust%20is%20likely%20to%20play%20a%20central%20role%20in%20enabling%20these%20informal%20and%20unsupervised%20interactions%2C%20we%20still%20lack%20an%20understanding%20of%20how%20people%20develop%20and%20experience%20it%20in%20this%20context.%20Seeking%20to%20fill%20this%20gap%2C%20we%20recruited%2024%20frequent%20users%20of%20generative%20AI%20for%20emotional%20support%20and%20conducted%20a%20qualitative%20study%20consisting%20of%20diary%20entries%20about%20interactions%2C%20transcripts%20of%20chats%20with%20AI%2C%20and%20in-depth%20interviews.%20Our%20results%20suggest%20important%20novel%20drivers%20of%20trust%20in%20this%20context%3A%20familiarity%20emerging%20from%20personalisation%2C%20nuanced%20mental%20models%20of%20generative%20AI%2C%20and%20awareness%20of%20people%27s%20control%20over%20conversations.%20Notably%2C%20generative%20AI%27s%20homogeneous%20use%20of%20personalised%2C%20positive%2C%20and%20persuasive%20language%20appears%20to%20promote%20some%20of%20these%20trust-building%20factors.%20However%2C%20this%20also%20seems%20to%20discourage%20other%20trust-related%20behaviours%2C%20such%20as%20remembering%20that%20generative%20AI%20is%20a%20machine%20trained%20to%20converse%20in%20human%20language.%20We%20present%20implications%20for%20future%20research%20that%20are%20likely%20to%20become%20critical%20as%20the%20use%20of%20generative%20AI%20for%20emotional%20support%20increasingly%20overlaps%20with%20therapeutic%20work.&entry.1838667208=http%3A//arxiv.org/abs/2601.16656v1&entry.124074799=Read"},
{"title": "Vertical Semi-Federated Learning for Efficient Online Advertising", "author": "Wenjie Li and Shu-Tao Xia and Jiangke Fan and Teng Zhang and Xingxing Wang", "abstract": "Traditional vertical federated learning schema suffers from two main issues: 1) restricted applicable scope to overlapped samples and 2) high system challenge of real-time federated serving, which limits its application to advertising systems. To this end, we advocate a new practical learning setting, Semi-VFL (Vertical Semi-Federated Learning), for real-world industrial applications, where the learned model retains sufficient advantages of federated learning while supporting independent local serving. To achieve this goal, we propose the carefully designed Joint Privileged Learning framework (JPL) to i) alleviate the absence of the passive party's feature with federated equivalence imitation and ii) adapt to the heterogeneous full sample space with cross-branch rank alignment. Extensive experiments conducted on real-world advertising datasets validate the effectiveness of our method over baseline methods.", "link": "http://arxiv.org/abs/2209.15635v4", "date": "2026-01-23", "relevancy": 2.4386, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4984}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4887}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vertical%20Semi-Federated%20Learning%20for%20Efficient%20Online%20Advertising&body=Title%3A%20Vertical%20Semi-Federated%20Learning%20for%20Efficient%20Online%20Advertising%0AAuthor%3A%20Wenjie%20Li%20and%20Shu-Tao%20Xia%20and%20Jiangke%20Fan%20and%20Teng%20Zhang%20and%20Xingxing%20Wang%0AAbstract%3A%20Traditional%20vertical%20federated%20learning%20schema%20suffers%20from%20two%20main%20issues%3A%201%29%20restricted%20applicable%20scope%20to%20overlapped%20samples%20and%202%29%20high%20system%20challenge%20of%20real-time%20federated%20serving%2C%20which%20limits%20its%20application%20to%20advertising%20systems.%20To%20this%20end%2C%20we%20advocate%20a%20new%20practical%20learning%20setting%2C%20Semi-VFL%20%28Vertical%20Semi-Federated%20Learning%29%2C%20for%20real-world%20industrial%20applications%2C%20where%20the%20learned%20model%20retains%20sufficient%20advantages%20of%20federated%20learning%20while%20supporting%20independent%20local%20serving.%20To%20achieve%20this%20goal%2C%20we%20propose%20the%20carefully%20designed%20Joint%20Privileged%20Learning%20framework%20%28JPL%29%20to%20i%29%20alleviate%20the%20absence%20of%20the%20passive%20party%27s%20feature%20with%20federated%20equivalence%20imitation%20and%20ii%29%20adapt%20to%20the%20heterogeneous%20full%20sample%20space%20with%20cross-branch%20rank%20alignment.%20Extensive%20experiments%20conducted%20on%20real-world%20advertising%20datasets%20validate%20the%20effectiveness%20of%20our%20method%20over%20baseline%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2209.15635v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVertical%2520Semi-Federated%2520Learning%2520for%2520Efficient%2520Online%2520Advertising%26entry.906535625%3DWenjie%2520Li%2520and%2520Shu-Tao%2520Xia%2520and%2520Jiangke%2520Fan%2520and%2520Teng%2520Zhang%2520and%2520Xingxing%2520Wang%26entry.1292438233%3DTraditional%2520vertical%2520federated%2520learning%2520schema%2520suffers%2520from%2520two%2520main%2520issues%253A%25201%2529%2520restricted%2520applicable%2520scope%2520to%2520overlapped%2520samples%2520and%25202%2529%2520high%2520system%2520challenge%2520of%2520real-time%2520federated%2520serving%252C%2520which%2520limits%2520its%2520application%2520to%2520advertising%2520systems.%2520To%2520this%2520end%252C%2520we%2520advocate%2520a%2520new%2520practical%2520learning%2520setting%252C%2520Semi-VFL%2520%2528Vertical%2520Semi-Federated%2520Learning%2529%252C%2520for%2520real-world%2520industrial%2520applications%252C%2520where%2520the%2520learned%2520model%2520retains%2520sufficient%2520advantages%2520of%2520federated%2520learning%2520while%2520supporting%2520independent%2520local%2520serving.%2520To%2520achieve%2520this%2520goal%252C%2520we%2520propose%2520the%2520carefully%2520designed%2520Joint%2520Privileged%2520Learning%2520framework%2520%2528JPL%2529%2520to%2520i%2529%2520alleviate%2520the%2520absence%2520of%2520the%2520passive%2520party%2527s%2520feature%2520with%2520federated%2520equivalence%2520imitation%2520and%2520ii%2529%2520adapt%2520to%2520the%2520heterogeneous%2520full%2520sample%2520space%2520with%2520cross-branch%2520rank%2520alignment.%2520Extensive%2520experiments%2520conducted%2520on%2520real-world%2520advertising%2520datasets%2520validate%2520the%2520effectiveness%2520of%2520our%2520method%2520over%2520baseline%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.15635v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vertical%20Semi-Federated%20Learning%20for%20Efficient%20Online%20Advertising&entry.906535625=Wenjie%20Li%20and%20Shu-Tao%20Xia%20and%20Jiangke%20Fan%20and%20Teng%20Zhang%20and%20Xingxing%20Wang&entry.1292438233=Traditional%20vertical%20federated%20learning%20schema%20suffers%20from%20two%20main%20issues%3A%201%29%20restricted%20applicable%20scope%20to%20overlapped%20samples%20and%202%29%20high%20system%20challenge%20of%20real-time%20federated%20serving%2C%20which%20limits%20its%20application%20to%20advertising%20systems.%20To%20this%20end%2C%20we%20advocate%20a%20new%20practical%20learning%20setting%2C%20Semi-VFL%20%28Vertical%20Semi-Federated%20Learning%29%2C%20for%20real-world%20industrial%20applications%2C%20where%20the%20learned%20model%20retains%20sufficient%20advantages%20of%20federated%20learning%20while%20supporting%20independent%20local%20serving.%20To%20achieve%20this%20goal%2C%20we%20propose%20the%20carefully%20designed%20Joint%20Privileged%20Learning%20framework%20%28JPL%29%20to%20i%29%20alleviate%20the%20absence%20of%20the%20passive%20party%27s%20feature%20with%20federated%20equivalence%20imitation%20and%20ii%29%20adapt%20to%20the%20heterogeneous%20full%20sample%20space%20with%20cross-branch%20rank%20alignment.%20Extensive%20experiments%20conducted%20on%20real-world%20advertising%20datasets%20validate%20the%20effectiveness%20of%20our%20method%20over%20baseline%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2209.15635v4&entry.124074799=Read"},
{"title": "Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in Dynamic Environments", "author": "Zheng Jia and Shengbin Yue and Wei Chen and Siyuan Wang and Yidong Liu and Zejun Li and Yun Song and Zhongyu Wei", "abstract": "The gap between static benchmarks and the dynamic nature of real-world legal practice poses a key barrier to advancing legal intelligence. To this end, we introduce J1-ENVS, the first interactive and dynamic legal environment tailored for LLM-based agents. Guided by legal experts, it comprises six representative scenarios from Chinese legal practices across three levels of environmental complexity. We further introduce J1-EVAL, a fine-grained evaluation framework, designed to assess both task performance and procedural compliance across varying levels of legal proficiency. Extensive experiments on 17 LLM agents reveal that, while many models demonstrate solid legal knowledge, they struggle with procedural execution in dynamic settings. Even the SOTA model, GPT-4o, falls short of 60% overall performance. These findings highlight persistent challenges in achieving dynamic legal intelligence and offer valuable insights to guide future research.", "link": "http://arxiv.org/abs/2507.04037v4", "date": "2026-01-23", "relevancy": 2.4235, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4922}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4809}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ready%20Jurist%20One%3A%20Benchmarking%20Language%20Agents%20for%20Legal%20Intelligence%20in%20Dynamic%20Environments&body=Title%3A%20Ready%20Jurist%20One%3A%20Benchmarking%20Language%20Agents%20for%20Legal%20Intelligence%20in%20Dynamic%20Environments%0AAuthor%3A%20Zheng%20Jia%20and%20Shengbin%20Yue%20and%20Wei%20Chen%20and%20Siyuan%20Wang%20and%20Yidong%20Liu%20and%20Zejun%20Li%20and%20Yun%20Song%20and%20Zhongyu%20Wei%0AAbstract%3A%20The%20gap%20between%20static%20benchmarks%20and%20the%20dynamic%20nature%20of%20real-world%20legal%20practice%20poses%20a%20key%20barrier%20to%20advancing%20legal%20intelligence.%20To%20this%20end%2C%20we%20introduce%20J1-ENVS%2C%20the%20first%20interactive%20and%20dynamic%20legal%20environment%20tailored%20for%20LLM-based%20agents.%20Guided%20by%20legal%20experts%2C%20it%20comprises%20six%20representative%20scenarios%20from%20Chinese%20legal%20practices%20across%20three%20levels%20of%20environmental%20complexity.%20We%20further%20introduce%20J1-EVAL%2C%20a%20fine-grained%20evaluation%20framework%2C%20designed%20to%20assess%20both%20task%20performance%20and%20procedural%20compliance%20across%20varying%20levels%20of%20legal%20proficiency.%20Extensive%20experiments%20on%2017%20LLM%20agents%20reveal%20that%2C%20while%20many%20models%20demonstrate%20solid%20legal%20knowledge%2C%20they%20struggle%20with%20procedural%20execution%20in%20dynamic%20settings.%20Even%20the%20SOTA%20model%2C%20GPT-4o%2C%20falls%20short%20of%2060%25%20overall%20performance.%20These%20findings%20highlight%20persistent%20challenges%20in%20achieving%20dynamic%20legal%20intelligence%20and%20offer%20valuable%20insights%20to%20guide%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2507.04037v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReady%2520Jurist%2520One%253A%2520Benchmarking%2520Language%2520Agents%2520for%2520Legal%2520Intelligence%2520in%2520Dynamic%2520Environments%26entry.906535625%3DZheng%2520Jia%2520and%2520Shengbin%2520Yue%2520and%2520Wei%2520Chen%2520and%2520Siyuan%2520Wang%2520and%2520Yidong%2520Liu%2520and%2520Zejun%2520Li%2520and%2520Yun%2520Song%2520and%2520Zhongyu%2520Wei%26entry.1292438233%3DThe%2520gap%2520between%2520static%2520benchmarks%2520and%2520the%2520dynamic%2520nature%2520of%2520real-world%2520legal%2520practice%2520poses%2520a%2520key%2520barrier%2520to%2520advancing%2520legal%2520intelligence.%2520To%2520this%2520end%252C%2520we%2520introduce%2520J1-ENVS%252C%2520the%2520first%2520interactive%2520and%2520dynamic%2520legal%2520environment%2520tailored%2520for%2520LLM-based%2520agents.%2520Guided%2520by%2520legal%2520experts%252C%2520it%2520comprises%2520six%2520representative%2520scenarios%2520from%2520Chinese%2520legal%2520practices%2520across%2520three%2520levels%2520of%2520environmental%2520complexity.%2520We%2520further%2520introduce%2520J1-EVAL%252C%2520a%2520fine-grained%2520evaluation%2520framework%252C%2520designed%2520to%2520assess%2520both%2520task%2520performance%2520and%2520procedural%2520compliance%2520across%2520varying%2520levels%2520of%2520legal%2520proficiency.%2520Extensive%2520experiments%2520on%252017%2520LLM%2520agents%2520reveal%2520that%252C%2520while%2520many%2520models%2520demonstrate%2520solid%2520legal%2520knowledge%252C%2520they%2520struggle%2520with%2520procedural%2520execution%2520in%2520dynamic%2520settings.%2520Even%2520the%2520SOTA%2520model%252C%2520GPT-4o%252C%2520falls%2520short%2520of%252060%2525%2520overall%2520performance.%2520These%2520findings%2520highlight%2520persistent%2520challenges%2520in%2520achieving%2520dynamic%2520legal%2520intelligence%2520and%2520offer%2520valuable%2520insights%2520to%2520guide%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04037v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ready%20Jurist%20One%3A%20Benchmarking%20Language%20Agents%20for%20Legal%20Intelligence%20in%20Dynamic%20Environments&entry.906535625=Zheng%20Jia%20and%20Shengbin%20Yue%20and%20Wei%20Chen%20and%20Siyuan%20Wang%20and%20Yidong%20Liu%20and%20Zejun%20Li%20and%20Yun%20Song%20and%20Zhongyu%20Wei&entry.1292438233=The%20gap%20between%20static%20benchmarks%20and%20the%20dynamic%20nature%20of%20real-world%20legal%20practice%20poses%20a%20key%20barrier%20to%20advancing%20legal%20intelligence.%20To%20this%20end%2C%20we%20introduce%20J1-ENVS%2C%20the%20first%20interactive%20and%20dynamic%20legal%20environment%20tailored%20for%20LLM-based%20agents.%20Guided%20by%20legal%20experts%2C%20it%20comprises%20six%20representative%20scenarios%20from%20Chinese%20legal%20practices%20across%20three%20levels%20of%20environmental%20complexity.%20We%20further%20introduce%20J1-EVAL%2C%20a%20fine-grained%20evaluation%20framework%2C%20designed%20to%20assess%20both%20task%20performance%20and%20procedural%20compliance%20across%20varying%20levels%20of%20legal%20proficiency.%20Extensive%20experiments%20on%2017%20LLM%20agents%20reveal%20that%2C%20while%20many%20models%20demonstrate%20solid%20legal%20knowledge%2C%20they%20struggle%20with%20procedural%20execution%20in%20dynamic%20settings.%20Even%20the%20SOTA%20model%2C%20GPT-4o%2C%20falls%20short%20of%2060%25%20overall%20performance.%20These%20findings%20highlight%20persistent%20challenges%20in%20achieving%20dynamic%20legal%20intelligence%20and%20offer%20valuable%20insights%20to%20guide%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2507.04037v4&entry.124074799=Read"},
{"title": "VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents", "author": "Zirui Wang and Junyi Zhang and Jiaxin Ge and Long Lian and Letian Fu and Lisa Dunlap and Ken Goldberg and XuDong Wang and Ion Stoica and David M. Chan and Sewon Min and Joseph E. Gonzalez", "abstract": "Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/.", "link": "http://arxiv.org/abs/2601.16973v1", "date": "2026-01-23", "relevancy": 2.4222, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6074}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6074}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisGym%3A%20Diverse%2C%20Customizable%2C%20Scalable%20Environments%20for%20Multimodal%20Agents&body=Title%3A%20VisGym%3A%20Diverse%2C%20Customizable%2C%20Scalable%20Environments%20for%20Multimodal%20Agents%0AAuthor%3A%20Zirui%20Wang%20and%20Junyi%20Zhang%20and%20Jiaxin%20Ge%20and%20Long%20Lian%20and%20Letian%20Fu%20and%20Lisa%20Dunlap%20and%20Ken%20Goldberg%20and%20XuDong%20Wang%20and%20Ion%20Stoica%20and%20David%20M.%20Chan%20and%20Sewon%20Min%20and%20Joseph%20E.%20Gonzalez%0AAbstract%3A%20Modern%20Vision-Language%20Models%20%28VLMs%29%20remain%20poorly%20characterized%20in%20multi-step%20visual%20interactions%2C%20particularly%20in%20how%20they%20integrate%20perception%2C%20memory%2C%20and%20action%20over%20long%20horizons.%20We%20introduce%20VisGym%2C%20a%20gymnasium%20of%2017%20environments%20for%20evaluating%20and%20training%20VLMs.%20The%20suite%20spans%20symbolic%20puzzles%2C%20real-image%20understanding%2C%20navigation%2C%20and%20manipulation%2C%20and%20provides%20flexible%20controls%20over%20difficulty%2C%20input%20representation%2C%20planning%20horizon%2C%20and%20feedback.%20We%20also%20provide%20multi-step%20solvers%20that%20generate%20structured%20demonstrations%2C%20enabling%20supervised%20finetuning.%20Our%20evaluations%20show%20that%20all%20frontier%20models%20struggle%20in%20interactive%20settings%2C%20achieving%20low%20success%20rates%20in%20both%20the%20easy%20%2846.6%25%29%20and%20hard%20%2826.0%25%29%20configurations.%20Our%20experiments%20reveal%20notable%20limitations%3A%20models%20struggle%20to%20effectively%20leverage%20long%20context%2C%20performing%20worse%20with%20an%20unbounded%20history%20than%20with%20truncated%20windows.%20Furthermore%2C%20we%20find%20that%20several%20text-based%20symbolic%20tasks%20become%20substantially%20harder%20once%20rendered%20visually.%20However%2C%20explicit%20goal%20observations%2C%20textual%20feedback%2C%20and%20exploratory%20demonstrations%20in%20partially%20observable%20or%20unknown-dynamics%20settings%20for%20supervised%20finetuning%20yield%20consistent%20gains%2C%20highlighting%20concrete%20failure%20modes%20and%20pathways%20for%20improving%20multi-step%20visual%20decision-making.%20Code%2C%20data%2C%20and%20models%20can%20be%20found%20at%3A%20https%3A//visgym.github.io/.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisGym%253A%2520Diverse%252C%2520Customizable%252C%2520Scalable%2520Environments%2520for%2520Multimodal%2520Agents%26entry.906535625%3DZirui%2520Wang%2520and%2520Junyi%2520Zhang%2520and%2520Jiaxin%2520Ge%2520and%2520Long%2520Lian%2520and%2520Letian%2520Fu%2520and%2520Lisa%2520Dunlap%2520and%2520Ken%2520Goldberg%2520and%2520XuDong%2520Wang%2520and%2520Ion%2520Stoica%2520and%2520David%2520M.%2520Chan%2520and%2520Sewon%2520Min%2520and%2520Joseph%2520E.%2520Gonzalez%26entry.1292438233%3DModern%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520remain%2520poorly%2520characterized%2520in%2520multi-step%2520visual%2520interactions%252C%2520particularly%2520in%2520how%2520they%2520integrate%2520perception%252C%2520memory%252C%2520and%2520action%2520over%2520long%2520horizons.%2520We%2520introduce%2520VisGym%252C%2520a%2520gymnasium%2520of%252017%2520environments%2520for%2520evaluating%2520and%2520training%2520VLMs.%2520The%2520suite%2520spans%2520symbolic%2520puzzles%252C%2520real-image%2520understanding%252C%2520navigation%252C%2520and%2520manipulation%252C%2520and%2520provides%2520flexible%2520controls%2520over%2520difficulty%252C%2520input%2520representation%252C%2520planning%2520horizon%252C%2520and%2520feedback.%2520We%2520also%2520provide%2520multi-step%2520solvers%2520that%2520generate%2520structured%2520demonstrations%252C%2520enabling%2520supervised%2520finetuning.%2520Our%2520evaluations%2520show%2520that%2520all%2520frontier%2520models%2520struggle%2520in%2520interactive%2520settings%252C%2520achieving%2520low%2520success%2520rates%2520in%2520both%2520the%2520easy%2520%252846.6%2525%2529%2520and%2520hard%2520%252826.0%2525%2529%2520configurations.%2520Our%2520experiments%2520reveal%2520notable%2520limitations%253A%2520models%2520struggle%2520to%2520effectively%2520leverage%2520long%2520context%252C%2520performing%2520worse%2520with%2520an%2520unbounded%2520history%2520than%2520with%2520truncated%2520windows.%2520Furthermore%252C%2520we%2520find%2520that%2520several%2520text-based%2520symbolic%2520tasks%2520become%2520substantially%2520harder%2520once%2520rendered%2520visually.%2520However%252C%2520explicit%2520goal%2520observations%252C%2520textual%2520feedback%252C%2520and%2520exploratory%2520demonstrations%2520in%2520partially%2520observable%2520or%2520unknown-dynamics%2520settings%2520for%2520supervised%2520finetuning%2520yield%2520consistent%2520gains%252C%2520highlighting%2520concrete%2520failure%2520modes%2520and%2520pathways%2520for%2520improving%2520multi-step%2520visual%2520decision-making.%2520Code%252C%2520data%252C%2520and%2520models%2520can%2520be%2520found%2520at%253A%2520https%253A//visgym.github.io/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisGym%3A%20Diverse%2C%20Customizable%2C%20Scalable%20Environments%20for%20Multimodal%20Agents&entry.906535625=Zirui%20Wang%20and%20Junyi%20Zhang%20and%20Jiaxin%20Ge%20and%20Long%20Lian%20and%20Letian%20Fu%20and%20Lisa%20Dunlap%20and%20Ken%20Goldberg%20and%20XuDong%20Wang%20and%20Ion%20Stoica%20and%20David%20M.%20Chan%20and%20Sewon%20Min%20and%20Joseph%20E.%20Gonzalez&entry.1292438233=Modern%20Vision-Language%20Models%20%28VLMs%29%20remain%20poorly%20characterized%20in%20multi-step%20visual%20interactions%2C%20particularly%20in%20how%20they%20integrate%20perception%2C%20memory%2C%20and%20action%20over%20long%20horizons.%20We%20introduce%20VisGym%2C%20a%20gymnasium%20of%2017%20environments%20for%20evaluating%20and%20training%20VLMs.%20The%20suite%20spans%20symbolic%20puzzles%2C%20real-image%20understanding%2C%20navigation%2C%20and%20manipulation%2C%20and%20provides%20flexible%20controls%20over%20difficulty%2C%20input%20representation%2C%20planning%20horizon%2C%20and%20feedback.%20We%20also%20provide%20multi-step%20solvers%20that%20generate%20structured%20demonstrations%2C%20enabling%20supervised%20finetuning.%20Our%20evaluations%20show%20that%20all%20frontier%20models%20struggle%20in%20interactive%20settings%2C%20achieving%20low%20success%20rates%20in%20both%20the%20easy%20%2846.6%25%29%20and%20hard%20%2826.0%25%29%20configurations.%20Our%20experiments%20reveal%20notable%20limitations%3A%20models%20struggle%20to%20effectively%20leverage%20long%20context%2C%20performing%20worse%20with%20an%20unbounded%20history%20than%20with%20truncated%20windows.%20Furthermore%2C%20we%20find%20that%20several%20text-based%20symbolic%20tasks%20become%20substantially%20harder%20once%20rendered%20visually.%20However%2C%20explicit%20goal%20observations%2C%20textual%20feedback%2C%20and%20exploratory%20demonstrations%20in%20partially%20observable%20or%20unknown-dynamics%20settings%20for%20supervised%20finetuning%20yield%20consistent%20gains%2C%20highlighting%20concrete%20failure%20modes%20and%20pathways%20for%20improving%20multi-step%20visual%20decision-making.%20Code%2C%20data%2C%20and%20models%20can%20be%20found%20at%3A%20https%3A//visgym.github.io/.&entry.1838667208=http%3A//arxiv.org/abs/2601.16973v1&entry.124074799=Read"},
{"title": "Adaptive Reinforcement and Model Predictive Control Switching for Safe Human-Robot Cooperative Navigation", "author": "Ning Liu and Sen Shen and Zheng Li and Matthew D'Souza and Jen Jen Chung and Thomas Braunl", "abstract": "This paper addresses the challenge of human-guided navigation for mobile collaborative robots under simultaneous proximity regulation and safety constraints. We introduce Adaptive Reinforcement and Model Predictive Control Switching (ARMS), a hybrid learning-control framework that integrates a reinforcement learning follower trained with Proximal Policy Optimization (PPO) and an analytical one-step Model Predictive Control (MPC) formulated as a quadratic program safety filter. To enable robust perception under partial observability and non-stationary human motion, ARMS employs a decoupled sensing architecture with a Long Short-Term Memory (LSTM) temporal encoder for the human-robot relative state and a spatial encoder for 360-degree LiDAR scans. The core contribution is a learned adaptive neural switcher that performs context-aware soft action fusion between the two controllers, favoring conservative, constraint-aware QP-based control in low-risk regions while progressively shifting control authority to the learned follower in highly cluttered or constrained scenarios where maneuverability is critical, and reverting to the follower action when the QP becomes infeasible. Extensive evaluations against Pure Pursuit, Dynamic Window Approach (DWA), and an RL-only baseline demonstrate that ARMS achieves an 82.5 percent success rate in highly cluttered environments, outperforming DWA and RL-only approaches by 7.1 percent and 3.1 percent, respectively, while reducing average computational latency by 33 percent to 5.2 milliseconds compared to a multi-step MPC baseline. Additional simulation transfer in Gazebo and initial real-world deployment results further indicate the practicality and robustness of ARMS for safe and efficient human-robot collaboration. Source code and a demonstration video are available at https://github.com/21ning/ARMS.git.", "link": "http://arxiv.org/abs/2601.16686v1", "date": "2026-01-23", "relevancy": 2.4163, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6204}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6128}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Reinforcement%20and%20Model%20Predictive%20Control%20Switching%20for%20Safe%20Human-Robot%20Cooperative%20Navigation&body=Title%3A%20Adaptive%20Reinforcement%20and%20Model%20Predictive%20Control%20Switching%20for%20Safe%20Human-Robot%20Cooperative%20Navigation%0AAuthor%3A%20Ning%20Liu%20and%20Sen%20Shen%20and%20Zheng%20Li%20and%20Matthew%20D%27Souza%20and%20Jen%20Jen%20Chung%20and%20Thomas%20Braunl%0AAbstract%3A%20This%20paper%20addresses%20the%20challenge%20of%20human-guided%20navigation%20for%20mobile%20collaborative%20robots%20under%20simultaneous%20proximity%20regulation%20and%20safety%20constraints.%20We%20introduce%20Adaptive%20Reinforcement%20and%20Model%20Predictive%20Control%20Switching%20%28ARMS%29%2C%20a%20hybrid%20learning-control%20framework%20that%20integrates%20a%20reinforcement%20learning%20follower%20trained%20with%20Proximal%20Policy%20Optimization%20%28PPO%29%20and%20an%20analytical%20one-step%20Model%20Predictive%20Control%20%28MPC%29%20formulated%20as%20a%20quadratic%20program%20safety%20filter.%20To%20enable%20robust%20perception%20under%20partial%20observability%20and%20non-stationary%20human%20motion%2C%20ARMS%20employs%20a%20decoupled%20sensing%20architecture%20with%20a%20Long%20Short-Term%20Memory%20%28LSTM%29%20temporal%20encoder%20for%20the%20human-robot%20relative%20state%20and%20a%20spatial%20encoder%20for%20360-degree%20LiDAR%20scans.%20The%20core%20contribution%20is%20a%20learned%20adaptive%20neural%20switcher%20that%20performs%20context-aware%20soft%20action%20fusion%20between%20the%20two%20controllers%2C%20favoring%20conservative%2C%20constraint-aware%20QP-based%20control%20in%20low-risk%20regions%20while%20progressively%20shifting%20control%20authority%20to%20the%20learned%20follower%20in%20highly%20cluttered%20or%20constrained%20scenarios%20where%20maneuverability%20is%20critical%2C%20and%20reverting%20to%20the%20follower%20action%20when%20the%20QP%20becomes%20infeasible.%20Extensive%20evaluations%20against%20Pure%20Pursuit%2C%20Dynamic%20Window%20Approach%20%28DWA%29%2C%20and%20an%20RL-only%20baseline%20demonstrate%20that%20ARMS%20achieves%20an%2082.5%20percent%20success%20rate%20in%20highly%20cluttered%20environments%2C%20outperforming%20DWA%20and%20RL-only%20approaches%20by%207.1%20percent%20and%203.1%20percent%2C%20respectively%2C%20while%20reducing%20average%20computational%20latency%20by%2033%20percent%20to%205.2%20milliseconds%20compared%20to%20a%20multi-step%20MPC%20baseline.%20Additional%20simulation%20transfer%20in%20Gazebo%20and%20initial%20real-world%20deployment%20results%20further%20indicate%20the%20practicality%20and%20robustness%20of%20ARMS%20for%20safe%20and%20efficient%20human-robot%20collaboration.%20Source%20code%20and%20a%20demonstration%20video%20are%20available%20at%20https%3A//github.com/21ning/ARMS.git.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Reinforcement%2520and%2520Model%2520Predictive%2520Control%2520Switching%2520for%2520Safe%2520Human-Robot%2520Cooperative%2520Navigation%26entry.906535625%3DNing%2520Liu%2520and%2520Sen%2520Shen%2520and%2520Zheng%2520Li%2520and%2520Matthew%2520D%2527Souza%2520and%2520Jen%2520Jen%2520Chung%2520and%2520Thomas%2520Braunl%26entry.1292438233%3DThis%2520paper%2520addresses%2520the%2520challenge%2520of%2520human-guided%2520navigation%2520for%2520mobile%2520collaborative%2520robots%2520under%2520simultaneous%2520proximity%2520regulation%2520and%2520safety%2520constraints.%2520We%2520introduce%2520Adaptive%2520Reinforcement%2520and%2520Model%2520Predictive%2520Control%2520Switching%2520%2528ARMS%2529%252C%2520a%2520hybrid%2520learning-control%2520framework%2520that%2520integrates%2520a%2520reinforcement%2520learning%2520follower%2520trained%2520with%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529%2520and%2520an%2520analytical%2520one-step%2520Model%2520Predictive%2520Control%2520%2528MPC%2529%2520formulated%2520as%2520a%2520quadratic%2520program%2520safety%2520filter.%2520To%2520enable%2520robust%2520perception%2520under%2520partial%2520observability%2520and%2520non-stationary%2520human%2520motion%252C%2520ARMS%2520employs%2520a%2520decoupled%2520sensing%2520architecture%2520with%2520a%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%2520temporal%2520encoder%2520for%2520the%2520human-robot%2520relative%2520state%2520and%2520a%2520spatial%2520encoder%2520for%2520360-degree%2520LiDAR%2520scans.%2520The%2520core%2520contribution%2520is%2520a%2520learned%2520adaptive%2520neural%2520switcher%2520that%2520performs%2520context-aware%2520soft%2520action%2520fusion%2520between%2520the%2520two%2520controllers%252C%2520favoring%2520conservative%252C%2520constraint-aware%2520QP-based%2520control%2520in%2520low-risk%2520regions%2520while%2520progressively%2520shifting%2520control%2520authority%2520to%2520the%2520learned%2520follower%2520in%2520highly%2520cluttered%2520or%2520constrained%2520scenarios%2520where%2520maneuverability%2520is%2520critical%252C%2520and%2520reverting%2520to%2520the%2520follower%2520action%2520when%2520the%2520QP%2520becomes%2520infeasible.%2520Extensive%2520evaluations%2520against%2520Pure%2520Pursuit%252C%2520Dynamic%2520Window%2520Approach%2520%2528DWA%2529%252C%2520and%2520an%2520RL-only%2520baseline%2520demonstrate%2520that%2520ARMS%2520achieves%2520an%252082.5%2520percent%2520success%2520rate%2520in%2520highly%2520cluttered%2520environments%252C%2520outperforming%2520DWA%2520and%2520RL-only%2520approaches%2520by%25207.1%2520percent%2520and%25203.1%2520percent%252C%2520respectively%252C%2520while%2520reducing%2520average%2520computational%2520latency%2520by%252033%2520percent%2520to%25205.2%2520milliseconds%2520compared%2520to%2520a%2520multi-step%2520MPC%2520baseline.%2520Additional%2520simulation%2520transfer%2520in%2520Gazebo%2520and%2520initial%2520real-world%2520deployment%2520results%2520further%2520indicate%2520the%2520practicality%2520and%2520robustness%2520of%2520ARMS%2520for%2520safe%2520and%2520efficient%2520human-robot%2520collaboration.%2520Source%2520code%2520and%2520a%2520demonstration%2520video%2520are%2520available%2520at%2520https%253A//github.com/21ning/ARMS.git.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Reinforcement%20and%20Model%20Predictive%20Control%20Switching%20for%20Safe%20Human-Robot%20Cooperative%20Navigation&entry.906535625=Ning%20Liu%20and%20Sen%20Shen%20and%20Zheng%20Li%20and%20Matthew%20D%27Souza%20and%20Jen%20Jen%20Chung%20and%20Thomas%20Braunl&entry.1292438233=This%20paper%20addresses%20the%20challenge%20of%20human-guided%20navigation%20for%20mobile%20collaborative%20robots%20under%20simultaneous%20proximity%20regulation%20and%20safety%20constraints.%20We%20introduce%20Adaptive%20Reinforcement%20and%20Model%20Predictive%20Control%20Switching%20%28ARMS%29%2C%20a%20hybrid%20learning-control%20framework%20that%20integrates%20a%20reinforcement%20learning%20follower%20trained%20with%20Proximal%20Policy%20Optimization%20%28PPO%29%20and%20an%20analytical%20one-step%20Model%20Predictive%20Control%20%28MPC%29%20formulated%20as%20a%20quadratic%20program%20safety%20filter.%20To%20enable%20robust%20perception%20under%20partial%20observability%20and%20non-stationary%20human%20motion%2C%20ARMS%20employs%20a%20decoupled%20sensing%20architecture%20with%20a%20Long%20Short-Term%20Memory%20%28LSTM%29%20temporal%20encoder%20for%20the%20human-robot%20relative%20state%20and%20a%20spatial%20encoder%20for%20360-degree%20LiDAR%20scans.%20The%20core%20contribution%20is%20a%20learned%20adaptive%20neural%20switcher%20that%20performs%20context-aware%20soft%20action%20fusion%20between%20the%20two%20controllers%2C%20favoring%20conservative%2C%20constraint-aware%20QP-based%20control%20in%20low-risk%20regions%20while%20progressively%20shifting%20control%20authority%20to%20the%20learned%20follower%20in%20highly%20cluttered%20or%20constrained%20scenarios%20where%20maneuverability%20is%20critical%2C%20and%20reverting%20to%20the%20follower%20action%20when%20the%20QP%20becomes%20infeasible.%20Extensive%20evaluations%20against%20Pure%20Pursuit%2C%20Dynamic%20Window%20Approach%20%28DWA%29%2C%20and%20an%20RL-only%20baseline%20demonstrate%20that%20ARMS%20achieves%20an%2082.5%20percent%20success%20rate%20in%20highly%20cluttered%20environments%2C%20outperforming%20DWA%20and%20RL-only%20approaches%20by%207.1%20percent%20and%203.1%20percent%2C%20respectively%2C%20while%20reducing%20average%20computational%20latency%20by%2033%20percent%20to%205.2%20milliseconds%20compared%20to%20a%20multi-step%20MPC%20baseline.%20Additional%20simulation%20transfer%20in%20Gazebo%20and%20initial%20real-world%20deployment%20results%20further%20indicate%20the%20practicality%20and%20robustness%20of%20ARMS%20for%20safe%20and%20efficient%20human-robot%20collaboration.%20Source%20code%20and%20a%20demonstration%20video%20are%20available%20at%20https%3A//github.com/21ning/ARMS.git.&entry.1838667208=http%3A//arxiv.org/abs/2601.16686v1&entry.124074799=Read"},
{"title": "HumanDiffusion: A Vision-Based Diffusion Trajectory Planner with Human-Conditioned Goals for Search and Rescue UAV", "author": "Faryal Batool and Iana Zhura and Valerii Serpiva and Roohan Ahmed Khan and Ivan Valuev and Issatay Tokmurziyev and Dzmitry Tsetserukou", "abstract": "Reliable human--robot collaboration in emergency scenarios requires autonomous systems that can detect humans, infer navigation goals, and operate safely in dynamic environments. This paper presents HumanDiffusion, a lightweight image-conditioned diffusion planner that generates human-aware navigation trajectories directly from RGB imagery. The system combines YOLO-11 based human detection with diffusion-driven trajectory generation, enabling a quadrotor to approach a target person and deliver medical assistance without relying on prior maps or computationally intensive planning pipelines. Trajectories are predicted in pixel space, ensuring smooth motion and a consistent safety margin around humans. We evaluate HumanDiffusion in simulation and real-world indoor mock-disaster scenarios. On a 300-sample test set, the model achieves a mean squared error of 0.02 in pixel-space trajectory reconstruction. Real-world experiments demonstrate an overall mission success rate of 80% across accident-response and search-and-locate tasks with partial occlusions. These results indicate that human-conditioned diffusion planning offers a practical and robust solution for human-aware UAV navigation in time-critical assistance settings.", "link": "http://arxiv.org/abs/2601.14973v2", "date": "2026-01-23", "relevancy": 2.3561, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6105}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.58}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanDiffusion%3A%20A%20Vision-Based%20Diffusion%20Trajectory%20Planner%20with%20Human-Conditioned%20Goals%20for%20Search%20and%20Rescue%20UAV&body=Title%3A%20HumanDiffusion%3A%20A%20Vision-Based%20Diffusion%20Trajectory%20Planner%20with%20Human-Conditioned%20Goals%20for%20Search%20and%20Rescue%20UAV%0AAuthor%3A%20Faryal%20Batool%20and%20Iana%20Zhura%20and%20Valerii%20Serpiva%20and%20Roohan%20Ahmed%20Khan%20and%20Ivan%20Valuev%20and%20Issatay%20Tokmurziyev%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20Reliable%20human--robot%20collaboration%20in%20emergency%20scenarios%20requires%20autonomous%20systems%20that%20can%20detect%20humans%2C%20infer%20navigation%20goals%2C%20and%20operate%20safely%20in%20dynamic%20environments.%20This%20paper%20presents%20HumanDiffusion%2C%20a%20lightweight%20image-conditioned%20diffusion%20planner%20that%20generates%20human-aware%20navigation%20trajectories%20directly%20from%20RGB%20imagery.%20The%20system%20combines%20YOLO-11%20based%20human%20detection%20with%20diffusion-driven%20trajectory%20generation%2C%20enabling%20a%20quadrotor%20to%20approach%20a%20target%20person%20and%20deliver%20medical%20assistance%20without%20relying%20on%20prior%20maps%20or%20computationally%20intensive%20planning%20pipelines.%20Trajectories%20are%20predicted%20in%20pixel%20space%2C%20ensuring%20smooth%20motion%20and%20a%20consistent%20safety%20margin%20around%20humans.%20We%20evaluate%20HumanDiffusion%20in%20simulation%20and%20real-world%20indoor%20mock-disaster%20scenarios.%20On%20a%20300-sample%20test%20set%2C%20the%20model%20achieves%20a%20mean%20squared%20error%20of%200.02%20in%20pixel-space%20trajectory%20reconstruction.%20Real-world%20experiments%20demonstrate%20an%20overall%20mission%20success%20rate%20of%2080%25%20across%20accident-response%20and%20search-and-locate%20tasks%20with%20partial%20occlusions.%20These%20results%20indicate%20that%20human-conditioned%20diffusion%20planning%20offers%20a%20practical%20and%20robust%20solution%20for%20human-aware%20UAV%20navigation%20in%20time-critical%20assistance%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14973v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanDiffusion%253A%2520A%2520Vision-Based%2520Diffusion%2520Trajectory%2520Planner%2520with%2520Human-Conditioned%2520Goals%2520for%2520Search%2520and%2520Rescue%2520UAV%26entry.906535625%3DFaryal%2520Batool%2520and%2520Iana%2520Zhura%2520and%2520Valerii%2520Serpiva%2520and%2520Roohan%2520Ahmed%2520Khan%2520and%2520Ivan%2520Valuev%2520and%2520Issatay%2520Tokmurziyev%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3DReliable%2520human--robot%2520collaboration%2520in%2520emergency%2520scenarios%2520requires%2520autonomous%2520systems%2520that%2520can%2520detect%2520humans%252C%2520infer%2520navigation%2520goals%252C%2520and%2520operate%2520safely%2520in%2520dynamic%2520environments.%2520This%2520paper%2520presents%2520HumanDiffusion%252C%2520a%2520lightweight%2520image-conditioned%2520diffusion%2520planner%2520that%2520generates%2520human-aware%2520navigation%2520trajectories%2520directly%2520from%2520RGB%2520imagery.%2520The%2520system%2520combines%2520YOLO-11%2520based%2520human%2520detection%2520with%2520diffusion-driven%2520trajectory%2520generation%252C%2520enabling%2520a%2520quadrotor%2520to%2520approach%2520a%2520target%2520person%2520and%2520deliver%2520medical%2520assistance%2520without%2520relying%2520on%2520prior%2520maps%2520or%2520computationally%2520intensive%2520planning%2520pipelines.%2520Trajectories%2520are%2520predicted%2520in%2520pixel%2520space%252C%2520ensuring%2520smooth%2520motion%2520and%2520a%2520consistent%2520safety%2520margin%2520around%2520humans.%2520We%2520evaluate%2520HumanDiffusion%2520in%2520simulation%2520and%2520real-world%2520indoor%2520mock-disaster%2520scenarios.%2520On%2520a%2520300-sample%2520test%2520set%252C%2520the%2520model%2520achieves%2520a%2520mean%2520squared%2520error%2520of%25200.02%2520in%2520pixel-space%2520trajectory%2520reconstruction.%2520Real-world%2520experiments%2520demonstrate%2520an%2520overall%2520mission%2520success%2520rate%2520of%252080%2525%2520across%2520accident-response%2520and%2520search-and-locate%2520tasks%2520with%2520partial%2520occlusions.%2520These%2520results%2520indicate%2520that%2520human-conditioned%2520diffusion%2520planning%2520offers%2520a%2520practical%2520and%2520robust%2520solution%2520for%2520human-aware%2520UAV%2520navigation%2520in%2520time-critical%2520assistance%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14973v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanDiffusion%3A%20A%20Vision-Based%20Diffusion%20Trajectory%20Planner%20with%20Human-Conditioned%20Goals%20for%20Search%20and%20Rescue%20UAV&entry.906535625=Faryal%20Batool%20and%20Iana%20Zhura%20and%20Valerii%20Serpiva%20and%20Roohan%20Ahmed%20Khan%20and%20Ivan%20Valuev%20and%20Issatay%20Tokmurziyev%20and%20Dzmitry%20Tsetserukou&entry.1292438233=Reliable%20human--robot%20collaboration%20in%20emergency%20scenarios%20requires%20autonomous%20systems%20that%20can%20detect%20humans%2C%20infer%20navigation%20goals%2C%20and%20operate%20safely%20in%20dynamic%20environments.%20This%20paper%20presents%20HumanDiffusion%2C%20a%20lightweight%20image-conditioned%20diffusion%20planner%20that%20generates%20human-aware%20navigation%20trajectories%20directly%20from%20RGB%20imagery.%20The%20system%20combines%20YOLO-11%20based%20human%20detection%20with%20diffusion-driven%20trajectory%20generation%2C%20enabling%20a%20quadrotor%20to%20approach%20a%20target%20person%20and%20deliver%20medical%20assistance%20without%20relying%20on%20prior%20maps%20or%20computationally%20intensive%20planning%20pipelines.%20Trajectories%20are%20predicted%20in%20pixel%20space%2C%20ensuring%20smooth%20motion%20and%20a%20consistent%20safety%20margin%20around%20humans.%20We%20evaluate%20HumanDiffusion%20in%20simulation%20and%20real-world%20indoor%20mock-disaster%20scenarios.%20On%20a%20300-sample%20test%20set%2C%20the%20model%20achieves%20a%20mean%20squared%20error%20of%200.02%20in%20pixel-space%20trajectory%20reconstruction.%20Real-world%20experiments%20demonstrate%20an%20overall%20mission%20success%20rate%20of%2080%25%20across%20accident-response%20and%20search-and-locate%20tasks%20with%20partial%20occlusions.%20These%20results%20indicate%20that%20human-conditioned%20diffusion%20planning%20offers%20a%20practical%20and%20robust%20solution%20for%20human-aware%20UAV%20navigation%20in%20time-critical%20assistance%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2601.14973v2&entry.124074799=Read"},
{"title": "A Multi-Stage Hybrid Framework for Automated Interpretation of Multi-View Engineering Drawings Using Vision Language Model", "author": "Muhammad Tayyab Khan and Zane Yong and Lequn Chen and Wenhe Feng and Nicholas Yew Jin Tan and Seung Ki Moon", "abstract": "Engineering drawings are fundamental to manufacturing communication, serving as the primary medium for conveying design intent, tolerances, and production details. However, interpreting complex multi-view drawings with dense annotations remains challenging using manual methods, generic optical character recognition (OCR) systems, or traditional deep learning approaches, due to varied layouts, orientations, and mixed symbolic-textual content. To address these challenges, this paper proposes a three-stage hybrid framework for the automated interpretation of 2D multi-view engineering drawings using modern detection and vision language models (VLMs). In the first stage, YOLOv11-det performs layout segmentation to localize key regions such as views, title blocks, and notes. The second stage uses YOLOv11-obb for orientation-aware, fine-grained detection of annotations, including measures, GD&T symbols, and surface roughness indicators. The third stage employs two Donut-based, OCR-free VLMs for semantic content parsing: the Alphabetical VLM extracts textual and categorical information from title blocks and notes, while the Numerical VLM interprets quantitative data such as measures, GD&T frames, and surface roughness. Two specialized datasets were developed to ensure robustness and generalization: 1,000 drawings for layout detection and 1,406 for annotation-level training. The Alphabetical VLM achieved an overall F1 score of 0.672, while the Numerical VLM reached 0.963, demonstrating strong performance in textual and quantitative interpretation, respectively. The unified JSON output enables seamless integration with CAD and manufacturing databases, providing a scalable solution for intelligent engineering drawing analysis.", "link": "http://arxiv.org/abs/2510.21862v2", "date": "2026-01-23", "relevancy": 2.356, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5963}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5963}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Stage%20Hybrid%20Framework%20for%20Automated%20Interpretation%20of%20Multi-View%20Engineering%20Drawings%20Using%20Vision%20Language%20Model&body=Title%3A%20A%20Multi-Stage%20Hybrid%20Framework%20for%20Automated%20Interpretation%20of%20Multi-View%20Engineering%20Drawings%20Using%20Vision%20Language%20Model%0AAuthor%3A%20Muhammad%20Tayyab%20Khan%20and%20Zane%20Yong%20and%20Lequn%20Chen%20and%20Wenhe%20Feng%20and%20Nicholas%20Yew%20Jin%20Tan%20and%20Seung%20Ki%20Moon%0AAbstract%3A%20Engineering%20drawings%20are%20fundamental%20to%20manufacturing%20communication%2C%20serving%20as%20the%20primary%20medium%20for%20conveying%20design%20intent%2C%20tolerances%2C%20and%20production%20details.%20However%2C%20interpreting%20complex%20multi-view%20drawings%20with%20dense%20annotations%20remains%20challenging%20using%20manual%20methods%2C%20generic%20optical%20character%20recognition%20%28OCR%29%20systems%2C%20or%20traditional%20deep%20learning%20approaches%2C%20due%20to%20varied%20layouts%2C%20orientations%2C%20and%20mixed%20symbolic-textual%20content.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%20three-stage%20hybrid%20framework%20for%20the%20automated%20interpretation%20of%202D%20multi-view%20engineering%20drawings%20using%20modern%20detection%20and%20vision%20language%20models%20%28VLMs%29.%20In%20the%20first%20stage%2C%20YOLOv11-det%20performs%20layout%20segmentation%20to%20localize%20key%20regions%20such%20as%20views%2C%20title%20blocks%2C%20and%20notes.%20The%20second%20stage%20uses%20YOLOv11-obb%20for%20orientation-aware%2C%20fine-grained%20detection%20of%20annotations%2C%20including%20measures%2C%20GD%26T%20symbols%2C%20and%20surface%20roughness%20indicators.%20The%20third%20stage%20employs%20two%20Donut-based%2C%20OCR-free%20VLMs%20for%20semantic%20content%20parsing%3A%20the%20Alphabetical%20VLM%20extracts%20textual%20and%20categorical%20information%20from%20title%20blocks%20and%20notes%2C%20while%20the%20Numerical%20VLM%20interprets%20quantitative%20data%20such%20as%20measures%2C%20GD%26T%20frames%2C%20and%20surface%20roughness.%20Two%20specialized%20datasets%20were%20developed%20to%20ensure%20robustness%20and%20generalization%3A%201%2C000%20drawings%20for%20layout%20detection%20and%201%2C406%20for%20annotation-level%20training.%20The%20Alphabetical%20VLM%20achieved%20an%20overall%20F1%20score%20of%200.672%2C%20while%20the%20Numerical%20VLM%20reached%200.963%2C%20demonstrating%20strong%20performance%20in%20textual%20and%20quantitative%20interpretation%2C%20respectively.%20The%20unified%20JSON%20output%20enables%20seamless%20integration%20with%20CAD%20and%20manufacturing%20databases%2C%20providing%20a%20scalable%20solution%20for%20intelligent%20engineering%20drawing%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2510.21862v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-Stage%2520Hybrid%2520Framework%2520for%2520Automated%2520Interpretation%2520of%2520Multi-View%2520Engineering%2520Drawings%2520Using%2520Vision%2520Language%2520Model%26entry.906535625%3DMuhammad%2520Tayyab%2520Khan%2520and%2520Zane%2520Yong%2520and%2520Lequn%2520Chen%2520and%2520Wenhe%2520Feng%2520and%2520Nicholas%2520Yew%2520Jin%2520Tan%2520and%2520Seung%2520Ki%2520Moon%26entry.1292438233%3DEngineering%2520drawings%2520are%2520fundamental%2520to%2520manufacturing%2520communication%252C%2520serving%2520as%2520the%2520primary%2520medium%2520for%2520conveying%2520design%2520intent%252C%2520tolerances%252C%2520and%2520production%2520details.%2520However%252C%2520interpreting%2520complex%2520multi-view%2520drawings%2520with%2520dense%2520annotations%2520remains%2520challenging%2520using%2520manual%2520methods%252C%2520generic%2520optical%2520character%2520recognition%2520%2528OCR%2529%2520systems%252C%2520or%2520traditional%2520deep%2520learning%2520approaches%252C%2520due%2520to%2520varied%2520layouts%252C%2520orientations%252C%2520and%2520mixed%2520symbolic-textual%2520content.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520proposes%2520a%2520three-stage%2520hybrid%2520framework%2520for%2520the%2520automated%2520interpretation%2520of%25202D%2520multi-view%2520engineering%2520drawings%2520using%2520modern%2520detection%2520and%2520vision%2520language%2520models%2520%2528VLMs%2529.%2520In%2520the%2520first%2520stage%252C%2520YOLOv11-det%2520performs%2520layout%2520segmentation%2520to%2520localize%2520key%2520regions%2520such%2520as%2520views%252C%2520title%2520blocks%252C%2520and%2520notes.%2520The%2520second%2520stage%2520uses%2520YOLOv11-obb%2520for%2520orientation-aware%252C%2520fine-grained%2520detection%2520of%2520annotations%252C%2520including%2520measures%252C%2520GD%2526T%2520symbols%252C%2520and%2520surface%2520roughness%2520indicators.%2520The%2520third%2520stage%2520employs%2520two%2520Donut-based%252C%2520OCR-free%2520VLMs%2520for%2520semantic%2520content%2520parsing%253A%2520the%2520Alphabetical%2520VLM%2520extracts%2520textual%2520and%2520categorical%2520information%2520from%2520title%2520blocks%2520and%2520notes%252C%2520while%2520the%2520Numerical%2520VLM%2520interprets%2520quantitative%2520data%2520such%2520as%2520measures%252C%2520GD%2526T%2520frames%252C%2520and%2520surface%2520roughness.%2520Two%2520specialized%2520datasets%2520were%2520developed%2520to%2520ensure%2520robustness%2520and%2520generalization%253A%25201%252C000%2520drawings%2520for%2520layout%2520detection%2520and%25201%252C406%2520for%2520annotation-level%2520training.%2520The%2520Alphabetical%2520VLM%2520achieved%2520an%2520overall%2520F1%2520score%2520of%25200.672%252C%2520while%2520the%2520Numerical%2520VLM%2520reached%25200.963%252C%2520demonstrating%2520strong%2520performance%2520in%2520textual%2520and%2520quantitative%2520interpretation%252C%2520respectively.%2520The%2520unified%2520JSON%2520output%2520enables%2520seamless%2520integration%2520with%2520CAD%2520and%2520manufacturing%2520databases%252C%2520providing%2520a%2520scalable%2520solution%2520for%2520intelligent%2520engineering%2520drawing%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.21862v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Stage%20Hybrid%20Framework%20for%20Automated%20Interpretation%20of%20Multi-View%20Engineering%20Drawings%20Using%20Vision%20Language%20Model&entry.906535625=Muhammad%20Tayyab%20Khan%20and%20Zane%20Yong%20and%20Lequn%20Chen%20and%20Wenhe%20Feng%20and%20Nicholas%20Yew%20Jin%20Tan%20and%20Seung%20Ki%20Moon&entry.1292438233=Engineering%20drawings%20are%20fundamental%20to%20manufacturing%20communication%2C%20serving%20as%20the%20primary%20medium%20for%20conveying%20design%20intent%2C%20tolerances%2C%20and%20production%20details.%20However%2C%20interpreting%20complex%20multi-view%20drawings%20with%20dense%20annotations%20remains%20challenging%20using%20manual%20methods%2C%20generic%20optical%20character%20recognition%20%28OCR%29%20systems%2C%20or%20traditional%20deep%20learning%20approaches%2C%20due%20to%20varied%20layouts%2C%20orientations%2C%20and%20mixed%20symbolic-textual%20content.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%20three-stage%20hybrid%20framework%20for%20the%20automated%20interpretation%20of%202D%20multi-view%20engineering%20drawings%20using%20modern%20detection%20and%20vision%20language%20models%20%28VLMs%29.%20In%20the%20first%20stage%2C%20YOLOv11-det%20performs%20layout%20segmentation%20to%20localize%20key%20regions%20such%20as%20views%2C%20title%20blocks%2C%20and%20notes.%20The%20second%20stage%20uses%20YOLOv11-obb%20for%20orientation-aware%2C%20fine-grained%20detection%20of%20annotations%2C%20including%20measures%2C%20GD%26T%20symbols%2C%20and%20surface%20roughness%20indicators.%20The%20third%20stage%20employs%20two%20Donut-based%2C%20OCR-free%20VLMs%20for%20semantic%20content%20parsing%3A%20the%20Alphabetical%20VLM%20extracts%20textual%20and%20categorical%20information%20from%20title%20blocks%20and%20notes%2C%20while%20the%20Numerical%20VLM%20interprets%20quantitative%20data%20such%20as%20measures%2C%20GD%26T%20frames%2C%20and%20surface%20roughness.%20Two%20specialized%20datasets%20were%20developed%20to%20ensure%20robustness%20and%20generalization%3A%201%2C000%20drawings%20for%20layout%20detection%20and%201%2C406%20for%20annotation-level%20training.%20The%20Alphabetical%20VLM%20achieved%20an%20overall%20F1%20score%20of%200.672%2C%20while%20the%20Numerical%20VLM%20reached%200.963%2C%20demonstrating%20strong%20performance%20in%20textual%20and%20quantitative%20interpretation%2C%20respectively.%20The%20unified%20JSON%20output%20enables%20seamless%20integration%20with%20CAD%20and%20manufacturing%20databases%2C%20providing%20a%20scalable%20solution%20for%20intelligent%20engineering%20drawing%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2510.21862v2&entry.124074799=Read"},
{"title": "Logical Expressiveness of Graph Neural Networks with Hierarchical Node Individualization", "author": "Arie Soeteman and Balder ten Cate", "abstract": "We propose and study Hierarchical Ego Graph Neural Networks (HEGNNs), an expressive extension of graph neural networks (GNNs) with hierarchical node individualization, inspired by the Individualization-Refinement paradigm for isomorphism testing. HEGNNs generalize subgraph-GNNs and form a hierarchy of increasingly expressive models that, in the limit, distinguish graphs up to isomorphism. We show that, over graphs of bounded degree, the separating power of HEGNN node classifiers equals that of graded hybrid logic. This characterization enables us to relate the separating power of HEGNNs to that of higher-order GNNs, GNNs enriched with local homomorphism count features, and color refinement algorithms based on Individualization-Refinement. Our experimental results confirm the practical feasibility of HEGNNs and show benefits in comparison with traditional GNN architectures, both with and without local homomorphism count features.", "link": "http://arxiv.org/abs/2506.13911v2", "date": "2026-01-23", "relevancy": 2.3553, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4972}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4748}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Logical%20Expressiveness%20of%20Graph%20Neural%20Networks%20with%20Hierarchical%20Node%20Individualization&body=Title%3A%20Logical%20Expressiveness%20of%20Graph%20Neural%20Networks%20with%20Hierarchical%20Node%20Individualization%0AAuthor%3A%20Arie%20Soeteman%20and%20Balder%20ten%20Cate%0AAbstract%3A%20We%20propose%20and%20study%20Hierarchical%20Ego%20Graph%20Neural%20Networks%20%28HEGNNs%29%2C%20an%20expressive%20extension%20of%20graph%20neural%20networks%20%28GNNs%29%20with%20hierarchical%20node%20individualization%2C%20inspired%20by%20the%20Individualization-Refinement%20paradigm%20for%20isomorphism%20testing.%20HEGNNs%20generalize%20subgraph-GNNs%20and%20form%20a%20hierarchy%20of%20increasingly%20expressive%20models%20that%2C%20in%20the%20limit%2C%20distinguish%20graphs%20up%20to%20isomorphism.%20We%20show%20that%2C%20over%20graphs%20of%20bounded%20degree%2C%20the%20separating%20power%20of%20HEGNN%20node%20classifiers%20equals%20that%20of%20graded%20hybrid%20logic.%20This%20characterization%20enables%20us%20to%20relate%20the%20separating%20power%20of%20HEGNNs%20to%20that%20of%20higher-order%20GNNs%2C%20GNNs%20enriched%20with%20local%20homomorphism%20count%20features%2C%20and%20color%20refinement%20algorithms%20based%20on%20Individualization-Refinement.%20Our%20experimental%20results%20confirm%20the%20practical%20feasibility%20of%20HEGNNs%20and%20show%20benefits%20in%20comparison%20with%20traditional%20GNN%20architectures%2C%20both%20with%20and%20without%20local%20homomorphism%20count%20features.%0ALink%3A%20http%3A//arxiv.org/abs/2506.13911v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogical%2520Expressiveness%2520of%2520Graph%2520Neural%2520Networks%2520with%2520Hierarchical%2520Node%2520Individualization%26entry.906535625%3DArie%2520Soeteman%2520and%2520Balder%2520ten%2520Cate%26entry.1292438233%3DWe%2520propose%2520and%2520study%2520Hierarchical%2520Ego%2520Graph%2520Neural%2520Networks%2520%2528HEGNNs%2529%252C%2520an%2520expressive%2520extension%2520of%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520with%2520hierarchical%2520node%2520individualization%252C%2520inspired%2520by%2520the%2520Individualization-Refinement%2520paradigm%2520for%2520isomorphism%2520testing.%2520HEGNNs%2520generalize%2520subgraph-GNNs%2520and%2520form%2520a%2520hierarchy%2520of%2520increasingly%2520expressive%2520models%2520that%252C%2520in%2520the%2520limit%252C%2520distinguish%2520graphs%2520up%2520to%2520isomorphism.%2520We%2520show%2520that%252C%2520over%2520graphs%2520of%2520bounded%2520degree%252C%2520the%2520separating%2520power%2520of%2520HEGNN%2520node%2520classifiers%2520equals%2520that%2520of%2520graded%2520hybrid%2520logic.%2520This%2520characterization%2520enables%2520us%2520to%2520relate%2520the%2520separating%2520power%2520of%2520HEGNNs%2520to%2520that%2520of%2520higher-order%2520GNNs%252C%2520GNNs%2520enriched%2520with%2520local%2520homomorphism%2520count%2520features%252C%2520and%2520color%2520refinement%2520algorithms%2520based%2520on%2520Individualization-Refinement.%2520Our%2520experimental%2520results%2520confirm%2520the%2520practical%2520feasibility%2520of%2520HEGNNs%2520and%2520show%2520benefits%2520in%2520comparison%2520with%2520traditional%2520GNN%2520architectures%252C%2520both%2520with%2520and%2520without%2520local%2520homomorphism%2520count%2520features.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13911v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Logical%20Expressiveness%20of%20Graph%20Neural%20Networks%20with%20Hierarchical%20Node%20Individualization&entry.906535625=Arie%20Soeteman%20and%20Balder%20ten%20Cate&entry.1292438233=We%20propose%20and%20study%20Hierarchical%20Ego%20Graph%20Neural%20Networks%20%28HEGNNs%29%2C%20an%20expressive%20extension%20of%20graph%20neural%20networks%20%28GNNs%29%20with%20hierarchical%20node%20individualization%2C%20inspired%20by%20the%20Individualization-Refinement%20paradigm%20for%20isomorphism%20testing.%20HEGNNs%20generalize%20subgraph-GNNs%20and%20form%20a%20hierarchy%20of%20increasingly%20expressive%20models%20that%2C%20in%20the%20limit%2C%20distinguish%20graphs%20up%20to%20isomorphism.%20We%20show%20that%2C%20over%20graphs%20of%20bounded%20degree%2C%20the%20separating%20power%20of%20HEGNN%20node%20classifiers%20equals%20that%20of%20graded%20hybrid%20logic.%20This%20characterization%20enables%20us%20to%20relate%20the%20separating%20power%20of%20HEGNNs%20to%20that%20of%20higher-order%20GNNs%2C%20GNNs%20enriched%20with%20local%20homomorphism%20count%20features%2C%20and%20color%20refinement%20algorithms%20based%20on%20Individualization-Refinement.%20Our%20experimental%20results%20confirm%20the%20practical%20feasibility%20of%20HEGNNs%20and%20show%20benefits%20in%20comparison%20with%20traditional%20GNN%20architectures%2C%20both%20with%20and%20without%20local%20homomorphism%20count%20features.&entry.1838667208=http%3A//arxiv.org/abs/2506.13911v2&entry.124074799=Read"},
{"title": "GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection", "author": "Haozhen Yan and Yan Hong and Suning Lang and Jiahui Zhan and Yikun Ji and Yujie Gao and Huijia Zhu and Jun Lan and Jianfu Zhang", "abstract": "With generative models becoming increasingly sophisticated and diverse, detecting AI-generated images has become increasingly challenging. While existing AI-genereted Image detectors achieve promising performance on in-distribution generated images, their generalization to unseen generative models remains limited. This limitation is largely attributed to their reliance on generation-specific artifacts, such as stylistic priors and compression patterns. To address these limitations, we propose GAMMA, a novel training framework designed to reduce domain bias and enhance semantic alignment. GAMMA introduces diverse manipulation strategies, such as inpainting-based manipulation and semantics-preserving perturbations, to ensure consistency between manipulated and authentic content. We employ multi-task supervision with dual segmentation heads and a classification head, enabling pixel-level source attribution across diverse generative domains. In addition, a reverse cross-attention mechanism is introduced to allow the segmentation heads to guide and correct biased representations in the classification branch. Our method achieves state-of-the-art generalization performance on the GenImage benchmark, imporving accuracy by 5.8%, but also maintains strong robustness on newly released generative model such as GPT-4o.", "link": "http://arxiv.org/abs/2509.10250v2", "date": "2026-01-23", "relevancy": 2.3399, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5922}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5861}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAMMA%3A%20Generalizable%20Alignment%20via%20Multi-task%20and%20Manipulation-Augmented%20Training%20for%20AI-Generated%20Image%20Detection&body=Title%3A%20GAMMA%3A%20Generalizable%20Alignment%20via%20Multi-task%20and%20Manipulation-Augmented%20Training%20for%20AI-Generated%20Image%20Detection%0AAuthor%3A%20Haozhen%20Yan%20and%20Yan%20Hong%20and%20Suning%20Lang%20and%20Jiahui%20Zhan%20and%20Yikun%20Ji%20and%20Yujie%20Gao%20and%20Huijia%20Zhu%20and%20Jun%20Lan%20and%20Jianfu%20Zhang%0AAbstract%3A%20With%20generative%20models%20becoming%20increasingly%20sophisticated%20and%20diverse%2C%20detecting%20AI-generated%20images%20has%20become%20increasingly%20challenging.%20While%20existing%20AI-genereted%20Image%20detectors%20achieve%20promising%20performance%20on%20in-distribution%20generated%20images%2C%20their%20generalization%20to%20unseen%20generative%20models%20remains%20limited.%20This%20limitation%20is%20largely%20attributed%20to%20their%20reliance%20on%20generation-specific%20artifacts%2C%20such%20as%20stylistic%20priors%20and%20compression%20patterns.%20To%20address%20these%20limitations%2C%20we%20propose%20GAMMA%2C%20a%20novel%20training%20framework%20designed%20to%20reduce%20domain%20bias%20and%20enhance%20semantic%20alignment.%20GAMMA%20introduces%20diverse%20manipulation%20strategies%2C%20such%20as%20inpainting-based%20manipulation%20and%20semantics-preserving%20perturbations%2C%20to%20ensure%20consistency%20between%20manipulated%20and%20authentic%20content.%20We%20employ%20multi-task%20supervision%20with%20dual%20segmentation%20heads%20and%20a%20classification%20head%2C%20enabling%20pixel-level%20source%20attribution%20across%20diverse%20generative%20domains.%20In%20addition%2C%20a%20reverse%20cross-attention%20mechanism%20is%20introduced%20to%20allow%20the%20segmentation%20heads%20to%20guide%20and%20correct%20biased%20representations%20in%20the%20classification%20branch.%20Our%20method%20achieves%20state-of-the-art%20generalization%20performance%20on%20the%20GenImage%20benchmark%2C%20imporving%20accuracy%20by%205.8%25%2C%20but%20also%20maintains%20strong%20robustness%20on%20newly%20released%20generative%20model%20such%20as%20GPT-4o.%0ALink%3A%20http%3A//arxiv.org/abs/2509.10250v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAMMA%253A%2520Generalizable%2520Alignment%2520via%2520Multi-task%2520and%2520Manipulation-Augmented%2520Training%2520for%2520AI-Generated%2520Image%2520Detection%26entry.906535625%3DHaozhen%2520Yan%2520and%2520Yan%2520Hong%2520and%2520Suning%2520Lang%2520and%2520Jiahui%2520Zhan%2520and%2520Yikun%2520Ji%2520and%2520Yujie%2520Gao%2520and%2520Huijia%2520Zhu%2520and%2520Jun%2520Lan%2520and%2520Jianfu%2520Zhang%26entry.1292438233%3DWith%2520generative%2520models%2520becoming%2520increasingly%2520sophisticated%2520and%2520diverse%252C%2520detecting%2520AI-generated%2520images%2520has%2520become%2520increasingly%2520challenging.%2520While%2520existing%2520AI-genereted%2520Image%2520detectors%2520achieve%2520promising%2520performance%2520on%2520in-distribution%2520generated%2520images%252C%2520their%2520generalization%2520to%2520unseen%2520generative%2520models%2520remains%2520limited.%2520This%2520limitation%2520is%2520largely%2520attributed%2520to%2520their%2520reliance%2520on%2520generation-specific%2520artifacts%252C%2520such%2520as%2520stylistic%2520priors%2520and%2520compression%2520patterns.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520GAMMA%252C%2520a%2520novel%2520training%2520framework%2520designed%2520to%2520reduce%2520domain%2520bias%2520and%2520enhance%2520semantic%2520alignment.%2520GAMMA%2520introduces%2520diverse%2520manipulation%2520strategies%252C%2520such%2520as%2520inpainting-based%2520manipulation%2520and%2520semantics-preserving%2520perturbations%252C%2520to%2520ensure%2520consistency%2520between%2520manipulated%2520and%2520authentic%2520content.%2520We%2520employ%2520multi-task%2520supervision%2520with%2520dual%2520segmentation%2520heads%2520and%2520a%2520classification%2520head%252C%2520enabling%2520pixel-level%2520source%2520attribution%2520across%2520diverse%2520generative%2520domains.%2520In%2520addition%252C%2520a%2520reverse%2520cross-attention%2520mechanism%2520is%2520introduced%2520to%2520allow%2520the%2520segmentation%2520heads%2520to%2520guide%2520and%2520correct%2520biased%2520representations%2520in%2520the%2520classification%2520branch.%2520Our%2520method%2520achieves%2520state-of-the-art%2520generalization%2520performance%2520on%2520the%2520GenImage%2520benchmark%252C%2520imporving%2520accuracy%2520by%25205.8%2525%252C%2520but%2520also%2520maintains%2520strong%2520robustness%2520on%2520newly%2520released%2520generative%2520model%2520such%2520as%2520GPT-4o.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.10250v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAMMA%3A%20Generalizable%20Alignment%20via%20Multi-task%20and%20Manipulation-Augmented%20Training%20for%20AI-Generated%20Image%20Detection&entry.906535625=Haozhen%20Yan%20and%20Yan%20Hong%20and%20Suning%20Lang%20and%20Jiahui%20Zhan%20and%20Yikun%20Ji%20and%20Yujie%20Gao%20and%20Huijia%20Zhu%20and%20Jun%20Lan%20and%20Jianfu%20Zhang&entry.1292438233=With%20generative%20models%20becoming%20increasingly%20sophisticated%20and%20diverse%2C%20detecting%20AI-generated%20images%20has%20become%20increasingly%20challenging.%20While%20existing%20AI-genereted%20Image%20detectors%20achieve%20promising%20performance%20on%20in-distribution%20generated%20images%2C%20their%20generalization%20to%20unseen%20generative%20models%20remains%20limited.%20This%20limitation%20is%20largely%20attributed%20to%20their%20reliance%20on%20generation-specific%20artifacts%2C%20such%20as%20stylistic%20priors%20and%20compression%20patterns.%20To%20address%20these%20limitations%2C%20we%20propose%20GAMMA%2C%20a%20novel%20training%20framework%20designed%20to%20reduce%20domain%20bias%20and%20enhance%20semantic%20alignment.%20GAMMA%20introduces%20diverse%20manipulation%20strategies%2C%20such%20as%20inpainting-based%20manipulation%20and%20semantics-preserving%20perturbations%2C%20to%20ensure%20consistency%20between%20manipulated%20and%20authentic%20content.%20We%20employ%20multi-task%20supervision%20with%20dual%20segmentation%20heads%20and%20a%20classification%20head%2C%20enabling%20pixel-level%20source%20attribution%20across%20diverse%20generative%20domains.%20In%20addition%2C%20a%20reverse%20cross-attention%20mechanism%20is%20introduced%20to%20allow%20the%20segmentation%20heads%20to%20guide%20and%20correct%20biased%20representations%20in%20the%20classification%20branch.%20Our%20method%20achieves%20state-of-the-art%20generalization%20performance%20on%20the%20GenImage%20benchmark%2C%20imporving%20accuracy%20by%205.8%25%2C%20but%20also%20maintains%20strong%20robustness%20on%20newly%20released%20generative%20model%20such%20as%20GPT-4o.&entry.1838667208=http%3A//arxiv.org/abs/2509.10250v2&entry.124074799=Read"},
{"title": "Adoption of Generative Artificial Intelligence in the German Software Engineering Industry: An Empirical Study", "author": "Ludwig Felder and Tobias Eisenreich and Mahsa Fischer and Stefan Wagner and Chunyang Chen", "abstract": "Generative artificial intelligence (GenAI) tools have seen rapid adoption among software developers. While adoption rates in the industry are rising, the underlying factors influencing the effective use of these tools, including the depth of interaction, organizational constraints, and experience-related considerations, have not been thoroughly investigated. This issue is particularly relevant in environments with stringent regulatory requirements, such as Germany, where practitioners must address the GDPR and the EU AI Act while balancing productivity gains with intellectual property considerations. Despite the significant impact of GenAI on software engineering, to the best of our knowledge, no empirical study has systematically examined the adoption dynamics of GenAI tools within the German context. To address this gap, we present a comprehensive mixed-methods study on GenAI adoption among German software engineers. Specifically, we conducted 18 exploratory interviews with practitioners, followed by a developer survey with 109 participants. We analyze patterns of tool adoption, prompting strategies, and organizational factors that influence effectiveness. Our results indicate that experience level moderates the perceived benefits of GenAI tools, and productivity gains are not evenly distributed among developers. Further, organizational size affects both tool selection and the intensity of tool use. Limited awareness of the project context is identified as the most significant barrier. We summarize a set of actionable implications for developers, organizations, and tool vendors seeking to advance artificial intelligence (AI) assisted software development.", "link": "http://arxiv.org/abs/2601.16700v1", "date": "2026-01-23", "relevancy": 2.3269, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4882}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.458}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adoption%20of%20Generative%20Artificial%20Intelligence%20in%20the%20German%20Software%20Engineering%20Industry%3A%20An%20Empirical%20Study&body=Title%3A%20Adoption%20of%20Generative%20Artificial%20Intelligence%20in%20the%20German%20Software%20Engineering%20Industry%3A%20An%20Empirical%20Study%0AAuthor%3A%20Ludwig%20Felder%20and%20Tobias%20Eisenreich%20and%20Mahsa%20Fischer%20and%20Stefan%20Wagner%20and%20Chunyang%20Chen%0AAbstract%3A%20Generative%20artificial%20intelligence%20%28GenAI%29%20tools%20have%20seen%20rapid%20adoption%20among%20software%20developers.%20While%20adoption%20rates%20in%20the%20industry%20are%20rising%2C%20the%20underlying%20factors%20influencing%20the%20effective%20use%20of%20these%20tools%2C%20including%20the%20depth%20of%20interaction%2C%20organizational%20constraints%2C%20and%20experience-related%20considerations%2C%20have%20not%20been%20thoroughly%20investigated.%20This%20issue%20is%20particularly%20relevant%20in%20environments%20with%20stringent%20regulatory%20requirements%2C%20such%20as%20Germany%2C%20where%20practitioners%20must%20address%20the%20GDPR%20and%20the%20EU%20AI%20Act%20while%20balancing%20productivity%20gains%20with%20intellectual%20property%20considerations.%20Despite%20the%20significant%20impact%20of%20GenAI%20on%20software%20engineering%2C%20to%20the%20best%20of%20our%20knowledge%2C%20no%20empirical%20study%20has%20systematically%20examined%20the%20adoption%20dynamics%20of%20GenAI%20tools%20within%20the%20German%20context.%20To%20address%20this%20gap%2C%20we%20present%20a%20comprehensive%20mixed-methods%20study%20on%20GenAI%20adoption%20among%20German%20software%20engineers.%20Specifically%2C%20we%20conducted%2018%20exploratory%20interviews%20with%20practitioners%2C%20followed%20by%20a%20developer%20survey%20with%20109%20participants.%20We%20analyze%20patterns%20of%20tool%20adoption%2C%20prompting%20strategies%2C%20and%20organizational%20factors%20that%20influence%20effectiveness.%20Our%20results%20indicate%20that%20experience%20level%20moderates%20the%20perceived%20benefits%20of%20GenAI%20tools%2C%20and%20productivity%20gains%20are%20not%20evenly%20distributed%20among%20developers.%20Further%2C%20organizational%20size%20affects%20both%20tool%20selection%20and%20the%20intensity%20of%20tool%20use.%20Limited%20awareness%20of%20the%20project%20context%20is%20identified%20as%20the%20most%20significant%20barrier.%20We%20summarize%20a%20set%20of%20actionable%20implications%20for%20developers%2C%20organizations%2C%20and%20tool%20vendors%20seeking%20to%20advance%20artificial%20intelligence%20%28AI%29%20assisted%20software%20development.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdoption%2520of%2520Generative%2520Artificial%2520Intelligence%2520in%2520the%2520German%2520Software%2520Engineering%2520Industry%253A%2520An%2520Empirical%2520Study%26entry.906535625%3DLudwig%2520Felder%2520and%2520Tobias%2520Eisenreich%2520and%2520Mahsa%2520Fischer%2520and%2520Stefan%2520Wagner%2520and%2520Chunyang%2520Chen%26entry.1292438233%3DGenerative%2520artificial%2520intelligence%2520%2528GenAI%2529%2520tools%2520have%2520seen%2520rapid%2520adoption%2520among%2520software%2520developers.%2520While%2520adoption%2520rates%2520in%2520the%2520industry%2520are%2520rising%252C%2520the%2520underlying%2520factors%2520influencing%2520the%2520effective%2520use%2520of%2520these%2520tools%252C%2520including%2520the%2520depth%2520of%2520interaction%252C%2520organizational%2520constraints%252C%2520and%2520experience-related%2520considerations%252C%2520have%2520not%2520been%2520thoroughly%2520investigated.%2520This%2520issue%2520is%2520particularly%2520relevant%2520in%2520environments%2520with%2520stringent%2520regulatory%2520requirements%252C%2520such%2520as%2520Germany%252C%2520where%2520practitioners%2520must%2520address%2520the%2520GDPR%2520and%2520the%2520EU%2520AI%2520Act%2520while%2520balancing%2520productivity%2520gains%2520with%2520intellectual%2520property%2520considerations.%2520Despite%2520the%2520significant%2520impact%2520of%2520GenAI%2520on%2520software%2520engineering%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%2520no%2520empirical%2520study%2520has%2520systematically%2520examined%2520the%2520adoption%2520dynamics%2520of%2520GenAI%2520tools%2520within%2520the%2520German%2520context.%2520To%2520address%2520this%2520gap%252C%2520we%2520present%2520a%2520comprehensive%2520mixed-methods%2520study%2520on%2520GenAI%2520adoption%2520among%2520German%2520software%2520engineers.%2520Specifically%252C%2520we%2520conducted%252018%2520exploratory%2520interviews%2520with%2520practitioners%252C%2520followed%2520by%2520a%2520developer%2520survey%2520with%2520109%2520participants.%2520We%2520analyze%2520patterns%2520of%2520tool%2520adoption%252C%2520prompting%2520strategies%252C%2520and%2520organizational%2520factors%2520that%2520influence%2520effectiveness.%2520Our%2520results%2520indicate%2520that%2520experience%2520level%2520moderates%2520the%2520perceived%2520benefits%2520of%2520GenAI%2520tools%252C%2520and%2520productivity%2520gains%2520are%2520not%2520evenly%2520distributed%2520among%2520developers.%2520Further%252C%2520organizational%2520size%2520affects%2520both%2520tool%2520selection%2520and%2520the%2520intensity%2520of%2520tool%2520use.%2520Limited%2520awareness%2520of%2520the%2520project%2520context%2520is%2520identified%2520as%2520the%2520most%2520significant%2520barrier.%2520We%2520summarize%2520a%2520set%2520of%2520actionable%2520implications%2520for%2520developers%252C%2520organizations%252C%2520and%2520tool%2520vendors%2520seeking%2520to%2520advance%2520artificial%2520intelligence%2520%2528AI%2529%2520assisted%2520software%2520development.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adoption%20of%20Generative%20Artificial%20Intelligence%20in%20the%20German%20Software%20Engineering%20Industry%3A%20An%20Empirical%20Study&entry.906535625=Ludwig%20Felder%20and%20Tobias%20Eisenreich%20and%20Mahsa%20Fischer%20and%20Stefan%20Wagner%20and%20Chunyang%20Chen&entry.1292438233=Generative%20artificial%20intelligence%20%28GenAI%29%20tools%20have%20seen%20rapid%20adoption%20among%20software%20developers.%20While%20adoption%20rates%20in%20the%20industry%20are%20rising%2C%20the%20underlying%20factors%20influencing%20the%20effective%20use%20of%20these%20tools%2C%20including%20the%20depth%20of%20interaction%2C%20organizational%20constraints%2C%20and%20experience-related%20considerations%2C%20have%20not%20been%20thoroughly%20investigated.%20This%20issue%20is%20particularly%20relevant%20in%20environments%20with%20stringent%20regulatory%20requirements%2C%20such%20as%20Germany%2C%20where%20practitioners%20must%20address%20the%20GDPR%20and%20the%20EU%20AI%20Act%20while%20balancing%20productivity%20gains%20with%20intellectual%20property%20considerations.%20Despite%20the%20significant%20impact%20of%20GenAI%20on%20software%20engineering%2C%20to%20the%20best%20of%20our%20knowledge%2C%20no%20empirical%20study%20has%20systematically%20examined%20the%20adoption%20dynamics%20of%20GenAI%20tools%20within%20the%20German%20context.%20To%20address%20this%20gap%2C%20we%20present%20a%20comprehensive%20mixed-methods%20study%20on%20GenAI%20adoption%20among%20German%20software%20engineers.%20Specifically%2C%20we%20conducted%2018%20exploratory%20interviews%20with%20practitioners%2C%20followed%20by%20a%20developer%20survey%20with%20109%20participants.%20We%20analyze%20patterns%20of%20tool%20adoption%2C%20prompting%20strategies%2C%20and%20organizational%20factors%20that%20influence%20effectiveness.%20Our%20results%20indicate%20that%20experience%20level%20moderates%20the%20perceived%20benefits%20of%20GenAI%20tools%2C%20and%20productivity%20gains%20are%20not%20evenly%20distributed%20among%20developers.%20Further%2C%20organizational%20size%20affects%20both%20tool%20selection%20and%20the%20intensity%20of%20tool%20use.%20Limited%20awareness%20of%20the%20project%20context%20is%20identified%20as%20the%20most%20significant%20barrier.%20We%20summarize%20a%20set%20of%20actionable%20implications%20for%20developers%2C%20organizations%2C%20and%20tool%20vendors%20seeking%20to%20advance%20artificial%20intelligence%20%28AI%29%20assisted%20software%20development.&entry.1838667208=http%3A//arxiv.org/abs/2601.16700v1&entry.124074799=Read"},
{"title": "Will It Survive? Deciphering the Fate of AI-Generated Code in Open Source", "author": "Musfiqur Rahman and Emad Shihab", "abstract": "The integration of AI agents as coding assistants into software development has raised questions about the long-term viability of AI agent-generated code. A prevailing hypothesis within the software engineering community suggests this code is \"disposable\", meaning it is merged quickly but discarded shortly thereafter. If true, organizations risk shifting maintenance burden from generation to post-deployment remediation. We investigate this hypothesis through survival analysis of 201 open-source projects, tracking over 200,000 code units authored by AI agents versus humans. Contrary to the disposable code narrative, agent-authored code survives significantly longer: at the line level, it exhibits a 15.8 percentage-point lower modification rate and 16% lower hazard of modification (HR = 0.842, p < 0.001). However, modification profiles differ. Agent-authored code shows modestly elevated corrective rates (26.3% vs. 23.0%), while human code shows higher adaptive rates. However, the effect sizes are small (Cram\u00e9r's V = 0.116), and per-agent variation exceeds the agent-human gap. Turning to prediction, textual features can identify modification-prone code (AUC-ROC = 0.671), but predicting when modifications occur remains challenging (Macro F1 = 0.285), suggesting timing depends on external organizational dynamics. The bottleneck for agent-generated code may not be generation quality, but the organizational practices that govern its long-term evolution.", "link": "http://arxiv.org/abs/2601.16809v1", "date": "2026-01-23", "relevancy": 2.3145, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4804}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4637}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Will%20It%20Survive%3F%20Deciphering%20the%20Fate%20of%20AI-Generated%20Code%20in%20Open%20Source&body=Title%3A%20Will%20It%20Survive%3F%20Deciphering%20the%20Fate%20of%20AI-Generated%20Code%20in%20Open%20Source%0AAuthor%3A%20Musfiqur%20Rahman%20and%20Emad%20Shihab%0AAbstract%3A%20The%20integration%20of%20AI%20agents%20as%20coding%20assistants%20into%20software%20development%20has%20raised%20questions%20about%20the%20long-term%20viability%20of%20AI%20agent-generated%20code.%20A%20prevailing%20hypothesis%20within%20the%20software%20engineering%20community%20suggests%20this%20code%20is%20%22disposable%22%2C%20meaning%20it%20is%20merged%20quickly%20but%20discarded%20shortly%20thereafter.%20If%20true%2C%20organizations%20risk%20shifting%20maintenance%20burden%20from%20generation%20to%20post-deployment%20remediation.%20We%20investigate%20this%20hypothesis%20through%20survival%20analysis%20of%20201%20open-source%20projects%2C%20tracking%20over%20200%2C000%20code%20units%20authored%20by%20AI%20agents%20versus%20humans.%20Contrary%20to%20the%20disposable%20code%20narrative%2C%20agent-authored%20code%20survives%20significantly%20longer%3A%20at%20the%20line%20level%2C%20it%20exhibits%20a%2015.8%20percentage-point%20lower%20modification%20rate%20and%2016%25%20lower%20hazard%20of%20modification%20%28HR%20%3D%200.842%2C%20p%20%3C%200.001%29.%20However%2C%20modification%20profiles%20differ.%20Agent-authored%20code%20shows%20modestly%20elevated%20corrective%20rates%20%2826.3%25%20vs.%2023.0%25%29%2C%20while%20human%20code%20shows%20higher%20adaptive%20rates.%20However%2C%20the%20effect%20sizes%20are%20small%20%28Cram%C3%A9r%27s%20V%20%3D%200.116%29%2C%20and%20per-agent%20variation%20exceeds%20the%20agent-human%20gap.%20Turning%20to%20prediction%2C%20textual%20features%20can%20identify%20modification-prone%20code%20%28AUC-ROC%20%3D%200.671%29%2C%20but%20predicting%20when%20modifications%20occur%20remains%20challenging%20%28Macro%20F1%20%3D%200.285%29%2C%20suggesting%20timing%20depends%20on%20external%20organizational%20dynamics.%20The%20bottleneck%20for%20agent-generated%20code%20may%20not%20be%20generation%20quality%2C%20but%20the%20organizational%20practices%20that%20govern%20its%20long-term%20evolution.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWill%2520It%2520Survive%253F%2520Deciphering%2520the%2520Fate%2520of%2520AI-Generated%2520Code%2520in%2520Open%2520Source%26entry.906535625%3DMusfiqur%2520Rahman%2520and%2520Emad%2520Shihab%26entry.1292438233%3DThe%2520integration%2520of%2520AI%2520agents%2520as%2520coding%2520assistants%2520into%2520software%2520development%2520has%2520raised%2520questions%2520about%2520the%2520long-term%2520viability%2520of%2520AI%2520agent-generated%2520code.%2520A%2520prevailing%2520hypothesis%2520within%2520the%2520software%2520engineering%2520community%2520suggests%2520this%2520code%2520is%2520%2522disposable%2522%252C%2520meaning%2520it%2520is%2520merged%2520quickly%2520but%2520discarded%2520shortly%2520thereafter.%2520If%2520true%252C%2520organizations%2520risk%2520shifting%2520maintenance%2520burden%2520from%2520generation%2520to%2520post-deployment%2520remediation.%2520We%2520investigate%2520this%2520hypothesis%2520through%2520survival%2520analysis%2520of%2520201%2520open-source%2520projects%252C%2520tracking%2520over%2520200%252C000%2520code%2520units%2520authored%2520by%2520AI%2520agents%2520versus%2520humans.%2520Contrary%2520to%2520the%2520disposable%2520code%2520narrative%252C%2520agent-authored%2520code%2520survives%2520significantly%2520longer%253A%2520at%2520the%2520line%2520level%252C%2520it%2520exhibits%2520a%252015.8%2520percentage-point%2520lower%2520modification%2520rate%2520and%252016%2525%2520lower%2520hazard%2520of%2520modification%2520%2528HR%2520%253D%25200.842%252C%2520p%2520%253C%25200.001%2529.%2520However%252C%2520modification%2520profiles%2520differ.%2520Agent-authored%2520code%2520shows%2520modestly%2520elevated%2520corrective%2520rates%2520%252826.3%2525%2520vs.%252023.0%2525%2529%252C%2520while%2520human%2520code%2520shows%2520higher%2520adaptive%2520rates.%2520However%252C%2520the%2520effect%2520sizes%2520are%2520small%2520%2528Cram%25C3%25A9r%2527s%2520V%2520%253D%25200.116%2529%252C%2520and%2520per-agent%2520variation%2520exceeds%2520the%2520agent-human%2520gap.%2520Turning%2520to%2520prediction%252C%2520textual%2520features%2520can%2520identify%2520modification-prone%2520code%2520%2528AUC-ROC%2520%253D%25200.671%2529%252C%2520but%2520predicting%2520when%2520modifications%2520occur%2520remains%2520challenging%2520%2528Macro%2520F1%2520%253D%25200.285%2529%252C%2520suggesting%2520timing%2520depends%2520on%2520external%2520organizational%2520dynamics.%2520The%2520bottleneck%2520for%2520agent-generated%2520code%2520may%2520not%2520be%2520generation%2520quality%252C%2520but%2520the%2520organizational%2520practices%2520that%2520govern%2520its%2520long-term%2520evolution.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Will%20It%20Survive%3F%20Deciphering%20the%20Fate%20of%20AI-Generated%20Code%20in%20Open%20Source&entry.906535625=Musfiqur%20Rahman%20and%20Emad%20Shihab&entry.1292438233=The%20integration%20of%20AI%20agents%20as%20coding%20assistants%20into%20software%20development%20has%20raised%20questions%20about%20the%20long-term%20viability%20of%20AI%20agent-generated%20code.%20A%20prevailing%20hypothesis%20within%20the%20software%20engineering%20community%20suggests%20this%20code%20is%20%22disposable%22%2C%20meaning%20it%20is%20merged%20quickly%20but%20discarded%20shortly%20thereafter.%20If%20true%2C%20organizations%20risk%20shifting%20maintenance%20burden%20from%20generation%20to%20post-deployment%20remediation.%20We%20investigate%20this%20hypothesis%20through%20survival%20analysis%20of%20201%20open-source%20projects%2C%20tracking%20over%20200%2C000%20code%20units%20authored%20by%20AI%20agents%20versus%20humans.%20Contrary%20to%20the%20disposable%20code%20narrative%2C%20agent-authored%20code%20survives%20significantly%20longer%3A%20at%20the%20line%20level%2C%20it%20exhibits%20a%2015.8%20percentage-point%20lower%20modification%20rate%20and%2016%25%20lower%20hazard%20of%20modification%20%28HR%20%3D%200.842%2C%20p%20%3C%200.001%29.%20However%2C%20modification%20profiles%20differ.%20Agent-authored%20code%20shows%20modestly%20elevated%20corrective%20rates%20%2826.3%25%20vs.%2023.0%25%29%2C%20while%20human%20code%20shows%20higher%20adaptive%20rates.%20However%2C%20the%20effect%20sizes%20are%20small%20%28Cram%C3%A9r%27s%20V%20%3D%200.116%29%2C%20and%20per-agent%20variation%20exceeds%20the%20agent-human%20gap.%20Turning%20to%20prediction%2C%20textual%20features%20can%20identify%20modification-prone%20code%20%28AUC-ROC%20%3D%200.671%29%2C%20but%20predicting%20when%20modifications%20occur%20remains%20challenging%20%28Macro%20F1%20%3D%200.285%29%2C%20suggesting%20timing%20depends%20on%20external%20organizational%20dynamics.%20The%20bottleneck%20for%20agent-generated%20code%20may%20not%20be%20generation%20quality%2C%20but%20the%20organizational%20practices%20that%20govern%20its%20long-term%20evolution.&entry.1838667208=http%3A//arxiv.org/abs/2601.16809v1&entry.124074799=Read"},
{"title": "Calibrated Similarity for Reliable Geometric Analysis of Embedding Spaces", "author": "Nicolas Tacheny", "abstract": "While raw cosine similarity in pretrained embedding spaces exhibits strong rank correlation with human judgments, anisotropy induces systematic miscalibration of absolute values: scores concentrate in a narrow high-similarity band regardless of actual semantic relatedness, limiting interpretability as a quantitative measure. Prior work addresses this by modifying the embedding space (whitening, contrastive fine tuning), but such transformations alter geometric structure and require recomputing all embeddings.\n  Using isotonic regression trained on human similarity judgments, we construct a monotonic transformation that achieves near-perfect calibration while preserving rank correlation and local stability(98% across seven perturbation types). Our contribution is not to replace cosine similarity, but to restore interpretability of its absolute values through monotone calibration, without altering its ranking properties.\n  We characterize isotonic calibration as an order-preserving reparameterization and prove that all order-based constructions (angular ordering, nearest neighbors, threshold graphs and quantile-based decisions) are invariant under this transformation.", "link": "http://arxiv.org/abs/2601.16907v1", "date": "2026-01-23", "relevancy": 2.2633, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4535}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4526}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calibrated%20Similarity%20for%20Reliable%20Geometric%20Analysis%20of%20Embedding%20Spaces&body=Title%3A%20Calibrated%20Similarity%20for%20Reliable%20Geometric%20Analysis%20of%20Embedding%20Spaces%0AAuthor%3A%20Nicolas%20Tacheny%0AAbstract%3A%20While%20raw%20cosine%20similarity%20in%20pretrained%20embedding%20spaces%20exhibits%20strong%20rank%20correlation%20with%20human%20judgments%2C%20anisotropy%20induces%20systematic%20miscalibration%20of%20absolute%20values%3A%20scores%20concentrate%20in%20a%20narrow%20high-similarity%20band%20regardless%20of%20actual%20semantic%20relatedness%2C%20limiting%20interpretability%20as%20a%20quantitative%20measure.%20Prior%20work%20addresses%20this%20by%20modifying%20the%20embedding%20space%20%28whitening%2C%20contrastive%20fine%20tuning%29%2C%20but%20such%20transformations%20alter%20geometric%20structure%20and%20require%20recomputing%20all%20embeddings.%0A%20%20Using%20isotonic%20regression%20trained%20on%20human%20similarity%20judgments%2C%20we%20construct%20a%20monotonic%20transformation%20that%20achieves%20near-perfect%20calibration%20while%20preserving%20rank%20correlation%20and%20local%20stability%2898%25%20across%20seven%20perturbation%20types%29.%20Our%20contribution%20is%20not%20to%20replace%20cosine%20similarity%2C%20but%20to%20restore%20interpretability%20of%20its%20absolute%20values%20through%20monotone%20calibration%2C%20without%20altering%20its%20ranking%20properties.%0A%20%20We%20characterize%20isotonic%20calibration%20as%20an%20order-preserving%20reparameterization%20and%20prove%20that%20all%20order-based%20constructions%20%28angular%20ordering%2C%20nearest%20neighbors%2C%20threshold%20graphs%20and%20quantile-based%20decisions%29%20are%20invariant%20under%20this%20transformation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalibrated%2520Similarity%2520for%2520Reliable%2520Geometric%2520Analysis%2520of%2520Embedding%2520Spaces%26entry.906535625%3DNicolas%2520Tacheny%26entry.1292438233%3DWhile%2520raw%2520cosine%2520similarity%2520in%2520pretrained%2520embedding%2520spaces%2520exhibits%2520strong%2520rank%2520correlation%2520with%2520human%2520judgments%252C%2520anisotropy%2520induces%2520systematic%2520miscalibration%2520of%2520absolute%2520values%253A%2520scores%2520concentrate%2520in%2520a%2520narrow%2520high-similarity%2520band%2520regardless%2520of%2520actual%2520semantic%2520relatedness%252C%2520limiting%2520interpretability%2520as%2520a%2520quantitative%2520measure.%2520Prior%2520work%2520addresses%2520this%2520by%2520modifying%2520the%2520embedding%2520space%2520%2528whitening%252C%2520contrastive%2520fine%2520tuning%2529%252C%2520but%2520such%2520transformations%2520alter%2520geometric%2520structure%2520and%2520require%2520recomputing%2520all%2520embeddings.%250A%2520%2520Using%2520isotonic%2520regression%2520trained%2520on%2520human%2520similarity%2520judgments%252C%2520we%2520construct%2520a%2520monotonic%2520transformation%2520that%2520achieves%2520near-perfect%2520calibration%2520while%2520preserving%2520rank%2520correlation%2520and%2520local%2520stability%252898%2525%2520across%2520seven%2520perturbation%2520types%2529.%2520Our%2520contribution%2520is%2520not%2520to%2520replace%2520cosine%2520similarity%252C%2520but%2520to%2520restore%2520interpretability%2520of%2520its%2520absolute%2520values%2520through%2520monotone%2520calibration%252C%2520without%2520altering%2520its%2520ranking%2520properties.%250A%2520%2520We%2520characterize%2520isotonic%2520calibration%2520as%2520an%2520order-preserving%2520reparameterization%2520and%2520prove%2520that%2520all%2520order-based%2520constructions%2520%2528angular%2520ordering%252C%2520nearest%2520neighbors%252C%2520threshold%2520graphs%2520and%2520quantile-based%2520decisions%2529%2520are%2520invariant%2520under%2520this%2520transformation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibrated%20Similarity%20for%20Reliable%20Geometric%20Analysis%20of%20Embedding%20Spaces&entry.906535625=Nicolas%20Tacheny&entry.1292438233=While%20raw%20cosine%20similarity%20in%20pretrained%20embedding%20spaces%20exhibits%20strong%20rank%20correlation%20with%20human%20judgments%2C%20anisotropy%20induces%20systematic%20miscalibration%20of%20absolute%20values%3A%20scores%20concentrate%20in%20a%20narrow%20high-similarity%20band%20regardless%20of%20actual%20semantic%20relatedness%2C%20limiting%20interpretability%20as%20a%20quantitative%20measure.%20Prior%20work%20addresses%20this%20by%20modifying%20the%20embedding%20space%20%28whitening%2C%20contrastive%20fine%20tuning%29%2C%20but%20such%20transformations%20alter%20geometric%20structure%20and%20require%20recomputing%20all%20embeddings.%0A%20%20Using%20isotonic%20regression%20trained%20on%20human%20similarity%20judgments%2C%20we%20construct%20a%20monotonic%20transformation%20that%20achieves%20near-perfect%20calibration%20while%20preserving%20rank%20correlation%20and%20local%20stability%2898%25%20across%20seven%20perturbation%20types%29.%20Our%20contribution%20is%20not%20to%20replace%20cosine%20similarity%2C%20but%20to%20restore%20interpretability%20of%20its%20absolute%20values%20through%20monotone%20calibration%2C%20without%20altering%20its%20ranking%20properties.%0A%20%20We%20characterize%20isotonic%20calibration%20as%20an%20order-preserving%20reparameterization%20and%20prove%20that%20all%20order-based%20constructions%20%28angular%20ordering%2C%20nearest%20neighbors%2C%20threshold%20graphs%20and%20quantile-based%20decisions%29%20are%20invariant%20under%20this%20transformation.&entry.1838667208=http%3A//arxiv.org/abs/2601.16907v1&entry.124074799=Read"},
{"title": "Evaluating Large Vision-language Models for Surgical Tool Detection", "author": "Nakul Poudel and Richard Simon and Cristian A. Linte", "abstract": "Surgery is a highly complex process, and artificial intelligence has emerged as a transformative force in supporting surgical guidance and decision-making. However, the unimodal nature of most current AI systems limits their ability to achieve a holistic understanding of surgical workflows. This highlights the need for general-purpose surgical AI systems capable of comprehensively modeling the interrelated components of surgical scenes. Recent advances in large vision-language models that integrate multimodal data processing offer strong potential for modeling surgical tasks and providing human-like scene reasoning and understanding. Despite their promise, systematic investigations of VLMs in surgical applications remain limited. In this study, we evaluate the effectiveness of large VLMs for the fundamental surgical vision task of detecting surgical tools. Specifically, we investigate three state-of-the-art VLMs, Qwen2.5, LLaVA1.5, and InternVL3.5, on the GraSP robotic surgery dataset under both zero-shot and parameter-efficient LoRA fine-tuning settings. Our results demonstrate that Qwen2.5 consistently achieves superior detection performance in both configurations among the evaluated VLMs. Furthermore, compared with the open-set detection baseline Grounding DINO, Qwen2.5 exhibits stronger zero-shot generalization and comparable fine-tuned performance. Notably, Qwen2.5 shows superior instrument recognition, while Grounding DINO demonstrates stronger localization.", "link": "http://arxiv.org/abs/2601.16895v1", "date": "2026-01-23", "relevancy": 2.2603, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5727}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5727}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Large%20Vision-language%20Models%20for%20Surgical%20Tool%20Detection&body=Title%3A%20Evaluating%20Large%20Vision-language%20Models%20for%20Surgical%20Tool%20Detection%0AAuthor%3A%20Nakul%20Poudel%20and%20Richard%20Simon%20and%20Cristian%20A.%20Linte%0AAbstract%3A%20Surgery%20is%20a%20highly%20complex%20process%2C%20and%20artificial%20intelligence%20has%20emerged%20as%20a%20transformative%20force%20in%20supporting%20surgical%20guidance%20and%20decision-making.%20However%2C%20the%20unimodal%20nature%20of%20most%20current%20AI%20systems%20limits%20their%20ability%20to%20achieve%20a%20holistic%20understanding%20of%20surgical%20workflows.%20This%20highlights%20the%20need%20for%20general-purpose%20surgical%20AI%20systems%20capable%20of%20comprehensively%20modeling%20the%20interrelated%20components%20of%20surgical%20scenes.%20Recent%20advances%20in%20large%20vision-language%20models%20that%20integrate%20multimodal%20data%20processing%20offer%20strong%20potential%20for%20modeling%20surgical%20tasks%20and%20providing%20human-like%20scene%20reasoning%20and%20understanding.%20Despite%20their%20promise%2C%20systematic%20investigations%20of%20VLMs%20in%20surgical%20applications%20remain%20limited.%20In%20this%20study%2C%20we%20evaluate%20the%20effectiveness%20of%20large%20VLMs%20for%20the%20fundamental%20surgical%20vision%20task%20of%20detecting%20surgical%20tools.%20Specifically%2C%20we%20investigate%20three%20state-of-the-art%20VLMs%2C%20Qwen2.5%2C%20LLaVA1.5%2C%20and%20InternVL3.5%2C%20on%20the%20GraSP%20robotic%20surgery%20dataset%20under%20both%20zero-shot%20and%20parameter-efficient%20LoRA%20fine-tuning%20settings.%20Our%20results%20demonstrate%20that%20Qwen2.5%20consistently%20achieves%20superior%20detection%20performance%20in%20both%20configurations%20among%20the%20evaluated%20VLMs.%20Furthermore%2C%20compared%20with%20the%20open-set%20detection%20baseline%20Grounding%20DINO%2C%20Qwen2.5%20exhibits%20stronger%20zero-shot%20generalization%20and%20comparable%20fine-tuned%20performance.%20Notably%2C%20Qwen2.5%20shows%20superior%20instrument%20recognition%2C%20while%20Grounding%20DINO%20demonstrates%20stronger%20localization.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Large%2520Vision-language%2520Models%2520for%2520Surgical%2520Tool%2520Detection%26entry.906535625%3DNakul%2520Poudel%2520and%2520Richard%2520Simon%2520and%2520Cristian%2520A.%2520Linte%26entry.1292438233%3DSurgery%2520is%2520a%2520highly%2520complex%2520process%252C%2520and%2520artificial%2520intelligence%2520has%2520emerged%2520as%2520a%2520transformative%2520force%2520in%2520supporting%2520surgical%2520guidance%2520and%2520decision-making.%2520However%252C%2520the%2520unimodal%2520nature%2520of%2520most%2520current%2520AI%2520systems%2520limits%2520their%2520ability%2520to%2520achieve%2520a%2520holistic%2520understanding%2520of%2520surgical%2520workflows.%2520This%2520highlights%2520the%2520need%2520for%2520general-purpose%2520surgical%2520AI%2520systems%2520capable%2520of%2520comprehensively%2520modeling%2520the%2520interrelated%2520components%2520of%2520surgical%2520scenes.%2520Recent%2520advances%2520in%2520large%2520vision-language%2520models%2520that%2520integrate%2520multimodal%2520data%2520processing%2520offer%2520strong%2520potential%2520for%2520modeling%2520surgical%2520tasks%2520and%2520providing%2520human-like%2520scene%2520reasoning%2520and%2520understanding.%2520Despite%2520their%2520promise%252C%2520systematic%2520investigations%2520of%2520VLMs%2520in%2520surgical%2520applications%2520remain%2520limited.%2520In%2520this%2520study%252C%2520we%2520evaluate%2520the%2520effectiveness%2520of%2520large%2520VLMs%2520for%2520the%2520fundamental%2520surgical%2520vision%2520task%2520of%2520detecting%2520surgical%2520tools.%2520Specifically%252C%2520we%2520investigate%2520three%2520state-of-the-art%2520VLMs%252C%2520Qwen2.5%252C%2520LLaVA1.5%252C%2520and%2520InternVL3.5%252C%2520on%2520the%2520GraSP%2520robotic%2520surgery%2520dataset%2520under%2520both%2520zero-shot%2520and%2520parameter-efficient%2520LoRA%2520fine-tuning%2520settings.%2520Our%2520results%2520demonstrate%2520that%2520Qwen2.5%2520consistently%2520achieves%2520superior%2520detection%2520performance%2520in%2520both%2520configurations%2520among%2520the%2520evaluated%2520VLMs.%2520Furthermore%252C%2520compared%2520with%2520the%2520open-set%2520detection%2520baseline%2520Grounding%2520DINO%252C%2520Qwen2.5%2520exhibits%2520stronger%2520zero-shot%2520generalization%2520and%2520comparable%2520fine-tuned%2520performance.%2520Notably%252C%2520Qwen2.5%2520shows%2520superior%2520instrument%2520recognition%252C%2520while%2520Grounding%2520DINO%2520demonstrates%2520stronger%2520localization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Large%20Vision-language%20Models%20for%20Surgical%20Tool%20Detection&entry.906535625=Nakul%20Poudel%20and%20Richard%20Simon%20and%20Cristian%20A.%20Linte&entry.1292438233=Surgery%20is%20a%20highly%20complex%20process%2C%20and%20artificial%20intelligence%20has%20emerged%20as%20a%20transformative%20force%20in%20supporting%20surgical%20guidance%20and%20decision-making.%20However%2C%20the%20unimodal%20nature%20of%20most%20current%20AI%20systems%20limits%20their%20ability%20to%20achieve%20a%20holistic%20understanding%20of%20surgical%20workflows.%20This%20highlights%20the%20need%20for%20general-purpose%20surgical%20AI%20systems%20capable%20of%20comprehensively%20modeling%20the%20interrelated%20components%20of%20surgical%20scenes.%20Recent%20advances%20in%20large%20vision-language%20models%20that%20integrate%20multimodal%20data%20processing%20offer%20strong%20potential%20for%20modeling%20surgical%20tasks%20and%20providing%20human-like%20scene%20reasoning%20and%20understanding.%20Despite%20their%20promise%2C%20systematic%20investigations%20of%20VLMs%20in%20surgical%20applications%20remain%20limited.%20In%20this%20study%2C%20we%20evaluate%20the%20effectiveness%20of%20large%20VLMs%20for%20the%20fundamental%20surgical%20vision%20task%20of%20detecting%20surgical%20tools.%20Specifically%2C%20we%20investigate%20three%20state-of-the-art%20VLMs%2C%20Qwen2.5%2C%20LLaVA1.5%2C%20and%20InternVL3.5%2C%20on%20the%20GraSP%20robotic%20surgery%20dataset%20under%20both%20zero-shot%20and%20parameter-efficient%20LoRA%20fine-tuning%20settings.%20Our%20results%20demonstrate%20that%20Qwen2.5%20consistently%20achieves%20superior%20detection%20performance%20in%20both%20configurations%20among%20the%20evaluated%20VLMs.%20Furthermore%2C%20compared%20with%20the%20open-set%20detection%20baseline%20Grounding%20DINO%2C%20Qwen2.5%20exhibits%20stronger%20zero-shot%20generalization%20and%20comparable%20fine-tuned%20performance.%20Notably%2C%20Qwen2.5%20shows%20superior%20instrument%20recognition%2C%20while%20Grounding%20DINO%20demonstrates%20stronger%20localization.&entry.1838667208=http%3A//arxiv.org/abs/2601.16895v1&entry.124074799=Read"},
{"title": "A Survey on Human-Centered Evaluation of Explainable AI Methods in Clinical Decision Support Systems", "author": "Alessandro Gambetti and Qiwei Han and Hong Shen and Claudia Soares", "abstract": "Explainable Artificial Intelligence (XAI) is essential for the transparency and clinical adoption of Clinical Decision Support Systems (CDSS). However, the real-world effectiveness of existing XAI methods remains limited and is inconsistently evaluated. This study conducts a systematic PRISMA-guided survey of 31 human-centered evaluations (HCE) of XAI applied to CDSS, classifying them by XAI methodology, evaluation design, and adoption barrier. Our findings reveal that most existing studies employ post-hoc, model-agnostic approaches such as SHAP and Grad-CAM, typically assessed through small-scale clinician studies. The results show that over 80% of the studies adopt post-hoc, model-agnostic approaches such as SHAP and Grad-CAM, and that clinician sample sizes remain below 25 participants. The findings indicate that explanations generally improve clinician trust and diagnostic confidence, but frequently increase cognitive load and exhibit misalignment with domain reasoning processes. To bridge these gaps, we propose a stakeholder-centric evaluation framework that integrates socio-technical principles and human-computer interaction to guide the future development of clinically viable and trustworthy XAI-based CDSS.", "link": "http://arxiv.org/abs/2502.09849v4", "date": "2026-01-23", "relevancy": 2.2554, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4509}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Human-Centered%20Evaluation%20of%20Explainable%20AI%20Methods%20in%20Clinical%20Decision%20Support%20Systems&body=Title%3A%20A%20Survey%20on%20Human-Centered%20Evaluation%20of%20Explainable%20AI%20Methods%20in%20Clinical%20Decision%20Support%20Systems%0AAuthor%3A%20Alessandro%20Gambetti%20and%20Qiwei%20Han%20and%20Hong%20Shen%20and%20Claudia%20Soares%0AAbstract%3A%20Explainable%20Artificial%20Intelligence%20%28XAI%29%20is%20essential%20for%20the%20transparency%20and%20clinical%20adoption%20of%20Clinical%20Decision%20Support%20Systems%20%28CDSS%29.%20However%2C%20the%20real-world%20effectiveness%20of%20existing%20XAI%20methods%20remains%20limited%20and%20is%20inconsistently%20evaluated.%20This%20study%20conducts%20a%20systematic%20PRISMA-guided%20survey%20of%2031%20human-centered%20evaluations%20%28HCE%29%20of%20XAI%20applied%20to%20CDSS%2C%20classifying%20them%20by%20XAI%20methodology%2C%20evaluation%20design%2C%20and%20adoption%20barrier.%20Our%20findings%20reveal%20that%20most%20existing%20studies%20employ%20post-hoc%2C%20model-agnostic%20approaches%20such%20as%20SHAP%20and%20Grad-CAM%2C%20typically%20assessed%20through%20small-scale%20clinician%20studies.%20The%20results%20show%20that%20over%2080%25%20of%20the%20studies%20adopt%20post-hoc%2C%20model-agnostic%20approaches%20such%20as%20SHAP%20and%20Grad-CAM%2C%20and%20that%20clinician%20sample%20sizes%20remain%20below%2025%20participants.%20The%20findings%20indicate%20that%20explanations%20generally%20improve%20clinician%20trust%20and%20diagnostic%20confidence%2C%20but%20frequently%20increase%20cognitive%20load%20and%20exhibit%20misalignment%20with%20domain%20reasoning%20processes.%20To%20bridge%20these%20gaps%2C%20we%20propose%20a%20stakeholder-centric%20evaluation%20framework%20that%20integrates%20socio-technical%20principles%20and%20human-computer%20interaction%20to%20guide%20the%20future%20development%20of%20clinically%20viable%20and%20trustworthy%20XAI-based%20CDSS.%0ALink%3A%20http%3A//arxiv.org/abs/2502.09849v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Human-Centered%2520Evaluation%2520of%2520Explainable%2520AI%2520Methods%2520in%2520Clinical%2520Decision%2520Support%2520Systems%26entry.906535625%3DAlessandro%2520Gambetti%2520and%2520Qiwei%2520Han%2520and%2520Hong%2520Shen%2520and%2520Claudia%2520Soares%26entry.1292438233%3DExplainable%2520Artificial%2520Intelligence%2520%2528XAI%2529%2520is%2520essential%2520for%2520the%2520transparency%2520and%2520clinical%2520adoption%2520of%2520Clinical%2520Decision%2520Support%2520Systems%2520%2528CDSS%2529.%2520However%252C%2520the%2520real-world%2520effectiveness%2520of%2520existing%2520XAI%2520methods%2520remains%2520limited%2520and%2520is%2520inconsistently%2520evaluated.%2520This%2520study%2520conducts%2520a%2520systematic%2520PRISMA-guided%2520survey%2520of%252031%2520human-centered%2520evaluations%2520%2528HCE%2529%2520of%2520XAI%2520applied%2520to%2520CDSS%252C%2520classifying%2520them%2520by%2520XAI%2520methodology%252C%2520evaluation%2520design%252C%2520and%2520adoption%2520barrier.%2520Our%2520findings%2520reveal%2520that%2520most%2520existing%2520studies%2520employ%2520post-hoc%252C%2520model-agnostic%2520approaches%2520such%2520as%2520SHAP%2520and%2520Grad-CAM%252C%2520typically%2520assessed%2520through%2520small-scale%2520clinician%2520studies.%2520The%2520results%2520show%2520that%2520over%252080%2525%2520of%2520the%2520studies%2520adopt%2520post-hoc%252C%2520model-agnostic%2520approaches%2520such%2520as%2520SHAP%2520and%2520Grad-CAM%252C%2520and%2520that%2520clinician%2520sample%2520sizes%2520remain%2520below%252025%2520participants.%2520The%2520findings%2520indicate%2520that%2520explanations%2520generally%2520improve%2520clinician%2520trust%2520and%2520diagnostic%2520confidence%252C%2520but%2520frequently%2520increase%2520cognitive%2520load%2520and%2520exhibit%2520misalignment%2520with%2520domain%2520reasoning%2520processes.%2520To%2520bridge%2520these%2520gaps%252C%2520we%2520propose%2520a%2520stakeholder-centric%2520evaluation%2520framework%2520that%2520integrates%2520socio-technical%2520principles%2520and%2520human-computer%2520interaction%2520to%2520guide%2520the%2520future%2520development%2520of%2520clinically%2520viable%2520and%2520trustworthy%2520XAI-based%2520CDSS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09849v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Human-Centered%20Evaluation%20of%20Explainable%20AI%20Methods%20in%20Clinical%20Decision%20Support%20Systems&entry.906535625=Alessandro%20Gambetti%20and%20Qiwei%20Han%20and%20Hong%20Shen%20and%20Claudia%20Soares&entry.1292438233=Explainable%20Artificial%20Intelligence%20%28XAI%29%20is%20essential%20for%20the%20transparency%20and%20clinical%20adoption%20of%20Clinical%20Decision%20Support%20Systems%20%28CDSS%29.%20However%2C%20the%20real-world%20effectiveness%20of%20existing%20XAI%20methods%20remains%20limited%20and%20is%20inconsistently%20evaluated.%20This%20study%20conducts%20a%20systematic%20PRISMA-guided%20survey%20of%2031%20human-centered%20evaluations%20%28HCE%29%20of%20XAI%20applied%20to%20CDSS%2C%20classifying%20them%20by%20XAI%20methodology%2C%20evaluation%20design%2C%20and%20adoption%20barrier.%20Our%20findings%20reveal%20that%20most%20existing%20studies%20employ%20post-hoc%2C%20model-agnostic%20approaches%20such%20as%20SHAP%20and%20Grad-CAM%2C%20typically%20assessed%20through%20small-scale%20clinician%20studies.%20The%20results%20show%20that%20over%2080%25%20of%20the%20studies%20adopt%20post-hoc%2C%20model-agnostic%20approaches%20such%20as%20SHAP%20and%20Grad-CAM%2C%20and%20that%20clinician%20sample%20sizes%20remain%20below%2025%20participants.%20The%20findings%20indicate%20that%20explanations%20generally%20improve%20clinician%20trust%20and%20diagnostic%20confidence%2C%20but%20frequently%20increase%20cognitive%20load%20and%20exhibit%20misalignment%20with%20domain%20reasoning%20processes.%20To%20bridge%20these%20gaps%2C%20we%20propose%20a%20stakeholder-centric%20evaluation%20framework%20that%20integrates%20socio-technical%20principles%20and%20human-computer%20interaction%20to%20guide%20the%20future%20development%20of%20clinically%20viable%20and%20trustworthy%20XAI-based%20CDSS.&entry.1838667208=http%3A//arxiv.org/abs/2502.09849v4&entry.124074799=Read"},
{"title": "ColorConceptBench: A Benchmark for Probabilistic Color-Concept Understanding in Text-to-Image Models", "author": "Chenxi Ruan and Yu Xiao and Yihan Hou and Guosheng Hu and Wei Zeng", "abstract": "While text-to-image (T2I) models have advanced considerably, their capability to associate colors with implicit concepts remains underexplored. To address the gap, we introduce ColorConceptBench, a new human-annotated benchmark to systematically evaluate color-concept associations through the lens of probabilistic color distributions. ColorConceptBench moves beyond explicit color names or codes by probing how models translate 1,281 implicit color concepts using a foundation of 6,369 human annotations. Our evaluation of seven leading T2I models reveals that current models lack sensitivity to abstract semantics, and crucially, this limitation appears resistant to standard interventions (e.g., scaling and guidance). This demonstrates that achieving human-like color semantics requires more than larger models, but demands a fundamental shift in how models learn and represent implicit meaning.", "link": "http://arxiv.org/abs/2601.16836v1", "date": "2026-01-23", "relevancy": 2.2332, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5636}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5636}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ColorConceptBench%3A%20A%20Benchmark%20for%20Probabilistic%20Color-Concept%20Understanding%20in%20Text-to-Image%20Models&body=Title%3A%20ColorConceptBench%3A%20A%20Benchmark%20for%20Probabilistic%20Color-Concept%20Understanding%20in%20Text-to-Image%20Models%0AAuthor%3A%20Chenxi%20Ruan%20and%20Yu%20Xiao%20and%20Yihan%20Hou%20and%20Guosheng%20Hu%20and%20Wei%20Zeng%0AAbstract%3A%20While%20text-to-image%20%28T2I%29%20models%20have%20advanced%20considerably%2C%20their%20capability%20to%20associate%20colors%20with%20implicit%20concepts%20remains%20underexplored.%20To%20address%20the%20gap%2C%20we%20introduce%20ColorConceptBench%2C%20a%20new%20human-annotated%20benchmark%20to%20systematically%20evaluate%20color-concept%20associations%20through%20the%20lens%20of%20probabilistic%20color%20distributions.%20ColorConceptBench%20moves%20beyond%20explicit%20color%20names%20or%20codes%20by%20probing%20how%20models%20translate%201%2C281%20implicit%20color%20concepts%20using%20a%20foundation%20of%206%2C369%20human%20annotations.%20Our%20evaluation%20of%20seven%20leading%20T2I%20models%20reveals%20that%20current%20models%20lack%20sensitivity%20to%20abstract%20semantics%2C%20and%20crucially%2C%20this%20limitation%20appears%20resistant%20to%20standard%20interventions%20%28e.g.%2C%20scaling%20and%20guidance%29.%20This%20demonstrates%20that%20achieving%20human-like%20color%20semantics%20requires%20more%20than%20larger%20models%2C%20but%20demands%20a%20fundamental%20shift%20in%20how%20models%20learn%20and%20represent%20implicit%20meaning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DColorConceptBench%253A%2520A%2520Benchmark%2520for%2520Probabilistic%2520Color-Concept%2520Understanding%2520in%2520Text-to-Image%2520Models%26entry.906535625%3DChenxi%2520Ruan%2520and%2520Yu%2520Xiao%2520and%2520Yihan%2520Hou%2520and%2520Guosheng%2520Hu%2520and%2520Wei%2520Zeng%26entry.1292438233%3DWhile%2520text-to-image%2520%2528T2I%2529%2520models%2520have%2520advanced%2520considerably%252C%2520their%2520capability%2520to%2520associate%2520colors%2520with%2520implicit%2520concepts%2520remains%2520underexplored.%2520To%2520address%2520the%2520gap%252C%2520we%2520introduce%2520ColorConceptBench%252C%2520a%2520new%2520human-annotated%2520benchmark%2520to%2520systematically%2520evaluate%2520color-concept%2520associations%2520through%2520the%2520lens%2520of%2520probabilistic%2520color%2520distributions.%2520ColorConceptBench%2520moves%2520beyond%2520explicit%2520color%2520names%2520or%2520codes%2520by%2520probing%2520how%2520models%2520translate%25201%252C281%2520implicit%2520color%2520concepts%2520using%2520a%2520foundation%2520of%25206%252C369%2520human%2520annotations.%2520Our%2520evaluation%2520of%2520seven%2520leading%2520T2I%2520models%2520reveals%2520that%2520current%2520models%2520lack%2520sensitivity%2520to%2520abstract%2520semantics%252C%2520and%2520crucially%252C%2520this%2520limitation%2520appears%2520resistant%2520to%2520standard%2520interventions%2520%2528e.g.%252C%2520scaling%2520and%2520guidance%2529.%2520This%2520demonstrates%2520that%2520achieving%2520human-like%2520color%2520semantics%2520requires%2520more%2520than%2520larger%2520models%252C%2520but%2520demands%2520a%2520fundamental%2520shift%2520in%2520how%2520models%2520learn%2520and%2520represent%2520implicit%2520meaning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ColorConceptBench%3A%20A%20Benchmark%20for%20Probabilistic%20Color-Concept%20Understanding%20in%20Text-to-Image%20Models&entry.906535625=Chenxi%20Ruan%20and%20Yu%20Xiao%20and%20Yihan%20Hou%20and%20Guosheng%20Hu%20and%20Wei%20Zeng&entry.1292438233=While%20text-to-image%20%28T2I%29%20models%20have%20advanced%20considerably%2C%20their%20capability%20to%20associate%20colors%20with%20implicit%20concepts%20remains%20underexplored.%20To%20address%20the%20gap%2C%20we%20introduce%20ColorConceptBench%2C%20a%20new%20human-annotated%20benchmark%20to%20systematically%20evaluate%20color-concept%20associations%20through%20the%20lens%20of%20probabilistic%20color%20distributions.%20ColorConceptBench%20moves%20beyond%20explicit%20color%20names%20or%20codes%20by%20probing%20how%20models%20translate%201%2C281%20implicit%20color%20concepts%20using%20a%20foundation%20of%206%2C369%20human%20annotations.%20Our%20evaluation%20of%20seven%20leading%20T2I%20models%20reveals%20that%20current%20models%20lack%20sensitivity%20to%20abstract%20semantics%2C%20and%20crucially%2C%20this%20limitation%20appears%20resistant%20to%20standard%20interventions%20%28e.g.%2C%20scaling%20and%20guidance%29.%20This%20demonstrates%20that%20achieving%20human-like%20color%20semantics%20requires%20more%20than%20larger%20models%2C%20but%20demands%20a%20fundamental%20shift%20in%20how%20models%20learn%20and%20represent%20implicit%20meaning.&entry.1838667208=http%3A//arxiv.org/abs/2601.16836v1&entry.124074799=Read"},
{"title": "Language-Guided and Motion-Aware Gait Representation for Generalizable Recognition", "author": "Zhengxian Wu and Chuanrui Zhang and Shenao Jiang and Hangrui Xu and Zirui Liao and Luyuan Zhang and Huaqiu Li and Peng Jiao and Haoqian Wang", "abstract": "Gait recognition is emerging as a promising technology and an innovative field within computer vision, with a wide range of applications in remote human identification. However, existing methods typically rely on complex architectures to directly extract features from images and apply pooling operations to obtain sequence-level representations. Such designs often lead to overfitting on static noise (e.g., clothing), while failing to effectively capture dynamic motion regions, such as the arms and legs. This bottleneck is particularly challenging in the presence of intra-class variation, where gait features of the same individual under different environmental conditions are significantly distant in the feature space. To address the above challenges, we present a Languageguided and Motion-aware gait recognition framework, named LMGait. To the best of our knowledge, LMGait is the first method to introduce natural language descriptions as explicit semantic priors into the gait recognition task. In particular, we utilize designed gait-related language cues to capture key motion features in gait sequences. To improve cross-modal alignment, we propose the Motion Awareness Module (MAM), which refines the language features by adaptively adjusting various levels of semantic information to ensure better alignment with the visual representations. Furthermore, we introduce the Motion Temporal Capture Module (MTCM) to enhance the discriminative capability of gait features and improve the model's motion tracking ability. We conducted extensive experiments across multiple datasets, and the results demonstrate the significant advantages of our proposed network. Specifically, our model achieved accuracies of 88.5%, 97.1%, and 97.5% on the CCPG, SUSTech1K, and CASIAB datasets, respectively, achieving state-of-the-art performance. Homepage: https://dingwu1021.github.io/LMGait/", "link": "http://arxiv.org/abs/2601.11931v2", "date": "2026-01-23", "relevancy": 2.2303, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5731}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5586}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-Guided%20and%20Motion-Aware%20Gait%20Representation%20for%20Generalizable%20Recognition&body=Title%3A%20Language-Guided%20and%20Motion-Aware%20Gait%20Representation%20for%20Generalizable%20Recognition%0AAuthor%3A%20Zhengxian%20Wu%20and%20Chuanrui%20Zhang%20and%20Shenao%20Jiang%20and%20Hangrui%20Xu%20and%20Zirui%20Liao%20and%20Luyuan%20Zhang%20and%20Huaqiu%20Li%20and%20Peng%20Jiao%20and%20Haoqian%20Wang%0AAbstract%3A%20Gait%20recognition%20is%20emerging%20as%20a%20promising%20technology%20and%20an%20innovative%20field%20within%20computer%20vision%2C%20with%20a%20wide%20range%20of%20applications%20in%20remote%20human%20identification.%20However%2C%20existing%20methods%20typically%20rely%20on%20complex%20architectures%20to%20directly%20extract%20features%20from%20images%20and%20apply%20pooling%20operations%20to%20obtain%20sequence-level%20representations.%20Such%20designs%20often%20lead%20to%20overfitting%20on%20static%20noise%20%28e.g.%2C%20clothing%29%2C%20while%20failing%20to%20effectively%20capture%20dynamic%20motion%20regions%2C%20such%20as%20the%20arms%20and%20legs.%20This%20bottleneck%20is%20particularly%20challenging%20in%20the%20presence%20of%20intra-class%20variation%2C%20where%20gait%20features%20of%20the%20same%20individual%20under%20different%20environmental%20conditions%20are%20significantly%20distant%20in%20the%20feature%20space.%20To%20address%20the%20above%20challenges%2C%20we%20present%20a%20Languageguided%20and%20Motion-aware%20gait%20recognition%20framework%2C%20named%20LMGait.%20To%20the%20best%20of%20our%20knowledge%2C%20LMGait%20is%20the%20first%20method%20to%20introduce%20natural%20language%20descriptions%20as%20explicit%20semantic%20priors%20into%20the%20gait%20recognition%20task.%20In%20particular%2C%20we%20utilize%20designed%20gait-related%20language%20cues%20to%20capture%20key%20motion%20features%20in%20gait%20sequences.%20To%20improve%20cross-modal%20alignment%2C%20we%20propose%20the%20Motion%20Awareness%20Module%20%28MAM%29%2C%20which%20refines%20the%20language%20features%20by%20adaptively%20adjusting%20various%20levels%20of%20semantic%20information%20to%20ensure%20better%20alignment%20with%20the%20visual%20representations.%20Furthermore%2C%20we%20introduce%20the%20Motion%20Temporal%20Capture%20Module%20%28MTCM%29%20to%20enhance%20the%20discriminative%20capability%20of%20gait%20features%20and%20improve%20the%20model%27s%20motion%20tracking%20ability.%20We%20conducted%20extensive%20experiments%20across%20multiple%20datasets%2C%20and%20the%20results%20demonstrate%20the%20significant%20advantages%20of%20our%20proposed%20network.%20Specifically%2C%20our%20model%20achieved%20accuracies%20of%2088.5%25%2C%2097.1%25%2C%20and%2097.5%25%20on%20the%20CCPG%2C%20SUSTech1K%2C%20and%20CASIAB%20datasets%2C%20respectively%2C%20achieving%20state-of-the-art%20performance.%20Homepage%3A%20https%3A//dingwu1021.github.io/LMGait/%0ALink%3A%20http%3A//arxiv.org/abs/2601.11931v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-Guided%2520and%2520Motion-Aware%2520Gait%2520Representation%2520for%2520Generalizable%2520Recognition%26entry.906535625%3DZhengxian%2520Wu%2520and%2520Chuanrui%2520Zhang%2520and%2520Shenao%2520Jiang%2520and%2520Hangrui%2520Xu%2520and%2520Zirui%2520Liao%2520and%2520Luyuan%2520Zhang%2520and%2520Huaqiu%2520Li%2520and%2520Peng%2520Jiao%2520and%2520Haoqian%2520Wang%26entry.1292438233%3DGait%2520recognition%2520is%2520emerging%2520as%2520a%2520promising%2520technology%2520and%2520an%2520innovative%2520field%2520within%2520computer%2520vision%252C%2520with%2520a%2520wide%2520range%2520of%2520applications%2520in%2520remote%2520human%2520identification.%2520However%252C%2520existing%2520methods%2520typically%2520rely%2520on%2520complex%2520architectures%2520to%2520directly%2520extract%2520features%2520from%2520images%2520and%2520apply%2520pooling%2520operations%2520to%2520obtain%2520sequence-level%2520representations.%2520Such%2520designs%2520often%2520lead%2520to%2520overfitting%2520on%2520static%2520noise%2520%2528e.g.%252C%2520clothing%2529%252C%2520while%2520failing%2520to%2520effectively%2520capture%2520dynamic%2520motion%2520regions%252C%2520such%2520as%2520the%2520arms%2520and%2520legs.%2520This%2520bottleneck%2520is%2520particularly%2520challenging%2520in%2520the%2520presence%2520of%2520intra-class%2520variation%252C%2520where%2520gait%2520features%2520of%2520the%2520same%2520individual%2520under%2520different%2520environmental%2520conditions%2520are%2520significantly%2520distant%2520in%2520the%2520feature%2520space.%2520To%2520address%2520the%2520above%2520challenges%252C%2520we%2520present%2520a%2520Languageguided%2520and%2520Motion-aware%2520gait%2520recognition%2520framework%252C%2520named%2520LMGait.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520LMGait%2520is%2520the%2520first%2520method%2520to%2520introduce%2520natural%2520language%2520descriptions%2520as%2520explicit%2520semantic%2520priors%2520into%2520the%2520gait%2520recognition%2520task.%2520In%2520particular%252C%2520we%2520utilize%2520designed%2520gait-related%2520language%2520cues%2520to%2520capture%2520key%2520motion%2520features%2520in%2520gait%2520sequences.%2520To%2520improve%2520cross-modal%2520alignment%252C%2520we%2520propose%2520the%2520Motion%2520Awareness%2520Module%2520%2528MAM%2529%252C%2520which%2520refines%2520the%2520language%2520features%2520by%2520adaptively%2520adjusting%2520various%2520levels%2520of%2520semantic%2520information%2520to%2520ensure%2520better%2520alignment%2520with%2520the%2520visual%2520representations.%2520Furthermore%252C%2520we%2520introduce%2520the%2520Motion%2520Temporal%2520Capture%2520Module%2520%2528MTCM%2529%2520to%2520enhance%2520the%2520discriminative%2520capability%2520of%2520gait%2520features%2520and%2520improve%2520the%2520model%2527s%2520motion%2520tracking%2520ability.%2520We%2520conducted%2520extensive%2520experiments%2520across%2520multiple%2520datasets%252C%2520and%2520the%2520results%2520demonstrate%2520the%2520significant%2520advantages%2520of%2520our%2520proposed%2520network.%2520Specifically%252C%2520our%2520model%2520achieved%2520accuracies%2520of%252088.5%2525%252C%252097.1%2525%252C%2520and%252097.5%2525%2520on%2520the%2520CCPG%252C%2520SUSTech1K%252C%2520and%2520CASIAB%2520datasets%252C%2520respectively%252C%2520achieving%2520state-of-the-art%2520performance.%2520Homepage%253A%2520https%253A//dingwu1021.github.io/LMGait/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11931v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Guided%20and%20Motion-Aware%20Gait%20Representation%20for%20Generalizable%20Recognition&entry.906535625=Zhengxian%20Wu%20and%20Chuanrui%20Zhang%20and%20Shenao%20Jiang%20and%20Hangrui%20Xu%20and%20Zirui%20Liao%20and%20Luyuan%20Zhang%20and%20Huaqiu%20Li%20and%20Peng%20Jiao%20and%20Haoqian%20Wang&entry.1292438233=Gait%20recognition%20is%20emerging%20as%20a%20promising%20technology%20and%20an%20innovative%20field%20within%20computer%20vision%2C%20with%20a%20wide%20range%20of%20applications%20in%20remote%20human%20identification.%20However%2C%20existing%20methods%20typically%20rely%20on%20complex%20architectures%20to%20directly%20extract%20features%20from%20images%20and%20apply%20pooling%20operations%20to%20obtain%20sequence-level%20representations.%20Such%20designs%20often%20lead%20to%20overfitting%20on%20static%20noise%20%28e.g.%2C%20clothing%29%2C%20while%20failing%20to%20effectively%20capture%20dynamic%20motion%20regions%2C%20such%20as%20the%20arms%20and%20legs.%20This%20bottleneck%20is%20particularly%20challenging%20in%20the%20presence%20of%20intra-class%20variation%2C%20where%20gait%20features%20of%20the%20same%20individual%20under%20different%20environmental%20conditions%20are%20significantly%20distant%20in%20the%20feature%20space.%20To%20address%20the%20above%20challenges%2C%20we%20present%20a%20Languageguided%20and%20Motion-aware%20gait%20recognition%20framework%2C%20named%20LMGait.%20To%20the%20best%20of%20our%20knowledge%2C%20LMGait%20is%20the%20first%20method%20to%20introduce%20natural%20language%20descriptions%20as%20explicit%20semantic%20priors%20into%20the%20gait%20recognition%20task.%20In%20particular%2C%20we%20utilize%20designed%20gait-related%20language%20cues%20to%20capture%20key%20motion%20features%20in%20gait%20sequences.%20To%20improve%20cross-modal%20alignment%2C%20we%20propose%20the%20Motion%20Awareness%20Module%20%28MAM%29%2C%20which%20refines%20the%20language%20features%20by%20adaptively%20adjusting%20various%20levels%20of%20semantic%20information%20to%20ensure%20better%20alignment%20with%20the%20visual%20representations.%20Furthermore%2C%20we%20introduce%20the%20Motion%20Temporal%20Capture%20Module%20%28MTCM%29%20to%20enhance%20the%20discriminative%20capability%20of%20gait%20features%20and%20improve%20the%20model%27s%20motion%20tracking%20ability.%20We%20conducted%20extensive%20experiments%20across%20multiple%20datasets%2C%20and%20the%20results%20demonstrate%20the%20significant%20advantages%20of%20our%20proposed%20network.%20Specifically%2C%20our%20model%20achieved%20accuracies%20of%2088.5%25%2C%2097.1%25%2C%20and%2097.5%25%20on%20the%20CCPG%2C%20SUSTech1K%2C%20and%20CASIAB%20datasets%2C%20respectively%2C%20achieving%20state-of-the-art%20performance.%20Homepage%3A%20https%3A//dingwu1021.github.io/LMGait/&entry.1838667208=http%3A//arxiv.org/abs/2601.11931v2&entry.124074799=Read"},
{"title": "ReViP: Reducing False Completion in Vision-Language-Action Models with Vision-Proprioception Rebalance", "author": "Zhuohao Li and Yinghao Li and Jian-Jian Jiang and Lang Zhou and Tianyu Zhang and Wei-Shi Zheng", "abstract": "Vision-Language-Action (VLA) models have advanced robotic manipulation by combining vision, language, and proprioception to predict actions. However, previous methods fuse proprioceptive signals directly with VLM-encoded vision-language features, resulting in state-dominant bias and false completions despite visible execution failures. We attribute this to modality imbalance, where policies over-rely on internal state while underusing visual evidence. To address this, we present ReViP, a novel VLA framework with Vision-Proprioception Rebalance to enhance visual grounding and robustness under perturbations. The key insight is to introduce auxiliary task-aware environment priors to adaptively modulate the coupling between semantic perception and proprioceptive dynamics. Specifically, we use an external VLM as a task-stage observer to extract real-time task-centric visual cues from visual observations, which drive a Vision-Proprioception Feature-wise Linear Modulation to enhance environmental awareness and reduce state-driven errors. Moreover, to evaluate false completion, we propose the first False-Completion Benchmark Suite built on LIBERO with controlled settings such as Object-Drop. Extensive experiments show that ReViP effectively reduces false-completion rates and improves success rates over strong VLA baselines on our suite, with gains extending to LIBERO, RoboTwin 2.0, and real-world evaluations.", "link": "http://arxiv.org/abs/2601.16667v1", "date": "2026-01-23", "relevancy": 2.2301, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5567}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReViP%3A%20Reducing%20False%20Completion%20in%20Vision-Language-Action%20Models%20with%20Vision-Proprioception%20Rebalance&body=Title%3A%20ReViP%3A%20Reducing%20False%20Completion%20in%20Vision-Language-Action%20Models%20with%20Vision-Proprioception%20Rebalance%0AAuthor%3A%20Zhuohao%20Li%20and%20Yinghao%20Li%20and%20Jian-Jian%20Jiang%20and%20Lang%20Zhou%20and%20Tianyu%20Zhang%20and%20Wei-Shi%20Zheng%0AAbstract%3A%20Vision-Language-Action%20%28VLA%29%20models%20have%20advanced%20robotic%20manipulation%20by%20combining%20vision%2C%20language%2C%20and%20proprioception%20to%20predict%20actions.%20However%2C%20previous%20methods%20fuse%20proprioceptive%20signals%20directly%20with%20VLM-encoded%20vision-language%20features%2C%20resulting%20in%20state-dominant%20bias%20and%20false%20completions%20despite%20visible%20execution%20failures.%20We%20attribute%20this%20to%20modality%20imbalance%2C%20where%20policies%20over-rely%20on%20internal%20state%20while%20underusing%20visual%20evidence.%20To%20address%20this%2C%20we%20present%20ReViP%2C%20a%20novel%20VLA%20framework%20with%20Vision-Proprioception%20Rebalance%20to%20enhance%20visual%20grounding%20and%20robustness%20under%20perturbations.%20The%20key%20insight%20is%20to%20introduce%20auxiliary%20task-aware%20environment%20priors%20to%20adaptively%20modulate%20the%20coupling%20between%20semantic%20perception%20and%20proprioceptive%20dynamics.%20Specifically%2C%20we%20use%20an%20external%20VLM%20as%20a%20task-stage%20observer%20to%20extract%20real-time%20task-centric%20visual%20cues%20from%20visual%20observations%2C%20which%20drive%20a%20Vision-Proprioception%20Feature-wise%20Linear%20Modulation%20to%20enhance%20environmental%20awareness%20and%20reduce%20state-driven%20errors.%20Moreover%2C%20to%20evaluate%20false%20completion%2C%20we%20propose%20the%20first%20False-Completion%20Benchmark%20Suite%20built%20on%20LIBERO%20with%20controlled%20settings%20such%20as%20Object-Drop.%20Extensive%20experiments%20show%20that%20ReViP%20effectively%20reduces%20false-completion%20rates%20and%20improves%20success%20rates%20over%20strong%20VLA%20baselines%20on%20our%20suite%2C%20with%20gains%20extending%20to%20LIBERO%2C%20RoboTwin%202.0%2C%20and%20real-world%20evaluations.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16667v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReViP%253A%2520Reducing%2520False%2520Completion%2520in%2520Vision-Language-Action%2520Models%2520with%2520Vision-Proprioception%2520Rebalance%26entry.906535625%3DZhuohao%2520Li%2520and%2520Yinghao%2520Li%2520and%2520Jian-Jian%2520Jiang%2520and%2520Lang%2520Zhou%2520and%2520Tianyu%2520Zhang%2520and%2520Wei-Shi%2520Zheng%26entry.1292438233%3DVision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520advanced%2520robotic%2520manipulation%2520by%2520combining%2520vision%252C%2520language%252C%2520and%2520proprioception%2520to%2520predict%2520actions.%2520However%252C%2520previous%2520methods%2520fuse%2520proprioceptive%2520signals%2520directly%2520with%2520VLM-encoded%2520vision-language%2520features%252C%2520resulting%2520in%2520state-dominant%2520bias%2520and%2520false%2520completions%2520despite%2520visible%2520execution%2520failures.%2520We%2520attribute%2520this%2520to%2520modality%2520imbalance%252C%2520where%2520policies%2520over-rely%2520on%2520internal%2520state%2520while%2520underusing%2520visual%2520evidence.%2520To%2520address%2520this%252C%2520we%2520present%2520ReViP%252C%2520a%2520novel%2520VLA%2520framework%2520with%2520Vision-Proprioception%2520Rebalance%2520to%2520enhance%2520visual%2520grounding%2520and%2520robustness%2520under%2520perturbations.%2520The%2520key%2520insight%2520is%2520to%2520introduce%2520auxiliary%2520task-aware%2520environment%2520priors%2520to%2520adaptively%2520modulate%2520the%2520coupling%2520between%2520semantic%2520perception%2520and%2520proprioceptive%2520dynamics.%2520Specifically%252C%2520we%2520use%2520an%2520external%2520VLM%2520as%2520a%2520task-stage%2520observer%2520to%2520extract%2520real-time%2520task-centric%2520visual%2520cues%2520from%2520visual%2520observations%252C%2520which%2520drive%2520a%2520Vision-Proprioception%2520Feature-wise%2520Linear%2520Modulation%2520to%2520enhance%2520environmental%2520awareness%2520and%2520reduce%2520state-driven%2520errors.%2520Moreover%252C%2520to%2520evaluate%2520false%2520completion%252C%2520we%2520propose%2520the%2520first%2520False-Completion%2520Benchmark%2520Suite%2520built%2520on%2520LIBERO%2520with%2520controlled%2520settings%2520such%2520as%2520Object-Drop.%2520Extensive%2520experiments%2520show%2520that%2520ReViP%2520effectively%2520reduces%2520false-completion%2520rates%2520and%2520improves%2520success%2520rates%2520over%2520strong%2520VLA%2520baselines%2520on%2520our%2520suite%252C%2520with%2520gains%2520extending%2520to%2520LIBERO%252C%2520RoboTwin%25202.0%252C%2520and%2520real-world%2520evaluations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16667v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReViP%3A%20Reducing%20False%20Completion%20in%20Vision-Language-Action%20Models%20with%20Vision-Proprioception%20Rebalance&entry.906535625=Zhuohao%20Li%20and%20Yinghao%20Li%20and%20Jian-Jian%20Jiang%20and%20Lang%20Zhou%20and%20Tianyu%20Zhang%20and%20Wei-Shi%20Zheng&entry.1292438233=Vision-Language-Action%20%28VLA%29%20models%20have%20advanced%20robotic%20manipulation%20by%20combining%20vision%2C%20language%2C%20and%20proprioception%20to%20predict%20actions.%20However%2C%20previous%20methods%20fuse%20proprioceptive%20signals%20directly%20with%20VLM-encoded%20vision-language%20features%2C%20resulting%20in%20state-dominant%20bias%20and%20false%20completions%20despite%20visible%20execution%20failures.%20We%20attribute%20this%20to%20modality%20imbalance%2C%20where%20policies%20over-rely%20on%20internal%20state%20while%20underusing%20visual%20evidence.%20To%20address%20this%2C%20we%20present%20ReViP%2C%20a%20novel%20VLA%20framework%20with%20Vision-Proprioception%20Rebalance%20to%20enhance%20visual%20grounding%20and%20robustness%20under%20perturbations.%20The%20key%20insight%20is%20to%20introduce%20auxiliary%20task-aware%20environment%20priors%20to%20adaptively%20modulate%20the%20coupling%20between%20semantic%20perception%20and%20proprioceptive%20dynamics.%20Specifically%2C%20we%20use%20an%20external%20VLM%20as%20a%20task-stage%20observer%20to%20extract%20real-time%20task-centric%20visual%20cues%20from%20visual%20observations%2C%20which%20drive%20a%20Vision-Proprioception%20Feature-wise%20Linear%20Modulation%20to%20enhance%20environmental%20awareness%20and%20reduce%20state-driven%20errors.%20Moreover%2C%20to%20evaluate%20false%20completion%2C%20we%20propose%20the%20first%20False-Completion%20Benchmark%20Suite%20built%20on%20LIBERO%20with%20controlled%20settings%20such%20as%20Object-Drop.%20Extensive%20experiments%20show%20that%20ReViP%20effectively%20reduces%20false-completion%20rates%20and%20improves%20success%20rates%20over%20strong%20VLA%20baselines%20on%20our%20suite%2C%20with%20gains%20extending%20to%20LIBERO%2C%20RoboTwin%202.0%2C%20and%20real-world%20evaluations.&entry.1838667208=http%3A//arxiv.org/abs/2601.16667v1&entry.124074799=Read"},
{"title": "Domain-invariant Mixed-domain Semi-supervised Medical Image Segmentation with Clustered Maximum Mean Discrepancy Alignment", "author": "Ba-Thinh Lam and Thanh-Huy Nguyen and Hoang-Thien Nguyen and Quang-Khai Bui-Tran and Nguyen Lan Vi Vu and Phat K. Huynh and Ulas Bagci and Min Xu", "abstract": "Deep learning has shown remarkable progress in medical image semantic segmentation, yet its success heavily depends on large-scale expert annotations and consistent data distributions. In practice, annotations are scarce, and images are collected from multiple scanners or centers, leading to mixed-domain settings with unknown domain labels and severe domain gaps. Existing semi-supervised or domain adaptation approaches typically assume either a single domain shift or access to explicit domain indices, which rarely hold in real-world deployment. In this paper, we propose a domain-invariant mixed-domain semi-supervised segmentation framework that jointly enhances data diversity and mitigates domain bias. A Copy-Paste Mechanism (CPM) augments the training set by transferring informative regions across domains, while a Cluster Maximum Mean Discrepancy (CMMD) block clusters unlabeled features and aligns them with labeled anchors via an MMD objective, encouraging domain-invariant representations. Integrated within a teacher-student framework, our method achieves robust and precise segmentation even with very few labeled examples and multiple unknown domain discrepancies. Experiments on Fundus and M&Ms benchmarks demonstrate that our approach consistently surpasses semi-supervised and domain adaptation methods, establishing a potential solution for mixed-domain semi-supervised medical image segmentation.", "link": "http://arxiv.org/abs/2601.16954v1", "date": "2026-01-23", "relevancy": 2.2138, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5607}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.554}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.55}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain-invariant%20Mixed-domain%20Semi-supervised%20Medical%20Image%20Segmentation%20with%20Clustered%20Maximum%20Mean%20Discrepancy%20Alignment&body=Title%3A%20Domain-invariant%20Mixed-domain%20Semi-supervised%20Medical%20Image%20Segmentation%20with%20Clustered%20Maximum%20Mean%20Discrepancy%20Alignment%0AAuthor%3A%20Ba-Thinh%20Lam%20and%20Thanh-Huy%20Nguyen%20and%20Hoang-Thien%20Nguyen%20and%20Quang-Khai%20Bui-Tran%20and%20Nguyen%20Lan%20Vi%20Vu%20and%20Phat%20K.%20Huynh%20and%20Ulas%20Bagci%20and%20Min%20Xu%0AAbstract%3A%20Deep%20learning%20has%20shown%20remarkable%20progress%20in%20medical%20image%20semantic%20segmentation%2C%20yet%20its%20success%20heavily%20depends%20on%20large-scale%20expert%20annotations%20and%20consistent%20data%20distributions.%20In%20practice%2C%20annotations%20are%20scarce%2C%20and%20images%20are%20collected%20from%20multiple%20scanners%20or%20centers%2C%20leading%20to%20mixed-domain%20settings%20with%20unknown%20domain%20labels%20and%20severe%20domain%20gaps.%20Existing%20semi-supervised%20or%20domain%20adaptation%20approaches%20typically%20assume%20either%20a%20single%20domain%20shift%20or%20access%20to%20explicit%20domain%20indices%2C%20which%20rarely%20hold%20in%20real-world%20deployment.%20In%20this%20paper%2C%20we%20propose%20a%20domain-invariant%20mixed-domain%20semi-supervised%20segmentation%20framework%20that%20jointly%20enhances%20data%20diversity%20and%20mitigates%20domain%20bias.%20A%20Copy-Paste%20Mechanism%20%28CPM%29%20augments%20the%20training%20set%20by%20transferring%20informative%20regions%20across%20domains%2C%20while%20a%20Cluster%20Maximum%20Mean%20Discrepancy%20%28CMMD%29%20block%20clusters%20unlabeled%20features%20and%20aligns%20them%20with%20labeled%20anchors%20via%20an%20MMD%20objective%2C%20encouraging%20domain-invariant%20representations.%20Integrated%20within%20a%20teacher-student%20framework%2C%20our%20method%20achieves%20robust%20and%20precise%20segmentation%20even%20with%20very%20few%20labeled%20examples%20and%20multiple%20unknown%20domain%20discrepancies.%20Experiments%20on%20Fundus%20and%20M%26Ms%20benchmarks%20demonstrate%20that%20our%20approach%20consistently%20surpasses%20semi-supervised%20and%20domain%20adaptation%20methods%2C%20establishing%20a%20potential%20solution%20for%20mixed-domain%20semi-supervised%20medical%20image%20segmentation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16954v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain-invariant%2520Mixed-domain%2520Semi-supervised%2520Medical%2520Image%2520Segmentation%2520with%2520Clustered%2520Maximum%2520Mean%2520Discrepancy%2520Alignment%26entry.906535625%3DBa-Thinh%2520Lam%2520and%2520Thanh-Huy%2520Nguyen%2520and%2520Hoang-Thien%2520Nguyen%2520and%2520Quang-Khai%2520Bui-Tran%2520and%2520Nguyen%2520Lan%2520Vi%2520Vu%2520and%2520Phat%2520K.%2520Huynh%2520and%2520Ulas%2520Bagci%2520and%2520Min%2520Xu%26entry.1292438233%3DDeep%2520learning%2520has%2520shown%2520remarkable%2520progress%2520in%2520medical%2520image%2520semantic%2520segmentation%252C%2520yet%2520its%2520success%2520heavily%2520depends%2520on%2520large-scale%2520expert%2520annotations%2520and%2520consistent%2520data%2520distributions.%2520In%2520practice%252C%2520annotations%2520are%2520scarce%252C%2520and%2520images%2520are%2520collected%2520from%2520multiple%2520scanners%2520or%2520centers%252C%2520leading%2520to%2520mixed-domain%2520settings%2520with%2520unknown%2520domain%2520labels%2520and%2520severe%2520domain%2520gaps.%2520Existing%2520semi-supervised%2520or%2520domain%2520adaptation%2520approaches%2520typically%2520assume%2520either%2520a%2520single%2520domain%2520shift%2520or%2520access%2520to%2520explicit%2520domain%2520indices%252C%2520which%2520rarely%2520hold%2520in%2520real-world%2520deployment.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520domain-invariant%2520mixed-domain%2520semi-supervised%2520segmentation%2520framework%2520that%2520jointly%2520enhances%2520data%2520diversity%2520and%2520mitigates%2520domain%2520bias.%2520A%2520Copy-Paste%2520Mechanism%2520%2528CPM%2529%2520augments%2520the%2520training%2520set%2520by%2520transferring%2520informative%2520regions%2520across%2520domains%252C%2520while%2520a%2520Cluster%2520Maximum%2520Mean%2520Discrepancy%2520%2528CMMD%2529%2520block%2520clusters%2520unlabeled%2520features%2520and%2520aligns%2520them%2520with%2520labeled%2520anchors%2520via%2520an%2520MMD%2520objective%252C%2520encouraging%2520domain-invariant%2520representations.%2520Integrated%2520within%2520a%2520teacher-student%2520framework%252C%2520our%2520method%2520achieves%2520robust%2520and%2520precise%2520segmentation%2520even%2520with%2520very%2520few%2520labeled%2520examples%2520and%2520multiple%2520unknown%2520domain%2520discrepancies.%2520Experiments%2520on%2520Fundus%2520and%2520M%2526Ms%2520benchmarks%2520demonstrate%2520that%2520our%2520approach%2520consistently%2520surpasses%2520semi-supervised%2520and%2520domain%2520adaptation%2520methods%252C%2520establishing%2520a%2520potential%2520solution%2520for%2520mixed-domain%2520semi-supervised%2520medical%2520image%2520segmentation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16954v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain-invariant%20Mixed-domain%20Semi-supervised%20Medical%20Image%20Segmentation%20with%20Clustered%20Maximum%20Mean%20Discrepancy%20Alignment&entry.906535625=Ba-Thinh%20Lam%20and%20Thanh-Huy%20Nguyen%20and%20Hoang-Thien%20Nguyen%20and%20Quang-Khai%20Bui-Tran%20and%20Nguyen%20Lan%20Vi%20Vu%20and%20Phat%20K.%20Huynh%20and%20Ulas%20Bagci%20and%20Min%20Xu&entry.1292438233=Deep%20learning%20has%20shown%20remarkable%20progress%20in%20medical%20image%20semantic%20segmentation%2C%20yet%20its%20success%20heavily%20depends%20on%20large-scale%20expert%20annotations%20and%20consistent%20data%20distributions.%20In%20practice%2C%20annotations%20are%20scarce%2C%20and%20images%20are%20collected%20from%20multiple%20scanners%20or%20centers%2C%20leading%20to%20mixed-domain%20settings%20with%20unknown%20domain%20labels%20and%20severe%20domain%20gaps.%20Existing%20semi-supervised%20or%20domain%20adaptation%20approaches%20typically%20assume%20either%20a%20single%20domain%20shift%20or%20access%20to%20explicit%20domain%20indices%2C%20which%20rarely%20hold%20in%20real-world%20deployment.%20In%20this%20paper%2C%20we%20propose%20a%20domain-invariant%20mixed-domain%20semi-supervised%20segmentation%20framework%20that%20jointly%20enhances%20data%20diversity%20and%20mitigates%20domain%20bias.%20A%20Copy-Paste%20Mechanism%20%28CPM%29%20augments%20the%20training%20set%20by%20transferring%20informative%20regions%20across%20domains%2C%20while%20a%20Cluster%20Maximum%20Mean%20Discrepancy%20%28CMMD%29%20block%20clusters%20unlabeled%20features%20and%20aligns%20them%20with%20labeled%20anchors%20via%20an%20MMD%20objective%2C%20encouraging%20domain-invariant%20representations.%20Integrated%20within%20a%20teacher-student%20framework%2C%20our%20method%20achieves%20robust%20and%20precise%20segmentation%20even%20with%20very%20few%20labeled%20examples%20and%20multiple%20unknown%20domain%20discrepancies.%20Experiments%20on%20Fundus%20and%20M%26Ms%20benchmarks%20demonstrate%20that%20our%20approach%20consistently%20surpasses%20semi-supervised%20and%20domain%20adaptation%20methods%2C%20establishing%20a%20potential%20solution%20for%20mixed-domain%20semi-supervised%20medical%20image%20segmentation.&entry.1838667208=http%3A//arxiv.org/abs/2601.16954v1&entry.124074799=Read"},
{"title": "SyncLight: Controllable and Consistent Multi-View Relighting", "author": "David Serrano-Lozano and Anand Bhattad and Luis Herranz and Jean-Fran\u00e7ois Lalonde and Javier Vazquez-Corral", "abstract": "We present SyncLight, the first method to enable consistent, parametric relighting across multiple uncalibrated views of a static scene. While single-view relighting has advanced significantly, existing generative approaches struggle to maintain the rigorous lighting consistency essential for multi-camera broadcasts, stereoscopic cinema, and virtual production. SyncLight addresses this by enabling precise control over light intensity and color across a multi-view capture of a scene, conditioned on a single reference edit. Our method leverages a multi-view diffusion transformer trained using a latent bridge matching formulation, achieving high-fidelity relighting of the entire image set in a single inference step. To facilitate training, we introduce a large-scale hybrid dataset comprising diverse synthetic environments -- curated from existing sources and newly designed scenes -- alongside high-fidelity, real-world multi-view captures under calibrated illumination. Surprisingly, though trained only on image pairs, SyncLight generalizes zero-shot to an arbitrary number of viewpoints, effectively propagating lighting changes across all views, without requiring camera pose information. SyncLight enables practical relighting workflows for multi-view capture systems.", "link": "http://arxiv.org/abs/2601.16981v1", "date": "2026-01-23", "relevancy": 2.2114, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5816}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5471}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SyncLight%3A%20Controllable%20and%20Consistent%20Multi-View%20Relighting&body=Title%3A%20SyncLight%3A%20Controllable%20and%20Consistent%20Multi-View%20Relighting%0AAuthor%3A%20David%20Serrano-Lozano%20and%20Anand%20Bhattad%20and%20Luis%20Herranz%20and%20Jean-Fran%C3%A7ois%20Lalonde%20and%20Javier%20Vazquez-Corral%0AAbstract%3A%20We%20present%20SyncLight%2C%20the%20first%20method%20to%20enable%20consistent%2C%20parametric%20relighting%20across%20multiple%20uncalibrated%20views%20of%20a%20static%20scene.%20While%20single-view%20relighting%20has%20advanced%20significantly%2C%20existing%20generative%20approaches%20struggle%20to%20maintain%20the%20rigorous%20lighting%20consistency%20essential%20for%20multi-camera%20broadcasts%2C%20stereoscopic%20cinema%2C%20and%20virtual%20production.%20SyncLight%20addresses%20this%20by%20enabling%20precise%20control%20over%20light%20intensity%20and%20color%20across%20a%20multi-view%20capture%20of%20a%20scene%2C%20conditioned%20on%20a%20single%20reference%20edit.%20Our%20method%20leverages%20a%20multi-view%20diffusion%20transformer%20trained%20using%20a%20latent%20bridge%20matching%20formulation%2C%20achieving%20high-fidelity%20relighting%20of%20the%20entire%20image%20set%20in%20a%20single%20inference%20step.%20To%20facilitate%20training%2C%20we%20introduce%20a%20large-scale%20hybrid%20dataset%20comprising%20diverse%20synthetic%20environments%20--%20curated%20from%20existing%20sources%20and%20newly%20designed%20scenes%20--%20alongside%20high-fidelity%2C%20real-world%20multi-view%20captures%20under%20calibrated%20illumination.%20Surprisingly%2C%20though%20trained%20only%20on%20image%20pairs%2C%20SyncLight%20generalizes%20zero-shot%20to%20an%20arbitrary%20number%20of%20viewpoints%2C%20effectively%20propagating%20lighting%20changes%20across%20all%20views%2C%20without%20requiring%20camera%20pose%20information.%20SyncLight%20enables%20practical%20relighting%20workflows%20for%20multi-view%20capture%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSyncLight%253A%2520Controllable%2520and%2520Consistent%2520Multi-View%2520Relighting%26entry.906535625%3DDavid%2520Serrano-Lozano%2520and%2520Anand%2520Bhattad%2520and%2520Luis%2520Herranz%2520and%2520Jean-Fran%25C3%25A7ois%2520Lalonde%2520and%2520Javier%2520Vazquez-Corral%26entry.1292438233%3DWe%2520present%2520SyncLight%252C%2520the%2520first%2520method%2520to%2520enable%2520consistent%252C%2520parametric%2520relighting%2520across%2520multiple%2520uncalibrated%2520views%2520of%2520a%2520static%2520scene.%2520While%2520single-view%2520relighting%2520has%2520advanced%2520significantly%252C%2520existing%2520generative%2520approaches%2520struggle%2520to%2520maintain%2520the%2520rigorous%2520lighting%2520consistency%2520essential%2520for%2520multi-camera%2520broadcasts%252C%2520stereoscopic%2520cinema%252C%2520and%2520virtual%2520production.%2520SyncLight%2520addresses%2520this%2520by%2520enabling%2520precise%2520control%2520over%2520light%2520intensity%2520and%2520color%2520across%2520a%2520multi-view%2520capture%2520of%2520a%2520scene%252C%2520conditioned%2520on%2520a%2520single%2520reference%2520edit.%2520Our%2520method%2520leverages%2520a%2520multi-view%2520diffusion%2520transformer%2520trained%2520using%2520a%2520latent%2520bridge%2520matching%2520formulation%252C%2520achieving%2520high-fidelity%2520relighting%2520of%2520the%2520entire%2520image%2520set%2520in%2520a%2520single%2520inference%2520step.%2520To%2520facilitate%2520training%252C%2520we%2520introduce%2520a%2520large-scale%2520hybrid%2520dataset%2520comprising%2520diverse%2520synthetic%2520environments%2520--%2520curated%2520from%2520existing%2520sources%2520and%2520newly%2520designed%2520scenes%2520--%2520alongside%2520high-fidelity%252C%2520real-world%2520multi-view%2520captures%2520under%2520calibrated%2520illumination.%2520Surprisingly%252C%2520though%2520trained%2520only%2520on%2520image%2520pairs%252C%2520SyncLight%2520generalizes%2520zero-shot%2520to%2520an%2520arbitrary%2520number%2520of%2520viewpoints%252C%2520effectively%2520propagating%2520lighting%2520changes%2520across%2520all%2520views%252C%2520without%2520requiring%2520camera%2520pose%2520information.%2520SyncLight%2520enables%2520practical%2520relighting%2520workflows%2520for%2520multi-view%2520capture%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SyncLight%3A%20Controllable%20and%20Consistent%20Multi-View%20Relighting&entry.906535625=David%20Serrano-Lozano%20and%20Anand%20Bhattad%20and%20Luis%20Herranz%20and%20Jean-Fran%C3%A7ois%20Lalonde%20and%20Javier%20Vazquez-Corral&entry.1292438233=We%20present%20SyncLight%2C%20the%20first%20method%20to%20enable%20consistent%2C%20parametric%20relighting%20across%20multiple%20uncalibrated%20views%20of%20a%20static%20scene.%20While%20single-view%20relighting%20has%20advanced%20significantly%2C%20existing%20generative%20approaches%20struggle%20to%20maintain%20the%20rigorous%20lighting%20consistency%20essential%20for%20multi-camera%20broadcasts%2C%20stereoscopic%20cinema%2C%20and%20virtual%20production.%20SyncLight%20addresses%20this%20by%20enabling%20precise%20control%20over%20light%20intensity%20and%20color%20across%20a%20multi-view%20capture%20of%20a%20scene%2C%20conditioned%20on%20a%20single%20reference%20edit.%20Our%20method%20leverages%20a%20multi-view%20diffusion%20transformer%20trained%20using%20a%20latent%20bridge%20matching%20formulation%2C%20achieving%20high-fidelity%20relighting%20of%20the%20entire%20image%20set%20in%20a%20single%20inference%20step.%20To%20facilitate%20training%2C%20we%20introduce%20a%20large-scale%20hybrid%20dataset%20comprising%20diverse%20synthetic%20environments%20--%20curated%20from%20existing%20sources%20and%20newly%20designed%20scenes%20--%20alongside%20high-fidelity%2C%20real-world%20multi-view%20captures%20under%20calibrated%20illumination.%20Surprisingly%2C%20though%20trained%20only%20on%20image%20pairs%2C%20SyncLight%20generalizes%20zero-shot%20to%20an%20arbitrary%20number%20of%20viewpoints%2C%20effectively%20propagating%20lighting%20changes%20across%20all%20views%2C%20without%20requiring%20camera%20pose%20information.%20SyncLight%20enables%20practical%20relighting%20workflows%20for%20multi-view%20capture%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.16981v1&entry.124074799=Read"},
{"title": "Sim-to-Real Transfer via a Style-Identified Cycle Consistent Generative Adversarial Network: Zero-Shot Deployment on Robotic Manipulators through Visual Domain Adaptation", "author": "Luc\u00eda G\u00fcitta-L\u00f3pez and Lionel G\u00fcitta-L\u00f3pez and Jaime Boal and \u00c1lvaro Jes\u00fas L\u00f3pez-L\u00f3pez", "abstract": "The sample efficiency challenge in Deep Reinforcement Learning (DRL) compromises its industrial adoption due to the high cost and time demands of real-world training. Virtual environments offer a cost-effective alternative for training DRL agents, but the transfer of learned policies to real setups is hindered by the sim-to-real gap. Achieving zero-shot transfer, where agents perform directly in real environments without additional tuning, is particularly desirable for its efficiency and practical value. This work proposes a novel domain adaptation approach relying on a Style-Identified Cycle Consistent Generative Adversarial Network (StyleID-CycleGAN or SICGAN), an original Cycle Consistent Generative Adversarial Network (CycleGAN) based model. SICGAN translates raw virtual observations into real-synthetic images, creating a hybrid domain for training DRL agents that combines virtual dynamics with real-like visual inputs. Following virtual training, the agent can be directly deployed, bypassing the need for real-world training. The pipeline is validated with two distinct industrial robots in the approaching phase of a pick-and-place operation. In virtual environments agents achieve success rates of 90 to 100\\%, and real-world deployment confirms robust zero-shot transfer (i.e., without additional training in the physical environment) with accuracies above 95\\% for most workspace regions. We use augmented reality targets to improve the evaluation process efficiency, and experimentally demonstrate that the agent successfully generalizes to real objects of varying colors and shapes, including LEGO\\textsuperscript{\\textregistered}~cubes and a mug. These results establish the proposed pipeline as an efficient, scalable solution to the sim-to-real problem.", "link": "http://arxiv.org/abs/2601.16677v1", "date": "2026-01-23", "relevancy": 2.2092, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.574}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5501}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sim-to-Real%20Transfer%20via%20a%20Style-Identified%20Cycle%20Consistent%20Generative%20Adversarial%20Network%3A%20Zero-Shot%20Deployment%20on%20Robotic%20Manipulators%20through%20Visual%20Domain%20Adaptation&body=Title%3A%20Sim-to-Real%20Transfer%20via%20a%20Style-Identified%20Cycle%20Consistent%20Generative%20Adversarial%20Network%3A%20Zero-Shot%20Deployment%20on%20Robotic%20Manipulators%20through%20Visual%20Domain%20Adaptation%0AAuthor%3A%20Luc%C3%ADa%20G%C3%BCitta-L%C3%B3pez%20and%20Lionel%20G%C3%BCitta-L%C3%B3pez%20and%20Jaime%20Boal%20and%20%C3%81lvaro%20Jes%C3%BAs%20L%C3%B3pez-L%C3%B3pez%0AAbstract%3A%20The%20sample%20efficiency%20challenge%20in%20Deep%20Reinforcement%20Learning%20%28DRL%29%20compromises%20its%20industrial%20adoption%20due%20to%20the%20high%20cost%20and%20time%20demands%20of%20real-world%20training.%20Virtual%20environments%20offer%20a%20cost-effective%20alternative%20for%20training%20DRL%20agents%2C%20but%20the%20transfer%20of%20learned%20policies%20to%20real%20setups%20is%20hindered%20by%20the%20sim-to-real%20gap.%20Achieving%20zero-shot%20transfer%2C%20where%20agents%20perform%20directly%20in%20real%20environments%20without%20additional%20tuning%2C%20is%20particularly%20desirable%20for%20its%20efficiency%20and%20practical%20value.%20This%20work%20proposes%20a%20novel%20domain%20adaptation%20approach%20relying%20on%20a%20Style-Identified%20Cycle%20Consistent%20Generative%20Adversarial%20Network%20%28StyleID-CycleGAN%20or%20SICGAN%29%2C%20an%20original%20Cycle%20Consistent%20Generative%20Adversarial%20Network%20%28CycleGAN%29%20based%20model.%20SICGAN%20translates%20raw%20virtual%20observations%20into%20real-synthetic%20images%2C%20creating%20a%20hybrid%20domain%20for%20training%20DRL%20agents%20that%20combines%20virtual%20dynamics%20with%20real-like%20visual%20inputs.%20Following%20virtual%20training%2C%20the%20agent%20can%20be%20directly%20deployed%2C%20bypassing%20the%20need%20for%20real-world%20training.%20The%20pipeline%20is%20validated%20with%20two%20distinct%20industrial%20robots%20in%20the%20approaching%20phase%20of%20a%20pick-and-place%20operation.%20In%20virtual%20environments%20agents%20achieve%20success%20rates%20of%2090%20to%20100%5C%25%2C%20and%20real-world%20deployment%20confirms%20robust%20zero-shot%20transfer%20%28i.e.%2C%20without%20additional%20training%20in%20the%20physical%20environment%29%20with%20accuracies%20above%2095%5C%25%20for%20most%20workspace%20regions.%20We%20use%20augmented%20reality%20targets%20to%20improve%20the%20evaluation%20process%20efficiency%2C%20and%20experimentally%20demonstrate%20that%20the%20agent%20successfully%20generalizes%20to%20real%20objects%20of%20varying%20colors%20and%20shapes%2C%20including%20LEGO%5Ctextsuperscript%7B%5Ctextregistered%7D~cubes%20and%20a%20mug.%20These%20results%20establish%20the%20proposed%20pipeline%20as%20an%20efficient%2C%20scalable%20solution%20to%20the%20sim-to-real%20problem.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSim-to-Real%2520Transfer%2520via%2520a%2520Style-Identified%2520Cycle%2520Consistent%2520Generative%2520Adversarial%2520Network%253A%2520Zero-Shot%2520Deployment%2520on%2520Robotic%2520Manipulators%2520through%2520Visual%2520Domain%2520Adaptation%26entry.906535625%3DLuc%25C3%25ADa%2520G%25C3%25BCitta-L%25C3%25B3pez%2520and%2520Lionel%2520G%25C3%25BCitta-L%25C3%25B3pez%2520and%2520Jaime%2520Boal%2520and%2520%25C3%2581lvaro%2520Jes%25C3%25BAs%2520L%25C3%25B3pez-L%25C3%25B3pez%26entry.1292438233%3DThe%2520sample%2520efficiency%2520challenge%2520in%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520compromises%2520its%2520industrial%2520adoption%2520due%2520to%2520the%2520high%2520cost%2520and%2520time%2520demands%2520of%2520real-world%2520training.%2520Virtual%2520environments%2520offer%2520a%2520cost-effective%2520alternative%2520for%2520training%2520DRL%2520agents%252C%2520but%2520the%2520transfer%2520of%2520learned%2520policies%2520to%2520real%2520setups%2520is%2520hindered%2520by%2520the%2520sim-to-real%2520gap.%2520Achieving%2520zero-shot%2520transfer%252C%2520where%2520agents%2520perform%2520directly%2520in%2520real%2520environments%2520without%2520additional%2520tuning%252C%2520is%2520particularly%2520desirable%2520for%2520its%2520efficiency%2520and%2520practical%2520value.%2520This%2520work%2520proposes%2520a%2520novel%2520domain%2520adaptation%2520approach%2520relying%2520on%2520a%2520Style-Identified%2520Cycle%2520Consistent%2520Generative%2520Adversarial%2520Network%2520%2528StyleID-CycleGAN%2520or%2520SICGAN%2529%252C%2520an%2520original%2520Cycle%2520Consistent%2520Generative%2520Adversarial%2520Network%2520%2528CycleGAN%2529%2520based%2520model.%2520SICGAN%2520translates%2520raw%2520virtual%2520observations%2520into%2520real-synthetic%2520images%252C%2520creating%2520a%2520hybrid%2520domain%2520for%2520training%2520DRL%2520agents%2520that%2520combines%2520virtual%2520dynamics%2520with%2520real-like%2520visual%2520inputs.%2520Following%2520virtual%2520training%252C%2520the%2520agent%2520can%2520be%2520directly%2520deployed%252C%2520bypassing%2520the%2520need%2520for%2520real-world%2520training.%2520The%2520pipeline%2520is%2520validated%2520with%2520two%2520distinct%2520industrial%2520robots%2520in%2520the%2520approaching%2520phase%2520of%2520a%2520pick-and-place%2520operation.%2520In%2520virtual%2520environments%2520agents%2520achieve%2520success%2520rates%2520of%252090%2520to%2520100%255C%2525%252C%2520and%2520real-world%2520deployment%2520confirms%2520robust%2520zero-shot%2520transfer%2520%2528i.e.%252C%2520without%2520additional%2520training%2520in%2520the%2520physical%2520environment%2529%2520with%2520accuracies%2520above%252095%255C%2525%2520for%2520most%2520workspace%2520regions.%2520We%2520use%2520augmented%2520reality%2520targets%2520to%2520improve%2520the%2520evaluation%2520process%2520efficiency%252C%2520and%2520experimentally%2520demonstrate%2520that%2520the%2520agent%2520successfully%2520generalizes%2520to%2520real%2520objects%2520of%2520varying%2520colors%2520and%2520shapes%252C%2520including%2520LEGO%255Ctextsuperscript%257B%255Ctextregistered%257D~cubes%2520and%2520a%2520mug.%2520These%2520results%2520establish%2520the%2520proposed%2520pipeline%2520as%2520an%2520efficient%252C%2520scalable%2520solution%2520to%2520the%2520sim-to-real%2520problem.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sim-to-Real%20Transfer%20via%20a%20Style-Identified%20Cycle%20Consistent%20Generative%20Adversarial%20Network%3A%20Zero-Shot%20Deployment%20on%20Robotic%20Manipulators%20through%20Visual%20Domain%20Adaptation&entry.906535625=Luc%C3%ADa%20G%C3%BCitta-L%C3%B3pez%20and%20Lionel%20G%C3%BCitta-L%C3%B3pez%20and%20Jaime%20Boal%20and%20%C3%81lvaro%20Jes%C3%BAs%20L%C3%B3pez-L%C3%B3pez&entry.1292438233=The%20sample%20efficiency%20challenge%20in%20Deep%20Reinforcement%20Learning%20%28DRL%29%20compromises%20its%20industrial%20adoption%20due%20to%20the%20high%20cost%20and%20time%20demands%20of%20real-world%20training.%20Virtual%20environments%20offer%20a%20cost-effective%20alternative%20for%20training%20DRL%20agents%2C%20but%20the%20transfer%20of%20learned%20policies%20to%20real%20setups%20is%20hindered%20by%20the%20sim-to-real%20gap.%20Achieving%20zero-shot%20transfer%2C%20where%20agents%20perform%20directly%20in%20real%20environments%20without%20additional%20tuning%2C%20is%20particularly%20desirable%20for%20its%20efficiency%20and%20practical%20value.%20This%20work%20proposes%20a%20novel%20domain%20adaptation%20approach%20relying%20on%20a%20Style-Identified%20Cycle%20Consistent%20Generative%20Adversarial%20Network%20%28StyleID-CycleGAN%20or%20SICGAN%29%2C%20an%20original%20Cycle%20Consistent%20Generative%20Adversarial%20Network%20%28CycleGAN%29%20based%20model.%20SICGAN%20translates%20raw%20virtual%20observations%20into%20real-synthetic%20images%2C%20creating%20a%20hybrid%20domain%20for%20training%20DRL%20agents%20that%20combines%20virtual%20dynamics%20with%20real-like%20visual%20inputs.%20Following%20virtual%20training%2C%20the%20agent%20can%20be%20directly%20deployed%2C%20bypassing%20the%20need%20for%20real-world%20training.%20The%20pipeline%20is%20validated%20with%20two%20distinct%20industrial%20robots%20in%20the%20approaching%20phase%20of%20a%20pick-and-place%20operation.%20In%20virtual%20environments%20agents%20achieve%20success%20rates%20of%2090%20to%20100%5C%25%2C%20and%20real-world%20deployment%20confirms%20robust%20zero-shot%20transfer%20%28i.e.%2C%20without%20additional%20training%20in%20the%20physical%20environment%29%20with%20accuracies%20above%2095%5C%25%20for%20most%20workspace%20regions.%20We%20use%20augmented%20reality%20targets%20to%20improve%20the%20evaluation%20process%20efficiency%2C%20and%20experimentally%20demonstrate%20that%20the%20agent%20successfully%20generalizes%20to%20real%20objects%20of%20varying%20colors%20and%20shapes%2C%20including%20LEGO%5Ctextsuperscript%7B%5Ctextregistered%7D~cubes%20and%20a%20mug.%20These%20results%20establish%20the%20proposed%20pipeline%20as%20an%20efficient%2C%20scalable%20solution%20to%20the%20sim-to-real%20problem.&entry.1838667208=http%3A//arxiv.org/abs/2601.16677v1&entry.124074799=Read"},
{"title": "Linguistic traces of stochastic empathy in language models", "author": "Bennett Kleinberg and Jari Zegers and Jonas Festor and Stefana Vida and Julian Pr\u00e4sent and Riccardo Loconte and Sanne Peereboom", "abstract": "Differentiating generated and human-written content is increasingly difficult. We examine how an incentive to convey humanness and task characteristics shape this human vs AI race across five studies. In Study 1-2 (n=530 and n=610) humans and a large language model (LLM) wrote relationship advice or relationship descriptions, either with or without instructions to sound human. New participants (n=428 and n=408) judged each text's source. Instructions to sound human were only effective for the LLM, reducing the human advantage. Study 3 (n=360 and n=350) showed that these effects persist when writers were instructed to avoid sounding like an LLM. Study 4 (n=219) tested empathy as mechanism of humanness and concluded that LLMs can produce empathy without humanness and humanness without empathy. Finally, computational text analysis (Study 5) indicated that LLMs become more human-like by applying an implicit representation of humanness to mimic stochastic empathy.", "link": "http://arxiv.org/abs/2410.01675v2", "date": "2026-01-23", "relevancy": 2.2077, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4473}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4473}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.43}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linguistic%20traces%20of%20stochastic%20empathy%20in%20language%20models&body=Title%3A%20Linguistic%20traces%20of%20stochastic%20empathy%20in%20language%20models%0AAuthor%3A%20Bennett%20Kleinberg%20and%20Jari%20Zegers%20and%20Jonas%20Festor%20and%20Stefana%20Vida%20and%20Julian%20Pr%C3%A4sent%20and%20Riccardo%20Loconte%20and%20Sanne%20Peereboom%0AAbstract%3A%20Differentiating%20generated%20and%20human-written%20content%20is%20increasingly%20difficult.%20We%20examine%20how%20an%20incentive%20to%20convey%20humanness%20and%20task%20characteristics%20shape%20this%20human%20vs%20AI%20race%20across%20five%20studies.%20In%20Study%201-2%20%28n%3D530%20and%20n%3D610%29%20humans%20and%20a%20large%20language%20model%20%28LLM%29%20wrote%20relationship%20advice%20or%20relationship%20descriptions%2C%20either%20with%20or%20without%20instructions%20to%20sound%20human.%20New%20participants%20%28n%3D428%20and%20n%3D408%29%20judged%20each%20text%27s%20source.%20Instructions%20to%20sound%20human%20were%20only%20effective%20for%20the%20LLM%2C%20reducing%20the%20human%20advantage.%20Study%203%20%28n%3D360%20and%20n%3D350%29%20showed%20that%20these%20effects%20persist%20when%20writers%20were%20instructed%20to%20avoid%20sounding%20like%20an%20LLM.%20Study%204%20%28n%3D219%29%20tested%20empathy%20as%20mechanism%20of%20humanness%20and%20concluded%20that%20LLMs%20can%20produce%20empathy%20without%20humanness%20and%20humanness%20without%20empathy.%20Finally%2C%20computational%20text%20analysis%20%28Study%205%29%20indicated%20that%20LLMs%20become%20more%20human-like%20by%20applying%20an%20implicit%20representation%20of%20humanness%20to%20mimic%20stochastic%20empathy.%0ALink%3A%20http%3A//arxiv.org/abs/2410.01675v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinguistic%2520traces%2520of%2520stochastic%2520empathy%2520in%2520language%2520models%26entry.906535625%3DBennett%2520Kleinberg%2520and%2520Jari%2520Zegers%2520and%2520Jonas%2520Festor%2520and%2520Stefana%2520Vida%2520and%2520Julian%2520Pr%25C3%25A4sent%2520and%2520Riccardo%2520Loconte%2520and%2520Sanne%2520Peereboom%26entry.1292438233%3DDifferentiating%2520generated%2520and%2520human-written%2520content%2520is%2520increasingly%2520difficult.%2520We%2520examine%2520how%2520an%2520incentive%2520to%2520convey%2520humanness%2520and%2520task%2520characteristics%2520shape%2520this%2520human%2520vs%2520AI%2520race%2520across%2520five%2520studies.%2520In%2520Study%25201-2%2520%2528n%253D530%2520and%2520n%253D610%2529%2520humans%2520and%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520wrote%2520relationship%2520advice%2520or%2520relationship%2520descriptions%252C%2520either%2520with%2520or%2520without%2520instructions%2520to%2520sound%2520human.%2520New%2520participants%2520%2528n%253D428%2520and%2520n%253D408%2529%2520judged%2520each%2520text%2527s%2520source.%2520Instructions%2520to%2520sound%2520human%2520were%2520only%2520effective%2520for%2520the%2520LLM%252C%2520reducing%2520the%2520human%2520advantage.%2520Study%25203%2520%2528n%253D360%2520and%2520n%253D350%2529%2520showed%2520that%2520these%2520effects%2520persist%2520when%2520writers%2520were%2520instructed%2520to%2520avoid%2520sounding%2520like%2520an%2520LLM.%2520Study%25204%2520%2528n%253D219%2529%2520tested%2520empathy%2520as%2520mechanism%2520of%2520humanness%2520and%2520concluded%2520that%2520LLMs%2520can%2520produce%2520empathy%2520without%2520humanness%2520and%2520humanness%2520without%2520empathy.%2520Finally%252C%2520computational%2520text%2520analysis%2520%2528Study%25205%2529%2520indicated%2520that%2520LLMs%2520become%2520more%2520human-like%2520by%2520applying%2520an%2520implicit%2520representation%2520of%2520humanness%2520to%2520mimic%2520stochastic%2520empathy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01675v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linguistic%20traces%20of%20stochastic%20empathy%20in%20language%20models&entry.906535625=Bennett%20Kleinberg%20and%20Jari%20Zegers%20and%20Jonas%20Festor%20and%20Stefana%20Vida%20and%20Julian%20Pr%C3%A4sent%20and%20Riccardo%20Loconte%20and%20Sanne%20Peereboom&entry.1292438233=Differentiating%20generated%20and%20human-written%20content%20is%20increasingly%20difficult.%20We%20examine%20how%20an%20incentive%20to%20convey%20humanness%20and%20task%20characteristics%20shape%20this%20human%20vs%20AI%20race%20across%20five%20studies.%20In%20Study%201-2%20%28n%3D530%20and%20n%3D610%29%20humans%20and%20a%20large%20language%20model%20%28LLM%29%20wrote%20relationship%20advice%20or%20relationship%20descriptions%2C%20either%20with%20or%20without%20instructions%20to%20sound%20human.%20New%20participants%20%28n%3D428%20and%20n%3D408%29%20judged%20each%20text%27s%20source.%20Instructions%20to%20sound%20human%20were%20only%20effective%20for%20the%20LLM%2C%20reducing%20the%20human%20advantage.%20Study%203%20%28n%3D360%20and%20n%3D350%29%20showed%20that%20these%20effects%20persist%20when%20writers%20were%20instructed%20to%20avoid%20sounding%20like%20an%20LLM.%20Study%204%20%28n%3D219%29%20tested%20empathy%20as%20mechanism%20of%20humanness%20and%20concluded%20that%20LLMs%20can%20produce%20empathy%20without%20humanness%20and%20humanness%20without%20empathy.%20Finally%2C%20computational%20text%20analysis%20%28Study%205%29%20indicated%20that%20LLMs%20become%20more%20human-like%20by%20applying%20an%20implicit%20representation%20of%20humanness%20to%20mimic%20stochastic%20empathy.&entry.1838667208=http%3A//arxiv.org/abs/2410.01675v2&entry.124074799=Read"},
{"title": "VMMU: A Vietnamese Multitask Multimodal Understanding and Reasoning Benchmark", "author": "Vy Tuong Dang and An Vo and Emilio Villa-Cueva and Quang Tau and Duc Dm and Thamar Solorio and Daeyoung Kim", "abstract": "We introduce VMMU, a Vietnamese Multitask Multimodal Understanding and Reasoning Benchmark designed to evaluate how vision-language models (VLMs) interpret and reason over visual and textual information beyond English. VMMU consists of 2.5k multimodal questions across 7 tasks, covering a diverse range of problem contexts, including STEM problem solving, data interpretation, rule-governed visual reasoning, and abstract visual reasoning. All questions require genuine multimodal integration, rather than reliance on text-only cues or OCR-based shortcuts. We evaluate a diverse set of state-of-the-art proprietary and open-source VLMs on VMMU. Despite strong Vietnamese OCR performance, proprietary models achieve only 66% mean accuracy. Further analysis shows that the primary source of failure is not OCR, but instead multimodal grounding and reasoning over text and visual evidence. Code and data are available at https://vmmu-bench.github.io/", "link": "http://arxiv.org/abs/2508.13680v4", "date": "2026-01-23", "relevancy": 2.2032, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5543}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5543}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VMMU%3A%20A%20Vietnamese%20Multitask%20Multimodal%20Understanding%20and%20Reasoning%20Benchmark&body=Title%3A%20VMMU%3A%20A%20Vietnamese%20Multitask%20Multimodal%20Understanding%20and%20Reasoning%20Benchmark%0AAuthor%3A%20Vy%20Tuong%20Dang%20and%20An%20Vo%20and%20Emilio%20Villa-Cueva%20and%20Quang%20Tau%20and%20Duc%20Dm%20and%20Thamar%20Solorio%20and%20Daeyoung%20Kim%0AAbstract%3A%20We%20introduce%20VMMU%2C%20a%20Vietnamese%20Multitask%20Multimodal%20Understanding%20and%20Reasoning%20Benchmark%20designed%20to%20evaluate%20how%20vision-language%20models%20%28VLMs%29%20interpret%20and%20reason%20over%20visual%20and%20textual%20information%20beyond%20English.%20VMMU%20consists%20of%202.5k%20multimodal%20questions%20across%207%20tasks%2C%20covering%20a%20diverse%20range%20of%20problem%20contexts%2C%20including%20STEM%20problem%20solving%2C%20data%20interpretation%2C%20rule-governed%20visual%20reasoning%2C%20and%20abstract%20visual%20reasoning.%20All%20questions%20require%20genuine%20multimodal%20integration%2C%20rather%20than%20reliance%20on%20text-only%20cues%20or%20OCR-based%20shortcuts.%20We%20evaluate%20a%20diverse%20set%20of%20state-of-the-art%20proprietary%20and%20open-source%20VLMs%20on%20VMMU.%20Despite%20strong%20Vietnamese%20OCR%20performance%2C%20proprietary%20models%20achieve%20only%2066%25%20mean%20accuracy.%20Further%20analysis%20shows%20that%20the%20primary%20source%20of%20failure%20is%20not%20OCR%2C%20but%20instead%20multimodal%20grounding%20and%20reasoning%20over%20text%20and%20visual%20evidence.%20Code%20and%20data%20are%20available%20at%20https%3A//vmmu-bench.github.io/%0ALink%3A%20http%3A//arxiv.org/abs/2508.13680v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVMMU%253A%2520A%2520Vietnamese%2520Multitask%2520Multimodal%2520Understanding%2520and%2520Reasoning%2520Benchmark%26entry.906535625%3DVy%2520Tuong%2520Dang%2520and%2520An%2520Vo%2520and%2520Emilio%2520Villa-Cueva%2520and%2520Quang%2520Tau%2520and%2520Duc%2520Dm%2520and%2520Thamar%2520Solorio%2520and%2520Daeyoung%2520Kim%26entry.1292438233%3DWe%2520introduce%2520VMMU%252C%2520a%2520Vietnamese%2520Multitask%2520Multimodal%2520Understanding%2520and%2520Reasoning%2520Benchmark%2520designed%2520to%2520evaluate%2520how%2520vision-language%2520models%2520%2528VLMs%2529%2520interpret%2520and%2520reason%2520over%2520visual%2520and%2520textual%2520information%2520beyond%2520English.%2520VMMU%2520consists%2520of%25202.5k%2520multimodal%2520questions%2520across%25207%2520tasks%252C%2520covering%2520a%2520diverse%2520range%2520of%2520problem%2520contexts%252C%2520including%2520STEM%2520problem%2520solving%252C%2520data%2520interpretation%252C%2520rule-governed%2520visual%2520reasoning%252C%2520and%2520abstract%2520visual%2520reasoning.%2520All%2520questions%2520require%2520genuine%2520multimodal%2520integration%252C%2520rather%2520than%2520reliance%2520on%2520text-only%2520cues%2520or%2520OCR-based%2520shortcuts.%2520We%2520evaluate%2520a%2520diverse%2520set%2520of%2520state-of-the-art%2520proprietary%2520and%2520open-source%2520VLMs%2520on%2520VMMU.%2520Despite%2520strong%2520Vietnamese%2520OCR%2520performance%252C%2520proprietary%2520models%2520achieve%2520only%252066%2525%2520mean%2520accuracy.%2520Further%2520analysis%2520shows%2520that%2520the%2520primary%2520source%2520of%2520failure%2520is%2520not%2520OCR%252C%2520but%2520instead%2520multimodal%2520grounding%2520and%2520reasoning%2520over%2520text%2520and%2520visual%2520evidence.%2520Code%2520and%2520data%2520are%2520available%2520at%2520https%253A//vmmu-bench.github.io/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13680v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VMMU%3A%20A%20Vietnamese%20Multitask%20Multimodal%20Understanding%20and%20Reasoning%20Benchmark&entry.906535625=Vy%20Tuong%20Dang%20and%20An%20Vo%20and%20Emilio%20Villa-Cueva%20and%20Quang%20Tau%20and%20Duc%20Dm%20and%20Thamar%20Solorio%20and%20Daeyoung%20Kim&entry.1292438233=We%20introduce%20VMMU%2C%20a%20Vietnamese%20Multitask%20Multimodal%20Understanding%20and%20Reasoning%20Benchmark%20designed%20to%20evaluate%20how%20vision-language%20models%20%28VLMs%29%20interpret%20and%20reason%20over%20visual%20and%20textual%20information%20beyond%20English.%20VMMU%20consists%20of%202.5k%20multimodal%20questions%20across%207%20tasks%2C%20covering%20a%20diverse%20range%20of%20problem%20contexts%2C%20including%20STEM%20problem%20solving%2C%20data%20interpretation%2C%20rule-governed%20visual%20reasoning%2C%20and%20abstract%20visual%20reasoning.%20All%20questions%20require%20genuine%20multimodal%20integration%2C%20rather%20than%20reliance%20on%20text-only%20cues%20or%20OCR-based%20shortcuts.%20We%20evaluate%20a%20diverse%20set%20of%20state-of-the-art%20proprietary%20and%20open-source%20VLMs%20on%20VMMU.%20Despite%20strong%20Vietnamese%20OCR%20performance%2C%20proprietary%20models%20achieve%20only%2066%25%20mean%20accuracy.%20Further%20analysis%20shows%20that%20the%20primary%20source%20of%20failure%20is%20not%20OCR%2C%20but%20instead%20multimodal%20grounding%20and%20reasoning%20over%20text%20and%20visual%20evidence.%20Code%20and%20data%20are%20available%20at%20https%3A//vmmu-bench.github.io/&entry.1838667208=http%3A//arxiv.org/abs/2508.13680v4&entry.124074799=Read"},
{"title": "XR$^3$: An Extended Reality Platform for Social-Physical Human-Robot Interaction", "author": "Chao Wang and Anna Belardinelli and Michael Gienger", "abstract": "Social-physical human-robot interaction (spHRI) is difficult to study: building and programming robots that integrate multiple interaction modalities is costly and slow, while VR-based prototypes often lack physical contact, breaking users' visuo-tactile expectations. We present XR$^3$, a co-located dual-VR-headset platform for HRI research in which an attendee and a hidden operator share the same physical space while experiencing different virtual embodiments. The attendee sees an expressive virtual robot that interacts face-to-face in a shared virtual environment. In real time, the robot's upper-body motion, head and gaze behavior, and facial expressions are mapped from the operator's tracked limbs and face signals. Because the operator is co-present and calibrated in the same coordinate frame, the operator can also touch the attendee, enabling perceived robot touch synchronized with the robot's visible hands. Finger and hand motion is mapped to the robot avatar using inverse kinematics to support precise contact. Beyond motion retargeting, XR$^3$ supports social retargeting of multiple nonverbal cues that can be experimentally varied while keeping physical interaction constant. We detail the system design and calibration, and demonstrate the platform in a touch-based Wizard-of-Oz study, lowering the barrier to prototyping and evaluating embodied, contact-based robot behaviors.", "link": "http://arxiv.org/abs/2601.12395v3", "date": "2026-01-23", "relevancy": 2.1942, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5545}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5466}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XR%24%5E3%24%3A%20An%20Extended%20Reality%20Platform%20for%20Social-Physical%20Human-Robot%20Interaction&body=Title%3A%20XR%24%5E3%24%3A%20An%20Extended%20Reality%20Platform%20for%20Social-Physical%20Human-Robot%20Interaction%0AAuthor%3A%20Chao%20Wang%20and%20Anna%20Belardinelli%20and%20Michael%20Gienger%0AAbstract%3A%20Social-physical%20human-robot%20interaction%20%28spHRI%29%20is%20difficult%20to%20study%3A%20building%20and%20programming%20robots%20that%20integrate%20multiple%20interaction%20modalities%20is%20costly%20and%20slow%2C%20while%20VR-based%20prototypes%20often%20lack%20physical%20contact%2C%20breaking%20users%27%20visuo-tactile%20expectations.%20We%20present%20XR%24%5E3%24%2C%20a%20co-located%20dual-VR-headset%20platform%20for%20HRI%20research%20in%20which%20an%20attendee%20and%20a%20hidden%20operator%20share%20the%20same%20physical%20space%20while%20experiencing%20different%20virtual%20embodiments.%20The%20attendee%20sees%20an%20expressive%20virtual%20robot%20that%20interacts%20face-to-face%20in%20a%20shared%20virtual%20environment.%20In%20real%20time%2C%20the%20robot%27s%20upper-body%20motion%2C%20head%20and%20gaze%20behavior%2C%20and%20facial%20expressions%20are%20mapped%20from%20the%20operator%27s%20tracked%20limbs%20and%20face%20signals.%20Because%20the%20operator%20is%20co-present%20and%20calibrated%20in%20the%20same%20coordinate%20frame%2C%20the%20operator%20can%20also%20touch%20the%20attendee%2C%20enabling%20perceived%20robot%20touch%20synchronized%20with%20the%20robot%27s%20visible%20hands.%20Finger%20and%20hand%20motion%20is%20mapped%20to%20the%20robot%20avatar%20using%20inverse%20kinematics%20to%20support%20precise%20contact.%20Beyond%20motion%20retargeting%2C%20XR%24%5E3%24%20supports%20social%20retargeting%20of%20multiple%20nonverbal%20cues%20that%20can%20be%20experimentally%20varied%20while%20keeping%20physical%20interaction%20constant.%20We%20detail%20the%20system%20design%20and%20calibration%2C%20and%20demonstrate%20the%20platform%20in%20a%20touch-based%20Wizard-of-Oz%20study%2C%20lowering%20the%20barrier%20to%20prototyping%20and%20evaluating%20embodied%2C%20contact-based%20robot%20behaviors.%0ALink%3A%20http%3A//arxiv.org/abs/2601.12395v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXR%2524%255E3%2524%253A%2520An%2520Extended%2520Reality%2520Platform%2520for%2520Social-Physical%2520Human-Robot%2520Interaction%26entry.906535625%3DChao%2520Wang%2520and%2520Anna%2520Belardinelli%2520and%2520Michael%2520Gienger%26entry.1292438233%3DSocial-physical%2520human-robot%2520interaction%2520%2528spHRI%2529%2520is%2520difficult%2520to%2520study%253A%2520building%2520and%2520programming%2520robots%2520that%2520integrate%2520multiple%2520interaction%2520modalities%2520is%2520costly%2520and%2520slow%252C%2520while%2520VR-based%2520prototypes%2520often%2520lack%2520physical%2520contact%252C%2520breaking%2520users%2527%2520visuo-tactile%2520expectations.%2520We%2520present%2520XR%2524%255E3%2524%252C%2520a%2520co-located%2520dual-VR-headset%2520platform%2520for%2520HRI%2520research%2520in%2520which%2520an%2520attendee%2520and%2520a%2520hidden%2520operator%2520share%2520the%2520same%2520physical%2520space%2520while%2520experiencing%2520different%2520virtual%2520embodiments.%2520The%2520attendee%2520sees%2520an%2520expressive%2520virtual%2520robot%2520that%2520interacts%2520face-to-face%2520in%2520a%2520shared%2520virtual%2520environment.%2520In%2520real%2520time%252C%2520the%2520robot%2527s%2520upper-body%2520motion%252C%2520head%2520and%2520gaze%2520behavior%252C%2520and%2520facial%2520expressions%2520are%2520mapped%2520from%2520the%2520operator%2527s%2520tracked%2520limbs%2520and%2520face%2520signals.%2520Because%2520the%2520operator%2520is%2520co-present%2520and%2520calibrated%2520in%2520the%2520same%2520coordinate%2520frame%252C%2520the%2520operator%2520can%2520also%2520touch%2520the%2520attendee%252C%2520enabling%2520perceived%2520robot%2520touch%2520synchronized%2520with%2520the%2520robot%2527s%2520visible%2520hands.%2520Finger%2520and%2520hand%2520motion%2520is%2520mapped%2520to%2520the%2520robot%2520avatar%2520using%2520inverse%2520kinematics%2520to%2520support%2520precise%2520contact.%2520Beyond%2520motion%2520retargeting%252C%2520XR%2524%255E3%2524%2520supports%2520social%2520retargeting%2520of%2520multiple%2520nonverbal%2520cues%2520that%2520can%2520be%2520experimentally%2520varied%2520while%2520keeping%2520physical%2520interaction%2520constant.%2520We%2520detail%2520the%2520system%2520design%2520and%2520calibration%252C%2520and%2520demonstrate%2520the%2520platform%2520in%2520a%2520touch-based%2520Wizard-of-Oz%2520study%252C%2520lowering%2520the%2520barrier%2520to%2520prototyping%2520and%2520evaluating%2520embodied%252C%2520contact-based%2520robot%2520behaviors.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.12395v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XR%24%5E3%24%3A%20An%20Extended%20Reality%20Platform%20for%20Social-Physical%20Human-Robot%20Interaction&entry.906535625=Chao%20Wang%20and%20Anna%20Belardinelli%20and%20Michael%20Gienger&entry.1292438233=Social-physical%20human-robot%20interaction%20%28spHRI%29%20is%20difficult%20to%20study%3A%20building%20and%20programming%20robots%20that%20integrate%20multiple%20interaction%20modalities%20is%20costly%20and%20slow%2C%20while%20VR-based%20prototypes%20often%20lack%20physical%20contact%2C%20breaking%20users%27%20visuo-tactile%20expectations.%20We%20present%20XR%24%5E3%24%2C%20a%20co-located%20dual-VR-headset%20platform%20for%20HRI%20research%20in%20which%20an%20attendee%20and%20a%20hidden%20operator%20share%20the%20same%20physical%20space%20while%20experiencing%20different%20virtual%20embodiments.%20The%20attendee%20sees%20an%20expressive%20virtual%20robot%20that%20interacts%20face-to-face%20in%20a%20shared%20virtual%20environment.%20In%20real%20time%2C%20the%20robot%27s%20upper-body%20motion%2C%20head%20and%20gaze%20behavior%2C%20and%20facial%20expressions%20are%20mapped%20from%20the%20operator%27s%20tracked%20limbs%20and%20face%20signals.%20Because%20the%20operator%20is%20co-present%20and%20calibrated%20in%20the%20same%20coordinate%20frame%2C%20the%20operator%20can%20also%20touch%20the%20attendee%2C%20enabling%20perceived%20robot%20touch%20synchronized%20with%20the%20robot%27s%20visible%20hands.%20Finger%20and%20hand%20motion%20is%20mapped%20to%20the%20robot%20avatar%20using%20inverse%20kinematics%20to%20support%20precise%20contact.%20Beyond%20motion%20retargeting%2C%20XR%24%5E3%24%20supports%20social%20retargeting%20of%20multiple%20nonverbal%20cues%20that%20can%20be%20experimentally%20varied%20while%20keeping%20physical%20interaction%20constant.%20We%20detail%20the%20system%20design%20and%20calibration%2C%20and%20demonstrate%20the%20platform%20in%20a%20touch-based%20Wizard-of-Oz%20study%2C%20lowering%20the%20barrier%20to%20prototyping%20and%20evaluating%20embodied%2C%20contact-based%20robot%20behaviors.&entry.1838667208=http%3A//arxiv.org/abs/2601.12395v3&entry.124074799=Read"},
{"title": "ViSymRe: Vision Multimodal Symbolic Regression", "author": "Da Li and Junping Yin and Jin Xu and Xinxin Li and Juan Zhang", "abstract": "Extracting interpretable equations from observational datasets to describe complex natural phenomena is one of the core goals of artificial intelligence. This field is known as symbolic regression (SR). In recent years, Transformer-based paradigms have become a new trend in SR, addressing the well-known problem of inefficient search. However, the modal heterogeneity between datasets and equations often hinders the convergence and generalization of these models. In this paper, we propose ViSymRe, a Vision Symbolic Regression framework, to explore the positive role of visual modality in enhancing the performance of Transformer-based SR paradigms. To overcome the challenge where the visual SR model is untrainable in high-dimensional scenarios, we present Multi-View Random Slicing (MVRS). By projecting multivariate equations into 2-D space using random affine transformations, MVRS avoids common defects in high-dimensional visualization, such as variable degradation, non-linear interaction missing, and exponentially increasing sampling complexity, enabling ViSymRe to be trained with low computational costs. To support dataset-only deployment of ViSymRe, we design a dual-vision pipeline architecture based on generative techniques, which reconstructs visual features directly from the datasets via an auxiliary Visual Decoder and automatically suppresses the attention weights of reconstruction noise through a proposed Biased Cross-Attention feature fusion module, ensuring that subsequent processes are not affected by noisy modalities. Ablation studies demonstrate the positive contribution of visual modality to improving model convergence level and enhancing various SR metrics. Furthermore, evaluation results on mainstream benchmarks indicate that ViSymRe achieves competitive performance compared to baselines, particularly in low-complexity and rapid-inference scenarios.", "link": "http://arxiv.org/abs/2412.11139v4", "date": "2026-01-23", "relevancy": 2.1917, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5484}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5478}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViSymRe%3A%20Vision%20Multimodal%20Symbolic%20Regression&body=Title%3A%20ViSymRe%3A%20Vision%20Multimodal%20Symbolic%20Regression%0AAuthor%3A%20Da%20Li%20and%20Junping%20Yin%20and%20Jin%20Xu%20and%20Xinxin%20Li%20and%20Juan%20Zhang%0AAbstract%3A%20Extracting%20interpretable%20equations%20from%20observational%20datasets%20to%20describe%20complex%20natural%20phenomena%20is%20one%20of%20the%20core%20goals%20of%20artificial%20intelligence.%20This%20field%20is%20known%20as%20symbolic%20regression%20%28SR%29.%20In%20recent%20years%2C%20Transformer-based%20paradigms%20have%20become%20a%20new%20trend%20in%20SR%2C%20addressing%20the%20well-known%20problem%20of%20inefficient%20search.%20However%2C%20the%20modal%20heterogeneity%20between%20datasets%20and%20equations%20often%20hinders%20the%20convergence%20and%20generalization%20of%20these%20models.%20In%20this%20paper%2C%20we%20propose%20ViSymRe%2C%20a%20Vision%20Symbolic%20Regression%20framework%2C%20to%20explore%20the%20positive%20role%20of%20visual%20modality%20in%20enhancing%20the%20performance%20of%20Transformer-based%20SR%20paradigms.%20To%20overcome%20the%20challenge%20where%20the%20visual%20SR%20model%20is%20untrainable%20in%20high-dimensional%20scenarios%2C%20we%20present%20Multi-View%20Random%20Slicing%20%28MVRS%29.%20By%20projecting%20multivariate%20equations%20into%202-D%20space%20using%20random%20affine%20transformations%2C%20MVRS%20avoids%20common%20defects%20in%20high-dimensional%20visualization%2C%20such%20as%20variable%20degradation%2C%20non-linear%20interaction%20missing%2C%20and%20exponentially%20increasing%20sampling%20complexity%2C%20enabling%20ViSymRe%20to%20be%20trained%20with%20low%20computational%20costs.%20To%20support%20dataset-only%20deployment%20of%20ViSymRe%2C%20we%20design%20a%20dual-vision%20pipeline%20architecture%20based%20on%20generative%20techniques%2C%20which%20reconstructs%20visual%20features%20directly%20from%20the%20datasets%20via%20an%20auxiliary%20Visual%20Decoder%20and%20automatically%20suppresses%20the%20attention%20weights%20of%20reconstruction%20noise%20through%20a%20proposed%20Biased%20Cross-Attention%20feature%20fusion%20module%2C%20ensuring%20that%20subsequent%20processes%20are%20not%20affected%20by%20noisy%20modalities.%20Ablation%20studies%20demonstrate%20the%20positive%20contribution%20of%20visual%20modality%20to%20improving%20model%20convergence%20level%20and%20enhancing%20various%20SR%20metrics.%20Furthermore%2C%20evaluation%20results%20on%20mainstream%20benchmarks%20indicate%20that%20ViSymRe%20achieves%20competitive%20performance%20compared%20to%20baselines%2C%20particularly%20in%20low-complexity%20and%20rapid-inference%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2412.11139v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViSymRe%253A%2520Vision%2520Multimodal%2520Symbolic%2520Regression%26entry.906535625%3DDa%2520Li%2520and%2520Junping%2520Yin%2520and%2520Jin%2520Xu%2520and%2520Xinxin%2520Li%2520and%2520Juan%2520Zhang%26entry.1292438233%3DExtracting%2520interpretable%2520equations%2520from%2520observational%2520datasets%2520to%2520describe%2520complex%2520natural%2520phenomena%2520is%2520one%2520of%2520the%2520core%2520goals%2520of%2520artificial%2520intelligence.%2520This%2520field%2520is%2520known%2520as%2520symbolic%2520regression%2520%2528SR%2529.%2520In%2520recent%2520years%252C%2520Transformer-based%2520paradigms%2520have%2520become%2520a%2520new%2520trend%2520in%2520SR%252C%2520addressing%2520the%2520well-known%2520problem%2520of%2520inefficient%2520search.%2520However%252C%2520the%2520modal%2520heterogeneity%2520between%2520datasets%2520and%2520equations%2520often%2520hinders%2520the%2520convergence%2520and%2520generalization%2520of%2520these%2520models.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ViSymRe%252C%2520a%2520Vision%2520Symbolic%2520Regression%2520framework%252C%2520to%2520explore%2520the%2520positive%2520role%2520of%2520visual%2520modality%2520in%2520enhancing%2520the%2520performance%2520of%2520Transformer-based%2520SR%2520paradigms.%2520To%2520overcome%2520the%2520challenge%2520where%2520the%2520visual%2520SR%2520model%2520is%2520untrainable%2520in%2520high-dimensional%2520scenarios%252C%2520we%2520present%2520Multi-View%2520Random%2520Slicing%2520%2528MVRS%2529.%2520By%2520projecting%2520multivariate%2520equations%2520into%25202-D%2520space%2520using%2520random%2520affine%2520transformations%252C%2520MVRS%2520avoids%2520common%2520defects%2520in%2520high-dimensional%2520visualization%252C%2520such%2520as%2520variable%2520degradation%252C%2520non-linear%2520interaction%2520missing%252C%2520and%2520exponentially%2520increasing%2520sampling%2520complexity%252C%2520enabling%2520ViSymRe%2520to%2520be%2520trained%2520with%2520low%2520computational%2520costs.%2520To%2520support%2520dataset-only%2520deployment%2520of%2520ViSymRe%252C%2520we%2520design%2520a%2520dual-vision%2520pipeline%2520architecture%2520based%2520on%2520generative%2520techniques%252C%2520which%2520reconstructs%2520visual%2520features%2520directly%2520from%2520the%2520datasets%2520via%2520an%2520auxiliary%2520Visual%2520Decoder%2520and%2520automatically%2520suppresses%2520the%2520attention%2520weights%2520of%2520reconstruction%2520noise%2520through%2520a%2520proposed%2520Biased%2520Cross-Attention%2520feature%2520fusion%2520module%252C%2520ensuring%2520that%2520subsequent%2520processes%2520are%2520not%2520affected%2520by%2520noisy%2520modalities.%2520Ablation%2520studies%2520demonstrate%2520the%2520positive%2520contribution%2520of%2520visual%2520modality%2520to%2520improving%2520model%2520convergence%2520level%2520and%2520enhancing%2520various%2520SR%2520metrics.%2520Furthermore%252C%2520evaluation%2520results%2520on%2520mainstream%2520benchmarks%2520indicate%2520that%2520ViSymRe%2520achieves%2520competitive%2520performance%2520compared%2520to%2520baselines%252C%2520particularly%2520in%2520low-complexity%2520and%2520rapid-inference%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11139v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViSymRe%3A%20Vision%20Multimodal%20Symbolic%20Regression&entry.906535625=Da%20Li%20and%20Junping%20Yin%20and%20Jin%20Xu%20and%20Xinxin%20Li%20and%20Juan%20Zhang&entry.1292438233=Extracting%20interpretable%20equations%20from%20observational%20datasets%20to%20describe%20complex%20natural%20phenomena%20is%20one%20of%20the%20core%20goals%20of%20artificial%20intelligence.%20This%20field%20is%20known%20as%20symbolic%20regression%20%28SR%29.%20In%20recent%20years%2C%20Transformer-based%20paradigms%20have%20become%20a%20new%20trend%20in%20SR%2C%20addressing%20the%20well-known%20problem%20of%20inefficient%20search.%20However%2C%20the%20modal%20heterogeneity%20between%20datasets%20and%20equations%20often%20hinders%20the%20convergence%20and%20generalization%20of%20these%20models.%20In%20this%20paper%2C%20we%20propose%20ViSymRe%2C%20a%20Vision%20Symbolic%20Regression%20framework%2C%20to%20explore%20the%20positive%20role%20of%20visual%20modality%20in%20enhancing%20the%20performance%20of%20Transformer-based%20SR%20paradigms.%20To%20overcome%20the%20challenge%20where%20the%20visual%20SR%20model%20is%20untrainable%20in%20high-dimensional%20scenarios%2C%20we%20present%20Multi-View%20Random%20Slicing%20%28MVRS%29.%20By%20projecting%20multivariate%20equations%20into%202-D%20space%20using%20random%20affine%20transformations%2C%20MVRS%20avoids%20common%20defects%20in%20high-dimensional%20visualization%2C%20such%20as%20variable%20degradation%2C%20non-linear%20interaction%20missing%2C%20and%20exponentially%20increasing%20sampling%20complexity%2C%20enabling%20ViSymRe%20to%20be%20trained%20with%20low%20computational%20costs.%20To%20support%20dataset-only%20deployment%20of%20ViSymRe%2C%20we%20design%20a%20dual-vision%20pipeline%20architecture%20based%20on%20generative%20techniques%2C%20which%20reconstructs%20visual%20features%20directly%20from%20the%20datasets%20via%20an%20auxiliary%20Visual%20Decoder%20and%20automatically%20suppresses%20the%20attention%20weights%20of%20reconstruction%20noise%20through%20a%20proposed%20Biased%20Cross-Attention%20feature%20fusion%20module%2C%20ensuring%20that%20subsequent%20processes%20are%20not%20affected%20by%20noisy%20modalities.%20Ablation%20studies%20demonstrate%20the%20positive%20contribution%20of%20visual%20modality%20to%20improving%20model%20convergence%20level%20and%20enhancing%20various%20SR%20metrics.%20Furthermore%2C%20evaluation%20results%20on%20mainstream%20benchmarks%20indicate%20that%20ViSymRe%20achieves%20competitive%20performance%20compared%20to%20baselines%2C%20particularly%20in%20low-complexity%20and%20rapid-inference%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2412.11139v4&entry.124074799=Read"},
{"title": "LongCat-Flash-Thinking-2601 Technical Report", "author": " Meituan LongCat Team and Anchun Gui and Bei Li and Bingyang Tao and Bole Zhou and Borun Chen and Chao Zhang and Chao Zhang and Chen Gao and Chen Zhang and Chengcheng Han and Chenhui Yang and Chuyu Zhang and Cong Chen and Cunguang Wang and Daoru Pan and Defei Bu and Dengchang Zhao and Di Xiu and Dishan Liu and Dongyu Ru and Dunwei Tu and Fan Wu and Fengcheng Yuan and Fengcun Li and Gang Xu and Guanyu Wu and Guoyuan Lin and Haibin Wang and Hansi Yang and Hao Yang and Haonan Yan and Haoxiang Ma and Haoxing Wen and Hongyan Hao and Hongyin Tang and Hongyu Zang and Hongzhi Ni and Hui Su and Jiacheng Zhang and Jiahong Zhou and Jiahuan Li and Jiaming Wang and Jian Yang and Jianfei Zhang and Jianhao Xu and Jianing Wang and Jiapeng Zhu and Jiaqi Sun and Jiarong Shi and Jiarui Zhao and Jingang Wang and Jinluan Yang and Jinrui Ding and Jinwei Xiao and Jiyuan He and Juncan Xu and Kefeng Zhang and Keheng Wang and Li Wei and Lianhui Ma and Lin Qiu and Lingbing Kong and Lingchuan Liu and Linsen Guo and Mengshen Zhu and Mengxia Shen and Mingyang Zhu and Peiguang Li and Peng Pei and Pengcheng Jia and Pengtao Zhang and Peng Zhao and Qi Gu and Qiong Huang and Qiyuan Duan and Quanchi Weng and Rongxiang Weng and Rongzhi Zhang and Rumei Li and Shanglin Lei and Shengnan An and Shijun Dai and Shuaikang Liu and Shuang Zhou and Shuo Wang and Songyuan Zhao and Tao Liang and Tianhao Hu and Tianze Chen and Wei Liu and Wei Shi and Wei Wang and Weifeng Tang and Wenjie Shi and Wenlong Zhu and Wentao Chen and Wentao Shi and Xi Su and Xiangcheng Liu and Xiandi Ma and Xiangyu Xi and Xiangyuan Liu and Xiangzhou Huang and Xiao Liu and Xiaodong Cai and Xiaolong Chen and Xiaowei Shi and Xiaoyu Li and Xin Chen and Xingchen Liu and Xuan Huang and Xuezhi Cao and Xunliang Cai and Yan Chen and Yang Bai and Yang Liu and Yang Yang and Yang Zheng and Yaoming Wang and Yaoming Zhu and Yaqi Huo and Yanyu Chen and Yaorui Shi and Yerui Sun and Yi Zhang and Yihao Chen and Yi-Kai Zhang and Yifan Lu and Yifan Zhao and Yitao Zhai and Yongjing Yin and Yongwei Zhou and Youshao Xiao and Yuchuan Dai and Yuchen Xie and Yuchen Yu and Yufei Zhang and Yuhuai Wei and Yulei Qian and Yunfan Liang and Yunke Zhao and Yuwei Jiang and Yuxin Bian and Yuxin Chen and Yuxin Liu and Yue Xu and Yueqing Sun and Zeyang Yu and Zhao Yang and Zhengsheng Huang and Zhengyu Chen and Zhijian Liu and Zhikang Xia and Zhimin Lin and Zhiyuan Yao and Zhuofan Chen and Zhuowen Han and Zijian Zhang and Ziran Li and Ziwen Wang and Ziyuan Zhuang", "abstract": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "link": "http://arxiv.org/abs/2601.16725v1", "date": "2026-01-23", "relevancy": 2.1879, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5495}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5495}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongCat-Flash-Thinking-2601%20Technical%20Report&body=Title%3A%20LongCat-Flash-Thinking-2601%20Technical%20Report%0AAuthor%3A%20%20Meituan%20LongCat%20Team%20and%20Anchun%20Gui%20and%20Bei%20Li%20and%20Bingyang%20Tao%20and%20Bole%20Zhou%20and%20Borun%20Chen%20and%20Chao%20Zhang%20and%20Chao%20Zhang%20and%20Chen%20Gao%20and%20Chen%20Zhang%20and%20Chengcheng%20Han%20and%20Chenhui%20Yang%20and%20Chuyu%20Zhang%20and%20Cong%20Chen%20and%20Cunguang%20Wang%20and%20Daoru%20Pan%20and%20Defei%20Bu%20and%20Dengchang%20Zhao%20and%20Di%20Xiu%20and%20Dishan%20Liu%20and%20Dongyu%20Ru%20and%20Dunwei%20Tu%20and%20Fan%20Wu%20and%20Fengcheng%20Yuan%20and%20Fengcun%20Li%20and%20Gang%20Xu%20and%20Guanyu%20Wu%20and%20Guoyuan%20Lin%20and%20Haibin%20Wang%20and%20Hansi%20Yang%20and%20Hao%20Yang%20and%20Haonan%20Yan%20and%20Haoxiang%20Ma%20and%20Haoxing%20Wen%20and%20Hongyan%20Hao%20and%20Hongyin%20Tang%20and%20Hongyu%20Zang%20and%20Hongzhi%20Ni%20and%20Hui%20Su%20and%20Jiacheng%20Zhang%20and%20Jiahong%20Zhou%20and%20Jiahuan%20Li%20and%20Jiaming%20Wang%20and%20Jian%20Yang%20and%20Jianfei%20Zhang%20and%20Jianhao%20Xu%20and%20Jianing%20Wang%20and%20Jiapeng%20Zhu%20and%20Jiaqi%20Sun%20and%20Jiarong%20Shi%20and%20Jiarui%20Zhao%20and%20Jingang%20Wang%20and%20Jinluan%20Yang%20and%20Jinrui%20Ding%20and%20Jinwei%20Xiao%20and%20Jiyuan%20He%20and%20Juncan%20Xu%20and%20Kefeng%20Zhang%20and%20Keheng%20Wang%20and%20Li%20Wei%20and%20Lianhui%20Ma%20and%20Lin%20Qiu%20and%20Lingbing%20Kong%20and%20Lingchuan%20Liu%20and%20Linsen%20Guo%20and%20Mengshen%20Zhu%20and%20Mengxia%20Shen%20and%20Mingyang%20Zhu%20and%20Peiguang%20Li%20and%20Peng%20Pei%20and%20Pengcheng%20Jia%20and%20Pengtao%20Zhang%20and%20Peng%20Zhao%20and%20Qi%20Gu%20and%20Qiong%20Huang%20and%20Qiyuan%20Duan%20and%20Quanchi%20Weng%20and%20Rongxiang%20Weng%20and%20Rongzhi%20Zhang%20and%20Rumei%20Li%20and%20Shanglin%20Lei%20and%20Shengnan%20An%20and%20Shijun%20Dai%20and%20Shuaikang%20Liu%20and%20Shuang%20Zhou%20and%20Shuo%20Wang%20and%20Songyuan%20Zhao%20and%20Tao%20Liang%20and%20Tianhao%20Hu%20and%20Tianze%20Chen%20and%20Wei%20Liu%20and%20Wei%20Shi%20and%20Wei%20Wang%20and%20Weifeng%20Tang%20and%20Wenjie%20Shi%20and%20Wenlong%20Zhu%20and%20Wentao%20Chen%20and%20Wentao%20Shi%20and%20Xi%20Su%20and%20Xiangcheng%20Liu%20and%20Xiandi%20Ma%20and%20Xiangyu%20Xi%20and%20Xiangyuan%20Liu%20and%20Xiangzhou%20Huang%20and%20Xiao%20Liu%20and%20Xiaodong%20Cai%20and%20Xiaolong%20Chen%20and%20Xiaowei%20Shi%20and%20Xiaoyu%20Li%20and%20Xin%20Chen%20and%20Xingchen%20Liu%20and%20Xuan%20Huang%20and%20Xuezhi%20Cao%20and%20Xunliang%20Cai%20and%20Yan%20Chen%20and%20Yang%20Bai%20and%20Yang%20Liu%20and%20Yang%20Yang%20and%20Yang%20Zheng%20and%20Yaoming%20Wang%20and%20Yaoming%20Zhu%20and%20Yaqi%20Huo%20and%20Yanyu%20Chen%20and%20Yaorui%20Shi%20and%20Yerui%20Sun%20and%20Yi%20Zhang%20and%20Yihao%20Chen%20and%20Yi-Kai%20Zhang%20and%20Yifan%20Lu%20and%20Yifan%20Zhao%20and%20Yitao%20Zhai%20and%20Yongjing%20Yin%20and%20Yongwei%20Zhou%20and%20Youshao%20Xiao%20and%20Yuchuan%20Dai%20and%20Yuchen%20Xie%20and%20Yuchen%20Yu%20and%20Yufei%20Zhang%20and%20Yuhuai%20Wei%20and%20Yulei%20Qian%20and%20Yunfan%20Liang%20and%20Yunke%20Zhao%20and%20Yuwei%20Jiang%20and%20Yuxin%20Bian%20and%20Yuxin%20Chen%20and%20Yuxin%20Liu%20and%20Yue%20Xu%20and%20Yueqing%20Sun%20and%20Zeyang%20Yu%20and%20Zhao%20Yang%20and%20Zhengsheng%20Huang%20and%20Zhengyu%20Chen%20and%20Zhijian%20Liu%20and%20Zhikang%20Xia%20and%20Zhimin%20Lin%20and%20Zhiyuan%20Yao%20and%20Zhuofan%20Chen%20and%20Zhuowen%20Han%20and%20Zijian%20Zhang%20and%20Ziran%20Li%20and%20Ziwen%20Wang%20and%20Ziyuan%20Zhuang%0AAbstract%3A%20We%20introduce%20LongCat-Flash-Thinking-2601%2C%20a%20560-billion-parameter%20open-source%20Mixture-of-Experts%20%28MoE%29%20reasoning%20model%20with%20superior%20agentic%20reasoning%20capability.%20LongCat-Flash-Thinking-2601%20achieves%20state-of-the-art%20performance%20among%20open-source%20models%20on%20a%20wide%20range%20of%20agentic%20benchmarks%2C%20including%20agentic%20search%2C%20agentic%20tool%20use%2C%20and%20tool-integrated%20reasoning.%20Beyond%20benchmark%20performance%2C%20the%20model%20demonstrates%20strong%20generalization%20to%20complex%20tool%20interactions%20and%20robust%20behavior%20under%20noisy%20real-world%20environments.%20Its%20advanced%20capability%20stems%20from%20a%20unified%20training%20framework%20that%20combines%20domain-parallel%20expert%20training%20with%20subsequent%20fusion%2C%20together%20with%20an%20end-to-end%20co-design%20of%20data%20construction%2C%20environments%2C%20algorithms%2C%20and%20infrastructure%20spanning%20from%20pre-training%20to%20post-training.%20In%20particular%2C%20the%20model%27s%20strong%20generalization%20capability%20in%20complex%20tool-use%20are%20driven%20by%20our%20in-depth%20exploration%20of%20environment%20scaling%20and%20principled%20task%20construction.%20To%20optimize%20long-tailed%2C%20skewed%20generation%20and%20multi-turn%20agentic%20interactions%2C%20and%20to%20enable%20stable%20training%20across%20over%2010%2C000%20environments%20spanning%20more%20than%2020%20domains%2C%20we%20systematically%20extend%20our%20asynchronous%20reinforcement%20learning%20framework%2C%20DORA%2C%20for%20stable%20and%20efficient%20large-scale%20multi-environment%20training.%20Furthermore%2C%20recognizing%20that%20real-world%20tasks%20are%20inherently%20noisy%2C%20we%20conduct%20a%20systematic%20analysis%20and%20decomposition%20of%20real-world%20noise%20patterns%2C%20and%20design%20targeted%20training%20procedures%20to%20explicitly%20incorporate%20such%20imperfections%20into%20the%20training%20process%2C%20resulting%20in%20improved%20robustness%20for%20real-world%20applications.%20To%20further%20enhance%20performance%20on%20complex%20reasoning%20tasks%2C%20we%20introduce%20a%20Heavy%20Thinking%20mode%20that%20enables%20effective%20test-time%20scaling%20by%20jointly%20expanding%20reasoning%20depth%20and%20width%20through%20intensive%20parallel%20thinking.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16725v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongCat-Flash-Thinking-2601%2520Technical%2520Report%26entry.906535625%3D%2520Meituan%2520LongCat%2520Team%2520and%2520Anchun%2520Gui%2520and%2520Bei%2520Li%2520and%2520Bingyang%2520Tao%2520and%2520Bole%2520Zhou%2520and%2520Borun%2520Chen%2520and%2520Chao%2520Zhang%2520and%2520Chao%2520Zhang%2520and%2520Chen%2520Gao%2520and%2520Chen%2520Zhang%2520and%2520Chengcheng%2520Han%2520and%2520Chenhui%2520Yang%2520and%2520Chuyu%2520Zhang%2520and%2520Cong%2520Chen%2520and%2520Cunguang%2520Wang%2520and%2520Daoru%2520Pan%2520and%2520Defei%2520Bu%2520and%2520Dengchang%2520Zhao%2520and%2520Di%2520Xiu%2520and%2520Dishan%2520Liu%2520and%2520Dongyu%2520Ru%2520and%2520Dunwei%2520Tu%2520and%2520Fan%2520Wu%2520and%2520Fengcheng%2520Yuan%2520and%2520Fengcun%2520Li%2520and%2520Gang%2520Xu%2520and%2520Guanyu%2520Wu%2520and%2520Guoyuan%2520Lin%2520and%2520Haibin%2520Wang%2520and%2520Hansi%2520Yang%2520and%2520Hao%2520Yang%2520and%2520Haonan%2520Yan%2520and%2520Haoxiang%2520Ma%2520and%2520Haoxing%2520Wen%2520and%2520Hongyan%2520Hao%2520and%2520Hongyin%2520Tang%2520and%2520Hongyu%2520Zang%2520and%2520Hongzhi%2520Ni%2520and%2520Hui%2520Su%2520and%2520Jiacheng%2520Zhang%2520and%2520Jiahong%2520Zhou%2520and%2520Jiahuan%2520Li%2520and%2520Jiaming%2520Wang%2520and%2520Jian%2520Yang%2520and%2520Jianfei%2520Zhang%2520and%2520Jianhao%2520Xu%2520and%2520Jianing%2520Wang%2520and%2520Jiapeng%2520Zhu%2520and%2520Jiaqi%2520Sun%2520and%2520Jiarong%2520Shi%2520and%2520Jiarui%2520Zhao%2520and%2520Jingang%2520Wang%2520and%2520Jinluan%2520Yang%2520and%2520Jinrui%2520Ding%2520and%2520Jinwei%2520Xiao%2520and%2520Jiyuan%2520He%2520and%2520Juncan%2520Xu%2520and%2520Kefeng%2520Zhang%2520and%2520Keheng%2520Wang%2520and%2520Li%2520Wei%2520and%2520Lianhui%2520Ma%2520and%2520Lin%2520Qiu%2520and%2520Lingbing%2520Kong%2520and%2520Lingchuan%2520Liu%2520and%2520Linsen%2520Guo%2520and%2520Mengshen%2520Zhu%2520and%2520Mengxia%2520Shen%2520and%2520Mingyang%2520Zhu%2520and%2520Peiguang%2520Li%2520and%2520Peng%2520Pei%2520and%2520Pengcheng%2520Jia%2520and%2520Pengtao%2520Zhang%2520and%2520Peng%2520Zhao%2520and%2520Qi%2520Gu%2520and%2520Qiong%2520Huang%2520and%2520Qiyuan%2520Duan%2520and%2520Quanchi%2520Weng%2520and%2520Rongxiang%2520Weng%2520and%2520Rongzhi%2520Zhang%2520and%2520Rumei%2520Li%2520and%2520Shanglin%2520Lei%2520and%2520Shengnan%2520An%2520and%2520Shijun%2520Dai%2520and%2520Shuaikang%2520Liu%2520and%2520Shuang%2520Zhou%2520and%2520Shuo%2520Wang%2520and%2520Songyuan%2520Zhao%2520and%2520Tao%2520Liang%2520and%2520Tianhao%2520Hu%2520and%2520Tianze%2520Chen%2520and%2520Wei%2520Liu%2520and%2520Wei%2520Shi%2520and%2520Wei%2520Wang%2520and%2520Weifeng%2520Tang%2520and%2520Wenjie%2520Shi%2520and%2520Wenlong%2520Zhu%2520and%2520Wentao%2520Chen%2520and%2520Wentao%2520Shi%2520and%2520Xi%2520Su%2520and%2520Xiangcheng%2520Liu%2520and%2520Xiandi%2520Ma%2520and%2520Xiangyu%2520Xi%2520and%2520Xiangyuan%2520Liu%2520and%2520Xiangzhou%2520Huang%2520and%2520Xiao%2520Liu%2520and%2520Xiaodong%2520Cai%2520and%2520Xiaolong%2520Chen%2520and%2520Xiaowei%2520Shi%2520and%2520Xiaoyu%2520Li%2520and%2520Xin%2520Chen%2520and%2520Xingchen%2520Liu%2520and%2520Xuan%2520Huang%2520and%2520Xuezhi%2520Cao%2520and%2520Xunliang%2520Cai%2520and%2520Yan%2520Chen%2520and%2520Yang%2520Bai%2520and%2520Yang%2520Liu%2520and%2520Yang%2520Yang%2520and%2520Yang%2520Zheng%2520and%2520Yaoming%2520Wang%2520and%2520Yaoming%2520Zhu%2520and%2520Yaqi%2520Huo%2520and%2520Yanyu%2520Chen%2520and%2520Yaorui%2520Shi%2520and%2520Yerui%2520Sun%2520and%2520Yi%2520Zhang%2520and%2520Yihao%2520Chen%2520and%2520Yi-Kai%2520Zhang%2520and%2520Yifan%2520Lu%2520and%2520Yifan%2520Zhao%2520and%2520Yitao%2520Zhai%2520and%2520Yongjing%2520Yin%2520and%2520Yongwei%2520Zhou%2520and%2520Youshao%2520Xiao%2520and%2520Yuchuan%2520Dai%2520and%2520Yuchen%2520Xie%2520and%2520Yuchen%2520Yu%2520and%2520Yufei%2520Zhang%2520and%2520Yuhuai%2520Wei%2520and%2520Yulei%2520Qian%2520and%2520Yunfan%2520Liang%2520and%2520Yunke%2520Zhao%2520and%2520Yuwei%2520Jiang%2520and%2520Yuxin%2520Bian%2520and%2520Yuxin%2520Chen%2520and%2520Yuxin%2520Liu%2520and%2520Yue%2520Xu%2520and%2520Yueqing%2520Sun%2520and%2520Zeyang%2520Yu%2520and%2520Zhao%2520Yang%2520and%2520Zhengsheng%2520Huang%2520and%2520Zhengyu%2520Chen%2520and%2520Zhijian%2520Liu%2520and%2520Zhikang%2520Xia%2520and%2520Zhimin%2520Lin%2520and%2520Zhiyuan%2520Yao%2520and%2520Zhuofan%2520Chen%2520and%2520Zhuowen%2520Han%2520and%2520Zijian%2520Zhang%2520and%2520Ziran%2520Li%2520and%2520Ziwen%2520Wang%2520and%2520Ziyuan%2520Zhuang%26entry.1292438233%3DWe%2520introduce%2520LongCat-Flash-Thinking-2601%252C%2520a%2520560-billion-parameter%2520open-source%2520Mixture-of-Experts%2520%2528MoE%2529%2520reasoning%2520model%2520with%2520superior%2520agentic%2520reasoning%2520capability.%2520LongCat-Flash-Thinking-2601%2520achieves%2520state-of-the-art%2520performance%2520among%2520open-source%2520models%2520on%2520a%2520wide%2520range%2520of%2520agentic%2520benchmarks%252C%2520including%2520agentic%2520search%252C%2520agentic%2520tool%2520use%252C%2520and%2520tool-integrated%2520reasoning.%2520Beyond%2520benchmark%2520performance%252C%2520the%2520model%2520demonstrates%2520strong%2520generalization%2520to%2520complex%2520tool%2520interactions%2520and%2520robust%2520behavior%2520under%2520noisy%2520real-world%2520environments.%2520Its%2520advanced%2520capability%2520stems%2520from%2520a%2520unified%2520training%2520framework%2520that%2520combines%2520domain-parallel%2520expert%2520training%2520with%2520subsequent%2520fusion%252C%2520together%2520with%2520an%2520end-to-end%2520co-design%2520of%2520data%2520construction%252C%2520environments%252C%2520algorithms%252C%2520and%2520infrastructure%2520spanning%2520from%2520pre-training%2520to%2520post-training.%2520In%2520particular%252C%2520the%2520model%2527s%2520strong%2520generalization%2520capability%2520in%2520complex%2520tool-use%2520are%2520driven%2520by%2520our%2520in-depth%2520exploration%2520of%2520environment%2520scaling%2520and%2520principled%2520task%2520construction.%2520To%2520optimize%2520long-tailed%252C%2520skewed%2520generation%2520and%2520multi-turn%2520agentic%2520interactions%252C%2520and%2520to%2520enable%2520stable%2520training%2520across%2520over%252010%252C000%2520environments%2520spanning%2520more%2520than%252020%2520domains%252C%2520we%2520systematically%2520extend%2520our%2520asynchronous%2520reinforcement%2520learning%2520framework%252C%2520DORA%252C%2520for%2520stable%2520and%2520efficient%2520large-scale%2520multi-environment%2520training.%2520Furthermore%252C%2520recognizing%2520that%2520real-world%2520tasks%2520are%2520inherently%2520noisy%252C%2520we%2520conduct%2520a%2520systematic%2520analysis%2520and%2520decomposition%2520of%2520real-world%2520noise%2520patterns%252C%2520and%2520design%2520targeted%2520training%2520procedures%2520to%2520explicitly%2520incorporate%2520such%2520imperfections%2520into%2520the%2520training%2520process%252C%2520resulting%2520in%2520improved%2520robustness%2520for%2520real-world%2520applications.%2520To%2520further%2520enhance%2520performance%2520on%2520complex%2520reasoning%2520tasks%252C%2520we%2520introduce%2520a%2520Heavy%2520Thinking%2520mode%2520that%2520enables%2520effective%2520test-time%2520scaling%2520by%2520jointly%2520expanding%2520reasoning%2520depth%2520and%2520width%2520through%2520intensive%2520parallel%2520thinking.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16725v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongCat-Flash-Thinking-2601%20Technical%20Report&entry.906535625=%20Meituan%20LongCat%20Team%20and%20Anchun%20Gui%20and%20Bei%20Li%20and%20Bingyang%20Tao%20and%20Bole%20Zhou%20and%20Borun%20Chen%20and%20Chao%20Zhang%20and%20Chao%20Zhang%20and%20Chen%20Gao%20and%20Chen%20Zhang%20and%20Chengcheng%20Han%20and%20Chenhui%20Yang%20and%20Chuyu%20Zhang%20and%20Cong%20Chen%20and%20Cunguang%20Wang%20and%20Daoru%20Pan%20and%20Defei%20Bu%20and%20Dengchang%20Zhao%20and%20Di%20Xiu%20and%20Dishan%20Liu%20and%20Dongyu%20Ru%20and%20Dunwei%20Tu%20and%20Fan%20Wu%20and%20Fengcheng%20Yuan%20and%20Fengcun%20Li%20and%20Gang%20Xu%20and%20Guanyu%20Wu%20and%20Guoyuan%20Lin%20and%20Haibin%20Wang%20and%20Hansi%20Yang%20and%20Hao%20Yang%20and%20Haonan%20Yan%20and%20Haoxiang%20Ma%20and%20Haoxing%20Wen%20and%20Hongyan%20Hao%20and%20Hongyin%20Tang%20and%20Hongyu%20Zang%20and%20Hongzhi%20Ni%20and%20Hui%20Su%20and%20Jiacheng%20Zhang%20and%20Jiahong%20Zhou%20and%20Jiahuan%20Li%20and%20Jiaming%20Wang%20and%20Jian%20Yang%20and%20Jianfei%20Zhang%20and%20Jianhao%20Xu%20and%20Jianing%20Wang%20and%20Jiapeng%20Zhu%20and%20Jiaqi%20Sun%20and%20Jiarong%20Shi%20and%20Jiarui%20Zhao%20and%20Jingang%20Wang%20and%20Jinluan%20Yang%20and%20Jinrui%20Ding%20and%20Jinwei%20Xiao%20and%20Jiyuan%20He%20and%20Juncan%20Xu%20and%20Kefeng%20Zhang%20and%20Keheng%20Wang%20and%20Li%20Wei%20and%20Lianhui%20Ma%20and%20Lin%20Qiu%20and%20Lingbing%20Kong%20and%20Lingchuan%20Liu%20and%20Linsen%20Guo%20and%20Mengshen%20Zhu%20and%20Mengxia%20Shen%20and%20Mingyang%20Zhu%20and%20Peiguang%20Li%20and%20Peng%20Pei%20and%20Pengcheng%20Jia%20and%20Pengtao%20Zhang%20and%20Peng%20Zhao%20and%20Qi%20Gu%20and%20Qiong%20Huang%20and%20Qiyuan%20Duan%20and%20Quanchi%20Weng%20and%20Rongxiang%20Weng%20and%20Rongzhi%20Zhang%20and%20Rumei%20Li%20and%20Shanglin%20Lei%20and%20Shengnan%20An%20and%20Shijun%20Dai%20and%20Shuaikang%20Liu%20and%20Shuang%20Zhou%20and%20Shuo%20Wang%20and%20Songyuan%20Zhao%20and%20Tao%20Liang%20and%20Tianhao%20Hu%20and%20Tianze%20Chen%20and%20Wei%20Liu%20and%20Wei%20Shi%20and%20Wei%20Wang%20and%20Weifeng%20Tang%20and%20Wenjie%20Shi%20and%20Wenlong%20Zhu%20and%20Wentao%20Chen%20and%20Wentao%20Shi%20and%20Xi%20Su%20and%20Xiangcheng%20Liu%20and%20Xiandi%20Ma%20and%20Xiangyu%20Xi%20and%20Xiangyuan%20Liu%20and%20Xiangzhou%20Huang%20and%20Xiao%20Liu%20and%20Xiaodong%20Cai%20and%20Xiaolong%20Chen%20and%20Xiaowei%20Shi%20and%20Xiaoyu%20Li%20and%20Xin%20Chen%20and%20Xingchen%20Liu%20and%20Xuan%20Huang%20and%20Xuezhi%20Cao%20and%20Xunliang%20Cai%20and%20Yan%20Chen%20and%20Yang%20Bai%20and%20Yang%20Liu%20and%20Yang%20Yang%20and%20Yang%20Zheng%20and%20Yaoming%20Wang%20and%20Yaoming%20Zhu%20and%20Yaqi%20Huo%20and%20Yanyu%20Chen%20and%20Yaorui%20Shi%20and%20Yerui%20Sun%20and%20Yi%20Zhang%20and%20Yihao%20Chen%20and%20Yi-Kai%20Zhang%20and%20Yifan%20Lu%20and%20Yifan%20Zhao%20and%20Yitao%20Zhai%20and%20Yongjing%20Yin%20and%20Yongwei%20Zhou%20and%20Youshao%20Xiao%20and%20Yuchuan%20Dai%20and%20Yuchen%20Xie%20and%20Yuchen%20Yu%20and%20Yufei%20Zhang%20and%20Yuhuai%20Wei%20and%20Yulei%20Qian%20and%20Yunfan%20Liang%20and%20Yunke%20Zhao%20and%20Yuwei%20Jiang%20and%20Yuxin%20Bian%20and%20Yuxin%20Chen%20and%20Yuxin%20Liu%20and%20Yue%20Xu%20and%20Yueqing%20Sun%20and%20Zeyang%20Yu%20and%20Zhao%20Yang%20and%20Zhengsheng%20Huang%20and%20Zhengyu%20Chen%20and%20Zhijian%20Liu%20and%20Zhikang%20Xia%20and%20Zhimin%20Lin%20and%20Zhiyuan%20Yao%20and%20Zhuofan%20Chen%20and%20Zhuowen%20Han%20and%20Zijian%20Zhang%20and%20Ziran%20Li%20and%20Ziwen%20Wang%20and%20Ziyuan%20Zhuang&entry.1292438233=We%20introduce%20LongCat-Flash-Thinking-2601%2C%20a%20560-billion-parameter%20open-source%20Mixture-of-Experts%20%28MoE%29%20reasoning%20model%20with%20superior%20agentic%20reasoning%20capability.%20LongCat-Flash-Thinking-2601%20achieves%20state-of-the-art%20performance%20among%20open-source%20models%20on%20a%20wide%20range%20of%20agentic%20benchmarks%2C%20including%20agentic%20search%2C%20agentic%20tool%20use%2C%20and%20tool-integrated%20reasoning.%20Beyond%20benchmark%20performance%2C%20the%20model%20demonstrates%20strong%20generalization%20to%20complex%20tool%20interactions%20and%20robust%20behavior%20under%20noisy%20real-world%20environments.%20Its%20advanced%20capability%20stems%20from%20a%20unified%20training%20framework%20that%20combines%20domain-parallel%20expert%20training%20with%20subsequent%20fusion%2C%20together%20with%20an%20end-to-end%20co-design%20of%20data%20construction%2C%20environments%2C%20algorithms%2C%20and%20infrastructure%20spanning%20from%20pre-training%20to%20post-training.%20In%20particular%2C%20the%20model%27s%20strong%20generalization%20capability%20in%20complex%20tool-use%20are%20driven%20by%20our%20in-depth%20exploration%20of%20environment%20scaling%20and%20principled%20task%20construction.%20To%20optimize%20long-tailed%2C%20skewed%20generation%20and%20multi-turn%20agentic%20interactions%2C%20and%20to%20enable%20stable%20training%20across%20over%2010%2C000%20environments%20spanning%20more%20than%2020%20domains%2C%20we%20systematically%20extend%20our%20asynchronous%20reinforcement%20learning%20framework%2C%20DORA%2C%20for%20stable%20and%20efficient%20large-scale%20multi-environment%20training.%20Furthermore%2C%20recognizing%20that%20real-world%20tasks%20are%20inherently%20noisy%2C%20we%20conduct%20a%20systematic%20analysis%20and%20decomposition%20of%20real-world%20noise%20patterns%2C%20and%20design%20targeted%20training%20procedures%20to%20explicitly%20incorporate%20such%20imperfections%20into%20the%20training%20process%2C%20resulting%20in%20improved%20robustness%20for%20real-world%20applications.%20To%20further%20enhance%20performance%20on%20complex%20reasoning%20tasks%2C%20we%20introduce%20a%20Heavy%20Thinking%20mode%20that%20enables%20effective%20test-time%20scaling%20by%20jointly%20expanding%20reasoning%20depth%20and%20width%20through%20intensive%20parallel%20thinking.&entry.1838667208=http%3A//arxiv.org/abs/2601.16725v1&entry.124074799=Read"},
{"title": "VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs", "author": "Wensi Huang and Shaohao Zhu and Meng Wei and Jinming Xu and Xihui Liu and Hanqing Wang and Tai Wang and Feng Zhao and Jiangmiao Pang", "abstract": "In most existing embodied navigation tasks, instructions are well-defined and unambiguous, such as instruction following and object searching. Under this idealized setting, agents are required solely to produce effective navigation outputs conditioned on vision and language inputs. However, real-world navigation instructions are often vague and ambiguous, requiring the agent to resolve uncertainty and infer user intent through active dialog. To address this gap, we propose Interactive Instance Goal Navigation (IIGN), a task that requires agents not only to generate navigation actions but also to produce language outputs via active dialog, thereby aligning more closely with practical settings. IIGN extends Instance Goal Navigation (IGN) by allowing agents to freely consult an oracle in natural language while navigating. Building on this task, we present the Vision Language-Language Navigation (VL-LN) benchmark, which provides a large-scale, automatically generated dataset and a comprehensive evaluation protocol for training and assessing dialog-enabled navigation models. VL-LN comprises over 41k long-horizon dialog-augmented trajectories for training and an automatic evaluation protocol with an oracle capable of responding to agent queries. Using this benchmark, we train a navigation model equipped with dialog capabilities and show that it achieves significant improvements over the baselines. Extensive experiments and analyses further demonstrate the effectiveness and reliability of VL-LN for advancing research on dialog-enabled embodied navigation. Code and dataset: https://0309hws.github.io/VL-LN.github.io/", "link": "http://arxiv.org/abs/2512.22342v4", "date": "2026-01-23", "relevancy": 2.166, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5522}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5434}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.53}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VL-LN%20Bench%3A%20Towards%20Long-horizon%20Goal-oriented%20Navigation%20with%20Active%20Dialogs&body=Title%3A%20VL-LN%20Bench%3A%20Towards%20Long-horizon%20Goal-oriented%20Navigation%20with%20Active%20Dialogs%0AAuthor%3A%20Wensi%20Huang%20and%20Shaohao%20Zhu%20and%20Meng%20Wei%20and%20Jinming%20Xu%20and%20Xihui%20Liu%20and%20Hanqing%20Wang%20and%20Tai%20Wang%20and%20Feng%20Zhao%20and%20Jiangmiao%20Pang%0AAbstract%3A%20In%20most%20existing%20embodied%20navigation%20tasks%2C%20instructions%20are%20well-defined%20and%20unambiguous%2C%20such%20as%20instruction%20following%20and%20object%20searching.%20Under%20this%20idealized%20setting%2C%20agents%20are%20required%20solely%20to%20produce%20effective%20navigation%20outputs%20conditioned%20on%20vision%20and%20language%20inputs.%20However%2C%20real-world%20navigation%20instructions%20are%20often%20vague%20and%20ambiguous%2C%20requiring%20the%20agent%20to%20resolve%20uncertainty%20and%20infer%20user%20intent%20through%20active%20dialog.%20To%20address%20this%20gap%2C%20we%20propose%20Interactive%20Instance%20Goal%20Navigation%20%28IIGN%29%2C%20a%20task%20that%20requires%20agents%20not%20only%20to%20generate%20navigation%20actions%20but%20also%20to%20produce%20language%20outputs%20via%20active%20dialog%2C%20thereby%20aligning%20more%20closely%20with%20practical%20settings.%20IIGN%20extends%20Instance%20Goal%20Navigation%20%28IGN%29%20by%20allowing%20agents%20to%20freely%20consult%20an%20oracle%20in%20natural%20language%20while%20navigating.%20Building%20on%20this%20task%2C%20we%20present%20the%20Vision%20Language-Language%20Navigation%20%28VL-LN%29%20benchmark%2C%20which%20provides%20a%20large-scale%2C%20automatically%20generated%20dataset%20and%20a%20comprehensive%20evaluation%20protocol%20for%20training%20and%20assessing%20dialog-enabled%20navigation%20models.%20VL-LN%20comprises%20over%2041k%20long-horizon%20dialog-augmented%20trajectories%20for%20training%20and%20an%20automatic%20evaluation%20protocol%20with%20an%20oracle%20capable%20of%20responding%20to%20agent%20queries.%20Using%20this%20benchmark%2C%20we%20train%20a%20navigation%20model%20equipped%20with%20dialog%20capabilities%20and%20show%20that%20it%20achieves%20significant%20improvements%20over%20the%20baselines.%20Extensive%20experiments%20and%20analyses%20further%20demonstrate%20the%20effectiveness%20and%20reliability%20of%20VL-LN%20for%20advancing%20research%20on%20dialog-enabled%20embodied%20navigation.%20Code%20and%20dataset%3A%20https%3A//0309hws.github.io/VL-LN.github.io/%0ALink%3A%20http%3A//arxiv.org/abs/2512.22342v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVL-LN%2520Bench%253A%2520Towards%2520Long-horizon%2520Goal-oriented%2520Navigation%2520with%2520Active%2520Dialogs%26entry.906535625%3DWensi%2520Huang%2520and%2520Shaohao%2520Zhu%2520and%2520Meng%2520Wei%2520and%2520Jinming%2520Xu%2520and%2520Xihui%2520Liu%2520and%2520Hanqing%2520Wang%2520and%2520Tai%2520Wang%2520and%2520Feng%2520Zhao%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3DIn%2520most%2520existing%2520embodied%2520navigation%2520tasks%252C%2520instructions%2520are%2520well-defined%2520and%2520unambiguous%252C%2520such%2520as%2520instruction%2520following%2520and%2520object%2520searching.%2520Under%2520this%2520idealized%2520setting%252C%2520agents%2520are%2520required%2520solely%2520to%2520produce%2520effective%2520navigation%2520outputs%2520conditioned%2520on%2520vision%2520and%2520language%2520inputs.%2520However%252C%2520real-world%2520navigation%2520instructions%2520are%2520often%2520vague%2520and%2520ambiguous%252C%2520requiring%2520the%2520agent%2520to%2520resolve%2520uncertainty%2520and%2520infer%2520user%2520intent%2520through%2520active%2520dialog.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520Interactive%2520Instance%2520Goal%2520Navigation%2520%2528IIGN%2529%252C%2520a%2520task%2520that%2520requires%2520agents%2520not%2520only%2520to%2520generate%2520navigation%2520actions%2520but%2520also%2520to%2520produce%2520language%2520outputs%2520via%2520active%2520dialog%252C%2520thereby%2520aligning%2520more%2520closely%2520with%2520practical%2520settings.%2520IIGN%2520extends%2520Instance%2520Goal%2520Navigation%2520%2528IGN%2529%2520by%2520allowing%2520agents%2520to%2520freely%2520consult%2520an%2520oracle%2520in%2520natural%2520language%2520while%2520navigating.%2520Building%2520on%2520this%2520task%252C%2520we%2520present%2520the%2520Vision%2520Language-Language%2520Navigation%2520%2528VL-LN%2529%2520benchmark%252C%2520which%2520provides%2520a%2520large-scale%252C%2520automatically%2520generated%2520dataset%2520and%2520a%2520comprehensive%2520evaluation%2520protocol%2520for%2520training%2520and%2520assessing%2520dialog-enabled%2520navigation%2520models.%2520VL-LN%2520comprises%2520over%252041k%2520long-horizon%2520dialog-augmented%2520trajectories%2520for%2520training%2520and%2520an%2520automatic%2520evaluation%2520protocol%2520with%2520an%2520oracle%2520capable%2520of%2520responding%2520to%2520agent%2520queries.%2520Using%2520this%2520benchmark%252C%2520we%2520train%2520a%2520navigation%2520model%2520equipped%2520with%2520dialog%2520capabilities%2520and%2520show%2520that%2520it%2520achieves%2520significant%2520improvements%2520over%2520the%2520baselines.%2520Extensive%2520experiments%2520and%2520analyses%2520further%2520demonstrate%2520the%2520effectiveness%2520and%2520reliability%2520of%2520VL-LN%2520for%2520advancing%2520research%2520on%2520dialog-enabled%2520embodied%2520navigation.%2520Code%2520and%2520dataset%253A%2520https%253A//0309hws.github.io/VL-LN.github.io/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22342v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VL-LN%20Bench%3A%20Towards%20Long-horizon%20Goal-oriented%20Navigation%20with%20Active%20Dialogs&entry.906535625=Wensi%20Huang%20and%20Shaohao%20Zhu%20and%20Meng%20Wei%20and%20Jinming%20Xu%20and%20Xihui%20Liu%20and%20Hanqing%20Wang%20and%20Tai%20Wang%20and%20Feng%20Zhao%20and%20Jiangmiao%20Pang&entry.1292438233=In%20most%20existing%20embodied%20navigation%20tasks%2C%20instructions%20are%20well-defined%20and%20unambiguous%2C%20such%20as%20instruction%20following%20and%20object%20searching.%20Under%20this%20idealized%20setting%2C%20agents%20are%20required%20solely%20to%20produce%20effective%20navigation%20outputs%20conditioned%20on%20vision%20and%20language%20inputs.%20However%2C%20real-world%20navigation%20instructions%20are%20often%20vague%20and%20ambiguous%2C%20requiring%20the%20agent%20to%20resolve%20uncertainty%20and%20infer%20user%20intent%20through%20active%20dialog.%20To%20address%20this%20gap%2C%20we%20propose%20Interactive%20Instance%20Goal%20Navigation%20%28IIGN%29%2C%20a%20task%20that%20requires%20agents%20not%20only%20to%20generate%20navigation%20actions%20but%20also%20to%20produce%20language%20outputs%20via%20active%20dialog%2C%20thereby%20aligning%20more%20closely%20with%20practical%20settings.%20IIGN%20extends%20Instance%20Goal%20Navigation%20%28IGN%29%20by%20allowing%20agents%20to%20freely%20consult%20an%20oracle%20in%20natural%20language%20while%20navigating.%20Building%20on%20this%20task%2C%20we%20present%20the%20Vision%20Language-Language%20Navigation%20%28VL-LN%29%20benchmark%2C%20which%20provides%20a%20large-scale%2C%20automatically%20generated%20dataset%20and%20a%20comprehensive%20evaluation%20protocol%20for%20training%20and%20assessing%20dialog-enabled%20navigation%20models.%20VL-LN%20comprises%20over%2041k%20long-horizon%20dialog-augmented%20trajectories%20for%20training%20and%20an%20automatic%20evaluation%20protocol%20with%20an%20oracle%20capable%20of%20responding%20to%20agent%20queries.%20Using%20this%20benchmark%2C%20we%20train%20a%20navigation%20model%20equipped%20with%20dialog%20capabilities%20and%20show%20that%20it%20achieves%20significant%20improvements%20over%20the%20baselines.%20Extensive%20experiments%20and%20analyses%20further%20demonstrate%20the%20effectiveness%20and%20reliability%20of%20VL-LN%20for%20advancing%20research%20on%20dialog-enabled%20embodied%20navigation.%20Code%20and%20dataset%3A%20https%3A//0309hws.github.io/VL-LN.github.io/&entry.1838667208=http%3A//arxiv.org/abs/2512.22342v4&entry.124074799=Read"},
{"title": "Self-supervised Learning of Echocardiographic Video Representations via Online Cluster Distillation", "author": "Divyanshu Mishra and Mohammadreza Salehi and Pramit Saha and Olga Patey and Aris T. Papageorghiou and Yuki M. Asano and J. Alison Noble", "abstract": "Self-supervised learning (SSL) has achieved major advances in natural images and video understanding, but challenges remain in domains like echocardiography (heart ultrasound) due to subtle anatomical structures, complex temporal dynamics, and the current lack of domain-specific pre-trained models. Existing SSL approaches such as contrastive, masked modeling, and clustering-based methods struggle with high intersample similarity, sensitivity to low PSNR inputs common in ultrasound, or aggressive augmentations that distort clinically relevant features. We present DISCOVR (Distilled Image Supervision for Cross Modal Video Representation), a self-supervised dual branch framework for cardiac ultrasound video representation learning. DISCOVR combines a clustering-based video encoder that models temporal dynamics with an online image encoder that extracts fine-grained spatial semantics. These branches are connected through a semantic cluster distillation loss that transfers anatomical knowledge from the evolving image encoder to the video encoder, enabling temporally coherent representations enriched with fine-grained semantic understanding.Evaluated on six echocardiography datasets spanning fetal, pediatric, and adult populations, DISCOVR outperforms both specialized video anomaly detection methods and state-of-the-art video-SSL baselines in zero-shot and linear probing setups,achieving superior segmentation transfer and strong downstream performance on clinically relevant tasks such as LVEF prediction. Code available at: https://github.com/mdivyanshu97/DISCOVR", "link": "http://arxiv.org/abs/2506.11777v3", "date": "2026-01-23", "relevancy": 2.1565, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5437}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5418}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Learning%20of%20Echocardiographic%20Video%20Representations%20via%20Online%20Cluster%20Distillation&body=Title%3A%20Self-supervised%20Learning%20of%20Echocardiographic%20Video%20Representations%20via%20Online%20Cluster%20Distillation%0AAuthor%3A%20Divyanshu%20Mishra%20and%20Mohammadreza%20Salehi%20and%20Pramit%20Saha%20and%20Olga%20Patey%20and%20Aris%20T.%20Papageorghiou%20and%20Yuki%20M.%20Asano%20and%20J.%20Alison%20Noble%0AAbstract%3A%20Self-supervised%20learning%20%28SSL%29%20has%20achieved%20major%20advances%20in%20natural%20images%20and%20video%20understanding%2C%20but%20challenges%20remain%20in%20domains%20like%20echocardiography%20%28heart%20ultrasound%29%20due%20to%20subtle%20anatomical%20structures%2C%20complex%20temporal%20dynamics%2C%20and%20the%20current%20lack%20of%20domain-specific%20pre-trained%20models.%20Existing%20SSL%20approaches%20such%20as%20contrastive%2C%20masked%20modeling%2C%20and%20clustering-based%20methods%20struggle%20with%20high%20intersample%20similarity%2C%20sensitivity%20to%20low%20PSNR%20inputs%20common%20in%20ultrasound%2C%20or%20aggressive%20augmentations%20that%20distort%20clinically%20relevant%20features.%20We%20present%20DISCOVR%20%28Distilled%20Image%20Supervision%20for%20Cross%20Modal%20Video%20Representation%29%2C%20a%20self-supervised%20dual%20branch%20framework%20for%20cardiac%20ultrasound%20video%20representation%20learning.%20DISCOVR%20combines%20a%20clustering-based%20video%20encoder%20that%20models%20temporal%20dynamics%20with%20an%20online%20image%20encoder%20that%20extracts%20fine-grained%20spatial%20semantics.%20These%20branches%20are%20connected%20through%20a%20semantic%20cluster%20distillation%20loss%20that%20transfers%20anatomical%20knowledge%20from%20the%20evolving%20image%20encoder%20to%20the%20video%20encoder%2C%20enabling%20temporally%20coherent%20representations%20enriched%20with%20fine-grained%20semantic%20understanding.Evaluated%20on%20six%20echocardiography%20datasets%20spanning%20fetal%2C%20pediatric%2C%20and%20adult%20populations%2C%20DISCOVR%20outperforms%20both%20specialized%20video%20anomaly%20detection%20methods%20and%20state-of-the-art%20video-SSL%20baselines%20in%20zero-shot%20and%20linear%20probing%20setups%2Cachieving%20superior%20segmentation%20transfer%20and%20strong%20downstream%20performance%20on%20clinically%20relevant%20tasks%20such%20as%20LVEF%20prediction.%20Code%20available%20at%3A%20https%3A//github.com/mdivyanshu97/DISCOVR%0ALink%3A%20http%3A//arxiv.org/abs/2506.11777v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520Learning%2520of%2520Echocardiographic%2520Video%2520Representations%2520via%2520Online%2520Cluster%2520Distillation%26entry.906535625%3DDivyanshu%2520Mishra%2520and%2520Mohammadreza%2520Salehi%2520and%2520Pramit%2520Saha%2520and%2520Olga%2520Patey%2520and%2520Aris%2520T.%2520Papageorghiou%2520and%2520Yuki%2520M.%2520Asano%2520and%2520J.%2520Alison%2520Noble%26entry.1292438233%3DSelf-supervised%2520learning%2520%2528SSL%2529%2520has%2520achieved%2520major%2520advances%2520in%2520natural%2520images%2520and%2520video%2520understanding%252C%2520but%2520challenges%2520remain%2520in%2520domains%2520like%2520echocardiography%2520%2528heart%2520ultrasound%2529%2520due%2520to%2520subtle%2520anatomical%2520structures%252C%2520complex%2520temporal%2520dynamics%252C%2520and%2520the%2520current%2520lack%2520of%2520domain-specific%2520pre-trained%2520models.%2520Existing%2520SSL%2520approaches%2520such%2520as%2520contrastive%252C%2520masked%2520modeling%252C%2520and%2520clustering-based%2520methods%2520struggle%2520with%2520high%2520intersample%2520similarity%252C%2520sensitivity%2520to%2520low%2520PSNR%2520inputs%2520common%2520in%2520ultrasound%252C%2520or%2520aggressive%2520augmentations%2520that%2520distort%2520clinically%2520relevant%2520features.%2520We%2520present%2520DISCOVR%2520%2528Distilled%2520Image%2520Supervision%2520for%2520Cross%2520Modal%2520Video%2520Representation%2529%252C%2520a%2520self-supervised%2520dual%2520branch%2520framework%2520for%2520cardiac%2520ultrasound%2520video%2520representation%2520learning.%2520DISCOVR%2520combines%2520a%2520clustering-based%2520video%2520encoder%2520that%2520models%2520temporal%2520dynamics%2520with%2520an%2520online%2520image%2520encoder%2520that%2520extracts%2520fine-grained%2520spatial%2520semantics.%2520These%2520branches%2520are%2520connected%2520through%2520a%2520semantic%2520cluster%2520distillation%2520loss%2520that%2520transfers%2520anatomical%2520knowledge%2520from%2520the%2520evolving%2520image%2520encoder%2520to%2520the%2520video%2520encoder%252C%2520enabling%2520temporally%2520coherent%2520representations%2520enriched%2520with%2520fine-grained%2520semantic%2520understanding.Evaluated%2520on%2520six%2520echocardiography%2520datasets%2520spanning%2520fetal%252C%2520pediatric%252C%2520and%2520adult%2520populations%252C%2520DISCOVR%2520outperforms%2520both%2520specialized%2520video%2520anomaly%2520detection%2520methods%2520and%2520state-of-the-art%2520video-SSL%2520baselines%2520in%2520zero-shot%2520and%2520linear%2520probing%2520setups%252Cachieving%2520superior%2520segmentation%2520transfer%2520and%2520strong%2520downstream%2520performance%2520on%2520clinically%2520relevant%2520tasks%2520such%2520as%2520LVEF%2520prediction.%2520Code%2520available%2520at%253A%2520https%253A//github.com/mdivyanshu97/DISCOVR%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11777v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Learning%20of%20Echocardiographic%20Video%20Representations%20via%20Online%20Cluster%20Distillation&entry.906535625=Divyanshu%20Mishra%20and%20Mohammadreza%20Salehi%20and%20Pramit%20Saha%20and%20Olga%20Patey%20and%20Aris%20T.%20Papageorghiou%20and%20Yuki%20M.%20Asano%20and%20J.%20Alison%20Noble&entry.1292438233=Self-supervised%20learning%20%28SSL%29%20has%20achieved%20major%20advances%20in%20natural%20images%20and%20video%20understanding%2C%20but%20challenges%20remain%20in%20domains%20like%20echocardiography%20%28heart%20ultrasound%29%20due%20to%20subtle%20anatomical%20structures%2C%20complex%20temporal%20dynamics%2C%20and%20the%20current%20lack%20of%20domain-specific%20pre-trained%20models.%20Existing%20SSL%20approaches%20such%20as%20contrastive%2C%20masked%20modeling%2C%20and%20clustering-based%20methods%20struggle%20with%20high%20intersample%20similarity%2C%20sensitivity%20to%20low%20PSNR%20inputs%20common%20in%20ultrasound%2C%20or%20aggressive%20augmentations%20that%20distort%20clinically%20relevant%20features.%20We%20present%20DISCOVR%20%28Distilled%20Image%20Supervision%20for%20Cross%20Modal%20Video%20Representation%29%2C%20a%20self-supervised%20dual%20branch%20framework%20for%20cardiac%20ultrasound%20video%20representation%20learning.%20DISCOVR%20combines%20a%20clustering-based%20video%20encoder%20that%20models%20temporal%20dynamics%20with%20an%20online%20image%20encoder%20that%20extracts%20fine-grained%20spatial%20semantics.%20These%20branches%20are%20connected%20through%20a%20semantic%20cluster%20distillation%20loss%20that%20transfers%20anatomical%20knowledge%20from%20the%20evolving%20image%20encoder%20to%20the%20video%20encoder%2C%20enabling%20temporally%20coherent%20representations%20enriched%20with%20fine-grained%20semantic%20understanding.Evaluated%20on%20six%20echocardiography%20datasets%20spanning%20fetal%2C%20pediatric%2C%20and%20adult%20populations%2C%20DISCOVR%20outperforms%20both%20specialized%20video%20anomaly%20detection%20methods%20and%20state-of-the-art%20video-SSL%20baselines%20in%20zero-shot%20and%20linear%20probing%20setups%2Cachieving%20superior%20segmentation%20transfer%20and%20strong%20downstream%20performance%20on%20clinically%20relevant%20tasks%20such%20as%20LVEF%20prediction.%20Code%20available%20at%3A%20https%3A//github.com/mdivyanshu97/DISCOVR&entry.1838667208=http%3A//arxiv.org/abs/2506.11777v3&entry.124074799=Read"},
{"title": "Multi-scale Graph Autoregressive Modeling: Molecular Property Prediction via Next Token Prediction", "author": "Zhuoyang Jiang and Yaosen Min and Peiran Jin and Lei Chen", "abstract": "We present Connection-Aware Motif Sequencing (CamS), a graph-to-sequence representation that enables decoder-only Transformers to learn molecular graphs via standard next-token prediction (NTP). For molecular property prediction, SMILES-based NTP scales well but lacks explicit topology, whereas graph-native masked modeling captures connectivity but risks disrupting the pivotal chemical details (e.g., activity cliffs). CamS bridges this gap by serializing molecular graphs into structure-rich causal sequences. CamS first mines data-driven connection-aware motifs. It then serializes motifs via scaffold-rooted breadth-first search (BFS) to establish a stable core-to-periphery order. Crucially, CamS enables hierarchical modeling by concatenating sequences from fine to coarse motif scales, allowing the model to condition global scaffolds on dense, uncorrupted local structural evidence. We instantiate CamS-LLaMA by pre-training a vanilla LLaMA backbone on CamS sequences. It achieves state-of-the-art performance on MoleculeNet and the activity-cliff benchmark MoleculeACE, outperforming both SMILES-based language models and strong graph baselines. Interpretability analysis confirms that our multi-scale causal serialization effectively drives attention toward cliff-determining differences.", "link": "http://arxiv.org/abs/2601.02530v3", "date": "2026-01-23", "relevancy": 2.1533, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5701}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5657}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-scale%20Graph%20Autoregressive%20Modeling%3A%20Molecular%20Property%20Prediction%20via%20Next%20Token%20Prediction&body=Title%3A%20Multi-scale%20Graph%20Autoregressive%20Modeling%3A%20Molecular%20Property%20Prediction%20via%20Next%20Token%20Prediction%0AAuthor%3A%20Zhuoyang%20Jiang%20and%20Yaosen%20Min%20and%20Peiran%20Jin%20and%20Lei%20Chen%0AAbstract%3A%20We%20present%20Connection-Aware%20Motif%20Sequencing%20%28CamS%29%2C%20a%20graph-to-sequence%20representation%20that%20enables%20decoder-only%20Transformers%20to%20learn%20molecular%20graphs%20via%20standard%20next-token%20prediction%20%28NTP%29.%20For%20molecular%20property%20prediction%2C%20SMILES-based%20NTP%20scales%20well%20but%20lacks%20explicit%20topology%2C%20whereas%20graph-native%20masked%20modeling%20captures%20connectivity%20but%20risks%20disrupting%20the%20pivotal%20chemical%20details%20%28e.g.%2C%20activity%20cliffs%29.%20CamS%20bridges%20this%20gap%20by%20serializing%20molecular%20graphs%20into%20structure-rich%20causal%20sequences.%20CamS%20first%20mines%20data-driven%20connection-aware%20motifs.%20It%20then%20serializes%20motifs%20via%20scaffold-rooted%20breadth-first%20search%20%28BFS%29%20to%20establish%20a%20stable%20core-to-periphery%20order.%20Crucially%2C%20CamS%20enables%20hierarchical%20modeling%20by%20concatenating%20sequences%20from%20fine%20to%20coarse%20motif%20scales%2C%20allowing%20the%20model%20to%20condition%20global%20scaffolds%20on%20dense%2C%20uncorrupted%20local%20structural%20evidence.%20We%20instantiate%20CamS-LLaMA%20by%20pre-training%20a%20vanilla%20LLaMA%20backbone%20on%20CamS%20sequences.%20It%20achieves%20state-of-the-art%20performance%20on%20MoleculeNet%20and%20the%20activity-cliff%20benchmark%20MoleculeACE%2C%20outperforming%20both%20SMILES-based%20language%20models%20and%20strong%20graph%20baselines.%20Interpretability%20analysis%20confirms%20that%20our%20multi-scale%20causal%20serialization%20effectively%20drives%20attention%20toward%20cliff-determining%20differences.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02530v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-scale%2520Graph%2520Autoregressive%2520Modeling%253A%2520Molecular%2520Property%2520Prediction%2520via%2520Next%2520Token%2520Prediction%26entry.906535625%3DZhuoyang%2520Jiang%2520and%2520Yaosen%2520Min%2520and%2520Peiran%2520Jin%2520and%2520Lei%2520Chen%26entry.1292438233%3DWe%2520present%2520Connection-Aware%2520Motif%2520Sequencing%2520%2528CamS%2529%252C%2520a%2520graph-to-sequence%2520representation%2520that%2520enables%2520decoder-only%2520Transformers%2520to%2520learn%2520molecular%2520graphs%2520via%2520standard%2520next-token%2520prediction%2520%2528NTP%2529.%2520For%2520molecular%2520property%2520prediction%252C%2520SMILES-based%2520NTP%2520scales%2520well%2520but%2520lacks%2520explicit%2520topology%252C%2520whereas%2520graph-native%2520masked%2520modeling%2520captures%2520connectivity%2520but%2520risks%2520disrupting%2520the%2520pivotal%2520chemical%2520details%2520%2528e.g.%252C%2520activity%2520cliffs%2529.%2520CamS%2520bridges%2520this%2520gap%2520by%2520serializing%2520molecular%2520graphs%2520into%2520structure-rich%2520causal%2520sequences.%2520CamS%2520first%2520mines%2520data-driven%2520connection-aware%2520motifs.%2520It%2520then%2520serializes%2520motifs%2520via%2520scaffold-rooted%2520breadth-first%2520search%2520%2528BFS%2529%2520to%2520establish%2520a%2520stable%2520core-to-periphery%2520order.%2520Crucially%252C%2520CamS%2520enables%2520hierarchical%2520modeling%2520by%2520concatenating%2520sequences%2520from%2520fine%2520to%2520coarse%2520motif%2520scales%252C%2520allowing%2520the%2520model%2520to%2520condition%2520global%2520scaffolds%2520on%2520dense%252C%2520uncorrupted%2520local%2520structural%2520evidence.%2520We%2520instantiate%2520CamS-LLaMA%2520by%2520pre-training%2520a%2520vanilla%2520LLaMA%2520backbone%2520on%2520CamS%2520sequences.%2520It%2520achieves%2520state-of-the-art%2520performance%2520on%2520MoleculeNet%2520and%2520the%2520activity-cliff%2520benchmark%2520MoleculeACE%252C%2520outperforming%2520both%2520SMILES-based%2520language%2520models%2520and%2520strong%2520graph%2520baselines.%2520Interpretability%2520analysis%2520confirms%2520that%2520our%2520multi-scale%2520causal%2520serialization%2520effectively%2520drives%2520attention%2520toward%2520cliff-determining%2520differences.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02530v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-scale%20Graph%20Autoregressive%20Modeling%3A%20Molecular%20Property%20Prediction%20via%20Next%20Token%20Prediction&entry.906535625=Zhuoyang%20Jiang%20and%20Yaosen%20Min%20and%20Peiran%20Jin%20and%20Lei%20Chen&entry.1292438233=We%20present%20Connection-Aware%20Motif%20Sequencing%20%28CamS%29%2C%20a%20graph-to-sequence%20representation%20that%20enables%20decoder-only%20Transformers%20to%20learn%20molecular%20graphs%20via%20standard%20next-token%20prediction%20%28NTP%29.%20For%20molecular%20property%20prediction%2C%20SMILES-based%20NTP%20scales%20well%20but%20lacks%20explicit%20topology%2C%20whereas%20graph-native%20masked%20modeling%20captures%20connectivity%20but%20risks%20disrupting%20the%20pivotal%20chemical%20details%20%28e.g.%2C%20activity%20cliffs%29.%20CamS%20bridges%20this%20gap%20by%20serializing%20molecular%20graphs%20into%20structure-rich%20causal%20sequences.%20CamS%20first%20mines%20data-driven%20connection-aware%20motifs.%20It%20then%20serializes%20motifs%20via%20scaffold-rooted%20breadth-first%20search%20%28BFS%29%20to%20establish%20a%20stable%20core-to-periphery%20order.%20Crucially%2C%20CamS%20enables%20hierarchical%20modeling%20by%20concatenating%20sequences%20from%20fine%20to%20coarse%20motif%20scales%2C%20allowing%20the%20model%20to%20condition%20global%20scaffolds%20on%20dense%2C%20uncorrupted%20local%20structural%20evidence.%20We%20instantiate%20CamS-LLaMA%20by%20pre-training%20a%20vanilla%20LLaMA%20backbone%20on%20CamS%20sequences.%20It%20achieves%20state-of-the-art%20performance%20on%20MoleculeNet%20and%20the%20activity-cliff%20benchmark%20MoleculeACE%2C%20outperforming%20both%20SMILES-based%20language%20models%20and%20strong%20graph%20baselines.%20Interpretability%20analysis%20confirms%20that%20our%20multi-scale%20causal%20serialization%20effectively%20drives%20attention%20toward%20cliff-determining%20differences.&entry.1838667208=http%3A//arxiv.org/abs/2601.02530v3&entry.124074799=Read"},
{"title": "LLM Reasoning for Cold-Start Item Recommendation", "author": "Shijun Li and Yu Wang and Jin Wang and Ying Li and Joydeep Ghosh and Anne Cocos", "abstract": "Large Language Models (LLMs) have shown significant potential for improving recommendation systems through their inherent reasoning capabilities and extensive knowledge base. Yet, existing studies predominantly address warm-start scenarios with abundant user-item interaction data, leaving the more challenging cold-start scenarios, where sparse interactions hinder traditional collaborative filtering methods, underexplored. To address this limitation, we propose novel reasoning strategies designed for cold-start item recommendations within the Netflix domain. Our method utilizes the advanced reasoning capabilities of LLMs to effectively infer user preferences, particularly for newly introduced or rarely interacted items. We systematically evaluate supervised fine-tuning, reinforcement learning-based fine-tuning, and hybrid approaches that combine both methods to optimize recommendation performance. Extensive experiments on real-world data demonstrate significant improvements in both methodological efficacy and practical performance in cold-start recommendation contexts. Remarkably, our reasoning-based fine-tuned models outperform Netflix's production ranking model by up to 8% in certain cases.", "link": "http://arxiv.org/abs/2511.18261v3", "date": "2026-01-23", "relevancy": 2.1473, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4345}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4345}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20Reasoning%20for%20Cold-Start%20Item%20Recommendation&body=Title%3A%20LLM%20Reasoning%20for%20Cold-Start%20Item%20Recommendation%0AAuthor%3A%20Shijun%20Li%20and%20Yu%20Wang%20and%20Jin%20Wang%20and%20Ying%20Li%20and%20Joydeep%20Ghosh%20and%20Anne%20Cocos%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20significant%20potential%20for%20improving%20recommendation%20systems%20through%20their%20inherent%20reasoning%20capabilities%20and%20extensive%20knowledge%20base.%20Yet%2C%20existing%20studies%20predominantly%20address%20warm-start%20scenarios%20with%20abundant%20user-item%20interaction%20data%2C%20leaving%20the%20more%20challenging%20cold-start%20scenarios%2C%20where%20sparse%20interactions%20hinder%20traditional%20collaborative%20filtering%20methods%2C%20underexplored.%20To%20address%20this%20limitation%2C%20we%20propose%20novel%20reasoning%20strategies%20designed%20for%20cold-start%20item%20recommendations%20within%20the%20Netflix%20domain.%20Our%20method%20utilizes%20the%20advanced%20reasoning%20capabilities%20of%20LLMs%20to%20effectively%20infer%20user%20preferences%2C%20particularly%20for%20newly%20introduced%20or%20rarely%20interacted%20items.%20We%20systematically%20evaluate%20supervised%20fine-tuning%2C%20reinforcement%20learning-based%20fine-tuning%2C%20and%20hybrid%20approaches%20that%20combine%20both%20methods%20to%20optimize%20recommendation%20performance.%20Extensive%20experiments%20on%20real-world%20data%20demonstrate%20significant%20improvements%20in%20both%20methodological%20efficacy%20and%20practical%20performance%20in%20cold-start%20recommendation%20contexts.%20Remarkably%2C%20our%20reasoning-based%20fine-tuned%20models%20outperform%20Netflix%27s%20production%20ranking%20model%20by%20up%20to%208%25%20in%20certain%20cases.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18261v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520Reasoning%2520for%2520Cold-Start%2520Item%2520Recommendation%26entry.906535625%3DShijun%2520Li%2520and%2520Yu%2520Wang%2520and%2520Jin%2520Wang%2520and%2520Ying%2520Li%2520and%2520Joydeep%2520Ghosh%2520and%2520Anne%2520Cocos%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520significant%2520potential%2520for%2520improving%2520recommendation%2520systems%2520through%2520their%2520inherent%2520reasoning%2520capabilities%2520and%2520extensive%2520knowledge%2520base.%2520Yet%252C%2520existing%2520studies%2520predominantly%2520address%2520warm-start%2520scenarios%2520with%2520abundant%2520user-item%2520interaction%2520data%252C%2520leaving%2520the%2520more%2520challenging%2520cold-start%2520scenarios%252C%2520where%2520sparse%2520interactions%2520hinder%2520traditional%2520collaborative%2520filtering%2520methods%252C%2520underexplored.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520novel%2520reasoning%2520strategies%2520designed%2520for%2520cold-start%2520item%2520recommendations%2520within%2520the%2520Netflix%2520domain.%2520Our%2520method%2520utilizes%2520the%2520advanced%2520reasoning%2520capabilities%2520of%2520LLMs%2520to%2520effectively%2520infer%2520user%2520preferences%252C%2520particularly%2520for%2520newly%2520introduced%2520or%2520rarely%2520interacted%2520items.%2520We%2520systematically%2520evaluate%2520supervised%2520fine-tuning%252C%2520reinforcement%2520learning-based%2520fine-tuning%252C%2520and%2520hybrid%2520approaches%2520that%2520combine%2520both%2520methods%2520to%2520optimize%2520recommendation%2520performance.%2520Extensive%2520experiments%2520on%2520real-world%2520data%2520demonstrate%2520significant%2520improvements%2520in%2520both%2520methodological%2520efficacy%2520and%2520practical%2520performance%2520in%2520cold-start%2520recommendation%2520contexts.%2520Remarkably%252C%2520our%2520reasoning-based%2520fine-tuned%2520models%2520outperform%2520Netflix%2527s%2520production%2520ranking%2520model%2520by%2520up%2520to%25208%2525%2520in%2520certain%2520cases.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18261v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20Reasoning%20for%20Cold-Start%20Item%20Recommendation&entry.906535625=Shijun%20Li%20and%20Yu%20Wang%20and%20Jin%20Wang%20and%20Ying%20Li%20and%20Joydeep%20Ghosh%20and%20Anne%20Cocos&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20shown%20significant%20potential%20for%20improving%20recommendation%20systems%20through%20their%20inherent%20reasoning%20capabilities%20and%20extensive%20knowledge%20base.%20Yet%2C%20existing%20studies%20predominantly%20address%20warm-start%20scenarios%20with%20abundant%20user-item%20interaction%20data%2C%20leaving%20the%20more%20challenging%20cold-start%20scenarios%2C%20where%20sparse%20interactions%20hinder%20traditional%20collaborative%20filtering%20methods%2C%20underexplored.%20To%20address%20this%20limitation%2C%20we%20propose%20novel%20reasoning%20strategies%20designed%20for%20cold-start%20item%20recommendations%20within%20the%20Netflix%20domain.%20Our%20method%20utilizes%20the%20advanced%20reasoning%20capabilities%20of%20LLMs%20to%20effectively%20infer%20user%20preferences%2C%20particularly%20for%20newly%20introduced%20or%20rarely%20interacted%20items.%20We%20systematically%20evaluate%20supervised%20fine-tuning%2C%20reinforcement%20learning-based%20fine-tuning%2C%20and%20hybrid%20approaches%20that%20combine%20both%20methods%20to%20optimize%20recommendation%20performance.%20Extensive%20experiments%20on%20real-world%20data%20demonstrate%20significant%20improvements%20in%20both%20methodological%20efficacy%20and%20practical%20performance%20in%20cold-start%20recommendation%20contexts.%20Remarkably%2C%20our%20reasoning-based%20fine-tuned%20models%20outperform%20Netflix%27s%20production%20ranking%20model%20by%20up%20to%208%25%20in%20certain%20cases.&entry.1838667208=http%3A//arxiv.org/abs/2511.18261v3&entry.124074799=Read"},
{"title": "On Fine-Grained I/O Complexity of Attention Backward Passes", "author": "Xiaoyu Li and Yingyu Liang and Zhenmei Shi and Zhao Song and Song Yue and Jiahao Zhang", "abstract": "Large Language Models (LLMs) exhibit exceptional proficiency in handling extensive context windows in natural language. Nevertheless, the quadratic scaling of attention computation relative to sequence length creates substantial efficiency bottlenecks, necessitating the development of I/O-optimized algorithms. In this work, we conduct a systematic examination of the I/O complexity inherent in attention mechanisms, with a specific emphasis on the backward pass under both small and large cache settings. By leveraging the red-blue pebble game framework, we derive tight bounds for I/O complexity across the full spectrum of cache sizes. We validate that FlashAttention, one of the current industry standards, achieves optimality in the large-cache scenario for both forward and backward passes. Conversely, for small-cache environments, we introduce a novel algorithm that outperforms contemporary methods and successfully attains theoretical tight bounds. Furthermore, we expand our investigation to include sparse attention by establishing granular lower bounds for both forward and backward passes across all cache configurations. Ultimately, our results solidify the theoretical framework regarding I/O complexity in attention mechanisms, providing critical guidance for the development of efficient LLM training and inference systems.", "link": "http://arxiv.org/abs/2410.09397v2", "date": "2026-01-23", "relevancy": 2.1275, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5648}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5127}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Fine-Grained%20I/O%20Complexity%20of%20Attention%20Backward%20Passes&body=Title%3A%20On%20Fine-Grained%20I/O%20Complexity%20of%20Attention%20Backward%20Passes%0AAuthor%3A%20Xiaoyu%20Li%20and%20Yingyu%20Liang%20and%20Zhenmei%20Shi%20and%20Zhao%20Song%20and%20Song%20Yue%20and%20Jiahao%20Zhang%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20exceptional%20proficiency%20in%20handling%20extensive%20context%20windows%20in%20natural%20language.%20Nevertheless%2C%20the%20quadratic%20scaling%20of%20attention%20computation%20relative%20to%20sequence%20length%20creates%20substantial%20efficiency%20bottlenecks%2C%20necessitating%20the%20development%20of%20I/O-optimized%20algorithms.%20In%20this%20work%2C%20we%20conduct%20a%20systematic%20examination%20of%20the%20I/O%20complexity%20inherent%20in%20attention%20mechanisms%2C%20with%20a%20specific%20emphasis%20on%20the%20backward%20pass%20under%20both%20small%20and%20large%20cache%20settings.%20By%20leveraging%20the%20red-blue%20pebble%20game%20framework%2C%20we%20derive%20tight%20bounds%20for%20I/O%20complexity%20across%20the%20full%20spectrum%20of%20cache%20sizes.%20We%20validate%20that%20FlashAttention%2C%20one%20of%20the%20current%20industry%20standards%2C%20achieves%20optimality%20in%20the%20large-cache%20scenario%20for%20both%20forward%20and%20backward%20passes.%20Conversely%2C%20for%20small-cache%20environments%2C%20we%20introduce%20a%20novel%20algorithm%20that%20outperforms%20contemporary%20methods%20and%20successfully%20attains%20theoretical%20tight%20bounds.%20Furthermore%2C%20we%20expand%20our%20investigation%20to%20include%20sparse%20attention%20by%20establishing%20granular%20lower%20bounds%20for%20both%20forward%20and%20backward%20passes%20across%20all%20cache%20configurations.%20Ultimately%2C%20our%20results%20solidify%20the%20theoretical%20framework%20regarding%20I/O%20complexity%20in%20attention%20mechanisms%2C%20providing%20critical%20guidance%20for%20the%20development%20of%20efficient%20LLM%20training%20and%20inference%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2410.09397v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Fine-Grained%2520I/O%2520Complexity%2520of%2520Attention%2520Backward%2520Passes%26entry.906535625%3DXiaoyu%2520Li%2520and%2520Yingyu%2520Liang%2520and%2520Zhenmei%2520Shi%2520and%2520Zhao%2520Song%2520and%2520Song%2520Yue%2520and%2520Jiahao%2520Zhang%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520exhibit%2520exceptional%2520proficiency%2520in%2520handling%2520extensive%2520context%2520windows%2520in%2520natural%2520language.%2520Nevertheless%252C%2520the%2520quadratic%2520scaling%2520of%2520attention%2520computation%2520relative%2520to%2520sequence%2520length%2520creates%2520substantial%2520efficiency%2520bottlenecks%252C%2520necessitating%2520the%2520development%2520of%2520I/O-optimized%2520algorithms.%2520In%2520this%2520work%252C%2520we%2520conduct%2520a%2520systematic%2520examination%2520of%2520the%2520I/O%2520complexity%2520inherent%2520in%2520attention%2520mechanisms%252C%2520with%2520a%2520specific%2520emphasis%2520on%2520the%2520backward%2520pass%2520under%2520both%2520small%2520and%2520large%2520cache%2520settings.%2520By%2520leveraging%2520the%2520red-blue%2520pebble%2520game%2520framework%252C%2520we%2520derive%2520tight%2520bounds%2520for%2520I/O%2520complexity%2520across%2520the%2520full%2520spectrum%2520of%2520cache%2520sizes.%2520We%2520validate%2520that%2520FlashAttention%252C%2520one%2520of%2520the%2520current%2520industry%2520standards%252C%2520achieves%2520optimality%2520in%2520the%2520large-cache%2520scenario%2520for%2520both%2520forward%2520and%2520backward%2520passes.%2520Conversely%252C%2520for%2520small-cache%2520environments%252C%2520we%2520introduce%2520a%2520novel%2520algorithm%2520that%2520outperforms%2520contemporary%2520methods%2520and%2520successfully%2520attains%2520theoretical%2520tight%2520bounds.%2520Furthermore%252C%2520we%2520expand%2520our%2520investigation%2520to%2520include%2520sparse%2520attention%2520by%2520establishing%2520granular%2520lower%2520bounds%2520for%2520both%2520forward%2520and%2520backward%2520passes%2520across%2520all%2520cache%2520configurations.%2520Ultimately%252C%2520our%2520results%2520solidify%2520the%2520theoretical%2520framework%2520regarding%2520I/O%2520complexity%2520in%2520attention%2520mechanisms%252C%2520providing%2520critical%2520guidance%2520for%2520the%2520development%2520of%2520efficient%2520LLM%2520training%2520and%2520inference%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09397v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Fine-Grained%20I/O%20Complexity%20of%20Attention%20Backward%20Passes&entry.906535625=Xiaoyu%20Li%20and%20Yingyu%20Liang%20and%20Zhenmei%20Shi%20and%20Zhao%20Song%20and%20Song%20Yue%20and%20Jiahao%20Zhang&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20exhibit%20exceptional%20proficiency%20in%20handling%20extensive%20context%20windows%20in%20natural%20language.%20Nevertheless%2C%20the%20quadratic%20scaling%20of%20attention%20computation%20relative%20to%20sequence%20length%20creates%20substantial%20efficiency%20bottlenecks%2C%20necessitating%20the%20development%20of%20I/O-optimized%20algorithms.%20In%20this%20work%2C%20we%20conduct%20a%20systematic%20examination%20of%20the%20I/O%20complexity%20inherent%20in%20attention%20mechanisms%2C%20with%20a%20specific%20emphasis%20on%20the%20backward%20pass%20under%20both%20small%20and%20large%20cache%20settings.%20By%20leveraging%20the%20red-blue%20pebble%20game%20framework%2C%20we%20derive%20tight%20bounds%20for%20I/O%20complexity%20across%20the%20full%20spectrum%20of%20cache%20sizes.%20We%20validate%20that%20FlashAttention%2C%20one%20of%20the%20current%20industry%20standards%2C%20achieves%20optimality%20in%20the%20large-cache%20scenario%20for%20both%20forward%20and%20backward%20passes.%20Conversely%2C%20for%20small-cache%20environments%2C%20we%20introduce%20a%20novel%20algorithm%20that%20outperforms%20contemporary%20methods%20and%20successfully%20attains%20theoretical%20tight%20bounds.%20Furthermore%2C%20we%20expand%20our%20investigation%20to%20include%20sparse%20attention%20by%20establishing%20granular%20lower%20bounds%20for%20both%20forward%20and%20backward%20passes%20across%20all%20cache%20configurations.%20Ultimately%2C%20our%20results%20solidify%20the%20theoretical%20framework%20regarding%20I/O%20complexity%20in%20attention%20mechanisms%2C%20providing%20critical%20guidance%20for%20the%20development%20of%20efficient%20LLM%20training%20and%20inference%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2410.09397v2&entry.124074799=Read"},
{"title": "Trapped in the past? Disentangling fluid and crystallized intelligence of large language models using chess", "author": "Leonard S. Pleiss and Maximilian Schiffer and Robert K. von Weizs\u00e4cker", "abstract": "Large Language Models (LLMs) exhibit remarkable capabilities, yet it remains unclear to what extent these reflect sophisticated recall (crystallized intelligence) or reasoning ability (fluid intelligence). We introduce chess as a controlled testbed for disentangling these faculties. Leveraging the game's structure and scalable engine evaluations, we construct a taxonomy of positions varying in training corpus proximity--ranging from common states solvable by memorization to novel ones requiring first-principles reasoning. We systematically evaluate multiple GPT generations under varying reasoning intensities. Our analysis reveals a clear gradient: performance consistently degrades as fluid intelligence demands increase. Notably, in out-of-distribution tasks, performance collapses to random levels. While newer models improve, progress slows significantly for tasks outside the training distribution. Furthermore, while reasoning-augmented inference improves performance, its marginal benefit per token decreases with distributional proximity. These results suggest current architectures remain limited in systematic generalization, highlighting the need for mechanisms beyond scale to achieve robust fluid intelligence.", "link": "http://arxiv.org/abs/2601.16823v1", "date": "2026-01-23", "relevancy": 2.0983, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5347}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5347}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trapped%20in%20the%20past%3F%20Disentangling%20fluid%20and%20crystallized%20intelligence%20of%20large%20language%20models%20using%20chess&body=Title%3A%20Trapped%20in%20the%20past%3F%20Disentangling%20fluid%20and%20crystallized%20intelligence%20of%20large%20language%20models%20using%20chess%0AAuthor%3A%20Leonard%20S.%20Pleiss%20and%20Maximilian%20Schiffer%20and%20Robert%20K.%20von%20Weizs%C3%A4cker%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20remarkable%20capabilities%2C%20yet%20it%20remains%20unclear%20to%20what%20extent%20these%20reflect%20sophisticated%20recall%20%28crystallized%20intelligence%29%20or%20reasoning%20ability%20%28fluid%20intelligence%29.%20We%20introduce%20chess%20as%20a%20controlled%20testbed%20for%20disentangling%20these%20faculties.%20Leveraging%20the%20game%27s%20structure%20and%20scalable%20engine%20evaluations%2C%20we%20construct%20a%20taxonomy%20of%20positions%20varying%20in%20training%20corpus%20proximity--ranging%20from%20common%20states%20solvable%20by%20memorization%20to%20novel%20ones%20requiring%20first-principles%20reasoning.%20We%20systematically%20evaluate%20multiple%20GPT%20generations%20under%20varying%20reasoning%20intensities.%20Our%20analysis%20reveals%20a%20clear%20gradient%3A%20performance%20consistently%20degrades%20as%20fluid%20intelligence%20demands%20increase.%20Notably%2C%20in%20out-of-distribution%20tasks%2C%20performance%20collapses%20to%20random%20levels.%20While%20newer%20models%20improve%2C%20progress%20slows%20significantly%20for%20tasks%20outside%20the%20training%20distribution.%20Furthermore%2C%20while%20reasoning-augmented%20inference%20improves%20performance%2C%20its%20marginal%20benefit%20per%20token%20decreases%20with%20distributional%20proximity.%20These%20results%20suggest%20current%20architectures%20remain%20limited%20in%20systematic%20generalization%2C%20highlighting%20the%20need%20for%20mechanisms%20beyond%20scale%20to%20achieve%20robust%20fluid%20intelligence.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16823v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrapped%2520in%2520the%2520past%253F%2520Disentangling%2520fluid%2520and%2520crystallized%2520intelligence%2520of%2520large%2520language%2520models%2520using%2520chess%26entry.906535625%3DLeonard%2520S.%2520Pleiss%2520and%2520Maximilian%2520Schiffer%2520and%2520Robert%2520K.%2520von%2520Weizs%25C3%25A4cker%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520exhibit%2520remarkable%2520capabilities%252C%2520yet%2520it%2520remains%2520unclear%2520to%2520what%2520extent%2520these%2520reflect%2520sophisticated%2520recall%2520%2528crystallized%2520intelligence%2529%2520or%2520reasoning%2520ability%2520%2528fluid%2520intelligence%2529.%2520We%2520introduce%2520chess%2520as%2520a%2520controlled%2520testbed%2520for%2520disentangling%2520these%2520faculties.%2520Leveraging%2520the%2520game%2527s%2520structure%2520and%2520scalable%2520engine%2520evaluations%252C%2520we%2520construct%2520a%2520taxonomy%2520of%2520positions%2520varying%2520in%2520training%2520corpus%2520proximity--ranging%2520from%2520common%2520states%2520solvable%2520by%2520memorization%2520to%2520novel%2520ones%2520requiring%2520first-principles%2520reasoning.%2520We%2520systematically%2520evaluate%2520multiple%2520GPT%2520generations%2520under%2520varying%2520reasoning%2520intensities.%2520Our%2520analysis%2520reveals%2520a%2520clear%2520gradient%253A%2520performance%2520consistently%2520degrades%2520as%2520fluid%2520intelligence%2520demands%2520increase.%2520Notably%252C%2520in%2520out-of-distribution%2520tasks%252C%2520performance%2520collapses%2520to%2520random%2520levels.%2520While%2520newer%2520models%2520improve%252C%2520progress%2520slows%2520significantly%2520for%2520tasks%2520outside%2520the%2520training%2520distribution.%2520Furthermore%252C%2520while%2520reasoning-augmented%2520inference%2520improves%2520performance%252C%2520its%2520marginal%2520benefit%2520per%2520token%2520decreases%2520with%2520distributional%2520proximity.%2520These%2520results%2520suggest%2520current%2520architectures%2520remain%2520limited%2520in%2520systematic%2520generalization%252C%2520highlighting%2520the%2520need%2520for%2520mechanisms%2520beyond%2520scale%2520to%2520achieve%2520robust%2520fluid%2520intelligence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16823v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trapped%20in%20the%20past%3F%20Disentangling%20fluid%20and%20crystallized%20intelligence%20of%20large%20language%20models%20using%20chess&entry.906535625=Leonard%20S.%20Pleiss%20and%20Maximilian%20Schiffer%20and%20Robert%20K.%20von%20Weizs%C3%A4cker&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20exhibit%20remarkable%20capabilities%2C%20yet%20it%20remains%20unclear%20to%20what%20extent%20these%20reflect%20sophisticated%20recall%20%28crystallized%20intelligence%29%20or%20reasoning%20ability%20%28fluid%20intelligence%29.%20We%20introduce%20chess%20as%20a%20controlled%20testbed%20for%20disentangling%20these%20faculties.%20Leveraging%20the%20game%27s%20structure%20and%20scalable%20engine%20evaluations%2C%20we%20construct%20a%20taxonomy%20of%20positions%20varying%20in%20training%20corpus%20proximity--ranging%20from%20common%20states%20solvable%20by%20memorization%20to%20novel%20ones%20requiring%20first-principles%20reasoning.%20We%20systematically%20evaluate%20multiple%20GPT%20generations%20under%20varying%20reasoning%20intensities.%20Our%20analysis%20reveals%20a%20clear%20gradient%3A%20performance%20consistently%20degrades%20as%20fluid%20intelligence%20demands%20increase.%20Notably%2C%20in%20out-of-distribution%20tasks%2C%20performance%20collapses%20to%20random%20levels.%20While%20newer%20models%20improve%2C%20progress%20slows%20significantly%20for%20tasks%20outside%20the%20training%20distribution.%20Furthermore%2C%20while%20reasoning-augmented%20inference%20improves%20performance%2C%20its%20marginal%20benefit%20per%20token%20decreases%20with%20distributional%20proximity.%20These%20results%20suggest%20current%20architectures%20remain%20limited%20in%20systematic%20generalization%2C%20highlighting%20the%20need%20for%20mechanisms%20beyond%20scale%20to%20achieve%20robust%20fluid%20intelligence.&entry.1838667208=http%3A//arxiv.org/abs/2601.16823v1&entry.124074799=Read"},
{"title": "DeepShield: Fortifying Deepfake Video Detection with Local and Global Forgery Analysis", "author": "Yinqi Cai and Jichang Li and Zhaolun Li and Weikai Chen and Rushi Lan and Xi Xie and Xiaonan Luo and Guanbin Li", "abstract": "Recent advances in deep generative models have made it easier to manipulate face videos, raising significant concerns about their potential misuse for fraud and misinformation. Existing detectors often perform well in in-domain scenarios but fail to generalize across diverse manipulation techniques due to their reliance on forgery-specific artifacts. In this work, we introduce DeepShield, a novel deepfake detection framework that balances local sensitivity and global generalization to improve robustness across unseen forgeries. DeepShield enhances the CLIP-ViT encoder through two key components: Local Patch Guidance (LPG) and Global Forgery Diversification (GFD). LPG applies spatiotemporal artifact modeling and patch-wise supervision to capture fine-grained inconsistencies often overlooked by global models. GFD introduces domain feature augmentation, leveraging domain-bridging and boundary-expanding feature generation to synthesize diverse forgeries, mitigating overfitting and enhancing cross-domain adaptability. Through the integration of novel local and global analysis for deepfake detection, DeepShield outperforms state-of-the-art methods in cross-dataset and cross-manipulation evaluations, achieving superior robustness against unseen deepfake attacks. Code is available at https://github.com/lijichang/DeepShield.", "link": "http://arxiv.org/abs/2510.25237v2", "date": "2026-01-23", "relevancy": 2.098, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5399}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5161}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepShield%3A%20Fortifying%20Deepfake%20Video%20Detection%20with%20Local%20and%20Global%20Forgery%20Analysis&body=Title%3A%20DeepShield%3A%20Fortifying%20Deepfake%20Video%20Detection%20with%20Local%20and%20Global%20Forgery%20Analysis%0AAuthor%3A%20Yinqi%20Cai%20and%20Jichang%20Li%20and%20Zhaolun%20Li%20and%20Weikai%20Chen%20and%20Rushi%20Lan%20and%20Xi%20Xie%20and%20Xiaonan%20Luo%20and%20Guanbin%20Li%0AAbstract%3A%20Recent%20advances%20in%20deep%20generative%20models%20have%20made%20it%20easier%20to%20manipulate%20face%20videos%2C%20raising%20significant%20concerns%20about%20their%20potential%20misuse%20for%20fraud%20and%20misinformation.%20Existing%20detectors%20often%20perform%20well%20in%20in-domain%20scenarios%20but%20fail%20to%20generalize%20across%20diverse%20manipulation%20techniques%20due%20to%20their%20reliance%20on%20forgery-specific%20artifacts.%20In%20this%20work%2C%20we%20introduce%20DeepShield%2C%20a%20novel%20deepfake%20detection%20framework%20that%20balances%20local%20sensitivity%20and%20global%20generalization%20to%20improve%20robustness%20across%20unseen%20forgeries.%20DeepShield%20enhances%20the%20CLIP-ViT%20encoder%20through%20two%20key%20components%3A%20Local%20Patch%20Guidance%20%28LPG%29%20and%20Global%20Forgery%20Diversification%20%28GFD%29.%20LPG%20applies%20spatiotemporal%20artifact%20modeling%20and%20patch-wise%20supervision%20to%20capture%20fine-grained%20inconsistencies%20often%20overlooked%20by%20global%20models.%20GFD%20introduces%20domain%20feature%20augmentation%2C%20leveraging%20domain-bridging%20and%20boundary-expanding%20feature%20generation%20to%20synthesize%20diverse%20forgeries%2C%20mitigating%20overfitting%20and%20enhancing%20cross-domain%20adaptability.%20Through%20the%20integration%20of%20novel%20local%20and%20global%20analysis%20for%20deepfake%20detection%2C%20DeepShield%20outperforms%20state-of-the-art%20methods%20in%20cross-dataset%20and%20cross-manipulation%20evaluations%2C%20achieving%20superior%20robustness%20against%20unseen%20deepfake%20attacks.%20Code%20is%20available%20at%20https%3A//github.com/lijichang/DeepShield.%0ALink%3A%20http%3A//arxiv.org/abs/2510.25237v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepShield%253A%2520Fortifying%2520Deepfake%2520Video%2520Detection%2520with%2520Local%2520and%2520Global%2520Forgery%2520Analysis%26entry.906535625%3DYinqi%2520Cai%2520and%2520Jichang%2520Li%2520and%2520Zhaolun%2520Li%2520and%2520Weikai%2520Chen%2520and%2520Rushi%2520Lan%2520and%2520Xi%2520Xie%2520and%2520Xiaonan%2520Luo%2520and%2520Guanbin%2520Li%26entry.1292438233%3DRecent%2520advances%2520in%2520deep%2520generative%2520models%2520have%2520made%2520it%2520easier%2520to%2520manipulate%2520face%2520videos%252C%2520raising%2520significant%2520concerns%2520about%2520their%2520potential%2520misuse%2520for%2520fraud%2520and%2520misinformation.%2520Existing%2520detectors%2520often%2520perform%2520well%2520in%2520in-domain%2520scenarios%2520but%2520fail%2520to%2520generalize%2520across%2520diverse%2520manipulation%2520techniques%2520due%2520to%2520their%2520reliance%2520on%2520forgery-specific%2520artifacts.%2520In%2520this%2520work%252C%2520we%2520introduce%2520DeepShield%252C%2520a%2520novel%2520deepfake%2520detection%2520framework%2520that%2520balances%2520local%2520sensitivity%2520and%2520global%2520generalization%2520to%2520improve%2520robustness%2520across%2520unseen%2520forgeries.%2520DeepShield%2520enhances%2520the%2520CLIP-ViT%2520encoder%2520through%2520two%2520key%2520components%253A%2520Local%2520Patch%2520Guidance%2520%2528LPG%2529%2520and%2520Global%2520Forgery%2520Diversification%2520%2528GFD%2529.%2520LPG%2520applies%2520spatiotemporal%2520artifact%2520modeling%2520and%2520patch-wise%2520supervision%2520to%2520capture%2520fine-grained%2520inconsistencies%2520often%2520overlooked%2520by%2520global%2520models.%2520GFD%2520introduces%2520domain%2520feature%2520augmentation%252C%2520leveraging%2520domain-bridging%2520and%2520boundary-expanding%2520feature%2520generation%2520to%2520synthesize%2520diverse%2520forgeries%252C%2520mitigating%2520overfitting%2520and%2520enhancing%2520cross-domain%2520adaptability.%2520Through%2520the%2520integration%2520of%2520novel%2520local%2520and%2520global%2520analysis%2520for%2520deepfake%2520detection%252C%2520DeepShield%2520outperforms%2520state-of-the-art%2520methods%2520in%2520cross-dataset%2520and%2520cross-manipulation%2520evaluations%252C%2520achieving%2520superior%2520robustness%2520against%2520unseen%2520deepfake%2520attacks.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/lijichang/DeepShield.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25237v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepShield%3A%20Fortifying%20Deepfake%20Video%20Detection%20with%20Local%20and%20Global%20Forgery%20Analysis&entry.906535625=Yinqi%20Cai%20and%20Jichang%20Li%20and%20Zhaolun%20Li%20and%20Weikai%20Chen%20and%20Rushi%20Lan%20and%20Xi%20Xie%20and%20Xiaonan%20Luo%20and%20Guanbin%20Li&entry.1292438233=Recent%20advances%20in%20deep%20generative%20models%20have%20made%20it%20easier%20to%20manipulate%20face%20videos%2C%20raising%20significant%20concerns%20about%20their%20potential%20misuse%20for%20fraud%20and%20misinformation.%20Existing%20detectors%20often%20perform%20well%20in%20in-domain%20scenarios%20but%20fail%20to%20generalize%20across%20diverse%20manipulation%20techniques%20due%20to%20their%20reliance%20on%20forgery-specific%20artifacts.%20In%20this%20work%2C%20we%20introduce%20DeepShield%2C%20a%20novel%20deepfake%20detection%20framework%20that%20balances%20local%20sensitivity%20and%20global%20generalization%20to%20improve%20robustness%20across%20unseen%20forgeries.%20DeepShield%20enhances%20the%20CLIP-ViT%20encoder%20through%20two%20key%20components%3A%20Local%20Patch%20Guidance%20%28LPG%29%20and%20Global%20Forgery%20Diversification%20%28GFD%29.%20LPG%20applies%20spatiotemporal%20artifact%20modeling%20and%20patch-wise%20supervision%20to%20capture%20fine-grained%20inconsistencies%20often%20overlooked%20by%20global%20models.%20GFD%20introduces%20domain%20feature%20augmentation%2C%20leveraging%20domain-bridging%20and%20boundary-expanding%20feature%20generation%20to%20synthesize%20diverse%20forgeries%2C%20mitigating%20overfitting%20and%20enhancing%20cross-domain%20adaptability.%20Through%20the%20integration%20of%20novel%20local%20and%20global%20analysis%20for%20deepfake%20detection%2C%20DeepShield%20outperforms%20state-of-the-art%20methods%20in%20cross-dataset%20and%20cross-manipulation%20evaluations%2C%20achieving%20superior%20robustness%20against%20unseen%20deepfake%20attacks.%20Code%20is%20available%20at%20https%3A//github.com/lijichang/DeepShield.&entry.1838667208=http%3A//arxiv.org/abs/2510.25237v2&entry.124074799=Read"},
{"title": "LUMINA: Long-horizon Understanding for Multi-turn Interactive Agents", "author": "Amin Rakhsha and Thomas Hehn and Pietro Mazzaglia and Fabio Valerio Massoli and Arash Behboodi and Tribhuvanesh Orekondy", "abstract": "Large language models can perform well on many isolated tasks, yet they continue to struggle on multi-turn, long-horizon agentic problems that require skills such as planning, state tracking, and long context processing. In this work, we aim to better understand the relative importance of advancing these underlying capabilities for success on such tasks. We develop an oracle counterfactual framework for multi-turn problems that asks: how would an agent perform if it could leverage an oracle to perfectly perform a specific task? The change in the agent's performance due to this oracle assistance allows us to measure the criticality of such oracle skill in the future advancement of AI agents. We introduce a suite of procedurally generated, game-like tasks with tunable complexity. These controlled environments allow us to provide precise oracle interventions, such as perfect planning or flawless state tracking, and make it possible to isolate the contribution of each oracle without confounding effects present in real-world benchmarks. Our results show that while some interventions (e.g., planning) consistently improve performance across settings, the usefulness of other skills is dependent on the properties of the environment and language model. Our work sheds light on the challenges of multi-turn agentic environments to guide the future efforts in the development of AI agents and language models.", "link": "http://arxiv.org/abs/2601.16649v1", "date": "2026-01-23", "relevancy": 2.0875, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LUMINA%3A%20Long-horizon%20Understanding%20for%20Multi-turn%20Interactive%20Agents&body=Title%3A%20LUMINA%3A%20Long-horizon%20Understanding%20for%20Multi-turn%20Interactive%20Agents%0AAuthor%3A%20Amin%20Rakhsha%20and%20Thomas%20Hehn%20and%20Pietro%20Mazzaglia%20and%20Fabio%20Valerio%20Massoli%20and%20Arash%20Behboodi%20and%20Tribhuvanesh%20Orekondy%0AAbstract%3A%20Large%20language%20models%20can%20perform%20well%20on%20many%20isolated%20tasks%2C%20yet%20they%20continue%20to%20struggle%20on%20multi-turn%2C%20long-horizon%20agentic%20problems%20that%20require%20skills%20such%20as%20planning%2C%20state%20tracking%2C%20and%20long%20context%20processing.%20In%20this%20work%2C%20we%20aim%20to%20better%20understand%20the%20relative%20importance%20of%20advancing%20these%20underlying%20capabilities%20for%20success%20on%20such%20tasks.%20We%20develop%20an%20oracle%20counterfactual%20framework%20for%20multi-turn%20problems%20that%20asks%3A%20how%20would%20an%20agent%20perform%20if%20it%20could%20leverage%20an%20oracle%20to%20perfectly%20perform%20a%20specific%20task%3F%20The%20change%20in%20the%20agent%27s%20performance%20due%20to%20this%20oracle%20assistance%20allows%20us%20to%20measure%20the%20criticality%20of%20such%20oracle%20skill%20in%20the%20future%20advancement%20of%20AI%20agents.%20We%20introduce%20a%20suite%20of%20procedurally%20generated%2C%20game-like%20tasks%20with%20tunable%20complexity.%20These%20controlled%20environments%20allow%20us%20to%20provide%20precise%20oracle%20interventions%2C%20such%20as%20perfect%20planning%20or%20flawless%20state%20tracking%2C%20and%20make%20it%20possible%20to%20isolate%20the%20contribution%20of%20each%20oracle%20without%20confounding%20effects%20present%20in%20real-world%20benchmarks.%20Our%20results%20show%20that%20while%20some%20interventions%20%28e.g.%2C%20planning%29%20consistently%20improve%20performance%20across%20settings%2C%20the%20usefulness%20of%20other%20skills%20is%20dependent%20on%20the%20properties%20of%20the%20environment%20and%20language%20model.%20Our%20work%20sheds%20light%20on%20the%20challenges%20of%20multi-turn%20agentic%20environments%20to%20guide%20the%20future%20efforts%20in%20the%20development%20of%20AI%20agents%20and%20language%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLUMINA%253A%2520Long-horizon%2520Understanding%2520for%2520Multi-turn%2520Interactive%2520Agents%26entry.906535625%3DAmin%2520Rakhsha%2520and%2520Thomas%2520Hehn%2520and%2520Pietro%2520Mazzaglia%2520and%2520Fabio%2520Valerio%2520Massoli%2520and%2520Arash%2520Behboodi%2520and%2520Tribhuvanesh%2520Orekondy%26entry.1292438233%3DLarge%2520language%2520models%2520can%2520perform%2520well%2520on%2520many%2520isolated%2520tasks%252C%2520yet%2520they%2520continue%2520to%2520struggle%2520on%2520multi-turn%252C%2520long-horizon%2520agentic%2520problems%2520that%2520require%2520skills%2520such%2520as%2520planning%252C%2520state%2520tracking%252C%2520and%2520long%2520context%2520processing.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520better%2520understand%2520the%2520relative%2520importance%2520of%2520advancing%2520these%2520underlying%2520capabilities%2520for%2520success%2520on%2520such%2520tasks.%2520We%2520develop%2520an%2520oracle%2520counterfactual%2520framework%2520for%2520multi-turn%2520problems%2520that%2520asks%253A%2520how%2520would%2520an%2520agent%2520perform%2520if%2520it%2520could%2520leverage%2520an%2520oracle%2520to%2520perfectly%2520perform%2520a%2520specific%2520task%253F%2520The%2520change%2520in%2520the%2520agent%2527s%2520performance%2520due%2520to%2520this%2520oracle%2520assistance%2520allows%2520us%2520to%2520measure%2520the%2520criticality%2520of%2520such%2520oracle%2520skill%2520in%2520the%2520future%2520advancement%2520of%2520AI%2520agents.%2520We%2520introduce%2520a%2520suite%2520of%2520procedurally%2520generated%252C%2520game-like%2520tasks%2520with%2520tunable%2520complexity.%2520These%2520controlled%2520environments%2520allow%2520us%2520to%2520provide%2520precise%2520oracle%2520interventions%252C%2520such%2520as%2520perfect%2520planning%2520or%2520flawless%2520state%2520tracking%252C%2520and%2520make%2520it%2520possible%2520to%2520isolate%2520the%2520contribution%2520of%2520each%2520oracle%2520without%2520confounding%2520effects%2520present%2520in%2520real-world%2520benchmarks.%2520Our%2520results%2520show%2520that%2520while%2520some%2520interventions%2520%2528e.g.%252C%2520planning%2529%2520consistently%2520improve%2520performance%2520across%2520settings%252C%2520the%2520usefulness%2520of%2520other%2520skills%2520is%2520dependent%2520on%2520the%2520properties%2520of%2520the%2520environment%2520and%2520language%2520model.%2520Our%2520work%2520sheds%2520light%2520on%2520the%2520challenges%2520of%2520multi-turn%2520agentic%2520environments%2520to%2520guide%2520the%2520future%2520efforts%2520in%2520the%2520development%2520of%2520AI%2520agents%2520and%2520language%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LUMINA%3A%20Long-horizon%20Understanding%20for%20Multi-turn%20Interactive%20Agents&entry.906535625=Amin%20Rakhsha%20and%20Thomas%20Hehn%20and%20Pietro%20Mazzaglia%20and%20Fabio%20Valerio%20Massoli%20and%20Arash%20Behboodi%20and%20Tribhuvanesh%20Orekondy&entry.1292438233=Large%20language%20models%20can%20perform%20well%20on%20many%20isolated%20tasks%2C%20yet%20they%20continue%20to%20struggle%20on%20multi-turn%2C%20long-horizon%20agentic%20problems%20that%20require%20skills%20such%20as%20planning%2C%20state%20tracking%2C%20and%20long%20context%20processing.%20In%20this%20work%2C%20we%20aim%20to%20better%20understand%20the%20relative%20importance%20of%20advancing%20these%20underlying%20capabilities%20for%20success%20on%20such%20tasks.%20We%20develop%20an%20oracle%20counterfactual%20framework%20for%20multi-turn%20problems%20that%20asks%3A%20how%20would%20an%20agent%20perform%20if%20it%20could%20leverage%20an%20oracle%20to%20perfectly%20perform%20a%20specific%20task%3F%20The%20change%20in%20the%20agent%27s%20performance%20due%20to%20this%20oracle%20assistance%20allows%20us%20to%20measure%20the%20criticality%20of%20such%20oracle%20skill%20in%20the%20future%20advancement%20of%20AI%20agents.%20We%20introduce%20a%20suite%20of%20procedurally%20generated%2C%20game-like%20tasks%20with%20tunable%20complexity.%20These%20controlled%20environments%20allow%20us%20to%20provide%20precise%20oracle%20interventions%2C%20such%20as%20perfect%20planning%20or%20flawless%20state%20tracking%2C%20and%20make%20it%20possible%20to%20isolate%20the%20contribution%20of%20each%20oracle%20without%20confounding%20effects%20present%20in%20real-world%20benchmarks.%20Our%20results%20show%20that%20while%20some%20interventions%20%28e.g.%2C%20planning%29%20consistently%20improve%20performance%20across%20settings%2C%20the%20usefulness%20of%20other%20skills%20is%20dependent%20on%20the%20properties%20of%20the%20environment%20and%20language%20model.%20Our%20work%20sheds%20light%20on%20the%20challenges%20of%20multi-turn%20agentic%20environments%20to%20guide%20the%20future%20efforts%20in%20the%20development%20of%20AI%20agents%20and%20language%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.16649v1&entry.124074799=Read"},
{"title": "GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints", "author": "Andy Zhu and Rongzhe Wei and Yupu Gu and Pan Li", "abstract": "Machine unlearning (MU) for large language models has become critical for AI safety, yet existing methods fail to generalize to Mixture-of-Experts (MoE) architectures. We identify that traditional unlearning methods exploit MoE's architectural vulnerability: they manipulate routers to redirect queries away from knowledgeable experts rather than erasing knowledge, causing a loss of model utility and superficial forgetting. We propose Geometric Routing Invariance Preservation (GRIP), an algorithm-agnostic framework for unlearning for MoE. Our core contribution is a geometric constraint, implemented by projecting router gradient updates into an expert-specific null-space. Crucially, this decouples routing stability from parameter rigidity: while discrete expert selections remain stable for retained knowledge, the continuous router parameters remain plastic within the null space, allowing the model to undergo necessary internal reconfiguration to satisfy unlearning objectives. This forces the unlearning optimization to erase knowledge directly from expert parameters rather than exploiting the superficial router manipulation shortcut. GRIP functions as an adapter, constraining router parameter updates without modifying the underlying unlearning algorithm. Extensive experiments on large-scale MoE models demonstrate that our adapter eliminates expert selection shift (achieving over 95% routing stability) across all tested unlearning methods while preserving their utility. By preventing existing algorithms from exploiting MoE model's router vulnerability, GRIP adapts existing unlearning research from dense architectures to MoEs.", "link": "http://arxiv.org/abs/2601.16905v1", "date": "2026-01-23", "relevancy": 2.0793, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5572}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5124}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRIP%3A%20Algorithm-Agnostic%20Machine%20Unlearning%20for%20Mixture-of-Experts%20via%20Geometric%20Router%20Constraints&body=Title%3A%20GRIP%3A%20Algorithm-Agnostic%20Machine%20Unlearning%20for%20Mixture-of-Experts%20via%20Geometric%20Router%20Constraints%0AAuthor%3A%20Andy%20Zhu%20and%20Rongzhe%20Wei%20and%20Yupu%20Gu%20and%20Pan%20Li%0AAbstract%3A%20Machine%20unlearning%20%28MU%29%20for%20large%20language%20models%20has%20become%20critical%20for%20AI%20safety%2C%20yet%20existing%20methods%20fail%20to%20generalize%20to%20Mixture-of-Experts%20%28MoE%29%20architectures.%20We%20identify%20that%20traditional%20unlearning%20methods%20exploit%20MoE%27s%20architectural%20vulnerability%3A%20they%20manipulate%20routers%20to%20redirect%20queries%20away%20from%20knowledgeable%20experts%20rather%20than%20erasing%20knowledge%2C%20causing%20a%20loss%20of%20model%20utility%20and%20superficial%20forgetting.%20We%20propose%20Geometric%20Routing%20Invariance%20Preservation%20%28GRIP%29%2C%20an%20algorithm-agnostic%20framework%20for%20unlearning%20for%20MoE.%20Our%20core%20contribution%20is%20a%20geometric%20constraint%2C%20implemented%20by%20projecting%20router%20gradient%20updates%20into%20an%20expert-specific%20null-space.%20Crucially%2C%20this%20decouples%20routing%20stability%20from%20parameter%20rigidity%3A%20while%20discrete%20expert%20selections%20remain%20stable%20for%20retained%20knowledge%2C%20the%20continuous%20router%20parameters%20remain%20plastic%20within%20the%20null%20space%2C%20allowing%20the%20model%20to%20undergo%20necessary%20internal%20reconfiguration%20to%20satisfy%20unlearning%20objectives.%20This%20forces%20the%20unlearning%20optimization%20to%20erase%20knowledge%20directly%20from%20expert%20parameters%20rather%20than%20exploiting%20the%20superficial%20router%20manipulation%20shortcut.%20GRIP%20functions%20as%20an%20adapter%2C%20constraining%20router%20parameter%20updates%20without%20modifying%20the%20underlying%20unlearning%20algorithm.%20Extensive%20experiments%20on%20large-scale%20MoE%20models%20demonstrate%20that%20our%20adapter%20eliminates%20expert%20selection%20shift%20%28achieving%20over%2095%25%20routing%20stability%29%20across%20all%20tested%20unlearning%20methods%20while%20preserving%20their%20utility.%20By%20preventing%20existing%20algorithms%20from%20exploiting%20MoE%20model%27s%20router%20vulnerability%2C%20GRIP%20adapts%20existing%20unlearning%20research%20from%20dense%20architectures%20to%20MoEs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRIP%253A%2520Algorithm-Agnostic%2520Machine%2520Unlearning%2520for%2520Mixture-of-Experts%2520via%2520Geometric%2520Router%2520Constraints%26entry.906535625%3DAndy%2520Zhu%2520and%2520Rongzhe%2520Wei%2520and%2520Yupu%2520Gu%2520and%2520Pan%2520Li%26entry.1292438233%3DMachine%2520unlearning%2520%2528MU%2529%2520for%2520large%2520language%2520models%2520has%2520become%2520critical%2520for%2520AI%2520safety%252C%2520yet%2520existing%2520methods%2520fail%2520to%2520generalize%2520to%2520Mixture-of-Experts%2520%2528MoE%2529%2520architectures.%2520We%2520identify%2520that%2520traditional%2520unlearning%2520methods%2520exploit%2520MoE%2527s%2520architectural%2520vulnerability%253A%2520they%2520manipulate%2520routers%2520to%2520redirect%2520queries%2520away%2520from%2520knowledgeable%2520experts%2520rather%2520than%2520erasing%2520knowledge%252C%2520causing%2520a%2520loss%2520of%2520model%2520utility%2520and%2520superficial%2520forgetting.%2520We%2520propose%2520Geometric%2520Routing%2520Invariance%2520Preservation%2520%2528GRIP%2529%252C%2520an%2520algorithm-agnostic%2520framework%2520for%2520unlearning%2520for%2520MoE.%2520Our%2520core%2520contribution%2520is%2520a%2520geometric%2520constraint%252C%2520implemented%2520by%2520projecting%2520router%2520gradient%2520updates%2520into%2520an%2520expert-specific%2520null-space.%2520Crucially%252C%2520this%2520decouples%2520routing%2520stability%2520from%2520parameter%2520rigidity%253A%2520while%2520discrete%2520expert%2520selections%2520remain%2520stable%2520for%2520retained%2520knowledge%252C%2520the%2520continuous%2520router%2520parameters%2520remain%2520plastic%2520within%2520the%2520null%2520space%252C%2520allowing%2520the%2520model%2520to%2520undergo%2520necessary%2520internal%2520reconfiguration%2520to%2520satisfy%2520unlearning%2520objectives.%2520This%2520forces%2520the%2520unlearning%2520optimization%2520to%2520erase%2520knowledge%2520directly%2520from%2520expert%2520parameters%2520rather%2520than%2520exploiting%2520the%2520superficial%2520router%2520manipulation%2520shortcut.%2520GRIP%2520functions%2520as%2520an%2520adapter%252C%2520constraining%2520router%2520parameter%2520updates%2520without%2520modifying%2520the%2520underlying%2520unlearning%2520algorithm.%2520Extensive%2520experiments%2520on%2520large-scale%2520MoE%2520models%2520demonstrate%2520that%2520our%2520adapter%2520eliminates%2520expert%2520selection%2520shift%2520%2528achieving%2520over%252095%2525%2520routing%2520stability%2529%2520across%2520all%2520tested%2520unlearning%2520methods%2520while%2520preserving%2520their%2520utility.%2520By%2520preventing%2520existing%2520algorithms%2520from%2520exploiting%2520MoE%2520model%2527s%2520router%2520vulnerability%252C%2520GRIP%2520adapts%2520existing%2520unlearning%2520research%2520from%2520dense%2520architectures%2520to%2520MoEs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRIP%3A%20Algorithm-Agnostic%20Machine%20Unlearning%20for%20Mixture-of-Experts%20via%20Geometric%20Router%20Constraints&entry.906535625=Andy%20Zhu%20and%20Rongzhe%20Wei%20and%20Yupu%20Gu%20and%20Pan%20Li&entry.1292438233=Machine%20unlearning%20%28MU%29%20for%20large%20language%20models%20has%20become%20critical%20for%20AI%20safety%2C%20yet%20existing%20methods%20fail%20to%20generalize%20to%20Mixture-of-Experts%20%28MoE%29%20architectures.%20We%20identify%20that%20traditional%20unlearning%20methods%20exploit%20MoE%27s%20architectural%20vulnerability%3A%20they%20manipulate%20routers%20to%20redirect%20queries%20away%20from%20knowledgeable%20experts%20rather%20than%20erasing%20knowledge%2C%20causing%20a%20loss%20of%20model%20utility%20and%20superficial%20forgetting.%20We%20propose%20Geometric%20Routing%20Invariance%20Preservation%20%28GRIP%29%2C%20an%20algorithm-agnostic%20framework%20for%20unlearning%20for%20MoE.%20Our%20core%20contribution%20is%20a%20geometric%20constraint%2C%20implemented%20by%20projecting%20router%20gradient%20updates%20into%20an%20expert-specific%20null-space.%20Crucially%2C%20this%20decouples%20routing%20stability%20from%20parameter%20rigidity%3A%20while%20discrete%20expert%20selections%20remain%20stable%20for%20retained%20knowledge%2C%20the%20continuous%20router%20parameters%20remain%20plastic%20within%20the%20null%20space%2C%20allowing%20the%20model%20to%20undergo%20necessary%20internal%20reconfiguration%20to%20satisfy%20unlearning%20objectives.%20This%20forces%20the%20unlearning%20optimization%20to%20erase%20knowledge%20directly%20from%20expert%20parameters%20rather%20than%20exploiting%20the%20superficial%20router%20manipulation%20shortcut.%20GRIP%20functions%20as%20an%20adapter%2C%20constraining%20router%20parameter%20updates%20without%20modifying%20the%20underlying%20unlearning%20algorithm.%20Extensive%20experiments%20on%20large-scale%20MoE%20models%20demonstrate%20that%20our%20adapter%20eliminates%20expert%20selection%20shift%20%28achieving%20over%2095%25%20routing%20stability%29%20across%20all%20tested%20unlearning%20methods%20while%20preserving%20their%20utility.%20By%20preventing%20existing%20algorithms%20from%20exploiting%20MoE%20model%27s%20router%20vulnerability%2C%20GRIP%20adapts%20existing%20unlearning%20research%20from%20dense%20architectures%20to%20MoEs.&entry.1838667208=http%3A//arxiv.org/abs/2601.16905v1&entry.124074799=Read"},
{"title": "Sycophancy Hides Linearly in the Attention Heads", "author": "Rifo Genadi and Munachiso Nwadike and Nurdaulet Mukhituly and Hilal Alquabeh and Tatsuya Hiraoka and Kentaro Inui", "abstract": "We find that correct-to-incorrect sycophancy signals are most linearly separable within multi-head attention activations. Motivated by the linear representation hypothesis, we train linear probes across the residual stream, multilayer perceptron (MLP), and attention layers to analyze where these signals emerge. Although separability appears in the residual stream and MLPs, steering using these probes is most effective in a sparse subset of middle-layer attention heads. Using TruthfulQA as the base dataset, we find that probes trained on it transfer effectively to other factual QA benchmarks. Furthermore, comparing our discovered direction to previously identified \"truthful\" directions reveals limited overlap, suggesting that factual accuracy, and deference resistance, arise from related but distinct mechanisms. Attention-pattern analysis further indicates that the influential heads attend disproportionately to expressions of user doubt, contributing to sycophantic shifts. Overall, these findings suggest that sycophancy can be mitigated through simple, targeted linear interventions that exploit the internal geometry of attention activations.", "link": "http://arxiv.org/abs/2601.16644v1", "date": "2026-01-23", "relevancy": 2.0742, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4179}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4133}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sycophancy%20Hides%20Linearly%20in%20the%20Attention%20Heads&body=Title%3A%20Sycophancy%20Hides%20Linearly%20in%20the%20Attention%20Heads%0AAuthor%3A%20Rifo%20Genadi%20and%20Munachiso%20Nwadike%20and%20Nurdaulet%20Mukhituly%20and%20Hilal%20Alquabeh%20and%20Tatsuya%20Hiraoka%20and%20Kentaro%20Inui%0AAbstract%3A%20We%20find%20that%20correct-to-incorrect%20sycophancy%20signals%20are%20most%20linearly%20separable%20within%20multi-head%20attention%20activations.%20Motivated%20by%20the%20linear%20representation%20hypothesis%2C%20we%20train%20linear%20probes%20across%20the%20residual%20stream%2C%20multilayer%20perceptron%20%28MLP%29%2C%20and%20attention%20layers%20to%20analyze%20where%20these%20signals%20emerge.%20Although%20separability%20appears%20in%20the%20residual%20stream%20and%20MLPs%2C%20steering%20using%20these%20probes%20is%20most%20effective%20in%20a%20sparse%20subset%20of%20middle-layer%20attention%20heads.%20Using%20TruthfulQA%20as%20the%20base%20dataset%2C%20we%20find%20that%20probes%20trained%20on%20it%20transfer%20effectively%20to%20other%20factual%20QA%20benchmarks.%20Furthermore%2C%20comparing%20our%20discovered%20direction%20to%20previously%20identified%20%22truthful%22%20directions%20reveals%20limited%20overlap%2C%20suggesting%20that%20factual%20accuracy%2C%20and%20deference%20resistance%2C%20arise%20from%20related%20but%20distinct%20mechanisms.%20Attention-pattern%20analysis%20further%20indicates%20that%20the%20influential%20heads%20attend%20disproportionately%20to%20expressions%20of%20user%20doubt%2C%20contributing%20to%20sycophantic%20shifts.%20Overall%2C%20these%20findings%20suggest%20that%20sycophancy%20can%20be%20mitigated%20through%20simple%2C%20targeted%20linear%20interventions%20that%20exploit%20the%20internal%20geometry%20of%20attention%20activations.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSycophancy%2520Hides%2520Linearly%2520in%2520the%2520Attention%2520Heads%26entry.906535625%3DRifo%2520Genadi%2520and%2520Munachiso%2520Nwadike%2520and%2520Nurdaulet%2520Mukhituly%2520and%2520Hilal%2520Alquabeh%2520and%2520Tatsuya%2520Hiraoka%2520and%2520Kentaro%2520Inui%26entry.1292438233%3DWe%2520find%2520that%2520correct-to-incorrect%2520sycophancy%2520signals%2520are%2520most%2520linearly%2520separable%2520within%2520multi-head%2520attention%2520activations.%2520Motivated%2520by%2520the%2520linear%2520representation%2520hypothesis%252C%2520we%2520train%2520linear%2520probes%2520across%2520the%2520residual%2520stream%252C%2520multilayer%2520perceptron%2520%2528MLP%2529%252C%2520and%2520attention%2520layers%2520to%2520analyze%2520where%2520these%2520signals%2520emerge.%2520Although%2520separability%2520appears%2520in%2520the%2520residual%2520stream%2520and%2520MLPs%252C%2520steering%2520using%2520these%2520probes%2520is%2520most%2520effective%2520in%2520a%2520sparse%2520subset%2520of%2520middle-layer%2520attention%2520heads.%2520Using%2520TruthfulQA%2520as%2520the%2520base%2520dataset%252C%2520we%2520find%2520that%2520probes%2520trained%2520on%2520it%2520transfer%2520effectively%2520to%2520other%2520factual%2520QA%2520benchmarks.%2520Furthermore%252C%2520comparing%2520our%2520discovered%2520direction%2520to%2520previously%2520identified%2520%2522truthful%2522%2520directions%2520reveals%2520limited%2520overlap%252C%2520suggesting%2520that%2520factual%2520accuracy%252C%2520and%2520deference%2520resistance%252C%2520arise%2520from%2520related%2520but%2520distinct%2520mechanisms.%2520Attention-pattern%2520analysis%2520further%2520indicates%2520that%2520the%2520influential%2520heads%2520attend%2520disproportionately%2520to%2520expressions%2520of%2520user%2520doubt%252C%2520contributing%2520to%2520sycophantic%2520shifts.%2520Overall%252C%2520these%2520findings%2520suggest%2520that%2520sycophancy%2520can%2520be%2520mitigated%2520through%2520simple%252C%2520targeted%2520linear%2520interventions%2520that%2520exploit%2520the%2520internal%2520geometry%2520of%2520attention%2520activations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sycophancy%20Hides%20Linearly%20in%20the%20Attention%20Heads&entry.906535625=Rifo%20Genadi%20and%20Munachiso%20Nwadike%20and%20Nurdaulet%20Mukhituly%20and%20Hilal%20Alquabeh%20and%20Tatsuya%20Hiraoka%20and%20Kentaro%20Inui&entry.1292438233=We%20find%20that%20correct-to-incorrect%20sycophancy%20signals%20are%20most%20linearly%20separable%20within%20multi-head%20attention%20activations.%20Motivated%20by%20the%20linear%20representation%20hypothesis%2C%20we%20train%20linear%20probes%20across%20the%20residual%20stream%2C%20multilayer%20perceptron%20%28MLP%29%2C%20and%20attention%20layers%20to%20analyze%20where%20these%20signals%20emerge.%20Although%20separability%20appears%20in%20the%20residual%20stream%20and%20MLPs%2C%20steering%20using%20these%20probes%20is%20most%20effective%20in%20a%20sparse%20subset%20of%20middle-layer%20attention%20heads.%20Using%20TruthfulQA%20as%20the%20base%20dataset%2C%20we%20find%20that%20probes%20trained%20on%20it%20transfer%20effectively%20to%20other%20factual%20QA%20benchmarks.%20Furthermore%2C%20comparing%20our%20discovered%20direction%20to%20previously%20identified%20%22truthful%22%20directions%20reveals%20limited%20overlap%2C%20suggesting%20that%20factual%20accuracy%2C%20and%20deference%20resistance%2C%20arise%20from%20related%20but%20distinct%20mechanisms.%20Attention-pattern%20analysis%20further%20indicates%20that%20the%20influential%20heads%20attend%20disproportionately%20to%20expressions%20of%20user%20doubt%2C%20contributing%20to%20sycophantic%20shifts.%20Overall%2C%20these%20findings%20suggest%20that%20sycophancy%20can%20be%20mitigated%20through%20simple%2C%20targeted%20linear%20interventions%20that%20exploit%20the%20internal%20geometry%20of%20attention%20activations.&entry.1838667208=http%3A//arxiv.org/abs/2601.16644v1&entry.124074799=Read"},
{"title": "Decoupling Multi-Contrast Super-Resolution: Self-Supervised Implicit Re-Representation for Unpaired Cross-Modal Synthesis", "author": "Yinzhe Wu and Hongyu Rui and Fanwen Wang and Jiahao Huang and Zhenxuan Zhang and Haosen Zhang and Zi Wang and Guang Yang", "abstract": "Multi-contrast super-resolution (MCSR) is crucial for enhancing MRI but current deep learning methods are limited. They typically require large, paired low- and high-resolution (LR/HR) training datasets, which are scarce, and are trained for fixed upsampling scales. While recent self-supervised methods remove the paired data requirement, they fail to leverage valuable population-level priors. In this work, we propose a novel, decoupled MCSR framework that resolves both limitations. We reformulate MCSR into two stages: (1) an unpaired cross-modal synthesis (uCMS) module, trained once on unpaired population data to learn a robust anatomical prior; and (2) a lightweight, patient-specific implicit re-representation (IrR) module. This IrR module is optimized in a self-supervised manner to fuse the population prior with the subject's own LR target data. This design uniquely fuses population-level knowledge with patient-specific fidelity without requiring any paired LR/HR or paired cross-modal training data. By building the IrR module on an implicit neural representation, our framework is also inherently scale-agnostic. Our method demonstrates superior quantitative performance on different datasets, with exceptional robustness at extreme scales (16x, 32x), a regime where competing methods fail. Our work presents a data-efficient, flexible, and computationally lightweight paradigm for MCSR, enabling high-fidelity, arbitrary-scale", "link": "http://arxiv.org/abs/2505.05855v3", "date": "2026-01-23", "relevancy": 2.0724, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5265}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5135}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupling%20Multi-Contrast%20Super-Resolution%3A%20Self-Supervised%20Implicit%20Re-Representation%20for%20Unpaired%20Cross-Modal%20Synthesis&body=Title%3A%20Decoupling%20Multi-Contrast%20Super-Resolution%3A%20Self-Supervised%20Implicit%20Re-Representation%20for%20Unpaired%20Cross-Modal%20Synthesis%0AAuthor%3A%20Yinzhe%20Wu%20and%20Hongyu%20Rui%20and%20Fanwen%20Wang%20and%20Jiahao%20Huang%20and%20Zhenxuan%20Zhang%20and%20Haosen%20Zhang%20and%20Zi%20Wang%20and%20Guang%20Yang%0AAbstract%3A%20Multi-contrast%20super-resolution%20%28MCSR%29%20is%20crucial%20for%20enhancing%20MRI%20but%20current%20deep%20learning%20methods%20are%20limited.%20They%20typically%20require%20large%2C%20paired%20low-%20and%20high-resolution%20%28LR/HR%29%20training%20datasets%2C%20which%20are%20scarce%2C%20and%20are%20trained%20for%20fixed%20upsampling%20scales.%20While%20recent%20self-supervised%20methods%20remove%20the%20paired%20data%20requirement%2C%20they%20fail%20to%20leverage%20valuable%20population-level%20priors.%20In%20this%20work%2C%20we%20propose%20a%20novel%2C%20decoupled%20MCSR%20framework%20that%20resolves%20both%20limitations.%20We%20reformulate%20MCSR%20into%20two%20stages%3A%20%281%29%20an%20unpaired%20cross-modal%20synthesis%20%28uCMS%29%20module%2C%20trained%20once%20on%20unpaired%20population%20data%20to%20learn%20a%20robust%20anatomical%20prior%3B%20and%20%282%29%20a%20lightweight%2C%20patient-specific%20implicit%20re-representation%20%28IrR%29%20module.%20This%20IrR%20module%20is%20optimized%20in%20a%20self-supervised%20manner%20to%20fuse%20the%20population%20prior%20with%20the%20subject%27s%20own%20LR%20target%20data.%20This%20design%20uniquely%20fuses%20population-level%20knowledge%20with%20patient-specific%20fidelity%20without%20requiring%20any%20paired%20LR/HR%20or%20paired%20cross-modal%20training%20data.%20By%20building%20the%20IrR%20module%20on%20an%20implicit%20neural%20representation%2C%20our%20framework%20is%20also%20inherently%20scale-agnostic.%20Our%20method%20demonstrates%20superior%20quantitative%20performance%20on%20different%20datasets%2C%20with%20exceptional%20robustness%20at%20extreme%20scales%20%2816x%2C%2032x%29%2C%20a%20regime%20where%20competing%20methods%20fail.%20Our%20work%20presents%20a%20data-efficient%2C%20flexible%2C%20and%20computationally%20lightweight%20paradigm%20for%20MCSR%2C%20enabling%20high-fidelity%2C%20arbitrary-scale%0ALink%3A%20http%3A//arxiv.org/abs/2505.05855v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupling%2520Multi-Contrast%2520Super-Resolution%253A%2520Self-Supervised%2520Implicit%2520Re-Representation%2520for%2520Unpaired%2520Cross-Modal%2520Synthesis%26entry.906535625%3DYinzhe%2520Wu%2520and%2520Hongyu%2520Rui%2520and%2520Fanwen%2520Wang%2520and%2520Jiahao%2520Huang%2520and%2520Zhenxuan%2520Zhang%2520and%2520Haosen%2520Zhang%2520and%2520Zi%2520Wang%2520and%2520Guang%2520Yang%26entry.1292438233%3DMulti-contrast%2520super-resolution%2520%2528MCSR%2529%2520is%2520crucial%2520for%2520enhancing%2520MRI%2520but%2520current%2520deep%2520learning%2520methods%2520are%2520limited.%2520They%2520typically%2520require%2520large%252C%2520paired%2520low-%2520and%2520high-resolution%2520%2528LR/HR%2529%2520training%2520datasets%252C%2520which%2520are%2520scarce%252C%2520and%2520are%2520trained%2520for%2520fixed%2520upsampling%2520scales.%2520While%2520recent%2520self-supervised%2520methods%2520remove%2520the%2520paired%2520data%2520requirement%252C%2520they%2520fail%2520to%2520leverage%2520valuable%2520population-level%2520priors.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%252C%2520decoupled%2520MCSR%2520framework%2520that%2520resolves%2520both%2520limitations.%2520We%2520reformulate%2520MCSR%2520into%2520two%2520stages%253A%2520%25281%2529%2520an%2520unpaired%2520cross-modal%2520synthesis%2520%2528uCMS%2529%2520module%252C%2520trained%2520once%2520on%2520unpaired%2520population%2520data%2520to%2520learn%2520a%2520robust%2520anatomical%2520prior%253B%2520and%2520%25282%2529%2520a%2520lightweight%252C%2520patient-specific%2520implicit%2520re-representation%2520%2528IrR%2529%2520module.%2520This%2520IrR%2520module%2520is%2520optimized%2520in%2520a%2520self-supervised%2520manner%2520to%2520fuse%2520the%2520population%2520prior%2520with%2520the%2520subject%2527s%2520own%2520LR%2520target%2520data.%2520This%2520design%2520uniquely%2520fuses%2520population-level%2520knowledge%2520with%2520patient-specific%2520fidelity%2520without%2520requiring%2520any%2520paired%2520LR/HR%2520or%2520paired%2520cross-modal%2520training%2520data.%2520By%2520building%2520the%2520IrR%2520module%2520on%2520an%2520implicit%2520neural%2520representation%252C%2520our%2520framework%2520is%2520also%2520inherently%2520scale-agnostic.%2520Our%2520method%2520demonstrates%2520superior%2520quantitative%2520performance%2520on%2520different%2520datasets%252C%2520with%2520exceptional%2520robustness%2520at%2520extreme%2520scales%2520%252816x%252C%252032x%2529%252C%2520a%2520regime%2520where%2520competing%2520methods%2520fail.%2520Our%2520work%2520presents%2520a%2520data-efficient%252C%2520flexible%252C%2520and%2520computationally%2520lightweight%2520paradigm%2520for%2520MCSR%252C%2520enabling%2520high-fidelity%252C%2520arbitrary-scale%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05855v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupling%20Multi-Contrast%20Super-Resolution%3A%20Self-Supervised%20Implicit%20Re-Representation%20for%20Unpaired%20Cross-Modal%20Synthesis&entry.906535625=Yinzhe%20Wu%20and%20Hongyu%20Rui%20and%20Fanwen%20Wang%20and%20Jiahao%20Huang%20and%20Zhenxuan%20Zhang%20and%20Haosen%20Zhang%20and%20Zi%20Wang%20and%20Guang%20Yang&entry.1292438233=Multi-contrast%20super-resolution%20%28MCSR%29%20is%20crucial%20for%20enhancing%20MRI%20but%20current%20deep%20learning%20methods%20are%20limited.%20They%20typically%20require%20large%2C%20paired%20low-%20and%20high-resolution%20%28LR/HR%29%20training%20datasets%2C%20which%20are%20scarce%2C%20and%20are%20trained%20for%20fixed%20upsampling%20scales.%20While%20recent%20self-supervised%20methods%20remove%20the%20paired%20data%20requirement%2C%20they%20fail%20to%20leverage%20valuable%20population-level%20priors.%20In%20this%20work%2C%20we%20propose%20a%20novel%2C%20decoupled%20MCSR%20framework%20that%20resolves%20both%20limitations.%20We%20reformulate%20MCSR%20into%20two%20stages%3A%20%281%29%20an%20unpaired%20cross-modal%20synthesis%20%28uCMS%29%20module%2C%20trained%20once%20on%20unpaired%20population%20data%20to%20learn%20a%20robust%20anatomical%20prior%3B%20and%20%282%29%20a%20lightweight%2C%20patient-specific%20implicit%20re-representation%20%28IrR%29%20module.%20This%20IrR%20module%20is%20optimized%20in%20a%20self-supervised%20manner%20to%20fuse%20the%20population%20prior%20with%20the%20subject%27s%20own%20LR%20target%20data.%20This%20design%20uniquely%20fuses%20population-level%20knowledge%20with%20patient-specific%20fidelity%20without%20requiring%20any%20paired%20LR/HR%20or%20paired%20cross-modal%20training%20data.%20By%20building%20the%20IrR%20module%20on%20an%20implicit%20neural%20representation%2C%20our%20framework%20is%20also%20inherently%20scale-agnostic.%20Our%20method%20demonstrates%20superior%20quantitative%20performance%20on%20different%20datasets%2C%20with%20exceptional%20robustness%20at%20extreme%20scales%20%2816x%2C%2032x%29%2C%20a%20regime%20where%20competing%20methods%20fail.%20Our%20work%20presents%20a%20data-efficient%2C%20flexible%2C%20and%20computationally%20lightweight%20paradigm%20for%20MCSR%2C%20enabling%20high-fidelity%2C%20arbitrary-scale&entry.1838667208=http%3A//arxiv.org/abs/2505.05855v3&entry.124074799=Read"},
{"title": "Scribble-Supervised Medical Image Segmentation with Dynamic Teacher Switching and Hierarchical Consistency", "author": "Thanh-Huy Nguyen and Hoang-Loc Cao and Dat T. Chung and Mai-Anh Vu and Thanh-Minh Nguyen and Minh Le and Phat K. Huynh and Ulas Bagci", "abstract": "Scribble-supervised methods have emerged to mitigate the prohibitive annotation burden in medical image segmentation. However, the inherent sparsity of these annotations introduces significant ambiguity, which results in noisy pseudo-label propagation and hinders the learning of robust anatomical boundaries. To address this challenge, we propose SDT-Net, a novel dual-teacher, single-student framework designed to maximize supervision quality from these weak signals. Our method features a Dynamic Teacher Switching (DTS) module to adaptively select the most reliable teacher. This selected teacher then guides the student via two synergistic mechanisms: high-confidence pseudo-labels, refined by a Pick Reliable Pixels (PRP) mechanism, and multi-level feature alignment, enforced by a Hierarchical Consistency (HiCo) module. Extensive experiments on the ACDC and MSCMRseg datasets demonstrate that SDT-Net achieves state-of-the-art performance, producing more accurate and anatomically plausible segmentation.", "link": "http://arxiv.org/abs/2601.14563v3", "date": "2026-01-23", "relevancy": 2.0665, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5221}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5149}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scribble-Supervised%20Medical%20Image%20Segmentation%20with%20Dynamic%20Teacher%20Switching%20and%20Hierarchical%20Consistency&body=Title%3A%20Scribble-Supervised%20Medical%20Image%20Segmentation%20with%20Dynamic%20Teacher%20Switching%20and%20Hierarchical%20Consistency%0AAuthor%3A%20Thanh-Huy%20Nguyen%20and%20Hoang-Loc%20Cao%20and%20Dat%20T.%20Chung%20and%20Mai-Anh%20Vu%20and%20Thanh-Minh%20Nguyen%20and%20Minh%20Le%20and%20Phat%20K.%20Huynh%20and%20Ulas%20Bagci%0AAbstract%3A%20Scribble-supervised%20methods%20have%20emerged%20to%20mitigate%20the%20prohibitive%20annotation%20burden%20in%20medical%20image%20segmentation.%20However%2C%20the%20inherent%20sparsity%20of%20these%20annotations%20introduces%20significant%20ambiguity%2C%20which%20results%20in%20noisy%20pseudo-label%20propagation%20and%20hinders%20the%20learning%20of%20robust%20anatomical%20boundaries.%20To%20address%20this%20challenge%2C%20we%20propose%20SDT-Net%2C%20a%20novel%20dual-teacher%2C%20single-student%20framework%20designed%20to%20maximize%20supervision%20quality%20from%20these%20weak%20signals.%20Our%20method%20features%20a%20Dynamic%20Teacher%20Switching%20%28DTS%29%20module%20to%20adaptively%20select%20the%20most%20reliable%20teacher.%20This%20selected%20teacher%20then%20guides%20the%20student%20via%20two%20synergistic%20mechanisms%3A%20high-confidence%20pseudo-labels%2C%20refined%20by%20a%20Pick%20Reliable%20Pixels%20%28PRP%29%20mechanism%2C%20and%20multi-level%20feature%20alignment%2C%20enforced%20by%20a%20Hierarchical%20Consistency%20%28HiCo%29%20module.%20Extensive%20experiments%20on%20the%20ACDC%20and%20MSCMRseg%20datasets%20demonstrate%20that%20SDT-Net%20achieves%20state-of-the-art%20performance%2C%20producing%20more%20accurate%20and%20anatomically%20plausible%20segmentation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14563v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScribble-Supervised%2520Medical%2520Image%2520Segmentation%2520with%2520Dynamic%2520Teacher%2520Switching%2520and%2520Hierarchical%2520Consistency%26entry.906535625%3DThanh-Huy%2520Nguyen%2520and%2520Hoang-Loc%2520Cao%2520and%2520Dat%2520T.%2520Chung%2520and%2520Mai-Anh%2520Vu%2520and%2520Thanh-Minh%2520Nguyen%2520and%2520Minh%2520Le%2520and%2520Phat%2520K.%2520Huynh%2520and%2520Ulas%2520Bagci%26entry.1292438233%3DScribble-supervised%2520methods%2520have%2520emerged%2520to%2520mitigate%2520the%2520prohibitive%2520annotation%2520burden%2520in%2520medical%2520image%2520segmentation.%2520However%252C%2520the%2520inherent%2520sparsity%2520of%2520these%2520annotations%2520introduces%2520significant%2520ambiguity%252C%2520which%2520results%2520in%2520noisy%2520pseudo-label%2520propagation%2520and%2520hinders%2520the%2520learning%2520of%2520robust%2520anatomical%2520boundaries.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520SDT-Net%252C%2520a%2520novel%2520dual-teacher%252C%2520single-student%2520framework%2520designed%2520to%2520maximize%2520supervision%2520quality%2520from%2520these%2520weak%2520signals.%2520Our%2520method%2520features%2520a%2520Dynamic%2520Teacher%2520Switching%2520%2528DTS%2529%2520module%2520to%2520adaptively%2520select%2520the%2520most%2520reliable%2520teacher.%2520This%2520selected%2520teacher%2520then%2520guides%2520the%2520student%2520via%2520two%2520synergistic%2520mechanisms%253A%2520high-confidence%2520pseudo-labels%252C%2520refined%2520by%2520a%2520Pick%2520Reliable%2520Pixels%2520%2528PRP%2529%2520mechanism%252C%2520and%2520multi-level%2520feature%2520alignment%252C%2520enforced%2520by%2520a%2520Hierarchical%2520Consistency%2520%2528HiCo%2529%2520module.%2520Extensive%2520experiments%2520on%2520the%2520ACDC%2520and%2520MSCMRseg%2520datasets%2520demonstrate%2520that%2520SDT-Net%2520achieves%2520state-of-the-art%2520performance%252C%2520producing%2520more%2520accurate%2520and%2520anatomically%2520plausible%2520segmentation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14563v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scribble-Supervised%20Medical%20Image%20Segmentation%20with%20Dynamic%20Teacher%20Switching%20and%20Hierarchical%20Consistency&entry.906535625=Thanh-Huy%20Nguyen%20and%20Hoang-Loc%20Cao%20and%20Dat%20T.%20Chung%20and%20Mai-Anh%20Vu%20and%20Thanh-Minh%20Nguyen%20and%20Minh%20Le%20and%20Phat%20K.%20Huynh%20and%20Ulas%20Bagci&entry.1292438233=Scribble-supervised%20methods%20have%20emerged%20to%20mitigate%20the%20prohibitive%20annotation%20burden%20in%20medical%20image%20segmentation.%20However%2C%20the%20inherent%20sparsity%20of%20these%20annotations%20introduces%20significant%20ambiguity%2C%20which%20results%20in%20noisy%20pseudo-label%20propagation%20and%20hinders%20the%20learning%20of%20robust%20anatomical%20boundaries.%20To%20address%20this%20challenge%2C%20we%20propose%20SDT-Net%2C%20a%20novel%20dual-teacher%2C%20single-student%20framework%20designed%20to%20maximize%20supervision%20quality%20from%20these%20weak%20signals.%20Our%20method%20features%20a%20Dynamic%20Teacher%20Switching%20%28DTS%29%20module%20to%20adaptively%20select%20the%20most%20reliable%20teacher.%20This%20selected%20teacher%20then%20guides%20the%20student%20via%20two%20synergistic%20mechanisms%3A%20high-confidence%20pseudo-labels%2C%20refined%20by%20a%20Pick%20Reliable%20Pixels%20%28PRP%29%20mechanism%2C%20and%20multi-level%20feature%20alignment%2C%20enforced%20by%20a%20Hierarchical%20Consistency%20%28HiCo%29%20module.%20Extensive%20experiments%20on%20the%20ACDC%20and%20MSCMRseg%20datasets%20demonstrate%20that%20SDT-Net%20achieves%20state-of-the-art%20performance%2C%20producing%20more%20accurate%20and%20anatomically%20plausible%20segmentation.&entry.1838667208=http%3A//arxiv.org/abs/2601.14563v3&entry.124074799=Read"},
{"title": "UltraFlwr -- An Efficient Federated Surgical Object Detection Framework", "author": "Yang Li and Soumya Snigdha Kundu and Maxence Boels and Toktam Mahmoodi and Sebastien Ourselin and Tom Vercauteren and Prokar Dasgupta and Jonathan Shapey and Alejandro Granados", "abstract": "Surgical object detection in laparoscopic videos enables real-time instrument identification for workflow analysis and skills assessment, but training robust models such as You Only Look Once (YOLO) is challenged by limited data, privacy constraints, and inter-institutional variability. Federated learning (FL) enables collaborative training without sharing raw data, yet practical support for modern YOLO pipelines under heterogeneous surgical data remains limited. We present UltraFlwr, an open-source, communication-efficient, and edge-deployable framework that integrates Ultralytics YOLO with the Flower FL platform and supports native Partial Aggregation (PA) of YOLO components (backbone, neck, head). Using two public laparoscopic surgical tool detection datasets, we conduct a systematic empirical study of federated YOLO training under Independent and Identically Distributed (IID) and multiple clinically motivated heterogeneous scenarios, including differences in data curation, video length, and label availability. Results show that standard FL aggregators (e.g., FedAvg) do not consistently match centralized training per client, but reduce inter-client performance variability. Aggregating both backbone and neck components achieves performance comparable to full aggregation with lower communication costs. Also, improving within-client data consistency can benefit FL even when it increases distribution shift across clients. These findings provide practical guidance for deploying federated YOLO-based object detection in heterogeneous surgical environments. UltraFlwr is publicly available at https://github.com/KCL-BMEIS/UltraFlwr.", "link": "http://arxiv.org/abs/2503.15161v2", "date": "2026-01-23", "relevancy": 2.0645, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5538}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5154}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UltraFlwr%20--%20An%20Efficient%20Federated%20Surgical%20Object%20Detection%20Framework&body=Title%3A%20UltraFlwr%20--%20An%20Efficient%20Federated%20Surgical%20Object%20Detection%20Framework%0AAuthor%3A%20Yang%20Li%20and%20Soumya%20Snigdha%20Kundu%20and%20Maxence%20Boels%20and%20Toktam%20Mahmoodi%20and%20Sebastien%20Ourselin%20and%20Tom%20Vercauteren%20and%20Prokar%20Dasgupta%20and%20Jonathan%20Shapey%20and%20Alejandro%20Granados%0AAbstract%3A%20Surgical%20object%20detection%20in%20laparoscopic%20videos%20enables%20real-time%20instrument%20identification%20for%20workflow%20analysis%20and%20skills%20assessment%2C%20but%20training%20robust%20models%20such%20as%20You%20Only%20Look%20Once%20%28YOLO%29%20is%20challenged%20by%20limited%20data%2C%20privacy%20constraints%2C%20and%20inter-institutional%20variability.%20Federated%20learning%20%28FL%29%20enables%20collaborative%20training%20without%20sharing%20raw%20data%2C%20yet%20practical%20support%20for%20modern%20YOLO%20pipelines%20under%20heterogeneous%20surgical%20data%20remains%20limited.%20We%20present%20UltraFlwr%2C%20an%20open-source%2C%20communication-efficient%2C%20and%20edge-deployable%20framework%20that%20integrates%20Ultralytics%20YOLO%20with%20the%20Flower%20FL%20platform%20and%20supports%20native%20Partial%20Aggregation%20%28PA%29%20of%20YOLO%20components%20%28backbone%2C%20neck%2C%20head%29.%20Using%20two%20public%20laparoscopic%20surgical%20tool%20detection%20datasets%2C%20we%20conduct%20a%20systematic%20empirical%20study%20of%20federated%20YOLO%20training%20under%20Independent%20and%20Identically%20Distributed%20%28IID%29%20and%20multiple%20clinically%20motivated%20heterogeneous%20scenarios%2C%20including%20differences%20in%20data%20curation%2C%20video%20length%2C%20and%20label%20availability.%20Results%20show%20that%20standard%20FL%20aggregators%20%28e.g.%2C%20FedAvg%29%20do%20not%20consistently%20match%20centralized%20training%20per%20client%2C%20but%20reduce%20inter-client%20performance%20variability.%20Aggregating%20both%20backbone%20and%20neck%20components%20achieves%20performance%20comparable%20to%20full%20aggregation%20with%20lower%20communication%20costs.%20Also%2C%20improving%20within-client%20data%20consistency%20can%20benefit%20FL%20even%20when%20it%20increases%20distribution%20shift%20across%20clients.%20These%20findings%20provide%20practical%20guidance%20for%20deploying%20federated%20YOLO-based%20object%20detection%20in%20heterogeneous%20surgical%20environments.%20UltraFlwr%20is%20publicly%20available%20at%20https%3A//github.com/KCL-BMEIS/UltraFlwr.%0ALink%3A%20http%3A//arxiv.org/abs/2503.15161v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltraFlwr%2520--%2520An%2520Efficient%2520Federated%2520Surgical%2520Object%2520Detection%2520Framework%26entry.906535625%3DYang%2520Li%2520and%2520Soumya%2520Snigdha%2520Kundu%2520and%2520Maxence%2520Boels%2520and%2520Toktam%2520Mahmoodi%2520and%2520Sebastien%2520Ourselin%2520and%2520Tom%2520Vercauteren%2520and%2520Prokar%2520Dasgupta%2520and%2520Jonathan%2520Shapey%2520and%2520Alejandro%2520Granados%26entry.1292438233%3DSurgical%2520object%2520detection%2520in%2520laparoscopic%2520videos%2520enables%2520real-time%2520instrument%2520identification%2520for%2520workflow%2520analysis%2520and%2520skills%2520assessment%252C%2520but%2520training%2520robust%2520models%2520such%2520as%2520You%2520Only%2520Look%2520Once%2520%2528YOLO%2529%2520is%2520challenged%2520by%2520limited%2520data%252C%2520privacy%2520constraints%252C%2520and%2520inter-institutional%2520variability.%2520Federated%2520learning%2520%2528FL%2529%2520enables%2520collaborative%2520training%2520without%2520sharing%2520raw%2520data%252C%2520yet%2520practical%2520support%2520for%2520modern%2520YOLO%2520pipelines%2520under%2520heterogeneous%2520surgical%2520data%2520remains%2520limited.%2520We%2520present%2520UltraFlwr%252C%2520an%2520open-source%252C%2520communication-efficient%252C%2520and%2520edge-deployable%2520framework%2520that%2520integrates%2520Ultralytics%2520YOLO%2520with%2520the%2520Flower%2520FL%2520platform%2520and%2520supports%2520native%2520Partial%2520Aggregation%2520%2528PA%2529%2520of%2520YOLO%2520components%2520%2528backbone%252C%2520neck%252C%2520head%2529.%2520Using%2520two%2520public%2520laparoscopic%2520surgical%2520tool%2520detection%2520datasets%252C%2520we%2520conduct%2520a%2520systematic%2520empirical%2520study%2520of%2520federated%2520YOLO%2520training%2520under%2520Independent%2520and%2520Identically%2520Distributed%2520%2528IID%2529%2520and%2520multiple%2520clinically%2520motivated%2520heterogeneous%2520scenarios%252C%2520including%2520differences%2520in%2520data%2520curation%252C%2520video%2520length%252C%2520and%2520label%2520availability.%2520Results%2520show%2520that%2520standard%2520FL%2520aggregators%2520%2528e.g.%252C%2520FedAvg%2529%2520do%2520not%2520consistently%2520match%2520centralized%2520training%2520per%2520client%252C%2520but%2520reduce%2520inter-client%2520performance%2520variability.%2520Aggregating%2520both%2520backbone%2520and%2520neck%2520components%2520achieves%2520performance%2520comparable%2520to%2520full%2520aggregation%2520with%2520lower%2520communication%2520costs.%2520Also%252C%2520improving%2520within-client%2520data%2520consistency%2520can%2520benefit%2520FL%2520even%2520when%2520it%2520increases%2520distribution%2520shift%2520across%2520clients.%2520These%2520findings%2520provide%2520practical%2520guidance%2520for%2520deploying%2520federated%2520YOLO-based%2520object%2520detection%2520in%2520heterogeneous%2520surgical%2520environments.%2520UltraFlwr%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/KCL-BMEIS/UltraFlwr.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.15161v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UltraFlwr%20--%20An%20Efficient%20Federated%20Surgical%20Object%20Detection%20Framework&entry.906535625=Yang%20Li%20and%20Soumya%20Snigdha%20Kundu%20and%20Maxence%20Boels%20and%20Toktam%20Mahmoodi%20and%20Sebastien%20Ourselin%20and%20Tom%20Vercauteren%20and%20Prokar%20Dasgupta%20and%20Jonathan%20Shapey%20and%20Alejandro%20Granados&entry.1292438233=Surgical%20object%20detection%20in%20laparoscopic%20videos%20enables%20real-time%20instrument%20identification%20for%20workflow%20analysis%20and%20skills%20assessment%2C%20but%20training%20robust%20models%20such%20as%20You%20Only%20Look%20Once%20%28YOLO%29%20is%20challenged%20by%20limited%20data%2C%20privacy%20constraints%2C%20and%20inter-institutional%20variability.%20Federated%20learning%20%28FL%29%20enables%20collaborative%20training%20without%20sharing%20raw%20data%2C%20yet%20practical%20support%20for%20modern%20YOLO%20pipelines%20under%20heterogeneous%20surgical%20data%20remains%20limited.%20We%20present%20UltraFlwr%2C%20an%20open-source%2C%20communication-efficient%2C%20and%20edge-deployable%20framework%20that%20integrates%20Ultralytics%20YOLO%20with%20the%20Flower%20FL%20platform%20and%20supports%20native%20Partial%20Aggregation%20%28PA%29%20of%20YOLO%20components%20%28backbone%2C%20neck%2C%20head%29.%20Using%20two%20public%20laparoscopic%20surgical%20tool%20detection%20datasets%2C%20we%20conduct%20a%20systematic%20empirical%20study%20of%20federated%20YOLO%20training%20under%20Independent%20and%20Identically%20Distributed%20%28IID%29%20and%20multiple%20clinically%20motivated%20heterogeneous%20scenarios%2C%20including%20differences%20in%20data%20curation%2C%20video%20length%2C%20and%20label%20availability.%20Results%20show%20that%20standard%20FL%20aggregators%20%28e.g.%2C%20FedAvg%29%20do%20not%20consistently%20match%20centralized%20training%20per%20client%2C%20but%20reduce%20inter-client%20performance%20variability.%20Aggregating%20both%20backbone%20and%20neck%20components%20achieves%20performance%20comparable%20to%20full%20aggregation%20with%20lower%20communication%20costs.%20Also%2C%20improving%20within-client%20data%20consistency%20can%20benefit%20FL%20even%20when%20it%20increases%20distribution%20shift%20across%20clients.%20These%20findings%20provide%20practical%20guidance%20for%20deploying%20federated%20YOLO-based%20object%20detection%20in%20heterogeneous%20surgical%20environments.%20UltraFlwr%20is%20publicly%20available%20at%20https%3A//github.com/KCL-BMEIS/UltraFlwr.&entry.1838667208=http%3A//arxiv.org/abs/2503.15161v2&entry.124074799=Read"},
{"title": "Beyond the LUMIR challenge: The pathway to foundational registration models", "author": "Junyu Chen and Shuwen Wei and Joel Honkamaa and Pekka Marttinen and Hang Zhang and Min Liu and Yichao Zhou and Zuopeng Tan and Zhuoyuan Wang and Yi Wang and Hongchao Zhou and Shunbo Hu and Yi Zhang and Qian Tao and Lukas F\u00f6rner and Thomas Wendler and Bailiang Jian and Benedikt Wiestler and Tim Hable and Jin Kim and Dan Ruan and Frederic Madesta and Thilo Sentker and Wiebke Heyer and Lianrui Zuo and Yuwei Dai and Jing Wu and Jerry L. Prince and Harrison Bai and Yong Du and Yihao Liu and Alessa Hering and Reuben Dorent and Lasse Hansen and Mattias P. Heinrich and Aaron Carass", "abstract": "Medical image challenges have played a transformative role in advancing the field, catalyzing innovation and establishing new performance benchmarks. Image registration, a foundational task in neuroimaging, has similarly advanced through the Learn2Reg initiative. Building on this, we introduce the Large-scale Unsupervised Brain MRI Image Registration (LUMIR) challenge, a next-generation benchmark for unsupervised brain MRI registration. Previous challenges relied upon anatomical label maps, however LUMIR provides 4,014 unlabeled T1-weighted MRIs for training, encouraging biologically plausible deformation modeling through self-supervision. Evaluation includes 590 in-domain test subjects and extensive zero-shot tasks across disease populations, imaging protocols, and species. Deep learning methods consistently achieved state-of-the-art performance and produced anatomically plausible, diffeomorphic deformation fields. They outperformed several leading optimization-based methods and remained robust to most domain shifts. These findings highlight the growing maturity of deep learning in neuroimaging registration and its potential to serve as a foundation model for general-purpose medical image registration.", "link": "http://arxiv.org/abs/2505.24160v2", "date": "2026-01-23", "relevancy": 2.0555, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.529}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5155}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20LUMIR%20challenge%3A%20The%20pathway%20to%20foundational%20registration%20models&body=Title%3A%20Beyond%20the%20LUMIR%20challenge%3A%20The%20pathway%20to%20foundational%20registration%20models%0AAuthor%3A%20Junyu%20Chen%20and%20Shuwen%20Wei%20and%20Joel%20Honkamaa%20and%20Pekka%20Marttinen%20and%20Hang%20Zhang%20and%20Min%20Liu%20and%20Yichao%20Zhou%20and%20Zuopeng%20Tan%20and%20Zhuoyuan%20Wang%20and%20Yi%20Wang%20and%20Hongchao%20Zhou%20and%20Shunbo%20Hu%20and%20Yi%20Zhang%20and%20Qian%20Tao%20and%20Lukas%20F%C3%B6rner%20and%20Thomas%20Wendler%20and%20Bailiang%20Jian%20and%20Benedikt%20Wiestler%20and%20Tim%20Hable%20and%20Jin%20Kim%20and%20Dan%20Ruan%20and%20Frederic%20Madesta%20and%20Thilo%20Sentker%20and%20Wiebke%20Heyer%20and%20Lianrui%20Zuo%20and%20Yuwei%20Dai%20and%20Jing%20Wu%20and%20Jerry%20L.%20Prince%20and%20Harrison%20Bai%20and%20Yong%20Du%20and%20Yihao%20Liu%20and%20Alessa%20Hering%20and%20Reuben%20Dorent%20and%20Lasse%20Hansen%20and%20Mattias%20P.%20Heinrich%20and%20Aaron%20Carass%0AAbstract%3A%20Medical%20image%20challenges%20have%20played%20a%20transformative%20role%20in%20advancing%20the%20field%2C%20catalyzing%20innovation%20and%20establishing%20new%20performance%20benchmarks.%20Image%20registration%2C%20a%20foundational%20task%20in%20neuroimaging%2C%20has%20similarly%20advanced%20through%20the%20Learn2Reg%20initiative.%20Building%20on%20this%2C%20we%20introduce%20the%20Large-scale%20Unsupervised%20Brain%20MRI%20Image%20Registration%20%28LUMIR%29%20challenge%2C%20a%20next-generation%20benchmark%20for%20unsupervised%20brain%20MRI%20registration.%20Previous%20challenges%20relied%20upon%20anatomical%20label%20maps%2C%20however%20LUMIR%20provides%204%2C014%20unlabeled%20T1-weighted%20MRIs%20for%20training%2C%20encouraging%20biologically%20plausible%20deformation%20modeling%20through%20self-supervision.%20Evaluation%20includes%20590%20in-domain%20test%20subjects%20and%20extensive%20zero-shot%20tasks%20across%20disease%20populations%2C%20imaging%20protocols%2C%20and%20species.%20Deep%20learning%20methods%20consistently%20achieved%20state-of-the-art%20performance%20and%20produced%20anatomically%20plausible%2C%20diffeomorphic%20deformation%20fields.%20They%20outperformed%20several%20leading%20optimization-based%20methods%20and%20remained%20robust%20to%20most%20domain%20shifts.%20These%20findings%20highlight%20the%20growing%20maturity%20of%20deep%20learning%20in%20neuroimaging%20registration%20and%20its%20potential%20to%20serve%20as%20a%20foundation%20model%20for%20general-purpose%20medical%20image%20registration.%0ALink%3A%20http%3A//arxiv.org/abs/2505.24160v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520LUMIR%2520challenge%253A%2520The%2520pathway%2520to%2520foundational%2520registration%2520models%26entry.906535625%3DJunyu%2520Chen%2520and%2520Shuwen%2520Wei%2520and%2520Joel%2520Honkamaa%2520and%2520Pekka%2520Marttinen%2520and%2520Hang%2520Zhang%2520and%2520Min%2520Liu%2520and%2520Yichao%2520Zhou%2520and%2520Zuopeng%2520Tan%2520and%2520Zhuoyuan%2520Wang%2520and%2520Yi%2520Wang%2520and%2520Hongchao%2520Zhou%2520and%2520Shunbo%2520Hu%2520and%2520Yi%2520Zhang%2520and%2520Qian%2520Tao%2520and%2520Lukas%2520F%25C3%25B6rner%2520and%2520Thomas%2520Wendler%2520and%2520Bailiang%2520Jian%2520and%2520Benedikt%2520Wiestler%2520and%2520Tim%2520Hable%2520and%2520Jin%2520Kim%2520and%2520Dan%2520Ruan%2520and%2520Frederic%2520Madesta%2520and%2520Thilo%2520Sentker%2520and%2520Wiebke%2520Heyer%2520and%2520Lianrui%2520Zuo%2520and%2520Yuwei%2520Dai%2520and%2520Jing%2520Wu%2520and%2520Jerry%2520L.%2520Prince%2520and%2520Harrison%2520Bai%2520and%2520Yong%2520Du%2520and%2520Yihao%2520Liu%2520and%2520Alessa%2520Hering%2520and%2520Reuben%2520Dorent%2520and%2520Lasse%2520Hansen%2520and%2520Mattias%2520P.%2520Heinrich%2520and%2520Aaron%2520Carass%26entry.1292438233%3DMedical%2520image%2520challenges%2520have%2520played%2520a%2520transformative%2520role%2520in%2520advancing%2520the%2520field%252C%2520catalyzing%2520innovation%2520and%2520establishing%2520new%2520performance%2520benchmarks.%2520Image%2520registration%252C%2520a%2520foundational%2520task%2520in%2520neuroimaging%252C%2520has%2520similarly%2520advanced%2520through%2520the%2520Learn2Reg%2520initiative.%2520Building%2520on%2520this%252C%2520we%2520introduce%2520the%2520Large-scale%2520Unsupervised%2520Brain%2520MRI%2520Image%2520Registration%2520%2528LUMIR%2529%2520challenge%252C%2520a%2520next-generation%2520benchmark%2520for%2520unsupervised%2520brain%2520MRI%2520registration.%2520Previous%2520challenges%2520relied%2520upon%2520anatomical%2520label%2520maps%252C%2520however%2520LUMIR%2520provides%25204%252C014%2520unlabeled%2520T1-weighted%2520MRIs%2520for%2520training%252C%2520encouraging%2520biologically%2520plausible%2520deformation%2520modeling%2520through%2520self-supervision.%2520Evaluation%2520includes%2520590%2520in-domain%2520test%2520subjects%2520and%2520extensive%2520zero-shot%2520tasks%2520across%2520disease%2520populations%252C%2520imaging%2520protocols%252C%2520and%2520species.%2520Deep%2520learning%2520methods%2520consistently%2520achieved%2520state-of-the-art%2520performance%2520and%2520produced%2520anatomically%2520plausible%252C%2520diffeomorphic%2520deformation%2520fields.%2520They%2520outperformed%2520several%2520leading%2520optimization-based%2520methods%2520and%2520remained%2520robust%2520to%2520most%2520domain%2520shifts.%2520These%2520findings%2520highlight%2520the%2520growing%2520maturity%2520of%2520deep%2520learning%2520in%2520neuroimaging%2520registration%2520and%2520its%2520potential%2520to%2520serve%2520as%2520a%2520foundation%2520model%2520for%2520general-purpose%2520medical%2520image%2520registration.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24160v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20LUMIR%20challenge%3A%20The%20pathway%20to%20foundational%20registration%20models&entry.906535625=Junyu%20Chen%20and%20Shuwen%20Wei%20and%20Joel%20Honkamaa%20and%20Pekka%20Marttinen%20and%20Hang%20Zhang%20and%20Min%20Liu%20and%20Yichao%20Zhou%20and%20Zuopeng%20Tan%20and%20Zhuoyuan%20Wang%20and%20Yi%20Wang%20and%20Hongchao%20Zhou%20and%20Shunbo%20Hu%20and%20Yi%20Zhang%20and%20Qian%20Tao%20and%20Lukas%20F%C3%B6rner%20and%20Thomas%20Wendler%20and%20Bailiang%20Jian%20and%20Benedikt%20Wiestler%20and%20Tim%20Hable%20and%20Jin%20Kim%20and%20Dan%20Ruan%20and%20Frederic%20Madesta%20and%20Thilo%20Sentker%20and%20Wiebke%20Heyer%20and%20Lianrui%20Zuo%20and%20Yuwei%20Dai%20and%20Jing%20Wu%20and%20Jerry%20L.%20Prince%20and%20Harrison%20Bai%20and%20Yong%20Du%20and%20Yihao%20Liu%20and%20Alessa%20Hering%20and%20Reuben%20Dorent%20and%20Lasse%20Hansen%20and%20Mattias%20P.%20Heinrich%20and%20Aaron%20Carass&entry.1292438233=Medical%20image%20challenges%20have%20played%20a%20transformative%20role%20in%20advancing%20the%20field%2C%20catalyzing%20innovation%20and%20establishing%20new%20performance%20benchmarks.%20Image%20registration%2C%20a%20foundational%20task%20in%20neuroimaging%2C%20has%20similarly%20advanced%20through%20the%20Learn2Reg%20initiative.%20Building%20on%20this%2C%20we%20introduce%20the%20Large-scale%20Unsupervised%20Brain%20MRI%20Image%20Registration%20%28LUMIR%29%20challenge%2C%20a%20next-generation%20benchmark%20for%20unsupervised%20brain%20MRI%20registration.%20Previous%20challenges%20relied%20upon%20anatomical%20label%20maps%2C%20however%20LUMIR%20provides%204%2C014%20unlabeled%20T1-weighted%20MRIs%20for%20training%2C%20encouraging%20biologically%20plausible%20deformation%20modeling%20through%20self-supervision.%20Evaluation%20includes%20590%20in-domain%20test%20subjects%20and%20extensive%20zero-shot%20tasks%20across%20disease%20populations%2C%20imaging%20protocols%2C%20and%20species.%20Deep%20learning%20methods%20consistently%20achieved%20state-of-the-art%20performance%20and%20produced%20anatomically%20plausible%2C%20diffeomorphic%20deformation%20fields.%20They%20outperformed%20several%20leading%20optimization-based%20methods%20and%20remained%20robust%20to%20most%20domain%20shifts.%20These%20findings%20highlight%20the%20growing%20maturity%20of%20deep%20learning%20in%20neuroimaging%20registration%20and%20its%20potential%20to%20serve%20as%20a%20foundation%20model%20for%20general-purpose%20medical%20image%20registration.&entry.1838667208=http%3A//arxiv.org/abs/2505.24160v2&entry.124074799=Read"},
{"title": "Programming over Thinking: Efficient and Robust Multi-Constraint Planning", "author": "Derrick Goh Xin Deik and Quanyu Long and Zhengyuan Liu and Nancy F. Chen and Wenya Wang", "abstract": "Multi-constraint planning involves identifying, evaluating, and refining candidate plans while satisfying multiple, potentially conflicting constraints. Existing large language model (LLM) approaches face fundamental limitations in this domain. Pure reasoning paradigms, which rely on long natural language chains, are prone to inconsistency, error accumulation, and prohibitive cost as constraints compound. Conversely, LLMs combined with coding- or solver-based strategies lack flexibility: they often generate problem-specific code from scratch or depend on fixed solvers, failing to capture generalizable logic across diverse problems. To address these challenges, we introduce the Scalable COde Planning Engine (SCOPE), a framework that disentangles query-specific reasoning from generic code execution. By separating reasoning from execution, SCOPE produces solver functions that are consistent, deterministic, and reusable across queries while requiring only minimal changes to input parameters. SCOPE achieves state-of-the-art performance while lowering cost and latency. For example, with GPT-4o, it reaches 93.1% success on TravelPlanner, a 61.6% gain over the best baseline (CoT) while cutting inference cost by 1.4x and time by ~4.67x. Code is available at https://github.com/DerrickGXD/SCOPE.", "link": "http://arxiv.org/abs/2601.09097v2", "date": "2026-01-23", "relevancy": 2.0509, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5226}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5107}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Programming%20over%20Thinking%3A%20Efficient%20and%20Robust%20Multi-Constraint%20Planning&body=Title%3A%20Programming%20over%20Thinking%3A%20Efficient%20and%20Robust%20Multi-Constraint%20Planning%0AAuthor%3A%20Derrick%20Goh%20Xin%20Deik%20and%20Quanyu%20Long%20and%20Zhengyuan%20Liu%20and%20Nancy%20F.%20Chen%20and%20Wenya%20Wang%0AAbstract%3A%20Multi-constraint%20planning%20involves%20identifying%2C%20evaluating%2C%20and%20refining%20candidate%20plans%20while%20satisfying%20multiple%2C%20potentially%20conflicting%20constraints.%20Existing%20large%20language%20model%20%28LLM%29%20approaches%20face%20fundamental%20limitations%20in%20this%20domain.%20Pure%20reasoning%20paradigms%2C%20which%20rely%20on%20long%20natural%20language%20chains%2C%20are%20prone%20to%20inconsistency%2C%20error%20accumulation%2C%20and%20prohibitive%20cost%20as%20constraints%20compound.%20Conversely%2C%20LLMs%20combined%20with%20coding-%20or%20solver-based%20strategies%20lack%20flexibility%3A%20they%20often%20generate%20problem-specific%20code%20from%20scratch%20or%20depend%20on%20fixed%20solvers%2C%20failing%20to%20capture%20generalizable%20logic%20across%20diverse%20problems.%20To%20address%20these%20challenges%2C%20we%20introduce%20the%20Scalable%20COde%20Planning%20Engine%20%28SCOPE%29%2C%20a%20framework%20that%20disentangles%20query-specific%20reasoning%20from%20generic%20code%20execution.%20By%20separating%20reasoning%20from%20execution%2C%20SCOPE%20produces%20solver%20functions%20that%20are%20consistent%2C%20deterministic%2C%20and%20reusable%20across%20queries%20while%20requiring%20only%20minimal%20changes%20to%20input%20parameters.%20SCOPE%20achieves%20state-of-the-art%20performance%20while%20lowering%20cost%20and%20latency.%20For%20example%2C%20with%20GPT-4o%2C%20it%20reaches%2093.1%25%20success%20on%20TravelPlanner%2C%20a%2061.6%25%20gain%20over%20the%20best%20baseline%20%28CoT%29%20while%20cutting%20inference%20cost%20by%201.4x%20and%20time%20by%20~4.67x.%20Code%20is%20available%20at%20https%3A//github.com/DerrickGXD/SCOPE.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09097v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgramming%2520over%2520Thinking%253A%2520Efficient%2520and%2520Robust%2520Multi-Constraint%2520Planning%26entry.906535625%3DDerrick%2520Goh%2520Xin%2520Deik%2520and%2520Quanyu%2520Long%2520and%2520Zhengyuan%2520Liu%2520and%2520Nancy%2520F.%2520Chen%2520and%2520Wenya%2520Wang%26entry.1292438233%3DMulti-constraint%2520planning%2520involves%2520identifying%252C%2520evaluating%252C%2520and%2520refining%2520candidate%2520plans%2520while%2520satisfying%2520multiple%252C%2520potentially%2520conflicting%2520constraints.%2520Existing%2520large%2520language%2520model%2520%2528LLM%2529%2520approaches%2520face%2520fundamental%2520limitations%2520in%2520this%2520domain.%2520Pure%2520reasoning%2520paradigms%252C%2520which%2520rely%2520on%2520long%2520natural%2520language%2520chains%252C%2520are%2520prone%2520to%2520inconsistency%252C%2520error%2520accumulation%252C%2520and%2520prohibitive%2520cost%2520as%2520constraints%2520compound.%2520Conversely%252C%2520LLMs%2520combined%2520with%2520coding-%2520or%2520solver-based%2520strategies%2520lack%2520flexibility%253A%2520they%2520often%2520generate%2520problem-specific%2520code%2520from%2520scratch%2520or%2520depend%2520on%2520fixed%2520solvers%252C%2520failing%2520to%2520capture%2520generalizable%2520logic%2520across%2520diverse%2520problems.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520the%2520Scalable%2520COde%2520Planning%2520Engine%2520%2528SCOPE%2529%252C%2520a%2520framework%2520that%2520disentangles%2520query-specific%2520reasoning%2520from%2520generic%2520code%2520execution.%2520By%2520separating%2520reasoning%2520from%2520execution%252C%2520SCOPE%2520produces%2520solver%2520functions%2520that%2520are%2520consistent%252C%2520deterministic%252C%2520and%2520reusable%2520across%2520queries%2520while%2520requiring%2520only%2520minimal%2520changes%2520to%2520input%2520parameters.%2520SCOPE%2520achieves%2520state-of-the-art%2520performance%2520while%2520lowering%2520cost%2520and%2520latency.%2520For%2520example%252C%2520with%2520GPT-4o%252C%2520it%2520reaches%252093.1%2525%2520success%2520on%2520TravelPlanner%252C%2520a%252061.6%2525%2520gain%2520over%2520the%2520best%2520baseline%2520%2528CoT%2529%2520while%2520cutting%2520inference%2520cost%2520by%25201.4x%2520and%2520time%2520by%2520~4.67x.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/DerrickGXD/SCOPE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09097v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Programming%20over%20Thinking%3A%20Efficient%20and%20Robust%20Multi-Constraint%20Planning&entry.906535625=Derrick%20Goh%20Xin%20Deik%20and%20Quanyu%20Long%20and%20Zhengyuan%20Liu%20and%20Nancy%20F.%20Chen%20and%20Wenya%20Wang&entry.1292438233=Multi-constraint%20planning%20involves%20identifying%2C%20evaluating%2C%20and%20refining%20candidate%20plans%20while%20satisfying%20multiple%2C%20potentially%20conflicting%20constraints.%20Existing%20large%20language%20model%20%28LLM%29%20approaches%20face%20fundamental%20limitations%20in%20this%20domain.%20Pure%20reasoning%20paradigms%2C%20which%20rely%20on%20long%20natural%20language%20chains%2C%20are%20prone%20to%20inconsistency%2C%20error%20accumulation%2C%20and%20prohibitive%20cost%20as%20constraints%20compound.%20Conversely%2C%20LLMs%20combined%20with%20coding-%20or%20solver-based%20strategies%20lack%20flexibility%3A%20they%20often%20generate%20problem-specific%20code%20from%20scratch%20or%20depend%20on%20fixed%20solvers%2C%20failing%20to%20capture%20generalizable%20logic%20across%20diverse%20problems.%20To%20address%20these%20challenges%2C%20we%20introduce%20the%20Scalable%20COde%20Planning%20Engine%20%28SCOPE%29%2C%20a%20framework%20that%20disentangles%20query-specific%20reasoning%20from%20generic%20code%20execution.%20By%20separating%20reasoning%20from%20execution%2C%20SCOPE%20produces%20solver%20functions%20that%20are%20consistent%2C%20deterministic%2C%20and%20reusable%20across%20queries%20while%20requiring%20only%20minimal%20changes%20to%20input%20parameters.%20SCOPE%20achieves%20state-of-the-art%20performance%20while%20lowering%20cost%20and%20latency.%20For%20example%2C%20with%20GPT-4o%2C%20it%20reaches%2093.1%25%20success%20on%20TravelPlanner%2C%20a%2061.6%25%20gain%20over%20the%20best%20baseline%20%28CoT%29%20while%20cutting%20inference%20cost%20by%201.4x%20and%20time%20by%20~4.67x.%20Code%20is%20available%20at%20https%3A//github.com/DerrickGXD/SCOPE.&entry.1838667208=http%3A//arxiv.org/abs/2601.09097v2&entry.124074799=Read"},
{"title": "A Lightweight Medical Image Classification Framework via Self-Supervised Contrastive Learning and Quantum-Enhanced Feature Modeling", "author": "Jingsong Xia and Siqi Wang", "abstract": "Intelligent medical image analysis is essential for clinical decision support but is often limited by scarce annotations, constrained computational resources, and suboptimal model generalization. To address these challenges, we propose a lightweight medical image classification framework that integrates self-supervised contrastive learning with quantum-enhanced feature modeling. MobileNetV2 is employed as a compact backbone and pretrained using a SimCLR-style self-supervised paradigm on unlabeled images. A lightweight parameterized quantum circuit (PQC) is embedded as a quantum feature enhancement module, forming a hybrid classical-quantum architecture, which is subsequently fine-tuned on limited labeled data. Experimental results demonstrate that, with only approximately 2-3 million parameters and low computational cost, the proposed method consistently outperforms classical baselines without self-supervised learning or quantum enhancement in terms of Accuracy, AUC, and F1-score. Feature visualization further indicates improved discriminability and representation stability. Overall, this work provides a practical and forward-looking solution for high-performance medical artificial intelligence under resource-constrained settings.", "link": "http://arxiv.org/abs/2601.16608v1", "date": "2026-01-23", "relevancy": 2.0447, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5227}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5062}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Lightweight%20Medical%20Image%20Classification%20Framework%20via%20Self-Supervised%20Contrastive%20Learning%20and%20Quantum-Enhanced%20Feature%20Modeling&body=Title%3A%20A%20Lightweight%20Medical%20Image%20Classification%20Framework%20via%20Self-Supervised%20Contrastive%20Learning%20and%20Quantum-Enhanced%20Feature%20Modeling%0AAuthor%3A%20Jingsong%20Xia%20and%20Siqi%20Wang%0AAbstract%3A%20Intelligent%20medical%20image%20analysis%20is%20essential%20for%20clinical%20decision%20support%20but%20is%20often%20limited%20by%20scarce%20annotations%2C%20constrained%20computational%20resources%2C%20and%20suboptimal%20model%20generalization.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20lightweight%20medical%20image%20classification%20framework%20that%20integrates%20self-supervised%20contrastive%20learning%20with%20quantum-enhanced%20feature%20modeling.%20MobileNetV2%20is%20employed%20as%20a%20compact%20backbone%20and%20pretrained%20using%20a%20SimCLR-style%20self-supervised%20paradigm%20on%20unlabeled%20images.%20A%20lightweight%20parameterized%20quantum%20circuit%20%28PQC%29%20is%20embedded%20as%20a%20quantum%20feature%20enhancement%20module%2C%20forming%20a%20hybrid%20classical-quantum%20architecture%2C%20which%20is%20subsequently%20fine-tuned%20on%20limited%20labeled%20data.%20Experimental%20results%20demonstrate%20that%2C%20with%20only%20approximately%202-3%20million%20parameters%20and%20low%20computational%20cost%2C%20the%20proposed%20method%20consistently%20outperforms%20classical%20baselines%20without%20self-supervised%20learning%20or%20quantum%20enhancement%20in%20terms%20of%20Accuracy%2C%20AUC%2C%20and%20F1-score.%20Feature%20visualization%20further%20indicates%20improved%20discriminability%20and%20representation%20stability.%20Overall%2C%20this%20work%20provides%20a%20practical%20and%20forward-looking%20solution%20for%20high-performance%20medical%20artificial%20intelligence%20under%20resource-constrained%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Lightweight%2520Medical%2520Image%2520Classification%2520Framework%2520via%2520Self-Supervised%2520Contrastive%2520Learning%2520and%2520Quantum-Enhanced%2520Feature%2520Modeling%26entry.906535625%3DJingsong%2520Xia%2520and%2520Siqi%2520Wang%26entry.1292438233%3DIntelligent%2520medical%2520image%2520analysis%2520is%2520essential%2520for%2520clinical%2520decision%2520support%2520but%2520is%2520often%2520limited%2520by%2520scarce%2520annotations%252C%2520constrained%2520computational%2520resources%252C%2520and%2520suboptimal%2520model%2520generalization.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520lightweight%2520medical%2520image%2520classification%2520framework%2520that%2520integrates%2520self-supervised%2520contrastive%2520learning%2520with%2520quantum-enhanced%2520feature%2520modeling.%2520MobileNetV2%2520is%2520employed%2520as%2520a%2520compact%2520backbone%2520and%2520pretrained%2520using%2520a%2520SimCLR-style%2520self-supervised%2520paradigm%2520on%2520unlabeled%2520images.%2520A%2520lightweight%2520parameterized%2520quantum%2520circuit%2520%2528PQC%2529%2520is%2520embedded%2520as%2520a%2520quantum%2520feature%2520enhancement%2520module%252C%2520forming%2520a%2520hybrid%2520classical-quantum%2520architecture%252C%2520which%2520is%2520subsequently%2520fine-tuned%2520on%2520limited%2520labeled%2520data.%2520Experimental%2520results%2520demonstrate%2520that%252C%2520with%2520only%2520approximately%25202-3%2520million%2520parameters%2520and%2520low%2520computational%2520cost%252C%2520the%2520proposed%2520method%2520consistently%2520outperforms%2520classical%2520baselines%2520without%2520self-supervised%2520learning%2520or%2520quantum%2520enhancement%2520in%2520terms%2520of%2520Accuracy%252C%2520AUC%252C%2520and%2520F1-score.%2520Feature%2520visualization%2520further%2520indicates%2520improved%2520discriminability%2520and%2520representation%2520stability.%2520Overall%252C%2520this%2520work%2520provides%2520a%2520practical%2520and%2520forward-looking%2520solution%2520for%2520high-performance%2520medical%2520artificial%2520intelligence%2520under%2520resource-constrained%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Lightweight%20Medical%20Image%20Classification%20Framework%20via%20Self-Supervised%20Contrastive%20Learning%20and%20Quantum-Enhanced%20Feature%20Modeling&entry.906535625=Jingsong%20Xia%20and%20Siqi%20Wang&entry.1292438233=Intelligent%20medical%20image%20analysis%20is%20essential%20for%20clinical%20decision%20support%20but%20is%20often%20limited%20by%20scarce%20annotations%2C%20constrained%20computational%20resources%2C%20and%20suboptimal%20model%20generalization.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20lightweight%20medical%20image%20classification%20framework%20that%20integrates%20self-supervised%20contrastive%20learning%20with%20quantum-enhanced%20feature%20modeling.%20MobileNetV2%20is%20employed%20as%20a%20compact%20backbone%20and%20pretrained%20using%20a%20SimCLR-style%20self-supervised%20paradigm%20on%20unlabeled%20images.%20A%20lightweight%20parameterized%20quantum%20circuit%20%28PQC%29%20is%20embedded%20as%20a%20quantum%20feature%20enhancement%20module%2C%20forming%20a%20hybrid%20classical-quantum%20architecture%2C%20which%20is%20subsequently%20fine-tuned%20on%20limited%20labeled%20data.%20Experimental%20results%20demonstrate%20that%2C%20with%20only%20approximately%202-3%20million%20parameters%20and%20low%20computational%20cost%2C%20the%20proposed%20method%20consistently%20outperforms%20classical%20baselines%20without%20self-supervised%20learning%20or%20quantum%20enhancement%20in%20terms%20of%20Accuracy%2C%20AUC%2C%20and%20F1-score.%20Feature%20visualization%20further%20indicates%20improved%20discriminability%20and%20representation%20stability.%20Overall%2C%20this%20work%20provides%20a%20practical%20and%20forward-looking%20solution%20for%20high-performance%20medical%20artificial%20intelligence%20under%20resource-constrained%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2601.16608v1&entry.124074799=Read"},
{"title": "LATTLE: LLM Attention Transplant for Transfer Learning of Tabular Data Across Disparate Domains", "author": "Ibna Kowsar and Kazi F. Akhter and Manar D. Samad", "abstract": "Transfer learning on tabular data is challenging due to disparate feature spaces across domains, in contrast to the homogeneous structures of image and text. Large language models (LLMs) offer a knowledge base to improve the limited effectiveness of cross-domain transfer learning for tabular data. However, LLM performance often stagnates due to subjective text prompts and the computational limitations of in-context learning. We present a novel language-to-tabular context-learning method that uses attention-specific transformer weights, enabling seamless transfer learning across disparate tabular data sets. The LLM attention transplant mechanism facilitates a domain-agnostic transfer learning, eliminating the need for shared features between tables, LLM prompt engineering, and large-scale pretrained models. Our experiments using ten pairs of disjoint source-target data sets and 12 baseline methods demonstrate the superiority of the proposed LLM-attention transplant for transfer learning (LATTLE) method over traditional ML models, state-of-the-art deep tabular architectures, and models trained on thousands to billions of tabular samples. The proposed cross-domain attention transfer demonstrates an effective solution for adapting LLMs to learning non-text tabular data in a low-resource environment. The source code of the LATTLE implementation is publicly available.", "link": "http://arxiv.org/abs/2511.06161v2", "date": "2026-01-23", "relevancy": 2.0415, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5585}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4761}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LATTLE%3A%20LLM%20Attention%20Transplant%20for%20Transfer%20Learning%20of%20Tabular%20Data%20Across%20Disparate%20Domains&body=Title%3A%20LATTLE%3A%20LLM%20Attention%20Transplant%20for%20Transfer%20Learning%20of%20Tabular%20Data%20Across%20Disparate%20Domains%0AAuthor%3A%20Ibna%20Kowsar%20and%20Kazi%20F.%20Akhter%20and%20Manar%20D.%20Samad%0AAbstract%3A%20Transfer%20learning%20on%20tabular%20data%20is%20challenging%20due%20to%20disparate%20feature%20spaces%20across%20domains%2C%20in%20contrast%20to%20the%20homogeneous%20structures%20of%20image%20and%20text.%20Large%20language%20models%20%28LLMs%29%20offer%20a%20knowledge%20base%20to%20improve%20the%20limited%20effectiveness%20of%20cross-domain%20transfer%20learning%20for%20tabular%20data.%20However%2C%20LLM%20performance%20often%20stagnates%20due%20to%20subjective%20text%20prompts%20and%20the%20computational%20limitations%20of%20in-context%20learning.%20We%20present%20a%20novel%20language-to-tabular%20context-learning%20method%20that%20uses%20attention-specific%20transformer%20weights%2C%20enabling%20seamless%20transfer%20learning%20across%20disparate%20tabular%20data%20sets.%20The%20LLM%20attention%20transplant%20mechanism%20facilitates%20a%20domain-agnostic%20transfer%20learning%2C%20eliminating%20the%20need%20for%20shared%20features%20between%20tables%2C%20LLM%20prompt%20engineering%2C%20and%20large-scale%20pretrained%20models.%20Our%20experiments%20using%20ten%20pairs%20of%20disjoint%20source-target%20data%20sets%20and%2012%20baseline%20methods%20demonstrate%20the%20superiority%20of%20the%20proposed%20LLM-attention%20transplant%20for%20transfer%20learning%20%28LATTLE%29%20method%20over%20traditional%20ML%20models%2C%20state-of-the-art%20deep%20tabular%20architectures%2C%20and%20models%20trained%20on%20thousands%20to%20billions%20of%20tabular%20samples.%20The%20proposed%20cross-domain%20attention%20transfer%20demonstrates%20an%20effective%20solution%20for%20adapting%20LLMs%20to%20learning%20non-text%20tabular%20data%20in%20a%20low-resource%20environment.%20The%20source%20code%20of%20the%20LATTLE%20implementation%20is%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2511.06161v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLATTLE%253A%2520LLM%2520Attention%2520Transplant%2520for%2520Transfer%2520Learning%2520of%2520Tabular%2520Data%2520Across%2520Disparate%2520Domains%26entry.906535625%3DIbna%2520Kowsar%2520and%2520Kazi%2520F.%2520Akhter%2520and%2520Manar%2520D.%2520Samad%26entry.1292438233%3DTransfer%2520learning%2520on%2520tabular%2520data%2520is%2520challenging%2520due%2520to%2520disparate%2520feature%2520spaces%2520across%2520domains%252C%2520in%2520contrast%2520to%2520the%2520homogeneous%2520structures%2520of%2520image%2520and%2520text.%2520Large%2520language%2520models%2520%2528LLMs%2529%2520offer%2520a%2520knowledge%2520base%2520to%2520improve%2520the%2520limited%2520effectiveness%2520of%2520cross-domain%2520transfer%2520learning%2520for%2520tabular%2520data.%2520However%252C%2520LLM%2520performance%2520often%2520stagnates%2520due%2520to%2520subjective%2520text%2520prompts%2520and%2520the%2520computational%2520limitations%2520of%2520in-context%2520learning.%2520We%2520present%2520a%2520novel%2520language-to-tabular%2520context-learning%2520method%2520that%2520uses%2520attention-specific%2520transformer%2520weights%252C%2520enabling%2520seamless%2520transfer%2520learning%2520across%2520disparate%2520tabular%2520data%2520sets.%2520The%2520LLM%2520attention%2520transplant%2520mechanism%2520facilitates%2520a%2520domain-agnostic%2520transfer%2520learning%252C%2520eliminating%2520the%2520need%2520for%2520shared%2520features%2520between%2520tables%252C%2520LLM%2520prompt%2520engineering%252C%2520and%2520large-scale%2520pretrained%2520models.%2520Our%2520experiments%2520using%2520ten%2520pairs%2520of%2520disjoint%2520source-target%2520data%2520sets%2520and%252012%2520baseline%2520methods%2520demonstrate%2520the%2520superiority%2520of%2520the%2520proposed%2520LLM-attention%2520transplant%2520for%2520transfer%2520learning%2520%2528LATTLE%2529%2520method%2520over%2520traditional%2520ML%2520models%252C%2520state-of-the-art%2520deep%2520tabular%2520architectures%252C%2520and%2520models%2520trained%2520on%2520thousands%2520to%2520billions%2520of%2520tabular%2520samples.%2520The%2520proposed%2520cross-domain%2520attention%2520transfer%2520demonstrates%2520an%2520effective%2520solution%2520for%2520adapting%2520LLMs%2520to%2520learning%2520non-text%2520tabular%2520data%2520in%2520a%2520low-resource%2520environment.%2520The%2520source%2520code%2520of%2520the%2520LATTLE%2520implementation%2520is%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.06161v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LATTLE%3A%20LLM%20Attention%20Transplant%20for%20Transfer%20Learning%20of%20Tabular%20Data%20Across%20Disparate%20Domains&entry.906535625=Ibna%20Kowsar%20and%20Kazi%20F.%20Akhter%20and%20Manar%20D.%20Samad&entry.1292438233=Transfer%20learning%20on%20tabular%20data%20is%20challenging%20due%20to%20disparate%20feature%20spaces%20across%20domains%2C%20in%20contrast%20to%20the%20homogeneous%20structures%20of%20image%20and%20text.%20Large%20language%20models%20%28LLMs%29%20offer%20a%20knowledge%20base%20to%20improve%20the%20limited%20effectiveness%20of%20cross-domain%20transfer%20learning%20for%20tabular%20data.%20However%2C%20LLM%20performance%20often%20stagnates%20due%20to%20subjective%20text%20prompts%20and%20the%20computational%20limitations%20of%20in-context%20learning.%20We%20present%20a%20novel%20language-to-tabular%20context-learning%20method%20that%20uses%20attention-specific%20transformer%20weights%2C%20enabling%20seamless%20transfer%20learning%20across%20disparate%20tabular%20data%20sets.%20The%20LLM%20attention%20transplant%20mechanism%20facilitates%20a%20domain-agnostic%20transfer%20learning%2C%20eliminating%20the%20need%20for%20shared%20features%20between%20tables%2C%20LLM%20prompt%20engineering%2C%20and%20large-scale%20pretrained%20models.%20Our%20experiments%20using%20ten%20pairs%20of%20disjoint%20source-target%20data%20sets%20and%2012%20baseline%20methods%20demonstrate%20the%20superiority%20of%20the%20proposed%20LLM-attention%20transplant%20for%20transfer%20learning%20%28LATTLE%29%20method%20over%20traditional%20ML%20models%2C%20state-of-the-art%20deep%20tabular%20architectures%2C%20and%20models%20trained%20on%20thousands%20to%20billions%20of%20tabular%20samples.%20The%20proposed%20cross-domain%20attention%20transfer%20demonstrates%20an%20effective%20solution%20for%20adapting%20LLMs%20to%20learning%20non-text%20tabular%20data%20in%20a%20low-resource%20environment.%20The%20source%20code%20of%20the%20LATTLE%20implementation%20is%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2511.06161v2&entry.124074799=Read"},
{"title": "Enhancing ECG Classification Robustness with Lightweight Unsupervised Anomaly Detection Filters", "author": "Mustafa Fuad Rifet Ibrahim and Maurice Meijer and Alexander Schlaefer and Peer Stelldinger", "abstract": "Continuous electrocardiogram (ECG) monitoring via wearable devices is vital for early cardiovascular disease detection. However, deploying deep learning models on resource-constrained microcontrollers faces reliability challenges, particularly from Out-of-Distribution (OOD) pathologies and noise. Standard classifiers often yield high-confidence errors on such data. Existing OOD detection methods either neglect computational constraints or address noise and unseen classes separately. This paper investigates Unsupervised Anomaly Detection (UAD) as a lightweight, upstream filtering mechanism. We perform a Neural Architecture Search (NAS) on six UAD approaches, including Deep Support Vector Data Description (Deep SVDD), input reconstruction with (Variational-)Autoencoders (AE/VAE), Masked Anomaly Detection (MAD), Normalizing Flows (NFs) and Denoising Diffusion Probabilistic Models (DDPM) under strict hardware constraints ($\\leq$512k parameters), suitable for microcontrollers. Evaluating on the PTB-XL and BUT QDB datasets, we demonstrate that a NAS-optimized Deep SVDD offers the superior Pareto efficiency between detection performance and model size. In a simulated deployment, this lightweight filter improves the accuracy of a diagnostic classifier by up to 21.0 percentage points, demonstrating that optimized UAD filters can safeguard ECG analysis on wearables.", "link": "http://arxiv.org/abs/2510.26501v2", "date": "2026-01-23", "relevancy": 2.0385, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5249}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5004}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20ECG%20Classification%20Robustness%20with%20Lightweight%20Unsupervised%20Anomaly%20Detection%20Filters&body=Title%3A%20Enhancing%20ECG%20Classification%20Robustness%20with%20Lightweight%20Unsupervised%20Anomaly%20Detection%20Filters%0AAuthor%3A%20Mustafa%20Fuad%20Rifet%20Ibrahim%20and%20Maurice%20Meijer%20and%20Alexander%20Schlaefer%20and%20Peer%20Stelldinger%0AAbstract%3A%20Continuous%20electrocardiogram%20%28ECG%29%20monitoring%20via%20wearable%20devices%20is%20vital%20for%20early%20cardiovascular%20disease%20detection.%20However%2C%20deploying%20deep%20learning%20models%20on%20resource-constrained%20microcontrollers%20faces%20reliability%20challenges%2C%20particularly%20from%20Out-of-Distribution%20%28OOD%29%20pathologies%20and%20noise.%20Standard%20classifiers%20often%20yield%20high-confidence%20errors%20on%20such%20data.%20Existing%20OOD%20detection%20methods%20either%20neglect%20computational%20constraints%20or%20address%20noise%20and%20unseen%20classes%20separately.%20This%20paper%20investigates%20Unsupervised%20Anomaly%20Detection%20%28UAD%29%20as%20a%20lightweight%2C%20upstream%20filtering%20mechanism.%20We%20perform%20a%20Neural%20Architecture%20Search%20%28NAS%29%20on%20six%20UAD%20approaches%2C%20including%20Deep%20Support%20Vector%20Data%20Description%20%28Deep%20SVDD%29%2C%20input%20reconstruction%20with%20%28Variational-%29Autoencoders%20%28AE/VAE%29%2C%20Masked%20Anomaly%20Detection%20%28MAD%29%2C%20Normalizing%20Flows%20%28NFs%29%20and%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPM%29%20under%20strict%20hardware%20constraints%20%28%24%5Cleq%24512k%20parameters%29%2C%20suitable%20for%20microcontrollers.%20Evaluating%20on%20the%20PTB-XL%20and%20BUT%20QDB%20datasets%2C%20we%20demonstrate%20that%20a%20NAS-optimized%20Deep%20SVDD%20offers%20the%20superior%20Pareto%20efficiency%20between%20detection%20performance%20and%20model%20size.%20In%20a%20simulated%20deployment%2C%20this%20lightweight%20filter%20improves%20the%20accuracy%20of%20a%20diagnostic%20classifier%20by%20up%20to%2021.0%20percentage%20points%2C%20demonstrating%20that%20optimized%20UAD%20filters%20can%20safeguard%20ECG%20analysis%20on%20wearables.%0ALink%3A%20http%3A//arxiv.org/abs/2510.26501v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520ECG%2520Classification%2520Robustness%2520with%2520Lightweight%2520Unsupervised%2520Anomaly%2520Detection%2520Filters%26entry.906535625%3DMustafa%2520Fuad%2520Rifet%2520Ibrahim%2520and%2520Maurice%2520Meijer%2520and%2520Alexander%2520Schlaefer%2520and%2520Peer%2520Stelldinger%26entry.1292438233%3DContinuous%2520electrocardiogram%2520%2528ECG%2529%2520monitoring%2520via%2520wearable%2520devices%2520is%2520vital%2520for%2520early%2520cardiovascular%2520disease%2520detection.%2520However%252C%2520deploying%2520deep%2520learning%2520models%2520on%2520resource-constrained%2520microcontrollers%2520faces%2520reliability%2520challenges%252C%2520particularly%2520from%2520Out-of-Distribution%2520%2528OOD%2529%2520pathologies%2520and%2520noise.%2520Standard%2520classifiers%2520often%2520yield%2520high-confidence%2520errors%2520on%2520such%2520data.%2520Existing%2520OOD%2520detection%2520methods%2520either%2520neglect%2520computational%2520constraints%2520or%2520address%2520noise%2520and%2520unseen%2520classes%2520separately.%2520This%2520paper%2520investigates%2520Unsupervised%2520Anomaly%2520Detection%2520%2528UAD%2529%2520as%2520a%2520lightweight%252C%2520upstream%2520filtering%2520mechanism.%2520We%2520perform%2520a%2520Neural%2520Architecture%2520Search%2520%2528NAS%2529%2520on%2520six%2520UAD%2520approaches%252C%2520including%2520Deep%2520Support%2520Vector%2520Data%2520Description%2520%2528Deep%2520SVDD%2529%252C%2520input%2520reconstruction%2520with%2520%2528Variational-%2529Autoencoders%2520%2528AE/VAE%2529%252C%2520Masked%2520Anomaly%2520Detection%2520%2528MAD%2529%252C%2520Normalizing%2520Flows%2520%2528NFs%2529%2520and%2520Denoising%2520Diffusion%2520Probabilistic%2520Models%2520%2528DDPM%2529%2520under%2520strict%2520hardware%2520constraints%2520%2528%2524%255Cleq%2524512k%2520parameters%2529%252C%2520suitable%2520for%2520microcontrollers.%2520Evaluating%2520on%2520the%2520PTB-XL%2520and%2520BUT%2520QDB%2520datasets%252C%2520we%2520demonstrate%2520that%2520a%2520NAS-optimized%2520Deep%2520SVDD%2520offers%2520the%2520superior%2520Pareto%2520efficiency%2520between%2520detection%2520performance%2520and%2520model%2520size.%2520In%2520a%2520simulated%2520deployment%252C%2520this%2520lightweight%2520filter%2520improves%2520the%2520accuracy%2520of%2520a%2520diagnostic%2520classifier%2520by%2520up%2520to%252021.0%2520percentage%2520points%252C%2520demonstrating%2520that%2520optimized%2520UAD%2520filters%2520can%2520safeguard%2520ECG%2520analysis%2520on%2520wearables.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26501v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20ECG%20Classification%20Robustness%20with%20Lightweight%20Unsupervised%20Anomaly%20Detection%20Filters&entry.906535625=Mustafa%20Fuad%20Rifet%20Ibrahim%20and%20Maurice%20Meijer%20and%20Alexander%20Schlaefer%20and%20Peer%20Stelldinger&entry.1292438233=Continuous%20electrocardiogram%20%28ECG%29%20monitoring%20via%20wearable%20devices%20is%20vital%20for%20early%20cardiovascular%20disease%20detection.%20However%2C%20deploying%20deep%20learning%20models%20on%20resource-constrained%20microcontrollers%20faces%20reliability%20challenges%2C%20particularly%20from%20Out-of-Distribution%20%28OOD%29%20pathologies%20and%20noise.%20Standard%20classifiers%20often%20yield%20high-confidence%20errors%20on%20such%20data.%20Existing%20OOD%20detection%20methods%20either%20neglect%20computational%20constraints%20or%20address%20noise%20and%20unseen%20classes%20separately.%20This%20paper%20investigates%20Unsupervised%20Anomaly%20Detection%20%28UAD%29%20as%20a%20lightweight%2C%20upstream%20filtering%20mechanism.%20We%20perform%20a%20Neural%20Architecture%20Search%20%28NAS%29%20on%20six%20UAD%20approaches%2C%20including%20Deep%20Support%20Vector%20Data%20Description%20%28Deep%20SVDD%29%2C%20input%20reconstruction%20with%20%28Variational-%29Autoencoders%20%28AE/VAE%29%2C%20Masked%20Anomaly%20Detection%20%28MAD%29%2C%20Normalizing%20Flows%20%28NFs%29%20and%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPM%29%20under%20strict%20hardware%20constraints%20%28%24%5Cleq%24512k%20parameters%29%2C%20suitable%20for%20microcontrollers.%20Evaluating%20on%20the%20PTB-XL%20and%20BUT%20QDB%20datasets%2C%20we%20demonstrate%20that%20a%20NAS-optimized%20Deep%20SVDD%20offers%20the%20superior%20Pareto%20efficiency%20between%20detection%20performance%20and%20model%20size.%20In%20a%20simulated%20deployment%2C%20this%20lightweight%20filter%20improves%20the%20accuracy%20of%20a%20diagnostic%20classifier%20by%20up%20to%2021.0%20percentage%20points%2C%20demonstrating%20that%20optimized%20UAD%20filters%20can%20safeguard%20ECG%20analysis%20on%20wearables.&entry.1838667208=http%3A//arxiv.org/abs/2510.26501v2&entry.124074799=Read"},
{"title": "A Unified Calibration Framework for High-Accuracy Articulated Robot Kinematics", "author": "Philip Tobuschat and Simon Duenser and Markus Bambach and Ivo Aschwanden", "abstract": "Researchers have identified various sources of tool positioning errors for articulated industrial robots and have proposed dedicated compensation strategies. However, these typically require individual, specialized experiments with separate models and identification procedures. This article presents a unified approach to the static calibration of industrial robots that identifies a robot model, including geometric and non-geometric effects (compliant bending, thermal deformation, gear transmission errors), using only a single, straightforward experiment for data collection. The model augments the kinematic chain with virtual joints for each modeled effect and realizes the identification using Gauss-Newton optimization with analytic gradients. Fisher information spectra show that the estimation is well-conditioned and the parameterization near-minimal, whereas systematic temporal cross-validation and model ablations demonstrate robustness of the model identification. The resulting model is very accurate and its identification robust, achieving a mean position error of 26.8 $\u03bcm$ on a KUKA KR30 industrial robot compared to 102.3 $\u03bcm$ for purely geometric calibration.", "link": "http://arxiv.org/abs/2601.16638v1", "date": "2026-01-23", "relevancy": 2.0317, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5472}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5059}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Calibration%20Framework%20for%20High-Accuracy%20Articulated%20Robot%20Kinematics&body=Title%3A%20A%20Unified%20Calibration%20Framework%20for%20High-Accuracy%20Articulated%20Robot%20Kinematics%0AAuthor%3A%20Philip%20Tobuschat%20and%20Simon%20Duenser%20and%20Markus%20Bambach%20and%20Ivo%20Aschwanden%0AAbstract%3A%20Researchers%20have%20identified%20various%20sources%20of%20tool%20positioning%20errors%20for%20articulated%20industrial%20robots%20and%20have%20proposed%20dedicated%20compensation%20strategies.%20However%2C%20these%20typically%20require%20individual%2C%20specialized%20experiments%20with%20separate%20models%20and%20identification%20procedures.%20This%20article%20presents%20a%20unified%20approach%20to%20the%20static%20calibration%20of%20industrial%20robots%20that%20identifies%20a%20robot%20model%2C%20including%20geometric%20and%20non-geometric%20effects%20%28compliant%20bending%2C%20thermal%20deformation%2C%20gear%20transmission%20errors%29%2C%20using%20only%20a%20single%2C%20straightforward%20experiment%20for%20data%20collection.%20The%20model%20augments%20the%20kinematic%20chain%20with%20virtual%20joints%20for%20each%20modeled%20effect%20and%20realizes%20the%20identification%20using%20Gauss-Newton%20optimization%20with%20analytic%20gradients.%20Fisher%20information%20spectra%20show%20that%20the%20estimation%20is%20well-conditioned%20and%20the%20parameterization%20near-minimal%2C%20whereas%20systematic%20temporal%20cross-validation%20and%20model%20ablations%20demonstrate%20robustness%20of%20the%20model%20identification.%20The%20resulting%20model%20is%20very%20accurate%20and%20its%20identification%20robust%2C%20achieving%20a%20mean%20position%20error%20of%2026.8%20%24%CE%BCm%24%20on%20a%20KUKA%20KR30%20industrial%20robot%20compared%20to%20102.3%20%24%CE%BCm%24%20for%20purely%20geometric%20calibration.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Calibration%2520Framework%2520for%2520High-Accuracy%2520Articulated%2520Robot%2520Kinematics%26entry.906535625%3DPhilip%2520Tobuschat%2520and%2520Simon%2520Duenser%2520and%2520Markus%2520Bambach%2520and%2520Ivo%2520Aschwanden%26entry.1292438233%3DResearchers%2520have%2520identified%2520various%2520sources%2520of%2520tool%2520positioning%2520errors%2520for%2520articulated%2520industrial%2520robots%2520and%2520have%2520proposed%2520dedicated%2520compensation%2520strategies.%2520However%252C%2520these%2520typically%2520require%2520individual%252C%2520specialized%2520experiments%2520with%2520separate%2520models%2520and%2520identification%2520procedures.%2520This%2520article%2520presents%2520a%2520unified%2520approach%2520to%2520the%2520static%2520calibration%2520of%2520industrial%2520robots%2520that%2520identifies%2520a%2520robot%2520model%252C%2520including%2520geometric%2520and%2520non-geometric%2520effects%2520%2528compliant%2520bending%252C%2520thermal%2520deformation%252C%2520gear%2520transmission%2520errors%2529%252C%2520using%2520only%2520a%2520single%252C%2520straightforward%2520experiment%2520for%2520data%2520collection.%2520The%2520model%2520augments%2520the%2520kinematic%2520chain%2520with%2520virtual%2520joints%2520for%2520each%2520modeled%2520effect%2520and%2520realizes%2520the%2520identification%2520using%2520Gauss-Newton%2520optimization%2520with%2520analytic%2520gradients.%2520Fisher%2520information%2520spectra%2520show%2520that%2520the%2520estimation%2520is%2520well-conditioned%2520and%2520the%2520parameterization%2520near-minimal%252C%2520whereas%2520systematic%2520temporal%2520cross-validation%2520and%2520model%2520ablations%2520demonstrate%2520robustness%2520of%2520the%2520model%2520identification.%2520The%2520resulting%2520model%2520is%2520very%2520accurate%2520and%2520its%2520identification%2520robust%252C%2520achieving%2520a%2520mean%2520position%2520error%2520of%252026.8%2520%2524%25CE%25BCm%2524%2520on%2520a%2520KUKA%2520KR30%2520industrial%2520robot%2520compared%2520to%2520102.3%2520%2524%25CE%25BCm%2524%2520for%2520purely%2520geometric%2520calibration.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Calibration%20Framework%20for%20High-Accuracy%20Articulated%20Robot%20Kinematics&entry.906535625=Philip%20Tobuschat%20and%20Simon%20Duenser%20and%20Markus%20Bambach%20and%20Ivo%20Aschwanden&entry.1292438233=Researchers%20have%20identified%20various%20sources%20of%20tool%20positioning%20errors%20for%20articulated%20industrial%20robots%20and%20have%20proposed%20dedicated%20compensation%20strategies.%20However%2C%20these%20typically%20require%20individual%2C%20specialized%20experiments%20with%20separate%20models%20and%20identification%20procedures.%20This%20article%20presents%20a%20unified%20approach%20to%20the%20static%20calibration%20of%20industrial%20robots%20that%20identifies%20a%20robot%20model%2C%20including%20geometric%20and%20non-geometric%20effects%20%28compliant%20bending%2C%20thermal%20deformation%2C%20gear%20transmission%20errors%29%2C%20using%20only%20a%20single%2C%20straightforward%20experiment%20for%20data%20collection.%20The%20model%20augments%20the%20kinematic%20chain%20with%20virtual%20joints%20for%20each%20modeled%20effect%20and%20realizes%20the%20identification%20using%20Gauss-Newton%20optimization%20with%20analytic%20gradients.%20Fisher%20information%20spectra%20show%20that%20the%20estimation%20is%20well-conditioned%20and%20the%20parameterization%20near-minimal%2C%20whereas%20systematic%20temporal%20cross-validation%20and%20model%20ablations%20demonstrate%20robustness%20of%20the%20model%20identification.%20The%20resulting%20model%20is%20very%20accurate%20and%20its%20identification%20robust%2C%20achieving%20a%20mean%20position%20error%20of%2026.8%20%24%CE%BCm%24%20on%20a%20KUKA%20KR30%20industrial%20robot%20compared%20to%20102.3%20%24%CE%BCm%24%20for%20purely%20geometric%20calibration.&entry.1838667208=http%3A//arxiv.org/abs/2601.16638v1&entry.124074799=Read"},
{"title": "FedSGM: A Unified Framework for Constraint Aware, Bidirectionally Compressed, Multi-Step Federated Optimization", "author": "Antesh Upadhyay and Sang Bin Moon and Abolfazl Hashemi", "abstract": "We introduce FedSGM, a unified framework for federated constrained optimization that addresses four major challenges in federated learning (FL): functional constraints, communication bottlenecks, local updates, and partial client participation. Building on the switching gradient method, FedSGM provides projection-free, primal-only updates, avoiding expensive dual-variable tuning or inner solvers. To handle communication limits, FedSGM incorporates bi-directional error feedback, correcting the bias introduced by compression while explicitly understanding the interaction between compression noise and multi-step local updates. We derive convergence guarantees showing that the averaged iterate achieves the canonical $\\boldsymbol{\\mathcal{O}}(1/\\sqrt{T})$ rate, with additional high-probability bounds that decouple optimization progress from sampling noise due to partial participation. Additionally, we introduce a soft switching version of FedSGM to stabilize updates near the feasibility boundary. To our knowledge, FedSGM is the first framework to unify functional constraints, compression, multiple local updates, and partial client participation, establishing a theoretically grounded foundation for constrained federated learning. Finally, we validate the theoretical guarantees of FedSGM via experimentation on Neyman-Pearson classification and constrained Markov decision process (CMDP) tasks.", "link": "http://arxiv.org/abs/2601.16897v1", "date": "2026-01-23", "relevancy": 2.0291, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5145}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5076}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedSGM%3A%20A%20Unified%20Framework%20for%20Constraint%20Aware%2C%20Bidirectionally%20Compressed%2C%20Multi-Step%20Federated%20Optimization&body=Title%3A%20FedSGM%3A%20A%20Unified%20Framework%20for%20Constraint%20Aware%2C%20Bidirectionally%20Compressed%2C%20Multi-Step%20Federated%20Optimization%0AAuthor%3A%20Antesh%20Upadhyay%20and%20Sang%20Bin%20Moon%20and%20Abolfazl%20Hashemi%0AAbstract%3A%20We%20introduce%20FedSGM%2C%20a%20unified%20framework%20for%20federated%20constrained%20optimization%20that%20addresses%20four%20major%20challenges%20in%20federated%20learning%20%28FL%29%3A%20functional%20constraints%2C%20communication%20bottlenecks%2C%20local%20updates%2C%20and%20partial%20client%20participation.%20Building%20on%20the%20switching%20gradient%20method%2C%20FedSGM%20provides%20projection-free%2C%20primal-only%20updates%2C%20avoiding%20expensive%20dual-variable%20tuning%20or%20inner%20solvers.%20To%20handle%20communication%20limits%2C%20FedSGM%20incorporates%20bi-directional%20error%20feedback%2C%20correcting%20the%20bias%20introduced%20by%20compression%20while%20explicitly%20understanding%20the%20interaction%20between%20compression%20noise%20and%20multi-step%20local%20updates.%20We%20derive%20convergence%20guarantees%20showing%20that%20the%20averaged%20iterate%20achieves%20the%20canonical%20%24%5Cboldsymbol%7B%5Cmathcal%7BO%7D%7D%281/%5Csqrt%7BT%7D%29%24%20rate%2C%20with%20additional%20high-probability%20bounds%20that%20decouple%20optimization%20progress%20from%20sampling%20noise%20due%20to%20partial%20participation.%20Additionally%2C%20we%20introduce%20a%20soft%20switching%20version%20of%20FedSGM%20to%20stabilize%20updates%20near%20the%20feasibility%20boundary.%20To%20our%20knowledge%2C%20FedSGM%20is%20the%20first%20framework%20to%20unify%20functional%20constraints%2C%20compression%2C%20multiple%20local%20updates%2C%20and%20partial%20client%20participation%2C%20establishing%20a%20theoretically%20grounded%20foundation%20for%20constrained%20federated%20learning.%20Finally%2C%20we%20validate%20the%20theoretical%20guarantees%20of%20FedSGM%20via%20experimentation%20on%20Neyman-Pearson%20classification%20and%20constrained%20Markov%20decision%20process%20%28CMDP%29%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedSGM%253A%2520A%2520Unified%2520Framework%2520for%2520Constraint%2520Aware%252C%2520Bidirectionally%2520Compressed%252C%2520Multi-Step%2520Federated%2520Optimization%26entry.906535625%3DAntesh%2520Upadhyay%2520and%2520Sang%2520Bin%2520Moon%2520and%2520Abolfazl%2520Hashemi%26entry.1292438233%3DWe%2520introduce%2520FedSGM%252C%2520a%2520unified%2520framework%2520for%2520federated%2520constrained%2520optimization%2520that%2520addresses%2520four%2520major%2520challenges%2520in%2520federated%2520learning%2520%2528FL%2529%253A%2520functional%2520constraints%252C%2520communication%2520bottlenecks%252C%2520local%2520updates%252C%2520and%2520partial%2520client%2520participation.%2520Building%2520on%2520the%2520switching%2520gradient%2520method%252C%2520FedSGM%2520provides%2520projection-free%252C%2520primal-only%2520updates%252C%2520avoiding%2520expensive%2520dual-variable%2520tuning%2520or%2520inner%2520solvers.%2520To%2520handle%2520communication%2520limits%252C%2520FedSGM%2520incorporates%2520bi-directional%2520error%2520feedback%252C%2520correcting%2520the%2520bias%2520introduced%2520by%2520compression%2520while%2520explicitly%2520understanding%2520the%2520interaction%2520between%2520compression%2520noise%2520and%2520multi-step%2520local%2520updates.%2520We%2520derive%2520convergence%2520guarantees%2520showing%2520that%2520the%2520averaged%2520iterate%2520achieves%2520the%2520canonical%2520%2524%255Cboldsymbol%257B%255Cmathcal%257BO%257D%257D%25281/%255Csqrt%257BT%257D%2529%2524%2520rate%252C%2520with%2520additional%2520high-probability%2520bounds%2520that%2520decouple%2520optimization%2520progress%2520from%2520sampling%2520noise%2520due%2520to%2520partial%2520participation.%2520Additionally%252C%2520we%2520introduce%2520a%2520soft%2520switching%2520version%2520of%2520FedSGM%2520to%2520stabilize%2520updates%2520near%2520the%2520feasibility%2520boundary.%2520To%2520our%2520knowledge%252C%2520FedSGM%2520is%2520the%2520first%2520framework%2520to%2520unify%2520functional%2520constraints%252C%2520compression%252C%2520multiple%2520local%2520updates%252C%2520and%2520partial%2520client%2520participation%252C%2520establishing%2520a%2520theoretically%2520grounded%2520foundation%2520for%2520constrained%2520federated%2520learning.%2520Finally%252C%2520we%2520validate%2520the%2520theoretical%2520guarantees%2520of%2520FedSGM%2520via%2520experimentation%2520on%2520Neyman-Pearson%2520classification%2520and%2520constrained%2520Markov%2520decision%2520process%2520%2528CMDP%2529%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedSGM%3A%20A%20Unified%20Framework%20for%20Constraint%20Aware%2C%20Bidirectionally%20Compressed%2C%20Multi-Step%20Federated%20Optimization&entry.906535625=Antesh%20Upadhyay%20and%20Sang%20Bin%20Moon%20and%20Abolfazl%20Hashemi&entry.1292438233=We%20introduce%20FedSGM%2C%20a%20unified%20framework%20for%20federated%20constrained%20optimization%20that%20addresses%20four%20major%20challenges%20in%20federated%20learning%20%28FL%29%3A%20functional%20constraints%2C%20communication%20bottlenecks%2C%20local%20updates%2C%20and%20partial%20client%20participation.%20Building%20on%20the%20switching%20gradient%20method%2C%20FedSGM%20provides%20projection-free%2C%20primal-only%20updates%2C%20avoiding%20expensive%20dual-variable%20tuning%20or%20inner%20solvers.%20To%20handle%20communication%20limits%2C%20FedSGM%20incorporates%20bi-directional%20error%20feedback%2C%20correcting%20the%20bias%20introduced%20by%20compression%20while%20explicitly%20understanding%20the%20interaction%20between%20compression%20noise%20and%20multi-step%20local%20updates.%20We%20derive%20convergence%20guarantees%20showing%20that%20the%20averaged%20iterate%20achieves%20the%20canonical%20%24%5Cboldsymbol%7B%5Cmathcal%7BO%7D%7D%281/%5Csqrt%7BT%7D%29%24%20rate%2C%20with%20additional%20high-probability%20bounds%20that%20decouple%20optimization%20progress%20from%20sampling%20noise%20due%20to%20partial%20participation.%20Additionally%2C%20we%20introduce%20a%20soft%20switching%20version%20of%20FedSGM%20to%20stabilize%20updates%20near%20the%20feasibility%20boundary.%20To%20our%20knowledge%2C%20FedSGM%20is%20the%20first%20framework%20to%20unify%20functional%20constraints%2C%20compression%2C%20multiple%20local%20updates%2C%20and%20partial%20client%20participation%2C%20establishing%20a%20theoretically%20grounded%20foundation%20for%20constrained%20federated%20learning.%20Finally%2C%20we%20validate%20the%20theoretical%20guarantees%20of%20FedSGM%20via%20experimentation%20on%20Neyman-Pearson%20classification%20and%20constrained%20Markov%20decision%20process%20%28CMDP%29%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.16897v1&entry.124074799=Read"},
{"title": "Reasoning Promotes Robustness in Theory of Mind Tasks", "author": "Ian B. de Haan and Peter van der Putten and Max van Duijn", "abstract": "Large language models (LLMs) have recently shown strong performance on Theory of Mind (ToM) tests, prompting debate about the nature and true performance of the underlying capabilities. At the same time, reasoning-oriented LLMs trained via reinforcement learning with verifiable rewards (RLVR) have achieved notable improvements across a range of benchmarks. This paper examines the behavior of such reasoning models in ToM tasks, using novel adaptations of machine psychological experiments and results from established benchmarks. We observe that reasoning models consistently exhibit increased robustness to prompt variations and task perturbations. Our analysis indicates that the observed gains are more plausibly attributed to increased robustness in finding the correct solution, rather than to fundamentally new forms of ToM reasoning. We discuss the implications of this interpretation for evaluating social-cognitive behavior in LLMs.", "link": "http://arxiv.org/abs/2601.16853v1", "date": "2026-01-23", "relevancy": 2.0287, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.508}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20Promotes%20Robustness%20in%20Theory%20of%20Mind%20Tasks&body=Title%3A%20Reasoning%20Promotes%20Robustness%20in%20Theory%20of%20Mind%20Tasks%0AAuthor%3A%20Ian%20B.%20de%20Haan%20and%20Peter%20van%20der%20Putten%20and%20Max%20van%20Duijn%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20recently%20shown%20strong%20performance%20on%20Theory%20of%20Mind%20%28ToM%29%20tests%2C%20prompting%20debate%20about%20the%20nature%20and%20true%20performance%20of%20the%20underlying%20capabilities.%20At%20the%20same%20time%2C%20reasoning-oriented%20LLMs%20trained%20via%20reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20have%20achieved%20notable%20improvements%20across%20a%20range%20of%20benchmarks.%20This%20paper%20examines%20the%20behavior%20of%20such%20reasoning%20models%20in%20ToM%20tasks%2C%20using%20novel%20adaptations%20of%20machine%20psychological%20experiments%20and%20results%20from%20established%20benchmarks.%20We%20observe%20that%20reasoning%20models%20consistently%20exhibit%20increased%20robustness%20to%20prompt%20variations%20and%20task%20perturbations.%20Our%20analysis%20indicates%20that%20the%20observed%20gains%20are%20more%20plausibly%20attributed%20to%20increased%20robustness%20in%20finding%20the%20correct%20solution%2C%20rather%20than%20to%20fundamentally%20new%20forms%20of%20ToM%20reasoning.%20We%20discuss%20the%20implications%20of%20this%20interpretation%20for%20evaluating%20social-cognitive%20behavior%20in%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520Promotes%2520Robustness%2520in%2520Theory%2520of%2520Mind%2520Tasks%26entry.906535625%3DIan%2520B.%2520de%2520Haan%2520and%2520Peter%2520van%2520der%2520Putten%2520and%2520Max%2520van%2520Duijn%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520recently%2520shown%2520strong%2520performance%2520on%2520Theory%2520of%2520Mind%2520%2528ToM%2529%2520tests%252C%2520prompting%2520debate%2520about%2520the%2520nature%2520and%2520true%2520performance%2520of%2520the%2520underlying%2520capabilities.%2520At%2520the%2520same%2520time%252C%2520reasoning-oriented%2520LLMs%2520trained%2520via%2520reinforcement%2520learning%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529%2520have%2520achieved%2520notable%2520improvements%2520across%2520a%2520range%2520of%2520benchmarks.%2520This%2520paper%2520examines%2520the%2520behavior%2520of%2520such%2520reasoning%2520models%2520in%2520ToM%2520tasks%252C%2520using%2520novel%2520adaptations%2520of%2520machine%2520psychological%2520experiments%2520and%2520results%2520from%2520established%2520benchmarks.%2520We%2520observe%2520that%2520reasoning%2520models%2520consistently%2520exhibit%2520increased%2520robustness%2520to%2520prompt%2520variations%2520and%2520task%2520perturbations.%2520Our%2520analysis%2520indicates%2520that%2520the%2520observed%2520gains%2520are%2520more%2520plausibly%2520attributed%2520to%2520increased%2520robustness%2520in%2520finding%2520the%2520correct%2520solution%252C%2520rather%2520than%2520to%2520fundamentally%2520new%2520forms%2520of%2520ToM%2520reasoning.%2520We%2520discuss%2520the%2520implications%2520of%2520this%2520interpretation%2520for%2520evaluating%2520social-cognitive%2520behavior%2520in%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20Promotes%20Robustness%20in%20Theory%20of%20Mind%20Tasks&entry.906535625=Ian%20B.%20de%20Haan%20and%20Peter%20van%20der%20Putten%20and%20Max%20van%20Duijn&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20recently%20shown%20strong%20performance%20on%20Theory%20of%20Mind%20%28ToM%29%20tests%2C%20prompting%20debate%20about%20the%20nature%20and%20true%20performance%20of%20the%20underlying%20capabilities.%20At%20the%20same%20time%2C%20reasoning-oriented%20LLMs%20trained%20via%20reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20have%20achieved%20notable%20improvements%20across%20a%20range%20of%20benchmarks.%20This%20paper%20examines%20the%20behavior%20of%20such%20reasoning%20models%20in%20ToM%20tasks%2C%20using%20novel%20adaptations%20of%20machine%20psychological%20experiments%20and%20results%20from%20established%20benchmarks.%20We%20observe%20that%20reasoning%20models%20consistently%20exhibit%20increased%20robustness%20to%20prompt%20variations%20and%20task%20perturbations.%20Our%20analysis%20indicates%20that%20the%20observed%20gains%20are%20more%20plausibly%20attributed%20to%20increased%20robustness%20in%20finding%20the%20correct%20solution%2C%20rather%20than%20to%20fundamentally%20new%20forms%20of%20ToM%20reasoning.%20We%20discuss%20the%20implications%20of%20this%20interpretation%20for%20evaluating%20social-cognitive%20behavior%20in%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2601.16853v1&entry.124074799=Read"},
{"title": "SLD: Segmentation-Based Landmark Detection for Spinal Ligaments", "author": "Lara Blomenkamp and Ivanna Kramer and Sabine Bauer and Theresa Sch\u00f6che", "abstract": "In biomechanical modeling, the representation of ligament attachments is crucial for a realistic simulation of the forces acting between the vertebrae. These forces are typically modeled as vectors connecting ligament landmarks on adjacent vertebrae, making precise identification of these landmarks a key requirement for constructing reliable spine models. Existing automated detection methods are either limited to specific spinal regions or lack sufficient accuracy. This work presents a novel approach for detecting spinal ligament landmarks, which first performs shape-based segmentation of 3D vertebrae and subsequently applies domain-specific rules to identify different types of attachment points. The proposed method outperforms existing approaches by achieving high accuracy and demonstrating strong generalization across all spinal regions. Validation on two independent spinal datasets from multiple patients yielded a mean absolute error (MAE) of 0.7 mm and a root mean square error (RMSE) of 1.1 mm.", "link": "http://arxiv.org/abs/2601.16782v1", "date": "2026-01-23", "relevancy": 2.0268, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5258}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4957}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLD%3A%20Segmentation-Based%20Landmark%20Detection%20for%20Spinal%20Ligaments&body=Title%3A%20SLD%3A%20Segmentation-Based%20Landmark%20Detection%20for%20Spinal%20Ligaments%0AAuthor%3A%20Lara%20Blomenkamp%20and%20Ivanna%20Kramer%20and%20Sabine%20Bauer%20and%20Theresa%20Sch%C3%B6che%0AAbstract%3A%20In%20biomechanical%20modeling%2C%20the%20representation%20of%20ligament%20attachments%20is%20crucial%20for%20a%20realistic%20simulation%20of%20the%20forces%20acting%20between%20the%20vertebrae.%20These%20forces%20are%20typically%20modeled%20as%20vectors%20connecting%20ligament%20landmarks%20on%20adjacent%20vertebrae%2C%20making%20precise%20identification%20of%20these%20landmarks%20a%20key%20requirement%20for%20constructing%20reliable%20spine%20models.%20Existing%20automated%20detection%20methods%20are%20either%20limited%20to%20specific%20spinal%20regions%20or%20lack%20sufficient%20accuracy.%20This%20work%20presents%20a%20novel%20approach%20for%20detecting%20spinal%20ligament%20landmarks%2C%20which%20first%20performs%20shape-based%20segmentation%20of%203D%20vertebrae%20and%20subsequently%20applies%20domain-specific%20rules%20to%20identify%20different%20types%20of%20attachment%20points.%20The%20proposed%20method%20outperforms%20existing%20approaches%20by%20achieving%20high%20accuracy%20and%20demonstrating%20strong%20generalization%20across%20all%20spinal%20regions.%20Validation%20on%20two%20independent%20spinal%20datasets%20from%20multiple%20patients%20yielded%20a%20mean%20absolute%20error%20%28MAE%29%20of%200.7%20mm%20and%20a%20root%20mean%20square%20error%20%28RMSE%29%20of%201.1%20mm.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLD%253A%2520Segmentation-Based%2520Landmark%2520Detection%2520for%2520Spinal%2520Ligaments%26entry.906535625%3DLara%2520Blomenkamp%2520and%2520Ivanna%2520Kramer%2520and%2520Sabine%2520Bauer%2520and%2520Theresa%2520Sch%25C3%25B6che%26entry.1292438233%3DIn%2520biomechanical%2520modeling%252C%2520the%2520representation%2520of%2520ligament%2520attachments%2520is%2520crucial%2520for%2520a%2520realistic%2520simulation%2520of%2520the%2520forces%2520acting%2520between%2520the%2520vertebrae.%2520These%2520forces%2520are%2520typically%2520modeled%2520as%2520vectors%2520connecting%2520ligament%2520landmarks%2520on%2520adjacent%2520vertebrae%252C%2520making%2520precise%2520identification%2520of%2520these%2520landmarks%2520a%2520key%2520requirement%2520for%2520constructing%2520reliable%2520spine%2520models.%2520Existing%2520automated%2520detection%2520methods%2520are%2520either%2520limited%2520to%2520specific%2520spinal%2520regions%2520or%2520lack%2520sufficient%2520accuracy.%2520This%2520work%2520presents%2520a%2520novel%2520approach%2520for%2520detecting%2520spinal%2520ligament%2520landmarks%252C%2520which%2520first%2520performs%2520shape-based%2520segmentation%2520of%25203D%2520vertebrae%2520and%2520subsequently%2520applies%2520domain-specific%2520rules%2520to%2520identify%2520different%2520types%2520of%2520attachment%2520points.%2520The%2520proposed%2520method%2520outperforms%2520existing%2520approaches%2520by%2520achieving%2520high%2520accuracy%2520and%2520demonstrating%2520strong%2520generalization%2520across%2520all%2520spinal%2520regions.%2520Validation%2520on%2520two%2520independent%2520spinal%2520datasets%2520from%2520multiple%2520patients%2520yielded%2520a%2520mean%2520absolute%2520error%2520%2528MAE%2529%2520of%25200.7%2520mm%2520and%2520a%2520root%2520mean%2520square%2520error%2520%2528RMSE%2529%2520of%25201.1%2520mm.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLD%3A%20Segmentation-Based%20Landmark%20Detection%20for%20Spinal%20Ligaments&entry.906535625=Lara%20Blomenkamp%20and%20Ivanna%20Kramer%20and%20Sabine%20Bauer%20and%20Theresa%20Sch%C3%B6che&entry.1292438233=In%20biomechanical%20modeling%2C%20the%20representation%20of%20ligament%20attachments%20is%20crucial%20for%20a%20realistic%20simulation%20of%20the%20forces%20acting%20between%20the%20vertebrae.%20These%20forces%20are%20typically%20modeled%20as%20vectors%20connecting%20ligament%20landmarks%20on%20adjacent%20vertebrae%2C%20making%20precise%20identification%20of%20these%20landmarks%20a%20key%20requirement%20for%20constructing%20reliable%20spine%20models.%20Existing%20automated%20detection%20methods%20are%20either%20limited%20to%20specific%20spinal%20regions%20or%20lack%20sufficient%20accuracy.%20This%20work%20presents%20a%20novel%20approach%20for%20detecting%20spinal%20ligament%20landmarks%2C%20which%20first%20performs%20shape-based%20segmentation%20of%203D%20vertebrae%20and%20subsequently%20applies%20domain-specific%20rules%20to%20identify%20different%20types%20of%20attachment%20points.%20The%20proposed%20method%20outperforms%20existing%20approaches%20by%20achieving%20high%20accuracy%20and%20demonstrating%20strong%20generalization%20across%20all%20spinal%20regions.%20Validation%20on%20two%20independent%20spinal%20datasets%20from%20multiple%20patients%20yielded%20a%20mean%20absolute%20error%20%28MAE%29%20of%200.7%20mm%20and%20a%20root%20mean%20square%20error%20%28RMSE%29%20of%201.1%20mm.&entry.1838667208=http%3A//arxiv.org/abs/2601.16782v1&entry.124074799=Read"},
{"title": "DSSmoothing: Toward Certified Dataset Ownership Verification for Pre-trained Language Models via Dual-Space Smoothing", "author": "Ting Qiao and Xing Liu and Wenke Huang and Jianbin Li and Zhaoxin Fan and Yiming Li", "abstract": "Large web-scale datasets have driven the rapid advancement of pre-trained language models (PLMs), but unauthorized data usage has raised serious copyright concerns. Existing dataset ownership verification (DOV) methods typically assume that watermarks remain stable during inference; however, this assumption often fails under natural noise and adversary-crafted perturbations. We propose the first certified dataset ownership verification method for PLMs under a gray-box setting (i.e., the defender can only query the suspicious model but is aware of its input representation module), based on dual-space smoothing (i.e., DSSmoothing). To address the challenges of text discreteness and semantic sensitivity, DSSmoothing introduces continuous perturbations in the embedding space to capture semantic robustness and applies controlled token reordering in the permutation space to capture sequential robustness. DSSmoothing consists of two stages: in the first stage, triggers are collaboratively embedded in both spaces to generate norm-constrained and robust watermarked datasets; in the second stage, randomized smoothing is applied in both spaces during verification to compute the watermark robustness (WR) of suspicious models and statistically compare it with the principal probability (PP) values of a set of benign models. Theoretically, DSSmoothing provides provable robustness guarantees for dataset ownership verification by ensuring that WR consistently exceeds PP under bounded dual-space perturbations. Extensive experiments on multiple representative web datasets demonstrate that DSSmoothing achieves stable and reliable verification performance and exhibits robustness against potential adaptive attacks. Our code is available at https://github.com/NcepuQiaoTing/DSSmoothing.", "link": "http://arxiv.org/abs/2510.15303v2", "date": "2026-01-23", "relevancy": 2.0255, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5239}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5055}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DSSmoothing%3A%20Toward%20Certified%20Dataset%20Ownership%20Verification%20for%20Pre-trained%20Language%20Models%20via%20Dual-Space%20Smoothing&body=Title%3A%20DSSmoothing%3A%20Toward%20Certified%20Dataset%20Ownership%20Verification%20for%20Pre-trained%20Language%20Models%20via%20Dual-Space%20Smoothing%0AAuthor%3A%20Ting%20Qiao%20and%20Xing%20Liu%20and%20Wenke%20Huang%20and%20Jianbin%20Li%20and%20Zhaoxin%20Fan%20and%20Yiming%20Li%0AAbstract%3A%20Large%20web-scale%20datasets%20have%20driven%20the%20rapid%20advancement%20of%20pre-trained%20language%20models%20%28PLMs%29%2C%20but%20unauthorized%20data%20usage%20has%20raised%20serious%20copyright%20concerns.%20Existing%20dataset%20ownership%20verification%20%28DOV%29%20methods%20typically%20assume%20that%20watermarks%20remain%20stable%20during%20inference%3B%20however%2C%20this%20assumption%20often%20fails%20under%20natural%20noise%20and%20adversary-crafted%20perturbations.%20We%20propose%20the%20first%20certified%20dataset%20ownership%20verification%20method%20for%20PLMs%20under%20a%20gray-box%20setting%20%28i.e.%2C%20the%20defender%20can%20only%20query%20the%20suspicious%20model%20but%20is%20aware%20of%20its%20input%20representation%20module%29%2C%20based%20on%20dual-space%20smoothing%20%28i.e.%2C%20DSSmoothing%29.%20To%20address%20the%20challenges%20of%20text%20discreteness%20and%20semantic%20sensitivity%2C%20DSSmoothing%20introduces%20continuous%20perturbations%20in%20the%20embedding%20space%20to%20capture%20semantic%20robustness%20and%20applies%20controlled%20token%20reordering%20in%20the%20permutation%20space%20to%20capture%20sequential%20robustness.%20DSSmoothing%20consists%20of%20two%20stages%3A%20in%20the%20first%20stage%2C%20triggers%20are%20collaboratively%20embedded%20in%20both%20spaces%20to%20generate%20norm-constrained%20and%20robust%20watermarked%20datasets%3B%20in%20the%20second%20stage%2C%20randomized%20smoothing%20is%20applied%20in%20both%20spaces%20during%20verification%20to%20compute%20the%20watermark%20robustness%20%28WR%29%20of%20suspicious%20models%20and%20statistically%20compare%20it%20with%20the%20principal%20probability%20%28PP%29%20values%20of%20a%20set%20of%20benign%20models.%20Theoretically%2C%20DSSmoothing%20provides%20provable%20robustness%20guarantees%20for%20dataset%20ownership%20verification%20by%20ensuring%20that%20WR%20consistently%20exceeds%20PP%20under%20bounded%20dual-space%20perturbations.%20Extensive%20experiments%20on%20multiple%20representative%20web%20datasets%20demonstrate%20that%20DSSmoothing%20achieves%20stable%20and%20reliable%20verification%20performance%20and%20exhibits%20robustness%20against%20potential%20adaptive%20attacks.%20Our%20code%20is%20available%20at%20https%3A//github.com/NcepuQiaoTing/DSSmoothing.%0ALink%3A%20http%3A//arxiv.org/abs/2510.15303v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDSSmoothing%253A%2520Toward%2520Certified%2520Dataset%2520Ownership%2520Verification%2520for%2520Pre-trained%2520Language%2520Models%2520via%2520Dual-Space%2520Smoothing%26entry.906535625%3DTing%2520Qiao%2520and%2520Xing%2520Liu%2520and%2520Wenke%2520Huang%2520and%2520Jianbin%2520Li%2520and%2520Zhaoxin%2520Fan%2520and%2520Yiming%2520Li%26entry.1292438233%3DLarge%2520web-scale%2520datasets%2520have%2520driven%2520the%2520rapid%2520advancement%2520of%2520pre-trained%2520language%2520models%2520%2528PLMs%2529%252C%2520but%2520unauthorized%2520data%2520usage%2520has%2520raised%2520serious%2520copyright%2520concerns.%2520Existing%2520dataset%2520ownership%2520verification%2520%2528DOV%2529%2520methods%2520typically%2520assume%2520that%2520watermarks%2520remain%2520stable%2520during%2520inference%253B%2520however%252C%2520this%2520assumption%2520often%2520fails%2520under%2520natural%2520noise%2520and%2520adversary-crafted%2520perturbations.%2520We%2520propose%2520the%2520first%2520certified%2520dataset%2520ownership%2520verification%2520method%2520for%2520PLMs%2520under%2520a%2520gray-box%2520setting%2520%2528i.e.%252C%2520the%2520defender%2520can%2520only%2520query%2520the%2520suspicious%2520model%2520but%2520is%2520aware%2520of%2520its%2520input%2520representation%2520module%2529%252C%2520based%2520on%2520dual-space%2520smoothing%2520%2528i.e.%252C%2520DSSmoothing%2529.%2520To%2520address%2520the%2520challenges%2520of%2520text%2520discreteness%2520and%2520semantic%2520sensitivity%252C%2520DSSmoothing%2520introduces%2520continuous%2520perturbations%2520in%2520the%2520embedding%2520space%2520to%2520capture%2520semantic%2520robustness%2520and%2520applies%2520controlled%2520token%2520reordering%2520in%2520the%2520permutation%2520space%2520to%2520capture%2520sequential%2520robustness.%2520DSSmoothing%2520consists%2520of%2520two%2520stages%253A%2520in%2520the%2520first%2520stage%252C%2520triggers%2520are%2520collaboratively%2520embedded%2520in%2520both%2520spaces%2520to%2520generate%2520norm-constrained%2520and%2520robust%2520watermarked%2520datasets%253B%2520in%2520the%2520second%2520stage%252C%2520randomized%2520smoothing%2520is%2520applied%2520in%2520both%2520spaces%2520during%2520verification%2520to%2520compute%2520the%2520watermark%2520robustness%2520%2528WR%2529%2520of%2520suspicious%2520models%2520and%2520statistically%2520compare%2520it%2520with%2520the%2520principal%2520probability%2520%2528PP%2529%2520values%2520of%2520a%2520set%2520of%2520benign%2520models.%2520Theoretically%252C%2520DSSmoothing%2520provides%2520provable%2520robustness%2520guarantees%2520for%2520dataset%2520ownership%2520verification%2520by%2520ensuring%2520that%2520WR%2520consistently%2520exceeds%2520PP%2520under%2520bounded%2520dual-space%2520perturbations.%2520Extensive%2520experiments%2520on%2520multiple%2520representative%2520web%2520datasets%2520demonstrate%2520that%2520DSSmoothing%2520achieves%2520stable%2520and%2520reliable%2520verification%2520performance%2520and%2520exhibits%2520robustness%2520against%2520potential%2520adaptive%2520attacks.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/NcepuQiaoTing/DSSmoothing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15303v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DSSmoothing%3A%20Toward%20Certified%20Dataset%20Ownership%20Verification%20for%20Pre-trained%20Language%20Models%20via%20Dual-Space%20Smoothing&entry.906535625=Ting%20Qiao%20and%20Xing%20Liu%20and%20Wenke%20Huang%20and%20Jianbin%20Li%20and%20Zhaoxin%20Fan%20and%20Yiming%20Li&entry.1292438233=Large%20web-scale%20datasets%20have%20driven%20the%20rapid%20advancement%20of%20pre-trained%20language%20models%20%28PLMs%29%2C%20but%20unauthorized%20data%20usage%20has%20raised%20serious%20copyright%20concerns.%20Existing%20dataset%20ownership%20verification%20%28DOV%29%20methods%20typically%20assume%20that%20watermarks%20remain%20stable%20during%20inference%3B%20however%2C%20this%20assumption%20often%20fails%20under%20natural%20noise%20and%20adversary-crafted%20perturbations.%20We%20propose%20the%20first%20certified%20dataset%20ownership%20verification%20method%20for%20PLMs%20under%20a%20gray-box%20setting%20%28i.e.%2C%20the%20defender%20can%20only%20query%20the%20suspicious%20model%20but%20is%20aware%20of%20its%20input%20representation%20module%29%2C%20based%20on%20dual-space%20smoothing%20%28i.e.%2C%20DSSmoothing%29.%20To%20address%20the%20challenges%20of%20text%20discreteness%20and%20semantic%20sensitivity%2C%20DSSmoothing%20introduces%20continuous%20perturbations%20in%20the%20embedding%20space%20to%20capture%20semantic%20robustness%20and%20applies%20controlled%20token%20reordering%20in%20the%20permutation%20space%20to%20capture%20sequential%20robustness.%20DSSmoothing%20consists%20of%20two%20stages%3A%20in%20the%20first%20stage%2C%20triggers%20are%20collaboratively%20embedded%20in%20both%20spaces%20to%20generate%20norm-constrained%20and%20robust%20watermarked%20datasets%3B%20in%20the%20second%20stage%2C%20randomized%20smoothing%20is%20applied%20in%20both%20spaces%20during%20verification%20to%20compute%20the%20watermark%20robustness%20%28WR%29%20of%20suspicious%20models%20and%20statistically%20compare%20it%20with%20the%20principal%20probability%20%28PP%29%20values%20of%20a%20set%20of%20benign%20models.%20Theoretically%2C%20DSSmoothing%20provides%20provable%20robustness%20guarantees%20for%20dataset%20ownership%20verification%20by%20ensuring%20that%20WR%20consistently%20exceeds%20PP%20under%20bounded%20dual-space%20perturbations.%20Extensive%20experiments%20on%20multiple%20representative%20web%20datasets%20demonstrate%20that%20DSSmoothing%20achieves%20stable%20and%20reliable%20verification%20performance%20and%20exhibits%20robustness%20against%20potential%20adaptive%20attacks.%20Our%20code%20is%20available%20at%20https%3A//github.com/NcepuQiaoTing/DSSmoothing.&entry.1838667208=http%3A//arxiv.org/abs/2510.15303v2&entry.124074799=Read"},
{"title": "A Feature Extraction Pipeline for Enhancing Lightweight Neural Networks in sEMG-based Joint Torque Estimation", "author": "Kartik Chari and Raid Dokhan and Anas Homsi and Niklas Kueper and Elsa Andrea Kirchner", "abstract": "Robot-assisted rehabilitation offers an effective approach, wherein exoskeletons adapt to users' needs and provide personalized assistance. However, to deliver such assistance, accurate prediction of the user's joint torques is essential. In this work, we propose a feature extraction pipeline using 8-channel surface electromyography (sEMG) signals to predict elbow and shoulder joint torques. For preliminary evaluation, this pipeline was integrated into two neural network models: the Multilayer Perceptron (MLP) and the Temporal Convolutional Network (TCN). Data were collected from a single subject performing elbow and shoulder movements under three load conditions (0 kg, 1.10 kg, and 1.85 kg) using three motion-capture cameras. Reference torques were estimated from center-of-mass kinematics under the assumption of static equilibrium. Our offline analyses showed that, with our feature extraction pipeline, MLP model achieved mean RMSE of 0.963 N m, 1.403 N m, and 1.434 N m (over five seeds) for elbow, front-shoulder, and side-shoulder joints, respectively, which were comparable to the TCN performance. These results demonstrate that the proposed feature extraction pipeline enables a simple MLP to achieve performance comparable to that of a network designed explicitly for temporal dependencies. This finding is particularly relevant for applications with limited training data, a common scenario patient care.", "link": "http://arxiv.org/abs/2601.16712v1", "date": "2026-01-23", "relevancy": 2.02, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5662}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4989}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Feature%20Extraction%20Pipeline%20for%20Enhancing%20Lightweight%20Neural%20Networks%20in%20sEMG-based%20Joint%20Torque%20Estimation&body=Title%3A%20A%20Feature%20Extraction%20Pipeline%20for%20Enhancing%20Lightweight%20Neural%20Networks%20in%20sEMG-based%20Joint%20Torque%20Estimation%0AAuthor%3A%20Kartik%20Chari%20and%20Raid%20Dokhan%20and%20Anas%20Homsi%20and%20Niklas%20Kueper%20and%20Elsa%20Andrea%20Kirchner%0AAbstract%3A%20Robot-assisted%20rehabilitation%20offers%20an%20effective%20approach%2C%20wherein%20exoskeletons%20adapt%20to%20users%27%20needs%20and%20provide%20personalized%20assistance.%20However%2C%20to%20deliver%20such%20assistance%2C%20accurate%20prediction%20of%20the%20user%27s%20joint%20torques%20is%20essential.%20In%20this%20work%2C%20we%20propose%20a%20feature%20extraction%20pipeline%20using%208-channel%20surface%20electromyography%20%28sEMG%29%20signals%20to%20predict%20elbow%20and%20shoulder%20joint%20torques.%20For%20preliminary%20evaluation%2C%20this%20pipeline%20was%20integrated%20into%20two%20neural%20network%20models%3A%20the%20Multilayer%20Perceptron%20%28MLP%29%20and%20the%20Temporal%20Convolutional%20Network%20%28TCN%29.%20Data%20were%20collected%20from%20a%20single%20subject%20performing%20elbow%20and%20shoulder%20movements%20under%20three%20load%20conditions%20%280%20kg%2C%201.10%20kg%2C%20and%201.85%20kg%29%20using%20three%20motion-capture%20cameras.%20Reference%20torques%20were%20estimated%20from%20center-of-mass%20kinematics%20under%20the%20assumption%20of%20static%20equilibrium.%20Our%20offline%20analyses%20showed%20that%2C%20with%20our%20feature%20extraction%20pipeline%2C%20MLP%20model%20achieved%20mean%20RMSE%20of%200.963%20N%20m%2C%201.403%20N%20m%2C%20and%201.434%20N%20m%20%28over%20five%20seeds%29%20for%20elbow%2C%20front-shoulder%2C%20and%20side-shoulder%20joints%2C%20respectively%2C%20which%20were%20comparable%20to%20the%20TCN%20performance.%20These%20results%20demonstrate%20that%20the%20proposed%20feature%20extraction%20pipeline%20enables%20a%20simple%20MLP%20to%20achieve%20performance%20comparable%20to%20that%20of%20a%20network%20designed%20explicitly%20for%20temporal%20dependencies.%20This%20finding%20is%20particularly%20relevant%20for%20applications%20with%20limited%20training%20data%2C%20a%20common%20scenario%20patient%20care.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Feature%2520Extraction%2520Pipeline%2520for%2520Enhancing%2520Lightweight%2520Neural%2520Networks%2520in%2520sEMG-based%2520Joint%2520Torque%2520Estimation%26entry.906535625%3DKartik%2520Chari%2520and%2520Raid%2520Dokhan%2520and%2520Anas%2520Homsi%2520and%2520Niklas%2520Kueper%2520and%2520Elsa%2520Andrea%2520Kirchner%26entry.1292438233%3DRobot-assisted%2520rehabilitation%2520offers%2520an%2520effective%2520approach%252C%2520wherein%2520exoskeletons%2520adapt%2520to%2520users%2527%2520needs%2520and%2520provide%2520personalized%2520assistance.%2520However%252C%2520to%2520deliver%2520such%2520assistance%252C%2520accurate%2520prediction%2520of%2520the%2520user%2527s%2520joint%2520torques%2520is%2520essential.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520feature%2520extraction%2520pipeline%2520using%25208-channel%2520surface%2520electromyography%2520%2528sEMG%2529%2520signals%2520to%2520predict%2520elbow%2520and%2520shoulder%2520joint%2520torques.%2520For%2520preliminary%2520evaluation%252C%2520this%2520pipeline%2520was%2520integrated%2520into%2520two%2520neural%2520network%2520models%253A%2520the%2520Multilayer%2520Perceptron%2520%2528MLP%2529%2520and%2520the%2520Temporal%2520Convolutional%2520Network%2520%2528TCN%2529.%2520Data%2520were%2520collected%2520from%2520a%2520single%2520subject%2520performing%2520elbow%2520and%2520shoulder%2520movements%2520under%2520three%2520load%2520conditions%2520%25280%2520kg%252C%25201.10%2520kg%252C%2520and%25201.85%2520kg%2529%2520using%2520three%2520motion-capture%2520cameras.%2520Reference%2520torques%2520were%2520estimated%2520from%2520center-of-mass%2520kinematics%2520under%2520the%2520assumption%2520of%2520static%2520equilibrium.%2520Our%2520offline%2520analyses%2520showed%2520that%252C%2520with%2520our%2520feature%2520extraction%2520pipeline%252C%2520MLP%2520model%2520achieved%2520mean%2520RMSE%2520of%25200.963%2520N%2520m%252C%25201.403%2520N%2520m%252C%2520and%25201.434%2520N%2520m%2520%2528over%2520five%2520seeds%2529%2520for%2520elbow%252C%2520front-shoulder%252C%2520and%2520side-shoulder%2520joints%252C%2520respectively%252C%2520which%2520were%2520comparable%2520to%2520the%2520TCN%2520performance.%2520These%2520results%2520demonstrate%2520that%2520the%2520proposed%2520feature%2520extraction%2520pipeline%2520enables%2520a%2520simple%2520MLP%2520to%2520achieve%2520performance%2520comparable%2520to%2520that%2520of%2520a%2520network%2520designed%2520explicitly%2520for%2520temporal%2520dependencies.%2520This%2520finding%2520is%2520particularly%2520relevant%2520for%2520applications%2520with%2520limited%2520training%2520data%252C%2520a%2520common%2520scenario%2520patient%2520care.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Feature%20Extraction%20Pipeline%20for%20Enhancing%20Lightweight%20Neural%20Networks%20in%20sEMG-based%20Joint%20Torque%20Estimation&entry.906535625=Kartik%20Chari%20and%20Raid%20Dokhan%20and%20Anas%20Homsi%20and%20Niklas%20Kueper%20and%20Elsa%20Andrea%20Kirchner&entry.1292438233=Robot-assisted%20rehabilitation%20offers%20an%20effective%20approach%2C%20wherein%20exoskeletons%20adapt%20to%20users%27%20needs%20and%20provide%20personalized%20assistance.%20However%2C%20to%20deliver%20such%20assistance%2C%20accurate%20prediction%20of%20the%20user%27s%20joint%20torques%20is%20essential.%20In%20this%20work%2C%20we%20propose%20a%20feature%20extraction%20pipeline%20using%208-channel%20surface%20electromyography%20%28sEMG%29%20signals%20to%20predict%20elbow%20and%20shoulder%20joint%20torques.%20For%20preliminary%20evaluation%2C%20this%20pipeline%20was%20integrated%20into%20two%20neural%20network%20models%3A%20the%20Multilayer%20Perceptron%20%28MLP%29%20and%20the%20Temporal%20Convolutional%20Network%20%28TCN%29.%20Data%20were%20collected%20from%20a%20single%20subject%20performing%20elbow%20and%20shoulder%20movements%20under%20three%20load%20conditions%20%280%20kg%2C%201.10%20kg%2C%20and%201.85%20kg%29%20using%20three%20motion-capture%20cameras.%20Reference%20torques%20were%20estimated%20from%20center-of-mass%20kinematics%20under%20the%20assumption%20of%20static%20equilibrium.%20Our%20offline%20analyses%20showed%20that%2C%20with%20our%20feature%20extraction%20pipeline%2C%20MLP%20model%20achieved%20mean%20RMSE%20of%200.963%20N%20m%2C%201.403%20N%20m%2C%20and%201.434%20N%20m%20%28over%20five%20seeds%29%20for%20elbow%2C%20front-shoulder%2C%20and%20side-shoulder%20joints%2C%20respectively%2C%20which%20were%20comparable%20to%20the%20TCN%20performance.%20These%20results%20demonstrate%20that%20the%20proposed%20feature%20extraction%20pipeline%20enables%20a%20simple%20MLP%20to%20achieve%20performance%20comparable%20to%20that%20of%20a%20network%20designed%20explicitly%20for%20temporal%20dependencies.%20This%20finding%20is%20particularly%20relevant%20for%20applications%20with%20limited%20training%20data%2C%20a%20common%20scenario%20patient%20care.&entry.1838667208=http%3A//arxiv.org/abs/2601.16712v1&entry.124074799=Read"},
{"title": "Information Representation Fairness in Long-Document Embeddings: The Peculiar Interaction of Positional and Language Bias", "author": "Elias Schuhmacher and Andrianos Michail and Juri Opitz and Rico Sennrich and Simon Clematide", "abstract": "To be discoverable in an embedding-based search process, each part of a document should be reflected in its embedding representation. To quantify any potential reflection biases, we introduce a permutation-based evaluation framework. With this, we observe that state-of-the-art embedding models exhibit systematic positional and language biases when documents are longer and consist of multiple segments. Specifically, early segments and segments in higher-resource languages like English are over-represented, while later segments and segments in lower-resource languages are marginalized. In our further analysis, we find that the positional bias stems from front-loaded attention distributions in pooling-token embeddings, where early tokens receive more attention. To mitigate this issue, we introduce an inference-time attention calibration method that redistributes attention more evenly across document positions, increasing discoverabiltiy of later segments. Our evaluation framework and attention calibration is available at https://github.com/impresso/fair-sentence-transformers", "link": "http://arxiv.org/abs/2601.16934v1", "date": "2026-01-23", "relevancy": 2.007, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5041}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5041}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Information%20Representation%20Fairness%20in%20Long-Document%20Embeddings%3A%20The%20Peculiar%20Interaction%20of%20Positional%20and%20Language%20Bias&body=Title%3A%20Information%20Representation%20Fairness%20in%20Long-Document%20Embeddings%3A%20The%20Peculiar%20Interaction%20of%20Positional%20and%20Language%20Bias%0AAuthor%3A%20Elias%20Schuhmacher%20and%20Andrianos%20Michail%20and%20Juri%20Opitz%20and%20Rico%20Sennrich%20and%20Simon%20Clematide%0AAbstract%3A%20To%20be%20discoverable%20in%20an%20embedding-based%20search%20process%2C%20each%20part%20of%20a%20document%20should%20be%20reflected%20in%20its%20embedding%20representation.%20To%20quantify%20any%20potential%20reflection%20biases%2C%20we%20introduce%20a%20permutation-based%20evaluation%20framework.%20With%20this%2C%20we%20observe%20that%20state-of-the-art%20embedding%20models%20exhibit%20systematic%20positional%20and%20language%20biases%20when%20documents%20are%20longer%20and%20consist%20of%20multiple%20segments.%20Specifically%2C%20early%20segments%20and%20segments%20in%20higher-resource%20languages%20like%20English%20are%20over-represented%2C%20while%20later%20segments%20and%20segments%20in%20lower-resource%20languages%20are%20marginalized.%20In%20our%20further%20analysis%2C%20we%20find%20that%20the%20positional%20bias%20stems%20from%20front-loaded%20attention%20distributions%20in%20pooling-token%20embeddings%2C%20where%20early%20tokens%20receive%20more%20attention.%20To%20mitigate%20this%20issue%2C%20we%20introduce%20an%20inference-time%20attention%20calibration%20method%20that%20redistributes%20attention%20more%20evenly%20across%20document%20positions%2C%20increasing%20discoverabiltiy%20of%20later%20segments.%20Our%20evaluation%20framework%20and%20attention%20calibration%20is%20available%20at%20https%3A//github.com/impresso/fair-sentence-transformers%0ALink%3A%20http%3A//arxiv.org/abs/2601.16934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInformation%2520Representation%2520Fairness%2520in%2520Long-Document%2520Embeddings%253A%2520The%2520Peculiar%2520Interaction%2520of%2520Positional%2520and%2520Language%2520Bias%26entry.906535625%3DElias%2520Schuhmacher%2520and%2520Andrianos%2520Michail%2520and%2520Juri%2520Opitz%2520and%2520Rico%2520Sennrich%2520and%2520Simon%2520Clematide%26entry.1292438233%3DTo%2520be%2520discoverable%2520in%2520an%2520embedding-based%2520search%2520process%252C%2520each%2520part%2520of%2520a%2520document%2520should%2520be%2520reflected%2520in%2520its%2520embedding%2520representation.%2520To%2520quantify%2520any%2520potential%2520reflection%2520biases%252C%2520we%2520introduce%2520a%2520permutation-based%2520evaluation%2520framework.%2520With%2520this%252C%2520we%2520observe%2520that%2520state-of-the-art%2520embedding%2520models%2520exhibit%2520systematic%2520positional%2520and%2520language%2520biases%2520when%2520documents%2520are%2520longer%2520and%2520consist%2520of%2520multiple%2520segments.%2520Specifically%252C%2520early%2520segments%2520and%2520segments%2520in%2520higher-resource%2520languages%2520like%2520English%2520are%2520over-represented%252C%2520while%2520later%2520segments%2520and%2520segments%2520in%2520lower-resource%2520languages%2520are%2520marginalized.%2520In%2520our%2520further%2520analysis%252C%2520we%2520find%2520that%2520the%2520positional%2520bias%2520stems%2520from%2520front-loaded%2520attention%2520distributions%2520in%2520pooling-token%2520embeddings%252C%2520where%2520early%2520tokens%2520receive%2520more%2520attention.%2520To%2520mitigate%2520this%2520issue%252C%2520we%2520introduce%2520an%2520inference-time%2520attention%2520calibration%2520method%2520that%2520redistributes%2520attention%2520more%2520evenly%2520across%2520document%2520positions%252C%2520increasing%2520discoverabiltiy%2520of%2520later%2520segments.%2520Our%2520evaluation%2520framework%2520and%2520attention%2520calibration%2520is%2520available%2520at%2520https%253A//github.com/impresso/fair-sentence-transformers%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Information%20Representation%20Fairness%20in%20Long-Document%20Embeddings%3A%20The%20Peculiar%20Interaction%20of%20Positional%20and%20Language%20Bias&entry.906535625=Elias%20Schuhmacher%20and%20Andrianos%20Michail%20and%20Juri%20Opitz%20and%20Rico%20Sennrich%20and%20Simon%20Clematide&entry.1292438233=To%20be%20discoverable%20in%20an%20embedding-based%20search%20process%2C%20each%20part%20of%20a%20document%20should%20be%20reflected%20in%20its%20embedding%20representation.%20To%20quantify%20any%20potential%20reflection%20biases%2C%20we%20introduce%20a%20permutation-based%20evaluation%20framework.%20With%20this%2C%20we%20observe%20that%20state-of-the-art%20embedding%20models%20exhibit%20systematic%20positional%20and%20language%20biases%20when%20documents%20are%20longer%20and%20consist%20of%20multiple%20segments.%20Specifically%2C%20early%20segments%20and%20segments%20in%20higher-resource%20languages%20like%20English%20are%20over-represented%2C%20while%20later%20segments%20and%20segments%20in%20lower-resource%20languages%20are%20marginalized.%20In%20our%20further%20analysis%2C%20we%20find%20that%20the%20positional%20bias%20stems%20from%20front-loaded%20attention%20distributions%20in%20pooling-token%20embeddings%2C%20where%20early%20tokens%20receive%20more%20attention.%20To%20mitigate%20this%20issue%2C%20we%20introduce%20an%20inference-time%20attention%20calibration%20method%20that%20redistributes%20attention%20more%20evenly%20across%20document%20positions%2C%20increasing%20discoverabiltiy%20of%20later%20segments.%20Our%20evaluation%20framework%20and%20attention%20calibration%20is%20available%20at%20https%3A//github.com/impresso/fair-sentence-transformers&entry.1838667208=http%3A//arxiv.org/abs/2601.16934v1&entry.124074799=Read"},
{"title": "The Trajectory Alignment Coefficient in Two Acts: From Reward Tuning to Reward Learning", "author": "Calarina Muslimani and Yunshu Du and Kenta Kawamoto and Kaushik Subramanian and Peter Stone and Peter Wurman", "abstract": "The success of reinforcement learning (RL) is fundamentally tied to having a reward function that accurately reflects the task objective. Yet, designing reward functions is notoriously time-consuming and prone to misspecification. To address this issue, our first goal is to understand how to support RL practitioners in specifying appropriate weights for a reward function. We leverage the Trajectory Alignment Coefficient (TAC), a metric that evaluates how closely a reward function's induced preferences match those of a domain expert. To evaluate whether TAC provides effective support in practice, we conducted a human-subject study in which RL practitioners tuned reward weights for Lunar Lander. We found that providing TAC during reward tuning led participants to produce more performant reward functions and report lower cognitive workload relative to standard tuning without TAC. However, the study also underscored that manual reward design, even with TAC, remains labor-intensive. This limitation motivated our second goal: to learn a reward model that maximizes TAC directly. Specifically, we propose Soft-TAC, a differentiable approximation of TAC that can be used as a loss function to train reward models from human preference data. Validated in the racing simulator Gran Turismo 7, reward models trained using Soft-TAC successfully captured preference-specific objectives, resulting in policies with qualitatively more distinct behaviors than models trained with standard Cross-Entropy loss. This work demonstrates that TAC can serve as both a practical tool for guiding reward tuning and a reward learning objective in complex domains.", "link": "http://arxiv.org/abs/2601.16906v1", "date": "2026-01-23", "relevancy": 1.9889, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5048}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4926}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Trajectory%20Alignment%20Coefficient%20in%20Two%20Acts%3A%20From%20Reward%20Tuning%20to%20Reward%20Learning&body=Title%3A%20The%20Trajectory%20Alignment%20Coefficient%20in%20Two%20Acts%3A%20From%20Reward%20Tuning%20to%20Reward%20Learning%0AAuthor%3A%20Calarina%20Muslimani%20and%20Yunshu%20Du%20and%20Kenta%20Kawamoto%20and%20Kaushik%20Subramanian%20and%20Peter%20Stone%20and%20Peter%20Wurman%0AAbstract%3A%20The%20success%20of%20reinforcement%20learning%20%28RL%29%20is%20fundamentally%20tied%20to%20having%20a%20reward%20function%20that%20accurately%20reflects%20the%20task%20objective.%20Yet%2C%20designing%20reward%20functions%20is%20notoriously%20time-consuming%20and%20prone%20to%20misspecification.%20To%20address%20this%20issue%2C%20our%20first%20goal%20is%20to%20understand%20how%20to%20support%20RL%20practitioners%20in%20specifying%20appropriate%20weights%20for%20a%20reward%20function.%20We%20leverage%20the%20Trajectory%20Alignment%20Coefficient%20%28TAC%29%2C%20a%20metric%20that%20evaluates%20how%20closely%20a%20reward%20function%27s%20induced%20preferences%20match%20those%20of%20a%20domain%20expert.%20To%20evaluate%20whether%20TAC%20provides%20effective%20support%20in%20practice%2C%20we%20conducted%20a%20human-subject%20study%20in%20which%20RL%20practitioners%20tuned%20reward%20weights%20for%20Lunar%20Lander.%20We%20found%20that%20providing%20TAC%20during%20reward%20tuning%20led%20participants%20to%20produce%20more%20performant%20reward%20functions%20and%20report%20lower%20cognitive%20workload%20relative%20to%20standard%20tuning%20without%20TAC.%20However%2C%20the%20study%20also%20underscored%20that%20manual%20reward%20design%2C%20even%20with%20TAC%2C%20remains%20labor-intensive.%20This%20limitation%20motivated%20our%20second%20goal%3A%20to%20learn%20a%20reward%20model%20that%20maximizes%20TAC%20directly.%20Specifically%2C%20we%20propose%20Soft-TAC%2C%20a%20differentiable%20approximation%20of%20TAC%20that%20can%20be%20used%20as%20a%20loss%20function%20to%20train%20reward%20models%20from%20human%20preference%20data.%20Validated%20in%20the%20racing%20simulator%20Gran%20Turismo%207%2C%20reward%20models%20trained%20using%20Soft-TAC%20successfully%20captured%20preference-specific%20objectives%2C%20resulting%20in%20policies%20with%20qualitatively%20more%20distinct%20behaviors%20than%20models%20trained%20with%20standard%20Cross-Entropy%20loss.%20This%20work%20demonstrates%20that%20TAC%20can%20serve%20as%20both%20a%20practical%20tool%20for%20guiding%20reward%20tuning%20and%20a%20reward%20learning%20objective%20in%20complex%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Trajectory%2520Alignment%2520Coefficient%2520in%2520Two%2520Acts%253A%2520From%2520Reward%2520Tuning%2520to%2520Reward%2520Learning%26entry.906535625%3DCalarina%2520Muslimani%2520and%2520Yunshu%2520Du%2520and%2520Kenta%2520Kawamoto%2520and%2520Kaushik%2520Subramanian%2520and%2520Peter%2520Stone%2520and%2520Peter%2520Wurman%26entry.1292438233%3DThe%2520success%2520of%2520reinforcement%2520learning%2520%2528RL%2529%2520is%2520fundamentally%2520tied%2520to%2520having%2520a%2520reward%2520function%2520that%2520accurately%2520reflects%2520the%2520task%2520objective.%2520Yet%252C%2520designing%2520reward%2520functions%2520is%2520notoriously%2520time-consuming%2520and%2520prone%2520to%2520misspecification.%2520To%2520address%2520this%2520issue%252C%2520our%2520first%2520goal%2520is%2520to%2520understand%2520how%2520to%2520support%2520RL%2520practitioners%2520in%2520specifying%2520appropriate%2520weights%2520for%2520a%2520reward%2520function.%2520We%2520leverage%2520the%2520Trajectory%2520Alignment%2520Coefficient%2520%2528TAC%2529%252C%2520a%2520metric%2520that%2520evaluates%2520how%2520closely%2520a%2520reward%2520function%2527s%2520induced%2520preferences%2520match%2520those%2520of%2520a%2520domain%2520expert.%2520To%2520evaluate%2520whether%2520TAC%2520provides%2520effective%2520support%2520in%2520practice%252C%2520we%2520conducted%2520a%2520human-subject%2520study%2520in%2520which%2520RL%2520practitioners%2520tuned%2520reward%2520weights%2520for%2520Lunar%2520Lander.%2520We%2520found%2520that%2520providing%2520TAC%2520during%2520reward%2520tuning%2520led%2520participants%2520to%2520produce%2520more%2520performant%2520reward%2520functions%2520and%2520report%2520lower%2520cognitive%2520workload%2520relative%2520to%2520standard%2520tuning%2520without%2520TAC.%2520However%252C%2520the%2520study%2520also%2520underscored%2520that%2520manual%2520reward%2520design%252C%2520even%2520with%2520TAC%252C%2520remains%2520labor-intensive.%2520This%2520limitation%2520motivated%2520our%2520second%2520goal%253A%2520to%2520learn%2520a%2520reward%2520model%2520that%2520maximizes%2520TAC%2520directly.%2520Specifically%252C%2520we%2520propose%2520Soft-TAC%252C%2520a%2520differentiable%2520approximation%2520of%2520TAC%2520that%2520can%2520be%2520used%2520as%2520a%2520loss%2520function%2520to%2520train%2520reward%2520models%2520from%2520human%2520preference%2520data.%2520Validated%2520in%2520the%2520racing%2520simulator%2520Gran%2520Turismo%25207%252C%2520reward%2520models%2520trained%2520using%2520Soft-TAC%2520successfully%2520captured%2520preference-specific%2520objectives%252C%2520resulting%2520in%2520policies%2520with%2520qualitatively%2520more%2520distinct%2520behaviors%2520than%2520models%2520trained%2520with%2520standard%2520Cross-Entropy%2520loss.%2520This%2520work%2520demonstrates%2520that%2520TAC%2520can%2520serve%2520as%2520both%2520a%2520practical%2520tool%2520for%2520guiding%2520reward%2520tuning%2520and%2520a%2520reward%2520learning%2520objective%2520in%2520complex%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Trajectory%20Alignment%20Coefficient%20in%20Two%20Acts%3A%20From%20Reward%20Tuning%20to%20Reward%20Learning&entry.906535625=Calarina%20Muslimani%20and%20Yunshu%20Du%20and%20Kenta%20Kawamoto%20and%20Kaushik%20Subramanian%20and%20Peter%20Stone%20and%20Peter%20Wurman&entry.1292438233=The%20success%20of%20reinforcement%20learning%20%28RL%29%20is%20fundamentally%20tied%20to%20having%20a%20reward%20function%20that%20accurately%20reflects%20the%20task%20objective.%20Yet%2C%20designing%20reward%20functions%20is%20notoriously%20time-consuming%20and%20prone%20to%20misspecification.%20To%20address%20this%20issue%2C%20our%20first%20goal%20is%20to%20understand%20how%20to%20support%20RL%20practitioners%20in%20specifying%20appropriate%20weights%20for%20a%20reward%20function.%20We%20leverage%20the%20Trajectory%20Alignment%20Coefficient%20%28TAC%29%2C%20a%20metric%20that%20evaluates%20how%20closely%20a%20reward%20function%27s%20induced%20preferences%20match%20those%20of%20a%20domain%20expert.%20To%20evaluate%20whether%20TAC%20provides%20effective%20support%20in%20practice%2C%20we%20conducted%20a%20human-subject%20study%20in%20which%20RL%20practitioners%20tuned%20reward%20weights%20for%20Lunar%20Lander.%20We%20found%20that%20providing%20TAC%20during%20reward%20tuning%20led%20participants%20to%20produce%20more%20performant%20reward%20functions%20and%20report%20lower%20cognitive%20workload%20relative%20to%20standard%20tuning%20without%20TAC.%20However%2C%20the%20study%20also%20underscored%20that%20manual%20reward%20design%2C%20even%20with%20TAC%2C%20remains%20labor-intensive.%20This%20limitation%20motivated%20our%20second%20goal%3A%20to%20learn%20a%20reward%20model%20that%20maximizes%20TAC%20directly.%20Specifically%2C%20we%20propose%20Soft-TAC%2C%20a%20differentiable%20approximation%20of%20TAC%20that%20can%20be%20used%20as%20a%20loss%20function%20to%20train%20reward%20models%20from%20human%20preference%20data.%20Validated%20in%20the%20racing%20simulator%20Gran%20Turismo%207%2C%20reward%20models%20trained%20using%20Soft-TAC%20successfully%20captured%20preference-specific%20objectives%2C%20resulting%20in%20policies%20with%20qualitatively%20more%20distinct%20behaviors%20than%20models%20trained%20with%20standard%20Cross-Entropy%20loss.%20This%20work%20demonstrates%20that%20TAC%20can%20serve%20as%20both%20a%20practical%20tool%20for%20guiding%20reward%20tuning%20and%20a%20reward%20learning%20objective%20in%20complex%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2601.16906v1&entry.124074799=Read"},
{"title": "No Validation, No Problem: Predicting Model Performance from a Single Gradient", "author": "Fangzheng Wu and Brian Summa", "abstract": "We propose a validation-free checkpointing signal from a single forward-backward pass: the Frobenius norm of the classifier-head gradient on one detached-feature batch, ||g||_F = ||dL/dW||_F. Across ImageNet-1k CNNs and Transformers, this proxy is strongly negative with Top-1 and positive with loss. Selecting the checkpoint with the minimum head gradient in a short tail window closes most of the gap to the oracle (4.24% +/- 2.00% with a universal setup, about 1.12% with light per-family tuning). For practical deployment, a head-scale normalization is more stable within classic CNN families (e.g., ResNets), while a feature-scale normalization works well for Transformers and modern CNNs. The same one-batch probe also predicts COCO detection/segmentation mAP. In diffusion (UNet/DDPM on CIFAR-10), it tracks progress and enables near-oracle tail-window selection; it is positively correlated with same-distribution probe MSE and negatively with FID (lower is better), so it can be used as a lightweight, label-free monitor. Validation labels are never used beyond reporting. The probe adds much less than 0.1% of an epoch and works as a drop-in for validation-free checkpoint selection and early stopping.", "link": "http://arxiv.org/abs/2601.16874v1", "date": "2026-01-23", "relevancy": 1.9766, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5056}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5027}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20Validation%2C%20No%20Problem%3A%20Predicting%20Model%20Performance%20from%20a%20Single%20Gradient&body=Title%3A%20No%20Validation%2C%20No%20Problem%3A%20Predicting%20Model%20Performance%20from%20a%20Single%20Gradient%0AAuthor%3A%20Fangzheng%20Wu%20and%20Brian%20Summa%0AAbstract%3A%20We%20propose%20a%20validation-free%20checkpointing%20signal%20from%20a%20single%20forward-backward%20pass%3A%20the%20Frobenius%20norm%20of%20the%20classifier-head%20gradient%20on%20one%20detached-feature%20batch%2C%20%7C%7Cg%7C%7C_F%20%3D%20%7C%7CdL/dW%7C%7C_F.%20Across%20ImageNet-1k%20CNNs%20and%20Transformers%2C%20this%20proxy%20is%20strongly%20negative%20with%20Top-1%20and%20positive%20with%20loss.%20Selecting%20the%20checkpoint%20with%20the%20minimum%20head%20gradient%20in%20a%20short%20tail%20window%20closes%20most%20of%20the%20gap%20to%20the%20oracle%20%284.24%25%20%2B/-%202.00%25%20with%20a%20universal%20setup%2C%20about%201.12%25%20with%20light%20per-family%20tuning%29.%20For%20practical%20deployment%2C%20a%20head-scale%20normalization%20is%20more%20stable%20within%20classic%20CNN%20families%20%28e.g.%2C%20ResNets%29%2C%20while%20a%20feature-scale%20normalization%20works%20well%20for%20Transformers%20and%20modern%20CNNs.%20The%20same%20one-batch%20probe%20also%20predicts%20COCO%20detection/segmentation%20mAP.%20In%20diffusion%20%28UNet/DDPM%20on%20CIFAR-10%29%2C%20it%20tracks%20progress%20and%20enables%20near-oracle%20tail-window%20selection%3B%20it%20is%20positively%20correlated%20with%20same-distribution%20probe%20MSE%20and%20negatively%20with%20FID%20%28lower%20is%20better%29%2C%20so%20it%20can%20be%20used%20as%20a%20lightweight%2C%20label-free%20monitor.%20Validation%20labels%20are%20never%20used%20beyond%20reporting.%20The%20probe%20adds%20much%20less%20than%200.1%25%20of%20an%20epoch%20and%20works%20as%20a%20drop-in%20for%20validation-free%20checkpoint%20selection%20and%20early%20stopping.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520Validation%252C%2520No%2520Problem%253A%2520Predicting%2520Model%2520Performance%2520from%2520a%2520Single%2520Gradient%26entry.906535625%3DFangzheng%2520Wu%2520and%2520Brian%2520Summa%26entry.1292438233%3DWe%2520propose%2520a%2520validation-free%2520checkpointing%2520signal%2520from%2520a%2520single%2520forward-backward%2520pass%253A%2520the%2520Frobenius%2520norm%2520of%2520the%2520classifier-head%2520gradient%2520on%2520one%2520detached-feature%2520batch%252C%2520%257C%257Cg%257C%257C_F%2520%253D%2520%257C%257CdL/dW%257C%257C_F.%2520Across%2520ImageNet-1k%2520CNNs%2520and%2520Transformers%252C%2520this%2520proxy%2520is%2520strongly%2520negative%2520with%2520Top-1%2520and%2520positive%2520with%2520loss.%2520Selecting%2520the%2520checkpoint%2520with%2520the%2520minimum%2520head%2520gradient%2520in%2520a%2520short%2520tail%2520window%2520closes%2520most%2520of%2520the%2520gap%2520to%2520the%2520oracle%2520%25284.24%2525%2520%252B/-%25202.00%2525%2520with%2520a%2520universal%2520setup%252C%2520about%25201.12%2525%2520with%2520light%2520per-family%2520tuning%2529.%2520For%2520practical%2520deployment%252C%2520a%2520head-scale%2520normalization%2520is%2520more%2520stable%2520within%2520classic%2520CNN%2520families%2520%2528e.g.%252C%2520ResNets%2529%252C%2520while%2520a%2520feature-scale%2520normalization%2520works%2520well%2520for%2520Transformers%2520and%2520modern%2520CNNs.%2520The%2520same%2520one-batch%2520probe%2520also%2520predicts%2520COCO%2520detection/segmentation%2520mAP.%2520In%2520diffusion%2520%2528UNet/DDPM%2520on%2520CIFAR-10%2529%252C%2520it%2520tracks%2520progress%2520and%2520enables%2520near-oracle%2520tail-window%2520selection%253B%2520it%2520is%2520positively%2520correlated%2520with%2520same-distribution%2520probe%2520MSE%2520and%2520negatively%2520with%2520FID%2520%2528lower%2520is%2520better%2529%252C%2520so%2520it%2520can%2520be%2520used%2520as%2520a%2520lightweight%252C%2520label-free%2520monitor.%2520Validation%2520labels%2520are%2520never%2520used%2520beyond%2520reporting.%2520The%2520probe%2520adds%2520much%2520less%2520than%25200.1%2525%2520of%2520an%2520epoch%2520and%2520works%2520as%2520a%2520drop-in%2520for%2520validation-free%2520checkpoint%2520selection%2520and%2520early%2520stopping.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20Validation%2C%20No%20Problem%3A%20Predicting%20Model%20Performance%20from%20a%20Single%20Gradient&entry.906535625=Fangzheng%20Wu%20and%20Brian%20Summa&entry.1292438233=We%20propose%20a%20validation-free%20checkpointing%20signal%20from%20a%20single%20forward-backward%20pass%3A%20the%20Frobenius%20norm%20of%20the%20classifier-head%20gradient%20on%20one%20detached-feature%20batch%2C%20%7C%7Cg%7C%7C_F%20%3D%20%7C%7CdL/dW%7C%7C_F.%20Across%20ImageNet-1k%20CNNs%20and%20Transformers%2C%20this%20proxy%20is%20strongly%20negative%20with%20Top-1%20and%20positive%20with%20loss.%20Selecting%20the%20checkpoint%20with%20the%20minimum%20head%20gradient%20in%20a%20short%20tail%20window%20closes%20most%20of%20the%20gap%20to%20the%20oracle%20%284.24%25%20%2B/-%202.00%25%20with%20a%20universal%20setup%2C%20about%201.12%25%20with%20light%20per-family%20tuning%29.%20For%20practical%20deployment%2C%20a%20head-scale%20normalization%20is%20more%20stable%20within%20classic%20CNN%20families%20%28e.g.%2C%20ResNets%29%2C%20while%20a%20feature-scale%20normalization%20works%20well%20for%20Transformers%20and%20modern%20CNNs.%20The%20same%20one-batch%20probe%20also%20predicts%20COCO%20detection/segmentation%20mAP.%20In%20diffusion%20%28UNet/DDPM%20on%20CIFAR-10%29%2C%20it%20tracks%20progress%20and%20enables%20near-oracle%20tail-window%20selection%3B%20it%20is%20positively%20correlated%20with%20same-distribution%20probe%20MSE%20and%20negatively%20with%20FID%20%28lower%20is%20better%29%2C%20so%20it%20can%20be%20used%20as%20a%20lightweight%2C%20label-free%20monitor.%20Validation%20labels%20are%20never%20used%20beyond%20reporting.%20The%20probe%20adds%20much%20less%20than%200.1%25%20of%20an%20epoch%20and%20works%20as%20a%20drop-in%20for%20validation-free%20checkpoint%20selection%20and%20early%20stopping.&entry.1838667208=http%3A//arxiv.org/abs/2601.16874v1&entry.124074799=Read"},
{"title": "The Curse of Depth in Large Language Models", "author": "Wenfang Sun and Xinyuan Song and Pengxiang Li and Lu Yin and Yefeng Zheng and Shiwei Liu", "abstract": "In this paper, we introduce the Curse of Depth, a concept that highlights, explains, and addresses the recent observation in modern Large Language Models (LLMs) where nearly half of the layers are less effective than expected. We first confirm the wide existence of this phenomenon across the most popular families of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis, theoretically and empirically, identifies that the underlying reason for the ineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer Normalization (Pre-LN). While Pre-LN stabilizes the training of Transformer LLMs, its output variance exponentially grows with the model depth, which undesirably causes the derivative of the deep Transformer blocks to be an identity matrix, and therefore barely contributes to the training. To resolve this training pitfall, we propose LayerNorm Scaling (LNS), which scales the variance of output of the layer normalization inversely by the square root of its depth. This simple modification mitigates the output variance explosion of deeper Transformer layers, improving their contribution. Across a wide range of model sizes (130M to 7B), our experiments show that LNS consistently outperforms previous normalization and scaling techniques in enhancing LLM pre-training performance. Moreover, this improvement seamlessly carries over to supervised fine-tuning. All these gains can be attributed to the fact that LayerNorm Scaling enables deeper layers to contribute more effectively during training. Our code is available at \\href{https://github.com/lmsdss/LayerNorm-Scaling}{LayerNorm-Scaling}.", "link": "http://arxiv.org/abs/2502.05795v4", "date": "2026-01-23", "relevancy": 1.975, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Curse%20of%20Depth%20in%20Large%20Language%20Models&body=Title%3A%20The%20Curse%20of%20Depth%20in%20Large%20Language%20Models%0AAuthor%3A%20Wenfang%20Sun%20and%20Xinyuan%20Song%20and%20Pengxiang%20Li%20and%20Lu%20Yin%20and%20Yefeng%20Zheng%20and%20Shiwei%20Liu%0AAbstract%3A%20In%20this%20paper%2C%20we%20introduce%20the%20Curse%20of%20Depth%2C%20a%20concept%20that%20highlights%2C%20explains%2C%20and%20addresses%20the%20recent%20observation%20in%20modern%20Large%20Language%20Models%20%28LLMs%29%20where%20nearly%20half%20of%20the%20layers%20are%20less%20effective%20than%20expected.%20We%20first%20confirm%20the%20wide%20existence%20of%20this%20phenomenon%20across%20the%20most%20popular%20families%20of%20LLMs%20such%20as%20Llama%2C%20Mistral%2C%20DeepSeek%2C%20and%20Qwen.%20Our%20analysis%2C%20theoretically%20and%20empirically%2C%20identifies%20that%20the%20underlying%20reason%20for%20the%20ineffectiveness%20of%20deep%20layers%20in%20LLMs%20is%20the%20widespread%20usage%20of%20Pre-Layer%20Normalization%20%28Pre-LN%29.%20While%20Pre-LN%20stabilizes%20the%20training%20of%20Transformer%20LLMs%2C%20its%20output%20variance%20exponentially%20grows%20with%20the%20model%20depth%2C%20which%20undesirably%20causes%20the%20derivative%20of%20the%20deep%20Transformer%20blocks%20to%20be%20an%20identity%20matrix%2C%20and%20therefore%20barely%20contributes%20to%20the%20training.%20To%20resolve%20this%20training%20pitfall%2C%20we%20propose%20LayerNorm%20Scaling%20%28LNS%29%2C%20which%20scales%20the%20variance%20of%20output%20of%20the%20layer%20normalization%20inversely%20by%20the%20square%20root%20of%20its%20depth.%20This%20simple%20modification%20mitigates%20the%20output%20variance%20explosion%20of%20deeper%20Transformer%20layers%2C%20improving%20their%20contribution.%20Across%20a%20wide%20range%20of%20model%20sizes%20%28130M%20to%207B%29%2C%20our%20experiments%20show%20that%20LNS%20consistently%20outperforms%20previous%20normalization%20and%20scaling%20techniques%20in%20enhancing%20LLM%20pre-training%20performance.%20Moreover%2C%20this%20improvement%20seamlessly%20carries%20over%20to%20supervised%20fine-tuning.%20All%20these%20gains%20can%20be%20attributed%20to%20the%20fact%20that%20LayerNorm%20Scaling%20enables%20deeper%20layers%20to%20contribute%20more%20effectively%20during%20training.%20Our%20code%20is%20available%20at%20%5Chref%7Bhttps%3A//github.com/lmsdss/LayerNorm-Scaling%7D%7BLayerNorm-Scaling%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2502.05795v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Curse%2520of%2520Depth%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DWenfang%2520Sun%2520and%2520Xinyuan%2520Song%2520and%2520Pengxiang%2520Li%2520and%2520Lu%2520Yin%2520and%2520Yefeng%2520Zheng%2520and%2520Shiwei%2520Liu%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Curse%2520of%2520Depth%252C%2520a%2520concept%2520that%2520highlights%252C%2520explains%252C%2520and%2520addresses%2520the%2520recent%2520observation%2520in%2520modern%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520where%2520nearly%2520half%2520of%2520the%2520layers%2520are%2520less%2520effective%2520than%2520expected.%2520We%2520first%2520confirm%2520the%2520wide%2520existence%2520of%2520this%2520phenomenon%2520across%2520the%2520most%2520popular%2520families%2520of%2520LLMs%2520such%2520as%2520Llama%252C%2520Mistral%252C%2520DeepSeek%252C%2520and%2520Qwen.%2520Our%2520analysis%252C%2520theoretically%2520and%2520empirically%252C%2520identifies%2520that%2520the%2520underlying%2520reason%2520for%2520the%2520ineffectiveness%2520of%2520deep%2520layers%2520in%2520LLMs%2520is%2520the%2520widespread%2520usage%2520of%2520Pre-Layer%2520Normalization%2520%2528Pre-LN%2529.%2520While%2520Pre-LN%2520stabilizes%2520the%2520training%2520of%2520Transformer%2520LLMs%252C%2520its%2520output%2520variance%2520exponentially%2520grows%2520with%2520the%2520model%2520depth%252C%2520which%2520undesirably%2520causes%2520the%2520derivative%2520of%2520the%2520deep%2520Transformer%2520blocks%2520to%2520be%2520an%2520identity%2520matrix%252C%2520and%2520therefore%2520barely%2520contributes%2520to%2520the%2520training.%2520To%2520resolve%2520this%2520training%2520pitfall%252C%2520we%2520propose%2520LayerNorm%2520Scaling%2520%2528LNS%2529%252C%2520which%2520scales%2520the%2520variance%2520of%2520output%2520of%2520the%2520layer%2520normalization%2520inversely%2520by%2520the%2520square%2520root%2520of%2520its%2520depth.%2520This%2520simple%2520modification%2520mitigates%2520the%2520output%2520variance%2520explosion%2520of%2520deeper%2520Transformer%2520layers%252C%2520improving%2520their%2520contribution.%2520Across%2520a%2520wide%2520range%2520of%2520model%2520sizes%2520%2528130M%2520to%25207B%2529%252C%2520our%2520experiments%2520show%2520that%2520LNS%2520consistently%2520outperforms%2520previous%2520normalization%2520and%2520scaling%2520techniques%2520in%2520enhancing%2520LLM%2520pre-training%2520performance.%2520Moreover%252C%2520this%2520improvement%2520seamlessly%2520carries%2520over%2520to%2520supervised%2520fine-tuning.%2520All%2520these%2520gains%2520can%2520be%2520attributed%2520to%2520the%2520fact%2520that%2520LayerNorm%2520Scaling%2520enables%2520deeper%2520layers%2520to%2520contribute%2520more%2520effectively%2520during%2520training.%2520Our%2520code%2520is%2520available%2520at%2520%255Chref%257Bhttps%253A//github.com/lmsdss/LayerNorm-Scaling%257D%257BLayerNorm-Scaling%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05795v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Curse%20of%20Depth%20in%20Large%20Language%20Models&entry.906535625=Wenfang%20Sun%20and%20Xinyuan%20Song%20and%20Pengxiang%20Li%20and%20Lu%20Yin%20and%20Yefeng%20Zheng%20and%20Shiwei%20Liu&entry.1292438233=In%20this%20paper%2C%20we%20introduce%20the%20Curse%20of%20Depth%2C%20a%20concept%20that%20highlights%2C%20explains%2C%20and%20addresses%20the%20recent%20observation%20in%20modern%20Large%20Language%20Models%20%28LLMs%29%20where%20nearly%20half%20of%20the%20layers%20are%20less%20effective%20than%20expected.%20We%20first%20confirm%20the%20wide%20existence%20of%20this%20phenomenon%20across%20the%20most%20popular%20families%20of%20LLMs%20such%20as%20Llama%2C%20Mistral%2C%20DeepSeek%2C%20and%20Qwen.%20Our%20analysis%2C%20theoretically%20and%20empirically%2C%20identifies%20that%20the%20underlying%20reason%20for%20the%20ineffectiveness%20of%20deep%20layers%20in%20LLMs%20is%20the%20widespread%20usage%20of%20Pre-Layer%20Normalization%20%28Pre-LN%29.%20While%20Pre-LN%20stabilizes%20the%20training%20of%20Transformer%20LLMs%2C%20its%20output%20variance%20exponentially%20grows%20with%20the%20model%20depth%2C%20which%20undesirably%20causes%20the%20derivative%20of%20the%20deep%20Transformer%20blocks%20to%20be%20an%20identity%20matrix%2C%20and%20therefore%20barely%20contributes%20to%20the%20training.%20To%20resolve%20this%20training%20pitfall%2C%20we%20propose%20LayerNorm%20Scaling%20%28LNS%29%2C%20which%20scales%20the%20variance%20of%20output%20of%20the%20layer%20normalization%20inversely%20by%20the%20square%20root%20of%20its%20depth.%20This%20simple%20modification%20mitigates%20the%20output%20variance%20explosion%20of%20deeper%20Transformer%20layers%2C%20improving%20their%20contribution.%20Across%20a%20wide%20range%20of%20model%20sizes%20%28130M%20to%207B%29%2C%20our%20experiments%20show%20that%20LNS%20consistently%20outperforms%20previous%20normalization%20and%20scaling%20techniques%20in%20enhancing%20LLM%20pre-training%20performance.%20Moreover%2C%20this%20improvement%20seamlessly%20carries%20over%20to%20supervised%20fine-tuning.%20All%20these%20gains%20can%20be%20attributed%20to%20the%20fact%20that%20LayerNorm%20Scaling%20enables%20deeper%20layers%20to%20contribute%20more%20effectively%20during%20training.%20Our%20code%20is%20available%20at%20%5Chref%7Bhttps%3A//github.com/lmsdss/LayerNorm-Scaling%7D%7BLayerNorm-Scaling%7D.&entry.1838667208=http%3A//arxiv.org/abs/2502.05795v4&entry.124074799=Read"},
{"title": "Kernel smoothing on manifolds", "author": "Eunseong Bae and Wolfgang Polonik", "abstract": "Under the assumption that data lie on a compact (unknown) manifold without boundary, we derive finite sample bounds for kernel smoothing and its (first and second) derivatives, and we establish asymptotic normality through Berry-Esseen type bounds. Special cases include kernel density estimation, kernel regression and the heat kernel signature. Connections to the graph Laplacian are also discussed.", "link": "http://arxiv.org/abs/2601.16777v1", "date": "2026-01-23", "relevancy": 1.9724, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4185}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.3897}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kernel%20smoothing%20on%20manifolds&body=Title%3A%20Kernel%20smoothing%20on%20manifolds%0AAuthor%3A%20Eunseong%20Bae%20and%20Wolfgang%20Polonik%0AAbstract%3A%20Under%20the%20assumption%20that%20data%20lie%20on%20a%20compact%20%28unknown%29%20manifold%20without%20boundary%2C%20we%20derive%20finite%20sample%20bounds%20for%20kernel%20smoothing%20and%20its%20%28first%20and%20second%29%20derivatives%2C%20and%20we%20establish%20asymptotic%20normality%20through%20Berry-Esseen%20type%20bounds.%20Special%20cases%20include%20kernel%20density%20estimation%2C%20kernel%20regression%20and%20the%20heat%20kernel%20signature.%20Connections%20to%20the%20graph%20Laplacian%20are%20also%20discussed.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKernel%2520smoothing%2520on%2520manifolds%26entry.906535625%3DEunseong%2520Bae%2520and%2520Wolfgang%2520Polonik%26entry.1292438233%3DUnder%2520the%2520assumption%2520that%2520data%2520lie%2520on%2520a%2520compact%2520%2528unknown%2529%2520manifold%2520without%2520boundary%252C%2520we%2520derive%2520finite%2520sample%2520bounds%2520for%2520kernel%2520smoothing%2520and%2520its%2520%2528first%2520and%2520second%2529%2520derivatives%252C%2520and%2520we%2520establish%2520asymptotic%2520normality%2520through%2520Berry-Esseen%2520type%2520bounds.%2520Special%2520cases%2520include%2520kernel%2520density%2520estimation%252C%2520kernel%2520regression%2520and%2520the%2520heat%2520kernel%2520signature.%2520Connections%2520to%2520the%2520graph%2520Laplacian%2520are%2520also%2520discussed.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kernel%20smoothing%20on%20manifolds&entry.906535625=Eunseong%20Bae%20and%20Wolfgang%20Polonik&entry.1292438233=Under%20the%20assumption%20that%20data%20lie%20on%20a%20compact%20%28unknown%29%20manifold%20without%20boundary%2C%20we%20derive%20finite%20sample%20bounds%20for%20kernel%20smoothing%20and%20its%20%28first%20and%20second%29%20derivatives%2C%20and%20we%20establish%20asymptotic%20normality%20through%20Berry-Esseen%20type%20bounds.%20Special%20cases%20include%20kernel%20density%20estimation%2C%20kernel%20regression%20and%20the%20heat%20kernel%20signature.%20Connections%20to%20the%20graph%20Laplacian%20are%20also%20discussed.&entry.1838667208=http%3A//arxiv.org/abs/2601.16777v1&entry.124074799=Read"},
{"title": "Sample-wise Constrained Learning via a Sequential Penalty Approach with Applications in Image Processing", "author": "Francesca Lanzillotta and Chiara Albisani and Davide Pucci and Daniele Baracchi and Alessandro Piva and Matteo Lapucci", "abstract": "In many learning tasks, certain requirements on the processing of individual data samples should arguably be formalized as strict constraints in the underlying optimization problem, rather than by means of arbitrary penalties. We show that, in these scenarios, learning can be carried out exploiting a sequential penalty method that allows to properly deal with constraints. The proposed algorithm is shown to possess convergence guarantees under assumptions that are reasonable in deep learning scenarios. Moreover, the results of experiments on image processing tasks show that the method is indeed viable to be used in practice.", "link": "http://arxiv.org/abs/2601.16812v1", "date": "2026-01-23", "relevancy": 1.9713, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.502}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4875}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sample-wise%20Constrained%20Learning%20via%20a%20Sequential%20Penalty%20Approach%20with%20Applications%20in%20Image%20Processing&body=Title%3A%20Sample-wise%20Constrained%20Learning%20via%20a%20Sequential%20Penalty%20Approach%20with%20Applications%20in%20Image%20Processing%0AAuthor%3A%20Francesca%20Lanzillotta%20and%20Chiara%20Albisani%20and%20Davide%20Pucci%20and%20Daniele%20Baracchi%20and%20Alessandro%20Piva%20and%20Matteo%20Lapucci%0AAbstract%3A%20In%20many%20learning%20tasks%2C%20certain%20requirements%20on%20the%20processing%20of%20individual%20data%20samples%20should%20arguably%20be%20formalized%20as%20strict%20constraints%20in%20the%20underlying%20optimization%20problem%2C%20rather%20than%20by%20means%20of%20arbitrary%20penalties.%20We%20show%20that%2C%20in%20these%20scenarios%2C%20learning%20can%20be%20carried%20out%20exploiting%20a%20sequential%20penalty%20method%20that%20allows%20to%20properly%20deal%20with%20constraints.%20The%20proposed%20algorithm%20is%20shown%20to%20possess%20convergence%20guarantees%20under%20assumptions%20that%20are%20reasonable%20in%20deep%20learning%20scenarios.%20Moreover%2C%20the%20results%20of%20experiments%20on%20image%20processing%20tasks%20show%20that%20the%20method%20is%20indeed%20viable%20to%20be%20used%20in%20practice.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSample-wise%2520Constrained%2520Learning%2520via%2520a%2520Sequential%2520Penalty%2520Approach%2520with%2520Applications%2520in%2520Image%2520Processing%26entry.906535625%3DFrancesca%2520Lanzillotta%2520and%2520Chiara%2520Albisani%2520and%2520Davide%2520Pucci%2520and%2520Daniele%2520Baracchi%2520and%2520Alessandro%2520Piva%2520and%2520Matteo%2520Lapucci%26entry.1292438233%3DIn%2520many%2520learning%2520tasks%252C%2520certain%2520requirements%2520on%2520the%2520processing%2520of%2520individual%2520data%2520samples%2520should%2520arguably%2520be%2520formalized%2520as%2520strict%2520constraints%2520in%2520the%2520underlying%2520optimization%2520problem%252C%2520rather%2520than%2520by%2520means%2520of%2520arbitrary%2520penalties.%2520We%2520show%2520that%252C%2520in%2520these%2520scenarios%252C%2520learning%2520can%2520be%2520carried%2520out%2520exploiting%2520a%2520sequential%2520penalty%2520method%2520that%2520allows%2520to%2520properly%2520deal%2520with%2520constraints.%2520The%2520proposed%2520algorithm%2520is%2520shown%2520to%2520possess%2520convergence%2520guarantees%2520under%2520assumptions%2520that%2520are%2520reasonable%2520in%2520deep%2520learning%2520scenarios.%2520Moreover%252C%2520the%2520results%2520of%2520experiments%2520on%2520image%2520processing%2520tasks%2520show%2520that%2520the%2520method%2520is%2520indeed%2520viable%2520to%2520be%2520used%2520in%2520practice.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sample-wise%20Constrained%20Learning%20via%20a%20Sequential%20Penalty%20Approach%20with%20Applications%20in%20Image%20Processing&entry.906535625=Francesca%20Lanzillotta%20and%20Chiara%20Albisani%20and%20Davide%20Pucci%20and%20Daniele%20Baracchi%20and%20Alessandro%20Piva%20and%20Matteo%20Lapucci&entry.1292438233=In%20many%20learning%20tasks%2C%20certain%20requirements%20on%20the%20processing%20of%20individual%20data%20samples%20should%20arguably%20be%20formalized%20as%20strict%20constraints%20in%20the%20underlying%20optimization%20problem%2C%20rather%20than%20by%20means%20of%20arbitrary%20penalties.%20We%20show%20that%2C%20in%20these%20scenarios%2C%20learning%20can%20be%20carried%20out%20exploiting%20a%20sequential%20penalty%20method%20that%20allows%20to%20properly%20deal%20with%20constraints.%20The%20proposed%20algorithm%20is%20shown%20to%20possess%20convergence%20guarantees%20under%20assumptions%20that%20are%20reasonable%20in%20deep%20learning%20scenarios.%20Moreover%2C%20the%20results%20of%20experiments%20on%20image%20processing%20tasks%20show%20that%20the%20method%20is%20indeed%20viable%20to%20be%20used%20in%20practice.&entry.1838667208=http%3A//arxiv.org/abs/2601.16812v1&entry.124074799=Read"},
{"title": "Enhanced Generative Machine Listener", "author": "Vishnu Raj and Gouthaman KV and Shiv Gehlot and Lars Villemoes and Arijit Biswas", "abstract": "We present GMLv2, a reference-based model designed for the prediction of subjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta distribution-based loss to model the listener ratings and incorporates additional neural audio coding (NAC) subjective datasets to extend its generalization and applicability. Extensive evaluations on diverse testset demonstrate that proposed GMLv2 consistently outperforms widely used metrics, such as PEAQ and ViSQOL, both in terms of correlation with subjective scores and in reliably predicting these scores across diverse content types and codec configurations. Consequently, GMLv2 offers a scalable and automated framework for perceptual audio quality evaluation, poised to accelerate research and development in modern audio coding technologies.", "link": "http://arxiv.org/abs/2509.21463v2", "date": "2026-01-23", "relevancy": 1.9598, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5038}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4815}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Generative%20Machine%20Listener&body=Title%3A%20Enhanced%20Generative%20Machine%20Listener%0AAuthor%3A%20Vishnu%20Raj%20and%20Gouthaman%20KV%20and%20Shiv%20Gehlot%20and%20Lars%20Villemoes%20and%20Arijit%20Biswas%0AAbstract%3A%20We%20present%20GMLv2%2C%20a%20reference-based%20model%20designed%20for%20the%20prediction%20of%20subjective%20audio%20quality%20as%20measured%20by%20MUSHRA%20scores.%20GMLv2%20introduces%20a%20Beta%20distribution-based%20loss%20to%20model%20the%20listener%20ratings%20and%20incorporates%20additional%20neural%20audio%20coding%20%28NAC%29%20subjective%20datasets%20to%20extend%20its%20generalization%20and%20applicability.%20Extensive%20evaluations%20on%20diverse%20testset%20demonstrate%20that%20proposed%20GMLv2%20consistently%20outperforms%20widely%20used%20metrics%2C%20such%20as%20PEAQ%20and%20ViSQOL%2C%20both%20in%20terms%20of%20correlation%20with%20subjective%20scores%20and%20in%20reliably%20predicting%20these%20scores%20across%20diverse%20content%20types%20and%20codec%20configurations.%20Consequently%2C%20GMLv2%20offers%20a%20scalable%20and%20automated%20framework%20for%20perceptual%20audio%20quality%20evaluation%2C%20poised%20to%20accelerate%20research%20and%20development%20in%20modern%20audio%20coding%20technologies.%0ALink%3A%20http%3A//arxiv.org/abs/2509.21463v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Generative%2520Machine%2520Listener%26entry.906535625%3DVishnu%2520Raj%2520and%2520Gouthaman%2520KV%2520and%2520Shiv%2520Gehlot%2520and%2520Lars%2520Villemoes%2520and%2520Arijit%2520Biswas%26entry.1292438233%3DWe%2520present%2520GMLv2%252C%2520a%2520reference-based%2520model%2520designed%2520for%2520the%2520prediction%2520of%2520subjective%2520audio%2520quality%2520as%2520measured%2520by%2520MUSHRA%2520scores.%2520GMLv2%2520introduces%2520a%2520Beta%2520distribution-based%2520loss%2520to%2520model%2520the%2520listener%2520ratings%2520and%2520incorporates%2520additional%2520neural%2520audio%2520coding%2520%2528NAC%2529%2520subjective%2520datasets%2520to%2520extend%2520its%2520generalization%2520and%2520applicability.%2520Extensive%2520evaluations%2520on%2520diverse%2520testset%2520demonstrate%2520that%2520proposed%2520GMLv2%2520consistently%2520outperforms%2520widely%2520used%2520metrics%252C%2520such%2520as%2520PEAQ%2520and%2520ViSQOL%252C%2520both%2520in%2520terms%2520of%2520correlation%2520with%2520subjective%2520scores%2520and%2520in%2520reliably%2520predicting%2520these%2520scores%2520across%2520diverse%2520content%2520types%2520and%2520codec%2520configurations.%2520Consequently%252C%2520GMLv2%2520offers%2520a%2520scalable%2520and%2520automated%2520framework%2520for%2520perceptual%2520audio%2520quality%2520evaluation%252C%2520poised%2520to%2520accelerate%2520research%2520and%2520development%2520in%2520modern%2520audio%2520coding%2520technologies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21463v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Generative%20Machine%20Listener&entry.906535625=Vishnu%20Raj%20and%20Gouthaman%20KV%20and%20Shiv%20Gehlot%20and%20Lars%20Villemoes%20and%20Arijit%20Biswas&entry.1292438233=We%20present%20GMLv2%2C%20a%20reference-based%20model%20designed%20for%20the%20prediction%20of%20subjective%20audio%20quality%20as%20measured%20by%20MUSHRA%20scores.%20GMLv2%20introduces%20a%20Beta%20distribution-based%20loss%20to%20model%20the%20listener%20ratings%20and%20incorporates%20additional%20neural%20audio%20coding%20%28NAC%29%20subjective%20datasets%20to%20extend%20its%20generalization%20and%20applicability.%20Extensive%20evaluations%20on%20diverse%20testset%20demonstrate%20that%20proposed%20GMLv2%20consistently%20outperforms%20widely%20used%20metrics%2C%20such%20as%20PEAQ%20and%20ViSQOL%2C%20both%20in%20terms%20of%20correlation%20with%20subjective%20scores%20and%20in%20reliably%20predicting%20these%20scores%20across%20diverse%20content%20types%20and%20codec%20configurations.%20Consequently%2C%20GMLv2%20offers%20a%20scalable%20and%20automated%20framework%20for%20perceptual%20audio%20quality%20evaluation%2C%20poised%20to%20accelerate%20research%20and%20development%20in%20modern%20audio%20coding%20technologies.&entry.1838667208=http%3A//arxiv.org/abs/2509.21463v2&entry.124074799=Read"},
{"title": "Standardizing Longitudinal Radiology Report Evaluation via Large Language Model Annotation", "author": "Xinyi Wang and Grazziela Figueredo and Ruizhe Li and Xin Chen", "abstract": "Longitudinal information in radiology reports refers to the sequential tracking of findings across multiple examinations over time, which is crucial for monitoring disease progression and guiding clinical decisions. Many recent automated radiology report generation methods are designed to capture longitudinal information; however, validating their performance is challenging. There is no proper tool to consistently label temporal changes in both ground-truth and model-generated texts for meaningful comparisons. Existing annotation methods are typically labor-intensive, relying on the use of manual lexicons and rules. Complex rules are closed-source, domain specific and hard to adapt, whereas overly simple ones tend to miss essential specialised information. Large language models (LLMs) offer a promising annotation alternative, as they are capable of capturing nuanced linguistic patterns and semantic similarities without extensive manual intervention. They also adapt well to new contexts. In this study, we therefore propose an LLM-based pipeline to automatically annotate longitudinal information in radiology reports. The pipeline first identifies sentences containing relevant information and then extracts the progression of diseases. We evaluate and compare five mainstream LLMs on these two tasks using 500 manually annotated reports. Considering both efficiency and performance, Qwen2.5-32B was subsequently selected and used to annotate another 95,169 reports from the public MIMIC-CXR dataset. Our Qwen2.5-32B-annotated dataset provided us with a standardized benchmark for evaluating report generation models. Using this new benchmark, we assessed seven state-of-the-art report generation models. Our LLM-based annotation method outperforms existing annotation solutions, achieving 11.3\\% and 5.3\\% higher F1-scores for longitudinal information detection and disease tracking, respectively.", "link": "http://arxiv.org/abs/2601.16753v1", "date": "2026-01-23", "relevancy": 1.9585, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5135}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4867}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Standardizing%20Longitudinal%20Radiology%20Report%20Evaluation%20via%20Large%20Language%20Model%20Annotation&body=Title%3A%20Standardizing%20Longitudinal%20Radiology%20Report%20Evaluation%20via%20Large%20Language%20Model%20Annotation%0AAuthor%3A%20Xinyi%20Wang%20and%20Grazziela%20Figueredo%20and%20Ruizhe%20Li%20and%20Xin%20Chen%0AAbstract%3A%20Longitudinal%20information%20in%20radiology%20reports%20refers%20to%20the%20sequential%20tracking%20of%20findings%20across%20multiple%20examinations%20over%20time%2C%20which%20is%20crucial%20for%20monitoring%20disease%20progression%20and%20guiding%20clinical%20decisions.%20Many%20recent%20automated%20radiology%20report%20generation%20methods%20are%20designed%20to%20capture%20longitudinal%20information%3B%20however%2C%20validating%20their%20performance%20is%20challenging.%20There%20is%20no%20proper%20tool%20to%20consistently%20label%20temporal%20changes%20in%20both%20ground-truth%20and%20model-generated%20texts%20for%20meaningful%20comparisons.%20Existing%20annotation%20methods%20are%20typically%20labor-intensive%2C%20relying%20on%20the%20use%20of%20manual%20lexicons%20and%20rules.%20Complex%20rules%20are%20closed-source%2C%20domain%20specific%20and%20hard%20to%20adapt%2C%20whereas%20overly%20simple%20ones%20tend%20to%20miss%20essential%20specialised%20information.%20Large%20language%20models%20%28LLMs%29%20offer%20a%20promising%20annotation%20alternative%2C%20as%20they%20are%20capable%20of%20capturing%20nuanced%20linguistic%20patterns%20and%20semantic%20similarities%20without%20extensive%20manual%20intervention.%20They%20also%20adapt%20well%20to%20new%20contexts.%20In%20this%20study%2C%20we%20therefore%20propose%20an%20LLM-based%20pipeline%20to%20automatically%20annotate%20longitudinal%20information%20in%20radiology%20reports.%20The%20pipeline%20first%20identifies%20sentences%20containing%20relevant%20information%20and%20then%20extracts%20the%20progression%20of%20diseases.%20We%20evaluate%20and%20compare%20five%20mainstream%20LLMs%20on%20these%20two%20tasks%20using%20500%20manually%20annotated%20reports.%20Considering%20both%20efficiency%20and%20performance%2C%20Qwen2.5-32B%20was%20subsequently%20selected%20and%20used%20to%20annotate%20another%2095%2C169%20reports%20from%20the%20public%20MIMIC-CXR%20dataset.%20Our%20Qwen2.5-32B-annotated%20dataset%20provided%20us%20with%20a%20standardized%20benchmark%20for%20evaluating%20report%20generation%20models.%20Using%20this%20new%20benchmark%2C%20we%20assessed%20seven%20state-of-the-art%20report%20generation%20models.%20Our%20LLM-based%20annotation%20method%20outperforms%20existing%20annotation%20solutions%2C%20achieving%2011.3%5C%25%20and%205.3%5C%25%20higher%20F1-scores%20for%20longitudinal%20information%20detection%20and%20disease%20tracking%2C%20respectively.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStandardizing%2520Longitudinal%2520Radiology%2520Report%2520Evaluation%2520via%2520Large%2520Language%2520Model%2520Annotation%26entry.906535625%3DXinyi%2520Wang%2520and%2520Grazziela%2520Figueredo%2520and%2520Ruizhe%2520Li%2520and%2520Xin%2520Chen%26entry.1292438233%3DLongitudinal%2520information%2520in%2520radiology%2520reports%2520refers%2520to%2520the%2520sequential%2520tracking%2520of%2520findings%2520across%2520multiple%2520examinations%2520over%2520time%252C%2520which%2520is%2520crucial%2520for%2520monitoring%2520disease%2520progression%2520and%2520guiding%2520clinical%2520decisions.%2520Many%2520recent%2520automated%2520radiology%2520report%2520generation%2520methods%2520are%2520designed%2520to%2520capture%2520longitudinal%2520information%253B%2520however%252C%2520validating%2520their%2520performance%2520is%2520challenging.%2520There%2520is%2520no%2520proper%2520tool%2520to%2520consistently%2520label%2520temporal%2520changes%2520in%2520both%2520ground-truth%2520and%2520model-generated%2520texts%2520for%2520meaningful%2520comparisons.%2520Existing%2520annotation%2520methods%2520are%2520typically%2520labor-intensive%252C%2520relying%2520on%2520the%2520use%2520of%2520manual%2520lexicons%2520and%2520rules.%2520Complex%2520rules%2520are%2520closed-source%252C%2520domain%2520specific%2520and%2520hard%2520to%2520adapt%252C%2520whereas%2520overly%2520simple%2520ones%2520tend%2520to%2520miss%2520essential%2520specialised%2520information.%2520Large%2520language%2520models%2520%2528LLMs%2529%2520offer%2520a%2520promising%2520annotation%2520alternative%252C%2520as%2520they%2520are%2520capable%2520of%2520capturing%2520nuanced%2520linguistic%2520patterns%2520and%2520semantic%2520similarities%2520without%2520extensive%2520manual%2520intervention.%2520They%2520also%2520adapt%2520well%2520to%2520new%2520contexts.%2520In%2520this%2520study%252C%2520we%2520therefore%2520propose%2520an%2520LLM-based%2520pipeline%2520to%2520automatically%2520annotate%2520longitudinal%2520information%2520in%2520radiology%2520reports.%2520The%2520pipeline%2520first%2520identifies%2520sentences%2520containing%2520relevant%2520information%2520and%2520then%2520extracts%2520the%2520progression%2520of%2520diseases.%2520We%2520evaluate%2520and%2520compare%2520five%2520mainstream%2520LLMs%2520on%2520these%2520two%2520tasks%2520using%2520500%2520manually%2520annotated%2520reports.%2520Considering%2520both%2520efficiency%2520and%2520performance%252C%2520Qwen2.5-32B%2520was%2520subsequently%2520selected%2520and%2520used%2520to%2520annotate%2520another%252095%252C169%2520reports%2520from%2520the%2520public%2520MIMIC-CXR%2520dataset.%2520Our%2520Qwen2.5-32B-annotated%2520dataset%2520provided%2520us%2520with%2520a%2520standardized%2520benchmark%2520for%2520evaluating%2520report%2520generation%2520models.%2520Using%2520this%2520new%2520benchmark%252C%2520we%2520assessed%2520seven%2520state-of-the-art%2520report%2520generation%2520models.%2520Our%2520LLM-based%2520annotation%2520method%2520outperforms%2520existing%2520annotation%2520solutions%252C%2520achieving%252011.3%255C%2525%2520and%25205.3%255C%2525%2520higher%2520F1-scores%2520for%2520longitudinal%2520information%2520detection%2520and%2520disease%2520tracking%252C%2520respectively.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Standardizing%20Longitudinal%20Radiology%20Report%20Evaluation%20via%20Large%20Language%20Model%20Annotation&entry.906535625=Xinyi%20Wang%20and%20Grazziela%20Figueredo%20and%20Ruizhe%20Li%20and%20Xin%20Chen&entry.1292438233=Longitudinal%20information%20in%20radiology%20reports%20refers%20to%20the%20sequential%20tracking%20of%20findings%20across%20multiple%20examinations%20over%20time%2C%20which%20is%20crucial%20for%20monitoring%20disease%20progression%20and%20guiding%20clinical%20decisions.%20Many%20recent%20automated%20radiology%20report%20generation%20methods%20are%20designed%20to%20capture%20longitudinal%20information%3B%20however%2C%20validating%20their%20performance%20is%20challenging.%20There%20is%20no%20proper%20tool%20to%20consistently%20label%20temporal%20changes%20in%20both%20ground-truth%20and%20model-generated%20texts%20for%20meaningful%20comparisons.%20Existing%20annotation%20methods%20are%20typically%20labor-intensive%2C%20relying%20on%20the%20use%20of%20manual%20lexicons%20and%20rules.%20Complex%20rules%20are%20closed-source%2C%20domain%20specific%20and%20hard%20to%20adapt%2C%20whereas%20overly%20simple%20ones%20tend%20to%20miss%20essential%20specialised%20information.%20Large%20language%20models%20%28LLMs%29%20offer%20a%20promising%20annotation%20alternative%2C%20as%20they%20are%20capable%20of%20capturing%20nuanced%20linguistic%20patterns%20and%20semantic%20similarities%20without%20extensive%20manual%20intervention.%20They%20also%20adapt%20well%20to%20new%20contexts.%20In%20this%20study%2C%20we%20therefore%20propose%20an%20LLM-based%20pipeline%20to%20automatically%20annotate%20longitudinal%20information%20in%20radiology%20reports.%20The%20pipeline%20first%20identifies%20sentences%20containing%20relevant%20information%20and%20then%20extracts%20the%20progression%20of%20diseases.%20We%20evaluate%20and%20compare%20five%20mainstream%20LLMs%20on%20these%20two%20tasks%20using%20500%20manually%20annotated%20reports.%20Considering%20both%20efficiency%20and%20performance%2C%20Qwen2.5-32B%20was%20subsequently%20selected%20and%20used%20to%20annotate%20another%2095%2C169%20reports%20from%20the%20public%20MIMIC-CXR%20dataset.%20Our%20Qwen2.5-32B-annotated%20dataset%20provided%20us%20with%20a%20standardized%20benchmark%20for%20evaluating%20report%20generation%20models.%20Using%20this%20new%20benchmark%2C%20we%20assessed%20seven%20state-of-the-art%20report%20generation%20models.%20Our%20LLM-based%20annotation%20method%20outperforms%20existing%20annotation%20solutions%2C%20achieving%2011.3%5C%25%20and%205.3%5C%25%20higher%20F1-scores%20for%20longitudinal%20information%20detection%20and%20disease%20tracking%2C%20respectively.&entry.1838667208=http%3A//arxiv.org/abs/2601.16753v1&entry.124074799=Read"},
{"title": "Is BatchEnsemble a Single Model? On Calibration and Diversity of Efficient Ensembles", "author": "Anton Zamyatin and Patrick Indri and Sagar Malhotra and Thomas G\u00e4rtner", "abstract": "In resource-constrained and low-latency settings, uncertainty estimates must be efficiently obtained. Deep Ensembles provide robust epistemic uncertainty (EU) but require training multiple full-size models. BatchEnsemble aims to deliver ensemble-like EU at far lower parameter and memory cost by applying learned rank-1 perturbations to a shared base network. We show that BatchEnsemble not only underperforms Deep Ensembles but closely tracks a single model baseline in terms of accuracy, calibration and out-of-distribution (OOD) detection on CIFAR10/10C/SVHN. A controlled study on MNIST finds members are near-identical in function and parameter space, indicating limited capacity to realize distinct predictive modes. Thus, BatchEnsemble behaves more like a single model than a true ensemble.", "link": "http://arxiv.org/abs/2601.16936v1", "date": "2026-01-23", "relevancy": 1.9496, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5235}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4814}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20BatchEnsemble%20a%20Single%20Model%3F%20On%20Calibration%20and%20Diversity%20of%20Efficient%20Ensembles&body=Title%3A%20Is%20BatchEnsemble%20a%20Single%20Model%3F%20On%20Calibration%20and%20Diversity%20of%20Efficient%20Ensembles%0AAuthor%3A%20Anton%20Zamyatin%20and%20Patrick%20Indri%20and%20Sagar%20Malhotra%20and%20Thomas%20G%C3%A4rtner%0AAbstract%3A%20In%20resource-constrained%20and%20low-latency%20settings%2C%20uncertainty%20estimates%20must%20be%20efficiently%20obtained.%20Deep%20Ensembles%20provide%20robust%20epistemic%20uncertainty%20%28EU%29%20but%20require%20training%20multiple%20full-size%20models.%20BatchEnsemble%20aims%20to%20deliver%20ensemble-like%20EU%20at%20far%20lower%20parameter%20and%20memory%20cost%20by%20applying%20learned%20rank-1%20perturbations%20to%20a%20shared%20base%20network.%20We%20show%20that%20BatchEnsemble%20not%20only%20underperforms%20Deep%20Ensembles%20but%20closely%20tracks%20a%20single%20model%20baseline%20in%20terms%20of%20accuracy%2C%20calibration%20and%20out-of-distribution%20%28OOD%29%20detection%20on%20CIFAR10/10C/SVHN.%20A%20controlled%20study%20on%20MNIST%20finds%20members%20are%20near-identical%20in%20function%20and%20parameter%20space%2C%20indicating%20limited%20capacity%20to%20realize%20distinct%20predictive%20modes.%20Thus%2C%20BatchEnsemble%20behaves%20more%20like%20a%20single%20model%20than%20a%20true%20ensemble.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520BatchEnsemble%2520a%2520Single%2520Model%253F%2520On%2520Calibration%2520and%2520Diversity%2520of%2520Efficient%2520Ensembles%26entry.906535625%3DAnton%2520Zamyatin%2520and%2520Patrick%2520Indri%2520and%2520Sagar%2520Malhotra%2520and%2520Thomas%2520G%25C3%25A4rtner%26entry.1292438233%3DIn%2520resource-constrained%2520and%2520low-latency%2520settings%252C%2520uncertainty%2520estimates%2520must%2520be%2520efficiently%2520obtained.%2520Deep%2520Ensembles%2520provide%2520robust%2520epistemic%2520uncertainty%2520%2528EU%2529%2520but%2520require%2520training%2520multiple%2520full-size%2520models.%2520BatchEnsemble%2520aims%2520to%2520deliver%2520ensemble-like%2520EU%2520at%2520far%2520lower%2520parameter%2520and%2520memory%2520cost%2520by%2520applying%2520learned%2520rank-1%2520perturbations%2520to%2520a%2520shared%2520base%2520network.%2520We%2520show%2520that%2520BatchEnsemble%2520not%2520only%2520underperforms%2520Deep%2520Ensembles%2520but%2520closely%2520tracks%2520a%2520single%2520model%2520baseline%2520in%2520terms%2520of%2520accuracy%252C%2520calibration%2520and%2520out-of-distribution%2520%2528OOD%2529%2520detection%2520on%2520CIFAR10/10C/SVHN.%2520A%2520controlled%2520study%2520on%2520MNIST%2520finds%2520members%2520are%2520near-identical%2520in%2520function%2520and%2520parameter%2520space%252C%2520indicating%2520limited%2520capacity%2520to%2520realize%2520distinct%2520predictive%2520modes.%2520Thus%252C%2520BatchEnsemble%2520behaves%2520more%2520like%2520a%2520single%2520model%2520than%2520a%2520true%2520ensemble.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20BatchEnsemble%20a%20Single%20Model%3F%20On%20Calibration%20and%20Diversity%20of%20Efficient%20Ensembles&entry.906535625=Anton%20Zamyatin%20and%20Patrick%20Indri%20and%20Sagar%20Malhotra%20and%20Thomas%20G%C3%A4rtner&entry.1292438233=In%20resource-constrained%20and%20low-latency%20settings%2C%20uncertainty%20estimates%20must%20be%20efficiently%20obtained.%20Deep%20Ensembles%20provide%20robust%20epistemic%20uncertainty%20%28EU%29%20but%20require%20training%20multiple%20full-size%20models.%20BatchEnsemble%20aims%20to%20deliver%20ensemble-like%20EU%20at%20far%20lower%20parameter%20and%20memory%20cost%20by%20applying%20learned%20rank-1%20perturbations%20to%20a%20shared%20base%20network.%20We%20show%20that%20BatchEnsemble%20not%20only%20underperforms%20Deep%20Ensembles%20but%20closely%20tracks%20a%20single%20model%20baseline%20in%20terms%20of%20accuracy%2C%20calibration%20and%20out-of-distribution%20%28OOD%29%20detection%20on%20CIFAR10/10C/SVHN.%20A%20controlled%20study%20on%20MNIST%20finds%20members%20are%20near-identical%20in%20function%20and%20parameter%20space%2C%20indicating%20limited%20capacity%20to%20realize%20distinct%20predictive%20modes.%20Thus%2C%20BatchEnsemble%20behaves%20more%20like%20a%20single%20model%20than%20a%20true%20ensemble.&entry.1838667208=http%3A//arxiv.org/abs/2601.16936v1&entry.124074799=Read"},
{"title": "Q-learning with Adjoint Matching", "author": "Qiyang Li and Sergey Levine", "abstract": "We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic's action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.", "link": "http://arxiv.org/abs/2601.14234v2", "date": "2026-01-23", "relevancy": 1.9451, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4924}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4844}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Q-learning%20with%20Adjoint%20Matching&body=Title%3A%20Q-learning%20with%20Adjoint%20Matching%0AAuthor%3A%20Qiyang%20Li%20and%20Sergey%20Levine%0AAbstract%3A%20We%20propose%20Q-learning%20with%20Adjoint%20Matching%20%28QAM%29%2C%20a%20novel%20TD-based%20reinforcement%20learning%20%28RL%29%20algorithm%20that%20tackles%20a%20long-standing%20challenge%20in%20continuous-action%20RL%3A%20efficient%20optimization%20of%20an%20expressive%20diffusion%20or%20flow-matching%20policy%20with%20respect%20to%20a%20parameterized%20Q-function.%20Effective%20optimization%20requires%20exploiting%20the%20first-order%20information%20of%20the%20critic%2C%20but%20it%20is%20challenging%20to%20do%20so%20for%20flow%20or%20diffusion%20policies%20because%20direct%20gradient-based%20optimization%20via%20backpropagation%20through%20their%20multi-step%20denoising%20process%20is%20numerically%20unstable.%20Existing%20methods%20work%20around%20this%20either%20by%20only%20using%20the%20value%20and%20discarding%20the%20gradient%20information%2C%20or%20by%20relying%20on%20approximations%20that%20sacrifice%20policy%20expressivity%20or%20bias%20the%20learned%20policy.%20QAM%20sidesteps%20both%20of%20these%20challenges%20by%20leveraging%20adjoint%20matching%2C%20a%20recently%20proposed%20technique%20in%20generative%20modeling%2C%20which%20transforms%20the%20critic%27s%20action%20gradient%20to%20form%20a%20step-wise%20objective%20function%20that%20is%20free%20from%20unstable%20backpropagation%2C%20while%20providing%20an%20unbiased%2C%20expressive%20policy%20at%20the%20optimum.%20Combined%20with%20temporal-difference%20backup%20for%20critic%20learning%2C%20QAM%20consistently%20outperforms%20prior%20approaches%20on%20hard%2C%20sparse%20reward%20tasks%20in%20both%20offline%20and%20offline-to-online%20RL.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14234v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQ-learning%2520with%2520Adjoint%2520Matching%26entry.906535625%3DQiyang%2520Li%2520and%2520Sergey%2520Levine%26entry.1292438233%3DWe%2520propose%2520Q-learning%2520with%2520Adjoint%2520Matching%2520%2528QAM%2529%252C%2520a%2520novel%2520TD-based%2520reinforcement%2520learning%2520%2528RL%2529%2520algorithm%2520that%2520tackles%2520a%2520long-standing%2520challenge%2520in%2520continuous-action%2520RL%253A%2520efficient%2520optimization%2520of%2520an%2520expressive%2520diffusion%2520or%2520flow-matching%2520policy%2520with%2520respect%2520to%2520a%2520parameterized%2520Q-function.%2520Effective%2520optimization%2520requires%2520exploiting%2520the%2520first-order%2520information%2520of%2520the%2520critic%252C%2520but%2520it%2520is%2520challenging%2520to%2520do%2520so%2520for%2520flow%2520or%2520diffusion%2520policies%2520because%2520direct%2520gradient-based%2520optimization%2520via%2520backpropagation%2520through%2520their%2520multi-step%2520denoising%2520process%2520is%2520numerically%2520unstable.%2520Existing%2520methods%2520work%2520around%2520this%2520either%2520by%2520only%2520using%2520the%2520value%2520and%2520discarding%2520the%2520gradient%2520information%252C%2520or%2520by%2520relying%2520on%2520approximations%2520that%2520sacrifice%2520policy%2520expressivity%2520or%2520bias%2520the%2520learned%2520policy.%2520QAM%2520sidesteps%2520both%2520of%2520these%2520challenges%2520by%2520leveraging%2520adjoint%2520matching%252C%2520a%2520recently%2520proposed%2520technique%2520in%2520generative%2520modeling%252C%2520which%2520transforms%2520the%2520critic%2527s%2520action%2520gradient%2520to%2520form%2520a%2520step-wise%2520objective%2520function%2520that%2520is%2520free%2520from%2520unstable%2520backpropagation%252C%2520while%2520providing%2520an%2520unbiased%252C%2520expressive%2520policy%2520at%2520the%2520optimum.%2520Combined%2520with%2520temporal-difference%2520backup%2520for%2520critic%2520learning%252C%2520QAM%2520consistently%2520outperforms%2520prior%2520approaches%2520on%2520hard%252C%2520sparse%2520reward%2520tasks%2520in%2520both%2520offline%2520and%2520offline-to-online%2520RL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14234v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q-learning%20with%20Adjoint%20Matching&entry.906535625=Qiyang%20Li%20and%20Sergey%20Levine&entry.1292438233=We%20propose%20Q-learning%20with%20Adjoint%20Matching%20%28QAM%29%2C%20a%20novel%20TD-based%20reinforcement%20learning%20%28RL%29%20algorithm%20that%20tackles%20a%20long-standing%20challenge%20in%20continuous-action%20RL%3A%20efficient%20optimization%20of%20an%20expressive%20diffusion%20or%20flow-matching%20policy%20with%20respect%20to%20a%20parameterized%20Q-function.%20Effective%20optimization%20requires%20exploiting%20the%20first-order%20information%20of%20the%20critic%2C%20but%20it%20is%20challenging%20to%20do%20so%20for%20flow%20or%20diffusion%20policies%20because%20direct%20gradient-based%20optimization%20via%20backpropagation%20through%20their%20multi-step%20denoising%20process%20is%20numerically%20unstable.%20Existing%20methods%20work%20around%20this%20either%20by%20only%20using%20the%20value%20and%20discarding%20the%20gradient%20information%2C%20or%20by%20relying%20on%20approximations%20that%20sacrifice%20policy%20expressivity%20or%20bias%20the%20learned%20policy.%20QAM%20sidesteps%20both%20of%20these%20challenges%20by%20leveraging%20adjoint%20matching%2C%20a%20recently%20proposed%20technique%20in%20generative%20modeling%2C%20which%20transforms%20the%20critic%27s%20action%20gradient%20to%20form%20a%20step-wise%20objective%20function%20that%20is%20free%20from%20unstable%20backpropagation%2C%20while%20providing%20an%20unbiased%2C%20expressive%20policy%20at%20the%20optimum.%20Combined%20with%20temporal-difference%20backup%20for%20critic%20learning%2C%20QAM%20consistently%20outperforms%20prior%20approaches%20on%20hard%2C%20sparse%20reward%20tasks%20in%20both%20offline%20and%20offline-to-online%20RL.&entry.1838667208=http%3A//arxiv.org/abs/2601.14234v2&entry.124074799=Read"},
{"title": "GlueNN: gluing patchwise analytic solutions with neural networks", "author": "Doyoung Kim and Donghee Lee and Hye-Sung Lee and Jiheon Lee and Jaeok Yi", "abstract": "In the analysis of complex physical systems, the objective often extends beyond merely computing a numerical solution to capturing the precise crossover between different regimes and extracting parameters containing meaningful information. However, standard numerical solvers and conventional deep learning approaches, such as Physics-Informed Neural Networks (PINNs), typically operate as black boxes that output solution fields without disentangling the solution into its interpretable constituent parts. In this work, we propose GlueNN, a physics-informed learning framework that decomposes the global solution into interpretable, patchwise analytic components. Rather than approximating the solution directly, GlueNN promotes the integration constants of local asymptotic expansions to learnable, scale-dependent coefficient functions. By constraining these coefficients with the differential equation, the network effectively performs regime transition, smoothly interpolating between asymptotic limits without requiring ad hoc boundary matching. We demonstrate that this coefficient-centric approach reproduces accurate global solutions in various examples and thus directly extracts physical information that is not explicitly available through standard numerical integration.", "link": "http://arxiv.org/abs/2601.05889v2", "date": "2026-01-23", "relevancy": 1.922, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4856}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4815}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GlueNN%3A%20gluing%20patchwise%20analytic%20solutions%20with%20neural%20networks&body=Title%3A%20GlueNN%3A%20gluing%20patchwise%20analytic%20solutions%20with%20neural%20networks%0AAuthor%3A%20Doyoung%20Kim%20and%20Donghee%20Lee%20and%20Hye-Sung%20Lee%20and%20Jiheon%20Lee%20and%20Jaeok%20Yi%0AAbstract%3A%20In%20the%20analysis%20of%20complex%20physical%20systems%2C%20the%20objective%20often%20extends%20beyond%20merely%20computing%20a%20numerical%20solution%20to%20capturing%20the%20precise%20crossover%20between%20different%20regimes%20and%20extracting%20parameters%20containing%20meaningful%20information.%20However%2C%20standard%20numerical%20solvers%20and%20conventional%20deep%20learning%20approaches%2C%20such%20as%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%2C%20typically%20operate%20as%20black%20boxes%20that%20output%20solution%20fields%20without%20disentangling%20the%20solution%20into%20its%20interpretable%20constituent%20parts.%20In%20this%20work%2C%20we%20propose%20GlueNN%2C%20a%20physics-informed%20learning%20framework%20that%20decomposes%20the%20global%20solution%20into%20interpretable%2C%20patchwise%20analytic%20components.%20Rather%20than%20approximating%20the%20solution%20directly%2C%20GlueNN%20promotes%20the%20integration%20constants%20of%20local%20asymptotic%20expansions%20to%20learnable%2C%20scale-dependent%20coefficient%20functions.%20By%20constraining%20these%20coefficients%20with%20the%20differential%20equation%2C%20the%20network%20effectively%20performs%20regime%20transition%2C%20smoothly%20interpolating%20between%20asymptotic%20limits%20without%20requiring%20ad%20hoc%20boundary%20matching.%20We%20demonstrate%20that%20this%20coefficient-centric%20approach%20reproduces%20accurate%20global%20solutions%20in%20various%20examples%20and%20thus%20directly%20extracts%20physical%20information%20that%20is%20not%20explicitly%20available%20through%20standard%20numerical%20integration.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05889v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlueNN%253A%2520gluing%2520patchwise%2520analytic%2520solutions%2520with%2520neural%2520networks%26entry.906535625%3DDoyoung%2520Kim%2520and%2520Donghee%2520Lee%2520and%2520Hye-Sung%2520Lee%2520and%2520Jiheon%2520Lee%2520and%2520Jaeok%2520Yi%26entry.1292438233%3DIn%2520the%2520analysis%2520of%2520complex%2520physical%2520systems%252C%2520the%2520objective%2520often%2520extends%2520beyond%2520merely%2520computing%2520a%2520numerical%2520solution%2520to%2520capturing%2520the%2520precise%2520crossover%2520between%2520different%2520regimes%2520and%2520extracting%2520parameters%2520containing%2520meaningful%2520information.%2520However%252C%2520standard%2520numerical%2520solvers%2520and%2520conventional%2520deep%2520learning%2520approaches%252C%2520such%2520as%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%252C%2520typically%2520operate%2520as%2520black%2520boxes%2520that%2520output%2520solution%2520fields%2520without%2520disentangling%2520the%2520solution%2520into%2520its%2520interpretable%2520constituent%2520parts.%2520In%2520this%2520work%252C%2520we%2520propose%2520GlueNN%252C%2520a%2520physics-informed%2520learning%2520framework%2520that%2520decomposes%2520the%2520global%2520solution%2520into%2520interpretable%252C%2520patchwise%2520analytic%2520components.%2520Rather%2520than%2520approximating%2520the%2520solution%2520directly%252C%2520GlueNN%2520promotes%2520the%2520integration%2520constants%2520of%2520local%2520asymptotic%2520expansions%2520to%2520learnable%252C%2520scale-dependent%2520coefficient%2520functions.%2520By%2520constraining%2520these%2520coefficients%2520with%2520the%2520differential%2520equation%252C%2520the%2520network%2520effectively%2520performs%2520regime%2520transition%252C%2520smoothly%2520interpolating%2520between%2520asymptotic%2520limits%2520without%2520requiring%2520ad%2520hoc%2520boundary%2520matching.%2520We%2520demonstrate%2520that%2520this%2520coefficient-centric%2520approach%2520reproduces%2520accurate%2520global%2520solutions%2520in%2520various%2520examples%2520and%2520thus%2520directly%2520extracts%2520physical%2520information%2520that%2520is%2520not%2520explicitly%2520available%2520through%2520standard%2520numerical%2520integration.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05889v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GlueNN%3A%20gluing%20patchwise%20analytic%20solutions%20with%20neural%20networks&entry.906535625=Doyoung%20Kim%20and%20Donghee%20Lee%20and%20Hye-Sung%20Lee%20and%20Jiheon%20Lee%20and%20Jaeok%20Yi&entry.1292438233=In%20the%20analysis%20of%20complex%20physical%20systems%2C%20the%20objective%20often%20extends%20beyond%20merely%20computing%20a%20numerical%20solution%20to%20capturing%20the%20precise%20crossover%20between%20different%20regimes%20and%20extracting%20parameters%20containing%20meaningful%20information.%20However%2C%20standard%20numerical%20solvers%20and%20conventional%20deep%20learning%20approaches%2C%20such%20as%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%2C%20typically%20operate%20as%20black%20boxes%20that%20output%20solution%20fields%20without%20disentangling%20the%20solution%20into%20its%20interpretable%20constituent%20parts.%20In%20this%20work%2C%20we%20propose%20GlueNN%2C%20a%20physics-informed%20learning%20framework%20that%20decomposes%20the%20global%20solution%20into%20interpretable%2C%20patchwise%20analytic%20components.%20Rather%20than%20approximating%20the%20solution%20directly%2C%20GlueNN%20promotes%20the%20integration%20constants%20of%20local%20asymptotic%20expansions%20to%20learnable%2C%20scale-dependent%20coefficient%20functions.%20By%20constraining%20these%20coefficients%20with%20the%20differential%20equation%2C%20the%20network%20effectively%20performs%20regime%20transition%2C%20smoothly%20interpolating%20between%20asymptotic%20limits%20without%20requiring%20ad%20hoc%20boundary%20matching.%20We%20demonstrate%20that%20this%20coefficient-centric%20approach%20reproduces%20accurate%20global%20solutions%20in%20various%20examples%20and%20thus%20directly%20extracts%20physical%20information%20that%20is%20not%20explicitly%20available%20through%20standard%20numerical%20integration.&entry.1838667208=http%3A//arxiv.org/abs/2601.05889v2&entry.124074799=Read"},
{"title": "Explaining Group Recommendations via Counterfactuals", "author": "Maria Stratigi and Nikos Bikakis", "abstract": "Group recommender systems help users make collective choices but often lack transparency, leaving group members uncertain about why items are suggested. Existing explanation methods focus on individuals, offering limited support for groups where multiple preferences interact. In this paper, we propose a framework for group counterfactual explanations, which reveal how removing specific past interactions would change a group recommendation. We formalize this concept, introduce utility and fairness measures tailored to groups, and design heuristic algorithms, such as Pareto-based filtering and grow-and-prune strategies, for efficient explanation discovery. Experiments on MovieLens and Amazon datasets show clear trade-offs: low-cost methods produce larger, less fair explanations, while other approaches yield concise and balanced results at higher cost. Furthermore, the Pareto-filtering heuristic demonstrates significant efficiency improvements in sparse settings.", "link": "http://arxiv.org/abs/2601.16882v1", "date": "2026-01-23", "relevancy": 1.2416, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4298}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4094}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explaining%20Group%20Recommendations%20via%20Counterfactuals&body=Title%3A%20Explaining%20Group%20Recommendations%20via%20Counterfactuals%0AAuthor%3A%20Maria%20Stratigi%20and%20Nikos%20Bikakis%0AAbstract%3A%20Group%20recommender%20systems%20help%20users%20make%20collective%20choices%20but%20often%20lack%20transparency%2C%20leaving%20group%20members%20uncertain%20about%20why%20items%20are%20suggested.%20Existing%20explanation%20methods%20focus%20on%20individuals%2C%20offering%20limited%20support%20for%20groups%20where%20multiple%20preferences%20interact.%20In%20this%20paper%2C%20we%20propose%20a%20framework%20for%20group%20counterfactual%20explanations%2C%20which%20reveal%20how%20removing%20specific%20past%20interactions%20would%20change%20a%20group%20recommendation.%20We%20formalize%20this%20concept%2C%20introduce%20utility%20and%20fairness%20measures%20tailored%20to%20groups%2C%20and%20design%20heuristic%20algorithms%2C%20such%20as%20Pareto-based%20filtering%20and%20grow-and-prune%20strategies%2C%20for%20efficient%20explanation%20discovery.%20Experiments%20on%20MovieLens%20and%20Amazon%20datasets%20show%20clear%20trade-offs%3A%20low-cost%20methods%20produce%20larger%2C%20less%20fair%20explanations%2C%20while%20other%20approaches%20yield%20concise%20and%20balanced%20results%20at%20higher%20cost.%20Furthermore%2C%20the%20Pareto-filtering%20heuristic%20demonstrates%20significant%20efficiency%20improvements%20in%20sparse%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16882v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplaining%2520Group%2520Recommendations%2520via%2520Counterfactuals%26entry.906535625%3DMaria%2520Stratigi%2520and%2520Nikos%2520Bikakis%26entry.1292438233%3DGroup%2520recommender%2520systems%2520help%2520users%2520make%2520collective%2520choices%2520but%2520often%2520lack%2520transparency%252C%2520leaving%2520group%2520members%2520uncertain%2520about%2520why%2520items%2520are%2520suggested.%2520Existing%2520explanation%2520methods%2520focus%2520on%2520individuals%252C%2520offering%2520limited%2520support%2520for%2520groups%2520where%2520multiple%2520preferences%2520interact.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520framework%2520for%2520group%2520counterfactual%2520explanations%252C%2520which%2520reveal%2520how%2520removing%2520specific%2520past%2520interactions%2520would%2520change%2520a%2520group%2520recommendation.%2520We%2520formalize%2520this%2520concept%252C%2520introduce%2520utility%2520and%2520fairness%2520measures%2520tailored%2520to%2520groups%252C%2520and%2520design%2520heuristic%2520algorithms%252C%2520such%2520as%2520Pareto-based%2520filtering%2520and%2520grow-and-prune%2520strategies%252C%2520for%2520efficient%2520explanation%2520discovery.%2520Experiments%2520on%2520MovieLens%2520and%2520Amazon%2520datasets%2520show%2520clear%2520trade-offs%253A%2520low-cost%2520methods%2520produce%2520larger%252C%2520less%2520fair%2520explanations%252C%2520while%2520other%2520approaches%2520yield%2520concise%2520and%2520balanced%2520results%2520at%2520higher%2520cost.%2520Furthermore%252C%2520the%2520Pareto-filtering%2520heuristic%2520demonstrates%2520significant%2520efficiency%2520improvements%2520in%2520sparse%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16882v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explaining%20Group%20Recommendations%20via%20Counterfactuals&entry.906535625=Maria%20Stratigi%20and%20Nikos%20Bikakis&entry.1292438233=Group%20recommender%20systems%20help%20users%20make%20collective%20choices%20but%20often%20lack%20transparency%2C%20leaving%20group%20members%20uncertain%20about%20why%20items%20are%20suggested.%20Existing%20explanation%20methods%20focus%20on%20individuals%2C%20offering%20limited%20support%20for%20groups%20where%20multiple%20preferences%20interact.%20In%20this%20paper%2C%20we%20propose%20a%20framework%20for%20group%20counterfactual%20explanations%2C%20which%20reveal%20how%20removing%20specific%20past%20interactions%20would%20change%20a%20group%20recommendation.%20We%20formalize%20this%20concept%2C%20introduce%20utility%20and%20fairness%20measures%20tailored%20to%20groups%2C%20and%20design%20heuristic%20algorithms%2C%20such%20as%20Pareto-based%20filtering%20and%20grow-and-prune%20strategies%2C%20for%20efficient%20explanation%20discovery.%20Experiments%20on%20MovieLens%20and%20Amazon%20datasets%20show%20clear%20trade-offs%3A%20low-cost%20methods%20produce%20larger%2C%20less%20fair%20explanations%2C%20while%20other%20approaches%20yield%20concise%20and%20balanced%20results%20at%20higher%20cost.%20Furthermore%2C%20the%20Pareto-filtering%20heuristic%20demonstrates%20significant%20efficiency%20improvements%20in%20sparse%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2601.16882v1&entry.124074799=Read"},
{"title": "MACTAS: Self-Attention-Based Inter-Agent Communication in Multi-Agent Reinforcement Learning with Action-Value Function Decomposition", "author": "Maciej Wojtala and Bogusz Stefa\u0144czyk and Dominik Bogucki and \u0141ukasz Lepak and Jakub Strykowski and Pawe\u0142 Wawrzy\u0144ski", "abstract": "Communication is essential for the collective execution of complex tasks by human agents, motivating interest in communication mechanisms for multi-agent reinforcement learning (MARL). However, existing communication protocols in MARL are often complex and non-differentiable. In this work, we introduce a self-attention-based communication method that exchanges information between the agents in MARL. Our proposed approach is fully differentiable, allowing agents to learn to generate messages in a reward-driven manner. The method can be seamlessly integrated with any action-value function decomposition algorithm and can be viewed as an orthogonal extension of such decompositions. Notably, it includes a fixed number of trainable parameters, independent of the number of agents, which makes it scalable to large systems. Experimental results on the SMACv2 benchmark demonstrate the effectiveness of our approach, which achieves state-of-the-art performance on a number of maps. makes it scalable to large systems. Experimental results on the SMACv2 benchmark demonstrate the effectiveness of our approach, which achieves state-of-the-art performance on a number of maps.", "link": "http://arxiv.org/abs/2508.13661v3", "date": "2026-01-23", "relevancy": 1.5158, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5068}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5063}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MACTAS%3A%20Self-Attention-Based%20Inter-Agent%20Communication%20in%20Multi-Agent%20Reinforcement%20Learning%20with%20Action-Value%20Function%20Decomposition&body=Title%3A%20MACTAS%3A%20Self-Attention-Based%20Inter-Agent%20Communication%20in%20Multi-Agent%20Reinforcement%20Learning%20with%20Action-Value%20Function%20Decomposition%0AAuthor%3A%20Maciej%20Wojtala%20and%20Bogusz%20Stefa%C5%84czyk%20and%20Dominik%20Bogucki%20and%20%C5%81ukasz%20Lepak%20and%20Jakub%20Strykowski%20and%20Pawe%C5%82%20Wawrzy%C5%84ski%0AAbstract%3A%20Communication%20is%20essential%20for%20the%20collective%20execution%20of%20complex%20tasks%20by%20human%20agents%2C%20motivating%20interest%20in%20communication%20mechanisms%20for%20multi-agent%20reinforcement%20learning%20%28MARL%29.%20However%2C%20existing%20communication%20protocols%20in%20MARL%20are%20often%20complex%20and%20non-differentiable.%20In%20this%20work%2C%20we%20introduce%20a%20self-attention-based%20communication%20method%20that%20exchanges%20information%20between%20the%20agents%20in%20MARL.%20Our%20proposed%20approach%20is%20fully%20differentiable%2C%20allowing%20agents%20to%20learn%20to%20generate%20messages%20in%20a%20reward-driven%20manner.%20The%20method%20can%20be%20seamlessly%20integrated%20with%20any%20action-value%20function%20decomposition%20algorithm%20and%20can%20be%20viewed%20as%20an%20orthogonal%20extension%20of%20such%20decompositions.%20Notably%2C%20it%20includes%20a%20fixed%20number%20of%20trainable%20parameters%2C%20independent%20of%20the%20number%20of%20agents%2C%20which%20makes%20it%20scalable%20to%20large%20systems.%20Experimental%20results%20on%20the%20SMACv2%20benchmark%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20which%20achieves%20state-of-the-art%20performance%20on%20a%20number%20of%20maps.%20makes%20it%20scalable%20to%20large%20systems.%20Experimental%20results%20on%20the%20SMACv2%20benchmark%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20which%20achieves%20state-of-the-art%20performance%20on%20a%20number%20of%20maps.%0ALink%3A%20http%3A//arxiv.org/abs/2508.13661v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMACTAS%253A%2520Self-Attention-Based%2520Inter-Agent%2520Communication%2520in%2520Multi-Agent%2520Reinforcement%2520Learning%2520with%2520Action-Value%2520Function%2520Decomposition%26entry.906535625%3DMaciej%2520Wojtala%2520and%2520Bogusz%2520Stefa%25C5%2584czyk%2520and%2520Dominik%2520Bogucki%2520and%2520%25C5%2581ukasz%2520Lepak%2520and%2520Jakub%2520Strykowski%2520and%2520Pawe%25C5%2582%2520Wawrzy%25C5%2584ski%26entry.1292438233%3DCommunication%2520is%2520essential%2520for%2520the%2520collective%2520execution%2520of%2520complex%2520tasks%2520by%2520human%2520agents%252C%2520motivating%2520interest%2520in%2520communication%2520mechanisms%2520for%2520multi-agent%2520reinforcement%2520learning%2520%2528MARL%2529.%2520However%252C%2520existing%2520communication%2520protocols%2520in%2520MARL%2520are%2520often%2520complex%2520and%2520non-differentiable.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520self-attention-based%2520communication%2520method%2520that%2520exchanges%2520information%2520between%2520the%2520agents%2520in%2520MARL.%2520Our%2520proposed%2520approach%2520is%2520fully%2520differentiable%252C%2520allowing%2520agents%2520to%2520learn%2520to%2520generate%2520messages%2520in%2520a%2520reward-driven%2520manner.%2520The%2520method%2520can%2520be%2520seamlessly%2520integrated%2520with%2520any%2520action-value%2520function%2520decomposition%2520algorithm%2520and%2520can%2520be%2520viewed%2520as%2520an%2520orthogonal%2520extension%2520of%2520such%2520decompositions.%2520Notably%252C%2520it%2520includes%2520a%2520fixed%2520number%2520of%2520trainable%2520parameters%252C%2520independent%2520of%2520the%2520number%2520of%2520agents%252C%2520which%2520makes%2520it%2520scalable%2520to%2520large%2520systems.%2520Experimental%2520results%2520on%2520the%2520SMACv2%2520benchmark%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520which%2520achieves%2520state-of-the-art%2520performance%2520on%2520a%2520number%2520of%2520maps.%2520makes%2520it%2520scalable%2520to%2520large%2520systems.%2520Experimental%2520results%2520on%2520the%2520SMACv2%2520benchmark%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520which%2520achieves%2520state-of-the-art%2520performance%2520on%2520a%2520number%2520of%2520maps.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13661v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MACTAS%3A%20Self-Attention-Based%20Inter-Agent%20Communication%20in%20Multi-Agent%20Reinforcement%20Learning%20with%20Action-Value%20Function%20Decomposition&entry.906535625=Maciej%20Wojtala%20and%20Bogusz%20Stefa%C5%84czyk%20and%20Dominik%20Bogucki%20and%20%C5%81ukasz%20Lepak%20and%20Jakub%20Strykowski%20and%20Pawe%C5%82%20Wawrzy%C5%84ski&entry.1292438233=Communication%20is%20essential%20for%20the%20collective%20execution%20of%20complex%20tasks%20by%20human%20agents%2C%20motivating%20interest%20in%20communication%20mechanisms%20for%20multi-agent%20reinforcement%20learning%20%28MARL%29.%20However%2C%20existing%20communication%20protocols%20in%20MARL%20are%20often%20complex%20and%20non-differentiable.%20In%20this%20work%2C%20we%20introduce%20a%20self-attention-based%20communication%20method%20that%20exchanges%20information%20between%20the%20agents%20in%20MARL.%20Our%20proposed%20approach%20is%20fully%20differentiable%2C%20allowing%20agents%20to%20learn%20to%20generate%20messages%20in%20a%20reward-driven%20manner.%20The%20method%20can%20be%20seamlessly%20integrated%20with%20any%20action-value%20function%20decomposition%20algorithm%20and%20can%20be%20viewed%20as%20an%20orthogonal%20extension%20of%20such%20decompositions.%20Notably%2C%20it%20includes%20a%20fixed%20number%20of%20trainable%20parameters%2C%20independent%20of%20the%20number%20of%20agents%2C%20which%20makes%20it%20scalable%20to%20large%20systems.%20Experimental%20results%20on%20the%20SMACv2%20benchmark%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20which%20achieves%20state-of-the-art%20performance%20on%20a%20number%20of%20maps.%20makes%20it%20scalable%20to%20large%20systems.%20Experimental%20results%20on%20the%20SMACv2%20benchmark%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20which%20achieves%20state-of-the-art%20performance%20on%20a%20number%20of%20maps.&entry.1838667208=http%3A//arxiv.org/abs/2508.13661v3&entry.124074799=Read"},
{"title": "LLM-Based Adversarial Persuasion Attacks on Fact-Checking Systems", "author": "Jo\u00e3o A. Leite and Olesya Razuvayevskaya and Kalina Bontcheva and Carolina Scarton", "abstract": "Automated fact-checking (AFC) systems are susceptible to adversarial attacks, enabling false claims to evade detection. Existing adversarial frameworks typically rely on injecting noise or altering semantics, yet no existing framework exploits the adversarial potential of persuasion techniques, which are widely used in disinformation campaigns to manipulate audiences. In this paper, we introduce a novel class of persuasive adversarial attacks on AFCs by employing a generative LLM to rephrase claims using persuasion techniques. Considering 15 techniques grouped into 6 categories, we study the effects of persuasion on both claim verification and evidence retrieval using a decoupled evaluation strategy. Experiments on the FEVER and FEVEROUS benchmarks show that persuasion attacks can substantially degrade both verification performance and evidence retrieval. Our analysis identifies persuasion techniques as a potent class of adversarial attacks, highlighting the need for more robust AFC systems.", "link": "http://arxiv.org/abs/2601.16890v1", "date": "2026-01-23", "relevancy": 1.7757, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4571}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4438}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Based%20Adversarial%20Persuasion%20Attacks%20on%20Fact-Checking%20Systems&body=Title%3A%20LLM-Based%20Adversarial%20Persuasion%20Attacks%20on%20Fact-Checking%20Systems%0AAuthor%3A%20Jo%C3%A3o%20A.%20Leite%20and%20Olesya%20Razuvayevskaya%20and%20Kalina%20Bontcheva%20and%20Carolina%20Scarton%0AAbstract%3A%20Automated%20fact-checking%20%28AFC%29%20systems%20are%20susceptible%20to%20adversarial%20attacks%2C%20enabling%20false%20claims%20to%20evade%20detection.%20Existing%20adversarial%20frameworks%20typically%20rely%20on%20injecting%20noise%20or%20altering%20semantics%2C%20yet%20no%20existing%20framework%20exploits%20the%20adversarial%20potential%20of%20persuasion%20techniques%2C%20which%20are%20widely%20used%20in%20disinformation%20campaigns%20to%20manipulate%20audiences.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20class%20of%20persuasive%20adversarial%20attacks%20on%20AFCs%20by%20employing%20a%20generative%20LLM%20to%20rephrase%20claims%20using%20persuasion%20techniques.%20Considering%2015%20techniques%20grouped%20into%206%20categories%2C%20we%20study%20the%20effects%20of%20persuasion%20on%20both%20claim%20verification%20and%20evidence%20retrieval%20using%20a%20decoupled%20evaluation%20strategy.%20Experiments%20on%20the%20FEVER%20and%20FEVEROUS%20benchmarks%20show%20that%20persuasion%20attacks%20can%20substantially%20degrade%20both%20verification%20performance%20and%20evidence%20retrieval.%20Our%20analysis%20identifies%20persuasion%20techniques%20as%20a%20potent%20class%20of%20adversarial%20attacks%2C%20highlighting%20the%20need%20for%20more%20robust%20AFC%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16890v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Based%2520Adversarial%2520Persuasion%2520Attacks%2520on%2520Fact-Checking%2520Systems%26entry.906535625%3DJo%25C3%25A3o%2520A.%2520Leite%2520and%2520Olesya%2520Razuvayevskaya%2520and%2520Kalina%2520Bontcheva%2520and%2520Carolina%2520Scarton%26entry.1292438233%3DAutomated%2520fact-checking%2520%2528AFC%2529%2520systems%2520are%2520susceptible%2520to%2520adversarial%2520attacks%252C%2520enabling%2520false%2520claims%2520to%2520evade%2520detection.%2520Existing%2520adversarial%2520frameworks%2520typically%2520rely%2520on%2520injecting%2520noise%2520or%2520altering%2520semantics%252C%2520yet%2520no%2520existing%2520framework%2520exploits%2520the%2520adversarial%2520potential%2520of%2520persuasion%2520techniques%252C%2520which%2520are%2520widely%2520used%2520in%2520disinformation%2520campaigns%2520to%2520manipulate%2520audiences.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520class%2520of%2520persuasive%2520adversarial%2520attacks%2520on%2520AFCs%2520by%2520employing%2520a%2520generative%2520LLM%2520to%2520rephrase%2520claims%2520using%2520persuasion%2520techniques.%2520Considering%252015%2520techniques%2520grouped%2520into%25206%2520categories%252C%2520we%2520study%2520the%2520effects%2520of%2520persuasion%2520on%2520both%2520claim%2520verification%2520and%2520evidence%2520retrieval%2520using%2520a%2520decoupled%2520evaluation%2520strategy.%2520Experiments%2520on%2520the%2520FEVER%2520and%2520FEVEROUS%2520benchmarks%2520show%2520that%2520persuasion%2520attacks%2520can%2520substantially%2520degrade%2520both%2520verification%2520performance%2520and%2520evidence%2520retrieval.%2520Our%2520analysis%2520identifies%2520persuasion%2520techniques%2520as%2520a%2520potent%2520class%2520of%2520adversarial%2520attacks%252C%2520highlighting%2520the%2520need%2520for%2520more%2520robust%2520AFC%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16890v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Based%20Adversarial%20Persuasion%20Attacks%20on%20Fact-Checking%20Systems&entry.906535625=Jo%C3%A3o%20A.%20Leite%20and%20Olesya%20Razuvayevskaya%20and%20Kalina%20Bontcheva%20and%20Carolina%20Scarton&entry.1292438233=Automated%20fact-checking%20%28AFC%29%20systems%20are%20susceptible%20to%20adversarial%20attacks%2C%20enabling%20false%20claims%20to%20evade%20detection.%20Existing%20adversarial%20frameworks%20typically%20rely%20on%20injecting%20noise%20or%20altering%20semantics%2C%20yet%20no%20existing%20framework%20exploits%20the%20adversarial%20potential%20of%20persuasion%20techniques%2C%20which%20are%20widely%20used%20in%20disinformation%20campaigns%20to%20manipulate%20audiences.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20class%20of%20persuasive%20adversarial%20attacks%20on%20AFCs%20by%20employing%20a%20generative%20LLM%20to%20rephrase%20claims%20using%20persuasion%20techniques.%20Considering%2015%20techniques%20grouped%20into%206%20categories%2C%20we%20study%20the%20effects%20of%20persuasion%20on%20both%20claim%20verification%20and%20evidence%20retrieval%20using%20a%20decoupled%20evaluation%20strategy.%20Experiments%20on%20the%20FEVER%20and%20FEVEROUS%20benchmarks%20show%20that%20persuasion%20attacks%20can%20substantially%20degrade%20both%20verification%20performance%20and%20evidence%20retrieval.%20Our%20analysis%20identifies%20persuasion%20techniques%20as%20a%20potent%20class%20of%20adversarial%20attacks%2C%20highlighting%20the%20need%20for%20more%20robust%20AFC%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.16890v1&entry.124074799=Read"},
{"title": "FRQI Pairs method for image classification using Quantum Recurrent Neural Network", "author": "Rafa\u0142 Potempa and Micha\u0142 Kordasz and Sundas Naqeeb Khan and Krzysztof Werner and Kamil Wereszczy\u0144ski and Krzysztof Simi\u0144ski and Krzysztof A. Cyran", "abstract": "This study aims to introduce the FRQI Pairs method to a wider audience, a novel approach to image classification using Quantum Recurrent Neural Networks (QRNN) with Flexible Representation for Quantum Images (FRQI). The study highlights an innovative approach to use quantum encoded data for an image classification task, suggesting that such quantum-based approaches could significantly reduce the complexity of quantum algorithms. Comparison of the FRQI Pairs method with contemporary techniques underscores the promise of integrating quantum computing principles with neural network architectures for the development of quantum machine learning.", "link": "http://arxiv.org/abs/2512.11499v2", "date": "2026-01-23", "relevancy": 1.7103, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4448}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4297}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FRQI%20Pairs%20method%20for%20image%20classification%20using%20Quantum%20Recurrent%20Neural%20Network&body=Title%3A%20FRQI%20Pairs%20method%20for%20image%20classification%20using%20Quantum%20Recurrent%20Neural%20Network%0AAuthor%3A%20Rafa%C5%82%20Potempa%20and%20Micha%C5%82%20Kordasz%20and%20Sundas%20Naqeeb%20Khan%20and%20Krzysztof%20Werner%20and%20Kamil%20Wereszczy%C5%84ski%20and%20Krzysztof%20Simi%C5%84ski%20and%20Krzysztof%20A.%20Cyran%0AAbstract%3A%20This%20study%20aims%20to%20introduce%20the%20FRQI%20Pairs%20method%20to%20a%20wider%20audience%2C%20a%20novel%20approach%20to%20image%20classification%20using%20Quantum%20Recurrent%20Neural%20Networks%20%28QRNN%29%20with%20Flexible%20Representation%20for%20Quantum%20Images%20%28FRQI%29.%20The%20study%20highlights%20an%20innovative%20approach%20to%20use%20quantum%20encoded%20data%20for%20an%20image%20classification%20task%2C%20suggesting%20that%20such%20quantum-based%20approaches%20could%20significantly%20reduce%20the%20complexity%20of%20quantum%20algorithms.%20Comparison%20of%20the%20FRQI%20Pairs%20method%20with%20contemporary%20techniques%20underscores%20the%20promise%20of%20integrating%20quantum%20computing%20principles%20with%20neural%20network%20architectures%20for%20the%20development%20of%20quantum%20machine%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11499v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFRQI%2520Pairs%2520method%2520for%2520image%2520classification%2520using%2520Quantum%2520Recurrent%2520Neural%2520Network%26entry.906535625%3DRafa%25C5%2582%2520Potempa%2520and%2520Micha%25C5%2582%2520Kordasz%2520and%2520Sundas%2520Naqeeb%2520Khan%2520and%2520Krzysztof%2520Werner%2520and%2520Kamil%2520Wereszczy%25C5%2584ski%2520and%2520Krzysztof%2520Simi%25C5%2584ski%2520and%2520Krzysztof%2520A.%2520Cyran%26entry.1292438233%3DThis%2520study%2520aims%2520to%2520introduce%2520the%2520FRQI%2520Pairs%2520method%2520to%2520a%2520wider%2520audience%252C%2520a%2520novel%2520approach%2520to%2520image%2520classification%2520using%2520Quantum%2520Recurrent%2520Neural%2520Networks%2520%2528QRNN%2529%2520with%2520Flexible%2520Representation%2520for%2520Quantum%2520Images%2520%2528FRQI%2529.%2520The%2520study%2520highlights%2520an%2520innovative%2520approach%2520to%2520use%2520quantum%2520encoded%2520data%2520for%2520an%2520image%2520classification%2520task%252C%2520suggesting%2520that%2520such%2520quantum-based%2520approaches%2520could%2520significantly%2520reduce%2520the%2520complexity%2520of%2520quantum%2520algorithms.%2520Comparison%2520of%2520the%2520FRQI%2520Pairs%2520method%2520with%2520contemporary%2520techniques%2520underscores%2520the%2520promise%2520of%2520integrating%2520quantum%2520computing%2520principles%2520with%2520neural%2520network%2520architectures%2520for%2520the%2520development%2520of%2520quantum%2520machine%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11499v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRQI%20Pairs%20method%20for%20image%20classification%20using%20Quantum%20Recurrent%20Neural%20Network&entry.906535625=Rafa%C5%82%20Potempa%20and%20Micha%C5%82%20Kordasz%20and%20Sundas%20Naqeeb%20Khan%20and%20Krzysztof%20Werner%20and%20Kamil%20Wereszczy%C5%84ski%20and%20Krzysztof%20Simi%C5%84ski%20and%20Krzysztof%20A.%20Cyran&entry.1292438233=This%20study%20aims%20to%20introduce%20the%20FRQI%20Pairs%20method%20to%20a%20wider%20audience%2C%20a%20novel%20approach%20to%20image%20classification%20using%20Quantum%20Recurrent%20Neural%20Networks%20%28QRNN%29%20with%20Flexible%20Representation%20for%20Quantum%20Images%20%28FRQI%29.%20The%20study%20highlights%20an%20innovative%20approach%20to%20use%20quantum%20encoded%20data%20for%20an%20image%20classification%20task%2C%20suggesting%20that%20such%20quantum-based%20approaches%20could%20significantly%20reduce%20the%20complexity%20of%20quantum%20algorithms.%20Comparison%20of%20the%20FRQI%20Pairs%20method%20with%20contemporary%20techniques%20underscores%20the%20promise%20of%20integrating%20quantum%20computing%20principles%20with%20neural%20network%20architectures%20for%20the%20development%20of%20quantum%20machine%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2512.11499v2&entry.124074799=Read"},
{"title": "Learning Logical Rules using Minimum Message Length", "author": "Ruben Sharma and Sebastijan Duman\u010di\u0107 and Ross D. King and Andrew Cropper", "abstract": "Unifying probabilistic and logical learning is a key challenge in AI. We introduce a Bayesian inductive logic programming approach that learns minimum message length hypotheses from noisy data. Our approach balances hypothesis complexity and data fit through priors, which favour more general programs, and a likelihood, which favours accurate programs. Our experiments on several domains, including game playing and drug design, show that our method significantly outperforms previous methods, notably those that learn minimum description length programs. Our results also show that our approach is data-efficient and insensitive to example balance, including the ability to learn from exclusively positive examples.", "link": "http://arxiv.org/abs/2508.06230v2", "date": "2026-01-23", "relevancy": 1.8053, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4819}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4806}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Logical%20Rules%20using%20Minimum%20Message%20Length&body=Title%3A%20Learning%20Logical%20Rules%20using%20Minimum%20Message%20Length%0AAuthor%3A%20Ruben%20Sharma%20and%20Sebastijan%20Duman%C4%8Di%C4%87%20and%20Ross%20D.%20King%20and%20Andrew%20Cropper%0AAbstract%3A%20Unifying%20probabilistic%20and%20logical%20learning%20is%20a%20key%20challenge%20in%20AI.%20We%20introduce%20a%20Bayesian%20inductive%20logic%20programming%20approach%20that%20learns%20minimum%20message%20length%20hypotheses%20from%20noisy%20data.%20Our%20approach%20balances%20hypothesis%20complexity%20and%20data%20fit%20through%20priors%2C%20which%20favour%20more%20general%20programs%2C%20and%20a%20likelihood%2C%20which%20favours%20accurate%20programs.%20Our%20experiments%20on%20several%20domains%2C%20including%20game%20playing%20and%20drug%20design%2C%20show%20that%20our%20method%20significantly%20outperforms%20previous%20methods%2C%20notably%20those%20that%20learn%20minimum%20description%20length%20programs.%20Our%20results%20also%20show%20that%20our%20approach%20is%20data-efficient%20and%20insensitive%20to%20example%20balance%2C%20including%20the%20ability%20to%20learn%20from%20exclusively%20positive%20examples.%0ALink%3A%20http%3A//arxiv.org/abs/2508.06230v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Logical%2520Rules%2520using%2520Minimum%2520Message%2520Length%26entry.906535625%3DRuben%2520Sharma%2520and%2520Sebastijan%2520Duman%25C4%258Di%25C4%2587%2520and%2520Ross%2520D.%2520King%2520and%2520Andrew%2520Cropper%26entry.1292438233%3DUnifying%2520probabilistic%2520and%2520logical%2520learning%2520is%2520a%2520key%2520challenge%2520in%2520AI.%2520We%2520introduce%2520a%2520Bayesian%2520inductive%2520logic%2520programming%2520approach%2520that%2520learns%2520minimum%2520message%2520length%2520hypotheses%2520from%2520noisy%2520data.%2520Our%2520approach%2520balances%2520hypothesis%2520complexity%2520and%2520data%2520fit%2520through%2520priors%252C%2520which%2520favour%2520more%2520general%2520programs%252C%2520and%2520a%2520likelihood%252C%2520which%2520favours%2520accurate%2520programs.%2520Our%2520experiments%2520on%2520several%2520domains%252C%2520including%2520game%2520playing%2520and%2520drug%2520design%252C%2520show%2520that%2520our%2520method%2520significantly%2520outperforms%2520previous%2520methods%252C%2520notably%2520those%2520that%2520learn%2520minimum%2520description%2520length%2520programs.%2520Our%2520results%2520also%2520show%2520that%2520our%2520approach%2520is%2520data-efficient%2520and%2520insensitive%2520to%2520example%2520balance%252C%2520including%2520the%2520ability%2520to%2520learn%2520from%2520exclusively%2520positive%2520examples.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06230v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Logical%20Rules%20using%20Minimum%20Message%20Length&entry.906535625=Ruben%20Sharma%20and%20Sebastijan%20Duman%C4%8Di%C4%87%20and%20Ross%20D.%20King%20and%20Andrew%20Cropper&entry.1292438233=Unifying%20probabilistic%20and%20logical%20learning%20is%20a%20key%20challenge%20in%20AI.%20We%20introduce%20a%20Bayesian%20inductive%20logic%20programming%20approach%20that%20learns%20minimum%20message%20length%20hypotheses%20from%20noisy%20data.%20Our%20approach%20balances%20hypothesis%20complexity%20and%20data%20fit%20through%20priors%2C%20which%20favour%20more%20general%20programs%2C%20and%20a%20likelihood%2C%20which%20favours%20accurate%20programs.%20Our%20experiments%20on%20several%20domains%2C%20including%20game%20playing%20and%20drug%20design%2C%20show%20that%20our%20method%20significantly%20outperforms%20previous%20methods%2C%20notably%20those%20that%20learn%20minimum%20description%20length%20programs.%20Our%20results%20also%20show%20that%20our%20approach%20is%20data-efficient%20and%20insensitive%20to%20example%20balance%2C%20including%20the%20ability%20to%20learn%20from%20exclusively%20positive%20examples.&entry.1838667208=http%3A//arxiv.org/abs/2508.06230v2&entry.124074799=Read"},
{"title": "Boosting Deep Reinforcement Learning with Semantic Knowledge for Robotic Manipulators", "author": "Luc\u00eda G\u00fcitta-L\u00f3pez and Vincenzo Suriani and Jaime Boal and \u00c1lvaro J. L\u00f3pez-L\u00f3pez and Daniele Nardi", "abstract": "Deep Reinforcement Learning (DRL) is a powerful framework for solving complex sequential decision-making problems, particularly in robotic control. However, its practical deployment is often hindered by the substantial amount of experience required for learning, which results in high computational and time costs. In this work, we propose a novel integration of DRL with semantic knowledge in the form of Knowledge Graph Embeddings (KGEs), aiming to enhance learning efficiency by providing contextual information to the agent. Our architecture combines KGEs with visual observations, enabling the agent to exploit environmental knowledge during training. Experimental validation with robotic manipulators in environments featuring both fixed and randomized target attributes demonstrates that our method achieves up to {60}{\\%} reduction in learning time and improves task accuracy by approximately 15 percentage points, without increasing training time or computational complexity. These results highlight the potential of semantic knowledge to reduce sample complexity and improve the effectiveness of DRL in robotic applications.", "link": "http://arxiv.org/abs/2601.16866v1", "date": "2026-01-23", "relevancy": 1.6325, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5977}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5945}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Deep%20Reinforcement%20Learning%20with%20Semantic%20Knowledge%20for%20Robotic%20Manipulators&body=Title%3A%20Boosting%20Deep%20Reinforcement%20Learning%20with%20Semantic%20Knowledge%20for%20Robotic%20Manipulators%0AAuthor%3A%20Luc%C3%ADa%20G%C3%BCitta-L%C3%B3pez%20and%20Vincenzo%20Suriani%20and%20Jaime%20Boal%20and%20%C3%81lvaro%20J.%20L%C3%B3pez-L%C3%B3pez%20and%20Daniele%20Nardi%0AAbstract%3A%20Deep%20Reinforcement%20Learning%20%28DRL%29%20is%20a%20powerful%20framework%20for%20solving%20complex%20sequential%20decision-making%20problems%2C%20particularly%20in%20robotic%20control.%20However%2C%20its%20practical%20deployment%20is%20often%20hindered%20by%20the%20substantial%20amount%20of%20experience%20required%20for%20learning%2C%20which%20results%20in%20high%20computational%20and%20time%20costs.%20In%20this%20work%2C%20we%20propose%20a%20novel%20integration%20of%20DRL%20with%20semantic%20knowledge%20in%20the%20form%20of%20Knowledge%20Graph%20Embeddings%20%28KGEs%29%2C%20aiming%20to%20enhance%20learning%20efficiency%20by%20providing%20contextual%20information%20to%20the%20agent.%20Our%20architecture%20combines%20KGEs%20with%20visual%20observations%2C%20enabling%20the%20agent%20to%20exploit%20environmental%20knowledge%20during%20training.%20Experimental%20validation%20with%20robotic%20manipulators%20in%20environments%20featuring%20both%20fixed%20and%20randomized%20target%20attributes%20demonstrates%20that%20our%20method%20achieves%20up%20to%20%7B60%7D%7B%5C%25%7D%20reduction%20in%20learning%20time%20and%20improves%20task%20accuracy%20by%20approximately%2015%20percentage%20points%2C%20without%20increasing%20training%20time%20or%20computational%20complexity.%20These%20results%20highlight%20the%20potential%20of%20semantic%20knowledge%20to%20reduce%20sample%20complexity%20and%20improve%20the%20effectiveness%20of%20DRL%20in%20robotic%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16866v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Deep%2520Reinforcement%2520Learning%2520with%2520Semantic%2520Knowledge%2520for%2520Robotic%2520Manipulators%26entry.906535625%3DLuc%25C3%25ADa%2520G%25C3%25BCitta-L%25C3%25B3pez%2520and%2520Vincenzo%2520Suriani%2520and%2520Jaime%2520Boal%2520and%2520%25C3%2581lvaro%2520J.%2520L%25C3%25B3pez-L%25C3%25B3pez%2520and%2520Daniele%2520Nardi%26entry.1292438233%3DDeep%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520is%2520a%2520powerful%2520framework%2520for%2520solving%2520complex%2520sequential%2520decision-making%2520problems%252C%2520particularly%2520in%2520robotic%2520control.%2520However%252C%2520its%2520practical%2520deployment%2520is%2520often%2520hindered%2520by%2520the%2520substantial%2520amount%2520of%2520experience%2520required%2520for%2520learning%252C%2520which%2520results%2520in%2520high%2520computational%2520and%2520time%2520costs.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520integration%2520of%2520DRL%2520with%2520semantic%2520knowledge%2520in%2520the%2520form%2520of%2520Knowledge%2520Graph%2520Embeddings%2520%2528KGEs%2529%252C%2520aiming%2520to%2520enhance%2520learning%2520efficiency%2520by%2520providing%2520contextual%2520information%2520to%2520the%2520agent.%2520Our%2520architecture%2520combines%2520KGEs%2520with%2520visual%2520observations%252C%2520enabling%2520the%2520agent%2520to%2520exploit%2520environmental%2520knowledge%2520during%2520training.%2520Experimental%2520validation%2520with%2520robotic%2520manipulators%2520in%2520environments%2520featuring%2520both%2520fixed%2520and%2520randomized%2520target%2520attributes%2520demonstrates%2520that%2520our%2520method%2520achieves%2520up%2520to%2520%257B60%257D%257B%255C%2525%257D%2520reduction%2520in%2520learning%2520time%2520and%2520improves%2520task%2520accuracy%2520by%2520approximately%252015%2520percentage%2520points%252C%2520without%2520increasing%2520training%2520time%2520or%2520computational%2520complexity.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520semantic%2520knowledge%2520to%2520reduce%2520sample%2520complexity%2520and%2520improve%2520the%2520effectiveness%2520of%2520DRL%2520in%2520robotic%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16866v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Deep%20Reinforcement%20Learning%20with%20Semantic%20Knowledge%20for%20Robotic%20Manipulators&entry.906535625=Luc%C3%ADa%20G%C3%BCitta-L%C3%B3pez%20and%20Vincenzo%20Suriani%20and%20Jaime%20Boal%20and%20%C3%81lvaro%20J.%20L%C3%B3pez-L%C3%B3pez%20and%20Daniele%20Nardi&entry.1292438233=Deep%20Reinforcement%20Learning%20%28DRL%29%20is%20a%20powerful%20framework%20for%20solving%20complex%20sequential%20decision-making%20problems%2C%20particularly%20in%20robotic%20control.%20However%2C%20its%20practical%20deployment%20is%20often%20hindered%20by%20the%20substantial%20amount%20of%20experience%20required%20for%20learning%2C%20which%20results%20in%20high%20computational%20and%20time%20costs.%20In%20this%20work%2C%20we%20propose%20a%20novel%20integration%20of%20DRL%20with%20semantic%20knowledge%20in%20the%20form%20of%20Knowledge%20Graph%20Embeddings%20%28KGEs%29%2C%20aiming%20to%20enhance%20learning%20efficiency%20by%20providing%20contextual%20information%20to%20the%20agent.%20Our%20architecture%20combines%20KGEs%20with%20visual%20observations%2C%20enabling%20the%20agent%20to%20exploit%20environmental%20knowledge%20during%20training.%20Experimental%20validation%20with%20robotic%20manipulators%20in%20environments%20featuring%20both%20fixed%20and%20randomized%20target%20attributes%20demonstrates%20that%20our%20method%20achieves%20up%20to%20%7B60%7D%7B%5C%25%7D%20reduction%20in%20learning%20time%20and%20improves%20task%20accuracy%20by%20approximately%2015%20percentage%20points%2C%20without%20increasing%20training%20time%20or%20computational%20complexity.%20These%20results%20highlight%20the%20potential%20of%20semantic%20knowledge%20to%20reduce%20sample%20complexity%20and%20improve%20the%20effectiveness%20of%20DRL%20in%20robotic%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2601.16866v1&entry.124074799=Read"},
{"title": "Provably Robust Bayesian Counterfactual Explanations under Model Changes", "author": "Jamie Duell and Xiuyi Fan", "abstract": "Counterfactual explanations (CEs) offer interpretable insights into machine learning predictions by answering ``what if?\" questions. However, in real-world settings where models are frequently updated, existing counterfactual explanations can quickly become invalid or unreliable. In this paper, we introduce Probabilistically Safe CEs (PSCE), a method for generating counterfactual explanations that are $\u03b4$-safe, to ensure high predictive confidence, and $\u03b5$-robust to ensure low predictive variance. Based on Bayesian principles, PSCE provides formal probabilistic guarantees for CEs under model changes which are adhered to in what we refer to as the $\\langle \u03b4, \u03b5\\rangle$-set. Uncertainty-aware constraints are integrated into our optimization framework and we validate our method empirically across diverse datasets. We compare our approach against state-of-the-art Bayesian CE methods, where PSCE produces counterfactual explanations that are not only more plausible and discriminative, but also provably robust under model change.", "link": "http://arxiv.org/abs/2601.16659v1", "date": "2026-01-23", "relevancy": 1.4335, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4995}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4742}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provably%20Robust%20Bayesian%20Counterfactual%20Explanations%20under%20Model%20Changes&body=Title%3A%20Provably%20Robust%20Bayesian%20Counterfactual%20Explanations%20under%20Model%20Changes%0AAuthor%3A%20Jamie%20Duell%20and%20Xiuyi%20Fan%0AAbstract%3A%20Counterfactual%20explanations%20%28CEs%29%20offer%20interpretable%20insights%20into%20machine%20learning%20predictions%20by%20answering%20%60%60what%20if%3F%22%20questions.%20However%2C%20in%20real-world%20settings%20where%20models%20are%20frequently%20updated%2C%20existing%20counterfactual%20explanations%20can%20quickly%20become%20invalid%20or%20unreliable.%20In%20this%20paper%2C%20we%20introduce%20Probabilistically%20Safe%20CEs%20%28PSCE%29%2C%20a%20method%20for%20generating%20counterfactual%20explanations%20that%20are%20%24%CE%B4%24-safe%2C%20to%20ensure%20high%20predictive%20confidence%2C%20and%20%24%CE%B5%24-robust%20to%20ensure%20low%20predictive%20variance.%20Based%20on%20Bayesian%20principles%2C%20PSCE%20provides%20formal%20probabilistic%20guarantees%20for%20CEs%20under%20model%20changes%20which%20are%20adhered%20to%20in%20what%20we%20refer%20to%20as%20the%20%24%5Clangle%20%CE%B4%2C%20%CE%B5%5Crangle%24-set.%20Uncertainty-aware%20constraints%20are%20integrated%20into%20our%20optimization%20framework%20and%20we%20validate%20our%20method%20empirically%20across%20diverse%20datasets.%20We%20compare%20our%20approach%20against%20state-of-the-art%20Bayesian%20CE%20methods%2C%20where%20PSCE%20produces%20counterfactual%20explanations%20that%20are%20not%20only%20more%20plausible%20and%20discriminative%2C%20but%20also%20provably%20robust%20under%20model%20change.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvably%2520Robust%2520Bayesian%2520Counterfactual%2520Explanations%2520under%2520Model%2520Changes%26entry.906535625%3DJamie%2520Duell%2520and%2520Xiuyi%2520Fan%26entry.1292438233%3DCounterfactual%2520explanations%2520%2528CEs%2529%2520offer%2520interpretable%2520insights%2520into%2520machine%2520learning%2520predictions%2520by%2520answering%2520%2560%2560what%2520if%253F%2522%2520questions.%2520However%252C%2520in%2520real-world%2520settings%2520where%2520models%2520are%2520frequently%2520updated%252C%2520existing%2520counterfactual%2520explanations%2520can%2520quickly%2520become%2520invalid%2520or%2520unreliable.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Probabilistically%2520Safe%2520CEs%2520%2528PSCE%2529%252C%2520a%2520method%2520for%2520generating%2520counterfactual%2520explanations%2520that%2520are%2520%2524%25CE%25B4%2524-safe%252C%2520to%2520ensure%2520high%2520predictive%2520confidence%252C%2520and%2520%2524%25CE%25B5%2524-robust%2520to%2520ensure%2520low%2520predictive%2520variance.%2520Based%2520on%2520Bayesian%2520principles%252C%2520PSCE%2520provides%2520formal%2520probabilistic%2520guarantees%2520for%2520CEs%2520under%2520model%2520changes%2520which%2520are%2520adhered%2520to%2520in%2520what%2520we%2520refer%2520to%2520as%2520the%2520%2524%255Clangle%2520%25CE%25B4%252C%2520%25CE%25B5%255Crangle%2524-set.%2520Uncertainty-aware%2520constraints%2520are%2520integrated%2520into%2520our%2520optimization%2520framework%2520and%2520we%2520validate%2520our%2520method%2520empirically%2520across%2520diverse%2520datasets.%2520We%2520compare%2520our%2520approach%2520against%2520state-of-the-art%2520Bayesian%2520CE%2520methods%252C%2520where%2520PSCE%2520produces%2520counterfactual%2520explanations%2520that%2520are%2520not%2520only%2520more%2520plausible%2520and%2520discriminative%252C%2520but%2520also%2520provably%2520robust%2520under%2520model%2520change.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provably%20Robust%20Bayesian%20Counterfactual%20Explanations%20under%20Model%20Changes&entry.906535625=Jamie%20Duell%20and%20Xiuyi%20Fan&entry.1292438233=Counterfactual%20explanations%20%28CEs%29%20offer%20interpretable%20insights%20into%20machine%20learning%20predictions%20by%20answering%20%60%60what%20if%3F%22%20questions.%20However%2C%20in%20real-world%20settings%20where%20models%20are%20frequently%20updated%2C%20existing%20counterfactual%20explanations%20can%20quickly%20become%20invalid%20or%20unreliable.%20In%20this%20paper%2C%20we%20introduce%20Probabilistically%20Safe%20CEs%20%28PSCE%29%2C%20a%20method%20for%20generating%20counterfactual%20explanations%20that%20are%20%24%CE%B4%24-safe%2C%20to%20ensure%20high%20predictive%20confidence%2C%20and%20%24%CE%B5%24-robust%20to%20ensure%20low%20predictive%20variance.%20Based%20on%20Bayesian%20principles%2C%20PSCE%20provides%20formal%20probabilistic%20guarantees%20for%20CEs%20under%20model%20changes%20which%20are%20adhered%20to%20in%20what%20we%20refer%20to%20as%20the%20%24%5Clangle%20%CE%B4%2C%20%CE%B5%5Crangle%24-set.%20Uncertainty-aware%20constraints%20are%20integrated%20into%20our%20optimization%20framework%20and%20we%20validate%20our%20method%20empirically%20across%20diverse%20datasets.%20We%20compare%20our%20approach%20against%20state-of-the-art%20Bayesian%20CE%20methods%2C%20where%20PSCE%20produces%20counterfactual%20explanations%20that%20are%20not%20only%20more%20plausible%20and%20discriminative%2C%20but%20also%20provably%20robust%20under%20model%20change.&entry.1838667208=http%3A//arxiv.org/abs/2601.16659v1&entry.124074799=Read"},
{"title": "A Multimodal Data Collection Framework for Dialogue-Driven Assistive Robotics to Clarify Ambiguities: A Wizard-of-Oz Pilot Study", "author": "Guangping Liu and Nicholas Hawkins and Billy Madden and Tipu Sultan and Flavio Esposito and Madi Babaiasl", "abstract": "Integrated control of wheelchairs and wheelchair-mounted robotic arms (WMRAs) has strong potential to increase independence for users with severe motor limitations, yet existing interfaces often lack the flexibility needed for intuitive assistive interaction. Although data-driven AI methods show promise, progress is limited by the lack of multimodal datasets that capture natural Human-Robot Interaction (HRI), particularly conversational ambiguity in dialogue-driven control. To address this gap, we propose a multimodal data collection framework that employs a dialogue-based interaction protocol and a two-room Wizard-of-Oz (WoZ) setup to simulate robot autonomy while eliciting natural user behavior. The framework records five synchronized modalities: RGB-D video, conversational audio, inertial measurement unit (IMU) signals, end-effector Cartesian pose, and whole-body joint states across five assistive tasks. Using this framework, we collected a pilot dataset of 53 trials from five participants and validated its quality through motion smoothness analysis and user feedback. The results show that the framework effectively captures diverse ambiguity types and supports natural dialogue-driven interaction, demonstrating its suitability for scaling to a larger dataset for learning, benchmarking, and evaluation of ambiguity-aware assistive control.", "link": "http://arxiv.org/abs/2601.16870v1", "date": "2026-01-23", "relevancy": 1.7813, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6243}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5679}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multimodal%20Data%20Collection%20Framework%20for%20Dialogue-Driven%20Assistive%20Robotics%20to%20Clarify%20Ambiguities%3A%20A%20Wizard-of-Oz%20Pilot%20Study&body=Title%3A%20A%20Multimodal%20Data%20Collection%20Framework%20for%20Dialogue-Driven%20Assistive%20Robotics%20to%20Clarify%20Ambiguities%3A%20A%20Wizard-of-Oz%20Pilot%20Study%0AAuthor%3A%20Guangping%20Liu%20and%20Nicholas%20Hawkins%20and%20Billy%20Madden%20and%20Tipu%20Sultan%20and%20Flavio%20Esposito%20and%20Madi%20Babaiasl%0AAbstract%3A%20Integrated%20control%20of%20wheelchairs%20and%20wheelchair-mounted%20robotic%20arms%20%28WMRAs%29%20has%20strong%20potential%20to%20increase%20independence%20for%20users%20with%20severe%20motor%20limitations%2C%20yet%20existing%20interfaces%20often%20lack%20the%20flexibility%20needed%20for%20intuitive%20assistive%20interaction.%20Although%20data-driven%20AI%20methods%20show%20promise%2C%20progress%20is%20limited%20by%20the%20lack%20of%20multimodal%20datasets%20that%20capture%20natural%20Human-Robot%20Interaction%20%28HRI%29%2C%20particularly%20conversational%20ambiguity%20in%20dialogue-driven%20control.%20To%20address%20this%20gap%2C%20we%20propose%20a%20multimodal%20data%20collection%20framework%20that%20employs%20a%20dialogue-based%20interaction%20protocol%20and%20a%20two-room%20Wizard-of-Oz%20%28WoZ%29%20setup%20to%20simulate%20robot%20autonomy%20while%20eliciting%20natural%20user%20behavior.%20The%20framework%20records%20five%20synchronized%20modalities%3A%20RGB-D%20video%2C%20conversational%20audio%2C%20inertial%20measurement%20unit%20%28IMU%29%20signals%2C%20end-effector%20Cartesian%20pose%2C%20and%20whole-body%20joint%20states%20across%20five%20assistive%20tasks.%20Using%20this%20framework%2C%20we%20collected%20a%20pilot%20dataset%20of%2053%20trials%20from%20five%20participants%20and%20validated%20its%20quality%20through%20motion%20smoothness%20analysis%20and%20user%20feedback.%20The%20results%20show%20that%20the%20framework%20effectively%20captures%20diverse%20ambiguity%20types%20and%20supports%20natural%20dialogue-driven%20interaction%2C%20demonstrating%20its%20suitability%20for%20scaling%20to%20a%20larger%20dataset%20for%20learning%2C%20benchmarking%2C%20and%20evaluation%20of%20ambiguity-aware%20assistive%20control.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multimodal%2520Data%2520Collection%2520Framework%2520for%2520Dialogue-Driven%2520Assistive%2520Robotics%2520to%2520Clarify%2520Ambiguities%253A%2520A%2520Wizard-of-Oz%2520Pilot%2520Study%26entry.906535625%3DGuangping%2520Liu%2520and%2520Nicholas%2520Hawkins%2520and%2520Billy%2520Madden%2520and%2520Tipu%2520Sultan%2520and%2520Flavio%2520Esposito%2520and%2520Madi%2520Babaiasl%26entry.1292438233%3DIntegrated%2520control%2520of%2520wheelchairs%2520and%2520wheelchair-mounted%2520robotic%2520arms%2520%2528WMRAs%2529%2520has%2520strong%2520potential%2520to%2520increase%2520independence%2520for%2520users%2520with%2520severe%2520motor%2520limitations%252C%2520yet%2520existing%2520interfaces%2520often%2520lack%2520the%2520flexibility%2520needed%2520for%2520intuitive%2520assistive%2520interaction.%2520Although%2520data-driven%2520AI%2520methods%2520show%2520promise%252C%2520progress%2520is%2520limited%2520by%2520the%2520lack%2520of%2520multimodal%2520datasets%2520that%2520capture%2520natural%2520Human-Robot%2520Interaction%2520%2528HRI%2529%252C%2520particularly%2520conversational%2520ambiguity%2520in%2520dialogue-driven%2520control.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520a%2520multimodal%2520data%2520collection%2520framework%2520that%2520employs%2520a%2520dialogue-based%2520interaction%2520protocol%2520and%2520a%2520two-room%2520Wizard-of-Oz%2520%2528WoZ%2529%2520setup%2520to%2520simulate%2520robot%2520autonomy%2520while%2520eliciting%2520natural%2520user%2520behavior.%2520The%2520framework%2520records%2520five%2520synchronized%2520modalities%253A%2520RGB-D%2520video%252C%2520conversational%2520audio%252C%2520inertial%2520measurement%2520unit%2520%2528IMU%2529%2520signals%252C%2520end-effector%2520Cartesian%2520pose%252C%2520and%2520whole-body%2520joint%2520states%2520across%2520five%2520assistive%2520tasks.%2520Using%2520this%2520framework%252C%2520we%2520collected%2520a%2520pilot%2520dataset%2520of%252053%2520trials%2520from%2520five%2520participants%2520and%2520validated%2520its%2520quality%2520through%2520motion%2520smoothness%2520analysis%2520and%2520user%2520feedback.%2520The%2520results%2520show%2520that%2520the%2520framework%2520effectively%2520captures%2520diverse%2520ambiguity%2520types%2520and%2520supports%2520natural%2520dialogue-driven%2520interaction%252C%2520demonstrating%2520its%2520suitability%2520for%2520scaling%2520to%2520a%2520larger%2520dataset%2520for%2520learning%252C%2520benchmarking%252C%2520and%2520evaluation%2520of%2520ambiguity-aware%2520assistive%2520control.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multimodal%20Data%20Collection%20Framework%20for%20Dialogue-Driven%20Assistive%20Robotics%20to%20Clarify%20Ambiguities%3A%20A%20Wizard-of-Oz%20Pilot%20Study&entry.906535625=Guangping%20Liu%20and%20Nicholas%20Hawkins%20and%20Billy%20Madden%20and%20Tipu%20Sultan%20and%20Flavio%20Esposito%20and%20Madi%20Babaiasl&entry.1292438233=Integrated%20control%20of%20wheelchairs%20and%20wheelchair-mounted%20robotic%20arms%20%28WMRAs%29%20has%20strong%20potential%20to%20increase%20independence%20for%20users%20with%20severe%20motor%20limitations%2C%20yet%20existing%20interfaces%20often%20lack%20the%20flexibility%20needed%20for%20intuitive%20assistive%20interaction.%20Although%20data-driven%20AI%20methods%20show%20promise%2C%20progress%20is%20limited%20by%20the%20lack%20of%20multimodal%20datasets%20that%20capture%20natural%20Human-Robot%20Interaction%20%28HRI%29%2C%20particularly%20conversational%20ambiguity%20in%20dialogue-driven%20control.%20To%20address%20this%20gap%2C%20we%20propose%20a%20multimodal%20data%20collection%20framework%20that%20employs%20a%20dialogue-based%20interaction%20protocol%20and%20a%20two-room%20Wizard-of-Oz%20%28WoZ%29%20setup%20to%20simulate%20robot%20autonomy%20while%20eliciting%20natural%20user%20behavior.%20The%20framework%20records%20five%20synchronized%20modalities%3A%20RGB-D%20video%2C%20conversational%20audio%2C%20inertial%20measurement%20unit%20%28IMU%29%20signals%2C%20end-effector%20Cartesian%20pose%2C%20and%20whole-body%20joint%20states%20across%20five%20assistive%20tasks.%20Using%20this%20framework%2C%20we%20collected%20a%20pilot%20dataset%20of%2053%20trials%20from%20five%20participants%20and%20validated%20its%20quality%20through%20motion%20smoothness%20analysis%20and%20user%20feedback.%20The%20results%20show%20that%20the%20framework%20effectively%20captures%20diverse%20ambiguity%20types%20and%20supports%20natural%20dialogue-driven%20interaction%2C%20demonstrating%20its%20suitability%20for%20scaling%20to%20a%20larger%20dataset%20for%20learning%2C%20benchmarking%2C%20and%20evaluation%20of%20ambiguity-aware%20assistive%20control.&entry.1838667208=http%3A//arxiv.org/abs/2601.16870v1&entry.124074799=Read"},
{"title": "EMemBench: Interactive Benchmarking of Episodic Memory for VLM Agents", "author": "Xinze Li and Ziyue Zhu and Siyuan Liu and Yubo Ma and Yuhang Zang and Yixin Cao and Aixin Sun", "abstract": "We introduce EMemBench, a programmatic benchmark for evaluating long-term memory of agents through interactive games. Rather than using a fixed set of questions, EMemBench generates questions from each agent's own trajectory, covering both text and visual game environments. Each template computes verifiable ground truth from underlying game signals, with controlled answerability and balanced coverage over memory skills: single/multi-hop recall, induction, temporal, spatial, logical, and adversarial. We evaluate memory agents with strong LMs/VLMs as backbones, using in-context prompting as baselines. Across 15 text games and multiple visual seeds, results are far from saturated: induction and spatial reasoning are persistent bottlenecks, especially in visual setting. Persistent memory yields clear gains for open backbones on text games, but improvements are less consistent for VLM agents, suggesting that visually grounded episodic memory remains an open challenge. A human study further confirms the difficulty of EMemBench.", "link": "http://arxiv.org/abs/2601.16690v1", "date": "2026-01-23", "relevancy": 1.87, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4769}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4656}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMemBench%3A%20Interactive%20Benchmarking%20of%20Episodic%20Memory%20for%20VLM%20Agents&body=Title%3A%20EMemBench%3A%20Interactive%20Benchmarking%20of%20Episodic%20Memory%20for%20VLM%20Agents%0AAuthor%3A%20Xinze%20Li%20and%20Ziyue%20Zhu%20and%20Siyuan%20Liu%20and%20Yubo%20Ma%20and%20Yuhang%20Zang%20and%20Yixin%20Cao%20and%20Aixin%20Sun%0AAbstract%3A%20We%20introduce%20EMemBench%2C%20a%20programmatic%20benchmark%20for%20evaluating%20long-term%20memory%20of%20agents%20through%20interactive%20games.%20Rather%20than%20using%20a%20fixed%20set%20of%20questions%2C%20EMemBench%20generates%20questions%20from%20each%20agent%27s%20own%20trajectory%2C%20covering%20both%20text%20and%20visual%20game%20environments.%20Each%20template%20computes%20verifiable%20ground%20truth%20from%20underlying%20game%20signals%2C%20with%20controlled%20answerability%20and%20balanced%20coverage%20over%20memory%20skills%3A%20single/multi-hop%20recall%2C%20induction%2C%20temporal%2C%20spatial%2C%20logical%2C%20and%20adversarial.%20We%20evaluate%20memory%20agents%20with%20strong%20LMs/VLMs%20as%20backbones%2C%20using%20in-context%20prompting%20as%20baselines.%20Across%2015%20text%20games%20and%20multiple%20visual%20seeds%2C%20results%20are%20far%20from%20saturated%3A%20induction%20and%20spatial%20reasoning%20are%20persistent%20bottlenecks%2C%20especially%20in%20visual%20setting.%20Persistent%20memory%20yields%20clear%20gains%20for%20open%20backbones%20on%20text%20games%2C%20but%20improvements%20are%20less%20consistent%20for%20VLM%20agents%2C%20suggesting%20that%20visually%20grounded%20episodic%20memory%20remains%20an%20open%20challenge.%20A%20human%20study%20further%20confirms%20the%20difficulty%20of%20EMemBench.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMemBench%253A%2520Interactive%2520Benchmarking%2520of%2520Episodic%2520Memory%2520for%2520VLM%2520Agents%26entry.906535625%3DXinze%2520Li%2520and%2520Ziyue%2520Zhu%2520and%2520Siyuan%2520Liu%2520and%2520Yubo%2520Ma%2520and%2520Yuhang%2520Zang%2520and%2520Yixin%2520Cao%2520and%2520Aixin%2520Sun%26entry.1292438233%3DWe%2520introduce%2520EMemBench%252C%2520a%2520programmatic%2520benchmark%2520for%2520evaluating%2520long-term%2520memory%2520of%2520agents%2520through%2520interactive%2520games.%2520Rather%2520than%2520using%2520a%2520fixed%2520set%2520of%2520questions%252C%2520EMemBench%2520generates%2520questions%2520from%2520each%2520agent%2527s%2520own%2520trajectory%252C%2520covering%2520both%2520text%2520and%2520visual%2520game%2520environments.%2520Each%2520template%2520computes%2520verifiable%2520ground%2520truth%2520from%2520underlying%2520game%2520signals%252C%2520with%2520controlled%2520answerability%2520and%2520balanced%2520coverage%2520over%2520memory%2520skills%253A%2520single/multi-hop%2520recall%252C%2520induction%252C%2520temporal%252C%2520spatial%252C%2520logical%252C%2520and%2520adversarial.%2520We%2520evaluate%2520memory%2520agents%2520with%2520strong%2520LMs/VLMs%2520as%2520backbones%252C%2520using%2520in-context%2520prompting%2520as%2520baselines.%2520Across%252015%2520text%2520games%2520and%2520multiple%2520visual%2520seeds%252C%2520results%2520are%2520far%2520from%2520saturated%253A%2520induction%2520and%2520spatial%2520reasoning%2520are%2520persistent%2520bottlenecks%252C%2520especially%2520in%2520visual%2520setting.%2520Persistent%2520memory%2520yields%2520clear%2520gains%2520for%2520open%2520backbones%2520on%2520text%2520games%252C%2520but%2520improvements%2520are%2520less%2520consistent%2520for%2520VLM%2520agents%252C%2520suggesting%2520that%2520visually%2520grounded%2520episodic%2520memory%2520remains%2520an%2520open%2520challenge.%2520A%2520human%2520study%2520further%2520confirms%2520the%2520difficulty%2520of%2520EMemBench.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMemBench%3A%20Interactive%20Benchmarking%20of%20Episodic%20Memory%20for%20VLM%20Agents&entry.906535625=Xinze%20Li%20and%20Ziyue%20Zhu%20and%20Siyuan%20Liu%20and%20Yubo%20Ma%20and%20Yuhang%20Zang%20and%20Yixin%20Cao%20and%20Aixin%20Sun&entry.1292438233=We%20introduce%20EMemBench%2C%20a%20programmatic%20benchmark%20for%20evaluating%20long-term%20memory%20of%20agents%20through%20interactive%20games.%20Rather%20than%20using%20a%20fixed%20set%20of%20questions%2C%20EMemBench%20generates%20questions%20from%20each%20agent%27s%20own%20trajectory%2C%20covering%20both%20text%20and%20visual%20game%20environments.%20Each%20template%20computes%20verifiable%20ground%20truth%20from%20underlying%20game%20signals%2C%20with%20controlled%20answerability%20and%20balanced%20coverage%20over%20memory%20skills%3A%20single/multi-hop%20recall%2C%20induction%2C%20temporal%2C%20spatial%2C%20logical%2C%20and%20adversarial.%20We%20evaluate%20memory%20agents%20with%20strong%20LMs/VLMs%20as%20backbones%2C%20using%20in-context%20prompting%20as%20baselines.%20Across%2015%20text%20games%20and%20multiple%20visual%20seeds%2C%20results%20are%20far%20from%20saturated%3A%20induction%20and%20spatial%20reasoning%20are%20persistent%20bottlenecks%2C%20especially%20in%20visual%20setting.%20Persistent%20memory%20yields%20clear%20gains%20for%20open%20backbones%20on%20text%20games%2C%20but%20improvements%20are%20less%20consistent%20for%20VLM%20agents%2C%20suggesting%20that%20visually%20grounded%20episodic%20memory%20remains%20an%20open%20challenge.%20A%20human%20study%20further%20confirms%20the%20difficulty%20of%20EMemBench.&entry.1838667208=http%3A//arxiv.org/abs/2601.16690v1&entry.124074799=Read"},
{"title": "Estimation of discrete distributions in relative entropy, and the deviations of the missing mass", "author": "Jaouad Mourtada", "abstract": "We study the problem of estimating a distribution over a finite alphabet from an i.i.d. sample, with accuracy measured in relative entropy (Kullback-Leibler divergence). While optimal bounds on the expected risk are known, high-probability guarantees remain less well-understood. First, we analyze the classical Laplace (add-one) estimator, obtaining matching upper and lower bounds on its performance and establishing its optimality among confidence-independent estimators. We then characterize the minimax-optimal high-probability risk and show that it is achieved by a simple confidence-dependent smoothing technique. Notably, the optimal non-asymptotic risk incurs an additional logarithmic factor compared to the ideal asymptotic rate. Next, motivated by regimes in which the alphabet size exceeds the sample size, we investigate methods that adapt to the sparsity of the underlying distribution. We introduce an estimator using data-dependent smoothing, for which we establish a high-probability risk bound depending on two effective sparsity parameters. As part of our analysis, we also derive a sharp high-probability upper bound on the missing mass.", "link": "http://arxiv.org/abs/2504.21787v3", "date": "2026-01-23", "relevancy": 1.7485, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4632}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4343}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimation%20of%20discrete%20distributions%20in%20relative%20entropy%2C%20and%20the%20deviations%20of%20the%20missing%20mass&body=Title%3A%20Estimation%20of%20discrete%20distributions%20in%20relative%20entropy%2C%20and%20the%20deviations%20of%20the%20missing%20mass%0AAuthor%3A%20Jaouad%20Mourtada%0AAbstract%3A%20We%20study%20the%20problem%20of%20estimating%20a%20distribution%20over%20a%20finite%20alphabet%20from%20an%20i.i.d.%20sample%2C%20with%20accuracy%20measured%20in%20relative%20entropy%20%28Kullback-Leibler%20divergence%29.%20While%20optimal%20bounds%20on%20the%20expected%20risk%20are%20known%2C%20high-probability%20guarantees%20remain%20less%20well-understood.%20First%2C%20we%20analyze%20the%20classical%20Laplace%20%28add-one%29%20estimator%2C%20obtaining%20matching%20upper%20and%20lower%20bounds%20on%20its%20performance%20and%20establishing%20its%20optimality%20among%20confidence-independent%20estimators.%20We%20then%20characterize%20the%20minimax-optimal%20high-probability%20risk%20and%20show%20that%20it%20is%20achieved%20by%20a%20simple%20confidence-dependent%20smoothing%20technique.%20Notably%2C%20the%20optimal%20non-asymptotic%20risk%20incurs%20an%20additional%20logarithmic%20factor%20compared%20to%20the%20ideal%20asymptotic%20rate.%20Next%2C%20motivated%20by%20regimes%20in%20which%20the%20alphabet%20size%20exceeds%20the%20sample%20size%2C%20we%20investigate%20methods%20that%20adapt%20to%20the%20sparsity%20of%20the%20underlying%20distribution.%20We%20introduce%20an%20estimator%20using%20data-dependent%20smoothing%2C%20for%20which%20we%20establish%20a%20high-probability%20risk%20bound%20depending%20on%20two%20effective%20sparsity%20parameters.%20As%20part%20of%20our%20analysis%2C%20we%20also%20derive%20a%20sharp%20high-probability%20upper%20bound%20on%20the%20missing%20mass.%0ALink%3A%20http%3A//arxiv.org/abs/2504.21787v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimation%2520of%2520discrete%2520distributions%2520in%2520relative%2520entropy%252C%2520and%2520the%2520deviations%2520of%2520the%2520missing%2520mass%26entry.906535625%3DJaouad%2520Mourtada%26entry.1292438233%3DWe%2520study%2520the%2520problem%2520of%2520estimating%2520a%2520distribution%2520over%2520a%2520finite%2520alphabet%2520from%2520an%2520i.i.d.%2520sample%252C%2520with%2520accuracy%2520measured%2520in%2520relative%2520entropy%2520%2528Kullback-Leibler%2520divergence%2529.%2520While%2520optimal%2520bounds%2520on%2520the%2520expected%2520risk%2520are%2520known%252C%2520high-probability%2520guarantees%2520remain%2520less%2520well-understood.%2520First%252C%2520we%2520analyze%2520the%2520classical%2520Laplace%2520%2528add-one%2529%2520estimator%252C%2520obtaining%2520matching%2520upper%2520and%2520lower%2520bounds%2520on%2520its%2520performance%2520and%2520establishing%2520its%2520optimality%2520among%2520confidence-independent%2520estimators.%2520We%2520then%2520characterize%2520the%2520minimax-optimal%2520high-probability%2520risk%2520and%2520show%2520that%2520it%2520is%2520achieved%2520by%2520a%2520simple%2520confidence-dependent%2520smoothing%2520technique.%2520Notably%252C%2520the%2520optimal%2520non-asymptotic%2520risk%2520incurs%2520an%2520additional%2520logarithmic%2520factor%2520compared%2520to%2520the%2520ideal%2520asymptotic%2520rate.%2520Next%252C%2520motivated%2520by%2520regimes%2520in%2520which%2520the%2520alphabet%2520size%2520exceeds%2520the%2520sample%2520size%252C%2520we%2520investigate%2520methods%2520that%2520adapt%2520to%2520the%2520sparsity%2520of%2520the%2520underlying%2520distribution.%2520We%2520introduce%2520an%2520estimator%2520using%2520data-dependent%2520smoothing%252C%2520for%2520which%2520we%2520establish%2520a%2520high-probability%2520risk%2520bound%2520depending%2520on%2520two%2520effective%2520sparsity%2520parameters.%2520As%2520part%2520of%2520our%2520analysis%252C%2520we%2520also%2520derive%2520a%2520sharp%2520high-probability%2520upper%2520bound%2520on%2520the%2520missing%2520mass.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21787v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimation%20of%20discrete%20distributions%20in%20relative%20entropy%2C%20and%20the%20deviations%20of%20the%20missing%20mass&entry.906535625=Jaouad%20Mourtada&entry.1292438233=We%20study%20the%20problem%20of%20estimating%20a%20distribution%20over%20a%20finite%20alphabet%20from%20an%20i.i.d.%20sample%2C%20with%20accuracy%20measured%20in%20relative%20entropy%20%28Kullback-Leibler%20divergence%29.%20While%20optimal%20bounds%20on%20the%20expected%20risk%20are%20known%2C%20high-probability%20guarantees%20remain%20less%20well-understood.%20First%2C%20we%20analyze%20the%20classical%20Laplace%20%28add-one%29%20estimator%2C%20obtaining%20matching%20upper%20and%20lower%20bounds%20on%20its%20performance%20and%20establishing%20its%20optimality%20among%20confidence-independent%20estimators.%20We%20then%20characterize%20the%20minimax-optimal%20high-probability%20risk%20and%20show%20that%20it%20is%20achieved%20by%20a%20simple%20confidence-dependent%20smoothing%20technique.%20Notably%2C%20the%20optimal%20non-asymptotic%20risk%20incurs%20an%20additional%20logarithmic%20factor%20compared%20to%20the%20ideal%20asymptotic%20rate.%20Next%2C%20motivated%20by%20regimes%20in%20which%20the%20alphabet%20size%20exceeds%20the%20sample%20size%2C%20we%20investigate%20methods%20that%20adapt%20to%20the%20sparsity%20of%20the%20underlying%20distribution.%20We%20introduce%20an%20estimator%20using%20data-dependent%20smoothing%2C%20for%20which%20we%20establish%20a%20high-probability%20risk%20bound%20depending%20on%20two%20effective%20sparsity%20parameters.%20As%20part%20of%20our%20analysis%2C%20we%20also%20derive%20a%20sharp%20high-probability%20upper%20bound%20on%20the%20missing%20mass.&entry.1838667208=http%3A//arxiv.org/abs/2504.21787v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


