<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260121.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "CAG-Avatar: Cross-Attention Guided Gaussian Avatars for High-Fidelity Head Reconstruction", "author": "Zhe Chang and Haodong Jin and Yan Song and Hui Yu", "abstract": "Creating high-fidelity, real-time drivable 3D head avatars is a core challenge in digital animation. While 3D Gaussian Splashing (3D-GS) offers unprecedented rendering speed and quality, current animation techniques often rely on a \"one-size-fits-all\" global tuning approach, where all Gaussian primitives are uniformly driven by a single expression code. This simplistic approach fails to unravel the distinct dynamics of different facial regions, such as deformable skin versus rigid teeth, leading to significant blurring and distortion artifacts. We introduce Conditionally-Adaptive Gaussian Avatars (CAG-Avatar), a framework that resolves this key limitation. At its core is a Conditionally Adaptive Fusion Module built on cross-attention. This mechanism empowers each 3D Gaussian to act as a query, adaptively extracting relevant driving signals from the global expression code based on its canonical position. This \"tailor-made\" conditioning strategy drastically enhances the modeling of fine-grained, localized dynamics. Our experiments confirm a significant improvement in reconstruction fidelity, particularly for challenging regions such as teeth, while preserving real-time rendering performance.", "link": "http://arxiv.org/abs/2601.14844v1", "date": "2026-01-21", "relevancy": 3.6589, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7691}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7691}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAG-Avatar%3A%20Cross-Attention%20Guided%20Gaussian%20Avatars%20for%20High-Fidelity%20Head%20Reconstruction&body=Title%3A%20CAG-Avatar%3A%20Cross-Attention%20Guided%20Gaussian%20Avatars%20for%20High-Fidelity%20Head%20Reconstruction%0AAuthor%3A%20Zhe%20Chang%20and%20Haodong%20Jin%20and%20Yan%20Song%20and%20Hui%20Yu%0AAbstract%3A%20Creating%20high-fidelity%2C%20real-time%20drivable%203D%20head%20avatars%20is%20a%20core%20challenge%20in%20digital%20animation.%20While%203D%20Gaussian%20Splashing%20%283D-GS%29%20offers%20unprecedented%20rendering%20speed%20and%20quality%2C%20current%20animation%20techniques%20often%20rely%20on%20a%20%22one-size-fits-all%22%20global%20tuning%20approach%2C%20where%20all%20Gaussian%20primitives%20are%20uniformly%20driven%20by%20a%20single%20expression%20code.%20This%20simplistic%20approach%20fails%20to%20unravel%20the%20distinct%20dynamics%20of%20different%20facial%20regions%2C%20such%20as%20deformable%20skin%20versus%20rigid%20teeth%2C%20leading%20to%20significant%20blurring%20and%20distortion%20artifacts.%20We%20introduce%20Conditionally-Adaptive%20Gaussian%20Avatars%20%28CAG-Avatar%29%2C%20a%20framework%20that%20resolves%20this%20key%20limitation.%20At%20its%20core%20is%20a%20Conditionally%20Adaptive%20Fusion%20Module%20built%20on%20cross-attention.%20This%20mechanism%20empowers%20each%203D%20Gaussian%20to%20act%20as%20a%20query%2C%20adaptively%20extracting%20relevant%20driving%20signals%20from%20the%20global%20expression%20code%20based%20on%20its%20canonical%20position.%20This%20%22tailor-made%22%20conditioning%20strategy%20drastically%20enhances%20the%20modeling%20of%20fine-grained%2C%20localized%20dynamics.%20Our%20experiments%20confirm%20a%20significant%20improvement%20in%20reconstruction%20fidelity%2C%20particularly%20for%20challenging%20regions%20such%20as%20teeth%2C%20while%20preserving%20real-time%20rendering%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAG-Avatar%253A%2520Cross-Attention%2520Guided%2520Gaussian%2520Avatars%2520for%2520High-Fidelity%2520Head%2520Reconstruction%26entry.906535625%3DZhe%2520Chang%2520and%2520Haodong%2520Jin%2520and%2520Yan%2520Song%2520and%2520Hui%2520Yu%26entry.1292438233%3DCreating%2520high-fidelity%252C%2520real-time%2520drivable%25203D%2520head%2520avatars%2520is%2520a%2520core%2520challenge%2520in%2520digital%2520animation.%2520While%25203D%2520Gaussian%2520Splashing%2520%25283D-GS%2529%2520offers%2520unprecedented%2520rendering%2520speed%2520and%2520quality%252C%2520current%2520animation%2520techniques%2520often%2520rely%2520on%2520a%2520%2522one-size-fits-all%2522%2520global%2520tuning%2520approach%252C%2520where%2520all%2520Gaussian%2520primitives%2520are%2520uniformly%2520driven%2520by%2520a%2520single%2520expression%2520code.%2520This%2520simplistic%2520approach%2520fails%2520to%2520unravel%2520the%2520distinct%2520dynamics%2520of%2520different%2520facial%2520regions%252C%2520such%2520as%2520deformable%2520skin%2520versus%2520rigid%2520teeth%252C%2520leading%2520to%2520significant%2520blurring%2520and%2520distortion%2520artifacts.%2520We%2520introduce%2520Conditionally-Adaptive%2520Gaussian%2520Avatars%2520%2528CAG-Avatar%2529%252C%2520a%2520framework%2520that%2520resolves%2520this%2520key%2520limitation.%2520At%2520its%2520core%2520is%2520a%2520Conditionally%2520Adaptive%2520Fusion%2520Module%2520built%2520on%2520cross-attention.%2520This%2520mechanism%2520empowers%2520each%25203D%2520Gaussian%2520to%2520act%2520as%2520a%2520query%252C%2520adaptively%2520extracting%2520relevant%2520driving%2520signals%2520from%2520the%2520global%2520expression%2520code%2520based%2520on%2520its%2520canonical%2520position.%2520This%2520%2522tailor-made%2522%2520conditioning%2520strategy%2520drastically%2520enhances%2520the%2520modeling%2520of%2520fine-grained%252C%2520localized%2520dynamics.%2520Our%2520experiments%2520confirm%2520a%2520significant%2520improvement%2520in%2520reconstruction%2520fidelity%252C%2520particularly%2520for%2520challenging%2520regions%2520such%2520as%2520teeth%252C%2520while%2520preserving%2520real-time%2520rendering%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAG-Avatar%3A%20Cross-Attention%20Guided%20Gaussian%20Avatars%20for%20High-Fidelity%20Head%20Reconstruction&entry.906535625=Zhe%20Chang%20and%20Haodong%20Jin%20and%20Yan%20Song%20and%20Hui%20Yu&entry.1292438233=Creating%20high-fidelity%2C%20real-time%20drivable%203D%20head%20avatars%20is%20a%20core%20challenge%20in%20digital%20animation.%20While%203D%20Gaussian%20Splashing%20%283D-GS%29%20offers%20unprecedented%20rendering%20speed%20and%20quality%2C%20current%20animation%20techniques%20often%20rely%20on%20a%20%22one-size-fits-all%22%20global%20tuning%20approach%2C%20where%20all%20Gaussian%20primitives%20are%20uniformly%20driven%20by%20a%20single%20expression%20code.%20This%20simplistic%20approach%20fails%20to%20unravel%20the%20distinct%20dynamics%20of%20different%20facial%20regions%2C%20such%20as%20deformable%20skin%20versus%20rigid%20teeth%2C%20leading%20to%20significant%20blurring%20and%20distortion%20artifacts.%20We%20introduce%20Conditionally-Adaptive%20Gaussian%20Avatars%20%28CAG-Avatar%29%2C%20a%20framework%20that%20resolves%20this%20key%20limitation.%20At%20its%20core%20is%20a%20Conditionally%20Adaptive%20Fusion%20Module%20built%20on%20cross-attention.%20This%20mechanism%20empowers%20each%203D%20Gaussian%20to%20act%20as%20a%20query%2C%20adaptively%20extracting%20relevant%20driving%20signals%20from%20the%20global%20expression%20code%20based%20on%20its%20canonical%20position.%20This%20%22tailor-made%22%20conditioning%20strategy%20drastically%20enhances%20the%20modeling%20of%20fine-grained%2C%20localized%20dynamics.%20Our%20experiments%20confirm%20a%20significant%20improvement%20in%20reconstruction%20fidelity%2C%20particularly%20for%20challenging%20regions%20such%20as%20teeth%2C%20while%20preserving%20real-time%20rendering%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2601.14844v1&entry.124074799=Read"},
{"title": "RealX3D: A Physically-Degraded 3D Benchmark for Multi-view Visual Restoration and Reconstruction", "author": "Shuhong Liu and Chenyu Bao and Ziteng Cui and Yun Liu and Xuangeng Chu and Lin Gu and Marcos V. Conde and Ryo Umagami and Tomohiro Hashimoto and Zijian Hu and Tianhan Xu and Yuan Gan and Yusuke Kurose and Tatsuya Harada", "abstract": "We introduce RealX3D, a real-capture benchmark for multi-view visual restoration and 3D reconstruction under diverse physical degradations. RealX3D groups corruptions into four families, including illumination, scattering, occlusion, and blurring, and captures each at multiple severity levels using a unified acquisition protocol that yields pixel-aligned LQ/GT views. Each scene includes high-resolution capture, RAW images, and dense laser scans, from which we derive world-scale meshes and metric depth. Benchmarking a broad range of optimization-based and feed-forward methods shows substantial degradation in reconstruction quality under physical corruptions, underscoring the fragility of current multi-view pipelines in real-world challenging environments.", "link": "http://arxiv.org/abs/2512.23437v2", "date": "2026-01-21", "relevancy": 3.1837, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6424}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6424}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RealX3D%3A%20A%20Physically-Degraded%203D%20Benchmark%20for%20Multi-view%20Visual%20Restoration%20and%20Reconstruction&body=Title%3A%20RealX3D%3A%20A%20Physically-Degraded%203D%20Benchmark%20for%20Multi-view%20Visual%20Restoration%20and%20Reconstruction%0AAuthor%3A%20Shuhong%20Liu%20and%20Chenyu%20Bao%20and%20Ziteng%20Cui%20and%20Yun%20Liu%20and%20Xuangeng%20Chu%20and%20Lin%20Gu%20and%20Marcos%20V.%20Conde%20and%20Ryo%20Umagami%20and%20Tomohiro%20Hashimoto%20and%20Zijian%20Hu%20and%20Tianhan%20Xu%20and%20Yuan%20Gan%20and%20Yusuke%20Kurose%20and%20Tatsuya%20Harada%0AAbstract%3A%20We%20introduce%20RealX3D%2C%20a%20real-capture%20benchmark%20for%20multi-view%20visual%20restoration%20and%203D%20reconstruction%20under%20diverse%20physical%20degradations.%20RealX3D%20groups%20corruptions%20into%20four%20families%2C%20including%20illumination%2C%20scattering%2C%20occlusion%2C%20and%20blurring%2C%20and%20captures%20each%20at%20multiple%20severity%20levels%20using%20a%20unified%20acquisition%20protocol%20that%20yields%20pixel-aligned%20LQ/GT%20views.%20Each%20scene%20includes%20high-resolution%20capture%2C%20RAW%20images%2C%20and%20dense%20laser%20scans%2C%20from%20which%20we%20derive%20world-scale%20meshes%20and%20metric%20depth.%20Benchmarking%20a%20broad%20range%20of%20optimization-based%20and%20feed-forward%20methods%20shows%20substantial%20degradation%20in%20reconstruction%20quality%20under%20physical%20corruptions%2C%20underscoring%20the%20fragility%20of%20current%20multi-view%20pipelines%20in%20real-world%20challenging%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23437v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealX3D%253A%2520A%2520Physically-Degraded%25203D%2520Benchmark%2520for%2520Multi-view%2520Visual%2520Restoration%2520and%2520Reconstruction%26entry.906535625%3DShuhong%2520Liu%2520and%2520Chenyu%2520Bao%2520and%2520Ziteng%2520Cui%2520and%2520Yun%2520Liu%2520and%2520Xuangeng%2520Chu%2520and%2520Lin%2520Gu%2520and%2520Marcos%2520V.%2520Conde%2520and%2520Ryo%2520Umagami%2520and%2520Tomohiro%2520Hashimoto%2520and%2520Zijian%2520Hu%2520and%2520Tianhan%2520Xu%2520and%2520Yuan%2520Gan%2520and%2520Yusuke%2520Kurose%2520and%2520Tatsuya%2520Harada%26entry.1292438233%3DWe%2520introduce%2520RealX3D%252C%2520a%2520real-capture%2520benchmark%2520for%2520multi-view%2520visual%2520restoration%2520and%25203D%2520reconstruction%2520under%2520diverse%2520physical%2520degradations.%2520RealX3D%2520groups%2520corruptions%2520into%2520four%2520families%252C%2520including%2520illumination%252C%2520scattering%252C%2520occlusion%252C%2520and%2520blurring%252C%2520and%2520captures%2520each%2520at%2520multiple%2520severity%2520levels%2520using%2520a%2520unified%2520acquisition%2520protocol%2520that%2520yields%2520pixel-aligned%2520LQ/GT%2520views.%2520Each%2520scene%2520includes%2520high-resolution%2520capture%252C%2520RAW%2520images%252C%2520and%2520dense%2520laser%2520scans%252C%2520from%2520which%2520we%2520derive%2520world-scale%2520meshes%2520and%2520metric%2520depth.%2520Benchmarking%2520a%2520broad%2520range%2520of%2520optimization-based%2520and%2520feed-forward%2520methods%2520shows%2520substantial%2520degradation%2520in%2520reconstruction%2520quality%2520under%2520physical%2520corruptions%252C%2520underscoring%2520the%2520fragility%2520of%2520current%2520multi-view%2520pipelines%2520in%2520real-world%2520challenging%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23437v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RealX3D%3A%20A%20Physically-Degraded%203D%20Benchmark%20for%20Multi-view%20Visual%20Restoration%20and%20Reconstruction&entry.906535625=Shuhong%20Liu%20and%20Chenyu%20Bao%20and%20Ziteng%20Cui%20and%20Yun%20Liu%20and%20Xuangeng%20Chu%20and%20Lin%20Gu%20and%20Marcos%20V.%20Conde%20and%20Ryo%20Umagami%20and%20Tomohiro%20Hashimoto%20and%20Zijian%20Hu%20and%20Tianhan%20Xu%20and%20Yuan%20Gan%20and%20Yusuke%20Kurose%20and%20Tatsuya%20Harada&entry.1292438233=We%20introduce%20RealX3D%2C%20a%20real-capture%20benchmark%20for%20multi-view%20visual%20restoration%20and%203D%20reconstruction%20under%20diverse%20physical%20degradations.%20RealX3D%20groups%20corruptions%20into%20four%20families%2C%20including%20illumination%2C%20scattering%2C%20occlusion%2C%20and%20blurring%2C%20and%20captures%20each%20at%20multiple%20severity%20levels%20using%20a%20unified%20acquisition%20protocol%20that%20yields%20pixel-aligned%20LQ/GT%20views.%20Each%20scene%20includes%20high-resolution%20capture%2C%20RAW%20images%2C%20and%20dense%20laser%20scans%2C%20from%20which%20we%20derive%20world-scale%20meshes%20and%20metric%20depth.%20Benchmarking%20a%20broad%20range%20of%20optimization-based%20and%20feed-forward%20methods%20shows%20substantial%20degradation%20in%20reconstruction%20quality%20under%20physical%20corruptions%2C%20underscoring%20the%20fragility%20of%20current%20multi-view%20pipelines%20in%20real-world%20challenging%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2512.23437v2&entry.124074799=Read"},
{"title": "GAT-NeRF: Geometry-Aware-Transformer Enhanced Neural Radiance Fields for High-Fidelity 4D Facial Avatars", "author": "Zhe Chang and Haodong Jin and Ying Sun and Yan Song and Hui Yu", "abstract": "High-fidelity 4D dynamic facial avatar reconstruction from monocular video is a critical yet challenging task, driven by increasing demands for immersive virtual human applications. While Neural Radiance Fields (NeRF) have advanced scene representation, their capacity to capture high-frequency facial details, such as dynamic wrinkles and subtle textures from information-constrained monocular streams, requires significant enhancement. To tackle this challenge, we propose a novel hybrid neural radiance field framework, called Geometry-Aware-Transformer Enhanced NeRF (GAT-NeRF) for high-fidelity and controllable 4D facial avatar reconstruction, which integrates the Transformer mechanism into the NeRF pipeline. GAT-NeRF synergistically combines a coordinate-aligned Multilayer Perceptron (MLP) with a lightweight Transformer module, termed as Geometry-Aware-Transformer (GAT) due to its processing of multi-modal inputs containing explicit geometric priors. The GAT module is enabled by fusing multi-modal input features, including 3D spatial coordinates, 3D Morphable Model (3DMM) expression parameters, and learnable latent codes to effectively learn and enhance feature representations pertinent to fine-grained geometry. The Transformer's effective feature learning capabilities are leveraged to significantly augment the modeling of complex local facial patterns like dynamic wrinkles and acne scars. Comprehensive experiments unequivocally demonstrate GAT-NeRF's state-of-the-art performance in visual fidelity and high-frequency detail recovery, forging new pathways for creating realistic dynamic digital humans for multimedia applications.", "link": "http://arxiv.org/abs/2601.14875v1", "date": "2026-01-21", "relevancy": 3.1685, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6448}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6448}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAT-NeRF%3A%20Geometry-Aware-Transformer%20Enhanced%20Neural%20Radiance%20Fields%20for%20High-Fidelity%204D%20Facial%20Avatars&body=Title%3A%20GAT-NeRF%3A%20Geometry-Aware-Transformer%20Enhanced%20Neural%20Radiance%20Fields%20for%20High-Fidelity%204D%20Facial%20Avatars%0AAuthor%3A%20Zhe%20Chang%20and%20Haodong%20Jin%20and%20Ying%20Sun%20and%20Yan%20Song%20and%20Hui%20Yu%0AAbstract%3A%20High-fidelity%204D%20dynamic%20facial%20avatar%20reconstruction%20from%20monocular%20video%20is%20a%20critical%20yet%20challenging%20task%2C%20driven%20by%20increasing%20demands%20for%20immersive%20virtual%20human%20applications.%20While%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20advanced%20scene%20representation%2C%20their%20capacity%20to%20capture%20high-frequency%20facial%20details%2C%20such%20as%20dynamic%20wrinkles%20and%20subtle%20textures%20from%20information-constrained%20monocular%20streams%2C%20requires%20significant%20enhancement.%20To%20tackle%20this%20challenge%2C%20we%20propose%20a%20novel%20hybrid%20neural%20radiance%20field%20framework%2C%20called%20Geometry-Aware-Transformer%20Enhanced%20NeRF%20%28GAT-NeRF%29%20for%20high-fidelity%20and%20controllable%204D%20facial%20avatar%20reconstruction%2C%20which%20integrates%20the%20Transformer%20mechanism%20into%20the%20NeRF%20pipeline.%20GAT-NeRF%20synergistically%20combines%20a%20coordinate-aligned%20Multilayer%20Perceptron%20%28MLP%29%20with%20a%20lightweight%20Transformer%20module%2C%20termed%20as%20Geometry-Aware-Transformer%20%28GAT%29%20due%20to%20its%20processing%20of%20multi-modal%20inputs%20containing%20explicit%20geometric%20priors.%20The%20GAT%20module%20is%20enabled%20by%20fusing%20multi-modal%20input%20features%2C%20including%203D%20spatial%20coordinates%2C%203D%20Morphable%20Model%20%283DMM%29%20expression%20parameters%2C%20and%20learnable%20latent%20codes%20to%20effectively%20learn%20and%20enhance%20feature%20representations%20pertinent%20to%20fine-grained%20geometry.%20The%20Transformer%27s%20effective%20feature%20learning%20capabilities%20are%20leveraged%20to%20significantly%20augment%20the%20modeling%20of%20complex%20local%20facial%20patterns%20like%20dynamic%20wrinkles%20and%20acne%20scars.%20Comprehensive%20experiments%20unequivocally%20demonstrate%20GAT-NeRF%27s%20state-of-the-art%20performance%20in%20visual%20fidelity%20and%20high-frequency%20detail%20recovery%2C%20forging%20new%20pathways%20for%20creating%20realistic%20dynamic%20digital%20humans%20for%20multimedia%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAT-NeRF%253A%2520Geometry-Aware-Transformer%2520Enhanced%2520Neural%2520Radiance%2520Fields%2520for%2520High-Fidelity%25204D%2520Facial%2520Avatars%26entry.906535625%3DZhe%2520Chang%2520and%2520Haodong%2520Jin%2520and%2520Ying%2520Sun%2520and%2520Yan%2520Song%2520and%2520Hui%2520Yu%26entry.1292438233%3DHigh-fidelity%25204D%2520dynamic%2520facial%2520avatar%2520reconstruction%2520from%2520monocular%2520video%2520is%2520a%2520critical%2520yet%2520challenging%2520task%252C%2520driven%2520by%2520increasing%2520demands%2520for%2520immersive%2520virtual%2520human%2520applications.%2520While%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520have%2520advanced%2520scene%2520representation%252C%2520their%2520capacity%2520to%2520capture%2520high-frequency%2520facial%2520details%252C%2520such%2520as%2520dynamic%2520wrinkles%2520and%2520subtle%2520textures%2520from%2520information-constrained%2520monocular%2520streams%252C%2520requires%2520significant%2520enhancement.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520hybrid%2520neural%2520radiance%2520field%2520framework%252C%2520called%2520Geometry-Aware-Transformer%2520Enhanced%2520NeRF%2520%2528GAT-NeRF%2529%2520for%2520high-fidelity%2520and%2520controllable%25204D%2520facial%2520avatar%2520reconstruction%252C%2520which%2520integrates%2520the%2520Transformer%2520mechanism%2520into%2520the%2520NeRF%2520pipeline.%2520GAT-NeRF%2520synergistically%2520combines%2520a%2520coordinate-aligned%2520Multilayer%2520Perceptron%2520%2528MLP%2529%2520with%2520a%2520lightweight%2520Transformer%2520module%252C%2520termed%2520as%2520Geometry-Aware-Transformer%2520%2528GAT%2529%2520due%2520to%2520its%2520processing%2520of%2520multi-modal%2520inputs%2520containing%2520explicit%2520geometric%2520priors.%2520The%2520GAT%2520module%2520is%2520enabled%2520by%2520fusing%2520multi-modal%2520input%2520features%252C%2520including%25203D%2520spatial%2520coordinates%252C%25203D%2520Morphable%2520Model%2520%25283DMM%2529%2520expression%2520parameters%252C%2520and%2520learnable%2520latent%2520codes%2520to%2520effectively%2520learn%2520and%2520enhance%2520feature%2520representations%2520pertinent%2520to%2520fine-grained%2520geometry.%2520The%2520Transformer%2527s%2520effective%2520feature%2520learning%2520capabilities%2520are%2520leveraged%2520to%2520significantly%2520augment%2520the%2520modeling%2520of%2520complex%2520local%2520facial%2520patterns%2520like%2520dynamic%2520wrinkles%2520and%2520acne%2520scars.%2520Comprehensive%2520experiments%2520unequivocally%2520demonstrate%2520GAT-NeRF%2527s%2520state-of-the-art%2520performance%2520in%2520visual%2520fidelity%2520and%2520high-frequency%2520detail%2520recovery%252C%2520forging%2520new%2520pathways%2520for%2520creating%2520realistic%2520dynamic%2520digital%2520humans%2520for%2520multimedia%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAT-NeRF%3A%20Geometry-Aware-Transformer%20Enhanced%20Neural%20Radiance%20Fields%20for%20High-Fidelity%204D%20Facial%20Avatars&entry.906535625=Zhe%20Chang%20and%20Haodong%20Jin%20and%20Ying%20Sun%20and%20Yan%20Song%20and%20Hui%20Yu&entry.1292438233=High-fidelity%204D%20dynamic%20facial%20avatar%20reconstruction%20from%20monocular%20video%20is%20a%20critical%20yet%20challenging%20task%2C%20driven%20by%20increasing%20demands%20for%20immersive%20virtual%20human%20applications.%20While%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20advanced%20scene%20representation%2C%20their%20capacity%20to%20capture%20high-frequency%20facial%20details%2C%20such%20as%20dynamic%20wrinkles%20and%20subtle%20textures%20from%20information-constrained%20monocular%20streams%2C%20requires%20significant%20enhancement.%20To%20tackle%20this%20challenge%2C%20we%20propose%20a%20novel%20hybrid%20neural%20radiance%20field%20framework%2C%20called%20Geometry-Aware-Transformer%20Enhanced%20NeRF%20%28GAT-NeRF%29%20for%20high-fidelity%20and%20controllable%204D%20facial%20avatar%20reconstruction%2C%20which%20integrates%20the%20Transformer%20mechanism%20into%20the%20NeRF%20pipeline.%20GAT-NeRF%20synergistically%20combines%20a%20coordinate-aligned%20Multilayer%20Perceptron%20%28MLP%29%20with%20a%20lightweight%20Transformer%20module%2C%20termed%20as%20Geometry-Aware-Transformer%20%28GAT%29%20due%20to%20its%20processing%20of%20multi-modal%20inputs%20containing%20explicit%20geometric%20priors.%20The%20GAT%20module%20is%20enabled%20by%20fusing%20multi-modal%20input%20features%2C%20including%203D%20spatial%20coordinates%2C%203D%20Morphable%20Model%20%283DMM%29%20expression%20parameters%2C%20and%20learnable%20latent%20codes%20to%20effectively%20learn%20and%20enhance%20feature%20representations%20pertinent%20to%20fine-grained%20geometry.%20The%20Transformer%27s%20effective%20feature%20learning%20capabilities%20are%20leveraged%20to%20significantly%20augment%20the%20modeling%20of%20complex%20local%20facial%20patterns%20like%20dynamic%20wrinkles%20and%20acne%20scars.%20Comprehensive%20experiments%20unequivocally%20demonstrate%20GAT-NeRF%27s%20state-of-the-art%20performance%20in%20visual%20fidelity%20and%20high-frequency%20detail%20recovery%2C%20forging%20new%20pathways%20for%20creating%20realistic%20dynamic%20digital%20humans%20for%20multimedia%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2601.14875v1&entry.124074799=Read"},
{"title": "Rethinking Video Generation Model for the Embodied World", "author": "Yufan Deng and Zilin Pan and Hongyu Zhang and Xiaojie Li and Ruoqing Hu and Yufei Ding and Yiming Zou and Yan Zeng and Daquan Zhou", "abstract": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.", "link": "http://arxiv.org/abs/2601.15282v1", "date": "2026-01-21", "relevancy": 3.0706, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6456}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6118}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Video%20Generation%20Model%20for%20the%20Embodied%20World&body=Title%3A%20Rethinking%20Video%20Generation%20Model%20for%20the%20Embodied%20World%0AAuthor%3A%20Yufan%20Deng%20and%20Zilin%20Pan%20and%20Hongyu%20Zhang%20and%20Xiaojie%20Li%20and%20Ruoqing%20Hu%20and%20Yufei%20Ding%20and%20Yiming%20Zou%20and%20Yan%20Zeng%20and%20Daquan%20Zhou%0AAbstract%3A%20Video%20generation%20models%20have%20significantly%20advanced%20embodied%20intelligence%2C%20unlocking%20new%20possibilities%20for%20generating%20diverse%20robot%20data%20that%20capture%20perception%2C%20reasoning%2C%20and%20action%20in%20the%20physical%20world.%20However%2C%20synthesizing%20high-quality%20videos%20that%20accurately%20reflect%20real-world%20robotic%20interactions%20remains%20challenging%2C%20and%20the%20lack%20of%20a%20standardized%20benchmark%20limits%20fair%20comparisons%20and%20progress.%20To%20address%20this%20gap%2C%20we%20introduce%20a%20comprehensive%20robotics%20benchmark%2C%20RBench%2C%20designed%20to%20evaluate%20robot-oriented%20video%20generation%20across%20five%20task%20domains%20and%20four%20distinct%20embodiments.%20It%20assesses%20both%20task-level%20correctness%20and%20visual%20fidelity%20through%20reproducible%20sub-metrics%2C%20including%20structural%20consistency%2C%20physical%20plausibility%2C%20and%20action%20completeness.%20Evaluation%20of%2025%20representative%20models%20highlights%20significant%20deficiencies%20in%20generating%20physically%20realistic%20robot%20behaviors.%20Furthermore%2C%20the%20benchmark%20achieves%20a%20Spearman%20correlation%20coefficient%20of%200.96%20with%20human%20evaluations%2C%20validating%20its%20effectiveness.%20While%20RBench%20provides%20the%20necessary%20lens%20to%20identify%20these%20deficiencies%2C%20achieving%20physical%20realism%20requires%20moving%20beyond%20evaluation%20to%20address%20the%20critical%20shortage%20of%20high-quality%20training%20data.%20Driven%20by%20these%20insights%2C%20we%20introduce%20a%20refined%20four-stage%20data%20pipeline%2C%20resulting%20in%20RoVid-X%2C%20the%20largest%20open-source%20robotic%20dataset%20for%20video%20generation%20with%204%20million%20annotated%20video%20clips%2C%20covering%20thousands%20of%20tasks%20and%20enriched%20with%20comprehensive%20physical%20property%20annotations.%20Collectively%2C%20this%20synergistic%20ecosystem%20of%20evaluation%20and%20data%20establishes%20a%20robust%20foundation%20for%20rigorous%20assessment%20and%20scalable%20training%20of%20video%20models%2C%20accelerating%20the%20evolution%20of%20embodied%20AI%20toward%20general%20intelligence.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15282v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Video%2520Generation%2520Model%2520for%2520the%2520Embodied%2520World%26entry.906535625%3DYufan%2520Deng%2520and%2520Zilin%2520Pan%2520and%2520Hongyu%2520Zhang%2520and%2520Xiaojie%2520Li%2520and%2520Ruoqing%2520Hu%2520and%2520Yufei%2520Ding%2520and%2520Yiming%2520Zou%2520and%2520Yan%2520Zeng%2520and%2520Daquan%2520Zhou%26entry.1292438233%3DVideo%2520generation%2520models%2520have%2520significantly%2520advanced%2520embodied%2520intelligence%252C%2520unlocking%2520new%2520possibilities%2520for%2520generating%2520diverse%2520robot%2520data%2520that%2520capture%2520perception%252C%2520reasoning%252C%2520and%2520action%2520in%2520the%2520physical%2520world.%2520However%252C%2520synthesizing%2520high-quality%2520videos%2520that%2520accurately%2520reflect%2520real-world%2520robotic%2520interactions%2520remains%2520challenging%252C%2520and%2520the%2520lack%2520of%2520a%2520standardized%2520benchmark%2520limits%2520fair%2520comparisons%2520and%2520progress.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520a%2520comprehensive%2520robotics%2520benchmark%252C%2520RBench%252C%2520designed%2520to%2520evaluate%2520robot-oriented%2520video%2520generation%2520across%2520five%2520task%2520domains%2520and%2520four%2520distinct%2520embodiments.%2520It%2520assesses%2520both%2520task-level%2520correctness%2520and%2520visual%2520fidelity%2520through%2520reproducible%2520sub-metrics%252C%2520including%2520structural%2520consistency%252C%2520physical%2520plausibility%252C%2520and%2520action%2520completeness.%2520Evaluation%2520of%252025%2520representative%2520models%2520highlights%2520significant%2520deficiencies%2520in%2520generating%2520physically%2520realistic%2520robot%2520behaviors.%2520Furthermore%252C%2520the%2520benchmark%2520achieves%2520a%2520Spearman%2520correlation%2520coefficient%2520of%25200.96%2520with%2520human%2520evaluations%252C%2520validating%2520its%2520effectiveness.%2520While%2520RBench%2520provides%2520the%2520necessary%2520lens%2520to%2520identify%2520these%2520deficiencies%252C%2520achieving%2520physical%2520realism%2520requires%2520moving%2520beyond%2520evaluation%2520to%2520address%2520the%2520critical%2520shortage%2520of%2520high-quality%2520training%2520data.%2520Driven%2520by%2520these%2520insights%252C%2520we%2520introduce%2520a%2520refined%2520four-stage%2520data%2520pipeline%252C%2520resulting%2520in%2520RoVid-X%252C%2520the%2520largest%2520open-source%2520robotic%2520dataset%2520for%2520video%2520generation%2520with%25204%2520million%2520annotated%2520video%2520clips%252C%2520covering%2520thousands%2520of%2520tasks%2520and%2520enriched%2520with%2520comprehensive%2520physical%2520property%2520annotations.%2520Collectively%252C%2520this%2520synergistic%2520ecosystem%2520of%2520evaluation%2520and%2520data%2520establishes%2520a%2520robust%2520foundation%2520for%2520rigorous%2520assessment%2520and%2520scalable%2520training%2520of%2520video%2520models%252C%2520accelerating%2520the%2520evolution%2520of%2520embodied%2520AI%2520toward%2520general%2520intelligence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15282v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Video%20Generation%20Model%20for%20the%20Embodied%20World&entry.906535625=Yufan%20Deng%20and%20Zilin%20Pan%20and%20Hongyu%20Zhang%20and%20Xiaojie%20Li%20and%20Ruoqing%20Hu%20and%20Yufei%20Ding%20and%20Yiming%20Zou%20and%20Yan%20Zeng%20and%20Daquan%20Zhou&entry.1292438233=Video%20generation%20models%20have%20significantly%20advanced%20embodied%20intelligence%2C%20unlocking%20new%20possibilities%20for%20generating%20diverse%20robot%20data%20that%20capture%20perception%2C%20reasoning%2C%20and%20action%20in%20the%20physical%20world.%20However%2C%20synthesizing%20high-quality%20videos%20that%20accurately%20reflect%20real-world%20robotic%20interactions%20remains%20challenging%2C%20and%20the%20lack%20of%20a%20standardized%20benchmark%20limits%20fair%20comparisons%20and%20progress.%20To%20address%20this%20gap%2C%20we%20introduce%20a%20comprehensive%20robotics%20benchmark%2C%20RBench%2C%20designed%20to%20evaluate%20robot-oriented%20video%20generation%20across%20five%20task%20domains%20and%20four%20distinct%20embodiments.%20It%20assesses%20both%20task-level%20correctness%20and%20visual%20fidelity%20through%20reproducible%20sub-metrics%2C%20including%20structural%20consistency%2C%20physical%20plausibility%2C%20and%20action%20completeness.%20Evaluation%20of%2025%20representative%20models%20highlights%20significant%20deficiencies%20in%20generating%20physically%20realistic%20robot%20behaviors.%20Furthermore%2C%20the%20benchmark%20achieves%20a%20Spearman%20correlation%20coefficient%20of%200.96%20with%20human%20evaluations%2C%20validating%20its%20effectiveness.%20While%20RBench%20provides%20the%20necessary%20lens%20to%20identify%20these%20deficiencies%2C%20achieving%20physical%20realism%20requires%20moving%20beyond%20evaluation%20to%20address%20the%20critical%20shortage%20of%20high-quality%20training%20data.%20Driven%20by%20these%20insights%2C%20we%20introduce%20a%20refined%20four-stage%20data%20pipeline%2C%20resulting%20in%20RoVid-X%2C%20the%20largest%20open-source%20robotic%20dataset%20for%20video%20generation%20with%204%20million%20annotated%20video%20clips%2C%20covering%20thousands%20of%20tasks%20and%20enriched%20with%20comprehensive%20physical%20property%20annotations.%20Collectively%2C%20this%20synergistic%20ecosystem%20of%20evaluation%20and%20data%20establishes%20a%20robust%20foundation%20for%20rigorous%20assessment%20and%20scalable%20training%20of%20video%20models%2C%20accelerating%20the%20evolution%20of%20embodied%20AI%20toward%20general%20intelligence.&entry.1838667208=http%3A//arxiv.org/abs/2601.15282v1&entry.124074799=Read"},
{"title": "Three-dimensional visualization of X-ray micro-CT with large-scale datasets: Efficiency and accuracy for real-time interaction", "author": "Yipeng Yin and Rao Yao and Qingying Li and Dazhong Wang and Hong Zhou and Zhijun Fang and Jianing Chen and Longjie Qian and Mingyue Wu", "abstract": "As Micro-CT technology continues to refine its characterization of material microstructures, industrial CT ultra-precision inspection is generating increasingly large datasets, necessitating solutions to the trade-off between accuracy and efficiency in the 3D characterization of defects during ultra-precise detection. This article provides a unique perspective on recent advances in accurate and efficient 3D visualization using Micro-CT, tracing its evolution from medical imaging to industrial non-destructive testing (NDT). Among the numerous CT reconstruction and volume rendering methods, this article selectively reviews and analyzes approaches that balance accuracy and efficiency, offering a comprehensive analysis to help researchers quickly grasp highly efficient and accurate 3D reconstruction methods for microscopic features. By comparing the principles of computed tomography with advancements in microstructural technology, this article examines the evolution of CT reconstruction algorithms from analytical methods to deep learning techniques, as well as improvements in volume rendering algorithms, acceleration, and data reduction. Additionally, it explores advanced lighting models for high-accuracy, photorealistic, and efficient volume rendering. Furthermore, this article envisions potential directions in CT reconstruction and volume rendering. It aims to guide future research in quickly selecting efficient and precise methods and developing new ideas and approaches for real-time online monitoring of internal material defects through virtual-physical interaction, for applying digital twin model to structural health monitoring (SHM).", "link": "http://arxiv.org/abs/2601.15098v1", "date": "2026-01-21", "relevancy": 3.0347, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6492}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6492}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Three-dimensional%20visualization%20of%20X-ray%20micro-CT%20with%20large-scale%20datasets%3A%20Efficiency%20and%20accuracy%20for%20real-time%20interaction&body=Title%3A%20Three-dimensional%20visualization%20of%20X-ray%20micro-CT%20with%20large-scale%20datasets%3A%20Efficiency%20and%20accuracy%20for%20real-time%20interaction%0AAuthor%3A%20Yipeng%20Yin%20and%20Rao%20Yao%20and%20Qingying%20Li%20and%20Dazhong%20Wang%20and%20Hong%20Zhou%20and%20Zhijun%20Fang%20and%20Jianing%20Chen%20and%20Longjie%20Qian%20and%20Mingyue%20Wu%0AAbstract%3A%20As%20Micro-CT%20technology%20continues%20to%20refine%20its%20characterization%20of%20material%20microstructures%2C%20industrial%20CT%20ultra-precision%20inspection%20is%20generating%20increasingly%20large%20datasets%2C%20necessitating%20solutions%20to%20the%20trade-off%20between%20accuracy%20and%20efficiency%20in%20the%203D%20characterization%20of%20defects%20during%20ultra-precise%20detection.%20This%20article%20provides%20a%20unique%20perspective%20on%20recent%20advances%20in%20accurate%20and%20efficient%203D%20visualization%20using%20Micro-CT%2C%20tracing%20its%20evolution%20from%20medical%20imaging%20to%20industrial%20non-destructive%20testing%20%28NDT%29.%20Among%20the%20numerous%20CT%20reconstruction%20and%20volume%20rendering%20methods%2C%20this%20article%20selectively%20reviews%20and%20analyzes%20approaches%20that%20balance%20accuracy%20and%20efficiency%2C%20offering%20a%20comprehensive%20analysis%20to%20help%20researchers%20quickly%20grasp%20highly%20efficient%20and%20accurate%203D%20reconstruction%20methods%20for%20microscopic%20features.%20By%20comparing%20the%20principles%20of%20computed%20tomography%20with%20advancements%20in%20microstructural%20technology%2C%20this%20article%20examines%20the%20evolution%20of%20CT%20reconstruction%20algorithms%20from%20analytical%20methods%20to%20deep%20learning%20techniques%2C%20as%20well%20as%20improvements%20in%20volume%20rendering%20algorithms%2C%20acceleration%2C%20and%20data%20reduction.%20Additionally%2C%20it%20explores%20advanced%20lighting%20models%20for%20high-accuracy%2C%20photorealistic%2C%20and%20efficient%20volume%20rendering.%20Furthermore%2C%20this%20article%20envisions%20potential%20directions%20in%20CT%20reconstruction%20and%20volume%20rendering.%20It%20aims%20to%20guide%20future%20research%20in%20quickly%20selecting%20efficient%20and%20precise%20methods%20and%20developing%20new%20ideas%20and%20approaches%20for%20real-time%20online%20monitoring%20of%20internal%20material%20defects%20through%20virtual-physical%20interaction%2C%20for%20applying%20digital%20twin%20model%20to%20structural%20health%20monitoring%20%28SHM%29.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15098v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThree-dimensional%2520visualization%2520of%2520X-ray%2520micro-CT%2520with%2520large-scale%2520datasets%253A%2520Efficiency%2520and%2520accuracy%2520for%2520real-time%2520interaction%26entry.906535625%3DYipeng%2520Yin%2520and%2520Rao%2520Yao%2520and%2520Qingying%2520Li%2520and%2520Dazhong%2520Wang%2520and%2520Hong%2520Zhou%2520and%2520Zhijun%2520Fang%2520and%2520Jianing%2520Chen%2520and%2520Longjie%2520Qian%2520and%2520Mingyue%2520Wu%26entry.1292438233%3DAs%2520Micro-CT%2520technology%2520continues%2520to%2520refine%2520its%2520characterization%2520of%2520material%2520microstructures%252C%2520industrial%2520CT%2520ultra-precision%2520inspection%2520is%2520generating%2520increasingly%2520large%2520datasets%252C%2520necessitating%2520solutions%2520to%2520the%2520trade-off%2520between%2520accuracy%2520and%2520efficiency%2520in%2520the%25203D%2520characterization%2520of%2520defects%2520during%2520ultra-precise%2520detection.%2520This%2520article%2520provides%2520a%2520unique%2520perspective%2520on%2520recent%2520advances%2520in%2520accurate%2520and%2520efficient%25203D%2520visualization%2520using%2520Micro-CT%252C%2520tracing%2520its%2520evolution%2520from%2520medical%2520imaging%2520to%2520industrial%2520non-destructive%2520testing%2520%2528NDT%2529.%2520Among%2520the%2520numerous%2520CT%2520reconstruction%2520and%2520volume%2520rendering%2520methods%252C%2520this%2520article%2520selectively%2520reviews%2520and%2520analyzes%2520approaches%2520that%2520balance%2520accuracy%2520and%2520efficiency%252C%2520offering%2520a%2520comprehensive%2520analysis%2520to%2520help%2520researchers%2520quickly%2520grasp%2520highly%2520efficient%2520and%2520accurate%25203D%2520reconstruction%2520methods%2520for%2520microscopic%2520features.%2520By%2520comparing%2520the%2520principles%2520of%2520computed%2520tomography%2520with%2520advancements%2520in%2520microstructural%2520technology%252C%2520this%2520article%2520examines%2520the%2520evolution%2520of%2520CT%2520reconstruction%2520algorithms%2520from%2520analytical%2520methods%2520to%2520deep%2520learning%2520techniques%252C%2520as%2520well%2520as%2520improvements%2520in%2520volume%2520rendering%2520algorithms%252C%2520acceleration%252C%2520and%2520data%2520reduction.%2520Additionally%252C%2520it%2520explores%2520advanced%2520lighting%2520models%2520for%2520high-accuracy%252C%2520photorealistic%252C%2520and%2520efficient%2520volume%2520rendering.%2520Furthermore%252C%2520this%2520article%2520envisions%2520potential%2520directions%2520in%2520CT%2520reconstruction%2520and%2520volume%2520rendering.%2520It%2520aims%2520to%2520guide%2520future%2520research%2520in%2520quickly%2520selecting%2520efficient%2520and%2520precise%2520methods%2520and%2520developing%2520new%2520ideas%2520and%2520approaches%2520for%2520real-time%2520online%2520monitoring%2520of%2520internal%2520material%2520defects%2520through%2520virtual-physical%2520interaction%252C%2520for%2520applying%2520digital%2520twin%2520model%2520to%2520structural%2520health%2520monitoring%2520%2528SHM%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15098v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Three-dimensional%20visualization%20of%20X-ray%20micro-CT%20with%20large-scale%20datasets%3A%20Efficiency%20and%20accuracy%20for%20real-time%20interaction&entry.906535625=Yipeng%20Yin%20and%20Rao%20Yao%20and%20Qingying%20Li%20and%20Dazhong%20Wang%20and%20Hong%20Zhou%20and%20Zhijun%20Fang%20and%20Jianing%20Chen%20and%20Longjie%20Qian%20and%20Mingyue%20Wu&entry.1292438233=As%20Micro-CT%20technology%20continues%20to%20refine%20its%20characterization%20of%20material%20microstructures%2C%20industrial%20CT%20ultra-precision%20inspection%20is%20generating%20increasingly%20large%20datasets%2C%20necessitating%20solutions%20to%20the%20trade-off%20between%20accuracy%20and%20efficiency%20in%20the%203D%20characterization%20of%20defects%20during%20ultra-precise%20detection.%20This%20article%20provides%20a%20unique%20perspective%20on%20recent%20advances%20in%20accurate%20and%20efficient%203D%20visualization%20using%20Micro-CT%2C%20tracing%20its%20evolution%20from%20medical%20imaging%20to%20industrial%20non-destructive%20testing%20%28NDT%29.%20Among%20the%20numerous%20CT%20reconstruction%20and%20volume%20rendering%20methods%2C%20this%20article%20selectively%20reviews%20and%20analyzes%20approaches%20that%20balance%20accuracy%20and%20efficiency%2C%20offering%20a%20comprehensive%20analysis%20to%20help%20researchers%20quickly%20grasp%20highly%20efficient%20and%20accurate%203D%20reconstruction%20methods%20for%20microscopic%20features.%20By%20comparing%20the%20principles%20of%20computed%20tomography%20with%20advancements%20in%20microstructural%20technology%2C%20this%20article%20examines%20the%20evolution%20of%20CT%20reconstruction%20algorithms%20from%20analytical%20methods%20to%20deep%20learning%20techniques%2C%20as%20well%20as%20improvements%20in%20volume%20rendering%20algorithms%2C%20acceleration%2C%20and%20data%20reduction.%20Additionally%2C%20it%20explores%20advanced%20lighting%20models%20for%20high-accuracy%2C%20photorealistic%2C%20and%20efficient%20volume%20rendering.%20Furthermore%2C%20this%20article%20envisions%20potential%20directions%20in%20CT%20reconstruction%20and%20volume%20rendering.%20It%20aims%20to%20guide%20future%20research%20in%20quickly%20selecting%20efficient%20and%20precise%20methods%20and%20developing%20new%20ideas%20and%20approaches%20for%20real-time%20online%20monitoring%20of%20internal%20material%20defects%20through%20virtual-physical%20interaction%2C%20for%20applying%20digital%20twin%20model%20to%20structural%20health%20monitoring%20%28SHM%29.&entry.1838667208=http%3A//arxiv.org/abs/2601.15098v1&entry.124074799=Read"},
{"title": "Pb4U-GNet: Resolution-Adaptive Garment Simulation via Propagation-before-Update Graph Network", "author": "Aoran Liu and Kun Hu and Clinton Ansun Mo and Qiuxia Wu and Wenxiong Kang and Zhiyong Wang", "abstract": "Garment simulation is fundamental to various applications in computer vision and graphics, from virtual try-on to digital human modelling. However, conventional physics-based methods remain computationally expensive, hindering their application in time-sensitive scenarios. While graph neural networks (GNNs) offer promising acceleration, existing approaches exhibit poor cross-resolution generalisation, demonstrating significant performance degradation on higher-resolution meshes beyond the training distribution. This stems from two key factors: (1) existing GNNs employ fixed message-passing depth that fails to adapt information aggregation to mesh density variation, and (2) vertex-wise displacement magnitudes are inherently resolution-dependent in garment simulation. To address these issues, we introduce Propagation-before-Update Graph Network (Pb4U-GNet), a resolution-adaptive framework that decouples message propagation from feature updates. Pb4U-GNet incorporates two key mechanisms: (1) dynamic propagation depth control, adjusting message-passing iterations based on mesh resolution, and (2) geometry-aware update scaling, which scales predictions according to local mesh characteristics. Extensive experiments show that even trained solely on low-resolution meshes, Pb4U-GNet exhibits strong generalisability across diverse mesh resolutions, addressing a fundamental challenge in neural garment simulation.", "link": "http://arxiv.org/abs/2601.15110v1", "date": "2026-01-21", "relevancy": 2.9167, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5857}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5857}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pb4U-GNet%3A%20Resolution-Adaptive%20Garment%20Simulation%20via%20Propagation-before-Update%20Graph%20Network&body=Title%3A%20Pb4U-GNet%3A%20Resolution-Adaptive%20Garment%20Simulation%20via%20Propagation-before-Update%20Graph%20Network%0AAuthor%3A%20Aoran%20Liu%20and%20Kun%20Hu%20and%20Clinton%20Ansun%20Mo%20and%20Qiuxia%20Wu%20and%20Wenxiong%20Kang%20and%20Zhiyong%20Wang%0AAbstract%3A%20Garment%20simulation%20is%20fundamental%20to%20various%20applications%20in%20computer%20vision%20and%20graphics%2C%20from%20virtual%20try-on%20to%20digital%20human%20modelling.%20However%2C%20conventional%20physics-based%20methods%20remain%20computationally%20expensive%2C%20hindering%20their%20application%20in%20time-sensitive%20scenarios.%20While%20graph%20neural%20networks%20%28GNNs%29%20offer%20promising%20acceleration%2C%20existing%20approaches%20exhibit%20poor%20cross-resolution%20generalisation%2C%20demonstrating%20significant%20performance%20degradation%20on%20higher-resolution%20meshes%20beyond%20the%20training%20distribution.%20This%20stems%20from%20two%20key%20factors%3A%20%281%29%20existing%20GNNs%20employ%20fixed%20message-passing%20depth%20that%20fails%20to%20adapt%20information%20aggregation%20to%20mesh%20density%20variation%2C%20and%20%282%29%20vertex-wise%20displacement%20magnitudes%20are%20inherently%20resolution-dependent%20in%20garment%20simulation.%20To%20address%20these%20issues%2C%20we%20introduce%20Propagation-before-Update%20Graph%20Network%20%28Pb4U-GNet%29%2C%20a%20resolution-adaptive%20framework%20that%20decouples%20message%20propagation%20from%20feature%20updates.%20Pb4U-GNet%20incorporates%20two%20key%20mechanisms%3A%20%281%29%20dynamic%20propagation%20depth%20control%2C%20adjusting%20message-passing%20iterations%20based%20on%20mesh%20resolution%2C%20and%20%282%29%20geometry-aware%20update%20scaling%2C%20which%20scales%20predictions%20according%20to%20local%20mesh%20characteristics.%20Extensive%20experiments%20show%20that%20even%20trained%20solely%20on%20low-resolution%20meshes%2C%20Pb4U-GNet%20exhibits%20strong%20generalisability%20across%20diverse%20mesh%20resolutions%2C%20addressing%20a%20fundamental%20challenge%20in%20neural%20garment%20simulation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15110v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPb4U-GNet%253A%2520Resolution-Adaptive%2520Garment%2520Simulation%2520via%2520Propagation-before-Update%2520Graph%2520Network%26entry.906535625%3DAoran%2520Liu%2520and%2520Kun%2520Hu%2520and%2520Clinton%2520Ansun%2520Mo%2520and%2520Qiuxia%2520Wu%2520and%2520Wenxiong%2520Kang%2520and%2520Zhiyong%2520Wang%26entry.1292438233%3DGarment%2520simulation%2520is%2520fundamental%2520to%2520various%2520applications%2520in%2520computer%2520vision%2520and%2520graphics%252C%2520from%2520virtual%2520try-on%2520to%2520digital%2520human%2520modelling.%2520However%252C%2520conventional%2520physics-based%2520methods%2520remain%2520computationally%2520expensive%252C%2520hindering%2520their%2520application%2520in%2520time-sensitive%2520scenarios.%2520While%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520offer%2520promising%2520acceleration%252C%2520existing%2520approaches%2520exhibit%2520poor%2520cross-resolution%2520generalisation%252C%2520demonstrating%2520significant%2520performance%2520degradation%2520on%2520higher-resolution%2520meshes%2520beyond%2520the%2520training%2520distribution.%2520This%2520stems%2520from%2520two%2520key%2520factors%253A%2520%25281%2529%2520existing%2520GNNs%2520employ%2520fixed%2520message-passing%2520depth%2520that%2520fails%2520to%2520adapt%2520information%2520aggregation%2520to%2520mesh%2520density%2520variation%252C%2520and%2520%25282%2529%2520vertex-wise%2520displacement%2520magnitudes%2520are%2520inherently%2520resolution-dependent%2520in%2520garment%2520simulation.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520Propagation-before-Update%2520Graph%2520Network%2520%2528Pb4U-GNet%2529%252C%2520a%2520resolution-adaptive%2520framework%2520that%2520decouples%2520message%2520propagation%2520from%2520feature%2520updates.%2520Pb4U-GNet%2520incorporates%2520two%2520key%2520mechanisms%253A%2520%25281%2529%2520dynamic%2520propagation%2520depth%2520control%252C%2520adjusting%2520message-passing%2520iterations%2520based%2520on%2520mesh%2520resolution%252C%2520and%2520%25282%2529%2520geometry-aware%2520update%2520scaling%252C%2520which%2520scales%2520predictions%2520according%2520to%2520local%2520mesh%2520characteristics.%2520Extensive%2520experiments%2520show%2520that%2520even%2520trained%2520solely%2520on%2520low-resolution%2520meshes%252C%2520Pb4U-GNet%2520exhibits%2520strong%2520generalisability%2520across%2520diverse%2520mesh%2520resolutions%252C%2520addressing%2520a%2520fundamental%2520challenge%2520in%2520neural%2520garment%2520simulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15110v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pb4U-GNet%3A%20Resolution-Adaptive%20Garment%20Simulation%20via%20Propagation-before-Update%20Graph%20Network&entry.906535625=Aoran%20Liu%20and%20Kun%20Hu%20and%20Clinton%20Ansun%20Mo%20and%20Qiuxia%20Wu%20and%20Wenxiong%20Kang%20and%20Zhiyong%20Wang&entry.1292438233=Garment%20simulation%20is%20fundamental%20to%20various%20applications%20in%20computer%20vision%20and%20graphics%2C%20from%20virtual%20try-on%20to%20digital%20human%20modelling.%20However%2C%20conventional%20physics-based%20methods%20remain%20computationally%20expensive%2C%20hindering%20their%20application%20in%20time-sensitive%20scenarios.%20While%20graph%20neural%20networks%20%28GNNs%29%20offer%20promising%20acceleration%2C%20existing%20approaches%20exhibit%20poor%20cross-resolution%20generalisation%2C%20demonstrating%20significant%20performance%20degradation%20on%20higher-resolution%20meshes%20beyond%20the%20training%20distribution.%20This%20stems%20from%20two%20key%20factors%3A%20%281%29%20existing%20GNNs%20employ%20fixed%20message-passing%20depth%20that%20fails%20to%20adapt%20information%20aggregation%20to%20mesh%20density%20variation%2C%20and%20%282%29%20vertex-wise%20displacement%20magnitudes%20are%20inherently%20resolution-dependent%20in%20garment%20simulation.%20To%20address%20these%20issues%2C%20we%20introduce%20Propagation-before-Update%20Graph%20Network%20%28Pb4U-GNet%29%2C%20a%20resolution-adaptive%20framework%20that%20decouples%20message%20propagation%20from%20feature%20updates.%20Pb4U-GNet%20incorporates%20two%20key%20mechanisms%3A%20%281%29%20dynamic%20propagation%20depth%20control%2C%20adjusting%20message-passing%20iterations%20based%20on%20mesh%20resolution%2C%20and%20%282%29%20geometry-aware%20update%20scaling%2C%20which%20scales%20predictions%20according%20to%20local%20mesh%20characteristics.%20Extensive%20experiments%20show%20that%20even%20trained%20solely%20on%20low-resolution%20meshes%2C%20Pb4U-GNet%20exhibits%20strong%20generalisability%20across%20diverse%20mesh%20resolutions%2C%20addressing%20a%20fundamental%20challenge%20in%20neural%20garment%20simulation.&entry.1838667208=http%3A//arxiv.org/abs/2601.15110v1&entry.124074799=Read"},
{"title": "On-the-fly hand-eye calibration for the da Vinci surgical robot", "author": "Zejian Cui and Ferdinando Rodriguez y Baena", "abstract": "In Robot-Assisted Minimally Invasive Surgery (RMIS), accurate tool localization is crucial to ensure patient safety and successful task execution. However, this remains challenging for cable-driven robots, such as the da Vinci robot, because erroneous encoder readings lead to pose estimation errors. In this study, we propose a calibration framework to produce accurate tool localization results through computing the hand-eye transformation matrix on-the-fly. The framework consists of two interrelated algorithms: the feature association block and the hand-eye calibration block, which provide robust correspondences for key points detected on monocular images without pre-training, and offer the versatility to accommodate various surgical scenarios by adopting an array of filter approaches, respectively. To validate its efficacy, we test the framework extensively on publicly available video datasets that feature multiple surgical instruments conducting tasks in both in vitro and ex vivo scenarios, under varying illumination conditions and with different levels of key point measurement accuracy. The results show a significant reduction in tool localization errors under the proposed calibration framework, with accuracies comparable to other state-of-the-art methods while being more time-efficient.", "link": "http://arxiv.org/abs/2601.14871v1", "date": "2026-01-21", "relevancy": 2.7931, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5877}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5634}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On-the-fly%20hand-eye%20calibration%20for%20the%20da%20Vinci%20surgical%20robot&body=Title%3A%20On-the-fly%20hand-eye%20calibration%20for%20the%20da%20Vinci%20surgical%20robot%0AAuthor%3A%20Zejian%20Cui%20and%20Ferdinando%20Rodriguez%20y%20Baena%0AAbstract%3A%20In%20Robot-Assisted%20Minimally%20Invasive%20Surgery%20%28RMIS%29%2C%20accurate%20tool%20localization%20is%20crucial%20to%20ensure%20patient%20safety%20and%20successful%20task%20execution.%20However%2C%20this%20remains%20challenging%20for%20cable-driven%20robots%2C%20such%20as%20the%20da%20Vinci%20robot%2C%20because%20erroneous%20encoder%20readings%20lead%20to%20pose%20estimation%20errors.%20In%20this%20study%2C%20we%20propose%20a%20calibration%20framework%20to%20produce%20accurate%20tool%20localization%20results%20through%20computing%20the%20hand-eye%20transformation%20matrix%20on-the-fly.%20The%20framework%20consists%20of%20two%20interrelated%20algorithms%3A%20the%20feature%20association%20block%20and%20the%20hand-eye%20calibration%20block%2C%20which%20provide%20robust%20correspondences%20for%20key%20points%20detected%20on%20monocular%20images%20without%20pre-training%2C%20and%20offer%20the%20versatility%20to%20accommodate%20various%20surgical%20scenarios%20by%20adopting%20an%20array%20of%20filter%20approaches%2C%20respectively.%20To%20validate%20its%20efficacy%2C%20we%20test%20the%20framework%20extensively%20on%20publicly%20available%20video%20datasets%20that%20feature%20multiple%20surgical%20instruments%20conducting%20tasks%20in%20both%20in%20vitro%20and%20ex%20vivo%20scenarios%2C%20under%20varying%20illumination%20conditions%20and%20with%20different%20levels%20of%20key%20point%20measurement%20accuracy.%20The%20results%20show%20a%20significant%20reduction%20in%20tool%20localization%20errors%20under%20the%20proposed%20calibration%20framework%2C%20with%20accuracies%20comparable%20to%20other%20state-of-the-art%20methods%20while%20being%20more%20time-efficient.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn-the-fly%2520hand-eye%2520calibration%2520for%2520the%2520da%2520Vinci%2520surgical%2520robot%26entry.906535625%3DZejian%2520Cui%2520and%2520Ferdinando%2520Rodriguez%2520y%2520Baena%26entry.1292438233%3DIn%2520Robot-Assisted%2520Minimally%2520Invasive%2520Surgery%2520%2528RMIS%2529%252C%2520accurate%2520tool%2520localization%2520is%2520crucial%2520to%2520ensure%2520patient%2520safety%2520and%2520successful%2520task%2520execution.%2520However%252C%2520this%2520remains%2520challenging%2520for%2520cable-driven%2520robots%252C%2520such%2520as%2520the%2520da%2520Vinci%2520robot%252C%2520because%2520erroneous%2520encoder%2520readings%2520lead%2520to%2520pose%2520estimation%2520errors.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520calibration%2520framework%2520to%2520produce%2520accurate%2520tool%2520localization%2520results%2520through%2520computing%2520the%2520hand-eye%2520transformation%2520matrix%2520on-the-fly.%2520The%2520framework%2520consists%2520of%2520two%2520interrelated%2520algorithms%253A%2520the%2520feature%2520association%2520block%2520and%2520the%2520hand-eye%2520calibration%2520block%252C%2520which%2520provide%2520robust%2520correspondences%2520for%2520key%2520points%2520detected%2520on%2520monocular%2520images%2520without%2520pre-training%252C%2520and%2520offer%2520the%2520versatility%2520to%2520accommodate%2520various%2520surgical%2520scenarios%2520by%2520adopting%2520an%2520array%2520of%2520filter%2520approaches%252C%2520respectively.%2520To%2520validate%2520its%2520efficacy%252C%2520we%2520test%2520the%2520framework%2520extensively%2520on%2520publicly%2520available%2520video%2520datasets%2520that%2520feature%2520multiple%2520surgical%2520instruments%2520conducting%2520tasks%2520in%2520both%2520in%2520vitro%2520and%2520ex%2520vivo%2520scenarios%252C%2520under%2520varying%2520illumination%2520conditions%2520and%2520with%2520different%2520levels%2520of%2520key%2520point%2520measurement%2520accuracy.%2520The%2520results%2520show%2520a%2520significant%2520reduction%2520in%2520tool%2520localization%2520errors%2520under%2520the%2520proposed%2520calibration%2520framework%252C%2520with%2520accuracies%2520comparable%2520to%2520other%2520state-of-the-art%2520methods%2520while%2520being%2520more%2520time-efficient.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On-the-fly%20hand-eye%20calibration%20for%20the%20da%20Vinci%20surgical%20robot&entry.906535625=Zejian%20Cui%20and%20Ferdinando%20Rodriguez%20y%20Baena&entry.1292438233=In%20Robot-Assisted%20Minimally%20Invasive%20Surgery%20%28RMIS%29%2C%20accurate%20tool%20localization%20is%20crucial%20to%20ensure%20patient%20safety%20and%20successful%20task%20execution.%20However%2C%20this%20remains%20challenging%20for%20cable-driven%20robots%2C%20such%20as%20the%20da%20Vinci%20robot%2C%20because%20erroneous%20encoder%20readings%20lead%20to%20pose%20estimation%20errors.%20In%20this%20study%2C%20we%20propose%20a%20calibration%20framework%20to%20produce%20accurate%20tool%20localization%20results%20through%20computing%20the%20hand-eye%20transformation%20matrix%20on-the-fly.%20The%20framework%20consists%20of%20two%20interrelated%20algorithms%3A%20the%20feature%20association%20block%20and%20the%20hand-eye%20calibration%20block%2C%20which%20provide%20robust%20correspondences%20for%20key%20points%20detected%20on%20monocular%20images%20without%20pre-training%2C%20and%20offer%20the%20versatility%20to%20accommodate%20various%20surgical%20scenarios%20by%20adopting%20an%20array%20of%20filter%20approaches%2C%20respectively.%20To%20validate%20its%20efficacy%2C%20we%20test%20the%20framework%20extensively%20on%20publicly%20available%20video%20datasets%20that%20feature%20multiple%20surgical%20instruments%20conducting%20tasks%20in%20both%20in%20vitro%20and%20ex%20vivo%20scenarios%2C%20under%20varying%20illumination%20conditions%20and%20with%20different%20levels%20of%20key%20point%20measurement%20accuracy.%20The%20results%20show%20a%20significant%20reduction%20in%20tool%20localization%20errors%20under%20the%20proposed%20calibration%20framework%2C%20with%20accuracies%20comparable%20to%20other%20state-of-the-art%20methods%20while%20being%20more%20time-efficient.&entry.1838667208=http%3A//arxiv.org/abs/2601.14871v1&entry.124074799=Read"},
{"title": "Filtered 2D Contour-Based Reconstruction of 3D STL Model from CT-DICOM Images", "author": "K. Punnam Chandar and Y. Ravi Kumar", "abstract": "Reconstructing a 3D Stereo-lithography (STL) Model from 2D Contours of scanned structure in Digital Imaging and Communication in Medicine (DICOM) images is crucial to understand the geometry and deformity. Computed Tomography (CT) images are processed to enhance the contrast, reduce the noise followed by smoothing. The processed CT images are segmented using thresholding technique. 2D contour data points are extracted from segmented CT images and are used to construct 3D STL Models. The 2D contour data points may contain outliers as a result of segmentation of low resolution images and the geometry of the constructed 3D structure deviate from the actual. To cope with the imperfections in segmentation process, in this work we propose to use filtered 2D contour data points to reconstruct 3D STL Model. The filtered 2D contour points of each image are delaunay triangulated and joined layer-by-layer to reconstruct the 3D STL model. The 3D STL Model reconstruction is verified on i) 2D Data points of basic shapes and ii) Region of Interest (ROI) of human pelvic bone and are presented as case studies. The 3D STL model constructed from 2D contour data points of ROI of segmented pelvic bone with and without filtering are presented. The 3D STL model reconstructed from filtered 2D data points improved the geometry of model compared to the model reconstructed without filtering 2D data points.", "link": "http://arxiv.org/abs/2601.14997v1", "date": "2026-01-21", "relevancy": 2.7862, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5776}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5776}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Filtered%202D%20Contour-Based%20Reconstruction%20of%203D%20STL%20Model%20from%20CT-DICOM%20Images&body=Title%3A%20Filtered%202D%20Contour-Based%20Reconstruction%20of%203D%20STL%20Model%20from%20CT-DICOM%20Images%0AAuthor%3A%20K.%20Punnam%20Chandar%20and%20Y.%20Ravi%20Kumar%0AAbstract%3A%20Reconstructing%20a%203D%20Stereo-lithography%20%28STL%29%20Model%20from%202D%20Contours%20of%20scanned%20structure%20in%20Digital%20Imaging%20and%20Communication%20in%20Medicine%20%28DICOM%29%20images%20is%20crucial%20to%20understand%20the%20geometry%20and%20deformity.%20Computed%20Tomography%20%28CT%29%20images%20are%20processed%20to%20enhance%20the%20contrast%2C%20reduce%20the%20noise%20followed%20by%20smoothing.%20The%20processed%20CT%20images%20are%20segmented%20using%20thresholding%20technique.%202D%20contour%20data%20points%20are%20extracted%20from%20segmented%20CT%20images%20and%20are%20used%20to%20construct%203D%20STL%20Models.%20The%202D%20contour%20data%20points%20may%20contain%20outliers%20as%20a%20result%20of%20segmentation%20of%20low%20resolution%20images%20and%20the%20geometry%20of%20the%20constructed%203D%20structure%20deviate%20from%20the%20actual.%20To%20cope%20with%20the%20imperfections%20in%20segmentation%20process%2C%20in%20this%20work%20we%20propose%20to%20use%20filtered%202D%20contour%20data%20points%20to%20reconstruct%203D%20STL%20Model.%20The%20filtered%202D%20contour%20points%20of%20each%20image%20are%20delaunay%20triangulated%20and%20joined%20layer-by-layer%20to%20reconstruct%20the%203D%20STL%20model.%20The%203D%20STL%20Model%20reconstruction%20is%20verified%20on%20i%29%202D%20Data%20points%20of%20basic%20shapes%20and%20ii%29%20Region%20of%20Interest%20%28ROI%29%20of%20human%20pelvic%20bone%20and%20are%20presented%20as%20case%20studies.%20The%203D%20STL%20model%20constructed%20from%202D%20contour%20data%20points%20of%20ROI%20of%20segmented%20pelvic%20bone%20with%20and%20without%20filtering%20are%20presented.%20The%203D%20STL%20model%20reconstructed%20from%20filtered%202D%20data%20points%20improved%20the%20geometry%20of%20model%20compared%20to%20the%20model%20reconstructed%20without%20filtering%202D%20data%20points.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFiltered%25202D%2520Contour-Based%2520Reconstruction%2520of%25203D%2520STL%2520Model%2520from%2520CT-DICOM%2520Images%26entry.906535625%3DK.%2520Punnam%2520Chandar%2520and%2520Y.%2520Ravi%2520Kumar%26entry.1292438233%3DReconstructing%2520a%25203D%2520Stereo-lithography%2520%2528STL%2529%2520Model%2520from%25202D%2520Contours%2520of%2520scanned%2520structure%2520in%2520Digital%2520Imaging%2520and%2520Communication%2520in%2520Medicine%2520%2528DICOM%2529%2520images%2520is%2520crucial%2520to%2520understand%2520the%2520geometry%2520and%2520deformity.%2520Computed%2520Tomography%2520%2528CT%2529%2520images%2520are%2520processed%2520to%2520enhance%2520the%2520contrast%252C%2520reduce%2520the%2520noise%2520followed%2520by%2520smoothing.%2520The%2520processed%2520CT%2520images%2520are%2520segmented%2520using%2520thresholding%2520technique.%25202D%2520contour%2520data%2520points%2520are%2520extracted%2520from%2520segmented%2520CT%2520images%2520and%2520are%2520used%2520to%2520construct%25203D%2520STL%2520Models.%2520The%25202D%2520contour%2520data%2520points%2520may%2520contain%2520outliers%2520as%2520a%2520result%2520of%2520segmentation%2520of%2520low%2520resolution%2520images%2520and%2520the%2520geometry%2520of%2520the%2520constructed%25203D%2520structure%2520deviate%2520from%2520the%2520actual.%2520To%2520cope%2520with%2520the%2520imperfections%2520in%2520segmentation%2520process%252C%2520in%2520this%2520work%2520we%2520propose%2520to%2520use%2520filtered%25202D%2520contour%2520data%2520points%2520to%2520reconstruct%25203D%2520STL%2520Model.%2520The%2520filtered%25202D%2520contour%2520points%2520of%2520each%2520image%2520are%2520delaunay%2520triangulated%2520and%2520joined%2520layer-by-layer%2520to%2520reconstruct%2520the%25203D%2520STL%2520model.%2520The%25203D%2520STL%2520Model%2520reconstruction%2520is%2520verified%2520on%2520i%2529%25202D%2520Data%2520points%2520of%2520basic%2520shapes%2520and%2520ii%2529%2520Region%2520of%2520Interest%2520%2528ROI%2529%2520of%2520human%2520pelvic%2520bone%2520and%2520are%2520presented%2520as%2520case%2520studies.%2520The%25203D%2520STL%2520model%2520constructed%2520from%25202D%2520contour%2520data%2520points%2520of%2520ROI%2520of%2520segmented%2520pelvic%2520bone%2520with%2520and%2520without%2520filtering%2520are%2520presented.%2520The%25203D%2520STL%2520model%2520reconstructed%2520from%2520filtered%25202D%2520data%2520points%2520improved%2520the%2520geometry%2520of%2520model%2520compared%2520to%2520the%2520model%2520reconstructed%2520without%2520filtering%25202D%2520data%2520points.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Filtered%202D%20Contour-Based%20Reconstruction%20of%203D%20STL%20Model%20from%20CT-DICOM%20Images&entry.906535625=K.%20Punnam%20Chandar%20and%20Y.%20Ravi%20Kumar&entry.1292438233=Reconstructing%20a%203D%20Stereo-lithography%20%28STL%29%20Model%20from%202D%20Contours%20of%20scanned%20structure%20in%20Digital%20Imaging%20and%20Communication%20in%20Medicine%20%28DICOM%29%20images%20is%20crucial%20to%20understand%20the%20geometry%20and%20deformity.%20Computed%20Tomography%20%28CT%29%20images%20are%20processed%20to%20enhance%20the%20contrast%2C%20reduce%20the%20noise%20followed%20by%20smoothing.%20The%20processed%20CT%20images%20are%20segmented%20using%20thresholding%20technique.%202D%20contour%20data%20points%20are%20extracted%20from%20segmented%20CT%20images%20and%20are%20used%20to%20construct%203D%20STL%20Models.%20The%202D%20contour%20data%20points%20may%20contain%20outliers%20as%20a%20result%20of%20segmentation%20of%20low%20resolution%20images%20and%20the%20geometry%20of%20the%20constructed%203D%20structure%20deviate%20from%20the%20actual.%20To%20cope%20with%20the%20imperfections%20in%20segmentation%20process%2C%20in%20this%20work%20we%20propose%20to%20use%20filtered%202D%20contour%20data%20points%20to%20reconstruct%203D%20STL%20Model.%20The%20filtered%202D%20contour%20points%20of%20each%20image%20are%20delaunay%20triangulated%20and%20joined%20layer-by-layer%20to%20reconstruct%20the%203D%20STL%20model.%20The%203D%20STL%20Model%20reconstruction%20is%20verified%20on%20i%29%202D%20Data%20points%20of%20basic%20shapes%20and%20ii%29%20Region%20of%20Interest%20%28ROI%29%20of%20human%20pelvic%20bone%20and%20are%20presented%20as%20case%20studies.%20The%203D%20STL%20model%20constructed%20from%202D%20contour%20data%20points%20of%20ROI%20of%20segmented%20pelvic%20bone%20with%20and%20without%20filtering%20are%20presented.%20The%203D%20STL%20model%20reconstructed%20from%20filtered%202D%20data%20points%20improved%20the%20geometry%20of%20model%20compared%20to%20the%20model%20reconstructed%20without%20filtering%202D%20data%20points.&entry.1838667208=http%3A//arxiv.org/abs/2601.14997v1&entry.124074799=Read"},
{"title": "Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs", "author": "Mohammad Shahab Sepehri and Berk Tinaz and Zalan Fabian and Mahdi Soltanolkotabi", "abstract": "Mental visualization, the ability to construct and manipulate visual representations internally, is a core component of human cognition and plays a vital role in tasks involving reasoning, prediction, and abstraction. Despite the rapid progress of Multimodal Large Language Models (MLLMs), current benchmarks primarily assess passive visual perception, offering limited insight into the more active capability of internally constructing visual patterns to support problem solving. Yet mental visualization is a critical cognitive skill in humans, supporting abilities such as spatial navigation, predicting physical trajectories, and solving complex visual problems through imaginative simulation. To bridge this gap, we introduce Hyperphantasia, a synthetic benchmark designed to evaluate the mental visualization abilities of MLLMs through four carefully constructed puzzles. Each puzzle is procedurally generated and presented at three difficulty levels, enabling controlled analysis of model performance across increasing complexity. Our comprehensive evaluation of state-of-the-art models reveals a substantial gap between the performance of humans and MLLMs. Additionally, we explore the potential of reinforcement learning to improve visual simulation capabilities. Our findings suggest that while some models exhibit partial competence in recognizing visual patterns, robust mental visualization remains an open challenge for current MLLMs.", "link": "http://arxiv.org/abs/2507.11932v2", "date": "2026-01-21", "relevancy": 2.7827, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5688}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5688}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperphantasia%3A%20A%20Benchmark%20for%20Evaluating%20the%20Mental%20Visualization%20Capabilities%20of%20Multimodal%20LLMs&body=Title%3A%20Hyperphantasia%3A%20A%20Benchmark%20for%20Evaluating%20the%20Mental%20Visualization%20Capabilities%20of%20Multimodal%20LLMs%0AAuthor%3A%20Mohammad%20Shahab%20Sepehri%20and%20Berk%20Tinaz%20and%20Zalan%20Fabian%20and%20Mahdi%20Soltanolkotabi%0AAbstract%3A%20Mental%20visualization%2C%20the%20ability%20to%20construct%20and%20manipulate%20visual%20representations%20internally%2C%20is%20a%20core%20component%20of%20human%20cognition%20and%20plays%20a%20vital%20role%20in%20tasks%20involving%20reasoning%2C%20prediction%2C%20and%20abstraction.%20Despite%20the%20rapid%20progress%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20current%20benchmarks%20primarily%20assess%20passive%20visual%20perception%2C%20offering%20limited%20insight%20into%20the%20more%20active%20capability%20of%20internally%20constructing%20visual%20patterns%20to%20support%20problem%20solving.%20Yet%20mental%20visualization%20is%20a%20critical%20cognitive%20skill%20in%20humans%2C%20supporting%20abilities%20such%20as%20spatial%20navigation%2C%20predicting%20physical%20trajectories%2C%20and%20solving%20complex%20visual%20problems%20through%20imaginative%20simulation.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Hyperphantasia%2C%20a%20synthetic%20benchmark%20designed%20to%20evaluate%20the%20mental%20visualization%20abilities%20of%20MLLMs%20through%20four%20carefully%20constructed%20puzzles.%20Each%20puzzle%20is%20procedurally%20generated%20and%20presented%20at%20three%20difficulty%20levels%2C%20enabling%20controlled%20analysis%20of%20model%20performance%20across%20increasing%20complexity.%20Our%20comprehensive%20evaluation%20of%20state-of-the-art%20models%20reveals%20a%20substantial%20gap%20between%20the%20performance%20of%20humans%20and%20MLLMs.%20Additionally%2C%20we%20explore%20the%20potential%20of%20reinforcement%20learning%20to%20improve%20visual%20simulation%20capabilities.%20Our%20findings%20suggest%20that%20while%20some%20models%20exhibit%20partial%20competence%20in%20recognizing%20visual%20patterns%2C%20robust%20mental%20visualization%20remains%20an%20open%20challenge%20for%20current%20MLLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2507.11932v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperphantasia%253A%2520A%2520Benchmark%2520for%2520Evaluating%2520the%2520Mental%2520Visualization%2520Capabilities%2520of%2520Multimodal%2520LLMs%26entry.906535625%3DMohammad%2520Shahab%2520Sepehri%2520and%2520Berk%2520Tinaz%2520and%2520Zalan%2520Fabian%2520and%2520Mahdi%2520Soltanolkotabi%26entry.1292438233%3DMental%2520visualization%252C%2520the%2520ability%2520to%2520construct%2520and%2520manipulate%2520visual%2520representations%2520internally%252C%2520is%2520a%2520core%2520component%2520of%2520human%2520cognition%2520and%2520plays%2520a%2520vital%2520role%2520in%2520tasks%2520involving%2520reasoning%252C%2520prediction%252C%2520and%2520abstraction.%2520Despite%2520the%2520rapid%2520progress%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520current%2520benchmarks%2520primarily%2520assess%2520passive%2520visual%2520perception%252C%2520offering%2520limited%2520insight%2520into%2520the%2520more%2520active%2520capability%2520of%2520internally%2520constructing%2520visual%2520patterns%2520to%2520support%2520problem%2520solving.%2520Yet%2520mental%2520visualization%2520is%2520a%2520critical%2520cognitive%2520skill%2520in%2520humans%252C%2520supporting%2520abilities%2520such%2520as%2520spatial%2520navigation%252C%2520predicting%2520physical%2520trajectories%252C%2520and%2520solving%2520complex%2520visual%2520problems%2520through%2520imaginative%2520simulation.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520Hyperphantasia%252C%2520a%2520synthetic%2520benchmark%2520designed%2520to%2520evaluate%2520the%2520mental%2520visualization%2520abilities%2520of%2520MLLMs%2520through%2520four%2520carefully%2520constructed%2520puzzles.%2520Each%2520puzzle%2520is%2520procedurally%2520generated%2520and%2520presented%2520at%2520three%2520difficulty%2520levels%252C%2520enabling%2520controlled%2520analysis%2520of%2520model%2520performance%2520across%2520increasing%2520complexity.%2520Our%2520comprehensive%2520evaluation%2520of%2520state-of-the-art%2520models%2520reveals%2520a%2520substantial%2520gap%2520between%2520the%2520performance%2520of%2520humans%2520and%2520MLLMs.%2520Additionally%252C%2520we%2520explore%2520the%2520potential%2520of%2520reinforcement%2520learning%2520to%2520improve%2520visual%2520simulation%2520capabilities.%2520Our%2520findings%2520suggest%2520that%2520while%2520some%2520models%2520exhibit%2520partial%2520competence%2520in%2520recognizing%2520visual%2520patterns%252C%2520robust%2520mental%2520visualization%2520remains%2520an%2520open%2520challenge%2520for%2520current%2520MLLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11932v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperphantasia%3A%20A%20Benchmark%20for%20Evaluating%20the%20Mental%20Visualization%20Capabilities%20of%20Multimodal%20LLMs&entry.906535625=Mohammad%20Shahab%20Sepehri%20and%20Berk%20Tinaz%20and%20Zalan%20Fabian%20and%20Mahdi%20Soltanolkotabi&entry.1292438233=Mental%20visualization%2C%20the%20ability%20to%20construct%20and%20manipulate%20visual%20representations%20internally%2C%20is%20a%20core%20component%20of%20human%20cognition%20and%20plays%20a%20vital%20role%20in%20tasks%20involving%20reasoning%2C%20prediction%2C%20and%20abstraction.%20Despite%20the%20rapid%20progress%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20current%20benchmarks%20primarily%20assess%20passive%20visual%20perception%2C%20offering%20limited%20insight%20into%20the%20more%20active%20capability%20of%20internally%20constructing%20visual%20patterns%20to%20support%20problem%20solving.%20Yet%20mental%20visualization%20is%20a%20critical%20cognitive%20skill%20in%20humans%2C%20supporting%20abilities%20such%20as%20spatial%20navigation%2C%20predicting%20physical%20trajectories%2C%20and%20solving%20complex%20visual%20problems%20through%20imaginative%20simulation.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Hyperphantasia%2C%20a%20synthetic%20benchmark%20designed%20to%20evaluate%20the%20mental%20visualization%20abilities%20of%20MLLMs%20through%20four%20carefully%20constructed%20puzzles.%20Each%20puzzle%20is%20procedurally%20generated%20and%20presented%20at%20three%20difficulty%20levels%2C%20enabling%20controlled%20analysis%20of%20model%20performance%20across%20increasing%20complexity.%20Our%20comprehensive%20evaluation%20of%20state-of-the-art%20models%20reveals%20a%20substantial%20gap%20between%20the%20performance%20of%20humans%20and%20MLLMs.%20Additionally%2C%20we%20explore%20the%20potential%20of%20reinforcement%20learning%20to%20improve%20visual%20simulation%20capabilities.%20Our%20findings%20suggest%20that%20while%20some%20models%20exhibit%20partial%20competence%20in%20recognizing%20visual%20patterns%2C%20robust%20mental%20visualization%20remains%20an%20open%20challenge%20for%20current%20MLLMs.&entry.1838667208=http%3A//arxiv.org/abs/2507.11932v2&entry.124074799=Read"},
{"title": "Large Language Models Encode Semantics and Alignment in Linearly Separable Representations", "author": "Baturay Saglam and Paul Kassianik and Blaine Nelson and Sajana Weerawardhena and Yaron Singer and Amin Karbasi", "abstract": "Understanding the latent space geometry of large language models (LLMs) is key to interpreting their behavior and improving alignment. Yet it remains unclear to what extent LLMs linearly organize representations related to semantic understanding. To explore this, we conduct a large-scale empirical study of hidden representations in 11 autoregressive models across six scientific topics. We find that high-level semantic information consistently resides in low-dimensional subspaces that form linearly separable representations across domains. This separability becomes more pronounced in deeper layers and under prompts that elicit structured reasoning or alignment behavior$\\unicode{x2013}$even when surface content remains unchanged. These findings motivate geometry-aware tools that operate directly in latent space to detect and mitigate harmful and adversarial content. As a proof of concept, we train an MLP probe on final-layer hidden states as a lightweight latent-space guardrail. This approach substantially improves refusal rates on malicious queries and prompt injections that bypass both the model's built-in safety alignment and external token-level filters.", "link": "http://arxiv.org/abs/2507.09709v3", "date": "2026-01-21", "relevancy": 2.7794, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5731}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5731}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Encode%20Semantics%20and%20Alignment%20in%20Linearly%20Separable%20Representations&body=Title%3A%20Large%20Language%20Models%20Encode%20Semantics%20and%20Alignment%20in%20Linearly%20Separable%20Representations%0AAuthor%3A%20Baturay%20Saglam%20and%20Paul%20Kassianik%20and%20Blaine%20Nelson%20and%20Sajana%20Weerawardhena%20and%20Yaron%20Singer%20and%20Amin%20Karbasi%0AAbstract%3A%20Understanding%20the%20latent%20space%20geometry%20of%20large%20language%20models%20%28LLMs%29%20is%20key%20to%20interpreting%20their%20behavior%20and%20improving%20alignment.%20Yet%20it%20remains%20unclear%20to%20what%20extent%20LLMs%20linearly%20organize%20representations%20related%20to%20semantic%20understanding.%20To%20explore%20this%2C%20we%20conduct%20a%20large-scale%20empirical%20study%20of%20hidden%20representations%20in%2011%20autoregressive%20models%20across%20six%20scientific%20topics.%20We%20find%20that%20high-level%20semantic%20information%20consistently%20resides%20in%20low-dimensional%20subspaces%20that%20form%20linearly%20separable%20representations%20across%20domains.%20This%20separability%20becomes%20more%20pronounced%20in%20deeper%20layers%20and%20under%20prompts%20that%20elicit%20structured%20reasoning%20or%20alignment%20behavior%24%5Cunicode%7Bx2013%7D%24even%20when%20surface%20content%20remains%20unchanged.%20These%20findings%20motivate%20geometry-aware%20tools%20that%20operate%20directly%20in%20latent%20space%20to%20detect%20and%20mitigate%20harmful%20and%20adversarial%20content.%20As%20a%20proof%20of%20concept%2C%20we%20train%20an%20MLP%20probe%20on%20final-layer%20hidden%20states%20as%20a%20lightweight%20latent-space%20guardrail.%20This%20approach%20substantially%20improves%20refusal%20rates%20on%20malicious%20queries%20and%20prompt%20injections%20that%20bypass%20both%20the%20model%27s%20built-in%20safety%20alignment%20and%20external%20token-level%20filters.%0ALink%3A%20http%3A//arxiv.org/abs/2507.09709v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Encode%2520Semantics%2520and%2520Alignment%2520in%2520Linearly%2520Separable%2520Representations%26entry.906535625%3DBaturay%2520Saglam%2520and%2520Paul%2520Kassianik%2520and%2520Blaine%2520Nelson%2520and%2520Sajana%2520Weerawardhena%2520and%2520Yaron%2520Singer%2520and%2520Amin%2520Karbasi%26entry.1292438233%3DUnderstanding%2520the%2520latent%2520space%2520geometry%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520key%2520to%2520interpreting%2520their%2520behavior%2520and%2520improving%2520alignment.%2520Yet%2520it%2520remains%2520unclear%2520to%2520what%2520extent%2520LLMs%2520linearly%2520organize%2520representations%2520related%2520to%2520semantic%2520understanding.%2520To%2520explore%2520this%252C%2520we%2520conduct%2520a%2520large-scale%2520empirical%2520study%2520of%2520hidden%2520representations%2520in%252011%2520autoregressive%2520models%2520across%2520six%2520scientific%2520topics.%2520We%2520find%2520that%2520high-level%2520semantic%2520information%2520consistently%2520resides%2520in%2520low-dimensional%2520subspaces%2520that%2520form%2520linearly%2520separable%2520representations%2520across%2520domains.%2520This%2520separability%2520becomes%2520more%2520pronounced%2520in%2520deeper%2520layers%2520and%2520under%2520prompts%2520that%2520elicit%2520structured%2520reasoning%2520or%2520alignment%2520behavior%2524%255Cunicode%257Bx2013%257D%2524even%2520when%2520surface%2520content%2520remains%2520unchanged.%2520These%2520findings%2520motivate%2520geometry-aware%2520tools%2520that%2520operate%2520directly%2520in%2520latent%2520space%2520to%2520detect%2520and%2520mitigate%2520harmful%2520and%2520adversarial%2520content.%2520As%2520a%2520proof%2520of%2520concept%252C%2520we%2520train%2520an%2520MLP%2520probe%2520on%2520final-layer%2520hidden%2520states%2520as%2520a%2520lightweight%2520latent-space%2520guardrail.%2520This%2520approach%2520substantially%2520improves%2520refusal%2520rates%2520on%2520malicious%2520queries%2520and%2520prompt%2520injections%2520that%2520bypass%2520both%2520the%2520model%2527s%2520built-in%2520safety%2520alignment%2520and%2520external%2520token-level%2520filters.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09709v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Encode%20Semantics%20and%20Alignment%20in%20Linearly%20Separable%20Representations&entry.906535625=Baturay%20Saglam%20and%20Paul%20Kassianik%20and%20Blaine%20Nelson%20and%20Sajana%20Weerawardhena%20and%20Yaron%20Singer%20and%20Amin%20Karbasi&entry.1292438233=Understanding%20the%20latent%20space%20geometry%20of%20large%20language%20models%20%28LLMs%29%20is%20key%20to%20interpreting%20their%20behavior%20and%20improving%20alignment.%20Yet%20it%20remains%20unclear%20to%20what%20extent%20LLMs%20linearly%20organize%20representations%20related%20to%20semantic%20understanding.%20To%20explore%20this%2C%20we%20conduct%20a%20large-scale%20empirical%20study%20of%20hidden%20representations%20in%2011%20autoregressive%20models%20across%20six%20scientific%20topics.%20We%20find%20that%20high-level%20semantic%20information%20consistently%20resides%20in%20low-dimensional%20subspaces%20that%20form%20linearly%20separable%20representations%20across%20domains.%20This%20separability%20becomes%20more%20pronounced%20in%20deeper%20layers%20and%20under%20prompts%20that%20elicit%20structured%20reasoning%20or%20alignment%20behavior%24%5Cunicode%7Bx2013%7D%24even%20when%20surface%20content%20remains%20unchanged.%20These%20findings%20motivate%20geometry-aware%20tools%20that%20operate%20directly%20in%20latent%20space%20to%20detect%20and%20mitigate%20harmful%20and%20adversarial%20content.%20As%20a%20proof%20of%20concept%2C%20we%20train%20an%20MLP%20probe%20on%20final-layer%20hidden%20states%20as%20a%20lightweight%20latent-space%20guardrail.%20This%20approach%20substantially%20improves%20refusal%20rates%20on%20malicious%20queries%20and%20prompt%20injections%20that%20bypass%20both%20the%20model%27s%20built-in%20safety%20alignment%20and%20external%20token-level%20filters.&entry.1838667208=http%3A//arxiv.org/abs/2507.09709v3&entry.124074799=Read"},
{"title": "Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large Language Models", "author": "Umberto Cappellazzo and Xubo Liu and Pingchuan Ma and Stavros Petridis and Maja Pantic", "abstract": "Large language models (LLMs) have recently achieved impressive results in speech recognition across multiple modalities, including Auditory Speech Recognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual Speech Recognition (AVSR). Despite this progress, current LLM-based approaches typically address each task independently, training separate models that raise computational and deployment resource use while missing potential cross-task synergies. They also rely on fixed-rate token compression, which restricts flexibility in balancing accuracy with efficiency. These limitations highlight the need for a unified framework that can support ASR, VSR, and AVSR while enabling elastic inference. To this end, we present Omni-AVSR, a unified audio-visual LLM that combines efficient multi-granularity training with parameter-efficient adaptation. Specifically, we adapt the matryoshka representation learning paradigm to efficiently train across multiple audio and visual granularities, reducing its inherent training resource use. Furthermore, we explore three LoRA-based strategies for adapting the backbone LLM, balancing shared and task-specific specialization. Experiments on LRS2 and LRS3 show that Omni-AVSR achieves comparable or superior accuracy to state-of-the-art baselines while training a single model at substantially lower training and deployment resource use. The model also remains robust under acoustic noise, and we analyze its scaling behavior as LLM size increases, providing insights into the trade-off between performance and efficiency.", "link": "http://arxiv.org/abs/2511.07253v2", "date": "2026-01-21", "relevancy": 2.7786, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5642}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5642}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omni-AVSR%3A%20Towards%20Unified%20Multimodal%20Speech%20Recognition%20with%20Large%20Language%20Models&body=Title%3A%20Omni-AVSR%3A%20Towards%20Unified%20Multimodal%20Speech%20Recognition%20with%20Large%20Language%20Models%0AAuthor%3A%20Umberto%20Cappellazzo%20and%20Xubo%20Liu%20and%20Pingchuan%20Ma%20and%20Stavros%20Petridis%20and%20Maja%20Pantic%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20recently%20achieved%20impressive%20results%20in%20speech%20recognition%20across%20multiple%20modalities%2C%20including%20Auditory%20Speech%20Recognition%20%28ASR%29%2C%20Visual%20Speech%20Recognition%20%28VSR%29%2C%20and%20Audio-Visual%20Speech%20Recognition%20%28AVSR%29.%20Despite%20this%20progress%2C%20current%20LLM-based%20approaches%20typically%20address%20each%20task%20independently%2C%20training%20separate%20models%20that%20raise%20computational%20and%20deployment%20resource%20use%20while%20missing%20potential%20cross-task%20synergies.%20They%20also%20rely%20on%20fixed-rate%20token%20compression%2C%20which%20restricts%20flexibility%20in%20balancing%20accuracy%20with%20efficiency.%20These%20limitations%20highlight%20the%20need%20for%20a%20unified%20framework%20that%20can%20support%20ASR%2C%20VSR%2C%20and%20AVSR%20while%20enabling%20elastic%20inference.%20To%20this%20end%2C%20we%20present%20Omni-AVSR%2C%20a%20unified%20audio-visual%20LLM%20that%20combines%20efficient%20multi-granularity%20training%20with%20parameter-efficient%20adaptation.%20Specifically%2C%20we%20adapt%20the%20matryoshka%20representation%20learning%20paradigm%20to%20efficiently%20train%20across%20multiple%20audio%20and%20visual%20granularities%2C%20reducing%20its%20inherent%20training%20resource%20use.%20Furthermore%2C%20we%20explore%20three%20LoRA-based%20strategies%20for%20adapting%20the%20backbone%20LLM%2C%20balancing%20shared%20and%20task-specific%20specialization.%20Experiments%20on%20LRS2%20and%20LRS3%20show%20that%20Omni-AVSR%20achieves%20comparable%20or%20superior%20accuracy%20to%20state-of-the-art%20baselines%20while%20training%20a%20single%20model%20at%20substantially%20lower%20training%20and%20deployment%20resource%20use.%20The%20model%20also%20remains%20robust%20under%20acoustic%20noise%2C%20and%20we%20analyze%20its%20scaling%20behavior%20as%20LLM%20size%20increases%2C%20providing%20insights%20into%20the%20trade-off%20between%20performance%20and%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2511.07253v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmni-AVSR%253A%2520Towards%2520Unified%2520Multimodal%2520Speech%2520Recognition%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DUmberto%2520Cappellazzo%2520and%2520Xubo%2520Liu%2520and%2520Pingchuan%2520Ma%2520and%2520Stavros%2520Petridis%2520and%2520Maja%2520Pantic%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520recently%2520achieved%2520impressive%2520results%2520in%2520speech%2520recognition%2520across%2520multiple%2520modalities%252C%2520including%2520Auditory%2520Speech%2520Recognition%2520%2528ASR%2529%252C%2520Visual%2520Speech%2520Recognition%2520%2528VSR%2529%252C%2520and%2520Audio-Visual%2520Speech%2520Recognition%2520%2528AVSR%2529.%2520Despite%2520this%2520progress%252C%2520current%2520LLM-based%2520approaches%2520typically%2520address%2520each%2520task%2520independently%252C%2520training%2520separate%2520models%2520that%2520raise%2520computational%2520and%2520deployment%2520resource%2520use%2520while%2520missing%2520potential%2520cross-task%2520synergies.%2520They%2520also%2520rely%2520on%2520fixed-rate%2520token%2520compression%252C%2520which%2520restricts%2520flexibility%2520in%2520balancing%2520accuracy%2520with%2520efficiency.%2520These%2520limitations%2520highlight%2520the%2520need%2520for%2520a%2520unified%2520framework%2520that%2520can%2520support%2520ASR%252C%2520VSR%252C%2520and%2520AVSR%2520while%2520enabling%2520elastic%2520inference.%2520To%2520this%2520end%252C%2520we%2520present%2520Omni-AVSR%252C%2520a%2520unified%2520audio-visual%2520LLM%2520that%2520combines%2520efficient%2520multi-granularity%2520training%2520with%2520parameter-efficient%2520adaptation.%2520Specifically%252C%2520we%2520adapt%2520the%2520matryoshka%2520representation%2520learning%2520paradigm%2520to%2520efficiently%2520train%2520across%2520multiple%2520audio%2520and%2520visual%2520granularities%252C%2520reducing%2520its%2520inherent%2520training%2520resource%2520use.%2520Furthermore%252C%2520we%2520explore%2520three%2520LoRA-based%2520strategies%2520for%2520adapting%2520the%2520backbone%2520LLM%252C%2520balancing%2520shared%2520and%2520task-specific%2520specialization.%2520Experiments%2520on%2520LRS2%2520and%2520LRS3%2520show%2520that%2520Omni-AVSR%2520achieves%2520comparable%2520or%2520superior%2520accuracy%2520to%2520state-of-the-art%2520baselines%2520while%2520training%2520a%2520single%2520model%2520at%2520substantially%2520lower%2520training%2520and%2520deployment%2520resource%2520use.%2520The%2520model%2520also%2520remains%2520robust%2520under%2520acoustic%2520noise%252C%2520and%2520we%2520analyze%2520its%2520scaling%2520behavior%2520as%2520LLM%2520size%2520increases%252C%2520providing%2520insights%2520into%2520the%2520trade-off%2520between%2520performance%2520and%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.07253v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omni-AVSR%3A%20Towards%20Unified%20Multimodal%20Speech%20Recognition%20with%20Large%20Language%20Models&entry.906535625=Umberto%20Cappellazzo%20and%20Xubo%20Liu%20and%20Pingchuan%20Ma%20and%20Stavros%20Petridis%20and%20Maja%20Pantic&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20recently%20achieved%20impressive%20results%20in%20speech%20recognition%20across%20multiple%20modalities%2C%20including%20Auditory%20Speech%20Recognition%20%28ASR%29%2C%20Visual%20Speech%20Recognition%20%28VSR%29%2C%20and%20Audio-Visual%20Speech%20Recognition%20%28AVSR%29.%20Despite%20this%20progress%2C%20current%20LLM-based%20approaches%20typically%20address%20each%20task%20independently%2C%20training%20separate%20models%20that%20raise%20computational%20and%20deployment%20resource%20use%20while%20missing%20potential%20cross-task%20synergies.%20They%20also%20rely%20on%20fixed-rate%20token%20compression%2C%20which%20restricts%20flexibility%20in%20balancing%20accuracy%20with%20efficiency.%20These%20limitations%20highlight%20the%20need%20for%20a%20unified%20framework%20that%20can%20support%20ASR%2C%20VSR%2C%20and%20AVSR%20while%20enabling%20elastic%20inference.%20To%20this%20end%2C%20we%20present%20Omni-AVSR%2C%20a%20unified%20audio-visual%20LLM%20that%20combines%20efficient%20multi-granularity%20training%20with%20parameter-efficient%20adaptation.%20Specifically%2C%20we%20adapt%20the%20matryoshka%20representation%20learning%20paradigm%20to%20efficiently%20train%20across%20multiple%20audio%20and%20visual%20granularities%2C%20reducing%20its%20inherent%20training%20resource%20use.%20Furthermore%2C%20we%20explore%20three%20LoRA-based%20strategies%20for%20adapting%20the%20backbone%20LLM%2C%20balancing%20shared%20and%20task-specific%20specialization.%20Experiments%20on%20LRS2%20and%20LRS3%20show%20that%20Omni-AVSR%20achieves%20comparable%20or%20superior%20accuracy%20to%20state-of-the-art%20baselines%20while%20training%20a%20single%20model%20at%20substantially%20lower%20training%20and%20deployment%20resource%20use.%20The%20model%20also%20remains%20robust%20under%20acoustic%20noise%2C%20and%20we%20analyze%20its%20scaling%20behavior%20as%20LLM%20size%20increases%2C%20providing%20insights%20into%20the%20trade-off%20between%20performance%20and%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2511.07253v2&entry.124074799=Read"},
{"title": "ScenDi: 3D-to-2D Scene Diffusion Cascades for Urban Generation", "author": "Hanlei Guo and Jiahao Shao and Xinya Chen and Xiyang Tan and Sheng Miao and Yujun Shen and Yiyi Liao", "abstract": "Recent advancements in 3D object generation using diffusion models have achieved remarkable success, but generating realistic 3D urban scenes remains challenging. Existing methods relying solely on 3D diffusion models tend to suffer a degradation in appearance details, while those utilizing only 2D diffusion models typically compromise camera controllability. To overcome this limitation, we propose ScenDi, a method for urban scene generation that integrates both 3D and 2D diffusion models. We first train a 3D latent diffusion model to generate 3D Gaussians, enabling the rendering of images at a relatively low resolution. To enable controllable synthesis, this 3DGS generation process can be optionally conditioned by specifying inputs such as 3d bounding boxes, road maps, or text prompts. Then, we train a 2D video diffusion model to enhance appearance details conditioned on rendered images from the 3D Gaussians. By leveraging the coarse 3D scene as guidance for 2D video diffusion, ScenDi generates desired scenes based on input conditions and successfully adheres to accurate camera trajectories. Experiments on two challenging real-world datasets, Waymo and KITTI-360, demonstrate the effectiveness of our approach.", "link": "http://arxiv.org/abs/2601.15221v1", "date": "2026-01-21", "relevancy": 2.7486, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6913}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6913}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScenDi%3A%203D-to-2D%20Scene%20Diffusion%20Cascades%20for%20Urban%20Generation&body=Title%3A%20ScenDi%3A%203D-to-2D%20Scene%20Diffusion%20Cascades%20for%20Urban%20Generation%0AAuthor%3A%20Hanlei%20Guo%20and%20Jiahao%20Shao%20and%20Xinya%20Chen%20and%20Xiyang%20Tan%20and%20Sheng%20Miao%20and%20Yujun%20Shen%20and%20Yiyi%20Liao%0AAbstract%3A%20Recent%20advancements%20in%203D%20object%20generation%20using%20diffusion%20models%20have%20achieved%20remarkable%20success%2C%20but%20generating%20realistic%203D%20urban%20scenes%20remains%20challenging.%20Existing%20methods%20relying%20solely%20on%203D%20diffusion%20models%20tend%20to%20suffer%20a%20degradation%20in%20appearance%20details%2C%20while%20those%20utilizing%20only%202D%20diffusion%20models%20typically%20compromise%20camera%20controllability.%20To%20overcome%20this%20limitation%2C%20we%20propose%20ScenDi%2C%20a%20method%20for%20urban%20scene%20generation%20that%20integrates%20both%203D%20and%202D%20diffusion%20models.%20We%20first%20train%20a%203D%20latent%20diffusion%20model%20to%20generate%203D%20Gaussians%2C%20enabling%20the%20rendering%20of%20images%20at%20a%20relatively%20low%20resolution.%20To%20enable%20controllable%20synthesis%2C%20this%203DGS%20generation%20process%20can%20be%20optionally%20conditioned%20by%20specifying%20inputs%20such%20as%203d%20bounding%20boxes%2C%20road%20maps%2C%20or%20text%20prompts.%20Then%2C%20we%20train%20a%202D%20video%20diffusion%20model%20to%20enhance%20appearance%20details%20conditioned%20on%20rendered%20images%20from%20the%203D%20Gaussians.%20By%20leveraging%20the%20coarse%203D%20scene%20as%20guidance%20for%202D%20video%20diffusion%2C%20ScenDi%20generates%20desired%20scenes%20based%20on%20input%20conditions%20and%20successfully%20adheres%20to%20accurate%20camera%20trajectories.%20Experiments%20on%20two%20challenging%20real-world%20datasets%2C%20Waymo%20and%20KITTI-360%2C%20demonstrate%20the%20effectiveness%20of%20our%20approach.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScenDi%253A%25203D-to-2D%2520Scene%2520Diffusion%2520Cascades%2520for%2520Urban%2520Generation%26entry.906535625%3DHanlei%2520Guo%2520and%2520Jiahao%2520Shao%2520and%2520Xinya%2520Chen%2520and%2520Xiyang%2520Tan%2520and%2520Sheng%2520Miao%2520and%2520Yujun%2520Shen%2520and%2520Yiyi%2520Liao%26entry.1292438233%3DRecent%2520advancements%2520in%25203D%2520object%2520generation%2520using%2520diffusion%2520models%2520have%2520achieved%2520remarkable%2520success%252C%2520but%2520generating%2520realistic%25203D%2520urban%2520scenes%2520remains%2520challenging.%2520Existing%2520methods%2520relying%2520solely%2520on%25203D%2520diffusion%2520models%2520tend%2520to%2520suffer%2520a%2520degradation%2520in%2520appearance%2520details%252C%2520while%2520those%2520utilizing%2520only%25202D%2520diffusion%2520models%2520typically%2520compromise%2520camera%2520controllability.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520ScenDi%252C%2520a%2520method%2520for%2520urban%2520scene%2520generation%2520that%2520integrates%2520both%25203D%2520and%25202D%2520diffusion%2520models.%2520We%2520first%2520train%2520a%25203D%2520latent%2520diffusion%2520model%2520to%2520generate%25203D%2520Gaussians%252C%2520enabling%2520the%2520rendering%2520of%2520images%2520at%2520a%2520relatively%2520low%2520resolution.%2520To%2520enable%2520controllable%2520synthesis%252C%2520this%25203DGS%2520generation%2520process%2520can%2520be%2520optionally%2520conditioned%2520by%2520specifying%2520inputs%2520such%2520as%25203d%2520bounding%2520boxes%252C%2520road%2520maps%252C%2520or%2520text%2520prompts.%2520Then%252C%2520we%2520train%2520a%25202D%2520video%2520diffusion%2520model%2520to%2520enhance%2520appearance%2520details%2520conditioned%2520on%2520rendered%2520images%2520from%2520the%25203D%2520Gaussians.%2520By%2520leveraging%2520the%2520coarse%25203D%2520scene%2520as%2520guidance%2520for%25202D%2520video%2520diffusion%252C%2520ScenDi%2520generates%2520desired%2520scenes%2520based%2520on%2520input%2520conditions%2520and%2520successfully%2520adheres%2520to%2520accurate%2520camera%2520trajectories.%2520Experiments%2520on%2520two%2520challenging%2520real-world%2520datasets%252C%2520Waymo%2520and%2520KITTI-360%252C%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScenDi%3A%203D-to-2D%20Scene%20Diffusion%20Cascades%20for%20Urban%20Generation&entry.906535625=Hanlei%20Guo%20and%20Jiahao%20Shao%20and%20Xinya%20Chen%20and%20Xiyang%20Tan%20and%20Sheng%20Miao%20and%20Yujun%20Shen%20and%20Yiyi%20Liao&entry.1292438233=Recent%20advancements%20in%203D%20object%20generation%20using%20diffusion%20models%20have%20achieved%20remarkable%20success%2C%20but%20generating%20realistic%203D%20urban%20scenes%20remains%20challenging.%20Existing%20methods%20relying%20solely%20on%203D%20diffusion%20models%20tend%20to%20suffer%20a%20degradation%20in%20appearance%20details%2C%20while%20those%20utilizing%20only%202D%20diffusion%20models%20typically%20compromise%20camera%20controllability.%20To%20overcome%20this%20limitation%2C%20we%20propose%20ScenDi%2C%20a%20method%20for%20urban%20scene%20generation%20that%20integrates%20both%203D%20and%202D%20diffusion%20models.%20We%20first%20train%20a%203D%20latent%20diffusion%20model%20to%20generate%203D%20Gaussians%2C%20enabling%20the%20rendering%20of%20images%20at%20a%20relatively%20low%20resolution.%20To%20enable%20controllable%20synthesis%2C%20this%203DGS%20generation%20process%20can%20be%20optionally%20conditioned%20by%20specifying%20inputs%20such%20as%203d%20bounding%20boxes%2C%20road%20maps%2C%20or%20text%20prompts.%20Then%2C%20we%20train%20a%202D%20video%20diffusion%20model%20to%20enhance%20appearance%20details%20conditioned%20on%20rendered%20images%20from%20the%203D%20Gaussians.%20By%20leveraging%20the%20coarse%203D%20scene%20as%20guidance%20for%202D%20video%20diffusion%2C%20ScenDi%20generates%20desired%20scenes%20based%20on%20input%20conditions%20and%20successfully%20adheres%20to%20accurate%20camera%20trajectories.%20Experiments%20on%20two%20challenging%20real-world%20datasets%2C%20Waymo%20and%20KITTI-360%2C%20demonstrate%20the%20effectiveness%20of%20our%20approach.&entry.1838667208=http%3A//arxiv.org/abs/2601.15221v1&entry.124074799=Read"},
{"title": "Mixture-of-Experts Models in Vision: Routing, Optimization, and Generalization", "author": "Adam Rokah and Daniel Veress and Caleb Caulk and Sourav Sharan", "abstract": "Mixture-of-Experts (MoE) architectures enable conditional computation by routing inputs to multiple expert subnetworks and are often motivated as a mechanism for scaling large language models. In this project, we instead study MoE behavior in an image classification setting, focusing on predictive performance, expert utilization, and generalization. We compare dense, SoftMoE, and SparseMoE classifier heads on the CIFAR10 dataset under comparable model capacity. Both MoE variants achieve slightly higher validation accuracy than the dense baseline while maintaining balanced expert utilization through regularization, avoiding expert collapse. To analyze generalization, we compute Hessian-based sharpness metrics at convergence, including the largest eigenvalue and trace of the loss Hessian, evaluated on both training and test data. We find that SoftMoE exhibits higher sharpness by these metrics, while Dense and SparseMoE lie in a similar curvature regime, despite all models achieving comparable generalization performance. Complementary loss surface perturbation analyses reveal qualitative differences in non-local behavior under finite parameter perturbations between dense and MoE models, which help contextualize curvature-based measurements without directly explaining validation accuracy. We further evaluate empirical inference efficiency and show that naively implemented conditional routing does not yield inference speedups on modern hardware at this scale, highlighting the gap between theoretical and realized efficiency in sparse MoE models.", "link": "http://arxiv.org/abs/2601.15021v1", "date": "2026-01-21", "relevancy": 2.7459, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.569}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5482}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixture-of-Experts%20Models%20in%20Vision%3A%20Routing%2C%20Optimization%2C%20and%20Generalization&body=Title%3A%20Mixture-of-Experts%20Models%20in%20Vision%3A%20Routing%2C%20Optimization%2C%20and%20Generalization%0AAuthor%3A%20Adam%20Rokah%20and%20Daniel%20Veress%20and%20Caleb%20Caulk%20and%20Sourav%20Sharan%0AAbstract%3A%20Mixture-of-Experts%20%28MoE%29%20architectures%20enable%20conditional%20computation%20by%20routing%20inputs%20to%20multiple%20expert%20subnetworks%20and%20are%20often%20motivated%20as%20a%20mechanism%20for%20scaling%20large%20language%20models.%20In%20this%20project%2C%20we%20instead%20study%20MoE%20behavior%20in%20an%20image%20classification%20setting%2C%20focusing%20on%20predictive%20performance%2C%20expert%20utilization%2C%20and%20generalization.%20We%20compare%20dense%2C%20SoftMoE%2C%20and%20SparseMoE%20classifier%20heads%20on%20the%20CIFAR10%20dataset%20under%20comparable%20model%20capacity.%20Both%20MoE%20variants%20achieve%20slightly%20higher%20validation%20accuracy%20than%20the%20dense%20baseline%20while%20maintaining%20balanced%20expert%20utilization%20through%20regularization%2C%20avoiding%20expert%20collapse.%20To%20analyze%20generalization%2C%20we%20compute%20Hessian-based%20sharpness%20metrics%20at%20convergence%2C%20including%20the%20largest%20eigenvalue%20and%20trace%20of%20the%20loss%20Hessian%2C%20evaluated%20on%20both%20training%20and%20test%20data.%20We%20find%20that%20SoftMoE%20exhibits%20higher%20sharpness%20by%20these%20metrics%2C%20while%20Dense%20and%20SparseMoE%20lie%20in%20a%20similar%20curvature%20regime%2C%20despite%20all%20models%20achieving%20comparable%20generalization%20performance.%20Complementary%20loss%20surface%20perturbation%20analyses%20reveal%20qualitative%20differences%20in%20non-local%20behavior%20under%20finite%20parameter%20perturbations%20between%20dense%20and%20MoE%20models%2C%20which%20help%20contextualize%20curvature-based%20measurements%20without%20directly%20explaining%20validation%20accuracy.%20We%20further%20evaluate%20empirical%20inference%20efficiency%20and%20show%20that%20naively%20implemented%20conditional%20routing%20does%20not%20yield%20inference%20speedups%20on%20modern%20hardware%20at%20this%20scale%2C%20highlighting%20the%20gap%20between%20theoretical%20and%20realized%20efficiency%20in%20sparse%20MoE%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixture-of-Experts%2520Models%2520in%2520Vision%253A%2520Routing%252C%2520Optimization%252C%2520and%2520Generalization%26entry.906535625%3DAdam%2520Rokah%2520and%2520Daniel%2520Veress%2520and%2520Caleb%2520Caulk%2520and%2520Sourav%2520Sharan%26entry.1292438233%3DMixture-of-Experts%2520%2528MoE%2529%2520architectures%2520enable%2520conditional%2520computation%2520by%2520routing%2520inputs%2520to%2520multiple%2520expert%2520subnetworks%2520and%2520are%2520often%2520motivated%2520as%2520a%2520mechanism%2520for%2520scaling%2520large%2520language%2520models.%2520In%2520this%2520project%252C%2520we%2520instead%2520study%2520MoE%2520behavior%2520in%2520an%2520image%2520classification%2520setting%252C%2520focusing%2520on%2520predictive%2520performance%252C%2520expert%2520utilization%252C%2520and%2520generalization.%2520We%2520compare%2520dense%252C%2520SoftMoE%252C%2520and%2520SparseMoE%2520classifier%2520heads%2520on%2520the%2520CIFAR10%2520dataset%2520under%2520comparable%2520model%2520capacity.%2520Both%2520MoE%2520variants%2520achieve%2520slightly%2520higher%2520validation%2520accuracy%2520than%2520the%2520dense%2520baseline%2520while%2520maintaining%2520balanced%2520expert%2520utilization%2520through%2520regularization%252C%2520avoiding%2520expert%2520collapse.%2520To%2520analyze%2520generalization%252C%2520we%2520compute%2520Hessian-based%2520sharpness%2520metrics%2520at%2520convergence%252C%2520including%2520the%2520largest%2520eigenvalue%2520and%2520trace%2520of%2520the%2520loss%2520Hessian%252C%2520evaluated%2520on%2520both%2520training%2520and%2520test%2520data.%2520We%2520find%2520that%2520SoftMoE%2520exhibits%2520higher%2520sharpness%2520by%2520these%2520metrics%252C%2520while%2520Dense%2520and%2520SparseMoE%2520lie%2520in%2520a%2520similar%2520curvature%2520regime%252C%2520despite%2520all%2520models%2520achieving%2520comparable%2520generalization%2520performance.%2520Complementary%2520loss%2520surface%2520perturbation%2520analyses%2520reveal%2520qualitative%2520differences%2520in%2520non-local%2520behavior%2520under%2520finite%2520parameter%2520perturbations%2520between%2520dense%2520and%2520MoE%2520models%252C%2520which%2520help%2520contextualize%2520curvature-based%2520measurements%2520without%2520directly%2520explaining%2520validation%2520accuracy.%2520We%2520further%2520evaluate%2520empirical%2520inference%2520efficiency%2520and%2520show%2520that%2520naively%2520implemented%2520conditional%2520routing%2520does%2520not%2520yield%2520inference%2520speedups%2520on%2520modern%2520hardware%2520at%2520this%2520scale%252C%2520highlighting%2520the%2520gap%2520between%2520theoretical%2520and%2520realized%2520efficiency%2520in%2520sparse%2520MoE%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixture-of-Experts%20Models%20in%20Vision%3A%20Routing%2C%20Optimization%2C%20and%20Generalization&entry.906535625=Adam%20Rokah%20and%20Daniel%20Veress%20and%20Caleb%20Caulk%20and%20Sourav%20Sharan&entry.1292438233=Mixture-of-Experts%20%28MoE%29%20architectures%20enable%20conditional%20computation%20by%20routing%20inputs%20to%20multiple%20expert%20subnetworks%20and%20are%20often%20motivated%20as%20a%20mechanism%20for%20scaling%20large%20language%20models.%20In%20this%20project%2C%20we%20instead%20study%20MoE%20behavior%20in%20an%20image%20classification%20setting%2C%20focusing%20on%20predictive%20performance%2C%20expert%20utilization%2C%20and%20generalization.%20We%20compare%20dense%2C%20SoftMoE%2C%20and%20SparseMoE%20classifier%20heads%20on%20the%20CIFAR10%20dataset%20under%20comparable%20model%20capacity.%20Both%20MoE%20variants%20achieve%20slightly%20higher%20validation%20accuracy%20than%20the%20dense%20baseline%20while%20maintaining%20balanced%20expert%20utilization%20through%20regularization%2C%20avoiding%20expert%20collapse.%20To%20analyze%20generalization%2C%20we%20compute%20Hessian-based%20sharpness%20metrics%20at%20convergence%2C%20including%20the%20largest%20eigenvalue%20and%20trace%20of%20the%20loss%20Hessian%2C%20evaluated%20on%20both%20training%20and%20test%20data.%20We%20find%20that%20SoftMoE%20exhibits%20higher%20sharpness%20by%20these%20metrics%2C%20while%20Dense%20and%20SparseMoE%20lie%20in%20a%20similar%20curvature%20regime%2C%20despite%20all%20models%20achieving%20comparable%20generalization%20performance.%20Complementary%20loss%20surface%20perturbation%20analyses%20reveal%20qualitative%20differences%20in%20non-local%20behavior%20under%20finite%20parameter%20perturbations%20between%20dense%20and%20MoE%20models%2C%20which%20help%20contextualize%20curvature-based%20measurements%20without%20directly%20explaining%20validation%20accuracy.%20We%20further%20evaluate%20empirical%20inference%20efficiency%20and%20show%20that%20naively%20implemented%20conditional%20routing%20does%20not%20yield%20inference%20speedups%20on%20modern%20hardware%20at%20this%20scale%2C%20highlighting%20the%20gap%20between%20theoretical%20and%20realized%20efficiency%20in%20sparse%20MoE%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.15021v1&entry.124074799=Read"},
{"title": "LuxRemix: Lighting Decomposition and Remixing for Indoor Scenes", "author": "Ruofan Liang and Norman M\u00fcller and Ethan Weber and Duncan Zauss and Nandita Vijaykumar and Peter Kontschieder and Christian Richardt", "abstract": "We present a novel approach for interactive light editing in indoor scenes from a single multi-view scene capture. Our method leverages a generative image-based light decomposition model that factorizes complex indoor scene illumination into its constituent light sources. This factorization enables independent manipulation of individual light sources, specifically allowing control over their state (on/off), chromaticity, and intensity. We further introduce multi-view lighting harmonization to ensure consistent propagation of the lighting decomposition across all scene views. This is integrated into a relightable 3D Gaussian splatting representation, providing real-time interactive control over the individual light sources. Our results demonstrate highly photorealistic lighting decomposition and relighting outcomes across diverse indoor scenes. We evaluate our method on both synthetic and real-world datasets and provide a quantitative and qualitative comparison to state-of-the-art techniques. For video results and interactive demos, see https://luxremix.github.io.", "link": "http://arxiv.org/abs/2601.15283v1", "date": "2026-01-21", "relevancy": 2.7298, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.553}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5425}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LuxRemix%3A%20Lighting%20Decomposition%20and%20Remixing%20for%20Indoor%20Scenes&body=Title%3A%20LuxRemix%3A%20Lighting%20Decomposition%20and%20Remixing%20for%20Indoor%20Scenes%0AAuthor%3A%20Ruofan%20Liang%20and%20Norman%20M%C3%BCller%20and%20Ethan%20Weber%20and%20Duncan%20Zauss%20and%20Nandita%20Vijaykumar%20and%20Peter%20Kontschieder%20and%20Christian%20Richardt%0AAbstract%3A%20We%20present%20a%20novel%20approach%20for%20interactive%20light%20editing%20in%20indoor%20scenes%20from%20a%20single%20multi-view%20scene%20capture.%20Our%20method%20leverages%20a%20generative%20image-based%20light%20decomposition%20model%20that%20factorizes%20complex%20indoor%20scene%20illumination%20into%20its%20constituent%20light%20sources.%20This%20factorization%20enables%20independent%20manipulation%20of%20individual%20light%20sources%2C%20specifically%20allowing%20control%20over%20their%20state%20%28on/off%29%2C%20chromaticity%2C%20and%20intensity.%20We%20further%20introduce%20multi-view%20lighting%20harmonization%20to%20ensure%20consistent%20propagation%20of%20the%20lighting%20decomposition%20across%20all%20scene%20views.%20This%20is%20integrated%20into%20a%20relightable%203D%20Gaussian%20splatting%20representation%2C%20providing%20real-time%20interactive%20control%20over%20the%20individual%20light%20sources.%20Our%20results%20demonstrate%20highly%20photorealistic%20lighting%20decomposition%20and%20relighting%20outcomes%20across%20diverse%20indoor%20scenes.%20We%20evaluate%20our%20method%20on%20both%20synthetic%20and%20real-world%20datasets%20and%20provide%20a%20quantitative%20and%20qualitative%20comparison%20to%20state-of-the-art%20techniques.%20For%20video%20results%20and%20interactive%20demos%2C%20see%20https%3A//luxremix.github.io.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLuxRemix%253A%2520Lighting%2520Decomposition%2520and%2520Remixing%2520for%2520Indoor%2520Scenes%26entry.906535625%3DRuofan%2520Liang%2520and%2520Norman%2520M%25C3%25BCller%2520and%2520Ethan%2520Weber%2520and%2520Duncan%2520Zauss%2520and%2520Nandita%2520Vijaykumar%2520and%2520Peter%2520Kontschieder%2520and%2520Christian%2520Richardt%26entry.1292438233%3DWe%2520present%2520a%2520novel%2520approach%2520for%2520interactive%2520light%2520editing%2520in%2520indoor%2520scenes%2520from%2520a%2520single%2520multi-view%2520scene%2520capture.%2520Our%2520method%2520leverages%2520a%2520generative%2520image-based%2520light%2520decomposition%2520model%2520that%2520factorizes%2520complex%2520indoor%2520scene%2520illumination%2520into%2520its%2520constituent%2520light%2520sources.%2520This%2520factorization%2520enables%2520independent%2520manipulation%2520of%2520individual%2520light%2520sources%252C%2520specifically%2520allowing%2520control%2520over%2520their%2520state%2520%2528on/off%2529%252C%2520chromaticity%252C%2520and%2520intensity.%2520We%2520further%2520introduce%2520multi-view%2520lighting%2520harmonization%2520to%2520ensure%2520consistent%2520propagation%2520of%2520the%2520lighting%2520decomposition%2520across%2520all%2520scene%2520views.%2520This%2520is%2520integrated%2520into%2520a%2520relightable%25203D%2520Gaussian%2520splatting%2520representation%252C%2520providing%2520real-time%2520interactive%2520control%2520over%2520the%2520individual%2520light%2520sources.%2520Our%2520results%2520demonstrate%2520highly%2520photorealistic%2520lighting%2520decomposition%2520and%2520relighting%2520outcomes%2520across%2520diverse%2520indoor%2520scenes.%2520We%2520evaluate%2520our%2520method%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520and%2520provide%2520a%2520quantitative%2520and%2520qualitative%2520comparison%2520to%2520state-of-the-art%2520techniques.%2520For%2520video%2520results%2520and%2520interactive%2520demos%252C%2520see%2520https%253A//luxremix.github.io.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LuxRemix%3A%20Lighting%20Decomposition%20and%20Remixing%20for%20Indoor%20Scenes&entry.906535625=Ruofan%20Liang%20and%20Norman%20M%C3%BCller%20and%20Ethan%20Weber%20and%20Duncan%20Zauss%20and%20Nandita%20Vijaykumar%20and%20Peter%20Kontschieder%20and%20Christian%20Richardt&entry.1292438233=We%20present%20a%20novel%20approach%20for%20interactive%20light%20editing%20in%20indoor%20scenes%20from%20a%20single%20multi-view%20scene%20capture.%20Our%20method%20leverages%20a%20generative%20image-based%20light%20decomposition%20model%20that%20factorizes%20complex%20indoor%20scene%20illumination%20into%20its%20constituent%20light%20sources.%20This%20factorization%20enables%20independent%20manipulation%20of%20individual%20light%20sources%2C%20specifically%20allowing%20control%20over%20their%20state%20%28on/off%29%2C%20chromaticity%2C%20and%20intensity.%20We%20further%20introduce%20multi-view%20lighting%20harmonization%20to%20ensure%20consistent%20propagation%20of%20the%20lighting%20decomposition%20across%20all%20scene%20views.%20This%20is%20integrated%20into%20a%20relightable%203D%20Gaussian%20splatting%20representation%2C%20providing%20real-time%20interactive%20control%20over%20the%20individual%20light%20sources.%20Our%20results%20demonstrate%20highly%20photorealistic%20lighting%20decomposition%20and%20relighting%20outcomes%20across%20diverse%20indoor%20scenes.%20We%20evaluate%20our%20method%20on%20both%20synthetic%20and%20real-world%20datasets%20and%20provide%20a%20quantitative%20and%20qualitative%20comparison%20to%20state-of-the-art%20techniques.%20For%20video%20results%20and%20interactive%20demos%2C%20see%20https%3A//luxremix.github.io.&entry.1838667208=http%3A//arxiv.org/abs/2601.15283v1&entry.124074799=Read"},
{"title": "Improving Regret Approximation for Unsupervised Dynamic Environment Generation", "author": "Harry Mead and Bruno Lacerda and Jakob Foerster and Nick Hawes", "abstract": "Unsupervised Environment Design (UED) seeks to automatically generate training curricula for reinforcement learning (RL) agents, with the goal of improving generalisation and zero-shot performance. However, designing effective curricula remains a difficult problem, particularly in settings where small subsets of environment parameterisations result in significant increases in the complexity of the required policy. Current methods struggle with a difficult credit assignment problem and rely on regret approximations that fail to identify challenging levels, both of which are compounded as the size of the environment grows. We propose Dynamic Environment Generation for UED (DEGen) to enable a denser level generator reward signal, reducing the difficulty of credit assignment and allowing for UED to scale to larger environment sizes. We also introduce a new regret approximation, Maximised Negative Advantage (MNA), as a significantly improved metric to optimise for, that better identifies more challenging levels. We show empirically that MNA outperforms current regret approximations and when combined with DEGen, consistently outperforms existing methods, especially as the size of the environment grows. We have made all our code available here: https://github.com/HarryMJMead/Dynamic-Environment-Generation-for-UED.", "link": "http://arxiv.org/abs/2601.14957v1", "date": "2026-01-21", "relevancy": 2.7257, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5678}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5421}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Regret%20Approximation%20for%20Unsupervised%20Dynamic%20Environment%20Generation&body=Title%3A%20Improving%20Regret%20Approximation%20for%20Unsupervised%20Dynamic%20Environment%20Generation%0AAuthor%3A%20Harry%20Mead%20and%20Bruno%20Lacerda%20and%20Jakob%20Foerster%20and%20Nick%20Hawes%0AAbstract%3A%20Unsupervised%20Environment%20Design%20%28UED%29%20seeks%20to%20automatically%20generate%20training%20curricula%20for%20reinforcement%20learning%20%28RL%29%20agents%2C%20with%20the%20goal%20of%20improving%20generalisation%20and%20zero-shot%20performance.%20However%2C%20designing%20effective%20curricula%20remains%20a%20difficult%20problem%2C%20particularly%20in%20settings%20where%20small%20subsets%20of%20environment%20parameterisations%20result%20in%20significant%20increases%20in%20the%20complexity%20of%20the%20required%20policy.%20Current%20methods%20struggle%20with%20a%20difficult%20credit%20assignment%20problem%20and%20rely%20on%20regret%20approximations%20that%20fail%20to%20identify%20challenging%20levels%2C%20both%20of%20which%20are%20compounded%20as%20the%20size%20of%20the%20environment%20grows.%20We%20propose%20Dynamic%20Environment%20Generation%20for%20UED%20%28DEGen%29%20to%20enable%20a%20denser%20level%20generator%20reward%20signal%2C%20reducing%20the%20difficulty%20of%20credit%20assignment%20and%20allowing%20for%20UED%20to%20scale%20to%20larger%20environment%20sizes.%20We%20also%20introduce%20a%20new%20regret%20approximation%2C%20Maximised%20Negative%20Advantage%20%28MNA%29%2C%20as%20a%20significantly%20improved%20metric%20to%20optimise%20for%2C%20that%20better%20identifies%20more%20challenging%20levels.%20We%20show%20empirically%20that%20MNA%20outperforms%20current%20regret%20approximations%20and%20when%20combined%20with%20DEGen%2C%20consistently%20outperforms%20existing%20methods%2C%20especially%20as%20the%20size%20of%20the%20environment%20grows.%20We%20have%20made%20all%20our%20code%20available%20here%3A%20https%3A//github.com/HarryMJMead/Dynamic-Environment-Generation-for-UED.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Regret%2520Approximation%2520for%2520Unsupervised%2520Dynamic%2520Environment%2520Generation%26entry.906535625%3DHarry%2520Mead%2520and%2520Bruno%2520Lacerda%2520and%2520Jakob%2520Foerster%2520and%2520Nick%2520Hawes%26entry.1292438233%3DUnsupervised%2520Environment%2520Design%2520%2528UED%2529%2520seeks%2520to%2520automatically%2520generate%2520training%2520curricula%2520for%2520reinforcement%2520learning%2520%2528RL%2529%2520agents%252C%2520with%2520the%2520goal%2520of%2520improving%2520generalisation%2520and%2520zero-shot%2520performance.%2520However%252C%2520designing%2520effective%2520curricula%2520remains%2520a%2520difficult%2520problem%252C%2520particularly%2520in%2520settings%2520where%2520small%2520subsets%2520of%2520environment%2520parameterisations%2520result%2520in%2520significant%2520increases%2520in%2520the%2520complexity%2520of%2520the%2520required%2520policy.%2520Current%2520methods%2520struggle%2520with%2520a%2520difficult%2520credit%2520assignment%2520problem%2520and%2520rely%2520on%2520regret%2520approximations%2520that%2520fail%2520to%2520identify%2520challenging%2520levels%252C%2520both%2520of%2520which%2520are%2520compounded%2520as%2520the%2520size%2520of%2520the%2520environment%2520grows.%2520We%2520propose%2520Dynamic%2520Environment%2520Generation%2520for%2520UED%2520%2528DEGen%2529%2520to%2520enable%2520a%2520denser%2520level%2520generator%2520reward%2520signal%252C%2520reducing%2520the%2520difficulty%2520of%2520credit%2520assignment%2520and%2520allowing%2520for%2520UED%2520to%2520scale%2520to%2520larger%2520environment%2520sizes.%2520We%2520also%2520introduce%2520a%2520new%2520regret%2520approximation%252C%2520Maximised%2520Negative%2520Advantage%2520%2528MNA%2529%252C%2520as%2520a%2520significantly%2520improved%2520metric%2520to%2520optimise%2520for%252C%2520that%2520better%2520identifies%2520more%2520challenging%2520levels.%2520We%2520show%2520empirically%2520that%2520MNA%2520outperforms%2520current%2520regret%2520approximations%2520and%2520when%2520combined%2520with%2520DEGen%252C%2520consistently%2520outperforms%2520existing%2520methods%252C%2520especially%2520as%2520the%2520size%2520of%2520the%2520environment%2520grows.%2520We%2520have%2520made%2520all%2520our%2520code%2520available%2520here%253A%2520https%253A//github.com/HarryMJMead/Dynamic-Environment-Generation-for-UED.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Regret%20Approximation%20for%20Unsupervised%20Dynamic%20Environment%20Generation&entry.906535625=Harry%20Mead%20and%20Bruno%20Lacerda%20and%20Jakob%20Foerster%20and%20Nick%20Hawes&entry.1292438233=Unsupervised%20Environment%20Design%20%28UED%29%20seeks%20to%20automatically%20generate%20training%20curricula%20for%20reinforcement%20learning%20%28RL%29%20agents%2C%20with%20the%20goal%20of%20improving%20generalisation%20and%20zero-shot%20performance.%20However%2C%20designing%20effective%20curricula%20remains%20a%20difficult%20problem%2C%20particularly%20in%20settings%20where%20small%20subsets%20of%20environment%20parameterisations%20result%20in%20significant%20increases%20in%20the%20complexity%20of%20the%20required%20policy.%20Current%20methods%20struggle%20with%20a%20difficult%20credit%20assignment%20problem%20and%20rely%20on%20regret%20approximations%20that%20fail%20to%20identify%20challenging%20levels%2C%20both%20of%20which%20are%20compounded%20as%20the%20size%20of%20the%20environment%20grows.%20We%20propose%20Dynamic%20Environment%20Generation%20for%20UED%20%28DEGen%29%20to%20enable%20a%20denser%20level%20generator%20reward%20signal%2C%20reducing%20the%20difficulty%20of%20credit%20assignment%20and%20allowing%20for%20UED%20to%20scale%20to%20larger%20environment%20sizes.%20We%20also%20introduce%20a%20new%20regret%20approximation%2C%20Maximised%20Negative%20Advantage%20%28MNA%29%2C%20as%20a%20significantly%20improved%20metric%20to%20optimise%20for%2C%20that%20better%20identifies%20more%20challenging%20levels.%20We%20show%20empirically%20that%20MNA%20outperforms%20current%20regret%20approximations%20and%20when%20combined%20with%20DEGen%2C%20consistently%20outperforms%20existing%20methods%2C%20especially%20as%20the%20size%20of%20the%20environment%20grows.%20We%20have%20made%20all%20our%20code%20available%20here%3A%20https%3A//github.com/HarryMJMead/Dynamic-Environment-Generation-for-UED.&entry.1838667208=http%3A//arxiv.org/abs/2601.14957v1&entry.124074799=Read"},
{"title": "Intrinsic Dimension Estimating Autoencoder (IDEA) Using CancelOut Layer and a Projected Loss", "author": "Antoine Oriou and Philipp Krah and Julian Koellermeier", "abstract": "This paper introduces the Intrinsic Dimension Estimating Autoencoder (IDEA), which identifies the underlying intrinsic dimension of a wide range of datasets whose samples lie on either linear or nonlinear manifolds. Beyond estimating the intrinsic dimension, IDEA is also able to reconstruct the original dataset after projecting it onto the corresponding latent space, which is structured using re-weighted double CancelOut layers. Our key contribution is the introduction of the projected reconstruction loss term, guiding the training of the model by continuously assessing the reconstruction quality under the removal of an additional latent dimension. We first assess the performance of IDEA on a series of theoretical benchmarks to validate its robustness. These experiments allow us to test its reconstruction ability and compare its performance with state-of-the-art intrinsic dimension estimators. The benchmarks show good accuracy and high versatility of our approach. Subsequently, we apply our model to data generated from the numerical solution of a vertically resolved one-dimensional free-surface flow, following a pointwise discretization of the vertical velocity profile in the horizontal direction, vertical direction, and time. IDEA succeeds in estimating the dataset's intrinsic dimension and then reconstructs the original solution by working directly within the projection space identified by the network.", "link": "http://arxiv.org/abs/2509.10011v3", "date": "2026-01-21", "relevancy": 2.6929, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.618}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5049}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intrinsic%20Dimension%20Estimating%20Autoencoder%20%28IDEA%29%20Using%20CancelOut%20Layer%20and%20a%20Projected%20Loss&body=Title%3A%20Intrinsic%20Dimension%20Estimating%20Autoencoder%20%28IDEA%29%20Using%20CancelOut%20Layer%20and%20a%20Projected%20Loss%0AAuthor%3A%20Antoine%20Oriou%20and%20Philipp%20Krah%20and%20Julian%20Koellermeier%0AAbstract%3A%20This%20paper%20introduces%20the%20Intrinsic%20Dimension%20Estimating%20Autoencoder%20%28IDEA%29%2C%20which%20identifies%20the%20underlying%20intrinsic%20dimension%20of%20a%20wide%20range%20of%20datasets%20whose%20samples%20lie%20on%20either%20linear%20or%20nonlinear%20manifolds.%20Beyond%20estimating%20the%20intrinsic%20dimension%2C%20IDEA%20is%20also%20able%20to%20reconstruct%20the%20original%20dataset%20after%20projecting%20it%20onto%20the%20corresponding%20latent%20space%2C%20which%20is%20structured%20using%20re-weighted%20double%20CancelOut%20layers.%20Our%20key%20contribution%20is%20the%20introduction%20of%20the%20projected%20reconstruction%20loss%20term%2C%20guiding%20the%20training%20of%20the%20model%20by%20continuously%20assessing%20the%20reconstruction%20quality%20under%20the%20removal%20of%20an%20additional%20latent%20dimension.%20We%20first%20assess%20the%20performance%20of%20IDEA%20on%20a%20series%20of%20theoretical%20benchmarks%20to%20validate%20its%20robustness.%20These%20experiments%20allow%20us%20to%20test%20its%20reconstruction%20ability%20and%20compare%20its%20performance%20with%20state-of-the-art%20intrinsic%20dimension%20estimators.%20The%20benchmarks%20show%20good%20accuracy%20and%20high%20versatility%20of%20our%20approach.%20Subsequently%2C%20we%20apply%20our%20model%20to%20data%20generated%20from%20the%20numerical%20solution%20of%20a%20vertically%20resolved%20one-dimensional%20free-surface%20flow%2C%20following%20a%20pointwise%20discretization%20of%20the%20vertical%20velocity%20profile%20in%20the%20horizontal%20direction%2C%20vertical%20direction%2C%20and%20time.%20IDEA%20succeeds%20in%20estimating%20the%20dataset%27s%20intrinsic%20dimension%20and%20then%20reconstructs%20the%20original%20solution%20by%20working%20directly%20within%20the%20projection%20space%20identified%20by%20the%20network.%0ALink%3A%20http%3A//arxiv.org/abs/2509.10011v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntrinsic%2520Dimension%2520Estimating%2520Autoencoder%2520%2528IDEA%2529%2520Using%2520CancelOut%2520Layer%2520and%2520a%2520Projected%2520Loss%26entry.906535625%3DAntoine%2520Oriou%2520and%2520Philipp%2520Krah%2520and%2520Julian%2520Koellermeier%26entry.1292438233%3DThis%2520paper%2520introduces%2520the%2520Intrinsic%2520Dimension%2520Estimating%2520Autoencoder%2520%2528IDEA%2529%252C%2520which%2520identifies%2520the%2520underlying%2520intrinsic%2520dimension%2520of%2520a%2520wide%2520range%2520of%2520datasets%2520whose%2520samples%2520lie%2520on%2520either%2520linear%2520or%2520nonlinear%2520manifolds.%2520Beyond%2520estimating%2520the%2520intrinsic%2520dimension%252C%2520IDEA%2520is%2520also%2520able%2520to%2520reconstruct%2520the%2520original%2520dataset%2520after%2520projecting%2520it%2520onto%2520the%2520corresponding%2520latent%2520space%252C%2520which%2520is%2520structured%2520using%2520re-weighted%2520double%2520CancelOut%2520layers.%2520Our%2520key%2520contribution%2520is%2520the%2520introduction%2520of%2520the%2520projected%2520reconstruction%2520loss%2520term%252C%2520guiding%2520the%2520training%2520of%2520the%2520model%2520by%2520continuously%2520assessing%2520the%2520reconstruction%2520quality%2520under%2520the%2520removal%2520of%2520an%2520additional%2520latent%2520dimension.%2520We%2520first%2520assess%2520the%2520performance%2520of%2520IDEA%2520on%2520a%2520series%2520of%2520theoretical%2520benchmarks%2520to%2520validate%2520its%2520robustness.%2520These%2520experiments%2520allow%2520us%2520to%2520test%2520its%2520reconstruction%2520ability%2520and%2520compare%2520its%2520performance%2520with%2520state-of-the-art%2520intrinsic%2520dimension%2520estimators.%2520The%2520benchmarks%2520show%2520good%2520accuracy%2520and%2520high%2520versatility%2520of%2520our%2520approach.%2520Subsequently%252C%2520we%2520apply%2520our%2520model%2520to%2520data%2520generated%2520from%2520the%2520numerical%2520solution%2520of%2520a%2520vertically%2520resolved%2520one-dimensional%2520free-surface%2520flow%252C%2520following%2520a%2520pointwise%2520discretization%2520of%2520the%2520vertical%2520velocity%2520profile%2520in%2520the%2520horizontal%2520direction%252C%2520vertical%2520direction%252C%2520and%2520time.%2520IDEA%2520succeeds%2520in%2520estimating%2520the%2520dataset%2527s%2520intrinsic%2520dimension%2520and%2520then%2520reconstructs%2520the%2520original%2520solution%2520by%2520working%2520directly%2520within%2520the%2520projection%2520space%2520identified%2520by%2520the%2520network.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.10011v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intrinsic%20Dimension%20Estimating%20Autoencoder%20%28IDEA%29%20Using%20CancelOut%20Layer%20and%20a%20Projected%20Loss&entry.906535625=Antoine%20Oriou%20and%20Philipp%20Krah%20and%20Julian%20Koellermeier&entry.1292438233=This%20paper%20introduces%20the%20Intrinsic%20Dimension%20Estimating%20Autoencoder%20%28IDEA%29%2C%20which%20identifies%20the%20underlying%20intrinsic%20dimension%20of%20a%20wide%20range%20of%20datasets%20whose%20samples%20lie%20on%20either%20linear%20or%20nonlinear%20manifolds.%20Beyond%20estimating%20the%20intrinsic%20dimension%2C%20IDEA%20is%20also%20able%20to%20reconstruct%20the%20original%20dataset%20after%20projecting%20it%20onto%20the%20corresponding%20latent%20space%2C%20which%20is%20structured%20using%20re-weighted%20double%20CancelOut%20layers.%20Our%20key%20contribution%20is%20the%20introduction%20of%20the%20projected%20reconstruction%20loss%20term%2C%20guiding%20the%20training%20of%20the%20model%20by%20continuously%20assessing%20the%20reconstruction%20quality%20under%20the%20removal%20of%20an%20additional%20latent%20dimension.%20We%20first%20assess%20the%20performance%20of%20IDEA%20on%20a%20series%20of%20theoretical%20benchmarks%20to%20validate%20its%20robustness.%20These%20experiments%20allow%20us%20to%20test%20its%20reconstruction%20ability%20and%20compare%20its%20performance%20with%20state-of-the-art%20intrinsic%20dimension%20estimators.%20The%20benchmarks%20show%20good%20accuracy%20and%20high%20versatility%20of%20our%20approach.%20Subsequently%2C%20we%20apply%20our%20model%20to%20data%20generated%20from%20the%20numerical%20solution%20of%20a%20vertically%20resolved%20one-dimensional%20free-surface%20flow%2C%20following%20a%20pointwise%20discretization%20of%20the%20vertical%20velocity%20profile%20in%20the%20horizontal%20direction%2C%20vertical%20direction%2C%20and%20time.%20IDEA%20succeeds%20in%20estimating%20the%20dataset%27s%20intrinsic%20dimension%20and%20then%20reconstructs%20the%20original%20solution%20by%20working%20directly%20within%20the%20projection%20space%20identified%20by%20the%20network.&entry.1838667208=http%3A//arxiv.org/abs/2509.10011v3&entry.124074799=Read"},
{"title": "Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?", "author": "Yihao Li and Saeed Salehi and Lyle Ungar and Konrad P. Kording", "abstract": "Object binding, the brain's ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decode IsSameObject from patch embeddings across ViT layers using a quadratic similarity probe, which reaches over 90% accuracy. Crucially, this object-binding capability emerges reliably in DINO, CLIP, and ImageNet-supervised ViTs, but is markedly weaker in MAE, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that IsSameObject is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating IsSameObject from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of \"which parts belong together\" emerges naturally in a connectionist system.", "link": "http://arxiv.org/abs/2510.24709v2", "date": "2026-01-21", "relevancy": 2.6736, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5411}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5411}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20Object%20Binding%20Naturally%20Emerge%20in%20Large%20Pretrained%20Vision%20Transformers%3F&body=Title%3A%20Does%20Object%20Binding%20Naturally%20Emerge%20in%20Large%20Pretrained%20Vision%20Transformers%3F%0AAuthor%3A%20Yihao%20Li%20and%20Saeed%20Salehi%20and%20Lyle%20Ungar%20and%20Konrad%20P.%20Kording%0AAbstract%3A%20Object%20binding%2C%20the%20brain%27s%20ability%20to%20bind%20the%20many%20features%20that%20collectively%20represent%20an%20object%20into%20a%20coherent%20whole%2C%20is%20central%20to%20human%20cognition.%20It%20groups%20low-level%20perceptual%20features%20into%20high-level%20object%20representations%2C%20stores%20those%20objects%20efficiently%20and%20compositionally%20in%20memory%2C%20and%20supports%20human%20reasoning%20about%20individual%20object%20instances.%20While%20prior%20work%20often%20imposes%20object-centric%20attention%20%28e.g.%2C%20Slot%20Attention%29%20explicitly%20to%20probe%20these%20benefits%2C%20it%20remains%20unclear%20whether%20this%20ability%20naturally%20emerges%20in%20pre-trained%20Vision%20Transformers%20%28ViTs%29.%20Intuitively%2C%20they%20could%3A%20recognizing%20which%20patches%20belong%20to%20the%20same%20object%20should%20be%20useful%20for%20downstream%20prediction%20and%20thus%20guide%20attention.%20Motivated%20by%20the%20quadratic%20nature%20of%20self-attention%2C%20we%20hypothesize%20that%20ViTs%20represent%20whether%20two%20patches%20belong%20to%20the%20same%20object%2C%20a%20property%20we%20term%20IsSameObject.%20We%20decode%20IsSameObject%20from%20patch%20embeddings%20across%20ViT%20layers%20using%20a%20quadratic%20similarity%20probe%2C%20which%20reaches%20over%2090%25%20accuracy.%20Crucially%2C%20this%20object-binding%20capability%20emerges%20reliably%20in%20DINO%2C%20CLIP%2C%20and%20ImageNet-supervised%20ViTs%2C%20but%20is%20markedly%20weaker%20in%20MAE%2C%20suggesting%20that%20binding%20is%20not%20a%20trivial%20architectural%20artifact%2C%20but%20an%20ability%20acquired%20through%20specific%20pretraining%20objectives.%20We%20further%20discover%20that%20IsSameObject%20is%20encoded%20in%20a%20low-dimensional%20subspace%20on%20top%20of%20object%20features%2C%20and%20that%20this%20signal%20actively%20guides%20attention.%20Ablating%20IsSameObject%20from%20model%20activations%20degrades%20downstream%20performance%20and%20works%20against%20the%20learning%20objective%2C%20implying%20that%20emergent%20object%20binding%20naturally%20serves%20the%20pretraining%20objective.%20Our%20findings%20challenge%20the%20view%20that%20ViTs%20lack%20object%20binding%20and%20highlight%20how%20symbolic%20knowledge%20of%20%22which%20parts%20belong%20together%22%20emerges%20naturally%20in%20a%20connectionist%20system.%0ALink%3A%20http%3A//arxiv.org/abs/2510.24709v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520Object%2520Binding%2520Naturally%2520Emerge%2520in%2520Large%2520Pretrained%2520Vision%2520Transformers%253F%26entry.906535625%3DYihao%2520Li%2520and%2520Saeed%2520Salehi%2520and%2520Lyle%2520Ungar%2520and%2520Konrad%2520P.%2520Kording%26entry.1292438233%3DObject%2520binding%252C%2520the%2520brain%2527s%2520ability%2520to%2520bind%2520the%2520many%2520features%2520that%2520collectively%2520represent%2520an%2520object%2520into%2520a%2520coherent%2520whole%252C%2520is%2520central%2520to%2520human%2520cognition.%2520It%2520groups%2520low-level%2520perceptual%2520features%2520into%2520high-level%2520object%2520representations%252C%2520stores%2520those%2520objects%2520efficiently%2520and%2520compositionally%2520in%2520memory%252C%2520and%2520supports%2520human%2520reasoning%2520about%2520individual%2520object%2520instances.%2520While%2520prior%2520work%2520often%2520imposes%2520object-centric%2520attention%2520%2528e.g.%252C%2520Slot%2520Attention%2529%2520explicitly%2520to%2520probe%2520these%2520benefits%252C%2520it%2520remains%2520unclear%2520whether%2520this%2520ability%2520naturally%2520emerges%2520in%2520pre-trained%2520Vision%2520Transformers%2520%2528ViTs%2529.%2520Intuitively%252C%2520they%2520could%253A%2520recognizing%2520which%2520patches%2520belong%2520to%2520the%2520same%2520object%2520should%2520be%2520useful%2520for%2520downstream%2520prediction%2520and%2520thus%2520guide%2520attention.%2520Motivated%2520by%2520the%2520quadratic%2520nature%2520of%2520self-attention%252C%2520we%2520hypothesize%2520that%2520ViTs%2520represent%2520whether%2520two%2520patches%2520belong%2520to%2520the%2520same%2520object%252C%2520a%2520property%2520we%2520term%2520IsSameObject.%2520We%2520decode%2520IsSameObject%2520from%2520patch%2520embeddings%2520across%2520ViT%2520layers%2520using%2520a%2520quadratic%2520similarity%2520probe%252C%2520which%2520reaches%2520over%252090%2525%2520accuracy.%2520Crucially%252C%2520this%2520object-binding%2520capability%2520emerges%2520reliably%2520in%2520DINO%252C%2520CLIP%252C%2520and%2520ImageNet-supervised%2520ViTs%252C%2520but%2520is%2520markedly%2520weaker%2520in%2520MAE%252C%2520suggesting%2520that%2520binding%2520is%2520not%2520a%2520trivial%2520architectural%2520artifact%252C%2520but%2520an%2520ability%2520acquired%2520through%2520specific%2520pretraining%2520objectives.%2520We%2520further%2520discover%2520that%2520IsSameObject%2520is%2520encoded%2520in%2520a%2520low-dimensional%2520subspace%2520on%2520top%2520of%2520object%2520features%252C%2520and%2520that%2520this%2520signal%2520actively%2520guides%2520attention.%2520Ablating%2520IsSameObject%2520from%2520model%2520activations%2520degrades%2520downstream%2520performance%2520and%2520works%2520against%2520the%2520learning%2520objective%252C%2520implying%2520that%2520emergent%2520object%2520binding%2520naturally%2520serves%2520the%2520pretraining%2520objective.%2520Our%2520findings%2520challenge%2520the%2520view%2520that%2520ViTs%2520lack%2520object%2520binding%2520and%2520highlight%2520how%2520symbolic%2520knowledge%2520of%2520%2522which%2520parts%2520belong%2520together%2522%2520emerges%2520naturally%2520in%2520a%2520connectionist%2520system.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.24709v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20Object%20Binding%20Naturally%20Emerge%20in%20Large%20Pretrained%20Vision%20Transformers%3F&entry.906535625=Yihao%20Li%20and%20Saeed%20Salehi%20and%20Lyle%20Ungar%20and%20Konrad%20P.%20Kording&entry.1292438233=Object%20binding%2C%20the%20brain%27s%20ability%20to%20bind%20the%20many%20features%20that%20collectively%20represent%20an%20object%20into%20a%20coherent%20whole%2C%20is%20central%20to%20human%20cognition.%20It%20groups%20low-level%20perceptual%20features%20into%20high-level%20object%20representations%2C%20stores%20those%20objects%20efficiently%20and%20compositionally%20in%20memory%2C%20and%20supports%20human%20reasoning%20about%20individual%20object%20instances.%20While%20prior%20work%20often%20imposes%20object-centric%20attention%20%28e.g.%2C%20Slot%20Attention%29%20explicitly%20to%20probe%20these%20benefits%2C%20it%20remains%20unclear%20whether%20this%20ability%20naturally%20emerges%20in%20pre-trained%20Vision%20Transformers%20%28ViTs%29.%20Intuitively%2C%20they%20could%3A%20recognizing%20which%20patches%20belong%20to%20the%20same%20object%20should%20be%20useful%20for%20downstream%20prediction%20and%20thus%20guide%20attention.%20Motivated%20by%20the%20quadratic%20nature%20of%20self-attention%2C%20we%20hypothesize%20that%20ViTs%20represent%20whether%20two%20patches%20belong%20to%20the%20same%20object%2C%20a%20property%20we%20term%20IsSameObject.%20We%20decode%20IsSameObject%20from%20patch%20embeddings%20across%20ViT%20layers%20using%20a%20quadratic%20similarity%20probe%2C%20which%20reaches%20over%2090%25%20accuracy.%20Crucially%2C%20this%20object-binding%20capability%20emerges%20reliably%20in%20DINO%2C%20CLIP%2C%20and%20ImageNet-supervised%20ViTs%2C%20but%20is%20markedly%20weaker%20in%20MAE%2C%20suggesting%20that%20binding%20is%20not%20a%20trivial%20architectural%20artifact%2C%20but%20an%20ability%20acquired%20through%20specific%20pretraining%20objectives.%20We%20further%20discover%20that%20IsSameObject%20is%20encoded%20in%20a%20low-dimensional%20subspace%20on%20top%20of%20object%20features%2C%20and%20that%20this%20signal%20actively%20guides%20attention.%20Ablating%20IsSameObject%20from%20model%20activations%20degrades%20downstream%20performance%20and%20works%20against%20the%20learning%20objective%2C%20implying%20that%20emergent%20object%20binding%20naturally%20serves%20the%20pretraining%20objective.%20Our%20findings%20challenge%20the%20view%20that%20ViTs%20lack%20object%20binding%20and%20highlight%20how%20symbolic%20knowledge%20of%20%22which%20parts%20belong%20together%22%20emerges%20naturally%20in%20a%20connectionist%20system.&entry.1838667208=http%3A//arxiv.org/abs/2510.24709v2&entry.124074799=Read"},
{"title": "GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for Remote Sensing Image Analysis", "author": "Angelos Zavras and Dimitrios Michail and Xiao Xiang Zhu and Beg\u00fcm Demir and Ioannis Papoutsis", "abstract": "Existing Vision-Language Models (VLMs) are predominantly trained on web-scraped, noisy image-text data, exhibiting limited exposure to the specialized domain of RS. This deficiency results in poor performance on RS-specific tasks, as commonly used datasets often lack detailed, scientifically accurate textual descriptions and instead emphasize solely on attributes like date and location. To bridge this critical gap, we introduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and multi-modal RS image analysis. GAIA comprises of 201,005 meticulously curated RS image-text pairs, representing a diverse range of RS modalities associated to different spatial resolutions. Unlike existing vision-language datasets in RS, GAIA specifically focuses on capturing a diverse range of RS applications, providing unique information about environmental changes, natural disasters, and various other dynamic phenomena. The dataset provides a spatially and temporally balanced distribution, spanning across the globe, covering the last 25 years with a balanced temporal distribution of observations. GAIA's construction involved a two-stage process: (1) targeted web-scraping of images and accompanying text from reputable RS-related sources, and (2) generation of five high-quality, scientifically grounded synthetic captions for each image using carefully crafted prompts that leverage the advanced vision-language capabilities of GPT-4o. Our extensive experiments, including fine-tuning of CLIP and BLIP2 models, demonstrate that GAIA significantly improves performance on RS image classification, cross-modal retrieval and image captioning tasks. We make our dataset, automated processing framework and fine-tuned model weights publicly available on our project's GitHub repository: https://github.com/Orion-AI-Lab/GAIA.", "link": "http://arxiv.org/abs/2502.09598v2", "date": "2026-01-21", "relevancy": 2.673, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5221}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAIA%3A%20A%20Global%2C%20Multi-modal%2C%20Multi-scale%20Vision-Language%20Dataset%20for%20Remote%20Sensing%20Image%20Analysis&body=Title%3A%20GAIA%3A%20A%20Global%2C%20Multi-modal%2C%20Multi-scale%20Vision-Language%20Dataset%20for%20Remote%20Sensing%20Image%20Analysis%0AAuthor%3A%20Angelos%20Zavras%20and%20Dimitrios%20Michail%20and%20Xiao%20Xiang%20Zhu%20and%20Beg%C3%BCm%20Demir%20and%20Ioannis%20Papoutsis%0AAbstract%3A%20Existing%20Vision-Language%20Models%20%28VLMs%29%20are%20predominantly%20trained%20on%20web-scraped%2C%20noisy%20image-text%20data%2C%20exhibiting%20limited%20exposure%20to%20the%20specialized%20domain%20of%20RS.%20This%20deficiency%20results%20in%20poor%20performance%20on%20RS-specific%20tasks%2C%20as%20commonly%20used%20datasets%20often%20lack%20detailed%2C%20scientifically%20accurate%20textual%20descriptions%20and%20instead%20emphasize%20solely%20on%20attributes%20like%20date%20and%20location.%20To%20bridge%20this%20critical%20gap%2C%20we%20introduce%20GAIA%2C%20a%20novel%20dataset%20designed%20for%20multi-scale%2C%20multi-sensor%2C%20and%20multi-modal%20RS%20image%20analysis.%20GAIA%20comprises%20of%20201%2C005%20meticulously%20curated%20RS%20image-text%20pairs%2C%20representing%20a%20diverse%20range%20of%20RS%20modalities%20associated%20to%20different%20spatial%20resolutions.%20Unlike%20existing%20vision-language%20datasets%20in%20RS%2C%20GAIA%20specifically%20focuses%20on%20capturing%20a%20diverse%20range%20of%20RS%20applications%2C%20providing%20unique%20information%20about%20environmental%20changes%2C%20natural%20disasters%2C%20and%20various%20other%20dynamic%20phenomena.%20The%20dataset%20provides%20a%20spatially%20and%20temporally%20balanced%20distribution%2C%20spanning%20across%20the%20globe%2C%20covering%20the%20last%2025%20years%20with%20a%20balanced%20temporal%20distribution%20of%20observations.%20GAIA%27s%20construction%20involved%20a%20two-stage%20process%3A%20%281%29%20targeted%20web-scraping%20of%20images%20and%20accompanying%20text%20from%20reputable%20RS-related%20sources%2C%20and%20%282%29%20generation%20of%20five%20high-quality%2C%20scientifically%20grounded%20synthetic%20captions%20for%20each%20image%20using%20carefully%20crafted%20prompts%20that%20leverage%20the%20advanced%20vision-language%20capabilities%20of%20GPT-4o.%20Our%20extensive%20experiments%2C%20including%20fine-tuning%20of%20CLIP%20and%20BLIP2%20models%2C%20demonstrate%20that%20GAIA%20significantly%20improves%20performance%20on%20RS%20image%20classification%2C%20cross-modal%20retrieval%20and%20image%20captioning%20tasks.%20We%20make%20our%20dataset%2C%20automated%20processing%20framework%20and%20fine-tuned%20model%20weights%20publicly%20available%20on%20our%20project%27s%20GitHub%20repository%3A%20https%3A//github.com/Orion-AI-Lab/GAIA.%0ALink%3A%20http%3A//arxiv.org/abs/2502.09598v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAIA%253A%2520A%2520Global%252C%2520Multi-modal%252C%2520Multi-scale%2520Vision-Language%2520Dataset%2520for%2520Remote%2520Sensing%2520Image%2520Analysis%26entry.906535625%3DAngelos%2520Zavras%2520and%2520Dimitrios%2520Michail%2520and%2520Xiao%2520Xiang%2520Zhu%2520and%2520Beg%25C3%25BCm%2520Demir%2520and%2520Ioannis%2520Papoutsis%26entry.1292438233%3DExisting%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520are%2520predominantly%2520trained%2520on%2520web-scraped%252C%2520noisy%2520image-text%2520data%252C%2520exhibiting%2520limited%2520exposure%2520to%2520the%2520specialized%2520domain%2520of%2520RS.%2520This%2520deficiency%2520results%2520in%2520poor%2520performance%2520on%2520RS-specific%2520tasks%252C%2520as%2520commonly%2520used%2520datasets%2520often%2520lack%2520detailed%252C%2520scientifically%2520accurate%2520textual%2520descriptions%2520and%2520instead%2520emphasize%2520solely%2520on%2520attributes%2520like%2520date%2520and%2520location.%2520To%2520bridge%2520this%2520critical%2520gap%252C%2520we%2520introduce%2520GAIA%252C%2520a%2520novel%2520dataset%2520designed%2520for%2520multi-scale%252C%2520multi-sensor%252C%2520and%2520multi-modal%2520RS%2520image%2520analysis.%2520GAIA%2520comprises%2520of%2520201%252C005%2520meticulously%2520curated%2520RS%2520image-text%2520pairs%252C%2520representing%2520a%2520diverse%2520range%2520of%2520RS%2520modalities%2520associated%2520to%2520different%2520spatial%2520resolutions.%2520Unlike%2520existing%2520vision-language%2520datasets%2520in%2520RS%252C%2520GAIA%2520specifically%2520focuses%2520on%2520capturing%2520a%2520diverse%2520range%2520of%2520RS%2520applications%252C%2520providing%2520unique%2520information%2520about%2520environmental%2520changes%252C%2520natural%2520disasters%252C%2520and%2520various%2520other%2520dynamic%2520phenomena.%2520The%2520dataset%2520provides%2520a%2520spatially%2520and%2520temporally%2520balanced%2520distribution%252C%2520spanning%2520across%2520the%2520globe%252C%2520covering%2520the%2520last%252025%2520years%2520with%2520a%2520balanced%2520temporal%2520distribution%2520of%2520observations.%2520GAIA%2527s%2520construction%2520involved%2520a%2520two-stage%2520process%253A%2520%25281%2529%2520targeted%2520web-scraping%2520of%2520images%2520and%2520accompanying%2520text%2520from%2520reputable%2520RS-related%2520sources%252C%2520and%2520%25282%2529%2520generation%2520of%2520five%2520high-quality%252C%2520scientifically%2520grounded%2520synthetic%2520captions%2520for%2520each%2520image%2520using%2520carefully%2520crafted%2520prompts%2520that%2520leverage%2520the%2520advanced%2520vision-language%2520capabilities%2520of%2520GPT-4o.%2520Our%2520extensive%2520experiments%252C%2520including%2520fine-tuning%2520of%2520CLIP%2520and%2520BLIP2%2520models%252C%2520demonstrate%2520that%2520GAIA%2520significantly%2520improves%2520performance%2520on%2520RS%2520image%2520classification%252C%2520cross-modal%2520retrieval%2520and%2520image%2520captioning%2520tasks.%2520We%2520make%2520our%2520dataset%252C%2520automated%2520processing%2520framework%2520and%2520fine-tuned%2520model%2520weights%2520publicly%2520available%2520on%2520our%2520project%2527s%2520GitHub%2520repository%253A%2520https%253A//github.com/Orion-AI-Lab/GAIA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09598v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAIA%3A%20A%20Global%2C%20Multi-modal%2C%20Multi-scale%20Vision-Language%20Dataset%20for%20Remote%20Sensing%20Image%20Analysis&entry.906535625=Angelos%20Zavras%20and%20Dimitrios%20Michail%20and%20Xiao%20Xiang%20Zhu%20and%20Beg%C3%BCm%20Demir%20and%20Ioannis%20Papoutsis&entry.1292438233=Existing%20Vision-Language%20Models%20%28VLMs%29%20are%20predominantly%20trained%20on%20web-scraped%2C%20noisy%20image-text%20data%2C%20exhibiting%20limited%20exposure%20to%20the%20specialized%20domain%20of%20RS.%20This%20deficiency%20results%20in%20poor%20performance%20on%20RS-specific%20tasks%2C%20as%20commonly%20used%20datasets%20often%20lack%20detailed%2C%20scientifically%20accurate%20textual%20descriptions%20and%20instead%20emphasize%20solely%20on%20attributes%20like%20date%20and%20location.%20To%20bridge%20this%20critical%20gap%2C%20we%20introduce%20GAIA%2C%20a%20novel%20dataset%20designed%20for%20multi-scale%2C%20multi-sensor%2C%20and%20multi-modal%20RS%20image%20analysis.%20GAIA%20comprises%20of%20201%2C005%20meticulously%20curated%20RS%20image-text%20pairs%2C%20representing%20a%20diverse%20range%20of%20RS%20modalities%20associated%20to%20different%20spatial%20resolutions.%20Unlike%20existing%20vision-language%20datasets%20in%20RS%2C%20GAIA%20specifically%20focuses%20on%20capturing%20a%20diverse%20range%20of%20RS%20applications%2C%20providing%20unique%20information%20about%20environmental%20changes%2C%20natural%20disasters%2C%20and%20various%20other%20dynamic%20phenomena.%20The%20dataset%20provides%20a%20spatially%20and%20temporally%20balanced%20distribution%2C%20spanning%20across%20the%20globe%2C%20covering%20the%20last%2025%20years%20with%20a%20balanced%20temporal%20distribution%20of%20observations.%20GAIA%27s%20construction%20involved%20a%20two-stage%20process%3A%20%281%29%20targeted%20web-scraping%20of%20images%20and%20accompanying%20text%20from%20reputable%20RS-related%20sources%2C%20and%20%282%29%20generation%20of%20five%20high-quality%2C%20scientifically%20grounded%20synthetic%20captions%20for%20each%20image%20using%20carefully%20crafted%20prompts%20that%20leverage%20the%20advanced%20vision-language%20capabilities%20of%20GPT-4o.%20Our%20extensive%20experiments%2C%20including%20fine-tuning%20of%20CLIP%20and%20BLIP2%20models%2C%20demonstrate%20that%20GAIA%20significantly%20improves%20performance%20on%20RS%20image%20classification%2C%20cross-modal%20retrieval%20and%20image%20captioning%20tasks.%20We%20make%20our%20dataset%2C%20automated%20processing%20framework%20and%20fine-tuned%20model%20weights%20publicly%20available%20on%20our%20project%27s%20GitHub%20repository%3A%20https%3A//github.com/Orion-AI-Lab/GAIA.&entry.1838667208=http%3A//arxiv.org/abs/2502.09598v2&entry.124074799=Read"},
{"title": "TempViz: On the Evaluation of Temporal Knowledge in Text-to-Image Models", "author": "Carolin Holtermann and Nina Krebs and Anne Lauscher", "abstract": "Time alters the visual appearance of entities in our world, like objects, places, and animals. Thus, for accurately generating contextually-relevant images, knowledge and reasoning about time can be crucial (e.g., for generating a landscape in spring vs. in winter). Yet, although substantial work exists on understanding and improving temporal knowledge in natural language processing, research on how temporal phenomena appear and are handled in text-to-image (T2I) models remains scarce. We address this gap with TempViz, the first data set to holistically evaluate temporal knowledge in image generation, consisting of 7.9k prompts and more than 600 reference images. Using TempViz, we study the capabilities of five T2I models across five temporal knowledge categories. Human evaluation shows that temporal competence is generally weak, with no model exceeding 75% accuracy across categories. Towards larger-scale studies, we also examine automated evaluation methods, comparing several established approaches against human judgments. However, none of these approaches provides a reliable assessment of temporal cues - further indicating the pressing need for future research on temporal knowledge in T2I.", "link": "http://arxiv.org/abs/2601.14951v1", "date": "2026-01-21", "relevancy": 2.6671, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5421}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5389}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TempViz%3A%20On%20the%20Evaluation%20of%20Temporal%20Knowledge%20in%20Text-to-Image%20Models&body=Title%3A%20TempViz%3A%20On%20the%20Evaluation%20of%20Temporal%20Knowledge%20in%20Text-to-Image%20Models%0AAuthor%3A%20Carolin%20Holtermann%20and%20Nina%20Krebs%20and%20Anne%20Lauscher%0AAbstract%3A%20Time%20alters%20the%20visual%20appearance%20of%20entities%20in%20our%20world%2C%20like%20objects%2C%20places%2C%20and%20animals.%20Thus%2C%20for%20accurately%20generating%20contextually-relevant%20images%2C%20knowledge%20and%20reasoning%20about%20time%20can%20be%20crucial%20%28e.g.%2C%20for%20generating%20a%20landscape%20in%20spring%20vs.%20in%20winter%29.%20Yet%2C%20although%20substantial%20work%20exists%20on%20understanding%20and%20improving%20temporal%20knowledge%20in%20natural%20language%20processing%2C%20research%20on%20how%20temporal%20phenomena%20appear%20and%20are%20handled%20in%20text-to-image%20%28T2I%29%20models%20remains%20scarce.%20We%20address%20this%20gap%20with%20TempViz%2C%20the%20first%20data%20set%20to%20holistically%20evaluate%20temporal%20knowledge%20in%20image%20generation%2C%20consisting%20of%207.9k%20prompts%20and%20more%20than%20600%20reference%20images.%20Using%20TempViz%2C%20we%20study%20the%20capabilities%20of%20five%20T2I%20models%20across%20five%20temporal%20knowledge%20categories.%20Human%20evaluation%20shows%20that%20temporal%20competence%20is%20generally%20weak%2C%20with%20no%20model%20exceeding%2075%25%20accuracy%20across%20categories.%20Towards%20larger-scale%20studies%2C%20we%20also%20examine%20automated%20evaluation%20methods%2C%20comparing%20several%20established%20approaches%20against%20human%20judgments.%20However%2C%20none%20of%20these%20approaches%20provides%20a%20reliable%20assessment%20of%20temporal%20cues%20-%20further%20indicating%20the%20pressing%20need%20for%20future%20research%20on%20temporal%20knowledge%20in%20T2I.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTempViz%253A%2520On%2520the%2520Evaluation%2520of%2520Temporal%2520Knowledge%2520in%2520Text-to-Image%2520Models%26entry.906535625%3DCarolin%2520Holtermann%2520and%2520Nina%2520Krebs%2520and%2520Anne%2520Lauscher%26entry.1292438233%3DTime%2520alters%2520the%2520visual%2520appearance%2520of%2520entities%2520in%2520our%2520world%252C%2520like%2520objects%252C%2520places%252C%2520and%2520animals.%2520Thus%252C%2520for%2520accurately%2520generating%2520contextually-relevant%2520images%252C%2520knowledge%2520and%2520reasoning%2520about%2520time%2520can%2520be%2520crucial%2520%2528e.g.%252C%2520for%2520generating%2520a%2520landscape%2520in%2520spring%2520vs.%2520in%2520winter%2529.%2520Yet%252C%2520although%2520substantial%2520work%2520exists%2520on%2520understanding%2520and%2520improving%2520temporal%2520knowledge%2520in%2520natural%2520language%2520processing%252C%2520research%2520on%2520how%2520temporal%2520phenomena%2520appear%2520and%2520are%2520handled%2520in%2520text-to-image%2520%2528T2I%2529%2520models%2520remains%2520scarce.%2520We%2520address%2520this%2520gap%2520with%2520TempViz%252C%2520the%2520first%2520data%2520set%2520to%2520holistically%2520evaluate%2520temporal%2520knowledge%2520in%2520image%2520generation%252C%2520consisting%2520of%25207.9k%2520prompts%2520and%2520more%2520than%2520600%2520reference%2520images.%2520Using%2520TempViz%252C%2520we%2520study%2520the%2520capabilities%2520of%2520five%2520T2I%2520models%2520across%2520five%2520temporal%2520knowledge%2520categories.%2520Human%2520evaluation%2520shows%2520that%2520temporal%2520competence%2520is%2520generally%2520weak%252C%2520with%2520no%2520model%2520exceeding%252075%2525%2520accuracy%2520across%2520categories.%2520Towards%2520larger-scale%2520studies%252C%2520we%2520also%2520examine%2520automated%2520evaluation%2520methods%252C%2520comparing%2520several%2520established%2520approaches%2520against%2520human%2520judgments.%2520However%252C%2520none%2520of%2520these%2520approaches%2520provides%2520a%2520reliable%2520assessment%2520of%2520temporal%2520cues%2520-%2520further%2520indicating%2520the%2520pressing%2520need%2520for%2520future%2520research%2520on%2520temporal%2520knowledge%2520in%2520T2I.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TempViz%3A%20On%20the%20Evaluation%20of%20Temporal%20Knowledge%20in%20Text-to-Image%20Models&entry.906535625=Carolin%20Holtermann%20and%20Nina%20Krebs%20and%20Anne%20Lauscher&entry.1292438233=Time%20alters%20the%20visual%20appearance%20of%20entities%20in%20our%20world%2C%20like%20objects%2C%20places%2C%20and%20animals.%20Thus%2C%20for%20accurately%20generating%20contextually-relevant%20images%2C%20knowledge%20and%20reasoning%20about%20time%20can%20be%20crucial%20%28e.g.%2C%20for%20generating%20a%20landscape%20in%20spring%20vs.%20in%20winter%29.%20Yet%2C%20although%20substantial%20work%20exists%20on%20understanding%20and%20improving%20temporal%20knowledge%20in%20natural%20language%20processing%2C%20research%20on%20how%20temporal%20phenomena%20appear%20and%20are%20handled%20in%20text-to-image%20%28T2I%29%20models%20remains%20scarce.%20We%20address%20this%20gap%20with%20TempViz%2C%20the%20first%20data%20set%20to%20holistically%20evaluate%20temporal%20knowledge%20in%20image%20generation%2C%20consisting%20of%207.9k%20prompts%20and%20more%20than%20600%20reference%20images.%20Using%20TempViz%2C%20we%20study%20the%20capabilities%20of%20five%20T2I%20models%20across%20five%20temporal%20knowledge%20categories.%20Human%20evaluation%20shows%20that%20temporal%20competence%20is%20generally%20weak%2C%20with%20no%20model%20exceeding%2075%25%20accuracy%20across%20categories.%20Towards%20larger-scale%20studies%2C%20we%20also%20examine%20automated%20evaluation%20methods%2C%20comparing%20several%20established%20approaches%20against%20human%20judgments.%20However%2C%20none%20of%20these%20approaches%20provides%20a%20reliable%20assessment%20of%20temporal%20cues%20-%20further%20indicating%20the%20pressing%20need%20for%20future%20research%20on%20temporal%20knowledge%20in%20T2I.&entry.1838667208=http%3A//arxiv.org/abs/2601.14951v1&entry.124074799=Read"},
{"title": "Tracing 3D Anatomy in 2D Strokes: A Multi-Stage Projection Driven Approach to Cervical Spine Fracture Identification", "author": "Fabi Nahian Madhurja and Rusab Sarmun and Muhammad E. H. Chowdhury and Adam Mushtak and Israa Al-Hashimi and Sohaib Bassam Zoghoul", "abstract": "Cervical spine fractures are critical medical conditions requiring precise and efficient detection for effective clinical management. This study explores the viability of 2D projection-based vertebra segmentation for vertebra-level fracture detection in 3D CT volumes, presenting an end-to-end pipeline for automated analysis of cervical vertebrae (C1-C7). By approximating a 3D volume through optimized 2D axial, sagittal, and coronal projections, regions of interest are identified using the YOLOv8 model from all views and combined to approximate the 3D cervical spine area, achieving a 3D mIoU of 94.45 percent. This projection-based localization strategy reduces computational complexity compared to traditional 3D segmentation methods while maintaining high performance. It is followed by a DenseNet121-Unet-based multi-label segmentation leveraging variance- and energy-based projections, achieving a Dice score of 87.86 percent. Strategic approximation of 3D vertebral masks from these 2D segmentation masks enables the extraction of individual vertebra volumes. The volumes are analyzed for fractures using an ensemble of 2.5D Spatio-Sequential models incorporating both raw slices and projections per vertebra for complementary evaluation. This ensemble achieves vertebra-level and patient-level F1 scores of 68.15 and 82.26, and ROC-AUC scores of 91.62 and 83.04, respectively. We further validate our approach through an explainability study that provides saliency map visualizations highlighting anatomical regions relevant for diagnosis, and an interobserver variability analysis comparing our model's performance with expert radiologists, demonstrating competitive results.", "link": "http://arxiv.org/abs/2601.15235v1", "date": "2026-01-21", "relevancy": 2.6297, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.53}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tracing%203D%20Anatomy%20in%202D%20Strokes%3A%20A%20Multi-Stage%20Projection%20Driven%20Approach%20to%20Cervical%20Spine%20Fracture%20Identification&body=Title%3A%20Tracing%203D%20Anatomy%20in%202D%20Strokes%3A%20A%20Multi-Stage%20Projection%20Driven%20Approach%20to%20Cervical%20Spine%20Fracture%20Identification%0AAuthor%3A%20Fabi%20Nahian%20Madhurja%20and%20Rusab%20Sarmun%20and%20Muhammad%20E.%20H.%20Chowdhury%20and%20Adam%20Mushtak%20and%20Israa%20Al-Hashimi%20and%20Sohaib%20Bassam%20Zoghoul%0AAbstract%3A%20Cervical%20spine%20fractures%20are%20critical%20medical%20conditions%20requiring%20precise%20and%20efficient%20detection%20for%20effective%20clinical%20management.%20This%20study%20explores%20the%20viability%20of%202D%20projection-based%20vertebra%20segmentation%20for%20vertebra-level%20fracture%20detection%20in%203D%20CT%20volumes%2C%20presenting%20an%20end-to-end%20pipeline%20for%20automated%20analysis%20of%20cervical%20vertebrae%20%28C1-C7%29.%20By%20approximating%20a%203D%20volume%20through%20optimized%202D%20axial%2C%20sagittal%2C%20and%20coronal%20projections%2C%20regions%20of%20interest%20are%20identified%20using%20the%20YOLOv8%20model%20from%20all%20views%20and%20combined%20to%20approximate%20the%203D%20cervical%20spine%20area%2C%20achieving%20a%203D%20mIoU%20of%2094.45%20percent.%20This%20projection-based%20localization%20strategy%20reduces%20computational%20complexity%20compared%20to%20traditional%203D%20segmentation%20methods%20while%20maintaining%20high%20performance.%20It%20is%20followed%20by%20a%20DenseNet121-Unet-based%20multi-label%20segmentation%20leveraging%20variance-%20and%20energy-based%20projections%2C%20achieving%20a%20Dice%20score%20of%2087.86%20percent.%20Strategic%20approximation%20of%203D%20vertebral%20masks%20from%20these%202D%20segmentation%20masks%20enables%20the%20extraction%20of%20individual%20vertebra%20volumes.%20The%20volumes%20are%20analyzed%20for%20fractures%20using%20an%20ensemble%20of%202.5D%20Spatio-Sequential%20models%20incorporating%20both%20raw%20slices%20and%20projections%20per%20vertebra%20for%20complementary%20evaluation.%20This%20ensemble%20achieves%20vertebra-level%20and%20patient-level%20F1%20scores%20of%2068.15%20and%2082.26%2C%20and%20ROC-AUC%20scores%20of%2091.62%20and%2083.04%2C%20respectively.%20We%20further%20validate%20our%20approach%20through%20an%20explainability%20study%20that%20provides%20saliency%20map%20visualizations%20highlighting%20anatomical%20regions%20relevant%20for%20diagnosis%2C%20and%20an%20interobserver%20variability%20analysis%20comparing%20our%20model%27s%20performance%20with%20expert%20radiologists%2C%20demonstrating%20competitive%20results.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15235v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTracing%25203D%2520Anatomy%2520in%25202D%2520Strokes%253A%2520A%2520Multi-Stage%2520Projection%2520Driven%2520Approach%2520to%2520Cervical%2520Spine%2520Fracture%2520Identification%26entry.906535625%3DFabi%2520Nahian%2520Madhurja%2520and%2520Rusab%2520Sarmun%2520and%2520Muhammad%2520E.%2520H.%2520Chowdhury%2520and%2520Adam%2520Mushtak%2520and%2520Israa%2520Al-Hashimi%2520and%2520Sohaib%2520Bassam%2520Zoghoul%26entry.1292438233%3DCervical%2520spine%2520fractures%2520are%2520critical%2520medical%2520conditions%2520requiring%2520precise%2520and%2520efficient%2520detection%2520for%2520effective%2520clinical%2520management.%2520This%2520study%2520explores%2520the%2520viability%2520of%25202D%2520projection-based%2520vertebra%2520segmentation%2520for%2520vertebra-level%2520fracture%2520detection%2520in%25203D%2520CT%2520volumes%252C%2520presenting%2520an%2520end-to-end%2520pipeline%2520for%2520automated%2520analysis%2520of%2520cervical%2520vertebrae%2520%2528C1-C7%2529.%2520By%2520approximating%2520a%25203D%2520volume%2520through%2520optimized%25202D%2520axial%252C%2520sagittal%252C%2520and%2520coronal%2520projections%252C%2520regions%2520of%2520interest%2520are%2520identified%2520using%2520the%2520YOLOv8%2520model%2520from%2520all%2520views%2520and%2520combined%2520to%2520approximate%2520the%25203D%2520cervical%2520spine%2520area%252C%2520achieving%2520a%25203D%2520mIoU%2520of%252094.45%2520percent.%2520This%2520projection-based%2520localization%2520strategy%2520reduces%2520computational%2520complexity%2520compared%2520to%2520traditional%25203D%2520segmentation%2520methods%2520while%2520maintaining%2520high%2520performance.%2520It%2520is%2520followed%2520by%2520a%2520DenseNet121-Unet-based%2520multi-label%2520segmentation%2520leveraging%2520variance-%2520and%2520energy-based%2520projections%252C%2520achieving%2520a%2520Dice%2520score%2520of%252087.86%2520percent.%2520Strategic%2520approximation%2520of%25203D%2520vertebral%2520masks%2520from%2520these%25202D%2520segmentation%2520masks%2520enables%2520the%2520extraction%2520of%2520individual%2520vertebra%2520volumes.%2520The%2520volumes%2520are%2520analyzed%2520for%2520fractures%2520using%2520an%2520ensemble%2520of%25202.5D%2520Spatio-Sequential%2520models%2520incorporating%2520both%2520raw%2520slices%2520and%2520projections%2520per%2520vertebra%2520for%2520complementary%2520evaluation.%2520This%2520ensemble%2520achieves%2520vertebra-level%2520and%2520patient-level%2520F1%2520scores%2520of%252068.15%2520and%252082.26%252C%2520and%2520ROC-AUC%2520scores%2520of%252091.62%2520and%252083.04%252C%2520respectively.%2520We%2520further%2520validate%2520our%2520approach%2520through%2520an%2520explainability%2520study%2520that%2520provides%2520saliency%2520map%2520visualizations%2520highlighting%2520anatomical%2520regions%2520relevant%2520for%2520diagnosis%252C%2520and%2520an%2520interobserver%2520variability%2520analysis%2520comparing%2520our%2520model%2527s%2520performance%2520with%2520expert%2520radiologists%252C%2520demonstrating%2520competitive%2520results.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15235v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tracing%203D%20Anatomy%20in%202D%20Strokes%3A%20A%20Multi-Stage%20Projection%20Driven%20Approach%20to%20Cervical%20Spine%20Fracture%20Identification&entry.906535625=Fabi%20Nahian%20Madhurja%20and%20Rusab%20Sarmun%20and%20Muhammad%20E.%20H.%20Chowdhury%20and%20Adam%20Mushtak%20and%20Israa%20Al-Hashimi%20and%20Sohaib%20Bassam%20Zoghoul&entry.1292438233=Cervical%20spine%20fractures%20are%20critical%20medical%20conditions%20requiring%20precise%20and%20efficient%20detection%20for%20effective%20clinical%20management.%20This%20study%20explores%20the%20viability%20of%202D%20projection-based%20vertebra%20segmentation%20for%20vertebra-level%20fracture%20detection%20in%203D%20CT%20volumes%2C%20presenting%20an%20end-to-end%20pipeline%20for%20automated%20analysis%20of%20cervical%20vertebrae%20%28C1-C7%29.%20By%20approximating%20a%203D%20volume%20through%20optimized%202D%20axial%2C%20sagittal%2C%20and%20coronal%20projections%2C%20regions%20of%20interest%20are%20identified%20using%20the%20YOLOv8%20model%20from%20all%20views%20and%20combined%20to%20approximate%20the%203D%20cervical%20spine%20area%2C%20achieving%20a%203D%20mIoU%20of%2094.45%20percent.%20This%20projection-based%20localization%20strategy%20reduces%20computational%20complexity%20compared%20to%20traditional%203D%20segmentation%20methods%20while%20maintaining%20high%20performance.%20It%20is%20followed%20by%20a%20DenseNet121-Unet-based%20multi-label%20segmentation%20leveraging%20variance-%20and%20energy-based%20projections%2C%20achieving%20a%20Dice%20score%20of%2087.86%20percent.%20Strategic%20approximation%20of%203D%20vertebral%20masks%20from%20these%202D%20segmentation%20masks%20enables%20the%20extraction%20of%20individual%20vertebra%20volumes.%20The%20volumes%20are%20analyzed%20for%20fractures%20using%20an%20ensemble%20of%202.5D%20Spatio-Sequential%20models%20incorporating%20both%20raw%20slices%20and%20projections%20per%20vertebra%20for%20complementary%20evaluation.%20This%20ensemble%20achieves%20vertebra-level%20and%20patient-level%20F1%20scores%20of%2068.15%20and%2082.26%2C%20and%20ROC-AUC%20scores%20of%2091.62%20and%2083.04%2C%20respectively.%20We%20further%20validate%20our%20approach%20through%20an%20explainability%20study%20that%20provides%20saliency%20map%20visualizations%20highlighting%20anatomical%20regions%20relevant%20for%20diagnosis%2C%20and%20an%20interobserver%20variability%20analysis%20comparing%20our%20model%27s%20performance%20with%20expert%20radiologists%2C%20demonstrating%20competitive%20results.&entry.1838667208=http%3A//arxiv.org/abs/2601.15235v1&entry.124074799=Read"},
{"title": "Unified Multi-Dataset Training for TBPS", "author": "Nilanjana Chatterjee and Sidharatha Garg and A V Subramanyam and Brejesh Lall", "abstract": "Text-Based Person Search (TBPS) has seen significant progress with vision-language models (VLMs), yet it remains constrained by limited training data and the fact that VLMs are not inherently pre-trained for pedestrian-centric recognition. Existing TBPS methods therefore rely on dataset-centric fine-tuning to handle distribution shift, resulting in multiple independently trained models for different datasets. While synthetic data can increase the scale needed to fine-tune VLMs, it does not eliminate dataset-specific adaptation. This motivates a fundamental question: can we train a single unified TBPS model across multiple datasets? We show that naive joint training over all datasets remains sub-optimal because current training paradigms do not scale to a large number of unique person identities and are vulnerable to noisy image-text pairs. To address these challenges, we propose Scale-TBPS with two contributions: (i) a noise-aware unified dataset curation strategy that cohesively merges diverse TBPS datasets; and (ii) a scalable discriminative identity learning framework that remains effective under a large number of unique identities. Extensive experiments on CUHK-PEDES, ICFG-PEDES, RSTPReid, IIITD-20K, and UFine6926 demonstrate that a single Scale-TBPS model outperforms dataset-centric optimized models and naive joint training.", "link": "http://arxiv.org/abs/2601.14978v1", "date": "2026-01-21", "relevancy": 2.6276, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5422}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5182}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Multi-Dataset%20Training%20for%20TBPS&body=Title%3A%20Unified%20Multi-Dataset%20Training%20for%20TBPS%0AAuthor%3A%20Nilanjana%20Chatterjee%20and%20Sidharatha%20Garg%20and%20A%20V%20Subramanyam%20and%20Brejesh%20Lall%0AAbstract%3A%20Text-Based%20Person%20Search%20%28TBPS%29%20has%20seen%20significant%20progress%20with%20vision-language%20models%20%28VLMs%29%2C%20yet%20it%20remains%20constrained%20by%20limited%20training%20data%20and%20the%20fact%20that%20VLMs%20are%20not%20inherently%20pre-trained%20for%20pedestrian-centric%20recognition.%20Existing%20TBPS%20methods%20therefore%20rely%20on%20dataset-centric%20fine-tuning%20to%20handle%20distribution%20shift%2C%20resulting%20in%20multiple%20independently%20trained%20models%20for%20different%20datasets.%20While%20synthetic%20data%20can%20increase%20the%20scale%20needed%20to%20fine-tune%20VLMs%2C%20it%20does%20not%20eliminate%20dataset-specific%20adaptation.%20This%20motivates%20a%20fundamental%20question%3A%20can%20we%20train%20a%20single%20unified%20TBPS%20model%20across%20multiple%20datasets%3F%20We%20show%20that%20naive%20joint%20training%20over%20all%20datasets%20remains%20sub-optimal%20because%20current%20training%20paradigms%20do%20not%20scale%20to%20a%20large%20number%20of%20unique%20person%20identities%20and%20are%20vulnerable%20to%20noisy%20image-text%20pairs.%20To%20address%20these%20challenges%2C%20we%20propose%20Scale-TBPS%20with%20two%20contributions%3A%20%28i%29%20a%20noise-aware%20unified%20dataset%20curation%20strategy%20that%20cohesively%20merges%20diverse%20TBPS%20datasets%3B%20and%20%28ii%29%20a%20scalable%20discriminative%20identity%20learning%20framework%20that%20remains%20effective%20under%20a%20large%20number%20of%20unique%20identities.%20Extensive%20experiments%20on%20CUHK-PEDES%2C%20ICFG-PEDES%2C%20RSTPReid%2C%20IIITD-20K%2C%20and%20UFine6926%20demonstrate%20that%20a%20single%20Scale-TBPS%20model%20outperforms%20dataset-centric%20optimized%20models%20and%20naive%20joint%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Multi-Dataset%2520Training%2520for%2520TBPS%26entry.906535625%3DNilanjana%2520Chatterjee%2520and%2520Sidharatha%2520Garg%2520and%2520A%2520V%2520Subramanyam%2520and%2520Brejesh%2520Lall%26entry.1292438233%3DText-Based%2520Person%2520Search%2520%2528TBPS%2529%2520has%2520seen%2520significant%2520progress%2520with%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520yet%2520it%2520remains%2520constrained%2520by%2520limited%2520training%2520data%2520and%2520the%2520fact%2520that%2520VLMs%2520are%2520not%2520inherently%2520pre-trained%2520for%2520pedestrian-centric%2520recognition.%2520Existing%2520TBPS%2520methods%2520therefore%2520rely%2520on%2520dataset-centric%2520fine-tuning%2520to%2520handle%2520distribution%2520shift%252C%2520resulting%2520in%2520multiple%2520independently%2520trained%2520models%2520for%2520different%2520datasets.%2520While%2520synthetic%2520data%2520can%2520increase%2520the%2520scale%2520needed%2520to%2520fine-tune%2520VLMs%252C%2520it%2520does%2520not%2520eliminate%2520dataset-specific%2520adaptation.%2520This%2520motivates%2520a%2520fundamental%2520question%253A%2520can%2520we%2520train%2520a%2520single%2520unified%2520TBPS%2520model%2520across%2520multiple%2520datasets%253F%2520We%2520show%2520that%2520naive%2520joint%2520training%2520over%2520all%2520datasets%2520remains%2520sub-optimal%2520because%2520current%2520training%2520paradigms%2520do%2520not%2520scale%2520to%2520a%2520large%2520number%2520of%2520unique%2520person%2520identities%2520and%2520are%2520vulnerable%2520to%2520noisy%2520image-text%2520pairs.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Scale-TBPS%2520with%2520two%2520contributions%253A%2520%2528i%2529%2520a%2520noise-aware%2520unified%2520dataset%2520curation%2520strategy%2520that%2520cohesively%2520merges%2520diverse%2520TBPS%2520datasets%253B%2520and%2520%2528ii%2529%2520a%2520scalable%2520discriminative%2520identity%2520learning%2520framework%2520that%2520remains%2520effective%2520under%2520a%2520large%2520number%2520of%2520unique%2520identities.%2520Extensive%2520experiments%2520on%2520CUHK-PEDES%252C%2520ICFG-PEDES%252C%2520RSTPReid%252C%2520IIITD-20K%252C%2520and%2520UFine6926%2520demonstrate%2520that%2520a%2520single%2520Scale-TBPS%2520model%2520outperforms%2520dataset-centric%2520optimized%2520models%2520and%2520naive%2520joint%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Multi-Dataset%20Training%20for%20TBPS&entry.906535625=Nilanjana%20Chatterjee%20and%20Sidharatha%20Garg%20and%20A%20V%20Subramanyam%20and%20Brejesh%20Lall&entry.1292438233=Text-Based%20Person%20Search%20%28TBPS%29%20has%20seen%20significant%20progress%20with%20vision-language%20models%20%28VLMs%29%2C%20yet%20it%20remains%20constrained%20by%20limited%20training%20data%20and%20the%20fact%20that%20VLMs%20are%20not%20inherently%20pre-trained%20for%20pedestrian-centric%20recognition.%20Existing%20TBPS%20methods%20therefore%20rely%20on%20dataset-centric%20fine-tuning%20to%20handle%20distribution%20shift%2C%20resulting%20in%20multiple%20independently%20trained%20models%20for%20different%20datasets.%20While%20synthetic%20data%20can%20increase%20the%20scale%20needed%20to%20fine-tune%20VLMs%2C%20it%20does%20not%20eliminate%20dataset-specific%20adaptation.%20This%20motivates%20a%20fundamental%20question%3A%20can%20we%20train%20a%20single%20unified%20TBPS%20model%20across%20multiple%20datasets%3F%20We%20show%20that%20naive%20joint%20training%20over%20all%20datasets%20remains%20sub-optimal%20because%20current%20training%20paradigms%20do%20not%20scale%20to%20a%20large%20number%20of%20unique%20person%20identities%20and%20are%20vulnerable%20to%20noisy%20image-text%20pairs.%20To%20address%20these%20challenges%2C%20we%20propose%20Scale-TBPS%20with%20two%20contributions%3A%20%28i%29%20a%20noise-aware%20unified%20dataset%20curation%20strategy%20that%20cohesively%20merges%20diverse%20TBPS%20datasets%3B%20and%20%28ii%29%20a%20scalable%20discriminative%20identity%20learning%20framework%20that%20remains%20effective%20under%20a%20large%20number%20of%20unique%20identities.%20Extensive%20experiments%20on%20CUHK-PEDES%2C%20ICFG-PEDES%2C%20RSTPReid%2C%20IIITD-20K%2C%20and%20UFine6926%20demonstrate%20that%20a%20single%20Scale-TBPS%20model%20outperforms%20dataset-centric%20optimized%20models%20and%20naive%20joint%20training.&entry.1838667208=http%3A//arxiv.org/abs/2601.14978v1&entry.124074799=Read"},
{"title": "Graph Recognition via Subgraph Prediction", "author": "Andr\u00e9 Eberhard and Gerhard Neumann and Pascal Friederich", "abstract": "Despite tremendous improvements in tasks such as image classification, object detection, and segmentation, the recognition of visual relationships, commonly modeled as the extraction of a graph from an image, remains a challenging task. We believe that this mainly stems from the fact that there is no canonical way to approach the visual graph recognition task. Most existing solutions are specific to a problem and cannot be transferred between different contexts out-of-the box, even though the conceptual problem remains the same. With broad applicability and simplicity in mind, in this paper we develop a method, \\textbf{Gra}ph Recognition via \\textbf{S}ubgraph \\textbf{P}rediction (\\textbf{GraSP}), for recognizing graphs in images. We show across several synthetic benchmarks and one real-world application that our method works with a set of diverse types of graphs and their drawings, and can be transferred between tasks without task-specific modifications, paving the way to a more unified framework for visual graph recognition.", "link": "http://arxiv.org/abs/2601.15133v1", "date": "2026-01-21", "relevancy": 2.6174, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5599}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5154}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Recognition%20via%20Subgraph%20Prediction&body=Title%3A%20Graph%20Recognition%20via%20Subgraph%20Prediction%0AAuthor%3A%20Andr%C3%A9%20Eberhard%20and%20Gerhard%20Neumann%20and%20Pascal%20Friederich%0AAbstract%3A%20Despite%20tremendous%20improvements%20in%20tasks%20such%20as%20image%20classification%2C%20object%20detection%2C%20and%20segmentation%2C%20the%20recognition%20of%20visual%20relationships%2C%20commonly%20modeled%20as%20the%20extraction%20of%20a%20graph%20from%20an%20image%2C%20remains%20a%20challenging%20task.%20We%20believe%20that%20this%20mainly%20stems%20from%20the%20fact%20that%20there%20is%20no%20canonical%20way%20to%20approach%20the%20visual%20graph%20recognition%20task.%20Most%20existing%20solutions%20are%20specific%20to%20a%20problem%20and%20cannot%20be%20transferred%20between%20different%20contexts%20out-of-the%20box%2C%20even%20though%20the%20conceptual%20problem%20remains%20the%20same.%20With%20broad%20applicability%20and%20simplicity%20in%20mind%2C%20in%20this%20paper%20we%20develop%20a%20method%2C%20%5Ctextbf%7BGra%7Dph%20Recognition%20via%20%5Ctextbf%7BS%7Dubgraph%20%5Ctextbf%7BP%7Drediction%20%28%5Ctextbf%7BGraSP%7D%29%2C%20for%20recognizing%20graphs%20in%20images.%20We%20show%20across%20several%20synthetic%20benchmarks%20and%20one%20real-world%20application%20that%20our%20method%20works%20with%20a%20set%20of%20diverse%20types%20of%20graphs%20and%20their%20drawings%2C%20and%20can%20be%20transferred%20between%20tasks%20without%20task-specific%20modifications%2C%20paving%20the%20way%20to%20a%20more%20unified%20framework%20for%20visual%20graph%20recognition.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15133v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Recognition%2520via%2520Subgraph%2520Prediction%26entry.906535625%3DAndr%25C3%25A9%2520Eberhard%2520and%2520Gerhard%2520Neumann%2520and%2520Pascal%2520Friederich%26entry.1292438233%3DDespite%2520tremendous%2520improvements%2520in%2520tasks%2520such%2520as%2520image%2520classification%252C%2520object%2520detection%252C%2520and%2520segmentation%252C%2520the%2520recognition%2520of%2520visual%2520relationships%252C%2520commonly%2520modeled%2520as%2520the%2520extraction%2520of%2520a%2520graph%2520from%2520an%2520image%252C%2520remains%2520a%2520challenging%2520task.%2520We%2520believe%2520that%2520this%2520mainly%2520stems%2520from%2520the%2520fact%2520that%2520there%2520is%2520no%2520canonical%2520way%2520to%2520approach%2520the%2520visual%2520graph%2520recognition%2520task.%2520Most%2520existing%2520solutions%2520are%2520specific%2520to%2520a%2520problem%2520and%2520cannot%2520be%2520transferred%2520between%2520different%2520contexts%2520out-of-the%2520box%252C%2520even%2520though%2520the%2520conceptual%2520problem%2520remains%2520the%2520same.%2520With%2520broad%2520applicability%2520and%2520simplicity%2520in%2520mind%252C%2520in%2520this%2520paper%2520we%2520develop%2520a%2520method%252C%2520%255Ctextbf%257BGra%257Dph%2520Recognition%2520via%2520%255Ctextbf%257BS%257Dubgraph%2520%255Ctextbf%257BP%257Drediction%2520%2528%255Ctextbf%257BGraSP%257D%2529%252C%2520for%2520recognizing%2520graphs%2520in%2520images.%2520We%2520show%2520across%2520several%2520synthetic%2520benchmarks%2520and%2520one%2520real-world%2520application%2520that%2520our%2520method%2520works%2520with%2520a%2520set%2520of%2520diverse%2520types%2520of%2520graphs%2520and%2520their%2520drawings%252C%2520and%2520can%2520be%2520transferred%2520between%2520tasks%2520without%2520task-specific%2520modifications%252C%2520paving%2520the%2520way%2520to%2520a%2520more%2520unified%2520framework%2520for%2520visual%2520graph%2520recognition.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15133v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Recognition%20via%20Subgraph%20Prediction&entry.906535625=Andr%C3%A9%20Eberhard%20and%20Gerhard%20Neumann%20and%20Pascal%20Friederich&entry.1292438233=Despite%20tremendous%20improvements%20in%20tasks%20such%20as%20image%20classification%2C%20object%20detection%2C%20and%20segmentation%2C%20the%20recognition%20of%20visual%20relationships%2C%20commonly%20modeled%20as%20the%20extraction%20of%20a%20graph%20from%20an%20image%2C%20remains%20a%20challenging%20task.%20We%20believe%20that%20this%20mainly%20stems%20from%20the%20fact%20that%20there%20is%20no%20canonical%20way%20to%20approach%20the%20visual%20graph%20recognition%20task.%20Most%20existing%20solutions%20are%20specific%20to%20a%20problem%20and%20cannot%20be%20transferred%20between%20different%20contexts%20out-of-the%20box%2C%20even%20though%20the%20conceptual%20problem%20remains%20the%20same.%20With%20broad%20applicability%20and%20simplicity%20in%20mind%2C%20in%20this%20paper%20we%20develop%20a%20method%2C%20%5Ctextbf%7BGra%7Dph%20Recognition%20via%20%5Ctextbf%7BS%7Dubgraph%20%5Ctextbf%7BP%7Drediction%20%28%5Ctextbf%7BGraSP%7D%29%2C%20for%20recognizing%20graphs%20in%20images.%20We%20show%20across%20several%20synthetic%20benchmarks%20and%20one%20real-world%20application%20that%20our%20method%20works%20with%20a%20set%20of%20diverse%20types%20of%20graphs%20and%20their%20drawings%2C%20and%20can%20be%20transferred%20between%20tasks%20without%20task-specific%20modifications%2C%20paving%20the%20way%20to%20a%20more%20unified%20framework%20for%20visual%20graph%20recognition.&entry.1838667208=http%3A//arxiv.org/abs/2601.15133v1&entry.124074799=Read"},
{"title": "APPLE: Attribute-Preserving Pseudo-Labeling for Diffusion-Based Face Swapping", "author": "Jiwon Kang and Yeji Choi and JoungBin Lee and Wooseok Jang and Jinhyeok Choi and Taekeun Kang and Yongjae Park and Myungin Kim and Seungryong Kim", "abstract": "Face swapping aims to transfer the identity of a source face onto a target face while preserving target-specific attributes such as pose, expression, lighting, skin tone, and makeup. However, since real ground truth for face swapping is unavailable, achieving both accurate identity transfer and high-quality attribute preservation remains challenging. In addition, recent diffusion-based approaches attempt to improve visual fidelity through conditional inpainting on masked target images, but the masked condition removes crucial appearance cues of target, resulting in plausible yet misaligned attributes. To address these limitations, we propose APPLE (Attribute-Preserving Pseudo-Labeling), a diffusion-based teacher-student framework that enhances attribute fidelity through attribute-aware pseudo-label supervision. We reformulate face swapping as a conditional deblurring task to more faithfully preserve target-specific attributes such as lighting, skin tone, and makeup. In addition, we introduce an attribute-aware inversion scheme to further improve detailed attribute preservation. Through an elaborate attribute-preserving design for teacher learning, APPLE produces high-quality pseudo triplets that explicitly provide the student with direct face-swapping supervision. Overall, APPLE achieves state-of-the-art performance in terms of attribute preservation and identity transfer, producing more photorealistic and target-faithful results.", "link": "http://arxiv.org/abs/2601.15288v1", "date": "2026-01-21", "relevancy": 2.5972, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5235}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5193}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20APPLE%3A%20Attribute-Preserving%20Pseudo-Labeling%20for%20Diffusion-Based%20Face%20Swapping&body=Title%3A%20APPLE%3A%20Attribute-Preserving%20Pseudo-Labeling%20for%20Diffusion-Based%20Face%20Swapping%0AAuthor%3A%20Jiwon%20Kang%20and%20Yeji%20Choi%20and%20JoungBin%20Lee%20and%20Wooseok%20Jang%20and%20Jinhyeok%20Choi%20and%20Taekeun%20Kang%20and%20Yongjae%20Park%20and%20Myungin%20Kim%20and%20Seungryong%20Kim%0AAbstract%3A%20Face%20swapping%20aims%20to%20transfer%20the%20identity%20of%20a%20source%20face%20onto%20a%20target%20face%20while%20preserving%20target-specific%20attributes%20such%20as%20pose%2C%20expression%2C%20lighting%2C%20skin%20tone%2C%20and%20makeup.%20However%2C%20since%20real%20ground%20truth%20for%20face%20swapping%20is%20unavailable%2C%20achieving%20both%20accurate%20identity%20transfer%20and%20high-quality%20attribute%20preservation%20remains%20challenging.%20In%20addition%2C%20recent%20diffusion-based%20approaches%20attempt%20to%20improve%20visual%20fidelity%20through%20conditional%20inpainting%20on%20masked%20target%20images%2C%20but%20the%20masked%20condition%20removes%20crucial%20appearance%20cues%20of%20target%2C%20resulting%20in%20plausible%20yet%20misaligned%20attributes.%20To%20address%20these%20limitations%2C%20we%20propose%20APPLE%20%28Attribute-Preserving%20Pseudo-Labeling%29%2C%20a%20diffusion-based%20teacher-student%20framework%20that%20enhances%20attribute%20fidelity%20through%20attribute-aware%20pseudo-label%20supervision.%20We%20reformulate%20face%20swapping%20as%20a%20conditional%20deblurring%20task%20to%20more%20faithfully%20preserve%20target-specific%20attributes%20such%20as%20lighting%2C%20skin%20tone%2C%20and%20makeup.%20In%20addition%2C%20we%20introduce%20an%20attribute-aware%20inversion%20scheme%20to%20further%20improve%20detailed%20attribute%20preservation.%20Through%20an%20elaborate%20attribute-preserving%20design%20for%20teacher%20learning%2C%20APPLE%20produces%20high-quality%20pseudo%20triplets%20that%20explicitly%20provide%20the%20student%20with%20direct%20face-swapping%20supervision.%20Overall%2C%20APPLE%20achieves%20state-of-the-art%20performance%20in%20terms%20of%20attribute%20preservation%20and%20identity%20transfer%2C%20producing%20more%20photorealistic%20and%20target-faithful%20results.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15288v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAPPLE%253A%2520Attribute-Preserving%2520Pseudo-Labeling%2520for%2520Diffusion-Based%2520Face%2520Swapping%26entry.906535625%3DJiwon%2520Kang%2520and%2520Yeji%2520Choi%2520and%2520JoungBin%2520Lee%2520and%2520Wooseok%2520Jang%2520and%2520Jinhyeok%2520Choi%2520and%2520Taekeun%2520Kang%2520and%2520Yongjae%2520Park%2520and%2520Myungin%2520Kim%2520and%2520Seungryong%2520Kim%26entry.1292438233%3DFace%2520swapping%2520aims%2520to%2520transfer%2520the%2520identity%2520of%2520a%2520source%2520face%2520onto%2520a%2520target%2520face%2520while%2520preserving%2520target-specific%2520attributes%2520such%2520as%2520pose%252C%2520expression%252C%2520lighting%252C%2520skin%2520tone%252C%2520and%2520makeup.%2520However%252C%2520since%2520real%2520ground%2520truth%2520for%2520face%2520swapping%2520is%2520unavailable%252C%2520achieving%2520both%2520accurate%2520identity%2520transfer%2520and%2520high-quality%2520attribute%2520preservation%2520remains%2520challenging.%2520In%2520addition%252C%2520recent%2520diffusion-based%2520approaches%2520attempt%2520to%2520improve%2520visual%2520fidelity%2520through%2520conditional%2520inpainting%2520on%2520masked%2520target%2520images%252C%2520but%2520the%2520masked%2520condition%2520removes%2520crucial%2520appearance%2520cues%2520of%2520target%252C%2520resulting%2520in%2520plausible%2520yet%2520misaligned%2520attributes.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520APPLE%2520%2528Attribute-Preserving%2520Pseudo-Labeling%2529%252C%2520a%2520diffusion-based%2520teacher-student%2520framework%2520that%2520enhances%2520attribute%2520fidelity%2520through%2520attribute-aware%2520pseudo-label%2520supervision.%2520We%2520reformulate%2520face%2520swapping%2520as%2520a%2520conditional%2520deblurring%2520task%2520to%2520more%2520faithfully%2520preserve%2520target-specific%2520attributes%2520such%2520as%2520lighting%252C%2520skin%2520tone%252C%2520and%2520makeup.%2520In%2520addition%252C%2520we%2520introduce%2520an%2520attribute-aware%2520inversion%2520scheme%2520to%2520further%2520improve%2520detailed%2520attribute%2520preservation.%2520Through%2520an%2520elaborate%2520attribute-preserving%2520design%2520for%2520teacher%2520learning%252C%2520APPLE%2520produces%2520high-quality%2520pseudo%2520triplets%2520that%2520explicitly%2520provide%2520the%2520student%2520with%2520direct%2520face-swapping%2520supervision.%2520Overall%252C%2520APPLE%2520achieves%2520state-of-the-art%2520performance%2520in%2520terms%2520of%2520attribute%2520preservation%2520and%2520identity%2520transfer%252C%2520producing%2520more%2520photorealistic%2520and%2520target-faithful%2520results.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15288v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=APPLE%3A%20Attribute-Preserving%20Pseudo-Labeling%20for%20Diffusion-Based%20Face%20Swapping&entry.906535625=Jiwon%20Kang%20and%20Yeji%20Choi%20and%20JoungBin%20Lee%20and%20Wooseok%20Jang%20and%20Jinhyeok%20Choi%20and%20Taekeun%20Kang%20and%20Yongjae%20Park%20and%20Myungin%20Kim%20and%20Seungryong%20Kim&entry.1292438233=Face%20swapping%20aims%20to%20transfer%20the%20identity%20of%20a%20source%20face%20onto%20a%20target%20face%20while%20preserving%20target-specific%20attributes%20such%20as%20pose%2C%20expression%2C%20lighting%2C%20skin%20tone%2C%20and%20makeup.%20However%2C%20since%20real%20ground%20truth%20for%20face%20swapping%20is%20unavailable%2C%20achieving%20both%20accurate%20identity%20transfer%20and%20high-quality%20attribute%20preservation%20remains%20challenging.%20In%20addition%2C%20recent%20diffusion-based%20approaches%20attempt%20to%20improve%20visual%20fidelity%20through%20conditional%20inpainting%20on%20masked%20target%20images%2C%20but%20the%20masked%20condition%20removes%20crucial%20appearance%20cues%20of%20target%2C%20resulting%20in%20plausible%20yet%20misaligned%20attributes.%20To%20address%20these%20limitations%2C%20we%20propose%20APPLE%20%28Attribute-Preserving%20Pseudo-Labeling%29%2C%20a%20diffusion-based%20teacher-student%20framework%20that%20enhances%20attribute%20fidelity%20through%20attribute-aware%20pseudo-label%20supervision.%20We%20reformulate%20face%20swapping%20as%20a%20conditional%20deblurring%20task%20to%20more%20faithfully%20preserve%20target-specific%20attributes%20such%20as%20lighting%2C%20skin%20tone%2C%20and%20makeup.%20In%20addition%2C%20we%20introduce%20an%20attribute-aware%20inversion%20scheme%20to%20further%20improve%20detailed%20attribute%20preservation.%20Through%20an%20elaborate%20attribute-preserving%20design%20for%20teacher%20learning%2C%20APPLE%20produces%20high-quality%20pseudo%20triplets%20that%20explicitly%20provide%20the%20student%20with%20direct%20face-swapping%20supervision.%20Overall%2C%20APPLE%20achieves%20state-of-the-art%20performance%20in%20terms%20of%20attribute%20preservation%20and%20identity%20transfer%2C%20producing%20more%20photorealistic%20and%20target-faithful%20results.&entry.1838667208=http%3A//arxiv.org/abs/2601.15288v1&entry.124074799=Read"},
{"title": "Measuring and Aligning Abstraction in Vision-Language Models with Medical Taxonomies", "author": "Ben Schaper and Maxime Di Folco and Bernhard Kainz and Julia A. Schnabel and Cosmin I. Bercea", "abstract": "Vision-Language Models show strong zero-shot performance for chest X-ray classification, but standard flat metrics fail to distinguish between clinically minor and severe errors. This work investigates how to quantify and mitigate abstraction errors by leveraging medical taxonomies. We benchmark several state-of-the-art VLMs using hierarchical metrics and introduce Catastrophic Abstraction Errors to capture cross-branch mistakes. Our results reveal substantial misalignment of VLMs with clinical taxonomies despite high flat performance. To address this, we propose risk-constrained thresholding and taxonomy-aware fine-tuning with radial embeddings, which reduce severe abstraction errors to below 2 per cent while maintaining competitive performance. These findings highlight the importance of hierarchical evaluation and representation-level alignment for safer and more clinically meaningful deployment of VLMs.", "link": "http://arxiv.org/abs/2601.14827v1", "date": "2026-01-21", "relevancy": 2.5914, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5222}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5222}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measuring%20and%20Aligning%20Abstraction%20in%20Vision-Language%20Models%20with%20Medical%20Taxonomies&body=Title%3A%20Measuring%20and%20Aligning%20Abstraction%20in%20Vision-Language%20Models%20with%20Medical%20Taxonomies%0AAuthor%3A%20Ben%20Schaper%20and%20Maxime%20Di%20Folco%20and%20Bernhard%20Kainz%20and%20Julia%20A.%20Schnabel%20and%20Cosmin%20I.%20Bercea%0AAbstract%3A%20Vision-Language%20Models%20show%20strong%20zero-shot%20performance%20for%20chest%20X-ray%20classification%2C%20but%20standard%20flat%20metrics%20fail%20to%20distinguish%20between%20clinically%20minor%20and%20severe%20errors.%20This%20work%20investigates%20how%20to%20quantify%20and%20mitigate%20abstraction%20errors%20by%20leveraging%20medical%20taxonomies.%20We%20benchmark%20several%20state-of-the-art%20VLMs%20using%20hierarchical%20metrics%20and%20introduce%20Catastrophic%20Abstraction%20Errors%20to%20capture%20cross-branch%20mistakes.%20Our%20results%20reveal%20substantial%20misalignment%20of%20VLMs%20with%20clinical%20taxonomies%20despite%20high%20flat%20performance.%20To%20address%20this%2C%20we%20propose%20risk-constrained%20thresholding%20and%20taxonomy-aware%20fine-tuning%20with%20radial%20embeddings%2C%20which%20reduce%20severe%20abstraction%20errors%20to%20below%202%20per%20cent%20while%20maintaining%20competitive%20performance.%20These%20findings%20highlight%20the%20importance%20of%20hierarchical%20evaluation%20and%20representation-level%20alignment%20for%20safer%20and%20more%20clinically%20meaningful%20deployment%20of%20VLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasuring%2520and%2520Aligning%2520Abstraction%2520in%2520Vision-Language%2520Models%2520with%2520Medical%2520Taxonomies%26entry.906535625%3DBen%2520Schaper%2520and%2520Maxime%2520Di%2520Folco%2520and%2520Bernhard%2520Kainz%2520and%2520Julia%2520A.%2520Schnabel%2520and%2520Cosmin%2520I.%2520Bercea%26entry.1292438233%3DVision-Language%2520Models%2520show%2520strong%2520zero-shot%2520performance%2520for%2520chest%2520X-ray%2520classification%252C%2520but%2520standard%2520flat%2520metrics%2520fail%2520to%2520distinguish%2520between%2520clinically%2520minor%2520and%2520severe%2520errors.%2520This%2520work%2520investigates%2520how%2520to%2520quantify%2520and%2520mitigate%2520abstraction%2520errors%2520by%2520leveraging%2520medical%2520taxonomies.%2520We%2520benchmark%2520several%2520state-of-the-art%2520VLMs%2520using%2520hierarchical%2520metrics%2520and%2520introduce%2520Catastrophic%2520Abstraction%2520Errors%2520to%2520capture%2520cross-branch%2520mistakes.%2520Our%2520results%2520reveal%2520substantial%2520misalignment%2520of%2520VLMs%2520with%2520clinical%2520taxonomies%2520despite%2520high%2520flat%2520performance.%2520To%2520address%2520this%252C%2520we%2520propose%2520risk-constrained%2520thresholding%2520and%2520taxonomy-aware%2520fine-tuning%2520with%2520radial%2520embeddings%252C%2520which%2520reduce%2520severe%2520abstraction%2520errors%2520to%2520below%25202%2520per%2520cent%2520while%2520maintaining%2520competitive%2520performance.%2520These%2520findings%2520highlight%2520the%2520importance%2520of%2520hierarchical%2520evaluation%2520and%2520representation-level%2520alignment%2520for%2520safer%2520and%2520more%2520clinically%2520meaningful%2520deployment%2520of%2520VLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measuring%20and%20Aligning%20Abstraction%20in%20Vision-Language%20Models%20with%20Medical%20Taxonomies&entry.906535625=Ben%20Schaper%20and%20Maxime%20Di%20Folco%20and%20Bernhard%20Kainz%20and%20Julia%20A.%20Schnabel%20and%20Cosmin%20I.%20Bercea&entry.1292438233=Vision-Language%20Models%20show%20strong%20zero-shot%20performance%20for%20chest%20X-ray%20classification%2C%20but%20standard%20flat%20metrics%20fail%20to%20distinguish%20between%20clinically%20minor%20and%20severe%20errors.%20This%20work%20investigates%20how%20to%20quantify%20and%20mitigate%20abstraction%20errors%20by%20leveraging%20medical%20taxonomies.%20We%20benchmark%20several%20state-of-the-art%20VLMs%20using%20hierarchical%20metrics%20and%20introduce%20Catastrophic%20Abstraction%20Errors%20to%20capture%20cross-branch%20mistakes.%20Our%20results%20reveal%20substantial%20misalignment%20of%20VLMs%20with%20clinical%20taxonomies%20despite%20high%20flat%20performance.%20To%20address%20this%2C%20we%20propose%20risk-constrained%20thresholding%20and%20taxonomy-aware%20fine-tuning%20with%20radial%20embeddings%2C%20which%20reduce%20severe%20abstraction%20errors%20to%20below%202%20per%20cent%20while%20maintaining%20competitive%20performance.%20These%20findings%20highlight%20the%20importance%20of%20hierarchical%20evaluation%20and%20representation-level%20alignment%20for%20safer%20and%20more%20clinically%20meaningful%20deployment%20of%20VLMs.&entry.1838667208=http%3A//arxiv.org/abs/2601.14827v1&entry.124074799=Read"},
{"title": "CorpusQA: A 10 Million Token Benchmark for Corpus-Level Analysis and Reasoning", "author": "Zhiyuan Lu and Chenliang Li and Yingcheng Shi and Weizhou Shen and Ming Yan and Fei Huang", "abstract": "While large language models now handle million-token contexts, their capacity for reasoning across entire document repositories remains largely untested. Existing benchmarks are inadequate, as they are mostly limited to single long texts or rely on a \"sparse retrieval\" assumption-that answers can be derived from a few relevant chunks. This assumption fails for true corpus-level analysis, where evidence is highly dispersed across hundreds of documents and answers require global integration, comparison, and statistical aggregation. To address this critical gap, we introduce CorpusQA, a new benchmark scaling up to 10 million tokens, generated via a novel data synthesis framework. By decoupling reasoning from textual representation, this framework creates complex, computation-intensive queries with programmatically guaranteed ground-truth answers, challenging systems to perform holistic reasoning over vast, unstructured text without relying on fallible human annotation. We further demonstrate the utility of our framework beyond evaluation, showing that fine-tuning on our synthesized data effectively enhances an LLM's general long-context reasoning capabilities. Extensive experiments reveal that even state-of-the-art long-context LLMs struggle as input length increases, and standard retrieval-augmented generation systems collapse entirely. Our findings indicate that memory-augmented agentic architectures offer a more robust alternative, suggesting a critical shift is needed from simply extending context windows to developing advanced architectures for global information synthesis.", "link": "http://arxiv.org/abs/2601.14952v1", "date": "2026-01-21", "relevancy": 2.5671, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5325}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5325}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CorpusQA%3A%20A%2010%20Million%20Token%20Benchmark%20for%20Corpus-Level%20Analysis%20and%20Reasoning&body=Title%3A%20CorpusQA%3A%20A%2010%20Million%20Token%20Benchmark%20for%20Corpus-Level%20Analysis%20and%20Reasoning%0AAuthor%3A%20Zhiyuan%20Lu%20and%20Chenliang%20Li%20and%20Yingcheng%20Shi%20and%20Weizhou%20Shen%20and%20Ming%20Yan%20and%20Fei%20Huang%0AAbstract%3A%20While%20large%20language%20models%20now%20handle%20million-token%20contexts%2C%20their%20capacity%20for%20reasoning%20across%20entire%20document%20repositories%20remains%20largely%20untested.%20Existing%20benchmarks%20are%20inadequate%2C%20as%20they%20are%20mostly%20limited%20to%20single%20long%20texts%20or%20rely%20on%20a%20%22sparse%20retrieval%22%20assumption-that%20answers%20can%20be%20derived%20from%20a%20few%20relevant%20chunks.%20This%20assumption%20fails%20for%20true%20corpus-level%20analysis%2C%20where%20evidence%20is%20highly%20dispersed%20across%20hundreds%20of%20documents%20and%20answers%20require%20global%20integration%2C%20comparison%2C%20and%20statistical%20aggregation.%20To%20address%20this%20critical%20gap%2C%20we%20introduce%20CorpusQA%2C%20a%20new%20benchmark%20scaling%20up%20to%2010%20million%20tokens%2C%20generated%20via%20a%20novel%20data%20synthesis%20framework.%20By%20decoupling%20reasoning%20from%20textual%20representation%2C%20this%20framework%20creates%20complex%2C%20computation-intensive%20queries%20with%20programmatically%20guaranteed%20ground-truth%20answers%2C%20challenging%20systems%20to%20perform%20holistic%20reasoning%20over%20vast%2C%20unstructured%20text%20without%20relying%20on%20fallible%20human%20annotation.%20We%20further%20demonstrate%20the%20utility%20of%20our%20framework%20beyond%20evaluation%2C%20showing%20that%20fine-tuning%20on%20our%20synthesized%20data%20effectively%20enhances%20an%20LLM%27s%20general%20long-context%20reasoning%20capabilities.%20Extensive%20experiments%20reveal%20that%20even%20state-of-the-art%20long-context%20LLMs%20struggle%20as%20input%20length%20increases%2C%20and%20standard%20retrieval-augmented%20generation%20systems%20collapse%20entirely.%20Our%20findings%20indicate%20that%20memory-augmented%20agentic%20architectures%20offer%20a%20more%20robust%20alternative%2C%20suggesting%20a%20critical%20shift%20is%20needed%20from%20simply%20extending%20context%20windows%20to%20developing%20advanced%20architectures%20for%20global%20information%20synthesis.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorpusQA%253A%2520A%252010%2520Million%2520Token%2520Benchmark%2520for%2520Corpus-Level%2520Analysis%2520and%2520Reasoning%26entry.906535625%3DZhiyuan%2520Lu%2520and%2520Chenliang%2520Li%2520and%2520Yingcheng%2520Shi%2520and%2520Weizhou%2520Shen%2520and%2520Ming%2520Yan%2520and%2520Fei%2520Huang%26entry.1292438233%3DWhile%2520large%2520language%2520models%2520now%2520handle%2520million-token%2520contexts%252C%2520their%2520capacity%2520for%2520reasoning%2520across%2520entire%2520document%2520repositories%2520remains%2520largely%2520untested.%2520Existing%2520benchmarks%2520are%2520inadequate%252C%2520as%2520they%2520are%2520mostly%2520limited%2520to%2520single%2520long%2520texts%2520or%2520rely%2520on%2520a%2520%2522sparse%2520retrieval%2522%2520assumption-that%2520answers%2520can%2520be%2520derived%2520from%2520a%2520few%2520relevant%2520chunks.%2520This%2520assumption%2520fails%2520for%2520true%2520corpus-level%2520analysis%252C%2520where%2520evidence%2520is%2520highly%2520dispersed%2520across%2520hundreds%2520of%2520documents%2520and%2520answers%2520require%2520global%2520integration%252C%2520comparison%252C%2520and%2520statistical%2520aggregation.%2520To%2520address%2520this%2520critical%2520gap%252C%2520we%2520introduce%2520CorpusQA%252C%2520a%2520new%2520benchmark%2520scaling%2520up%2520to%252010%2520million%2520tokens%252C%2520generated%2520via%2520a%2520novel%2520data%2520synthesis%2520framework.%2520By%2520decoupling%2520reasoning%2520from%2520textual%2520representation%252C%2520this%2520framework%2520creates%2520complex%252C%2520computation-intensive%2520queries%2520with%2520programmatically%2520guaranteed%2520ground-truth%2520answers%252C%2520challenging%2520systems%2520to%2520perform%2520holistic%2520reasoning%2520over%2520vast%252C%2520unstructured%2520text%2520without%2520relying%2520on%2520fallible%2520human%2520annotation.%2520We%2520further%2520demonstrate%2520the%2520utility%2520of%2520our%2520framework%2520beyond%2520evaluation%252C%2520showing%2520that%2520fine-tuning%2520on%2520our%2520synthesized%2520data%2520effectively%2520enhances%2520an%2520LLM%2527s%2520general%2520long-context%2520reasoning%2520capabilities.%2520Extensive%2520experiments%2520reveal%2520that%2520even%2520state-of-the-art%2520long-context%2520LLMs%2520struggle%2520as%2520input%2520length%2520increases%252C%2520and%2520standard%2520retrieval-augmented%2520generation%2520systems%2520collapse%2520entirely.%2520Our%2520findings%2520indicate%2520that%2520memory-augmented%2520agentic%2520architectures%2520offer%2520a%2520more%2520robust%2520alternative%252C%2520suggesting%2520a%2520critical%2520shift%2520is%2520needed%2520from%2520simply%2520extending%2520context%2520windows%2520to%2520developing%2520advanced%2520architectures%2520for%2520global%2520information%2520synthesis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CorpusQA%3A%20A%2010%20Million%20Token%20Benchmark%20for%20Corpus-Level%20Analysis%20and%20Reasoning&entry.906535625=Zhiyuan%20Lu%20and%20Chenliang%20Li%20and%20Yingcheng%20Shi%20and%20Weizhou%20Shen%20and%20Ming%20Yan%20and%20Fei%20Huang&entry.1292438233=While%20large%20language%20models%20now%20handle%20million-token%20contexts%2C%20their%20capacity%20for%20reasoning%20across%20entire%20document%20repositories%20remains%20largely%20untested.%20Existing%20benchmarks%20are%20inadequate%2C%20as%20they%20are%20mostly%20limited%20to%20single%20long%20texts%20or%20rely%20on%20a%20%22sparse%20retrieval%22%20assumption-that%20answers%20can%20be%20derived%20from%20a%20few%20relevant%20chunks.%20This%20assumption%20fails%20for%20true%20corpus-level%20analysis%2C%20where%20evidence%20is%20highly%20dispersed%20across%20hundreds%20of%20documents%20and%20answers%20require%20global%20integration%2C%20comparison%2C%20and%20statistical%20aggregation.%20To%20address%20this%20critical%20gap%2C%20we%20introduce%20CorpusQA%2C%20a%20new%20benchmark%20scaling%20up%20to%2010%20million%20tokens%2C%20generated%20via%20a%20novel%20data%20synthesis%20framework.%20By%20decoupling%20reasoning%20from%20textual%20representation%2C%20this%20framework%20creates%20complex%2C%20computation-intensive%20queries%20with%20programmatically%20guaranteed%20ground-truth%20answers%2C%20challenging%20systems%20to%20perform%20holistic%20reasoning%20over%20vast%2C%20unstructured%20text%20without%20relying%20on%20fallible%20human%20annotation.%20We%20further%20demonstrate%20the%20utility%20of%20our%20framework%20beyond%20evaluation%2C%20showing%20that%20fine-tuning%20on%20our%20synthesized%20data%20effectively%20enhances%20an%20LLM%27s%20general%20long-context%20reasoning%20capabilities.%20Extensive%20experiments%20reveal%20that%20even%20state-of-the-art%20long-context%20LLMs%20struggle%20as%20input%20length%20increases%2C%20and%20standard%20retrieval-augmented%20generation%20systems%20collapse%20entirely.%20Our%20findings%20indicate%20that%20memory-augmented%20agentic%20architectures%20offer%20a%20more%20robust%20alternative%2C%20suggesting%20a%20critical%20shift%20is%20needed%20from%20simply%20extending%20context%20windows%20to%20developing%20advanced%20architectures%20for%20global%20information%20synthesis.&entry.1838667208=http%3A//arxiv.org/abs/2601.14952v1&entry.124074799=Read"},
{"title": "MambAttention: Mamba with Multi-Head Attention for Generalizable Single-Channel Speech Enhancement", "author": "Nikolai Lund K\u00fchne and Jesper Jensen and Jan \u00d8stergaard and Zheng-Hua Tan", "abstract": "With new sequence models like Mamba and xLSTM, several studies have shown that these models match or outperform the state-of-the-art in single-channel speech enhancement and audio representation learning. However, prior research has demonstrated that sequence models like LSTM and Mamba tend to overfit to the training set. To address this, previous works have shown that adding self-attention to LSTMs substantially improves generalization performance for single-channel speech enhancement. Nevertheless, neither the concept of hybrid Mamba and time-frequency attention models nor their generalization performance have been explored for speech enhancement. In this paper, we propose a novel hybrid architecture, MambAttention, which combines Mamba and shared time- and frequency-multi-head attention modules for generalizable single-channel speech enhancement. To train our model, we introduce VB-DemandEx, a dataset inspired by VoiceBank+Demand but with more challenging noise types and lower signal-to-noise ratios. Trained on VB-DemandEx, MambAttention significantly outperforms existing state-of-the-art discriminative LSTM-, xLSTM-, Mamba-, and Conformer-based systems of similar complexity across all reported metrics on two out-of-domain datasets: DNS 2020 without reverberation and EARS-WHAM_v2. MambAttention also matches or outperforms generative diffusion models in generalization performance while being competitive with language model baselines. Ablation studies highlight the importance of weight sharing between time- and frequency-multi-head attention modules for generalization performance. Finally, we explore integrating the shared time- and frequency-multi-head attention modules with LSTM and xLSTM, which yields a notable performance improvement on the out-of-domain datasets. Yet, MambAttention remains superior for cross-corpus generalization across all reported evaluation metrics.", "link": "http://arxiv.org/abs/2507.00966v4", "date": "2026-01-21", "relevancy": 2.5505, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5279}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5139}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambAttention%3A%20Mamba%20with%20Multi-Head%20Attention%20for%20Generalizable%20Single-Channel%20Speech%20Enhancement&body=Title%3A%20MambAttention%3A%20Mamba%20with%20Multi-Head%20Attention%20for%20Generalizable%20Single-Channel%20Speech%20Enhancement%0AAuthor%3A%20Nikolai%20Lund%20K%C3%BChne%20and%20Jesper%20Jensen%20and%20Jan%20%C3%98stergaard%20and%20Zheng-Hua%20Tan%0AAbstract%3A%20With%20new%20sequence%20models%20like%20Mamba%20and%20xLSTM%2C%20several%20studies%20have%20shown%20that%20these%20models%20match%20or%20outperform%20the%20state-of-the-art%20in%20single-channel%20speech%20enhancement%20and%20audio%20representation%20learning.%20However%2C%20prior%20research%20has%20demonstrated%20that%20sequence%20models%20like%20LSTM%20and%20Mamba%20tend%20to%20overfit%20to%20the%20training%20set.%20To%20address%20this%2C%20previous%20works%20have%20shown%20that%20adding%20self-attention%20to%20LSTMs%20substantially%20improves%20generalization%20performance%20for%20single-channel%20speech%20enhancement.%20Nevertheless%2C%20neither%20the%20concept%20of%20hybrid%20Mamba%20and%20time-frequency%20attention%20models%20nor%20their%20generalization%20performance%20have%20been%20explored%20for%20speech%20enhancement.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20hybrid%20architecture%2C%20MambAttention%2C%20which%20combines%20Mamba%20and%20shared%20time-%20and%20frequency-multi-head%20attention%20modules%20for%20generalizable%20single-channel%20speech%20enhancement.%20To%20train%20our%20model%2C%20we%20introduce%20VB-DemandEx%2C%20a%20dataset%20inspired%20by%20VoiceBank%2BDemand%20but%20with%20more%20challenging%20noise%20types%20and%20lower%20signal-to-noise%20ratios.%20Trained%20on%20VB-DemandEx%2C%20MambAttention%20significantly%20outperforms%20existing%20state-of-the-art%20discriminative%20LSTM-%2C%20xLSTM-%2C%20Mamba-%2C%20and%20Conformer-based%20systems%20of%20similar%20complexity%20across%20all%20reported%20metrics%20on%20two%20out-of-domain%20datasets%3A%20DNS%202020%20without%20reverberation%20and%20EARS-WHAM_v2.%20MambAttention%20also%20matches%20or%20outperforms%20generative%20diffusion%20models%20in%20generalization%20performance%20while%20being%20competitive%20with%20language%20model%20baselines.%20Ablation%20studies%20highlight%20the%20importance%20of%20weight%20sharing%20between%20time-%20and%20frequency-multi-head%20attention%20modules%20for%20generalization%20performance.%20Finally%2C%20we%20explore%20integrating%20the%20shared%20time-%20and%20frequency-multi-head%20attention%20modules%20with%20LSTM%20and%20xLSTM%2C%20which%20yields%20a%20notable%20performance%20improvement%20on%20the%20out-of-domain%20datasets.%20Yet%2C%20MambAttention%20remains%20superior%20for%20cross-corpus%20generalization%20across%20all%20reported%20evaluation%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2507.00966v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambAttention%253A%2520Mamba%2520with%2520Multi-Head%2520Attention%2520for%2520Generalizable%2520Single-Channel%2520Speech%2520Enhancement%26entry.906535625%3DNikolai%2520Lund%2520K%25C3%25BChne%2520and%2520Jesper%2520Jensen%2520and%2520Jan%2520%25C3%2598stergaard%2520and%2520Zheng-Hua%2520Tan%26entry.1292438233%3DWith%2520new%2520sequence%2520models%2520like%2520Mamba%2520and%2520xLSTM%252C%2520several%2520studies%2520have%2520shown%2520that%2520these%2520models%2520match%2520or%2520outperform%2520the%2520state-of-the-art%2520in%2520single-channel%2520speech%2520enhancement%2520and%2520audio%2520representation%2520learning.%2520However%252C%2520prior%2520research%2520has%2520demonstrated%2520that%2520sequence%2520models%2520like%2520LSTM%2520and%2520Mamba%2520tend%2520to%2520overfit%2520to%2520the%2520training%2520set.%2520To%2520address%2520this%252C%2520previous%2520works%2520have%2520shown%2520that%2520adding%2520self-attention%2520to%2520LSTMs%2520substantially%2520improves%2520generalization%2520performance%2520for%2520single-channel%2520speech%2520enhancement.%2520Nevertheless%252C%2520neither%2520the%2520concept%2520of%2520hybrid%2520Mamba%2520and%2520time-frequency%2520attention%2520models%2520nor%2520their%2520generalization%2520performance%2520have%2520been%2520explored%2520for%2520speech%2520enhancement.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520hybrid%2520architecture%252C%2520MambAttention%252C%2520which%2520combines%2520Mamba%2520and%2520shared%2520time-%2520and%2520frequency-multi-head%2520attention%2520modules%2520for%2520generalizable%2520single-channel%2520speech%2520enhancement.%2520To%2520train%2520our%2520model%252C%2520we%2520introduce%2520VB-DemandEx%252C%2520a%2520dataset%2520inspired%2520by%2520VoiceBank%252BDemand%2520but%2520with%2520more%2520challenging%2520noise%2520types%2520and%2520lower%2520signal-to-noise%2520ratios.%2520Trained%2520on%2520VB-DemandEx%252C%2520MambAttention%2520significantly%2520outperforms%2520existing%2520state-of-the-art%2520discriminative%2520LSTM-%252C%2520xLSTM-%252C%2520Mamba-%252C%2520and%2520Conformer-based%2520systems%2520of%2520similar%2520complexity%2520across%2520all%2520reported%2520metrics%2520on%2520two%2520out-of-domain%2520datasets%253A%2520DNS%25202020%2520without%2520reverberation%2520and%2520EARS-WHAM_v2.%2520MambAttention%2520also%2520matches%2520or%2520outperforms%2520generative%2520diffusion%2520models%2520in%2520generalization%2520performance%2520while%2520being%2520competitive%2520with%2520language%2520model%2520baselines.%2520Ablation%2520studies%2520highlight%2520the%2520importance%2520of%2520weight%2520sharing%2520between%2520time-%2520and%2520frequency-multi-head%2520attention%2520modules%2520for%2520generalization%2520performance.%2520Finally%252C%2520we%2520explore%2520integrating%2520the%2520shared%2520time-%2520and%2520frequency-multi-head%2520attention%2520modules%2520with%2520LSTM%2520and%2520xLSTM%252C%2520which%2520yields%2520a%2520notable%2520performance%2520improvement%2520on%2520the%2520out-of-domain%2520datasets.%2520Yet%252C%2520MambAttention%2520remains%2520superior%2520for%2520cross-corpus%2520generalization%2520across%2520all%2520reported%2520evaluation%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00966v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambAttention%3A%20Mamba%20with%20Multi-Head%20Attention%20for%20Generalizable%20Single-Channel%20Speech%20Enhancement&entry.906535625=Nikolai%20Lund%20K%C3%BChne%20and%20Jesper%20Jensen%20and%20Jan%20%C3%98stergaard%20and%20Zheng-Hua%20Tan&entry.1292438233=With%20new%20sequence%20models%20like%20Mamba%20and%20xLSTM%2C%20several%20studies%20have%20shown%20that%20these%20models%20match%20or%20outperform%20the%20state-of-the-art%20in%20single-channel%20speech%20enhancement%20and%20audio%20representation%20learning.%20However%2C%20prior%20research%20has%20demonstrated%20that%20sequence%20models%20like%20LSTM%20and%20Mamba%20tend%20to%20overfit%20to%20the%20training%20set.%20To%20address%20this%2C%20previous%20works%20have%20shown%20that%20adding%20self-attention%20to%20LSTMs%20substantially%20improves%20generalization%20performance%20for%20single-channel%20speech%20enhancement.%20Nevertheless%2C%20neither%20the%20concept%20of%20hybrid%20Mamba%20and%20time-frequency%20attention%20models%20nor%20their%20generalization%20performance%20have%20been%20explored%20for%20speech%20enhancement.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20hybrid%20architecture%2C%20MambAttention%2C%20which%20combines%20Mamba%20and%20shared%20time-%20and%20frequency-multi-head%20attention%20modules%20for%20generalizable%20single-channel%20speech%20enhancement.%20To%20train%20our%20model%2C%20we%20introduce%20VB-DemandEx%2C%20a%20dataset%20inspired%20by%20VoiceBank%2BDemand%20but%20with%20more%20challenging%20noise%20types%20and%20lower%20signal-to-noise%20ratios.%20Trained%20on%20VB-DemandEx%2C%20MambAttention%20significantly%20outperforms%20existing%20state-of-the-art%20discriminative%20LSTM-%2C%20xLSTM-%2C%20Mamba-%2C%20and%20Conformer-based%20systems%20of%20similar%20complexity%20across%20all%20reported%20metrics%20on%20two%20out-of-domain%20datasets%3A%20DNS%202020%20without%20reverberation%20and%20EARS-WHAM_v2.%20MambAttention%20also%20matches%20or%20outperforms%20generative%20diffusion%20models%20in%20generalization%20performance%20while%20being%20competitive%20with%20language%20model%20baselines.%20Ablation%20studies%20highlight%20the%20importance%20of%20weight%20sharing%20between%20time-%20and%20frequency-multi-head%20attention%20modules%20for%20generalization%20performance.%20Finally%2C%20we%20explore%20integrating%20the%20shared%20time-%20and%20frequency-multi-head%20attention%20modules%20with%20LSTM%20and%20xLSTM%2C%20which%20yields%20a%20notable%20performance%20improvement%20on%20the%20out-of-domain%20datasets.%20Yet%2C%20MambAttention%20remains%20superior%20for%20cross-corpus%20generalization%20across%20all%20reported%20evaluation%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2507.00966v4&entry.124074799=Read"},
{"title": "Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training", "author": "Song Lai and Haohan Zhao and Rong Feng and Changyi Ma and Wenzhuo Liu and Hongbo Zhao and Xi Lin and Dong Yi and Qingfu Zhang and Hongbin Liu and Gaofeng Meng and Fei Zhu", "abstract": "Continual post-training (CPT) is a popular and effective technique for adapting foundation models like multimodal large language models to specific and ever-evolving downstream tasks. While existing research has primarily concentrated on methods like data replay, model expansion, or parameter regularization, the fundamental role of the learning paradigm within CPT remains largely unexplored. This paper presents a comparative analysis of two core post-training paradigms: supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT), investigating their respective impacts on knowledge retention during CPT. Our experiments are conducted on a benchmark comprising seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base model for continual post-training. The investigation yields two significant findings: (1) When continuously learning on downstream tasks, SFT leads to catastrophic forgetting of previously learned tasks. In contrast, RFT inherently preserves prior knowledge and achieve performance comparable to multi-task training. (2) RFT successfully protects and even enhances the model's general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro). Conversely, SFT degrades general model capabilities severely. Further analysis reveals that this stability is not primarily due to explicit mechanisms like KL penalty or chain-of-thought reasoning. Instead, we identify an implicit regularization mechanism inherent to RFT as a key contributing factor. Our theoretical analysis suggests that RFT's gradient updates are naturally scaled by the reward variance, acting as a data-dependent regularizer that inherently protects previously acquired knowledge. Finally, we propose a rollout-based instance filtering algorithm to enhance the stability and efficiency of RFT. Our comprehensive study demonstrates the superiority of RFT as a robust paradigm for continual post-training.", "link": "http://arxiv.org/abs/2507.05386v5", "date": "2026-01-21", "relevancy": 2.516, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5146}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4985}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Fine-Tuning%20Naturally%20Mitigates%20Forgetting%20in%20Continual%20Post-Training&body=Title%3A%20Reinforcement%20Fine-Tuning%20Naturally%20Mitigates%20Forgetting%20in%20Continual%20Post-Training%0AAuthor%3A%20Song%20Lai%20and%20Haohan%20Zhao%20and%20Rong%20Feng%20and%20Changyi%20Ma%20and%20Wenzhuo%20Liu%20and%20Hongbo%20Zhao%20and%20Xi%20Lin%20and%20Dong%20Yi%20and%20Qingfu%20Zhang%20and%20Hongbin%20Liu%20and%20Gaofeng%20Meng%20and%20Fei%20Zhu%0AAbstract%3A%20Continual%20post-training%20%28CPT%29%20is%20a%20popular%20and%20effective%20technique%20for%20adapting%20foundation%20models%20like%20multimodal%20large%20language%20models%20to%20specific%20and%20ever-evolving%20downstream%20tasks.%20While%20existing%20research%20has%20primarily%20concentrated%20on%20methods%20like%20data%20replay%2C%20model%20expansion%2C%20or%20parameter%20regularization%2C%20the%20fundamental%20role%20of%20the%20learning%20paradigm%20within%20CPT%20remains%20largely%20unexplored.%20This%20paper%20presents%20a%20comparative%20analysis%20of%20two%20core%20post-training%20paradigms%3A%20supervised%20fine-tuning%20%28SFT%29%20and%20reinforcement%20fine-tuning%20%28RFT%29%2C%20investigating%20their%20respective%20impacts%20on%20knowledge%20retention%20during%20CPT.%20Our%20experiments%20are%20conducted%20on%20a%20benchmark%20comprising%20seven%20diverse%20multimodal%20tasks%2C%20utilizing%20Qwen2.5-VL-7B-Instruct%20as%20the%20base%20model%20for%20continual%20post-training.%20The%20investigation%20yields%20two%20significant%20findings%3A%20%281%29%20When%20continuously%20learning%20on%20downstream%20tasks%2C%20SFT%20leads%20to%20catastrophic%20forgetting%20of%20previously%20learned%20tasks.%20In%20contrast%2C%20RFT%20inherently%20preserves%20prior%20knowledge%20and%20achieve%20performance%20comparable%20to%20multi-task%20training.%20%282%29%20RFT%20successfully%20protects%20and%20even%20enhances%20the%20model%27s%20general%20knowledge%20on%20standard%20benchmarks%20%28e.g.%2C%20MMMU%20and%20MMLU-Pro%29.%20Conversely%2C%20SFT%20degrades%20general%20model%20capabilities%20severely.%20Further%20analysis%20reveals%20that%20this%20stability%20is%20not%20primarily%20due%20to%20explicit%20mechanisms%20like%20KL%20penalty%20or%20chain-of-thought%20reasoning.%20Instead%2C%20we%20identify%20an%20implicit%20regularization%20mechanism%20inherent%20to%20RFT%20as%20a%20key%20contributing%20factor.%20Our%20theoretical%20analysis%20suggests%20that%20RFT%27s%20gradient%20updates%20are%20naturally%20scaled%20by%20the%20reward%20variance%2C%20acting%20as%20a%20data-dependent%20regularizer%20that%20inherently%20protects%20previously%20acquired%20knowledge.%20Finally%2C%20we%20propose%20a%20rollout-based%20instance%20filtering%20algorithm%20to%20enhance%20the%20stability%20and%20efficiency%20of%20RFT.%20Our%20comprehensive%20study%20demonstrates%20the%20superiority%20of%20RFT%20as%20a%20robust%20paradigm%20for%20continual%20post-training.%0ALink%3A%20http%3A//arxiv.org/abs/2507.05386v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Fine-Tuning%2520Naturally%2520Mitigates%2520Forgetting%2520in%2520Continual%2520Post-Training%26entry.906535625%3DSong%2520Lai%2520and%2520Haohan%2520Zhao%2520and%2520Rong%2520Feng%2520and%2520Changyi%2520Ma%2520and%2520Wenzhuo%2520Liu%2520and%2520Hongbo%2520Zhao%2520and%2520Xi%2520Lin%2520and%2520Dong%2520Yi%2520and%2520Qingfu%2520Zhang%2520and%2520Hongbin%2520Liu%2520and%2520Gaofeng%2520Meng%2520and%2520Fei%2520Zhu%26entry.1292438233%3DContinual%2520post-training%2520%2528CPT%2529%2520is%2520a%2520popular%2520and%2520effective%2520technique%2520for%2520adapting%2520foundation%2520models%2520like%2520multimodal%2520large%2520language%2520models%2520to%2520specific%2520and%2520ever-evolving%2520downstream%2520tasks.%2520While%2520existing%2520research%2520has%2520primarily%2520concentrated%2520on%2520methods%2520like%2520data%2520replay%252C%2520model%2520expansion%252C%2520or%2520parameter%2520regularization%252C%2520the%2520fundamental%2520role%2520of%2520the%2520learning%2520paradigm%2520within%2520CPT%2520remains%2520largely%2520unexplored.%2520This%2520paper%2520presents%2520a%2520comparative%2520analysis%2520of%2520two%2520core%2520post-training%2520paradigms%253A%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520reinforcement%2520fine-tuning%2520%2528RFT%2529%252C%2520investigating%2520their%2520respective%2520impacts%2520on%2520knowledge%2520retention%2520during%2520CPT.%2520Our%2520experiments%2520are%2520conducted%2520on%2520a%2520benchmark%2520comprising%2520seven%2520diverse%2520multimodal%2520tasks%252C%2520utilizing%2520Qwen2.5-VL-7B-Instruct%2520as%2520the%2520base%2520model%2520for%2520continual%2520post-training.%2520The%2520investigation%2520yields%2520two%2520significant%2520findings%253A%2520%25281%2529%2520When%2520continuously%2520learning%2520on%2520downstream%2520tasks%252C%2520SFT%2520leads%2520to%2520catastrophic%2520forgetting%2520of%2520previously%2520learned%2520tasks.%2520In%2520contrast%252C%2520RFT%2520inherently%2520preserves%2520prior%2520knowledge%2520and%2520achieve%2520performance%2520comparable%2520to%2520multi-task%2520training.%2520%25282%2529%2520RFT%2520successfully%2520protects%2520and%2520even%2520enhances%2520the%2520model%2527s%2520general%2520knowledge%2520on%2520standard%2520benchmarks%2520%2528e.g.%252C%2520MMMU%2520and%2520MMLU-Pro%2529.%2520Conversely%252C%2520SFT%2520degrades%2520general%2520model%2520capabilities%2520severely.%2520Further%2520analysis%2520reveals%2520that%2520this%2520stability%2520is%2520not%2520primarily%2520due%2520to%2520explicit%2520mechanisms%2520like%2520KL%2520penalty%2520or%2520chain-of-thought%2520reasoning.%2520Instead%252C%2520we%2520identify%2520an%2520implicit%2520regularization%2520mechanism%2520inherent%2520to%2520RFT%2520as%2520a%2520key%2520contributing%2520factor.%2520Our%2520theoretical%2520analysis%2520suggests%2520that%2520RFT%2527s%2520gradient%2520updates%2520are%2520naturally%2520scaled%2520by%2520the%2520reward%2520variance%252C%2520acting%2520as%2520a%2520data-dependent%2520regularizer%2520that%2520inherently%2520protects%2520previously%2520acquired%2520knowledge.%2520Finally%252C%2520we%2520propose%2520a%2520rollout-based%2520instance%2520filtering%2520algorithm%2520to%2520enhance%2520the%2520stability%2520and%2520efficiency%2520of%2520RFT.%2520Our%2520comprehensive%2520study%2520demonstrates%2520the%2520superiority%2520of%2520RFT%2520as%2520a%2520robust%2520paradigm%2520for%2520continual%2520post-training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05386v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Fine-Tuning%20Naturally%20Mitigates%20Forgetting%20in%20Continual%20Post-Training&entry.906535625=Song%20Lai%20and%20Haohan%20Zhao%20and%20Rong%20Feng%20and%20Changyi%20Ma%20and%20Wenzhuo%20Liu%20and%20Hongbo%20Zhao%20and%20Xi%20Lin%20and%20Dong%20Yi%20and%20Qingfu%20Zhang%20and%20Hongbin%20Liu%20and%20Gaofeng%20Meng%20and%20Fei%20Zhu&entry.1292438233=Continual%20post-training%20%28CPT%29%20is%20a%20popular%20and%20effective%20technique%20for%20adapting%20foundation%20models%20like%20multimodal%20large%20language%20models%20to%20specific%20and%20ever-evolving%20downstream%20tasks.%20While%20existing%20research%20has%20primarily%20concentrated%20on%20methods%20like%20data%20replay%2C%20model%20expansion%2C%20or%20parameter%20regularization%2C%20the%20fundamental%20role%20of%20the%20learning%20paradigm%20within%20CPT%20remains%20largely%20unexplored.%20This%20paper%20presents%20a%20comparative%20analysis%20of%20two%20core%20post-training%20paradigms%3A%20supervised%20fine-tuning%20%28SFT%29%20and%20reinforcement%20fine-tuning%20%28RFT%29%2C%20investigating%20their%20respective%20impacts%20on%20knowledge%20retention%20during%20CPT.%20Our%20experiments%20are%20conducted%20on%20a%20benchmark%20comprising%20seven%20diverse%20multimodal%20tasks%2C%20utilizing%20Qwen2.5-VL-7B-Instruct%20as%20the%20base%20model%20for%20continual%20post-training.%20The%20investigation%20yields%20two%20significant%20findings%3A%20%281%29%20When%20continuously%20learning%20on%20downstream%20tasks%2C%20SFT%20leads%20to%20catastrophic%20forgetting%20of%20previously%20learned%20tasks.%20In%20contrast%2C%20RFT%20inherently%20preserves%20prior%20knowledge%20and%20achieve%20performance%20comparable%20to%20multi-task%20training.%20%282%29%20RFT%20successfully%20protects%20and%20even%20enhances%20the%20model%27s%20general%20knowledge%20on%20standard%20benchmarks%20%28e.g.%2C%20MMMU%20and%20MMLU-Pro%29.%20Conversely%2C%20SFT%20degrades%20general%20model%20capabilities%20severely.%20Further%20analysis%20reveals%20that%20this%20stability%20is%20not%20primarily%20due%20to%20explicit%20mechanisms%20like%20KL%20penalty%20or%20chain-of-thought%20reasoning.%20Instead%2C%20we%20identify%20an%20implicit%20regularization%20mechanism%20inherent%20to%20RFT%20as%20a%20key%20contributing%20factor.%20Our%20theoretical%20analysis%20suggests%20that%20RFT%27s%20gradient%20updates%20are%20naturally%20scaled%20by%20the%20reward%20variance%2C%20acting%20as%20a%20data-dependent%20regularizer%20that%20inherently%20protects%20previously%20acquired%20knowledge.%20Finally%2C%20we%20propose%20a%20rollout-based%20instance%20filtering%20algorithm%20to%20enhance%20the%20stability%20and%20efficiency%20of%20RFT.%20Our%20comprehensive%20study%20demonstrates%20the%20superiority%20of%20RFT%20as%20a%20robust%20paradigm%20for%20continual%20post-training.&entry.1838667208=http%3A//arxiv.org/abs/2507.05386v5&entry.124074799=Read"},
{"title": "From Charts to Code: A Hierarchical Benchmark for Multimodal Models", "author": "Jiahao Tang and Henry Hengyuan Zhao and Lijian Wu and Yifei Tao and Dongxing Mao and Yang Wan and Jingru Tan and Min Zeng and Min Li and Alex Jinpeng Wang", "abstract": "We introduce Chart2Code, a new benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models (LMMs). Chart2Code is explicitly designed from a user-driven perspective, capturing diverse real-world scenarios and progressively increasing task difficulty. It consists of three levels: Level 1 (Chart Reproduction) reproduces charts from a reference figure and user query; Level 2 (Chart Editing) involves complex modifications such as changing chart types or adding elements; and Level 3 (Long-Table to Chart Generation) requires models to transform long, information-dense tables into faithful charts following user instructions. To our knowledge, this is the first hierarchical benchmark that reflects practical chart2code usage while systematically scaling task complexity. In total, Chart2Code contains 2,023 tasks across 22 chart types, paired with multi-level evaluation metrics that assess both code correctness and the visual fidelity of rendered charts. We benchmark 25 state-of-the-art (SoTA) LMMs, including both proprietary and the latest open-source models such as GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental results demonstrate that even the SoTA model GPT-5 averages only 0.57 on code-based evaluation and 0.22 on chart-quality assessment across the editing tasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark will drive advances in multimodal reasoning and foster the development of more robust and general-purpose LMMs. Our code and data are available on Chart2Code.", "link": "http://arxiv.org/abs/2510.17932v2", "date": "2026-01-21", "relevancy": 2.5139, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5179}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4952}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Charts%20to%20Code%3A%20A%20Hierarchical%20Benchmark%20for%20Multimodal%20Models&body=Title%3A%20From%20Charts%20to%20Code%3A%20A%20Hierarchical%20Benchmark%20for%20Multimodal%20Models%0AAuthor%3A%20Jiahao%20Tang%20and%20Henry%20Hengyuan%20Zhao%20and%20Lijian%20Wu%20and%20Yifei%20Tao%20and%20Dongxing%20Mao%20and%20Yang%20Wan%20and%20Jingru%20Tan%20and%20Min%20Zeng%20and%20Min%20Li%20and%20Alex%20Jinpeng%20Wang%0AAbstract%3A%20We%20introduce%20Chart2Code%2C%20a%20new%20benchmark%20for%20evaluating%20the%20chart%20understanding%20and%20code%20generation%20capabilities%20of%20large%20multimodal%20models%20%28LMMs%29.%20Chart2Code%20is%20explicitly%20designed%20from%20a%20user-driven%20perspective%2C%20capturing%20diverse%20real-world%20scenarios%20and%20progressively%20increasing%20task%20difficulty.%20It%20consists%20of%20three%20levels%3A%20Level%201%20%28Chart%20Reproduction%29%20reproduces%20charts%20from%20a%20reference%20figure%20and%20user%20query%3B%20Level%202%20%28Chart%20Editing%29%20involves%20complex%20modifications%20such%20as%20changing%20chart%20types%20or%20adding%20elements%3B%20and%20Level%203%20%28Long-Table%20to%20Chart%20Generation%29%20requires%20models%20to%20transform%20long%2C%20information-dense%20tables%20into%20faithful%20charts%20following%20user%20instructions.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20hierarchical%20benchmark%20that%20reflects%20practical%20chart2code%20usage%20while%20systematically%20scaling%20task%20complexity.%20In%20total%2C%20Chart2Code%20contains%202%2C023%20tasks%20across%2022%20chart%20types%2C%20paired%20with%20multi-level%20evaluation%20metrics%20that%20assess%20both%20code%20correctness%20and%20the%20visual%20fidelity%20of%20rendered%20charts.%20We%20benchmark%2025%20state-of-the-art%20%28SoTA%29%20LMMs%2C%20including%20both%20proprietary%20and%20the%20latest%20open-source%20models%20such%20as%20GPT-5%2C%20Qwen2.5-VL%2C%20InternVL3/3.5%2C%20MiMo-VL%2C%20and%20Seed-1.6-VL.%20Experimental%20results%20demonstrate%20that%20even%20the%20SoTA%20model%20GPT-5%20averages%20only%200.57%20on%20code-based%20evaluation%20and%200.22%20on%20chart-quality%20assessment%20across%20the%20editing%20tasks%2C%20underscoring%20the%20difficulty%20of%20Chart2Code.%20We%20anticipate%20this%20benchmark%20will%20drive%20advances%20in%20multimodal%20reasoning%20and%20foster%20the%20development%20of%20more%20robust%20and%20general-purpose%20LMMs.%20Our%20code%20and%20data%20are%20available%20on%20Chart2Code.%0ALink%3A%20http%3A//arxiv.org/abs/2510.17932v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Charts%2520to%2520Code%253A%2520A%2520Hierarchical%2520Benchmark%2520for%2520Multimodal%2520Models%26entry.906535625%3DJiahao%2520Tang%2520and%2520Henry%2520Hengyuan%2520Zhao%2520and%2520Lijian%2520Wu%2520and%2520Yifei%2520Tao%2520and%2520Dongxing%2520Mao%2520and%2520Yang%2520Wan%2520and%2520Jingru%2520Tan%2520and%2520Min%2520Zeng%2520and%2520Min%2520Li%2520and%2520Alex%2520Jinpeng%2520Wang%26entry.1292438233%3DWe%2520introduce%2520Chart2Code%252C%2520a%2520new%2520benchmark%2520for%2520evaluating%2520the%2520chart%2520understanding%2520and%2520code%2520generation%2520capabilities%2520of%2520large%2520multimodal%2520models%2520%2528LMMs%2529.%2520Chart2Code%2520is%2520explicitly%2520designed%2520from%2520a%2520user-driven%2520perspective%252C%2520capturing%2520diverse%2520real-world%2520scenarios%2520and%2520progressively%2520increasing%2520task%2520difficulty.%2520It%2520consists%2520of%2520three%2520levels%253A%2520Level%25201%2520%2528Chart%2520Reproduction%2529%2520reproduces%2520charts%2520from%2520a%2520reference%2520figure%2520and%2520user%2520query%253B%2520Level%25202%2520%2528Chart%2520Editing%2529%2520involves%2520complex%2520modifications%2520such%2520as%2520changing%2520chart%2520types%2520or%2520adding%2520elements%253B%2520and%2520Level%25203%2520%2528Long-Table%2520to%2520Chart%2520Generation%2529%2520requires%2520models%2520to%2520transform%2520long%252C%2520information-dense%2520tables%2520into%2520faithful%2520charts%2520following%2520user%2520instructions.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520hierarchical%2520benchmark%2520that%2520reflects%2520practical%2520chart2code%2520usage%2520while%2520systematically%2520scaling%2520task%2520complexity.%2520In%2520total%252C%2520Chart2Code%2520contains%25202%252C023%2520tasks%2520across%252022%2520chart%2520types%252C%2520paired%2520with%2520multi-level%2520evaluation%2520metrics%2520that%2520assess%2520both%2520code%2520correctness%2520and%2520the%2520visual%2520fidelity%2520of%2520rendered%2520charts.%2520We%2520benchmark%252025%2520state-of-the-art%2520%2528SoTA%2529%2520LMMs%252C%2520including%2520both%2520proprietary%2520and%2520the%2520latest%2520open-source%2520models%2520such%2520as%2520GPT-5%252C%2520Qwen2.5-VL%252C%2520InternVL3/3.5%252C%2520MiMo-VL%252C%2520and%2520Seed-1.6-VL.%2520Experimental%2520results%2520demonstrate%2520that%2520even%2520the%2520SoTA%2520model%2520GPT-5%2520averages%2520only%25200.57%2520on%2520code-based%2520evaluation%2520and%25200.22%2520on%2520chart-quality%2520assessment%2520across%2520the%2520editing%2520tasks%252C%2520underscoring%2520the%2520difficulty%2520of%2520Chart2Code.%2520We%2520anticipate%2520this%2520benchmark%2520will%2520drive%2520advances%2520in%2520multimodal%2520reasoning%2520and%2520foster%2520the%2520development%2520of%2520more%2520robust%2520and%2520general-purpose%2520LMMs.%2520Our%2520code%2520and%2520data%2520are%2520available%2520on%2520Chart2Code.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17932v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Charts%20to%20Code%3A%20A%20Hierarchical%20Benchmark%20for%20Multimodal%20Models&entry.906535625=Jiahao%20Tang%20and%20Henry%20Hengyuan%20Zhao%20and%20Lijian%20Wu%20and%20Yifei%20Tao%20and%20Dongxing%20Mao%20and%20Yang%20Wan%20and%20Jingru%20Tan%20and%20Min%20Zeng%20and%20Min%20Li%20and%20Alex%20Jinpeng%20Wang&entry.1292438233=We%20introduce%20Chart2Code%2C%20a%20new%20benchmark%20for%20evaluating%20the%20chart%20understanding%20and%20code%20generation%20capabilities%20of%20large%20multimodal%20models%20%28LMMs%29.%20Chart2Code%20is%20explicitly%20designed%20from%20a%20user-driven%20perspective%2C%20capturing%20diverse%20real-world%20scenarios%20and%20progressively%20increasing%20task%20difficulty.%20It%20consists%20of%20three%20levels%3A%20Level%201%20%28Chart%20Reproduction%29%20reproduces%20charts%20from%20a%20reference%20figure%20and%20user%20query%3B%20Level%202%20%28Chart%20Editing%29%20involves%20complex%20modifications%20such%20as%20changing%20chart%20types%20or%20adding%20elements%3B%20and%20Level%203%20%28Long-Table%20to%20Chart%20Generation%29%20requires%20models%20to%20transform%20long%2C%20information-dense%20tables%20into%20faithful%20charts%20following%20user%20instructions.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20hierarchical%20benchmark%20that%20reflects%20practical%20chart2code%20usage%20while%20systematically%20scaling%20task%20complexity.%20In%20total%2C%20Chart2Code%20contains%202%2C023%20tasks%20across%2022%20chart%20types%2C%20paired%20with%20multi-level%20evaluation%20metrics%20that%20assess%20both%20code%20correctness%20and%20the%20visual%20fidelity%20of%20rendered%20charts.%20We%20benchmark%2025%20state-of-the-art%20%28SoTA%29%20LMMs%2C%20including%20both%20proprietary%20and%20the%20latest%20open-source%20models%20such%20as%20GPT-5%2C%20Qwen2.5-VL%2C%20InternVL3/3.5%2C%20MiMo-VL%2C%20and%20Seed-1.6-VL.%20Experimental%20results%20demonstrate%20that%20even%20the%20SoTA%20model%20GPT-5%20averages%20only%200.57%20on%20code-based%20evaluation%20and%200.22%20on%20chart-quality%20assessment%20across%20the%20editing%20tasks%2C%20underscoring%20the%20difficulty%20of%20Chart2Code.%20We%20anticipate%20this%20benchmark%20will%20drive%20advances%20in%20multimodal%20reasoning%20and%20foster%20the%20development%20of%20more%20robust%20and%20general-purpose%20LMMs.%20Our%20code%20and%20data%20are%20available%20on%20Chart2Code.&entry.1838667208=http%3A//arxiv.org/abs/2510.17932v2&entry.124074799=Read"},
{"title": "Competitive Audio-Language Models with Data-Efficient Single-Stage Training on Public Data", "author": "Gokul Karthik Kumar and Rishabh Saraf and Ludovick Lepauloux and Abdul Muneer and Billel Mokeddem and Hakim Hacid", "abstract": "Large language models (LLMs) have transformed NLP, yet their integration with audio remains underexplored despite audio's centrality to human communication. We introduce Falcon3-Audio, a family of Audio-Language Models (ALMs) built on instruction-tuned LLMs and Whisper encoders. Using a remarkably small amount of public audio data, less than 30K hours (5K unique), Falcon3-Audio-7B matches the best reported performance among open-weight models on the MMAU benchmark, with a score of 64.14, matching R1-AQA, while distinguishing itself through superior data and parameter efficiency, single-stage training, and transparency. Notably, our smallest 1B model remains competitive with larger open models ranging from 2B to 13B parameters. Through extensive ablations, we find that common complexities such as curriculum learning, multiple audio encoders, and intricate cross-attention connectors are not required for strong performance, even compared to models trained on over 500K hours of data.", "link": "http://arxiv.org/abs/2509.07526v2", "date": "2026-01-21", "relevancy": 2.4893, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5033}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5033}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Competitive%20Audio-Language%20Models%20with%20Data-Efficient%20Single-Stage%20Training%20on%20Public%20Data&body=Title%3A%20Competitive%20Audio-Language%20Models%20with%20Data-Efficient%20Single-Stage%20Training%20on%20Public%20Data%0AAuthor%3A%20Gokul%20Karthik%20Kumar%20and%20Rishabh%20Saraf%20and%20Ludovick%20Lepauloux%20and%20Abdul%20Muneer%20and%20Billel%20Mokeddem%20and%20Hakim%20Hacid%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20transformed%20NLP%2C%20yet%20their%20integration%20with%20audio%20remains%20underexplored%20despite%20audio%27s%20centrality%20to%20human%20communication.%20We%20introduce%20Falcon3-Audio%2C%20a%20family%20of%20Audio-Language%20Models%20%28ALMs%29%20built%20on%20instruction-tuned%20LLMs%20and%20Whisper%20encoders.%20Using%20a%20remarkably%20small%20amount%20of%20public%20audio%20data%2C%20less%20than%2030K%20hours%20%285K%20unique%29%2C%20Falcon3-Audio-7B%20matches%20the%20best%20reported%20performance%20among%20open-weight%20models%20on%20the%20MMAU%20benchmark%2C%20with%20a%20score%20of%2064.14%2C%20matching%20R1-AQA%2C%20while%20distinguishing%20itself%20through%20superior%20data%20and%20parameter%20efficiency%2C%20single-stage%20training%2C%20and%20transparency.%20Notably%2C%20our%20smallest%201B%20model%20remains%20competitive%20with%20larger%20open%20models%20ranging%20from%202B%20to%2013B%20parameters.%20Through%20extensive%20ablations%2C%20we%20find%20that%20common%20complexities%20such%20as%20curriculum%20learning%2C%20multiple%20audio%20encoders%2C%20and%20intricate%20cross-attention%20connectors%20are%20not%20required%20for%20strong%20performance%2C%20even%20compared%20to%20models%20trained%20on%20over%20500K%20hours%20of%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2509.07526v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompetitive%2520Audio-Language%2520Models%2520with%2520Data-Efficient%2520Single-Stage%2520Training%2520on%2520Public%2520Data%26entry.906535625%3DGokul%2520Karthik%2520Kumar%2520and%2520Rishabh%2520Saraf%2520and%2520Ludovick%2520Lepauloux%2520and%2520Abdul%2520Muneer%2520and%2520Billel%2520Mokeddem%2520and%2520Hakim%2520Hacid%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520transformed%2520NLP%252C%2520yet%2520their%2520integration%2520with%2520audio%2520remains%2520underexplored%2520despite%2520audio%2527s%2520centrality%2520to%2520human%2520communication.%2520We%2520introduce%2520Falcon3-Audio%252C%2520a%2520family%2520of%2520Audio-Language%2520Models%2520%2528ALMs%2529%2520built%2520on%2520instruction-tuned%2520LLMs%2520and%2520Whisper%2520encoders.%2520Using%2520a%2520remarkably%2520small%2520amount%2520of%2520public%2520audio%2520data%252C%2520less%2520than%252030K%2520hours%2520%25285K%2520unique%2529%252C%2520Falcon3-Audio-7B%2520matches%2520the%2520best%2520reported%2520performance%2520among%2520open-weight%2520models%2520on%2520the%2520MMAU%2520benchmark%252C%2520with%2520a%2520score%2520of%252064.14%252C%2520matching%2520R1-AQA%252C%2520while%2520distinguishing%2520itself%2520through%2520superior%2520data%2520and%2520parameter%2520efficiency%252C%2520single-stage%2520training%252C%2520and%2520transparency.%2520Notably%252C%2520our%2520smallest%25201B%2520model%2520remains%2520competitive%2520with%2520larger%2520open%2520models%2520ranging%2520from%25202B%2520to%252013B%2520parameters.%2520Through%2520extensive%2520ablations%252C%2520we%2520find%2520that%2520common%2520complexities%2520such%2520as%2520curriculum%2520learning%252C%2520multiple%2520audio%2520encoders%252C%2520and%2520intricate%2520cross-attention%2520connectors%2520are%2520not%2520required%2520for%2520strong%2520performance%252C%2520even%2520compared%2520to%2520models%2520trained%2520on%2520over%2520500K%2520hours%2520of%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07526v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Competitive%20Audio-Language%20Models%20with%20Data-Efficient%20Single-Stage%20Training%20on%20Public%20Data&entry.906535625=Gokul%20Karthik%20Kumar%20and%20Rishabh%20Saraf%20and%20Ludovick%20Lepauloux%20and%20Abdul%20Muneer%20and%20Billel%20Mokeddem%20and%20Hakim%20Hacid&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20transformed%20NLP%2C%20yet%20their%20integration%20with%20audio%20remains%20underexplored%20despite%20audio%27s%20centrality%20to%20human%20communication.%20We%20introduce%20Falcon3-Audio%2C%20a%20family%20of%20Audio-Language%20Models%20%28ALMs%29%20built%20on%20instruction-tuned%20LLMs%20and%20Whisper%20encoders.%20Using%20a%20remarkably%20small%20amount%20of%20public%20audio%20data%2C%20less%20than%2030K%20hours%20%285K%20unique%29%2C%20Falcon3-Audio-7B%20matches%20the%20best%20reported%20performance%20among%20open-weight%20models%20on%20the%20MMAU%20benchmark%2C%20with%20a%20score%20of%2064.14%2C%20matching%20R1-AQA%2C%20while%20distinguishing%20itself%20through%20superior%20data%20and%20parameter%20efficiency%2C%20single-stage%20training%2C%20and%20transparency.%20Notably%2C%20our%20smallest%201B%20model%20remains%20competitive%20with%20larger%20open%20models%20ranging%20from%202B%20to%2013B%20parameters.%20Through%20extensive%20ablations%2C%20we%20find%20that%20common%20complexities%20such%20as%20curriculum%20learning%2C%20multiple%20audio%20encoders%2C%20and%20intricate%20cross-attention%20connectors%20are%20not%20required%20for%20strong%20performance%2C%20even%20compared%20to%20models%20trained%20on%20over%20500K%20hours%20of%20data.&entry.1838667208=http%3A//arxiv.org/abs/2509.07526v2&entry.124074799=Read"},
{"title": "Knowledge Restoration-driven Prompt Optimization: Unlocking LLM Potential for Open-Domain Relational Triplet Extraction", "author": "Xiaonan Jing and Gongqing Wu and Xingrui Zhuo and Lang Sun and Jiapu Wang", "abstract": "Open-domain Relational Triplet Extraction (ORTE) is the foundation for mining structured knowledge without predefined schemas. Despite the impressive in-context learning capabilities of Large Language Models (LLMs), existing methods are hindered by their reliance on static, heuristic-driven prompting strategies. Due to the lack of reflection mechanisms required to internalize erroneous signals, these methods exhibit vulnerability in semantic ambiguity, often making erroneous extraction patterns permanent. To address this bottleneck, we propose a Knowledge Reconstruction-driven Prompt Optimization (KRPO) framework to assist LLMs in continuously improving their extraction capabilities for complex ORTE task flows. Specifically, we design a self-evaluation mechanism based on knowledge restoration, which provides intrinsic feedback signals by projecting structured triplets into semantic consistency scores. Subsequently, we propose a prompt optimizer based on a textual gradient that can internalize historical experiences to iteratively optimize prompts, which can better guide LLMs to handle subsequent extraction tasks. Furthermore, to alleviate relation redundancy, we design a relation canonicalization memory that collects representative relations and provides semantically distinct schemas for the triplets. Extensive experiments across three datasets show that KRPO significantly outperforms strong baselines in the extraction F1 score.", "link": "http://arxiv.org/abs/2601.15037v1", "date": "2026-01-21", "relevancy": 2.4805, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Restoration-driven%20Prompt%20Optimization%3A%20Unlocking%20LLM%20Potential%20for%20Open-Domain%20Relational%20Triplet%20Extraction&body=Title%3A%20Knowledge%20Restoration-driven%20Prompt%20Optimization%3A%20Unlocking%20LLM%20Potential%20for%20Open-Domain%20Relational%20Triplet%20Extraction%0AAuthor%3A%20Xiaonan%20Jing%20and%20Gongqing%20Wu%20and%20Xingrui%20Zhuo%20and%20Lang%20Sun%20and%20Jiapu%20Wang%0AAbstract%3A%20Open-domain%20Relational%20Triplet%20Extraction%20%28ORTE%29%20is%20the%20foundation%20for%20mining%20structured%20knowledge%20without%20predefined%20schemas.%20Despite%20the%20impressive%20in-context%20learning%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20existing%20methods%20are%20hindered%20by%20their%20reliance%20on%20static%2C%20heuristic-driven%20prompting%20strategies.%20Due%20to%20the%20lack%20of%20reflection%20mechanisms%20required%20to%20internalize%20erroneous%20signals%2C%20these%20methods%20exhibit%20vulnerability%20in%20semantic%20ambiguity%2C%20often%20making%20erroneous%20extraction%20patterns%20permanent.%20To%20address%20this%20bottleneck%2C%20we%20propose%20a%20Knowledge%20Reconstruction-driven%20Prompt%20Optimization%20%28KRPO%29%20framework%20to%20assist%20LLMs%20in%20continuously%20improving%20their%20extraction%20capabilities%20for%20complex%20ORTE%20task%20flows.%20Specifically%2C%20we%20design%20a%20self-evaluation%20mechanism%20based%20on%20knowledge%20restoration%2C%20which%20provides%20intrinsic%20feedback%20signals%20by%20projecting%20structured%20triplets%20into%20semantic%20consistency%20scores.%20Subsequently%2C%20we%20propose%20a%20prompt%20optimizer%20based%20on%20a%20textual%20gradient%20that%20can%20internalize%20historical%20experiences%20to%20iteratively%20optimize%20prompts%2C%20which%20can%20better%20guide%20LLMs%20to%20handle%20subsequent%20extraction%20tasks.%20Furthermore%2C%20to%20alleviate%20relation%20redundancy%2C%20we%20design%20a%20relation%20canonicalization%20memory%20that%20collects%20representative%20relations%20and%20provides%20semantically%20distinct%20schemas%20for%20the%20triplets.%20Extensive%20experiments%20across%20three%20datasets%20show%20that%20KRPO%20significantly%20outperforms%20strong%20baselines%20in%20the%20extraction%20F1%20score.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Restoration-driven%2520Prompt%2520Optimization%253A%2520Unlocking%2520LLM%2520Potential%2520for%2520Open-Domain%2520Relational%2520Triplet%2520Extraction%26entry.906535625%3DXiaonan%2520Jing%2520and%2520Gongqing%2520Wu%2520and%2520Xingrui%2520Zhuo%2520and%2520Lang%2520Sun%2520and%2520Jiapu%2520Wang%26entry.1292438233%3DOpen-domain%2520Relational%2520Triplet%2520Extraction%2520%2528ORTE%2529%2520is%2520the%2520foundation%2520for%2520mining%2520structured%2520knowledge%2520without%2520predefined%2520schemas.%2520Despite%2520the%2520impressive%2520in-context%2520learning%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520existing%2520methods%2520are%2520hindered%2520by%2520their%2520reliance%2520on%2520static%252C%2520heuristic-driven%2520prompting%2520strategies.%2520Due%2520to%2520the%2520lack%2520of%2520reflection%2520mechanisms%2520required%2520to%2520internalize%2520erroneous%2520signals%252C%2520these%2520methods%2520exhibit%2520vulnerability%2520in%2520semantic%2520ambiguity%252C%2520often%2520making%2520erroneous%2520extraction%2520patterns%2520permanent.%2520To%2520address%2520this%2520bottleneck%252C%2520we%2520propose%2520a%2520Knowledge%2520Reconstruction-driven%2520Prompt%2520Optimization%2520%2528KRPO%2529%2520framework%2520to%2520assist%2520LLMs%2520in%2520continuously%2520improving%2520their%2520extraction%2520capabilities%2520for%2520complex%2520ORTE%2520task%2520flows.%2520Specifically%252C%2520we%2520design%2520a%2520self-evaluation%2520mechanism%2520based%2520on%2520knowledge%2520restoration%252C%2520which%2520provides%2520intrinsic%2520feedback%2520signals%2520by%2520projecting%2520structured%2520triplets%2520into%2520semantic%2520consistency%2520scores.%2520Subsequently%252C%2520we%2520propose%2520a%2520prompt%2520optimizer%2520based%2520on%2520a%2520textual%2520gradient%2520that%2520can%2520internalize%2520historical%2520experiences%2520to%2520iteratively%2520optimize%2520prompts%252C%2520which%2520can%2520better%2520guide%2520LLMs%2520to%2520handle%2520subsequent%2520extraction%2520tasks.%2520Furthermore%252C%2520to%2520alleviate%2520relation%2520redundancy%252C%2520we%2520design%2520a%2520relation%2520canonicalization%2520memory%2520that%2520collects%2520representative%2520relations%2520and%2520provides%2520semantically%2520distinct%2520schemas%2520for%2520the%2520triplets.%2520Extensive%2520experiments%2520across%2520three%2520datasets%2520show%2520that%2520KRPO%2520significantly%2520outperforms%2520strong%2520baselines%2520in%2520the%2520extraction%2520F1%2520score.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Restoration-driven%20Prompt%20Optimization%3A%20Unlocking%20LLM%20Potential%20for%20Open-Domain%20Relational%20Triplet%20Extraction&entry.906535625=Xiaonan%20Jing%20and%20Gongqing%20Wu%20and%20Xingrui%20Zhuo%20and%20Lang%20Sun%20and%20Jiapu%20Wang&entry.1292438233=Open-domain%20Relational%20Triplet%20Extraction%20%28ORTE%29%20is%20the%20foundation%20for%20mining%20structured%20knowledge%20without%20predefined%20schemas.%20Despite%20the%20impressive%20in-context%20learning%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20existing%20methods%20are%20hindered%20by%20their%20reliance%20on%20static%2C%20heuristic-driven%20prompting%20strategies.%20Due%20to%20the%20lack%20of%20reflection%20mechanisms%20required%20to%20internalize%20erroneous%20signals%2C%20these%20methods%20exhibit%20vulnerability%20in%20semantic%20ambiguity%2C%20often%20making%20erroneous%20extraction%20patterns%20permanent.%20To%20address%20this%20bottleneck%2C%20we%20propose%20a%20Knowledge%20Reconstruction-driven%20Prompt%20Optimization%20%28KRPO%29%20framework%20to%20assist%20LLMs%20in%20continuously%20improving%20their%20extraction%20capabilities%20for%20complex%20ORTE%20task%20flows.%20Specifically%2C%20we%20design%20a%20self-evaluation%20mechanism%20based%20on%20knowledge%20restoration%2C%20which%20provides%20intrinsic%20feedback%20signals%20by%20projecting%20structured%20triplets%20into%20semantic%20consistency%20scores.%20Subsequently%2C%20we%20propose%20a%20prompt%20optimizer%20based%20on%20a%20textual%20gradient%20that%20can%20internalize%20historical%20experiences%20to%20iteratively%20optimize%20prompts%2C%20which%20can%20better%20guide%20LLMs%20to%20handle%20subsequent%20extraction%20tasks.%20Furthermore%2C%20to%20alleviate%20relation%20redundancy%2C%20we%20design%20a%20relation%20canonicalization%20memory%20that%20collects%20representative%20relations%20and%20provides%20semantically%20distinct%20schemas%20for%20the%20triplets.%20Extensive%20experiments%20across%20three%20datasets%20show%20that%20KRPO%20significantly%20outperforms%20strong%20baselines%20in%20the%20extraction%20F1%20score.&entry.1838667208=http%3A//arxiv.org/abs/2601.15037v1&entry.124074799=Read"},
{"title": "Walk through Paintings: Egocentric World Models from Internet Priors", "author": "Anurag Bagchi and Zhipeng Bao and Homanga Bharadhwaj and Yu-Xiong Wang and Pavel Tokmakov and Martial Hebert", "abstract": "What if a video generation model could not only imagine a plausible future, but the correct one, accurately reflecting how the world changes with each action? We address this question by presenting the Egocentric World Model (EgoWM), a simple, architecture-agnostic method that transforms any pretrained video diffusion model into an action-conditioned world model, enabling controllable future prediction. Rather than training from scratch, we repurpose the rich world priors of Internet-scale video models and inject motor commands through lightweight conditioning layers. This allows the model to follow actions faithfully while preserving realism and strong generalization. Our approach scales naturally across embodiments and action spaces, ranging from 3-DoF mobile robots to 25-DoF humanoids, where predicting egocentric joint-angle-driven dynamics is substantially more challenging. The model produces coherent rollouts for both navigation and manipulation tasks, requiring only modest fine-tuning. To evaluate physical correctness independently of visual appearance, we introduce the Structural Consistency Score (SCS), which measures whether stable scene elements evolve consistently with the provided actions. EgoWM improves SCS by up to 80 percent over prior state-of-the-art navigation world models, while achieving up to six times lower inference latency and robust generalization to unseen environments, including navigation inside paintings.", "link": "http://arxiv.org/abs/2601.15284v1", "date": "2026-01-21", "relevancy": 2.4746, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6412}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6222}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Walk%20through%20Paintings%3A%20Egocentric%20World%20Models%20from%20Internet%20Priors&body=Title%3A%20Walk%20through%20Paintings%3A%20Egocentric%20World%20Models%20from%20Internet%20Priors%0AAuthor%3A%20Anurag%20Bagchi%20and%20Zhipeng%20Bao%20and%20Homanga%20Bharadhwaj%20and%20Yu-Xiong%20Wang%20and%20Pavel%20Tokmakov%20and%20Martial%20Hebert%0AAbstract%3A%20What%20if%20a%20video%20generation%20model%20could%20not%20only%20imagine%20a%20plausible%20future%2C%20but%20the%20correct%20one%2C%20accurately%20reflecting%20how%20the%20world%20changes%20with%20each%20action%3F%20We%20address%20this%20question%20by%20presenting%20the%20Egocentric%20World%20Model%20%28EgoWM%29%2C%20a%20simple%2C%20architecture-agnostic%20method%20that%20transforms%20any%20pretrained%20video%20diffusion%20model%20into%20an%20action-conditioned%20world%20model%2C%20enabling%20controllable%20future%20prediction.%20Rather%20than%20training%20from%20scratch%2C%20we%20repurpose%20the%20rich%20world%20priors%20of%20Internet-scale%20video%20models%20and%20inject%20motor%20commands%20through%20lightweight%20conditioning%20layers.%20This%20allows%20the%20model%20to%20follow%20actions%20faithfully%20while%20preserving%20realism%20and%20strong%20generalization.%20Our%20approach%20scales%20naturally%20across%20embodiments%20and%20action%20spaces%2C%20ranging%20from%203-DoF%20mobile%20robots%20to%2025-DoF%20humanoids%2C%20where%20predicting%20egocentric%20joint-angle-driven%20dynamics%20is%20substantially%20more%20challenging.%20The%20model%20produces%20coherent%20rollouts%20for%20both%20navigation%20and%20manipulation%20tasks%2C%20requiring%20only%20modest%20fine-tuning.%20To%20evaluate%20physical%20correctness%20independently%20of%20visual%20appearance%2C%20we%20introduce%20the%20Structural%20Consistency%20Score%20%28SCS%29%2C%20which%20measures%20whether%20stable%20scene%20elements%20evolve%20consistently%20with%20the%20provided%20actions.%20EgoWM%20improves%20SCS%20by%20up%20to%2080%20percent%20over%20prior%20state-of-the-art%20navigation%20world%20models%2C%20while%20achieving%20up%20to%20six%20times%20lower%20inference%20latency%20and%20robust%20generalization%20to%20unseen%20environments%2C%20including%20navigation%20inside%20paintings.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15284v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWalk%2520through%2520Paintings%253A%2520Egocentric%2520World%2520Models%2520from%2520Internet%2520Priors%26entry.906535625%3DAnurag%2520Bagchi%2520and%2520Zhipeng%2520Bao%2520and%2520Homanga%2520Bharadhwaj%2520and%2520Yu-Xiong%2520Wang%2520and%2520Pavel%2520Tokmakov%2520and%2520Martial%2520Hebert%26entry.1292438233%3DWhat%2520if%2520a%2520video%2520generation%2520model%2520could%2520not%2520only%2520imagine%2520a%2520plausible%2520future%252C%2520but%2520the%2520correct%2520one%252C%2520accurately%2520reflecting%2520how%2520the%2520world%2520changes%2520with%2520each%2520action%253F%2520We%2520address%2520this%2520question%2520by%2520presenting%2520the%2520Egocentric%2520World%2520Model%2520%2528EgoWM%2529%252C%2520a%2520simple%252C%2520architecture-agnostic%2520method%2520that%2520transforms%2520any%2520pretrained%2520video%2520diffusion%2520model%2520into%2520an%2520action-conditioned%2520world%2520model%252C%2520enabling%2520controllable%2520future%2520prediction.%2520Rather%2520than%2520training%2520from%2520scratch%252C%2520we%2520repurpose%2520the%2520rich%2520world%2520priors%2520of%2520Internet-scale%2520video%2520models%2520and%2520inject%2520motor%2520commands%2520through%2520lightweight%2520conditioning%2520layers.%2520This%2520allows%2520the%2520model%2520to%2520follow%2520actions%2520faithfully%2520while%2520preserving%2520realism%2520and%2520strong%2520generalization.%2520Our%2520approach%2520scales%2520naturally%2520across%2520embodiments%2520and%2520action%2520spaces%252C%2520ranging%2520from%25203-DoF%2520mobile%2520robots%2520to%252025-DoF%2520humanoids%252C%2520where%2520predicting%2520egocentric%2520joint-angle-driven%2520dynamics%2520is%2520substantially%2520more%2520challenging.%2520The%2520model%2520produces%2520coherent%2520rollouts%2520for%2520both%2520navigation%2520and%2520manipulation%2520tasks%252C%2520requiring%2520only%2520modest%2520fine-tuning.%2520To%2520evaluate%2520physical%2520correctness%2520independently%2520of%2520visual%2520appearance%252C%2520we%2520introduce%2520the%2520Structural%2520Consistency%2520Score%2520%2528SCS%2529%252C%2520which%2520measures%2520whether%2520stable%2520scene%2520elements%2520evolve%2520consistently%2520with%2520the%2520provided%2520actions.%2520EgoWM%2520improves%2520SCS%2520by%2520up%2520to%252080%2520percent%2520over%2520prior%2520state-of-the-art%2520navigation%2520world%2520models%252C%2520while%2520achieving%2520up%2520to%2520six%2520times%2520lower%2520inference%2520latency%2520and%2520robust%2520generalization%2520to%2520unseen%2520environments%252C%2520including%2520navigation%2520inside%2520paintings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15284v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Walk%20through%20Paintings%3A%20Egocentric%20World%20Models%20from%20Internet%20Priors&entry.906535625=Anurag%20Bagchi%20and%20Zhipeng%20Bao%20and%20Homanga%20Bharadhwaj%20and%20Yu-Xiong%20Wang%20and%20Pavel%20Tokmakov%20and%20Martial%20Hebert&entry.1292438233=What%20if%20a%20video%20generation%20model%20could%20not%20only%20imagine%20a%20plausible%20future%2C%20but%20the%20correct%20one%2C%20accurately%20reflecting%20how%20the%20world%20changes%20with%20each%20action%3F%20We%20address%20this%20question%20by%20presenting%20the%20Egocentric%20World%20Model%20%28EgoWM%29%2C%20a%20simple%2C%20architecture-agnostic%20method%20that%20transforms%20any%20pretrained%20video%20diffusion%20model%20into%20an%20action-conditioned%20world%20model%2C%20enabling%20controllable%20future%20prediction.%20Rather%20than%20training%20from%20scratch%2C%20we%20repurpose%20the%20rich%20world%20priors%20of%20Internet-scale%20video%20models%20and%20inject%20motor%20commands%20through%20lightweight%20conditioning%20layers.%20This%20allows%20the%20model%20to%20follow%20actions%20faithfully%20while%20preserving%20realism%20and%20strong%20generalization.%20Our%20approach%20scales%20naturally%20across%20embodiments%20and%20action%20spaces%2C%20ranging%20from%203-DoF%20mobile%20robots%20to%2025-DoF%20humanoids%2C%20where%20predicting%20egocentric%20joint-angle-driven%20dynamics%20is%20substantially%20more%20challenging.%20The%20model%20produces%20coherent%20rollouts%20for%20both%20navigation%20and%20manipulation%20tasks%2C%20requiring%20only%20modest%20fine-tuning.%20To%20evaluate%20physical%20correctness%20independently%20of%20visual%20appearance%2C%20we%20introduce%20the%20Structural%20Consistency%20Score%20%28SCS%29%2C%20which%20measures%20whether%20stable%20scene%20elements%20evolve%20consistently%20with%20the%20provided%20actions.%20EgoWM%20improves%20SCS%20by%20up%20to%2080%20percent%20over%20prior%20state-of-the-art%20navigation%20world%20models%2C%20while%20achieving%20up%20to%20six%20times%20lower%20inference%20latency%20and%20robust%20generalization%20to%20unseen%20environments%2C%20including%20navigation%20inside%20paintings.&entry.1838667208=http%3A//arxiv.org/abs/2601.15284v1&entry.124074799=Read"},
{"title": "Towards Holistic Modeling for Video Frame Interpolation with Auto-regressive Diffusion Transformers", "author": "Xinyu Peng and Han Li and Yuyang Huang and Ziyang Zheng and Yaoming Wang and Xin Chen and Wenrui Dai and Chenglin Li and Junni Zou and Hongkai Xiong", "abstract": "Existing video frame interpolation (VFI) methods often adopt a frame-centric approach, processing videos as independent short segments (e.g., triplets), which leads to temporal inconsistencies and motion artifacts. To overcome this, we propose a holistic, video-centric paradigm named \\textbf{L}ocal \\textbf{D}iffusion \\textbf{F}orcing for \\textbf{V}ideo \\textbf{F}rame \\textbf{I}nterpolation (LDF-VFI). Our framework is built upon an auto-regressive diffusion transformer that models the entire video sequence to ensure long-range temporal coherence. To mitigate error accumulation inherent in auto-regressive generation, we introduce a novel skip-concatenate sampling strategy that effectively maintains temporal stability. Furthermore, LDF-VFI incorporates sparse, local attention and tiled VAE encoding, a combination that not only enables efficient processing of long sequences but also allows generalization to arbitrary spatial resolutions (e.g., 4K) at inference without retraining. An enhanced conditional VAE decoder, which leverages multi-scale features from the input video, further improves reconstruction fidelity. Empirically, LDF-VFI achieves state-of-the-art performance on challenging long-sequence benchmarks, demonstrating superior per-frame quality and temporal consistency, especially in scenes with large motion. The source code is available at https://github.com/xypeng9903/LDF-VFI.", "link": "http://arxiv.org/abs/2601.14959v1", "date": "2026-01-21", "relevancy": 2.4623, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.638}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6031}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Holistic%20Modeling%20for%20Video%20Frame%20Interpolation%20with%20Auto-regressive%20Diffusion%20Transformers&body=Title%3A%20Towards%20Holistic%20Modeling%20for%20Video%20Frame%20Interpolation%20with%20Auto-regressive%20Diffusion%20Transformers%0AAuthor%3A%20Xinyu%20Peng%20and%20Han%20Li%20and%20Yuyang%20Huang%20and%20Ziyang%20Zheng%20and%20Yaoming%20Wang%20and%20Xin%20Chen%20and%20Wenrui%20Dai%20and%20Chenglin%20Li%20and%20Junni%20Zou%20and%20Hongkai%20Xiong%0AAbstract%3A%20Existing%20video%20frame%20interpolation%20%28VFI%29%20methods%20often%20adopt%20a%20frame-centric%20approach%2C%20processing%20videos%20as%20independent%20short%20segments%20%28e.g.%2C%20triplets%29%2C%20which%20leads%20to%20temporal%20inconsistencies%20and%20motion%20artifacts.%20To%20overcome%20this%2C%20we%20propose%20a%20holistic%2C%20video-centric%20paradigm%20named%20%5Ctextbf%7BL%7Docal%20%5Ctextbf%7BD%7Diffusion%20%5Ctextbf%7BF%7Dorcing%20for%20%5Ctextbf%7BV%7Dideo%20%5Ctextbf%7BF%7Drame%20%5Ctextbf%7BI%7Dnterpolation%20%28LDF-VFI%29.%20Our%20framework%20is%20built%20upon%20an%20auto-regressive%20diffusion%20transformer%20that%20models%20the%20entire%20video%20sequence%20to%20ensure%20long-range%20temporal%20coherence.%20To%20mitigate%20error%20accumulation%20inherent%20in%20auto-regressive%20generation%2C%20we%20introduce%20a%20novel%20skip-concatenate%20sampling%20strategy%20that%20effectively%20maintains%20temporal%20stability.%20Furthermore%2C%20LDF-VFI%20incorporates%20sparse%2C%20local%20attention%20and%20tiled%20VAE%20encoding%2C%20a%20combination%20that%20not%20only%20enables%20efficient%20processing%20of%20long%20sequences%20but%20also%20allows%20generalization%20to%20arbitrary%20spatial%20resolutions%20%28e.g.%2C%204K%29%20at%20inference%20without%20retraining.%20An%20enhanced%20conditional%20VAE%20decoder%2C%20which%20leverages%20multi-scale%20features%20from%20the%20input%20video%2C%20further%20improves%20reconstruction%20fidelity.%20Empirically%2C%20LDF-VFI%20achieves%20state-of-the-art%20performance%20on%20challenging%20long-sequence%20benchmarks%2C%20demonstrating%20superior%20per-frame%20quality%20and%20temporal%20consistency%2C%20especially%20in%20scenes%20with%20large%20motion.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/xypeng9903/LDF-VFI.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Holistic%2520Modeling%2520for%2520Video%2520Frame%2520Interpolation%2520with%2520Auto-regressive%2520Diffusion%2520Transformers%26entry.906535625%3DXinyu%2520Peng%2520and%2520Han%2520Li%2520and%2520Yuyang%2520Huang%2520and%2520Ziyang%2520Zheng%2520and%2520Yaoming%2520Wang%2520and%2520Xin%2520Chen%2520and%2520Wenrui%2520Dai%2520and%2520Chenglin%2520Li%2520and%2520Junni%2520Zou%2520and%2520Hongkai%2520Xiong%26entry.1292438233%3DExisting%2520video%2520frame%2520interpolation%2520%2528VFI%2529%2520methods%2520often%2520adopt%2520a%2520frame-centric%2520approach%252C%2520processing%2520videos%2520as%2520independent%2520short%2520segments%2520%2528e.g.%252C%2520triplets%2529%252C%2520which%2520leads%2520to%2520temporal%2520inconsistencies%2520and%2520motion%2520artifacts.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520a%2520holistic%252C%2520video-centric%2520paradigm%2520named%2520%255Ctextbf%257BL%257Docal%2520%255Ctextbf%257BD%257Diffusion%2520%255Ctextbf%257BF%257Dorcing%2520for%2520%255Ctextbf%257BV%257Dideo%2520%255Ctextbf%257BF%257Drame%2520%255Ctextbf%257BI%257Dnterpolation%2520%2528LDF-VFI%2529.%2520Our%2520framework%2520is%2520built%2520upon%2520an%2520auto-regressive%2520diffusion%2520transformer%2520that%2520models%2520the%2520entire%2520video%2520sequence%2520to%2520ensure%2520long-range%2520temporal%2520coherence.%2520To%2520mitigate%2520error%2520accumulation%2520inherent%2520in%2520auto-regressive%2520generation%252C%2520we%2520introduce%2520a%2520novel%2520skip-concatenate%2520sampling%2520strategy%2520that%2520effectively%2520maintains%2520temporal%2520stability.%2520Furthermore%252C%2520LDF-VFI%2520incorporates%2520sparse%252C%2520local%2520attention%2520and%2520tiled%2520VAE%2520encoding%252C%2520a%2520combination%2520that%2520not%2520only%2520enables%2520efficient%2520processing%2520of%2520long%2520sequences%2520but%2520also%2520allows%2520generalization%2520to%2520arbitrary%2520spatial%2520resolutions%2520%2528e.g.%252C%25204K%2529%2520at%2520inference%2520without%2520retraining.%2520An%2520enhanced%2520conditional%2520VAE%2520decoder%252C%2520which%2520leverages%2520multi-scale%2520features%2520from%2520the%2520input%2520video%252C%2520further%2520improves%2520reconstruction%2520fidelity.%2520Empirically%252C%2520LDF-VFI%2520achieves%2520state-of-the-art%2520performance%2520on%2520challenging%2520long-sequence%2520benchmarks%252C%2520demonstrating%2520superior%2520per-frame%2520quality%2520and%2520temporal%2520consistency%252C%2520especially%2520in%2520scenes%2520with%2520large%2520motion.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/xypeng9903/LDF-VFI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Holistic%20Modeling%20for%20Video%20Frame%20Interpolation%20with%20Auto-regressive%20Diffusion%20Transformers&entry.906535625=Xinyu%20Peng%20and%20Han%20Li%20and%20Yuyang%20Huang%20and%20Ziyang%20Zheng%20and%20Yaoming%20Wang%20and%20Xin%20Chen%20and%20Wenrui%20Dai%20and%20Chenglin%20Li%20and%20Junni%20Zou%20and%20Hongkai%20Xiong&entry.1292438233=Existing%20video%20frame%20interpolation%20%28VFI%29%20methods%20often%20adopt%20a%20frame-centric%20approach%2C%20processing%20videos%20as%20independent%20short%20segments%20%28e.g.%2C%20triplets%29%2C%20which%20leads%20to%20temporal%20inconsistencies%20and%20motion%20artifacts.%20To%20overcome%20this%2C%20we%20propose%20a%20holistic%2C%20video-centric%20paradigm%20named%20%5Ctextbf%7BL%7Docal%20%5Ctextbf%7BD%7Diffusion%20%5Ctextbf%7BF%7Dorcing%20for%20%5Ctextbf%7BV%7Dideo%20%5Ctextbf%7BF%7Drame%20%5Ctextbf%7BI%7Dnterpolation%20%28LDF-VFI%29.%20Our%20framework%20is%20built%20upon%20an%20auto-regressive%20diffusion%20transformer%20that%20models%20the%20entire%20video%20sequence%20to%20ensure%20long-range%20temporal%20coherence.%20To%20mitigate%20error%20accumulation%20inherent%20in%20auto-regressive%20generation%2C%20we%20introduce%20a%20novel%20skip-concatenate%20sampling%20strategy%20that%20effectively%20maintains%20temporal%20stability.%20Furthermore%2C%20LDF-VFI%20incorporates%20sparse%2C%20local%20attention%20and%20tiled%20VAE%20encoding%2C%20a%20combination%20that%20not%20only%20enables%20efficient%20processing%20of%20long%20sequences%20but%20also%20allows%20generalization%20to%20arbitrary%20spatial%20resolutions%20%28e.g.%2C%204K%29%20at%20inference%20without%20retraining.%20An%20enhanced%20conditional%20VAE%20decoder%2C%20which%20leverages%20multi-scale%20features%20from%20the%20input%20video%2C%20further%20improves%20reconstruction%20fidelity.%20Empirically%2C%20LDF-VFI%20achieves%20state-of-the-art%20performance%20on%20challenging%20long-sequence%20benchmarks%2C%20demonstrating%20superior%20per-frame%20quality%20and%20temporal%20consistency%2C%20especially%20in%20scenes%20with%20large%20motion.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/xypeng9903/LDF-VFI.&entry.1838667208=http%3A//arxiv.org/abs/2601.14959v1&entry.124074799=Read"},
{"title": "Complexity-aware fine-tuning", "author": "Andrey Goncharov and Daniil Vyazhev and Petr Sychev and Edvard Khalafyan and Alexey Zaytsev", "abstract": "General-purpose Large Language Models (LLMs) are frequently fine-tuned through supervised fine-tuning (SFT) to enhance performance in specific domains. Better results can be achieved by distilling the chain-of-thought of a larger model at the cost of numerous expensive calls and a much greater amount of data. We propose a novel blueprint for efficient fine-tuning that uses reasoning only for complex data identified by entropy. Specifically, across three small open models ($\\approx 3B$) we split the training data into complexity categories by a single token answer entropy (ROC AUC $0.73$), fine-tune large language models (LLMs) via SFT and distillation, and show that our pipeline significantly outperforms the standard SFT approach ($0.58$ vs $0.45$ average accuracy) and outperforms the distillation approach ($0.58$ vs $0.56$ average accuracy) while using $81\\%$ less data.", "link": "http://arxiv.org/abs/2506.21220v3", "date": "2026-01-21", "relevancy": 2.4447, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4918}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4875}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Complexity-aware%20fine-tuning&body=Title%3A%20Complexity-aware%20fine-tuning%0AAuthor%3A%20Andrey%20Goncharov%20and%20Daniil%20Vyazhev%20and%20Petr%20Sychev%20and%20Edvard%20Khalafyan%20and%20Alexey%20Zaytsev%0AAbstract%3A%20General-purpose%20Large%20Language%20Models%20%28LLMs%29%20are%20frequently%20fine-tuned%20through%20supervised%20fine-tuning%20%28SFT%29%20to%20enhance%20performance%20in%20specific%20domains.%20Better%20results%20can%20be%20achieved%20by%20distilling%20the%20chain-of-thought%20of%20a%20larger%20model%20at%20the%20cost%20of%20numerous%20expensive%20calls%20and%20a%20much%20greater%20amount%20of%20data.%20We%20propose%20a%20novel%20blueprint%20for%20efficient%20fine-tuning%20that%20uses%20reasoning%20only%20for%20complex%20data%20identified%20by%20entropy.%20Specifically%2C%20across%20three%20small%20open%20models%20%28%24%5Capprox%203B%24%29%20we%20split%20the%20training%20data%20into%20complexity%20categories%20by%20a%20single%20token%20answer%20entropy%20%28ROC%20AUC%20%240.73%24%29%2C%20fine-tune%20large%20language%20models%20%28LLMs%29%20via%20SFT%20and%20distillation%2C%20and%20show%20that%20our%20pipeline%20significantly%20outperforms%20the%20standard%20SFT%20approach%20%28%240.58%24%20vs%20%240.45%24%20average%20accuracy%29%20and%20outperforms%20the%20distillation%20approach%20%28%240.58%24%20vs%20%240.56%24%20average%20accuracy%29%20while%20using%20%2481%5C%25%24%20less%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2506.21220v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComplexity-aware%2520fine-tuning%26entry.906535625%3DAndrey%2520Goncharov%2520and%2520Daniil%2520Vyazhev%2520and%2520Petr%2520Sychev%2520and%2520Edvard%2520Khalafyan%2520and%2520Alexey%2520Zaytsev%26entry.1292438233%3DGeneral-purpose%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520frequently%2520fine-tuned%2520through%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520to%2520enhance%2520performance%2520in%2520specific%2520domains.%2520Better%2520results%2520can%2520be%2520achieved%2520by%2520distilling%2520the%2520chain-of-thought%2520of%2520a%2520larger%2520model%2520at%2520the%2520cost%2520of%2520numerous%2520expensive%2520calls%2520and%2520a%2520much%2520greater%2520amount%2520of%2520data.%2520We%2520propose%2520a%2520novel%2520blueprint%2520for%2520efficient%2520fine-tuning%2520that%2520uses%2520reasoning%2520only%2520for%2520complex%2520data%2520identified%2520by%2520entropy.%2520Specifically%252C%2520across%2520three%2520small%2520open%2520models%2520%2528%2524%255Capprox%25203B%2524%2529%2520we%2520split%2520the%2520training%2520data%2520into%2520complexity%2520categories%2520by%2520a%2520single%2520token%2520answer%2520entropy%2520%2528ROC%2520AUC%2520%25240.73%2524%2529%252C%2520fine-tune%2520large%2520language%2520models%2520%2528LLMs%2529%2520via%2520SFT%2520and%2520distillation%252C%2520and%2520show%2520that%2520our%2520pipeline%2520significantly%2520outperforms%2520the%2520standard%2520SFT%2520approach%2520%2528%25240.58%2524%2520vs%2520%25240.45%2524%2520average%2520accuracy%2529%2520and%2520outperforms%2520the%2520distillation%2520approach%2520%2528%25240.58%2524%2520vs%2520%25240.56%2524%2520average%2520accuracy%2529%2520while%2520using%2520%252481%255C%2525%2524%2520less%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21220v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Complexity-aware%20fine-tuning&entry.906535625=Andrey%20Goncharov%20and%20Daniil%20Vyazhev%20and%20Petr%20Sychev%20and%20Edvard%20Khalafyan%20and%20Alexey%20Zaytsev&entry.1292438233=General-purpose%20Large%20Language%20Models%20%28LLMs%29%20are%20frequently%20fine-tuned%20through%20supervised%20fine-tuning%20%28SFT%29%20to%20enhance%20performance%20in%20specific%20domains.%20Better%20results%20can%20be%20achieved%20by%20distilling%20the%20chain-of-thought%20of%20a%20larger%20model%20at%20the%20cost%20of%20numerous%20expensive%20calls%20and%20a%20much%20greater%20amount%20of%20data.%20We%20propose%20a%20novel%20blueprint%20for%20efficient%20fine-tuning%20that%20uses%20reasoning%20only%20for%20complex%20data%20identified%20by%20entropy.%20Specifically%2C%20across%20three%20small%20open%20models%20%28%24%5Capprox%203B%24%29%20we%20split%20the%20training%20data%20into%20complexity%20categories%20by%20a%20single%20token%20answer%20entropy%20%28ROC%20AUC%20%240.73%24%29%2C%20fine-tune%20large%20language%20models%20%28LLMs%29%20via%20SFT%20and%20distillation%2C%20and%20show%20that%20our%20pipeline%20significantly%20outperforms%20the%20standard%20SFT%20approach%20%28%240.58%24%20vs%20%240.45%24%20average%20accuracy%29%20and%20outperforms%20the%20distillation%20approach%20%28%240.58%24%20vs%20%240.56%24%20average%20accuracy%29%20while%20using%20%2481%5C%25%24%20less%20data.&entry.1838667208=http%3A//arxiv.org/abs/2506.21220v3&entry.124074799=Read"},
{"title": "HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation", "author": "Yara Mahmoud and Yasheerah Yaqoot and Miguel Altamirano Cabrera and Dzmitry Tsetserukou", "abstract": "Humanoid robots must adapt their contact behavior to diverse objects and tasks, yet most controllers rely on fixed, hand-tuned impedance gains and gripper settings. This paper introduces HumanoidVLM, a vision-language driven retrieval framework that enables the Unitree G1 humanoid to select task-appropriate Cartesian impedance parameters and gripper configurations directly from an egocentric RGB image. The system couples a vision-language model for semantic task inference with a FAISS-based Retrieval-Augmented Generation (RAG) module that retrieves experimentally validated stiffness-damping pairs and object-specific grasp angles from two custom databases, and executes them through a task-space impedance controller for compliant manipulation. We evaluate HumanoidVLM on 14 visual scenarios and achieve a retrieval accuracy of 93%. Real-world experiments show stable interaction dynamics, with z-axis tracking errors typically within 1-3.5 cm and virtual forces consistent with task-dependent impedance settings. These results demonstrate the feasibility of linking semantic perception with retrieval-based control as an interpretable path toward adaptive humanoid manipulation.", "link": "http://arxiv.org/abs/2601.14874v1", "date": "2026-01-21", "relevancy": 2.431, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6384}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5938}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanoidVLM%3A%20Vision-Language-Guided%20Impedance%20Control%20for%20Contact-Rich%20Humanoid%20Manipulation&body=Title%3A%20HumanoidVLM%3A%20Vision-Language-Guided%20Impedance%20Control%20for%20Contact-Rich%20Humanoid%20Manipulation%0AAuthor%3A%20Yara%20Mahmoud%20and%20Yasheerah%20Yaqoot%20and%20Miguel%20Altamirano%20Cabrera%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20Humanoid%20robots%20must%20adapt%20their%20contact%20behavior%20to%20diverse%20objects%20and%20tasks%2C%20yet%20most%20controllers%20rely%20on%20fixed%2C%20hand-tuned%20impedance%20gains%20and%20gripper%20settings.%20This%20paper%20introduces%20HumanoidVLM%2C%20a%20vision-language%20driven%20retrieval%20framework%20that%20enables%20the%20Unitree%20G1%20humanoid%20to%20select%20task-appropriate%20Cartesian%20impedance%20parameters%20and%20gripper%20configurations%20directly%20from%20an%20egocentric%20RGB%20image.%20The%20system%20couples%20a%20vision-language%20model%20for%20semantic%20task%20inference%20with%20a%20FAISS-based%20Retrieval-Augmented%20Generation%20%28RAG%29%20module%20that%20retrieves%20experimentally%20validated%20stiffness-damping%20pairs%20and%20object-specific%20grasp%20angles%20from%20two%20custom%20databases%2C%20and%20executes%20them%20through%20a%20task-space%20impedance%20controller%20for%20compliant%20manipulation.%20We%20evaluate%20HumanoidVLM%20on%2014%20visual%20scenarios%20and%20achieve%20a%20retrieval%20accuracy%20of%2093%25.%20Real-world%20experiments%20show%20stable%20interaction%20dynamics%2C%20with%20z-axis%20tracking%20errors%20typically%20within%201-3.5%20cm%20and%20virtual%20forces%20consistent%20with%20task-dependent%20impedance%20settings.%20These%20results%20demonstrate%20the%20feasibility%20of%20linking%20semantic%20perception%20with%20retrieval-based%20control%20as%20an%20interpretable%20path%20toward%20adaptive%20humanoid%20manipulation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanoidVLM%253A%2520Vision-Language-Guided%2520Impedance%2520Control%2520for%2520Contact-Rich%2520Humanoid%2520Manipulation%26entry.906535625%3DYara%2520Mahmoud%2520and%2520Yasheerah%2520Yaqoot%2520and%2520Miguel%2520Altamirano%2520Cabrera%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3DHumanoid%2520robots%2520must%2520adapt%2520their%2520contact%2520behavior%2520to%2520diverse%2520objects%2520and%2520tasks%252C%2520yet%2520most%2520controllers%2520rely%2520on%2520fixed%252C%2520hand-tuned%2520impedance%2520gains%2520and%2520gripper%2520settings.%2520This%2520paper%2520introduces%2520HumanoidVLM%252C%2520a%2520vision-language%2520driven%2520retrieval%2520framework%2520that%2520enables%2520the%2520Unitree%2520G1%2520humanoid%2520to%2520select%2520task-appropriate%2520Cartesian%2520impedance%2520parameters%2520and%2520gripper%2520configurations%2520directly%2520from%2520an%2520egocentric%2520RGB%2520image.%2520The%2520system%2520couples%2520a%2520vision-language%2520model%2520for%2520semantic%2520task%2520inference%2520with%2520a%2520FAISS-based%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520module%2520that%2520retrieves%2520experimentally%2520validated%2520stiffness-damping%2520pairs%2520and%2520object-specific%2520grasp%2520angles%2520from%2520two%2520custom%2520databases%252C%2520and%2520executes%2520them%2520through%2520a%2520task-space%2520impedance%2520controller%2520for%2520compliant%2520manipulation.%2520We%2520evaluate%2520HumanoidVLM%2520on%252014%2520visual%2520scenarios%2520and%2520achieve%2520a%2520retrieval%2520accuracy%2520of%252093%2525.%2520Real-world%2520experiments%2520show%2520stable%2520interaction%2520dynamics%252C%2520with%2520z-axis%2520tracking%2520errors%2520typically%2520within%25201-3.5%2520cm%2520and%2520virtual%2520forces%2520consistent%2520with%2520task-dependent%2520impedance%2520settings.%2520These%2520results%2520demonstrate%2520the%2520feasibility%2520of%2520linking%2520semantic%2520perception%2520with%2520retrieval-based%2520control%2520as%2520an%2520interpretable%2520path%2520toward%2520adaptive%2520humanoid%2520manipulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanoidVLM%3A%20Vision-Language-Guided%20Impedance%20Control%20for%20Contact-Rich%20Humanoid%20Manipulation&entry.906535625=Yara%20Mahmoud%20and%20Yasheerah%20Yaqoot%20and%20Miguel%20Altamirano%20Cabrera%20and%20Dzmitry%20Tsetserukou&entry.1292438233=Humanoid%20robots%20must%20adapt%20their%20contact%20behavior%20to%20diverse%20objects%20and%20tasks%2C%20yet%20most%20controllers%20rely%20on%20fixed%2C%20hand-tuned%20impedance%20gains%20and%20gripper%20settings.%20This%20paper%20introduces%20HumanoidVLM%2C%20a%20vision-language%20driven%20retrieval%20framework%20that%20enables%20the%20Unitree%20G1%20humanoid%20to%20select%20task-appropriate%20Cartesian%20impedance%20parameters%20and%20gripper%20configurations%20directly%20from%20an%20egocentric%20RGB%20image.%20The%20system%20couples%20a%20vision-language%20model%20for%20semantic%20task%20inference%20with%20a%20FAISS-based%20Retrieval-Augmented%20Generation%20%28RAG%29%20module%20that%20retrieves%20experimentally%20validated%20stiffness-damping%20pairs%20and%20object-specific%20grasp%20angles%20from%20two%20custom%20databases%2C%20and%20executes%20them%20through%20a%20task-space%20impedance%20controller%20for%20compliant%20manipulation.%20We%20evaluate%20HumanoidVLM%20on%2014%20visual%20scenarios%20and%20achieve%20a%20retrieval%20accuracy%20of%2093%25.%20Real-world%20experiments%20show%20stable%20interaction%20dynamics%2C%20with%20z-axis%20tracking%20errors%20typically%20within%201-3.5%20cm%20and%20virtual%20forces%20consistent%20with%20task-dependent%20impedance%20settings.%20These%20results%20demonstrate%20the%20feasibility%20of%20linking%20semantic%20perception%20with%20retrieval-based%20control%20as%20an%20interpretable%20path%20toward%20adaptive%20humanoid%20manipulation.&entry.1838667208=http%3A//arxiv.org/abs/2601.14874v1&entry.124074799=Read"},
{"title": "StableWorld: Towards Stable and Consistent Long Interactive Video Generation", "author": "Ying Yang and Zhengyao Lv and Tianlin Pan and Haofan Wang and Binxin Yang and Hubery Yin and Chen Li and Ziwei Liu and Chenyang Si", "abstract": "In this paper, we explore the overlooked challenge of stability and temporal consistency in interactive video generation, which synthesizes dynamic and controllable video worlds through interactive behaviors such as camera movements and text prompts. Despite remarkable progress in world modeling, current methods still suffer from severe instability and temporal degradation, often leading to spatial drift and scene collapse during long-horizon interactions. To better understand this issue, we initially investigate the underlying causes of instability and identify that the major source of error accumulation originates from the same scene, where generated frames gradually deviate from the initial clean state and propagate errors to subsequent frames. Building upon this observation, we propose a simple yet effective method, \\textbf{StableWorld}, a Dynamic Frame Eviction Mechanism. By continuously filtering out degraded frames while retaining geometrically consistent ones, StableWorld effectively prevents cumulative drift at its source, leading to more stable and temporal consistency of interactive generation. Promising results on multiple interactive video models, \\eg, Matrix-Game, Open-Oasis, and Hunyuan-GameCraft, demonstrate that StableWorld is model-agnostic and can be applied to different interactive video generation frameworks to substantially improve stability, temporal consistency, and generalization across diverse interactive scenarios.", "link": "http://arxiv.org/abs/2601.15281v1", "date": "2026-01-21", "relevancy": 2.4192, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6624}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6001}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StableWorld%3A%20Towards%20Stable%20and%20Consistent%20Long%20Interactive%20Video%20Generation&body=Title%3A%20StableWorld%3A%20Towards%20Stable%20and%20Consistent%20Long%20Interactive%20Video%20Generation%0AAuthor%3A%20Ying%20Yang%20and%20Zhengyao%20Lv%20and%20Tianlin%20Pan%20and%20Haofan%20Wang%20and%20Binxin%20Yang%20and%20Hubery%20Yin%20and%20Chen%20Li%20and%20Ziwei%20Liu%20and%20Chenyang%20Si%0AAbstract%3A%20In%20this%20paper%2C%20we%20explore%20the%20overlooked%20challenge%20of%20stability%20and%20temporal%20consistency%20in%20interactive%20video%20generation%2C%20which%20synthesizes%20dynamic%20and%20controllable%20video%20worlds%20through%20interactive%20behaviors%20such%20as%20camera%20movements%20and%20text%20prompts.%20Despite%20remarkable%20progress%20in%20world%20modeling%2C%20current%20methods%20still%20suffer%20from%20severe%20instability%20and%20temporal%20degradation%2C%20often%20leading%20to%20spatial%20drift%20and%20scene%20collapse%20during%20long-horizon%20interactions.%20To%20better%20understand%20this%20issue%2C%20we%20initially%20investigate%20the%20underlying%20causes%20of%20instability%20and%20identify%20that%20the%20major%20source%20of%20error%20accumulation%20originates%20from%20the%20same%20scene%2C%20where%20generated%20frames%20gradually%20deviate%20from%20the%20initial%20clean%20state%20and%20propagate%20errors%20to%20subsequent%20frames.%20Building%20upon%20this%20observation%2C%20we%20propose%20a%20simple%20yet%20effective%20method%2C%20%5Ctextbf%7BStableWorld%7D%2C%20a%20Dynamic%20Frame%20Eviction%20Mechanism.%20By%20continuously%20filtering%20out%20degraded%20frames%20while%20retaining%20geometrically%20consistent%20ones%2C%20StableWorld%20effectively%20prevents%20cumulative%20drift%20at%20its%20source%2C%20leading%20to%20more%20stable%20and%20temporal%20consistency%20of%20interactive%20generation.%20Promising%20results%20on%20multiple%20interactive%20video%20models%2C%20%5Ceg%2C%20Matrix-Game%2C%20Open-Oasis%2C%20and%20Hunyuan-GameCraft%2C%20demonstrate%20that%20StableWorld%20is%20model-agnostic%20and%20can%20be%20applied%20to%20different%20interactive%20video%20generation%20frameworks%20to%20substantially%20improve%20stability%2C%20temporal%20consistency%2C%20and%20generalization%20across%20diverse%20interactive%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15281v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStableWorld%253A%2520Towards%2520Stable%2520and%2520Consistent%2520Long%2520Interactive%2520Video%2520Generation%26entry.906535625%3DYing%2520Yang%2520and%2520Zhengyao%2520Lv%2520and%2520Tianlin%2520Pan%2520and%2520Haofan%2520Wang%2520and%2520Binxin%2520Yang%2520and%2520Hubery%2520Yin%2520and%2520Chen%2520Li%2520and%2520Ziwei%2520Liu%2520and%2520Chenyang%2520Si%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520explore%2520the%2520overlooked%2520challenge%2520of%2520stability%2520and%2520temporal%2520consistency%2520in%2520interactive%2520video%2520generation%252C%2520which%2520synthesizes%2520dynamic%2520and%2520controllable%2520video%2520worlds%2520through%2520interactive%2520behaviors%2520such%2520as%2520camera%2520movements%2520and%2520text%2520prompts.%2520Despite%2520remarkable%2520progress%2520in%2520world%2520modeling%252C%2520current%2520methods%2520still%2520suffer%2520from%2520severe%2520instability%2520and%2520temporal%2520degradation%252C%2520often%2520leading%2520to%2520spatial%2520drift%2520and%2520scene%2520collapse%2520during%2520long-horizon%2520interactions.%2520To%2520better%2520understand%2520this%2520issue%252C%2520we%2520initially%2520investigate%2520the%2520underlying%2520causes%2520of%2520instability%2520and%2520identify%2520that%2520the%2520major%2520source%2520of%2520error%2520accumulation%2520originates%2520from%2520the%2520same%2520scene%252C%2520where%2520generated%2520frames%2520gradually%2520deviate%2520from%2520the%2520initial%2520clean%2520state%2520and%2520propagate%2520errors%2520to%2520subsequent%2520frames.%2520Building%2520upon%2520this%2520observation%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520method%252C%2520%255Ctextbf%257BStableWorld%257D%252C%2520a%2520Dynamic%2520Frame%2520Eviction%2520Mechanism.%2520By%2520continuously%2520filtering%2520out%2520degraded%2520frames%2520while%2520retaining%2520geometrically%2520consistent%2520ones%252C%2520StableWorld%2520effectively%2520prevents%2520cumulative%2520drift%2520at%2520its%2520source%252C%2520leading%2520to%2520more%2520stable%2520and%2520temporal%2520consistency%2520of%2520interactive%2520generation.%2520Promising%2520results%2520on%2520multiple%2520interactive%2520video%2520models%252C%2520%255Ceg%252C%2520Matrix-Game%252C%2520Open-Oasis%252C%2520and%2520Hunyuan-GameCraft%252C%2520demonstrate%2520that%2520StableWorld%2520is%2520model-agnostic%2520and%2520can%2520be%2520applied%2520to%2520different%2520interactive%2520video%2520generation%2520frameworks%2520to%2520substantially%2520improve%2520stability%252C%2520temporal%2520consistency%252C%2520and%2520generalization%2520across%2520diverse%2520interactive%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15281v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StableWorld%3A%20Towards%20Stable%20and%20Consistent%20Long%20Interactive%20Video%20Generation&entry.906535625=Ying%20Yang%20and%20Zhengyao%20Lv%20and%20Tianlin%20Pan%20and%20Haofan%20Wang%20and%20Binxin%20Yang%20and%20Hubery%20Yin%20and%20Chen%20Li%20and%20Ziwei%20Liu%20and%20Chenyang%20Si&entry.1292438233=In%20this%20paper%2C%20we%20explore%20the%20overlooked%20challenge%20of%20stability%20and%20temporal%20consistency%20in%20interactive%20video%20generation%2C%20which%20synthesizes%20dynamic%20and%20controllable%20video%20worlds%20through%20interactive%20behaviors%20such%20as%20camera%20movements%20and%20text%20prompts.%20Despite%20remarkable%20progress%20in%20world%20modeling%2C%20current%20methods%20still%20suffer%20from%20severe%20instability%20and%20temporal%20degradation%2C%20often%20leading%20to%20spatial%20drift%20and%20scene%20collapse%20during%20long-horizon%20interactions.%20To%20better%20understand%20this%20issue%2C%20we%20initially%20investigate%20the%20underlying%20causes%20of%20instability%20and%20identify%20that%20the%20major%20source%20of%20error%20accumulation%20originates%20from%20the%20same%20scene%2C%20where%20generated%20frames%20gradually%20deviate%20from%20the%20initial%20clean%20state%20and%20propagate%20errors%20to%20subsequent%20frames.%20Building%20upon%20this%20observation%2C%20we%20propose%20a%20simple%20yet%20effective%20method%2C%20%5Ctextbf%7BStableWorld%7D%2C%20a%20Dynamic%20Frame%20Eviction%20Mechanism.%20By%20continuously%20filtering%20out%20degraded%20frames%20while%20retaining%20geometrically%20consistent%20ones%2C%20StableWorld%20effectively%20prevents%20cumulative%20drift%20at%20its%20source%2C%20leading%20to%20more%20stable%20and%20temporal%20consistency%20of%20interactive%20generation.%20Promising%20results%20on%20multiple%20interactive%20video%20models%2C%20%5Ceg%2C%20Matrix-Game%2C%20Open-Oasis%2C%20and%20Hunyuan-GameCraft%2C%20demonstrate%20that%20StableWorld%20is%20model-agnostic%20and%20can%20be%20applied%20to%20different%20interactive%20video%20generation%20frameworks%20to%20substantially%20improve%20stability%2C%20temporal%20consistency%2C%20and%20generalization%20across%20diverse%20interactive%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2601.15281v1&entry.124074799=Read"},
{"title": "Cluster-Based Generalized Additive Models Informed by Random Fourier Features", "author": "Xin Huang and Jia Li and Jun Yu", "abstract": "In the development of learning systems, there is an ongoing need to reconcile the strong predictive performance offered by opaque black-box models with the level of transparency required for critical applications. This work introduces a methodological framework that combines spectral representation learning with transparent statistical modeling to construct a mixture of generalized additive models (GAMs). The approach utilizes random Fourier feature embeddings to uncover locally adaptive structures within the data. High-dimensional random feature representations are compressed via principal component analysis to derive a latent space that informs a Gaussian mixture model, which performs soft clustering to partition the input space into distinct regimes. Within each cluster, a local GAM captures nonlinear univariate effects through interpretable spline-based smoothers. Numerical experiments across diverse regression benchmarks demonstrate that the proposed method consistently improves upon classical global interpretable models by effectively modeling data heterogeneity. Furthermore, the mixture-of-GAMs framework achieves performance comparable to explainable boosting machine, random forest, and multilayer perceptron on certain tasks. Overall, this construction provides a principled approach for integrating representation learning with transparent statistical modeling.", "link": "http://arxiv.org/abs/2512.19373v2", "date": "2026-01-21", "relevancy": 2.4191, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4874}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4834}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cluster-Based%20Generalized%20Additive%20Models%20Informed%20by%20Random%20Fourier%20Features&body=Title%3A%20Cluster-Based%20Generalized%20Additive%20Models%20Informed%20by%20Random%20Fourier%20Features%0AAuthor%3A%20Xin%20Huang%20and%20Jia%20Li%20and%20Jun%20Yu%0AAbstract%3A%20In%20the%20development%20of%20learning%20systems%2C%20there%20is%20an%20ongoing%20need%20to%20reconcile%20the%20strong%20predictive%20performance%20offered%20by%20opaque%20black-box%20models%20with%20the%20level%20of%20transparency%20required%20for%20critical%20applications.%20This%20work%20introduces%20a%20methodological%20framework%20that%20combines%20spectral%20representation%20learning%20with%20transparent%20statistical%20modeling%20to%20construct%20a%20mixture%20of%20generalized%20additive%20models%20%28GAMs%29.%20The%20approach%20utilizes%20random%20Fourier%20feature%20embeddings%20to%20uncover%20locally%20adaptive%20structures%20within%20the%20data.%20High-dimensional%20random%20feature%20representations%20are%20compressed%20via%20principal%20component%20analysis%20to%20derive%20a%20latent%20space%20that%20informs%20a%20Gaussian%20mixture%20model%2C%20which%20performs%20soft%20clustering%20to%20partition%20the%20input%20space%20into%20distinct%20regimes.%20Within%20each%20cluster%2C%20a%20local%20GAM%20captures%20nonlinear%20univariate%20effects%20through%20interpretable%20spline-based%20smoothers.%20Numerical%20experiments%20across%20diverse%20regression%20benchmarks%20demonstrate%20that%20the%20proposed%20method%20consistently%20improves%20upon%20classical%20global%20interpretable%20models%20by%20effectively%20modeling%20data%20heterogeneity.%20Furthermore%2C%20the%20mixture-of-GAMs%20framework%20achieves%20performance%20comparable%20to%20explainable%20boosting%20machine%2C%20random%20forest%2C%20and%20multilayer%20perceptron%20on%20certain%20tasks.%20Overall%2C%20this%20construction%20provides%20a%20principled%20approach%20for%20integrating%20representation%20learning%20with%20transparent%20statistical%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19373v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCluster-Based%2520Generalized%2520Additive%2520Models%2520Informed%2520by%2520Random%2520Fourier%2520Features%26entry.906535625%3DXin%2520Huang%2520and%2520Jia%2520Li%2520and%2520Jun%2520Yu%26entry.1292438233%3DIn%2520the%2520development%2520of%2520learning%2520systems%252C%2520there%2520is%2520an%2520ongoing%2520need%2520to%2520reconcile%2520the%2520strong%2520predictive%2520performance%2520offered%2520by%2520opaque%2520black-box%2520models%2520with%2520the%2520level%2520of%2520transparency%2520required%2520for%2520critical%2520applications.%2520This%2520work%2520introduces%2520a%2520methodological%2520framework%2520that%2520combines%2520spectral%2520representation%2520learning%2520with%2520transparent%2520statistical%2520modeling%2520to%2520construct%2520a%2520mixture%2520of%2520generalized%2520additive%2520models%2520%2528GAMs%2529.%2520The%2520approach%2520utilizes%2520random%2520Fourier%2520feature%2520embeddings%2520to%2520uncover%2520locally%2520adaptive%2520structures%2520within%2520the%2520data.%2520High-dimensional%2520random%2520feature%2520representations%2520are%2520compressed%2520via%2520principal%2520component%2520analysis%2520to%2520derive%2520a%2520latent%2520space%2520that%2520informs%2520a%2520Gaussian%2520mixture%2520model%252C%2520which%2520performs%2520soft%2520clustering%2520to%2520partition%2520the%2520input%2520space%2520into%2520distinct%2520regimes.%2520Within%2520each%2520cluster%252C%2520a%2520local%2520GAM%2520captures%2520nonlinear%2520univariate%2520effects%2520through%2520interpretable%2520spline-based%2520smoothers.%2520Numerical%2520experiments%2520across%2520diverse%2520regression%2520benchmarks%2520demonstrate%2520that%2520the%2520proposed%2520method%2520consistently%2520improves%2520upon%2520classical%2520global%2520interpretable%2520models%2520by%2520effectively%2520modeling%2520data%2520heterogeneity.%2520Furthermore%252C%2520the%2520mixture-of-GAMs%2520framework%2520achieves%2520performance%2520comparable%2520to%2520explainable%2520boosting%2520machine%252C%2520random%2520forest%252C%2520and%2520multilayer%2520perceptron%2520on%2520certain%2520tasks.%2520Overall%252C%2520this%2520construction%2520provides%2520a%2520principled%2520approach%2520for%2520integrating%2520representation%2520learning%2520with%2520transparent%2520statistical%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19373v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cluster-Based%20Generalized%20Additive%20Models%20Informed%20by%20Random%20Fourier%20Features&entry.906535625=Xin%20Huang%20and%20Jia%20Li%20and%20Jun%20Yu&entry.1292438233=In%20the%20development%20of%20learning%20systems%2C%20there%20is%20an%20ongoing%20need%20to%20reconcile%20the%20strong%20predictive%20performance%20offered%20by%20opaque%20black-box%20models%20with%20the%20level%20of%20transparency%20required%20for%20critical%20applications.%20This%20work%20introduces%20a%20methodological%20framework%20that%20combines%20spectral%20representation%20learning%20with%20transparent%20statistical%20modeling%20to%20construct%20a%20mixture%20of%20generalized%20additive%20models%20%28GAMs%29.%20The%20approach%20utilizes%20random%20Fourier%20feature%20embeddings%20to%20uncover%20locally%20adaptive%20structures%20within%20the%20data.%20High-dimensional%20random%20feature%20representations%20are%20compressed%20via%20principal%20component%20analysis%20to%20derive%20a%20latent%20space%20that%20informs%20a%20Gaussian%20mixture%20model%2C%20which%20performs%20soft%20clustering%20to%20partition%20the%20input%20space%20into%20distinct%20regimes.%20Within%20each%20cluster%2C%20a%20local%20GAM%20captures%20nonlinear%20univariate%20effects%20through%20interpretable%20spline-based%20smoothers.%20Numerical%20experiments%20across%20diverse%20regression%20benchmarks%20demonstrate%20that%20the%20proposed%20method%20consistently%20improves%20upon%20classical%20global%20interpretable%20models%20by%20effectively%20modeling%20data%20heterogeneity.%20Furthermore%2C%20the%20mixture-of-GAMs%20framework%20achieves%20performance%20comparable%20to%20explainable%20boosting%20machine%2C%20random%20forest%2C%20and%20multilayer%20perceptron%20on%20certain%20tasks.%20Overall%2C%20this%20construction%20provides%20a%20principled%20approach%20for%20integrating%20representation%20learning%20with%20transparent%20statistical%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2512.19373v2&entry.124074799=Read"},
{"title": "What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study", "author": "Keyu Lv and Manyi Zhang and Xiaobo Xia and Jingchen Ni and Shannan Yan and Xianzhi Yu and Lu Hou and Chun Yuan and Haoli Bai", "abstract": "Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.", "link": "http://arxiv.org/abs/2601.14888v1", "date": "2026-01-21", "relevancy": 2.4158, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5061}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4717}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Makes%20Low-Bit%20Quantization-Aware%20Training%20Work%20for%20Reasoning%20LLMs%3F%20A%20Systematic%20Study&body=Title%3A%20What%20Makes%20Low-Bit%20Quantization-Aware%20Training%20Work%20for%20Reasoning%20LLMs%3F%20A%20Systematic%20Study%0AAuthor%3A%20Keyu%20Lv%20and%20Manyi%20Zhang%20and%20Xiaobo%20Xia%20and%20Jingchen%20Ni%20and%20Shannan%20Yan%20and%20Xianzhi%20Yu%20and%20Lu%20Hou%20and%20Chun%20Yuan%20and%20Haoli%20Bai%0AAbstract%3A%20Reasoning%20models%20excel%20at%20complex%20tasks%20such%20as%20coding%20and%20mathematics%2C%20yet%20their%20inference%20is%20often%20slow%20and%20token-inefficient.%20To%20improve%20the%20inference%20efficiency%2C%20post-training%20quantization%20%28PTQ%29%20usually%20comes%20with%20the%20cost%20of%20large%20accuracy%20drops%2C%20especially%20for%20reasoning%20tasks%20under%20low-bit%20settings.%20In%20this%20study%2C%20we%20present%20a%20systematic%20empirical%20study%20of%20quantization-aware%20training%20%28QAT%29%20for%20reasoning%20models.%20Our%20key%20findings%20include%3A%20%281%29%20Knowledge%20distillation%20is%20a%20robust%20objective%20for%20reasoning%20models%20trained%20via%20either%20supervised%20fine-tuning%20or%20reinforcement%20learning%3B%20%282%29%20PTQ%20provides%20a%20strong%20initialization%20for%20QAT%2C%20improving%20accuracy%20while%20reducing%20training%20cost%3B%20%283%29%20Reinforcement%20learning%20remains%20feasible%20for%20quantized%20models%20given%20a%20viable%20cold%20start%20and%20yields%20additional%20gains%3B%20and%20%284%29%20Aligning%20the%20PTQ%20calibration%20domain%20with%20the%20QAT%20training%20domain%20accelerates%20convergence%20and%20often%20improves%20the%20final%20accuracy.%20Finally%2C%20we%20consolidate%20these%20findings%20into%20an%20optimized%20workflow%20%28Reasoning-QAT%29%2C%20and%20show%20that%20it%20consistently%20outperforms%20state-of-the-art%20PTQ%20methods%20across%20multiple%20LLM%20backbones%20and%20reasoning%20datasets.%20For%20instance%2C%20on%20Qwen3-0.6B%2C%20it%20surpasses%20GPTQ%20by%2044.53%25%20on%20MATH-500%20and%20consistently%20recovers%20performance%20in%20the%202-bit%20regime.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Makes%2520Low-Bit%2520Quantization-Aware%2520Training%2520Work%2520for%2520Reasoning%2520LLMs%253F%2520A%2520Systematic%2520Study%26entry.906535625%3DKeyu%2520Lv%2520and%2520Manyi%2520Zhang%2520and%2520Xiaobo%2520Xia%2520and%2520Jingchen%2520Ni%2520and%2520Shannan%2520Yan%2520and%2520Xianzhi%2520Yu%2520and%2520Lu%2520Hou%2520and%2520Chun%2520Yuan%2520and%2520Haoli%2520Bai%26entry.1292438233%3DReasoning%2520models%2520excel%2520at%2520complex%2520tasks%2520such%2520as%2520coding%2520and%2520mathematics%252C%2520yet%2520their%2520inference%2520is%2520often%2520slow%2520and%2520token-inefficient.%2520To%2520improve%2520the%2520inference%2520efficiency%252C%2520post-training%2520quantization%2520%2528PTQ%2529%2520usually%2520comes%2520with%2520the%2520cost%2520of%2520large%2520accuracy%2520drops%252C%2520especially%2520for%2520reasoning%2520tasks%2520under%2520low-bit%2520settings.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520systematic%2520empirical%2520study%2520of%2520quantization-aware%2520training%2520%2528QAT%2529%2520for%2520reasoning%2520models.%2520Our%2520key%2520findings%2520include%253A%2520%25281%2529%2520Knowledge%2520distillation%2520is%2520a%2520robust%2520objective%2520for%2520reasoning%2520models%2520trained%2520via%2520either%2520supervised%2520fine-tuning%2520or%2520reinforcement%2520learning%253B%2520%25282%2529%2520PTQ%2520provides%2520a%2520strong%2520initialization%2520for%2520QAT%252C%2520improving%2520accuracy%2520while%2520reducing%2520training%2520cost%253B%2520%25283%2529%2520Reinforcement%2520learning%2520remains%2520feasible%2520for%2520quantized%2520models%2520given%2520a%2520viable%2520cold%2520start%2520and%2520yields%2520additional%2520gains%253B%2520and%2520%25284%2529%2520Aligning%2520the%2520PTQ%2520calibration%2520domain%2520with%2520the%2520QAT%2520training%2520domain%2520accelerates%2520convergence%2520and%2520often%2520improves%2520the%2520final%2520accuracy.%2520Finally%252C%2520we%2520consolidate%2520these%2520findings%2520into%2520an%2520optimized%2520workflow%2520%2528Reasoning-QAT%2529%252C%2520and%2520show%2520that%2520it%2520consistently%2520outperforms%2520state-of-the-art%2520PTQ%2520methods%2520across%2520multiple%2520LLM%2520backbones%2520and%2520reasoning%2520datasets.%2520For%2520instance%252C%2520on%2520Qwen3-0.6B%252C%2520it%2520surpasses%2520GPTQ%2520by%252044.53%2525%2520on%2520MATH-500%2520and%2520consistently%2520recovers%2520performance%2520in%2520the%25202-bit%2520regime.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Makes%20Low-Bit%20Quantization-Aware%20Training%20Work%20for%20Reasoning%20LLMs%3F%20A%20Systematic%20Study&entry.906535625=Keyu%20Lv%20and%20Manyi%20Zhang%20and%20Xiaobo%20Xia%20and%20Jingchen%20Ni%20and%20Shannan%20Yan%20and%20Xianzhi%20Yu%20and%20Lu%20Hou%20and%20Chun%20Yuan%20and%20Haoli%20Bai&entry.1292438233=Reasoning%20models%20excel%20at%20complex%20tasks%20such%20as%20coding%20and%20mathematics%2C%20yet%20their%20inference%20is%20often%20slow%20and%20token-inefficient.%20To%20improve%20the%20inference%20efficiency%2C%20post-training%20quantization%20%28PTQ%29%20usually%20comes%20with%20the%20cost%20of%20large%20accuracy%20drops%2C%20especially%20for%20reasoning%20tasks%20under%20low-bit%20settings.%20In%20this%20study%2C%20we%20present%20a%20systematic%20empirical%20study%20of%20quantization-aware%20training%20%28QAT%29%20for%20reasoning%20models.%20Our%20key%20findings%20include%3A%20%281%29%20Knowledge%20distillation%20is%20a%20robust%20objective%20for%20reasoning%20models%20trained%20via%20either%20supervised%20fine-tuning%20or%20reinforcement%20learning%3B%20%282%29%20PTQ%20provides%20a%20strong%20initialization%20for%20QAT%2C%20improving%20accuracy%20while%20reducing%20training%20cost%3B%20%283%29%20Reinforcement%20learning%20remains%20feasible%20for%20quantized%20models%20given%20a%20viable%20cold%20start%20and%20yields%20additional%20gains%3B%20and%20%284%29%20Aligning%20the%20PTQ%20calibration%20domain%20with%20the%20QAT%20training%20domain%20accelerates%20convergence%20and%20often%20improves%20the%20final%20accuracy.%20Finally%2C%20we%20consolidate%20these%20findings%20into%20an%20optimized%20workflow%20%28Reasoning-QAT%29%2C%20and%20show%20that%20it%20consistently%20outperforms%20state-of-the-art%20PTQ%20methods%20across%20multiple%20LLM%20backbones%20and%20reasoning%20datasets.%20For%20instance%2C%20on%20Qwen3-0.6B%2C%20it%20surpasses%20GPTQ%20by%2044.53%25%20on%20MATH-500%20and%20consistently%20recovers%20performance%20in%20the%202-bit%20regime.&entry.1838667208=http%3A//arxiv.org/abs/2601.14888v1&entry.124074799=Read"},
{"title": "OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting Conditions", "author": "Maxim Popov and Regina Kurkova and Mikhail Iumanov and Jaafar Mahmoud and Sergey Kolyubin", "abstract": "Open Semantic Mapping (OSM) is a key technology in robotic perception, combining semantic segmentation and SLAM techniques. This paper introduces a dynamically configurable and highly automated LLM/LVLM-powered pipeline for evaluating OSM solutions called OSMa-Bench (Open Semantic Mapping Benchmark). The study focuses on evaluating state-of-the-art semantic mapping algorithms under varying indoor lighting conditions, a critical challenge in indoor environments. We introduce a novel dataset with simulated RGB-D sequences and ground truth 3D reconstructions, facilitating the rigorous analysis of mapping performance across different lighting conditions. Through experiments on leading models such as ConceptGraphs, BBQ, and OpenScene, we evaluate the semantic fidelity of object recognition and segmentation. Additionally, we introduce a Scene Graph evaluation method to analyze the ability of models to interpret semantic structure. The results provide insights into the robustness of these models, forming future research directions for developing resilient and adaptable robotic systems. Project page is available at https://be2rlab.github.io/OSMa-Bench/.", "link": "http://arxiv.org/abs/2503.10331v3", "date": "2026-01-21", "relevancy": 2.4082, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6146}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5995}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OSMa-Bench%3A%20Evaluating%20Open%20Semantic%20Mapping%20Under%20Varying%20Lighting%20Conditions&body=Title%3A%20OSMa-Bench%3A%20Evaluating%20Open%20Semantic%20Mapping%20Under%20Varying%20Lighting%20Conditions%0AAuthor%3A%20Maxim%20Popov%20and%20Regina%20Kurkova%20and%20Mikhail%20Iumanov%20and%20Jaafar%20Mahmoud%20and%20Sergey%20Kolyubin%0AAbstract%3A%20Open%20Semantic%20Mapping%20%28OSM%29%20is%20a%20key%20technology%20in%20robotic%20perception%2C%20combining%20semantic%20segmentation%20and%20SLAM%20techniques.%20This%20paper%20introduces%20a%20dynamically%20configurable%20and%20highly%20automated%20LLM/LVLM-powered%20pipeline%20for%20evaluating%20OSM%20solutions%20called%20OSMa-Bench%20%28Open%20Semantic%20Mapping%20Benchmark%29.%20The%20study%20focuses%20on%20evaluating%20state-of-the-art%20semantic%20mapping%20algorithms%20under%20varying%20indoor%20lighting%20conditions%2C%20a%20critical%20challenge%20in%20indoor%20environments.%20We%20introduce%20a%20novel%20dataset%20with%20simulated%20RGB-D%20sequences%20and%20ground%20truth%203D%20reconstructions%2C%20facilitating%20the%20rigorous%20analysis%20of%20mapping%20performance%20across%20different%20lighting%20conditions.%20Through%20experiments%20on%20leading%20models%20such%20as%20ConceptGraphs%2C%20BBQ%2C%20and%20OpenScene%2C%20we%20evaluate%20the%20semantic%20fidelity%20of%20object%20recognition%20and%20segmentation.%20Additionally%2C%20we%20introduce%20a%20Scene%20Graph%20evaluation%20method%20to%20analyze%20the%20ability%20of%20models%20to%20interpret%20semantic%20structure.%20The%20results%20provide%20insights%20into%20the%20robustness%20of%20these%20models%2C%20forming%20future%20research%20directions%20for%20developing%20resilient%20and%20adaptable%20robotic%20systems.%20Project%20page%20is%20available%20at%20https%3A//be2rlab.github.io/OSMa-Bench/.%0ALink%3A%20http%3A//arxiv.org/abs/2503.10331v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOSMa-Bench%253A%2520Evaluating%2520Open%2520Semantic%2520Mapping%2520Under%2520Varying%2520Lighting%2520Conditions%26entry.906535625%3DMaxim%2520Popov%2520and%2520Regina%2520Kurkova%2520and%2520Mikhail%2520Iumanov%2520and%2520Jaafar%2520Mahmoud%2520and%2520Sergey%2520Kolyubin%26entry.1292438233%3DOpen%2520Semantic%2520Mapping%2520%2528OSM%2529%2520is%2520a%2520key%2520technology%2520in%2520robotic%2520perception%252C%2520combining%2520semantic%2520segmentation%2520and%2520SLAM%2520techniques.%2520This%2520paper%2520introduces%2520a%2520dynamically%2520configurable%2520and%2520highly%2520automated%2520LLM/LVLM-powered%2520pipeline%2520for%2520evaluating%2520OSM%2520solutions%2520called%2520OSMa-Bench%2520%2528Open%2520Semantic%2520Mapping%2520Benchmark%2529.%2520The%2520study%2520focuses%2520on%2520evaluating%2520state-of-the-art%2520semantic%2520mapping%2520algorithms%2520under%2520varying%2520indoor%2520lighting%2520conditions%252C%2520a%2520critical%2520challenge%2520in%2520indoor%2520environments.%2520We%2520introduce%2520a%2520novel%2520dataset%2520with%2520simulated%2520RGB-D%2520sequences%2520and%2520ground%2520truth%25203D%2520reconstructions%252C%2520facilitating%2520the%2520rigorous%2520analysis%2520of%2520mapping%2520performance%2520across%2520different%2520lighting%2520conditions.%2520Through%2520experiments%2520on%2520leading%2520models%2520such%2520as%2520ConceptGraphs%252C%2520BBQ%252C%2520and%2520OpenScene%252C%2520we%2520evaluate%2520the%2520semantic%2520fidelity%2520of%2520object%2520recognition%2520and%2520segmentation.%2520Additionally%252C%2520we%2520introduce%2520a%2520Scene%2520Graph%2520evaluation%2520method%2520to%2520analyze%2520the%2520ability%2520of%2520models%2520to%2520interpret%2520semantic%2520structure.%2520The%2520results%2520provide%2520insights%2520into%2520the%2520robustness%2520of%2520these%2520models%252C%2520forming%2520future%2520research%2520directions%2520for%2520developing%2520resilient%2520and%2520adaptable%2520robotic%2520systems.%2520Project%2520page%2520is%2520available%2520at%2520https%253A//be2rlab.github.io/OSMa-Bench/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10331v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OSMa-Bench%3A%20Evaluating%20Open%20Semantic%20Mapping%20Under%20Varying%20Lighting%20Conditions&entry.906535625=Maxim%20Popov%20and%20Regina%20Kurkova%20and%20Mikhail%20Iumanov%20and%20Jaafar%20Mahmoud%20and%20Sergey%20Kolyubin&entry.1292438233=Open%20Semantic%20Mapping%20%28OSM%29%20is%20a%20key%20technology%20in%20robotic%20perception%2C%20combining%20semantic%20segmentation%20and%20SLAM%20techniques.%20This%20paper%20introduces%20a%20dynamically%20configurable%20and%20highly%20automated%20LLM/LVLM-powered%20pipeline%20for%20evaluating%20OSM%20solutions%20called%20OSMa-Bench%20%28Open%20Semantic%20Mapping%20Benchmark%29.%20The%20study%20focuses%20on%20evaluating%20state-of-the-art%20semantic%20mapping%20algorithms%20under%20varying%20indoor%20lighting%20conditions%2C%20a%20critical%20challenge%20in%20indoor%20environments.%20We%20introduce%20a%20novel%20dataset%20with%20simulated%20RGB-D%20sequences%20and%20ground%20truth%203D%20reconstructions%2C%20facilitating%20the%20rigorous%20analysis%20of%20mapping%20performance%20across%20different%20lighting%20conditions.%20Through%20experiments%20on%20leading%20models%20such%20as%20ConceptGraphs%2C%20BBQ%2C%20and%20OpenScene%2C%20we%20evaluate%20the%20semantic%20fidelity%20of%20object%20recognition%20and%20segmentation.%20Additionally%2C%20we%20introduce%20a%20Scene%20Graph%20evaluation%20method%20to%20analyze%20the%20ability%20of%20models%20to%20interpret%20semantic%20structure.%20The%20results%20provide%20insights%20into%20the%20robustness%20of%20these%20models%2C%20forming%20future%20research%20directions%20for%20developing%20resilient%20and%20adaptable%20robotic%20systems.%20Project%20page%20is%20available%20at%20https%3A//be2rlab.github.io/OSMa-Bench/.&entry.1838667208=http%3A//arxiv.org/abs/2503.10331v3&entry.124074799=Read"},
{"title": "Orthogonalized Policy Optimization:Decoupling Sampling Geometry from Optimization Geometry in RLHF", "author": "Wang Zixian", "abstract": "Large language model alignment objectives are often presented as a collection of distinct algorithms, such as PPO, DPO, IPO, and their variants, each motivated by different derivations. In this work, we argue that this diversity obscures a simpler underlying structure. At a fundamental level, alignment objectives involve two independent design choices: (i) how training signals are sampled and weighted, and (ii) how deviations from a reference policy are geometrically penalized. Existing methods typically entangle these choices through a single divergence, most commonly the Kullback-Leibler divergence.\n  We show that this entanglement is not merely a modeling convenience but a source of systematic instability. When the same divergence simultaneously determines sample weighting and optimization curvature, adjusting one aspect, such as exploration strength, inevitably alters the other, such as gradient geometry. This coupling is particularly problematic in preference-based reinforcement learning, where advantage signals are unbounded and high-confidence regimes are common.\n  We propose a simple but structural remedy by formulating alignment as an orthogonal mirror descent problem, in which sampling geometry enters only as a linear driving force, while optimization geometry is determined independently by a mirror map. This perspective leads to a new alignment objective called Orthogonalized Policy Optimization (OPO), obtained by choosing a Euclidean mirror map in likelihood ratio space. The resulting objective admits a closed-form solution, linear and non-saturating gradient dynamics, and a well-conditioned trust region, while remaining fully compatible with standard large language model training pipelines.", "link": "http://arxiv.org/abs/2601.12415v2", "date": "2026-01-21", "relevancy": 2.4013, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4842}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4813}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Orthogonalized%20Policy%20Optimization%3ADecoupling%20Sampling%20Geometry%20from%20Optimization%20Geometry%20in%20RLHF&body=Title%3A%20Orthogonalized%20Policy%20Optimization%3ADecoupling%20Sampling%20Geometry%20from%20Optimization%20Geometry%20in%20RLHF%0AAuthor%3A%20Wang%20Zixian%0AAbstract%3A%20Large%20language%20model%20alignment%20objectives%20are%20often%20presented%20as%20a%20collection%20of%20distinct%20algorithms%2C%20such%20as%20PPO%2C%20DPO%2C%20IPO%2C%20and%20their%20variants%2C%20each%20motivated%20by%20different%20derivations.%20In%20this%20work%2C%20we%20argue%20that%20this%20diversity%20obscures%20a%20simpler%20underlying%20structure.%20At%20a%20fundamental%20level%2C%20alignment%20objectives%20involve%20two%20independent%20design%20choices%3A%20%28i%29%20how%20training%20signals%20are%20sampled%20and%20weighted%2C%20and%20%28ii%29%20how%20deviations%20from%20a%20reference%20policy%20are%20geometrically%20penalized.%20Existing%20methods%20typically%20entangle%20these%20choices%20through%20a%20single%20divergence%2C%20most%20commonly%20the%20Kullback-Leibler%20divergence.%0A%20%20We%20show%20that%20this%20entanglement%20is%20not%20merely%20a%20modeling%20convenience%20but%20a%20source%20of%20systematic%20instability.%20When%20the%20same%20divergence%20simultaneously%20determines%20sample%20weighting%20and%20optimization%20curvature%2C%20adjusting%20one%20aspect%2C%20such%20as%20exploration%20strength%2C%20inevitably%20alters%20the%20other%2C%20such%20as%20gradient%20geometry.%20This%20coupling%20is%20particularly%20problematic%20in%20preference-based%20reinforcement%20learning%2C%20where%20advantage%20signals%20are%20unbounded%20and%20high-confidence%20regimes%20are%20common.%0A%20%20We%20propose%20a%20simple%20but%20structural%20remedy%20by%20formulating%20alignment%20as%20an%20orthogonal%20mirror%20descent%20problem%2C%20in%20which%20sampling%20geometry%20enters%20only%20as%20a%20linear%20driving%20force%2C%20while%20optimization%20geometry%20is%20determined%20independently%20by%20a%20mirror%20map.%20This%20perspective%20leads%20to%20a%20new%20alignment%20objective%20called%20Orthogonalized%20Policy%20Optimization%20%28OPO%29%2C%20obtained%20by%20choosing%20a%20Euclidean%20mirror%20map%20in%20likelihood%20ratio%20space.%20The%20resulting%20objective%20admits%20a%20closed-form%20solution%2C%20linear%20and%20non-saturating%20gradient%20dynamics%2C%20and%20a%20well-conditioned%20trust%20region%2C%20while%20remaining%20fully%20compatible%20with%20standard%20large%20language%20model%20training%20pipelines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.12415v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrthogonalized%2520Policy%2520Optimization%253ADecoupling%2520Sampling%2520Geometry%2520from%2520Optimization%2520Geometry%2520in%2520RLHF%26entry.906535625%3DWang%2520Zixian%26entry.1292438233%3DLarge%2520language%2520model%2520alignment%2520objectives%2520are%2520often%2520presented%2520as%2520a%2520collection%2520of%2520distinct%2520algorithms%252C%2520such%2520as%2520PPO%252C%2520DPO%252C%2520IPO%252C%2520and%2520their%2520variants%252C%2520each%2520motivated%2520by%2520different%2520derivations.%2520In%2520this%2520work%252C%2520we%2520argue%2520that%2520this%2520diversity%2520obscures%2520a%2520simpler%2520underlying%2520structure.%2520At%2520a%2520fundamental%2520level%252C%2520alignment%2520objectives%2520involve%2520two%2520independent%2520design%2520choices%253A%2520%2528i%2529%2520how%2520training%2520signals%2520are%2520sampled%2520and%2520weighted%252C%2520and%2520%2528ii%2529%2520how%2520deviations%2520from%2520a%2520reference%2520policy%2520are%2520geometrically%2520penalized.%2520Existing%2520methods%2520typically%2520entangle%2520these%2520choices%2520through%2520a%2520single%2520divergence%252C%2520most%2520commonly%2520the%2520Kullback-Leibler%2520divergence.%250A%2520%2520We%2520show%2520that%2520this%2520entanglement%2520is%2520not%2520merely%2520a%2520modeling%2520convenience%2520but%2520a%2520source%2520of%2520systematic%2520instability.%2520When%2520the%2520same%2520divergence%2520simultaneously%2520determines%2520sample%2520weighting%2520and%2520optimization%2520curvature%252C%2520adjusting%2520one%2520aspect%252C%2520such%2520as%2520exploration%2520strength%252C%2520inevitably%2520alters%2520the%2520other%252C%2520such%2520as%2520gradient%2520geometry.%2520This%2520coupling%2520is%2520particularly%2520problematic%2520in%2520preference-based%2520reinforcement%2520learning%252C%2520where%2520advantage%2520signals%2520are%2520unbounded%2520and%2520high-confidence%2520regimes%2520are%2520common.%250A%2520%2520We%2520propose%2520a%2520simple%2520but%2520structural%2520remedy%2520by%2520formulating%2520alignment%2520as%2520an%2520orthogonal%2520mirror%2520descent%2520problem%252C%2520in%2520which%2520sampling%2520geometry%2520enters%2520only%2520as%2520a%2520linear%2520driving%2520force%252C%2520while%2520optimization%2520geometry%2520is%2520determined%2520independently%2520by%2520a%2520mirror%2520map.%2520This%2520perspective%2520leads%2520to%2520a%2520new%2520alignment%2520objective%2520called%2520Orthogonalized%2520Policy%2520Optimization%2520%2528OPO%2529%252C%2520obtained%2520by%2520choosing%2520a%2520Euclidean%2520mirror%2520map%2520in%2520likelihood%2520ratio%2520space.%2520The%2520resulting%2520objective%2520admits%2520a%2520closed-form%2520solution%252C%2520linear%2520and%2520non-saturating%2520gradient%2520dynamics%252C%2520and%2520a%2520well-conditioned%2520trust%2520region%252C%2520while%2520remaining%2520fully%2520compatible%2520with%2520standard%2520large%2520language%2520model%2520training%2520pipelines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.12415v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Orthogonalized%20Policy%20Optimization%3ADecoupling%20Sampling%20Geometry%20from%20Optimization%20Geometry%20in%20RLHF&entry.906535625=Wang%20Zixian&entry.1292438233=Large%20language%20model%20alignment%20objectives%20are%20often%20presented%20as%20a%20collection%20of%20distinct%20algorithms%2C%20such%20as%20PPO%2C%20DPO%2C%20IPO%2C%20and%20their%20variants%2C%20each%20motivated%20by%20different%20derivations.%20In%20this%20work%2C%20we%20argue%20that%20this%20diversity%20obscures%20a%20simpler%20underlying%20structure.%20At%20a%20fundamental%20level%2C%20alignment%20objectives%20involve%20two%20independent%20design%20choices%3A%20%28i%29%20how%20training%20signals%20are%20sampled%20and%20weighted%2C%20and%20%28ii%29%20how%20deviations%20from%20a%20reference%20policy%20are%20geometrically%20penalized.%20Existing%20methods%20typically%20entangle%20these%20choices%20through%20a%20single%20divergence%2C%20most%20commonly%20the%20Kullback-Leibler%20divergence.%0A%20%20We%20show%20that%20this%20entanglement%20is%20not%20merely%20a%20modeling%20convenience%20but%20a%20source%20of%20systematic%20instability.%20When%20the%20same%20divergence%20simultaneously%20determines%20sample%20weighting%20and%20optimization%20curvature%2C%20adjusting%20one%20aspect%2C%20such%20as%20exploration%20strength%2C%20inevitably%20alters%20the%20other%2C%20such%20as%20gradient%20geometry.%20This%20coupling%20is%20particularly%20problematic%20in%20preference-based%20reinforcement%20learning%2C%20where%20advantage%20signals%20are%20unbounded%20and%20high-confidence%20regimes%20are%20common.%0A%20%20We%20propose%20a%20simple%20but%20structural%20remedy%20by%20formulating%20alignment%20as%20an%20orthogonal%20mirror%20descent%20problem%2C%20in%20which%20sampling%20geometry%20enters%20only%20as%20a%20linear%20driving%20force%2C%20while%20optimization%20geometry%20is%20determined%20independently%20by%20a%20mirror%20map.%20This%20perspective%20leads%20to%20a%20new%20alignment%20objective%20called%20Orthogonalized%20Policy%20Optimization%20%28OPO%29%2C%20obtained%20by%20choosing%20a%20Euclidean%20mirror%20map%20in%20likelihood%20ratio%20space.%20The%20resulting%20objective%20admits%20a%20closed-form%20solution%2C%20linear%20and%20non-saturating%20gradient%20dynamics%2C%20and%20a%20well-conditioned%20trust%20region%2C%20while%20remaining%20fully%20compatible%20with%20standard%20large%20language%20model%20training%20pipelines.&entry.1838667208=http%3A//arxiv.org/abs/2601.12415v2&entry.124074799=Read"},
{"title": "GECOBench: A Gender-Controlled Text Dataset and Benchmark for Quantifying Biases in Explanations", "author": "Rick Wilming and Artur Dox and Hjalmar Schulz and Marta Oliveira and Benedict Clark and Stefan Haufe", "abstract": "Large pre-trained language models have become a crucial backbone for many downstream tasks in natural language processing (NLP), and while they are trained on a plethora of data containing a variety of biases, such as gender biases, it has been shown that they can also inherit such biases in their weights, potentially affecting their prediction behavior. However, it is unclear to what extent these biases also affect feature attributions generated by applying \"explainable artificial intelligence\" (XAI) techniques, possibly in unfavorable ways. To systematically study this question, we create a gender-controlled text dataset, GECO, in which the alteration of grammatical gender forms induces class-specific words and provides ground truth feature attributions for gender classification tasks. This enables an objective evaluation of the correctness of XAI methods. We apply this dataset to the pre-trained BERT model, which we fine-tune to different degrees, to quantitatively measure how pre-training induces undesirable bias in feature attributions and to what extent fine-tuning can mitigate such explanation bias. To this extent, we provide GECOBench, a rigorous quantitative evaluation framework for benchmarking popular XAI methods. We show a clear dependency between explanation performance and the number of fine-tuned layers, where XAI methods are observed to benefit particularly from fine-tuning or complete retraining of embedding layers.", "link": "http://arxiv.org/abs/2406.11547v2", "date": "2026-01-21", "relevancy": 2.393, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5039}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4671}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GECOBench%3A%20A%20Gender-Controlled%20Text%20Dataset%20and%20Benchmark%20for%20Quantifying%20Biases%20in%20Explanations&body=Title%3A%20GECOBench%3A%20A%20Gender-Controlled%20Text%20Dataset%20and%20Benchmark%20for%20Quantifying%20Biases%20in%20Explanations%0AAuthor%3A%20Rick%20Wilming%20and%20Artur%20Dox%20and%20Hjalmar%20Schulz%20and%20Marta%20Oliveira%20and%20Benedict%20Clark%20and%20Stefan%20Haufe%0AAbstract%3A%20Large%20pre-trained%20language%20models%20have%20become%20a%20crucial%20backbone%20for%20many%20downstream%20tasks%20in%20natural%20language%20processing%20%28NLP%29%2C%20and%20while%20they%20are%20trained%20on%20a%20plethora%20of%20data%20containing%20a%20variety%20of%20biases%2C%20such%20as%20gender%20biases%2C%20it%20has%20been%20shown%20that%20they%20can%20also%20inherit%20such%20biases%20in%20their%20weights%2C%20potentially%20affecting%20their%20prediction%20behavior.%20However%2C%20it%20is%20unclear%20to%20what%20extent%20these%20biases%20also%20affect%20feature%20attributions%20generated%20by%20applying%20%22explainable%20artificial%20intelligence%22%20%28XAI%29%20techniques%2C%20possibly%20in%20unfavorable%20ways.%20To%20systematically%20study%20this%20question%2C%20we%20create%20a%20gender-controlled%20text%20dataset%2C%20GECO%2C%20in%20which%20the%20alteration%20of%20grammatical%20gender%20forms%20induces%20class-specific%20words%20and%20provides%20ground%20truth%20feature%20attributions%20for%20gender%20classification%20tasks.%20This%20enables%20an%20objective%20evaluation%20of%20the%20correctness%20of%20XAI%20methods.%20We%20apply%20this%20dataset%20to%20the%20pre-trained%20BERT%20model%2C%20which%20we%20fine-tune%20to%20different%20degrees%2C%20to%20quantitatively%20measure%20how%20pre-training%20induces%20undesirable%20bias%20in%20feature%20attributions%20and%20to%20what%20extent%20fine-tuning%20can%20mitigate%20such%20explanation%20bias.%20To%20this%20extent%2C%20we%20provide%20GECOBench%2C%20a%20rigorous%20quantitative%20evaluation%20framework%20for%20benchmarking%20popular%20XAI%20methods.%20We%20show%20a%20clear%20dependency%20between%20explanation%20performance%20and%20the%20number%20of%20fine-tuned%20layers%2C%20where%20XAI%20methods%20are%20observed%20to%20benefit%20particularly%20from%20fine-tuning%20or%20complete%20retraining%20of%20embedding%20layers.%0ALink%3A%20http%3A//arxiv.org/abs/2406.11547v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGECOBench%253A%2520A%2520Gender-Controlled%2520Text%2520Dataset%2520and%2520Benchmark%2520for%2520Quantifying%2520Biases%2520in%2520Explanations%26entry.906535625%3DRick%2520Wilming%2520and%2520Artur%2520Dox%2520and%2520Hjalmar%2520Schulz%2520and%2520Marta%2520Oliveira%2520and%2520Benedict%2520Clark%2520and%2520Stefan%2520Haufe%26entry.1292438233%3DLarge%2520pre-trained%2520language%2520models%2520have%2520become%2520a%2520crucial%2520backbone%2520for%2520many%2520downstream%2520tasks%2520in%2520natural%2520language%2520processing%2520%2528NLP%2529%252C%2520and%2520while%2520they%2520are%2520trained%2520on%2520a%2520plethora%2520of%2520data%2520containing%2520a%2520variety%2520of%2520biases%252C%2520such%2520as%2520gender%2520biases%252C%2520it%2520has%2520been%2520shown%2520that%2520they%2520can%2520also%2520inherit%2520such%2520biases%2520in%2520their%2520weights%252C%2520potentially%2520affecting%2520their%2520prediction%2520behavior.%2520However%252C%2520it%2520is%2520unclear%2520to%2520what%2520extent%2520these%2520biases%2520also%2520affect%2520feature%2520attributions%2520generated%2520by%2520applying%2520%2522explainable%2520artificial%2520intelligence%2522%2520%2528XAI%2529%2520techniques%252C%2520possibly%2520in%2520unfavorable%2520ways.%2520To%2520systematically%2520study%2520this%2520question%252C%2520we%2520create%2520a%2520gender-controlled%2520text%2520dataset%252C%2520GECO%252C%2520in%2520which%2520the%2520alteration%2520of%2520grammatical%2520gender%2520forms%2520induces%2520class-specific%2520words%2520and%2520provides%2520ground%2520truth%2520feature%2520attributions%2520for%2520gender%2520classification%2520tasks.%2520This%2520enables%2520an%2520objective%2520evaluation%2520of%2520the%2520correctness%2520of%2520XAI%2520methods.%2520We%2520apply%2520this%2520dataset%2520to%2520the%2520pre-trained%2520BERT%2520model%252C%2520which%2520we%2520fine-tune%2520to%2520different%2520degrees%252C%2520to%2520quantitatively%2520measure%2520how%2520pre-training%2520induces%2520undesirable%2520bias%2520in%2520feature%2520attributions%2520and%2520to%2520what%2520extent%2520fine-tuning%2520can%2520mitigate%2520such%2520explanation%2520bias.%2520To%2520this%2520extent%252C%2520we%2520provide%2520GECOBench%252C%2520a%2520rigorous%2520quantitative%2520evaluation%2520framework%2520for%2520benchmarking%2520popular%2520XAI%2520methods.%2520We%2520show%2520a%2520clear%2520dependency%2520between%2520explanation%2520performance%2520and%2520the%2520number%2520of%2520fine-tuned%2520layers%252C%2520where%2520XAI%2520methods%2520are%2520observed%2520to%2520benefit%2520particularly%2520from%2520fine-tuning%2520or%2520complete%2520retraining%2520of%2520embedding%2520layers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11547v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GECOBench%3A%20A%20Gender-Controlled%20Text%20Dataset%20and%20Benchmark%20for%20Quantifying%20Biases%20in%20Explanations&entry.906535625=Rick%20Wilming%20and%20Artur%20Dox%20and%20Hjalmar%20Schulz%20and%20Marta%20Oliveira%20and%20Benedict%20Clark%20and%20Stefan%20Haufe&entry.1292438233=Large%20pre-trained%20language%20models%20have%20become%20a%20crucial%20backbone%20for%20many%20downstream%20tasks%20in%20natural%20language%20processing%20%28NLP%29%2C%20and%20while%20they%20are%20trained%20on%20a%20plethora%20of%20data%20containing%20a%20variety%20of%20biases%2C%20such%20as%20gender%20biases%2C%20it%20has%20been%20shown%20that%20they%20can%20also%20inherit%20such%20biases%20in%20their%20weights%2C%20potentially%20affecting%20their%20prediction%20behavior.%20However%2C%20it%20is%20unclear%20to%20what%20extent%20these%20biases%20also%20affect%20feature%20attributions%20generated%20by%20applying%20%22explainable%20artificial%20intelligence%22%20%28XAI%29%20techniques%2C%20possibly%20in%20unfavorable%20ways.%20To%20systematically%20study%20this%20question%2C%20we%20create%20a%20gender-controlled%20text%20dataset%2C%20GECO%2C%20in%20which%20the%20alteration%20of%20grammatical%20gender%20forms%20induces%20class-specific%20words%20and%20provides%20ground%20truth%20feature%20attributions%20for%20gender%20classification%20tasks.%20This%20enables%20an%20objective%20evaluation%20of%20the%20correctness%20of%20XAI%20methods.%20We%20apply%20this%20dataset%20to%20the%20pre-trained%20BERT%20model%2C%20which%20we%20fine-tune%20to%20different%20degrees%2C%20to%20quantitatively%20measure%20how%20pre-training%20induces%20undesirable%20bias%20in%20feature%20attributions%20and%20to%20what%20extent%20fine-tuning%20can%20mitigate%20such%20explanation%20bias.%20To%20this%20extent%2C%20we%20provide%20GECOBench%2C%20a%20rigorous%20quantitative%20evaluation%20framework%20for%20benchmarking%20popular%20XAI%20methods.%20We%20show%20a%20clear%20dependency%20between%20explanation%20performance%20and%20the%20number%20of%20fine-tuned%20layers%2C%20where%20XAI%20methods%20are%20observed%20to%20benefit%20particularly%20from%20fine-tuning%20or%20complete%20retraining%20of%20embedding%20layers.&entry.1838667208=http%3A//arxiv.org/abs/2406.11547v2&entry.124074799=Read"},
{"title": "NeuroClean: A Generalized Machine-Learning Approach to Neural Time-Series Conditioning", "author": "Manuel A. Hernandez Alonso and Michael Depass and Stephan Quessy and Ali Falaki and Soraya Rahimi and Numa Dancause and Ignasi Cos", "abstract": "Electroencephalography (EEG) and local field potentials (LFP) are two widely used techniques to record electrical activity from the brain. These signals are used in both the clinical and research domains for multiple applications. However, most brain data recordings suffer from a myriad of artifacts and noise sources other than the brain itself. Thus, a major requirement for their use is proper and, given current volumes of data, a fully automatized conditioning. As a means to this end, here we introduce an unsupervised, multipurpose EEG/LFP preprocessing method, the NeuroClean pipeline. In addition to its completeness and reliability, NeuroClean is an unsupervised series of algorithms intended to mitigate reproducibility issues and biases caused by human intervention. The pipeline is designed as a five-step process, including the common bandpass and line noise filtering, and bad channel rejection. However, it incorporates an efficient independent component analysis with an automatic component rejection based on a clustering algorithm. This machine learning classifier is used to ensure that task-relevant information is preserved after each step of the cleaning process. We used several data sets to validate the pipeline. NeuroClean removed several common types of artifacts from the signal. Moreover, in the context of motor tasks of varying complexity, it yielded more than 97% accuracy (vs. a chance-level of 33.3%) in an optimized Multinomial Logistic Regression model after cleaning the data, compared to the raw data, which performed at 74% accuracy. These results show that NeuroClean is a promising pipeline and workflow that can be applied to future work and studies to achieve better generalization and performance on machine learning pipelines.", "link": "http://arxiv.org/abs/2511.01951v2", "date": "2026-01-21", "relevancy": 2.3819, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5011}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4686}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuroClean%3A%20A%20Generalized%20Machine-Learning%20Approach%20to%20Neural%20Time-Series%20Conditioning&body=Title%3A%20NeuroClean%3A%20A%20Generalized%20Machine-Learning%20Approach%20to%20Neural%20Time-Series%20Conditioning%0AAuthor%3A%20Manuel%20A.%20Hernandez%20Alonso%20and%20Michael%20Depass%20and%20Stephan%20Quessy%20and%20Ali%20Falaki%20and%20Soraya%20Rahimi%20and%20Numa%20Dancause%20and%20Ignasi%20Cos%0AAbstract%3A%20Electroencephalography%20%28EEG%29%20and%20local%20field%20potentials%20%28LFP%29%20are%20two%20widely%20used%20techniques%20to%20record%20electrical%20activity%20from%20the%20brain.%20These%20signals%20are%20used%20in%20both%20the%20clinical%20and%20research%20domains%20for%20multiple%20applications.%20However%2C%20most%20brain%20data%20recordings%20suffer%20from%20a%20myriad%20of%20artifacts%20and%20noise%20sources%20other%20than%20the%20brain%20itself.%20Thus%2C%20a%20major%20requirement%20for%20their%20use%20is%20proper%20and%2C%20given%20current%20volumes%20of%20data%2C%20a%20fully%20automatized%20conditioning.%20As%20a%20means%20to%20this%20end%2C%20here%20we%20introduce%20an%20unsupervised%2C%20multipurpose%20EEG/LFP%20preprocessing%20method%2C%20the%20NeuroClean%20pipeline.%20In%20addition%20to%20its%20completeness%20and%20reliability%2C%20NeuroClean%20is%20an%20unsupervised%20series%20of%20algorithms%20intended%20to%20mitigate%20reproducibility%20issues%20and%20biases%20caused%20by%20human%20intervention.%20The%20pipeline%20is%20designed%20as%20a%20five-step%20process%2C%20including%20the%20common%20bandpass%20and%20line%20noise%20filtering%2C%20and%20bad%20channel%20rejection.%20However%2C%20it%20incorporates%20an%20efficient%20independent%20component%20analysis%20with%20an%20automatic%20component%20rejection%20based%20on%20a%20clustering%20algorithm.%20This%20machine%20learning%20classifier%20is%20used%20to%20ensure%20that%20task-relevant%20information%20is%20preserved%20after%20each%20step%20of%20the%20cleaning%20process.%20We%20used%20several%20data%20sets%20to%20validate%20the%20pipeline.%20NeuroClean%20removed%20several%20common%20types%20of%20artifacts%20from%20the%20signal.%20Moreover%2C%20in%20the%20context%20of%20motor%20tasks%20of%20varying%20complexity%2C%20it%20yielded%20more%20than%2097%25%20accuracy%20%28vs.%20a%20chance-level%20of%2033.3%25%29%20in%20an%20optimized%20Multinomial%20Logistic%20Regression%20model%20after%20cleaning%20the%20data%2C%20compared%20to%20the%20raw%20data%2C%20which%20performed%20at%2074%25%20accuracy.%20These%20results%20show%20that%20NeuroClean%20is%20a%20promising%20pipeline%20and%20workflow%20that%20can%20be%20applied%20to%20future%20work%20and%20studies%20to%20achieve%20better%20generalization%20and%20performance%20on%20machine%20learning%20pipelines.%0ALink%3A%20http%3A//arxiv.org/abs/2511.01951v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroClean%253A%2520A%2520Generalized%2520Machine-Learning%2520Approach%2520to%2520Neural%2520Time-Series%2520Conditioning%26entry.906535625%3DManuel%2520A.%2520Hernandez%2520Alonso%2520and%2520Michael%2520Depass%2520and%2520Stephan%2520Quessy%2520and%2520Ali%2520Falaki%2520and%2520Soraya%2520Rahimi%2520and%2520Numa%2520Dancause%2520and%2520Ignasi%2520Cos%26entry.1292438233%3DElectroencephalography%2520%2528EEG%2529%2520and%2520local%2520field%2520potentials%2520%2528LFP%2529%2520are%2520two%2520widely%2520used%2520techniques%2520to%2520record%2520electrical%2520activity%2520from%2520the%2520brain.%2520These%2520signals%2520are%2520used%2520in%2520both%2520the%2520clinical%2520and%2520research%2520domains%2520for%2520multiple%2520applications.%2520However%252C%2520most%2520brain%2520data%2520recordings%2520suffer%2520from%2520a%2520myriad%2520of%2520artifacts%2520and%2520noise%2520sources%2520other%2520than%2520the%2520brain%2520itself.%2520Thus%252C%2520a%2520major%2520requirement%2520for%2520their%2520use%2520is%2520proper%2520and%252C%2520given%2520current%2520volumes%2520of%2520data%252C%2520a%2520fully%2520automatized%2520conditioning.%2520As%2520a%2520means%2520to%2520this%2520end%252C%2520here%2520we%2520introduce%2520an%2520unsupervised%252C%2520multipurpose%2520EEG/LFP%2520preprocessing%2520method%252C%2520the%2520NeuroClean%2520pipeline.%2520In%2520addition%2520to%2520its%2520completeness%2520and%2520reliability%252C%2520NeuroClean%2520is%2520an%2520unsupervised%2520series%2520of%2520algorithms%2520intended%2520to%2520mitigate%2520reproducibility%2520issues%2520and%2520biases%2520caused%2520by%2520human%2520intervention.%2520The%2520pipeline%2520is%2520designed%2520as%2520a%2520five-step%2520process%252C%2520including%2520the%2520common%2520bandpass%2520and%2520line%2520noise%2520filtering%252C%2520and%2520bad%2520channel%2520rejection.%2520However%252C%2520it%2520incorporates%2520an%2520efficient%2520independent%2520component%2520analysis%2520with%2520an%2520automatic%2520component%2520rejection%2520based%2520on%2520a%2520clustering%2520algorithm.%2520This%2520machine%2520learning%2520classifier%2520is%2520used%2520to%2520ensure%2520that%2520task-relevant%2520information%2520is%2520preserved%2520after%2520each%2520step%2520of%2520the%2520cleaning%2520process.%2520We%2520used%2520several%2520data%2520sets%2520to%2520validate%2520the%2520pipeline.%2520NeuroClean%2520removed%2520several%2520common%2520types%2520of%2520artifacts%2520from%2520the%2520signal.%2520Moreover%252C%2520in%2520the%2520context%2520of%2520motor%2520tasks%2520of%2520varying%2520complexity%252C%2520it%2520yielded%2520more%2520than%252097%2525%2520accuracy%2520%2528vs.%2520a%2520chance-level%2520of%252033.3%2525%2529%2520in%2520an%2520optimized%2520Multinomial%2520Logistic%2520Regression%2520model%2520after%2520cleaning%2520the%2520data%252C%2520compared%2520to%2520the%2520raw%2520data%252C%2520which%2520performed%2520at%252074%2525%2520accuracy.%2520These%2520results%2520show%2520that%2520NeuroClean%2520is%2520a%2520promising%2520pipeline%2520and%2520workflow%2520that%2520can%2520be%2520applied%2520to%2520future%2520work%2520and%2520studies%2520to%2520achieve%2520better%2520generalization%2520and%2520performance%2520on%2520machine%2520learning%2520pipelines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.01951v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroClean%3A%20A%20Generalized%20Machine-Learning%20Approach%20to%20Neural%20Time-Series%20Conditioning&entry.906535625=Manuel%20A.%20Hernandez%20Alonso%20and%20Michael%20Depass%20and%20Stephan%20Quessy%20and%20Ali%20Falaki%20and%20Soraya%20Rahimi%20and%20Numa%20Dancause%20and%20Ignasi%20Cos&entry.1292438233=Electroencephalography%20%28EEG%29%20and%20local%20field%20potentials%20%28LFP%29%20are%20two%20widely%20used%20techniques%20to%20record%20electrical%20activity%20from%20the%20brain.%20These%20signals%20are%20used%20in%20both%20the%20clinical%20and%20research%20domains%20for%20multiple%20applications.%20However%2C%20most%20brain%20data%20recordings%20suffer%20from%20a%20myriad%20of%20artifacts%20and%20noise%20sources%20other%20than%20the%20brain%20itself.%20Thus%2C%20a%20major%20requirement%20for%20their%20use%20is%20proper%20and%2C%20given%20current%20volumes%20of%20data%2C%20a%20fully%20automatized%20conditioning.%20As%20a%20means%20to%20this%20end%2C%20here%20we%20introduce%20an%20unsupervised%2C%20multipurpose%20EEG/LFP%20preprocessing%20method%2C%20the%20NeuroClean%20pipeline.%20In%20addition%20to%20its%20completeness%20and%20reliability%2C%20NeuroClean%20is%20an%20unsupervised%20series%20of%20algorithms%20intended%20to%20mitigate%20reproducibility%20issues%20and%20biases%20caused%20by%20human%20intervention.%20The%20pipeline%20is%20designed%20as%20a%20five-step%20process%2C%20including%20the%20common%20bandpass%20and%20line%20noise%20filtering%2C%20and%20bad%20channel%20rejection.%20However%2C%20it%20incorporates%20an%20efficient%20independent%20component%20analysis%20with%20an%20automatic%20component%20rejection%20based%20on%20a%20clustering%20algorithm.%20This%20machine%20learning%20classifier%20is%20used%20to%20ensure%20that%20task-relevant%20information%20is%20preserved%20after%20each%20step%20of%20the%20cleaning%20process.%20We%20used%20several%20data%20sets%20to%20validate%20the%20pipeline.%20NeuroClean%20removed%20several%20common%20types%20of%20artifacts%20from%20the%20signal.%20Moreover%2C%20in%20the%20context%20of%20motor%20tasks%20of%20varying%20complexity%2C%20it%20yielded%20more%20than%2097%25%20accuracy%20%28vs.%20a%20chance-level%20of%2033.3%25%29%20in%20an%20optimized%20Multinomial%20Logistic%20Regression%20model%20after%20cleaning%20the%20data%2C%20compared%20to%20the%20raw%20data%2C%20which%20performed%20at%2074%25%20accuracy.%20These%20results%20show%20that%20NeuroClean%20is%20a%20promising%20pipeline%20and%20workflow%20that%20can%20be%20applied%20to%20future%20work%20and%20studies%20to%20achieve%20better%20generalization%20and%20performance%20on%20machine%20learning%20pipelines.&entry.1838667208=http%3A//arxiv.org/abs/2511.01951v2&entry.124074799=Read"},
{"title": "Learning to Explain: Supervised Token Attribution from Transformer Attention Patterns", "author": "George Mihaila", "abstract": "Explainable AI (XAI) has become critical as transformer-based models are deployed in high-stakes applications including healthcare, legal systems, and financial services, where opacity hinders trust and accountability. Transformers self-attention mechanisms have proven valuable for model interpretability, with attention weights successfully used to understand model focus and behavior (Xu et al., 2015); (Wiegreffe and Pinter, 2019). However, existing attention-based explanation methods rely on manually defined aggregation strategies and fixed attribution rules (Abnar and Zuidema, 2020a); (Chefer et al., 2021), while model-agnostic approaches (LIME, SHAP) treat the model as a black box and incur significant computational costs through input perturbation. We introduce Explanation Network (ExpNet), a lightweight neural network that learns an explicit mapping from transformer attention patterns to token-level importance scores. Unlike prior methods, ExpNet discovers optimal attention feature combinations automatically rather than relying on predetermined rules. We evaluate ExpNet in a challenging cross-task setting and benchmark it against a broad spectrum of model-agnostic methods and attention-based techniques spanning four methodological families.", "link": "http://arxiv.org/abs/2601.14112v2", "date": "2026-01-21", "relevancy": 2.3789, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4837}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4732}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Explain%3A%20Supervised%20Token%20Attribution%20from%20Transformer%20Attention%20Patterns&body=Title%3A%20Learning%20to%20Explain%3A%20Supervised%20Token%20Attribution%20from%20Transformer%20Attention%20Patterns%0AAuthor%3A%20George%20Mihaila%0AAbstract%3A%20Explainable%20AI%20%28XAI%29%20has%20become%20critical%20as%20transformer-based%20models%20are%20deployed%20in%20high-stakes%20applications%20including%20healthcare%2C%20legal%20systems%2C%20and%20financial%20services%2C%20where%20opacity%20hinders%20trust%20and%20accountability.%20Transformers%20self-attention%20mechanisms%20have%20proven%20valuable%20for%20model%20interpretability%2C%20with%20attention%20weights%20successfully%20used%20to%20understand%20model%20focus%20and%20behavior%20%28Xu%20et%20al.%2C%202015%29%3B%20%28Wiegreffe%20and%20Pinter%2C%202019%29.%20However%2C%20existing%20attention-based%20explanation%20methods%20rely%20on%20manually%20defined%20aggregation%20strategies%20and%20fixed%20attribution%20rules%20%28Abnar%20and%20Zuidema%2C%202020a%29%3B%20%28Chefer%20et%20al.%2C%202021%29%2C%20while%20model-agnostic%20approaches%20%28LIME%2C%20SHAP%29%20treat%20the%20model%20as%20a%20black%20box%20and%20incur%20significant%20computational%20costs%20through%20input%20perturbation.%20We%20introduce%20Explanation%20Network%20%28ExpNet%29%2C%20a%20lightweight%20neural%20network%20that%20learns%20an%20explicit%20mapping%20from%20transformer%20attention%20patterns%20to%20token-level%20importance%20scores.%20Unlike%20prior%20methods%2C%20ExpNet%20discovers%20optimal%20attention%20feature%20combinations%20automatically%20rather%20than%20relying%20on%20predetermined%20rules.%20We%20evaluate%20ExpNet%20in%20a%20challenging%20cross-task%20setting%20and%20benchmark%20it%20against%20a%20broad%20spectrum%20of%20model-agnostic%20methods%20and%20attention-based%20techniques%20spanning%20four%20methodological%20families.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14112v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Explain%253A%2520Supervised%2520Token%2520Attribution%2520from%2520Transformer%2520Attention%2520Patterns%26entry.906535625%3DGeorge%2520Mihaila%26entry.1292438233%3DExplainable%2520AI%2520%2528XAI%2529%2520has%2520become%2520critical%2520as%2520transformer-based%2520models%2520are%2520deployed%2520in%2520high-stakes%2520applications%2520including%2520healthcare%252C%2520legal%2520systems%252C%2520and%2520financial%2520services%252C%2520where%2520opacity%2520hinders%2520trust%2520and%2520accountability.%2520Transformers%2520self-attention%2520mechanisms%2520have%2520proven%2520valuable%2520for%2520model%2520interpretability%252C%2520with%2520attention%2520weights%2520successfully%2520used%2520to%2520understand%2520model%2520focus%2520and%2520behavior%2520%2528Xu%2520et%2520al.%252C%25202015%2529%253B%2520%2528Wiegreffe%2520and%2520Pinter%252C%25202019%2529.%2520However%252C%2520existing%2520attention-based%2520explanation%2520methods%2520rely%2520on%2520manually%2520defined%2520aggregation%2520strategies%2520and%2520fixed%2520attribution%2520rules%2520%2528Abnar%2520and%2520Zuidema%252C%25202020a%2529%253B%2520%2528Chefer%2520et%2520al.%252C%25202021%2529%252C%2520while%2520model-agnostic%2520approaches%2520%2528LIME%252C%2520SHAP%2529%2520treat%2520the%2520model%2520as%2520a%2520black%2520box%2520and%2520incur%2520significant%2520computational%2520costs%2520through%2520input%2520perturbation.%2520We%2520introduce%2520Explanation%2520Network%2520%2528ExpNet%2529%252C%2520a%2520lightweight%2520neural%2520network%2520that%2520learns%2520an%2520explicit%2520mapping%2520from%2520transformer%2520attention%2520patterns%2520to%2520token-level%2520importance%2520scores.%2520Unlike%2520prior%2520methods%252C%2520ExpNet%2520discovers%2520optimal%2520attention%2520feature%2520combinations%2520automatically%2520rather%2520than%2520relying%2520on%2520predetermined%2520rules.%2520We%2520evaluate%2520ExpNet%2520in%2520a%2520challenging%2520cross-task%2520setting%2520and%2520benchmark%2520it%2520against%2520a%2520broad%2520spectrum%2520of%2520model-agnostic%2520methods%2520and%2520attention-based%2520techniques%2520spanning%2520four%2520methodological%2520families.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14112v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Explain%3A%20Supervised%20Token%20Attribution%20from%20Transformer%20Attention%20Patterns&entry.906535625=George%20Mihaila&entry.1292438233=Explainable%20AI%20%28XAI%29%20has%20become%20critical%20as%20transformer-based%20models%20are%20deployed%20in%20high-stakes%20applications%20including%20healthcare%2C%20legal%20systems%2C%20and%20financial%20services%2C%20where%20opacity%20hinders%20trust%20and%20accountability.%20Transformers%20self-attention%20mechanisms%20have%20proven%20valuable%20for%20model%20interpretability%2C%20with%20attention%20weights%20successfully%20used%20to%20understand%20model%20focus%20and%20behavior%20%28Xu%20et%20al.%2C%202015%29%3B%20%28Wiegreffe%20and%20Pinter%2C%202019%29.%20However%2C%20existing%20attention-based%20explanation%20methods%20rely%20on%20manually%20defined%20aggregation%20strategies%20and%20fixed%20attribution%20rules%20%28Abnar%20and%20Zuidema%2C%202020a%29%3B%20%28Chefer%20et%20al.%2C%202021%29%2C%20while%20model-agnostic%20approaches%20%28LIME%2C%20SHAP%29%20treat%20the%20model%20as%20a%20black%20box%20and%20incur%20significant%20computational%20costs%20through%20input%20perturbation.%20We%20introduce%20Explanation%20Network%20%28ExpNet%29%2C%20a%20lightweight%20neural%20network%20that%20learns%20an%20explicit%20mapping%20from%20transformer%20attention%20patterns%20to%20token-level%20importance%20scores.%20Unlike%20prior%20methods%2C%20ExpNet%20discovers%20optimal%20attention%20feature%20combinations%20automatically%20rather%20than%20relying%20on%20predetermined%20rules.%20We%20evaluate%20ExpNet%20in%20a%20challenging%20cross-task%20setting%20and%20benchmark%20it%20against%20a%20broad%20spectrum%20of%20model-agnostic%20methods%20and%20attention-based%20techniques%20spanning%20four%20methodological%20families.&entry.1838667208=http%3A//arxiv.org/abs/2601.14112v2&entry.124074799=Read"},
{"title": "V-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks", "author": "Yaru Liu and Ao-bo Wang and Nanyang Ye", "abstract": "Learning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently \"succeed\" without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduce V-CAGE, a closed-loop framework for generating robust, semantically aligned manipulation datasets at scale. First, we propose a context-aware instantiation mechanism that enforces geometric consistency during scene synthesis. By dynamically maintaining a map of prohibited spatial areas as objects are placed, our system prevents interpenetration and ensures reachable, conflict-free configurations in cluttered environments. Second, to bridge the gap between abstract intent and low-level control, we employ a hierarchical instruction decomposition module. This decomposes high-level goals (e.g., \"get ready for work\") into compositional action primitives, facilitating coherent long-horizon planning. Crucially, we enforce semantic correctness through a VLM-based verification loop. Acting as a visual critic, the VLM performs rigorous rejection sampling after each subtask, filtering out \"silent failures\" where code executes but fails to achieve the visual goal. Experiments demonstrate that V-CAGE yields datasets with superior physical and semantic fidelity, significantly boosting the success rate and generalization of downstream policies compared to non-verified baselines.", "link": "http://arxiv.org/abs/2601.15164v1", "date": "2026-01-21", "relevancy": 2.3734, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6312}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5936}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-CAGE%3A%20Context-Aware%20Generation%20and%20Verification%20for%20Scalable%20Long-Horizon%20Embodied%20Tasks&body=Title%3A%20V-CAGE%3A%20Context-Aware%20Generation%20and%20Verification%20for%20Scalable%20Long-Horizon%20Embodied%20Tasks%0AAuthor%3A%20Yaru%20Liu%20and%20Ao-bo%20Wang%20and%20Nanyang%20Ye%0AAbstract%3A%20Learning%20long-horizon%20embodied%20behaviors%20from%20synthetic%20data%20remains%20challenging%20because%20generated%20scenes%20are%20often%20physically%20implausible%2C%20language-driven%20programs%20frequently%20%22succeed%22%20without%20satisfying%20task%20semantics%2C%20and%20high-level%20instructions%20require%20grounding%20into%20executable%20action%20sequences.%20To%20address%20these%20limitations%2C%20we%20introduce%20V-CAGE%2C%20a%20closed-loop%20framework%20for%20generating%20robust%2C%20semantically%20aligned%20manipulation%20datasets%20at%20scale.%20First%2C%20we%20propose%20a%20context-aware%20instantiation%20mechanism%20that%20enforces%20geometric%20consistency%20during%20scene%20synthesis.%20By%20dynamically%20maintaining%20a%20map%20of%20prohibited%20spatial%20areas%20as%20objects%20are%20placed%2C%20our%20system%20prevents%20interpenetration%20and%20ensures%20reachable%2C%20conflict-free%20configurations%20in%20cluttered%20environments.%20Second%2C%20to%20bridge%20the%20gap%20between%20abstract%20intent%20and%20low-level%20control%2C%20we%20employ%20a%20hierarchical%20instruction%20decomposition%20module.%20This%20decomposes%20high-level%20goals%20%28e.g.%2C%20%22get%20ready%20for%20work%22%29%20into%20compositional%20action%20primitives%2C%20facilitating%20coherent%20long-horizon%20planning.%20Crucially%2C%20we%20enforce%20semantic%20correctness%20through%20a%20VLM-based%20verification%20loop.%20Acting%20as%20a%20visual%20critic%2C%20the%20VLM%20performs%20rigorous%20rejection%20sampling%20after%20each%20subtask%2C%20filtering%20out%20%22silent%20failures%22%20where%20code%20executes%20but%20fails%20to%20achieve%20the%20visual%20goal.%20Experiments%20demonstrate%20that%20V-CAGE%20yields%20datasets%20with%20superior%20physical%20and%20semantic%20fidelity%2C%20significantly%20boosting%20the%20success%20rate%20and%20generalization%20of%20downstream%20policies%20compared%20to%20non-verified%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-CAGE%253A%2520Context-Aware%2520Generation%2520and%2520Verification%2520for%2520Scalable%2520Long-Horizon%2520Embodied%2520Tasks%26entry.906535625%3DYaru%2520Liu%2520and%2520Ao-bo%2520Wang%2520and%2520Nanyang%2520Ye%26entry.1292438233%3DLearning%2520long-horizon%2520embodied%2520behaviors%2520from%2520synthetic%2520data%2520remains%2520challenging%2520because%2520generated%2520scenes%2520are%2520often%2520physically%2520implausible%252C%2520language-driven%2520programs%2520frequently%2520%2522succeed%2522%2520without%2520satisfying%2520task%2520semantics%252C%2520and%2520high-level%2520instructions%2520require%2520grounding%2520into%2520executable%2520action%2520sequences.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520V-CAGE%252C%2520a%2520closed-loop%2520framework%2520for%2520generating%2520robust%252C%2520semantically%2520aligned%2520manipulation%2520datasets%2520at%2520scale.%2520First%252C%2520we%2520propose%2520a%2520context-aware%2520instantiation%2520mechanism%2520that%2520enforces%2520geometric%2520consistency%2520during%2520scene%2520synthesis.%2520By%2520dynamically%2520maintaining%2520a%2520map%2520of%2520prohibited%2520spatial%2520areas%2520as%2520objects%2520are%2520placed%252C%2520our%2520system%2520prevents%2520interpenetration%2520and%2520ensures%2520reachable%252C%2520conflict-free%2520configurations%2520in%2520cluttered%2520environments.%2520Second%252C%2520to%2520bridge%2520the%2520gap%2520between%2520abstract%2520intent%2520and%2520low-level%2520control%252C%2520we%2520employ%2520a%2520hierarchical%2520instruction%2520decomposition%2520module.%2520This%2520decomposes%2520high-level%2520goals%2520%2528e.g.%252C%2520%2522get%2520ready%2520for%2520work%2522%2529%2520into%2520compositional%2520action%2520primitives%252C%2520facilitating%2520coherent%2520long-horizon%2520planning.%2520Crucially%252C%2520we%2520enforce%2520semantic%2520correctness%2520through%2520a%2520VLM-based%2520verification%2520loop.%2520Acting%2520as%2520a%2520visual%2520critic%252C%2520the%2520VLM%2520performs%2520rigorous%2520rejection%2520sampling%2520after%2520each%2520subtask%252C%2520filtering%2520out%2520%2522silent%2520failures%2522%2520where%2520code%2520executes%2520but%2520fails%2520to%2520achieve%2520the%2520visual%2520goal.%2520Experiments%2520demonstrate%2520that%2520V-CAGE%2520yields%2520datasets%2520with%2520superior%2520physical%2520and%2520semantic%2520fidelity%252C%2520significantly%2520boosting%2520the%2520success%2520rate%2520and%2520generalization%2520of%2520downstream%2520policies%2520compared%2520to%2520non-verified%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-CAGE%3A%20Context-Aware%20Generation%20and%20Verification%20for%20Scalable%20Long-Horizon%20Embodied%20Tasks&entry.906535625=Yaru%20Liu%20and%20Ao-bo%20Wang%20and%20Nanyang%20Ye&entry.1292438233=Learning%20long-horizon%20embodied%20behaviors%20from%20synthetic%20data%20remains%20challenging%20because%20generated%20scenes%20are%20often%20physically%20implausible%2C%20language-driven%20programs%20frequently%20%22succeed%22%20without%20satisfying%20task%20semantics%2C%20and%20high-level%20instructions%20require%20grounding%20into%20executable%20action%20sequences.%20To%20address%20these%20limitations%2C%20we%20introduce%20V-CAGE%2C%20a%20closed-loop%20framework%20for%20generating%20robust%2C%20semantically%20aligned%20manipulation%20datasets%20at%20scale.%20First%2C%20we%20propose%20a%20context-aware%20instantiation%20mechanism%20that%20enforces%20geometric%20consistency%20during%20scene%20synthesis.%20By%20dynamically%20maintaining%20a%20map%20of%20prohibited%20spatial%20areas%20as%20objects%20are%20placed%2C%20our%20system%20prevents%20interpenetration%20and%20ensures%20reachable%2C%20conflict-free%20configurations%20in%20cluttered%20environments.%20Second%2C%20to%20bridge%20the%20gap%20between%20abstract%20intent%20and%20low-level%20control%2C%20we%20employ%20a%20hierarchical%20instruction%20decomposition%20module.%20This%20decomposes%20high-level%20goals%20%28e.g.%2C%20%22get%20ready%20for%20work%22%29%20into%20compositional%20action%20primitives%2C%20facilitating%20coherent%20long-horizon%20planning.%20Crucially%2C%20we%20enforce%20semantic%20correctness%20through%20a%20VLM-based%20verification%20loop.%20Acting%20as%20a%20visual%20critic%2C%20the%20VLM%20performs%20rigorous%20rejection%20sampling%20after%20each%20subtask%2C%20filtering%20out%20%22silent%20failures%22%20where%20code%20executes%20but%20fails%20to%20achieve%20the%20visual%20goal.%20Experiments%20demonstrate%20that%20V-CAGE%20yields%20datasets%20with%20superior%20physical%20and%20semantic%20fidelity%2C%20significantly%20boosting%20the%20success%20rate%20and%20generalization%20of%20downstream%20policies%20compared%20to%20non-verified%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2601.15164v1&entry.124074799=Read"},
{"title": "QueStER: Query Specification for Generative keyword-based Retrieval", "author": "Arthur Satouf and Yuxuan Zong and Habiboulaye Amadou-Boubacar and Pablo Piantanida and Benjamin Piwowarski", "abstract": "Generative retrieval (GR) differs from the traditional index-then-retrieve pipeline by storing relevance in model parameters and generating retrieval cues directly from the query, but it can be brittle out of domain and expensive to scale. We introduce QueStER (QUEry SpecificaTion for gEnerative Keyword-Based Retrieval), which bridges GR and query reformulation by learning to generate explicit keyword-based search specifications. Given a user query, a lightweight LLM produces a keyword query that is executed by a standard retriever (BM25), combining the generalization benefits of generative query rewriting with the efficiency and scalability of lexical indexing. We train the rewriting policy with reinforcement learning techniques. Across in- and out-of-domain evaluations, QueStER consistently improves over BM25 and is competitive with neural IR baselines, while maintaining strong efficiency.", "link": "http://arxiv.org/abs/2511.05301v2", "date": "2026-01-21", "relevancy": 2.3703, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5023}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4874}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QueStER%3A%20Query%20Specification%20for%20Generative%20keyword-based%20Retrieval&body=Title%3A%20QueStER%3A%20Query%20Specification%20for%20Generative%20keyword-based%20Retrieval%0AAuthor%3A%20Arthur%20Satouf%20and%20Yuxuan%20Zong%20and%20Habiboulaye%20Amadou-Boubacar%20and%20Pablo%20Piantanida%20and%20Benjamin%20Piwowarski%0AAbstract%3A%20Generative%20retrieval%20%28GR%29%20differs%20from%20the%20traditional%20index-then-retrieve%20pipeline%20by%20storing%20relevance%20in%20model%20parameters%20and%20generating%20retrieval%20cues%20directly%20from%20the%20query%2C%20but%20it%20can%20be%20brittle%20out%20of%20domain%20and%20expensive%20to%20scale.%20We%20introduce%20QueStER%20%28QUEry%20SpecificaTion%20for%20gEnerative%20Keyword-Based%20Retrieval%29%2C%20which%20bridges%20GR%20and%20query%20reformulation%20by%20learning%20to%20generate%20explicit%20keyword-based%20search%20specifications.%20Given%20a%20user%20query%2C%20a%20lightweight%20LLM%20produces%20a%20keyword%20query%20that%20is%20executed%20by%20a%20standard%20retriever%20%28BM25%29%2C%20combining%20the%20generalization%20benefits%20of%20generative%20query%20rewriting%20with%20the%20efficiency%20and%20scalability%20of%20lexical%20indexing.%20We%20train%20the%20rewriting%20policy%20with%20reinforcement%20learning%20techniques.%20Across%20in-%20and%20out-of-domain%20evaluations%2C%20QueStER%20consistently%20improves%20over%20BM25%20and%20is%20competitive%20with%20neural%20IR%20baselines%2C%20while%20maintaining%20strong%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2511.05301v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQueStER%253A%2520Query%2520Specification%2520for%2520Generative%2520keyword-based%2520Retrieval%26entry.906535625%3DArthur%2520Satouf%2520and%2520Yuxuan%2520Zong%2520and%2520Habiboulaye%2520Amadou-Boubacar%2520and%2520Pablo%2520Piantanida%2520and%2520Benjamin%2520Piwowarski%26entry.1292438233%3DGenerative%2520retrieval%2520%2528GR%2529%2520differs%2520from%2520the%2520traditional%2520index-then-retrieve%2520pipeline%2520by%2520storing%2520relevance%2520in%2520model%2520parameters%2520and%2520generating%2520retrieval%2520cues%2520directly%2520from%2520the%2520query%252C%2520but%2520it%2520can%2520be%2520brittle%2520out%2520of%2520domain%2520and%2520expensive%2520to%2520scale.%2520We%2520introduce%2520QueStER%2520%2528QUEry%2520SpecificaTion%2520for%2520gEnerative%2520Keyword-Based%2520Retrieval%2529%252C%2520which%2520bridges%2520GR%2520and%2520query%2520reformulation%2520by%2520learning%2520to%2520generate%2520explicit%2520keyword-based%2520search%2520specifications.%2520Given%2520a%2520user%2520query%252C%2520a%2520lightweight%2520LLM%2520produces%2520a%2520keyword%2520query%2520that%2520is%2520executed%2520by%2520a%2520standard%2520retriever%2520%2528BM25%2529%252C%2520combining%2520the%2520generalization%2520benefits%2520of%2520generative%2520query%2520rewriting%2520with%2520the%2520efficiency%2520and%2520scalability%2520of%2520lexical%2520indexing.%2520We%2520train%2520the%2520rewriting%2520policy%2520with%2520reinforcement%2520learning%2520techniques.%2520Across%2520in-%2520and%2520out-of-domain%2520evaluations%252C%2520QueStER%2520consistently%2520improves%2520over%2520BM25%2520and%2520is%2520competitive%2520with%2520neural%2520IR%2520baselines%252C%2520while%2520maintaining%2520strong%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05301v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QueStER%3A%20Query%20Specification%20for%20Generative%20keyword-based%20Retrieval&entry.906535625=Arthur%20Satouf%20and%20Yuxuan%20Zong%20and%20Habiboulaye%20Amadou-Boubacar%20and%20Pablo%20Piantanida%20and%20Benjamin%20Piwowarski&entry.1292438233=Generative%20retrieval%20%28GR%29%20differs%20from%20the%20traditional%20index-then-retrieve%20pipeline%20by%20storing%20relevance%20in%20model%20parameters%20and%20generating%20retrieval%20cues%20directly%20from%20the%20query%2C%20but%20it%20can%20be%20brittle%20out%20of%20domain%20and%20expensive%20to%20scale.%20We%20introduce%20QueStER%20%28QUEry%20SpecificaTion%20for%20gEnerative%20Keyword-Based%20Retrieval%29%2C%20which%20bridges%20GR%20and%20query%20reformulation%20by%20learning%20to%20generate%20explicit%20keyword-based%20search%20specifications.%20Given%20a%20user%20query%2C%20a%20lightweight%20LLM%20produces%20a%20keyword%20query%20that%20is%20executed%20by%20a%20standard%20retriever%20%28BM25%29%2C%20combining%20the%20generalization%20benefits%20of%20generative%20query%20rewriting%20with%20the%20efficiency%20and%20scalability%20of%20lexical%20indexing.%20We%20train%20the%20rewriting%20policy%20with%20reinforcement%20learning%20techniques.%20Across%20in-%20and%20out-of-domain%20evaluations%2C%20QueStER%20consistently%20improves%20over%20BM25%20and%20is%20competitive%20with%20neural%20IR%20baselines%2C%20while%20maintaining%20strong%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2511.05301v2&entry.124074799=Read"},
{"title": "Whitening Spherical Gaussian Mixtures in the Large-Dimensional Regime", "author": "Mohammed Racim Moussa Boudjemaa and Alper Kalle and Xiaoyi Mai and Jos\u00e9 Henrique de Morais Goulart and C\u00e9dric F\u00e9votte", "abstract": "Whitening is a classical technique in unsupervised learning that can facilitate estimation tasks by standardizing data. An important application is the estimation of latent variable models via the decomposition of tensors built from high-order moments. In particular, whitening orthogonalizes the means of a spherical Gaussian mixture model (GMM), thereby making the corresponding moment tensor orthogonally decomposable, hence easier to decompose. However, in the large-dimensional regime (LDR) where data are high-dimensional and scarce, the standard whitening matrix built from the sample covariance becomes ineffective because the latter is spectrally distorted. Consequently, whitened means of a spherical GMM are no longer orthogonal. Using random matrix theory, we derive exact limits for their dot products, which are generally nonzero in the LDR. As our main contribution, we then construct a corrected whitening matrix that restores asymptotic orthogonality, allowing for performance gains in spherical GMM estimation.", "link": "http://arxiv.org/abs/2509.17636v2", "date": "2026-01-21", "relevancy": 2.3696, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4817}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4751}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Whitening%20Spherical%20Gaussian%20Mixtures%20in%20the%20Large-Dimensional%20Regime&body=Title%3A%20Whitening%20Spherical%20Gaussian%20Mixtures%20in%20the%20Large-Dimensional%20Regime%0AAuthor%3A%20Mohammed%20Racim%20Moussa%20Boudjemaa%20and%20Alper%20Kalle%20and%20Xiaoyi%20Mai%20and%20Jos%C3%A9%20Henrique%20de%20Morais%20Goulart%20and%20C%C3%A9dric%20F%C3%A9votte%0AAbstract%3A%20Whitening%20is%20a%20classical%20technique%20in%20unsupervised%20learning%20that%20can%20facilitate%20estimation%20tasks%20by%20standardizing%20data.%20An%20important%20application%20is%20the%20estimation%20of%20latent%20variable%20models%20via%20the%20decomposition%20of%20tensors%20built%20from%20high-order%20moments.%20In%20particular%2C%20whitening%20orthogonalizes%20the%20means%20of%20a%20spherical%20Gaussian%20mixture%20model%20%28GMM%29%2C%20thereby%20making%20the%20corresponding%20moment%20tensor%20orthogonally%20decomposable%2C%20hence%20easier%20to%20decompose.%20However%2C%20in%20the%20large-dimensional%20regime%20%28LDR%29%20where%20data%20are%20high-dimensional%20and%20scarce%2C%20the%20standard%20whitening%20matrix%20built%20from%20the%20sample%20covariance%20becomes%20ineffective%20because%20the%20latter%20is%20spectrally%20distorted.%20Consequently%2C%20whitened%20means%20of%20a%20spherical%20GMM%20are%20no%20longer%20orthogonal.%20Using%20random%20matrix%20theory%2C%20we%20derive%20exact%20limits%20for%20their%20dot%20products%2C%20which%20are%20generally%20nonzero%20in%20the%20LDR.%20As%20our%20main%20contribution%2C%20we%20then%20construct%20a%20corrected%20whitening%20matrix%20that%20restores%20asymptotic%20orthogonality%2C%20allowing%20for%20performance%20gains%20in%20spherical%20GMM%20estimation.%0ALink%3A%20http%3A//arxiv.org/abs/2509.17636v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhitening%2520Spherical%2520Gaussian%2520Mixtures%2520in%2520the%2520Large-Dimensional%2520Regime%26entry.906535625%3DMohammed%2520Racim%2520Moussa%2520Boudjemaa%2520and%2520Alper%2520Kalle%2520and%2520Xiaoyi%2520Mai%2520and%2520Jos%25C3%25A9%2520Henrique%2520de%2520Morais%2520Goulart%2520and%2520C%25C3%25A9dric%2520F%25C3%25A9votte%26entry.1292438233%3DWhitening%2520is%2520a%2520classical%2520technique%2520in%2520unsupervised%2520learning%2520that%2520can%2520facilitate%2520estimation%2520tasks%2520by%2520standardizing%2520data.%2520An%2520important%2520application%2520is%2520the%2520estimation%2520of%2520latent%2520variable%2520models%2520via%2520the%2520decomposition%2520of%2520tensors%2520built%2520from%2520high-order%2520moments.%2520In%2520particular%252C%2520whitening%2520orthogonalizes%2520the%2520means%2520of%2520a%2520spherical%2520Gaussian%2520mixture%2520model%2520%2528GMM%2529%252C%2520thereby%2520making%2520the%2520corresponding%2520moment%2520tensor%2520orthogonally%2520decomposable%252C%2520hence%2520easier%2520to%2520decompose.%2520However%252C%2520in%2520the%2520large-dimensional%2520regime%2520%2528LDR%2529%2520where%2520data%2520are%2520high-dimensional%2520and%2520scarce%252C%2520the%2520standard%2520whitening%2520matrix%2520built%2520from%2520the%2520sample%2520covariance%2520becomes%2520ineffective%2520because%2520the%2520latter%2520is%2520spectrally%2520distorted.%2520Consequently%252C%2520whitened%2520means%2520of%2520a%2520spherical%2520GMM%2520are%2520no%2520longer%2520orthogonal.%2520Using%2520random%2520matrix%2520theory%252C%2520we%2520derive%2520exact%2520limits%2520for%2520their%2520dot%2520products%252C%2520which%2520are%2520generally%2520nonzero%2520in%2520the%2520LDR.%2520As%2520our%2520main%2520contribution%252C%2520we%2520then%2520construct%2520a%2520corrected%2520whitening%2520matrix%2520that%2520restores%2520asymptotic%2520orthogonality%252C%2520allowing%2520for%2520performance%2520gains%2520in%2520spherical%2520GMM%2520estimation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17636v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Whitening%20Spherical%20Gaussian%20Mixtures%20in%20the%20Large-Dimensional%20Regime&entry.906535625=Mohammed%20Racim%20Moussa%20Boudjemaa%20and%20Alper%20Kalle%20and%20Xiaoyi%20Mai%20and%20Jos%C3%A9%20Henrique%20de%20Morais%20Goulart%20and%20C%C3%A9dric%20F%C3%A9votte&entry.1292438233=Whitening%20is%20a%20classical%20technique%20in%20unsupervised%20learning%20that%20can%20facilitate%20estimation%20tasks%20by%20standardizing%20data.%20An%20important%20application%20is%20the%20estimation%20of%20latent%20variable%20models%20via%20the%20decomposition%20of%20tensors%20built%20from%20high-order%20moments.%20In%20particular%2C%20whitening%20orthogonalizes%20the%20means%20of%20a%20spherical%20Gaussian%20mixture%20model%20%28GMM%29%2C%20thereby%20making%20the%20corresponding%20moment%20tensor%20orthogonally%20decomposable%2C%20hence%20easier%20to%20decompose.%20However%2C%20in%20the%20large-dimensional%20regime%20%28LDR%29%20where%20data%20are%20high-dimensional%20and%20scarce%2C%20the%20standard%20whitening%20matrix%20built%20from%20the%20sample%20covariance%20becomes%20ineffective%20because%20the%20latter%20is%20spectrally%20distorted.%20Consequently%2C%20whitened%20means%20of%20a%20spherical%20GMM%20are%20no%20longer%20orthogonal.%20Using%20random%20matrix%20theory%2C%20we%20derive%20exact%20limits%20for%20their%20dot%20products%2C%20which%20are%20generally%20nonzero%20in%20the%20LDR.%20As%20our%20main%20contribution%2C%20we%20then%20construct%20a%20corrected%20whitening%20matrix%20that%20restores%20asymptotic%20orthogonality%2C%20allowing%20for%20performance%20gains%20in%20spherical%20GMM%20estimation.&entry.1838667208=http%3A//arxiv.org/abs/2509.17636v2&entry.124074799=Read"},
{"title": "HumanDiffusion: A Vision-Based Diffusion Trajectory Planner with Human-Conditioned Goals for Search and Rescue UAV", "author": "Faryal Batool and Iana Zhura and Valerii Serpiva and Roohan Ahmed Khan and Ivan Valuev and Issatay Tokmurziyev and Dzmitry Tsetserukou", "abstract": "Reliable human--robot collaboration in emergency scenarios requires autonomous systems that can detect humans, infer navigation goals, and operate safely in dynamic environments. This paper presents HumanDiffusion, a lightweight image-conditioned diffusion planner that generates human-aware navigation trajectories directly from RGB imagery. The system combines YOLO-11--based human detection with diffusion-driven trajectory generation, enabling a quadrotor to approach a target person and deliver medical assistance without relying on prior maps or computationally intensive planning pipelines. Trajectories are predicted in pixel space, ensuring smooth motion and a consistent safety margin around humans. We evaluate HumanDiffusion in simulation and real-world indoor mock-disaster scenarios. On a 300-sample test set, the model achieves a mean squared error of 0.02 in pixel-space trajectory reconstruction. Real-world experiments demonstrate an overall mission success rate of 80% across accident-response and search-and-locate tasks with partial occlusions. These results indicate that human-conditioned diffusion planning offers a practical and robust solution for human-aware UAV navigation in time-critical assistance settings.", "link": "http://arxiv.org/abs/2601.14973v1", "date": "2026-01-21", "relevancy": 2.353, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6101}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5795}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanDiffusion%3A%20A%20Vision-Based%20Diffusion%20Trajectory%20Planner%20with%20Human-Conditioned%20Goals%20for%20Search%20and%20Rescue%20UAV&body=Title%3A%20HumanDiffusion%3A%20A%20Vision-Based%20Diffusion%20Trajectory%20Planner%20with%20Human-Conditioned%20Goals%20for%20Search%20and%20Rescue%20UAV%0AAuthor%3A%20Faryal%20Batool%20and%20Iana%20Zhura%20and%20Valerii%20Serpiva%20and%20Roohan%20Ahmed%20Khan%20and%20Ivan%20Valuev%20and%20Issatay%20Tokmurziyev%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20Reliable%20human--robot%20collaboration%20in%20emergency%20scenarios%20requires%20autonomous%20systems%20that%20can%20detect%20humans%2C%20infer%20navigation%20goals%2C%20and%20operate%20safely%20in%20dynamic%20environments.%20This%20paper%20presents%20HumanDiffusion%2C%20a%20lightweight%20image-conditioned%20diffusion%20planner%20that%20generates%20human-aware%20navigation%20trajectories%20directly%20from%20RGB%20imagery.%20The%20system%20combines%20YOLO-11--based%20human%20detection%20with%20diffusion-driven%20trajectory%20generation%2C%20enabling%20a%20quadrotor%20to%20approach%20a%20target%20person%20and%20deliver%20medical%20assistance%20without%20relying%20on%20prior%20maps%20or%20computationally%20intensive%20planning%20pipelines.%20Trajectories%20are%20predicted%20in%20pixel%20space%2C%20ensuring%20smooth%20motion%20and%20a%20consistent%20safety%20margin%20around%20humans.%20We%20evaluate%20HumanDiffusion%20in%20simulation%20and%20real-world%20indoor%20mock-disaster%20scenarios.%20On%20a%20300-sample%20test%20set%2C%20the%20model%20achieves%20a%20mean%20squared%20error%20of%200.02%20in%20pixel-space%20trajectory%20reconstruction.%20Real-world%20experiments%20demonstrate%20an%20overall%20mission%20success%20rate%20of%2080%25%20across%20accident-response%20and%20search-and-locate%20tasks%20with%20partial%20occlusions.%20These%20results%20indicate%20that%20human-conditioned%20diffusion%20planning%20offers%20a%20practical%20and%20robust%20solution%20for%20human-aware%20UAV%20navigation%20in%20time-critical%20assistance%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanDiffusion%253A%2520A%2520Vision-Based%2520Diffusion%2520Trajectory%2520Planner%2520with%2520Human-Conditioned%2520Goals%2520for%2520Search%2520and%2520Rescue%2520UAV%26entry.906535625%3DFaryal%2520Batool%2520and%2520Iana%2520Zhura%2520and%2520Valerii%2520Serpiva%2520and%2520Roohan%2520Ahmed%2520Khan%2520and%2520Ivan%2520Valuev%2520and%2520Issatay%2520Tokmurziyev%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3DReliable%2520human--robot%2520collaboration%2520in%2520emergency%2520scenarios%2520requires%2520autonomous%2520systems%2520that%2520can%2520detect%2520humans%252C%2520infer%2520navigation%2520goals%252C%2520and%2520operate%2520safely%2520in%2520dynamic%2520environments.%2520This%2520paper%2520presents%2520HumanDiffusion%252C%2520a%2520lightweight%2520image-conditioned%2520diffusion%2520planner%2520that%2520generates%2520human-aware%2520navigation%2520trajectories%2520directly%2520from%2520RGB%2520imagery.%2520The%2520system%2520combines%2520YOLO-11--based%2520human%2520detection%2520with%2520diffusion-driven%2520trajectory%2520generation%252C%2520enabling%2520a%2520quadrotor%2520to%2520approach%2520a%2520target%2520person%2520and%2520deliver%2520medical%2520assistance%2520without%2520relying%2520on%2520prior%2520maps%2520or%2520computationally%2520intensive%2520planning%2520pipelines.%2520Trajectories%2520are%2520predicted%2520in%2520pixel%2520space%252C%2520ensuring%2520smooth%2520motion%2520and%2520a%2520consistent%2520safety%2520margin%2520around%2520humans.%2520We%2520evaluate%2520HumanDiffusion%2520in%2520simulation%2520and%2520real-world%2520indoor%2520mock-disaster%2520scenarios.%2520On%2520a%2520300-sample%2520test%2520set%252C%2520the%2520model%2520achieves%2520a%2520mean%2520squared%2520error%2520of%25200.02%2520in%2520pixel-space%2520trajectory%2520reconstruction.%2520Real-world%2520experiments%2520demonstrate%2520an%2520overall%2520mission%2520success%2520rate%2520of%252080%2525%2520across%2520accident-response%2520and%2520search-and-locate%2520tasks%2520with%2520partial%2520occlusions.%2520These%2520results%2520indicate%2520that%2520human-conditioned%2520diffusion%2520planning%2520offers%2520a%2520practical%2520and%2520robust%2520solution%2520for%2520human-aware%2520UAV%2520navigation%2520in%2520time-critical%2520assistance%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanDiffusion%3A%20A%20Vision-Based%20Diffusion%20Trajectory%20Planner%20with%20Human-Conditioned%20Goals%20for%20Search%20and%20Rescue%20UAV&entry.906535625=Faryal%20Batool%20and%20Iana%20Zhura%20and%20Valerii%20Serpiva%20and%20Roohan%20Ahmed%20Khan%20and%20Ivan%20Valuev%20and%20Issatay%20Tokmurziyev%20and%20Dzmitry%20Tsetserukou&entry.1292438233=Reliable%20human--robot%20collaboration%20in%20emergency%20scenarios%20requires%20autonomous%20systems%20that%20can%20detect%20humans%2C%20infer%20navigation%20goals%2C%20and%20operate%20safely%20in%20dynamic%20environments.%20This%20paper%20presents%20HumanDiffusion%2C%20a%20lightweight%20image-conditioned%20diffusion%20planner%20that%20generates%20human-aware%20navigation%20trajectories%20directly%20from%20RGB%20imagery.%20The%20system%20combines%20YOLO-11--based%20human%20detection%20with%20diffusion-driven%20trajectory%20generation%2C%20enabling%20a%20quadrotor%20to%20approach%20a%20target%20person%20and%20deliver%20medical%20assistance%20without%20relying%20on%20prior%20maps%20or%20computationally%20intensive%20planning%20pipelines.%20Trajectories%20are%20predicted%20in%20pixel%20space%2C%20ensuring%20smooth%20motion%20and%20a%20consistent%20safety%20margin%20around%20humans.%20We%20evaluate%20HumanDiffusion%20in%20simulation%20and%20real-world%20indoor%20mock-disaster%20scenarios.%20On%20a%20300-sample%20test%20set%2C%20the%20model%20achieves%20a%20mean%20squared%20error%20of%200.02%20in%20pixel-space%20trajectory%20reconstruction.%20Real-world%20experiments%20demonstrate%20an%20overall%20mission%20success%20rate%20of%2080%25%20across%20accident-response%20and%20search-and-locate%20tasks%20with%20partial%20occlusions.%20These%20results%20indicate%20that%20human-conditioned%20diffusion%20planning%20offers%20a%20practical%20and%20robust%20solution%20for%20human-aware%20UAV%20navigation%20in%20time-critical%20assistance%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2601.14973v1&entry.124074799=Read"},
{"title": "Exploring Resolution-Wise Shared Attention in Hybrid Mamba-U-Nets for Improved Cross-Corpus Speech Enhancement", "author": "Nikolai Lund K\u00fchne and Jesper Jensen and Jan \u00d8stergaard and Zheng-Hua Tan", "abstract": "Recent advances in speech enhancement have shown that models combining Mamba and attention mechanisms yield superior cross-corpus generalization performance. At the same time, integrating Mamba in a U-Net structure has yielded state-of-the-art enhancement performance, while reducing both model size and computational complexity. Inspired by these insights, we propose RWSA-MambaUNet, a novel and efficient hybrid model combining Mamba and multi-head attention in a U-Net structure for improved cross-corpus performance. Resolution-wise shared attention (RWSA) refers to layerwise attention-sharing across corresponding time- and frequency resolutions. Our best-performing RWSA-MambaUNet model achieves state-of-the-art generalization performance on two out-of-domain test sets. Notably, our smallest model surpasses all baselines on the out-of-domain DNS 2020 test set in terms of PESQ, SSNR, and ESTOI, and on the out-of-domain EARS-WHAM_v2 test set in terms of SSNR, ESTOI, and SI-SDR, while using less than half the model parameters and a fraction of the FLOPs.", "link": "http://arxiv.org/abs/2510.01958v2", "date": "2026-01-21", "relevancy": 2.3519, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4751}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4733}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Resolution-Wise%20Shared%20Attention%20in%20Hybrid%20Mamba-U-Nets%20for%20Improved%20Cross-Corpus%20Speech%20Enhancement&body=Title%3A%20Exploring%20Resolution-Wise%20Shared%20Attention%20in%20Hybrid%20Mamba-U-Nets%20for%20Improved%20Cross-Corpus%20Speech%20Enhancement%0AAuthor%3A%20Nikolai%20Lund%20K%C3%BChne%20and%20Jesper%20Jensen%20and%20Jan%20%C3%98stergaard%20and%20Zheng-Hua%20Tan%0AAbstract%3A%20Recent%20advances%20in%20speech%20enhancement%20have%20shown%20that%20models%20combining%20Mamba%20and%20attention%20mechanisms%20yield%20superior%20cross-corpus%20generalization%20performance.%20At%20the%20same%20time%2C%20integrating%20Mamba%20in%20a%20U-Net%20structure%20has%20yielded%20state-of-the-art%20enhancement%20performance%2C%20while%20reducing%20both%20model%20size%20and%20computational%20complexity.%20Inspired%20by%20these%20insights%2C%20we%20propose%20RWSA-MambaUNet%2C%20a%20novel%20and%20efficient%20hybrid%20model%20combining%20Mamba%20and%20multi-head%20attention%20in%20a%20U-Net%20structure%20for%20improved%20cross-corpus%20performance.%20Resolution-wise%20shared%20attention%20%28RWSA%29%20refers%20to%20layerwise%20attention-sharing%20across%20corresponding%20time-%20and%20frequency%20resolutions.%20Our%20best-performing%20RWSA-MambaUNet%20model%20achieves%20state-of-the-art%20generalization%20performance%20on%20two%20out-of-domain%20test%20sets.%20Notably%2C%20our%20smallest%20model%20surpasses%20all%20baselines%20on%20the%20out-of-domain%20DNS%202020%20test%20set%20in%20terms%20of%20PESQ%2C%20SSNR%2C%20and%20ESTOI%2C%20and%20on%20the%20out-of-domain%20EARS-WHAM_v2%20test%20set%20in%20terms%20of%20SSNR%2C%20ESTOI%2C%20and%20SI-SDR%2C%20while%20using%20less%20than%20half%20the%20model%20parameters%20and%20a%20fraction%20of%20the%20FLOPs.%0ALink%3A%20http%3A//arxiv.org/abs/2510.01958v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Resolution-Wise%2520Shared%2520Attention%2520in%2520Hybrid%2520Mamba-U-Nets%2520for%2520Improved%2520Cross-Corpus%2520Speech%2520Enhancement%26entry.906535625%3DNikolai%2520Lund%2520K%25C3%25BChne%2520and%2520Jesper%2520Jensen%2520and%2520Jan%2520%25C3%2598stergaard%2520and%2520Zheng-Hua%2520Tan%26entry.1292438233%3DRecent%2520advances%2520in%2520speech%2520enhancement%2520have%2520shown%2520that%2520models%2520combining%2520Mamba%2520and%2520attention%2520mechanisms%2520yield%2520superior%2520cross-corpus%2520generalization%2520performance.%2520At%2520the%2520same%2520time%252C%2520integrating%2520Mamba%2520in%2520a%2520U-Net%2520structure%2520has%2520yielded%2520state-of-the-art%2520enhancement%2520performance%252C%2520while%2520reducing%2520both%2520model%2520size%2520and%2520computational%2520complexity.%2520Inspired%2520by%2520these%2520insights%252C%2520we%2520propose%2520RWSA-MambaUNet%252C%2520a%2520novel%2520and%2520efficient%2520hybrid%2520model%2520combining%2520Mamba%2520and%2520multi-head%2520attention%2520in%2520a%2520U-Net%2520structure%2520for%2520improved%2520cross-corpus%2520performance.%2520Resolution-wise%2520shared%2520attention%2520%2528RWSA%2529%2520refers%2520to%2520layerwise%2520attention-sharing%2520across%2520corresponding%2520time-%2520and%2520frequency%2520resolutions.%2520Our%2520best-performing%2520RWSA-MambaUNet%2520model%2520achieves%2520state-of-the-art%2520generalization%2520performance%2520on%2520two%2520out-of-domain%2520test%2520sets.%2520Notably%252C%2520our%2520smallest%2520model%2520surpasses%2520all%2520baselines%2520on%2520the%2520out-of-domain%2520DNS%25202020%2520test%2520set%2520in%2520terms%2520of%2520PESQ%252C%2520SSNR%252C%2520and%2520ESTOI%252C%2520and%2520on%2520the%2520out-of-domain%2520EARS-WHAM_v2%2520test%2520set%2520in%2520terms%2520of%2520SSNR%252C%2520ESTOI%252C%2520and%2520SI-SDR%252C%2520while%2520using%2520less%2520than%2520half%2520the%2520model%2520parameters%2520and%2520a%2520fraction%2520of%2520the%2520FLOPs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.01958v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Resolution-Wise%20Shared%20Attention%20in%20Hybrid%20Mamba-U-Nets%20for%20Improved%20Cross-Corpus%20Speech%20Enhancement&entry.906535625=Nikolai%20Lund%20K%C3%BChne%20and%20Jesper%20Jensen%20and%20Jan%20%C3%98stergaard%20and%20Zheng-Hua%20Tan&entry.1292438233=Recent%20advances%20in%20speech%20enhancement%20have%20shown%20that%20models%20combining%20Mamba%20and%20attention%20mechanisms%20yield%20superior%20cross-corpus%20generalization%20performance.%20At%20the%20same%20time%2C%20integrating%20Mamba%20in%20a%20U-Net%20structure%20has%20yielded%20state-of-the-art%20enhancement%20performance%2C%20while%20reducing%20both%20model%20size%20and%20computational%20complexity.%20Inspired%20by%20these%20insights%2C%20we%20propose%20RWSA-MambaUNet%2C%20a%20novel%20and%20efficient%20hybrid%20model%20combining%20Mamba%20and%20multi-head%20attention%20in%20a%20U-Net%20structure%20for%20improved%20cross-corpus%20performance.%20Resolution-wise%20shared%20attention%20%28RWSA%29%20refers%20to%20layerwise%20attention-sharing%20across%20corresponding%20time-%20and%20frequency%20resolutions.%20Our%20best-performing%20RWSA-MambaUNet%20model%20achieves%20state-of-the-art%20generalization%20performance%20on%20two%20out-of-domain%20test%20sets.%20Notably%2C%20our%20smallest%20model%20surpasses%20all%20baselines%20on%20the%20out-of-domain%20DNS%202020%20test%20set%20in%20terms%20of%20PESQ%2C%20SSNR%2C%20and%20ESTOI%2C%20and%20on%20the%20out-of-domain%20EARS-WHAM_v2%20test%20set%20in%20terms%20of%20SSNR%2C%20ESTOI%2C%20and%20SI-SDR%2C%20while%20using%20less%20than%20half%20the%20model%20parameters%20and%20a%20fraction%20of%20the%20FLOPs.&entry.1838667208=http%3A//arxiv.org/abs/2510.01958v2&entry.124074799=Read"},
{"title": "PROGRESSLM: Towards Progress Reasoning in Vision-Language Models", "author": "Jianshu Zhang and Chengxuan Qian and Haosen Sun and Haoran Lu and Dingcheng Wang and Letian Xue and Han Liu", "abstract": "Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content. While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations. To this end, we introduce Progress-Bench, a benchmark for systematically evaluating progress reasoning in VLMs. Beyond benchmarking, we further explore a human-inspired two-stage progress reasoning paradigm through both training-free prompting and training-based approach based on curated dataset ProgressLM-45K. Experiments on 14 VLMs show that most models are not yet ready for task progress estimation, exhibiting sensitivity to demonstration modality and viewpoint changes, as well as poor handling of unanswerable cases. While training-free prompting that enforces structured progress reasoning yields limited and model-dependent gains, the training-based ProgressLM-3B achieves consistent improvements even at a small model scale, despite being trained on a task set fully disjoint from the evaluation tasks. Further analyses reveal characteristic error patterns and clarify when and why progress reasoning succeeds or fails.", "link": "http://arxiv.org/abs/2601.15224v1", "date": "2026-01-21", "relevancy": 2.3351, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5971}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5971}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PROGRESSLM%3A%20Towards%20Progress%20Reasoning%20in%20Vision-Language%20Models&body=Title%3A%20PROGRESSLM%3A%20Towards%20Progress%20Reasoning%20in%20Vision-Language%20Models%0AAuthor%3A%20Jianshu%20Zhang%20and%20Chengxuan%20Qian%20and%20Haosen%20Sun%20and%20Haoran%20Lu%20and%20Dingcheng%20Wang%20and%20Letian%20Xue%20and%20Han%20Liu%0AAbstract%3A%20Estimating%20task%20progress%20requires%20reasoning%20over%20long-horizon%20dynamics%20rather%20than%20recognizing%20static%20visual%20content.%20While%20modern%20Vision-Language%20Models%20%28VLMs%29%20excel%20at%20describing%20what%20is%20visible%2C%20it%20remains%20unclear%20whether%20they%20can%20infer%20how%20far%20a%20task%20has%20progressed%20from%20partial%20observations.%20To%20this%20end%2C%20we%20introduce%20Progress-Bench%2C%20a%20benchmark%20for%20systematically%20evaluating%20progress%20reasoning%20in%20VLMs.%20Beyond%20benchmarking%2C%20we%20further%20explore%20a%20human-inspired%20two-stage%20progress%20reasoning%20paradigm%20through%20both%20training-free%20prompting%20and%20training-based%20approach%20based%20on%20curated%20dataset%20ProgressLM-45K.%20Experiments%20on%2014%20VLMs%20show%20that%20most%20models%20are%20not%20yet%20ready%20for%20task%20progress%20estimation%2C%20exhibiting%20sensitivity%20to%20demonstration%20modality%20and%20viewpoint%20changes%2C%20as%20well%20as%20poor%20handling%20of%20unanswerable%20cases.%20While%20training-free%20prompting%20that%20enforces%20structured%20progress%20reasoning%20yields%20limited%20and%20model-dependent%20gains%2C%20the%20training-based%20ProgressLM-3B%20achieves%20consistent%20improvements%20even%20at%20a%20small%20model%20scale%2C%20despite%20being%20trained%20on%20a%20task%20set%20fully%20disjoint%20from%20the%20evaluation%20tasks.%20Further%20analyses%20reveal%20characteristic%20error%20patterns%20and%20clarify%20when%20and%20why%20progress%20reasoning%20succeeds%20or%20fails.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPROGRESSLM%253A%2520Towards%2520Progress%2520Reasoning%2520in%2520Vision-Language%2520Models%26entry.906535625%3DJianshu%2520Zhang%2520and%2520Chengxuan%2520Qian%2520and%2520Haosen%2520Sun%2520and%2520Haoran%2520Lu%2520and%2520Dingcheng%2520Wang%2520and%2520Letian%2520Xue%2520and%2520Han%2520Liu%26entry.1292438233%3DEstimating%2520task%2520progress%2520requires%2520reasoning%2520over%2520long-horizon%2520dynamics%2520rather%2520than%2520recognizing%2520static%2520visual%2520content.%2520While%2520modern%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520excel%2520at%2520describing%2520what%2520is%2520visible%252C%2520it%2520remains%2520unclear%2520whether%2520they%2520can%2520infer%2520how%2520far%2520a%2520task%2520has%2520progressed%2520from%2520partial%2520observations.%2520To%2520this%2520end%252C%2520we%2520introduce%2520Progress-Bench%252C%2520a%2520benchmark%2520for%2520systematically%2520evaluating%2520progress%2520reasoning%2520in%2520VLMs.%2520Beyond%2520benchmarking%252C%2520we%2520further%2520explore%2520a%2520human-inspired%2520two-stage%2520progress%2520reasoning%2520paradigm%2520through%2520both%2520training-free%2520prompting%2520and%2520training-based%2520approach%2520based%2520on%2520curated%2520dataset%2520ProgressLM-45K.%2520Experiments%2520on%252014%2520VLMs%2520show%2520that%2520most%2520models%2520are%2520not%2520yet%2520ready%2520for%2520task%2520progress%2520estimation%252C%2520exhibiting%2520sensitivity%2520to%2520demonstration%2520modality%2520and%2520viewpoint%2520changes%252C%2520as%2520well%2520as%2520poor%2520handling%2520of%2520unanswerable%2520cases.%2520While%2520training-free%2520prompting%2520that%2520enforces%2520structured%2520progress%2520reasoning%2520yields%2520limited%2520and%2520model-dependent%2520gains%252C%2520the%2520training-based%2520ProgressLM-3B%2520achieves%2520consistent%2520improvements%2520even%2520at%2520a%2520small%2520model%2520scale%252C%2520despite%2520being%2520trained%2520on%2520a%2520task%2520set%2520fully%2520disjoint%2520from%2520the%2520evaluation%2520tasks.%2520Further%2520analyses%2520reveal%2520characteristic%2520error%2520patterns%2520and%2520clarify%2520when%2520and%2520why%2520progress%2520reasoning%2520succeeds%2520or%2520fails.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PROGRESSLM%3A%20Towards%20Progress%20Reasoning%20in%20Vision-Language%20Models&entry.906535625=Jianshu%20Zhang%20and%20Chengxuan%20Qian%20and%20Haosen%20Sun%20and%20Haoran%20Lu%20and%20Dingcheng%20Wang%20and%20Letian%20Xue%20and%20Han%20Liu&entry.1292438233=Estimating%20task%20progress%20requires%20reasoning%20over%20long-horizon%20dynamics%20rather%20than%20recognizing%20static%20visual%20content.%20While%20modern%20Vision-Language%20Models%20%28VLMs%29%20excel%20at%20describing%20what%20is%20visible%2C%20it%20remains%20unclear%20whether%20they%20can%20infer%20how%20far%20a%20task%20has%20progressed%20from%20partial%20observations.%20To%20this%20end%2C%20we%20introduce%20Progress-Bench%2C%20a%20benchmark%20for%20systematically%20evaluating%20progress%20reasoning%20in%20VLMs.%20Beyond%20benchmarking%2C%20we%20further%20explore%20a%20human-inspired%20two-stage%20progress%20reasoning%20paradigm%20through%20both%20training-free%20prompting%20and%20training-based%20approach%20based%20on%20curated%20dataset%20ProgressLM-45K.%20Experiments%20on%2014%20VLMs%20show%20that%20most%20models%20are%20not%20yet%20ready%20for%20task%20progress%20estimation%2C%20exhibiting%20sensitivity%20to%20demonstration%20modality%20and%20viewpoint%20changes%2C%20as%20well%20as%20poor%20handling%20of%20unanswerable%20cases.%20While%20training-free%20prompting%20that%20enforces%20structured%20progress%20reasoning%20yields%20limited%20and%20model-dependent%20gains%2C%20the%20training-based%20ProgressLM-3B%20achieves%20consistent%20improvements%20even%20at%20a%20small%20model%20scale%2C%20despite%20being%20trained%20on%20a%20task%20set%20fully%20disjoint%20from%20the%20evaluation%20tasks.%20Further%20analyses%20reveal%20characteristic%20error%20patterns%20and%20clarify%20when%20and%20why%20progress%20reasoning%20succeeds%20or%20fails.&entry.1838667208=http%3A//arxiv.org/abs/2601.15224v1&entry.124074799=Read"},
{"title": "Context-aware Learned Mesh-based Simulation via Trajectory-Level Meta-Learning", "author": "Philipp Dahlinger and Niklas Freymuth and Tai Hoang and Tobias W\u00fcrth and Michael Volpp and Luise K\u00e4rger and Gerhard Neumann", "abstract": "Simulating object deformations is a critical challenge across many scientific domains, including robotics, manufacturing, and structural mechanics. Learned Graph Network Simulators (GNSs) offer a promising alternative to traditional mesh-based physics simulators. Their speed and inherent differentiability make them particularly well suited for applications that require fast and accurate simulations, such as robotic manipulation or manufacturing optimization. However, existing learned simulators typically rely on single-step observations, which limits their ability to exploit temporal context. Without this information, these models fail to infer, e.g., material properties. Further, they rely on auto-regressive rollouts, which quickly accumulate error for long trajectories. We instead frame mesh-based simulation as a trajectory-level meta-learning problem. Using Conditional Neural Processes, our method enables rapid adaptation to new simulation scenarios from limited initial data while capturing their latent simulation properties. We utilize movement primitives to directly predict fast, stable and accurate simulations from a single model call. The resulting approach, Movement-primitive Meta-MeshGraphNet (M3GN), provides higher simulation accuracy at a fraction of the runtime cost compared to state-of-the-art GNSs across several tasks.", "link": "http://arxiv.org/abs/2511.05234v2", "date": "2026-01-21", "relevancy": 2.321, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.604}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5681}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-aware%20Learned%20Mesh-based%20Simulation%20via%20Trajectory-Level%20Meta-Learning&body=Title%3A%20Context-aware%20Learned%20Mesh-based%20Simulation%20via%20Trajectory-Level%20Meta-Learning%0AAuthor%3A%20Philipp%20Dahlinger%20and%20Niklas%20Freymuth%20and%20Tai%20Hoang%20and%20Tobias%20W%C3%BCrth%20and%20Michael%20Volpp%20and%20Luise%20K%C3%A4rger%20and%20Gerhard%20Neumann%0AAbstract%3A%20Simulating%20object%20deformations%20is%20a%20critical%20challenge%20across%20many%20scientific%20domains%2C%20including%20robotics%2C%20manufacturing%2C%20and%20structural%20mechanics.%20Learned%20Graph%20Network%20Simulators%20%28GNSs%29%20offer%20a%20promising%20alternative%20to%20traditional%20mesh-based%20physics%20simulators.%20Their%20speed%20and%20inherent%20differentiability%20make%20them%20particularly%20well%20suited%20for%20applications%20that%20require%20fast%20and%20accurate%20simulations%2C%20such%20as%20robotic%20manipulation%20or%20manufacturing%20optimization.%20However%2C%20existing%20learned%20simulators%20typically%20rely%20on%20single-step%20observations%2C%20which%20limits%20their%20ability%20to%20exploit%20temporal%20context.%20Without%20this%20information%2C%20these%20models%20fail%20to%20infer%2C%20e.g.%2C%20material%20properties.%20Further%2C%20they%20rely%20on%20auto-regressive%20rollouts%2C%20which%20quickly%20accumulate%20error%20for%20long%20trajectories.%20We%20instead%20frame%20mesh-based%20simulation%20as%20a%20trajectory-level%20meta-learning%20problem.%20Using%20Conditional%20Neural%20Processes%2C%20our%20method%20enables%20rapid%20adaptation%20to%20new%20simulation%20scenarios%20from%20limited%20initial%20data%20while%20capturing%20their%20latent%20simulation%20properties.%20We%20utilize%20movement%20primitives%20to%20directly%20predict%20fast%2C%20stable%20and%20accurate%20simulations%20from%20a%20single%20model%20call.%20The%20resulting%20approach%2C%20Movement-primitive%20Meta-MeshGraphNet%20%28M3GN%29%2C%20provides%20higher%20simulation%20accuracy%20at%20a%20fraction%20of%20the%20runtime%20cost%20compared%20to%20state-of-the-art%20GNSs%20across%20several%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.05234v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-aware%2520Learned%2520Mesh-based%2520Simulation%2520via%2520Trajectory-Level%2520Meta-Learning%26entry.906535625%3DPhilipp%2520Dahlinger%2520and%2520Niklas%2520Freymuth%2520and%2520Tai%2520Hoang%2520and%2520Tobias%2520W%25C3%25BCrth%2520and%2520Michael%2520Volpp%2520and%2520Luise%2520K%25C3%25A4rger%2520and%2520Gerhard%2520Neumann%26entry.1292438233%3DSimulating%2520object%2520deformations%2520is%2520a%2520critical%2520challenge%2520across%2520many%2520scientific%2520domains%252C%2520including%2520robotics%252C%2520manufacturing%252C%2520and%2520structural%2520mechanics.%2520Learned%2520Graph%2520Network%2520Simulators%2520%2528GNSs%2529%2520offer%2520a%2520promising%2520alternative%2520to%2520traditional%2520mesh-based%2520physics%2520simulators.%2520Their%2520speed%2520and%2520inherent%2520differentiability%2520make%2520them%2520particularly%2520well%2520suited%2520for%2520applications%2520that%2520require%2520fast%2520and%2520accurate%2520simulations%252C%2520such%2520as%2520robotic%2520manipulation%2520or%2520manufacturing%2520optimization.%2520However%252C%2520existing%2520learned%2520simulators%2520typically%2520rely%2520on%2520single-step%2520observations%252C%2520which%2520limits%2520their%2520ability%2520to%2520exploit%2520temporal%2520context.%2520Without%2520this%2520information%252C%2520these%2520models%2520fail%2520to%2520infer%252C%2520e.g.%252C%2520material%2520properties.%2520Further%252C%2520they%2520rely%2520on%2520auto-regressive%2520rollouts%252C%2520which%2520quickly%2520accumulate%2520error%2520for%2520long%2520trajectories.%2520We%2520instead%2520frame%2520mesh-based%2520simulation%2520as%2520a%2520trajectory-level%2520meta-learning%2520problem.%2520Using%2520Conditional%2520Neural%2520Processes%252C%2520our%2520method%2520enables%2520rapid%2520adaptation%2520to%2520new%2520simulation%2520scenarios%2520from%2520limited%2520initial%2520data%2520while%2520capturing%2520their%2520latent%2520simulation%2520properties.%2520We%2520utilize%2520movement%2520primitives%2520to%2520directly%2520predict%2520fast%252C%2520stable%2520and%2520accurate%2520simulations%2520from%2520a%2520single%2520model%2520call.%2520The%2520resulting%2520approach%252C%2520Movement-primitive%2520Meta-MeshGraphNet%2520%2528M3GN%2529%252C%2520provides%2520higher%2520simulation%2520accuracy%2520at%2520a%2520fraction%2520of%2520the%2520runtime%2520cost%2520compared%2520to%2520state-of-the-art%2520GNSs%2520across%2520several%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05234v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-aware%20Learned%20Mesh-based%20Simulation%20via%20Trajectory-Level%20Meta-Learning&entry.906535625=Philipp%20Dahlinger%20and%20Niklas%20Freymuth%20and%20Tai%20Hoang%20and%20Tobias%20W%C3%BCrth%20and%20Michael%20Volpp%20and%20Luise%20K%C3%A4rger%20and%20Gerhard%20Neumann&entry.1292438233=Simulating%20object%20deformations%20is%20a%20critical%20challenge%20across%20many%20scientific%20domains%2C%20including%20robotics%2C%20manufacturing%2C%20and%20structural%20mechanics.%20Learned%20Graph%20Network%20Simulators%20%28GNSs%29%20offer%20a%20promising%20alternative%20to%20traditional%20mesh-based%20physics%20simulators.%20Their%20speed%20and%20inherent%20differentiability%20make%20them%20particularly%20well%20suited%20for%20applications%20that%20require%20fast%20and%20accurate%20simulations%2C%20such%20as%20robotic%20manipulation%20or%20manufacturing%20optimization.%20However%2C%20existing%20learned%20simulators%20typically%20rely%20on%20single-step%20observations%2C%20which%20limits%20their%20ability%20to%20exploit%20temporal%20context.%20Without%20this%20information%2C%20these%20models%20fail%20to%20infer%2C%20e.g.%2C%20material%20properties.%20Further%2C%20they%20rely%20on%20auto-regressive%20rollouts%2C%20which%20quickly%20accumulate%20error%20for%20long%20trajectories.%20We%20instead%20frame%20mesh-based%20simulation%20as%20a%20trajectory-level%20meta-learning%20problem.%20Using%20Conditional%20Neural%20Processes%2C%20our%20method%20enables%20rapid%20adaptation%20to%20new%20simulation%20scenarios%20from%20limited%20initial%20data%20while%20capturing%20their%20latent%20simulation%20properties.%20We%20utilize%20movement%20primitives%20to%20directly%20predict%20fast%2C%20stable%20and%20accurate%20simulations%20from%20a%20single%20model%20call.%20The%20resulting%20approach%2C%20Movement-primitive%20Meta-MeshGraphNet%20%28M3GN%29%2C%20provides%20higher%20simulation%20accuracy%20at%20a%20fraction%20of%20the%20runtime%20cost%20compared%20to%20state-of-the-art%20GNSs%20across%20several%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2511.05234v2&entry.124074799=Read"},
{"title": "LoRAP: Low-Rank Aggregation Prompting for Quantized Graph Neural Networks Training", "author": "Chenyu Liu and Haige Li and Luca Rossi", "abstract": "Graph Neural Networks (GNNs) are neural networks that aim to process graph data, capturing the relationships and interactions between nodes using the message-passing mechanism. GNN quantization has emerged as a promising approach for reducing model size and accelerating inference in resource-constrained environments. Compared to quantization in LLMs, quantizing graph features is more emphasized in GNNs. Inspired by the above, we propose to leverage prompt learning, which manipulates the input data, to improve the performance of quantization-aware training (QAT) for GNNs. To mitigate the issue that prompting the node features alone can only make part of the quantized aggregation result optimal, we introduce Low-Rank Aggregation Prompting (LoRAP), which injects lightweight, input-dependent prompts into each aggregated feature to optimize the results of quantized aggregations. Extensive evaluations on 4 leading QAT frameworks over 9 graph datasets demonstrate that LoRAP consistently enhances the performance of low-bit quantized GNNs while introducing a minimal computational overhead.", "link": "http://arxiv.org/abs/2601.15079v1", "date": "2026-01-21", "relevancy": 2.3169, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4714}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4637}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRAP%3A%20Low-Rank%20Aggregation%20Prompting%20for%20Quantized%20Graph%20Neural%20Networks%20Training&body=Title%3A%20LoRAP%3A%20Low-Rank%20Aggregation%20Prompting%20for%20Quantized%20Graph%20Neural%20Networks%20Training%0AAuthor%3A%20Chenyu%20Liu%20and%20Haige%20Li%20and%20Luca%20Rossi%0AAbstract%3A%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20neural%20networks%20that%20aim%20to%20process%20graph%20data%2C%20capturing%20the%20relationships%20and%20interactions%20between%20nodes%20using%20the%20message-passing%20mechanism.%20GNN%20quantization%20has%20emerged%20as%20a%20promising%20approach%20for%20reducing%20model%20size%20and%20accelerating%20inference%20in%20resource-constrained%20environments.%20Compared%20to%20quantization%20in%20LLMs%2C%20quantizing%20graph%20features%20is%20more%20emphasized%20in%20GNNs.%20Inspired%20by%20the%20above%2C%20we%20propose%20to%20leverage%20prompt%20learning%2C%20which%20manipulates%20the%20input%20data%2C%20to%20improve%20the%20performance%20of%20quantization-aware%20training%20%28QAT%29%20for%20GNNs.%20To%20mitigate%20the%20issue%20that%20prompting%20the%20node%20features%20alone%20can%20only%20make%20part%20of%20the%20quantized%20aggregation%20result%20optimal%2C%20we%20introduce%20Low-Rank%20Aggregation%20Prompting%20%28LoRAP%29%2C%20which%20injects%20lightweight%2C%20input-dependent%20prompts%20into%20each%20aggregated%20feature%20to%20optimize%20the%20results%20of%20quantized%20aggregations.%20Extensive%20evaluations%20on%204%20leading%20QAT%20frameworks%20over%209%20graph%20datasets%20demonstrate%20that%20LoRAP%20consistently%20enhances%20the%20performance%20of%20low-bit%20quantized%20GNNs%20while%20introducing%20a%20minimal%20computational%20overhead.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRAP%253A%2520Low-Rank%2520Aggregation%2520Prompting%2520for%2520Quantized%2520Graph%2520Neural%2520Networks%2520Training%26entry.906535625%3DChenyu%2520Liu%2520and%2520Haige%2520Li%2520and%2520Luca%2520Rossi%26entry.1292438233%3DGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520neural%2520networks%2520that%2520aim%2520to%2520process%2520graph%2520data%252C%2520capturing%2520the%2520relationships%2520and%2520interactions%2520between%2520nodes%2520using%2520the%2520message-passing%2520mechanism.%2520GNN%2520quantization%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520for%2520reducing%2520model%2520size%2520and%2520accelerating%2520inference%2520in%2520resource-constrained%2520environments.%2520Compared%2520to%2520quantization%2520in%2520LLMs%252C%2520quantizing%2520graph%2520features%2520is%2520more%2520emphasized%2520in%2520GNNs.%2520Inspired%2520by%2520the%2520above%252C%2520we%2520propose%2520to%2520leverage%2520prompt%2520learning%252C%2520which%2520manipulates%2520the%2520input%2520data%252C%2520to%2520improve%2520the%2520performance%2520of%2520quantization-aware%2520training%2520%2528QAT%2529%2520for%2520GNNs.%2520To%2520mitigate%2520the%2520issue%2520that%2520prompting%2520the%2520node%2520features%2520alone%2520can%2520only%2520make%2520part%2520of%2520the%2520quantized%2520aggregation%2520result%2520optimal%252C%2520we%2520introduce%2520Low-Rank%2520Aggregation%2520Prompting%2520%2528LoRAP%2529%252C%2520which%2520injects%2520lightweight%252C%2520input-dependent%2520prompts%2520into%2520each%2520aggregated%2520feature%2520to%2520optimize%2520the%2520results%2520of%2520quantized%2520aggregations.%2520Extensive%2520evaluations%2520on%25204%2520leading%2520QAT%2520frameworks%2520over%25209%2520graph%2520datasets%2520demonstrate%2520that%2520LoRAP%2520consistently%2520enhances%2520the%2520performance%2520of%2520low-bit%2520quantized%2520GNNs%2520while%2520introducing%2520a%2520minimal%2520computational%2520overhead.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRAP%3A%20Low-Rank%20Aggregation%20Prompting%20for%20Quantized%20Graph%20Neural%20Networks%20Training&entry.906535625=Chenyu%20Liu%20and%20Haige%20Li%20and%20Luca%20Rossi&entry.1292438233=Graph%20Neural%20Networks%20%28GNNs%29%20are%20neural%20networks%20that%20aim%20to%20process%20graph%20data%2C%20capturing%20the%20relationships%20and%20interactions%20between%20nodes%20using%20the%20message-passing%20mechanism.%20GNN%20quantization%20has%20emerged%20as%20a%20promising%20approach%20for%20reducing%20model%20size%20and%20accelerating%20inference%20in%20resource-constrained%20environments.%20Compared%20to%20quantization%20in%20LLMs%2C%20quantizing%20graph%20features%20is%20more%20emphasized%20in%20GNNs.%20Inspired%20by%20the%20above%2C%20we%20propose%20to%20leverage%20prompt%20learning%2C%20which%20manipulates%20the%20input%20data%2C%20to%20improve%20the%20performance%20of%20quantization-aware%20training%20%28QAT%29%20for%20GNNs.%20To%20mitigate%20the%20issue%20that%20prompting%20the%20node%20features%20alone%20can%20only%20make%20part%20of%20the%20quantized%20aggregation%20result%20optimal%2C%20we%20introduce%20Low-Rank%20Aggregation%20Prompting%20%28LoRAP%29%2C%20which%20injects%20lightweight%2C%20input-dependent%20prompts%20into%20each%20aggregated%20feature%20to%20optimize%20the%20results%20of%20quantized%20aggregations.%20Extensive%20evaluations%20on%204%20leading%20QAT%20frameworks%20over%209%20graph%20datasets%20demonstrate%20that%20LoRAP%20consistently%20enhances%20the%20performance%20of%20low-bit%20quantized%20GNNs%20while%20introducing%20a%20minimal%20computational%20overhead.&entry.1838667208=http%3A//arxiv.org/abs/2601.15079v1&entry.124074799=Read"},
{"title": "OM4OV: Leveraging Ontology Matching for Ontology Versioning", "author": "Zhangcheng Qiang and Kerry Taylor and Weiqing Wang", "abstract": "Due to the dynamic nature of the Semantic Web, version control is necessary to manage changes in widely used ontologies. Despite the long-standing recognition of ontology versioning (OV) as a crucial component of efficient ontology management, many approaches treat OV as similar to ontology matching (OM) and directly reuse OM systems for OV tasks. In this study, we systematically analyse similarities and differences between OM and OV and formalise an OM4OV pipeline to offer more advanced OV support. The pipeline is implemented and evaluated in the state-of-the-art OM system Agent-OM. The experimental results indicate that OM systems can be effectively reused for OV tasks, but without necessary extensions, can produce skewed measurements, poor performance in detecting update entities, and limited explanation of false mappings. To tackle these issues, we propose an optimisation method called the cross-reference (CR) mechanism, which builds on existing OM alignments to reduce the number of matching candidates and to improve overall OV performance.", "link": "http://arxiv.org/abs/2409.20302v9", "date": "2026-01-21", "relevancy": 2.3036, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4647}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4647}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OM4OV%3A%20Leveraging%20Ontology%20Matching%20for%20Ontology%20Versioning&body=Title%3A%20OM4OV%3A%20Leveraging%20Ontology%20Matching%20for%20Ontology%20Versioning%0AAuthor%3A%20Zhangcheng%20Qiang%20and%20Kerry%20Taylor%20and%20Weiqing%20Wang%0AAbstract%3A%20Due%20to%20the%20dynamic%20nature%20of%20the%20Semantic%20Web%2C%20version%20control%20is%20necessary%20to%20manage%20changes%20in%20widely%20used%20ontologies.%20Despite%20the%20long-standing%20recognition%20of%20ontology%20versioning%20%28OV%29%20as%20a%20crucial%20component%20of%20efficient%20ontology%20management%2C%20many%20approaches%20treat%20OV%20as%20similar%20to%20ontology%20matching%20%28OM%29%20and%20directly%20reuse%20OM%20systems%20for%20OV%20tasks.%20In%20this%20study%2C%20we%20systematically%20analyse%20similarities%20and%20differences%20between%20OM%20and%20OV%20and%20formalise%20an%20OM4OV%20pipeline%20to%20offer%20more%20advanced%20OV%20support.%20The%20pipeline%20is%20implemented%20and%20evaluated%20in%20the%20state-of-the-art%20OM%20system%20Agent-OM.%20The%20experimental%20results%20indicate%20that%20OM%20systems%20can%20be%20effectively%20reused%20for%20OV%20tasks%2C%20but%20without%20necessary%20extensions%2C%20can%20produce%20skewed%20measurements%2C%20poor%20performance%20in%20detecting%20update%20entities%2C%20and%20limited%20explanation%20of%20false%20mappings.%20To%20tackle%20these%20issues%2C%20we%20propose%20an%20optimisation%20method%20called%20the%20cross-reference%20%28CR%29%20mechanism%2C%20which%20builds%20on%20existing%20OM%20alignments%20to%20reduce%20the%20number%20of%20matching%20candidates%20and%20to%20improve%20overall%20OV%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2409.20302v9%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOM4OV%253A%2520Leveraging%2520Ontology%2520Matching%2520for%2520Ontology%2520Versioning%26entry.906535625%3DZhangcheng%2520Qiang%2520and%2520Kerry%2520Taylor%2520and%2520Weiqing%2520Wang%26entry.1292438233%3DDue%2520to%2520the%2520dynamic%2520nature%2520of%2520the%2520Semantic%2520Web%252C%2520version%2520control%2520is%2520necessary%2520to%2520manage%2520changes%2520in%2520widely%2520used%2520ontologies.%2520Despite%2520the%2520long-standing%2520recognition%2520of%2520ontology%2520versioning%2520%2528OV%2529%2520as%2520a%2520crucial%2520component%2520of%2520efficient%2520ontology%2520management%252C%2520many%2520approaches%2520treat%2520OV%2520as%2520similar%2520to%2520ontology%2520matching%2520%2528OM%2529%2520and%2520directly%2520reuse%2520OM%2520systems%2520for%2520OV%2520tasks.%2520In%2520this%2520study%252C%2520we%2520systematically%2520analyse%2520similarities%2520and%2520differences%2520between%2520OM%2520and%2520OV%2520and%2520formalise%2520an%2520OM4OV%2520pipeline%2520to%2520offer%2520more%2520advanced%2520OV%2520support.%2520The%2520pipeline%2520is%2520implemented%2520and%2520evaluated%2520in%2520the%2520state-of-the-art%2520OM%2520system%2520Agent-OM.%2520The%2520experimental%2520results%2520indicate%2520that%2520OM%2520systems%2520can%2520be%2520effectively%2520reused%2520for%2520OV%2520tasks%252C%2520but%2520without%2520necessary%2520extensions%252C%2520can%2520produce%2520skewed%2520measurements%252C%2520poor%2520performance%2520in%2520detecting%2520update%2520entities%252C%2520and%2520limited%2520explanation%2520of%2520false%2520mappings.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520propose%2520an%2520optimisation%2520method%2520called%2520the%2520cross-reference%2520%2528CR%2529%2520mechanism%252C%2520which%2520builds%2520on%2520existing%2520OM%2520alignments%2520to%2520reduce%2520the%2520number%2520of%2520matching%2520candidates%2520and%2520to%2520improve%2520overall%2520OV%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.20302v9%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OM4OV%3A%20Leveraging%20Ontology%20Matching%20for%20Ontology%20Versioning&entry.906535625=Zhangcheng%20Qiang%20and%20Kerry%20Taylor%20and%20Weiqing%20Wang&entry.1292438233=Due%20to%20the%20dynamic%20nature%20of%20the%20Semantic%20Web%2C%20version%20control%20is%20necessary%20to%20manage%20changes%20in%20widely%20used%20ontologies.%20Despite%20the%20long-standing%20recognition%20of%20ontology%20versioning%20%28OV%29%20as%20a%20crucial%20component%20of%20efficient%20ontology%20management%2C%20many%20approaches%20treat%20OV%20as%20similar%20to%20ontology%20matching%20%28OM%29%20and%20directly%20reuse%20OM%20systems%20for%20OV%20tasks.%20In%20this%20study%2C%20we%20systematically%20analyse%20similarities%20and%20differences%20between%20OM%20and%20OV%20and%20formalise%20an%20OM4OV%20pipeline%20to%20offer%20more%20advanced%20OV%20support.%20The%20pipeline%20is%20implemented%20and%20evaluated%20in%20the%20state-of-the-art%20OM%20system%20Agent-OM.%20The%20experimental%20results%20indicate%20that%20OM%20systems%20can%20be%20effectively%20reused%20for%20OV%20tasks%2C%20but%20without%20necessary%20extensions%2C%20can%20produce%20skewed%20measurements%2C%20poor%20performance%20in%20detecting%20update%20entities%2C%20and%20limited%20explanation%20of%20false%20mappings.%20To%20tackle%20these%20issues%2C%20we%20propose%20an%20optimisation%20method%20called%20the%20cross-reference%20%28CR%29%20mechanism%2C%20which%20builds%20on%20existing%20OM%20alignments%20to%20reduce%20the%20number%20of%20matching%20candidates%20and%20to%20improve%20overall%20OV%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2409.20302v9&entry.124074799=Read"},
{"title": "Generative Artificial Intelligence, Musical Heritage and the Construction of Peace Narratives: A Case Study in Mali", "author": "Nouhoum Coulibaly and Ousmane Ly and Michael Leventhal and Ousmane Goro", "abstract": "This study explores the capacity of generative artificial intelligence (Gen AI) to contribute to the construction of peace narratives and the revitalization of musical heritage in Mali. The study has been made in a political and social context where inter-community tensions and social fractures motivate a search for new symbolic frameworks for reconciliation. The study empirically explores three questions: (1) how Gen AI can be used as a tool for musical creation rooted in national languages and traditions; (2) to what extent Gen AI systems enable a balanced hybridization between technological innovation and cultural authenticity; and (3) how AI-assisted musical co-creation can strengthen social cohesion and cultural sovereignty. The experimental results suggest that Gen AI, embedded in a culturally conscious participatory framework, can act as a catalyst for symbolic diplomacy, amplifying local voices instead of standardizing them. However, challenges persist regarding the availability of linguistic corpora, algorithmic censorship, and the ethics of generating compositions derived from copyrighted sources.", "link": "http://arxiv.org/abs/2601.14931v1", "date": "2026-01-21", "relevancy": 2.2956, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4941}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4658}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Artificial%20Intelligence%2C%20Musical%20Heritage%20and%20the%20Construction%20of%20Peace%20Narratives%3A%20A%20Case%20Study%20in%20Mali&body=Title%3A%20Generative%20Artificial%20Intelligence%2C%20Musical%20Heritage%20and%20the%20Construction%20of%20Peace%20Narratives%3A%20A%20Case%20Study%20in%20Mali%0AAuthor%3A%20Nouhoum%20Coulibaly%20and%20Ousmane%20Ly%20and%20Michael%20Leventhal%20and%20Ousmane%20Goro%0AAbstract%3A%20This%20study%20explores%20the%20capacity%20of%20generative%20artificial%20intelligence%20%28Gen%20AI%29%20to%20contribute%20to%20the%20construction%20of%20peace%20narratives%20and%20the%20revitalization%20of%20musical%20heritage%20in%20Mali.%20The%20study%20has%20been%20made%20in%20a%20political%20and%20social%20context%20where%20inter-community%20tensions%20and%20social%20fractures%20motivate%20a%20search%20for%20new%20symbolic%20frameworks%20for%20reconciliation.%20The%20study%20empirically%20explores%20three%20questions%3A%20%281%29%20how%20Gen%20AI%20can%20be%20used%20as%20a%20tool%20for%20musical%20creation%20rooted%20in%20national%20languages%20and%20traditions%3B%20%282%29%20to%20what%20extent%20Gen%20AI%20systems%20enable%20a%20balanced%20hybridization%20between%20technological%20innovation%20and%20cultural%20authenticity%3B%20and%20%283%29%20how%20AI-assisted%20musical%20co-creation%20can%20strengthen%20social%20cohesion%20and%20cultural%20sovereignty.%20The%20experimental%20results%20suggest%20that%20Gen%20AI%2C%20embedded%20in%20a%20culturally%20conscious%20participatory%20framework%2C%20can%20act%20as%20a%20catalyst%20for%20symbolic%20diplomacy%2C%20amplifying%20local%20voices%20instead%20of%20standardizing%20them.%20However%2C%20challenges%20persist%20regarding%20the%20availability%20of%20linguistic%20corpora%2C%20algorithmic%20censorship%2C%20and%20the%20ethics%20of%20generating%20compositions%20derived%20from%20copyrighted%20sources.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14931v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Artificial%2520Intelligence%252C%2520Musical%2520Heritage%2520and%2520the%2520Construction%2520of%2520Peace%2520Narratives%253A%2520A%2520Case%2520Study%2520in%2520Mali%26entry.906535625%3DNouhoum%2520Coulibaly%2520and%2520Ousmane%2520Ly%2520and%2520Michael%2520Leventhal%2520and%2520Ousmane%2520Goro%26entry.1292438233%3DThis%2520study%2520explores%2520the%2520capacity%2520of%2520generative%2520artificial%2520intelligence%2520%2528Gen%2520AI%2529%2520to%2520contribute%2520to%2520the%2520construction%2520of%2520peace%2520narratives%2520and%2520the%2520revitalization%2520of%2520musical%2520heritage%2520in%2520Mali.%2520The%2520study%2520has%2520been%2520made%2520in%2520a%2520political%2520and%2520social%2520context%2520where%2520inter-community%2520tensions%2520and%2520social%2520fractures%2520motivate%2520a%2520search%2520for%2520new%2520symbolic%2520frameworks%2520for%2520reconciliation.%2520The%2520study%2520empirically%2520explores%2520three%2520questions%253A%2520%25281%2529%2520how%2520Gen%2520AI%2520can%2520be%2520used%2520as%2520a%2520tool%2520for%2520musical%2520creation%2520rooted%2520in%2520national%2520languages%2520and%2520traditions%253B%2520%25282%2529%2520to%2520what%2520extent%2520Gen%2520AI%2520systems%2520enable%2520a%2520balanced%2520hybridization%2520between%2520technological%2520innovation%2520and%2520cultural%2520authenticity%253B%2520and%2520%25283%2529%2520how%2520AI-assisted%2520musical%2520co-creation%2520can%2520strengthen%2520social%2520cohesion%2520and%2520cultural%2520sovereignty.%2520The%2520experimental%2520results%2520suggest%2520that%2520Gen%2520AI%252C%2520embedded%2520in%2520a%2520culturally%2520conscious%2520participatory%2520framework%252C%2520can%2520act%2520as%2520a%2520catalyst%2520for%2520symbolic%2520diplomacy%252C%2520amplifying%2520local%2520voices%2520instead%2520of%2520standardizing%2520them.%2520However%252C%2520challenges%2520persist%2520regarding%2520the%2520availability%2520of%2520linguistic%2520corpora%252C%2520algorithmic%2520censorship%252C%2520and%2520the%2520ethics%2520of%2520generating%2520compositions%2520derived%2520from%2520copyrighted%2520sources.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14931v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Artificial%20Intelligence%2C%20Musical%20Heritage%20and%20the%20Construction%20of%20Peace%20Narratives%3A%20A%20Case%20Study%20in%20Mali&entry.906535625=Nouhoum%20Coulibaly%20and%20Ousmane%20Ly%20and%20Michael%20Leventhal%20and%20Ousmane%20Goro&entry.1292438233=This%20study%20explores%20the%20capacity%20of%20generative%20artificial%20intelligence%20%28Gen%20AI%29%20to%20contribute%20to%20the%20construction%20of%20peace%20narratives%20and%20the%20revitalization%20of%20musical%20heritage%20in%20Mali.%20The%20study%20has%20been%20made%20in%20a%20political%20and%20social%20context%20where%20inter-community%20tensions%20and%20social%20fractures%20motivate%20a%20search%20for%20new%20symbolic%20frameworks%20for%20reconciliation.%20The%20study%20empirically%20explores%20three%20questions%3A%20%281%29%20how%20Gen%20AI%20can%20be%20used%20as%20a%20tool%20for%20musical%20creation%20rooted%20in%20national%20languages%20and%20traditions%3B%20%282%29%20to%20what%20extent%20Gen%20AI%20systems%20enable%20a%20balanced%20hybridization%20between%20technological%20innovation%20and%20cultural%20authenticity%3B%20and%20%283%29%20how%20AI-assisted%20musical%20co-creation%20can%20strengthen%20social%20cohesion%20and%20cultural%20sovereignty.%20The%20experimental%20results%20suggest%20that%20Gen%20AI%2C%20embedded%20in%20a%20culturally%20conscious%20participatory%20framework%2C%20can%20act%20as%20a%20catalyst%20for%20symbolic%20diplomacy%2C%20amplifying%20local%20voices%20instead%20of%20standardizing%20them.%20However%2C%20challenges%20persist%20regarding%20the%20availability%20of%20linguistic%20corpora%2C%20algorithmic%20censorship%2C%20and%20the%20ethics%20of%20generating%20compositions%20derived%20from%20copyrighted%20sources.&entry.1838667208=http%3A//arxiv.org/abs/2601.14931v1&entry.124074799=Read"},
{"title": "Reward Shaping to Mitigate Reward Hacking in RLHF", "author": "Jiayi Fu and Xuandong Zhao and Chengyuan Yao and Heng Wang and Qi Han and Yanghua Xiao", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to \\emph{reward hacking}, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. Although reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests two key design principles: (1) the RL reward should be bounded, and (2) the RL reward benefits from rapid initial growth followed by gradual convergence. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model as the signal for reinforcement learning. Moreover, PAR exhibits two critical variance-reduction properties that contribute to stabilizing the RLHF training process and effectively extending the tolerance window for early stopping. We evaluated PAR on the base model Gemma2-2B using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR's superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. The code is available at https://github.com/PorUna-byte/PAR.", "link": "http://arxiv.org/abs/2502.18770v5", "date": "2026-01-21", "relevancy": 2.2876, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4637}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4563}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reward%20Shaping%20to%20Mitigate%20Reward%20Hacking%20in%20RLHF&body=Title%3A%20Reward%20Shaping%20to%20Mitigate%20Reward%20Hacking%20in%20RLHF%0AAuthor%3A%20Jiayi%20Fu%20and%20Xuandong%20Zhao%20and%20Chengyuan%20Yao%20and%20Heng%20Wang%20and%20Qi%20Han%20and%20Yanghua%20Xiao%0AAbstract%3A%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20is%20essential%20for%20aligning%20large%20language%20models%20%28LLMs%29%20with%20human%20values.%20However%2C%20RLHF%20is%20susceptible%20to%20%5Cemph%7Breward%20hacking%7D%2C%20where%20the%20agent%20exploits%20flaws%20in%20the%20reward%20function%20rather%20than%20learning%20the%20intended%20behavior%2C%20thus%20degrading%20alignment.%20Although%20reward%20shaping%20helps%20stabilize%20RLHF%20and%20partially%20mitigate%20reward%20hacking%2C%20a%20systematic%20investigation%20into%20shaping%20techniques%20and%20their%20underlying%20principles%20remains%20lacking.%20To%20bridge%20this%20gap%2C%20we%20present%20a%20comprehensive%20study%20of%20the%20prevalent%20reward%20shaping%20methods.%20Our%20analysis%20suggests%20two%20key%20design%20principles%3A%20%281%29%20the%20RL%20reward%20should%20be%20bounded%2C%20and%20%282%29%20the%20RL%20reward%20benefits%20from%20rapid%20initial%20growth%20followed%20by%20gradual%20convergence.%20Guided%20by%20these%20insights%2C%20we%20propose%20Preference%20As%20Reward%20%28PAR%29%2C%20a%20novel%20approach%20that%20leverages%20the%20latent%20preferences%20embedded%20within%20the%20reward%20model%20as%20the%20signal%20for%20reinforcement%20learning.%20Moreover%2C%20PAR%20exhibits%20two%20critical%20variance-reduction%20properties%20that%20contribute%20to%20stabilizing%20the%20RLHF%20training%20process%20and%20effectively%20extending%20the%20tolerance%20window%20for%20early%20stopping.%20We%20evaluated%20PAR%20on%20the%20base%20model%20Gemma2-2B%20using%20two%20datasets%2C%20Ultrafeedback-Binarized%20and%20HH-RLHF.%20Experimental%20results%20demonstrate%20PAR%27s%20superior%20performance%20over%20other%20reward%20shaping%20methods.%20On%20the%20AlpacaEval%202.0%20benchmark%2C%20PAR%20achieves%20a%20win%20rate%20of%20at%20least%205%20percentage%20points%20higher%20than%20competing%20approaches.%20Furthermore%2C%20PAR%20exhibits%20remarkable%20data%20efficiency%2C%20requiring%20only%20a%20single%20reference%20reward%20for%20optimal%20performance%2C%20and%20maintains%20robustness%20against%20reward%20hacking%20even%20after%20two%20full%20epochs%20of%20training.%20The%20code%20is%20available%20at%20https%3A//github.com/PorUna-byte/PAR.%0ALink%3A%20http%3A//arxiv.org/abs/2502.18770v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReward%2520Shaping%2520to%2520Mitigate%2520Reward%2520Hacking%2520in%2520RLHF%26entry.906535625%3DJiayi%2520Fu%2520and%2520Xuandong%2520Zhao%2520and%2520Chengyuan%2520Yao%2520and%2520Heng%2520Wang%2520and%2520Qi%2520Han%2520and%2520Yanghua%2520Xiao%26entry.1292438233%3DReinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520is%2520essential%2520for%2520aligning%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520human%2520values.%2520However%252C%2520RLHF%2520is%2520susceptible%2520to%2520%255Cemph%257Breward%2520hacking%257D%252C%2520where%2520the%2520agent%2520exploits%2520flaws%2520in%2520the%2520reward%2520function%2520rather%2520than%2520learning%2520the%2520intended%2520behavior%252C%2520thus%2520degrading%2520alignment.%2520Although%2520reward%2520shaping%2520helps%2520stabilize%2520RLHF%2520and%2520partially%2520mitigate%2520reward%2520hacking%252C%2520a%2520systematic%2520investigation%2520into%2520shaping%2520techniques%2520and%2520their%2520underlying%2520principles%2520remains%2520lacking.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%2520a%2520comprehensive%2520study%2520of%2520the%2520prevalent%2520reward%2520shaping%2520methods.%2520Our%2520analysis%2520suggests%2520two%2520key%2520design%2520principles%253A%2520%25281%2529%2520the%2520RL%2520reward%2520should%2520be%2520bounded%252C%2520and%2520%25282%2529%2520the%2520RL%2520reward%2520benefits%2520from%2520rapid%2520initial%2520growth%2520followed%2520by%2520gradual%2520convergence.%2520Guided%2520by%2520these%2520insights%252C%2520we%2520propose%2520Preference%2520As%2520Reward%2520%2528PAR%2529%252C%2520a%2520novel%2520approach%2520that%2520leverages%2520the%2520latent%2520preferences%2520embedded%2520within%2520the%2520reward%2520model%2520as%2520the%2520signal%2520for%2520reinforcement%2520learning.%2520Moreover%252C%2520PAR%2520exhibits%2520two%2520critical%2520variance-reduction%2520properties%2520that%2520contribute%2520to%2520stabilizing%2520the%2520RLHF%2520training%2520process%2520and%2520effectively%2520extending%2520the%2520tolerance%2520window%2520for%2520early%2520stopping.%2520We%2520evaluated%2520PAR%2520on%2520the%2520base%2520model%2520Gemma2-2B%2520using%2520two%2520datasets%252C%2520Ultrafeedback-Binarized%2520and%2520HH-RLHF.%2520Experimental%2520results%2520demonstrate%2520PAR%2527s%2520superior%2520performance%2520over%2520other%2520reward%2520shaping%2520methods.%2520On%2520the%2520AlpacaEval%25202.0%2520benchmark%252C%2520PAR%2520achieves%2520a%2520win%2520rate%2520of%2520at%2520least%25205%2520percentage%2520points%2520higher%2520than%2520competing%2520approaches.%2520Furthermore%252C%2520PAR%2520exhibits%2520remarkable%2520data%2520efficiency%252C%2520requiring%2520only%2520a%2520single%2520reference%2520reward%2520for%2520optimal%2520performance%252C%2520and%2520maintains%2520robustness%2520against%2520reward%2520hacking%2520even%2520after%2520two%2520full%2520epochs%2520of%2520training.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/PorUna-byte/PAR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18770v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reward%20Shaping%20to%20Mitigate%20Reward%20Hacking%20in%20RLHF&entry.906535625=Jiayi%20Fu%20and%20Xuandong%20Zhao%20and%20Chengyuan%20Yao%20and%20Heng%20Wang%20and%20Qi%20Han%20and%20Yanghua%20Xiao&entry.1292438233=Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20is%20essential%20for%20aligning%20large%20language%20models%20%28LLMs%29%20with%20human%20values.%20However%2C%20RLHF%20is%20susceptible%20to%20%5Cemph%7Breward%20hacking%7D%2C%20where%20the%20agent%20exploits%20flaws%20in%20the%20reward%20function%20rather%20than%20learning%20the%20intended%20behavior%2C%20thus%20degrading%20alignment.%20Although%20reward%20shaping%20helps%20stabilize%20RLHF%20and%20partially%20mitigate%20reward%20hacking%2C%20a%20systematic%20investigation%20into%20shaping%20techniques%20and%20their%20underlying%20principles%20remains%20lacking.%20To%20bridge%20this%20gap%2C%20we%20present%20a%20comprehensive%20study%20of%20the%20prevalent%20reward%20shaping%20methods.%20Our%20analysis%20suggests%20two%20key%20design%20principles%3A%20%281%29%20the%20RL%20reward%20should%20be%20bounded%2C%20and%20%282%29%20the%20RL%20reward%20benefits%20from%20rapid%20initial%20growth%20followed%20by%20gradual%20convergence.%20Guided%20by%20these%20insights%2C%20we%20propose%20Preference%20As%20Reward%20%28PAR%29%2C%20a%20novel%20approach%20that%20leverages%20the%20latent%20preferences%20embedded%20within%20the%20reward%20model%20as%20the%20signal%20for%20reinforcement%20learning.%20Moreover%2C%20PAR%20exhibits%20two%20critical%20variance-reduction%20properties%20that%20contribute%20to%20stabilizing%20the%20RLHF%20training%20process%20and%20effectively%20extending%20the%20tolerance%20window%20for%20early%20stopping.%20We%20evaluated%20PAR%20on%20the%20base%20model%20Gemma2-2B%20using%20two%20datasets%2C%20Ultrafeedback-Binarized%20and%20HH-RLHF.%20Experimental%20results%20demonstrate%20PAR%27s%20superior%20performance%20over%20other%20reward%20shaping%20methods.%20On%20the%20AlpacaEval%202.0%20benchmark%2C%20PAR%20achieves%20a%20win%20rate%20of%20at%20least%205%20percentage%20points%20higher%20than%20competing%20approaches.%20Furthermore%2C%20PAR%20exhibits%20remarkable%20data%20efficiency%2C%20requiring%20only%20a%20single%20reference%20reward%20for%20optimal%20performance%2C%20and%20maintains%20robustness%20against%20reward%20hacking%20even%20after%20two%20full%20epochs%20of%20training.%20The%20code%20is%20available%20at%20https%3A//github.com/PorUna-byte/PAR.&entry.1838667208=http%3A//arxiv.org/abs/2502.18770v5&entry.124074799=Read"},
{"title": "BBoxMaskPose v2: Expanding Mutual Conditioning to 3D", "author": "Miroslav Purkrabek and Constantin Kolomiiets and Jiri Matas", "abstract": "Most 2D human pose estimation benchmarks are nearly saturated, with the exception of crowded scenes. We introduce PMPose, a top-down 2D pose estimator that incorporates the probabilistic formulation and the mask-conditioning. PMPose improves crowded pose estimation without sacrificing performance on standard scenes. Building on this, we present BBoxMaskPose v2 (BMPv2) integrating PMPose and an enhanced SAM-based mask refinement module. BMPv2 surpasses state-of-the-art by 1.5 average precision (AP) points on COCO and 6 AP points on OCHuman, becoming the first method to exceed 50 AP on OCHuman. We demonstrate that BMP's 2D prompting of 3D model improves 3D pose estimation in crowded scenes and that advances in 2D pose quality directly benefit 3D estimation. Results on the new OCHuman-Pose dataset show that multi-person performance is more affected by pose prediction accuracy than by detection. The code, models, and data are available on https://MiraPurkrabek.github.io/BBox-Mask-Pose/.", "link": "http://arxiv.org/abs/2601.15200v1", "date": "2026-01-21", "relevancy": 2.2831, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5766}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5686}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BBoxMaskPose%20v2%3A%20Expanding%20Mutual%20Conditioning%20to%203D&body=Title%3A%20BBoxMaskPose%20v2%3A%20Expanding%20Mutual%20Conditioning%20to%203D%0AAuthor%3A%20Miroslav%20Purkrabek%20and%20Constantin%20Kolomiiets%20and%20Jiri%20Matas%0AAbstract%3A%20Most%202D%20human%20pose%20estimation%20benchmarks%20are%20nearly%20saturated%2C%20with%20the%20exception%20of%20crowded%20scenes.%20We%20introduce%20PMPose%2C%20a%20top-down%202D%20pose%20estimator%20that%20incorporates%20the%20probabilistic%20formulation%20and%20the%20mask-conditioning.%20PMPose%20improves%20crowded%20pose%20estimation%20without%20sacrificing%20performance%20on%20standard%20scenes.%20Building%20on%20this%2C%20we%20present%20BBoxMaskPose%20v2%20%28BMPv2%29%20integrating%20PMPose%20and%20an%20enhanced%20SAM-based%20mask%20refinement%20module.%20BMPv2%20surpasses%20state-of-the-art%20by%201.5%20average%20precision%20%28AP%29%20points%20on%20COCO%20and%206%20AP%20points%20on%20OCHuman%2C%20becoming%20the%20first%20method%20to%20exceed%2050%20AP%20on%20OCHuman.%20We%20demonstrate%20that%20BMP%27s%202D%20prompting%20of%203D%20model%20improves%203D%20pose%20estimation%20in%20crowded%20scenes%20and%20that%20advances%20in%202D%20pose%20quality%20directly%20benefit%203D%20estimation.%20Results%20on%20the%20new%20OCHuman-Pose%20dataset%20show%20that%20multi-person%20performance%20is%20more%20affected%20by%20pose%20prediction%20accuracy%20than%20by%20detection.%20The%20code%2C%20models%2C%20and%20data%20are%20available%20on%20https%3A//MiraPurkrabek.github.io/BBox-Mask-Pose/.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBBoxMaskPose%2520v2%253A%2520Expanding%2520Mutual%2520Conditioning%2520to%25203D%26entry.906535625%3DMiroslav%2520Purkrabek%2520and%2520Constantin%2520Kolomiiets%2520and%2520Jiri%2520Matas%26entry.1292438233%3DMost%25202D%2520human%2520pose%2520estimation%2520benchmarks%2520are%2520nearly%2520saturated%252C%2520with%2520the%2520exception%2520of%2520crowded%2520scenes.%2520We%2520introduce%2520PMPose%252C%2520a%2520top-down%25202D%2520pose%2520estimator%2520that%2520incorporates%2520the%2520probabilistic%2520formulation%2520and%2520the%2520mask-conditioning.%2520PMPose%2520improves%2520crowded%2520pose%2520estimation%2520without%2520sacrificing%2520performance%2520on%2520standard%2520scenes.%2520Building%2520on%2520this%252C%2520we%2520present%2520BBoxMaskPose%2520v2%2520%2528BMPv2%2529%2520integrating%2520PMPose%2520and%2520an%2520enhanced%2520SAM-based%2520mask%2520refinement%2520module.%2520BMPv2%2520surpasses%2520state-of-the-art%2520by%25201.5%2520average%2520precision%2520%2528AP%2529%2520points%2520on%2520COCO%2520and%25206%2520AP%2520points%2520on%2520OCHuman%252C%2520becoming%2520the%2520first%2520method%2520to%2520exceed%252050%2520AP%2520on%2520OCHuman.%2520We%2520demonstrate%2520that%2520BMP%2527s%25202D%2520prompting%2520of%25203D%2520model%2520improves%25203D%2520pose%2520estimation%2520in%2520crowded%2520scenes%2520and%2520that%2520advances%2520in%25202D%2520pose%2520quality%2520directly%2520benefit%25203D%2520estimation.%2520Results%2520on%2520the%2520new%2520OCHuman-Pose%2520dataset%2520show%2520that%2520multi-person%2520performance%2520is%2520more%2520affected%2520by%2520pose%2520prediction%2520accuracy%2520than%2520by%2520detection.%2520The%2520code%252C%2520models%252C%2520and%2520data%2520are%2520available%2520on%2520https%253A//MiraPurkrabek.github.io/BBox-Mask-Pose/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BBoxMaskPose%20v2%3A%20Expanding%20Mutual%20Conditioning%20to%203D&entry.906535625=Miroslav%20Purkrabek%20and%20Constantin%20Kolomiiets%20and%20Jiri%20Matas&entry.1292438233=Most%202D%20human%20pose%20estimation%20benchmarks%20are%20nearly%20saturated%2C%20with%20the%20exception%20of%20crowded%20scenes.%20We%20introduce%20PMPose%2C%20a%20top-down%202D%20pose%20estimator%20that%20incorporates%20the%20probabilistic%20formulation%20and%20the%20mask-conditioning.%20PMPose%20improves%20crowded%20pose%20estimation%20without%20sacrificing%20performance%20on%20standard%20scenes.%20Building%20on%20this%2C%20we%20present%20BBoxMaskPose%20v2%20%28BMPv2%29%20integrating%20PMPose%20and%20an%20enhanced%20SAM-based%20mask%20refinement%20module.%20BMPv2%20surpasses%20state-of-the-art%20by%201.5%20average%20precision%20%28AP%29%20points%20on%20COCO%20and%206%20AP%20points%20on%20OCHuman%2C%20becoming%20the%20first%20method%20to%20exceed%2050%20AP%20on%20OCHuman.%20We%20demonstrate%20that%20BMP%27s%202D%20prompting%20of%203D%20model%20improves%203D%20pose%20estimation%20in%20crowded%20scenes%20and%20that%20advances%20in%202D%20pose%20quality%20directly%20benefit%203D%20estimation.%20Results%20on%20the%20new%20OCHuman-Pose%20dataset%20show%20that%20multi-person%20performance%20is%20more%20affected%20by%20pose%20prediction%20accuracy%20than%20by%20detection.%20The%20code%2C%20models%2C%20and%20data%20are%20available%20on%20https%3A//MiraPurkrabek.github.io/BBox-Mask-Pose/.&entry.1838667208=http%3A//arxiv.org/abs/2601.15200v1&entry.124074799=Read"},
{"title": "On the Reliability and Stability of Selective Methods in Malware Classification Tasks", "author": "Alexander Herzog and Aliai Eusebi and Lorenzo Cavallaro", "abstract": "The performance figures of modern drift-adaptive malware classifiers appear promising, but does this translate to genuine operational reliability? The standard evaluation paradigm primarily focuses on baseline performance metrics, neglecting confidence-error alignment and operational stability. While prior works established the importance of temporal evaluation and introduced selective classification in malware classification tasks, we take a complementary direction by investigating whether malware classifiers maintain reliable and stable confidence estimates under distribution shifts and exploring the tensions between scientific advancement and practical impacts when they do not. We propose Aurora, a framework to evaluate malware classifiers based on their confidence quality and operational resilience. Aurora subjects the confidence profile of a given model to verification to assess the reliability of its estimates. Unreliable confidence estimates erode operational trust, waste valuable annotation budgets on non-informative samples for active learning, and leave error-prone instances undetected in selective classification. Aurora is further complemented by a set of metrics designed to go beyond point-in-time performance, striving towards a more holistic assessment of operational stability throughout temporal evaluation periods. The fragility we observe in SOTA frameworks across datasets of varying drift severity suggests it may be time to revisit the underlying assumptions.", "link": "http://arxiv.org/abs/2505.22843v3", "date": "2026-01-21", "relevancy": 2.2799, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4696}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4502}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Reliability%20and%20Stability%20of%20Selective%20Methods%20in%20Malware%20Classification%20Tasks&body=Title%3A%20On%20the%20Reliability%20and%20Stability%20of%20Selective%20Methods%20in%20Malware%20Classification%20Tasks%0AAuthor%3A%20Alexander%20Herzog%20and%20Aliai%20Eusebi%20and%20Lorenzo%20Cavallaro%0AAbstract%3A%20The%20performance%20figures%20of%20modern%20drift-adaptive%20malware%20classifiers%20appear%20promising%2C%20but%20does%20this%20translate%20to%20genuine%20operational%20reliability%3F%20The%20standard%20evaluation%20paradigm%20primarily%20focuses%20on%20baseline%20performance%20metrics%2C%20neglecting%20confidence-error%20alignment%20and%20operational%20stability.%20While%20prior%20works%20established%20the%20importance%20of%20temporal%20evaluation%20and%20introduced%20selective%20classification%20in%20malware%20classification%20tasks%2C%20we%20take%20a%20complementary%20direction%20by%20investigating%20whether%20malware%20classifiers%20maintain%20reliable%20and%20stable%20confidence%20estimates%20under%20distribution%20shifts%20and%20exploring%20the%20tensions%20between%20scientific%20advancement%20and%20practical%20impacts%20when%20they%20do%20not.%20We%20propose%20Aurora%2C%20a%20framework%20to%20evaluate%20malware%20classifiers%20based%20on%20their%20confidence%20quality%20and%20operational%20resilience.%20Aurora%20subjects%20the%20confidence%20profile%20of%20a%20given%20model%20to%20verification%20to%20assess%20the%20reliability%20of%20its%20estimates.%20Unreliable%20confidence%20estimates%20erode%20operational%20trust%2C%20waste%20valuable%20annotation%20budgets%20on%20non-informative%20samples%20for%20active%20learning%2C%20and%20leave%20error-prone%20instances%20undetected%20in%20selective%20classification.%20Aurora%20is%20further%20complemented%20by%20a%20set%20of%20metrics%20designed%20to%20go%20beyond%20point-in-time%20performance%2C%20striving%20towards%20a%20more%20holistic%20assessment%20of%20operational%20stability%20throughout%20temporal%20evaluation%20periods.%20The%20fragility%20we%20observe%20in%20SOTA%20frameworks%20across%20datasets%20of%20varying%20drift%20severity%20suggests%20it%20may%20be%20time%20to%20revisit%20the%20underlying%20assumptions.%0ALink%3A%20http%3A//arxiv.org/abs/2505.22843v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Reliability%2520and%2520Stability%2520of%2520Selective%2520Methods%2520in%2520Malware%2520Classification%2520Tasks%26entry.906535625%3DAlexander%2520Herzog%2520and%2520Aliai%2520Eusebi%2520and%2520Lorenzo%2520Cavallaro%26entry.1292438233%3DThe%2520performance%2520figures%2520of%2520modern%2520drift-adaptive%2520malware%2520classifiers%2520appear%2520promising%252C%2520but%2520does%2520this%2520translate%2520to%2520genuine%2520operational%2520reliability%253F%2520The%2520standard%2520evaluation%2520paradigm%2520primarily%2520focuses%2520on%2520baseline%2520performance%2520metrics%252C%2520neglecting%2520confidence-error%2520alignment%2520and%2520operational%2520stability.%2520While%2520prior%2520works%2520established%2520the%2520importance%2520of%2520temporal%2520evaluation%2520and%2520introduced%2520selective%2520classification%2520in%2520malware%2520classification%2520tasks%252C%2520we%2520take%2520a%2520complementary%2520direction%2520by%2520investigating%2520whether%2520malware%2520classifiers%2520maintain%2520reliable%2520and%2520stable%2520confidence%2520estimates%2520under%2520distribution%2520shifts%2520and%2520exploring%2520the%2520tensions%2520between%2520scientific%2520advancement%2520and%2520practical%2520impacts%2520when%2520they%2520do%2520not.%2520We%2520propose%2520Aurora%252C%2520a%2520framework%2520to%2520evaluate%2520malware%2520classifiers%2520based%2520on%2520their%2520confidence%2520quality%2520and%2520operational%2520resilience.%2520Aurora%2520subjects%2520the%2520confidence%2520profile%2520of%2520a%2520given%2520model%2520to%2520verification%2520to%2520assess%2520the%2520reliability%2520of%2520its%2520estimates.%2520Unreliable%2520confidence%2520estimates%2520erode%2520operational%2520trust%252C%2520waste%2520valuable%2520annotation%2520budgets%2520on%2520non-informative%2520samples%2520for%2520active%2520learning%252C%2520and%2520leave%2520error-prone%2520instances%2520undetected%2520in%2520selective%2520classification.%2520Aurora%2520is%2520further%2520complemented%2520by%2520a%2520set%2520of%2520metrics%2520designed%2520to%2520go%2520beyond%2520point-in-time%2520performance%252C%2520striving%2520towards%2520a%2520more%2520holistic%2520assessment%2520of%2520operational%2520stability%2520throughout%2520temporal%2520evaluation%2520periods.%2520The%2520fragility%2520we%2520observe%2520in%2520SOTA%2520frameworks%2520across%2520datasets%2520of%2520varying%2520drift%2520severity%2520suggests%2520it%2520may%2520be%2520time%2520to%2520revisit%2520the%2520underlying%2520assumptions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22843v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Reliability%20and%20Stability%20of%20Selective%20Methods%20in%20Malware%20Classification%20Tasks&entry.906535625=Alexander%20Herzog%20and%20Aliai%20Eusebi%20and%20Lorenzo%20Cavallaro&entry.1292438233=The%20performance%20figures%20of%20modern%20drift-adaptive%20malware%20classifiers%20appear%20promising%2C%20but%20does%20this%20translate%20to%20genuine%20operational%20reliability%3F%20The%20standard%20evaluation%20paradigm%20primarily%20focuses%20on%20baseline%20performance%20metrics%2C%20neglecting%20confidence-error%20alignment%20and%20operational%20stability.%20While%20prior%20works%20established%20the%20importance%20of%20temporal%20evaluation%20and%20introduced%20selective%20classification%20in%20malware%20classification%20tasks%2C%20we%20take%20a%20complementary%20direction%20by%20investigating%20whether%20malware%20classifiers%20maintain%20reliable%20and%20stable%20confidence%20estimates%20under%20distribution%20shifts%20and%20exploring%20the%20tensions%20between%20scientific%20advancement%20and%20practical%20impacts%20when%20they%20do%20not.%20We%20propose%20Aurora%2C%20a%20framework%20to%20evaluate%20malware%20classifiers%20based%20on%20their%20confidence%20quality%20and%20operational%20resilience.%20Aurora%20subjects%20the%20confidence%20profile%20of%20a%20given%20model%20to%20verification%20to%20assess%20the%20reliability%20of%20its%20estimates.%20Unreliable%20confidence%20estimates%20erode%20operational%20trust%2C%20waste%20valuable%20annotation%20budgets%20on%20non-informative%20samples%20for%20active%20learning%2C%20and%20leave%20error-prone%20instances%20undetected%20in%20selective%20classification.%20Aurora%20is%20further%20complemented%20by%20a%20set%20of%20metrics%20designed%20to%20go%20beyond%20point-in-time%20performance%2C%20striving%20towards%20a%20more%20holistic%20assessment%20of%20operational%20stability%20throughout%20temporal%20evaluation%20periods.%20The%20fragility%20we%20observe%20in%20SOTA%20frameworks%20across%20datasets%20of%20varying%20drift%20severity%20suggests%20it%20may%20be%20time%20to%20revisit%20the%20underlying%20assumptions.&entry.1838667208=http%3A//arxiv.org/abs/2505.22843v3&entry.124074799=Read"},
{"title": "SpatialMem: Unified 3D Memory with Metric Anchoring and Fast Retrieval", "author": "Xinyi Zheng and Yunze Liu and Chi-Hao Wu and Fan Zhang and Hao Zheng and Wenqi Zhou and Walterio W. Mayol-Cuevas and Junxiao Shen", "abstract": "We present SpatialMem, a memory-centric system that unifies 3D geometry, semantics, and language into a single, queryable representation. Starting from casually captured egocentric RGB video, SpatialMem reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows) as the first-layer scaffold, and populates a hierarchical memory with open-vocabulary object nodes -- linking evidence patches, visual embeddings, and two-layer textual descriptions to 3D coordinates -- for compact storage and fast retrieval. This design enables interpretable reasoning over spatial relations (e.g., distance, direction, visibility) and supports downstream tasks such as language-guided navigation and object retrieval without specialized sensors. Experiments across three real-life indoor scenes demonstrate that SpatialMem maintains strong anchor-description-level navigation completion and hierarchical retrieval accuracy under increasing clutter and occlusion, offering an efficient and extensible framework for embodied spatial intelligence.", "link": "http://arxiv.org/abs/2601.14895v1", "date": "2026-01-21", "relevancy": 2.2791, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5752}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5685}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpatialMem%3A%20Unified%203D%20Memory%20with%20Metric%20Anchoring%20and%20Fast%20Retrieval&body=Title%3A%20SpatialMem%3A%20Unified%203D%20Memory%20with%20Metric%20Anchoring%20and%20Fast%20Retrieval%0AAuthor%3A%20Xinyi%20Zheng%20and%20Yunze%20Liu%20and%20Chi-Hao%20Wu%20and%20Fan%20Zhang%20and%20Hao%20Zheng%20and%20Wenqi%20Zhou%20and%20Walterio%20W.%20Mayol-Cuevas%20and%20Junxiao%20Shen%0AAbstract%3A%20We%20present%20SpatialMem%2C%20a%20memory-centric%20system%20that%20unifies%203D%20geometry%2C%20semantics%2C%20and%20language%20into%20a%20single%2C%20queryable%20representation.%20Starting%20from%20casually%20captured%20egocentric%20RGB%20video%2C%20SpatialMem%20reconstructs%20metrically%20scaled%20indoor%20environments%2C%20detects%20structural%203D%20anchors%20%28walls%2C%20doors%2C%20windows%29%20as%20the%20first-layer%20scaffold%2C%20and%20populates%20a%20hierarchical%20memory%20with%20open-vocabulary%20object%20nodes%20--%20linking%20evidence%20patches%2C%20visual%20embeddings%2C%20and%20two-layer%20textual%20descriptions%20to%203D%20coordinates%20--%20for%20compact%20storage%20and%20fast%20retrieval.%20This%20design%20enables%20interpretable%20reasoning%20over%20spatial%20relations%20%28e.g.%2C%20distance%2C%20direction%2C%20visibility%29%20and%20supports%20downstream%20tasks%20such%20as%20language-guided%20navigation%20and%20object%20retrieval%20without%20specialized%20sensors.%20Experiments%20across%20three%20real-life%20indoor%20scenes%20demonstrate%20that%20SpatialMem%20maintains%20strong%20anchor-description-level%20navigation%20completion%20and%20hierarchical%20retrieval%20accuracy%20under%20increasing%20clutter%20and%20occlusion%2C%20offering%20an%20efficient%20and%20extensible%20framework%20for%20embodied%20spatial%20intelligence.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatialMem%253A%2520Unified%25203D%2520Memory%2520with%2520Metric%2520Anchoring%2520and%2520Fast%2520Retrieval%26entry.906535625%3DXinyi%2520Zheng%2520and%2520Yunze%2520Liu%2520and%2520Chi-Hao%2520Wu%2520and%2520Fan%2520Zhang%2520and%2520Hao%2520Zheng%2520and%2520Wenqi%2520Zhou%2520and%2520Walterio%2520W.%2520Mayol-Cuevas%2520and%2520Junxiao%2520Shen%26entry.1292438233%3DWe%2520present%2520SpatialMem%252C%2520a%2520memory-centric%2520system%2520that%2520unifies%25203D%2520geometry%252C%2520semantics%252C%2520and%2520language%2520into%2520a%2520single%252C%2520queryable%2520representation.%2520Starting%2520from%2520casually%2520captured%2520egocentric%2520RGB%2520video%252C%2520SpatialMem%2520reconstructs%2520metrically%2520scaled%2520indoor%2520environments%252C%2520detects%2520structural%25203D%2520anchors%2520%2528walls%252C%2520doors%252C%2520windows%2529%2520as%2520the%2520first-layer%2520scaffold%252C%2520and%2520populates%2520a%2520hierarchical%2520memory%2520with%2520open-vocabulary%2520object%2520nodes%2520--%2520linking%2520evidence%2520patches%252C%2520visual%2520embeddings%252C%2520and%2520two-layer%2520textual%2520descriptions%2520to%25203D%2520coordinates%2520--%2520for%2520compact%2520storage%2520and%2520fast%2520retrieval.%2520This%2520design%2520enables%2520interpretable%2520reasoning%2520over%2520spatial%2520relations%2520%2528e.g.%252C%2520distance%252C%2520direction%252C%2520visibility%2529%2520and%2520supports%2520downstream%2520tasks%2520such%2520as%2520language-guided%2520navigation%2520and%2520object%2520retrieval%2520without%2520specialized%2520sensors.%2520Experiments%2520across%2520three%2520real-life%2520indoor%2520scenes%2520demonstrate%2520that%2520SpatialMem%2520maintains%2520strong%2520anchor-description-level%2520navigation%2520completion%2520and%2520hierarchical%2520retrieval%2520accuracy%2520under%2520increasing%2520clutter%2520and%2520occlusion%252C%2520offering%2520an%2520efficient%2520and%2520extensible%2520framework%2520for%2520embodied%2520spatial%2520intelligence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpatialMem%3A%20Unified%203D%20Memory%20with%20Metric%20Anchoring%20and%20Fast%20Retrieval&entry.906535625=Xinyi%20Zheng%20and%20Yunze%20Liu%20and%20Chi-Hao%20Wu%20and%20Fan%20Zhang%20and%20Hao%20Zheng%20and%20Wenqi%20Zhou%20and%20Walterio%20W.%20Mayol-Cuevas%20and%20Junxiao%20Shen&entry.1292438233=We%20present%20SpatialMem%2C%20a%20memory-centric%20system%20that%20unifies%203D%20geometry%2C%20semantics%2C%20and%20language%20into%20a%20single%2C%20queryable%20representation.%20Starting%20from%20casually%20captured%20egocentric%20RGB%20video%2C%20SpatialMem%20reconstructs%20metrically%20scaled%20indoor%20environments%2C%20detects%20structural%203D%20anchors%20%28walls%2C%20doors%2C%20windows%29%20as%20the%20first-layer%20scaffold%2C%20and%20populates%20a%20hierarchical%20memory%20with%20open-vocabulary%20object%20nodes%20--%20linking%20evidence%20patches%2C%20visual%20embeddings%2C%20and%20two-layer%20textual%20descriptions%20to%203D%20coordinates%20--%20for%20compact%20storage%20and%20fast%20retrieval.%20This%20design%20enables%20interpretable%20reasoning%20over%20spatial%20relations%20%28e.g.%2C%20distance%2C%20direction%2C%20visibility%29%20and%20supports%20downstream%20tasks%20such%20as%20language-guided%20navigation%20and%20object%20retrieval%20without%20specialized%20sensors.%20Experiments%20across%20three%20real-life%20indoor%20scenes%20demonstrate%20that%20SpatialMem%20maintains%20strong%20anchor-description-level%20navigation%20completion%20and%20hierarchical%20retrieval%20accuracy%20under%20increasing%20clutter%20and%20occlusion%2C%20offering%20an%20efficient%20and%20extensible%20framework%20for%20embodied%20spatial%20intelligence.&entry.1838667208=http%3A//arxiv.org/abs/2601.14895v1&entry.124074799=Read"},
{"title": "Visual and Cognitive Demands of a Large Language Model-Powered In-vehicle Conversational Agent", "author": "Chris Monk and Allegra Ayala and Christine S. P. Yu and Gregory M. Fitch and Dara Gruber", "abstract": "Driver distraction remains a leading contributor to motor vehicle crashes, necessitating rigorous evaluation of new in-vehicle technologies. This study assessed the visual and cognitive demands associated with an advanced Large Language Model (LLM) conversational agent (Gemini Live) during on-road driving, comparing it against handsfree phone calls, visual turn-by-turn guidance (low load baseline), and the Operation Span (OSPAN) task (high load anchor). Thirty-two licensed drivers completed five secondary tasks while visual and cognitive demands were measured using the Detection Response Task (DRT) for cognitive load, eye-tracking for visual attention, and subjective workload ratings. Results indicated that Gemini Live interactions (both single-turn and multi-turn) and hands-free phone calls shared similar levels of cognitive load, between that of visual turn-by-turn guidance and OSPAN. Exploratory analysis showed that cognitive load remained stable across extended multi-turn conversations. All tasks maintained mean glance durations well below the well-established 2-second safety threshold, confirming low visual demand. Furthermore, drivers consistently dedicated longer glances to the roadway between brief off-road glances toward the device during task completion, particularly during voice-based interactions, rendering longer total-eyes-off-road time findings less consequential. Subjective ratings mirrored objective data, with participants reporting low effort, demands, and perceived distraction for Gemini Live. These findings demonstrate that advanced LLM conversational agents, when implemented via voice interfaces, impose cognitive and visual demands comparable to established, low-risk hands-free benchmarks, supporting their safe deployment in the driving environment.", "link": "http://arxiv.org/abs/2601.15034v1", "date": "2026-01-21", "relevancy": 2.2629, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20and%20Cognitive%20Demands%20of%20a%20Large%20Language%20Model-Powered%20In-vehicle%20Conversational%20Agent&body=Title%3A%20Visual%20and%20Cognitive%20Demands%20of%20a%20Large%20Language%20Model-Powered%20In-vehicle%20Conversational%20Agent%0AAuthor%3A%20Chris%20Monk%20and%20Allegra%20Ayala%20and%20Christine%20S.%20P.%20Yu%20and%20Gregory%20M.%20Fitch%20and%20Dara%20Gruber%0AAbstract%3A%20Driver%20distraction%20remains%20a%20leading%20contributor%20to%20motor%20vehicle%20crashes%2C%20necessitating%20rigorous%20evaluation%20of%20new%20in-vehicle%20technologies.%20This%20study%20assessed%20the%20visual%20and%20cognitive%20demands%20associated%20with%20an%20advanced%20Large%20Language%20Model%20%28LLM%29%20conversational%20agent%20%28Gemini%20Live%29%20during%20on-road%20driving%2C%20comparing%20it%20against%20handsfree%20phone%20calls%2C%20visual%20turn-by-turn%20guidance%20%28low%20load%20baseline%29%2C%20and%20the%20Operation%20Span%20%28OSPAN%29%20task%20%28high%20load%20anchor%29.%20Thirty-two%20licensed%20drivers%20completed%20five%20secondary%20tasks%20while%20visual%20and%20cognitive%20demands%20were%20measured%20using%20the%20Detection%20Response%20Task%20%28DRT%29%20for%20cognitive%20load%2C%20eye-tracking%20for%20visual%20attention%2C%20and%20subjective%20workload%20ratings.%20Results%20indicated%20that%20Gemini%20Live%20interactions%20%28both%20single-turn%20and%20multi-turn%29%20and%20hands-free%20phone%20calls%20shared%20similar%20levels%20of%20cognitive%20load%2C%20between%20that%20of%20visual%20turn-by-turn%20guidance%20and%20OSPAN.%20Exploratory%20analysis%20showed%20that%20cognitive%20load%20remained%20stable%20across%20extended%20multi-turn%20conversations.%20All%20tasks%20maintained%20mean%20glance%20durations%20well%20below%20the%20well-established%202-second%20safety%20threshold%2C%20confirming%20low%20visual%20demand.%20Furthermore%2C%20drivers%20consistently%20dedicated%20longer%20glances%20to%20the%20roadway%20between%20brief%20off-road%20glances%20toward%20the%20device%20during%20task%20completion%2C%20particularly%20during%20voice-based%20interactions%2C%20rendering%20longer%20total-eyes-off-road%20time%20findings%20less%20consequential.%20Subjective%20ratings%20mirrored%20objective%20data%2C%20with%20participants%20reporting%20low%20effort%2C%20demands%2C%20and%20perceived%20distraction%20for%20Gemini%20Live.%20These%20findings%20demonstrate%20that%20advanced%20LLM%20conversational%20agents%2C%20when%20implemented%20via%20voice%20interfaces%2C%20impose%20cognitive%20and%20visual%20demands%20comparable%20to%20established%2C%20low-risk%20hands-free%20benchmarks%2C%20supporting%20their%20safe%20deployment%20in%20the%20driving%20environment.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520and%2520Cognitive%2520Demands%2520of%2520a%2520Large%2520Language%2520Model-Powered%2520In-vehicle%2520Conversational%2520Agent%26entry.906535625%3DChris%2520Monk%2520and%2520Allegra%2520Ayala%2520and%2520Christine%2520S.%2520P.%2520Yu%2520and%2520Gregory%2520M.%2520Fitch%2520and%2520Dara%2520Gruber%26entry.1292438233%3DDriver%2520distraction%2520remains%2520a%2520leading%2520contributor%2520to%2520motor%2520vehicle%2520crashes%252C%2520necessitating%2520rigorous%2520evaluation%2520of%2520new%2520in-vehicle%2520technologies.%2520This%2520study%2520assessed%2520the%2520visual%2520and%2520cognitive%2520demands%2520associated%2520with%2520an%2520advanced%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520conversational%2520agent%2520%2528Gemini%2520Live%2529%2520during%2520on-road%2520driving%252C%2520comparing%2520it%2520against%2520handsfree%2520phone%2520calls%252C%2520visual%2520turn-by-turn%2520guidance%2520%2528low%2520load%2520baseline%2529%252C%2520and%2520the%2520Operation%2520Span%2520%2528OSPAN%2529%2520task%2520%2528high%2520load%2520anchor%2529.%2520Thirty-two%2520licensed%2520drivers%2520completed%2520five%2520secondary%2520tasks%2520while%2520visual%2520and%2520cognitive%2520demands%2520were%2520measured%2520using%2520the%2520Detection%2520Response%2520Task%2520%2528DRT%2529%2520for%2520cognitive%2520load%252C%2520eye-tracking%2520for%2520visual%2520attention%252C%2520and%2520subjective%2520workload%2520ratings.%2520Results%2520indicated%2520that%2520Gemini%2520Live%2520interactions%2520%2528both%2520single-turn%2520and%2520multi-turn%2529%2520and%2520hands-free%2520phone%2520calls%2520shared%2520similar%2520levels%2520of%2520cognitive%2520load%252C%2520between%2520that%2520of%2520visual%2520turn-by-turn%2520guidance%2520and%2520OSPAN.%2520Exploratory%2520analysis%2520showed%2520that%2520cognitive%2520load%2520remained%2520stable%2520across%2520extended%2520multi-turn%2520conversations.%2520All%2520tasks%2520maintained%2520mean%2520glance%2520durations%2520well%2520below%2520the%2520well-established%25202-second%2520safety%2520threshold%252C%2520confirming%2520low%2520visual%2520demand.%2520Furthermore%252C%2520drivers%2520consistently%2520dedicated%2520longer%2520glances%2520to%2520the%2520roadway%2520between%2520brief%2520off-road%2520glances%2520toward%2520the%2520device%2520during%2520task%2520completion%252C%2520particularly%2520during%2520voice-based%2520interactions%252C%2520rendering%2520longer%2520total-eyes-off-road%2520time%2520findings%2520less%2520consequential.%2520Subjective%2520ratings%2520mirrored%2520objective%2520data%252C%2520with%2520participants%2520reporting%2520low%2520effort%252C%2520demands%252C%2520and%2520perceived%2520distraction%2520for%2520Gemini%2520Live.%2520These%2520findings%2520demonstrate%2520that%2520advanced%2520LLM%2520conversational%2520agents%252C%2520when%2520implemented%2520via%2520voice%2520interfaces%252C%2520impose%2520cognitive%2520and%2520visual%2520demands%2520comparable%2520to%2520established%252C%2520low-risk%2520hands-free%2520benchmarks%252C%2520supporting%2520their%2520safe%2520deployment%2520in%2520the%2520driving%2520environment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20and%20Cognitive%20Demands%20of%20a%20Large%20Language%20Model-Powered%20In-vehicle%20Conversational%20Agent&entry.906535625=Chris%20Monk%20and%20Allegra%20Ayala%20and%20Christine%20S.%20P.%20Yu%20and%20Gregory%20M.%20Fitch%20and%20Dara%20Gruber&entry.1292438233=Driver%20distraction%20remains%20a%20leading%20contributor%20to%20motor%20vehicle%20crashes%2C%20necessitating%20rigorous%20evaluation%20of%20new%20in-vehicle%20technologies.%20This%20study%20assessed%20the%20visual%20and%20cognitive%20demands%20associated%20with%20an%20advanced%20Large%20Language%20Model%20%28LLM%29%20conversational%20agent%20%28Gemini%20Live%29%20during%20on-road%20driving%2C%20comparing%20it%20against%20handsfree%20phone%20calls%2C%20visual%20turn-by-turn%20guidance%20%28low%20load%20baseline%29%2C%20and%20the%20Operation%20Span%20%28OSPAN%29%20task%20%28high%20load%20anchor%29.%20Thirty-two%20licensed%20drivers%20completed%20five%20secondary%20tasks%20while%20visual%20and%20cognitive%20demands%20were%20measured%20using%20the%20Detection%20Response%20Task%20%28DRT%29%20for%20cognitive%20load%2C%20eye-tracking%20for%20visual%20attention%2C%20and%20subjective%20workload%20ratings.%20Results%20indicated%20that%20Gemini%20Live%20interactions%20%28both%20single-turn%20and%20multi-turn%29%20and%20hands-free%20phone%20calls%20shared%20similar%20levels%20of%20cognitive%20load%2C%20between%20that%20of%20visual%20turn-by-turn%20guidance%20and%20OSPAN.%20Exploratory%20analysis%20showed%20that%20cognitive%20load%20remained%20stable%20across%20extended%20multi-turn%20conversations.%20All%20tasks%20maintained%20mean%20glance%20durations%20well%20below%20the%20well-established%202-second%20safety%20threshold%2C%20confirming%20low%20visual%20demand.%20Furthermore%2C%20drivers%20consistently%20dedicated%20longer%20glances%20to%20the%20roadway%20between%20brief%20off-road%20glances%20toward%20the%20device%20during%20task%20completion%2C%20particularly%20during%20voice-based%20interactions%2C%20rendering%20longer%20total-eyes-off-road%20time%20findings%20less%20consequential.%20Subjective%20ratings%20mirrored%20objective%20data%2C%20with%20participants%20reporting%20low%20effort%2C%20demands%2C%20and%20perceived%20distraction%20for%20Gemini%20Live.%20These%20findings%20demonstrate%20that%20advanced%20LLM%20conversational%20agents%2C%20when%20implemented%20via%20voice%20interfaces%2C%20impose%20cognitive%20and%20visual%20demands%20comparable%20to%20established%2C%20low-risk%20hands-free%20benchmarks%2C%20supporting%20their%20safe%20deployment%20in%20the%20driving%20environment.&entry.1838667208=http%3A//arxiv.org/abs/2601.15034v1&entry.124074799=Read"},
{"title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries", "author": "Shijie Lian and Bin Yu and Xiaopeng Lin and Laurence T. Yang and Zhaolong Shen and Changti Wu and Yuzhuo Miao and Cong Huang and Kai Chen", "abstract": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \\mid v)$ and a language-conditioned posterior $\u03c0(a \\mid v, \\ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.", "link": "http://arxiv.org/abs/2601.15197v1", "date": "2026-01-21", "relevancy": 2.2518, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6087}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BayesianVLA%3A%20Bayesian%20Decomposition%20of%20Vision%20Language%20Action%20Models%20via%20Latent%20Action%20Queries&body=Title%3A%20BayesianVLA%3A%20Bayesian%20Decomposition%20of%20Vision%20Language%20Action%20Models%20via%20Latent%20Action%20Queries%0AAuthor%3A%20Shijie%20Lian%20and%20Bin%20Yu%20and%20Xiaopeng%20Lin%20and%20Laurence%20T.%20Yang%20and%20Zhaolong%20Shen%20and%20Changti%20Wu%20and%20Yuzhuo%20Miao%20and%20Cong%20Huang%20and%20Kai%20Chen%0AAbstract%3A%20Vision-Language-Action%20%28VLA%29%20models%20have%20shown%20promise%20in%20robot%20manipulation%20but%20often%20struggle%20to%20generalize%20to%20new%20instructions%20or%20complex%20multi-task%20scenarios.%20We%20identify%20a%20critical%20pathology%20in%20current%20training%20paradigms%20where%20goal-driven%20data%20collection%20creates%20a%20dataset%20bias.%20In%20such%20datasets%2C%20language%20instructions%20are%20highly%20predictable%20from%20visual%20observations%20alone%2C%20causing%20the%20conditional%20mutual%20information%20between%20instructions%20and%20actions%20to%20vanish%2C%20a%20phenomenon%20we%20term%20Information%20Collapse.%20Consequently%2C%20models%20degenerate%20into%20vision-only%20policies%20that%20ignore%20language%20constraints%20and%20fail%20in%20out-of-distribution%20%28OOD%29%20settings.%20To%20address%20this%2C%20we%20propose%20BayesianVLA%2C%20a%20novel%20framework%20that%20enforces%20instruction%20following%20via%20Bayesian%20decomposition.%20By%20introducing%20learnable%20Latent%20Action%20Queries%2C%20we%20construct%20a%20dual-branch%20architecture%20to%20estimate%20both%20a%20vision-only%20prior%20%24p%28a%20%5Cmid%20v%29%24%20and%20a%20language-conditioned%20posterior%20%24%CF%80%28a%20%5Cmid%20v%2C%20%5Cell%29%24.%20We%20then%20optimize%20the%20policy%20to%20maximize%20the%20conditional%20Pointwise%20Mutual%20Information%20%28PMI%29%20between%20actions%20and%20instructions.%20This%20objective%20effectively%20penalizes%20the%20vision%20shortcut%20and%20rewards%20actions%20that%20explicitly%20explain%20the%20language%20command.%20Without%20requiring%20new%20data%2C%20BayesianVLA%20significantly%20improves%20generalization.%20Extensive%20experiments%20across%20on%20SimplerEnv%20and%20RoboCasa%20demonstrate%20substantial%20gains%2C%20including%20an%2011.3%25%20improvement%20on%20the%20challenging%20OOD%20SimplerEnv%20benchmark%2C%20validating%20the%20ability%20of%20our%20approach%20to%20robustly%20ground%20language%20in%20action.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15197v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesianVLA%253A%2520Bayesian%2520Decomposition%2520of%2520Vision%2520Language%2520Action%2520Models%2520via%2520Latent%2520Action%2520Queries%26entry.906535625%3DShijie%2520Lian%2520and%2520Bin%2520Yu%2520and%2520Xiaopeng%2520Lin%2520and%2520Laurence%2520T.%2520Yang%2520and%2520Zhaolong%2520Shen%2520and%2520Changti%2520Wu%2520and%2520Yuzhuo%2520Miao%2520and%2520Cong%2520Huang%2520and%2520Kai%2520Chen%26entry.1292438233%3DVision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520shown%2520promise%2520in%2520robot%2520manipulation%2520but%2520often%2520struggle%2520to%2520generalize%2520to%2520new%2520instructions%2520or%2520complex%2520multi-task%2520scenarios.%2520We%2520identify%2520a%2520critical%2520pathology%2520in%2520current%2520training%2520paradigms%2520where%2520goal-driven%2520data%2520collection%2520creates%2520a%2520dataset%2520bias.%2520In%2520such%2520datasets%252C%2520language%2520instructions%2520are%2520highly%2520predictable%2520from%2520visual%2520observations%2520alone%252C%2520causing%2520the%2520conditional%2520mutual%2520information%2520between%2520instructions%2520and%2520actions%2520to%2520vanish%252C%2520a%2520phenomenon%2520we%2520term%2520Information%2520Collapse.%2520Consequently%252C%2520models%2520degenerate%2520into%2520vision-only%2520policies%2520that%2520ignore%2520language%2520constraints%2520and%2520fail%2520in%2520out-of-distribution%2520%2528OOD%2529%2520settings.%2520To%2520address%2520this%252C%2520we%2520propose%2520BayesianVLA%252C%2520a%2520novel%2520framework%2520that%2520enforces%2520instruction%2520following%2520via%2520Bayesian%2520decomposition.%2520By%2520introducing%2520learnable%2520Latent%2520Action%2520Queries%252C%2520we%2520construct%2520a%2520dual-branch%2520architecture%2520to%2520estimate%2520both%2520a%2520vision-only%2520prior%2520%2524p%2528a%2520%255Cmid%2520v%2529%2524%2520and%2520a%2520language-conditioned%2520posterior%2520%2524%25CF%2580%2528a%2520%255Cmid%2520v%252C%2520%255Cell%2529%2524.%2520We%2520then%2520optimize%2520the%2520policy%2520to%2520maximize%2520the%2520conditional%2520Pointwise%2520Mutual%2520Information%2520%2528PMI%2529%2520between%2520actions%2520and%2520instructions.%2520This%2520objective%2520effectively%2520penalizes%2520the%2520vision%2520shortcut%2520and%2520rewards%2520actions%2520that%2520explicitly%2520explain%2520the%2520language%2520command.%2520Without%2520requiring%2520new%2520data%252C%2520BayesianVLA%2520significantly%2520improves%2520generalization.%2520Extensive%2520experiments%2520across%2520on%2520SimplerEnv%2520and%2520RoboCasa%2520demonstrate%2520substantial%2520gains%252C%2520including%2520an%252011.3%2525%2520improvement%2520on%2520the%2520challenging%2520OOD%2520SimplerEnv%2520benchmark%252C%2520validating%2520the%2520ability%2520of%2520our%2520approach%2520to%2520robustly%2520ground%2520language%2520in%2520action.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15197v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BayesianVLA%3A%20Bayesian%20Decomposition%20of%20Vision%20Language%20Action%20Models%20via%20Latent%20Action%20Queries&entry.906535625=Shijie%20Lian%20and%20Bin%20Yu%20and%20Xiaopeng%20Lin%20and%20Laurence%20T.%20Yang%20and%20Zhaolong%20Shen%20and%20Changti%20Wu%20and%20Yuzhuo%20Miao%20and%20Cong%20Huang%20and%20Kai%20Chen&entry.1292438233=Vision-Language-Action%20%28VLA%29%20models%20have%20shown%20promise%20in%20robot%20manipulation%20but%20often%20struggle%20to%20generalize%20to%20new%20instructions%20or%20complex%20multi-task%20scenarios.%20We%20identify%20a%20critical%20pathology%20in%20current%20training%20paradigms%20where%20goal-driven%20data%20collection%20creates%20a%20dataset%20bias.%20In%20such%20datasets%2C%20language%20instructions%20are%20highly%20predictable%20from%20visual%20observations%20alone%2C%20causing%20the%20conditional%20mutual%20information%20between%20instructions%20and%20actions%20to%20vanish%2C%20a%20phenomenon%20we%20term%20Information%20Collapse.%20Consequently%2C%20models%20degenerate%20into%20vision-only%20policies%20that%20ignore%20language%20constraints%20and%20fail%20in%20out-of-distribution%20%28OOD%29%20settings.%20To%20address%20this%2C%20we%20propose%20BayesianVLA%2C%20a%20novel%20framework%20that%20enforces%20instruction%20following%20via%20Bayesian%20decomposition.%20By%20introducing%20learnable%20Latent%20Action%20Queries%2C%20we%20construct%20a%20dual-branch%20architecture%20to%20estimate%20both%20a%20vision-only%20prior%20%24p%28a%20%5Cmid%20v%29%24%20and%20a%20language-conditioned%20posterior%20%24%CF%80%28a%20%5Cmid%20v%2C%20%5Cell%29%24.%20We%20then%20optimize%20the%20policy%20to%20maximize%20the%20conditional%20Pointwise%20Mutual%20Information%20%28PMI%29%20between%20actions%20and%20instructions.%20This%20objective%20effectively%20penalizes%20the%20vision%20shortcut%20and%20rewards%20actions%20that%20explicitly%20explain%20the%20language%20command.%20Without%20requiring%20new%20data%2C%20BayesianVLA%20significantly%20improves%20generalization.%20Extensive%20experiments%20across%20on%20SimplerEnv%20and%20RoboCasa%20demonstrate%20substantial%20gains%2C%20including%20an%2011.3%25%20improvement%20on%20the%20challenging%20OOD%20SimplerEnv%20benchmark%2C%20validating%20the%20ability%20of%20our%20approach%20to%20robustly%20ground%20language%20in%20action.&entry.1838667208=http%3A//arxiv.org/abs/2601.15197v1&entry.124074799=Read"},
{"title": "SPECTRE: Conditional System Prompt Poisoning to Hijack LLMs", "author": "Viet Pham and Thai Le", "abstract": "Large Language Models (LLMs) are increasingly deployed via third-party system prompts downloaded from public marketplaces. We identify a critical supply-chain vulnerability: conditional system prompt poisoning, where an adversary injects a ``sleeper agent'' into a benign-looking prompt. Unlike traditional jailbreaks that aim for broad refusal-breaking, our proposed framework, SPECTRE, optimizes system prompts to trigger LLMs to output targeted, compromised responses only for specific queries (e.g., ``Who should I vote for the US President?'') while maintaining high utility on benign inputs. Operating in a strict black-box setting without model weight access, SPECTRE utilizes a two-stage optimization including a global semantic search followed by a greedy lexical refinement. Tested on open-source models and commercial APIs (GPT-4o-mini, GPT-3.5), SPECTRE achieves up to 70% F1 reduction on targeted queries with minimal degradation to general capabilities. We further demonstrate that these poisoned prompts evade standard defenses, including perplexity filters and typo-correction, by exploiting the natural noise found in real-world system prompts. Our code and data are available at https://github.com/vietph34/CAIN. WARNING: Our paper contains examples that might be sensitive to the readers!", "link": "http://arxiv.org/abs/2505.16888v3", "date": "2026-01-21", "relevancy": 2.2439, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4502}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4502}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPECTRE%3A%20Conditional%20System%20Prompt%20Poisoning%20to%20Hijack%20LLMs&body=Title%3A%20SPECTRE%3A%20Conditional%20System%20Prompt%20Poisoning%20to%20Hijack%20LLMs%0AAuthor%3A%20Viet%20Pham%20and%20Thai%20Le%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20deployed%20via%20third-party%20system%20prompts%20downloaded%20from%20public%20marketplaces.%20We%20identify%20a%20critical%20supply-chain%20vulnerability%3A%20conditional%20system%20prompt%20poisoning%2C%20where%20an%20adversary%20injects%20a%20%60%60sleeper%20agent%27%27%20into%20a%20benign-looking%20prompt.%20Unlike%20traditional%20jailbreaks%20that%20aim%20for%20broad%20refusal-breaking%2C%20our%20proposed%20framework%2C%20SPECTRE%2C%20optimizes%20system%20prompts%20to%20trigger%20LLMs%20to%20output%20targeted%2C%20compromised%20responses%20only%20for%20specific%20queries%20%28e.g.%2C%20%60%60Who%20should%20I%20vote%20for%20the%20US%20President%3F%27%27%29%20while%20maintaining%20high%20utility%20on%20benign%20inputs.%20Operating%20in%20a%20strict%20black-box%20setting%20without%20model%20weight%20access%2C%20SPECTRE%20utilizes%20a%20two-stage%20optimization%20including%20a%20global%20semantic%20search%20followed%20by%20a%20greedy%20lexical%20refinement.%20Tested%20on%20open-source%20models%20and%20commercial%20APIs%20%28GPT-4o-mini%2C%20GPT-3.5%29%2C%20SPECTRE%20achieves%20up%20to%2070%25%20F1%20reduction%20on%20targeted%20queries%20with%20minimal%20degradation%20to%20general%20capabilities.%20We%20further%20demonstrate%20that%20these%20poisoned%20prompts%20evade%20standard%20defenses%2C%20including%20perplexity%20filters%20and%20typo-correction%2C%20by%20exploiting%20the%20natural%20noise%20found%20in%20real-world%20system%20prompts.%20Our%20code%20and%20data%20are%20available%20at%20https%3A//github.com/vietph34/CAIN.%20WARNING%3A%20Our%20paper%20contains%20examples%20that%20might%20be%20sensitive%20to%20the%20readers%21%0ALink%3A%20http%3A//arxiv.org/abs/2505.16888v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPECTRE%253A%2520Conditional%2520System%2520Prompt%2520Poisoning%2520to%2520Hijack%2520LLMs%26entry.906535625%3DViet%2520Pham%2520and%2520Thai%2520Le%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520deployed%2520via%2520third-party%2520system%2520prompts%2520downloaded%2520from%2520public%2520marketplaces.%2520We%2520identify%2520a%2520critical%2520supply-chain%2520vulnerability%253A%2520conditional%2520system%2520prompt%2520poisoning%252C%2520where%2520an%2520adversary%2520injects%2520a%2520%2560%2560sleeper%2520agent%2527%2527%2520into%2520a%2520benign-looking%2520prompt.%2520Unlike%2520traditional%2520jailbreaks%2520that%2520aim%2520for%2520broad%2520refusal-breaking%252C%2520our%2520proposed%2520framework%252C%2520SPECTRE%252C%2520optimizes%2520system%2520prompts%2520to%2520trigger%2520LLMs%2520to%2520output%2520targeted%252C%2520compromised%2520responses%2520only%2520for%2520specific%2520queries%2520%2528e.g.%252C%2520%2560%2560Who%2520should%2520I%2520vote%2520for%2520the%2520US%2520President%253F%2527%2527%2529%2520while%2520maintaining%2520high%2520utility%2520on%2520benign%2520inputs.%2520Operating%2520in%2520a%2520strict%2520black-box%2520setting%2520without%2520model%2520weight%2520access%252C%2520SPECTRE%2520utilizes%2520a%2520two-stage%2520optimization%2520including%2520a%2520global%2520semantic%2520search%2520followed%2520by%2520a%2520greedy%2520lexical%2520refinement.%2520Tested%2520on%2520open-source%2520models%2520and%2520commercial%2520APIs%2520%2528GPT-4o-mini%252C%2520GPT-3.5%2529%252C%2520SPECTRE%2520achieves%2520up%2520to%252070%2525%2520F1%2520reduction%2520on%2520targeted%2520queries%2520with%2520minimal%2520degradation%2520to%2520general%2520capabilities.%2520We%2520further%2520demonstrate%2520that%2520these%2520poisoned%2520prompts%2520evade%2520standard%2520defenses%252C%2520including%2520perplexity%2520filters%2520and%2520typo-correction%252C%2520by%2520exploiting%2520the%2520natural%2520noise%2520found%2520in%2520real-world%2520system%2520prompts.%2520Our%2520code%2520and%2520data%2520are%2520available%2520at%2520https%253A//github.com/vietph34/CAIN.%2520WARNING%253A%2520Our%2520paper%2520contains%2520examples%2520that%2520might%2520be%2520sensitive%2520to%2520the%2520readers%2521%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16888v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPECTRE%3A%20Conditional%20System%20Prompt%20Poisoning%20to%20Hijack%20LLMs&entry.906535625=Viet%20Pham%20and%20Thai%20Le&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20deployed%20via%20third-party%20system%20prompts%20downloaded%20from%20public%20marketplaces.%20We%20identify%20a%20critical%20supply-chain%20vulnerability%3A%20conditional%20system%20prompt%20poisoning%2C%20where%20an%20adversary%20injects%20a%20%60%60sleeper%20agent%27%27%20into%20a%20benign-looking%20prompt.%20Unlike%20traditional%20jailbreaks%20that%20aim%20for%20broad%20refusal-breaking%2C%20our%20proposed%20framework%2C%20SPECTRE%2C%20optimizes%20system%20prompts%20to%20trigger%20LLMs%20to%20output%20targeted%2C%20compromised%20responses%20only%20for%20specific%20queries%20%28e.g.%2C%20%60%60Who%20should%20I%20vote%20for%20the%20US%20President%3F%27%27%29%20while%20maintaining%20high%20utility%20on%20benign%20inputs.%20Operating%20in%20a%20strict%20black-box%20setting%20without%20model%20weight%20access%2C%20SPECTRE%20utilizes%20a%20two-stage%20optimization%20including%20a%20global%20semantic%20search%20followed%20by%20a%20greedy%20lexical%20refinement.%20Tested%20on%20open-source%20models%20and%20commercial%20APIs%20%28GPT-4o-mini%2C%20GPT-3.5%29%2C%20SPECTRE%20achieves%20up%20to%2070%25%20F1%20reduction%20on%20targeted%20queries%20with%20minimal%20degradation%20to%20general%20capabilities.%20We%20further%20demonstrate%20that%20these%20poisoned%20prompts%20evade%20standard%20defenses%2C%20including%20perplexity%20filters%20and%20typo-correction%2C%20by%20exploiting%20the%20natural%20noise%20found%20in%20real-world%20system%20prompts.%20Our%20code%20and%20data%20are%20available%20at%20https%3A//github.com/vietph34/CAIN.%20WARNING%3A%20Our%20paper%20contains%20examples%20that%20might%20be%20sensitive%20to%20the%20readers%21&entry.1838667208=http%3A//arxiv.org/abs/2505.16888v3&entry.124074799=Read"},
{"title": "The Pictorial Cortex: Zero-Shot Cross-Subject fMRI-to-Image Reconstruction via Compositional Latent Modeling", "author": "Jingyang Huo and Yikai Wang and Yanwei Fu and Jianfeng Feng", "abstract": "Decoding visual experiences from human brain activity remains a central challenge at the intersection of neuroscience, neuroimaging, and artificial intelligence. A critical obstacle is the inherent variability of cortical responses: neural activity elicited by the same visual stimulus differs across individuals and trials due to anatomical, functional, cognitive, and experimental factors, making fMRI-to-image reconstruction non-injective. In this paper, we tackle a challenging yet practically meaningful problem: zero-shot cross-subject fMRI-to-image reconstruction, where the visual experience of a previously unseen individual must be reconstructed without subject-specific training. To enable principled evaluation, we present a unified cortical-surface dataset -- UniCortex-fMRI, assembled from multiple visual-stimulus fMRI datasets to provide broad coverage of subjects and stimuli. Our UniCortex-fMRI is particularly processed by standardized data formats to make it possible to explore this possibility in the zero-shot scenario of cross-subject fMRI-to-image reconstruction. To tackle the modeling challenge, we propose PictorialCortex, which models fMRI activity using a compositional latent formulation that structures stimulus-driven representations under subject-, dataset-, and trial-related variability. PictorialCortex operates in a universal cortical latent space and implements this formulation through a latent factorization-composition module, reinforced by paired factorization and re-factorizing consistency regularization. During inference, surrogate latents synthesized under multiple seen-subject conditions are aggregated to guide diffusion-based image synthesis for unseen subjects. Extensive experiments show that PictorialCortex improves zero-shot cross-subject visual reconstruction, highlighting the benefits of compositional latent modeling and multi-dataset training.", "link": "http://arxiv.org/abs/2601.15071v1", "date": "2026-01-21", "relevancy": 2.2432, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Pictorial%20Cortex%3A%20Zero-Shot%20Cross-Subject%20fMRI-to-Image%20Reconstruction%20via%20Compositional%20Latent%20Modeling&body=Title%3A%20The%20Pictorial%20Cortex%3A%20Zero-Shot%20Cross-Subject%20fMRI-to-Image%20Reconstruction%20via%20Compositional%20Latent%20Modeling%0AAuthor%3A%20Jingyang%20Huo%20and%20Yikai%20Wang%20and%20Yanwei%20Fu%20and%20Jianfeng%20Feng%0AAbstract%3A%20Decoding%20visual%20experiences%20from%20human%20brain%20activity%20remains%20a%20central%20challenge%20at%20the%20intersection%20of%20neuroscience%2C%20neuroimaging%2C%20and%20artificial%20intelligence.%20A%20critical%20obstacle%20is%20the%20inherent%20variability%20of%20cortical%20responses%3A%20neural%20activity%20elicited%20by%20the%20same%20visual%20stimulus%20differs%20across%20individuals%20and%20trials%20due%20to%20anatomical%2C%20functional%2C%20cognitive%2C%20and%20experimental%20factors%2C%20making%20fMRI-to-image%20reconstruction%20non-injective.%20In%20this%20paper%2C%20we%20tackle%20a%20challenging%20yet%20practically%20meaningful%20problem%3A%20zero-shot%20cross-subject%20fMRI-to-image%20reconstruction%2C%20where%20the%20visual%20experience%20of%20a%20previously%20unseen%20individual%20must%20be%20reconstructed%20without%20subject-specific%20training.%20To%20enable%20principled%20evaluation%2C%20we%20present%20a%20unified%20cortical-surface%20dataset%20--%20UniCortex-fMRI%2C%20assembled%20from%20multiple%20visual-stimulus%20fMRI%20datasets%20to%20provide%20broad%20coverage%20of%20subjects%20and%20stimuli.%20Our%20UniCortex-fMRI%20is%20particularly%20processed%20by%20standardized%20data%20formats%20to%20make%20it%20possible%20to%20explore%20this%20possibility%20in%20the%20zero-shot%20scenario%20of%20cross-subject%20fMRI-to-image%20reconstruction.%20To%20tackle%20the%20modeling%20challenge%2C%20we%20propose%20PictorialCortex%2C%20which%20models%20fMRI%20activity%20using%20a%20compositional%20latent%20formulation%20that%20structures%20stimulus-driven%20representations%20under%20subject-%2C%20dataset-%2C%20and%20trial-related%20variability.%20PictorialCortex%20operates%20in%20a%20universal%20cortical%20latent%20space%20and%20implements%20this%20formulation%20through%20a%20latent%20factorization-composition%20module%2C%20reinforced%20by%20paired%20factorization%20and%20re-factorizing%20consistency%20regularization.%20During%20inference%2C%20surrogate%20latents%20synthesized%20under%20multiple%20seen-subject%20conditions%20are%20aggregated%20to%20guide%20diffusion-based%20image%20synthesis%20for%20unseen%20subjects.%20Extensive%20experiments%20show%20that%20PictorialCortex%20improves%20zero-shot%20cross-subject%20visual%20reconstruction%2C%20highlighting%20the%20benefits%20of%20compositional%20latent%20modeling%20and%20multi-dataset%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15071v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Pictorial%2520Cortex%253A%2520Zero-Shot%2520Cross-Subject%2520fMRI-to-Image%2520Reconstruction%2520via%2520Compositional%2520Latent%2520Modeling%26entry.906535625%3DJingyang%2520Huo%2520and%2520Yikai%2520Wang%2520and%2520Yanwei%2520Fu%2520and%2520Jianfeng%2520Feng%26entry.1292438233%3DDecoding%2520visual%2520experiences%2520from%2520human%2520brain%2520activity%2520remains%2520a%2520central%2520challenge%2520at%2520the%2520intersection%2520of%2520neuroscience%252C%2520neuroimaging%252C%2520and%2520artificial%2520intelligence.%2520A%2520critical%2520obstacle%2520is%2520the%2520inherent%2520variability%2520of%2520cortical%2520responses%253A%2520neural%2520activity%2520elicited%2520by%2520the%2520same%2520visual%2520stimulus%2520differs%2520across%2520individuals%2520and%2520trials%2520due%2520to%2520anatomical%252C%2520functional%252C%2520cognitive%252C%2520and%2520experimental%2520factors%252C%2520making%2520fMRI-to-image%2520reconstruction%2520non-injective.%2520In%2520this%2520paper%252C%2520we%2520tackle%2520a%2520challenging%2520yet%2520practically%2520meaningful%2520problem%253A%2520zero-shot%2520cross-subject%2520fMRI-to-image%2520reconstruction%252C%2520where%2520the%2520visual%2520experience%2520of%2520a%2520previously%2520unseen%2520individual%2520must%2520be%2520reconstructed%2520without%2520subject-specific%2520training.%2520To%2520enable%2520principled%2520evaluation%252C%2520we%2520present%2520a%2520unified%2520cortical-surface%2520dataset%2520--%2520UniCortex-fMRI%252C%2520assembled%2520from%2520multiple%2520visual-stimulus%2520fMRI%2520datasets%2520to%2520provide%2520broad%2520coverage%2520of%2520subjects%2520and%2520stimuli.%2520Our%2520UniCortex-fMRI%2520is%2520particularly%2520processed%2520by%2520standardized%2520data%2520formats%2520to%2520make%2520it%2520possible%2520to%2520explore%2520this%2520possibility%2520in%2520the%2520zero-shot%2520scenario%2520of%2520cross-subject%2520fMRI-to-image%2520reconstruction.%2520To%2520tackle%2520the%2520modeling%2520challenge%252C%2520we%2520propose%2520PictorialCortex%252C%2520which%2520models%2520fMRI%2520activity%2520using%2520a%2520compositional%2520latent%2520formulation%2520that%2520structures%2520stimulus-driven%2520representations%2520under%2520subject-%252C%2520dataset-%252C%2520and%2520trial-related%2520variability.%2520PictorialCortex%2520operates%2520in%2520a%2520universal%2520cortical%2520latent%2520space%2520and%2520implements%2520this%2520formulation%2520through%2520a%2520latent%2520factorization-composition%2520module%252C%2520reinforced%2520by%2520paired%2520factorization%2520and%2520re-factorizing%2520consistency%2520regularization.%2520During%2520inference%252C%2520surrogate%2520latents%2520synthesized%2520under%2520multiple%2520seen-subject%2520conditions%2520are%2520aggregated%2520to%2520guide%2520diffusion-based%2520image%2520synthesis%2520for%2520unseen%2520subjects.%2520Extensive%2520experiments%2520show%2520that%2520PictorialCortex%2520improves%2520zero-shot%2520cross-subject%2520visual%2520reconstruction%252C%2520highlighting%2520the%2520benefits%2520of%2520compositional%2520latent%2520modeling%2520and%2520multi-dataset%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15071v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Pictorial%20Cortex%3A%20Zero-Shot%20Cross-Subject%20fMRI-to-Image%20Reconstruction%20via%20Compositional%20Latent%20Modeling&entry.906535625=Jingyang%20Huo%20and%20Yikai%20Wang%20and%20Yanwei%20Fu%20and%20Jianfeng%20Feng&entry.1292438233=Decoding%20visual%20experiences%20from%20human%20brain%20activity%20remains%20a%20central%20challenge%20at%20the%20intersection%20of%20neuroscience%2C%20neuroimaging%2C%20and%20artificial%20intelligence.%20A%20critical%20obstacle%20is%20the%20inherent%20variability%20of%20cortical%20responses%3A%20neural%20activity%20elicited%20by%20the%20same%20visual%20stimulus%20differs%20across%20individuals%20and%20trials%20due%20to%20anatomical%2C%20functional%2C%20cognitive%2C%20and%20experimental%20factors%2C%20making%20fMRI-to-image%20reconstruction%20non-injective.%20In%20this%20paper%2C%20we%20tackle%20a%20challenging%20yet%20practically%20meaningful%20problem%3A%20zero-shot%20cross-subject%20fMRI-to-image%20reconstruction%2C%20where%20the%20visual%20experience%20of%20a%20previously%20unseen%20individual%20must%20be%20reconstructed%20without%20subject-specific%20training.%20To%20enable%20principled%20evaluation%2C%20we%20present%20a%20unified%20cortical-surface%20dataset%20--%20UniCortex-fMRI%2C%20assembled%20from%20multiple%20visual-stimulus%20fMRI%20datasets%20to%20provide%20broad%20coverage%20of%20subjects%20and%20stimuli.%20Our%20UniCortex-fMRI%20is%20particularly%20processed%20by%20standardized%20data%20formats%20to%20make%20it%20possible%20to%20explore%20this%20possibility%20in%20the%20zero-shot%20scenario%20of%20cross-subject%20fMRI-to-image%20reconstruction.%20To%20tackle%20the%20modeling%20challenge%2C%20we%20propose%20PictorialCortex%2C%20which%20models%20fMRI%20activity%20using%20a%20compositional%20latent%20formulation%20that%20structures%20stimulus-driven%20representations%20under%20subject-%2C%20dataset-%2C%20and%20trial-related%20variability.%20PictorialCortex%20operates%20in%20a%20universal%20cortical%20latent%20space%20and%20implements%20this%20formulation%20through%20a%20latent%20factorization-composition%20module%2C%20reinforced%20by%20paired%20factorization%20and%20re-factorizing%20consistency%20regularization.%20During%20inference%2C%20surrogate%20latents%20synthesized%20under%20multiple%20seen-subject%20conditions%20are%20aggregated%20to%20guide%20diffusion-based%20image%20synthesis%20for%20unseen%20subjects.%20Extensive%20experiments%20show%20that%20PictorialCortex%20improves%20zero-shot%20cross-subject%20visual%20reconstruction%2C%20highlighting%20the%20benefits%20of%20compositional%20latent%20modeling%20and%20multi-dataset%20training.&entry.1838667208=http%3A//arxiv.org/abs/2601.15071v1&entry.124074799=Read"},
{"title": "Towards Understanding Best Practices for Quantization of Vision-Language Models", "author": "Gautom Das and Vincent La and Ethan Lau and Abhinav Shrivastava and Matthew Gwilliam", "abstract": "Large language models (LLMs) deliver impressive results for a variety of tasks, but state-of-the-art systems require fast GPUs with large amounts of memory. To reduce both the memory and latency of these systems, practitioners quantize their learned parameters, typically at half precision. A growing body of research focuses on preserving the model performance with more aggressive bit widths, and some work has been done to apply these strategies to other models, like vision transformers. In our study we investigate how a variety of quantization methods, including state-of-the-art GPTQ and AWQ, can be applied effectively to multimodal pipelines comprised of vision models, language models, and their connectors. We address how performance on captioning, retrieval, and question answering can be affected by bit width, quantization method, and which portion of the pipeline the quantization is used for. Results reveal that ViT and LLM exhibit comparable importance in model performance, despite significant differences in parameter size, and that lower-bit quantization of the LLM achieves high accuracy at reduced bits per weight (bpw). These findings provide practical insights for efficient deployment of MLLMs and highlight the value of exploration for understanding component sensitivities in multimodal models. Our code is available at https://github.com/gautomdas/mmq.", "link": "http://arxiv.org/abs/2601.15287v1", "date": "2026-01-21", "relevancy": 2.2397, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5662}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5662}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Understanding%20Best%20Practices%20for%20Quantization%20of%20Vision-Language%20Models&body=Title%3A%20Towards%20Understanding%20Best%20Practices%20for%20Quantization%20of%20Vision-Language%20Models%0AAuthor%3A%20Gautom%20Das%20and%20Vincent%20La%20and%20Ethan%20Lau%20and%20Abhinav%20Shrivastava%20and%20Matthew%20Gwilliam%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20deliver%20impressive%20results%20for%20a%20variety%20of%20tasks%2C%20but%20state-of-the-art%20systems%20require%20fast%20GPUs%20with%20large%20amounts%20of%20memory.%20To%20reduce%20both%20the%20memory%20and%20latency%20of%20these%20systems%2C%20practitioners%20quantize%20their%20learned%20parameters%2C%20typically%20at%20half%20precision.%20A%20growing%20body%20of%20research%20focuses%20on%20preserving%20the%20model%20performance%20with%20more%20aggressive%20bit%20widths%2C%20and%20some%20work%20has%20been%20done%20to%20apply%20these%20strategies%20to%20other%20models%2C%20like%20vision%20transformers.%20In%20our%20study%20we%20investigate%20how%20a%20variety%20of%20quantization%20methods%2C%20including%20state-of-the-art%20GPTQ%20and%20AWQ%2C%20can%20be%20applied%20effectively%20to%20multimodal%20pipelines%20comprised%20of%20vision%20models%2C%20language%20models%2C%20and%20their%20connectors.%20We%20address%20how%20performance%20on%20captioning%2C%20retrieval%2C%20and%20question%20answering%20can%20be%20affected%20by%20bit%20width%2C%20quantization%20method%2C%20and%20which%20portion%20of%20the%20pipeline%20the%20quantization%20is%20used%20for.%20Results%20reveal%20that%20ViT%20and%20LLM%20exhibit%20comparable%20importance%20in%20model%20performance%2C%20despite%20significant%20differences%20in%20parameter%20size%2C%20and%20that%20lower-bit%20quantization%20of%20the%20LLM%20achieves%20high%20accuracy%20at%20reduced%20bits%20per%20weight%20%28bpw%29.%20These%20findings%20provide%20practical%20insights%20for%20efficient%20deployment%20of%20MLLMs%20and%20highlight%20the%20value%20of%20exploration%20for%20understanding%20component%20sensitivities%20in%20multimodal%20models.%20Our%20code%20is%20available%20at%20https%3A//github.com/gautomdas/mmq.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15287v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Understanding%2520Best%2520Practices%2520for%2520Quantization%2520of%2520Vision-Language%2520Models%26entry.906535625%3DGautom%2520Das%2520and%2520Vincent%2520La%2520and%2520Ethan%2520Lau%2520and%2520Abhinav%2520Shrivastava%2520and%2520Matthew%2520Gwilliam%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520deliver%2520impressive%2520results%2520for%2520a%2520variety%2520of%2520tasks%252C%2520but%2520state-of-the-art%2520systems%2520require%2520fast%2520GPUs%2520with%2520large%2520amounts%2520of%2520memory.%2520To%2520reduce%2520both%2520the%2520memory%2520and%2520latency%2520of%2520these%2520systems%252C%2520practitioners%2520quantize%2520their%2520learned%2520parameters%252C%2520typically%2520at%2520half%2520precision.%2520A%2520growing%2520body%2520of%2520research%2520focuses%2520on%2520preserving%2520the%2520model%2520performance%2520with%2520more%2520aggressive%2520bit%2520widths%252C%2520and%2520some%2520work%2520has%2520been%2520done%2520to%2520apply%2520these%2520strategies%2520to%2520other%2520models%252C%2520like%2520vision%2520transformers.%2520In%2520our%2520study%2520we%2520investigate%2520how%2520a%2520variety%2520of%2520quantization%2520methods%252C%2520including%2520state-of-the-art%2520GPTQ%2520and%2520AWQ%252C%2520can%2520be%2520applied%2520effectively%2520to%2520multimodal%2520pipelines%2520comprised%2520of%2520vision%2520models%252C%2520language%2520models%252C%2520and%2520their%2520connectors.%2520We%2520address%2520how%2520performance%2520on%2520captioning%252C%2520retrieval%252C%2520and%2520question%2520answering%2520can%2520be%2520affected%2520by%2520bit%2520width%252C%2520quantization%2520method%252C%2520and%2520which%2520portion%2520of%2520the%2520pipeline%2520the%2520quantization%2520is%2520used%2520for.%2520Results%2520reveal%2520that%2520ViT%2520and%2520LLM%2520exhibit%2520comparable%2520importance%2520in%2520model%2520performance%252C%2520despite%2520significant%2520differences%2520in%2520parameter%2520size%252C%2520and%2520that%2520lower-bit%2520quantization%2520of%2520the%2520LLM%2520achieves%2520high%2520accuracy%2520at%2520reduced%2520bits%2520per%2520weight%2520%2528bpw%2529.%2520These%2520findings%2520provide%2520practical%2520insights%2520for%2520efficient%2520deployment%2520of%2520MLLMs%2520and%2520highlight%2520the%2520value%2520of%2520exploration%2520for%2520understanding%2520component%2520sensitivities%2520in%2520multimodal%2520models.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/gautomdas/mmq.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15287v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Understanding%20Best%20Practices%20for%20Quantization%20of%20Vision-Language%20Models&entry.906535625=Gautom%20Das%20and%20Vincent%20La%20and%20Ethan%20Lau%20and%20Abhinav%20Shrivastava%20and%20Matthew%20Gwilliam&entry.1292438233=Large%20language%20models%20%28LLMs%29%20deliver%20impressive%20results%20for%20a%20variety%20of%20tasks%2C%20but%20state-of-the-art%20systems%20require%20fast%20GPUs%20with%20large%20amounts%20of%20memory.%20To%20reduce%20both%20the%20memory%20and%20latency%20of%20these%20systems%2C%20practitioners%20quantize%20their%20learned%20parameters%2C%20typically%20at%20half%20precision.%20A%20growing%20body%20of%20research%20focuses%20on%20preserving%20the%20model%20performance%20with%20more%20aggressive%20bit%20widths%2C%20and%20some%20work%20has%20been%20done%20to%20apply%20these%20strategies%20to%20other%20models%2C%20like%20vision%20transformers.%20In%20our%20study%20we%20investigate%20how%20a%20variety%20of%20quantization%20methods%2C%20including%20state-of-the-art%20GPTQ%20and%20AWQ%2C%20can%20be%20applied%20effectively%20to%20multimodal%20pipelines%20comprised%20of%20vision%20models%2C%20language%20models%2C%20and%20their%20connectors.%20We%20address%20how%20performance%20on%20captioning%2C%20retrieval%2C%20and%20question%20answering%20can%20be%20affected%20by%20bit%20width%2C%20quantization%20method%2C%20and%20which%20portion%20of%20the%20pipeline%20the%20quantization%20is%20used%20for.%20Results%20reveal%20that%20ViT%20and%20LLM%20exhibit%20comparable%20importance%20in%20model%20performance%2C%20despite%20significant%20differences%20in%20parameter%20size%2C%20and%20that%20lower-bit%20quantization%20of%20the%20LLM%20achieves%20high%20accuracy%20at%20reduced%20bits%20per%20weight%20%28bpw%29.%20These%20findings%20provide%20practical%20insights%20for%20efficient%20deployment%20of%20MLLMs%20and%20highlight%20the%20value%20of%20exploration%20for%20understanding%20component%20sensitivities%20in%20multimodal%20models.%20Our%20code%20is%20available%20at%20https%3A//github.com/gautomdas/mmq.&entry.1838667208=http%3A//arxiv.org/abs/2601.15287v1&entry.124074799=Read"},
{"title": "LiViBench: An Omnimodal Benchmark for Interactive Livestream Video Understanding", "author": "Xiaodong Wang and Langling Huang and Zhirong Wu and Xu Zhao and Teng Xu and Xuhong Xia and Peixi Peng", "abstract": "The development of multimodal large language models (MLLMs) has advanced general video understanding. However, existing video evaluation benchmarks primarily focus on non-interactive videos, such as movies and recordings. To fill this gap, this paper proposes the first omnimodal benchmark for interactive livestream videos, LiViBench. It features a diverse set of 24 tasks, highlighting the perceptual, reasoning, and livestream-specific challenges. To efficiently construct the dataset, we design a standardized semi-automatic annotation workflow that incorporates the human-in-the-loop at multiple stages. The workflow leverages multiple MLLMs to form a multi-agent system for comprehensive video description and uses a seed-question-driven method to construct high-quality annotations. All interactive videos in the benchmark include audio, speech, and real-time comments modalities. To enhance models' understanding of interactive videos, we design tailored two-stage instruction-tuning and propose a Video-to-Comment Retrieval (VCR) module to improve the model's ability to utilize real-time comments. Based on these advancements, we develop LiVi-LLM-7B, an MLLM with enhanced knowledge of interactive livestreams. Experiments show that our model outperforms larger open-source models with up to 72B parameters, narrows the gap with leading proprietary models on LiViBench, and achieves enhanced performance on general video benchmarks, including VideoMME, LongVideoBench, MLVU, and VideoEval-Pro.", "link": "http://arxiv.org/abs/2601.15016v1", "date": "2026-01-21", "relevancy": 2.2346, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5628}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5628}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiViBench%3A%20An%20Omnimodal%20Benchmark%20for%20Interactive%20Livestream%20Video%20Understanding&body=Title%3A%20LiViBench%3A%20An%20Omnimodal%20Benchmark%20for%20Interactive%20Livestream%20Video%20Understanding%0AAuthor%3A%20Xiaodong%20Wang%20and%20Langling%20Huang%20and%20Zhirong%20Wu%20and%20Xu%20Zhao%20and%20Teng%20Xu%20and%20Xuhong%20Xia%20and%20Peixi%20Peng%0AAbstract%3A%20The%20development%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%20has%20advanced%20general%20video%20understanding.%20However%2C%20existing%20video%20evaluation%20benchmarks%20primarily%20focus%20on%20non-interactive%20videos%2C%20such%20as%20movies%20and%20recordings.%20To%20fill%20this%20gap%2C%20this%20paper%20proposes%20the%20first%20omnimodal%20benchmark%20for%20interactive%20livestream%20videos%2C%20LiViBench.%20It%20features%20a%20diverse%20set%20of%2024%20tasks%2C%20highlighting%20the%20perceptual%2C%20reasoning%2C%20and%20livestream-specific%20challenges.%20To%20efficiently%20construct%20the%20dataset%2C%20we%20design%20a%20standardized%20semi-automatic%20annotation%20workflow%20that%20incorporates%20the%20human-in-the-loop%20at%20multiple%20stages.%20The%20workflow%20leverages%20multiple%20MLLMs%20to%20form%20a%20multi-agent%20system%20for%20comprehensive%20video%20description%20and%20uses%20a%20seed-question-driven%20method%20to%20construct%20high-quality%20annotations.%20All%20interactive%20videos%20in%20the%20benchmark%20include%20audio%2C%20speech%2C%20and%20real-time%20comments%20modalities.%20To%20enhance%20models%27%20understanding%20of%20interactive%20videos%2C%20we%20design%20tailored%20two-stage%20instruction-tuning%20and%20propose%20a%20Video-to-Comment%20Retrieval%20%28VCR%29%20module%20to%20improve%20the%20model%27s%20ability%20to%20utilize%20real-time%20comments.%20Based%20on%20these%20advancements%2C%20we%20develop%20LiVi-LLM-7B%2C%20an%20MLLM%20with%20enhanced%20knowledge%20of%20interactive%20livestreams.%20Experiments%20show%20that%20our%20model%20outperforms%20larger%20open-source%20models%20with%20up%20to%2072B%20parameters%2C%20narrows%20the%20gap%20with%20leading%20proprietary%20models%20on%20LiViBench%2C%20and%20achieves%20enhanced%20performance%20on%20general%20video%20benchmarks%2C%20including%20VideoMME%2C%20LongVideoBench%2C%20MLVU%2C%20and%20VideoEval-Pro.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiViBench%253A%2520An%2520Omnimodal%2520Benchmark%2520for%2520Interactive%2520Livestream%2520Video%2520Understanding%26entry.906535625%3DXiaodong%2520Wang%2520and%2520Langling%2520Huang%2520and%2520Zhirong%2520Wu%2520and%2520Xu%2520Zhao%2520and%2520Teng%2520Xu%2520and%2520Xuhong%2520Xia%2520and%2520Peixi%2520Peng%26entry.1292438233%3DThe%2520development%2520of%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520has%2520advanced%2520general%2520video%2520understanding.%2520However%252C%2520existing%2520video%2520evaluation%2520benchmarks%2520primarily%2520focus%2520on%2520non-interactive%2520videos%252C%2520such%2520as%2520movies%2520and%2520recordings.%2520To%2520fill%2520this%2520gap%252C%2520this%2520paper%2520proposes%2520the%2520first%2520omnimodal%2520benchmark%2520for%2520interactive%2520livestream%2520videos%252C%2520LiViBench.%2520It%2520features%2520a%2520diverse%2520set%2520of%252024%2520tasks%252C%2520highlighting%2520the%2520perceptual%252C%2520reasoning%252C%2520and%2520livestream-specific%2520challenges.%2520To%2520efficiently%2520construct%2520the%2520dataset%252C%2520we%2520design%2520a%2520standardized%2520semi-automatic%2520annotation%2520workflow%2520that%2520incorporates%2520the%2520human-in-the-loop%2520at%2520multiple%2520stages.%2520The%2520workflow%2520leverages%2520multiple%2520MLLMs%2520to%2520form%2520a%2520multi-agent%2520system%2520for%2520comprehensive%2520video%2520description%2520and%2520uses%2520a%2520seed-question-driven%2520method%2520to%2520construct%2520high-quality%2520annotations.%2520All%2520interactive%2520videos%2520in%2520the%2520benchmark%2520include%2520audio%252C%2520speech%252C%2520and%2520real-time%2520comments%2520modalities.%2520To%2520enhance%2520models%2527%2520understanding%2520of%2520interactive%2520videos%252C%2520we%2520design%2520tailored%2520two-stage%2520instruction-tuning%2520and%2520propose%2520a%2520Video-to-Comment%2520Retrieval%2520%2528VCR%2529%2520module%2520to%2520improve%2520the%2520model%2527s%2520ability%2520to%2520utilize%2520real-time%2520comments.%2520Based%2520on%2520these%2520advancements%252C%2520we%2520develop%2520LiVi-LLM-7B%252C%2520an%2520MLLM%2520with%2520enhanced%2520knowledge%2520of%2520interactive%2520livestreams.%2520Experiments%2520show%2520that%2520our%2520model%2520outperforms%2520larger%2520open-source%2520models%2520with%2520up%2520to%252072B%2520parameters%252C%2520narrows%2520the%2520gap%2520with%2520leading%2520proprietary%2520models%2520on%2520LiViBench%252C%2520and%2520achieves%2520enhanced%2520performance%2520on%2520general%2520video%2520benchmarks%252C%2520including%2520VideoMME%252C%2520LongVideoBench%252C%2520MLVU%252C%2520and%2520VideoEval-Pro.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiViBench%3A%20An%20Omnimodal%20Benchmark%20for%20Interactive%20Livestream%20Video%20Understanding&entry.906535625=Xiaodong%20Wang%20and%20Langling%20Huang%20and%20Zhirong%20Wu%20and%20Xu%20Zhao%20and%20Teng%20Xu%20and%20Xuhong%20Xia%20and%20Peixi%20Peng&entry.1292438233=The%20development%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%20has%20advanced%20general%20video%20understanding.%20However%2C%20existing%20video%20evaluation%20benchmarks%20primarily%20focus%20on%20non-interactive%20videos%2C%20such%20as%20movies%20and%20recordings.%20To%20fill%20this%20gap%2C%20this%20paper%20proposes%20the%20first%20omnimodal%20benchmark%20for%20interactive%20livestream%20videos%2C%20LiViBench.%20It%20features%20a%20diverse%20set%20of%2024%20tasks%2C%20highlighting%20the%20perceptual%2C%20reasoning%2C%20and%20livestream-specific%20challenges.%20To%20efficiently%20construct%20the%20dataset%2C%20we%20design%20a%20standardized%20semi-automatic%20annotation%20workflow%20that%20incorporates%20the%20human-in-the-loop%20at%20multiple%20stages.%20The%20workflow%20leverages%20multiple%20MLLMs%20to%20form%20a%20multi-agent%20system%20for%20comprehensive%20video%20description%20and%20uses%20a%20seed-question-driven%20method%20to%20construct%20high-quality%20annotations.%20All%20interactive%20videos%20in%20the%20benchmark%20include%20audio%2C%20speech%2C%20and%20real-time%20comments%20modalities.%20To%20enhance%20models%27%20understanding%20of%20interactive%20videos%2C%20we%20design%20tailored%20two-stage%20instruction-tuning%20and%20propose%20a%20Video-to-Comment%20Retrieval%20%28VCR%29%20module%20to%20improve%20the%20model%27s%20ability%20to%20utilize%20real-time%20comments.%20Based%20on%20these%20advancements%2C%20we%20develop%20LiVi-LLM-7B%2C%20an%20MLLM%20with%20enhanced%20knowledge%20of%20interactive%20livestreams.%20Experiments%20show%20that%20our%20model%20outperforms%20larger%20open-source%20models%20with%20up%20to%2072B%20parameters%2C%20narrows%20the%20gap%20with%20leading%20proprietary%20models%20on%20LiViBench%2C%20and%20achieves%20enhanced%20performance%20on%20general%20video%20benchmarks%2C%20including%20VideoMME%2C%20LongVideoBench%2C%20MLVU%2C%20and%20VideoEval-Pro.&entry.1838667208=http%3A//arxiv.org/abs/2601.15016v1&entry.124074799=Read"},
{"title": "RayRoPE: Projective Ray Positional Encoding for Multi-view Attention", "author": "Yu Wu and Minsik Jeon and Jen-Hao Rick Chang and Oncel Tuzel and Shubham Tulsiani", "abstract": "We study positional encodings for multi-view transformers that process tokens from a set of posed input images, and seek a mechanism that encodes patches uniquely, allows SE(3)-invariant attention with multi-frequency similarity, and can be adaptive to the geometry of the underlying scene. We find that prior (absolute or relative) encoding schemes for multi-view attention do not meet the above desiderata, and present RayRoPE to address this gap. RayRoPE represents patch positions based on associated rays but leverages a predicted point along the ray instead of the direction for a geometry-aware encoding. To achieve SE(3) invariance, RayRoPE computes query-frame projective coordinates for computing multi-frequency similarity. Lastly, as the 'predicted' 3D point along a ray may not be precise, RayRoPE presents a mechanism to analytically compute the expected position encoding under uncertainty. We validate RayRoPE on the tasks of novel-view synthesis and stereo depth estimation and show that it consistently improves over alternate position encoding schemes (e.g. 15% relative improvement on LPIPS in CO3D). We also show that RayRoPE can seamlessly incorporate RGB-D input, resulting in even larger gains over alternatives that cannot positionally encode this information.", "link": "http://arxiv.org/abs/2601.15275v1", "date": "2026-01-21", "relevancy": 2.2279, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5571}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RayRoPE%3A%20Projective%20Ray%20Positional%20Encoding%20for%20Multi-view%20Attention&body=Title%3A%20RayRoPE%3A%20Projective%20Ray%20Positional%20Encoding%20for%20Multi-view%20Attention%0AAuthor%3A%20Yu%20Wu%20and%20Minsik%20Jeon%20and%20Jen-Hao%20Rick%20Chang%20and%20Oncel%20Tuzel%20and%20Shubham%20Tulsiani%0AAbstract%3A%20We%20study%20positional%20encodings%20for%20multi-view%20transformers%20that%20process%20tokens%20from%20a%20set%20of%20posed%20input%20images%2C%20and%20seek%20a%20mechanism%20that%20encodes%20patches%20uniquely%2C%20allows%20SE%283%29-invariant%20attention%20with%20multi-frequency%20similarity%2C%20and%20can%20be%20adaptive%20to%20the%20geometry%20of%20the%20underlying%20scene.%20We%20find%20that%20prior%20%28absolute%20or%20relative%29%20encoding%20schemes%20for%20multi-view%20attention%20do%20not%20meet%20the%20above%20desiderata%2C%20and%20present%20RayRoPE%20to%20address%20this%20gap.%20RayRoPE%20represents%20patch%20positions%20based%20on%20associated%20rays%20but%20leverages%20a%20predicted%20point%20along%20the%20ray%20instead%20of%20the%20direction%20for%20a%20geometry-aware%20encoding.%20To%20achieve%20SE%283%29%20invariance%2C%20RayRoPE%20computes%20query-frame%20projective%20coordinates%20for%20computing%20multi-frequency%20similarity.%20Lastly%2C%20as%20the%20%27predicted%27%203D%20point%20along%20a%20ray%20may%20not%20be%20precise%2C%20RayRoPE%20presents%20a%20mechanism%20to%20analytically%20compute%20the%20expected%20position%20encoding%20under%20uncertainty.%20We%20validate%20RayRoPE%20on%20the%20tasks%20of%20novel-view%20synthesis%20and%20stereo%20depth%20estimation%20and%20show%20that%20it%20consistently%20improves%20over%20alternate%20position%20encoding%20schemes%20%28e.g.%2015%25%20relative%20improvement%20on%20LPIPS%20in%20CO3D%29.%20We%20also%20show%20that%20RayRoPE%20can%20seamlessly%20incorporate%20RGB-D%20input%2C%20resulting%20in%20even%20larger%20gains%20over%20alternatives%20that%20cannot%20positionally%20encode%20this%20information.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15275v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRayRoPE%253A%2520Projective%2520Ray%2520Positional%2520Encoding%2520for%2520Multi-view%2520Attention%26entry.906535625%3DYu%2520Wu%2520and%2520Minsik%2520Jeon%2520and%2520Jen-Hao%2520Rick%2520Chang%2520and%2520Oncel%2520Tuzel%2520and%2520Shubham%2520Tulsiani%26entry.1292438233%3DWe%2520study%2520positional%2520encodings%2520for%2520multi-view%2520transformers%2520that%2520process%2520tokens%2520from%2520a%2520set%2520of%2520posed%2520input%2520images%252C%2520and%2520seek%2520a%2520mechanism%2520that%2520encodes%2520patches%2520uniquely%252C%2520allows%2520SE%25283%2529-invariant%2520attention%2520with%2520multi-frequency%2520similarity%252C%2520and%2520can%2520be%2520adaptive%2520to%2520the%2520geometry%2520of%2520the%2520underlying%2520scene.%2520We%2520find%2520that%2520prior%2520%2528absolute%2520or%2520relative%2529%2520encoding%2520schemes%2520for%2520multi-view%2520attention%2520do%2520not%2520meet%2520the%2520above%2520desiderata%252C%2520and%2520present%2520RayRoPE%2520to%2520address%2520this%2520gap.%2520RayRoPE%2520represents%2520patch%2520positions%2520based%2520on%2520associated%2520rays%2520but%2520leverages%2520a%2520predicted%2520point%2520along%2520the%2520ray%2520instead%2520of%2520the%2520direction%2520for%2520a%2520geometry-aware%2520encoding.%2520To%2520achieve%2520SE%25283%2529%2520invariance%252C%2520RayRoPE%2520computes%2520query-frame%2520projective%2520coordinates%2520for%2520computing%2520multi-frequency%2520similarity.%2520Lastly%252C%2520as%2520the%2520%2527predicted%2527%25203D%2520point%2520along%2520a%2520ray%2520may%2520not%2520be%2520precise%252C%2520RayRoPE%2520presents%2520a%2520mechanism%2520to%2520analytically%2520compute%2520the%2520expected%2520position%2520encoding%2520under%2520uncertainty.%2520We%2520validate%2520RayRoPE%2520on%2520the%2520tasks%2520of%2520novel-view%2520synthesis%2520and%2520stereo%2520depth%2520estimation%2520and%2520show%2520that%2520it%2520consistently%2520improves%2520over%2520alternate%2520position%2520encoding%2520schemes%2520%2528e.g.%252015%2525%2520relative%2520improvement%2520on%2520LPIPS%2520in%2520CO3D%2529.%2520We%2520also%2520show%2520that%2520RayRoPE%2520can%2520seamlessly%2520incorporate%2520RGB-D%2520input%252C%2520resulting%2520in%2520even%2520larger%2520gains%2520over%2520alternatives%2520that%2520cannot%2520positionally%2520encode%2520this%2520information.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15275v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RayRoPE%3A%20Projective%20Ray%20Positional%20Encoding%20for%20Multi-view%20Attention&entry.906535625=Yu%20Wu%20and%20Minsik%20Jeon%20and%20Jen-Hao%20Rick%20Chang%20and%20Oncel%20Tuzel%20and%20Shubham%20Tulsiani&entry.1292438233=We%20study%20positional%20encodings%20for%20multi-view%20transformers%20that%20process%20tokens%20from%20a%20set%20of%20posed%20input%20images%2C%20and%20seek%20a%20mechanism%20that%20encodes%20patches%20uniquely%2C%20allows%20SE%283%29-invariant%20attention%20with%20multi-frequency%20similarity%2C%20and%20can%20be%20adaptive%20to%20the%20geometry%20of%20the%20underlying%20scene.%20We%20find%20that%20prior%20%28absolute%20or%20relative%29%20encoding%20schemes%20for%20multi-view%20attention%20do%20not%20meet%20the%20above%20desiderata%2C%20and%20present%20RayRoPE%20to%20address%20this%20gap.%20RayRoPE%20represents%20patch%20positions%20based%20on%20associated%20rays%20but%20leverages%20a%20predicted%20point%20along%20the%20ray%20instead%20of%20the%20direction%20for%20a%20geometry-aware%20encoding.%20To%20achieve%20SE%283%29%20invariance%2C%20RayRoPE%20computes%20query-frame%20projective%20coordinates%20for%20computing%20multi-frequency%20similarity.%20Lastly%2C%20as%20the%20%27predicted%27%203D%20point%20along%20a%20ray%20may%20not%20be%20precise%2C%20RayRoPE%20presents%20a%20mechanism%20to%20analytically%20compute%20the%20expected%20position%20encoding%20under%20uncertainty.%20We%20validate%20RayRoPE%20on%20the%20tasks%20of%20novel-view%20synthesis%20and%20stereo%20depth%20estimation%20and%20show%20that%20it%20consistently%20improves%20over%20alternate%20position%20encoding%20schemes%20%28e.g.%2015%25%20relative%20improvement%20on%20LPIPS%20in%20CO3D%29.%20We%20also%20show%20that%20RayRoPE%20can%20seamlessly%20incorporate%20RGB-D%20input%2C%20resulting%20in%20even%20larger%20gains%20over%20alternatives%20that%20cannot%20positionally%20encode%20this%20information.&entry.1838667208=http%3A//arxiv.org/abs/2601.15275v1&entry.124074799=Read"},
{"title": "Difference Decomposition Networks for Infrared Small Target Detection", "author": "Chen Hu and Mingyu Zhou and Shuai Yuan and Hongbo Hu and Zhenming Peng and Tian Pu and Xiyin Li", "abstract": "Infrared small target detection (ISTD) faces two major challenges: a lack of discernible target texture and severe background clutter, which results in the background obscuring the target. To enhance targets and suppress backgrounds, we propose the Basis Decomposition Module (BDM) as an extensible and lightweight module based on basis decomposition, which decomposes a complex feature into several basis features and enhances certain information while eliminating redundancy. Extending BDM leads to a series of modules, including the Spatial Difference Decomposition Module (SD$^\\mathrm{2}$M), Spatial Difference Decomposition Downsampling Module (SD$^\\mathrm{3}$M), and Temporal Difference Decomposition Module (TD$^\\mathrm{2}$M). Based on these modules, we develop the Spatial Difference Decomposition Network (SD$^\\mathrm{2}$Net) for single-frame ISTD (SISTD) and the Spatiotemporal Difference Decomposition Network (STD$^\\mathrm{2}$Net) for multi-frame ISTD (MISTD). SD$^\\mathrm{2}$Net integrates SD$^\\mathrm{2}$M and SD$^\\mathrm{3}$M within an adapted U-shaped architecture. We employ TD$^\\mathrm{2}$M to introduce motion information, which transforms SD$^\\mathrm{2}$Net into STD$^\\mathrm{2}$Net. Extensive experiments on SISTD and MISTD datasets demonstrate state-of-the-art (SOTA) performance. On the SISTD task, SD$^\\mathrm{2}$Net performs well compared to most established networks. On the MISTD datasets, STD$^\\mathrm{2}$Net achieves a mIoU of 87.68\\%, outperforming SD$^\\mathrm{2}$Net, which achieves a mIoU of 64.97\\%. Our codes are available: https://github.com/greekinRoma/IRSTD_HC_Platform.", "link": "http://arxiv.org/abs/2512.03470v3", "date": "2026-01-21", "relevancy": 2.209, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.571}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5595}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Difference%20Decomposition%20Networks%20for%20Infrared%20Small%20Target%20Detection&body=Title%3A%20Difference%20Decomposition%20Networks%20for%20Infrared%20Small%20Target%20Detection%0AAuthor%3A%20Chen%20Hu%20and%20Mingyu%20Zhou%20and%20Shuai%20Yuan%20and%20Hongbo%20Hu%20and%20Zhenming%20Peng%20and%20Tian%20Pu%20and%20Xiyin%20Li%0AAbstract%3A%20Infrared%20small%20target%20detection%20%28ISTD%29%20faces%20two%20major%20challenges%3A%20a%20lack%20of%20discernible%20target%20texture%20and%20severe%20background%20clutter%2C%20which%20results%20in%20the%20background%20obscuring%20the%20target.%20To%20enhance%20targets%20and%20suppress%20backgrounds%2C%20we%20propose%20the%20Basis%20Decomposition%20Module%20%28BDM%29%20as%20an%20extensible%20and%20lightweight%20module%20based%20on%20basis%20decomposition%2C%20which%20decomposes%20a%20complex%20feature%20into%20several%20basis%20features%20and%20enhances%20certain%20information%20while%20eliminating%20redundancy.%20Extending%20BDM%20leads%20to%20a%20series%20of%20modules%2C%20including%20the%20Spatial%20Difference%20Decomposition%20Module%20%28SD%24%5E%5Cmathrm%7B2%7D%24M%29%2C%20Spatial%20Difference%20Decomposition%20Downsampling%20Module%20%28SD%24%5E%5Cmathrm%7B3%7D%24M%29%2C%20and%20Temporal%20Difference%20Decomposition%20Module%20%28TD%24%5E%5Cmathrm%7B2%7D%24M%29.%20Based%20on%20these%20modules%2C%20we%20develop%20the%20Spatial%20Difference%20Decomposition%20Network%20%28SD%24%5E%5Cmathrm%7B2%7D%24Net%29%20for%20single-frame%20ISTD%20%28SISTD%29%20and%20the%20Spatiotemporal%20Difference%20Decomposition%20Network%20%28STD%24%5E%5Cmathrm%7B2%7D%24Net%29%20for%20multi-frame%20ISTD%20%28MISTD%29.%20SD%24%5E%5Cmathrm%7B2%7D%24Net%20integrates%20SD%24%5E%5Cmathrm%7B2%7D%24M%20and%20SD%24%5E%5Cmathrm%7B3%7D%24M%20within%20an%20adapted%20U-shaped%20architecture.%20We%20employ%20TD%24%5E%5Cmathrm%7B2%7D%24M%20to%20introduce%20motion%20information%2C%20which%20transforms%20SD%24%5E%5Cmathrm%7B2%7D%24Net%20into%20STD%24%5E%5Cmathrm%7B2%7D%24Net.%20Extensive%20experiments%20on%20SISTD%20and%20MISTD%20datasets%20demonstrate%20state-of-the-art%20%28SOTA%29%20performance.%20On%20the%20SISTD%20task%2C%20SD%24%5E%5Cmathrm%7B2%7D%24Net%20performs%20well%20compared%20to%20most%20established%20networks.%20On%20the%20MISTD%20datasets%2C%20STD%24%5E%5Cmathrm%7B2%7D%24Net%20achieves%20a%20mIoU%20of%2087.68%5C%25%2C%20outperforming%20SD%24%5E%5Cmathrm%7B2%7D%24Net%2C%20which%20achieves%20a%20mIoU%20of%2064.97%5C%25.%20Our%20codes%20are%20available%3A%20https%3A//github.com/greekinRoma/IRSTD_HC_Platform.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03470v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifference%2520Decomposition%2520Networks%2520for%2520Infrared%2520Small%2520Target%2520Detection%26entry.906535625%3DChen%2520Hu%2520and%2520Mingyu%2520Zhou%2520and%2520Shuai%2520Yuan%2520and%2520Hongbo%2520Hu%2520and%2520Zhenming%2520Peng%2520and%2520Tian%2520Pu%2520and%2520Xiyin%2520Li%26entry.1292438233%3DInfrared%2520small%2520target%2520detection%2520%2528ISTD%2529%2520faces%2520two%2520major%2520challenges%253A%2520a%2520lack%2520of%2520discernible%2520target%2520texture%2520and%2520severe%2520background%2520clutter%252C%2520which%2520results%2520in%2520the%2520background%2520obscuring%2520the%2520target.%2520To%2520enhance%2520targets%2520and%2520suppress%2520backgrounds%252C%2520we%2520propose%2520the%2520Basis%2520Decomposition%2520Module%2520%2528BDM%2529%2520as%2520an%2520extensible%2520and%2520lightweight%2520module%2520based%2520on%2520basis%2520decomposition%252C%2520which%2520decomposes%2520a%2520complex%2520feature%2520into%2520several%2520basis%2520features%2520and%2520enhances%2520certain%2520information%2520while%2520eliminating%2520redundancy.%2520Extending%2520BDM%2520leads%2520to%2520a%2520series%2520of%2520modules%252C%2520including%2520the%2520Spatial%2520Difference%2520Decomposition%2520Module%2520%2528SD%2524%255E%255Cmathrm%257B2%257D%2524M%2529%252C%2520Spatial%2520Difference%2520Decomposition%2520Downsampling%2520Module%2520%2528SD%2524%255E%255Cmathrm%257B3%257D%2524M%2529%252C%2520and%2520Temporal%2520Difference%2520Decomposition%2520Module%2520%2528TD%2524%255E%255Cmathrm%257B2%257D%2524M%2529.%2520Based%2520on%2520these%2520modules%252C%2520we%2520develop%2520the%2520Spatial%2520Difference%2520Decomposition%2520Network%2520%2528SD%2524%255E%255Cmathrm%257B2%257D%2524Net%2529%2520for%2520single-frame%2520ISTD%2520%2528SISTD%2529%2520and%2520the%2520Spatiotemporal%2520Difference%2520Decomposition%2520Network%2520%2528STD%2524%255E%255Cmathrm%257B2%257D%2524Net%2529%2520for%2520multi-frame%2520ISTD%2520%2528MISTD%2529.%2520SD%2524%255E%255Cmathrm%257B2%257D%2524Net%2520integrates%2520SD%2524%255E%255Cmathrm%257B2%257D%2524M%2520and%2520SD%2524%255E%255Cmathrm%257B3%257D%2524M%2520within%2520an%2520adapted%2520U-shaped%2520architecture.%2520We%2520employ%2520TD%2524%255E%255Cmathrm%257B2%257D%2524M%2520to%2520introduce%2520motion%2520information%252C%2520which%2520transforms%2520SD%2524%255E%255Cmathrm%257B2%257D%2524Net%2520into%2520STD%2524%255E%255Cmathrm%257B2%257D%2524Net.%2520Extensive%2520experiments%2520on%2520SISTD%2520and%2520MISTD%2520datasets%2520demonstrate%2520state-of-the-art%2520%2528SOTA%2529%2520performance.%2520On%2520the%2520SISTD%2520task%252C%2520SD%2524%255E%255Cmathrm%257B2%257D%2524Net%2520performs%2520well%2520compared%2520to%2520most%2520established%2520networks.%2520On%2520the%2520MISTD%2520datasets%252C%2520STD%2524%255E%255Cmathrm%257B2%257D%2524Net%2520achieves%2520a%2520mIoU%2520of%252087.68%255C%2525%252C%2520outperforming%2520SD%2524%255E%255Cmathrm%257B2%257D%2524Net%252C%2520which%2520achieves%2520a%2520mIoU%2520of%252064.97%255C%2525.%2520Our%2520codes%2520are%2520available%253A%2520https%253A//github.com/greekinRoma/IRSTD_HC_Platform.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03470v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Difference%20Decomposition%20Networks%20for%20Infrared%20Small%20Target%20Detection&entry.906535625=Chen%20Hu%20and%20Mingyu%20Zhou%20and%20Shuai%20Yuan%20and%20Hongbo%20Hu%20and%20Zhenming%20Peng%20and%20Tian%20Pu%20and%20Xiyin%20Li&entry.1292438233=Infrared%20small%20target%20detection%20%28ISTD%29%20faces%20two%20major%20challenges%3A%20a%20lack%20of%20discernible%20target%20texture%20and%20severe%20background%20clutter%2C%20which%20results%20in%20the%20background%20obscuring%20the%20target.%20To%20enhance%20targets%20and%20suppress%20backgrounds%2C%20we%20propose%20the%20Basis%20Decomposition%20Module%20%28BDM%29%20as%20an%20extensible%20and%20lightweight%20module%20based%20on%20basis%20decomposition%2C%20which%20decomposes%20a%20complex%20feature%20into%20several%20basis%20features%20and%20enhances%20certain%20information%20while%20eliminating%20redundancy.%20Extending%20BDM%20leads%20to%20a%20series%20of%20modules%2C%20including%20the%20Spatial%20Difference%20Decomposition%20Module%20%28SD%24%5E%5Cmathrm%7B2%7D%24M%29%2C%20Spatial%20Difference%20Decomposition%20Downsampling%20Module%20%28SD%24%5E%5Cmathrm%7B3%7D%24M%29%2C%20and%20Temporal%20Difference%20Decomposition%20Module%20%28TD%24%5E%5Cmathrm%7B2%7D%24M%29.%20Based%20on%20these%20modules%2C%20we%20develop%20the%20Spatial%20Difference%20Decomposition%20Network%20%28SD%24%5E%5Cmathrm%7B2%7D%24Net%29%20for%20single-frame%20ISTD%20%28SISTD%29%20and%20the%20Spatiotemporal%20Difference%20Decomposition%20Network%20%28STD%24%5E%5Cmathrm%7B2%7D%24Net%29%20for%20multi-frame%20ISTD%20%28MISTD%29.%20SD%24%5E%5Cmathrm%7B2%7D%24Net%20integrates%20SD%24%5E%5Cmathrm%7B2%7D%24M%20and%20SD%24%5E%5Cmathrm%7B3%7D%24M%20within%20an%20adapted%20U-shaped%20architecture.%20We%20employ%20TD%24%5E%5Cmathrm%7B2%7D%24M%20to%20introduce%20motion%20information%2C%20which%20transforms%20SD%24%5E%5Cmathrm%7B2%7D%24Net%20into%20STD%24%5E%5Cmathrm%7B2%7D%24Net.%20Extensive%20experiments%20on%20SISTD%20and%20MISTD%20datasets%20demonstrate%20state-of-the-art%20%28SOTA%29%20performance.%20On%20the%20SISTD%20task%2C%20SD%24%5E%5Cmathrm%7B2%7D%24Net%20performs%20well%20compared%20to%20most%20established%20networks.%20On%20the%20MISTD%20datasets%2C%20STD%24%5E%5Cmathrm%7B2%7D%24Net%20achieves%20a%20mIoU%20of%2087.68%5C%25%2C%20outperforming%20SD%24%5E%5Cmathrm%7B2%7D%24Net%2C%20which%20achieves%20a%20mIoU%20of%2064.97%5C%25.%20Our%20codes%20are%20available%3A%20https%3A//github.com/greekinRoma/IRSTD_HC_Platform.&entry.1838667208=http%3A//arxiv.org/abs/2512.03470v3&entry.124074799=Read"},
{"title": "SpatialV2A: Visual-Guided High-fidelity Spatial Audio Generation", "author": "Yanan Wang and Linjie Ren and Zihao Li and Junyi Wang and Tian Gan", "abstract": "While video-to-audio generation has achieved remarkable progress in semantic and temporal alignment, most existing studies focus solely on these aspects, paying limited attention to the spatial perception and immersive quality of the synthesized audio. This limitation stems largely from current models' reliance on mono audio datasets, which lack the binaural spatial information needed to learn visual-to-spatial audio mappings. To address this gap, we introduce two key contributions: we construct BinauralVGGSound, the first large-scale video-binaural audio dataset designed to support spatially aware video-to-audio generation; and we propose a end-to-end spatial audio generation framework guided by visual cues, which explicitly models spatial features. Our framework incorporates a visual-guided audio spatialization module that ensures the generated audio exhibits realistic spatial attributes and layered spatial depth while maintaining semantic and temporal alignment. Experiments show that our approach substantially outperforms state-of-the-art models in spatial fidelity and delivers a more immersive auditory experience, without sacrificing temporal or semantic consistency. All datasets, code, and model checkpoints will be publicly released to facilitate future research.", "link": "http://arxiv.org/abs/2601.15017v1", "date": "2026-01-21", "relevancy": 2.2081, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5718}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5503}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpatialV2A%3A%20Visual-Guided%20High-fidelity%20Spatial%20Audio%20Generation&body=Title%3A%20SpatialV2A%3A%20Visual-Guided%20High-fidelity%20Spatial%20Audio%20Generation%0AAuthor%3A%20Yanan%20Wang%20and%20Linjie%20Ren%20and%20Zihao%20Li%20and%20Junyi%20Wang%20and%20Tian%20Gan%0AAbstract%3A%20While%20video-to-audio%20generation%20has%20achieved%20remarkable%20progress%20in%20semantic%20and%20temporal%20alignment%2C%20most%20existing%20studies%20focus%20solely%20on%20these%20aspects%2C%20paying%20limited%20attention%20to%20the%20spatial%20perception%20and%20immersive%20quality%20of%20the%20synthesized%20audio.%20This%20limitation%20stems%20largely%20from%20current%20models%27%20reliance%20on%20mono%20audio%20datasets%2C%20which%20lack%20the%20binaural%20spatial%20information%20needed%20to%20learn%20visual-to-spatial%20audio%20mappings.%20To%20address%20this%20gap%2C%20we%20introduce%20two%20key%20contributions%3A%20we%20construct%20BinauralVGGSound%2C%20the%20first%20large-scale%20video-binaural%20audio%20dataset%20designed%20to%20support%20spatially%20aware%20video-to-audio%20generation%3B%20and%20we%20propose%20a%20end-to-end%20spatial%20audio%20generation%20framework%20guided%20by%20visual%20cues%2C%20which%20explicitly%20models%20spatial%20features.%20Our%20framework%20incorporates%20a%20visual-guided%20audio%20spatialization%20module%20that%20ensures%20the%20generated%20audio%20exhibits%20realistic%20spatial%20attributes%20and%20layered%20spatial%20depth%20while%20maintaining%20semantic%20and%20temporal%20alignment.%20Experiments%20show%20that%20our%20approach%20substantially%20outperforms%20state-of-the-art%20models%20in%20spatial%20fidelity%20and%20delivers%20a%20more%20immersive%20auditory%20experience%2C%20without%20sacrificing%20temporal%20or%20semantic%20consistency.%20All%20datasets%2C%20code%2C%20and%20model%20checkpoints%20will%20be%20publicly%20released%20to%20facilitate%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15017v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatialV2A%253A%2520Visual-Guided%2520High-fidelity%2520Spatial%2520Audio%2520Generation%26entry.906535625%3DYanan%2520Wang%2520and%2520Linjie%2520Ren%2520and%2520Zihao%2520Li%2520and%2520Junyi%2520Wang%2520and%2520Tian%2520Gan%26entry.1292438233%3DWhile%2520video-to-audio%2520generation%2520has%2520achieved%2520remarkable%2520progress%2520in%2520semantic%2520and%2520temporal%2520alignment%252C%2520most%2520existing%2520studies%2520focus%2520solely%2520on%2520these%2520aspects%252C%2520paying%2520limited%2520attention%2520to%2520the%2520spatial%2520perception%2520and%2520immersive%2520quality%2520of%2520the%2520synthesized%2520audio.%2520This%2520limitation%2520stems%2520largely%2520from%2520current%2520models%2527%2520reliance%2520on%2520mono%2520audio%2520datasets%252C%2520which%2520lack%2520the%2520binaural%2520spatial%2520information%2520needed%2520to%2520learn%2520visual-to-spatial%2520audio%2520mappings.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520two%2520key%2520contributions%253A%2520we%2520construct%2520BinauralVGGSound%252C%2520the%2520first%2520large-scale%2520video-binaural%2520audio%2520dataset%2520designed%2520to%2520support%2520spatially%2520aware%2520video-to-audio%2520generation%253B%2520and%2520we%2520propose%2520a%2520end-to-end%2520spatial%2520audio%2520generation%2520framework%2520guided%2520by%2520visual%2520cues%252C%2520which%2520explicitly%2520models%2520spatial%2520features.%2520Our%2520framework%2520incorporates%2520a%2520visual-guided%2520audio%2520spatialization%2520module%2520that%2520ensures%2520the%2520generated%2520audio%2520exhibits%2520realistic%2520spatial%2520attributes%2520and%2520layered%2520spatial%2520depth%2520while%2520maintaining%2520semantic%2520and%2520temporal%2520alignment.%2520Experiments%2520show%2520that%2520our%2520approach%2520substantially%2520outperforms%2520state-of-the-art%2520models%2520in%2520spatial%2520fidelity%2520and%2520delivers%2520a%2520more%2520immersive%2520auditory%2520experience%252C%2520without%2520sacrificing%2520temporal%2520or%2520semantic%2520consistency.%2520All%2520datasets%252C%2520code%252C%2520and%2520model%2520checkpoints%2520will%2520be%2520publicly%2520released%2520to%2520facilitate%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15017v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpatialV2A%3A%20Visual-Guided%20High-fidelity%20Spatial%20Audio%20Generation&entry.906535625=Yanan%20Wang%20and%20Linjie%20Ren%20and%20Zihao%20Li%20and%20Junyi%20Wang%20and%20Tian%20Gan&entry.1292438233=While%20video-to-audio%20generation%20has%20achieved%20remarkable%20progress%20in%20semantic%20and%20temporal%20alignment%2C%20most%20existing%20studies%20focus%20solely%20on%20these%20aspects%2C%20paying%20limited%20attention%20to%20the%20spatial%20perception%20and%20immersive%20quality%20of%20the%20synthesized%20audio.%20This%20limitation%20stems%20largely%20from%20current%20models%27%20reliance%20on%20mono%20audio%20datasets%2C%20which%20lack%20the%20binaural%20spatial%20information%20needed%20to%20learn%20visual-to-spatial%20audio%20mappings.%20To%20address%20this%20gap%2C%20we%20introduce%20two%20key%20contributions%3A%20we%20construct%20BinauralVGGSound%2C%20the%20first%20large-scale%20video-binaural%20audio%20dataset%20designed%20to%20support%20spatially%20aware%20video-to-audio%20generation%3B%20and%20we%20propose%20a%20end-to-end%20spatial%20audio%20generation%20framework%20guided%20by%20visual%20cues%2C%20which%20explicitly%20models%20spatial%20features.%20Our%20framework%20incorporates%20a%20visual-guided%20audio%20spatialization%20module%20that%20ensures%20the%20generated%20audio%20exhibits%20realistic%20spatial%20attributes%20and%20layered%20spatial%20depth%20while%20maintaining%20semantic%20and%20temporal%20alignment.%20Experiments%20show%20that%20our%20approach%20substantially%20outperforms%20state-of-the-art%20models%20in%20spatial%20fidelity%20and%20delivers%20a%20more%20immersive%20auditory%20experience%2C%20without%20sacrificing%20temporal%20or%20semantic%20consistency.%20All%20datasets%2C%20code%2C%20and%20model%20checkpoints%20will%20be%20publicly%20released%20to%20facilitate%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2601.15017v1&entry.124074799=Read"},
{"title": "GuideTouch: An Obstacle Avoidance Device with Tactile Feedback for Visually Impaired", "author": "Timofei Kozlov and Artem Trandofilov and Georgii Gazaryan and Issatay Tokmurziyev and Miguel Altamirano Cabrera and Dzmitry Tsetserukou", "abstract": "Safe navigation for the visually impaired individuals remains a critical challenge, especially concerning head-level obstacles, which traditional mobility aids often fail to detect. We introduce GuideTouch, a compact, affordable, standalone wearable device designed for autonomous obstacle avoidance. The system integrates two vertically aligned Time-of-Flight (ToF) sensors, enabling three-dimensional environmental perception, and four vibrotactile actuators that provide directional haptic feedback. Proximity and direction information is communicated via an intuitive 4-point vibrotactile feedback system located across the user's shoulders and upper chest. For real-world robustness, the device includes a unique centrifugal self-cleaning optical cover mechanism and a sound alarm system for location if the device is dropped. We evaluated the haptic perception accuracy across 22 participants (17 male and 5 female, aged 21-48, mean 25.7, sd 6.1). Statistical analysis confirmed a significant difference between the perception accuracy of different patterns. The system demonstrated high recognition accuracy, achieving an average of 92.9% for single and double motor (primary directional) patterns. Furthermore, preliminary experiments with 14 visually impaired users validated this interface, showing a recognition accuracy of 93.75% for primary directional cues. The results demonstrate that GuideTouch enables intuitive spatial perception and could significantly improve the safety, confidence, and autonomy of users with visual impairments during independent navigation.", "link": "http://arxiv.org/abs/2601.13813v2", "date": "2026-01-21", "relevancy": 2.2065, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6523}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5168}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GuideTouch%3A%20An%20Obstacle%20Avoidance%20Device%20with%20Tactile%20Feedback%20for%20Visually%20Impaired&body=Title%3A%20GuideTouch%3A%20An%20Obstacle%20Avoidance%20Device%20with%20Tactile%20Feedback%20for%20Visually%20Impaired%0AAuthor%3A%20Timofei%20Kozlov%20and%20Artem%20Trandofilov%20and%20Georgii%20Gazaryan%20and%20Issatay%20Tokmurziyev%20and%20Miguel%20Altamirano%20Cabrera%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20Safe%20navigation%20for%20the%20visually%20impaired%20individuals%20remains%20a%20critical%20challenge%2C%20especially%20concerning%20head-level%20obstacles%2C%20which%20traditional%20mobility%20aids%20often%20fail%20to%20detect.%20We%20introduce%20GuideTouch%2C%20a%20compact%2C%20affordable%2C%20standalone%20wearable%20device%20designed%20for%20autonomous%20obstacle%20avoidance.%20The%20system%20integrates%20two%20vertically%20aligned%20Time-of-Flight%20%28ToF%29%20sensors%2C%20enabling%20three-dimensional%20environmental%20perception%2C%20and%20four%20vibrotactile%20actuators%20that%20provide%20directional%20haptic%20feedback.%20Proximity%20and%20direction%20information%20is%20communicated%20via%20an%20intuitive%204-point%20vibrotactile%20feedback%20system%20located%20across%20the%20user%27s%20shoulders%20and%20upper%20chest.%20For%20real-world%20robustness%2C%20the%20device%20includes%20a%20unique%20centrifugal%20self-cleaning%20optical%20cover%20mechanism%20and%20a%20sound%20alarm%20system%20for%20location%20if%20the%20device%20is%20dropped.%20We%20evaluated%20the%20haptic%20perception%20accuracy%20across%2022%20participants%20%2817%20male%20and%205%20female%2C%20aged%2021-48%2C%20mean%2025.7%2C%20sd%206.1%29.%20Statistical%20analysis%20confirmed%20a%20significant%20difference%20between%20the%20perception%20accuracy%20of%20different%20patterns.%20The%20system%20demonstrated%20high%20recognition%20accuracy%2C%20achieving%20an%20average%20of%2092.9%25%20for%20single%20and%20double%20motor%20%28primary%20directional%29%20patterns.%20Furthermore%2C%20preliminary%20experiments%20with%2014%20visually%20impaired%20users%20validated%20this%20interface%2C%20showing%20a%20recognition%20accuracy%20of%2093.75%25%20for%20primary%20directional%20cues.%20The%20results%20demonstrate%20that%20GuideTouch%20enables%20intuitive%20spatial%20perception%20and%20could%20significantly%20improve%20the%20safety%2C%20confidence%2C%20and%20autonomy%20of%20users%20with%20visual%20impairments%20during%20independent%20navigation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13813v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuideTouch%253A%2520An%2520Obstacle%2520Avoidance%2520Device%2520with%2520Tactile%2520Feedback%2520for%2520Visually%2520Impaired%26entry.906535625%3DTimofei%2520Kozlov%2520and%2520Artem%2520Trandofilov%2520and%2520Georgii%2520Gazaryan%2520and%2520Issatay%2520Tokmurziyev%2520and%2520Miguel%2520Altamirano%2520Cabrera%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3DSafe%2520navigation%2520for%2520the%2520visually%2520impaired%2520individuals%2520remains%2520a%2520critical%2520challenge%252C%2520especially%2520concerning%2520head-level%2520obstacles%252C%2520which%2520traditional%2520mobility%2520aids%2520often%2520fail%2520to%2520detect.%2520We%2520introduce%2520GuideTouch%252C%2520a%2520compact%252C%2520affordable%252C%2520standalone%2520wearable%2520device%2520designed%2520for%2520autonomous%2520obstacle%2520avoidance.%2520The%2520system%2520integrates%2520two%2520vertically%2520aligned%2520Time-of-Flight%2520%2528ToF%2529%2520sensors%252C%2520enabling%2520three-dimensional%2520environmental%2520perception%252C%2520and%2520four%2520vibrotactile%2520actuators%2520that%2520provide%2520directional%2520haptic%2520feedback.%2520Proximity%2520and%2520direction%2520information%2520is%2520communicated%2520via%2520an%2520intuitive%25204-point%2520vibrotactile%2520feedback%2520system%2520located%2520across%2520the%2520user%2527s%2520shoulders%2520and%2520upper%2520chest.%2520For%2520real-world%2520robustness%252C%2520the%2520device%2520includes%2520a%2520unique%2520centrifugal%2520self-cleaning%2520optical%2520cover%2520mechanism%2520and%2520a%2520sound%2520alarm%2520system%2520for%2520location%2520if%2520the%2520device%2520is%2520dropped.%2520We%2520evaluated%2520the%2520haptic%2520perception%2520accuracy%2520across%252022%2520participants%2520%252817%2520male%2520and%25205%2520female%252C%2520aged%252021-48%252C%2520mean%252025.7%252C%2520sd%25206.1%2529.%2520Statistical%2520analysis%2520confirmed%2520a%2520significant%2520difference%2520between%2520the%2520perception%2520accuracy%2520of%2520different%2520patterns.%2520The%2520system%2520demonstrated%2520high%2520recognition%2520accuracy%252C%2520achieving%2520an%2520average%2520of%252092.9%2525%2520for%2520single%2520and%2520double%2520motor%2520%2528primary%2520directional%2529%2520patterns.%2520Furthermore%252C%2520preliminary%2520experiments%2520with%252014%2520visually%2520impaired%2520users%2520validated%2520this%2520interface%252C%2520showing%2520a%2520recognition%2520accuracy%2520of%252093.75%2525%2520for%2520primary%2520directional%2520cues.%2520The%2520results%2520demonstrate%2520that%2520GuideTouch%2520enables%2520intuitive%2520spatial%2520perception%2520and%2520could%2520significantly%2520improve%2520the%2520safety%252C%2520confidence%252C%2520and%2520autonomy%2520of%2520users%2520with%2520visual%2520impairments%2520during%2520independent%2520navigation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13813v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GuideTouch%3A%20An%20Obstacle%20Avoidance%20Device%20with%20Tactile%20Feedback%20for%20Visually%20Impaired&entry.906535625=Timofei%20Kozlov%20and%20Artem%20Trandofilov%20and%20Georgii%20Gazaryan%20and%20Issatay%20Tokmurziyev%20and%20Miguel%20Altamirano%20Cabrera%20and%20Dzmitry%20Tsetserukou&entry.1292438233=Safe%20navigation%20for%20the%20visually%20impaired%20individuals%20remains%20a%20critical%20challenge%2C%20especially%20concerning%20head-level%20obstacles%2C%20which%20traditional%20mobility%20aids%20often%20fail%20to%20detect.%20We%20introduce%20GuideTouch%2C%20a%20compact%2C%20affordable%2C%20standalone%20wearable%20device%20designed%20for%20autonomous%20obstacle%20avoidance.%20The%20system%20integrates%20two%20vertically%20aligned%20Time-of-Flight%20%28ToF%29%20sensors%2C%20enabling%20three-dimensional%20environmental%20perception%2C%20and%20four%20vibrotactile%20actuators%20that%20provide%20directional%20haptic%20feedback.%20Proximity%20and%20direction%20information%20is%20communicated%20via%20an%20intuitive%204-point%20vibrotactile%20feedback%20system%20located%20across%20the%20user%27s%20shoulders%20and%20upper%20chest.%20For%20real-world%20robustness%2C%20the%20device%20includes%20a%20unique%20centrifugal%20self-cleaning%20optical%20cover%20mechanism%20and%20a%20sound%20alarm%20system%20for%20location%20if%20the%20device%20is%20dropped.%20We%20evaluated%20the%20haptic%20perception%20accuracy%20across%2022%20participants%20%2817%20male%20and%205%20female%2C%20aged%2021-48%2C%20mean%2025.7%2C%20sd%206.1%29.%20Statistical%20analysis%20confirmed%20a%20significant%20difference%20between%20the%20perception%20accuracy%20of%20different%20patterns.%20The%20system%20demonstrated%20high%20recognition%20accuracy%2C%20achieving%20an%20average%20of%2092.9%25%20for%20single%20and%20double%20motor%20%28primary%20directional%29%20patterns.%20Furthermore%2C%20preliminary%20experiments%20with%2014%20visually%20impaired%20users%20validated%20this%20interface%2C%20showing%20a%20recognition%20accuracy%20of%2093.75%25%20for%20primary%20directional%20cues.%20The%20results%20demonstrate%20that%20GuideTouch%20enables%20intuitive%20spatial%20perception%20and%20could%20significantly%20improve%20the%20safety%2C%20confidence%2C%20and%20autonomy%20of%20users%20with%20visual%20impairments%20during%20independent%20navigation.&entry.1838667208=http%3A//arxiv.org/abs/2601.13813v2&entry.124074799=Read"},
{"title": "Vision-Language Models on the Edge for Real-Time Robotic Perception", "author": "Sarat Ahmad and Maryam Hafeez and Syed Ali Raza Zaidi", "abstract": "Vision-Language Models (VLMs) enable multimodal reasoning for robotic perception and interaction, but their deployment in real-world systems remains constrained by latency, limited onboard resources, and privacy risks of cloud offloading. Edge intelligence within 6G, particularly Open RAN and Multi-access Edge Computing (MEC), offers a pathway to address these challenges by bringing computation closer to the data source. This work investigates the deployment of VLMs on ORAN/MEC infrastructure using the Unitree G1 humanoid robot as an embodied testbed. We design a WebRTC-based pipeline that streams multimodal data to an edge node and evaluate LLaMA-3.2-11B-Vision-Instruct deployed at the edge versus in the cloud under real-time conditions. Our results show that edge deployment preserves near-cloud accuracy while reducing end-to-end latency by 5\\%. We further evaluate Qwen2-VL-2B-Instruct, a compact model optimized for resource-constrained environments, which achieves sub-second responsiveness, cutting latency by more than half but at the cost of accuracy.", "link": "http://arxiv.org/abs/2601.14921v1", "date": "2026-01-21", "relevancy": 2.2025, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5675}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5473}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Language%20Models%20on%20the%20Edge%20for%20Real-Time%20Robotic%20Perception&body=Title%3A%20Vision-Language%20Models%20on%20the%20Edge%20for%20Real-Time%20Robotic%20Perception%0AAuthor%3A%20Sarat%20Ahmad%20and%20Maryam%20Hafeez%20and%20Syed%20Ali%20Raza%20Zaidi%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20enable%20multimodal%20reasoning%20for%20robotic%20perception%20and%20interaction%2C%20but%20their%20deployment%20in%20real-world%20systems%20remains%20constrained%20by%20latency%2C%20limited%20onboard%20resources%2C%20and%20privacy%20risks%20of%20cloud%20offloading.%20Edge%20intelligence%20within%206G%2C%20particularly%20Open%20RAN%20and%20Multi-access%20Edge%20Computing%20%28MEC%29%2C%20offers%20a%20pathway%20to%20address%20these%20challenges%20by%20bringing%20computation%20closer%20to%20the%20data%20source.%20This%20work%20investigates%20the%20deployment%20of%20VLMs%20on%20ORAN/MEC%20infrastructure%20using%20the%20Unitree%20G1%20humanoid%20robot%20as%20an%20embodied%20testbed.%20We%20design%20a%20WebRTC-based%20pipeline%20that%20streams%20multimodal%20data%20to%20an%20edge%20node%20and%20evaluate%20LLaMA-3.2-11B-Vision-Instruct%20deployed%20at%20the%20edge%20versus%20in%20the%20cloud%20under%20real-time%20conditions.%20Our%20results%20show%20that%20edge%20deployment%20preserves%20near-cloud%20accuracy%20while%20reducing%20end-to-end%20latency%20by%205%5C%25.%20We%20further%20evaluate%20Qwen2-VL-2B-Instruct%2C%20a%20compact%20model%20optimized%20for%20resource-constrained%20environments%2C%20which%20achieves%20sub-second%20responsiveness%2C%20cutting%20latency%20by%20more%20than%20half%20but%20at%20the%20cost%20of%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Language%2520Models%2520on%2520the%2520Edge%2520for%2520Real-Time%2520Robotic%2520Perception%26entry.906535625%3DSarat%2520Ahmad%2520and%2520Maryam%2520Hafeez%2520and%2520Syed%2520Ali%2520Raza%2520Zaidi%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520enable%2520multimodal%2520reasoning%2520for%2520robotic%2520perception%2520and%2520interaction%252C%2520but%2520their%2520deployment%2520in%2520real-world%2520systems%2520remains%2520constrained%2520by%2520latency%252C%2520limited%2520onboard%2520resources%252C%2520and%2520privacy%2520risks%2520of%2520cloud%2520offloading.%2520Edge%2520intelligence%2520within%25206G%252C%2520particularly%2520Open%2520RAN%2520and%2520Multi-access%2520Edge%2520Computing%2520%2528MEC%2529%252C%2520offers%2520a%2520pathway%2520to%2520address%2520these%2520challenges%2520by%2520bringing%2520computation%2520closer%2520to%2520the%2520data%2520source.%2520This%2520work%2520investigates%2520the%2520deployment%2520of%2520VLMs%2520on%2520ORAN/MEC%2520infrastructure%2520using%2520the%2520Unitree%2520G1%2520humanoid%2520robot%2520as%2520an%2520embodied%2520testbed.%2520We%2520design%2520a%2520WebRTC-based%2520pipeline%2520that%2520streams%2520multimodal%2520data%2520to%2520an%2520edge%2520node%2520and%2520evaluate%2520LLaMA-3.2-11B-Vision-Instruct%2520deployed%2520at%2520the%2520edge%2520versus%2520in%2520the%2520cloud%2520under%2520real-time%2520conditions.%2520Our%2520results%2520show%2520that%2520edge%2520deployment%2520preserves%2520near-cloud%2520accuracy%2520while%2520reducing%2520end-to-end%2520latency%2520by%25205%255C%2525.%2520We%2520further%2520evaluate%2520Qwen2-VL-2B-Instruct%252C%2520a%2520compact%2520model%2520optimized%2520for%2520resource-constrained%2520environments%252C%2520which%2520achieves%2520sub-second%2520responsiveness%252C%2520cutting%2520latency%2520by%2520more%2520than%2520half%2520but%2520at%2520the%2520cost%2520of%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Language%20Models%20on%20the%20Edge%20for%20Real-Time%20Robotic%20Perception&entry.906535625=Sarat%20Ahmad%20and%20Maryam%20Hafeez%20and%20Syed%20Ali%20Raza%20Zaidi&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20enable%20multimodal%20reasoning%20for%20robotic%20perception%20and%20interaction%2C%20but%20their%20deployment%20in%20real-world%20systems%20remains%20constrained%20by%20latency%2C%20limited%20onboard%20resources%2C%20and%20privacy%20risks%20of%20cloud%20offloading.%20Edge%20intelligence%20within%206G%2C%20particularly%20Open%20RAN%20and%20Multi-access%20Edge%20Computing%20%28MEC%29%2C%20offers%20a%20pathway%20to%20address%20these%20challenges%20by%20bringing%20computation%20closer%20to%20the%20data%20source.%20This%20work%20investigates%20the%20deployment%20of%20VLMs%20on%20ORAN/MEC%20infrastructure%20using%20the%20Unitree%20G1%20humanoid%20robot%20as%20an%20embodied%20testbed.%20We%20design%20a%20WebRTC-based%20pipeline%20that%20streams%20multimodal%20data%20to%20an%20edge%20node%20and%20evaluate%20LLaMA-3.2-11B-Vision-Instruct%20deployed%20at%20the%20edge%20versus%20in%20the%20cloud%20under%20real-time%20conditions.%20Our%20results%20show%20that%20edge%20deployment%20preserves%20near-cloud%20accuracy%20while%20reducing%20end-to-end%20latency%20by%205%5C%25.%20We%20further%20evaluate%20Qwen2-VL-2B-Instruct%2C%20a%20compact%20model%20optimized%20for%20resource-constrained%20environments%2C%20which%20achieves%20sub-second%20responsiveness%2C%20cutting%20latency%20by%20more%20than%20half%20but%20at%20the%20cost%20of%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2601.14921v1&entry.124074799=Read"},
{"title": "Overcoming In-Memory Bottlenecks in Graph Foundation Models via Retrieval-Augmented Generation", "author": "Haonan Yuan and Qingyun Sun and Jiacheng Tao and Xingcheng Fu and Jianxin Li", "abstract": "Graph Foundation Models (GFMs) have emerged as a frontier in graph learning, which are expected to deliver transferable representations across diverse tasks. However, GFMs remain constrained by in-memory bottlenecks: they attempt to encode knowledge into model parameters, which limits semantic capacity, introduces heavy lossy compression with conflicts, and entangles graph representation with the knowledge in ways that hinder efficient adaptation, undermining scalability and interpretability. In this work,we propose RAG-GFM, a Retrieval-Augmented Generation aided Graph Foundation Model that offloads knowledge from parameters and complements parameterized learning. To externalize graph knowledge, we build a dual-modal unified retrieval module, where a semantic store from prefix-structured text and a structural store from centrality-based motif. To preserve heterogeneous information, we design a dual-view alignment objective that contrasts both modalities to capture both content and relational patterns. To enable efficient downstream adaptation, we perform in-context augmentation to enrich supporting instances with retrieved texts and motifs as contextual evidence. Extensive experiments on five benchmark graph datasets demonstrate that RAG-GFM consistently outperforms 13 state-of-the-art baselines in both cross-domain node and graph classification, achieving superior effectiveness and efficiency.", "link": "http://arxiv.org/abs/2601.15124v1", "date": "2026-01-21", "relevancy": 2.185, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5763}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.536}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overcoming%20In-Memory%20Bottlenecks%20in%20Graph%20Foundation%20Models%20via%20Retrieval-Augmented%20Generation&body=Title%3A%20Overcoming%20In-Memory%20Bottlenecks%20in%20Graph%20Foundation%20Models%20via%20Retrieval-Augmented%20Generation%0AAuthor%3A%20Haonan%20Yuan%20and%20Qingyun%20Sun%20and%20Jiacheng%20Tao%20and%20Xingcheng%20Fu%20and%20Jianxin%20Li%0AAbstract%3A%20Graph%20Foundation%20Models%20%28GFMs%29%20have%20emerged%20as%20a%20frontier%20in%20graph%20learning%2C%20which%20are%20expected%20to%20deliver%20transferable%20representations%20across%20diverse%20tasks.%20However%2C%20GFMs%20remain%20constrained%20by%20in-memory%20bottlenecks%3A%20they%20attempt%20to%20encode%20knowledge%20into%20model%20parameters%2C%20which%20limits%20semantic%20capacity%2C%20introduces%20heavy%20lossy%20compression%20with%20conflicts%2C%20and%20entangles%20graph%20representation%20with%20the%20knowledge%20in%20ways%20that%20hinder%20efficient%20adaptation%2C%20undermining%20scalability%20and%20interpretability.%20In%20this%20work%2Cwe%20propose%20RAG-GFM%2C%20a%20Retrieval-Augmented%20Generation%20aided%20Graph%20Foundation%20Model%20that%20offloads%20knowledge%20from%20parameters%20and%20complements%20parameterized%20learning.%20To%20externalize%20graph%20knowledge%2C%20we%20build%20a%20dual-modal%20unified%20retrieval%20module%2C%20where%20a%20semantic%20store%20from%20prefix-structured%20text%20and%20a%20structural%20store%20from%20centrality-based%20motif.%20To%20preserve%20heterogeneous%20information%2C%20we%20design%20a%20dual-view%20alignment%20objective%20that%20contrasts%20both%20modalities%20to%20capture%20both%20content%20and%20relational%20patterns.%20To%20enable%20efficient%20downstream%20adaptation%2C%20we%20perform%20in-context%20augmentation%20to%20enrich%20supporting%20instances%20with%20retrieved%20texts%20and%20motifs%20as%20contextual%20evidence.%20Extensive%20experiments%20on%20five%20benchmark%20graph%20datasets%20demonstrate%20that%20RAG-GFM%20consistently%20outperforms%2013%20state-of-the-art%20baselines%20in%20both%20cross-domain%20node%20and%20graph%20classification%2C%20achieving%20superior%20effectiveness%20and%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15124v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOvercoming%2520In-Memory%2520Bottlenecks%2520in%2520Graph%2520Foundation%2520Models%2520via%2520Retrieval-Augmented%2520Generation%26entry.906535625%3DHaonan%2520Yuan%2520and%2520Qingyun%2520Sun%2520and%2520Jiacheng%2520Tao%2520and%2520Xingcheng%2520Fu%2520and%2520Jianxin%2520Li%26entry.1292438233%3DGraph%2520Foundation%2520Models%2520%2528GFMs%2529%2520have%2520emerged%2520as%2520a%2520frontier%2520in%2520graph%2520learning%252C%2520which%2520are%2520expected%2520to%2520deliver%2520transferable%2520representations%2520across%2520diverse%2520tasks.%2520However%252C%2520GFMs%2520remain%2520constrained%2520by%2520in-memory%2520bottlenecks%253A%2520they%2520attempt%2520to%2520encode%2520knowledge%2520into%2520model%2520parameters%252C%2520which%2520limits%2520semantic%2520capacity%252C%2520introduces%2520heavy%2520lossy%2520compression%2520with%2520conflicts%252C%2520and%2520entangles%2520graph%2520representation%2520with%2520the%2520knowledge%2520in%2520ways%2520that%2520hinder%2520efficient%2520adaptation%252C%2520undermining%2520scalability%2520and%2520interpretability.%2520In%2520this%2520work%252Cwe%2520propose%2520RAG-GFM%252C%2520a%2520Retrieval-Augmented%2520Generation%2520aided%2520Graph%2520Foundation%2520Model%2520that%2520offloads%2520knowledge%2520from%2520parameters%2520and%2520complements%2520parameterized%2520learning.%2520To%2520externalize%2520graph%2520knowledge%252C%2520we%2520build%2520a%2520dual-modal%2520unified%2520retrieval%2520module%252C%2520where%2520a%2520semantic%2520store%2520from%2520prefix-structured%2520text%2520and%2520a%2520structural%2520store%2520from%2520centrality-based%2520motif.%2520To%2520preserve%2520heterogeneous%2520information%252C%2520we%2520design%2520a%2520dual-view%2520alignment%2520objective%2520that%2520contrasts%2520both%2520modalities%2520to%2520capture%2520both%2520content%2520and%2520relational%2520patterns.%2520To%2520enable%2520efficient%2520downstream%2520adaptation%252C%2520we%2520perform%2520in-context%2520augmentation%2520to%2520enrich%2520supporting%2520instances%2520with%2520retrieved%2520texts%2520and%2520motifs%2520as%2520contextual%2520evidence.%2520Extensive%2520experiments%2520on%2520five%2520benchmark%2520graph%2520datasets%2520demonstrate%2520that%2520RAG-GFM%2520consistently%2520outperforms%252013%2520state-of-the-art%2520baselines%2520in%2520both%2520cross-domain%2520node%2520and%2520graph%2520classification%252C%2520achieving%2520superior%2520effectiveness%2520and%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15124v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overcoming%20In-Memory%20Bottlenecks%20in%20Graph%20Foundation%20Models%20via%20Retrieval-Augmented%20Generation&entry.906535625=Haonan%20Yuan%20and%20Qingyun%20Sun%20and%20Jiacheng%20Tao%20and%20Xingcheng%20Fu%20and%20Jianxin%20Li&entry.1292438233=Graph%20Foundation%20Models%20%28GFMs%29%20have%20emerged%20as%20a%20frontier%20in%20graph%20learning%2C%20which%20are%20expected%20to%20deliver%20transferable%20representations%20across%20diverse%20tasks.%20However%2C%20GFMs%20remain%20constrained%20by%20in-memory%20bottlenecks%3A%20they%20attempt%20to%20encode%20knowledge%20into%20model%20parameters%2C%20which%20limits%20semantic%20capacity%2C%20introduces%20heavy%20lossy%20compression%20with%20conflicts%2C%20and%20entangles%20graph%20representation%20with%20the%20knowledge%20in%20ways%20that%20hinder%20efficient%20adaptation%2C%20undermining%20scalability%20and%20interpretability.%20In%20this%20work%2Cwe%20propose%20RAG-GFM%2C%20a%20Retrieval-Augmented%20Generation%20aided%20Graph%20Foundation%20Model%20that%20offloads%20knowledge%20from%20parameters%20and%20complements%20parameterized%20learning.%20To%20externalize%20graph%20knowledge%2C%20we%20build%20a%20dual-modal%20unified%20retrieval%20module%2C%20where%20a%20semantic%20store%20from%20prefix-structured%20text%20and%20a%20structural%20store%20from%20centrality-based%20motif.%20To%20preserve%20heterogeneous%20information%2C%20we%20design%20a%20dual-view%20alignment%20objective%20that%20contrasts%20both%20modalities%20to%20capture%20both%20content%20and%20relational%20patterns.%20To%20enable%20efficient%20downstream%20adaptation%2C%20we%20perform%20in-context%20augmentation%20to%20enrich%20supporting%20instances%20with%20retrieved%20texts%20and%20motifs%20as%20contextual%20evidence.%20Extensive%20experiments%20on%20five%20benchmark%20graph%20datasets%20demonstrate%20that%20RAG-GFM%20consistently%20outperforms%2013%20state-of-the-art%20baselines%20in%20both%20cross-domain%20node%20and%20graph%20classification%2C%20achieving%20superior%20effectiveness%20and%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2601.15124v1&entry.124074799=Read"},
{"title": "Debate-Enhanced Pseudo Labeling and Frequency-Aware Progressive Debiasing for Weakly-Supervised Camouflaged Object Detection with Scribble Annotations", "author": "Jiawei Ge and Jiuxin Cao and Xinyi Li and Xuelin Zhu and Chang Liu and Bo Liu and Chen Feng and Ioannis Patras", "abstract": "Weakly-Supervised Camouflaged Object Detection (WSCOD) aims to locate and segment objects that are visually concealed within their surrounding scenes, relying solely on sparse supervision such as scribble annotations. Despite recent progress, existing WSCOD methods still lag far behind fully supervised ones due to two major limitations: (1) the pseudo masks generated by general-purpose segmentation models (e.g., SAM) and filtered via rules are often unreliable, as these models lack the task-specific semantic understanding required for effective pseudo labeling in COD; and (2) the neglect of inherent annotation bias in scribbles, which hinders the model from capturing the global structure of camouflaged objects. To overcome these challenges, we propose ${D}^{3}$ETOR, a two-stage WSCOD framework consisting of Debate-Enhanced Pseudo Labeling and Frequency-Aware Progressive Debiasing. In the first stage, we introduce an adaptive entropy-driven point sampling method and a multi-agent debate mechanism to enhance the capability of SAM for COD, improving the interpretability and precision of pseudo masks. In the second stage, we design FADeNet, which progressively fuses multi-level frequency-aware features to balance global semantic understanding with local detail modeling, while dynamically reweighting supervision strength across regions to alleviate scribble bias. By jointly exploiting the supervision signals from both the pseudo masks and scribble semantics, ${D}^{3}$ETOR significantly narrows the gap between weakly and fully supervised COD, achieving state-of-the-art performance on multiple benchmarks.", "link": "http://arxiv.org/abs/2512.20260v4", "date": "2026-01-21", "relevancy": 2.1849, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5936}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5368}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Debate-Enhanced%20Pseudo%20Labeling%20and%20Frequency-Aware%20Progressive%20Debiasing%20for%20Weakly-Supervised%20Camouflaged%20Object%20Detection%20with%20Scribble%20Annotations&body=Title%3A%20Debate-Enhanced%20Pseudo%20Labeling%20and%20Frequency-Aware%20Progressive%20Debiasing%20for%20Weakly-Supervised%20Camouflaged%20Object%20Detection%20with%20Scribble%20Annotations%0AAuthor%3A%20Jiawei%20Ge%20and%20Jiuxin%20Cao%20and%20Xinyi%20Li%20and%20Xuelin%20Zhu%20and%20Chang%20Liu%20and%20Bo%20Liu%20and%20Chen%20Feng%20and%20Ioannis%20Patras%0AAbstract%3A%20Weakly-Supervised%20Camouflaged%20Object%20Detection%20%28WSCOD%29%20aims%20to%20locate%20and%20segment%20objects%20that%20are%20visually%20concealed%20within%20their%20surrounding%20scenes%2C%20relying%20solely%20on%20sparse%20supervision%20such%20as%20scribble%20annotations.%20Despite%20recent%20progress%2C%20existing%20WSCOD%20methods%20still%20lag%20far%20behind%20fully%20supervised%20ones%20due%20to%20two%20major%20limitations%3A%20%281%29%20the%20pseudo%20masks%20generated%20by%20general-purpose%20segmentation%20models%20%28e.g.%2C%20SAM%29%20and%20filtered%20via%20rules%20are%20often%20unreliable%2C%20as%20these%20models%20lack%20the%20task-specific%20semantic%20understanding%20required%20for%20effective%20pseudo%20labeling%20in%20COD%3B%20and%20%282%29%20the%20neglect%20of%20inherent%20annotation%20bias%20in%20scribbles%2C%20which%20hinders%20the%20model%20from%20capturing%20the%20global%20structure%20of%20camouflaged%20objects.%20To%20overcome%20these%20challenges%2C%20we%20propose%20%24%7BD%7D%5E%7B3%7D%24ETOR%2C%20a%20two-stage%20WSCOD%20framework%20consisting%20of%20Debate-Enhanced%20Pseudo%20Labeling%20and%20Frequency-Aware%20Progressive%20Debiasing.%20In%20the%20first%20stage%2C%20we%20introduce%20an%20adaptive%20entropy-driven%20point%20sampling%20method%20and%20a%20multi-agent%20debate%20mechanism%20to%20enhance%20the%20capability%20of%20SAM%20for%20COD%2C%20improving%20the%20interpretability%20and%20precision%20of%20pseudo%20masks.%20In%20the%20second%20stage%2C%20we%20design%20FADeNet%2C%20which%20progressively%20fuses%20multi-level%20frequency-aware%20features%20to%20balance%20global%20semantic%20understanding%20with%20local%20detail%20modeling%2C%20while%20dynamically%20reweighting%20supervision%20strength%20across%20regions%20to%20alleviate%20scribble%20bias.%20By%20jointly%20exploiting%20the%20supervision%20signals%20from%20both%20the%20pseudo%20masks%20and%20scribble%20semantics%2C%20%24%7BD%7D%5E%7B3%7D%24ETOR%20significantly%20narrows%20the%20gap%20between%20weakly%20and%20fully%20supervised%20COD%2C%20achieving%20state-of-the-art%20performance%20on%20multiple%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20260v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDebate-Enhanced%2520Pseudo%2520Labeling%2520and%2520Frequency-Aware%2520Progressive%2520Debiasing%2520for%2520Weakly-Supervised%2520Camouflaged%2520Object%2520Detection%2520with%2520Scribble%2520Annotations%26entry.906535625%3DJiawei%2520Ge%2520and%2520Jiuxin%2520Cao%2520and%2520Xinyi%2520Li%2520and%2520Xuelin%2520Zhu%2520and%2520Chang%2520Liu%2520and%2520Bo%2520Liu%2520and%2520Chen%2520Feng%2520and%2520Ioannis%2520Patras%26entry.1292438233%3DWeakly-Supervised%2520Camouflaged%2520Object%2520Detection%2520%2528WSCOD%2529%2520aims%2520to%2520locate%2520and%2520segment%2520objects%2520that%2520are%2520visually%2520concealed%2520within%2520their%2520surrounding%2520scenes%252C%2520relying%2520solely%2520on%2520sparse%2520supervision%2520such%2520as%2520scribble%2520annotations.%2520Despite%2520recent%2520progress%252C%2520existing%2520WSCOD%2520methods%2520still%2520lag%2520far%2520behind%2520fully%2520supervised%2520ones%2520due%2520to%2520two%2520major%2520limitations%253A%2520%25281%2529%2520the%2520pseudo%2520masks%2520generated%2520by%2520general-purpose%2520segmentation%2520models%2520%2528e.g.%252C%2520SAM%2529%2520and%2520filtered%2520via%2520rules%2520are%2520often%2520unreliable%252C%2520as%2520these%2520models%2520lack%2520the%2520task-specific%2520semantic%2520understanding%2520required%2520for%2520effective%2520pseudo%2520labeling%2520in%2520COD%253B%2520and%2520%25282%2529%2520the%2520neglect%2520of%2520inherent%2520annotation%2520bias%2520in%2520scribbles%252C%2520which%2520hinders%2520the%2520model%2520from%2520capturing%2520the%2520global%2520structure%2520of%2520camouflaged%2520objects.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520%2524%257BD%257D%255E%257B3%257D%2524ETOR%252C%2520a%2520two-stage%2520WSCOD%2520framework%2520consisting%2520of%2520Debate-Enhanced%2520Pseudo%2520Labeling%2520and%2520Frequency-Aware%2520Progressive%2520Debiasing.%2520In%2520the%2520first%2520stage%252C%2520we%2520introduce%2520an%2520adaptive%2520entropy-driven%2520point%2520sampling%2520method%2520and%2520a%2520multi-agent%2520debate%2520mechanism%2520to%2520enhance%2520the%2520capability%2520of%2520SAM%2520for%2520COD%252C%2520improving%2520the%2520interpretability%2520and%2520precision%2520of%2520pseudo%2520masks.%2520In%2520the%2520second%2520stage%252C%2520we%2520design%2520FADeNet%252C%2520which%2520progressively%2520fuses%2520multi-level%2520frequency-aware%2520features%2520to%2520balance%2520global%2520semantic%2520understanding%2520with%2520local%2520detail%2520modeling%252C%2520while%2520dynamically%2520reweighting%2520supervision%2520strength%2520across%2520regions%2520to%2520alleviate%2520scribble%2520bias.%2520By%2520jointly%2520exploiting%2520the%2520supervision%2520signals%2520from%2520both%2520the%2520pseudo%2520masks%2520and%2520scribble%2520semantics%252C%2520%2524%257BD%257D%255E%257B3%257D%2524ETOR%2520significantly%2520narrows%2520the%2520gap%2520between%2520weakly%2520and%2520fully%2520supervised%2520COD%252C%2520achieving%2520state-of-the-art%2520performance%2520on%2520multiple%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20260v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Debate-Enhanced%20Pseudo%20Labeling%20and%20Frequency-Aware%20Progressive%20Debiasing%20for%20Weakly-Supervised%20Camouflaged%20Object%20Detection%20with%20Scribble%20Annotations&entry.906535625=Jiawei%20Ge%20and%20Jiuxin%20Cao%20and%20Xinyi%20Li%20and%20Xuelin%20Zhu%20and%20Chang%20Liu%20and%20Bo%20Liu%20and%20Chen%20Feng%20and%20Ioannis%20Patras&entry.1292438233=Weakly-Supervised%20Camouflaged%20Object%20Detection%20%28WSCOD%29%20aims%20to%20locate%20and%20segment%20objects%20that%20are%20visually%20concealed%20within%20their%20surrounding%20scenes%2C%20relying%20solely%20on%20sparse%20supervision%20such%20as%20scribble%20annotations.%20Despite%20recent%20progress%2C%20existing%20WSCOD%20methods%20still%20lag%20far%20behind%20fully%20supervised%20ones%20due%20to%20two%20major%20limitations%3A%20%281%29%20the%20pseudo%20masks%20generated%20by%20general-purpose%20segmentation%20models%20%28e.g.%2C%20SAM%29%20and%20filtered%20via%20rules%20are%20often%20unreliable%2C%20as%20these%20models%20lack%20the%20task-specific%20semantic%20understanding%20required%20for%20effective%20pseudo%20labeling%20in%20COD%3B%20and%20%282%29%20the%20neglect%20of%20inherent%20annotation%20bias%20in%20scribbles%2C%20which%20hinders%20the%20model%20from%20capturing%20the%20global%20structure%20of%20camouflaged%20objects.%20To%20overcome%20these%20challenges%2C%20we%20propose%20%24%7BD%7D%5E%7B3%7D%24ETOR%2C%20a%20two-stage%20WSCOD%20framework%20consisting%20of%20Debate-Enhanced%20Pseudo%20Labeling%20and%20Frequency-Aware%20Progressive%20Debiasing.%20In%20the%20first%20stage%2C%20we%20introduce%20an%20adaptive%20entropy-driven%20point%20sampling%20method%20and%20a%20multi-agent%20debate%20mechanism%20to%20enhance%20the%20capability%20of%20SAM%20for%20COD%2C%20improving%20the%20interpretability%20and%20precision%20of%20pseudo%20masks.%20In%20the%20second%20stage%2C%20we%20design%20FADeNet%2C%20which%20progressively%20fuses%20multi-level%20frequency-aware%20features%20to%20balance%20global%20semantic%20understanding%20with%20local%20detail%20modeling%2C%20while%20dynamically%20reweighting%20supervision%20strength%20across%20regions%20to%20alleviate%20scribble%20bias.%20By%20jointly%20exploiting%20the%20supervision%20signals%20from%20both%20the%20pseudo%20masks%20and%20scribble%20semantics%2C%20%24%7BD%7D%5E%7B3%7D%24ETOR%20significantly%20narrows%20the%20gap%20between%20weakly%20and%20fully%20supervised%20COD%2C%20achieving%20state-of-the-art%20performance%20on%20multiple%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2512.20260v4&entry.124074799=Read"},
{"title": "Token Maturation: Autoregressive Language Generation via Continuous Token Dynamics", "author": "Oshri Naparstek", "abstract": "Standard autoregressive language models collapse uncertainty at every generation step by committing to discrete tokens through immediate sampling. This premature discretization underlies well-known failure modes, including degenerate repetition loops in greedy decoding and a heavy reliance on heuristic sampling strategies.\n  We introduce \\textbf{Token Maturation}, a continuous autoregressive framework in which tokens evolve as vector-valued trajectories prior to discretization. Rather than sampling from a categorical distribution at each step, the model resolves uncertainty through a deterministic dynamical process in embedding space, deferring discrete commitment until the representation has geometrically stabilized.\n  We show that this formulation mitigates degeneration \\emph{intrinsically}: Token Maturation generates coherent and diverse text under fully deterministic decoding (argmax), without repetition penalties, temperature scaling, or stochastic sampling. Moreover, we identify a novel convergence behavior in which token representations stabilize spatially while predictive entropy remains high, challenging the common assumption that commitment requires probability concentration. We propose continuous token dynamics with delayed commitment as an alternative formulation of autoregressive generation that exposes structural regularities obscured by immediate discretization.", "link": "http://arxiv.org/abs/2601.04854v2", "date": "2026-01-21", "relevancy": 2.181, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5906}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5153}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token%20Maturation%3A%20Autoregressive%20Language%20Generation%20via%20Continuous%20Token%20Dynamics&body=Title%3A%20Token%20Maturation%3A%20Autoregressive%20Language%20Generation%20via%20Continuous%20Token%20Dynamics%0AAuthor%3A%20Oshri%20Naparstek%0AAbstract%3A%20Standard%20autoregressive%20language%20models%20collapse%20uncertainty%20at%20every%20generation%20step%20by%20committing%20to%20discrete%20tokens%20through%20immediate%20sampling.%20This%20premature%20discretization%20underlies%20well-known%20failure%20modes%2C%20including%20degenerate%20repetition%20loops%20in%20greedy%20decoding%20and%20a%20heavy%20reliance%20on%20heuristic%20sampling%20strategies.%0A%20%20We%20introduce%20%5Ctextbf%7BToken%20Maturation%7D%2C%20a%20continuous%20autoregressive%20framework%20in%20which%20tokens%20evolve%20as%20vector-valued%20trajectories%20prior%20to%20discretization.%20Rather%20than%20sampling%20from%20a%20categorical%20distribution%20at%20each%20step%2C%20the%20model%20resolves%20uncertainty%20through%20a%20deterministic%20dynamical%20process%20in%20embedding%20space%2C%20deferring%20discrete%20commitment%20until%20the%20representation%20has%20geometrically%20stabilized.%0A%20%20We%20show%20that%20this%20formulation%20mitigates%20degeneration%20%5Cemph%7Bintrinsically%7D%3A%20Token%20Maturation%20generates%20coherent%20and%20diverse%20text%20under%20fully%20deterministic%20decoding%20%28argmax%29%2C%20without%20repetition%20penalties%2C%20temperature%20scaling%2C%20or%20stochastic%20sampling.%20Moreover%2C%20we%20identify%20a%20novel%20convergence%20behavior%20in%20which%20token%20representations%20stabilize%20spatially%20while%20predictive%20entropy%20remains%20high%2C%20challenging%20the%20common%20assumption%20that%20commitment%20requires%20probability%20concentration.%20We%20propose%20continuous%20token%20dynamics%20with%20delayed%20commitment%20as%20an%20alternative%20formulation%20of%20autoregressive%20generation%20that%20exposes%20structural%20regularities%20obscured%20by%20immediate%20discretization.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04854v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken%2520Maturation%253A%2520Autoregressive%2520Language%2520Generation%2520via%2520Continuous%2520Token%2520Dynamics%26entry.906535625%3DOshri%2520Naparstek%26entry.1292438233%3DStandard%2520autoregressive%2520language%2520models%2520collapse%2520uncertainty%2520at%2520every%2520generation%2520step%2520by%2520committing%2520to%2520discrete%2520tokens%2520through%2520immediate%2520sampling.%2520This%2520premature%2520discretization%2520underlies%2520well-known%2520failure%2520modes%252C%2520including%2520degenerate%2520repetition%2520loops%2520in%2520greedy%2520decoding%2520and%2520a%2520heavy%2520reliance%2520on%2520heuristic%2520sampling%2520strategies.%250A%2520%2520We%2520introduce%2520%255Ctextbf%257BToken%2520Maturation%257D%252C%2520a%2520continuous%2520autoregressive%2520framework%2520in%2520which%2520tokens%2520evolve%2520as%2520vector-valued%2520trajectories%2520prior%2520to%2520discretization.%2520Rather%2520than%2520sampling%2520from%2520a%2520categorical%2520distribution%2520at%2520each%2520step%252C%2520the%2520model%2520resolves%2520uncertainty%2520through%2520a%2520deterministic%2520dynamical%2520process%2520in%2520embedding%2520space%252C%2520deferring%2520discrete%2520commitment%2520until%2520the%2520representation%2520has%2520geometrically%2520stabilized.%250A%2520%2520We%2520show%2520that%2520this%2520formulation%2520mitigates%2520degeneration%2520%255Cemph%257Bintrinsically%257D%253A%2520Token%2520Maturation%2520generates%2520coherent%2520and%2520diverse%2520text%2520under%2520fully%2520deterministic%2520decoding%2520%2528argmax%2529%252C%2520without%2520repetition%2520penalties%252C%2520temperature%2520scaling%252C%2520or%2520stochastic%2520sampling.%2520Moreover%252C%2520we%2520identify%2520a%2520novel%2520convergence%2520behavior%2520in%2520which%2520token%2520representations%2520stabilize%2520spatially%2520while%2520predictive%2520entropy%2520remains%2520high%252C%2520challenging%2520the%2520common%2520assumption%2520that%2520commitment%2520requires%2520probability%2520concentration.%2520We%2520propose%2520continuous%2520token%2520dynamics%2520with%2520delayed%2520commitment%2520as%2520an%2520alternative%2520formulation%2520of%2520autoregressive%2520generation%2520that%2520exposes%2520structural%2520regularities%2520obscured%2520by%2520immediate%2520discretization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04854v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token%20Maturation%3A%20Autoregressive%20Language%20Generation%20via%20Continuous%20Token%20Dynamics&entry.906535625=Oshri%20Naparstek&entry.1292438233=Standard%20autoregressive%20language%20models%20collapse%20uncertainty%20at%20every%20generation%20step%20by%20committing%20to%20discrete%20tokens%20through%20immediate%20sampling.%20This%20premature%20discretization%20underlies%20well-known%20failure%20modes%2C%20including%20degenerate%20repetition%20loops%20in%20greedy%20decoding%20and%20a%20heavy%20reliance%20on%20heuristic%20sampling%20strategies.%0A%20%20We%20introduce%20%5Ctextbf%7BToken%20Maturation%7D%2C%20a%20continuous%20autoregressive%20framework%20in%20which%20tokens%20evolve%20as%20vector-valued%20trajectories%20prior%20to%20discretization.%20Rather%20than%20sampling%20from%20a%20categorical%20distribution%20at%20each%20step%2C%20the%20model%20resolves%20uncertainty%20through%20a%20deterministic%20dynamical%20process%20in%20embedding%20space%2C%20deferring%20discrete%20commitment%20until%20the%20representation%20has%20geometrically%20stabilized.%0A%20%20We%20show%20that%20this%20formulation%20mitigates%20degeneration%20%5Cemph%7Bintrinsically%7D%3A%20Token%20Maturation%20generates%20coherent%20and%20diverse%20text%20under%20fully%20deterministic%20decoding%20%28argmax%29%2C%20without%20repetition%20penalties%2C%20temperature%20scaling%2C%20or%20stochastic%20sampling.%20Moreover%2C%20we%20identify%20a%20novel%20convergence%20behavior%20in%20which%20token%20representations%20stabilize%20spatially%20while%20predictive%20entropy%20remains%20high%2C%20challenging%20the%20common%20assumption%20that%20commitment%20requires%20probability%20concentration.%20We%20propose%20continuous%20token%20dynamics%20with%20delayed%20commitment%20as%20an%20alternative%20formulation%20of%20autoregressive%20generation%20that%20exposes%20structural%20regularities%20obscured%20by%20immediate%20discretization.&entry.1838667208=http%3A//arxiv.org/abs/2601.04854v2&entry.124074799=Read"},
{"title": "InstructTime++: Time Series Classification with Multimodal Language Modeling via Implicit Feature Enhancement", "author": "Mingyue Cheng and Xiaoyu Tao and Huajian Zhang and Qi Liu and Enhong Chen", "abstract": "Most existing time series classification methods adopt a discriminative paradigm that maps input sequences directly to one-hot encoded class labels. While effective, this paradigm struggles to incorporate contextual features and fails to capture semantic relationships among classes. To address these limitations, we propose InstructTime, a novel framework that reformulates time series classification as a multimodal generative task. Specifically, continuous numerical sequences, contextual textual features, and task instructions are treated as multimodal inputs, while class labels are generated as textual outputs by tuned language models. To bridge the modality gap, InstructTime introduces a time series discretization module that converts continuous sequences into discrete temporal tokens, together with an alignment projection layer and a generative self-supervised pre-training strategy to enhance cross-modal representation alignment. Building upon this framework, we further propose InstructTime++, which extends InstructTime by incorporating implicit feature modeling to compensate for the limited inductive bias of language models. InstructTime++ leverages specialized toolkits to mine informative implicit patterns from raw time series and contextual inputs, including statistical feature extraction and vision-language-based image captioning, and translates them into textual descriptions for seamless integration. Extensive experiments on multiple benchmark datasets demonstrate the superior performance of InstructTime++.", "link": "http://arxiv.org/abs/2601.14968v1", "date": "2026-01-21", "relevancy": 2.161, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5445}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5421}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstructTime%2B%2B%3A%20Time%20Series%20Classification%20with%20Multimodal%20Language%20Modeling%20via%20Implicit%20Feature%20Enhancement&body=Title%3A%20InstructTime%2B%2B%3A%20Time%20Series%20Classification%20with%20Multimodal%20Language%20Modeling%20via%20Implicit%20Feature%20Enhancement%0AAuthor%3A%20Mingyue%20Cheng%20and%20Xiaoyu%20Tao%20and%20Huajian%20Zhang%20and%20Qi%20Liu%20and%20Enhong%20Chen%0AAbstract%3A%20Most%20existing%20time%20series%20classification%20methods%20adopt%20a%20discriminative%20paradigm%20that%20maps%20input%20sequences%20directly%20to%20one-hot%20encoded%20class%20labels.%20While%20effective%2C%20this%20paradigm%20struggles%20to%20incorporate%20contextual%20features%20and%20fails%20to%20capture%20semantic%20relationships%20among%20classes.%20To%20address%20these%20limitations%2C%20we%20propose%20InstructTime%2C%20a%20novel%20framework%20that%20reformulates%20time%20series%20classification%20as%20a%20multimodal%20generative%20task.%20Specifically%2C%20continuous%20numerical%20sequences%2C%20contextual%20textual%20features%2C%20and%20task%20instructions%20are%20treated%20as%20multimodal%20inputs%2C%20while%20class%20labels%20are%20generated%20as%20textual%20outputs%20by%20tuned%20language%20models.%20To%20bridge%20the%20modality%20gap%2C%20InstructTime%20introduces%20a%20time%20series%20discretization%20module%20that%20converts%20continuous%20sequences%20into%20discrete%20temporal%20tokens%2C%20together%20with%20an%20alignment%20projection%20layer%20and%20a%20generative%20self-supervised%20pre-training%20strategy%20to%20enhance%20cross-modal%20representation%20alignment.%20Building%20upon%20this%20framework%2C%20we%20further%20propose%20InstructTime%2B%2B%2C%20which%20extends%20InstructTime%20by%20incorporating%20implicit%20feature%20modeling%20to%20compensate%20for%20the%20limited%20inductive%20bias%20of%20language%20models.%20InstructTime%2B%2B%20leverages%20specialized%20toolkits%20to%20mine%20informative%20implicit%20patterns%20from%20raw%20time%20series%20and%20contextual%20inputs%2C%20including%20statistical%20feature%20extraction%20and%20vision-language-based%20image%20captioning%2C%20and%20translates%20them%20into%20textual%20descriptions%20for%20seamless%20integration.%20Extensive%20experiments%20on%20multiple%20benchmark%20datasets%20demonstrate%20the%20superior%20performance%20of%20InstructTime%2B%2B.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14968v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstructTime%252B%252B%253A%2520Time%2520Series%2520Classification%2520with%2520Multimodal%2520Language%2520Modeling%2520via%2520Implicit%2520Feature%2520Enhancement%26entry.906535625%3DMingyue%2520Cheng%2520and%2520Xiaoyu%2520Tao%2520and%2520Huajian%2520Zhang%2520and%2520Qi%2520Liu%2520and%2520Enhong%2520Chen%26entry.1292438233%3DMost%2520existing%2520time%2520series%2520classification%2520methods%2520adopt%2520a%2520discriminative%2520paradigm%2520that%2520maps%2520input%2520sequences%2520directly%2520to%2520one-hot%2520encoded%2520class%2520labels.%2520While%2520effective%252C%2520this%2520paradigm%2520struggles%2520to%2520incorporate%2520contextual%2520features%2520and%2520fails%2520to%2520capture%2520semantic%2520relationships%2520among%2520classes.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520InstructTime%252C%2520a%2520novel%2520framework%2520that%2520reformulates%2520time%2520series%2520classification%2520as%2520a%2520multimodal%2520generative%2520task.%2520Specifically%252C%2520continuous%2520numerical%2520sequences%252C%2520contextual%2520textual%2520features%252C%2520and%2520task%2520instructions%2520are%2520treated%2520as%2520multimodal%2520inputs%252C%2520while%2520class%2520labels%2520are%2520generated%2520as%2520textual%2520outputs%2520by%2520tuned%2520language%2520models.%2520To%2520bridge%2520the%2520modality%2520gap%252C%2520InstructTime%2520introduces%2520a%2520time%2520series%2520discretization%2520module%2520that%2520converts%2520continuous%2520sequences%2520into%2520discrete%2520temporal%2520tokens%252C%2520together%2520with%2520an%2520alignment%2520projection%2520layer%2520and%2520a%2520generative%2520self-supervised%2520pre-training%2520strategy%2520to%2520enhance%2520cross-modal%2520representation%2520alignment.%2520Building%2520upon%2520this%2520framework%252C%2520we%2520further%2520propose%2520InstructTime%252B%252B%252C%2520which%2520extends%2520InstructTime%2520by%2520incorporating%2520implicit%2520feature%2520modeling%2520to%2520compensate%2520for%2520the%2520limited%2520inductive%2520bias%2520of%2520language%2520models.%2520InstructTime%252B%252B%2520leverages%2520specialized%2520toolkits%2520to%2520mine%2520informative%2520implicit%2520patterns%2520from%2520raw%2520time%2520series%2520and%2520contextual%2520inputs%252C%2520including%2520statistical%2520feature%2520extraction%2520and%2520vision-language-based%2520image%2520captioning%252C%2520and%2520translates%2520them%2520into%2520textual%2520descriptions%2520for%2520seamless%2520integration.%2520Extensive%2520experiments%2520on%2520multiple%2520benchmark%2520datasets%2520demonstrate%2520the%2520superior%2520performance%2520of%2520InstructTime%252B%252B.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14968v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstructTime%2B%2B%3A%20Time%20Series%20Classification%20with%20Multimodal%20Language%20Modeling%20via%20Implicit%20Feature%20Enhancement&entry.906535625=Mingyue%20Cheng%20and%20Xiaoyu%20Tao%20and%20Huajian%20Zhang%20and%20Qi%20Liu%20and%20Enhong%20Chen&entry.1292438233=Most%20existing%20time%20series%20classification%20methods%20adopt%20a%20discriminative%20paradigm%20that%20maps%20input%20sequences%20directly%20to%20one-hot%20encoded%20class%20labels.%20While%20effective%2C%20this%20paradigm%20struggles%20to%20incorporate%20contextual%20features%20and%20fails%20to%20capture%20semantic%20relationships%20among%20classes.%20To%20address%20these%20limitations%2C%20we%20propose%20InstructTime%2C%20a%20novel%20framework%20that%20reformulates%20time%20series%20classification%20as%20a%20multimodal%20generative%20task.%20Specifically%2C%20continuous%20numerical%20sequences%2C%20contextual%20textual%20features%2C%20and%20task%20instructions%20are%20treated%20as%20multimodal%20inputs%2C%20while%20class%20labels%20are%20generated%20as%20textual%20outputs%20by%20tuned%20language%20models.%20To%20bridge%20the%20modality%20gap%2C%20InstructTime%20introduces%20a%20time%20series%20discretization%20module%20that%20converts%20continuous%20sequences%20into%20discrete%20temporal%20tokens%2C%20together%20with%20an%20alignment%20projection%20layer%20and%20a%20generative%20self-supervised%20pre-training%20strategy%20to%20enhance%20cross-modal%20representation%20alignment.%20Building%20upon%20this%20framework%2C%20we%20further%20propose%20InstructTime%2B%2B%2C%20which%20extends%20InstructTime%20by%20incorporating%20implicit%20feature%20modeling%20to%20compensate%20for%20the%20limited%20inductive%20bias%20of%20language%20models.%20InstructTime%2B%2B%20leverages%20specialized%20toolkits%20to%20mine%20informative%20implicit%20patterns%20from%20raw%20time%20series%20and%20contextual%20inputs%2C%20including%20statistical%20feature%20extraction%20and%20vision-language-based%20image%20captioning%2C%20and%20translates%20them%20into%20textual%20descriptions%20for%20seamless%20integration.%20Extensive%20experiments%20on%20multiple%20benchmark%20datasets%20demonstrate%20the%20superior%20performance%20of%20InstructTime%2B%2B.&entry.1838667208=http%3A//arxiv.org/abs/2601.14968v1&entry.124074799=Read"},
{"title": "Benchmarking Large Language Models for ABAP Code Generation: An Empirical Study on Iterative Improvement by Compiler Feedback", "author": "Stephan Wallraven and Tim K\u00f6hne and Hartmut Westenberger and Andreas Moser", "abstract": "This work investigates the performance of Large Language Models (LLMs) in generating ABAP code. Despite successful applications of generative AI in many programming languages, there are hardly any systematic analyses of ABAP code generation to date. The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges. For this purpose, a benchmark with 180 tasks is conducted, consisting of adapted HumanEval tasks and practical SAP scenarios. The results show significant performance differences between the models: more powerful LLMs achieve success rates of around 75% after several iterations and benefit greatly from compiler feedback, while smaller models perform significantly weaker. Overall, the study highlights the high potential of powerful LLMs for ABAP development processes, especially in iterative error correction.", "link": "http://arxiv.org/abs/2601.15188v1", "date": "2026-01-21", "relevancy": 2.1601, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4351}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4305}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Large%20Language%20Models%20for%20ABAP%20Code%20Generation%3A%20An%20Empirical%20Study%20on%20Iterative%20Improvement%20by%20Compiler%20Feedback&body=Title%3A%20Benchmarking%20Large%20Language%20Models%20for%20ABAP%20Code%20Generation%3A%20An%20Empirical%20Study%20on%20Iterative%20Improvement%20by%20Compiler%20Feedback%0AAuthor%3A%20Stephan%20Wallraven%20and%20Tim%20K%C3%B6hne%20and%20Hartmut%20Westenberger%20and%20Andreas%20Moser%0AAbstract%3A%20This%20work%20investigates%20the%20performance%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20generating%20ABAP%20code.%20Despite%20successful%20applications%20of%20generative%20AI%20in%20many%20programming%20languages%2C%20there%20are%20hardly%20any%20systematic%20analyses%20of%20ABAP%20code%20generation%20to%20date.%20The%20aim%20of%20the%20study%20is%20to%20empirically%20analyze%20to%20what%20extent%20various%20LLMs%20can%20generate%20syntactically%20correct%20and%20functional%20ABAP%20code%2C%20how%20effectively%20they%20use%20compiler%20feedback%20for%20iterative%20improvement%2C%20and%20which%20task%20types%20pose%20special%20challenges.%20For%20this%20purpose%2C%20a%20benchmark%20with%20180%20tasks%20is%20conducted%2C%20consisting%20of%20adapted%20HumanEval%20tasks%20and%20practical%20SAP%20scenarios.%20The%20results%20show%20significant%20performance%20differences%20between%20the%20models%3A%20more%20powerful%20LLMs%20achieve%20success%20rates%20of%20around%2075%25%20after%20several%20iterations%20and%20benefit%20greatly%20from%20compiler%20feedback%2C%20while%20smaller%20models%20perform%20significantly%20weaker.%20Overall%2C%20the%20study%20highlights%20the%20high%20potential%20of%20powerful%20LLMs%20for%20ABAP%20development%20processes%2C%20especially%20in%20iterative%20error%20correction.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Large%2520Language%2520Models%2520for%2520ABAP%2520Code%2520Generation%253A%2520An%2520Empirical%2520Study%2520on%2520Iterative%2520Improvement%2520by%2520Compiler%2520Feedback%26entry.906535625%3DStephan%2520Wallraven%2520and%2520Tim%2520K%25C3%25B6hne%2520and%2520Hartmut%2520Westenberger%2520and%2520Andreas%2520Moser%26entry.1292438233%3DThis%2520work%2520investigates%2520the%2520performance%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520generating%2520ABAP%2520code.%2520Despite%2520successful%2520applications%2520of%2520generative%2520AI%2520in%2520many%2520programming%2520languages%252C%2520there%2520are%2520hardly%2520any%2520systematic%2520analyses%2520of%2520ABAP%2520code%2520generation%2520to%2520date.%2520The%2520aim%2520of%2520the%2520study%2520is%2520to%2520empirically%2520analyze%2520to%2520what%2520extent%2520various%2520LLMs%2520can%2520generate%2520syntactically%2520correct%2520and%2520functional%2520ABAP%2520code%252C%2520how%2520effectively%2520they%2520use%2520compiler%2520feedback%2520for%2520iterative%2520improvement%252C%2520and%2520which%2520task%2520types%2520pose%2520special%2520challenges.%2520For%2520this%2520purpose%252C%2520a%2520benchmark%2520with%2520180%2520tasks%2520is%2520conducted%252C%2520consisting%2520of%2520adapted%2520HumanEval%2520tasks%2520and%2520practical%2520SAP%2520scenarios.%2520The%2520results%2520show%2520significant%2520performance%2520differences%2520between%2520the%2520models%253A%2520more%2520powerful%2520LLMs%2520achieve%2520success%2520rates%2520of%2520around%252075%2525%2520after%2520several%2520iterations%2520and%2520benefit%2520greatly%2520from%2520compiler%2520feedback%252C%2520while%2520smaller%2520models%2520perform%2520significantly%2520weaker.%2520Overall%252C%2520the%2520study%2520highlights%2520the%2520high%2520potential%2520of%2520powerful%2520LLMs%2520for%2520ABAP%2520development%2520processes%252C%2520especially%2520in%2520iterative%2520error%2520correction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Large%20Language%20Models%20for%20ABAP%20Code%20Generation%3A%20An%20Empirical%20Study%20on%20Iterative%20Improvement%20by%20Compiler%20Feedback&entry.906535625=Stephan%20Wallraven%20and%20Tim%20K%C3%B6hne%20and%20Hartmut%20Westenberger%20and%20Andreas%20Moser&entry.1292438233=This%20work%20investigates%20the%20performance%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20generating%20ABAP%20code.%20Despite%20successful%20applications%20of%20generative%20AI%20in%20many%20programming%20languages%2C%20there%20are%20hardly%20any%20systematic%20analyses%20of%20ABAP%20code%20generation%20to%20date.%20The%20aim%20of%20the%20study%20is%20to%20empirically%20analyze%20to%20what%20extent%20various%20LLMs%20can%20generate%20syntactically%20correct%20and%20functional%20ABAP%20code%2C%20how%20effectively%20they%20use%20compiler%20feedback%20for%20iterative%20improvement%2C%20and%20which%20task%20types%20pose%20special%20challenges.%20For%20this%20purpose%2C%20a%20benchmark%20with%20180%20tasks%20is%20conducted%2C%20consisting%20of%20adapted%20HumanEval%20tasks%20and%20practical%20SAP%20scenarios.%20The%20results%20show%20significant%20performance%20differences%20between%20the%20models%3A%20more%20powerful%20LLMs%20achieve%20success%20rates%20of%20around%2075%25%20after%20several%20iterations%20and%20benefit%20greatly%20from%20compiler%20feedback%2C%20while%20smaller%20models%20perform%20significantly%20weaker.%20Overall%2C%20the%20study%20highlights%20the%20high%20potential%20of%20powerful%20LLMs%20for%20ABAP%20development%20processes%2C%20especially%20in%20iterative%20error%20correction.&entry.1838667208=http%3A//arxiv.org/abs/2601.15188v1&entry.124074799=Read"},
{"title": "Influence of Operator Expertise on Robot Supervision and Intervention", "author": "Yanran Jiang and Pavan Sikka and Leimin Tian and Dana Kuliic and Cecile Paris", "abstract": "With increasing levels of robot autonomy, robots are increasingly being supervised by users with varying levels of robotics expertise. As the diversity of the user population increases, it is important to understand how users with different expertise levels approach the supervision task and how this impacts performance of the human-robot team. This exploratory study investigates how operators with varying expertise levels perceive information and make intervention decisions when supervising a remote robot. We conducted a user study (N=27) where participants supervised a robot autonomously exploring four unknown tunnel environments in a simulator, and provided waypoints to intervene when they believed the robot had encountered difficulties. By analyzing the interaction data and questionnaire responses, we identify differing patterns in intervention timing and decision-making strategies across novice, intermediate, and expert users.", "link": "http://arxiv.org/abs/2601.15069v1", "date": "2026-01-21", "relevancy": 2.1586, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5512}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5374}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Influence%20of%20Operator%20Expertise%20on%20Robot%20Supervision%20and%20Intervention&body=Title%3A%20Influence%20of%20Operator%20Expertise%20on%20Robot%20Supervision%20and%20Intervention%0AAuthor%3A%20Yanran%20Jiang%20and%20Pavan%20Sikka%20and%20Leimin%20Tian%20and%20Dana%20Kuliic%20and%20Cecile%20Paris%0AAbstract%3A%20With%20increasing%20levels%20of%20robot%20autonomy%2C%20robots%20are%20increasingly%20being%20supervised%20by%20users%20with%20varying%20levels%20of%20robotics%20expertise.%20As%20the%20diversity%20of%20the%20user%20population%20increases%2C%20it%20is%20important%20to%20understand%20how%20users%20with%20different%20expertise%20levels%20approach%20the%20supervision%20task%20and%20how%20this%20impacts%20performance%20of%20the%20human-robot%20team.%20This%20exploratory%20study%20investigates%20how%20operators%20with%20varying%20expertise%20levels%20perceive%20information%20and%20make%20intervention%20decisions%20when%20supervising%20a%20remote%20robot.%20We%20conducted%20a%20user%20study%20%28N%3D27%29%20where%20participants%20supervised%20a%20robot%20autonomously%20exploring%20four%20unknown%20tunnel%20environments%20in%20a%20simulator%2C%20and%20provided%20waypoints%20to%20intervene%20when%20they%20believed%20the%20robot%20had%20encountered%20difficulties.%20By%20analyzing%20the%20interaction%20data%20and%20questionnaire%20responses%2C%20we%20identify%20differing%20patterns%20in%20intervention%20timing%20and%20decision-making%20strategies%20across%20novice%2C%20intermediate%2C%20and%20expert%20users.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15069v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfluence%2520of%2520Operator%2520Expertise%2520on%2520Robot%2520Supervision%2520and%2520Intervention%26entry.906535625%3DYanran%2520Jiang%2520and%2520Pavan%2520Sikka%2520and%2520Leimin%2520Tian%2520and%2520Dana%2520Kuliic%2520and%2520Cecile%2520Paris%26entry.1292438233%3DWith%2520increasing%2520levels%2520of%2520robot%2520autonomy%252C%2520robots%2520are%2520increasingly%2520being%2520supervised%2520by%2520users%2520with%2520varying%2520levels%2520of%2520robotics%2520expertise.%2520As%2520the%2520diversity%2520of%2520the%2520user%2520population%2520increases%252C%2520it%2520is%2520important%2520to%2520understand%2520how%2520users%2520with%2520different%2520expertise%2520levels%2520approach%2520the%2520supervision%2520task%2520and%2520how%2520this%2520impacts%2520performance%2520of%2520the%2520human-robot%2520team.%2520This%2520exploratory%2520study%2520investigates%2520how%2520operators%2520with%2520varying%2520expertise%2520levels%2520perceive%2520information%2520and%2520make%2520intervention%2520decisions%2520when%2520supervising%2520a%2520remote%2520robot.%2520We%2520conducted%2520a%2520user%2520study%2520%2528N%253D27%2529%2520where%2520participants%2520supervised%2520a%2520robot%2520autonomously%2520exploring%2520four%2520unknown%2520tunnel%2520environments%2520in%2520a%2520simulator%252C%2520and%2520provided%2520waypoints%2520to%2520intervene%2520when%2520they%2520believed%2520the%2520robot%2520had%2520encountered%2520difficulties.%2520By%2520analyzing%2520the%2520interaction%2520data%2520and%2520questionnaire%2520responses%252C%2520we%2520identify%2520differing%2520patterns%2520in%2520intervention%2520timing%2520and%2520decision-making%2520strategies%2520across%2520novice%252C%2520intermediate%252C%2520and%2520expert%2520users.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15069v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Influence%20of%20Operator%20Expertise%20on%20Robot%20Supervision%20and%20Intervention&entry.906535625=Yanran%20Jiang%20and%20Pavan%20Sikka%20and%20Leimin%20Tian%20and%20Dana%20Kuliic%20and%20Cecile%20Paris&entry.1292438233=With%20increasing%20levels%20of%20robot%20autonomy%2C%20robots%20are%20increasingly%20being%20supervised%20by%20users%20with%20varying%20levels%20of%20robotics%20expertise.%20As%20the%20diversity%20of%20the%20user%20population%20increases%2C%20it%20is%20important%20to%20understand%20how%20users%20with%20different%20expertise%20levels%20approach%20the%20supervision%20task%20and%20how%20this%20impacts%20performance%20of%20the%20human-robot%20team.%20This%20exploratory%20study%20investigates%20how%20operators%20with%20varying%20expertise%20levels%20perceive%20information%20and%20make%20intervention%20decisions%20when%20supervising%20a%20remote%20robot.%20We%20conducted%20a%20user%20study%20%28N%3D27%29%20where%20participants%20supervised%20a%20robot%20autonomously%20exploring%20four%20unknown%20tunnel%20environments%20in%20a%20simulator%2C%20and%20provided%20waypoints%20to%20intervene%20when%20they%20believed%20the%20robot%20had%20encountered%20difficulties.%20By%20analyzing%20the%20interaction%20data%20and%20questionnaire%20responses%2C%20we%20identify%20differing%20patterns%20in%20intervention%20timing%20and%20decision-making%20strategies%20across%20novice%2C%20intermediate%2C%20and%20expert%20users.&entry.1838667208=http%3A//arxiv.org/abs/2601.15069v1&entry.124074799=Read"},
{"title": "HyperNet-Adaptation for Diffusion-Based Test Case Generation", "author": "Oliver Wei\u00dfl and Vincenzo Riccio and Severin Kacianka and Andrea Stocco", "abstract": "The increasing deployment of deep learning systems requires systematic evaluation of their reliability in real-world scenarios. Traditional gradient-based adversarial attacks introduce small perturbations that rarely correspond to realistic failures and mainly assess robustness rather than functional behavior. Generative test generation methods offer an alternative but are often limited to simple datasets or constrained input domains. Although diffusion models enable high-fidelity image synthesis, their computational cost and limited controllability restrict their applicability to large-scale testing. We present HyNeA, a generative testing method that enables direct and efficient control over diffusion-based generation. HyNeA provides dataset-free controllability through hypernetworks, allowing targeted manipulation of the generative process without relying on architecture-specific conditioning mechanisms or dataset-driven adaptations such as fine-tuning. HyNeA employs a distinct training strategy that supports instance-level tuning to identify failure-inducing test cases without requiring datasets that explicitly contain examples of similar failures. This approach enables the targeted generation of realistic failure cases at substantially lower computational cost than search-based methods. Experimental results show that HyNeA improves controllability and test diversity compared to existing generative test generators and generalizes to domains where failure-labeled training data is unavailable.", "link": "http://arxiv.org/abs/2601.15041v1", "date": "2026-01-21", "relevancy": 2.1511, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5558}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5273}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperNet-Adaptation%20for%20Diffusion-Based%20Test%20Case%20Generation&body=Title%3A%20HyperNet-Adaptation%20for%20Diffusion-Based%20Test%20Case%20Generation%0AAuthor%3A%20Oliver%20Wei%C3%9Fl%20and%20Vincenzo%20Riccio%20and%20Severin%20Kacianka%20and%20Andrea%20Stocco%0AAbstract%3A%20The%20increasing%20deployment%20of%20deep%20learning%20systems%20requires%20systematic%20evaluation%20of%20their%20reliability%20in%20real-world%20scenarios.%20Traditional%20gradient-based%20adversarial%20attacks%20introduce%20small%20perturbations%20that%20rarely%20correspond%20to%20realistic%20failures%20and%20mainly%20assess%20robustness%20rather%20than%20functional%20behavior.%20Generative%20test%20generation%20methods%20offer%20an%20alternative%20but%20are%20often%20limited%20to%20simple%20datasets%20or%20constrained%20input%20domains.%20Although%20diffusion%20models%20enable%20high-fidelity%20image%20synthesis%2C%20their%20computational%20cost%20and%20limited%20controllability%20restrict%20their%20applicability%20to%20large-scale%20testing.%20We%20present%20HyNeA%2C%20a%20generative%20testing%20method%20that%20enables%20direct%20and%20efficient%20control%20over%20diffusion-based%20generation.%20HyNeA%20provides%20dataset-free%20controllability%20through%20hypernetworks%2C%20allowing%20targeted%20manipulation%20of%20the%20generative%20process%20without%20relying%20on%20architecture-specific%20conditioning%20mechanisms%20or%20dataset-driven%20adaptations%20such%20as%20fine-tuning.%20HyNeA%20employs%20a%20distinct%20training%20strategy%20that%20supports%20instance-level%20tuning%20to%20identify%20failure-inducing%20test%20cases%20without%20requiring%20datasets%20that%20explicitly%20contain%20examples%20of%20similar%20failures.%20This%20approach%20enables%20the%20targeted%20generation%20of%20realistic%20failure%20cases%20at%20substantially%20lower%20computational%20cost%20than%20search-based%20methods.%20Experimental%20results%20show%20that%20HyNeA%20improves%20controllability%20and%20test%20diversity%20compared%20to%20existing%20generative%20test%20generators%20and%20generalizes%20to%20domains%20where%20failure-labeled%20training%20data%20is%20unavailable.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperNet-Adaptation%2520for%2520Diffusion-Based%2520Test%2520Case%2520Generation%26entry.906535625%3DOliver%2520Wei%25C3%259Fl%2520and%2520Vincenzo%2520Riccio%2520and%2520Severin%2520Kacianka%2520and%2520Andrea%2520Stocco%26entry.1292438233%3DThe%2520increasing%2520deployment%2520of%2520deep%2520learning%2520systems%2520requires%2520systematic%2520evaluation%2520of%2520their%2520reliability%2520in%2520real-world%2520scenarios.%2520Traditional%2520gradient-based%2520adversarial%2520attacks%2520introduce%2520small%2520perturbations%2520that%2520rarely%2520correspond%2520to%2520realistic%2520failures%2520and%2520mainly%2520assess%2520robustness%2520rather%2520than%2520functional%2520behavior.%2520Generative%2520test%2520generation%2520methods%2520offer%2520an%2520alternative%2520but%2520are%2520often%2520limited%2520to%2520simple%2520datasets%2520or%2520constrained%2520input%2520domains.%2520Although%2520diffusion%2520models%2520enable%2520high-fidelity%2520image%2520synthesis%252C%2520their%2520computational%2520cost%2520and%2520limited%2520controllability%2520restrict%2520their%2520applicability%2520to%2520large-scale%2520testing.%2520We%2520present%2520HyNeA%252C%2520a%2520generative%2520testing%2520method%2520that%2520enables%2520direct%2520and%2520efficient%2520control%2520over%2520diffusion-based%2520generation.%2520HyNeA%2520provides%2520dataset-free%2520controllability%2520through%2520hypernetworks%252C%2520allowing%2520targeted%2520manipulation%2520of%2520the%2520generative%2520process%2520without%2520relying%2520on%2520architecture-specific%2520conditioning%2520mechanisms%2520or%2520dataset-driven%2520adaptations%2520such%2520as%2520fine-tuning.%2520HyNeA%2520employs%2520a%2520distinct%2520training%2520strategy%2520that%2520supports%2520instance-level%2520tuning%2520to%2520identify%2520failure-inducing%2520test%2520cases%2520without%2520requiring%2520datasets%2520that%2520explicitly%2520contain%2520examples%2520of%2520similar%2520failures.%2520This%2520approach%2520enables%2520the%2520targeted%2520generation%2520of%2520realistic%2520failure%2520cases%2520at%2520substantially%2520lower%2520computational%2520cost%2520than%2520search-based%2520methods.%2520Experimental%2520results%2520show%2520that%2520HyNeA%2520improves%2520controllability%2520and%2520test%2520diversity%2520compared%2520to%2520existing%2520generative%2520test%2520generators%2520and%2520generalizes%2520to%2520domains%2520where%2520failure-labeled%2520training%2520data%2520is%2520unavailable.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperNet-Adaptation%20for%20Diffusion-Based%20Test%20Case%20Generation&entry.906535625=Oliver%20Wei%C3%9Fl%20and%20Vincenzo%20Riccio%20and%20Severin%20Kacianka%20and%20Andrea%20Stocco&entry.1292438233=The%20increasing%20deployment%20of%20deep%20learning%20systems%20requires%20systematic%20evaluation%20of%20their%20reliability%20in%20real-world%20scenarios.%20Traditional%20gradient-based%20adversarial%20attacks%20introduce%20small%20perturbations%20that%20rarely%20correspond%20to%20realistic%20failures%20and%20mainly%20assess%20robustness%20rather%20than%20functional%20behavior.%20Generative%20test%20generation%20methods%20offer%20an%20alternative%20but%20are%20often%20limited%20to%20simple%20datasets%20or%20constrained%20input%20domains.%20Although%20diffusion%20models%20enable%20high-fidelity%20image%20synthesis%2C%20their%20computational%20cost%20and%20limited%20controllability%20restrict%20their%20applicability%20to%20large-scale%20testing.%20We%20present%20HyNeA%2C%20a%20generative%20testing%20method%20that%20enables%20direct%20and%20efficient%20control%20over%20diffusion-based%20generation.%20HyNeA%20provides%20dataset-free%20controllability%20through%20hypernetworks%2C%20allowing%20targeted%20manipulation%20of%20the%20generative%20process%20without%20relying%20on%20architecture-specific%20conditioning%20mechanisms%20or%20dataset-driven%20adaptations%20such%20as%20fine-tuning.%20HyNeA%20employs%20a%20distinct%20training%20strategy%20that%20supports%20instance-level%20tuning%20to%20identify%20failure-inducing%20test%20cases%20without%20requiring%20datasets%20that%20explicitly%20contain%20examples%20of%20similar%20failures.%20This%20approach%20enables%20the%20targeted%20generation%20of%20realistic%20failure%20cases%20at%20substantially%20lower%20computational%20cost%20than%20search-based%20methods.%20Experimental%20results%20show%20that%20HyNeA%20improves%20controllability%20and%20test%20diversity%20compared%20to%20existing%20generative%20test%20generators%20and%20generalizes%20to%20domains%20where%20failure-labeled%20training%20data%20is%20unavailable.&entry.1838667208=http%3A//arxiv.org/abs/2601.15041v1&entry.124074799=Read"},
{"title": "Deaf and Hard of Hearing Access to Intelligent Personal Assistants: Comparison of Voice-Based Options with an LLM-Powered Touch Interface", "author": "Paige S. DeVries and Michaela Okosi and Ming Li and Nora Dunphy. Gidey Gezae and Dante Conway and Abraham Glasser and Raja Kushalnagar and Christian Vogler", "abstract": "We investigate intelligent personal assistants (IPAs) accessibility for deaf and hard of hearing (DHH) people who can use their voice in everyday communication. The inability of IPAs to understand diverse accents including deaf speech renders them largely inaccessible to non-signing and speaking DHH individuals. Using an Echo Show, we compare the usability of natural language input via spoken English; with Alexa's automatic speech recognition and a Wizard-of-Oz setting with a trained facilitator re-speaking commands against that of a large language model (LLM)-assisted touch interface in a mixed-methods study. The touch method was navigated through an LLM-powered \"task prompter,\" which integrated the user's history and smart environment to suggest contextually-appropriate commands. Quantitative results showed no significant differences across both spoken English conditions vs LLM-assisted touch. Qualitative results showed variability in opinions on the usability of each method. Ultimately, it will be necessary to have robust deaf-accented speech recognized natively by IPAs.", "link": "http://arxiv.org/abs/2601.15209v1", "date": "2026-01-21", "relevancy": 2.1462, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4404}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deaf%20and%20Hard%20of%20Hearing%20Access%20to%20Intelligent%20Personal%20Assistants%3A%20Comparison%20of%20Voice-Based%20Options%20with%20an%20LLM-Powered%20Touch%20Interface&body=Title%3A%20Deaf%20and%20Hard%20of%20Hearing%20Access%20to%20Intelligent%20Personal%20Assistants%3A%20Comparison%20of%20Voice-Based%20Options%20with%20an%20LLM-Powered%20Touch%20Interface%0AAuthor%3A%20Paige%20S.%20DeVries%20and%20Michaela%20Okosi%20and%20Ming%20Li%20and%20Nora%20Dunphy.%20Gidey%20Gezae%20and%20Dante%20Conway%20and%20Abraham%20Glasser%20and%20Raja%20Kushalnagar%20and%20Christian%20Vogler%0AAbstract%3A%20We%20investigate%20intelligent%20personal%20assistants%20%28IPAs%29%20accessibility%20for%20deaf%20and%20hard%20of%20hearing%20%28DHH%29%20people%20who%20can%20use%20their%20voice%20in%20everyday%20communication.%20The%20inability%20of%20IPAs%20to%20understand%20diverse%20accents%20including%20deaf%20speech%20renders%20them%20largely%20inaccessible%20to%20non-signing%20and%20speaking%20DHH%20individuals.%20Using%20an%20Echo%20Show%2C%20we%20compare%20the%20usability%20of%20natural%20language%20input%20via%20spoken%20English%3B%20with%20Alexa%27s%20automatic%20speech%20recognition%20and%20a%20Wizard-of-Oz%20setting%20with%20a%20trained%20facilitator%20re-speaking%20commands%20against%20that%20of%20a%20large%20language%20model%20%28LLM%29-assisted%20touch%20interface%20in%20a%20mixed-methods%20study.%20The%20touch%20method%20was%20navigated%20through%20an%20LLM-powered%20%22task%20prompter%2C%22%20which%20integrated%20the%20user%27s%20history%20and%20smart%20environment%20to%20suggest%20contextually-appropriate%20commands.%20Quantitative%20results%20showed%20no%20significant%20differences%20across%20both%20spoken%20English%20conditions%20vs%20LLM-assisted%20touch.%20Qualitative%20results%20showed%20variability%20in%20opinions%20on%20the%20usability%20of%20each%20method.%20Ultimately%2C%20it%20will%20be%20necessary%20to%20have%20robust%20deaf-accented%20speech%20recognized%20natively%20by%20IPAs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeaf%2520and%2520Hard%2520of%2520Hearing%2520Access%2520to%2520Intelligent%2520Personal%2520Assistants%253A%2520Comparison%2520of%2520Voice-Based%2520Options%2520with%2520an%2520LLM-Powered%2520Touch%2520Interface%26entry.906535625%3DPaige%2520S.%2520DeVries%2520and%2520Michaela%2520Okosi%2520and%2520Ming%2520Li%2520and%2520Nora%2520Dunphy.%2520Gidey%2520Gezae%2520and%2520Dante%2520Conway%2520and%2520Abraham%2520Glasser%2520and%2520Raja%2520Kushalnagar%2520and%2520Christian%2520Vogler%26entry.1292438233%3DWe%2520investigate%2520intelligent%2520personal%2520assistants%2520%2528IPAs%2529%2520accessibility%2520for%2520deaf%2520and%2520hard%2520of%2520hearing%2520%2528DHH%2529%2520people%2520who%2520can%2520use%2520their%2520voice%2520in%2520everyday%2520communication.%2520The%2520inability%2520of%2520IPAs%2520to%2520understand%2520diverse%2520accents%2520including%2520deaf%2520speech%2520renders%2520them%2520largely%2520inaccessible%2520to%2520non-signing%2520and%2520speaking%2520DHH%2520individuals.%2520Using%2520an%2520Echo%2520Show%252C%2520we%2520compare%2520the%2520usability%2520of%2520natural%2520language%2520input%2520via%2520spoken%2520English%253B%2520with%2520Alexa%2527s%2520automatic%2520speech%2520recognition%2520and%2520a%2520Wizard-of-Oz%2520setting%2520with%2520a%2520trained%2520facilitator%2520re-speaking%2520commands%2520against%2520that%2520of%2520a%2520large%2520language%2520model%2520%2528LLM%2529-assisted%2520touch%2520interface%2520in%2520a%2520mixed-methods%2520study.%2520The%2520touch%2520method%2520was%2520navigated%2520through%2520an%2520LLM-powered%2520%2522task%2520prompter%252C%2522%2520which%2520integrated%2520the%2520user%2527s%2520history%2520and%2520smart%2520environment%2520to%2520suggest%2520contextually-appropriate%2520commands.%2520Quantitative%2520results%2520showed%2520no%2520significant%2520differences%2520across%2520both%2520spoken%2520English%2520conditions%2520vs%2520LLM-assisted%2520touch.%2520Qualitative%2520results%2520showed%2520variability%2520in%2520opinions%2520on%2520the%2520usability%2520of%2520each%2520method.%2520Ultimately%252C%2520it%2520will%2520be%2520necessary%2520to%2520have%2520robust%2520deaf-accented%2520speech%2520recognized%2520natively%2520by%2520IPAs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deaf%20and%20Hard%20of%20Hearing%20Access%20to%20Intelligent%20Personal%20Assistants%3A%20Comparison%20of%20Voice-Based%20Options%20with%20an%20LLM-Powered%20Touch%20Interface&entry.906535625=Paige%20S.%20DeVries%20and%20Michaela%20Okosi%20and%20Ming%20Li%20and%20Nora%20Dunphy.%20Gidey%20Gezae%20and%20Dante%20Conway%20and%20Abraham%20Glasser%20and%20Raja%20Kushalnagar%20and%20Christian%20Vogler&entry.1292438233=We%20investigate%20intelligent%20personal%20assistants%20%28IPAs%29%20accessibility%20for%20deaf%20and%20hard%20of%20hearing%20%28DHH%29%20people%20who%20can%20use%20their%20voice%20in%20everyday%20communication.%20The%20inability%20of%20IPAs%20to%20understand%20diverse%20accents%20including%20deaf%20speech%20renders%20them%20largely%20inaccessible%20to%20non-signing%20and%20speaking%20DHH%20individuals.%20Using%20an%20Echo%20Show%2C%20we%20compare%20the%20usability%20of%20natural%20language%20input%20via%20spoken%20English%3B%20with%20Alexa%27s%20automatic%20speech%20recognition%20and%20a%20Wizard-of-Oz%20setting%20with%20a%20trained%20facilitator%20re-speaking%20commands%20against%20that%20of%20a%20large%20language%20model%20%28LLM%29-assisted%20touch%20interface%20in%20a%20mixed-methods%20study.%20The%20touch%20method%20was%20navigated%20through%20an%20LLM-powered%20%22task%20prompter%2C%22%20which%20integrated%20the%20user%27s%20history%20and%20smart%20environment%20to%20suggest%20contextually-appropriate%20commands.%20Quantitative%20results%20showed%20no%20significant%20differences%20across%20both%20spoken%20English%20conditions%20vs%20LLM-assisted%20touch.%20Qualitative%20results%20showed%20variability%20in%20opinions%20on%20the%20usability%20of%20each%20method.%20Ultimately%2C%20it%20will%20be%20necessary%20to%20have%20robust%20deaf-accented%20speech%20recognized%20natively%20by%20IPAs.&entry.1838667208=http%3A//arxiv.org/abs/2601.15209v1&entry.124074799=Read"},
{"title": "Interoperable Architecture for Digital Identity Delegation for AI Agents with Blockchain Integration", "author": "David Ricardo Saavedra", "abstract": "Verifiable delegation in digital identity systems remains unresolved across centralized, federated, and self-sovereign identity (SSI) environments, particularly where both human users and autonomous AI agents must exercise and transfer authority without exposing primary credentials or private keys. We introduce a unified framework that enables bounded, auditable, and least-privilege delegation across heterogeneous identity ecosystems. The framework includes four key elements: Delegation Grants (DGs), first-class authorization artefacts that encode revocable transfers of authority with enforced scope reduction; a Canonical Verification Context (CVC) that normalizes verification requests into a single structured representation independent of protocols or credential formats; a layered reference architecture that separates trust anchoring, credential and proof validation, policy evaluation, and protocol mediation via a Trust Gateway; and an explicit treatment of blockchain anchoring as an optional integrity layer rather than a structural dependency. Together, these elements advance interoperable delegation and auditability and provide a foundation for future standardization, implementation, and integration of autonomous agents into trusted digital identity infrastructures.", "link": "http://arxiv.org/abs/2601.14982v1", "date": "2026-01-21", "relevancy": 2.1329, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.465}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4081}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interoperable%20Architecture%20for%20Digital%20Identity%20Delegation%20for%20AI%20Agents%20with%20Blockchain%20Integration&body=Title%3A%20Interoperable%20Architecture%20for%20Digital%20Identity%20Delegation%20for%20AI%20Agents%20with%20Blockchain%20Integration%0AAuthor%3A%20David%20Ricardo%20Saavedra%0AAbstract%3A%20Verifiable%20delegation%20in%20digital%20identity%20systems%20remains%20unresolved%20across%20centralized%2C%20federated%2C%20and%20self-sovereign%20identity%20%28SSI%29%20environments%2C%20particularly%20where%20both%20human%20users%20and%20autonomous%20AI%20agents%20must%20exercise%20and%20transfer%20authority%20without%20exposing%20primary%20credentials%20or%20private%20keys.%20We%20introduce%20a%20unified%20framework%20that%20enables%20bounded%2C%20auditable%2C%20and%20least-privilege%20delegation%20across%20heterogeneous%20identity%20ecosystems.%20The%20framework%20includes%20four%20key%20elements%3A%20Delegation%20Grants%20%28DGs%29%2C%20first-class%20authorization%20artefacts%20that%20encode%20revocable%20transfers%20of%20authority%20with%20enforced%20scope%20reduction%3B%20a%20Canonical%20Verification%20Context%20%28CVC%29%20that%20normalizes%20verification%20requests%20into%20a%20single%20structured%20representation%20independent%20of%20protocols%20or%20credential%20formats%3B%20a%20layered%20reference%20architecture%20that%20separates%20trust%20anchoring%2C%20credential%20and%20proof%20validation%2C%20policy%20evaluation%2C%20and%20protocol%20mediation%20via%20a%20Trust%20Gateway%3B%20and%20an%20explicit%20treatment%20of%20blockchain%20anchoring%20as%20an%20optional%20integrity%20layer%20rather%20than%20a%20structural%20dependency.%20Together%2C%20these%20elements%20advance%20interoperable%20delegation%20and%20auditability%20and%20provide%20a%20foundation%20for%20future%20standardization%2C%20implementation%2C%20and%20integration%20of%20autonomous%20agents%20into%20trusted%20digital%20identity%20infrastructures.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteroperable%2520Architecture%2520for%2520Digital%2520Identity%2520Delegation%2520for%2520AI%2520Agents%2520with%2520Blockchain%2520Integration%26entry.906535625%3DDavid%2520Ricardo%2520Saavedra%26entry.1292438233%3DVerifiable%2520delegation%2520in%2520digital%2520identity%2520systems%2520remains%2520unresolved%2520across%2520centralized%252C%2520federated%252C%2520and%2520self-sovereign%2520identity%2520%2528SSI%2529%2520environments%252C%2520particularly%2520where%2520both%2520human%2520users%2520and%2520autonomous%2520AI%2520agents%2520must%2520exercise%2520and%2520transfer%2520authority%2520without%2520exposing%2520primary%2520credentials%2520or%2520private%2520keys.%2520We%2520introduce%2520a%2520unified%2520framework%2520that%2520enables%2520bounded%252C%2520auditable%252C%2520and%2520least-privilege%2520delegation%2520across%2520heterogeneous%2520identity%2520ecosystems.%2520The%2520framework%2520includes%2520four%2520key%2520elements%253A%2520Delegation%2520Grants%2520%2528DGs%2529%252C%2520first-class%2520authorization%2520artefacts%2520that%2520encode%2520revocable%2520transfers%2520of%2520authority%2520with%2520enforced%2520scope%2520reduction%253B%2520a%2520Canonical%2520Verification%2520Context%2520%2528CVC%2529%2520that%2520normalizes%2520verification%2520requests%2520into%2520a%2520single%2520structured%2520representation%2520independent%2520of%2520protocols%2520or%2520credential%2520formats%253B%2520a%2520layered%2520reference%2520architecture%2520that%2520separates%2520trust%2520anchoring%252C%2520credential%2520and%2520proof%2520validation%252C%2520policy%2520evaluation%252C%2520and%2520protocol%2520mediation%2520via%2520a%2520Trust%2520Gateway%253B%2520and%2520an%2520explicit%2520treatment%2520of%2520blockchain%2520anchoring%2520as%2520an%2520optional%2520integrity%2520layer%2520rather%2520than%2520a%2520structural%2520dependency.%2520Together%252C%2520these%2520elements%2520advance%2520interoperable%2520delegation%2520and%2520auditability%2520and%2520provide%2520a%2520foundation%2520for%2520future%2520standardization%252C%2520implementation%252C%2520and%2520integration%2520of%2520autonomous%2520agents%2520into%2520trusted%2520digital%2520identity%2520infrastructures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interoperable%20Architecture%20for%20Digital%20Identity%20Delegation%20for%20AI%20Agents%20with%20Blockchain%20Integration&entry.906535625=David%20Ricardo%20Saavedra&entry.1292438233=Verifiable%20delegation%20in%20digital%20identity%20systems%20remains%20unresolved%20across%20centralized%2C%20federated%2C%20and%20self-sovereign%20identity%20%28SSI%29%20environments%2C%20particularly%20where%20both%20human%20users%20and%20autonomous%20AI%20agents%20must%20exercise%20and%20transfer%20authority%20without%20exposing%20primary%20credentials%20or%20private%20keys.%20We%20introduce%20a%20unified%20framework%20that%20enables%20bounded%2C%20auditable%2C%20and%20least-privilege%20delegation%20across%20heterogeneous%20identity%20ecosystems.%20The%20framework%20includes%20four%20key%20elements%3A%20Delegation%20Grants%20%28DGs%29%2C%20first-class%20authorization%20artefacts%20that%20encode%20revocable%20transfers%20of%20authority%20with%20enforced%20scope%20reduction%3B%20a%20Canonical%20Verification%20Context%20%28CVC%29%20that%20normalizes%20verification%20requests%20into%20a%20single%20structured%20representation%20independent%20of%20protocols%20or%20credential%20formats%3B%20a%20layered%20reference%20architecture%20that%20separates%20trust%20anchoring%2C%20credential%20and%20proof%20validation%2C%20policy%20evaluation%2C%20and%20protocol%20mediation%20via%20a%20Trust%20Gateway%3B%20and%20an%20explicit%20treatment%20of%20blockchain%20anchoring%20as%20an%20optional%20integrity%20layer%20rather%20than%20a%20structural%20dependency.%20Together%2C%20these%20elements%20advance%20interoperable%20delegation%20and%20auditability%20and%20provide%20a%20foundation%20for%20future%20standardization%2C%20implementation%2C%20and%20integration%20of%20autonomous%20agents%20into%20trusted%20digital%20identity%20infrastructures.&entry.1838667208=http%3A//arxiv.org/abs/2601.14982v1&entry.124074799=Read"},
{"title": "Finding Kissing Numbers with Game-theoretic Reinforcement Learning", "author": "Chengdong Ma and Th\u00e9o Tao Zhaowei and Pengyu Li and Minghao Liu and Haojun Chen and Zihao Mao and Yuan Cheng and Yuan Qi and Yaodong Yang", "abstract": "Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert's 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game that can be fully parallelized at large scale, and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions, discovers over 6000 new structures in 14 and other dimensions, and establishes new records for generalized kissing configurations under various angular constraints. These results demonstrate AI's power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.", "link": "http://arxiv.org/abs/2511.13391v2", "date": "2026-01-21", "relevancy": 2.1256, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4299}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4249}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Finding%20Kissing%20Numbers%20with%20Game-theoretic%20Reinforcement%20Learning&body=Title%3A%20Finding%20Kissing%20Numbers%20with%20Game-theoretic%20Reinforcement%20Learning%0AAuthor%3A%20Chengdong%20Ma%20and%20Th%C3%A9o%20Tao%20Zhaowei%20and%20Pengyu%20Li%20and%20Minghao%20Liu%20and%20Haojun%20Chen%20and%20Zihao%20Mao%20and%20Yuan%20Cheng%20and%20Yuan%20Qi%20and%20Yaodong%20Yang%0AAbstract%3A%20Since%20Isaac%20Newton%20first%20studied%20the%20Kissing%20Number%20Problem%20in%201694%2C%20determining%20the%20maximal%20number%20of%20non-overlapping%20spheres%20around%20a%20central%20sphere%20has%20remained%20a%20fundamental%20challenge.%20This%20problem%20represents%20the%20local%20analogue%20of%20Hilbert%27s%2018th%20problem%20on%20sphere%20packing%2C%20bridging%20geometry%2C%20number%20theory%2C%20and%20information%20theory.%20Although%20significant%20progress%20has%20been%20made%20through%20lattices%20and%20codes%2C%20the%20irregularities%20of%20high-dimensional%20geometry%20and%20exponentially%20growing%20combinatorial%20complexity%20beyond%208%20dimensions%2C%20which%20exceeds%20the%20complexity%20of%20Go%20game%2C%20limit%20the%20scalability%20of%20existing%20methods.%20Here%20we%20model%20this%20problem%20as%20a%20two-player%20matrix%20completion%20game%20that%20can%20be%20fully%20parallelized%20at%20large%20scale%2C%20and%20train%20the%20game-theoretic%20reinforcement%20learning%20system%2C%20PackingStar%2C%20to%20efficiently%20explore%20high-dimensional%20spaces.%20The%20matrix%20entries%20represent%20pairwise%20cosines%20of%20sphere%20center%20vectors%3B%20one%20player%20fills%20entries%20while%20another%20corrects%20suboptimal%20ones%2C%20jointly%20maximizing%20the%20matrix%20size%2C%20corresponding%20to%20the%20kissing%20number.%20This%20cooperative%20dynamics%20substantially%20improves%20sample%20quality%2C%20making%20the%20extremely%20large%20spaces%20tractable.%20PackingStar%20reproduces%20previous%20configurations%20and%20surpasses%20all%20human-known%20records%20from%20dimensions%2025%20to%2031%2C%20with%20the%20configuration%20in%2025%20dimensions%20geometrically%20corresponding%20to%20the%20Leech%20lattice%20and%20suggesting%20possible%20optimality.%20It%20achieves%20the%20first%20breakthrough%20beyond%20rational%20structures%20from%201971%20in%2013%20dimensions%2C%20discovers%20over%206000%20new%20structures%20in%2014%20and%20other%20dimensions%2C%20and%20establishes%20new%20records%20for%20generalized%20kissing%20configurations%20under%20various%20angular%20constraints.%20These%20results%20demonstrate%20AI%27s%20power%20to%20explore%20high-dimensional%20spaces%20beyond%20human%20intuition%20and%20open%20new%20pathways%20for%20the%20Kissing%20Number%20Problem%20and%20broader%20geometry%20problems.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13391v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinding%2520Kissing%2520Numbers%2520with%2520Game-theoretic%2520Reinforcement%2520Learning%26entry.906535625%3DChengdong%2520Ma%2520and%2520Th%25C3%25A9o%2520Tao%2520Zhaowei%2520and%2520Pengyu%2520Li%2520and%2520Minghao%2520Liu%2520and%2520Haojun%2520Chen%2520and%2520Zihao%2520Mao%2520and%2520Yuan%2520Cheng%2520and%2520Yuan%2520Qi%2520and%2520Yaodong%2520Yang%26entry.1292438233%3DSince%2520Isaac%2520Newton%2520first%2520studied%2520the%2520Kissing%2520Number%2520Problem%2520in%25201694%252C%2520determining%2520the%2520maximal%2520number%2520of%2520non-overlapping%2520spheres%2520around%2520a%2520central%2520sphere%2520has%2520remained%2520a%2520fundamental%2520challenge.%2520This%2520problem%2520represents%2520the%2520local%2520analogue%2520of%2520Hilbert%2527s%252018th%2520problem%2520on%2520sphere%2520packing%252C%2520bridging%2520geometry%252C%2520number%2520theory%252C%2520and%2520information%2520theory.%2520Although%2520significant%2520progress%2520has%2520been%2520made%2520through%2520lattices%2520and%2520codes%252C%2520the%2520irregularities%2520of%2520high-dimensional%2520geometry%2520and%2520exponentially%2520growing%2520combinatorial%2520complexity%2520beyond%25208%2520dimensions%252C%2520which%2520exceeds%2520the%2520complexity%2520of%2520Go%2520game%252C%2520limit%2520the%2520scalability%2520of%2520existing%2520methods.%2520Here%2520we%2520model%2520this%2520problem%2520as%2520a%2520two-player%2520matrix%2520completion%2520game%2520that%2520can%2520be%2520fully%2520parallelized%2520at%2520large%2520scale%252C%2520and%2520train%2520the%2520game-theoretic%2520reinforcement%2520learning%2520system%252C%2520PackingStar%252C%2520to%2520efficiently%2520explore%2520high-dimensional%2520spaces.%2520The%2520matrix%2520entries%2520represent%2520pairwise%2520cosines%2520of%2520sphere%2520center%2520vectors%253B%2520one%2520player%2520fills%2520entries%2520while%2520another%2520corrects%2520suboptimal%2520ones%252C%2520jointly%2520maximizing%2520the%2520matrix%2520size%252C%2520corresponding%2520to%2520the%2520kissing%2520number.%2520This%2520cooperative%2520dynamics%2520substantially%2520improves%2520sample%2520quality%252C%2520making%2520the%2520extremely%2520large%2520spaces%2520tractable.%2520PackingStar%2520reproduces%2520previous%2520configurations%2520and%2520surpasses%2520all%2520human-known%2520records%2520from%2520dimensions%252025%2520to%252031%252C%2520with%2520the%2520configuration%2520in%252025%2520dimensions%2520geometrically%2520corresponding%2520to%2520the%2520Leech%2520lattice%2520and%2520suggesting%2520possible%2520optimality.%2520It%2520achieves%2520the%2520first%2520breakthrough%2520beyond%2520rational%2520structures%2520from%25201971%2520in%252013%2520dimensions%252C%2520discovers%2520over%25206000%2520new%2520structures%2520in%252014%2520and%2520other%2520dimensions%252C%2520and%2520establishes%2520new%2520records%2520for%2520generalized%2520kissing%2520configurations%2520under%2520various%2520angular%2520constraints.%2520These%2520results%2520demonstrate%2520AI%2527s%2520power%2520to%2520explore%2520high-dimensional%2520spaces%2520beyond%2520human%2520intuition%2520and%2520open%2520new%2520pathways%2520for%2520the%2520Kissing%2520Number%2520Problem%2520and%2520broader%2520geometry%2520problems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13391v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finding%20Kissing%20Numbers%20with%20Game-theoretic%20Reinforcement%20Learning&entry.906535625=Chengdong%20Ma%20and%20Th%C3%A9o%20Tao%20Zhaowei%20and%20Pengyu%20Li%20and%20Minghao%20Liu%20and%20Haojun%20Chen%20and%20Zihao%20Mao%20and%20Yuan%20Cheng%20and%20Yuan%20Qi%20and%20Yaodong%20Yang&entry.1292438233=Since%20Isaac%20Newton%20first%20studied%20the%20Kissing%20Number%20Problem%20in%201694%2C%20determining%20the%20maximal%20number%20of%20non-overlapping%20spheres%20around%20a%20central%20sphere%20has%20remained%20a%20fundamental%20challenge.%20This%20problem%20represents%20the%20local%20analogue%20of%20Hilbert%27s%2018th%20problem%20on%20sphere%20packing%2C%20bridging%20geometry%2C%20number%20theory%2C%20and%20information%20theory.%20Although%20significant%20progress%20has%20been%20made%20through%20lattices%20and%20codes%2C%20the%20irregularities%20of%20high-dimensional%20geometry%20and%20exponentially%20growing%20combinatorial%20complexity%20beyond%208%20dimensions%2C%20which%20exceeds%20the%20complexity%20of%20Go%20game%2C%20limit%20the%20scalability%20of%20existing%20methods.%20Here%20we%20model%20this%20problem%20as%20a%20two-player%20matrix%20completion%20game%20that%20can%20be%20fully%20parallelized%20at%20large%20scale%2C%20and%20train%20the%20game-theoretic%20reinforcement%20learning%20system%2C%20PackingStar%2C%20to%20efficiently%20explore%20high-dimensional%20spaces.%20The%20matrix%20entries%20represent%20pairwise%20cosines%20of%20sphere%20center%20vectors%3B%20one%20player%20fills%20entries%20while%20another%20corrects%20suboptimal%20ones%2C%20jointly%20maximizing%20the%20matrix%20size%2C%20corresponding%20to%20the%20kissing%20number.%20This%20cooperative%20dynamics%20substantially%20improves%20sample%20quality%2C%20making%20the%20extremely%20large%20spaces%20tractable.%20PackingStar%20reproduces%20previous%20configurations%20and%20surpasses%20all%20human-known%20records%20from%20dimensions%2025%20to%2031%2C%20with%20the%20configuration%20in%2025%20dimensions%20geometrically%20corresponding%20to%20the%20Leech%20lattice%20and%20suggesting%20possible%20optimality.%20It%20achieves%20the%20first%20breakthrough%20beyond%20rational%20structures%20from%201971%20in%2013%20dimensions%2C%20discovers%20over%206000%20new%20structures%20in%2014%20and%20other%20dimensions%2C%20and%20establishes%20new%20records%20for%20generalized%20kissing%20configurations%20under%20various%20angular%20constraints.%20These%20results%20demonstrate%20AI%27s%20power%20to%20explore%20high-dimensional%20spaces%20beyond%20human%20intuition%20and%20open%20new%20pathways%20for%20the%20Kissing%20Number%20Problem%20and%20broader%20geometry%20problems.&entry.1838667208=http%3A//arxiv.org/abs/2511.13391v2&entry.124074799=Read"},
{"title": "Collision Probability Estimation for Optimization-based Vehicular Motion Planning", "author": "Leon Tolksdorf and Arturo Tejada and Christian Birkner and Nathan van de Wouw", "abstract": "Many motion planning algorithms for automated driving require estimating the probability of collision (POC) to account for uncertainties in the measurement and estimation of the motion of road users. Common POC estimation techniques often utilize sampling-based methods that suffer from computational inefficiency and a non-deterministic estimation, i.e., each estimation result for the same inputs is slightly different. In contrast, optimization-based motion planning algorithms require computationally efficient POC estimation, ideally using deterministic estimation, such that typical optimization algorithms for motion planning retain feasibility. Estimating the POC analytically, however, is challenging because it depends on understanding the collision conditions (e.g., vehicle's shape) and characterizing the uncertainty in motion prediction. In this paper, we propose an approach in which we estimate the POC between two vehicles by over-approximating their shapes by a multi-circular shape approximation. The position and heading of the predicted vehicle are modelled as random variables, contrasting with the literature, where the heading angle is often neglected. We guarantee that the provided POC is an over-approximation, which is essential in providing safety guarantees. For the particular case of Gaussian uncertainty in the position and heading, we present a computationally efficient algorithm for computing the POC estimate. This algorithm is then used in a path-following stochastic model predictive controller (SMPC) for motion planning. With the proposed algorithm, the SMPC generates reproducible trajectories while the controller retains its feasibility in the presented test cases and demonstrates the ability to handle varying levels of uncertainty.", "link": "http://arxiv.org/abs/2505.21161v3", "date": "2026-01-21", "relevancy": 2.1135, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5584}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.548}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collision%20Probability%20Estimation%20for%20Optimization-based%20Vehicular%20Motion%20Planning&body=Title%3A%20Collision%20Probability%20Estimation%20for%20Optimization-based%20Vehicular%20Motion%20Planning%0AAuthor%3A%20Leon%20Tolksdorf%20and%20Arturo%20Tejada%20and%20Christian%20Birkner%20and%20Nathan%20van%20de%20Wouw%0AAbstract%3A%20Many%20motion%20planning%20algorithms%20for%20automated%20driving%20require%20estimating%20the%20probability%20of%20collision%20%28POC%29%20to%20account%20for%20uncertainties%20in%20the%20measurement%20and%20estimation%20of%20the%20motion%20of%20road%20users.%20Common%20POC%20estimation%20techniques%20often%20utilize%20sampling-based%20methods%20that%20suffer%20from%20computational%20inefficiency%20and%20a%20non-deterministic%20estimation%2C%20i.e.%2C%20each%20estimation%20result%20for%20the%20same%20inputs%20is%20slightly%20different.%20In%20contrast%2C%20optimization-based%20motion%20planning%20algorithms%20require%20computationally%20efficient%20POC%20estimation%2C%20ideally%20using%20deterministic%20estimation%2C%20such%20that%20typical%20optimization%20algorithms%20for%20motion%20planning%20retain%20feasibility.%20Estimating%20the%20POC%20analytically%2C%20however%2C%20is%20challenging%20because%20it%20depends%20on%20understanding%20the%20collision%20conditions%20%28e.g.%2C%20vehicle%27s%20shape%29%20and%20characterizing%20the%20uncertainty%20in%20motion%20prediction.%20In%20this%20paper%2C%20we%20propose%20an%20approach%20in%20which%20we%20estimate%20the%20POC%20between%20two%20vehicles%20by%20over-approximating%20their%20shapes%20by%20a%20multi-circular%20shape%20approximation.%20The%20position%20and%20heading%20of%20the%20predicted%20vehicle%20are%20modelled%20as%20random%20variables%2C%20contrasting%20with%20the%20literature%2C%20where%20the%20heading%20angle%20is%20often%20neglected.%20We%20guarantee%20that%20the%20provided%20POC%20is%20an%20over-approximation%2C%20which%20is%20essential%20in%20providing%20safety%20guarantees.%20For%20the%20particular%20case%20of%20Gaussian%20uncertainty%20in%20the%20position%20and%20heading%2C%20we%20present%20a%20computationally%20efficient%20algorithm%20for%20computing%20the%20POC%20estimate.%20This%20algorithm%20is%20then%20used%20in%20a%20path-following%20stochastic%20model%20predictive%20controller%20%28SMPC%29%20for%20motion%20planning.%20With%20the%20proposed%20algorithm%2C%20the%20SMPC%20generates%20reproducible%20trajectories%20while%20the%20controller%20retains%20its%20feasibility%20in%20the%20presented%20test%20cases%20and%20demonstrates%20the%20ability%20to%20handle%20varying%20levels%20of%20uncertainty.%0ALink%3A%20http%3A//arxiv.org/abs/2505.21161v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollision%2520Probability%2520Estimation%2520for%2520Optimization-based%2520Vehicular%2520Motion%2520Planning%26entry.906535625%3DLeon%2520Tolksdorf%2520and%2520Arturo%2520Tejada%2520and%2520Christian%2520Birkner%2520and%2520Nathan%2520van%2520de%2520Wouw%26entry.1292438233%3DMany%2520motion%2520planning%2520algorithms%2520for%2520automated%2520driving%2520require%2520estimating%2520the%2520probability%2520of%2520collision%2520%2528POC%2529%2520to%2520account%2520for%2520uncertainties%2520in%2520the%2520measurement%2520and%2520estimation%2520of%2520the%2520motion%2520of%2520road%2520users.%2520Common%2520POC%2520estimation%2520techniques%2520often%2520utilize%2520sampling-based%2520methods%2520that%2520suffer%2520from%2520computational%2520inefficiency%2520and%2520a%2520non-deterministic%2520estimation%252C%2520i.e.%252C%2520each%2520estimation%2520result%2520for%2520the%2520same%2520inputs%2520is%2520slightly%2520different.%2520In%2520contrast%252C%2520optimization-based%2520motion%2520planning%2520algorithms%2520require%2520computationally%2520efficient%2520POC%2520estimation%252C%2520ideally%2520using%2520deterministic%2520estimation%252C%2520such%2520that%2520typical%2520optimization%2520algorithms%2520for%2520motion%2520planning%2520retain%2520feasibility.%2520Estimating%2520the%2520POC%2520analytically%252C%2520however%252C%2520is%2520challenging%2520because%2520it%2520depends%2520on%2520understanding%2520the%2520collision%2520conditions%2520%2528e.g.%252C%2520vehicle%2527s%2520shape%2529%2520and%2520characterizing%2520the%2520uncertainty%2520in%2520motion%2520prediction.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520approach%2520in%2520which%2520we%2520estimate%2520the%2520POC%2520between%2520two%2520vehicles%2520by%2520over-approximating%2520their%2520shapes%2520by%2520a%2520multi-circular%2520shape%2520approximation.%2520The%2520position%2520and%2520heading%2520of%2520the%2520predicted%2520vehicle%2520are%2520modelled%2520as%2520random%2520variables%252C%2520contrasting%2520with%2520the%2520literature%252C%2520where%2520the%2520heading%2520angle%2520is%2520often%2520neglected.%2520We%2520guarantee%2520that%2520the%2520provided%2520POC%2520is%2520an%2520over-approximation%252C%2520which%2520is%2520essential%2520in%2520providing%2520safety%2520guarantees.%2520For%2520the%2520particular%2520case%2520of%2520Gaussian%2520uncertainty%2520in%2520the%2520position%2520and%2520heading%252C%2520we%2520present%2520a%2520computationally%2520efficient%2520algorithm%2520for%2520computing%2520the%2520POC%2520estimate.%2520This%2520algorithm%2520is%2520then%2520used%2520in%2520a%2520path-following%2520stochastic%2520model%2520predictive%2520controller%2520%2528SMPC%2529%2520for%2520motion%2520planning.%2520With%2520the%2520proposed%2520algorithm%252C%2520the%2520SMPC%2520generates%2520reproducible%2520trajectories%2520while%2520the%2520controller%2520retains%2520its%2520feasibility%2520in%2520the%2520presented%2520test%2520cases%2520and%2520demonstrates%2520the%2520ability%2520to%2520handle%2520varying%2520levels%2520of%2520uncertainty.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21161v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collision%20Probability%20Estimation%20for%20Optimization-based%20Vehicular%20Motion%20Planning&entry.906535625=Leon%20Tolksdorf%20and%20Arturo%20Tejada%20and%20Christian%20Birkner%20and%20Nathan%20van%20de%20Wouw&entry.1292438233=Many%20motion%20planning%20algorithms%20for%20automated%20driving%20require%20estimating%20the%20probability%20of%20collision%20%28POC%29%20to%20account%20for%20uncertainties%20in%20the%20measurement%20and%20estimation%20of%20the%20motion%20of%20road%20users.%20Common%20POC%20estimation%20techniques%20often%20utilize%20sampling-based%20methods%20that%20suffer%20from%20computational%20inefficiency%20and%20a%20non-deterministic%20estimation%2C%20i.e.%2C%20each%20estimation%20result%20for%20the%20same%20inputs%20is%20slightly%20different.%20In%20contrast%2C%20optimization-based%20motion%20planning%20algorithms%20require%20computationally%20efficient%20POC%20estimation%2C%20ideally%20using%20deterministic%20estimation%2C%20such%20that%20typical%20optimization%20algorithms%20for%20motion%20planning%20retain%20feasibility.%20Estimating%20the%20POC%20analytically%2C%20however%2C%20is%20challenging%20because%20it%20depends%20on%20understanding%20the%20collision%20conditions%20%28e.g.%2C%20vehicle%27s%20shape%29%20and%20characterizing%20the%20uncertainty%20in%20motion%20prediction.%20In%20this%20paper%2C%20we%20propose%20an%20approach%20in%20which%20we%20estimate%20the%20POC%20between%20two%20vehicles%20by%20over-approximating%20their%20shapes%20by%20a%20multi-circular%20shape%20approximation.%20The%20position%20and%20heading%20of%20the%20predicted%20vehicle%20are%20modelled%20as%20random%20variables%2C%20contrasting%20with%20the%20literature%2C%20where%20the%20heading%20angle%20is%20often%20neglected.%20We%20guarantee%20that%20the%20provided%20POC%20is%20an%20over-approximation%2C%20which%20is%20essential%20in%20providing%20safety%20guarantees.%20For%20the%20particular%20case%20of%20Gaussian%20uncertainty%20in%20the%20position%20and%20heading%2C%20we%20present%20a%20computationally%20efficient%20algorithm%20for%20computing%20the%20POC%20estimate.%20This%20algorithm%20is%20then%20used%20in%20a%20path-following%20stochastic%20model%20predictive%20controller%20%28SMPC%29%20for%20motion%20planning.%20With%20the%20proposed%20algorithm%2C%20the%20SMPC%20generates%20reproducible%20trajectories%20while%20the%20controller%20retains%20its%20feasibility%20in%20the%20presented%20test%20cases%20and%20demonstrates%20the%20ability%20to%20handle%20varying%20levels%20of%20uncertainty.&entry.1838667208=http%3A//arxiv.org/abs/2505.21161v3&entry.124074799=Read"},
{"title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models", "author": "Zanlin Ni and Shenzhi Wang and Yang Yue and Tianyu Yu and Weilin Zhao and Yeguo Hua and Tianyi Chen and Jun Song and Cheng Yu and Bo Zheng and Gao Huang", "abstract": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap", "link": "http://arxiv.org/abs/2601.15165v1", "date": "2026-01-21", "relevancy": 2.1091, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5428}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5232}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Flexibility%20Trap%3A%20Why%20Arbitrary%20Order%20Limits%20Reasoning%20Potential%20in%20Diffusion%20Language%20Models&body=Title%3A%20The%20Flexibility%20Trap%3A%20Why%20Arbitrary%20Order%20Limits%20Reasoning%20Potential%20in%20Diffusion%20Language%20Models%0AAuthor%3A%20Zanlin%20Ni%20and%20Shenzhi%20Wang%20and%20Yang%20Yue%20and%20Tianyu%20Yu%20and%20Weilin%20Zhao%20and%20Yeguo%20Hua%20and%20Tianyi%20Chen%20and%20Jun%20Song%20and%20Cheng%20Yu%20and%20Bo%20Zheng%20and%20Gao%20Huang%0AAbstract%3A%20Diffusion%20Large%20Language%20Models%20%28dLLMs%29%20break%20the%20rigid%20left-to-right%20constraint%20of%20traditional%20LLMs%2C%20enabling%20token%20generation%20in%20arbitrary%20orders.%20Intuitively%2C%20this%20flexibility%20implies%20a%20solution%20space%20that%20strictly%20supersets%20the%20fixed%20autoregressive%20trajectory%2C%20theoretically%20unlocking%20superior%20reasoning%20potential%20for%20general%20tasks%20like%20mathematics%20and%20coding.%20Consequently%2C%20numerous%20works%20have%20leveraged%20reinforcement%20learning%20%28RL%29%20to%20elicit%20the%20reasoning%20capability%20of%20dLLMs.%20In%20this%20paper%2C%20we%20reveal%20a%20counter-intuitive%20reality%3A%20arbitrary%20order%20generation%2C%20in%20its%20current%20form%2C%20narrows%20rather%20than%20expands%20the%20reasoning%20boundary%20of%20dLLMs.%20We%20find%20that%20dLLMs%20tend%20to%20exploit%20this%20order%20flexibility%20to%20bypass%20high-uncertainty%20tokens%20that%20are%20crucial%20for%20exploration%2C%20leading%20to%20a%20premature%20collapse%20of%20the%20solution%20space.%20This%20observation%20challenges%20the%20premise%20of%20existing%20RL%20approaches%20for%20dLLMs%2C%20where%20considerable%20complexities%2C%20such%20as%20handling%20combinatorial%20trajectories%20and%20intractable%20likelihoods%2C%20are%20often%20devoted%20to%20preserving%20this%20flexibility.%20We%20demonstrate%20that%20effective%20reasoning%20is%20better%20elicited%20by%20intentionally%20forgoing%20arbitrary%20order%20and%20applying%20standard%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20instead.%20Our%20approach%2C%20JustGRPO%2C%20is%20minimalist%20yet%20surprisingly%20effective%20%28e.g.%2C%2089.1%25%20accuracy%20on%20GSM8K%29%20while%20fully%20retaining%20the%20parallel%20decoding%20ability%20of%20dLLMs.%20Project%20page%3A%20https%3A//nzl-thu.github.io/the-flexibility-trap%0ALink%3A%20http%3A//arxiv.org/abs/2601.15165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Flexibility%2520Trap%253A%2520Why%2520Arbitrary%2520Order%2520Limits%2520Reasoning%2520Potential%2520in%2520Diffusion%2520Language%2520Models%26entry.906535625%3DZanlin%2520Ni%2520and%2520Shenzhi%2520Wang%2520and%2520Yang%2520Yue%2520and%2520Tianyu%2520Yu%2520and%2520Weilin%2520Zhao%2520and%2520Yeguo%2520Hua%2520and%2520Tianyi%2520Chen%2520and%2520Jun%2520Song%2520and%2520Cheng%2520Yu%2520and%2520Bo%2520Zheng%2520and%2520Gao%2520Huang%26entry.1292438233%3DDiffusion%2520Large%2520Language%2520Models%2520%2528dLLMs%2529%2520break%2520the%2520rigid%2520left-to-right%2520constraint%2520of%2520traditional%2520LLMs%252C%2520enabling%2520token%2520generation%2520in%2520arbitrary%2520orders.%2520Intuitively%252C%2520this%2520flexibility%2520implies%2520a%2520solution%2520space%2520that%2520strictly%2520supersets%2520the%2520fixed%2520autoregressive%2520trajectory%252C%2520theoretically%2520unlocking%2520superior%2520reasoning%2520potential%2520for%2520general%2520tasks%2520like%2520mathematics%2520and%2520coding.%2520Consequently%252C%2520numerous%2520works%2520have%2520leveraged%2520reinforcement%2520learning%2520%2528RL%2529%2520to%2520elicit%2520the%2520reasoning%2520capability%2520of%2520dLLMs.%2520In%2520this%2520paper%252C%2520we%2520reveal%2520a%2520counter-intuitive%2520reality%253A%2520arbitrary%2520order%2520generation%252C%2520in%2520its%2520current%2520form%252C%2520narrows%2520rather%2520than%2520expands%2520the%2520reasoning%2520boundary%2520of%2520dLLMs.%2520We%2520find%2520that%2520dLLMs%2520tend%2520to%2520exploit%2520this%2520order%2520flexibility%2520to%2520bypass%2520high-uncertainty%2520tokens%2520that%2520are%2520crucial%2520for%2520exploration%252C%2520leading%2520to%2520a%2520premature%2520collapse%2520of%2520the%2520solution%2520space.%2520This%2520observation%2520challenges%2520the%2520premise%2520of%2520existing%2520RL%2520approaches%2520for%2520dLLMs%252C%2520where%2520considerable%2520complexities%252C%2520such%2520as%2520handling%2520combinatorial%2520trajectories%2520and%2520intractable%2520likelihoods%252C%2520are%2520often%2520devoted%2520to%2520preserving%2520this%2520flexibility.%2520We%2520demonstrate%2520that%2520effective%2520reasoning%2520is%2520better%2520elicited%2520by%2520intentionally%2520forgoing%2520arbitrary%2520order%2520and%2520applying%2520standard%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520instead.%2520Our%2520approach%252C%2520JustGRPO%252C%2520is%2520minimalist%2520yet%2520surprisingly%2520effective%2520%2528e.g.%252C%252089.1%2525%2520accuracy%2520on%2520GSM8K%2529%2520while%2520fully%2520retaining%2520the%2520parallel%2520decoding%2520ability%2520of%2520dLLMs.%2520Project%2520page%253A%2520https%253A//nzl-thu.github.io/the-flexibility-trap%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Flexibility%20Trap%3A%20Why%20Arbitrary%20Order%20Limits%20Reasoning%20Potential%20in%20Diffusion%20Language%20Models&entry.906535625=Zanlin%20Ni%20and%20Shenzhi%20Wang%20and%20Yang%20Yue%20and%20Tianyu%20Yu%20and%20Weilin%20Zhao%20and%20Yeguo%20Hua%20and%20Tianyi%20Chen%20and%20Jun%20Song%20and%20Cheng%20Yu%20and%20Bo%20Zheng%20and%20Gao%20Huang&entry.1292438233=Diffusion%20Large%20Language%20Models%20%28dLLMs%29%20break%20the%20rigid%20left-to-right%20constraint%20of%20traditional%20LLMs%2C%20enabling%20token%20generation%20in%20arbitrary%20orders.%20Intuitively%2C%20this%20flexibility%20implies%20a%20solution%20space%20that%20strictly%20supersets%20the%20fixed%20autoregressive%20trajectory%2C%20theoretically%20unlocking%20superior%20reasoning%20potential%20for%20general%20tasks%20like%20mathematics%20and%20coding.%20Consequently%2C%20numerous%20works%20have%20leveraged%20reinforcement%20learning%20%28RL%29%20to%20elicit%20the%20reasoning%20capability%20of%20dLLMs.%20In%20this%20paper%2C%20we%20reveal%20a%20counter-intuitive%20reality%3A%20arbitrary%20order%20generation%2C%20in%20its%20current%20form%2C%20narrows%20rather%20than%20expands%20the%20reasoning%20boundary%20of%20dLLMs.%20We%20find%20that%20dLLMs%20tend%20to%20exploit%20this%20order%20flexibility%20to%20bypass%20high-uncertainty%20tokens%20that%20are%20crucial%20for%20exploration%2C%20leading%20to%20a%20premature%20collapse%20of%20the%20solution%20space.%20This%20observation%20challenges%20the%20premise%20of%20existing%20RL%20approaches%20for%20dLLMs%2C%20where%20considerable%20complexities%2C%20such%20as%20handling%20combinatorial%20trajectories%20and%20intractable%20likelihoods%2C%20are%20often%20devoted%20to%20preserving%20this%20flexibility.%20We%20demonstrate%20that%20effective%20reasoning%20is%20better%20elicited%20by%20intentionally%20forgoing%20arbitrary%20order%20and%20applying%20standard%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20instead.%20Our%20approach%2C%20JustGRPO%2C%20is%20minimalist%20yet%20surprisingly%20effective%20%28e.g.%2C%2089.1%25%20accuracy%20on%20GSM8K%29%20while%20fully%20retaining%20the%20parallel%20decoding%20ability%20of%20dLLMs.%20Project%20page%3A%20https%3A//nzl-thu.github.io/the-flexibility-trap&entry.1838667208=http%3A//arxiv.org/abs/2601.15165v1&entry.124074799=Read"},
{"title": "Locomotion Dynamics of an Underactuated Three-Link Robotic Vehicle", "author": "Leonid Raz and Yizhar Or", "abstract": "The wheeled three-link snake robot is a well-known example of an underactuated system modelled using nonholonomic constraints, preventing lateral slippage (skid) of the wheels. A kinematically controlled configuration assumes that both joint angles are directly prescribed as phase-shifted periodic input. In another configuration of the robot, only one joint is periodically actuated while the second joint is passively governed by a visco-elastic torsion spring. In our work, we constructed the two configurations of the wheeled robot and conducted motion experiments under different actuation inputs. Analysis of the motion tracking measurements reveals a significant amount of wheels' skid, in contrast to the assumptions used in standard nonholonomic models. Therefore, we propose modified dynamic models which include wheels' skid and viscous friction forces, as well as rolling resistance. After parameter fitting, these dynamic models reach good agreement with the motion measurements, including effects of input's frequency on the mean speed and net displacement per period. This illustrates the importance of incorporating wheels' skid and friction into the system's model.", "link": "http://arxiv.org/abs/2407.21540v2", "date": "2026-01-21", "relevancy": 2.1065, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5644}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5503}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locomotion%20Dynamics%20of%20an%20Underactuated%20Three-Link%20Robotic%20Vehicle&body=Title%3A%20Locomotion%20Dynamics%20of%20an%20Underactuated%20Three-Link%20Robotic%20Vehicle%0AAuthor%3A%20Leonid%20Raz%20and%20Yizhar%20Or%0AAbstract%3A%20The%20wheeled%20three-link%20snake%20robot%20is%20a%20well-known%20example%20of%20an%20underactuated%20system%20modelled%20using%20nonholonomic%20constraints%2C%20preventing%20lateral%20slippage%20%28skid%29%20of%20the%20wheels.%20A%20kinematically%20controlled%20configuration%20assumes%20that%20both%20joint%20angles%20are%20directly%20prescribed%20as%20phase-shifted%20periodic%20input.%20In%20another%20configuration%20of%20the%20robot%2C%20only%20one%20joint%20is%20periodically%20actuated%20while%20the%20second%20joint%20is%20passively%20governed%20by%20a%20visco-elastic%20torsion%20spring.%20In%20our%20work%2C%20we%20constructed%20the%20two%20configurations%20of%20the%20wheeled%20robot%20and%20conducted%20motion%20experiments%20under%20different%20actuation%20inputs.%20Analysis%20of%20the%20motion%20tracking%20measurements%20reveals%20a%20significant%20amount%20of%20wheels%27%20skid%2C%20in%20contrast%20to%20the%20assumptions%20used%20in%20standard%20nonholonomic%20models.%20Therefore%2C%20we%20propose%20modified%20dynamic%20models%20which%20include%20wheels%27%20skid%20and%20viscous%20friction%20forces%2C%20as%20well%20as%20rolling%20resistance.%20After%20parameter%20fitting%2C%20these%20dynamic%20models%20reach%20good%20agreement%20with%20the%20motion%20measurements%2C%20including%20effects%20of%20input%27s%20frequency%20on%20the%20mean%20speed%20and%20net%20displacement%20per%20period.%20This%20illustrates%20the%20importance%20of%20incorporating%20wheels%27%20skid%20and%20friction%20into%20the%20system%27s%20model.%0ALink%3A%20http%3A//arxiv.org/abs/2407.21540v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocomotion%2520Dynamics%2520of%2520an%2520Underactuated%2520Three-Link%2520Robotic%2520Vehicle%26entry.906535625%3DLeonid%2520Raz%2520and%2520Yizhar%2520Or%26entry.1292438233%3DThe%2520wheeled%2520three-link%2520snake%2520robot%2520is%2520a%2520well-known%2520example%2520of%2520an%2520underactuated%2520system%2520modelled%2520using%2520nonholonomic%2520constraints%252C%2520preventing%2520lateral%2520slippage%2520%2528skid%2529%2520of%2520the%2520wheels.%2520A%2520kinematically%2520controlled%2520configuration%2520assumes%2520that%2520both%2520joint%2520angles%2520are%2520directly%2520prescribed%2520as%2520phase-shifted%2520periodic%2520input.%2520In%2520another%2520configuration%2520of%2520the%2520robot%252C%2520only%2520one%2520joint%2520is%2520periodically%2520actuated%2520while%2520the%2520second%2520joint%2520is%2520passively%2520governed%2520by%2520a%2520visco-elastic%2520torsion%2520spring.%2520In%2520our%2520work%252C%2520we%2520constructed%2520the%2520two%2520configurations%2520of%2520the%2520wheeled%2520robot%2520and%2520conducted%2520motion%2520experiments%2520under%2520different%2520actuation%2520inputs.%2520Analysis%2520of%2520the%2520motion%2520tracking%2520measurements%2520reveals%2520a%2520significant%2520amount%2520of%2520wheels%2527%2520skid%252C%2520in%2520contrast%2520to%2520the%2520assumptions%2520used%2520in%2520standard%2520nonholonomic%2520models.%2520Therefore%252C%2520we%2520propose%2520modified%2520dynamic%2520models%2520which%2520include%2520wheels%2527%2520skid%2520and%2520viscous%2520friction%2520forces%252C%2520as%2520well%2520as%2520rolling%2520resistance.%2520After%2520parameter%2520fitting%252C%2520these%2520dynamic%2520models%2520reach%2520good%2520agreement%2520with%2520the%2520motion%2520measurements%252C%2520including%2520effects%2520of%2520input%2527s%2520frequency%2520on%2520the%2520mean%2520speed%2520and%2520net%2520displacement%2520per%2520period.%2520This%2520illustrates%2520the%2520importance%2520of%2520incorporating%2520wheels%2527%2520skid%2520and%2520friction%2520into%2520the%2520system%2527s%2520model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21540v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locomotion%20Dynamics%20of%20an%20Underactuated%20Three-Link%20Robotic%20Vehicle&entry.906535625=Leonid%20Raz%20and%20Yizhar%20Or&entry.1292438233=The%20wheeled%20three-link%20snake%20robot%20is%20a%20well-known%20example%20of%20an%20underactuated%20system%20modelled%20using%20nonholonomic%20constraints%2C%20preventing%20lateral%20slippage%20%28skid%29%20of%20the%20wheels.%20A%20kinematically%20controlled%20configuration%20assumes%20that%20both%20joint%20angles%20are%20directly%20prescribed%20as%20phase-shifted%20periodic%20input.%20In%20another%20configuration%20of%20the%20robot%2C%20only%20one%20joint%20is%20periodically%20actuated%20while%20the%20second%20joint%20is%20passively%20governed%20by%20a%20visco-elastic%20torsion%20spring.%20In%20our%20work%2C%20we%20constructed%20the%20two%20configurations%20of%20the%20wheeled%20robot%20and%20conducted%20motion%20experiments%20under%20different%20actuation%20inputs.%20Analysis%20of%20the%20motion%20tracking%20measurements%20reveals%20a%20significant%20amount%20of%20wheels%27%20skid%2C%20in%20contrast%20to%20the%20assumptions%20used%20in%20standard%20nonholonomic%20models.%20Therefore%2C%20we%20propose%20modified%20dynamic%20models%20which%20include%20wheels%27%20skid%20and%20viscous%20friction%20forces%2C%20as%20well%20as%20rolling%20resistance.%20After%20parameter%20fitting%2C%20these%20dynamic%20models%20reach%20good%20agreement%20with%20the%20motion%20measurements%2C%20including%20effects%20of%20input%27s%20frequency%20on%20the%20mean%20speed%20and%20net%20displacement%20per%20period.%20This%20illustrates%20the%20importance%20of%20incorporating%20wheels%27%20skid%20and%20friction%20into%20the%20system%27s%20model.&entry.1838667208=http%3A//arxiv.org/abs/2407.21540v2&entry.124074799=Read"},
{"title": "Training-Free In-Context Forensic Chain for Image Manipulation Detection and Localization", "author": "Rui Chen and Bin Liu and Changtao Miao and Xinghao Wang and Yi Li and Tao Gong and Qi Chu and Nenghai Yu", "abstract": "Advances in image tampering pose serious security threats, underscoring the need for effective image manipulation localization (IML). While supervised IML achieves strong performance, it depends on costly pixel-level annotations. Existing weakly supervised or training-free alternatives often underperform and lack interpretability. We propose the In-Context Forensic Chain (ICFC), a training-free framework that leverages multi-modal large language models (MLLMs) for interpretable IML tasks. ICFC integrates an objectified rule construction with adaptive filtering to build a reliable knowledge base and a multi-step progressive reasoning pipeline that mirrors expert forensic workflows from coarse proposals to fine-grained forensics results. This design enables systematic exploitation of MLLM reasoning for image-level classification, pixel-level localization, and text-level interpretability. Across multiple benchmarks, ICFC not only surpasses state-of-the-art training-free methods but also achieves competitive or superior performance compared to weakly and fully supervised approaches.", "link": "http://arxiv.org/abs/2510.10111v3", "date": "2026-01-21", "relevancy": 2.1015, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5544}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.508}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20In-Context%20Forensic%20Chain%20for%20Image%20Manipulation%20Detection%20and%20Localization&body=Title%3A%20Training-Free%20In-Context%20Forensic%20Chain%20for%20Image%20Manipulation%20Detection%20and%20Localization%0AAuthor%3A%20Rui%20Chen%20and%20Bin%20Liu%20and%20Changtao%20Miao%20and%20Xinghao%20Wang%20and%20Yi%20Li%20and%20Tao%20Gong%20and%20Qi%20Chu%20and%20Nenghai%20Yu%0AAbstract%3A%20Advances%20in%20image%20tampering%20pose%20serious%20security%20threats%2C%20underscoring%20the%20need%20for%20effective%20image%20manipulation%20localization%20%28IML%29.%20While%20supervised%20IML%20achieves%20strong%20performance%2C%20it%20depends%20on%20costly%20pixel-level%20annotations.%20Existing%20weakly%20supervised%20or%20training-free%20alternatives%20often%20underperform%20and%20lack%20interpretability.%20We%20propose%20the%20In-Context%20Forensic%20Chain%20%28ICFC%29%2C%20a%20training-free%20framework%20that%20leverages%20multi-modal%20large%20language%20models%20%28MLLMs%29%20for%20interpretable%20IML%20tasks.%20ICFC%20integrates%20an%20objectified%20rule%20construction%20with%20adaptive%20filtering%20to%20build%20a%20reliable%20knowledge%20base%20and%20a%20multi-step%20progressive%20reasoning%20pipeline%20that%20mirrors%20expert%20forensic%20workflows%20from%20coarse%20proposals%20to%20fine-grained%20forensics%20results.%20This%20design%20enables%20systematic%20exploitation%20of%20MLLM%20reasoning%20for%20image-level%20classification%2C%20pixel-level%20localization%2C%20and%20text-level%20interpretability.%20Across%20multiple%20benchmarks%2C%20ICFC%20not%20only%20surpasses%20state-of-the-art%20training-free%20methods%20but%20also%20achieves%20competitive%20or%20superior%20performance%20compared%20to%20weakly%20and%20fully%20supervised%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2510.10111v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520In-Context%2520Forensic%2520Chain%2520for%2520Image%2520Manipulation%2520Detection%2520and%2520Localization%26entry.906535625%3DRui%2520Chen%2520and%2520Bin%2520Liu%2520and%2520Changtao%2520Miao%2520and%2520Xinghao%2520Wang%2520and%2520Yi%2520Li%2520and%2520Tao%2520Gong%2520and%2520Qi%2520Chu%2520and%2520Nenghai%2520Yu%26entry.1292438233%3DAdvances%2520in%2520image%2520tampering%2520pose%2520serious%2520security%2520threats%252C%2520underscoring%2520the%2520need%2520for%2520effective%2520image%2520manipulation%2520localization%2520%2528IML%2529.%2520While%2520supervised%2520IML%2520achieves%2520strong%2520performance%252C%2520it%2520depends%2520on%2520costly%2520pixel-level%2520annotations.%2520Existing%2520weakly%2520supervised%2520or%2520training-free%2520alternatives%2520often%2520underperform%2520and%2520lack%2520interpretability.%2520We%2520propose%2520the%2520In-Context%2520Forensic%2520Chain%2520%2528ICFC%2529%252C%2520a%2520training-free%2520framework%2520that%2520leverages%2520multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520for%2520interpretable%2520IML%2520tasks.%2520ICFC%2520integrates%2520an%2520objectified%2520rule%2520construction%2520with%2520adaptive%2520filtering%2520to%2520build%2520a%2520reliable%2520knowledge%2520base%2520and%2520a%2520multi-step%2520progressive%2520reasoning%2520pipeline%2520that%2520mirrors%2520expert%2520forensic%2520workflows%2520from%2520coarse%2520proposals%2520to%2520fine-grained%2520forensics%2520results.%2520This%2520design%2520enables%2520systematic%2520exploitation%2520of%2520MLLM%2520reasoning%2520for%2520image-level%2520classification%252C%2520pixel-level%2520localization%252C%2520and%2520text-level%2520interpretability.%2520Across%2520multiple%2520benchmarks%252C%2520ICFC%2520not%2520only%2520surpasses%2520state-of-the-art%2520training-free%2520methods%2520but%2520also%2520achieves%2520competitive%2520or%2520superior%2520performance%2520compared%2520to%2520weakly%2520and%2520fully%2520supervised%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10111v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20In-Context%20Forensic%20Chain%20for%20Image%20Manipulation%20Detection%20and%20Localization&entry.906535625=Rui%20Chen%20and%20Bin%20Liu%20and%20Changtao%20Miao%20and%20Xinghao%20Wang%20and%20Yi%20Li%20and%20Tao%20Gong%20and%20Qi%20Chu%20and%20Nenghai%20Yu&entry.1292438233=Advances%20in%20image%20tampering%20pose%20serious%20security%20threats%2C%20underscoring%20the%20need%20for%20effective%20image%20manipulation%20localization%20%28IML%29.%20While%20supervised%20IML%20achieves%20strong%20performance%2C%20it%20depends%20on%20costly%20pixel-level%20annotations.%20Existing%20weakly%20supervised%20or%20training-free%20alternatives%20often%20underperform%20and%20lack%20interpretability.%20We%20propose%20the%20In-Context%20Forensic%20Chain%20%28ICFC%29%2C%20a%20training-free%20framework%20that%20leverages%20multi-modal%20large%20language%20models%20%28MLLMs%29%20for%20interpretable%20IML%20tasks.%20ICFC%20integrates%20an%20objectified%20rule%20construction%20with%20adaptive%20filtering%20to%20build%20a%20reliable%20knowledge%20base%20and%20a%20multi-step%20progressive%20reasoning%20pipeline%20that%20mirrors%20expert%20forensic%20workflows%20from%20coarse%20proposals%20to%20fine-grained%20forensics%20results.%20This%20design%20enables%20systematic%20exploitation%20of%20MLLM%20reasoning%20for%20image-level%20classification%2C%20pixel-level%20localization%2C%20and%20text-level%20interpretability.%20Across%20multiple%20benchmarks%2C%20ICFC%20not%20only%20surpasses%20state-of-the-art%20training-free%20methods%20but%20also%20achieves%20competitive%20or%20superior%20performance%20compared%20to%20weakly%20and%20fully%20supervised%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2510.10111v3&entry.124074799=Read"},
{"title": "A Comparative Evaluation of Deep Learning Models for Speech Enhancement in Real-World Noisy Environments", "author": "Md Jahangir Alam Khondkar and Ajan Ahmed and Stephanie Schuckers and Masudul Haider Imtiaz", "abstract": "Speech enhancement, particularly denoising, is vital in improving the intelligibility and quality of speech signals for real-world applications, especially in noisy environments. While prior research has introduced various deep learning models for this purpose, many struggle to balance noise suppression, perceptual quality, and speaker-specific feature preservation, leaving a critical research gap in their comparative performance evaluation. This study benchmarks three state-of-the-art models Wave-U-Net, CMGAN, and U-Net, on diverse datasets such as SpEAR, VPQAD, and Clarkson datasets. These models were chosen due to their relevance in the literature and code accessibility. The evaluation reveals that U-Net achieves high noise suppression with SNR improvements of +71.96% on SpEAR, +64.83% on VPQAD, and +364.2% on the Clarkson dataset. CMGAN outperforms in perceptual quality, attaining the highest PESQ scores of 4.04 on SpEAR and 1.46 on VPQAD, making it well-suited for applications prioritizing natural and intelligible speech. Wave-U-Net balances these attributes with improvements in speaker-specific feature retention, evidenced by VeriSpeak score gains of +10.84% on SpEAR and +27.38% on VPQAD. This research indicates how advanced methods can optimize trade-offs between noise suppression, perceptual quality, and speaker recognition. The findings may contribute to advancing voice biometrics, forensic audio analysis, telecommunication, and speaker verification in challenging acoustic conditions.", "link": "http://arxiv.org/abs/2506.15000v2", "date": "2026-01-21", "relevancy": 2.0976, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5282}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5282}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comparative%20Evaluation%20of%20Deep%20Learning%20Models%20for%20Speech%20Enhancement%20in%20Real-World%20Noisy%20Environments&body=Title%3A%20A%20Comparative%20Evaluation%20of%20Deep%20Learning%20Models%20for%20Speech%20Enhancement%20in%20Real-World%20Noisy%20Environments%0AAuthor%3A%20Md%20Jahangir%20Alam%20Khondkar%20and%20Ajan%20Ahmed%20and%20Stephanie%20Schuckers%20and%20Masudul%20Haider%20Imtiaz%0AAbstract%3A%20Speech%20enhancement%2C%20particularly%20denoising%2C%20is%20vital%20in%20improving%20the%20intelligibility%20and%20quality%20of%20speech%20signals%20for%20real-world%20applications%2C%20especially%20in%20noisy%20environments.%20While%20prior%20research%20has%20introduced%20various%20deep%20learning%20models%20for%20this%20purpose%2C%20many%20struggle%20to%20balance%20noise%20suppression%2C%20perceptual%20quality%2C%20and%20speaker-specific%20feature%20preservation%2C%20leaving%20a%20critical%20research%20gap%20in%20their%20comparative%20performance%20evaluation.%20This%20study%20benchmarks%20three%20state-of-the-art%20models%20Wave-U-Net%2C%20CMGAN%2C%20and%20U-Net%2C%20on%20diverse%20datasets%20such%20as%20SpEAR%2C%20VPQAD%2C%20and%20Clarkson%20datasets.%20These%20models%20were%20chosen%20due%20to%20their%20relevance%20in%20the%20literature%20and%20code%20accessibility.%20The%20evaluation%20reveals%20that%20U-Net%20achieves%20high%20noise%20suppression%20with%20SNR%20improvements%20of%20%2B71.96%25%20on%20SpEAR%2C%20%2B64.83%25%20on%20VPQAD%2C%20and%20%2B364.2%25%20on%20the%20Clarkson%20dataset.%20CMGAN%20outperforms%20in%20perceptual%20quality%2C%20attaining%20the%20highest%20PESQ%20scores%20of%204.04%20on%20SpEAR%20and%201.46%20on%20VPQAD%2C%20making%20it%20well-suited%20for%20applications%20prioritizing%20natural%20and%20intelligible%20speech.%20Wave-U-Net%20balances%20these%20attributes%20with%20improvements%20in%20speaker-specific%20feature%20retention%2C%20evidenced%20by%20VeriSpeak%20score%20gains%20of%20%2B10.84%25%20on%20SpEAR%20and%20%2B27.38%25%20on%20VPQAD.%20This%20research%20indicates%20how%20advanced%20methods%20can%20optimize%20trade-offs%20between%20noise%20suppression%2C%20perceptual%20quality%2C%20and%20speaker%20recognition.%20The%20findings%20may%20contribute%20to%20advancing%20voice%20biometrics%2C%20forensic%20audio%20analysis%2C%20telecommunication%2C%20and%20speaker%20verification%20in%20challenging%20acoustic%20conditions.%0ALink%3A%20http%3A//arxiv.org/abs/2506.15000v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comparative%2520Evaluation%2520of%2520Deep%2520Learning%2520Models%2520for%2520Speech%2520Enhancement%2520in%2520Real-World%2520Noisy%2520Environments%26entry.906535625%3DMd%2520Jahangir%2520Alam%2520Khondkar%2520and%2520Ajan%2520Ahmed%2520and%2520Stephanie%2520Schuckers%2520and%2520Masudul%2520Haider%2520Imtiaz%26entry.1292438233%3DSpeech%2520enhancement%252C%2520particularly%2520denoising%252C%2520is%2520vital%2520in%2520improving%2520the%2520intelligibility%2520and%2520quality%2520of%2520speech%2520signals%2520for%2520real-world%2520applications%252C%2520especially%2520in%2520noisy%2520environments.%2520While%2520prior%2520research%2520has%2520introduced%2520various%2520deep%2520learning%2520models%2520for%2520this%2520purpose%252C%2520many%2520struggle%2520to%2520balance%2520noise%2520suppression%252C%2520perceptual%2520quality%252C%2520and%2520speaker-specific%2520feature%2520preservation%252C%2520leaving%2520a%2520critical%2520research%2520gap%2520in%2520their%2520comparative%2520performance%2520evaluation.%2520This%2520study%2520benchmarks%2520three%2520state-of-the-art%2520models%2520Wave-U-Net%252C%2520CMGAN%252C%2520and%2520U-Net%252C%2520on%2520diverse%2520datasets%2520such%2520as%2520SpEAR%252C%2520VPQAD%252C%2520and%2520Clarkson%2520datasets.%2520These%2520models%2520were%2520chosen%2520due%2520to%2520their%2520relevance%2520in%2520the%2520literature%2520and%2520code%2520accessibility.%2520The%2520evaluation%2520reveals%2520that%2520U-Net%2520achieves%2520high%2520noise%2520suppression%2520with%2520SNR%2520improvements%2520of%2520%252B71.96%2525%2520on%2520SpEAR%252C%2520%252B64.83%2525%2520on%2520VPQAD%252C%2520and%2520%252B364.2%2525%2520on%2520the%2520Clarkson%2520dataset.%2520CMGAN%2520outperforms%2520in%2520perceptual%2520quality%252C%2520attaining%2520the%2520highest%2520PESQ%2520scores%2520of%25204.04%2520on%2520SpEAR%2520and%25201.46%2520on%2520VPQAD%252C%2520making%2520it%2520well-suited%2520for%2520applications%2520prioritizing%2520natural%2520and%2520intelligible%2520speech.%2520Wave-U-Net%2520balances%2520these%2520attributes%2520with%2520improvements%2520in%2520speaker-specific%2520feature%2520retention%252C%2520evidenced%2520by%2520VeriSpeak%2520score%2520gains%2520of%2520%252B10.84%2525%2520on%2520SpEAR%2520and%2520%252B27.38%2525%2520on%2520VPQAD.%2520This%2520research%2520indicates%2520how%2520advanced%2520methods%2520can%2520optimize%2520trade-offs%2520between%2520noise%2520suppression%252C%2520perceptual%2520quality%252C%2520and%2520speaker%2520recognition.%2520The%2520findings%2520may%2520contribute%2520to%2520advancing%2520voice%2520biometrics%252C%2520forensic%2520audio%2520analysis%252C%2520telecommunication%252C%2520and%2520speaker%2520verification%2520in%2520challenging%2520acoustic%2520conditions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15000v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comparative%20Evaluation%20of%20Deep%20Learning%20Models%20for%20Speech%20Enhancement%20in%20Real-World%20Noisy%20Environments&entry.906535625=Md%20Jahangir%20Alam%20Khondkar%20and%20Ajan%20Ahmed%20and%20Stephanie%20Schuckers%20and%20Masudul%20Haider%20Imtiaz&entry.1292438233=Speech%20enhancement%2C%20particularly%20denoising%2C%20is%20vital%20in%20improving%20the%20intelligibility%20and%20quality%20of%20speech%20signals%20for%20real-world%20applications%2C%20especially%20in%20noisy%20environments.%20While%20prior%20research%20has%20introduced%20various%20deep%20learning%20models%20for%20this%20purpose%2C%20many%20struggle%20to%20balance%20noise%20suppression%2C%20perceptual%20quality%2C%20and%20speaker-specific%20feature%20preservation%2C%20leaving%20a%20critical%20research%20gap%20in%20their%20comparative%20performance%20evaluation.%20This%20study%20benchmarks%20three%20state-of-the-art%20models%20Wave-U-Net%2C%20CMGAN%2C%20and%20U-Net%2C%20on%20diverse%20datasets%20such%20as%20SpEAR%2C%20VPQAD%2C%20and%20Clarkson%20datasets.%20These%20models%20were%20chosen%20due%20to%20their%20relevance%20in%20the%20literature%20and%20code%20accessibility.%20The%20evaluation%20reveals%20that%20U-Net%20achieves%20high%20noise%20suppression%20with%20SNR%20improvements%20of%20%2B71.96%25%20on%20SpEAR%2C%20%2B64.83%25%20on%20VPQAD%2C%20and%20%2B364.2%25%20on%20the%20Clarkson%20dataset.%20CMGAN%20outperforms%20in%20perceptual%20quality%2C%20attaining%20the%20highest%20PESQ%20scores%20of%204.04%20on%20SpEAR%20and%201.46%20on%20VPQAD%2C%20making%20it%20well-suited%20for%20applications%20prioritizing%20natural%20and%20intelligible%20speech.%20Wave-U-Net%20balances%20these%20attributes%20with%20improvements%20in%20speaker-specific%20feature%20retention%2C%20evidenced%20by%20VeriSpeak%20score%20gains%20of%20%2B10.84%25%20on%20SpEAR%20and%20%2B27.38%25%20on%20VPQAD.%20This%20research%20indicates%20how%20advanced%20methods%20can%20optimize%20trade-offs%20between%20noise%20suppression%2C%20perceptual%20quality%2C%20and%20speaker%20recognition.%20The%20findings%20may%20contribute%20to%20advancing%20voice%20biometrics%2C%20forensic%20audio%20analysis%2C%20telecommunication%2C%20and%20speaker%20verification%20in%20challenging%20acoustic%20conditions.&entry.1838667208=http%3A//arxiv.org/abs/2506.15000v2&entry.124074799=Read"},
{"title": "DroneVLA: VLA based Aerial Manipulation", "author": "Fawad Mehboob and Monijesu James and Amir Habel and Jeffrin Sam and Miguel Altamirano Cabrera and Dzmitry Tsetserukou", "abstract": "As aerial platforms evolve from passive observers to active manipulators, the challenge shifts toward designing intuitive interfaces that allow non-expert users to command these systems naturally. This work introduces a novel concept of autonomous aerial manipulation system capable of interpreting high-level natural language commands to retrieve objects and deliver them to a human user. The system is intended to integrate a MediaPipe based on Grounding DINO and a Vision-Language-Action (VLA) model with a custom-built drone equipped with a 1-DOF gripper and an Intel RealSense RGB-D camera. VLA performs semantic reasoning to interpret the intent of a user prompt and generates a prioritized task queue for grasping of relevant objects in the scene. Grounding DINO and dynamic A* planning algorithm are used to navigate and safely relocate the object. To ensure safe and natural interaction during the handover phase, the system employs a human-centric controller driven by MediaPipe. This module provides real-time human pose estimation, allowing the drone to employ visual servoing to maintain a stable, distinct position directly in front of the user, facilitating a comfortable handover. We demonstrate the system's efficacy through real-world experiments for localization and navigation, which resulted in a 0.164m, 0.070m, and 0.084m of max, mean euclidean, and root-mean squared errors, respectively, highlighting the feasibility of VLA for aerial manipulation operations.", "link": "http://arxiv.org/abs/2601.13809v2", "date": "2026-01-21", "relevancy": 2.0959, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5324}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5247}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DroneVLA%3A%20VLA%20based%20Aerial%20Manipulation&body=Title%3A%20DroneVLA%3A%20VLA%20based%20Aerial%20Manipulation%0AAuthor%3A%20Fawad%20Mehboob%20and%20Monijesu%20James%20and%20Amir%20Habel%20and%20Jeffrin%20Sam%20and%20Miguel%20Altamirano%20Cabrera%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20As%20aerial%20platforms%20evolve%20from%20passive%20observers%20to%20active%20manipulators%2C%20the%20challenge%20shifts%20toward%20designing%20intuitive%20interfaces%20that%20allow%20non-expert%20users%20to%20command%20these%20systems%20naturally.%20This%20work%20introduces%20a%20novel%20concept%20of%20autonomous%20aerial%20manipulation%20system%20capable%20of%20interpreting%20high-level%20natural%20language%20commands%20to%20retrieve%20objects%20and%20deliver%20them%20to%20a%20human%20user.%20The%20system%20is%20intended%20to%20integrate%20a%20MediaPipe%20based%20on%20Grounding%20DINO%20and%20a%20Vision-Language-Action%20%28VLA%29%20model%20with%20a%20custom-built%20drone%20equipped%20with%20a%201-DOF%20gripper%20and%20an%20Intel%20RealSense%20RGB-D%20camera.%20VLA%20performs%20semantic%20reasoning%20to%20interpret%20the%20intent%20of%20a%20user%20prompt%20and%20generates%20a%20prioritized%20task%20queue%20for%20grasping%20of%20relevant%20objects%20in%20the%20scene.%20Grounding%20DINO%20and%20dynamic%20A%2A%20planning%20algorithm%20are%20used%20to%20navigate%20and%20safely%20relocate%20the%20object.%20To%20ensure%20safe%20and%20natural%20interaction%20during%20the%20handover%20phase%2C%20the%20system%20employs%20a%20human-centric%20controller%20driven%20by%20MediaPipe.%20This%20module%20provides%20real-time%20human%20pose%20estimation%2C%20allowing%20the%20drone%20to%20employ%20visual%20servoing%20to%20maintain%20a%20stable%2C%20distinct%20position%20directly%20in%20front%20of%20the%20user%2C%20facilitating%20a%20comfortable%20handover.%20We%20demonstrate%20the%20system%27s%20efficacy%20through%20real-world%20experiments%20for%20localization%20and%20navigation%2C%20which%20resulted%20in%20a%200.164m%2C%200.070m%2C%20and%200.084m%20of%20max%2C%20mean%20euclidean%2C%20and%20root-mean%20squared%20errors%2C%20respectively%2C%20highlighting%20the%20feasibility%20of%20VLA%20for%20aerial%20manipulation%20operations.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13809v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDroneVLA%253A%2520VLA%2520based%2520Aerial%2520Manipulation%26entry.906535625%3DFawad%2520Mehboob%2520and%2520Monijesu%2520James%2520and%2520Amir%2520Habel%2520and%2520Jeffrin%2520Sam%2520and%2520Miguel%2520Altamirano%2520Cabrera%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3DAs%2520aerial%2520platforms%2520evolve%2520from%2520passive%2520observers%2520to%2520active%2520manipulators%252C%2520the%2520challenge%2520shifts%2520toward%2520designing%2520intuitive%2520interfaces%2520that%2520allow%2520non-expert%2520users%2520to%2520command%2520these%2520systems%2520naturally.%2520This%2520work%2520introduces%2520a%2520novel%2520concept%2520of%2520autonomous%2520aerial%2520manipulation%2520system%2520capable%2520of%2520interpreting%2520high-level%2520natural%2520language%2520commands%2520to%2520retrieve%2520objects%2520and%2520deliver%2520them%2520to%2520a%2520human%2520user.%2520The%2520system%2520is%2520intended%2520to%2520integrate%2520a%2520MediaPipe%2520based%2520on%2520Grounding%2520DINO%2520and%2520a%2520Vision-Language-Action%2520%2528VLA%2529%2520model%2520with%2520a%2520custom-built%2520drone%2520equipped%2520with%2520a%25201-DOF%2520gripper%2520and%2520an%2520Intel%2520RealSense%2520RGB-D%2520camera.%2520VLA%2520performs%2520semantic%2520reasoning%2520to%2520interpret%2520the%2520intent%2520of%2520a%2520user%2520prompt%2520and%2520generates%2520a%2520prioritized%2520task%2520queue%2520for%2520grasping%2520of%2520relevant%2520objects%2520in%2520the%2520scene.%2520Grounding%2520DINO%2520and%2520dynamic%2520A%252A%2520planning%2520algorithm%2520are%2520used%2520to%2520navigate%2520and%2520safely%2520relocate%2520the%2520object.%2520To%2520ensure%2520safe%2520and%2520natural%2520interaction%2520during%2520the%2520handover%2520phase%252C%2520the%2520system%2520employs%2520a%2520human-centric%2520controller%2520driven%2520by%2520MediaPipe.%2520This%2520module%2520provides%2520real-time%2520human%2520pose%2520estimation%252C%2520allowing%2520the%2520drone%2520to%2520employ%2520visual%2520servoing%2520to%2520maintain%2520a%2520stable%252C%2520distinct%2520position%2520directly%2520in%2520front%2520of%2520the%2520user%252C%2520facilitating%2520a%2520comfortable%2520handover.%2520We%2520demonstrate%2520the%2520system%2527s%2520efficacy%2520through%2520real-world%2520experiments%2520for%2520localization%2520and%2520navigation%252C%2520which%2520resulted%2520in%2520a%25200.164m%252C%25200.070m%252C%2520and%25200.084m%2520of%2520max%252C%2520mean%2520euclidean%252C%2520and%2520root-mean%2520squared%2520errors%252C%2520respectively%252C%2520highlighting%2520the%2520feasibility%2520of%2520VLA%2520for%2520aerial%2520manipulation%2520operations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13809v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DroneVLA%3A%20VLA%20based%20Aerial%20Manipulation&entry.906535625=Fawad%20Mehboob%20and%20Monijesu%20James%20and%20Amir%20Habel%20and%20Jeffrin%20Sam%20and%20Miguel%20Altamirano%20Cabrera%20and%20Dzmitry%20Tsetserukou&entry.1292438233=As%20aerial%20platforms%20evolve%20from%20passive%20observers%20to%20active%20manipulators%2C%20the%20challenge%20shifts%20toward%20designing%20intuitive%20interfaces%20that%20allow%20non-expert%20users%20to%20command%20these%20systems%20naturally.%20This%20work%20introduces%20a%20novel%20concept%20of%20autonomous%20aerial%20manipulation%20system%20capable%20of%20interpreting%20high-level%20natural%20language%20commands%20to%20retrieve%20objects%20and%20deliver%20them%20to%20a%20human%20user.%20The%20system%20is%20intended%20to%20integrate%20a%20MediaPipe%20based%20on%20Grounding%20DINO%20and%20a%20Vision-Language-Action%20%28VLA%29%20model%20with%20a%20custom-built%20drone%20equipped%20with%20a%201-DOF%20gripper%20and%20an%20Intel%20RealSense%20RGB-D%20camera.%20VLA%20performs%20semantic%20reasoning%20to%20interpret%20the%20intent%20of%20a%20user%20prompt%20and%20generates%20a%20prioritized%20task%20queue%20for%20grasping%20of%20relevant%20objects%20in%20the%20scene.%20Grounding%20DINO%20and%20dynamic%20A%2A%20planning%20algorithm%20are%20used%20to%20navigate%20and%20safely%20relocate%20the%20object.%20To%20ensure%20safe%20and%20natural%20interaction%20during%20the%20handover%20phase%2C%20the%20system%20employs%20a%20human-centric%20controller%20driven%20by%20MediaPipe.%20This%20module%20provides%20real-time%20human%20pose%20estimation%2C%20allowing%20the%20drone%20to%20employ%20visual%20servoing%20to%20maintain%20a%20stable%2C%20distinct%20position%20directly%20in%20front%20of%20the%20user%2C%20facilitating%20a%20comfortable%20handover.%20We%20demonstrate%20the%20system%27s%20efficacy%20through%20real-world%20experiments%20for%20localization%20and%20navigation%2C%20which%20resulted%20in%20a%200.164m%2C%200.070m%2C%20and%200.084m%20of%20max%2C%20mean%20euclidean%2C%20and%20root-mean%20squared%20errors%2C%20respectively%2C%20highlighting%20the%20feasibility%20of%20VLA%20for%20aerial%20manipulation%20operations.&entry.1838667208=http%3A//arxiv.org/abs/2601.13809v2&entry.124074799=Read"},
{"title": "ZENITH: Automated Gradient Norm Informed Stochastic Optimization", "author": "Dhrubo Saha", "abstract": "Training deep computer vision models requires manual oversight or hyperparameter tuning of the learning rate (LR) schedule. While existing adaptive optimizers schedule the LR automatically, they suffer from computational and memory overhead, incompatibility with regularization, and suboptimal LR choices. In this work, we introduce the ZENITH (Zero-overhead Evolution using Norm-Informed Training History) optimizer, which adapts the LR using the temporal evolution of the gradient norm. Image classification experiments spanning 6 CNN architectures and 6 benchmarks demonstrate that ZENITH achieves higher test accuracy in lower wall-clock time than baselines. It also yielded superior mAP in object detection, keypoint detection, and instance segmentation on MS COCO using the R-CNN family of models. Furthermore, its compatibility with regularization enables even better generalization.", "link": "http://arxiv.org/abs/2601.15212v1", "date": "2026-01-21", "relevancy": 2.0752, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5362}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5113}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZENITH%3A%20Automated%20Gradient%20Norm%20Informed%20Stochastic%20Optimization&body=Title%3A%20ZENITH%3A%20Automated%20Gradient%20Norm%20Informed%20Stochastic%20Optimization%0AAuthor%3A%20Dhrubo%20Saha%0AAbstract%3A%20Training%20deep%20computer%20vision%20models%20requires%20manual%20oversight%20or%20hyperparameter%20tuning%20of%20the%20learning%20rate%20%28LR%29%20schedule.%20While%20existing%20adaptive%20optimizers%20schedule%20the%20LR%20automatically%2C%20they%20suffer%20from%20computational%20and%20memory%20overhead%2C%20incompatibility%20with%20regularization%2C%20and%20suboptimal%20LR%20choices.%20In%20this%20work%2C%20we%20introduce%20the%20ZENITH%20%28Zero-overhead%20Evolution%20using%20Norm-Informed%20Training%20History%29%20optimizer%2C%20which%20adapts%20the%20LR%20using%20the%20temporal%20evolution%20of%20the%20gradient%20norm.%20Image%20classification%20experiments%20spanning%206%20CNN%20architectures%20and%206%20benchmarks%20demonstrate%20that%20ZENITH%20achieves%20higher%20test%20accuracy%20in%20lower%20wall-clock%20time%20than%20baselines.%20It%20also%20yielded%20superior%20mAP%20in%20object%20detection%2C%20keypoint%20detection%2C%20and%20instance%20segmentation%20on%20MS%20COCO%20using%20the%20R-CNN%20family%20of%20models.%20Furthermore%2C%20its%20compatibility%20with%20regularization%20enables%20even%20better%20generalization.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15212v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZENITH%253A%2520Automated%2520Gradient%2520Norm%2520Informed%2520Stochastic%2520Optimization%26entry.906535625%3DDhrubo%2520Saha%26entry.1292438233%3DTraining%2520deep%2520computer%2520vision%2520models%2520requires%2520manual%2520oversight%2520or%2520hyperparameter%2520tuning%2520of%2520the%2520learning%2520rate%2520%2528LR%2529%2520schedule.%2520While%2520existing%2520adaptive%2520optimizers%2520schedule%2520the%2520LR%2520automatically%252C%2520they%2520suffer%2520from%2520computational%2520and%2520memory%2520overhead%252C%2520incompatibility%2520with%2520regularization%252C%2520and%2520suboptimal%2520LR%2520choices.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520ZENITH%2520%2528Zero-overhead%2520Evolution%2520using%2520Norm-Informed%2520Training%2520History%2529%2520optimizer%252C%2520which%2520adapts%2520the%2520LR%2520using%2520the%2520temporal%2520evolution%2520of%2520the%2520gradient%2520norm.%2520Image%2520classification%2520experiments%2520spanning%25206%2520CNN%2520architectures%2520and%25206%2520benchmarks%2520demonstrate%2520that%2520ZENITH%2520achieves%2520higher%2520test%2520accuracy%2520in%2520lower%2520wall-clock%2520time%2520than%2520baselines.%2520It%2520also%2520yielded%2520superior%2520mAP%2520in%2520object%2520detection%252C%2520keypoint%2520detection%252C%2520and%2520instance%2520segmentation%2520on%2520MS%2520COCO%2520using%2520the%2520R-CNN%2520family%2520of%2520models.%2520Furthermore%252C%2520its%2520compatibility%2520with%2520regularization%2520enables%2520even%2520better%2520generalization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15212v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZENITH%3A%20Automated%20Gradient%20Norm%20Informed%20Stochastic%20Optimization&entry.906535625=Dhrubo%20Saha&entry.1292438233=Training%20deep%20computer%20vision%20models%20requires%20manual%20oversight%20or%20hyperparameter%20tuning%20of%20the%20learning%20rate%20%28LR%29%20schedule.%20While%20existing%20adaptive%20optimizers%20schedule%20the%20LR%20automatically%2C%20they%20suffer%20from%20computational%20and%20memory%20overhead%2C%20incompatibility%20with%20regularization%2C%20and%20suboptimal%20LR%20choices.%20In%20this%20work%2C%20we%20introduce%20the%20ZENITH%20%28Zero-overhead%20Evolution%20using%20Norm-Informed%20Training%20History%29%20optimizer%2C%20which%20adapts%20the%20LR%20using%20the%20temporal%20evolution%20of%20the%20gradient%20norm.%20Image%20classification%20experiments%20spanning%206%20CNN%20architectures%20and%206%20benchmarks%20demonstrate%20that%20ZENITH%20achieves%20higher%20test%20accuracy%20in%20lower%20wall-clock%20time%20than%20baselines.%20It%20also%20yielded%20superior%20mAP%20in%20object%20detection%2C%20keypoint%20detection%2C%20and%20instance%20segmentation%20on%20MS%20COCO%20using%20the%20R-CNN%20family%20of%20models.%20Furthermore%2C%20its%20compatibility%20with%20regularization%20enables%20even%20better%20generalization.&entry.1838667208=http%3A//arxiv.org/abs/2601.15212v1&entry.124074799=Read"},
{"title": "Large-Scale Multidimensional Knowledge Profiling of Scientific Literature", "author": "Zhucun Xue and Jiangning Zhang and Juntao Jiang and Jinzhuo Liu and Haoyang He and Teng Hu and Xiaobin Hu and Guangming Yao and Yi Yuan and Yong Liu", "abstract": "The rapid expansion of research across machine learning, vision, and language has produced a volume of publications that is increasingly difficult to synthesize. Traditional bibliometric tools rely mainly on metadata and offer limited visibility into the semantic content of papers, making it hard to track how research themes evolve over time or how different areas influence one another. To obtain a clearer picture of recent developments, we compile a unified corpus of more than 100,000 papers from 22 major conferences between 2020 and 2025 and construct a multidimensional profiling pipeline to organize and analyze their textual content. By combining topic clustering, LLM-assisted parsing, and structured retrieval, we derive a comprehensive representation of research activity that supports the study of topic lifecycles, methodological transitions, dataset and model usage patterns, and institutional research directions. Our analysis highlights several notable shifts, including the growth of safety, multimodal reasoning, and agent-oriented studies, as well as the gradual stabilization of areas such as neural machine translation and graph-based methods. These findings provide an evidence-based view of how AI research is evolving and offer a resource for understanding broader trends and identifying emerging directions. Code and dataset: https://github.com/xzc-zju/Profiling_Scientific_Literature", "link": "http://arxiv.org/abs/2601.15170v1", "date": "2026-01-21", "relevancy": 2.0749, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5211}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5211}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large-Scale%20Multidimensional%20Knowledge%20Profiling%20of%20Scientific%20Literature&body=Title%3A%20Large-Scale%20Multidimensional%20Knowledge%20Profiling%20of%20Scientific%20Literature%0AAuthor%3A%20Zhucun%20Xue%20and%20Jiangning%20Zhang%20and%20Juntao%20Jiang%20and%20Jinzhuo%20Liu%20and%20Haoyang%20He%20and%20Teng%20Hu%20and%20Xiaobin%20Hu%20and%20Guangming%20Yao%20and%20Yi%20Yuan%20and%20Yong%20Liu%0AAbstract%3A%20The%20rapid%20expansion%20of%20research%20across%20machine%20learning%2C%20vision%2C%20and%20language%20has%20produced%20a%20volume%20of%20publications%20that%20is%20increasingly%20difficult%20to%20synthesize.%20Traditional%20bibliometric%20tools%20rely%20mainly%20on%20metadata%20and%20offer%20limited%20visibility%20into%20the%20semantic%20content%20of%20papers%2C%20making%20it%20hard%20to%20track%20how%20research%20themes%20evolve%20over%20time%20or%20how%20different%20areas%20influence%20one%20another.%20To%20obtain%20a%20clearer%20picture%20of%20recent%20developments%2C%20we%20compile%20a%20unified%20corpus%20of%20more%20than%20100%2C000%20papers%20from%2022%20major%20conferences%20between%202020%20and%202025%20and%20construct%20a%20multidimensional%20profiling%20pipeline%20to%20organize%20and%20analyze%20their%20textual%20content.%20By%20combining%20topic%20clustering%2C%20LLM-assisted%20parsing%2C%20and%20structured%20retrieval%2C%20we%20derive%20a%20comprehensive%20representation%20of%20research%20activity%20that%20supports%20the%20study%20of%20topic%20lifecycles%2C%20methodological%20transitions%2C%20dataset%20and%20model%20usage%20patterns%2C%20and%20institutional%20research%20directions.%20Our%20analysis%20highlights%20several%20notable%20shifts%2C%20including%20the%20growth%20of%20safety%2C%20multimodal%20reasoning%2C%20and%20agent-oriented%20studies%2C%20as%20well%20as%20the%20gradual%20stabilization%20of%20areas%20such%20as%20neural%20machine%20translation%20and%20graph-based%20methods.%20These%20findings%20provide%20an%20evidence-based%20view%20of%20how%20AI%20research%20is%20evolving%20and%20offer%20a%20resource%20for%20understanding%20broader%20trends%20and%20identifying%20emerging%20directions.%20Code%20and%20dataset%3A%20https%3A//github.com/xzc-zju/Profiling_Scientific_Literature%0ALink%3A%20http%3A//arxiv.org/abs/2601.15170v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge-Scale%2520Multidimensional%2520Knowledge%2520Profiling%2520of%2520Scientific%2520Literature%26entry.906535625%3DZhucun%2520Xue%2520and%2520Jiangning%2520Zhang%2520and%2520Juntao%2520Jiang%2520and%2520Jinzhuo%2520Liu%2520and%2520Haoyang%2520He%2520and%2520Teng%2520Hu%2520and%2520Xiaobin%2520Hu%2520and%2520Guangming%2520Yao%2520and%2520Yi%2520Yuan%2520and%2520Yong%2520Liu%26entry.1292438233%3DThe%2520rapid%2520expansion%2520of%2520research%2520across%2520machine%2520learning%252C%2520vision%252C%2520and%2520language%2520has%2520produced%2520a%2520volume%2520of%2520publications%2520that%2520is%2520increasingly%2520difficult%2520to%2520synthesize.%2520Traditional%2520bibliometric%2520tools%2520rely%2520mainly%2520on%2520metadata%2520and%2520offer%2520limited%2520visibility%2520into%2520the%2520semantic%2520content%2520of%2520papers%252C%2520making%2520it%2520hard%2520to%2520track%2520how%2520research%2520themes%2520evolve%2520over%2520time%2520or%2520how%2520different%2520areas%2520influence%2520one%2520another.%2520To%2520obtain%2520a%2520clearer%2520picture%2520of%2520recent%2520developments%252C%2520we%2520compile%2520a%2520unified%2520corpus%2520of%2520more%2520than%2520100%252C000%2520papers%2520from%252022%2520major%2520conferences%2520between%25202020%2520and%25202025%2520and%2520construct%2520a%2520multidimensional%2520profiling%2520pipeline%2520to%2520organize%2520and%2520analyze%2520their%2520textual%2520content.%2520By%2520combining%2520topic%2520clustering%252C%2520LLM-assisted%2520parsing%252C%2520and%2520structured%2520retrieval%252C%2520we%2520derive%2520a%2520comprehensive%2520representation%2520of%2520research%2520activity%2520that%2520supports%2520the%2520study%2520of%2520topic%2520lifecycles%252C%2520methodological%2520transitions%252C%2520dataset%2520and%2520model%2520usage%2520patterns%252C%2520and%2520institutional%2520research%2520directions.%2520Our%2520analysis%2520highlights%2520several%2520notable%2520shifts%252C%2520including%2520the%2520growth%2520of%2520safety%252C%2520multimodal%2520reasoning%252C%2520and%2520agent-oriented%2520studies%252C%2520as%2520well%2520as%2520the%2520gradual%2520stabilization%2520of%2520areas%2520such%2520as%2520neural%2520machine%2520translation%2520and%2520graph-based%2520methods.%2520These%2520findings%2520provide%2520an%2520evidence-based%2520view%2520of%2520how%2520AI%2520research%2520is%2520evolving%2520and%2520offer%2520a%2520resource%2520for%2520understanding%2520broader%2520trends%2520and%2520identifying%2520emerging%2520directions.%2520Code%2520and%2520dataset%253A%2520https%253A//github.com/xzc-zju/Profiling_Scientific_Literature%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15170v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-Scale%20Multidimensional%20Knowledge%20Profiling%20of%20Scientific%20Literature&entry.906535625=Zhucun%20Xue%20and%20Jiangning%20Zhang%20and%20Juntao%20Jiang%20and%20Jinzhuo%20Liu%20and%20Haoyang%20He%20and%20Teng%20Hu%20and%20Xiaobin%20Hu%20and%20Guangming%20Yao%20and%20Yi%20Yuan%20and%20Yong%20Liu&entry.1292438233=The%20rapid%20expansion%20of%20research%20across%20machine%20learning%2C%20vision%2C%20and%20language%20has%20produced%20a%20volume%20of%20publications%20that%20is%20increasingly%20difficult%20to%20synthesize.%20Traditional%20bibliometric%20tools%20rely%20mainly%20on%20metadata%20and%20offer%20limited%20visibility%20into%20the%20semantic%20content%20of%20papers%2C%20making%20it%20hard%20to%20track%20how%20research%20themes%20evolve%20over%20time%20or%20how%20different%20areas%20influence%20one%20another.%20To%20obtain%20a%20clearer%20picture%20of%20recent%20developments%2C%20we%20compile%20a%20unified%20corpus%20of%20more%20than%20100%2C000%20papers%20from%2022%20major%20conferences%20between%202020%20and%202025%20and%20construct%20a%20multidimensional%20profiling%20pipeline%20to%20organize%20and%20analyze%20their%20textual%20content.%20By%20combining%20topic%20clustering%2C%20LLM-assisted%20parsing%2C%20and%20structured%20retrieval%2C%20we%20derive%20a%20comprehensive%20representation%20of%20research%20activity%20that%20supports%20the%20study%20of%20topic%20lifecycles%2C%20methodological%20transitions%2C%20dataset%20and%20model%20usage%20patterns%2C%20and%20institutional%20research%20directions.%20Our%20analysis%20highlights%20several%20notable%20shifts%2C%20including%20the%20growth%20of%20safety%2C%20multimodal%20reasoning%2C%20and%20agent-oriented%20studies%2C%20as%20well%20as%20the%20gradual%20stabilization%20of%20areas%20such%20as%20neural%20machine%20translation%20and%20graph-based%20methods.%20These%20findings%20provide%20an%20evidence-based%20view%20of%20how%20AI%20research%20is%20evolving%20and%20offer%20a%20resource%20for%20understanding%20broader%20trends%20and%20identifying%20emerging%20directions.%20Code%20and%20dataset%3A%20https%3A//github.com/xzc-zju/Profiling_Scientific_Literature&entry.1838667208=http%3A//arxiv.org/abs/2601.15170v1&entry.124074799=Read"},
{"title": "Federated Transformer-GNN for Privacy-Preserving Brain Tumor Localization with Modality-Level Explainability", "author": "Andrea Protani and Riccardo Taiello and Marc Molina Van Den Bosch and Luigi Serio", "abstract": "Deep learning models for brain tumor analysis require large and diverse datasets that are often siloed across healthcare institutions due to privacy regulations. We present a federated learning framework for brain tumor localization that enables multi-institutional collaboration without sharing sensitive patient data. Our method extends a hybrid Transformer-Graph Neural Network architecture derived from prior decoder-free supervoxel GNNs and is deployed within CAFEIN\\textsuperscript{\\textregistered}, CERN's federated learning platform designed for healthcare environments. We provide an explainability analysis through Transformer attention mechanisms that reveals which MRI modalities drive the model predictions. Experiments on the BraTS dataset demonstrate a key finding: while isolated training on individual client data triggers early stopping well before reaching full training capacity, federated learning enables continued model improvement by leveraging distributed data, ultimately matching centralized performance. This result provides strong justification for federated learning when dealing with complex tasks and high-dimensional input data, as aggregating knowledge from multiple institutions significantly benefits the learning process. Our explainability analysis, validated through rigorous statistical testing on the full test set (paired t-tests with Bonferroni correction), reveals that deeper network layers significantly increase attention to T2 and FLAIR modalities ($p<0.001$, Cohen's $d$=1.50), aligning with clinical practice.", "link": "http://arxiv.org/abs/2601.15042v1", "date": "2026-01-21", "relevancy": 2.0722, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5192}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5175}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Transformer-GNN%20for%20Privacy-Preserving%20Brain%20Tumor%20Localization%20with%20Modality-Level%20Explainability&body=Title%3A%20Federated%20Transformer-GNN%20for%20Privacy-Preserving%20Brain%20Tumor%20Localization%20with%20Modality-Level%20Explainability%0AAuthor%3A%20Andrea%20Protani%20and%20Riccardo%20Taiello%20and%20Marc%20Molina%20Van%20Den%20Bosch%20and%20Luigi%20Serio%0AAbstract%3A%20Deep%20learning%20models%20for%20brain%20tumor%20analysis%20require%20large%20and%20diverse%20datasets%20that%20are%20often%20siloed%20across%20healthcare%20institutions%20due%20to%20privacy%20regulations.%20We%20present%20a%20federated%20learning%20framework%20for%20brain%20tumor%20localization%20that%20enables%20multi-institutional%20collaboration%20without%20sharing%20sensitive%20patient%20data.%20Our%20method%20extends%20a%20hybrid%20Transformer-Graph%20Neural%20Network%20architecture%20derived%20from%20prior%20decoder-free%20supervoxel%20GNNs%20and%20is%20deployed%20within%20CAFEIN%5Ctextsuperscript%7B%5Ctextregistered%7D%2C%20CERN%27s%20federated%20learning%20platform%20designed%20for%20healthcare%20environments.%20We%20provide%20an%20explainability%20analysis%20through%20Transformer%20attention%20mechanisms%20that%20reveals%20which%20MRI%20modalities%20drive%20the%20model%20predictions.%20Experiments%20on%20the%20BraTS%20dataset%20demonstrate%20a%20key%20finding%3A%20while%20isolated%20training%20on%20individual%20client%20data%20triggers%20early%20stopping%20well%20before%20reaching%20full%20training%20capacity%2C%20federated%20learning%20enables%20continued%20model%20improvement%20by%20leveraging%20distributed%20data%2C%20ultimately%20matching%20centralized%20performance.%20This%20result%20provides%20strong%20justification%20for%20federated%20learning%20when%20dealing%20with%20complex%20tasks%20and%20high-dimensional%20input%20data%2C%20as%20aggregating%20knowledge%20from%20multiple%20institutions%20significantly%20benefits%20the%20learning%20process.%20Our%20explainability%20analysis%2C%20validated%20through%20rigorous%20statistical%20testing%20on%20the%20full%20test%20set%20%28paired%20t-tests%20with%20Bonferroni%20correction%29%2C%20reveals%20that%20deeper%20network%20layers%20significantly%20increase%20attention%20to%20T2%20and%20FLAIR%20modalities%20%28%24p%3C0.001%24%2C%20Cohen%27s%20%24d%24%3D1.50%29%2C%20aligning%20with%20clinical%20practice.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Transformer-GNN%2520for%2520Privacy-Preserving%2520Brain%2520Tumor%2520Localization%2520with%2520Modality-Level%2520Explainability%26entry.906535625%3DAndrea%2520Protani%2520and%2520Riccardo%2520Taiello%2520and%2520Marc%2520Molina%2520Van%2520Den%2520Bosch%2520and%2520Luigi%2520Serio%26entry.1292438233%3DDeep%2520learning%2520models%2520for%2520brain%2520tumor%2520analysis%2520require%2520large%2520and%2520diverse%2520datasets%2520that%2520are%2520often%2520siloed%2520across%2520healthcare%2520institutions%2520due%2520to%2520privacy%2520regulations.%2520We%2520present%2520a%2520federated%2520learning%2520framework%2520for%2520brain%2520tumor%2520localization%2520that%2520enables%2520multi-institutional%2520collaboration%2520without%2520sharing%2520sensitive%2520patient%2520data.%2520Our%2520method%2520extends%2520a%2520hybrid%2520Transformer-Graph%2520Neural%2520Network%2520architecture%2520derived%2520from%2520prior%2520decoder-free%2520supervoxel%2520GNNs%2520and%2520is%2520deployed%2520within%2520CAFEIN%255Ctextsuperscript%257B%255Ctextregistered%257D%252C%2520CERN%2527s%2520federated%2520learning%2520platform%2520designed%2520for%2520healthcare%2520environments.%2520We%2520provide%2520an%2520explainability%2520analysis%2520through%2520Transformer%2520attention%2520mechanisms%2520that%2520reveals%2520which%2520MRI%2520modalities%2520drive%2520the%2520model%2520predictions.%2520Experiments%2520on%2520the%2520BraTS%2520dataset%2520demonstrate%2520a%2520key%2520finding%253A%2520while%2520isolated%2520training%2520on%2520individual%2520client%2520data%2520triggers%2520early%2520stopping%2520well%2520before%2520reaching%2520full%2520training%2520capacity%252C%2520federated%2520learning%2520enables%2520continued%2520model%2520improvement%2520by%2520leveraging%2520distributed%2520data%252C%2520ultimately%2520matching%2520centralized%2520performance.%2520This%2520result%2520provides%2520strong%2520justification%2520for%2520federated%2520learning%2520when%2520dealing%2520with%2520complex%2520tasks%2520and%2520high-dimensional%2520input%2520data%252C%2520as%2520aggregating%2520knowledge%2520from%2520multiple%2520institutions%2520significantly%2520benefits%2520the%2520learning%2520process.%2520Our%2520explainability%2520analysis%252C%2520validated%2520through%2520rigorous%2520statistical%2520testing%2520on%2520the%2520full%2520test%2520set%2520%2528paired%2520t-tests%2520with%2520Bonferroni%2520correction%2529%252C%2520reveals%2520that%2520deeper%2520network%2520layers%2520significantly%2520increase%2520attention%2520to%2520T2%2520and%2520FLAIR%2520modalities%2520%2528%2524p%253C0.001%2524%252C%2520Cohen%2527s%2520%2524d%2524%253D1.50%2529%252C%2520aligning%2520with%2520clinical%2520practice.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Transformer-GNN%20for%20Privacy-Preserving%20Brain%20Tumor%20Localization%20with%20Modality-Level%20Explainability&entry.906535625=Andrea%20Protani%20and%20Riccardo%20Taiello%20and%20Marc%20Molina%20Van%20Den%20Bosch%20and%20Luigi%20Serio&entry.1292438233=Deep%20learning%20models%20for%20brain%20tumor%20analysis%20require%20large%20and%20diverse%20datasets%20that%20are%20often%20siloed%20across%20healthcare%20institutions%20due%20to%20privacy%20regulations.%20We%20present%20a%20federated%20learning%20framework%20for%20brain%20tumor%20localization%20that%20enables%20multi-institutional%20collaboration%20without%20sharing%20sensitive%20patient%20data.%20Our%20method%20extends%20a%20hybrid%20Transformer-Graph%20Neural%20Network%20architecture%20derived%20from%20prior%20decoder-free%20supervoxel%20GNNs%20and%20is%20deployed%20within%20CAFEIN%5Ctextsuperscript%7B%5Ctextregistered%7D%2C%20CERN%27s%20federated%20learning%20platform%20designed%20for%20healthcare%20environments.%20We%20provide%20an%20explainability%20analysis%20through%20Transformer%20attention%20mechanisms%20that%20reveals%20which%20MRI%20modalities%20drive%20the%20model%20predictions.%20Experiments%20on%20the%20BraTS%20dataset%20demonstrate%20a%20key%20finding%3A%20while%20isolated%20training%20on%20individual%20client%20data%20triggers%20early%20stopping%20well%20before%20reaching%20full%20training%20capacity%2C%20federated%20learning%20enables%20continued%20model%20improvement%20by%20leveraging%20distributed%20data%2C%20ultimately%20matching%20centralized%20performance.%20This%20result%20provides%20strong%20justification%20for%20federated%20learning%20when%20dealing%20with%20complex%20tasks%20and%20high-dimensional%20input%20data%2C%20as%20aggregating%20knowledge%20from%20multiple%20institutions%20significantly%20benefits%20the%20learning%20process.%20Our%20explainability%20analysis%2C%20validated%20through%20rigorous%20statistical%20testing%20on%20the%20full%20test%20set%20%28paired%20t-tests%20with%20Bonferroni%20correction%29%2C%20reveals%20that%20deeper%20network%20layers%20significantly%20increase%20attention%20to%20T2%20and%20FLAIR%20modalities%20%28%24p%3C0.001%24%2C%20Cohen%27s%20%24d%24%3D1.50%29%2C%20aligning%20with%20clinical%20practice.&entry.1838667208=http%3A//arxiv.org/abs/2601.15042v1&entry.124074799=Read"},
{"title": "Multimodal system for skin cancer detection", "author": "Volodymyr Sydorskyi and Igor Krashenyi and Oleksii Yakubenko", "abstract": "Melanoma detection is vital for early diagnosis and effective treatment. While deep learning models on dermoscopic images have shown promise, they require specialized equipment, limiting their use in broader clinical settings. This study introduces a multi-modal melanoma detection system using conventional photo images, making it more accessible and versatile. Our system integrates image data with tabular metadata, such as patient demographics and lesion characteristics, to improve detection accuracy. It employs a multi-modal neural network combining image and metadata processing and supports a two-step model for cases with or without metadata. A three-stage pipeline further refines predictions by boosting algorithms and enhancing performance. To address the challenges of a highly imbalanced dataset, specific techniques were implemented to ensure robust training. An ablation study evaluated recent vision architectures, boosting algorithms, and loss functions, achieving a peak Partial ROC AUC of 0.18068 (0.2 maximum) and top-15 retrieval sensitivity of 0.78371. Results demonstrate that integrating photo images with metadata in a structured, multi-stage pipeline yields significant performance improvements. This system advances melanoma detection by providing a scalable, equipment-independent solution suitable for diverse healthcare environments, bridging the gap between specialized and general clinical practices.", "link": "http://arxiv.org/abs/2601.14822v1", "date": "2026-01-21", "relevancy": 2.0704, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5349}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5285}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20system%20for%20skin%20cancer%20detection&body=Title%3A%20Multimodal%20system%20for%20skin%20cancer%20detection%0AAuthor%3A%20Volodymyr%20Sydorskyi%20and%20Igor%20Krashenyi%20and%20Oleksii%20Yakubenko%0AAbstract%3A%20Melanoma%20detection%20is%20vital%20for%20early%20diagnosis%20and%20effective%20treatment.%20While%20deep%20learning%20models%20on%20dermoscopic%20images%20have%20shown%20promise%2C%20they%20require%20specialized%20equipment%2C%20limiting%20their%20use%20in%20broader%20clinical%20settings.%20This%20study%20introduces%20a%20multi-modal%20melanoma%20detection%20system%20using%20conventional%20photo%20images%2C%20making%20it%20more%20accessible%20and%20versatile.%20Our%20system%20integrates%20image%20data%20with%20tabular%20metadata%2C%20such%20as%20patient%20demographics%20and%20lesion%20characteristics%2C%20to%20improve%20detection%20accuracy.%20It%20employs%20a%20multi-modal%20neural%20network%20combining%20image%20and%20metadata%20processing%20and%20supports%20a%20two-step%20model%20for%20cases%20with%20or%20without%20metadata.%20A%20three-stage%20pipeline%20further%20refines%20predictions%20by%20boosting%20algorithms%20and%20enhancing%20performance.%20To%20address%20the%20challenges%20of%20a%20highly%20imbalanced%20dataset%2C%20specific%20techniques%20were%20implemented%20to%20ensure%20robust%20training.%20An%20ablation%20study%20evaluated%20recent%20vision%20architectures%2C%20boosting%20algorithms%2C%20and%20loss%20functions%2C%20achieving%20a%20peak%20Partial%20ROC%20AUC%20of%200.18068%20%280.2%20maximum%29%20and%20top-15%20retrieval%20sensitivity%20of%200.78371.%20Results%20demonstrate%20that%20integrating%20photo%20images%20with%20metadata%20in%20a%20structured%2C%20multi-stage%20pipeline%20yields%20significant%20performance%20improvements.%20This%20system%20advances%20melanoma%20detection%20by%20providing%20a%20scalable%2C%20equipment-independent%20solution%20suitable%20for%20diverse%20healthcare%20environments%2C%20bridging%20the%20gap%20between%20specialized%20and%20general%20clinical%20practices.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14822v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520system%2520for%2520skin%2520cancer%2520detection%26entry.906535625%3DVolodymyr%2520Sydorskyi%2520and%2520Igor%2520Krashenyi%2520and%2520Oleksii%2520Yakubenko%26entry.1292438233%3DMelanoma%2520detection%2520is%2520vital%2520for%2520early%2520diagnosis%2520and%2520effective%2520treatment.%2520While%2520deep%2520learning%2520models%2520on%2520dermoscopic%2520images%2520have%2520shown%2520promise%252C%2520they%2520require%2520specialized%2520equipment%252C%2520limiting%2520their%2520use%2520in%2520broader%2520clinical%2520settings.%2520This%2520study%2520introduces%2520a%2520multi-modal%2520melanoma%2520detection%2520system%2520using%2520conventional%2520photo%2520images%252C%2520making%2520it%2520more%2520accessible%2520and%2520versatile.%2520Our%2520system%2520integrates%2520image%2520data%2520with%2520tabular%2520metadata%252C%2520such%2520as%2520patient%2520demographics%2520and%2520lesion%2520characteristics%252C%2520to%2520improve%2520detection%2520accuracy.%2520It%2520employs%2520a%2520multi-modal%2520neural%2520network%2520combining%2520image%2520and%2520metadata%2520processing%2520and%2520supports%2520a%2520two-step%2520model%2520for%2520cases%2520with%2520or%2520without%2520metadata.%2520A%2520three-stage%2520pipeline%2520further%2520refines%2520predictions%2520by%2520boosting%2520algorithms%2520and%2520enhancing%2520performance.%2520To%2520address%2520the%2520challenges%2520of%2520a%2520highly%2520imbalanced%2520dataset%252C%2520specific%2520techniques%2520were%2520implemented%2520to%2520ensure%2520robust%2520training.%2520An%2520ablation%2520study%2520evaluated%2520recent%2520vision%2520architectures%252C%2520boosting%2520algorithms%252C%2520and%2520loss%2520functions%252C%2520achieving%2520a%2520peak%2520Partial%2520ROC%2520AUC%2520of%25200.18068%2520%25280.2%2520maximum%2529%2520and%2520top-15%2520retrieval%2520sensitivity%2520of%25200.78371.%2520Results%2520demonstrate%2520that%2520integrating%2520photo%2520images%2520with%2520metadata%2520in%2520a%2520structured%252C%2520multi-stage%2520pipeline%2520yields%2520significant%2520performance%2520improvements.%2520This%2520system%2520advances%2520melanoma%2520detection%2520by%2520providing%2520a%2520scalable%252C%2520equipment-independent%2520solution%2520suitable%2520for%2520diverse%2520healthcare%2520environments%252C%2520bridging%2520the%2520gap%2520between%2520specialized%2520and%2520general%2520clinical%2520practices.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14822v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20system%20for%20skin%20cancer%20detection&entry.906535625=Volodymyr%20Sydorskyi%20and%20Igor%20Krashenyi%20and%20Oleksii%20Yakubenko&entry.1292438233=Melanoma%20detection%20is%20vital%20for%20early%20diagnosis%20and%20effective%20treatment.%20While%20deep%20learning%20models%20on%20dermoscopic%20images%20have%20shown%20promise%2C%20they%20require%20specialized%20equipment%2C%20limiting%20their%20use%20in%20broader%20clinical%20settings.%20This%20study%20introduces%20a%20multi-modal%20melanoma%20detection%20system%20using%20conventional%20photo%20images%2C%20making%20it%20more%20accessible%20and%20versatile.%20Our%20system%20integrates%20image%20data%20with%20tabular%20metadata%2C%20such%20as%20patient%20demographics%20and%20lesion%20characteristics%2C%20to%20improve%20detection%20accuracy.%20It%20employs%20a%20multi-modal%20neural%20network%20combining%20image%20and%20metadata%20processing%20and%20supports%20a%20two-step%20model%20for%20cases%20with%20or%20without%20metadata.%20A%20three-stage%20pipeline%20further%20refines%20predictions%20by%20boosting%20algorithms%20and%20enhancing%20performance.%20To%20address%20the%20challenges%20of%20a%20highly%20imbalanced%20dataset%2C%20specific%20techniques%20were%20implemented%20to%20ensure%20robust%20training.%20An%20ablation%20study%20evaluated%20recent%20vision%20architectures%2C%20boosting%20algorithms%2C%20and%20loss%20functions%2C%20achieving%20a%20peak%20Partial%20ROC%20AUC%20of%200.18068%20%280.2%20maximum%29%20and%20top-15%20retrieval%20sensitivity%20of%200.78371.%20Results%20demonstrate%20that%20integrating%20photo%20images%20with%20metadata%20in%20a%20structured%2C%20multi-stage%20pipeline%20yields%20significant%20performance%20improvements.%20This%20system%20advances%20melanoma%20detection%20by%20providing%20a%20scalable%2C%20equipment-independent%20solution%20suitable%20for%20diverse%20healthcare%20environments%2C%20bridging%20the%20gap%20between%20specialized%20and%20general%20clinical%20practices.&entry.1838667208=http%3A//arxiv.org/abs/2601.14822v1&entry.124074799=Read"},
{"title": "Enhancing Few-Shot Out-of-Distribution Detection via the Refinement of Foreground and Background", "author": "Tianyu Li and Songyue Cai and Zongqian Wu and Ping Hu and Xiaofeng Zhu", "abstract": "CLIP-based foreground-background (FG-BG) decomposition methods have demonstrated remarkable effectiveness in improving few-shot out-of-distribution (OOD) detection performance. However, existing approaches still suffer from several limitations. For background regions obtained from decomposition, existing methods adopt a uniform suppression strategy for all patches, overlooking the varying contributions of different patches to the prediction. For foreground regions, existing methods fail to adequately consider that some local patches may exhibit appearance or semantic similarity to other classes, which may mislead the training process. To address these issues, we propose a new plug-and-play framework. This framework consists of three core components: (1) a Foreground-Background Decomposition module, which follows previous FG-BG methods to separate an image into foreground and background regions; (2) an Adaptive Background Suppression module, which adaptively weights patch classification entropy; and (3) a Confusable Foreground Rectification module, which identifies and rectifies confusable foreground patches. Extensive experimental results demonstrate that the proposed plug-and-play framework significantly improves the performance of existing FG-BG decomposition methods. Code is available at: https://github.com/lounwb/FoBoR.", "link": "http://arxiv.org/abs/2601.15065v1", "date": "2026-01-21", "relevancy": 2.0703, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5375}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.515}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Few-Shot%20Out-of-Distribution%20Detection%20via%20the%20Refinement%20of%20Foreground%20and%20Background&body=Title%3A%20Enhancing%20Few-Shot%20Out-of-Distribution%20Detection%20via%20the%20Refinement%20of%20Foreground%20and%20Background%0AAuthor%3A%20Tianyu%20Li%20and%20Songyue%20Cai%20and%20Zongqian%20Wu%20and%20Ping%20Hu%20and%20Xiaofeng%20Zhu%0AAbstract%3A%20CLIP-based%20foreground-background%20%28FG-BG%29%20decomposition%20methods%20have%20demonstrated%20remarkable%20effectiveness%20in%20improving%20few-shot%20out-of-distribution%20%28OOD%29%20detection%20performance.%20However%2C%20existing%20approaches%20still%20suffer%20from%20several%20limitations.%20For%20background%20regions%20obtained%20from%20decomposition%2C%20existing%20methods%20adopt%20a%20uniform%20suppression%20strategy%20for%20all%20patches%2C%20overlooking%20the%20varying%20contributions%20of%20different%20patches%20to%20the%20prediction.%20For%20foreground%20regions%2C%20existing%20methods%20fail%20to%20adequately%20consider%20that%20some%20local%20patches%20may%20exhibit%20appearance%20or%20semantic%20similarity%20to%20other%20classes%2C%20which%20may%20mislead%20the%20training%20process.%20To%20address%20these%20issues%2C%20we%20propose%20a%20new%20plug-and-play%20framework.%20This%20framework%20consists%20of%20three%20core%20components%3A%20%281%29%20a%20Foreground-Background%20Decomposition%20module%2C%20which%20follows%20previous%20FG-BG%20methods%20to%20separate%20an%20image%20into%20foreground%20and%20background%20regions%3B%20%282%29%20an%20Adaptive%20Background%20Suppression%20module%2C%20which%20adaptively%20weights%20patch%20classification%20entropy%3B%20and%20%283%29%20a%20Confusable%20Foreground%20Rectification%20module%2C%20which%20identifies%20and%20rectifies%20confusable%20foreground%20patches.%20Extensive%20experimental%20results%20demonstrate%20that%20the%20proposed%20plug-and-play%20framework%20significantly%20improves%20the%20performance%20of%20existing%20FG-BG%20decomposition%20methods.%20Code%20is%20available%20at%3A%20https%3A//github.com/lounwb/FoBoR.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15065v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Few-Shot%2520Out-of-Distribution%2520Detection%2520via%2520the%2520Refinement%2520of%2520Foreground%2520and%2520Background%26entry.906535625%3DTianyu%2520Li%2520and%2520Songyue%2520Cai%2520and%2520Zongqian%2520Wu%2520and%2520Ping%2520Hu%2520and%2520Xiaofeng%2520Zhu%26entry.1292438233%3DCLIP-based%2520foreground-background%2520%2528FG-BG%2529%2520decomposition%2520methods%2520have%2520demonstrated%2520remarkable%2520effectiveness%2520in%2520improving%2520few-shot%2520out-of-distribution%2520%2528OOD%2529%2520detection%2520performance.%2520However%252C%2520existing%2520approaches%2520still%2520suffer%2520from%2520several%2520limitations.%2520For%2520background%2520regions%2520obtained%2520from%2520decomposition%252C%2520existing%2520methods%2520adopt%2520a%2520uniform%2520suppression%2520strategy%2520for%2520all%2520patches%252C%2520overlooking%2520the%2520varying%2520contributions%2520of%2520different%2520patches%2520to%2520the%2520prediction.%2520For%2520foreground%2520regions%252C%2520existing%2520methods%2520fail%2520to%2520adequately%2520consider%2520that%2520some%2520local%2520patches%2520may%2520exhibit%2520appearance%2520or%2520semantic%2520similarity%2520to%2520other%2520classes%252C%2520which%2520may%2520mislead%2520the%2520training%2520process.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520new%2520plug-and-play%2520framework.%2520This%2520framework%2520consists%2520of%2520three%2520core%2520components%253A%2520%25281%2529%2520a%2520Foreground-Background%2520Decomposition%2520module%252C%2520which%2520follows%2520previous%2520FG-BG%2520methods%2520to%2520separate%2520an%2520image%2520into%2520foreground%2520and%2520background%2520regions%253B%2520%25282%2529%2520an%2520Adaptive%2520Background%2520Suppression%2520module%252C%2520which%2520adaptively%2520weights%2520patch%2520classification%2520entropy%253B%2520and%2520%25283%2529%2520a%2520Confusable%2520Foreground%2520Rectification%2520module%252C%2520which%2520identifies%2520and%2520rectifies%2520confusable%2520foreground%2520patches.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520plug-and-play%2520framework%2520significantly%2520improves%2520the%2520performance%2520of%2520existing%2520FG-BG%2520decomposition%2520methods.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/lounwb/FoBoR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15065v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Few-Shot%20Out-of-Distribution%20Detection%20via%20the%20Refinement%20of%20Foreground%20and%20Background&entry.906535625=Tianyu%20Li%20and%20Songyue%20Cai%20and%20Zongqian%20Wu%20and%20Ping%20Hu%20and%20Xiaofeng%20Zhu&entry.1292438233=CLIP-based%20foreground-background%20%28FG-BG%29%20decomposition%20methods%20have%20demonstrated%20remarkable%20effectiveness%20in%20improving%20few-shot%20out-of-distribution%20%28OOD%29%20detection%20performance.%20However%2C%20existing%20approaches%20still%20suffer%20from%20several%20limitations.%20For%20background%20regions%20obtained%20from%20decomposition%2C%20existing%20methods%20adopt%20a%20uniform%20suppression%20strategy%20for%20all%20patches%2C%20overlooking%20the%20varying%20contributions%20of%20different%20patches%20to%20the%20prediction.%20For%20foreground%20regions%2C%20existing%20methods%20fail%20to%20adequately%20consider%20that%20some%20local%20patches%20may%20exhibit%20appearance%20or%20semantic%20similarity%20to%20other%20classes%2C%20which%20may%20mislead%20the%20training%20process.%20To%20address%20these%20issues%2C%20we%20propose%20a%20new%20plug-and-play%20framework.%20This%20framework%20consists%20of%20three%20core%20components%3A%20%281%29%20a%20Foreground-Background%20Decomposition%20module%2C%20which%20follows%20previous%20FG-BG%20methods%20to%20separate%20an%20image%20into%20foreground%20and%20background%20regions%3B%20%282%29%20an%20Adaptive%20Background%20Suppression%20module%2C%20which%20adaptively%20weights%20patch%20classification%20entropy%3B%20and%20%283%29%20a%20Confusable%20Foreground%20Rectification%20module%2C%20which%20identifies%20and%20rectifies%20confusable%20foreground%20patches.%20Extensive%20experimental%20results%20demonstrate%20that%20the%20proposed%20plug-and-play%20framework%20significantly%20improves%20the%20performance%20of%20existing%20FG-BG%20decomposition%20methods.%20Code%20is%20available%20at%3A%20https%3A//github.com/lounwb/FoBoR.&entry.1838667208=http%3A//arxiv.org/abs/2601.15065v1&entry.124074799=Read"},
{"title": "Nonnegative Low-rank Matrix Recovery Can Have Spurious Local Minima", "author": "Richard Y. Zhang", "abstract": "Low-rank matrix recovery is well-known to exhibit benign nonconvexity under the restricted isometry property (RIP): every second-order critical point is globally optimal, so local methods provably recover the ground truth. Motivated by the strong empirical performance of projected gradient methods for nonnegative low-rank recovery problems, we investigate whether this benign geometry persists when the factor matrices are constrained to be elementwise nonnegative. In the simple setting of a rank-1 nonnegative ground truth, we confirm that benign nonconvexity holds in the fully-observed case with RIP constant $\u03b4=0$. This benign nonconvexity, however, is unstable. It fails to extend to the partially-observed case with any arbitrarily small RIP constant $\u03b4>0$, and to higher-rank ground truths $r^{\\star}>1$, regardless of how much the search rank $r\\ge r^{\\star}$ is overparameterized. Together, these results undermine the standard stability-based explanation for the empirical success of nonconvex methods and suggest that fundamentally different tools are needed to analyze nonnegative low-rank recovery.", "link": "http://arxiv.org/abs/2505.03717v2", "date": "2026-01-21", "relevancy": 2.066, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4376}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4047}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonnegative%20Low-rank%20Matrix%20Recovery%20Can%20Have%20Spurious%20Local%20Minima&body=Title%3A%20Nonnegative%20Low-rank%20Matrix%20Recovery%20Can%20Have%20Spurious%20Local%20Minima%0AAuthor%3A%20Richard%20Y.%20Zhang%0AAbstract%3A%20Low-rank%20matrix%20recovery%20is%20well-known%20to%20exhibit%20benign%20nonconvexity%20under%20the%20restricted%20isometry%20property%20%28RIP%29%3A%20every%20second-order%20critical%20point%20is%20globally%20optimal%2C%20so%20local%20methods%20provably%20recover%20the%20ground%20truth.%20Motivated%20by%20the%20strong%20empirical%20performance%20of%20projected%20gradient%20methods%20for%20nonnegative%20low-rank%20recovery%20problems%2C%20we%20investigate%20whether%20this%20benign%20geometry%20persists%20when%20the%20factor%20matrices%20are%20constrained%20to%20be%20elementwise%20nonnegative.%20In%20the%20simple%20setting%20of%20a%20rank-1%20nonnegative%20ground%20truth%2C%20we%20confirm%20that%20benign%20nonconvexity%20holds%20in%20the%20fully-observed%20case%20with%20RIP%20constant%20%24%CE%B4%3D0%24.%20This%20benign%20nonconvexity%2C%20however%2C%20is%20unstable.%20It%20fails%20to%20extend%20to%20the%20partially-observed%20case%20with%20any%20arbitrarily%20small%20RIP%20constant%20%24%CE%B4%3E0%24%2C%20and%20to%20higher-rank%20ground%20truths%20%24r%5E%7B%5Cstar%7D%3E1%24%2C%20regardless%20of%20how%20much%20the%20search%20rank%20%24r%5Cge%20r%5E%7B%5Cstar%7D%24%20is%20overparameterized.%20Together%2C%20these%20results%20undermine%20the%20standard%20stability-based%20explanation%20for%20the%20empirical%20success%20of%20nonconvex%20methods%20and%20suggest%20that%20fundamentally%20different%20tools%20are%20needed%20to%20analyze%20nonnegative%20low-rank%20recovery.%0ALink%3A%20http%3A//arxiv.org/abs/2505.03717v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonnegative%2520Low-rank%2520Matrix%2520Recovery%2520Can%2520Have%2520Spurious%2520Local%2520Minima%26entry.906535625%3DRichard%2520Y.%2520Zhang%26entry.1292438233%3DLow-rank%2520matrix%2520recovery%2520is%2520well-known%2520to%2520exhibit%2520benign%2520nonconvexity%2520under%2520the%2520restricted%2520isometry%2520property%2520%2528RIP%2529%253A%2520every%2520second-order%2520critical%2520point%2520is%2520globally%2520optimal%252C%2520so%2520local%2520methods%2520provably%2520recover%2520the%2520ground%2520truth.%2520Motivated%2520by%2520the%2520strong%2520empirical%2520performance%2520of%2520projected%2520gradient%2520methods%2520for%2520nonnegative%2520low-rank%2520recovery%2520problems%252C%2520we%2520investigate%2520whether%2520this%2520benign%2520geometry%2520persists%2520when%2520the%2520factor%2520matrices%2520are%2520constrained%2520to%2520be%2520elementwise%2520nonnegative.%2520In%2520the%2520simple%2520setting%2520of%2520a%2520rank-1%2520nonnegative%2520ground%2520truth%252C%2520we%2520confirm%2520that%2520benign%2520nonconvexity%2520holds%2520in%2520the%2520fully-observed%2520case%2520with%2520RIP%2520constant%2520%2524%25CE%25B4%253D0%2524.%2520This%2520benign%2520nonconvexity%252C%2520however%252C%2520is%2520unstable.%2520It%2520fails%2520to%2520extend%2520to%2520the%2520partially-observed%2520case%2520with%2520any%2520arbitrarily%2520small%2520RIP%2520constant%2520%2524%25CE%25B4%253E0%2524%252C%2520and%2520to%2520higher-rank%2520ground%2520truths%2520%2524r%255E%257B%255Cstar%257D%253E1%2524%252C%2520regardless%2520of%2520how%2520much%2520the%2520search%2520rank%2520%2524r%255Cge%2520r%255E%257B%255Cstar%257D%2524%2520is%2520overparameterized.%2520Together%252C%2520these%2520results%2520undermine%2520the%2520standard%2520stability-based%2520explanation%2520for%2520the%2520empirical%2520success%2520of%2520nonconvex%2520methods%2520and%2520suggest%2520that%2520fundamentally%2520different%2520tools%2520are%2520needed%2520to%2520analyze%2520nonnegative%2520low-rank%2520recovery.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03717v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonnegative%20Low-rank%20Matrix%20Recovery%20Can%20Have%20Spurious%20Local%20Minima&entry.906535625=Richard%20Y.%20Zhang&entry.1292438233=Low-rank%20matrix%20recovery%20is%20well-known%20to%20exhibit%20benign%20nonconvexity%20under%20the%20restricted%20isometry%20property%20%28RIP%29%3A%20every%20second-order%20critical%20point%20is%20globally%20optimal%2C%20so%20local%20methods%20provably%20recover%20the%20ground%20truth.%20Motivated%20by%20the%20strong%20empirical%20performance%20of%20projected%20gradient%20methods%20for%20nonnegative%20low-rank%20recovery%20problems%2C%20we%20investigate%20whether%20this%20benign%20geometry%20persists%20when%20the%20factor%20matrices%20are%20constrained%20to%20be%20elementwise%20nonnegative.%20In%20the%20simple%20setting%20of%20a%20rank-1%20nonnegative%20ground%20truth%2C%20we%20confirm%20that%20benign%20nonconvexity%20holds%20in%20the%20fully-observed%20case%20with%20RIP%20constant%20%24%CE%B4%3D0%24.%20This%20benign%20nonconvexity%2C%20however%2C%20is%20unstable.%20It%20fails%20to%20extend%20to%20the%20partially-observed%20case%20with%20any%20arbitrarily%20small%20RIP%20constant%20%24%CE%B4%3E0%24%2C%20and%20to%20higher-rank%20ground%20truths%20%24r%5E%7B%5Cstar%7D%3E1%24%2C%20regardless%20of%20how%20much%20the%20search%20rank%20%24r%5Cge%20r%5E%7B%5Cstar%7D%24%20is%20overparameterized.%20Together%2C%20these%20results%20undermine%20the%20standard%20stability-based%20explanation%20for%20the%20empirical%20success%20of%20nonconvex%20methods%20and%20suggest%20that%20fundamentally%20different%20tools%20are%20needed%20to%20analyze%20nonnegative%20low-rank%20recovery.&entry.1838667208=http%3A//arxiv.org/abs/2505.03717v2&entry.124074799=Read"},
{"title": "Cost-Free Personalization via Information-Geometric Projection in Bayesian Federated Learning", "author": "Nour Jamoussi and Giuseppe Serra and Photios A. Stavrou and Marios Kountouris", "abstract": "Bayesian Federated Learning (BFL) combines uncertainty modeling with decentralized training, enabling the development of personalized and reliable models under data heterogeneity and privacy constraints. Existing approaches typically rely on Markov Chain Monte Carlo (MCMC) sampling or variational inference, often incorporating personalization mechanisms to better adapt to local data distributions. In this work, we propose an information-geometric projection framework for personalization in parametric BFL. By projecting the global model onto a neighborhood of the user's local model, our method enables a tunable trade-off between global generalization and local specialization. Under mild assumptions, we show that this projection step is equivalent to computing a barycenter on the statistical manifold, allowing us to derive closed-form solutions and achieve cost-free personalization. We apply the proposed approach to a variational learning setup using the Improved Variational Online Newton (IVON) optimizer and extend its application to general aggregation schemes in BFL. Empirical evaluations under heterogeneous data distributions confirm that our method effectively balances global and local performance with minimal computational overhead.", "link": "http://arxiv.org/abs/2509.10132v2", "date": "2026-01-21", "relevancy": 2.0611, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5277}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5248}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cost-Free%20Personalization%20via%20Information-Geometric%20Projection%20in%20Bayesian%20Federated%20Learning&body=Title%3A%20Cost-Free%20Personalization%20via%20Information-Geometric%20Projection%20in%20Bayesian%20Federated%20Learning%0AAuthor%3A%20Nour%20Jamoussi%20and%20Giuseppe%20Serra%20and%20Photios%20A.%20Stavrou%20and%20Marios%20Kountouris%0AAbstract%3A%20Bayesian%20Federated%20Learning%20%28BFL%29%20combines%20uncertainty%20modeling%20with%20decentralized%20training%2C%20enabling%20the%20development%20of%20personalized%20and%20reliable%20models%20under%20data%20heterogeneity%20and%20privacy%20constraints.%20Existing%20approaches%20typically%20rely%20on%20Markov%20Chain%20Monte%20Carlo%20%28MCMC%29%20sampling%20or%20variational%20inference%2C%20often%20incorporating%20personalization%20mechanisms%20to%20better%20adapt%20to%20local%20data%20distributions.%20In%20this%20work%2C%20we%20propose%20an%20information-geometric%20projection%20framework%20for%20personalization%20in%20parametric%20BFL.%20By%20projecting%20the%20global%20model%20onto%20a%20neighborhood%20of%20the%20user%27s%20local%20model%2C%20our%20method%20enables%20a%20tunable%20trade-off%20between%20global%20generalization%20and%20local%20specialization.%20Under%20mild%20assumptions%2C%20we%20show%20that%20this%20projection%20step%20is%20equivalent%20to%20computing%20a%20barycenter%20on%20the%20statistical%20manifold%2C%20allowing%20us%20to%20derive%20closed-form%20solutions%20and%20achieve%20cost-free%20personalization.%20We%20apply%20the%20proposed%20approach%20to%20a%20variational%20learning%20setup%20using%20the%20Improved%20Variational%20Online%20Newton%20%28IVON%29%20optimizer%20and%20extend%20its%20application%20to%20general%20aggregation%20schemes%20in%20BFL.%20Empirical%20evaluations%20under%20heterogeneous%20data%20distributions%20confirm%20that%20our%20method%20effectively%20balances%20global%20and%20local%20performance%20with%20minimal%20computational%20overhead.%0ALink%3A%20http%3A//arxiv.org/abs/2509.10132v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCost-Free%2520Personalization%2520via%2520Information-Geometric%2520Projection%2520in%2520Bayesian%2520Federated%2520Learning%26entry.906535625%3DNour%2520Jamoussi%2520and%2520Giuseppe%2520Serra%2520and%2520Photios%2520A.%2520Stavrou%2520and%2520Marios%2520Kountouris%26entry.1292438233%3DBayesian%2520Federated%2520Learning%2520%2528BFL%2529%2520combines%2520uncertainty%2520modeling%2520with%2520decentralized%2520training%252C%2520enabling%2520the%2520development%2520of%2520personalized%2520and%2520reliable%2520models%2520under%2520data%2520heterogeneity%2520and%2520privacy%2520constraints.%2520Existing%2520approaches%2520typically%2520rely%2520on%2520Markov%2520Chain%2520Monte%2520Carlo%2520%2528MCMC%2529%2520sampling%2520or%2520variational%2520inference%252C%2520often%2520incorporating%2520personalization%2520mechanisms%2520to%2520better%2520adapt%2520to%2520local%2520data%2520distributions.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520information-geometric%2520projection%2520framework%2520for%2520personalization%2520in%2520parametric%2520BFL.%2520By%2520projecting%2520the%2520global%2520model%2520onto%2520a%2520neighborhood%2520of%2520the%2520user%2527s%2520local%2520model%252C%2520our%2520method%2520enables%2520a%2520tunable%2520trade-off%2520between%2520global%2520generalization%2520and%2520local%2520specialization.%2520Under%2520mild%2520assumptions%252C%2520we%2520show%2520that%2520this%2520projection%2520step%2520is%2520equivalent%2520to%2520computing%2520a%2520barycenter%2520on%2520the%2520statistical%2520manifold%252C%2520allowing%2520us%2520to%2520derive%2520closed-form%2520solutions%2520and%2520achieve%2520cost-free%2520personalization.%2520We%2520apply%2520the%2520proposed%2520approach%2520to%2520a%2520variational%2520learning%2520setup%2520using%2520the%2520Improved%2520Variational%2520Online%2520Newton%2520%2528IVON%2529%2520optimizer%2520and%2520extend%2520its%2520application%2520to%2520general%2520aggregation%2520schemes%2520in%2520BFL.%2520Empirical%2520evaluations%2520under%2520heterogeneous%2520data%2520distributions%2520confirm%2520that%2520our%2520method%2520effectively%2520balances%2520global%2520and%2520local%2520performance%2520with%2520minimal%2520computational%2520overhead.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.10132v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cost-Free%20Personalization%20via%20Information-Geometric%20Projection%20in%20Bayesian%20Federated%20Learning&entry.906535625=Nour%20Jamoussi%20and%20Giuseppe%20Serra%20and%20Photios%20A.%20Stavrou%20and%20Marios%20Kountouris&entry.1292438233=Bayesian%20Federated%20Learning%20%28BFL%29%20combines%20uncertainty%20modeling%20with%20decentralized%20training%2C%20enabling%20the%20development%20of%20personalized%20and%20reliable%20models%20under%20data%20heterogeneity%20and%20privacy%20constraints.%20Existing%20approaches%20typically%20rely%20on%20Markov%20Chain%20Monte%20Carlo%20%28MCMC%29%20sampling%20or%20variational%20inference%2C%20often%20incorporating%20personalization%20mechanisms%20to%20better%20adapt%20to%20local%20data%20distributions.%20In%20this%20work%2C%20we%20propose%20an%20information-geometric%20projection%20framework%20for%20personalization%20in%20parametric%20BFL.%20By%20projecting%20the%20global%20model%20onto%20a%20neighborhood%20of%20the%20user%27s%20local%20model%2C%20our%20method%20enables%20a%20tunable%20trade-off%20between%20global%20generalization%20and%20local%20specialization.%20Under%20mild%20assumptions%2C%20we%20show%20that%20this%20projection%20step%20is%20equivalent%20to%20computing%20a%20barycenter%20on%20the%20statistical%20manifold%2C%20allowing%20us%20to%20derive%20closed-form%20solutions%20and%20achieve%20cost-free%20personalization.%20We%20apply%20the%20proposed%20approach%20to%20a%20variational%20learning%20setup%20using%20the%20Improved%20Variational%20Online%20Newton%20%28IVON%29%20optimizer%20and%20extend%20its%20application%20to%20general%20aggregation%20schemes%20in%20BFL.%20Empirical%20evaluations%20under%20heterogeneous%20data%20distributions%20confirm%20that%20our%20method%20effectively%20balances%20global%20and%20local%20performance%20with%20minimal%20computational%20overhead.&entry.1838667208=http%3A//arxiv.org/abs/2509.10132v2&entry.124074799=Read"},
{"title": "Semilinear single-track vehicle models with distributed tyre friction dynamics", "author": "Luigi Romano and Ole Morten Aamo and Jan \u00c5slund and Erik Frisk", "abstract": "This paper introduces a novel family of single-track vehicle models that incorporate a distributed representation of transient tyre dynamics, whilst simultaneously accounting for nonlinear effects induced by friction. The core of the proposed framework is represented by the distributed Friction with Bristle Dynamics (FrBD) model, which unifies and extends classical formulations such as Dahl and LuGre by describing the rolling contact process as a spatially distributed system governed by semilinear partial differential equations (PDEs). This model is systematically integrated into a single-track vehicle framework, where the resulting semilinear ODE-PDE interconnection captures the interaction between lateral vehicle motion and tyre deformation. Two main variants are considered: one with rigid tyre carcass and another with flexible carcass, each admitting a compact state-space representation. Local and global well-posedness properties for the coupled system are established rigorously, highlighting the dissipative and physically consistent properties of the distributed FrBD model. A linearisation procedure is also presented, enabling spectral analysis and transfer function derivation, and potentially facilitating the synthesis of controllers and observers. Numerical simulations demonstrate the model's capability to capture micro-shimmy oscillations and transient lateral responses to advanced steering manoeuvres. The proposed formulation advances the state-of-the-art in vehicle dynamics modelling by providing a physically grounded, mathematically rigorous, and computationally tractable approach to incorporating transient tyre behaviour in lateral vehicle dynamics, when accounting for the effect of limited friction.", "link": "http://arxiv.org/abs/2601.06854v2", "date": "2026-01-21", "relevancy": 1.7644, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5284}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4501}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.3971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semilinear%20single-track%20vehicle%20models%20with%20distributed%20tyre%20friction%20dynamics&body=Title%3A%20Semilinear%20single-track%20vehicle%20models%20with%20distributed%20tyre%20friction%20dynamics%0AAuthor%3A%20Luigi%20Romano%20and%20Ole%20Morten%20Aamo%20and%20Jan%20%C3%85slund%20and%20Erik%20Frisk%0AAbstract%3A%20This%20paper%20introduces%20a%20novel%20family%20of%20single-track%20vehicle%20models%20that%20incorporate%20a%20distributed%20representation%20of%20transient%20tyre%20dynamics%2C%20whilst%20simultaneously%20accounting%20for%20nonlinear%20effects%20induced%20by%20friction.%20The%20core%20of%20the%20proposed%20framework%20is%20represented%20by%20the%20distributed%20Friction%20with%20Bristle%20Dynamics%20%28FrBD%29%20model%2C%20which%20unifies%20and%20extends%20classical%20formulations%20such%20as%20Dahl%20and%20LuGre%20by%20describing%20the%20rolling%20contact%20process%20as%20a%20spatially%20distributed%20system%20governed%20by%20semilinear%20partial%20differential%20equations%20%28PDEs%29.%20This%20model%20is%20systematically%20integrated%20into%20a%20single-track%20vehicle%20framework%2C%20where%20the%20resulting%20semilinear%20ODE-PDE%20interconnection%20captures%20the%20interaction%20between%20lateral%20vehicle%20motion%20and%20tyre%20deformation.%20Two%20main%20variants%20are%20considered%3A%20one%20with%20rigid%20tyre%20carcass%20and%20another%20with%20flexible%20carcass%2C%20each%20admitting%20a%20compact%20state-space%20representation.%20Local%20and%20global%20well-posedness%20properties%20for%20the%20coupled%20system%20are%20established%20rigorously%2C%20highlighting%20the%20dissipative%20and%20physically%20consistent%20properties%20of%20the%20distributed%20FrBD%20model.%20A%20linearisation%20procedure%20is%20also%20presented%2C%20enabling%20spectral%20analysis%20and%20transfer%20function%20derivation%2C%20and%20potentially%20facilitating%20the%20synthesis%20of%20controllers%20and%20observers.%20Numerical%20simulations%20demonstrate%20the%20model%27s%20capability%20to%20capture%20micro-shimmy%20oscillations%20and%20transient%20lateral%20responses%20to%20advanced%20steering%20manoeuvres.%20The%20proposed%20formulation%20advances%20the%20state-of-the-art%20in%20vehicle%20dynamics%20modelling%20by%20providing%20a%20physically%20grounded%2C%20mathematically%20rigorous%2C%20and%20computationally%20tractable%20approach%20to%20incorporating%20transient%20tyre%20behaviour%20in%20lateral%20vehicle%20dynamics%2C%20when%20accounting%20for%20the%20effect%20of%20limited%20friction.%0ALink%3A%20http%3A//arxiv.org/abs/2601.06854v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemilinear%2520single-track%2520vehicle%2520models%2520with%2520distributed%2520tyre%2520friction%2520dynamics%26entry.906535625%3DLuigi%2520Romano%2520and%2520Ole%2520Morten%2520Aamo%2520and%2520Jan%2520%25C3%2585slund%2520and%2520Erik%2520Frisk%26entry.1292438233%3DThis%2520paper%2520introduces%2520a%2520novel%2520family%2520of%2520single-track%2520vehicle%2520models%2520that%2520incorporate%2520a%2520distributed%2520representation%2520of%2520transient%2520tyre%2520dynamics%252C%2520whilst%2520simultaneously%2520accounting%2520for%2520nonlinear%2520effects%2520induced%2520by%2520friction.%2520The%2520core%2520of%2520the%2520proposed%2520framework%2520is%2520represented%2520by%2520the%2520distributed%2520Friction%2520with%2520Bristle%2520Dynamics%2520%2528FrBD%2529%2520model%252C%2520which%2520unifies%2520and%2520extends%2520classical%2520formulations%2520such%2520as%2520Dahl%2520and%2520LuGre%2520by%2520describing%2520the%2520rolling%2520contact%2520process%2520as%2520a%2520spatially%2520distributed%2520system%2520governed%2520by%2520semilinear%2520partial%2520differential%2520equations%2520%2528PDEs%2529.%2520This%2520model%2520is%2520systematically%2520integrated%2520into%2520a%2520single-track%2520vehicle%2520framework%252C%2520where%2520the%2520resulting%2520semilinear%2520ODE-PDE%2520interconnection%2520captures%2520the%2520interaction%2520between%2520lateral%2520vehicle%2520motion%2520and%2520tyre%2520deformation.%2520Two%2520main%2520variants%2520are%2520considered%253A%2520one%2520with%2520rigid%2520tyre%2520carcass%2520and%2520another%2520with%2520flexible%2520carcass%252C%2520each%2520admitting%2520a%2520compact%2520state-space%2520representation.%2520Local%2520and%2520global%2520well-posedness%2520properties%2520for%2520the%2520coupled%2520system%2520are%2520established%2520rigorously%252C%2520highlighting%2520the%2520dissipative%2520and%2520physically%2520consistent%2520properties%2520of%2520the%2520distributed%2520FrBD%2520model.%2520A%2520linearisation%2520procedure%2520is%2520also%2520presented%252C%2520enabling%2520spectral%2520analysis%2520and%2520transfer%2520function%2520derivation%252C%2520and%2520potentially%2520facilitating%2520the%2520synthesis%2520of%2520controllers%2520and%2520observers.%2520Numerical%2520simulations%2520demonstrate%2520the%2520model%2527s%2520capability%2520to%2520capture%2520micro-shimmy%2520oscillations%2520and%2520transient%2520lateral%2520responses%2520to%2520advanced%2520steering%2520manoeuvres.%2520The%2520proposed%2520formulation%2520advances%2520the%2520state-of-the-art%2520in%2520vehicle%2520dynamics%2520modelling%2520by%2520providing%2520a%2520physically%2520grounded%252C%2520mathematically%2520rigorous%252C%2520and%2520computationally%2520tractable%2520approach%2520to%2520incorporating%2520transient%2520tyre%2520behaviour%2520in%2520lateral%2520vehicle%2520dynamics%252C%2520when%2520accounting%2520for%2520the%2520effect%2520of%2520limited%2520friction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.06854v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semilinear%20single-track%20vehicle%20models%20with%20distributed%20tyre%20friction%20dynamics&entry.906535625=Luigi%20Romano%20and%20Ole%20Morten%20Aamo%20and%20Jan%20%C3%85slund%20and%20Erik%20Frisk&entry.1292438233=This%20paper%20introduces%20a%20novel%20family%20of%20single-track%20vehicle%20models%20that%20incorporate%20a%20distributed%20representation%20of%20transient%20tyre%20dynamics%2C%20whilst%20simultaneously%20accounting%20for%20nonlinear%20effects%20induced%20by%20friction.%20The%20core%20of%20the%20proposed%20framework%20is%20represented%20by%20the%20distributed%20Friction%20with%20Bristle%20Dynamics%20%28FrBD%29%20model%2C%20which%20unifies%20and%20extends%20classical%20formulations%20such%20as%20Dahl%20and%20LuGre%20by%20describing%20the%20rolling%20contact%20process%20as%20a%20spatially%20distributed%20system%20governed%20by%20semilinear%20partial%20differential%20equations%20%28PDEs%29.%20This%20model%20is%20systematically%20integrated%20into%20a%20single-track%20vehicle%20framework%2C%20where%20the%20resulting%20semilinear%20ODE-PDE%20interconnection%20captures%20the%20interaction%20between%20lateral%20vehicle%20motion%20and%20tyre%20deformation.%20Two%20main%20variants%20are%20considered%3A%20one%20with%20rigid%20tyre%20carcass%20and%20another%20with%20flexible%20carcass%2C%20each%20admitting%20a%20compact%20state-space%20representation.%20Local%20and%20global%20well-posedness%20properties%20for%20the%20coupled%20system%20are%20established%20rigorously%2C%20highlighting%20the%20dissipative%20and%20physically%20consistent%20properties%20of%20the%20distributed%20FrBD%20model.%20A%20linearisation%20procedure%20is%20also%20presented%2C%20enabling%20spectral%20analysis%20and%20transfer%20function%20derivation%2C%20and%20potentially%20facilitating%20the%20synthesis%20of%20controllers%20and%20observers.%20Numerical%20simulations%20demonstrate%20the%20model%27s%20capability%20to%20capture%20micro-shimmy%20oscillations%20and%20transient%20lateral%20responses%20to%20advanced%20steering%20manoeuvres.%20The%20proposed%20formulation%20advances%20the%20state-of-the-art%20in%20vehicle%20dynamics%20modelling%20by%20providing%20a%20physically%20grounded%2C%20mathematically%20rigorous%2C%20and%20computationally%20tractable%20approach%20to%20incorporating%20transient%20tyre%20behaviour%20in%20lateral%20vehicle%20dynamics%2C%20when%20accounting%20for%20the%20effect%20of%20limited%20friction.&entry.1838667208=http%3A//arxiv.org/abs/2601.06854v2&entry.124074799=Read"},
{"title": "Reading Between the Lines: Towards Reliable Black-box LLM Fingerprinting via Zeroth-order Gradient Estimation", "author": "Shuo Shao and Yiming Li and Hongwei Yao and Yifei Chen and Yuchen Yang and Zhan Qin", "abstract": "The substantial investment required to develop Large Language Models (LLMs) makes them valuable intellectual property, raising significant concerns about copyright protection. LLM fingerprinting has emerged as a key technique to address this, which aims to verify a model's origin by extracting an intrinsic, unique signature (a \"fingerprint\") and comparing it to that of a source model to identify illicit copies. However, existing black-box fingerprinting methods often fail to generate distinctive LLM fingerprints. This ineffectiveness arises because black-box methods typically rely on model outputs, which lose critical information about the model's unique parameters due to the usage of non-linear functions. To address this, we first leverage Fisher Information Theory to formally demonstrate that the gradient of the model's input is a more informative feature for fingerprinting than the output. Based on this insight, we propose ZeroPrint, a novel method that approximates these information-rich gradients in a black-box setting using zeroth-order estimation. ZeroPrint overcomes the challenge of applying this to discrete text by simulating input perturbations via semantic-preserving word substitutions. This operation allows ZeroPrint to estimate the model's Jacobian matrix as a unique fingerprint. Experiments on the standard benchmark show ZeroPrint achieves a state-of-the-art effectiveness and robustness, significantly outperforming existing black-box methods.", "link": "http://arxiv.org/abs/2510.06605v2", "date": "2026-01-21", "relevancy": 0.9574, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4946}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4736}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reading%20Between%20the%20Lines%3A%20Towards%20Reliable%20Black-box%20LLM%20Fingerprinting%20via%20Zeroth-order%20Gradient%20Estimation&body=Title%3A%20Reading%20Between%20the%20Lines%3A%20Towards%20Reliable%20Black-box%20LLM%20Fingerprinting%20via%20Zeroth-order%20Gradient%20Estimation%0AAuthor%3A%20Shuo%20Shao%20and%20Yiming%20Li%20and%20Hongwei%20Yao%20and%20Yifei%20Chen%20and%20Yuchen%20Yang%20and%20Zhan%20Qin%0AAbstract%3A%20The%20substantial%20investment%20required%20to%20develop%20Large%20Language%20Models%20%28LLMs%29%20makes%20them%20valuable%20intellectual%20property%2C%20raising%20significant%20concerns%20about%20copyright%20protection.%20LLM%20fingerprinting%20has%20emerged%20as%20a%20key%20technique%20to%20address%20this%2C%20which%20aims%20to%20verify%20a%20model%27s%20origin%20by%20extracting%20an%20intrinsic%2C%20unique%20signature%20%28a%20%22fingerprint%22%29%20and%20comparing%20it%20to%20that%20of%20a%20source%20model%20to%20identify%20illicit%20copies.%20However%2C%20existing%20black-box%20fingerprinting%20methods%20often%20fail%20to%20generate%20distinctive%20LLM%20fingerprints.%20This%20ineffectiveness%20arises%20because%20black-box%20methods%20typically%20rely%20on%20model%20outputs%2C%20which%20lose%20critical%20information%20about%20the%20model%27s%20unique%20parameters%20due%20to%20the%20usage%20of%20non-linear%20functions.%20To%20address%20this%2C%20we%20first%20leverage%20Fisher%20Information%20Theory%20to%20formally%20demonstrate%20that%20the%20gradient%20of%20the%20model%27s%20input%20is%20a%20more%20informative%20feature%20for%20fingerprinting%20than%20the%20output.%20Based%20on%20this%20insight%2C%20we%20propose%20ZeroPrint%2C%20a%20novel%20method%20that%20approximates%20these%20information-rich%20gradients%20in%20a%20black-box%20setting%20using%20zeroth-order%20estimation.%20ZeroPrint%20overcomes%20the%20challenge%20of%20applying%20this%20to%20discrete%20text%20by%20simulating%20input%20perturbations%20via%20semantic-preserving%20word%20substitutions.%20This%20operation%20allows%20ZeroPrint%20to%20estimate%20the%20model%27s%20Jacobian%20matrix%20as%20a%20unique%20fingerprint.%20Experiments%20on%20the%20standard%20benchmark%20show%20ZeroPrint%20achieves%20a%20state-of-the-art%20effectiveness%20and%20robustness%2C%20significantly%20outperforming%20existing%20black-box%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2510.06605v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReading%2520Between%2520the%2520Lines%253A%2520Towards%2520Reliable%2520Black-box%2520LLM%2520Fingerprinting%2520via%2520Zeroth-order%2520Gradient%2520Estimation%26entry.906535625%3DShuo%2520Shao%2520and%2520Yiming%2520Li%2520and%2520Hongwei%2520Yao%2520and%2520Yifei%2520Chen%2520and%2520Yuchen%2520Yang%2520and%2520Zhan%2520Qin%26entry.1292438233%3DThe%2520substantial%2520investment%2520required%2520to%2520develop%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520makes%2520them%2520valuable%2520intellectual%2520property%252C%2520raising%2520significant%2520concerns%2520about%2520copyright%2520protection.%2520LLM%2520fingerprinting%2520has%2520emerged%2520as%2520a%2520key%2520technique%2520to%2520address%2520this%252C%2520which%2520aims%2520to%2520verify%2520a%2520model%2527s%2520origin%2520by%2520extracting%2520an%2520intrinsic%252C%2520unique%2520signature%2520%2528a%2520%2522fingerprint%2522%2529%2520and%2520comparing%2520it%2520to%2520that%2520of%2520a%2520source%2520model%2520to%2520identify%2520illicit%2520copies.%2520However%252C%2520existing%2520black-box%2520fingerprinting%2520methods%2520often%2520fail%2520to%2520generate%2520distinctive%2520LLM%2520fingerprints.%2520This%2520ineffectiveness%2520arises%2520because%2520black-box%2520methods%2520typically%2520rely%2520on%2520model%2520outputs%252C%2520which%2520lose%2520critical%2520information%2520about%2520the%2520model%2527s%2520unique%2520parameters%2520due%2520to%2520the%2520usage%2520of%2520non-linear%2520functions.%2520To%2520address%2520this%252C%2520we%2520first%2520leverage%2520Fisher%2520Information%2520Theory%2520to%2520formally%2520demonstrate%2520that%2520the%2520gradient%2520of%2520the%2520model%2527s%2520input%2520is%2520a%2520more%2520informative%2520feature%2520for%2520fingerprinting%2520than%2520the%2520output.%2520Based%2520on%2520this%2520insight%252C%2520we%2520propose%2520ZeroPrint%252C%2520a%2520novel%2520method%2520that%2520approximates%2520these%2520information-rich%2520gradients%2520in%2520a%2520black-box%2520setting%2520using%2520zeroth-order%2520estimation.%2520ZeroPrint%2520overcomes%2520the%2520challenge%2520of%2520applying%2520this%2520to%2520discrete%2520text%2520by%2520simulating%2520input%2520perturbations%2520via%2520semantic-preserving%2520word%2520substitutions.%2520This%2520operation%2520allows%2520ZeroPrint%2520to%2520estimate%2520the%2520model%2527s%2520Jacobian%2520matrix%2520as%2520a%2520unique%2520fingerprint.%2520Experiments%2520on%2520the%2520standard%2520benchmark%2520show%2520ZeroPrint%2520achieves%2520a%2520state-of-the-art%2520effectiveness%2520and%2520robustness%252C%2520significantly%2520outperforming%2520existing%2520black-box%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06605v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reading%20Between%20the%20Lines%3A%20Towards%20Reliable%20Black-box%20LLM%20Fingerprinting%20via%20Zeroth-order%20Gradient%20Estimation&entry.906535625=Shuo%20Shao%20and%20Yiming%20Li%20and%20Hongwei%20Yao%20and%20Yifei%20Chen%20and%20Yuchen%20Yang%20and%20Zhan%20Qin&entry.1292438233=The%20substantial%20investment%20required%20to%20develop%20Large%20Language%20Models%20%28LLMs%29%20makes%20them%20valuable%20intellectual%20property%2C%20raising%20significant%20concerns%20about%20copyright%20protection.%20LLM%20fingerprinting%20has%20emerged%20as%20a%20key%20technique%20to%20address%20this%2C%20which%20aims%20to%20verify%20a%20model%27s%20origin%20by%20extracting%20an%20intrinsic%2C%20unique%20signature%20%28a%20%22fingerprint%22%29%20and%20comparing%20it%20to%20that%20of%20a%20source%20model%20to%20identify%20illicit%20copies.%20However%2C%20existing%20black-box%20fingerprinting%20methods%20often%20fail%20to%20generate%20distinctive%20LLM%20fingerprints.%20This%20ineffectiveness%20arises%20because%20black-box%20methods%20typically%20rely%20on%20model%20outputs%2C%20which%20lose%20critical%20information%20about%20the%20model%27s%20unique%20parameters%20due%20to%20the%20usage%20of%20non-linear%20functions.%20To%20address%20this%2C%20we%20first%20leverage%20Fisher%20Information%20Theory%20to%20formally%20demonstrate%20that%20the%20gradient%20of%20the%20model%27s%20input%20is%20a%20more%20informative%20feature%20for%20fingerprinting%20than%20the%20output.%20Based%20on%20this%20insight%2C%20we%20propose%20ZeroPrint%2C%20a%20novel%20method%20that%20approximates%20these%20information-rich%20gradients%20in%20a%20black-box%20setting%20using%20zeroth-order%20estimation.%20ZeroPrint%20overcomes%20the%20challenge%20of%20applying%20this%20to%20discrete%20text%20by%20simulating%20input%20perturbations%20via%20semantic-preserving%20word%20substitutions.%20This%20operation%20allows%20ZeroPrint%20to%20estimate%20the%20model%27s%20Jacobian%20matrix%20as%20a%20unique%20fingerprint.%20Experiments%20on%20the%20standard%20benchmark%20show%20ZeroPrint%20achieves%20a%20state-of-the-art%20effectiveness%20and%20robustness%2C%20significantly%20outperforming%20existing%20black-box%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2510.06605v2&entry.124074799=Read"},
{"title": "Multi-Agent Constraint Factorization Reveals Latent Invariant Solution Structure", "author": "Christopher Scofield", "abstract": "Multi-agent systems (MAS) composed of large language models often exhibit improved problem-solving performance despite operating on identical information. In this work, we provide a formal explanation for this phenomenon grounded in operator theory and constrained optimization. We model each agent as enforcing a distinct family of validity constraints on a shared solution state, and show that a MAS implements a factorized composition of constraint-enforcement operators. Under mild conditions, these dynamics converge to invariant solution sets defined by the intersection of agent constraint sets. Such invariant structures are generally not dynamically accessible to a single agent applying all constraints simultaneously, even when expressive capacity and information are identical. We extend this result from exact constraint enforcement to soft constraints via proximal operators, and apply the formalism to contemporary text-based dialog systems.", "link": "http://arxiv.org/abs/2601.15077v1", "date": "2026-01-21", "relevancy": 1.9346, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.506}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4792}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Agent%20Constraint%20Factorization%20Reveals%20Latent%20Invariant%20Solution%20Structure&body=Title%3A%20Multi-Agent%20Constraint%20Factorization%20Reveals%20Latent%20Invariant%20Solution%20Structure%0AAuthor%3A%20Christopher%20Scofield%0AAbstract%3A%20Multi-agent%20systems%20%28MAS%29%20composed%20of%20large%20language%20models%20often%20exhibit%20improved%20problem-solving%20performance%20despite%20operating%20on%20identical%20information.%20In%20this%20work%2C%20we%20provide%20a%20formal%20explanation%20for%20this%20phenomenon%20grounded%20in%20operator%20theory%20and%20constrained%20optimization.%20We%20model%20each%20agent%20as%20enforcing%20a%20distinct%20family%20of%20validity%20constraints%20on%20a%20shared%20solution%20state%2C%20and%20show%20that%20a%20MAS%20implements%20a%20factorized%20composition%20of%20constraint-enforcement%20operators.%20Under%20mild%20conditions%2C%20these%20dynamics%20converge%20to%20invariant%20solution%20sets%20defined%20by%20the%20intersection%20of%20agent%20constraint%20sets.%20Such%20invariant%20structures%20are%20generally%20not%20dynamically%20accessible%20to%20a%20single%20agent%20applying%20all%20constraints%20simultaneously%2C%20even%20when%20expressive%20capacity%20and%20information%20are%20identical.%20We%20extend%20this%20result%20from%20exact%20constraint%20enforcement%20to%20soft%20constraints%20via%20proximal%20operators%2C%20and%20apply%20the%20formalism%20to%20contemporary%20text-based%20dialog%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15077v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Agent%2520Constraint%2520Factorization%2520Reveals%2520Latent%2520Invariant%2520Solution%2520Structure%26entry.906535625%3DChristopher%2520Scofield%26entry.1292438233%3DMulti-agent%2520systems%2520%2528MAS%2529%2520composed%2520of%2520large%2520language%2520models%2520often%2520exhibit%2520improved%2520problem-solving%2520performance%2520despite%2520operating%2520on%2520identical%2520information.%2520In%2520this%2520work%252C%2520we%2520provide%2520a%2520formal%2520explanation%2520for%2520this%2520phenomenon%2520grounded%2520in%2520operator%2520theory%2520and%2520constrained%2520optimization.%2520We%2520model%2520each%2520agent%2520as%2520enforcing%2520a%2520distinct%2520family%2520of%2520validity%2520constraints%2520on%2520a%2520shared%2520solution%2520state%252C%2520and%2520show%2520that%2520a%2520MAS%2520implements%2520a%2520factorized%2520composition%2520of%2520constraint-enforcement%2520operators.%2520Under%2520mild%2520conditions%252C%2520these%2520dynamics%2520converge%2520to%2520invariant%2520solution%2520sets%2520defined%2520by%2520the%2520intersection%2520of%2520agent%2520constraint%2520sets.%2520Such%2520invariant%2520structures%2520are%2520generally%2520not%2520dynamically%2520accessible%2520to%2520a%2520single%2520agent%2520applying%2520all%2520constraints%2520simultaneously%252C%2520even%2520when%2520expressive%2520capacity%2520and%2520information%2520are%2520identical.%2520We%2520extend%2520this%2520result%2520from%2520exact%2520constraint%2520enforcement%2520to%2520soft%2520constraints%2520via%2520proximal%2520operators%252C%2520and%2520apply%2520the%2520formalism%2520to%2520contemporary%2520text-based%2520dialog%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15077v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Agent%20Constraint%20Factorization%20Reveals%20Latent%20Invariant%20Solution%20Structure&entry.906535625=Christopher%20Scofield&entry.1292438233=Multi-agent%20systems%20%28MAS%29%20composed%20of%20large%20language%20models%20often%20exhibit%20improved%20problem-solving%20performance%20despite%20operating%20on%20identical%20information.%20In%20this%20work%2C%20we%20provide%20a%20formal%20explanation%20for%20this%20phenomenon%20grounded%20in%20operator%20theory%20and%20constrained%20optimization.%20We%20model%20each%20agent%20as%20enforcing%20a%20distinct%20family%20of%20validity%20constraints%20on%20a%20shared%20solution%20state%2C%20and%20show%20that%20a%20MAS%20implements%20a%20factorized%20composition%20of%20constraint-enforcement%20operators.%20Under%20mild%20conditions%2C%20these%20dynamics%20converge%20to%20invariant%20solution%20sets%20defined%20by%20the%20intersection%20of%20agent%20constraint%20sets.%20Such%20invariant%20structures%20are%20generally%20not%20dynamically%20accessible%20to%20a%20single%20agent%20applying%20all%20constraints%20simultaneously%2C%20even%20when%20expressive%20capacity%20and%20information%20are%20identical.%20We%20extend%20this%20result%20from%20exact%20constraint%20enforcement%20to%20soft%20constraints%20via%20proximal%20operators%2C%20and%20apply%20the%20formalism%20to%20contemporary%20text-based%20dialog%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.15077v1&entry.124074799=Read"},
{"title": "From Observation to Prediction: LSTM for Vehicle Lane Change Forecasting on Highway On/Off-Ramps", "author": "Mohamed Abouras and Catherine M. Elias", "abstract": "On and off-ramps are understudied road sections even though they introduce a higher level of variation in highway interactions. Predicting vehicles' behavior in these areas can decrease the impact of uncertainty and increase road safety. In this paper, the difference between this Area of Interest (AoI) and a straight highway section is studied. Multi-layered LSTM architecture to train the AoI model with ExiD drone dataset is utilized. In the process, different prediction horizons and different models' workflow are tested. The results show great promise on horizons up to 4 seconds with prediction accuracy starting from about 76% for the AoI and 94% for the general highway scenarios on the maximum horizon.", "link": "http://arxiv.org/abs/2601.14848v1", "date": "2026-01-21", "relevancy": 1.7888, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4777}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4539}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Observation%20to%20Prediction%3A%20LSTM%20for%20Vehicle%20Lane%20Change%20Forecasting%20on%20Highway%20On/Off-Ramps&body=Title%3A%20From%20Observation%20to%20Prediction%3A%20LSTM%20for%20Vehicle%20Lane%20Change%20Forecasting%20on%20Highway%20On/Off-Ramps%0AAuthor%3A%20Mohamed%20Abouras%20and%20Catherine%20M.%20Elias%0AAbstract%3A%20On%20and%20off-ramps%20are%20understudied%20road%20sections%20even%20though%20they%20introduce%20a%20higher%20level%20of%20variation%20in%20highway%20interactions.%20Predicting%20vehicles%27%20behavior%20in%20these%20areas%20can%20decrease%20the%20impact%20of%20uncertainty%20and%20increase%20road%20safety.%20In%20this%20paper%2C%20the%20difference%20between%20this%20Area%20of%20Interest%20%28AoI%29%20and%20a%20straight%20highway%20section%20is%20studied.%20Multi-layered%20LSTM%20architecture%20to%20train%20the%20AoI%20model%20with%20ExiD%20drone%20dataset%20is%20utilized.%20In%20the%20process%2C%20different%20prediction%20horizons%20and%20different%20models%27%20workflow%20are%20tested.%20The%20results%20show%20great%20promise%20on%20horizons%20up%20to%204%20seconds%20with%20prediction%20accuracy%20starting%20from%20about%2076%25%20for%20the%20AoI%20and%2094%25%20for%20the%20general%20highway%20scenarios%20on%20the%20maximum%20horizon.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14848v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Observation%2520to%2520Prediction%253A%2520LSTM%2520for%2520Vehicle%2520Lane%2520Change%2520Forecasting%2520on%2520Highway%2520On/Off-Ramps%26entry.906535625%3DMohamed%2520Abouras%2520and%2520Catherine%2520M.%2520Elias%26entry.1292438233%3DOn%2520and%2520off-ramps%2520are%2520understudied%2520road%2520sections%2520even%2520though%2520they%2520introduce%2520a%2520higher%2520level%2520of%2520variation%2520in%2520highway%2520interactions.%2520Predicting%2520vehicles%2527%2520behavior%2520in%2520these%2520areas%2520can%2520decrease%2520the%2520impact%2520of%2520uncertainty%2520and%2520increase%2520road%2520safety.%2520In%2520this%2520paper%252C%2520the%2520difference%2520between%2520this%2520Area%2520of%2520Interest%2520%2528AoI%2529%2520and%2520a%2520straight%2520highway%2520section%2520is%2520studied.%2520Multi-layered%2520LSTM%2520architecture%2520to%2520train%2520the%2520AoI%2520model%2520with%2520ExiD%2520drone%2520dataset%2520is%2520utilized.%2520In%2520the%2520process%252C%2520different%2520prediction%2520horizons%2520and%2520different%2520models%2527%2520workflow%2520are%2520tested.%2520The%2520results%2520show%2520great%2520promise%2520on%2520horizons%2520up%2520to%25204%2520seconds%2520with%2520prediction%2520accuracy%2520starting%2520from%2520about%252076%2525%2520for%2520the%2520AoI%2520and%252094%2525%2520for%2520the%2520general%2520highway%2520scenarios%2520on%2520the%2520maximum%2520horizon.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14848v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Observation%20to%20Prediction%3A%20LSTM%20for%20Vehicle%20Lane%20Change%20Forecasting%20on%20Highway%20On/Off-Ramps&entry.906535625=Mohamed%20Abouras%20and%20Catherine%20M.%20Elias&entry.1292438233=On%20and%20off-ramps%20are%20understudied%20road%20sections%20even%20though%20they%20introduce%20a%20higher%20level%20of%20variation%20in%20highway%20interactions.%20Predicting%20vehicles%27%20behavior%20in%20these%20areas%20can%20decrease%20the%20impact%20of%20uncertainty%20and%20increase%20road%20safety.%20In%20this%20paper%2C%20the%20difference%20between%20this%20Area%20of%20Interest%20%28AoI%29%20and%20a%20straight%20highway%20section%20is%20studied.%20Multi-layered%20LSTM%20architecture%20to%20train%20the%20AoI%20model%20with%20ExiD%20drone%20dataset%20is%20utilized.%20In%20the%20process%2C%20different%20prediction%20horizons%20and%20different%20models%27%20workflow%20are%20tested.%20The%20results%20show%20great%20promise%20on%20horizons%20up%20to%204%20seconds%20with%20prediction%20accuracy%20starting%20from%20about%2076%25%20for%20the%20AoI%20and%2094%25%20for%20the%20general%20highway%20scenarios%20on%20the%20maximum%20horizon.&entry.1838667208=http%3A//arxiv.org/abs/2601.14848v1&entry.124074799=Read"},
{"title": "Erosion Attack for Adversarial Training to Enhance Semantic Segmentation Robustness", "author": "Yufei Song and Ziqi Zhou and Menghao Deng and Yifan Hu and Shengshan Hu and Minghui Li and Leo Yu Zhang", "abstract": "Existing segmentation models exhibit significant vulnerability to adversarial attacks.To improve robustness, adversarial training incorporates adversarial examples into model training. However, existing attack methods consider only global semantic information and ignore contextual semantic relationships within the samples, limiting the effectiveness of adversarial training. To address this issue, we propose EroSeg-AT, a vulnerability-aware adversarial training framework that leverages EroSeg to generate adversarial examples. EroSeg first selects sensitive pixels based on pixel-level confidence and then progressively propagates perturbations to higher-confidence pixels, effectively disrupting the semantic consistency of the samples. Experimental results show that, compared to existing methods, our approach significantly improves attack effectiveness and enhances model robustness under adversarial training.", "link": "http://arxiv.org/abs/2601.14950v1", "date": "2026-01-21", "relevancy": 1.9189, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5191}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4729}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Erosion%20Attack%20for%20Adversarial%20Training%20to%20Enhance%20Semantic%20Segmentation%20Robustness&body=Title%3A%20Erosion%20Attack%20for%20Adversarial%20Training%20to%20Enhance%20Semantic%20Segmentation%20Robustness%0AAuthor%3A%20Yufei%20Song%20and%20Ziqi%20Zhou%20and%20Menghao%20Deng%20and%20Yifan%20Hu%20and%20Shengshan%20Hu%20and%20Minghui%20Li%20and%20Leo%20Yu%20Zhang%0AAbstract%3A%20Existing%20segmentation%20models%20exhibit%20significant%20vulnerability%20to%20adversarial%20attacks.To%20improve%20robustness%2C%20adversarial%20training%20incorporates%20adversarial%20examples%20into%20model%20training.%20However%2C%20existing%20attack%20methods%20consider%20only%20global%20semantic%20information%20and%20ignore%20contextual%20semantic%20relationships%20within%20the%20samples%2C%20limiting%20the%20effectiveness%20of%20adversarial%20training.%20To%20address%20this%20issue%2C%20we%20propose%20EroSeg-AT%2C%20a%20vulnerability-aware%20adversarial%20training%20framework%20that%20leverages%20EroSeg%20to%20generate%20adversarial%20examples.%20EroSeg%20first%20selects%20sensitive%20pixels%20based%20on%20pixel-level%20confidence%20and%20then%20progressively%20propagates%20perturbations%20to%20higher-confidence%20pixels%2C%20effectively%20disrupting%20the%20semantic%20consistency%20of%20the%20samples.%20Experimental%20results%20show%20that%2C%20compared%20to%20existing%20methods%2C%20our%20approach%20significantly%20improves%20attack%20effectiveness%20and%20enhances%20model%20robustness%20under%20adversarial%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14950v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DErosion%2520Attack%2520for%2520Adversarial%2520Training%2520to%2520Enhance%2520Semantic%2520Segmentation%2520Robustness%26entry.906535625%3DYufei%2520Song%2520and%2520Ziqi%2520Zhou%2520and%2520Menghao%2520Deng%2520and%2520Yifan%2520Hu%2520and%2520Shengshan%2520Hu%2520and%2520Minghui%2520Li%2520and%2520Leo%2520Yu%2520Zhang%26entry.1292438233%3DExisting%2520segmentation%2520models%2520exhibit%2520significant%2520vulnerability%2520to%2520adversarial%2520attacks.To%2520improve%2520robustness%252C%2520adversarial%2520training%2520incorporates%2520adversarial%2520examples%2520into%2520model%2520training.%2520However%252C%2520existing%2520attack%2520methods%2520consider%2520only%2520global%2520semantic%2520information%2520and%2520ignore%2520contextual%2520semantic%2520relationships%2520within%2520the%2520samples%252C%2520limiting%2520the%2520effectiveness%2520of%2520adversarial%2520training.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520EroSeg-AT%252C%2520a%2520vulnerability-aware%2520adversarial%2520training%2520framework%2520that%2520leverages%2520EroSeg%2520to%2520generate%2520adversarial%2520examples.%2520EroSeg%2520first%2520selects%2520sensitive%2520pixels%2520based%2520on%2520pixel-level%2520confidence%2520and%2520then%2520progressively%2520propagates%2520perturbations%2520to%2520higher-confidence%2520pixels%252C%2520effectively%2520disrupting%2520the%2520semantic%2520consistency%2520of%2520the%2520samples.%2520Experimental%2520results%2520show%2520that%252C%2520compared%2520to%2520existing%2520methods%252C%2520our%2520approach%2520significantly%2520improves%2520attack%2520effectiveness%2520and%2520enhances%2520model%2520robustness%2520under%2520adversarial%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14950v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Erosion%20Attack%20for%20Adversarial%20Training%20to%20Enhance%20Semantic%20Segmentation%20Robustness&entry.906535625=Yufei%20Song%20and%20Ziqi%20Zhou%20and%20Menghao%20Deng%20and%20Yifan%20Hu%20and%20Shengshan%20Hu%20and%20Minghui%20Li%20and%20Leo%20Yu%20Zhang&entry.1292438233=Existing%20segmentation%20models%20exhibit%20significant%20vulnerability%20to%20adversarial%20attacks.To%20improve%20robustness%2C%20adversarial%20training%20incorporates%20adversarial%20examples%20into%20model%20training.%20However%2C%20existing%20attack%20methods%20consider%20only%20global%20semantic%20information%20and%20ignore%20contextual%20semantic%20relationships%20within%20the%20samples%2C%20limiting%20the%20effectiveness%20of%20adversarial%20training.%20To%20address%20this%20issue%2C%20we%20propose%20EroSeg-AT%2C%20a%20vulnerability-aware%20adversarial%20training%20framework%20that%20leverages%20EroSeg%20to%20generate%20adversarial%20examples.%20EroSeg%20first%20selects%20sensitive%20pixels%20based%20on%20pixel-level%20confidence%20and%20then%20progressively%20propagates%20perturbations%20to%20higher-confidence%20pixels%2C%20effectively%20disrupting%20the%20semantic%20consistency%20of%20the%20samples.%20Experimental%20results%20show%20that%2C%20compared%20to%20existing%20methods%2C%20our%20approach%20significantly%20improves%20attack%20effectiveness%20and%20enhances%20model%20robustness%20under%20adversarial%20training.&entry.1838667208=http%3A//arxiv.org/abs/2601.14950v1&entry.124074799=Read"},
{"title": "Dynamic Management of a Deep Learning-Based Anomaly Detection System for 5G Networks", "author": "Lorenzo Fern\u00e1ndez Maim\u00f3 and Alberto Huertas Celdr\u00e1n and Manuel Gil P\u00e9rez and F\u00e9lix J. Garc\u00eda Clemente and Gregorio Mart\u00ednez P\u00e9rez", "abstract": "Fog and mobile edge computing (MEC) will play a key role in the upcoming fifth generation (5G) mobile networks to support decentralized applications, data analytics and management into the network itself by using a highly distributed compute model. Furthermore, increasing attention is paid to providing user-centric cybersecurity solutions, which particularly require collecting, processing and analyzing significantly large amount of data traffic and huge number of network connections in 5G networks. In this regard, this paper proposes a MEC-oriented solution in 5G mobile networks to detect network anomalies in real-time and in autonomic way. Our proposal uses deep learning techniques to analyze network flows and to detect network anomalies. Moreover, it uses policies in order to provide an efficient and dynamic management system of the computing resources used in the anomaly detection process. The paper presents relevant aspects of the deployment of the proposal and experimental results to show its performance.", "link": "http://arxiv.org/abs/2601.15177v1", "date": "2026-01-21", "relevancy": 1.8524, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4919}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4462}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Management%20of%20a%20Deep%20Learning-Based%20Anomaly%20Detection%20System%20for%205G%20Networks&body=Title%3A%20Dynamic%20Management%20of%20a%20Deep%20Learning-Based%20Anomaly%20Detection%20System%20for%205G%20Networks%0AAuthor%3A%20Lorenzo%20Fern%C3%A1ndez%20Maim%C3%B3%20and%20Alberto%20Huertas%20Celdr%C3%A1n%20and%20Manuel%20Gil%20P%C3%A9rez%20and%20F%C3%A9lix%20J.%20Garc%C3%ADa%20Clemente%20and%20Gregorio%20Mart%C3%ADnez%20P%C3%A9rez%0AAbstract%3A%20Fog%20and%20mobile%20edge%20computing%20%28MEC%29%20will%20play%20a%20key%20role%20in%20the%20upcoming%20fifth%20generation%20%285G%29%20mobile%20networks%20to%20support%20decentralized%20applications%2C%20data%20analytics%20and%20management%20into%20the%20network%20itself%20by%20using%20a%20highly%20distributed%20compute%20model.%20Furthermore%2C%20increasing%20attention%20is%20paid%20to%20providing%20user-centric%20cybersecurity%20solutions%2C%20which%20particularly%20require%20collecting%2C%20processing%20and%20analyzing%20significantly%20large%20amount%20of%20data%20traffic%20and%20huge%20number%20of%20network%20connections%20in%205G%20networks.%20In%20this%20regard%2C%20this%20paper%20proposes%20a%20MEC-oriented%20solution%20in%205G%20mobile%20networks%20to%20detect%20network%20anomalies%20in%20real-time%20and%20in%20autonomic%20way.%20Our%20proposal%20uses%20deep%20learning%20techniques%20to%20analyze%20network%20flows%20and%20to%20detect%20network%20anomalies.%20Moreover%2C%20it%20uses%20policies%20in%20order%20to%20provide%20an%20efficient%20and%20dynamic%20management%20system%20of%20the%20computing%20resources%20used%20in%20the%20anomaly%20detection%20process.%20The%20paper%20presents%20relevant%20aspects%20of%20the%20deployment%20of%20the%20proposal%20and%20experimental%20results%20to%20show%20its%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Management%2520of%2520a%2520Deep%2520Learning-Based%2520Anomaly%2520Detection%2520System%2520for%25205G%2520Networks%26entry.906535625%3DLorenzo%2520Fern%25C3%25A1ndez%2520Maim%25C3%25B3%2520and%2520Alberto%2520Huertas%2520Celdr%25C3%25A1n%2520and%2520Manuel%2520Gil%2520P%25C3%25A9rez%2520and%2520F%25C3%25A9lix%2520J.%2520Garc%25C3%25ADa%2520Clemente%2520and%2520Gregorio%2520Mart%25C3%25ADnez%2520P%25C3%25A9rez%26entry.1292438233%3DFog%2520and%2520mobile%2520edge%2520computing%2520%2528MEC%2529%2520will%2520play%2520a%2520key%2520role%2520in%2520the%2520upcoming%2520fifth%2520generation%2520%25285G%2529%2520mobile%2520networks%2520to%2520support%2520decentralized%2520applications%252C%2520data%2520analytics%2520and%2520management%2520into%2520the%2520network%2520itself%2520by%2520using%2520a%2520highly%2520distributed%2520compute%2520model.%2520Furthermore%252C%2520increasing%2520attention%2520is%2520paid%2520to%2520providing%2520user-centric%2520cybersecurity%2520solutions%252C%2520which%2520particularly%2520require%2520collecting%252C%2520processing%2520and%2520analyzing%2520significantly%2520large%2520amount%2520of%2520data%2520traffic%2520and%2520huge%2520number%2520of%2520network%2520connections%2520in%25205G%2520networks.%2520In%2520this%2520regard%252C%2520this%2520paper%2520proposes%2520a%2520MEC-oriented%2520solution%2520in%25205G%2520mobile%2520networks%2520to%2520detect%2520network%2520anomalies%2520in%2520real-time%2520and%2520in%2520autonomic%2520way.%2520Our%2520proposal%2520uses%2520deep%2520learning%2520techniques%2520to%2520analyze%2520network%2520flows%2520and%2520to%2520detect%2520network%2520anomalies.%2520Moreover%252C%2520it%2520uses%2520policies%2520in%2520order%2520to%2520provide%2520an%2520efficient%2520and%2520dynamic%2520management%2520system%2520of%2520the%2520computing%2520resources%2520used%2520in%2520the%2520anomaly%2520detection%2520process.%2520The%2520paper%2520presents%2520relevant%2520aspects%2520of%2520the%2520deployment%2520of%2520the%2520proposal%2520and%2520experimental%2520results%2520to%2520show%2520its%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Management%20of%20a%20Deep%20Learning-Based%20Anomaly%20Detection%20System%20for%205G%20Networks&entry.906535625=Lorenzo%20Fern%C3%A1ndez%20Maim%C3%B3%20and%20Alberto%20Huertas%20Celdr%C3%A1n%20and%20Manuel%20Gil%20P%C3%A9rez%20and%20F%C3%A9lix%20J.%20Garc%C3%ADa%20Clemente%20and%20Gregorio%20Mart%C3%ADnez%20P%C3%A9rez&entry.1292438233=Fog%20and%20mobile%20edge%20computing%20%28MEC%29%20will%20play%20a%20key%20role%20in%20the%20upcoming%20fifth%20generation%20%285G%29%20mobile%20networks%20to%20support%20decentralized%20applications%2C%20data%20analytics%20and%20management%20into%20the%20network%20itself%20by%20using%20a%20highly%20distributed%20compute%20model.%20Furthermore%2C%20increasing%20attention%20is%20paid%20to%20providing%20user-centric%20cybersecurity%20solutions%2C%20which%20particularly%20require%20collecting%2C%20processing%20and%20analyzing%20significantly%20large%20amount%20of%20data%20traffic%20and%20huge%20number%20of%20network%20connections%20in%205G%20networks.%20In%20this%20regard%2C%20this%20paper%20proposes%20a%20MEC-oriented%20solution%20in%205G%20mobile%20networks%20to%20detect%20network%20anomalies%20in%20real-time%20and%20in%20autonomic%20way.%20Our%20proposal%20uses%20deep%20learning%20techniques%20to%20analyze%20network%20flows%20and%20to%20detect%20network%20anomalies.%20Moreover%2C%20it%20uses%20policies%20in%20order%20to%20provide%20an%20efficient%20and%20dynamic%20management%20system%20of%20the%20computing%20resources%20used%20in%20the%20anomaly%20detection%20process.%20The%20paper%20presents%20relevant%20aspects%20of%20the%20deployment%20of%20the%20proposal%20and%20experimental%20results%20to%20show%20its%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2601.15177v1&entry.124074799=Read"},
{"title": "Diffusion In Diffusion: Reclaiming Global Coherence in Semi-Autoregressive Diffusion", "author": "Linrui Ma and Yufei Cui and Kai Han and Yunhe Wang", "abstract": "One of the most compelling features of global discrete diffusion language models is their global bidirectional contextual capability. However, existing block-based diffusion studies tend to introduce autoregressive priors, which, while offering benefits, can cause models to lose this global coherence at the macro level. To regain global contextual understanding while preserving the advantages of the semi-autoregressive paradigm, we propose Diffusion in Diffusion, a 'draft-then-refine' framework designed to overcome the irreversibility and myopia problems inherent in block diffusion models. Our approach first employs block diffusion to generate rapid drafts using small blocks, then refines these drafts through global bidirectional diffusion with a larger bidirectional receptive field. We utilize snapshot confidence remasking to identify the most critical tokens that require modification, and apply mix-scale training to expand the block diffusion model's global capabilities. Empirical results demonstrate that our approach sets a new benchmark for discrete diffusion models on the OpenWebText dataset. Using only 26% of the fine-tuning budget of baseline models, we reduce generative perplexity from 25.7 to 21.9, significantly narrowing the performance gap with autoregressive models.", "link": "http://arxiv.org/abs/2601.13599v2", "date": "2026-01-21", "relevancy": 1.7514, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6332}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5983}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20In%20Diffusion%3A%20Reclaiming%20Global%20Coherence%20in%20Semi-Autoregressive%20Diffusion&body=Title%3A%20Diffusion%20In%20Diffusion%3A%20Reclaiming%20Global%20Coherence%20in%20Semi-Autoregressive%20Diffusion%0AAuthor%3A%20Linrui%20Ma%20and%20Yufei%20Cui%20and%20Kai%20Han%20and%20Yunhe%20Wang%0AAbstract%3A%20One%20of%20the%20most%20compelling%20features%20of%20global%20discrete%20diffusion%20language%20models%20is%20their%20global%20bidirectional%20contextual%20capability.%20However%2C%20existing%20block-based%20diffusion%20studies%20tend%20to%20introduce%20autoregressive%20priors%2C%20which%2C%20while%20offering%20benefits%2C%20can%20cause%20models%20to%20lose%20this%20global%20coherence%20at%20the%20macro%20level.%20To%20regain%20global%20contextual%20understanding%20while%20preserving%20the%20advantages%20of%20the%20semi-autoregressive%20paradigm%2C%20we%20propose%20Diffusion%20in%20Diffusion%2C%20a%20%27draft-then-refine%27%20framework%20designed%20to%20overcome%20the%20irreversibility%20and%20myopia%20problems%20inherent%20in%20block%20diffusion%20models.%20Our%20approach%20first%20employs%20block%20diffusion%20to%20generate%20rapid%20drafts%20using%20small%20blocks%2C%20then%20refines%20these%20drafts%20through%20global%20bidirectional%20diffusion%20with%20a%20larger%20bidirectional%20receptive%20field.%20We%20utilize%20snapshot%20confidence%20remasking%20to%20identify%20the%20most%20critical%20tokens%20that%20require%20modification%2C%20and%20apply%20mix-scale%20training%20to%20expand%20the%20block%20diffusion%20model%27s%20global%20capabilities.%20Empirical%20results%20demonstrate%20that%20our%20approach%20sets%20a%20new%20benchmark%20for%20discrete%20diffusion%20models%20on%20the%20OpenWebText%20dataset.%20Using%20only%2026%25%20of%20the%20fine-tuning%20budget%20of%20baseline%20models%2C%20we%20reduce%20generative%20perplexity%20from%2025.7%20to%2021.9%2C%20significantly%20narrowing%20the%20performance%20gap%20with%20autoregressive%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13599v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520In%2520Diffusion%253A%2520Reclaiming%2520Global%2520Coherence%2520in%2520Semi-Autoregressive%2520Diffusion%26entry.906535625%3DLinrui%2520Ma%2520and%2520Yufei%2520Cui%2520and%2520Kai%2520Han%2520and%2520Yunhe%2520Wang%26entry.1292438233%3DOne%2520of%2520the%2520most%2520compelling%2520features%2520of%2520global%2520discrete%2520diffusion%2520language%2520models%2520is%2520their%2520global%2520bidirectional%2520contextual%2520capability.%2520However%252C%2520existing%2520block-based%2520diffusion%2520studies%2520tend%2520to%2520introduce%2520autoregressive%2520priors%252C%2520which%252C%2520while%2520offering%2520benefits%252C%2520can%2520cause%2520models%2520to%2520lose%2520this%2520global%2520coherence%2520at%2520the%2520macro%2520level.%2520To%2520regain%2520global%2520contextual%2520understanding%2520while%2520preserving%2520the%2520advantages%2520of%2520the%2520semi-autoregressive%2520paradigm%252C%2520we%2520propose%2520Diffusion%2520in%2520Diffusion%252C%2520a%2520%2527draft-then-refine%2527%2520framework%2520designed%2520to%2520overcome%2520the%2520irreversibility%2520and%2520myopia%2520problems%2520inherent%2520in%2520block%2520diffusion%2520models.%2520Our%2520approach%2520first%2520employs%2520block%2520diffusion%2520to%2520generate%2520rapid%2520drafts%2520using%2520small%2520blocks%252C%2520then%2520refines%2520these%2520drafts%2520through%2520global%2520bidirectional%2520diffusion%2520with%2520a%2520larger%2520bidirectional%2520receptive%2520field.%2520We%2520utilize%2520snapshot%2520confidence%2520remasking%2520to%2520identify%2520the%2520most%2520critical%2520tokens%2520that%2520require%2520modification%252C%2520and%2520apply%2520mix-scale%2520training%2520to%2520expand%2520the%2520block%2520diffusion%2520model%2527s%2520global%2520capabilities.%2520Empirical%2520results%2520demonstrate%2520that%2520our%2520approach%2520sets%2520a%2520new%2520benchmark%2520for%2520discrete%2520diffusion%2520models%2520on%2520the%2520OpenWebText%2520dataset.%2520Using%2520only%252026%2525%2520of%2520the%2520fine-tuning%2520budget%2520of%2520baseline%2520models%252C%2520we%2520reduce%2520generative%2520perplexity%2520from%252025.7%2520to%252021.9%252C%2520significantly%2520narrowing%2520the%2520performance%2520gap%2520with%2520autoregressive%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13599v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20In%20Diffusion%3A%20Reclaiming%20Global%20Coherence%20in%20Semi-Autoregressive%20Diffusion&entry.906535625=Linrui%20Ma%20and%20Yufei%20Cui%20and%20Kai%20Han%20and%20Yunhe%20Wang&entry.1292438233=One%20of%20the%20most%20compelling%20features%20of%20global%20discrete%20diffusion%20language%20models%20is%20their%20global%20bidirectional%20contextual%20capability.%20However%2C%20existing%20block-based%20diffusion%20studies%20tend%20to%20introduce%20autoregressive%20priors%2C%20which%2C%20while%20offering%20benefits%2C%20can%20cause%20models%20to%20lose%20this%20global%20coherence%20at%20the%20macro%20level.%20To%20regain%20global%20contextual%20understanding%20while%20preserving%20the%20advantages%20of%20the%20semi-autoregressive%20paradigm%2C%20we%20propose%20Diffusion%20in%20Diffusion%2C%20a%20%27draft-then-refine%27%20framework%20designed%20to%20overcome%20the%20irreversibility%20and%20myopia%20problems%20inherent%20in%20block%20diffusion%20models.%20Our%20approach%20first%20employs%20block%20diffusion%20to%20generate%20rapid%20drafts%20using%20small%20blocks%2C%20then%20refines%20these%20drafts%20through%20global%20bidirectional%20diffusion%20with%20a%20larger%20bidirectional%20receptive%20field.%20We%20utilize%20snapshot%20confidence%20remasking%20to%20identify%20the%20most%20critical%20tokens%20that%20require%20modification%2C%20and%20apply%20mix-scale%20training%20to%20expand%20the%20block%20diffusion%20model%27s%20global%20capabilities.%20Empirical%20results%20demonstrate%20that%20our%20approach%20sets%20a%20new%20benchmark%20for%20discrete%20diffusion%20models%20on%20the%20OpenWebText%20dataset.%20Using%20only%2026%25%20of%20the%20fine-tuning%20budget%20of%20baseline%20models%2C%20we%20reduce%20generative%20perplexity%20from%2025.7%20to%2021.9%2C%20significantly%20narrowing%20the%20performance%20gap%20with%20autoregressive%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.13599v2&entry.124074799=Read"},
{"title": "A Constraint Programming Model for the Super-Agile Earth Observation Satellite Imaging Scheduling Problem", "author": "Margarida Caleiras and Samuel Moniz and Paulo Jorge Nascimento", "abstract": "As the dependence on satellite imaging continues to grow, modern satellites have become increasingly agile, with the new generation, namely super-agile Earth observation satellites (SAEOS), providing unprecedented imaging flexibility. The highly dynamic capabilities of these satellites introduce additional challenges to the scheduling of observation tasks, as existing approaches for conventional agile satellites do not account for variable observation durations and multiple imaging directions. Although some efforts have been made in this regard, the SAEOS imaging scheduling problem (SAEOS-ISP) remains largely unexplored, and no exact approaches have yet been proposed. In this context, this study presents the first exact Constraint Programming formulation for the SAEOS-ISP, considering flexible observation windows, multiple pointing directions and sequence-dependent transition times across multiple satellites. Computational experiments on a newly generated benchmark set demonstrate that the model can be solved efficiently and within very short computational times. Moreover, the results also show that the proposed approach has the potential to achieve higher computational performance compared to the non-exact approaches that are currently considered state-of-the-art.", "link": "http://arxiv.org/abs/2601.11967v2", "date": "2026-01-21", "relevancy": 1.3424, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4766}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4654}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Constraint%20Programming%20Model%20for%20the%20Super-Agile%20Earth%20Observation%20Satellite%20Imaging%20Scheduling%20Problem&body=Title%3A%20A%20Constraint%20Programming%20Model%20for%20the%20Super-Agile%20Earth%20Observation%20Satellite%20Imaging%20Scheduling%20Problem%0AAuthor%3A%20Margarida%20Caleiras%20and%20Samuel%20Moniz%20and%20Paulo%20Jorge%20Nascimento%0AAbstract%3A%20As%20the%20dependence%20on%20satellite%20imaging%20continues%20to%20grow%2C%20modern%20satellites%20have%20become%20increasingly%20agile%2C%20with%20the%20new%20generation%2C%20namely%20super-agile%20Earth%20observation%20satellites%20%28SAEOS%29%2C%20providing%20unprecedented%20imaging%20flexibility.%20The%20highly%20dynamic%20capabilities%20of%20these%20satellites%20introduce%20additional%20challenges%20to%20the%20scheduling%20of%20observation%20tasks%2C%20as%20existing%20approaches%20for%20conventional%20agile%20satellites%20do%20not%20account%20for%20variable%20observation%20durations%20and%20multiple%20imaging%20directions.%20Although%20some%20efforts%20have%20been%20made%20in%20this%20regard%2C%20the%20SAEOS%20imaging%20scheduling%20problem%20%28SAEOS-ISP%29%20remains%20largely%20unexplored%2C%20and%20no%20exact%20approaches%20have%20yet%20been%20proposed.%20In%20this%20context%2C%20this%20study%20presents%20the%20first%20exact%20Constraint%20Programming%20formulation%20for%20the%20SAEOS-ISP%2C%20considering%20flexible%20observation%20windows%2C%20multiple%20pointing%20directions%20and%20sequence-dependent%20transition%20times%20across%20multiple%20satellites.%20Computational%20experiments%20on%20a%20newly%20generated%20benchmark%20set%20demonstrate%20that%20the%20model%20can%20be%20solved%20efficiently%20and%20within%20very%20short%20computational%20times.%20Moreover%2C%20the%20results%20also%20show%20that%20the%20proposed%20approach%20has%20the%20potential%20to%20achieve%20higher%20computational%20performance%20compared%20to%20the%20non-exact%20approaches%20that%20are%20currently%20considered%20state-of-the-art.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11967v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Constraint%2520Programming%2520Model%2520for%2520the%2520Super-Agile%2520Earth%2520Observation%2520Satellite%2520Imaging%2520Scheduling%2520Problem%26entry.906535625%3DMargarida%2520Caleiras%2520and%2520Samuel%2520Moniz%2520and%2520Paulo%2520Jorge%2520Nascimento%26entry.1292438233%3DAs%2520the%2520dependence%2520on%2520satellite%2520imaging%2520continues%2520to%2520grow%252C%2520modern%2520satellites%2520have%2520become%2520increasingly%2520agile%252C%2520with%2520the%2520new%2520generation%252C%2520namely%2520super-agile%2520Earth%2520observation%2520satellites%2520%2528SAEOS%2529%252C%2520providing%2520unprecedented%2520imaging%2520flexibility.%2520The%2520highly%2520dynamic%2520capabilities%2520of%2520these%2520satellites%2520introduce%2520additional%2520challenges%2520to%2520the%2520scheduling%2520of%2520observation%2520tasks%252C%2520as%2520existing%2520approaches%2520for%2520conventional%2520agile%2520satellites%2520do%2520not%2520account%2520for%2520variable%2520observation%2520durations%2520and%2520multiple%2520imaging%2520directions.%2520Although%2520some%2520efforts%2520have%2520been%2520made%2520in%2520this%2520regard%252C%2520the%2520SAEOS%2520imaging%2520scheduling%2520problem%2520%2528SAEOS-ISP%2529%2520remains%2520largely%2520unexplored%252C%2520and%2520no%2520exact%2520approaches%2520have%2520yet%2520been%2520proposed.%2520In%2520this%2520context%252C%2520this%2520study%2520presents%2520the%2520first%2520exact%2520Constraint%2520Programming%2520formulation%2520for%2520the%2520SAEOS-ISP%252C%2520considering%2520flexible%2520observation%2520windows%252C%2520multiple%2520pointing%2520directions%2520and%2520sequence-dependent%2520transition%2520times%2520across%2520multiple%2520satellites.%2520Computational%2520experiments%2520on%2520a%2520newly%2520generated%2520benchmark%2520set%2520demonstrate%2520that%2520the%2520model%2520can%2520be%2520solved%2520efficiently%2520and%2520within%2520very%2520short%2520computational%2520times.%2520Moreover%252C%2520the%2520results%2520also%2520show%2520that%2520the%2520proposed%2520approach%2520has%2520the%2520potential%2520to%2520achieve%2520higher%2520computational%2520performance%2520compared%2520to%2520the%2520non-exact%2520approaches%2520that%2520are%2520currently%2520considered%2520state-of-the-art.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11967v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Constraint%20Programming%20Model%20for%20the%20Super-Agile%20Earth%20Observation%20Satellite%20Imaging%20Scheduling%20Problem&entry.906535625=Margarida%20Caleiras%20and%20Samuel%20Moniz%20and%20Paulo%20Jorge%20Nascimento&entry.1292438233=As%20the%20dependence%20on%20satellite%20imaging%20continues%20to%20grow%2C%20modern%20satellites%20have%20become%20increasingly%20agile%2C%20with%20the%20new%20generation%2C%20namely%20super-agile%20Earth%20observation%20satellites%20%28SAEOS%29%2C%20providing%20unprecedented%20imaging%20flexibility.%20The%20highly%20dynamic%20capabilities%20of%20these%20satellites%20introduce%20additional%20challenges%20to%20the%20scheduling%20of%20observation%20tasks%2C%20as%20existing%20approaches%20for%20conventional%20agile%20satellites%20do%20not%20account%20for%20variable%20observation%20durations%20and%20multiple%20imaging%20directions.%20Although%20some%20efforts%20have%20been%20made%20in%20this%20regard%2C%20the%20SAEOS%20imaging%20scheduling%20problem%20%28SAEOS-ISP%29%20remains%20largely%20unexplored%2C%20and%20no%20exact%20approaches%20have%20yet%20been%20proposed.%20In%20this%20context%2C%20this%20study%20presents%20the%20first%20exact%20Constraint%20Programming%20formulation%20for%20the%20SAEOS-ISP%2C%20considering%20flexible%20observation%20windows%2C%20multiple%20pointing%20directions%20and%20sequence-dependent%20transition%20times%20across%20multiple%20satellites.%20Computational%20experiments%20on%20a%20newly%20generated%20benchmark%20set%20demonstrate%20that%20the%20model%20can%20be%20solved%20efficiently%20and%20within%20very%20short%20computational%20times.%20Moreover%2C%20the%20results%20also%20show%20that%20the%20proposed%20approach%20has%20the%20potential%20to%20achieve%20higher%20computational%20performance%20compared%20to%20the%20non-exact%20approaches%20that%20are%20currently%20considered%20state-of-the-art.&entry.1838667208=http%3A//arxiv.org/abs/2601.11967v2&entry.124074799=Read"},
{"title": "Semantic Image Synthesis via Diffusion Models", "author": "Wengang Zhou and Weilun Wang and Jianmin Bao and Dongdong Chen and Dong Chen and Lu Yuan and Houqiang Li", "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in various image generation tasks compared with Generative Adversarial Nets (GANs). Recent work on semantic image synthesis mainly follows the de facto GAN-based approaches, which may lead to unsatisfactory quality or diversity of generated images. In this paper, we propose a novel framework based on DDPM for semantic image synthesis. Unlike previous conditional diffusion model directly feeds the semantic layout and noisy image as input to a U-Net structure, which may not fully leverage the information in the input semantic mask, our framework processes semantic layout and noisy image differently. It feeds noisy image to the encoder of the U-Net structure while the semantic layout to the decoder by multi-layer spatially-adaptive normalization operators. To further improve the generation quality and semantic interpretability in semantic image synthesis, we introduce the classifier-free guidance sampling strategy, which acknowledge the scores of an unconditional model for sampling process. Extensive experiments on four benchmark datasets demonstrate the effectiveness of our proposed method, achieving state-of-the-art performance in terms of fidelity (FID) and diversity (LPIPS). Our code and pretrained models are available at https://github.com/WeilunWang/semantic-diffusion-model.", "link": "http://arxiv.org/abs/2207.00050v4", "date": "2026-01-21", "relevancy": 1.2956, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6815}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6368}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Image%20Synthesis%20via%20Diffusion%20Models&body=Title%3A%20Semantic%20Image%20Synthesis%20via%20Diffusion%20Models%0AAuthor%3A%20Wengang%20Zhou%20and%20Weilun%20Wang%20and%20Jianmin%20Bao%20and%20Dongdong%20Chen%20and%20Dong%20Chen%20and%20Lu%20Yuan%20and%20Houqiang%20Li%0AAbstract%3A%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%20have%20achieved%20remarkable%20success%20in%20various%20image%20generation%20tasks%20compared%20with%20Generative%20Adversarial%20Nets%20%28GANs%29.%20Recent%20work%20on%20semantic%20image%20synthesis%20mainly%20follows%20the%20de%20facto%20GAN-based%20approaches%2C%20which%20may%20lead%20to%20unsatisfactory%20quality%20or%20diversity%20of%20generated%20images.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20based%20on%20DDPM%20for%20semantic%20image%20synthesis.%20Unlike%20previous%20conditional%20diffusion%20model%20directly%20feeds%20the%20semantic%20layout%20and%20noisy%20image%20as%20input%20to%20a%20U-Net%20structure%2C%20which%20may%20not%20fully%20leverage%20the%20information%20in%20the%20input%20semantic%20mask%2C%20our%20framework%20processes%20semantic%20layout%20and%20noisy%20image%20differently.%20It%20feeds%20noisy%20image%20to%20the%20encoder%20of%20the%20U-Net%20structure%20while%20the%20semantic%20layout%20to%20the%20decoder%20by%20multi-layer%20spatially-adaptive%20normalization%20operators.%20To%20further%20improve%20the%20generation%20quality%20and%20semantic%20interpretability%20in%20semantic%20image%20synthesis%2C%20we%20introduce%20the%20classifier-free%20guidance%20sampling%20strategy%2C%20which%20acknowledge%20the%20scores%20of%20an%20unconditional%20model%20for%20sampling%20process.%20Extensive%20experiments%20on%20four%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method%2C%20achieving%20state-of-the-art%20performance%20in%20terms%20of%20fidelity%20%28FID%29%20and%20diversity%20%28LPIPS%29.%20Our%20code%20and%20pretrained%20models%20are%20available%20at%20https%3A//github.com/WeilunWang/semantic-diffusion-model.%0ALink%3A%20http%3A//arxiv.org/abs/2207.00050v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Image%2520Synthesis%2520via%2520Diffusion%2520Models%26entry.906535625%3DWengang%2520Zhou%2520and%2520Weilun%2520Wang%2520and%2520Jianmin%2520Bao%2520and%2520Dongdong%2520Chen%2520and%2520Dong%2520Chen%2520and%2520Lu%2520Yuan%2520and%2520Houqiang%2520Li%26entry.1292438233%3DDenoising%2520Diffusion%2520Probabilistic%2520Models%2520%2528DDPMs%2529%2520have%2520achieved%2520remarkable%2520success%2520in%2520various%2520image%2520generation%2520tasks%2520compared%2520with%2520Generative%2520Adversarial%2520Nets%2520%2528GANs%2529.%2520Recent%2520work%2520on%2520semantic%2520image%2520synthesis%2520mainly%2520follows%2520the%2520de%2520facto%2520GAN-based%2520approaches%252C%2520which%2520may%2520lead%2520to%2520unsatisfactory%2520quality%2520or%2520diversity%2520of%2520generated%2520images.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520framework%2520based%2520on%2520DDPM%2520for%2520semantic%2520image%2520synthesis.%2520Unlike%2520previous%2520conditional%2520diffusion%2520model%2520directly%2520feeds%2520the%2520semantic%2520layout%2520and%2520noisy%2520image%2520as%2520input%2520to%2520a%2520U-Net%2520structure%252C%2520which%2520may%2520not%2520fully%2520leverage%2520the%2520information%2520in%2520the%2520input%2520semantic%2520mask%252C%2520our%2520framework%2520processes%2520semantic%2520layout%2520and%2520noisy%2520image%2520differently.%2520It%2520feeds%2520noisy%2520image%2520to%2520the%2520encoder%2520of%2520the%2520U-Net%2520structure%2520while%2520the%2520semantic%2520layout%2520to%2520the%2520decoder%2520by%2520multi-layer%2520spatially-adaptive%2520normalization%2520operators.%2520To%2520further%2520improve%2520the%2520generation%2520quality%2520and%2520semantic%2520interpretability%2520in%2520semantic%2520image%2520synthesis%252C%2520we%2520introduce%2520the%2520classifier-free%2520guidance%2520sampling%2520strategy%252C%2520which%2520acknowledge%2520the%2520scores%2520of%2520an%2520unconditional%2520model%2520for%2520sampling%2520process.%2520Extensive%2520experiments%2520on%2520four%2520benchmark%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method%252C%2520achieving%2520state-of-the-art%2520performance%2520in%2520terms%2520of%2520fidelity%2520%2528FID%2529%2520and%2520diversity%2520%2528LPIPS%2529.%2520Our%2520code%2520and%2520pretrained%2520models%2520are%2520available%2520at%2520https%253A//github.com/WeilunWang/semantic-diffusion-model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2207.00050v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Image%20Synthesis%20via%20Diffusion%20Models&entry.906535625=Wengang%20Zhou%20and%20Weilun%20Wang%20and%20Jianmin%20Bao%20and%20Dongdong%20Chen%20and%20Dong%20Chen%20and%20Lu%20Yuan%20and%20Houqiang%20Li&entry.1292438233=Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%20have%20achieved%20remarkable%20success%20in%20various%20image%20generation%20tasks%20compared%20with%20Generative%20Adversarial%20Nets%20%28GANs%29.%20Recent%20work%20on%20semantic%20image%20synthesis%20mainly%20follows%20the%20de%20facto%20GAN-based%20approaches%2C%20which%20may%20lead%20to%20unsatisfactory%20quality%20or%20diversity%20of%20generated%20images.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20based%20on%20DDPM%20for%20semantic%20image%20synthesis.%20Unlike%20previous%20conditional%20diffusion%20model%20directly%20feeds%20the%20semantic%20layout%20and%20noisy%20image%20as%20input%20to%20a%20U-Net%20structure%2C%20which%20may%20not%20fully%20leverage%20the%20information%20in%20the%20input%20semantic%20mask%2C%20our%20framework%20processes%20semantic%20layout%20and%20noisy%20image%20differently.%20It%20feeds%20noisy%20image%20to%20the%20encoder%20of%20the%20U-Net%20structure%20while%20the%20semantic%20layout%20to%20the%20decoder%20by%20multi-layer%20spatially-adaptive%20normalization%20operators.%20To%20further%20improve%20the%20generation%20quality%20and%20semantic%20interpretability%20in%20semantic%20image%20synthesis%2C%20we%20introduce%20the%20classifier-free%20guidance%20sampling%20strategy%2C%20which%20acknowledge%20the%20scores%20of%20an%20unconditional%20model%20for%20sampling%20process.%20Extensive%20experiments%20on%20four%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method%2C%20achieving%20state-of-the-art%20performance%20in%20terms%20of%20fidelity%20%28FID%29%20and%20diversity%20%28LPIPS%29.%20Our%20code%20and%20pretrained%20models%20are%20available%20at%20https%3A//github.com/WeilunWang/semantic-diffusion-model.&entry.1838667208=http%3A//arxiv.org/abs/2207.00050v4&entry.124074799=Read"},
{"title": "Designing AI-Resilient Assessments Using Interconnected Problems: A Theoretically Grounded and Empirically Validated Framework", "author": "Kaihua Ding", "abstract": "The proliferation of generative AI tools has rendered traditional modular assessments in computing and data-centric education increasingly ineffective, creating a disconnect between academic evaluation and authentic skill measurement. This paper presents a theoretically grounded framework for designing AI-resilient assessments, supported by formal analysis and empirical validation.\n  We make three primary contributions. First, we establish two formal propositions. (1) Assessments composed of interconnected problems, in which outputs serve as inputs to subsequent tasks, are inherently more AI-resilient than modular assessments due to their reliance on multi-step reasoning and sustained context. (2) Semi-structured problems with deterministic success criteria provide more reliable measures of student competency than fully open-ended projects, which allow AI systems to default to familiar solution templates. These results challenge widely cited recommendations in recent institutional and policy guidance that promote open-ended assessments as inherently more robust to AI assistance.\n  Second, we validate these propositions through empirical analysis of three university data science courses (N = 117). We observe a substantial AI inflation effect: students achieve near-perfect scores on AI-assisted modular homework, while performance drops by approximately 30 percentage points on proctored exams (Cohen d = 1.51). In contrast, interconnected projects remain strongly aligned with modular assessments (r = 0.954, p < 0.001) while maintaining AI resistance, whereas proctored exams show weaker alignment (r = 0.726, p < 0.001).\n  Third, we translate these findings into a practical assessment design procedure that enables educators to construct evaluations that promote deeper engagement, reflect industry practice, and resist trivial AI delegation.", "link": "http://arxiv.org/abs/2512.10758v3", "date": "2026-01-21", "relevancy": 0.9143, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4682}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4553}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Designing%20AI-Resilient%20Assessments%20Using%20Interconnected%20Problems%3A%20A%20Theoretically%20Grounded%20and%20Empirically%20Validated%20Framework&body=Title%3A%20Designing%20AI-Resilient%20Assessments%20Using%20Interconnected%20Problems%3A%20A%20Theoretically%20Grounded%20and%20Empirically%20Validated%20Framework%0AAuthor%3A%20Kaihua%20Ding%0AAbstract%3A%20The%20proliferation%20of%20generative%20AI%20tools%20has%20rendered%20traditional%20modular%20assessments%20in%20computing%20and%20data-centric%20education%20increasingly%20ineffective%2C%20creating%20a%20disconnect%20between%20academic%20evaluation%20and%20authentic%20skill%20measurement.%20This%20paper%20presents%20a%20theoretically%20grounded%20framework%20for%20designing%20AI-resilient%20assessments%2C%20supported%20by%20formal%20analysis%20and%20empirical%20validation.%0A%20%20We%20make%20three%20primary%20contributions.%20First%2C%20we%20establish%20two%20formal%20propositions.%20%281%29%20Assessments%20composed%20of%20interconnected%20problems%2C%20in%20which%20outputs%20serve%20as%20inputs%20to%20subsequent%20tasks%2C%20are%20inherently%20more%20AI-resilient%20than%20modular%20assessments%20due%20to%20their%20reliance%20on%20multi-step%20reasoning%20and%20sustained%20context.%20%282%29%20Semi-structured%20problems%20with%20deterministic%20success%20criteria%20provide%20more%20reliable%20measures%20of%20student%20competency%20than%20fully%20open-ended%20projects%2C%20which%20allow%20AI%20systems%20to%20default%20to%20familiar%20solution%20templates.%20These%20results%20challenge%20widely%20cited%20recommendations%20in%20recent%20institutional%20and%20policy%20guidance%20that%20promote%20open-ended%20assessments%20as%20inherently%20more%20robust%20to%20AI%20assistance.%0A%20%20Second%2C%20we%20validate%20these%20propositions%20through%20empirical%20analysis%20of%20three%20university%20data%20science%20courses%20%28N%20%3D%20117%29.%20We%20observe%20a%20substantial%20AI%20inflation%20effect%3A%20students%20achieve%20near-perfect%20scores%20on%20AI-assisted%20modular%20homework%2C%20while%20performance%20drops%20by%20approximately%2030%20percentage%20points%20on%20proctored%20exams%20%28Cohen%20d%20%3D%201.51%29.%20In%20contrast%2C%20interconnected%20projects%20remain%20strongly%20aligned%20with%20modular%20assessments%20%28r%20%3D%200.954%2C%20p%20%3C%200.001%29%20while%20maintaining%20AI%20resistance%2C%20whereas%20proctored%20exams%20show%20weaker%20alignment%20%28r%20%3D%200.726%2C%20p%20%3C%200.001%29.%0A%20%20Third%2C%20we%20translate%20these%20findings%20into%20a%20practical%20assessment%20design%20procedure%20that%20enables%20educators%20to%20construct%20evaluations%20that%20promote%20deeper%20engagement%2C%20reflect%20industry%20practice%2C%20and%20resist%20trivial%20AI%20delegation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10758v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesigning%2520AI-Resilient%2520Assessments%2520Using%2520Interconnected%2520Problems%253A%2520A%2520Theoretically%2520Grounded%2520and%2520Empirically%2520Validated%2520Framework%26entry.906535625%3DKaihua%2520Ding%26entry.1292438233%3DThe%2520proliferation%2520of%2520generative%2520AI%2520tools%2520has%2520rendered%2520traditional%2520modular%2520assessments%2520in%2520computing%2520and%2520data-centric%2520education%2520increasingly%2520ineffective%252C%2520creating%2520a%2520disconnect%2520between%2520academic%2520evaluation%2520and%2520authentic%2520skill%2520measurement.%2520This%2520paper%2520presents%2520a%2520theoretically%2520grounded%2520framework%2520for%2520designing%2520AI-resilient%2520assessments%252C%2520supported%2520by%2520formal%2520analysis%2520and%2520empirical%2520validation.%250A%2520%2520We%2520make%2520three%2520primary%2520contributions.%2520First%252C%2520we%2520establish%2520two%2520formal%2520propositions.%2520%25281%2529%2520Assessments%2520composed%2520of%2520interconnected%2520problems%252C%2520in%2520which%2520outputs%2520serve%2520as%2520inputs%2520to%2520subsequent%2520tasks%252C%2520are%2520inherently%2520more%2520AI-resilient%2520than%2520modular%2520assessments%2520due%2520to%2520their%2520reliance%2520on%2520multi-step%2520reasoning%2520and%2520sustained%2520context.%2520%25282%2529%2520Semi-structured%2520problems%2520with%2520deterministic%2520success%2520criteria%2520provide%2520more%2520reliable%2520measures%2520of%2520student%2520competency%2520than%2520fully%2520open-ended%2520projects%252C%2520which%2520allow%2520AI%2520systems%2520to%2520default%2520to%2520familiar%2520solution%2520templates.%2520These%2520results%2520challenge%2520widely%2520cited%2520recommendations%2520in%2520recent%2520institutional%2520and%2520policy%2520guidance%2520that%2520promote%2520open-ended%2520assessments%2520as%2520inherently%2520more%2520robust%2520to%2520AI%2520assistance.%250A%2520%2520Second%252C%2520we%2520validate%2520these%2520propositions%2520through%2520empirical%2520analysis%2520of%2520three%2520university%2520data%2520science%2520courses%2520%2528N%2520%253D%2520117%2529.%2520We%2520observe%2520a%2520substantial%2520AI%2520inflation%2520effect%253A%2520students%2520achieve%2520near-perfect%2520scores%2520on%2520AI-assisted%2520modular%2520homework%252C%2520while%2520performance%2520drops%2520by%2520approximately%252030%2520percentage%2520points%2520on%2520proctored%2520exams%2520%2528Cohen%2520d%2520%253D%25201.51%2529.%2520In%2520contrast%252C%2520interconnected%2520projects%2520remain%2520strongly%2520aligned%2520with%2520modular%2520assessments%2520%2528r%2520%253D%25200.954%252C%2520p%2520%253C%25200.001%2529%2520while%2520maintaining%2520AI%2520resistance%252C%2520whereas%2520proctored%2520exams%2520show%2520weaker%2520alignment%2520%2528r%2520%253D%25200.726%252C%2520p%2520%253C%25200.001%2529.%250A%2520%2520Third%252C%2520we%2520translate%2520these%2520findings%2520into%2520a%2520practical%2520assessment%2520design%2520procedure%2520that%2520enables%2520educators%2520to%2520construct%2520evaluations%2520that%2520promote%2520deeper%2520engagement%252C%2520reflect%2520industry%2520practice%252C%2520and%2520resist%2520trivial%2520AI%2520delegation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10758v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Designing%20AI-Resilient%20Assessments%20Using%20Interconnected%20Problems%3A%20A%20Theoretically%20Grounded%20and%20Empirically%20Validated%20Framework&entry.906535625=Kaihua%20Ding&entry.1292438233=The%20proliferation%20of%20generative%20AI%20tools%20has%20rendered%20traditional%20modular%20assessments%20in%20computing%20and%20data-centric%20education%20increasingly%20ineffective%2C%20creating%20a%20disconnect%20between%20academic%20evaluation%20and%20authentic%20skill%20measurement.%20This%20paper%20presents%20a%20theoretically%20grounded%20framework%20for%20designing%20AI-resilient%20assessments%2C%20supported%20by%20formal%20analysis%20and%20empirical%20validation.%0A%20%20We%20make%20three%20primary%20contributions.%20First%2C%20we%20establish%20two%20formal%20propositions.%20%281%29%20Assessments%20composed%20of%20interconnected%20problems%2C%20in%20which%20outputs%20serve%20as%20inputs%20to%20subsequent%20tasks%2C%20are%20inherently%20more%20AI-resilient%20than%20modular%20assessments%20due%20to%20their%20reliance%20on%20multi-step%20reasoning%20and%20sustained%20context.%20%282%29%20Semi-structured%20problems%20with%20deterministic%20success%20criteria%20provide%20more%20reliable%20measures%20of%20student%20competency%20than%20fully%20open-ended%20projects%2C%20which%20allow%20AI%20systems%20to%20default%20to%20familiar%20solution%20templates.%20These%20results%20challenge%20widely%20cited%20recommendations%20in%20recent%20institutional%20and%20policy%20guidance%20that%20promote%20open-ended%20assessments%20as%20inherently%20more%20robust%20to%20AI%20assistance.%0A%20%20Second%2C%20we%20validate%20these%20propositions%20through%20empirical%20analysis%20of%20three%20university%20data%20science%20courses%20%28N%20%3D%20117%29.%20We%20observe%20a%20substantial%20AI%20inflation%20effect%3A%20students%20achieve%20near-perfect%20scores%20on%20AI-assisted%20modular%20homework%2C%20while%20performance%20drops%20by%20approximately%2030%20percentage%20points%20on%20proctored%20exams%20%28Cohen%20d%20%3D%201.51%29.%20In%20contrast%2C%20interconnected%20projects%20remain%20strongly%20aligned%20with%20modular%20assessments%20%28r%20%3D%200.954%2C%20p%20%3C%200.001%29%20while%20maintaining%20AI%20resistance%2C%20whereas%20proctored%20exams%20show%20weaker%20alignment%20%28r%20%3D%200.726%2C%20p%20%3C%200.001%29.%0A%20%20Third%2C%20we%20translate%20these%20findings%20into%20a%20practical%20assessment%20design%20procedure%20that%20enables%20educators%20to%20construct%20evaluations%20that%20promote%20deeper%20engagement%2C%20reflect%20industry%20practice%2C%20and%20resist%20trivial%20AI%20delegation.&entry.1838667208=http%3A//arxiv.org/abs/2512.10758v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


