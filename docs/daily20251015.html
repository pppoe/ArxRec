<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251014.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D\n  Reconstruction", "author": "Fengzhi Guo and Chih-Chuan Hsu and Sihao Ding and Cheng Zhang", "abstract": "  Reconstructing dynamic 3D scenes from monocular input is fundamentally\nunder-constrained, with ambiguities arising from occlusion and extreme novel\nviews. While dynamic Gaussian Splatting offers an efficient representation,\nvanilla models optimize all Gaussian primitives uniformly, ignoring whether\nthey are well or poorly observed. This limitation leads to motion drifts under\nocclusion and degraded synthesis when extrapolating to unseen views. We argue\nthat uncertainty matters: Gaussians with recurring observations across views\nand time act as reliable anchors to guide motion, whereas those with limited\nvisibility are treated as less reliable. To this end, we introduce USplat4D, a\nnovel Uncertainty-aware dynamic Gaussian Splatting framework that propagates\nreliable motion cues to enhance 4D reconstruction. Our key insight is to\nestimate time-varying per-Gaussian uncertainty and leverages it to construct a\nspatio-temporal graph for uncertainty-aware optimization. Experiments on\ndiverse real and synthetic datasets show that explicitly modeling uncertainty\nconsistently improves dynamic Gaussian Splatting models, yielding more stable\ngeometry under occlusion and high-quality synthesis at extreme viewpoints.\n", "link": "http://arxiv.org/abs/2510.12768v1", "date": "2025-10-14", "relevancy": 3.5148, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7216}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7024}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20Matters%20in%20Dynamic%20Gaussian%20Splatting%20for%20Monocular%204D%0A%20%20Reconstruction&body=Title%3A%20Uncertainty%20Matters%20in%20Dynamic%20Gaussian%20Splatting%20for%20Monocular%204D%0A%20%20Reconstruction%0AAuthor%3A%20Fengzhi%20Guo%20and%20Chih-Chuan%20Hsu%20and%20Sihao%20Ding%20and%20Cheng%20Zhang%0AAbstract%3A%20%20%20Reconstructing%20dynamic%203D%20scenes%20from%20monocular%20input%20is%20fundamentally%0Aunder-constrained%2C%20with%20ambiguities%20arising%20from%20occlusion%20and%20extreme%20novel%0Aviews.%20While%20dynamic%20Gaussian%20Splatting%20offers%20an%20efficient%20representation%2C%0Avanilla%20models%20optimize%20all%20Gaussian%20primitives%20uniformly%2C%20ignoring%20whether%0Athey%20are%20well%20or%20poorly%20observed.%20This%20limitation%20leads%20to%20motion%20drifts%20under%0Aocclusion%20and%20degraded%20synthesis%20when%20extrapolating%20to%20unseen%20views.%20We%20argue%0Athat%20uncertainty%20matters%3A%20Gaussians%20with%20recurring%20observations%20across%20views%0Aand%20time%20act%20as%20reliable%20anchors%20to%20guide%20motion%2C%20whereas%20those%20with%20limited%0Avisibility%20are%20treated%20as%20less%20reliable.%20To%20this%20end%2C%20we%20introduce%20USplat4D%2C%20a%0Anovel%20Uncertainty-aware%20dynamic%20Gaussian%20Splatting%20framework%20that%20propagates%0Areliable%20motion%20cues%20to%20enhance%204D%20reconstruction.%20Our%20key%20insight%20is%20to%0Aestimate%20time-varying%20per-Gaussian%20uncertainty%20and%20leverages%20it%20to%20construct%20a%0Aspatio-temporal%20graph%20for%20uncertainty-aware%20optimization.%20Experiments%20on%0Adiverse%20real%20and%20synthetic%20datasets%20show%20that%20explicitly%20modeling%20uncertainty%0Aconsistently%20improves%20dynamic%20Gaussian%20Splatting%20models%2C%20yielding%20more%20stable%0Ageometry%20under%20occlusion%20and%20high-quality%20synthesis%20at%20extreme%20viewpoints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520Matters%2520in%2520Dynamic%2520Gaussian%2520Splatting%2520for%2520Monocular%25204D%250A%2520%2520Reconstruction%26entry.906535625%3DFengzhi%2520Guo%2520and%2520Chih-Chuan%2520Hsu%2520and%2520Sihao%2520Ding%2520and%2520Cheng%2520Zhang%26entry.1292438233%3D%2520%2520Reconstructing%2520dynamic%25203D%2520scenes%2520from%2520monocular%2520input%2520is%2520fundamentally%250Aunder-constrained%252C%2520with%2520ambiguities%2520arising%2520from%2520occlusion%2520and%2520extreme%2520novel%250Aviews.%2520While%2520dynamic%2520Gaussian%2520Splatting%2520offers%2520an%2520efficient%2520representation%252C%250Avanilla%2520models%2520optimize%2520all%2520Gaussian%2520primitives%2520uniformly%252C%2520ignoring%2520whether%250Athey%2520are%2520well%2520or%2520poorly%2520observed.%2520This%2520limitation%2520leads%2520to%2520motion%2520drifts%2520under%250Aocclusion%2520and%2520degraded%2520synthesis%2520when%2520extrapolating%2520to%2520unseen%2520views.%2520We%2520argue%250Athat%2520uncertainty%2520matters%253A%2520Gaussians%2520with%2520recurring%2520observations%2520across%2520views%250Aand%2520time%2520act%2520as%2520reliable%2520anchors%2520to%2520guide%2520motion%252C%2520whereas%2520those%2520with%2520limited%250Avisibility%2520are%2520treated%2520as%2520less%2520reliable.%2520To%2520this%2520end%252C%2520we%2520introduce%2520USplat4D%252C%2520a%250Anovel%2520Uncertainty-aware%2520dynamic%2520Gaussian%2520Splatting%2520framework%2520that%2520propagates%250Areliable%2520motion%2520cues%2520to%2520enhance%25204D%2520reconstruction.%2520Our%2520key%2520insight%2520is%2520to%250Aestimate%2520time-varying%2520per-Gaussian%2520uncertainty%2520and%2520leverages%2520it%2520to%2520construct%2520a%250Aspatio-temporal%2520graph%2520for%2520uncertainty-aware%2520optimization.%2520Experiments%2520on%250Adiverse%2520real%2520and%2520synthetic%2520datasets%2520show%2520that%2520explicitly%2520modeling%2520uncertainty%250Aconsistently%2520improves%2520dynamic%2520Gaussian%2520Splatting%2520models%252C%2520yielding%2520more%2520stable%250Ageometry%2520under%2520occlusion%2520and%2520high-quality%2520synthesis%2520at%2520extreme%2520viewpoints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Matters%20in%20Dynamic%20Gaussian%20Splatting%20for%20Monocular%204D%0A%20%20Reconstruction&entry.906535625=Fengzhi%20Guo%20and%20Chih-Chuan%20Hsu%20and%20Sihao%20Ding%20and%20Cheng%20Zhang&entry.1292438233=%20%20Reconstructing%20dynamic%203D%20scenes%20from%20monocular%20input%20is%20fundamentally%0Aunder-constrained%2C%20with%20ambiguities%20arising%20from%20occlusion%20and%20extreme%20novel%0Aviews.%20While%20dynamic%20Gaussian%20Splatting%20offers%20an%20efficient%20representation%2C%0Avanilla%20models%20optimize%20all%20Gaussian%20primitives%20uniformly%2C%20ignoring%20whether%0Athey%20are%20well%20or%20poorly%20observed.%20This%20limitation%20leads%20to%20motion%20drifts%20under%0Aocclusion%20and%20degraded%20synthesis%20when%20extrapolating%20to%20unseen%20views.%20We%20argue%0Athat%20uncertainty%20matters%3A%20Gaussians%20with%20recurring%20observations%20across%20views%0Aand%20time%20act%20as%20reliable%20anchors%20to%20guide%20motion%2C%20whereas%20those%20with%20limited%0Avisibility%20are%20treated%20as%20less%20reliable.%20To%20this%20end%2C%20we%20introduce%20USplat4D%2C%20a%0Anovel%20Uncertainty-aware%20dynamic%20Gaussian%20Splatting%20framework%20that%20propagates%0Areliable%20motion%20cues%20to%20enhance%204D%20reconstruction.%20Our%20key%20insight%20is%20to%0Aestimate%20time-varying%20per-Gaussian%20uncertainty%20and%20leverages%20it%20to%20construct%20a%0Aspatio-temporal%20graph%20for%20uncertainty-aware%20optimization.%20Experiments%20on%0Adiverse%20real%20and%20synthetic%20datasets%20show%20that%20explicitly%20modeling%20uncertainty%0Aconsistently%20improves%20dynamic%20Gaussian%20Splatting%20models%2C%20yielding%20more%20stable%0Ageometry%20under%20occlusion%20and%20high-quality%20synthesis%20at%20extreme%20viewpoints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12768v1&entry.124074799=Read"},
{"title": "MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars", "author": "Felix Taubner and Ruihang Zhang and Mathieu Tuli and Sherwin Bahmani and David B. Lindell", "abstract": "  Digital human avatars aim to simulate the dynamic appearance of humans in\nvirtual environments, enabling immersive experiences across gaming, film,\nvirtual reality, and more. However, the conventional process for creating and\nanimating photorealistic human avatars is expensive and time-consuming,\nrequiring large camera capture rigs and significant manual effort from\nprofessional 3D artists. With the advent of capable image and video generation\nmodels, recent methods enable automatic rendering of realistic animated avatars\nfrom a single casually captured reference image of a target subject. While\nthese techniques significantly lower barriers to avatar creation and offer\ncompelling realism, they lack constraints provided by multi-view information or\nan explicit 3D representation. So, image quality and realism degrade when\nrendered from viewpoints that deviate strongly from the reference image. Here,\nwe build a video model that generates animatable multi-view videos of digital\nhumans based on a single reference image and target expressions. Our model,\nMVP4D, is based on a state-of-the-art pre-trained video diffusion model and\ngenerates hundreds of frames simultaneously from viewpoints varying by up to\n360 degrees around a target subject. We show how to distill the outputs of this\nmodel into a 4D avatar that can be rendered in real-time. Our approach\nsignificantly improves the realism, temporal consistency, and 3D consistency of\ngenerated avatars compared to previous methods.\n", "link": "http://arxiv.org/abs/2510.12785v1", "date": "2025-10-14", "relevancy": 3.4993, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7137}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6929}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVP4D%3A%20Multi-View%20Portrait%20Video%20Diffusion%20for%20Animatable%204D%20Avatars&body=Title%3A%20MVP4D%3A%20Multi-View%20Portrait%20Video%20Diffusion%20for%20Animatable%204D%20Avatars%0AAuthor%3A%20Felix%20Taubner%20and%20Ruihang%20Zhang%20and%20Mathieu%20Tuli%20and%20Sherwin%20Bahmani%20and%20David%20B.%20Lindell%0AAbstract%3A%20%20%20Digital%20human%20avatars%20aim%20to%20simulate%20the%20dynamic%20appearance%20of%20humans%20in%0Avirtual%20environments%2C%20enabling%20immersive%20experiences%20across%20gaming%2C%20film%2C%0Avirtual%20reality%2C%20and%20more.%20However%2C%20the%20conventional%20process%20for%20creating%20and%0Aanimating%20photorealistic%20human%20avatars%20is%20expensive%20and%20time-consuming%2C%0Arequiring%20large%20camera%20capture%20rigs%20and%20significant%20manual%20effort%20from%0Aprofessional%203D%20artists.%20With%20the%20advent%20of%20capable%20image%20and%20video%20generation%0Amodels%2C%20recent%20methods%20enable%20automatic%20rendering%20of%20realistic%20animated%20avatars%0Afrom%20a%20single%20casually%20captured%20reference%20image%20of%20a%20target%20subject.%20While%0Athese%20techniques%20significantly%20lower%20barriers%20to%20avatar%20creation%20and%20offer%0Acompelling%20realism%2C%20they%20lack%20constraints%20provided%20by%20multi-view%20information%20or%0Aan%20explicit%203D%20representation.%20So%2C%20image%20quality%20and%20realism%20degrade%20when%0Arendered%20from%20viewpoints%20that%20deviate%20strongly%20from%20the%20reference%20image.%20Here%2C%0Awe%20build%20a%20video%20model%20that%20generates%20animatable%20multi-view%20videos%20of%20digital%0Ahumans%20based%20on%20a%20single%20reference%20image%20and%20target%20expressions.%20Our%20model%2C%0AMVP4D%2C%20is%20based%20on%20a%20state-of-the-art%20pre-trained%20video%20diffusion%20model%20and%0Agenerates%20hundreds%20of%20frames%20simultaneously%20from%20viewpoints%20varying%20by%20up%20to%0A360%20degrees%20around%20a%20target%20subject.%20We%20show%20how%20to%20distill%20the%20outputs%20of%20this%0Amodel%20into%20a%204D%20avatar%20that%20can%20be%20rendered%20in%20real-time.%20Our%20approach%0Asignificantly%20improves%20the%20realism%2C%20temporal%20consistency%2C%20and%203D%20consistency%20of%0Agenerated%20avatars%20compared%20to%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVP4D%253A%2520Multi-View%2520Portrait%2520Video%2520Diffusion%2520for%2520Animatable%25204D%2520Avatars%26entry.906535625%3DFelix%2520Taubner%2520and%2520Ruihang%2520Zhang%2520and%2520Mathieu%2520Tuli%2520and%2520Sherwin%2520Bahmani%2520and%2520David%2520B.%2520Lindell%26entry.1292438233%3D%2520%2520Digital%2520human%2520avatars%2520aim%2520to%2520simulate%2520the%2520dynamic%2520appearance%2520of%2520humans%2520in%250Avirtual%2520environments%252C%2520enabling%2520immersive%2520experiences%2520across%2520gaming%252C%2520film%252C%250Avirtual%2520reality%252C%2520and%2520more.%2520However%252C%2520the%2520conventional%2520process%2520for%2520creating%2520and%250Aanimating%2520photorealistic%2520human%2520avatars%2520is%2520expensive%2520and%2520time-consuming%252C%250Arequiring%2520large%2520camera%2520capture%2520rigs%2520and%2520significant%2520manual%2520effort%2520from%250Aprofessional%25203D%2520artists.%2520With%2520the%2520advent%2520of%2520capable%2520image%2520and%2520video%2520generation%250Amodels%252C%2520recent%2520methods%2520enable%2520automatic%2520rendering%2520of%2520realistic%2520animated%2520avatars%250Afrom%2520a%2520single%2520casually%2520captured%2520reference%2520image%2520of%2520a%2520target%2520subject.%2520While%250Athese%2520techniques%2520significantly%2520lower%2520barriers%2520to%2520avatar%2520creation%2520and%2520offer%250Acompelling%2520realism%252C%2520they%2520lack%2520constraints%2520provided%2520by%2520multi-view%2520information%2520or%250Aan%2520explicit%25203D%2520representation.%2520So%252C%2520image%2520quality%2520and%2520realism%2520degrade%2520when%250Arendered%2520from%2520viewpoints%2520that%2520deviate%2520strongly%2520from%2520the%2520reference%2520image.%2520Here%252C%250Awe%2520build%2520a%2520video%2520model%2520that%2520generates%2520animatable%2520multi-view%2520videos%2520of%2520digital%250Ahumans%2520based%2520on%2520a%2520single%2520reference%2520image%2520and%2520target%2520expressions.%2520Our%2520model%252C%250AMVP4D%252C%2520is%2520based%2520on%2520a%2520state-of-the-art%2520pre-trained%2520video%2520diffusion%2520model%2520and%250Agenerates%2520hundreds%2520of%2520frames%2520simultaneously%2520from%2520viewpoints%2520varying%2520by%2520up%2520to%250A360%2520degrees%2520around%2520a%2520target%2520subject.%2520We%2520show%2520how%2520to%2520distill%2520the%2520outputs%2520of%2520this%250Amodel%2520into%2520a%25204D%2520avatar%2520that%2520can%2520be%2520rendered%2520in%2520real-time.%2520Our%2520approach%250Asignificantly%2520improves%2520the%2520realism%252C%2520temporal%2520consistency%252C%2520and%25203D%2520consistency%2520of%250Agenerated%2520avatars%2520compared%2520to%2520previous%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVP4D%3A%20Multi-View%20Portrait%20Video%20Diffusion%20for%20Animatable%204D%20Avatars&entry.906535625=Felix%20Taubner%20and%20Ruihang%20Zhang%20and%20Mathieu%20Tuli%20and%20Sherwin%20Bahmani%20and%20David%20B.%20Lindell&entry.1292438233=%20%20Digital%20human%20avatars%20aim%20to%20simulate%20the%20dynamic%20appearance%20of%20humans%20in%0Avirtual%20environments%2C%20enabling%20immersive%20experiences%20across%20gaming%2C%20film%2C%0Avirtual%20reality%2C%20and%20more.%20However%2C%20the%20conventional%20process%20for%20creating%20and%0Aanimating%20photorealistic%20human%20avatars%20is%20expensive%20and%20time-consuming%2C%0Arequiring%20large%20camera%20capture%20rigs%20and%20significant%20manual%20effort%20from%0Aprofessional%203D%20artists.%20With%20the%20advent%20of%20capable%20image%20and%20video%20generation%0Amodels%2C%20recent%20methods%20enable%20automatic%20rendering%20of%20realistic%20animated%20avatars%0Afrom%20a%20single%20casually%20captured%20reference%20image%20of%20a%20target%20subject.%20While%0Athese%20techniques%20significantly%20lower%20barriers%20to%20avatar%20creation%20and%20offer%0Acompelling%20realism%2C%20they%20lack%20constraints%20provided%20by%20multi-view%20information%20or%0Aan%20explicit%203D%20representation.%20So%2C%20image%20quality%20and%20realism%20degrade%20when%0Arendered%20from%20viewpoints%20that%20deviate%20strongly%20from%20the%20reference%20image.%20Here%2C%0Awe%20build%20a%20video%20model%20that%20generates%20animatable%20multi-view%20videos%20of%20digital%0Ahumans%20based%20on%20a%20single%20reference%20image%20and%20target%20expressions.%20Our%20model%2C%0AMVP4D%2C%20is%20based%20on%20a%20state-of-the-art%20pre-trained%20video%20diffusion%20model%20and%0Agenerates%20hundreds%20of%20frames%20simultaneously%20from%20viewpoints%20varying%20by%20up%20to%0A360%20degrees%20around%20a%20target%20subject.%20We%20show%20how%20to%20distill%20the%20outputs%20of%20this%0Amodel%20into%20a%204D%20avatar%20that%20can%20be%20rendered%20in%20real-time.%20Our%20approach%0Asignificantly%20improves%20the%20realism%2C%20temporal%20consistency%2C%20and%203D%20consistency%20of%0Agenerated%20avatars%20compared%20to%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12785v1&entry.124074799=Read"},
{"title": "OpenLex3D: A Tiered Evaluation Benchmark for Open-Vocabulary 3D Scene\n  Representations", "author": "Christina Kassab and Sacha Morin and Martin B\u00fcchner and Mat\u00edas Mattamala and Kumaraditya Gupta and Abhinav Valada and Liam Paull and Maurice Fallon", "abstract": "  3D scene understanding has been transformed by open-vocabulary language\nmodels that enable interaction via natural language. However, at present the\nevaluation of these representations is limited to datasets with closed-set\nsemantics that do not capture the richness of language. This work presents\nOpenLex3D, a dedicated benchmark for evaluating 3D open-vocabulary scene\nrepresentations. OpenLex3D provides entirely new label annotations for scenes\nfrom Replica, ScanNet++, and HM3D, which capture real-world linguistic\nvariability by introducing synonymical object categories and additional nuanced\ndescriptions. Our label sets provide 13 times more labels per scene than the\noriginal datasets. By introducing an open-set 3D semantic segmentation task and\nan object retrieval task, we evaluate various existing 3D open-vocabulary\nmethods on OpenLex3D, showcasing failure cases, and avenues for improvement.\nOur experiments provide insights on feature precision, segmentation, and\ndownstream capabilities. The benchmark is publicly available at:\nhttps://openlex3d.github.io/.\n", "link": "http://arxiv.org/abs/2503.19764v2", "date": "2025-10-14", "relevancy": 3.4552, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.7361}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.7361}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenLex3D%3A%20A%20Tiered%20Evaluation%20Benchmark%20for%20Open-Vocabulary%203D%20Scene%0A%20%20Representations&body=Title%3A%20OpenLex3D%3A%20A%20Tiered%20Evaluation%20Benchmark%20for%20Open-Vocabulary%203D%20Scene%0A%20%20Representations%0AAuthor%3A%20Christina%20Kassab%20and%20Sacha%20Morin%20and%20Martin%20B%C3%BCchner%20and%20Mat%C3%ADas%20Mattamala%20and%20Kumaraditya%20Gupta%20and%20Abhinav%20Valada%20and%20Liam%20Paull%20and%20Maurice%20Fallon%0AAbstract%3A%20%20%203D%20scene%20understanding%20has%20been%20transformed%20by%20open-vocabulary%20language%0Amodels%20that%20enable%20interaction%20via%20natural%20language.%20However%2C%20at%20present%20the%0Aevaluation%20of%20these%20representations%20is%20limited%20to%20datasets%20with%20closed-set%0Asemantics%20that%20do%20not%20capture%20the%20richness%20of%20language.%20This%20work%20presents%0AOpenLex3D%2C%20a%20dedicated%20benchmark%20for%20evaluating%203D%20open-vocabulary%20scene%0Arepresentations.%20OpenLex3D%20provides%20entirely%20new%20label%20annotations%20for%20scenes%0Afrom%20Replica%2C%20ScanNet%2B%2B%2C%20and%20HM3D%2C%20which%20capture%20real-world%20linguistic%0Avariability%20by%20introducing%20synonymical%20object%20categories%20and%20additional%20nuanced%0Adescriptions.%20Our%20label%20sets%20provide%2013%20times%20more%20labels%20per%20scene%20than%20the%0Aoriginal%20datasets.%20By%20introducing%20an%20open-set%203D%20semantic%20segmentation%20task%20and%0Aan%20object%20retrieval%20task%2C%20we%20evaluate%20various%20existing%203D%20open-vocabulary%0Amethods%20on%20OpenLex3D%2C%20showcasing%20failure%20cases%2C%20and%20avenues%20for%20improvement.%0AOur%20experiments%20provide%20insights%20on%20feature%20precision%2C%20segmentation%2C%20and%0Adownstream%20capabilities.%20The%20benchmark%20is%20publicly%20available%20at%3A%0Ahttps%3A//openlex3d.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.19764v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenLex3D%253A%2520A%2520Tiered%2520Evaluation%2520Benchmark%2520for%2520Open-Vocabulary%25203D%2520Scene%250A%2520%2520Representations%26entry.906535625%3DChristina%2520Kassab%2520and%2520Sacha%2520Morin%2520and%2520Martin%2520B%25C3%25BCchner%2520and%2520Mat%25C3%25ADas%2520Mattamala%2520and%2520Kumaraditya%2520Gupta%2520and%2520Abhinav%2520Valada%2520and%2520Liam%2520Paull%2520and%2520Maurice%2520Fallon%26entry.1292438233%3D%2520%25203D%2520scene%2520understanding%2520has%2520been%2520transformed%2520by%2520open-vocabulary%2520language%250Amodels%2520that%2520enable%2520interaction%2520via%2520natural%2520language.%2520However%252C%2520at%2520present%2520the%250Aevaluation%2520of%2520these%2520representations%2520is%2520limited%2520to%2520datasets%2520with%2520closed-set%250Asemantics%2520that%2520do%2520not%2520capture%2520the%2520richness%2520of%2520language.%2520This%2520work%2520presents%250AOpenLex3D%252C%2520a%2520dedicated%2520benchmark%2520for%2520evaluating%25203D%2520open-vocabulary%2520scene%250Arepresentations.%2520OpenLex3D%2520provides%2520entirely%2520new%2520label%2520annotations%2520for%2520scenes%250Afrom%2520Replica%252C%2520ScanNet%252B%252B%252C%2520and%2520HM3D%252C%2520which%2520capture%2520real-world%2520linguistic%250Avariability%2520by%2520introducing%2520synonymical%2520object%2520categories%2520and%2520additional%2520nuanced%250Adescriptions.%2520Our%2520label%2520sets%2520provide%252013%2520times%2520more%2520labels%2520per%2520scene%2520than%2520the%250Aoriginal%2520datasets.%2520By%2520introducing%2520an%2520open-set%25203D%2520semantic%2520segmentation%2520task%2520and%250Aan%2520object%2520retrieval%2520task%252C%2520we%2520evaluate%2520various%2520existing%25203D%2520open-vocabulary%250Amethods%2520on%2520OpenLex3D%252C%2520showcasing%2520failure%2520cases%252C%2520and%2520avenues%2520for%2520improvement.%250AOur%2520experiments%2520provide%2520insights%2520on%2520feature%2520precision%252C%2520segmentation%252C%2520and%250Adownstream%2520capabilities.%2520The%2520benchmark%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//openlex3d.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19764v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenLex3D%3A%20A%20Tiered%20Evaluation%20Benchmark%20for%20Open-Vocabulary%203D%20Scene%0A%20%20Representations&entry.906535625=Christina%20Kassab%20and%20Sacha%20Morin%20and%20Martin%20B%C3%BCchner%20and%20Mat%C3%ADas%20Mattamala%20and%20Kumaraditya%20Gupta%20and%20Abhinav%20Valada%20and%20Liam%20Paull%20and%20Maurice%20Fallon&entry.1292438233=%20%203D%20scene%20understanding%20has%20been%20transformed%20by%20open-vocabulary%20language%0Amodels%20that%20enable%20interaction%20via%20natural%20language.%20However%2C%20at%20present%20the%0Aevaluation%20of%20these%20representations%20is%20limited%20to%20datasets%20with%20closed-set%0Asemantics%20that%20do%20not%20capture%20the%20richness%20of%20language.%20This%20work%20presents%0AOpenLex3D%2C%20a%20dedicated%20benchmark%20for%20evaluating%203D%20open-vocabulary%20scene%0Arepresentations.%20OpenLex3D%20provides%20entirely%20new%20label%20annotations%20for%20scenes%0Afrom%20Replica%2C%20ScanNet%2B%2B%2C%20and%20HM3D%2C%20which%20capture%20real-world%20linguistic%0Avariability%20by%20introducing%20synonymical%20object%20categories%20and%20additional%20nuanced%0Adescriptions.%20Our%20label%20sets%20provide%2013%20times%20more%20labels%20per%20scene%20than%20the%0Aoriginal%20datasets.%20By%20introducing%20an%20open-set%203D%20semantic%20segmentation%20task%20and%0Aan%20object%20retrieval%20task%2C%20we%20evaluate%20various%20existing%203D%20open-vocabulary%0Amethods%20on%20OpenLex3D%2C%20showcasing%20failure%20cases%2C%20and%20avenues%20for%20improvement.%0AOur%20experiments%20provide%20insights%20on%20feature%20precision%2C%20segmentation%2C%20and%0Adownstream%20capabilities.%20The%20benchmark%20is%20publicly%20available%20at%3A%0Ahttps%3A//openlex3d.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.19764v2&entry.124074799=Read"},
{"title": "BSGS: Bi-stage 3D Gaussian Splatting for Camera Motion Deblurring", "author": "An Zhao and Piaopiao Yu and Zhe Zhu and Mingqiang Wei", "abstract": "  3D Gaussian Splatting has exhibited remarkable capabilities in 3D scene\nreconstruction.However, reconstructing high-quality 3D scenes from\nmotion-blurred images caused by camera motion poses a significant challenge.The\nperformance of existing 3DGS-based deblurring methods are limited due to their\ninherent mechanisms, such as extreme dependence on the accuracy of camera poses\nand inability to effectively control erroneous Gaussian primitives\ndensification caused by motion blur.To solve these problems, we introduce a\nnovel framework, Bi-Stage 3D Gaussian Splatting, to accurately reconstruct 3D\nscenes from motion-blurred images.BSGS contains two stages. First, Camera Pose\nRefinement roughly optimizes camera poses to reduce motion-induced distortions.\nSecond, with fixed rough camera poses, Global RigidTransformation further\ncorrects motion-induced blur distortions.To alleviate multi-subframe gradient\nconflicts, we propose a subframe gradient aggregation strategy to optimize both\nstages.Furthermore, a space-time bi-stage optimization strategy is introduced\nto dynamically adjust primitive densification thresholds and prevent premature\nnoisy Gaussian generation in blurred regions. Comprehensive experiments verify\nthe effectiveness of our proposed deblurring method and show its superiority\nover the state of the arts.\n", "link": "http://arxiv.org/abs/2510.12493v1", "date": "2025-10-14", "relevancy": 3.3041, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7087}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6546}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BSGS%3A%20Bi-stage%203D%20Gaussian%20Splatting%20for%20Camera%20Motion%20Deblurring&body=Title%3A%20BSGS%3A%20Bi-stage%203D%20Gaussian%20Splatting%20for%20Camera%20Motion%20Deblurring%0AAuthor%3A%20An%20Zhao%20and%20Piaopiao%20Yu%20and%20Zhe%20Zhu%20and%20Mingqiang%20Wei%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20has%20exhibited%20remarkable%20capabilities%20in%203D%20scene%0Areconstruction.However%2C%20reconstructing%20high-quality%203D%20scenes%20from%0Amotion-blurred%20images%20caused%20by%20camera%20motion%20poses%20a%20significant%20challenge.The%0Aperformance%20of%20existing%203DGS-based%20deblurring%20methods%20are%20limited%20due%20to%20their%0Ainherent%20mechanisms%2C%20such%20as%20extreme%20dependence%20on%20the%20accuracy%20of%20camera%20poses%0Aand%20inability%20to%20effectively%20control%20erroneous%20Gaussian%20primitives%0Adensification%20caused%20by%20motion%20blur.To%20solve%20these%20problems%2C%20we%20introduce%20a%0Anovel%20framework%2C%20Bi-Stage%203D%20Gaussian%20Splatting%2C%20to%20accurately%20reconstruct%203D%0Ascenes%20from%20motion-blurred%20images.BSGS%20contains%20two%20stages.%20First%2C%20Camera%20Pose%0ARefinement%20roughly%20optimizes%20camera%20poses%20to%20reduce%20motion-induced%20distortions.%0ASecond%2C%20with%20fixed%20rough%20camera%20poses%2C%20Global%20RigidTransformation%20further%0Acorrects%20motion-induced%20blur%20distortions.To%20alleviate%20multi-subframe%20gradient%0Aconflicts%2C%20we%20propose%20a%20subframe%20gradient%20aggregation%20strategy%20to%20optimize%20both%0Astages.Furthermore%2C%20a%20space-time%20bi-stage%20optimization%20strategy%20is%20introduced%0Ato%20dynamically%20adjust%20primitive%20densification%20thresholds%20and%20prevent%20premature%0Anoisy%20Gaussian%20generation%20in%20blurred%20regions.%20Comprehensive%20experiments%20verify%0Athe%20effectiveness%20of%20our%20proposed%20deblurring%20method%20and%20show%20its%20superiority%0Aover%20the%20state%20of%20the%20arts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12493v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBSGS%253A%2520Bi-stage%25203D%2520Gaussian%2520Splatting%2520for%2520Camera%2520Motion%2520Deblurring%26entry.906535625%3DAn%2520Zhao%2520and%2520Piaopiao%2520Yu%2520and%2520Zhe%2520Zhu%2520and%2520Mingqiang%2520Wei%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520has%2520exhibited%2520remarkable%2520capabilities%2520in%25203D%2520scene%250Areconstruction.However%252C%2520reconstructing%2520high-quality%25203D%2520scenes%2520from%250Amotion-blurred%2520images%2520caused%2520by%2520camera%2520motion%2520poses%2520a%2520significant%2520challenge.The%250Aperformance%2520of%2520existing%25203DGS-based%2520deblurring%2520methods%2520are%2520limited%2520due%2520to%2520their%250Ainherent%2520mechanisms%252C%2520such%2520as%2520extreme%2520dependence%2520on%2520the%2520accuracy%2520of%2520camera%2520poses%250Aand%2520inability%2520to%2520effectively%2520control%2520erroneous%2520Gaussian%2520primitives%250Adensification%2520caused%2520by%2520motion%2520blur.To%2520solve%2520these%2520problems%252C%2520we%2520introduce%2520a%250Anovel%2520framework%252C%2520Bi-Stage%25203D%2520Gaussian%2520Splatting%252C%2520to%2520accurately%2520reconstruct%25203D%250Ascenes%2520from%2520motion-blurred%2520images.BSGS%2520contains%2520two%2520stages.%2520First%252C%2520Camera%2520Pose%250ARefinement%2520roughly%2520optimizes%2520camera%2520poses%2520to%2520reduce%2520motion-induced%2520distortions.%250ASecond%252C%2520with%2520fixed%2520rough%2520camera%2520poses%252C%2520Global%2520RigidTransformation%2520further%250Acorrects%2520motion-induced%2520blur%2520distortions.To%2520alleviate%2520multi-subframe%2520gradient%250Aconflicts%252C%2520we%2520propose%2520a%2520subframe%2520gradient%2520aggregation%2520strategy%2520to%2520optimize%2520both%250Astages.Furthermore%252C%2520a%2520space-time%2520bi-stage%2520optimization%2520strategy%2520is%2520introduced%250Ato%2520dynamically%2520adjust%2520primitive%2520densification%2520thresholds%2520and%2520prevent%2520premature%250Anoisy%2520Gaussian%2520generation%2520in%2520blurred%2520regions.%2520Comprehensive%2520experiments%2520verify%250Athe%2520effectiveness%2520of%2520our%2520proposed%2520deblurring%2520method%2520and%2520show%2520its%2520superiority%250Aover%2520the%2520state%2520of%2520the%2520arts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12493v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BSGS%3A%20Bi-stage%203D%20Gaussian%20Splatting%20for%20Camera%20Motion%20Deblurring&entry.906535625=An%20Zhao%20and%20Piaopiao%20Yu%20and%20Zhe%20Zhu%20and%20Mingqiang%20Wei&entry.1292438233=%20%203D%20Gaussian%20Splatting%20has%20exhibited%20remarkable%20capabilities%20in%203D%20scene%0Areconstruction.However%2C%20reconstructing%20high-quality%203D%20scenes%20from%0Amotion-blurred%20images%20caused%20by%20camera%20motion%20poses%20a%20significant%20challenge.The%0Aperformance%20of%20existing%203DGS-based%20deblurring%20methods%20are%20limited%20due%20to%20their%0Ainherent%20mechanisms%2C%20such%20as%20extreme%20dependence%20on%20the%20accuracy%20of%20camera%20poses%0Aand%20inability%20to%20effectively%20control%20erroneous%20Gaussian%20primitives%0Adensification%20caused%20by%20motion%20blur.To%20solve%20these%20problems%2C%20we%20introduce%20a%0Anovel%20framework%2C%20Bi-Stage%203D%20Gaussian%20Splatting%2C%20to%20accurately%20reconstruct%203D%0Ascenes%20from%20motion-blurred%20images.BSGS%20contains%20two%20stages.%20First%2C%20Camera%20Pose%0ARefinement%20roughly%20optimizes%20camera%20poses%20to%20reduce%20motion-induced%20distortions.%0ASecond%2C%20with%20fixed%20rough%20camera%20poses%2C%20Global%20RigidTransformation%20further%0Acorrects%20motion-induced%20blur%20distortions.To%20alleviate%20multi-subframe%20gradient%0Aconflicts%2C%20we%20propose%20a%20subframe%20gradient%20aggregation%20strategy%20to%20optimize%20both%0Astages.Furthermore%2C%20a%20space-time%20bi-stage%20optimization%20strategy%20is%20introduced%0Ato%20dynamically%20adjust%20primitive%20densification%20thresholds%20and%20prevent%20premature%0Anoisy%20Gaussian%20generation%20in%20blurred%20regions.%20Comprehensive%20experiments%20verify%0Athe%20effectiveness%20of%20our%20proposed%20deblurring%20method%20and%20show%20its%20superiority%0Aover%20the%20state%20of%20the%20arts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12493v1&entry.124074799=Read"},
{"title": "Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding", "author": "Lin Long and Changdae Oh and Seongheon Park and Sharon Li", "abstract": "  Large vision-language models (LVLMs) achieve strong performance on multimodal\ntasks, yet they often default to their language prior (LP) -- memorized textual\npatterns from pre-training while under-utilizing visual evidence. Prior\nanalyses of LP mostly rely on input-output probing, which fails to reveal the\ninternal mechanisms governing when and how vision influences model behavior. To\naddress this gap, we present the first systematic analysis of language prior\nthrough the lens of chain-of-embedding, which examines the layer-wise\nrepresentation dynamics within LVLMs. Our analysis reveals a universal\nphenomenon: each model exhibits a Visual Integration Point (VIP), a critical\nlayer at which visual information begins to meaningfully reshape hidden\nrepresentations and influence decoding. Building on this observation, we\nintroduce the Total Visual Integration (TVI) estimator, which aggregates\nrepresentation distance beyond the VIP to quantify how strongly visual query\ninfluences response generation. Across 54 model-dataset combinations spanning 9\ncontemporary LVLMs and 6 benchmarks, we demonstrate that VIP consistently\nemerges, and that TVI reliably predicts the strength of language prior. This\noffers a principled toolkit for diagnosing and understanding language prior in\nLVLMs.\n", "link": "http://arxiv.org/abs/2509.23050v2", "date": "2025-10-14", "relevancy": 3.1317, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.667}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.667}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Language%20Prior%20of%20LVLMs%20by%20Contrasting%20Chain-of-Embedding&body=Title%3A%20Understanding%20Language%20Prior%20of%20LVLMs%20by%20Contrasting%20Chain-of-Embedding%0AAuthor%3A%20Lin%20Long%20and%20Changdae%20Oh%20and%20Seongheon%20Park%20and%20Sharon%20Li%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28LVLMs%29%20achieve%20strong%20performance%20on%20multimodal%0Atasks%2C%20yet%20they%20often%20default%20to%20their%20language%20prior%20%28LP%29%20--%20memorized%20textual%0Apatterns%20from%20pre-training%20while%20under-utilizing%20visual%20evidence.%20Prior%0Aanalyses%20of%20LP%20mostly%20rely%20on%20input-output%20probing%2C%20which%20fails%20to%20reveal%20the%0Ainternal%20mechanisms%20governing%20when%20and%20how%20vision%20influences%20model%20behavior.%20To%0Aaddress%20this%20gap%2C%20we%20present%20the%20first%20systematic%20analysis%20of%20language%20prior%0Athrough%20the%20lens%20of%20chain-of-embedding%2C%20which%20examines%20the%20layer-wise%0Arepresentation%20dynamics%20within%20LVLMs.%20Our%20analysis%20reveals%20a%20universal%0Aphenomenon%3A%20each%20model%20exhibits%20a%20Visual%20Integration%20Point%20%28VIP%29%2C%20a%20critical%0Alayer%20at%20which%20visual%20information%20begins%20to%20meaningfully%20reshape%20hidden%0Arepresentations%20and%20influence%20decoding.%20Building%20on%20this%20observation%2C%20we%0Aintroduce%20the%20Total%20Visual%20Integration%20%28TVI%29%20estimator%2C%20which%20aggregates%0Arepresentation%20distance%20beyond%20the%20VIP%20to%20quantify%20how%20strongly%20visual%20query%0Ainfluences%20response%20generation.%20Across%2054%20model-dataset%20combinations%20spanning%209%0Acontemporary%20LVLMs%20and%206%20benchmarks%2C%20we%20demonstrate%20that%20VIP%20consistently%0Aemerges%2C%20and%20that%20TVI%20reliably%20predicts%20the%20strength%20of%20language%20prior.%20This%0Aoffers%20a%20principled%20toolkit%20for%20diagnosing%20and%20understanding%20language%20prior%20in%0ALVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.23050v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Language%2520Prior%2520of%2520LVLMs%2520by%2520Contrasting%2520Chain-of-Embedding%26entry.906535625%3DLin%2520Long%2520and%2520Changdae%2520Oh%2520and%2520Seongheon%2520Park%2520and%2520Sharon%2520Li%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528LVLMs%2529%2520achieve%2520strong%2520performance%2520on%2520multimodal%250Atasks%252C%2520yet%2520they%2520often%2520default%2520to%2520their%2520language%2520prior%2520%2528LP%2529%2520--%2520memorized%2520textual%250Apatterns%2520from%2520pre-training%2520while%2520under-utilizing%2520visual%2520evidence.%2520Prior%250Aanalyses%2520of%2520LP%2520mostly%2520rely%2520on%2520input-output%2520probing%252C%2520which%2520fails%2520to%2520reveal%2520the%250Ainternal%2520mechanisms%2520governing%2520when%2520and%2520how%2520vision%2520influences%2520model%2520behavior.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520present%2520the%2520first%2520systematic%2520analysis%2520of%2520language%2520prior%250Athrough%2520the%2520lens%2520of%2520chain-of-embedding%252C%2520which%2520examines%2520the%2520layer-wise%250Arepresentation%2520dynamics%2520within%2520LVLMs.%2520Our%2520analysis%2520reveals%2520a%2520universal%250Aphenomenon%253A%2520each%2520model%2520exhibits%2520a%2520Visual%2520Integration%2520Point%2520%2528VIP%2529%252C%2520a%2520critical%250Alayer%2520at%2520which%2520visual%2520information%2520begins%2520to%2520meaningfully%2520reshape%2520hidden%250Arepresentations%2520and%2520influence%2520decoding.%2520Building%2520on%2520this%2520observation%252C%2520we%250Aintroduce%2520the%2520Total%2520Visual%2520Integration%2520%2528TVI%2529%2520estimator%252C%2520which%2520aggregates%250Arepresentation%2520distance%2520beyond%2520the%2520VIP%2520to%2520quantify%2520how%2520strongly%2520visual%2520query%250Ainfluences%2520response%2520generation.%2520Across%252054%2520model-dataset%2520combinations%2520spanning%25209%250Acontemporary%2520LVLMs%2520and%25206%2520benchmarks%252C%2520we%2520demonstrate%2520that%2520VIP%2520consistently%250Aemerges%252C%2520and%2520that%2520TVI%2520reliably%2520predicts%2520the%2520strength%2520of%2520language%2520prior.%2520This%250Aoffers%2520a%2520principled%2520toolkit%2520for%2520diagnosing%2520and%2520understanding%2520language%2520prior%2520in%250ALVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23050v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Language%20Prior%20of%20LVLMs%20by%20Contrasting%20Chain-of-Embedding&entry.906535625=Lin%20Long%20and%20Changdae%20Oh%20and%20Seongheon%20Park%20and%20Sharon%20Li&entry.1292438233=%20%20Large%20vision-language%20models%20%28LVLMs%29%20achieve%20strong%20performance%20on%20multimodal%0Atasks%2C%20yet%20they%20often%20default%20to%20their%20language%20prior%20%28LP%29%20--%20memorized%20textual%0Apatterns%20from%20pre-training%20while%20under-utilizing%20visual%20evidence.%20Prior%0Aanalyses%20of%20LP%20mostly%20rely%20on%20input-output%20probing%2C%20which%20fails%20to%20reveal%20the%0Ainternal%20mechanisms%20governing%20when%20and%20how%20vision%20influences%20model%20behavior.%20To%0Aaddress%20this%20gap%2C%20we%20present%20the%20first%20systematic%20analysis%20of%20language%20prior%0Athrough%20the%20lens%20of%20chain-of-embedding%2C%20which%20examines%20the%20layer-wise%0Arepresentation%20dynamics%20within%20LVLMs.%20Our%20analysis%20reveals%20a%20universal%0Aphenomenon%3A%20each%20model%20exhibits%20a%20Visual%20Integration%20Point%20%28VIP%29%2C%20a%20critical%0Alayer%20at%20which%20visual%20information%20begins%20to%20meaningfully%20reshape%20hidden%0Arepresentations%20and%20influence%20decoding.%20Building%20on%20this%20observation%2C%20we%0Aintroduce%20the%20Total%20Visual%20Integration%20%28TVI%29%20estimator%2C%20which%20aggregates%0Arepresentation%20distance%20beyond%20the%20VIP%20to%20quantify%20how%20strongly%20visual%20query%0Ainfluences%20response%20generation.%20Across%2054%20model-dataset%20combinations%20spanning%209%0Acontemporary%20LVLMs%20and%206%20benchmarks%2C%20we%20demonstrate%20that%20VIP%20consistently%0Aemerges%2C%20and%20that%20TVI%20reliably%20predicts%20the%20strength%20of%20language%20prior.%20This%0Aoffers%20a%20principled%20toolkit%20for%20diagnosing%20and%20understanding%20language%20prior%20in%0ALVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.23050v2&entry.124074799=Read"},
{"title": "TTT3R: 3D Reconstruction as Test-Time Training", "author": "Xingyu Chen and Yue Chen and Yuliang Xiu and Andreas Geiger and Anpei Chen", "abstract": "  Modern Recurrent Neural Networks have become a competitive architecture for\n3D reconstruction due to their linear-time complexity. However, their\nperformance degrades significantly when applied beyond the training context\nlength, revealing limited length generalization. In this work, we revisit the\n3D reconstruction foundation models from a Test-Time Training perspective,\nframing their designs as an online learning problem. Building on this\nperspective, we leverage the alignment confidence between the memory state and\nincoming observations to derive a closed-form learning rate for memory updates,\nto balance between retaining historical information and adapting to new\nobservations. This training-free intervention, termed TTT3R, substantially\nimproves length generalization, achieving a $2\\times$ improvement in global\npose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU\nmemory to process thousands of images. Code available in\nhttps://rover-xingyu.github.io/TTT3R\n", "link": "http://arxiv.org/abs/2509.26645v2", "date": "2025-10-14", "relevancy": 3.0765, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6474}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6095}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TTT3R%3A%203D%20Reconstruction%20as%20Test-Time%20Training&body=Title%3A%20TTT3R%3A%203D%20Reconstruction%20as%20Test-Time%20Training%0AAuthor%3A%20Xingyu%20Chen%20and%20Yue%20Chen%20and%20Yuliang%20Xiu%20and%20Andreas%20Geiger%20and%20Anpei%20Chen%0AAbstract%3A%20%20%20Modern%20Recurrent%20Neural%20Networks%20have%20become%20a%20competitive%20architecture%20for%0A3D%20reconstruction%20due%20to%20their%20linear-time%20complexity.%20However%2C%20their%0Aperformance%20degrades%20significantly%20when%20applied%20beyond%20the%20training%20context%0Alength%2C%20revealing%20limited%20length%20generalization.%20In%20this%20work%2C%20we%20revisit%20the%0A3D%20reconstruction%20foundation%20models%20from%20a%20Test-Time%20Training%20perspective%2C%0Aframing%20their%20designs%20as%20an%20online%20learning%20problem.%20Building%20on%20this%0Aperspective%2C%20we%20leverage%20the%20alignment%20confidence%20between%20the%20memory%20state%20and%0Aincoming%20observations%20to%20derive%20a%20closed-form%20learning%20rate%20for%20memory%20updates%2C%0Ato%20balance%20between%20retaining%20historical%20information%20and%20adapting%20to%20new%0Aobservations.%20This%20training-free%20intervention%2C%20termed%20TTT3R%2C%20substantially%0Aimproves%20length%20generalization%2C%20achieving%20a%20%242%5Ctimes%24%20improvement%20in%20global%0Apose%20estimation%20over%20baselines%2C%20while%20operating%20at%2020%20FPS%20with%20just%206%20GB%20of%20GPU%0Amemory%20to%20process%20thousands%20of%20images.%20Code%20available%20in%0Ahttps%3A//rover-xingyu.github.io/TTT3R%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26645v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTTT3R%253A%25203D%2520Reconstruction%2520as%2520Test-Time%2520Training%26entry.906535625%3DXingyu%2520Chen%2520and%2520Yue%2520Chen%2520and%2520Yuliang%2520Xiu%2520and%2520Andreas%2520Geiger%2520and%2520Anpei%2520Chen%26entry.1292438233%3D%2520%2520Modern%2520Recurrent%2520Neural%2520Networks%2520have%2520become%2520a%2520competitive%2520architecture%2520for%250A3D%2520reconstruction%2520due%2520to%2520their%2520linear-time%2520complexity.%2520However%252C%2520their%250Aperformance%2520degrades%2520significantly%2520when%2520applied%2520beyond%2520the%2520training%2520context%250Alength%252C%2520revealing%2520limited%2520length%2520generalization.%2520In%2520this%2520work%252C%2520we%2520revisit%2520the%250A3D%2520reconstruction%2520foundation%2520models%2520from%2520a%2520Test-Time%2520Training%2520perspective%252C%250Aframing%2520their%2520designs%2520as%2520an%2520online%2520learning%2520problem.%2520Building%2520on%2520this%250Aperspective%252C%2520we%2520leverage%2520the%2520alignment%2520confidence%2520between%2520the%2520memory%2520state%2520and%250Aincoming%2520observations%2520to%2520derive%2520a%2520closed-form%2520learning%2520rate%2520for%2520memory%2520updates%252C%250Ato%2520balance%2520between%2520retaining%2520historical%2520information%2520and%2520adapting%2520to%2520new%250Aobservations.%2520This%2520training-free%2520intervention%252C%2520termed%2520TTT3R%252C%2520substantially%250Aimproves%2520length%2520generalization%252C%2520achieving%2520a%2520%25242%255Ctimes%2524%2520improvement%2520in%2520global%250Apose%2520estimation%2520over%2520baselines%252C%2520while%2520operating%2520at%252020%2520FPS%2520with%2520just%25206%2520GB%2520of%2520GPU%250Amemory%2520to%2520process%2520thousands%2520of%2520images.%2520Code%2520available%2520in%250Ahttps%253A//rover-xingyu.github.io/TTT3R%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26645v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TTT3R%3A%203D%20Reconstruction%20as%20Test-Time%20Training&entry.906535625=Xingyu%20Chen%20and%20Yue%20Chen%20and%20Yuliang%20Xiu%20and%20Andreas%20Geiger%20and%20Anpei%20Chen&entry.1292438233=%20%20Modern%20Recurrent%20Neural%20Networks%20have%20become%20a%20competitive%20architecture%20for%0A3D%20reconstruction%20due%20to%20their%20linear-time%20complexity.%20However%2C%20their%0Aperformance%20degrades%20significantly%20when%20applied%20beyond%20the%20training%20context%0Alength%2C%20revealing%20limited%20length%20generalization.%20In%20this%20work%2C%20we%20revisit%20the%0A3D%20reconstruction%20foundation%20models%20from%20a%20Test-Time%20Training%20perspective%2C%0Aframing%20their%20designs%20as%20an%20online%20learning%20problem.%20Building%20on%20this%0Aperspective%2C%20we%20leverage%20the%20alignment%20confidence%20between%20the%20memory%20state%20and%0Aincoming%20observations%20to%20derive%20a%20closed-form%20learning%20rate%20for%20memory%20updates%2C%0Ato%20balance%20between%20retaining%20historical%20information%20and%20adapting%20to%20new%0Aobservations.%20This%20training-free%20intervention%2C%20termed%20TTT3R%2C%20substantially%0Aimproves%20length%20generalization%2C%20achieving%20a%20%242%5Ctimes%24%20improvement%20in%20global%0Apose%20estimation%20over%20baselines%2C%20while%20operating%20at%2020%20FPS%20with%20just%206%20GB%20of%20GPU%0Amemory%20to%20process%20thousands%20of%20images.%20Code%20available%20in%0Ahttps%3A//rover-xingyu.github.io/TTT3R%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26645v2&entry.124074799=Read"},
{"title": "Detect Anything via Next Point Prediction", "author": "Qing Jiang and Junan Huo and Xingyu Chen and Yuda Xiong and Zhaoyang Zeng and Yihao Chen and Tianhe Ren and Junzhi Yu and Lei Zhang", "abstract": "  Object detection has long been dominated by traditional coordinate\nregression-based models, such as YOLO, DETR, and Grounding DINO. Although\nrecent efforts have attempted to leverage MLLMs to tackle this task, they face\nchallenges like low recall rate, duplicate predictions, coordinate\nmisalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a\n3B-scale MLLM that achieves state-of-the-art object perception performance. On\nbenchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or\nexceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot\nsetting. This is enabled by three key designs: 1) Task Formulation: we use\nspecial tokens to represent quantized coordinates from 0 to 999, reducing the\nmodel's learning difficulty and improving token efficiency for coordinate\nprediction; 2) Data Engines: we construct multiple data engines to generate\nhigh-quality grounding, referring, and pointing data, providing semantically\nrich supervision for training; \\3) Training Pipelines: we employ a two-stage\ntraining process, combining supervised fine-tuning on 22 million data with\nGRPO-based reinforcement post-training. This RL post-training leverages\ngeometry-aware rewards to effectively bridge the discrete-to-continuous\ncoordinate prediction gap, improve box accuracy, and mitigate undesirable\nbehaviors like duplicate predictions that stem from the teacher-guided nature\nof the initial SFT stage. Beyond conventional detection, Rex-Omni's inherent\nlanguage understanding enables versatile capabilities such as object referring,\npointing, visual prompting, GUI grounding, spatial referring, OCR and\nkey-pointing, all systematically evaluated on dedicated benchmarks. We believe\nthat Rex-Omni paves the way for more versatile and language-aware visual\nperception systems.\n", "link": "http://arxiv.org/abs/2510.12798v1", "date": "2025-10-14", "relevancy": 2.9771, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6036}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6036}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detect%20Anything%20via%20Next%20Point%20Prediction&body=Title%3A%20Detect%20Anything%20via%20Next%20Point%20Prediction%0AAuthor%3A%20Qing%20Jiang%20and%20Junan%20Huo%20and%20Xingyu%20Chen%20and%20Yuda%20Xiong%20and%20Zhaoyang%20Zeng%20and%20Yihao%20Chen%20and%20Tianhe%20Ren%20and%20Junzhi%20Yu%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20Object%20detection%20has%20long%20been%20dominated%20by%20traditional%20coordinate%0Aregression-based%20models%2C%20such%20as%20YOLO%2C%20DETR%2C%20and%20Grounding%20DINO.%20Although%0Arecent%20efforts%20have%20attempted%20to%20leverage%20MLLMs%20to%20tackle%20this%20task%2C%20they%20face%0Achallenges%20like%20low%20recall%20rate%2C%20duplicate%20predictions%2C%20coordinate%0Amisalignment%2C%20etc.%20In%20this%20work%2C%20we%20bridge%20this%20gap%20and%20propose%20Rex-Omni%2C%20a%0A3B-scale%20MLLM%20that%20achieves%20state-of-the-art%20object%20perception%20performance.%20On%0Abenchmarks%20like%20COCO%20and%20LVIS%2C%20Rex-Omni%20attains%20performance%20comparable%20to%20or%0Aexceeding%20regression-based%20models%20%28e.g.%2C%20DINO%2C%20Grounding%20DINO%29%20in%20a%20zero-shot%0Asetting.%20This%20is%20enabled%20by%20three%20key%20designs%3A%201%29%20Task%20Formulation%3A%20we%20use%0Aspecial%20tokens%20to%20represent%20quantized%20coordinates%20from%200%20to%20999%2C%20reducing%20the%0Amodel%27s%20learning%20difficulty%20and%20improving%20token%20efficiency%20for%20coordinate%0Aprediction%3B%202%29%20Data%20Engines%3A%20we%20construct%20multiple%20data%20engines%20to%20generate%0Ahigh-quality%20grounding%2C%20referring%2C%20and%20pointing%20data%2C%20providing%20semantically%0Arich%20supervision%20for%20training%3B%20%5C3%29%20Training%20Pipelines%3A%20we%20employ%20a%20two-stage%0Atraining%20process%2C%20combining%20supervised%20fine-tuning%20on%2022%20million%20data%20with%0AGRPO-based%20reinforcement%20post-training.%20This%20RL%20post-training%20leverages%0Ageometry-aware%20rewards%20to%20effectively%20bridge%20the%20discrete-to-continuous%0Acoordinate%20prediction%20gap%2C%20improve%20box%20accuracy%2C%20and%20mitigate%20undesirable%0Abehaviors%20like%20duplicate%20predictions%20that%20stem%20from%20the%20teacher-guided%20nature%0Aof%20the%20initial%20SFT%20stage.%20Beyond%20conventional%20detection%2C%20Rex-Omni%27s%20inherent%0Alanguage%20understanding%20enables%20versatile%20capabilities%20such%20as%20object%20referring%2C%0Apointing%2C%20visual%20prompting%2C%20GUI%20grounding%2C%20spatial%20referring%2C%20OCR%20and%0Akey-pointing%2C%20all%20systematically%20evaluated%20on%20dedicated%20benchmarks.%20We%20believe%0Athat%20Rex-Omni%20paves%20the%20way%20for%20more%20versatile%20and%20language-aware%20visual%0Aperception%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetect%2520Anything%2520via%2520Next%2520Point%2520Prediction%26entry.906535625%3DQing%2520Jiang%2520and%2520Junan%2520Huo%2520and%2520Xingyu%2520Chen%2520and%2520Yuda%2520Xiong%2520and%2520Zhaoyang%2520Zeng%2520and%2520Yihao%2520Chen%2520and%2520Tianhe%2520Ren%2520and%2520Junzhi%2520Yu%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520Object%2520detection%2520has%2520long%2520been%2520dominated%2520by%2520traditional%2520coordinate%250Aregression-based%2520models%252C%2520such%2520as%2520YOLO%252C%2520DETR%252C%2520and%2520Grounding%2520DINO.%2520Although%250Arecent%2520efforts%2520have%2520attempted%2520to%2520leverage%2520MLLMs%2520to%2520tackle%2520this%2520task%252C%2520they%2520face%250Achallenges%2520like%2520low%2520recall%2520rate%252C%2520duplicate%2520predictions%252C%2520coordinate%250Amisalignment%252C%2520etc.%2520In%2520this%2520work%252C%2520we%2520bridge%2520this%2520gap%2520and%2520propose%2520Rex-Omni%252C%2520a%250A3B-scale%2520MLLM%2520that%2520achieves%2520state-of-the-art%2520object%2520perception%2520performance.%2520On%250Abenchmarks%2520like%2520COCO%2520and%2520LVIS%252C%2520Rex-Omni%2520attains%2520performance%2520comparable%2520to%2520or%250Aexceeding%2520regression-based%2520models%2520%2528e.g.%252C%2520DINO%252C%2520Grounding%2520DINO%2529%2520in%2520a%2520zero-shot%250Asetting.%2520This%2520is%2520enabled%2520by%2520three%2520key%2520designs%253A%25201%2529%2520Task%2520Formulation%253A%2520we%2520use%250Aspecial%2520tokens%2520to%2520represent%2520quantized%2520coordinates%2520from%25200%2520to%2520999%252C%2520reducing%2520the%250Amodel%2527s%2520learning%2520difficulty%2520and%2520improving%2520token%2520efficiency%2520for%2520coordinate%250Aprediction%253B%25202%2529%2520Data%2520Engines%253A%2520we%2520construct%2520multiple%2520data%2520engines%2520to%2520generate%250Ahigh-quality%2520grounding%252C%2520referring%252C%2520and%2520pointing%2520data%252C%2520providing%2520semantically%250Arich%2520supervision%2520for%2520training%253B%2520%255C3%2529%2520Training%2520Pipelines%253A%2520we%2520employ%2520a%2520two-stage%250Atraining%2520process%252C%2520combining%2520supervised%2520fine-tuning%2520on%252022%2520million%2520data%2520with%250AGRPO-based%2520reinforcement%2520post-training.%2520This%2520RL%2520post-training%2520leverages%250Ageometry-aware%2520rewards%2520to%2520effectively%2520bridge%2520the%2520discrete-to-continuous%250Acoordinate%2520prediction%2520gap%252C%2520improve%2520box%2520accuracy%252C%2520and%2520mitigate%2520undesirable%250Abehaviors%2520like%2520duplicate%2520predictions%2520that%2520stem%2520from%2520the%2520teacher-guided%2520nature%250Aof%2520the%2520initial%2520SFT%2520stage.%2520Beyond%2520conventional%2520detection%252C%2520Rex-Omni%2527s%2520inherent%250Alanguage%2520understanding%2520enables%2520versatile%2520capabilities%2520such%2520as%2520object%2520referring%252C%250Apointing%252C%2520visual%2520prompting%252C%2520GUI%2520grounding%252C%2520spatial%2520referring%252C%2520OCR%2520and%250Akey-pointing%252C%2520all%2520systematically%2520evaluated%2520on%2520dedicated%2520benchmarks.%2520We%2520believe%250Athat%2520Rex-Omni%2520paves%2520the%2520way%2520for%2520more%2520versatile%2520and%2520language-aware%2520visual%250Aperception%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detect%20Anything%20via%20Next%20Point%20Prediction&entry.906535625=Qing%20Jiang%20and%20Junan%20Huo%20and%20Xingyu%20Chen%20and%20Yuda%20Xiong%20and%20Zhaoyang%20Zeng%20and%20Yihao%20Chen%20and%20Tianhe%20Ren%20and%20Junzhi%20Yu%20and%20Lei%20Zhang&entry.1292438233=%20%20Object%20detection%20has%20long%20been%20dominated%20by%20traditional%20coordinate%0Aregression-based%20models%2C%20such%20as%20YOLO%2C%20DETR%2C%20and%20Grounding%20DINO.%20Although%0Arecent%20efforts%20have%20attempted%20to%20leverage%20MLLMs%20to%20tackle%20this%20task%2C%20they%20face%0Achallenges%20like%20low%20recall%20rate%2C%20duplicate%20predictions%2C%20coordinate%0Amisalignment%2C%20etc.%20In%20this%20work%2C%20we%20bridge%20this%20gap%20and%20propose%20Rex-Omni%2C%20a%0A3B-scale%20MLLM%20that%20achieves%20state-of-the-art%20object%20perception%20performance.%20On%0Abenchmarks%20like%20COCO%20and%20LVIS%2C%20Rex-Omni%20attains%20performance%20comparable%20to%20or%0Aexceeding%20regression-based%20models%20%28e.g.%2C%20DINO%2C%20Grounding%20DINO%29%20in%20a%20zero-shot%0Asetting.%20This%20is%20enabled%20by%20three%20key%20designs%3A%201%29%20Task%20Formulation%3A%20we%20use%0Aspecial%20tokens%20to%20represent%20quantized%20coordinates%20from%200%20to%20999%2C%20reducing%20the%0Amodel%27s%20learning%20difficulty%20and%20improving%20token%20efficiency%20for%20coordinate%0Aprediction%3B%202%29%20Data%20Engines%3A%20we%20construct%20multiple%20data%20engines%20to%20generate%0Ahigh-quality%20grounding%2C%20referring%2C%20and%20pointing%20data%2C%20providing%20semantically%0Arich%20supervision%20for%20training%3B%20%5C3%29%20Training%20Pipelines%3A%20we%20employ%20a%20two-stage%0Atraining%20process%2C%20combining%20supervised%20fine-tuning%20on%2022%20million%20data%20with%0AGRPO-based%20reinforcement%20post-training.%20This%20RL%20post-training%20leverages%0Ageometry-aware%20rewards%20to%20effectively%20bridge%20the%20discrete-to-continuous%0Acoordinate%20prediction%20gap%2C%20improve%20box%20accuracy%2C%20and%20mitigate%20undesirable%0Abehaviors%20like%20duplicate%20predictions%20that%20stem%20from%20the%20teacher-guided%20nature%0Aof%20the%20initial%20SFT%20stage.%20Beyond%20conventional%20detection%2C%20Rex-Omni%27s%20inherent%0Alanguage%20understanding%20enables%20versatile%20capabilities%20such%20as%20object%20referring%2C%0Apointing%2C%20visual%20prompting%2C%20GUI%20grounding%2C%20spatial%20referring%2C%20OCR%20and%0Akey-pointing%2C%20all%20systematically%20evaluated%20on%20dedicated%20benchmarks.%20We%20believe%0Athat%20Rex-Omni%20paves%20the%20way%20for%20more%20versatile%20and%20language-aware%20visual%0Aperception%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12798v1&entry.124074799=Read"},
{"title": "Modular Embedding Recomposition for Incremental Learning", "author": "Aniello Panariello and Emanuele Frascaroli and Pietro Buzzega and Lorenzo Bonicelli and Angelo Porrello and Simone Calderara", "abstract": "  The advent of pre-trained Vision-Language Models (VLMs) has significantly\ntransformed Continual Learning (CL), mainly due to their zero-shot\nclassification abilities. Such proficiency makes VLMs well-suited for\nreal-world applications, enabling robust performance on novel unseen classes\nwithout requiring adaptation. However, fine-tuning remains essential when\ndownstream tasks deviate significantly from the pre-training domain. Prior CL\napproaches primarily focus on preserving the zero-shot capabilities of VLMs\nduring incremental fine-tuning on a downstream task. We take a step further by\ndevising an approach that transforms preservation into enhancement of the\nzero-shot capabilities of VLMs. Our approach, named MoDular Embedding\nRecomposition (MoDER), introduces a modular framework that trains multiple\ntextual experts, each specialized in a single seen class, and stores them in a\nfoundational hub. At inference time, for each unseen class, we query the hub\nand compose the retrieved experts to synthesize a refined prototype that\nimproves classification. We show the effectiveness of our method across two\npopular zero-shot incremental protocols, Class-IL and MTIL, comprising a total\nof 14 datasets. The codebase is available at\nhttps://github.com/aimagelab/mammoth.\n", "link": "http://arxiv.org/abs/2508.16463v2", "date": "2025-10-14", "relevancy": 2.9426, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6198}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5729}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modular%20Embedding%20Recomposition%20for%20Incremental%20Learning&body=Title%3A%20Modular%20Embedding%20Recomposition%20for%20Incremental%20Learning%0AAuthor%3A%20Aniello%20Panariello%20and%20Emanuele%20Frascaroli%20and%20Pietro%20Buzzega%20and%20Lorenzo%20Bonicelli%20and%20Angelo%20Porrello%20and%20Simone%20Calderara%0AAbstract%3A%20%20%20The%20advent%20of%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20has%20significantly%0Atransformed%20Continual%20Learning%20%28CL%29%2C%20mainly%20due%20to%20their%20zero-shot%0Aclassification%20abilities.%20Such%20proficiency%20makes%20VLMs%20well-suited%20for%0Areal-world%20applications%2C%20enabling%20robust%20performance%20on%20novel%20unseen%20classes%0Awithout%20requiring%20adaptation.%20However%2C%20fine-tuning%20remains%20essential%20when%0Adownstream%20tasks%20deviate%20significantly%20from%20the%20pre-training%20domain.%20Prior%20CL%0Aapproaches%20primarily%20focus%20on%20preserving%20the%20zero-shot%20capabilities%20of%20VLMs%0Aduring%20incremental%20fine-tuning%20on%20a%20downstream%20task.%20We%20take%20a%20step%20further%20by%0Adevising%20an%20approach%20that%20transforms%20preservation%20into%20enhancement%20of%20the%0Azero-shot%20capabilities%20of%20VLMs.%20Our%20approach%2C%20named%20MoDular%20Embedding%0ARecomposition%20%28MoDER%29%2C%20introduces%20a%20modular%20framework%20that%20trains%20multiple%0Atextual%20experts%2C%20each%20specialized%20in%20a%20single%20seen%20class%2C%20and%20stores%20them%20in%20a%0Afoundational%20hub.%20At%20inference%20time%2C%20for%20each%20unseen%20class%2C%20we%20query%20the%20hub%0Aand%20compose%20the%20retrieved%20experts%20to%20synthesize%20a%20refined%20prototype%20that%0Aimproves%20classification.%20We%20show%20the%20effectiveness%20of%20our%20method%20across%20two%0Apopular%20zero-shot%20incremental%20protocols%2C%20Class-IL%20and%20MTIL%2C%20comprising%20a%20total%0Aof%2014%20datasets.%20The%20codebase%20is%20available%20at%0Ahttps%3A//github.com/aimagelab/mammoth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16463v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModular%2520Embedding%2520Recomposition%2520for%2520Incremental%2520Learning%26entry.906535625%3DAniello%2520Panariello%2520and%2520Emanuele%2520Frascaroli%2520and%2520Pietro%2520Buzzega%2520and%2520Lorenzo%2520Bonicelli%2520and%2520Angelo%2520Porrello%2520and%2520Simone%2520Calderara%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520pre-trained%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520has%2520significantly%250Atransformed%2520Continual%2520Learning%2520%2528CL%2529%252C%2520mainly%2520due%2520to%2520their%2520zero-shot%250Aclassification%2520abilities.%2520Such%2520proficiency%2520makes%2520VLMs%2520well-suited%2520for%250Areal-world%2520applications%252C%2520enabling%2520robust%2520performance%2520on%2520novel%2520unseen%2520classes%250Awithout%2520requiring%2520adaptation.%2520However%252C%2520fine-tuning%2520remains%2520essential%2520when%250Adownstream%2520tasks%2520deviate%2520significantly%2520from%2520the%2520pre-training%2520domain.%2520Prior%2520CL%250Aapproaches%2520primarily%2520focus%2520on%2520preserving%2520the%2520zero-shot%2520capabilities%2520of%2520VLMs%250Aduring%2520incremental%2520fine-tuning%2520on%2520a%2520downstream%2520task.%2520We%2520take%2520a%2520step%2520further%2520by%250Adevising%2520an%2520approach%2520that%2520transforms%2520preservation%2520into%2520enhancement%2520of%2520the%250Azero-shot%2520capabilities%2520of%2520VLMs.%2520Our%2520approach%252C%2520named%2520MoDular%2520Embedding%250ARecomposition%2520%2528MoDER%2529%252C%2520introduces%2520a%2520modular%2520framework%2520that%2520trains%2520multiple%250Atextual%2520experts%252C%2520each%2520specialized%2520in%2520a%2520single%2520seen%2520class%252C%2520and%2520stores%2520them%2520in%2520a%250Afoundational%2520hub.%2520At%2520inference%2520time%252C%2520for%2520each%2520unseen%2520class%252C%2520we%2520query%2520the%2520hub%250Aand%2520compose%2520the%2520retrieved%2520experts%2520to%2520synthesize%2520a%2520refined%2520prototype%2520that%250Aimproves%2520classification.%2520We%2520show%2520the%2520effectiveness%2520of%2520our%2520method%2520across%2520two%250Apopular%2520zero-shot%2520incremental%2520protocols%252C%2520Class-IL%2520and%2520MTIL%252C%2520comprising%2520a%2520total%250Aof%252014%2520datasets.%2520The%2520codebase%2520is%2520available%2520at%250Ahttps%253A//github.com/aimagelab/mammoth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16463v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modular%20Embedding%20Recomposition%20for%20Incremental%20Learning&entry.906535625=Aniello%20Panariello%20and%20Emanuele%20Frascaroli%20and%20Pietro%20Buzzega%20and%20Lorenzo%20Bonicelli%20and%20Angelo%20Porrello%20and%20Simone%20Calderara&entry.1292438233=%20%20The%20advent%20of%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20has%20significantly%0Atransformed%20Continual%20Learning%20%28CL%29%2C%20mainly%20due%20to%20their%20zero-shot%0Aclassification%20abilities.%20Such%20proficiency%20makes%20VLMs%20well-suited%20for%0Areal-world%20applications%2C%20enabling%20robust%20performance%20on%20novel%20unseen%20classes%0Awithout%20requiring%20adaptation.%20However%2C%20fine-tuning%20remains%20essential%20when%0Adownstream%20tasks%20deviate%20significantly%20from%20the%20pre-training%20domain.%20Prior%20CL%0Aapproaches%20primarily%20focus%20on%20preserving%20the%20zero-shot%20capabilities%20of%20VLMs%0Aduring%20incremental%20fine-tuning%20on%20a%20downstream%20task.%20We%20take%20a%20step%20further%20by%0Adevising%20an%20approach%20that%20transforms%20preservation%20into%20enhancement%20of%20the%0Azero-shot%20capabilities%20of%20VLMs.%20Our%20approach%2C%20named%20MoDular%20Embedding%0ARecomposition%20%28MoDER%29%2C%20introduces%20a%20modular%20framework%20that%20trains%20multiple%0Atextual%20experts%2C%20each%20specialized%20in%20a%20single%20seen%20class%2C%20and%20stores%20them%20in%20a%0Afoundational%20hub.%20At%20inference%20time%2C%20for%20each%20unseen%20class%2C%20we%20query%20the%20hub%0Aand%20compose%20the%20retrieved%20experts%20to%20synthesize%20a%20refined%20prototype%20that%0Aimproves%20classification.%20We%20show%20the%20effectiveness%20of%20our%20method%20across%20two%0Apopular%20zero-shot%20incremental%20protocols%2C%20Class-IL%20and%20MTIL%2C%20comprising%20a%20total%0Aof%2014%20datasets.%20The%20codebase%20is%20available%20at%0Ahttps%3A//github.com/aimagelab/mammoth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16463v2&entry.124074799=Read"},
{"title": "VISaGE: Understanding Visual Generics and Exceptions", "author": "Stella Frank and Emily Allaway", "abstract": "  While Vision Language Models (VLMs) learn conceptual representations, in the\nform of generalized knowledge, during training, they are typically used to\nanalyze individual instances. When evaluation instances are atypical, this\nparadigm results in tension between two priors in the model. The first is a\npragmatic prior that the textual and visual input are both relevant, arising\nfrom VLM finetuning on congruent inputs; the second is a semantic prior that\nthe conceptual representation is generally true for instances of the category.\nIn order to understand how VLMs trade off these priors, we introduce a new\nevaluation dataset, VISaGE, consisting of both typical and exceptional images.\nIn carefully balanced experiments, we show that conceptual understanding\ndegrades when the assumption of congruency underlying the pragmatic prior is\nviolated with incongruent images. This effect is stronger than the effect of\nthe semantic prior when querying about individual instances.\n", "link": "http://arxiv.org/abs/2510.12548v1", "date": "2025-10-14", "relevancy": 2.9303, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5911}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5911}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VISaGE%3A%20Understanding%20Visual%20Generics%20and%20Exceptions&body=Title%3A%20VISaGE%3A%20Understanding%20Visual%20Generics%20and%20Exceptions%0AAuthor%3A%20Stella%20Frank%20and%20Emily%20Allaway%0AAbstract%3A%20%20%20While%20Vision%20Language%20Models%20%28VLMs%29%20learn%20conceptual%20representations%2C%20in%20the%0Aform%20of%20generalized%20knowledge%2C%20during%20training%2C%20they%20are%20typically%20used%20to%0Aanalyze%20individual%20instances.%20When%20evaluation%20instances%20are%20atypical%2C%20this%0Aparadigm%20results%20in%20tension%20between%20two%20priors%20in%20the%20model.%20The%20first%20is%20a%0Apragmatic%20prior%20that%20the%20textual%20and%20visual%20input%20are%20both%20relevant%2C%20arising%0Afrom%20VLM%20finetuning%20on%20congruent%20inputs%3B%20the%20second%20is%20a%20semantic%20prior%20that%0Athe%20conceptual%20representation%20is%20generally%20true%20for%20instances%20of%20the%20category.%0AIn%20order%20to%20understand%20how%20VLMs%20trade%20off%20these%20priors%2C%20we%20introduce%20a%20new%0Aevaluation%20dataset%2C%20VISaGE%2C%20consisting%20of%20both%20typical%20and%20exceptional%20images.%0AIn%20carefully%20balanced%20experiments%2C%20we%20show%20that%20conceptual%20understanding%0Adegrades%20when%20the%20assumption%20of%20congruency%20underlying%20the%20pragmatic%20prior%20is%0Aviolated%20with%20incongruent%20images.%20This%20effect%20is%20stronger%20than%20the%20effect%20of%0Athe%20semantic%20prior%20when%20querying%20about%20individual%20instances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVISaGE%253A%2520Understanding%2520Visual%2520Generics%2520and%2520Exceptions%26entry.906535625%3DStella%2520Frank%2520and%2520Emily%2520Allaway%26entry.1292438233%3D%2520%2520While%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520learn%2520conceptual%2520representations%252C%2520in%2520the%250Aform%2520of%2520generalized%2520knowledge%252C%2520during%2520training%252C%2520they%2520are%2520typically%2520used%2520to%250Aanalyze%2520individual%2520instances.%2520When%2520evaluation%2520instances%2520are%2520atypical%252C%2520this%250Aparadigm%2520results%2520in%2520tension%2520between%2520two%2520priors%2520in%2520the%2520model.%2520The%2520first%2520is%2520a%250Apragmatic%2520prior%2520that%2520the%2520textual%2520and%2520visual%2520input%2520are%2520both%2520relevant%252C%2520arising%250Afrom%2520VLM%2520finetuning%2520on%2520congruent%2520inputs%253B%2520the%2520second%2520is%2520a%2520semantic%2520prior%2520that%250Athe%2520conceptual%2520representation%2520is%2520generally%2520true%2520for%2520instances%2520of%2520the%2520category.%250AIn%2520order%2520to%2520understand%2520how%2520VLMs%2520trade%2520off%2520these%2520priors%252C%2520we%2520introduce%2520a%2520new%250Aevaluation%2520dataset%252C%2520VISaGE%252C%2520consisting%2520of%2520both%2520typical%2520and%2520exceptional%2520images.%250AIn%2520carefully%2520balanced%2520experiments%252C%2520we%2520show%2520that%2520conceptual%2520understanding%250Adegrades%2520when%2520the%2520assumption%2520of%2520congruency%2520underlying%2520the%2520pragmatic%2520prior%2520is%250Aviolated%2520with%2520incongruent%2520images.%2520This%2520effect%2520is%2520stronger%2520than%2520the%2520effect%2520of%250Athe%2520semantic%2520prior%2520when%2520querying%2520about%2520individual%2520instances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VISaGE%3A%20Understanding%20Visual%20Generics%20and%20Exceptions&entry.906535625=Stella%20Frank%20and%20Emily%20Allaway&entry.1292438233=%20%20While%20Vision%20Language%20Models%20%28VLMs%29%20learn%20conceptual%20representations%2C%20in%20the%0Aform%20of%20generalized%20knowledge%2C%20during%20training%2C%20they%20are%20typically%20used%20to%0Aanalyze%20individual%20instances.%20When%20evaluation%20instances%20are%20atypical%2C%20this%0Aparadigm%20results%20in%20tension%20between%20two%20priors%20in%20the%20model.%20The%20first%20is%20a%0Apragmatic%20prior%20that%20the%20textual%20and%20visual%20input%20are%20both%20relevant%2C%20arising%0Afrom%20VLM%20finetuning%20on%20congruent%20inputs%3B%20the%20second%20is%20a%20semantic%20prior%20that%0Athe%20conceptual%20representation%20is%20generally%20true%20for%20instances%20of%20the%20category.%0AIn%20order%20to%20understand%20how%20VLMs%20trade%20off%20these%20priors%2C%20we%20introduce%20a%20new%0Aevaluation%20dataset%2C%20VISaGE%2C%20consisting%20of%20both%20typical%20and%20exceptional%20images.%0AIn%20carefully%20balanced%20experiments%2C%20we%20show%20that%20conceptual%20understanding%0Adegrades%20when%20the%20assumption%20of%20congruency%20underlying%20the%20pragmatic%20prior%20is%0Aviolated%20with%20incongruent%20images.%20This%20effect%20is%20stronger%20than%20the%20effect%20of%0Athe%20semantic%20prior%20when%20querying%20about%20individual%20instances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12548v1&entry.124074799=Read"},
{"title": "On the Use of Hierarchical Vision Foundation Models for Low-Cost Human\n  Mesh Recovery and Pose Estimation", "author": "Shuhei Tarashima and Yushan Wang and Norio Tagawa", "abstract": "  In this work, we aim to develop simple and efficient models for human mesh\nrecovery (HMR) and its predecessor task, human pose estimation (HPE).\nState-of-the-art HMR methods, such as HMR2.0 and its successors, rely on large,\nnon-hierarchical vision transformers as encoders, which are inherited from the\ncorresponding HPE models like ViTPose. To establish baselines across varying\ncomputational budgets, we first construct three lightweight HMR2.0 variants by\nadapting the corresponding ViTPose models. In addition, we propose leveraging\nthe early stages of hierarchical vision foundation models (VFMs), including\nSwin Transformer, GroupMixFormer, and VMamba, as encoders. This design is\nmotivated by the observation that intermediate stages of hierarchical VFMs\nproduce feature maps with resolutions comparable to or higher than those of\nnon-hierarchical counterparts. We conduct a comprehensive evaluation of 27\nhierarchical-VFM-based HMR and HPE models, demonstrating that using only the\nfirst two or three stages achieves performance on par with full-stage models.\nMoreover, we show that the resulting truncated models exhibit better trade-offs\nbetween accuracy and computational efficiency compared to existing lightweight\nalternatives.\n", "link": "http://arxiv.org/abs/2510.12660v1", "date": "2025-10-14", "relevancy": 2.9102, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6022}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5793}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Use%20of%20Hierarchical%20Vision%20Foundation%20Models%20for%20Low-Cost%20Human%0A%20%20Mesh%20Recovery%20and%20Pose%20Estimation&body=Title%3A%20On%20the%20Use%20of%20Hierarchical%20Vision%20Foundation%20Models%20for%20Low-Cost%20Human%0A%20%20Mesh%20Recovery%20and%20Pose%20Estimation%0AAuthor%3A%20Shuhei%20Tarashima%20and%20Yushan%20Wang%20and%20Norio%20Tagawa%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20aim%20to%20develop%20simple%20and%20efficient%20models%20for%20human%20mesh%0Arecovery%20%28HMR%29%20and%20its%20predecessor%20task%2C%20human%20pose%20estimation%20%28HPE%29.%0AState-of-the-art%20HMR%20methods%2C%20such%20as%20HMR2.0%20and%20its%20successors%2C%20rely%20on%20large%2C%0Anon-hierarchical%20vision%20transformers%20as%20encoders%2C%20which%20are%20inherited%20from%20the%0Acorresponding%20HPE%20models%20like%20ViTPose.%20To%20establish%20baselines%20across%20varying%0Acomputational%20budgets%2C%20we%20first%20construct%20three%20lightweight%20HMR2.0%20variants%20by%0Aadapting%20the%20corresponding%20ViTPose%20models.%20In%20addition%2C%20we%20propose%20leveraging%0Athe%20early%20stages%20of%20hierarchical%20vision%20foundation%20models%20%28VFMs%29%2C%20including%0ASwin%20Transformer%2C%20GroupMixFormer%2C%20and%20VMamba%2C%20as%20encoders.%20This%20design%20is%0Amotivated%20by%20the%20observation%20that%20intermediate%20stages%20of%20hierarchical%20VFMs%0Aproduce%20feature%20maps%20with%20resolutions%20comparable%20to%20or%20higher%20than%20those%20of%0Anon-hierarchical%20counterparts.%20We%20conduct%20a%20comprehensive%20evaluation%20of%2027%0Ahierarchical-VFM-based%20HMR%20and%20HPE%20models%2C%20demonstrating%20that%20using%20only%20the%0Afirst%20two%20or%20three%20stages%20achieves%20performance%20on%20par%20with%20full-stage%20models.%0AMoreover%2C%20we%20show%20that%20the%20resulting%20truncated%20models%20exhibit%20better%20trade-offs%0Abetween%20accuracy%20and%20computational%20efficiency%20compared%20to%20existing%20lightweight%0Aalternatives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Use%2520of%2520Hierarchical%2520Vision%2520Foundation%2520Models%2520for%2520Low-Cost%2520Human%250A%2520%2520Mesh%2520Recovery%2520and%2520Pose%2520Estimation%26entry.906535625%3DShuhei%2520Tarashima%2520and%2520Yushan%2520Wang%2520and%2520Norio%2520Tagawa%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520develop%2520simple%2520and%2520efficient%2520models%2520for%2520human%2520mesh%250Arecovery%2520%2528HMR%2529%2520and%2520its%2520predecessor%2520task%252C%2520human%2520pose%2520estimation%2520%2528HPE%2529.%250AState-of-the-art%2520HMR%2520methods%252C%2520such%2520as%2520HMR2.0%2520and%2520its%2520successors%252C%2520rely%2520on%2520large%252C%250Anon-hierarchical%2520vision%2520transformers%2520as%2520encoders%252C%2520which%2520are%2520inherited%2520from%2520the%250Acorresponding%2520HPE%2520models%2520like%2520ViTPose.%2520To%2520establish%2520baselines%2520across%2520varying%250Acomputational%2520budgets%252C%2520we%2520first%2520construct%2520three%2520lightweight%2520HMR2.0%2520variants%2520by%250Aadapting%2520the%2520corresponding%2520ViTPose%2520models.%2520In%2520addition%252C%2520we%2520propose%2520leveraging%250Athe%2520early%2520stages%2520of%2520hierarchical%2520vision%2520foundation%2520models%2520%2528VFMs%2529%252C%2520including%250ASwin%2520Transformer%252C%2520GroupMixFormer%252C%2520and%2520VMamba%252C%2520as%2520encoders.%2520This%2520design%2520is%250Amotivated%2520by%2520the%2520observation%2520that%2520intermediate%2520stages%2520of%2520hierarchical%2520VFMs%250Aproduce%2520feature%2520maps%2520with%2520resolutions%2520comparable%2520to%2520or%2520higher%2520than%2520those%2520of%250Anon-hierarchical%2520counterparts.%2520We%2520conduct%2520a%2520comprehensive%2520evaluation%2520of%252027%250Ahierarchical-VFM-based%2520HMR%2520and%2520HPE%2520models%252C%2520demonstrating%2520that%2520using%2520only%2520the%250Afirst%2520two%2520or%2520three%2520stages%2520achieves%2520performance%2520on%2520par%2520with%2520full-stage%2520models.%250AMoreover%252C%2520we%2520show%2520that%2520the%2520resulting%2520truncated%2520models%2520exhibit%2520better%2520trade-offs%250Abetween%2520accuracy%2520and%2520computational%2520efficiency%2520compared%2520to%2520existing%2520lightweight%250Aalternatives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Use%20of%20Hierarchical%20Vision%20Foundation%20Models%20for%20Low-Cost%20Human%0A%20%20Mesh%20Recovery%20and%20Pose%20Estimation&entry.906535625=Shuhei%20Tarashima%20and%20Yushan%20Wang%20and%20Norio%20Tagawa&entry.1292438233=%20%20In%20this%20work%2C%20we%20aim%20to%20develop%20simple%20and%20efficient%20models%20for%20human%20mesh%0Arecovery%20%28HMR%29%20and%20its%20predecessor%20task%2C%20human%20pose%20estimation%20%28HPE%29.%0AState-of-the-art%20HMR%20methods%2C%20such%20as%20HMR2.0%20and%20its%20successors%2C%20rely%20on%20large%2C%0Anon-hierarchical%20vision%20transformers%20as%20encoders%2C%20which%20are%20inherited%20from%20the%0Acorresponding%20HPE%20models%20like%20ViTPose.%20To%20establish%20baselines%20across%20varying%0Acomputational%20budgets%2C%20we%20first%20construct%20three%20lightweight%20HMR2.0%20variants%20by%0Aadapting%20the%20corresponding%20ViTPose%20models.%20In%20addition%2C%20we%20propose%20leveraging%0Athe%20early%20stages%20of%20hierarchical%20vision%20foundation%20models%20%28VFMs%29%2C%20including%0ASwin%20Transformer%2C%20GroupMixFormer%2C%20and%20VMamba%2C%20as%20encoders.%20This%20design%20is%0Amotivated%20by%20the%20observation%20that%20intermediate%20stages%20of%20hierarchical%20VFMs%0Aproduce%20feature%20maps%20with%20resolutions%20comparable%20to%20or%20higher%20than%20those%20of%0Anon-hierarchical%20counterparts.%20We%20conduct%20a%20comprehensive%20evaluation%20of%2027%0Ahierarchical-VFM-based%20HMR%20and%20HPE%20models%2C%20demonstrating%20that%20using%20only%20the%0Afirst%20two%20or%20three%20stages%20achieves%20performance%20on%20par%20with%20full-stage%20models.%0AMoreover%2C%20we%20show%20that%20the%20resulting%20truncated%20models%20exhibit%20better%20trade-offs%0Abetween%20accuracy%20and%20computational%20efficiency%20compared%20to%20existing%20lightweight%0Aalternatives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12660v1&entry.124074799=Read"},
{"title": "Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image\n  Perception, Transformation, and Reasoning", "author": "Xingang Guo and Utkarsh Tyagi and Advait Gosai and Paula Vergara and Ernesto Gabriel Hern\u00e1ndez Montoya and Chen Bo Calvin Zhang and Bin Hu and Yunzhong He and Bing Liu and Rakshith Sharma Srinivasa", "abstract": "  Multimodal Large Language Models (MLLMs) are increasingly applied in\nreal-world scenarios where user-provided images are often imperfect, requiring\nactive image manipulations such as cropping, editing, or enhancement to uncover\nsalient visual cues. Beyond static visual perception, MLLMs must also think\nwith images: dynamically transforming visual content and integrating it with\nother tools to solve complex tasks. However, this shift from treating vision as\npassive context to a manipulable cognitive workspace remains underexplored.\nMost existing benchmarks still follow a think about images paradigm, where\nimages are regarded as static inputs. To address this gap, we introduce IRIS,\nan Interactive Reasoning with Images and Systems that evaluates MLLMs' ability\nto perceive, transform, and reason across complex visual-textual tasks under\nthe think with images paradigm. IRIS comprises 1,204 challenging, open-ended\nvision tasks (603 single-turn, 601 multi-turn) spanning across five diverse\ndomains, each paired with detailed rubrics to enable systematic evaluation. Our\nevaluation shows that current MLLMs struggle with tasks requiring effective\nintegration of vision and general-purpose tools. Even the strongest model\n(GPT-5-think) reaches only 18.68% pass rate. We further observe divergent\ntool-use behaviors, with OpenAI models benefiting from diverse image\nmanipulations while Gemini-2.5-pro shows no improvement. By introducing the\nfirst benchmark centered on think with images, IRIS offers critical insights\nfor advancing visual intelligence in MLLMs.\n", "link": "http://arxiv.org/abs/2510.12712v1", "date": "2025-10-14", "relevancy": 2.9002, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5857}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5857}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Seeing%3A%20Evaluating%20Multimodal%20LLMs%20on%20Tool-Enabled%20Image%0A%20%20Perception%2C%20Transformation%2C%20and%20Reasoning&body=Title%3A%20Beyond%20Seeing%3A%20Evaluating%20Multimodal%20LLMs%20on%20Tool-Enabled%20Image%0A%20%20Perception%2C%20Transformation%2C%20and%20Reasoning%0AAuthor%3A%20Xingang%20Guo%20and%20Utkarsh%20Tyagi%20and%20Advait%20Gosai%20and%20Paula%20Vergara%20and%20Ernesto%20Gabriel%20Hern%C3%A1ndez%20Montoya%20and%20Chen%20Bo%20Calvin%20Zhang%20and%20Bin%20Hu%20and%20Yunzhong%20He%20and%20Bing%20Liu%20and%20Rakshith%20Sharma%20Srinivasa%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20increasingly%20applied%20in%0Areal-world%20scenarios%20where%20user-provided%20images%20are%20often%20imperfect%2C%20requiring%0Aactive%20image%20manipulations%20such%20as%20cropping%2C%20editing%2C%20or%20enhancement%20to%20uncover%0Asalient%20visual%20cues.%20Beyond%20static%20visual%20perception%2C%20MLLMs%20must%20also%20think%0Awith%20images%3A%20dynamically%20transforming%20visual%20content%20and%20integrating%20it%20with%0Aother%20tools%20to%20solve%20complex%20tasks.%20However%2C%20this%20shift%20from%20treating%20vision%20as%0Apassive%20context%20to%20a%20manipulable%20cognitive%20workspace%20remains%20underexplored.%0AMost%20existing%20benchmarks%20still%20follow%20a%20think%20about%20images%20paradigm%2C%20where%0Aimages%20are%20regarded%20as%20static%20inputs.%20To%20address%20this%20gap%2C%20we%20introduce%20IRIS%2C%0Aan%20Interactive%20Reasoning%20with%20Images%20and%20Systems%20that%20evaluates%20MLLMs%27%20ability%0Ato%20perceive%2C%20transform%2C%20and%20reason%20across%20complex%20visual-textual%20tasks%20under%0Athe%20think%20with%20images%20paradigm.%20IRIS%20comprises%201%2C204%20challenging%2C%20open-ended%0Avision%20tasks%20%28603%20single-turn%2C%20601%20multi-turn%29%20spanning%20across%20five%20diverse%0Adomains%2C%20each%20paired%20with%20detailed%20rubrics%20to%20enable%20systematic%20evaluation.%20Our%0Aevaluation%20shows%20that%20current%20MLLMs%20struggle%20with%20tasks%20requiring%20effective%0Aintegration%20of%20vision%20and%20general-purpose%20tools.%20Even%20the%20strongest%20model%0A%28GPT-5-think%29%20reaches%20only%2018.68%25%20pass%20rate.%20We%20further%20observe%20divergent%0Atool-use%20behaviors%2C%20with%20OpenAI%20models%20benefiting%20from%20diverse%20image%0Amanipulations%20while%20Gemini-2.5-pro%20shows%20no%20improvement.%20By%20introducing%20the%0Afirst%20benchmark%20centered%20on%20think%20with%20images%2C%20IRIS%20offers%20critical%20insights%0Afor%20advancing%20visual%20intelligence%20in%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Seeing%253A%2520Evaluating%2520Multimodal%2520LLMs%2520on%2520Tool-Enabled%2520Image%250A%2520%2520Perception%252C%2520Transformation%252C%2520and%2520Reasoning%26entry.906535625%3DXingang%2520Guo%2520and%2520Utkarsh%2520Tyagi%2520and%2520Advait%2520Gosai%2520and%2520Paula%2520Vergara%2520and%2520Ernesto%2520Gabriel%2520Hern%25C3%25A1ndez%2520Montoya%2520and%2520Chen%2520Bo%2520Calvin%2520Zhang%2520and%2520Bin%2520Hu%2520and%2520Yunzhong%2520He%2520and%2520Bing%2520Liu%2520and%2520Rakshith%2520Sharma%2520Srinivasa%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520are%2520increasingly%2520applied%2520in%250Areal-world%2520scenarios%2520where%2520user-provided%2520images%2520are%2520often%2520imperfect%252C%2520requiring%250Aactive%2520image%2520manipulations%2520such%2520as%2520cropping%252C%2520editing%252C%2520or%2520enhancement%2520to%2520uncover%250Asalient%2520visual%2520cues.%2520Beyond%2520static%2520visual%2520perception%252C%2520MLLMs%2520must%2520also%2520think%250Awith%2520images%253A%2520dynamically%2520transforming%2520visual%2520content%2520and%2520integrating%2520it%2520with%250Aother%2520tools%2520to%2520solve%2520complex%2520tasks.%2520However%252C%2520this%2520shift%2520from%2520treating%2520vision%2520as%250Apassive%2520context%2520to%2520a%2520manipulable%2520cognitive%2520workspace%2520remains%2520underexplored.%250AMost%2520existing%2520benchmarks%2520still%2520follow%2520a%2520think%2520about%2520images%2520paradigm%252C%2520where%250Aimages%2520are%2520regarded%2520as%2520static%2520inputs.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520IRIS%252C%250Aan%2520Interactive%2520Reasoning%2520with%2520Images%2520and%2520Systems%2520that%2520evaluates%2520MLLMs%2527%2520ability%250Ato%2520perceive%252C%2520transform%252C%2520and%2520reason%2520across%2520complex%2520visual-textual%2520tasks%2520under%250Athe%2520think%2520with%2520images%2520paradigm.%2520IRIS%2520comprises%25201%252C204%2520challenging%252C%2520open-ended%250Avision%2520tasks%2520%2528603%2520single-turn%252C%2520601%2520multi-turn%2529%2520spanning%2520across%2520five%2520diverse%250Adomains%252C%2520each%2520paired%2520with%2520detailed%2520rubrics%2520to%2520enable%2520systematic%2520evaluation.%2520Our%250Aevaluation%2520shows%2520that%2520current%2520MLLMs%2520struggle%2520with%2520tasks%2520requiring%2520effective%250Aintegration%2520of%2520vision%2520and%2520general-purpose%2520tools.%2520Even%2520the%2520strongest%2520model%250A%2528GPT-5-think%2529%2520reaches%2520only%252018.68%2525%2520pass%2520rate.%2520We%2520further%2520observe%2520divergent%250Atool-use%2520behaviors%252C%2520with%2520OpenAI%2520models%2520benefiting%2520from%2520diverse%2520image%250Amanipulations%2520while%2520Gemini-2.5-pro%2520shows%2520no%2520improvement.%2520By%2520introducing%2520the%250Afirst%2520benchmark%2520centered%2520on%2520think%2520with%2520images%252C%2520IRIS%2520offers%2520critical%2520insights%250Afor%2520advancing%2520visual%2520intelligence%2520in%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Seeing%3A%20Evaluating%20Multimodal%20LLMs%20on%20Tool-Enabled%20Image%0A%20%20Perception%2C%20Transformation%2C%20and%20Reasoning&entry.906535625=Xingang%20Guo%20and%20Utkarsh%20Tyagi%20and%20Advait%20Gosai%20and%20Paula%20Vergara%20and%20Ernesto%20Gabriel%20Hern%C3%A1ndez%20Montoya%20and%20Chen%20Bo%20Calvin%20Zhang%20and%20Bin%20Hu%20and%20Yunzhong%20He%20and%20Bing%20Liu%20and%20Rakshith%20Sharma%20Srinivasa&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20increasingly%20applied%20in%0Areal-world%20scenarios%20where%20user-provided%20images%20are%20often%20imperfect%2C%20requiring%0Aactive%20image%20manipulations%20such%20as%20cropping%2C%20editing%2C%20or%20enhancement%20to%20uncover%0Asalient%20visual%20cues.%20Beyond%20static%20visual%20perception%2C%20MLLMs%20must%20also%20think%0Awith%20images%3A%20dynamically%20transforming%20visual%20content%20and%20integrating%20it%20with%0Aother%20tools%20to%20solve%20complex%20tasks.%20However%2C%20this%20shift%20from%20treating%20vision%20as%0Apassive%20context%20to%20a%20manipulable%20cognitive%20workspace%20remains%20underexplored.%0AMost%20existing%20benchmarks%20still%20follow%20a%20think%20about%20images%20paradigm%2C%20where%0Aimages%20are%20regarded%20as%20static%20inputs.%20To%20address%20this%20gap%2C%20we%20introduce%20IRIS%2C%0Aan%20Interactive%20Reasoning%20with%20Images%20and%20Systems%20that%20evaluates%20MLLMs%27%20ability%0Ato%20perceive%2C%20transform%2C%20and%20reason%20across%20complex%20visual-textual%20tasks%20under%0Athe%20think%20with%20images%20paradigm.%20IRIS%20comprises%201%2C204%20challenging%2C%20open-ended%0Avision%20tasks%20%28603%20single-turn%2C%20601%20multi-turn%29%20spanning%20across%20five%20diverse%0Adomains%2C%20each%20paired%20with%20detailed%20rubrics%20to%20enable%20systematic%20evaluation.%20Our%0Aevaluation%20shows%20that%20current%20MLLMs%20struggle%20with%20tasks%20requiring%20effective%0Aintegration%20of%20vision%20and%20general-purpose%20tools.%20Even%20the%20strongest%20model%0A%28GPT-5-think%29%20reaches%20only%2018.68%25%20pass%20rate.%20We%20further%20observe%20divergent%0Atool-use%20behaviors%2C%20with%20OpenAI%20models%20benefiting%20from%20diverse%20image%0Amanipulations%20while%20Gemini-2.5-pro%20shows%20no%20improvement.%20By%20introducing%20the%0Afirst%20benchmark%20centered%20on%20think%20with%20images%2C%20IRIS%20offers%20critical%20insights%0Afor%20advancing%20visual%20intelligence%20in%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12712v1&entry.124074799=Read"},
{"title": "SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model", "author": "Lin Lin and Jiefeng Long and Zhihe Wan and Yuchi Wang and Dingkang Yang and Shuang Yang and Yueyang Yao and Xu Chen and Zirui Guo and Shengqiang Li and Weiran Li and Hanyu Li and Yaling Mou and Yan Qiu and Haiyang Yu and Xiao Liang and Hongsheng Li and Chao Feng", "abstract": "  Multimodal embedding models aim to yield informative unified representations\nthat empower diverse cross-modal tasks. Despite promising developments in the\nevolution from CLIP-based dual-tower architectures to large vision-language\nmodels, prior works still face unavoidable challenges in real-world\napplications and business scenarios, such as the limited modality support,\nunstable training mechanisms, and industrial domain gaps. In this work, we\nintroduce SAIL-Embedding, an omni-modal embedding foundation model that\naddresses these issues through tailored training strategies and architectural\ndesign. In the optimization procedure, we propose a multi-stage training scheme\nto boost the multifaceted effectiveness of representation learning.\nSpecifically, the content-aware progressive training aims to enhance the\nmodel's adaptability to diverse downstream tasks and master enriched\ncross-modal proficiency. The collaboration-aware recommendation enhancement\ntraining further adapts multimodal representations for recommendation scenarios\nby distilling knowledge from sequence-to-item and ID-to-item embeddings while\nmining user historical interests. Concurrently, we develop the stochastic\nspecialization and dataset-driven pattern matching to strengthen model training\nflexibility and generalizability. Experimental results show that SAIL-Embedding\nachieves SOTA performance compared to other methods in different retrieval\ntasks. In online experiments across various real-world scenarios integrated\nwith our model, we observe a significant increase in Lifetime (LT), which is a\ncrucial indicator for the recommendation experience. For instance, the model\ndelivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the\nDouyin-Selected scenario. For the Douyin feed rank model, the match features\nproduced by SAIL-Embedding yield a +0.08% AUC gain.\n", "link": "http://arxiv.org/abs/2510.12709v1", "date": "2025-10-14", "relevancy": 2.8612, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5958}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5605}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAIL-Embedding%20Technical%20Report%3A%20Omni-modal%20Embedding%20Foundation%20Model&body=Title%3A%20SAIL-Embedding%20Technical%20Report%3A%20Omni-modal%20Embedding%20Foundation%20Model%0AAuthor%3A%20Lin%20Lin%20and%20Jiefeng%20Long%20and%20Zhihe%20Wan%20and%20Yuchi%20Wang%20and%20Dingkang%20Yang%20and%20Shuang%20Yang%20and%20Yueyang%20Yao%20and%20Xu%20Chen%20and%20Zirui%20Guo%20and%20Shengqiang%20Li%20and%20Weiran%20Li%20and%20Hanyu%20Li%20and%20Yaling%20Mou%20and%20Yan%20Qiu%20and%20Haiyang%20Yu%20and%20Xiao%20Liang%20and%20Hongsheng%20Li%20and%20Chao%20Feng%0AAbstract%3A%20%20%20Multimodal%20embedding%20models%20aim%20to%20yield%20informative%20unified%20representations%0Athat%20empower%20diverse%20cross-modal%20tasks.%20Despite%20promising%20developments%20in%20the%0Aevolution%20from%20CLIP-based%20dual-tower%20architectures%20to%20large%20vision-language%0Amodels%2C%20prior%20works%20still%20face%20unavoidable%20challenges%20in%20real-world%0Aapplications%20and%20business%20scenarios%2C%20such%20as%20the%20limited%20modality%20support%2C%0Aunstable%20training%20mechanisms%2C%20and%20industrial%20domain%20gaps.%20In%20this%20work%2C%20we%0Aintroduce%20SAIL-Embedding%2C%20an%20omni-modal%20embedding%20foundation%20model%20that%0Aaddresses%20these%20issues%20through%20tailored%20training%20strategies%20and%20architectural%0Adesign.%20In%20the%20optimization%20procedure%2C%20we%20propose%20a%20multi-stage%20training%20scheme%0Ato%20boost%20the%20multifaceted%20effectiveness%20of%20representation%20learning.%0ASpecifically%2C%20the%20content-aware%20progressive%20training%20aims%20to%20enhance%20the%0Amodel%27s%20adaptability%20to%20diverse%20downstream%20tasks%20and%20master%20enriched%0Across-modal%20proficiency.%20The%20collaboration-aware%20recommendation%20enhancement%0Atraining%20further%20adapts%20multimodal%20representations%20for%20recommendation%20scenarios%0Aby%20distilling%20knowledge%20from%20sequence-to-item%20and%20ID-to-item%20embeddings%20while%0Amining%20user%20historical%20interests.%20Concurrently%2C%20we%20develop%20the%20stochastic%0Aspecialization%20and%20dataset-driven%20pattern%20matching%20to%20strengthen%20model%20training%0Aflexibility%20and%20generalizability.%20Experimental%20results%20show%20that%20SAIL-Embedding%0Aachieves%20SOTA%20performance%20compared%20to%20other%20methods%20in%20different%20retrieval%0Atasks.%20In%20online%20experiments%20across%20various%20real-world%20scenarios%20integrated%0Awith%20our%20model%2C%20we%20observe%20a%20significant%20increase%20in%20Lifetime%20%28LT%29%2C%20which%20is%20a%0Acrucial%20indicator%20for%20the%20recommendation%20experience.%20For%20instance%2C%20the%20model%0Adelivers%20the%207-day%20LT%20gain%20of%20%2B0.158%25%20and%20the%2014-day%20LT%20gain%20of%20%2B0.144%25%20in%20the%0ADouyin-Selected%20scenario.%20For%20the%20Douyin%20feed%20rank%20model%2C%20the%20match%20features%0Aproduced%20by%20SAIL-Embedding%20yield%20a%20%2B0.08%25%20AUC%20gain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAIL-Embedding%2520Technical%2520Report%253A%2520Omni-modal%2520Embedding%2520Foundation%2520Model%26entry.906535625%3DLin%2520Lin%2520and%2520Jiefeng%2520Long%2520and%2520Zhihe%2520Wan%2520and%2520Yuchi%2520Wang%2520and%2520Dingkang%2520Yang%2520and%2520Shuang%2520Yang%2520and%2520Yueyang%2520Yao%2520and%2520Xu%2520Chen%2520and%2520Zirui%2520Guo%2520and%2520Shengqiang%2520Li%2520and%2520Weiran%2520Li%2520and%2520Hanyu%2520Li%2520and%2520Yaling%2520Mou%2520and%2520Yan%2520Qiu%2520and%2520Haiyang%2520Yu%2520and%2520Xiao%2520Liang%2520and%2520Hongsheng%2520Li%2520and%2520Chao%2520Feng%26entry.1292438233%3D%2520%2520Multimodal%2520embedding%2520models%2520aim%2520to%2520yield%2520informative%2520unified%2520representations%250Athat%2520empower%2520diverse%2520cross-modal%2520tasks.%2520Despite%2520promising%2520developments%2520in%2520the%250Aevolution%2520from%2520CLIP-based%2520dual-tower%2520architectures%2520to%2520large%2520vision-language%250Amodels%252C%2520prior%2520works%2520still%2520face%2520unavoidable%2520challenges%2520in%2520real-world%250Aapplications%2520and%2520business%2520scenarios%252C%2520such%2520as%2520the%2520limited%2520modality%2520support%252C%250Aunstable%2520training%2520mechanisms%252C%2520and%2520industrial%2520domain%2520gaps.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520SAIL-Embedding%252C%2520an%2520omni-modal%2520embedding%2520foundation%2520model%2520that%250Aaddresses%2520these%2520issues%2520through%2520tailored%2520training%2520strategies%2520and%2520architectural%250Adesign.%2520In%2520the%2520optimization%2520procedure%252C%2520we%2520propose%2520a%2520multi-stage%2520training%2520scheme%250Ato%2520boost%2520the%2520multifaceted%2520effectiveness%2520of%2520representation%2520learning.%250ASpecifically%252C%2520the%2520content-aware%2520progressive%2520training%2520aims%2520to%2520enhance%2520the%250Amodel%2527s%2520adaptability%2520to%2520diverse%2520downstream%2520tasks%2520and%2520master%2520enriched%250Across-modal%2520proficiency.%2520The%2520collaboration-aware%2520recommendation%2520enhancement%250Atraining%2520further%2520adapts%2520multimodal%2520representations%2520for%2520recommendation%2520scenarios%250Aby%2520distilling%2520knowledge%2520from%2520sequence-to-item%2520and%2520ID-to-item%2520embeddings%2520while%250Amining%2520user%2520historical%2520interests.%2520Concurrently%252C%2520we%2520develop%2520the%2520stochastic%250Aspecialization%2520and%2520dataset-driven%2520pattern%2520matching%2520to%2520strengthen%2520model%2520training%250Aflexibility%2520and%2520generalizability.%2520Experimental%2520results%2520show%2520that%2520SAIL-Embedding%250Aachieves%2520SOTA%2520performance%2520compared%2520to%2520other%2520methods%2520in%2520different%2520retrieval%250Atasks.%2520In%2520online%2520experiments%2520across%2520various%2520real-world%2520scenarios%2520integrated%250Awith%2520our%2520model%252C%2520we%2520observe%2520a%2520significant%2520increase%2520in%2520Lifetime%2520%2528LT%2529%252C%2520which%2520is%2520a%250Acrucial%2520indicator%2520for%2520the%2520recommendation%2520experience.%2520For%2520instance%252C%2520the%2520model%250Adelivers%2520the%25207-day%2520LT%2520gain%2520of%2520%252B0.158%2525%2520and%2520the%252014-day%2520LT%2520gain%2520of%2520%252B0.144%2525%2520in%2520the%250ADouyin-Selected%2520scenario.%2520For%2520the%2520Douyin%2520feed%2520rank%2520model%252C%2520the%2520match%2520features%250Aproduced%2520by%2520SAIL-Embedding%2520yield%2520a%2520%252B0.08%2525%2520AUC%2520gain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAIL-Embedding%20Technical%20Report%3A%20Omni-modal%20Embedding%20Foundation%20Model&entry.906535625=Lin%20Lin%20and%20Jiefeng%20Long%20and%20Zhihe%20Wan%20and%20Yuchi%20Wang%20and%20Dingkang%20Yang%20and%20Shuang%20Yang%20and%20Yueyang%20Yao%20and%20Xu%20Chen%20and%20Zirui%20Guo%20and%20Shengqiang%20Li%20and%20Weiran%20Li%20and%20Hanyu%20Li%20and%20Yaling%20Mou%20and%20Yan%20Qiu%20and%20Haiyang%20Yu%20and%20Xiao%20Liang%20and%20Hongsheng%20Li%20and%20Chao%20Feng&entry.1292438233=%20%20Multimodal%20embedding%20models%20aim%20to%20yield%20informative%20unified%20representations%0Athat%20empower%20diverse%20cross-modal%20tasks.%20Despite%20promising%20developments%20in%20the%0Aevolution%20from%20CLIP-based%20dual-tower%20architectures%20to%20large%20vision-language%0Amodels%2C%20prior%20works%20still%20face%20unavoidable%20challenges%20in%20real-world%0Aapplications%20and%20business%20scenarios%2C%20such%20as%20the%20limited%20modality%20support%2C%0Aunstable%20training%20mechanisms%2C%20and%20industrial%20domain%20gaps.%20In%20this%20work%2C%20we%0Aintroduce%20SAIL-Embedding%2C%20an%20omni-modal%20embedding%20foundation%20model%20that%0Aaddresses%20these%20issues%20through%20tailored%20training%20strategies%20and%20architectural%0Adesign.%20In%20the%20optimization%20procedure%2C%20we%20propose%20a%20multi-stage%20training%20scheme%0Ato%20boost%20the%20multifaceted%20effectiveness%20of%20representation%20learning.%0ASpecifically%2C%20the%20content-aware%20progressive%20training%20aims%20to%20enhance%20the%0Amodel%27s%20adaptability%20to%20diverse%20downstream%20tasks%20and%20master%20enriched%0Across-modal%20proficiency.%20The%20collaboration-aware%20recommendation%20enhancement%0Atraining%20further%20adapts%20multimodal%20representations%20for%20recommendation%20scenarios%0Aby%20distilling%20knowledge%20from%20sequence-to-item%20and%20ID-to-item%20embeddings%20while%0Amining%20user%20historical%20interests.%20Concurrently%2C%20we%20develop%20the%20stochastic%0Aspecialization%20and%20dataset-driven%20pattern%20matching%20to%20strengthen%20model%20training%0Aflexibility%20and%20generalizability.%20Experimental%20results%20show%20that%20SAIL-Embedding%0Aachieves%20SOTA%20performance%20compared%20to%20other%20methods%20in%20different%20retrieval%0Atasks.%20In%20online%20experiments%20across%20various%20real-world%20scenarios%20integrated%0Awith%20our%20model%2C%20we%20observe%20a%20significant%20increase%20in%20Lifetime%20%28LT%29%2C%20which%20is%20a%0Acrucial%20indicator%20for%20the%20recommendation%20experience.%20For%20instance%2C%20the%20model%0Adelivers%20the%207-day%20LT%20gain%20of%20%2B0.158%25%20and%20the%2014-day%20LT%20gain%20of%20%2B0.144%25%20in%20the%0ADouyin-Selected%20scenario.%20For%20the%20Douyin%20feed%20rank%20model%2C%20the%20match%20features%0Aproduced%20by%20SAIL-Embedding%20yield%20a%20%2B0.08%25%20AUC%20gain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12709v1&entry.124074799=Read"},
{"title": "Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space", "author": "Chao Chen and Zhixin Ma and Yongqi Li and Yupeng Hu and Yinwei Wei and Wenjie Li and Liqiang Nie", "abstract": "  Multimodal reasoning aims to enhance the capabilities of MLLMs by\nincorporating intermediate reasoning steps before reaching the final answer. It\nhas evolved from text-only reasoning to the integration of visual information,\nenabling the thought process to be conveyed through both images and text.\nDespite its effectiveness, current multimodal reasoning methods depend on\nexplicit reasoning steps that require labor-intensive vision-text annotations\nand inherently introduce significant inference latency. To address these\nissues, we introduce multimodal latent reasoning with the advantages of\nmultimodal representation, reduced annotation, and inference efficiency. To\nfacilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR),\nwhich injects both visual and textual information in the reasoning process\nwithin the latent space. Specifically, IVT-LR represents each reasoning step by\ncombining two implicit parts: latent text (the hidden states from the previous\nstep) and latent vision (a set of selected image embeddings). We further\nintroduce a progressive multi-stage training strategy to enable MLLMs to\nperform the above multimodal latent reasoning steps. Experiments on M3CoT and\nScienceQA demonstrate that our IVT-LR method achieves an average performance\nincrease of 5.45% in accuracy, while simultaneously achieving a speed increase\nof over 5 times compared to existing approaches. Code available at\nhttps://github.com/FYYDCC/IVT-LR.\n", "link": "http://arxiv.org/abs/2510.12603v1", "date": "2025-10-14", "relevancy": 2.8478, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5763}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5763}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20in%20the%20Dark%3A%20Interleaved%20Vision-Text%20Reasoning%20in%20Latent%20Space&body=Title%3A%20Reasoning%20in%20the%20Dark%3A%20Interleaved%20Vision-Text%20Reasoning%20in%20Latent%20Space%0AAuthor%3A%20Chao%20Chen%20and%20Zhixin%20Ma%20and%20Yongqi%20Li%20and%20Yupeng%20Hu%20and%20Yinwei%20Wei%20and%20Wenjie%20Li%20and%20Liqiang%20Nie%0AAbstract%3A%20%20%20Multimodal%20reasoning%20aims%20to%20enhance%20the%20capabilities%20of%20MLLMs%20by%0Aincorporating%20intermediate%20reasoning%20steps%20before%20reaching%20the%20final%20answer.%20It%0Ahas%20evolved%20from%20text-only%20reasoning%20to%20the%20integration%20of%20visual%20information%2C%0Aenabling%20the%20thought%20process%20to%20be%20conveyed%20through%20both%20images%20and%20text.%0ADespite%20its%20effectiveness%2C%20current%20multimodal%20reasoning%20methods%20depend%20on%0Aexplicit%20reasoning%20steps%20that%20require%20labor-intensive%20vision-text%20annotations%0Aand%20inherently%20introduce%20significant%20inference%20latency.%20To%20address%20these%0Aissues%2C%20we%20introduce%20multimodal%20latent%20reasoning%20with%20the%20advantages%20of%0Amultimodal%20representation%2C%20reduced%20annotation%2C%20and%20inference%20efficiency.%20To%0Afacilicate%20it%2C%20we%20propose%20Interleaved%20Vision-Text%20Latent%20Reasoning%20%28IVT-LR%29%2C%0Awhich%20injects%20both%20visual%20and%20textual%20information%20in%20the%20reasoning%20process%0Awithin%20the%20latent%20space.%20Specifically%2C%20IVT-LR%20represents%20each%20reasoning%20step%20by%0Acombining%20two%20implicit%20parts%3A%20latent%20text%20%28the%20hidden%20states%20from%20the%20previous%0Astep%29%20and%20latent%20vision%20%28a%20set%20of%20selected%20image%20embeddings%29.%20We%20further%0Aintroduce%20a%20progressive%20multi-stage%20training%20strategy%20to%20enable%20MLLMs%20to%0Aperform%20the%20above%20multimodal%20latent%20reasoning%20steps.%20Experiments%20on%20M3CoT%20and%0AScienceQA%20demonstrate%20that%20our%20IVT-LR%20method%20achieves%20an%20average%20performance%0Aincrease%20of%205.45%25%20in%20accuracy%2C%20while%20simultaneously%20achieving%20a%20speed%20increase%0Aof%20over%205%20times%20compared%20to%20existing%20approaches.%20Code%20available%20at%0Ahttps%3A//github.com/FYYDCC/IVT-LR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520in%2520the%2520Dark%253A%2520Interleaved%2520Vision-Text%2520Reasoning%2520in%2520Latent%2520Space%26entry.906535625%3DChao%2520Chen%2520and%2520Zhixin%2520Ma%2520and%2520Yongqi%2520Li%2520and%2520Yupeng%2520Hu%2520and%2520Yinwei%2520Wei%2520and%2520Wenjie%2520Li%2520and%2520Liqiang%2520Nie%26entry.1292438233%3D%2520%2520Multimodal%2520reasoning%2520aims%2520to%2520enhance%2520the%2520capabilities%2520of%2520MLLMs%2520by%250Aincorporating%2520intermediate%2520reasoning%2520steps%2520before%2520reaching%2520the%2520final%2520answer.%2520It%250Ahas%2520evolved%2520from%2520text-only%2520reasoning%2520to%2520the%2520integration%2520of%2520visual%2520information%252C%250Aenabling%2520the%2520thought%2520process%2520to%2520be%2520conveyed%2520through%2520both%2520images%2520and%2520text.%250ADespite%2520its%2520effectiveness%252C%2520current%2520multimodal%2520reasoning%2520methods%2520depend%2520on%250Aexplicit%2520reasoning%2520steps%2520that%2520require%2520labor-intensive%2520vision-text%2520annotations%250Aand%2520inherently%2520introduce%2520significant%2520inference%2520latency.%2520To%2520address%2520these%250Aissues%252C%2520we%2520introduce%2520multimodal%2520latent%2520reasoning%2520with%2520the%2520advantages%2520of%250Amultimodal%2520representation%252C%2520reduced%2520annotation%252C%2520and%2520inference%2520efficiency.%2520To%250Afacilicate%2520it%252C%2520we%2520propose%2520Interleaved%2520Vision-Text%2520Latent%2520Reasoning%2520%2528IVT-LR%2529%252C%250Awhich%2520injects%2520both%2520visual%2520and%2520textual%2520information%2520in%2520the%2520reasoning%2520process%250Awithin%2520the%2520latent%2520space.%2520Specifically%252C%2520IVT-LR%2520represents%2520each%2520reasoning%2520step%2520by%250Acombining%2520two%2520implicit%2520parts%253A%2520latent%2520text%2520%2528the%2520hidden%2520states%2520from%2520the%2520previous%250Astep%2529%2520and%2520latent%2520vision%2520%2528a%2520set%2520of%2520selected%2520image%2520embeddings%2529.%2520We%2520further%250Aintroduce%2520a%2520progressive%2520multi-stage%2520training%2520strategy%2520to%2520enable%2520MLLMs%2520to%250Aperform%2520the%2520above%2520multimodal%2520latent%2520reasoning%2520steps.%2520Experiments%2520on%2520M3CoT%2520and%250AScienceQA%2520demonstrate%2520that%2520our%2520IVT-LR%2520method%2520achieves%2520an%2520average%2520performance%250Aincrease%2520of%25205.45%2525%2520in%2520accuracy%252C%2520while%2520simultaneously%2520achieving%2520a%2520speed%2520increase%250Aof%2520over%25205%2520times%2520compared%2520to%2520existing%2520approaches.%2520Code%2520available%2520at%250Ahttps%253A//github.com/FYYDCC/IVT-LR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20in%20the%20Dark%3A%20Interleaved%20Vision-Text%20Reasoning%20in%20Latent%20Space&entry.906535625=Chao%20Chen%20and%20Zhixin%20Ma%20and%20Yongqi%20Li%20and%20Yupeng%20Hu%20and%20Yinwei%20Wei%20and%20Wenjie%20Li%20and%20Liqiang%20Nie&entry.1292438233=%20%20Multimodal%20reasoning%20aims%20to%20enhance%20the%20capabilities%20of%20MLLMs%20by%0Aincorporating%20intermediate%20reasoning%20steps%20before%20reaching%20the%20final%20answer.%20It%0Ahas%20evolved%20from%20text-only%20reasoning%20to%20the%20integration%20of%20visual%20information%2C%0Aenabling%20the%20thought%20process%20to%20be%20conveyed%20through%20both%20images%20and%20text.%0ADespite%20its%20effectiveness%2C%20current%20multimodal%20reasoning%20methods%20depend%20on%0Aexplicit%20reasoning%20steps%20that%20require%20labor-intensive%20vision-text%20annotations%0Aand%20inherently%20introduce%20significant%20inference%20latency.%20To%20address%20these%0Aissues%2C%20we%20introduce%20multimodal%20latent%20reasoning%20with%20the%20advantages%20of%0Amultimodal%20representation%2C%20reduced%20annotation%2C%20and%20inference%20efficiency.%20To%0Afacilicate%20it%2C%20we%20propose%20Interleaved%20Vision-Text%20Latent%20Reasoning%20%28IVT-LR%29%2C%0Awhich%20injects%20both%20visual%20and%20textual%20information%20in%20the%20reasoning%20process%0Awithin%20the%20latent%20space.%20Specifically%2C%20IVT-LR%20represents%20each%20reasoning%20step%20by%0Acombining%20two%20implicit%20parts%3A%20latent%20text%20%28the%20hidden%20states%20from%20the%20previous%0Astep%29%20and%20latent%20vision%20%28a%20set%20of%20selected%20image%20embeddings%29.%20We%20further%0Aintroduce%20a%20progressive%20multi-stage%20training%20strategy%20to%20enable%20MLLMs%20to%0Aperform%20the%20above%20multimodal%20latent%20reasoning%20steps.%20Experiments%20on%20M3CoT%20and%0AScienceQA%20demonstrate%20that%20our%20IVT-LR%20method%20achieves%20an%20average%20performance%0Aincrease%20of%205.45%25%20in%20accuracy%2C%20while%20simultaneously%20achieving%20a%20speed%20increase%0Aof%20over%205%20times%20compared%20to%20existing%20approaches.%20Code%20available%20at%0Ahttps%3A//github.com/FYYDCC/IVT-LR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12603v1&entry.124074799=Read"},
{"title": "SMEC: Rethinking Matryoshka Representation Learning for Retrieval\n  Embedding Compression", "author": "Biao Zhang and Lixin Chen and Tong Liu and Bo Zheng", "abstract": "  Large language models (LLMs) generate high-dimensional embeddings that\ncapture rich semantic and syntactic information. However, high-dimensional\nembeddings exacerbate computational complexity and storage requirements,\nthereby hindering practical deployment. To address these challenges, we propose\na novel training framework named Sequential Matryoshka Embedding Compression\n(SMEC). This framework introduces the Sequential Matryoshka Representation\nLearning(SMRL) method to mitigate gradient variance during training, the\nAdaptive Dimension Selection (ADS) module to reduce information degradation\nduring dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module\nto enhance unsupervised learning between high- and low-dimensional embeddings.\nExperiments on image, text, and multimodal datasets demonstrate that SMEC\nachieves significant dimensionality reduction while maintaining performance.\nFor instance, on the BEIR dataset, our approach improves the performance of\ncompressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points\ncompared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.\n", "link": "http://arxiv.org/abs/2510.12474v1", "date": "2025-10-14", "relevancy": 2.8168, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5844}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMEC%3A%20Rethinking%20Matryoshka%20Representation%20Learning%20for%20Retrieval%0A%20%20Embedding%20Compression&body=Title%3A%20SMEC%3A%20Rethinking%20Matryoshka%20Representation%20Learning%20for%20Retrieval%0A%20%20Embedding%20Compression%0AAuthor%3A%20Biao%20Zhang%20and%20Lixin%20Chen%20and%20Tong%20Liu%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20generate%20high-dimensional%20embeddings%20that%0Acapture%20rich%20semantic%20and%20syntactic%20information.%20However%2C%20high-dimensional%0Aembeddings%20exacerbate%20computational%20complexity%20and%20storage%20requirements%2C%0Athereby%20hindering%20practical%20deployment.%20To%20address%20these%20challenges%2C%20we%20propose%0Aa%20novel%20training%20framework%20named%20Sequential%20Matryoshka%20Embedding%20Compression%0A%28SMEC%29.%20This%20framework%20introduces%20the%20Sequential%20Matryoshka%20Representation%0ALearning%28SMRL%29%20method%20to%20mitigate%20gradient%20variance%20during%20training%2C%20the%0AAdaptive%20Dimension%20Selection%20%28ADS%29%20module%20to%20reduce%20information%20degradation%0Aduring%20dimension%20pruning%2C%20and%20the%20Selectable%20Cross-batch%20Memory%20%28S-XBM%29%20module%0Ato%20enhance%20unsupervised%20learning%20between%20high-%20and%20low-dimensional%20embeddings.%0AExperiments%20on%20image%2C%20text%2C%20and%20multimodal%20datasets%20demonstrate%20that%20SMEC%0Aachieves%20significant%20dimensionality%20reduction%20while%20maintaining%20performance.%0AFor%20instance%2C%20on%20the%20BEIR%20dataset%2C%20our%20approach%20improves%20the%20performance%20of%0Acompressed%20LLM2Vec%20embeddings%20%28256%20dimensions%29%20by%201.1%20points%20and%202.7%20points%0Acompared%20to%20the%20Matryoshka-Adaptor%20and%20Search-Adaptor%20models%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12474v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMEC%253A%2520Rethinking%2520Matryoshka%2520Representation%2520Learning%2520for%2520Retrieval%250A%2520%2520Embedding%2520Compression%26entry.906535625%3DBiao%2520Zhang%2520and%2520Lixin%2520Chen%2520and%2520Tong%2520Liu%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520generate%2520high-dimensional%2520embeddings%2520that%250Acapture%2520rich%2520semantic%2520and%2520syntactic%2520information.%2520However%252C%2520high-dimensional%250Aembeddings%2520exacerbate%2520computational%2520complexity%2520and%2520storage%2520requirements%252C%250Athereby%2520hindering%2520practical%2520deployment.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250Aa%2520novel%2520training%2520framework%2520named%2520Sequential%2520Matryoshka%2520Embedding%2520Compression%250A%2528SMEC%2529.%2520This%2520framework%2520introduces%2520the%2520Sequential%2520Matryoshka%2520Representation%250ALearning%2528SMRL%2529%2520method%2520to%2520mitigate%2520gradient%2520variance%2520during%2520training%252C%2520the%250AAdaptive%2520Dimension%2520Selection%2520%2528ADS%2529%2520module%2520to%2520reduce%2520information%2520degradation%250Aduring%2520dimension%2520pruning%252C%2520and%2520the%2520Selectable%2520Cross-batch%2520Memory%2520%2528S-XBM%2529%2520module%250Ato%2520enhance%2520unsupervised%2520learning%2520between%2520high-%2520and%2520low-dimensional%2520embeddings.%250AExperiments%2520on%2520image%252C%2520text%252C%2520and%2520multimodal%2520datasets%2520demonstrate%2520that%2520SMEC%250Aachieves%2520significant%2520dimensionality%2520reduction%2520while%2520maintaining%2520performance.%250AFor%2520instance%252C%2520on%2520the%2520BEIR%2520dataset%252C%2520our%2520approach%2520improves%2520the%2520performance%2520of%250Acompressed%2520LLM2Vec%2520embeddings%2520%2528256%2520dimensions%2529%2520by%25201.1%2520points%2520and%25202.7%2520points%250Acompared%2520to%2520the%2520Matryoshka-Adaptor%2520and%2520Search-Adaptor%2520models%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12474v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMEC%3A%20Rethinking%20Matryoshka%20Representation%20Learning%20for%20Retrieval%0A%20%20Embedding%20Compression&entry.906535625=Biao%20Zhang%20and%20Lixin%20Chen%20and%20Tong%20Liu%20and%20Bo%20Zheng&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20generate%20high-dimensional%20embeddings%20that%0Acapture%20rich%20semantic%20and%20syntactic%20information.%20However%2C%20high-dimensional%0Aembeddings%20exacerbate%20computational%20complexity%20and%20storage%20requirements%2C%0Athereby%20hindering%20practical%20deployment.%20To%20address%20these%20challenges%2C%20we%20propose%0Aa%20novel%20training%20framework%20named%20Sequential%20Matryoshka%20Embedding%20Compression%0A%28SMEC%29.%20This%20framework%20introduces%20the%20Sequential%20Matryoshka%20Representation%0ALearning%28SMRL%29%20method%20to%20mitigate%20gradient%20variance%20during%20training%2C%20the%0AAdaptive%20Dimension%20Selection%20%28ADS%29%20module%20to%20reduce%20information%20degradation%0Aduring%20dimension%20pruning%2C%20and%20the%20Selectable%20Cross-batch%20Memory%20%28S-XBM%29%20module%0Ato%20enhance%20unsupervised%20learning%20between%20high-%20and%20low-dimensional%20embeddings.%0AExperiments%20on%20image%2C%20text%2C%20and%20multimodal%20datasets%20demonstrate%20that%20SMEC%0Aachieves%20significant%20dimensionality%20reduction%20while%20maintaining%20performance.%0AFor%20instance%2C%20on%20the%20BEIR%20dataset%2C%20our%20approach%20improves%20the%20performance%20of%0Acompressed%20LLM2Vec%20embeddings%20%28256%20dimensions%29%20by%201.1%20points%20and%202.7%20points%0Acompared%20to%20the%20Matryoshka-Adaptor%20and%20Search-Adaptor%20models%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12474v1&entry.124074799=Read"},
{"title": "VQArt-Bench: A semantically rich VQA Benchmark for Art and Cultural\n  Heritage", "author": "A. Alfarano and L. Venturoli and D. Negueruela del Castillo", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated significant\ncapabilities in joint visual and linguistic tasks. However, existing Visual\nQuestion Answering (VQA) benchmarks often fail to evaluate deep semantic\nunderstanding, particularly in complex domains like visual art analysis.\nConfined to simple syntactic structures and surface-level attributes, these\nquestions fail to capture the diversity and depth of human visual inquiry. This\nlimitation incentivizes models to exploit statistical shortcuts rather than\nengage in visual reasoning. To address this gap, we introduce VQArt-Bench, a\nnew, large-scale VQA benchmark for the cultural heritage domain. This benchmark\nis constructed using a novel multi-agent pipeline where specialized agents\ncollaborate to generate nuanced, validated, and linguistically diverse\nquestions. The resulting benchmark is structured along relevant visual\nunderstanding dimensions that probe a model's ability to interpret symbolic\nmeaning, narratives, and complex visual relationships. Our evaluation of 14\nstate-of-the-art MLLMs on this benchmark reveals significant limitations in\ncurrent models, including a surprising weakness in simple counting tasks and a\nclear performance gap between proprietary and open-source models.\n", "link": "http://arxiv.org/abs/2510.12750v1", "date": "2025-10-14", "relevancy": 2.8031, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.573}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.573}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VQArt-Bench%3A%20A%20semantically%20rich%20VQA%20Benchmark%20for%20Art%20and%20Cultural%0A%20%20Heritage&body=Title%3A%20VQArt-Bench%3A%20A%20semantically%20rich%20VQA%20Benchmark%20for%20Art%20and%20Cultural%0A%20%20Heritage%0AAuthor%3A%20A.%20Alfarano%20and%20L.%20Venturoli%20and%20D.%20Negueruela%20del%20Castillo%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20significant%0Acapabilities%20in%20joint%20visual%20and%20linguistic%20tasks.%20However%2C%20existing%20Visual%0AQuestion%20Answering%20%28VQA%29%20benchmarks%20often%20fail%20to%20evaluate%20deep%20semantic%0Aunderstanding%2C%20particularly%20in%20complex%20domains%20like%20visual%20art%20analysis.%0AConfined%20to%20simple%20syntactic%20structures%20and%20surface-level%20attributes%2C%20these%0Aquestions%20fail%20to%20capture%20the%20diversity%20and%20depth%20of%20human%20visual%20inquiry.%20This%0Alimitation%20incentivizes%20models%20to%20exploit%20statistical%20shortcuts%20rather%20than%0Aengage%20in%20visual%20reasoning.%20To%20address%20this%20gap%2C%20we%20introduce%20VQArt-Bench%2C%20a%0Anew%2C%20large-scale%20VQA%20benchmark%20for%20the%20cultural%20heritage%20domain.%20This%20benchmark%0Ais%20constructed%20using%20a%20novel%20multi-agent%20pipeline%20where%20specialized%20agents%0Acollaborate%20to%20generate%20nuanced%2C%20validated%2C%20and%20linguistically%20diverse%0Aquestions.%20The%20resulting%20benchmark%20is%20structured%20along%20relevant%20visual%0Aunderstanding%20dimensions%20that%20probe%20a%20model%27s%20ability%20to%20interpret%20symbolic%0Ameaning%2C%20narratives%2C%20and%20complex%20visual%20relationships.%20Our%20evaluation%20of%2014%0Astate-of-the-art%20MLLMs%20on%20this%20benchmark%20reveals%20significant%20limitations%20in%0Acurrent%20models%2C%20including%20a%20surprising%20weakness%20in%20simple%20counting%20tasks%20and%20a%0Aclear%20performance%20gap%20between%20proprietary%20and%20open-source%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVQArt-Bench%253A%2520A%2520semantically%2520rich%2520VQA%2520Benchmark%2520for%2520Art%2520and%2520Cultural%250A%2520%2520Heritage%26entry.906535625%3DA.%2520Alfarano%2520and%2520L.%2520Venturoli%2520and%2520D.%2520Negueruela%2520del%2520Castillo%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520significant%250Acapabilities%2520in%2520joint%2520visual%2520and%2520linguistic%2520tasks.%2520However%252C%2520existing%2520Visual%250AQuestion%2520Answering%2520%2528VQA%2529%2520benchmarks%2520often%2520fail%2520to%2520evaluate%2520deep%2520semantic%250Aunderstanding%252C%2520particularly%2520in%2520complex%2520domains%2520like%2520visual%2520art%2520analysis.%250AConfined%2520to%2520simple%2520syntactic%2520structures%2520and%2520surface-level%2520attributes%252C%2520these%250Aquestions%2520fail%2520to%2520capture%2520the%2520diversity%2520and%2520depth%2520of%2520human%2520visual%2520inquiry.%2520This%250Alimitation%2520incentivizes%2520models%2520to%2520exploit%2520statistical%2520shortcuts%2520rather%2520than%250Aengage%2520in%2520visual%2520reasoning.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520VQArt-Bench%252C%2520a%250Anew%252C%2520large-scale%2520VQA%2520benchmark%2520for%2520the%2520cultural%2520heritage%2520domain.%2520This%2520benchmark%250Ais%2520constructed%2520using%2520a%2520novel%2520multi-agent%2520pipeline%2520where%2520specialized%2520agents%250Acollaborate%2520to%2520generate%2520nuanced%252C%2520validated%252C%2520and%2520linguistically%2520diverse%250Aquestions.%2520The%2520resulting%2520benchmark%2520is%2520structured%2520along%2520relevant%2520visual%250Aunderstanding%2520dimensions%2520that%2520probe%2520a%2520model%2527s%2520ability%2520to%2520interpret%2520symbolic%250Ameaning%252C%2520narratives%252C%2520and%2520complex%2520visual%2520relationships.%2520Our%2520evaluation%2520of%252014%250Astate-of-the-art%2520MLLMs%2520on%2520this%2520benchmark%2520reveals%2520significant%2520limitations%2520in%250Acurrent%2520models%252C%2520including%2520a%2520surprising%2520weakness%2520in%2520simple%2520counting%2520tasks%2520and%2520a%250Aclear%2520performance%2520gap%2520between%2520proprietary%2520and%2520open-source%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VQArt-Bench%3A%20A%20semantically%20rich%20VQA%20Benchmark%20for%20Art%20and%20Cultural%0A%20%20Heritage&entry.906535625=A.%20Alfarano%20and%20L.%20Venturoli%20and%20D.%20Negueruela%20del%20Castillo&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20significant%0Acapabilities%20in%20joint%20visual%20and%20linguistic%20tasks.%20However%2C%20existing%20Visual%0AQuestion%20Answering%20%28VQA%29%20benchmarks%20often%20fail%20to%20evaluate%20deep%20semantic%0Aunderstanding%2C%20particularly%20in%20complex%20domains%20like%20visual%20art%20analysis.%0AConfined%20to%20simple%20syntactic%20structures%20and%20surface-level%20attributes%2C%20these%0Aquestions%20fail%20to%20capture%20the%20diversity%20and%20depth%20of%20human%20visual%20inquiry.%20This%0Alimitation%20incentivizes%20models%20to%20exploit%20statistical%20shortcuts%20rather%20than%0Aengage%20in%20visual%20reasoning.%20To%20address%20this%20gap%2C%20we%20introduce%20VQArt-Bench%2C%20a%0Anew%2C%20large-scale%20VQA%20benchmark%20for%20the%20cultural%20heritage%20domain.%20This%20benchmark%0Ais%20constructed%20using%20a%20novel%20multi-agent%20pipeline%20where%20specialized%20agents%0Acollaborate%20to%20generate%20nuanced%2C%20validated%2C%20and%20linguistically%20diverse%0Aquestions.%20The%20resulting%20benchmark%20is%20structured%20along%20relevant%20visual%0Aunderstanding%20dimensions%20that%20probe%20a%20model%27s%20ability%20to%20interpret%20symbolic%0Ameaning%2C%20narratives%2C%20and%20complex%20visual%20relationships.%20Our%20evaluation%20of%2014%0Astate-of-the-art%20MLLMs%20on%20this%20benchmark%20reveals%20significant%20limitations%20in%0Acurrent%20models%2C%20including%20a%20surprising%20weakness%20in%20simple%20counting%20tasks%20and%20a%0Aclear%20performance%20gap%20between%20proprietary%20and%20open-source%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12750v1&entry.124074799=Read"},
{"title": "UrbanTwin: Building High-Fidelity Digital Twins for Sim2Real LiDAR\n  Perception and Evaluation", "author": "Muhammad Shahbaz and Shaurya Agarwal", "abstract": "  LiDAR-based perception in intelligent transportation systems (ITS) relies on\ndeep neural networks trained with large-scale labeled datasets. However,\ncreating such datasets is expensive, time-consuming, and labor-intensive,\nlimiting the scalability of perception systems. Sim2Real learning offers a\nscalable alternative, but its success depends on the simulation's fidelity to\nreal-world environments, dynamics, and sensors. This tutorial introduces a\nreproducible workflow for building high-fidelity digital twins (HiFi DTs) to\ngenerate realistic synthetic datasets. We outline practical steps for modeling\nstatic geometry, road infrastructure, and dynamic traffic using open-source\nresources such as satellite imagery, OpenStreetMap, and sensor specifications.\nThe resulting environments support scalable and cost-effective data generation\nfor robust Sim2Real learning. Using this workflow, we have released three\nsynthetic LiDAR datasets, namely UT-LUMPI, UT-V2X-Real, and UT-TUMTraf-I, which\nclosely replicate real locations and outperform real-data-trained baselines in\nperception tasks. This guide enables broader adoption of HiFi DTs in ITS\nresearch and deployment.\n", "link": "http://arxiv.org/abs/2509.02903v2", "date": "2025-10-14", "relevancy": 2.7605, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5546}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5508}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UrbanTwin%3A%20Building%20High-Fidelity%20Digital%20Twins%20for%20Sim2Real%20LiDAR%0A%20%20Perception%20and%20Evaluation&body=Title%3A%20UrbanTwin%3A%20Building%20High-Fidelity%20Digital%20Twins%20for%20Sim2Real%20LiDAR%0A%20%20Perception%20and%20Evaluation%0AAuthor%3A%20Muhammad%20Shahbaz%20and%20Shaurya%20Agarwal%0AAbstract%3A%20%20%20LiDAR-based%20perception%20in%20intelligent%20transportation%20systems%20%28ITS%29%20relies%20on%0Adeep%20neural%20networks%20trained%20with%20large-scale%20labeled%20datasets.%20However%2C%0Acreating%20such%20datasets%20is%20expensive%2C%20time-consuming%2C%20and%20labor-intensive%2C%0Alimiting%20the%20scalability%20of%20perception%20systems.%20Sim2Real%20learning%20offers%20a%0Ascalable%20alternative%2C%20but%20its%20success%20depends%20on%20the%20simulation%27s%20fidelity%20to%0Areal-world%20environments%2C%20dynamics%2C%20and%20sensors.%20This%20tutorial%20introduces%20a%0Areproducible%20workflow%20for%20building%20high-fidelity%20digital%20twins%20%28HiFi%20DTs%29%20to%0Agenerate%20realistic%20synthetic%20datasets.%20We%20outline%20practical%20steps%20for%20modeling%0Astatic%20geometry%2C%20road%20infrastructure%2C%20and%20dynamic%20traffic%20using%20open-source%0Aresources%20such%20as%20satellite%20imagery%2C%20OpenStreetMap%2C%20and%20sensor%20specifications.%0AThe%20resulting%20environments%20support%20scalable%20and%20cost-effective%20data%20generation%0Afor%20robust%20Sim2Real%20learning.%20Using%20this%20workflow%2C%20we%20have%20released%20three%0Asynthetic%20LiDAR%20datasets%2C%20namely%20UT-LUMPI%2C%20UT-V2X-Real%2C%20and%20UT-TUMTraf-I%2C%20which%0Aclosely%20replicate%20real%20locations%20and%20outperform%20real-data-trained%20baselines%20in%0Aperception%20tasks.%20This%20guide%20enables%20broader%20adoption%20of%20HiFi%20DTs%20in%20ITS%0Aresearch%20and%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.02903v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrbanTwin%253A%2520Building%2520High-Fidelity%2520Digital%2520Twins%2520for%2520Sim2Real%2520LiDAR%250A%2520%2520Perception%2520and%2520Evaluation%26entry.906535625%3DMuhammad%2520Shahbaz%2520and%2520Shaurya%2520Agarwal%26entry.1292438233%3D%2520%2520LiDAR-based%2520perception%2520in%2520intelligent%2520transportation%2520systems%2520%2528ITS%2529%2520relies%2520on%250Adeep%2520neural%2520networks%2520trained%2520with%2520large-scale%2520labeled%2520datasets.%2520However%252C%250Acreating%2520such%2520datasets%2520is%2520expensive%252C%2520time-consuming%252C%2520and%2520labor-intensive%252C%250Alimiting%2520the%2520scalability%2520of%2520perception%2520systems.%2520Sim2Real%2520learning%2520offers%2520a%250Ascalable%2520alternative%252C%2520but%2520its%2520success%2520depends%2520on%2520the%2520simulation%2527s%2520fidelity%2520to%250Areal-world%2520environments%252C%2520dynamics%252C%2520and%2520sensors.%2520This%2520tutorial%2520introduces%2520a%250Areproducible%2520workflow%2520for%2520building%2520high-fidelity%2520digital%2520twins%2520%2528HiFi%2520DTs%2529%2520to%250Agenerate%2520realistic%2520synthetic%2520datasets.%2520We%2520outline%2520practical%2520steps%2520for%2520modeling%250Astatic%2520geometry%252C%2520road%2520infrastructure%252C%2520and%2520dynamic%2520traffic%2520using%2520open-source%250Aresources%2520such%2520as%2520satellite%2520imagery%252C%2520OpenStreetMap%252C%2520and%2520sensor%2520specifications.%250AThe%2520resulting%2520environments%2520support%2520scalable%2520and%2520cost-effective%2520data%2520generation%250Afor%2520robust%2520Sim2Real%2520learning.%2520Using%2520this%2520workflow%252C%2520we%2520have%2520released%2520three%250Asynthetic%2520LiDAR%2520datasets%252C%2520namely%2520UT-LUMPI%252C%2520UT-V2X-Real%252C%2520and%2520UT-TUMTraf-I%252C%2520which%250Aclosely%2520replicate%2520real%2520locations%2520and%2520outperform%2520real-data-trained%2520baselines%2520in%250Aperception%2520tasks.%2520This%2520guide%2520enables%2520broader%2520adoption%2520of%2520HiFi%2520DTs%2520in%2520ITS%250Aresearch%2520and%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.02903v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UrbanTwin%3A%20Building%20High-Fidelity%20Digital%20Twins%20for%20Sim2Real%20LiDAR%0A%20%20Perception%20and%20Evaluation&entry.906535625=Muhammad%20Shahbaz%20and%20Shaurya%20Agarwal&entry.1292438233=%20%20LiDAR-based%20perception%20in%20intelligent%20transportation%20systems%20%28ITS%29%20relies%20on%0Adeep%20neural%20networks%20trained%20with%20large-scale%20labeled%20datasets.%20However%2C%0Acreating%20such%20datasets%20is%20expensive%2C%20time-consuming%2C%20and%20labor-intensive%2C%0Alimiting%20the%20scalability%20of%20perception%20systems.%20Sim2Real%20learning%20offers%20a%0Ascalable%20alternative%2C%20but%20its%20success%20depends%20on%20the%20simulation%27s%20fidelity%20to%0Areal-world%20environments%2C%20dynamics%2C%20and%20sensors.%20This%20tutorial%20introduces%20a%0Areproducible%20workflow%20for%20building%20high-fidelity%20digital%20twins%20%28HiFi%20DTs%29%20to%0Agenerate%20realistic%20synthetic%20datasets.%20We%20outline%20practical%20steps%20for%20modeling%0Astatic%20geometry%2C%20road%20infrastructure%2C%20and%20dynamic%20traffic%20using%20open-source%0Aresources%20such%20as%20satellite%20imagery%2C%20OpenStreetMap%2C%20and%20sensor%20specifications.%0AThe%20resulting%20environments%20support%20scalable%20and%20cost-effective%20data%20generation%0Afor%20robust%20Sim2Real%20learning.%20Using%20this%20workflow%2C%20we%20have%20released%20three%0Asynthetic%20LiDAR%20datasets%2C%20namely%20UT-LUMPI%2C%20UT-V2X-Real%2C%20and%20UT-TUMTraf-I%2C%20which%0Aclosely%20replicate%20real%20locations%20and%20outperform%20real-data-trained%20baselines%20in%0Aperception%20tasks.%20This%20guide%20enables%20broader%20adoption%20of%20HiFi%20DTs%20in%20ITS%0Aresearch%20and%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.02903v2&entry.124074799=Read"},
{"title": "Enhancing Representations through Heterogeneous Self-Supervised Learning", "author": "Zhong-Yu Li and Bo-Wen Yin and Yongxiang Liu and Li Liu and Ming-Ming Cheng", "abstract": "  Incorporating heterogeneous representations from different architectures has\nfacilitated various vision tasks, e.g., some hybrid networks combine\ntransformers and convolutions. However, complementarity between such\nheterogeneous architectures has not been well exploited in self-supervised\nlearning. Thus, we propose Heterogeneous Self-Supervised Learning (HSSL), which\nenforces a base model to learn from an auxiliary head whose architecture is\nheterogeneous from the base model. In this process, HSSL endows the base model\nwith new characteristics in a representation learning way without structural\nchanges. To comprehensively understand the HSSL, we conduct experiments on\nvarious heterogeneous pairs containing a base model and an auxiliary head. We\ndiscover that the representation quality of the base model moves up as their\narchitecture discrepancy grows. This observation motivates us to propose a\nsearch strategy that quickly determines the most suitable auxiliary head for a\nspecific base model to learn and several simple but effective methods to\nenlarge the model discrepancy. The HSSL is compatible with various\nself-supervised methods, achieving superior performances on various downstream\ntasks, including image classification, semantic segmentation, instance\nsegmentation, and object detection. The codes are available at\nhttps://github.com/NK-JittorCV/Self-Supervised/.\n", "link": "http://arxiv.org/abs/2310.05108v4", "date": "2025-10-14", "relevancy": 2.673, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5734}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5194}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Representations%20through%20Heterogeneous%20Self-Supervised%20Learning&body=Title%3A%20Enhancing%20Representations%20through%20Heterogeneous%20Self-Supervised%20Learning%0AAuthor%3A%20Zhong-Yu%20Li%20and%20Bo-Wen%20Yin%20and%20Yongxiang%20Liu%20and%20Li%20Liu%20and%20Ming-Ming%20Cheng%0AAbstract%3A%20%20%20Incorporating%20heterogeneous%20representations%20from%20different%20architectures%20has%0Afacilitated%20various%20vision%20tasks%2C%20e.g.%2C%20some%20hybrid%20networks%20combine%0Atransformers%20and%20convolutions.%20However%2C%20complementarity%20between%20such%0Aheterogeneous%20architectures%20has%20not%20been%20well%20exploited%20in%20self-supervised%0Alearning.%20Thus%2C%20we%20propose%20Heterogeneous%20Self-Supervised%20Learning%20%28HSSL%29%2C%20which%0Aenforces%20a%20base%20model%20to%20learn%20from%20an%20auxiliary%20head%20whose%20architecture%20is%0Aheterogeneous%20from%20the%20base%20model.%20In%20this%20process%2C%20HSSL%20endows%20the%20base%20model%0Awith%20new%20characteristics%20in%20a%20representation%20learning%20way%20without%20structural%0Achanges.%20To%20comprehensively%20understand%20the%20HSSL%2C%20we%20conduct%20experiments%20on%0Avarious%20heterogeneous%20pairs%20containing%20a%20base%20model%20and%20an%20auxiliary%20head.%20We%0Adiscover%20that%20the%20representation%20quality%20of%20the%20base%20model%20moves%20up%20as%20their%0Aarchitecture%20discrepancy%20grows.%20This%20observation%20motivates%20us%20to%20propose%20a%0Asearch%20strategy%20that%20quickly%20determines%20the%20most%20suitable%20auxiliary%20head%20for%20a%0Aspecific%20base%20model%20to%20learn%20and%20several%20simple%20but%20effective%20methods%20to%0Aenlarge%20the%20model%20discrepancy.%20The%20HSSL%20is%20compatible%20with%20various%0Aself-supervised%20methods%2C%20achieving%20superior%20performances%20on%20various%20downstream%0Atasks%2C%20including%20image%20classification%2C%20semantic%20segmentation%2C%20instance%0Asegmentation%2C%20and%20object%20detection.%20The%20codes%20are%20available%20at%0Ahttps%3A//github.com/NK-JittorCV/Self-Supervised/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05108v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Representations%2520through%2520Heterogeneous%2520Self-Supervised%2520Learning%26entry.906535625%3DZhong-Yu%2520Li%2520and%2520Bo-Wen%2520Yin%2520and%2520Yongxiang%2520Liu%2520and%2520Li%2520Liu%2520and%2520Ming-Ming%2520Cheng%26entry.1292438233%3D%2520%2520Incorporating%2520heterogeneous%2520representations%2520from%2520different%2520architectures%2520has%250Afacilitated%2520various%2520vision%2520tasks%252C%2520e.g.%252C%2520some%2520hybrid%2520networks%2520combine%250Atransformers%2520and%2520convolutions.%2520However%252C%2520complementarity%2520between%2520such%250Aheterogeneous%2520architectures%2520has%2520not%2520been%2520well%2520exploited%2520in%2520self-supervised%250Alearning.%2520Thus%252C%2520we%2520propose%2520Heterogeneous%2520Self-Supervised%2520Learning%2520%2528HSSL%2529%252C%2520which%250Aenforces%2520a%2520base%2520model%2520to%2520learn%2520from%2520an%2520auxiliary%2520head%2520whose%2520architecture%2520is%250Aheterogeneous%2520from%2520the%2520base%2520model.%2520In%2520this%2520process%252C%2520HSSL%2520endows%2520the%2520base%2520model%250Awith%2520new%2520characteristics%2520in%2520a%2520representation%2520learning%2520way%2520without%2520structural%250Achanges.%2520To%2520comprehensively%2520understand%2520the%2520HSSL%252C%2520we%2520conduct%2520experiments%2520on%250Avarious%2520heterogeneous%2520pairs%2520containing%2520a%2520base%2520model%2520and%2520an%2520auxiliary%2520head.%2520We%250Adiscover%2520that%2520the%2520representation%2520quality%2520of%2520the%2520base%2520model%2520moves%2520up%2520as%2520their%250Aarchitecture%2520discrepancy%2520grows.%2520This%2520observation%2520motivates%2520us%2520to%2520propose%2520a%250Asearch%2520strategy%2520that%2520quickly%2520determines%2520the%2520most%2520suitable%2520auxiliary%2520head%2520for%2520a%250Aspecific%2520base%2520model%2520to%2520learn%2520and%2520several%2520simple%2520but%2520effective%2520methods%2520to%250Aenlarge%2520the%2520model%2520discrepancy.%2520The%2520HSSL%2520is%2520compatible%2520with%2520various%250Aself-supervised%2520methods%252C%2520achieving%2520superior%2520performances%2520on%2520various%2520downstream%250Atasks%252C%2520including%2520image%2520classification%252C%2520semantic%2520segmentation%252C%2520instance%250Asegmentation%252C%2520and%2520object%2520detection.%2520The%2520codes%2520are%2520available%2520at%250Ahttps%253A//github.com/NK-JittorCV/Self-Supervised/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.05108v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Representations%20through%20Heterogeneous%20Self-Supervised%20Learning&entry.906535625=Zhong-Yu%20Li%20and%20Bo-Wen%20Yin%20and%20Yongxiang%20Liu%20and%20Li%20Liu%20and%20Ming-Ming%20Cheng&entry.1292438233=%20%20Incorporating%20heterogeneous%20representations%20from%20different%20architectures%20has%0Afacilitated%20various%20vision%20tasks%2C%20e.g.%2C%20some%20hybrid%20networks%20combine%0Atransformers%20and%20convolutions.%20However%2C%20complementarity%20between%20such%0Aheterogeneous%20architectures%20has%20not%20been%20well%20exploited%20in%20self-supervised%0Alearning.%20Thus%2C%20we%20propose%20Heterogeneous%20Self-Supervised%20Learning%20%28HSSL%29%2C%20which%0Aenforces%20a%20base%20model%20to%20learn%20from%20an%20auxiliary%20head%20whose%20architecture%20is%0Aheterogeneous%20from%20the%20base%20model.%20In%20this%20process%2C%20HSSL%20endows%20the%20base%20model%0Awith%20new%20characteristics%20in%20a%20representation%20learning%20way%20without%20structural%0Achanges.%20To%20comprehensively%20understand%20the%20HSSL%2C%20we%20conduct%20experiments%20on%0Avarious%20heterogeneous%20pairs%20containing%20a%20base%20model%20and%20an%20auxiliary%20head.%20We%0Adiscover%20that%20the%20representation%20quality%20of%20the%20base%20model%20moves%20up%20as%20their%0Aarchitecture%20discrepancy%20grows.%20This%20observation%20motivates%20us%20to%20propose%20a%0Asearch%20strategy%20that%20quickly%20determines%20the%20most%20suitable%20auxiliary%20head%20for%20a%0Aspecific%20base%20model%20to%20learn%20and%20several%20simple%20but%20effective%20methods%20to%0Aenlarge%20the%20model%20discrepancy.%20The%20HSSL%20is%20compatible%20with%20various%0Aself-supervised%20methods%2C%20achieving%20superior%20performances%20on%20various%20downstream%0Atasks%2C%20including%20image%20classification%2C%20semantic%20segmentation%2C%20instance%0Asegmentation%2C%20and%20object%20detection.%20The%20codes%20are%20available%20at%0Ahttps%3A//github.com/NK-JittorCV/Self-Supervised/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05108v4&entry.124074799=Read"},
{"title": "Reconstruction of SINR Maps from Sparse Measurements using Group\n  Equivariant Non-Expansive Operators", "author": "Lorenzo Mario Amorosa and Francesco Conti and Nicola Quercioli and Flavio Zabini and Tayebeh Lotfi Mahyari and Yiqun Ge and Patrizio Frosini", "abstract": "  As sixth generation (6G) wireless networks evolve, accurate\nsignal-to-interference-noise ratio (SINR) maps are becoming increasingly\ncritical for effective resource management and optimization. However, acquiring\nsuch maps at high resolution is often cost-prohibitive, creating a severe data\nscarcity challenge. This necessitates machine learning (ML) approaches capable\nof robustly reconstructing the full map from extremely sparse measurements. To\naddress this, we introduce a novel reconstruction framework based on Group\nEquivariant Non-Expansive Operators (GENEOs). Unlike data-hungry ML models,\nGENEOs are low-complexity operators that embed domain-specific geometric\npriors, such as translation invariance, directly into their structure. This\nprovides a strong inductive bias, enabling effective reconstruction from very\nfew samples. Our key insight is that for network management, preserving the\ntopological structure of the SINR map, such as the geometry of coverage holes\nand interference patterns, is often more critical than minimizing pixel-wise\nerror. We validate our approach on realistic ray-tracing-based urban scenarios,\nevaluating performance with both traditional statistical metrics (mean squared\nerror (MSE)) and, crucially, a topological metric (1-Wasserstein distance).\nResults show that while maintaining competitive MSE, our method dramatically\noutperforms established ML baselines in topological fidelity. This demonstrates\nthe practical advantage of GENEOs for creating structurally accurate SINR maps\nthat are more reliable for downstream network optimization tasks.\n", "link": "http://arxiv.org/abs/2507.19349v2", "date": "2025-10-14", "relevancy": 2.6665, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5555}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5437}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconstruction%20of%20SINR%20Maps%20from%20Sparse%20Measurements%20using%20Group%0A%20%20Equivariant%20Non-Expansive%20Operators&body=Title%3A%20Reconstruction%20of%20SINR%20Maps%20from%20Sparse%20Measurements%20using%20Group%0A%20%20Equivariant%20Non-Expansive%20Operators%0AAuthor%3A%20Lorenzo%20Mario%20Amorosa%20and%20Francesco%20Conti%20and%20Nicola%20Quercioli%20and%20Flavio%20Zabini%20and%20Tayebeh%20Lotfi%20Mahyari%20and%20Yiqun%20Ge%20and%20Patrizio%20Frosini%0AAbstract%3A%20%20%20As%20sixth%20generation%20%286G%29%20wireless%20networks%20evolve%2C%20accurate%0Asignal-to-interference-noise%20ratio%20%28SINR%29%20maps%20are%20becoming%20increasingly%0Acritical%20for%20effective%20resource%20management%20and%20optimization.%20However%2C%20acquiring%0Asuch%20maps%20at%20high%20resolution%20is%20often%20cost-prohibitive%2C%20creating%20a%20severe%20data%0Ascarcity%20challenge.%20This%20necessitates%20machine%20learning%20%28ML%29%20approaches%20capable%0Aof%20robustly%20reconstructing%20the%20full%20map%20from%20extremely%20sparse%20measurements.%20To%0Aaddress%20this%2C%20we%20introduce%20a%20novel%20reconstruction%20framework%20based%20on%20Group%0AEquivariant%20Non-Expansive%20Operators%20%28GENEOs%29.%20Unlike%20data-hungry%20ML%20models%2C%0AGENEOs%20are%20low-complexity%20operators%20that%20embed%20domain-specific%20geometric%0Apriors%2C%20such%20as%20translation%20invariance%2C%20directly%20into%20their%20structure.%20This%0Aprovides%20a%20strong%20inductive%20bias%2C%20enabling%20effective%20reconstruction%20from%20very%0Afew%20samples.%20Our%20key%20insight%20is%20that%20for%20network%20management%2C%20preserving%20the%0Atopological%20structure%20of%20the%20SINR%20map%2C%20such%20as%20the%20geometry%20of%20coverage%20holes%0Aand%20interference%20patterns%2C%20is%20often%20more%20critical%20than%20minimizing%20pixel-wise%0Aerror.%20We%20validate%20our%20approach%20on%20realistic%20ray-tracing-based%20urban%20scenarios%2C%0Aevaluating%20performance%20with%20both%20traditional%20statistical%20metrics%20%28mean%20squared%0Aerror%20%28MSE%29%29%20and%2C%20crucially%2C%20a%20topological%20metric%20%281-Wasserstein%20distance%29.%0AResults%20show%20that%20while%20maintaining%20competitive%20MSE%2C%20our%20method%20dramatically%0Aoutperforms%20established%20ML%20baselines%20in%20topological%20fidelity.%20This%20demonstrates%0Athe%20practical%20advantage%20of%20GENEOs%20for%20creating%20structurally%20accurate%20SINR%20maps%0Athat%20are%20more%20reliable%20for%20downstream%20network%20optimization%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19349v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconstruction%2520of%2520SINR%2520Maps%2520from%2520Sparse%2520Measurements%2520using%2520Group%250A%2520%2520Equivariant%2520Non-Expansive%2520Operators%26entry.906535625%3DLorenzo%2520Mario%2520Amorosa%2520and%2520Francesco%2520Conti%2520and%2520Nicola%2520Quercioli%2520and%2520Flavio%2520Zabini%2520and%2520Tayebeh%2520Lotfi%2520Mahyari%2520and%2520Yiqun%2520Ge%2520and%2520Patrizio%2520Frosini%26entry.1292438233%3D%2520%2520As%2520sixth%2520generation%2520%25286G%2529%2520wireless%2520networks%2520evolve%252C%2520accurate%250Asignal-to-interference-noise%2520ratio%2520%2528SINR%2529%2520maps%2520are%2520becoming%2520increasingly%250Acritical%2520for%2520effective%2520resource%2520management%2520and%2520optimization.%2520However%252C%2520acquiring%250Asuch%2520maps%2520at%2520high%2520resolution%2520is%2520often%2520cost-prohibitive%252C%2520creating%2520a%2520severe%2520data%250Ascarcity%2520challenge.%2520This%2520necessitates%2520machine%2520learning%2520%2528ML%2529%2520approaches%2520capable%250Aof%2520robustly%2520reconstructing%2520the%2520full%2520map%2520from%2520extremely%2520sparse%2520measurements.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520a%2520novel%2520reconstruction%2520framework%2520based%2520on%2520Group%250AEquivariant%2520Non-Expansive%2520Operators%2520%2528GENEOs%2529.%2520Unlike%2520data-hungry%2520ML%2520models%252C%250AGENEOs%2520are%2520low-complexity%2520operators%2520that%2520embed%2520domain-specific%2520geometric%250Apriors%252C%2520such%2520as%2520translation%2520invariance%252C%2520directly%2520into%2520their%2520structure.%2520This%250Aprovides%2520a%2520strong%2520inductive%2520bias%252C%2520enabling%2520effective%2520reconstruction%2520from%2520very%250Afew%2520samples.%2520Our%2520key%2520insight%2520is%2520that%2520for%2520network%2520management%252C%2520preserving%2520the%250Atopological%2520structure%2520of%2520the%2520SINR%2520map%252C%2520such%2520as%2520the%2520geometry%2520of%2520coverage%2520holes%250Aand%2520interference%2520patterns%252C%2520is%2520often%2520more%2520critical%2520than%2520minimizing%2520pixel-wise%250Aerror.%2520We%2520validate%2520our%2520approach%2520on%2520realistic%2520ray-tracing-based%2520urban%2520scenarios%252C%250Aevaluating%2520performance%2520with%2520both%2520traditional%2520statistical%2520metrics%2520%2528mean%2520squared%250Aerror%2520%2528MSE%2529%2529%2520and%252C%2520crucially%252C%2520a%2520topological%2520metric%2520%25281-Wasserstein%2520distance%2529.%250AResults%2520show%2520that%2520while%2520maintaining%2520competitive%2520MSE%252C%2520our%2520method%2520dramatically%250Aoutperforms%2520established%2520ML%2520baselines%2520in%2520topological%2520fidelity.%2520This%2520demonstrates%250Athe%2520practical%2520advantage%2520of%2520GENEOs%2520for%2520creating%2520structurally%2520accurate%2520SINR%2520maps%250Athat%2520are%2520more%2520reliable%2520for%2520downstream%2520network%2520optimization%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19349v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconstruction%20of%20SINR%20Maps%20from%20Sparse%20Measurements%20using%20Group%0A%20%20Equivariant%20Non-Expansive%20Operators&entry.906535625=Lorenzo%20Mario%20Amorosa%20and%20Francesco%20Conti%20and%20Nicola%20Quercioli%20and%20Flavio%20Zabini%20and%20Tayebeh%20Lotfi%20Mahyari%20and%20Yiqun%20Ge%20and%20Patrizio%20Frosini&entry.1292438233=%20%20As%20sixth%20generation%20%286G%29%20wireless%20networks%20evolve%2C%20accurate%0Asignal-to-interference-noise%20ratio%20%28SINR%29%20maps%20are%20becoming%20increasingly%0Acritical%20for%20effective%20resource%20management%20and%20optimization.%20However%2C%20acquiring%0Asuch%20maps%20at%20high%20resolution%20is%20often%20cost-prohibitive%2C%20creating%20a%20severe%20data%0Ascarcity%20challenge.%20This%20necessitates%20machine%20learning%20%28ML%29%20approaches%20capable%0Aof%20robustly%20reconstructing%20the%20full%20map%20from%20extremely%20sparse%20measurements.%20To%0Aaddress%20this%2C%20we%20introduce%20a%20novel%20reconstruction%20framework%20based%20on%20Group%0AEquivariant%20Non-Expansive%20Operators%20%28GENEOs%29.%20Unlike%20data-hungry%20ML%20models%2C%0AGENEOs%20are%20low-complexity%20operators%20that%20embed%20domain-specific%20geometric%0Apriors%2C%20such%20as%20translation%20invariance%2C%20directly%20into%20their%20structure.%20This%0Aprovides%20a%20strong%20inductive%20bias%2C%20enabling%20effective%20reconstruction%20from%20very%0Afew%20samples.%20Our%20key%20insight%20is%20that%20for%20network%20management%2C%20preserving%20the%0Atopological%20structure%20of%20the%20SINR%20map%2C%20such%20as%20the%20geometry%20of%20coverage%20holes%0Aand%20interference%20patterns%2C%20is%20often%20more%20critical%20than%20minimizing%20pixel-wise%0Aerror.%20We%20validate%20our%20approach%20on%20realistic%20ray-tracing-based%20urban%20scenarios%2C%0Aevaluating%20performance%20with%20both%20traditional%20statistical%20metrics%20%28mean%20squared%0Aerror%20%28MSE%29%29%20and%2C%20crucially%2C%20a%20topological%20metric%20%281-Wasserstein%20distance%29.%0AResults%20show%20that%20while%20maintaining%20competitive%20MSE%2C%20our%20method%20dramatically%0Aoutperforms%20established%20ML%20baselines%20in%20topological%20fidelity.%20This%20demonstrates%0Athe%20practical%20advantage%20of%20GENEOs%20for%20creating%20structurally%20accurate%20SINR%20maps%0Athat%20are%20more%20reliable%20for%20downstream%20network%20optimization%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19349v2&entry.124074799=Read"},
{"title": "Joint Embedding vs Reconstruction: Provable Benefits of Latent Space\n  Prediction for Self Supervised Learning", "author": "Hugues Van Assel and Mark Ibrahim and Tommaso Biancalani and Aviv Regev and Randall Balestriero", "abstract": "  Reconstruction and joint embedding have emerged as two leading paradigms in\nSelf Supervised Learning (SSL). Reconstruction methods focus on recovering the\noriginal sample from a different view in input space. On the other hand, joint\nembedding methods align the representations of different views in latent space.\nBoth approaches offer compelling advantages, yet practitioners lack clear\nguidelines for choosing between them. In this work, we unveil the core\nmechanisms that distinguish each paradigm. By leveraging closed form solutions\nfor both approaches, we precisely characterize how the view generation process,\ne.g. data augmentation, impacts the learned representations. We then\ndemonstrate that, unlike supervised learning, both SSL paradigms require a\nminimal alignment between augmentations and irrelevant features to achieve\nasymptotic optimality with increasing sample size. Our findings indicate that\nin scenarios where these irrelevant features have a large magnitude, joint\nembedding methods are preferable because they impose a strictly weaker\nalignment condition compared to reconstruction based methods. These results not\nonly clarify the trade offs between the two paradigms but also substantiate the\nempirical success of joint embedding approaches on real world challenging\ndatasets.\n", "link": "http://arxiv.org/abs/2505.12477v2", "date": "2025-10-14", "relevancy": 2.6614, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5365}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5338}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Embedding%20vs%20Reconstruction%3A%20Provable%20Benefits%20of%20Latent%20Space%0A%20%20Prediction%20for%20Self%20Supervised%20Learning&body=Title%3A%20Joint%20Embedding%20vs%20Reconstruction%3A%20Provable%20Benefits%20of%20Latent%20Space%0A%20%20Prediction%20for%20Self%20Supervised%20Learning%0AAuthor%3A%20Hugues%20Van%20Assel%20and%20Mark%20Ibrahim%20and%20Tommaso%20Biancalani%20and%20Aviv%20Regev%20and%20Randall%20Balestriero%0AAbstract%3A%20%20%20Reconstruction%20and%20joint%20embedding%20have%20emerged%20as%20two%20leading%20paradigms%20in%0ASelf%20Supervised%20Learning%20%28SSL%29.%20Reconstruction%20methods%20focus%20on%20recovering%20the%0Aoriginal%20sample%20from%20a%20different%20view%20in%20input%20space.%20On%20the%20other%20hand%2C%20joint%0Aembedding%20methods%20align%20the%20representations%20of%20different%20views%20in%20latent%20space.%0ABoth%20approaches%20offer%20compelling%20advantages%2C%20yet%20practitioners%20lack%20clear%0Aguidelines%20for%20choosing%20between%20them.%20In%20this%20work%2C%20we%20unveil%20the%20core%0Amechanisms%20that%20distinguish%20each%20paradigm.%20By%20leveraging%20closed%20form%20solutions%0Afor%20both%20approaches%2C%20we%20precisely%20characterize%20how%20the%20view%20generation%20process%2C%0Ae.g.%20data%20augmentation%2C%20impacts%20the%20learned%20representations.%20We%20then%0Ademonstrate%20that%2C%20unlike%20supervised%20learning%2C%20both%20SSL%20paradigms%20require%20a%0Aminimal%20alignment%20between%20augmentations%20and%20irrelevant%20features%20to%20achieve%0Aasymptotic%20optimality%20with%20increasing%20sample%20size.%20Our%20findings%20indicate%20that%0Ain%20scenarios%20where%20these%20irrelevant%20features%20have%20a%20large%20magnitude%2C%20joint%0Aembedding%20methods%20are%20preferable%20because%20they%20impose%20a%20strictly%20weaker%0Aalignment%20condition%20compared%20to%20reconstruction%20based%20methods.%20These%20results%20not%0Aonly%20clarify%20the%20trade%20offs%20between%20the%20two%20paradigms%20but%20also%20substantiate%20the%0Aempirical%20success%20of%20joint%20embedding%20approaches%20on%20real%20world%20challenging%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12477v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Embedding%2520vs%2520Reconstruction%253A%2520Provable%2520Benefits%2520of%2520Latent%2520Space%250A%2520%2520Prediction%2520for%2520Self%2520Supervised%2520Learning%26entry.906535625%3DHugues%2520Van%2520Assel%2520and%2520Mark%2520Ibrahim%2520and%2520Tommaso%2520Biancalani%2520and%2520Aviv%2520Regev%2520and%2520Randall%2520Balestriero%26entry.1292438233%3D%2520%2520Reconstruction%2520and%2520joint%2520embedding%2520have%2520emerged%2520as%2520two%2520leading%2520paradigms%2520in%250ASelf%2520Supervised%2520Learning%2520%2528SSL%2529.%2520Reconstruction%2520methods%2520focus%2520on%2520recovering%2520the%250Aoriginal%2520sample%2520from%2520a%2520different%2520view%2520in%2520input%2520space.%2520On%2520the%2520other%2520hand%252C%2520joint%250Aembedding%2520methods%2520align%2520the%2520representations%2520of%2520different%2520views%2520in%2520latent%2520space.%250ABoth%2520approaches%2520offer%2520compelling%2520advantages%252C%2520yet%2520practitioners%2520lack%2520clear%250Aguidelines%2520for%2520choosing%2520between%2520them.%2520In%2520this%2520work%252C%2520we%2520unveil%2520the%2520core%250Amechanisms%2520that%2520distinguish%2520each%2520paradigm.%2520By%2520leveraging%2520closed%2520form%2520solutions%250Afor%2520both%2520approaches%252C%2520we%2520precisely%2520characterize%2520how%2520the%2520view%2520generation%2520process%252C%250Ae.g.%2520data%2520augmentation%252C%2520impacts%2520the%2520learned%2520representations.%2520We%2520then%250Ademonstrate%2520that%252C%2520unlike%2520supervised%2520learning%252C%2520both%2520SSL%2520paradigms%2520require%2520a%250Aminimal%2520alignment%2520between%2520augmentations%2520and%2520irrelevant%2520features%2520to%2520achieve%250Aasymptotic%2520optimality%2520with%2520increasing%2520sample%2520size.%2520Our%2520findings%2520indicate%2520that%250Ain%2520scenarios%2520where%2520these%2520irrelevant%2520features%2520have%2520a%2520large%2520magnitude%252C%2520joint%250Aembedding%2520methods%2520are%2520preferable%2520because%2520they%2520impose%2520a%2520strictly%2520weaker%250Aalignment%2520condition%2520compared%2520to%2520reconstruction%2520based%2520methods.%2520These%2520results%2520not%250Aonly%2520clarify%2520the%2520trade%2520offs%2520between%2520the%2520two%2520paradigms%2520but%2520also%2520substantiate%2520the%250Aempirical%2520success%2520of%2520joint%2520embedding%2520approaches%2520on%2520real%2520world%2520challenging%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12477v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Embedding%20vs%20Reconstruction%3A%20Provable%20Benefits%20of%20Latent%20Space%0A%20%20Prediction%20for%20Self%20Supervised%20Learning&entry.906535625=Hugues%20Van%20Assel%20and%20Mark%20Ibrahim%20and%20Tommaso%20Biancalani%20and%20Aviv%20Regev%20and%20Randall%20Balestriero&entry.1292438233=%20%20Reconstruction%20and%20joint%20embedding%20have%20emerged%20as%20two%20leading%20paradigms%20in%0ASelf%20Supervised%20Learning%20%28SSL%29.%20Reconstruction%20methods%20focus%20on%20recovering%20the%0Aoriginal%20sample%20from%20a%20different%20view%20in%20input%20space.%20On%20the%20other%20hand%2C%20joint%0Aembedding%20methods%20align%20the%20representations%20of%20different%20views%20in%20latent%20space.%0ABoth%20approaches%20offer%20compelling%20advantages%2C%20yet%20practitioners%20lack%20clear%0Aguidelines%20for%20choosing%20between%20them.%20In%20this%20work%2C%20we%20unveil%20the%20core%0Amechanisms%20that%20distinguish%20each%20paradigm.%20By%20leveraging%20closed%20form%20solutions%0Afor%20both%20approaches%2C%20we%20precisely%20characterize%20how%20the%20view%20generation%20process%2C%0Ae.g.%20data%20augmentation%2C%20impacts%20the%20learned%20representations.%20We%20then%0Ademonstrate%20that%2C%20unlike%20supervised%20learning%2C%20both%20SSL%20paradigms%20require%20a%0Aminimal%20alignment%20between%20augmentations%20and%20irrelevant%20features%20to%20achieve%0Aasymptotic%20optimality%20with%20increasing%20sample%20size.%20Our%20findings%20indicate%20that%0Ain%20scenarios%20where%20these%20irrelevant%20features%20have%20a%20large%20magnitude%2C%20joint%0Aembedding%20methods%20are%20preferable%20because%20they%20impose%20a%20strictly%20weaker%0Aalignment%20condition%20compared%20to%20reconstruction%20based%20methods.%20These%20results%20not%0Aonly%20clarify%20the%20trade%20offs%20between%20the%20two%20paradigms%20but%20also%20substantiate%20the%0Aempirical%20success%20of%20joint%20embedding%20approaches%20on%20real%20world%20challenging%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12477v2&entry.124074799=Read"},
{"title": "Few Shot Semi-Supervised Learning for Abnormal Stop Detection from\n  Sparse GPS Trajectories", "author": "Muhammad Ayub Sabir and Junbiao Pang and Jiaqi Wu and Fatima Ashraf", "abstract": "  Abnormal stop detection (ASD) in intercity coach transportation is critical\nfor ensuring passenger safety, operational reliability, and regulatory\ncompliance. However, two key challenges hinder ASD effectiveness: sparse GPS\ntrajectories, which obscure short or unauthorized stops, and limited labeled\ndata, which restricts supervised learning. Existing methods often assume dense\nsampling or regular movement patterns, limiting their applicability. To address\ndata sparsity, we propose a Sparsity-Aware Segmentation (SAS) method that\nadaptively defines segment boundaries based on local spatial-temporal density.\nBuilding upon these segments, we introduce three domain-specific indicators to\ncapture abnormal stop behaviors. To further mitigate the impact of sparsity, we\ndevelop Locally Temporal-Indicator Guided Adjustment (LTIGA), which smooths\nthese indicators via local similarity graphs. To overcome label scarcity, we\nconstruct a spatial-temporal graph where each segment is a node with\nLTIGA-refined features. We apply label propagation to expand weak supervision\nacross the graph, followed by a GCN to learn relational patterns. A final\nself-training module incorporates high-confidence pseudo-labels to iteratively\nimprove predictions. Experiments on real-world coach data show an AUC of 0.854\nand AP of 0.866 using only 10 labeled instances, outperforming prior methods.\nThe code and dataset are publicly available at\n\\href{https://github.com/pangjunbiao/Abnormal-Stop-Detection-SSL.git}\n", "link": "http://arxiv.org/abs/2510.12686v1", "date": "2025-10-14", "relevancy": 2.6479, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5787}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5079}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few%20Shot%20Semi-Supervised%20Learning%20for%20Abnormal%20Stop%20Detection%20from%0A%20%20Sparse%20GPS%20Trajectories&body=Title%3A%20Few%20Shot%20Semi-Supervised%20Learning%20for%20Abnormal%20Stop%20Detection%20from%0A%20%20Sparse%20GPS%20Trajectories%0AAuthor%3A%20Muhammad%20Ayub%20Sabir%20and%20Junbiao%20Pang%20and%20Jiaqi%20Wu%20and%20Fatima%20Ashraf%0AAbstract%3A%20%20%20Abnormal%20stop%20detection%20%28ASD%29%20in%20intercity%20coach%20transportation%20is%20critical%0Afor%20ensuring%20passenger%20safety%2C%20operational%20reliability%2C%20and%20regulatory%0Acompliance.%20However%2C%20two%20key%20challenges%20hinder%20ASD%20effectiveness%3A%20sparse%20GPS%0Atrajectories%2C%20which%20obscure%20short%20or%20unauthorized%20stops%2C%20and%20limited%20labeled%0Adata%2C%20which%20restricts%20supervised%20learning.%20Existing%20methods%20often%20assume%20dense%0Asampling%20or%20regular%20movement%20patterns%2C%20limiting%20their%20applicability.%20To%20address%0Adata%20sparsity%2C%20we%20propose%20a%20Sparsity-Aware%20Segmentation%20%28SAS%29%20method%20that%0Aadaptively%20defines%20segment%20boundaries%20based%20on%20local%20spatial-temporal%20density.%0ABuilding%20upon%20these%20segments%2C%20we%20introduce%20three%20domain-specific%20indicators%20to%0Acapture%20abnormal%20stop%20behaviors.%20To%20further%20mitigate%20the%20impact%20of%20sparsity%2C%20we%0Adevelop%20Locally%20Temporal-Indicator%20Guided%20Adjustment%20%28LTIGA%29%2C%20which%20smooths%0Athese%20indicators%20via%20local%20similarity%20graphs.%20To%20overcome%20label%20scarcity%2C%20we%0Aconstruct%20a%20spatial-temporal%20graph%20where%20each%20segment%20is%20a%20node%20with%0ALTIGA-refined%20features.%20We%20apply%20label%20propagation%20to%20expand%20weak%20supervision%0Aacross%20the%20graph%2C%20followed%20by%20a%20GCN%20to%20learn%20relational%20patterns.%20A%20final%0Aself-training%20module%20incorporates%20high-confidence%20pseudo-labels%20to%20iteratively%0Aimprove%20predictions.%20Experiments%20on%20real-world%20coach%20data%20show%20an%20AUC%20of%200.854%0Aand%20AP%20of%200.866%20using%20only%2010%20labeled%20instances%2C%20outperforming%20prior%20methods.%0AThe%20code%20and%20dataset%20are%20publicly%20available%20at%0A%5Chref%7Bhttps%3A//github.com/pangjunbiao/Abnormal-Stop-Detection-SSL.git%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew%2520Shot%2520Semi-Supervised%2520Learning%2520for%2520Abnormal%2520Stop%2520Detection%2520from%250A%2520%2520Sparse%2520GPS%2520Trajectories%26entry.906535625%3DMuhammad%2520Ayub%2520Sabir%2520and%2520Junbiao%2520Pang%2520and%2520Jiaqi%2520Wu%2520and%2520Fatima%2520Ashraf%26entry.1292438233%3D%2520%2520Abnormal%2520stop%2520detection%2520%2528ASD%2529%2520in%2520intercity%2520coach%2520transportation%2520is%2520critical%250Afor%2520ensuring%2520passenger%2520safety%252C%2520operational%2520reliability%252C%2520and%2520regulatory%250Acompliance.%2520However%252C%2520two%2520key%2520challenges%2520hinder%2520ASD%2520effectiveness%253A%2520sparse%2520GPS%250Atrajectories%252C%2520which%2520obscure%2520short%2520or%2520unauthorized%2520stops%252C%2520and%2520limited%2520labeled%250Adata%252C%2520which%2520restricts%2520supervised%2520learning.%2520Existing%2520methods%2520often%2520assume%2520dense%250Asampling%2520or%2520regular%2520movement%2520patterns%252C%2520limiting%2520their%2520applicability.%2520To%2520address%250Adata%2520sparsity%252C%2520we%2520propose%2520a%2520Sparsity-Aware%2520Segmentation%2520%2528SAS%2529%2520method%2520that%250Aadaptively%2520defines%2520segment%2520boundaries%2520based%2520on%2520local%2520spatial-temporal%2520density.%250ABuilding%2520upon%2520these%2520segments%252C%2520we%2520introduce%2520three%2520domain-specific%2520indicators%2520to%250Acapture%2520abnormal%2520stop%2520behaviors.%2520To%2520further%2520mitigate%2520the%2520impact%2520of%2520sparsity%252C%2520we%250Adevelop%2520Locally%2520Temporal-Indicator%2520Guided%2520Adjustment%2520%2528LTIGA%2529%252C%2520which%2520smooths%250Athese%2520indicators%2520via%2520local%2520similarity%2520graphs.%2520To%2520overcome%2520label%2520scarcity%252C%2520we%250Aconstruct%2520a%2520spatial-temporal%2520graph%2520where%2520each%2520segment%2520is%2520a%2520node%2520with%250ALTIGA-refined%2520features.%2520We%2520apply%2520label%2520propagation%2520to%2520expand%2520weak%2520supervision%250Aacross%2520the%2520graph%252C%2520followed%2520by%2520a%2520GCN%2520to%2520learn%2520relational%2520patterns.%2520A%2520final%250Aself-training%2520module%2520incorporates%2520high-confidence%2520pseudo-labels%2520to%2520iteratively%250Aimprove%2520predictions.%2520Experiments%2520on%2520real-world%2520coach%2520data%2520show%2520an%2520AUC%2520of%25200.854%250Aand%2520AP%2520of%25200.866%2520using%2520only%252010%2520labeled%2520instances%252C%2520outperforming%2520prior%2520methods.%250AThe%2520code%2520and%2520dataset%2520are%2520publicly%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/pangjunbiao/Abnormal-Stop-Detection-SSL.git%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few%20Shot%20Semi-Supervised%20Learning%20for%20Abnormal%20Stop%20Detection%20from%0A%20%20Sparse%20GPS%20Trajectories&entry.906535625=Muhammad%20Ayub%20Sabir%20and%20Junbiao%20Pang%20and%20Jiaqi%20Wu%20and%20Fatima%20Ashraf&entry.1292438233=%20%20Abnormal%20stop%20detection%20%28ASD%29%20in%20intercity%20coach%20transportation%20is%20critical%0Afor%20ensuring%20passenger%20safety%2C%20operational%20reliability%2C%20and%20regulatory%0Acompliance.%20However%2C%20two%20key%20challenges%20hinder%20ASD%20effectiveness%3A%20sparse%20GPS%0Atrajectories%2C%20which%20obscure%20short%20or%20unauthorized%20stops%2C%20and%20limited%20labeled%0Adata%2C%20which%20restricts%20supervised%20learning.%20Existing%20methods%20often%20assume%20dense%0Asampling%20or%20regular%20movement%20patterns%2C%20limiting%20their%20applicability.%20To%20address%0Adata%20sparsity%2C%20we%20propose%20a%20Sparsity-Aware%20Segmentation%20%28SAS%29%20method%20that%0Aadaptively%20defines%20segment%20boundaries%20based%20on%20local%20spatial-temporal%20density.%0ABuilding%20upon%20these%20segments%2C%20we%20introduce%20three%20domain-specific%20indicators%20to%0Acapture%20abnormal%20stop%20behaviors.%20To%20further%20mitigate%20the%20impact%20of%20sparsity%2C%20we%0Adevelop%20Locally%20Temporal-Indicator%20Guided%20Adjustment%20%28LTIGA%29%2C%20which%20smooths%0Athese%20indicators%20via%20local%20similarity%20graphs.%20To%20overcome%20label%20scarcity%2C%20we%0Aconstruct%20a%20spatial-temporal%20graph%20where%20each%20segment%20is%20a%20node%20with%0ALTIGA-refined%20features.%20We%20apply%20label%20propagation%20to%20expand%20weak%20supervision%0Aacross%20the%20graph%2C%20followed%20by%20a%20GCN%20to%20learn%20relational%20patterns.%20A%20final%0Aself-training%20module%20incorporates%20high-confidence%20pseudo-labels%20to%20iteratively%0Aimprove%20predictions.%20Experiments%20on%20real-world%20coach%20data%20show%20an%20AUC%20of%200.854%0Aand%20AP%20of%200.866%20using%20only%2010%20labeled%20instances%2C%20outperforming%20prior%20methods.%0AThe%20code%20and%20dataset%20are%20publicly%20available%20at%0A%5Chref%7Bhttps%3A//github.com/pangjunbiao/Abnormal-Stop-Detection-SSL.git%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12686v1&entry.124074799=Read"},
{"title": "Personalized Federated Fine-Tuning of Vision Foundation Models for\n  Healthcare", "author": "Adam Tupper and Christian Gagn\u00e9", "abstract": "  Foundation models open up new possibilities for the use of AI in healthcare.\nHowever, even when pre-trained on health data, they still need to be fine-tuned\nfor specific downstream tasks. Furthermore, although foundation models reduce\nthe amount of training data required to achieve good performance, obtaining\nsufficient data is still a challenge. This is due, in part, to restrictions on\nsharing and aggregating data from different sources to protect patients'\nprivacy. One possible solution to this is to fine-tune foundation models via\nfederated learning across multiple participating clients (i.e., hospitals,\nclinics, etc.). In this work, we propose a new personalized federated\nfine-tuning method that learns orthogonal LoRA adapters to disentangle general\nand client-specific knowledge, enabling each client to fully exploit both their\nown data and the data of others. Our preliminary results on real-world\nfederated medical imaging tasks demonstrate that our approach is competitive\nagainst current federated fine-tuning methods.\n", "link": "http://arxiv.org/abs/2510.12741v1", "date": "2025-10-14", "relevancy": 2.6477, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5305}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5305}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Federated%20Fine-Tuning%20of%20Vision%20Foundation%20Models%20for%0A%20%20Healthcare&body=Title%3A%20Personalized%20Federated%20Fine-Tuning%20of%20Vision%20Foundation%20Models%20for%0A%20%20Healthcare%0AAuthor%3A%20Adam%20Tupper%20and%20Christian%20Gagn%C3%A9%0AAbstract%3A%20%20%20Foundation%20models%20open%20up%20new%20possibilities%20for%20the%20use%20of%20AI%20in%20healthcare.%0AHowever%2C%20even%20when%20pre-trained%20on%20health%20data%2C%20they%20still%20need%20to%20be%20fine-tuned%0Afor%20specific%20downstream%20tasks.%20Furthermore%2C%20although%20foundation%20models%20reduce%0Athe%20amount%20of%20training%20data%20required%20to%20achieve%20good%20performance%2C%20obtaining%0Asufficient%20data%20is%20still%20a%20challenge.%20This%20is%20due%2C%20in%20part%2C%20to%20restrictions%20on%0Asharing%20and%20aggregating%20data%20from%20different%20sources%20to%20protect%20patients%27%0Aprivacy.%20One%20possible%20solution%20to%20this%20is%20to%20fine-tune%20foundation%20models%20via%0Afederated%20learning%20across%20multiple%20participating%20clients%20%28i.e.%2C%20hospitals%2C%0Aclinics%2C%20etc.%29.%20In%20this%20work%2C%20we%20propose%20a%20new%20personalized%20federated%0Afine-tuning%20method%20that%20learns%20orthogonal%20LoRA%20adapters%20to%20disentangle%20general%0Aand%20client-specific%20knowledge%2C%20enabling%20each%20client%20to%20fully%20exploit%20both%20their%0Aown%20data%20and%20the%20data%20of%20others.%20Our%20preliminary%20results%20on%20real-world%0Afederated%20medical%20imaging%20tasks%20demonstrate%20that%20our%20approach%20is%20competitive%0Aagainst%20current%20federated%20fine-tuning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12741v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Federated%2520Fine-Tuning%2520of%2520Vision%2520Foundation%2520Models%2520for%250A%2520%2520Healthcare%26entry.906535625%3DAdam%2520Tupper%2520and%2520Christian%2520Gagn%25C3%25A9%26entry.1292438233%3D%2520%2520Foundation%2520models%2520open%2520up%2520new%2520possibilities%2520for%2520the%2520use%2520of%2520AI%2520in%2520healthcare.%250AHowever%252C%2520even%2520when%2520pre-trained%2520on%2520health%2520data%252C%2520they%2520still%2520need%2520to%2520be%2520fine-tuned%250Afor%2520specific%2520downstream%2520tasks.%2520Furthermore%252C%2520although%2520foundation%2520models%2520reduce%250Athe%2520amount%2520of%2520training%2520data%2520required%2520to%2520achieve%2520good%2520performance%252C%2520obtaining%250Asufficient%2520data%2520is%2520still%2520a%2520challenge.%2520This%2520is%2520due%252C%2520in%2520part%252C%2520to%2520restrictions%2520on%250Asharing%2520and%2520aggregating%2520data%2520from%2520different%2520sources%2520to%2520protect%2520patients%2527%250Aprivacy.%2520One%2520possible%2520solution%2520to%2520this%2520is%2520to%2520fine-tune%2520foundation%2520models%2520via%250Afederated%2520learning%2520across%2520multiple%2520participating%2520clients%2520%2528i.e.%252C%2520hospitals%252C%250Aclinics%252C%2520etc.%2529.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520personalized%2520federated%250Afine-tuning%2520method%2520that%2520learns%2520orthogonal%2520LoRA%2520adapters%2520to%2520disentangle%2520general%250Aand%2520client-specific%2520knowledge%252C%2520enabling%2520each%2520client%2520to%2520fully%2520exploit%2520both%2520their%250Aown%2520data%2520and%2520the%2520data%2520of%2520others.%2520Our%2520preliminary%2520results%2520on%2520real-world%250Afederated%2520medical%2520imaging%2520tasks%2520demonstrate%2520that%2520our%2520approach%2520is%2520competitive%250Aagainst%2520current%2520federated%2520fine-tuning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12741v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Federated%20Fine-Tuning%20of%20Vision%20Foundation%20Models%20for%0A%20%20Healthcare&entry.906535625=Adam%20Tupper%20and%20Christian%20Gagn%C3%A9&entry.1292438233=%20%20Foundation%20models%20open%20up%20new%20possibilities%20for%20the%20use%20of%20AI%20in%20healthcare.%0AHowever%2C%20even%20when%20pre-trained%20on%20health%20data%2C%20they%20still%20need%20to%20be%20fine-tuned%0Afor%20specific%20downstream%20tasks.%20Furthermore%2C%20although%20foundation%20models%20reduce%0Athe%20amount%20of%20training%20data%20required%20to%20achieve%20good%20performance%2C%20obtaining%0Asufficient%20data%20is%20still%20a%20challenge.%20This%20is%20due%2C%20in%20part%2C%20to%20restrictions%20on%0Asharing%20and%20aggregating%20data%20from%20different%20sources%20to%20protect%20patients%27%0Aprivacy.%20One%20possible%20solution%20to%20this%20is%20to%20fine-tune%20foundation%20models%20via%0Afederated%20learning%20across%20multiple%20participating%20clients%20%28i.e.%2C%20hospitals%2C%0Aclinics%2C%20etc.%29.%20In%20this%20work%2C%20we%20propose%20a%20new%20personalized%20federated%0Afine-tuning%20method%20that%20learns%20orthogonal%20LoRA%20adapters%20to%20disentangle%20general%0Aand%20client-specific%20knowledge%2C%20enabling%20each%20client%20to%20fully%20exploit%20both%20their%0Aown%20data%20and%20the%20data%20of%20others.%20Our%20preliminary%20results%20on%20real-world%0Afederated%20medical%20imaging%20tasks%20demonstrate%20that%20our%20approach%20is%20competitive%0Aagainst%20current%20federated%20fine-tuning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12741v1&entry.124074799=Read"},
{"title": "CuMPerLay: Learning Cubical Multiparameter Persistence Vectorizations", "author": "Caner Korkmaz and Brighton Nuwagira and Bar\u0131\u015f Co\u015fkunuzer and Tolga Birdal", "abstract": "  We present CuMPerLay, a novel differentiable vectorization layer that enables\nthe integration of Cubical Multiparameter Persistence (CMP) into deep learning\npipelines. While CMP presents a natural and powerful way to topologically work\nwith images, its use is hindered by the complexity of multifiltration\nstructures as well as the vectorization of CMP. In face of these challenges, we\nintroduce a new algorithm for vectorizing MP homologies of cubical complexes.\nOur CuMPerLay decomposes the CMP into a combination of individual, learnable\nsingle-parameter persistence, where the bifiltration functions are jointly\nlearned. Thanks to the differentiability, its robust topological feature\nvectors can be seamlessly used within state-of-the-art architectures such as\nSwin Transformers. We establish theoretical guarantees for the stability of our\nvectorization under generalized Wasserstein metrics. Our experiments on\nbenchmark medical imaging and computer vision datasets show the benefit\nCuMPerLay on classification and segmentation performance, particularly in\nlimited-data scenarios. Overall, CuMPerLay offers a promising direction for\nintegrating global structural information into deep networks for structured\nimage analysis.\n", "link": "http://arxiv.org/abs/2510.12795v1", "date": "2025-10-14", "relevancy": 2.6298, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5458}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5333}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CuMPerLay%3A%20Learning%20Cubical%20Multiparameter%20Persistence%20Vectorizations&body=Title%3A%20CuMPerLay%3A%20Learning%20Cubical%20Multiparameter%20Persistence%20Vectorizations%0AAuthor%3A%20Caner%20Korkmaz%20and%20Brighton%20Nuwagira%20and%20Bar%C4%B1%C5%9F%20Co%C5%9Fkunuzer%20and%20Tolga%20Birdal%0AAbstract%3A%20%20%20We%20present%20CuMPerLay%2C%20a%20novel%20differentiable%20vectorization%20layer%20that%20enables%0Athe%20integration%20of%20Cubical%20Multiparameter%20Persistence%20%28CMP%29%20into%20deep%20learning%0Apipelines.%20While%20CMP%20presents%20a%20natural%20and%20powerful%20way%20to%20topologically%20work%0Awith%20images%2C%20its%20use%20is%20hindered%20by%20the%20complexity%20of%20multifiltration%0Astructures%20as%20well%20as%20the%20vectorization%20of%20CMP.%20In%20face%20of%20these%20challenges%2C%20we%0Aintroduce%20a%20new%20algorithm%20for%20vectorizing%20MP%20homologies%20of%20cubical%20complexes.%0AOur%20CuMPerLay%20decomposes%20the%20CMP%20into%20a%20combination%20of%20individual%2C%20learnable%0Asingle-parameter%20persistence%2C%20where%20the%20bifiltration%20functions%20are%20jointly%0Alearned.%20Thanks%20to%20the%20differentiability%2C%20its%20robust%20topological%20feature%0Avectors%20can%20be%20seamlessly%20used%20within%20state-of-the-art%20architectures%20such%20as%0ASwin%20Transformers.%20We%20establish%20theoretical%20guarantees%20for%20the%20stability%20of%20our%0Avectorization%20under%20generalized%20Wasserstein%20metrics.%20Our%20experiments%20on%0Abenchmark%20medical%20imaging%20and%20computer%20vision%20datasets%20show%20the%20benefit%0ACuMPerLay%20on%20classification%20and%20segmentation%20performance%2C%20particularly%20in%0Alimited-data%20scenarios.%20Overall%2C%20CuMPerLay%20offers%20a%20promising%20direction%20for%0Aintegrating%20global%20structural%20information%20into%20deep%20networks%20for%20structured%0Aimage%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCuMPerLay%253A%2520Learning%2520Cubical%2520Multiparameter%2520Persistence%2520Vectorizations%26entry.906535625%3DCaner%2520Korkmaz%2520and%2520Brighton%2520Nuwagira%2520and%2520Bar%25C4%25B1%25C5%259F%2520Co%25C5%259Fkunuzer%2520and%2520Tolga%2520Birdal%26entry.1292438233%3D%2520%2520We%2520present%2520CuMPerLay%252C%2520a%2520novel%2520differentiable%2520vectorization%2520layer%2520that%2520enables%250Athe%2520integration%2520of%2520Cubical%2520Multiparameter%2520Persistence%2520%2528CMP%2529%2520into%2520deep%2520learning%250Apipelines.%2520While%2520CMP%2520presents%2520a%2520natural%2520and%2520powerful%2520way%2520to%2520topologically%2520work%250Awith%2520images%252C%2520its%2520use%2520is%2520hindered%2520by%2520the%2520complexity%2520of%2520multifiltration%250Astructures%2520as%2520well%2520as%2520the%2520vectorization%2520of%2520CMP.%2520In%2520face%2520of%2520these%2520challenges%252C%2520we%250Aintroduce%2520a%2520new%2520algorithm%2520for%2520vectorizing%2520MP%2520homologies%2520of%2520cubical%2520complexes.%250AOur%2520CuMPerLay%2520decomposes%2520the%2520CMP%2520into%2520a%2520combination%2520of%2520individual%252C%2520learnable%250Asingle-parameter%2520persistence%252C%2520where%2520the%2520bifiltration%2520functions%2520are%2520jointly%250Alearned.%2520Thanks%2520to%2520the%2520differentiability%252C%2520its%2520robust%2520topological%2520feature%250Avectors%2520can%2520be%2520seamlessly%2520used%2520within%2520state-of-the-art%2520architectures%2520such%2520as%250ASwin%2520Transformers.%2520We%2520establish%2520theoretical%2520guarantees%2520for%2520the%2520stability%2520of%2520our%250Avectorization%2520under%2520generalized%2520Wasserstein%2520metrics.%2520Our%2520experiments%2520on%250Abenchmark%2520medical%2520imaging%2520and%2520computer%2520vision%2520datasets%2520show%2520the%2520benefit%250ACuMPerLay%2520on%2520classification%2520and%2520segmentation%2520performance%252C%2520particularly%2520in%250Alimited-data%2520scenarios.%2520Overall%252C%2520CuMPerLay%2520offers%2520a%2520promising%2520direction%2520for%250Aintegrating%2520global%2520structural%2520information%2520into%2520deep%2520networks%2520for%2520structured%250Aimage%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CuMPerLay%3A%20Learning%20Cubical%20Multiparameter%20Persistence%20Vectorizations&entry.906535625=Caner%20Korkmaz%20and%20Brighton%20Nuwagira%20and%20Bar%C4%B1%C5%9F%20Co%C5%9Fkunuzer%20and%20Tolga%20Birdal&entry.1292438233=%20%20We%20present%20CuMPerLay%2C%20a%20novel%20differentiable%20vectorization%20layer%20that%20enables%0Athe%20integration%20of%20Cubical%20Multiparameter%20Persistence%20%28CMP%29%20into%20deep%20learning%0Apipelines.%20While%20CMP%20presents%20a%20natural%20and%20powerful%20way%20to%20topologically%20work%0Awith%20images%2C%20its%20use%20is%20hindered%20by%20the%20complexity%20of%20multifiltration%0Astructures%20as%20well%20as%20the%20vectorization%20of%20CMP.%20In%20face%20of%20these%20challenges%2C%20we%0Aintroduce%20a%20new%20algorithm%20for%20vectorizing%20MP%20homologies%20of%20cubical%20complexes.%0AOur%20CuMPerLay%20decomposes%20the%20CMP%20into%20a%20combination%20of%20individual%2C%20learnable%0Asingle-parameter%20persistence%2C%20where%20the%20bifiltration%20functions%20are%20jointly%0Alearned.%20Thanks%20to%20the%20differentiability%2C%20its%20robust%20topological%20feature%0Avectors%20can%20be%20seamlessly%20used%20within%20state-of-the-art%20architectures%20such%20as%0ASwin%20Transformers.%20We%20establish%20theoretical%20guarantees%20for%20the%20stability%20of%20our%0Avectorization%20under%20generalized%20Wasserstein%20metrics.%20Our%20experiments%20on%0Abenchmark%20medical%20imaging%20and%20computer%20vision%20datasets%20show%20the%20benefit%0ACuMPerLay%20on%20classification%20and%20segmentation%20performance%2C%20particularly%20in%0Alimited-data%20scenarios.%20Overall%2C%20CuMPerLay%20offers%20a%20promising%20direction%20for%0Aintegrating%20global%20structural%20information%20into%20deep%20networks%20for%20structured%0Aimage%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12795v1&entry.124074799=Read"},
{"title": "Protein Design with Dynamic Protein Vocabulary", "author": "Nuowei Liu and Jiahao Kuang and Yanting Liu and Tao Ji and Changzhi Sun and Man Lan and Yuanbin Wu", "abstract": "  Protein design is a fundamental challenge in biotechnology, aiming to design\nnovel sequences with specific functions within the vast space of possible\nproteins. Recent advances in deep generative models have enabled function-based\nprotein design from textual descriptions, yet struggle with structural\nplausibility. Inspired by classical protein design methods that leverage\nnatural protein structures, we explore whether incorporating fragments from\nnatural proteins can enhance foldability in generative models. Our empirical\nresults show that even random incorporation of fragments improves foldability.\nBuilding on this insight, we introduce ProDVa, a novel protein design approach\nthat integrates a text encoder for functional descriptions, a protein language\nmodel for designing proteins, and a fragment encoder to dynamically retrieve\nprotein fragments based on textual functional descriptions. Experimental\nresults demonstrate that our approach effectively designs protein sequences\nthat are both functionally aligned and structurally plausible. Compared to\nstate-of-the-art models, ProDVa achieves comparable function alignment using\nless than 0.04% of the training data, while designing significantly more\nwell-folded proteins, with the proportion of proteins having pLDDT above 70\nincreasing by 7.38% and those with PAE below 10 increasing by 9.6%.\n", "link": "http://arxiv.org/abs/2505.18966v2", "date": "2025-10-14", "relevancy": 2.6295, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5417}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5324}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Protein%20Design%20with%20Dynamic%20Protein%20Vocabulary&body=Title%3A%20Protein%20Design%20with%20Dynamic%20Protein%20Vocabulary%0AAuthor%3A%20Nuowei%20Liu%20and%20Jiahao%20Kuang%20and%20Yanting%20Liu%20and%20Tao%20Ji%20and%20Changzhi%20Sun%20and%20Man%20Lan%20and%20Yuanbin%20Wu%0AAbstract%3A%20%20%20Protein%20design%20is%20a%20fundamental%20challenge%20in%20biotechnology%2C%20aiming%20to%20design%0Anovel%20sequences%20with%20specific%20functions%20within%20the%20vast%20space%20of%20possible%0Aproteins.%20Recent%20advances%20in%20deep%20generative%20models%20have%20enabled%20function-based%0Aprotein%20design%20from%20textual%20descriptions%2C%20yet%20struggle%20with%20structural%0Aplausibility.%20Inspired%20by%20classical%20protein%20design%20methods%20that%20leverage%0Anatural%20protein%20structures%2C%20we%20explore%20whether%20incorporating%20fragments%20from%0Anatural%20proteins%20can%20enhance%20foldability%20in%20generative%20models.%20Our%20empirical%0Aresults%20show%20that%20even%20random%20incorporation%20of%20fragments%20improves%20foldability.%0ABuilding%20on%20this%20insight%2C%20we%20introduce%20ProDVa%2C%20a%20novel%20protein%20design%20approach%0Athat%20integrates%20a%20text%20encoder%20for%20functional%20descriptions%2C%20a%20protein%20language%0Amodel%20for%20designing%20proteins%2C%20and%20a%20fragment%20encoder%20to%20dynamically%20retrieve%0Aprotein%20fragments%20based%20on%20textual%20functional%20descriptions.%20Experimental%0Aresults%20demonstrate%20that%20our%20approach%20effectively%20designs%20protein%20sequences%0Athat%20are%20both%20functionally%20aligned%20and%20structurally%20plausible.%20Compared%20to%0Astate-of-the-art%20models%2C%20ProDVa%20achieves%20comparable%20function%20alignment%20using%0Aless%20than%200.04%25%20of%20the%20training%20data%2C%20while%20designing%20significantly%20more%0Awell-folded%20proteins%2C%20with%20the%20proportion%20of%20proteins%20having%20pLDDT%20above%2070%0Aincreasing%20by%207.38%25%20and%20those%20with%20PAE%20below%2010%20increasing%20by%209.6%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18966v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProtein%2520Design%2520with%2520Dynamic%2520Protein%2520Vocabulary%26entry.906535625%3DNuowei%2520Liu%2520and%2520Jiahao%2520Kuang%2520and%2520Yanting%2520Liu%2520and%2520Tao%2520Ji%2520and%2520Changzhi%2520Sun%2520and%2520Man%2520Lan%2520and%2520Yuanbin%2520Wu%26entry.1292438233%3D%2520%2520Protein%2520design%2520is%2520a%2520fundamental%2520challenge%2520in%2520biotechnology%252C%2520aiming%2520to%2520design%250Anovel%2520sequences%2520with%2520specific%2520functions%2520within%2520the%2520vast%2520space%2520of%2520possible%250Aproteins.%2520Recent%2520advances%2520in%2520deep%2520generative%2520models%2520have%2520enabled%2520function-based%250Aprotein%2520design%2520from%2520textual%2520descriptions%252C%2520yet%2520struggle%2520with%2520structural%250Aplausibility.%2520Inspired%2520by%2520classical%2520protein%2520design%2520methods%2520that%2520leverage%250Anatural%2520protein%2520structures%252C%2520we%2520explore%2520whether%2520incorporating%2520fragments%2520from%250Anatural%2520proteins%2520can%2520enhance%2520foldability%2520in%2520generative%2520models.%2520Our%2520empirical%250Aresults%2520show%2520that%2520even%2520random%2520incorporation%2520of%2520fragments%2520improves%2520foldability.%250ABuilding%2520on%2520this%2520insight%252C%2520we%2520introduce%2520ProDVa%252C%2520a%2520novel%2520protein%2520design%2520approach%250Athat%2520integrates%2520a%2520text%2520encoder%2520for%2520functional%2520descriptions%252C%2520a%2520protein%2520language%250Amodel%2520for%2520designing%2520proteins%252C%2520and%2520a%2520fragment%2520encoder%2520to%2520dynamically%2520retrieve%250Aprotein%2520fragments%2520based%2520on%2520textual%2520functional%2520descriptions.%2520Experimental%250Aresults%2520demonstrate%2520that%2520our%2520approach%2520effectively%2520designs%2520protein%2520sequences%250Athat%2520are%2520both%2520functionally%2520aligned%2520and%2520structurally%2520plausible.%2520Compared%2520to%250Astate-of-the-art%2520models%252C%2520ProDVa%2520achieves%2520comparable%2520function%2520alignment%2520using%250Aless%2520than%25200.04%2525%2520of%2520the%2520training%2520data%252C%2520while%2520designing%2520significantly%2520more%250Awell-folded%2520proteins%252C%2520with%2520the%2520proportion%2520of%2520proteins%2520having%2520pLDDT%2520above%252070%250Aincreasing%2520by%25207.38%2525%2520and%2520those%2520with%2520PAE%2520below%252010%2520increasing%2520by%25209.6%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18966v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Protein%20Design%20with%20Dynamic%20Protein%20Vocabulary&entry.906535625=Nuowei%20Liu%20and%20Jiahao%20Kuang%20and%20Yanting%20Liu%20and%20Tao%20Ji%20and%20Changzhi%20Sun%20and%20Man%20Lan%20and%20Yuanbin%20Wu&entry.1292438233=%20%20Protein%20design%20is%20a%20fundamental%20challenge%20in%20biotechnology%2C%20aiming%20to%20design%0Anovel%20sequences%20with%20specific%20functions%20within%20the%20vast%20space%20of%20possible%0Aproteins.%20Recent%20advances%20in%20deep%20generative%20models%20have%20enabled%20function-based%0Aprotein%20design%20from%20textual%20descriptions%2C%20yet%20struggle%20with%20structural%0Aplausibility.%20Inspired%20by%20classical%20protein%20design%20methods%20that%20leverage%0Anatural%20protein%20structures%2C%20we%20explore%20whether%20incorporating%20fragments%20from%0Anatural%20proteins%20can%20enhance%20foldability%20in%20generative%20models.%20Our%20empirical%0Aresults%20show%20that%20even%20random%20incorporation%20of%20fragments%20improves%20foldability.%0ABuilding%20on%20this%20insight%2C%20we%20introduce%20ProDVa%2C%20a%20novel%20protein%20design%20approach%0Athat%20integrates%20a%20text%20encoder%20for%20functional%20descriptions%2C%20a%20protein%20language%0Amodel%20for%20designing%20proteins%2C%20and%20a%20fragment%20encoder%20to%20dynamically%20retrieve%0Aprotein%20fragments%20based%20on%20textual%20functional%20descriptions.%20Experimental%0Aresults%20demonstrate%20that%20our%20approach%20effectively%20designs%20protein%20sequences%0Athat%20are%20both%20functionally%20aligned%20and%20structurally%20plausible.%20Compared%20to%0Astate-of-the-art%20models%2C%20ProDVa%20achieves%20comparable%20function%20alignment%20using%0Aless%20than%200.04%25%20of%20the%20training%20data%2C%20while%20designing%20significantly%20more%0Awell-folded%20proteins%2C%20with%20the%20proportion%20of%20proteins%20having%20pLDDT%20above%2070%0Aincreasing%20by%207.38%25%20and%20those%20with%20PAE%20below%2010%20increasing%20by%209.6%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18966v2&entry.124074799=Read"},
{"title": "The Philosophical Foundations of Growing AI Like A Child", "author": "Dezhi Luo and Yijiang Li and Hokin Deng", "abstract": "  Despite excelling in high-level reasoning, current language models lack\nrobustness in real-world scenarios and perform poorly on fundamental\nproblem-solving tasks that are intuitive to humans. This paper argues that both\nchallenges stem from a core discrepancy between human and machine cognitive\ndevelopment. While both systems rely on increasing representational power, the\nabsence of core knowledge, foundational cognitive structures in humans,\nprevents language models from developing robust, generalizable abilities, where\ncomplex skills are grounded in simpler ones within their respective domains. It\nexplores empirical evidence of core knowledge in humans, analyzes why language\nmodels fail to acquire it, and argues that this limitation is not an inherent\narchitectural constraint. Finally, it outlines a workable proposal for\nsystematically integrating core knowledge into future multi-modal language\nmodels through the large-scale generation of synthetic training data using a\ncognitive prototyping strategy.\n", "link": "http://arxiv.org/abs/2502.10742v2", "date": "2025-10-14", "relevancy": 2.6243, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Philosophical%20Foundations%20of%20Growing%20AI%20Like%20A%20Child&body=Title%3A%20The%20Philosophical%20Foundations%20of%20Growing%20AI%20Like%20A%20Child%0AAuthor%3A%20Dezhi%20Luo%20and%20Yijiang%20Li%20and%20Hokin%20Deng%0AAbstract%3A%20%20%20Despite%20excelling%20in%20high-level%20reasoning%2C%20current%20language%20models%20lack%0Arobustness%20in%20real-world%20scenarios%20and%20perform%20poorly%20on%20fundamental%0Aproblem-solving%20tasks%20that%20are%20intuitive%20to%20humans.%20This%20paper%20argues%20that%20both%0Achallenges%20stem%20from%20a%20core%20discrepancy%20between%20human%20and%20machine%20cognitive%0Adevelopment.%20While%20both%20systems%20rely%20on%20increasing%20representational%20power%2C%20the%0Aabsence%20of%20core%20knowledge%2C%20foundational%20cognitive%20structures%20in%20humans%2C%0Aprevents%20language%20models%20from%20developing%20robust%2C%20generalizable%20abilities%2C%20where%0Acomplex%20skills%20are%20grounded%20in%20simpler%20ones%20within%20their%20respective%20domains.%20It%0Aexplores%20empirical%20evidence%20of%20core%20knowledge%20in%20humans%2C%20analyzes%20why%20language%0Amodels%20fail%20to%20acquire%20it%2C%20and%20argues%20that%20this%20limitation%20is%20not%20an%20inherent%0Aarchitectural%20constraint.%20Finally%2C%20it%20outlines%20a%20workable%20proposal%20for%0Asystematically%20integrating%20core%20knowledge%20into%20future%20multi-modal%20language%0Amodels%20through%20the%20large-scale%20generation%20of%20synthetic%20training%20data%20using%20a%0Acognitive%20prototyping%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10742v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Philosophical%2520Foundations%2520of%2520Growing%2520AI%2520Like%2520A%2520Child%26entry.906535625%3DDezhi%2520Luo%2520and%2520Yijiang%2520Li%2520and%2520Hokin%2520Deng%26entry.1292438233%3D%2520%2520Despite%2520excelling%2520in%2520high-level%2520reasoning%252C%2520current%2520language%2520models%2520lack%250Arobustness%2520in%2520real-world%2520scenarios%2520and%2520perform%2520poorly%2520on%2520fundamental%250Aproblem-solving%2520tasks%2520that%2520are%2520intuitive%2520to%2520humans.%2520This%2520paper%2520argues%2520that%2520both%250Achallenges%2520stem%2520from%2520a%2520core%2520discrepancy%2520between%2520human%2520and%2520machine%2520cognitive%250Adevelopment.%2520While%2520both%2520systems%2520rely%2520on%2520increasing%2520representational%2520power%252C%2520the%250Aabsence%2520of%2520core%2520knowledge%252C%2520foundational%2520cognitive%2520structures%2520in%2520humans%252C%250Aprevents%2520language%2520models%2520from%2520developing%2520robust%252C%2520generalizable%2520abilities%252C%2520where%250Acomplex%2520skills%2520are%2520grounded%2520in%2520simpler%2520ones%2520within%2520their%2520respective%2520domains.%2520It%250Aexplores%2520empirical%2520evidence%2520of%2520core%2520knowledge%2520in%2520humans%252C%2520analyzes%2520why%2520language%250Amodels%2520fail%2520to%2520acquire%2520it%252C%2520and%2520argues%2520that%2520this%2520limitation%2520is%2520not%2520an%2520inherent%250Aarchitectural%2520constraint.%2520Finally%252C%2520it%2520outlines%2520a%2520workable%2520proposal%2520for%250Asystematically%2520integrating%2520core%2520knowledge%2520into%2520future%2520multi-modal%2520language%250Amodels%2520through%2520the%2520large-scale%2520generation%2520of%2520synthetic%2520training%2520data%2520using%2520a%250Acognitive%2520prototyping%2520strategy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10742v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Philosophical%20Foundations%20of%20Growing%20AI%20Like%20A%20Child&entry.906535625=Dezhi%20Luo%20and%20Yijiang%20Li%20and%20Hokin%20Deng&entry.1292438233=%20%20Despite%20excelling%20in%20high-level%20reasoning%2C%20current%20language%20models%20lack%0Arobustness%20in%20real-world%20scenarios%20and%20perform%20poorly%20on%20fundamental%0Aproblem-solving%20tasks%20that%20are%20intuitive%20to%20humans.%20This%20paper%20argues%20that%20both%0Achallenges%20stem%20from%20a%20core%20discrepancy%20between%20human%20and%20machine%20cognitive%0Adevelopment.%20While%20both%20systems%20rely%20on%20increasing%20representational%20power%2C%20the%0Aabsence%20of%20core%20knowledge%2C%20foundational%20cognitive%20structures%20in%20humans%2C%0Aprevents%20language%20models%20from%20developing%20robust%2C%20generalizable%20abilities%2C%20where%0Acomplex%20skills%20are%20grounded%20in%20simpler%20ones%20within%20their%20respective%20domains.%20It%0Aexplores%20empirical%20evidence%20of%20core%20knowledge%20in%20humans%2C%20analyzes%20why%20language%0Amodels%20fail%20to%20acquire%20it%2C%20and%20argues%20that%20this%20limitation%20is%20not%20an%20inherent%0Aarchitectural%20constraint.%20Finally%2C%20it%20outlines%20a%20workable%20proposal%20for%0Asystematically%20integrating%20core%20knowledge%20into%20future%20multi-modal%20language%0Amodels%20through%20the%20large-scale%20generation%20of%20synthetic%20training%20data%20using%20a%0Acognitive%20prototyping%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10742v2&entry.124074799=Read"},
{"title": "SG-XDEAT: Sparsity-Guided Cross-Dimensional and Cross-Encoding Attention\n  with Target-Aware Conditioning in Tabular Learning", "author": "Chih-Chuan Cheng and Yi-Ju Tseng", "abstract": "  We propose SG-XDEAT (Sparsity-Guided Cross Dimensional and Cross-Encoding\nAttention with Target Aware Conditioning), a novel framework designed for\nsupervised learning on tabular data. At its core, SG-XDEAT employs a\ndual-stream encoder that decomposes each input feature into two parallel\nrepresentations: a raw value stream and a target-conditioned (label-aware)\nstream. These dual representations are then propagated through a hierarchical\nstack of attention-based modules. SG-XDEAT integrates three key components: (i)\nCross-Dimensional self-attention, which captures intra-view dependencies among\nfeatures within each stream; (ii) Cross-Encoding self-attention, which enables\nbidirectional interaction between raw and target-aware representations; and\n(iii) an Adaptive Sparse Self-Attention (ASSA) mechanism, which dynamically\nsuppresses low-utility tokens by driving their attention weights toward\nzero--thereby mitigating the impact of noise. Empirical results on multiple\npublic benchmarks show consistent gains over strong baselines, confirming that\njointly modeling raw and target-aware views--while adaptively filtering\nnoise--yields a more robust deep tabular learner.\n", "link": "http://arxiv.org/abs/2510.12659v1", "date": "2025-10-14", "relevancy": 2.6104, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5313}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5179}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SG-XDEAT%3A%20Sparsity-Guided%20Cross-Dimensional%20and%20Cross-Encoding%20Attention%0A%20%20with%20Target-Aware%20Conditioning%20in%20Tabular%20Learning&body=Title%3A%20SG-XDEAT%3A%20Sparsity-Guided%20Cross-Dimensional%20and%20Cross-Encoding%20Attention%0A%20%20with%20Target-Aware%20Conditioning%20in%20Tabular%20Learning%0AAuthor%3A%20Chih-Chuan%20Cheng%20and%20Yi-Ju%20Tseng%0AAbstract%3A%20%20%20We%20propose%20SG-XDEAT%20%28Sparsity-Guided%20Cross%20Dimensional%20and%20Cross-Encoding%0AAttention%20with%20Target%20Aware%20Conditioning%29%2C%20a%20novel%20framework%20designed%20for%0Asupervised%20learning%20on%20tabular%20data.%20At%20its%20core%2C%20SG-XDEAT%20employs%20a%0Adual-stream%20encoder%20that%20decomposes%20each%20input%20feature%20into%20two%20parallel%0Arepresentations%3A%20a%20raw%20value%20stream%20and%20a%20target-conditioned%20%28label-aware%29%0Astream.%20These%20dual%20representations%20are%20then%20propagated%20through%20a%20hierarchical%0Astack%20of%20attention-based%20modules.%20SG-XDEAT%20integrates%20three%20key%20components%3A%20%28i%29%0ACross-Dimensional%20self-attention%2C%20which%20captures%20intra-view%20dependencies%20among%0Afeatures%20within%20each%20stream%3B%20%28ii%29%20Cross-Encoding%20self-attention%2C%20which%20enables%0Abidirectional%20interaction%20between%20raw%20and%20target-aware%20representations%3B%20and%0A%28iii%29%20an%20Adaptive%20Sparse%20Self-Attention%20%28ASSA%29%20mechanism%2C%20which%20dynamically%0Asuppresses%20low-utility%20tokens%20by%20driving%20their%20attention%20weights%20toward%0Azero--thereby%20mitigating%20the%20impact%20of%20noise.%20Empirical%20results%20on%20multiple%0Apublic%20benchmarks%20show%20consistent%20gains%20over%20strong%20baselines%2C%20confirming%20that%0Ajointly%20modeling%20raw%20and%20target-aware%20views--while%20adaptively%20filtering%0Anoise--yields%20a%20more%20robust%20deep%20tabular%20learner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSG-XDEAT%253A%2520Sparsity-Guided%2520Cross-Dimensional%2520and%2520Cross-Encoding%2520Attention%250A%2520%2520with%2520Target-Aware%2520Conditioning%2520in%2520Tabular%2520Learning%26entry.906535625%3DChih-Chuan%2520Cheng%2520and%2520Yi-Ju%2520Tseng%26entry.1292438233%3D%2520%2520We%2520propose%2520SG-XDEAT%2520%2528Sparsity-Guided%2520Cross%2520Dimensional%2520and%2520Cross-Encoding%250AAttention%2520with%2520Target%2520Aware%2520Conditioning%2529%252C%2520a%2520novel%2520framework%2520designed%2520for%250Asupervised%2520learning%2520on%2520tabular%2520data.%2520At%2520its%2520core%252C%2520SG-XDEAT%2520employs%2520a%250Adual-stream%2520encoder%2520that%2520decomposes%2520each%2520input%2520feature%2520into%2520two%2520parallel%250Arepresentations%253A%2520a%2520raw%2520value%2520stream%2520and%2520a%2520target-conditioned%2520%2528label-aware%2529%250Astream.%2520These%2520dual%2520representations%2520are%2520then%2520propagated%2520through%2520a%2520hierarchical%250Astack%2520of%2520attention-based%2520modules.%2520SG-XDEAT%2520integrates%2520three%2520key%2520components%253A%2520%2528i%2529%250ACross-Dimensional%2520self-attention%252C%2520which%2520captures%2520intra-view%2520dependencies%2520among%250Afeatures%2520within%2520each%2520stream%253B%2520%2528ii%2529%2520Cross-Encoding%2520self-attention%252C%2520which%2520enables%250Abidirectional%2520interaction%2520between%2520raw%2520and%2520target-aware%2520representations%253B%2520and%250A%2528iii%2529%2520an%2520Adaptive%2520Sparse%2520Self-Attention%2520%2528ASSA%2529%2520mechanism%252C%2520which%2520dynamically%250Asuppresses%2520low-utility%2520tokens%2520by%2520driving%2520their%2520attention%2520weights%2520toward%250Azero--thereby%2520mitigating%2520the%2520impact%2520of%2520noise.%2520Empirical%2520results%2520on%2520multiple%250Apublic%2520benchmarks%2520show%2520consistent%2520gains%2520over%2520strong%2520baselines%252C%2520confirming%2520that%250Ajointly%2520modeling%2520raw%2520and%2520target-aware%2520views--while%2520adaptively%2520filtering%250Anoise--yields%2520a%2520more%2520robust%2520deep%2520tabular%2520learner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SG-XDEAT%3A%20Sparsity-Guided%20Cross-Dimensional%20and%20Cross-Encoding%20Attention%0A%20%20with%20Target-Aware%20Conditioning%20in%20Tabular%20Learning&entry.906535625=Chih-Chuan%20Cheng%20and%20Yi-Ju%20Tseng&entry.1292438233=%20%20We%20propose%20SG-XDEAT%20%28Sparsity-Guided%20Cross%20Dimensional%20and%20Cross-Encoding%0AAttention%20with%20Target%20Aware%20Conditioning%29%2C%20a%20novel%20framework%20designed%20for%0Asupervised%20learning%20on%20tabular%20data.%20At%20its%20core%2C%20SG-XDEAT%20employs%20a%0Adual-stream%20encoder%20that%20decomposes%20each%20input%20feature%20into%20two%20parallel%0Arepresentations%3A%20a%20raw%20value%20stream%20and%20a%20target-conditioned%20%28label-aware%29%0Astream.%20These%20dual%20representations%20are%20then%20propagated%20through%20a%20hierarchical%0Astack%20of%20attention-based%20modules.%20SG-XDEAT%20integrates%20three%20key%20components%3A%20%28i%29%0ACross-Dimensional%20self-attention%2C%20which%20captures%20intra-view%20dependencies%20among%0Afeatures%20within%20each%20stream%3B%20%28ii%29%20Cross-Encoding%20self-attention%2C%20which%20enables%0Abidirectional%20interaction%20between%20raw%20and%20target-aware%20representations%3B%20and%0A%28iii%29%20an%20Adaptive%20Sparse%20Self-Attention%20%28ASSA%29%20mechanism%2C%20which%20dynamically%0Asuppresses%20low-utility%20tokens%20by%20driving%20their%20attention%20weights%20toward%0Azero--thereby%20mitigating%20the%20impact%20of%20noise.%20Empirical%20results%20on%20multiple%0Apublic%20benchmarks%20show%20consistent%20gains%20over%20strong%20baselines%2C%20confirming%20that%0Ajointly%20modeling%20raw%20and%20target-aware%20views--while%20adaptively%20filtering%0Anoise--yields%20a%20more%20robust%20deep%20tabular%20learner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12659v1&entry.124074799=Read"},
{"title": "FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning\n  Models on Automatically Verifiable Textual and Visual Questions", "author": "Bowen Qin and Chen Yue and Fang Yin and Hui Wang and JG Yao and Jiakang Liu and Jing-Shu Zheng and Miguel Hu Chen and Richeng Xuan and Shibei Meng and Shiqi Zhou and Teng Dai and Tong-Shuai Ren and Wei Cui and Xi Yang and Xialin Du and Xiaojing Xu and Xue Sun and Xuejing Li and Yaming Liu and Yesheng Liu and Ying Liu and Yonghua Lin and Yu Zhao and Yunduo Zhang and Yuwen Luo and Zheqi He and Zhiyuan He and Zhongyuan Wang", "abstract": "  We conduct a moderate-scale contamination-free (to some extent) evaluation of\ncurrent large reasoning models (LRMs) with some preliminary findings. We also\nrelease ROME, our evaluation benchmark for vision language models intended to\ntest reasoning from visual clues. We attach links to the benchmark, evaluation\ndata, and other updates on this website:\nhttps://flageval-baai.github.io/LRM-Eval/\n", "link": "http://arxiv.org/abs/2509.17177v2", "date": "2025-10-14", "relevancy": 2.5969, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5392}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5392}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlagEval%20Findings%20Report%3A%20A%20Preliminary%20Evaluation%20of%20Large%20Reasoning%0A%20%20Models%20on%20Automatically%20Verifiable%20Textual%20and%20Visual%20Questions&body=Title%3A%20FlagEval%20Findings%20Report%3A%20A%20Preliminary%20Evaluation%20of%20Large%20Reasoning%0A%20%20Models%20on%20Automatically%20Verifiable%20Textual%20and%20Visual%20Questions%0AAuthor%3A%20Bowen%20Qin%20and%20Chen%20Yue%20and%20Fang%20Yin%20and%20Hui%20Wang%20and%20JG%20Yao%20and%20Jiakang%20Liu%20and%20Jing-Shu%20Zheng%20and%20Miguel%20Hu%20Chen%20and%20Richeng%20Xuan%20and%20Shibei%20Meng%20and%20Shiqi%20Zhou%20and%20Teng%20Dai%20and%20Tong-Shuai%20Ren%20and%20Wei%20Cui%20and%20Xi%20Yang%20and%20Xialin%20Du%20and%20Xiaojing%20Xu%20and%20Xue%20Sun%20and%20Xuejing%20Li%20and%20Yaming%20Liu%20and%20Yesheng%20Liu%20and%20Ying%20Liu%20and%20Yonghua%20Lin%20and%20Yu%20Zhao%20and%20Yunduo%20Zhang%20and%20Yuwen%20Luo%20and%20Zheqi%20He%20and%20Zhiyuan%20He%20and%20Zhongyuan%20Wang%0AAbstract%3A%20%20%20We%20conduct%20a%20moderate-scale%20contamination-free%20%28to%20some%20extent%29%20evaluation%20of%0Acurrent%20large%20reasoning%20models%20%28LRMs%29%20with%20some%20preliminary%20findings.%20We%20also%0Arelease%20ROME%2C%20our%20evaluation%20benchmark%20for%20vision%20language%20models%20intended%20to%0Atest%20reasoning%20from%20visual%20clues.%20We%20attach%20links%20to%20the%20benchmark%2C%20evaluation%0Adata%2C%20and%20other%20updates%20on%20this%20website%3A%0Ahttps%3A//flageval-baai.github.io/LRM-Eval/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17177v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlagEval%2520Findings%2520Report%253A%2520A%2520Preliminary%2520Evaluation%2520of%2520Large%2520Reasoning%250A%2520%2520Models%2520on%2520Automatically%2520Verifiable%2520Textual%2520and%2520Visual%2520Questions%26entry.906535625%3DBowen%2520Qin%2520and%2520Chen%2520Yue%2520and%2520Fang%2520Yin%2520and%2520Hui%2520Wang%2520and%2520JG%2520Yao%2520and%2520Jiakang%2520Liu%2520and%2520Jing-Shu%2520Zheng%2520and%2520Miguel%2520Hu%2520Chen%2520and%2520Richeng%2520Xuan%2520and%2520Shibei%2520Meng%2520and%2520Shiqi%2520Zhou%2520and%2520Teng%2520Dai%2520and%2520Tong-Shuai%2520Ren%2520and%2520Wei%2520Cui%2520and%2520Xi%2520Yang%2520and%2520Xialin%2520Du%2520and%2520Xiaojing%2520Xu%2520and%2520Xue%2520Sun%2520and%2520Xuejing%2520Li%2520and%2520Yaming%2520Liu%2520and%2520Yesheng%2520Liu%2520and%2520Ying%2520Liu%2520and%2520Yonghua%2520Lin%2520and%2520Yu%2520Zhao%2520and%2520Yunduo%2520Zhang%2520and%2520Yuwen%2520Luo%2520and%2520Zheqi%2520He%2520and%2520Zhiyuan%2520He%2520and%2520Zhongyuan%2520Wang%26entry.1292438233%3D%2520%2520We%2520conduct%2520a%2520moderate-scale%2520contamination-free%2520%2528to%2520some%2520extent%2529%2520evaluation%2520of%250Acurrent%2520large%2520reasoning%2520models%2520%2528LRMs%2529%2520with%2520some%2520preliminary%2520findings.%2520We%2520also%250Arelease%2520ROME%252C%2520our%2520evaluation%2520benchmark%2520for%2520vision%2520language%2520models%2520intended%2520to%250Atest%2520reasoning%2520from%2520visual%2520clues.%2520We%2520attach%2520links%2520to%2520the%2520benchmark%252C%2520evaluation%250Adata%252C%2520and%2520other%2520updates%2520on%2520this%2520website%253A%250Ahttps%253A//flageval-baai.github.io/LRM-Eval/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17177v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlagEval%20Findings%20Report%3A%20A%20Preliminary%20Evaluation%20of%20Large%20Reasoning%0A%20%20Models%20on%20Automatically%20Verifiable%20Textual%20and%20Visual%20Questions&entry.906535625=Bowen%20Qin%20and%20Chen%20Yue%20and%20Fang%20Yin%20and%20Hui%20Wang%20and%20JG%20Yao%20and%20Jiakang%20Liu%20and%20Jing-Shu%20Zheng%20and%20Miguel%20Hu%20Chen%20and%20Richeng%20Xuan%20and%20Shibei%20Meng%20and%20Shiqi%20Zhou%20and%20Teng%20Dai%20and%20Tong-Shuai%20Ren%20and%20Wei%20Cui%20and%20Xi%20Yang%20and%20Xialin%20Du%20and%20Xiaojing%20Xu%20and%20Xue%20Sun%20and%20Xuejing%20Li%20and%20Yaming%20Liu%20and%20Yesheng%20Liu%20and%20Ying%20Liu%20and%20Yonghua%20Lin%20and%20Yu%20Zhao%20and%20Yunduo%20Zhang%20and%20Yuwen%20Luo%20and%20Zheqi%20He%20and%20Zhiyuan%20He%20and%20Zhongyuan%20Wang&entry.1292438233=%20%20We%20conduct%20a%20moderate-scale%20contamination-free%20%28to%20some%20extent%29%20evaluation%20of%0Acurrent%20large%20reasoning%20models%20%28LRMs%29%20with%20some%20preliminary%20findings.%20We%20also%0Arelease%20ROME%2C%20our%20evaluation%20benchmark%20for%20vision%20language%20models%20intended%20to%0Atest%20reasoning%20from%20visual%20clues.%20We%20attach%20links%20to%20the%20benchmark%2C%20evaluation%0Adata%2C%20and%20other%20updates%20on%20this%20website%3A%0Ahttps%3A//flageval-baai.github.io/LRM-Eval/%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17177v2&entry.124074799=Read"},
{"title": "Leveraging Importance Sampling to Detach Alignment Modules from Large\n  Language Models", "author": "Yi Liu and Dianqing Liu and Mingye Zhu and Junbo Guo and Yongdong Zhang and Zhendong Mao", "abstract": "  The widespread adoption of large language models (LLMs) across industries has\nincreased the demand for high-quality and customizable outputs. However,\ntraditional alignment methods often require retraining large pretrained models,\nmaking it difficult to quickly adapt and optimize LLMs for diverse\napplications. To address this limitation, we propose a novel \\textit{Residual\nAlignment Model} (\\textit{RAM}) that formalizes the alignment process as a type\nof importance sampling. In this framework, the unaligned upstream model serves\nas the proposal distribution, while the alignment process is framed as\nsecondary sampling based on an autoregressive alignment module that acts as an\nestimator of the importance weights. This design enables a natural detachment\nof the alignment module from the target aligned model, improving flexibility\nand scalability. Based on this model, we derive an efficient sequence-level\ntraining strategy for the alignment module, which operates independently of the\nproposal module. Additionally, we develop a resampling algorithm with iterative\ntoken-level decoding to address the common first-token latency issue in\ncomparable methods. Experimental evaluations on two leading open-source LLMs\nacross diverse tasks, including instruction following, domain adaptation, and\npreference optimization, demonstrate that our approach consistently outperforms\nbaseline models.\n", "link": "http://arxiv.org/abs/2505.19700v2", "date": "2025-10-14", "relevancy": 2.594, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5417}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5131}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Importance%20Sampling%20to%20Detach%20Alignment%20Modules%20from%20Large%0A%20%20Language%20Models&body=Title%3A%20Leveraging%20Importance%20Sampling%20to%20Detach%20Alignment%20Modules%20from%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Yi%20Liu%20and%20Dianqing%20Liu%20and%20Mingye%20Zhu%20and%20Junbo%20Guo%20and%20Yongdong%20Zhang%20and%20Zhendong%20Mao%0AAbstract%3A%20%20%20The%20widespread%20adoption%20of%20large%20language%20models%20%28LLMs%29%20across%20industries%20has%0Aincreased%20the%20demand%20for%20high-quality%20and%20customizable%20outputs.%20However%2C%0Atraditional%20alignment%20methods%20often%20require%20retraining%20large%20pretrained%20models%2C%0Amaking%20it%20difficult%20to%20quickly%20adapt%20and%20optimize%20LLMs%20for%20diverse%0Aapplications.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%20%5Ctextit%7BResidual%0AAlignment%20Model%7D%20%28%5Ctextit%7BRAM%7D%29%20that%20formalizes%20the%20alignment%20process%20as%20a%20type%0Aof%20importance%20sampling.%20In%20this%20framework%2C%20the%20unaligned%20upstream%20model%20serves%0Aas%20the%20proposal%20distribution%2C%20while%20the%20alignment%20process%20is%20framed%20as%0Asecondary%20sampling%20based%20on%20an%20autoregressive%20alignment%20module%20that%20acts%20as%20an%0Aestimator%20of%20the%20importance%20weights.%20This%20design%20enables%20a%20natural%20detachment%0Aof%20the%20alignment%20module%20from%20the%20target%20aligned%20model%2C%20improving%20flexibility%0Aand%20scalability.%20Based%20on%20this%20model%2C%20we%20derive%20an%20efficient%20sequence-level%0Atraining%20strategy%20for%20the%20alignment%20module%2C%20which%20operates%20independently%20of%20the%0Aproposal%20module.%20Additionally%2C%20we%20develop%20a%20resampling%20algorithm%20with%20iterative%0Atoken-level%20decoding%20to%20address%20the%20common%20first-token%20latency%20issue%20in%0Acomparable%20methods.%20Experimental%20evaluations%20on%20two%20leading%20open-source%20LLMs%0Aacross%20diverse%20tasks%2C%20including%20instruction%20following%2C%20domain%20adaptation%2C%20and%0Apreference%20optimization%2C%20demonstrate%20that%20our%20approach%20consistently%20outperforms%0Abaseline%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19700v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Importance%2520Sampling%2520to%2520Detach%2520Alignment%2520Modules%2520from%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DYi%2520Liu%2520and%2520Dianqing%2520Liu%2520and%2520Mingye%2520Zhu%2520and%2520Junbo%2520Guo%2520and%2520Yongdong%2520Zhang%2520and%2520Zhendong%2520Mao%26entry.1292438233%3D%2520%2520The%2520widespread%2520adoption%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520across%2520industries%2520has%250Aincreased%2520the%2520demand%2520for%2520high-quality%2520and%2520customizable%2520outputs.%2520However%252C%250Atraditional%2520alignment%2520methods%2520often%2520require%2520retraining%2520large%2520pretrained%2520models%252C%250Amaking%2520it%2520difficult%2520to%2520quickly%2520adapt%2520and%2520optimize%2520LLMs%2520for%2520diverse%250Aapplications.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520%255Ctextit%257BResidual%250AAlignment%2520Model%257D%2520%2528%255Ctextit%257BRAM%257D%2529%2520that%2520formalizes%2520the%2520alignment%2520process%2520as%2520a%2520type%250Aof%2520importance%2520sampling.%2520In%2520this%2520framework%252C%2520the%2520unaligned%2520upstream%2520model%2520serves%250Aas%2520the%2520proposal%2520distribution%252C%2520while%2520the%2520alignment%2520process%2520is%2520framed%2520as%250Asecondary%2520sampling%2520based%2520on%2520an%2520autoregressive%2520alignment%2520module%2520that%2520acts%2520as%2520an%250Aestimator%2520of%2520the%2520importance%2520weights.%2520This%2520design%2520enables%2520a%2520natural%2520detachment%250Aof%2520the%2520alignment%2520module%2520from%2520the%2520target%2520aligned%2520model%252C%2520improving%2520flexibility%250Aand%2520scalability.%2520Based%2520on%2520this%2520model%252C%2520we%2520derive%2520an%2520efficient%2520sequence-level%250Atraining%2520strategy%2520for%2520the%2520alignment%2520module%252C%2520which%2520operates%2520independently%2520of%2520the%250Aproposal%2520module.%2520Additionally%252C%2520we%2520develop%2520a%2520resampling%2520algorithm%2520with%2520iterative%250Atoken-level%2520decoding%2520to%2520address%2520the%2520common%2520first-token%2520latency%2520issue%2520in%250Acomparable%2520methods.%2520Experimental%2520evaluations%2520on%2520two%2520leading%2520open-source%2520LLMs%250Aacross%2520diverse%2520tasks%252C%2520including%2520instruction%2520following%252C%2520domain%2520adaptation%252C%2520and%250Apreference%2520optimization%252C%2520demonstrate%2520that%2520our%2520approach%2520consistently%2520outperforms%250Abaseline%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19700v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Importance%20Sampling%20to%20Detach%20Alignment%20Modules%20from%20Large%0A%20%20Language%20Models&entry.906535625=Yi%20Liu%20and%20Dianqing%20Liu%20and%20Mingye%20Zhu%20and%20Junbo%20Guo%20and%20Yongdong%20Zhang%20and%20Zhendong%20Mao&entry.1292438233=%20%20The%20widespread%20adoption%20of%20large%20language%20models%20%28LLMs%29%20across%20industries%20has%0Aincreased%20the%20demand%20for%20high-quality%20and%20customizable%20outputs.%20However%2C%0Atraditional%20alignment%20methods%20often%20require%20retraining%20large%20pretrained%20models%2C%0Amaking%20it%20difficult%20to%20quickly%20adapt%20and%20optimize%20LLMs%20for%20diverse%0Aapplications.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%20%5Ctextit%7BResidual%0AAlignment%20Model%7D%20%28%5Ctextit%7BRAM%7D%29%20that%20formalizes%20the%20alignment%20process%20as%20a%20type%0Aof%20importance%20sampling.%20In%20this%20framework%2C%20the%20unaligned%20upstream%20model%20serves%0Aas%20the%20proposal%20distribution%2C%20while%20the%20alignment%20process%20is%20framed%20as%0Asecondary%20sampling%20based%20on%20an%20autoregressive%20alignment%20module%20that%20acts%20as%20an%0Aestimator%20of%20the%20importance%20weights.%20This%20design%20enables%20a%20natural%20detachment%0Aof%20the%20alignment%20module%20from%20the%20target%20aligned%20model%2C%20improving%20flexibility%0Aand%20scalability.%20Based%20on%20this%20model%2C%20we%20derive%20an%20efficient%20sequence-level%0Atraining%20strategy%20for%20the%20alignment%20module%2C%20which%20operates%20independently%20of%20the%0Aproposal%20module.%20Additionally%2C%20we%20develop%20a%20resampling%20algorithm%20with%20iterative%0Atoken-level%20decoding%20to%20address%20the%20common%20first-token%20latency%20issue%20in%0Acomparable%20methods.%20Experimental%20evaluations%20on%20two%20leading%20open-source%20LLMs%0Aacross%20diverse%20tasks%2C%20including%20instruction%20following%2C%20domain%20adaptation%2C%20and%0Apreference%20optimization%2C%20demonstrate%20that%20our%20approach%20consistently%20outperforms%0Abaseline%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19700v2&entry.124074799=Read"},
{"title": "Dual Perspectives on Non-Contrastive Self-Supervised Learning", "author": "Jean Ponce and Basile Terver and Martial Hebert and Michael Arbel", "abstract": "  The {\\em stop gradient} and {\\em exponential moving average} iterative\nprocedures are commonly used in non-contrastive approaches to self-supervised\nlearning to avoid representation collapse, with excellent performance in\ndownstream applications in practice. This presentation investigates these\nprocedures from the dual viewpoints of optimization and dynamical systems. We\nshow that, in general, although they {\\em do not} optimize the original\nobjective, or {\\em any} other smooth function, they {\\em do} avoid collapse\nFollowing~\\citet{Tian21}, but without any of the extra assumptions used in\ntheir proofs, we then show using a dynamical system perspective that, in the\nlinear case, minimizing the original objective function without the use of a\nstop gradient or exponential moving average {\\em always} leads to collapse.\nConversely, we characterize explicitly the equilibria of the dynamical systems\nassociated with these two procedures in this linear setting as algebraic\nvarieties in their parameter space, and show that they are, in general, {\\em\nasymptotically stable}. Our theoretical findings are illustrated by empirical\nexperiments with real and synthetic data.\n", "link": "http://arxiv.org/abs/2507.01028v2", "date": "2025-10-14", "relevancy": 2.5693, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5194}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5147}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual%20Perspectives%20on%20Non-Contrastive%20Self-Supervised%20Learning&body=Title%3A%20Dual%20Perspectives%20on%20Non-Contrastive%20Self-Supervised%20Learning%0AAuthor%3A%20Jean%20Ponce%20and%20Basile%20Terver%20and%20Martial%20Hebert%20and%20Michael%20Arbel%0AAbstract%3A%20%20%20The%20%7B%5Cem%20stop%20gradient%7D%20and%20%7B%5Cem%20exponential%20moving%20average%7D%20iterative%0Aprocedures%20are%20commonly%20used%20in%20non-contrastive%20approaches%20to%20self-supervised%0Alearning%20to%20avoid%20representation%20collapse%2C%20with%20excellent%20performance%20in%0Adownstream%20applications%20in%20practice.%20This%20presentation%20investigates%20these%0Aprocedures%20from%20the%20dual%20viewpoints%20of%20optimization%20and%20dynamical%20systems.%20We%0Ashow%20that%2C%20in%20general%2C%20although%20they%20%7B%5Cem%20do%20not%7D%20optimize%20the%20original%0Aobjective%2C%20or%20%7B%5Cem%20any%7D%20other%20smooth%20function%2C%20they%20%7B%5Cem%20do%7D%20avoid%20collapse%0AFollowing~%5Ccitet%7BTian21%7D%2C%20but%20without%20any%20of%20the%20extra%20assumptions%20used%20in%0Atheir%20proofs%2C%20we%20then%20show%20using%20a%20dynamical%20system%20perspective%20that%2C%20in%20the%0Alinear%20case%2C%20minimizing%20the%20original%20objective%20function%20without%20the%20use%20of%20a%0Astop%20gradient%20or%20exponential%20moving%20average%20%7B%5Cem%20always%7D%20leads%20to%20collapse.%0AConversely%2C%20we%20characterize%20explicitly%20the%20equilibria%20of%20the%20dynamical%20systems%0Aassociated%20with%20these%20two%20procedures%20in%20this%20linear%20setting%20as%20algebraic%0Avarieties%20in%20their%20parameter%20space%2C%20and%20show%20that%20they%20are%2C%20in%20general%2C%20%7B%5Cem%0Aasymptotically%20stable%7D.%20Our%20theoretical%20findings%20are%20illustrated%20by%20empirical%0Aexperiments%20with%20real%20and%20synthetic%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01028v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual%2520Perspectives%2520on%2520Non-Contrastive%2520Self-Supervised%2520Learning%26entry.906535625%3DJean%2520Ponce%2520and%2520Basile%2520Terver%2520and%2520Martial%2520Hebert%2520and%2520Michael%2520Arbel%26entry.1292438233%3D%2520%2520The%2520%257B%255Cem%2520stop%2520gradient%257D%2520and%2520%257B%255Cem%2520exponential%2520moving%2520average%257D%2520iterative%250Aprocedures%2520are%2520commonly%2520used%2520in%2520non-contrastive%2520approaches%2520to%2520self-supervised%250Alearning%2520to%2520avoid%2520representation%2520collapse%252C%2520with%2520excellent%2520performance%2520in%250Adownstream%2520applications%2520in%2520practice.%2520This%2520presentation%2520investigates%2520these%250Aprocedures%2520from%2520the%2520dual%2520viewpoints%2520of%2520optimization%2520and%2520dynamical%2520systems.%2520We%250Ashow%2520that%252C%2520in%2520general%252C%2520although%2520they%2520%257B%255Cem%2520do%2520not%257D%2520optimize%2520the%2520original%250Aobjective%252C%2520or%2520%257B%255Cem%2520any%257D%2520other%2520smooth%2520function%252C%2520they%2520%257B%255Cem%2520do%257D%2520avoid%2520collapse%250AFollowing~%255Ccitet%257BTian21%257D%252C%2520but%2520without%2520any%2520of%2520the%2520extra%2520assumptions%2520used%2520in%250Atheir%2520proofs%252C%2520we%2520then%2520show%2520using%2520a%2520dynamical%2520system%2520perspective%2520that%252C%2520in%2520the%250Alinear%2520case%252C%2520minimizing%2520the%2520original%2520objective%2520function%2520without%2520the%2520use%2520of%2520a%250Astop%2520gradient%2520or%2520exponential%2520moving%2520average%2520%257B%255Cem%2520always%257D%2520leads%2520to%2520collapse.%250AConversely%252C%2520we%2520characterize%2520explicitly%2520the%2520equilibria%2520of%2520the%2520dynamical%2520systems%250Aassociated%2520with%2520these%2520two%2520procedures%2520in%2520this%2520linear%2520setting%2520as%2520algebraic%250Avarieties%2520in%2520their%2520parameter%2520space%252C%2520and%2520show%2520that%2520they%2520are%252C%2520in%2520general%252C%2520%257B%255Cem%250Aasymptotically%2520stable%257D.%2520Our%2520theoretical%2520findings%2520are%2520illustrated%2520by%2520empirical%250Aexperiments%2520with%2520real%2520and%2520synthetic%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01028v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Perspectives%20on%20Non-Contrastive%20Self-Supervised%20Learning&entry.906535625=Jean%20Ponce%20and%20Basile%20Terver%20and%20Martial%20Hebert%20and%20Michael%20Arbel&entry.1292438233=%20%20The%20%7B%5Cem%20stop%20gradient%7D%20and%20%7B%5Cem%20exponential%20moving%20average%7D%20iterative%0Aprocedures%20are%20commonly%20used%20in%20non-contrastive%20approaches%20to%20self-supervised%0Alearning%20to%20avoid%20representation%20collapse%2C%20with%20excellent%20performance%20in%0Adownstream%20applications%20in%20practice.%20This%20presentation%20investigates%20these%0Aprocedures%20from%20the%20dual%20viewpoints%20of%20optimization%20and%20dynamical%20systems.%20We%0Ashow%20that%2C%20in%20general%2C%20although%20they%20%7B%5Cem%20do%20not%7D%20optimize%20the%20original%0Aobjective%2C%20or%20%7B%5Cem%20any%7D%20other%20smooth%20function%2C%20they%20%7B%5Cem%20do%7D%20avoid%20collapse%0AFollowing~%5Ccitet%7BTian21%7D%2C%20but%20without%20any%20of%20the%20extra%20assumptions%20used%20in%0Atheir%20proofs%2C%20we%20then%20show%20using%20a%20dynamical%20system%20perspective%20that%2C%20in%20the%0Alinear%20case%2C%20minimizing%20the%20original%20objective%20function%20without%20the%20use%20of%20a%0Astop%20gradient%20or%20exponential%20moving%20average%20%7B%5Cem%20always%7D%20leads%20to%20collapse.%0AConversely%2C%20we%20characterize%20explicitly%20the%20equilibria%20of%20the%20dynamical%20systems%0Aassociated%20with%20these%20two%20procedures%20in%20this%20linear%20setting%20as%20algebraic%0Avarieties%20in%20their%20parameter%20space%2C%20and%20show%20that%20they%20are%2C%20in%20general%2C%20%7B%5Cem%0Aasymptotically%20stable%7D.%20Our%20theoretical%20findings%20are%20illustrated%20by%20empirical%0Aexperiments%20with%20real%20and%20synthetic%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01028v2&entry.124074799=Read"},
{"title": "KonfAI: A Modular and Fully Configurable Framework for Deep Learning in\n  Medical Imaging", "author": "Valentin Boussot and Jean-Louis Dillenseger", "abstract": "  KonfAI is a modular, extensible, and fully configurable deep learning\nframework specifically designed for medical imaging tasks. It enables users to\ndefine complete training, inference, and evaluation workflows through\nstructured YAML configuration files, without modifying the underlying code.\nThis declarative approach enhances reproducibility, transparency, and\nexperimental traceability while reducing development time. Beyond the\ncapabilities of standard pipelines, KonfAI provides native abstractions for\nadvanced strategies including patch-based learning, test-time augmentation,\nmodel ensembling, and direct access to intermediate feature representations for\ndeep supervision. It also supports complex multi-model training setups such as\ngenerative adversarial architectures. Thanks to its modular and extensible\narchitecture, KonfAI can easily accommodate custom models, loss functions, and\ndata processing components. The framework has been successfully applied to\nsegmentation, registration, and image synthesis tasks, and has contributed to\ntop-ranking results in several international medical imaging challenges. KonfAI\nis open source and available at https://github.com/vboussot/KonfAI.\n", "link": "http://arxiv.org/abs/2508.09823v2", "date": "2025-10-14", "relevancy": 2.5469, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5215}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5034}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KonfAI%3A%20A%20Modular%20and%20Fully%20Configurable%20Framework%20for%20Deep%20Learning%20in%0A%20%20Medical%20Imaging&body=Title%3A%20KonfAI%3A%20A%20Modular%20and%20Fully%20Configurable%20Framework%20for%20Deep%20Learning%20in%0A%20%20Medical%20Imaging%0AAuthor%3A%20Valentin%20Boussot%20and%20Jean-Louis%20Dillenseger%0AAbstract%3A%20%20%20KonfAI%20is%20a%20modular%2C%20extensible%2C%20and%20fully%20configurable%20deep%20learning%0Aframework%20specifically%20designed%20for%20medical%20imaging%20tasks.%20It%20enables%20users%20to%0Adefine%20complete%20training%2C%20inference%2C%20and%20evaluation%20workflows%20through%0Astructured%20YAML%20configuration%20files%2C%20without%20modifying%20the%20underlying%20code.%0AThis%20declarative%20approach%20enhances%20reproducibility%2C%20transparency%2C%20and%0Aexperimental%20traceability%20while%20reducing%20development%20time.%20Beyond%20the%0Acapabilities%20of%20standard%20pipelines%2C%20KonfAI%20provides%20native%20abstractions%20for%0Aadvanced%20strategies%20including%20patch-based%20learning%2C%20test-time%20augmentation%2C%0Amodel%20ensembling%2C%20and%20direct%20access%20to%20intermediate%20feature%20representations%20for%0Adeep%20supervision.%20It%20also%20supports%20complex%20multi-model%20training%20setups%20such%20as%0Agenerative%20adversarial%20architectures.%20Thanks%20to%20its%20modular%20and%20extensible%0Aarchitecture%2C%20KonfAI%20can%20easily%20accommodate%20custom%20models%2C%20loss%20functions%2C%20and%0Adata%20processing%20components.%20The%20framework%20has%20been%20successfully%20applied%20to%0Asegmentation%2C%20registration%2C%20and%20image%20synthesis%20tasks%2C%20and%20has%20contributed%20to%0Atop-ranking%20results%20in%20several%20international%20medical%20imaging%20challenges.%20KonfAI%0Ais%20open%20source%20and%20available%20at%20https%3A//github.com/vboussot/KonfAI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09823v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKonfAI%253A%2520A%2520Modular%2520and%2520Fully%2520Configurable%2520Framework%2520for%2520Deep%2520Learning%2520in%250A%2520%2520Medical%2520Imaging%26entry.906535625%3DValentin%2520Boussot%2520and%2520Jean-Louis%2520Dillenseger%26entry.1292438233%3D%2520%2520KonfAI%2520is%2520a%2520modular%252C%2520extensible%252C%2520and%2520fully%2520configurable%2520deep%2520learning%250Aframework%2520specifically%2520designed%2520for%2520medical%2520imaging%2520tasks.%2520It%2520enables%2520users%2520to%250Adefine%2520complete%2520training%252C%2520inference%252C%2520and%2520evaluation%2520workflows%2520through%250Astructured%2520YAML%2520configuration%2520files%252C%2520without%2520modifying%2520the%2520underlying%2520code.%250AThis%2520declarative%2520approach%2520enhances%2520reproducibility%252C%2520transparency%252C%2520and%250Aexperimental%2520traceability%2520while%2520reducing%2520development%2520time.%2520Beyond%2520the%250Acapabilities%2520of%2520standard%2520pipelines%252C%2520KonfAI%2520provides%2520native%2520abstractions%2520for%250Aadvanced%2520strategies%2520including%2520patch-based%2520learning%252C%2520test-time%2520augmentation%252C%250Amodel%2520ensembling%252C%2520and%2520direct%2520access%2520to%2520intermediate%2520feature%2520representations%2520for%250Adeep%2520supervision.%2520It%2520also%2520supports%2520complex%2520multi-model%2520training%2520setups%2520such%2520as%250Agenerative%2520adversarial%2520architectures.%2520Thanks%2520to%2520its%2520modular%2520and%2520extensible%250Aarchitecture%252C%2520KonfAI%2520can%2520easily%2520accommodate%2520custom%2520models%252C%2520loss%2520functions%252C%2520and%250Adata%2520processing%2520components.%2520The%2520framework%2520has%2520been%2520successfully%2520applied%2520to%250Asegmentation%252C%2520registration%252C%2520and%2520image%2520synthesis%2520tasks%252C%2520and%2520has%2520contributed%2520to%250Atop-ranking%2520results%2520in%2520several%2520international%2520medical%2520imaging%2520challenges.%2520KonfAI%250Ais%2520open%2520source%2520and%2520available%2520at%2520https%253A//github.com/vboussot/KonfAI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09823v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KonfAI%3A%20A%20Modular%20and%20Fully%20Configurable%20Framework%20for%20Deep%20Learning%20in%0A%20%20Medical%20Imaging&entry.906535625=Valentin%20Boussot%20and%20Jean-Louis%20Dillenseger&entry.1292438233=%20%20KonfAI%20is%20a%20modular%2C%20extensible%2C%20and%20fully%20configurable%20deep%20learning%0Aframework%20specifically%20designed%20for%20medical%20imaging%20tasks.%20It%20enables%20users%20to%0Adefine%20complete%20training%2C%20inference%2C%20and%20evaluation%20workflows%20through%0Astructured%20YAML%20configuration%20files%2C%20without%20modifying%20the%20underlying%20code.%0AThis%20declarative%20approach%20enhances%20reproducibility%2C%20transparency%2C%20and%0Aexperimental%20traceability%20while%20reducing%20development%20time.%20Beyond%20the%0Acapabilities%20of%20standard%20pipelines%2C%20KonfAI%20provides%20native%20abstractions%20for%0Aadvanced%20strategies%20including%20patch-based%20learning%2C%20test-time%20augmentation%2C%0Amodel%20ensembling%2C%20and%20direct%20access%20to%20intermediate%20feature%20representations%20for%0Adeep%20supervision.%20It%20also%20supports%20complex%20multi-model%20training%20setups%20such%20as%0Agenerative%20adversarial%20architectures.%20Thanks%20to%20its%20modular%20and%20extensible%0Aarchitecture%2C%20KonfAI%20can%20easily%20accommodate%20custom%20models%2C%20loss%20functions%2C%20and%0Adata%20processing%20components.%20The%20framework%20has%20been%20successfully%20applied%20to%0Asegmentation%2C%20registration%2C%20and%20image%20synthesis%20tasks%2C%20and%20has%20contributed%20to%0Atop-ranking%20results%20in%20several%20international%20medical%20imaging%20challenges.%20KonfAI%0Ais%20open%20source%20and%20available%20at%20https%3A//github.com/vboussot/KonfAI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09823v2&entry.124074799=Read"},
{"title": "Gen-C: Populating Virtual Worlds with Generative Crowds", "author": "Andreas Panayiotou and Panayiotis Charalambous and Ioannis Karamouzas", "abstract": "  Over the past two decades, researchers have made significant steps in\nsimulating agent-based human crowds, yet most efforts remain focused on\nlow-level tasks such as collision avoidance, path following, and flocking.\nRealistic simulations, however, require modeling high-level behaviors that\nemerge from agents interacting with each other and with their environment over\ntime. We introduce Generative Crowds (Gen-C), a generative framework that\nproduces crowd scenarios capturing agent-agent and agent-environment\ninteractions, shaping coherent high-level crowd plans. To avoid the\nlabor-intensive process of collecting and annotating real crowd video data, we\nleverage large language models (LLMs) to bootstrap synthetic datasets of crowd\nscenarios. We propose a time-expanded graph representation, encoding actions,\ninteractions, and spatial context. Gen-C employs a dual Variational Graph\nAutoencoder (VGAE) architecture that jointly learns connectivity patterns and\nnode features conditioned on textual and structural signals, overcoming the\nlimitations of direct LLM generation to enable scalable, environment-aware\nmulti-agent crowd simulations. We demonstrate the effectiveness of Gen-C on\nscenarios with diverse behaviors such as a University Campus and a Train\nStation, showing that it generates heterogeneous crowds, coherent interactions,\nand high-level decision-making patterns consistent with real-world crowd\ndynamics.\n", "link": "http://arxiv.org/abs/2504.01924v3", "date": "2025-10-14", "relevancy": 2.5441, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6822}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6088}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gen-C%3A%20Populating%20Virtual%20Worlds%20with%20Generative%20Crowds&body=Title%3A%20Gen-C%3A%20Populating%20Virtual%20Worlds%20with%20Generative%20Crowds%0AAuthor%3A%20Andreas%20Panayiotou%20and%20Panayiotis%20Charalambous%20and%20Ioannis%20Karamouzas%0AAbstract%3A%20%20%20Over%20the%20past%20two%20decades%2C%20researchers%20have%20made%20significant%20steps%20in%0Asimulating%20agent-based%20human%20crowds%2C%20yet%20most%20efforts%20remain%20focused%20on%0Alow-level%20tasks%20such%20as%20collision%20avoidance%2C%20path%20following%2C%20and%20flocking.%0ARealistic%20simulations%2C%20however%2C%20require%20modeling%20high-level%20behaviors%20that%0Aemerge%20from%20agents%20interacting%20with%20each%20other%20and%20with%20their%20environment%20over%0Atime.%20We%20introduce%20Generative%20Crowds%20%28Gen-C%29%2C%20a%20generative%20framework%20that%0Aproduces%20crowd%20scenarios%20capturing%20agent-agent%20and%20agent-environment%0Ainteractions%2C%20shaping%20coherent%20high-level%20crowd%20plans.%20To%20avoid%20the%0Alabor-intensive%20process%20of%20collecting%20and%20annotating%20real%20crowd%20video%20data%2C%20we%0Aleverage%20large%20language%20models%20%28LLMs%29%20to%20bootstrap%20synthetic%20datasets%20of%20crowd%0Ascenarios.%20We%20propose%20a%20time-expanded%20graph%20representation%2C%20encoding%20actions%2C%0Ainteractions%2C%20and%20spatial%20context.%20Gen-C%20employs%20a%20dual%20Variational%20Graph%0AAutoencoder%20%28VGAE%29%20architecture%20that%20jointly%20learns%20connectivity%20patterns%20and%0Anode%20features%20conditioned%20on%20textual%20and%20structural%20signals%2C%20overcoming%20the%0Alimitations%20of%20direct%20LLM%20generation%20to%20enable%20scalable%2C%20environment-aware%0Amulti-agent%20crowd%20simulations.%20We%20demonstrate%20the%20effectiveness%20of%20Gen-C%20on%0Ascenarios%20with%20diverse%20behaviors%20such%20as%20a%20University%20Campus%20and%20a%20Train%0AStation%2C%20showing%20that%20it%20generates%20heterogeneous%20crowds%2C%20coherent%20interactions%2C%0Aand%20high-level%20decision-making%20patterns%20consistent%20with%20real-world%20crowd%0Adynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.01924v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGen-C%253A%2520Populating%2520Virtual%2520Worlds%2520with%2520Generative%2520Crowds%26entry.906535625%3DAndreas%2520Panayiotou%2520and%2520Panayiotis%2520Charalambous%2520and%2520Ioannis%2520Karamouzas%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520two%2520decades%252C%2520researchers%2520have%2520made%2520significant%2520steps%2520in%250Asimulating%2520agent-based%2520human%2520crowds%252C%2520yet%2520most%2520efforts%2520remain%2520focused%2520on%250Alow-level%2520tasks%2520such%2520as%2520collision%2520avoidance%252C%2520path%2520following%252C%2520and%2520flocking.%250ARealistic%2520simulations%252C%2520however%252C%2520require%2520modeling%2520high-level%2520behaviors%2520that%250Aemerge%2520from%2520agents%2520interacting%2520with%2520each%2520other%2520and%2520with%2520their%2520environment%2520over%250Atime.%2520We%2520introduce%2520Generative%2520Crowds%2520%2528Gen-C%2529%252C%2520a%2520generative%2520framework%2520that%250Aproduces%2520crowd%2520scenarios%2520capturing%2520agent-agent%2520and%2520agent-environment%250Ainteractions%252C%2520shaping%2520coherent%2520high-level%2520crowd%2520plans.%2520To%2520avoid%2520the%250Alabor-intensive%2520process%2520of%2520collecting%2520and%2520annotating%2520real%2520crowd%2520video%2520data%252C%2520we%250Aleverage%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520bootstrap%2520synthetic%2520datasets%2520of%2520crowd%250Ascenarios.%2520We%2520propose%2520a%2520time-expanded%2520graph%2520representation%252C%2520encoding%2520actions%252C%250Ainteractions%252C%2520and%2520spatial%2520context.%2520Gen-C%2520employs%2520a%2520dual%2520Variational%2520Graph%250AAutoencoder%2520%2528VGAE%2529%2520architecture%2520that%2520jointly%2520learns%2520connectivity%2520patterns%2520and%250Anode%2520features%2520conditioned%2520on%2520textual%2520and%2520structural%2520signals%252C%2520overcoming%2520the%250Alimitations%2520of%2520direct%2520LLM%2520generation%2520to%2520enable%2520scalable%252C%2520environment-aware%250Amulti-agent%2520crowd%2520simulations.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520Gen-C%2520on%250Ascenarios%2520with%2520diverse%2520behaviors%2520such%2520as%2520a%2520University%2520Campus%2520and%2520a%2520Train%250AStation%252C%2520showing%2520that%2520it%2520generates%2520heterogeneous%2520crowds%252C%2520coherent%2520interactions%252C%250Aand%2520high-level%2520decision-making%2520patterns%2520consistent%2520with%2520real-world%2520crowd%250Adynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.01924v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gen-C%3A%20Populating%20Virtual%20Worlds%20with%20Generative%20Crowds&entry.906535625=Andreas%20Panayiotou%20and%20Panayiotis%20Charalambous%20and%20Ioannis%20Karamouzas&entry.1292438233=%20%20Over%20the%20past%20two%20decades%2C%20researchers%20have%20made%20significant%20steps%20in%0Asimulating%20agent-based%20human%20crowds%2C%20yet%20most%20efforts%20remain%20focused%20on%0Alow-level%20tasks%20such%20as%20collision%20avoidance%2C%20path%20following%2C%20and%20flocking.%0ARealistic%20simulations%2C%20however%2C%20require%20modeling%20high-level%20behaviors%20that%0Aemerge%20from%20agents%20interacting%20with%20each%20other%20and%20with%20their%20environment%20over%0Atime.%20We%20introduce%20Generative%20Crowds%20%28Gen-C%29%2C%20a%20generative%20framework%20that%0Aproduces%20crowd%20scenarios%20capturing%20agent-agent%20and%20agent-environment%0Ainteractions%2C%20shaping%20coherent%20high-level%20crowd%20plans.%20To%20avoid%20the%0Alabor-intensive%20process%20of%20collecting%20and%20annotating%20real%20crowd%20video%20data%2C%20we%0Aleverage%20large%20language%20models%20%28LLMs%29%20to%20bootstrap%20synthetic%20datasets%20of%20crowd%0Ascenarios.%20We%20propose%20a%20time-expanded%20graph%20representation%2C%20encoding%20actions%2C%0Ainteractions%2C%20and%20spatial%20context.%20Gen-C%20employs%20a%20dual%20Variational%20Graph%0AAutoencoder%20%28VGAE%29%20architecture%20that%20jointly%20learns%20connectivity%20patterns%20and%0Anode%20features%20conditioned%20on%20textual%20and%20structural%20signals%2C%20overcoming%20the%0Alimitations%20of%20direct%20LLM%20generation%20to%20enable%20scalable%2C%20environment-aware%0Amulti-agent%20crowd%20simulations.%20We%20demonstrate%20the%20effectiveness%20of%20Gen-C%20on%0Ascenarios%20with%20diverse%20behaviors%20such%20as%20a%20University%20Campus%20and%20a%20Train%0AStation%2C%20showing%20that%20it%20generates%20heterogeneous%20crowds%2C%20coherent%20interactions%2C%0Aand%20high-level%20decision-making%20patterns%20consistent%20with%20real-world%20crowd%0Adynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.01924v3&entry.124074799=Read"},
{"title": "CARVQ: Corrective Adaptor with Group Residual Vector Quantization for\n  LLM Embedding Compression", "author": "Dayin Gou and Sanghyun Byun and Nilesh Malpeddi and Gabrielle De Micheli and Prathamesh Vaste and Jacob Song and Woo Seong Chung", "abstract": "  Large Language Models (LLMs) typically rely on a large number of parameters\nfor token embedding, leading to substantial storage requirements and memory\nfootprints. In particular, LLMs deployed on edge devices are memory-bound, and\nreducing the memory footprint by compressing the embedding layer not only frees\nup the memory bandwidth but also speeds up inference. To address this, we\nintroduce CARVQ, a post-training novel Corrective Adaptor combined with group\nResidual Vector Quantization. CARVQ relies on the composition of both linear\nand non-linear maps and mimics the original model embedding to compress to\napproximately 1.6 bits without requiring specialized hardware to support\nlower-bit storage. We test our method on pre-trained LLMs such as LLaMA-3.2-1B,\nLLaMA-3.2-3B, LLaMA-3.2-3B-Instruct, LLaMA-3.1-8B, Qwen2.5-7B, Qwen2.5-Math-7B\nand Phi-4, evaluating on common generative, discriminative, math and reasoning\ntasks. We show that in most cases, CARVQ can achieve lower average\nbitwidth-per-parameter while maintaining reasonable perplexity and accuracy\ncompared to scalar quantization. Our contributions include a novel compression\ntechnique that is compatible with state-of-the-art transformer quantization\nmethods and can be seamlessly integrated into any hardware supporting 4-bit\nmemory to reduce the model's memory footprint in memory-constrained devices.\nThis work demonstrates a crucial step toward the efficient deployment of LLMs\non edge devices.\n", "link": "http://arxiv.org/abs/2510.12721v1", "date": "2025-10-14", "relevancy": 2.5324, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5127}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5062}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CARVQ%3A%20Corrective%20Adaptor%20with%20Group%20Residual%20Vector%20Quantization%20for%0A%20%20LLM%20Embedding%20Compression&body=Title%3A%20CARVQ%3A%20Corrective%20Adaptor%20with%20Group%20Residual%20Vector%20Quantization%20for%0A%20%20LLM%20Embedding%20Compression%0AAuthor%3A%20Dayin%20Gou%20and%20Sanghyun%20Byun%20and%20Nilesh%20Malpeddi%20and%20Gabrielle%20De%20Micheli%20and%20Prathamesh%20Vaste%20and%20Jacob%20Song%20and%20Woo%20Seong%20Chung%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20typically%20rely%20on%20a%20large%20number%20of%20parameters%0Afor%20token%20embedding%2C%20leading%20to%20substantial%20storage%20requirements%20and%20memory%0Afootprints.%20In%20particular%2C%20LLMs%20deployed%20on%20edge%20devices%20are%20memory-bound%2C%20and%0Areducing%20the%20memory%20footprint%20by%20compressing%20the%20embedding%20layer%20not%20only%20frees%0Aup%20the%20memory%20bandwidth%20but%20also%20speeds%20up%20inference.%20To%20address%20this%2C%20we%0Aintroduce%20CARVQ%2C%20a%20post-training%20novel%20Corrective%20Adaptor%20combined%20with%20group%0AResidual%20Vector%20Quantization.%20CARVQ%20relies%20on%20the%20composition%20of%20both%20linear%0Aand%20non-linear%20maps%20and%20mimics%20the%20original%20model%20embedding%20to%20compress%20to%0Aapproximately%201.6%20bits%20without%20requiring%20specialized%20hardware%20to%20support%0Alower-bit%20storage.%20We%20test%20our%20method%20on%20pre-trained%20LLMs%20such%20as%20LLaMA-3.2-1B%2C%0ALLaMA-3.2-3B%2C%20LLaMA-3.2-3B-Instruct%2C%20LLaMA-3.1-8B%2C%20Qwen2.5-7B%2C%20Qwen2.5-Math-7B%0Aand%20Phi-4%2C%20evaluating%20on%20common%20generative%2C%20discriminative%2C%20math%20and%20reasoning%0Atasks.%20We%20show%20that%20in%20most%20cases%2C%20CARVQ%20can%20achieve%20lower%20average%0Abitwidth-per-parameter%20while%20maintaining%20reasonable%20perplexity%20and%20accuracy%0Acompared%20to%20scalar%20quantization.%20Our%20contributions%20include%20a%20novel%20compression%0Atechnique%20that%20is%20compatible%20with%20state-of-the-art%20transformer%20quantization%0Amethods%20and%20can%20be%20seamlessly%20integrated%20into%20any%20hardware%20supporting%204-bit%0Amemory%20to%20reduce%20the%20model%27s%20memory%20footprint%20in%20memory-constrained%20devices.%0AThis%20work%20demonstrates%20a%20crucial%20step%20toward%20the%20efficient%20deployment%20of%20LLMs%0Aon%20edge%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12721v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCARVQ%253A%2520Corrective%2520Adaptor%2520with%2520Group%2520Residual%2520Vector%2520Quantization%2520for%250A%2520%2520LLM%2520Embedding%2520Compression%26entry.906535625%3DDayin%2520Gou%2520and%2520Sanghyun%2520Byun%2520and%2520Nilesh%2520Malpeddi%2520and%2520Gabrielle%2520De%2520Micheli%2520and%2520Prathamesh%2520Vaste%2520and%2520Jacob%2520Song%2520and%2520Woo%2520Seong%2520Chung%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520typically%2520rely%2520on%2520a%2520large%2520number%2520of%2520parameters%250Afor%2520token%2520embedding%252C%2520leading%2520to%2520substantial%2520storage%2520requirements%2520and%2520memory%250Afootprints.%2520In%2520particular%252C%2520LLMs%2520deployed%2520on%2520edge%2520devices%2520are%2520memory-bound%252C%2520and%250Areducing%2520the%2520memory%2520footprint%2520by%2520compressing%2520the%2520embedding%2520layer%2520not%2520only%2520frees%250Aup%2520the%2520memory%2520bandwidth%2520but%2520also%2520speeds%2520up%2520inference.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520CARVQ%252C%2520a%2520post-training%2520novel%2520Corrective%2520Adaptor%2520combined%2520with%2520group%250AResidual%2520Vector%2520Quantization.%2520CARVQ%2520relies%2520on%2520the%2520composition%2520of%2520both%2520linear%250Aand%2520non-linear%2520maps%2520and%2520mimics%2520the%2520original%2520model%2520embedding%2520to%2520compress%2520to%250Aapproximately%25201.6%2520bits%2520without%2520requiring%2520specialized%2520hardware%2520to%2520support%250Alower-bit%2520storage.%2520We%2520test%2520our%2520method%2520on%2520pre-trained%2520LLMs%2520such%2520as%2520LLaMA-3.2-1B%252C%250ALLaMA-3.2-3B%252C%2520LLaMA-3.2-3B-Instruct%252C%2520LLaMA-3.1-8B%252C%2520Qwen2.5-7B%252C%2520Qwen2.5-Math-7B%250Aand%2520Phi-4%252C%2520evaluating%2520on%2520common%2520generative%252C%2520discriminative%252C%2520math%2520and%2520reasoning%250Atasks.%2520We%2520show%2520that%2520in%2520most%2520cases%252C%2520CARVQ%2520can%2520achieve%2520lower%2520average%250Abitwidth-per-parameter%2520while%2520maintaining%2520reasonable%2520perplexity%2520and%2520accuracy%250Acompared%2520to%2520scalar%2520quantization.%2520Our%2520contributions%2520include%2520a%2520novel%2520compression%250Atechnique%2520that%2520is%2520compatible%2520with%2520state-of-the-art%2520transformer%2520quantization%250Amethods%2520and%2520can%2520be%2520seamlessly%2520integrated%2520into%2520any%2520hardware%2520supporting%25204-bit%250Amemory%2520to%2520reduce%2520the%2520model%2527s%2520memory%2520footprint%2520in%2520memory-constrained%2520devices.%250AThis%2520work%2520demonstrates%2520a%2520crucial%2520step%2520toward%2520the%2520efficient%2520deployment%2520of%2520LLMs%250Aon%2520edge%2520devices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12721v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CARVQ%3A%20Corrective%20Adaptor%20with%20Group%20Residual%20Vector%20Quantization%20for%0A%20%20LLM%20Embedding%20Compression&entry.906535625=Dayin%20Gou%20and%20Sanghyun%20Byun%20and%20Nilesh%20Malpeddi%20and%20Gabrielle%20De%20Micheli%20and%20Prathamesh%20Vaste%20and%20Jacob%20Song%20and%20Woo%20Seong%20Chung&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20typically%20rely%20on%20a%20large%20number%20of%20parameters%0Afor%20token%20embedding%2C%20leading%20to%20substantial%20storage%20requirements%20and%20memory%0Afootprints.%20In%20particular%2C%20LLMs%20deployed%20on%20edge%20devices%20are%20memory-bound%2C%20and%0Areducing%20the%20memory%20footprint%20by%20compressing%20the%20embedding%20layer%20not%20only%20frees%0Aup%20the%20memory%20bandwidth%20but%20also%20speeds%20up%20inference.%20To%20address%20this%2C%20we%0Aintroduce%20CARVQ%2C%20a%20post-training%20novel%20Corrective%20Adaptor%20combined%20with%20group%0AResidual%20Vector%20Quantization.%20CARVQ%20relies%20on%20the%20composition%20of%20both%20linear%0Aand%20non-linear%20maps%20and%20mimics%20the%20original%20model%20embedding%20to%20compress%20to%0Aapproximately%201.6%20bits%20without%20requiring%20specialized%20hardware%20to%20support%0Alower-bit%20storage.%20We%20test%20our%20method%20on%20pre-trained%20LLMs%20such%20as%20LLaMA-3.2-1B%2C%0ALLaMA-3.2-3B%2C%20LLaMA-3.2-3B-Instruct%2C%20LLaMA-3.1-8B%2C%20Qwen2.5-7B%2C%20Qwen2.5-Math-7B%0Aand%20Phi-4%2C%20evaluating%20on%20common%20generative%2C%20discriminative%2C%20math%20and%20reasoning%0Atasks.%20We%20show%20that%20in%20most%20cases%2C%20CARVQ%20can%20achieve%20lower%20average%0Abitwidth-per-parameter%20while%20maintaining%20reasonable%20perplexity%20and%20accuracy%0Acompared%20to%20scalar%20quantization.%20Our%20contributions%20include%20a%20novel%20compression%0Atechnique%20that%20is%20compatible%20with%20state-of-the-art%20transformer%20quantization%0Amethods%20and%20can%20be%20seamlessly%20integrated%20into%20any%20hardware%20supporting%204-bit%0Amemory%20to%20reduce%20the%20model%27s%20memory%20footprint%20in%20memory-constrained%20devices.%0AThis%20work%20demonstrates%20a%20crucial%20step%20toward%20the%20efficient%20deployment%20of%20LLMs%0Aon%20edge%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12721v1&entry.124074799=Read"},
{"title": "MS-GAGA: Metric-Selective Guided Adversarial Generation Attack", "author": "Dion J. X. Ho and Gabriel Lee Jun Rong and Niharika Shrivastava and Harshavardhan Abichandani and Pai Chet Ng and Xiaoxiao Miao", "abstract": "  We present MS-GAGA (Metric-Selective Guided Adversarial Generation Attack), a\ntwo-stage framework for crafting transferable and visually imperceptible\nadversarial examples against deepfake detectors in black-box settings. In Stage\n1, a dual-stream attack module generates adversarial candidates: MNTD-PGD\napplies enhanced gradient calculations optimized for small perturbation\nbudgets, while SG-PGD focuses perturbations on visually salient regions. This\ncomplementary design expands the adversarial search space and improves\ntransferability across unseen models. In Stage 2, a metric-aware selection\nmodule evaluates candidates based on both their success against black-box\nmodels and their structural similarity (SSIM) to the original image. By jointly\noptimizing transferability and imperceptibility, MS-GAGA achieves up to 27%\nhigher misclassification rates on unseen detectors compared to state-of-the-art\nattacks.\n", "link": "http://arxiv.org/abs/2510.12468v1", "date": "2025-10-14", "relevancy": 2.522, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5101}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.502}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MS-GAGA%3A%20Metric-Selective%20Guided%20Adversarial%20Generation%20Attack&body=Title%3A%20MS-GAGA%3A%20Metric-Selective%20Guided%20Adversarial%20Generation%20Attack%0AAuthor%3A%20Dion%20J.%20X.%20Ho%20and%20Gabriel%20Lee%20Jun%20Rong%20and%20Niharika%20Shrivastava%20and%20Harshavardhan%20Abichandani%20and%20Pai%20Chet%20Ng%20and%20Xiaoxiao%20Miao%0AAbstract%3A%20%20%20We%20present%20MS-GAGA%20%28Metric-Selective%20Guided%20Adversarial%20Generation%20Attack%29%2C%20a%0Atwo-stage%20framework%20for%20crafting%20transferable%20and%20visually%20imperceptible%0Aadversarial%20examples%20against%20deepfake%20detectors%20in%20black-box%20settings.%20In%20Stage%0A1%2C%20a%20dual-stream%20attack%20module%20generates%20adversarial%20candidates%3A%20MNTD-PGD%0Aapplies%20enhanced%20gradient%20calculations%20optimized%20for%20small%20perturbation%0Abudgets%2C%20while%20SG-PGD%20focuses%20perturbations%20on%20visually%20salient%20regions.%20This%0Acomplementary%20design%20expands%20the%20adversarial%20search%20space%20and%20improves%0Atransferability%20across%20unseen%20models.%20In%20Stage%202%2C%20a%20metric-aware%20selection%0Amodule%20evaluates%20candidates%20based%20on%20both%20their%20success%20against%20black-box%0Amodels%20and%20their%20structural%20similarity%20%28SSIM%29%20to%20the%20original%20image.%20By%20jointly%0Aoptimizing%20transferability%20and%20imperceptibility%2C%20MS-GAGA%20achieves%20up%20to%2027%25%0Ahigher%20misclassification%20rates%20on%20unseen%20detectors%20compared%20to%20state-of-the-art%0Aattacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMS-GAGA%253A%2520Metric-Selective%2520Guided%2520Adversarial%2520Generation%2520Attack%26entry.906535625%3DDion%2520J.%2520X.%2520Ho%2520and%2520Gabriel%2520Lee%2520Jun%2520Rong%2520and%2520Niharika%2520Shrivastava%2520and%2520Harshavardhan%2520Abichandani%2520and%2520Pai%2520Chet%2520Ng%2520and%2520Xiaoxiao%2520Miao%26entry.1292438233%3D%2520%2520We%2520present%2520MS-GAGA%2520%2528Metric-Selective%2520Guided%2520Adversarial%2520Generation%2520Attack%2529%252C%2520a%250Atwo-stage%2520framework%2520for%2520crafting%2520transferable%2520and%2520visually%2520imperceptible%250Aadversarial%2520examples%2520against%2520deepfake%2520detectors%2520in%2520black-box%2520settings.%2520In%2520Stage%250A1%252C%2520a%2520dual-stream%2520attack%2520module%2520generates%2520adversarial%2520candidates%253A%2520MNTD-PGD%250Aapplies%2520enhanced%2520gradient%2520calculations%2520optimized%2520for%2520small%2520perturbation%250Abudgets%252C%2520while%2520SG-PGD%2520focuses%2520perturbations%2520on%2520visually%2520salient%2520regions.%2520This%250Acomplementary%2520design%2520expands%2520the%2520adversarial%2520search%2520space%2520and%2520improves%250Atransferability%2520across%2520unseen%2520models.%2520In%2520Stage%25202%252C%2520a%2520metric-aware%2520selection%250Amodule%2520evaluates%2520candidates%2520based%2520on%2520both%2520their%2520success%2520against%2520black-box%250Amodels%2520and%2520their%2520structural%2520similarity%2520%2528SSIM%2529%2520to%2520the%2520original%2520image.%2520By%2520jointly%250Aoptimizing%2520transferability%2520and%2520imperceptibility%252C%2520MS-GAGA%2520achieves%2520up%2520to%252027%2525%250Ahigher%2520misclassification%2520rates%2520on%2520unseen%2520detectors%2520compared%2520to%2520state-of-the-art%250Aattacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MS-GAGA%3A%20Metric-Selective%20Guided%20Adversarial%20Generation%20Attack&entry.906535625=Dion%20J.%20X.%20Ho%20and%20Gabriel%20Lee%20Jun%20Rong%20and%20Niharika%20Shrivastava%20and%20Harshavardhan%20Abichandani%20and%20Pai%20Chet%20Ng%20and%20Xiaoxiao%20Miao&entry.1292438233=%20%20We%20present%20MS-GAGA%20%28Metric-Selective%20Guided%20Adversarial%20Generation%20Attack%29%2C%20a%0Atwo-stage%20framework%20for%20crafting%20transferable%20and%20visually%20imperceptible%0Aadversarial%20examples%20against%20deepfake%20detectors%20in%20black-box%20settings.%20In%20Stage%0A1%2C%20a%20dual-stream%20attack%20module%20generates%20adversarial%20candidates%3A%20MNTD-PGD%0Aapplies%20enhanced%20gradient%20calculations%20optimized%20for%20small%20perturbation%0Abudgets%2C%20while%20SG-PGD%20focuses%20perturbations%20on%20visually%20salient%20regions.%20This%0Acomplementary%20design%20expands%20the%20adversarial%20search%20space%20and%20improves%0Atransferability%20across%20unseen%20models.%20In%20Stage%202%2C%20a%20metric-aware%20selection%0Amodule%20evaluates%20candidates%20based%20on%20both%20their%20success%20against%20black-box%0Amodels%20and%20their%20structural%20similarity%20%28SSIM%29%20to%20the%20original%20image.%20By%20jointly%0Aoptimizing%20transferability%20and%20imperceptibility%2C%20MS-GAGA%20achieves%20up%20to%2027%25%0Ahigher%20misclassification%20rates%20on%20unseen%20detectors%20compared%20to%20state-of-the-art%0Aattacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12468v1&entry.124074799=Read"},
{"title": "Multi-View Graph Learning with Graph-Tuple", "author": "Shiyu Chen and Ningyuan Huang and Soledad Villar", "abstract": "  Graph Neural Networks (GNNs) typically scale with the number of graph edges,\nmaking them well suited for sparse graphs but less efficient on dense graphs,\nsuch as point clouds or molecular interactions. A common remedy is to sparsify\nthe graph via similarity thresholding or distance pruning, but this forces an\narbitrary choice of a single interaction scale and discards crucial information\nfrom other scales. To overcome this limitation, we introduce a multi-view\ngraph-tuple framework. Instead of a single graph, our graph-tuple framework\npartitions the graph into disjoint subgraphs, capturing primary local\ninteractions and weaker, long-range connections. We then learn multi-view\nrepresentations from the graph-tuple via a heterogeneous message-passing\narchitecture inspired by the theory of non-commuting operators, which we\nformally prove is strictly more expressive and guarantees a lower oracle risk\ncompared to single-graph message-passing models. We instantiate our framework\non two scientific domains: molecular property prediction from feature-scarce\nCoulomb matrices and cosmological parameter inference from geometric point\nclouds. On both applications, our multi-view graph-tuple models demonstrate\nbetter performance than single-graph baselines, highlighting the power and\nversatility of our multi-view approach.\n", "link": "http://arxiv.org/abs/2510.10341v2", "date": "2025-10-14", "relevancy": 2.5152, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5079}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5008}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-View%20Graph%20Learning%20with%20Graph-Tuple&body=Title%3A%20Multi-View%20Graph%20Learning%20with%20Graph-Tuple%0AAuthor%3A%20Shiyu%20Chen%20and%20Ningyuan%20Huang%20and%20Soledad%20Villar%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20typically%20scale%20with%20the%20number%20of%20graph%20edges%2C%0Amaking%20them%20well%20suited%20for%20sparse%20graphs%20but%20less%20efficient%20on%20dense%20graphs%2C%0Asuch%20as%20point%20clouds%20or%20molecular%20interactions.%20A%20common%20remedy%20is%20to%20sparsify%0Athe%20graph%20via%20similarity%20thresholding%20or%20distance%20pruning%2C%20but%20this%20forces%20an%0Aarbitrary%20choice%20of%20a%20single%20interaction%20scale%20and%20discards%20crucial%20information%0Afrom%20other%20scales.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20a%20multi-view%0Agraph-tuple%20framework.%20Instead%20of%20a%20single%20graph%2C%20our%20graph-tuple%20framework%0Apartitions%20the%20graph%20into%20disjoint%20subgraphs%2C%20capturing%20primary%20local%0Ainteractions%20and%20weaker%2C%20long-range%20connections.%20We%20then%20learn%20multi-view%0Arepresentations%20from%20the%20graph-tuple%20via%20a%20heterogeneous%20message-passing%0Aarchitecture%20inspired%20by%20the%20theory%20of%20non-commuting%20operators%2C%20which%20we%0Aformally%20prove%20is%20strictly%20more%20expressive%20and%20guarantees%20a%20lower%20oracle%20risk%0Acompared%20to%20single-graph%20message-passing%20models.%20We%20instantiate%20our%20framework%0Aon%20two%20scientific%20domains%3A%20molecular%20property%20prediction%20from%20feature-scarce%0ACoulomb%20matrices%20and%20cosmological%20parameter%20inference%20from%20geometric%20point%0Aclouds.%20On%20both%20applications%2C%20our%20multi-view%20graph-tuple%20models%20demonstrate%0Abetter%20performance%20than%20single-graph%20baselines%2C%20highlighting%20the%20power%20and%0Aversatility%20of%20our%20multi-view%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.10341v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-View%2520Graph%2520Learning%2520with%2520Graph-Tuple%26entry.906535625%3DShiyu%2520Chen%2520and%2520Ningyuan%2520Huang%2520and%2520Soledad%2520Villar%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520typically%2520scale%2520with%2520the%2520number%2520of%2520graph%2520edges%252C%250Amaking%2520them%2520well%2520suited%2520for%2520sparse%2520graphs%2520but%2520less%2520efficient%2520on%2520dense%2520graphs%252C%250Asuch%2520as%2520point%2520clouds%2520or%2520molecular%2520interactions.%2520A%2520common%2520remedy%2520is%2520to%2520sparsify%250Athe%2520graph%2520via%2520similarity%2520thresholding%2520or%2520distance%2520pruning%252C%2520but%2520this%2520forces%2520an%250Aarbitrary%2520choice%2520of%2520a%2520single%2520interaction%2520scale%2520and%2520discards%2520crucial%2520information%250Afrom%2520other%2520scales.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%2520a%2520multi-view%250Agraph-tuple%2520framework.%2520Instead%2520of%2520a%2520single%2520graph%252C%2520our%2520graph-tuple%2520framework%250Apartitions%2520the%2520graph%2520into%2520disjoint%2520subgraphs%252C%2520capturing%2520primary%2520local%250Ainteractions%2520and%2520weaker%252C%2520long-range%2520connections.%2520We%2520then%2520learn%2520multi-view%250Arepresentations%2520from%2520the%2520graph-tuple%2520via%2520a%2520heterogeneous%2520message-passing%250Aarchitecture%2520inspired%2520by%2520the%2520theory%2520of%2520non-commuting%2520operators%252C%2520which%2520we%250Aformally%2520prove%2520is%2520strictly%2520more%2520expressive%2520and%2520guarantees%2520a%2520lower%2520oracle%2520risk%250Acompared%2520to%2520single-graph%2520message-passing%2520models.%2520We%2520instantiate%2520our%2520framework%250Aon%2520two%2520scientific%2520domains%253A%2520molecular%2520property%2520prediction%2520from%2520feature-scarce%250ACoulomb%2520matrices%2520and%2520cosmological%2520parameter%2520inference%2520from%2520geometric%2520point%250Aclouds.%2520On%2520both%2520applications%252C%2520our%2520multi-view%2520graph-tuple%2520models%2520demonstrate%250Abetter%2520performance%2520than%2520single-graph%2520baselines%252C%2520highlighting%2520the%2520power%2520and%250Aversatility%2520of%2520our%2520multi-view%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10341v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-View%20Graph%20Learning%20with%20Graph-Tuple&entry.906535625=Shiyu%20Chen%20and%20Ningyuan%20Huang%20and%20Soledad%20Villar&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20typically%20scale%20with%20the%20number%20of%20graph%20edges%2C%0Amaking%20them%20well%20suited%20for%20sparse%20graphs%20but%20less%20efficient%20on%20dense%20graphs%2C%0Asuch%20as%20point%20clouds%20or%20molecular%20interactions.%20A%20common%20remedy%20is%20to%20sparsify%0Athe%20graph%20via%20similarity%20thresholding%20or%20distance%20pruning%2C%20but%20this%20forces%20an%0Aarbitrary%20choice%20of%20a%20single%20interaction%20scale%20and%20discards%20crucial%20information%0Afrom%20other%20scales.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20a%20multi-view%0Agraph-tuple%20framework.%20Instead%20of%20a%20single%20graph%2C%20our%20graph-tuple%20framework%0Apartitions%20the%20graph%20into%20disjoint%20subgraphs%2C%20capturing%20primary%20local%0Ainteractions%20and%20weaker%2C%20long-range%20connections.%20We%20then%20learn%20multi-view%0Arepresentations%20from%20the%20graph-tuple%20via%20a%20heterogeneous%20message-passing%0Aarchitecture%20inspired%20by%20the%20theory%20of%20non-commuting%20operators%2C%20which%20we%0Aformally%20prove%20is%20strictly%20more%20expressive%20and%20guarantees%20a%20lower%20oracle%20risk%0Acompared%20to%20single-graph%20message-passing%20models.%20We%20instantiate%20our%20framework%0Aon%20two%20scientific%20domains%3A%20molecular%20property%20prediction%20from%20feature-scarce%0ACoulomb%20matrices%20and%20cosmological%20parameter%20inference%20from%20geometric%20point%0Aclouds.%20On%20both%20applications%2C%20our%20multi-view%20graph-tuple%20models%20demonstrate%0Abetter%20performance%20than%20single-graph%20baselines%2C%20highlighting%20the%20power%20and%0Aversatility%20of%20our%20multi-view%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.10341v2&entry.124074799=Read"},
{"title": "Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed\n  Perception", "author": "Ziyang Ma and Ruiyang Xu and Zhenghao Xing and Yunfei Chu and Yuxuan Wang and Jinzheng He and Jin Xu and Pheng-Ann Heng and Kai Yu and Junyang Lin and Eng Siong Chng and Xie Chen", "abstract": "  Fine-grained perception of multimodal information is critical for advancing\nhuman-AI interaction. With recent progress in audio-visual technologies, Omni\nLanguage Models (OLMs), capable of processing audio and video signals in\nparallel, have emerged as a promising paradigm for achieving richer\nunderstanding and reasoning. However, their capacity to capture and describe\nfine-grained details remains limited explored. In this work, we present a\nsystematic and comprehensive investigation of omni detailed perception from the\nperspectives of the data pipeline, models, and benchmark. We first identify an\ninherent \"co-growth\" between detail and hallucination in current OLMs. To\naddress this, we propose Omni-Detective, an agentic data generation pipeline\nintegrating tool-calling, to autonomously produce highly detailed yet minimally\nhallucinatory multimodal data. Based on the data generated with Omni-Detective,\nwe train two captioning models: Audio-Captioner for audio-only detailed\nperception, and Omni-Captioner for audio-visual detailed perception. Under the\ncascade evaluation protocol, Audio-Captioner achieves the best performance on\nMMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and\ndelivering performance comparable to Gemini 2.5 Pro. On existing detailed\ncaptioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and\nachieves the best trade-off between detail and hallucination on the\nvideo-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni\ndetailed perception, we design Omni-Cloze, a novel cloze-style evaluation for\ndetailed audio, visual, and audio-visual captioning that ensures stable,\nefficient, and reliable assessment. Experimental results and analysis\ndemonstrate the effectiveness of Omni-Detective in generating high-quality\ndetailed captions, as well as the superiority of Omni-Cloze in evaluating such\ndetailed captions.\n", "link": "http://arxiv.org/abs/2510.12720v1", "date": "2025-10-14", "relevancy": 2.5015, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.639}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.639}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omni-Captioner%3A%20Data%20Pipeline%2C%20Models%2C%20and%20Benchmark%20for%20Omni%20Detailed%0A%20%20Perception&body=Title%3A%20Omni-Captioner%3A%20Data%20Pipeline%2C%20Models%2C%20and%20Benchmark%20for%20Omni%20Detailed%0A%20%20Perception%0AAuthor%3A%20Ziyang%20Ma%20and%20Ruiyang%20Xu%20and%20Zhenghao%20Xing%20and%20Yunfei%20Chu%20and%20Yuxuan%20Wang%20and%20Jinzheng%20He%20and%20Jin%20Xu%20and%20Pheng-Ann%20Heng%20and%20Kai%20Yu%20and%20Junyang%20Lin%20and%20Eng%20Siong%20Chng%20and%20Xie%20Chen%0AAbstract%3A%20%20%20Fine-grained%20perception%20of%20multimodal%20information%20is%20critical%20for%20advancing%0Ahuman-AI%20interaction.%20With%20recent%20progress%20in%20audio-visual%20technologies%2C%20Omni%0ALanguage%20Models%20%28OLMs%29%2C%20capable%20of%20processing%20audio%20and%20video%20signals%20in%0Aparallel%2C%20have%20emerged%20as%20a%20promising%20paradigm%20for%20achieving%20richer%0Aunderstanding%20and%20reasoning.%20However%2C%20their%20capacity%20to%20capture%20and%20describe%0Afine-grained%20details%20remains%20limited%20explored.%20In%20this%20work%2C%20we%20present%20a%0Asystematic%20and%20comprehensive%20investigation%20of%20omni%20detailed%20perception%20from%20the%0Aperspectives%20of%20the%20data%20pipeline%2C%20models%2C%20and%20benchmark.%20We%20first%20identify%20an%0Ainherent%20%22co-growth%22%20between%20detail%20and%20hallucination%20in%20current%20OLMs.%20To%0Aaddress%20this%2C%20we%20propose%20Omni-Detective%2C%20an%20agentic%20data%20generation%20pipeline%0Aintegrating%20tool-calling%2C%20to%20autonomously%20produce%20highly%20detailed%20yet%20minimally%0Ahallucinatory%20multimodal%20data.%20Based%20on%20the%20data%20generated%20with%20Omni-Detective%2C%0Awe%20train%20two%20captioning%20models%3A%20Audio-Captioner%20for%20audio-only%20detailed%0Aperception%2C%20and%20Omni-Captioner%20for%20audio-visual%20detailed%20perception.%20Under%20the%0Acascade%20evaluation%20protocol%2C%20Audio-Captioner%20achieves%20the%20best%20performance%20on%0AMMAU%20and%20MMAR%20among%20all%20open-source%20models%2C%20surpassing%20Gemini%202.5%20Flash%20and%0Adelivering%20performance%20comparable%20to%20Gemini%202.5%20Pro.%20On%20existing%20detailed%0Acaptioning%20benchmarks%2C%20Omni-Captioner%20sets%20a%20new%20state-of-the-art%20on%20VDC%20and%0Aachieves%20the%20best%20trade-off%20between%20detail%20and%20hallucination%20on%20the%0Avideo-SALMONN%202%20testset.%20Given%20the%20absence%20of%20a%20dedicated%20benchmark%20for%20omni%0Adetailed%20perception%2C%20we%20design%20Omni-Cloze%2C%20a%20novel%20cloze-style%20evaluation%20for%0Adetailed%20audio%2C%20visual%2C%20and%20audio-visual%20captioning%20that%20ensures%20stable%2C%0Aefficient%2C%20and%20reliable%20assessment.%20Experimental%20results%20and%20analysis%0Ademonstrate%20the%20effectiveness%20of%20Omni-Detective%20in%20generating%20high-quality%0Adetailed%20captions%2C%20as%20well%20as%20the%20superiority%20of%20Omni-Cloze%20in%20evaluating%20such%0Adetailed%20captions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmni-Captioner%253A%2520Data%2520Pipeline%252C%2520Models%252C%2520and%2520Benchmark%2520for%2520Omni%2520Detailed%250A%2520%2520Perception%26entry.906535625%3DZiyang%2520Ma%2520and%2520Ruiyang%2520Xu%2520and%2520Zhenghao%2520Xing%2520and%2520Yunfei%2520Chu%2520and%2520Yuxuan%2520Wang%2520and%2520Jinzheng%2520He%2520and%2520Jin%2520Xu%2520and%2520Pheng-Ann%2520Heng%2520and%2520Kai%2520Yu%2520and%2520Junyang%2520Lin%2520and%2520Eng%2520Siong%2520Chng%2520and%2520Xie%2520Chen%26entry.1292438233%3D%2520%2520Fine-grained%2520perception%2520of%2520multimodal%2520information%2520is%2520critical%2520for%2520advancing%250Ahuman-AI%2520interaction.%2520With%2520recent%2520progress%2520in%2520audio-visual%2520technologies%252C%2520Omni%250ALanguage%2520Models%2520%2528OLMs%2529%252C%2520capable%2520of%2520processing%2520audio%2520and%2520video%2520signals%2520in%250Aparallel%252C%2520have%2520emerged%2520as%2520a%2520promising%2520paradigm%2520for%2520achieving%2520richer%250Aunderstanding%2520and%2520reasoning.%2520However%252C%2520their%2520capacity%2520to%2520capture%2520and%2520describe%250Afine-grained%2520details%2520remains%2520limited%2520explored.%2520In%2520this%2520work%252C%2520we%2520present%2520a%250Asystematic%2520and%2520comprehensive%2520investigation%2520of%2520omni%2520detailed%2520perception%2520from%2520the%250Aperspectives%2520of%2520the%2520data%2520pipeline%252C%2520models%252C%2520and%2520benchmark.%2520We%2520first%2520identify%2520an%250Ainherent%2520%2522co-growth%2522%2520between%2520detail%2520and%2520hallucination%2520in%2520current%2520OLMs.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520Omni-Detective%252C%2520an%2520agentic%2520data%2520generation%2520pipeline%250Aintegrating%2520tool-calling%252C%2520to%2520autonomously%2520produce%2520highly%2520detailed%2520yet%2520minimally%250Ahallucinatory%2520multimodal%2520data.%2520Based%2520on%2520the%2520data%2520generated%2520with%2520Omni-Detective%252C%250Awe%2520train%2520two%2520captioning%2520models%253A%2520Audio-Captioner%2520for%2520audio-only%2520detailed%250Aperception%252C%2520and%2520Omni-Captioner%2520for%2520audio-visual%2520detailed%2520perception.%2520Under%2520the%250Acascade%2520evaluation%2520protocol%252C%2520Audio-Captioner%2520achieves%2520the%2520best%2520performance%2520on%250AMMAU%2520and%2520MMAR%2520among%2520all%2520open-source%2520models%252C%2520surpassing%2520Gemini%25202.5%2520Flash%2520and%250Adelivering%2520performance%2520comparable%2520to%2520Gemini%25202.5%2520Pro.%2520On%2520existing%2520detailed%250Acaptioning%2520benchmarks%252C%2520Omni-Captioner%2520sets%2520a%2520new%2520state-of-the-art%2520on%2520VDC%2520and%250Aachieves%2520the%2520best%2520trade-off%2520between%2520detail%2520and%2520hallucination%2520on%2520the%250Avideo-SALMONN%25202%2520testset.%2520Given%2520the%2520absence%2520of%2520a%2520dedicated%2520benchmark%2520for%2520omni%250Adetailed%2520perception%252C%2520we%2520design%2520Omni-Cloze%252C%2520a%2520novel%2520cloze-style%2520evaluation%2520for%250Adetailed%2520audio%252C%2520visual%252C%2520and%2520audio-visual%2520captioning%2520that%2520ensures%2520stable%252C%250Aefficient%252C%2520and%2520reliable%2520assessment.%2520Experimental%2520results%2520and%2520analysis%250Ademonstrate%2520the%2520effectiveness%2520of%2520Omni-Detective%2520in%2520generating%2520high-quality%250Adetailed%2520captions%252C%2520as%2520well%2520as%2520the%2520superiority%2520of%2520Omni-Cloze%2520in%2520evaluating%2520such%250Adetailed%2520captions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omni-Captioner%3A%20Data%20Pipeline%2C%20Models%2C%20and%20Benchmark%20for%20Omni%20Detailed%0A%20%20Perception&entry.906535625=Ziyang%20Ma%20and%20Ruiyang%20Xu%20and%20Zhenghao%20Xing%20and%20Yunfei%20Chu%20and%20Yuxuan%20Wang%20and%20Jinzheng%20He%20and%20Jin%20Xu%20and%20Pheng-Ann%20Heng%20and%20Kai%20Yu%20and%20Junyang%20Lin%20and%20Eng%20Siong%20Chng%20and%20Xie%20Chen&entry.1292438233=%20%20Fine-grained%20perception%20of%20multimodal%20information%20is%20critical%20for%20advancing%0Ahuman-AI%20interaction.%20With%20recent%20progress%20in%20audio-visual%20technologies%2C%20Omni%0ALanguage%20Models%20%28OLMs%29%2C%20capable%20of%20processing%20audio%20and%20video%20signals%20in%0Aparallel%2C%20have%20emerged%20as%20a%20promising%20paradigm%20for%20achieving%20richer%0Aunderstanding%20and%20reasoning.%20However%2C%20their%20capacity%20to%20capture%20and%20describe%0Afine-grained%20details%20remains%20limited%20explored.%20In%20this%20work%2C%20we%20present%20a%0Asystematic%20and%20comprehensive%20investigation%20of%20omni%20detailed%20perception%20from%20the%0Aperspectives%20of%20the%20data%20pipeline%2C%20models%2C%20and%20benchmark.%20We%20first%20identify%20an%0Ainherent%20%22co-growth%22%20between%20detail%20and%20hallucination%20in%20current%20OLMs.%20To%0Aaddress%20this%2C%20we%20propose%20Omni-Detective%2C%20an%20agentic%20data%20generation%20pipeline%0Aintegrating%20tool-calling%2C%20to%20autonomously%20produce%20highly%20detailed%20yet%20minimally%0Ahallucinatory%20multimodal%20data.%20Based%20on%20the%20data%20generated%20with%20Omni-Detective%2C%0Awe%20train%20two%20captioning%20models%3A%20Audio-Captioner%20for%20audio-only%20detailed%0Aperception%2C%20and%20Omni-Captioner%20for%20audio-visual%20detailed%20perception.%20Under%20the%0Acascade%20evaluation%20protocol%2C%20Audio-Captioner%20achieves%20the%20best%20performance%20on%0AMMAU%20and%20MMAR%20among%20all%20open-source%20models%2C%20surpassing%20Gemini%202.5%20Flash%20and%0Adelivering%20performance%20comparable%20to%20Gemini%202.5%20Pro.%20On%20existing%20detailed%0Acaptioning%20benchmarks%2C%20Omni-Captioner%20sets%20a%20new%20state-of-the-art%20on%20VDC%20and%0Aachieves%20the%20best%20trade-off%20between%20detail%20and%20hallucination%20on%20the%0Avideo-SALMONN%202%20testset.%20Given%20the%20absence%20of%20a%20dedicated%20benchmark%20for%20omni%0Adetailed%20perception%2C%20we%20design%20Omni-Cloze%2C%20a%20novel%20cloze-style%20evaluation%20for%0Adetailed%20audio%2C%20visual%2C%20and%20audio-visual%20captioning%20that%20ensures%20stable%2C%0Aefficient%2C%20and%20reliable%20assessment.%20Experimental%20results%20and%20analysis%0Ademonstrate%20the%20effectiveness%20of%20Omni-Detective%20in%20generating%20high-quality%0Adetailed%20captions%2C%20as%20well%20as%20the%20superiority%20of%20Omni-Cloze%20in%20evaluating%20such%0Adetailed%20captions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12720v1&entry.124074799=Read"},
{"title": "Dimension Reduction with Locally Adjusted Graphs", "author": "Yingfan Wang and Yiyang Sun and Haiyang Huang and Cynthia Rudin", "abstract": "  Dimension reduction (DR) algorithms have proven to be extremely useful for\ngaining insight into large-scale high-dimensional datasets, particularly\nfinding clusters in transcriptomic data. The initial phase of these DR methods\noften involves converting the original high-dimensional data into a graph. In\nthis graph, each edge represents the similarity or dissimilarity between pairs\nof data points. However, this graph is frequently suboptimal due to unreliable\nhigh-dimensional distances and the limited information extracted from the\nhigh-dimensional data. This problem is exacerbated as the dataset size\nincreases. If we reduce the size of the dataset by selecting points for a\nspecific sections of the embeddings, the clusters observed through DR are more\nseparable since the extracted subgraphs are more reliable. In this paper, we\nintroduce LocalMAP, a new dimensionality reduction algorithm that dynamically\nand locally adjusts the graph to address this challenge. By dynamically\nextracting subgraphs and updating the graph on-the-fly, LocalMAP is capable of\nidentifying and separating real clusters within the data that other DR methods\nmay overlook or combine. We demonstrate the benefits of LocalMAP through a case\nstudy on biological datasets, highlighting its utility in helping users more\naccurately identify clusters for real-world problems.\n", "link": "http://arxiv.org/abs/2412.15426v2", "date": "2025-10-14", "relevancy": 2.4957, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5228}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.49}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dimension%20Reduction%20with%20Locally%20Adjusted%20Graphs&body=Title%3A%20Dimension%20Reduction%20with%20Locally%20Adjusted%20Graphs%0AAuthor%3A%20Yingfan%20Wang%20and%20Yiyang%20Sun%20and%20Haiyang%20Huang%20and%20Cynthia%20Rudin%0AAbstract%3A%20%20%20Dimension%20reduction%20%28DR%29%20algorithms%20have%20proven%20to%20be%20extremely%20useful%20for%0Againing%20insight%20into%20large-scale%20high-dimensional%20datasets%2C%20particularly%0Afinding%20clusters%20in%20transcriptomic%20data.%20The%20initial%20phase%20of%20these%20DR%20methods%0Aoften%20involves%20converting%20the%20original%20high-dimensional%20data%20into%20a%20graph.%20In%0Athis%20graph%2C%20each%20edge%20represents%20the%20similarity%20or%20dissimilarity%20between%20pairs%0Aof%20data%20points.%20However%2C%20this%20graph%20is%20frequently%20suboptimal%20due%20to%20unreliable%0Ahigh-dimensional%20distances%20and%20the%20limited%20information%20extracted%20from%20the%0Ahigh-dimensional%20data.%20This%20problem%20is%20exacerbated%20as%20the%20dataset%20size%0Aincreases.%20If%20we%20reduce%20the%20size%20of%20the%20dataset%20by%20selecting%20points%20for%20a%0Aspecific%20sections%20of%20the%20embeddings%2C%20the%20clusters%20observed%20through%20DR%20are%20more%0Aseparable%20since%20the%20extracted%20subgraphs%20are%20more%20reliable.%20In%20this%20paper%2C%20we%0Aintroduce%20LocalMAP%2C%20a%20new%20dimensionality%20reduction%20algorithm%20that%20dynamically%0Aand%20locally%20adjusts%20the%20graph%20to%20address%20this%20challenge.%20By%20dynamically%0Aextracting%20subgraphs%20and%20updating%20the%20graph%20on-the-fly%2C%20LocalMAP%20is%20capable%20of%0Aidentifying%20and%20separating%20real%20clusters%20within%20the%20data%20that%20other%20DR%20methods%0Amay%20overlook%20or%20combine.%20We%20demonstrate%20the%20benefits%20of%20LocalMAP%20through%20a%20case%0Astudy%20on%20biological%20datasets%2C%20highlighting%20its%20utility%20in%20helping%20users%20more%0Aaccurately%20identify%20clusters%20for%20real-world%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15426v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDimension%2520Reduction%2520with%2520Locally%2520Adjusted%2520Graphs%26entry.906535625%3DYingfan%2520Wang%2520and%2520Yiyang%2520Sun%2520and%2520Haiyang%2520Huang%2520and%2520Cynthia%2520Rudin%26entry.1292438233%3D%2520%2520Dimension%2520reduction%2520%2528DR%2529%2520algorithms%2520have%2520proven%2520to%2520be%2520extremely%2520useful%2520for%250Againing%2520insight%2520into%2520large-scale%2520high-dimensional%2520datasets%252C%2520particularly%250Afinding%2520clusters%2520in%2520transcriptomic%2520data.%2520The%2520initial%2520phase%2520of%2520these%2520DR%2520methods%250Aoften%2520involves%2520converting%2520the%2520original%2520high-dimensional%2520data%2520into%2520a%2520graph.%2520In%250Athis%2520graph%252C%2520each%2520edge%2520represents%2520the%2520similarity%2520or%2520dissimilarity%2520between%2520pairs%250Aof%2520data%2520points.%2520However%252C%2520this%2520graph%2520is%2520frequently%2520suboptimal%2520due%2520to%2520unreliable%250Ahigh-dimensional%2520distances%2520and%2520the%2520limited%2520information%2520extracted%2520from%2520the%250Ahigh-dimensional%2520data.%2520This%2520problem%2520is%2520exacerbated%2520as%2520the%2520dataset%2520size%250Aincreases.%2520If%2520we%2520reduce%2520the%2520size%2520of%2520the%2520dataset%2520by%2520selecting%2520points%2520for%2520a%250Aspecific%2520sections%2520of%2520the%2520embeddings%252C%2520the%2520clusters%2520observed%2520through%2520DR%2520are%2520more%250Aseparable%2520since%2520the%2520extracted%2520subgraphs%2520are%2520more%2520reliable.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520LocalMAP%252C%2520a%2520new%2520dimensionality%2520reduction%2520algorithm%2520that%2520dynamically%250Aand%2520locally%2520adjusts%2520the%2520graph%2520to%2520address%2520this%2520challenge.%2520By%2520dynamically%250Aextracting%2520subgraphs%2520and%2520updating%2520the%2520graph%2520on-the-fly%252C%2520LocalMAP%2520is%2520capable%2520of%250Aidentifying%2520and%2520separating%2520real%2520clusters%2520within%2520the%2520data%2520that%2520other%2520DR%2520methods%250Amay%2520overlook%2520or%2520combine.%2520We%2520demonstrate%2520the%2520benefits%2520of%2520LocalMAP%2520through%2520a%2520case%250Astudy%2520on%2520biological%2520datasets%252C%2520highlighting%2520its%2520utility%2520in%2520helping%2520users%2520more%250Aaccurately%2520identify%2520clusters%2520for%2520real-world%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15426v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dimension%20Reduction%20with%20Locally%20Adjusted%20Graphs&entry.906535625=Yingfan%20Wang%20and%20Yiyang%20Sun%20and%20Haiyang%20Huang%20and%20Cynthia%20Rudin&entry.1292438233=%20%20Dimension%20reduction%20%28DR%29%20algorithms%20have%20proven%20to%20be%20extremely%20useful%20for%0Againing%20insight%20into%20large-scale%20high-dimensional%20datasets%2C%20particularly%0Afinding%20clusters%20in%20transcriptomic%20data.%20The%20initial%20phase%20of%20these%20DR%20methods%0Aoften%20involves%20converting%20the%20original%20high-dimensional%20data%20into%20a%20graph.%20In%0Athis%20graph%2C%20each%20edge%20represents%20the%20similarity%20or%20dissimilarity%20between%20pairs%0Aof%20data%20points.%20However%2C%20this%20graph%20is%20frequently%20suboptimal%20due%20to%20unreliable%0Ahigh-dimensional%20distances%20and%20the%20limited%20information%20extracted%20from%20the%0Ahigh-dimensional%20data.%20This%20problem%20is%20exacerbated%20as%20the%20dataset%20size%0Aincreases.%20If%20we%20reduce%20the%20size%20of%20the%20dataset%20by%20selecting%20points%20for%20a%0Aspecific%20sections%20of%20the%20embeddings%2C%20the%20clusters%20observed%20through%20DR%20are%20more%0Aseparable%20since%20the%20extracted%20subgraphs%20are%20more%20reliable.%20In%20this%20paper%2C%20we%0Aintroduce%20LocalMAP%2C%20a%20new%20dimensionality%20reduction%20algorithm%20that%20dynamically%0Aand%20locally%20adjusts%20the%20graph%20to%20address%20this%20challenge.%20By%20dynamically%0Aextracting%20subgraphs%20and%20updating%20the%20graph%20on-the-fly%2C%20LocalMAP%20is%20capable%20of%0Aidentifying%20and%20separating%20real%20clusters%20within%20the%20data%20that%20other%20DR%20methods%0Amay%20overlook%20or%20combine.%20We%20demonstrate%20the%20benefits%20of%20LocalMAP%20through%20a%20case%0Astudy%20on%20biological%20datasets%2C%20highlighting%20its%20utility%20in%20helping%20users%20more%0Aaccurately%20identify%20clusters%20for%20real-world%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15426v2&entry.124074799=Read"},
{"title": "SPORTS: Simultaneous Panoptic Odometry, Rendering, Tracking and\n  Segmentation for Urban Scenes Understanding", "author": "Zhiliu Yang and Jinyu Dai and Jianyuan Zhang and Zhu Yang", "abstract": "  The scene perception, understanding, and simulation are fundamental\ntechniques for embodied-AI agents, while existing solutions are still prone to\nsegmentation deficiency, dynamic objects' interference, sensor data sparsity,\nand view-limitation problems. This paper proposes a novel framework, named\nSPORTS, for holistic scene understanding via tightly integrating Video Panoptic\nSegmentation (VPS), Visual Odometry (VO), and Scene Rendering (SR) tasks into\nan iterative and unified perspective. Firstly, VPS designs an adaptive\nattention-based geometric fusion mechanism to align cross-frame features via\nenrolling the pose, depth, and optical flow modality, which automatically\nadjust feature maps for different decoding stages. And a post-matching strategy\nis integrated to improve identities tracking. In VO, panoptic segmentation\nresults from VPS are combined with the optical flow map to improve the\nconfidence estimation of dynamic objects, which enhances the accuracy of the\ncamera pose estimation and completeness of the depth map generation via the\nlearning-based paradigm. Furthermore, the point-based rendering of SR is\nbeneficial from VO, transforming sparse point clouds into neural fields to\nsynthesize high-fidelity RGB views and twin panoptic views. Extensive\nexperiments on three public datasets demonstrate that our attention-based\nfeature fusion outperforms most existing state-of-the-art methods on the\nodometry, tracking, segmentation, and novel view synthesis tasks.\n", "link": "http://arxiv.org/abs/2510.12749v1", "date": "2025-10-14", "relevancy": 2.4644, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.621}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPORTS%3A%20Simultaneous%20Panoptic%20Odometry%2C%20Rendering%2C%20Tracking%20and%0A%20%20Segmentation%20for%20Urban%20Scenes%20Understanding&body=Title%3A%20SPORTS%3A%20Simultaneous%20Panoptic%20Odometry%2C%20Rendering%2C%20Tracking%20and%0A%20%20Segmentation%20for%20Urban%20Scenes%20Understanding%0AAuthor%3A%20Zhiliu%20Yang%20and%20Jinyu%20Dai%20and%20Jianyuan%20Zhang%20and%20Zhu%20Yang%0AAbstract%3A%20%20%20The%20scene%20perception%2C%20understanding%2C%20and%20simulation%20are%20fundamental%0Atechniques%20for%20embodied-AI%20agents%2C%20while%20existing%20solutions%20are%20still%20prone%20to%0Asegmentation%20deficiency%2C%20dynamic%20objects%27%20interference%2C%20sensor%20data%20sparsity%2C%0Aand%20view-limitation%20problems.%20This%20paper%20proposes%20a%20novel%20framework%2C%20named%0ASPORTS%2C%20for%20holistic%20scene%20understanding%20via%20tightly%20integrating%20Video%20Panoptic%0ASegmentation%20%28VPS%29%2C%20Visual%20Odometry%20%28VO%29%2C%20and%20Scene%20Rendering%20%28SR%29%20tasks%20into%0Aan%20iterative%20and%20unified%20perspective.%20Firstly%2C%20VPS%20designs%20an%20adaptive%0Aattention-based%20geometric%20fusion%20mechanism%20to%20align%20cross-frame%20features%20via%0Aenrolling%20the%20pose%2C%20depth%2C%20and%20optical%20flow%20modality%2C%20which%20automatically%0Aadjust%20feature%20maps%20for%20different%20decoding%20stages.%20And%20a%20post-matching%20strategy%0Ais%20integrated%20to%20improve%20identities%20tracking.%20In%20VO%2C%20panoptic%20segmentation%0Aresults%20from%20VPS%20are%20combined%20with%20the%20optical%20flow%20map%20to%20improve%20the%0Aconfidence%20estimation%20of%20dynamic%20objects%2C%20which%20enhances%20the%20accuracy%20of%20the%0Acamera%20pose%20estimation%20and%20completeness%20of%20the%20depth%20map%20generation%20via%20the%0Alearning-based%20paradigm.%20Furthermore%2C%20the%20point-based%20rendering%20of%20SR%20is%0Abeneficial%20from%20VO%2C%20transforming%20sparse%20point%20clouds%20into%20neural%20fields%20to%0Asynthesize%20high-fidelity%20RGB%20views%20and%20twin%20panoptic%20views.%20Extensive%0Aexperiments%20on%20three%20public%20datasets%20demonstrate%20that%20our%20attention-based%0Afeature%20fusion%20outperforms%20most%20existing%20state-of-the-art%20methods%20on%20the%0Aodometry%2C%20tracking%2C%20segmentation%2C%20and%20novel%20view%20synthesis%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPORTS%253A%2520Simultaneous%2520Panoptic%2520Odometry%252C%2520Rendering%252C%2520Tracking%2520and%250A%2520%2520Segmentation%2520for%2520Urban%2520Scenes%2520Understanding%26entry.906535625%3DZhiliu%2520Yang%2520and%2520Jinyu%2520Dai%2520and%2520Jianyuan%2520Zhang%2520and%2520Zhu%2520Yang%26entry.1292438233%3D%2520%2520The%2520scene%2520perception%252C%2520understanding%252C%2520and%2520simulation%2520are%2520fundamental%250Atechniques%2520for%2520embodied-AI%2520agents%252C%2520while%2520existing%2520solutions%2520are%2520still%2520prone%2520to%250Asegmentation%2520deficiency%252C%2520dynamic%2520objects%2527%2520interference%252C%2520sensor%2520data%2520sparsity%252C%250Aand%2520view-limitation%2520problems.%2520This%2520paper%2520proposes%2520a%2520novel%2520framework%252C%2520named%250ASPORTS%252C%2520for%2520holistic%2520scene%2520understanding%2520via%2520tightly%2520integrating%2520Video%2520Panoptic%250ASegmentation%2520%2528VPS%2529%252C%2520Visual%2520Odometry%2520%2528VO%2529%252C%2520and%2520Scene%2520Rendering%2520%2528SR%2529%2520tasks%2520into%250Aan%2520iterative%2520and%2520unified%2520perspective.%2520Firstly%252C%2520VPS%2520designs%2520an%2520adaptive%250Aattention-based%2520geometric%2520fusion%2520mechanism%2520to%2520align%2520cross-frame%2520features%2520via%250Aenrolling%2520the%2520pose%252C%2520depth%252C%2520and%2520optical%2520flow%2520modality%252C%2520which%2520automatically%250Aadjust%2520feature%2520maps%2520for%2520different%2520decoding%2520stages.%2520And%2520a%2520post-matching%2520strategy%250Ais%2520integrated%2520to%2520improve%2520identities%2520tracking.%2520In%2520VO%252C%2520panoptic%2520segmentation%250Aresults%2520from%2520VPS%2520are%2520combined%2520with%2520the%2520optical%2520flow%2520map%2520to%2520improve%2520the%250Aconfidence%2520estimation%2520of%2520dynamic%2520objects%252C%2520which%2520enhances%2520the%2520accuracy%2520of%2520the%250Acamera%2520pose%2520estimation%2520and%2520completeness%2520of%2520the%2520depth%2520map%2520generation%2520via%2520the%250Alearning-based%2520paradigm.%2520Furthermore%252C%2520the%2520point-based%2520rendering%2520of%2520SR%2520is%250Abeneficial%2520from%2520VO%252C%2520transforming%2520sparse%2520point%2520clouds%2520into%2520neural%2520fields%2520to%250Asynthesize%2520high-fidelity%2520RGB%2520views%2520and%2520twin%2520panoptic%2520views.%2520Extensive%250Aexperiments%2520on%2520three%2520public%2520datasets%2520demonstrate%2520that%2520our%2520attention-based%250Afeature%2520fusion%2520outperforms%2520most%2520existing%2520state-of-the-art%2520methods%2520on%2520the%250Aodometry%252C%2520tracking%252C%2520segmentation%252C%2520and%2520novel%2520view%2520synthesis%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPORTS%3A%20Simultaneous%20Panoptic%20Odometry%2C%20Rendering%2C%20Tracking%20and%0A%20%20Segmentation%20for%20Urban%20Scenes%20Understanding&entry.906535625=Zhiliu%20Yang%20and%20Jinyu%20Dai%20and%20Jianyuan%20Zhang%20and%20Zhu%20Yang&entry.1292438233=%20%20The%20scene%20perception%2C%20understanding%2C%20and%20simulation%20are%20fundamental%0Atechniques%20for%20embodied-AI%20agents%2C%20while%20existing%20solutions%20are%20still%20prone%20to%0Asegmentation%20deficiency%2C%20dynamic%20objects%27%20interference%2C%20sensor%20data%20sparsity%2C%0Aand%20view-limitation%20problems.%20This%20paper%20proposes%20a%20novel%20framework%2C%20named%0ASPORTS%2C%20for%20holistic%20scene%20understanding%20via%20tightly%20integrating%20Video%20Panoptic%0ASegmentation%20%28VPS%29%2C%20Visual%20Odometry%20%28VO%29%2C%20and%20Scene%20Rendering%20%28SR%29%20tasks%20into%0Aan%20iterative%20and%20unified%20perspective.%20Firstly%2C%20VPS%20designs%20an%20adaptive%0Aattention-based%20geometric%20fusion%20mechanism%20to%20align%20cross-frame%20features%20via%0Aenrolling%20the%20pose%2C%20depth%2C%20and%20optical%20flow%20modality%2C%20which%20automatically%0Aadjust%20feature%20maps%20for%20different%20decoding%20stages.%20And%20a%20post-matching%20strategy%0Ais%20integrated%20to%20improve%20identities%20tracking.%20In%20VO%2C%20panoptic%20segmentation%0Aresults%20from%20VPS%20are%20combined%20with%20the%20optical%20flow%20map%20to%20improve%20the%0Aconfidence%20estimation%20of%20dynamic%20objects%2C%20which%20enhances%20the%20accuracy%20of%20the%0Acamera%20pose%20estimation%20and%20completeness%20of%20the%20depth%20map%20generation%20via%20the%0Alearning-based%20paradigm.%20Furthermore%2C%20the%20point-based%20rendering%20of%20SR%20is%0Abeneficial%20from%20VO%2C%20transforming%20sparse%20point%20clouds%20into%20neural%20fields%20to%0Asynthesize%20high-fidelity%20RGB%20views%20and%20twin%20panoptic%20views.%20Extensive%0Aexperiments%20on%20three%20public%20datasets%20demonstrate%20that%20our%20attention-based%0Afeature%20fusion%20outperforms%20most%20existing%20state-of-the-art%20methods%20on%20the%0Aodometry%2C%20tracking%2C%20segmentation%2C%20and%20novel%20view%20synthesis%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12749v1&entry.124074799=Read"},
{"title": "A Review of Longitudinal Radiology Report Generation: Dataset\n  Composition, Methods, and Performance Evaluation", "author": "Shaoyang Zhou and Yingshu Li and Yunyi Liu and Lingqiao Liu and Lei Wang and Luping Zhou", "abstract": "  Chest Xray imaging is a widely used diagnostic tool in modern medicine, and\nits high utilization creates substantial workloads for radiologists. To\nalleviate this burden, vision language models are increasingly applied to\nautomate Chest Xray radiology report generation (CXRRRG), aiming for clinically\naccurate descriptions while reducing manual effort. Conventional approaches,\nhowever, typically rely on single images, failing to capture the longitudinal\ncontext necessary for producing clinically faithful comparison statements.\nRecently, growing attention has been directed toward incorporating longitudinal\ndata into CXR RRG, enabling models to leverage historical studies in ways that\nmirror radiologists diagnostic workflows. Nevertheless, existing surveys\nprimarily address single image CXRRRG and offer limited guidance for\nlongitudinal settings, leaving researchers without a systematic framework for\nmodel design. To address this gap, this survey provides the first comprehensive\nreview of longitudinal radiology report generation (LRRG). Specifically, we\nexamine dataset construction strategies, report generation architectures\nalongside longitudinally tailored designs, and evaluation protocols\nencompassing both longitudinal specific measures and widely used benchmarks. We\nfurther summarize LRRG methods performance, alongside analyses of different\nablation studies, which collectively highlight the critical role of\nlongitudinal information and architectural design choices in improving model\nperformance. Finally, we summarize five major limitations of current research\nand outline promising directions for future development, aiming to lay a\nfoundation for advancing this emerging field.\n", "link": "http://arxiv.org/abs/2510.12444v1", "date": "2025-10-14", "relevancy": 2.4513, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4912}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4898}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Review%20of%20Longitudinal%20Radiology%20Report%20Generation%3A%20Dataset%0A%20%20Composition%2C%20Methods%2C%20and%20Performance%20Evaluation&body=Title%3A%20A%20Review%20of%20Longitudinal%20Radiology%20Report%20Generation%3A%20Dataset%0A%20%20Composition%2C%20Methods%2C%20and%20Performance%20Evaluation%0AAuthor%3A%20Shaoyang%20Zhou%20and%20Yingshu%20Li%20and%20Yunyi%20Liu%20and%20Lingqiao%20Liu%20and%20Lei%20Wang%20and%20Luping%20Zhou%0AAbstract%3A%20%20%20Chest%20Xray%20imaging%20is%20a%20widely%20used%20diagnostic%20tool%20in%20modern%20medicine%2C%20and%0Aits%20high%20utilization%20creates%20substantial%20workloads%20for%20radiologists.%20To%0Aalleviate%20this%20burden%2C%20vision%20language%20models%20are%20increasingly%20applied%20to%0Aautomate%20Chest%20Xray%20radiology%20report%20generation%20%28CXRRRG%29%2C%20aiming%20for%20clinically%0Aaccurate%20descriptions%20while%20reducing%20manual%20effort.%20Conventional%20approaches%2C%0Ahowever%2C%20typically%20rely%20on%20single%20images%2C%20failing%20to%20capture%20the%20longitudinal%0Acontext%20necessary%20for%20producing%20clinically%20faithful%20comparison%20statements.%0ARecently%2C%20growing%20attention%20has%20been%20directed%20toward%20incorporating%20longitudinal%0Adata%20into%20CXR%20RRG%2C%20enabling%20models%20to%20leverage%20historical%20studies%20in%20ways%20that%0Amirror%20radiologists%20diagnostic%20workflows.%20Nevertheless%2C%20existing%20surveys%0Aprimarily%20address%20single%20image%20CXRRRG%20and%20offer%20limited%20guidance%20for%0Alongitudinal%20settings%2C%20leaving%20researchers%20without%20a%20systematic%20framework%20for%0Amodel%20design.%20To%20address%20this%20gap%2C%20this%20survey%20provides%20the%20first%20comprehensive%0Areview%20of%20longitudinal%20radiology%20report%20generation%20%28LRRG%29.%20Specifically%2C%20we%0Aexamine%20dataset%20construction%20strategies%2C%20report%20generation%20architectures%0Aalongside%20longitudinally%20tailored%20designs%2C%20and%20evaluation%20protocols%0Aencompassing%20both%20longitudinal%20specific%20measures%20and%20widely%20used%20benchmarks.%20We%0Afurther%20summarize%20LRRG%20methods%20performance%2C%20alongside%20analyses%20of%20different%0Aablation%20studies%2C%20which%20collectively%20highlight%20the%20critical%20role%20of%0Alongitudinal%20information%20and%20architectural%20design%20choices%20in%20improving%20model%0Aperformance.%20Finally%2C%20we%20summarize%20five%20major%20limitations%20of%20current%20research%0Aand%20outline%20promising%20directions%20for%20future%20development%2C%20aiming%20to%20lay%20a%0Afoundation%20for%20advancing%20this%20emerging%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Review%2520of%2520Longitudinal%2520Radiology%2520Report%2520Generation%253A%2520Dataset%250A%2520%2520Composition%252C%2520Methods%252C%2520and%2520Performance%2520Evaluation%26entry.906535625%3DShaoyang%2520Zhou%2520and%2520Yingshu%2520Li%2520and%2520Yunyi%2520Liu%2520and%2520Lingqiao%2520Liu%2520and%2520Lei%2520Wang%2520and%2520Luping%2520Zhou%26entry.1292438233%3D%2520%2520Chest%2520Xray%2520imaging%2520is%2520a%2520widely%2520used%2520diagnostic%2520tool%2520in%2520modern%2520medicine%252C%2520and%250Aits%2520high%2520utilization%2520creates%2520substantial%2520workloads%2520for%2520radiologists.%2520To%250Aalleviate%2520this%2520burden%252C%2520vision%2520language%2520models%2520are%2520increasingly%2520applied%2520to%250Aautomate%2520Chest%2520Xray%2520radiology%2520report%2520generation%2520%2528CXRRRG%2529%252C%2520aiming%2520for%2520clinically%250Aaccurate%2520descriptions%2520while%2520reducing%2520manual%2520effort.%2520Conventional%2520approaches%252C%250Ahowever%252C%2520typically%2520rely%2520on%2520single%2520images%252C%2520failing%2520to%2520capture%2520the%2520longitudinal%250Acontext%2520necessary%2520for%2520producing%2520clinically%2520faithful%2520comparison%2520statements.%250ARecently%252C%2520growing%2520attention%2520has%2520been%2520directed%2520toward%2520incorporating%2520longitudinal%250Adata%2520into%2520CXR%2520RRG%252C%2520enabling%2520models%2520to%2520leverage%2520historical%2520studies%2520in%2520ways%2520that%250Amirror%2520radiologists%2520diagnostic%2520workflows.%2520Nevertheless%252C%2520existing%2520surveys%250Aprimarily%2520address%2520single%2520image%2520CXRRRG%2520and%2520offer%2520limited%2520guidance%2520for%250Alongitudinal%2520settings%252C%2520leaving%2520researchers%2520without%2520a%2520systematic%2520framework%2520for%250Amodel%2520design.%2520To%2520address%2520this%2520gap%252C%2520this%2520survey%2520provides%2520the%2520first%2520comprehensive%250Areview%2520of%2520longitudinal%2520radiology%2520report%2520generation%2520%2528LRRG%2529.%2520Specifically%252C%2520we%250Aexamine%2520dataset%2520construction%2520strategies%252C%2520report%2520generation%2520architectures%250Aalongside%2520longitudinally%2520tailored%2520designs%252C%2520and%2520evaluation%2520protocols%250Aencompassing%2520both%2520longitudinal%2520specific%2520measures%2520and%2520widely%2520used%2520benchmarks.%2520We%250Afurther%2520summarize%2520LRRG%2520methods%2520performance%252C%2520alongside%2520analyses%2520of%2520different%250Aablation%2520studies%252C%2520which%2520collectively%2520highlight%2520the%2520critical%2520role%2520of%250Alongitudinal%2520information%2520and%2520architectural%2520design%2520choices%2520in%2520improving%2520model%250Aperformance.%2520Finally%252C%2520we%2520summarize%2520five%2520major%2520limitations%2520of%2520current%2520research%250Aand%2520outline%2520promising%2520directions%2520for%2520future%2520development%252C%2520aiming%2520to%2520lay%2520a%250Afoundation%2520for%2520advancing%2520this%2520emerging%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Review%20of%20Longitudinal%20Radiology%20Report%20Generation%3A%20Dataset%0A%20%20Composition%2C%20Methods%2C%20and%20Performance%20Evaluation&entry.906535625=Shaoyang%20Zhou%20and%20Yingshu%20Li%20and%20Yunyi%20Liu%20and%20Lingqiao%20Liu%20and%20Lei%20Wang%20and%20Luping%20Zhou&entry.1292438233=%20%20Chest%20Xray%20imaging%20is%20a%20widely%20used%20diagnostic%20tool%20in%20modern%20medicine%2C%20and%0Aits%20high%20utilization%20creates%20substantial%20workloads%20for%20radiologists.%20To%0Aalleviate%20this%20burden%2C%20vision%20language%20models%20are%20increasingly%20applied%20to%0Aautomate%20Chest%20Xray%20radiology%20report%20generation%20%28CXRRRG%29%2C%20aiming%20for%20clinically%0Aaccurate%20descriptions%20while%20reducing%20manual%20effort.%20Conventional%20approaches%2C%0Ahowever%2C%20typically%20rely%20on%20single%20images%2C%20failing%20to%20capture%20the%20longitudinal%0Acontext%20necessary%20for%20producing%20clinically%20faithful%20comparison%20statements.%0ARecently%2C%20growing%20attention%20has%20been%20directed%20toward%20incorporating%20longitudinal%0Adata%20into%20CXR%20RRG%2C%20enabling%20models%20to%20leverage%20historical%20studies%20in%20ways%20that%0Amirror%20radiologists%20diagnostic%20workflows.%20Nevertheless%2C%20existing%20surveys%0Aprimarily%20address%20single%20image%20CXRRRG%20and%20offer%20limited%20guidance%20for%0Alongitudinal%20settings%2C%20leaving%20researchers%20without%20a%20systematic%20framework%20for%0Amodel%20design.%20To%20address%20this%20gap%2C%20this%20survey%20provides%20the%20first%20comprehensive%0Areview%20of%20longitudinal%20radiology%20report%20generation%20%28LRRG%29.%20Specifically%2C%20we%0Aexamine%20dataset%20construction%20strategies%2C%20report%20generation%20architectures%0Aalongside%20longitudinally%20tailored%20designs%2C%20and%20evaluation%20protocols%0Aencompassing%20both%20longitudinal%20specific%20measures%20and%20widely%20used%20benchmarks.%20We%0Afurther%20summarize%20LRRG%20methods%20performance%2C%20alongside%20analyses%20of%20different%0Aablation%20studies%2C%20which%20collectively%20highlight%20the%20critical%20role%20of%0Alongitudinal%20information%20and%20architectural%20design%20choices%20in%20improving%20model%0Aperformance.%20Finally%2C%20we%20summarize%20five%20major%20limitations%20of%20current%20research%0Aand%20outline%20promising%20directions%20for%20future%20development%2C%20aiming%20to%20lay%20a%0Afoundation%20for%20advancing%20this%20emerging%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12444v1&entry.124074799=Read"},
{"title": "Learning to sample fibers for goodness-of-fit testing", "author": "Ivan Gvozdanovi\u0107 and Sonja Petrovi\u0107", "abstract": "  We consider the problem of constructing exact goodness-of-fit tests for\ndiscrete exponential family models. This classical problem remains practically\nunsolved for many types of structured or sparse data, as it rests on a\ncomputationally difficult core task: to produce a reliable sample from lattice\npoints in a high-dimensional polytope. We translate the problem into a Markov\ndecision process and demonstrate a reinforcement learning approach for learning\n`good moves' for sampling. We illustrate the approach on data sets and models\nfor which traditional MCMC samplers converge too slowly due to problem size,\nsparsity structure, and the requirement to use prohibitive non-linear algebra\ncomputations in the process. The differentiating factor is the use of scalable\ntools from \\emph{linear} algebra in the context of theoretical guarantees\nprovided by \\emph{non-linear} algebra. Our algorithm is based on an\nactor-critic sampling scheme, with provable convergence.\n  The discovered moves can be used to efficiently obtain an exchangeable\nsample, significantly cutting computational times with regards to statistical\ntesting.\n", "link": "http://arxiv.org/abs/2405.13950v4", "date": "2025-10-14", "relevancy": 2.4417, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5193}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4776}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20sample%20fibers%20for%20goodness-of-fit%20testing&body=Title%3A%20Learning%20to%20sample%20fibers%20for%20goodness-of-fit%20testing%0AAuthor%3A%20Ivan%20Gvozdanovi%C4%87%20and%20Sonja%20Petrovi%C4%87%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20constructing%20exact%20goodness-of-fit%20tests%20for%0Adiscrete%20exponential%20family%20models.%20This%20classical%20problem%20remains%20practically%0Aunsolved%20for%20many%20types%20of%20structured%20or%20sparse%20data%2C%20as%20it%20rests%20on%20a%0Acomputationally%20difficult%20core%20task%3A%20to%20produce%20a%20reliable%20sample%20from%20lattice%0Apoints%20in%20a%20high-dimensional%20polytope.%20We%20translate%20the%20problem%20into%20a%20Markov%0Adecision%20process%20and%20demonstrate%20a%20reinforcement%20learning%20approach%20for%20learning%0A%60good%20moves%27%20for%20sampling.%20We%20illustrate%20the%20approach%20on%20data%20sets%20and%20models%0Afor%20which%20traditional%20MCMC%20samplers%20converge%20too%20slowly%20due%20to%20problem%20size%2C%0Asparsity%20structure%2C%20and%20the%20requirement%20to%20use%20prohibitive%20non-linear%20algebra%0Acomputations%20in%20the%20process.%20The%20differentiating%20factor%20is%20the%20use%20of%20scalable%0Atools%20from%20%5Cemph%7Blinear%7D%20algebra%20in%20the%20context%20of%20theoretical%20guarantees%0Aprovided%20by%20%5Cemph%7Bnon-linear%7D%20algebra.%20Our%20algorithm%20is%20based%20on%20an%0Aactor-critic%20sampling%20scheme%2C%20with%20provable%20convergence.%0A%20%20The%20discovered%20moves%20can%20be%20used%20to%20efficiently%20obtain%20an%20exchangeable%0Asample%2C%20significantly%20cutting%20computational%20times%20with%20regards%20to%20statistical%0Atesting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13950v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520sample%2520fibers%2520for%2520goodness-of-fit%2520testing%26entry.906535625%3DIvan%2520Gvozdanovi%25C4%2587%2520and%2520Sonja%2520Petrovi%25C4%2587%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520constructing%2520exact%2520goodness-of-fit%2520tests%2520for%250Adiscrete%2520exponential%2520family%2520models.%2520This%2520classical%2520problem%2520remains%2520practically%250Aunsolved%2520for%2520many%2520types%2520of%2520structured%2520or%2520sparse%2520data%252C%2520as%2520it%2520rests%2520on%2520a%250Acomputationally%2520difficult%2520core%2520task%253A%2520to%2520produce%2520a%2520reliable%2520sample%2520from%2520lattice%250Apoints%2520in%2520a%2520high-dimensional%2520polytope.%2520We%2520translate%2520the%2520problem%2520into%2520a%2520Markov%250Adecision%2520process%2520and%2520demonstrate%2520a%2520reinforcement%2520learning%2520approach%2520for%2520learning%250A%2560good%2520moves%2527%2520for%2520sampling.%2520We%2520illustrate%2520the%2520approach%2520on%2520data%2520sets%2520and%2520models%250Afor%2520which%2520traditional%2520MCMC%2520samplers%2520converge%2520too%2520slowly%2520due%2520to%2520problem%2520size%252C%250Asparsity%2520structure%252C%2520and%2520the%2520requirement%2520to%2520use%2520prohibitive%2520non-linear%2520algebra%250Acomputations%2520in%2520the%2520process.%2520The%2520differentiating%2520factor%2520is%2520the%2520use%2520of%2520scalable%250Atools%2520from%2520%255Cemph%257Blinear%257D%2520algebra%2520in%2520the%2520context%2520of%2520theoretical%2520guarantees%250Aprovided%2520by%2520%255Cemph%257Bnon-linear%257D%2520algebra.%2520Our%2520algorithm%2520is%2520based%2520on%2520an%250Aactor-critic%2520sampling%2520scheme%252C%2520with%2520provable%2520convergence.%250A%2520%2520The%2520discovered%2520moves%2520can%2520be%2520used%2520to%2520efficiently%2520obtain%2520an%2520exchangeable%250Asample%252C%2520significantly%2520cutting%2520computational%2520times%2520with%2520regards%2520to%2520statistical%250Atesting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13950v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20sample%20fibers%20for%20goodness-of-fit%20testing&entry.906535625=Ivan%20Gvozdanovi%C4%87%20and%20Sonja%20Petrovi%C4%87&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20constructing%20exact%20goodness-of-fit%20tests%20for%0Adiscrete%20exponential%20family%20models.%20This%20classical%20problem%20remains%20practically%0Aunsolved%20for%20many%20types%20of%20structured%20or%20sparse%20data%2C%20as%20it%20rests%20on%20a%0Acomputationally%20difficult%20core%20task%3A%20to%20produce%20a%20reliable%20sample%20from%20lattice%0Apoints%20in%20a%20high-dimensional%20polytope.%20We%20translate%20the%20problem%20into%20a%20Markov%0Adecision%20process%20and%20demonstrate%20a%20reinforcement%20learning%20approach%20for%20learning%0A%60good%20moves%27%20for%20sampling.%20We%20illustrate%20the%20approach%20on%20data%20sets%20and%20models%0Afor%20which%20traditional%20MCMC%20samplers%20converge%20too%20slowly%20due%20to%20problem%20size%2C%0Asparsity%20structure%2C%20and%20the%20requirement%20to%20use%20prohibitive%20non-linear%20algebra%0Acomputations%20in%20the%20process.%20The%20differentiating%20factor%20is%20the%20use%20of%20scalable%0Atools%20from%20%5Cemph%7Blinear%7D%20algebra%20in%20the%20context%20of%20theoretical%20guarantees%0Aprovided%20by%20%5Cemph%7Bnon-linear%7D%20algebra.%20Our%20algorithm%20is%20based%20on%20an%0Aactor-critic%20sampling%20scheme%2C%20with%20provable%20convergence.%0A%20%20The%20discovered%20moves%20can%20be%20used%20to%20efficiently%20obtain%20an%20exchangeable%0Asample%2C%20significantly%20cutting%20computational%20times%20with%20regards%20to%20statistical%0Atesting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13950v4&entry.124074799=Read"},
{"title": "CrossAD: Time Series Anomaly Detection with Cross-scale Associations and\n  Cross-window Modeling", "author": "Beibu Li and Qichao Shentu and Yang Shu and Hui Zhang and Ming Li and Ning Jin and Bin Yang and Chenjuan Guo", "abstract": "  Time series anomaly detection plays a crucial role in a wide range of\nreal-world applications. Given that time series data can exhibit different\npatterns at different sampling granularities, multi-scale modeling has proven\nbeneficial for uncovering latent anomaly patterns that may not be apparent at a\nsingle scale. However, existing methods often model multi-scale information\nindependently or rely on simple feature fusion strategies, neglecting the\ndynamic changes in cross-scale associations that occur during anomalies.\nMoreover, most approaches perform multi-scale modeling based on fixed sliding\nwindows, which limits their ability to capture comprehensive contextual\ninformation. In this work, we propose CrossAD, a novel framework for time\nseries Anomaly Detection that takes Cross-scale associations and Cross-window\nmodeling into account. We propose a cross-scale reconstruction that\nreconstructs fine-grained series from coarser series, explicitly capturing\ncross-scale associations. Furthermore, we design a query library and\nincorporate global multi-scale context to overcome the limitations imposed by\nfixed window sizes. Extensive experiments conducted on multiple real-world\ndatasets using nine evaluation metrics validate the effectiveness of CrossAD,\ndemonstrating state-of-the-art performance in anomaly detection.\n", "link": "http://arxiv.org/abs/2510.12489v1", "date": "2025-10-14", "relevancy": 2.4391, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4977}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4833}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CrossAD%3A%20Time%20Series%20Anomaly%20Detection%20with%20Cross-scale%20Associations%20and%0A%20%20Cross-window%20Modeling&body=Title%3A%20CrossAD%3A%20Time%20Series%20Anomaly%20Detection%20with%20Cross-scale%20Associations%20and%0A%20%20Cross-window%20Modeling%0AAuthor%3A%20Beibu%20Li%20and%20Qichao%20Shentu%20and%20Yang%20Shu%20and%20Hui%20Zhang%20and%20Ming%20Li%20and%20Ning%20Jin%20and%20Bin%20Yang%20and%20Chenjuan%20Guo%0AAbstract%3A%20%20%20Time%20series%20anomaly%20detection%20plays%20a%20crucial%20role%20in%20a%20wide%20range%20of%0Areal-world%20applications.%20Given%20that%20time%20series%20data%20can%20exhibit%20different%0Apatterns%20at%20different%20sampling%20granularities%2C%20multi-scale%20modeling%20has%20proven%0Abeneficial%20for%20uncovering%20latent%20anomaly%20patterns%20that%20may%20not%20be%20apparent%20at%20a%0Asingle%20scale.%20However%2C%20existing%20methods%20often%20model%20multi-scale%20information%0Aindependently%20or%20rely%20on%20simple%20feature%20fusion%20strategies%2C%20neglecting%20the%0Adynamic%20changes%20in%20cross-scale%20associations%20that%20occur%20during%20anomalies.%0AMoreover%2C%20most%20approaches%20perform%20multi-scale%20modeling%20based%20on%20fixed%20sliding%0Awindows%2C%20which%20limits%20their%20ability%20to%20capture%20comprehensive%20contextual%0Ainformation.%20In%20this%20work%2C%20we%20propose%20CrossAD%2C%20a%20novel%20framework%20for%20time%0Aseries%20Anomaly%20Detection%20that%20takes%20Cross-scale%20associations%20and%20Cross-window%0Amodeling%20into%20account.%20We%20propose%20a%20cross-scale%20reconstruction%20that%0Areconstructs%20fine-grained%20series%20from%20coarser%20series%2C%20explicitly%20capturing%0Across-scale%20associations.%20Furthermore%2C%20we%20design%20a%20query%20library%20and%0Aincorporate%20global%20multi-scale%20context%20to%20overcome%20the%20limitations%20imposed%20by%0Afixed%20window%20sizes.%20Extensive%20experiments%20conducted%20on%20multiple%20real-world%0Adatasets%20using%20nine%20evaluation%20metrics%20validate%20the%20effectiveness%20of%20CrossAD%2C%0Ademonstrating%20state-of-the-art%20performance%20in%20anomaly%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrossAD%253A%2520Time%2520Series%2520Anomaly%2520Detection%2520with%2520Cross-scale%2520Associations%2520and%250A%2520%2520Cross-window%2520Modeling%26entry.906535625%3DBeibu%2520Li%2520and%2520Qichao%2520Shentu%2520and%2520Yang%2520Shu%2520and%2520Hui%2520Zhang%2520and%2520Ming%2520Li%2520and%2520Ning%2520Jin%2520and%2520Bin%2520Yang%2520and%2520Chenjuan%2520Guo%26entry.1292438233%3D%2520%2520Time%2520series%2520anomaly%2520detection%2520plays%2520a%2520crucial%2520role%2520in%2520a%2520wide%2520range%2520of%250Areal-world%2520applications.%2520Given%2520that%2520time%2520series%2520data%2520can%2520exhibit%2520different%250Apatterns%2520at%2520different%2520sampling%2520granularities%252C%2520multi-scale%2520modeling%2520has%2520proven%250Abeneficial%2520for%2520uncovering%2520latent%2520anomaly%2520patterns%2520that%2520may%2520not%2520be%2520apparent%2520at%2520a%250Asingle%2520scale.%2520However%252C%2520existing%2520methods%2520often%2520model%2520multi-scale%2520information%250Aindependently%2520or%2520rely%2520on%2520simple%2520feature%2520fusion%2520strategies%252C%2520neglecting%2520the%250Adynamic%2520changes%2520in%2520cross-scale%2520associations%2520that%2520occur%2520during%2520anomalies.%250AMoreover%252C%2520most%2520approaches%2520perform%2520multi-scale%2520modeling%2520based%2520on%2520fixed%2520sliding%250Awindows%252C%2520which%2520limits%2520their%2520ability%2520to%2520capture%2520comprehensive%2520contextual%250Ainformation.%2520In%2520this%2520work%252C%2520we%2520propose%2520CrossAD%252C%2520a%2520novel%2520framework%2520for%2520time%250Aseries%2520Anomaly%2520Detection%2520that%2520takes%2520Cross-scale%2520associations%2520and%2520Cross-window%250Amodeling%2520into%2520account.%2520We%2520propose%2520a%2520cross-scale%2520reconstruction%2520that%250Areconstructs%2520fine-grained%2520series%2520from%2520coarser%2520series%252C%2520explicitly%2520capturing%250Across-scale%2520associations.%2520Furthermore%252C%2520we%2520design%2520a%2520query%2520library%2520and%250Aincorporate%2520global%2520multi-scale%2520context%2520to%2520overcome%2520the%2520limitations%2520imposed%2520by%250Afixed%2520window%2520sizes.%2520Extensive%2520experiments%2520conducted%2520on%2520multiple%2520real-world%250Adatasets%2520using%2520nine%2520evaluation%2520metrics%2520validate%2520the%2520effectiveness%2520of%2520CrossAD%252C%250Ademonstrating%2520state-of-the-art%2520performance%2520in%2520anomaly%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CrossAD%3A%20Time%20Series%20Anomaly%20Detection%20with%20Cross-scale%20Associations%20and%0A%20%20Cross-window%20Modeling&entry.906535625=Beibu%20Li%20and%20Qichao%20Shentu%20and%20Yang%20Shu%20and%20Hui%20Zhang%20and%20Ming%20Li%20and%20Ning%20Jin%20and%20Bin%20Yang%20and%20Chenjuan%20Guo&entry.1292438233=%20%20Time%20series%20anomaly%20detection%20plays%20a%20crucial%20role%20in%20a%20wide%20range%20of%0Areal-world%20applications.%20Given%20that%20time%20series%20data%20can%20exhibit%20different%0Apatterns%20at%20different%20sampling%20granularities%2C%20multi-scale%20modeling%20has%20proven%0Abeneficial%20for%20uncovering%20latent%20anomaly%20patterns%20that%20may%20not%20be%20apparent%20at%20a%0Asingle%20scale.%20However%2C%20existing%20methods%20often%20model%20multi-scale%20information%0Aindependently%20or%20rely%20on%20simple%20feature%20fusion%20strategies%2C%20neglecting%20the%0Adynamic%20changes%20in%20cross-scale%20associations%20that%20occur%20during%20anomalies.%0AMoreover%2C%20most%20approaches%20perform%20multi-scale%20modeling%20based%20on%20fixed%20sliding%0Awindows%2C%20which%20limits%20their%20ability%20to%20capture%20comprehensive%20contextual%0Ainformation.%20In%20this%20work%2C%20we%20propose%20CrossAD%2C%20a%20novel%20framework%20for%20time%0Aseries%20Anomaly%20Detection%20that%20takes%20Cross-scale%20associations%20and%20Cross-window%0Amodeling%20into%20account.%20We%20propose%20a%20cross-scale%20reconstruction%20that%0Areconstructs%20fine-grained%20series%20from%20coarser%20series%2C%20explicitly%20capturing%0Across-scale%20associations.%20Furthermore%2C%20we%20design%20a%20query%20library%20and%0Aincorporate%20global%20multi-scale%20context%20to%20overcome%20the%20limitations%20imposed%20by%0Afixed%20window%20sizes.%20Extensive%20experiments%20conducted%20on%20multiple%20real-world%0Adatasets%20using%20nine%20evaluation%20metrics%20validate%20the%20effectiveness%20of%20CrossAD%2C%0Ademonstrating%20state-of-the-art%20performance%20in%20anomaly%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12489v1&entry.124074799=Read"},
{"title": "T(R,O) Grasp: Efficient Graph Diffusion of Robot-Object Spatial\n  Transformation for Cross-Embodiment Dexterous Grasping", "author": "Xin Fei and Zhixuan Xu and Huaicong Fang and Tianrui Zhang and Lin Shao", "abstract": "  Dexterous grasping remains a central challenge in robotics due to the\ncomplexity of its high-dimensional state and action space. We introduce T(R,O)\nGrasp, a diffusion-based framework that efficiently generates accurate and\ndiverse grasps across multiple robotic hands. At its core is the T(R,O) Graph,\na unified representation that models spatial transformations between robotic\nhands and objects while encoding their geometric properties. A graph diffusion\nmodel, coupled with an efficient inverse kinematics solver, supports both\nunconditioned and conditioned grasp synthesis. Extensive experiments on a\ndiverse set of dexterous hands show that T(R,O) Grasp achieves average success\nrate of 94.83%, inference speed of 0.21s, and throughput of 41 grasps per\nsecond on an NVIDIA A100 40GB GPU, substantially outperforming existing\nbaselines. In addition, our approach is robust and generalizable across\nembodiments while significantly reducing memory consumption. More importantly,\nthe high inference speed enables closed-loop dexterous manipulation,\nunderscoring the potential of T(R,O) Grasp to scale into a foundation model for\ndexterous grasping.\n", "link": "http://arxiv.org/abs/2510.12724v1", "date": "2025-10-14", "relevancy": 2.4335, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.693}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.596}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T%28R%2CO%29%20Grasp%3A%20Efficient%20Graph%20Diffusion%20of%20Robot-Object%20Spatial%0A%20%20Transformation%20for%20Cross-Embodiment%20Dexterous%20Grasping&body=Title%3A%20T%28R%2CO%29%20Grasp%3A%20Efficient%20Graph%20Diffusion%20of%20Robot-Object%20Spatial%0A%20%20Transformation%20for%20Cross-Embodiment%20Dexterous%20Grasping%0AAuthor%3A%20Xin%20Fei%20and%20Zhixuan%20Xu%20and%20Huaicong%20Fang%20and%20Tianrui%20Zhang%20and%20Lin%20Shao%0AAbstract%3A%20%20%20Dexterous%20grasping%20remains%20a%20central%20challenge%20in%20robotics%20due%20to%20the%0Acomplexity%20of%20its%20high-dimensional%20state%20and%20action%20space.%20We%20introduce%20T%28R%2CO%29%0AGrasp%2C%20a%20diffusion-based%20framework%20that%20efficiently%20generates%20accurate%20and%0Adiverse%20grasps%20across%20multiple%20robotic%20hands.%20At%20its%20core%20is%20the%20T%28R%2CO%29%20Graph%2C%0Aa%20unified%20representation%20that%20models%20spatial%20transformations%20between%20robotic%0Ahands%20and%20objects%20while%20encoding%20their%20geometric%20properties.%20A%20graph%20diffusion%0Amodel%2C%20coupled%20with%20an%20efficient%20inverse%20kinematics%20solver%2C%20supports%20both%0Aunconditioned%20and%20conditioned%20grasp%20synthesis.%20Extensive%20experiments%20on%20a%0Adiverse%20set%20of%20dexterous%20hands%20show%20that%20T%28R%2CO%29%20Grasp%20achieves%20average%20success%0Arate%20of%2094.83%25%2C%20inference%20speed%20of%200.21s%2C%20and%20throughput%20of%2041%20grasps%20per%0Asecond%20on%20an%20NVIDIA%20A100%2040GB%20GPU%2C%20substantially%20outperforming%20existing%0Abaselines.%20In%20addition%2C%20our%20approach%20is%20robust%20and%20generalizable%20across%0Aembodiments%20while%20significantly%20reducing%20memory%20consumption.%20More%20importantly%2C%0Athe%20high%20inference%20speed%20enables%20closed-loop%20dexterous%20manipulation%2C%0Aunderscoring%20the%20potential%20of%20T%28R%2CO%29%20Grasp%20to%20scale%20into%20a%20foundation%20model%20for%0Adexterous%20grasping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT%2528R%252CO%2529%2520Grasp%253A%2520Efficient%2520Graph%2520Diffusion%2520of%2520Robot-Object%2520Spatial%250A%2520%2520Transformation%2520for%2520Cross-Embodiment%2520Dexterous%2520Grasping%26entry.906535625%3DXin%2520Fei%2520and%2520Zhixuan%2520Xu%2520and%2520Huaicong%2520Fang%2520and%2520Tianrui%2520Zhang%2520and%2520Lin%2520Shao%26entry.1292438233%3D%2520%2520Dexterous%2520grasping%2520remains%2520a%2520central%2520challenge%2520in%2520robotics%2520due%2520to%2520the%250Acomplexity%2520of%2520its%2520high-dimensional%2520state%2520and%2520action%2520space.%2520We%2520introduce%2520T%2528R%252CO%2529%250AGrasp%252C%2520a%2520diffusion-based%2520framework%2520that%2520efficiently%2520generates%2520accurate%2520and%250Adiverse%2520grasps%2520across%2520multiple%2520robotic%2520hands.%2520At%2520its%2520core%2520is%2520the%2520T%2528R%252CO%2529%2520Graph%252C%250Aa%2520unified%2520representation%2520that%2520models%2520spatial%2520transformations%2520between%2520robotic%250Ahands%2520and%2520objects%2520while%2520encoding%2520their%2520geometric%2520properties.%2520A%2520graph%2520diffusion%250Amodel%252C%2520coupled%2520with%2520an%2520efficient%2520inverse%2520kinematics%2520solver%252C%2520supports%2520both%250Aunconditioned%2520and%2520conditioned%2520grasp%2520synthesis.%2520Extensive%2520experiments%2520on%2520a%250Adiverse%2520set%2520of%2520dexterous%2520hands%2520show%2520that%2520T%2528R%252CO%2529%2520Grasp%2520achieves%2520average%2520success%250Arate%2520of%252094.83%2525%252C%2520inference%2520speed%2520of%25200.21s%252C%2520and%2520throughput%2520of%252041%2520grasps%2520per%250Asecond%2520on%2520an%2520NVIDIA%2520A100%252040GB%2520GPU%252C%2520substantially%2520outperforming%2520existing%250Abaselines.%2520In%2520addition%252C%2520our%2520approach%2520is%2520robust%2520and%2520generalizable%2520across%250Aembodiments%2520while%2520significantly%2520reducing%2520memory%2520consumption.%2520More%2520importantly%252C%250Athe%2520high%2520inference%2520speed%2520enables%2520closed-loop%2520dexterous%2520manipulation%252C%250Aunderscoring%2520the%2520potential%2520of%2520T%2528R%252CO%2529%2520Grasp%2520to%2520scale%2520into%2520a%2520foundation%2520model%2520for%250Adexterous%2520grasping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T%28R%2CO%29%20Grasp%3A%20Efficient%20Graph%20Diffusion%20of%20Robot-Object%20Spatial%0A%20%20Transformation%20for%20Cross-Embodiment%20Dexterous%20Grasping&entry.906535625=Xin%20Fei%20and%20Zhixuan%20Xu%20and%20Huaicong%20Fang%20and%20Tianrui%20Zhang%20and%20Lin%20Shao&entry.1292438233=%20%20Dexterous%20grasping%20remains%20a%20central%20challenge%20in%20robotics%20due%20to%20the%0Acomplexity%20of%20its%20high-dimensional%20state%20and%20action%20space.%20We%20introduce%20T%28R%2CO%29%0AGrasp%2C%20a%20diffusion-based%20framework%20that%20efficiently%20generates%20accurate%20and%0Adiverse%20grasps%20across%20multiple%20robotic%20hands.%20At%20its%20core%20is%20the%20T%28R%2CO%29%20Graph%2C%0Aa%20unified%20representation%20that%20models%20spatial%20transformations%20between%20robotic%0Ahands%20and%20objects%20while%20encoding%20their%20geometric%20properties.%20A%20graph%20diffusion%0Amodel%2C%20coupled%20with%20an%20efficient%20inverse%20kinematics%20solver%2C%20supports%20both%0Aunconditioned%20and%20conditioned%20grasp%20synthesis.%20Extensive%20experiments%20on%20a%0Adiverse%20set%20of%20dexterous%20hands%20show%20that%20T%28R%2CO%29%20Grasp%20achieves%20average%20success%0Arate%20of%2094.83%25%2C%20inference%20speed%20of%200.21s%2C%20and%20throughput%20of%2041%20grasps%20per%0Asecond%20on%20an%20NVIDIA%20A100%2040GB%20GPU%2C%20substantially%20outperforming%20existing%0Abaselines.%20In%20addition%2C%20our%20approach%20is%20robust%20and%20generalizable%20across%0Aembodiments%20while%20significantly%20reducing%20memory%20consumption.%20More%20importantly%2C%0Athe%20high%20inference%20speed%20enables%20closed-loop%20dexterous%20manipulation%2C%0Aunderscoring%20the%20potential%20of%20T%28R%2CO%29%20Grasp%20to%20scale%20into%20a%20foundation%20model%20for%0Adexterous%20grasping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12724v1&entry.124074799=Read"},
{"title": "Time-Correlated Video Bridge Matching", "author": "Viacheslav Vasilev and Arseny Ivanov and Nikita Gushchin and Maria Kovaleva and Alexander Korotin", "abstract": "  Diffusion models excel in noise-to-data generation tasks, providing a mapping\nfrom a Gaussian distribution to a more complex data distribution. However they\nstruggle to model translations between complex distributions, limiting their\neffectiveness in data-to-data tasks. While Bridge Matching (BM) models address\nthis by finding the translation between data distributions, their application\nto time-correlated data sequences remains unexplored. This is a critical\nlimitation for video generation and manipulation tasks, where maintaining\ntemporal coherence is particularly important. To address this gap, we propose\nTime-Correlated Video Bridge Matching (TCVBM), a framework that extends BM to\ntime-correlated data sequences in the video domain. TCVBM explicitly models\ninter-sequence dependencies within the diffusion bridge, directly incorporating\ntemporal correlations into the sampling process. We compare our approach to\nclassical methods based on bridge matching and diffusion models for three\nvideo-related tasks: frame interpolation, image-to-video generation, and video\nsuper-resolution. TCVBM achieves superior performance across multiple\nquantitative metrics, demonstrating enhanced generation quality and\nreconstruction fidelity.\n", "link": "http://arxiv.org/abs/2510.12453v1", "date": "2025-10-14", "relevancy": 2.4262, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6226}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6164}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time-Correlated%20Video%20Bridge%20Matching&body=Title%3A%20Time-Correlated%20Video%20Bridge%20Matching%0AAuthor%3A%20Viacheslav%20Vasilev%20and%20Arseny%20Ivanov%20and%20Nikita%20Gushchin%20and%20Maria%20Kovaleva%20and%20Alexander%20Korotin%0AAbstract%3A%20%20%20Diffusion%20models%20excel%20in%20noise-to-data%20generation%20tasks%2C%20providing%20a%20mapping%0Afrom%20a%20Gaussian%20distribution%20to%20a%20more%20complex%20data%20distribution.%20However%20they%0Astruggle%20to%20model%20translations%20between%20complex%20distributions%2C%20limiting%20their%0Aeffectiveness%20in%20data-to-data%20tasks.%20While%20Bridge%20Matching%20%28BM%29%20models%20address%0Athis%20by%20finding%20the%20translation%20between%20data%20distributions%2C%20their%20application%0Ato%20time-correlated%20data%20sequences%20remains%20unexplored.%20This%20is%20a%20critical%0Alimitation%20for%20video%20generation%20and%20manipulation%20tasks%2C%20where%20maintaining%0Atemporal%20coherence%20is%20particularly%20important.%20To%20address%20this%20gap%2C%20we%20propose%0ATime-Correlated%20Video%20Bridge%20Matching%20%28TCVBM%29%2C%20a%20framework%20that%20extends%20BM%20to%0Atime-correlated%20data%20sequences%20in%20the%20video%20domain.%20TCVBM%20explicitly%20models%0Ainter-sequence%20dependencies%20within%20the%20diffusion%20bridge%2C%20directly%20incorporating%0Atemporal%20correlations%20into%20the%20sampling%20process.%20We%20compare%20our%20approach%20to%0Aclassical%20methods%20based%20on%20bridge%20matching%20and%20diffusion%20models%20for%20three%0Avideo-related%20tasks%3A%20frame%20interpolation%2C%20image-to-video%20generation%2C%20and%20video%0Asuper-resolution.%20TCVBM%20achieves%20superior%20performance%20across%20multiple%0Aquantitative%20metrics%2C%20demonstrating%20enhanced%20generation%20quality%20and%0Areconstruction%20fidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime-Correlated%2520Video%2520Bridge%2520Matching%26entry.906535625%3DViacheslav%2520Vasilev%2520and%2520Arseny%2520Ivanov%2520and%2520Nikita%2520Gushchin%2520and%2520Maria%2520Kovaleva%2520and%2520Alexander%2520Korotin%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520excel%2520in%2520noise-to-data%2520generation%2520tasks%252C%2520providing%2520a%2520mapping%250Afrom%2520a%2520Gaussian%2520distribution%2520to%2520a%2520more%2520complex%2520data%2520distribution.%2520However%2520they%250Astruggle%2520to%2520model%2520translations%2520between%2520complex%2520distributions%252C%2520limiting%2520their%250Aeffectiveness%2520in%2520data-to-data%2520tasks.%2520While%2520Bridge%2520Matching%2520%2528BM%2529%2520models%2520address%250Athis%2520by%2520finding%2520the%2520translation%2520between%2520data%2520distributions%252C%2520their%2520application%250Ato%2520time-correlated%2520data%2520sequences%2520remains%2520unexplored.%2520This%2520is%2520a%2520critical%250Alimitation%2520for%2520video%2520generation%2520and%2520manipulation%2520tasks%252C%2520where%2520maintaining%250Atemporal%2520coherence%2520is%2520particularly%2520important.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%250ATime-Correlated%2520Video%2520Bridge%2520Matching%2520%2528TCVBM%2529%252C%2520a%2520framework%2520that%2520extends%2520BM%2520to%250Atime-correlated%2520data%2520sequences%2520in%2520the%2520video%2520domain.%2520TCVBM%2520explicitly%2520models%250Ainter-sequence%2520dependencies%2520within%2520the%2520diffusion%2520bridge%252C%2520directly%2520incorporating%250Atemporal%2520correlations%2520into%2520the%2520sampling%2520process.%2520We%2520compare%2520our%2520approach%2520to%250Aclassical%2520methods%2520based%2520on%2520bridge%2520matching%2520and%2520diffusion%2520models%2520for%2520three%250Avideo-related%2520tasks%253A%2520frame%2520interpolation%252C%2520image-to-video%2520generation%252C%2520and%2520video%250Asuper-resolution.%2520TCVBM%2520achieves%2520superior%2520performance%2520across%2520multiple%250Aquantitative%2520metrics%252C%2520demonstrating%2520enhanced%2520generation%2520quality%2520and%250Areconstruction%2520fidelity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time-Correlated%20Video%20Bridge%20Matching&entry.906535625=Viacheslav%20Vasilev%20and%20Arseny%20Ivanov%20and%20Nikita%20Gushchin%20and%20Maria%20Kovaleva%20and%20Alexander%20Korotin&entry.1292438233=%20%20Diffusion%20models%20excel%20in%20noise-to-data%20generation%20tasks%2C%20providing%20a%20mapping%0Afrom%20a%20Gaussian%20distribution%20to%20a%20more%20complex%20data%20distribution.%20However%20they%0Astruggle%20to%20model%20translations%20between%20complex%20distributions%2C%20limiting%20their%0Aeffectiveness%20in%20data-to-data%20tasks.%20While%20Bridge%20Matching%20%28BM%29%20models%20address%0Athis%20by%20finding%20the%20translation%20between%20data%20distributions%2C%20their%20application%0Ato%20time-correlated%20data%20sequences%20remains%20unexplored.%20This%20is%20a%20critical%0Alimitation%20for%20video%20generation%20and%20manipulation%20tasks%2C%20where%20maintaining%0Atemporal%20coherence%20is%20particularly%20important.%20To%20address%20this%20gap%2C%20we%20propose%0ATime-Correlated%20Video%20Bridge%20Matching%20%28TCVBM%29%2C%20a%20framework%20that%20extends%20BM%20to%0Atime-correlated%20data%20sequences%20in%20the%20video%20domain.%20TCVBM%20explicitly%20models%0Ainter-sequence%20dependencies%20within%20the%20diffusion%20bridge%2C%20directly%20incorporating%0Atemporal%20correlations%20into%20the%20sampling%20process.%20We%20compare%20our%20approach%20to%0Aclassical%20methods%20based%20on%20bridge%20matching%20and%20diffusion%20models%20for%20three%0Avideo-related%20tasks%3A%20frame%20interpolation%2C%20image-to-video%20generation%2C%20and%20video%0Asuper-resolution.%20TCVBM%20achieves%20superior%20performance%20across%20multiple%0Aquantitative%20metrics%2C%20demonstrating%20enhanced%20generation%20quality%20and%0Areconstruction%20fidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12453v1&entry.124074799=Read"},
{"title": "Resource-Constrained Federated Continual Learning: What Does Matter?", "author": "Yichen Li and Yuying Wang and Jiahua Dong and Haozhao Wang and Yining Qi and Rui Zhang and Ruixuan Li", "abstract": "  Federated Continual Learning (FCL) aims to enable sequentially\nprivacy-preserving model training on streams of incoming data that vary in edge\ndevices by preserving previous knowledge while adapting to new data. Current\nFCL literature focuses on restricted data privacy and access to previously seen\ndata while imposing no constraints on the training overhead. This is\nunreasonable for FCL applications in real-world scenarios, where edge devices\nare primarily constrained by resources such as storage, computational budget,\nand label rate. We revisit this problem with a large-scale benchmark and\nanalyze the performance of state-of-the-art FCL approaches under different\nresource-constrained settings. Various typical FCL techniques and six datasets\nin two incremental learning scenarios (Class-IL and Domain-IL) are involved in\nour experiments. Through extensive experiments amounting to a total of over\n1,000+ GPU hours, we find that, under limited resource-constrained settings,\nexisting FCL approaches, with no exception, fail to achieve the expected\nperformance. Our conclusions are consistent in the sensitivity analysis. This\nsuggests that most existing FCL methods are particularly too resource-dependent\nfor real-world deployment. Moreover, we study the performance of typical FCL\ntechniques with resource constraints and shed light on future research\ndirections in FCL.\n", "link": "http://arxiv.org/abs/2501.08737v2", "date": "2025-10-14", "relevancy": 2.4117, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4826}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4826}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resource-Constrained%20Federated%20Continual%20Learning%3A%20What%20Does%20Matter%3F&body=Title%3A%20Resource-Constrained%20Federated%20Continual%20Learning%3A%20What%20Does%20Matter%3F%0AAuthor%3A%20Yichen%20Li%20and%20Yuying%20Wang%20and%20Jiahua%20Dong%20and%20Haozhao%20Wang%20and%20Yining%20Qi%20and%20Rui%20Zhang%20and%20Ruixuan%20Li%0AAbstract%3A%20%20%20Federated%20Continual%20Learning%20%28FCL%29%20aims%20to%20enable%20sequentially%0Aprivacy-preserving%20model%20training%20on%20streams%20of%20incoming%20data%20that%20vary%20in%20edge%0Adevices%20by%20preserving%20previous%20knowledge%20while%20adapting%20to%20new%20data.%20Current%0AFCL%20literature%20focuses%20on%20restricted%20data%20privacy%20and%20access%20to%20previously%20seen%0Adata%20while%20imposing%20no%20constraints%20on%20the%20training%20overhead.%20This%20is%0Aunreasonable%20for%20FCL%20applications%20in%20real-world%20scenarios%2C%20where%20edge%20devices%0Aare%20primarily%20constrained%20by%20resources%20such%20as%20storage%2C%20computational%20budget%2C%0Aand%20label%20rate.%20We%20revisit%20this%20problem%20with%20a%20large-scale%20benchmark%20and%0Aanalyze%20the%20performance%20of%20state-of-the-art%20FCL%20approaches%20under%20different%0Aresource-constrained%20settings.%20Various%20typical%20FCL%20techniques%20and%20six%20datasets%0Ain%20two%20incremental%20learning%20scenarios%20%28Class-IL%20and%20Domain-IL%29%20are%20involved%20in%0Aour%20experiments.%20Through%20extensive%20experiments%20amounting%20to%20a%20total%20of%20over%0A1%2C000%2B%20GPU%20hours%2C%20we%20find%20that%2C%20under%20limited%20resource-constrained%20settings%2C%0Aexisting%20FCL%20approaches%2C%20with%20no%20exception%2C%20fail%20to%20achieve%20the%20expected%0Aperformance.%20Our%20conclusions%20are%20consistent%20in%20the%20sensitivity%20analysis.%20This%0Asuggests%20that%20most%20existing%20FCL%20methods%20are%20particularly%20too%20resource-dependent%0Afor%20real-world%20deployment.%20Moreover%2C%20we%20study%20the%20performance%20of%20typical%20FCL%0Atechniques%20with%20resource%20constraints%20and%20shed%20light%20on%20future%20research%0Adirections%20in%20FCL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08737v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResource-Constrained%2520Federated%2520Continual%2520Learning%253A%2520What%2520Does%2520Matter%253F%26entry.906535625%3DYichen%2520Li%2520and%2520Yuying%2520Wang%2520and%2520Jiahua%2520Dong%2520and%2520Haozhao%2520Wang%2520and%2520Yining%2520Qi%2520and%2520Rui%2520Zhang%2520and%2520Ruixuan%2520Li%26entry.1292438233%3D%2520%2520Federated%2520Continual%2520Learning%2520%2528FCL%2529%2520aims%2520to%2520enable%2520sequentially%250Aprivacy-preserving%2520model%2520training%2520on%2520streams%2520of%2520incoming%2520data%2520that%2520vary%2520in%2520edge%250Adevices%2520by%2520preserving%2520previous%2520knowledge%2520while%2520adapting%2520to%2520new%2520data.%2520Current%250AFCL%2520literature%2520focuses%2520on%2520restricted%2520data%2520privacy%2520and%2520access%2520to%2520previously%2520seen%250Adata%2520while%2520imposing%2520no%2520constraints%2520on%2520the%2520training%2520overhead.%2520This%2520is%250Aunreasonable%2520for%2520FCL%2520applications%2520in%2520real-world%2520scenarios%252C%2520where%2520edge%2520devices%250Aare%2520primarily%2520constrained%2520by%2520resources%2520such%2520as%2520storage%252C%2520computational%2520budget%252C%250Aand%2520label%2520rate.%2520We%2520revisit%2520this%2520problem%2520with%2520a%2520large-scale%2520benchmark%2520and%250Aanalyze%2520the%2520performance%2520of%2520state-of-the-art%2520FCL%2520approaches%2520under%2520different%250Aresource-constrained%2520settings.%2520Various%2520typical%2520FCL%2520techniques%2520and%2520six%2520datasets%250Ain%2520two%2520incremental%2520learning%2520scenarios%2520%2528Class-IL%2520and%2520Domain-IL%2529%2520are%2520involved%2520in%250Aour%2520experiments.%2520Through%2520extensive%2520experiments%2520amounting%2520to%2520a%2520total%2520of%2520over%250A1%252C000%252B%2520GPU%2520hours%252C%2520we%2520find%2520that%252C%2520under%2520limited%2520resource-constrained%2520settings%252C%250Aexisting%2520FCL%2520approaches%252C%2520with%2520no%2520exception%252C%2520fail%2520to%2520achieve%2520the%2520expected%250Aperformance.%2520Our%2520conclusions%2520are%2520consistent%2520in%2520the%2520sensitivity%2520analysis.%2520This%250Asuggests%2520that%2520most%2520existing%2520FCL%2520methods%2520are%2520particularly%2520too%2520resource-dependent%250Afor%2520real-world%2520deployment.%2520Moreover%252C%2520we%2520study%2520the%2520performance%2520of%2520typical%2520FCL%250Atechniques%2520with%2520resource%2520constraints%2520and%2520shed%2520light%2520on%2520future%2520research%250Adirections%2520in%2520FCL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08737v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resource-Constrained%20Federated%20Continual%20Learning%3A%20What%20Does%20Matter%3F&entry.906535625=Yichen%20Li%20and%20Yuying%20Wang%20and%20Jiahua%20Dong%20and%20Haozhao%20Wang%20and%20Yining%20Qi%20and%20Rui%20Zhang%20and%20Ruixuan%20Li&entry.1292438233=%20%20Federated%20Continual%20Learning%20%28FCL%29%20aims%20to%20enable%20sequentially%0Aprivacy-preserving%20model%20training%20on%20streams%20of%20incoming%20data%20that%20vary%20in%20edge%0Adevices%20by%20preserving%20previous%20knowledge%20while%20adapting%20to%20new%20data.%20Current%0AFCL%20literature%20focuses%20on%20restricted%20data%20privacy%20and%20access%20to%20previously%20seen%0Adata%20while%20imposing%20no%20constraints%20on%20the%20training%20overhead.%20This%20is%0Aunreasonable%20for%20FCL%20applications%20in%20real-world%20scenarios%2C%20where%20edge%20devices%0Aare%20primarily%20constrained%20by%20resources%20such%20as%20storage%2C%20computational%20budget%2C%0Aand%20label%20rate.%20We%20revisit%20this%20problem%20with%20a%20large-scale%20benchmark%20and%0Aanalyze%20the%20performance%20of%20state-of-the-art%20FCL%20approaches%20under%20different%0Aresource-constrained%20settings.%20Various%20typical%20FCL%20techniques%20and%20six%20datasets%0Ain%20two%20incremental%20learning%20scenarios%20%28Class-IL%20and%20Domain-IL%29%20are%20involved%20in%0Aour%20experiments.%20Through%20extensive%20experiments%20amounting%20to%20a%20total%20of%20over%0A1%2C000%2B%20GPU%20hours%2C%20we%20find%20that%2C%20under%20limited%20resource-constrained%20settings%2C%0Aexisting%20FCL%20approaches%2C%20with%20no%20exception%2C%20fail%20to%20achieve%20the%20expected%0Aperformance.%20Our%20conclusions%20are%20consistent%20in%20the%20sensitivity%20analysis.%20This%0Asuggests%20that%20most%20existing%20FCL%20methods%20are%20particularly%20too%20resource-dependent%0Afor%20real-world%20deployment.%20Moreover%2C%20we%20study%20the%20performance%20of%20typical%20FCL%0Atechniques%20with%20resource%20constraints%20and%20shed%20light%20on%20future%20research%0Adirections%20in%20FCL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08737v2&entry.124074799=Read"},
{"title": "What If : Understanding Motion Through Sparse Interactions", "author": "Stefan Andreas Baumann and Nick Stracke and Timy Phan and Bj\u00f6rn Ommer", "abstract": "  Understanding the dynamics of a physical scene involves reasoning about the\ndiverse ways it can potentially change, especially as a result of local\ninteractions. We present the Flow Poke Transformer (FPT), a novel framework for\ndirectly predicting the distribution of local motion, conditioned on sparse\ninteractions termed \"pokes\". Unlike traditional methods that typically only\nenable dense sampling of a single realization of scene dynamics, FPT provides\nan interpretable directly accessible representation of multi-modal scene\nmotion, its dependency on physical interactions and the inherent uncertainties\nof scene dynamics. We also evaluate our model on several downstream tasks to\nenable comparisons with prior methods and highlight the flexibility of our\napproach. On dense face motion generation, our generic pre-trained model\nsurpasses specialized baselines. FPT can be fine-tuned in strongly\nout-of-distribution tasks such as synthetic datasets to enable significant\nimprovements over in-domain methods in articulated object motion estimation.\nAdditionally, predicting explicit motion distributions directly enables our\nmethod to achieve competitive performance on tasks like moving part\nsegmentation from pokes which further demonstrates the versatility of our FPT.\nCode and models are publicly available at\nhttps://compvis.github.io/flow-poke-transformer.\n", "link": "http://arxiv.org/abs/2510.12777v1", "date": "2025-10-14", "relevancy": 2.403, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6367}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5801}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20If%20%3A%20Understanding%20Motion%20Through%20Sparse%20Interactions&body=Title%3A%20What%20If%20%3A%20Understanding%20Motion%20Through%20Sparse%20Interactions%0AAuthor%3A%20Stefan%20Andreas%20Baumann%20and%20Nick%20Stracke%20and%20Timy%20Phan%20and%20Bj%C3%B6rn%20Ommer%0AAbstract%3A%20%20%20Understanding%20the%20dynamics%20of%20a%20physical%20scene%20involves%20reasoning%20about%20the%0Adiverse%20ways%20it%20can%20potentially%20change%2C%20especially%20as%20a%20result%20of%20local%0Ainteractions.%20We%20present%20the%20Flow%20Poke%20Transformer%20%28FPT%29%2C%20a%20novel%20framework%20for%0Adirectly%20predicting%20the%20distribution%20of%20local%20motion%2C%20conditioned%20on%20sparse%0Ainteractions%20termed%20%22pokes%22.%20Unlike%20traditional%20methods%20that%20typically%20only%0Aenable%20dense%20sampling%20of%20a%20single%20realization%20of%20scene%20dynamics%2C%20FPT%20provides%0Aan%20interpretable%20directly%20accessible%20representation%20of%20multi-modal%20scene%0Amotion%2C%20its%20dependency%20on%20physical%20interactions%20and%20the%20inherent%20uncertainties%0Aof%20scene%20dynamics.%20We%20also%20evaluate%20our%20model%20on%20several%20downstream%20tasks%20to%0Aenable%20comparisons%20with%20prior%20methods%20and%20highlight%20the%20flexibility%20of%20our%0Aapproach.%20On%20dense%20face%20motion%20generation%2C%20our%20generic%20pre-trained%20model%0Asurpasses%20specialized%20baselines.%20FPT%20can%20be%20fine-tuned%20in%20strongly%0Aout-of-distribution%20tasks%20such%20as%20synthetic%20datasets%20to%20enable%20significant%0Aimprovements%20over%20in-domain%20methods%20in%20articulated%20object%20motion%20estimation.%0AAdditionally%2C%20predicting%20explicit%20motion%20distributions%20directly%20enables%20our%0Amethod%20to%20achieve%20competitive%20performance%20on%20tasks%20like%20moving%20part%0Asegmentation%20from%20pokes%20which%20further%20demonstrates%20the%20versatility%20of%20our%20FPT.%0ACode%20and%20models%20are%20publicly%20available%20at%0Ahttps%3A//compvis.github.io/flow-poke-transformer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520If%2520%253A%2520Understanding%2520Motion%2520Through%2520Sparse%2520Interactions%26entry.906535625%3DStefan%2520Andreas%2520Baumann%2520and%2520Nick%2520Stracke%2520and%2520Timy%2520Phan%2520and%2520Bj%25C3%25B6rn%2520Ommer%26entry.1292438233%3D%2520%2520Understanding%2520the%2520dynamics%2520of%2520a%2520physical%2520scene%2520involves%2520reasoning%2520about%2520the%250Adiverse%2520ways%2520it%2520can%2520potentially%2520change%252C%2520especially%2520as%2520a%2520result%2520of%2520local%250Ainteractions.%2520We%2520present%2520the%2520Flow%2520Poke%2520Transformer%2520%2528FPT%2529%252C%2520a%2520novel%2520framework%2520for%250Adirectly%2520predicting%2520the%2520distribution%2520of%2520local%2520motion%252C%2520conditioned%2520on%2520sparse%250Ainteractions%2520termed%2520%2522pokes%2522.%2520Unlike%2520traditional%2520methods%2520that%2520typically%2520only%250Aenable%2520dense%2520sampling%2520of%2520a%2520single%2520realization%2520of%2520scene%2520dynamics%252C%2520FPT%2520provides%250Aan%2520interpretable%2520directly%2520accessible%2520representation%2520of%2520multi-modal%2520scene%250Amotion%252C%2520its%2520dependency%2520on%2520physical%2520interactions%2520and%2520the%2520inherent%2520uncertainties%250Aof%2520scene%2520dynamics.%2520We%2520also%2520evaluate%2520our%2520model%2520on%2520several%2520downstream%2520tasks%2520to%250Aenable%2520comparisons%2520with%2520prior%2520methods%2520and%2520highlight%2520the%2520flexibility%2520of%2520our%250Aapproach.%2520On%2520dense%2520face%2520motion%2520generation%252C%2520our%2520generic%2520pre-trained%2520model%250Asurpasses%2520specialized%2520baselines.%2520FPT%2520can%2520be%2520fine-tuned%2520in%2520strongly%250Aout-of-distribution%2520tasks%2520such%2520as%2520synthetic%2520datasets%2520to%2520enable%2520significant%250Aimprovements%2520over%2520in-domain%2520methods%2520in%2520articulated%2520object%2520motion%2520estimation.%250AAdditionally%252C%2520predicting%2520explicit%2520motion%2520distributions%2520directly%2520enables%2520our%250Amethod%2520to%2520achieve%2520competitive%2520performance%2520on%2520tasks%2520like%2520moving%2520part%250Asegmentation%2520from%2520pokes%2520which%2520further%2520demonstrates%2520the%2520versatility%2520of%2520our%2520FPT.%250ACode%2520and%2520models%2520are%2520publicly%2520available%2520at%250Ahttps%253A//compvis.github.io/flow-poke-transformer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20If%20%3A%20Understanding%20Motion%20Through%20Sparse%20Interactions&entry.906535625=Stefan%20Andreas%20Baumann%20and%20Nick%20Stracke%20and%20Timy%20Phan%20and%20Bj%C3%B6rn%20Ommer&entry.1292438233=%20%20Understanding%20the%20dynamics%20of%20a%20physical%20scene%20involves%20reasoning%20about%20the%0Adiverse%20ways%20it%20can%20potentially%20change%2C%20especially%20as%20a%20result%20of%20local%0Ainteractions.%20We%20present%20the%20Flow%20Poke%20Transformer%20%28FPT%29%2C%20a%20novel%20framework%20for%0Adirectly%20predicting%20the%20distribution%20of%20local%20motion%2C%20conditioned%20on%20sparse%0Ainteractions%20termed%20%22pokes%22.%20Unlike%20traditional%20methods%20that%20typically%20only%0Aenable%20dense%20sampling%20of%20a%20single%20realization%20of%20scene%20dynamics%2C%20FPT%20provides%0Aan%20interpretable%20directly%20accessible%20representation%20of%20multi-modal%20scene%0Amotion%2C%20its%20dependency%20on%20physical%20interactions%20and%20the%20inherent%20uncertainties%0Aof%20scene%20dynamics.%20We%20also%20evaluate%20our%20model%20on%20several%20downstream%20tasks%20to%0Aenable%20comparisons%20with%20prior%20methods%20and%20highlight%20the%20flexibility%20of%20our%0Aapproach.%20On%20dense%20face%20motion%20generation%2C%20our%20generic%20pre-trained%20model%0Asurpasses%20specialized%20baselines.%20FPT%20can%20be%20fine-tuned%20in%20strongly%0Aout-of-distribution%20tasks%20such%20as%20synthetic%20datasets%20to%20enable%20significant%0Aimprovements%20over%20in-domain%20methods%20in%20articulated%20object%20motion%20estimation.%0AAdditionally%2C%20predicting%20explicit%20motion%20distributions%20directly%20enables%20our%0Amethod%20to%20achieve%20competitive%20performance%20on%20tasks%20like%20moving%20part%0Asegmentation%20from%20pokes%20which%20further%20demonstrates%20the%20versatility%20of%20our%20FPT.%0ACode%20and%20models%20are%20publicly%20available%20at%0Ahttps%3A//compvis.github.io/flow-poke-transformer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12777v1&entry.124074799=Read"},
{"title": "ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution", "author": "Long Cui and Weiyun Wang and Jie Shao and Zichen Wen and Gen Luo and Linfeng Zhang and Yanting Zhang and Yu Qiao and Wenhai Wang", "abstract": "  Existing Multimodal Large Language Models (MLLMs) suffer from increased\ninference costs due to the additional vision tokens introduced by image inputs.\nIn this work, we propose Visual Consistency Learning (ViCO), a novel training\nalgorithm that enables the model to represent images of varying semantic\ncomplexities using different numbers of vision tokens. The key idea behind our\nmethod is to employ multiple MLP connectors, each with a different image\ncompression ratio, to downsample the vision tokens based on the semantic\ncomplexity of the image. During training, we minimize the KL divergence between\nthe responses conditioned on different MLP connectors. At inference time, we\nintroduce an image router, termed Visual Resolution Router (ViR), that\nautomatically selects the appropriate compression rate for each image patch.\nCompared with existing dynamic high-resolution strategies, which adjust the\nnumber of visual tokens based on image resolutions, our method dynamically\nadapts the number of visual tokens according to semantic complexity.\nExperimental results demonstrate that our method can reduce the number of\nvision tokens by up to 50% while maintaining the model's perception, reasoning,\nand OCR capabilities. We hope this work will contribute to the development of\nmore efficient MLLMs. The code and models will be released to facilitate future\nresearch.\n", "link": "http://arxiv.org/abs/2510.12793v1", "date": "2025-10-14", "relevancy": 2.3642, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5949}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5949}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViCO%3A%20A%20Training%20Strategy%20towards%20Semantic%20Aware%20Dynamic%20High-Resolution&body=Title%3A%20ViCO%3A%20A%20Training%20Strategy%20towards%20Semantic%20Aware%20Dynamic%20High-Resolution%0AAuthor%3A%20Long%20Cui%20and%20Weiyun%20Wang%20and%20Jie%20Shao%20and%20Zichen%20Wen%20and%20Gen%20Luo%20and%20Linfeng%20Zhang%20and%20Yanting%20Zhang%20and%20Yu%20Qiao%20and%20Wenhai%20Wang%0AAbstract%3A%20%20%20Existing%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20suffer%20from%20increased%0Ainference%20costs%20due%20to%20the%20additional%20vision%20tokens%20introduced%20by%20image%20inputs.%0AIn%20this%20work%2C%20we%20propose%20Visual%20Consistency%20Learning%20%28ViCO%29%2C%20a%20novel%20training%0Aalgorithm%20that%20enables%20the%20model%20to%20represent%20images%20of%20varying%20semantic%0Acomplexities%20using%20different%20numbers%20of%20vision%20tokens.%20The%20key%20idea%20behind%20our%0Amethod%20is%20to%20employ%20multiple%20MLP%20connectors%2C%20each%20with%20a%20different%20image%0Acompression%20ratio%2C%20to%20downsample%20the%20vision%20tokens%20based%20on%20the%20semantic%0Acomplexity%20of%20the%20image.%20During%20training%2C%20we%20minimize%20the%20KL%20divergence%20between%0Athe%20responses%20conditioned%20on%20different%20MLP%20connectors.%20At%20inference%20time%2C%20we%0Aintroduce%20an%20image%20router%2C%20termed%20Visual%20Resolution%20Router%20%28ViR%29%2C%20that%0Aautomatically%20selects%20the%20appropriate%20compression%20rate%20for%20each%20image%20patch.%0ACompared%20with%20existing%20dynamic%20high-resolution%20strategies%2C%20which%20adjust%20the%0Anumber%20of%20visual%20tokens%20based%20on%20image%20resolutions%2C%20our%20method%20dynamically%0Aadapts%20the%20number%20of%20visual%20tokens%20according%20to%20semantic%20complexity.%0AExperimental%20results%20demonstrate%20that%20our%20method%20can%20reduce%20the%20number%20of%0Avision%20tokens%20by%20up%20to%2050%25%20while%20maintaining%20the%20model%27s%20perception%2C%20reasoning%2C%0Aand%20OCR%20capabilities.%20We%20hope%20this%20work%20will%20contribute%20to%20the%20development%20of%0Amore%20efficient%20MLLMs.%20The%20code%20and%20models%20will%20be%20released%20to%20facilitate%20future%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12793v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViCO%253A%2520A%2520Training%2520Strategy%2520towards%2520Semantic%2520Aware%2520Dynamic%2520High-Resolution%26entry.906535625%3DLong%2520Cui%2520and%2520Weiyun%2520Wang%2520and%2520Jie%2520Shao%2520and%2520Zichen%2520Wen%2520and%2520Gen%2520Luo%2520and%2520Linfeng%2520Zhang%2520and%2520Yanting%2520Zhang%2520and%2520Yu%2520Qiao%2520and%2520Wenhai%2520Wang%26entry.1292438233%3D%2520%2520Existing%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520suffer%2520from%2520increased%250Ainference%2520costs%2520due%2520to%2520the%2520additional%2520vision%2520tokens%2520introduced%2520by%2520image%2520inputs.%250AIn%2520this%2520work%252C%2520we%2520propose%2520Visual%2520Consistency%2520Learning%2520%2528ViCO%2529%252C%2520a%2520novel%2520training%250Aalgorithm%2520that%2520enables%2520the%2520model%2520to%2520represent%2520images%2520of%2520varying%2520semantic%250Acomplexities%2520using%2520different%2520numbers%2520of%2520vision%2520tokens.%2520The%2520key%2520idea%2520behind%2520our%250Amethod%2520is%2520to%2520employ%2520multiple%2520MLP%2520connectors%252C%2520each%2520with%2520a%2520different%2520image%250Acompression%2520ratio%252C%2520to%2520downsample%2520the%2520vision%2520tokens%2520based%2520on%2520the%2520semantic%250Acomplexity%2520of%2520the%2520image.%2520During%2520training%252C%2520we%2520minimize%2520the%2520KL%2520divergence%2520between%250Athe%2520responses%2520conditioned%2520on%2520different%2520MLP%2520connectors.%2520At%2520inference%2520time%252C%2520we%250Aintroduce%2520an%2520image%2520router%252C%2520termed%2520Visual%2520Resolution%2520Router%2520%2528ViR%2529%252C%2520that%250Aautomatically%2520selects%2520the%2520appropriate%2520compression%2520rate%2520for%2520each%2520image%2520patch.%250ACompared%2520with%2520existing%2520dynamic%2520high-resolution%2520strategies%252C%2520which%2520adjust%2520the%250Anumber%2520of%2520visual%2520tokens%2520based%2520on%2520image%2520resolutions%252C%2520our%2520method%2520dynamically%250Aadapts%2520the%2520number%2520of%2520visual%2520tokens%2520according%2520to%2520semantic%2520complexity.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520method%2520can%2520reduce%2520the%2520number%2520of%250Avision%2520tokens%2520by%2520up%2520to%252050%2525%2520while%2520maintaining%2520the%2520model%2527s%2520perception%252C%2520reasoning%252C%250Aand%2520OCR%2520capabilities.%2520We%2520hope%2520this%2520work%2520will%2520contribute%2520to%2520the%2520development%2520of%250Amore%2520efficient%2520MLLMs.%2520The%2520code%2520and%2520models%2520will%2520be%2520released%2520to%2520facilitate%2520future%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12793v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViCO%3A%20A%20Training%20Strategy%20towards%20Semantic%20Aware%20Dynamic%20High-Resolution&entry.906535625=Long%20Cui%20and%20Weiyun%20Wang%20and%20Jie%20Shao%20and%20Zichen%20Wen%20and%20Gen%20Luo%20and%20Linfeng%20Zhang%20and%20Yanting%20Zhang%20and%20Yu%20Qiao%20and%20Wenhai%20Wang&entry.1292438233=%20%20Existing%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20suffer%20from%20increased%0Ainference%20costs%20due%20to%20the%20additional%20vision%20tokens%20introduced%20by%20image%20inputs.%0AIn%20this%20work%2C%20we%20propose%20Visual%20Consistency%20Learning%20%28ViCO%29%2C%20a%20novel%20training%0Aalgorithm%20that%20enables%20the%20model%20to%20represent%20images%20of%20varying%20semantic%0Acomplexities%20using%20different%20numbers%20of%20vision%20tokens.%20The%20key%20idea%20behind%20our%0Amethod%20is%20to%20employ%20multiple%20MLP%20connectors%2C%20each%20with%20a%20different%20image%0Acompression%20ratio%2C%20to%20downsample%20the%20vision%20tokens%20based%20on%20the%20semantic%0Acomplexity%20of%20the%20image.%20During%20training%2C%20we%20minimize%20the%20KL%20divergence%20between%0Athe%20responses%20conditioned%20on%20different%20MLP%20connectors.%20At%20inference%20time%2C%20we%0Aintroduce%20an%20image%20router%2C%20termed%20Visual%20Resolution%20Router%20%28ViR%29%2C%20that%0Aautomatically%20selects%20the%20appropriate%20compression%20rate%20for%20each%20image%20patch.%0ACompared%20with%20existing%20dynamic%20high-resolution%20strategies%2C%20which%20adjust%20the%0Anumber%20of%20visual%20tokens%20based%20on%20image%20resolutions%2C%20our%20method%20dynamically%0Aadapts%20the%20number%20of%20visual%20tokens%20according%20to%20semantic%20complexity.%0AExperimental%20results%20demonstrate%20that%20our%20method%20can%20reduce%20the%20number%20of%0Avision%20tokens%20by%20up%20to%2050%25%20while%20maintaining%20the%20model%27s%20perception%2C%20reasoning%2C%0Aand%20OCR%20capabilities.%20We%20hope%20this%20work%20will%20contribute%20to%20the%20development%20of%0Amore%20efficient%20MLLMs.%20The%20code%20and%20models%20will%20be%20released%20to%20facilitate%20future%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12793v1&entry.124074799=Read"},
{"title": "Dendrograms of Mixing Measures for Softmax-Gated Gaussian Mixture of\n  Experts: Consistency without Model Sweeps", "author": "Do Tien Hai and Trung Nguyen Mai and TrungTin Nguyen and Nhat Ho and Binh T. Nguyen and Christopher Drovandi", "abstract": "  We develop a unified statistical framework for softmax-gated Gaussian mixture\nof experts (SGMoE) that addresses three long-standing obstacles in parameter\nestimation and model selection: (i) non-identifiability of gating parameters up\nto common translations, (ii) intrinsic gate-expert interactions that induce\ncoupled differential relations in the likelihood, and (iii) the tight\nnumerator-denominator coupling in the softmax-induced conditional density. Our\napproach introduces Voronoi-type loss functions aligned with the gate-partition\ngeometry and establishes finite-sample convergence rates for the maximum\nlikelihood estimator (MLE). In over-specified models, we reveal a link between\nthe MLE's convergence rate and the solvability of an associated system of\npolynomial equations characterizing near-nonidentifiable directions. For model\nselection, we adapt dendrograms of mixing measures to SGMoE, yielding a\nconsistent, sweep-free selector of the number of experts that attains\npointwise-optimal parameter rates under overfitting while avoiding multi-size\ntraining. Simulations on synthetic data corroborate the theory, accurately\nrecovering the expert count and achieving the predicted rates for parameter\nestimation while closely approximating the regression function. Under model\nmisspecification (e.g., $\\epsilon$-contamination), the dendrogram selection\ncriterion is robust, recovering the true number of mixture components, while\nthe Akaike information criterion, the Bayesian information criterion, and the\nintegrated completed likelihood tend to overselect as sample size grows. On a\nmaize proteomics dataset of drought-responsive traits, our dendrogram-guided\nSGMoE selects two experts, exposes a clear mixing-measure hierarchy, stabilizes\nthe likelihood early, and yields interpretable genotype-phenotype maps,\noutperforming standard criteria without multi-size training.\n", "link": "http://arxiv.org/abs/2510.12744v1", "date": "2025-10-14", "relevancy": 2.3622, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4759}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4716}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dendrograms%20of%20Mixing%20Measures%20for%20Softmax-Gated%20Gaussian%20Mixture%20of%0A%20%20Experts%3A%20Consistency%20without%20Model%20Sweeps&body=Title%3A%20Dendrograms%20of%20Mixing%20Measures%20for%20Softmax-Gated%20Gaussian%20Mixture%20of%0A%20%20Experts%3A%20Consistency%20without%20Model%20Sweeps%0AAuthor%3A%20Do%20Tien%20Hai%20and%20Trung%20Nguyen%20Mai%20and%20TrungTin%20Nguyen%20and%20Nhat%20Ho%20and%20Binh%20T.%20Nguyen%20and%20Christopher%20Drovandi%0AAbstract%3A%20%20%20We%20develop%20a%20unified%20statistical%20framework%20for%20softmax-gated%20Gaussian%20mixture%0Aof%20experts%20%28SGMoE%29%20that%20addresses%20three%20long-standing%20obstacles%20in%20parameter%0Aestimation%20and%20model%20selection%3A%20%28i%29%20non-identifiability%20of%20gating%20parameters%20up%0Ato%20common%20translations%2C%20%28ii%29%20intrinsic%20gate-expert%20interactions%20that%20induce%0Acoupled%20differential%20relations%20in%20the%20likelihood%2C%20and%20%28iii%29%20the%20tight%0Anumerator-denominator%20coupling%20in%20the%20softmax-induced%20conditional%20density.%20Our%0Aapproach%20introduces%20Voronoi-type%20loss%20functions%20aligned%20with%20the%20gate-partition%0Ageometry%20and%20establishes%20finite-sample%20convergence%20rates%20for%20the%20maximum%0Alikelihood%20estimator%20%28MLE%29.%20In%20over-specified%20models%2C%20we%20reveal%20a%20link%20between%0Athe%20MLE%27s%20convergence%20rate%20and%20the%20solvability%20of%20an%20associated%20system%20of%0Apolynomial%20equations%20characterizing%20near-nonidentifiable%20directions.%20For%20model%0Aselection%2C%20we%20adapt%20dendrograms%20of%20mixing%20measures%20to%20SGMoE%2C%20yielding%20a%0Aconsistent%2C%20sweep-free%20selector%20of%20the%20number%20of%20experts%20that%20attains%0Apointwise-optimal%20parameter%20rates%20under%20overfitting%20while%20avoiding%20multi-size%0Atraining.%20Simulations%20on%20synthetic%20data%20corroborate%20the%20theory%2C%20accurately%0Arecovering%20the%20expert%20count%20and%20achieving%20the%20predicted%20rates%20for%20parameter%0Aestimation%20while%20closely%20approximating%20the%20regression%20function.%20Under%20model%0Amisspecification%20%28e.g.%2C%20%24%5Cepsilon%24-contamination%29%2C%20the%20dendrogram%20selection%0Acriterion%20is%20robust%2C%20recovering%20the%20true%20number%20of%20mixture%20components%2C%20while%0Athe%20Akaike%20information%20criterion%2C%20the%20Bayesian%20information%20criterion%2C%20and%20the%0Aintegrated%20completed%20likelihood%20tend%20to%20overselect%20as%20sample%20size%20grows.%20On%20a%0Amaize%20proteomics%20dataset%20of%20drought-responsive%20traits%2C%20our%20dendrogram-guided%0ASGMoE%20selects%20two%20experts%2C%20exposes%20a%20clear%20mixing-measure%20hierarchy%2C%20stabilizes%0Athe%20likelihood%20early%2C%20and%20yields%20interpretable%20genotype-phenotype%20maps%2C%0Aoutperforming%20standard%20criteria%20without%20multi-size%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12744v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDendrograms%2520of%2520Mixing%2520Measures%2520for%2520Softmax-Gated%2520Gaussian%2520Mixture%2520of%250A%2520%2520Experts%253A%2520Consistency%2520without%2520Model%2520Sweeps%26entry.906535625%3DDo%2520Tien%2520Hai%2520and%2520Trung%2520Nguyen%2520Mai%2520and%2520TrungTin%2520Nguyen%2520and%2520Nhat%2520Ho%2520and%2520Binh%2520T.%2520Nguyen%2520and%2520Christopher%2520Drovandi%26entry.1292438233%3D%2520%2520We%2520develop%2520a%2520unified%2520statistical%2520framework%2520for%2520softmax-gated%2520Gaussian%2520mixture%250Aof%2520experts%2520%2528SGMoE%2529%2520that%2520addresses%2520three%2520long-standing%2520obstacles%2520in%2520parameter%250Aestimation%2520and%2520model%2520selection%253A%2520%2528i%2529%2520non-identifiability%2520of%2520gating%2520parameters%2520up%250Ato%2520common%2520translations%252C%2520%2528ii%2529%2520intrinsic%2520gate-expert%2520interactions%2520that%2520induce%250Acoupled%2520differential%2520relations%2520in%2520the%2520likelihood%252C%2520and%2520%2528iii%2529%2520the%2520tight%250Anumerator-denominator%2520coupling%2520in%2520the%2520softmax-induced%2520conditional%2520density.%2520Our%250Aapproach%2520introduces%2520Voronoi-type%2520loss%2520functions%2520aligned%2520with%2520the%2520gate-partition%250Ageometry%2520and%2520establishes%2520finite-sample%2520convergence%2520rates%2520for%2520the%2520maximum%250Alikelihood%2520estimator%2520%2528MLE%2529.%2520In%2520over-specified%2520models%252C%2520we%2520reveal%2520a%2520link%2520between%250Athe%2520MLE%2527s%2520convergence%2520rate%2520and%2520the%2520solvability%2520of%2520an%2520associated%2520system%2520of%250Apolynomial%2520equations%2520characterizing%2520near-nonidentifiable%2520directions.%2520For%2520model%250Aselection%252C%2520we%2520adapt%2520dendrograms%2520of%2520mixing%2520measures%2520to%2520SGMoE%252C%2520yielding%2520a%250Aconsistent%252C%2520sweep-free%2520selector%2520of%2520the%2520number%2520of%2520experts%2520that%2520attains%250Apointwise-optimal%2520parameter%2520rates%2520under%2520overfitting%2520while%2520avoiding%2520multi-size%250Atraining.%2520Simulations%2520on%2520synthetic%2520data%2520corroborate%2520the%2520theory%252C%2520accurately%250Arecovering%2520the%2520expert%2520count%2520and%2520achieving%2520the%2520predicted%2520rates%2520for%2520parameter%250Aestimation%2520while%2520closely%2520approximating%2520the%2520regression%2520function.%2520Under%2520model%250Amisspecification%2520%2528e.g.%252C%2520%2524%255Cepsilon%2524-contamination%2529%252C%2520the%2520dendrogram%2520selection%250Acriterion%2520is%2520robust%252C%2520recovering%2520the%2520true%2520number%2520of%2520mixture%2520components%252C%2520while%250Athe%2520Akaike%2520information%2520criterion%252C%2520the%2520Bayesian%2520information%2520criterion%252C%2520and%2520the%250Aintegrated%2520completed%2520likelihood%2520tend%2520to%2520overselect%2520as%2520sample%2520size%2520grows.%2520On%2520a%250Amaize%2520proteomics%2520dataset%2520of%2520drought-responsive%2520traits%252C%2520our%2520dendrogram-guided%250ASGMoE%2520selects%2520two%2520experts%252C%2520exposes%2520a%2520clear%2520mixing-measure%2520hierarchy%252C%2520stabilizes%250Athe%2520likelihood%2520early%252C%2520and%2520yields%2520interpretable%2520genotype-phenotype%2520maps%252C%250Aoutperforming%2520standard%2520criteria%2520without%2520multi-size%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12744v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dendrograms%20of%20Mixing%20Measures%20for%20Softmax-Gated%20Gaussian%20Mixture%20of%0A%20%20Experts%3A%20Consistency%20without%20Model%20Sweeps&entry.906535625=Do%20Tien%20Hai%20and%20Trung%20Nguyen%20Mai%20and%20TrungTin%20Nguyen%20and%20Nhat%20Ho%20and%20Binh%20T.%20Nguyen%20and%20Christopher%20Drovandi&entry.1292438233=%20%20We%20develop%20a%20unified%20statistical%20framework%20for%20softmax-gated%20Gaussian%20mixture%0Aof%20experts%20%28SGMoE%29%20that%20addresses%20three%20long-standing%20obstacles%20in%20parameter%0Aestimation%20and%20model%20selection%3A%20%28i%29%20non-identifiability%20of%20gating%20parameters%20up%0Ato%20common%20translations%2C%20%28ii%29%20intrinsic%20gate-expert%20interactions%20that%20induce%0Acoupled%20differential%20relations%20in%20the%20likelihood%2C%20and%20%28iii%29%20the%20tight%0Anumerator-denominator%20coupling%20in%20the%20softmax-induced%20conditional%20density.%20Our%0Aapproach%20introduces%20Voronoi-type%20loss%20functions%20aligned%20with%20the%20gate-partition%0Ageometry%20and%20establishes%20finite-sample%20convergence%20rates%20for%20the%20maximum%0Alikelihood%20estimator%20%28MLE%29.%20In%20over-specified%20models%2C%20we%20reveal%20a%20link%20between%0Athe%20MLE%27s%20convergence%20rate%20and%20the%20solvability%20of%20an%20associated%20system%20of%0Apolynomial%20equations%20characterizing%20near-nonidentifiable%20directions.%20For%20model%0Aselection%2C%20we%20adapt%20dendrograms%20of%20mixing%20measures%20to%20SGMoE%2C%20yielding%20a%0Aconsistent%2C%20sweep-free%20selector%20of%20the%20number%20of%20experts%20that%20attains%0Apointwise-optimal%20parameter%20rates%20under%20overfitting%20while%20avoiding%20multi-size%0Atraining.%20Simulations%20on%20synthetic%20data%20corroborate%20the%20theory%2C%20accurately%0Arecovering%20the%20expert%20count%20and%20achieving%20the%20predicted%20rates%20for%20parameter%0Aestimation%20while%20closely%20approximating%20the%20regression%20function.%20Under%20model%0Amisspecification%20%28e.g.%2C%20%24%5Cepsilon%24-contamination%29%2C%20the%20dendrogram%20selection%0Acriterion%20is%20robust%2C%20recovering%20the%20true%20number%20of%20mixture%20components%2C%20while%0Athe%20Akaike%20information%20criterion%2C%20the%20Bayesian%20information%20criterion%2C%20and%20the%0Aintegrated%20completed%20likelihood%20tend%20to%20overselect%20as%20sample%20size%20grows.%20On%20a%0Amaize%20proteomics%20dataset%20of%20drought-responsive%20traits%2C%20our%20dendrogram-guided%0ASGMoE%20selects%20two%20experts%2C%20exposes%20a%20clear%20mixing-measure%20hierarchy%2C%20stabilizes%0Athe%20likelihood%20early%2C%20and%20yields%20interpretable%20genotype-phenotype%20maps%2C%0Aoutperforming%20standard%20criteria%20without%20multi-size%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12744v1&entry.124074799=Read"},
{"title": "Human-MME: A Holistic Evaluation Benchmark for Human-Centric Multimodal\n  Large Language Models", "author": "Yuansen Liu and Haiming Tang and Jinlong Peng and Jiangning Zhang and Xiaozhong Ji and Qingdong He and Wenbin Wu and Donghao Luo and Zhenye Gan and Junwei Zhu and Yunhang Shen and Chaoyou Fu and Chengjie Wang and Xiaobin Hu and Shuicheng Yan", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated significant\nadvances in visual understanding tasks. However, their capacity to comprehend\nhuman-centric scenes has rarely been explored, primarily due to the absence of\ncomprehensive evaluation benchmarks that take into account both the\nhuman-oriented granular level and higher-dimensional causal reasoning ability.\nSuch high-quality evaluation benchmarks face tough obstacles, given the\nphysical complexity of the human body and the difficulty of annotating granular\nstructures. In this paper, we propose Human-MME, a curated benchmark designed\nto provide a more holistic evaluation of MLLMs in human-centric scene\nunderstanding. Compared with other existing benchmarks, our work provides three\nkey features: 1. Diversity in human scene, spanning 4 primary visual domains\nwith 15 secondary domains and 43 sub-fields to ensure broad scenario coverage.\n2. Progressive and diverse evaluation dimensions, evaluating the human-based\nactivities progressively from the human-oriented granular perception to the\nhigher-dimensional reasoning, consisting of eight dimensions with 19,945\nreal-world image question pairs and an evaluation suite. 3. High-quality\nannotations with rich data paradigms, constructing the automated annotation\npipeline and human-annotation platform, supporting rigorous manual labeling to\nfacilitate precise and reliable model assessment. Our benchmark extends the\nsingle-target understanding to the multi-person and multi-image mutual\nunderstanding by constructing the choice, short-answer, grounding, ranking and\njudgment question components, and complex questions of their combination. The\nextensive experiments on 17 state-of-the-art MLLMs effectively expose the\nlimitations and guide future MLLMs research toward better human-centric image\nunderstanding. All data and code are available at\nhttps://github.com/Yuan-Hou/Human-MME.\n", "link": "http://arxiv.org/abs/2509.26165v2", "date": "2025-10-14", "relevancy": 2.3535, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5901}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-MME%3A%20A%20Holistic%20Evaluation%20Benchmark%20for%20Human-Centric%20Multimodal%0A%20%20Large%20Language%20Models&body=Title%3A%20Human-MME%3A%20A%20Holistic%20Evaluation%20Benchmark%20for%20Human-Centric%20Multimodal%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Yuansen%20Liu%20and%20Haiming%20Tang%20and%20Jinlong%20Peng%20and%20Jiangning%20Zhang%20and%20Xiaozhong%20Ji%20and%20Qingdong%20He%20and%20Wenbin%20Wu%20and%20Donghao%20Luo%20and%20Zhenye%20Gan%20and%20Junwei%20Zhu%20and%20Yunhang%20Shen%20and%20Chaoyou%20Fu%20and%20Chengjie%20Wang%20and%20Xiaobin%20Hu%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20significant%0Aadvances%20in%20visual%20understanding%20tasks.%20However%2C%20their%20capacity%20to%20comprehend%0Ahuman-centric%20scenes%20has%20rarely%20been%20explored%2C%20primarily%20due%20to%20the%20absence%20of%0Acomprehensive%20evaluation%20benchmarks%20that%20take%20into%20account%20both%20the%0Ahuman-oriented%20granular%20level%20and%20higher-dimensional%20causal%20reasoning%20ability.%0ASuch%20high-quality%20evaluation%20benchmarks%20face%20tough%20obstacles%2C%20given%20the%0Aphysical%20complexity%20of%20the%20human%20body%20and%20the%20difficulty%20of%20annotating%20granular%0Astructures.%20In%20this%20paper%2C%20we%20propose%20Human-MME%2C%20a%20curated%20benchmark%20designed%0Ato%20provide%20a%20more%20holistic%20evaluation%20of%20MLLMs%20in%20human-centric%20scene%0Aunderstanding.%20Compared%20with%20other%20existing%20benchmarks%2C%20our%20work%20provides%20three%0Akey%20features%3A%201.%20Diversity%20in%20human%20scene%2C%20spanning%204%20primary%20visual%20domains%0Awith%2015%20secondary%20domains%20and%2043%20sub-fields%20to%20ensure%20broad%20scenario%20coverage.%0A2.%20Progressive%20and%20diverse%20evaluation%20dimensions%2C%20evaluating%20the%20human-based%0Aactivities%20progressively%20from%20the%20human-oriented%20granular%20perception%20to%20the%0Ahigher-dimensional%20reasoning%2C%20consisting%20of%20eight%20dimensions%20with%2019%2C945%0Areal-world%20image%20question%20pairs%20and%20an%20evaluation%20suite.%203.%20High-quality%0Aannotations%20with%20rich%20data%20paradigms%2C%20constructing%20the%20automated%20annotation%0Apipeline%20and%20human-annotation%20platform%2C%20supporting%20rigorous%20manual%20labeling%20to%0Afacilitate%20precise%20and%20reliable%20model%20assessment.%20Our%20benchmark%20extends%20the%0Asingle-target%20understanding%20to%20the%20multi-person%20and%20multi-image%20mutual%0Aunderstanding%20by%20constructing%20the%20choice%2C%20short-answer%2C%20grounding%2C%20ranking%20and%0Ajudgment%20question%20components%2C%20and%20complex%20questions%20of%20their%20combination.%20The%0Aextensive%20experiments%20on%2017%20state-of-the-art%20MLLMs%20effectively%20expose%20the%0Alimitations%20and%20guide%20future%20MLLMs%20research%20toward%20better%20human-centric%20image%0Aunderstanding.%20All%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/Yuan-Hou/Human-MME.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26165v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-MME%253A%2520A%2520Holistic%2520Evaluation%2520Benchmark%2520for%2520Human-Centric%2520Multimodal%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DYuansen%2520Liu%2520and%2520Haiming%2520Tang%2520and%2520Jinlong%2520Peng%2520and%2520Jiangning%2520Zhang%2520and%2520Xiaozhong%2520Ji%2520and%2520Qingdong%2520He%2520and%2520Wenbin%2520Wu%2520and%2520Donghao%2520Luo%2520and%2520Zhenye%2520Gan%2520and%2520Junwei%2520Zhu%2520and%2520Yunhang%2520Shen%2520and%2520Chaoyou%2520Fu%2520and%2520Chengjie%2520Wang%2520and%2520Xiaobin%2520Hu%2520and%2520Shuicheng%2520Yan%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520significant%250Aadvances%2520in%2520visual%2520understanding%2520tasks.%2520However%252C%2520their%2520capacity%2520to%2520comprehend%250Ahuman-centric%2520scenes%2520has%2520rarely%2520been%2520explored%252C%2520primarily%2520due%2520to%2520the%2520absence%2520of%250Acomprehensive%2520evaluation%2520benchmarks%2520that%2520take%2520into%2520account%2520both%2520the%250Ahuman-oriented%2520granular%2520level%2520and%2520higher-dimensional%2520causal%2520reasoning%2520ability.%250ASuch%2520high-quality%2520evaluation%2520benchmarks%2520face%2520tough%2520obstacles%252C%2520given%2520the%250Aphysical%2520complexity%2520of%2520the%2520human%2520body%2520and%2520the%2520difficulty%2520of%2520annotating%2520granular%250Astructures.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Human-MME%252C%2520a%2520curated%2520benchmark%2520designed%250Ato%2520provide%2520a%2520more%2520holistic%2520evaluation%2520of%2520MLLMs%2520in%2520human-centric%2520scene%250Aunderstanding.%2520Compared%2520with%2520other%2520existing%2520benchmarks%252C%2520our%2520work%2520provides%2520three%250Akey%2520features%253A%25201.%2520Diversity%2520in%2520human%2520scene%252C%2520spanning%25204%2520primary%2520visual%2520domains%250Awith%252015%2520secondary%2520domains%2520and%252043%2520sub-fields%2520to%2520ensure%2520broad%2520scenario%2520coverage.%250A2.%2520Progressive%2520and%2520diverse%2520evaluation%2520dimensions%252C%2520evaluating%2520the%2520human-based%250Aactivities%2520progressively%2520from%2520the%2520human-oriented%2520granular%2520perception%2520to%2520the%250Ahigher-dimensional%2520reasoning%252C%2520consisting%2520of%2520eight%2520dimensions%2520with%252019%252C945%250Areal-world%2520image%2520question%2520pairs%2520and%2520an%2520evaluation%2520suite.%25203.%2520High-quality%250Aannotations%2520with%2520rich%2520data%2520paradigms%252C%2520constructing%2520the%2520automated%2520annotation%250Apipeline%2520and%2520human-annotation%2520platform%252C%2520supporting%2520rigorous%2520manual%2520labeling%2520to%250Afacilitate%2520precise%2520and%2520reliable%2520model%2520assessment.%2520Our%2520benchmark%2520extends%2520the%250Asingle-target%2520understanding%2520to%2520the%2520multi-person%2520and%2520multi-image%2520mutual%250Aunderstanding%2520by%2520constructing%2520the%2520choice%252C%2520short-answer%252C%2520grounding%252C%2520ranking%2520and%250Ajudgment%2520question%2520components%252C%2520and%2520complex%2520questions%2520of%2520their%2520combination.%2520The%250Aextensive%2520experiments%2520on%252017%2520state-of-the-art%2520MLLMs%2520effectively%2520expose%2520the%250Alimitations%2520and%2520guide%2520future%2520MLLMs%2520research%2520toward%2520better%2520human-centric%2520image%250Aunderstanding.%2520All%2520data%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/Yuan-Hou/Human-MME.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26165v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-MME%3A%20A%20Holistic%20Evaluation%20Benchmark%20for%20Human-Centric%20Multimodal%0A%20%20Large%20Language%20Models&entry.906535625=Yuansen%20Liu%20and%20Haiming%20Tang%20and%20Jinlong%20Peng%20and%20Jiangning%20Zhang%20and%20Xiaozhong%20Ji%20and%20Qingdong%20He%20and%20Wenbin%20Wu%20and%20Donghao%20Luo%20and%20Zhenye%20Gan%20and%20Junwei%20Zhu%20and%20Yunhang%20Shen%20and%20Chaoyou%20Fu%20and%20Chengjie%20Wang%20and%20Xiaobin%20Hu%20and%20Shuicheng%20Yan&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20significant%0Aadvances%20in%20visual%20understanding%20tasks.%20However%2C%20their%20capacity%20to%20comprehend%0Ahuman-centric%20scenes%20has%20rarely%20been%20explored%2C%20primarily%20due%20to%20the%20absence%20of%0Acomprehensive%20evaluation%20benchmarks%20that%20take%20into%20account%20both%20the%0Ahuman-oriented%20granular%20level%20and%20higher-dimensional%20causal%20reasoning%20ability.%0ASuch%20high-quality%20evaluation%20benchmarks%20face%20tough%20obstacles%2C%20given%20the%0Aphysical%20complexity%20of%20the%20human%20body%20and%20the%20difficulty%20of%20annotating%20granular%0Astructures.%20In%20this%20paper%2C%20we%20propose%20Human-MME%2C%20a%20curated%20benchmark%20designed%0Ato%20provide%20a%20more%20holistic%20evaluation%20of%20MLLMs%20in%20human-centric%20scene%0Aunderstanding.%20Compared%20with%20other%20existing%20benchmarks%2C%20our%20work%20provides%20three%0Akey%20features%3A%201.%20Diversity%20in%20human%20scene%2C%20spanning%204%20primary%20visual%20domains%0Awith%2015%20secondary%20domains%20and%2043%20sub-fields%20to%20ensure%20broad%20scenario%20coverage.%0A2.%20Progressive%20and%20diverse%20evaluation%20dimensions%2C%20evaluating%20the%20human-based%0Aactivities%20progressively%20from%20the%20human-oriented%20granular%20perception%20to%20the%0Ahigher-dimensional%20reasoning%2C%20consisting%20of%20eight%20dimensions%20with%2019%2C945%0Areal-world%20image%20question%20pairs%20and%20an%20evaluation%20suite.%203.%20High-quality%0Aannotations%20with%20rich%20data%20paradigms%2C%20constructing%20the%20automated%20annotation%0Apipeline%20and%20human-annotation%20platform%2C%20supporting%20rigorous%20manual%20labeling%20to%0Afacilitate%20precise%20and%20reliable%20model%20assessment.%20Our%20benchmark%20extends%20the%0Asingle-target%20understanding%20to%20the%20multi-person%20and%20multi-image%20mutual%0Aunderstanding%20by%20constructing%20the%20choice%2C%20short-answer%2C%20grounding%2C%20ranking%20and%0Ajudgment%20question%20components%2C%20and%20complex%20questions%20of%20their%20combination.%20The%0Aextensive%20experiments%20on%2017%20state-of-the-art%20MLLMs%20effectively%20expose%20the%0Alimitations%20and%20guide%20future%20MLLMs%20research%20toward%20better%20human-centric%20image%0Aunderstanding.%20All%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/Yuan-Hou/Human-MME.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26165v2&entry.124074799=Read"},
{"title": "Probabilistic Temporal Masked Attention for Cross-view Online Action\n  Detection", "author": "Liping Xie and Yang Tan and Shicheng Jing and Huimin Lu and Kanjian Zhang", "abstract": "  As a critical task in video sequence classification within computer vision,\nOnline Action Detection (OAD) has garnered significant attention. The\nsensitivity of mainstream OAD models to varying video viewpoints often hampers\ntheir generalization when confronted with unseen sources. To address this\nlimitation, we propose a novel Probabilistic Temporal Masked Attention (PTMA)\nmodel, which leverages probabilistic modeling to derive latent compressed\nrepresentations of video frames in a cross-view setting. The PTMA model\nincorporates a GRU-based temporal masked attention (TMA) cell, which leverages\nthese representations to effectively query the input video sequence, thereby\nenhancing information interaction and facilitating autoregressive frame-level\nvideo analysis. Additionally, multi-view information can be integrated into the\nprobabilistic modeling to facilitate the extraction of view-invariant features.\nExperiments conducted under three evaluation protocols: cross-subject (cs),\ncross-view (cv), and cross-subject-view (csv) show that PTMA achieves\nstate-of-the-art performance on the DAHLIA, IKEA ASM, and Breakfast datasets.\n", "link": "http://arxiv.org/abs/2508.17025v2", "date": "2025-10-14", "relevancy": 2.338, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6032}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5712}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Temporal%20Masked%20Attention%20for%20Cross-view%20Online%20Action%0A%20%20Detection&body=Title%3A%20Probabilistic%20Temporal%20Masked%20Attention%20for%20Cross-view%20Online%20Action%0A%20%20Detection%0AAuthor%3A%20Liping%20Xie%20and%20Yang%20Tan%20and%20Shicheng%20Jing%20and%20Huimin%20Lu%20and%20Kanjian%20Zhang%0AAbstract%3A%20%20%20As%20a%20critical%20task%20in%20video%20sequence%20classification%20within%20computer%20vision%2C%0AOnline%20Action%20Detection%20%28OAD%29%20has%20garnered%20significant%20attention.%20The%0Asensitivity%20of%20mainstream%20OAD%20models%20to%20varying%20video%20viewpoints%20often%20hampers%0Atheir%20generalization%20when%20confronted%20with%20unseen%20sources.%20To%20address%20this%0Alimitation%2C%20we%20propose%20a%20novel%20Probabilistic%20Temporal%20Masked%20Attention%20%28PTMA%29%0Amodel%2C%20which%20leverages%20probabilistic%20modeling%20to%20derive%20latent%20compressed%0Arepresentations%20of%20video%20frames%20in%20a%20cross-view%20setting.%20The%20PTMA%20model%0Aincorporates%20a%20GRU-based%20temporal%20masked%20attention%20%28TMA%29%20cell%2C%20which%20leverages%0Athese%20representations%20to%20effectively%20query%20the%20input%20video%20sequence%2C%20thereby%0Aenhancing%20information%20interaction%20and%20facilitating%20autoregressive%20frame-level%0Avideo%20analysis.%20Additionally%2C%20multi-view%20information%20can%20be%20integrated%20into%20the%0Aprobabilistic%20modeling%20to%20facilitate%20the%20extraction%20of%20view-invariant%20features.%0AExperiments%20conducted%20under%20three%20evaluation%20protocols%3A%20cross-subject%20%28cs%29%2C%0Across-view%20%28cv%29%2C%20and%20cross-subject-view%20%28csv%29%20show%20that%20PTMA%20achieves%0Astate-of-the-art%20performance%20on%20the%20DAHLIA%2C%20IKEA%20ASM%2C%20and%20Breakfast%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17025v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Temporal%2520Masked%2520Attention%2520for%2520Cross-view%2520Online%2520Action%250A%2520%2520Detection%26entry.906535625%3DLiping%2520Xie%2520and%2520Yang%2520Tan%2520and%2520Shicheng%2520Jing%2520and%2520Huimin%2520Lu%2520and%2520Kanjian%2520Zhang%26entry.1292438233%3D%2520%2520As%2520a%2520critical%2520task%2520in%2520video%2520sequence%2520classification%2520within%2520computer%2520vision%252C%250AOnline%2520Action%2520Detection%2520%2528OAD%2529%2520has%2520garnered%2520significant%2520attention.%2520The%250Asensitivity%2520of%2520mainstream%2520OAD%2520models%2520to%2520varying%2520video%2520viewpoints%2520often%2520hampers%250Atheir%2520generalization%2520when%2520confronted%2520with%2520unseen%2520sources.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520a%2520novel%2520Probabilistic%2520Temporal%2520Masked%2520Attention%2520%2528PTMA%2529%250Amodel%252C%2520which%2520leverages%2520probabilistic%2520modeling%2520to%2520derive%2520latent%2520compressed%250Arepresentations%2520of%2520video%2520frames%2520in%2520a%2520cross-view%2520setting.%2520The%2520PTMA%2520model%250Aincorporates%2520a%2520GRU-based%2520temporal%2520masked%2520attention%2520%2528TMA%2529%2520cell%252C%2520which%2520leverages%250Athese%2520representations%2520to%2520effectively%2520query%2520the%2520input%2520video%2520sequence%252C%2520thereby%250Aenhancing%2520information%2520interaction%2520and%2520facilitating%2520autoregressive%2520frame-level%250Avideo%2520analysis.%2520Additionally%252C%2520multi-view%2520information%2520can%2520be%2520integrated%2520into%2520the%250Aprobabilistic%2520modeling%2520to%2520facilitate%2520the%2520extraction%2520of%2520view-invariant%2520features.%250AExperiments%2520conducted%2520under%2520three%2520evaluation%2520protocols%253A%2520cross-subject%2520%2528cs%2529%252C%250Across-view%2520%2528cv%2529%252C%2520and%2520cross-subject-view%2520%2528csv%2529%2520show%2520that%2520PTMA%2520achieves%250Astate-of-the-art%2520performance%2520on%2520the%2520DAHLIA%252C%2520IKEA%2520ASM%252C%2520and%2520Breakfast%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17025v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Temporal%20Masked%20Attention%20for%20Cross-view%20Online%20Action%0A%20%20Detection&entry.906535625=Liping%20Xie%20and%20Yang%20Tan%20and%20Shicheng%20Jing%20and%20Huimin%20Lu%20and%20Kanjian%20Zhang&entry.1292438233=%20%20As%20a%20critical%20task%20in%20video%20sequence%20classification%20within%20computer%20vision%2C%0AOnline%20Action%20Detection%20%28OAD%29%20has%20garnered%20significant%20attention.%20The%0Asensitivity%20of%20mainstream%20OAD%20models%20to%20varying%20video%20viewpoints%20often%20hampers%0Atheir%20generalization%20when%20confronted%20with%20unseen%20sources.%20To%20address%20this%0Alimitation%2C%20we%20propose%20a%20novel%20Probabilistic%20Temporal%20Masked%20Attention%20%28PTMA%29%0Amodel%2C%20which%20leverages%20probabilistic%20modeling%20to%20derive%20latent%20compressed%0Arepresentations%20of%20video%20frames%20in%20a%20cross-view%20setting.%20The%20PTMA%20model%0Aincorporates%20a%20GRU-based%20temporal%20masked%20attention%20%28TMA%29%20cell%2C%20which%20leverages%0Athese%20representations%20to%20effectively%20query%20the%20input%20video%20sequence%2C%20thereby%0Aenhancing%20information%20interaction%20and%20facilitating%20autoregressive%20frame-level%0Avideo%20analysis.%20Additionally%2C%20multi-view%20information%20can%20be%20integrated%20into%20the%0Aprobabilistic%20modeling%20to%20facilitate%20the%20extraction%20of%20view-invariant%20features.%0AExperiments%20conducted%20under%20three%20evaluation%20protocols%3A%20cross-subject%20%28cs%29%2C%0Across-view%20%28cv%29%2C%20and%20cross-subject-view%20%28csv%29%20show%20that%20PTMA%20achieves%0Astate-of-the-art%20performance%20on%20the%20DAHLIA%2C%20IKEA%20ASM%2C%20and%20Breakfast%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17025v2&entry.124074799=Read"},
{"title": "E-MoFlow: Learning Egomotion and Optical Flow from Event Data via\n  Implicit Regularization", "author": "Wenpu Li and Bangyan Liao and Yi Zhou and Qi Xu and Pian Wan and Peidong Liu", "abstract": "  The estimation of optical flow and 6-DoF ego-motion, two fundamental tasks in\n3D vision, has typically been addressed independently. For neuromorphic vision\n(e.g., event cameras), however, the lack of robust data association makes\nsolving the two problems separately an ill-posed challenge, especially in the\nabsence of supervision via ground truth. Existing works mitigate this\nill-posedness by either enforcing the smoothness of the flow field via an\nexplicit variational regularizer or leveraging explicit structure-and-motion\npriors in the parametrization to improve event alignment. The former notably\nintroduces bias in results and computational overhead, while the latter, which\nparametrizes the optical flow in terms of the scene depth and the camera\nmotion, often converges to suboptimal local minima. To address these issues, we\npropose an unsupervised framework that jointly optimizes egomotion and optical\nflow via implicit spatial-temporal and geometric regularization. First, by\nmodeling camera's egomotion as a continuous spline and optical flow as an\nimplicit neural representation, our method inherently embeds spatial-temporal\ncoherence through inductive biases. Second, we incorporate structure-and-motion\npriors through differential geometric constraints, bypassing explicit depth\nestimation while maintaining rigorous geometric consistency. As a result, our\nframework (called E-MoFlow) unifies egomotion and optical flow estimation via\nimplicit regularization under a fully unsupervised paradigm. Experiments\ndemonstrate its versatility to general 6-DoF motion scenarios, achieving\nstate-of-the-art performance among unsupervised methods and competitive even\nwith supervised approaches.\n", "link": "http://arxiv.org/abs/2510.12753v1", "date": "2025-10-14", "relevancy": 2.3102, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5917}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5725}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E-MoFlow%3A%20Learning%20Egomotion%20and%20Optical%20Flow%20from%20Event%20Data%20via%0A%20%20Implicit%20Regularization&body=Title%3A%20E-MoFlow%3A%20Learning%20Egomotion%20and%20Optical%20Flow%20from%20Event%20Data%20via%0A%20%20Implicit%20Regularization%0AAuthor%3A%20Wenpu%20Li%20and%20Bangyan%20Liao%20and%20Yi%20Zhou%20and%20Qi%20Xu%20and%20Pian%20Wan%20and%20Peidong%20Liu%0AAbstract%3A%20%20%20The%20estimation%20of%20optical%20flow%20and%206-DoF%20ego-motion%2C%20two%20fundamental%20tasks%20in%0A3D%20vision%2C%20has%20typically%20been%20addressed%20independently.%20For%20neuromorphic%20vision%0A%28e.g.%2C%20event%20cameras%29%2C%20however%2C%20the%20lack%20of%20robust%20data%20association%20makes%0Asolving%20the%20two%20problems%20separately%20an%20ill-posed%20challenge%2C%20especially%20in%20the%0Aabsence%20of%20supervision%20via%20ground%20truth.%20Existing%20works%20mitigate%20this%0Aill-posedness%20by%20either%20enforcing%20the%20smoothness%20of%20the%20flow%20field%20via%20an%0Aexplicit%20variational%20regularizer%20or%20leveraging%20explicit%20structure-and-motion%0Apriors%20in%20the%20parametrization%20to%20improve%20event%20alignment.%20The%20former%20notably%0Aintroduces%20bias%20in%20results%20and%20computational%20overhead%2C%20while%20the%20latter%2C%20which%0Aparametrizes%20the%20optical%20flow%20in%20terms%20of%20the%20scene%20depth%20and%20the%20camera%0Amotion%2C%20often%20converges%20to%20suboptimal%20local%20minima.%20To%20address%20these%20issues%2C%20we%0Apropose%20an%20unsupervised%20framework%20that%20jointly%20optimizes%20egomotion%20and%20optical%0Aflow%20via%20implicit%20spatial-temporal%20and%20geometric%20regularization.%20First%2C%20by%0Amodeling%20camera%27s%20egomotion%20as%20a%20continuous%20spline%20and%20optical%20flow%20as%20an%0Aimplicit%20neural%20representation%2C%20our%20method%20inherently%20embeds%20spatial-temporal%0Acoherence%20through%20inductive%20biases.%20Second%2C%20we%20incorporate%20structure-and-motion%0Apriors%20through%20differential%20geometric%20constraints%2C%20bypassing%20explicit%20depth%0Aestimation%20while%20maintaining%20rigorous%20geometric%20consistency.%20As%20a%20result%2C%20our%0Aframework%20%28called%20E-MoFlow%29%20unifies%20egomotion%20and%20optical%20flow%20estimation%20via%0Aimplicit%20regularization%20under%20a%20fully%20unsupervised%20paradigm.%20Experiments%0Ademonstrate%20its%20versatility%20to%20general%206-DoF%20motion%20scenarios%2C%20achieving%0Astate-of-the-art%20performance%20among%20unsupervised%20methods%20and%20competitive%20even%0Awith%20supervised%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE-MoFlow%253A%2520Learning%2520Egomotion%2520and%2520Optical%2520Flow%2520from%2520Event%2520Data%2520via%250A%2520%2520Implicit%2520Regularization%26entry.906535625%3DWenpu%2520Li%2520and%2520Bangyan%2520Liao%2520and%2520Yi%2520Zhou%2520and%2520Qi%2520Xu%2520and%2520Pian%2520Wan%2520and%2520Peidong%2520Liu%26entry.1292438233%3D%2520%2520The%2520estimation%2520of%2520optical%2520flow%2520and%25206-DoF%2520ego-motion%252C%2520two%2520fundamental%2520tasks%2520in%250A3D%2520vision%252C%2520has%2520typically%2520been%2520addressed%2520independently.%2520For%2520neuromorphic%2520vision%250A%2528e.g.%252C%2520event%2520cameras%2529%252C%2520however%252C%2520the%2520lack%2520of%2520robust%2520data%2520association%2520makes%250Asolving%2520the%2520two%2520problems%2520separately%2520an%2520ill-posed%2520challenge%252C%2520especially%2520in%2520the%250Aabsence%2520of%2520supervision%2520via%2520ground%2520truth.%2520Existing%2520works%2520mitigate%2520this%250Aill-posedness%2520by%2520either%2520enforcing%2520the%2520smoothness%2520of%2520the%2520flow%2520field%2520via%2520an%250Aexplicit%2520variational%2520regularizer%2520or%2520leveraging%2520explicit%2520structure-and-motion%250Apriors%2520in%2520the%2520parametrization%2520to%2520improve%2520event%2520alignment.%2520The%2520former%2520notably%250Aintroduces%2520bias%2520in%2520results%2520and%2520computational%2520overhead%252C%2520while%2520the%2520latter%252C%2520which%250Aparametrizes%2520the%2520optical%2520flow%2520in%2520terms%2520of%2520the%2520scene%2520depth%2520and%2520the%2520camera%250Amotion%252C%2520often%2520converges%2520to%2520suboptimal%2520local%2520minima.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520an%2520unsupervised%2520framework%2520that%2520jointly%2520optimizes%2520egomotion%2520and%2520optical%250Aflow%2520via%2520implicit%2520spatial-temporal%2520and%2520geometric%2520regularization.%2520First%252C%2520by%250Amodeling%2520camera%2527s%2520egomotion%2520as%2520a%2520continuous%2520spline%2520and%2520optical%2520flow%2520as%2520an%250Aimplicit%2520neural%2520representation%252C%2520our%2520method%2520inherently%2520embeds%2520spatial-temporal%250Acoherence%2520through%2520inductive%2520biases.%2520Second%252C%2520we%2520incorporate%2520structure-and-motion%250Apriors%2520through%2520differential%2520geometric%2520constraints%252C%2520bypassing%2520explicit%2520depth%250Aestimation%2520while%2520maintaining%2520rigorous%2520geometric%2520consistency.%2520As%2520a%2520result%252C%2520our%250Aframework%2520%2528called%2520E-MoFlow%2529%2520unifies%2520egomotion%2520and%2520optical%2520flow%2520estimation%2520via%250Aimplicit%2520regularization%2520under%2520a%2520fully%2520unsupervised%2520paradigm.%2520Experiments%250Ademonstrate%2520its%2520versatility%2520to%2520general%25206-DoF%2520motion%2520scenarios%252C%2520achieving%250Astate-of-the-art%2520performance%2520among%2520unsupervised%2520methods%2520and%2520competitive%2520even%250Awith%2520supervised%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E-MoFlow%3A%20Learning%20Egomotion%20and%20Optical%20Flow%20from%20Event%20Data%20via%0A%20%20Implicit%20Regularization&entry.906535625=Wenpu%20Li%20and%20Bangyan%20Liao%20and%20Yi%20Zhou%20and%20Qi%20Xu%20and%20Pian%20Wan%20and%20Peidong%20Liu&entry.1292438233=%20%20The%20estimation%20of%20optical%20flow%20and%206-DoF%20ego-motion%2C%20two%20fundamental%20tasks%20in%0A3D%20vision%2C%20has%20typically%20been%20addressed%20independently.%20For%20neuromorphic%20vision%0A%28e.g.%2C%20event%20cameras%29%2C%20however%2C%20the%20lack%20of%20robust%20data%20association%20makes%0Asolving%20the%20two%20problems%20separately%20an%20ill-posed%20challenge%2C%20especially%20in%20the%0Aabsence%20of%20supervision%20via%20ground%20truth.%20Existing%20works%20mitigate%20this%0Aill-posedness%20by%20either%20enforcing%20the%20smoothness%20of%20the%20flow%20field%20via%20an%0Aexplicit%20variational%20regularizer%20or%20leveraging%20explicit%20structure-and-motion%0Apriors%20in%20the%20parametrization%20to%20improve%20event%20alignment.%20The%20former%20notably%0Aintroduces%20bias%20in%20results%20and%20computational%20overhead%2C%20while%20the%20latter%2C%20which%0Aparametrizes%20the%20optical%20flow%20in%20terms%20of%20the%20scene%20depth%20and%20the%20camera%0Amotion%2C%20often%20converges%20to%20suboptimal%20local%20minima.%20To%20address%20these%20issues%2C%20we%0Apropose%20an%20unsupervised%20framework%20that%20jointly%20optimizes%20egomotion%20and%20optical%0Aflow%20via%20implicit%20spatial-temporal%20and%20geometric%20regularization.%20First%2C%20by%0Amodeling%20camera%27s%20egomotion%20as%20a%20continuous%20spline%20and%20optical%20flow%20as%20an%0Aimplicit%20neural%20representation%2C%20our%20method%20inherently%20embeds%20spatial-temporal%0Acoherence%20through%20inductive%20biases.%20Second%2C%20we%20incorporate%20structure-and-motion%0Apriors%20through%20differential%20geometric%20constraints%2C%20bypassing%20explicit%20depth%0Aestimation%20while%20maintaining%20rigorous%20geometric%20consistency.%20As%20a%20result%2C%20our%0Aframework%20%28called%20E-MoFlow%29%20unifies%20egomotion%20and%20optical%20flow%20estimation%20via%0Aimplicit%20regularization%20under%20a%20fully%20unsupervised%20paradigm.%20Experiments%0Ademonstrate%20its%20versatility%20to%20general%206-DoF%20motion%20scenarios%2C%20achieving%0Astate-of-the-art%20performance%20among%20unsupervised%20methods%20and%20competitive%20even%0Awith%20supervised%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12753v1&entry.124074799=Read"},
{"title": "ACCO: Accumulate While You Communicate for Communication-Overlapped\n  Sharded LLM Training", "author": "Adel Nabli and Louis Fournier and Pierre Erbacher and Louis Serrano and Eugene Belilovsky and Edouard Oyallon", "abstract": "  Training LLMs relies on distributed implementations using multiple GPUs to\ncompute gradients in parallel with sharded optimizers. However, synchronizing\ngradients in data parallel setups introduces communication overhead that grows\nwith the number of workers, limiting parallelization efficiency. Local\noptimization algorithms reduce communications but incur high memory costs as\nthey prevent optimizer state sharding, hindering scalability. To address this,\nwe propose \\textbf{AC}cumulate while \\textbf{CO}mmunicate (ACCO), a\nmemory-efficient optimization algorithm for distributed LLM training. By\nsynchronizing delayed gradients while computing new ones, ACCO reduces GPU idle\ntime and supports heterogeneous hardware. To mitigate the convergence issues\ncaused by delayed updates, we introduce a novel technique ensuring training\ndynamics align with standard distributed optimization. Compared to ZeRO-1, our\napproach is significantly faster and scales effectively across heterogeneous\nhardware.\n", "link": "http://arxiv.org/abs/2406.02613v3", "date": "2025-10-14", "relevancy": 2.3069, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4687}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4594}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ACCO%3A%20Accumulate%20While%20You%20Communicate%20for%20Communication-Overlapped%0A%20%20Sharded%20LLM%20Training&body=Title%3A%20ACCO%3A%20Accumulate%20While%20You%20Communicate%20for%20Communication-Overlapped%0A%20%20Sharded%20LLM%20Training%0AAuthor%3A%20Adel%20Nabli%20and%20Louis%20Fournier%20and%20Pierre%20Erbacher%20and%20Louis%20Serrano%20and%20Eugene%20Belilovsky%20and%20Edouard%20Oyallon%0AAbstract%3A%20%20%20Training%20LLMs%20relies%20on%20distributed%20implementations%20using%20multiple%20GPUs%20to%0Acompute%20gradients%20in%20parallel%20with%20sharded%20optimizers.%20However%2C%20synchronizing%0Agradients%20in%20data%20parallel%20setups%20introduces%20communication%20overhead%20that%20grows%0Awith%20the%20number%20of%20workers%2C%20limiting%20parallelization%20efficiency.%20Local%0Aoptimization%20algorithms%20reduce%20communications%20but%20incur%20high%20memory%20costs%20as%0Athey%20prevent%20optimizer%20state%20sharding%2C%20hindering%20scalability.%20To%20address%20this%2C%0Awe%20propose%20%5Ctextbf%7BAC%7Dcumulate%20while%20%5Ctextbf%7BCO%7Dmmunicate%20%28ACCO%29%2C%20a%0Amemory-efficient%20optimization%20algorithm%20for%20distributed%20LLM%20training.%20By%0Asynchronizing%20delayed%20gradients%20while%20computing%20new%20ones%2C%20ACCO%20reduces%20GPU%20idle%0Atime%20and%20supports%20heterogeneous%20hardware.%20To%20mitigate%20the%20convergence%20issues%0Acaused%20by%20delayed%20updates%2C%20we%20introduce%20a%20novel%20technique%20ensuring%20training%0Adynamics%20align%20with%20standard%20distributed%20optimization.%20Compared%20to%20ZeRO-1%2C%20our%0Aapproach%20is%20significantly%20faster%20and%20scales%20effectively%20across%20heterogeneous%0Ahardware.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02613v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DACCO%253A%2520Accumulate%2520While%2520You%2520Communicate%2520for%2520Communication-Overlapped%250A%2520%2520Sharded%2520LLM%2520Training%26entry.906535625%3DAdel%2520Nabli%2520and%2520Louis%2520Fournier%2520and%2520Pierre%2520Erbacher%2520and%2520Louis%2520Serrano%2520and%2520Eugene%2520Belilovsky%2520and%2520Edouard%2520Oyallon%26entry.1292438233%3D%2520%2520Training%2520LLMs%2520relies%2520on%2520distributed%2520implementations%2520using%2520multiple%2520GPUs%2520to%250Acompute%2520gradients%2520in%2520parallel%2520with%2520sharded%2520optimizers.%2520However%252C%2520synchronizing%250Agradients%2520in%2520data%2520parallel%2520setups%2520introduces%2520communication%2520overhead%2520that%2520grows%250Awith%2520the%2520number%2520of%2520workers%252C%2520limiting%2520parallelization%2520efficiency.%2520Local%250Aoptimization%2520algorithms%2520reduce%2520communications%2520but%2520incur%2520high%2520memory%2520costs%2520as%250Athey%2520prevent%2520optimizer%2520state%2520sharding%252C%2520hindering%2520scalability.%2520To%2520address%2520this%252C%250Awe%2520propose%2520%255Ctextbf%257BAC%257Dcumulate%2520while%2520%255Ctextbf%257BCO%257Dmmunicate%2520%2528ACCO%2529%252C%2520a%250Amemory-efficient%2520optimization%2520algorithm%2520for%2520distributed%2520LLM%2520training.%2520By%250Asynchronizing%2520delayed%2520gradients%2520while%2520computing%2520new%2520ones%252C%2520ACCO%2520reduces%2520GPU%2520idle%250Atime%2520and%2520supports%2520heterogeneous%2520hardware.%2520To%2520mitigate%2520the%2520convergence%2520issues%250Acaused%2520by%2520delayed%2520updates%252C%2520we%2520introduce%2520a%2520novel%2520technique%2520ensuring%2520training%250Adynamics%2520align%2520with%2520standard%2520distributed%2520optimization.%2520Compared%2520to%2520ZeRO-1%252C%2520our%250Aapproach%2520is%2520significantly%2520faster%2520and%2520scales%2520effectively%2520across%2520heterogeneous%250Ahardware.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02613v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ACCO%3A%20Accumulate%20While%20You%20Communicate%20for%20Communication-Overlapped%0A%20%20Sharded%20LLM%20Training&entry.906535625=Adel%20Nabli%20and%20Louis%20Fournier%20and%20Pierre%20Erbacher%20and%20Louis%20Serrano%20and%20Eugene%20Belilovsky%20and%20Edouard%20Oyallon&entry.1292438233=%20%20Training%20LLMs%20relies%20on%20distributed%20implementations%20using%20multiple%20GPUs%20to%0Acompute%20gradients%20in%20parallel%20with%20sharded%20optimizers.%20However%2C%20synchronizing%0Agradients%20in%20data%20parallel%20setups%20introduces%20communication%20overhead%20that%20grows%0Awith%20the%20number%20of%20workers%2C%20limiting%20parallelization%20efficiency.%20Local%0Aoptimization%20algorithms%20reduce%20communications%20but%20incur%20high%20memory%20costs%20as%0Athey%20prevent%20optimizer%20state%20sharding%2C%20hindering%20scalability.%20To%20address%20this%2C%0Awe%20propose%20%5Ctextbf%7BAC%7Dcumulate%20while%20%5Ctextbf%7BCO%7Dmmunicate%20%28ACCO%29%2C%20a%0Amemory-efficient%20optimization%20algorithm%20for%20distributed%20LLM%20training.%20By%0Asynchronizing%20delayed%20gradients%20while%20computing%20new%20ones%2C%20ACCO%20reduces%20GPU%20idle%0Atime%20and%20supports%20heterogeneous%20hardware.%20To%20mitigate%20the%20convergence%20issues%0Acaused%20by%20delayed%20updates%2C%20we%20introduce%20a%20novel%20technique%20ensuring%20training%0Adynamics%20align%20with%20standard%20distributed%20optimization.%20Compared%20to%20ZeRO-1%2C%20our%0Aapproach%20is%20significantly%20faster%20and%20scales%20effectively%20across%20heterogeneous%0Ahardware.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02613v3&entry.124074799=Read"},
{"title": "SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models", "author": "Weiyang Jin and Yuwei Niu and Jiaqi Liao and Chengqi Duan and Aoxue Li and Shenghua Gao and Xihui Liu", "abstract": "  Recently, remarkable progress has been made in Unified Multimodal Models\n(UMMs), which integrate vision-language generation and understanding\ncapabilities within a single framework. However, a significant gap exists where\na model's strong visual understanding often fails to transfer to its visual\ngeneration. A model might correctly understand an image based on user\ninstructions, yet be unable to generate a faithful image from text prompts.\nThis phenomenon directly raises a compelling question: Can a model achieve\nself-improvement by using its understanding module to reward its generation\nmodule? To bridge this gap and achieve self-improvement, we introduce SRUM, a\nself-rewarding post-training framework that can be directly applied to existing\nUMMs of various designs. SRUM creates a feedback loop where the model's own\nunderstanding module acts as an internal ``evaluator'', providing corrective\nsignals to improve its generation module, without requiring additional\nhuman-labeled data. To ensure this feedback is comprehensive, we designed a\nglobal-local dual reward system. To tackle the inherent structural complexity\nof images, this system offers multi-scale guidance: a \\textbf{global reward}\nensures the correctness of the overall visual semantics and layout, while a\n\\textbf{local reward} refines fine-grained, object-level fidelity. SRUM leads\nto powerful capabilities and shows strong generalization, boosting performance\non T2I-CompBench from 82.18 to \\textbf{88.37} and on T2I-ReasonBench from 43.82\nto \\textbf{46.75}. Overall, our work establishes a powerful new paradigm for\nenabling a UMMs' understanding module to guide and enhance its own generation\nvia self-rewarding.\n", "link": "http://arxiv.org/abs/2510.12784v1", "date": "2025-10-14", "relevancy": 2.2757, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5769}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5725}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SRUM%3A%20Fine-Grained%20Self-Rewarding%20for%20Unified%20Multimodal%20Models&body=Title%3A%20SRUM%3A%20Fine-Grained%20Self-Rewarding%20for%20Unified%20Multimodal%20Models%0AAuthor%3A%20Weiyang%20Jin%20and%20Yuwei%20Niu%20and%20Jiaqi%20Liao%20and%20Chengqi%20Duan%20and%20Aoxue%20Li%20and%20Shenghua%20Gao%20and%20Xihui%20Liu%0AAbstract%3A%20%20%20Recently%2C%20remarkable%20progress%20has%20been%20made%20in%20Unified%20Multimodal%20Models%0A%28UMMs%29%2C%20which%20integrate%20vision-language%20generation%20and%20understanding%0Acapabilities%20within%20a%20single%20framework.%20However%2C%20a%20significant%20gap%20exists%20where%0Aa%20model%27s%20strong%20visual%20understanding%20often%20fails%20to%20transfer%20to%20its%20visual%0Ageneration.%20A%20model%20might%20correctly%20understand%20an%20image%20based%20on%20user%0Ainstructions%2C%20yet%20be%20unable%20to%20generate%20a%20faithful%20image%20from%20text%20prompts.%0AThis%20phenomenon%20directly%20raises%20a%20compelling%20question%3A%20Can%20a%20model%20achieve%0Aself-improvement%20by%20using%20its%20understanding%20module%20to%20reward%20its%20generation%0Amodule%3F%20To%20bridge%20this%20gap%20and%20achieve%20self-improvement%2C%20we%20introduce%20SRUM%2C%20a%0Aself-rewarding%20post-training%20framework%20that%20can%20be%20directly%20applied%20to%20existing%0AUMMs%20of%20various%20designs.%20SRUM%20creates%20a%20feedback%20loop%20where%20the%20model%27s%20own%0Aunderstanding%20module%20acts%20as%20an%20internal%20%60%60evaluator%27%27%2C%20providing%20corrective%0Asignals%20to%20improve%20its%20generation%20module%2C%20without%20requiring%20additional%0Ahuman-labeled%20data.%20To%20ensure%20this%20feedback%20is%20comprehensive%2C%20we%20designed%20a%0Aglobal-local%20dual%20reward%20system.%20To%20tackle%20the%20inherent%20structural%20complexity%0Aof%20images%2C%20this%20system%20offers%20multi-scale%20guidance%3A%20a%20%5Ctextbf%7Bglobal%20reward%7D%0Aensures%20the%20correctness%20of%20the%20overall%20visual%20semantics%20and%20layout%2C%20while%20a%0A%5Ctextbf%7Blocal%20reward%7D%20refines%20fine-grained%2C%20object-level%20fidelity.%20SRUM%20leads%0Ato%20powerful%20capabilities%20and%20shows%20strong%20generalization%2C%20boosting%20performance%0Aon%20T2I-CompBench%20from%2082.18%20to%20%5Ctextbf%7B88.37%7D%20and%20on%20T2I-ReasonBench%20from%2043.82%0Ato%20%5Ctextbf%7B46.75%7D.%20Overall%2C%20our%20work%20establishes%20a%20powerful%20new%20paradigm%20for%0Aenabling%20a%20UMMs%27%20understanding%20module%20to%20guide%20and%20enhance%20its%20own%20generation%0Avia%20self-rewarding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSRUM%253A%2520Fine-Grained%2520Self-Rewarding%2520for%2520Unified%2520Multimodal%2520Models%26entry.906535625%3DWeiyang%2520Jin%2520and%2520Yuwei%2520Niu%2520and%2520Jiaqi%2520Liao%2520and%2520Chengqi%2520Duan%2520and%2520Aoxue%2520Li%2520and%2520Shenghua%2520Gao%2520and%2520Xihui%2520Liu%26entry.1292438233%3D%2520%2520Recently%252C%2520remarkable%2520progress%2520has%2520been%2520made%2520in%2520Unified%2520Multimodal%2520Models%250A%2528UMMs%2529%252C%2520which%2520integrate%2520vision-language%2520generation%2520and%2520understanding%250Acapabilities%2520within%2520a%2520single%2520framework.%2520However%252C%2520a%2520significant%2520gap%2520exists%2520where%250Aa%2520model%2527s%2520strong%2520visual%2520understanding%2520often%2520fails%2520to%2520transfer%2520to%2520its%2520visual%250Ageneration.%2520A%2520model%2520might%2520correctly%2520understand%2520an%2520image%2520based%2520on%2520user%250Ainstructions%252C%2520yet%2520be%2520unable%2520to%2520generate%2520a%2520faithful%2520image%2520from%2520text%2520prompts.%250AThis%2520phenomenon%2520directly%2520raises%2520a%2520compelling%2520question%253A%2520Can%2520a%2520model%2520achieve%250Aself-improvement%2520by%2520using%2520its%2520understanding%2520module%2520to%2520reward%2520its%2520generation%250Amodule%253F%2520To%2520bridge%2520this%2520gap%2520and%2520achieve%2520self-improvement%252C%2520we%2520introduce%2520SRUM%252C%2520a%250Aself-rewarding%2520post-training%2520framework%2520that%2520can%2520be%2520directly%2520applied%2520to%2520existing%250AUMMs%2520of%2520various%2520designs.%2520SRUM%2520creates%2520a%2520feedback%2520loop%2520where%2520the%2520model%2527s%2520own%250Aunderstanding%2520module%2520acts%2520as%2520an%2520internal%2520%2560%2560evaluator%2527%2527%252C%2520providing%2520corrective%250Asignals%2520to%2520improve%2520its%2520generation%2520module%252C%2520without%2520requiring%2520additional%250Ahuman-labeled%2520data.%2520To%2520ensure%2520this%2520feedback%2520is%2520comprehensive%252C%2520we%2520designed%2520a%250Aglobal-local%2520dual%2520reward%2520system.%2520To%2520tackle%2520the%2520inherent%2520structural%2520complexity%250Aof%2520images%252C%2520this%2520system%2520offers%2520multi-scale%2520guidance%253A%2520a%2520%255Ctextbf%257Bglobal%2520reward%257D%250Aensures%2520the%2520correctness%2520of%2520the%2520overall%2520visual%2520semantics%2520and%2520layout%252C%2520while%2520a%250A%255Ctextbf%257Blocal%2520reward%257D%2520refines%2520fine-grained%252C%2520object-level%2520fidelity.%2520SRUM%2520leads%250Ato%2520powerful%2520capabilities%2520and%2520shows%2520strong%2520generalization%252C%2520boosting%2520performance%250Aon%2520T2I-CompBench%2520from%252082.18%2520to%2520%255Ctextbf%257B88.37%257D%2520and%2520on%2520T2I-ReasonBench%2520from%252043.82%250Ato%2520%255Ctextbf%257B46.75%257D.%2520Overall%252C%2520our%2520work%2520establishes%2520a%2520powerful%2520new%2520paradigm%2520for%250Aenabling%2520a%2520UMMs%2527%2520understanding%2520module%2520to%2520guide%2520and%2520enhance%2520its%2520own%2520generation%250Avia%2520self-rewarding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SRUM%3A%20Fine-Grained%20Self-Rewarding%20for%20Unified%20Multimodal%20Models&entry.906535625=Weiyang%20Jin%20and%20Yuwei%20Niu%20and%20Jiaqi%20Liao%20and%20Chengqi%20Duan%20and%20Aoxue%20Li%20and%20Shenghua%20Gao%20and%20Xihui%20Liu&entry.1292438233=%20%20Recently%2C%20remarkable%20progress%20has%20been%20made%20in%20Unified%20Multimodal%20Models%0A%28UMMs%29%2C%20which%20integrate%20vision-language%20generation%20and%20understanding%0Acapabilities%20within%20a%20single%20framework.%20However%2C%20a%20significant%20gap%20exists%20where%0Aa%20model%27s%20strong%20visual%20understanding%20often%20fails%20to%20transfer%20to%20its%20visual%0Ageneration.%20A%20model%20might%20correctly%20understand%20an%20image%20based%20on%20user%0Ainstructions%2C%20yet%20be%20unable%20to%20generate%20a%20faithful%20image%20from%20text%20prompts.%0AThis%20phenomenon%20directly%20raises%20a%20compelling%20question%3A%20Can%20a%20model%20achieve%0Aself-improvement%20by%20using%20its%20understanding%20module%20to%20reward%20its%20generation%0Amodule%3F%20To%20bridge%20this%20gap%20and%20achieve%20self-improvement%2C%20we%20introduce%20SRUM%2C%20a%0Aself-rewarding%20post-training%20framework%20that%20can%20be%20directly%20applied%20to%20existing%0AUMMs%20of%20various%20designs.%20SRUM%20creates%20a%20feedback%20loop%20where%20the%20model%27s%20own%0Aunderstanding%20module%20acts%20as%20an%20internal%20%60%60evaluator%27%27%2C%20providing%20corrective%0Asignals%20to%20improve%20its%20generation%20module%2C%20without%20requiring%20additional%0Ahuman-labeled%20data.%20To%20ensure%20this%20feedback%20is%20comprehensive%2C%20we%20designed%20a%0Aglobal-local%20dual%20reward%20system.%20To%20tackle%20the%20inherent%20structural%20complexity%0Aof%20images%2C%20this%20system%20offers%20multi-scale%20guidance%3A%20a%20%5Ctextbf%7Bglobal%20reward%7D%0Aensures%20the%20correctness%20of%20the%20overall%20visual%20semantics%20and%20layout%2C%20while%20a%0A%5Ctextbf%7Blocal%20reward%7D%20refines%20fine-grained%2C%20object-level%20fidelity.%20SRUM%20leads%0Ato%20powerful%20capabilities%20and%20shows%20strong%20generalization%2C%20boosting%20performance%0Aon%20T2I-CompBench%20from%2082.18%20to%20%5Ctextbf%7B88.37%7D%20and%20on%20T2I-ReasonBench%20from%2043.82%0Ato%20%5Ctextbf%7B46.75%7D.%20Overall%2C%20our%20work%20establishes%20a%20powerful%20new%20paradigm%20for%0Aenabling%20a%20UMMs%27%20understanding%20module%20to%20guide%20and%20enhance%20its%20own%20generation%0Avia%20self-rewarding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12784v1&entry.124074799=Read"},
{"title": "Structure-Aware Spectral Sparsification via Uniform Edge Sampling", "author": "Kaiwen He and Petros Drineas and Rajiv Khanna", "abstract": "  Spectral clustering is a fundamental method for graph partitioning, but its\nreliance on eigenvector computation limits scalability to massive graphs.\nClassical sparsification methods preserve spectral properties by sampling edges\nproportionally to their effective resistances, but require expensive\npreprocessing to estimate these resistances. We study whether uniform edge\nsampling-a simple, structure-agnostic strategy-can suffice for spectral\nclustering. Our main result shows that for graphs admitting a well-separated\n$k$-clustering, characterized by a large structure ratio $\\Upsilon(k) =\n\\lambda_{k+1} / \\rho_G(k)$, uniform sampling preserves the spectral subspace\nused for clustering. Specifically, we prove that uniformly sampling $O(\\gamma^2\nn \\log n / \\epsilon^2)$ edges, where $\\gamma$ is the Laplacian condition\nnumber, yields a sparsifier whose top $(n-k)$-dimensional eigenspace is\napproximately orthogonal to the cluster indicators. This ensures that the\nspectral embedding remains faithful, and clustering quality is preserved. Our\nanalysis introduces new resistance bounds for intra-cluster edges, a\nrank-$(n-k)$ effective resistance formulation, and a matrix Chernoff bound\nadapted to the dominant eigenspace. These tools allow us to bypass importance\nsampling entirely. Conceptually, our result connects recent coreset-based\nclustering theory to spectral sparsification, showing that under strong\nclusterability, even uniform sampling is structure-aware. This provides the\nfirst provable guarantee that uniform edge sampling suffices for\nstructure-preserving spectral clustering.\n", "link": "http://arxiv.org/abs/2510.12669v1", "date": "2025-10-14", "relevancy": 2.2632, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4779}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4562}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure-Aware%20Spectral%20Sparsification%20via%20Uniform%20Edge%20Sampling&body=Title%3A%20Structure-Aware%20Spectral%20Sparsification%20via%20Uniform%20Edge%20Sampling%0AAuthor%3A%20Kaiwen%20He%20and%20Petros%20Drineas%20and%20Rajiv%20Khanna%0AAbstract%3A%20%20%20Spectral%20clustering%20is%20a%20fundamental%20method%20for%20graph%20partitioning%2C%20but%20its%0Areliance%20on%20eigenvector%20computation%20limits%20scalability%20to%20massive%20graphs.%0AClassical%20sparsification%20methods%20preserve%20spectral%20properties%20by%20sampling%20edges%0Aproportionally%20to%20their%20effective%20resistances%2C%20but%20require%20expensive%0Apreprocessing%20to%20estimate%20these%20resistances.%20We%20study%20whether%20uniform%20edge%0Asampling-a%20simple%2C%20structure-agnostic%20strategy-can%20suffice%20for%20spectral%0Aclustering.%20Our%20main%20result%20shows%20that%20for%20graphs%20admitting%20a%20well-separated%0A%24k%24-clustering%2C%20characterized%20by%20a%20large%20structure%20ratio%20%24%5CUpsilon%28k%29%20%3D%0A%5Clambda_%7Bk%2B1%7D%20/%20%5Crho_G%28k%29%24%2C%20uniform%20sampling%20preserves%20the%20spectral%20subspace%0Aused%20for%20clustering.%20Specifically%2C%20we%20prove%20that%20uniformly%20sampling%20%24O%28%5Cgamma%5E2%0An%20%5Clog%20n%20/%20%5Cepsilon%5E2%29%24%20edges%2C%20where%20%24%5Cgamma%24%20is%20the%20Laplacian%20condition%0Anumber%2C%20yields%20a%20sparsifier%20whose%20top%20%24%28n-k%29%24-dimensional%20eigenspace%20is%0Aapproximately%20orthogonal%20to%20the%20cluster%20indicators.%20This%20ensures%20that%20the%0Aspectral%20embedding%20remains%20faithful%2C%20and%20clustering%20quality%20is%20preserved.%20Our%0Aanalysis%20introduces%20new%20resistance%20bounds%20for%20intra-cluster%20edges%2C%20a%0Arank-%24%28n-k%29%24%20effective%20resistance%20formulation%2C%20and%20a%20matrix%20Chernoff%20bound%0Aadapted%20to%20the%20dominant%20eigenspace.%20These%20tools%20allow%20us%20to%20bypass%20importance%0Asampling%20entirely.%20Conceptually%2C%20our%20result%20connects%20recent%20coreset-based%0Aclustering%20theory%20to%20spectral%20sparsification%2C%20showing%20that%20under%20strong%0Aclusterability%2C%20even%20uniform%20sampling%20is%20structure-aware.%20This%20provides%20the%0Afirst%20provable%20guarantee%20that%20uniform%20edge%20sampling%20suffices%20for%0Astructure-preserving%20spectral%20clustering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12669v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure-Aware%2520Spectral%2520Sparsification%2520via%2520Uniform%2520Edge%2520Sampling%26entry.906535625%3DKaiwen%2520He%2520and%2520Petros%2520Drineas%2520and%2520Rajiv%2520Khanna%26entry.1292438233%3D%2520%2520Spectral%2520clustering%2520is%2520a%2520fundamental%2520method%2520for%2520graph%2520partitioning%252C%2520but%2520its%250Areliance%2520on%2520eigenvector%2520computation%2520limits%2520scalability%2520to%2520massive%2520graphs.%250AClassical%2520sparsification%2520methods%2520preserve%2520spectral%2520properties%2520by%2520sampling%2520edges%250Aproportionally%2520to%2520their%2520effective%2520resistances%252C%2520but%2520require%2520expensive%250Apreprocessing%2520to%2520estimate%2520these%2520resistances.%2520We%2520study%2520whether%2520uniform%2520edge%250Asampling-a%2520simple%252C%2520structure-agnostic%2520strategy-can%2520suffice%2520for%2520spectral%250Aclustering.%2520Our%2520main%2520result%2520shows%2520that%2520for%2520graphs%2520admitting%2520a%2520well-separated%250A%2524k%2524-clustering%252C%2520characterized%2520by%2520a%2520large%2520structure%2520ratio%2520%2524%255CUpsilon%2528k%2529%2520%253D%250A%255Clambda_%257Bk%252B1%257D%2520/%2520%255Crho_G%2528k%2529%2524%252C%2520uniform%2520sampling%2520preserves%2520the%2520spectral%2520subspace%250Aused%2520for%2520clustering.%2520Specifically%252C%2520we%2520prove%2520that%2520uniformly%2520sampling%2520%2524O%2528%255Cgamma%255E2%250An%2520%255Clog%2520n%2520/%2520%255Cepsilon%255E2%2529%2524%2520edges%252C%2520where%2520%2524%255Cgamma%2524%2520is%2520the%2520Laplacian%2520condition%250Anumber%252C%2520yields%2520a%2520sparsifier%2520whose%2520top%2520%2524%2528n-k%2529%2524-dimensional%2520eigenspace%2520is%250Aapproximately%2520orthogonal%2520to%2520the%2520cluster%2520indicators.%2520This%2520ensures%2520that%2520the%250Aspectral%2520embedding%2520remains%2520faithful%252C%2520and%2520clustering%2520quality%2520is%2520preserved.%2520Our%250Aanalysis%2520introduces%2520new%2520resistance%2520bounds%2520for%2520intra-cluster%2520edges%252C%2520a%250Arank-%2524%2528n-k%2529%2524%2520effective%2520resistance%2520formulation%252C%2520and%2520a%2520matrix%2520Chernoff%2520bound%250Aadapted%2520to%2520the%2520dominant%2520eigenspace.%2520These%2520tools%2520allow%2520us%2520to%2520bypass%2520importance%250Asampling%2520entirely.%2520Conceptually%252C%2520our%2520result%2520connects%2520recent%2520coreset-based%250Aclustering%2520theory%2520to%2520spectral%2520sparsification%252C%2520showing%2520that%2520under%2520strong%250Aclusterability%252C%2520even%2520uniform%2520sampling%2520is%2520structure-aware.%2520This%2520provides%2520the%250Afirst%2520provable%2520guarantee%2520that%2520uniform%2520edge%2520sampling%2520suffices%2520for%250Astructure-preserving%2520spectral%2520clustering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12669v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-Aware%20Spectral%20Sparsification%20via%20Uniform%20Edge%20Sampling&entry.906535625=Kaiwen%20He%20and%20Petros%20Drineas%20and%20Rajiv%20Khanna&entry.1292438233=%20%20Spectral%20clustering%20is%20a%20fundamental%20method%20for%20graph%20partitioning%2C%20but%20its%0Areliance%20on%20eigenvector%20computation%20limits%20scalability%20to%20massive%20graphs.%0AClassical%20sparsification%20methods%20preserve%20spectral%20properties%20by%20sampling%20edges%0Aproportionally%20to%20their%20effective%20resistances%2C%20but%20require%20expensive%0Apreprocessing%20to%20estimate%20these%20resistances.%20We%20study%20whether%20uniform%20edge%0Asampling-a%20simple%2C%20structure-agnostic%20strategy-can%20suffice%20for%20spectral%0Aclustering.%20Our%20main%20result%20shows%20that%20for%20graphs%20admitting%20a%20well-separated%0A%24k%24-clustering%2C%20characterized%20by%20a%20large%20structure%20ratio%20%24%5CUpsilon%28k%29%20%3D%0A%5Clambda_%7Bk%2B1%7D%20/%20%5Crho_G%28k%29%24%2C%20uniform%20sampling%20preserves%20the%20spectral%20subspace%0Aused%20for%20clustering.%20Specifically%2C%20we%20prove%20that%20uniformly%20sampling%20%24O%28%5Cgamma%5E2%0An%20%5Clog%20n%20/%20%5Cepsilon%5E2%29%24%20edges%2C%20where%20%24%5Cgamma%24%20is%20the%20Laplacian%20condition%0Anumber%2C%20yields%20a%20sparsifier%20whose%20top%20%24%28n-k%29%24-dimensional%20eigenspace%20is%0Aapproximately%20orthogonal%20to%20the%20cluster%20indicators.%20This%20ensures%20that%20the%0Aspectral%20embedding%20remains%20faithful%2C%20and%20clustering%20quality%20is%20preserved.%20Our%0Aanalysis%20introduces%20new%20resistance%20bounds%20for%20intra-cluster%20edges%2C%20a%0Arank-%24%28n-k%29%24%20effective%20resistance%20formulation%2C%20and%20a%20matrix%20Chernoff%20bound%0Aadapted%20to%20the%20dominant%20eigenspace.%20These%20tools%20allow%20us%20to%20bypass%20importance%0Asampling%20entirely.%20Conceptually%2C%20our%20result%20connects%20recent%20coreset-based%0Aclustering%20theory%20to%20spectral%20sparsification%2C%20showing%20that%20under%20strong%0Aclusterability%2C%20even%20uniform%20sampling%20is%20structure-aware.%20This%20provides%20the%0Afirst%20provable%20guarantee%20that%20uniform%20edge%20sampling%20suffices%20for%0Astructure-preserving%20spectral%20clustering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12669v1&entry.124074799=Read"},
{"title": "Research in Collaborative Learning Does Not Serve Cross-Silo Federated\n  Learning in Practice", "author": "Kevin Kuo and Chhavi Yadav and Virginia Smith", "abstract": "  Cross-silo federated learning (FL) is a promising approach to enable\ncross-organization collaboration in machine learning model development without\ndirectly sharing private data. Despite growing organizational interest driven\nby data protection regulations such as GDPR and HIPAA, the adoption of\ncross-silo FL remains limited in practice. In this paper, we conduct an\ninterview study to understand the practical challenges associated with\ncross-silo FL adoption. With interviews spanning a diverse set of stakeholders\nsuch as user organizations, software providers, and academic researchers, we\nuncover various barriers, from concerns about model performance to questions of\nincentives and trust between participating organizations. Our study shows that\ncross-silo FL faces a set of challenges that have yet to be well-captured by\nexisting research in the area and are quite distinct from other forms of\nfederated learning such as cross-device FL. We end with a discussion on future\nresearch directions that can help overcome these challenges.\n", "link": "http://arxiv.org/abs/2510.12595v1", "date": "2025-10-14", "relevancy": 2.2358, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4524}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4524}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Research%20in%20Collaborative%20Learning%20Does%20Not%20Serve%20Cross-Silo%20Federated%0A%20%20Learning%20in%20Practice&body=Title%3A%20Research%20in%20Collaborative%20Learning%20Does%20Not%20Serve%20Cross-Silo%20Federated%0A%20%20Learning%20in%20Practice%0AAuthor%3A%20Kevin%20Kuo%20and%20Chhavi%20Yadav%20and%20Virginia%20Smith%0AAbstract%3A%20%20%20Cross-silo%20federated%20learning%20%28FL%29%20is%20a%20promising%20approach%20to%20enable%0Across-organization%20collaboration%20in%20machine%20learning%20model%20development%20without%0Adirectly%20sharing%20private%20data.%20Despite%20growing%20organizational%20interest%20driven%0Aby%20data%20protection%20regulations%20such%20as%20GDPR%20and%20HIPAA%2C%20the%20adoption%20of%0Across-silo%20FL%20remains%20limited%20in%20practice.%20In%20this%20paper%2C%20we%20conduct%20an%0Ainterview%20study%20to%20understand%20the%20practical%20challenges%20associated%20with%0Across-silo%20FL%20adoption.%20With%20interviews%20spanning%20a%20diverse%20set%20of%20stakeholders%0Asuch%20as%20user%20organizations%2C%20software%20providers%2C%20and%20academic%20researchers%2C%20we%0Auncover%20various%20barriers%2C%20from%20concerns%20about%20model%20performance%20to%20questions%20of%0Aincentives%20and%20trust%20between%20participating%20organizations.%20Our%20study%20shows%20that%0Across-silo%20FL%20faces%20a%20set%20of%20challenges%20that%20have%20yet%20to%20be%20well-captured%20by%0Aexisting%20research%20in%20the%20area%20and%20are%20quite%20distinct%20from%20other%20forms%20of%0Afederated%20learning%20such%20as%20cross-device%20FL.%20We%20end%20with%20a%20discussion%20on%20future%0Aresearch%20directions%20that%20can%20help%20overcome%20these%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResearch%2520in%2520Collaborative%2520Learning%2520Does%2520Not%2520Serve%2520Cross-Silo%2520Federated%250A%2520%2520Learning%2520in%2520Practice%26entry.906535625%3DKevin%2520Kuo%2520and%2520Chhavi%2520Yadav%2520and%2520Virginia%2520Smith%26entry.1292438233%3D%2520%2520Cross-silo%2520federated%2520learning%2520%2528FL%2529%2520is%2520a%2520promising%2520approach%2520to%2520enable%250Across-organization%2520collaboration%2520in%2520machine%2520learning%2520model%2520development%2520without%250Adirectly%2520sharing%2520private%2520data.%2520Despite%2520growing%2520organizational%2520interest%2520driven%250Aby%2520data%2520protection%2520regulations%2520such%2520as%2520GDPR%2520and%2520HIPAA%252C%2520the%2520adoption%2520of%250Across-silo%2520FL%2520remains%2520limited%2520in%2520practice.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520an%250Ainterview%2520study%2520to%2520understand%2520the%2520practical%2520challenges%2520associated%2520with%250Across-silo%2520FL%2520adoption.%2520With%2520interviews%2520spanning%2520a%2520diverse%2520set%2520of%2520stakeholders%250Asuch%2520as%2520user%2520organizations%252C%2520software%2520providers%252C%2520and%2520academic%2520researchers%252C%2520we%250Auncover%2520various%2520barriers%252C%2520from%2520concerns%2520about%2520model%2520performance%2520to%2520questions%2520of%250Aincentives%2520and%2520trust%2520between%2520participating%2520organizations.%2520Our%2520study%2520shows%2520that%250Across-silo%2520FL%2520faces%2520a%2520set%2520of%2520challenges%2520that%2520have%2520yet%2520to%2520be%2520well-captured%2520by%250Aexisting%2520research%2520in%2520the%2520area%2520and%2520are%2520quite%2520distinct%2520from%2520other%2520forms%2520of%250Afederated%2520learning%2520such%2520as%2520cross-device%2520FL.%2520We%2520end%2520with%2520a%2520discussion%2520on%2520future%250Aresearch%2520directions%2520that%2520can%2520help%2520overcome%2520these%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Research%20in%20Collaborative%20Learning%20Does%20Not%20Serve%20Cross-Silo%20Federated%0A%20%20Learning%20in%20Practice&entry.906535625=Kevin%20Kuo%20and%20Chhavi%20Yadav%20and%20Virginia%20Smith&entry.1292438233=%20%20Cross-silo%20federated%20learning%20%28FL%29%20is%20a%20promising%20approach%20to%20enable%0Across-organization%20collaboration%20in%20machine%20learning%20model%20development%20without%0Adirectly%20sharing%20private%20data.%20Despite%20growing%20organizational%20interest%20driven%0Aby%20data%20protection%20regulations%20such%20as%20GDPR%20and%20HIPAA%2C%20the%20adoption%20of%0Across-silo%20FL%20remains%20limited%20in%20practice.%20In%20this%20paper%2C%20we%20conduct%20an%0Ainterview%20study%20to%20understand%20the%20practical%20challenges%20associated%20with%0Across-silo%20FL%20adoption.%20With%20interviews%20spanning%20a%20diverse%20set%20of%20stakeholders%0Asuch%20as%20user%20organizations%2C%20software%20providers%2C%20and%20academic%20researchers%2C%20we%0Auncover%20various%20barriers%2C%20from%20concerns%20about%20model%20performance%20to%20questions%20of%0Aincentives%20and%20trust%20between%20participating%20organizations.%20Our%20study%20shows%20that%0Across-silo%20FL%20faces%20a%20set%20of%20challenges%20that%20have%20yet%20to%20be%20well-captured%20by%0Aexisting%20research%20in%20the%20area%20and%20are%20quite%20distinct%20from%20other%20forms%20of%0Afederated%20learning%20such%20as%20cross-device%20FL.%20We%20end%20with%20a%20discussion%20on%20future%0Aresearch%20directions%20that%20can%20help%20overcome%20these%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12595v1&entry.124074799=Read"},
{"title": "Can ChatGPT support software verification?", "author": "Christian Jan\u00dfen and Cedric Richter and Heike Wehrheim", "abstract": "  Large language models have become increasingly effective in software\nengineering tasks such as code generation, debugging and repair. Language\nmodels like ChatGPT can not only generate code, but also explain its inner\nworkings and in particular its correctness. This raises the question whether we\ncan utilize ChatGPT to support formal software verification.\n  In this paper, we take some first steps towards answering this question. More\nspecifically, we investigate whether ChatGPT can generate loop invariants. Loop\ninvariant generation is a core task in software verification, and the\ngeneration of valid and useful invariants would likely help formal verifiers.\nTo provide some first evidence on this hypothesis, we ask ChatGPT to annotate\n106 C programs with loop invariants. We check validity and usefulness of the\ngenerated invariants by passing them to two verifiers, Frama-C and CPAchecker.\nOur evaluation shows that ChatGPT is able to produce valid and useful\ninvariants allowing Frama-C to verify tasks that it could not solve before.\nBased on our initial insights, we propose ways of combining ChatGPT (or large\nlanguage models in general) and software verifiers, and discuss current\nlimitations and open issues.\n", "link": "http://arxiv.org/abs/2311.02433v2", "date": "2025-10-14", "relevancy": 2.2332, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4719}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4372}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20ChatGPT%20support%20software%20verification%3F&body=Title%3A%20Can%20ChatGPT%20support%20software%20verification%3F%0AAuthor%3A%20Christian%20Jan%C3%9Fen%20and%20Cedric%20Richter%20and%20Heike%20Wehrheim%0AAbstract%3A%20%20%20Large%20language%20models%20have%20become%20increasingly%20effective%20in%20software%0Aengineering%20tasks%20such%20as%20code%20generation%2C%20debugging%20and%20repair.%20Language%0Amodels%20like%20ChatGPT%20can%20not%20only%20generate%20code%2C%20but%20also%20explain%20its%20inner%0Aworkings%20and%20in%20particular%20its%20correctness.%20This%20raises%20the%20question%20whether%20we%0Acan%20utilize%20ChatGPT%20to%20support%20formal%20software%20verification.%0A%20%20In%20this%20paper%2C%20we%20take%20some%20first%20steps%20towards%20answering%20this%20question.%20More%0Aspecifically%2C%20we%20investigate%20whether%20ChatGPT%20can%20generate%20loop%20invariants.%20Loop%0Ainvariant%20generation%20is%20a%20core%20task%20in%20software%20verification%2C%20and%20the%0Ageneration%20of%20valid%20and%20useful%20invariants%20would%20likely%20help%20formal%20verifiers.%0ATo%20provide%20some%20first%20evidence%20on%20this%20hypothesis%2C%20we%20ask%20ChatGPT%20to%20annotate%0A106%20C%20programs%20with%20loop%20invariants.%20We%20check%20validity%20and%20usefulness%20of%20the%0Agenerated%20invariants%20by%20passing%20them%20to%20two%20verifiers%2C%20Frama-C%20and%20CPAchecker.%0AOur%20evaluation%20shows%20that%20ChatGPT%20is%20able%20to%20produce%20valid%20and%20useful%0Ainvariants%20allowing%20Frama-C%20to%20verify%20tasks%20that%20it%20could%20not%20solve%20before.%0ABased%20on%20our%20initial%20insights%2C%20we%20propose%20ways%20of%20combining%20ChatGPT%20%28or%20large%0Alanguage%20models%20in%20general%29%20and%20software%20verifiers%2C%20and%20discuss%20current%0Alimitations%20and%20open%20issues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.02433v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520ChatGPT%2520support%2520software%2520verification%253F%26entry.906535625%3DChristian%2520Jan%25C3%259Fen%2520and%2520Cedric%2520Richter%2520and%2520Heike%2520Wehrheim%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520become%2520increasingly%2520effective%2520in%2520software%250Aengineering%2520tasks%2520such%2520as%2520code%2520generation%252C%2520debugging%2520and%2520repair.%2520Language%250Amodels%2520like%2520ChatGPT%2520can%2520not%2520only%2520generate%2520code%252C%2520but%2520also%2520explain%2520its%2520inner%250Aworkings%2520and%2520in%2520particular%2520its%2520correctness.%2520This%2520raises%2520the%2520question%2520whether%2520we%250Acan%2520utilize%2520ChatGPT%2520to%2520support%2520formal%2520software%2520verification.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520take%2520some%2520first%2520steps%2520towards%2520answering%2520this%2520question.%2520More%250Aspecifically%252C%2520we%2520investigate%2520whether%2520ChatGPT%2520can%2520generate%2520loop%2520invariants.%2520Loop%250Ainvariant%2520generation%2520is%2520a%2520core%2520task%2520in%2520software%2520verification%252C%2520and%2520the%250Ageneration%2520of%2520valid%2520and%2520useful%2520invariants%2520would%2520likely%2520help%2520formal%2520verifiers.%250ATo%2520provide%2520some%2520first%2520evidence%2520on%2520this%2520hypothesis%252C%2520we%2520ask%2520ChatGPT%2520to%2520annotate%250A106%2520C%2520programs%2520with%2520loop%2520invariants.%2520We%2520check%2520validity%2520and%2520usefulness%2520of%2520the%250Agenerated%2520invariants%2520by%2520passing%2520them%2520to%2520two%2520verifiers%252C%2520Frama-C%2520and%2520CPAchecker.%250AOur%2520evaluation%2520shows%2520that%2520ChatGPT%2520is%2520able%2520to%2520produce%2520valid%2520and%2520useful%250Ainvariants%2520allowing%2520Frama-C%2520to%2520verify%2520tasks%2520that%2520it%2520could%2520not%2520solve%2520before.%250ABased%2520on%2520our%2520initial%2520insights%252C%2520we%2520propose%2520ways%2520of%2520combining%2520ChatGPT%2520%2528or%2520large%250Alanguage%2520models%2520in%2520general%2529%2520and%2520software%2520verifiers%252C%2520and%2520discuss%2520current%250Alimitations%2520and%2520open%2520issues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.02433v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20ChatGPT%20support%20software%20verification%3F&entry.906535625=Christian%20Jan%C3%9Fen%20and%20Cedric%20Richter%20and%20Heike%20Wehrheim&entry.1292438233=%20%20Large%20language%20models%20have%20become%20increasingly%20effective%20in%20software%0Aengineering%20tasks%20such%20as%20code%20generation%2C%20debugging%20and%20repair.%20Language%0Amodels%20like%20ChatGPT%20can%20not%20only%20generate%20code%2C%20but%20also%20explain%20its%20inner%0Aworkings%20and%20in%20particular%20its%20correctness.%20This%20raises%20the%20question%20whether%20we%0Acan%20utilize%20ChatGPT%20to%20support%20formal%20software%20verification.%0A%20%20In%20this%20paper%2C%20we%20take%20some%20first%20steps%20towards%20answering%20this%20question.%20More%0Aspecifically%2C%20we%20investigate%20whether%20ChatGPT%20can%20generate%20loop%20invariants.%20Loop%0Ainvariant%20generation%20is%20a%20core%20task%20in%20software%20verification%2C%20and%20the%0Ageneration%20of%20valid%20and%20useful%20invariants%20would%20likely%20help%20formal%20verifiers.%0ATo%20provide%20some%20first%20evidence%20on%20this%20hypothesis%2C%20we%20ask%20ChatGPT%20to%20annotate%0A106%20C%20programs%20with%20loop%20invariants.%20We%20check%20validity%20and%20usefulness%20of%20the%0Agenerated%20invariants%20by%20passing%20them%20to%20two%20verifiers%2C%20Frama-C%20and%20CPAchecker.%0AOur%20evaluation%20shows%20that%20ChatGPT%20is%20able%20to%20produce%20valid%20and%20useful%0Ainvariants%20allowing%20Frama-C%20to%20verify%20tasks%20that%20it%20could%20not%20solve%20before.%0ABased%20on%20our%20initial%20insights%2C%20we%20propose%20ways%20of%20combining%20ChatGPT%20%28or%20large%0Alanguage%20models%20in%20general%29%20and%20software%20verifiers%2C%20and%20discuss%20current%0Alimitations%20and%20open%20issues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.02433v2&entry.124074799=Read"},
{"title": "A Function Centric Perspective On Flat and Sharp Minima", "author": "Israel Mason-Williams and Gabryel Mason-Williams and Helen Yannakoudakis", "abstract": "  Flat minima are widely believed to correlate with improved generalisation in\ndeep neural networks. However, this connection has proven more nuanced in\nrecent studies, with both theoretical counterexamples and empirical exceptions\nemerging in the literature. In this paper, we revisit the role of sharpness in\nmodel performance, proposing that sharpness is better understood as a\nfunction-dependent property rather than a reliable indicator of poor\ngeneralisation. We conduct extensive empirical studies, from single-objective\noptimisation to modern image classification tasks, showing that sharper minima\noften emerge when models are regularised (e.g., via SAM, weight decay, or data\naugmentation), and that these sharp minima can coincide with better\ngeneralisation, calibration, robustness, and functional consistency. Across a\nrange of models and datasets, we find that baselines without regularisation\ntend to converge to flatter minima yet often perform worse across all safety\nmetrics. Our findings demonstrate that function complexity, rather than\nflatness alone, governs the geometry of solutions, and that sharper minima can\nreflect more appropriate inductive biases (especially under regularisation),\ncalling for a function-centric reappraisal of loss landscape geometry.\n", "link": "http://arxiv.org/abs/2510.12451v1", "date": "2025-10-14", "relevancy": 2.2262, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4541}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4469}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Function%20Centric%20Perspective%20On%20Flat%20and%20Sharp%20Minima&body=Title%3A%20A%20Function%20Centric%20Perspective%20On%20Flat%20and%20Sharp%20Minima%0AAuthor%3A%20Israel%20Mason-Williams%20and%20Gabryel%20Mason-Williams%20and%20Helen%20Yannakoudakis%0AAbstract%3A%20%20%20Flat%20minima%20are%20widely%20believed%20to%20correlate%20with%20improved%20generalisation%20in%0Adeep%20neural%20networks.%20However%2C%20this%20connection%20has%20proven%20more%20nuanced%20in%0Arecent%20studies%2C%20with%20both%20theoretical%20counterexamples%20and%20empirical%20exceptions%0Aemerging%20in%20the%20literature.%20In%20this%20paper%2C%20we%20revisit%20the%20role%20of%20sharpness%20in%0Amodel%20performance%2C%20proposing%20that%20sharpness%20is%20better%20understood%20as%20a%0Afunction-dependent%20property%20rather%20than%20a%20reliable%20indicator%20of%20poor%0Ageneralisation.%20We%20conduct%20extensive%20empirical%20studies%2C%20from%20single-objective%0Aoptimisation%20to%20modern%20image%20classification%20tasks%2C%20showing%20that%20sharper%20minima%0Aoften%20emerge%20when%20models%20are%20regularised%20%28e.g.%2C%20via%20SAM%2C%20weight%20decay%2C%20or%20data%0Aaugmentation%29%2C%20and%20that%20these%20sharp%20minima%20can%20coincide%20with%20better%0Ageneralisation%2C%20calibration%2C%20robustness%2C%20and%20functional%20consistency.%20Across%20a%0Arange%20of%20models%20and%20datasets%2C%20we%20find%20that%20baselines%20without%20regularisation%0Atend%20to%20converge%20to%20flatter%20minima%20yet%20often%20perform%20worse%20across%20all%20safety%0Ametrics.%20Our%20findings%20demonstrate%20that%20function%20complexity%2C%20rather%20than%0Aflatness%20alone%2C%20governs%20the%20geometry%20of%20solutions%2C%20and%20that%20sharper%20minima%20can%0Areflect%20more%20appropriate%20inductive%20biases%20%28especially%20under%20regularisation%29%2C%0Acalling%20for%20a%20function-centric%20reappraisal%20of%20loss%20landscape%20geometry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Function%2520Centric%2520Perspective%2520On%2520Flat%2520and%2520Sharp%2520Minima%26entry.906535625%3DIsrael%2520Mason-Williams%2520and%2520Gabryel%2520Mason-Williams%2520and%2520Helen%2520Yannakoudakis%26entry.1292438233%3D%2520%2520Flat%2520minima%2520are%2520widely%2520believed%2520to%2520correlate%2520with%2520improved%2520generalisation%2520in%250Adeep%2520neural%2520networks.%2520However%252C%2520this%2520connection%2520has%2520proven%2520more%2520nuanced%2520in%250Arecent%2520studies%252C%2520with%2520both%2520theoretical%2520counterexamples%2520and%2520empirical%2520exceptions%250Aemerging%2520in%2520the%2520literature.%2520In%2520this%2520paper%252C%2520we%2520revisit%2520the%2520role%2520of%2520sharpness%2520in%250Amodel%2520performance%252C%2520proposing%2520that%2520sharpness%2520is%2520better%2520understood%2520as%2520a%250Afunction-dependent%2520property%2520rather%2520than%2520a%2520reliable%2520indicator%2520of%2520poor%250Ageneralisation.%2520We%2520conduct%2520extensive%2520empirical%2520studies%252C%2520from%2520single-objective%250Aoptimisation%2520to%2520modern%2520image%2520classification%2520tasks%252C%2520showing%2520that%2520sharper%2520minima%250Aoften%2520emerge%2520when%2520models%2520are%2520regularised%2520%2528e.g.%252C%2520via%2520SAM%252C%2520weight%2520decay%252C%2520or%2520data%250Aaugmentation%2529%252C%2520and%2520that%2520these%2520sharp%2520minima%2520can%2520coincide%2520with%2520better%250Ageneralisation%252C%2520calibration%252C%2520robustness%252C%2520and%2520functional%2520consistency.%2520Across%2520a%250Arange%2520of%2520models%2520and%2520datasets%252C%2520we%2520find%2520that%2520baselines%2520without%2520regularisation%250Atend%2520to%2520converge%2520to%2520flatter%2520minima%2520yet%2520often%2520perform%2520worse%2520across%2520all%2520safety%250Ametrics.%2520Our%2520findings%2520demonstrate%2520that%2520function%2520complexity%252C%2520rather%2520than%250Aflatness%2520alone%252C%2520governs%2520the%2520geometry%2520of%2520solutions%252C%2520and%2520that%2520sharper%2520minima%2520can%250Areflect%2520more%2520appropriate%2520inductive%2520biases%2520%2528especially%2520under%2520regularisation%2529%252C%250Acalling%2520for%2520a%2520function-centric%2520reappraisal%2520of%2520loss%2520landscape%2520geometry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Function%20Centric%20Perspective%20On%20Flat%20and%20Sharp%20Minima&entry.906535625=Israel%20Mason-Williams%20and%20Gabryel%20Mason-Williams%20and%20Helen%20Yannakoudakis&entry.1292438233=%20%20Flat%20minima%20are%20widely%20believed%20to%20correlate%20with%20improved%20generalisation%20in%0Adeep%20neural%20networks.%20However%2C%20this%20connection%20has%20proven%20more%20nuanced%20in%0Arecent%20studies%2C%20with%20both%20theoretical%20counterexamples%20and%20empirical%20exceptions%0Aemerging%20in%20the%20literature.%20In%20this%20paper%2C%20we%20revisit%20the%20role%20of%20sharpness%20in%0Amodel%20performance%2C%20proposing%20that%20sharpness%20is%20better%20understood%20as%20a%0Afunction-dependent%20property%20rather%20than%20a%20reliable%20indicator%20of%20poor%0Ageneralisation.%20We%20conduct%20extensive%20empirical%20studies%2C%20from%20single-objective%0Aoptimisation%20to%20modern%20image%20classification%20tasks%2C%20showing%20that%20sharper%20minima%0Aoften%20emerge%20when%20models%20are%20regularised%20%28e.g.%2C%20via%20SAM%2C%20weight%20decay%2C%20or%20data%0Aaugmentation%29%2C%20and%20that%20these%20sharp%20minima%20can%20coincide%20with%20better%0Ageneralisation%2C%20calibration%2C%20robustness%2C%20and%20functional%20consistency.%20Across%20a%0Arange%20of%20models%20and%20datasets%2C%20we%20find%20that%20baselines%20without%20regularisation%0Atend%20to%20converge%20to%20flatter%20minima%20yet%20often%20perform%20worse%20across%20all%20safety%0Ametrics.%20Our%20findings%20demonstrate%20that%20function%20complexity%2C%20rather%20than%0Aflatness%20alone%2C%20governs%20the%20geometry%20of%20solutions%2C%20and%20that%20sharper%20minima%20can%0Areflect%20more%20appropriate%20inductive%20biases%20%28especially%20under%20regularisation%29%2C%0Acalling%20for%20a%20function-centric%20reappraisal%20of%20loss%20landscape%20geometry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12451v1&entry.124074799=Read"},
{"title": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search", "author": "Kartik Narayan and Yang Xu and Tian Cao and Kavya Nerella and Vishal M. Patel and Navid Shiee and Peter Grasch and Chao Jia and Yinfei Yang and Zhe Gan", "abstract": "  Multimodal Large Language Models (MLLMs) in real-world applications require\naccess to external knowledge sources and must remain responsive to the dynamic\nand ever-changing real-world information in order to address\ninformation-seeking and knowledge-intensive user queries. Existing approaches,\nsuch as retrieval augmented generation (RAG) methods, search agents, and search\nequipped MLLMs, often suffer from rigid pipelines, excessive search calls, and\npoorly constructed search queries, which result in inefficiencies and\nsuboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,\nthe first multimodal LLM capable of performing on-demand, multi-turn web\nsearches and dynamically crafting queries for both image and text search tools.\nSpecifically, DeepMMSearch-R1 can initiate web searches based on relevant crops\nof the input image making the image search more effective, and can iteratively\nadapt text search queries based on retrieved information, thereby enabling\nself-reflection and self-correction. Our approach relies on a two-stage\ntraining pipeline: a cold start supervised finetuning phase followed by an\nonline reinforcement learning optimization. For training, we introduce\nDeepMMSearchVQA, a novel multimodal VQA dataset created through an automated\npipeline intermixed with real-world information from web search tools. This\ndataset contains diverse, multi-hop queries that integrate textual and visual\ninformation, teaching the model when to search, what to search for, which\nsearch tool to use and how to reason over the retrieved information. We conduct\nextensive experiments across a range of knowledge-intensive benchmarks to\ndemonstrate the superiority of our approach. Finally, we analyze the results\nand provide insights that are valuable for advancing multimodal web-search.\n", "link": "http://arxiv.org/abs/2510.12801v1", "date": "2025-10-14", "relevancy": 2.2172, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5727}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.558}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepMMSearch-R1%3A%20Empowering%20Multimodal%20LLMs%20in%20Multimodal%20Web%20Search&body=Title%3A%20DeepMMSearch-R1%3A%20Empowering%20Multimodal%20LLMs%20in%20Multimodal%20Web%20Search%0AAuthor%3A%20Kartik%20Narayan%20and%20Yang%20Xu%20and%20Tian%20Cao%20and%20Kavya%20Nerella%20and%20Vishal%20M.%20Patel%20and%20Navid%20Shiee%20and%20Peter%20Grasch%20and%20Chao%20Jia%20and%20Yinfei%20Yang%20and%20Zhe%20Gan%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20in%20real-world%20applications%20require%0Aaccess%20to%20external%20knowledge%20sources%20and%20must%20remain%20responsive%20to%20the%20dynamic%0Aand%20ever-changing%20real-world%20information%20in%20order%20to%20address%0Ainformation-seeking%20and%20knowledge-intensive%20user%20queries.%20Existing%20approaches%2C%0Asuch%20as%20retrieval%20augmented%20generation%20%28RAG%29%20methods%2C%20search%20agents%2C%20and%20search%0Aequipped%20MLLMs%2C%20often%20suffer%20from%20rigid%20pipelines%2C%20excessive%20search%20calls%2C%20and%0Apoorly%20constructed%20search%20queries%2C%20which%20result%20in%20inefficiencies%20and%0Asuboptimal%20outcomes.%20To%20address%20these%20limitations%2C%20we%20present%20DeepMMSearch-R1%2C%0Athe%20first%20multimodal%20LLM%20capable%20of%20performing%20on-demand%2C%20multi-turn%20web%0Asearches%20and%20dynamically%20crafting%20queries%20for%20both%20image%20and%20text%20search%20tools.%0ASpecifically%2C%20DeepMMSearch-R1%20can%20initiate%20web%20searches%20based%20on%20relevant%20crops%0Aof%20the%20input%20image%20making%20the%20image%20search%20more%20effective%2C%20and%20can%20iteratively%0Aadapt%20text%20search%20queries%20based%20on%20retrieved%20information%2C%20thereby%20enabling%0Aself-reflection%20and%20self-correction.%20Our%20approach%20relies%20on%20a%20two-stage%0Atraining%20pipeline%3A%20a%20cold%20start%20supervised%20finetuning%20phase%20followed%20by%20an%0Aonline%20reinforcement%20learning%20optimization.%20For%20training%2C%20we%20introduce%0ADeepMMSearchVQA%2C%20a%20novel%20multimodal%20VQA%20dataset%20created%20through%20an%20automated%0Apipeline%20intermixed%20with%20real-world%20information%20from%20web%20search%20tools.%20This%0Adataset%20contains%20diverse%2C%20multi-hop%20queries%20that%20integrate%20textual%20and%20visual%0Ainformation%2C%20teaching%20the%20model%20when%20to%20search%2C%20what%20to%20search%20for%2C%20which%0Asearch%20tool%20to%20use%20and%20how%20to%20reason%20over%20the%20retrieved%20information.%20We%20conduct%0Aextensive%20experiments%20across%20a%20range%20of%20knowledge-intensive%20benchmarks%20to%0Ademonstrate%20the%20superiority%20of%20our%20approach.%20Finally%2C%20we%20analyze%20the%20results%0Aand%20provide%20insights%20that%20are%20valuable%20for%20advancing%20multimodal%20web-search.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepMMSearch-R1%253A%2520Empowering%2520Multimodal%2520LLMs%2520in%2520Multimodal%2520Web%2520Search%26entry.906535625%3DKartik%2520Narayan%2520and%2520Yang%2520Xu%2520and%2520Tian%2520Cao%2520and%2520Kavya%2520Nerella%2520and%2520Vishal%2520M.%2520Patel%2520and%2520Navid%2520Shiee%2520and%2520Peter%2520Grasch%2520and%2520Chao%2520Jia%2520and%2520Yinfei%2520Yang%2520and%2520Zhe%2520Gan%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520in%2520real-world%2520applications%2520require%250Aaccess%2520to%2520external%2520knowledge%2520sources%2520and%2520must%2520remain%2520responsive%2520to%2520the%2520dynamic%250Aand%2520ever-changing%2520real-world%2520information%2520in%2520order%2520to%2520address%250Ainformation-seeking%2520and%2520knowledge-intensive%2520user%2520queries.%2520Existing%2520approaches%252C%250Asuch%2520as%2520retrieval%2520augmented%2520generation%2520%2528RAG%2529%2520methods%252C%2520search%2520agents%252C%2520and%2520search%250Aequipped%2520MLLMs%252C%2520often%2520suffer%2520from%2520rigid%2520pipelines%252C%2520excessive%2520search%2520calls%252C%2520and%250Apoorly%2520constructed%2520search%2520queries%252C%2520which%2520result%2520in%2520inefficiencies%2520and%250Asuboptimal%2520outcomes.%2520To%2520address%2520these%2520limitations%252C%2520we%2520present%2520DeepMMSearch-R1%252C%250Athe%2520first%2520multimodal%2520LLM%2520capable%2520of%2520performing%2520on-demand%252C%2520multi-turn%2520web%250Asearches%2520and%2520dynamically%2520crafting%2520queries%2520for%2520both%2520image%2520and%2520text%2520search%2520tools.%250ASpecifically%252C%2520DeepMMSearch-R1%2520can%2520initiate%2520web%2520searches%2520based%2520on%2520relevant%2520crops%250Aof%2520the%2520input%2520image%2520making%2520the%2520image%2520search%2520more%2520effective%252C%2520and%2520can%2520iteratively%250Aadapt%2520text%2520search%2520queries%2520based%2520on%2520retrieved%2520information%252C%2520thereby%2520enabling%250Aself-reflection%2520and%2520self-correction.%2520Our%2520approach%2520relies%2520on%2520a%2520two-stage%250Atraining%2520pipeline%253A%2520a%2520cold%2520start%2520supervised%2520finetuning%2520phase%2520followed%2520by%2520an%250Aonline%2520reinforcement%2520learning%2520optimization.%2520For%2520training%252C%2520we%2520introduce%250ADeepMMSearchVQA%252C%2520a%2520novel%2520multimodal%2520VQA%2520dataset%2520created%2520through%2520an%2520automated%250Apipeline%2520intermixed%2520with%2520real-world%2520information%2520from%2520web%2520search%2520tools.%2520This%250Adataset%2520contains%2520diverse%252C%2520multi-hop%2520queries%2520that%2520integrate%2520textual%2520and%2520visual%250Ainformation%252C%2520teaching%2520the%2520model%2520when%2520to%2520search%252C%2520what%2520to%2520search%2520for%252C%2520which%250Asearch%2520tool%2520to%2520use%2520and%2520how%2520to%2520reason%2520over%2520the%2520retrieved%2520information.%2520We%2520conduct%250Aextensive%2520experiments%2520across%2520a%2520range%2520of%2520knowledge-intensive%2520benchmarks%2520to%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520approach.%2520Finally%252C%2520we%2520analyze%2520the%2520results%250Aand%2520provide%2520insights%2520that%2520are%2520valuable%2520for%2520advancing%2520multimodal%2520web-search.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepMMSearch-R1%3A%20Empowering%20Multimodal%20LLMs%20in%20Multimodal%20Web%20Search&entry.906535625=Kartik%20Narayan%20and%20Yang%20Xu%20and%20Tian%20Cao%20and%20Kavya%20Nerella%20and%20Vishal%20M.%20Patel%20and%20Navid%20Shiee%20and%20Peter%20Grasch%20and%20Chao%20Jia%20and%20Yinfei%20Yang%20and%20Zhe%20Gan&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20in%20real-world%20applications%20require%0Aaccess%20to%20external%20knowledge%20sources%20and%20must%20remain%20responsive%20to%20the%20dynamic%0Aand%20ever-changing%20real-world%20information%20in%20order%20to%20address%0Ainformation-seeking%20and%20knowledge-intensive%20user%20queries.%20Existing%20approaches%2C%0Asuch%20as%20retrieval%20augmented%20generation%20%28RAG%29%20methods%2C%20search%20agents%2C%20and%20search%0Aequipped%20MLLMs%2C%20often%20suffer%20from%20rigid%20pipelines%2C%20excessive%20search%20calls%2C%20and%0Apoorly%20constructed%20search%20queries%2C%20which%20result%20in%20inefficiencies%20and%0Asuboptimal%20outcomes.%20To%20address%20these%20limitations%2C%20we%20present%20DeepMMSearch-R1%2C%0Athe%20first%20multimodal%20LLM%20capable%20of%20performing%20on-demand%2C%20multi-turn%20web%0Asearches%20and%20dynamically%20crafting%20queries%20for%20both%20image%20and%20text%20search%20tools.%0ASpecifically%2C%20DeepMMSearch-R1%20can%20initiate%20web%20searches%20based%20on%20relevant%20crops%0Aof%20the%20input%20image%20making%20the%20image%20search%20more%20effective%2C%20and%20can%20iteratively%0Aadapt%20text%20search%20queries%20based%20on%20retrieved%20information%2C%20thereby%20enabling%0Aself-reflection%20and%20self-correction.%20Our%20approach%20relies%20on%20a%20two-stage%0Atraining%20pipeline%3A%20a%20cold%20start%20supervised%20finetuning%20phase%20followed%20by%20an%0Aonline%20reinforcement%20learning%20optimization.%20For%20training%2C%20we%20introduce%0ADeepMMSearchVQA%2C%20a%20novel%20multimodal%20VQA%20dataset%20created%20through%20an%20automated%0Apipeline%20intermixed%20with%20real-world%20information%20from%20web%20search%20tools.%20This%0Adataset%20contains%20diverse%2C%20multi-hop%20queries%20that%20integrate%20textual%20and%20visual%0Ainformation%2C%20teaching%20the%20model%20when%20to%20search%2C%20what%20to%20search%20for%2C%20which%0Asearch%20tool%20to%20use%20and%20how%20to%20reason%20over%20the%20retrieved%20information.%20We%20conduct%0Aextensive%20experiments%20across%20a%20range%20of%20knowledge-intensive%20benchmarks%20to%0Ademonstrate%20the%20superiority%20of%20our%20approach.%20Finally%2C%20we%20analyze%20the%20results%0Aand%20provide%20insights%20that%20are%20valuable%20for%20advancing%20multimodal%20web-search.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12801v1&entry.124074799=Read"},
{"title": "Variational Rank Reduction Autoencoders", "author": "Jad Mounayer and Alicia Tierz and Jerome Tomezyk and Chady Ghnatios and Francisco Chinesta", "abstract": "  Deterministic Rank Reduction Autoencoders (RRAEs) enforce by construction a\nregularization on the latent space by applying a truncated SVD. While this\nregularization makes Autoencoders more powerful, using them for generative\npurposes is counter-intuitive due to their deterministic nature. On the other\nhand, Variational Autoencoders (VAEs) are well known for their generative\nabilities by learning a probabilistic latent space. In this paper, we present\nVariational Rank Reduction Autoencoders (VRRAEs), a model that leverages the\nadvantages of both RRAEs and VAEs. Our claims and results show that when\ncarefully sampling the latent space of RRAEs and further regularizing with the\nKullback-Leibler (KL) divergence (similarly to VAEs), VRRAEs outperform RRAEs\nand VAEs. Additionally, we show that the regularization induced by the SVD not\nonly makes VRRAEs better generators than VAEs, but also reduces the possibility\nof posterior collapse. Our results include a synthetic dataset of a small size\nthat showcases the robustness of VRRAEs against collapse, and three real-world\ndatasets; the MNIST, CelebA, and CIFAR-10, over which VRRAEs are shown to\noutperform both VAEs and RRAEs on many random generation and interpolation\ntasks based on the FID score. We developed an open-source implementation of\nVRRAEs in JAX (Equinox), available at https://github.com/JadM133/RRAEs.git.\n", "link": "http://arxiv.org/abs/2505.09458v2", "date": "2025-10-14", "relevancy": 2.2162, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5788}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5462}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Rank%20Reduction%20Autoencoders&body=Title%3A%20Variational%20Rank%20Reduction%20Autoencoders%0AAuthor%3A%20Jad%20Mounayer%20and%20Alicia%20Tierz%20and%20Jerome%20Tomezyk%20and%20Chady%20Ghnatios%20and%20Francisco%20Chinesta%0AAbstract%3A%20%20%20Deterministic%20Rank%20Reduction%20Autoencoders%20%28RRAEs%29%20enforce%20by%20construction%20a%0Aregularization%20on%20the%20latent%20space%20by%20applying%20a%20truncated%20SVD.%20While%20this%0Aregularization%20makes%20Autoencoders%20more%20powerful%2C%20using%20them%20for%20generative%0Apurposes%20is%20counter-intuitive%20due%20to%20their%20deterministic%20nature.%20On%20the%20other%0Ahand%2C%20Variational%20Autoencoders%20%28VAEs%29%20are%20well%20known%20for%20their%20generative%0Aabilities%20by%20learning%20a%20probabilistic%20latent%20space.%20In%20this%20paper%2C%20we%20present%0AVariational%20Rank%20Reduction%20Autoencoders%20%28VRRAEs%29%2C%20a%20model%20that%20leverages%20the%0Aadvantages%20of%20both%20RRAEs%20and%20VAEs.%20Our%20claims%20and%20results%20show%20that%20when%0Acarefully%20sampling%20the%20latent%20space%20of%20RRAEs%20and%20further%20regularizing%20with%20the%0AKullback-Leibler%20%28KL%29%20divergence%20%28similarly%20to%20VAEs%29%2C%20VRRAEs%20outperform%20RRAEs%0Aand%20VAEs.%20Additionally%2C%20we%20show%20that%20the%20regularization%20induced%20by%20the%20SVD%20not%0Aonly%20makes%20VRRAEs%20better%20generators%20than%20VAEs%2C%20but%20also%20reduces%20the%20possibility%0Aof%20posterior%20collapse.%20Our%20results%20include%20a%20synthetic%20dataset%20of%20a%20small%20size%0Athat%20showcases%20the%20robustness%20of%20VRRAEs%20against%20collapse%2C%20and%20three%20real-world%0Adatasets%3B%20the%20MNIST%2C%20CelebA%2C%20and%20CIFAR-10%2C%20over%20which%20VRRAEs%20are%20shown%20to%0Aoutperform%20both%20VAEs%20and%20RRAEs%20on%20many%20random%20generation%20and%20interpolation%0Atasks%20based%20on%20the%20FID%20score.%20We%20developed%20an%20open-source%20implementation%20of%0AVRRAEs%20in%20JAX%20%28Equinox%29%2C%20available%20at%20https%3A//github.com/JadM133/RRAEs.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09458v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Rank%2520Reduction%2520Autoencoders%26entry.906535625%3DJad%2520Mounayer%2520and%2520Alicia%2520Tierz%2520and%2520Jerome%2520Tomezyk%2520and%2520Chady%2520Ghnatios%2520and%2520Francisco%2520Chinesta%26entry.1292438233%3D%2520%2520Deterministic%2520Rank%2520Reduction%2520Autoencoders%2520%2528RRAEs%2529%2520enforce%2520by%2520construction%2520a%250Aregularization%2520on%2520the%2520latent%2520space%2520by%2520applying%2520a%2520truncated%2520SVD.%2520While%2520this%250Aregularization%2520makes%2520Autoencoders%2520more%2520powerful%252C%2520using%2520them%2520for%2520generative%250Apurposes%2520is%2520counter-intuitive%2520due%2520to%2520their%2520deterministic%2520nature.%2520On%2520the%2520other%250Ahand%252C%2520Variational%2520Autoencoders%2520%2528VAEs%2529%2520are%2520well%2520known%2520for%2520their%2520generative%250Aabilities%2520by%2520learning%2520a%2520probabilistic%2520latent%2520space.%2520In%2520this%2520paper%252C%2520we%2520present%250AVariational%2520Rank%2520Reduction%2520Autoencoders%2520%2528VRRAEs%2529%252C%2520a%2520model%2520that%2520leverages%2520the%250Aadvantages%2520of%2520both%2520RRAEs%2520and%2520VAEs.%2520Our%2520claims%2520and%2520results%2520show%2520that%2520when%250Acarefully%2520sampling%2520the%2520latent%2520space%2520of%2520RRAEs%2520and%2520further%2520regularizing%2520with%2520the%250AKullback-Leibler%2520%2528KL%2529%2520divergence%2520%2528similarly%2520to%2520VAEs%2529%252C%2520VRRAEs%2520outperform%2520RRAEs%250Aand%2520VAEs.%2520Additionally%252C%2520we%2520show%2520that%2520the%2520regularization%2520induced%2520by%2520the%2520SVD%2520not%250Aonly%2520makes%2520VRRAEs%2520better%2520generators%2520than%2520VAEs%252C%2520but%2520also%2520reduces%2520the%2520possibility%250Aof%2520posterior%2520collapse.%2520Our%2520results%2520include%2520a%2520synthetic%2520dataset%2520of%2520a%2520small%2520size%250Athat%2520showcases%2520the%2520robustness%2520of%2520VRRAEs%2520against%2520collapse%252C%2520and%2520three%2520real-world%250Adatasets%253B%2520the%2520MNIST%252C%2520CelebA%252C%2520and%2520CIFAR-10%252C%2520over%2520which%2520VRRAEs%2520are%2520shown%2520to%250Aoutperform%2520both%2520VAEs%2520and%2520RRAEs%2520on%2520many%2520random%2520generation%2520and%2520interpolation%250Atasks%2520based%2520on%2520the%2520FID%2520score.%2520We%2520developed%2520an%2520open-source%2520implementation%2520of%250AVRRAEs%2520in%2520JAX%2520%2528Equinox%2529%252C%2520available%2520at%2520https%253A//github.com/JadM133/RRAEs.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09458v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Rank%20Reduction%20Autoencoders&entry.906535625=Jad%20Mounayer%20and%20Alicia%20Tierz%20and%20Jerome%20Tomezyk%20and%20Chady%20Ghnatios%20and%20Francisco%20Chinesta&entry.1292438233=%20%20Deterministic%20Rank%20Reduction%20Autoencoders%20%28RRAEs%29%20enforce%20by%20construction%20a%0Aregularization%20on%20the%20latent%20space%20by%20applying%20a%20truncated%20SVD.%20While%20this%0Aregularization%20makes%20Autoencoders%20more%20powerful%2C%20using%20them%20for%20generative%0Apurposes%20is%20counter-intuitive%20due%20to%20their%20deterministic%20nature.%20On%20the%20other%0Ahand%2C%20Variational%20Autoencoders%20%28VAEs%29%20are%20well%20known%20for%20their%20generative%0Aabilities%20by%20learning%20a%20probabilistic%20latent%20space.%20In%20this%20paper%2C%20we%20present%0AVariational%20Rank%20Reduction%20Autoencoders%20%28VRRAEs%29%2C%20a%20model%20that%20leverages%20the%0Aadvantages%20of%20both%20RRAEs%20and%20VAEs.%20Our%20claims%20and%20results%20show%20that%20when%0Acarefully%20sampling%20the%20latent%20space%20of%20RRAEs%20and%20further%20regularizing%20with%20the%0AKullback-Leibler%20%28KL%29%20divergence%20%28similarly%20to%20VAEs%29%2C%20VRRAEs%20outperform%20RRAEs%0Aand%20VAEs.%20Additionally%2C%20we%20show%20that%20the%20regularization%20induced%20by%20the%20SVD%20not%0Aonly%20makes%20VRRAEs%20better%20generators%20than%20VAEs%2C%20but%20also%20reduces%20the%20possibility%0Aof%20posterior%20collapse.%20Our%20results%20include%20a%20synthetic%20dataset%20of%20a%20small%20size%0Athat%20showcases%20the%20robustness%20of%20VRRAEs%20against%20collapse%2C%20and%20three%20real-world%0Adatasets%3B%20the%20MNIST%2C%20CelebA%2C%20and%20CIFAR-10%2C%20over%20which%20VRRAEs%20are%20shown%20to%0Aoutperform%20both%20VAEs%20and%20RRAEs%20on%20many%20random%20generation%20and%20interpolation%0Atasks%20based%20on%20the%20FID%20score.%20We%20developed%20an%20open-source%20implementation%20of%0AVRRAEs%20in%20JAX%20%28Equinox%29%2C%20available%20at%20https%3A//github.com/JadM133/RRAEs.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09458v2&entry.124074799=Read"},
{"title": "Generation Space Size: Understanding and Calibrating Open-Endedness of\n  LLM Generations", "author": "Sunny Yu and Ahmad Jabbar and Robert Hawkins and Dan Jurafsky and Myra Cheng", "abstract": "  Different open-ended generation tasks require different degrees of output\ndiversity. However, current LLMs are often miscalibrated. They collapse to\noverly homogeneous outputs for creative tasks and hallucinate diverse but\nincorrect responses for factual tasks. We argue that these two failure modes\nare unified by, and can both be addressed by, the notion of effective\ngeneration space size (GSS) -- the set of semantically distinct outputs a model\nconsiders for a prompt. We present GSSBench, a task suite of prompt pairs with\nground-truth GSS relationships to assess different metrics and understand where\nmodels diverge from desired behavior. We find that hallucination detection\nmetrics, particularly EigenScore, consistently outperform standard diversity\nand uncertainty quantification metrics, while using only model internals,\nproviding interpretable insights into a model's internal task representations.\nWe demonstrate three applications of GSS: (1) detecting prompt ambiguity and\npredicting clarification questions for better grounding, (2) interpreting\noverthinking and underthinking in reasoning models, and (3) steering models to\nexpand their generation space to yield high-quality and diverse outputs.\n", "link": "http://arxiv.org/abs/2510.12699v1", "date": "2025-10-14", "relevancy": 2.2017, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5783}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5555}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generation%20Space%20Size%3A%20Understanding%20and%20Calibrating%20Open-Endedness%20of%0A%20%20LLM%20Generations&body=Title%3A%20Generation%20Space%20Size%3A%20Understanding%20and%20Calibrating%20Open-Endedness%20of%0A%20%20LLM%20Generations%0AAuthor%3A%20Sunny%20Yu%20and%20Ahmad%20Jabbar%20and%20Robert%20Hawkins%20and%20Dan%20Jurafsky%20and%20Myra%20Cheng%0AAbstract%3A%20%20%20Different%20open-ended%20generation%20tasks%20require%20different%20degrees%20of%20output%0Adiversity.%20However%2C%20current%20LLMs%20are%20often%20miscalibrated.%20They%20collapse%20to%0Aoverly%20homogeneous%20outputs%20for%20creative%20tasks%20and%20hallucinate%20diverse%20but%0Aincorrect%20responses%20for%20factual%20tasks.%20We%20argue%20that%20these%20two%20failure%20modes%0Aare%20unified%20by%2C%20and%20can%20both%20be%20addressed%20by%2C%20the%20notion%20of%20effective%0Ageneration%20space%20size%20%28GSS%29%20--%20the%20set%20of%20semantically%20distinct%20outputs%20a%20model%0Aconsiders%20for%20a%20prompt.%20We%20present%20GSSBench%2C%20a%20task%20suite%20of%20prompt%20pairs%20with%0Aground-truth%20GSS%20relationships%20to%20assess%20different%20metrics%20and%20understand%20where%0Amodels%20diverge%20from%20desired%20behavior.%20We%20find%20that%20hallucination%20detection%0Ametrics%2C%20particularly%20EigenScore%2C%20consistently%20outperform%20standard%20diversity%0Aand%20uncertainty%20quantification%20metrics%2C%20while%20using%20only%20model%20internals%2C%0Aproviding%20interpretable%20insights%20into%20a%20model%27s%20internal%20task%20representations.%0AWe%20demonstrate%20three%20applications%20of%20GSS%3A%20%281%29%20detecting%20prompt%20ambiguity%20and%0Apredicting%20clarification%20questions%20for%20better%20grounding%2C%20%282%29%20interpreting%0Aoverthinking%20and%20underthinking%20in%20reasoning%20models%2C%20and%20%283%29%20steering%20models%20to%0Aexpand%20their%20generation%20space%20to%20yield%20high-quality%20and%20diverse%20outputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneration%2520Space%2520Size%253A%2520Understanding%2520and%2520Calibrating%2520Open-Endedness%2520of%250A%2520%2520LLM%2520Generations%26entry.906535625%3DSunny%2520Yu%2520and%2520Ahmad%2520Jabbar%2520and%2520Robert%2520Hawkins%2520and%2520Dan%2520Jurafsky%2520and%2520Myra%2520Cheng%26entry.1292438233%3D%2520%2520Different%2520open-ended%2520generation%2520tasks%2520require%2520different%2520degrees%2520of%2520output%250Adiversity.%2520However%252C%2520current%2520LLMs%2520are%2520often%2520miscalibrated.%2520They%2520collapse%2520to%250Aoverly%2520homogeneous%2520outputs%2520for%2520creative%2520tasks%2520and%2520hallucinate%2520diverse%2520but%250Aincorrect%2520responses%2520for%2520factual%2520tasks.%2520We%2520argue%2520that%2520these%2520two%2520failure%2520modes%250Aare%2520unified%2520by%252C%2520and%2520can%2520both%2520be%2520addressed%2520by%252C%2520the%2520notion%2520of%2520effective%250Ageneration%2520space%2520size%2520%2528GSS%2529%2520--%2520the%2520set%2520of%2520semantically%2520distinct%2520outputs%2520a%2520model%250Aconsiders%2520for%2520a%2520prompt.%2520We%2520present%2520GSSBench%252C%2520a%2520task%2520suite%2520of%2520prompt%2520pairs%2520with%250Aground-truth%2520GSS%2520relationships%2520to%2520assess%2520different%2520metrics%2520and%2520understand%2520where%250Amodels%2520diverge%2520from%2520desired%2520behavior.%2520We%2520find%2520that%2520hallucination%2520detection%250Ametrics%252C%2520particularly%2520EigenScore%252C%2520consistently%2520outperform%2520standard%2520diversity%250Aand%2520uncertainty%2520quantification%2520metrics%252C%2520while%2520using%2520only%2520model%2520internals%252C%250Aproviding%2520interpretable%2520insights%2520into%2520a%2520model%2527s%2520internal%2520task%2520representations.%250AWe%2520demonstrate%2520three%2520applications%2520of%2520GSS%253A%2520%25281%2529%2520detecting%2520prompt%2520ambiguity%2520and%250Apredicting%2520clarification%2520questions%2520for%2520better%2520grounding%252C%2520%25282%2529%2520interpreting%250Aoverthinking%2520and%2520underthinking%2520in%2520reasoning%2520models%252C%2520and%2520%25283%2529%2520steering%2520models%2520to%250Aexpand%2520their%2520generation%2520space%2520to%2520yield%2520high-quality%2520and%2520diverse%2520outputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generation%20Space%20Size%3A%20Understanding%20and%20Calibrating%20Open-Endedness%20of%0A%20%20LLM%20Generations&entry.906535625=Sunny%20Yu%20and%20Ahmad%20Jabbar%20and%20Robert%20Hawkins%20and%20Dan%20Jurafsky%20and%20Myra%20Cheng&entry.1292438233=%20%20Different%20open-ended%20generation%20tasks%20require%20different%20degrees%20of%20output%0Adiversity.%20However%2C%20current%20LLMs%20are%20often%20miscalibrated.%20They%20collapse%20to%0Aoverly%20homogeneous%20outputs%20for%20creative%20tasks%20and%20hallucinate%20diverse%20but%0Aincorrect%20responses%20for%20factual%20tasks.%20We%20argue%20that%20these%20two%20failure%20modes%0Aare%20unified%20by%2C%20and%20can%20both%20be%20addressed%20by%2C%20the%20notion%20of%20effective%0Ageneration%20space%20size%20%28GSS%29%20--%20the%20set%20of%20semantically%20distinct%20outputs%20a%20model%0Aconsiders%20for%20a%20prompt.%20We%20present%20GSSBench%2C%20a%20task%20suite%20of%20prompt%20pairs%20with%0Aground-truth%20GSS%20relationships%20to%20assess%20different%20metrics%20and%20understand%20where%0Amodels%20diverge%20from%20desired%20behavior.%20We%20find%20that%20hallucination%20detection%0Ametrics%2C%20particularly%20EigenScore%2C%20consistently%20outperform%20standard%20diversity%0Aand%20uncertainty%20quantification%20metrics%2C%20while%20using%20only%20model%20internals%2C%0Aproviding%20interpretable%20insights%20into%20a%20model%27s%20internal%20task%20representations.%0AWe%20demonstrate%20three%20applications%20of%20GSS%3A%20%281%29%20detecting%20prompt%20ambiguity%20and%0Apredicting%20clarification%20questions%20for%20better%20grounding%2C%20%282%29%20interpreting%0Aoverthinking%20and%20underthinking%20in%20reasoning%20models%2C%20and%20%283%29%20steering%20models%20to%0Aexpand%20their%20generation%20space%20to%20yield%20high-quality%20and%20diverse%20outputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12699v1&entry.124074799=Read"},
{"title": "Reflection-Based Task Adaptation for Self-Improving VLA", "author": "Baicheng Li and Dong Wu and Zike Yan and Xinchen Liu and Zecui Zeng and Lusong Li and Hongbin Zha", "abstract": "  Pre-trained Vision-Language-Action (VLA) models represent a major leap\ntowards general-purpose robots, yet efficiently adapting them to novel,\nspecific tasks in-situ remains a significant hurdle. While reinforcement\nlearning (RL) is a promising avenue for such adaptation, the process often\nsuffers from low efficiency, hindering rapid task mastery. We introduce\nReflective Self-Adaptation, a framework for rapid, autonomous task adaptation\nwithout human intervention. Our framework establishes a self-improving loop\nwhere the agent learns from its own experience to enhance both strategy and\nexecution.\n  The core of our framework is a dual-pathway architecture that addresses the\nfull adaptation lifecycle. First, a Failure-Driven Reflective RL pathway\nenables rapid learning by using the VLM's causal reasoning to automatically\nsynthesize a targeted, dense reward function from failure analysis. This\nprovides a focused learning signal that significantly accelerates policy\nexploration. However, optimizing such proxy rewards introduces a potential risk\nof \"reward hacking,\" where the agent masters the reward function but fails the\nactual task. To counteract this, our second pathway, Success-Driven\nQuality-Guided SFT, grounds the policy in holistic success. It identifies and\nselectively imitates high-quality successful trajectories, ensuring the agent\nremains aligned with the ultimate task goal. This pathway is strengthened by a\nconditional curriculum mechanism to aid initial exploration.\n  We conduct experiments in challenging manipulation tasks. The results\ndemonstrate that our framework achieves faster convergence and higher final\nsuccess rates compared to representative baselines. Our work presents a robust\nsolution for creating self-improving agents that can efficiently and reliably\nadapt to new environments.\n", "link": "http://arxiv.org/abs/2510.12710v1", "date": "2025-10-14", "relevancy": 2.1733, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5522}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5417}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reflection-Based%20Task%20Adaptation%20for%20Self-Improving%20VLA&body=Title%3A%20Reflection-Based%20Task%20Adaptation%20for%20Self-Improving%20VLA%0AAuthor%3A%20Baicheng%20Li%20and%20Dong%20Wu%20and%20Zike%20Yan%20and%20Xinchen%20Liu%20and%20Zecui%20Zeng%20and%20Lusong%20Li%20and%20Hongbin%20Zha%0AAbstract%3A%20%20%20Pre-trained%20Vision-Language-Action%20%28VLA%29%20models%20represent%20a%20major%20leap%0Atowards%20general-purpose%20robots%2C%20yet%20efficiently%20adapting%20them%20to%20novel%2C%0Aspecific%20tasks%20in-situ%20remains%20a%20significant%20hurdle.%20While%20reinforcement%0Alearning%20%28RL%29%20is%20a%20promising%20avenue%20for%20such%20adaptation%2C%20the%20process%20often%0Asuffers%20from%20low%20efficiency%2C%20hindering%20rapid%20task%20mastery.%20We%20introduce%0AReflective%20Self-Adaptation%2C%20a%20framework%20for%20rapid%2C%20autonomous%20task%20adaptation%0Awithout%20human%20intervention.%20Our%20framework%20establishes%20a%20self-improving%20loop%0Awhere%20the%20agent%20learns%20from%20its%20own%20experience%20to%20enhance%20both%20strategy%20and%0Aexecution.%0A%20%20The%20core%20of%20our%20framework%20is%20a%20dual-pathway%20architecture%20that%20addresses%20the%0Afull%20adaptation%20lifecycle.%20First%2C%20a%20Failure-Driven%20Reflective%20RL%20pathway%0Aenables%20rapid%20learning%20by%20using%20the%20VLM%27s%20causal%20reasoning%20to%20automatically%0Asynthesize%20a%20targeted%2C%20dense%20reward%20function%20from%20failure%20analysis.%20This%0Aprovides%20a%20focused%20learning%20signal%20that%20significantly%20accelerates%20policy%0Aexploration.%20However%2C%20optimizing%20such%20proxy%20rewards%20introduces%20a%20potential%20risk%0Aof%20%22reward%20hacking%2C%22%20where%20the%20agent%20masters%20the%20reward%20function%20but%20fails%20the%0Aactual%20task.%20To%20counteract%20this%2C%20our%20second%20pathway%2C%20Success-Driven%0AQuality-Guided%20SFT%2C%20grounds%20the%20policy%20in%20holistic%20success.%20It%20identifies%20and%0Aselectively%20imitates%20high-quality%20successful%20trajectories%2C%20ensuring%20the%20agent%0Aremains%20aligned%20with%20the%20ultimate%20task%20goal.%20This%20pathway%20is%20strengthened%20by%20a%0Aconditional%20curriculum%20mechanism%20to%20aid%20initial%20exploration.%0A%20%20We%20conduct%20experiments%20in%20challenging%20manipulation%20tasks.%20The%20results%0Ademonstrate%20that%20our%20framework%20achieves%20faster%20convergence%20and%20higher%20final%0Asuccess%20rates%20compared%20to%20representative%20baselines.%20Our%20work%20presents%20a%20robust%0Asolution%20for%20creating%20self-improving%20agents%20that%20can%20efficiently%20and%20reliably%0Aadapt%20to%20new%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReflection-Based%2520Task%2520Adaptation%2520for%2520Self-Improving%2520VLA%26entry.906535625%3DBaicheng%2520Li%2520and%2520Dong%2520Wu%2520and%2520Zike%2520Yan%2520and%2520Xinchen%2520Liu%2520and%2520Zecui%2520Zeng%2520and%2520Lusong%2520Li%2520and%2520Hongbin%2520Zha%26entry.1292438233%3D%2520%2520Pre-trained%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520represent%2520a%2520major%2520leap%250Atowards%2520general-purpose%2520robots%252C%2520yet%2520efficiently%2520adapting%2520them%2520to%2520novel%252C%250Aspecific%2520tasks%2520in-situ%2520remains%2520a%2520significant%2520hurdle.%2520While%2520reinforcement%250Alearning%2520%2528RL%2529%2520is%2520a%2520promising%2520avenue%2520for%2520such%2520adaptation%252C%2520the%2520process%2520often%250Asuffers%2520from%2520low%2520efficiency%252C%2520hindering%2520rapid%2520task%2520mastery.%2520We%2520introduce%250AReflective%2520Self-Adaptation%252C%2520a%2520framework%2520for%2520rapid%252C%2520autonomous%2520task%2520adaptation%250Awithout%2520human%2520intervention.%2520Our%2520framework%2520establishes%2520a%2520self-improving%2520loop%250Awhere%2520the%2520agent%2520learns%2520from%2520its%2520own%2520experience%2520to%2520enhance%2520both%2520strategy%2520and%250Aexecution.%250A%2520%2520The%2520core%2520of%2520our%2520framework%2520is%2520a%2520dual-pathway%2520architecture%2520that%2520addresses%2520the%250Afull%2520adaptation%2520lifecycle.%2520First%252C%2520a%2520Failure-Driven%2520Reflective%2520RL%2520pathway%250Aenables%2520rapid%2520learning%2520by%2520using%2520the%2520VLM%2527s%2520causal%2520reasoning%2520to%2520automatically%250Asynthesize%2520a%2520targeted%252C%2520dense%2520reward%2520function%2520from%2520failure%2520analysis.%2520This%250Aprovides%2520a%2520focused%2520learning%2520signal%2520that%2520significantly%2520accelerates%2520policy%250Aexploration.%2520However%252C%2520optimizing%2520such%2520proxy%2520rewards%2520introduces%2520a%2520potential%2520risk%250Aof%2520%2522reward%2520hacking%252C%2522%2520where%2520the%2520agent%2520masters%2520the%2520reward%2520function%2520but%2520fails%2520the%250Aactual%2520task.%2520To%2520counteract%2520this%252C%2520our%2520second%2520pathway%252C%2520Success-Driven%250AQuality-Guided%2520SFT%252C%2520grounds%2520the%2520policy%2520in%2520holistic%2520success.%2520It%2520identifies%2520and%250Aselectively%2520imitates%2520high-quality%2520successful%2520trajectories%252C%2520ensuring%2520the%2520agent%250Aremains%2520aligned%2520with%2520the%2520ultimate%2520task%2520goal.%2520This%2520pathway%2520is%2520strengthened%2520by%2520a%250Aconditional%2520curriculum%2520mechanism%2520to%2520aid%2520initial%2520exploration.%250A%2520%2520We%2520conduct%2520experiments%2520in%2520challenging%2520manipulation%2520tasks.%2520The%2520results%250Ademonstrate%2520that%2520our%2520framework%2520achieves%2520faster%2520convergence%2520and%2520higher%2520final%250Asuccess%2520rates%2520compared%2520to%2520representative%2520baselines.%2520Our%2520work%2520presents%2520a%2520robust%250Asolution%2520for%2520creating%2520self-improving%2520agents%2520that%2520can%2520efficiently%2520and%2520reliably%250Aadapt%2520to%2520new%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reflection-Based%20Task%20Adaptation%20for%20Self-Improving%20VLA&entry.906535625=Baicheng%20Li%20and%20Dong%20Wu%20and%20Zike%20Yan%20and%20Xinchen%20Liu%20and%20Zecui%20Zeng%20and%20Lusong%20Li%20and%20Hongbin%20Zha&entry.1292438233=%20%20Pre-trained%20Vision-Language-Action%20%28VLA%29%20models%20represent%20a%20major%20leap%0Atowards%20general-purpose%20robots%2C%20yet%20efficiently%20adapting%20them%20to%20novel%2C%0Aspecific%20tasks%20in-situ%20remains%20a%20significant%20hurdle.%20While%20reinforcement%0Alearning%20%28RL%29%20is%20a%20promising%20avenue%20for%20such%20adaptation%2C%20the%20process%20often%0Asuffers%20from%20low%20efficiency%2C%20hindering%20rapid%20task%20mastery.%20We%20introduce%0AReflective%20Self-Adaptation%2C%20a%20framework%20for%20rapid%2C%20autonomous%20task%20adaptation%0Awithout%20human%20intervention.%20Our%20framework%20establishes%20a%20self-improving%20loop%0Awhere%20the%20agent%20learns%20from%20its%20own%20experience%20to%20enhance%20both%20strategy%20and%0Aexecution.%0A%20%20The%20core%20of%20our%20framework%20is%20a%20dual-pathway%20architecture%20that%20addresses%20the%0Afull%20adaptation%20lifecycle.%20First%2C%20a%20Failure-Driven%20Reflective%20RL%20pathway%0Aenables%20rapid%20learning%20by%20using%20the%20VLM%27s%20causal%20reasoning%20to%20automatically%0Asynthesize%20a%20targeted%2C%20dense%20reward%20function%20from%20failure%20analysis.%20This%0Aprovides%20a%20focused%20learning%20signal%20that%20significantly%20accelerates%20policy%0Aexploration.%20However%2C%20optimizing%20such%20proxy%20rewards%20introduces%20a%20potential%20risk%0Aof%20%22reward%20hacking%2C%22%20where%20the%20agent%20masters%20the%20reward%20function%20but%20fails%20the%0Aactual%20task.%20To%20counteract%20this%2C%20our%20second%20pathway%2C%20Success-Driven%0AQuality-Guided%20SFT%2C%20grounds%20the%20policy%20in%20holistic%20success.%20It%20identifies%20and%0Aselectively%20imitates%20high-quality%20successful%20trajectories%2C%20ensuring%20the%20agent%0Aremains%20aligned%20with%20the%20ultimate%20task%20goal.%20This%20pathway%20is%20strengthened%20by%20a%0Aconditional%20curriculum%20mechanism%20to%20aid%20initial%20exploration.%0A%20%20We%20conduct%20experiments%20in%20challenging%20manipulation%20tasks.%20The%20results%0Ademonstrate%20that%20our%20framework%20achieves%20faster%20convergence%20and%20higher%20final%0Asuccess%20rates%20compared%20to%20representative%20baselines.%20Our%20work%20presents%20a%20robust%0Asolution%20for%20creating%20self-improving%20agents%20that%20can%20efficiently%20and%20reliably%0Aadapt%20to%20new%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12710v1&entry.124074799=Read"},
{"title": "Query Brand Entity Linking in E-Commerce Search", "author": "Dong Liu and Sreyashi Nag", "abstract": "  In this work, we address the brand entity linking problem for e-commerce\nsearch queries. The entity linking task is done by either i)a two-stage process\nconsisting of entity mention detection followed by entity disambiguation or ii)\nan end-to-end linking approaches that directly fetch the target entity given\nthe input text. The task presents unique challenges: queries are extremely\nshort (averaging 2.4 words), lack natural language structure, and must handle a\nmassive space of unique brands. We present a two-stage approach combining\nnamed-entity recognition with matching, and a novel end-to-end solution using\nextreme multi-class classification. We validate our solutions by both offline\nbenchmarks and the impact of online A/B test.\n", "link": "http://arxiv.org/abs/2502.01555v2", "date": "2025-10-14", "relevancy": 2.1681, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4614}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4197}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Query%20Brand%20Entity%20Linking%20in%20E-Commerce%20Search&body=Title%3A%20Query%20Brand%20Entity%20Linking%20in%20E-Commerce%20Search%0AAuthor%3A%20Dong%20Liu%20and%20Sreyashi%20Nag%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20address%20the%20brand%20entity%20linking%20problem%20for%20e-commerce%0Asearch%20queries.%20The%20entity%20linking%20task%20is%20done%20by%20either%20i%29a%20two-stage%20process%0Aconsisting%20of%20entity%20mention%20detection%20followed%20by%20entity%20disambiguation%20or%20ii%29%0Aan%20end-to-end%20linking%20approaches%20that%20directly%20fetch%20the%20target%20entity%20given%0Athe%20input%20text.%20The%20task%20presents%20unique%20challenges%3A%20queries%20are%20extremely%0Ashort%20%28averaging%202.4%20words%29%2C%20lack%20natural%20language%20structure%2C%20and%20must%20handle%20a%0Amassive%20space%20of%20unique%20brands.%20We%20present%20a%20two-stage%20approach%20combining%0Anamed-entity%20recognition%20with%20matching%2C%20and%20a%20novel%20end-to-end%20solution%20using%0Aextreme%20multi-class%20classification.%20We%20validate%20our%20solutions%20by%20both%20offline%0Abenchmarks%20and%20the%20impact%20of%20online%20A/B%20test.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01555v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuery%2520Brand%2520Entity%2520Linking%2520in%2520E-Commerce%2520Search%26entry.906535625%3DDong%2520Liu%2520and%2520Sreyashi%2520Nag%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520brand%2520entity%2520linking%2520problem%2520for%2520e-commerce%250Asearch%2520queries.%2520The%2520entity%2520linking%2520task%2520is%2520done%2520by%2520either%2520i%2529a%2520two-stage%2520process%250Aconsisting%2520of%2520entity%2520mention%2520detection%2520followed%2520by%2520entity%2520disambiguation%2520or%2520ii%2529%250Aan%2520end-to-end%2520linking%2520approaches%2520that%2520directly%2520fetch%2520the%2520target%2520entity%2520given%250Athe%2520input%2520text.%2520The%2520task%2520presents%2520unique%2520challenges%253A%2520queries%2520are%2520extremely%250Ashort%2520%2528averaging%25202.4%2520words%2529%252C%2520lack%2520natural%2520language%2520structure%252C%2520and%2520must%2520handle%2520a%250Amassive%2520space%2520of%2520unique%2520brands.%2520We%2520present%2520a%2520two-stage%2520approach%2520combining%250Anamed-entity%2520recognition%2520with%2520matching%252C%2520and%2520a%2520novel%2520end-to-end%2520solution%2520using%250Aextreme%2520multi-class%2520classification.%2520We%2520validate%2520our%2520solutions%2520by%2520both%2520offline%250Abenchmarks%2520and%2520the%2520impact%2520of%2520online%2520A/B%2520test.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01555v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Query%20Brand%20Entity%20Linking%20in%20E-Commerce%20Search&entry.906535625=Dong%20Liu%20and%20Sreyashi%20Nag&entry.1292438233=%20%20In%20this%20work%2C%20we%20address%20the%20brand%20entity%20linking%20problem%20for%20e-commerce%0Asearch%20queries.%20The%20entity%20linking%20task%20is%20done%20by%20either%20i%29a%20two-stage%20process%0Aconsisting%20of%20entity%20mention%20detection%20followed%20by%20entity%20disambiguation%20or%20ii%29%0Aan%20end-to-end%20linking%20approaches%20that%20directly%20fetch%20the%20target%20entity%20given%0Athe%20input%20text.%20The%20task%20presents%20unique%20challenges%3A%20queries%20are%20extremely%0Ashort%20%28averaging%202.4%20words%29%2C%20lack%20natural%20language%20structure%2C%20and%20must%20handle%20a%0Amassive%20space%20of%20unique%20brands.%20We%20present%20a%20two-stage%20approach%20combining%0Anamed-entity%20recognition%20with%20matching%2C%20and%20a%20novel%20end-to-end%20solution%20using%0Aextreme%20multi-class%20classification.%20We%20validate%20our%20solutions%20by%20both%20offline%0Abenchmarks%20and%20the%20impact%20of%20online%20A/B%20test.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01555v2&entry.124074799=Read"},
{"title": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via\n  Reinforced Fine-Tuning", "author": "Qi Wang and Yanrui Yu and Ye Yuan and Rui Mao and Tianfei Zhou", "abstract": "  Reinforcement fine-tuning (RFT) has shown great promise in achieving\nhumanlevel reasoning capabilities of Large Language Models (LLMs), and has\nrecently been extended to MLLMs. Nevertheless, reasoning about videos, which is\na fundamental aspect of human intelligence, remains a persistent challenge due\nto the complex logic, temporal and causal structures inherent in video data. To\nfill this gap, we propose VideoRFT, a novel approach that extends the RFT\nparadigm to cultivate human-like video reasoning capabilities in MLLMs.\nVideoRFT follows the standard two-stage scheme in RFT: supervised fine-tuning\n(SFT) with chain-of-thought (CoT) annotations, followed by reinforcement\nlearning (RL) to improve generalization. A central challenge to achieve this in\nthe video domain lies in the scarcity of large-scale, high-quality video CoT\ndatasets. We address this by building a multi-expert-driven, cognition-inspired\nCoT curation pipeline. First, we devise a cognition-inspired prompting strategy\nto elicit a reasoning LLM to generate preliminary CoTs based solely on rich,\nstructured, and literal representations of video content. Subsequently, these\nCoTs are revised by a MLLM conditioned on the actual video, ensuring visual\nconsistency and reducing visual hallucinations. This pipeline results in two\nnew datasets, i.e.VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To\nfurther strengthen the RL phase, we introduce a novel semantic-consistency\nreward that explicitly promotes the alignment between textual reasoning and\nvisual evidence. This reward encourages the model to produce coherent,\ncontext-aware reasoning outputs grounded in visual input. Extensive experiments\nshow that VideoRFT achieves state-of-the-art performance on six video reasoning\nbenchmarks.\n", "link": "http://arxiv.org/abs/2505.12434v4", "date": "2025-10-14", "relevancy": 2.168, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5429}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5429}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoRFT%3A%20Incentivizing%20Video%20Reasoning%20Capability%20in%20MLLMs%20via%0A%20%20Reinforced%20Fine-Tuning&body=Title%3A%20VideoRFT%3A%20Incentivizing%20Video%20Reasoning%20Capability%20in%20MLLMs%20via%0A%20%20Reinforced%20Fine-Tuning%0AAuthor%3A%20Qi%20Wang%20and%20Yanrui%20Yu%20and%20Ye%20Yuan%20and%20Rui%20Mao%20and%20Tianfei%20Zhou%0AAbstract%3A%20%20%20Reinforcement%20fine-tuning%20%28RFT%29%20has%20shown%20great%20promise%20in%20achieving%0Ahumanlevel%20reasoning%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20and%20has%0Arecently%20been%20extended%20to%20MLLMs.%20Nevertheless%2C%20reasoning%20about%20videos%2C%20which%20is%0Aa%20fundamental%20aspect%20of%20human%20intelligence%2C%20remains%20a%20persistent%20challenge%20due%0Ato%20the%20complex%20logic%2C%20temporal%20and%20causal%20structures%20inherent%20in%20video%20data.%20To%0Afill%20this%20gap%2C%20we%20propose%20VideoRFT%2C%20a%20novel%20approach%20that%20extends%20the%20RFT%0Aparadigm%20to%20cultivate%20human-like%20video%20reasoning%20capabilities%20in%20MLLMs.%0AVideoRFT%20follows%20the%20standard%20two-stage%20scheme%20in%20RFT%3A%20supervised%20fine-tuning%0A%28SFT%29%20with%20chain-of-thought%20%28CoT%29%20annotations%2C%20followed%20by%20reinforcement%0Alearning%20%28RL%29%20to%20improve%20generalization.%20A%20central%20challenge%20to%20achieve%20this%20in%0Athe%20video%20domain%20lies%20in%20the%20scarcity%20of%20large-scale%2C%20high-quality%20video%20CoT%0Adatasets.%20We%20address%20this%20by%20building%20a%20multi-expert-driven%2C%20cognition-inspired%0ACoT%20curation%20pipeline.%20First%2C%20we%20devise%20a%20cognition-inspired%20prompting%20strategy%0Ato%20elicit%20a%20reasoning%20LLM%20to%20generate%20preliminary%20CoTs%20based%20solely%20on%20rich%2C%0Astructured%2C%20and%20literal%20representations%20of%20video%20content.%20Subsequently%2C%20these%0ACoTs%20are%20revised%20by%20a%20MLLM%20conditioned%20on%20the%20actual%20video%2C%20ensuring%20visual%0Aconsistency%20and%20reducing%20visual%20hallucinations.%20This%20pipeline%20results%20in%20two%0Anew%20datasets%2C%20i.e.VideoRFT-CoT-102K%20for%20SFT%20and%20VideoRFT-RL-310K%20for%20RL.%20To%0Afurther%20strengthen%20the%20RL%20phase%2C%20we%20introduce%20a%20novel%20semantic-consistency%0Areward%20that%20explicitly%20promotes%20the%20alignment%20between%20textual%20reasoning%20and%0Avisual%20evidence.%20This%20reward%20encourages%20the%20model%20to%20produce%20coherent%2C%0Acontext-aware%20reasoning%20outputs%20grounded%20in%20visual%20input.%20Extensive%20experiments%0Ashow%20that%20VideoRFT%20achieves%20state-of-the-art%20performance%20on%20six%20video%20reasoning%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12434v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoRFT%253A%2520Incentivizing%2520Video%2520Reasoning%2520Capability%2520in%2520MLLMs%2520via%250A%2520%2520Reinforced%2520Fine-Tuning%26entry.906535625%3DQi%2520Wang%2520and%2520Yanrui%2520Yu%2520and%2520Ye%2520Yuan%2520and%2520Rui%2520Mao%2520and%2520Tianfei%2520Zhou%26entry.1292438233%3D%2520%2520Reinforcement%2520fine-tuning%2520%2528RFT%2529%2520has%2520shown%2520great%2520promise%2520in%2520achieving%250Ahumanlevel%2520reasoning%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520and%2520has%250Arecently%2520been%2520extended%2520to%2520MLLMs.%2520Nevertheless%252C%2520reasoning%2520about%2520videos%252C%2520which%2520is%250Aa%2520fundamental%2520aspect%2520of%2520human%2520intelligence%252C%2520remains%2520a%2520persistent%2520challenge%2520due%250Ato%2520the%2520complex%2520logic%252C%2520temporal%2520and%2520causal%2520structures%2520inherent%2520in%2520video%2520data.%2520To%250Afill%2520this%2520gap%252C%2520we%2520propose%2520VideoRFT%252C%2520a%2520novel%2520approach%2520that%2520extends%2520the%2520RFT%250Aparadigm%2520to%2520cultivate%2520human-like%2520video%2520reasoning%2520capabilities%2520in%2520MLLMs.%250AVideoRFT%2520follows%2520the%2520standard%2520two-stage%2520scheme%2520in%2520RFT%253A%2520supervised%2520fine-tuning%250A%2528SFT%2529%2520with%2520chain-of-thought%2520%2528CoT%2529%2520annotations%252C%2520followed%2520by%2520reinforcement%250Alearning%2520%2528RL%2529%2520to%2520improve%2520generalization.%2520A%2520central%2520challenge%2520to%2520achieve%2520this%2520in%250Athe%2520video%2520domain%2520lies%2520in%2520the%2520scarcity%2520of%2520large-scale%252C%2520high-quality%2520video%2520CoT%250Adatasets.%2520We%2520address%2520this%2520by%2520building%2520a%2520multi-expert-driven%252C%2520cognition-inspired%250ACoT%2520curation%2520pipeline.%2520First%252C%2520we%2520devise%2520a%2520cognition-inspired%2520prompting%2520strategy%250Ato%2520elicit%2520a%2520reasoning%2520LLM%2520to%2520generate%2520preliminary%2520CoTs%2520based%2520solely%2520on%2520rich%252C%250Astructured%252C%2520and%2520literal%2520representations%2520of%2520video%2520content.%2520Subsequently%252C%2520these%250ACoTs%2520are%2520revised%2520by%2520a%2520MLLM%2520conditioned%2520on%2520the%2520actual%2520video%252C%2520ensuring%2520visual%250Aconsistency%2520and%2520reducing%2520visual%2520hallucinations.%2520This%2520pipeline%2520results%2520in%2520two%250Anew%2520datasets%252C%2520i.e.VideoRFT-CoT-102K%2520for%2520SFT%2520and%2520VideoRFT-RL-310K%2520for%2520RL.%2520To%250Afurther%2520strengthen%2520the%2520RL%2520phase%252C%2520we%2520introduce%2520a%2520novel%2520semantic-consistency%250Areward%2520that%2520explicitly%2520promotes%2520the%2520alignment%2520between%2520textual%2520reasoning%2520and%250Avisual%2520evidence.%2520This%2520reward%2520encourages%2520the%2520model%2520to%2520produce%2520coherent%252C%250Acontext-aware%2520reasoning%2520outputs%2520grounded%2520in%2520visual%2520input.%2520Extensive%2520experiments%250Ashow%2520that%2520VideoRFT%2520achieves%2520state-of-the-art%2520performance%2520on%2520six%2520video%2520reasoning%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12434v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoRFT%3A%20Incentivizing%20Video%20Reasoning%20Capability%20in%20MLLMs%20via%0A%20%20Reinforced%20Fine-Tuning&entry.906535625=Qi%20Wang%20and%20Yanrui%20Yu%20and%20Ye%20Yuan%20and%20Rui%20Mao%20and%20Tianfei%20Zhou&entry.1292438233=%20%20Reinforcement%20fine-tuning%20%28RFT%29%20has%20shown%20great%20promise%20in%20achieving%0Ahumanlevel%20reasoning%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20and%20has%0Arecently%20been%20extended%20to%20MLLMs.%20Nevertheless%2C%20reasoning%20about%20videos%2C%20which%20is%0Aa%20fundamental%20aspect%20of%20human%20intelligence%2C%20remains%20a%20persistent%20challenge%20due%0Ato%20the%20complex%20logic%2C%20temporal%20and%20causal%20structures%20inherent%20in%20video%20data.%20To%0Afill%20this%20gap%2C%20we%20propose%20VideoRFT%2C%20a%20novel%20approach%20that%20extends%20the%20RFT%0Aparadigm%20to%20cultivate%20human-like%20video%20reasoning%20capabilities%20in%20MLLMs.%0AVideoRFT%20follows%20the%20standard%20two-stage%20scheme%20in%20RFT%3A%20supervised%20fine-tuning%0A%28SFT%29%20with%20chain-of-thought%20%28CoT%29%20annotations%2C%20followed%20by%20reinforcement%0Alearning%20%28RL%29%20to%20improve%20generalization.%20A%20central%20challenge%20to%20achieve%20this%20in%0Athe%20video%20domain%20lies%20in%20the%20scarcity%20of%20large-scale%2C%20high-quality%20video%20CoT%0Adatasets.%20We%20address%20this%20by%20building%20a%20multi-expert-driven%2C%20cognition-inspired%0ACoT%20curation%20pipeline.%20First%2C%20we%20devise%20a%20cognition-inspired%20prompting%20strategy%0Ato%20elicit%20a%20reasoning%20LLM%20to%20generate%20preliminary%20CoTs%20based%20solely%20on%20rich%2C%0Astructured%2C%20and%20literal%20representations%20of%20video%20content.%20Subsequently%2C%20these%0ACoTs%20are%20revised%20by%20a%20MLLM%20conditioned%20on%20the%20actual%20video%2C%20ensuring%20visual%0Aconsistency%20and%20reducing%20visual%20hallucinations.%20This%20pipeline%20results%20in%20two%0Anew%20datasets%2C%20i.e.VideoRFT-CoT-102K%20for%20SFT%20and%20VideoRFT-RL-310K%20for%20RL.%20To%0Afurther%20strengthen%20the%20RL%20phase%2C%20we%20introduce%20a%20novel%20semantic-consistency%0Areward%20that%20explicitly%20promotes%20the%20alignment%20between%20textual%20reasoning%20and%0Avisual%20evidence.%20This%20reward%20encourages%20the%20model%20to%20produce%20coherent%2C%0Acontext-aware%20reasoning%20outputs%20grounded%20in%20visual%20input.%20Extensive%20experiments%0Ashow%20that%20VideoRFT%20achieves%20state-of-the-art%20performance%20on%20six%20video%20reasoning%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12434v4&entry.124074799=Read"},
{"title": "Computing Systemic Risk Measures with Graph Neural Networks", "author": "Lukas Gonon and Thilo Meyer-Brandis and Niklas Weber", "abstract": "  This paper investigates systemic risk measures for stochastic financial\nnetworks of explicitly modelled bilateral liabilities. We extend the notion of\nsystemic risk measures from Biagini, Fouque, Fritelli and Meyer-Brandis (2019)\nto graph structured data. In particular, we focus on an aggregation function\nthat is derived from a market clearing algorithm proposed by Eisenberg and Noe\n(2001). In this setting, we show the existence of an optimal random allocation\nthat distributes the overall minimal bailout capital and secures the network.\nWe study numerical methods for the approximation of systemic risk and optimal\nrandom allocations. We propose to use permutation equivariant architectures of\nneural networks like graph neural networks (GNNs) and a class that we name\n(extended) permutation equivariant neural networks ((X)PENNs). We compare their\nperformance to several benchmark allocations. The main feature of GNNs and\n(X)PENNs is that they are permutation equivariant with respect to the\nunderlying graph data. In numerical experiments we find evidence that these\npermutation equivariant methods are superior to other approaches.\n", "link": "http://arxiv.org/abs/2410.07222v2", "date": "2025-10-14", "relevancy": 2.1633, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4417}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4372}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computing%20Systemic%20Risk%20Measures%20with%20Graph%20Neural%20Networks&body=Title%3A%20Computing%20Systemic%20Risk%20Measures%20with%20Graph%20Neural%20Networks%0AAuthor%3A%20Lukas%20Gonon%20and%20Thilo%20Meyer-Brandis%20and%20Niklas%20Weber%0AAbstract%3A%20%20%20This%20paper%20investigates%20systemic%20risk%20measures%20for%20stochastic%20financial%0Anetworks%20of%20explicitly%20modelled%20bilateral%20liabilities.%20We%20extend%20the%20notion%20of%0Asystemic%20risk%20measures%20from%20Biagini%2C%20Fouque%2C%20Fritelli%20and%20Meyer-Brandis%20%282019%29%0Ato%20graph%20structured%20data.%20In%20particular%2C%20we%20focus%20on%20an%20aggregation%20function%0Athat%20is%20derived%20from%20a%20market%20clearing%20algorithm%20proposed%20by%20Eisenberg%20and%20Noe%0A%282001%29.%20In%20this%20setting%2C%20we%20show%20the%20existence%20of%20an%20optimal%20random%20allocation%0Athat%20distributes%20the%20overall%20minimal%20bailout%20capital%20and%20secures%20the%20network.%0AWe%20study%20numerical%20methods%20for%20the%20approximation%20of%20systemic%20risk%20and%20optimal%0Arandom%20allocations.%20We%20propose%20to%20use%20permutation%20equivariant%20architectures%20of%0Aneural%20networks%20like%20graph%20neural%20networks%20%28GNNs%29%20and%20a%20class%20that%20we%20name%0A%28extended%29%20permutation%20equivariant%20neural%20networks%20%28%28X%29PENNs%29.%20We%20compare%20their%0Aperformance%20to%20several%20benchmark%20allocations.%20The%20main%20feature%20of%20GNNs%20and%0A%28X%29PENNs%20is%20that%20they%20are%20permutation%20equivariant%20with%20respect%20to%20the%0Aunderlying%20graph%20data.%20In%20numerical%20experiments%20we%20find%20evidence%20that%20these%0Apermutation%20equivariant%20methods%20are%20superior%20to%20other%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07222v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputing%2520Systemic%2520Risk%2520Measures%2520with%2520Graph%2520Neural%2520Networks%26entry.906535625%3DLukas%2520Gonon%2520and%2520Thilo%2520Meyer-Brandis%2520and%2520Niklas%2520Weber%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520systemic%2520risk%2520measures%2520for%2520stochastic%2520financial%250Anetworks%2520of%2520explicitly%2520modelled%2520bilateral%2520liabilities.%2520We%2520extend%2520the%2520notion%2520of%250Asystemic%2520risk%2520measures%2520from%2520Biagini%252C%2520Fouque%252C%2520Fritelli%2520and%2520Meyer-Brandis%2520%25282019%2529%250Ato%2520graph%2520structured%2520data.%2520In%2520particular%252C%2520we%2520focus%2520on%2520an%2520aggregation%2520function%250Athat%2520is%2520derived%2520from%2520a%2520market%2520clearing%2520algorithm%2520proposed%2520by%2520Eisenberg%2520and%2520Noe%250A%25282001%2529.%2520In%2520this%2520setting%252C%2520we%2520show%2520the%2520existence%2520of%2520an%2520optimal%2520random%2520allocation%250Athat%2520distributes%2520the%2520overall%2520minimal%2520bailout%2520capital%2520and%2520secures%2520the%2520network.%250AWe%2520study%2520numerical%2520methods%2520for%2520the%2520approximation%2520of%2520systemic%2520risk%2520and%2520optimal%250Arandom%2520allocations.%2520We%2520propose%2520to%2520use%2520permutation%2520equivariant%2520architectures%2520of%250Aneural%2520networks%2520like%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520and%2520a%2520class%2520that%2520we%2520name%250A%2528extended%2529%2520permutation%2520equivariant%2520neural%2520networks%2520%2528%2528X%2529PENNs%2529.%2520We%2520compare%2520their%250Aperformance%2520to%2520several%2520benchmark%2520allocations.%2520The%2520main%2520feature%2520of%2520GNNs%2520and%250A%2528X%2529PENNs%2520is%2520that%2520they%2520are%2520permutation%2520equivariant%2520with%2520respect%2520to%2520the%250Aunderlying%2520graph%2520data.%2520In%2520numerical%2520experiments%2520we%2520find%2520evidence%2520that%2520these%250Apermutation%2520equivariant%2520methods%2520are%2520superior%2520to%2520other%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07222v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computing%20Systemic%20Risk%20Measures%20with%20Graph%20Neural%20Networks&entry.906535625=Lukas%20Gonon%20and%20Thilo%20Meyer-Brandis%20and%20Niklas%20Weber&entry.1292438233=%20%20This%20paper%20investigates%20systemic%20risk%20measures%20for%20stochastic%20financial%0Anetworks%20of%20explicitly%20modelled%20bilateral%20liabilities.%20We%20extend%20the%20notion%20of%0Asystemic%20risk%20measures%20from%20Biagini%2C%20Fouque%2C%20Fritelli%20and%20Meyer-Brandis%20%282019%29%0Ato%20graph%20structured%20data.%20In%20particular%2C%20we%20focus%20on%20an%20aggregation%20function%0Athat%20is%20derived%20from%20a%20market%20clearing%20algorithm%20proposed%20by%20Eisenberg%20and%20Noe%0A%282001%29.%20In%20this%20setting%2C%20we%20show%20the%20existence%20of%20an%20optimal%20random%20allocation%0Athat%20distributes%20the%20overall%20minimal%20bailout%20capital%20and%20secures%20the%20network.%0AWe%20study%20numerical%20methods%20for%20the%20approximation%20of%20systemic%20risk%20and%20optimal%0Arandom%20allocations.%20We%20propose%20to%20use%20permutation%20equivariant%20architectures%20of%0Aneural%20networks%20like%20graph%20neural%20networks%20%28GNNs%29%20and%20a%20class%20that%20we%20name%0A%28extended%29%20permutation%20equivariant%20neural%20networks%20%28%28X%29PENNs%29.%20We%20compare%20their%0Aperformance%20to%20several%20benchmark%20allocations.%20The%20main%20feature%20of%20GNNs%20and%0A%28X%29PENNs%20is%20that%20they%20are%20permutation%20equivariant%20with%20respect%20to%20the%0Aunderlying%20graph%20data.%20In%20numerical%20experiments%20we%20find%20evidence%20that%20these%0Apermutation%20equivariant%20methods%20are%20superior%20to%20other%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07222v2&entry.124074799=Read"},
{"title": "PET Head Motion Estimation Using Supervised Deep Learning with Attention", "author": "Zhuotong Cai and Tianyi Zeng and Jiazhen Zhang and El\u00e9onore V. Lieffrig and Kathryn Fontaine and Chenyu You and Enette Mae Revilla and James S. Duncan and Jingmin Xin and Yihuan Lu and John A. Onofrey", "abstract": "  Head movement poses a significant challenge in brain positron emission\ntomography (PET) imaging, resulting in image artifacts and tracer uptake\nquantification inaccuracies. Effective head motion estimation and correction\nare crucial for precise quantitative image analysis and accurate diagnosis of\nneurological disorders. Hardware-based motion tracking (HMT) has limited\napplicability in real-world clinical practice. To overcome this limitation, we\npropose a deep-learning head motion correction approach with cross-attention\n(DL-HMC++) to predict rigid head motion from one-second 3D PET raw data.\nDL-HMC++ is trained in a supervised manner by leveraging existing dynamic PET\nscans with gold-standard motion measurements from external HMT. We evaluate\nDL-HMC++ on two PET scanners (HRRT and mCT) and four radiotracers (18F-FDG,\n18F-FPEB, 11C-UCB-J, and 11C-LSN3172176) to demonstrate the effectiveness and\ngeneralization of the approach in large cohort PET studies. Quantitative and\nqualitative results demonstrate that DL-HMC++ consistently outperforms\nstate-of-the-art data-driven motion estimation methods, producing motion-free\nimages with clear delineation of brain structures and reduced motion artifacts\nthat are indistinguishable from gold-standard HMT. Brain region of interest\nstandard uptake value analysis exhibits average difference ratios between\nDL-HMC++ and gold-standard HMT to be 1.2 plus-minus 0.5% for HRRT and 0.5\nplus-minus 0.2% for mCT. DL-HMC++ demonstrates the potential for data-driven\nPET head motion correction to remove the burden of HMT, making motion\ncorrection accessible to clinical populations beyond research settings. The\ncode is available at https://github.com/maxxxxxxcai/DL-HMC-TMI.\n", "link": "http://arxiv.org/abs/2510.12758v1", "date": "2025-10-14", "relevancy": 2.1631, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5626}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5424}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PET%20Head%20Motion%20Estimation%20Using%20Supervised%20Deep%20Learning%20with%20Attention&body=Title%3A%20PET%20Head%20Motion%20Estimation%20Using%20Supervised%20Deep%20Learning%20with%20Attention%0AAuthor%3A%20Zhuotong%20Cai%20and%20Tianyi%20Zeng%20and%20Jiazhen%20Zhang%20and%20El%C3%A9onore%20V.%20Lieffrig%20and%20Kathryn%20Fontaine%20and%20Chenyu%20You%20and%20Enette%20Mae%20Revilla%20and%20James%20S.%20Duncan%20and%20Jingmin%20Xin%20and%20Yihuan%20Lu%20and%20John%20A.%20Onofrey%0AAbstract%3A%20%20%20Head%20movement%20poses%20a%20significant%20challenge%20in%20brain%20positron%20emission%0Atomography%20%28PET%29%20imaging%2C%20resulting%20in%20image%20artifacts%20and%20tracer%20uptake%0Aquantification%20inaccuracies.%20Effective%20head%20motion%20estimation%20and%20correction%0Aare%20crucial%20for%20precise%20quantitative%20image%20analysis%20and%20accurate%20diagnosis%20of%0Aneurological%20disorders.%20Hardware-based%20motion%20tracking%20%28HMT%29%20has%20limited%0Aapplicability%20in%20real-world%20clinical%20practice.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20a%20deep-learning%20head%20motion%20correction%20approach%20with%20cross-attention%0A%28DL-HMC%2B%2B%29%20to%20predict%20rigid%20head%20motion%20from%20one-second%203D%20PET%20raw%20data.%0ADL-HMC%2B%2B%20is%20trained%20in%20a%20supervised%20manner%20by%20leveraging%20existing%20dynamic%20PET%0Ascans%20with%20gold-standard%20motion%20measurements%20from%20external%20HMT.%20We%20evaluate%0ADL-HMC%2B%2B%20on%20two%20PET%20scanners%20%28HRRT%20and%20mCT%29%20and%20four%20radiotracers%20%2818F-FDG%2C%0A18F-FPEB%2C%2011C-UCB-J%2C%20and%2011C-LSN3172176%29%20to%20demonstrate%20the%20effectiveness%20and%0Ageneralization%20of%20the%20approach%20in%20large%20cohort%20PET%20studies.%20Quantitative%20and%0Aqualitative%20results%20demonstrate%20that%20DL-HMC%2B%2B%20consistently%20outperforms%0Astate-of-the-art%20data-driven%20motion%20estimation%20methods%2C%20producing%20motion-free%0Aimages%20with%20clear%20delineation%20of%20brain%20structures%20and%20reduced%20motion%20artifacts%0Athat%20are%20indistinguishable%20from%20gold-standard%20HMT.%20Brain%20region%20of%20interest%0Astandard%20uptake%20value%20analysis%20exhibits%20average%20difference%20ratios%20between%0ADL-HMC%2B%2B%20and%20gold-standard%20HMT%20to%20be%201.2%20plus-minus%200.5%25%20for%20HRRT%20and%200.5%0Aplus-minus%200.2%25%20for%20mCT.%20DL-HMC%2B%2B%20demonstrates%20the%20potential%20for%20data-driven%0APET%20head%20motion%20correction%20to%20remove%20the%20burden%20of%20HMT%2C%20making%20motion%0Acorrection%20accessible%20to%20clinical%20populations%20beyond%20research%20settings.%20The%0Acode%20is%20available%20at%20https%3A//github.com/maxxxxxxcai/DL-HMC-TMI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12758v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPET%2520Head%2520Motion%2520Estimation%2520Using%2520Supervised%2520Deep%2520Learning%2520with%2520Attention%26entry.906535625%3DZhuotong%2520Cai%2520and%2520Tianyi%2520Zeng%2520and%2520Jiazhen%2520Zhang%2520and%2520El%25C3%25A9onore%2520V.%2520Lieffrig%2520and%2520Kathryn%2520Fontaine%2520and%2520Chenyu%2520You%2520and%2520Enette%2520Mae%2520Revilla%2520and%2520James%2520S.%2520Duncan%2520and%2520Jingmin%2520Xin%2520and%2520Yihuan%2520Lu%2520and%2520John%2520A.%2520Onofrey%26entry.1292438233%3D%2520%2520Head%2520movement%2520poses%2520a%2520significant%2520challenge%2520in%2520brain%2520positron%2520emission%250Atomography%2520%2528PET%2529%2520imaging%252C%2520resulting%2520in%2520image%2520artifacts%2520and%2520tracer%2520uptake%250Aquantification%2520inaccuracies.%2520Effective%2520head%2520motion%2520estimation%2520and%2520correction%250Aare%2520crucial%2520for%2520precise%2520quantitative%2520image%2520analysis%2520and%2520accurate%2520diagnosis%2520of%250Aneurological%2520disorders.%2520Hardware-based%2520motion%2520tracking%2520%2528HMT%2529%2520has%2520limited%250Aapplicability%2520in%2520real-world%2520clinical%2520practice.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Apropose%2520a%2520deep-learning%2520head%2520motion%2520correction%2520approach%2520with%2520cross-attention%250A%2528DL-HMC%252B%252B%2529%2520to%2520predict%2520rigid%2520head%2520motion%2520from%2520one-second%25203D%2520PET%2520raw%2520data.%250ADL-HMC%252B%252B%2520is%2520trained%2520in%2520a%2520supervised%2520manner%2520by%2520leveraging%2520existing%2520dynamic%2520PET%250Ascans%2520with%2520gold-standard%2520motion%2520measurements%2520from%2520external%2520HMT.%2520We%2520evaluate%250ADL-HMC%252B%252B%2520on%2520two%2520PET%2520scanners%2520%2528HRRT%2520and%2520mCT%2529%2520and%2520four%2520radiotracers%2520%252818F-FDG%252C%250A18F-FPEB%252C%252011C-UCB-J%252C%2520and%252011C-LSN3172176%2529%2520to%2520demonstrate%2520the%2520effectiveness%2520and%250Ageneralization%2520of%2520the%2520approach%2520in%2520large%2520cohort%2520PET%2520studies.%2520Quantitative%2520and%250Aqualitative%2520results%2520demonstrate%2520that%2520DL-HMC%252B%252B%2520consistently%2520outperforms%250Astate-of-the-art%2520data-driven%2520motion%2520estimation%2520methods%252C%2520producing%2520motion-free%250Aimages%2520with%2520clear%2520delineation%2520of%2520brain%2520structures%2520and%2520reduced%2520motion%2520artifacts%250Athat%2520are%2520indistinguishable%2520from%2520gold-standard%2520HMT.%2520Brain%2520region%2520of%2520interest%250Astandard%2520uptake%2520value%2520analysis%2520exhibits%2520average%2520difference%2520ratios%2520between%250ADL-HMC%252B%252B%2520and%2520gold-standard%2520HMT%2520to%2520be%25201.2%2520plus-minus%25200.5%2525%2520for%2520HRRT%2520and%25200.5%250Aplus-minus%25200.2%2525%2520for%2520mCT.%2520DL-HMC%252B%252B%2520demonstrates%2520the%2520potential%2520for%2520data-driven%250APET%2520head%2520motion%2520correction%2520to%2520remove%2520the%2520burden%2520of%2520HMT%252C%2520making%2520motion%250Acorrection%2520accessible%2520to%2520clinical%2520populations%2520beyond%2520research%2520settings.%2520The%250Acode%2520is%2520available%2520at%2520https%253A//github.com/maxxxxxxcai/DL-HMC-TMI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12758v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PET%20Head%20Motion%20Estimation%20Using%20Supervised%20Deep%20Learning%20with%20Attention&entry.906535625=Zhuotong%20Cai%20and%20Tianyi%20Zeng%20and%20Jiazhen%20Zhang%20and%20El%C3%A9onore%20V.%20Lieffrig%20and%20Kathryn%20Fontaine%20and%20Chenyu%20You%20and%20Enette%20Mae%20Revilla%20and%20James%20S.%20Duncan%20and%20Jingmin%20Xin%20and%20Yihuan%20Lu%20and%20John%20A.%20Onofrey&entry.1292438233=%20%20Head%20movement%20poses%20a%20significant%20challenge%20in%20brain%20positron%20emission%0Atomography%20%28PET%29%20imaging%2C%20resulting%20in%20image%20artifacts%20and%20tracer%20uptake%0Aquantification%20inaccuracies.%20Effective%20head%20motion%20estimation%20and%20correction%0Aare%20crucial%20for%20precise%20quantitative%20image%20analysis%20and%20accurate%20diagnosis%20of%0Aneurological%20disorders.%20Hardware-based%20motion%20tracking%20%28HMT%29%20has%20limited%0Aapplicability%20in%20real-world%20clinical%20practice.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20a%20deep-learning%20head%20motion%20correction%20approach%20with%20cross-attention%0A%28DL-HMC%2B%2B%29%20to%20predict%20rigid%20head%20motion%20from%20one-second%203D%20PET%20raw%20data.%0ADL-HMC%2B%2B%20is%20trained%20in%20a%20supervised%20manner%20by%20leveraging%20existing%20dynamic%20PET%0Ascans%20with%20gold-standard%20motion%20measurements%20from%20external%20HMT.%20We%20evaluate%0ADL-HMC%2B%2B%20on%20two%20PET%20scanners%20%28HRRT%20and%20mCT%29%20and%20four%20radiotracers%20%2818F-FDG%2C%0A18F-FPEB%2C%2011C-UCB-J%2C%20and%2011C-LSN3172176%29%20to%20demonstrate%20the%20effectiveness%20and%0Ageneralization%20of%20the%20approach%20in%20large%20cohort%20PET%20studies.%20Quantitative%20and%0Aqualitative%20results%20demonstrate%20that%20DL-HMC%2B%2B%20consistently%20outperforms%0Astate-of-the-art%20data-driven%20motion%20estimation%20methods%2C%20producing%20motion-free%0Aimages%20with%20clear%20delineation%20of%20brain%20structures%20and%20reduced%20motion%20artifacts%0Athat%20are%20indistinguishable%20from%20gold-standard%20HMT.%20Brain%20region%20of%20interest%0Astandard%20uptake%20value%20analysis%20exhibits%20average%20difference%20ratios%20between%0ADL-HMC%2B%2B%20and%20gold-standard%20HMT%20to%20be%201.2%20plus-minus%200.5%25%20for%20HRRT%20and%200.5%0Aplus-minus%200.2%25%20for%20mCT.%20DL-HMC%2B%2B%20demonstrates%20the%20potential%20for%20data-driven%0APET%20head%20motion%20correction%20to%20remove%20the%20burden%20of%20HMT%2C%20making%20motion%0Acorrection%20accessible%20to%20clinical%20populations%20beyond%20research%20settings.%20The%0Acode%20is%20available%20at%20https%3A//github.com/maxxxxxxcai/DL-HMC-TMI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12758v1&entry.124074799=Read"},
{"title": "Mind the (Data) Gap: Evaluating Vision Systems in Small Data\n  Applications", "author": "Samuel Stevens and S M Rayeed and Jenna Kline", "abstract": "  The practical application of AI tools for specific computer vision tasks\nrelies on the \"small-data regime\" of hundreds to thousands of labeled samples.\nThis small-data regime is vital for applications requiring expensive expert\nannotations, such as ecological monitoring, medical diagnostics or industrial\nquality control. We find, however, that computer vision research has ignored\nthe small data regime as evaluations increasingly focus on zero- and few-shot\nlearning. We use the Natural World Tasks (NeWT) benchmark to compare\nmulti-modal large language models (MLLMs) and vision-only methods across\nvarying training set sizes. MLLMs exhibit early performance plateaus, while\nvision-only methods improve throughout the small-data regime, with performance\ngaps widening beyond 10 training examples. We provide the first comprehensive\ncomparison between these approaches in small-data contexts and advocate for\nexplicit small-data evaluations in AI research to better bridge theoretical\nadvances with practical deployments.\n", "link": "http://arxiv.org/abs/2504.06486v2", "date": "2025-10-14", "relevancy": 2.1619, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5451}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5451}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20the%20%28Data%29%20Gap%3A%20Evaluating%20Vision%20Systems%20in%20Small%20Data%0A%20%20Applications&body=Title%3A%20Mind%20the%20%28Data%29%20Gap%3A%20Evaluating%20Vision%20Systems%20in%20Small%20Data%0A%20%20Applications%0AAuthor%3A%20Samuel%20Stevens%20and%20S%20M%20Rayeed%20and%20Jenna%20Kline%0AAbstract%3A%20%20%20The%20practical%20application%20of%20AI%20tools%20for%20specific%20computer%20vision%20tasks%0Arelies%20on%20the%20%22small-data%20regime%22%20of%20hundreds%20to%20thousands%20of%20labeled%20samples.%0AThis%20small-data%20regime%20is%20vital%20for%20applications%20requiring%20expensive%20expert%0Aannotations%2C%20such%20as%20ecological%20monitoring%2C%20medical%20diagnostics%20or%20industrial%0Aquality%20control.%20We%20find%2C%20however%2C%20that%20computer%20vision%20research%20has%20ignored%0Athe%20small%20data%20regime%20as%20evaluations%20increasingly%20focus%20on%20zero-%20and%20few-shot%0Alearning.%20We%20use%20the%20Natural%20World%20Tasks%20%28NeWT%29%20benchmark%20to%20compare%0Amulti-modal%20large%20language%20models%20%28MLLMs%29%20and%20vision-only%20methods%20across%0Avarying%20training%20set%20sizes.%20MLLMs%20exhibit%20early%20performance%20plateaus%2C%20while%0Avision-only%20methods%20improve%20throughout%20the%20small-data%20regime%2C%20with%20performance%0Agaps%20widening%20beyond%2010%20training%20examples.%20We%20provide%20the%20first%20comprehensive%0Acomparison%20between%20these%20approaches%20in%20small-data%20contexts%20and%20advocate%20for%0Aexplicit%20small-data%20evaluations%20in%20AI%20research%20to%20better%20bridge%20theoretical%0Aadvances%20with%20practical%20deployments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.06486v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520the%2520%2528Data%2529%2520Gap%253A%2520Evaluating%2520Vision%2520Systems%2520in%2520Small%2520Data%250A%2520%2520Applications%26entry.906535625%3DSamuel%2520Stevens%2520and%2520S%2520M%2520Rayeed%2520and%2520Jenna%2520Kline%26entry.1292438233%3D%2520%2520The%2520practical%2520application%2520of%2520AI%2520tools%2520for%2520specific%2520computer%2520vision%2520tasks%250Arelies%2520on%2520the%2520%2522small-data%2520regime%2522%2520of%2520hundreds%2520to%2520thousands%2520of%2520labeled%2520samples.%250AThis%2520small-data%2520regime%2520is%2520vital%2520for%2520applications%2520requiring%2520expensive%2520expert%250Aannotations%252C%2520such%2520as%2520ecological%2520monitoring%252C%2520medical%2520diagnostics%2520or%2520industrial%250Aquality%2520control.%2520We%2520find%252C%2520however%252C%2520that%2520computer%2520vision%2520research%2520has%2520ignored%250Athe%2520small%2520data%2520regime%2520as%2520evaluations%2520increasingly%2520focus%2520on%2520zero-%2520and%2520few-shot%250Alearning.%2520We%2520use%2520the%2520Natural%2520World%2520Tasks%2520%2528NeWT%2529%2520benchmark%2520to%2520compare%250Amulti-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520and%2520vision-only%2520methods%2520across%250Avarying%2520training%2520set%2520sizes.%2520MLLMs%2520exhibit%2520early%2520performance%2520plateaus%252C%2520while%250Avision-only%2520methods%2520improve%2520throughout%2520the%2520small-data%2520regime%252C%2520with%2520performance%250Agaps%2520widening%2520beyond%252010%2520training%2520examples.%2520We%2520provide%2520the%2520first%2520comprehensive%250Acomparison%2520between%2520these%2520approaches%2520in%2520small-data%2520contexts%2520and%2520advocate%2520for%250Aexplicit%2520small-data%2520evaluations%2520in%2520AI%2520research%2520to%2520better%2520bridge%2520theoretical%250Aadvances%2520with%2520practical%2520deployments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06486v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20the%20%28Data%29%20Gap%3A%20Evaluating%20Vision%20Systems%20in%20Small%20Data%0A%20%20Applications&entry.906535625=Samuel%20Stevens%20and%20S%20M%20Rayeed%20and%20Jenna%20Kline&entry.1292438233=%20%20The%20practical%20application%20of%20AI%20tools%20for%20specific%20computer%20vision%20tasks%0Arelies%20on%20the%20%22small-data%20regime%22%20of%20hundreds%20to%20thousands%20of%20labeled%20samples.%0AThis%20small-data%20regime%20is%20vital%20for%20applications%20requiring%20expensive%20expert%0Aannotations%2C%20such%20as%20ecological%20monitoring%2C%20medical%20diagnostics%20or%20industrial%0Aquality%20control.%20We%20find%2C%20however%2C%20that%20computer%20vision%20research%20has%20ignored%0Athe%20small%20data%20regime%20as%20evaluations%20increasingly%20focus%20on%20zero-%20and%20few-shot%0Alearning.%20We%20use%20the%20Natural%20World%20Tasks%20%28NeWT%29%20benchmark%20to%20compare%0Amulti-modal%20large%20language%20models%20%28MLLMs%29%20and%20vision-only%20methods%20across%0Avarying%20training%20set%20sizes.%20MLLMs%20exhibit%20early%20performance%20plateaus%2C%20while%0Avision-only%20methods%20improve%20throughout%20the%20small-data%20regime%2C%20with%20performance%0Agaps%20widening%20beyond%2010%20training%20examples.%20We%20provide%20the%20first%20comprehensive%0Acomparison%20between%20these%20approaches%20in%20small-data%20contexts%20and%20advocate%20for%0Aexplicit%20small-data%20evaluations%20in%20AI%20research%20to%20better%20bridge%20theoretical%0Aadvances%20with%20practical%20deployments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.06486v2&entry.124074799=Read"},
{"title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding", "author": "Ramchalam Kinattinkara Ramakrishnan and Zhaocong Yuan and Shaojie Zhuo and Chen Feng and Yicheng Lin and Chenzheng Su and Xiaopeng Zhang", "abstract": "  Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.\n", "link": "http://arxiv.org/abs/2507.02659v3", "date": "2025-10-14", "relevancy": 2.1564, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5422}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5385}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniDraft%3A%20A%20Cross-vocabulary%2C%20Online%20Adaptive%20Drafter%20for%20On-device%0A%20%20Speculative%20Decoding&body=Title%3A%20OmniDraft%3A%20A%20Cross-vocabulary%2C%20Online%20Adaptive%20Drafter%20for%20On-device%0A%20%20Speculative%20Decoding%0AAuthor%3A%20Ramchalam%20Kinattinkara%20Ramakrishnan%20and%20Zhaocong%20Yuan%20and%20Shaojie%20Zhuo%20and%20Chen%20Feng%20and%20Yicheng%20Lin%20and%20Chenzheng%20Su%20and%20Xiaopeng%20Zhang%0AAbstract%3A%20%20%20Speculative%20decoding%20generally%20dictates%20having%20a%20small%2C%20efficient%20draft%20model%0Athat%20is%20either%20pretrained%20or%20distilled%20offline%20to%20a%20particular%20target%20model%0Aseries%2C%20for%20instance%2C%20Llama%20or%20Qwen%20models.%20However%2C%20within%20online%20deployment%0Asettings%2C%20there%20are%20two%20major%20challenges%3A%201%29%20usage%20of%20a%20target%20model%20that%20is%0Aincompatible%20with%20the%20draft%20model%3B%202%29%20expectation%20of%20latency%20improvements%20over%0Ausage%20and%20time.%20In%20this%20work%2C%20we%20propose%20OmniDraft%2C%20a%20unified%20framework%20that%0Aenables%20a%20single%20draft%20model%20to%20operate%20with%20any%20target%20model%20and%20adapt%0Adynamically%20to%20user%20data.%20We%20introduce%20an%20online%20n-gram%20cache%20with%20hybrid%0Adistillation%20fine-tuning%20to%20address%20the%20cross-vocabulary%20mismatch%20across%20draft%0Aand%20target%20models%3B%20and%20further%20improve%20decoding%20speed%20by%20leveraging%20adaptive%0Adrafting%20techniques.%20OmniDraft%20is%20particularly%20suitable%20for%20on-device%20LLM%0Aapplications%20where%20model%20cost%2C%20efficiency%20and%20user%20customization%20are%20the%20major%0Apoints%20of%20contention.%20This%20further%20highlights%20the%20need%20to%20tackle%20the%20above%0Achallenges%20and%20motivates%20the%20%5Ctextit%7B%60%60one%20drafter%20for%20all%27%27%7D%20paradigm.%20We%0Ashowcase%20the%20proficiency%20of%20the%20OmniDraft%20framework%20by%20performing%20online%0Alearning%20on%20math%20reasoning%2C%20coding%20and%20text%20generation%20tasks.%20Notably%2C%0AOmniDraft%20enables%20a%20single%20Llama-68M%20model%20to%20pair%20with%20various%20target%20models%0Aincluding%20Vicuna-7B%2C%20Qwen2-7B%20and%20Llama3-8B%20models%20for%20speculative%20decoding%3B%0Aand%20additionally%20provides%20up%20to%201.5-2x%20speedup.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02659v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniDraft%253A%2520A%2520Cross-vocabulary%252C%2520Online%2520Adaptive%2520Drafter%2520for%2520On-device%250A%2520%2520Speculative%2520Decoding%26entry.906535625%3DRamchalam%2520Kinattinkara%2520Ramakrishnan%2520and%2520Zhaocong%2520Yuan%2520and%2520Shaojie%2520Zhuo%2520and%2520Chen%2520Feng%2520and%2520Yicheng%2520Lin%2520and%2520Chenzheng%2520Su%2520and%2520Xiaopeng%2520Zhang%26entry.1292438233%3D%2520%2520Speculative%2520decoding%2520generally%2520dictates%2520having%2520a%2520small%252C%2520efficient%2520draft%2520model%250Athat%2520is%2520either%2520pretrained%2520or%2520distilled%2520offline%2520to%2520a%2520particular%2520target%2520model%250Aseries%252C%2520for%2520instance%252C%2520Llama%2520or%2520Qwen%2520models.%2520However%252C%2520within%2520online%2520deployment%250Asettings%252C%2520there%2520are%2520two%2520major%2520challenges%253A%25201%2529%2520usage%2520of%2520a%2520target%2520model%2520that%2520is%250Aincompatible%2520with%2520the%2520draft%2520model%253B%25202%2529%2520expectation%2520of%2520latency%2520improvements%2520over%250Ausage%2520and%2520time.%2520In%2520this%2520work%252C%2520we%2520propose%2520OmniDraft%252C%2520a%2520unified%2520framework%2520that%250Aenables%2520a%2520single%2520draft%2520model%2520to%2520operate%2520with%2520any%2520target%2520model%2520and%2520adapt%250Adynamically%2520to%2520user%2520data.%2520We%2520introduce%2520an%2520online%2520n-gram%2520cache%2520with%2520hybrid%250Adistillation%2520fine-tuning%2520to%2520address%2520the%2520cross-vocabulary%2520mismatch%2520across%2520draft%250Aand%2520target%2520models%253B%2520and%2520further%2520improve%2520decoding%2520speed%2520by%2520leveraging%2520adaptive%250Adrafting%2520techniques.%2520OmniDraft%2520is%2520particularly%2520suitable%2520for%2520on-device%2520LLM%250Aapplications%2520where%2520model%2520cost%252C%2520efficiency%2520and%2520user%2520customization%2520are%2520the%2520major%250Apoints%2520of%2520contention.%2520This%2520further%2520highlights%2520the%2520need%2520to%2520tackle%2520the%2520above%250Achallenges%2520and%2520motivates%2520the%2520%255Ctextit%257B%2560%2560one%2520drafter%2520for%2520all%2527%2527%257D%2520paradigm.%2520We%250Ashowcase%2520the%2520proficiency%2520of%2520the%2520OmniDraft%2520framework%2520by%2520performing%2520online%250Alearning%2520on%2520math%2520reasoning%252C%2520coding%2520and%2520text%2520generation%2520tasks.%2520Notably%252C%250AOmniDraft%2520enables%2520a%2520single%2520Llama-68M%2520model%2520to%2520pair%2520with%2520various%2520target%2520models%250Aincluding%2520Vicuna-7B%252C%2520Qwen2-7B%2520and%2520Llama3-8B%2520models%2520for%2520speculative%2520decoding%253B%250Aand%2520additionally%2520provides%2520up%2520to%25201.5-2x%2520speedup.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02659v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniDraft%3A%20A%20Cross-vocabulary%2C%20Online%20Adaptive%20Drafter%20for%20On-device%0A%20%20Speculative%20Decoding&entry.906535625=Ramchalam%20Kinattinkara%20Ramakrishnan%20and%20Zhaocong%20Yuan%20and%20Shaojie%20Zhuo%20and%20Chen%20Feng%20and%20Yicheng%20Lin%20and%20Chenzheng%20Su%20and%20Xiaopeng%20Zhang&entry.1292438233=%20%20Speculative%20decoding%20generally%20dictates%20having%20a%20small%2C%20efficient%20draft%20model%0Athat%20is%20either%20pretrained%20or%20distilled%20offline%20to%20a%20particular%20target%20model%0Aseries%2C%20for%20instance%2C%20Llama%20or%20Qwen%20models.%20However%2C%20within%20online%20deployment%0Asettings%2C%20there%20are%20two%20major%20challenges%3A%201%29%20usage%20of%20a%20target%20model%20that%20is%0Aincompatible%20with%20the%20draft%20model%3B%202%29%20expectation%20of%20latency%20improvements%20over%0Ausage%20and%20time.%20In%20this%20work%2C%20we%20propose%20OmniDraft%2C%20a%20unified%20framework%20that%0Aenables%20a%20single%20draft%20model%20to%20operate%20with%20any%20target%20model%20and%20adapt%0Adynamically%20to%20user%20data.%20We%20introduce%20an%20online%20n-gram%20cache%20with%20hybrid%0Adistillation%20fine-tuning%20to%20address%20the%20cross-vocabulary%20mismatch%20across%20draft%0Aand%20target%20models%3B%20and%20further%20improve%20decoding%20speed%20by%20leveraging%20adaptive%0Adrafting%20techniques.%20OmniDraft%20is%20particularly%20suitable%20for%20on-device%20LLM%0Aapplications%20where%20model%20cost%2C%20efficiency%20and%20user%20customization%20are%20the%20major%0Apoints%20of%20contention.%20This%20further%20highlights%20the%20need%20to%20tackle%20the%20above%0Achallenges%20and%20motivates%20the%20%5Ctextit%7B%60%60one%20drafter%20for%20all%27%27%7D%20paradigm.%20We%0Ashowcase%20the%20proficiency%20of%20the%20OmniDraft%20framework%20by%20performing%20online%0Alearning%20on%20math%20reasoning%2C%20coding%20and%20text%20generation%20tasks.%20Notably%2C%0AOmniDraft%20enables%20a%20single%20Llama-68M%20model%20to%20pair%20with%20various%20target%20models%0Aincluding%20Vicuna-7B%2C%20Qwen2-7B%20and%20Llama3-8B%20models%20for%20speculative%20decoding%3B%0Aand%20additionally%20provides%20up%20to%201.5-2x%20speedup.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02659v3&entry.124074799=Read"},
{"title": "HYPE: Hybrid Planning with Ego Proposal-Conditioned Predictions", "author": "Hang Yu and Julian Jordan and Julian Schmidt and Silvan Lindner and Alessandro Canevaro and Wilhelm Stork", "abstract": "  Safe and interpretable motion planning in complex urban environments needs to\nreason about bidirectional multi-agent interactions. This reasoning requires to\nestimate the costs of potential ego driving maneuvers. Many existing planners\ngenerate initial trajectories with sampling-based methods and refine them by\noptimizing on learned predictions of future environment states, which requires\na cost function that encodes the desired vehicle behavior. Designing such a\ncost function can be very challenging, especially if a wide range of complex\nurban scenarios has to be considered. We propose HYPE: HYbrid Planning with Ego\nproposal-conditioned predictions, a planner that integrates multimodal\ntrajectory proposals from a learned proposal model as heuristic priors into a\nMonte Carlo Tree Search (MCTS) refinement. To model bidirectional interactions,\nwe introduce an ego-conditioned occupancy prediction model, enabling\nconsistent, scene-aware reasoning. Our design significantly simplifies cost\nfunction design in refinement by considering proposal-driven guidance,\nrequiring only minimalistic grid-based cost terms. Evaluations on large-scale\nreal-world benchmarks nuPlan and DeepUrban show that HYPE effectively achieves\nstate-of-the-art performance, especially in safety and adaptability.\n", "link": "http://arxiv.org/abs/2510.12733v1", "date": "2025-10-14", "relevancy": 2.1379, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5936}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.524}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HYPE%3A%20Hybrid%20Planning%20with%20Ego%20Proposal-Conditioned%20Predictions&body=Title%3A%20HYPE%3A%20Hybrid%20Planning%20with%20Ego%20Proposal-Conditioned%20Predictions%0AAuthor%3A%20Hang%20Yu%20and%20Julian%20Jordan%20and%20Julian%20Schmidt%20and%20Silvan%20Lindner%20and%20Alessandro%20Canevaro%20and%20Wilhelm%20Stork%0AAbstract%3A%20%20%20Safe%20and%20interpretable%20motion%20planning%20in%20complex%20urban%20environments%20needs%20to%0Areason%20about%20bidirectional%20multi-agent%20interactions.%20This%20reasoning%20requires%20to%0Aestimate%20the%20costs%20of%20potential%20ego%20driving%20maneuvers.%20Many%20existing%20planners%0Agenerate%20initial%20trajectories%20with%20sampling-based%20methods%20and%20refine%20them%20by%0Aoptimizing%20on%20learned%20predictions%20of%20future%20environment%20states%2C%20which%20requires%0Aa%20cost%20function%20that%20encodes%20the%20desired%20vehicle%20behavior.%20Designing%20such%20a%0Acost%20function%20can%20be%20very%20challenging%2C%20especially%20if%20a%20wide%20range%20of%20complex%0Aurban%20scenarios%20has%20to%20be%20considered.%20We%20propose%20HYPE%3A%20HYbrid%20Planning%20with%20Ego%0Aproposal-conditioned%20predictions%2C%20a%20planner%20that%20integrates%20multimodal%0Atrajectory%20proposals%20from%20a%20learned%20proposal%20model%20as%20heuristic%20priors%20into%20a%0AMonte%20Carlo%20Tree%20Search%20%28MCTS%29%20refinement.%20To%20model%20bidirectional%20interactions%2C%0Awe%20introduce%20an%20ego-conditioned%20occupancy%20prediction%20model%2C%20enabling%0Aconsistent%2C%20scene-aware%20reasoning.%20Our%20design%20significantly%20simplifies%20cost%0Afunction%20design%20in%20refinement%20by%20considering%20proposal-driven%20guidance%2C%0Arequiring%20only%20minimalistic%20grid-based%20cost%20terms.%20Evaluations%20on%20large-scale%0Areal-world%20benchmarks%20nuPlan%20and%20DeepUrban%20show%20that%20HYPE%20effectively%20achieves%0Astate-of-the-art%20performance%2C%20especially%20in%20safety%20and%20adaptability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHYPE%253A%2520Hybrid%2520Planning%2520with%2520Ego%2520Proposal-Conditioned%2520Predictions%26entry.906535625%3DHang%2520Yu%2520and%2520Julian%2520Jordan%2520and%2520Julian%2520Schmidt%2520and%2520Silvan%2520Lindner%2520and%2520Alessandro%2520Canevaro%2520and%2520Wilhelm%2520Stork%26entry.1292438233%3D%2520%2520Safe%2520and%2520interpretable%2520motion%2520planning%2520in%2520complex%2520urban%2520environments%2520needs%2520to%250Areason%2520about%2520bidirectional%2520multi-agent%2520interactions.%2520This%2520reasoning%2520requires%2520to%250Aestimate%2520the%2520costs%2520of%2520potential%2520ego%2520driving%2520maneuvers.%2520Many%2520existing%2520planners%250Agenerate%2520initial%2520trajectories%2520with%2520sampling-based%2520methods%2520and%2520refine%2520them%2520by%250Aoptimizing%2520on%2520learned%2520predictions%2520of%2520future%2520environment%2520states%252C%2520which%2520requires%250Aa%2520cost%2520function%2520that%2520encodes%2520the%2520desired%2520vehicle%2520behavior.%2520Designing%2520such%2520a%250Acost%2520function%2520can%2520be%2520very%2520challenging%252C%2520especially%2520if%2520a%2520wide%2520range%2520of%2520complex%250Aurban%2520scenarios%2520has%2520to%2520be%2520considered.%2520We%2520propose%2520HYPE%253A%2520HYbrid%2520Planning%2520with%2520Ego%250Aproposal-conditioned%2520predictions%252C%2520a%2520planner%2520that%2520integrates%2520multimodal%250Atrajectory%2520proposals%2520from%2520a%2520learned%2520proposal%2520model%2520as%2520heuristic%2520priors%2520into%2520a%250AMonte%2520Carlo%2520Tree%2520Search%2520%2528MCTS%2529%2520refinement.%2520To%2520model%2520bidirectional%2520interactions%252C%250Awe%2520introduce%2520an%2520ego-conditioned%2520occupancy%2520prediction%2520model%252C%2520enabling%250Aconsistent%252C%2520scene-aware%2520reasoning.%2520Our%2520design%2520significantly%2520simplifies%2520cost%250Afunction%2520design%2520in%2520refinement%2520by%2520considering%2520proposal-driven%2520guidance%252C%250Arequiring%2520only%2520minimalistic%2520grid-based%2520cost%2520terms.%2520Evaluations%2520on%2520large-scale%250Areal-world%2520benchmarks%2520nuPlan%2520and%2520DeepUrban%2520show%2520that%2520HYPE%2520effectively%2520achieves%250Astate-of-the-art%2520performance%252C%2520especially%2520in%2520safety%2520and%2520adaptability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HYPE%3A%20Hybrid%20Planning%20with%20Ego%20Proposal-Conditioned%20Predictions&entry.906535625=Hang%20Yu%20and%20Julian%20Jordan%20and%20Julian%20Schmidt%20and%20Silvan%20Lindner%20and%20Alessandro%20Canevaro%20and%20Wilhelm%20Stork&entry.1292438233=%20%20Safe%20and%20interpretable%20motion%20planning%20in%20complex%20urban%20environments%20needs%20to%0Areason%20about%20bidirectional%20multi-agent%20interactions.%20This%20reasoning%20requires%20to%0Aestimate%20the%20costs%20of%20potential%20ego%20driving%20maneuvers.%20Many%20existing%20planners%0Agenerate%20initial%20trajectories%20with%20sampling-based%20methods%20and%20refine%20them%20by%0Aoptimizing%20on%20learned%20predictions%20of%20future%20environment%20states%2C%20which%20requires%0Aa%20cost%20function%20that%20encodes%20the%20desired%20vehicle%20behavior.%20Designing%20such%20a%0Acost%20function%20can%20be%20very%20challenging%2C%20especially%20if%20a%20wide%20range%20of%20complex%0Aurban%20scenarios%20has%20to%20be%20considered.%20We%20propose%20HYPE%3A%20HYbrid%20Planning%20with%20Ego%0Aproposal-conditioned%20predictions%2C%20a%20planner%20that%20integrates%20multimodal%0Atrajectory%20proposals%20from%20a%20learned%20proposal%20model%20as%20heuristic%20priors%20into%20a%0AMonte%20Carlo%20Tree%20Search%20%28MCTS%29%20refinement.%20To%20model%20bidirectional%20interactions%2C%0Awe%20introduce%20an%20ego-conditioned%20occupancy%20prediction%20model%2C%20enabling%0Aconsistent%2C%20scene-aware%20reasoning.%20Our%20design%20significantly%20simplifies%20cost%0Afunction%20design%20in%20refinement%20by%20considering%20proposal-driven%20guidance%2C%0Arequiring%20only%20minimalistic%20grid-based%20cost%20terms.%20Evaluations%20on%20large-scale%0Areal-world%20benchmarks%20nuPlan%20and%20DeepUrban%20show%20that%20HYPE%20effectively%20achieves%0Astate-of-the-art%20performance%2C%20especially%20in%20safety%20and%20adaptability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12733v1&entry.124074799=Read"},
{"title": "Residual MPC: Blending Reinforcement Learning with GPU-Parallelized\n  Model Predictive Control", "author": "Se Hwan Jeon and Ho Jae Lee and Seungwoo Hong and Sangbae Kim", "abstract": "  Model Predictive Control (MPC) provides interpretable, tunable locomotion\ncontrollers grounded in physical models, but its robustness depends on frequent\nreplanning and is limited by model mismatch and real-time computational\nconstraints. Reinforcement Learning (RL), by contrast, can produce highly\nrobust behaviors through stochastic training but often lacks interpretability,\nsuffers from out-of-distribution failures, and requires intensive reward\nengineering. This work presents a GPU-parallelized residual architecture that\ntightly integrates MPC and RL by blending their outputs at the torque-control\nlevel. We develop a kinodynamic whole-body MPC formulation evaluated across\nthousands of agents in parallel at 100 Hz for RL training. The residual policy\nlearns to make targeted corrections to the MPC outputs, combining the\ninterpretability and constraint handling of model-based control with the\nadaptability of RL. The model-based control prior acts as a strong bias,\ninitializing and guiding the policy towards desirable behavior with a simple\nset of rewards. Compared to standalone MPC or end-to-end RL, our approach\nachieves higher sample efficiency, converges to greater asymptotic rewards,\nexpands the range of trackable velocity commands, and enables zero-shot\nadaptation to unseen gaits and uneven terrain.\n", "link": "http://arxiv.org/abs/2510.12717v1", "date": "2025-10-14", "relevancy": 2.1367, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5922}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5255}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Residual%20MPC%3A%20Blending%20Reinforcement%20Learning%20with%20GPU-Parallelized%0A%20%20Model%20Predictive%20Control&body=Title%3A%20Residual%20MPC%3A%20Blending%20Reinforcement%20Learning%20with%20GPU-Parallelized%0A%20%20Model%20Predictive%20Control%0AAuthor%3A%20Se%20Hwan%20Jeon%20and%20Ho%20Jae%20Lee%20and%20Seungwoo%20Hong%20and%20Sangbae%20Kim%0AAbstract%3A%20%20%20Model%20Predictive%20Control%20%28MPC%29%20provides%20interpretable%2C%20tunable%20locomotion%0Acontrollers%20grounded%20in%20physical%20models%2C%20but%20its%20robustness%20depends%20on%20frequent%0Areplanning%20and%20is%20limited%20by%20model%20mismatch%20and%20real-time%20computational%0Aconstraints.%20Reinforcement%20Learning%20%28RL%29%2C%20by%20contrast%2C%20can%20produce%20highly%0Arobust%20behaviors%20through%20stochastic%20training%20but%20often%20lacks%20interpretability%2C%0Asuffers%20from%20out-of-distribution%20failures%2C%20and%20requires%20intensive%20reward%0Aengineering.%20This%20work%20presents%20a%20GPU-parallelized%20residual%20architecture%20that%0Atightly%20integrates%20MPC%20and%20RL%20by%20blending%20their%20outputs%20at%20the%20torque-control%0Alevel.%20We%20develop%20a%20kinodynamic%20whole-body%20MPC%20formulation%20evaluated%20across%0Athousands%20of%20agents%20in%20parallel%20at%20100%20Hz%20for%20RL%20training.%20The%20residual%20policy%0Alearns%20to%20make%20targeted%20corrections%20to%20the%20MPC%20outputs%2C%20combining%20the%0Ainterpretability%20and%20constraint%20handling%20of%20model-based%20control%20with%20the%0Aadaptability%20of%20RL.%20The%20model-based%20control%20prior%20acts%20as%20a%20strong%20bias%2C%0Ainitializing%20and%20guiding%20the%20policy%20towards%20desirable%20behavior%20with%20a%20simple%0Aset%20of%20rewards.%20Compared%20to%20standalone%20MPC%20or%20end-to-end%20RL%2C%20our%20approach%0Aachieves%20higher%20sample%20efficiency%2C%20converges%20to%20greater%20asymptotic%20rewards%2C%0Aexpands%20the%20range%20of%20trackable%20velocity%20commands%2C%20and%20enables%20zero-shot%0Aadaptation%20to%20unseen%20gaits%20and%20uneven%20terrain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResidual%2520MPC%253A%2520Blending%2520Reinforcement%2520Learning%2520with%2520GPU-Parallelized%250A%2520%2520Model%2520Predictive%2520Control%26entry.906535625%3DSe%2520Hwan%2520Jeon%2520and%2520Ho%2520Jae%2520Lee%2520and%2520Seungwoo%2520Hong%2520and%2520Sangbae%2520Kim%26entry.1292438233%3D%2520%2520Model%2520Predictive%2520Control%2520%2528MPC%2529%2520provides%2520interpretable%252C%2520tunable%2520locomotion%250Acontrollers%2520grounded%2520in%2520physical%2520models%252C%2520but%2520its%2520robustness%2520depends%2520on%2520frequent%250Areplanning%2520and%2520is%2520limited%2520by%2520model%2520mismatch%2520and%2520real-time%2520computational%250Aconstraints.%2520Reinforcement%2520Learning%2520%2528RL%2529%252C%2520by%2520contrast%252C%2520can%2520produce%2520highly%250Arobust%2520behaviors%2520through%2520stochastic%2520training%2520but%2520often%2520lacks%2520interpretability%252C%250Asuffers%2520from%2520out-of-distribution%2520failures%252C%2520and%2520requires%2520intensive%2520reward%250Aengineering.%2520This%2520work%2520presents%2520a%2520GPU-parallelized%2520residual%2520architecture%2520that%250Atightly%2520integrates%2520MPC%2520and%2520RL%2520by%2520blending%2520their%2520outputs%2520at%2520the%2520torque-control%250Alevel.%2520We%2520develop%2520a%2520kinodynamic%2520whole-body%2520MPC%2520formulation%2520evaluated%2520across%250Athousands%2520of%2520agents%2520in%2520parallel%2520at%2520100%2520Hz%2520for%2520RL%2520training.%2520The%2520residual%2520policy%250Alearns%2520to%2520make%2520targeted%2520corrections%2520to%2520the%2520MPC%2520outputs%252C%2520combining%2520the%250Ainterpretability%2520and%2520constraint%2520handling%2520of%2520model-based%2520control%2520with%2520the%250Aadaptability%2520of%2520RL.%2520The%2520model-based%2520control%2520prior%2520acts%2520as%2520a%2520strong%2520bias%252C%250Ainitializing%2520and%2520guiding%2520the%2520policy%2520towards%2520desirable%2520behavior%2520with%2520a%2520simple%250Aset%2520of%2520rewards.%2520Compared%2520to%2520standalone%2520MPC%2520or%2520end-to-end%2520RL%252C%2520our%2520approach%250Aachieves%2520higher%2520sample%2520efficiency%252C%2520converges%2520to%2520greater%2520asymptotic%2520rewards%252C%250Aexpands%2520the%2520range%2520of%2520trackable%2520velocity%2520commands%252C%2520and%2520enables%2520zero-shot%250Aadaptation%2520to%2520unseen%2520gaits%2520and%2520uneven%2520terrain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Residual%20MPC%3A%20Blending%20Reinforcement%20Learning%20with%20GPU-Parallelized%0A%20%20Model%20Predictive%20Control&entry.906535625=Se%20Hwan%20Jeon%20and%20Ho%20Jae%20Lee%20and%20Seungwoo%20Hong%20and%20Sangbae%20Kim&entry.1292438233=%20%20Model%20Predictive%20Control%20%28MPC%29%20provides%20interpretable%2C%20tunable%20locomotion%0Acontrollers%20grounded%20in%20physical%20models%2C%20but%20its%20robustness%20depends%20on%20frequent%0Areplanning%20and%20is%20limited%20by%20model%20mismatch%20and%20real-time%20computational%0Aconstraints.%20Reinforcement%20Learning%20%28RL%29%2C%20by%20contrast%2C%20can%20produce%20highly%0Arobust%20behaviors%20through%20stochastic%20training%20but%20often%20lacks%20interpretability%2C%0Asuffers%20from%20out-of-distribution%20failures%2C%20and%20requires%20intensive%20reward%0Aengineering.%20This%20work%20presents%20a%20GPU-parallelized%20residual%20architecture%20that%0Atightly%20integrates%20MPC%20and%20RL%20by%20blending%20their%20outputs%20at%20the%20torque-control%0Alevel.%20We%20develop%20a%20kinodynamic%20whole-body%20MPC%20formulation%20evaluated%20across%0Athousands%20of%20agents%20in%20parallel%20at%20100%20Hz%20for%20RL%20training.%20The%20residual%20policy%0Alearns%20to%20make%20targeted%20corrections%20to%20the%20MPC%20outputs%2C%20combining%20the%0Ainterpretability%20and%20constraint%20handling%20of%20model-based%20control%20with%20the%0Aadaptability%20of%20RL.%20The%20model-based%20control%20prior%20acts%20as%20a%20strong%20bias%2C%0Ainitializing%20and%20guiding%20the%20policy%20towards%20desirable%20behavior%20with%20a%20simple%0Aset%20of%20rewards.%20Compared%20to%20standalone%20MPC%20or%20end-to-end%20RL%2C%20our%20approach%0Aachieves%20higher%20sample%20efficiency%2C%20converges%20to%20greater%20asymptotic%20rewards%2C%0Aexpands%20the%20range%20of%20trackable%20velocity%20commands%2C%20and%20enables%20zero-shot%0Aadaptation%20to%20unseen%20gaits%20and%20uneven%20terrain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12717v1&entry.124074799=Read"},
{"title": "Two-stream network-driven vision-based tactile sensor for object feature\n  extraction and fusion perception", "author": "Muxing Huang and Zibin Chen and Weiliang Xu and Zilan Li and Yuanzhi Zhou and Guoyuan Zhou and Wenjing Chen and Xinming Li", "abstract": "  Tactile perception is crucial for embodied intelligent robots to recognize\nobjects. Vision-based tactile sensors extract object physical attributes\nmultidimensionally using high spatial resolution; however, this process\ngenerates abundant redundant information. Furthermore, single-dimensional\nextraction, lacking effective fusion, fails to fully characterize object\nattributes. These challenges hinder the improvement of recognition accuracy. To\naddress this issue, this study introduces a two-stream network feature\nextraction and fusion perception strategy for vision-based tactile systems.\nThis strategy employs a distributed approach to extract internal and external\nobject features. It obtains depth map information through three-dimensional\nreconstruction while simultaneously acquiring hardness information by measuring\ncontact force data. After extracting features with a convolutional neural\nnetwork (CNN), weighted fusion is applied to create a more informative and\neffective feature representation. In standard tests on objects of varying\nshapes and hardness, the force prediction error is 0.06 N (within a 12 N\nrange). Hardness recognition accuracy reaches 98.0%, and shape recognition\naccuracy reaches 93.75%. With fusion algorithms, object recognition accuracy in\nactual grasping scenarios exceeds 98.5%. Focused on object physical attributes\nperception, this method enhances the artificial tactile system ability to\ntransition from perception to cognition, enabling its use in embodied\nperception applications.\n", "link": "http://arxiv.org/abs/2510.12528v1", "date": "2025-10-14", "relevancy": 2.13, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5789}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5261}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two-stream%20network-driven%20vision-based%20tactile%20sensor%20for%20object%20feature%0A%20%20extraction%20and%20fusion%20perception&body=Title%3A%20Two-stream%20network-driven%20vision-based%20tactile%20sensor%20for%20object%20feature%0A%20%20extraction%20and%20fusion%20perception%0AAuthor%3A%20Muxing%20Huang%20and%20Zibin%20Chen%20and%20Weiliang%20Xu%20and%20Zilan%20Li%20and%20Yuanzhi%20Zhou%20and%20Guoyuan%20Zhou%20and%20Wenjing%20Chen%20and%20Xinming%20Li%0AAbstract%3A%20%20%20Tactile%20perception%20is%20crucial%20for%20embodied%20intelligent%20robots%20to%20recognize%0Aobjects.%20Vision-based%20tactile%20sensors%20extract%20object%20physical%20attributes%0Amultidimensionally%20using%20high%20spatial%20resolution%3B%20however%2C%20this%20process%0Agenerates%20abundant%20redundant%20information.%20Furthermore%2C%20single-dimensional%0Aextraction%2C%20lacking%20effective%20fusion%2C%20fails%20to%20fully%20characterize%20object%0Aattributes.%20These%20challenges%20hinder%20the%20improvement%20of%20recognition%20accuracy.%20To%0Aaddress%20this%20issue%2C%20this%20study%20introduces%20a%20two-stream%20network%20feature%0Aextraction%20and%20fusion%20perception%20strategy%20for%20vision-based%20tactile%20systems.%0AThis%20strategy%20employs%20a%20distributed%20approach%20to%20extract%20internal%20and%20external%0Aobject%20features.%20It%20obtains%20depth%20map%20information%20through%20three-dimensional%0Areconstruction%20while%20simultaneously%20acquiring%20hardness%20information%20by%20measuring%0Acontact%20force%20data.%20After%20extracting%20features%20with%20a%20convolutional%20neural%0Anetwork%20%28CNN%29%2C%20weighted%20fusion%20is%20applied%20to%20create%20a%20more%20informative%20and%0Aeffective%20feature%20representation.%20In%20standard%20tests%20on%20objects%20of%20varying%0Ashapes%20and%20hardness%2C%20the%20force%20prediction%20error%20is%200.06%20N%20%28within%20a%2012%20N%0Arange%29.%20Hardness%20recognition%20accuracy%20reaches%2098.0%25%2C%20and%20shape%20recognition%0Aaccuracy%20reaches%2093.75%25.%20With%20fusion%20algorithms%2C%20object%20recognition%20accuracy%20in%0Aactual%20grasping%20scenarios%20exceeds%2098.5%25.%20Focused%20on%20object%20physical%20attributes%0Aperception%2C%20this%20method%20enhances%20the%20artificial%20tactile%20system%20ability%20to%0Atransition%20from%20perception%20to%20cognition%2C%20enabling%20its%20use%20in%20embodied%0Aperception%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12528v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo-stream%2520network-driven%2520vision-based%2520tactile%2520sensor%2520for%2520object%2520feature%250A%2520%2520extraction%2520and%2520fusion%2520perception%26entry.906535625%3DMuxing%2520Huang%2520and%2520Zibin%2520Chen%2520and%2520Weiliang%2520Xu%2520and%2520Zilan%2520Li%2520and%2520Yuanzhi%2520Zhou%2520and%2520Guoyuan%2520Zhou%2520and%2520Wenjing%2520Chen%2520and%2520Xinming%2520Li%26entry.1292438233%3D%2520%2520Tactile%2520perception%2520is%2520crucial%2520for%2520embodied%2520intelligent%2520robots%2520to%2520recognize%250Aobjects.%2520Vision-based%2520tactile%2520sensors%2520extract%2520object%2520physical%2520attributes%250Amultidimensionally%2520using%2520high%2520spatial%2520resolution%253B%2520however%252C%2520this%2520process%250Agenerates%2520abundant%2520redundant%2520information.%2520Furthermore%252C%2520single-dimensional%250Aextraction%252C%2520lacking%2520effective%2520fusion%252C%2520fails%2520to%2520fully%2520characterize%2520object%250Aattributes.%2520These%2520challenges%2520hinder%2520the%2520improvement%2520of%2520recognition%2520accuracy.%2520To%250Aaddress%2520this%2520issue%252C%2520this%2520study%2520introduces%2520a%2520two-stream%2520network%2520feature%250Aextraction%2520and%2520fusion%2520perception%2520strategy%2520for%2520vision-based%2520tactile%2520systems.%250AThis%2520strategy%2520employs%2520a%2520distributed%2520approach%2520to%2520extract%2520internal%2520and%2520external%250Aobject%2520features.%2520It%2520obtains%2520depth%2520map%2520information%2520through%2520three-dimensional%250Areconstruction%2520while%2520simultaneously%2520acquiring%2520hardness%2520information%2520by%2520measuring%250Acontact%2520force%2520data.%2520After%2520extracting%2520features%2520with%2520a%2520convolutional%2520neural%250Anetwork%2520%2528CNN%2529%252C%2520weighted%2520fusion%2520is%2520applied%2520to%2520create%2520a%2520more%2520informative%2520and%250Aeffective%2520feature%2520representation.%2520In%2520standard%2520tests%2520on%2520objects%2520of%2520varying%250Ashapes%2520and%2520hardness%252C%2520the%2520force%2520prediction%2520error%2520is%25200.06%2520N%2520%2528within%2520a%252012%2520N%250Arange%2529.%2520Hardness%2520recognition%2520accuracy%2520reaches%252098.0%2525%252C%2520and%2520shape%2520recognition%250Aaccuracy%2520reaches%252093.75%2525.%2520With%2520fusion%2520algorithms%252C%2520object%2520recognition%2520accuracy%2520in%250Aactual%2520grasping%2520scenarios%2520exceeds%252098.5%2525.%2520Focused%2520on%2520object%2520physical%2520attributes%250Aperception%252C%2520this%2520method%2520enhances%2520the%2520artificial%2520tactile%2520system%2520ability%2520to%250Atransition%2520from%2520perception%2520to%2520cognition%252C%2520enabling%2520its%2520use%2520in%2520embodied%250Aperception%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12528v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two-stream%20network-driven%20vision-based%20tactile%20sensor%20for%20object%20feature%0A%20%20extraction%20and%20fusion%20perception&entry.906535625=Muxing%20Huang%20and%20Zibin%20Chen%20and%20Weiliang%20Xu%20and%20Zilan%20Li%20and%20Yuanzhi%20Zhou%20and%20Guoyuan%20Zhou%20and%20Wenjing%20Chen%20and%20Xinming%20Li&entry.1292438233=%20%20Tactile%20perception%20is%20crucial%20for%20embodied%20intelligent%20robots%20to%20recognize%0Aobjects.%20Vision-based%20tactile%20sensors%20extract%20object%20physical%20attributes%0Amultidimensionally%20using%20high%20spatial%20resolution%3B%20however%2C%20this%20process%0Agenerates%20abundant%20redundant%20information.%20Furthermore%2C%20single-dimensional%0Aextraction%2C%20lacking%20effective%20fusion%2C%20fails%20to%20fully%20characterize%20object%0Aattributes.%20These%20challenges%20hinder%20the%20improvement%20of%20recognition%20accuracy.%20To%0Aaddress%20this%20issue%2C%20this%20study%20introduces%20a%20two-stream%20network%20feature%0Aextraction%20and%20fusion%20perception%20strategy%20for%20vision-based%20tactile%20systems.%0AThis%20strategy%20employs%20a%20distributed%20approach%20to%20extract%20internal%20and%20external%0Aobject%20features.%20It%20obtains%20depth%20map%20information%20through%20three-dimensional%0Areconstruction%20while%20simultaneously%20acquiring%20hardness%20information%20by%20measuring%0Acontact%20force%20data.%20After%20extracting%20features%20with%20a%20convolutional%20neural%0Anetwork%20%28CNN%29%2C%20weighted%20fusion%20is%20applied%20to%20create%20a%20more%20informative%20and%0Aeffective%20feature%20representation.%20In%20standard%20tests%20on%20objects%20of%20varying%0Ashapes%20and%20hardness%2C%20the%20force%20prediction%20error%20is%200.06%20N%20%28within%20a%2012%20N%0Arange%29.%20Hardness%20recognition%20accuracy%20reaches%2098.0%25%2C%20and%20shape%20recognition%0Aaccuracy%20reaches%2093.75%25.%20With%20fusion%20algorithms%2C%20object%20recognition%20accuracy%20in%0Aactual%20grasping%20scenarios%20exceeds%2098.5%25.%20Focused%20on%20object%20physical%20attributes%0Aperception%2C%20this%20method%20enhances%20the%20artificial%20tactile%20system%20ability%20to%0Atransition%20from%20perception%20to%20cognition%2C%20enabling%20its%20use%20in%20embodied%0Aperception%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12528v1&entry.124074799=Read"},
{"title": "Hybrid Explanation-Guided Learning for Transformer-Based Chest X-Ray\n  Diagnosis", "author": "Shelley Zixin Shu and Haozhe Luo and Alexander Poellinger and Mauricio Reyes", "abstract": "  Transformer-based deep learning models have demonstrated exceptional\nperformance in medical imaging by leveraging attention mechanisms for feature\nrepresentation and interpretability. However, these models are prone to\nlearning spurious correlations, leading to biases and limited generalization.\nWhile human-AI attention alignment can mitigate these issues, it often depends\non costly manual supervision. In this work, we propose a Hybrid\nExplanation-Guided Learning (H-EGL) framework that combines self-supervised and\nhuman-guided constraints to enhance attention alignment and improve\ngeneralization. The self-supervised component of H-EGL leverages\nclass-distinctive attention without relying on restrictive priors, promoting\nrobustness and flexibility. We validate our approach on chest X-ray\nclassification using the Vision Transformer (ViT), where H-EGL outperforms two\nstate-of-the-art Explanation-Guided Learning (EGL) methods, demonstrating\nsuperior classification accuracy and generalization capability. Additionally,\nit produces attention maps that are better aligned with human expertise.\n", "link": "http://arxiv.org/abs/2510.12704v1", "date": "2025-10-14", "relevancy": 2.127, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.554}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.528}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Explanation-Guided%20Learning%20for%20Transformer-Based%20Chest%20X-Ray%0A%20%20Diagnosis&body=Title%3A%20Hybrid%20Explanation-Guided%20Learning%20for%20Transformer-Based%20Chest%20X-Ray%0A%20%20Diagnosis%0AAuthor%3A%20Shelley%20Zixin%20Shu%20and%20Haozhe%20Luo%20and%20Alexander%20Poellinger%20and%20Mauricio%20Reyes%0AAbstract%3A%20%20%20Transformer-based%20deep%20learning%20models%20have%20demonstrated%20exceptional%0Aperformance%20in%20medical%20imaging%20by%20leveraging%20attention%20mechanisms%20for%20feature%0Arepresentation%20and%20interpretability.%20However%2C%20these%20models%20are%20prone%20to%0Alearning%20spurious%20correlations%2C%20leading%20to%20biases%20and%20limited%20generalization.%0AWhile%20human-AI%20attention%20alignment%20can%20mitigate%20these%20issues%2C%20it%20often%20depends%0Aon%20costly%20manual%20supervision.%20In%20this%20work%2C%20we%20propose%20a%20Hybrid%0AExplanation-Guided%20Learning%20%28H-EGL%29%20framework%20that%20combines%20self-supervised%20and%0Ahuman-guided%20constraints%20to%20enhance%20attention%20alignment%20and%20improve%0Ageneralization.%20The%20self-supervised%20component%20of%20H-EGL%20leverages%0Aclass-distinctive%20attention%20without%20relying%20on%20restrictive%20priors%2C%20promoting%0Arobustness%20and%20flexibility.%20We%20validate%20our%20approach%20on%20chest%20X-ray%0Aclassification%20using%20the%20Vision%20Transformer%20%28ViT%29%2C%20where%20H-EGL%20outperforms%20two%0Astate-of-the-art%20Explanation-Guided%20Learning%20%28EGL%29%20methods%2C%20demonstrating%0Asuperior%20classification%20accuracy%20and%20generalization%20capability.%20Additionally%2C%0Ait%20produces%20attention%20maps%20that%20are%20better%20aligned%20with%20human%20expertise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Explanation-Guided%2520Learning%2520for%2520Transformer-Based%2520Chest%2520X-Ray%250A%2520%2520Diagnosis%26entry.906535625%3DShelley%2520Zixin%2520Shu%2520and%2520Haozhe%2520Luo%2520and%2520Alexander%2520Poellinger%2520and%2520Mauricio%2520Reyes%26entry.1292438233%3D%2520%2520Transformer-based%2520deep%2520learning%2520models%2520have%2520demonstrated%2520exceptional%250Aperformance%2520in%2520medical%2520imaging%2520by%2520leveraging%2520attention%2520mechanisms%2520for%2520feature%250Arepresentation%2520and%2520interpretability.%2520However%252C%2520these%2520models%2520are%2520prone%2520to%250Alearning%2520spurious%2520correlations%252C%2520leading%2520to%2520biases%2520and%2520limited%2520generalization.%250AWhile%2520human-AI%2520attention%2520alignment%2520can%2520mitigate%2520these%2520issues%252C%2520it%2520often%2520depends%250Aon%2520costly%2520manual%2520supervision.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520Hybrid%250AExplanation-Guided%2520Learning%2520%2528H-EGL%2529%2520framework%2520that%2520combines%2520self-supervised%2520and%250Ahuman-guided%2520constraints%2520to%2520enhance%2520attention%2520alignment%2520and%2520improve%250Ageneralization.%2520The%2520self-supervised%2520component%2520of%2520H-EGL%2520leverages%250Aclass-distinctive%2520attention%2520without%2520relying%2520on%2520restrictive%2520priors%252C%2520promoting%250Arobustness%2520and%2520flexibility.%2520We%2520validate%2520our%2520approach%2520on%2520chest%2520X-ray%250Aclassification%2520using%2520the%2520Vision%2520Transformer%2520%2528ViT%2529%252C%2520where%2520H-EGL%2520outperforms%2520two%250Astate-of-the-art%2520Explanation-Guided%2520Learning%2520%2528EGL%2529%2520methods%252C%2520demonstrating%250Asuperior%2520classification%2520accuracy%2520and%2520generalization%2520capability.%2520Additionally%252C%250Ait%2520produces%2520attention%2520maps%2520that%2520are%2520better%2520aligned%2520with%2520human%2520expertise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Explanation-Guided%20Learning%20for%20Transformer-Based%20Chest%20X-Ray%0A%20%20Diagnosis&entry.906535625=Shelley%20Zixin%20Shu%20and%20Haozhe%20Luo%20and%20Alexander%20Poellinger%20and%20Mauricio%20Reyes&entry.1292438233=%20%20Transformer-based%20deep%20learning%20models%20have%20demonstrated%20exceptional%0Aperformance%20in%20medical%20imaging%20by%20leveraging%20attention%20mechanisms%20for%20feature%0Arepresentation%20and%20interpretability.%20However%2C%20these%20models%20are%20prone%20to%0Alearning%20spurious%20correlations%2C%20leading%20to%20biases%20and%20limited%20generalization.%0AWhile%20human-AI%20attention%20alignment%20can%20mitigate%20these%20issues%2C%20it%20often%20depends%0Aon%20costly%20manual%20supervision.%20In%20this%20work%2C%20we%20propose%20a%20Hybrid%0AExplanation-Guided%20Learning%20%28H-EGL%29%20framework%20that%20combines%20self-supervised%20and%0Ahuman-guided%20constraints%20to%20enhance%20attention%20alignment%20and%20improve%0Ageneralization.%20The%20self-supervised%20component%20of%20H-EGL%20leverages%0Aclass-distinctive%20attention%20without%20relying%20on%20restrictive%20priors%2C%20promoting%0Arobustness%20and%20flexibility.%20We%20validate%20our%20approach%20on%20chest%20X-ray%0Aclassification%20using%20the%20Vision%20Transformer%20%28ViT%29%2C%20where%20H-EGL%20outperforms%20two%0Astate-of-the-art%20Explanation-Guided%20Learning%20%28EGL%29%20methods%2C%20demonstrating%0Asuperior%20classification%20accuracy%20and%20generalization%20capability.%20Additionally%2C%0Ait%20produces%20attention%20maps%20that%20are%20better%20aligned%20with%20human%20expertise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12704v1&entry.124074799=Read"},
{"title": "CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in\n  Latent World Models for Autonomous Driving", "author": "Xiaoji Zheng and Ziyuan Yang and Yanhao Chen and Yuhang Peng and Yuanrong Tang and Gengyuan Liu and Bokui Chen and Jiangtao Gong", "abstract": "  End-to-end autonomous driving models trained solely with imitation learning\n(IL) often suffer from poor generalization. In contrast, reinforcement learning\n(RL) promotes exploration through reward maximization but faces challenges such\nas sample inefficiency and unstable convergence. A natural solution is to\ncombine IL and RL. Moving beyond the conventional two-stage paradigm (IL\npretraining followed by RL fine-tuning), we propose CoIRL-AD, a competitive\ndual-policy framework that enables IL and RL agents to interact during\ntraining. CoIRL-AD introduces a competition-based mechanism that facilitates\nknowledge exchange while preventing gradient conflicts. Experiments on the\nnuScenes dataset show an 18% reduction in collision rate compared to baselines,\nalong with stronger generalization and improved performance on long-tail\nscenarios. Code is available at: https://github.com/SEU-zxj/CoIRL-AD.\n", "link": "http://arxiv.org/abs/2510.12560v1", "date": "2025-10-14", "relevancy": 2.1262, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5456}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5327}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoIRL-AD%3A%20Collaborative-Competitive%20Imitation-Reinforcement%20Learning%20in%0A%20%20Latent%20World%20Models%20for%20Autonomous%20Driving&body=Title%3A%20CoIRL-AD%3A%20Collaborative-Competitive%20Imitation-Reinforcement%20Learning%20in%0A%20%20Latent%20World%20Models%20for%20Autonomous%20Driving%0AAuthor%3A%20Xiaoji%20Zheng%20and%20Ziyuan%20Yang%20and%20Yanhao%20Chen%20and%20Yuhang%20Peng%20and%20Yuanrong%20Tang%20and%20Gengyuan%20Liu%20and%20Bokui%20Chen%20and%20Jiangtao%20Gong%0AAbstract%3A%20%20%20End-to-end%20autonomous%20driving%20models%20trained%20solely%20with%20imitation%20learning%0A%28IL%29%20often%20suffer%20from%20poor%20generalization.%20In%20contrast%2C%20reinforcement%20learning%0A%28RL%29%20promotes%20exploration%20through%20reward%20maximization%20but%20faces%20challenges%20such%0Aas%20sample%20inefficiency%20and%20unstable%20convergence.%20A%20natural%20solution%20is%20to%0Acombine%20IL%20and%20RL.%20Moving%20beyond%20the%20conventional%20two-stage%20paradigm%20%28IL%0Apretraining%20followed%20by%20RL%20fine-tuning%29%2C%20we%20propose%20CoIRL-AD%2C%20a%20competitive%0Adual-policy%20framework%20that%20enables%20IL%20and%20RL%20agents%20to%20interact%20during%0Atraining.%20CoIRL-AD%20introduces%20a%20competition-based%20mechanism%20that%20facilitates%0Aknowledge%20exchange%20while%20preventing%20gradient%20conflicts.%20Experiments%20on%20the%0AnuScenes%20dataset%20show%20an%2018%25%20reduction%20in%20collision%20rate%20compared%20to%20baselines%2C%0Aalong%20with%20stronger%20generalization%20and%20improved%20performance%20on%20long-tail%0Ascenarios.%20Code%20is%20available%20at%3A%20https%3A//github.com/SEU-zxj/CoIRL-AD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoIRL-AD%253A%2520Collaborative-Competitive%2520Imitation-Reinforcement%2520Learning%2520in%250A%2520%2520Latent%2520World%2520Models%2520for%2520Autonomous%2520Driving%26entry.906535625%3DXiaoji%2520Zheng%2520and%2520Ziyuan%2520Yang%2520and%2520Yanhao%2520Chen%2520and%2520Yuhang%2520Peng%2520and%2520Yuanrong%2520Tang%2520and%2520Gengyuan%2520Liu%2520and%2520Bokui%2520Chen%2520and%2520Jiangtao%2520Gong%26entry.1292438233%3D%2520%2520End-to-end%2520autonomous%2520driving%2520models%2520trained%2520solely%2520with%2520imitation%2520learning%250A%2528IL%2529%2520often%2520suffer%2520from%2520poor%2520generalization.%2520In%2520contrast%252C%2520reinforcement%2520learning%250A%2528RL%2529%2520promotes%2520exploration%2520through%2520reward%2520maximization%2520but%2520faces%2520challenges%2520such%250Aas%2520sample%2520inefficiency%2520and%2520unstable%2520convergence.%2520A%2520natural%2520solution%2520is%2520to%250Acombine%2520IL%2520and%2520RL.%2520Moving%2520beyond%2520the%2520conventional%2520two-stage%2520paradigm%2520%2528IL%250Apretraining%2520followed%2520by%2520RL%2520fine-tuning%2529%252C%2520we%2520propose%2520CoIRL-AD%252C%2520a%2520competitive%250Adual-policy%2520framework%2520that%2520enables%2520IL%2520and%2520RL%2520agents%2520to%2520interact%2520during%250Atraining.%2520CoIRL-AD%2520introduces%2520a%2520competition-based%2520mechanism%2520that%2520facilitates%250Aknowledge%2520exchange%2520while%2520preventing%2520gradient%2520conflicts.%2520Experiments%2520on%2520the%250AnuScenes%2520dataset%2520show%2520an%252018%2525%2520reduction%2520in%2520collision%2520rate%2520compared%2520to%2520baselines%252C%250Aalong%2520with%2520stronger%2520generalization%2520and%2520improved%2520performance%2520on%2520long-tail%250Ascenarios.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/SEU-zxj/CoIRL-AD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoIRL-AD%3A%20Collaborative-Competitive%20Imitation-Reinforcement%20Learning%20in%0A%20%20Latent%20World%20Models%20for%20Autonomous%20Driving&entry.906535625=Xiaoji%20Zheng%20and%20Ziyuan%20Yang%20and%20Yanhao%20Chen%20and%20Yuhang%20Peng%20and%20Yuanrong%20Tang%20and%20Gengyuan%20Liu%20and%20Bokui%20Chen%20and%20Jiangtao%20Gong&entry.1292438233=%20%20End-to-end%20autonomous%20driving%20models%20trained%20solely%20with%20imitation%20learning%0A%28IL%29%20often%20suffer%20from%20poor%20generalization.%20In%20contrast%2C%20reinforcement%20learning%0A%28RL%29%20promotes%20exploration%20through%20reward%20maximization%20but%20faces%20challenges%20such%0Aas%20sample%20inefficiency%20and%20unstable%20convergence.%20A%20natural%20solution%20is%20to%0Acombine%20IL%20and%20RL.%20Moving%20beyond%20the%20conventional%20two-stage%20paradigm%20%28IL%0Apretraining%20followed%20by%20RL%20fine-tuning%29%2C%20we%20propose%20CoIRL-AD%2C%20a%20competitive%0Adual-policy%20framework%20that%20enables%20IL%20and%20RL%20agents%20to%20interact%20during%0Atraining.%20CoIRL-AD%20introduces%20a%20competition-based%20mechanism%20that%20facilitates%0Aknowledge%20exchange%20while%20preventing%20gradient%20conflicts.%20Experiments%20on%20the%0AnuScenes%20dataset%20show%20an%2018%25%20reduction%20in%20collision%20rate%20compared%20to%20baselines%2C%0Aalong%20with%20stronger%20generalization%20and%20improved%20performance%20on%20long-tail%0Ascenarios.%20Code%20is%20available%20at%3A%20https%3A//github.com/SEU-zxj/CoIRL-AD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12560v1&entry.124074799=Read"},
{"title": "Zero-Shot CFC: Fast Real-World Image Denoising based on Cross-Frequency\n  Consistency", "author": "Yanlin Jiang and Yuchen Liu and Mingren Liu", "abstract": "  Zero-shot denoisers address the dataset dependency of deep-learning-based\ndenoisers, enabling the denoising of unseen single images. Nonetheless,\nexisting zero-shot methods suffer from long training times and rely on the\nassumption of noise independence and a zero-mean property, limiting their\neffectiveness in real-world denoising scenarios where noise characteristics are\nmore complicated. This paper proposes an efficient and effective method for\nreal-world denoising, the Zero-Shot denoiser based on Cross-Frequency\nConsistency (ZSCFC), which enables training and denoising with a single noisy\nimage and does not rely on assumptions about noise distribution. Specifically,\nimage textures exhibit position similarity and content consistency across\ndifferent frequency bands, while noise does not. Based on this property, we\ndeveloped cross-frequency consistency loss and an ultralight network to realize\nimage denoising. Experiments on various real-world image datasets demonstrate\nthat our ZSCFC outperforms other state-of-the-art zero-shot methods in terms of\ncomputational efficiency and denoising performance.\n", "link": "http://arxiv.org/abs/2510.12646v1", "date": "2025-10-14", "relevancy": 2.1179, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.544}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5358}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20CFC%3A%20Fast%20Real-World%20Image%20Denoising%20based%20on%20Cross-Frequency%0A%20%20Consistency&body=Title%3A%20Zero-Shot%20CFC%3A%20Fast%20Real-World%20Image%20Denoising%20based%20on%20Cross-Frequency%0A%20%20Consistency%0AAuthor%3A%20Yanlin%20Jiang%20and%20Yuchen%20Liu%20and%20Mingren%20Liu%0AAbstract%3A%20%20%20Zero-shot%20denoisers%20address%20the%20dataset%20dependency%20of%20deep-learning-based%0Adenoisers%2C%20enabling%20the%20denoising%20of%20unseen%20single%20images.%20Nonetheless%2C%0Aexisting%20zero-shot%20methods%20suffer%20from%20long%20training%20times%20and%20rely%20on%20the%0Aassumption%20of%20noise%20independence%20and%20a%20zero-mean%20property%2C%20limiting%20their%0Aeffectiveness%20in%20real-world%20denoising%20scenarios%20where%20noise%20characteristics%20are%0Amore%20complicated.%20This%20paper%20proposes%20an%20efficient%20and%20effective%20method%20for%0Areal-world%20denoising%2C%20the%20Zero-Shot%20denoiser%20based%20on%20Cross-Frequency%0AConsistency%20%28ZSCFC%29%2C%20which%20enables%20training%20and%20denoising%20with%20a%20single%20noisy%0Aimage%20and%20does%20not%20rely%20on%20assumptions%20about%20noise%20distribution.%20Specifically%2C%0Aimage%20textures%20exhibit%20position%20similarity%20and%20content%20consistency%20across%0Adifferent%20frequency%20bands%2C%20while%20noise%20does%20not.%20Based%20on%20this%20property%2C%20we%0Adeveloped%20cross-frequency%20consistency%20loss%20and%20an%20ultralight%20network%20to%20realize%0Aimage%20denoising.%20Experiments%20on%20various%20real-world%20image%20datasets%20demonstrate%0Athat%20our%20ZSCFC%20outperforms%20other%20state-of-the-art%20zero-shot%20methods%20in%20terms%20of%0Acomputational%20efficiency%20and%20denoising%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520CFC%253A%2520Fast%2520Real-World%2520Image%2520Denoising%2520based%2520on%2520Cross-Frequency%250A%2520%2520Consistency%26entry.906535625%3DYanlin%2520Jiang%2520and%2520Yuchen%2520Liu%2520and%2520Mingren%2520Liu%26entry.1292438233%3D%2520%2520Zero-shot%2520denoisers%2520address%2520the%2520dataset%2520dependency%2520of%2520deep-learning-based%250Adenoisers%252C%2520enabling%2520the%2520denoising%2520of%2520unseen%2520single%2520images.%2520Nonetheless%252C%250Aexisting%2520zero-shot%2520methods%2520suffer%2520from%2520long%2520training%2520times%2520and%2520rely%2520on%2520the%250Aassumption%2520of%2520noise%2520independence%2520and%2520a%2520zero-mean%2520property%252C%2520limiting%2520their%250Aeffectiveness%2520in%2520real-world%2520denoising%2520scenarios%2520where%2520noise%2520characteristics%2520are%250Amore%2520complicated.%2520This%2520paper%2520proposes%2520an%2520efficient%2520and%2520effective%2520method%2520for%250Areal-world%2520denoising%252C%2520the%2520Zero-Shot%2520denoiser%2520based%2520on%2520Cross-Frequency%250AConsistency%2520%2528ZSCFC%2529%252C%2520which%2520enables%2520training%2520and%2520denoising%2520with%2520a%2520single%2520noisy%250Aimage%2520and%2520does%2520not%2520rely%2520on%2520assumptions%2520about%2520noise%2520distribution.%2520Specifically%252C%250Aimage%2520textures%2520exhibit%2520position%2520similarity%2520and%2520content%2520consistency%2520across%250Adifferent%2520frequency%2520bands%252C%2520while%2520noise%2520does%2520not.%2520Based%2520on%2520this%2520property%252C%2520we%250Adeveloped%2520cross-frequency%2520consistency%2520loss%2520and%2520an%2520ultralight%2520network%2520to%2520realize%250Aimage%2520denoising.%2520Experiments%2520on%2520various%2520real-world%2520image%2520datasets%2520demonstrate%250Athat%2520our%2520ZSCFC%2520outperforms%2520other%2520state-of-the-art%2520zero-shot%2520methods%2520in%2520terms%2520of%250Acomputational%2520efficiency%2520and%2520denoising%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20CFC%3A%20Fast%20Real-World%20Image%20Denoising%20based%20on%20Cross-Frequency%0A%20%20Consistency&entry.906535625=Yanlin%20Jiang%20and%20Yuchen%20Liu%20and%20Mingren%20Liu&entry.1292438233=%20%20Zero-shot%20denoisers%20address%20the%20dataset%20dependency%20of%20deep-learning-based%0Adenoisers%2C%20enabling%20the%20denoising%20of%20unseen%20single%20images.%20Nonetheless%2C%0Aexisting%20zero-shot%20methods%20suffer%20from%20long%20training%20times%20and%20rely%20on%20the%0Aassumption%20of%20noise%20independence%20and%20a%20zero-mean%20property%2C%20limiting%20their%0Aeffectiveness%20in%20real-world%20denoising%20scenarios%20where%20noise%20characteristics%20are%0Amore%20complicated.%20This%20paper%20proposes%20an%20efficient%20and%20effective%20method%20for%0Areal-world%20denoising%2C%20the%20Zero-Shot%20denoiser%20based%20on%20Cross-Frequency%0AConsistency%20%28ZSCFC%29%2C%20which%20enables%20training%20and%20denoising%20with%20a%20single%20noisy%0Aimage%20and%20does%20not%20rely%20on%20assumptions%20about%20noise%20distribution.%20Specifically%2C%0Aimage%20textures%20exhibit%20position%20similarity%20and%20content%20consistency%20across%0Adifferent%20frequency%20bands%2C%20while%20noise%20does%20not.%20Based%20on%20this%20property%2C%20we%0Adeveloped%20cross-frequency%20consistency%20loss%20and%20an%20ultralight%20network%20to%20realize%0Aimage%20denoising.%20Experiments%20on%20various%20real-world%20image%20datasets%20demonstrate%0Athat%20our%20ZSCFC%20outperforms%20other%20state-of-the-art%20zero-shot%20methods%20in%20terms%20of%0Acomputational%20efficiency%20and%20denoising%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12646v1&entry.124074799=Read"},
{"title": "ProtoSiTex: Learning Semi-Interpretable Prototypes for Multi-label Text\n  Classification", "author": "Utsav Kumar Nareti and Suraj Kumar and Soumya Pandey and Soumi Chattopadhyay and Chandranath Adak", "abstract": "  The surge in user-generated reviews has amplified the need for interpretable\nmodels that can provide fine-grained insights. Existing prototype-based models\noffer intuitive explanations but typically operate at coarse granularity\n(sentence or document level) and fail to address the multi-label nature of\nreal-world text classification. We propose ProtoSiTex, a semi-interpretable\nframework designed for fine-grained multi-label text classification. ProtoSiTex\nemploys a dual-phase alternating training strategy: an unsupervised prototype\ndiscovery phase that learns semantically coherent and diverse prototypes, and a\nsupervised classification phase that maps these prototypes to class labels. A\nhierarchical loss function enforces consistency across sub-sentence, sentence,\nand document levels, enhancing interpretability and alignment. Unlike prior\napproaches, ProtoSiTex captures overlapping and conflicting semantics using\nadaptive prototypes and multi-head attention. We also introduce a benchmark\ndataset of hotel reviews annotated at the sub-sentence level with multiple\nlabels. Experiments on this dataset and two public benchmarks (binary and\nmulti-class) show that ProtoSiTex achieves state-of-the-art performance while\ndelivering faithful, human-aligned explanations, establishing it as a robust\nsolution for semi-interpretable multi-label text classification.\n", "link": "http://arxiv.org/abs/2510.12534v1", "date": "2025-10-14", "relevancy": 2.1102, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5655}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5131}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProtoSiTex%3A%20Learning%20Semi-Interpretable%20Prototypes%20for%20Multi-label%20Text%0A%20%20Classification&body=Title%3A%20ProtoSiTex%3A%20Learning%20Semi-Interpretable%20Prototypes%20for%20Multi-label%20Text%0A%20%20Classification%0AAuthor%3A%20Utsav%20Kumar%20Nareti%20and%20Suraj%20Kumar%20and%20Soumya%20Pandey%20and%20Soumi%20Chattopadhyay%20and%20Chandranath%20Adak%0AAbstract%3A%20%20%20The%20surge%20in%20user-generated%20reviews%20has%20amplified%20the%20need%20for%20interpretable%0Amodels%20that%20can%20provide%20fine-grained%20insights.%20Existing%20prototype-based%20models%0Aoffer%20intuitive%20explanations%20but%20typically%20operate%20at%20coarse%20granularity%0A%28sentence%20or%20document%20level%29%20and%20fail%20to%20address%20the%20multi-label%20nature%20of%0Areal-world%20text%20classification.%20We%20propose%20ProtoSiTex%2C%20a%20semi-interpretable%0Aframework%20designed%20for%20fine-grained%20multi-label%20text%20classification.%20ProtoSiTex%0Aemploys%20a%20dual-phase%20alternating%20training%20strategy%3A%20an%20unsupervised%20prototype%0Adiscovery%20phase%20that%20learns%20semantically%20coherent%20and%20diverse%20prototypes%2C%20and%20a%0Asupervised%20classification%20phase%20that%20maps%20these%20prototypes%20to%20class%20labels.%20A%0Ahierarchical%20loss%20function%20enforces%20consistency%20across%20sub-sentence%2C%20sentence%2C%0Aand%20document%20levels%2C%20enhancing%20interpretability%20and%20alignment.%20Unlike%20prior%0Aapproaches%2C%20ProtoSiTex%20captures%20overlapping%20and%20conflicting%20semantics%20using%0Aadaptive%20prototypes%20and%20multi-head%20attention.%20We%20also%20introduce%20a%20benchmark%0Adataset%20of%20hotel%20reviews%20annotated%20at%20the%20sub-sentence%20level%20with%20multiple%0Alabels.%20Experiments%20on%20this%20dataset%20and%20two%20public%20benchmarks%20%28binary%20and%0Amulti-class%29%20show%20that%20ProtoSiTex%20achieves%20state-of-the-art%20performance%20while%0Adelivering%20faithful%2C%20human-aligned%20explanations%2C%20establishing%20it%20as%20a%20robust%0Asolution%20for%20semi-interpretable%20multi-label%20text%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12534v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProtoSiTex%253A%2520Learning%2520Semi-Interpretable%2520Prototypes%2520for%2520Multi-label%2520Text%250A%2520%2520Classification%26entry.906535625%3DUtsav%2520Kumar%2520Nareti%2520and%2520Suraj%2520Kumar%2520and%2520Soumya%2520Pandey%2520and%2520Soumi%2520Chattopadhyay%2520and%2520Chandranath%2520Adak%26entry.1292438233%3D%2520%2520The%2520surge%2520in%2520user-generated%2520reviews%2520has%2520amplified%2520the%2520need%2520for%2520interpretable%250Amodels%2520that%2520can%2520provide%2520fine-grained%2520insights.%2520Existing%2520prototype-based%2520models%250Aoffer%2520intuitive%2520explanations%2520but%2520typically%2520operate%2520at%2520coarse%2520granularity%250A%2528sentence%2520or%2520document%2520level%2529%2520and%2520fail%2520to%2520address%2520the%2520multi-label%2520nature%2520of%250Areal-world%2520text%2520classification.%2520We%2520propose%2520ProtoSiTex%252C%2520a%2520semi-interpretable%250Aframework%2520designed%2520for%2520fine-grained%2520multi-label%2520text%2520classification.%2520ProtoSiTex%250Aemploys%2520a%2520dual-phase%2520alternating%2520training%2520strategy%253A%2520an%2520unsupervised%2520prototype%250Adiscovery%2520phase%2520that%2520learns%2520semantically%2520coherent%2520and%2520diverse%2520prototypes%252C%2520and%2520a%250Asupervised%2520classification%2520phase%2520that%2520maps%2520these%2520prototypes%2520to%2520class%2520labels.%2520A%250Ahierarchical%2520loss%2520function%2520enforces%2520consistency%2520across%2520sub-sentence%252C%2520sentence%252C%250Aand%2520document%2520levels%252C%2520enhancing%2520interpretability%2520and%2520alignment.%2520Unlike%2520prior%250Aapproaches%252C%2520ProtoSiTex%2520captures%2520overlapping%2520and%2520conflicting%2520semantics%2520using%250Aadaptive%2520prototypes%2520and%2520multi-head%2520attention.%2520We%2520also%2520introduce%2520a%2520benchmark%250Adataset%2520of%2520hotel%2520reviews%2520annotated%2520at%2520the%2520sub-sentence%2520level%2520with%2520multiple%250Alabels.%2520Experiments%2520on%2520this%2520dataset%2520and%2520two%2520public%2520benchmarks%2520%2528binary%2520and%250Amulti-class%2529%2520show%2520that%2520ProtoSiTex%2520achieves%2520state-of-the-art%2520performance%2520while%250Adelivering%2520faithful%252C%2520human-aligned%2520explanations%252C%2520establishing%2520it%2520as%2520a%2520robust%250Asolution%2520for%2520semi-interpretable%2520multi-label%2520text%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12534v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProtoSiTex%3A%20Learning%20Semi-Interpretable%20Prototypes%20for%20Multi-label%20Text%0A%20%20Classification&entry.906535625=Utsav%20Kumar%20Nareti%20and%20Suraj%20Kumar%20and%20Soumya%20Pandey%20and%20Soumi%20Chattopadhyay%20and%20Chandranath%20Adak&entry.1292438233=%20%20The%20surge%20in%20user-generated%20reviews%20has%20amplified%20the%20need%20for%20interpretable%0Amodels%20that%20can%20provide%20fine-grained%20insights.%20Existing%20prototype-based%20models%0Aoffer%20intuitive%20explanations%20but%20typically%20operate%20at%20coarse%20granularity%0A%28sentence%20or%20document%20level%29%20and%20fail%20to%20address%20the%20multi-label%20nature%20of%0Areal-world%20text%20classification.%20We%20propose%20ProtoSiTex%2C%20a%20semi-interpretable%0Aframework%20designed%20for%20fine-grained%20multi-label%20text%20classification.%20ProtoSiTex%0Aemploys%20a%20dual-phase%20alternating%20training%20strategy%3A%20an%20unsupervised%20prototype%0Adiscovery%20phase%20that%20learns%20semantically%20coherent%20and%20diverse%20prototypes%2C%20and%20a%0Asupervised%20classification%20phase%20that%20maps%20these%20prototypes%20to%20class%20labels.%20A%0Ahierarchical%20loss%20function%20enforces%20consistency%20across%20sub-sentence%2C%20sentence%2C%0Aand%20document%20levels%2C%20enhancing%20interpretability%20and%20alignment.%20Unlike%20prior%0Aapproaches%2C%20ProtoSiTex%20captures%20overlapping%20and%20conflicting%20semantics%20using%0Aadaptive%20prototypes%20and%20multi-head%20attention.%20We%20also%20introduce%20a%20benchmark%0Adataset%20of%20hotel%20reviews%20annotated%20at%20the%20sub-sentence%20level%20with%20multiple%0Alabels.%20Experiments%20on%20this%20dataset%20and%20two%20public%20benchmarks%20%28binary%20and%0Amulti-class%29%20show%20that%20ProtoSiTex%20achieves%20state-of-the-art%20performance%20while%0Adelivering%20faithful%2C%20human-aligned%20explanations%2C%20establishing%20it%20as%20a%20robust%0Asolution%20for%20semi-interpretable%20multi-label%20text%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12534v1&entry.124074799=Read"},
{"title": "Inclusive Fitness as a Key Step Towards More Advanced Social Behaviors\n  in Multi-Agent Reinforcement Learning Settings", "author": "Andries Rosseau and Rapha\u00ebl Avalos and Ann Now\u00e9", "abstract": "  The competitive and cooperative forces of natural selection have driven the\nevolution of intelligence for millions of years, culminating in nature's vast\nbiodiversity and the complexity of human minds. Inspired by this process, we\npropose a novel multi-agent reinforcement learning framework where each agent\nis assigned a genotype and where reward functions are modelled after the\nconcept of inclusive fitness. An agent's genetic material may be shared with\nother agents, and our inclusive reward function naturally accounts for this. We\nstudy the resulting social dynamics in two types of network games with\nprisoner's dilemmas and find that our results align with well-established\nprinciples from biology, such as Hamilton's rule. Furthermore, we outline how\nthis framework can extend to more open-ended environments with spatial and\ntemporal structure, finite resources, and evolving populations. We hypothesize\nthe emergence of an arms race of strategies, where each new strategy is a\ngradual improvement over earlier adaptations of other agents, effectively\nproducing a multi-agent autocurriculum analogous to biological evolution. In\ncontrast to the binary team-based structures prevalent in earlier research, our\ngene-based reward structure introduces a spectrum of cooperation ranging from\nfull adversity to full cooperativeness based on genetic similarity, enabling\nunique non team-based social dynamics. For example, one agent having a mutual\ncooperative relationship with two other agents, while the two other agents\nbehave adversarially towards each other. We argue that incorporating inclusive\nfitness in agents provides a foundation for the emergence of more strategically\nadvanced and socially intelligent agents.\n", "link": "http://arxiv.org/abs/2510.12555v1", "date": "2025-10-14", "relevancy": 2.1006, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5314}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5292}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inclusive%20Fitness%20as%20a%20Key%20Step%20Towards%20More%20Advanced%20Social%20Behaviors%0A%20%20in%20Multi-Agent%20Reinforcement%20Learning%20Settings&body=Title%3A%20Inclusive%20Fitness%20as%20a%20Key%20Step%20Towards%20More%20Advanced%20Social%20Behaviors%0A%20%20in%20Multi-Agent%20Reinforcement%20Learning%20Settings%0AAuthor%3A%20Andries%20Rosseau%20and%20Rapha%C3%ABl%20Avalos%20and%20Ann%20Now%C3%A9%0AAbstract%3A%20%20%20The%20competitive%20and%20cooperative%20forces%20of%20natural%20selection%20have%20driven%20the%0Aevolution%20of%20intelligence%20for%20millions%20of%20years%2C%20culminating%20in%20nature%27s%20vast%0Abiodiversity%20and%20the%20complexity%20of%20human%20minds.%20Inspired%20by%20this%20process%2C%20we%0Apropose%20a%20novel%20multi-agent%20reinforcement%20learning%20framework%20where%20each%20agent%0Ais%20assigned%20a%20genotype%20and%20where%20reward%20functions%20are%20modelled%20after%20the%0Aconcept%20of%20inclusive%20fitness.%20An%20agent%27s%20genetic%20material%20may%20be%20shared%20with%0Aother%20agents%2C%20and%20our%20inclusive%20reward%20function%20naturally%20accounts%20for%20this.%20We%0Astudy%20the%20resulting%20social%20dynamics%20in%20two%20types%20of%20network%20games%20with%0Aprisoner%27s%20dilemmas%20and%20find%20that%20our%20results%20align%20with%20well-established%0Aprinciples%20from%20biology%2C%20such%20as%20Hamilton%27s%20rule.%20Furthermore%2C%20we%20outline%20how%0Athis%20framework%20can%20extend%20to%20more%20open-ended%20environments%20with%20spatial%20and%0Atemporal%20structure%2C%20finite%20resources%2C%20and%20evolving%20populations.%20We%20hypothesize%0Athe%20emergence%20of%20an%20arms%20race%20of%20strategies%2C%20where%20each%20new%20strategy%20is%20a%0Agradual%20improvement%20over%20earlier%20adaptations%20of%20other%20agents%2C%20effectively%0Aproducing%20a%20multi-agent%20autocurriculum%20analogous%20to%20biological%20evolution.%20In%0Acontrast%20to%20the%20binary%20team-based%20structures%20prevalent%20in%20earlier%20research%2C%20our%0Agene-based%20reward%20structure%20introduces%20a%20spectrum%20of%20cooperation%20ranging%20from%0Afull%20adversity%20to%20full%20cooperativeness%20based%20on%20genetic%20similarity%2C%20enabling%0Aunique%20non%20team-based%20social%20dynamics.%20For%20example%2C%20one%20agent%20having%20a%20mutual%0Acooperative%20relationship%20with%20two%20other%20agents%2C%20while%20the%20two%20other%20agents%0Abehave%20adversarially%20towards%20each%20other.%20We%20argue%20that%20incorporating%20inclusive%0Afitness%20in%20agents%20provides%20a%20foundation%20for%20the%20emergence%20of%20more%20strategically%0Aadvanced%20and%20socially%20intelligent%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInclusive%2520Fitness%2520as%2520a%2520Key%2520Step%2520Towards%2520More%2520Advanced%2520Social%2520Behaviors%250A%2520%2520in%2520Multi-Agent%2520Reinforcement%2520Learning%2520Settings%26entry.906535625%3DAndries%2520Rosseau%2520and%2520Rapha%25C3%25ABl%2520Avalos%2520and%2520Ann%2520Now%25C3%25A9%26entry.1292438233%3D%2520%2520The%2520competitive%2520and%2520cooperative%2520forces%2520of%2520natural%2520selection%2520have%2520driven%2520the%250Aevolution%2520of%2520intelligence%2520for%2520millions%2520of%2520years%252C%2520culminating%2520in%2520nature%2527s%2520vast%250Abiodiversity%2520and%2520the%2520complexity%2520of%2520human%2520minds.%2520Inspired%2520by%2520this%2520process%252C%2520we%250Apropose%2520a%2520novel%2520multi-agent%2520reinforcement%2520learning%2520framework%2520where%2520each%2520agent%250Ais%2520assigned%2520a%2520genotype%2520and%2520where%2520reward%2520functions%2520are%2520modelled%2520after%2520the%250Aconcept%2520of%2520inclusive%2520fitness.%2520An%2520agent%2527s%2520genetic%2520material%2520may%2520be%2520shared%2520with%250Aother%2520agents%252C%2520and%2520our%2520inclusive%2520reward%2520function%2520naturally%2520accounts%2520for%2520this.%2520We%250Astudy%2520the%2520resulting%2520social%2520dynamics%2520in%2520two%2520types%2520of%2520network%2520games%2520with%250Aprisoner%2527s%2520dilemmas%2520and%2520find%2520that%2520our%2520results%2520align%2520with%2520well-established%250Aprinciples%2520from%2520biology%252C%2520such%2520as%2520Hamilton%2527s%2520rule.%2520Furthermore%252C%2520we%2520outline%2520how%250Athis%2520framework%2520can%2520extend%2520to%2520more%2520open-ended%2520environments%2520with%2520spatial%2520and%250Atemporal%2520structure%252C%2520finite%2520resources%252C%2520and%2520evolving%2520populations.%2520We%2520hypothesize%250Athe%2520emergence%2520of%2520an%2520arms%2520race%2520of%2520strategies%252C%2520where%2520each%2520new%2520strategy%2520is%2520a%250Agradual%2520improvement%2520over%2520earlier%2520adaptations%2520of%2520other%2520agents%252C%2520effectively%250Aproducing%2520a%2520multi-agent%2520autocurriculum%2520analogous%2520to%2520biological%2520evolution.%2520In%250Acontrast%2520to%2520the%2520binary%2520team-based%2520structures%2520prevalent%2520in%2520earlier%2520research%252C%2520our%250Agene-based%2520reward%2520structure%2520introduces%2520a%2520spectrum%2520of%2520cooperation%2520ranging%2520from%250Afull%2520adversity%2520to%2520full%2520cooperativeness%2520based%2520on%2520genetic%2520similarity%252C%2520enabling%250Aunique%2520non%2520team-based%2520social%2520dynamics.%2520For%2520example%252C%2520one%2520agent%2520having%2520a%2520mutual%250Acooperative%2520relationship%2520with%2520two%2520other%2520agents%252C%2520while%2520the%2520two%2520other%2520agents%250Abehave%2520adversarially%2520towards%2520each%2520other.%2520We%2520argue%2520that%2520incorporating%2520inclusive%250Afitness%2520in%2520agents%2520provides%2520a%2520foundation%2520for%2520the%2520emergence%2520of%2520more%2520strategically%250Aadvanced%2520and%2520socially%2520intelligent%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inclusive%20Fitness%20as%20a%20Key%20Step%20Towards%20More%20Advanced%20Social%20Behaviors%0A%20%20in%20Multi-Agent%20Reinforcement%20Learning%20Settings&entry.906535625=Andries%20Rosseau%20and%20Rapha%C3%ABl%20Avalos%20and%20Ann%20Now%C3%A9&entry.1292438233=%20%20The%20competitive%20and%20cooperative%20forces%20of%20natural%20selection%20have%20driven%20the%0Aevolution%20of%20intelligence%20for%20millions%20of%20years%2C%20culminating%20in%20nature%27s%20vast%0Abiodiversity%20and%20the%20complexity%20of%20human%20minds.%20Inspired%20by%20this%20process%2C%20we%0Apropose%20a%20novel%20multi-agent%20reinforcement%20learning%20framework%20where%20each%20agent%0Ais%20assigned%20a%20genotype%20and%20where%20reward%20functions%20are%20modelled%20after%20the%0Aconcept%20of%20inclusive%20fitness.%20An%20agent%27s%20genetic%20material%20may%20be%20shared%20with%0Aother%20agents%2C%20and%20our%20inclusive%20reward%20function%20naturally%20accounts%20for%20this.%20We%0Astudy%20the%20resulting%20social%20dynamics%20in%20two%20types%20of%20network%20games%20with%0Aprisoner%27s%20dilemmas%20and%20find%20that%20our%20results%20align%20with%20well-established%0Aprinciples%20from%20biology%2C%20such%20as%20Hamilton%27s%20rule.%20Furthermore%2C%20we%20outline%20how%0Athis%20framework%20can%20extend%20to%20more%20open-ended%20environments%20with%20spatial%20and%0Atemporal%20structure%2C%20finite%20resources%2C%20and%20evolving%20populations.%20We%20hypothesize%0Athe%20emergence%20of%20an%20arms%20race%20of%20strategies%2C%20where%20each%20new%20strategy%20is%20a%0Agradual%20improvement%20over%20earlier%20adaptations%20of%20other%20agents%2C%20effectively%0Aproducing%20a%20multi-agent%20autocurriculum%20analogous%20to%20biological%20evolution.%20In%0Acontrast%20to%20the%20binary%20team-based%20structures%20prevalent%20in%20earlier%20research%2C%20our%0Agene-based%20reward%20structure%20introduces%20a%20spectrum%20of%20cooperation%20ranging%20from%0Afull%20adversity%20to%20full%20cooperativeness%20based%20on%20genetic%20similarity%2C%20enabling%0Aunique%20non%20team-based%20social%20dynamics.%20For%20example%2C%20one%20agent%20having%20a%20mutual%0Acooperative%20relationship%20with%20two%20other%20agents%2C%20while%20the%20two%20other%20agents%0Abehave%20adversarially%20towards%20each%20other.%20We%20argue%20that%20incorporating%20inclusive%0Afitness%20in%20agents%20provides%20a%20foundation%20for%20the%20emergence%20of%20more%20strategically%0Aadvanced%20and%20socially%20intelligent%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12555v1&entry.124074799=Read"},
{"title": "Efficient Fine-Tuning of DINOv3 Pretrained on Natural Images for\n  Atypical Mitotic Figure Classification (MIDOG 2025 Task 2 Winner)", "author": "Guillaume Balezo and Hana Feki and Rapha\u00ebl Bourgade and Lily Monnier and Matthieu Blons and Alice Blondel and Etienne Decenci\u00e8re and Albert Pla Planas and Thomas Walter", "abstract": "  Atypical mitotic figures (AMFs) represent abnormal cell division associated\nwith poor prognosis. Yet their detection remains difficult due to low\nprevalence, subtle morphology, and inter-observer variability. The MIDOG 2025\nchallenge introduces a benchmark for AMF classification across multiple\ndomains. In this work, we fine-tuned the recently published DINOv3-H+ vision\ntransformer, pretrained on natural images, using low-rank adaptation (LoRA),\ntraining only ~1.3M parameters in combination with extensive augmentation and a\ndomain-weighted Focal Loss to handle domain heterogeneity. Despite the domain\ngap, our fine-tuned DINOv3 transfers effectively to histopathology, reaching\nfirst place on the final test set. These results highlight the advantages of\nDINOv3 pretraining and underline the efficiency and robustness of our\nfine-tuning strategy, yielding state-of-the-art results for the atypical\nmitosis classification challenge in MIDOG 2025.\n", "link": "http://arxiv.org/abs/2508.21041v3", "date": "2025-10-14", "relevancy": 2.1001, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5261}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.526}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Fine-Tuning%20of%20DINOv3%20Pretrained%20on%20Natural%20Images%20for%0A%20%20Atypical%20Mitotic%20Figure%20Classification%20%28MIDOG%202025%20Task%202%20Winner%29&body=Title%3A%20Efficient%20Fine-Tuning%20of%20DINOv3%20Pretrained%20on%20Natural%20Images%20for%0A%20%20Atypical%20Mitotic%20Figure%20Classification%20%28MIDOG%202025%20Task%202%20Winner%29%0AAuthor%3A%20Guillaume%20Balezo%20and%20Hana%20Feki%20and%20Rapha%C3%ABl%20Bourgade%20and%20Lily%20Monnier%20and%20Matthieu%20Blons%20and%20Alice%20Blondel%20and%20Etienne%20Decenci%C3%A8re%20and%20Albert%20Pla%20Planas%20and%20Thomas%20Walter%0AAbstract%3A%20%20%20Atypical%20mitotic%20figures%20%28AMFs%29%20represent%20abnormal%20cell%20division%20associated%0Awith%20poor%20prognosis.%20Yet%20their%20detection%20remains%20difficult%20due%20to%20low%0Aprevalence%2C%20subtle%20morphology%2C%20and%20inter-observer%20variability.%20The%20MIDOG%202025%0Achallenge%20introduces%20a%20benchmark%20for%20AMF%20classification%20across%20multiple%0Adomains.%20In%20this%20work%2C%20we%20fine-tuned%20the%20recently%20published%20DINOv3-H%2B%20vision%0Atransformer%2C%20pretrained%20on%20natural%20images%2C%20using%20low-rank%20adaptation%20%28LoRA%29%2C%0Atraining%20only%20~1.3M%20parameters%20in%20combination%20with%20extensive%20augmentation%20and%20a%0Adomain-weighted%20Focal%20Loss%20to%20handle%20domain%20heterogeneity.%20Despite%20the%20domain%0Agap%2C%20our%20fine-tuned%20DINOv3%20transfers%20effectively%20to%20histopathology%2C%20reaching%0Afirst%20place%20on%20the%20final%20test%20set.%20These%20results%20highlight%20the%20advantages%20of%0ADINOv3%20pretraining%20and%20underline%20the%20efficiency%20and%20robustness%20of%20our%0Afine-tuning%20strategy%2C%20yielding%20state-of-the-art%20results%20for%20the%20atypical%0Amitosis%20classification%20challenge%20in%20MIDOG%202025.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21041v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Fine-Tuning%2520of%2520DINOv3%2520Pretrained%2520on%2520Natural%2520Images%2520for%250A%2520%2520Atypical%2520Mitotic%2520Figure%2520Classification%2520%2528MIDOG%25202025%2520Task%25202%2520Winner%2529%26entry.906535625%3DGuillaume%2520Balezo%2520and%2520Hana%2520Feki%2520and%2520Rapha%25C3%25ABl%2520Bourgade%2520and%2520Lily%2520Monnier%2520and%2520Matthieu%2520Blons%2520and%2520Alice%2520Blondel%2520and%2520Etienne%2520Decenci%25C3%25A8re%2520and%2520Albert%2520Pla%2520Planas%2520and%2520Thomas%2520Walter%26entry.1292438233%3D%2520%2520Atypical%2520mitotic%2520figures%2520%2528AMFs%2529%2520represent%2520abnormal%2520cell%2520division%2520associated%250Awith%2520poor%2520prognosis.%2520Yet%2520their%2520detection%2520remains%2520difficult%2520due%2520to%2520low%250Aprevalence%252C%2520subtle%2520morphology%252C%2520and%2520inter-observer%2520variability.%2520The%2520MIDOG%25202025%250Achallenge%2520introduces%2520a%2520benchmark%2520for%2520AMF%2520classification%2520across%2520multiple%250Adomains.%2520In%2520this%2520work%252C%2520we%2520fine-tuned%2520the%2520recently%2520published%2520DINOv3-H%252B%2520vision%250Atransformer%252C%2520pretrained%2520on%2520natural%2520images%252C%2520using%2520low-rank%2520adaptation%2520%2528LoRA%2529%252C%250Atraining%2520only%2520~1.3M%2520parameters%2520in%2520combination%2520with%2520extensive%2520augmentation%2520and%2520a%250Adomain-weighted%2520Focal%2520Loss%2520to%2520handle%2520domain%2520heterogeneity.%2520Despite%2520the%2520domain%250Agap%252C%2520our%2520fine-tuned%2520DINOv3%2520transfers%2520effectively%2520to%2520histopathology%252C%2520reaching%250Afirst%2520place%2520on%2520the%2520final%2520test%2520set.%2520These%2520results%2520highlight%2520the%2520advantages%2520of%250ADINOv3%2520pretraining%2520and%2520underline%2520the%2520efficiency%2520and%2520robustness%2520of%2520our%250Afine-tuning%2520strategy%252C%2520yielding%2520state-of-the-art%2520results%2520for%2520the%2520atypical%250Amitosis%2520classification%2520challenge%2520in%2520MIDOG%25202025.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21041v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Fine-Tuning%20of%20DINOv3%20Pretrained%20on%20Natural%20Images%20for%0A%20%20Atypical%20Mitotic%20Figure%20Classification%20%28MIDOG%202025%20Task%202%20Winner%29&entry.906535625=Guillaume%20Balezo%20and%20Hana%20Feki%20and%20Rapha%C3%ABl%20Bourgade%20and%20Lily%20Monnier%20and%20Matthieu%20Blons%20and%20Alice%20Blondel%20and%20Etienne%20Decenci%C3%A8re%20and%20Albert%20Pla%20Planas%20and%20Thomas%20Walter&entry.1292438233=%20%20Atypical%20mitotic%20figures%20%28AMFs%29%20represent%20abnormal%20cell%20division%20associated%0Awith%20poor%20prognosis.%20Yet%20their%20detection%20remains%20difficult%20due%20to%20low%0Aprevalence%2C%20subtle%20morphology%2C%20and%20inter-observer%20variability.%20The%20MIDOG%202025%0Achallenge%20introduces%20a%20benchmark%20for%20AMF%20classification%20across%20multiple%0Adomains.%20In%20this%20work%2C%20we%20fine-tuned%20the%20recently%20published%20DINOv3-H%2B%20vision%0Atransformer%2C%20pretrained%20on%20natural%20images%2C%20using%20low-rank%20adaptation%20%28LoRA%29%2C%0Atraining%20only%20~1.3M%20parameters%20in%20combination%20with%20extensive%20augmentation%20and%20a%0Adomain-weighted%20Focal%20Loss%20to%20handle%20domain%20heterogeneity.%20Despite%20the%20domain%0Agap%2C%20our%20fine-tuned%20DINOv3%20transfers%20effectively%20to%20histopathology%2C%20reaching%0Afirst%20place%20on%20the%20final%20test%20set.%20These%20results%20highlight%20the%20advantages%20of%0ADINOv3%20pretraining%20and%20underline%20the%20efficiency%20and%20robustness%20of%20our%0Afine-tuning%20strategy%2C%20yielding%20state-of-the-art%20results%20for%20the%20atypical%0Amitosis%20classification%20challenge%20in%20MIDOG%202025.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21041v3&entry.124074799=Read"},
{"title": "BAAF: A benchmark attention adaptive framework for medical ultrasound\n  image segmentation tasks", "author": "Gongping Chen and Lei Zhao and Xiaotao Yin and Liang Cui and Jianxun Zhang and Yu Dai and Ningning Liu", "abstract": "  The AI-based assisted diagnosis programs have been widely investigated on\nmedical ultrasound images. Complex scenario of ultrasound image, in which the\ncoupled interference of internal and external factors is severe, brings a\nunique challenge for localize the object region automatically and precisely in\nultrasound images. In this study, we seek to propose a more general and robust\nBenchmark Attention Adaptive Framework (BAAF) to assist doctors segment or\ndiagnose lesions and tissues in ultrasound images more quickly and accurately.\nDifferent from existing attention schemes, the BAAF consists of a parallel\nhybrid attention module (PHAM) and an adaptive calibration mechanism (ACM).\nSpecifically, BAAF first coarsely calibrates the input features from the\nchannel and spatial dimensions, and then adaptively selects more robust lesion\nor tissue characterizations from the coarse-calibrated feature maps. The design\nof BAAF further optimizes the \"what\" and \"where\" focus and selection problems\nin CNNs and seeks to improve the segmentation accuracy of lesions or tissues in\nmedical ultrasound images. The method is evaluated on four medical ultrasound\nsegmentation tasks, and the adequate experimental results demonstrate the\nremarkable performance improvement over existing state-of-the-art methods. In\naddition, the comparison with existing attention mechanisms also demonstrates\nthe superiority of BAAF. This work provides the possibility for automated\nmedical ultrasound assisted diagnosis and reduces reliance on human accuracy\nand precision.\n", "link": "http://arxiv.org/abs/2310.00919v3", "date": "2025-10-14", "relevancy": 2.0991, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5328}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5288}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BAAF%3A%20A%20benchmark%20attention%20adaptive%20framework%20for%20medical%20ultrasound%0A%20%20image%20segmentation%20tasks&body=Title%3A%20BAAF%3A%20A%20benchmark%20attention%20adaptive%20framework%20for%20medical%20ultrasound%0A%20%20image%20segmentation%20tasks%0AAuthor%3A%20Gongping%20Chen%20and%20Lei%20Zhao%20and%20Xiaotao%20Yin%20and%20Liang%20Cui%20and%20Jianxun%20Zhang%20and%20Yu%20Dai%20and%20Ningning%20Liu%0AAbstract%3A%20%20%20The%20AI-based%20assisted%20diagnosis%20programs%20have%20been%20widely%20investigated%20on%0Amedical%20ultrasound%20images.%20Complex%20scenario%20of%20ultrasound%20image%2C%20in%20which%20the%0Acoupled%20interference%20of%20internal%20and%20external%20factors%20is%20severe%2C%20brings%20a%0Aunique%20challenge%20for%20localize%20the%20object%20region%20automatically%20and%20precisely%20in%0Aultrasound%20images.%20In%20this%20study%2C%20we%20seek%20to%20propose%20a%20more%20general%20and%20robust%0ABenchmark%20Attention%20Adaptive%20Framework%20%28BAAF%29%20to%20assist%20doctors%20segment%20or%0Adiagnose%20lesions%20and%20tissues%20in%20ultrasound%20images%20more%20quickly%20and%20accurately.%0ADifferent%20from%20existing%20attention%20schemes%2C%20the%20BAAF%20consists%20of%20a%20parallel%0Ahybrid%20attention%20module%20%28PHAM%29%20and%20an%20adaptive%20calibration%20mechanism%20%28ACM%29.%0ASpecifically%2C%20BAAF%20first%20coarsely%20calibrates%20the%20input%20features%20from%20the%0Achannel%20and%20spatial%20dimensions%2C%20and%20then%20adaptively%20selects%20more%20robust%20lesion%0Aor%20tissue%20characterizations%20from%20the%20coarse-calibrated%20feature%20maps.%20The%20design%0Aof%20BAAF%20further%20optimizes%20the%20%22what%22%20and%20%22where%22%20focus%20and%20selection%20problems%0Ain%20CNNs%20and%20seeks%20to%20improve%20the%20segmentation%20accuracy%20of%20lesions%20or%20tissues%20in%0Amedical%20ultrasound%20images.%20The%20method%20is%20evaluated%20on%20four%20medical%20ultrasound%0Asegmentation%20tasks%2C%20and%20the%20adequate%20experimental%20results%20demonstrate%20the%0Aremarkable%20performance%20improvement%20over%20existing%20state-of-the-art%20methods.%20In%0Aaddition%2C%20the%20comparison%20with%20existing%20attention%20mechanisms%20also%20demonstrates%0Athe%20superiority%20of%20BAAF.%20This%20work%20provides%20the%20possibility%20for%20automated%0Amedical%20ultrasound%20assisted%20diagnosis%20and%20reduces%20reliance%20on%20human%20accuracy%0Aand%20precision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.00919v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBAAF%253A%2520A%2520benchmark%2520attention%2520adaptive%2520framework%2520for%2520medical%2520ultrasound%250A%2520%2520image%2520segmentation%2520tasks%26entry.906535625%3DGongping%2520Chen%2520and%2520Lei%2520Zhao%2520and%2520Xiaotao%2520Yin%2520and%2520Liang%2520Cui%2520and%2520Jianxun%2520Zhang%2520and%2520Yu%2520Dai%2520and%2520Ningning%2520Liu%26entry.1292438233%3D%2520%2520The%2520AI-based%2520assisted%2520diagnosis%2520programs%2520have%2520been%2520widely%2520investigated%2520on%250Amedical%2520ultrasound%2520images.%2520Complex%2520scenario%2520of%2520ultrasound%2520image%252C%2520in%2520which%2520the%250Acoupled%2520interference%2520of%2520internal%2520and%2520external%2520factors%2520is%2520severe%252C%2520brings%2520a%250Aunique%2520challenge%2520for%2520localize%2520the%2520object%2520region%2520automatically%2520and%2520precisely%2520in%250Aultrasound%2520images.%2520In%2520this%2520study%252C%2520we%2520seek%2520to%2520propose%2520a%2520more%2520general%2520and%2520robust%250ABenchmark%2520Attention%2520Adaptive%2520Framework%2520%2528BAAF%2529%2520to%2520assist%2520doctors%2520segment%2520or%250Adiagnose%2520lesions%2520and%2520tissues%2520in%2520ultrasound%2520images%2520more%2520quickly%2520and%2520accurately.%250ADifferent%2520from%2520existing%2520attention%2520schemes%252C%2520the%2520BAAF%2520consists%2520of%2520a%2520parallel%250Ahybrid%2520attention%2520module%2520%2528PHAM%2529%2520and%2520an%2520adaptive%2520calibration%2520mechanism%2520%2528ACM%2529.%250ASpecifically%252C%2520BAAF%2520first%2520coarsely%2520calibrates%2520the%2520input%2520features%2520from%2520the%250Achannel%2520and%2520spatial%2520dimensions%252C%2520and%2520then%2520adaptively%2520selects%2520more%2520robust%2520lesion%250Aor%2520tissue%2520characterizations%2520from%2520the%2520coarse-calibrated%2520feature%2520maps.%2520The%2520design%250Aof%2520BAAF%2520further%2520optimizes%2520the%2520%2522what%2522%2520and%2520%2522where%2522%2520focus%2520and%2520selection%2520problems%250Ain%2520CNNs%2520and%2520seeks%2520to%2520improve%2520the%2520segmentation%2520accuracy%2520of%2520lesions%2520or%2520tissues%2520in%250Amedical%2520ultrasound%2520images.%2520The%2520method%2520is%2520evaluated%2520on%2520four%2520medical%2520ultrasound%250Asegmentation%2520tasks%252C%2520and%2520the%2520adequate%2520experimental%2520results%2520demonstrate%2520the%250Aremarkable%2520performance%2520improvement%2520over%2520existing%2520state-of-the-art%2520methods.%2520In%250Aaddition%252C%2520the%2520comparison%2520with%2520existing%2520attention%2520mechanisms%2520also%2520demonstrates%250Athe%2520superiority%2520of%2520BAAF.%2520This%2520work%2520provides%2520the%2520possibility%2520for%2520automated%250Amedical%2520ultrasound%2520assisted%2520diagnosis%2520and%2520reduces%2520reliance%2520on%2520human%2520accuracy%250Aand%2520precision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.00919v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BAAF%3A%20A%20benchmark%20attention%20adaptive%20framework%20for%20medical%20ultrasound%0A%20%20image%20segmentation%20tasks&entry.906535625=Gongping%20Chen%20and%20Lei%20Zhao%20and%20Xiaotao%20Yin%20and%20Liang%20Cui%20and%20Jianxun%20Zhang%20and%20Yu%20Dai%20and%20Ningning%20Liu&entry.1292438233=%20%20The%20AI-based%20assisted%20diagnosis%20programs%20have%20been%20widely%20investigated%20on%0Amedical%20ultrasound%20images.%20Complex%20scenario%20of%20ultrasound%20image%2C%20in%20which%20the%0Acoupled%20interference%20of%20internal%20and%20external%20factors%20is%20severe%2C%20brings%20a%0Aunique%20challenge%20for%20localize%20the%20object%20region%20automatically%20and%20precisely%20in%0Aultrasound%20images.%20In%20this%20study%2C%20we%20seek%20to%20propose%20a%20more%20general%20and%20robust%0ABenchmark%20Attention%20Adaptive%20Framework%20%28BAAF%29%20to%20assist%20doctors%20segment%20or%0Adiagnose%20lesions%20and%20tissues%20in%20ultrasound%20images%20more%20quickly%20and%20accurately.%0ADifferent%20from%20existing%20attention%20schemes%2C%20the%20BAAF%20consists%20of%20a%20parallel%0Ahybrid%20attention%20module%20%28PHAM%29%20and%20an%20adaptive%20calibration%20mechanism%20%28ACM%29.%0ASpecifically%2C%20BAAF%20first%20coarsely%20calibrates%20the%20input%20features%20from%20the%0Achannel%20and%20spatial%20dimensions%2C%20and%20then%20adaptively%20selects%20more%20robust%20lesion%0Aor%20tissue%20characterizations%20from%20the%20coarse-calibrated%20feature%20maps.%20The%20design%0Aof%20BAAF%20further%20optimizes%20the%20%22what%22%20and%20%22where%22%20focus%20and%20selection%20problems%0Ain%20CNNs%20and%20seeks%20to%20improve%20the%20segmentation%20accuracy%20of%20lesions%20or%20tissues%20in%0Amedical%20ultrasound%20images.%20The%20method%20is%20evaluated%20on%20four%20medical%20ultrasound%0Asegmentation%20tasks%2C%20and%20the%20adequate%20experimental%20results%20demonstrate%20the%0Aremarkable%20performance%20improvement%20over%20existing%20state-of-the-art%20methods.%20In%0Aaddition%2C%20the%20comparison%20with%20existing%20attention%20mechanisms%20also%20demonstrates%0Athe%20superiority%20of%20BAAF.%20This%20work%20provides%20the%20possibility%20for%20automated%0Amedical%20ultrasound%20assisted%20diagnosis%20and%20reduces%20reliance%20on%20human%20accuracy%0Aand%20precision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.00919v3&entry.124074799=Read"},
{"title": "Towards Robust Artificial Intelligence: Self-Supervised Learning\n  Approach for Out-of-Distribution Detection", "author": "Wissam Salhab and Darine Ameyed and Hamid Mcheick and Fehmi Jaafar", "abstract": "  Robustness in AI systems refers to their ability to maintain reliable and\naccurate performance under various conditions, including out-of-distribution\n(OOD) samples, adversarial attacks, and environmental changes. This is crucial\nin safety-critical systems, such as autonomous vehicles, transportation, or\nhealthcare, where malfunctions could have severe consequences. This paper\nproposes an approach to improve OOD detection without the need of labeled data,\nthereby increasing the AI systems' robustness. The proposed approach leverages\nthe principles of self-supervised learning, allowing the model to learn useful\nrepresentations from unlabeled data. Combined with graph-theoretical\ntechniques, this enables the more efficient identification and categorization\nof OOD samples. Compared to existing state-of-the-art methods, this approach\nachieved an Area Under the Receiver Operating Characteristic Curve (AUROC) =\n0.99.\n", "link": "http://arxiv.org/abs/2510.12713v1", "date": "2025-10-14", "relevancy": 2.0938, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5476}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5131}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Robust%20Artificial%20Intelligence%3A%20Self-Supervised%20Learning%0A%20%20Approach%20for%20Out-of-Distribution%20Detection&body=Title%3A%20Towards%20Robust%20Artificial%20Intelligence%3A%20Self-Supervised%20Learning%0A%20%20Approach%20for%20Out-of-Distribution%20Detection%0AAuthor%3A%20Wissam%20Salhab%20and%20Darine%20Ameyed%20and%20Hamid%20Mcheick%20and%20Fehmi%20Jaafar%0AAbstract%3A%20%20%20Robustness%20in%20AI%20systems%20refers%20to%20their%20ability%20to%20maintain%20reliable%20and%0Aaccurate%20performance%20under%20various%20conditions%2C%20including%20out-of-distribution%0A%28OOD%29%20samples%2C%20adversarial%20attacks%2C%20and%20environmental%20changes.%20This%20is%20crucial%0Ain%20safety-critical%20systems%2C%20such%20as%20autonomous%20vehicles%2C%20transportation%2C%20or%0Ahealthcare%2C%20where%20malfunctions%20could%20have%20severe%20consequences.%20This%20paper%0Aproposes%20an%20approach%20to%20improve%20OOD%20detection%20without%20the%20need%20of%20labeled%20data%2C%0Athereby%20increasing%20the%20AI%20systems%27%20robustness.%20The%20proposed%20approach%20leverages%0Athe%20principles%20of%20self-supervised%20learning%2C%20allowing%20the%20model%20to%20learn%20useful%0Arepresentations%20from%20unlabeled%20data.%20Combined%20with%20graph-theoretical%0Atechniques%2C%20this%20enables%20the%20more%20efficient%20identification%20and%20categorization%0Aof%20OOD%20samples.%20Compared%20to%20existing%20state-of-the-art%20methods%2C%20this%20approach%0Aachieved%20an%20Area%20Under%20the%20Receiver%20Operating%20Characteristic%20Curve%20%28AUROC%29%20%3D%0A0.99.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Robust%2520Artificial%2520Intelligence%253A%2520Self-Supervised%2520Learning%250A%2520%2520Approach%2520for%2520Out-of-Distribution%2520Detection%26entry.906535625%3DWissam%2520Salhab%2520and%2520Darine%2520Ameyed%2520and%2520Hamid%2520Mcheick%2520and%2520Fehmi%2520Jaafar%26entry.1292438233%3D%2520%2520Robustness%2520in%2520AI%2520systems%2520refers%2520to%2520their%2520ability%2520to%2520maintain%2520reliable%2520and%250Aaccurate%2520performance%2520under%2520various%2520conditions%252C%2520including%2520out-of-distribution%250A%2528OOD%2529%2520samples%252C%2520adversarial%2520attacks%252C%2520and%2520environmental%2520changes.%2520This%2520is%2520crucial%250Ain%2520safety-critical%2520systems%252C%2520such%2520as%2520autonomous%2520vehicles%252C%2520transportation%252C%2520or%250Ahealthcare%252C%2520where%2520malfunctions%2520could%2520have%2520severe%2520consequences.%2520This%2520paper%250Aproposes%2520an%2520approach%2520to%2520improve%2520OOD%2520detection%2520without%2520the%2520need%2520of%2520labeled%2520data%252C%250Athereby%2520increasing%2520the%2520AI%2520systems%2527%2520robustness.%2520The%2520proposed%2520approach%2520leverages%250Athe%2520principles%2520of%2520self-supervised%2520learning%252C%2520allowing%2520the%2520model%2520to%2520learn%2520useful%250Arepresentations%2520from%2520unlabeled%2520data.%2520Combined%2520with%2520graph-theoretical%250Atechniques%252C%2520this%2520enables%2520the%2520more%2520efficient%2520identification%2520and%2520categorization%250Aof%2520OOD%2520samples.%2520Compared%2520to%2520existing%2520state-of-the-art%2520methods%252C%2520this%2520approach%250Aachieved%2520an%2520Area%2520Under%2520the%2520Receiver%2520Operating%2520Characteristic%2520Curve%2520%2528AUROC%2529%2520%253D%250A0.99.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Robust%20Artificial%20Intelligence%3A%20Self-Supervised%20Learning%0A%20%20Approach%20for%20Out-of-Distribution%20Detection&entry.906535625=Wissam%20Salhab%20and%20Darine%20Ameyed%20and%20Hamid%20Mcheick%20and%20Fehmi%20Jaafar&entry.1292438233=%20%20Robustness%20in%20AI%20systems%20refers%20to%20their%20ability%20to%20maintain%20reliable%20and%0Aaccurate%20performance%20under%20various%20conditions%2C%20including%20out-of-distribution%0A%28OOD%29%20samples%2C%20adversarial%20attacks%2C%20and%20environmental%20changes.%20This%20is%20crucial%0Ain%20safety-critical%20systems%2C%20such%20as%20autonomous%20vehicles%2C%20transportation%2C%20or%0Ahealthcare%2C%20where%20malfunctions%20could%20have%20severe%20consequences.%20This%20paper%0Aproposes%20an%20approach%20to%20improve%20OOD%20detection%20without%20the%20need%20of%20labeled%20data%2C%0Athereby%20increasing%20the%20AI%20systems%27%20robustness.%20The%20proposed%20approach%20leverages%0Athe%20principles%20of%20self-supervised%20learning%2C%20allowing%20the%20model%20to%20learn%20useful%0Arepresentations%20from%20unlabeled%20data.%20Combined%20with%20graph-theoretical%0Atechniques%2C%20this%20enables%20the%20more%20efficient%20identification%20and%20categorization%0Aof%20OOD%20samples.%20Compared%20to%20existing%20state-of-the-art%20methods%2C%20this%20approach%0Aachieved%20an%20Area%20Under%20the%20Receiver%20Operating%20Characteristic%20Curve%20%28AUROC%29%20%3D%0A0.99.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12713v1&entry.124074799=Read"},
{"title": "Improving Decision Trees through the Lens of Parameterized Local Search", "author": "Juha Harviainen and Frank Sommer and Manuel Sorge", "abstract": "  Algorithms for learning decision trees often include heuristic local-search\noperations such as (1) adjusting the threshold of a cut or (2) also exchanging\nthe feature of that cut. We study minimizing the number of classification\nerrors by performing a fixed number of a single type of these operations.\nAlthough we discover that the corresponding problems are NP-complete in\ngeneral, we provide a comprehensive parameterized-complexity analysis with the\naim of determining those properties of the problems that explain the hardness\nand those that make the problems tractable. For instance, we show that the\nproblems remain hard for a small number $d$ of features or small domain size\n$D$ but the combination of both yields fixed-parameter tractability. That is,\nthe problems are solvable in $(D + 1)^{2d} \\cdot |I|^{O(1)}$ time, where $|I|$\nis the size of the input. We also provide a proof-of-concept implementation of\nthis algorithm and report on empirical results.\n", "link": "http://arxiv.org/abs/2510.12726v1", "date": "2025-10-14", "relevancy": 2.0932, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.431}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4166}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Decision%20Trees%20through%20the%20Lens%20of%20Parameterized%20Local%20Search&body=Title%3A%20Improving%20Decision%20Trees%20through%20the%20Lens%20of%20Parameterized%20Local%20Search%0AAuthor%3A%20Juha%20Harviainen%20and%20Frank%20Sommer%20and%20Manuel%20Sorge%0AAbstract%3A%20%20%20Algorithms%20for%20learning%20decision%20trees%20often%20include%20heuristic%20local-search%0Aoperations%20such%20as%20%281%29%20adjusting%20the%20threshold%20of%20a%20cut%20or%20%282%29%20also%20exchanging%0Athe%20feature%20of%20that%20cut.%20We%20study%20minimizing%20the%20number%20of%20classification%0Aerrors%20by%20performing%20a%20fixed%20number%20of%20a%20single%20type%20of%20these%20operations.%0AAlthough%20we%20discover%20that%20the%20corresponding%20problems%20are%20NP-complete%20in%0Ageneral%2C%20we%20provide%20a%20comprehensive%20parameterized-complexity%20analysis%20with%20the%0Aaim%20of%20determining%20those%20properties%20of%20the%20problems%20that%20explain%20the%20hardness%0Aand%20those%20that%20make%20the%20problems%20tractable.%20For%20instance%2C%20we%20show%20that%20the%0Aproblems%20remain%20hard%20for%20a%20small%20number%20%24d%24%20of%20features%20or%20small%20domain%20size%0A%24D%24%20but%20the%20combination%20of%20both%20yields%20fixed-parameter%20tractability.%20That%20is%2C%0Athe%20problems%20are%20solvable%20in%20%24%28D%20%2B%201%29%5E%7B2d%7D%20%5Ccdot%20%7CI%7C%5E%7BO%281%29%7D%24%20time%2C%20where%20%24%7CI%7C%24%0Ais%20the%20size%20of%20the%20input.%20We%20also%20provide%20a%20proof-of-concept%20implementation%20of%0Athis%20algorithm%20and%20report%20on%20empirical%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Decision%2520Trees%2520through%2520the%2520Lens%2520of%2520Parameterized%2520Local%2520Search%26entry.906535625%3DJuha%2520Harviainen%2520and%2520Frank%2520Sommer%2520and%2520Manuel%2520Sorge%26entry.1292438233%3D%2520%2520Algorithms%2520for%2520learning%2520decision%2520trees%2520often%2520include%2520heuristic%2520local-search%250Aoperations%2520such%2520as%2520%25281%2529%2520adjusting%2520the%2520threshold%2520of%2520a%2520cut%2520or%2520%25282%2529%2520also%2520exchanging%250Athe%2520feature%2520of%2520that%2520cut.%2520We%2520study%2520minimizing%2520the%2520number%2520of%2520classification%250Aerrors%2520by%2520performing%2520a%2520fixed%2520number%2520of%2520a%2520single%2520type%2520of%2520these%2520operations.%250AAlthough%2520we%2520discover%2520that%2520the%2520corresponding%2520problems%2520are%2520NP-complete%2520in%250Ageneral%252C%2520we%2520provide%2520a%2520comprehensive%2520parameterized-complexity%2520analysis%2520with%2520the%250Aaim%2520of%2520determining%2520those%2520properties%2520of%2520the%2520problems%2520that%2520explain%2520the%2520hardness%250Aand%2520those%2520that%2520make%2520the%2520problems%2520tractable.%2520For%2520instance%252C%2520we%2520show%2520that%2520the%250Aproblems%2520remain%2520hard%2520for%2520a%2520small%2520number%2520%2524d%2524%2520of%2520features%2520or%2520small%2520domain%2520size%250A%2524D%2524%2520but%2520the%2520combination%2520of%2520both%2520yields%2520fixed-parameter%2520tractability.%2520That%2520is%252C%250Athe%2520problems%2520are%2520solvable%2520in%2520%2524%2528D%2520%252B%25201%2529%255E%257B2d%257D%2520%255Ccdot%2520%257CI%257C%255E%257BO%25281%2529%257D%2524%2520time%252C%2520where%2520%2524%257CI%257C%2524%250Ais%2520the%2520size%2520of%2520the%2520input.%2520We%2520also%2520provide%2520a%2520proof-of-concept%2520implementation%2520of%250Athis%2520algorithm%2520and%2520report%2520on%2520empirical%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Decision%20Trees%20through%20the%20Lens%20of%20Parameterized%20Local%20Search&entry.906535625=Juha%20Harviainen%20and%20Frank%20Sommer%20and%20Manuel%20Sorge&entry.1292438233=%20%20Algorithms%20for%20learning%20decision%20trees%20often%20include%20heuristic%20local-search%0Aoperations%20such%20as%20%281%29%20adjusting%20the%20threshold%20of%20a%20cut%20or%20%282%29%20also%20exchanging%0Athe%20feature%20of%20that%20cut.%20We%20study%20minimizing%20the%20number%20of%20classification%0Aerrors%20by%20performing%20a%20fixed%20number%20of%20a%20single%20type%20of%20these%20operations.%0AAlthough%20we%20discover%20that%20the%20corresponding%20problems%20are%20NP-complete%20in%0Ageneral%2C%20we%20provide%20a%20comprehensive%20parameterized-complexity%20analysis%20with%20the%0Aaim%20of%20determining%20those%20properties%20of%20the%20problems%20that%20explain%20the%20hardness%0Aand%20those%20that%20make%20the%20problems%20tractable.%20For%20instance%2C%20we%20show%20that%20the%0Aproblems%20remain%20hard%20for%20a%20small%20number%20%24d%24%20of%20features%20or%20small%20domain%20size%0A%24D%24%20but%20the%20combination%20of%20both%20yields%20fixed-parameter%20tractability.%20That%20is%2C%0Athe%20problems%20are%20solvable%20in%20%24%28D%20%2B%201%29%5E%7B2d%7D%20%5Ccdot%20%7CI%7C%5E%7BO%281%29%7D%24%20time%2C%20where%20%24%7CI%7C%24%0Ais%20the%20size%20of%20the%20input.%20We%20also%20provide%20a%20proof-of-concept%20implementation%20of%0Athis%20algorithm%20and%20report%20on%20empirical%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12726v1&entry.124074799=Read"},
{"title": "Same model, better performance: the impact of shuffling on DNA Language\n  Models benchmarking", "author": "Davide Greco and Konrad Rawlik", "abstract": "  Large Language Models are increasingly popular in genomics due to their\npotential to decode complex biological sequences. Hence, researchers require a\nstandardized benchmark to evaluate DNA Language Models (DNA LMs) capabilities.\nHowever, evaluating DNA LMs is a complex task that intersects genomic's\ndomain-specific challenges and machine learning methodologies, where seemingly\nminor implementation details can significantly compromise benchmark validity.\nWe demonstrate this through BEND (Benchmarking DNA Language Models), where\nhardware-dependent hyperparameters -- number of data loading workers and buffer\nsizes -- create spurious performance variations of up to 4% for identical\nmodels. The problem stems from inadequate data shuffling interacting with\ndomain specific data characteristics. Experiments with three DNA language\nmodels (HyenaDNA, DNABERT-2, ResNet-LM) show these artifacts affect both\nabsolute performance and relative model rankings. We propose a simple solution:\npre-shuffling data before storage eliminates hardware dependencies while\nmaintaining efficiency. This work highlights how standard ML practices can\ninteract unexpectedly with domain-specific data characteristics, with broader\nimplications for benchmark design in specialized domains.\n", "link": "http://arxiv.org/abs/2510.12617v1", "date": "2025-10-14", "relevancy": 2.0922, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.53}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.53}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Same%20model%2C%20better%20performance%3A%20the%20impact%20of%20shuffling%20on%20DNA%20Language%0A%20%20Models%20benchmarking&body=Title%3A%20Same%20model%2C%20better%20performance%3A%20the%20impact%20of%20shuffling%20on%20DNA%20Language%0A%20%20Models%20benchmarking%0AAuthor%3A%20Davide%20Greco%20and%20Konrad%20Rawlik%0AAbstract%3A%20%20%20Large%20Language%20Models%20are%20increasingly%20popular%20in%20genomics%20due%20to%20their%0Apotential%20to%20decode%20complex%20biological%20sequences.%20Hence%2C%20researchers%20require%20a%0Astandardized%20benchmark%20to%20evaluate%20DNA%20Language%20Models%20%28DNA%20LMs%29%20capabilities.%0AHowever%2C%20evaluating%20DNA%20LMs%20is%20a%20complex%20task%20that%20intersects%20genomic%27s%0Adomain-specific%20challenges%20and%20machine%20learning%20methodologies%2C%20where%20seemingly%0Aminor%20implementation%20details%20can%20significantly%20compromise%20benchmark%20validity.%0AWe%20demonstrate%20this%20through%20BEND%20%28Benchmarking%20DNA%20Language%20Models%29%2C%20where%0Ahardware-dependent%20hyperparameters%20--%20number%20of%20data%20loading%20workers%20and%20buffer%0Asizes%20--%20create%20spurious%20performance%20variations%20of%20up%20to%204%25%20for%20identical%0Amodels.%20The%20problem%20stems%20from%20inadequate%20data%20shuffling%20interacting%20with%0Adomain%20specific%20data%20characteristics.%20Experiments%20with%20three%20DNA%20language%0Amodels%20%28HyenaDNA%2C%20DNABERT-2%2C%20ResNet-LM%29%20show%20these%20artifacts%20affect%20both%0Aabsolute%20performance%20and%20relative%20model%20rankings.%20We%20propose%20a%20simple%20solution%3A%0Apre-shuffling%20data%20before%20storage%20eliminates%20hardware%20dependencies%20while%0Amaintaining%20efficiency.%20This%20work%20highlights%20how%20standard%20ML%20practices%20can%0Ainteract%20unexpectedly%20with%20domain-specific%20data%20characteristics%2C%20with%20broader%0Aimplications%20for%20benchmark%20design%20in%20specialized%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSame%2520model%252C%2520better%2520performance%253A%2520the%2520impact%2520of%2520shuffling%2520on%2520DNA%2520Language%250A%2520%2520Models%2520benchmarking%26entry.906535625%3DDavide%2520Greco%2520and%2520Konrad%2520Rawlik%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520are%2520increasingly%2520popular%2520in%2520genomics%2520due%2520to%2520their%250Apotential%2520to%2520decode%2520complex%2520biological%2520sequences.%2520Hence%252C%2520researchers%2520require%2520a%250Astandardized%2520benchmark%2520to%2520evaluate%2520DNA%2520Language%2520Models%2520%2528DNA%2520LMs%2529%2520capabilities.%250AHowever%252C%2520evaluating%2520DNA%2520LMs%2520is%2520a%2520complex%2520task%2520that%2520intersects%2520genomic%2527s%250Adomain-specific%2520challenges%2520and%2520machine%2520learning%2520methodologies%252C%2520where%2520seemingly%250Aminor%2520implementation%2520details%2520can%2520significantly%2520compromise%2520benchmark%2520validity.%250AWe%2520demonstrate%2520this%2520through%2520BEND%2520%2528Benchmarking%2520DNA%2520Language%2520Models%2529%252C%2520where%250Ahardware-dependent%2520hyperparameters%2520--%2520number%2520of%2520data%2520loading%2520workers%2520and%2520buffer%250Asizes%2520--%2520create%2520spurious%2520performance%2520variations%2520of%2520up%2520to%25204%2525%2520for%2520identical%250Amodels.%2520The%2520problem%2520stems%2520from%2520inadequate%2520data%2520shuffling%2520interacting%2520with%250Adomain%2520specific%2520data%2520characteristics.%2520Experiments%2520with%2520three%2520DNA%2520language%250Amodels%2520%2528HyenaDNA%252C%2520DNABERT-2%252C%2520ResNet-LM%2529%2520show%2520these%2520artifacts%2520affect%2520both%250Aabsolute%2520performance%2520and%2520relative%2520model%2520rankings.%2520We%2520propose%2520a%2520simple%2520solution%253A%250Apre-shuffling%2520data%2520before%2520storage%2520eliminates%2520hardware%2520dependencies%2520while%250Amaintaining%2520efficiency.%2520This%2520work%2520highlights%2520how%2520standard%2520ML%2520practices%2520can%250Ainteract%2520unexpectedly%2520with%2520domain-specific%2520data%2520characteristics%252C%2520with%2520broader%250Aimplications%2520for%2520benchmark%2520design%2520in%2520specialized%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Same%20model%2C%20better%20performance%3A%20the%20impact%20of%20shuffling%20on%20DNA%20Language%0A%20%20Models%20benchmarking&entry.906535625=Davide%20Greco%20and%20Konrad%20Rawlik&entry.1292438233=%20%20Large%20Language%20Models%20are%20increasingly%20popular%20in%20genomics%20due%20to%20their%0Apotential%20to%20decode%20complex%20biological%20sequences.%20Hence%2C%20researchers%20require%20a%0Astandardized%20benchmark%20to%20evaluate%20DNA%20Language%20Models%20%28DNA%20LMs%29%20capabilities.%0AHowever%2C%20evaluating%20DNA%20LMs%20is%20a%20complex%20task%20that%20intersects%20genomic%27s%0Adomain-specific%20challenges%20and%20machine%20learning%20methodologies%2C%20where%20seemingly%0Aminor%20implementation%20details%20can%20significantly%20compromise%20benchmark%20validity.%0AWe%20demonstrate%20this%20through%20BEND%20%28Benchmarking%20DNA%20Language%20Models%29%2C%20where%0Ahardware-dependent%20hyperparameters%20--%20number%20of%20data%20loading%20workers%20and%20buffer%0Asizes%20--%20create%20spurious%20performance%20variations%20of%20up%20to%204%25%20for%20identical%0Amodels.%20The%20problem%20stems%20from%20inadequate%20data%20shuffling%20interacting%20with%0Adomain%20specific%20data%20characteristics.%20Experiments%20with%20three%20DNA%20language%0Amodels%20%28HyenaDNA%2C%20DNABERT-2%2C%20ResNet-LM%29%20show%20these%20artifacts%20affect%20both%0Aabsolute%20performance%20and%20relative%20model%20rankings.%20We%20propose%20a%20simple%20solution%3A%0Apre-shuffling%20data%20before%20storage%20eliminates%20hardware%20dependencies%20while%0Amaintaining%20efficiency.%20This%20work%20highlights%20how%20standard%20ML%20practices%20can%0Ainteract%20unexpectedly%20with%20domain-specific%20data%20characteristics%2C%20with%20broader%0Aimplications%20for%20benchmark%20design%20in%20specialized%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12617v1&entry.124074799=Read"},
{"title": "Integration of the TIAGo Robot into Isaac Sim with Mecanum Drive\n  Modeling and Learned S-Curve Velocity Profiles", "author": "Vincent Schoenbach and Marvin Wiedemann and Raphael Memmesheimer and Malte Mosbach and Sven Behnke", "abstract": "  Efficient physics simulation has significantly accelerated research progress\nin robotics applications such as grasping and assembly. The advent of\nGPU-accelerated simulation frameworks like Isaac Sim has particularly empowered\nlearning-based methods, enabling them to tackle increasingly complex tasks. The\nPAL Robotics TIAGo++ Omni is a versatile mobile manipulator equipped with a\nmecanum-wheeled base, allowing omnidirectional movement and a wide range of\ntask capabilities. However, until now, no model of the robot has been available\nin Isaac Sim. In this paper, we introduce such a model, calibrated to\napproximate the behavior of the real robot, with a focus on its omnidirectional\ndrive dynamics. We present two control models for the omnidirectional drive: a\nphysically accurate model that replicates real-world wheel dynamics and a\nlightweight velocity-based model optimized for learning-based applications.\nWith these models, we introduce a learning-based calibration approach to\napproximate the real robot's S-shaped velocity profile using minimal trajectory\ndata recordings. This simulation should allow researchers to experiment with\nthe robot and perform efficient learning-based control in diverse environments.\nWe provide the integration publicly at https://github.com/AIS-Bonn/tiago_isaac.\n", "link": "http://arxiv.org/abs/2510.10273v2", "date": "2025-10-14", "relevancy": 2.091, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5696}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5194}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integration%20of%20the%20TIAGo%20Robot%20into%20Isaac%20Sim%20with%20Mecanum%20Drive%0A%20%20Modeling%20and%20Learned%20S-Curve%20Velocity%20Profiles&body=Title%3A%20Integration%20of%20the%20TIAGo%20Robot%20into%20Isaac%20Sim%20with%20Mecanum%20Drive%0A%20%20Modeling%20and%20Learned%20S-Curve%20Velocity%20Profiles%0AAuthor%3A%20Vincent%20Schoenbach%20and%20Marvin%20Wiedemann%20and%20Raphael%20Memmesheimer%20and%20Malte%20Mosbach%20and%20Sven%20Behnke%0AAbstract%3A%20%20%20Efficient%20physics%20simulation%20has%20significantly%20accelerated%20research%20progress%0Ain%20robotics%20applications%20such%20as%20grasping%20and%20assembly.%20The%20advent%20of%0AGPU-accelerated%20simulation%20frameworks%20like%20Isaac%20Sim%20has%20particularly%20empowered%0Alearning-based%20methods%2C%20enabling%20them%20to%20tackle%20increasingly%20complex%20tasks.%20The%0APAL%20Robotics%20TIAGo%2B%2B%20Omni%20is%20a%20versatile%20mobile%20manipulator%20equipped%20with%20a%0Amecanum-wheeled%20base%2C%20allowing%20omnidirectional%20movement%20and%20a%20wide%20range%20of%0Atask%20capabilities.%20However%2C%20until%20now%2C%20no%20model%20of%20the%20robot%20has%20been%20available%0Ain%20Isaac%20Sim.%20In%20this%20paper%2C%20we%20introduce%20such%20a%20model%2C%20calibrated%20to%0Aapproximate%20the%20behavior%20of%20the%20real%20robot%2C%20with%20a%20focus%20on%20its%20omnidirectional%0Adrive%20dynamics.%20We%20present%20two%20control%20models%20for%20the%20omnidirectional%20drive%3A%20a%0Aphysically%20accurate%20model%20that%20replicates%20real-world%20wheel%20dynamics%20and%20a%0Alightweight%20velocity-based%20model%20optimized%20for%20learning-based%20applications.%0AWith%20these%20models%2C%20we%20introduce%20a%20learning-based%20calibration%20approach%20to%0Aapproximate%20the%20real%20robot%27s%20S-shaped%20velocity%20profile%20using%20minimal%20trajectory%0Adata%20recordings.%20This%20simulation%20should%20allow%20researchers%20to%20experiment%20with%0Athe%20robot%20and%20perform%20efficient%20learning-based%20control%20in%20diverse%20environments.%0AWe%20provide%20the%20integration%20publicly%20at%20https%3A//github.com/AIS-Bonn/tiago_isaac.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.10273v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegration%2520of%2520the%2520TIAGo%2520Robot%2520into%2520Isaac%2520Sim%2520with%2520Mecanum%2520Drive%250A%2520%2520Modeling%2520and%2520Learned%2520S-Curve%2520Velocity%2520Profiles%26entry.906535625%3DVincent%2520Schoenbach%2520and%2520Marvin%2520Wiedemann%2520and%2520Raphael%2520Memmesheimer%2520and%2520Malte%2520Mosbach%2520and%2520Sven%2520Behnke%26entry.1292438233%3D%2520%2520Efficient%2520physics%2520simulation%2520has%2520significantly%2520accelerated%2520research%2520progress%250Ain%2520robotics%2520applications%2520such%2520as%2520grasping%2520and%2520assembly.%2520The%2520advent%2520of%250AGPU-accelerated%2520simulation%2520frameworks%2520like%2520Isaac%2520Sim%2520has%2520particularly%2520empowered%250Alearning-based%2520methods%252C%2520enabling%2520them%2520to%2520tackle%2520increasingly%2520complex%2520tasks.%2520The%250APAL%2520Robotics%2520TIAGo%252B%252B%2520Omni%2520is%2520a%2520versatile%2520mobile%2520manipulator%2520equipped%2520with%2520a%250Amecanum-wheeled%2520base%252C%2520allowing%2520omnidirectional%2520movement%2520and%2520a%2520wide%2520range%2520of%250Atask%2520capabilities.%2520However%252C%2520until%2520now%252C%2520no%2520model%2520of%2520the%2520robot%2520has%2520been%2520available%250Ain%2520Isaac%2520Sim.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520such%2520a%2520model%252C%2520calibrated%2520to%250Aapproximate%2520the%2520behavior%2520of%2520the%2520real%2520robot%252C%2520with%2520a%2520focus%2520on%2520its%2520omnidirectional%250Adrive%2520dynamics.%2520We%2520present%2520two%2520control%2520models%2520for%2520the%2520omnidirectional%2520drive%253A%2520a%250Aphysically%2520accurate%2520model%2520that%2520replicates%2520real-world%2520wheel%2520dynamics%2520and%2520a%250Alightweight%2520velocity-based%2520model%2520optimized%2520for%2520learning-based%2520applications.%250AWith%2520these%2520models%252C%2520we%2520introduce%2520a%2520learning-based%2520calibration%2520approach%2520to%250Aapproximate%2520the%2520real%2520robot%2527s%2520S-shaped%2520velocity%2520profile%2520using%2520minimal%2520trajectory%250Adata%2520recordings.%2520This%2520simulation%2520should%2520allow%2520researchers%2520to%2520experiment%2520with%250Athe%2520robot%2520and%2520perform%2520efficient%2520learning-based%2520control%2520in%2520diverse%2520environments.%250AWe%2520provide%2520the%2520integration%2520publicly%2520at%2520https%253A//github.com/AIS-Bonn/tiago_isaac.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10273v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integration%20of%20the%20TIAGo%20Robot%20into%20Isaac%20Sim%20with%20Mecanum%20Drive%0A%20%20Modeling%20and%20Learned%20S-Curve%20Velocity%20Profiles&entry.906535625=Vincent%20Schoenbach%20and%20Marvin%20Wiedemann%20and%20Raphael%20Memmesheimer%20and%20Malte%20Mosbach%20and%20Sven%20Behnke&entry.1292438233=%20%20Efficient%20physics%20simulation%20has%20significantly%20accelerated%20research%20progress%0Ain%20robotics%20applications%20such%20as%20grasping%20and%20assembly.%20The%20advent%20of%0AGPU-accelerated%20simulation%20frameworks%20like%20Isaac%20Sim%20has%20particularly%20empowered%0Alearning-based%20methods%2C%20enabling%20them%20to%20tackle%20increasingly%20complex%20tasks.%20The%0APAL%20Robotics%20TIAGo%2B%2B%20Omni%20is%20a%20versatile%20mobile%20manipulator%20equipped%20with%20a%0Amecanum-wheeled%20base%2C%20allowing%20omnidirectional%20movement%20and%20a%20wide%20range%20of%0Atask%20capabilities.%20However%2C%20until%20now%2C%20no%20model%20of%20the%20robot%20has%20been%20available%0Ain%20Isaac%20Sim.%20In%20this%20paper%2C%20we%20introduce%20such%20a%20model%2C%20calibrated%20to%0Aapproximate%20the%20behavior%20of%20the%20real%20robot%2C%20with%20a%20focus%20on%20its%20omnidirectional%0Adrive%20dynamics.%20We%20present%20two%20control%20models%20for%20the%20omnidirectional%20drive%3A%20a%0Aphysically%20accurate%20model%20that%20replicates%20real-world%20wheel%20dynamics%20and%20a%0Alightweight%20velocity-based%20model%20optimized%20for%20learning-based%20applications.%0AWith%20these%20models%2C%20we%20introduce%20a%20learning-based%20calibration%20approach%20to%0Aapproximate%20the%20real%20robot%27s%20S-shaped%20velocity%20profile%20using%20minimal%20trajectory%0Adata%20recordings.%20This%20simulation%20should%20allow%20researchers%20to%20experiment%20with%0Athe%20robot%20and%20perform%20efficient%20learning-based%20control%20in%20diverse%20environments.%0AWe%20provide%20the%20integration%20publicly%20at%20https%3A//github.com/AIS-Bonn/tiago_isaac.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.10273v2&entry.124074799=Read"},
{"title": "Voronoi-Assisted Diffusion for Computing Unsigned Distance Fields from\n  Unoriented Points", "author": "Jiayi Kong and Chen Zong and Junkai Deng and Xuhui Chen and Fei Hou and Shiqing Xin and Junhui Hou and Chen Qian and Ying He", "abstract": "  Unsigned Distance Fields (UDFs) provide a flexible representation for 3D\nshapes with arbitrary topology, including open and closed surfaces, orientable\nand non-orientable geometries, and non-manifold structures. While recent neural\napproaches have shown promise in learning UDFs, they often suffer from\nnumerical instability, high computational cost, and limited controllability. We\npresent a lightweight, network-free method, Voronoi-Assisted Diffusion (VAD),\nfor computing UDFs directly from unoriented point clouds. Our approach begins\nby assigning bi-directional normals to input points, guided by two\nVoronoi-based geometric criteria encoded in an energy function for optimal\nalignment. The aligned normals are then diffused to form an approximate UDF\ngradient field, which is subsequently integrated to recover the final UDF.\nExperiments demonstrate that VAD robustly handles watertight and open surfaces,\nas well as complex non-manifold and non-orientable geometries, while remaining\ncomputationally efficient and stable.\n", "link": "http://arxiv.org/abs/2510.12524v1", "date": "2025-10-14", "relevancy": 2.0888, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5349}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5219}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Voronoi-Assisted%20Diffusion%20for%20Computing%20Unsigned%20Distance%20Fields%20from%0A%20%20Unoriented%20Points&body=Title%3A%20Voronoi-Assisted%20Diffusion%20for%20Computing%20Unsigned%20Distance%20Fields%20from%0A%20%20Unoriented%20Points%0AAuthor%3A%20Jiayi%20Kong%20and%20Chen%20Zong%20and%20Junkai%20Deng%20and%20Xuhui%20Chen%20and%20Fei%20Hou%20and%20Shiqing%20Xin%20and%20Junhui%20Hou%20and%20Chen%20Qian%20and%20Ying%20He%0AAbstract%3A%20%20%20Unsigned%20Distance%20Fields%20%28UDFs%29%20provide%20a%20flexible%20representation%20for%203D%0Ashapes%20with%20arbitrary%20topology%2C%20including%20open%20and%20closed%20surfaces%2C%20orientable%0Aand%20non-orientable%20geometries%2C%20and%20non-manifold%20structures.%20While%20recent%20neural%0Aapproaches%20have%20shown%20promise%20in%20learning%20UDFs%2C%20they%20often%20suffer%20from%0Anumerical%20instability%2C%20high%20computational%20cost%2C%20and%20limited%20controllability.%20We%0Apresent%20a%20lightweight%2C%20network-free%20method%2C%20Voronoi-Assisted%20Diffusion%20%28VAD%29%2C%0Afor%20computing%20UDFs%20directly%20from%20unoriented%20point%20clouds.%20Our%20approach%20begins%0Aby%20assigning%20bi-directional%20normals%20to%20input%20points%2C%20guided%20by%20two%0AVoronoi-based%20geometric%20criteria%20encoded%20in%20an%20energy%20function%20for%20optimal%0Aalignment.%20The%20aligned%20normals%20are%20then%20diffused%20to%20form%20an%20approximate%20UDF%0Agradient%20field%2C%20which%20is%20subsequently%20integrated%20to%20recover%20the%20final%20UDF.%0AExperiments%20demonstrate%20that%20VAD%20robustly%20handles%20watertight%20and%20open%20surfaces%2C%0Aas%20well%20as%20complex%20non-manifold%20and%20non-orientable%20geometries%2C%20while%20remaining%0Acomputationally%20efficient%20and%20stable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoronoi-Assisted%2520Diffusion%2520for%2520Computing%2520Unsigned%2520Distance%2520Fields%2520from%250A%2520%2520Unoriented%2520Points%26entry.906535625%3DJiayi%2520Kong%2520and%2520Chen%2520Zong%2520and%2520Junkai%2520Deng%2520and%2520Xuhui%2520Chen%2520and%2520Fei%2520Hou%2520and%2520Shiqing%2520Xin%2520and%2520Junhui%2520Hou%2520and%2520Chen%2520Qian%2520and%2520Ying%2520He%26entry.1292438233%3D%2520%2520Unsigned%2520Distance%2520Fields%2520%2528UDFs%2529%2520provide%2520a%2520flexible%2520representation%2520for%25203D%250Ashapes%2520with%2520arbitrary%2520topology%252C%2520including%2520open%2520and%2520closed%2520surfaces%252C%2520orientable%250Aand%2520non-orientable%2520geometries%252C%2520and%2520non-manifold%2520structures.%2520While%2520recent%2520neural%250Aapproaches%2520have%2520shown%2520promise%2520in%2520learning%2520UDFs%252C%2520they%2520often%2520suffer%2520from%250Anumerical%2520instability%252C%2520high%2520computational%2520cost%252C%2520and%2520limited%2520controllability.%2520We%250Apresent%2520a%2520lightweight%252C%2520network-free%2520method%252C%2520Voronoi-Assisted%2520Diffusion%2520%2528VAD%2529%252C%250Afor%2520computing%2520UDFs%2520directly%2520from%2520unoriented%2520point%2520clouds.%2520Our%2520approach%2520begins%250Aby%2520assigning%2520bi-directional%2520normals%2520to%2520input%2520points%252C%2520guided%2520by%2520two%250AVoronoi-based%2520geometric%2520criteria%2520encoded%2520in%2520an%2520energy%2520function%2520for%2520optimal%250Aalignment.%2520The%2520aligned%2520normals%2520are%2520then%2520diffused%2520to%2520form%2520an%2520approximate%2520UDF%250Agradient%2520field%252C%2520which%2520is%2520subsequently%2520integrated%2520to%2520recover%2520the%2520final%2520UDF.%250AExperiments%2520demonstrate%2520that%2520VAD%2520robustly%2520handles%2520watertight%2520and%2520open%2520surfaces%252C%250Aas%2520well%2520as%2520complex%2520non-manifold%2520and%2520non-orientable%2520geometries%252C%2520while%2520remaining%250Acomputationally%2520efficient%2520and%2520stable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Voronoi-Assisted%20Diffusion%20for%20Computing%20Unsigned%20Distance%20Fields%20from%0A%20%20Unoriented%20Points&entry.906535625=Jiayi%20Kong%20and%20Chen%20Zong%20and%20Junkai%20Deng%20and%20Xuhui%20Chen%20and%20Fei%20Hou%20and%20Shiqing%20Xin%20and%20Junhui%20Hou%20and%20Chen%20Qian%20and%20Ying%20He&entry.1292438233=%20%20Unsigned%20Distance%20Fields%20%28UDFs%29%20provide%20a%20flexible%20representation%20for%203D%0Ashapes%20with%20arbitrary%20topology%2C%20including%20open%20and%20closed%20surfaces%2C%20orientable%0Aand%20non-orientable%20geometries%2C%20and%20non-manifold%20structures.%20While%20recent%20neural%0Aapproaches%20have%20shown%20promise%20in%20learning%20UDFs%2C%20they%20often%20suffer%20from%0Anumerical%20instability%2C%20high%20computational%20cost%2C%20and%20limited%20controllability.%20We%0Apresent%20a%20lightweight%2C%20network-free%20method%2C%20Voronoi-Assisted%20Diffusion%20%28VAD%29%2C%0Afor%20computing%20UDFs%20directly%20from%20unoriented%20point%20clouds.%20Our%20approach%20begins%0Aby%20assigning%20bi-directional%20normals%20to%20input%20points%2C%20guided%20by%20two%0AVoronoi-based%20geometric%20criteria%20encoded%20in%20an%20energy%20function%20for%20optimal%0Aalignment.%20The%20aligned%20normals%20are%20then%20diffused%20to%20form%20an%20approximate%20UDF%0Agradient%20field%2C%20which%20is%20subsequently%20integrated%20to%20recover%20the%20final%20UDF.%0AExperiments%20demonstrate%20that%20VAD%20robustly%20handles%20watertight%20and%20open%20surfaces%2C%0Aas%20well%20as%20complex%20non-manifold%20and%20non-orientable%20geometries%2C%20while%20remaining%0Acomputationally%20efficient%20and%20stable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12524v1&entry.124074799=Read"},
{"title": "J-RAS: Enhancing Medical Image Segmentation via Retrieval-Augmented\n  Joint Training", "author": "Salma J. Ahmed and Emad A. Mohammed and Azam Asilian Bidgoli", "abstract": "  Image segmentation, the process of dividing images into meaningful regions,\nis critical in medical applications for accurate diagnosis, treatment planning,\nand disease monitoring. Although manual segmentation by healthcare\nprofessionals produces precise outcomes, it is time-consuming, costly, and\nprone to variability due to differences in human expertise. Artificial\nintelligence (AI)-based methods have been developed to address these\nlimitations by automating segmentation tasks; however, they often require\nlarge, annotated datasets that are rarely available in practice and frequently\nstruggle to generalize across diverse imaging conditions due to inter-patient\nvariability and rare pathological cases. In this paper, we propose Joint\nRetrieval Augmented Segmentation (J-RAS), a joint training method for guided\nimage segmentation that integrates a segmentation model with a retrieval model.\nBoth models are jointly optimized, enabling the segmentation model to leverage\nretrieved image-mask pairs to enrich its anatomical understanding, while the\nretrieval model learns segmentation-relevant features beyond simple visual\nsimilarity. This joint optimization ensures that retrieval actively contributes\nmeaningful contextual cues to guide boundary delineation, thereby enhancing the\noverall segmentation performance. We validate J-RAS across multiple\nsegmentation backbones, including U-Net, TransUNet, SAM, and SegFormer, on two\nbenchmark datasets: ACDC and M&Ms, demonstrating consistent improvements. For\nexample, on the ACDC dataset, SegFormer without J-RAS achieves a mean Dice\nscore of 0.8708$\\pm$0.042 and a mean Hausdorff Distance (HD) of\n1.8130$\\pm$2.49, whereas with J-RAS, the performance improves substantially to\na mean Dice score of 0.9115$\\pm$0.031 and a mean HD of 1.1489$\\pm$0.30. These\nresults highlight the method's effectiveness and its generalizability across\narchitectures and datasets.\n", "link": "http://arxiv.org/abs/2510.09953v2", "date": "2025-10-14", "relevancy": 2.0856, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5384}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5187}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20J-RAS%3A%20Enhancing%20Medical%20Image%20Segmentation%20via%20Retrieval-Augmented%0A%20%20Joint%20Training&body=Title%3A%20J-RAS%3A%20Enhancing%20Medical%20Image%20Segmentation%20via%20Retrieval-Augmented%0A%20%20Joint%20Training%0AAuthor%3A%20Salma%20J.%20Ahmed%20and%20Emad%20A.%20Mohammed%20and%20Azam%20Asilian%20Bidgoli%0AAbstract%3A%20%20%20Image%20segmentation%2C%20the%20process%20of%20dividing%20images%20into%20meaningful%20regions%2C%0Ais%20critical%20in%20medical%20applications%20for%20accurate%20diagnosis%2C%20treatment%20planning%2C%0Aand%20disease%20monitoring.%20Although%20manual%20segmentation%20by%20healthcare%0Aprofessionals%20produces%20precise%20outcomes%2C%20it%20is%20time-consuming%2C%20costly%2C%20and%0Aprone%20to%20variability%20due%20to%20differences%20in%20human%20expertise.%20Artificial%0Aintelligence%20%28AI%29-based%20methods%20have%20been%20developed%20to%20address%20these%0Alimitations%20by%20automating%20segmentation%20tasks%3B%20however%2C%20they%20often%20require%0Alarge%2C%20annotated%20datasets%20that%20are%20rarely%20available%20in%20practice%20and%20frequently%0Astruggle%20to%20generalize%20across%20diverse%20imaging%20conditions%20due%20to%20inter-patient%0Avariability%20and%20rare%20pathological%20cases.%20In%20this%20paper%2C%20we%20propose%20Joint%0ARetrieval%20Augmented%20Segmentation%20%28J-RAS%29%2C%20a%20joint%20training%20method%20for%20guided%0Aimage%20segmentation%20that%20integrates%20a%20segmentation%20model%20with%20a%20retrieval%20model.%0ABoth%20models%20are%20jointly%20optimized%2C%20enabling%20the%20segmentation%20model%20to%20leverage%0Aretrieved%20image-mask%20pairs%20to%20enrich%20its%20anatomical%20understanding%2C%20while%20the%0Aretrieval%20model%20learns%20segmentation-relevant%20features%20beyond%20simple%20visual%0Asimilarity.%20This%20joint%20optimization%20ensures%20that%20retrieval%20actively%20contributes%0Ameaningful%20contextual%20cues%20to%20guide%20boundary%20delineation%2C%20thereby%20enhancing%20the%0Aoverall%20segmentation%20performance.%20We%20validate%20J-RAS%20across%20multiple%0Asegmentation%20backbones%2C%20including%20U-Net%2C%20TransUNet%2C%20SAM%2C%20and%20SegFormer%2C%20on%20two%0Abenchmark%20datasets%3A%20ACDC%20and%20M%26Ms%2C%20demonstrating%20consistent%20improvements.%20For%0Aexample%2C%20on%20the%20ACDC%20dataset%2C%20SegFormer%20without%20J-RAS%20achieves%20a%20mean%20Dice%0Ascore%20of%200.8708%24%5Cpm%240.042%20and%20a%20mean%20Hausdorff%20Distance%20%28HD%29%20of%0A1.8130%24%5Cpm%242.49%2C%20whereas%20with%20J-RAS%2C%20the%20performance%20improves%20substantially%20to%0Aa%20mean%20Dice%20score%20of%200.9115%24%5Cpm%240.031%20and%20a%20mean%20HD%20of%201.1489%24%5Cpm%240.30.%20These%0Aresults%20highlight%20the%20method%27s%20effectiveness%20and%20its%20generalizability%20across%0Aarchitectures%20and%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09953v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJ-RAS%253A%2520Enhancing%2520Medical%2520Image%2520Segmentation%2520via%2520Retrieval-Augmented%250A%2520%2520Joint%2520Training%26entry.906535625%3DSalma%2520J.%2520Ahmed%2520and%2520Emad%2520A.%2520Mohammed%2520and%2520Azam%2520Asilian%2520Bidgoli%26entry.1292438233%3D%2520%2520Image%2520segmentation%252C%2520the%2520process%2520of%2520dividing%2520images%2520into%2520meaningful%2520regions%252C%250Ais%2520critical%2520in%2520medical%2520applications%2520for%2520accurate%2520diagnosis%252C%2520treatment%2520planning%252C%250Aand%2520disease%2520monitoring.%2520Although%2520manual%2520segmentation%2520by%2520healthcare%250Aprofessionals%2520produces%2520precise%2520outcomes%252C%2520it%2520is%2520time-consuming%252C%2520costly%252C%2520and%250Aprone%2520to%2520variability%2520due%2520to%2520differences%2520in%2520human%2520expertise.%2520Artificial%250Aintelligence%2520%2528AI%2529-based%2520methods%2520have%2520been%2520developed%2520to%2520address%2520these%250Alimitations%2520by%2520automating%2520segmentation%2520tasks%253B%2520however%252C%2520they%2520often%2520require%250Alarge%252C%2520annotated%2520datasets%2520that%2520are%2520rarely%2520available%2520in%2520practice%2520and%2520frequently%250Astruggle%2520to%2520generalize%2520across%2520diverse%2520imaging%2520conditions%2520due%2520to%2520inter-patient%250Avariability%2520and%2520rare%2520pathological%2520cases.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Joint%250ARetrieval%2520Augmented%2520Segmentation%2520%2528J-RAS%2529%252C%2520a%2520joint%2520training%2520method%2520for%2520guided%250Aimage%2520segmentation%2520that%2520integrates%2520a%2520segmentation%2520model%2520with%2520a%2520retrieval%2520model.%250ABoth%2520models%2520are%2520jointly%2520optimized%252C%2520enabling%2520the%2520segmentation%2520model%2520to%2520leverage%250Aretrieved%2520image-mask%2520pairs%2520to%2520enrich%2520its%2520anatomical%2520understanding%252C%2520while%2520the%250Aretrieval%2520model%2520learns%2520segmentation-relevant%2520features%2520beyond%2520simple%2520visual%250Asimilarity.%2520This%2520joint%2520optimization%2520ensures%2520that%2520retrieval%2520actively%2520contributes%250Ameaningful%2520contextual%2520cues%2520to%2520guide%2520boundary%2520delineation%252C%2520thereby%2520enhancing%2520the%250Aoverall%2520segmentation%2520performance.%2520We%2520validate%2520J-RAS%2520across%2520multiple%250Asegmentation%2520backbones%252C%2520including%2520U-Net%252C%2520TransUNet%252C%2520SAM%252C%2520and%2520SegFormer%252C%2520on%2520two%250Abenchmark%2520datasets%253A%2520ACDC%2520and%2520M%2526Ms%252C%2520demonstrating%2520consistent%2520improvements.%2520For%250Aexample%252C%2520on%2520the%2520ACDC%2520dataset%252C%2520SegFormer%2520without%2520J-RAS%2520achieves%2520a%2520mean%2520Dice%250Ascore%2520of%25200.8708%2524%255Cpm%25240.042%2520and%2520a%2520mean%2520Hausdorff%2520Distance%2520%2528HD%2529%2520of%250A1.8130%2524%255Cpm%25242.49%252C%2520whereas%2520with%2520J-RAS%252C%2520the%2520performance%2520improves%2520substantially%2520to%250Aa%2520mean%2520Dice%2520score%2520of%25200.9115%2524%255Cpm%25240.031%2520and%2520a%2520mean%2520HD%2520of%25201.1489%2524%255Cpm%25240.30.%2520These%250Aresults%2520highlight%2520the%2520method%2527s%2520effectiveness%2520and%2520its%2520generalizability%2520across%250Aarchitectures%2520and%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09953v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=J-RAS%3A%20Enhancing%20Medical%20Image%20Segmentation%20via%20Retrieval-Augmented%0A%20%20Joint%20Training&entry.906535625=Salma%20J.%20Ahmed%20and%20Emad%20A.%20Mohammed%20and%20Azam%20Asilian%20Bidgoli&entry.1292438233=%20%20Image%20segmentation%2C%20the%20process%20of%20dividing%20images%20into%20meaningful%20regions%2C%0Ais%20critical%20in%20medical%20applications%20for%20accurate%20diagnosis%2C%20treatment%20planning%2C%0Aand%20disease%20monitoring.%20Although%20manual%20segmentation%20by%20healthcare%0Aprofessionals%20produces%20precise%20outcomes%2C%20it%20is%20time-consuming%2C%20costly%2C%20and%0Aprone%20to%20variability%20due%20to%20differences%20in%20human%20expertise.%20Artificial%0Aintelligence%20%28AI%29-based%20methods%20have%20been%20developed%20to%20address%20these%0Alimitations%20by%20automating%20segmentation%20tasks%3B%20however%2C%20they%20often%20require%0Alarge%2C%20annotated%20datasets%20that%20are%20rarely%20available%20in%20practice%20and%20frequently%0Astruggle%20to%20generalize%20across%20diverse%20imaging%20conditions%20due%20to%20inter-patient%0Avariability%20and%20rare%20pathological%20cases.%20In%20this%20paper%2C%20we%20propose%20Joint%0ARetrieval%20Augmented%20Segmentation%20%28J-RAS%29%2C%20a%20joint%20training%20method%20for%20guided%0Aimage%20segmentation%20that%20integrates%20a%20segmentation%20model%20with%20a%20retrieval%20model.%0ABoth%20models%20are%20jointly%20optimized%2C%20enabling%20the%20segmentation%20model%20to%20leverage%0Aretrieved%20image-mask%20pairs%20to%20enrich%20its%20anatomical%20understanding%2C%20while%20the%0Aretrieval%20model%20learns%20segmentation-relevant%20features%20beyond%20simple%20visual%0Asimilarity.%20This%20joint%20optimization%20ensures%20that%20retrieval%20actively%20contributes%0Ameaningful%20contextual%20cues%20to%20guide%20boundary%20delineation%2C%20thereby%20enhancing%20the%0Aoverall%20segmentation%20performance.%20We%20validate%20J-RAS%20across%20multiple%0Asegmentation%20backbones%2C%20including%20U-Net%2C%20TransUNet%2C%20SAM%2C%20and%20SegFormer%2C%20on%20two%0Abenchmark%20datasets%3A%20ACDC%20and%20M%26Ms%2C%20demonstrating%20consistent%20improvements.%20For%0Aexample%2C%20on%20the%20ACDC%20dataset%2C%20SegFormer%20without%20J-RAS%20achieves%20a%20mean%20Dice%0Ascore%20of%200.8708%24%5Cpm%240.042%20and%20a%20mean%20Hausdorff%20Distance%20%28HD%29%20of%0A1.8130%24%5Cpm%242.49%2C%20whereas%20with%20J-RAS%2C%20the%20performance%20improves%20substantially%20to%0Aa%20mean%20Dice%20score%20of%200.9115%24%5Cpm%240.031%20and%20a%20mean%20HD%20of%201.1489%24%5Cpm%240.30.%20These%0Aresults%20highlight%20the%20method%27s%20effectiveness%20and%20its%20generalizability%20across%0Aarchitectures%20and%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09953v2&entry.124074799=Read"},
{"title": "Efficient Perceptual Image Super Resolution: AIM 2025 Study and\n  Benchmark", "author": "Bruno Longarela and Marcos V. Conde and Alvaro Garcia and Radu Timofte", "abstract": "  This paper presents a comprehensive study and benchmark on Efficient\nPerceptual Super-Resolution (EPSR). While significant progress has been made in\nefficient PSNR-oriented super resolution, approaches focusing on perceptual\nquality metrics remain relatively inefficient. Motivated by this gap, we aim to\nreplicate or improve the perceptual results of Real-ESRGAN while meeting strict\nefficiency constraints: a maximum of 5M parameters and 2000 GFLOPs, calculated\nfor an input size of 960x540 pixels. The proposed solutions were evaluated on a\nnovel dataset consisting of 500 test images of 4K resolution, each degraded\nusing multiple degradation types, without providing the original high-quality\ncounterparts. This design aims to reflect realistic deployment conditions and\nserves as a diverse and challenging benchmark. The top-performing approach\nmanages to outperform Real-ESRGAN across all benchmark datasets, demonstrating\nthe potential of efficient methods in the perceptual domain. This paper\nestablishes the modern baselines for efficient perceptual super resolution.\n", "link": "http://arxiv.org/abs/2510.12765v1", "date": "2025-10-14", "relevancy": 2.0808, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5347}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5107}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Perceptual%20Image%20Super%20Resolution%3A%20AIM%202025%20Study%20and%0A%20%20Benchmark&body=Title%3A%20Efficient%20Perceptual%20Image%20Super%20Resolution%3A%20AIM%202025%20Study%20and%0A%20%20Benchmark%0AAuthor%3A%20Bruno%20Longarela%20and%20Marcos%20V.%20Conde%20and%20Alvaro%20Garcia%20and%20Radu%20Timofte%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20comprehensive%20study%20and%20benchmark%20on%20Efficient%0APerceptual%20Super-Resolution%20%28EPSR%29.%20While%20significant%20progress%20has%20been%20made%20in%0Aefficient%20PSNR-oriented%20super%20resolution%2C%20approaches%20focusing%20on%20perceptual%0Aquality%20metrics%20remain%20relatively%20inefficient.%20Motivated%20by%20this%20gap%2C%20we%20aim%20to%0Areplicate%20or%20improve%20the%20perceptual%20results%20of%20Real-ESRGAN%20while%20meeting%20strict%0Aefficiency%20constraints%3A%20a%20maximum%20of%205M%20parameters%20and%202000%20GFLOPs%2C%20calculated%0Afor%20an%20input%20size%20of%20960x540%20pixels.%20The%20proposed%20solutions%20were%20evaluated%20on%20a%0Anovel%20dataset%20consisting%20of%20500%20test%20images%20of%204K%20resolution%2C%20each%20degraded%0Ausing%20multiple%20degradation%20types%2C%20without%20providing%20the%20original%20high-quality%0Acounterparts.%20This%20design%20aims%20to%20reflect%20realistic%20deployment%20conditions%20and%0Aserves%20as%20a%20diverse%20and%20challenging%20benchmark.%20The%20top-performing%20approach%0Amanages%20to%20outperform%20Real-ESRGAN%20across%20all%20benchmark%20datasets%2C%20demonstrating%0Athe%20potential%20of%20efficient%20methods%20in%20the%20perceptual%20domain.%20This%20paper%0Aestablishes%20the%20modern%20baselines%20for%20efficient%20perceptual%20super%20resolution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Perceptual%2520Image%2520Super%2520Resolution%253A%2520AIM%25202025%2520Study%2520and%250A%2520%2520Benchmark%26entry.906535625%3DBruno%2520Longarela%2520and%2520Marcos%2520V.%2520Conde%2520and%2520Alvaro%2520Garcia%2520and%2520Radu%2520Timofte%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520comprehensive%2520study%2520and%2520benchmark%2520on%2520Efficient%250APerceptual%2520Super-Resolution%2520%2528EPSR%2529.%2520While%2520significant%2520progress%2520has%2520been%2520made%2520in%250Aefficient%2520PSNR-oriented%2520super%2520resolution%252C%2520approaches%2520focusing%2520on%2520perceptual%250Aquality%2520metrics%2520remain%2520relatively%2520inefficient.%2520Motivated%2520by%2520this%2520gap%252C%2520we%2520aim%2520to%250Areplicate%2520or%2520improve%2520the%2520perceptual%2520results%2520of%2520Real-ESRGAN%2520while%2520meeting%2520strict%250Aefficiency%2520constraints%253A%2520a%2520maximum%2520of%25205M%2520parameters%2520and%25202000%2520GFLOPs%252C%2520calculated%250Afor%2520an%2520input%2520size%2520of%2520960x540%2520pixels.%2520The%2520proposed%2520solutions%2520were%2520evaluated%2520on%2520a%250Anovel%2520dataset%2520consisting%2520of%2520500%2520test%2520images%2520of%25204K%2520resolution%252C%2520each%2520degraded%250Ausing%2520multiple%2520degradation%2520types%252C%2520without%2520providing%2520the%2520original%2520high-quality%250Acounterparts.%2520This%2520design%2520aims%2520to%2520reflect%2520realistic%2520deployment%2520conditions%2520and%250Aserves%2520as%2520a%2520diverse%2520and%2520challenging%2520benchmark.%2520The%2520top-performing%2520approach%250Amanages%2520to%2520outperform%2520Real-ESRGAN%2520across%2520all%2520benchmark%2520datasets%252C%2520demonstrating%250Athe%2520potential%2520of%2520efficient%2520methods%2520in%2520the%2520perceptual%2520domain.%2520This%2520paper%250Aestablishes%2520the%2520modern%2520baselines%2520for%2520efficient%2520perceptual%2520super%2520resolution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Perceptual%20Image%20Super%20Resolution%3A%20AIM%202025%20Study%20and%0A%20%20Benchmark&entry.906535625=Bruno%20Longarela%20and%20Marcos%20V.%20Conde%20and%20Alvaro%20Garcia%20and%20Radu%20Timofte&entry.1292438233=%20%20This%20paper%20presents%20a%20comprehensive%20study%20and%20benchmark%20on%20Efficient%0APerceptual%20Super-Resolution%20%28EPSR%29.%20While%20significant%20progress%20has%20been%20made%20in%0Aefficient%20PSNR-oriented%20super%20resolution%2C%20approaches%20focusing%20on%20perceptual%0Aquality%20metrics%20remain%20relatively%20inefficient.%20Motivated%20by%20this%20gap%2C%20we%20aim%20to%0Areplicate%20or%20improve%20the%20perceptual%20results%20of%20Real-ESRGAN%20while%20meeting%20strict%0Aefficiency%20constraints%3A%20a%20maximum%20of%205M%20parameters%20and%202000%20GFLOPs%2C%20calculated%0Afor%20an%20input%20size%20of%20960x540%20pixels.%20The%20proposed%20solutions%20were%20evaluated%20on%20a%0Anovel%20dataset%20consisting%20of%20500%20test%20images%20of%204K%20resolution%2C%20each%20degraded%0Ausing%20multiple%20degradation%20types%2C%20without%20providing%20the%20original%20high-quality%0Acounterparts.%20This%20design%20aims%20to%20reflect%20realistic%20deployment%20conditions%20and%0Aserves%20as%20a%20diverse%20and%20challenging%20benchmark.%20The%20top-performing%20approach%0Amanages%20to%20outperform%20Real-ESRGAN%20across%20all%20benchmark%20datasets%2C%20demonstrating%0Athe%20potential%20of%20efficient%20methods%20in%20the%20perceptual%20domain.%20This%20paper%0Aestablishes%20the%20modern%20baselines%20for%20efficient%20perceptual%20super%20resolution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12765v1&entry.124074799=Read"},
{"title": "Optimistic Multi-Agent Policy Gradient", "author": "Wenshuai Zhao and Yi Zhao and Zhiyuan Li and Juho Kannala and Joni Pajarinen", "abstract": "  *Relative overgeneralization* (RO) occurs in cooperative multi-agent learning\ntasks when agents converge towards a suboptimal joint policy due to overfitting\nto suboptimal behavior of other agents. No methods have been proposed for\naddressing RO in multi-agent policy gradient (MAPG) methods although these\nmethods produce state-of-the-art results. To address this gap, we propose a\ngeneral, yet simple, framework to enable optimistic updates in MAPG methods\nthat alleviate the RO problem. Our approach involves clipping the advantage to\neliminate negative values, thereby facilitating optimistic updates in MAPG. The\noptimism prevents individual agents from quickly converging to a local optimum.\nAdditionally, we provide a formal analysis to show that the proposed method\nretains optimality at a fixed point. In extensive evaluations on a diverse set\nof tasks including the *Multi-agent MuJoCo* and *Overcooked* benchmarks, our\nmethod outperforms strong baselines on 13 out of 19 tested tasks and matches\nthe performance on the rest.\n", "link": "http://arxiv.org/abs/2311.01953v3", "date": "2025-10-14", "relevancy": 2.0803, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5649}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5194}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimistic%20Multi-Agent%20Policy%20Gradient&body=Title%3A%20Optimistic%20Multi-Agent%20Policy%20Gradient%0AAuthor%3A%20Wenshuai%20Zhao%20and%20Yi%20Zhao%20and%20Zhiyuan%20Li%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen%0AAbstract%3A%20%20%20%2ARelative%20overgeneralization%2A%20%28RO%29%20occurs%20in%20cooperative%20multi-agent%20learning%0Atasks%20when%20agents%20converge%20towards%20a%20suboptimal%20joint%20policy%20due%20to%20overfitting%0Ato%20suboptimal%20behavior%20of%20other%20agents.%20No%20methods%20have%20been%20proposed%20for%0Aaddressing%20RO%20in%20multi-agent%20policy%20gradient%20%28MAPG%29%20methods%20although%20these%0Amethods%20produce%20state-of-the-art%20results.%20To%20address%20this%20gap%2C%20we%20propose%20a%0Ageneral%2C%20yet%20simple%2C%20framework%20to%20enable%20optimistic%20updates%20in%20MAPG%20methods%0Athat%20alleviate%20the%20RO%20problem.%20Our%20approach%20involves%20clipping%20the%20advantage%20to%0Aeliminate%20negative%20values%2C%20thereby%20facilitating%20optimistic%20updates%20in%20MAPG.%20The%0Aoptimism%20prevents%20individual%20agents%20from%20quickly%20converging%20to%20a%20local%20optimum.%0AAdditionally%2C%20we%20provide%20a%20formal%20analysis%20to%20show%20that%20the%20proposed%20method%0Aretains%20optimality%20at%20a%20fixed%20point.%20In%20extensive%20evaluations%20on%20a%20diverse%20set%0Aof%20tasks%20including%20the%20%2AMulti-agent%20MuJoCo%2A%20and%20%2AOvercooked%2A%20benchmarks%2C%20our%0Amethod%20outperforms%20strong%20baselines%20on%2013%20out%20of%2019%20tested%20tasks%20and%20matches%0Athe%20performance%20on%20the%20rest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.01953v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimistic%2520Multi-Agent%2520Policy%2520Gradient%26entry.906535625%3DWenshuai%2520Zhao%2520and%2520Yi%2520Zhao%2520and%2520Zhiyuan%2520Li%2520and%2520Juho%2520Kannala%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3D%2520%2520%252ARelative%2520overgeneralization%252A%2520%2528RO%2529%2520occurs%2520in%2520cooperative%2520multi-agent%2520learning%250Atasks%2520when%2520agents%2520converge%2520towards%2520a%2520suboptimal%2520joint%2520policy%2520due%2520to%2520overfitting%250Ato%2520suboptimal%2520behavior%2520of%2520other%2520agents.%2520No%2520methods%2520have%2520been%2520proposed%2520for%250Aaddressing%2520RO%2520in%2520multi-agent%2520policy%2520gradient%2520%2528MAPG%2529%2520methods%2520although%2520these%250Amethods%2520produce%2520state-of-the-art%2520results.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520a%250Ageneral%252C%2520yet%2520simple%252C%2520framework%2520to%2520enable%2520optimistic%2520updates%2520in%2520MAPG%2520methods%250Athat%2520alleviate%2520the%2520RO%2520problem.%2520Our%2520approach%2520involves%2520clipping%2520the%2520advantage%2520to%250Aeliminate%2520negative%2520values%252C%2520thereby%2520facilitating%2520optimistic%2520updates%2520in%2520MAPG.%2520The%250Aoptimism%2520prevents%2520individual%2520agents%2520from%2520quickly%2520converging%2520to%2520a%2520local%2520optimum.%250AAdditionally%252C%2520we%2520provide%2520a%2520formal%2520analysis%2520to%2520show%2520that%2520the%2520proposed%2520method%250Aretains%2520optimality%2520at%2520a%2520fixed%2520point.%2520In%2520extensive%2520evaluations%2520on%2520a%2520diverse%2520set%250Aof%2520tasks%2520including%2520the%2520%252AMulti-agent%2520MuJoCo%252A%2520and%2520%252AOvercooked%252A%2520benchmarks%252C%2520our%250Amethod%2520outperforms%2520strong%2520baselines%2520on%252013%2520out%2520of%252019%2520tested%2520tasks%2520and%2520matches%250Athe%2520performance%2520on%2520the%2520rest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.01953v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimistic%20Multi-Agent%20Policy%20Gradient&entry.906535625=Wenshuai%20Zhao%20and%20Yi%20Zhao%20and%20Zhiyuan%20Li%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen&entry.1292438233=%20%20%2ARelative%20overgeneralization%2A%20%28RO%29%20occurs%20in%20cooperative%20multi-agent%20learning%0Atasks%20when%20agents%20converge%20towards%20a%20suboptimal%20joint%20policy%20due%20to%20overfitting%0Ato%20suboptimal%20behavior%20of%20other%20agents.%20No%20methods%20have%20been%20proposed%20for%0Aaddressing%20RO%20in%20multi-agent%20policy%20gradient%20%28MAPG%29%20methods%20although%20these%0Amethods%20produce%20state-of-the-art%20results.%20To%20address%20this%20gap%2C%20we%20propose%20a%0Ageneral%2C%20yet%20simple%2C%20framework%20to%20enable%20optimistic%20updates%20in%20MAPG%20methods%0Athat%20alleviate%20the%20RO%20problem.%20Our%20approach%20involves%20clipping%20the%20advantage%20to%0Aeliminate%20negative%20values%2C%20thereby%20facilitating%20optimistic%20updates%20in%20MAPG.%20The%0Aoptimism%20prevents%20individual%20agents%20from%20quickly%20converging%20to%20a%20local%20optimum.%0AAdditionally%2C%20we%20provide%20a%20formal%20analysis%20to%20show%20that%20the%20proposed%20method%0Aretains%20optimality%20at%20a%20fixed%20point.%20In%20extensive%20evaluations%20on%20a%20diverse%20set%0Aof%20tasks%20including%20the%20%2AMulti-agent%20MuJoCo%2A%20and%20%2AOvercooked%2A%20benchmarks%2C%20our%0Amethod%20outperforms%20strong%20baselines%20on%2013%20out%20of%2019%20tested%20tasks%20and%20matches%0Athe%20performance%20on%20the%20rest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.01953v3&entry.124074799=Read"},
{"title": "Designing Tools with Control Confidence", "author": "Ajith Anil Meera and Abian Torres and Pablo Lanillos", "abstract": "  Prehistoric humans invented stone tools for specialized tasks by not just\nmaximizing the tool's immediate goal-completion accuracy, but also increasing\ntheir confidence in the tool for later use under similar settings. This factor\ncontributed to the increased robustness of the tool, i.e., the least\nperformance deviations under environmental uncertainties. However, the current\nautonomous tool design frameworks solely rely on performance optimization,\nwithout considering the agent's confidence in tool use for repeated use. Here,\nwe take a step towards filling this gap by i) defining an optimization\nframework for task-conditioned autonomous hand tool design for robots, where\nii) we introduce a neuro-inspired control confidence term into the optimization\nroutine that helps the agent to design tools with higher robustness. Through\nrigorous simulations using a robotic arm, we show that tools designed with\ncontrol confidence as the objective function are more robust to environmental\nuncertainties during tool use than a pure accuracy-driven objective. We further\nshow that adding control confidence to the objective function for tool design\nprovides a balance between the robustness and goal accuracy of the designed\ntools under control perturbations. Finally, we show that our CMAES-based\nevolutionary optimization strategy for autonomous tool design outperforms other\nstate-of-the-art optimizers by designing the optimal tool within the fewest\niterations. Code: https://github.com/ajitham123/Tool_design_control_confidence.\n", "link": "http://arxiv.org/abs/2510.12630v1", "date": "2025-10-14", "relevancy": 2.0719, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5495}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5237}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Designing%20Tools%20with%20Control%20Confidence&body=Title%3A%20Designing%20Tools%20with%20Control%20Confidence%0AAuthor%3A%20Ajith%20Anil%20Meera%20and%20Abian%20Torres%20and%20Pablo%20Lanillos%0AAbstract%3A%20%20%20Prehistoric%20humans%20invented%20stone%20tools%20for%20specialized%20tasks%20by%20not%20just%0Amaximizing%20the%20tool%27s%20immediate%20goal-completion%20accuracy%2C%20but%20also%20increasing%0Atheir%20confidence%20in%20the%20tool%20for%20later%20use%20under%20similar%20settings.%20This%20factor%0Acontributed%20to%20the%20increased%20robustness%20of%20the%20tool%2C%20i.e.%2C%20the%20least%0Aperformance%20deviations%20under%20environmental%20uncertainties.%20However%2C%20the%20current%0Aautonomous%20tool%20design%20frameworks%20solely%20rely%20on%20performance%20optimization%2C%0Awithout%20considering%20the%20agent%27s%20confidence%20in%20tool%20use%20for%20repeated%20use.%20Here%2C%0Awe%20take%20a%20step%20towards%20filling%20this%20gap%20by%20i%29%20defining%20an%20optimization%0Aframework%20for%20task-conditioned%20autonomous%20hand%20tool%20design%20for%20robots%2C%20where%0Aii%29%20we%20introduce%20a%20neuro-inspired%20control%20confidence%20term%20into%20the%20optimization%0Aroutine%20that%20helps%20the%20agent%20to%20design%20tools%20with%20higher%20robustness.%20Through%0Arigorous%20simulations%20using%20a%20robotic%20arm%2C%20we%20show%20that%20tools%20designed%20with%0Acontrol%20confidence%20as%20the%20objective%20function%20are%20more%20robust%20to%20environmental%0Auncertainties%20during%20tool%20use%20than%20a%20pure%20accuracy-driven%20objective.%20We%20further%0Ashow%20that%20adding%20control%20confidence%20to%20the%20objective%20function%20for%20tool%20design%0Aprovides%20a%20balance%20between%20the%20robustness%20and%20goal%20accuracy%20of%20the%20designed%0Atools%20under%20control%20perturbations.%20Finally%2C%20we%20show%20that%20our%20CMAES-based%0Aevolutionary%20optimization%20strategy%20for%20autonomous%20tool%20design%20outperforms%20other%0Astate-of-the-art%20optimizers%20by%20designing%20the%20optimal%20tool%20within%20the%20fewest%0Aiterations.%20Code%3A%20https%3A//github.com/ajitham123/Tool_design_control_confidence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12630v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesigning%2520Tools%2520with%2520Control%2520Confidence%26entry.906535625%3DAjith%2520Anil%2520Meera%2520and%2520Abian%2520Torres%2520and%2520Pablo%2520Lanillos%26entry.1292438233%3D%2520%2520Prehistoric%2520humans%2520invented%2520stone%2520tools%2520for%2520specialized%2520tasks%2520by%2520not%2520just%250Amaximizing%2520the%2520tool%2527s%2520immediate%2520goal-completion%2520accuracy%252C%2520but%2520also%2520increasing%250Atheir%2520confidence%2520in%2520the%2520tool%2520for%2520later%2520use%2520under%2520similar%2520settings.%2520This%2520factor%250Acontributed%2520to%2520the%2520increased%2520robustness%2520of%2520the%2520tool%252C%2520i.e.%252C%2520the%2520least%250Aperformance%2520deviations%2520under%2520environmental%2520uncertainties.%2520However%252C%2520the%2520current%250Aautonomous%2520tool%2520design%2520frameworks%2520solely%2520rely%2520on%2520performance%2520optimization%252C%250Awithout%2520considering%2520the%2520agent%2527s%2520confidence%2520in%2520tool%2520use%2520for%2520repeated%2520use.%2520Here%252C%250Awe%2520take%2520a%2520step%2520towards%2520filling%2520this%2520gap%2520by%2520i%2529%2520defining%2520an%2520optimization%250Aframework%2520for%2520task-conditioned%2520autonomous%2520hand%2520tool%2520design%2520for%2520robots%252C%2520where%250Aii%2529%2520we%2520introduce%2520a%2520neuro-inspired%2520control%2520confidence%2520term%2520into%2520the%2520optimization%250Aroutine%2520that%2520helps%2520the%2520agent%2520to%2520design%2520tools%2520with%2520higher%2520robustness.%2520Through%250Arigorous%2520simulations%2520using%2520a%2520robotic%2520arm%252C%2520we%2520show%2520that%2520tools%2520designed%2520with%250Acontrol%2520confidence%2520as%2520the%2520objective%2520function%2520are%2520more%2520robust%2520to%2520environmental%250Auncertainties%2520during%2520tool%2520use%2520than%2520a%2520pure%2520accuracy-driven%2520objective.%2520We%2520further%250Ashow%2520that%2520adding%2520control%2520confidence%2520to%2520the%2520objective%2520function%2520for%2520tool%2520design%250Aprovides%2520a%2520balance%2520between%2520the%2520robustness%2520and%2520goal%2520accuracy%2520of%2520the%2520designed%250Atools%2520under%2520control%2520perturbations.%2520Finally%252C%2520we%2520show%2520that%2520our%2520CMAES-based%250Aevolutionary%2520optimization%2520strategy%2520for%2520autonomous%2520tool%2520design%2520outperforms%2520other%250Astate-of-the-art%2520optimizers%2520by%2520designing%2520the%2520optimal%2520tool%2520within%2520the%2520fewest%250Aiterations.%2520Code%253A%2520https%253A//github.com/ajitham123/Tool_design_control_confidence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12630v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Designing%20Tools%20with%20Control%20Confidence&entry.906535625=Ajith%20Anil%20Meera%20and%20Abian%20Torres%20and%20Pablo%20Lanillos&entry.1292438233=%20%20Prehistoric%20humans%20invented%20stone%20tools%20for%20specialized%20tasks%20by%20not%20just%0Amaximizing%20the%20tool%27s%20immediate%20goal-completion%20accuracy%2C%20but%20also%20increasing%0Atheir%20confidence%20in%20the%20tool%20for%20later%20use%20under%20similar%20settings.%20This%20factor%0Acontributed%20to%20the%20increased%20robustness%20of%20the%20tool%2C%20i.e.%2C%20the%20least%0Aperformance%20deviations%20under%20environmental%20uncertainties.%20However%2C%20the%20current%0Aautonomous%20tool%20design%20frameworks%20solely%20rely%20on%20performance%20optimization%2C%0Awithout%20considering%20the%20agent%27s%20confidence%20in%20tool%20use%20for%20repeated%20use.%20Here%2C%0Awe%20take%20a%20step%20towards%20filling%20this%20gap%20by%20i%29%20defining%20an%20optimization%0Aframework%20for%20task-conditioned%20autonomous%20hand%20tool%20design%20for%20robots%2C%20where%0Aii%29%20we%20introduce%20a%20neuro-inspired%20control%20confidence%20term%20into%20the%20optimization%0Aroutine%20that%20helps%20the%20agent%20to%20design%20tools%20with%20higher%20robustness.%20Through%0Arigorous%20simulations%20using%20a%20robotic%20arm%2C%20we%20show%20that%20tools%20designed%20with%0Acontrol%20confidence%20as%20the%20objective%20function%20are%20more%20robust%20to%20environmental%0Auncertainties%20during%20tool%20use%20than%20a%20pure%20accuracy-driven%20objective.%20We%20further%0Ashow%20that%20adding%20control%20confidence%20to%20the%20objective%20function%20for%20tool%20design%0Aprovides%20a%20balance%20between%20the%20robustness%20and%20goal%20accuracy%20of%20the%20designed%0Atools%20under%20control%20perturbations.%20Finally%2C%20we%20show%20that%20our%20CMAES-based%0Aevolutionary%20optimization%20strategy%20for%20autonomous%20tool%20design%20outperforms%20other%0Astate-of-the-art%20optimizers%20by%20designing%20the%20optimal%20tool%20within%20the%20fewest%0Aiterations.%20Code%3A%20https%3A//github.com/ajitham123/Tool_design_control_confidence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12630v1&entry.124074799=Read"},
{"title": "Diff-XYZ: A Benchmark for Evaluating Diff Understanding", "author": "Evgeniy Glukhov and Michele Conti and Egor Bogomolov and Yaroslav Golubev and Alexander Bezzubov", "abstract": "  Reliable handling of code diffs is central to agents that edit and refactor\nrepositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff\nunderstanding with three supervised tasks: apply (old code $+$ diff\n$\\rightarrow$ new code), anti-apply (new code $-$ diff $\\rightarrow$ old code),\nand diff generation (new code $-$ old code $\\rightarrow$ diff). Instances in\nthe benchmark are triples $\\langle \\textit{old code}, \\textit{new code},\n\\textit{diff} \\rangle$ drawn from real commits in CommitPackFT, paired with\nautomatic metrics and a clear evaluation protocol. We use the benchmark to do a\nfocused empirical study of the unified diff format and run a cross-format\ncomparison of different diff representations. Our findings reveal that\ndifferent formats should be used depending on the use case and model size. For\nexample, representing diffs in search-replace format is good for larger models\nin the diff generation scenario, yet not suited well for diff analysis and\nsmaller models. The Diff-XYZ benchmark is a reusable foundation for assessing\nand improving diff handling in LLMs that can aid future development of diff\nformats and models editing code. The dataset is published on HuggingFace Hub:\nhttps://huggingface.co/datasets/JetBrains-Research/diff-xyz.\n", "link": "http://arxiv.org/abs/2510.12487v1", "date": "2025-10-14", "relevancy": 2.0666, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diff-XYZ%3A%20A%20Benchmark%20for%20Evaluating%20Diff%20Understanding&body=Title%3A%20Diff-XYZ%3A%20A%20Benchmark%20for%20Evaluating%20Diff%20Understanding%0AAuthor%3A%20Evgeniy%20Glukhov%20and%20Michele%20Conti%20and%20Egor%20Bogomolov%20and%20Yaroslav%20Golubev%20and%20Alexander%20Bezzubov%0AAbstract%3A%20%20%20Reliable%20handling%20of%20code%20diffs%20is%20central%20to%20agents%20that%20edit%20and%20refactor%0Arepositories%20at%20scale.%20We%20introduce%20Diff-XYZ%2C%20a%20compact%20benchmark%20for%20code-diff%0Aunderstanding%20with%20three%20supervised%20tasks%3A%20apply%20%28old%20code%20%24%2B%24%20diff%0A%24%5Crightarrow%24%20new%20code%29%2C%20anti-apply%20%28new%20code%20%24-%24%20diff%20%24%5Crightarrow%24%20old%20code%29%2C%0Aand%20diff%20generation%20%28new%20code%20%24-%24%20old%20code%20%24%5Crightarrow%24%20diff%29.%20Instances%20in%0Athe%20benchmark%20are%20triples%20%24%5Clangle%20%5Ctextit%7Bold%20code%7D%2C%20%5Ctextit%7Bnew%20code%7D%2C%0A%5Ctextit%7Bdiff%7D%20%5Crangle%24%20drawn%20from%20real%20commits%20in%20CommitPackFT%2C%20paired%20with%0Aautomatic%20metrics%20and%20a%20clear%20evaluation%20protocol.%20We%20use%20the%20benchmark%20to%20do%20a%0Afocused%20empirical%20study%20of%20the%20unified%20diff%20format%20and%20run%20a%20cross-format%0Acomparison%20of%20different%20diff%20representations.%20Our%20findings%20reveal%20that%0Adifferent%20formats%20should%20be%20used%20depending%20on%20the%20use%20case%20and%20model%20size.%20For%0Aexample%2C%20representing%20diffs%20in%20search-replace%20format%20is%20good%20for%20larger%20models%0Ain%20the%20diff%20generation%20scenario%2C%20yet%20not%20suited%20well%20for%20diff%20analysis%20and%0Asmaller%20models.%20The%20Diff-XYZ%20benchmark%20is%20a%20reusable%20foundation%20for%20assessing%0Aand%20improving%20diff%20handling%20in%20LLMs%20that%20can%20aid%20future%20development%20of%20diff%0Aformats%20and%20models%20editing%20code.%20The%20dataset%20is%20published%20on%20HuggingFace%20Hub%3A%0Ahttps%3A//huggingface.co/datasets/JetBrains-Research/diff-xyz.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiff-XYZ%253A%2520A%2520Benchmark%2520for%2520Evaluating%2520Diff%2520Understanding%26entry.906535625%3DEvgeniy%2520Glukhov%2520and%2520Michele%2520Conti%2520and%2520Egor%2520Bogomolov%2520and%2520Yaroslav%2520Golubev%2520and%2520Alexander%2520Bezzubov%26entry.1292438233%3D%2520%2520Reliable%2520handling%2520of%2520code%2520diffs%2520is%2520central%2520to%2520agents%2520that%2520edit%2520and%2520refactor%250Arepositories%2520at%2520scale.%2520We%2520introduce%2520Diff-XYZ%252C%2520a%2520compact%2520benchmark%2520for%2520code-diff%250Aunderstanding%2520with%2520three%2520supervised%2520tasks%253A%2520apply%2520%2528old%2520code%2520%2524%252B%2524%2520diff%250A%2524%255Crightarrow%2524%2520new%2520code%2529%252C%2520anti-apply%2520%2528new%2520code%2520%2524-%2524%2520diff%2520%2524%255Crightarrow%2524%2520old%2520code%2529%252C%250Aand%2520diff%2520generation%2520%2528new%2520code%2520%2524-%2524%2520old%2520code%2520%2524%255Crightarrow%2524%2520diff%2529.%2520Instances%2520in%250Athe%2520benchmark%2520are%2520triples%2520%2524%255Clangle%2520%255Ctextit%257Bold%2520code%257D%252C%2520%255Ctextit%257Bnew%2520code%257D%252C%250A%255Ctextit%257Bdiff%257D%2520%255Crangle%2524%2520drawn%2520from%2520real%2520commits%2520in%2520CommitPackFT%252C%2520paired%2520with%250Aautomatic%2520metrics%2520and%2520a%2520clear%2520evaluation%2520protocol.%2520We%2520use%2520the%2520benchmark%2520to%2520do%2520a%250Afocused%2520empirical%2520study%2520of%2520the%2520unified%2520diff%2520format%2520and%2520run%2520a%2520cross-format%250Acomparison%2520of%2520different%2520diff%2520representations.%2520Our%2520findings%2520reveal%2520that%250Adifferent%2520formats%2520should%2520be%2520used%2520depending%2520on%2520the%2520use%2520case%2520and%2520model%2520size.%2520For%250Aexample%252C%2520representing%2520diffs%2520in%2520search-replace%2520format%2520is%2520good%2520for%2520larger%2520models%250Ain%2520the%2520diff%2520generation%2520scenario%252C%2520yet%2520not%2520suited%2520well%2520for%2520diff%2520analysis%2520and%250Asmaller%2520models.%2520The%2520Diff-XYZ%2520benchmark%2520is%2520a%2520reusable%2520foundation%2520for%2520assessing%250Aand%2520improving%2520diff%2520handling%2520in%2520LLMs%2520that%2520can%2520aid%2520future%2520development%2520of%2520diff%250Aformats%2520and%2520models%2520editing%2520code.%2520The%2520dataset%2520is%2520published%2520on%2520HuggingFace%2520Hub%253A%250Ahttps%253A//huggingface.co/datasets/JetBrains-Research/diff-xyz.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diff-XYZ%3A%20A%20Benchmark%20for%20Evaluating%20Diff%20Understanding&entry.906535625=Evgeniy%20Glukhov%20and%20Michele%20Conti%20and%20Egor%20Bogomolov%20and%20Yaroslav%20Golubev%20and%20Alexander%20Bezzubov&entry.1292438233=%20%20Reliable%20handling%20of%20code%20diffs%20is%20central%20to%20agents%20that%20edit%20and%20refactor%0Arepositories%20at%20scale.%20We%20introduce%20Diff-XYZ%2C%20a%20compact%20benchmark%20for%20code-diff%0Aunderstanding%20with%20three%20supervised%20tasks%3A%20apply%20%28old%20code%20%24%2B%24%20diff%0A%24%5Crightarrow%24%20new%20code%29%2C%20anti-apply%20%28new%20code%20%24-%24%20diff%20%24%5Crightarrow%24%20old%20code%29%2C%0Aand%20diff%20generation%20%28new%20code%20%24-%24%20old%20code%20%24%5Crightarrow%24%20diff%29.%20Instances%20in%0Athe%20benchmark%20are%20triples%20%24%5Clangle%20%5Ctextit%7Bold%20code%7D%2C%20%5Ctextit%7Bnew%20code%7D%2C%0A%5Ctextit%7Bdiff%7D%20%5Crangle%24%20drawn%20from%20real%20commits%20in%20CommitPackFT%2C%20paired%20with%0Aautomatic%20metrics%20and%20a%20clear%20evaluation%20protocol.%20We%20use%20the%20benchmark%20to%20do%20a%0Afocused%20empirical%20study%20of%20the%20unified%20diff%20format%20and%20run%20a%20cross-format%0Acomparison%20of%20different%20diff%20representations.%20Our%20findings%20reveal%20that%0Adifferent%20formats%20should%20be%20used%20depending%20on%20the%20use%20case%20and%20model%20size.%20For%0Aexample%2C%20representing%20diffs%20in%20search-replace%20format%20is%20good%20for%20larger%20models%0Ain%20the%20diff%20generation%20scenario%2C%20yet%20not%20suited%20well%20for%20diff%20analysis%20and%0Asmaller%20models.%20The%20Diff-XYZ%20benchmark%20is%20a%20reusable%20foundation%20for%20assessing%0Aand%20improving%20diff%20handling%20in%20LLMs%20that%20can%20aid%20future%20development%20of%20diff%0Aformats%20and%20models%20editing%20code.%20The%20dataset%20is%20published%20on%20HuggingFace%20Hub%3A%0Ahttps%3A//huggingface.co/datasets/JetBrains-Research/diff-xyz.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12487v1&entry.124074799=Read"},
{"title": "HardcoreLogic: Challenging Large Reasoning Models with Long-tail Logic\n  Puzzle Games", "author": "Jingcong Liang and Shijun Wan and Xuehai Wu and Siyuan Wang and Yitong Li and Qianglong Chen and Duyu Tang and Zhongyu Wei", "abstract": "  Large Reasoning Models (LRMs) have demonstrated impressive performance on\ncomplex tasks, including logical puzzle games that require deriving solutions\nsatisfying all constraints. However, whether they can flexibly apply\nappropriate rules to varying conditions, particularly when faced with\nnon-canonical game variants, remains an open question. Existing corpora focus\non popular puzzles like 9x9 Sudoku, risking overfitting to canonical formats\nand memorization of solution patterns, which can mask deficiencies in\nunderstanding novel rules or adapting strategies to new variants. To address\nthis, we introduce HardcoreLogic, a challenging benchmark of over 5,000 puzzles\nacross 10 games, designed to test the robustness of LRMs on the \"long-tail\" of\nlogical games. HardcoreLogic systematically transforms canonical puzzles\nthrough three dimensions: Increased Complexity (IC), Uncommon Elements (UE),\nand Unsolvable Puzzles (UP), reducing reliance on shortcut memorization.\nEvaluations on a diverse set of LRMs reveal significant performance drops, even\nfor models achieving top scores on existing benchmarks, indicating heavy\nreliance on memorized stereotypes. While increased complexity is the dominant\nsource of difficulty, models also struggle with subtle rule variations that do\nnot necessarily increase puzzle difficulty. Our systematic error analysis on\nsolvable and unsolvable puzzles further highlights gaps in genuine reasoning.\nOverall, HardcoreLogic exposes the limitations of current LRMs and establishes\na benchmark for advancing high-level logical reasoning.\n", "link": "http://arxiv.org/abs/2510.12563v1", "date": "2025-10-14", "relevancy": 2.066, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5217}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5217}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HardcoreLogic%3A%20Challenging%20Large%20Reasoning%20Models%20with%20Long-tail%20Logic%0A%20%20Puzzle%20Games&body=Title%3A%20HardcoreLogic%3A%20Challenging%20Large%20Reasoning%20Models%20with%20Long-tail%20Logic%0A%20%20Puzzle%20Games%0AAuthor%3A%20Jingcong%20Liang%20and%20Shijun%20Wan%20and%20Xuehai%20Wu%20and%20Siyuan%20Wang%20and%20Yitong%20Li%20and%20Qianglong%20Chen%20and%20Duyu%20Tang%20and%20Zhongyu%20Wei%0AAbstract%3A%20%20%20Large%20Reasoning%20Models%20%28LRMs%29%20have%20demonstrated%20impressive%20performance%20on%0Acomplex%20tasks%2C%20including%20logical%20puzzle%20games%20that%20require%20deriving%20solutions%0Asatisfying%20all%20constraints.%20However%2C%20whether%20they%20can%20flexibly%20apply%0Aappropriate%20rules%20to%20varying%20conditions%2C%20particularly%20when%20faced%20with%0Anon-canonical%20game%20variants%2C%20remains%20an%20open%20question.%20Existing%20corpora%20focus%0Aon%20popular%20puzzles%20like%209x9%20Sudoku%2C%20risking%20overfitting%20to%20canonical%20formats%0Aand%20memorization%20of%20solution%20patterns%2C%20which%20can%20mask%20deficiencies%20in%0Aunderstanding%20novel%20rules%20or%20adapting%20strategies%20to%20new%20variants.%20To%20address%0Athis%2C%20we%20introduce%20HardcoreLogic%2C%20a%20challenging%20benchmark%20of%20over%205%2C000%20puzzles%0Aacross%2010%20games%2C%20designed%20to%20test%20the%20robustness%20of%20LRMs%20on%20the%20%22long-tail%22%20of%0Alogical%20games.%20HardcoreLogic%20systematically%20transforms%20canonical%20puzzles%0Athrough%20three%20dimensions%3A%20Increased%20Complexity%20%28IC%29%2C%20Uncommon%20Elements%20%28UE%29%2C%0Aand%20Unsolvable%20Puzzles%20%28UP%29%2C%20reducing%20reliance%20on%20shortcut%20memorization.%0AEvaluations%20on%20a%20diverse%20set%20of%20LRMs%20reveal%20significant%20performance%20drops%2C%20even%0Afor%20models%20achieving%20top%20scores%20on%20existing%20benchmarks%2C%20indicating%20heavy%0Areliance%20on%20memorized%20stereotypes.%20While%20increased%20complexity%20is%20the%20dominant%0Asource%20of%20difficulty%2C%20models%20also%20struggle%20with%20subtle%20rule%20variations%20that%20do%0Anot%20necessarily%20increase%20puzzle%20difficulty.%20Our%20systematic%20error%20analysis%20on%0Asolvable%20and%20unsolvable%20puzzles%20further%20highlights%20gaps%20in%20genuine%20reasoning.%0AOverall%2C%20HardcoreLogic%20exposes%20the%20limitations%20of%20current%20LRMs%20and%20establishes%0Aa%20benchmark%20for%20advancing%20high-level%20logical%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12563v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHardcoreLogic%253A%2520Challenging%2520Large%2520Reasoning%2520Models%2520with%2520Long-tail%2520Logic%250A%2520%2520Puzzle%2520Games%26entry.906535625%3DJingcong%2520Liang%2520and%2520Shijun%2520Wan%2520and%2520Xuehai%2520Wu%2520and%2520Siyuan%2520Wang%2520and%2520Yitong%2520Li%2520and%2520Qianglong%2520Chen%2520and%2520Duyu%2520Tang%2520and%2520Zhongyu%2520Wei%26entry.1292438233%3D%2520%2520Large%2520Reasoning%2520Models%2520%2528LRMs%2529%2520have%2520demonstrated%2520impressive%2520performance%2520on%250Acomplex%2520tasks%252C%2520including%2520logical%2520puzzle%2520games%2520that%2520require%2520deriving%2520solutions%250Asatisfying%2520all%2520constraints.%2520However%252C%2520whether%2520they%2520can%2520flexibly%2520apply%250Aappropriate%2520rules%2520to%2520varying%2520conditions%252C%2520particularly%2520when%2520faced%2520with%250Anon-canonical%2520game%2520variants%252C%2520remains%2520an%2520open%2520question.%2520Existing%2520corpora%2520focus%250Aon%2520popular%2520puzzles%2520like%25209x9%2520Sudoku%252C%2520risking%2520overfitting%2520to%2520canonical%2520formats%250Aand%2520memorization%2520of%2520solution%2520patterns%252C%2520which%2520can%2520mask%2520deficiencies%2520in%250Aunderstanding%2520novel%2520rules%2520or%2520adapting%2520strategies%2520to%2520new%2520variants.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520HardcoreLogic%252C%2520a%2520challenging%2520benchmark%2520of%2520over%25205%252C000%2520puzzles%250Aacross%252010%2520games%252C%2520designed%2520to%2520test%2520the%2520robustness%2520of%2520LRMs%2520on%2520the%2520%2522long-tail%2522%2520of%250Alogical%2520games.%2520HardcoreLogic%2520systematically%2520transforms%2520canonical%2520puzzles%250Athrough%2520three%2520dimensions%253A%2520Increased%2520Complexity%2520%2528IC%2529%252C%2520Uncommon%2520Elements%2520%2528UE%2529%252C%250Aand%2520Unsolvable%2520Puzzles%2520%2528UP%2529%252C%2520reducing%2520reliance%2520on%2520shortcut%2520memorization.%250AEvaluations%2520on%2520a%2520diverse%2520set%2520of%2520LRMs%2520reveal%2520significant%2520performance%2520drops%252C%2520even%250Afor%2520models%2520achieving%2520top%2520scores%2520on%2520existing%2520benchmarks%252C%2520indicating%2520heavy%250Areliance%2520on%2520memorized%2520stereotypes.%2520While%2520increased%2520complexity%2520is%2520the%2520dominant%250Asource%2520of%2520difficulty%252C%2520models%2520also%2520struggle%2520with%2520subtle%2520rule%2520variations%2520that%2520do%250Anot%2520necessarily%2520increase%2520puzzle%2520difficulty.%2520Our%2520systematic%2520error%2520analysis%2520on%250Asolvable%2520and%2520unsolvable%2520puzzles%2520further%2520highlights%2520gaps%2520in%2520genuine%2520reasoning.%250AOverall%252C%2520HardcoreLogic%2520exposes%2520the%2520limitations%2520of%2520current%2520LRMs%2520and%2520establishes%250Aa%2520benchmark%2520for%2520advancing%2520high-level%2520logical%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12563v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HardcoreLogic%3A%20Challenging%20Large%20Reasoning%20Models%20with%20Long-tail%20Logic%0A%20%20Puzzle%20Games&entry.906535625=Jingcong%20Liang%20and%20Shijun%20Wan%20and%20Xuehai%20Wu%20and%20Siyuan%20Wang%20and%20Yitong%20Li%20and%20Qianglong%20Chen%20and%20Duyu%20Tang%20and%20Zhongyu%20Wei&entry.1292438233=%20%20Large%20Reasoning%20Models%20%28LRMs%29%20have%20demonstrated%20impressive%20performance%20on%0Acomplex%20tasks%2C%20including%20logical%20puzzle%20games%20that%20require%20deriving%20solutions%0Asatisfying%20all%20constraints.%20However%2C%20whether%20they%20can%20flexibly%20apply%0Aappropriate%20rules%20to%20varying%20conditions%2C%20particularly%20when%20faced%20with%0Anon-canonical%20game%20variants%2C%20remains%20an%20open%20question.%20Existing%20corpora%20focus%0Aon%20popular%20puzzles%20like%209x9%20Sudoku%2C%20risking%20overfitting%20to%20canonical%20formats%0Aand%20memorization%20of%20solution%20patterns%2C%20which%20can%20mask%20deficiencies%20in%0Aunderstanding%20novel%20rules%20or%20adapting%20strategies%20to%20new%20variants.%20To%20address%0Athis%2C%20we%20introduce%20HardcoreLogic%2C%20a%20challenging%20benchmark%20of%20over%205%2C000%20puzzles%0Aacross%2010%20games%2C%20designed%20to%20test%20the%20robustness%20of%20LRMs%20on%20the%20%22long-tail%22%20of%0Alogical%20games.%20HardcoreLogic%20systematically%20transforms%20canonical%20puzzles%0Athrough%20three%20dimensions%3A%20Increased%20Complexity%20%28IC%29%2C%20Uncommon%20Elements%20%28UE%29%2C%0Aand%20Unsolvable%20Puzzles%20%28UP%29%2C%20reducing%20reliance%20on%20shortcut%20memorization.%0AEvaluations%20on%20a%20diverse%20set%20of%20LRMs%20reveal%20significant%20performance%20drops%2C%20even%0Afor%20models%20achieving%20top%20scores%20on%20existing%20benchmarks%2C%20indicating%20heavy%0Areliance%20on%20memorized%20stereotypes.%20While%20increased%20complexity%20is%20the%20dominant%0Asource%20of%20difficulty%2C%20models%20also%20struggle%20with%20subtle%20rule%20variations%20that%20do%0Anot%20necessarily%20increase%20puzzle%20difficulty.%20Our%20systematic%20error%20analysis%20on%0Asolvable%20and%20unsolvable%20puzzles%20further%20highlights%20gaps%20in%20genuine%20reasoning.%0AOverall%2C%20HardcoreLogic%20exposes%20the%20limitations%20of%20current%20LRMs%20and%20establishes%0Aa%20benchmark%20for%20advancing%20high-level%20logical%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12563v1&entry.124074799=Read"},
{"title": "Reasoning Pattern Matters: Learning to Reason without Human Rationales", "author": "Chaoxu Pang and Yixuan Cao and Ping Luo", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable reasoning\ncapabilities under the widely adopted SFT+RLVR paradigm, which first performs\nSupervised Fine-Tuning (SFT) on human-annotated reasoning trajectories\n(rationales) to establish initial reasoning behaviors, then applies\nReinforcement Learning with Verifiable Rewards (RLVR) to optimize the model\nusing verifiable signals without golden rationales. However, annotating\nhigh-quality rationales for the SFT stage remains prohibitively expensive. This\npaper investigates when and how rationale annotation costs can be substantially\nreduced without compromising reasoning performance. We identify a broad class\nof problems, termed patterned reasoning tasks, where reasoning follows a fixed,\nprocedural strategy consistent across instances. Although instances vary in\ncontent such as domain knowledge, factual information, or numeric values, the\nsolution derives from applying a shared reasoning pattern. We argue that the\nsuccess of SFT+RLVR on such tasks primarily stems from its ability to enable\nmodels to internalize these reasoning patterns. Using numerical semantic\nmatching as a representative task, we provide both causal and behavioral\nevidence showing that reasoning patterns rather than the quantity or quality of\nrationales are the key determinant of performance. Building on these insights,\nwe propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet\neffective framework that enables LLMs to generate rationales aligned with\ntask-specific reasoning patterns without requiring human rationale annotations.\nExperiments show that PARO-generated rationales achieve comparable SFT+RLVR\nperformance to human rationales that are 10 times larger. These results suggest\nthat large-scale human rationale annotations can be replaced with LLM-based\nautomatic annotations requiring only limited human supervision over reasoning\npatterns.\n", "link": "http://arxiv.org/abs/2510.12643v1", "date": "2025-10-14", "relevancy": 2.0635, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20Pattern%20Matters%3A%20Learning%20to%20Reason%20without%20Human%20Rationales&body=Title%3A%20Reasoning%20Pattern%20Matters%3A%20Learning%20to%20Reason%20without%20Human%20Rationales%0AAuthor%3A%20Chaoxu%20Pang%20and%20Yixuan%20Cao%20and%20Ping%20Luo%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20reasoning%0Acapabilities%20under%20the%20widely%20adopted%20SFT%2BRLVR%20paradigm%2C%20which%20first%20performs%0ASupervised%20Fine-Tuning%20%28SFT%29%20on%20human-annotated%20reasoning%20trajectories%0A%28rationales%29%20to%20establish%20initial%20reasoning%20behaviors%2C%20then%20applies%0AReinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20to%20optimize%20the%20model%0Ausing%20verifiable%20signals%20without%20golden%20rationales.%20However%2C%20annotating%0Ahigh-quality%20rationales%20for%20the%20SFT%20stage%20remains%20prohibitively%20expensive.%20This%0Apaper%20investigates%20when%20and%20how%20rationale%20annotation%20costs%20can%20be%20substantially%0Areduced%20without%20compromising%20reasoning%20performance.%20We%20identify%20a%20broad%20class%0Aof%20problems%2C%20termed%20patterned%20reasoning%20tasks%2C%20where%20reasoning%20follows%20a%20fixed%2C%0Aprocedural%20strategy%20consistent%20across%20instances.%20Although%20instances%20vary%20in%0Acontent%20such%20as%20domain%20knowledge%2C%20factual%20information%2C%20or%20numeric%20values%2C%20the%0Asolution%20derives%20from%20applying%20a%20shared%20reasoning%20pattern.%20We%20argue%20that%20the%0Asuccess%20of%20SFT%2BRLVR%20on%20such%20tasks%20primarily%20stems%20from%20its%20ability%20to%20enable%0Amodels%20to%20internalize%20these%20reasoning%20patterns.%20Using%20numerical%20semantic%0Amatching%20as%20a%20representative%20task%2C%20we%20provide%20both%20causal%20and%20behavioral%0Aevidence%20showing%20that%20reasoning%20patterns%20rather%20than%20the%20quantity%20or%20quality%20of%0Arationales%20are%20the%20key%20determinant%20of%20performance.%20Building%20on%20these%20insights%2C%0Awe%20propose%20Pattern-Aware%20LLMs%20as%20Rationale%20AnnOtators%20%28PARO%29%2C%20a%20simple%20yet%0Aeffective%20framework%20that%20enables%20LLMs%20to%20generate%20rationales%20aligned%20with%0Atask-specific%20reasoning%20patterns%20without%20requiring%20human%20rationale%20annotations.%0AExperiments%20show%20that%20PARO-generated%20rationales%20achieve%20comparable%20SFT%2BRLVR%0Aperformance%20to%20human%20rationales%20that%20are%2010%20times%20larger.%20These%20results%20suggest%0Athat%20large-scale%20human%20rationale%20annotations%20can%20be%20replaced%20with%20LLM-based%0Aautomatic%20annotations%20requiring%20only%20limited%20human%20supervision%20over%20reasoning%0Apatterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520Pattern%2520Matters%253A%2520Learning%2520to%2520Reason%2520without%2520Human%2520Rationales%26entry.906535625%3DChaoxu%2520Pang%2520and%2520Yixuan%2520Cao%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520reasoning%250Acapabilities%2520under%2520the%2520widely%2520adopted%2520SFT%252BRLVR%2520paradigm%252C%2520which%2520first%2520performs%250ASupervised%2520Fine-Tuning%2520%2528SFT%2529%2520on%2520human-annotated%2520reasoning%2520trajectories%250A%2528rationales%2529%2520to%2520establish%2520initial%2520reasoning%2520behaviors%252C%2520then%2520applies%250AReinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520to%2520optimize%2520the%2520model%250Ausing%2520verifiable%2520signals%2520without%2520golden%2520rationales.%2520However%252C%2520annotating%250Ahigh-quality%2520rationales%2520for%2520the%2520SFT%2520stage%2520remains%2520prohibitively%2520expensive.%2520This%250Apaper%2520investigates%2520when%2520and%2520how%2520rationale%2520annotation%2520costs%2520can%2520be%2520substantially%250Areduced%2520without%2520compromising%2520reasoning%2520performance.%2520We%2520identify%2520a%2520broad%2520class%250Aof%2520problems%252C%2520termed%2520patterned%2520reasoning%2520tasks%252C%2520where%2520reasoning%2520follows%2520a%2520fixed%252C%250Aprocedural%2520strategy%2520consistent%2520across%2520instances.%2520Although%2520instances%2520vary%2520in%250Acontent%2520such%2520as%2520domain%2520knowledge%252C%2520factual%2520information%252C%2520or%2520numeric%2520values%252C%2520the%250Asolution%2520derives%2520from%2520applying%2520a%2520shared%2520reasoning%2520pattern.%2520We%2520argue%2520that%2520the%250Asuccess%2520of%2520SFT%252BRLVR%2520on%2520such%2520tasks%2520primarily%2520stems%2520from%2520its%2520ability%2520to%2520enable%250Amodels%2520to%2520internalize%2520these%2520reasoning%2520patterns.%2520Using%2520numerical%2520semantic%250Amatching%2520as%2520a%2520representative%2520task%252C%2520we%2520provide%2520both%2520causal%2520and%2520behavioral%250Aevidence%2520showing%2520that%2520reasoning%2520patterns%2520rather%2520than%2520the%2520quantity%2520or%2520quality%2520of%250Arationales%2520are%2520the%2520key%2520determinant%2520of%2520performance.%2520Building%2520on%2520these%2520insights%252C%250Awe%2520propose%2520Pattern-Aware%2520LLMs%2520as%2520Rationale%2520AnnOtators%2520%2528PARO%2529%252C%2520a%2520simple%2520yet%250Aeffective%2520framework%2520that%2520enables%2520LLMs%2520to%2520generate%2520rationales%2520aligned%2520with%250Atask-specific%2520reasoning%2520patterns%2520without%2520requiring%2520human%2520rationale%2520annotations.%250AExperiments%2520show%2520that%2520PARO-generated%2520rationales%2520achieve%2520comparable%2520SFT%252BRLVR%250Aperformance%2520to%2520human%2520rationales%2520that%2520are%252010%2520times%2520larger.%2520These%2520results%2520suggest%250Athat%2520large-scale%2520human%2520rationale%2520annotations%2520can%2520be%2520replaced%2520with%2520LLM-based%250Aautomatic%2520annotations%2520requiring%2520only%2520limited%2520human%2520supervision%2520over%2520reasoning%250Apatterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20Pattern%20Matters%3A%20Learning%20to%20Reason%20without%20Human%20Rationales&entry.906535625=Chaoxu%20Pang%20and%20Yixuan%20Cao%20and%20Ping%20Luo&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20reasoning%0Acapabilities%20under%20the%20widely%20adopted%20SFT%2BRLVR%20paradigm%2C%20which%20first%20performs%0ASupervised%20Fine-Tuning%20%28SFT%29%20on%20human-annotated%20reasoning%20trajectories%0A%28rationales%29%20to%20establish%20initial%20reasoning%20behaviors%2C%20then%20applies%0AReinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20to%20optimize%20the%20model%0Ausing%20verifiable%20signals%20without%20golden%20rationales.%20However%2C%20annotating%0Ahigh-quality%20rationales%20for%20the%20SFT%20stage%20remains%20prohibitively%20expensive.%20This%0Apaper%20investigates%20when%20and%20how%20rationale%20annotation%20costs%20can%20be%20substantially%0Areduced%20without%20compromising%20reasoning%20performance.%20We%20identify%20a%20broad%20class%0Aof%20problems%2C%20termed%20patterned%20reasoning%20tasks%2C%20where%20reasoning%20follows%20a%20fixed%2C%0Aprocedural%20strategy%20consistent%20across%20instances.%20Although%20instances%20vary%20in%0Acontent%20such%20as%20domain%20knowledge%2C%20factual%20information%2C%20or%20numeric%20values%2C%20the%0Asolution%20derives%20from%20applying%20a%20shared%20reasoning%20pattern.%20We%20argue%20that%20the%0Asuccess%20of%20SFT%2BRLVR%20on%20such%20tasks%20primarily%20stems%20from%20its%20ability%20to%20enable%0Amodels%20to%20internalize%20these%20reasoning%20patterns.%20Using%20numerical%20semantic%0Amatching%20as%20a%20representative%20task%2C%20we%20provide%20both%20causal%20and%20behavioral%0Aevidence%20showing%20that%20reasoning%20patterns%20rather%20than%20the%20quantity%20or%20quality%20of%0Arationales%20are%20the%20key%20determinant%20of%20performance.%20Building%20on%20these%20insights%2C%0Awe%20propose%20Pattern-Aware%20LLMs%20as%20Rationale%20AnnOtators%20%28PARO%29%2C%20a%20simple%20yet%0Aeffective%20framework%20that%20enables%20LLMs%20to%20generate%20rationales%20aligned%20with%0Atask-specific%20reasoning%20patterns%20without%20requiring%20human%20rationale%20annotations.%0AExperiments%20show%20that%20PARO-generated%20rationales%20achieve%20comparable%20SFT%2BRLVR%0Aperformance%20to%20human%20rationales%20that%20are%2010%20times%20larger.%20These%20results%20suggest%0Athat%20large-scale%20human%20rationale%20annotations%20can%20be%20replaced%20with%20LLM-based%0Aautomatic%20annotations%20requiring%20only%20limited%20human%20supervision%20over%20reasoning%0Apatterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12643v1&entry.124074799=Read"},
{"title": "KoALA: KL-L0 Adversarial Detector via Label Agreement", "author": "Siqi Li and Yasser Shoukry", "abstract": "  Deep neural networks are highly susceptible to adversarial attacks, which\npose significant risks to security- and safety-critical applications. We\npresent KoALA (KL-L0 Adversarial detection via Label Agreement), a novel,\nsemantics-free adversarial detector that requires no architectural changes or\nadversarial retraining. KoALA operates on a simple principle: it detects an\nadversarial attack when class predictions from two complementary similarity\nmetrics disagree. These metrics-KL divergence and an L0-based similarity-are\nspecifically chosen to detect different types of perturbations. The KL\ndivergence metric is sensitive to dense, low-amplitude shifts, while the\nL0-based similarity is designed for sparse, high-impact changes. We provide a\nformal proof of correctness for our approach. The only training required is a\nsimple fine-tuning step on a pre-trained image encoder using clean images to\nensure the embeddings align well with both metrics. This makes KOALA a\nlightweight, plug-and-play solution for existing models and various data\nmodalities. Our extensive experiments on ResNet/CIFAR-10 and CLIP/Tiny-ImageNet\nconfirm our theoretical claims. When the theorem's conditions are met, KoALA\nconsistently and effectively detects adversarial examples. On the full test\nsets, KoALA achieves a precision of 0.94 and a recall of 0.81 on\nResNet/CIFAR-10, and a precision of 0.66 and a recall of 0.85 on\nCLIP/Tiny-ImageNet.\n", "link": "http://arxiv.org/abs/2510.12752v1", "date": "2025-10-14", "relevancy": 2.0599, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5472}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5112}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KoALA%3A%20KL-L0%20Adversarial%20Detector%20via%20Label%20Agreement&body=Title%3A%20KoALA%3A%20KL-L0%20Adversarial%20Detector%20via%20Label%20Agreement%0AAuthor%3A%20Siqi%20Li%20and%20Yasser%20Shoukry%0AAbstract%3A%20%20%20Deep%20neural%20networks%20are%20highly%20susceptible%20to%20adversarial%20attacks%2C%20which%0Apose%20significant%20risks%20to%20security-%20and%20safety-critical%20applications.%20We%0Apresent%20KoALA%20%28KL-L0%20Adversarial%20detection%20via%20Label%20Agreement%29%2C%20a%20novel%2C%0Asemantics-free%20adversarial%20detector%20that%20requires%20no%20architectural%20changes%20or%0Aadversarial%20retraining.%20KoALA%20operates%20on%20a%20simple%20principle%3A%20it%20detects%20an%0Aadversarial%20attack%20when%20class%20predictions%20from%20two%20complementary%20similarity%0Ametrics%20disagree.%20These%20metrics-KL%20divergence%20and%20an%20L0-based%20similarity-are%0Aspecifically%20chosen%20to%20detect%20different%20types%20of%20perturbations.%20The%20KL%0Adivergence%20metric%20is%20sensitive%20to%20dense%2C%20low-amplitude%20shifts%2C%20while%20the%0AL0-based%20similarity%20is%20designed%20for%20sparse%2C%20high-impact%20changes.%20We%20provide%20a%0Aformal%20proof%20of%20correctness%20for%20our%20approach.%20The%20only%20training%20required%20is%20a%0Asimple%20fine-tuning%20step%20on%20a%20pre-trained%20image%20encoder%20using%20clean%20images%20to%0Aensure%20the%20embeddings%20align%20well%20with%20both%20metrics.%20This%20makes%20KOALA%20a%0Alightweight%2C%20plug-and-play%20solution%20for%20existing%20models%20and%20various%20data%0Amodalities.%20Our%20extensive%20experiments%20on%20ResNet/CIFAR-10%20and%20CLIP/Tiny-ImageNet%0Aconfirm%20our%20theoretical%20claims.%20When%20the%20theorem%27s%20conditions%20are%20met%2C%20KoALA%0Aconsistently%20and%20effectively%20detects%20adversarial%20examples.%20On%20the%20full%20test%0Asets%2C%20KoALA%20achieves%20a%20precision%20of%200.94%20and%20a%20recall%20of%200.81%20on%0AResNet/CIFAR-10%2C%20and%20a%20precision%20of%200.66%20and%20a%20recall%20of%200.85%20on%0ACLIP/Tiny-ImageNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12752v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKoALA%253A%2520KL-L0%2520Adversarial%2520Detector%2520via%2520Label%2520Agreement%26entry.906535625%3DSiqi%2520Li%2520and%2520Yasser%2520Shoukry%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520are%2520highly%2520susceptible%2520to%2520adversarial%2520attacks%252C%2520which%250Apose%2520significant%2520risks%2520to%2520security-%2520and%2520safety-critical%2520applications.%2520We%250Apresent%2520KoALA%2520%2528KL-L0%2520Adversarial%2520detection%2520via%2520Label%2520Agreement%2529%252C%2520a%2520novel%252C%250Asemantics-free%2520adversarial%2520detector%2520that%2520requires%2520no%2520architectural%2520changes%2520or%250Aadversarial%2520retraining.%2520KoALA%2520operates%2520on%2520a%2520simple%2520principle%253A%2520it%2520detects%2520an%250Aadversarial%2520attack%2520when%2520class%2520predictions%2520from%2520two%2520complementary%2520similarity%250Ametrics%2520disagree.%2520These%2520metrics-KL%2520divergence%2520and%2520an%2520L0-based%2520similarity-are%250Aspecifically%2520chosen%2520to%2520detect%2520different%2520types%2520of%2520perturbations.%2520The%2520KL%250Adivergence%2520metric%2520is%2520sensitive%2520to%2520dense%252C%2520low-amplitude%2520shifts%252C%2520while%2520the%250AL0-based%2520similarity%2520is%2520designed%2520for%2520sparse%252C%2520high-impact%2520changes.%2520We%2520provide%2520a%250Aformal%2520proof%2520of%2520correctness%2520for%2520our%2520approach.%2520The%2520only%2520training%2520required%2520is%2520a%250Asimple%2520fine-tuning%2520step%2520on%2520a%2520pre-trained%2520image%2520encoder%2520using%2520clean%2520images%2520to%250Aensure%2520the%2520embeddings%2520align%2520well%2520with%2520both%2520metrics.%2520This%2520makes%2520KOALA%2520a%250Alightweight%252C%2520plug-and-play%2520solution%2520for%2520existing%2520models%2520and%2520various%2520data%250Amodalities.%2520Our%2520extensive%2520experiments%2520on%2520ResNet/CIFAR-10%2520and%2520CLIP/Tiny-ImageNet%250Aconfirm%2520our%2520theoretical%2520claims.%2520When%2520the%2520theorem%2527s%2520conditions%2520are%2520met%252C%2520KoALA%250Aconsistently%2520and%2520effectively%2520detects%2520adversarial%2520examples.%2520On%2520the%2520full%2520test%250Asets%252C%2520KoALA%2520achieves%2520a%2520precision%2520of%25200.94%2520and%2520a%2520recall%2520of%25200.81%2520on%250AResNet/CIFAR-10%252C%2520and%2520a%2520precision%2520of%25200.66%2520and%2520a%2520recall%2520of%25200.85%2520on%250ACLIP/Tiny-ImageNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12752v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KoALA%3A%20KL-L0%20Adversarial%20Detector%20via%20Label%20Agreement&entry.906535625=Siqi%20Li%20and%20Yasser%20Shoukry&entry.1292438233=%20%20Deep%20neural%20networks%20are%20highly%20susceptible%20to%20adversarial%20attacks%2C%20which%0Apose%20significant%20risks%20to%20security-%20and%20safety-critical%20applications.%20We%0Apresent%20KoALA%20%28KL-L0%20Adversarial%20detection%20via%20Label%20Agreement%29%2C%20a%20novel%2C%0Asemantics-free%20adversarial%20detector%20that%20requires%20no%20architectural%20changes%20or%0Aadversarial%20retraining.%20KoALA%20operates%20on%20a%20simple%20principle%3A%20it%20detects%20an%0Aadversarial%20attack%20when%20class%20predictions%20from%20two%20complementary%20similarity%0Ametrics%20disagree.%20These%20metrics-KL%20divergence%20and%20an%20L0-based%20similarity-are%0Aspecifically%20chosen%20to%20detect%20different%20types%20of%20perturbations.%20The%20KL%0Adivergence%20metric%20is%20sensitive%20to%20dense%2C%20low-amplitude%20shifts%2C%20while%20the%0AL0-based%20similarity%20is%20designed%20for%20sparse%2C%20high-impact%20changes.%20We%20provide%20a%0Aformal%20proof%20of%20correctness%20for%20our%20approach.%20The%20only%20training%20required%20is%20a%0Asimple%20fine-tuning%20step%20on%20a%20pre-trained%20image%20encoder%20using%20clean%20images%20to%0Aensure%20the%20embeddings%20align%20well%20with%20both%20metrics.%20This%20makes%20KOALA%20a%0Alightweight%2C%20plug-and-play%20solution%20for%20existing%20models%20and%20various%20data%0Amodalities.%20Our%20extensive%20experiments%20on%20ResNet/CIFAR-10%20and%20CLIP/Tiny-ImageNet%0Aconfirm%20our%20theoretical%20claims.%20When%20the%20theorem%27s%20conditions%20are%20met%2C%20KoALA%0Aconsistently%20and%20effectively%20detects%20adversarial%20examples.%20On%20the%20full%20test%0Asets%2C%20KoALA%20achieves%20a%20precision%20of%200.94%20and%20a%20recall%20of%200.81%20on%0AResNet/CIFAR-10%2C%20and%20a%20precision%20of%200.66%20and%20a%20recall%20of%200.85%20on%0ACLIP/Tiny-ImageNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12752v1&entry.124074799=Read"},
{"title": "AgentBuilder: Exploring Scaffolds for Prototyping User Experiences of\n  Interface Agents", "author": "Jenny T. Liang and Titus Barik and Jeffrey Nichols and Eldon Schoop and Ruijia Cheng", "abstract": "  Interface agents powered by generative AI models (referred to as \"agents\")\ncan automate actions based on user commands. An important aspect of developing\nagents is their user experience (i.e., agent experience). There is a growing\nneed to provide scaffolds for a broader set of individuals beyond AI engineers\nto prototype agent experiences, since they can contribute valuable perspectives\nto designing agent experiences. In this work, we explore the affordances agent\nprototyping systems should offer by conducting a requirements elicitation study\nwith 12 participants with varying experience with agents. We identify key\nactivities in agent experience prototyping and the desired capabilities of\nagent prototyping systems. We instantiate those capabilities in the\nAgentBuilder design probe for agent prototyping. We conduct an in situ agent\nprototyping study with 14 participants using AgentBuilder to validate the\ndesign requirements and elicit insights on how developers prototype agents and\nwhat their needs are in this process.\n", "link": "http://arxiv.org/abs/2510.04452v2", "date": "2025-10-14", "relevancy": 2.0577, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5655}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5135}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentBuilder%3A%20Exploring%20Scaffolds%20for%20Prototyping%20User%20Experiences%20of%0A%20%20Interface%20Agents&body=Title%3A%20AgentBuilder%3A%20Exploring%20Scaffolds%20for%20Prototyping%20User%20Experiences%20of%0A%20%20Interface%20Agents%0AAuthor%3A%20Jenny%20T.%20Liang%20and%20Titus%20Barik%20and%20Jeffrey%20Nichols%20and%20Eldon%20Schoop%20and%20Ruijia%20Cheng%0AAbstract%3A%20%20%20Interface%20agents%20powered%20by%20generative%20AI%20models%20%28referred%20to%20as%20%22agents%22%29%0Acan%20automate%20actions%20based%20on%20user%20commands.%20An%20important%20aspect%20of%20developing%0Aagents%20is%20their%20user%20experience%20%28i.e.%2C%20agent%20experience%29.%20There%20is%20a%20growing%0Aneed%20to%20provide%20scaffolds%20for%20a%20broader%20set%20of%20individuals%20beyond%20AI%20engineers%0Ato%20prototype%20agent%20experiences%2C%20since%20they%20can%20contribute%20valuable%20perspectives%0Ato%20designing%20agent%20experiences.%20In%20this%20work%2C%20we%20explore%20the%20affordances%20agent%0Aprototyping%20systems%20should%20offer%20by%20conducting%20a%20requirements%20elicitation%20study%0Awith%2012%20participants%20with%20varying%20experience%20with%20agents.%20We%20identify%20key%0Aactivities%20in%20agent%20experience%20prototyping%20and%20the%20desired%20capabilities%20of%0Aagent%20prototyping%20systems.%20We%20instantiate%20those%20capabilities%20in%20the%0AAgentBuilder%20design%20probe%20for%20agent%20prototyping.%20We%20conduct%20an%20in%20situ%20agent%0Aprototyping%20study%20with%2014%20participants%20using%20AgentBuilder%20to%20validate%20the%0Adesign%20requirements%20and%20elicit%20insights%20on%20how%20developers%20prototype%20agents%20and%0Awhat%20their%20needs%20are%20in%20this%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04452v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentBuilder%253A%2520Exploring%2520Scaffolds%2520for%2520Prototyping%2520User%2520Experiences%2520of%250A%2520%2520Interface%2520Agents%26entry.906535625%3DJenny%2520T.%2520Liang%2520and%2520Titus%2520Barik%2520and%2520Jeffrey%2520Nichols%2520and%2520Eldon%2520Schoop%2520and%2520Ruijia%2520Cheng%26entry.1292438233%3D%2520%2520Interface%2520agents%2520powered%2520by%2520generative%2520AI%2520models%2520%2528referred%2520to%2520as%2520%2522agents%2522%2529%250Acan%2520automate%2520actions%2520based%2520on%2520user%2520commands.%2520An%2520important%2520aspect%2520of%2520developing%250Aagents%2520is%2520their%2520user%2520experience%2520%2528i.e.%252C%2520agent%2520experience%2529.%2520There%2520is%2520a%2520growing%250Aneed%2520to%2520provide%2520scaffolds%2520for%2520a%2520broader%2520set%2520of%2520individuals%2520beyond%2520AI%2520engineers%250Ato%2520prototype%2520agent%2520experiences%252C%2520since%2520they%2520can%2520contribute%2520valuable%2520perspectives%250Ato%2520designing%2520agent%2520experiences.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520affordances%2520agent%250Aprototyping%2520systems%2520should%2520offer%2520by%2520conducting%2520a%2520requirements%2520elicitation%2520study%250Awith%252012%2520participants%2520with%2520varying%2520experience%2520with%2520agents.%2520We%2520identify%2520key%250Aactivities%2520in%2520agent%2520experience%2520prototyping%2520and%2520the%2520desired%2520capabilities%2520of%250Aagent%2520prototyping%2520systems.%2520We%2520instantiate%2520those%2520capabilities%2520in%2520the%250AAgentBuilder%2520design%2520probe%2520for%2520agent%2520prototyping.%2520We%2520conduct%2520an%2520in%2520situ%2520agent%250Aprototyping%2520study%2520with%252014%2520participants%2520using%2520AgentBuilder%2520to%2520validate%2520the%250Adesign%2520requirements%2520and%2520elicit%2520insights%2520on%2520how%2520developers%2520prototype%2520agents%2520and%250Awhat%2520their%2520needs%2520are%2520in%2520this%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04452v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentBuilder%3A%20Exploring%20Scaffolds%20for%20Prototyping%20User%20Experiences%20of%0A%20%20Interface%20Agents&entry.906535625=Jenny%20T.%20Liang%20and%20Titus%20Barik%20and%20Jeffrey%20Nichols%20and%20Eldon%20Schoop%20and%20Ruijia%20Cheng&entry.1292438233=%20%20Interface%20agents%20powered%20by%20generative%20AI%20models%20%28referred%20to%20as%20%22agents%22%29%0Acan%20automate%20actions%20based%20on%20user%20commands.%20An%20important%20aspect%20of%20developing%0Aagents%20is%20their%20user%20experience%20%28i.e.%2C%20agent%20experience%29.%20There%20is%20a%20growing%0Aneed%20to%20provide%20scaffolds%20for%20a%20broader%20set%20of%20individuals%20beyond%20AI%20engineers%0Ato%20prototype%20agent%20experiences%2C%20since%20they%20can%20contribute%20valuable%20perspectives%0Ato%20designing%20agent%20experiences.%20In%20this%20work%2C%20we%20explore%20the%20affordances%20agent%0Aprototyping%20systems%20should%20offer%20by%20conducting%20a%20requirements%20elicitation%20study%0Awith%2012%20participants%20with%20varying%20experience%20with%20agents.%20We%20identify%20key%0Aactivities%20in%20agent%20experience%20prototyping%20and%20the%20desired%20capabilities%20of%0Aagent%20prototyping%20systems.%20We%20instantiate%20those%20capabilities%20in%20the%0AAgentBuilder%20design%20probe%20for%20agent%20prototyping.%20We%20conduct%20an%20in%20situ%20agent%0Aprototyping%20study%20with%2014%20participants%20using%20AgentBuilder%20to%20validate%20the%0Adesign%20requirements%20and%20elicit%20insights%20on%20how%20developers%20prototype%20agents%20and%0Awhat%20their%20needs%20are%20in%20this%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04452v2&entry.124074799=Read"},
{"title": "Capturing More: Learning Multi-Domain Representations for Robust Online\n  Handwriting Verification", "author": "Peirong Zhang and Kai Ding and Lianwen Jin", "abstract": "  In this paper, we propose SPECTRUM, a temporal-frequency synergistic model\nthat unlocks the untapped potential of multi-domain representation learning for\nonline handwriting verification (OHV). SPECTRUM comprises three core\ncomponents: (1) a multi-scale interactor that finely combines temporal and\nfrequency features through dual-modal sequence interaction and multi-scale\naggregation, (2) a self-gated fusion module that dynamically integrates global\ntemporal and frequency features via self-driven balancing. These two components\nwork synergistically to achieve micro-to-macro spectral-temporal integration.\n(3) A multi-domain distance-based verifier then utilizes both temporal and\nfrequency representations to improve discrimination between genuine and forged\nhandwriting, surpassing conventional temporal-only approaches. Extensive\nexperiments demonstrate SPECTRUM's superior performance over existing OHV\nmethods, underscoring the effectiveness of temporal-frequency multi-domain\nlearning. Furthermore, we reveal that incorporating multiple handwritten\nbiometrics fundamentally enhances the discriminative power of handwriting\nrepresentations and facilitates verification. These findings not only validate\nthe efficacy of multi-domain learning in OHV but also pave the way for future\nresearch in multi-domain approaches across both feature and biometric domains.\nCode is publicly available at https://github.com/NiceRingNode/SPECTRUM.\n", "link": "http://arxiv.org/abs/2508.01427v2", "date": "2025-10-14", "relevancy": 2.0569, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5213}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.52}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Capturing%20More%3A%20Learning%20Multi-Domain%20Representations%20for%20Robust%20Online%0A%20%20Handwriting%20Verification&body=Title%3A%20Capturing%20More%3A%20Learning%20Multi-Domain%20Representations%20for%20Robust%20Online%0A%20%20Handwriting%20Verification%0AAuthor%3A%20Peirong%20Zhang%20and%20Kai%20Ding%20and%20Lianwen%20Jin%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20SPECTRUM%2C%20a%20temporal-frequency%20synergistic%20model%0Athat%20unlocks%20the%20untapped%20potential%20of%20multi-domain%20representation%20learning%20for%0Aonline%20handwriting%20verification%20%28OHV%29.%20SPECTRUM%20comprises%20three%20core%0Acomponents%3A%20%281%29%20a%20multi-scale%20interactor%20that%20finely%20combines%20temporal%20and%0Afrequency%20features%20through%20dual-modal%20sequence%20interaction%20and%20multi-scale%0Aaggregation%2C%20%282%29%20a%20self-gated%20fusion%20module%20that%20dynamically%20integrates%20global%0Atemporal%20and%20frequency%20features%20via%20self-driven%20balancing.%20These%20two%20components%0Awork%20synergistically%20to%20achieve%20micro-to-macro%20spectral-temporal%20integration.%0A%283%29%20A%20multi-domain%20distance-based%20verifier%20then%20utilizes%20both%20temporal%20and%0Afrequency%20representations%20to%20improve%20discrimination%20between%20genuine%20and%20forged%0Ahandwriting%2C%20surpassing%20conventional%20temporal-only%20approaches.%20Extensive%0Aexperiments%20demonstrate%20SPECTRUM%27s%20superior%20performance%20over%20existing%20OHV%0Amethods%2C%20underscoring%20the%20effectiveness%20of%20temporal-frequency%20multi-domain%0Alearning.%20Furthermore%2C%20we%20reveal%20that%20incorporating%20multiple%20handwritten%0Abiometrics%20fundamentally%20enhances%20the%20discriminative%20power%20of%20handwriting%0Arepresentations%20and%20facilitates%20verification.%20These%20findings%20not%20only%20validate%0Athe%20efficacy%20of%20multi-domain%20learning%20in%20OHV%20but%20also%20pave%20the%20way%20for%20future%0Aresearch%20in%20multi-domain%20approaches%20across%20both%20feature%20and%20biometric%20domains.%0ACode%20is%20publicly%20available%20at%20https%3A//github.com/NiceRingNode/SPECTRUM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.01427v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCapturing%2520More%253A%2520Learning%2520Multi-Domain%2520Representations%2520for%2520Robust%2520Online%250A%2520%2520Handwriting%2520Verification%26entry.906535625%3DPeirong%2520Zhang%2520and%2520Kai%2520Ding%2520and%2520Lianwen%2520Jin%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520SPECTRUM%252C%2520a%2520temporal-frequency%2520synergistic%2520model%250Athat%2520unlocks%2520the%2520untapped%2520potential%2520of%2520multi-domain%2520representation%2520learning%2520for%250Aonline%2520handwriting%2520verification%2520%2528OHV%2529.%2520SPECTRUM%2520comprises%2520three%2520core%250Acomponents%253A%2520%25281%2529%2520a%2520multi-scale%2520interactor%2520that%2520finely%2520combines%2520temporal%2520and%250Afrequency%2520features%2520through%2520dual-modal%2520sequence%2520interaction%2520and%2520multi-scale%250Aaggregation%252C%2520%25282%2529%2520a%2520self-gated%2520fusion%2520module%2520that%2520dynamically%2520integrates%2520global%250Atemporal%2520and%2520frequency%2520features%2520via%2520self-driven%2520balancing.%2520These%2520two%2520components%250Awork%2520synergistically%2520to%2520achieve%2520micro-to-macro%2520spectral-temporal%2520integration.%250A%25283%2529%2520A%2520multi-domain%2520distance-based%2520verifier%2520then%2520utilizes%2520both%2520temporal%2520and%250Afrequency%2520representations%2520to%2520improve%2520discrimination%2520between%2520genuine%2520and%2520forged%250Ahandwriting%252C%2520surpassing%2520conventional%2520temporal-only%2520approaches.%2520Extensive%250Aexperiments%2520demonstrate%2520SPECTRUM%2527s%2520superior%2520performance%2520over%2520existing%2520OHV%250Amethods%252C%2520underscoring%2520the%2520effectiveness%2520of%2520temporal-frequency%2520multi-domain%250Alearning.%2520Furthermore%252C%2520we%2520reveal%2520that%2520incorporating%2520multiple%2520handwritten%250Abiometrics%2520fundamentally%2520enhances%2520the%2520discriminative%2520power%2520of%2520handwriting%250Arepresentations%2520and%2520facilitates%2520verification.%2520These%2520findings%2520not%2520only%2520validate%250Athe%2520efficacy%2520of%2520multi-domain%2520learning%2520in%2520OHV%2520but%2520also%2520pave%2520the%2520way%2520for%2520future%250Aresearch%2520in%2520multi-domain%2520approaches%2520across%2520both%2520feature%2520and%2520biometric%2520domains.%250ACode%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/NiceRingNode/SPECTRUM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01427v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Capturing%20More%3A%20Learning%20Multi-Domain%20Representations%20for%20Robust%20Online%0A%20%20Handwriting%20Verification&entry.906535625=Peirong%20Zhang%20and%20Kai%20Ding%20and%20Lianwen%20Jin&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20SPECTRUM%2C%20a%20temporal-frequency%20synergistic%20model%0Athat%20unlocks%20the%20untapped%20potential%20of%20multi-domain%20representation%20learning%20for%0Aonline%20handwriting%20verification%20%28OHV%29.%20SPECTRUM%20comprises%20three%20core%0Acomponents%3A%20%281%29%20a%20multi-scale%20interactor%20that%20finely%20combines%20temporal%20and%0Afrequency%20features%20through%20dual-modal%20sequence%20interaction%20and%20multi-scale%0Aaggregation%2C%20%282%29%20a%20self-gated%20fusion%20module%20that%20dynamically%20integrates%20global%0Atemporal%20and%20frequency%20features%20via%20self-driven%20balancing.%20These%20two%20components%0Awork%20synergistically%20to%20achieve%20micro-to-macro%20spectral-temporal%20integration.%0A%283%29%20A%20multi-domain%20distance-based%20verifier%20then%20utilizes%20both%20temporal%20and%0Afrequency%20representations%20to%20improve%20discrimination%20between%20genuine%20and%20forged%0Ahandwriting%2C%20surpassing%20conventional%20temporal-only%20approaches.%20Extensive%0Aexperiments%20demonstrate%20SPECTRUM%27s%20superior%20performance%20over%20existing%20OHV%0Amethods%2C%20underscoring%20the%20effectiveness%20of%20temporal-frequency%20multi-domain%0Alearning.%20Furthermore%2C%20we%20reveal%20that%20incorporating%20multiple%20handwritten%0Abiometrics%20fundamentally%20enhances%20the%20discriminative%20power%20of%20handwriting%0Arepresentations%20and%20facilitates%20verification.%20These%20findings%20not%20only%20validate%0Athe%20efficacy%20of%20multi-domain%20learning%20in%20OHV%20but%20also%20pave%20the%20way%20for%20future%0Aresearch%20in%20multi-domain%20approaches%20across%20both%20feature%20and%20biometric%20domains.%0ACode%20is%20publicly%20available%20at%20https%3A//github.com/NiceRingNode/SPECTRUM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.01427v2&entry.124074799=Read"},
{"title": "Laminar: A Scalable Asynchronous RL Post-Training Framework", "author": "Guangming Sheng and Yuxuan Tong and Borui Wan and Wang Zhang and Chaobo Jia and Xibin Wu and Yuqi Wu and Xiang Li and Chi Zhang and Yanghua Peng and Haibin Lin and Xin Liu and Chuan Wu", "abstract": "  Reinforcement learning (RL) post-training for Large Language Models (LLMs) is\nnow scaling to large clusters and running for extended durations to enhance\nmodel reasoning performance. However, the scalability of existing RL frameworks\nis limited, as extreme long-tail skewness in RL trajectory generation causes\nsevere GPU underutilization. Current asynchronous RL systems attempt to\nmitigate this, but they rely on global weight synchronization between the actor\nand all rollouts, which creates a rigid model update schedule. This global\nsynchronization is ill-suited for the highly skewed and evolving distribution\nof trajectory generation latency in RL training, crippling training efficiency.\nOur key insight is that efficient scaling requires breaking this lockstep\nthrough trajectory-level asynchrony, which generates and consumes each\ntrajectory independently. We propose Laminar, a scalable and robust RL\npost-training system built on a fully decoupled architecture. First, we replace\nglobal updates with a tier of relay workers acting as a distributed parameter\nservice. This enables asynchronous and fine-grained weight synchronization,\nallowing rollouts to pull the latest weight anytime without stalling the\nactor's training loop. Second, a dynamic repack mechanism consolidates\nlong-tail trajectories onto a few dedicated rollouts, maximizing generation\nthroughput. The fully decoupled design also isolates failures, ensuring\nrobustness for long-running jobs. Our evaluation on a 1024-GPU cluster shows\nthat Laminar achieves up to 5.48$\\times$ training throughput speedup over\nstate-of-the-art systems, while reducing model convergence time.\n", "link": "http://arxiv.org/abs/2510.12633v1", "date": "2025-10-14", "relevancy": 2.0564, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5286}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5214}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Laminar%3A%20A%20Scalable%20Asynchronous%20RL%20Post-Training%20Framework&body=Title%3A%20Laminar%3A%20A%20Scalable%20Asynchronous%20RL%20Post-Training%20Framework%0AAuthor%3A%20Guangming%20Sheng%20and%20Yuxuan%20Tong%20and%20Borui%20Wan%20and%20Wang%20Zhang%20and%20Chaobo%20Jia%20and%20Xibin%20Wu%20and%20Yuqi%20Wu%20and%20Xiang%20Li%20and%20Chi%20Zhang%20and%20Yanghua%20Peng%20and%20Haibin%20Lin%20and%20Xin%20Liu%20and%20Chuan%20Wu%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20post-training%20for%20Large%20Language%20Models%20%28LLMs%29%20is%0Anow%20scaling%20to%20large%20clusters%20and%20running%20for%20extended%20durations%20to%20enhance%0Amodel%20reasoning%20performance.%20However%2C%20the%20scalability%20of%20existing%20RL%20frameworks%0Ais%20limited%2C%20as%20extreme%20long-tail%20skewness%20in%20RL%20trajectory%20generation%20causes%0Asevere%20GPU%20underutilization.%20Current%20asynchronous%20RL%20systems%20attempt%20to%0Amitigate%20this%2C%20but%20they%20rely%20on%20global%20weight%20synchronization%20between%20the%20actor%0Aand%20all%20rollouts%2C%20which%20creates%20a%20rigid%20model%20update%20schedule.%20This%20global%0Asynchronization%20is%20ill-suited%20for%20the%20highly%20skewed%20and%20evolving%20distribution%0Aof%20trajectory%20generation%20latency%20in%20RL%20training%2C%20crippling%20training%20efficiency.%0AOur%20key%20insight%20is%20that%20efficient%20scaling%20requires%20breaking%20this%20lockstep%0Athrough%20trajectory-level%20asynchrony%2C%20which%20generates%20and%20consumes%20each%0Atrajectory%20independently.%20We%20propose%20Laminar%2C%20a%20scalable%20and%20robust%20RL%0Apost-training%20system%20built%20on%20a%20fully%20decoupled%20architecture.%20First%2C%20we%20replace%0Aglobal%20updates%20with%20a%20tier%20of%20relay%20workers%20acting%20as%20a%20distributed%20parameter%0Aservice.%20This%20enables%20asynchronous%20and%20fine-grained%20weight%20synchronization%2C%0Aallowing%20rollouts%20to%20pull%20the%20latest%20weight%20anytime%20without%20stalling%20the%0Aactor%27s%20training%20loop.%20Second%2C%20a%20dynamic%20repack%20mechanism%20consolidates%0Along-tail%20trajectories%20onto%20a%20few%20dedicated%20rollouts%2C%20maximizing%20generation%0Athroughput.%20The%20fully%20decoupled%20design%20also%20isolates%20failures%2C%20ensuring%0Arobustness%20for%20long-running%20jobs.%20Our%20evaluation%20on%20a%201024-GPU%20cluster%20shows%0Athat%20Laminar%20achieves%20up%20to%205.48%24%5Ctimes%24%20training%20throughput%20speedup%20over%0Astate-of-the-art%20systems%2C%20while%20reducing%20model%20convergence%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaminar%253A%2520A%2520Scalable%2520Asynchronous%2520RL%2520Post-Training%2520Framework%26entry.906535625%3DGuangming%2520Sheng%2520and%2520Yuxuan%2520Tong%2520and%2520Borui%2520Wan%2520and%2520Wang%2520Zhang%2520and%2520Chaobo%2520Jia%2520and%2520Xibin%2520Wu%2520and%2520Yuqi%2520Wu%2520and%2520Xiang%2520Li%2520and%2520Chi%2520Zhang%2520and%2520Yanghua%2520Peng%2520and%2520Haibin%2520Lin%2520and%2520Xin%2520Liu%2520and%2520Chuan%2520Wu%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520post-training%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%250Anow%2520scaling%2520to%2520large%2520clusters%2520and%2520running%2520for%2520extended%2520durations%2520to%2520enhance%250Amodel%2520reasoning%2520performance.%2520However%252C%2520the%2520scalability%2520of%2520existing%2520RL%2520frameworks%250Ais%2520limited%252C%2520as%2520extreme%2520long-tail%2520skewness%2520in%2520RL%2520trajectory%2520generation%2520causes%250Asevere%2520GPU%2520underutilization.%2520Current%2520asynchronous%2520RL%2520systems%2520attempt%2520to%250Amitigate%2520this%252C%2520but%2520they%2520rely%2520on%2520global%2520weight%2520synchronization%2520between%2520the%2520actor%250Aand%2520all%2520rollouts%252C%2520which%2520creates%2520a%2520rigid%2520model%2520update%2520schedule.%2520This%2520global%250Asynchronization%2520is%2520ill-suited%2520for%2520the%2520highly%2520skewed%2520and%2520evolving%2520distribution%250Aof%2520trajectory%2520generation%2520latency%2520in%2520RL%2520training%252C%2520crippling%2520training%2520efficiency.%250AOur%2520key%2520insight%2520is%2520that%2520efficient%2520scaling%2520requires%2520breaking%2520this%2520lockstep%250Athrough%2520trajectory-level%2520asynchrony%252C%2520which%2520generates%2520and%2520consumes%2520each%250Atrajectory%2520independently.%2520We%2520propose%2520Laminar%252C%2520a%2520scalable%2520and%2520robust%2520RL%250Apost-training%2520system%2520built%2520on%2520a%2520fully%2520decoupled%2520architecture.%2520First%252C%2520we%2520replace%250Aglobal%2520updates%2520with%2520a%2520tier%2520of%2520relay%2520workers%2520acting%2520as%2520a%2520distributed%2520parameter%250Aservice.%2520This%2520enables%2520asynchronous%2520and%2520fine-grained%2520weight%2520synchronization%252C%250Aallowing%2520rollouts%2520to%2520pull%2520the%2520latest%2520weight%2520anytime%2520without%2520stalling%2520the%250Aactor%2527s%2520training%2520loop.%2520Second%252C%2520a%2520dynamic%2520repack%2520mechanism%2520consolidates%250Along-tail%2520trajectories%2520onto%2520a%2520few%2520dedicated%2520rollouts%252C%2520maximizing%2520generation%250Athroughput.%2520The%2520fully%2520decoupled%2520design%2520also%2520isolates%2520failures%252C%2520ensuring%250Arobustness%2520for%2520long-running%2520jobs.%2520Our%2520evaluation%2520on%2520a%25201024-GPU%2520cluster%2520shows%250Athat%2520Laminar%2520achieves%2520up%2520to%25205.48%2524%255Ctimes%2524%2520training%2520throughput%2520speedup%2520over%250Astate-of-the-art%2520systems%252C%2520while%2520reducing%2520model%2520convergence%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Laminar%3A%20A%20Scalable%20Asynchronous%20RL%20Post-Training%20Framework&entry.906535625=Guangming%20Sheng%20and%20Yuxuan%20Tong%20and%20Borui%20Wan%20and%20Wang%20Zhang%20and%20Chaobo%20Jia%20and%20Xibin%20Wu%20and%20Yuqi%20Wu%20and%20Xiang%20Li%20and%20Chi%20Zhang%20and%20Yanghua%20Peng%20and%20Haibin%20Lin%20and%20Xin%20Liu%20and%20Chuan%20Wu&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20post-training%20for%20Large%20Language%20Models%20%28LLMs%29%20is%0Anow%20scaling%20to%20large%20clusters%20and%20running%20for%20extended%20durations%20to%20enhance%0Amodel%20reasoning%20performance.%20However%2C%20the%20scalability%20of%20existing%20RL%20frameworks%0Ais%20limited%2C%20as%20extreme%20long-tail%20skewness%20in%20RL%20trajectory%20generation%20causes%0Asevere%20GPU%20underutilization.%20Current%20asynchronous%20RL%20systems%20attempt%20to%0Amitigate%20this%2C%20but%20they%20rely%20on%20global%20weight%20synchronization%20between%20the%20actor%0Aand%20all%20rollouts%2C%20which%20creates%20a%20rigid%20model%20update%20schedule.%20This%20global%0Asynchronization%20is%20ill-suited%20for%20the%20highly%20skewed%20and%20evolving%20distribution%0Aof%20trajectory%20generation%20latency%20in%20RL%20training%2C%20crippling%20training%20efficiency.%0AOur%20key%20insight%20is%20that%20efficient%20scaling%20requires%20breaking%20this%20lockstep%0Athrough%20trajectory-level%20asynchrony%2C%20which%20generates%20and%20consumes%20each%0Atrajectory%20independently.%20We%20propose%20Laminar%2C%20a%20scalable%20and%20robust%20RL%0Apost-training%20system%20built%20on%20a%20fully%20decoupled%20architecture.%20First%2C%20we%20replace%0Aglobal%20updates%20with%20a%20tier%20of%20relay%20workers%20acting%20as%20a%20distributed%20parameter%0Aservice.%20This%20enables%20asynchronous%20and%20fine-grained%20weight%20synchronization%2C%0Aallowing%20rollouts%20to%20pull%20the%20latest%20weight%20anytime%20without%20stalling%20the%0Aactor%27s%20training%20loop.%20Second%2C%20a%20dynamic%20repack%20mechanism%20consolidates%0Along-tail%20trajectories%20onto%20a%20few%20dedicated%20rollouts%2C%20maximizing%20generation%0Athroughput.%20The%20fully%20decoupled%20design%20also%20isolates%20failures%2C%20ensuring%0Arobustness%20for%20long-running%20jobs.%20Our%20evaluation%20on%20a%201024-GPU%20cluster%20shows%0Athat%20Laminar%20achieves%20up%20to%205.48%24%5Ctimes%24%20training%20throughput%20speedup%20over%0Astate-of-the-art%20systems%2C%20while%20reducing%20model%20convergence%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12633v1&entry.124074799=Read"},
{"title": "PubSub-VFL: Towards Efficient Two-Party Split Learning in Heterogeneous\n  Environments via Publisher/Subscriber Architecture", "author": "Yi Liu and Yang Liu and Leqian Zheng and Jue Hong and Junjie Shi and Qingyou Yang and Ye Wu and Cong Wang", "abstract": "  With the rapid advancement of the digital economy, data collaboration between\norganizations has become a well-established business model, driving the growth\nof various industries. However, privacy concerns make direct data sharing\nimpractical. To address this, Two-Party Split Learning (a.k.a. Vertical\nFederated Learning (VFL)) has emerged as a promising solution for secure\ncollaborative learning. Despite its advantages, this architecture still suffers\nfrom low computational resource utilization and training efficiency.\nSpecifically, its synchronous dependency design increases training latency,\nwhile resource and data heterogeneity among participants further hinder\nefficient computation. To overcome these challenges, we propose PubSub-VFL, a\nnovel VFL paradigm with a Publisher/Subscriber architecture optimized for\ntwo-party collaborative learning with high computational efficiency. PubSub-VFL\nleverages the decoupling capabilities of the Pub/Sub architecture and the data\nparallelism of the parameter server architecture to design a hierarchical\nasynchronous mechanism, reducing training latency and improving system\nefficiency. Additionally, to mitigate the training imbalance caused by resource\nand data heterogeneity, we formalize an optimization problem based on\nparticipants' system profiles, enabling the selection of optimal\nhyperparameters while preserving privacy. We conduct a theoretical analysis to\ndemonstrate that PubSub-VFL achieves stable convergence and is compatible with\nsecurity protocols such as differential privacy. Extensive case studies on five\nbenchmark datasets further validate its effectiveness, showing that, compared\nto state-of-the-art baselines, PubSub-VFL not only accelerates training by $2\n\\sim 7\\times$ without compromising accuracy, but also achieves a computational\nresource utilization rate of up to 91.07%.\n", "link": "http://arxiv.org/abs/2510.12494v1", "date": "2025-10-14", "relevancy": 1.406, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4791}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4589}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PubSub-VFL%3A%20Towards%20Efficient%20Two-Party%20Split%20Learning%20in%20Heterogeneous%0A%20%20Environments%20via%20Publisher/Subscriber%20Architecture&body=Title%3A%20PubSub-VFL%3A%20Towards%20Efficient%20Two-Party%20Split%20Learning%20in%20Heterogeneous%0A%20%20Environments%20via%20Publisher/Subscriber%20Architecture%0AAuthor%3A%20Yi%20Liu%20and%20Yang%20Liu%20and%20Leqian%20Zheng%20and%20Jue%20Hong%20and%20Junjie%20Shi%20and%20Qingyou%20Yang%20and%20Ye%20Wu%20and%20Cong%20Wang%0AAbstract%3A%20%20%20With%20the%20rapid%20advancement%20of%20the%20digital%20economy%2C%20data%20collaboration%20between%0Aorganizations%20has%20become%20a%20well-established%20business%20model%2C%20driving%20the%20growth%0Aof%20various%20industries.%20However%2C%20privacy%20concerns%20make%20direct%20data%20sharing%0Aimpractical.%20To%20address%20this%2C%20Two-Party%20Split%20Learning%20%28a.k.a.%20Vertical%0AFederated%20Learning%20%28VFL%29%29%20has%20emerged%20as%20a%20promising%20solution%20for%20secure%0Acollaborative%20learning.%20Despite%20its%20advantages%2C%20this%20architecture%20still%20suffers%0Afrom%20low%20computational%20resource%20utilization%20and%20training%20efficiency.%0ASpecifically%2C%20its%20synchronous%20dependency%20design%20increases%20training%20latency%2C%0Awhile%20resource%20and%20data%20heterogeneity%20among%20participants%20further%20hinder%0Aefficient%20computation.%20To%20overcome%20these%20challenges%2C%20we%20propose%20PubSub-VFL%2C%20a%0Anovel%20VFL%20paradigm%20with%20a%20Publisher/Subscriber%20architecture%20optimized%20for%0Atwo-party%20collaborative%20learning%20with%20high%20computational%20efficiency.%20PubSub-VFL%0Aleverages%20the%20decoupling%20capabilities%20of%20the%20Pub/Sub%20architecture%20and%20the%20data%0Aparallelism%20of%20the%20parameter%20server%20architecture%20to%20design%20a%20hierarchical%0Aasynchronous%20mechanism%2C%20reducing%20training%20latency%20and%20improving%20system%0Aefficiency.%20Additionally%2C%20to%20mitigate%20the%20training%20imbalance%20caused%20by%20resource%0Aand%20data%20heterogeneity%2C%20we%20formalize%20an%20optimization%20problem%20based%20on%0Aparticipants%27%20system%20profiles%2C%20enabling%20the%20selection%20of%20optimal%0Ahyperparameters%20while%20preserving%20privacy.%20We%20conduct%20a%20theoretical%20analysis%20to%0Ademonstrate%20that%20PubSub-VFL%20achieves%20stable%20convergence%20and%20is%20compatible%20with%0Asecurity%20protocols%20such%20as%20differential%20privacy.%20Extensive%20case%20studies%20on%20five%0Abenchmark%20datasets%20further%20validate%20its%20effectiveness%2C%20showing%20that%2C%20compared%0Ato%20state-of-the-art%20baselines%2C%20PubSub-VFL%20not%20only%20accelerates%20training%20by%20%242%0A%5Csim%207%5Ctimes%24%20without%20compromising%20accuracy%2C%20but%20also%20achieves%20a%20computational%0Aresource%20utilization%20rate%20of%20up%20to%2091.07%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPubSub-VFL%253A%2520Towards%2520Efficient%2520Two-Party%2520Split%2520Learning%2520in%2520Heterogeneous%250A%2520%2520Environments%2520via%2520Publisher/Subscriber%2520Architecture%26entry.906535625%3DYi%2520Liu%2520and%2520Yang%2520Liu%2520and%2520Leqian%2520Zheng%2520and%2520Jue%2520Hong%2520and%2520Junjie%2520Shi%2520and%2520Qingyou%2520Yang%2520and%2520Ye%2520Wu%2520and%2520Cong%2520Wang%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancement%2520of%2520the%2520digital%2520economy%252C%2520data%2520collaboration%2520between%250Aorganizations%2520has%2520become%2520a%2520well-established%2520business%2520model%252C%2520driving%2520the%2520growth%250Aof%2520various%2520industries.%2520However%252C%2520privacy%2520concerns%2520make%2520direct%2520data%2520sharing%250Aimpractical.%2520To%2520address%2520this%252C%2520Two-Party%2520Split%2520Learning%2520%2528a.k.a.%2520Vertical%250AFederated%2520Learning%2520%2528VFL%2529%2529%2520has%2520emerged%2520as%2520a%2520promising%2520solution%2520for%2520secure%250Acollaborative%2520learning.%2520Despite%2520its%2520advantages%252C%2520this%2520architecture%2520still%2520suffers%250Afrom%2520low%2520computational%2520resource%2520utilization%2520and%2520training%2520efficiency.%250ASpecifically%252C%2520its%2520synchronous%2520dependency%2520design%2520increases%2520training%2520latency%252C%250Awhile%2520resource%2520and%2520data%2520heterogeneity%2520among%2520participants%2520further%2520hinder%250Aefficient%2520computation.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520PubSub-VFL%252C%2520a%250Anovel%2520VFL%2520paradigm%2520with%2520a%2520Publisher/Subscriber%2520architecture%2520optimized%2520for%250Atwo-party%2520collaborative%2520learning%2520with%2520high%2520computational%2520efficiency.%2520PubSub-VFL%250Aleverages%2520the%2520decoupling%2520capabilities%2520of%2520the%2520Pub/Sub%2520architecture%2520and%2520the%2520data%250Aparallelism%2520of%2520the%2520parameter%2520server%2520architecture%2520to%2520design%2520a%2520hierarchical%250Aasynchronous%2520mechanism%252C%2520reducing%2520training%2520latency%2520and%2520improving%2520system%250Aefficiency.%2520Additionally%252C%2520to%2520mitigate%2520the%2520training%2520imbalance%2520caused%2520by%2520resource%250Aand%2520data%2520heterogeneity%252C%2520we%2520formalize%2520an%2520optimization%2520problem%2520based%2520on%250Aparticipants%2527%2520system%2520profiles%252C%2520enabling%2520the%2520selection%2520of%2520optimal%250Ahyperparameters%2520while%2520preserving%2520privacy.%2520We%2520conduct%2520a%2520theoretical%2520analysis%2520to%250Ademonstrate%2520that%2520PubSub-VFL%2520achieves%2520stable%2520convergence%2520and%2520is%2520compatible%2520with%250Asecurity%2520protocols%2520such%2520as%2520differential%2520privacy.%2520Extensive%2520case%2520studies%2520on%2520five%250Abenchmark%2520datasets%2520further%2520validate%2520its%2520effectiveness%252C%2520showing%2520that%252C%2520compared%250Ato%2520state-of-the-art%2520baselines%252C%2520PubSub-VFL%2520not%2520only%2520accelerates%2520training%2520by%2520%25242%250A%255Csim%25207%255Ctimes%2524%2520without%2520compromising%2520accuracy%252C%2520but%2520also%2520achieves%2520a%2520computational%250Aresource%2520utilization%2520rate%2520of%2520up%2520to%252091.07%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PubSub-VFL%3A%20Towards%20Efficient%20Two-Party%20Split%20Learning%20in%20Heterogeneous%0A%20%20Environments%20via%20Publisher/Subscriber%20Architecture&entry.906535625=Yi%20Liu%20and%20Yang%20Liu%20and%20Leqian%20Zheng%20and%20Jue%20Hong%20and%20Junjie%20Shi%20and%20Qingyou%20Yang%20and%20Ye%20Wu%20and%20Cong%20Wang&entry.1292438233=%20%20With%20the%20rapid%20advancement%20of%20the%20digital%20economy%2C%20data%20collaboration%20between%0Aorganizations%20has%20become%20a%20well-established%20business%20model%2C%20driving%20the%20growth%0Aof%20various%20industries.%20However%2C%20privacy%20concerns%20make%20direct%20data%20sharing%0Aimpractical.%20To%20address%20this%2C%20Two-Party%20Split%20Learning%20%28a.k.a.%20Vertical%0AFederated%20Learning%20%28VFL%29%29%20has%20emerged%20as%20a%20promising%20solution%20for%20secure%0Acollaborative%20learning.%20Despite%20its%20advantages%2C%20this%20architecture%20still%20suffers%0Afrom%20low%20computational%20resource%20utilization%20and%20training%20efficiency.%0ASpecifically%2C%20its%20synchronous%20dependency%20design%20increases%20training%20latency%2C%0Awhile%20resource%20and%20data%20heterogeneity%20among%20participants%20further%20hinder%0Aefficient%20computation.%20To%20overcome%20these%20challenges%2C%20we%20propose%20PubSub-VFL%2C%20a%0Anovel%20VFL%20paradigm%20with%20a%20Publisher/Subscriber%20architecture%20optimized%20for%0Atwo-party%20collaborative%20learning%20with%20high%20computational%20efficiency.%20PubSub-VFL%0Aleverages%20the%20decoupling%20capabilities%20of%20the%20Pub/Sub%20architecture%20and%20the%20data%0Aparallelism%20of%20the%20parameter%20server%20architecture%20to%20design%20a%20hierarchical%0Aasynchronous%20mechanism%2C%20reducing%20training%20latency%20and%20improving%20system%0Aefficiency.%20Additionally%2C%20to%20mitigate%20the%20training%20imbalance%20caused%20by%20resource%0Aand%20data%20heterogeneity%2C%20we%20formalize%20an%20optimization%20problem%20based%20on%0Aparticipants%27%20system%20profiles%2C%20enabling%20the%20selection%20of%20optimal%0Ahyperparameters%20while%20preserving%20privacy.%20We%20conduct%20a%20theoretical%20analysis%20to%0Ademonstrate%20that%20PubSub-VFL%20achieves%20stable%20convergence%20and%20is%20compatible%20with%0Asecurity%20protocols%20such%20as%20differential%20privacy.%20Extensive%20case%20studies%20on%20five%0Abenchmark%20datasets%20further%20validate%20its%20effectiveness%2C%20showing%20that%2C%20compared%0Ato%20state-of-the-art%20baselines%2C%20PubSub-VFL%20not%20only%20accelerates%20training%20by%20%242%0A%5Csim%207%5Ctimes%24%20without%20compromising%20accuracy%2C%20but%20also%20achieves%20a%20computational%0Aresource%20utilization%20rate%20of%20up%20to%2091.07%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12494v1&entry.124074799=Read"},
{"title": "CTRL-Rec: Controlling Recommender Systems With Natural Language", "author": "Micah Carroll and Adeline Foote and Kevin Feng and Marcus Williams and Anca Dragan and W. Bradley Knox and Smitha Milli", "abstract": "  When users are dissatisfied with recommendations from a recommender system,\nthey often lack fine-grained controls for changing them. Large language models\n(LLMs) offer a solution by allowing users to guide their recommendations\nthrough natural language requests (e.g., \"I want to see respectful posts with a\ndifferent perspective than mine\"). We propose a method, CTRL-Rec, that allows\nfor natural language control of traditional recommender systems in real-time\nwith computational efficiency. Specifically, at training time, we use an LLM to\nsimulate whether users would approve of items based on their language requests,\nand we train embedding models that approximate such simulated judgments. We\nthen integrate these user-request-based predictions into the standard weighting\nof signals that traditional recommender systems optimize. At deployment time,\nwe require only a single LLM embedding computation per user request, allowing\nfor real-time control of recommendations. In experiments with the MovieLens\ndataset, our method consistently allows for fine-grained control across a\ndiversity of requests. In a study with 19 Letterboxd users, we find that\nCTRL-Rec was positively received by users and significantly enhanced users'\nsense of control and satisfaction with recommendations compared to traditional\ncontrols.\n", "link": "http://arxiv.org/abs/2510.12742v1", "date": "2025-10-14", "relevancy": 1.4585, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.495}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4838}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CTRL-Rec%3A%20Controlling%20Recommender%20Systems%20With%20Natural%20Language&body=Title%3A%20CTRL-Rec%3A%20Controlling%20Recommender%20Systems%20With%20Natural%20Language%0AAuthor%3A%20Micah%20Carroll%20and%20Adeline%20Foote%20and%20Kevin%20Feng%20and%20Marcus%20Williams%20and%20Anca%20Dragan%20and%20W.%20Bradley%20Knox%20and%20Smitha%20Milli%0AAbstract%3A%20%20%20When%20users%20are%20dissatisfied%20with%20recommendations%20from%20a%20recommender%20system%2C%0Athey%20often%20lack%20fine-grained%20controls%20for%20changing%20them.%20Large%20language%20models%0A%28LLMs%29%20offer%20a%20solution%20by%20allowing%20users%20to%20guide%20their%20recommendations%0Athrough%20natural%20language%20requests%20%28e.g.%2C%20%22I%20want%20to%20see%20respectful%20posts%20with%20a%0Adifferent%20perspective%20than%20mine%22%29.%20We%20propose%20a%20method%2C%20CTRL-Rec%2C%20that%20allows%0Afor%20natural%20language%20control%20of%20traditional%20recommender%20systems%20in%20real-time%0Awith%20computational%20efficiency.%20Specifically%2C%20at%20training%20time%2C%20we%20use%20an%20LLM%20to%0Asimulate%20whether%20users%20would%20approve%20of%20items%20based%20on%20their%20language%20requests%2C%0Aand%20we%20train%20embedding%20models%20that%20approximate%20such%20simulated%20judgments.%20We%0Athen%20integrate%20these%20user-request-based%20predictions%20into%20the%20standard%20weighting%0Aof%20signals%20that%20traditional%20recommender%20systems%20optimize.%20At%20deployment%20time%2C%0Awe%20require%20only%20a%20single%20LLM%20embedding%20computation%20per%20user%20request%2C%20allowing%0Afor%20real-time%20control%20of%20recommendations.%20In%20experiments%20with%20the%20MovieLens%0Adataset%2C%20our%20method%20consistently%20allows%20for%20fine-grained%20control%20across%20a%0Adiversity%20of%20requests.%20In%20a%20study%20with%2019%20Letterboxd%20users%2C%20we%20find%20that%0ACTRL-Rec%20was%20positively%20received%20by%20users%20and%20significantly%20enhanced%20users%27%0Asense%20of%20control%20and%20satisfaction%20with%20recommendations%20compared%20to%20traditional%0Acontrols.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCTRL-Rec%253A%2520Controlling%2520Recommender%2520Systems%2520With%2520Natural%2520Language%26entry.906535625%3DMicah%2520Carroll%2520and%2520Adeline%2520Foote%2520and%2520Kevin%2520Feng%2520and%2520Marcus%2520Williams%2520and%2520Anca%2520Dragan%2520and%2520W.%2520Bradley%2520Knox%2520and%2520Smitha%2520Milli%26entry.1292438233%3D%2520%2520When%2520users%2520are%2520dissatisfied%2520with%2520recommendations%2520from%2520a%2520recommender%2520system%252C%250Athey%2520often%2520lack%2520fine-grained%2520controls%2520for%2520changing%2520them.%2520Large%2520language%2520models%250A%2528LLMs%2529%2520offer%2520a%2520solution%2520by%2520allowing%2520users%2520to%2520guide%2520their%2520recommendations%250Athrough%2520natural%2520language%2520requests%2520%2528e.g.%252C%2520%2522I%2520want%2520to%2520see%2520respectful%2520posts%2520with%2520a%250Adifferent%2520perspective%2520than%2520mine%2522%2529.%2520We%2520propose%2520a%2520method%252C%2520CTRL-Rec%252C%2520that%2520allows%250Afor%2520natural%2520language%2520control%2520of%2520traditional%2520recommender%2520systems%2520in%2520real-time%250Awith%2520computational%2520efficiency.%2520Specifically%252C%2520at%2520training%2520time%252C%2520we%2520use%2520an%2520LLM%2520to%250Asimulate%2520whether%2520users%2520would%2520approve%2520of%2520items%2520based%2520on%2520their%2520language%2520requests%252C%250Aand%2520we%2520train%2520embedding%2520models%2520that%2520approximate%2520such%2520simulated%2520judgments.%2520We%250Athen%2520integrate%2520these%2520user-request-based%2520predictions%2520into%2520the%2520standard%2520weighting%250Aof%2520signals%2520that%2520traditional%2520recommender%2520systems%2520optimize.%2520At%2520deployment%2520time%252C%250Awe%2520require%2520only%2520a%2520single%2520LLM%2520embedding%2520computation%2520per%2520user%2520request%252C%2520allowing%250Afor%2520real-time%2520control%2520of%2520recommendations.%2520In%2520experiments%2520with%2520the%2520MovieLens%250Adataset%252C%2520our%2520method%2520consistently%2520allows%2520for%2520fine-grained%2520control%2520across%2520a%250Adiversity%2520of%2520requests.%2520In%2520a%2520study%2520with%252019%2520Letterboxd%2520users%252C%2520we%2520find%2520that%250ACTRL-Rec%2520was%2520positively%2520received%2520by%2520users%2520and%2520significantly%2520enhanced%2520users%2527%250Asense%2520of%2520control%2520and%2520satisfaction%2520with%2520recommendations%2520compared%2520to%2520traditional%250Acontrols.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CTRL-Rec%3A%20Controlling%20Recommender%20Systems%20With%20Natural%20Language&entry.906535625=Micah%20Carroll%20and%20Adeline%20Foote%20and%20Kevin%20Feng%20and%20Marcus%20Williams%20and%20Anca%20Dragan%20and%20W.%20Bradley%20Knox%20and%20Smitha%20Milli&entry.1292438233=%20%20When%20users%20are%20dissatisfied%20with%20recommendations%20from%20a%20recommender%20system%2C%0Athey%20often%20lack%20fine-grained%20controls%20for%20changing%20them.%20Large%20language%20models%0A%28LLMs%29%20offer%20a%20solution%20by%20allowing%20users%20to%20guide%20their%20recommendations%0Athrough%20natural%20language%20requests%20%28e.g.%2C%20%22I%20want%20to%20see%20respectful%20posts%20with%20a%0Adifferent%20perspective%20than%20mine%22%29.%20We%20propose%20a%20method%2C%20CTRL-Rec%2C%20that%20allows%0Afor%20natural%20language%20control%20of%20traditional%20recommender%20systems%20in%20real-time%0Awith%20computational%20efficiency.%20Specifically%2C%20at%20training%20time%2C%20we%20use%20an%20LLM%20to%0Asimulate%20whether%20users%20would%20approve%20of%20items%20based%20on%20their%20language%20requests%2C%0Aand%20we%20train%20embedding%20models%20that%20approximate%20such%20simulated%20judgments.%20We%0Athen%20integrate%20these%20user-request-based%20predictions%20into%20the%20standard%20weighting%0Aof%20signals%20that%20traditional%20recommender%20systems%20optimize.%20At%20deployment%20time%2C%0Awe%20require%20only%20a%20single%20LLM%20embedding%20computation%20per%20user%20request%2C%20allowing%0Afor%20real-time%20control%20of%20recommendations.%20In%20experiments%20with%20the%20MovieLens%0Adataset%2C%20our%20method%20consistently%20allows%20for%20fine-grained%20control%20across%20a%0Adiversity%20of%20requests.%20In%20a%20study%20with%2019%20Letterboxd%20users%2C%20we%20find%20that%0ACTRL-Rec%20was%20positively%20received%20by%20users%20and%20significantly%20enhanced%20users%27%0Asense%20of%20control%20and%20satisfaction%20with%20recommendations%20compared%20to%20traditional%0Acontrols.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12742v1&entry.124074799=Read"},
{"title": "Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems", "author": "Jiaxin Gao and Chen Chen and Yanwen Jia and Xueluan Gong and Kwok-Yan Lam and Qian Wang", "abstract": "  Large Language Models (LLMs) are increasingly being used to autonomously\nevaluate the quality of content in communication systems, e.g., to assess\nresponses in telecom customer support chatbots. However, the impartiality of\nthese AI \"judges\" is not guaranteed, and any biases in their evaluation\ncriteria could skew outcomes and undermine user trust. In this paper, we\nsystematically investigate judgment biases in two LLM-as-a-judge models (i.e.,\nGPT-Judge and JudgeLM) under the point-wise scoring setting, encompassing 11\ntypes of biases that cover both implicit and explicit forms. We observed that\nstate-of-the-art LLM judges demonstrate robustness to biased inputs, generally\nassigning them lower scores than the corresponding clean samples. Providing a\ndetailed scoring rubric further enhances this robustness. We further found that\nfine-tuning an LLM on high-scoring yet biased responses can significantly\ndegrade its performance, highlighting the risk of training on biased data. We\nalso discovered that the judged scores correlate with task difficulty: a\nchallenging dataset like GPQA yields lower average scores, whereas an\nopen-ended reasoning dataset (e.g., JudgeLM-val) sees higher average scores.\nFinally, we proposed four potential mitigation strategies to ensure fair and\nreliable AI judging in practical communication scenarios.\n", "link": "http://arxiv.org/abs/2510.12462v1", "date": "2025-10-14", "relevancy": 1.4224, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4868}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4719}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20and%20Mitigating%20LLM-as-a-judge%20Bias%20in%20Communication%20Systems&body=Title%3A%20Evaluating%20and%20Mitigating%20LLM-as-a-judge%20Bias%20in%20Communication%20Systems%0AAuthor%3A%20Jiaxin%20Gao%20and%20Chen%20Chen%20and%20Yanwen%20Jia%20and%20Xueluan%20Gong%20and%20Kwok-Yan%20Lam%20and%20Qian%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20being%20used%20to%20autonomously%0Aevaluate%20the%20quality%20of%20content%20in%20communication%20systems%2C%20e.g.%2C%20to%20assess%0Aresponses%20in%20telecom%20customer%20support%20chatbots.%20However%2C%20the%20impartiality%20of%0Athese%20AI%20%22judges%22%20is%20not%20guaranteed%2C%20and%20any%20biases%20in%20their%20evaluation%0Acriteria%20could%20skew%20outcomes%20and%20undermine%20user%20trust.%20In%20this%20paper%2C%20we%0Asystematically%20investigate%20judgment%20biases%20in%20two%20LLM-as-a-judge%20models%20%28i.e.%2C%0AGPT-Judge%20and%20JudgeLM%29%20under%20the%20point-wise%20scoring%20setting%2C%20encompassing%2011%0Atypes%20of%20biases%20that%20cover%20both%20implicit%20and%20explicit%20forms.%20We%20observed%20that%0Astate-of-the-art%20LLM%20judges%20demonstrate%20robustness%20to%20biased%20inputs%2C%20generally%0Aassigning%20them%20lower%20scores%20than%20the%20corresponding%20clean%20samples.%20Providing%20a%0Adetailed%20scoring%20rubric%20further%20enhances%20this%20robustness.%20We%20further%20found%20that%0Afine-tuning%20an%20LLM%20on%20high-scoring%20yet%20biased%20responses%20can%20significantly%0Adegrade%20its%20performance%2C%20highlighting%20the%20risk%20of%20training%20on%20biased%20data.%20We%0Aalso%20discovered%20that%20the%20judged%20scores%20correlate%20with%20task%20difficulty%3A%20a%0Achallenging%20dataset%20like%20GPQA%20yields%20lower%20average%20scores%2C%20whereas%20an%0Aopen-ended%20reasoning%20dataset%20%28e.g.%2C%20JudgeLM-val%29%20sees%20higher%20average%20scores.%0AFinally%2C%20we%20proposed%20four%20potential%20mitigation%20strategies%20to%20ensure%20fair%20and%0Areliable%20AI%20judging%20in%20practical%20communication%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520and%2520Mitigating%2520LLM-as-a-judge%2520Bias%2520in%2520Communication%2520Systems%26entry.906535625%3DJiaxin%2520Gao%2520and%2520Chen%2520Chen%2520and%2520Yanwen%2520Jia%2520and%2520Xueluan%2520Gong%2520and%2520Kwok-Yan%2520Lam%2520and%2520Qian%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520being%2520used%2520to%2520autonomously%250Aevaluate%2520the%2520quality%2520of%2520content%2520in%2520communication%2520systems%252C%2520e.g.%252C%2520to%2520assess%250Aresponses%2520in%2520telecom%2520customer%2520support%2520chatbots.%2520However%252C%2520the%2520impartiality%2520of%250Athese%2520AI%2520%2522judges%2522%2520is%2520not%2520guaranteed%252C%2520and%2520any%2520biases%2520in%2520their%2520evaluation%250Acriteria%2520could%2520skew%2520outcomes%2520and%2520undermine%2520user%2520trust.%2520In%2520this%2520paper%252C%2520we%250Asystematically%2520investigate%2520judgment%2520biases%2520in%2520two%2520LLM-as-a-judge%2520models%2520%2528i.e.%252C%250AGPT-Judge%2520and%2520JudgeLM%2529%2520under%2520the%2520point-wise%2520scoring%2520setting%252C%2520encompassing%252011%250Atypes%2520of%2520biases%2520that%2520cover%2520both%2520implicit%2520and%2520explicit%2520forms.%2520We%2520observed%2520that%250Astate-of-the-art%2520LLM%2520judges%2520demonstrate%2520robustness%2520to%2520biased%2520inputs%252C%2520generally%250Aassigning%2520them%2520lower%2520scores%2520than%2520the%2520corresponding%2520clean%2520samples.%2520Providing%2520a%250Adetailed%2520scoring%2520rubric%2520further%2520enhances%2520this%2520robustness.%2520We%2520further%2520found%2520that%250Afine-tuning%2520an%2520LLM%2520on%2520high-scoring%2520yet%2520biased%2520responses%2520can%2520significantly%250Adegrade%2520its%2520performance%252C%2520highlighting%2520the%2520risk%2520of%2520training%2520on%2520biased%2520data.%2520We%250Aalso%2520discovered%2520that%2520the%2520judged%2520scores%2520correlate%2520with%2520task%2520difficulty%253A%2520a%250Achallenging%2520dataset%2520like%2520GPQA%2520yields%2520lower%2520average%2520scores%252C%2520whereas%2520an%250Aopen-ended%2520reasoning%2520dataset%2520%2528e.g.%252C%2520JudgeLM-val%2529%2520sees%2520higher%2520average%2520scores.%250AFinally%252C%2520we%2520proposed%2520four%2520potential%2520mitigation%2520strategies%2520to%2520ensure%2520fair%2520and%250Areliable%2520AI%2520judging%2520in%2520practical%2520communication%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20and%20Mitigating%20LLM-as-a-judge%20Bias%20in%20Communication%20Systems&entry.906535625=Jiaxin%20Gao%20and%20Chen%20Chen%20and%20Yanwen%20Jia%20and%20Xueluan%20Gong%20and%20Kwok-Yan%20Lam%20and%20Qian%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20being%20used%20to%20autonomously%0Aevaluate%20the%20quality%20of%20content%20in%20communication%20systems%2C%20e.g.%2C%20to%20assess%0Aresponses%20in%20telecom%20customer%20support%20chatbots.%20However%2C%20the%20impartiality%20of%0Athese%20AI%20%22judges%22%20is%20not%20guaranteed%2C%20and%20any%20biases%20in%20their%20evaluation%0Acriteria%20could%20skew%20outcomes%20and%20undermine%20user%20trust.%20In%20this%20paper%2C%20we%0Asystematically%20investigate%20judgment%20biases%20in%20two%20LLM-as-a-judge%20models%20%28i.e.%2C%0AGPT-Judge%20and%20JudgeLM%29%20under%20the%20point-wise%20scoring%20setting%2C%20encompassing%2011%0Atypes%20of%20biases%20that%20cover%20both%20implicit%20and%20explicit%20forms.%20We%20observed%20that%0Astate-of-the-art%20LLM%20judges%20demonstrate%20robustness%20to%20biased%20inputs%2C%20generally%0Aassigning%20them%20lower%20scores%20than%20the%20corresponding%20clean%20samples.%20Providing%20a%0Adetailed%20scoring%20rubric%20further%20enhances%20this%20robustness.%20We%20further%20found%20that%0Afine-tuning%20an%20LLM%20on%20high-scoring%20yet%20biased%20responses%20can%20significantly%0Adegrade%20its%20performance%2C%20highlighting%20the%20risk%20of%20training%20on%20biased%20data.%20We%0Aalso%20discovered%20that%20the%20judged%20scores%20correlate%20with%20task%20difficulty%3A%20a%0Achallenging%20dataset%20like%20GPQA%20yields%20lower%20average%20scores%2C%20whereas%20an%0Aopen-ended%20reasoning%20dataset%20%28e.g.%2C%20JudgeLM-val%29%20sees%20higher%20average%20scores.%0AFinally%2C%20we%20proposed%20four%20potential%20mitigation%20strategies%20to%20ensure%20fair%20and%0Areliable%20AI%20judging%20in%20practical%20communication%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12462v1&entry.124074799=Read"},
{"title": "PSN Game: Game-theoretic Prediction and Planning via a Player Selection\n  Network", "author": "Tianyu Qiu and Eric Ouano and Fernando Palafox and Christian Ellis and David Fridovich-Keil", "abstract": "  While game-theoretic planning frameworks are effective at modeling\nmulti-agent interactions, they require solving large optimization problems\nwhere the number of variables increases with the number of agents, resulting in\nlong computation times that limit their use in large-scale, real-time systems.\nTo address this issue, we propose 1) PSN Game: a learning-based, game-theoretic\nprediction and planning framework that reduces runtime by learning a Player\nSelection Network (PSN); and 2) a Goal Inference Network (GIN) that makes it\npossible to use the PSN in incomplete information games where agents'\nintentions are unknown. A PSN outputs a player selection mask that\ndistinguishes influential players from less relevant ones, enabling the ego\nplayer to solve a smaller, masked game involving only selected players. By\nreducing the number of players in the game, and therefore reducing the number\nof variables in the corresponding optimization problem, PSN directly lowers\ncomputation time. The PSN Game framework is more flexible than existing player\nselection methods as it 1) relies solely on observations of players' past\ntrajectories, without requiring full state, action, or other game-specific\ninformation; and 2) requires no online parameter tuning. Experiments in both\nsimulated scenarios and human trajectory datasets demonstrate that PSNs\noutperform baseline selection methods in 1) prediction accuracy; and 2)\nplanning safety. PSNs also generalize effectively to real-world scenarios in\nwhich agents' objectives are unknown without fine-tuning. By selecting only the\nmost relevant players for decision-making, PSN Game offers a general mechanism\nfor reducing planning complexity that can be seamlessly integrated into\nexisting multi-agent planning frameworks.\n", "link": "http://arxiv.org/abs/2505.00213v2", "date": "2025-10-14", "relevancy": 1.4147, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4822}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4794}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PSN%20Game%3A%20Game-theoretic%20Prediction%20and%20Planning%20via%20a%20Player%20Selection%0A%20%20Network&body=Title%3A%20PSN%20Game%3A%20Game-theoretic%20Prediction%20and%20Planning%20via%20a%20Player%20Selection%0A%20%20Network%0AAuthor%3A%20Tianyu%20Qiu%20and%20Eric%20Ouano%20and%20Fernando%20Palafox%20and%20Christian%20Ellis%20and%20David%20Fridovich-Keil%0AAbstract%3A%20%20%20While%20game-theoretic%20planning%20frameworks%20are%20effective%20at%20modeling%0Amulti-agent%20interactions%2C%20they%20require%20solving%20large%20optimization%20problems%0Awhere%20the%20number%20of%20variables%20increases%20with%20the%20number%20of%20agents%2C%20resulting%20in%0Along%20computation%20times%20that%20limit%20their%20use%20in%20large-scale%2C%20real-time%20systems.%0ATo%20address%20this%20issue%2C%20we%20propose%201%29%20PSN%20Game%3A%20a%20learning-based%2C%20game-theoretic%0Aprediction%20and%20planning%20framework%20that%20reduces%20runtime%20by%20learning%20a%20Player%0ASelection%20Network%20%28PSN%29%3B%20and%202%29%20a%20Goal%20Inference%20Network%20%28GIN%29%20that%20makes%20it%0Apossible%20to%20use%20the%20PSN%20in%20incomplete%20information%20games%20where%20agents%27%0Aintentions%20are%20unknown.%20A%20PSN%20outputs%20a%20player%20selection%20mask%20that%0Adistinguishes%20influential%20players%20from%20less%20relevant%20ones%2C%20enabling%20the%20ego%0Aplayer%20to%20solve%20a%20smaller%2C%20masked%20game%20involving%20only%20selected%20players.%20By%0Areducing%20the%20number%20of%20players%20in%20the%20game%2C%20and%20therefore%20reducing%20the%20number%0Aof%20variables%20in%20the%20corresponding%20optimization%20problem%2C%20PSN%20directly%20lowers%0Acomputation%20time.%20The%20PSN%20Game%20framework%20is%20more%20flexible%20than%20existing%20player%0Aselection%20methods%20as%20it%201%29%20relies%20solely%20on%20observations%20of%20players%27%20past%0Atrajectories%2C%20without%20requiring%20full%20state%2C%20action%2C%20or%20other%20game-specific%0Ainformation%3B%20and%202%29%20requires%20no%20online%20parameter%20tuning.%20Experiments%20in%20both%0Asimulated%20scenarios%20and%20human%20trajectory%20datasets%20demonstrate%20that%20PSNs%0Aoutperform%20baseline%20selection%20methods%20in%201%29%20prediction%20accuracy%3B%20and%202%29%0Aplanning%20safety.%20PSNs%20also%20generalize%20effectively%20to%20real-world%20scenarios%20in%0Awhich%20agents%27%20objectives%20are%20unknown%20without%20fine-tuning.%20By%20selecting%20only%20the%0Amost%20relevant%20players%20for%20decision-making%2C%20PSN%20Game%20offers%20a%20general%20mechanism%0Afor%20reducing%20planning%20complexity%20that%20can%20be%20seamlessly%20integrated%20into%0Aexisting%20multi-agent%20planning%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00213v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPSN%2520Game%253A%2520Game-theoretic%2520Prediction%2520and%2520Planning%2520via%2520a%2520Player%2520Selection%250A%2520%2520Network%26entry.906535625%3DTianyu%2520Qiu%2520and%2520Eric%2520Ouano%2520and%2520Fernando%2520Palafox%2520and%2520Christian%2520Ellis%2520and%2520David%2520Fridovich-Keil%26entry.1292438233%3D%2520%2520While%2520game-theoretic%2520planning%2520frameworks%2520are%2520effective%2520at%2520modeling%250Amulti-agent%2520interactions%252C%2520they%2520require%2520solving%2520large%2520optimization%2520problems%250Awhere%2520the%2520number%2520of%2520variables%2520increases%2520with%2520the%2520number%2520of%2520agents%252C%2520resulting%2520in%250Along%2520computation%2520times%2520that%2520limit%2520their%2520use%2520in%2520large-scale%252C%2520real-time%2520systems.%250ATo%2520address%2520this%2520issue%252C%2520we%2520propose%25201%2529%2520PSN%2520Game%253A%2520a%2520learning-based%252C%2520game-theoretic%250Aprediction%2520and%2520planning%2520framework%2520that%2520reduces%2520runtime%2520by%2520learning%2520a%2520Player%250ASelection%2520Network%2520%2528PSN%2529%253B%2520and%25202%2529%2520a%2520Goal%2520Inference%2520Network%2520%2528GIN%2529%2520that%2520makes%2520it%250Apossible%2520to%2520use%2520the%2520PSN%2520in%2520incomplete%2520information%2520games%2520where%2520agents%2527%250Aintentions%2520are%2520unknown.%2520A%2520PSN%2520outputs%2520a%2520player%2520selection%2520mask%2520that%250Adistinguishes%2520influential%2520players%2520from%2520less%2520relevant%2520ones%252C%2520enabling%2520the%2520ego%250Aplayer%2520to%2520solve%2520a%2520smaller%252C%2520masked%2520game%2520involving%2520only%2520selected%2520players.%2520By%250Areducing%2520the%2520number%2520of%2520players%2520in%2520the%2520game%252C%2520and%2520therefore%2520reducing%2520the%2520number%250Aof%2520variables%2520in%2520the%2520corresponding%2520optimization%2520problem%252C%2520PSN%2520directly%2520lowers%250Acomputation%2520time.%2520The%2520PSN%2520Game%2520framework%2520is%2520more%2520flexible%2520than%2520existing%2520player%250Aselection%2520methods%2520as%2520it%25201%2529%2520relies%2520solely%2520on%2520observations%2520of%2520players%2527%2520past%250Atrajectories%252C%2520without%2520requiring%2520full%2520state%252C%2520action%252C%2520or%2520other%2520game-specific%250Ainformation%253B%2520and%25202%2529%2520requires%2520no%2520online%2520parameter%2520tuning.%2520Experiments%2520in%2520both%250Asimulated%2520scenarios%2520and%2520human%2520trajectory%2520datasets%2520demonstrate%2520that%2520PSNs%250Aoutperform%2520baseline%2520selection%2520methods%2520in%25201%2529%2520prediction%2520accuracy%253B%2520and%25202%2529%250Aplanning%2520safety.%2520PSNs%2520also%2520generalize%2520effectively%2520to%2520real-world%2520scenarios%2520in%250Awhich%2520agents%2527%2520objectives%2520are%2520unknown%2520without%2520fine-tuning.%2520By%2520selecting%2520only%2520the%250Amost%2520relevant%2520players%2520for%2520decision-making%252C%2520PSN%2520Game%2520offers%2520a%2520general%2520mechanism%250Afor%2520reducing%2520planning%2520complexity%2520that%2520can%2520be%2520seamlessly%2520integrated%2520into%250Aexisting%2520multi-agent%2520planning%2520frameworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00213v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PSN%20Game%3A%20Game-theoretic%20Prediction%20and%20Planning%20via%20a%20Player%20Selection%0A%20%20Network&entry.906535625=Tianyu%20Qiu%20and%20Eric%20Ouano%20and%20Fernando%20Palafox%20and%20Christian%20Ellis%20and%20David%20Fridovich-Keil&entry.1292438233=%20%20While%20game-theoretic%20planning%20frameworks%20are%20effective%20at%20modeling%0Amulti-agent%20interactions%2C%20they%20require%20solving%20large%20optimization%20problems%0Awhere%20the%20number%20of%20variables%20increases%20with%20the%20number%20of%20agents%2C%20resulting%20in%0Along%20computation%20times%20that%20limit%20their%20use%20in%20large-scale%2C%20real-time%20systems.%0ATo%20address%20this%20issue%2C%20we%20propose%201%29%20PSN%20Game%3A%20a%20learning-based%2C%20game-theoretic%0Aprediction%20and%20planning%20framework%20that%20reduces%20runtime%20by%20learning%20a%20Player%0ASelection%20Network%20%28PSN%29%3B%20and%202%29%20a%20Goal%20Inference%20Network%20%28GIN%29%20that%20makes%20it%0Apossible%20to%20use%20the%20PSN%20in%20incomplete%20information%20games%20where%20agents%27%0Aintentions%20are%20unknown.%20A%20PSN%20outputs%20a%20player%20selection%20mask%20that%0Adistinguishes%20influential%20players%20from%20less%20relevant%20ones%2C%20enabling%20the%20ego%0Aplayer%20to%20solve%20a%20smaller%2C%20masked%20game%20involving%20only%20selected%20players.%20By%0Areducing%20the%20number%20of%20players%20in%20the%20game%2C%20and%20therefore%20reducing%20the%20number%0Aof%20variables%20in%20the%20corresponding%20optimization%20problem%2C%20PSN%20directly%20lowers%0Acomputation%20time.%20The%20PSN%20Game%20framework%20is%20more%20flexible%20than%20existing%20player%0Aselection%20methods%20as%20it%201%29%20relies%20solely%20on%20observations%20of%20players%27%20past%0Atrajectories%2C%20without%20requiring%20full%20state%2C%20action%2C%20or%20other%20game-specific%0Ainformation%3B%20and%202%29%20requires%20no%20online%20parameter%20tuning.%20Experiments%20in%20both%0Asimulated%20scenarios%20and%20human%20trajectory%20datasets%20demonstrate%20that%20PSNs%0Aoutperform%20baseline%20selection%20methods%20in%201%29%20prediction%20accuracy%3B%20and%202%29%0Aplanning%20safety.%20PSNs%20also%20generalize%20effectively%20to%20real-world%20scenarios%20in%0Awhich%20agents%27%20objectives%20are%20unknown%20without%20fine-tuning.%20By%20selecting%20only%20the%0Amost%20relevant%20players%20for%20decision-making%2C%20PSN%20Game%20offers%20a%20general%20mechanism%0Afor%20reducing%20planning%20complexity%20that%20can%20be%20seamlessly%20integrated%20into%0Aexisting%20multi-agent%20planning%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00213v2&entry.124074799=Read"},
{"title": "PEAR: Planner-Executor Agent Robustness Benchmark", "author": "Shen Dong and Mingxuan Zhang and Pengfei He and Li Ma and Bhavani Thuraisingham and Hui Liu and Yue Xing", "abstract": "  Large Language Model (LLM)-based Multi-Agent Systems (MAS) have emerged as a\npowerful paradigm for tackling complex, multi-step tasks across diverse\ndomains. However, despite their impressive capabilities, MAS remain susceptible\nto adversarial manipulation. Existing studies typically examine isolated attack\nsurfaces or specific scenarios, leaving a lack of holistic understanding of MAS\nvulnerabilities. To bridge this gap, we introduce PEAR, a benchmark for\nsystematically evaluating both the utility and vulnerability of\nplanner-executor MAS. While compatible with various MAS architectures, our\nbenchmark focuses on the planner-executor structure, which is a practical and\nwidely adopted design. Through extensive experiments, we find that (1) a weak\nplanner degrades overall clean task performance more severely than a weak\nexecutor; (2) while a memory module is essential for the planner, having a\nmemory module for the executor does not impact the clean task performance; (3)\nthere exists a trade-off between task performance and robustness; and (4)\nattacks targeting the planner are particularly effective at misleading the\nsystem. These findings offer actionable insights for enhancing the robustness\nof MAS and lay the groundwork for principled defenses in multi-agent settings.\n", "link": "http://arxiv.org/abs/2510.07505v2", "date": "2025-10-14", "relevancy": 1.3456, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4735}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4522}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PEAR%3A%20Planner-Executor%20Agent%20Robustness%20Benchmark&body=Title%3A%20PEAR%3A%20Planner-Executor%20Agent%20Robustness%20Benchmark%0AAuthor%3A%20Shen%20Dong%20and%20Mingxuan%20Zhang%20and%20Pengfei%20He%20and%20Li%20Ma%20and%20Bhavani%20Thuraisingham%20and%20Hui%20Liu%20and%20Yue%20Xing%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29-based%20Multi-Agent%20Systems%20%28MAS%29%20have%20emerged%20as%20a%0Apowerful%20paradigm%20for%20tackling%20complex%2C%20multi-step%20tasks%20across%20diverse%0Adomains.%20However%2C%20despite%20their%20impressive%20capabilities%2C%20MAS%20remain%20susceptible%0Ato%20adversarial%20manipulation.%20Existing%20studies%20typically%20examine%20isolated%20attack%0Asurfaces%20or%20specific%20scenarios%2C%20leaving%20a%20lack%20of%20holistic%20understanding%20of%20MAS%0Avulnerabilities.%20To%20bridge%20this%20gap%2C%20we%20introduce%20PEAR%2C%20a%20benchmark%20for%0Asystematically%20evaluating%20both%20the%20utility%20and%20vulnerability%20of%0Aplanner-executor%20MAS.%20While%20compatible%20with%20various%20MAS%20architectures%2C%20our%0Abenchmark%20focuses%20on%20the%20planner-executor%20structure%2C%20which%20is%20a%20practical%20and%0Awidely%20adopted%20design.%20Through%20extensive%20experiments%2C%20we%20find%20that%20%281%29%20a%20weak%0Aplanner%20degrades%20overall%20clean%20task%20performance%20more%20severely%20than%20a%20weak%0Aexecutor%3B%20%282%29%20while%20a%20memory%20module%20is%20essential%20for%20the%20planner%2C%20having%20a%0Amemory%20module%20for%20the%20executor%20does%20not%20impact%20the%20clean%20task%20performance%3B%20%283%29%0Athere%20exists%20a%20trade-off%20between%20task%20performance%20and%20robustness%3B%20and%20%284%29%0Aattacks%20targeting%20the%20planner%20are%20particularly%20effective%20at%20misleading%20the%0Asystem.%20These%20findings%20offer%20actionable%20insights%20for%20enhancing%20the%20robustness%0Aof%20MAS%20and%20lay%20the%20groundwork%20for%20principled%20defenses%20in%20multi-agent%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07505v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPEAR%253A%2520Planner-Executor%2520Agent%2520Robustness%2520Benchmark%26entry.906535625%3DShen%2520Dong%2520and%2520Mingxuan%2520Zhang%2520and%2520Pengfei%2520He%2520and%2520Li%2520Ma%2520and%2520Bhavani%2520Thuraisingham%2520and%2520Hui%2520Liu%2520and%2520Yue%2520Xing%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529-based%2520Multi-Agent%2520Systems%2520%2528MAS%2529%2520have%2520emerged%2520as%2520a%250Apowerful%2520paradigm%2520for%2520tackling%2520complex%252C%2520multi-step%2520tasks%2520across%2520diverse%250Adomains.%2520However%252C%2520despite%2520their%2520impressive%2520capabilities%252C%2520MAS%2520remain%2520susceptible%250Ato%2520adversarial%2520manipulation.%2520Existing%2520studies%2520typically%2520examine%2520isolated%2520attack%250Asurfaces%2520or%2520specific%2520scenarios%252C%2520leaving%2520a%2520lack%2520of%2520holistic%2520understanding%2520of%2520MAS%250Avulnerabilities.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520PEAR%252C%2520a%2520benchmark%2520for%250Asystematically%2520evaluating%2520both%2520the%2520utility%2520and%2520vulnerability%2520of%250Aplanner-executor%2520MAS.%2520While%2520compatible%2520with%2520various%2520MAS%2520architectures%252C%2520our%250Abenchmark%2520focuses%2520on%2520the%2520planner-executor%2520structure%252C%2520which%2520is%2520a%2520practical%2520and%250Awidely%2520adopted%2520design.%2520Through%2520extensive%2520experiments%252C%2520we%2520find%2520that%2520%25281%2529%2520a%2520weak%250Aplanner%2520degrades%2520overall%2520clean%2520task%2520performance%2520more%2520severely%2520than%2520a%2520weak%250Aexecutor%253B%2520%25282%2529%2520while%2520a%2520memory%2520module%2520is%2520essential%2520for%2520the%2520planner%252C%2520having%2520a%250Amemory%2520module%2520for%2520the%2520executor%2520does%2520not%2520impact%2520the%2520clean%2520task%2520performance%253B%2520%25283%2529%250Athere%2520exists%2520a%2520trade-off%2520between%2520task%2520performance%2520and%2520robustness%253B%2520and%2520%25284%2529%250Aattacks%2520targeting%2520the%2520planner%2520are%2520particularly%2520effective%2520at%2520misleading%2520the%250Asystem.%2520These%2520findings%2520offer%2520actionable%2520insights%2520for%2520enhancing%2520the%2520robustness%250Aof%2520MAS%2520and%2520lay%2520the%2520groundwork%2520for%2520principled%2520defenses%2520in%2520multi-agent%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07505v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PEAR%3A%20Planner-Executor%20Agent%20Robustness%20Benchmark&entry.906535625=Shen%20Dong%20and%20Mingxuan%20Zhang%20and%20Pengfei%20He%20and%20Li%20Ma%20and%20Bhavani%20Thuraisingham%20and%20Hui%20Liu%20and%20Yue%20Xing&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29-based%20Multi-Agent%20Systems%20%28MAS%29%20have%20emerged%20as%20a%0Apowerful%20paradigm%20for%20tackling%20complex%2C%20multi-step%20tasks%20across%20diverse%0Adomains.%20However%2C%20despite%20their%20impressive%20capabilities%2C%20MAS%20remain%20susceptible%0Ato%20adversarial%20manipulation.%20Existing%20studies%20typically%20examine%20isolated%20attack%0Asurfaces%20or%20specific%20scenarios%2C%20leaving%20a%20lack%20of%20holistic%20understanding%20of%20MAS%0Avulnerabilities.%20To%20bridge%20this%20gap%2C%20we%20introduce%20PEAR%2C%20a%20benchmark%20for%0Asystematically%20evaluating%20both%20the%20utility%20and%20vulnerability%20of%0Aplanner-executor%20MAS.%20While%20compatible%20with%20various%20MAS%20architectures%2C%20our%0Abenchmark%20focuses%20on%20the%20planner-executor%20structure%2C%20which%20is%20a%20practical%20and%0Awidely%20adopted%20design.%20Through%20extensive%20experiments%2C%20we%20find%20that%20%281%29%20a%20weak%0Aplanner%20degrades%20overall%20clean%20task%20performance%20more%20severely%20than%20a%20weak%0Aexecutor%3B%20%282%29%20while%20a%20memory%20module%20is%20essential%20for%20the%20planner%2C%20having%20a%0Amemory%20module%20for%20the%20executor%20does%20not%20impact%20the%20clean%20task%20performance%3B%20%283%29%0Athere%20exists%20a%20trade-off%20between%20task%20performance%20and%20robustness%3B%20and%20%284%29%0Aattacks%20targeting%20the%20planner%20are%20particularly%20effective%20at%20misleading%20the%0Asystem.%20These%20findings%20offer%20actionable%20insights%20for%20enhancing%20the%20robustness%0Aof%20MAS%20and%20lay%20the%20groundwork%20for%20principled%20defenses%20in%20multi-agent%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07505v2&entry.124074799=Read"},
{"title": "Autonomous Legged Mobile Manipulation for Lunar Surface Operations via\n  Constrained Reinforcement Learning", "author": "Alvaro Belmonte-Baeza and Miguel Cazorla and Gabriel J. Garc\u00eda and Carlos J. P\u00e9rez-Del-Pulgar and Jorge Pomares", "abstract": "  Robotics plays a pivotal role in planetary science and exploration, where\nautonomous and reliable systems are crucial due to the risks and challenges\ninherent to space environments. The establishment of permanent lunar bases\ndemands robotic platforms capable of navigating and manipulating in the harsh\nlunar terrain. While wheeled rovers have been the mainstay for planetary\nexploration, their limitations in unstructured and steep terrains motivate the\nadoption of legged robots, which offer superior mobility and adaptability. This\npaper introduces a constrained reinforcement learning framework designed for\nautonomous quadrupedal mobile manipulators operating in lunar environments. The\nproposed framework integrates whole-body locomotion and manipulation\ncapabilities while explicitly addressing critical safety constraints, including\ncollision avoidance, dynamic stability, and power efficiency, in order to\nensure robust performance under lunar-specific conditions, such as reduced\ngravity and irregular terrain. Experimental results demonstrate the framework's\neffectiveness in achieving precise 6D task-space end-effector pose tracking,\nachieving an average positional accuracy of 4 cm and orientation accuracy of\n8.1 degrees. The system consistently respects both soft and hard constraints,\nexhibiting adaptive behaviors optimized for lunar gravity conditions. This work\neffectively bridges adaptive learning with essential mission-critical safety\nrequirements, paving the way for advanced autonomous robotic explorers for\nfuture lunar missions.\n", "link": "http://arxiv.org/abs/2510.12684v1", "date": "2025-10-14", "relevancy": 1.6791, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5777}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5649}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Legged%20Mobile%20Manipulation%20for%20Lunar%20Surface%20Operations%20via%0A%20%20Constrained%20Reinforcement%20Learning&body=Title%3A%20Autonomous%20Legged%20Mobile%20Manipulation%20for%20Lunar%20Surface%20Operations%20via%0A%20%20Constrained%20Reinforcement%20Learning%0AAuthor%3A%20Alvaro%20Belmonte-Baeza%20and%20Miguel%20Cazorla%20and%20Gabriel%20J.%20Garc%C3%ADa%20and%20Carlos%20J.%20P%C3%A9rez-Del-Pulgar%20and%20Jorge%20Pomares%0AAbstract%3A%20%20%20Robotics%20plays%20a%20pivotal%20role%20in%20planetary%20science%20and%20exploration%2C%20where%0Aautonomous%20and%20reliable%20systems%20are%20crucial%20due%20to%20the%20risks%20and%20challenges%0Ainherent%20to%20space%20environments.%20The%20establishment%20of%20permanent%20lunar%20bases%0Ademands%20robotic%20platforms%20capable%20of%20navigating%20and%20manipulating%20in%20the%20harsh%0Alunar%20terrain.%20While%20wheeled%20rovers%20have%20been%20the%20mainstay%20for%20planetary%0Aexploration%2C%20their%20limitations%20in%20unstructured%20and%20steep%20terrains%20motivate%20the%0Aadoption%20of%20legged%20robots%2C%20which%20offer%20superior%20mobility%20and%20adaptability.%20This%0Apaper%20introduces%20a%20constrained%20reinforcement%20learning%20framework%20designed%20for%0Aautonomous%20quadrupedal%20mobile%20manipulators%20operating%20in%20lunar%20environments.%20The%0Aproposed%20framework%20integrates%20whole-body%20locomotion%20and%20manipulation%0Acapabilities%20while%20explicitly%20addressing%20critical%20safety%20constraints%2C%20including%0Acollision%20avoidance%2C%20dynamic%20stability%2C%20and%20power%20efficiency%2C%20in%20order%20to%0Aensure%20robust%20performance%20under%20lunar-specific%20conditions%2C%20such%20as%20reduced%0Agravity%20and%20irregular%20terrain.%20Experimental%20results%20demonstrate%20the%20framework%27s%0Aeffectiveness%20in%20achieving%20precise%206D%20task-space%20end-effector%20pose%20tracking%2C%0Aachieving%20an%20average%20positional%20accuracy%20of%204%20cm%20and%20orientation%20accuracy%20of%0A8.1%20degrees.%20The%20system%20consistently%20respects%20both%20soft%20and%20hard%20constraints%2C%0Aexhibiting%20adaptive%20behaviors%20optimized%20for%20lunar%20gravity%20conditions.%20This%20work%0Aeffectively%20bridges%20adaptive%20learning%20with%20essential%20mission-critical%20safety%0Arequirements%2C%20paving%20the%20way%20for%20advanced%20autonomous%20robotic%20explorers%20for%0Afuture%20lunar%20missions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Legged%2520Mobile%2520Manipulation%2520for%2520Lunar%2520Surface%2520Operations%2520via%250A%2520%2520Constrained%2520Reinforcement%2520Learning%26entry.906535625%3DAlvaro%2520Belmonte-Baeza%2520and%2520Miguel%2520Cazorla%2520and%2520Gabriel%2520J.%2520Garc%25C3%25ADa%2520and%2520Carlos%2520J.%2520P%25C3%25A9rez-Del-Pulgar%2520and%2520Jorge%2520Pomares%26entry.1292438233%3D%2520%2520Robotics%2520plays%2520a%2520pivotal%2520role%2520in%2520planetary%2520science%2520and%2520exploration%252C%2520where%250Aautonomous%2520and%2520reliable%2520systems%2520are%2520crucial%2520due%2520to%2520the%2520risks%2520and%2520challenges%250Ainherent%2520to%2520space%2520environments.%2520The%2520establishment%2520of%2520permanent%2520lunar%2520bases%250Ademands%2520robotic%2520platforms%2520capable%2520of%2520navigating%2520and%2520manipulating%2520in%2520the%2520harsh%250Alunar%2520terrain.%2520While%2520wheeled%2520rovers%2520have%2520been%2520the%2520mainstay%2520for%2520planetary%250Aexploration%252C%2520their%2520limitations%2520in%2520unstructured%2520and%2520steep%2520terrains%2520motivate%2520the%250Aadoption%2520of%2520legged%2520robots%252C%2520which%2520offer%2520superior%2520mobility%2520and%2520adaptability.%2520This%250Apaper%2520introduces%2520a%2520constrained%2520reinforcement%2520learning%2520framework%2520designed%2520for%250Aautonomous%2520quadrupedal%2520mobile%2520manipulators%2520operating%2520in%2520lunar%2520environments.%2520The%250Aproposed%2520framework%2520integrates%2520whole-body%2520locomotion%2520and%2520manipulation%250Acapabilities%2520while%2520explicitly%2520addressing%2520critical%2520safety%2520constraints%252C%2520including%250Acollision%2520avoidance%252C%2520dynamic%2520stability%252C%2520and%2520power%2520efficiency%252C%2520in%2520order%2520to%250Aensure%2520robust%2520performance%2520under%2520lunar-specific%2520conditions%252C%2520such%2520as%2520reduced%250Agravity%2520and%2520irregular%2520terrain.%2520Experimental%2520results%2520demonstrate%2520the%2520framework%2527s%250Aeffectiveness%2520in%2520achieving%2520precise%25206D%2520task-space%2520end-effector%2520pose%2520tracking%252C%250Aachieving%2520an%2520average%2520positional%2520accuracy%2520of%25204%2520cm%2520and%2520orientation%2520accuracy%2520of%250A8.1%2520degrees.%2520The%2520system%2520consistently%2520respects%2520both%2520soft%2520and%2520hard%2520constraints%252C%250Aexhibiting%2520adaptive%2520behaviors%2520optimized%2520for%2520lunar%2520gravity%2520conditions.%2520This%2520work%250Aeffectively%2520bridges%2520adaptive%2520learning%2520with%2520essential%2520mission-critical%2520safety%250Arequirements%252C%2520paving%2520the%2520way%2520for%2520advanced%2520autonomous%2520robotic%2520explorers%2520for%250Afuture%2520lunar%2520missions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Legged%20Mobile%20Manipulation%20for%20Lunar%20Surface%20Operations%20via%0A%20%20Constrained%20Reinforcement%20Learning&entry.906535625=Alvaro%20Belmonte-Baeza%20and%20Miguel%20Cazorla%20and%20Gabriel%20J.%20Garc%C3%ADa%20and%20Carlos%20J.%20P%C3%A9rez-Del-Pulgar%20and%20Jorge%20Pomares&entry.1292438233=%20%20Robotics%20plays%20a%20pivotal%20role%20in%20planetary%20science%20and%20exploration%2C%20where%0Aautonomous%20and%20reliable%20systems%20are%20crucial%20due%20to%20the%20risks%20and%20challenges%0Ainherent%20to%20space%20environments.%20The%20establishment%20of%20permanent%20lunar%20bases%0Ademands%20robotic%20platforms%20capable%20of%20navigating%20and%20manipulating%20in%20the%20harsh%0Alunar%20terrain.%20While%20wheeled%20rovers%20have%20been%20the%20mainstay%20for%20planetary%0Aexploration%2C%20their%20limitations%20in%20unstructured%20and%20steep%20terrains%20motivate%20the%0Aadoption%20of%20legged%20robots%2C%20which%20offer%20superior%20mobility%20and%20adaptability.%20This%0Apaper%20introduces%20a%20constrained%20reinforcement%20learning%20framework%20designed%20for%0Aautonomous%20quadrupedal%20mobile%20manipulators%20operating%20in%20lunar%20environments.%20The%0Aproposed%20framework%20integrates%20whole-body%20locomotion%20and%20manipulation%0Acapabilities%20while%20explicitly%20addressing%20critical%20safety%20constraints%2C%20including%0Acollision%20avoidance%2C%20dynamic%20stability%2C%20and%20power%20efficiency%2C%20in%20order%20to%0Aensure%20robust%20performance%20under%20lunar-specific%20conditions%2C%20such%20as%20reduced%0Agravity%20and%20irregular%20terrain.%20Experimental%20results%20demonstrate%20the%20framework%27s%0Aeffectiveness%20in%20achieving%20precise%206D%20task-space%20end-effector%20pose%20tracking%2C%0Aachieving%20an%20average%20positional%20accuracy%20of%204%20cm%20and%20orientation%20accuracy%20of%0A8.1%20degrees.%20The%20system%20consistently%20respects%20both%20soft%20and%20hard%20constraints%2C%0Aexhibiting%20adaptive%20behaviors%20optimized%20for%20lunar%20gravity%20conditions.%20This%20work%0Aeffectively%20bridges%20adaptive%20learning%20with%20essential%20mission-critical%20safety%0Arequirements%2C%20paving%20the%20way%20for%20advanced%20autonomous%20robotic%20explorers%20for%0Afuture%20lunar%20missions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12684v1&entry.124074799=Read"},
{"title": "Large language models management of medications: three performance\n  analyses", "author": "Kelli Henry and Steven Xu and Kaitlin Blotske and Moriah Cargile and Erin F. Barreto and Brian Murray and Susan Smith and Seth R. Bauer and Xingmeng Zhao and Adeleine Tilley and Yanjun Gao and Tianming Liu and Sunghwan Sohn and Andrea Sikora", "abstract": "  Purpose: Large language models (LLMs) have proven performance for certain\ndiagnostic tasks, however limited studies have evaluated their consistency in\nrecommending appropriate medication regimens for a given diagnosis. Medication\nmanagement is a complex task that requires synthesis of drug formulation and\ncomplete order instructions for safe use. Here, the performance of GPT 4o, an\nLLM available with ChatGPT, was tested for three medication management tasks.\nMethods: GPT-4o performance was tested using three medication tasks:\nidentifying available formulations for a given generic drug name, identifying\ndrug-drug interactions (DDI) for a given medication regimen, and preparing a\nmedication order for a given generic drug name. For each experiment, the models\nraw text response was captured exactly as returned and evaluated using\nclinician evaluation in addition to standard LLM metrics, including Term\nFrequency-Inverse Document Frequency (TF IDF) vectors, normalized Levenshtein\nsimilarity, and Recall-Oriented Understudy for Gisting Evaluation (ROUGE\n1/ROUGE L F1) between each response and its reference string. Results: For the\nfirst task of drug-formulation matching, GPT-4o had 49% accuracy for generic\nmedications being matched to all available formulations, with an average of\n1.23 omissions per medication and 1.14 hallucinations per medication. For the\nsecond task of drug-drug interaction identification, the accuracy was 54.7% for\nidentifying the DDI pair. For the third task, GPT-4o generated order sentences\ncontaining no medication or abbreviation errors in 65.8% of cases. Conclusions:\nModel performance for basic medication tasks was consistently poor. This\nevaluation highlights the need for domain-specific training through\nclinician-annotated datasets and a comprehensive evaluation framework for\nbenchmarking performance.\n", "link": "http://arxiv.org/abs/2509.22926v2", "date": "2025-10-14", "relevancy": 1.8846, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4863}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4681}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20language%20models%20management%20of%20medications%3A%20three%20performance%0A%20%20analyses&body=Title%3A%20Large%20language%20models%20management%20of%20medications%3A%20three%20performance%0A%20%20analyses%0AAuthor%3A%20Kelli%20Henry%20and%20Steven%20Xu%20and%20Kaitlin%20Blotske%20and%20Moriah%20Cargile%20and%20Erin%20F.%20Barreto%20and%20Brian%20Murray%20and%20Susan%20Smith%20and%20Seth%20R.%20Bauer%20and%20Xingmeng%20Zhao%20and%20Adeleine%20Tilley%20and%20Yanjun%20Gao%20and%20Tianming%20Liu%20and%20Sunghwan%20Sohn%20and%20Andrea%20Sikora%0AAbstract%3A%20%20%20Purpose%3A%20Large%20language%20models%20%28LLMs%29%20have%20proven%20performance%20for%20certain%0Adiagnostic%20tasks%2C%20however%20limited%20studies%20have%20evaluated%20their%20consistency%20in%0Arecommending%20appropriate%20medication%20regimens%20for%20a%20given%20diagnosis.%20Medication%0Amanagement%20is%20a%20complex%20task%20that%20requires%20synthesis%20of%20drug%20formulation%20and%0Acomplete%20order%20instructions%20for%20safe%20use.%20Here%2C%20the%20performance%20of%20GPT%204o%2C%20an%0ALLM%20available%20with%20ChatGPT%2C%20was%20tested%20for%20three%20medication%20management%20tasks.%0AMethods%3A%20GPT-4o%20performance%20was%20tested%20using%20three%20medication%20tasks%3A%0Aidentifying%20available%20formulations%20for%20a%20given%20generic%20drug%20name%2C%20identifying%0Adrug-drug%20interactions%20%28DDI%29%20for%20a%20given%20medication%20regimen%2C%20and%20preparing%20a%0Amedication%20order%20for%20a%20given%20generic%20drug%20name.%20For%20each%20experiment%2C%20the%20models%0Araw%20text%20response%20was%20captured%20exactly%20as%20returned%20and%20evaluated%20using%0Aclinician%20evaluation%20in%20addition%20to%20standard%20LLM%20metrics%2C%20including%20Term%0AFrequency-Inverse%20Document%20Frequency%20%28TF%20IDF%29%20vectors%2C%20normalized%20Levenshtein%0Asimilarity%2C%20and%20Recall-Oriented%20Understudy%20for%20Gisting%20Evaluation%20%28ROUGE%0A1/ROUGE%20L%20F1%29%20between%20each%20response%20and%20its%20reference%20string.%20Results%3A%20For%20the%0Afirst%20task%20of%20drug-formulation%20matching%2C%20GPT-4o%20had%2049%25%20accuracy%20for%20generic%0Amedications%20being%20matched%20to%20all%20available%20formulations%2C%20with%20an%20average%20of%0A1.23%20omissions%20per%20medication%20and%201.14%20hallucinations%20per%20medication.%20For%20the%0Asecond%20task%20of%20drug-drug%20interaction%20identification%2C%20the%20accuracy%20was%2054.7%25%20for%0Aidentifying%20the%20DDI%20pair.%20For%20the%20third%20task%2C%20GPT-4o%20generated%20order%20sentences%0Acontaining%20no%20medication%20or%20abbreviation%20errors%20in%2065.8%25%20of%20cases.%20Conclusions%3A%0AModel%20performance%20for%20basic%20medication%20tasks%20was%20consistently%20poor.%20This%0Aevaluation%20highlights%20the%20need%20for%20domain-specific%20training%20through%0Aclinician-annotated%20datasets%20and%20a%20comprehensive%20evaluation%20framework%20for%0Abenchmarking%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22926v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520language%2520models%2520management%2520of%2520medications%253A%2520three%2520performance%250A%2520%2520analyses%26entry.906535625%3DKelli%2520Henry%2520and%2520Steven%2520Xu%2520and%2520Kaitlin%2520Blotske%2520and%2520Moriah%2520Cargile%2520and%2520Erin%2520F.%2520Barreto%2520and%2520Brian%2520Murray%2520and%2520Susan%2520Smith%2520and%2520Seth%2520R.%2520Bauer%2520and%2520Xingmeng%2520Zhao%2520and%2520Adeleine%2520Tilley%2520and%2520Yanjun%2520Gao%2520and%2520Tianming%2520Liu%2520and%2520Sunghwan%2520Sohn%2520and%2520Andrea%2520Sikora%26entry.1292438233%3D%2520%2520Purpose%253A%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520proven%2520performance%2520for%2520certain%250Adiagnostic%2520tasks%252C%2520however%2520limited%2520studies%2520have%2520evaluated%2520their%2520consistency%2520in%250Arecommending%2520appropriate%2520medication%2520regimens%2520for%2520a%2520given%2520diagnosis.%2520Medication%250Amanagement%2520is%2520a%2520complex%2520task%2520that%2520requires%2520synthesis%2520of%2520drug%2520formulation%2520and%250Acomplete%2520order%2520instructions%2520for%2520safe%2520use.%2520Here%252C%2520the%2520performance%2520of%2520GPT%25204o%252C%2520an%250ALLM%2520available%2520with%2520ChatGPT%252C%2520was%2520tested%2520for%2520three%2520medication%2520management%2520tasks.%250AMethods%253A%2520GPT-4o%2520performance%2520was%2520tested%2520using%2520three%2520medication%2520tasks%253A%250Aidentifying%2520available%2520formulations%2520for%2520a%2520given%2520generic%2520drug%2520name%252C%2520identifying%250Adrug-drug%2520interactions%2520%2528DDI%2529%2520for%2520a%2520given%2520medication%2520regimen%252C%2520and%2520preparing%2520a%250Amedication%2520order%2520for%2520a%2520given%2520generic%2520drug%2520name.%2520For%2520each%2520experiment%252C%2520the%2520models%250Araw%2520text%2520response%2520was%2520captured%2520exactly%2520as%2520returned%2520and%2520evaluated%2520using%250Aclinician%2520evaluation%2520in%2520addition%2520to%2520standard%2520LLM%2520metrics%252C%2520including%2520Term%250AFrequency-Inverse%2520Document%2520Frequency%2520%2528TF%2520IDF%2529%2520vectors%252C%2520normalized%2520Levenshtein%250Asimilarity%252C%2520and%2520Recall-Oriented%2520Understudy%2520for%2520Gisting%2520Evaluation%2520%2528ROUGE%250A1/ROUGE%2520L%2520F1%2529%2520between%2520each%2520response%2520and%2520its%2520reference%2520string.%2520Results%253A%2520For%2520the%250Afirst%2520task%2520of%2520drug-formulation%2520matching%252C%2520GPT-4o%2520had%252049%2525%2520accuracy%2520for%2520generic%250Amedications%2520being%2520matched%2520to%2520all%2520available%2520formulations%252C%2520with%2520an%2520average%2520of%250A1.23%2520omissions%2520per%2520medication%2520and%25201.14%2520hallucinations%2520per%2520medication.%2520For%2520the%250Asecond%2520task%2520of%2520drug-drug%2520interaction%2520identification%252C%2520the%2520accuracy%2520was%252054.7%2525%2520for%250Aidentifying%2520the%2520DDI%2520pair.%2520For%2520the%2520third%2520task%252C%2520GPT-4o%2520generated%2520order%2520sentences%250Acontaining%2520no%2520medication%2520or%2520abbreviation%2520errors%2520in%252065.8%2525%2520of%2520cases.%2520Conclusions%253A%250AModel%2520performance%2520for%2520basic%2520medication%2520tasks%2520was%2520consistently%2520poor.%2520This%250Aevaluation%2520highlights%2520the%2520need%2520for%2520domain-specific%2520training%2520through%250Aclinician-annotated%2520datasets%2520and%2520a%2520comprehensive%2520evaluation%2520framework%2520for%250Abenchmarking%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22926v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20language%20models%20management%20of%20medications%3A%20three%20performance%0A%20%20analyses&entry.906535625=Kelli%20Henry%20and%20Steven%20Xu%20and%20Kaitlin%20Blotske%20and%20Moriah%20Cargile%20and%20Erin%20F.%20Barreto%20and%20Brian%20Murray%20and%20Susan%20Smith%20and%20Seth%20R.%20Bauer%20and%20Xingmeng%20Zhao%20and%20Adeleine%20Tilley%20and%20Yanjun%20Gao%20and%20Tianming%20Liu%20and%20Sunghwan%20Sohn%20and%20Andrea%20Sikora&entry.1292438233=%20%20Purpose%3A%20Large%20language%20models%20%28LLMs%29%20have%20proven%20performance%20for%20certain%0Adiagnostic%20tasks%2C%20however%20limited%20studies%20have%20evaluated%20their%20consistency%20in%0Arecommending%20appropriate%20medication%20regimens%20for%20a%20given%20diagnosis.%20Medication%0Amanagement%20is%20a%20complex%20task%20that%20requires%20synthesis%20of%20drug%20formulation%20and%0Acomplete%20order%20instructions%20for%20safe%20use.%20Here%2C%20the%20performance%20of%20GPT%204o%2C%20an%0ALLM%20available%20with%20ChatGPT%2C%20was%20tested%20for%20three%20medication%20management%20tasks.%0AMethods%3A%20GPT-4o%20performance%20was%20tested%20using%20three%20medication%20tasks%3A%0Aidentifying%20available%20formulations%20for%20a%20given%20generic%20drug%20name%2C%20identifying%0Adrug-drug%20interactions%20%28DDI%29%20for%20a%20given%20medication%20regimen%2C%20and%20preparing%20a%0Amedication%20order%20for%20a%20given%20generic%20drug%20name.%20For%20each%20experiment%2C%20the%20models%0Araw%20text%20response%20was%20captured%20exactly%20as%20returned%20and%20evaluated%20using%0Aclinician%20evaluation%20in%20addition%20to%20standard%20LLM%20metrics%2C%20including%20Term%0AFrequency-Inverse%20Document%20Frequency%20%28TF%20IDF%29%20vectors%2C%20normalized%20Levenshtein%0Asimilarity%2C%20and%20Recall-Oriented%20Understudy%20for%20Gisting%20Evaluation%20%28ROUGE%0A1/ROUGE%20L%20F1%29%20between%20each%20response%20and%20its%20reference%20string.%20Results%3A%20For%20the%0Afirst%20task%20of%20drug-formulation%20matching%2C%20GPT-4o%20had%2049%25%20accuracy%20for%20generic%0Amedications%20being%20matched%20to%20all%20available%20formulations%2C%20with%20an%20average%20of%0A1.23%20omissions%20per%20medication%20and%201.14%20hallucinations%20per%20medication.%20For%20the%0Asecond%20task%20of%20drug-drug%20interaction%20identification%2C%20the%20accuracy%20was%2054.7%25%20for%0Aidentifying%20the%20DDI%20pair.%20For%20the%20third%20task%2C%20GPT-4o%20generated%20order%20sentences%0Acontaining%20no%20medication%20or%20abbreviation%20errors%20in%2065.8%25%20of%20cases.%20Conclusions%3A%0AModel%20performance%20for%20basic%20medication%20tasks%20was%20consistently%20poor.%20This%0Aevaluation%20highlights%20the%20need%20for%20domain-specific%20training%20through%0Aclinician-annotated%20datasets%20and%20a%20comprehensive%20evaluation%20framework%20for%0Abenchmarking%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22926v2&entry.124074799=Read"},
{"title": "Keep Calm and Avoid Harmful Content: Concept Alignment and Latent\n  Manipulation Towards Safer Answers", "author": "Ruben Belo and Claudia Soares and Marta Guimaraes", "abstract": "  Large Language Models are susceptible to jailbreak attacks that bypass\nbuilt-in safety guardrails (e.g., by tricking the model with adversarial\nprompts). We propose Concept Alignment and Concept Manipulation \\textbf{CALM},\nan inference-time method that suppresses harmful concepts by modifying latent\nrepresentations of the last layer of the model, without retraining. Leveraging\n\\gls*{cw} technique from Computer Vision combined with orthogonal projection,\nCALM removes unwanted latent directions associated with harmful content while\npreserving model performance. Experiments show that CALM reduces harmful\noutputs and outperforms baseline methods in most metrics, offering a\nlightweight approach to AI safety with no additional training data or model\nfine-tuning, while incurring only a small computational overhead at inference.\n", "link": "http://arxiv.org/abs/2510.12672v1", "date": "2025-10-14", "relevancy": 1.4172, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.48}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4721}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Keep%20Calm%20and%20Avoid%20Harmful%20Content%3A%20Concept%20Alignment%20and%20Latent%0A%20%20Manipulation%20Towards%20Safer%20Answers&body=Title%3A%20Keep%20Calm%20and%20Avoid%20Harmful%20Content%3A%20Concept%20Alignment%20and%20Latent%0A%20%20Manipulation%20Towards%20Safer%20Answers%0AAuthor%3A%20Ruben%20Belo%20and%20Claudia%20Soares%20and%20Marta%20Guimaraes%0AAbstract%3A%20%20%20Large%20Language%20Models%20are%20susceptible%20to%20jailbreak%20attacks%20that%20bypass%0Abuilt-in%20safety%20guardrails%20%28e.g.%2C%20by%20tricking%20the%20model%20with%20adversarial%0Aprompts%29.%20We%20propose%20Concept%20Alignment%20and%20Concept%20Manipulation%20%5Ctextbf%7BCALM%7D%2C%0Aan%20inference-time%20method%20that%20suppresses%20harmful%20concepts%20by%20modifying%20latent%0Arepresentations%20of%20the%20last%20layer%20of%20the%20model%2C%20without%20retraining.%20Leveraging%0A%5Cgls%2A%7Bcw%7D%20technique%20from%20Computer%20Vision%20combined%20with%20orthogonal%20projection%2C%0ACALM%20removes%20unwanted%20latent%20directions%20associated%20with%20harmful%20content%20while%0Apreserving%20model%20performance.%20Experiments%20show%20that%20CALM%20reduces%20harmful%0Aoutputs%20and%20outperforms%20baseline%20methods%20in%20most%20metrics%2C%20offering%20a%0Alightweight%20approach%20to%20AI%20safety%20with%20no%20additional%20training%20data%20or%20model%0Afine-tuning%2C%20while%20incurring%20only%20a%20small%20computational%20overhead%20at%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKeep%2520Calm%2520and%2520Avoid%2520Harmful%2520Content%253A%2520Concept%2520Alignment%2520and%2520Latent%250A%2520%2520Manipulation%2520Towards%2520Safer%2520Answers%26entry.906535625%3DRuben%2520Belo%2520and%2520Claudia%2520Soares%2520and%2520Marta%2520Guimaraes%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520are%2520susceptible%2520to%2520jailbreak%2520attacks%2520that%2520bypass%250Abuilt-in%2520safety%2520guardrails%2520%2528e.g.%252C%2520by%2520tricking%2520the%2520model%2520with%2520adversarial%250Aprompts%2529.%2520We%2520propose%2520Concept%2520Alignment%2520and%2520Concept%2520Manipulation%2520%255Ctextbf%257BCALM%257D%252C%250Aan%2520inference-time%2520method%2520that%2520suppresses%2520harmful%2520concepts%2520by%2520modifying%2520latent%250Arepresentations%2520of%2520the%2520last%2520layer%2520of%2520the%2520model%252C%2520without%2520retraining.%2520Leveraging%250A%255Cgls%252A%257Bcw%257D%2520technique%2520from%2520Computer%2520Vision%2520combined%2520with%2520orthogonal%2520projection%252C%250ACALM%2520removes%2520unwanted%2520latent%2520directions%2520associated%2520with%2520harmful%2520content%2520while%250Apreserving%2520model%2520performance.%2520Experiments%2520show%2520that%2520CALM%2520reduces%2520harmful%250Aoutputs%2520and%2520outperforms%2520baseline%2520methods%2520in%2520most%2520metrics%252C%2520offering%2520a%250Alightweight%2520approach%2520to%2520AI%2520safety%2520with%2520no%2520additional%2520training%2520data%2520or%2520model%250Afine-tuning%252C%2520while%2520incurring%2520only%2520a%2520small%2520computational%2520overhead%2520at%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Keep%20Calm%20and%20Avoid%20Harmful%20Content%3A%20Concept%20Alignment%20and%20Latent%0A%20%20Manipulation%20Towards%20Safer%20Answers&entry.906535625=Ruben%20Belo%20and%20Claudia%20Soares%20and%20Marta%20Guimaraes&entry.1292438233=%20%20Large%20Language%20Models%20are%20susceptible%20to%20jailbreak%20attacks%20that%20bypass%0Abuilt-in%20safety%20guardrails%20%28e.g.%2C%20by%20tricking%20the%20model%20with%20adversarial%0Aprompts%29.%20We%20propose%20Concept%20Alignment%20and%20Concept%20Manipulation%20%5Ctextbf%7BCALM%7D%2C%0Aan%20inference-time%20method%20that%20suppresses%20harmful%20concepts%20by%20modifying%20latent%0Arepresentations%20of%20the%20last%20layer%20of%20the%20model%2C%20without%20retraining.%20Leveraging%0A%5Cgls%2A%7Bcw%7D%20technique%20from%20Computer%20Vision%20combined%20with%20orthogonal%20projection%2C%0ACALM%20removes%20unwanted%20latent%20directions%20associated%20with%20harmful%20content%20while%0Apreserving%20model%20performance.%20Experiments%20show%20that%20CALM%20reduces%20harmful%0Aoutputs%20and%20outperforms%20baseline%20methods%20in%20most%20metrics%2C%20offering%20a%0Alightweight%20approach%20to%20AI%20safety%20with%20no%20additional%20training%20data%20or%20model%0Afine-tuning%2C%20while%20incurring%20only%20a%20small%20computational%20overhead%20at%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12672v1&entry.124074799=Read"},
{"title": "Universal Adaptive Environment Discovery", "author": "Madi Matymov and Ba-Hien Tran and Maurizio Filippone", "abstract": "  An open problem in Machine Learning is how to avoid models to exploit\nspurious correlations in the data; a famous example is the background-label\nshortcut in the Waterbirds dataset. A common remedy is to train a model across\nmultiple environments; in the Waterbirds dataset, this corresponds to training\nby randomizing the background. However, selecting the right environments is a\nchallenging problem, given that these are rarely known a priori. We propose\nUniversal Adaptive Environment Discovery (UAED), a unified framework that\nlearns a distribution over data transformations that instantiate environments,\nand optimizes any robust objective averaged over this learned distribution.\nUAED yields adaptive variants of IRM, REx, GroupDRO, and CORAL without\npredefined groups or manual environment design. We provide a theoretical\nanalysis by providing PAC-Bayes bounds and by showing robustness to test\nenvironment distributions under standard conditions. Empirically, UAED\ndiscovers interpretable environment distributions and improves worst-case\naccuracy on standard benchmarks, while remaining competitive on mean accuracy.\nOur results indicate that making environments adaptive is a practical route to\nout-of-distribution generalization.\n", "link": "http://arxiv.org/abs/2510.12547v1", "date": "2025-10-14", "relevancy": 1.6438, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5644}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5467}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Adaptive%20Environment%20Discovery&body=Title%3A%20Universal%20Adaptive%20Environment%20Discovery%0AAuthor%3A%20Madi%20Matymov%20and%20Ba-Hien%20Tran%20and%20Maurizio%20Filippone%0AAbstract%3A%20%20%20An%20open%20problem%20in%20Machine%20Learning%20is%20how%20to%20avoid%20models%20to%20exploit%0Aspurious%20correlations%20in%20the%20data%3B%20a%20famous%20example%20is%20the%20background-label%0Ashortcut%20in%20the%20Waterbirds%20dataset.%20A%20common%20remedy%20is%20to%20train%20a%20model%20across%0Amultiple%20environments%3B%20in%20the%20Waterbirds%20dataset%2C%20this%20corresponds%20to%20training%0Aby%20randomizing%20the%20background.%20However%2C%20selecting%20the%20right%20environments%20is%20a%0Achallenging%20problem%2C%20given%20that%20these%20are%20rarely%20known%20a%20priori.%20We%20propose%0AUniversal%20Adaptive%20Environment%20Discovery%20%28UAED%29%2C%20a%20unified%20framework%20that%0Alearns%20a%20distribution%20over%20data%20transformations%20that%20instantiate%20environments%2C%0Aand%20optimizes%20any%20robust%20objective%20averaged%20over%20this%20learned%20distribution.%0AUAED%20yields%20adaptive%20variants%20of%20IRM%2C%20REx%2C%20GroupDRO%2C%20and%20CORAL%20without%0Apredefined%20groups%20or%20manual%20environment%20design.%20We%20provide%20a%20theoretical%0Aanalysis%20by%20providing%20PAC-Bayes%20bounds%20and%20by%20showing%20robustness%20to%20test%0Aenvironment%20distributions%20under%20standard%20conditions.%20Empirically%2C%20UAED%0Adiscovers%20interpretable%20environment%20distributions%20and%20improves%20worst-case%0Aaccuracy%20on%20standard%20benchmarks%2C%20while%20remaining%20competitive%20on%20mean%20accuracy.%0AOur%20results%20indicate%20that%20making%20environments%20adaptive%20is%20a%20practical%20route%20to%0Aout-of-distribution%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12547v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Adaptive%2520Environment%2520Discovery%26entry.906535625%3DMadi%2520Matymov%2520and%2520Ba-Hien%2520Tran%2520and%2520Maurizio%2520Filippone%26entry.1292438233%3D%2520%2520An%2520open%2520problem%2520in%2520Machine%2520Learning%2520is%2520how%2520to%2520avoid%2520models%2520to%2520exploit%250Aspurious%2520correlations%2520in%2520the%2520data%253B%2520a%2520famous%2520example%2520is%2520the%2520background-label%250Ashortcut%2520in%2520the%2520Waterbirds%2520dataset.%2520A%2520common%2520remedy%2520is%2520to%2520train%2520a%2520model%2520across%250Amultiple%2520environments%253B%2520in%2520the%2520Waterbirds%2520dataset%252C%2520this%2520corresponds%2520to%2520training%250Aby%2520randomizing%2520the%2520background.%2520However%252C%2520selecting%2520the%2520right%2520environments%2520is%2520a%250Achallenging%2520problem%252C%2520given%2520that%2520these%2520are%2520rarely%2520known%2520a%2520priori.%2520We%2520propose%250AUniversal%2520Adaptive%2520Environment%2520Discovery%2520%2528UAED%2529%252C%2520a%2520unified%2520framework%2520that%250Alearns%2520a%2520distribution%2520over%2520data%2520transformations%2520that%2520instantiate%2520environments%252C%250Aand%2520optimizes%2520any%2520robust%2520objective%2520averaged%2520over%2520this%2520learned%2520distribution.%250AUAED%2520yields%2520adaptive%2520variants%2520of%2520IRM%252C%2520REx%252C%2520GroupDRO%252C%2520and%2520CORAL%2520without%250Apredefined%2520groups%2520or%2520manual%2520environment%2520design.%2520We%2520provide%2520a%2520theoretical%250Aanalysis%2520by%2520providing%2520PAC-Bayes%2520bounds%2520and%2520by%2520showing%2520robustness%2520to%2520test%250Aenvironment%2520distributions%2520under%2520standard%2520conditions.%2520Empirically%252C%2520UAED%250Adiscovers%2520interpretable%2520environment%2520distributions%2520and%2520improves%2520worst-case%250Aaccuracy%2520on%2520standard%2520benchmarks%252C%2520while%2520remaining%2520competitive%2520on%2520mean%2520accuracy.%250AOur%2520results%2520indicate%2520that%2520making%2520environments%2520adaptive%2520is%2520a%2520practical%2520route%2520to%250Aout-of-distribution%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12547v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Adaptive%20Environment%20Discovery&entry.906535625=Madi%20Matymov%20and%20Ba-Hien%20Tran%20and%20Maurizio%20Filippone&entry.1292438233=%20%20An%20open%20problem%20in%20Machine%20Learning%20is%20how%20to%20avoid%20models%20to%20exploit%0Aspurious%20correlations%20in%20the%20data%3B%20a%20famous%20example%20is%20the%20background-label%0Ashortcut%20in%20the%20Waterbirds%20dataset.%20A%20common%20remedy%20is%20to%20train%20a%20model%20across%0Amultiple%20environments%3B%20in%20the%20Waterbirds%20dataset%2C%20this%20corresponds%20to%20training%0Aby%20randomizing%20the%20background.%20However%2C%20selecting%20the%20right%20environments%20is%20a%0Achallenging%20problem%2C%20given%20that%20these%20are%20rarely%20known%20a%20priori.%20We%20propose%0AUniversal%20Adaptive%20Environment%20Discovery%20%28UAED%29%2C%20a%20unified%20framework%20that%0Alearns%20a%20distribution%20over%20data%20transformations%20that%20instantiate%20environments%2C%0Aand%20optimizes%20any%20robust%20objective%20averaged%20over%20this%20learned%20distribution.%0AUAED%20yields%20adaptive%20variants%20of%20IRM%2C%20REx%2C%20GroupDRO%2C%20and%20CORAL%20without%0Apredefined%20groups%20or%20manual%20environment%20design.%20We%20provide%20a%20theoretical%0Aanalysis%20by%20providing%20PAC-Bayes%20bounds%20and%20by%20showing%20robustness%20to%20test%0Aenvironment%20distributions%20under%20standard%20conditions.%20Empirically%2C%20UAED%0Adiscovers%20interpretable%20environment%20distributions%20and%20improves%20worst-case%0Aaccuracy%20on%20standard%20benchmarks%2C%20while%20remaining%20competitive%20on%20mean%20accuracy.%0AOur%20results%20indicate%20that%20making%20environments%20adaptive%20is%20a%20practical%20route%20to%0Aout-of-distribution%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12547v1&entry.124074799=Read"},
{"title": "Physics-Informed Autonomous LLM Agents for Explainable Power Electronics\n  Modulation Design", "author": "Junhua Liu and Fanfan Lin and Xinze Li and Kwan Hui Lim and Shuai Zhao", "abstract": "  LLM-based autonomous agents have recently shown strong capabilities in\nsolving complex industrial design tasks. However, in domains aiming for carbon\nneutrality and high-performance renewable energy systems, current AI-assisted\ndesign automation methods face critical challenges in explainability,\nscalability, and practical usability. To address these limitations, we\nintroduce PHIA (Physics-Informed Autonomous Agent), an LLM-driven system that\nautomates modulation design for power converters in Power Electronics Systems\nwith minimal human intervention. In contrast to traditional pipeline-based\nmethods, PHIA incorporates an LLM-based planning module that interactively\nacquires and verifies design requirements via a user-friendly chat interface.\nThis planner collaborates with physics-informed simulation and optimization\ncomponents to autonomously generate and iteratively refine modulation designs.\nThe interactive interface also supports interpretability by providing textual\nexplanations and visual outputs throughout the design process. Experimental\nresults show that PHIA reduces standard mean absolute error by 63.2% compared\nto the second-best benchmark and accelerates the overall design process by over\n33 times. A user study involving 20 domain experts further confirms PHIA's\nsuperior design efficiency and usability, highlighting its potential to\ntransform industrial design workflows in power electronics.\n", "link": "http://arxiv.org/abs/2411.14214v2", "date": "2025-10-14", "relevancy": 1.4696, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5022}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4873}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Informed%20Autonomous%20LLM%20Agents%20for%20Explainable%20Power%20Electronics%0A%20%20Modulation%20Design&body=Title%3A%20Physics-Informed%20Autonomous%20LLM%20Agents%20for%20Explainable%20Power%20Electronics%0A%20%20Modulation%20Design%0AAuthor%3A%20Junhua%20Liu%20and%20Fanfan%20Lin%20and%20Xinze%20Li%20and%20Kwan%20Hui%20Lim%20and%20Shuai%20Zhao%0AAbstract%3A%20%20%20LLM-based%20autonomous%20agents%20have%20recently%20shown%20strong%20capabilities%20in%0Asolving%20complex%20industrial%20design%20tasks.%20However%2C%20in%20domains%20aiming%20for%20carbon%0Aneutrality%20and%20high-performance%20renewable%20energy%20systems%2C%20current%20AI-assisted%0Adesign%20automation%20methods%20face%20critical%20challenges%20in%20explainability%2C%0Ascalability%2C%20and%20practical%20usability.%20To%20address%20these%20limitations%2C%20we%0Aintroduce%20PHIA%20%28Physics-Informed%20Autonomous%20Agent%29%2C%20an%20LLM-driven%20system%20that%0Aautomates%20modulation%20design%20for%20power%20converters%20in%20Power%20Electronics%20Systems%0Awith%20minimal%20human%20intervention.%20In%20contrast%20to%20traditional%20pipeline-based%0Amethods%2C%20PHIA%20incorporates%20an%20LLM-based%20planning%20module%20that%20interactively%0Aacquires%20and%20verifies%20design%20requirements%20via%20a%20user-friendly%20chat%20interface.%0AThis%20planner%20collaborates%20with%20physics-informed%20simulation%20and%20optimization%0Acomponents%20to%20autonomously%20generate%20and%20iteratively%20refine%20modulation%20designs.%0AThe%20interactive%20interface%20also%20supports%20interpretability%20by%20providing%20textual%0Aexplanations%20and%20visual%20outputs%20throughout%20the%20design%20process.%20Experimental%0Aresults%20show%20that%20PHIA%20reduces%20standard%20mean%20absolute%20error%20by%2063.2%25%20compared%0Ato%20the%20second-best%20benchmark%20and%20accelerates%20the%20overall%20design%20process%20by%20over%0A33%20times.%20A%20user%20study%20involving%2020%20domain%20experts%20further%20confirms%20PHIA%27s%0Asuperior%20design%20efficiency%20and%20usability%2C%20highlighting%20its%20potential%20to%0Atransform%20industrial%20design%20workflows%20in%20power%20electronics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14214v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Informed%2520Autonomous%2520LLM%2520Agents%2520for%2520Explainable%2520Power%2520Electronics%250A%2520%2520Modulation%2520Design%26entry.906535625%3DJunhua%2520Liu%2520and%2520Fanfan%2520Lin%2520and%2520Xinze%2520Li%2520and%2520Kwan%2520Hui%2520Lim%2520and%2520Shuai%2520Zhao%26entry.1292438233%3D%2520%2520LLM-based%2520autonomous%2520agents%2520have%2520recently%2520shown%2520strong%2520capabilities%2520in%250Asolving%2520complex%2520industrial%2520design%2520tasks.%2520However%252C%2520in%2520domains%2520aiming%2520for%2520carbon%250Aneutrality%2520and%2520high-performance%2520renewable%2520energy%2520systems%252C%2520current%2520AI-assisted%250Adesign%2520automation%2520methods%2520face%2520critical%2520challenges%2520in%2520explainability%252C%250Ascalability%252C%2520and%2520practical%2520usability.%2520To%2520address%2520these%2520limitations%252C%2520we%250Aintroduce%2520PHIA%2520%2528Physics-Informed%2520Autonomous%2520Agent%2529%252C%2520an%2520LLM-driven%2520system%2520that%250Aautomates%2520modulation%2520design%2520for%2520power%2520converters%2520in%2520Power%2520Electronics%2520Systems%250Awith%2520minimal%2520human%2520intervention.%2520In%2520contrast%2520to%2520traditional%2520pipeline-based%250Amethods%252C%2520PHIA%2520incorporates%2520an%2520LLM-based%2520planning%2520module%2520that%2520interactively%250Aacquires%2520and%2520verifies%2520design%2520requirements%2520via%2520a%2520user-friendly%2520chat%2520interface.%250AThis%2520planner%2520collaborates%2520with%2520physics-informed%2520simulation%2520and%2520optimization%250Acomponents%2520to%2520autonomously%2520generate%2520and%2520iteratively%2520refine%2520modulation%2520designs.%250AThe%2520interactive%2520interface%2520also%2520supports%2520interpretability%2520by%2520providing%2520textual%250Aexplanations%2520and%2520visual%2520outputs%2520throughout%2520the%2520design%2520process.%2520Experimental%250Aresults%2520show%2520that%2520PHIA%2520reduces%2520standard%2520mean%2520absolute%2520error%2520by%252063.2%2525%2520compared%250Ato%2520the%2520second-best%2520benchmark%2520and%2520accelerates%2520the%2520overall%2520design%2520process%2520by%2520over%250A33%2520times.%2520A%2520user%2520study%2520involving%252020%2520domain%2520experts%2520further%2520confirms%2520PHIA%2527s%250Asuperior%2520design%2520efficiency%2520and%2520usability%252C%2520highlighting%2520its%2520potential%2520to%250Atransform%2520industrial%2520design%2520workflows%2520in%2520power%2520electronics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14214v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Informed%20Autonomous%20LLM%20Agents%20for%20Explainable%20Power%20Electronics%0A%20%20Modulation%20Design&entry.906535625=Junhua%20Liu%20and%20Fanfan%20Lin%20and%20Xinze%20Li%20and%20Kwan%20Hui%20Lim%20and%20Shuai%20Zhao&entry.1292438233=%20%20LLM-based%20autonomous%20agents%20have%20recently%20shown%20strong%20capabilities%20in%0Asolving%20complex%20industrial%20design%20tasks.%20However%2C%20in%20domains%20aiming%20for%20carbon%0Aneutrality%20and%20high-performance%20renewable%20energy%20systems%2C%20current%20AI-assisted%0Adesign%20automation%20methods%20face%20critical%20challenges%20in%20explainability%2C%0Ascalability%2C%20and%20practical%20usability.%20To%20address%20these%20limitations%2C%20we%0Aintroduce%20PHIA%20%28Physics-Informed%20Autonomous%20Agent%29%2C%20an%20LLM-driven%20system%20that%0Aautomates%20modulation%20design%20for%20power%20converters%20in%20Power%20Electronics%20Systems%0Awith%20minimal%20human%20intervention.%20In%20contrast%20to%20traditional%20pipeline-based%0Amethods%2C%20PHIA%20incorporates%20an%20LLM-based%20planning%20module%20that%20interactively%0Aacquires%20and%20verifies%20design%20requirements%20via%20a%20user-friendly%20chat%20interface.%0AThis%20planner%20collaborates%20with%20physics-informed%20simulation%20and%20optimization%0Acomponents%20to%20autonomously%20generate%20and%20iteratively%20refine%20modulation%20designs.%0AThe%20interactive%20interface%20also%20supports%20interpretability%20by%20providing%20textual%0Aexplanations%20and%20visual%20outputs%20throughout%20the%20design%20process.%20Experimental%0Aresults%20show%20that%20PHIA%20reduces%20standard%20mean%20absolute%20error%20by%2063.2%25%20compared%0Ato%20the%20second-best%20benchmark%20and%20accelerates%20the%20overall%20design%20process%20by%20over%0A33%20times.%20A%20user%20study%20involving%2020%20domain%20experts%20further%20confirms%20PHIA%27s%0Asuperior%20design%20efficiency%20and%20usability%2C%20highlighting%20its%20potential%20to%0Atransform%20industrial%20design%20workflows%20in%20power%20electronics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14214v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


