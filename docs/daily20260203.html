<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260202.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction", "author": "Changbai Li and Haodong Zhu and Hanlin Chen and Xiuping Liang and Tongfei Chen and Shuwei Shao and Linlin Yang and Huobin Tan and Baochang Zhang", "abstract": "While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction.", "link": "http://arxiv.org/abs/2602.02089v1", "date": "2026-02-02", "relevancy": 3.4729, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7396}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6944}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UrbanGS%3A%20A%20Scalable%20and%20Efficient%20Architecture%20for%20Geometrically%20Accurate%20Large-Scene%20Reconstruction&body=Title%3A%20UrbanGS%3A%20A%20Scalable%20and%20Efficient%20Architecture%20for%20Geometrically%20Accurate%20Large-Scene%20Reconstruction%0AAuthor%3A%20Changbai%20Li%20and%20Haodong%20Zhu%20and%20Hanlin%20Chen%20and%20Xiuping%20Liang%20and%20Tongfei%20Chen%20and%20Shuwei%20Shao%20and%20Linlin%20Yang%20and%20Huobin%20Tan%20and%20Baochang%20Zhang%0AAbstract%3A%20While%203D%20Gaussian%20Splatting%20%283DGS%29%20enables%20high-quality%2C%20real-time%20rendering%20for%20bounded%20scenes%2C%20its%20extension%20to%20large-scale%20urban%20environments%20gives%20rise%20to%20critical%20challenges%20in%20terms%20of%20geometric%20consistency%2C%20memory%20efficiency%2C%20and%20computational%20scalability.%20To%20address%20these%20issues%2C%20we%20present%20UrbanGS%2C%20a%20scalable%20reconstruction%20framework%20that%20effectively%20tackles%20these%20challenges%20for%20city-scale%20applications.%20First%2C%20we%20propose%20a%20Depth-Consistent%20D-Normal%20Regularization%20module.%20Unlike%20existing%20approaches%20that%20rely%20solely%20on%20monocular%20normal%20estimators%2C%20which%20can%20effectively%20update%20rotation%20parameters%20yet%20struggle%20to%20update%20position%20parameters%2C%20our%20method%20integrates%20D-Normal%20constraints%20with%20external%20depth%20supervision.%20This%20allows%20for%20comprehensive%20updates%20of%20all%20geometric%20parameters.%20By%20further%20incorporating%20an%20adaptive%20confidence%20weighting%20mechanism%20based%20on%20gradient%20consistency%20and%20inverse%20depth%20deviation%2C%20our%20approach%20significantly%20enhances%20multi-view%20depth%20alignment%20and%20geometric%20coherence%2C%20which%20effectively%20resolves%20the%20issue%20of%20geometric%20accuracy%20in%20complex%20large-scale%20scenes.%20To%20improve%20scalability%2C%20we%20introduce%20a%20Spatially%20Adaptive%20Gaussian%20Pruning%20%28SAGP%29%20strategy%2C%20which%20dynamically%20adjusts%20Gaussian%20density%20based%20on%20local%20geometric%20complexity%20and%20visibility%20to%20reduce%20redundancy.%20Additionally%2C%20a%20unified%20partitioning%20and%20view%20assignment%20scheme%20is%20designed%20to%20eliminate%20boundary%20artifacts%20and%20optimize%20computational%20load.%20Extensive%20experiments%20on%20multiple%20urban%20datasets%20demonstrate%20that%20UrbanGS%20achieves%20superior%20performance%20in%20rendering%20quality%2C%20geometric%20accuracy%2C%20and%20memory%20efficiency%2C%20providing%20a%20systematic%20solution%20for%20high-fidelity%20large-scale%20scene%20reconstruction.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrbanGS%253A%2520A%2520Scalable%2520and%2520Efficient%2520Architecture%2520for%2520Geometrically%2520Accurate%2520Large-Scene%2520Reconstruction%26entry.906535625%3DChangbai%2520Li%2520and%2520Haodong%2520Zhu%2520and%2520Hanlin%2520Chen%2520and%2520Xiuping%2520Liang%2520and%2520Tongfei%2520Chen%2520and%2520Shuwei%2520Shao%2520and%2520Linlin%2520Yang%2520and%2520Huobin%2520Tan%2520and%2520Baochang%2520Zhang%26entry.1292438233%3DWhile%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520enables%2520high-quality%252C%2520real-time%2520rendering%2520for%2520bounded%2520scenes%252C%2520its%2520extension%2520to%2520large-scale%2520urban%2520environments%2520gives%2520rise%2520to%2520critical%2520challenges%2520in%2520terms%2520of%2520geometric%2520consistency%252C%2520memory%2520efficiency%252C%2520and%2520computational%2520scalability.%2520To%2520address%2520these%2520issues%252C%2520we%2520present%2520UrbanGS%252C%2520a%2520scalable%2520reconstruction%2520framework%2520that%2520effectively%2520tackles%2520these%2520challenges%2520for%2520city-scale%2520applications.%2520First%252C%2520we%2520propose%2520a%2520Depth-Consistent%2520D-Normal%2520Regularization%2520module.%2520Unlike%2520existing%2520approaches%2520that%2520rely%2520solely%2520on%2520monocular%2520normal%2520estimators%252C%2520which%2520can%2520effectively%2520update%2520rotation%2520parameters%2520yet%2520struggle%2520to%2520update%2520position%2520parameters%252C%2520our%2520method%2520integrates%2520D-Normal%2520constraints%2520with%2520external%2520depth%2520supervision.%2520This%2520allows%2520for%2520comprehensive%2520updates%2520of%2520all%2520geometric%2520parameters.%2520By%2520further%2520incorporating%2520an%2520adaptive%2520confidence%2520weighting%2520mechanism%2520based%2520on%2520gradient%2520consistency%2520and%2520inverse%2520depth%2520deviation%252C%2520our%2520approach%2520significantly%2520enhances%2520multi-view%2520depth%2520alignment%2520and%2520geometric%2520coherence%252C%2520which%2520effectively%2520resolves%2520the%2520issue%2520of%2520geometric%2520accuracy%2520in%2520complex%2520large-scale%2520scenes.%2520To%2520improve%2520scalability%252C%2520we%2520introduce%2520a%2520Spatially%2520Adaptive%2520Gaussian%2520Pruning%2520%2528SAGP%2529%2520strategy%252C%2520which%2520dynamically%2520adjusts%2520Gaussian%2520density%2520based%2520on%2520local%2520geometric%2520complexity%2520and%2520visibility%2520to%2520reduce%2520redundancy.%2520Additionally%252C%2520a%2520unified%2520partitioning%2520and%2520view%2520assignment%2520scheme%2520is%2520designed%2520to%2520eliminate%2520boundary%2520artifacts%2520and%2520optimize%2520computational%2520load.%2520Extensive%2520experiments%2520on%2520multiple%2520urban%2520datasets%2520demonstrate%2520that%2520UrbanGS%2520achieves%2520superior%2520performance%2520in%2520rendering%2520quality%252C%2520geometric%2520accuracy%252C%2520and%2520memory%2520efficiency%252C%2520providing%2520a%2520systematic%2520solution%2520for%2520high-fidelity%2520large-scale%2520scene%2520reconstruction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UrbanGS%3A%20A%20Scalable%20and%20Efficient%20Architecture%20for%20Geometrically%20Accurate%20Large-Scene%20Reconstruction&entry.906535625=Changbai%20Li%20and%20Haodong%20Zhu%20and%20Hanlin%20Chen%20and%20Xiuping%20Liang%20and%20Tongfei%20Chen%20and%20Shuwei%20Shao%20and%20Linlin%20Yang%20and%20Huobin%20Tan%20and%20Baochang%20Zhang&entry.1292438233=While%203D%20Gaussian%20Splatting%20%283DGS%29%20enables%20high-quality%2C%20real-time%20rendering%20for%20bounded%20scenes%2C%20its%20extension%20to%20large-scale%20urban%20environments%20gives%20rise%20to%20critical%20challenges%20in%20terms%20of%20geometric%20consistency%2C%20memory%20efficiency%2C%20and%20computational%20scalability.%20To%20address%20these%20issues%2C%20we%20present%20UrbanGS%2C%20a%20scalable%20reconstruction%20framework%20that%20effectively%20tackles%20these%20challenges%20for%20city-scale%20applications.%20First%2C%20we%20propose%20a%20Depth-Consistent%20D-Normal%20Regularization%20module.%20Unlike%20existing%20approaches%20that%20rely%20solely%20on%20monocular%20normal%20estimators%2C%20which%20can%20effectively%20update%20rotation%20parameters%20yet%20struggle%20to%20update%20position%20parameters%2C%20our%20method%20integrates%20D-Normal%20constraints%20with%20external%20depth%20supervision.%20This%20allows%20for%20comprehensive%20updates%20of%20all%20geometric%20parameters.%20By%20further%20incorporating%20an%20adaptive%20confidence%20weighting%20mechanism%20based%20on%20gradient%20consistency%20and%20inverse%20depth%20deviation%2C%20our%20approach%20significantly%20enhances%20multi-view%20depth%20alignment%20and%20geometric%20coherence%2C%20which%20effectively%20resolves%20the%20issue%20of%20geometric%20accuracy%20in%20complex%20large-scale%20scenes.%20To%20improve%20scalability%2C%20we%20introduce%20a%20Spatially%20Adaptive%20Gaussian%20Pruning%20%28SAGP%29%20strategy%2C%20which%20dynamically%20adjusts%20Gaussian%20density%20based%20on%20local%20geometric%20complexity%20and%20visibility%20to%20reduce%20redundancy.%20Additionally%2C%20a%20unified%20partitioning%20and%20view%20assignment%20scheme%20is%20designed%20to%20eliminate%20boundary%20artifacts%20and%20optimize%20computational%20load.%20Extensive%20experiments%20on%20multiple%20urban%20datasets%20demonstrate%20that%20UrbanGS%20achieves%20superior%20performance%20in%20rendering%20quality%2C%20geometric%20accuracy%2C%20and%20memory%20efficiency%2C%20providing%20a%20systematic%20solution%20for%20high-fidelity%20large-scale%20scene%20reconstruction.&entry.1838667208=http%3A//arxiv.org/abs/2602.02089v1&entry.124074799=Read"},
{"title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors", "author": "Bing He and Jingnan Gao and Yunuo Chen and Ning Cao and Gang Chen and Zhengxue Cheng and Li Song and Wenjun Zhang", "abstract": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: https://hebing-sjtu.github.io/SurfSplat-website/", "link": "http://arxiv.org/abs/2602.02000v1", "date": "2026-02-02", "relevancy": 3.4722, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7465}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6849}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SurfSplat%3A%20Conquering%20Feedforward%202D%20Gaussian%20Splatting%20with%20Surface%20Continuity%20Priors&body=Title%3A%20SurfSplat%3A%20Conquering%20Feedforward%202D%20Gaussian%20Splatting%20with%20Surface%20Continuity%20Priors%0AAuthor%3A%20Bing%20He%20and%20Jingnan%20Gao%20and%20Yunuo%20Chen%20and%20Ning%20Cao%20and%20Gang%20Chen%20and%20Zhengxue%20Cheng%20and%20Li%20Song%20and%20Wenjun%20Zhang%0AAbstract%3A%20Reconstructing%203D%20scenes%20from%20sparse%20images%20remains%20a%20challenging%20task%20due%20to%20the%20difficulty%20of%20recovering%20accurate%20geometry%20and%20texture%20without%20optimization.%20Recent%20approaches%20leverage%20generalizable%20models%20to%20generate%203D%20scenes%20using%203D%20Gaussian%20Splatting%20%283DGS%29%20primitive.%20However%2C%20they%20often%20fail%20to%20produce%20continuous%20surfaces%20and%20instead%20yield%20discrete%2C%20color-biased%20point%20clouds%20that%20appear%20plausible%20at%20normal%20resolution%20but%20reveal%20severe%20artifacts%20under%20close-up%20views.%20To%20address%20this%20issue%2C%20we%20present%20SurfSplat%2C%20a%20feedforward%20framework%20based%20on%202D%20Gaussian%20Splatting%20%282DGS%29%20primitive%2C%20which%20provides%20stronger%20anisotropy%20and%20higher%20geometric%20precision.%20By%20incorporating%20a%20surface%20continuity%20prior%20and%20a%20forced%20alpha%20blending%20strategy%2C%20SurfSplat%20reconstructs%20coherent%20geometry%20together%20with%20faithful%20textures.%20Furthermore%2C%20we%20introduce%20High-Resolution%20Rendering%20Consistency%20%28HRRC%29%2C%20a%20new%20evaluation%20metric%20designed%20to%20evaluate%20high-resolution%20reconstruction%20quality.%20Extensive%20experiments%20on%20RealEstate10K%2C%20DL3DV%2C%20and%20ScanNet%20demonstrate%20that%20SurfSplat%20consistently%20outperforms%20prior%20methods%20on%20both%20standard%20metrics%20and%20HRRC%2C%20establishing%20a%20robust%20solution%20for%20high-fidelity%203D%20reconstruction%20from%20sparse%20inputs.%20Project%20page%3A%20https%3A//hebing-sjtu.github.io/SurfSplat-website/%0ALink%3A%20http%3A//arxiv.org/abs/2602.02000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurfSplat%253A%2520Conquering%2520Feedforward%25202D%2520Gaussian%2520Splatting%2520with%2520Surface%2520Continuity%2520Priors%26entry.906535625%3DBing%2520He%2520and%2520Jingnan%2520Gao%2520and%2520Yunuo%2520Chen%2520and%2520Ning%2520Cao%2520and%2520Gang%2520Chen%2520and%2520Zhengxue%2520Cheng%2520and%2520Li%2520Song%2520and%2520Wenjun%2520Zhang%26entry.1292438233%3DReconstructing%25203D%2520scenes%2520from%2520sparse%2520images%2520remains%2520a%2520challenging%2520task%2520due%2520to%2520the%2520difficulty%2520of%2520recovering%2520accurate%2520geometry%2520and%2520texture%2520without%2520optimization.%2520Recent%2520approaches%2520leverage%2520generalizable%2520models%2520to%2520generate%25203D%2520scenes%2520using%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520primitive.%2520However%252C%2520they%2520often%2520fail%2520to%2520produce%2520continuous%2520surfaces%2520and%2520instead%2520yield%2520discrete%252C%2520color-biased%2520point%2520clouds%2520that%2520appear%2520plausible%2520at%2520normal%2520resolution%2520but%2520reveal%2520severe%2520artifacts%2520under%2520close-up%2520views.%2520To%2520address%2520this%2520issue%252C%2520we%2520present%2520SurfSplat%252C%2520a%2520feedforward%2520framework%2520based%2520on%25202D%2520Gaussian%2520Splatting%2520%25282DGS%2529%2520primitive%252C%2520which%2520provides%2520stronger%2520anisotropy%2520and%2520higher%2520geometric%2520precision.%2520By%2520incorporating%2520a%2520surface%2520continuity%2520prior%2520and%2520a%2520forced%2520alpha%2520blending%2520strategy%252C%2520SurfSplat%2520reconstructs%2520coherent%2520geometry%2520together%2520with%2520faithful%2520textures.%2520Furthermore%252C%2520we%2520introduce%2520High-Resolution%2520Rendering%2520Consistency%2520%2528HRRC%2529%252C%2520a%2520new%2520evaluation%2520metric%2520designed%2520to%2520evaluate%2520high-resolution%2520reconstruction%2520quality.%2520Extensive%2520experiments%2520on%2520RealEstate10K%252C%2520DL3DV%252C%2520and%2520ScanNet%2520demonstrate%2520that%2520SurfSplat%2520consistently%2520outperforms%2520prior%2520methods%2520on%2520both%2520standard%2520metrics%2520and%2520HRRC%252C%2520establishing%2520a%2520robust%2520solution%2520for%2520high-fidelity%25203D%2520reconstruction%2520from%2520sparse%2520inputs.%2520Project%2520page%253A%2520https%253A//hebing-sjtu.github.io/SurfSplat-website/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SurfSplat%3A%20Conquering%20Feedforward%202D%20Gaussian%20Splatting%20with%20Surface%20Continuity%20Priors&entry.906535625=Bing%20He%20and%20Jingnan%20Gao%20and%20Yunuo%20Chen%20and%20Ning%20Cao%20and%20Gang%20Chen%20and%20Zhengxue%20Cheng%20and%20Li%20Song%20and%20Wenjun%20Zhang&entry.1292438233=Reconstructing%203D%20scenes%20from%20sparse%20images%20remains%20a%20challenging%20task%20due%20to%20the%20difficulty%20of%20recovering%20accurate%20geometry%20and%20texture%20without%20optimization.%20Recent%20approaches%20leverage%20generalizable%20models%20to%20generate%203D%20scenes%20using%203D%20Gaussian%20Splatting%20%283DGS%29%20primitive.%20However%2C%20they%20often%20fail%20to%20produce%20continuous%20surfaces%20and%20instead%20yield%20discrete%2C%20color-biased%20point%20clouds%20that%20appear%20plausible%20at%20normal%20resolution%20but%20reveal%20severe%20artifacts%20under%20close-up%20views.%20To%20address%20this%20issue%2C%20we%20present%20SurfSplat%2C%20a%20feedforward%20framework%20based%20on%202D%20Gaussian%20Splatting%20%282DGS%29%20primitive%2C%20which%20provides%20stronger%20anisotropy%20and%20higher%20geometric%20precision.%20By%20incorporating%20a%20surface%20continuity%20prior%20and%20a%20forced%20alpha%20blending%20strategy%2C%20SurfSplat%20reconstructs%20coherent%20geometry%20together%20with%20faithful%20textures.%20Furthermore%2C%20we%20introduce%20High-Resolution%20Rendering%20Consistency%20%28HRRC%29%2C%20a%20new%20evaluation%20metric%20designed%20to%20evaluate%20high-resolution%20reconstruction%20quality.%20Extensive%20experiments%20on%20RealEstate10K%2C%20DL3DV%2C%20and%20ScanNet%20demonstrate%20that%20SurfSplat%20consistently%20outperforms%20prior%20methods%20on%20both%20standard%20metrics%20and%20HRRC%2C%20establishing%20a%20robust%20solution%20for%20high-fidelity%203D%20reconstruction%20from%20sparse%20inputs.%20Project%20page%3A%20https%3A//hebing-sjtu.github.io/SurfSplat-website/&entry.1838667208=http%3A//arxiv.org/abs/2602.02000v1&entry.124074799=Read"},
{"title": "HI-SLAM2: Geometry-Aware Gaussian SLAM for Fast Monocular Scene Reconstruction", "author": "Wei Zhang and Qing Cheng and David Skuddis and Niclas Zeller and Daniel Cremers and Norbert Haala", "abstract": "We present HI-SLAM2, a geometry-aware Gaussian SLAM system that achieves fast and accurate monocular scene reconstruction using only RGB input. Existing Neural SLAM or 3DGS-based SLAM methods often trade off between rendering quality and geometry accuracy, our research demonstrates that both can be achieved simultaneously with RGB input alone. The key idea of our approach is to enhance the ability for geometry estimation by combining easy-to-obtain monocular priors with learning-based dense SLAM, and then using 3D Gaussian splatting as our core map representation to efficiently model the scene. Upon loop closure, our method ensures on-the-fly global consistency through efficient pose graph bundle adjustment and instant map updates by explicitly deforming the 3D Gaussian units based on anchored keyframe updates. Furthermore, we introduce a grid-based scale alignment strategy to maintain improved scale consistency in prior depths for finer depth details. Through extensive experiments on Replica, ScanNet, and ScanNet++, we demonstrate significant improvements over existing Neural SLAM methods and even surpass RGB-D-based methods in both reconstruction and rendering quality. The project page and source code will be made available at https://hi-slam2.github.io/.", "link": "http://arxiv.org/abs/2411.17982v3", "date": "2026-02-02", "relevancy": 3.4527, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7861}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6645}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HI-SLAM2%3A%20Geometry-Aware%20Gaussian%20SLAM%20for%20Fast%20Monocular%20Scene%20Reconstruction&body=Title%3A%20HI-SLAM2%3A%20Geometry-Aware%20Gaussian%20SLAM%20for%20Fast%20Monocular%20Scene%20Reconstruction%0AAuthor%3A%20Wei%20Zhang%20and%20Qing%20Cheng%20and%20David%20Skuddis%20and%20Niclas%20Zeller%20and%20Daniel%20Cremers%20and%20Norbert%20Haala%0AAbstract%3A%20We%20present%20HI-SLAM2%2C%20a%20geometry-aware%20Gaussian%20SLAM%20system%20that%20achieves%20fast%20and%20accurate%20monocular%20scene%20reconstruction%20using%20only%20RGB%20input.%20Existing%20Neural%20SLAM%20or%203DGS-based%20SLAM%20methods%20often%20trade%20off%20between%20rendering%20quality%20and%20geometry%20accuracy%2C%20our%20research%20demonstrates%20that%20both%20can%20be%20achieved%20simultaneously%20with%20RGB%20input%20alone.%20The%20key%20idea%20of%20our%20approach%20is%20to%20enhance%20the%20ability%20for%20geometry%20estimation%20by%20combining%20easy-to-obtain%20monocular%20priors%20with%20learning-based%20dense%20SLAM%2C%20and%20then%20using%203D%20Gaussian%20splatting%20as%20our%20core%20map%20representation%20to%20efficiently%20model%20the%20scene.%20Upon%20loop%20closure%2C%20our%20method%20ensures%20on-the-fly%20global%20consistency%20through%20efficient%20pose%20graph%20bundle%20adjustment%20and%20instant%20map%20updates%20by%20explicitly%20deforming%20the%203D%20Gaussian%20units%20based%20on%20anchored%20keyframe%20updates.%20Furthermore%2C%20we%20introduce%20a%20grid-based%20scale%20alignment%20strategy%20to%20maintain%20improved%20scale%20consistency%20in%20prior%20depths%20for%20finer%20depth%20details.%20Through%20extensive%20experiments%20on%20Replica%2C%20ScanNet%2C%20and%20ScanNet%2B%2B%2C%20we%20demonstrate%20significant%20improvements%20over%20existing%20Neural%20SLAM%20methods%20and%20even%20surpass%20RGB-D-based%20methods%20in%20both%20reconstruction%20and%20rendering%20quality.%20The%20project%20page%20and%20source%20code%20will%20be%20made%20available%20at%20https%3A//hi-slam2.github.io/.%0ALink%3A%20http%3A//arxiv.org/abs/2411.17982v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHI-SLAM2%253A%2520Geometry-Aware%2520Gaussian%2520SLAM%2520for%2520Fast%2520Monocular%2520Scene%2520Reconstruction%26entry.906535625%3DWei%2520Zhang%2520and%2520Qing%2520Cheng%2520and%2520David%2520Skuddis%2520and%2520Niclas%2520Zeller%2520and%2520Daniel%2520Cremers%2520and%2520Norbert%2520Haala%26entry.1292438233%3DWe%2520present%2520HI-SLAM2%252C%2520a%2520geometry-aware%2520Gaussian%2520SLAM%2520system%2520that%2520achieves%2520fast%2520and%2520accurate%2520monocular%2520scene%2520reconstruction%2520using%2520only%2520RGB%2520input.%2520Existing%2520Neural%2520SLAM%2520or%25203DGS-based%2520SLAM%2520methods%2520often%2520trade%2520off%2520between%2520rendering%2520quality%2520and%2520geometry%2520accuracy%252C%2520our%2520research%2520demonstrates%2520that%2520both%2520can%2520be%2520achieved%2520simultaneously%2520with%2520RGB%2520input%2520alone.%2520The%2520key%2520idea%2520of%2520our%2520approach%2520is%2520to%2520enhance%2520the%2520ability%2520for%2520geometry%2520estimation%2520by%2520combining%2520easy-to-obtain%2520monocular%2520priors%2520with%2520learning-based%2520dense%2520SLAM%252C%2520and%2520then%2520using%25203D%2520Gaussian%2520splatting%2520as%2520our%2520core%2520map%2520representation%2520to%2520efficiently%2520model%2520the%2520scene.%2520Upon%2520loop%2520closure%252C%2520our%2520method%2520ensures%2520on-the-fly%2520global%2520consistency%2520through%2520efficient%2520pose%2520graph%2520bundle%2520adjustment%2520and%2520instant%2520map%2520updates%2520by%2520explicitly%2520deforming%2520the%25203D%2520Gaussian%2520units%2520based%2520on%2520anchored%2520keyframe%2520updates.%2520Furthermore%252C%2520we%2520introduce%2520a%2520grid-based%2520scale%2520alignment%2520strategy%2520to%2520maintain%2520improved%2520scale%2520consistency%2520in%2520prior%2520depths%2520for%2520finer%2520depth%2520details.%2520Through%2520extensive%2520experiments%2520on%2520Replica%252C%2520ScanNet%252C%2520and%2520ScanNet%252B%252B%252C%2520we%2520demonstrate%2520significant%2520improvements%2520over%2520existing%2520Neural%2520SLAM%2520methods%2520and%2520even%2520surpass%2520RGB-D-based%2520methods%2520in%2520both%2520reconstruction%2520and%2520rendering%2520quality.%2520The%2520project%2520page%2520and%2520source%2520code%2520will%2520be%2520made%2520available%2520at%2520https%253A//hi-slam2.github.io/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17982v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HI-SLAM2%3A%20Geometry-Aware%20Gaussian%20SLAM%20for%20Fast%20Monocular%20Scene%20Reconstruction&entry.906535625=Wei%20Zhang%20and%20Qing%20Cheng%20and%20David%20Skuddis%20and%20Niclas%20Zeller%20and%20Daniel%20Cremers%20and%20Norbert%20Haala&entry.1292438233=We%20present%20HI-SLAM2%2C%20a%20geometry-aware%20Gaussian%20SLAM%20system%20that%20achieves%20fast%20and%20accurate%20monocular%20scene%20reconstruction%20using%20only%20RGB%20input.%20Existing%20Neural%20SLAM%20or%203DGS-based%20SLAM%20methods%20often%20trade%20off%20between%20rendering%20quality%20and%20geometry%20accuracy%2C%20our%20research%20demonstrates%20that%20both%20can%20be%20achieved%20simultaneously%20with%20RGB%20input%20alone.%20The%20key%20idea%20of%20our%20approach%20is%20to%20enhance%20the%20ability%20for%20geometry%20estimation%20by%20combining%20easy-to-obtain%20monocular%20priors%20with%20learning-based%20dense%20SLAM%2C%20and%20then%20using%203D%20Gaussian%20splatting%20as%20our%20core%20map%20representation%20to%20efficiently%20model%20the%20scene.%20Upon%20loop%20closure%2C%20our%20method%20ensures%20on-the-fly%20global%20consistency%20through%20efficient%20pose%20graph%20bundle%20adjustment%20and%20instant%20map%20updates%20by%20explicitly%20deforming%20the%203D%20Gaussian%20units%20based%20on%20anchored%20keyframe%20updates.%20Furthermore%2C%20we%20introduce%20a%20grid-based%20scale%20alignment%20strategy%20to%20maintain%20improved%20scale%20consistency%20in%20prior%20depths%20for%20finer%20depth%20details.%20Through%20extensive%20experiments%20on%20Replica%2C%20ScanNet%2C%20and%20ScanNet%2B%2B%2C%20we%20demonstrate%20significant%20improvements%20over%20existing%20Neural%20SLAM%20methods%20and%20even%20surpass%20RGB-D-based%20methods%20in%20both%20reconstruction%20and%20rendering%20quality.%20The%20project%20page%20and%20source%20code%20will%20be%20made%20available%20at%20https%3A//hi-slam2.github.io/.&entry.1838667208=http%3A//arxiv.org/abs/2411.17982v3&entry.124074799=Read"},
{"title": "3D Foundation Model-Based Loop Closing for Decentralized Collaborative SLAM", "author": "Pierre-Yves Lajoie and Benjamin Ramtoula and Daniele De Martini and Giovanni Beltrame", "abstract": "Decentralized Collaborative Simultaneous Localization And Mapping (C-SLAM) techniques often struggle to identify map overlaps due to significant viewpoint variations among robots. Motivated by recent advancements in 3D foundation models, which can register images despite large viewpoint differences, we propose a robust loop closing approach that leverages these models to establish inter-robot measurements. In contrast to resource-intensive methods requiring full 3D reconstruction within a centralized map, our approach integrates foundation models into existing SLAM pipelines, yielding scalable and robust multi-robot mapping. Our contributions include: (1) integrating 3D foundation models to reliably estimate relative poses from monocular image pairs within decentralized C-SLAM; (2) introducing robust outlier mitigation techniques critical to the use of these relative poses; and (3) developing specialized pose graph optimization formulations that efficiently resolve scale ambiguities. We evaluate our method against state-of-the-art approaches, demonstrating improvements in localization and mapping accuracy, alongside significant gains in computational and memory efficiency. These results highlight the potential of our approach for deployment in large-scale multi-robot scenarios.", "link": "http://arxiv.org/abs/2602.02430v1", "date": "2026-02-02", "relevancy": 3.0068, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6175}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5992}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Foundation%20Model-Based%20Loop%20Closing%20for%20Decentralized%20Collaborative%20SLAM&body=Title%3A%203D%20Foundation%20Model-Based%20Loop%20Closing%20for%20Decentralized%20Collaborative%20SLAM%0AAuthor%3A%20Pierre-Yves%20Lajoie%20and%20Benjamin%20Ramtoula%20and%20Daniele%20De%20Martini%20and%20Giovanni%20Beltrame%0AAbstract%3A%20Decentralized%20Collaborative%20Simultaneous%20Localization%20And%20Mapping%20%28C-SLAM%29%20techniques%20often%20struggle%20to%20identify%20map%20overlaps%20due%20to%20significant%20viewpoint%20variations%20among%20robots.%20Motivated%20by%20recent%20advancements%20in%203D%20foundation%20models%2C%20which%20can%20register%20images%20despite%20large%20viewpoint%20differences%2C%20we%20propose%20a%20robust%20loop%20closing%20approach%20that%20leverages%20these%20models%20to%20establish%20inter-robot%20measurements.%20In%20contrast%20to%20resource-intensive%20methods%20requiring%20full%203D%20reconstruction%20within%20a%20centralized%20map%2C%20our%20approach%20integrates%20foundation%20models%20into%20existing%20SLAM%20pipelines%2C%20yielding%20scalable%20and%20robust%20multi-robot%20mapping.%20Our%20contributions%20include%3A%20%281%29%20integrating%203D%20foundation%20models%20to%20reliably%20estimate%20relative%20poses%20from%20monocular%20image%20pairs%20within%20decentralized%20C-SLAM%3B%20%282%29%20introducing%20robust%20outlier%20mitigation%20techniques%20critical%20to%20the%20use%20of%20these%20relative%20poses%3B%20and%20%283%29%20developing%20specialized%20pose%20graph%20optimization%20formulations%20that%20efficiently%20resolve%20scale%20ambiguities.%20We%20evaluate%20our%20method%20against%20state-of-the-art%20approaches%2C%20demonstrating%20improvements%20in%20localization%20and%20mapping%20accuracy%2C%20alongside%20significant%20gains%20in%20computational%20and%20memory%20efficiency.%20These%20results%20highlight%20the%20potential%20of%20our%20approach%20for%20deployment%20in%20large-scale%20multi-robot%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Foundation%2520Model-Based%2520Loop%2520Closing%2520for%2520Decentralized%2520Collaborative%2520SLAM%26entry.906535625%3DPierre-Yves%2520Lajoie%2520and%2520Benjamin%2520Ramtoula%2520and%2520Daniele%2520De%2520Martini%2520and%2520Giovanni%2520Beltrame%26entry.1292438233%3DDecentralized%2520Collaborative%2520Simultaneous%2520Localization%2520And%2520Mapping%2520%2528C-SLAM%2529%2520techniques%2520often%2520struggle%2520to%2520identify%2520map%2520overlaps%2520due%2520to%2520significant%2520viewpoint%2520variations%2520among%2520robots.%2520Motivated%2520by%2520recent%2520advancements%2520in%25203D%2520foundation%2520models%252C%2520which%2520can%2520register%2520images%2520despite%2520large%2520viewpoint%2520differences%252C%2520we%2520propose%2520a%2520robust%2520loop%2520closing%2520approach%2520that%2520leverages%2520these%2520models%2520to%2520establish%2520inter-robot%2520measurements.%2520In%2520contrast%2520to%2520resource-intensive%2520methods%2520requiring%2520full%25203D%2520reconstruction%2520within%2520a%2520centralized%2520map%252C%2520our%2520approach%2520integrates%2520foundation%2520models%2520into%2520existing%2520SLAM%2520pipelines%252C%2520yielding%2520scalable%2520and%2520robust%2520multi-robot%2520mapping.%2520Our%2520contributions%2520include%253A%2520%25281%2529%2520integrating%25203D%2520foundation%2520models%2520to%2520reliably%2520estimate%2520relative%2520poses%2520from%2520monocular%2520image%2520pairs%2520within%2520decentralized%2520C-SLAM%253B%2520%25282%2529%2520introducing%2520robust%2520outlier%2520mitigation%2520techniques%2520critical%2520to%2520the%2520use%2520of%2520these%2520relative%2520poses%253B%2520and%2520%25283%2529%2520developing%2520specialized%2520pose%2520graph%2520optimization%2520formulations%2520that%2520efficiently%2520resolve%2520scale%2520ambiguities.%2520We%2520evaluate%2520our%2520method%2520against%2520state-of-the-art%2520approaches%252C%2520demonstrating%2520improvements%2520in%2520localization%2520and%2520mapping%2520accuracy%252C%2520alongside%2520significant%2520gains%2520in%2520computational%2520and%2520memory%2520efficiency.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520our%2520approach%2520for%2520deployment%2520in%2520large-scale%2520multi-robot%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Foundation%20Model-Based%20Loop%20Closing%20for%20Decentralized%20Collaborative%20SLAM&entry.906535625=Pierre-Yves%20Lajoie%20and%20Benjamin%20Ramtoula%20and%20Daniele%20De%20Martini%20and%20Giovanni%20Beltrame&entry.1292438233=Decentralized%20Collaborative%20Simultaneous%20Localization%20And%20Mapping%20%28C-SLAM%29%20techniques%20often%20struggle%20to%20identify%20map%20overlaps%20due%20to%20significant%20viewpoint%20variations%20among%20robots.%20Motivated%20by%20recent%20advancements%20in%203D%20foundation%20models%2C%20which%20can%20register%20images%20despite%20large%20viewpoint%20differences%2C%20we%20propose%20a%20robust%20loop%20closing%20approach%20that%20leverages%20these%20models%20to%20establish%20inter-robot%20measurements.%20In%20contrast%20to%20resource-intensive%20methods%20requiring%20full%203D%20reconstruction%20within%20a%20centralized%20map%2C%20our%20approach%20integrates%20foundation%20models%20into%20existing%20SLAM%20pipelines%2C%20yielding%20scalable%20and%20robust%20multi-robot%20mapping.%20Our%20contributions%20include%3A%20%281%29%20integrating%203D%20foundation%20models%20to%20reliably%20estimate%20relative%20poses%20from%20monocular%20image%20pairs%20within%20decentralized%20C-SLAM%3B%20%282%29%20introducing%20robust%20outlier%20mitigation%20techniques%20critical%20to%20the%20use%20of%20these%20relative%20poses%3B%20and%20%283%29%20developing%20specialized%20pose%20graph%20optimization%20formulations%20that%20efficiently%20resolve%20scale%20ambiguities.%20We%20evaluate%20our%20method%20against%20state-of-the-art%20approaches%2C%20demonstrating%20improvements%20in%20localization%20and%20mapping%20accuracy%2C%20alongside%20significant%20gains%20in%20computational%20and%20memory%20efficiency.%20These%20results%20highlight%20the%20potential%20of%20our%20approach%20for%20deployment%20in%20large-scale%20multi-robot%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2602.02430v1&entry.124074799=Read"},
{"title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation", "author": "Xinshun Wang and Peiming Li and Ziyi Wang and Zhongbin Fang and Zhichao Deng and Songtao Wu and Jason Li and Mengyuan Liu", "abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception'' models that understand motion from video but only output text, and ``generation'' models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons.", "link": "http://arxiv.org/abs/2602.02401v1", "date": "2026-02-02", "relevancy": 2.9787, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6397}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5738}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Superman%3A%20Unifying%20Skeleton%20and%20Vision%20for%20Human%20Motion%20Perception%20and%20Generation&body=Title%3A%20Superman%3A%20Unifying%20Skeleton%20and%20Vision%20for%20Human%20Motion%20Perception%20and%20Generation%0AAuthor%3A%20Xinshun%20Wang%20and%20Peiming%20Li%20and%20Ziyi%20Wang%20and%20Zhongbin%20Fang%20and%20Zhichao%20Deng%20and%20Songtao%20Wu%20and%20Jason%20Li%20and%20Mengyuan%20Liu%0AAbstract%3A%20Human%20motion%20analysis%20tasks%2C%20such%20as%20temporal%203D%20pose%20estimation%2C%20motion%20prediction%2C%20and%20motion%20in-betweening%2C%20play%20an%20essential%20role%20in%20computer%20vision.%20However%2C%20current%20paradigms%20suffer%20from%20severe%20fragmentation.%20First%2C%20the%20field%20is%20split%20between%20%60%60perception%27%27%20models%20that%20understand%20motion%20from%20video%20but%20only%20output%20text%2C%20and%20%60%60generation%27%27%20models%20that%20cannot%20perceive%20from%20raw%20visual%20input.%20Second%2C%20generative%20MLLMs%20are%20often%20limited%20to%20single-frame%2C%20static%20poses%20using%20dense%2C%20parametric%20SMPL%20models%2C%20failing%20to%20handle%20temporal%20motion.%20Third%2C%20existing%20motion%20vocabularies%20are%20built%20from%20skeleton%20data%20alone%2C%20severing%20the%20link%20to%20the%20visual%20domain.%20To%20address%20these%20challenges%2C%20we%20introduce%20Superman%2C%20a%20unified%20framework%20that%20bridges%20visual%20perception%20with%20temporal%2C%20skeleton-based%20motion%20generation.%20Our%20solution%20is%20twofold.%20First%2C%20to%20overcome%20the%20modality%20disconnect%2C%20we%20propose%20a%20Vision-Guided%20Motion%20Tokenizer.%20Leveraging%20the%20natural%20geometric%20alignment%20between%203D%20skeletons%20and%20visual%20data%2C%20this%20module%20pioneers%20robust%20joint%20learning%20from%20both%20modalities%2C%20creating%20a%20unified%2C%20cross-modal%20motion%20vocabulary.%20Second%2C%20grounded%20in%20this%20motion%20language%2C%20a%20single%2C%20unified%20MLLM%20architecture%20is%20trained%20to%20handle%20all%20tasks.%20This%20module%20flexibly%20processes%20diverse%2C%20temporal%20inputs%2C%20unifying%203D%20skeleton%20pose%20estimation%20from%20video%20%28perception%29%20with%20skeleton-based%20motion%20prediction%20and%20in-betweening%20%28generation%29.%20Extensive%20experiments%20on%20standard%20benchmarks%2C%20including%20Human3.6M%2C%20demonstrate%20that%20our%20unified%20method%20achieves%20state-of-the-art%20or%20competitive%20performance%20across%20all%20motion%20tasks.%20This%20showcases%20a%20more%20efficient%20and%20scalable%20path%20for%20generative%20motion%20analysis%20using%20skeletons.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperman%253A%2520Unifying%2520Skeleton%2520and%2520Vision%2520for%2520Human%2520Motion%2520Perception%2520and%2520Generation%26entry.906535625%3DXinshun%2520Wang%2520and%2520Peiming%2520Li%2520and%2520Ziyi%2520Wang%2520and%2520Zhongbin%2520Fang%2520and%2520Zhichao%2520Deng%2520and%2520Songtao%2520Wu%2520and%2520Jason%2520Li%2520and%2520Mengyuan%2520Liu%26entry.1292438233%3DHuman%2520motion%2520analysis%2520tasks%252C%2520such%2520as%2520temporal%25203D%2520pose%2520estimation%252C%2520motion%2520prediction%252C%2520and%2520motion%2520in-betweening%252C%2520play%2520an%2520essential%2520role%2520in%2520computer%2520vision.%2520However%252C%2520current%2520paradigms%2520suffer%2520from%2520severe%2520fragmentation.%2520First%252C%2520the%2520field%2520is%2520split%2520between%2520%2560%2560perception%2527%2527%2520models%2520that%2520understand%2520motion%2520from%2520video%2520but%2520only%2520output%2520text%252C%2520and%2520%2560%2560generation%2527%2527%2520models%2520that%2520cannot%2520perceive%2520from%2520raw%2520visual%2520input.%2520Second%252C%2520generative%2520MLLMs%2520are%2520often%2520limited%2520to%2520single-frame%252C%2520static%2520poses%2520using%2520dense%252C%2520parametric%2520SMPL%2520models%252C%2520failing%2520to%2520handle%2520temporal%2520motion.%2520Third%252C%2520existing%2520motion%2520vocabularies%2520are%2520built%2520from%2520skeleton%2520data%2520alone%252C%2520severing%2520the%2520link%2520to%2520the%2520visual%2520domain.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Superman%252C%2520a%2520unified%2520framework%2520that%2520bridges%2520visual%2520perception%2520with%2520temporal%252C%2520skeleton-based%2520motion%2520generation.%2520Our%2520solution%2520is%2520twofold.%2520First%252C%2520to%2520overcome%2520the%2520modality%2520disconnect%252C%2520we%2520propose%2520a%2520Vision-Guided%2520Motion%2520Tokenizer.%2520Leveraging%2520the%2520natural%2520geometric%2520alignment%2520between%25203D%2520skeletons%2520and%2520visual%2520data%252C%2520this%2520module%2520pioneers%2520robust%2520joint%2520learning%2520from%2520both%2520modalities%252C%2520creating%2520a%2520unified%252C%2520cross-modal%2520motion%2520vocabulary.%2520Second%252C%2520grounded%2520in%2520this%2520motion%2520language%252C%2520a%2520single%252C%2520unified%2520MLLM%2520architecture%2520is%2520trained%2520to%2520handle%2520all%2520tasks.%2520This%2520module%2520flexibly%2520processes%2520diverse%252C%2520temporal%2520inputs%252C%2520unifying%25203D%2520skeleton%2520pose%2520estimation%2520from%2520video%2520%2528perception%2529%2520with%2520skeleton-based%2520motion%2520prediction%2520and%2520in-betweening%2520%2528generation%2529.%2520Extensive%2520experiments%2520on%2520standard%2520benchmarks%252C%2520including%2520Human3.6M%252C%2520demonstrate%2520that%2520our%2520unified%2520method%2520achieves%2520state-of-the-art%2520or%2520competitive%2520performance%2520across%2520all%2520motion%2520tasks.%2520This%2520showcases%2520a%2520more%2520efficient%2520and%2520scalable%2520path%2520for%2520generative%2520motion%2520analysis%2520using%2520skeletons.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Superman%3A%20Unifying%20Skeleton%20and%20Vision%20for%20Human%20Motion%20Perception%20and%20Generation&entry.906535625=Xinshun%20Wang%20and%20Peiming%20Li%20and%20Ziyi%20Wang%20and%20Zhongbin%20Fang%20and%20Zhichao%20Deng%20and%20Songtao%20Wu%20and%20Jason%20Li%20and%20Mengyuan%20Liu&entry.1292438233=Human%20motion%20analysis%20tasks%2C%20such%20as%20temporal%203D%20pose%20estimation%2C%20motion%20prediction%2C%20and%20motion%20in-betweening%2C%20play%20an%20essential%20role%20in%20computer%20vision.%20However%2C%20current%20paradigms%20suffer%20from%20severe%20fragmentation.%20First%2C%20the%20field%20is%20split%20between%20%60%60perception%27%27%20models%20that%20understand%20motion%20from%20video%20but%20only%20output%20text%2C%20and%20%60%60generation%27%27%20models%20that%20cannot%20perceive%20from%20raw%20visual%20input.%20Second%2C%20generative%20MLLMs%20are%20often%20limited%20to%20single-frame%2C%20static%20poses%20using%20dense%2C%20parametric%20SMPL%20models%2C%20failing%20to%20handle%20temporal%20motion.%20Third%2C%20existing%20motion%20vocabularies%20are%20built%20from%20skeleton%20data%20alone%2C%20severing%20the%20link%20to%20the%20visual%20domain.%20To%20address%20these%20challenges%2C%20we%20introduce%20Superman%2C%20a%20unified%20framework%20that%20bridges%20visual%20perception%20with%20temporal%2C%20skeleton-based%20motion%20generation.%20Our%20solution%20is%20twofold.%20First%2C%20to%20overcome%20the%20modality%20disconnect%2C%20we%20propose%20a%20Vision-Guided%20Motion%20Tokenizer.%20Leveraging%20the%20natural%20geometric%20alignment%20between%203D%20skeletons%20and%20visual%20data%2C%20this%20module%20pioneers%20robust%20joint%20learning%20from%20both%20modalities%2C%20creating%20a%20unified%2C%20cross-modal%20motion%20vocabulary.%20Second%2C%20grounded%20in%20this%20motion%20language%2C%20a%20single%2C%20unified%20MLLM%20architecture%20is%20trained%20to%20handle%20all%20tasks.%20This%20module%20flexibly%20processes%20diverse%2C%20temporal%20inputs%2C%20unifying%203D%20skeleton%20pose%20estimation%20from%20video%20%28perception%29%20with%20skeleton-based%20motion%20prediction%20and%20in-betweening%20%28generation%29.%20Extensive%20experiments%20on%20standard%20benchmarks%2C%20including%20Human3.6M%2C%20demonstrate%20that%20our%20unified%20method%20achieves%20state-of-the-art%20or%20competitive%20performance%20across%20all%20motion%20tasks.%20This%20showcases%20a%20more%20efficient%20and%20scalable%20path%20for%20generative%20motion%20analysis%20using%20skeletons.&entry.1838667208=http%3A//arxiv.org/abs/2602.02401v1&entry.124074799=Read"},
{"title": "Beyond Global Alignment: Fine-Grained Motion-Language Retrieval via Pyramidal Shapley-Taylor Learning", "author": "Hanmo Chen and Guangtao Lyu and Chenghao Xu and Jiexi Yan and Xu Yang and Cheng Deng", "abstract": "As a foundational task in human-centric cross-modal intelligence, motion-language retrieval aims to bridge the semantic gap between natural language and human motion, enabling intuitive motion analysis, yet existing approaches predominantly focus on aligning entire motion sequences with global textual representations. This global-centric paradigm overlooks fine-grained interactions between local motion segments and individual body joints and text tokens, inevitably leading to suboptimal retrieval performance. To address this limitation, we draw inspiration from the pyramidal process of human motion perception (from joint dynamics to segment coherence, and finally to holistic comprehension) and propose a novel Pyramidal Shapley-Taylor (PST) learning framework for fine-grained motion-language retrieval. Specifically, the framework decomposes human motion into temporal segments and spatial body joints, and learns cross-modal correspondences through progressive joint-wise and segment-wise alignment in a pyramidal fashion, effectively capturing both local semantic details and hierarchical structural relationships. Extensive experiments on multiple public benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods, achieving precise alignment between motion segments and body joints and their corresponding text tokens. The code of this work will be released upon acceptance.", "link": "http://arxiv.org/abs/2601.21904v2", "date": "2026-02-02", "relevancy": 2.9123, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5853}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5817}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Global%20Alignment%3A%20Fine-Grained%20Motion-Language%20Retrieval%20via%20Pyramidal%20Shapley-Taylor%20Learning&body=Title%3A%20Beyond%20Global%20Alignment%3A%20Fine-Grained%20Motion-Language%20Retrieval%20via%20Pyramidal%20Shapley-Taylor%20Learning%0AAuthor%3A%20Hanmo%20Chen%20and%20Guangtao%20Lyu%20and%20Chenghao%20Xu%20and%20Jiexi%20Yan%20and%20Xu%20Yang%20and%20Cheng%20Deng%0AAbstract%3A%20As%20a%20foundational%20task%20in%20human-centric%20cross-modal%20intelligence%2C%20motion-language%20retrieval%20aims%20to%20bridge%20the%20semantic%20gap%20between%20natural%20language%20and%20human%20motion%2C%20enabling%20intuitive%20motion%20analysis%2C%20yet%20existing%20approaches%20predominantly%20focus%20on%20aligning%20entire%20motion%20sequences%20with%20global%20textual%20representations.%20This%20global-centric%20paradigm%20overlooks%20fine-grained%20interactions%20between%20local%20motion%20segments%20and%20individual%20body%20joints%20and%20text%20tokens%2C%20inevitably%20leading%20to%20suboptimal%20retrieval%20performance.%20To%20address%20this%20limitation%2C%20we%20draw%20inspiration%20from%20the%20pyramidal%20process%20of%20human%20motion%20perception%20%28from%20joint%20dynamics%20to%20segment%20coherence%2C%20and%20finally%20to%20holistic%20comprehension%29%20and%20propose%20a%20novel%20Pyramidal%20Shapley-Taylor%20%28PST%29%20learning%20framework%20for%20fine-grained%20motion-language%20retrieval.%20Specifically%2C%20the%20framework%20decomposes%20human%20motion%20into%20temporal%20segments%20and%20spatial%20body%20joints%2C%20and%20learns%20cross-modal%20correspondences%20through%20progressive%20joint-wise%20and%20segment-wise%20alignment%20in%20a%20pyramidal%20fashion%2C%20effectively%20capturing%20both%20local%20semantic%20details%20and%20hierarchical%20structural%20relationships.%20Extensive%20experiments%20on%20multiple%20public%20benchmark%20datasets%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20state-of-the-art%20methods%2C%20achieving%20precise%20alignment%20between%20motion%20segments%20and%20body%20joints%20and%20their%20corresponding%20text%20tokens.%20The%20code%20of%20this%20work%20will%20be%20released%20upon%20acceptance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21904v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Global%2520Alignment%253A%2520Fine-Grained%2520Motion-Language%2520Retrieval%2520via%2520Pyramidal%2520Shapley-Taylor%2520Learning%26entry.906535625%3DHanmo%2520Chen%2520and%2520Guangtao%2520Lyu%2520and%2520Chenghao%2520Xu%2520and%2520Jiexi%2520Yan%2520and%2520Xu%2520Yang%2520and%2520Cheng%2520Deng%26entry.1292438233%3DAs%2520a%2520foundational%2520task%2520in%2520human-centric%2520cross-modal%2520intelligence%252C%2520motion-language%2520retrieval%2520aims%2520to%2520bridge%2520the%2520semantic%2520gap%2520between%2520natural%2520language%2520and%2520human%2520motion%252C%2520enabling%2520intuitive%2520motion%2520analysis%252C%2520yet%2520existing%2520approaches%2520predominantly%2520focus%2520on%2520aligning%2520entire%2520motion%2520sequences%2520with%2520global%2520textual%2520representations.%2520This%2520global-centric%2520paradigm%2520overlooks%2520fine-grained%2520interactions%2520between%2520local%2520motion%2520segments%2520and%2520individual%2520body%2520joints%2520and%2520text%2520tokens%252C%2520inevitably%2520leading%2520to%2520suboptimal%2520retrieval%2520performance.%2520To%2520address%2520this%2520limitation%252C%2520we%2520draw%2520inspiration%2520from%2520the%2520pyramidal%2520process%2520of%2520human%2520motion%2520perception%2520%2528from%2520joint%2520dynamics%2520to%2520segment%2520coherence%252C%2520and%2520finally%2520to%2520holistic%2520comprehension%2529%2520and%2520propose%2520a%2520novel%2520Pyramidal%2520Shapley-Taylor%2520%2528PST%2529%2520learning%2520framework%2520for%2520fine-grained%2520motion-language%2520retrieval.%2520Specifically%252C%2520the%2520framework%2520decomposes%2520human%2520motion%2520into%2520temporal%2520segments%2520and%2520spatial%2520body%2520joints%252C%2520and%2520learns%2520cross-modal%2520correspondences%2520through%2520progressive%2520joint-wise%2520and%2520segment-wise%2520alignment%2520in%2520a%2520pyramidal%2520fashion%252C%2520effectively%2520capturing%2520both%2520local%2520semantic%2520details%2520and%2520hierarchical%2520structural%2520relationships.%2520Extensive%2520experiments%2520on%2520multiple%2520public%2520benchmark%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520outperforms%2520state-of-the-art%2520methods%252C%2520achieving%2520precise%2520alignment%2520between%2520motion%2520segments%2520and%2520body%2520joints%2520and%2520their%2520corresponding%2520text%2520tokens.%2520The%2520code%2520of%2520this%2520work%2520will%2520be%2520released%2520upon%2520acceptance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21904v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Global%20Alignment%3A%20Fine-Grained%20Motion-Language%20Retrieval%20via%20Pyramidal%20Shapley-Taylor%20Learning&entry.906535625=Hanmo%20Chen%20and%20Guangtao%20Lyu%20and%20Chenghao%20Xu%20and%20Jiexi%20Yan%20and%20Xu%20Yang%20and%20Cheng%20Deng&entry.1292438233=As%20a%20foundational%20task%20in%20human-centric%20cross-modal%20intelligence%2C%20motion-language%20retrieval%20aims%20to%20bridge%20the%20semantic%20gap%20between%20natural%20language%20and%20human%20motion%2C%20enabling%20intuitive%20motion%20analysis%2C%20yet%20existing%20approaches%20predominantly%20focus%20on%20aligning%20entire%20motion%20sequences%20with%20global%20textual%20representations.%20This%20global-centric%20paradigm%20overlooks%20fine-grained%20interactions%20between%20local%20motion%20segments%20and%20individual%20body%20joints%20and%20text%20tokens%2C%20inevitably%20leading%20to%20suboptimal%20retrieval%20performance.%20To%20address%20this%20limitation%2C%20we%20draw%20inspiration%20from%20the%20pyramidal%20process%20of%20human%20motion%20perception%20%28from%20joint%20dynamics%20to%20segment%20coherence%2C%20and%20finally%20to%20holistic%20comprehension%29%20and%20propose%20a%20novel%20Pyramidal%20Shapley-Taylor%20%28PST%29%20learning%20framework%20for%20fine-grained%20motion-language%20retrieval.%20Specifically%2C%20the%20framework%20decomposes%20human%20motion%20into%20temporal%20segments%20and%20spatial%20body%20joints%2C%20and%20learns%20cross-modal%20correspondences%20through%20progressive%20joint-wise%20and%20segment-wise%20alignment%20in%20a%20pyramidal%20fashion%2C%20effectively%20capturing%20both%20local%20semantic%20details%20and%20hierarchical%20structural%20relationships.%20Extensive%20experiments%20on%20multiple%20public%20benchmark%20datasets%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20state-of-the-art%20methods%2C%20achieving%20precise%20alignment%20between%20motion%20segments%20and%20body%20joints%20and%20their%20corresponding%20text%20tokens.%20The%20code%20of%20this%20work%20will%20be%20released%20upon%20acceptance.&entry.1838667208=http%3A//arxiv.org/abs/2601.21904v2&entry.124074799=Read"},
{"title": "Enhancing Multi-Image Understanding through Delimiter Token Scaling", "author": "Minyoung Lee and Yeji Park and Dongjun Hwang and Yejin Kim and Seong Joon Oh and Junsuk Choe", "abstract": "Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.", "link": "http://arxiv.org/abs/2602.01984v1", "date": "2026-02-02", "relevancy": 2.9022, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5839}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5787}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Multi-Image%20Understanding%20through%20Delimiter%20Token%20Scaling&body=Title%3A%20Enhancing%20Multi-Image%20Understanding%20through%20Delimiter%20Token%20Scaling%0AAuthor%3A%20Minyoung%20Lee%20and%20Yeji%20Park%20and%20Dongjun%20Hwang%20and%20Yejin%20Kim%20and%20Seong%20Joon%20Oh%20and%20Junsuk%20Choe%0AAbstract%3A%20Large%20Vision-Language%20Models%20%28LVLMs%29%20achieve%20strong%20performance%20on%20single-image%20tasks%2C%20but%20their%20performance%20declines%20when%20multiple%20images%20are%20provided%20as%20input.%20One%20major%20reason%20is%20the%20cross-image%20information%20leakage%2C%20where%20the%20model%20struggles%20to%20distinguish%20information%20across%20different%20images.%20Existing%20LVLMs%20already%20employ%20delimiter%20tokens%20to%20mark%20the%20start%20and%20end%20of%20each%20image%2C%20yet%20our%20analysis%20reveals%20that%20these%20tokens%20fail%20to%20effectively%20block%20cross-image%20information%20leakage.%20To%20enhance%20their%20effectiveness%2C%20we%20propose%20a%20method%20that%20scales%20the%20hidden%20states%20of%20delimiter%20tokens.%20This%20enhances%20the%20model%27s%20ability%20to%20preserve%20image-specific%20information%20by%20reinforcing%20intra-image%20interaction%20and%20limiting%20undesired%20cross-image%20interactions.%20Consequently%2C%20the%20model%20is%20better%20able%20to%20distinguish%20between%20images%20and%20reason%20over%20them%20more%20accurately.%20Experiments%20show%20performance%20gains%20on%20multi-image%20benchmarks%20such%20as%20Mantis%2C%20MuirBench%2C%20MIRB%2C%20and%20QBench2.%20We%20further%20evaluate%20our%20method%20on%20text-only%20tasks%20that%20require%20clear%20distinction.%20The%20method%20improves%20performance%20on%20multi-document%20and%20multi-table%20understanding%20benchmarks%2C%20including%20TQABench%2C%20MultiNews%2C%20and%20WCEP-10.%20Notably%2C%20our%20method%20requires%20no%20additional%20training%20or%20inference%20cost.%0ALink%3A%20http%3A//arxiv.org/abs/2602.01984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Multi-Image%2520Understanding%2520through%2520Delimiter%2520Token%2520Scaling%26entry.906535625%3DMinyoung%2520Lee%2520and%2520Yeji%2520Park%2520and%2520Dongjun%2520Hwang%2520and%2520Yejin%2520Kim%2520and%2520Seong%2520Joon%2520Oh%2520and%2520Junsuk%2520Choe%26entry.1292438233%3DLarge%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520achieve%2520strong%2520performance%2520on%2520single-image%2520tasks%252C%2520but%2520their%2520performance%2520declines%2520when%2520multiple%2520images%2520are%2520provided%2520as%2520input.%2520One%2520major%2520reason%2520is%2520the%2520cross-image%2520information%2520leakage%252C%2520where%2520the%2520model%2520struggles%2520to%2520distinguish%2520information%2520across%2520different%2520images.%2520Existing%2520LVLMs%2520already%2520employ%2520delimiter%2520tokens%2520to%2520mark%2520the%2520start%2520and%2520end%2520of%2520each%2520image%252C%2520yet%2520our%2520analysis%2520reveals%2520that%2520these%2520tokens%2520fail%2520to%2520effectively%2520block%2520cross-image%2520information%2520leakage.%2520To%2520enhance%2520their%2520effectiveness%252C%2520we%2520propose%2520a%2520method%2520that%2520scales%2520the%2520hidden%2520states%2520of%2520delimiter%2520tokens.%2520This%2520enhances%2520the%2520model%2527s%2520ability%2520to%2520preserve%2520image-specific%2520information%2520by%2520reinforcing%2520intra-image%2520interaction%2520and%2520limiting%2520undesired%2520cross-image%2520interactions.%2520Consequently%252C%2520the%2520model%2520is%2520better%2520able%2520to%2520distinguish%2520between%2520images%2520and%2520reason%2520over%2520them%2520more%2520accurately.%2520Experiments%2520show%2520performance%2520gains%2520on%2520multi-image%2520benchmarks%2520such%2520as%2520Mantis%252C%2520MuirBench%252C%2520MIRB%252C%2520and%2520QBench2.%2520We%2520further%2520evaluate%2520our%2520method%2520on%2520text-only%2520tasks%2520that%2520require%2520clear%2520distinction.%2520The%2520method%2520improves%2520performance%2520on%2520multi-document%2520and%2520multi-table%2520understanding%2520benchmarks%252C%2520including%2520TQABench%252C%2520MultiNews%252C%2520and%2520WCEP-10.%2520Notably%252C%2520our%2520method%2520requires%2520no%2520additional%2520training%2520or%2520inference%2520cost.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.01984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Multi-Image%20Understanding%20through%20Delimiter%20Token%20Scaling&entry.906535625=Minyoung%20Lee%20and%20Yeji%20Park%20and%20Dongjun%20Hwang%20and%20Yejin%20Kim%20and%20Seong%20Joon%20Oh%20and%20Junsuk%20Choe&entry.1292438233=Large%20Vision-Language%20Models%20%28LVLMs%29%20achieve%20strong%20performance%20on%20single-image%20tasks%2C%20but%20their%20performance%20declines%20when%20multiple%20images%20are%20provided%20as%20input.%20One%20major%20reason%20is%20the%20cross-image%20information%20leakage%2C%20where%20the%20model%20struggles%20to%20distinguish%20information%20across%20different%20images.%20Existing%20LVLMs%20already%20employ%20delimiter%20tokens%20to%20mark%20the%20start%20and%20end%20of%20each%20image%2C%20yet%20our%20analysis%20reveals%20that%20these%20tokens%20fail%20to%20effectively%20block%20cross-image%20information%20leakage.%20To%20enhance%20their%20effectiveness%2C%20we%20propose%20a%20method%20that%20scales%20the%20hidden%20states%20of%20delimiter%20tokens.%20This%20enhances%20the%20model%27s%20ability%20to%20preserve%20image-specific%20information%20by%20reinforcing%20intra-image%20interaction%20and%20limiting%20undesired%20cross-image%20interactions.%20Consequently%2C%20the%20model%20is%20better%20able%20to%20distinguish%20between%20images%20and%20reason%20over%20them%20more%20accurately.%20Experiments%20show%20performance%20gains%20on%20multi-image%20benchmarks%20such%20as%20Mantis%2C%20MuirBench%2C%20MIRB%2C%20and%20QBench2.%20We%20further%20evaluate%20our%20method%20on%20text-only%20tasks%20that%20require%20clear%20distinction.%20The%20method%20improves%20performance%20on%20multi-document%20and%20multi-table%20understanding%20benchmarks%2C%20including%20TQABench%2C%20MultiNews%2C%20and%20WCEP-10.%20Notably%2C%20our%20method%20requires%20no%20additional%20training%20or%20inference%20cost.&entry.1838667208=http%3A//arxiv.org/abs/2602.01984v1&entry.124074799=Read"},
{"title": "Unified Personalized Reward Model for Vision Generation", "author": "Yibin Wang and Yuhang Zang and Feng Han and Jiazi Bu and Yujie Zhou and Cheng Jin and Jiaqi Wang", "abstract": "Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.", "link": "http://arxiv.org/abs/2602.02380v1", "date": "2026-02-02", "relevancy": 2.8915, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.582}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5773}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Personalized%20Reward%20Model%20for%20Vision%20Generation&body=Title%3A%20Unified%20Personalized%20Reward%20Model%20for%20Vision%20Generation%0AAuthor%3A%20Yibin%20Wang%20and%20Yuhang%20Zang%20and%20Feng%20Han%20and%20Jiazi%20Bu%20and%20Yujie%20Zhou%20and%20Cheng%20Jin%20and%20Jiaqi%20Wang%0AAbstract%3A%20Recent%20advancements%20in%20multimodal%20reward%20models%20%28RMs%29%20have%20significantly%20propelled%20the%20development%20of%20visual%20generation.%20Existing%20frameworks%20typically%20adopt%20Bradley-Terry-style%20preference%20modeling%20or%20leverage%20generative%20VLMs%20as%20judges%2C%20and%20subsequently%20optimize%20visual%20generation%20models%20via%20reinforcement%20learning.%20However%2C%20current%20RMs%20suffer%20from%20inherent%20limitations%3A%20they%20often%20follow%20a%20one-size-fits-all%20paradigm%20that%20assumes%20a%20monolithic%20preference%20distribution%20or%20relies%20on%20fixed%20evaluation%20rubrics.%20As%20a%20result%2C%20they%20are%20insensitive%20to%20content-specific%20visual%20cues%2C%20leading%20to%20systematic%20misalignment%20with%20subjective%20and%20context-dependent%20human%20preferences.%20To%20this%20end%2C%20inspired%20by%20human%20assessment%2C%20we%20propose%20UnifiedReward-Flex%2C%20a%20unified%20personalized%20reward%20model%20for%20vision%20generation%20that%20couples%20reward%20modeling%20with%20flexible%20and%20context-adaptive%20reasoning.%20Specifically%2C%20given%20a%20prompt%20and%20the%20generated%20visual%20content%2C%20it%20first%20interprets%20the%20semantic%20intent%20and%20grounds%20on%20visual%20evidence%2C%20then%20dynamically%20constructs%20a%20hierarchical%20assessment%20by%20instantiating%20fine-grained%20criteria%20under%20both%20predefined%20and%20self-generated%20high-level%20dimensions.%20Our%20training%20pipeline%20follows%20a%20two-stage%20process%3A%20%281%29%20we%20first%20distill%20structured%2C%20high-quality%20reasoning%20traces%20from%20advanced%20closed-source%20VLMs%20to%20bootstrap%20SFT%2C%20equipping%20the%20model%20with%20flexible%20and%20context-adaptive%20reasoning%20behaviors%3B%20%282%29%20we%20then%20perform%20direct%20preference%20optimization%20%28DPO%29%20on%20carefully%20curated%20preference%20pairs%20to%20further%20strengthen%20reasoning%20fidelity%20and%20discriminative%20alignment.%20To%20validate%20the%20effectiveness%2C%20we%20integrate%20UnifiedReward-Flex%20into%20the%20GRPO%20framework%20for%20image%20and%20video%20synthesis%2C%20and%20extensive%20results%20demonstrate%20its%20superiority.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Personalized%2520Reward%2520Model%2520for%2520Vision%2520Generation%26entry.906535625%3DYibin%2520Wang%2520and%2520Yuhang%2520Zang%2520and%2520Feng%2520Han%2520and%2520Jiazi%2520Bu%2520and%2520Yujie%2520Zhou%2520and%2520Cheng%2520Jin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3DRecent%2520advancements%2520in%2520multimodal%2520reward%2520models%2520%2528RMs%2529%2520have%2520significantly%2520propelled%2520the%2520development%2520of%2520visual%2520generation.%2520Existing%2520frameworks%2520typically%2520adopt%2520Bradley-Terry-style%2520preference%2520modeling%2520or%2520leverage%2520generative%2520VLMs%2520as%2520judges%252C%2520and%2520subsequently%2520optimize%2520visual%2520generation%2520models%2520via%2520reinforcement%2520learning.%2520However%252C%2520current%2520RMs%2520suffer%2520from%2520inherent%2520limitations%253A%2520they%2520often%2520follow%2520a%2520one-size-fits-all%2520paradigm%2520that%2520assumes%2520a%2520monolithic%2520preference%2520distribution%2520or%2520relies%2520on%2520fixed%2520evaluation%2520rubrics.%2520As%2520a%2520result%252C%2520they%2520are%2520insensitive%2520to%2520content-specific%2520visual%2520cues%252C%2520leading%2520to%2520systematic%2520misalignment%2520with%2520subjective%2520and%2520context-dependent%2520human%2520preferences.%2520To%2520this%2520end%252C%2520inspired%2520by%2520human%2520assessment%252C%2520we%2520propose%2520UnifiedReward-Flex%252C%2520a%2520unified%2520personalized%2520reward%2520model%2520for%2520vision%2520generation%2520that%2520couples%2520reward%2520modeling%2520with%2520flexible%2520and%2520context-adaptive%2520reasoning.%2520Specifically%252C%2520given%2520a%2520prompt%2520and%2520the%2520generated%2520visual%2520content%252C%2520it%2520first%2520interprets%2520the%2520semantic%2520intent%2520and%2520grounds%2520on%2520visual%2520evidence%252C%2520then%2520dynamically%2520constructs%2520a%2520hierarchical%2520assessment%2520by%2520instantiating%2520fine-grained%2520criteria%2520under%2520both%2520predefined%2520and%2520self-generated%2520high-level%2520dimensions.%2520Our%2520training%2520pipeline%2520follows%2520a%2520two-stage%2520process%253A%2520%25281%2529%2520we%2520first%2520distill%2520structured%252C%2520high-quality%2520reasoning%2520traces%2520from%2520advanced%2520closed-source%2520VLMs%2520to%2520bootstrap%2520SFT%252C%2520equipping%2520the%2520model%2520with%2520flexible%2520and%2520context-adaptive%2520reasoning%2520behaviors%253B%2520%25282%2529%2520we%2520then%2520perform%2520direct%2520preference%2520optimization%2520%2528DPO%2529%2520on%2520carefully%2520curated%2520preference%2520pairs%2520to%2520further%2520strengthen%2520reasoning%2520fidelity%2520and%2520discriminative%2520alignment.%2520To%2520validate%2520the%2520effectiveness%252C%2520we%2520integrate%2520UnifiedReward-Flex%2520into%2520the%2520GRPO%2520framework%2520for%2520image%2520and%2520video%2520synthesis%252C%2520and%2520extensive%2520results%2520demonstrate%2520its%2520superiority.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Personalized%20Reward%20Model%20for%20Vision%20Generation&entry.906535625=Yibin%20Wang%20and%20Yuhang%20Zang%20and%20Feng%20Han%20and%20Jiazi%20Bu%20and%20Yujie%20Zhou%20and%20Cheng%20Jin%20and%20Jiaqi%20Wang&entry.1292438233=Recent%20advancements%20in%20multimodal%20reward%20models%20%28RMs%29%20have%20significantly%20propelled%20the%20development%20of%20visual%20generation.%20Existing%20frameworks%20typically%20adopt%20Bradley-Terry-style%20preference%20modeling%20or%20leverage%20generative%20VLMs%20as%20judges%2C%20and%20subsequently%20optimize%20visual%20generation%20models%20via%20reinforcement%20learning.%20However%2C%20current%20RMs%20suffer%20from%20inherent%20limitations%3A%20they%20often%20follow%20a%20one-size-fits-all%20paradigm%20that%20assumes%20a%20monolithic%20preference%20distribution%20or%20relies%20on%20fixed%20evaluation%20rubrics.%20As%20a%20result%2C%20they%20are%20insensitive%20to%20content-specific%20visual%20cues%2C%20leading%20to%20systematic%20misalignment%20with%20subjective%20and%20context-dependent%20human%20preferences.%20To%20this%20end%2C%20inspired%20by%20human%20assessment%2C%20we%20propose%20UnifiedReward-Flex%2C%20a%20unified%20personalized%20reward%20model%20for%20vision%20generation%20that%20couples%20reward%20modeling%20with%20flexible%20and%20context-adaptive%20reasoning.%20Specifically%2C%20given%20a%20prompt%20and%20the%20generated%20visual%20content%2C%20it%20first%20interprets%20the%20semantic%20intent%20and%20grounds%20on%20visual%20evidence%2C%20then%20dynamically%20constructs%20a%20hierarchical%20assessment%20by%20instantiating%20fine-grained%20criteria%20under%20both%20predefined%20and%20self-generated%20high-level%20dimensions.%20Our%20training%20pipeline%20follows%20a%20two-stage%20process%3A%20%281%29%20we%20first%20distill%20structured%2C%20high-quality%20reasoning%20traces%20from%20advanced%20closed-source%20VLMs%20to%20bootstrap%20SFT%2C%20equipping%20the%20model%20with%20flexible%20and%20context-adaptive%20reasoning%20behaviors%3B%20%282%29%20we%20then%20perform%20direct%20preference%20optimization%20%28DPO%29%20on%20carefully%20curated%20preference%20pairs%20to%20further%20strengthen%20reasoning%20fidelity%20and%20discriminative%20alignment.%20To%20validate%20the%20effectiveness%2C%20we%20integrate%20UnifiedReward-Flex%20into%20the%20GRPO%20framework%20for%20image%20and%20video%20synthesis%2C%20and%20extensive%20results%20demonstrate%20its%20superiority.&entry.1838667208=http%3A//arxiv.org/abs/2602.02380v1&entry.124074799=Read"},
{"title": "CoT-RVS: Zero-Shot Chain-of-Thought Reasoning Segmentation for Videos", "author": "Shiu-hong Kao and Yu-Wing Tai and Chi-Keung Tang", "abstract": "Reasoning Video Object Segmentation is a challenging task, aiming at generating a mask sequence from an input video given a complex and implicit text query. While existing works finetune Multimodal Large Language Models (MLLM) for the task, they still fail in video inputs given complex temporally-sensitive queries, indicating their lack of temporal and spatial integration in complex scenarios. In this paper, we propose CoT-RVS, a novel framework employing the zero-shot Chain-of-Thought (CoT) capability of MLLM to address these complex challenges by temporal-semantic reasoning: CoT-RVS analyzes the visible objects within a given frame that possibly match the language query (semantic), and chooses a corresponding keyframe for each object that can be observed effortlessly among all frames (temporal). Notably, the CoT-RVS framework is training-free and compatible with closed-source MLLMs, which can be applied to Reasoning Video Instance Segmentation. Our framework's training-free feature further allows its extension to process online video streams, where the CoT is used at test time to update the object of interest when a better target starts to emerge and becomes visible. We conduct extensive experiments on video object segmentation with explicit and implicit queries. The results show that CoT-RVS significantly outperforms previous works in both cases, qualitatively and quantitatively.", "link": "http://arxiv.org/abs/2505.18561v4", "date": "2026-02-02", "relevancy": 2.8488, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5703}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5703}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoT-RVS%3A%20Zero-Shot%20Chain-of-Thought%20Reasoning%20Segmentation%20for%20Videos&body=Title%3A%20CoT-RVS%3A%20Zero-Shot%20Chain-of-Thought%20Reasoning%20Segmentation%20for%20Videos%0AAuthor%3A%20Shiu-hong%20Kao%20and%20Yu-Wing%20Tai%20and%20Chi-Keung%20Tang%0AAbstract%3A%20Reasoning%20Video%20Object%20Segmentation%20is%20a%20challenging%20task%2C%20aiming%20at%20generating%20a%20mask%20sequence%20from%20an%20input%20video%20given%20a%20complex%20and%20implicit%20text%20query.%20While%20existing%20works%20finetune%20Multimodal%20Large%20Language%20Models%20%28MLLM%29%20for%20the%20task%2C%20they%20still%20fail%20in%20video%20inputs%20given%20complex%20temporally-sensitive%20queries%2C%20indicating%20their%20lack%20of%20temporal%20and%20spatial%20integration%20in%20complex%20scenarios.%20In%20this%20paper%2C%20we%20propose%20CoT-RVS%2C%20a%20novel%20framework%20employing%20the%20zero-shot%20Chain-of-Thought%20%28CoT%29%20capability%20of%20MLLM%20to%20address%20these%20complex%20challenges%20by%20temporal-semantic%20reasoning%3A%20CoT-RVS%20analyzes%20the%20visible%20objects%20within%20a%20given%20frame%20that%20possibly%20match%20the%20language%20query%20%28semantic%29%2C%20and%20chooses%20a%20corresponding%20keyframe%20for%20each%20object%20that%20can%20be%20observed%20effortlessly%20among%20all%20frames%20%28temporal%29.%20Notably%2C%20the%20CoT-RVS%20framework%20is%20training-free%20and%20compatible%20with%20closed-source%20MLLMs%2C%20which%20can%20be%20applied%20to%20Reasoning%20Video%20Instance%20Segmentation.%20Our%20framework%27s%20training-free%20feature%20further%20allows%20its%20extension%20to%20process%20online%20video%20streams%2C%20where%20the%20CoT%20is%20used%20at%20test%20time%20to%20update%20the%20object%20of%20interest%20when%20a%20better%20target%20starts%20to%20emerge%20and%20becomes%20visible.%20We%20conduct%20extensive%20experiments%20on%20video%20object%20segmentation%20with%20explicit%20and%20implicit%20queries.%20The%20results%20show%20that%20CoT-RVS%20significantly%20outperforms%20previous%20works%20in%20both%20cases%2C%20qualitatively%20and%20quantitatively.%0ALink%3A%20http%3A//arxiv.org/abs/2505.18561v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoT-RVS%253A%2520Zero-Shot%2520Chain-of-Thought%2520Reasoning%2520Segmentation%2520for%2520Videos%26entry.906535625%3DShiu-hong%2520Kao%2520and%2520Yu-Wing%2520Tai%2520and%2520Chi-Keung%2520Tang%26entry.1292438233%3DReasoning%2520Video%2520Object%2520Segmentation%2520is%2520a%2520challenging%2520task%252C%2520aiming%2520at%2520generating%2520a%2520mask%2520sequence%2520from%2520an%2520input%2520video%2520given%2520a%2520complex%2520and%2520implicit%2520text%2520query.%2520While%2520existing%2520works%2520finetune%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLM%2529%2520for%2520the%2520task%252C%2520they%2520still%2520fail%2520in%2520video%2520inputs%2520given%2520complex%2520temporally-sensitive%2520queries%252C%2520indicating%2520their%2520lack%2520of%2520temporal%2520and%2520spatial%2520integration%2520in%2520complex%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520propose%2520CoT-RVS%252C%2520a%2520novel%2520framework%2520employing%2520the%2520zero-shot%2520Chain-of-Thought%2520%2528CoT%2529%2520capability%2520of%2520MLLM%2520to%2520address%2520these%2520complex%2520challenges%2520by%2520temporal-semantic%2520reasoning%253A%2520CoT-RVS%2520analyzes%2520the%2520visible%2520objects%2520within%2520a%2520given%2520frame%2520that%2520possibly%2520match%2520the%2520language%2520query%2520%2528semantic%2529%252C%2520and%2520chooses%2520a%2520corresponding%2520keyframe%2520for%2520each%2520object%2520that%2520can%2520be%2520observed%2520effortlessly%2520among%2520all%2520frames%2520%2528temporal%2529.%2520Notably%252C%2520the%2520CoT-RVS%2520framework%2520is%2520training-free%2520and%2520compatible%2520with%2520closed-source%2520MLLMs%252C%2520which%2520can%2520be%2520applied%2520to%2520Reasoning%2520Video%2520Instance%2520Segmentation.%2520Our%2520framework%2527s%2520training-free%2520feature%2520further%2520allows%2520its%2520extension%2520to%2520process%2520online%2520video%2520streams%252C%2520where%2520the%2520CoT%2520is%2520used%2520at%2520test%2520time%2520to%2520update%2520the%2520object%2520of%2520interest%2520when%2520a%2520better%2520target%2520starts%2520to%2520emerge%2520and%2520becomes%2520visible.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520video%2520object%2520segmentation%2520with%2520explicit%2520and%2520implicit%2520queries.%2520The%2520results%2520show%2520that%2520CoT-RVS%2520significantly%2520outperforms%2520previous%2520works%2520in%2520both%2520cases%252C%2520qualitatively%2520and%2520quantitatively.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18561v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoT-RVS%3A%20Zero-Shot%20Chain-of-Thought%20Reasoning%20Segmentation%20for%20Videos&entry.906535625=Shiu-hong%20Kao%20and%20Yu-Wing%20Tai%20and%20Chi-Keung%20Tang&entry.1292438233=Reasoning%20Video%20Object%20Segmentation%20is%20a%20challenging%20task%2C%20aiming%20at%20generating%20a%20mask%20sequence%20from%20an%20input%20video%20given%20a%20complex%20and%20implicit%20text%20query.%20While%20existing%20works%20finetune%20Multimodal%20Large%20Language%20Models%20%28MLLM%29%20for%20the%20task%2C%20they%20still%20fail%20in%20video%20inputs%20given%20complex%20temporally-sensitive%20queries%2C%20indicating%20their%20lack%20of%20temporal%20and%20spatial%20integration%20in%20complex%20scenarios.%20In%20this%20paper%2C%20we%20propose%20CoT-RVS%2C%20a%20novel%20framework%20employing%20the%20zero-shot%20Chain-of-Thought%20%28CoT%29%20capability%20of%20MLLM%20to%20address%20these%20complex%20challenges%20by%20temporal-semantic%20reasoning%3A%20CoT-RVS%20analyzes%20the%20visible%20objects%20within%20a%20given%20frame%20that%20possibly%20match%20the%20language%20query%20%28semantic%29%2C%20and%20chooses%20a%20corresponding%20keyframe%20for%20each%20object%20that%20can%20be%20observed%20effortlessly%20among%20all%20frames%20%28temporal%29.%20Notably%2C%20the%20CoT-RVS%20framework%20is%20training-free%20and%20compatible%20with%20closed-source%20MLLMs%2C%20which%20can%20be%20applied%20to%20Reasoning%20Video%20Instance%20Segmentation.%20Our%20framework%27s%20training-free%20feature%20further%20allows%20its%20extension%20to%20process%20online%20video%20streams%2C%20where%20the%20CoT%20is%20used%20at%20test%20time%20to%20update%20the%20object%20of%20interest%20when%20a%20better%20target%20starts%20to%20emerge%20and%20becomes%20visible.%20We%20conduct%20extensive%20experiments%20on%20video%20object%20segmentation%20with%20explicit%20and%20implicit%20queries.%20The%20results%20show%20that%20CoT-RVS%20significantly%20outperforms%20previous%20works%20in%20both%20cases%2C%20qualitatively%20and%20quantitatively.&entry.1838667208=http%3A//arxiv.org/abs/2505.18561v4&entry.124074799=Read"},
{"title": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery", "author": "Jana Zeller and Thadd\u00e4us Wiedemer and Fanfei Li and Thomas Klein and Prasanna Mayilvahanan and Matthias Bethge and Felix Wichmann and Ryan Cotterell and Wieland Brendel", "abstract": "Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.", "link": "http://arxiv.org/abs/2602.02465v1", "date": "2026-02-02", "relevancy": 2.8364, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5816}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5816}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MentisOculi%3A%20Revealing%20the%20Limits%20of%20Reasoning%20with%20Mental%20Imagery&body=Title%3A%20MentisOculi%3A%20Revealing%20the%20Limits%20of%20Reasoning%20with%20Mental%20Imagery%0AAuthor%3A%20Jana%20Zeller%20and%20Thadd%C3%A4us%20Wiedemer%20and%20Fanfei%20Li%20and%20Thomas%20Klein%20and%20Prasanna%20Mayilvahanan%20and%20Matthias%20Bethge%20and%20Felix%20Wichmann%20and%20Ryan%20Cotterell%20and%20Wieland%20Brendel%0AAbstract%3A%20Frontier%20models%20are%20transitioning%20from%20multimodal%20large%20language%20models%20%28MLLMs%29%20that%20merely%20ingest%20visual%20information%20to%20unified%20multimodal%20models%20%28UMMs%29%20capable%20of%20native%20interleaved%20generation.%20This%20shift%20has%20sparked%20interest%20in%20using%20intermediate%20visualizations%20as%20a%20reasoning%20aid%2C%20akin%20to%20human%20mental%20imagery.%20Central%20to%20this%20idea%20is%20the%20ability%20to%20form%2C%20maintain%2C%20and%20manipulate%20visual%20representations%20in%20a%20goal-oriented%20manner.%20To%20evaluate%20and%20probe%20this%20capability%2C%20we%20develop%20MentisOculi%2C%20a%20procedural%2C%20stratified%20suite%20of%20multi-step%20reasoning%20problems%20amenable%20to%20visual%20solution%2C%20tuned%20to%20challenge%20frontier%20models.%20Evaluating%20visual%20strategies%20ranging%20from%20latent%20tokens%20to%20explicit%20generated%20imagery%2C%20we%20find%20they%20generally%20fail%20to%20improve%20performance.%20Analysis%20of%20UMMs%20specifically%20exposes%20a%20critical%20limitation%3A%20While%20they%20possess%20the%20textual%20reasoning%20capacity%20to%20solve%20a%20task%20and%20can%20sometimes%20generate%20correct%20visuals%2C%20they%20suffer%20from%20compounding%20generation%20errors%20and%20fail%20to%20leverage%20even%20ground-truth%20visualizations.%20Our%20findings%20suggest%20that%20despite%20their%20inherent%20appeal%2C%20visual%20thoughts%20do%20not%20yet%20benefit%20model%20reasoning.%20MentisOculi%20establishes%20the%20necessary%20foundation%20to%20analyze%20and%20close%20this%20gap%20across%20diverse%20model%20families.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMentisOculi%253A%2520Revealing%2520the%2520Limits%2520of%2520Reasoning%2520with%2520Mental%2520Imagery%26entry.906535625%3DJana%2520Zeller%2520and%2520Thadd%25C3%25A4us%2520Wiedemer%2520and%2520Fanfei%2520Li%2520and%2520Thomas%2520Klein%2520and%2520Prasanna%2520Mayilvahanan%2520and%2520Matthias%2520Bethge%2520and%2520Felix%2520Wichmann%2520and%2520Ryan%2520Cotterell%2520and%2520Wieland%2520Brendel%26entry.1292438233%3DFrontier%2520models%2520are%2520transitioning%2520from%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520that%2520merely%2520ingest%2520visual%2520information%2520to%2520unified%2520multimodal%2520models%2520%2528UMMs%2529%2520capable%2520of%2520native%2520interleaved%2520generation.%2520This%2520shift%2520has%2520sparked%2520interest%2520in%2520using%2520intermediate%2520visualizations%2520as%2520a%2520reasoning%2520aid%252C%2520akin%2520to%2520human%2520mental%2520imagery.%2520Central%2520to%2520this%2520idea%2520is%2520the%2520ability%2520to%2520form%252C%2520maintain%252C%2520and%2520manipulate%2520visual%2520representations%2520in%2520a%2520goal-oriented%2520manner.%2520To%2520evaluate%2520and%2520probe%2520this%2520capability%252C%2520we%2520develop%2520MentisOculi%252C%2520a%2520procedural%252C%2520stratified%2520suite%2520of%2520multi-step%2520reasoning%2520problems%2520amenable%2520to%2520visual%2520solution%252C%2520tuned%2520to%2520challenge%2520frontier%2520models.%2520Evaluating%2520visual%2520strategies%2520ranging%2520from%2520latent%2520tokens%2520to%2520explicit%2520generated%2520imagery%252C%2520we%2520find%2520they%2520generally%2520fail%2520to%2520improve%2520performance.%2520Analysis%2520of%2520UMMs%2520specifically%2520exposes%2520a%2520critical%2520limitation%253A%2520While%2520they%2520possess%2520the%2520textual%2520reasoning%2520capacity%2520to%2520solve%2520a%2520task%2520and%2520can%2520sometimes%2520generate%2520correct%2520visuals%252C%2520they%2520suffer%2520from%2520compounding%2520generation%2520errors%2520and%2520fail%2520to%2520leverage%2520even%2520ground-truth%2520visualizations.%2520Our%2520findings%2520suggest%2520that%2520despite%2520their%2520inherent%2520appeal%252C%2520visual%2520thoughts%2520do%2520not%2520yet%2520benefit%2520model%2520reasoning.%2520MentisOculi%2520establishes%2520the%2520necessary%2520foundation%2520to%2520analyze%2520and%2520close%2520this%2520gap%2520across%2520diverse%2520model%2520families.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MentisOculi%3A%20Revealing%20the%20Limits%20of%20Reasoning%20with%20Mental%20Imagery&entry.906535625=Jana%20Zeller%20and%20Thadd%C3%A4us%20Wiedemer%20and%20Fanfei%20Li%20and%20Thomas%20Klein%20and%20Prasanna%20Mayilvahanan%20and%20Matthias%20Bethge%20and%20Felix%20Wichmann%20and%20Ryan%20Cotterell%20and%20Wieland%20Brendel&entry.1292438233=Frontier%20models%20are%20transitioning%20from%20multimodal%20large%20language%20models%20%28MLLMs%29%20that%20merely%20ingest%20visual%20information%20to%20unified%20multimodal%20models%20%28UMMs%29%20capable%20of%20native%20interleaved%20generation.%20This%20shift%20has%20sparked%20interest%20in%20using%20intermediate%20visualizations%20as%20a%20reasoning%20aid%2C%20akin%20to%20human%20mental%20imagery.%20Central%20to%20this%20idea%20is%20the%20ability%20to%20form%2C%20maintain%2C%20and%20manipulate%20visual%20representations%20in%20a%20goal-oriented%20manner.%20To%20evaluate%20and%20probe%20this%20capability%2C%20we%20develop%20MentisOculi%2C%20a%20procedural%2C%20stratified%20suite%20of%20multi-step%20reasoning%20problems%20amenable%20to%20visual%20solution%2C%20tuned%20to%20challenge%20frontier%20models.%20Evaluating%20visual%20strategies%20ranging%20from%20latent%20tokens%20to%20explicit%20generated%20imagery%2C%20we%20find%20they%20generally%20fail%20to%20improve%20performance.%20Analysis%20of%20UMMs%20specifically%20exposes%20a%20critical%20limitation%3A%20While%20they%20possess%20the%20textual%20reasoning%20capacity%20to%20solve%20a%20task%20and%20can%20sometimes%20generate%20correct%20visuals%2C%20they%20suffer%20from%20compounding%20generation%20errors%20and%20fail%20to%20leverage%20even%20ground-truth%20visualizations.%20Our%20findings%20suggest%20that%20despite%20their%20inherent%20appeal%2C%20visual%20thoughts%20do%20not%20yet%20benefit%20model%20reasoning.%20MentisOculi%20establishes%20the%20necessary%20foundation%20to%20analyze%20and%20close%20this%20gap%20across%20diverse%20model%20families.&entry.1838667208=http%3A//arxiv.org/abs/2602.02465v1&entry.124074799=Read"},
{"title": "LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation", "author": "Bo Miao and Weijia Liu and Jun Luo and Lachlan Shinnick and Jian Liu and Thomas Hamilton-Smith and Yuhe Yang and Zijie Wu and Vanja Videnovic and Feras Dayoub and Anton van den Hengel", "abstract": "The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap", "link": "http://arxiv.org/abs/2602.02220v1", "date": "2026-02-02", "relevancy": 2.7948, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5733}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LangMap%3A%20A%20Hierarchical%20Benchmark%20for%20Open-Vocabulary%20Goal%20Navigation&body=Title%3A%20LangMap%3A%20A%20Hierarchical%20Benchmark%20for%20Open-Vocabulary%20Goal%20Navigation%0AAuthor%3A%20Bo%20Miao%20and%20Weijia%20Liu%20and%20Jun%20Luo%20and%20Lachlan%20Shinnick%20and%20Jian%20Liu%20and%20Thomas%20Hamilton-Smith%20and%20Yuhe%20Yang%20and%20Zijie%20Wu%20and%20Vanja%20Videnovic%20and%20Feras%20Dayoub%20and%20Anton%20van%20den%20Hengel%0AAbstract%3A%20The%20relationships%20between%20objects%20and%20language%20are%20fundamental%20to%20meaningful%20communication%20between%20humans%20and%20AI%2C%20and%20to%20practically%20useful%20embodied%20intelligence.%20We%20introduce%20HieraNav%2C%20a%20multi-granularity%2C%20open-vocabulary%20goal%20navigation%20task%20where%20agents%20interpret%20natural%20language%20instructions%20to%20reach%20targets%20at%20four%20semantic%20levels%3A%20scene%2C%20room%2C%20region%2C%20and%20instance.%20To%20this%20end%2C%20we%20present%20Language%20as%20a%20Map%20%28LangMap%29%2C%20a%20large-scale%20benchmark%20built%20on%20real-world%203D%20indoor%20scans%20with%20comprehensive%20human-verified%20annotations%20and%20tasks%20spanning%20these%20levels.%20LangMap%20provides%20region%20labels%2C%20discriminative%20region%20descriptions%2C%20discriminative%20instance%20descriptions%20covering%20414%20object%20categories%2C%20and%20over%2018K%20navigation%20tasks.%20Each%20target%20features%20both%20concise%20and%20detailed%20descriptions%2C%20enabling%20evaluation%20across%20different%20instruction%20styles.%20LangMap%20achieves%20superior%20annotation%20quality%2C%20outperforming%20GOAT-Bench%20by%2023.8%25%20in%20discriminative%20accuracy%20using%20four%20times%20fewer%20words.%20Comprehensive%20evaluations%20of%20zero-shot%20and%20supervised%20models%20on%20LangMap%20reveal%20that%20richer%20context%20and%20memory%20improve%20success%2C%20while%20long-tailed%2C%20small%2C%20context-dependent%2C%20and%20distant%20goals%2C%20as%20well%20as%20multi-goal%20completion%2C%20remain%20challenging.%20HieraNav%20and%20LangMap%20establish%20a%20rigorous%20testbed%20for%20advancing%20language-driven%20embodied%20navigation.%20Project%3A%20https%3A//bo-miao.github.io/LangMap%0ALink%3A%20http%3A//arxiv.org/abs/2602.02220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLangMap%253A%2520A%2520Hierarchical%2520Benchmark%2520for%2520Open-Vocabulary%2520Goal%2520Navigation%26entry.906535625%3DBo%2520Miao%2520and%2520Weijia%2520Liu%2520and%2520Jun%2520Luo%2520and%2520Lachlan%2520Shinnick%2520and%2520Jian%2520Liu%2520and%2520Thomas%2520Hamilton-Smith%2520and%2520Yuhe%2520Yang%2520and%2520Zijie%2520Wu%2520and%2520Vanja%2520Videnovic%2520and%2520Feras%2520Dayoub%2520and%2520Anton%2520van%2520den%2520Hengel%26entry.1292438233%3DThe%2520relationships%2520between%2520objects%2520and%2520language%2520are%2520fundamental%2520to%2520meaningful%2520communication%2520between%2520humans%2520and%2520AI%252C%2520and%2520to%2520practically%2520useful%2520embodied%2520intelligence.%2520We%2520introduce%2520HieraNav%252C%2520a%2520multi-granularity%252C%2520open-vocabulary%2520goal%2520navigation%2520task%2520where%2520agents%2520interpret%2520natural%2520language%2520instructions%2520to%2520reach%2520targets%2520at%2520four%2520semantic%2520levels%253A%2520scene%252C%2520room%252C%2520region%252C%2520and%2520instance.%2520To%2520this%2520end%252C%2520we%2520present%2520Language%2520as%2520a%2520Map%2520%2528LangMap%2529%252C%2520a%2520large-scale%2520benchmark%2520built%2520on%2520real-world%25203D%2520indoor%2520scans%2520with%2520comprehensive%2520human-verified%2520annotations%2520and%2520tasks%2520spanning%2520these%2520levels.%2520LangMap%2520provides%2520region%2520labels%252C%2520discriminative%2520region%2520descriptions%252C%2520discriminative%2520instance%2520descriptions%2520covering%2520414%2520object%2520categories%252C%2520and%2520over%252018K%2520navigation%2520tasks.%2520Each%2520target%2520features%2520both%2520concise%2520and%2520detailed%2520descriptions%252C%2520enabling%2520evaluation%2520across%2520different%2520instruction%2520styles.%2520LangMap%2520achieves%2520superior%2520annotation%2520quality%252C%2520outperforming%2520GOAT-Bench%2520by%252023.8%2525%2520in%2520discriminative%2520accuracy%2520using%2520four%2520times%2520fewer%2520words.%2520Comprehensive%2520evaluations%2520of%2520zero-shot%2520and%2520supervised%2520models%2520on%2520LangMap%2520reveal%2520that%2520richer%2520context%2520and%2520memory%2520improve%2520success%252C%2520while%2520long-tailed%252C%2520small%252C%2520context-dependent%252C%2520and%2520distant%2520goals%252C%2520as%2520well%2520as%2520multi-goal%2520completion%252C%2520remain%2520challenging.%2520HieraNav%2520and%2520LangMap%2520establish%2520a%2520rigorous%2520testbed%2520for%2520advancing%2520language-driven%2520embodied%2520navigation.%2520Project%253A%2520https%253A//bo-miao.github.io/LangMap%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LangMap%3A%20A%20Hierarchical%20Benchmark%20for%20Open-Vocabulary%20Goal%20Navigation&entry.906535625=Bo%20Miao%20and%20Weijia%20Liu%20and%20Jun%20Luo%20and%20Lachlan%20Shinnick%20and%20Jian%20Liu%20and%20Thomas%20Hamilton-Smith%20and%20Yuhe%20Yang%20and%20Zijie%20Wu%20and%20Vanja%20Videnovic%20and%20Feras%20Dayoub%20and%20Anton%20van%20den%20Hengel&entry.1292438233=The%20relationships%20between%20objects%20and%20language%20are%20fundamental%20to%20meaningful%20communication%20between%20humans%20and%20AI%2C%20and%20to%20practically%20useful%20embodied%20intelligence.%20We%20introduce%20HieraNav%2C%20a%20multi-granularity%2C%20open-vocabulary%20goal%20navigation%20task%20where%20agents%20interpret%20natural%20language%20instructions%20to%20reach%20targets%20at%20four%20semantic%20levels%3A%20scene%2C%20room%2C%20region%2C%20and%20instance.%20To%20this%20end%2C%20we%20present%20Language%20as%20a%20Map%20%28LangMap%29%2C%20a%20large-scale%20benchmark%20built%20on%20real-world%203D%20indoor%20scans%20with%20comprehensive%20human-verified%20annotations%20and%20tasks%20spanning%20these%20levels.%20LangMap%20provides%20region%20labels%2C%20discriminative%20region%20descriptions%2C%20discriminative%20instance%20descriptions%20covering%20414%20object%20categories%2C%20and%20over%2018K%20navigation%20tasks.%20Each%20target%20features%20both%20concise%20and%20detailed%20descriptions%2C%20enabling%20evaluation%20across%20different%20instruction%20styles.%20LangMap%20achieves%20superior%20annotation%20quality%2C%20outperforming%20GOAT-Bench%20by%2023.8%25%20in%20discriminative%20accuracy%20using%20four%20times%20fewer%20words.%20Comprehensive%20evaluations%20of%20zero-shot%20and%20supervised%20models%20on%20LangMap%20reveal%20that%20richer%20context%20and%20memory%20improve%20success%2C%20while%20long-tailed%2C%20small%2C%20context-dependent%2C%20and%20distant%20goals%2C%20as%20well%20as%20multi-goal%20completion%2C%20remain%20challenging.%20HieraNav%20and%20LangMap%20establish%20a%20rigorous%20testbed%20for%20advancing%20language-driven%20embodied%20navigation.%20Project%3A%20https%3A//bo-miao.github.io/LangMap&entry.1838667208=http%3A//arxiv.org/abs/2602.02220v1&entry.124074799=Read"},
{"title": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language", "author": "Delong Chen and Mustafa Shukor and Theo Moutakanni and Willy Chung and Jade Yu and Tejaswi Kasarla and Yejin Bang and Allen Bolourchi and Yann LeCun and Pascale Fung", "abstract": "We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA's embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters.", "link": "http://arxiv.org/abs/2512.10942v2", "date": "2026-02-02", "relevancy": 2.7779, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VL-JEPA%3A%20Joint%20Embedding%20Predictive%20Architecture%20for%20Vision-language&body=Title%3A%20VL-JEPA%3A%20Joint%20Embedding%20Predictive%20Architecture%20for%20Vision-language%0AAuthor%3A%20Delong%20Chen%20and%20Mustafa%20Shukor%20and%20Theo%20Moutakanni%20and%20Willy%20Chung%20and%20Jade%20Yu%20and%20Tejaswi%20Kasarla%20and%20Yejin%20Bang%20and%20Allen%20Bolourchi%20and%20Yann%20LeCun%20and%20Pascale%20Fung%0AAbstract%3A%20We%20introduce%20VL-JEPA%2C%20a%20vision-language%20model%20built%20on%20a%20Joint%20Embedding%20Predictive%20Architecture%20%28JEPA%29.%20Instead%20of%20autoregressively%20generating%20tokens%20as%20in%20classical%20VLMs%2C%20VL-JEPA%20predicts%20continuous%20embeddings%20of%20the%20target%20texts.%20By%20learning%20in%20an%20abstract%20representation%20space%2C%20the%20model%20focuses%20on%20task-relevant%20semantics%20while%20abstracting%20away%20surface-level%20linguistic%20variability.%20In%20a%20strictly%20controlled%20comparison%20against%20standard%20token-space%20VLM%20training%20with%20the%20same%20vision%20encoder%20and%20training%20data%2C%20VL-JEPA%20achieves%20stronger%20performance%20while%20having%2050%25%20fewer%20trainable%20parameters.%20At%20inference%20time%2C%20a%20lightweight%20text%20decoder%20is%20invoked%20only%20when%20needed%20to%20translate%20VL-JEPA%20predicted%20embeddings%20into%20text.%20We%20show%20that%20VL-JEPA%20natively%20supports%20selective%20decoding%20that%20reduces%20the%20number%20of%20decoding%20operations%20by%202.85x%20while%20maintaining%20similar%20performance%20compared%20to%20non-adaptive%20uniform%20decoding.%20Beyond%20generation%2C%20the%20VL-JEPA%27s%20embedding%20space%20naturally%20supports%20open-vocabulary%20classification%2C%20text-to-video%20retrieval%2C%20and%20discriminative%20VQA%20without%20any%20architecture%20modification.%20On%20eight%20video%20classification%20and%20eight%20video%20retrieval%20datasets%2C%20the%20average%20performance%20VL-JEPA%20surpasses%20that%20of%20CLIP%2C%20SigLIP2%2C%20and%20Perception%20Encoder.%20At%20the%20same%20time%2C%20the%20model%20achieves%20comparable%20performance%20as%20classical%20VLMs%20%28InstructBLIP%2C%20QwenVL%29%20on%20four%20VQA%20datasets%3A%20GQA%2C%20TallyQA%2C%20POPE%20and%20POPEv2%2C%20despite%20only%20having%201.6B%20parameters.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10942v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVL-JEPA%253A%2520Joint%2520Embedding%2520Predictive%2520Architecture%2520for%2520Vision-language%26entry.906535625%3DDelong%2520Chen%2520and%2520Mustafa%2520Shukor%2520and%2520Theo%2520Moutakanni%2520and%2520Willy%2520Chung%2520and%2520Jade%2520Yu%2520and%2520Tejaswi%2520Kasarla%2520and%2520Yejin%2520Bang%2520and%2520Allen%2520Bolourchi%2520and%2520Yann%2520LeCun%2520and%2520Pascale%2520Fung%26entry.1292438233%3DWe%2520introduce%2520VL-JEPA%252C%2520a%2520vision-language%2520model%2520built%2520on%2520a%2520Joint%2520Embedding%2520Predictive%2520Architecture%2520%2528JEPA%2529.%2520Instead%2520of%2520autoregressively%2520generating%2520tokens%2520as%2520in%2520classical%2520VLMs%252C%2520VL-JEPA%2520predicts%2520continuous%2520embeddings%2520of%2520the%2520target%2520texts.%2520By%2520learning%2520in%2520an%2520abstract%2520representation%2520space%252C%2520the%2520model%2520focuses%2520on%2520task-relevant%2520semantics%2520while%2520abstracting%2520away%2520surface-level%2520linguistic%2520variability.%2520In%2520a%2520strictly%2520controlled%2520comparison%2520against%2520standard%2520token-space%2520VLM%2520training%2520with%2520the%2520same%2520vision%2520encoder%2520and%2520training%2520data%252C%2520VL-JEPA%2520achieves%2520stronger%2520performance%2520while%2520having%252050%2525%2520fewer%2520trainable%2520parameters.%2520At%2520inference%2520time%252C%2520a%2520lightweight%2520text%2520decoder%2520is%2520invoked%2520only%2520when%2520needed%2520to%2520translate%2520VL-JEPA%2520predicted%2520embeddings%2520into%2520text.%2520We%2520show%2520that%2520VL-JEPA%2520natively%2520supports%2520selective%2520decoding%2520that%2520reduces%2520the%2520number%2520of%2520decoding%2520operations%2520by%25202.85x%2520while%2520maintaining%2520similar%2520performance%2520compared%2520to%2520non-adaptive%2520uniform%2520decoding.%2520Beyond%2520generation%252C%2520the%2520VL-JEPA%2527s%2520embedding%2520space%2520naturally%2520supports%2520open-vocabulary%2520classification%252C%2520text-to-video%2520retrieval%252C%2520and%2520discriminative%2520VQA%2520without%2520any%2520architecture%2520modification.%2520On%2520eight%2520video%2520classification%2520and%2520eight%2520video%2520retrieval%2520datasets%252C%2520the%2520average%2520performance%2520VL-JEPA%2520surpasses%2520that%2520of%2520CLIP%252C%2520SigLIP2%252C%2520and%2520Perception%2520Encoder.%2520At%2520the%2520same%2520time%252C%2520the%2520model%2520achieves%2520comparable%2520performance%2520as%2520classical%2520VLMs%2520%2528InstructBLIP%252C%2520QwenVL%2529%2520on%2520four%2520VQA%2520datasets%253A%2520GQA%252C%2520TallyQA%252C%2520POPE%2520and%2520POPEv2%252C%2520despite%2520only%2520having%25201.6B%2520parameters.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10942v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VL-JEPA%3A%20Joint%20Embedding%20Predictive%20Architecture%20for%20Vision-language&entry.906535625=Delong%20Chen%20and%20Mustafa%20Shukor%20and%20Theo%20Moutakanni%20and%20Willy%20Chung%20and%20Jade%20Yu%20and%20Tejaswi%20Kasarla%20and%20Yejin%20Bang%20and%20Allen%20Bolourchi%20and%20Yann%20LeCun%20and%20Pascale%20Fung&entry.1292438233=We%20introduce%20VL-JEPA%2C%20a%20vision-language%20model%20built%20on%20a%20Joint%20Embedding%20Predictive%20Architecture%20%28JEPA%29.%20Instead%20of%20autoregressively%20generating%20tokens%20as%20in%20classical%20VLMs%2C%20VL-JEPA%20predicts%20continuous%20embeddings%20of%20the%20target%20texts.%20By%20learning%20in%20an%20abstract%20representation%20space%2C%20the%20model%20focuses%20on%20task-relevant%20semantics%20while%20abstracting%20away%20surface-level%20linguistic%20variability.%20In%20a%20strictly%20controlled%20comparison%20against%20standard%20token-space%20VLM%20training%20with%20the%20same%20vision%20encoder%20and%20training%20data%2C%20VL-JEPA%20achieves%20stronger%20performance%20while%20having%2050%25%20fewer%20trainable%20parameters.%20At%20inference%20time%2C%20a%20lightweight%20text%20decoder%20is%20invoked%20only%20when%20needed%20to%20translate%20VL-JEPA%20predicted%20embeddings%20into%20text.%20We%20show%20that%20VL-JEPA%20natively%20supports%20selective%20decoding%20that%20reduces%20the%20number%20of%20decoding%20operations%20by%202.85x%20while%20maintaining%20similar%20performance%20compared%20to%20non-adaptive%20uniform%20decoding.%20Beyond%20generation%2C%20the%20VL-JEPA%27s%20embedding%20space%20naturally%20supports%20open-vocabulary%20classification%2C%20text-to-video%20retrieval%2C%20and%20discriminative%20VQA%20without%20any%20architecture%20modification.%20On%20eight%20video%20classification%20and%20eight%20video%20retrieval%20datasets%2C%20the%20average%20performance%20VL-JEPA%20surpasses%20that%20of%20CLIP%2C%20SigLIP2%2C%20and%20Perception%20Encoder.%20At%20the%20same%20time%2C%20the%20model%20achieves%20comparable%20performance%20as%20classical%20VLMs%20%28InstructBLIP%2C%20QwenVL%29%20on%20four%20VQA%20datasets%3A%20GQA%2C%20TallyQA%2C%20POPE%20and%20POPEv2%2C%20despite%20only%20having%201.6B%20parameters.&entry.1838667208=http%3A//arxiv.org/abs/2512.10942v2&entry.124074799=Read"},
{"title": "Segment to Focus: Guiding Latent Action Models in the Presence of Distractors", "author": "Hamza Adnan and Matthew T. Jackson and Alexey Zakharov", "abstract": "Latent Action Models (LAMs) learn to extract action-relevant representations solely from raw observations, enabling reinforcement learning from unlabelled videos and significantly scaling available training data. However, LAMs face a critical challenge in disentangling action-relevant features from action-correlated noise (e.g., background motion). Failing to filter these distractors causes LAMs to capture spurious correlations and build sub-optimal latent action spaces. In this paper, we introduce MaskLAM -- a lightweight modification to LAM training to mitigate this issue by incorporating visual agent segmentation. MaskLAM utilises segmentation masks from pretrained foundation models to weight the LAM reconstruction loss, thereby prioritising salient information over background elements while requiring no architectural modifications. We demonstrate the effectiveness of our method on continuous-control MuJoCo tasks, modified with action-correlated background noise. Our approach yields up to a 4x increase in accrued rewards compared to standard baselines and a 3x improvement in the latent action quality, as evidenced by linear probe evaluation.", "link": "http://arxiv.org/abs/2602.02259v1", "date": "2026-02-02", "relevancy": 2.7669, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5712}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5575}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20to%20Focus%3A%20Guiding%20Latent%20Action%20Models%20in%20the%20Presence%20of%20Distractors&body=Title%3A%20Segment%20to%20Focus%3A%20Guiding%20Latent%20Action%20Models%20in%20the%20Presence%20of%20Distractors%0AAuthor%3A%20Hamza%20Adnan%20and%20Matthew%20T.%20Jackson%20and%20Alexey%20Zakharov%0AAbstract%3A%20Latent%20Action%20Models%20%28LAMs%29%20learn%20to%20extract%20action-relevant%20representations%20solely%20from%20raw%20observations%2C%20enabling%20reinforcement%20learning%20from%20unlabelled%20videos%20and%20significantly%20scaling%20available%20training%20data.%20However%2C%20LAMs%20face%20a%20critical%20challenge%20in%20disentangling%20action-relevant%20features%20from%20action-correlated%20noise%20%28e.g.%2C%20background%20motion%29.%20Failing%20to%20filter%20these%20distractors%20causes%20LAMs%20to%20capture%20spurious%20correlations%20and%20build%20sub-optimal%20latent%20action%20spaces.%20In%20this%20paper%2C%20we%20introduce%20MaskLAM%20--%20a%20lightweight%20modification%20to%20LAM%20training%20to%20mitigate%20this%20issue%20by%20incorporating%20visual%20agent%20segmentation.%20MaskLAM%20utilises%20segmentation%20masks%20from%20pretrained%20foundation%20models%20to%20weight%20the%20LAM%20reconstruction%20loss%2C%20thereby%20prioritising%20salient%20information%20over%20background%20elements%20while%20requiring%20no%20architectural%20modifications.%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%20on%20continuous-control%20MuJoCo%20tasks%2C%20modified%20with%20action-correlated%20background%20noise.%20Our%20approach%20yields%20up%20to%20a%204x%20increase%20in%20accrued%20rewards%20compared%20to%20standard%20baselines%20and%20a%203x%20improvement%20in%20the%20latent%20action%20quality%2C%20as%20evidenced%20by%20linear%20probe%20evaluation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520to%2520Focus%253A%2520Guiding%2520Latent%2520Action%2520Models%2520in%2520the%2520Presence%2520of%2520Distractors%26entry.906535625%3DHamza%2520Adnan%2520and%2520Matthew%2520T.%2520Jackson%2520and%2520Alexey%2520Zakharov%26entry.1292438233%3DLatent%2520Action%2520Models%2520%2528LAMs%2529%2520learn%2520to%2520extract%2520action-relevant%2520representations%2520solely%2520from%2520raw%2520observations%252C%2520enabling%2520reinforcement%2520learning%2520from%2520unlabelled%2520videos%2520and%2520significantly%2520scaling%2520available%2520training%2520data.%2520However%252C%2520LAMs%2520face%2520a%2520critical%2520challenge%2520in%2520disentangling%2520action-relevant%2520features%2520from%2520action-correlated%2520noise%2520%2528e.g.%252C%2520background%2520motion%2529.%2520Failing%2520to%2520filter%2520these%2520distractors%2520causes%2520LAMs%2520to%2520capture%2520spurious%2520correlations%2520and%2520build%2520sub-optimal%2520latent%2520action%2520spaces.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520MaskLAM%2520--%2520a%2520lightweight%2520modification%2520to%2520LAM%2520training%2520to%2520mitigate%2520this%2520issue%2520by%2520incorporating%2520visual%2520agent%2520segmentation.%2520MaskLAM%2520utilises%2520segmentation%2520masks%2520from%2520pretrained%2520foundation%2520models%2520to%2520weight%2520the%2520LAM%2520reconstruction%2520loss%252C%2520thereby%2520prioritising%2520salient%2520information%2520over%2520background%2520elements%2520while%2520requiring%2520no%2520architectural%2520modifications.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520on%2520continuous-control%2520MuJoCo%2520tasks%252C%2520modified%2520with%2520action-correlated%2520background%2520noise.%2520Our%2520approach%2520yields%2520up%2520to%2520a%25204x%2520increase%2520in%2520accrued%2520rewards%2520compared%2520to%2520standard%2520baselines%2520and%2520a%25203x%2520improvement%2520in%2520the%2520latent%2520action%2520quality%252C%2520as%2520evidenced%2520by%2520linear%2520probe%2520evaluation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20to%20Focus%3A%20Guiding%20Latent%20Action%20Models%20in%20the%20Presence%20of%20Distractors&entry.906535625=Hamza%20Adnan%20and%20Matthew%20T.%20Jackson%20and%20Alexey%20Zakharov&entry.1292438233=Latent%20Action%20Models%20%28LAMs%29%20learn%20to%20extract%20action-relevant%20representations%20solely%20from%20raw%20observations%2C%20enabling%20reinforcement%20learning%20from%20unlabelled%20videos%20and%20significantly%20scaling%20available%20training%20data.%20However%2C%20LAMs%20face%20a%20critical%20challenge%20in%20disentangling%20action-relevant%20features%20from%20action-correlated%20noise%20%28e.g.%2C%20background%20motion%29.%20Failing%20to%20filter%20these%20distractors%20causes%20LAMs%20to%20capture%20spurious%20correlations%20and%20build%20sub-optimal%20latent%20action%20spaces.%20In%20this%20paper%2C%20we%20introduce%20MaskLAM%20--%20a%20lightweight%20modification%20to%20LAM%20training%20to%20mitigate%20this%20issue%20by%20incorporating%20visual%20agent%20segmentation.%20MaskLAM%20utilises%20segmentation%20masks%20from%20pretrained%20foundation%20models%20to%20weight%20the%20LAM%20reconstruction%20loss%2C%20thereby%20prioritising%20salient%20information%20over%20background%20elements%20while%20requiring%20no%20architectural%20modifications.%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%20on%20continuous-control%20MuJoCo%20tasks%2C%20modified%20with%20action-correlated%20background%20noise.%20Our%20approach%20yields%20up%20to%20a%204x%20increase%20in%20accrued%20rewards%20compared%20to%20standard%20baselines%20and%20a%203x%20improvement%20in%20the%20latent%20action%20quality%2C%20as%20evidenced%20by%20linear%20probe%20evaluation.&entry.1838667208=http%3A//arxiv.org/abs/2602.02259v1&entry.124074799=Read"},
{"title": "Learning Sparse Visual Representations via Spatial-Semantic Factorization", "author": "Theodore Zhengde Zhao and Sid Kiblawi and Jianwei Yang and Naoto Usuyama and Reuben Tan and Noel C Codella and Tristan Naumann and Hoifung Poon and Mu Wei", "abstract": "Self-supervised learning (SSL) faces a fundamental conflict between semantic understanding and image reconstruction. High-level semantic SSL (e.g., DINO) relies on global tokens that are forced to be location-invariant for augmentation alignment, a process that inherently discards the spatial coordinates required for reconstruction. Conversely, generative SSL (e.g., MAE) preserves dense feature grids for reconstruction but fails to produce high-level abstractions. We introduce STELLAR, a framework that resolves this tension by factorizing visual features into a low-rank product of semantic concepts and their spatial distributions. This disentanglement allows us to perform DINO-style augmentation alignment on the semantic tokens while maintaining the precise spatial mapping in the localization matrix necessary for pixel-level reconstruction. We demonstrate that as few as 16 sparse tokens under this factorized form are sufficient to simultaneously support high-quality reconstruction (2.60 FID) and match the semantic performance of dense backbones (79.10% ImageNet accuracy). Our results highlight STELLAR as a versatile sparse representation that bridges the gap between discriminative and generative vision by strategically separating semantic identity from spatial geometry. Code available at https://aka.ms/stellar.", "link": "http://arxiv.org/abs/2602.01905v1", "date": "2026-02-02", "relevancy": 2.746, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5589}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5443}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Sparse%20Visual%20Representations%20via%20Spatial-Semantic%20Factorization&body=Title%3A%20Learning%20Sparse%20Visual%20Representations%20via%20Spatial-Semantic%20Factorization%0AAuthor%3A%20Theodore%20Zhengde%20Zhao%20and%20Sid%20Kiblawi%20and%20Jianwei%20Yang%20and%20Naoto%20Usuyama%20and%20Reuben%20Tan%20and%20Noel%20C%20Codella%20and%20Tristan%20Naumann%20and%20Hoifung%20Poon%20and%20Mu%20Wei%0AAbstract%3A%20Self-supervised%20learning%20%28SSL%29%20faces%20a%20fundamental%20conflict%20between%20semantic%20understanding%20and%20image%20reconstruction.%20High-level%20semantic%20SSL%20%28e.g.%2C%20DINO%29%20relies%20on%20global%20tokens%20that%20are%20forced%20to%20be%20location-invariant%20for%20augmentation%20alignment%2C%20a%20process%20that%20inherently%20discards%20the%20spatial%20coordinates%20required%20for%20reconstruction.%20Conversely%2C%20generative%20SSL%20%28e.g.%2C%20MAE%29%20preserves%20dense%20feature%20grids%20for%20reconstruction%20but%20fails%20to%20produce%20high-level%20abstractions.%20We%20introduce%20STELLAR%2C%20a%20framework%20that%20resolves%20this%20tension%20by%20factorizing%20visual%20features%20into%20a%20low-rank%20product%20of%20semantic%20concepts%20and%20their%20spatial%20distributions.%20This%20disentanglement%20allows%20us%20to%20perform%20DINO-style%20augmentation%20alignment%20on%20the%20semantic%20tokens%20while%20maintaining%20the%20precise%20spatial%20mapping%20in%20the%20localization%20matrix%20necessary%20for%20pixel-level%20reconstruction.%20We%20demonstrate%20that%20as%20few%20as%2016%20sparse%20tokens%20under%20this%20factorized%20form%20are%20sufficient%20to%20simultaneously%20support%20high-quality%20reconstruction%20%282.60%20FID%29%20and%20match%20the%20semantic%20performance%20of%20dense%20backbones%20%2879.10%25%20ImageNet%20accuracy%29.%20Our%20results%20highlight%20STELLAR%20as%20a%20versatile%20sparse%20representation%20that%20bridges%20the%20gap%20between%20discriminative%20and%20generative%20vision%20by%20strategically%20separating%20semantic%20identity%20from%20spatial%20geometry.%20Code%20available%20at%20https%3A//aka.ms/stellar.%0ALink%3A%20http%3A//arxiv.org/abs/2602.01905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Sparse%2520Visual%2520Representations%2520via%2520Spatial-Semantic%2520Factorization%26entry.906535625%3DTheodore%2520Zhengde%2520Zhao%2520and%2520Sid%2520Kiblawi%2520and%2520Jianwei%2520Yang%2520and%2520Naoto%2520Usuyama%2520and%2520Reuben%2520Tan%2520and%2520Noel%2520C%2520Codella%2520and%2520Tristan%2520Naumann%2520and%2520Hoifung%2520Poon%2520and%2520Mu%2520Wei%26entry.1292438233%3DSelf-supervised%2520learning%2520%2528SSL%2529%2520faces%2520a%2520fundamental%2520conflict%2520between%2520semantic%2520understanding%2520and%2520image%2520reconstruction.%2520High-level%2520semantic%2520SSL%2520%2528e.g.%252C%2520DINO%2529%2520relies%2520on%2520global%2520tokens%2520that%2520are%2520forced%2520to%2520be%2520location-invariant%2520for%2520augmentation%2520alignment%252C%2520a%2520process%2520that%2520inherently%2520discards%2520the%2520spatial%2520coordinates%2520required%2520for%2520reconstruction.%2520Conversely%252C%2520generative%2520SSL%2520%2528e.g.%252C%2520MAE%2529%2520preserves%2520dense%2520feature%2520grids%2520for%2520reconstruction%2520but%2520fails%2520to%2520produce%2520high-level%2520abstractions.%2520We%2520introduce%2520STELLAR%252C%2520a%2520framework%2520that%2520resolves%2520this%2520tension%2520by%2520factorizing%2520visual%2520features%2520into%2520a%2520low-rank%2520product%2520of%2520semantic%2520concepts%2520and%2520their%2520spatial%2520distributions.%2520This%2520disentanglement%2520allows%2520us%2520to%2520perform%2520DINO-style%2520augmentation%2520alignment%2520on%2520the%2520semantic%2520tokens%2520while%2520maintaining%2520the%2520precise%2520spatial%2520mapping%2520in%2520the%2520localization%2520matrix%2520necessary%2520for%2520pixel-level%2520reconstruction.%2520We%2520demonstrate%2520that%2520as%2520few%2520as%252016%2520sparse%2520tokens%2520under%2520this%2520factorized%2520form%2520are%2520sufficient%2520to%2520simultaneously%2520support%2520high-quality%2520reconstruction%2520%25282.60%2520FID%2529%2520and%2520match%2520the%2520semantic%2520performance%2520of%2520dense%2520backbones%2520%252879.10%2525%2520ImageNet%2520accuracy%2529.%2520Our%2520results%2520highlight%2520STELLAR%2520as%2520a%2520versatile%2520sparse%2520representation%2520that%2520bridges%2520the%2520gap%2520between%2520discriminative%2520and%2520generative%2520vision%2520by%2520strategically%2520separating%2520semantic%2520identity%2520from%2520spatial%2520geometry.%2520Code%2520available%2520at%2520https%253A//aka.ms/stellar.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.01905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Sparse%20Visual%20Representations%20via%20Spatial-Semantic%20Factorization&entry.906535625=Theodore%20Zhengde%20Zhao%20and%20Sid%20Kiblawi%20and%20Jianwei%20Yang%20and%20Naoto%20Usuyama%20and%20Reuben%20Tan%20and%20Noel%20C%20Codella%20and%20Tristan%20Naumann%20and%20Hoifung%20Poon%20and%20Mu%20Wei&entry.1292438233=Self-supervised%20learning%20%28SSL%29%20faces%20a%20fundamental%20conflict%20between%20semantic%20understanding%20and%20image%20reconstruction.%20High-level%20semantic%20SSL%20%28e.g.%2C%20DINO%29%20relies%20on%20global%20tokens%20that%20are%20forced%20to%20be%20location-invariant%20for%20augmentation%20alignment%2C%20a%20process%20that%20inherently%20discards%20the%20spatial%20coordinates%20required%20for%20reconstruction.%20Conversely%2C%20generative%20SSL%20%28e.g.%2C%20MAE%29%20preserves%20dense%20feature%20grids%20for%20reconstruction%20but%20fails%20to%20produce%20high-level%20abstractions.%20We%20introduce%20STELLAR%2C%20a%20framework%20that%20resolves%20this%20tension%20by%20factorizing%20visual%20features%20into%20a%20low-rank%20product%20of%20semantic%20concepts%20and%20their%20spatial%20distributions.%20This%20disentanglement%20allows%20us%20to%20perform%20DINO-style%20augmentation%20alignment%20on%20the%20semantic%20tokens%20while%20maintaining%20the%20precise%20spatial%20mapping%20in%20the%20localization%20matrix%20necessary%20for%20pixel-level%20reconstruction.%20We%20demonstrate%20that%20as%20few%20as%2016%20sparse%20tokens%20under%20this%20factorized%20form%20are%20sufficient%20to%20simultaneously%20support%20high-quality%20reconstruction%20%282.60%20FID%29%20and%20match%20the%20semantic%20performance%20of%20dense%20backbones%20%2879.10%25%20ImageNet%20accuracy%29.%20Our%20results%20highlight%20STELLAR%20as%20a%20versatile%20sparse%20representation%20that%20bridges%20the%20gap%20between%20discriminative%20and%20generative%20vision%20by%20strategically%20separating%20semantic%20identity%20from%20spatial%20geometry.%20Code%20available%20at%20https%3A//aka.ms/stellar.&entry.1838667208=http%3A//arxiv.org/abs/2602.01905v1&entry.124074799=Read"},
{"title": "Large Multimodal Models for Low-Resource Languages: A Survey", "author": "Marian Lupascu and Ana-Cristina Rogoz and Mihai Sorin Stupariu and Radu Tudor Ionescu", "abstract": "In this survey, we systematically analyze techniques used to adapt large multimodal models (LMMs) for low-resource (LR) languages, examining approaches ranging from visual enhancement and data creation to cross-modal transfer and fusion strategies. Through a comprehensive analysis of 117 studies across 96 LR languages, we identify key patterns in how researchers tackle the challenges of limited data and computational resources. We categorize works into resource-oriented and method-oriented contributions, further dividing contributions into relevant sub-categories. We compare method-oriented contributions in terms of performance and efficiency, discussing benefits and limitations of representative studies. We find that visual information often serves as a crucial bridge for improving model performance in LR settings, though significant challenges remain in areas such as hallucination mitigation and computational efficiency. In summary, we provide researchers with a clear understanding of current approaches and remaining challenges in making LMMs more accessible to speakers of LR (understudied) languages. We complement our survey with an open-source repository available at: https://github.com/marianlupascu/LMM4LRL-Survey.", "link": "http://arxiv.org/abs/2502.05568v4", "date": "2026-02-02", "relevancy": 2.7366, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5556}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Multimodal%20Models%20for%20Low-Resource%20Languages%3A%20A%20Survey&body=Title%3A%20Large%20Multimodal%20Models%20for%20Low-Resource%20Languages%3A%20A%20Survey%0AAuthor%3A%20Marian%20Lupascu%20and%20Ana-Cristina%20Rogoz%20and%20Mihai%20Sorin%20Stupariu%20and%20Radu%20Tudor%20Ionescu%0AAbstract%3A%20In%20this%20survey%2C%20we%20systematically%20analyze%20techniques%20used%20to%20adapt%20large%20multimodal%20models%20%28LMMs%29%20for%20low-resource%20%28LR%29%20languages%2C%20examining%20approaches%20ranging%20from%20visual%20enhancement%20and%20data%20creation%20to%20cross-modal%20transfer%20and%20fusion%20strategies.%20Through%20a%20comprehensive%20analysis%20of%20117%20studies%20across%2096%20LR%20languages%2C%20we%20identify%20key%20patterns%20in%20how%20researchers%20tackle%20the%20challenges%20of%20limited%20data%20and%20computational%20resources.%20We%20categorize%20works%20into%20resource-oriented%20and%20method-oriented%20contributions%2C%20further%20dividing%20contributions%20into%20relevant%20sub-categories.%20We%20compare%20method-oriented%20contributions%20in%20terms%20of%20performance%20and%20efficiency%2C%20discussing%20benefits%20and%20limitations%20of%20representative%20studies.%20We%20find%20that%20visual%20information%20often%20serves%20as%20a%20crucial%20bridge%20for%20improving%20model%20performance%20in%20LR%20settings%2C%20though%20significant%20challenges%20remain%20in%20areas%20such%20as%20hallucination%20mitigation%20and%20computational%20efficiency.%20In%20summary%2C%20we%20provide%20researchers%20with%20a%20clear%20understanding%20of%20current%20approaches%20and%20remaining%20challenges%20in%20making%20LMMs%20more%20accessible%20to%20speakers%20of%20LR%20%28understudied%29%20languages.%20We%20complement%20our%20survey%20with%20an%20open-source%20repository%20available%20at%3A%20https%3A//github.com/marianlupascu/LMM4LRL-Survey.%0ALink%3A%20http%3A//arxiv.org/abs/2502.05568v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Multimodal%2520Models%2520for%2520Low-Resource%2520Languages%253A%2520A%2520Survey%26entry.906535625%3DMarian%2520Lupascu%2520and%2520Ana-Cristina%2520Rogoz%2520and%2520Mihai%2520Sorin%2520Stupariu%2520and%2520Radu%2520Tudor%2520Ionescu%26entry.1292438233%3DIn%2520this%2520survey%252C%2520we%2520systematically%2520analyze%2520techniques%2520used%2520to%2520adapt%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520for%2520low-resource%2520%2528LR%2529%2520languages%252C%2520examining%2520approaches%2520ranging%2520from%2520visual%2520enhancement%2520and%2520data%2520creation%2520to%2520cross-modal%2520transfer%2520and%2520fusion%2520strategies.%2520Through%2520a%2520comprehensive%2520analysis%2520of%2520117%2520studies%2520across%252096%2520LR%2520languages%252C%2520we%2520identify%2520key%2520patterns%2520in%2520how%2520researchers%2520tackle%2520the%2520challenges%2520of%2520limited%2520data%2520and%2520computational%2520resources.%2520We%2520categorize%2520works%2520into%2520resource-oriented%2520and%2520method-oriented%2520contributions%252C%2520further%2520dividing%2520contributions%2520into%2520relevant%2520sub-categories.%2520We%2520compare%2520method-oriented%2520contributions%2520in%2520terms%2520of%2520performance%2520and%2520efficiency%252C%2520discussing%2520benefits%2520and%2520limitations%2520of%2520representative%2520studies.%2520We%2520find%2520that%2520visual%2520information%2520often%2520serves%2520as%2520a%2520crucial%2520bridge%2520for%2520improving%2520model%2520performance%2520in%2520LR%2520settings%252C%2520though%2520significant%2520challenges%2520remain%2520in%2520areas%2520such%2520as%2520hallucination%2520mitigation%2520and%2520computational%2520efficiency.%2520In%2520summary%252C%2520we%2520provide%2520researchers%2520with%2520a%2520clear%2520understanding%2520of%2520current%2520approaches%2520and%2520remaining%2520challenges%2520in%2520making%2520LMMs%2520more%2520accessible%2520to%2520speakers%2520of%2520LR%2520%2528understudied%2529%2520languages.%2520We%2520complement%2520our%2520survey%2520with%2520an%2520open-source%2520repository%2520available%2520at%253A%2520https%253A//github.com/marianlupascu/LMM4LRL-Survey.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05568v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Multimodal%20Models%20for%20Low-Resource%20Languages%3A%20A%20Survey&entry.906535625=Marian%20Lupascu%20and%20Ana-Cristina%20Rogoz%20and%20Mihai%20Sorin%20Stupariu%20and%20Radu%20Tudor%20Ionescu&entry.1292438233=In%20this%20survey%2C%20we%20systematically%20analyze%20techniques%20used%20to%20adapt%20large%20multimodal%20models%20%28LMMs%29%20for%20low-resource%20%28LR%29%20languages%2C%20examining%20approaches%20ranging%20from%20visual%20enhancement%20and%20data%20creation%20to%20cross-modal%20transfer%20and%20fusion%20strategies.%20Through%20a%20comprehensive%20analysis%20of%20117%20studies%20across%2096%20LR%20languages%2C%20we%20identify%20key%20patterns%20in%20how%20researchers%20tackle%20the%20challenges%20of%20limited%20data%20and%20computational%20resources.%20We%20categorize%20works%20into%20resource-oriented%20and%20method-oriented%20contributions%2C%20further%20dividing%20contributions%20into%20relevant%20sub-categories.%20We%20compare%20method-oriented%20contributions%20in%20terms%20of%20performance%20and%20efficiency%2C%20discussing%20benefits%20and%20limitations%20of%20representative%20studies.%20We%20find%20that%20visual%20information%20often%20serves%20as%20a%20crucial%20bridge%20for%20improving%20model%20performance%20in%20LR%20settings%2C%20though%20significant%20challenges%20remain%20in%20areas%20such%20as%20hallucination%20mitigation%20and%20computational%20efficiency.%20In%20summary%2C%20we%20provide%20researchers%20with%20a%20clear%20understanding%20of%20current%20approaches%20and%20remaining%20challenges%20in%20making%20LMMs%20more%20accessible%20to%20speakers%20of%20LR%20%28understudied%29%20languages.%20We%20complement%20our%20survey%20with%20an%20open-source%20repository%20available%20at%3A%20https%3A//github.com/marianlupascu/LMM4LRL-Survey.&entry.1838667208=http%3A//arxiv.org/abs/2502.05568v4&entry.124074799=Read"},
{"title": "ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning", "author": "Gongli Xi and Kun Wang and Zeming Gao and Huahui Yi and Haolang Lu and Ye Tian and Wendong Wang", "abstract": "Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \\emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model's reasoning pathway (question $\\rightarrow$ outputs $\\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \\textbf{without any additional training}, ClueTracer improves all \\textbf{reasoning} architectures (including \\texttt{R1-OneVision}, \\texttt{Ocean-R1}, \\texttt{MM-Eureka}, \\emph{etc}.) by $\\mathbf{1.21\\times}$ on reasoning benchmarks. When transferred to \\textbf{non-reasoning} settings, it yields a $\\mathbf{1.14\\times}$ gain.", "link": "http://arxiv.org/abs/2602.02004v1", "date": "2026-02-02", "relevancy": 2.7197, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ClueTracer%3A%20Question-to-Vision%20Clue%20Tracing%20for%20Training-Free%20Hallucination%20Suppression%20in%20Multimodal%20Reasoning&body=Title%3A%20ClueTracer%3A%20Question-to-Vision%20Clue%20Tracing%20for%20Training-Free%20Hallucination%20Suppression%20in%20Multimodal%20Reasoning%0AAuthor%3A%20Gongli%20Xi%20and%20Kun%20Wang%20and%20Zeming%20Gao%20and%20Huahui%20Yi%20and%20Haolang%20Lu%20and%20Ye%20Tian%20and%20Wendong%20Wang%0AAbstract%3A%20Large%20multimodal%20reasoning%20models%20solve%20challenging%20visual%20problems%20via%20explicit%20long-chain%20inference%3A%20they%20gather%20visual%20clues%20from%20images%20and%20decode%20clues%20into%20textual%20tokens.%20Yet%20this%20capability%20also%20increases%20hallucinations%2C%20where%20the%20model%20generates%20content%20that%20is%20not%20supported%20by%20the%20input%20image%20or%20the%20question.%20To%20understand%20this%20failure%20mode%2C%20we%20identify%20%5Cemph%7Breasoning%20drift%7D%3A%20during%20clue%20gathering%2C%20the%20model%20over-focuses%20on%20question-irrelevant%20entities%2C%20diluting%20focus%20on%20task-relevant%20cues%20and%20gradually%20decoupling%20the%20reasoning%20trace%20from%20visual%20grounding.%20As%20a%20consequence%2C%20many%20inference-time%20localization%20or%20intervention%20methods%20developed%20for%20non-reasoning%20models%20fail%20to%20pinpoint%20the%20true%20clues%20in%20reasoning%20settings.%20Motivated%20by%20these%20insights%2C%20we%20introduce%20ClueRecall%2C%20a%20metric%20for%20assessing%20visual%20clue%20retrieval%2C%20and%20present%20ClueTracer%2C%20a%20training-free%2C%20parameter-free%2C%20and%20architecture-agnostic%20plugin%20for%20hallucination%20suppression.%20ClueTracer%20starts%20from%20the%20question%20and%20traces%20how%20key%20clues%20propagate%20along%20the%20model%27s%20reasoning%20pathway%20%28question%20%24%5Crightarrow%24%20outputs%20%24%5Crightarrow%24%20visual%20tokens%29%2C%20thereby%20localizing%20task-relevant%20patches%20while%20suppressing%20spurious%20attention%20to%20irrelevant%20regions.%20Remarkably%2C%20%5Ctextbf%7Bwithout%20any%20additional%20training%7D%2C%20ClueTracer%20improves%20all%20%5Ctextbf%7Breasoning%7D%20architectures%20%28including%20%5Ctexttt%7BR1-OneVision%7D%2C%20%5Ctexttt%7BOcean-R1%7D%2C%20%5Ctexttt%7BMM-Eureka%7D%2C%20%5Cemph%7Betc%7D.%29%20by%20%24%5Cmathbf%7B1.21%5Ctimes%7D%24%20on%20reasoning%20benchmarks.%20When%20transferred%20to%20%5Ctextbf%7Bnon-reasoning%7D%20settings%2C%20it%20yields%20a%20%24%5Cmathbf%7B1.14%5Ctimes%7D%24%20gain.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02004v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClueTracer%253A%2520Question-to-Vision%2520Clue%2520Tracing%2520for%2520Training-Free%2520Hallucination%2520Suppression%2520in%2520Multimodal%2520Reasoning%26entry.906535625%3DGongli%2520Xi%2520and%2520Kun%2520Wang%2520and%2520Zeming%2520Gao%2520and%2520Huahui%2520Yi%2520and%2520Haolang%2520Lu%2520and%2520Ye%2520Tian%2520and%2520Wendong%2520Wang%26entry.1292438233%3DLarge%2520multimodal%2520reasoning%2520models%2520solve%2520challenging%2520visual%2520problems%2520via%2520explicit%2520long-chain%2520inference%253A%2520they%2520gather%2520visual%2520clues%2520from%2520images%2520and%2520decode%2520clues%2520into%2520textual%2520tokens.%2520Yet%2520this%2520capability%2520also%2520increases%2520hallucinations%252C%2520where%2520the%2520model%2520generates%2520content%2520that%2520is%2520not%2520supported%2520by%2520the%2520input%2520image%2520or%2520the%2520question.%2520To%2520understand%2520this%2520failure%2520mode%252C%2520we%2520identify%2520%255Cemph%257Breasoning%2520drift%257D%253A%2520during%2520clue%2520gathering%252C%2520the%2520model%2520over-focuses%2520on%2520question-irrelevant%2520entities%252C%2520diluting%2520focus%2520on%2520task-relevant%2520cues%2520and%2520gradually%2520decoupling%2520the%2520reasoning%2520trace%2520from%2520visual%2520grounding.%2520As%2520a%2520consequence%252C%2520many%2520inference-time%2520localization%2520or%2520intervention%2520methods%2520developed%2520for%2520non-reasoning%2520models%2520fail%2520to%2520pinpoint%2520the%2520true%2520clues%2520in%2520reasoning%2520settings.%2520Motivated%2520by%2520these%2520insights%252C%2520we%2520introduce%2520ClueRecall%252C%2520a%2520metric%2520for%2520assessing%2520visual%2520clue%2520retrieval%252C%2520and%2520present%2520ClueTracer%252C%2520a%2520training-free%252C%2520parameter-free%252C%2520and%2520architecture-agnostic%2520plugin%2520for%2520hallucination%2520suppression.%2520ClueTracer%2520starts%2520from%2520the%2520question%2520and%2520traces%2520how%2520key%2520clues%2520propagate%2520along%2520the%2520model%2527s%2520reasoning%2520pathway%2520%2528question%2520%2524%255Crightarrow%2524%2520outputs%2520%2524%255Crightarrow%2524%2520visual%2520tokens%2529%252C%2520thereby%2520localizing%2520task-relevant%2520patches%2520while%2520suppressing%2520spurious%2520attention%2520to%2520irrelevant%2520regions.%2520Remarkably%252C%2520%255Ctextbf%257Bwithout%2520any%2520additional%2520training%257D%252C%2520ClueTracer%2520improves%2520all%2520%255Ctextbf%257Breasoning%257D%2520architectures%2520%2528including%2520%255Ctexttt%257BR1-OneVision%257D%252C%2520%255Ctexttt%257BOcean-R1%257D%252C%2520%255Ctexttt%257BMM-Eureka%257D%252C%2520%255Cemph%257Betc%257D.%2529%2520by%2520%2524%255Cmathbf%257B1.21%255Ctimes%257D%2524%2520on%2520reasoning%2520benchmarks.%2520When%2520transferred%2520to%2520%255Ctextbf%257Bnon-reasoning%257D%2520settings%252C%2520it%2520yields%2520a%2520%2524%255Cmathbf%257B1.14%255Ctimes%257D%2524%2520gain.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02004v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClueTracer%3A%20Question-to-Vision%20Clue%20Tracing%20for%20Training-Free%20Hallucination%20Suppression%20in%20Multimodal%20Reasoning&entry.906535625=Gongli%20Xi%20and%20Kun%20Wang%20and%20Zeming%20Gao%20and%20Huahui%20Yi%20and%20Haolang%20Lu%20and%20Ye%20Tian%20and%20Wendong%20Wang&entry.1292438233=Large%20multimodal%20reasoning%20models%20solve%20challenging%20visual%20problems%20via%20explicit%20long-chain%20inference%3A%20they%20gather%20visual%20clues%20from%20images%20and%20decode%20clues%20into%20textual%20tokens.%20Yet%20this%20capability%20also%20increases%20hallucinations%2C%20where%20the%20model%20generates%20content%20that%20is%20not%20supported%20by%20the%20input%20image%20or%20the%20question.%20To%20understand%20this%20failure%20mode%2C%20we%20identify%20%5Cemph%7Breasoning%20drift%7D%3A%20during%20clue%20gathering%2C%20the%20model%20over-focuses%20on%20question-irrelevant%20entities%2C%20diluting%20focus%20on%20task-relevant%20cues%20and%20gradually%20decoupling%20the%20reasoning%20trace%20from%20visual%20grounding.%20As%20a%20consequence%2C%20many%20inference-time%20localization%20or%20intervention%20methods%20developed%20for%20non-reasoning%20models%20fail%20to%20pinpoint%20the%20true%20clues%20in%20reasoning%20settings.%20Motivated%20by%20these%20insights%2C%20we%20introduce%20ClueRecall%2C%20a%20metric%20for%20assessing%20visual%20clue%20retrieval%2C%20and%20present%20ClueTracer%2C%20a%20training-free%2C%20parameter-free%2C%20and%20architecture-agnostic%20plugin%20for%20hallucination%20suppression.%20ClueTracer%20starts%20from%20the%20question%20and%20traces%20how%20key%20clues%20propagate%20along%20the%20model%27s%20reasoning%20pathway%20%28question%20%24%5Crightarrow%24%20outputs%20%24%5Crightarrow%24%20visual%20tokens%29%2C%20thereby%20localizing%20task-relevant%20patches%20while%20suppressing%20spurious%20attention%20to%20irrelevant%20regions.%20Remarkably%2C%20%5Ctextbf%7Bwithout%20any%20additional%20training%7D%2C%20ClueTracer%20improves%20all%20%5Ctextbf%7Breasoning%7D%20architectures%20%28including%20%5Ctexttt%7BR1-OneVision%7D%2C%20%5Ctexttt%7BOcean-R1%7D%2C%20%5Ctexttt%7BMM-Eureka%7D%2C%20%5Cemph%7Betc%7D.%29%20by%20%24%5Cmathbf%7B1.21%5Ctimes%7D%24%20on%20reasoning%20benchmarks.%20When%20transferred%20to%20%5Ctextbf%7Bnon-reasoning%7D%20settings%2C%20it%20yields%20a%20%24%5Cmathbf%7B1.14%5Ctimes%7D%24%20gain.&entry.1838667208=http%3A//arxiv.org/abs/2602.02004v1&entry.124074799=Read"},
{"title": "NAB: Neural Adaptive Binning for Sparse-View CT reconstruction", "author": "Wangduo Xie and Matthew B. Blaschko", "abstract": "Computed Tomography (CT) plays a vital role in inspecting the internal structures of industrial objects. Furthermore, achieving high-quality CT reconstruction from sparse views is essential for reducing production costs. While classic implicit neural networks have shown promising results for sparse reconstruction, they are unable to leverage shape priors of objects. Motivated by the observation that numerous industrial objects exhibit rectangular structures, we propose a novel \\textbf{N}eural \\textbf{A}daptive \\textbf{B}inning (\\textbf{NAB}) method that effectively integrates rectangular priors into the reconstruction process. Specifically, our approach first maps coordinate space into a binned vector space. This mapping relies on an innovative binning mechanism based on differences between shifted hyperbolic tangent functions, with our extension enabling rotations around the input-plane normal vector. The resulting representations are then processed by a neural network to predict CT attenuation coefficients. This design enables end-to-end optimization of the encoding parameters -- including position, size, steepness, and rotation -- via gradient flow from the projection data, thus enhancing reconstruction accuracy. By adjusting the smoothness of the binning function, NAB can generalize to objects with more complex geometries. This research provides a new perspective on integrating shape priors into neural network-based reconstruction. Extensive experiments demonstrate that NAB achieves superior performance on two industrial datasets. It also maintains robust on medical datasets when the binning function is extended to more general expression. The code will be made available.", "link": "http://arxiv.org/abs/2602.02356v1", "date": "2026-02-02", "relevancy": 2.6767, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5411}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5325}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NAB%3A%20Neural%20Adaptive%20Binning%20for%20Sparse-View%20CT%20reconstruction&body=Title%3A%20NAB%3A%20Neural%20Adaptive%20Binning%20for%20Sparse-View%20CT%20reconstruction%0AAuthor%3A%20Wangduo%20Xie%20and%20Matthew%20B.%20Blaschko%0AAbstract%3A%20Computed%20Tomography%20%28CT%29%20plays%20a%20vital%20role%20in%20inspecting%20the%20internal%20structures%20of%20industrial%20objects.%20Furthermore%2C%20achieving%20high-quality%20CT%20reconstruction%20from%20sparse%20views%20is%20essential%20for%20reducing%20production%20costs.%20While%20classic%20implicit%20neural%20networks%20have%20shown%20promising%20results%20for%20sparse%20reconstruction%2C%20they%20are%20unable%20to%20leverage%20shape%20priors%20of%20objects.%20Motivated%20by%20the%20observation%20that%20numerous%20industrial%20objects%20exhibit%20rectangular%20structures%2C%20we%20propose%20a%20novel%20%5Ctextbf%7BN%7Deural%20%5Ctextbf%7BA%7Ddaptive%20%5Ctextbf%7BB%7Dinning%20%28%5Ctextbf%7BNAB%7D%29%20method%20that%20effectively%20integrates%20rectangular%20priors%20into%20the%20reconstruction%20process.%20Specifically%2C%20our%20approach%20first%20maps%20coordinate%20space%20into%20a%20binned%20vector%20space.%20This%20mapping%20relies%20on%20an%20innovative%20binning%20mechanism%20based%20on%20differences%20between%20shifted%20hyperbolic%20tangent%20functions%2C%20with%20our%20extension%20enabling%20rotations%20around%20the%20input-plane%20normal%20vector.%20The%20resulting%20representations%20are%20then%20processed%20by%20a%20neural%20network%20to%20predict%20CT%20attenuation%20coefficients.%20This%20design%20enables%20end-to-end%20optimization%20of%20the%20encoding%20parameters%20--%20including%20position%2C%20size%2C%20steepness%2C%20and%20rotation%20--%20via%20gradient%20flow%20from%20the%20projection%20data%2C%20thus%20enhancing%20reconstruction%20accuracy.%20By%20adjusting%20the%20smoothness%20of%20the%20binning%20function%2C%20NAB%20can%20generalize%20to%20objects%20with%20more%20complex%20geometries.%20This%20research%20provides%20a%20new%20perspective%20on%20integrating%20shape%20priors%20into%20neural%20network-based%20reconstruction.%20Extensive%20experiments%20demonstrate%20that%20NAB%20achieves%20superior%20performance%20on%20two%20industrial%20datasets.%20It%20also%20maintains%20robust%20on%20medical%20datasets%20when%20the%20binning%20function%20is%20extended%20to%20more%20general%20expression.%20The%20code%20will%20be%20made%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNAB%253A%2520Neural%2520Adaptive%2520Binning%2520for%2520Sparse-View%2520CT%2520reconstruction%26entry.906535625%3DWangduo%2520Xie%2520and%2520Matthew%2520B.%2520Blaschko%26entry.1292438233%3DComputed%2520Tomography%2520%2528CT%2529%2520plays%2520a%2520vital%2520role%2520in%2520inspecting%2520the%2520internal%2520structures%2520of%2520industrial%2520objects.%2520Furthermore%252C%2520achieving%2520high-quality%2520CT%2520reconstruction%2520from%2520sparse%2520views%2520is%2520essential%2520for%2520reducing%2520production%2520costs.%2520While%2520classic%2520implicit%2520neural%2520networks%2520have%2520shown%2520promising%2520results%2520for%2520sparse%2520reconstruction%252C%2520they%2520are%2520unable%2520to%2520leverage%2520shape%2520priors%2520of%2520objects.%2520Motivated%2520by%2520the%2520observation%2520that%2520numerous%2520industrial%2520objects%2520exhibit%2520rectangular%2520structures%252C%2520we%2520propose%2520a%2520novel%2520%255Ctextbf%257BN%257Deural%2520%255Ctextbf%257BA%257Ddaptive%2520%255Ctextbf%257BB%257Dinning%2520%2528%255Ctextbf%257BNAB%257D%2529%2520method%2520that%2520effectively%2520integrates%2520rectangular%2520priors%2520into%2520the%2520reconstruction%2520process.%2520Specifically%252C%2520our%2520approach%2520first%2520maps%2520coordinate%2520space%2520into%2520a%2520binned%2520vector%2520space.%2520This%2520mapping%2520relies%2520on%2520an%2520innovative%2520binning%2520mechanism%2520based%2520on%2520differences%2520between%2520shifted%2520hyperbolic%2520tangent%2520functions%252C%2520with%2520our%2520extension%2520enabling%2520rotations%2520around%2520the%2520input-plane%2520normal%2520vector.%2520The%2520resulting%2520representations%2520are%2520then%2520processed%2520by%2520a%2520neural%2520network%2520to%2520predict%2520CT%2520attenuation%2520coefficients.%2520This%2520design%2520enables%2520end-to-end%2520optimization%2520of%2520the%2520encoding%2520parameters%2520--%2520including%2520position%252C%2520size%252C%2520steepness%252C%2520and%2520rotation%2520--%2520via%2520gradient%2520flow%2520from%2520the%2520projection%2520data%252C%2520thus%2520enhancing%2520reconstruction%2520accuracy.%2520By%2520adjusting%2520the%2520smoothness%2520of%2520the%2520binning%2520function%252C%2520NAB%2520can%2520generalize%2520to%2520objects%2520with%2520more%2520complex%2520geometries.%2520This%2520research%2520provides%2520a%2520new%2520perspective%2520on%2520integrating%2520shape%2520priors%2520into%2520neural%2520network-based%2520reconstruction.%2520Extensive%2520experiments%2520demonstrate%2520that%2520NAB%2520achieves%2520superior%2520performance%2520on%2520two%2520industrial%2520datasets.%2520It%2520also%2520maintains%2520robust%2520on%2520medical%2520datasets%2520when%2520the%2520binning%2520function%2520is%2520extended%2520to%2520more%2520general%2520expression.%2520The%2520code%2520will%2520be%2520made%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NAB%3A%20Neural%20Adaptive%20Binning%20for%20Sparse-View%20CT%20reconstruction&entry.906535625=Wangduo%20Xie%20and%20Matthew%20B.%20Blaschko&entry.1292438233=Computed%20Tomography%20%28CT%29%20plays%20a%20vital%20role%20in%20inspecting%20the%20internal%20structures%20of%20industrial%20objects.%20Furthermore%2C%20achieving%20high-quality%20CT%20reconstruction%20from%20sparse%20views%20is%20essential%20for%20reducing%20production%20costs.%20While%20classic%20implicit%20neural%20networks%20have%20shown%20promising%20results%20for%20sparse%20reconstruction%2C%20they%20are%20unable%20to%20leverage%20shape%20priors%20of%20objects.%20Motivated%20by%20the%20observation%20that%20numerous%20industrial%20objects%20exhibit%20rectangular%20structures%2C%20we%20propose%20a%20novel%20%5Ctextbf%7BN%7Deural%20%5Ctextbf%7BA%7Ddaptive%20%5Ctextbf%7BB%7Dinning%20%28%5Ctextbf%7BNAB%7D%29%20method%20that%20effectively%20integrates%20rectangular%20priors%20into%20the%20reconstruction%20process.%20Specifically%2C%20our%20approach%20first%20maps%20coordinate%20space%20into%20a%20binned%20vector%20space.%20This%20mapping%20relies%20on%20an%20innovative%20binning%20mechanism%20based%20on%20differences%20between%20shifted%20hyperbolic%20tangent%20functions%2C%20with%20our%20extension%20enabling%20rotations%20around%20the%20input-plane%20normal%20vector.%20The%20resulting%20representations%20are%20then%20processed%20by%20a%20neural%20network%20to%20predict%20CT%20attenuation%20coefficients.%20This%20design%20enables%20end-to-end%20optimization%20of%20the%20encoding%20parameters%20--%20including%20position%2C%20size%2C%20steepness%2C%20and%20rotation%20--%20via%20gradient%20flow%20from%20the%20projection%20data%2C%20thus%20enhancing%20reconstruction%20accuracy.%20By%20adjusting%20the%20smoothness%20of%20the%20binning%20function%2C%20NAB%20can%20generalize%20to%20objects%20with%20more%20complex%20geometries.%20This%20research%20provides%20a%20new%20perspective%20on%20integrating%20shape%20priors%20into%20neural%20network-based%20reconstruction.%20Extensive%20experiments%20demonstrate%20that%20NAB%20achieves%20superior%20performance%20on%20two%20industrial%20datasets.%20It%20also%20maintains%20robust%20on%20medical%20datasets%20when%20the%20binning%20function%20is%20extended%20to%20more%20general%20expression.%20The%20code%20will%20be%20made%20available.&entry.1838667208=http%3A//arxiv.org/abs/2602.02356v1&entry.124074799=Read"},
{"title": "Active Transfer Bagging: A New Approach for Accelerated Active Learning Acquisition of Data by Combined Transfer Learning and Bagging Based Models", "author": "Vivienne Pelletier and Daniel J. Rivera and Obinna Nwokonkwo and Steven A. Wilson and Christopher L. Muhich", "abstract": "Modern machine learning has achieved remarkable success on many problems, but this success often depends on the existence of large, labeled datasets. While active learning can dramatically reduce labeling cost when annotations are expensive, early performance is frequently dominated by the initial seed set, typically chosen at random. In many applications, however, related or approximate datasets are readily available and can be leveraged to construct a better seed set. We introduce a new method for selecting the seed data set for active learning, Active-Transfer Bagging (ATBagging). ATBagging estimates the informativeness of candidate data point from a Bayesian interpretation of bagged ensemble models by comparing in-bag and out-of-bag predictive distributions from the labeled dataset, yielding an information-gain proxy. To avoid redundant selections, we impose feature-space diversity by sampling a determinantal point process (DPP) whose kernel uses Random Fourier Features and a quality-diversity factorization that incorporates the informativeness scores. This same blended method is used for selection of new data points to collect during the active learning phase. We evaluate ATBagging on four real-world datasets covering both target-transfer and feature-shift scenarios (QM9, ERA5, Forbes 2000, and Beijing PM2.5). Across seed sizes nseed = 10-100, ATBagging improves or ties early active learning and increases area under the learning-curve relative to alternative seed subset selection methodologies in almost all cases, with strongest benefits in low-data regimes. Thus, ATBagging provides a low-cost, high reward means to initiating active learning-based data collection.", "link": "http://arxiv.org/abs/2602.02415v1", "date": "2026-02-02", "relevancy": 2.6475, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5443}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5394}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Transfer%20Bagging%3A%20A%20New%20Approach%20for%20Accelerated%20Active%20Learning%20Acquisition%20of%20Data%20by%20Combined%20Transfer%20Learning%20and%20Bagging%20Based%20Models&body=Title%3A%20Active%20Transfer%20Bagging%3A%20A%20New%20Approach%20for%20Accelerated%20Active%20Learning%20Acquisition%20of%20Data%20by%20Combined%20Transfer%20Learning%20and%20Bagging%20Based%20Models%0AAuthor%3A%20Vivienne%20Pelletier%20and%20Daniel%20J.%20Rivera%20and%20Obinna%20Nwokonkwo%20and%20Steven%20A.%20Wilson%20and%20Christopher%20L.%20Muhich%0AAbstract%3A%20Modern%20machine%20learning%20has%20achieved%20remarkable%20success%20on%20many%20problems%2C%20but%20this%20success%20often%20depends%20on%20the%20existence%20of%20large%2C%20labeled%20datasets.%20While%20active%20learning%20can%20dramatically%20reduce%20labeling%20cost%20when%20annotations%20are%20expensive%2C%20early%20performance%20is%20frequently%20dominated%20by%20the%20initial%20seed%20set%2C%20typically%20chosen%20at%20random.%20In%20many%20applications%2C%20however%2C%20related%20or%20approximate%20datasets%20are%20readily%20available%20and%20can%20be%20leveraged%20to%20construct%20a%20better%20seed%20set.%20We%20introduce%20a%20new%20method%20for%20selecting%20the%20seed%20data%20set%20for%20active%20learning%2C%20Active-Transfer%20Bagging%20%28ATBagging%29.%20ATBagging%20estimates%20the%20informativeness%20of%20candidate%20data%20point%20from%20a%20Bayesian%20interpretation%20of%20bagged%20ensemble%20models%20by%20comparing%20in-bag%20and%20out-of-bag%20predictive%20distributions%20from%20the%20labeled%20dataset%2C%20yielding%20an%20information-gain%20proxy.%20To%20avoid%20redundant%20selections%2C%20we%20impose%20feature-space%20diversity%20by%20sampling%20a%20determinantal%20point%20process%20%28DPP%29%20whose%20kernel%20uses%20Random%20Fourier%20Features%20and%20a%20quality-diversity%20factorization%20that%20incorporates%20the%20informativeness%20scores.%20This%20same%20blended%20method%20is%20used%20for%20selection%20of%20new%20data%20points%20to%20collect%20during%20the%20active%20learning%20phase.%20We%20evaluate%20ATBagging%20on%20four%20real-world%20datasets%20covering%20both%20target-transfer%20and%20feature-shift%20scenarios%20%28QM9%2C%20ERA5%2C%20Forbes%202000%2C%20and%20Beijing%20PM2.5%29.%20Across%20seed%20sizes%20nseed%20%3D%2010-100%2C%20ATBagging%20improves%20or%20ties%20early%20active%20learning%20and%20increases%20area%20under%20the%20learning-curve%20relative%20to%20alternative%20seed%20subset%20selection%20methodologies%20in%20almost%20all%20cases%2C%20with%20strongest%20benefits%20in%20low-data%20regimes.%20Thus%2C%20ATBagging%20provides%20a%20low-cost%2C%20high%20reward%20means%20to%20initiating%20active%20learning-based%20data%20collection.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02415v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Transfer%2520Bagging%253A%2520A%2520New%2520Approach%2520for%2520Accelerated%2520Active%2520Learning%2520Acquisition%2520of%2520Data%2520by%2520Combined%2520Transfer%2520Learning%2520and%2520Bagging%2520Based%2520Models%26entry.906535625%3DVivienne%2520Pelletier%2520and%2520Daniel%2520J.%2520Rivera%2520and%2520Obinna%2520Nwokonkwo%2520and%2520Steven%2520A.%2520Wilson%2520and%2520Christopher%2520L.%2520Muhich%26entry.1292438233%3DModern%2520machine%2520learning%2520has%2520achieved%2520remarkable%2520success%2520on%2520many%2520problems%252C%2520but%2520this%2520success%2520often%2520depends%2520on%2520the%2520existence%2520of%2520large%252C%2520labeled%2520datasets.%2520While%2520active%2520learning%2520can%2520dramatically%2520reduce%2520labeling%2520cost%2520when%2520annotations%2520are%2520expensive%252C%2520early%2520performance%2520is%2520frequently%2520dominated%2520by%2520the%2520initial%2520seed%2520set%252C%2520typically%2520chosen%2520at%2520random.%2520In%2520many%2520applications%252C%2520however%252C%2520related%2520or%2520approximate%2520datasets%2520are%2520readily%2520available%2520and%2520can%2520be%2520leveraged%2520to%2520construct%2520a%2520better%2520seed%2520set.%2520We%2520introduce%2520a%2520new%2520method%2520for%2520selecting%2520the%2520seed%2520data%2520set%2520for%2520active%2520learning%252C%2520Active-Transfer%2520Bagging%2520%2528ATBagging%2529.%2520ATBagging%2520estimates%2520the%2520informativeness%2520of%2520candidate%2520data%2520point%2520from%2520a%2520Bayesian%2520interpretation%2520of%2520bagged%2520ensemble%2520models%2520by%2520comparing%2520in-bag%2520and%2520out-of-bag%2520predictive%2520distributions%2520from%2520the%2520labeled%2520dataset%252C%2520yielding%2520an%2520information-gain%2520proxy.%2520To%2520avoid%2520redundant%2520selections%252C%2520we%2520impose%2520feature-space%2520diversity%2520by%2520sampling%2520a%2520determinantal%2520point%2520process%2520%2528DPP%2529%2520whose%2520kernel%2520uses%2520Random%2520Fourier%2520Features%2520and%2520a%2520quality-diversity%2520factorization%2520that%2520incorporates%2520the%2520informativeness%2520scores.%2520This%2520same%2520blended%2520method%2520is%2520used%2520for%2520selection%2520of%2520new%2520data%2520points%2520to%2520collect%2520during%2520the%2520active%2520learning%2520phase.%2520We%2520evaluate%2520ATBagging%2520on%2520four%2520real-world%2520datasets%2520covering%2520both%2520target-transfer%2520and%2520feature-shift%2520scenarios%2520%2528QM9%252C%2520ERA5%252C%2520Forbes%25202000%252C%2520and%2520Beijing%2520PM2.5%2529.%2520Across%2520seed%2520sizes%2520nseed%2520%253D%252010-100%252C%2520ATBagging%2520improves%2520or%2520ties%2520early%2520active%2520learning%2520and%2520increases%2520area%2520under%2520the%2520learning-curve%2520relative%2520to%2520alternative%2520seed%2520subset%2520selection%2520methodologies%2520in%2520almost%2520all%2520cases%252C%2520with%2520strongest%2520benefits%2520in%2520low-data%2520regimes.%2520Thus%252C%2520ATBagging%2520provides%2520a%2520low-cost%252C%2520high%2520reward%2520means%2520to%2520initiating%2520active%2520learning-based%2520data%2520collection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02415v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Transfer%20Bagging%3A%20A%20New%20Approach%20for%20Accelerated%20Active%20Learning%20Acquisition%20of%20Data%20by%20Combined%20Transfer%20Learning%20and%20Bagging%20Based%20Models&entry.906535625=Vivienne%20Pelletier%20and%20Daniel%20J.%20Rivera%20and%20Obinna%20Nwokonkwo%20and%20Steven%20A.%20Wilson%20and%20Christopher%20L.%20Muhich&entry.1292438233=Modern%20machine%20learning%20has%20achieved%20remarkable%20success%20on%20many%20problems%2C%20but%20this%20success%20often%20depends%20on%20the%20existence%20of%20large%2C%20labeled%20datasets.%20While%20active%20learning%20can%20dramatically%20reduce%20labeling%20cost%20when%20annotations%20are%20expensive%2C%20early%20performance%20is%20frequently%20dominated%20by%20the%20initial%20seed%20set%2C%20typically%20chosen%20at%20random.%20In%20many%20applications%2C%20however%2C%20related%20or%20approximate%20datasets%20are%20readily%20available%20and%20can%20be%20leveraged%20to%20construct%20a%20better%20seed%20set.%20We%20introduce%20a%20new%20method%20for%20selecting%20the%20seed%20data%20set%20for%20active%20learning%2C%20Active-Transfer%20Bagging%20%28ATBagging%29.%20ATBagging%20estimates%20the%20informativeness%20of%20candidate%20data%20point%20from%20a%20Bayesian%20interpretation%20of%20bagged%20ensemble%20models%20by%20comparing%20in-bag%20and%20out-of-bag%20predictive%20distributions%20from%20the%20labeled%20dataset%2C%20yielding%20an%20information-gain%20proxy.%20To%20avoid%20redundant%20selections%2C%20we%20impose%20feature-space%20diversity%20by%20sampling%20a%20determinantal%20point%20process%20%28DPP%29%20whose%20kernel%20uses%20Random%20Fourier%20Features%20and%20a%20quality-diversity%20factorization%20that%20incorporates%20the%20informativeness%20scores.%20This%20same%20blended%20method%20is%20used%20for%20selection%20of%20new%20data%20points%20to%20collect%20during%20the%20active%20learning%20phase.%20We%20evaluate%20ATBagging%20on%20four%20real-world%20datasets%20covering%20both%20target-transfer%20and%20feature-shift%20scenarios%20%28QM9%2C%20ERA5%2C%20Forbes%202000%2C%20and%20Beijing%20PM2.5%29.%20Across%20seed%20sizes%20nseed%20%3D%2010-100%2C%20ATBagging%20improves%20or%20ties%20early%20active%20learning%20and%20increases%20area%20under%20the%20learning-curve%20relative%20to%20alternative%20seed%20subset%20selection%20methodologies%20in%20almost%20all%20cases%2C%20with%20strongest%20benefits%20in%20low-data%20regimes.%20Thus%2C%20ATBagging%20provides%20a%20low-cost%2C%20high%20reward%20means%20to%20initiating%20active%20learning-based%20data%20collection.&entry.1838667208=http%3A//arxiv.org/abs/2602.02415v1&entry.124074799=Read"},
{"title": "SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations", "author": "Wenhao Yan and Sheng Ye and Zhuoyi Yang and Jiayan Teng and ZhenHui Dong and Kairui Wen and Xiaotao Gu and Yong-Jin Liu and Jie Tang", "abstract": "Achieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from a driving video to a reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity animations. In this work, we present \\textbf{SCAIL} (a framework toward \\textbf{S}tudio-grade \\textbf{C}haracter \\textbf{A}nimation via \\textbf{I}n-context \\textbf{L}earning), a framework designed to address these challenges from two key innovations. First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal. Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation. Experiments show that \\textbf{SCAIL} achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism.", "link": "http://arxiv.org/abs/2512.05905v2", "date": "2026-02-02", "relevancy": 2.6462, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.694}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6682}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCAIL%3A%20Towards%20Studio-Grade%20Character%20Animation%20via%20In-Context%20Learning%20of%203D-Consistent%20Pose%20Representations&body=Title%3A%20SCAIL%3A%20Towards%20Studio-Grade%20Character%20Animation%20via%20In-Context%20Learning%20of%203D-Consistent%20Pose%20Representations%0AAuthor%3A%20Wenhao%20Yan%20and%20Sheng%20Ye%20and%20Zhuoyi%20Yang%20and%20Jiayan%20Teng%20and%20ZhenHui%20Dong%20and%20Kairui%20Wen%20and%20Xiaotao%20Gu%20and%20Yong-Jin%20Liu%20and%20Jie%20Tang%0AAbstract%3A%20Achieving%20character%20animation%20that%20meets%20studio-grade%20production%20standards%20remains%20challenging%20despite%20recent%20progress.%20Existing%20approaches%20can%20transfer%20motion%20from%20a%20driving%20video%20to%20a%20reference%20image%2C%20but%20often%20fail%20to%20preserve%20structural%20fidelity%20and%20temporal%20consistency%20in%20wild%20scenarios%20involving%20complex%20motion%20and%20cross-identity%20animations.%20In%20this%20work%2C%20we%20present%20%5Ctextbf%7BSCAIL%7D%20%28a%20framework%20toward%20%5Ctextbf%7BS%7Dtudio-grade%20%5Ctextbf%7BC%7Dharacter%20%5Ctextbf%7BA%7Dnimation%20via%20%5Ctextbf%7BI%7Dn-context%20%5Ctextbf%7BL%7Dearning%29%2C%20a%20framework%20designed%20to%20address%20these%20challenges%20from%20two%20key%20innovations.%20First%2C%20we%20propose%20a%20novel%203D%20pose%20representation%2C%20providing%20a%20more%20robust%20and%20flexible%20motion%20signal.%20Second%2C%20we%20introduce%20a%20full-context%20pose%20injection%20mechanism%20within%20a%20diffusion-transformer%20architecture%2C%20enabling%20effective%20spatio-temporal%20reasoning%20over%20full%20motion%20sequences.%20To%20align%20with%20studio-level%20requirements%2C%20we%20develop%20a%20curated%20data%20pipeline%20ensuring%20both%20diversity%20and%20quality%2C%20and%20establish%20a%20comprehensive%20benchmark%20for%20systematic%20evaluation.%20Experiments%20show%20that%20%5Ctextbf%7BSCAIL%7D%20achieves%20state-of-the-art%20performance%20and%20advances%20character%20animation%20toward%20studio-grade%20reliability%20and%20realism.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05905v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCAIL%253A%2520Towards%2520Studio-Grade%2520Character%2520Animation%2520via%2520In-Context%2520Learning%2520of%25203D-Consistent%2520Pose%2520Representations%26entry.906535625%3DWenhao%2520Yan%2520and%2520Sheng%2520Ye%2520and%2520Zhuoyi%2520Yang%2520and%2520Jiayan%2520Teng%2520and%2520ZhenHui%2520Dong%2520and%2520Kairui%2520Wen%2520and%2520Xiaotao%2520Gu%2520and%2520Yong-Jin%2520Liu%2520and%2520Jie%2520Tang%26entry.1292438233%3DAchieving%2520character%2520animation%2520that%2520meets%2520studio-grade%2520production%2520standards%2520remains%2520challenging%2520despite%2520recent%2520progress.%2520Existing%2520approaches%2520can%2520transfer%2520motion%2520from%2520a%2520driving%2520video%2520to%2520a%2520reference%2520image%252C%2520but%2520often%2520fail%2520to%2520preserve%2520structural%2520fidelity%2520and%2520temporal%2520consistency%2520in%2520wild%2520scenarios%2520involving%2520complex%2520motion%2520and%2520cross-identity%2520animations.%2520In%2520this%2520work%252C%2520we%2520present%2520%255Ctextbf%257BSCAIL%257D%2520%2528a%2520framework%2520toward%2520%255Ctextbf%257BS%257Dtudio-grade%2520%255Ctextbf%257BC%257Dharacter%2520%255Ctextbf%257BA%257Dnimation%2520via%2520%255Ctextbf%257BI%257Dn-context%2520%255Ctextbf%257BL%257Dearning%2529%252C%2520a%2520framework%2520designed%2520to%2520address%2520these%2520challenges%2520from%2520two%2520key%2520innovations.%2520First%252C%2520we%2520propose%2520a%2520novel%25203D%2520pose%2520representation%252C%2520providing%2520a%2520more%2520robust%2520and%2520flexible%2520motion%2520signal.%2520Second%252C%2520we%2520introduce%2520a%2520full-context%2520pose%2520injection%2520mechanism%2520within%2520a%2520diffusion-transformer%2520architecture%252C%2520enabling%2520effective%2520spatio-temporal%2520reasoning%2520over%2520full%2520motion%2520sequences.%2520To%2520align%2520with%2520studio-level%2520requirements%252C%2520we%2520develop%2520a%2520curated%2520data%2520pipeline%2520ensuring%2520both%2520diversity%2520and%2520quality%252C%2520and%2520establish%2520a%2520comprehensive%2520benchmark%2520for%2520systematic%2520evaluation.%2520Experiments%2520show%2520that%2520%255Ctextbf%257BSCAIL%257D%2520achieves%2520state-of-the-art%2520performance%2520and%2520advances%2520character%2520animation%2520toward%2520studio-grade%2520reliability%2520and%2520realism.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05905v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCAIL%3A%20Towards%20Studio-Grade%20Character%20Animation%20via%20In-Context%20Learning%20of%203D-Consistent%20Pose%20Representations&entry.906535625=Wenhao%20Yan%20and%20Sheng%20Ye%20and%20Zhuoyi%20Yang%20and%20Jiayan%20Teng%20and%20ZhenHui%20Dong%20and%20Kairui%20Wen%20and%20Xiaotao%20Gu%20and%20Yong-Jin%20Liu%20and%20Jie%20Tang&entry.1292438233=Achieving%20character%20animation%20that%20meets%20studio-grade%20production%20standards%20remains%20challenging%20despite%20recent%20progress.%20Existing%20approaches%20can%20transfer%20motion%20from%20a%20driving%20video%20to%20a%20reference%20image%2C%20but%20often%20fail%20to%20preserve%20structural%20fidelity%20and%20temporal%20consistency%20in%20wild%20scenarios%20involving%20complex%20motion%20and%20cross-identity%20animations.%20In%20this%20work%2C%20we%20present%20%5Ctextbf%7BSCAIL%7D%20%28a%20framework%20toward%20%5Ctextbf%7BS%7Dtudio-grade%20%5Ctextbf%7BC%7Dharacter%20%5Ctextbf%7BA%7Dnimation%20via%20%5Ctextbf%7BI%7Dn-context%20%5Ctextbf%7BL%7Dearning%29%2C%20a%20framework%20designed%20to%20address%20these%20challenges%20from%20two%20key%20innovations.%20First%2C%20we%20propose%20a%20novel%203D%20pose%20representation%2C%20providing%20a%20more%20robust%20and%20flexible%20motion%20signal.%20Second%2C%20we%20introduce%20a%20full-context%20pose%20injection%20mechanism%20within%20a%20diffusion-transformer%20architecture%2C%20enabling%20effective%20spatio-temporal%20reasoning%20over%20full%20motion%20sequences.%20To%20align%20with%20studio-level%20requirements%2C%20we%20develop%20a%20curated%20data%20pipeline%20ensuring%20both%20diversity%20and%20quality%2C%20and%20establish%20a%20comprehensive%20benchmark%20for%20systematic%20evaluation.%20Experiments%20show%20that%20%5Ctextbf%7BSCAIL%7D%20achieves%20state-of-the-art%20performance%20and%20advances%20character%20animation%20toward%20studio-grade%20reliability%20and%20realism.&entry.1838667208=http%3A//arxiv.org/abs/2512.05905v2&entry.124074799=Read"},
{"title": "Helios 2.0: A Robust, Ultra-Low Power Gesture Recognition System Optimised for Event-Sensor based Wearables", "author": "Prarthana Bhattacharyya and Joshua Mitton and Ryan Page and Owen Morgan and Oliver Powell and Benjamin Menzies and Gabriel Homewood and Kemi Jacobs and Paolo Baesso and Taru Muhonen and Richard Vigars and Louis Berridge", "abstract": "We present an advance in wearable technology: a mobile-optimized, real-time, ultra-low-power event camera system that enables natural hand gesture control for smart glasses, dramatically improving user experience. While hand gesture recognition in computer vision has advanced significantly, critical challenges remain in creating systems that are intuitive, adaptable across diverse users and environments, and energy-efficient enough for practical wearable applications. Our approach tackles these challenges through carefully selected microgestures: lateral thumb swipes across the index finger (in both directions) and a double pinch between thumb and index fingertips. These human-centered interactions leverage natural hand movements, ensuring intuitive usability without requiring users to learn complex command sequences. To overcome variability in users and environments, we developed a novel simulation methodology that enables comprehensive domain sampling without extensive real-world data collection. Our power-optimised architecture maintains exceptional performance, achieving F1 scores above 80\\% on benchmark datasets featuring diverse users and environments. The resulting models operate at just 6-8 mW when exploiting the Qualcomm Snapdragon Hexagon DSP, with our 2-channel implementation exceeding 70\\% F1 accuracy and our 6-channel model surpassing 80\\% F1 accuracy across all gesture classes in user studies. These results were achieved using only synthetic training data. This improves on the state-of-the-art for F1 accuracy by 20\\% with a power reduction 25x when using DSP. This advancement brings deploying ultra-low-power vision systems in wearable devices closer and opens new possibilities for seamless human-computer interaction.", "link": "http://arxiv.org/abs/2503.07825v2", "date": "2026-02-02", "relevancy": 2.6371, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5532}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5165}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Helios%202.0%3A%20A%20Robust%2C%20Ultra-Low%20Power%20Gesture%20Recognition%20System%20Optimised%20for%20Event-Sensor%20based%20Wearables&body=Title%3A%20Helios%202.0%3A%20A%20Robust%2C%20Ultra-Low%20Power%20Gesture%20Recognition%20System%20Optimised%20for%20Event-Sensor%20based%20Wearables%0AAuthor%3A%20Prarthana%20Bhattacharyya%20and%20Joshua%20Mitton%20and%20Ryan%20Page%20and%20Owen%20Morgan%20and%20Oliver%20Powell%20and%20Benjamin%20Menzies%20and%20Gabriel%20Homewood%20and%20Kemi%20Jacobs%20and%20Paolo%20Baesso%20and%20Taru%20Muhonen%20and%20Richard%20Vigars%20and%20Louis%20Berridge%0AAbstract%3A%20We%20present%20an%20advance%20in%20wearable%20technology%3A%20a%20mobile-optimized%2C%20real-time%2C%20ultra-low-power%20event%20camera%20system%20that%20enables%20natural%20hand%20gesture%20control%20for%20smart%20glasses%2C%20dramatically%20improving%20user%20experience.%20While%20hand%20gesture%20recognition%20in%20computer%20vision%20has%20advanced%20significantly%2C%20critical%20challenges%20remain%20in%20creating%20systems%20that%20are%20intuitive%2C%20adaptable%20across%20diverse%20users%20and%20environments%2C%20and%20energy-efficient%20enough%20for%20practical%20wearable%20applications.%20Our%20approach%20tackles%20these%20challenges%20through%20carefully%20selected%20microgestures%3A%20lateral%20thumb%20swipes%20across%20the%20index%20finger%20%28in%20both%20directions%29%20and%20a%20double%20pinch%20between%20thumb%20and%20index%20fingertips.%20These%20human-centered%20interactions%20leverage%20natural%20hand%20movements%2C%20ensuring%20intuitive%20usability%20without%20requiring%20users%20to%20learn%20complex%20command%20sequences.%20To%20overcome%20variability%20in%20users%20and%20environments%2C%20we%20developed%20a%20novel%20simulation%20methodology%20that%20enables%20comprehensive%20domain%20sampling%20without%20extensive%20real-world%20data%20collection.%20Our%20power-optimised%20architecture%20maintains%20exceptional%20performance%2C%20achieving%20F1%20scores%20above%2080%5C%25%20on%20benchmark%20datasets%20featuring%20diverse%20users%20and%20environments.%20The%20resulting%20models%20operate%20at%20just%206-8%20mW%20when%20exploiting%20the%20Qualcomm%20Snapdragon%20Hexagon%20DSP%2C%20with%20our%202-channel%20implementation%20exceeding%2070%5C%25%20F1%20accuracy%20and%20our%206-channel%20model%20surpassing%2080%5C%25%20F1%20accuracy%20across%20all%20gesture%20classes%20in%20user%20studies.%20These%20results%20were%20achieved%20using%20only%20synthetic%20training%20data.%20This%20improves%20on%20the%20state-of-the-art%20for%20F1%20accuracy%20by%2020%5C%25%20with%20a%20power%20reduction%2025x%20when%20using%20DSP.%20This%20advancement%20brings%20deploying%20ultra-low-power%20vision%20systems%20in%20wearable%20devices%20closer%20and%20opens%20new%20possibilities%20for%20seamless%20human-computer%20interaction.%0ALink%3A%20http%3A//arxiv.org/abs/2503.07825v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHelios%25202.0%253A%2520A%2520Robust%252C%2520Ultra-Low%2520Power%2520Gesture%2520Recognition%2520System%2520Optimised%2520for%2520Event-Sensor%2520based%2520Wearables%26entry.906535625%3DPrarthana%2520Bhattacharyya%2520and%2520Joshua%2520Mitton%2520and%2520Ryan%2520Page%2520and%2520Owen%2520Morgan%2520and%2520Oliver%2520Powell%2520and%2520Benjamin%2520Menzies%2520and%2520Gabriel%2520Homewood%2520and%2520Kemi%2520Jacobs%2520and%2520Paolo%2520Baesso%2520and%2520Taru%2520Muhonen%2520and%2520Richard%2520Vigars%2520and%2520Louis%2520Berridge%26entry.1292438233%3DWe%2520present%2520an%2520advance%2520in%2520wearable%2520technology%253A%2520a%2520mobile-optimized%252C%2520real-time%252C%2520ultra-low-power%2520event%2520camera%2520system%2520that%2520enables%2520natural%2520hand%2520gesture%2520control%2520for%2520smart%2520glasses%252C%2520dramatically%2520improving%2520user%2520experience.%2520While%2520hand%2520gesture%2520recognition%2520in%2520computer%2520vision%2520has%2520advanced%2520significantly%252C%2520critical%2520challenges%2520remain%2520in%2520creating%2520systems%2520that%2520are%2520intuitive%252C%2520adaptable%2520across%2520diverse%2520users%2520and%2520environments%252C%2520and%2520energy-efficient%2520enough%2520for%2520practical%2520wearable%2520applications.%2520Our%2520approach%2520tackles%2520these%2520challenges%2520through%2520carefully%2520selected%2520microgestures%253A%2520lateral%2520thumb%2520swipes%2520across%2520the%2520index%2520finger%2520%2528in%2520both%2520directions%2529%2520and%2520a%2520double%2520pinch%2520between%2520thumb%2520and%2520index%2520fingertips.%2520These%2520human-centered%2520interactions%2520leverage%2520natural%2520hand%2520movements%252C%2520ensuring%2520intuitive%2520usability%2520without%2520requiring%2520users%2520to%2520learn%2520complex%2520command%2520sequences.%2520To%2520overcome%2520variability%2520in%2520users%2520and%2520environments%252C%2520we%2520developed%2520a%2520novel%2520simulation%2520methodology%2520that%2520enables%2520comprehensive%2520domain%2520sampling%2520without%2520extensive%2520real-world%2520data%2520collection.%2520Our%2520power-optimised%2520architecture%2520maintains%2520exceptional%2520performance%252C%2520achieving%2520F1%2520scores%2520above%252080%255C%2525%2520on%2520benchmark%2520datasets%2520featuring%2520diverse%2520users%2520and%2520environments.%2520The%2520resulting%2520models%2520operate%2520at%2520just%25206-8%2520mW%2520when%2520exploiting%2520the%2520Qualcomm%2520Snapdragon%2520Hexagon%2520DSP%252C%2520with%2520our%25202-channel%2520implementation%2520exceeding%252070%255C%2525%2520F1%2520accuracy%2520and%2520our%25206-channel%2520model%2520surpassing%252080%255C%2525%2520F1%2520accuracy%2520across%2520all%2520gesture%2520classes%2520in%2520user%2520studies.%2520These%2520results%2520were%2520achieved%2520using%2520only%2520synthetic%2520training%2520data.%2520This%2520improves%2520on%2520the%2520state-of-the-art%2520for%2520F1%2520accuracy%2520by%252020%255C%2525%2520with%2520a%2520power%2520reduction%252025x%2520when%2520using%2520DSP.%2520This%2520advancement%2520brings%2520deploying%2520ultra-low-power%2520vision%2520systems%2520in%2520wearable%2520devices%2520closer%2520and%2520opens%2520new%2520possibilities%2520for%2520seamless%2520human-computer%2520interaction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07825v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Helios%202.0%3A%20A%20Robust%2C%20Ultra-Low%20Power%20Gesture%20Recognition%20System%20Optimised%20for%20Event-Sensor%20based%20Wearables&entry.906535625=Prarthana%20Bhattacharyya%20and%20Joshua%20Mitton%20and%20Ryan%20Page%20and%20Owen%20Morgan%20and%20Oliver%20Powell%20and%20Benjamin%20Menzies%20and%20Gabriel%20Homewood%20and%20Kemi%20Jacobs%20and%20Paolo%20Baesso%20and%20Taru%20Muhonen%20and%20Richard%20Vigars%20and%20Louis%20Berridge&entry.1292438233=We%20present%20an%20advance%20in%20wearable%20technology%3A%20a%20mobile-optimized%2C%20real-time%2C%20ultra-low-power%20event%20camera%20system%20that%20enables%20natural%20hand%20gesture%20control%20for%20smart%20glasses%2C%20dramatically%20improving%20user%20experience.%20While%20hand%20gesture%20recognition%20in%20computer%20vision%20has%20advanced%20significantly%2C%20critical%20challenges%20remain%20in%20creating%20systems%20that%20are%20intuitive%2C%20adaptable%20across%20diverse%20users%20and%20environments%2C%20and%20energy-efficient%20enough%20for%20practical%20wearable%20applications.%20Our%20approach%20tackles%20these%20challenges%20through%20carefully%20selected%20microgestures%3A%20lateral%20thumb%20swipes%20across%20the%20index%20finger%20%28in%20both%20directions%29%20and%20a%20double%20pinch%20between%20thumb%20and%20index%20fingertips.%20These%20human-centered%20interactions%20leverage%20natural%20hand%20movements%2C%20ensuring%20intuitive%20usability%20without%20requiring%20users%20to%20learn%20complex%20command%20sequences.%20To%20overcome%20variability%20in%20users%20and%20environments%2C%20we%20developed%20a%20novel%20simulation%20methodology%20that%20enables%20comprehensive%20domain%20sampling%20without%20extensive%20real-world%20data%20collection.%20Our%20power-optimised%20architecture%20maintains%20exceptional%20performance%2C%20achieving%20F1%20scores%20above%2080%5C%25%20on%20benchmark%20datasets%20featuring%20diverse%20users%20and%20environments.%20The%20resulting%20models%20operate%20at%20just%206-8%20mW%20when%20exploiting%20the%20Qualcomm%20Snapdragon%20Hexagon%20DSP%2C%20with%20our%202-channel%20implementation%20exceeding%2070%5C%25%20F1%20accuracy%20and%20our%206-channel%20model%20surpassing%2080%5C%25%20F1%20accuracy%20across%20all%20gesture%20classes%20in%20user%20studies.%20These%20results%20were%20achieved%20using%20only%20synthetic%20training%20data.%20This%20improves%20on%20the%20state-of-the-art%20for%20F1%20accuracy%20by%2020%5C%25%20with%20a%20power%20reduction%2025x%20when%20using%20DSP.%20This%20advancement%20brings%20deploying%20ultra-low-power%20vision%20systems%20in%20wearable%20devices%20closer%20and%20opens%20new%20possibilities%20for%20seamless%20human-computer%20interaction.&entry.1838667208=http%3A//arxiv.org/abs/2503.07825v2&entry.124074799=Read"},
{"title": "Implicit neural representation of textures", "author": "Albert Kwok and Zheyuan Hu and Dounia Hammou", "abstract": "Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space. Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality, with considerable memory usage and rendering inference time. We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INR-space generation.", "link": "http://arxiv.org/abs/2602.02354v1", "date": "2026-02-02", "relevancy": 2.6294, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5578}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5294}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20neural%20representation%20of%20textures&body=Title%3A%20Implicit%20neural%20representation%20of%20textures%0AAuthor%3A%20Albert%20Kwok%20and%20Zheyuan%20Hu%20and%20Dounia%20Hammou%0AAbstract%3A%20Implicit%20neural%20representation%20%28INR%29%20has%20proven%20to%20be%20accurate%20and%20efficient%20in%20various%20domains.%20In%20this%20work%2C%20we%20explore%20how%20different%20neural%20networks%20can%20be%20designed%20as%20a%20new%20texture%20INR%2C%20which%20operates%20in%20a%20continuous%20manner%20rather%20than%20a%20discrete%20one%20over%20the%20input%20UV%20coordinate%20space.%20Through%20thorough%20experiments%2C%20we%20demonstrate%20that%20these%20INRs%20perform%20well%20in%20terms%20of%20image%20quality%2C%20with%20considerable%20memory%20usage%20and%20rendering%20inference%20time.%20We%20analyze%20the%20balance%20between%20these%20objectives.%20In%20addition%2C%20we%20investigate%20various%20related%20applications%20in%20real-time%20rendering%20and%20down-stream%20tasks%2C%20e.g.%20mipmap%20fitting%20and%20INR-space%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02354v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520neural%2520representation%2520of%2520textures%26entry.906535625%3DAlbert%2520Kwok%2520and%2520Zheyuan%2520Hu%2520and%2520Dounia%2520Hammou%26entry.1292438233%3DImplicit%2520neural%2520representation%2520%2528INR%2529%2520has%2520proven%2520to%2520be%2520accurate%2520and%2520efficient%2520in%2520various%2520domains.%2520In%2520this%2520work%252C%2520we%2520explore%2520how%2520different%2520neural%2520networks%2520can%2520be%2520designed%2520as%2520a%2520new%2520texture%2520INR%252C%2520which%2520operates%2520in%2520a%2520continuous%2520manner%2520rather%2520than%2520a%2520discrete%2520one%2520over%2520the%2520input%2520UV%2520coordinate%2520space.%2520Through%2520thorough%2520experiments%252C%2520we%2520demonstrate%2520that%2520these%2520INRs%2520perform%2520well%2520in%2520terms%2520of%2520image%2520quality%252C%2520with%2520considerable%2520memory%2520usage%2520and%2520rendering%2520inference%2520time.%2520We%2520analyze%2520the%2520balance%2520between%2520these%2520objectives.%2520In%2520addition%252C%2520we%2520investigate%2520various%2520related%2520applications%2520in%2520real-time%2520rendering%2520and%2520down-stream%2520tasks%252C%2520e.g.%2520mipmap%2520fitting%2520and%2520INR-space%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02354v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20neural%20representation%20of%20textures&entry.906535625=Albert%20Kwok%20and%20Zheyuan%20Hu%20and%20Dounia%20Hammou&entry.1292438233=Implicit%20neural%20representation%20%28INR%29%20has%20proven%20to%20be%20accurate%20and%20efficient%20in%20various%20domains.%20In%20this%20work%2C%20we%20explore%20how%20different%20neural%20networks%20can%20be%20designed%20as%20a%20new%20texture%20INR%2C%20which%20operates%20in%20a%20continuous%20manner%20rather%20than%20a%20discrete%20one%20over%20the%20input%20UV%20coordinate%20space.%20Through%20thorough%20experiments%2C%20we%20demonstrate%20that%20these%20INRs%20perform%20well%20in%20terms%20of%20image%20quality%2C%20with%20considerable%20memory%20usage%20and%20rendering%20inference%20time.%20We%20analyze%20the%20balance%20between%20these%20objectives.%20In%20addition%2C%20we%20investigate%20various%20related%20applications%20in%20real-time%20rendering%20and%20down-stream%20tasks%2C%20e.g.%20mipmap%20fitting%20and%20INR-space%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2602.02354v1&entry.124074799=Read"},
{"title": "Hybrid Lie semi-group and cascade structures for the generalized Gaussian derivative model for visual receptive fields", "author": "Tony Lindeberg", "abstract": "Because of the variabilities of real-world image structures under the natural image transformations that arise when observing similar objects or spatio-temporal events under different viewing conditions, the receptive field responses computed in the earliest layers of the visual hierarchy may be strongly influenced by such geometric image transformations. One way of handling this variability is by basing the vision system on covariant receptive field families, which expand the receptive field shapes over the degrees of freedom in the image transformations.\n  This paper addresses the problem of deriving relationships between spatial and spatio-temporal receptive field responses obtained for different values of the shape parameters in the resulting multi-parameter families of receptive fields. For this purpose, we derive both (i) infinitesimal relationships, roughly corresponding to a combination of notions from semi-groups and Lie groups, as well as (ii) macroscopic cascade smoothing properties, which describe how receptive field responses at coarser spatial and temporal scales can be computed by applying smaller support incremental filters to the output from corresponding receptive fields at finer spatial and temporal scales, structurally related to the notion of Lie algebras, although with directional preferences.\n  The presented results provide (i) a deeper understanding of the relationships between spatial and spatio-temporal receptive field responses for different values of the filter parameters, which can be used for both (ii) designing more efficient schemes for computing receptive field responses over populations of multi-parameter families of receptive fields, as well as (iii)~formulating idealized theoretical models of the computations of simple cells in biological vision.", "link": "http://arxiv.org/abs/2509.15748v2", "date": "2026-02-02", "relevancy": 2.6285, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.549}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.517}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Lie%20semi-group%20and%20cascade%20structures%20for%20the%20generalized%20Gaussian%20derivative%20model%20for%20visual%20receptive%20fields&body=Title%3A%20Hybrid%20Lie%20semi-group%20and%20cascade%20structures%20for%20the%20generalized%20Gaussian%20derivative%20model%20for%20visual%20receptive%20fields%0AAuthor%3A%20Tony%20Lindeberg%0AAbstract%3A%20Because%20of%20the%20variabilities%20of%20real-world%20image%20structures%20under%20the%20natural%20image%20transformations%20that%20arise%20when%20observing%20similar%20objects%20or%20spatio-temporal%20events%20under%20different%20viewing%20conditions%2C%20the%20receptive%20field%20responses%20computed%20in%20the%20earliest%20layers%20of%20the%20visual%20hierarchy%20may%20be%20strongly%20influenced%20by%20such%20geometric%20image%20transformations.%20One%20way%20of%20handling%20this%20variability%20is%20by%20basing%20the%20vision%20system%20on%20covariant%20receptive%20field%20families%2C%20which%20expand%20the%20receptive%20field%20shapes%20over%20the%20degrees%20of%20freedom%20in%20the%20image%20transformations.%0A%20%20This%20paper%20addresses%20the%20problem%20of%20deriving%20relationships%20between%20spatial%20and%20spatio-temporal%20receptive%20field%20responses%20obtained%20for%20different%20values%20of%20the%20shape%20parameters%20in%20the%20resulting%20multi-parameter%20families%20of%20receptive%20fields.%20For%20this%20purpose%2C%20we%20derive%20both%20%28i%29%20infinitesimal%20relationships%2C%20roughly%20corresponding%20to%20a%20combination%20of%20notions%20from%20semi-groups%20and%20Lie%20groups%2C%20as%20well%20as%20%28ii%29%20macroscopic%20cascade%20smoothing%20properties%2C%20which%20describe%20how%20receptive%20field%20responses%20at%20coarser%20spatial%20and%20temporal%20scales%20can%20be%20computed%20by%20applying%20smaller%20support%20incremental%20filters%20to%20the%20output%20from%20corresponding%20receptive%20fields%20at%20finer%20spatial%20and%20temporal%20scales%2C%20structurally%20related%20to%20the%20notion%20of%20Lie%20algebras%2C%20although%20with%20directional%20preferences.%0A%20%20The%20presented%20results%20provide%20%28i%29%20a%20deeper%20understanding%20of%20the%20relationships%20between%20spatial%20and%20spatio-temporal%20receptive%20field%20responses%20for%20different%20values%20of%20the%20filter%20parameters%2C%20which%20can%20be%20used%20for%20both%20%28ii%29%20designing%20more%20efficient%20schemes%20for%20computing%20receptive%20field%20responses%20over%20populations%20of%20multi-parameter%20families%20of%20receptive%20fields%2C%20as%20well%20as%20%28iii%29~formulating%20idealized%20theoretical%20models%20of%20the%20computations%20of%20simple%20cells%20in%20biological%20vision.%0ALink%3A%20http%3A//arxiv.org/abs/2509.15748v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Lie%2520semi-group%2520and%2520cascade%2520structures%2520for%2520the%2520generalized%2520Gaussian%2520derivative%2520model%2520for%2520visual%2520receptive%2520fields%26entry.906535625%3DTony%2520Lindeberg%26entry.1292438233%3DBecause%2520of%2520the%2520variabilities%2520of%2520real-world%2520image%2520structures%2520under%2520the%2520natural%2520image%2520transformations%2520that%2520arise%2520when%2520observing%2520similar%2520objects%2520or%2520spatio-temporal%2520events%2520under%2520different%2520viewing%2520conditions%252C%2520the%2520receptive%2520field%2520responses%2520computed%2520in%2520the%2520earliest%2520layers%2520of%2520the%2520visual%2520hierarchy%2520may%2520be%2520strongly%2520influenced%2520by%2520such%2520geometric%2520image%2520transformations.%2520One%2520way%2520of%2520handling%2520this%2520variability%2520is%2520by%2520basing%2520the%2520vision%2520system%2520on%2520covariant%2520receptive%2520field%2520families%252C%2520which%2520expand%2520the%2520receptive%2520field%2520shapes%2520over%2520the%2520degrees%2520of%2520freedom%2520in%2520the%2520image%2520transformations.%250A%2520%2520This%2520paper%2520addresses%2520the%2520problem%2520of%2520deriving%2520relationships%2520between%2520spatial%2520and%2520spatio-temporal%2520receptive%2520field%2520responses%2520obtained%2520for%2520different%2520values%2520of%2520the%2520shape%2520parameters%2520in%2520the%2520resulting%2520multi-parameter%2520families%2520of%2520receptive%2520fields.%2520For%2520this%2520purpose%252C%2520we%2520derive%2520both%2520%2528i%2529%2520infinitesimal%2520relationships%252C%2520roughly%2520corresponding%2520to%2520a%2520combination%2520of%2520notions%2520from%2520semi-groups%2520and%2520Lie%2520groups%252C%2520as%2520well%2520as%2520%2528ii%2529%2520macroscopic%2520cascade%2520smoothing%2520properties%252C%2520which%2520describe%2520how%2520receptive%2520field%2520responses%2520at%2520coarser%2520spatial%2520and%2520temporal%2520scales%2520can%2520be%2520computed%2520by%2520applying%2520smaller%2520support%2520incremental%2520filters%2520to%2520the%2520output%2520from%2520corresponding%2520receptive%2520fields%2520at%2520finer%2520spatial%2520and%2520temporal%2520scales%252C%2520structurally%2520related%2520to%2520the%2520notion%2520of%2520Lie%2520algebras%252C%2520although%2520with%2520directional%2520preferences.%250A%2520%2520The%2520presented%2520results%2520provide%2520%2528i%2529%2520a%2520deeper%2520understanding%2520of%2520the%2520relationships%2520between%2520spatial%2520and%2520spatio-temporal%2520receptive%2520field%2520responses%2520for%2520different%2520values%2520of%2520the%2520filter%2520parameters%252C%2520which%2520can%2520be%2520used%2520for%2520both%2520%2528ii%2529%2520designing%2520more%2520efficient%2520schemes%2520for%2520computing%2520receptive%2520field%2520responses%2520over%2520populations%2520of%2520multi-parameter%2520families%2520of%2520receptive%2520fields%252C%2520as%2520well%2520as%2520%2528iii%2529~formulating%2520idealized%2520theoretical%2520models%2520of%2520the%2520computations%2520of%2520simple%2520cells%2520in%2520biological%2520vision.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15748v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Lie%20semi-group%20and%20cascade%20structures%20for%20the%20generalized%20Gaussian%20derivative%20model%20for%20visual%20receptive%20fields&entry.906535625=Tony%20Lindeberg&entry.1292438233=Because%20of%20the%20variabilities%20of%20real-world%20image%20structures%20under%20the%20natural%20image%20transformations%20that%20arise%20when%20observing%20similar%20objects%20or%20spatio-temporal%20events%20under%20different%20viewing%20conditions%2C%20the%20receptive%20field%20responses%20computed%20in%20the%20earliest%20layers%20of%20the%20visual%20hierarchy%20may%20be%20strongly%20influenced%20by%20such%20geometric%20image%20transformations.%20One%20way%20of%20handling%20this%20variability%20is%20by%20basing%20the%20vision%20system%20on%20covariant%20receptive%20field%20families%2C%20which%20expand%20the%20receptive%20field%20shapes%20over%20the%20degrees%20of%20freedom%20in%20the%20image%20transformations.%0A%20%20This%20paper%20addresses%20the%20problem%20of%20deriving%20relationships%20between%20spatial%20and%20spatio-temporal%20receptive%20field%20responses%20obtained%20for%20different%20values%20of%20the%20shape%20parameters%20in%20the%20resulting%20multi-parameter%20families%20of%20receptive%20fields.%20For%20this%20purpose%2C%20we%20derive%20both%20%28i%29%20infinitesimal%20relationships%2C%20roughly%20corresponding%20to%20a%20combination%20of%20notions%20from%20semi-groups%20and%20Lie%20groups%2C%20as%20well%20as%20%28ii%29%20macroscopic%20cascade%20smoothing%20properties%2C%20which%20describe%20how%20receptive%20field%20responses%20at%20coarser%20spatial%20and%20temporal%20scales%20can%20be%20computed%20by%20applying%20smaller%20support%20incremental%20filters%20to%20the%20output%20from%20corresponding%20receptive%20fields%20at%20finer%20spatial%20and%20temporal%20scales%2C%20structurally%20related%20to%20the%20notion%20of%20Lie%20algebras%2C%20although%20with%20directional%20preferences.%0A%20%20The%20presented%20results%20provide%20%28i%29%20a%20deeper%20understanding%20of%20the%20relationships%20between%20spatial%20and%20spatio-temporal%20receptive%20field%20responses%20for%20different%20values%20of%20the%20filter%20parameters%2C%20which%20can%20be%20used%20for%20both%20%28ii%29%20designing%20more%20efficient%20schemes%20for%20computing%20receptive%20field%20responses%20over%20populations%20of%20multi-parameter%20families%20of%20receptive%20fields%2C%20as%20well%20as%20%28iii%29~formulating%20idealized%20theoretical%20models%20of%20the%20computations%20of%20simple%20cells%20in%20biological%20vision.&entry.1838667208=http%3A//arxiv.org/abs/2509.15748v2&entry.124074799=Read"},
{"title": "AI-Based Stroke Rehabilitation Domiciliary Assessment System with ST_GCN Attention", "author": "Suhyeon Lim and Ye-eun Kim and Andrew J. Choi", "abstract": "Effective stroke recovery requires continuous rehabilitation integrated with daily living. To support this need, we propose a home-based rehabilitation exercise and feedback system. The system consists of (1) hardware setup with RGB-D camera and wearable sensors to capture stroke movements, (2) a mobile application for exercise guidance, and (3) an AI server for assessment and feedback. When a stroke user exercises following the application guidance, the system records skeleton sequences, which are then assessed by the deep learning model, RAST-G@ (Rehabilitation Assessment Spatio-Temporal Graph ATtention). The model employs a spatio-temporal graph convolutional network to extract skeletal features and integrates transformer-based temporal attention to figure out action quality. For system implementation, we constructed the NRC dataset, include 10 upper-limb activities of daily living (ADL) and 5 range-of-motion (ROM) collected from stroke and non-disabled participants, with Score annotations provided by licensed physiotherapists. Results on the KIMORE and NRC datasets show that RAST-G@ improves over baseline in terms of MAD, RMSE, and MAPE. Furthermore, the system provides user feedback that combines patient-centered assessment and monitoring. The results demonstrate that the proposed system offers a scalable approach for quantitative and consistent domiciliary rehabilitation assessment.", "link": "http://arxiv.org/abs/2510.00049v2", "date": "2026-02-02", "relevancy": 2.6267, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5561}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5221}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-Based%20Stroke%20Rehabilitation%20Domiciliary%20Assessment%20System%20with%20ST_GCN%20Attention&body=Title%3A%20AI-Based%20Stroke%20Rehabilitation%20Domiciliary%20Assessment%20System%20with%20ST_GCN%20Attention%0AAuthor%3A%20Suhyeon%20Lim%20and%20Ye-eun%20Kim%20and%20Andrew%20J.%20Choi%0AAbstract%3A%20Effective%20stroke%20recovery%20requires%20continuous%20rehabilitation%20integrated%20with%20daily%20living.%20To%20support%20this%20need%2C%20we%20propose%20a%20home-based%20rehabilitation%20exercise%20and%20feedback%20system.%20The%20system%20consists%20of%20%281%29%20hardware%20setup%20with%20RGB-D%20camera%20and%20wearable%20sensors%20to%20capture%20stroke%20movements%2C%20%282%29%20a%20mobile%20application%20for%20exercise%20guidance%2C%20and%20%283%29%20an%20AI%20server%20for%20assessment%20and%20feedback.%20When%20a%20stroke%20user%20exercises%20following%20the%20application%20guidance%2C%20the%20system%20records%20skeleton%20sequences%2C%20which%20are%20then%20assessed%20by%20the%20deep%20learning%20model%2C%20RAST-G%40%20%28Rehabilitation%20Assessment%20Spatio-Temporal%20Graph%20ATtention%29.%20The%20model%20employs%20a%20spatio-temporal%20graph%20convolutional%20network%20to%20extract%20skeletal%20features%20and%20integrates%20transformer-based%20temporal%20attention%20to%20figure%20out%20action%20quality.%20For%20system%20implementation%2C%20we%20constructed%20the%20NRC%20dataset%2C%20include%2010%20upper-limb%20activities%20of%20daily%20living%20%28ADL%29%20and%205%20range-of-motion%20%28ROM%29%20collected%20from%20stroke%20and%20non-disabled%20participants%2C%20with%20Score%20annotations%20provided%20by%20licensed%20physiotherapists.%20Results%20on%20the%20KIMORE%20and%20NRC%20datasets%20show%20that%20RAST-G%40%20improves%20over%20baseline%20in%20terms%20of%20MAD%2C%20RMSE%2C%20and%20MAPE.%20Furthermore%2C%20the%20system%20provides%20user%20feedback%20that%20combines%20patient-centered%20assessment%20and%20monitoring.%20The%20results%20demonstrate%20that%20the%20proposed%20system%20offers%20a%20scalable%20approach%20for%20quantitative%20and%20consistent%20domiciliary%20rehabilitation%20assessment.%0ALink%3A%20http%3A//arxiv.org/abs/2510.00049v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-Based%2520Stroke%2520Rehabilitation%2520Domiciliary%2520Assessment%2520System%2520with%2520ST_GCN%2520Attention%26entry.906535625%3DSuhyeon%2520Lim%2520and%2520Ye-eun%2520Kim%2520and%2520Andrew%2520J.%2520Choi%26entry.1292438233%3DEffective%2520stroke%2520recovery%2520requires%2520continuous%2520rehabilitation%2520integrated%2520with%2520daily%2520living.%2520To%2520support%2520this%2520need%252C%2520we%2520propose%2520a%2520home-based%2520rehabilitation%2520exercise%2520and%2520feedback%2520system.%2520The%2520system%2520consists%2520of%2520%25281%2529%2520hardware%2520setup%2520with%2520RGB-D%2520camera%2520and%2520wearable%2520sensors%2520to%2520capture%2520stroke%2520movements%252C%2520%25282%2529%2520a%2520mobile%2520application%2520for%2520exercise%2520guidance%252C%2520and%2520%25283%2529%2520an%2520AI%2520server%2520for%2520assessment%2520and%2520feedback.%2520When%2520a%2520stroke%2520user%2520exercises%2520following%2520the%2520application%2520guidance%252C%2520the%2520system%2520records%2520skeleton%2520sequences%252C%2520which%2520are%2520then%2520assessed%2520by%2520the%2520deep%2520learning%2520model%252C%2520RAST-G%2540%2520%2528Rehabilitation%2520Assessment%2520Spatio-Temporal%2520Graph%2520ATtention%2529.%2520The%2520model%2520employs%2520a%2520spatio-temporal%2520graph%2520convolutional%2520network%2520to%2520extract%2520skeletal%2520features%2520and%2520integrates%2520transformer-based%2520temporal%2520attention%2520to%2520figure%2520out%2520action%2520quality.%2520For%2520system%2520implementation%252C%2520we%2520constructed%2520the%2520NRC%2520dataset%252C%2520include%252010%2520upper-limb%2520activities%2520of%2520daily%2520living%2520%2528ADL%2529%2520and%25205%2520range-of-motion%2520%2528ROM%2529%2520collected%2520from%2520stroke%2520and%2520non-disabled%2520participants%252C%2520with%2520Score%2520annotations%2520provided%2520by%2520licensed%2520physiotherapists.%2520Results%2520on%2520the%2520KIMORE%2520and%2520NRC%2520datasets%2520show%2520that%2520RAST-G%2540%2520improves%2520over%2520baseline%2520in%2520terms%2520of%2520MAD%252C%2520RMSE%252C%2520and%2520MAPE.%2520Furthermore%252C%2520the%2520system%2520provides%2520user%2520feedback%2520that%2520combines%2520patient-centered%2520assessment%2520and%2520monitoring.%2520The%2520results%2520demonstrate%2520that%2520the%2520proposed%2520system%2520offers%2520a%2520scalable%2520approach%2520for%2520quantitative%2520and%2520consistent%2520domiciliary%2520rehabilitation%2520assessment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.00049v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Based%20Stroke%20Rehabilitation%20Domiciliary%20Assessment%20System%20with%20ST_GCN%20Attention&entry.906535625=Suhyeon%20Lim%20and%20Ye-eun%20Kim%20and%20Andrew%20J.%20Choi&entry.1292438233=Effective%20stroke%20recovery%20requires%20continuous%20rehabilitation%20integrated%20with%20daily%20living.%20To%20support%20this%20need%2C%20we%20propose%20a%20home-based%20rehabilitation%20exercise%20and%20feedback%20system.%20The%20system%20consists%20of%20%281%29%20hardware%20setup%20with%20RGB-D%20camera%20and%20wearable%20sensors%20to%20capture%20stroke%20movements%2C%20%282%29%20a%20mobile%20application%20for%20exercise%20guidance%2C%20and%20%283%29%20an%20AI%20server%20for%20assessment%20and%20feedback.%20When%20a%20stroke%20user%20exercises%20following%20the%20application%20guidance%2C%20the%20system%20records%20skeleton%20sequences%2C%20which%20are%20then%20assessed%20by%20the%20deep%20learning%20model%2C%20RAST-G%40%20%28Rehabilitation%20Assessment%20Spatio-Temporal%20Graph%20ATtention%29.%20The%20model%20employs%20a%20spatio-temporal%20graph%20convolutional%20network%20to%20extract%20skeletal%20features%20and%20integrates%20transformer-based%20temporal%20attention%20to%20figure%20out%20action%20quality.%20For%20system%20implementation%2C%20we%20constructed%20the%20NRC%20dataset%2C%20include%2010%20upper-limb%20activities%20of%20daily%20living%20%28ADL%29%20and%205%20range-of-motion%20%28ROM%29%20collected%20from%20stroke%20and%20non-disabled%20participants%2C%20with%20Score%20annotations%20provided%20by%20licensed%20physiotherapists.%20Results%20on%20the%20KIMORE%20and%20NRC%20datasets%20show%20that%20RAST-G%40%20improves%20over%20baseline%20in%20terms%20of%20MAD%2C%20RMSE%2C%20and%20MAPE.%20Furthermore%2C%20the%20system%20provides%20user%20feedback%20that%20combines%20patient-centered%20assessment%20and%20monitoring.%20The%20results%20demonstrate%20that%20the%20proposed%20system%20offers%20a%20scalable%20approach%20for%20quantitative%20and%20consistent%20domiciliary%20rehabilitation%20assessment.&entry.1838667208=http%3A//arxiv.org/abs/2510.00049v2&entry.124074799=Read"},
{"title": "Self-Supervised Learning from Structural Invariance", "author": "Yipeng Zhang and Hafez Ghaemi and Jungyoon Lee and Shahab Bakhtiari and Eilif B. Muller and Laurent Charlin", "abstract": "Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs. We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets. This arises when data pairs come from naturally occurring generative processes, e.g., successive video frames. We show that existing methods struggle to flexibly capture this conditional uncertainty. As a remedy, we introduce a latent variable to account for this uncertainty and derive a variational lower bound on the mutual information between paired embeddings. Our derivation yields a simple regularization term for standard SSL objectives. The resulting method, which we call AdaSSL, applies to both contrastive and distillation-based SSL objectives, and we empirically show its versatility in causal representation learning, fine-grained image understanding, and world modeling on videos.", "link": "http://arxiv.org/abs/2602.02381v1", "date": "2026-02-02", "relevancy": 2.6263, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5488}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5212}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20from%20Structural%20Invariance&body=Title%3A%20Self-Supervised%20Learning%20from%20Structural%20Invariance%0AAuthor%3A%20Yipeng%20Zhang%20and%20Hafez%20Ghaemi%20and%20Jungyoon%20Lee%20and%20Shahab%20Bakhtiari%20and%20Eilif%20B.%20Muller%20and%20Laurent%20Charlin%0AAbstract%3A%20Joint-embedding%20self-supervised%20learning%20%28SSL%29%2C%20the%20key%20paradigm%20for%20unsupervised%20representation%20learning%20from%20visual%20data%2C%20learns%20from%20invariances%20between%20semantically-related%20data%20pairs.%20We%20study%20the%20one-to-many%20mapping%20problem%20in%20SSL%2C%20where%20each%20datum%20may%20be%20mapped%20to%20multiple%20valid%20targets.%20This%20arises%20when%20data%20pairs%20come%20from%20naturally%20occurring%20generative%20processes%2C%20e.g.%2C%20successive%20video%20frames.%20We%20show%20that%20existing%20methods%20struggle%20to%20flexibly%20capture%20this%20conditional%20uncertainty.%20As%20a%20remedy%2C%20we%20introduce%20a%20latent%20variable%20to%20account%20for%20this%20uncertainty%20and%20derive%20a%20variational%20lower%20bound%20on%20the%20mutual%20information%20between%20paired%20embeddings.%20Our%20derivation%20yields%20a%20simple%20regularization%20term%20for%20standard%20SSL%20objectives.%20The%20resulting%20method%2C%20which%20we%20call%20AdaSSL%2C%20applies%20to%20both%20contrastive%20and%20distillation-based%20SSL%20objectives%2C%20and%20we%20empirically%20show%20its%20versatility%20in%20causal%20representation%20learning%2C%20fine-grained%20image%20understanding%2C%20and%20world%20modeling%20on%20videos.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Learning%2520from%2520Structural%2520Invariance%26entry.906535625%3DYipeng%2520Zhang%2520and%2520Hafez%2520Ghaemi%2520and%2520Jungyoon%2520Lee%2520and%2520Shahab%2520Bakhtiari%2520and%2520Eilif%2520B.%2520Muller%2520and%2520Laurent%2520Charlin%26entry.1292438233%3DJoint-embedding%2520self-supervised%2520learning%2520%2528SSL%2529%252C%2520the%2520key%2520paradigm%2520for%2520unsupervised%2520representation%2520learning%2520from%2520visual%2520data%252C%2520learns%2520from%2520invariances%2520between%2520semantically-related%2520data%2520pairs.%2520We%2520study%2520the%2520one-to-many%2520mapping%2520problem%2520in%2520SSL%252C%2520where%2520each%2520datum%2520may%2520be%2520mapped%2520to%2520multiple%2520valid%2520targets.%2520This%2520arises%2520when%2520data%2520pairs%2520come%2520from%2520naturally%2520occurring%2520generative%2520processes%252C%2520e.g.%252C%2520successive%2520video%2520frames.%2520We%2520show%2520that%2520existing%2520methods%2520struggle%2520to%2520flexibly%2520capture%2520this%2520conditional%2520uncertainty.%2520As%2520a%2520remedy%252C%2520we%2520introduce%2520a%2520latent%2520variable%2520to%2520account%2520for%2520this%2520uncertainty%2520and%2520derive%2520a%2520variational%2520lower%2520bound%2520on%2520the%2520mutual%2520information%2520between%2520paired%2520embeddings.%2520Our%2520derivation%2520yields%2520a%2520simple%2520regularization%2520term%2520for%2520standard%2520SSL%2520objectives.%2520The%2520resulting%2520method%252C%2520which%2520we%2520call%2520AdaSSL%252C%2520applies%2520to%2520both%2520contrastive%2520and%2520distillation-based%2520SSL%2520objectives%252C%2520and%2520we%2520empirically%2520show%2520its%2520versatility%2520in%2520causal%2520representation%2520learning%252C%2520fine-grained%2520image%2520understanding%252C%2520and%2520world%2520modeling%2520on%2520videos.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20from%20Structural%20Invariance&entry.906535625=Yipeng%20Zhang%20and%20Hafez%20Ghaemi%20and%20Jungyoon%20Lee%20and%20Shahab%20Bakhtiari%20and%20Eilif%20B.%20Muller%20and%20Laurent%20Charlin&entry.1292438233=Joint-embedding%20self-supervised%20learning%20%28SSL%29%2C%20the%20key%20paradigm%20for%20unsupervised%20representation%20learning%20from%20visual%20data%2C%20learns%20from%20invariances%20between%20semantically-related%20data%20pairs.%20We%20study%20the%20one-to-many%20mapping%20problem%20in%20SSL%2C%20where%20each%20datum%20may%20be%20mapped%20to%20multiple%20valid%20targets.%20This%20arises%20when%20data%20pairs%20come%20from%20naturally%20occurring%20generative%20processes%2C%20e.g.%2C%20successive%20video%20frames.%20We%20show%20that%20existing%20methods%20struggle%20to%20flexibly%20capture%20this%20conditional%20uncertainty.%20As%20a%20remedy%2C%20we%20introduce%20a%20latent%20variable%20to%20account%20for%20this%20uncertainty%20and%20derive%20a%20variational%20lower%20bound%20on%20the%20mutual%20information%20between%20paired%20embeddings.%20Our%20derivation%20yields%20a%20simple%20regularization%20term%20for%20standard%20SSL%20objectives.%20The%20resulting%20method%2C%20which%20we%20call%20AdaSSL%2C%20applies%20to%20both%20contrastive%20and%20distillation-based%20SSL%20objectives%2C%20and%20we%20empirically%20show%20its%20versatility%20in%20causal%20representation%20learning%2C%20fine-grained%20image%20understanding%2C%20and%20world%20modeling%20on%20videos.&entry.1838667208=http%3A//arxiv.org/abs/2602.02381v1&entry.124074799=Read"},
{"title": "Probe and Skip: Self-Predictive Token Skipping for Efficient Long-Context LLM Inference", "author": "Zimeng Wu and Donghao Wang and Chaozhe Jin and Jiaxin Chen and Yunhong Wang", "abstract": "Long-context inference enhances the reasoning capability of Large Language Models (LLMs), but incurs significant computational overhead. Token-oriented methods, such as pruning and skipping, have shown great promise in reducing inference latency, yet still suffer from inherently insufficient structure optimization, outdated selection criteria, and redundancy interference, resulting in suboptimal speed-accuracy trade-off. To address these issues, we propose a novel training-free framework dubbed Self-Predictive Token Skipping (SPTS), for efficient long-context LLM inference. Specifically, motivated by probing the influence of target layers prior to skipping, we design two selective token skipping strategies for typical structures, including Partial Attention Probing (PAP) for multi-head attention and Low-rank Transformation Probing (LTP) for feed forward network. The former selects informative tokens via partial forward attention computation, while the latter constructs a low-rank proxy network to predict token transformations. In addition, a Multi-Stage Delayed Pruning (MSDP) strategy reallocates skipping budgets and progressively removes redundant tokens across layers. Extensive experiments display the effectiveness of our method, achieving up to 2.46$\\times$ and 2.29$\\times$ speedups for prefilling and end-to-end generation, respectively, while maintaining state-of-the-art accuracy. We will release the source code upon acceptance.", "link": "http://arxiv.org/abs/2601.13155v2", "date": "2026-02-02", "relevancy": 2.6234, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5515}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5113}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probe%20and%20Skip%3A%20Self-Predictive%20Token%20Skipping%20for%20Efficient%20Long-Context%20LLM%20Inference&body=Title%3A%20Probe%20and%20Skip%3A%20Self-Predictive%20Token%20Skipping%20for%20Efficient%20Long-Context%20LLM%20Inference%0AAuthor%3A%20Zimeng%20Wu%20and%20Donghao%20Wang%20and%20Chaozhe%20Jin%20and%20Jiaxin%20Chen%20and%20Yunhong%20Wang%0AAbstract%3A%20Long-context%20inference%20enhances%20the%20reasoning%20capability%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20but%20incurs%20significant%20computational%20overhead.%20Token-oriented%20methods%2C%20such%20as%20pruning%20and%20skipping%2C%20have%20shown%20great%20promise%20in%20reducing%20inference%20latency%2C%20yet%20still%20suffer%20from%20inherently%20insufficient%20structure%20optimization%2C%20outdated%20selection%20criteria%2C%20and%20redundancy%20interference%2C%20resulting%20in%20suboptimal%20speed-accuracy%20trade-off.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20training-free%20framework%20dubbed%20Self-Predictive%20Token%20Skipping%20%28SPTS%29%2C%20for%20efficient%20long-context%20LLM%20inference.%20Specifically%2C%20motivated%20by%20probing%20the%20influence%20of%20target%20layers%20prior%20to%20skipping%2C%20we%20design%20two%20selective%20token%20skipping%20strategies%20for%20typical%20structures%2C%20including%20Partial%20Attention%20Probing%20%28PAP%29%20for%20multi-head%20attention%20and%20Low-rank%20Transformation%20Probing%20%28LTP%29%20for%20feed%20forward%20network.%20The%20former%20selects%20informative%20tokens%20via%20partial%20forward%20attention%20computation%2C%20while%20the%20latter%20constructs%20a%20low-rank%20proxy%20network%20to%20predict%20token%20transformations.%20In%20addition%2C%20a%20Multi-Stage%20Delayed%20Pruning%20%28MSDP%29%20strategy%20reallocates%20skipping%20budgets%20and%20progressively%20removes%20redundant%20tokens%20across%20layers.%20Extensive%20experiments%20display%20the%20effectiveness%20of%20our%20method%2C%20achieving%20up%20to%202.46%24%5Ctimes%24%20and%202.29%24%5Ctimes%24%20speedups%20for%20prefilling%20and%20end-to-end%20generation%2C%20respectively%2C%20while%20maintaining%20state-of-the-art%20accuracy.%20We%20will%20release%20the%20source%20code%20upon%20acceptance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13155v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbe%2520and%2520Skip%253A%2520Self-Predictive%2520Token%2520Skipping%2520for%2520Efficient%2520Long-Context%2520LLM%2520Inference%26entry.906535625%3DZimeng%2520Wu%2520and%2520Donghao%2520Wang%2520and%2520Chaozhe%2520Jin%2520and%2520Jiaxin%2520Chen%2520and%2520Yunhong%2520Wang%26entry.1292438233%3DLong-context%2520inference%2520enhances%2520the%2520reasoning%2520capability%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520but%2520incurs%2520significant%2520computational%2520overhead.%2520Token-oriented%2520methods%252C%2520such%2520as%2520pruning%2520and%2520skipping%252C%2520have%2520shown%2520great%2520promise%2520in%2520reducing%2520inference%2520latency%252C%2520yet%2520still%2520suffer%2520from%2520inherently%2520insufficient%2520structure%2520optimization%252C%2520outdated%2520selection%2520criteria%252C%2520and%2520redundancy%2520interference%252C%2520resulting%2520in%2520suboptimal%2520speed-accuracy%2520trade-off.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520training-free%2520framework%2520dubbed%2520Self-Predictive%2520Token%2520Skipping%2520%2528SPTS%2529%252C%2520for%2520efficient%2520long-context%2520LLM%2520inference.%2520Specifically%252C%2520motivated%2520by%2520probing%2520the%2520influence%2520of%2520target%2520layers%2520prior%2520to%2520skipping%252C%2520we%2520design%2520two%2520selective%2520token%2520skipping%2520strategies%2520for%2520typical%2520structures%252C%2520including%2520Partial%2520Attention%2520Probing%2520%2528PAP%2529%2520for%2520multi-head%2520attention%2520and%2520Low-rank%2520Transformation%2520Probing%2520%2528LTP%2529%2520for%2520feed%2520forward%2520network.%2520The%2520former%2520selects%2520informative%2520tokens%2520via%2520partial%2520forward%2520attention%2520computation%252C%2520while%2520the%2520latter%2520constructs%2520a%2520low-rank%2520proxy%2520network%2520to%2520predict%2520token%2520transformations.%2520In%2520addition%252C%2520a%2520Multi-Stage%2520Delayed%2520Pruning%2520%2528MSDP%2529%2520strategy%2520reallocates%2520skipping%2520budgets%2520and%2520progressively%2520removes%2520redundant%2520tokens%2520across%2520layers.%2520Extensive%2520experiments%2520display%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520achieving%2520up%2520to%25202.46%2524%255Ctimes%2524%2520and%25202.29%2524%255Ctimes%2524%2520speedups%2520for%2520prefilling%2520and%2520end-to-end%2520generation%252C%2520respectively%252C%2520while%2520maintaining%2520state-of-the-art%2520accuracy.%2520We%2520will%2520release%2520the%2520source%2520code%2520upon%2520acceptance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13155v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probe%20and%20Skip%3A%20Self-Predictive%20Token%20Skipping%20for%20Efficient%20Long-Context%20LLM%20Inference&entry.906535625=Zimeng%20Wu%20and%20Donghao%20Wang%20and%20Chaozhe%20Jin%20and%20Jiaxin%20Chen%20and%20Yunhong%20Wang&entry.1292438233=Long-context%20inference%20enhances%20the%20reasoning%20capability%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20but%20incurs%20significant%20computational%20overhead.%20Token-oriented%20methods%2C%20such%20as%20pruning%20and%20skipping%2C%20have%20shown%20great%20promise%20in%20reducing%20inference%20latency%2C%20yet%20still%20suffer%20from%20inherently%20insufficient%20structure%20optimization%2C%20outdated%20selection%20criteria%2C%20and%20redundancy%20interference%2C%20resulting%20in%20suboptimal%20speed-accuracy%20trade-off.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20training-free%20framework%20dubbed%20Self-Predictive%20Token%20Skipping%20%28SPTS%29%2C%20for%20efficient%20long-context%20LLM%20inference.%20Specifically%2C%20motivated%20by%20probing%20the%20influence%20of%20target%20layers%20prior%20to%20skipping%2C%20we%20design%20two%20selective%20token%20skipping%20strategies%20for%20typical%20structures%2C%20including%20Partial%20Attention%20Probing%20%28PAP%29%20for%20multi-head%20attention%20and%20Low-rank%20Transformation%20Probing%20%28LTP%29%20for%20feed%20forward%20network.%20The%20former%20selects%20informative%20tokens%20via%20partial%20forward%20attention%20computation%2C%20while%20the%20latter%20constructs%20a%20low-rank%20proxy%20network%20to%20predict%20token%20transformations.%20In%20addition%2C%20a%20Multi-Stage%20Delayed%20Pruning%20%28MSDP%29%20strategy%20reallocates%20skipping%20budgets%20and%20progressively%20removes%20redundant%20tokens%20across%20layers.%20Extensive%20experiments%20display%20the%20effectiveness%20of%20our%20method%2C%20achieving%20up%20to%202.46%24%5Ctimes%24%20and%202.29%24%5Ctimes%24%20speedups%20for%20prefilling%20and%20end-to-end%20generation%2C%20respectively%2C%20while%20maintaining%20state-of-the-art%20accuracy.%20We%20will%20release%20the%20source%20code%20upon%20acceptance.&entry.1838667208=http%3A//arxiv.org/abs/2601.13155v2&entry.124074799=Read"},
{"title": "CAARMA: Class Augmentation with Adversarial Mixup Regularization", "author": "Massa Baali and Xiang Li and Hao Chen and Syed Abdul Hannan and Rita Singh and Bhiksha Raj", "abstract": "Speaker verification is a typical zero-shot learning task, where inference of unseen classes is performed by comparing embeddings of test instances to known examples. The models performing inference must hence naturally generate embeddings that cluster same-class instances compactly, while maintaining separation across classes. In order to learn to do so, they are typically trained on a large number of classes (speakers), often using specialized losses. However real-world speaker datasets often lack the class diversity needed to effectively learn this in a generalizable manner. We introduce CAARMA, a class augmentation framework that addresses this problem by generating synthetic classes through data mixing in the embedding space, expanding the number of training classes. To ensure the authenticity of the synthetic classes we adopt a novel adversarial refinement mechanism that minimizes categorical distinctions between synthetic and real classes. We evaluate CAARMA on multiple speaker verification tasks, as well as other representative zero-shot comparison-based speech analysis tasks and obtain consistent improvements: our framework demonstrates a significant improvement of 8\\% over all baseline models. The code is available at: https://github.com/massabaali7/CAARMA/", "link": "http://arxiv.org/abs/2503.16718v4", "date": "2026-02-02", "relevancy": 2.614, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5688}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5286}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAARMA%3A%20Class%20Augmentation%20with%20Adversarial%20Mixup%20Regularization&body=Title%3A%20CAARMA%3A%20Class%20Augmentation%20with%20Adversarial%20Mixup%20Regularization%0AAuthor%3A%20Massa%20Baali%20and%20Xiang%20Li%20and%20Hao%20Chen%20and%20Syed%20Abdul%20Hannan%20and%20Rita%20Singh%20and%20Bhiksha%20Raj%0AAbstract%3A%20Speaker%20verification%20is%20a%20typical%20zero-shot%20learning%20task%2C%20where%20inference%20of%20unseen%20classes%20is%20performed%20by%20comparing%20embeddings%20of%20test%20instances%20to%20known%20examples.%20The%20models%20performing%20inference%20must%20hence%20naturally%20generate%20embeddings%20that%20cluster%20same-class%20instances%20compactly%2C%20while%20maintaining%20separation%20across%20classes.%20In%20order%20to%20learn%20to%20do%20so%2C%20they%20are%20typically%20trained%20on%20a%20large%20number%20of%20classes%20%28speakers%29%2C%20often%20using%20specialized%20losses.%20However%20real-world%20speaker%20datasets%20often%20lack%20the%20class%20diversity%20needed%20to%20effectively%20learn%20this%20in%20a%20generalizable%20manner.%20We%20introduce%20CAARMA%2C%20a%20class%20augmentation%20framework%20that%20addresses%20this%20problem%20by%20generating%20synthetic%20classes%20through%20data%20mixing%20in%20the%20embedding%20space%2C%20expanding%20the%20number%20of%20training%20classes.%20To%20ensure%20the%20authenticity%20of%20the%20synthetic%20classes%20we%20adopt%20a%20novel%20adversarial%20refinement%20mechanism%20that%20minimizes%20categorical%20distinctions%20between%20synthetic%20and%20real%20classes.%20We%20evaluate%20CAARMA%20on%20multiple%20speaker%20verification%20tasks%2C%20as%20well%20as%20other%20representative%20zero-shot%20comparison-based%20speech%20analysis%20tasks%20and%20obtain%20consistent%20improvements%3A%20our%20framework%20demonstrates%20a%20significant%20improvement%20of%208%5C%25%20over%20all%20baseline%20models.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/massabaali7/CAARMA/%0ALink%3A%20http%3A//arxiv.org/abs/2503.16718v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAARMA%253A%2520Class%2520Augmentation%2520with%2520Adversarial%2520Mixup%2520Regularization%26entry.906535625%3DMassa%2520Baali%2520and%2520Xiang%2520Li%2520and%2520Hao%2520Chen%2520and%2520Syed%2520Abdul%2520Hannan%2520and%2520Rita%2520Singh%2520and%2520Bhiksha%2520Raj%26entry.1292438233%3DSpeaker%2520verification%2520is%2520a%2520typical%2520zero-shot%2520learning%2520task%252C%2520where%2520inference%2520of%2520unseen%2520classes%2520is%2520performed%2520by%2520comparing%2520embeddings%2520of%2520test%2520instances%2520to%2520known%2520examples.%2520The%2520models%2520performing%2520inference%2520must%2520hence%2520naturally%2520generate%2520embeddings%2520that%2520cluster%2520same-class%2520instances%2520compactly%252C%2520while%2520maintaining%2520separation%2520across%2520classes.%2520In%2520order%2520to%2520learn%2520to%2520do%2520so%252C%2520they%2520are%2520typically%2520trained%2520on%2520a%2520large%2520number%2520of%2520classes%2520%2528speakers%2529%252C%2520often%2520using%2520specialized%2520losses.%2520However%2520real-world%2520speaker%2520datasets%2520often%2520lack%2520the%2520class%2520diversity%2520needed%2520to%2520effectively%2520learn%2520this%2520in%2520a%2520generalizable%2520manner.%2520We%2520introduce%2520CAARMA%252C%2520a%2520class%2520augmentation%2520framework%2520that%2520addresses%2520this%2520problem%2520by%2520generating%2520synthetic%2520classes%2520through%2520data%2520mixing%2520in%2520the%2520embedding%2520space%252C%2520expanding%2520the%2520number%2520of%2520training%2520classes.%2520To%2520ensure%2520the%2520authenticity%2520of%2520the%2520synthetic%2520classes%2520we%2520adopt%2520a%2520novel%2520adversarial%2520refinement%2520mechanism%2520that%2520minimizes%2520categorical%2520distinctions%2520between%2520synthetic%2520and%2520real%2520classes.%2520We%2520evaluate%2520CAARMA%2520on%2520multiple%2520speaker%2520verification%2520tasks%252C%2520as%2520well%2520as%2520other%2520representative%2520zero-shot%2520comparison-based%2520speech%2520analysis%2520tasks%2520and%2520obtain%2520consistent%2520improvements%253A%2520our%2520framework%2520demonstrates%2520a%2520significant%2520improvement%2520of%25208%255C%2525%2520over%2520all%2520baseline%2520models.%2520The%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/massabaali7/CAARMA/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.16718v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAARMA%3A%20Class%20Augmentation%20with%20Adversarial%20Mixup%20Regularization&entry.906535625=Massa%20Baali%20and%20Xiang%20Li%20and%20Hao%20Chen%20and%20Syed%20Abdul%20Hannan%20and%20Rita%20Singh%20and%20Bhiksha%20Raj&entry.1292438233=Speaker%20verification%20is%20a%20typical%20zero-shot%20learning%20task%2C%20where%20inference%20of%20unseen%20classes%20is%20performed%20by%20comparing%20embeddings%20of%20test%20instances%20to%20known%20examples.%20The%20models%20performing%20inference%20must%20hence%20naturally%20generate%20embeddings%20that%20cluster%20same-class%20instances%20compactly%2C%20while%20maintaining%20separation%20across%20classes.%20In%20order%20to%20learn%20to%20do%20so%2C%20they%20are%20typically%20trained%20on%20a%20large%20number%20of%20classes%20%28speakers%29%2C%20often%20using%20specialized%20losses.%20However%20real-world%20speaker%20datasets%20often%20lack%20the%20class%20diversity%20needed%20to%20effectively%20learn%20this%20in%20a%20generalizable%20manner.%20We%20introduce%20CAARMA%2C%20a%20class%20augmentation%20framework%20that%20addresses%20this%20problem%20by%20generating%20synthetic%20classes%20through%20data%20mixing%20in%20the%20embedding%20space%2C%20expanding%20the%20number%20of%20training%20classes.%20To%20ensure%20the%20authenticity%20of%20the%20synthetic%20classes%20we%20adopt%20a%20novel%20adversarial%20refinement%20mechanism%20that%20minimizes%20categorical%20distinctions%20between%20synthetic%20and%20real%20classes.%20We%20evaluate%20CAARMA%20on%20multiple%20speaker%20verification%20tasks%2C%20as%20well%20as%20other%20representative%20zero-shot%20comparison-based%20speech%20analysis%20tasks%20and%20obtain%20consistent%20improvements%3A%20our%20framework%20demonstrates%20a%20significant%20improvement%20of%208%5C%25%20over%20all%20baseline%20models.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/massabaali7/CAARMA/&entry.1838667208=http%3A//arxiv.org/abs/2503.16718v4&entry.124074799=Read"},
{"title": "Hierarchical Adaptive Eviction for KV Cache Management in Multimodal Language Models", "author": "Xindian Ma and Yidi Lu and Peng Zhang and Jing Zhang", "abstract": "The integration of visual information into Large Language Models (LLMs) has enabled Multimodal LLMs (MLLMs), but the quadratic memory and computational costs of Transformer architectures remain a bottleneck. Existing KV cache eviction strategies fail to address the heterogeneous attention distributions between visual and text tokens, leading to suboptimal efficiency or degraded performance. In this paper, we propose Hierarchical Adaptive Eviction (HAE), a KV cache eviction framework that optimizes text-visual token interaction in MLLMs by implementing Dual-Attention Pruning during pre-filling (leveraging visual token sparsity and attention variance) and a Dynamic Decoding Eviction Strategy (inspired by OS Recycle Bins) during decoding. HAE minimizes KV cache usage across layers, reduces computational overhead via index broadcasting, and theoretically ensures superior information integrity and lower error bounds compared to greedy strategies, enhancing efficiency in both comprehension and generation tasks. Empirically, HAE reduces KV-Cache memory by 41\\% with minimal accuracy loss (0.3\\% drop) in image understanding tasks and accelerates story generation inference by 1.5x while maintaining output quality on Phi3.5-Vision-Instruct model.", "link": "http://arxiv.org/abs/2602.02197v1", "date": "2026-02-02", "relevancy": 2.608, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5318}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5165}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Adaptive%20Eviction%20for%20KV%20Cache%20Management%20in%20Multimodal%20Language%20Models&body=Title%3A%20Hierarchical%20Adaptive%20Eviction%20for%20KV%20Cache%20Management%20in%20Multimodal%20Language%20Models%0AAuthor%3A%20Xindian%20Ma%20and%20Yidi%20Lu%20and%20Peng%20Zhang%20and%20Jing%20Zhang%0AAbstract%3A%20The%20integration%20of%20visual%20information%20into%20Large%20Language%20Models%20%28LLMs%29%20has%20enabled%20Multimodal%20LLMs%20%28MLLMs%29%2C%20but%20the%20quadratic%20memory%20and%20computational%20costs%20of%20Transformer%20architectures%20remain%20a%20bottleneck.%20Existing%20KV%20cache%20eviction%20strategies%20fail%20to%20address%20the%20heterogeneous%20attention%20distributions%20between%20visual%20and%20text%20tokens%2C%20leading%20to%20suboptimal%20efficiency%20or%20degraded%20performance.%20In%20this%20paper%2C%20we%20propose%20Hierarchical%20Adaptive%20Eviction%20%28HAE%29%2C%20a%20KV%20cache%20eviction%20framework%20that%20optimizes%20text-visual%20token%20interaction%20in%20MLLMs%20by%20implementing%20Dual-Attention%20Pruning%20during%20pre-filling%20%28leveraging%20visual%20token%20sparsity%20and%20attention%20variance%29%20and%20a%20Dynamic%20Decoding%20Eviction%20Strategy%20%28inspired%20by%20OS%20Recycle%20Bins%29%20during%20decoding.%20HAE%20minimizes%20KV%20cache%20usage%20across%20layers%2C%20reduces%20computational%20overhead%20via%20index%20broadcasting%2C%20and%20theoretically%20ensures%20superior%20information%20integrity%20and%20lower%20error%20bounds%20compared%20to%20greedy%20strategies%2C%20enhancing%20efficiency%20in%20both%20comprehension%20and%20generation%20tasks.%20Empirically%2C%20HAE%20reduces%20KV-Cache%20memory%20by%2041%5C%25%20with%20minimal%20accuracy%20loss%20%280.3%5C%25%20drop%29%20in%20image%20understanding%20tasks%20and%20accelerates%20story%20generation%20inference%20by%201.5x%20while%20maintaining%20output%20quality%20on%20Phi3.5-Vision-Instruct%20model.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02197v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Adaptive%2520Eviction%2520for%2520KV%2520Cache%2520Management%2520in%2520Multimodal%2520Language%2520Models%26entry.906535625%3DXindian%2520Ma%2520and%2520Yidi%2520Lu%2520and%2520Peng%2520Zhang%2520and%2520Jing%2520Zhang%26entry.1292438233%3DThe%2520integration%2520of%2520visual%2520information%2520into%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520enabled%2520Multimodal%2520LLMs%2520%2528MLLMs%2529%252C%2520but%2520the%2520quadratic%2520memory%2520and%2520computational%2520costs%2520of%2520Transformer%2520architectures%2520remain%2520a%2520bottleneck.%2520Existing%2520KV%2520cache%2520eviction%2520strategies%2520fail%2520to%2520address%2520the%2520heterogeneous%2520attention%2520distributions%2520between%2520visual%2520and%2520text%2520tokens%252C%2520leading%2520to%2520suboptimal%2520efficiency%2520or%2520degraded%2520performance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Hierarchical%2520Adaptive%2520Eviction%2520%2528HAE%2529%252C%2520a%2520KV%2520cache%2520eviction%2520framework%2520that%2520optimizes%2520text-visual%2520token%2520interaction%2520in%2520MLLMs%2520by%2520implementing%2520Dual-Attention%2520Pruning%2520during%2520pre-filling%2520%2528leveraging%2520visual%2520token%2520sparsity%2520and%2520attention%2520variance%2529%2520and%2520a%2520Dynamic%2520Decoding%2520Eviction%2520Strategy%2520%2528inspired%2520by%2520OS%2520Recycle%2520Bins%2529%2520during%2520decoding.%2520HAE%2520minimizes%2520KV%2520cache%2520usage%2520across%2520layers%252C%2520reduces%2520computational%2520overhead%2520via%2520index%2520broadcasting%252C%2520and%2520theoretically%2520ensures%2520superior%2520information%2520integrity%2520and%2520lower%2520error%2520bounds%2520compared%2520to%2520greedy%2520strategies%252C%2520enhancing%2520efficiency%2520in%2520both%2520comprehension%2520and%2520generation%2520tasks.%2520Empirically%252C%2520HAE%2520reduces%2520KV-Cache%2520memory%2520by%252041%255C%2525%2520with%2520minimal%2520accuracy%2520loss%2520%25280.3%255C%2525%2520drop%2529%2520in%2520image%2520understanding%2520tasks%2520and%2520accelerates%2520story%2520generation%2520inference%2520by%25201.5x%2520while%2520maintaining%2520output%2520quality%2520on%2520Phi3.5-Vision-Instruct%2520model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02197v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Adaptive%20Eviction%20for%20KV%20Cache%20Management%20in%20Multimodal%20Language%20Models&entry.906535625=Xindian%20Ma%20and%20Yidi%20Lu%20and%20Peng%20Zhang%20and%20Jing%20Zhang&entry.1292438233=The%20integration%20of%20visual%20information%20into%20Large%20Language%20Models%20%28LLMs%29%20has%20enabled%20Multimodal%20LLMs%20%28MLLMs%29%2C%20but%20the%20quadratic%20memory%20and%20computational%20costs%20of%20Transformer%20architectures%20remain%20a%20bottleneck.%20Existing%20KV%20cache%20eviction%20strategies%20fail%20to%20address%20the%20heterogeneous%20attention%20distributions%20between%20visual%20and%20text%20tokens%2C%20leading%20to%20suboptimal%20efficiency%20or%20degraded%20performance.%20In%20this%20paper%2C%20we%20propose%20Hierarchical%20Adaptive%20Eviction%20%28HAE%29%2C%20a%20KV%20cache%20eviction%20framework%20that%20optimizes%20text-visual%20token%20interaction%20in%20MLLMs%20by%20implementing%20Dual-Attention%20Pruning%20during%20pre-filling%20%28leveraging%20visual%20token%20sparsity%20and%20attention%20variance%29%20and%20a%20Dynamic%20Decoding%20Eviction%20Strategy%20%28inspired%20by%20OS%20Recycle%20Bins%29%20during%20decoding.%20HAE%20minimizes%20KV%20cache%20usage%20across%20layers%2C%20reduces%20computational%20overhead%20via%20index%20broadcasting%2C%20and%20theoretically%20ensures%20superior%20information%20integrity%20and%20lower%20error%20bounds%20compared%20to%20greedy%20strategies%2C%20enhancing%20efficiency%20in%20both%20comprehension%20and%20generation%20tasks.%20Empirically%2C%20HAE%20reduces%20KV-Cache%20memory%20by%2041%5C%25%20with%20minimal%20accuracy%20loss%20%280.3%5C%25%20drop%29%20in%20image%20understanding%20tasks%20and%20accelerates%20story%20generation%20inference%20by%201.5x%20while%20maintaining%20output%20quality%20on%20Phi3.5-Vision-Instruct%20model.&entry.1838667208=http%3A//arxiv.org/abs/2602.02197v1&entry.124074799=Read"},
{"title": "HunyuanImage 3.0 Technical Report", "author": "Siyu Cao and Hangting Chen and Peng Chen and Yiji Cheng and Yutao Cui and Xinchi Deng and Ying Dong and Kipper Gong and Tianpeng Gu and Xiusen Gu and Tiankai Hang and Duojun Huang and Jie Jiang and Zhengkai Jiang and Weijie Kong and Changlin Li and Donghao Li and Junzhe Li and Xin Li and Yang Li and Zhenxi Li and Zhimin Li and Jiaxin Lin and  Linus and Lucaz Liu and Shu Liu and Songtao Liu and Yu Liu and Yuhong Liu and Yanxin Long and Fanbin Lu and Qinglin Lu and Yuyang Peng and Yuanbo Peng and Xiangwei Shen and Yixuan Shi and Jiale Tao and Yangyu Tao and Qi Tian and Pengfei Wan and Chunyu Wang and Kai Wang and Lei Wang and Linqing Wang and Lucas Wang and Qixun Wang and Weiyan Wang and Hao Wen and Bing Wu and Jianbing Wu and Yue Wu and Senhao Xie and Fang Yang and Miles Yang and Xiaofeng Yang and Xuan Yang and Zhantao Yang and Jingmiao Yu and Zheng Yuan and Chao Zhang and Jian-Wei Zhang and Peizhen Zhang and Shi-Xue Zhang and Tao Zhang and Weigang Zhang and Yepeng Zhang and Yingfang Zhang and Zihao Zhang and Zijian Zhang and Penghao Zhao and Zhiyuan Zhao and Xuefei Zhe and Jianchen Zhu and Zhao Zhong", "abstract": "We present HunyuanImage 3.0, a native multimodal model that unifies multimodal understanding and generation within an autoregressive framework, with its image generation module publicly available. The achievement of HunyuanImage 3.0 relies on several key components, including meticulous data curation, advanced architecture design, a native Chain-of-Thoughts schema, progressive model pre-training, aggressive model post-training, and an efficient infrastructure that enables large-scale training and inference. With these advancements, we successfully trained a Mixture-of-Experts (MoE) model comprising over 80 billion parameters in total, with 13 billion parameters activated per token during inference, making it the largest and most powerful open-source image generative model to date. We conducted extensive experiments and the results of automatic and human evaluation of text-image alignment and visual quality demonstrate that HunyuanImage 3.0 rivals previous state-of-the-art models. By releasing the code and weights of HunyuanImage 3.0, we aim to enable the community to explore new ideas with a state-of-the-art foundation model, fostering a dynamic and vibrant multimodal ecosystem. All open source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanImage-3.0", "link": "http://arxiv.org/abs/2509.23951v2", "date": "2026-02-02", "relevancy": 2.6072, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5301}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.524}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HunyuanImage%203.0%20Technical%20Report&body=Title%3A%20HunyuanImage%203.0%20Technical%20Report%0AAuthor%3A%20Siyu%20Cao%20and%20Hangting%20Chen%20and%20Peng%20Chen%20and%20Yiji%20Cheng%20and%20Yutao%20Cui%20and%20Xinchi%20Deng%20and%20Ying%20Dong%20and%20Kipper%20Gong%20and%20Tianpeng%20Gu%20and%20Xiusen%20Gu%20and%20Tiankai%20Hang%20and%20Duojun%20Huang%20and%20Jie%20Jiang%20and%20Zhengkai%20Jiang%20and%20Weijie%20Kong%20and%20Changlin%20Li%20and%20Donghao%20Li%20and%20Junzhe%20Li%20and%20Xin%20Li%20and%20Yang%20Li%20and%20Zhenxi%20Li%20and%20Zhimin%20Li%20and%20Jiaxin%20Lin%20and%20%20Linus%20and%20Lucaz%20Liu%20and%20Shu%20Liu%20and%20Songtao%20Liu%20and%20Yu%20Liu%20and%20Yuhong%20Liu%20and%20Yanxin%20Long%20and%20Fanbin%20Lu%20and%20Qinglin%20Lu%20and%20Yuyang%20Peng%20and%20Yuanbo%20Peng%20and%20Xiangwei%20Shen%20and%20Yixuan%20Shi%20and%20Jiale%20Tao%20and%20Yangyu%20Tao%20and%20Qi%20Tian%20and%20Pengfei%20Wan%20and%20Chunyu%20Wang%20and%20Kai%20Wang%20and%20Lei%20Wang%20and%20Linqing%20Wang%20and%20Lucas%20Wang%20and%20Qixun%20Wang%20and%20Weiyan%20Wang%20and%20Hao%20Wen%20and%20Bing%20Wu%20and%20Jianbing%20Wu%20and%20Yue%20Wu%20and%20Senhao%20Xie%20and%20Fang%20Yang%20and%20Miles%20Yang%20and%20Xiaofeng%20Yang%20and%20Xuan%20Yang%20and%20Zhantao%20Yang%20and%20Jingmiao%20Yu%20and%20Zheng%20Yuan%20and%20Chao%20Zhang%20and%20Jian-Wei%20Zhang%20and%20Peizhen%20Zhang%20and%20Shi-Xue%20Zhang%20and%20Tao%20Zhang%20and%20Weigang%20Zhang%20and%20Yepeng%20Zhang%20and%20Yingfang%20Zhang%20and%20Zihao%20Zhang%20and%20Zijian%20Zhang%20and%20Penghao%20Zhao%20and%20Zhiyuan%20Zhao%20and%20Xuefei%20Zhe%20and%20Jianchen%20Zhu%20and%20Zhao%20Zhong%0AAbstract%3A%20We%20present%20HunyuanImage%203.0%2C%20a%20native%20multimodal%20model%20that%20unifies%20multimodal%20understanding%20and%20generation%20within%20an%20autoregressive%20framework%2C%20with%20its%20image%20generation%20module%20publicly%20available.%20The%20achievement%20of%20HunyuanImage%203.0%20relies%20on%20several%20key%20components%2C%20including%20meticulous%20data%20curation%2C%20advanced%20architecture%20design%2C%20a%20native%20Chain-of-Thoughts%20schema%2C%20progressive%20model%20pre-training%2C%20aggressive%20model%20post-training%2C%20and%20an%20efficient%20infrastructure%20that%20enables%20large-scale%20training%20and%20inference.%20With%20these%20advancements%2C%20we%20successfully%20trained%20a%20Mixture-of-Experts%20%28MoE%29%20model%20comprising%20over%2080%20billion%20parameters%20in%20total%2C%20with%2013%20billion%20parameters%20activated%20per%20token%20during%20inference%2C%20making%20it%20the%20largest%20and%20most%20powerful%20open-source%20image%20generative%20model%20to%20date.%20We%20conducted%20extensive%20experiments%20and%20the%20results%20of%20automatic%20and%20human%20evaluation%20of%20text-image%20alignment%20and%20visual%20quality%20demonstrate%20that%20HunyuanImage%203.0%20rivals%20previous%20state-of-the-art%20models.%20By%20releasing%20the%20code%20and%20weights%20of%20HunyuanImage%203.0%2C%20we%20aim%20to%20enable%20the%20community%20to%20explore%20new%20ideas%20with%20a%20state-of-the-art%20foundation%20model%2C%20fostering%20a%20dynamic%20and%20vibrant%20multimodal%20ecosystem.%20All%20open%20source%20assets%20are%20publicly%20available%20at%20https%3A//github.com/Tencent-Hunyuan/HunyuanImage-3.0%0ALink%3A%20http%3A//arxiv.org/abs/2509.23951v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHunyuanImage%25203.0%2520Technical%2520Report%26entry.906535625%3DSiyu%2520Cao%2520and%2520Hangting%2520Chen%2520and%2520Peng%2520Chen%2520and%2520Yiji%2520Cheng%2520and%2520Yutao%2520Cui%2520and%2520Xinchi%2520Deng%2520and%2520Ying%2520Dong%2520and%2520Kipper%2520Gong%2520and%2520Tianpeng%2520Gu%2520and%2520Xiusen%2520Gu%2520and%2520Tiankai%2520Hang%2520and%2520Duojun%2520Huang%2520and%2520Jie%2520Jiang%2520and%2520Zhengkai%2520Jiang%2520and%2520Weijie%2520Kong%2520and%2520Changlin%2520Li%2520and%2520Donghao%2520Li%2520and%2520Junzhe%2520Li%2520and%2520Xin%2520Li%2520and%2520Yang%2520Li%2520and%2520Zhenxi%2520Li%2520and%2520Zhimin%2520Li%2520and%2520Jiaxin%2520Lin%2520and%2520%2520Linus%2520and%2520Lucaz%2520Liu%2520and%2520Shu%2520Liu%2520and%2520Songtao%2520Liu%2520and%2520Yu%2520Liu%2520and%2520Yuhong%2520Liu%2520and%2520Yanxin%2520Long%2520and%2520Fanbin%2520Lu%2520and%2520Qinglin%2520Lu%2520and%2520Yuyang%2520Peng%2520and%2520Yuanbo%2520Peng%2520and%2520Xiangwei%2520Shen%2520and%2520Yixuan%2520Shi%2520and%2520Jiale%2520Tao%2520and%2520Yangyu%2520Tao%2520and%2520Qi%2520Tian%2520and%2520Pengfei%2520Wan%2520and%2520Chunyu%2520Wang%2520and%2520Kai%2520Wang%2520and%2520Lei%2520Wang%2520and%2520Linqing%2520Wang%2520and%2520Lucas%2520Wang%2520and%2520Qixun%2520Wang%2520and%2520Weiyan%2520Wang%2520and%2520Hao%2520Wen%2520and%2520Bing%2520Wu%2520and%2520Jianbing%2520Wu%2520and%2520Yue%2520Wu%2520and%2520Senhao%2520Xie%2520and%2520Fang%2520Yang%2520and%2520Miles%2520Yang%2520and%2520Xiaofeng%2520Yang%2520and%2520Xuan%2520Yang%2520and%2520Zhantao%2520Yang%2520and%2520Jingmiao%2520Yu%2520and%2520Zheng%2520Yuan%2520and%2520Chao%2520Zhang%2520and%2520Jian-Wei%2520Zhang%2520and%2520Peizhen%2520Zhang%2520and%2520Shi-Xue%2520Zhang%2520and%2520Tao%2520Zhang%2520and%2520Weigang%2520Zhang%2520and%2520Yepeng%2520Zhang%2520and%2520Yingfang%2520Zhang%2520and%2520Zihao%2520Zhang%2520and%2520Zijian%2520Zhang%2520and%2520Penghao%2520Zhao%2520and%2520Zhiyuan%2520Zhao%2520and%2520Xuefei%2520Zhe%2520and%2520Jianchen%2520Zhu%2520and%2520Zhao%2520Zhong%26entry.1292438233%3DWe%2520present%2520HunyuanImage%25203.0%252C%2520a%2520native%2520multimodal%2520model%2520that%2520unifies%2520multimodal%2520understanding%2520and%2520generation%2520within%2520an%2520autoregressive%2520framework%252C%2520with%2520its%2520image%2520generation%2520module%2520publicly%2520available.%2520The%2520achievement%2520of%2520HunyuanImage%25203.0%2520relies%2520on%2520several%2520key%2520components%252C%2520including%2520meticulous%2520data%2520curation%252C%2520advanced%2520architecture%2520design%252C%2520a%2520native%2520Chain-of-Thoughts%2520schema%252C%2520progressive%2520model%2520pre-training%252C%2520aggressive%2520model%2520post-training%252C%2520and%2520an%2520efficient%2520infrastructure%2520that%2520enables%2520large-scale%2520training%2520and%2520inference.%2520With%2520these%2520advancements%252C%2520we%2520successfully%2520trained%2520a%2520Mixture-of-Experts%2520%2528MoE%2529%2520model%2520comprising%2520over%252080%2520billion%2520parameters%2520in%2520total%252C%2520with%252013%2520billion%2520parameters%2520activated%2520per%2520token%2520during%2520inference%252C%2520making%2520it%2520the%2520largest%2520and%2520most%2520powerful%2520open-source%2520image%2520generative%2520model%2520to%2520date.%2520We%2520conducted%2520extensive%2520experiments%2520and%2520the%2520results%2520of%2520automatic%2520and%2520human%2520evaluation%2520of%2520text-image%2520alignment%2520and%2520visual%2520quality%2520demonstrate%2520that%2520HunyuanImage%25203.0%2520rivals%2520previous%2520state-of-the-art%2520models.%2520By%2520releasing%2520the%2520code%2520and%2520weights%2520of%2520HunyuanImage%25203.0%252C%2520we%2520aim%2520to%2520enable%2520the%2520community%2520to%2520explore%2520new%2520ideas%2520with%2520a%2520state-of-the-art%2520foundation%2520model%252C%2520fostering%2520a%2520dynamic%2520and%2520vibrant%2520multimodal%2520ecosystem.%2520All%2520open%2520source%2520assets%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/Tencent-Hunyuan/HunyuanImage-3.0%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23951v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HunyuanImage%203.0%20Technical%20Report&entry.906535625=Siyu%20Cao%20and%20Hangting%20Chen%20and%20Peng%20Chen%20and%20Yiji%20Cheng%20and%20Yutao%20Cui%20and%20Xinchi%20Deng%20and%20Ying%20Dong%20and%20Kipper%20Gong%20and%20Tianpeng%20Gu%20and%20Xiusen%20Gu%20and%20Tiankai%20Hang%20and%20Duojun%20Huang%20and%20Jie%20Jiang%20and%20Zhengkai%20Jiang%20and%20Weijie%20Kong%20and%20Changlin%20Li%20and%20Donghao%20Li%20and%20Junzhe%20Li%20and%20Xin%20Li%20and%20Yang%20Li%20and%20Zhenxi%20Li%20and%20Zhimin%20Li%20and%20Jiaxin%20Lin%20and%20%20Linus%20and%20Lucaz%20Liu%20and%20Shu%20Liu%20and%20Songtao%20Liu%20and%20Yu%20Liu%20and%20Yuhong%20Liu%20and%20Yanxin%20Long%20and%20Fanbin%20Lu%20and%20Qinglin%20Lu%20and%20Yuyang%20Peng%20and%20Yuanbo%20Peng%20and%20Xiangwei%20Shen%20and%20Yixuan%20Shi%20and%20Jiale%20Tao%20and%20Yangyu%20Tao%20and%20Qi%20Tian%20and%20Pengfei%20Wan%20and%20Chunyu%20Wang%20and%20Kai%20Wang%20and%20Lei%20Wang%20and%20Linqing%20Wang%20and%20Lucas%20Wang%20and%20Qixun%20Wang%20and%20Weiyan%20Wang%20and%20Hao%20Wen%20and%20Bing%20Wu%20and%20Jianbing%20Wu%20and%20Yue%20Wu%20and%20Senhao%20Xie%20and%20Fang%20Yang%20and%20Miles%20Yang%20and%20Xiaofeng%20Yang%20and%20Xuan%20Yang%20and%20Zhantao%20Yang%20and%20Jingmiao%20Yu%20and%20Zheng%20Yuan%20and%20Chao%20Zhang%20and%20Jian-Wei%20Zhang%20and%20Peizhen%20Zhang%20and%20Shi-Xue%20Zhang%20and%20Tao%20Zhang%20and%20Weigang%20Zhang%20and%20Yepeng%20Zhang%20and%20Yingfang%20Zhang%20and%20Zihao%20Zhang%20and%20Zijian%20Zhang%20and%20Penghao%20Zhao%20and%20Zhiyuan%20Zhao%20and%20Xuefei%20Zhe%20and%20Jianchen%20Zhu%20and%20Zhao%20Zhong&entry.1292438233=We%20present%20HunyuanImage%203.0%2C%20a%20native%20multimodal%20model%20that%20unifies%20multimodal%20understanding%20and%20generation%20within%20an%20autoregressive%20framework%2C%20with%20its%20image%20generation%20module%20publicly%20available.%20The%20achievement%20of%20HunyuanImage%203.0%20relies%20on%20several%20key%20components%2C%20including%20meticulous%20data%20curation%2C%20advanced%20architecture%20design%2C%20a%20native%20Chain-of-Thoughts%20schema%2C%20progressive%20model%20pre-training%2C%20aggressive%20model%20post-training%2C%20and%20an%20efficient%20infrastructure%20that%20enables%20large-scale%20training%20and%20inference.%20With%20these%20advancements%2C%20we%20successfully%20trained%20a%20Mixture-of-Experts%20%28MoE%29%20model%20comprising%20over%2080%20billion%20parameters%20in%20total%2C%20with%2013%20billion%20parameters%20activated%20per%20token%20during%20inference%2C%20making%20it%20the%20largest%20and%20most%20powerful%20open-source%20image%20generative%20model%20to%20date.%20We%20conducted%20extensive%20experiments%20and%20the%20results%20of%20automatic%20and%20human%20evaluation%20of%20text-image%20alignment%20and%20visual%20quality%20demonstrate%20that%20HunyuanImage%203.0%20rivals%20previous%20state-of-the-art%20models.%20By%20releasing%20the%20code%20and%20weights%20of%20HunyuanImage%203.0%2C%20we%20aim%20to%20enable%20the%20community%20to%20explore%20new%20ideas%20with%20a%20state-of-the-art%20foundation%20model%2C%20fostering%20a%20dynamic%20and%20vibrant%20multimodal%20ecosystem.%20All%20open%20source%20assets%20are%20publicly%20available%20at%20https%3A//github.com/Tencent-Hunyuan/HunyuanImage-3.0&entry.1838667208=http%3A//arxiv.org/abs/2509.23951v2&entry.124074799=Read"},
{"title": "Enhanced Detection of Tiny Objects in Aerial Images", "author": "Kihyun Kim and Michalis Lazarou and Tania Stathaki", "abstract": "While one-stage detectors like YOLOv8 offer fast training speed, they often under-perform on detecting small objects as a trade-off. This becomes even more critical when detecting tiny objects in aerial imagery due to low-resolution targets and cluttered backgrounds.\n  To address this, we introduce four enhancement strategies-input image resolution adjustment, data augmentation, attention mechanisms, and an alternative gating function for attention modules-that can be easily implemented on YOLOv8. We demonstrate that image size enlargement and the proper use of augmentation can lead to enhancement. Additionally, we designed a Mixture of Orthogonal Neural-modules Network (MoonNet) pipeline which consists of multiple attention-module-augmented CNNs. Two well-known attention modules, Squeeze-and-Excitation (SE) Block and Convolutional Block Attention Module (CBAM), were integrated into the backbone of YOLOv8 to form the MoonNet design, and the MoonNet backbone obtained improved detection accuracy compared to the original YOLOv8 backbone and single-type attention-module-augmented backbones. MoonNet further proved its adaptability and potential by achieving state-of-the-art performance on a tiny-object benchmark when integrated with the YOLC model.\n  Our code is available at: https://github.com/Kihyun11/MoonNet", "link": "http://arxiv.org/abs/2509.17078v2", "date": "2026-02-02", "relevancy": 2.6041, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5242}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5213}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Detection%20of%20Tiny%20Objects%20in%20Aerial%20Images&body=Title%3A%20Enhanced%20Detection%20of%20Tiny%20Objects%20in%20Aerial%20Images%0AAuthor%3A%20Kihyun%20Kim%20and%20Michalis%20Lazarou%20and%20Tania%20Stathaki%0AAbstract%3A%20While%20one-stage%20detectors%20like%20YOLOv8%20offer%20fast%20training%20speed%2C%20they%20often%20under-perform%20on%20detecting%20small%20objects%20as%20a%20trade-off.%20This%20becomes%20even%20more%20critical%20when%20detecting%20tiny%20objects%20in%20aerial%20imagery%20due%20to%20low-resolution%20targets%20and%20cluttered%20backgrounds.%0A%20%20To%20address%20this%2C%20we%20introduce%20four%20enhancement%20strategies-input%20image%20resolution%20adjustment%2C%20data%20augmentation%2C%20attention%20mechanisms%2C%20and%20an%20alternative%20gating%20function%20for%20attention%20modules-that%20can%20be%20easily%20implemented%20on%20YOLOv8.%20We%20demonstrate%20that%20image%20size%20enlargement%20and%20the%20proper%20use%20of%20augmentation%20can%20lead%20to%20enhancement.%20Additionally%2C%20we%20designed%20a%20Mixture%20of%20Orthogonal%20Neural-modules%20Network%20%28MoonNet%29%20pipeline%20which%20consists%20of%20multiple%20attention-module-augmented%20CNNs.%20Two%20well-known%20attention%20modules%2C%20Squeeze-and-Excitation%20%28SE%29%20Block%20and%20Convolutional%20Block%20Attention%20Module%20%28CBAM%29%2C%20were%20integrated%20into%20the%20backbone%20of%20YOLOv8%20to%20form%20the%20MoonNet%20design%2C%20and%20the%20MoonNet%20backbone%20obtained%20improved%20detection%20accuracy%20compared%20to%20the%20original%20YOLOv8%20backbone%20and%20single-type%20attention-module-augmented%20backbones.%20MoonNet%20further%20proved%20its%20adaptability%20and%20potential%20by%20achieving%20state-of-the-art%20performance%20on%20a%20tiny-object%20benchmark%20when%20integrated%20with%20the%20YOLC%20model.%0A%20%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/Kihyun11/MoonNet%0ALink%3A%20http%3A//arxiv.org/abs/2509.17078v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Detection%2520of%2520Tiny%2520Objects%2520in%2520Aerial%2520Images%26entry.906535625%3DKihyun%2520Kim%2520and%2520Michalis%2520Lazarou%2520and%2520Tania%2520Stathaki%26entry.1292438233%3DWhile%2520one-stage%2520detectors%2520like%2520YOLOv8%2520offer%2520fast%2520training%2520speed%252C%2520they%2520often%2520under-perform%2520on%2520detecting%2520small%2520objects%2520as%2520a%2520trade-off.%2520This%2520becomes%2520even%2520more%2520critical%2520when%2520detecting%2520tiny%2520objects%2520in%2520aerial%2520imagery%2520due%2520to%2520low-resolution%2520targets%2520and%2520cluttered%2520backgrounds.%250A%2520%2520To%2520address%2520this%252C%2520we%2520introduce%2520four%2520enhancement%2520strategies-input%2520image%2520resolution%2520adjustment%252C%2520data%2520augmentation%252C%2520attention%2520mechanisms%252C%2520and%2520an%2520alternative%2520gating%2520function%2520for%2520attention%2520modules-that%2520can%2520be%2520easily%2520implemented%2520on%2520YOLOv8.%2520We%2520demonstrate%2520that%2520image%2520size%2520enlargement%2520and%2520the%2520proper%2520use%2520of%2520augmentation%2520can%2520lead%2520to%2520enhancement.%2520Additionally%252C%2520we%2520designed%2520a%2520Mixture%2520of%2520Orthogonal%2520Neural-modules%2520Network%2520%2528MoonNet%2529%2520pipeline%2520which%2520consists%2520of%2520multiple%2520attention-module-augmented%2520CNNs.%2520Two%2520well-known%2520attention%2520modules%252C%2520Squeeze-and-Excitation%2520%2528SE%2529%2520Block%2520and%2520Convolutional%2520Block%2520Attention%2520Module%2520%2528CBAM%2529%252C%2520were%2520integrated%2520into%2520the%2520backbone%2520of%2520YOLOv8%2520to%2520form%2520the%2520MoonNet%2520design%252C%2520and%2520the%2520MoonNet%2520backbone%2520obtained%2520improved%2520detection%2520accuracy%2520compared%2520to%2520the%2520original%2520YOLOv8%2520backbone%2520and%2520single-type%2520attention-module-augmented%2520backbones.%2520MoonNet%2520further%2520proved%2520its%2520adaptability%2520and%2520potential%2520by%2520achieving%2520state-of-the-art%2520performance%2520on%2520a%2520tiny-object%2520benchmark%2520when%2520integrated%2520with%2520the%2520YOLC%2520model.%250A%2520%2520Our%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/Kihyun11/MoonNet%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17078v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Detection%20of%20Tiny%20Objects%20in%20Aerial%20Images&entry.906535625=Kihyun%20Kim%20and%20Michalis%20Lazarou%20and%20Tania%20Stathaki&entry.1292438233=While%20one-stage%20detectors%20like%20YOLOv8%20offer%20fast%20training%20speed%2C%20they%20often%20under-perform%20on%20detecting%20small%20objects%20as%20a%20trade-off.%20This%20becomes%20even%20more%20critical%20when%20detecting%20tiny%20objects%20in%20aerial%20imagery%20due%20to%20low-resolution%20targets%20and%20cluttered%20backgrounds.%0A%20%20To%20address%20this%2C%20we%20introduce%20four%20enhancement%20strategies-input%20image%20resolution%20adjustment%2C%20data%20augmentation%2C%20attention%20mechanisms%2C%20and%20an%20alternative%20gating%20function%20for%20attention%20modules-that%20can%20be%20easily%20implemented%20on%20YOLOv8.%20We%20demonstrate%20that%20image%20size%20enlargement%20and%20the%20proper%20use%20of%20augmentation%20can%20lead%20to%20enhancement.%20Additionally%2C%20we%20designed%20a%20Mixture%20of%20Orthogonal%20Neural-modules%20Network%20%28MoonNet%29%20pipeline%20which%20consists%20of%20multiple%20attention-module-augmented%20CNNs.%20Two%20well-known%20attention%20modules%2C%20Squeeze-and-Excitation%20%28SE%29%20Block%20and%20Convolutional%20Block%20Attention%20Module%20%28CBAM%29%2C%20were%20integrated%20into%20the%20backbone%20of%20YOLOv8%20to%20form%20the%20MoonNet%20design%2C%20and%20the%20MoonNet%20backbone%20obtained%20improved%20detection%20accuracy%20compared%20to%20the%20original%20YOLOv8%20backbone%20and%20single-type%20attention-module-augmented%20backbones.%20MoonNet%20further%20proved%20its%20adaptability%20and%20potential%20by%20achieving%20state-of-the-art%20performance%20on%20a%20tiny-object%20benchmark%20when%20integrated%20with%20the%20YOLC%20model.%0A%20%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/Kihyun11/MoonNet&entry.1838667208=http%3A//arxiv.org/abs/2509.17078v2&entry.124074799=Read"},
{"title": "Attention in Geometry: Scalable Spatial Modeling via Adaptive Density Fields and FAISS-Accelerated Kernels", "author": "Zhaowen Fan", "abstract": "This work introduces Adaptive Density Fields (ADF), a geometric attention framework that formulates spatial aggregation as a query-conditioned, metric-induced attention operator in continuous space. By reinterpreting spatial influence as geometry-preserving attention grounded in physical distance, ADF bridges concepts from adaptive kernel methods and attention mechanisms. Scalability is achieved via FAISS-accelerated inverted file indices, treating approximate nearest-neighbor search as an intrinsic component of the attention mechanism. We demonstrate the framework through a case study on aircraft trajectory analysis in the Chengdu region, extracting trajectory-conditioned Zones of Influence (ZOI) to reveal recurrent airspace structures and localized deviations.", "link": "http://arxiv.org/abs/2601.06135v2", "date": "2026-02-02", "relevancy": 2.5908, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5278}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5217}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20in%20Geometry%3A%20Scalable%20Spatial%20Modeling%20via%20Adaptive%20Density%20Fields%20and%20FAISS-Accelerated%20Kernels&body=Title%3A%20Attention%20in%20Geometry%3A%20Scalable%20Spatial%20Modeling%20via%20Adaptive%20Density%20Fields%20and%20FAISS-Accelerated%20Kernels%0AAuthor%3A%20Zhaowen%20Fan%0AAbstract%3A%20This%20work%20introduces%20Adaptive%20Density%20Fields%20%28ADF%29%2C%20a%20geometric%20attention%20framework%20that%20formulates%20spatial%20aggregation%20as%20a%20query-conditioned%2C%20metric-induced%20attention%20operator%20in%20continuous%20space.%20By%20reinterpreting%20spatial%20influence%20as%20geometry-preserving%20attention%20grounded%20in%20physical%20distance%2C%20ADF%20bridges%20concepts%20from%20adaptive%20kernel%20methods%20and%20attention%20mechanisms.%20Scalability%20is%20achieved%20via%20FAISS-accelerated%20inverted%20file%20indices%2C%20treating%20approximate%20nearest-neighbor%20search%20as%20an%20intrinsic%20component%20of%20the%20attention%20mechanism.%20We%20demonstrate%20the%20framework%20through%20a%20case%20study%20on%20aircraft%20trajectory%20analysis%20in%20the%20Chengdu%20region%2C%20extracting%20trajectory-conditioned%20Zones%20of%20Influence%20%28ZOI%29%20to%20reveal%20recurrent%20airspace%20structures%20and%20localized%20deviations.%0ALink%3A%20http%3A//arxiv.org/abs/2601.06135v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520in%2520Geometry%253A%2520Scalable%2520Spatial%2520Modeling%2520via%2520Adaptive%2520Density%2520Fields%2520and%2520FAISS-Accelerated%2520Kernels%26entry.906535625%3DZhaowen%2520Fan%26entry.1292438233%3DThis%2520work%2520introduces%2520Adaptive%2520Density%2520Fields%2520%2528ADF%2529%252C%2520a%2520geometric%2520attention%2520framework%2520that%2520formulates%2520spatial%2520aggregation%2520as%2520a%2520query-conditioned%252C%2520metric-induced%2520attention%2520operator%2520in%2520continuous%2520space.%2520By%2520reinterpreting%2520spatial%2520influence%2520as%2520geometry-preserving%2520attention%2520grounded%2520in%2520physical%2520distance%252C%2520ADF%2520bridges%2520concepts%2520from%2520adaptive%2520kernel%2520methods%2520and%2520attention%2520mechanisms.%2520Scalability%2520is%2520achieved%2520via%2520FAISS-accelerated%2520inverted%2520file%2520indices%252C%2520treating%2520approximate%2520nearest-neighbor%2520search%2520as%2520an%2520intrinsic%2520component%2520of%2520the%2520attention%2520mechanism.%2520We%2520demonstrate%2520the%2520framework%2520through%2520a%2520case%2520study%2520on%2520aircraft%2520trajectory%2520analysis%2520in%2520the%2520Chengdu%2520region%252C%2520extracting%2520trajectory-conditioned%2520Zones%2520of%2520Influence%2520%2528ZOI%2529%2520to%2520reveal%2520recurrent%2520airspace%2520structures%2520and%2520localized%2520deviations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.06135v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20in%20Geometry%3A%20Scalable%20Spatial%20Modeling%20via%20Adaptive%20Density%20Fields%20and%20FAISS-Accelerated%20Kernels&entry.906535625=Zhaowen%20Fan&entry.1292438233=This%20work%20introduces%20Adaptive%20Density%20Fields%20%28ADF%29%2C%20a%20geometric%20attention%20framework%20that%20formulates%20spatial%20aggregation%20as%20a%20query-conditioned%2C%20metric-induced%20attention%20operator%20in%20continuous%20space.%20By%20reinterpreting%20spatial%20influence%20as%20geometry-preserving%20attention%20grounded%20in%20physical%20distance%2C%20ADF%20bridges%20concepts%20from%20adaptive%20kernel%20methods%20and%20attention%20mechanisms.%20Scalability%20is%20achieved%20via%20FAISS-accelerated%20inverted%20file%20indices%2C%20treating%20approximate%20nearest-neighbor%20search%20as%20an%20intrinsic%20component%20of%20the%20attention%20mechanism.%20We%20demonstrate%20the%20framework%20through%20a%20case%20study%20on%20aircraft%20trajectory%20analysis%20in%20the%20Chengdu%20region%2C%20extracting%20trajectory-conditioned%20Zones%20of%20Influence%20%28ZOI%29%20to%20reveal%20recurrent%20airspace%20structures%20and%20localized%20deviations.&entry.1838667208=http%3A//arxiv.org/abs/2601.06135v2&entry.124074799=Read"},
{"title": "Entropy-Lens: Uncovering Decision Strategies in LLMs", "author": "Riccardo Ali and Francesco Caso and Christopher Irwin and Pietro Li\u00f2", "abstract": "In large language models (LLMs), each block operates on the residual stream to map input token sequences to output token distributions. However, most of the interpretability literature focuses on internal latent representations, leaving token-space dynamics underexplored. The high dimensionality and categoricity of token distributions hinder their analysis, as standard statistical descriptors are not suitable. We show that the entropy of logit-lens predictions overcomes these issues. In doing so, it provides a per-layer scalar, permutation-invariant metric. We introduce Entropy-Lens to distill the token-space dynamics of the residual stream into a low-dimensional signal. We call this signal the entropy profile. We apply our method to a variety of model sizes and families, showing that (i) entropy profiles uncover token prediction dynamics driven by expansion and pruning strategies; (ii) these dynamics are family-specific and invariant under depth rescaling; (iii) they are characteristic of task type and output format; (iv) these strategies have unequal impact on downstream performance, with the expansion strategy usually being more critical. Ultimately, our findings further enhance our understanding of the residual stream, enabling a granular assessment of how information is processed across model depth.", "link": "http://arxiv.org/abs/2502.16570v3", "date": "2026-02-02", "relevancy": 2.5765, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5232}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5232}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entropy-Lens%3A%20Uncovering%20Decision%20Strategies%20in%20LLMs&body=Title%3A%20Entropy-Lens%3A%20Uncovering%20Decision%20Strategies%20in%20LLMs%0AAuthor%3A%20Riccardo%20Ali%20and%20Francesco%20Caso%20and%20Christopher%20Irwin%20and%20Pietro%20Li%C3%B2%0AAbstract%3A%20In%20large%20language%20models%20%28LLMs%29%2C%20each%20block%20operates%20on%20the%20residual%20stream%20to%20map%20input%20token%20sequences%20to%20output%20token%20distributions.%20However%2C%20most%20of%20the%20interpretability%20literature%20focuses%20on%20internal%20latent%20representations%2C%20leaving%20token-space%20dynamics%20underexplored.%20The%20high%20dimensionality%20and%20categoricity%20of%20token%20distributions%20hinder%20their%20analysis%2C%20as%20standard%20statistical%20descriptors%20are%20not%20suitable.%20We%20show%20that%20the%20entropy%20of%20logit-lens%20predictions%20overcomes%20these%20issues.%20In%20doing%20so%2C%20it%20provides%20a%20per-layer%20scalar%2C%20permutation-invariant%20metric.%20We%20introduce%20Entropy-Lens%20to%20distill%20the%20token-space%20dynamics%20of%20the%20residual%20stream%20into%20a%20low-dimensional%20signal.%20We%20call%20this%20signal%20the%20entropy%20profile.%20We%20apply%20our%20method%20to%20a%20variety%20of%20model%20sizes%20and%20families%2C%20showing%20that%20%28i%29%20entropy%20profiles%20uncover%20token%20prediction%20dynamics%20driven%20by%20expansion%20and%20pruning%20strategies%3B%20%28ii%29%20these%20dynamics%20are%20family-specific%20and%20invariant%20under%20depth%20rescaling%3B%20%28iii%29%20they%20are%20characteristic%20of%20task%20type%20and%20output%20format%3B%20%28iv%29%20these%20strategies%20have%20unequal%20impact%20on%20downstream%20performance%2C%20with%20the%20expansion%20strategy%20usually%20being%20more%20critical.%20Ultimately%2C%20our%20findings%20further%20enhance%20our%20understanding%20of%20the%20residual%20stream%2C%20enabling%20a%20granular%20assessment%20of%20how%20information%20is%20processed%20across%20model%20depth.%0ALink%3A%20http%3A//arxiv.org/abs/2502.16570v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntropy-Lens%253A%2520Uncovering%2520Decision%2520Strategies%2520in%2520LLMs%26entry.906535625%3DRiccardo%2520Ali%2520and%2520Francesco%2520Caso%2520and%2520Christopher%2520Irwin%2520and%2520Pietro%2520Li%25C3%25B2%26entry.1292438233%3DIn%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520each%2520block%2520operates%2520on%2520the%2520residual%2520stream%2520to%2520map%2520input%2520token%2520sequences%2520to%2520output%2520token%2520distributions.%2520However%252C%2520most%2520of%2520the%2520interpretability%2520literature%2520focuses%2520on%2520internal%2520latent%2520representations%252C%2520leaving%2520token-space%2520dynamics%2520underexplored.%2520The%2520high%2520dimensionality%2520and%2520categoricity%2520of%2520token%2520distributions%2520hinder%2520their%2520analysis%252C%2520as%2520standard%2520statistical%2520descriptors%2520are%2520not%2520suitable.%2520We%2520show%2520that%2520the%2520entropy%2520of%2520logit-lens%2520predictions%2520overcomes%2520these%2520issues.%2520In%2520doing%2520so%252C%2520it%2520provides%2520a%2520per-layer%2520scalar%252C%2520permutation-invariant%2520metric.%2520We%2520introduce%2520Entropy-Lens%2520to%2520distill%2520the%2520token-space%2520dynamics%2520of%2520the%2520residual%2520stream%2520into%2520a%2520low-dimensional%2520signal.%2520We%2520call%2520this%2520signal%2520the%2520entropy%2520profile.%2520We%2520apply%2520our%2520method%2520to%2520a%2520variety%2520of%2520model%2520sizes%2520and%2520families%252C%2520showing%2520that%2520%2528i%2529%2520entropy%2520profiles%2520uncover%2520token%2520prediction%2520dynamics%2520driven%2520by%2520expansion%2520and%2520pruning%2520strategies%253B%2520%2528ii%2529%2520these%2520dynamics%2520are%2520family-specific%2520and%2520invariant%2520under%2520depth%2520rescaling%253B%2520%2528iii%2529%2520they%2520are%2520characteristic%2520of%2520task%2520type%2520and%2520output%2520format%253B%2520%2528iv%2529%2520these%2520strategies%2520have%2520unequal%2520impact%2520on%2520downstream%2520performance%252C%2520with%2520the%2520expansion%2520strategy%2520usually%2520being%2520more%2520critical.%2520Ultimately%252C%2520our%2520findings%2520further%2520enhance%2520our%2520understanding%2520of%2520the%2520residual%2520stream%252C%2520enabling%2520a%2520granular%2520assessment%2520of%2520how%2520information%2520is%2520processed%2520across%2520model%2520depth.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.16570v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entropy-Lens%3A%20Uncovering%20Decision%20Strategies%20in%20LLMs&entry.906535625=Riccardo%20Ali%20and%20Francesco%20Caso%20and%20Christopher%20Irwin%20and%20Pietro%20Li%C3%B2&entry.1292438233=In%20large%20language%20models%20%28LLMs%29%2C%20each%20block%20operates%20on%20the%20residual%20stream%20to%20map%20input%20token%20sequences%20to%20output%20token%20distributions.%20However%2C%20most%20of%20the%20interpretability%20literature%20focuses%20on%20internal%20latent%20representations%2C%20leaving%20token-space%20dynamics%20underexplored.%20The%20high%20dimensionality%20and%20categoricity%20of%20token%20distributions%20hinder%20their%20analysis%2C%20as%20standard%20statistical%20descriptors%20are%20not%20suitable.%20We%20show%20that%20the%20entropy%20of%20logit-lens%20predictions%20overcomes%20these%20issues.%20In%20doing%20so%2C%20it%20provides%20a%20per-layer%20scalar%2C%20permutation-invariant%20metric.%20We%20introduce%20Entropy-Lens%20to%20distill%20the%20token-space%20dynamics%20of%20the%20residual%20stream%20into%20a%20low-dimensional%20signal.%20We%20call%20this%20signal%20the%20entropy%20profile.%20We%20apply%20our%20method%20to%20a%20variety%20of%20model%20sizes%20and%20families%2C%20showing%20that%20%28i%29%20entropy%20profiles%20uncover%20token%20prediction%20dynamics%20driven%20by%20expansion%20and%20pruning%20strategies%3B%20%28ii%29%20these%20dynamics%20are%20family-specific%20and%20invariant%20under%20depth%20rescaling%3B%20%28iii%29%20they%20are%20characteristic%20of%20task%20type%20and%20output%20format%3B%20%28iv%29%20these%20strategies%20have%20unequal%20impact%20on%20downstream%20performance%2C%20with%20the%20expansion%20strategy%20usually%20being%20more%20critical.%20Ultimately%2C%20our%20findings%20further%20enhance%20our%20understanding%20of%20the%20residual%20stream%2C%20enabling%20a%20granular%20assessment%20of%20how%20information%20is%20processed%20across%20model%20depth.&entry.1838667208=http%3A//arxiv.org/abs/2502.16570v3&entry.124074799=Read"},
{"title": "Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning", "author": "Albert Gassol Puigjaner and Angelos Zacharia and Kostas Alexis", "abstract": "Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings. While traditional Simultaneous Localization and Mapping (SLAM) methods generate metric reconstructions and can be extended to metric-semantic mapping, they lack a higher level of abstraction and relational reasoning. To address this gap, 3D scene graphs have emerged as a powerful representation for capturing hierarchical structures and object relationships. In this work, we propose an enhanced hierarchical 3D scene graph that integrates open-vocabulary features across multiple abstraction levels and supports object-relational reasoning. Our approach leverages a Vision Language Model (VLM) to infer semantic relationships. Notably, we introduce a task reasoning module that combines Large Language Models (LLM) and a VLM to interpret the scene graph's semantic and relational information, enabling agents to reason about tasks and interact with their environment more intelligently. We validate our method by deploying it on a quadruped robot in multiple environments and tasks, highlighting its ability to reason about them.", "link": "http://arxiv.org/abs/2602.02456v1", "date": "2026-02-02", "relevancy": 2.5677, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6735}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6356}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relationship-Aware%20Hierarchical%203D%20Scene%20Graph%20for%20Task%20Reasoning&body=Title%3A%20Relationship-Aware%20Hierarchical%203D%20Scene%20Graph%20for%20Task%20Reasoning%0AAuthor%3A%20Albert%20Gassol%20Puigjaner%20and%20Angelos%20Zacharia%20and%20Kostas%20Alexis%0AAbstract%3A%20Representing%20and%20understanding%203D%20environments%20in%20a%20structured%20manner%20is%20crucial%20for%20autonomous%20agents%20to%20navigate%20and%20reason%20about%20their%20surroundings.%20While%20traditional%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20methods%20generate%20metric%20reconstructions%20and%20can%20be%20extended%20to%20metric-semantic%20mapping%2C%20they%20lack%20a%20higher%20level%20of%20abstraction%20and%20relational%20reasoning.%20To%20address%20this%20gap%2C%203D%20scene%20graphs%20have%20emerged%20as%20a%20powerful%20representation%20for%20capturing%20hierarchical%20structures%20and%20object%20relationships.%20In%20this%20work%2C%20we%20propose%20an%20enhanced%20hierarchical%203D%20scene%20graph%20that%20integrates%20open-vocabulary%20features%20across%20multiple%20abstraction%20levels%20and%20supports%20object-relational%20reasoning.%20Our%20approach%20leverages%20a%20Vision%20Language%20Model%20%28VLM%29%20to%20infer%20semantic%20relationships.%20Notably%2C%20we%20introduce%20a%20task%20reasoning%20module%20that%20combines%20Large%20Language%20Models%20%28LLM%29%20and%20a%20VLM%20to%20interpret%20the%20scene%20graph%27s%20semantic%20and%20relational%20information%2C%20enabling%20agents%20to%20reason%20about%20tasks%20and%20interact%20with%20their%20environment%20more%20intelligently.%20We%20validate%20our%20method%20by%20deploying%20it%20on%20a%20quadruped%20robot%20in%20multiple%20environments%20and%20tasks%2C%20highlighting%20its%20ability%20to%20reason%20about%20them.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02456v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelationship-Aware%2520Hierarchical%25203D%2520Scene%2520Graph%2520for%2520Task%2520Reasoning%26entry.906535625%3DAlbert%2520Gassol%2520Puigjaner%2520and%2520Angelos%2520Zacharia%2520and%2520Kostas%2520Alexis%26entry.1292438233%3DRepresenting%2520and%2520understanding%25203D%2520environments%2520in%2520a%2520structured%2520manner%2520is%2520crucial%2520for%2520autonomous%2520agents%2520to%2520navigate%2520and%2520reason%2520about%2520their%2520surroundings.%2520While%2520traditional%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520methods%2520generate%2520metric%2520reconstructions%2520and%2520can%2520be%2520extended%2520to%2520metric-semantic%2520mapping%252C%2520they%2520lack%2520a%2520higher%2520level%2520of%2520abstraction%2520and%2520relational%2520reasoning.%2520To%2520address%2520this%2520gap%252C%25203D%2520scene%2520graphs%2520have%2520emerged%2520as%2520a%2520powerful%2520representation%2520for%2520capturing%2520hierarchical%2520structures%2520and%2520object%2520relationships.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520enhanced%2520hierarchical%25203D%2520scene%2520graph%2520that%2520integrates%2520open-vocabulary%2520features%2520across%2520multiple%2520abstraction%2520levels%2520and%2520supports%2520object-relational%2520reasoning.%2520Our%2520approach%2520leverages%2520a%2520Vision%2520Language%2520Model%2520%2528VLM%2529%2520to%2520infer%2520semantic%2520relationships.%2520Notably%252C%2520we%2520introduce%2520a%2520task%2520reasoning%2520module%2520that%2520combines%2520Large%2520Language%2520Models%2520%2528LLM%2529%2520and%2520a%2520VLM%2520to%2520interpret%2520the%2520scene%2520graph%2527s%2520semantic%2520and%2520relational%2520information%252C%2520enabling%2520agents%2520to%2520reason%2520about%2520tasks%2520and%2520interact%2520with%2520their%2520environment%2520more%2520intelligently.%2520We%2520validate%2520our%2520method%2520by%2520deploying%2520it%2520on%2520a%2520quadruped%2520robot%2520in%2520multiple%2520environments%2520and%2520tasks%252C%2520highlighting%2520its%2520ability%2520to%2520reason%2520about%2520them.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02456v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relationship-Aware%20Hierarchical%203D%20Scene%20Graph%20for%20Task%20Reasoning&entry.906535625=Albert%20Gassol%20Puigjaner%20and%20Angelos%20Zacharia%20and%20Kostas%20Alexis&entry.1292438233=Representing%20and%20understanding%203D%20environments%20in%20a%20structured%20manner%20is%20crucial%20for%20autonomous%20agents%20to%20navigate%20and%20reason%20about%20their%20surroundings.%20While%20traditional%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20methods%20generate%20metric%20reconstructions%20and%20can%20be%20extended%20to%20metric-semantic%20mapping%2C%20they%20lack%20a%20higher%20level%20of%20abstraction%20and%20relational%20reasoning.%20To%20address%20this%20gap%2C%203D%20scene%20graphs%20have%20emerged%20as%20a%20powerful%20representation%20for%20capturing%20hierarchical%20structures%20and%20object%20relationships.%20In%20this%20work%2C%20we%20propose%20an%20enhanced%20hierarchical%203D%20scene%20graph%20that%20integrates%20open-vocabulary%20features%20across%20multiple%20abstraction%20levels%20and%20supports%20object-relational%20reasoning.%20Our%20approach%20leverages%20a%20Vision%20Language%20Model%20%28VLM%29%20to%20infer%20semantic%20relationships.%20Notably%2C%20we%20introduce%20a%20task%20reasoning%20module%20that%20combines%20Large%20Language%20Models%20%28LLM%29%20and%20a%20VLM%20to%20interpret%20the%20scene%20graph%27s%20semantic%20and%20relational%20information%2C%20enabling%20agents%20to%20reason%20about%20tasks%20and%20interact%20with%20their%20environment%20more%20intelligently.%20We%20validate%20our%20method%20by%20deploying%20it%20on%20a%20quadruped%20robot%20in%20multiple%20environments%20and%20tasks%2C%20highlighting%20its%20ability%20to%20reason%20about%20them.&entry.1838667208=http%3A//arxiv.org/abs/2602.02456v1&entry.124074799=Read"},
{"title": "See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers", "author": "Ding Xia and Xinyue Gui and Mark Colley and Fan Gao and Zhongyi Zhou and Dongyuan Li and Renhe Jiang and Takeo Igarashi", "abstract": "Automated vehicles lack natural communication channels with other road users, making external Human-Machine Interfaces (eHMIs) essential for conveying intent and maintaining trust in shared environments. However, most eHMI studies rely on developer-crafted message-action pairs, which are difficult to adapt to diverse and dynamic traffic contexts. A promising alternative is to use Large Language Models (LLMs) as action designers that generate context-conditioned eHMI actions, yet such designers lack perceptual verification and typically depend on fixed prompts or costly human-annotated feedback for improvement. We present See2Refine, a human-free, closed-loop framework that uses vision-language model (VLM) perceptual evaluation as automated visual feedback to improve an LLM-based eHMI action designer. Given a driving context and a candidate eHMI action, the VLM evaluates the perceived appropriateness of the action, and this feedback is used to iteratively revise the designer's outputs, enabling systematic refinement without human supervision. We evaluate our framework across three eHMI modalities (lightbar, eyes, and arm) and multiple LLM model sizes. Across settings, our framework consistently outperforms prompt-only LLM designers and manually specified baselines in both VLM-based metrics and human-subject evaluations. Results further indicate that the improvements generalize across modalities and that VLM evaluations are well aligned with human preferences, supporting the robustness and effectiveness of See2Refine for scalable action design.", "link": "http://arxiv.org/abs/2602.02063v1", "date": "2026-02-02", "relevancy": 2.5668, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5151}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5151}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20See2Refine%3A%20Vision-Language%20Feedback%20Improves%20LLM-Based%20eHMI%20Action%20Designers&body=Title%3A%20See2Refine%3A%20Vision-Language%20Feedback%20Improves%20LLM-Based%20eHMI%20Action%20Designers%0AAuthor%3A%20Ding%20Xia%20and%20Xinyue%20Gui%20and%20Mark%20Colley%20and%20Fan%20Gao%20and%20Zhongyi%20Zhou%20and%20Dongyuan%20Li%20and%20Renhe%20Jiang%20and%20Takeo%20Igarashi%0AAbstract%3A%20Automated%20vehicles%20lack%20natural%20communication%20channels%20with%20other%20road%20users%2C%20making%20external%20Human-Machine%20Interfaces%20%28eHMIs%29%20essential%20for%20conveying%20intent%20and%20maintaining%20trust%20in%20shared%20environments.%20However%2C%20most%20eHMI%20studies%20rely%20on%20developer-crafted%20message-action%20pairs%2C%20which%20are%20difficult%20to%20adapt%20to%20diverse%20and%20dynamic%20traffic%20contexts.%20A%20promising%20alternative%20is%20to%20use%20Large%20Language%20Models%20%28LLMs%29%20as%20action%20designers%20that%20generate%20context-conditioned%20eHMI%20actions%2C%20yet%20such%20designers%20lack%20perceptual%20verification%20and%20typically%20depend%20on%20fixed%20prompts%20or%20costly%20human-annotated%20feedback%20for%20improvement.%20We%20present%20See2Refine%2C%20a%20human-free%2C%20closed-loop%20framework%20that%20uses%20vision-language%20model%20%28VLM%29%20perceptual%20evaluation%20as%20automated%20visual%20feedback%20to%20improve%20an%20LLM-based%20eHMI%20action%20designer.%20Given%20a%20driving%20context%20and%20a%20candidate%20eHMI%20action%2C%20the%20VLM%20evaluates%20the%20perceived%20appropriateness%20of%20the%20action%2C%20and%20this%20feedback%20is%20used%20to%20iteratively%20revise%20the%20designer%27s%20outputs%2C%20enabling%20systematic%20refinement%20without%20human%20supervision.%20We%20evaluate%20our%20framework%20across%20three%20eHMI%20modalities%20%28lightbar%2C%20eyes%2C%20and%20arm%29%20and%20multiple%20LLM%20model%20sizes.%20Across%20settings%2C%20our%20framework%20consistently%20outperforms%20prompt-only%20LLM%20designers%20and%20manually%20specified%20baselines%20in%20both%20VLM-based%20metrics%20and%20human-subject%20evaluations.%20Results%20further%20indicate%20that%20the%20improvements%20generalize%20across%20modalities%20and%20that%20VLM%20evaluations%20are%20well%20aligned%20with%20human%20preferences%2C%20supporting%20the%20robustness%20and%20effectiveness%20of%20See2Refine%20for%20scalable%20action%20design.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02063v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSee2Refine%253A%2520Vision-Language%2520Feedback%2520Improves%2520LLM-Based%2520eHMI%2520Action%2520Designers%26entry.906535625%3DDing%2520Xia%2520and%2520Xinyue%2520Gui%2520and%2520Mark%2520Colley%2520and%2520Fan%2520Gao%2520and%2520Zhongyi%2520Zhou%2520and%2520Dongyuan%2520Li%2520and%2520Renhe%2520Jiang%2520and%2520Takeo%2520Igarashi%26entry.1292438233%3DAutomated%2520vehicles%2520lack%2520natural%2520communication%2520channels%2520with%2520other%2520road%2520users%252C%2520making%2520external%2520Human-Machine%2520Interfaces%2520%2528eHMIs%2529%2520essential%2520for%2520conveying%2520intent%2520and%2520maintaining%2520trust%2520in%2520shared%2520environments.%2520However%252C%2520most%2520eHMI%2520studies%2520rely%2520on%2520developer-crafted%2520message-action%2520pairs%252C%2520which%2520are%2520difficult%2520to%2520adapt%2520to%2520diverse%2520and%2520dynamic%2520traffic%2520contexts.%2520A%2520promising%2520alternative%2520is%2520to%2520use%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520as%2520action%2520designers%2520that%2520generate%2520context-conditioned%2520eHMI%2520actions%252C%2520yet%2520such%2520designers%2520lack%2520perceptual%2520verification%2520and%2520typically%2520depend%2520on%2520fixed%2520prompts%2520or%2520costly%2520human-annotated%2520feedback%2520for%2520improvement.%2520We%2520present%2520See2Refine%252C%2520a%2520human-free%252C%2520closed-loop%2520framework%2520that%2520uses%2520vision-language%2520model%2520%2528VLM%2529%2520perceptual%2520evaluation%2520as%2520automated%2520visual%2520feedback%2520to%2520improve%2520an%2520LLM-based%2520eHMI%2520action%2520designer.%2520Given%2520a%2520driving%2520context%2520and%2520a%2520candidate%2520eHMI%2520action%252C%2520the%2520VLM%2520evaluates%2520the%2520perceived%2520appropriateness%2520of%2520the%2520action%252C%2520and%2520this%2520feedback%2520is%2520used%2520to%2520iteratively%2520revise%2520the%2520designer%2527s%2520outputs%252C%2520enabling%2520systematic%2520refinement%2520without%2520human%2520supervision.%2520We%2520evaluate%2520our%2520framework%2520across%2520three%2520eHMI%2520modalities%2520%2528lightbar%252C%2520eyes%252C%2520and%2520arm%2529%2520and%2520multiple%2520LLM%2520model%2520sizes.%2520Across%2520settings%252C%2520our%2520framework%2520consistently%2520outperforms%2520prompt-only%2520LLM%2520designers%2520and%2520manually%2520specified%2520baselines%2520in%2520both%2520VLM-based%2520metrics%2520and%2520human-subject%2520evaluations.%2520Results%2520further%2520indicate%2520that%2520the%2520improvements%2520generalize%2520across%2520modalities%2520and%2520that%2520VLM%2520evaluations%2520are%2520well%2520aligned%2520with%2520human%2520preferences%252C%2520supporting%2520the%2520robustness%2520and%2520effectiveness%2520of%2520See2Refine%2520for%2520scalable%2520action%2520design.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02063v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=See2Refine%3A%20Vision-Language%20Feedback%20Improves%20LLM-Based%20eHMI%20Action%20Designers&entry.906535625=Ding%20Xia%20and%20Xinyue%20Gui%20and%20Mark%20Colley%20and%20Fan%20Gao%20and%20Zhongyi%20Zhou%20and%20Dongyuan%20Li%20and%20Renhe%20Jiang%20and%20Takeo%20Igarashi&entry.1292438233=Automated%20vehicles%20lack%20natural%20communication%20channels%20with%20other%20road%20users%2C%20making%20external%20Human-Machine%20Interfaces%20%28eHMIs%29%20essential%20for%20conveying%20intent%20and%20maintaining%20trust%20in%20shared%20environments.%20However%2C%20most%20eHMI%20studies%20rely%20on%20developer-crafted%20message-action%20pairs%2C%20which%20are%20difficult%20to%20adapt%20to%20diverse%20and%20dynamic%20traffic%20contexts.%20A%20promising%20alternative%20is%20to%20use%20Large%20Language%20Models%20%28LLMs%29%20as%20action%20designers%20that%20generate%20context-conditioned%20eHMI%20actions%2C%20yet%20such%20designers%20lack%20perceptual%20verification%20and%20typically%20depend%20on%20fixed%20prompts%20or%20costly%20human-annotated%20feedback%20for%20improvement.%20We%20present%20See2Refine%2C%20a%20human-free%2C%20closed-loop%20framework%20that%20uses%20vision-language%20model%20%28VLM%29%20perceptual%20evaluation%20as%20automated%20visual%20feedback%20to%20improve%20an%20LLM-based%20eHMI%20action%20designer.%20Given%20a%20driving%20context%20and%20a%20candidate%20eHMI%20action%2C%20the%20VLM%20evaluates%20the%20perceived%20appropriateness%20of%20the%20action%2C%20and%20this%20feedback%20is%20used%20to%20iteratively%20revise%20the%20designer%27s%20outputs%2C%20enabling%20systematic%20refinement%20without%20human%20supervision.%20We%20evaluate%20our%20framework%20across%20three%20eHMI%20modalities%20%28lightbar%2C%20eyes%2C%20and%20arm%29%20and%20multiple%20LLM%20model%20sizes.%20Across%20settings%2C%20our%20framework%20consistently%20outperforms%20prompt-only%20LLM%20designers%20and%20manually%20specified%20baselines%20in%20both%20VLM-based%20metrics%20and%20human-subject%20evaluations.%20Results%20further%20indicate%20that%20the%20improvements%20generalize%20across%20modalities%20and%20that%20VLM%20evaluations%20are%20well%20aligned%20with%20human%20preferences%2C%20supporting%20the%20robustness%20and%20effectiveness%20of%20See2Refine%20for%20scalable%20action%20design.&entry.1838667208=http%3A//arxiv.org/abs/2602.02063v1&entry.124074799=Read"},
{"title": "No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs", "author": "Liyan Xu and Mo Yu and Fandong Meng and Jie Zhou", "abstract": "This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.", "link": "http://arxiv.org/abs/2602.02103v1", "date": "2026-02-02", "relevancy": 2.564, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5157}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20Global%20Plan%20in%20Chain-of-Thought%3A%20Uncover%20the%20Latent%20Planning%20Horizon%20of%20LLMs&body=Title%3A%20No%20Global%20Plan%20in%20Chain-of-Thought%3A%20Uncover%20the%20Latent%20Planning%20Horizon%20of%20LLMs%0AAuthor%3A%20Liyan%20Xu%20and%20Mo%20Yu%20and%20Fandong%20Meng%20and%20Jie%20Zhou%0AAbstract%3A%20This%20work%20stems%20from%20prior%20complementary%20observations%20on%20the%20dynamics%20of%20Chain-of-Thought%20%28CoT%29%3A%20Large%20Language%20Models%20%28LLMs%29%20is%20shown%20latent%20planning%20of%20subsequent%20reasoning%20prior%20to%20CoT%20emergence%2C%20thereby%20diminishing%20the%20significance%20of%20explicit%20CoT%3B%20whereas%20CoT%20remains%20critical%20for%20tasks%20requiring%20multi-step%20reasoning.%20To%20deepen%20the%20understanding%20between%20LLM%27s%20internal%20states%20and%20its%20verbalized%20reasoning%20trajectories%2C%20we%20investigate%20the%20latent%20planning%20strength%20of%20LLMs%2C%20through%20our%20probing%20method%2C%20Tele-Lens%2C%20applying%20to%20hidden%20states%20across%20diverse%20task%20domains.%20Our%20empirical%20results%20indicate%20that%20LLMs%20exhibit%20a%20myopic%20horizon%2C%20primarily%20conducting%20incremental%20transitions%20without%20precise%20global%20planning.%20Leveraging%20this%20characteristic%2C%20we%20propose%20a%20hypothesis%20on%20enhancing%20uncertainty%20estimation%20of%20CoT%2C%20which%20we%20validate%20that%20a%20small%20subset%20of%20CoT%20positions%20can%20effectively%20represent%20the%20uncertainty%20of%20the%20entire%20path.%20We%20further%20underscore%20the%20significance%20of%20exploiting%20CoT%20dynamics%2C%20and%20demonstrate%20that%20automatic%20recognition%20of%20CoT%20bypass%20can%20be%20achieved%20without%20performance%20degradation.%20Our%20code%2C%20data%20and%20models%20are%20released%20at%20https%3A//github.com/lxucs/tele-lens.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02103v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520Global%2520Plan%2520in%2520Chain-of-Thought%253A%2520Uncover%2520the%2520Latent%2520Planning%2520Horizon%2520of%2520LLMs%26entry.906535625%3DLiyan%2520Xu%2520and%2520Mo%2520Yu%2520and%2520Fandong%2520Meng%2520and%2520Jie%2520Zhou%26entry.1292438233%3DThis%2520work%2520stems%2520from%2520prior%2520complementary%2520observations%2520on%2520the%2520dynamics%2520of%2520Chain-of-Thought%2520%2528CoT%2529%253A%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520shown%2520latent%2520planning%2520of%2520subsequent%2520reasoning%2520prior%2520to%2520CoT%2520emergence%252C%2520thereby%2520diminishing%2520the%2520significance%2520of%2520explicit%2520CoT%253B%2520whereas%2520CoT%2520remains%2520critical%2520for%2520tasks%2520requiring%2520multi-step%2520reasoning.%2520To%2520deepen%2520the%2520understanding%2520between%2520LLM%2527s%2520internal%2520states%2520and%2520its%2520verbalized%2520reasoning%2520trajectories%252C%2520we%2520investigate%2520the%2520latent%2520planning%2520strength%2520of%2520LLMs%252C%2520through%2520our%2520probing%2520method%252C%2520Tele-Lens%252C%2520applying%2520to%2520hidden%2520states%2520across%2520diverse%2520task%2520domains.%2520Our%2520empirical%2520results%2520indicate%2520that%2520LLMs%2520exhibit%2520a%2520myopic%2520horizon%252C%2520primarily%2520conducting%2520incremental%2520transitions%2520without%2520precise%2520global%2520planning.%2520Leveraging%2520this%2520characteristic%252C%2520we%2520propose%2520a%2520hypothesis%2520on%2520enhancing%2520uncertainty%2520estimation%2520of%2520CoT%252C%2520which%2520we%2520validate%2520that%2520a%2520small%2520subset%2520of%2520CoT%2520positions%2520can%2520effectively%2520represent%2520the%2520uncertainty%2520of%2520the%2520entire%2520path.%2520We%2520further%2520underscore%2520the%2520significance%2520of%2520exploiting%2520CoT%2520dynamics%252C%2520and%2520demonstrate%2520that%2520automatic%2520recognition%2520of%2520CoT%2520bypass%2520can%2520be%2520achieved%2520without%2520performance%2520degradation.%2520Our%2520code%252C%2520data%2520and%2520models%2520are%2520released%2520at%2520https%253A//github.com/lxucs/tele-lens.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02103v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20Global%20Plan%20in%20Chain-of-Thought%3A%20Uncover%20the%20Latent%20Planning%20Horizon%20of%20LLMs&entry.906535625=Liyan%20Xu%20and%20Mo%20Yu%20and%20Fandong%20Meng%20and%20Jie%20Zhou&entry.1292438233=This%20work%20stems%20from%20prior%20complementary%20observations%20on%20the%20dynamics%20of%20Chain-of-Thought%20%28CoT%29%3A%20Large%20Language%20Models%20%28LLMs%29%20is%20shown%20latent%20planning%20of%20subsequent%20reasoning%20prior%20to%20CoT%20emergence%2C%20thereby%20diminishing%20the%20significance%20of%20explicit%20CoT%3B%20whereas%20CoT%20remains%20critical%20for%20tasks%20requiring%20multi-step%20reasoning.%20To%20deepen%20the%20understanding%20between%20LLM%27s%20internal%20states%20and%20its%20verbalized%20reasoning%20trajectories%2C%20we%20investigate%20the%20latent%20planning%20strength%20of%20LLMs%2C%20through%20our%20probing%20method%2C%20Tele-Lens%2C%20applying%20to%20hidden%20states%20across%20diverse%20task%20domains.%20Our%20empirical%20results%20indicate%20that%20LLMs%20exhibit%20a%20myopic%20horizon%2C%20primarily%20conducting%20incremental%20transitions%20without%20precise%20global%20planning.%20Leveraging%20this%20characteristic%2C%20we%20propose%20a%20hypothesis%20on%20enhancing%20uncertainty%20estimation%20of%20CoT%2C%20which%20we%20validate%20that%20a%20small%20subset%20of%20CoT%20positions%20can%20effectively%20represent%20the%20uncertainty%20of%20the%20entire%20path.%20We%20further%20underscore%20the%20significance%20of%20exploiting%20CoT%20dynamics%2C%20and%20demonstrate%20that%20automatic%20recognition%20of%20CoT%20bypass%20can%20be%20achieved%20without%20performance%20degradation.%20Our%20code%2C%20data%20and%20models%20are%20released%20at%20https%3A//github.com/lxucs/tele-lens.&entry.1838667208=http%3A//arxiv.org/abs/2602.02103v1&entry.124074799=Read"},
{"title": "Geometric Analysis of Token Selection in Multi-Head Attention", "author": "Timur Mudarisov and Mikhal Burtsev and Tatiana Petrova and Radu State", "abstract": "We present a geometric framework for analysing multi-head attention in large language models (LLMs). Without altering the mechanism, we view standard attention through a top-N selection lens and study its behaviour directly in value-state space. We define geometric metrics - Precision, Recall, and F-score - to quantify separability between selected and non-selected tokens, and derive non-asymptotic bounds with explicit dependence on dimension and margin under empirically motivated assumptions (stable value norms with a compressed sink token, exponential similarity decay, and piecewise attention weight profiles). The theory predicts a small-N operating regime of strongest non-trivial separability and clarifies how sequence length and sink similarity shape the metrics. Empirically, across LLaMA-2-7B, Gemma-7B, and Mistral-7B, measurements closely track the theoretical envelopes: top-N selection sharpens separability, sink similarity correlates with Recall. We also found that in LLaMA-2-7B heads specialize into three regimes - Retriever, Mixer, Reset - with distinct geometric signatures. Overall, attention behaves as a structured geometric classifier with measurable criteria for token selection, offering head level interpretability and informing geometry-aware sparsification and design of attention in LLMs.", "link": "http://arxiv.org/abs/2602.01893v1", "date": "2026-02-02", "relevancy": 2.564, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5344}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5089}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Analysis%20of%20Token%20Selection%20in%20Multi-Head%20Attention&body=Title%3A%20Geometric%20Analysis%20of%20Token%20Selection%20in%20Multi-Head%20Attention%0AAuthor%3A%20Timur%20Mudarisov%20and%20Mikhal%20Burtsev%20and%20Tatiana%20Petrova%20and%20Radu%20State%0AAbstract%3A%20We%20present%20a%20geometric%20framework%20for%20analysing%20multi-head%20attention%20in%20large%20language%20models%20%28LLMs%29.%20Without%20altering%20the%20mechanism%2C%20we%20view%20standard%20attention%20through%20a%20top-N%20selection%20lens%20and%20study%20its%20behaviour%20directly%20in%20value-state%20space.%20We%20define%20geometric%20metrics%20-%20Precision%2C%20Recall%2C%20and%20F-score%20-%20to%20quantify%20separability%20between%20selected%20and%20non-selected%20tokens%2C%20and%20derive%20non-asymptotic%20bounds%20with%20explicit%20dependence%20on%20dimension%20and%20margin%20under%20empirically%20motivated%20assumptions%20%28stable%20value%20norms%20with%20a%20compressed%20sink%20token%2C%20exponential%20similarity%20decay%2C%20and%20piecewise%20attention%20weight%20profiles%29.%20The%20theory%20predicts%20a%20small-N%20operating%20regime%20of%20strongest%20non-trivial%20separability%20and%20clarifies%20how%20sequence%20length%20and%20sink%20similarity%20shape%20the%20metrics.%20Empirically%2C%20across%20LLaMA-2-7B%2C%20Gemma-7B%2C%20and%20Mistral-7B%2C%20measurements%20closely%20track%20the%20theoretical%20envelopes%3A%20top-N%20selection%20sharpens%20separability%2C%20sink%20similarity%20correlates%20with%20Recall.%20We%20also%20found%20that%20in%20LLaMA-2-7B%20heads%20specialize%20into%20three%20regimes%20-%20Retriever%2C%20Mixer%2C%20Reset%20-%20with%20distinct%20geometric%20signatures.%20Overall%2C%20attention%20behaves%20as%20a%20structured%20geometric%20classifier%20with%20measurable%20criteria%20for%20token%20selection%2C%20offering%20head%20level%20interpretability%20and%20informing%20geometry-aware%20sparsification%20and%20design%20of%20attention%20in%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2602.01893v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Analysis%2520of%2520Token%2520Selection%2520in%2520Multi-Head%2520Attention%26entry.906535625%3DTimur%2520Mudarisov%2520and%2520Mikhal%2520Burtsev%2520and%2520Tatiana%2520Petrova%2520and%2520Radu%2520State%26entry.1292438233%3DWe%2520present%2520a%2520geometric%2520framework%2520for%2520analysing%2520multi-head%2520attention%2520in%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Without%2520altering%2520the%2520mechanism%252C%2520we%2520view%2520standard%2520attention%2520through%2520a%2520top-N%2520selection%2520lens%2520and%2520study%2520its%2520behaviour%2520directly%2520in%2520value-state%2520space.%2520We%2520define%2520geometric%2520metrics%2520-%2520Precision%252C%2520Recall%252C%2520and%2520F-score%2520-%2520to%2520quantify%2520separability%2520between%2520selected%2520and%2520non-selected%2520tokens%252C%2520and%2520derive%2520non-asymptotic%2520bounds%2520with%2520explicit%2520dependence%2520on%2520dimension%2520and%2520margin%2520under%2520empirically%2520motivated%2520assumptions%2520%2528stable%2520value%2520norms%2520with%2520a%2520compressed%2520sink%2520token%252C%2520exponential%2520similarity%2520decay%252C%2520and%2520piecewise%2520attention%2520weight%2520profiles%2529.%2520The%2520theory%2520predicts%2520a%2520small-N%2520operating%2520regime%2520of%2520strongest%2520non-trivial%2520separability%2520and%2520clarifies%2520how%2520sequence%2520length%2520and%2520sink%2520similarity%2520shape%2520the%2520metrics.%2520Empirically%252C%2520across%2520LLaMA-2-7B%252C%2520Gemma-7B%252C%2520and%2520Mistral-7B%252C%2520measurements%2520closely%2520track%2520the%2520theoretical%2520envelopes%253A%2520top-N%2520selection%2520sharpens%2520separability%252C%2520sink%2520similarity%2520correlates%2520with%2520Recall.%2520We%2520also%2520found%2520that%2520in%2520LLaMA-2-7B%2520heads%2520specialize%2520into%2520three%2520regimes%2520-%2520Retriever%252C%2520Mixer%252C%2520Reset%2520-%2520with%2520distinct%2520geometric%2520signatures.%2520Overall%252C%2520attention%2520behaves%2520as%2520a%2520structured%2520geometric%2520classifier%2520with%2520measurable%2520criteria%2520for%2520token%2520selection%252C%2520offering%2520head%2520level%2520interpretability%2520and%2520informing%2520geometry-aware%2520sparsification%2520and%2520design%2520of%2520attention%2520in%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.01893v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Analysis%20of%20Token%20Selection%20in%20Multi-Head%20Attention&entry.906535625=Timur%20Mudarisov%20and%20Mikhal%20Burtsev%20and%20Tatiana%20Petrova%20and%20Radu%20State&entry.1292438233=We%20present%20a%20geometric%20framework%20for%20analysing%20multi-head%20attention%20in%20large%20language%20models%20%28LLMs%29.%20Without%20altering%20the%20mechanism%2C%20we%20view%20standard%20attention%20through%20a%20top-N%20selection%20lens%20and%20study%20its%20behaviour%20directly%20in%20value-state%20space.%20We%20define%20geometric%20metrics%20-%20Precision%2C%20Recall%2C%20and%20F-score%20-%20to%20quantify%20separability%20between%20selected%20and%20non-selected%20tokens%2C%20and%20derive%20non-asymptotic%20bounds%20with%20explicit%20dependence%20on%20dimension%20and%20margin%20under%20empirically%20motivated%20assumptions%20%28stable%20value%20norms%20with%20a%20compressed%20sink%20token%2C%20exponential%20similarity%20decay%2C%20and%20piecewise%20attention%20weight%20profiles%29.%20The%20theory%20predicts%20a%20small-N%20operating%20regime%20of%20strongest%20non-trivial%20separability%20and%20clarifies%20how%20sequence%20length%20and%20sink%20similarity%20shape%20the%20metrics.%20Empirically%2C%20across%20LLaMA-2-7B%2C%20Gemma-7B%2C%20and%20Mistral-7B%2C%20measurements%20closely%20track%20the%20theoretical%20envelopes%3A%20top-N%20selection%20sharpens%20separability%2C%20sink%20similarity%20correlates%20with%20Recall.%20We%20also%20found%20that%20in%20LLaMA-2-7B%20heads%20specialize%20into%20three%20regimes%20-%20Retriever%2C%20Mixer%2C%20Reset%20-%20with%20distinct%20geometric%20signatures.%20Overall%2C%20attention%20behaves%20as%20a%20structured%20geometric%20classifier%20with%20measurable%20criteria%20for%20token%20selection%2C%20offering%20head%20level%20interpretability%20and%20informing%20geometry-aware%20sparsification%20and%20design%20of%20attention%20in%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2602.01893v1&entry.124074799=Read"},
{"title": "MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training", "author": "Dulhan Jayalath and Oiwi Parker Jones", "abstract": "Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .", "link": "http://arxiv.org/abs/2602.02494v1", "date": "2026-02-02", "relevancy": 2.5615, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEG-XL%3A%20Data-Efficient%20Brain-to-Text%20via%20Long-Context%20Pre-Training&body=Title%3A%20MEG-XL%3A%20Data-Efficient%20Brain-to-Text%20via%20Long-Context%20Pre-Training%0AAuthor%3A%20Dulhan%20Jayalath%20and%20Oiwi%20Parker%20Jones%0AAbstract%3A%20Clinical%20brain-to-text%20interfaces%20are%20designed%20for%20paralysed%20patients%20who%20cannot%20provide%20extensive%20training%20recordings.%20Pre-training%20improves%20data-efficient%20generalisation%20by%20learning%20statistical%20priors%20across%20subjects%2C%20but%20these%20priors%20critically%20depend%20on%20context.%20While%20natural%20speech%20might%20unfold%20gradually%20over%20minutes%2C%20most%20methods%20pre-train%20with%20only%20a%20few%20seconds%20of%20context.%20Thus%2C%20we%20propose%20MEG-XL%2C%20a%20model%20pre-trained%20with%202.5%20minutes%20of%20MEG%20context%20per%20sample%2C%205-300x%20longer%20than%20prior%20work%2C%20and%20equivalent%20to%20191k%20tokens%2C%20capturing%20extended%20neural%20context.%20Fine-tuning%20on%20the%20task%20of%20word%20decoding%20from%20brain%20data%2C%20MEG-XL%20matches%20supervised%20performance%20with%20a%20fraction%20of%20the%20data%20%28e.g.%201hr%20vs%2050hrs%29%20and%20outperforms%20brain%20foundation%20models.%20We%20find%20that%20models%20pre-trained%20with%20longer%20contexts%20learn%20representations%20that%20transfer%20better%20to%20word%20decoding.%20Our%20results%20indicate%20that%20long-context%20pre-training%20helps%20exploit%20extended%20neural%20context%20that%20other%20methods%20unnecessarily%20discard.%20Code%2C%20model%20weights%2C%20and%20instructions%20are%20available%20at%20https%3A//github.com/neural-processing-lab/MEG-XL%20.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEG-XL%253A%2520Data-Efficient%2520Brain-to-Text%2520via%2520Long-Context%2520Pre-Training%26entry.906535625%3DDulhan%2520Jayalath%2520and%2520Oiwi%2520Parker%2520Jones%26entry.1292438233%3DClinical%2520brain-to-text%2520interfaces%2520are%2520designed%2520for%2520paralysed%2520patients%2520who%2520cannot%2520provide%2520extensive%2520training%2520recordings.%2520Pre-training%2520improves%2520data-efficient%2520generalisation%2520by%2520learning%2520statistical%2520priors%2520across%2520subjects%252C%2520but%2520these%2520priors%2520critically%2520depend%2520on%2520context.%2520While%2520natural%2520speech%2520might%2520unfold%2520gradually%2520over%2520minutes%252C%2520most%2520methods%2520pre-train%2520with%2520only%2520a%2520few%2520seconds%2520of%2520context.%2520Thus%252C%2520we%2520propose%2520MEG-XL%252C%2520a%2520model%2520pre-trained%2520with%25202.5%2520minutes%2520of%2520MEG%2520context%2520per%2520sample%252C%25205-300x%2520longer%2520than%2520prior%2520work%252C%2520and%2520equivalent%2520to%2520191k%2520tokens%252C%2520capturing%2520extended%2520neural%2520context.%2520Fine-tuning%2520on%2520the%2520task%2520of%2520word%2520decoding%2520from%2520brain%2520data%252C%2520MEG-XL%2520matches%2520supervised%2520performance%2520with%2520a%2520fraction%2520of%2520the%2520data%2520%2528e.g.%25201hr%2520vs%252050hrs%2529%2520and%2520outperforms%2520brain%2520foundation%2520models.%2520We%2520find%2520that%2520models%2520pre-trained%2520with%2520longer%2520contexts%2520learn%2520representations%2520that%2520transfer%2520better%2520to%2520word%2520decoding.%2520Our%2520results%2520indicate%2520that%2520long-context%2520pre-training%2520helps%2520exploit%2520extended%2520neural%2520context%2520that%2520other%2520methods%2520unnecessarily%2520discard.%2520Code%252C%2520model%2520weights%252C%2520and%2520instructions%2520are%2520available%2520at%2520https%253A//github.com/neural-processing-lab/MEG-XL%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEG-XL%3A%20Data-Efficient%20Brain-to-Text%20via%20Long-Context%20Pre-Training&entry.906535625=Dulhan%20Jayalath%20and%20Oiwi%20Parker%20Jones&entry.1292438233=Clinical%20brain-to-text%20interfaces%20are%20designed%20for%20paralysed%20patients%20who%20cannot%20provide%20extensive%20training%20recordings.%20Pre-training%20improves%20data-efficient%20generalisation%20by%20learning%20statistical%20priors%20across%20subjects%2C%20but%20these%20priors%20critically%20depend%20on%20context.%20While%20natural%20speech%20might%20unfold%20gradually%20over%20minutes%2C%20most%20methods%20pre-train%20with%20only%20a%20few%20seconds%20of%20context.%20Thus%2C%20we%20propose%20MEG-XL%2C%20a%20model%20pre-trained%20with%202.5%20minutes%20of%20MEG%20context%20per%20sample%2C%205-300x%20longer%20than%20prior%20work%2C%20and%20equivalent%20to%20191k%20tokens%2C%20capturing%20extended%20neural%20context.%20Fine-tuning%20on%20the%20task%20of%20word%20decoding%20from%20brain%20data%2C%20MEG-XL%20matches%20supervised%20performance%20with%20a%20fraction%20of%20the%20data%20%28e.g.%201hr%20vs%2050hrs%29%20and%20outperforms%20brain%20foundation%20models.%20We%20find%20that%20models%20pre-trained%20with%20longer%20contexts%20learn%20representations%20that%20transfer%20better%20to%20word%20decoding.%20Our%20results%20indicate%20that%20long-context%20pre-training%20helps%20exploit%20extended%20neural%20context%20that%20other%20methods%20unnecessarily%20discard.%20Code%2C%20model%20weights%2C%20and%20instructions%20are%20available%20at%20https%3A//github.com/neural-processing-lab/MEG-XL%20.&entry.1838667208=http%3A//arxiv.org/abs/2602.02494v1&entry.124074799=Read"},
{"title": "LIEREx: Language-Image Embeddings for Robotic Exploration", "author": "Felix Igelbrink and Lennart Niecksch and Marian Renz and Martin G\u00fcnther and Martin Atzmueller", "abstract": "Semantic maps allow a robot to reason about its surroundings to fulfill tasks such as navigating known environments, finding specific objects, and exploring unmapped areas. Traditional mapping approaches provide accurate geometric representations but are often constrained by pre-designed symbolic vocabularies. The reliance on fixed object classes makes it impractical to handle out-of-distribution knowledge not defined at design time. Recent advances in Vision-Language Foundation Models, such as CLIP, enable open-set mapping, where objects are encoded as high-dimensional embeddings rather than fixed labels. In LIEREx, we integrate these VLFMs with established 3D Semantic Scene Graphs to enable target-directed exploration by an autonomous agent in partially unknown environments.", "link": "http://arxiv.org/abs/2602.01930v1", "date": "2026-02-02", "relevancy": 2.5607, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6418}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6399}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LIEREx%3A%20Language-Image%20Embeddings%20for%20Robotic%20Exploration&body=Title%3A%20LIEREx%3A%20Language-Image%20Embeddings%20for%20Robotic%20Exploration%0AAuthor%3A%20Felix%20Igelbrink%20and%20Lennart%20Niecksch%20and%20Marian%20Renz%20and%20Martin%20G%C3%BCnther%20and%20Martin%20Atzmueller%0AAbstract%3A%20Semantic%20maps%20allow%20a%20robot%20to%20reason%20about%20its%20surroundings%20to%20fulfill%20tasks%20such%20as%20navigating%20known%20environments%2C%20finding%20specific%20objects%2C%20and%20exploring%20unmapped%20areas.%20Traditional%20mapping%20approaches%20provide%20accurate%20geometric%20representations%20but%20are%20often%20constrained%20by%20pre-designed%20symbolic%20vocabularies.%20The%20reliance%20on%20fixed%20object%20classes%20makes%20it%20impractical%20to%20handle%20out-of-distribution%20knowledge%20not%20defined%20at%20design%20time.%20Recent%20advances%20in%20Vision-Language%20Foundation%20Models%2C%20such%20as%20CLIP%2C%20enable%20open-set%20mapping%2C%20where%20objects%20are%20encoded%20as%20high-dimensional%20embeddings%20rather%20than%20fixed%20labels.%20In%20LIEREx%2C%20we%20integrate%20these%20VLFMs%20with%20established%203D%20Semantic%20Scene%20Graphs%20to%20enable%20target-directed%20exploration%20by%20an%20autonomous%20agent%20in%20partially%20unknown%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2602.01930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLIEREx%253A%2520Language-Image%2520Embeddings%2520for%2520Robotic%2520Exploration%26entry.906535625%3DFelix%2520Igelbrink%2520and%2520Lennart%2520Niecksch%2520and%2520Marian%2520Renz%2520and%2520Martin%2520G%25C3%25BCnther%2520and%2520Martin%2520Atzmueller%26entry.1292438233%3DSemantic%2520maps%2520allow%2520a%2520robot%2520to%2520reason%2520about%2520its%2520surroundings%2520to%2520fulfill%2520tasks%2520such%2520as%2520navigating%2520known%2520environments%252C%2520finding%2520specific%2520objects%252C%2520and%2520exploring%2520unmapped%2520areas.%2520Traditional%2520mapping%2520approaches%2520provide%2520accurate%2520geometric%2520representations%2520but%2520are%2520often%2520constrained%2520by%2520pre-designed%2520symbolic%2520vocabularies.%2520The%2520reliance%2520on%2520fixed%2520object%2520classes%2520makes%2520it%2520impractical%2520to%2520handle%2520out-of-distribution%2520knowledge%2520not%2520defined%2520at%2520design%2520time.%2520Recent%2520advances%2520in%2520Vision-Language%2520Foundation%2520Models%252C%2520such%2520as%2520CLIP%252C%2520enable%2520open-set%2520mapping%252C%2520where%2520objects%2520are%2520encoded%2520as%2520high-dimensional%2520embeddings%2520rather%2520than%2520fixed%2520labels.%2520In%2520LIEREx%252C%2520we%2520integrate%2520these%2520VLFMs%2520with%2520established%25203D%2520Semantic%2520Scene%2520Graphs%2520to%2520enable%2520target-directed%2520exploration%2520by%2520an%2520autonomous%2520agent%2520in%2520partially%2520unknown%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.01930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LIEREx%3A%20Language-Image%20Embeddings%20for%20Robotic%20Exploration&entry.906535625=Felix%20Igelbrink%20and%20Lennart%20Niecksch%20and%20Marian%20Renz%20and%20Martin%20G%C3%BCnther%20and%20Martin%20Atzmueller&entry.1292438233=Semantic%20maps%20allow%20a%20robot%20to%20reason%20about%20its%20surroundings%20to%20fulfill%20tasks%20such%20as%20navigating%20known%20environments%2C%20finding%20specific%20objects%2C%20and%20exploring%20unmapped%20areas.%20Traditional%20mapping%20approaches%20provide%20accurate%20geometric%20representations%20but%20are%20often%20constrained%20by%20pre-designed%20symbolic%20vocabularies.%20The%20reliance%20on%20fixed%20object%20classes%20makes%20it%20impractical%20to%20handle%20out-of-distribution%20knowledge%20not%20defined%20at%20design%20time.%20Recent%20advances%20in%20Vision-Language%20Foundation%20Models%2C%20such%20as%20CLIP%2C%20enable%20open-set%20mapping%2C%20where%20objects%20are%20encoded%20as%20high-dimensional%20embeddings%20rather%20than%20fixed%20labels.%20In%20LIEREx%2C%20we%20integrate%20these%20VLFMs%20with%20established%203D%20Semantic%20Scene%20Graphs%20to%20enable%20target-directed%20exploration%20by%20an%20autonomous%20agent%20in%20partially%20unknown%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2602.01930v1&entry.124074799=Read"},
{"title": "Learning Robust Intervention Representations with Delta Embeddings", "author": "Panagiotis Alimisis and Christos Diou", "abstract": "Causal representation learning has attracted significant research interest during the past few years, as a means for improving model generalization and robustness. Causal representations of interventional image pairs (also called ``actionable counterfactuals'' in the literature), have the property that only variables corresponding to scene elements affected by the intervention / action are changed between the start state and the end state. While most work in this area has focused on identifying and representing the variables of the scene under a causal model, fewer efforts have focused on representations of the interventions themselves. In this work, we show that an effective strategy for improving out of distribution (OOD) robustness is to focus on the representation of actionable counterfactuals in the latent space. Specifically, we propose that an intervention can be represented by a Causal Delta Embedding that is invariant to the visual scene and sparse in terms of the causal variables it affects. Leveraging this insight, we propose a method for learning causal representations from image pairs, without any additional supervision. Experiments in the Causal Triplet challenge demonstrate that Causal Delta Embeddings are highly effective in OOD settings, significantly exceeding baseline performance in both synthetic and real-world benchmarks.", "link": "http://arxiv.org/abs/2508.04492v2", "date": "2026-02-02", "relevancy": 2.5455, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5128}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5089}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Robust%20Intervention%20Representations%20with%20Delta%20Embeddings&body=Title%3A%20Learning%20Robust%20Intervention%20Representations%20with%20Delta%20Embeddings%0AAuthor%3A%20Panagiotis%20Alimisis%20and%20Christos%20Diou%0AAbstract%3A%20Causal%20representation%20learning%20has%20attracted%20significant%20research%20interest%20during%20the%20past%20few%20years%2C%20as%20a%20means%20for%20improving%20model%20generalization%20and%20robustness.%20Causal%20representations%20of%20interventional%20image%20pairs%20%28also%20called%20%60%60actionable%20counterfactuals%27%27%20in%20the%20literature%29%2C%20have%20the%20property%20that%20only%20variables%20corresponding%20to%20scene%20elements%20affected%20by%20the%20intervention%20/%20action%20are%20changed%20between%20the%20start%20state%20and%20the%20end%20state.%20While%20most%20work%20in%20this%20area%20has%20focused%20on%20identifying%20and%20representing%20the%20variables%20of%20the%20scene%20under%20a%20causal%20model%2C%20fewer%20efforts%20have%20focused%20on%20representations%20of%20the%20interventions%20themselves.%20In%20this%20work%2C%20we%20show%20that%20an%20effective%20strategy%20for%20improving%20out%20of%20distribution%20%28OOD%29%20robustness%20is%20to%20focus%20on%20the%20representation%20of%20actionable%20counterfactuals%20in%20the%20latent%20space.%20Specifically%2C%20we%20propose%20that%20an%20intervention%20can%20be%20represented%20by%20a%20Causal%20Delta%20Embedding%20that%20is%20invariant%20to%20the%20visual%20scene%20and%20sparse%20in%20terms%20of%20the%20causal%20variables%20it%20affects.%20Leveraging%20this%20insight%2C%20we%20propose%20a%20method%20for%20learning%20causal%20representations%20from%20image%20pairs%2C%20without%20any%20additional%20supervision.%20Experiments%20in%20the%20Causal%20Triplet%20challenge%20demonstrate%20that%20Causal%20Delta%20Embeddings%20are%20highly%20effective%20in%20OOD%20settings%2C%20significantly%20exceeding%20baseline%20performance%20in%20both%20synthetic%20and%20real-world%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2508.04492v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Robust%2520Intervention%2520Representations%2520with%2520Delta%2520Embeddings%26entry.906535625%3DPanagiotis%2520Alimisis%2520and%2520Christos%2520Diou%26entry.1292438233%3DCausal%2520representation%2520learning%2520has%2520attracted%2520significant%2520research%2520interest%2520during%2520the%2520past%2520few%2520years%252C%2520as%2520a%2520means%2520for%2520improving%2520model%2520generalization%2520and%2520robustness.%2520Causal%2520representations%2520of%2520interventional%2520image%2520pairs%2520%2528also%2520called%2520%2560%2560actionable%2520counterfactuals%2527%2527%2520in%2520the%2520literature%2529%252C%2520have%2520the%2520property%2520that%2520only%2520variables%2520corresponding%2520to%2520scene%2520elements%2520affected%2520by%2520the%2520intervention%2520/%2520action%2520are%2520changed%2520between%2520the%2520start%2520state%2520and%2520the%2520end%2520state.%2520While%2520most%2520work%2520in%2520this%2520area%2520has%2520focused%2520on%2520identifying%2520and%2520representing%2520the%2520variables%2520of%2520the%2520scene%2520under%2520a%2520causal%2520model%252C%2520fewer%2520efforts%2520have%2520focused%2520on%2520representations%2520of%2520the%2520interventions%2520themselves.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520an%2520effective%2520strategy%2520for%2520improving%2520out%2520of%2520distribution%2520%2528OOD%2529%2520robustness%2520is%2520to%2520focus%2520on%2520the%2520representation%2520of%2520actionable%2520counterfactuals%2520in%2520the%2520latent%2520space.%2520Specifically%252C%2520we%2520propose%2520that%2520an%2520intervention%2520can%2520be%2520represented%2520by%2520a%2520Causal%2520Delta%2520Embedding%2520that%2520is%2520invariant%2520to%2520the%2520visual%2520scene%2520and%2520sparse%2520in%2520terms%2520of%2520the%2520causal%2520variables%2520it%2520affects.%2520Leveraging%2520this%2520insight%252C%2520we%2520propose%2520a%2520method%2520for%2520learning%2520causal%2520representations%2520from%2520image%2520pairs%252C%2520without%2520any%2520additional%2520supervision.%2520Experiments%2520in%2520the%2520Causal%2520Triplet%2520challenge%2520demonstrate%2520that%2520Causal%2520Delta%2520Embeddings%2520are%2520highly%2520effective%2520in%2520OOD%2520settings%252C%2520significantly%2520exceeding%2520baseline%2520performance%2520in%2520both%2520synthetic%2520and%2520real-world%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04492v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Robust%20Intervention%20Representations%20with%20Delta%20Embeddings&entry.906535625=Panagiotis%20Alimisis%20and%20Christos%20Diou&entry.1292438233=Causal%20representation%20learning%20has%20attracted%20significant%20research%20interest%20during%20the%20past%20few%20years%2C%20as%20a%20means%20for%20improving%20model%20generalization%20and%20robustness.%20Causal%20representations%20of%20interventional%20image%20pairs%20%28also%20called%20%60%60actionable%20counterfactuals%27%27%20in%20the%20literature%29%2C%20have%20the%20property%20that%20only%20variables%20corresponding%20to%20scene%20elements%20affected%20by%20the%20intervention%20/%20action%20are%20changed%20between%20the%20start%20state%20and%20the%20end%20state.%20While%20most%20work%20in%20this%20area%20has%20focused%20on%20identifying%20and%20representing%20the%20variables%20of%20the%20scene%20under%20a%20causal%20model%2C%20fewer%20efforts%20have%20focused%20on%20representations%20of%20the%20interventions%20themselves.%20In%20this%20work%2C%20we%20show%20that%20an%20effective%20strategy%20for%20improving%20out%20of%20distribution%20%28OOD%29%20robustness%20is%20to%20focus%20on%20the%20representation%20of%20actionable%20counterfactuals%20in%20the%20latent%20space.%20Specifically%2C%20we%20propose%20that%20an%20intervention%20can%20be%20represented%20by%20a%20Causal%20Delta%20Embedding%20that%20is%20invariant%20to%20the%20visual%20scene%20and%20sparse%20in%20terms%20of%20the%20causal%20variables%20it%20affects.%20Leveraging%20this%20insight%2C%20we%20propose%20a%20method%20for%20learning%20causal%20representations%20from%20image%20pairs%2C%20without%20any%20additional%20supervision.%20Experiments%20in%20the%20Causal%20Triplet%20challenge%20demonstrate%20that%20Causal%20Delta%20Embeddings%20are%20highly%20effective%20in%20OOD%20settings%2C%20significantly%20exceeding%20baseline%20performance%20in%20both%20synthetic%20and%20real-world%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2508.04492v2&entry.124074799=Read"},
{"title": "Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling", "author": "Andong Chen and Wenxin Zhu and Qiuyu Ding and Yuchen Song and Muyun Yang and Tiejun Zhao", "abstract": "Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.", "link": "http://arxiv.org/abs/2602.02453v1", "date": "2026-02-02", "relevancy": 2.5314, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5171}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5171}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinking%20with%20Comics%3A%20Enhancing%20Multimodal%20Reasoning%20through%20Structured%20Visual%20Storytelling&body=Title%3A%20Thinking%20with%20Comics%3A%20Enhancing%20Multimodal%20Reasoning%20through%20Structured%20Visual%20Storytelling%0AAuthor%3A%20Andong%20Chen%20and%20Wenxin%20Zhu%20and%20Qiuyu%20Ding%20and%20Yuchen%20Song%20and%20Muyun%20Yang%20and%20Tiejun%20Zhao%0AAbstract%3A%20Chain-of-Thought%20reasoning%20has%20driven%20large%20language%20models%20to%20extend%20from%20thinking%20with%20text%20to%20thinking%20with%20images%20and%20videos.%20However%2C%20different%20modalities%20still%20have%20clear%20limitations%3A%20static%20images%20struggle%20to%20represent%20temporal%20structure%2C%20while%20videos%20introduce%20substantial%20redundancy%20and%20computational%20cost.%20In%20this%20work%2C%20we%20propose%20Thinking%20with%20Comics%2C%20a%20visual%20reasoning%20paradigm%20that%20uses%20comics%20as%20a%20high%20information-density%20medium%20positioned%20between%20images%20and%20videos.%20Comics%20preserve%20temporal%20structure%2C%20embedded%20text%2C%20and%20narrative%20coherence%20while%20requiring%20significantly%20lower%20reasoning%20cost.%20We%20systematically%20study%20two%20reasoning%20paths%20based%20on%20comics%20and%20evaluate%20them%20on%20a%20range%20of%20reasoning%20tasks%20and%20long-context%20understanding%20tasks.%20Experimental%20results%20show%20that%20Thinking%20with%20Comics%20outperforms%20Thinking%20with%20Images%20on%20multi-step%20temporal%20and%20causal%20reasoning%20tasks%2C%20while%20remaining%20substantially%20more%20efficient%20than%20Thinking%20with%20Video.%20Further%20analysis%20indicates%20that%20different%20comic%20narrative%20structures%20and%20styles%20consistently%20affect%20performance%20across%20tasks%2C%20suggesting%20that%20comics%20serve%20as%20an%20effective%20intermediate%20visual%20representation%20for%20improving%20multimodal%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinking%2520with%2520Comics%253A%2520Enhancing%2520Multimodal%2520Reasoning%2520through%2520Structured%2520Visual%2520Storytelling%26entry.906535625%3DAndong%2520Chen%2520and%2520Wenxin%2520Zhu%2520and%2520Qiuyu%2520Ding%2520and%2520Yuchen%2520Song%2520and%2520Muyun%2520Yang%2520and%2520Tiejun%2520Zhao%26entry.1292438233%3DChain-of-Thought%2520reasoning%2520has%2520driven%2520large%2520language%2520models%2520to%2520extend%2520from%2520thinking%2520with%2520text%2520to%2520thinking%2520with%2520images%2520and%2520videos.%2520However%252C%2520different%2520modalities%2520still%2520have%2520clear%2520limitations%253A%2520static%2520images%2520struggle%2520to%2520represent%2520temporal%2520structure%252C%2520while%2520videos%2520introduce%2520substantial%2520redundancy%2520and%2520computational%2520cost.%2520In%2520this%2520work%252C%2520we%2520propose%2520Thinking%2520with%2520Comics%252C%2520a%2520visual%2520reasoning%2520paradigm%2520that%2520uses%2520comics%2520as%2520a%2520high%2520information-density%2520medium%2520positioned%2520between%2520images%2520and%2520videos.%2520Comics%2520preserve%2520temporal%2520structure%252C%2520embedded%2520text%252C%2520and%2520narrative%2520coherence%2520while%2520requiring%2520significantly%2520lower%2520reasoning%2520cost.%2520We%2520systematically%2520study%2520two%2520reasoning%2520paths%2520based%2520on%2520comics%2520and%2520evaluate%2520them%2520on%2520a%2520range%2520of%2520reasoning%2520tasks%2520and%2520long-context%2520understanding%2520tasks.%2520Experimental%2520results%2520show%2520that%2520Thinking%2520with%2520Comics%2520outperforms%2520Thinking%2520with%2520Images%2520on%2520multi-step%2520temporal%2520and%2520causal%2520reasoning%2520tasks%252C%2520while%2520remaining%2520substantially%2520more%2520efficient%2520than%2520Thinking%2520with%2520Video.%2520Further%2520analysis%2520indicates%2520that%2520different%2520comic%2520narrative%2520structures%2520and%2520styles%2520consistently%2520affect%2520performance%2520across%2520tasks%252C%2520suggesting%2520that%2520comics%2520serve%2520as%2520an%2520effective%2520intermediate%2520visual%2520representation%2520for%2520improving%2520multimodal%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinking%20with%20Comics%3A%20Enhancing%20Multimodal%20Reasoning%20through%20Structured%20Visual%20Storytelling&entry.906535625=Andong%20Chen%20and%20Wenxin%20Zhu%20and%20Qiuyu%20Ding%20and%20Yuchen%20Song%20and%20Muyun%20Yang%20and%20Tiejun%20Zhao&entry.1292438233=Chain-of-Thought%20reasoning%20has%20driven%20large%20language%20models%20to%20extend%20from%20thinking%20with%20text%20to%20thinking%20with%20images%20and%20videos.%20However%2C%20different%20modalities%20still%20have%20clear%20limitations%3A%20static%20images%20struggle%20to%20represent%20temporal%20structure%2C%20while%20videos%20introduce%20substantial%20redundancy%20and%20computational%20cost.%20In%20this%20work%2C%20we%20propose%20Thinking%20with%20Comics%2C%20a%20visual%20reasoning%20paradigm%20that%20uses%20comics%20as%20a%20high%20information-density%20medium%20positioned%20between%20images%20and%20videos.%20Comics%20preserve%20temporal%20structure%2C%20embedded%20text%2C%20and%20narrative%20coherence%20while%20requiring%20significantly%20lower%20reasoning%20cost.%20We%20systematically%20study%20two%20reasoning%20paths%20based%20on%20comics%20and%20evaluate%20them%20on%20a%20range%20of%20reasoning%20tasks%20and%20long-context%20understanding%20tasks.%20Experimental%20results%20show%20that%20Thinking%20with%20Comics%20outperforms%20Thinking%20with%20Images%20on%20multi-step%20temporal%20and%20causal%20reasoning%20tasks%2C%20while%20remaining%20substantially%20more%20efficient%20than%20Thinking%20with%20Video.%20Further%20analysis%20indicates%20that%20different%20comic%20narrative%20structures%20and%20styles%20consistently%20affect%20performance%20across%20tasks%2C%20suggesting%20that%20comics%20serve%20as%20an%20effective%20intermediate%20visual%20representation%20for%20improving%20multimodal%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2602.02453v1&entry.124074799=Read"},
{"title": "Zero-Shot Off-Policy Learning", "author": "Arip Asadulaev and Maksim Bobrin and Salem Lahlou and Dmitry Dylov and Fakhri Karray and Martin Takac", "abstract": "Off-policy learning methods seek to derive an optimal policy directly from a fixed dataset of prior interactions. This objective presents significant challenges, primarily due to the inherent distributional shift and value function overestimation bias. These issues become even more noticeable in zero-shot reinforcement learning, where an agent trained on reward-free data must adapt to new tasks at test time without additional training. In this work, we address the off-policy problem in a zero-shot setting by discovering a theoretical connection of successor measures to stationary density ratios. Using this insight, our algorithm can infer optimal importance sampling ratios, effectively performing a stationary distribution correction with an optimal policy for any task on the fly. We benchmark our method in motion tracking tasks on SMPL Humanoid, continuous control on ExoRL, and for the long-horizon OGBench tasks. Our technique seamlessly integrates into forward-backward representation frameworks and enables fast-adaptation to new tasks in a training-free regime. More broadly, this work bridges off-policy learning and zero-shot adaptation, offering benefits to both research areas.", "link": "http://arxiv.org/abs/2602.01962v1", "date": "2026-02-02", "relevancy": 2.5114, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5076}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Off-Policy%20Learning&body=Title%3A%20Zero-Shot%20Off-Policy%20Learning%0AAuthor%3A%20Arip%20Asadulaev%20and%20Maksim%20Bobrin%20and%20Salem%20Lahlou%20and%20Dmitry%20Dylov%20and%20Fakhri%20Karray%20and%20Martin%20Takac%0AAbstract%3A%20Off-policy%20learning%20methods%20seek%20to%20derive%20an%20optimal%20policy%20directly%20from%20a%20fixed%20dataset%20of%20prior%20interactions.%20This%20objective%20presents%20significant%20challenges%2C%20primarily%20due%20to%20the%20inherent%20distributional%20shift%20and%20value%20function%20overestimation%20bias.%20These%20issues%20become%20even%20more%20noticeable%20in%20zero-shot%20reinforcement%20learning%2C%20where%20an%20agent%20trained%20on%20reward-free%20data%20must%20adapt%20to%20new%20tasks%20at%20test%20time%20without%20additional%20training.%20In%20this%20work%2C%20we%20address%20the%20off-policy%20problem%20in%20a%20zero-shot%20setting%20by%20discovering%20a%20theoretical%20connection%20of%20successor%20measures%20to%20stationary%20density%20ratios.%20Using%20this%20insight%2C%20our%20algorithm%20can%20infer%20optimal%20importance%20sampling%20ratios%2C%20effectively%20performing%20a%20stationary%20distribution%20correction%20with%20an%20optimal%20policy%20for%20any%20task%20on%20the%20fly.%20We%20benchmark%20our%20method%20in%20motion%20tracking%20tasks%20on%20SMPL%20Humanoid%2C%20continuous%20control%20on%20ExoRL%2C%20and%20for%20the%20long-horizon%20OGBench%20tasks.%20Our%20technique%20seamlessly%20integrates%20into%20forward-backward%20representation%20frameworks%20and%20enables%20fast-adaptation%20to%20new%20tasks%20in%20a%20training-free%20regime.%20More%20broadly%2C%20this%20work%20bridges%20off-policy%20learning%20and%20zero-shot%20adaptation%2C%20offering%20benefits%20to%20both%20research%20areas.%0ALink%3A%20http%3A//arxiv.org/abs/2602.01962v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Off-Policy%2520Learning%26entry.906535625%3DArip%2520Asadulaev%2520and%2520Maksim%2520Bobrin%2520and%2520Salem%2520Lahlou%2520and%2520Dmitry%2520Dylov%2520and%2520Fakhri%2520Karray%2520and%2520Martin%2520Takac%26entry.1292438233%3DOff-policy%2520learning%2520methods%2520seek%2520to%2520derive%2520an%2520optimal%2520policy%2520directly%2520from%2520a%2520fixed%2520dataset%2520of%2520prior%2520interactions.%2520This%2520objective%2520presents%2520significant%2520challenges%252C%2520primarily%2520due%2520to%2520the%2520inherent%2520distributional%2520shift%2520and%2520value%2520function%2520overestimation%2520bias.%2520These%2520issues%2520become%2520even%2520more%2520noticeable%2520in%2520zero-shot%2520reinforcement%2520learning%252C%2520where%2520an%2520agent%2520trained%2520on%2520reward-free%2520data%2520must%2520adapt%2520to%2520new%2520tasks%2520at%2520test%2520time%2520without%2520additional%2520training.%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520off-policy%2520problem%2520in%2520a%2520zero-shot%2520setting%2520by%2520discovering%2520a%2520theoretical%2520connection%2520of%2520successor%2520measures%2520to%2520stationary%2520density%2520ratios.%2520Using%2520this%2520insight%252C%2520our%2520algorithm%2520can%2520infer%2520optimal%2520importance%2520sampling%2520ratios%252C%2520effectively%2520performing%2520a%2520stationary%2520distribution%2520correction%2520with%2520an%2520optimal%2520policy%2520for%2520any%2520task%2520on%2520the%2520fly.%2520We%2520benchmark%2520our%2520method%2520in%2520motion%2520tracking%2520tasks%2520on%2520SMPL%2520Humanoid%252C%2520continuous%2520control%2520on%2520ExoRL%252C%2520and%2520for%2520the%2520long-horizon%2520OGBench%2520tasks.%2520Our%2520technique%2520seamlessly%2520integrates%2520into%2520forward-backward%2520representation%2520frameworks%2520and%2520enables%2520fast-adaptation%2520to%2520new%2520tasks%2520in%2520a%2520training-free%2520regime.%2520More%2520broadly%252C%2520this%2520work%2520bridges%2520off-policy%2520learning%2520and%2520zero-shot%2520adaptation%252C%2520offering%2520benefits%2520to%2520both%2520research%2520areas.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.01962v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Off-Policy%20Learning&entry.906535625=Arip%20Asadulaev%20and%20Maksim%20Bobrin%20and%20Salem%20Lahlou%20and%20Dmitry%20Dylov%20and%20Fakhri%20Karray%20and%20Martin%20Takac&entry.1292438233=Off-policy%20learning%20methods%20seek%20to%20derive%20an%20optimal%20policy%20directly%20from%20a%20fixed%20dataset%20of%20prior%20interactions.%20This%20objective%20presents%20significant%20challenges%2C%20primarily%20due%20to%20the%20inherent%20distributional%20shift%20and%20value%20function%20overestimation%20bias.%20These%20issues%20become%20even%20more%20noticeable%20in%20zero-shot%20reinforcement%20learning%2C%20where%20an%20agent%20trained%20on%20reward-free%20data%20must%20adapt%20to%20new%20tasks%20at%20test%20time%20without%20additional%20training.%20In%20this%20work%2C%20we%20address%20the%20off-policy%20problem%20in%20a%20zero-shot%20setting%20by%20discovering%20a%20theoretical%20connection%20of%20successor%20measures%20to%20stationary%20density%20ratios.%20Using%20this%20insight%2C%20our%20algorithm%20can%20infer%20optimal%20importance%20sampling%20ratios%2C%20effectively%20performing%20a%20stationary%20distribution%20correction%20with%20an%20optimal%20policy%20for%20any%20task%20on%20the%20fly.%20We%20benchmark%20our%20method%20in%20motion%20tracking%20tasks%20on%20SMPL%20Humanoid%2C%20continuous%20control%20on%20ExoRL%2C%20and%20for%20the%20long-horizon%20OGBench%20tasks.%20Our%20technique%20seamlessly%20integrates%20into%20forward-backward%20representation%20frameworks%20and%20enables%20fast-adaptation%20to%20new%20tasks%20in%20a%20training-free%20regime.%20More%20broadly%2C%20this%20work%20bridges%20off-policy%20learning%20and%20zero-shot%20adaptation%2C%20offering%20benefits%20to%20both%20research%20areas.&entry.1838667208=http%3A//arxiv.org/abs/2602.01962v1&entry.124074799=Read"},
{"title": "Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models", "author": "Cristian Sbrolli and Matteo Matteucci and Toshihiko Yamasaki", "abstract": "Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing \"a red cube and a blue sphere\" with \"a blue cube and a red sphere\". Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., \"a monitor to the left of a bicycle on a white background\") and LLM-generated Contextual captions (e.g., \"In a brightly lit photography studio, a monitor is positioned to the left of a bicycle\"), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel \"Confusion Benchmark\" reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp).", "link": "http://arxiv.org/abs/2602.02043v1", "date": "2026-02-02", "relevancy": 2.5108, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6392}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6392}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auto-Comp%3A%20An%20Automated%20Pipeline%20for%20Scalable%20Compositional%20Probing%20of%20Contrastive%20Vision-Language%20Models&body=Title%3A%20Auto-Comp%3A%20An%20Automated%20Pipeline%20for%20Scalable%20Compositional%20Probing%20of%20Contrastive%20Vision-Language%20Models%0AAuthor%3A%20Cristian%20Sbrolli%20and%20Matteo%20Matteucci%20and%20Toshihiko%20Yamasaki%0AAbstract%3A%20Modern%20Vision-Language%20Models%20%28VLMs%29%20exhibit%20a%20critical%20flaw%20in%20compositional%20reasoning%2C%20often%20confusing%20%22a%20red%20cube%20and%20a%20blue%20sphere%22%20with%20%22a%20blue%20cube%20and%20a%20red%20sphere%22.%20Disentangling%20the%20visual%20and%20linguistic%20roots%20of%20these%20failures%20is%20a%20fundamental%20challenge%20for%20robust%20evaluation.%20To%20enable%20fine-grained%2C%20controllable%20analysis%2C%20we%20introduce%20Auto-Comp%2C%20a%20fully%20automated%20and%20synthetic%20pipeline%20for%20generating%20scalable%20benchmarks.%20Its%20controllable%20nature%20is%20key%20to%20dissecting%20and%20isolating%20different%20reasoning%20skills.%20Auto-Comp%20generates%20paired%20images%20from%20Minimal%20%28e.g.%2C%20%22a%20monitor%20to%20the%20left%20of%20a%20bicycle%20on%20a%20white%20background%22%29%20and%20LLM-generated%20Contextual%20captions%20%28e.g.%2C%20%22In%20a%20brightly%20lit%20photography%20studio%2C%20a%20monitor%20is%20positioned%20to%20the%20left%20of%20a%20bicycle%22%29%2C%20allowing%20a%20controlled%20A/B%20test%20to%20disentangle%20core%20binding%20ability%20from%20visio-linguistic%20complexity.%20Our%20evaluation%20of%2020%20VLMs%20on%20novel%20benchmarks%20for%20color%20binding%20and%20spatial%20relations%20reveals%20universal%20compositional%20failures%20in%20both%20CLIP%20and%20SigLIP%20model%20families.%20Crucially%2C%20our%20novel%20%22Confusion%20Benchmark%22%20reveals%20a%20deeper%20flaw%20beyond%20simple%20attribute%20swaps%3A%20models%20are%20highly%20susceptible%20to%20low-entropy%20distractors%20%28e.g.%2C%20repeated%20objects%20or%20colors%29%2C%20demonstrating%20their%20compositional%20failures%20extend%20beyond%20known%20bag-of-words%20limitations.%20we%20uncover%20a%20surprising%20trade-off%3A%20visio-linguistic%20context%2C%20which%20provides%20global%20scene%20cues%2C%20aids%20spatial%20reasoning%20but%20simultaneously%20hinders%20local%20attribute%20binding%20by%20introducing%20visual%20clutter.%20We%20release%20the%20Auto-Comp%20pipeline%20to%20facilitate%20future%20benchmark%20creation%2C%20alongside%20all%20our%20generated%20benchmarks%20%28https%3A//huggingface.co/AutoComp%29.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02043v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuto-Comp%253A%2520An%2520Automated%2520Pipeline%2520for%2520Scalable%2520Compositional%2520Probing%2520of%2520Contrastive%2520Vision-Language%2520Models%26entry.906535625%3DCristian%2520Sbrolli%2520and%2520Matteo%2520Matteucci%2520and%2520Toshihiko%2520Yamasaki%26entry.1292438233%3DModern%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520exhibit%2520a%2520critical%2520flaw%2520in%2520compositional%2520reasoning%252C%2520often%2520confusing%2520%2522a%2520red%2520cube%2520and%2520a%2520blue%2520sphere%2522%2520with%2520%2522a%2520blue%2520cube%2520and%2520a%2520red%2520sphere%2522.%2520Disentangling%2520the%2520visual%2520and%2520linguistic%2520roots%2520of%2520these%2520failures%2520is%2520a%2520fundamental%2520challenge%2520for%2520robust%2520evaluation.%2520To%2520enable%2520fine-grained%252C%2520controllable%2520analysis%252C%2520we%2520introduce%2520Auto-Comp%252C%2520a%2520fully%2520automated%2520and%2520synthetic%2520pipeline%2520for%2520generating%2520scalable%2520benchmarks.%2520Its%2520controllable%2520nature%2520is%2520key%2520to%2520dissecting%2520and%2520isolating%2520different%2520reasoning%2520skills.%2520Auto-Comp%2520generates%2520paired%2520images%2520from%2520Minimal%2520%2528e.g.%252C%2520%2522a%2520monitor%2520to%2520the%2520left%2520of%2520a%2520bicycle%2520on%2520a%2520white%2520background%2522%2529%2520and%2520LLM-generated%2520Contextual%2520captions%2520%2528e.g.%252C%2520%2522In%2520a%2520brightly%2520lit%2520photography%2520studio%252C%2520a%2520monitor%2520is%2520positioned%2520to%2520the%2520left%2520of%2520a%2520bicycle%2522%2529%252C%2520allowing%2520a%2520controlled%2520A/B%2520test%2520to%2520disentangle%2520core%2520binding%2520ability%2520from%2520visio-linguistic%2520complexity.%2520Our%2520evaluation%2520of%252020%2520VLMs%2520on%2520novel%2520benchmarks%2520for%2520color%2520binding%2520and%2520spatial%2520relations%2520reveals%2520universal%2520compositional%2520failures%2520in%2520both%2520CLIP%2520and%2520SigLIP%2520model%2520families.%2520Crucially%252C%2520our%2520novel%2520%2522Confusion%2520Benchmark%2522%2520reveals%2520a%2520deeper%2520flaw%2520beyond%2520simple%2520attribute%2520swaps%253A%2520models%2520are%2520highly%2520susceptible%2520to%2520low-entropy%2520distractors%2520%2528e.g.%252C%2520repeated%2520objects%2520or%2520colors%2529%252C%2520demonstrating%2520their%2520compositional%2520failures%2520extend%2520beyond%2520known%2520bag-of-words%2520limitations.%2520we%2520uncover%2520a%2520surprising%2520trade-off%253A%2520visio-linguistic%2520context%252C%2520which%2520provides%2520global%2520scene%2520cues%252C%2520aids%2520spatial%2520reasoning%2520but%2520simultaneously%2520hinders%2520local%2520attribute%2520binding%2520by%2520introducing%2520visual%2520clutter.%2520We%2520release%2520the%2520Auto-Comp%2520pipeline%2520to%2520facilitate%2520future%2520benchmark%2520creation%252C%2520alongside%2520all%2520our%2520generated%2520benchmarks%2520%2528https%253A//huggingface.co/AutoComp%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02043v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auto-Comp%3A%20An%20Automated%20Pipeline%20for%20Scalable%20Compositional%20Probing%20of%20Contrastive%20Vision-Language%20Models&entry.906535625=Cristian%20Sbrolli%20and%20Matteo%20Matteucci%20and%20Toshihiko%20Yamasaki&entry.1292438233=Modern%20Vision-Language%20Models%20%28VLMs%29%20exhibit%20a%20critical%20flaw%20in%20compositional%20reasoning%2C%20often%20confusing%20%22a%20red%20cube%20and%20a%20blue%20sphere%22%20with%20%22a%20blue%20cube%20and%20a%20red%20sphere%22.%20Disentangling%20the%20visual%20and%20linguistic%20roots%20of%20these%20failures%20is%20a%20fundamental%20challenge%20for%20robust%20evaluation.%20To%20enable%20fine-grained%2C%20controllable%20analysis%2C%20we%20introduce%20Auto-Comp%2C%20a%20fully%20automated%20and%20synthetic%20pipeline%20for%20generating%20scalable%20benchmarks.%20Its%20controllable%20nature%20is%20key%20to%20dissecting%20and%20isolating%20different%20reasoning%20skills.%20Auto-Comp%20generates%20paired%20images%20from%20Minimal%20%28e.g.%2C%20%22a%20monitor%20to%20the%20left%20of%20a%20bicycle%20on%20a%20white%20background%22%29%20and%20LLM-generated%20Contextual%20captions%20%28e.g.%2C%20%22In%20a%20brightly%20lit%20photography%20studio%2C%20a%20monitor%20is%20positioned%20to%20the%20left%20of%20a%20bicycle%22%29%2C%20allowing%20a%20controlled%20A/B%20test%20to%20disentangle%20core%20binding%20ability%20from%20visio-linguistic%20complexity.%20Our%20evaluation%20of%2020%20VLMs%20on%20novel%20benchmarks%20for%20color%20binding%20and%20spatial%20relations%20reveals%20universal%20compositional%20failures%20in%20both%20CLIP%20and%20SigLIP%20model%20families.%20Crucially%2C%20our%20novel%20%22Confusion%20Benchmark%22%20reveals%20a%20deeper%20flaw%20beyond%20simple%20attribute%20swaps%3A%20models%20are%20highly%20susceptible%20to%20low-entropy%20distractors%20%28e.g.%2C%20repeated%20objects%20or%20colors%29%2C%20demonstrating%20their%20compositional%20failures%20extend%20beyond%20known%20bag-of-words%20limitations.%20we%20uncover%20a%20surprising%20trade-off%3A%20visio-linguistic%20context%2C%20which%20provides%20global%20scene%20cues%2C%20aids%20spatial%20reasoning%20but%20simultaneously%20hinders%20local%20attribute%20binding%20by%20introducing%20visual%20clutter.%20We%20release%20the%20Auto-Comp%20pipeline%20to%20facilitate%20future%20benchmark%20creation%2C%20alongside%20all%20our%20generated%20benchmarks%20%28https%3A//huggingface.co/AutoComp%29.&entry.1838667208=http%3A//arxiv.org/abs/2602.02043v1&entry.124074799=Read"},
{"title": "Think Dense, Not Long: Dynamic Decoupled Conditional Advantage for Efficient Reasoning", "author": "Keqin Peng and Yuanxin Ouyang and Xuebo Liu and Zhiliang Tian and Ruijian Han and Yancheng Yuan and Liang Ding", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) can elicit strong multi-step reasoning, yet it often encourages overly verbose traces. Moreover, naive length penalties in group-relative optimization can severely hurt accuracy. We attribute this failure to two structural issues: (i) Dilution of Length Baseline, where incorrect responses (with zero length reward) depress the group baseline and over-penalize correct solutions; and (ii) Difficulty-Penalty Mismatch, where a static penalty cannot adapt to problem difficulty, suppressing necessary reasoning on hard instances while leaving redundancy on easy ones. We propose Dynamic Decoupled Conditional Advantage (DDCA) to decouple efficiency optimization from correctness. DDCA computes length advantages conditionally within the correct-response cluster to eliminate baseline dilution, and dynamically scales the penalty strength using the group pass rate as a proxy for difficulty. Experiments on GSM8K, MATH500, AMC23, and AIME25 show that DDCA consistently improves the efficiency--accuracy trade-off relative to adaptive baselines, reducing generated tokens by approximately 60% on simpler tasks (e.g., GSM8K) versus over 20% on harder benchmarks (e.g., AIME25), thereby maintaining or improving accuracy. Code is available at https://github.com/alphadl/DDCA.", "link": "http://arxiv.org/abs/2602.02099v1", "date": "2026-02-02", "relevancy": 2.5103, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5091}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5032}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Think%20Dense%2C%20Not%20Long%3A%20Dynamic%20Decoupled%20Conditional%20Advantage%20for%20Efficient%20Reasoning&body=Title%3A%20Think%20Dense%2C%20Not%20Long%3A%20Dynamic%20Decoupled%20Conditional%20Advantage%20for%20Efficient%20Reasoning%0AAuthor%3A%20Keqin%20Peng%20and%20Yuanxin%20Ouyang%20and%20Xuebo%20Liu%20and%20Zhiliang%20Tian%20and%20Ruijian%20Han%20and%20Yancheng%20Yuan%20and%20Liang%20Ding%0AAbstract%3A%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20can%20elicit%20strong%20multi-step%20reasoning%2C%20yet%20it%20often%20encourages%20overly%20verbose%20traces.%20Moreover%2C%20naive%20length%20penalties%20in%20group-relative%20optimization%20can%20severely%20hurt%20accuracy.%20We%20attribute%20this%20failure%20to%20two%20structural%20issues%3A%20%28i%29%20Dilution%20of%20Length%20Baseline%2C%20where%20incorrect%20responses%20%28with%20zero%20length%20reward%29%20depress%20the%20group%20baseline%20and%20over-penalize%20correct%20solutions%3B%20and%20%28ii%29%20Difficulty-Penalty%20Mismatch%2C%20where%20a%20static%20penalty%20cannot%20adapt%20to%20problem%20difficulty%2C%20suppressing%20necessary%20reasoning%20on%20hard%20instances%20while%20leaving%20redundancy%20on%20easy%20ones.%20We%20propose%20Dynamic%20Decoupled%20Conditional%20Advantage%20%28DDCA%29%20to%20decouple%20efficiency%20optimization%20from%20correctness.%20DDCA%20computes%20length%20advantages%20conditionally%20within%20the%20correct-response%20cluster%20to%20eliminate%20baseline%20dilution%2C%20and%20dynamically%20scales%20the%20penalty%20strength%20using%20the%20group%20pass%20rate%20as%20a%20proxy%20for%20difficulty.%20Experiments%20on%20GSM8K%2C%20MATH500%2C%20AMC23%2C%20and%20AIME25%20show%20that%20DDCA%20consistently%20improves%20the%20efficiency--accuracy%20trade-off%20relative%20to%20adaptive%20baselines%2C%20reducing%20generated%20tokens%20by%20approximately%2060%25%20on%20simpler%20tasks%20%28e.g.%2C%20GSM8K%29%20versus%20over%2020%25%20on%20harder%20benchmarks%20%28e.g.%2C%20AIME25%29%2C%20thereby%20maintaining%20or%20improving%20accuracy.%20Code%20is%20available%20at%20https%3A//github.com/alphadl/DDCA.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02099v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThink%2520Dense%252C%2520Not%2520Long%253A%2520Dynamic%2520Decoupled%2520Conditional%2520Advantage%2520for%2520Efficient%2520Reasoning%26entry.906535625%3DKeqin%2520Peng%2520and%2520Yuanxin%2520Ouyang%2520and%2520Xuebo%2520Liu%2520and%2520Zhiliang%2520Tian%2520and%2520Ruijian%2520Han%2520and%2520Yancheng%2520Yuan%2520and%2520Liang%2520Ding%26entry.1292438233%3DReinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520can%2520elicit%2520strong%2520multi-step%2520reasoning%252C%2520yet%2520it%2520often%2520encourages%2520overly%2520verbose%2520traces.%2520Moreover%252C%2520naive%2520length%2520penalties%2520in%2520group-relative%2520optimization%2520can%2520severely%2520hurt%2520accuracy.%2520We%2520attribute%2520this%2520failure%2520to%2520two%2520structural%2520issues%253A%2520%2528i%2529%2520Dilution%2520of%2520Length%2520Baseline%252C%2520where%2520incorrect%2520responses%2520%2528with%2520zero%2520length%2520reward%2529%2520depress%2520the%2520group%2520baseline%2520and%2520over-penalize%2520correct%2520solutions%253B%2520and%2520%2528ii%2529%2520Difficulty-Penalty%2520Mismatch%252C%2520where%2520a%2520static%2520penalty%2520cannot%2520adapt%2520to%2520problem%2520difficulty%252C%2520suppressing%2520necessary%2520reasoning%2520on%2520hard%2520instances%2520while%2520leaving%2520redundancy%2520on%2520easy%2520ones.%2520We%2520propose%2520Dynamic%2520Decoupled%2520Conditional%2520Advantage%2520%2528DDCA%2529%2520to%2520decouple%2520efficiency%2520optimization%2520from%2520correctness.%2520DDCA%2520computes%2520length%2520advantages%2520conditionally%2520within%2520the%2520correct-response%2520cluster%2520to%2520eliminate%2520baseline%2520dilution%252C%2520and%2520dynamically%2520scales%2520the%2520penalty%2520strength%2520using%2520the%2520group%2520pass%2520rate%2520as%2520a%2520proxy%2520for%2520difficulty.%2520Experiments%2520on%2520GSM8K%252C%2520MATH500%252C%2520AMC23%252C%2520and%2520AIME25%2520show%2520that%2520DDCA%2520consistently%2520improves%2520the%2520efficiency--accuracy%2520trade-off%2520relative%2520to%2520adaptive%2520baselines%252C%2520reducing%2520generated%2520tokens%2520by%2520approximately%252060%2525%2520on%2520simpler%2520tasks%2520%2528e.g.%252C%2520GSM8K%2529%2520versus%2520over%252020%2525%2520on%2520harder%2520benchmarks%2520%2528e.g.%252C%2520AIME25%2529%252C%2520thereby%2520maintaining%2520or%2520improving%2520accuracy.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/alphadl/DDCA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02099v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think%20Dense%2C%20Not%20Long%3A%20Dynamic%20Decoupled%20Conditional%20Advantage%20for%20Efficient%20Reasoning&entry.906535625=Keqin%20Peng%20and%20Yuanxin%20Ouyang%20and%20Xuebo%20Liu%20and%20Zhiliang%20Tian%20and%20Ruijian%20Han%20and%20Yancheng%20Yuan%20and%20Liang%20Ding&entry.1292438233=Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20can%20elicit%20strong%20multi-step%20reasoning%2C%20yet%20it%20often%20encourages%20overly%20verbose%20traces.%20Moreover%2C%20naive%20length%20penalties%20in%20group-relative%20optimization%20can%20severely%20hurt%20accuracy.%20We%20attribute%20this%20failure%20to%20two%20structural%20issues%3A%20%28i%29%20Dilution%20of%20Length%20Baseline%2C%20where%20incorrect%20responses%20%28with%20zero%20length%20reward%29%20depress%20the%20group%20baseline%20and%20over-penalize%20correct%20solutions%3B%20and%20%28ii%29%20Difficulty-Penalty%20Mismatch%2C%20where%20a%20static%20penalty%20cannot%20adapt%20to%20problem%20difficulty%2C%20suppressing%20necessary%20reasoning%20on%20hard%20instances%20while%20leaving%20redundancy%20on%20easy%20ones.%20We%20propose%20Dynamic%20Decoupled%20Conditional%20Advantage%20%28DDCA%29%20to%20decouple%20efficiency%20optimization%20from%20correctness.%20DDCA%20computes%20length%20advantages%20conditionally%20within%20the%20correct-response%20cluster%20to%20eliminate%20baseline%20dilution%2C%20and%20dynamically%20scales%20the%20penalty%20strength%20using%20the%20group%20pass%20rate%20as%20a%20proxy%20for%20difficulty.%20Experiments%20on%20GSM8K%2C%20MATH500%2C%20AMC23%2C%20and%20AIME25%20show%20that%20DDCA%20consistently%20improves%20the%20efficiency--accuracy%20trade-off%20relative%20to%20adaptive%20baselines%2C%20reducing%20generated%20tokens%20by%20approximately%2060%25%20on%20simpler%20tasks%20%28e.g.%2C%20GSM8K%29%20versus%20over%2020%25%20on%20harder%20benchmarks%20%28e.g.%2C%20AIME25%29%2C%20thereby%20maintaining%20or%20improving%20accuracy.%20Code%20is%20available%20at%20https%3A//github.com/alphadl/DDCA.&entry.1838667208=http%3A//arxiv.org/abs/2602.02099v1&entry.124074799=Read"},
{"title": "Decoding Generalization from Memorization in Deep Neural Networks", "author": "Simran Ketha and Venkatakrishnan Ramaswamy", "abstract": "Overparameterized deep networks that generalize well have been key to the dramatic success of deep learning in recent years. The reasons for their remarkable ability to generalize are not well understood yet. When class labels in the training set are shuffled to varying degrees, it is known that deep networks can still reach perfect training accuracy at the detriment of generalization to true labels -- a phenomenon that has been called memorization. It has, however, been unclear why the poor generalization to true labels that accompanies such memorization, comes about. One possibility is that during training, all layers of the network irretrievably re-organize their representations in a manner that makes generalization to true labels difficult. The other possibility is that one or more layers of the trained network retain significantly more latent ability to generalize to true labels, but the network somehow \"chooses\" to readout in a manner that is detrimental to generalization to true labels. Here, we provide evidence for the latter possibility by demonstrating, empirically, that such models possess information in their representations for substantially-improved generalization to true labels. Furthermore, such abilities can be easily decoded from the internals of the trained model, and we build a technique to do so. We demonstrate results on multiple models trained with standard datasets. Our code is available at: https://github.com/simranketha/MASC_DNN.", "link": "http://arxiv.org/abs/2501.14687v2", "date": "2026-02-02", "relevancy": 2.5048, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5361}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4873}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20Generalization%20from%20Memorization%20in%20Deep%20Neural%20Networks&body=Title%3A%20Decoding%20Generalization%20from%20Memorization%20in%20Deep%20Neural%20Networks%0AAuthor%3A%20Simran%20Ketha%20and%20Venkatakrishnan%20Ramaswamy%0AAbstract%3A%20Overparameterized%20deep%20networks%20that%20generalize%20well%20have%20been%20key%20to%20the%20dramatic%20success%20of%20deep%20learning%20in%20recent%20years.%20The%20reasons%20for%20their%20remarkable%20ability%20to%20generalize%20are%20not%20well%20understood%20yet.%20When%20class%20labels%20in%20the%20training%20set%20are%20shuffled%20to%20varying%20degrees%2C%20it%20is%20known%20that%20deep%20networks%20can%20still%20reach%20perfect%20training%20accuracy%20at%20the%20detriment%20of%20generalization%20to%20true%20labels%20--%20a%20phenomenon%20that%20has%20been%20called%20memorization.%20It%20has%2C%20however%2C%20been%20unclear%20why%20the%20poor%20generalization%20to%20true%20labels%20that%20accompanies%20such%20memorization%2C%20comes%20about.%20One%20possibility%20is%20that%20during%20training%2C%20all%20layers%20of%20the%20network%20irretrievably%20re-organize%20their%20representations%20in%20a%20manner%20that%20makes%20generalization%20to%20true%20labels%20difficult.%20The%20other%20possibility%20is%20that%20one%20or%20more%20layers%20of%20the%20trained%20network%20retain%20significantly%20more%20latent%20ability%20to%20generalize%20to%20true%20labels%2C%20but%20the%20network%20somehow%20%22chooses%22%20to%20readout%20in%20a%20manner%20that%20is%20detrimental%20to%20generalization%20to%20true%20labels.%20Here%2C%20we%20provide%20evidence%20for%20the%20latter%20possibility%20by%20demonstrating%2C%20empirically%2C%20that%20such%20models%20possess%20information%20in%20their%20representations%20for%20substantially-improved%20generalization%20to%20true%20labels.%20Furthermore%2C%20such%20abilities%20can%20be%20easily%20decoded%20from%20the%20internals%20of%20the%20trained%20model%2C%20and%20we%20build%20a%20technique%20to%20do%20so.%20We%20demonstrate%20results%20on%20multiple%20models%20trained%20with%20standard%20datasets.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/simranketha/MASC_DNN.%0ALink%3A%20http%3A//arxiv.org/abs/2501.14687v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520Generalization%2520from%2520Memorization%2520in%2520Deep%2520Neural%2520Networks%26entry.906535625%3DSimran%2520Ketha%2520and%2520Venkatakrishnan%2520Ramaswamy%26entry.1292438233%3DOverparameterized%2520deep%2520networks%2520that%2520generalize%2520well%2520have%2520been%2520key%2520to%2520the%2520dramatic%2520success%2520of%2520deep%2520learning%2520in%2520recent%2520years.%2520The%2520reasons%2520for%2520their%2520remarkable%2520ability%2520to%2520generalize%2520are%2520not%2520well%2520understood%2520yet.%2520When%2520class%2520labels%2520in%2520the%2520training%2520set%2520are%2520shuffled%2520to%2520varying%2520degrees%252C%2520it%2520is%2520known%2520that%2520deep%2520networks%2520can%2520still%2520reach%2520perfect%2520training%2520accuracy%2520at%2520the%2520detriment%2520of%2520generalization%2520to%2520true%2520labels%2520--%2520a%2520phenomenon%2520that%2520has%2520been%2520called%2520memorization.%2520It%2520has%252C%2520however%252C%2520been%2520unclear%2520why%2520the%2520poor%2520generalization%2520to%2520true%2520labels%2520that%2520accompanies%2520such%2520memorization%252C%2520comes%2520about.%2520One%2520possibility%2520is%2520that%2520during%2520training%252C%2520all%2520layers%2520of%2520the%2520network%2520irretrievably%2520re-organize%2520their%2520representations%2520in%2520a%2520manner%2520that%2520makes%2520generalization%2520to%2520true%2520labels%2520difficult.%2520The%2520other%2520possibility%2520is%2520that%2520one%2520or%2520more%2520layers%2520of%2520the%2520trained%2520network%2520retain%2520significantly%2520more%2520latent%2520ability%2520to%2520generalize%2520to%2520true%2520labels%252C%2520but%2520the%2520network%2520somehow%2520%2522chooses%2522%2520to%2520readout%2520in%2520a%2520manner%2520that%2520is%2520detrimental%2520to%2520generalization%2520to%2520true%2520labels.%2520Here%252C%2520we%2520provide%2520evidence%2520for%2520the%2520latter%2520possibility%2520by%2520demonstrating%252C%2520empirically%252C%2520that%2520such%2520models%2520possess%2520information%2520in%2520their%2520representations%2520for%2520substantially-improved%2520generalization%2520to%2520true%2520labels.%2520Furthermore%252C%2520such%2520abilities%2520can%2520be%2520easily%2520decoded%2520from%2520the%2520internals%2520of%2520the%2520trained%2520model%252C%2520and%2520we%2520build%2520a%2520technique%2520to%2520do%2520so.%2520We%2520demonstrate%2520results%2520on%2520multiple%2520models%2520trained%2520with%2520standard%2520datasets.%2520Our%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/simranketha/MASC_DNN.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14687v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20Generalization%20from%20Memorization%20in%20Deep%20Neural%20Networks&entry.906535625=Simran%20Ketha%20and%20Venkatakrishnan%20Ramaswamy&entry.1292438233=Overparameterized%20deep%20networks%20that%20generalize%20well%20have%20been%20key%20to%20the%20dramatic%20success%20of%20deep%20learning%20in%20recent%20years.%20The%20reasons%20for%20their%20remarkable%20ability%20to%20generalize%20are%20not%20well%20understood%20yet.%20When%20class%20labels%20in%20the%20training%20set%20are%20shuffled%20to%20varying%20degrees%2C%20it%20is%20known%20that%20deep%20networks%20can%20still%20reach%20perfect%20training%20accuracy%20at%20the%20detriment%20of%20generalization%20to%20true%20labels%20--%20a%20phenomenon%20that%20has%20been%20called%20memorization.%20It%20has%2C%20however%2C%20been%20unclear%20why%20the%20poor%20generalization%20to%20true%20labels%20that%20accompanies%20such%20memorization%2C%20comes%20about.%20One%20possibility%20is%20that%20during%20training%2C%20all%20layers%20of%20the%20network%20irretrievably%20re-organize%20their%20representations%20in%20a%20manner%20that%20makes%20generalization%20to%20true%20labels%20difficult.%20The%20other%20possibility%20is%20that%20one%20or%20more%20layers%20of%20the%20trained%20network%20retain%20significantly%20more%20latent%20ability%20to%20generalize%20to%20true%20labels%2C%20but%20the%20network%20somehow%20%22chooses%22%20to%20readout%20in%20a%20manner%20that%20is%20detrimental%20to%20generalization%20to%20true%20labels.%20Here%2C%20we%20provide%20evidence%20for%20the%20latter%20possibility%20by%20demonstrating%2C%20empirically%2C%20that%20such%20models%20possess%20information%20in%20their%20representations%20for%20substantially-improved%20generalization%20to%20true%20labels.%20Furthermore%2C%20such%20abilities%20can%20be%20easily%20decoded%20from%20the%20internals%20of%20the%20trained%20model%2C%20and%20we%20build%20a%20technique%20to%20do%20so.%20We%20demonstrate%20results%20on%20multiple%20models%20trained%20with%20standard%20datasets.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/simranketha/MASC_DNN.&entry.1838667208=http%3A//arxiv.org/abs/2501.14687v2&entry.124074799=Read"},
{"title": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention", "author": "J Rosser and Jos\u00e9 Luis Redondo Garc\u00eda and Gustavo Penha and Konstantina Palla and Hugues Bouchard", "abstract": "As Large Language Models (LLMs) scale to million-token contexts, traditional Mechanistic Interpretability techniques for analyzing attention scale quadratically with context length, demanding terabytes of memory beyond 100,000 tokens. We introduce Sparse Tracing, a novel technique that leverages dynamic sparse attention to efficiently analyze long context attention patterns. We present Stream, a compilable hierarchical pruning algorithm that estimates per-head sparse attention masks in near-linear time $O(T \\log T)$ and linear space $O(T)$, enabling one-pass interpretability at scale. Stream performs a binary-search-style refinement to retain only the top-$k$ key blocks per query while preserving the model's next-token behavior. We apply Stream to long chain-of-thought reasoning traces and identify thought anchors while pruning 97-99\\% of token interactions. On the RULER benchmark, Stream preserves critical retrieval paths while discarding 90-96\\% of interactions and exposes layer-wise routes from the needle to output. Our method offers a practical drop-in tool for analyzing attention patterns and tracing information flow without terabytes of caches. By making long context interpretability feasible on consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring. Code is available at https://anonymous.4open.science/r/stream-03B8/.", "link": "http://arxiv.org/abs/2510.19875v2", "date": "2026-02-02", "relevancy": 2.5015, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5071}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stream%3A%20Scaling%20up%20Mechanistic%20Interpretability%20to%20Long%20Context%20in%20LLMs%20via%20Sparse%20Attention&body=Title%3A%20Stream%3A%20Scaling%20up%20Mechanistic%20Interpretability%20to%20Long%20Context%20in%20LLMs%20via%20Sparse%20Attention%0AAuthor%3A%20J%20Rosser%20and%20Jos%C3%A9%20Luis%20Redondo%20Garc%C3%ADa%20and%20Gustavo%20Penha%20and%20Konstantina%20Palla%20and%20Hugues%20Bouchard%0AAbstract%3A%20As%20Large%20Language%20Models%20%28LLMs%29%20scale%20to%20million-token%20contexts%2C%20traditional%20Mechanistic%20Interpretability%20techniques%20for%20analyzing%20attention%20scale%20quadratically%20with%20context%20length%2C%20demanding%20terabytes%20of%20memory%20beyond%20100%2C000%20tokens.%20We%20introduce%20Sparse%20Tracing%2C%20a%20novel%20technique%20that%20leverages%20dynamic%20sparse%20attention%20to%20efficiently%20analyze%20long%20context%20attention%20patterns.%20We%20present%20Stream%2C%20a%20compilable%20hierarchical%20pruning%20algorithm%20that%20estimates%20per-head%20sparse%20attention%20masks%20in%20near-linear%20time%20%24O%28T%20%5Clog%20T%29%24%20and%20linear%20space%20%24O%28T%29%24%2C%20enabling%20one-pass%20interpretability%20at%20scale.%20Stream%20performs%20a%20binary-search-style%20refinement%20to%20retain%20only%20the%20top-%24k%24%20key%20blocks%20per%20query%20while%20preserving%20the%20model%27s%20next-token%20behavior.%20We%20apply%20Stream%20to%20long%20chain-of-thought%20reasoning%20traces%20and%20identify%20thought%20anchors%20while%20pruning%2097-99%5C%25%20of%20token%20interactions.%20On%20the%20RULER%20benchmark%2C%20Stream%20preserves%20critical%20retrieval%20paths%20while%20discarding%2090-96%5C%25%20of%20interactions%20and%20exposes%20layer-wise%20routes%20from%20the%20needle%20to%20output.%20Our%20method%20offers%20a%20practical%20drop-in%20tool%20for%20analyzing%20attention%20patterns%20and%20tracing%20information%20flow%20without%20terabytes%20of%20caches.%20By%20making%20long%20context%20interpretability%20feasible%20on%20consumer%20GPUs%2C%20Sparse%20Tracing%20helps%20democratize%20chain-of-thought%20monitoring.%20Code%20is%20available%20at%20https%3A//anonymous.4open.science/r/stream-03B8/.%0ALink%3A%20http%3A//arxiv.org/abs/2510.19875v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStream%253A%2520Scaling%2520up%2520Mechanistic%2520Interpretability%2520to%2520Long%2520Context%2520in%2520LLMs%2520via%2520Sparse%2520Attention%26entry.906535625%3DJ%2520Rosser%2520and%2520Jos%25C3%25A9%2520Luis%2520Redondo%2520Garc%25C3%25ADa%2520and%2520Gustavo%2520Penha%2520and%2520Konstantina%2520Palla%2520and%2520Hugues%2520Bouchard%26entry.1292438233%3DAs%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520scale%2520to%2520million-token%2520contexts%252C%2520traditional%2520Mechanistic%2520Interpretability%2520techniques%2520for%2520analyzing%2520attention%2520scale%2520quadratically%2520with%2520context%2520length%252C%2520demanding%2520terabytes%2520of%2520memory%2520beyond%2520100%252C000%2520tokens.%2520We%2520introduce%2520Sparse%2520Tracing%252C%2520a%2520novel%2520technique%2520that%2520leverages%2520dynamic%2520sparse%2520attention%2520to%2520efficiently%2520analyze%2520long%2520context%2520attention%2520patterns.%2520We%2520present%2520Stream%252C%2520a%2520compilable%2520hierarchical%2520pruning%2520algorithm%2520that%2520estimates%2520per-head%2520sparse%2520attention%2520masks%2520in%2520near-linear%2520time%2520%2524O%2528T%2520%255Clog%2520T%2529%2524%2520and%2520linear%2520space%2520%2524O%2528T%2529%2524%252C%2520enabling%2520one-pass%2520interpretability%2520at%2520scale.%2520Stream%2520performs%2520a%2520binary-search-style%2520refinement%2520to%2520retain%2520only%2520the%2520top-%2524k%2524%2520key%2520blocks%2520per%2520query%2520while%2520preserving%2520the%2520model%2527s%2520next-token%2520behavior.%2520We%2520apply%2520Stream%2520to%2520long%2520chain-of-thought%2520reasoning%2520traces%2520and%2520identify%2520thought%2520anchors%2520while%2520pruning%252097-99%255C%2525%2520of%2520token%2520interactions.%2520On%2520the%2520RULER%2520benchmark%252C%2520Stream%2520preserves%2520critical%2520retrieval%2520paths%2520while%2520discarding%252090-96%255C%2525%2520of%2520interactions%2520and%2520exposes%2520layer-wise%2520routes%2520from%2520the%2520needle%2520to%2520output.%2520Our%2520method%2520offers%2520a%2520practical%2520drop-in%2520tool%2520for%2520analyzing%2520attention%2520patterns%2520and%2520tracing%2520information%2520flow%2520without%2520terabytes%2520of%2520caches.%2520By%2520making%2520long%2520context%2520interpretability%2520feasible%2520on%2520consumer%2520GPUs%252C%2520Sparse%2520Tracing%2520helps%2520democratize%2520chain-of-thought%2520monitoring.%2520Code%2520is%2520available%2520at%2520https%253A//anonymous.4open.science/r/stream-03B8/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19875v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stream%3A%20Scaling%20up%20Mechanistic%20Interpretability%20to%20Long%20Context%20in%20LLMs%20via%20Sparse%20Attention&entry.906535625=J%20Rosser%20and%20Jos%C3%A9%20Luis%20Redondo%20Garc%C3%ADa%20and%20Gustavo%20Penha%20and%20Konstantina%20Palla%20and%20Hugues%20Bouchard&entry.1292438233=As%20Large%20Language%20Models%20%28LLMs%29%20scale%20to%20million-token%20contexts%2C%20traditional%20Mechanistic%20Interpretability%20techniques%20for%20analyzing%20attention%20scale%20quadratically%20with%20context%20length%2C%20demanding%20terabytes%20of%20memory%20beyond%20100%2C000%20tokens.%20We%20introduce%20Sparse%20Tracing%2C%20a%20novel%20technique%20that%20leverages%20dynamic%20sparse%20attention%20to%20efficiently%20analyze%20long%20context%20attention%20patterns.%20We%20present%20Stream%2C%20a%20compilable%20hierarchical%20pruning%20algorithm%20that%20estimates%20per-head%20sparse%20attention%20masks%20in%20near-linear%20time%20%24O%28T%20%5Clog%20T%29%24%20and%20linear%20space%20%24O%28T%29%24%2C%20enabling%20one-pass%20interpretability%20at%20scale.%20Stream%20performs%20a%20binary-search-style%20refinement%20to%20retain%20only%20the%20top-%24k%24%20key%20blocks%20per%20query%20while%20preserving%20the%20model%27s%20next-token%20behavior.%20We%20apply%20Stream%20to%20long%20chain-of-thought%20reasoning%20traces%20and%20identify%20thought%20anchors%20while%20pruning%2097-99%5C%25%20of%20token%20interactions.%20On%20the%20RULER%20benchmark%2C%20Stream%20preserves%20critical%20retrieval%20paths%20while%20discarding%2090-96%5C%25%20of%20interactions%20and%20exposes%20layer-wise%20routes%20from%20the%20needle%20to%20output.%20Our%20method%20offers%20a%20practical%20drop-in%20tool%20for%20analyzing%20attention%20patterns%20and%20tracing%20information%20flow%20without%20terabytes%20of%20caches.%20By%20making%20long%20context%20interpretability%20feasible%20on%20consumer%20GPUs%2C%20Sparse%20Tracing%20helps%20democratize%20chain-of-thought%20monitoring.%20Code%20is%20available%20at%20https%3A//anonymous.4open.science/r/stream-03B8/.&entry.1838667208=http%3A//arxiv.org/abs/2510.19875v2&entry.124074799=Read"},
{"title": "Embedding Learning on Multiplex Networks for Link Prediction", "author": "Orell Trautmann and Olaf Wolkenhauer and Cl\u00e9mence R\u00e9da", "abstract": "Over the past years, embedding learning on networks has shown tremendous results in link prediction tasks for complex systems, with a wide range of real-life applications. Learning a representation for each node in a knowledge graph allows us to capture topological and semantic information, which can be processed in downstream analyses later. In the link prediction task, high-dimensional network information is encoded into low-dimensional vectors, which are then fed to a predictor to infer new connections between nodes in the network. As the network complexity (that is, the numbers of connections and types of interactions) grows, embedding learning turns out increasingly challenging. This review covers published models on embedding learning on multiplex networks for link prediction. First, we propose refined taxonomies to classify and compare models, depending on the type of embeddings and embedding techniques. Second, we review and address the problem of reproducible and fair evaluation of embedding learning on multiplex networks for the link prediction task. Finally, we tackle evaluation on directed multiplex networks by proposing a novel and fair testing procedure. This review constitutes a crucial step towards the development of more performant and tractable embedding learning approaches for multiplex networks and their fair evaluation for the link prediction task. We also suggest guidelines on the evaluation of models, and provide an informed perspective on the challenges and tools currently available to address downstream analyses applied to multiplex networks.", "link": "http://arxiv.org/abs/2602.01922v1", "date": "2026-02-02", "relevancy": 2.5013, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5102}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4953}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embedding%20Learning%20on%20Multiplex%20Networks%20for%20Link%20Prediction&body=Title%3A%20Embedding%20Learning%20on%20Multiplex%20Networks%20for%20Link%20Prediction%0AAuthor%3A%20Orell%20Trautmann%20and%20Olaf%20Wolkenhauer%20and%20Cl%C3%A9mence%20R%C3%A9da%0AAbstract%3A%20Over%20the%20past%20years%2C%20embedding%20learning%20on%20networks%20has%20shown%20tremendous%20results%20in%20link%20prediction%20tasks%20for%20complex%20systems%2C%20with%20a%20wide%20range%20of%20real-life%20applications.%20Learning%20a%20representation%20for%20each%20node%20in%20a%20knowledge%20graph%20allows%20us%20to%20capture%20topological%20and%20semantic%20information%2C%20which%20can%20be%20processed%20in%20downstream%20analyses%20later.%20In%20the%20link%20prediction%20task%2C%20high-dimensional%20network%20information%20is%20encoded%20into%20low-dimensional%20vectors%2C%20which%20are%20then%20fed%20to%20a%20predictor%20to%20infer%20new%20connections%20between%20nodes%20in%20the%20network.%20As%20the%20network%20complexity%20%28that%20is%2C%20the%20numbers%20of%20connections%20and%20types%20of%20interactions%29%20grows%2C%20embedding%20learning%20turns%20out%20increasingly%20challenging.%20This%20review%20covers%20published%20models%20on%20embedding%20learning%20on%20multiplex%20networks%20for%20link%20prediction.%20First%2C%20we%20propose%20refined%20taxonomies%20to%20classify%20and%20compare%20models%2C%20depending%20on%20the%20type%20of%20embeddings%20and%20embedding%20techniques.%20Second%2C%20we%20review%20and%20address%20the%20problem%20of%20reproducible%20and%20fair%20evaluation%20of%20embedding%20learning%20on%20multiplex%20networks%20for%20the%20link%20prediction%20task.%20Finally%2C%20we%20tackle%20evaluation%20on%20directed%20multiplex%20networks%20by%20proposing%20a%20novel%20and%20fair%20testing%20procedure.%20This%20review%20constitutes%20a%20crucial%20step%20towards%20the%20development%20of%20more%20performant%20and%20tractable%20embedding%20learning%20approaches%20for%20multiplex%20networks%20and%20their%20fair%20evaluation%20for%20the%20link%20prediction%20task.%20We%20also%20suggest%20guidelines%20on%20the%20evaluation%20of%20models%2C%20and%20provide%20an%20informed%20perspective%20on%20the%20challenges%20and%20tools%20currently%20available%20to%20address%20downstream%20analyses%20applied%20to%20multiplex%20networks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.01922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbedding%2520Learning%2520on%2520Multiplex%2520Networks%2520for%2520Link%2520Prediction%26entry.906535625%3DOrell%2520Trautmann%2520and%2520Olaf%2520Wolkenhauer%2520and%2520Cl%25C3%25A9mence%2520R%25C3%25A9da%26entry.1292438233%3DOver%2520the%2520past%2520years%252C%2520embedding%2520learning%2520on%2520networks%2520has%2520shown%2520tremendous%2520results%2520in%2520link%2520prediction%2520tasks%2520for%2520complex%2520systems%252C%2520with%2520a%2520wide%2520range%2520of%2520real-life%2520applications.%2520Learning%2520a%2520representation%2520for%2520each%2520node%2520in%2520a%2520knowledge%2520graph%2520allows%2520us%2520to%2520capture%2520topological%2520and%2520semantic%2520information%252C%2520which%2520can%2520be%2520processed%2520in%2520downstream%2520analyses%2520later.%2520In%2520the%2520link%2520prediction%2520task%252C%2520high-dimensional%2520network%2520information%2520is%2520encoded%2520into%2520low-dimensional%2520vectors%252C%2520which%2520are%2520then%2520fed%2520to%2520a%2520predictor%2520to%2520infer%2520new%2520connections%2520between%2520nodes%2520in%2520the%2520network.%2520As%2520the%2520network%2520complexity%2520%2528that%2520is%252C%2520the%2520numbers%2520of%2520connections%2520and%2520types%2520of%2520interactions%2529%2520grows%252C%2520embedding%2520learning%2520turns%2520out%2520increasingly%2520challenging.%2520This%2520review%2520covers%2520published%2520models%2520on%2520embedding%2520learning%2520on%2520multiplex%2520networks%2520for%2520link%2520prediction.%2520First%252C%2520we%2520propose%2520refined%2520taxonomies%2520to%2520classify%2520and%2520compare%2520models%252C%2520depending%2520on%2520the%2520type%2520of%2520embeddings%2520and%2520embedding%2520techniques.%2520Second%252C%2520we%2520review%2520and%2520address%2520the%2520problem%2520of%2520reproducible%2520and%2520fair%2520evaluation%2520of%2520embedding%2520learning%2520on%2520multiplex%2520networks%2520for%2520the%2520link%2520prediction%2520task.%2520Finally%252C%2520we%2520tackle%2520evaluation%2520on%2520directed%2520multiplex%2520networks%2520by%2520proposing%2520a%2520novel%2520and%2520fair%2520testing%2520procedure.%2520This%2520review%2520constitutes%2520a%2520crucial%2520step%2520towards%2520the%2520development%2520of%2520more%2520performant%2520and%2520tractable%2520embedding%2520learning%2520approaches%2520for%2520multiplex%2520networks%2520and%2520their%2520fair%2520evaluation%2520for%2520the%2520link%2520prediction%2520task.%2520We%2520also%2520suggest%2520guidelines%2520on%2520the%2520evaluation%2520of%2520models%252C%2520and%2520provide%2520an%2520informed%2520perspective%2520on%2520the%2520challenges%2520and%2520tools%2520currently%2520available%2520to%2520address%2520downstream%2520analyses%2520applied%2520to%2520multiplex%2520networks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.01922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedding%20Learning%20on%20Multiplex%20Networks%20for%20Link%20Prediction&entry.906535625=Orell%20Trautmann%20and%20Olaf%20Wolkenhauer%20and%20Cl%C3%A9mence%20R%C3%A9da&entry.1292438233=Over%20the%20past%20years%2C%20embedding%20learning%20on%20networks%20has%20shown%20tremendous%20results%20in%20link%20prediction%20tasks%20for%20complex%20systems%2C%20with%20a%20wide%20range%20of%20real-life%20applications.%20Learning%20a%20representation%20for%20each%20node%20in%20a%20knowledge%20graph%20allows%20us%20to%20capture%20topological%20and%20semantic%20information%2C%20which%20can%20be%20processed%20in%20downstream%20analyses%20later.%20In%20the%20link%20prediction%20task%2C%20high-dimensional%20network%20information%20is%20encoded%20into%20low-dimensional%20vectors%2C%20which%20are%20then%20fed%20to%20a%20predictor%20to%20infer%20new%20connections%20between%20nodes%20in%20the%20network.%20As%20the%20network%20complexity%20%28that%20is%2C%20the%20numbers%20of%20connections%20and%20types%20of%20interactions%29%20grows%2C%20embedding%20learning%20turns%20out%20increasingly%20challenging.%20This%20review%20covers%20published%20models%20on%20embedding%20learning%20on%20multiplex%20networks%20for%20link%20prediction.%20First%2C%20we%20propose%20refined%20taxonomies%20to%20classify%20and%20compare%20models%2C%20depending%20on%20the%20type%20of%20embeddings%20and%20embedding%20techniques.%20Second%2C%20we%20review%20and%20address%20the%20problem%20of%20reproducible%20and%20fair%20evaluation%20of%20embedding%20learning%20on%20multiplex%20networks%20for%20the%20link%20prediction%20task.%20Finally%2C%20we%20tackle%20evaluation%20on%20directed%20multiplex%20networks%20by%20proposing%20a%20novel%20and%20fair%20testing%20procedure.%20This%20review%20constitutes%20a%20crucial%20step%20towards%20the%20development%20of%20more%20performant%20and%20tractable%20embedding%20learning%20approaches%20for%20multiplex%20networks%20and%20their%20fair%20evaluation%20for%20the%20link%20prediction%20task.%20We%20also%20suggest%20guidelines%20on%20the%20evaluation%20of%20models%2C%20and%20provide%20an%20informed%20perspective%20on%20the%20challenges%20and%20tools%20currently%20available%20to%20address%20downstream%20analyses%20applied%20to%20multiplex%20networks.&entry.1838667208=http%3A//arxiv.org/abs/2602.01922v1&entry.124074799=Read"},
{"title": "Alignment-Aware Model Adaptation via Feedback-Guided Optimization", "author": "Gaurav Bhatt and Aditya Chinchure and Jiawei Zhou and Leonid Sigal", "abstract": "Fine-tuning is the primary mechanism for adapting foundation models to downstream tasks; however, standard approaches largely optimize task objectives in isolation and do not account for secondary yet critical alignment objectives (e.g., safety and hallucination avoidance). As a result, downstream fine-tuning can degrade alignment and fail to correct pre-existing misaligned behavior. We propose an alignment-aware fine-tuning framework that integrates feedback from an external alignment signal through policy-gradient-based regularization. Our method introduces an adaptive gating mechanism that dynamically balances supervised and alignment-driven gradients on a per-sample basis, prioritizing uncertain or misaligned cases while allowing well-aligned examples to follow standard supervised updates. The framework further learns abstention behavior for fully misaligned inputs, incorporating conservative responses directly into the fine-tuned model. Experiments on general and domain-specific instruction-tuning benchmarks demonstrate consistent reductions in harmful and hallucinated outputs without sacrificing downstream task performance. Additional analyses show robustness to adversarial fine-tuning, prompt-based attacks, and unsafe initializations, establishing adaptively gated alignment optimization as an effective approach for alignment-preserving and alignment-recovering model adaptation.", "link": "http://arxiv.org/abs/2602.02258v1", "date": "2026-02-02", "relevancy": 2.4936, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5049}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5047}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alignment-Aware%20Model%20Adaptation%20via%20Feedback-Guided%20Optimization&body=Title%3A%20Alignment-Aware%20Model%20Adaptation%20via%20Feedback-Guided%20Optimization%0AAuthor%3A%20Gaurav%20Bhatt%20and%20Aditya%20Chinchure%20and%20Jiawei%20Zhou%20and%20Leonid%20Sigal%0AAbstract%3A%20Fine-tuning%20is%20the%20primary%20mechanism%20for%20adapting%20foundation%20models%20to%20downstream%20tasks%3B%20however%2C%20standard%20approaches%20largely%20optimize%20task%20objectives%20in%20isolation%20and%20do%20not%20account%20for%20secondary%20yet%20critical%20alignment%20objectives%20%28e.g.%2C%20safety%20and%20hallucination%20avoidance%29.%20As%20a%20result%2C%20downstream%20fine-tuning%20can%20degrade%20alignment%20and%20fail%20to%20correct%20pre-existing%20misaligned%20behavior.%20We%20propose%20an%20alignment-aware%20fine-tuning%20framework%20that%20integrates%20feedback%20from%20an%20external%20alignment%20signal%20through%20policy-gradient-based%20regularization.%20Our%20method%20introduces%20an%20adaptive%20gating%20mechanism%20that%20dynamically%20balances%20supervised%20and%20alignment-driven%20gradients%20on%20a%20per-sample%20basis%2C%20prioritizing%20uncertain%20or%20misaligned%20cases%20while%20allowing%20well-aligned%20examples%20to%20follow%20standard%20supervised%20updates.%20The%20framework%20further%20learns%20abstention%20behavior%20for%20fully%20misaligned%20inputs%2C%20incorporating%20conservative%20responses%20directly%20into%20the%20fine-tuned%20model.%20Experiments%20on%20general%20and%20domain-specific%20instruction-tuning%20benchmarks%20demonstrate%20consistent%20reductions%20in%20harmful%20and%20hallucinated%20outputs%20without%20sacrificing%20downstream%20task%20performance.%20Additional%20analyses%20show%20robustness%20to%20adversarial%20fine-tuning%2C%20prompt-based%20attacks%2C%20and%20unsafe%20initializations%2C%20establishing%20adaptively%20gated%20alignment%20optimization%20as%20an%20effective%20approach%20for%20alignment-preserving%20and%20alignment-recovering%20model%20adaptation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignment-Aware%2520Model%2520Adaptation%2520via%2520Feedback-Guided%2520Optimization%26entry.906535625%3DGaurav%2520Bhatt%2520and%2520Aditya%2520Chinchure%2520and%2520Jiawei%2520Zhou%2520and%2520Leonid%2520Sigal%26entry.1292438233%3DFine-tuning%2520is%2520the%2520primary%2520mechanism%2520for%2520adapting%2520foundation%2520models%2520to%2520downstream%2520tasks%253B%2520however%252C%2520standard%2520approaches%2520largely%2520optimize%2520task%2520objectives%2520in%2520isolation%2520and%2520do%2520not%2520account%2520for%2520secondary%2520yet%2520critical%2520alignment%2520objectives%2520%2528e.g.%252C%2520safety%2520and%2520hallucination%2520avoidance%2529.%2520As%2520a%2520result%252C%2520downstream%2520fine-tuning%2520can%2520degrade%2520alignment%2520and%2520fail%2520to%2520correct%2520pre-existing%2520misaligned%2520behavior.%2520We%2520propose%2520an%2520alignment-aware%2520fine-tuning%2520framework%2520that%2520integrates%2520feedback%2520from%2520an%2520external%2520alignment%2520signal%2520through%2520policy-gradient-based%2520regularization.%2520Our%2520method%2520introduces%2520an%2520adaptive%2520gating%2520mechanism%2520that%2520dynamically%2520balances%2520supervised%2520and%2520alignment-driven%2520gradients%2520on%2520a%2520per-sample%2520basis%252C%2520prioritizing%2520uncertain%2520or%2520misaligned%2520cases%2520while%2520allowing%2520well-aligned%2520examples%2520to%2520follow%2520standard%2520supervised%2520updates.%2520The%2520framework%2520further%2520learns%2520abstention%2520behavior%2520for%2520fully%2520misaligned%2520inputs%252C%2520incorporating%2520conservative%2520responses%2520directly%2520into%2520the%2520fine-tuned%2520model.%2520Experiments%2520on%2520general%2520and%2520domain-specific%2520instruction-tuning%2520benchmarks%2520demonstrate%2520consistent%2520reductions%2520in%2520harmful%2520and%2520hallucinated%2520outputs%2520without%2520sacrificing%2520downstream%2520task%2520performance.%2520Additional%2520analyses%2520show%2520robustness%2520to%2520adversarial%2520fine-tuning%252C%2520prompt-based%2520attacks%252C%2520and%2520unsafe%2520initializations%252C%2520establishing%2520adaptively%2520gated%2520alignment%2520optimization%2520as%2520an%2520effective%2520approach%2520for%2520alignment-preserving%2520and%2520alignment-recovering%2520model%2520adaptation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alignment-Aware%20Model%20Adaptation%20via%20Feedback-Guided%20Optimization&entry.906535625=Gaurav%20Bhatt%20and%20Aditya%20Chinchure%20and%20Jiawei%20Zhou%20and%20Leonid%20Sigal&entry.1292438233=Fine-tuning%20is%20the%20primary%20mechanism%20for%20adapting%20foundation%20models%20to%20downstream%20tasks%3B%20however%2C%20standard%20approaches%20largely%20optimize%20task%20objectives%20in%20isolation%20and%20do%20not%20account%20for%20secondary%20yet%20critical%20alignment%20objectives%20%28e.g.%2C%20safety%20and%20hallucination%20avoidance%29.%20As%20a%20result%2C%20downstream%20fine-tuning%20can%20degrade%20alignment%20and%20fail%20to%20correct%20pre-existing%20misaligned%20behavior.%20We%20propose%20an%20alignment-aware%20fine-tuning%20framework%20that%20integrates%20feedback%20from%20an%20external%20alignment%20signal%20through%20policy-gradient-based%20regularization.%20Our%20method%20introduces%20an%20adaptive%20gating%20mechanism%20that%20dynamically%20balances%20supervised%20and%20alignment-driven%20gradients%20on%20a%20per-sample%20basis%2C%20prioritizing%20uncertain%20or%20misaligned%20cases%20while%20allowing%20well-aligned%20examples%20to%20follow%20standard%20supervised%20updates.%20The%20framework%20further%20learns%20abstention%20behavior%20for%20fully%20misaligned%20inputs%2C%20incorporating%20conservative%20responses%20directly%20into%20the%20fine-tuned%20model.%20Experiments%20on%20general%20and%20domain-specific%20instruction-tuning%20benchmarks%20demonstrate%20consistent%20reductions%20in%20harmful%20and%20hallucinated%20outputs%20without%20sacrificing%20downstream%20task%20performance.%20Additional%20analyses%20show%20robustness%20to%20adversarial%20fine-tuning%2C%20prompt-based%20attacks%2C%20and%20unsafe%20initializations%2C%20establishing%20adaptively%20gated%20alignment%20optimization%20as%20an%20effective%20approach%20for%20alignment-preserving%20and%20alignment-recovering%20model%20adaptation.&entry.1838667208=http%3A//arxiv.org/abs/2602.02258v1&entry.124074799=Read"},
{"title": "More Than a Quick Glance: Overcoming the Greedy Bias in KV-Cache Compression", "author": "Aryan Sood and Tanvi Sharma and Vansh Agrawal", "abstract": "While Large Language Models (LLMs) can theoretically support extensive context windows, their actual deployment is constrained by the linear growth of Key-Value (KV) cache memory. Prevailing compression strategies mitigate this through various pruning mechanisms, yet trade-off semantic recall for memory efficiency. In this work, we present LASER-KV (Layer Accumulated Selection with Exact-LSH Recall), a framework designed to test the limits of KV compression under a strict accumulative budgeting policy. We deviate from the standard fixed summary size approach by implementing a block-wise accumulation strategy governed by a protection divisor (n). This allows us to isolate the effects of compression from sliding window artifacts. Our experiments on the Babilong benchmark reveal performance degradation in previous compression methods by 15-30% on various long context tasks. LASER-KV maintains stable performance, achieving superior accuracies by a margin of upto 10% at 128k. These findings challenge the prevailing assumption that attention scores alone are a sufficient proxy for token utility.", "link": "http://arxiv.org/abs/2602.02199v1", "date": "2026-02-02", "relevancy": 2.4894, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20More%20Than%20a%20Quick%20Glance%3A%20Overcoming%20the%20Greedy%20Bias%20in%20KV-Cache%20Compression&body=Title%3A%20More%20Than%20a%20Quick%20Glance%3A%20Overcoming%20the%20Greedy%20Bias%20in%20KV-Cache%20Compression%0AAuthor%3A%20Aryan%20Sood%20and%20Tanvi%20Sharma%20and%20Vansh%20Agrawal%0AAbstract%3A%20While%20Large%20Language%20Models%20%28LLMs%29%20can%20theoretically%20support%20extensive%20context%20windows%2C%20their%20actual%20deployment%20is%20constrained%20by%20the%20linear%20growth%20of%20Key-Value%20%28KV%29%20cache%20memory.%20Prevailing%20compression%20strategies%20mitigate%20this%20through%20various%20pruning%20mechanisms%2C%20yet%20trade-off%20semantic%20recall%20for%20memory%20efficiency.%20In%20this%20work%2C%20we%20present%20LASER-KV%20%28Layer%20Accumulated%20Selection%20with%20Exact-LSH%20Recall%29%2C%20a%20framework%20designed%20to%20test%20the%20limits%20of%20KV%20compression%20under%20a%20strict%20accumulative%20budgeting%20policy.%20We%20deviate%20from%20the%20standard%20fixed%20summary%20size%20approach%20by%20implementing%20a%20block-wise%20accumulation%20strategy%20governed%20by%20a%20protection%20divisor%20%28n%29.%20This%20allows%20us%20to%20isolate%20the%20effects%20of%20compression%20from%20sliding%20window%20artifacts.%20Our%20experiments%20on%20the%20Babilong%20benchmark%20reveal%20performance%20degradation%20in%20previous%20compression%20methods%20by%2015-30%25%20on%20various%20long%20context%20tasks.%20LASER-KV%20maintains%20stable%20performance%2C%20achieving%20superior%20accuracies%20by%20a%20margin%20of%20upto%2010%25%20at%20128k.%20These%20findings%20challenge%20the%20prevailing%20assumption%20that%20attention%20scores%20alone%20are%20a%20sufficient%20proxy%20for%20token%20utility.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02199v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMore%2520Than%2520a%2520Quick%2520Glance%253A%2520Overcoming%2520the%2520Greedy%2520Bias%2520in%2520KV-Cache%2520Compression%26entry.906535625%3DAryan%2520Sood%2520and%2520Tanvi%2520Sharma%2520and%2520Vansh%2520Agrawal%26entry.1292438233%3DWhile%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520theoretically%2520support%2520extensive%2520context%2520windows%252C%2520their%2520actual%2520deployment%2520is%2520constrained%2520by%2520the%2520linear%2520growth%2520of%2520Key-Value%2520%2528KV%2529%2520cache%2520memory.%2520Prevailing%2520compression%2520strategies%2520mitigate%2520this%2520through%2520various%2520pruning%2520mechanisms%252C%2520yet%2520trade-off%2520semantic%2520recall%2520for%2520memory%2520efficiency.%2520In%2520this%2520work%252C%2520we%2520present%2520LASER-KV%2520%2528Layer%2520Accumulated%2520Selection%2520with%2520Exact-LSH%2520Recall%2529%252C%2520a%2520framework%2520designed%2520to%2520test%2520the%2520limits%2520of%2520KV%2520compression%2520under%2520a%2520strict%2520accumulative%2520budgeting%2520policy.%2520We%2520deviate%2520from%2520the%2520standard%2520fixed%2520summary%2520size%2520approach%2520by%2520implementing%2520a%2520block-wise%2520accumulation%2520strategy%2520governed%2520by%2520a%2520protection%2520divisor%2520%2528n%2529.%2520This%2520allows%2520us%2520to%2520isolate%2520the%2520effects%2520of%2520compression%2520from%2520sliding%2520window%2520artifacts.%2520Our%2520experiments%2520on%2520the%2520Babilong%2520benchmark%2520reveal%2520performance%2520degradation%2520in%2520previous%2520compression%2520methods%2520by%252015-30%2525%2520on%2520various%2520long%2520context%2520tasks.%2520LASER-KV%2520maintains%2520stable%2520performance%252C%2520achieving%2520superior%2520accuracies%2520by%2520a%2520margin%2520of%2520upto%252010%2525%2520at%2520128k.%2520These%2520findings%2520challenge%2520the%2520prevailing%2520assumption%2520that%2520attention%2520scores%2520alone%2520are%2520a%2520sufficient%2520proxy%2520for%2520token%2520utility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02199v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=More%20Than%20a%20Quick%20Glance%3A%20Overcoming%20the%20Greedy%20Bias%20in%20KV-Cache%20Compression&entry.906535625=Aryan%20Sood%20and%20Tanvi%20Sharma%20and%20Vansh%20Agrawal&entry.1292438233=While%20Large%20Language%20Models%20%28LLMs%29%20can%20theoretically%20support%20extensive%20context%20windows%2C%20their%20actual%20deployment%20is%20constrained%20by%20the%20linear%20growth%20of%20Key-Value%20%28KV%29%20cache%20memory.%20Prevailing%20compression%20strategies%20mitigate%20this%20through%20various%20pruning%20mechanisms%2C%20yet%20trade-off%20semantic%20recall%20for%20memory%20efficiency.%20In%20this%20work%2C%20we%20present%20LASER-KV%20%28Layer%20Accumulated%20Selection%20with%20Exact-LSH%20Recall%29%2C%20a%20framework%20designed%20to%20test%20the%20limits%20of%20KV%20compression%20under%20a%20strict%20accumulative%20budgeting%20policy.%20We%20deviate%20from%20the%20standard%20fixed%20summary%20size%20approach%20by%20implementing%20a%20block-wise%20accumulation%20strategy%20governed%20by%20a%20protection%20divisor%20%28n%29.%20This%20allows%20us%20to%20isolate%20the%20effects%20of%20compression%20from%20sliding%20window%20artifacts.%20Our%20experiments%20on%20the%20Babilong%20benchmark%20reveal%20performance%20degradation%20in%20previous%20compression%20methods%20by%2015-30%25%20on%20various%20long%20context%20tasks.%20LASER-KV%20maintains%20stable%20performance%2C%20achieving%20superior%20accuracies%20by%20a%20margin%20of%20upto%2010%25%20at%20128k.%20These%20findings%20challenge%20the%20prevailing%20assumption%20that%20attention%20scores%20alone%20are%20a%20sufficient%20proxy%20for%20token%20utility.&entry.1838667208=http%3A//arxiv.org/abs/2602.02199v1&entry.124074799=Read"},
{"title": "EvoXplain: When Machine Learning Models Agree on Predictions but Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs", "author": "Chama Bensmail", "abstract": "Machine learning models are primarily judged by predictive performance, especially in applied settings. Once a model reaches high accuracy, its explanation is often assumed to be correct and trustworthy. This assumption raises an overlooked question: when two models achieve high accuracy, do they rely on the same internal logic, or do they reach the same outcome via different and potentially competing mechanisms? We introduce EvoXplain, a diagnostic framework that measures the stability of model explanations across repeated training. Rather than analysing the explanation of a single trained model, EvoXplain treats explanations as samples drawn from the training and model selection pipeline itself, without aggregating predictions or constructing ensembles. It examines whether these samples form a single coherent explanation or separate into multiple structured explanatory modes. We evaluate EvoXplain on the Breast Cancer and COMPAS datasets using Logistic Regression and Random Forests. Although all models achieve high predictive accuracy, their explanations frequently exhibit clear multimodality. Even models commonly assumed to be stable, such as Logistic Regression, can give rise to distinct explanation modes under repeated training on the same data split. Crucially, these modes can coexist at near-identical hyperparameter configurations, indicating explanation non-identifiability rather than smooth sensitivity to regularisation strength. EvoXplain does not attempt to select a correct explanation. Instead, it makes explanatory instability visible and quantifiable, revealing when single-instance or averaged explanations obscure the existence of multiple underlying mechanisms. More broadly, EvoXplain reframes interpretability as a property of a model class under repeated instantiation, rather than of any single trained model.", "link": "http://arxiv.org/abs/2512.22240v3", "date": "2026-02-02", "relevancy": 2.4893, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5076}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvoXplain%3A%20When%20Machine%20Learning%20Models%20Agree%20on%20Predictions%20but%20Disagree%20on%20Why%20--%20Measuring%20Mechanistic%20Multiplicity%20Across%20Training%20Runs&body=Title%3A%20EvoXplain%3A%20When%20Machine%20Learning%20Models%20Agree%20on%20Predictions%20but%20Disagree%20on%20Why%20--%20Measuring%20Mechanistic%20Multiplicity%20Across%20Training%20Runs%0AAuthor%3A%20Chama%20Bensmail%0AAbstract%3A%20Machine%20learning%20models%20are%20primarily%20judged%20by%20predictive%20performance%2C%20especially%20in%20applied%20settings.%20Once%20a%20model%20reaches%20high%20accuracy%2C%20its%20explanation%20is%20often%20assumed%20to%20be%20correct%20and%20trustworthy.%20This%20assumption%20raises%20an%20overlooked%20question%3A%20when%20two%20models%20achieve%20high%20accuracy%2C%20do%20they%20rely%20on%20the%20same%20internal%20logic%2C%20or%20do%20they%20reach%20the%20same%20outcome%20via%20different%20and%20potentially%20competing%20mechanisms%3F%20We%20introduce%20EvoXplain%2C%20a%20diagnostic%20framework%20that%20measures%20the%20stability%20of%20model%20explanations%20across%20repeated%20training.%20Rather%20than%20analysing%20the%20explanation%20of%20a%20single%20trained%20model%2C%20EvoXplain%20treats%20explanations%20as%20samples%20drawn%20from%20the%20training%20and%20model%20selection%20pipeline%20itself%2C%20without%20aggregating%20predictions%20or%20constructing%20ensembles.%20It%20examines%20whether%20these%20samples%20form%20a%20single%20coherent%20explanation%20or%20separate%20into%20multiple%20structured%20explanatory%20modes.%20We%20evaluate%20EvoXplain%20on%20the%20Breast%20Cancer%20and%20COMPAS%20datasets%20using%20Logistic%20Regression%20and%20Random%20Forests.%20Although%20all%20models%20achieve%20high%20predictive%20accuracy%2C%20their%20explanations%20frequently%20exhibit%20clear%20multimodality.%20Even%20models%20commonly%20assumed%20to%20be%20stable%2C%20such%20as%20Logistic%20Regression%2C%20can%20give%20rise%20to%20distinct%20explanation%20modes%20under%20repeated%20training%20on%20the%20same%20data%20split.%20Crucially%2C%20these%20modes%20can%20coexist%20at%20near-identical%20hyperparameter%20configurations%2C%20indicating%20explanation%20non-identifiability%20rather%20than%20smooth%20sensitivity%20to%20regularisation%20strength.%20EvoXplain%20does%20not%20attempt%20to%20select%20a%20correct%20explanation.%20Instead%2C%20it%20makes%20explanatory%20instability%20visible%20and%20quantifiable%2C%20revealing%20when%20single-instance%20or%20averaged%20explanations%20obscure%20the%20existence%20of%20multiple%20underlying%20mechanisms.%20More%20broadly%2C%20EvoXplain%20reframes%20interpretability%20as%20a%20property%20of%20a%20model%20class%20under%20repeated%20instantiation%2C%20rather%20than%20of%20any%20single%20trained%20model.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22240v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvoXplain%253A%2520When%2520Machine%2520Learning%2520Models%2520Agree%2520on%2520Predictions%2520but%2520Disagree%2520on%2520Why%2520--%2520Measuring%2520Mechanistic%2520Multiplicity%2520Across%2520Training%2520Runs%26entry.906535625%3DChama%2520Bensmail%26entry.1292438233%3DMachine%2520learning%2520models%2520are%2520primarily%2520judged%2520by%2520predictive%2520performance%252C%2520especially%2520in%2520applied%2520settings.%2520Once%2520a%2520model%2520reaches%2520high%2520accuracy%252C%2520its%2520explanation%2520is%2520often%2520assumed%2520to%2520be%2520correct%2520and%2520trustworthy.%2520This%2520assumption%2520raises%2520an%2520overlooked%2520question%253A%2520when%2520two%2520models%2520achieve%2520high%2520accuracy%252C%2520do%2520they%2520rely%2520on%2520the%2520same%2520internal%2520logic%252C%2520or%2520do%2520they%2520reach%2520the%2520same%2520outcome%2520via%2520different%2520and%2520potentially%2520competing%2520mechanisms%253F%2520We%2520introduce%2520EvoXplain%252C%2520a%2520diagnostic%2520framework%2520that%2520measures%2520the%2520stability%2520of%2520model%2520explanations%2520across%2520repeated%2520training.%2520Rather%2520than%2520analysing%2520the%2520explanation%2520of%2520a%2520single%2520trained%2520model%252C%2520EvoXplain%2520treats%2520explanations%2520as%2520samples%2520drawn%2520from%2520the%2520training%2520and%2520model%2520selection%2520pipeline%2520itself%252C%2520without%2520aggregating%2520predictions%2520or%2520constructing%2520ensembles.%2520It%2520examines%2520whether%2520these%2520samples%2520form%2520a%2520single%2520coherent%2520explanation%2520or%2520separate%2520into%2520multiple%2520structured%2520explanatory%2520modes.%2520We%2520evaluate%2520EvoXplain%2520on%2520the%2520Breast%2520Cancer%2520and%2520COMPAS%2520datasets%2520using%2520Logistic%2520Regression%2520and%2520Random%2520Forests.%2520Although%2520all%2520models%2520achieve%2520high%2520predictive%2520accuracy%252C%2520their%2520explanations%2520frequently%2520exhibit%2520clear%2520multimodality.%2520Even%2520models%2520commonly%2520assumed%2520to%2520be%2520stable%252C%2520such%2520as%2520Logistic%2520Regression%252C%2520can%2520give%2520rise%2520to%2520distinct%2520explanation%2520modes%2520under%2520repeated%2520training%2520on%2520the%2520same%2520data%2520split.%2520Crucially%252C%2520these%2520modes%2520can%2520coexist%2520at%2520near-identical%2520hyperparameter%2520configurations%252C%2520indicating%2520explanation%2520non-identifiability%2520rather%2520than%2520smooth%2520sensitivity%2520to%2520regularisation%2520strength.%2520EvoXplain%2520does%2520not%2520attempt%2520to%2520select%2520a%2520correct%2520explanation.%2520Instead%252C%2520it%2520makes%2520explanatory%2520instability%2520visible%2520and%2520quantifiable%252C%2520revealing%2520when%2520single-instance%2520or%2520averaged%2520explanations%2520obscure%2520the%2520existence%2520of%2520multiple%2520underlying%2520mechanisms.%2520More%2520broadly%252C%2520EvoXplain%2520reframes%2520interpretability%2520as%2520a%2520property%2520of%2520a%2520model%2520class%2520under%2520repeated%2520instantiation%252C%2520rather%2520than%2520of%2520any%2520single%2520trained%2520model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22240v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvoXplain%3A%20When%20Machine%20Learning%20Models%20Agree%20on%20Predictions%20but%20Disagree%20on%20Why%20--%20Measuring%20Mechanistic%20Multiplicity%20Across%20Training%20Runs&entry.906535625=Chama%20Bensmail&entry.1292438233=Machine%20learning%20models%20are%20primarily%20judged%20by%20predictive%20performance%2C%20especially%20in%20applied%20settings.%20Once%20a%20model%20reaches%20high%20accuracy%2C%20its%20explanation%20is%20often%20assumed%20to%20be%20correct%20and%20trustworthy.%20This%20assumption%20raises%20an%20overlooked%20question%3A%20when%20two%20models%20achieve%20high%20accuracy%2C%20do%20they%20rely%20on%20the%20same%20internal%20logic%2C%20or%20do%20they%20reach%20the%20same%20outcome%20via%20different%20and%20potentially%20competing%20mechanisms%3F%20We%20introduce%20EvoXplain%2C%20a%20diagnostic%20framework%20that%20measures%20the%20stability%20of%20model%20explanations%20across%20repeated%20training.%20Rather%20than%20analysing%20the%20explanation%20of%20a%20single%20trained%20model%2C%20EvoXplain%20treats%20explanations%20as%20samples%20drawn%20from%20the%20training%20and%20model%20selection%20pipeline%20itself%2C%20without%20aggregating%20predictions%20or%20constructing%20ensembles.%20It%20examines%20whether%20these%20samples%20form%20a%20single%20coherent%20explanation%20or%20separate%20into%20multiple%20structured%20explanatory%20modes.%20We%20evaluate%20EvoXplain%20on%20the%20Breast%20Cancer%20and%20COMPAS%20datasets%20using%20Logistic%20Regression%20and%20Random%20Forests.%20Although%20all%20models%20achieve%20high%20predictive%20accuracy%2C%20their%20explanations%20frequently%20exhibit%20clear%20multimodality.%20Even%20models%20commonly%20assumed%20to%20be%20stable%2C%20such%20as%20Logistic%20Regression%2C%20can%20give%20rise%20to%20distinct%20explanation%20modes%20under%20repeated%20training%20on%20the%20same%20data%20split.%20Crucially%2C%20these%20modes%20can%20coexist%20at%20near-identical%20hyperparameter%20configurations%2C%20indicating%20explanation%20non-identifiability%20rather%20than%20smooth%20sensitivity%20to%20regularisation%20strength.%20EvoXplain%20does%20not%20attempt%20to%20select%20a%20correct%20explanation.%20Instead%2C%20it%20makes%20explanatory%20instability%20visible%20and%20quantifiable%2C%20revealing%20when%20single-instance%20or%20averaged%20explanations%20obscure%20the%20existence%20of%20multiple%20underlying%20mechanisms.%20More%20broadly%2C%20EvoXplain%20reframes%20interpretability%20as%20a%20property%20of%20a%20model%20class%20under%20repeated%20instantiation%2C%20rather%20than%20of%20any%20single%20trained%20model.&entry.1838667208=http%3A//arxiv.org/abs/2512.22240v3&entry.124074799=Read"},
{"title": "STILL: Selecting Tokens for Intra-Layer Hybrid Attention to Linearize LLMs", "author": "Weikang Meng and Liangyu Huo and Yadan Luo and Jiawen Guan and Jingyi Zhang and Yingjian Li and Zheng Zhang", "abstract": "Linearizing pretrained large language models (LLMs) primarily relies on intra-layer hybrid attention mechanisms to alleviate the quadratic complexity of standard softmax attention. Existing methods perform token routing based on sliding-window partitions, resulting in position-based selection and fails to capture token-specific global importance. Meanwhile, linear attention further suffers from distribution shift caused by learnable feature maps that distort pretrained feature magnitudes. Motivated by these limitations, we propose STILL, an intra-layer hybrid linearization framework for efficiently linearizing LLMs. STILL introduces a Self-Saliency Score with strong local-global consistency, enabling accurate token selection using sliding-window computation, and retains salient tokens for sparse softmax attention while summarizing the remaining context via linear attention. To preserve pretrained representations, we design a Norm-Preserved Feature Map (NP-Map) that decouples feature direction from magnitude and reinjects pretrained norms. We further adopt a unified training-inference architecture with chunk-wise parallelization and delayed selection to improve hardware efficiency. Experiments show that STILL matches or surpasses the original pretrained model on commonsense and general reasoning tasks, and achieves up to a 86.2% relative improvement over prior linearized attention methods on long-context benchmarks.", "link": "http://arxiv.org/abs/2602.02180v1", "date": "2026-02-02", "relevancy": 2.4856, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5288}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4894}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STILL%3A%20Selecting%20Tokens%20for%20Intra-Layer%20Hybrid%20Attention%20to%20Linearize%20LLMs&body=Title%3A%20STILL%3A%20Selecting%20Tokens%20for%20Intra-Layer%20Hybrid%20Attention%20to%20Linearize%20LLMs%0AAuthor%3A%20Weikang%20Meng%20and%20Liangyu%20Huo%20and%20Yadan%20Luo%20and%20Jiawen%20Guan%20and%20Jingyi%20Zhang%20and%20Yingjian%20Li%20and%20Zheng%20Zhang%0AAbstract%3A%20Linearizing%20pretrained%20large%20language%20models%20%28LLMs%29%20primarily%20relies%20on%20intra-layer%20hybrid%20attention%20mechanisms%20to%20alleviate%20the%20quadratic%20complexity%20of%20standard%20softmax%20attention.%20Existing%20methods%20perform%20token%20routing%20based%20on%20sliding-window%20partitions%2C%20resulting%20in%20position-based%20selection%20and%20fails%20to%20capture%20token-specific%20global%20importance.%20Meanwhile%2C%20linear%20attention%20further%20suffers%20from%20distribution%20shift%20caused%20by%20learnable%20feature%20maps%20that%20distort%20pretrained%20feature%20magnitudes.%20Motivated%20by%20these%20limitations%2C%20we%20propose%20STILL%2C%20an%20intra-layer%20hybrid%20linearization%20framework%20for%20efficiently%20linearizing%20LLMs.%20STILL%20introduces%20a%20Self-Saliency%20Score%20with%20strong%20local-global%20consistency%2C%20enabling%20accurate%20token%20selection%20using%20sliding-window%20computation%2C%20and%20retains%20salient%20tokens%20for%20sparse%20softmax%20attention%20while%20summarizing%20the%20remaining%20context%20via%20linear%20attention.%20To%20preserve%20pretrained%20representations%2C%20we%20design%20a%20Norm-Preserved%20Feature%20Map%20%28NP-Map%29%20that%20decouples%20feature%20direction%20from%20magnitude%20and%20reinjects%20pretrained%20norms.%20We%20further%20adopt%20a%20unified%20training-inference%20architecture%20with%20chunk-wise%20parallelization%20and%20delayed%20selection%20to%20improve%20hardware%20efficiency.%20Experiments%20show%20that%20STILL%20matches%20or%20surpasses%20the%20original%20pretrained%20model%20on%20commonsense%20and%20general%20reasoning%20tasks%2C%20and%20achieves%20up%20to%20a%2086.2%25%20relative%20improvement%20over%20prior%20linearized%20attention%20methods%20on%20long-context%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTILL%253A%2520Selecting%2520Tokens%2520for%2520Intra-Layer%2520Hybrid%2520Attention%2520to%2520Linearize%2520LLMs%26entry.906535625%3DWeikang%2520Meng%2520and%2520Liangyu%2520Huo%2520and%2520Yadan%2520Luo%2520and%2520Jiawen%2520Guan%2520and%2520Jingyi%2520Zhang%2520and%2520Yingjian%2520Li%2520and%2520Zheng%2520Zhang%26entry.1292438233%3DLinearizing%2520pretrained%2520large%2520language%2520models%2520%2528LLMs%2529%2520primarily%2520relies%2520on%2520intra-layer%2520hybrid%2520attention%2520mechanisms%2520to%2520alleviate%2520the%2520quadratic%2520complexity%2520of%2520standard%2520softmax%2520attention.%2520Existing%2520methods%2520perform%2520token%2520routing%2520based%2520on%2520sliding-window%2520partitions%252C%2520resulting%2520in%2520position-based%2520selection%2520and%2520fails%2520to%2520capture%2520token-specific%2520global%2520importance.%2520Meanwhile%252C%2520linear%2520attention%2520further%2520suffers%2520from%2520distribution%2520shift%2520caused%2520by%2520learnable%2520feature%2520maps%2520that%2520distort%2520pretrained%2520feature%2520magnitudes.%2520Motivated%2520by%2520these%2520limitations%252C%2520we%2520propose%2520STILL%252C%2520an%2520intra-layer%2520hybrid%2520linearization%2520framework%2520for%2520efficiently%2520linearizing%2520LLMs.%2520STILL%2520introduces%2520a%2520Self-Saliency%2520Score%2520with%2520strong%2520local-global%2520consistency%252C%2520enabling%2520accurate%2520token%2520selection%2520using%2520sliding-window%2520computation%252C%2520and%2520retains%2520salient%2520tokens%2520for%2520sparse%2520softmax%2520attention%2520while%2520summarizing%2520the%2520remaining%2520context%2520via%2520linear%2520attention.%2520To%2520preserve%2520pretrained%2520representations%252C%2520we%2520design%2520a%2520Norm-Preserved%2520Feature%2520Map%2520%2528NP-Map%2529%2520that%2520decouples%2520feature%2520direction%2520from%2520magnitude%2520and%2520reinjects%2520pretrained%2520norms.%2520We%2520further%2520adopt%2520a%2520unified%2520training-inference%2520architecture%2520with%2520chunk-wise%2520parallelization%2520and%2520delayed%2520selection%2520to%2520improve%2520hardware%2520efficiency.%2520Experiments%2520show%2520that%2520STILL%2520matches%2520or%2520surpasses%2520the%2520original%2520pretrained%2520model%2520on%2520commonsense%2520and%2520general%2520reasoning%2520tasks%252C%2520and%2520achieves%2520up%2520to%2520a%252086.2%2525%2520relative%2520improvement%2520over%2520prior%2520linearized%2520attention%2520methods%2520on%2520long-context%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STILL%3A%20Selecting%20Tokens%20for%20Intra-Layer%20Hybrid%20Attention%20to%20Linearize%20LLMs&entry.906535625=Weikang%20Meng%20and%20Liangyu%20Huo%20and%20Yadan%20Luo%20and%20Jiawen%20Guan%20and%20Jingyi%20Zhang%20and%20Yingjian%20Li%20and%20Zheng%20Zhang&entry.1292438233=Linearizing%20pretrained%20large%20language%20models%20%28LLMs%29%20primarily%20relies%20on%20intra-layer%20hybrid%20attention%20mechanisms%20to%20alleviate%20the%20quadratic%20complexity%20of%20standard%20softmax%20attention.%20Existing%20methods%20perform%20token%20routing%20based%20on%20sliding-window%20partitions%2C%20resulting%20in%20position-based%20selection%20and%20fails%20to%20capture%20token-specific%20global%20importance.%20Meanwhile%2C%20linear%20attention%20further%20suffers%20from%20distribution%20shift%20caused%20by%20learnable%20feature%20maps%20that%20distort%20pretrained%20feature%20magnitudes.%20Motivated%20by%20these%20limitations%2C%20we%20propose%20STILL%2C%20an%20intra-layer%20hybrid%20linearization%20framework%20for%20efficiently%20linearizing%20LLMs.%20STILL%20introduces%20a%20Self-Saliency%20Score%20with%20strong%20local-global%20consistency%2C%20enabling%20accurate%20token%20selection%20using%20sliding-window%20computation%2C%20and%20retains%20salient%20tokens%20for%20sparse%20softmax%20attention%20while%20summarizing%20the%20remaining%20context%20via%20linear%20attention.%20To%20preserve%20pretrained%20representations%2C%20we%20design%20a%20Norm-Preserved%20Feature%20Map%20%28NP-Map%29%20that%20decouples%20feature%20direction%20from%20magnitude%20and%20reinjects%20pretrained%20norms.%20We%20further%20adopt%20a%20unified%20training-inference%20architecture%20with%20chunk-wise%20parallelization%20and%20delayed%20selection%20to%20improve%20hardware%20efficiency.%20Experiments%20show%20that%20STILL%20matches%20or%20surpasses%20the%20original%20pretrained%20model%20on%20commonsense%20and%20general%20reasoning%20tasks%2C%20and%20achieves%20up%20to%20a%2086.2%25%20relative%20improvement%20over%20prior%20linearized%20attention%20methods%20on%20long-context%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2602.02180v1&entry.124074799=Read"},
{"title": "Eliminating Registration Bias in Synthetic CT Generation: A Physics-Based Simulation Framework", "author": "Lukas Zimmermann and Michael Rauter and Maximilian Schmid and Dietmar Georg and Barbara Kn\u00e4usl", "abstract": "Supervised synthetic CT generation from CBCT requires registered training pairs, yet perfect registration between separately acquired scans remains unattainable. This registration bias propagates into trained models and corrupts standard evaluation metrics. This may suggest that superior benchmark performance indicates better reproduction of registration artifacts rather than anatomical fidelity. We propose physics-based CBCT simulation to provide geometrically aligned training pairs by construction, combined with evaluation using geometric alignment metrics against input CBCT rather than biased ground truth. On two independent pelvic datasets, models trained on synthetic data achieved superior geometric alignment (Normalized Mutual Information: 0.31 vs 0.22) despite lower conventional intensity scores. Intensity metrics showed inverted correlations with clinical assessment for deformably registered data, while Normalized Mutual Information consistently predicted observer preference across registration methodologies (rho = 0.31, p < 0.001). Clinical observers preferred synthetic-trained outputs in 87% of cases, demonstrating that geometric fidelity, not intensity agreement with biased ground truth, aligns with clinical requirements.", "link": "http://arxiv.org/abs/2602.02130v1", "date": "2026-02-02", "relevancy": 2.4841, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4997}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4962}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eliminating%20Registration%20Bias%20in%20Synthetic%20CT%20Generation%3A%20A%20Physics-Based%20Simulation%20Framework&body=Title%3A%20Eliminating%20Registration%20Bias%20in%20Synthetic%20CT%20Generation%3A%20A%20Physics-Based%20Simulation%20Framework%0AAuthor%3A%20Lukas%20Zimmermann%20and%20Michael%20Rauter%20and%20Maximilian%20Schmid%20and%20Dietmar%20Georg%20and%20Barbara%20Kn%C3%A4usl%0AAbstract%3A%20Supervised%20synthetic%20CT%20generation%20from%20CBCT%20requires%20registered%20training%20pairs%2C%20yet%20perfect%20registration%20between%20separately%20acquired%20scans%20remains%20unattainable.%20This%20registration%20bias%20propagates%20into%20trained%20models%20and%20corrupts%20standard%20evaluation%20metrics.%20This%20may%20suggest%20that%20superior%20benchmark%20performance%20indicates%20better%20reproduction%20of%20registration%20artifacts%20rather%20than%20anatomical%20fidelity.%20We%20propose%20physics-based%20CBCT%20simulation%20to%20provide%20geometrically%20aligned%20training%20pairs%20by%20construction%2C%20combined%20with%20evaluation%20using%20geometric%20alignment%20metrics%20against%20input%20CBCT%20rather%20than%20biased%20ground%20truth.%20On%20two%20independent%20pelvic%20datasets%2C%20models%20trained%20on%20synthetic%20data%20achieved%20superior%20geometric%20alignment%20%28Normalized%20Mutual%20Information%3A%200.31%20vs%200.22%29%20despite%20lower%20conventional%20intensity%20scores.%20Intensity%20metrics%20showed%20inverted%20correlations%20with%20clinical%20assessment%20for%20deformably%20registered%20data%2C%20while%20Normalized%20Mutual%20Information%20consistently%20predicted%20observer%20preference%20across%20registration%20methodologies%20%28rho%20%3D%200.31%2C%20p%20%3C%200.001%29.%20Clinical%20observers%20preferred%20synthetic-trained%20outputs%20in%2087%25%20of%20cases%2C%20demonstrating%20that%20geometric%20fidelity%2C%20not%20intensity%20agreement%20with%20biased%20ground%20truth%2C%20aligns%20with%20clinical%20requirements.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEliminating%2520Registration%2520Bias%2520in%2520Synthetic%2520CT%2520Generation%253A%2520A%2520Physics-Based%2520Simulation%2520Framework%26entry.906535625%3DLukas%2520Zimmermann%2520and%2520Michael%2520Rauter%2520and%2520Maximilian%2520Schmid%2520and%2520Dietmar%2520Georg%2520and%2520Barbara%2520Kn%25C3%25A4usl%26entry.1292438233%3DSupervised%2520synthetic%2520CT%2520generation%2520from%2520CBCT%2520requires%2520registered%2520training%2520pairs%252C%2520yet%2520perfect%2520registration%2520between%2520separately%2520acquired%2520scans%2520remains%2520unattainable.%2520This%2520registration%2520bias%2520propagates%2520into%2520trained%2520models%2520and%2520corrupts%2520standard%2520evaluation%2520metrics.%2520This%2520may%2520suggest%2520that%2520superior%2520benchmark%2520performance%2520indicates%2520better%2520reproduction%2520of%2520registration%2520artifacts%2520rather%2520than%2520anatomical%2520fidelity.%2520We%2520propose%2520physics-based%2520CBCT%2520simulation%2520to%2520provide%2520geometrically%2520aligned%2520training%2520pairs%2520by%2520construction%252C%2520combined%2520with%2520evaluation%2520using%2520geometric%2520alignment%2520metrics%2520against%2520input%2520CBCT%2520rather%2520than%2520biased%2520ground%2520truth.%2520On%2520two%2520independent%2520pelvic%2520datasets%252C%2520models%2520trained%2520on%2520synthetic%2520data%2520achieved%2520superior%2520geometric%2520alignment%2520%2528Normalized%2520Mutual%2520Information%253A%25200.31%2520vs%25200.22%2529%2520despite%2520lower%2520conventional%2520intensity%2520scores.%2520Intensity%2520metrics%2520showed%2520inverted%2520correlations%2520with%2520clinical%2520assessment%2520for%2520deformably%2520registered%2520data%252C%2520while%2520Normalized%2520Mutual%2520Information%2520consistently%2520predicted%2520observer%2520preference%2520across%2520registration%2520methodologies%2520%2528rho%2520%253D%25200.31%252C%2520p%2520%253C%25200.001%2529.%2520Clinical%2520observers%2520preferred%2520synthetic-trained%2520outputs%2520in%252087%2525%2520of%2520cases%252C%2520demonstrating%2520that%2520geometric%2520fidelity%252C%2520not%2520intensity%2520agreement%2520with%2520biased%2520ground%2520truth%252C%2520aligns%2520with%2520clinical%2520requirements.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eliminating%20Registration%20Bias%20in%20Synthetic%20CT%20Generation%3A%20A%20Physics-Based%20Simulation%20Framework&entry.906535625=Lukas%20Zimmermann%20and%20Michael%20Rauter%20and%20Maximilian%20Schmid%20and%20Dietmar%20Georg%20and%20Barbara%20Kn%C3%A4usl&entry.1292438233=Supervised%20synthetic%20CT%20generation%20from%20CBCT%20requires%20registered%20training%20pairs%2C%20yet%20perfect%20registration%20between%20separately%20acquired%20scans%20remains%20unattainable.%20This%20registration%20bias%20propagates%20into%20trained%20models%20and%20corrupts%20standard%20evaluation%20metrics.%20This%20may%20suggest%20that%20superior%20benchmark%20performance%20indicates%20better%20reproduction%20of%20registration%20artifacts%20rather%20than%20anatomical%20fidelity.%20We%20propose%20physics-based%20CBCT%20simulation%20to%20provide%20geometrically%20aligned%20training%20pairs%20by%20construction%2C%20combined%20with%20evaluation%20using%20geometric%20alignment%20metrics%20against%20input%20CBCT%20rather%20than%20biased%20ground%20truth.%20On%20two%20independent%20pelvic%20datasets%2C%20models%20trained%20on%20synthetic%20data%20achieved%20superior%20geometric%20alignment%20%28Normalized%20Mutual%20Information%3A%200.31%20vs%200.22%29%20despite%20lower%20conventional%20intensity%20scores.%20Intensity%20metrics%20showed%20inverted%20correlations%20with%20clinical%20assessment%20for%20deformably%20registered%20data%2C%20while%20Normalized%20Mutual%20Information%20consistently%20predicted%20observer%20preference%20across%20registration%20methodologies%20%28rho%20%3D%200.31%2C%20p%20%3C%200.001%29.%20Clinical%20observers%20preferred%20synthetic-trained%20outputs%20in%2087%25%20of%20cases%2C%20demonstrating%20that%20geometric%20fidelity%2C%20not%20intensity%20agreement%20with%20biased%20ground%20truth%2C%20aligns%20with%20clinical%20requirements.&entry.1838667208=http%3A//arxiv.org/abs/2602.02130v1&entry.124074799=Read"},
{"title": "Investigating Modality Contribution in Audio LLMs for Music", "author": "Giovana Morais and Magdalena Fuentes", "abstract": "Audio Large Language Models (Audio LLMs) enable human-like conversation about music, yet it is unclear if they are truly listening to the audio or just using textual reasoning, as recent benchmarks suggest. This paper investigates this issue by quantifying the contribution of each modality to a model's output. We adapt the MM-SHAP framework, a performance-agnostic score based on Shapley values that quantifies the relative contribution of each modality to a model's prediction. We evaluate two models on the MuChoMusic benchmark and find that the model with higher accuracy relies more on text to answer questions, but further inspection shows that even if the overall audio contribution is low, models can successfully localize key sound events, suggesting that audio is not entirely ignored. Our study is the first application of MM-SHAP to Audio LLMs and we hope it will serve as a foundational step for future research in explainable AI and audio.", "link": "http://arxiv.org/abs/2509.20641v2", "date": "2026-02-02", "relevancy": 2.4836, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5022}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.494}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20Modality%20Contribution%20in%20Audio%20LLMs%20for%20Music&body=Title%3A%20Investigating%20Modality%20Contribution%20in%20Audio%20LLMs%20for%20Music%0AAuthor%3A%20Giovana%20Morais%20and%20Magdalena%20Fuentes%0AAbstract%3A%20Audio%20Large%20Language%20Models%20%28Audio%20LLMs%29%20enable%20human-like%20conversation%20about%20music%2C%20yet%20it%20is%20unclear%20if%20they%20are%20truly%20listening%20to%20the%20audio%20or%20just%20using%20textual%20reasoning%2C%20as%20recent%20benchmarks%20suggest.%20This%20paper%20investigates%20this%20issue%20by%20quantifying%20the%20contribution%20of%20each%20modality%20to%20a%20model%27s%20output.%20We%20adapt%20the%20MM-SHAP%20framework%2C%20a%20performance-agnostic%20score%20based%20on%20Shapley%20values%20that%20quantifies%20the%20relative%20contribution%20of%20each%20modality%20to%20a%20model%27s%20prediction.%20We%20evaluate%20two%20models%20on%20the%20MuChoMusic%20benchmark%20and%20find%20that%20the%20model%20with%20higher%20accuracy%20relies%20more%20on%20text%20to%20answer%20questions%2C%20but%20further%20inspection%20shows%20that%20even%20if%20the%20overall%20audio%20contribution%20is%20low%2C%20models%20can%20successfully%20localize%20key%20sound%20events%2C%20suggesting%20that%20audio%20is%20not%20entirely%20ignored.%20Our%20study%20is%20the%20first%20application%20of%20MM-SHAP%20to%20Audio%20LLMs%20and%20we%20hope%20it%20will%20serve%20as%20a%20foundational%20step%20for%20future%20research%20in%20explainable%20AI%20and%20audio.%0ALink%3A%20http%3A//arxiv.org/abs/2509.20641v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520Modality%2520Contribution%2520in%2520Audio%2520LLMs%2520for%2520Music%26entry.906535625%3DGiovana%2520Morais%2520and%2520Magdalena%2520Fuentes%26entry.1292438233%3DAudio%2520Large%2520Language%2520Models%2520%2528Audio%2520LLMs%2529%2520enable%2520human-like%2520conversation%2520about%2520music%252C%2520yet%2520it%2520is%2520unclear%2520if%2520they%2520are%2520truly%2520listening%2520to%2520the%2520audio%2520or%2520just%2520using%2520textual%2520reasoning%252C%2520as%2520recent%2520benchmarks%2520suggest.%2520This%2520paper%2520investigates%2520this%2520issue%2520by%2520quantifying%2520the%2520contribution%2520of%2520each%2520modality%2520to%2520a%2520model%2527s%2520output.%2520We%2520adapt%2520the%2520MM-SHAP%2520framework%252C%2520a%2520performance-agnostic%2520score%2520based%2520on%2520Shapley%2520values%2520that%2520quantifies%2520the%2520relative%2520contribution%2520of%2520each%2520modality%2520to%2520a%2520model%2527s%2520prediction.%2520We%2520evaluate%2520two%2520models%2520on%2520the%2520MuChoMusic%2520benchmark%2520and%2520find%2520that%2520the%2520model%2520with%2520higher%2520accuracy%2520relies%2520more%2520on%2520text%2520to%2520answer%2520questions%252C%2520but%2520further%2520inspection%2520shows%2520that%2520even%2520if%2520the%2520overall%2520audio%2520contribution%2520is%2520low%252C%2520models%2520can%2520successfully%2520localize%2520key%2520sound%2520events%252C%2520suggesting%2520that%2520audio%2520is%2520not%2520entirely%2520ignored.%2520Our%2520study%2520is%2520the%2520first%2520application%2520of%2520MM-SHAP%2520to%2520Audio%2520LLMs%2520and%2520we%2520hope%2520it%2520will%2520serve%2520as%2520a%2520foundational%2520step%2520for%2520future%2520research%2520in%2520explainable%2520AI%2520and%2520audio.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20641v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20Modality%20Contribution%20in%20Audio%20LLMs%20for%20Music&entry.906535625=Giovana%20Morais%20and%20Magdalena%20Fuentes&entry.1292438233=Audio%20Large%20Language%20Models%20%28Audio%20LLMs%29%20enable%20human-like%20conversation%20about%20music%2C%20yet%20it%20is%20unclear%20if%20they%20are%20truly%20listening%20to%20the%20audio%20or%20just%20using%20textual%20reasoning%2C%20as%20recent%20benchmarks%20suggest.%20This%20paper%20investigates%20this%20issue%20by%20quantifying%20the%20contribution%20of%20each%20modality%20to%20a%20model%27s%20output.%20We%20adapt%20the%20MM-SHAP%20framework%2C%20a%20performance-agnostic%20score%20based%20on%20Shapley%20values%20that%20quantifies%20the%20relative%20contribution%20of%20each%20modality%20to%20a%20model%27s%20prediction.%20We%20evaluate%20two%20models%20on%20the%20MuChoMusic%20benchmark%20and%20find%20that%20the%20model%20with%20higher%20accuracy%20relies%20more%20on%20text%20to%20answer%20questions%2C%20but%20further%20inspection%20shows%20that%20even%20if%20the%20overall%20audio%20contribution%20is%20low%2C%20models%20can%20successfully%20localize%20key%20sound%20events%2C%20suggesting%20that%20audio%20is%20not%20entirely%20ignored.%20Our%20study%20is%20the%20first%20application%20of%20MM-SHAP%20to%20Audio%20LLMs%20and%20we%20hope%20it%20will%20serve%20as%20a%20foundational%20step%20for%20future%20research%20in%20explainable%20AI%20and%20audio.&entry.1838667208=http%3A//arxiv.org/abs/2509.20641v2&entry.124074799=Read"},
{"title": "AICD Bench: A Challenging Benchmark for AI-Generated Code Detection", "author": "Daniil Orel and Dilshod Azizov and Indraneil Paul and Yuxia Wang and Iryna Gurevych and Preslav Nakov", "abstract": "Large language models (LLMs) are increasingly capable of generating functional source code, raising concerns about authorship, accountability, and security. While detecting AI-generated code is critical, existing datasets and benchmarks are narrow, typically limited to binary human-machine classification under in-distribution settings. To bridge this gap, we introduce $\\emph{AICD Bench}$, the most comprehensive benchmark for AI-generated code detection. It spans $\\emph{2M examples}$, $\\emph{77 models}$ across $\\emph{11 families}$, and $\\emph{9 programming languages}$, including recent reasoning models. Beyond scale, AICD Bench introduces three realistic detection tasks: ($\\emph{i}$)~$\\emph{Robust Binary Classification}$ under distribution shifts in language and domain, ($\\emph{ii}$)~$\\emph{Model Family Attribution}$, grouping generators by architectural lineage, and ($\\emph{iii}$)~$\\emph{Fine-Grained Human-Machine Classification}$ across human, machine, hybrid, and adversarial code. Extensive evaluation on neural and classical detectors shows that performance remains far below practical usability, particularly under distribution shift and for hybrid or adversarial code. We release AICD Bench as a $\\emph{unified, challenging evaluation suite}$ to drive the next generation of robust approaches for AI-generated code detection. The data and the code are available at https://huggingface.co/AICD-bench}.", "link": "http://arxiv.org/abs/2602.02079v1", "date": "2026-02-02", "relevancy": 2.477, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5219}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4873}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AICD%20Bench%3A%20A%20Challenging%20Benchmark%20for%20AI-Generated%20Code%20Detection&body=Title%3A%20AICD%20Bench%3A%20A%20Challenging%20Benchmark%20for%20AI-Generated%20Code%20Detection%0AAuthor%3A%20Daniil%20Orel%20and%20Dilshod%20Azizov%20and%20Indraneil%20Paul%20and%20Yuxia%20Wang%20and%20Iryna%20Gurevych%20and%20Preslav%20Nakov%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20capable%20of%20generating%20functional%20source%20code%2C%20raising%20concerns%20about%20authorship%2C%20accountability%2C%20and%20security.%20While%20detecting%20AI-generated%20code%20is%20critical%2C%20existing%20datasets%20and%20benchmarks%20are%20narrow%2C%20typically%20limited%20to%20binary%20human-machine%20classification%20under%20in-distribution%20settings.%20To%20bridge%20this%20gap%2C%20we%20introduce%20%24%5Cemph%7BAICD%20Bench%7D%24%2C%20the%20most%20comprehensive%20benchmark%20for%20AI-generated%20code%20detection.%20It%20spans%20%24%5Cemph%7B2M%20examples%7D%24%2C%20%24%5Cemph%7B77%20models%7D%24%20across%20%24%5Cemph%7B11%20families%7D%24%2C%20and%20%24%5Cemph%7B9%20programming%20languages%7D%24%2C%20including%20recent%20reasoning%20models.%20Beyond%20scale%2C%20AICD%20Bench%20introduces%20three%20realistic%20detection%20tasks%3A%20%28%24%5Cemph%7Bi%7D%24%29~%24%5Cemph%7BRobust%20Binary%20Classification%7D%24%20under%20distribution%20shifts%20in%20language%20and%20domain%2C%20%28%24%5Cemph%7Bii%7D%24%29~%24%5Cemph%7BModel%20Family%20Attribution%7D%24%2C%20grouping%20generators%20by%20architectural%20lineage%2C%20and%20%28%24%5Cemph%7Biii%7D%24%29~%24%5Cemph%7BFine-Grained%20Human-Machine%20Classification%7D%24%20across%20human%2C%20machine%2C%20hybrid%2C%20and%20adversarial%20code.%20Extensive%20evaluation%20on%20neural%20and%20classical%20detectors%20shows%20that%20performance%20remains%20far%20below%20practical%20usability%2C%20particularly%20under%20distribution%20shift%20and%20for%20hybrid%20or%20adversarial%20code.%20We%20release%20AICD%20Bench%20as%20a%20%24%5Cemph%7Bunified%2C%20challenging%20evaluation%20suite%7D%24%20to%20drive%20the%20next%20generation%20of%20robust%20approaches%20for%20AI-generated%20code%20detection.%20The%20data%20and%20the%20code%20are%20available%20at%20https%3A//huggingface.co/AICD-bench%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAICD%2520Bench%253A%2520A%2520Challenging%2520Benchmark%2520for%2520AI-Generated%2520Code%2520Detection%26entry.906535625%3DDaniil%2520Orel%2520and%2520Dilshod%2520Azizov%2520and%2520Indraneil%2520Paul%2520and%2520Yuxia%2520Wang%2520and%2520Iryna%2520Gurevych%2520and%2520Preslav%2520Nakov%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520capable%2520of%2520generating%2520functional%2520source%2520code%252C%2520raising%2520concerns%2520about%2520authorship%252C%2520accountability%252C%2520and%2520security.%2520While%2520detecting%2520AI-generated%2520code%2520is%2520critical%252C%2520existing%2520datasets%2520and%2520benchmarks%2520are%2520narrow%252C%2520typically%2520limited%2520to%2520binary%2520human-machine%2520classification%2520under%2520in-distribution%2520settings.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520%2524%255Cemph%257BAICD%2520Bench%257D%2524%252C%2520the%2520most%2520comprehensive%2520benchmark%2520for%2520AI-generated%2520code%2520detection.%2520It%2520spans%2520%2524%255Cemph%257B2M%2520examples%257D%2524%252C%2520%2524%255Cemph%257B77%2520models%257D%2524%2520across%2520%2524%255Cemph%257B11%2520families%257D%2524%252C%2520and%2520%2524%255Cemph%257B9%2520programming%2520languages%257D%2524%252C%2520including%2520recent%2520reasoning%2520models.%2520Beyond%2520scale%252C%2520AICD%2520Bench%2520introduces%2520three%2520realistic%2520detection%2520tasks%253A%2520%2528%2524%255Cemph%257Bi%257D%2524%2529~%2524%255Cemph%257BRobust%2520Binary%2520Classification%257D%2524%2520under%2520distribution%2520shifts%2520in%2520language%2520and%2520domain%252C%2520%2528%2524%255Cemph%257Bii%257D%2524%2529~%2524%255Cemph%257BModel%2520Family%2520Attribution%257D%2524%252C%2520grouping%2520generators%2520by%2520architectural%2520lineage%252C%2520and%2520%2528%2524%255Cemph%257Biii%257D%2524%2529~%2524%255Cemph%257BFine-Grained%2520Human-Machine%2520Classification%257D%2524%2520across%2520human%252C%2520machine%252C%2520hybrid%252C%2520and%2520adversarial%2520code.%2520Extensive%2520evaluation%2520on%2520neural%2520and%2520classical%2520detectors%2520shows%2520that%2520performance%2520remains%2520far%2520below%2520practical%2520usability%252C%2520particularly%2520under%2520distribution%2520shift%2520and%2520for%2520hybrid%2520or%2520adversarial%2520code.%2520We%2520release%2520AICD%2520Bench%2520as%2520a%2520%2524%255Cemph%257Bunified%252C%2520challenging%2520evaluation%2520suite%257D%2524%2520to%2520drive%2520the%2520next%2520generation%2520of%2520robust%2520approaches%2520for%2520AI-generated%2520code%2520detection.%2520The%2520data%2520and%2520the%2520code%2520are%2520available%2520at%2520https%253A//huggingface.co/AICD-bench%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AICD%20Bench%3A%20A%20Challenging%20Benchmark%20for%20AI-Generated%20Code%20Detection&entry.906535625=Daniil%20Orel%20and%20Dilshod%20Azizov%20and%20Indraneil%20Paul%20and%20Yuxia%20Wang%20and%20Iryna%20Gurevych%20and%20Preslav%20Nakov&entry.1292438233=Large%20language%20models%20%28LLMs%29%20are%20increasingly%20capable%20of%20generating%20functional%20source%20code%2C%20raising%20concerns%20about%20authorship%2C%20accountability%2C%20and%20security.%20While%20detecting%20AI-generated%20code%20is%20critical%2C%20existing%20datasets%20and%20benchmarks%20are%20narrow%2C%20typically%20limited%20to%20binary%20human-machine%20classification%20under%20in-distribution%20settings.%20To%20bridge%20this%20gap%2C%20we%20introduce%20%24%5Cemph%7BAICD%20Bench%7D%24%2C%20the%20most%20comprehensive%20benchmark%20for%20AI-generated%20code%20detection.%20It%20spans%20%24%5Cemph%7B2M%20examples%7D%24%2C%20%24%5Cemph%7B77%20models%7D%24%20across%20%24%5Cemph%7B11%20families%7D%24%2C%20and%20%24%5Cemph%7B9%20programming%20languages%7D%24%2C%20including%20recent%20reasoning%20models.%20Beyond%20scale%2C%20AICD%20Bench%20introduces%20three%20realistic%20detection%20tasks%3A%20%28%24%5Cemph%7Bi%7D%24%29~%24%5Cemph%7BRobust%20Binary%20Classification%7D%24%20under%20distribution%20shifts%20in%20language%20and%20domain%2C%20%28%24%5Cemph%7Bii%7D%24%29~%24%5Cemph%7BModel%20Family%20Attribution%7D%24%2C%20grouping%20generators%20by%20architectural%20lineage%2C%20and%20%28%24%5Cemph%7Biii%7D%24%29~%24%5Cemph%7BFine-Grained%20Human-Machine%20Classification%7D%24%20across%20human%2C%20machine%2C%20hybrid%2C%20and%20adversarial%20code.%20Extensive%20evaluation%20on%20neural%20and%20classical%20detectors%20shows%20that%20performance%20remains%20far%20below%20practical%20usability%2C%20particularly%20under%20distribution%20shift%20and%20for%20hybrid%20or%20adversarial%20code.%20We%20release%20AICD%20Bench%20as%20a%20%24%5Cemph%7Bunified%2C%20challenging%20evaluation%20suite%7D%24%20to%20drive%20the%20next%20generation%20of%20robust%20approaches%20for%20AI-generated%20code%20detection.%20The%20data%20and%20the%20code%20are%20available%20at%20https%3A//huggingface.co/AICD-bench%7D.&entry.1838667208=http%3A//arxiv.org/abs/2602.02079v1&entry.124074799=Read"},
{"title": "Spectral Superposition: A Theory of Feature Geometry", "author": "Georgi Ivanov and Narmeen Oozeer and Shivam Raval and Tasana Pejovic and Shriyash Upadhyay and Amir Abdullah", "abstract": "Neural networks represent more features than they have dimensions via superposition, forcing features to share representational space. Current methods decompose activations into sparse linear features but discard geometric structure. We develop a theory for studying the geometric structre of features by analyzing the spectra (eigenvalues, eigenspaces, etc.) of weight derived matrices. In particular, we introduce the frame operator $F = WW^\\top$, which gives us a spectral measure that describes how each feature allocates norm across eigenspaces. While previous tools could describe the pairwise interactions between features, spectral methods capture the global geometry (``how do all features interact?''). In toy models of superposition, we use this theory to prove that capacity saturation forces spectral localization: features collapse onto single eigenspaces, organize into tight frames, and admit discrete classification via association schemes, classifying all geometries from prior work (simplices, polygons, antiprisms). The spectral measure formalism applies to arbitrary weight matrices, enabling diagnosis of feature localization beyond toy settings. These results point toward a broader program: applying operator theory to interpretability.", "link": "http://arxiv.org/abs/2602.02224v1", "date": "2026-02-02", "relevancy": 2.4737, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4985}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4941}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Superposition%3A%20A%20Theory%20of%20Feature%20Geometry&body=Title%3A%20Spectral%20Superposition%3A%20A%20Theory%20of%20Feature%20Geometry%0AAuthor%3A%20Georgi%20Ivanov%20and%20Narmeen%20Oozeer%20and%20Shivam%20Raval%20and%20Tasana%20Pejovic%20and%20Shriyash%20Upadhyay%20and%20Amir%20Abdullah%0AAbstract%3A%20Neural%20networks%20represent%20more%20features%20than%20they%20have%20dimensions%20via%20superposition%2C%20forcing%20features%20to%20share%20representational%20space.%20Current%20methods%20decompose%20activations%20into%20sparse%20linear%20features%20but%20discard%20geometric%20structure.%20We%20develop%20a%20theory%20for%20studying%20the%20geometric%20structre%20of%20features%20by%20analyzing%20the%20spectra%20%28eigenvalues%2C%20eigenspaces%2C%20etc.%29%20of%20weight%20derived%20matrices.%20In%20particular%2C%20we%20introduce%20the%20frame%20operator%20%24F%20%3D%20WW%5E%5Ctop%24%2C%20which%20gives%20us%20a%20spectral%20measure%20that%20describes%20how%20each%20feature%20allocates%20norm%20across%20eigenspaces.%20While%20previous%20tools%20could%20describe%20the%20pairwise%20interactions%20between%20features%2C%20spectral%20methods%20capture%20the%20global%20geometry%20%28%60%60how%20do%20all%20features%20interact%3F%27%27%29.%20In%20toy%20models%20of%20superposition%2C%20we%20use%20this%20theory%20to%20prove%20that%20capacity%20saturation%20forces%20spectral%20localization%3A%20features%20collapse%20onto%20single%20eigenspaces%2C%20organize%20into%20tight%20frames%2C%20and%20admit%20discrete%20classification%20via%20association%20schemes%2C%20classifying%20all%20geometries%20from%20prior%20work%20%28simplices%2C%20polygons%2C%20antiprisms%29.%20The%20spectral%20measure%20formalism%20applies%20to%20arbitrary%20weight%20matrices%2C%20enabling%20diagnosis%20of%20feature%20localization%20beyond%20toy%20settings.%20These%20results%20point%20toward%20a%20broader%20program%3A%20applying%20operator%20theory%20to%20interpretability.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Superposition%253A%2520A%2520Theory%2520of%2520Feature%2520Geometry%26entry.906535625%3DGeorgi%2520Ivanov%2520and%2520Narmeen%2520Oozeer%2520and%2520Shivam%2520Raval%2520and%2520Tasana%2520Pejovic%2520and%2520Shriyash%2520Upadhyay%2520and%2520Amir%2520Abdullah%26entry.1292438233%3DNeural%2520networks%2520represent%2520more%2520features%2520than%2520they%2520have%2520dimensions%2520via%2520superposition%252C%2520forcing%2520features%2520to%2520share%2520representational%2520space.%2520Current%2520methods%2520decompose%2520activations%2520into%2520sparse%2520linear%2520features%2520but%2520discard%2520geometric%2520structure.%2520We%2520develop%2520a%2520theory%2520for%2520studying%2520the%2520geometric%2520structre%2520of%2520features%2520by%2520analyzing%2520the%2520spectra%2520%2528eigenvalues%252C%2520eigenspaces%252C%2520etc.%2529%2520of%2520weight%2520derived%2520matrices.%2520In%2520particular%252C%2520we%2520introduce%2520the%2520frame%2520operator%2520%2524F%2520%253D%2520WW%255E%255Ctop%2524%252C%2520which%2520gives%2520us%2520a%2520spectral%2520measure%2520that%2520describes%2520how%2520each%2520feature%2520allocates%2520norm%2520across%2520eigenspaces.%2520While%2520previous%2520tools%2520could%2520describe%2520the%2520pairwise%2520interactions%2520between%2520features%252C%2520spectral%2520methods%2520capture%2520the%2520global%2520geometry%2520%2528%2560%2560how%2520do%2520all%2520features%2520interact%253F%2527%2527%2529.%2520In%2520toy%2520models%2520of%2520superposition%252C%2520we%2520use%2520this%2520theory%2520to%2520prove%2520that%2520capacity%2520saturation%2520forces%2520spectral%2520localization%253A%2520features%2520collapse%2520onto%2520single%2520eigenspaces%252C%2520organize%2520into%2520tight%2520frames%252C%2520and%2520admit%2520discrete%2520classification%2520via%2520association%2520schemes%252C%2520classifying%2520all%2520geometries%2520from%2520prior%2520work%2520%2528simplices%252C%2520polygons%252C%2520antiprisms%2529.%2520The%2520spectral%2520measure%2520formalism%2520applies%2520to%2520arbitrary%2520weight%2520matrices%252C%2520enabling%2520diagnosis%2520of%2520feature%2520localization%2520beyond%2520toy%2520settings.%2520These%2520results%2520point%2520toward%2520a%2520broader%2520program%253A%2520applying%2520operator%2520theory%2520to%2520interpretability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Superposition%3A%20A%20Theory%20of%20Feature%20Geometry&entry.906535625=Georgi%20Ivanov%20and%20Narmeen%20Oozeer%20and%20Shivam%20Raval%20and%20Tasana%20Pejovic%20and%20Shriyash%20Upadhyay%20and%20Amir%20Abdullah&entry.1292438233=Neural%20networks%20represent%20more%20features%20than%20they%20have%20dimensions%20via%20superposition%2C%20forcing%20features%20to%20share%20representational%20space.%20Current%20methods%20decompose%20activations%20into%20sparse%20linear%20features%20but%20discard%20geometric%20structure.%20We%20develop%20a%20theory%20for%20studying%20the%20geometric%20structre%20of%20features%20by%20analyzing%20the%20spectra%20%28eigenvalues%2C%20eigenspaces%2C%20etc.%29%20of%20weight%20derived%20matrices.%20In%20particular%2C%20we%20introduce%20the%20frame%20operator%20%24F%20%3D%20WW%5E%5Ctop%24%2C%20which%20gives%20us%20a%20spectral%20measure%20that%20describes%20how%20each%20feature%20allocates%20norm%20across%20eigenspaces.%20While%20previous%20tools%20could%20describe%20the%20pairwise%20interactions%20between%20features%2C%20spectral%20methods%20capture%20the%20global%20geometry%20%28%60%60how%20do%20all%20features%20interact%3F%27%27%29.%20In%20toy%20models%20of%20superposition%2C%20we%20use%20this%20theory%20to%20prove%20that%20capacity%20saturation%20forces%20spectral%20localization%3A%20features%20collapse%20onto%20single%20eigenspaces%2C%20organize%20into%20tight%20frames%2C%20and%20admit%20discrete%20classification%20via%20association%20schemes%2C%20classifying%20all%20geometries%20from%20prior%20work%20%28simplices%2C%20polygons%2C%20antiprisms%29.%20The%20spectral%20measure%20formalism%20applies%20to%20arbitrary%20weight%20matrices%2C%20enabling%20diagnosis%20of%20feature%20localization%20beyond%20toy%20settings.%20These%20results%20point%20toward%20a%20broader%20program%3A%20applying%20operator%20theory%20to%20interpretability.&entry.1838667208=http%3A//arxiv.org/abs/2602.02224v1&entry.124074799=Read"},
{"title": "Large Language Model and Formal Concept Analysis: a comparative study for Topic Modeling", "author": "Fabrice Boissier and Monica Sen and Irina Rychkova", "abstract": "Topic modeling is a research field finding increasing applications: historically from document retrieving, to sentiment analysis and text summarization. Large Language Models (LLM) are currently a major trend in text processing, but few works study their usefulness for this task. Formal Concept Analysis (FCA) has recently been presented as a candidate for topic modeling, but no real applied case study has been conducted. In this work, we compare LLM and FCA to better understand their strengths and weakneses in the topic modeling field. FCA is evaluated through the CREA pipeline used in past experiments on topic modeling and visualization, whereas GPT-5 is used for the LLM. A strategy based on three prompts is applied with GPT-5 in a zero-shot setup: topic generation from document batches, merging of batch results into final topics, and topic labeling. A first experiment reuses the teaching materials previously used to evaluate CREA, while a second experiment analyzes 40 research articles in information systems to compare the extracted topics with the underling subfields.", "link": "http://arxiv.org/abs/2602.01933v1", "date": "2026-02-02", "relevancy": 2.473, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.512}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model%20and%20Formal%20Concept%20Analysis%3A%20a%20comparative%20study%20for%20Topic%20Modeling&body=Title%3A%20Large%20Language%20Model%20and%20Formal%20Concept%20Analysis%3A%20a%20comparative%20study%20for%20Topic%20Modeling%0AAuthor%3A%20Fabrice%20Boissier%20and%20Monica%20Sen%20and%20Irina%20Rychkova%0AAbstract%3A%20Topic%20modeling%20is%20a%20research%20field%20finding%20increasing%20applications%3A%20historically%20from%20document%20retrieving%2C%20to%20sentiment%20analysis%20and%20text%20summarization.%20Large%20Language%20Models%20%28LLM%29%20are%20currently%20a%20major%20trend%20in%20text%20processing%2C%20but%20few%20works%20study%20their%20usefulness%20for%20this%20task.%20Formal%20Concept%20Analysis%20%28FCA%29%20has%20recently%20been%20presented%20as%20a%20candidate%20for%20topic%20modeling%2C%20but%20no%20real%20applied%20case%20study%20has%20been%20conducted.%20In%20this%20work%2C%20we%20compare%20LLM%20and%20FCA%20to%20better%20understand%20their%20strengths%20and%20weakneses%20in%20the%20topic%20modeling%20field.%20FCA%20is%20evaluated%20through%20the%20CREA%20pipeline%20used%20in%20past%20experiments%20on%20topic%20modeling%20and%20visualization%2C%20whereas%20GPT-5%20is%20used%20for%20the%20LLM.%20A%20strategy%20based%20on%20three%20prompts%20is%20applied%20with%20GPT-5%20in%20a%20zero-shot%20setup%3A%20topic%20generation%20from%20document%20batches%2C%20merging%20of%20batch%20results%20into%20final%20topics%2C%20and%20topic%20labeling.%20A%20first%20experiment%20reuses%20the%20teaching%20materials%20previously%20used%20to%20evaluate%20CREA%2C%20while%20a%20second%20experiment%20analyzes%2040%20research%20articles%20in%20information%20systems%20to%20compare%20the%20extracted%20topics%20with%20the%20underling%20subfields.%0ALink%3A%20http%3A//arxiv.org/abs/2602.01933v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model%2520and%2520Formal%2520Concept%2520Analysis%253A%2520a%2520comparative%2520study%2520for%2520Topic%2520Modeling%26entry.906535625%3DFabrice%2520Boissier%2520and%2520Monica%2520Sen%2520and%2520Irina%2520Rychkova%26entry.1292438233%3DTopic%2520modeling%2520is%2520a%2520research%2520field%2520finding%2520increasing%2520applications%253A%2520historically%2520from%2520document%2520retrieving%252C%2520to%2520sentiment%2520analysis%2520and%2520text%2520summarization.%2520Large%2520Language%2520Models%2520%2528LLM%2529%2520are%2520currently%2520a%2520major%2520trend%2520in%2520text%2520processing%252C%2520but%2520few%2520works%2520study%2520their%2520usefulness%2520for%2520this%2520task.%2520Formal%2520Concept%2520Analysis%2520%2528FCA%2529%2520has%2520recently%2520been%2520presented%2520as%2520a%2520candidate%2520for%2520topic%2520modeling%252C%2520but%2520no%2520real%2520applied%2520case%2520study%2520has%2520been%2520conducted.%2520In%2520this%2520work%252C%2520we%2520compare%2520LLM%2520and%2520FCA%2520to%2520better%2520understand%2520their%2520strengths%2520and%2520weakneses%2520in%2520the%2520topic%2520modeling%2520field.%2520FCA%2520is%2520evaluated%2520through%2520the%2520CREA%2520pipeline%2520used%2520in%2520past%2520experiments%2520on%2520topic%2520modeling%2520and%2520visualization%252C%2520whereas%2520GPT-5%2520is%2520used%2520for%2520the%2520LLM.%2520A%2520strategy%2520based%2520on%2520three%2520prompts%2520is%2520applied%2520with%2520GPT-5%2520in%2520a%2520zero-shot%2520setup%253A%2520topic%2520generation%2520from%2520document%2520batches%252C%2520merging%2520of%2520batch%2520results%2520into%2520final%2520topics%252C%2520and%2520topic%2520labeling.%2520A%2520first%2520experiment%2520reuses%2520the%2520teaching%2520materials%2520previously%2520used%2520to%2520evaluate%2520CREA%252C%2520while%2520a%2520second%2520experiment%2520analyzes%252040%2520research%2520articles%2520in%2520information%2520systems%2520to%2520compare%2520the%2520extracted%2520topics%2520with%2520the%2520underling%2520subfields.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.01933v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model%20and%20Formal%20Concept%20Analysis%3A%20a%20comparative%20study%20for%20Topic%20Modeling&entry.906535625=Fabrice%20Boissier%20and%20Monica%20Sen%20and%20Irina%20Rychkova&entry.1292438233=Topic%20modeling%20is%20a%20research%20field%20finding%20increasing%20applications%3A%20historically%20from%20document%20retrieving%2C%20to%20sentiment%20analysis%20and%20text%20summarization.%20Large%20Language%20Models%20%28LLM%29%20are%20currently%20a%20major%20trend%20in%20text%20processing%2C%20but%20few%20works%20study%20their%20usefulness%20for%20this%20task.%20Formal%20Concept%20Analysis%20%28FCA%29%20has%20recently%20been%20presented%20as%20a%20candidate%20for%20topic%20modeling%2C%20but%20no%20real%20applied%20case%20study%20has%20been%20conducted.%20In%20this%20work%2C%20we%20compare%20LLM%20and%20FCA%20to%20better%20understand%20their%20strengths%20and%20weakneses%20in%20the%20topic%20modeling%20field.%20FCA%20is%20evaluated%20through%20the%20CREA%20pipeline%20used%20in%20past%20experiments%20on%20topic%20modeling%20and%20visualization%2C%20whereas%20GPT-5%20is%20used%20for%20the%20LLM.%20A%20strategy%20based%20on%20three%20prompts%20is%20applied%20with%20GPT-5%20in%20a%20zero-shot%20setup%3A%20topic%20generation%20from%20document%20batches%2C%20merging%20of%20batch%20results%20into%20final%20topics%2C%20and%20topic%20labeling.%20A%20first%20experiment%20reuses%20the%20teaching%20materials%20previously%20used%20to%20evaluate%20CREA%2C%20while%20a%20second%20experiment%20analyzes%2040%20research%20articles%20in%20information%20systems%20to%20compare%20the%20extracted%20topics%20with%20the%20underling%20subfields.&entry.1838667208=http%3A//arxiv.org/abs/2602.01933v1&entry.124074799=Read"},
{"title": "Grounding Generated Videos in Feasible Plans via World Models", "author": "Christos Ziakas and Amir Bar and Alessandra Russo", "abstract": "Large-scale video generative models have shown emerging capabilities as zero-shot visual planners, yet video-generated plans often violate temporal consistency and physical constraints, leading to failures when mapped to executable actions. To address this, we propose Grounding Video Plans with World Models (GVP-WM), a planning method that grounds video-generated plans into feasible action sequences using a learned action-conditioned world model. At test-time, GVP-WM first generates a video plan from initial and goal observations, then projects the video guidance onto the manifold of dynamically feasible latent trajectories via video-guided latent collocation. In particular, we formulate grounding as a goal-conditioned latent-space trajectory optimization problem that jointly optimizes latent states and actions under world-model dynamics, while preserving semantic alignment with the video-generated plan. Empirically, GVP-WM recovers feasible long-horizon plans from zero-shot image-to-video-generated and motion-blurred videos that violate physical constraints, across navigation and manipulation simulation tasks.", "link": "http://arxiv.org/abs/2602.01960v1", "date": "2026-02-02", "relevancy": 2.4575, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6359}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6128}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounding%20Generated%20Videos%20in%20Feasible%20Plans%20via%20World%20Models&body=Title%3A%20Grounding%20Generated%20Videos%20in%20Feasible%20Plans%20via%20World%20Models%0AAuthor%3A%20Christos%20Ziakas%20and%20Amir%20Bar%20and%20Alessandra%20Russo%0AAbstract%3A%20Large-scale%20video%20generative%20models%20have%20shown%20emerging%20capabilities%20as%20zero-shot%20visual%20planners%2C%20yet%20video-generated%20plans%20often%20violate%20temporal%20consistency%20and%20physical%20constraints%2C%20leading%20to%20failures%20when%20mapped%20to%20executable%20actions.%20To%20address%20this%2C%20we%20propose%20Grounding%20Video%20Plans%20with%20World%20Models%20%28GVP-WM%29%2C%20a%20planning%20method%20that%20grounds%20video-generated%20plans%20into%20feasible%20action%20sequences%20using%20a%20learned%20action-conditioned%20world%20model.%20At%20test-time%2C%20GVP-WM%20first%20generates%20a%20video%20plan%20from%20initial%20and%20goal%20observations%2C%20then%20projects%20the%20video%20guidance%20onto%20the%20manifold%20of%20dynamically%20feasible%20latent%20trajectories%20via%20video-guided%20latent%20collocation.%20In%20particular%2C%20we%20formulate%20grounding%20as%20a%20goal-conditioned%20latent-space%20trajectory%20optimization%20problem%20that%20jointly%20optimizes%20latent%20states%20and%20actions%20under%20world-model%20dynamics%2C%20while%20preserving%20semantic%20alignment%20with%20the%20video-generated%20plan.%20Empirically%2C%20GVP-WM%20recovers%20feasible%20long-horizon%20plans%20from%20zero-shot%20image-to-video-generated%20and%20motion-blurred%20videos%20that%20violate%20physical%20constraints%2C%20across%20navigation%20and%20manipulation%20simulation%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.01960v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounding%2520Generated%2520Videos%2520in%2520Feasible%2520Plans%2520via%2520World%2520Models%26entry.906535625%3DChristos%2520Ziakas%2520and%2520Amir%2520Bar%2520and%2520Alessandra%2520Russo%26entry.1292438233%3DLarge-scale%2520video%2520generative%2520models%2520have%2520shown%2520emerging%2520capabilities%2520as%2520zero-shot%2520visual%2520planners%252C%2520yet%2520video-generated%2520plans%2520often%2520violate%2520temporal%2520consistency%2520and%2520physical%2520constraints%252C%2520leading%2520to%2520failures%2520when%2520mapped%2520to%2520executable%2520actions.%2520To%2520address%2520this%252C%2520we%2520propose%2520Grounding%2520Video%2520Plans%2520with%2520World%2520Models%2520%2528GVP-WM%2529%252C%2520a%2520planning%2520method%2520that%2520grounds%2520video-generated%2520plans%2520into%2520feasible%2520action%2520sequences%2520using%2520a%2520learned%2520action-conditioned%2520world%2520model.%2520At%2520test-time%252C%2520GVP-WM%2520first%2520generates%2520a%2520video%2520plan%2520from%2520initial%2520and%2520goal%2520observations%252C%2520then%2520projects%2520the%2520video%2520guidance%2520onto%2520the%2520manifold%2520of%2520dynamically%2520feasible%2520latent%2520trajectories%2520via%2520video-guided%2520latent%2520collocation.%2520In%2520particular%252C%2520we%2520formulate%2520grounding%2520as%2520a%2520goal-conditioned%2520latent-space%2520trajectory%2520optimization%2520problem%2520that%2520jointly%2520optimizes%2520latent%2520states%2520and%2520actions%2520under%2520world-model%2520dynamics%252C%2520while%2520preserving%2520semantic%2520alignment%2520with%2520the%2520video-generated%2520plan.%2520Empirically%252C%2520GVP-WM%2520recovers%2520feasible%2520long-horizon%2520plans%2520from%2520zero-shot%2520image-to-video-generated%2520and%2520motion-blurred%2520videos%2520that%2520violate%2520physical%2520constraints%252C%2520across%2520navigation%2520and%2520manipulation%2520simulation%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.01960v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounding%20Generated%20Videos%20in%20Feasible%20Plans%20via%20World%20Models&entry.906535625=Christos%20Ziakas%20and%20Amir%20Bar%20and%20Alessandra%20Russo&entry.1292438233=Large-scale%20video%20generative%20models%20have%20shown%20emerging%20capabilities%20as%20zero-shot%20visual%20planners%2C%20yet%20video-generated%20plans%20often%20violate%20temporal%20consistency%20and%20physical%20constraints%2C%20leading%20to%20failures%20when%20mapped%20to%20executable%20actions.%20To%20address%20this%2C%20we%20propose%20Grounding%20Video%20Plans%20with%20World%20Models%20%28GVP-WM%29%2C%20a%20planning%20method%20that%20grounds%20video-generated%20plans%20into%20feasible%20action%20sequences%20using%20a%20learned%20action-conditioned%20world%20model.%20At%20test-time%2C%20GVP-WM%20first%20generates%20a%20video%20plan%20from%20initial%20and%20goal%20observations%2C%20then%20projects%20the%20video%20guidance%20onto%20the%20manifold%20of%20dynamically%20feasible%20latent%20trajectories%20via%20video-guided%20latent%20collocation.%20In%20particular%2C%20we%20formulate%20grounding%20as%20a%20goal-conditioned%20latent-space%20trajectory%20optimization%20problem%20that%20jointly%20optimizes%20latent%20states%20and%20actions%20under%20world-model%20dynamics%2C%20while%20preserving%20semantic%20alignment%20with%20the%20video-generated%20plan.%20Empirically%2C%20GVP-WM%20recovers%20feasible%20long-horizon%20plans%20from%20zero-shot%20image-to-video-generated%20and%20motion-blurred%20videos%20that%20violate%20physical%20constraints%2C%20across%20navigation%20and%20manipulation%20simulation%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2602.01960v1&entry.124074799=Read"},
{"title": "Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory", "author": "Ruiqi Wu and Xuanhua He and Meng Cheng and Tianyu Yang and Yong Zhang and Zhuoliang Kang and Xunliang Cai and Xiaoming Wei and Chunle Guo and Chongyi Li and Ming-Ming Cheng", "abstract": "We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone, HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic. This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities. Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency.", "link": "http://arxiv.org/abs/2602.02393v1", "date": "2026-02-02", "relevancy": 2.455, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.642}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6238}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Infinite-World%3A%20Scaling%20Interactive%20World%20Models%20to%201000-Frame%20Horizons%20via%20Pose-Free%20Hierarchical%20Memory&body=Title%3A%20Infinite-World%3A%20Scaling%20Interactive%20World%20Models%20to%201000-Frame%20Horizons%20via%20Pose-Free%20Hierarchical%20Memory%0AAuthor%3A%20Ruiqi%20Wu%20and%20Xuanhua%20He%20and%20Meng%20Cheng%20and%20Tianyu%20Yang%20and%20Yong%20Zhang%20and%20Zhuoliang%20Kang%20and%20Xunliang%20Cai%20and%20Xiaoming%20Wei%20and%20Chunle%20Guo%20and%20Chongyi%20Li%20and%20Ming-Ming%20Cheng%0AAbstract%3A%20We%20propose%20Infinite-World%2C%20a%20robust%20interactive%20world%20model%20capable%20of%20maintaining%20coherent%20visual%20memory%20over%201000%2B%20frames%20in%20complex%20real-world%20environments.%20While%20existing%20world%20models%20can%20be%20efficiently%20optimized%20on%20synthetic%20data%20with%20perfect%20ground-truth%2C%20they%20lack%20an%20effective%20training%20paradigm%20for%20real-world%20videos%20due%20to%20noisy%20pose%20estimations%20and%20the%20scarcity%20of%20viewpoint%20revisits.%20To%20bridge%20this%20gap%2C%20we%20first%20introduce%20a%20Hierarchical%20Pose-free%20Memory%20Compressor%20%28HPMC%29%20that%20recursively%20distills%20historical%20latents%20into%20a%20fixed-budget%20representation.%20By%20jointly%20optimizing%20the%20compressor%20with%20the%20generative%20backbone%2C%20HPMC%20enables%20the%20model%20to%20autonomously%20anchor%20generations%20in%20the%20distant%20past%20with%20bounded%20computational%20cost%2C%20eliminating%20the%20need%20for%20explicit%20geometric%20priors.%20Second%2C%20we%20propose%20an%20Uncertainty-aware%20Action%20Labeling%20module%20that%20discretizes%20continuous%20motion%20into%20a%20tri-state%20logic.%20This%20strategy%20maximizes%20the%20utilization%20of%20raw%20video%20data%20while%20shielding%20the%20deterministic%20action%20space%20from%20being%20corrupted%20by%20noisy%20trajectories%2C%20ensuring%20robust%20action-response%20learning.%20Furthermore%2C%20guided%20by%20insights%20from%20a%20pilot%20toy%20study%2C%20we%20employ%20a%20Revisit-Dense%20Finetuning%20Strategy%20using%20a%20compact%2C%2030-minute%20dataset%20to%20efficiently%20activate%20the%20model%27s%20long-range%20loop-closure%20capabilities.%20Extensive%20experiments%2C%20including%20objective%20metrics%20and%20user%20studies%2C%20demonstrate%20that%20Infinite-World%20achieves%20superior%20performance%20in%20visual%20quality%2C%20action%20controllability%2C%20and%20spatial%20consistency.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02393v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfinite-World%253A%2520Scaling%2520Interactive%2520World%2520Models%2520to%25201000-Frame%2520Horizons%2520via%2520Pose-Free%2520Hierarchical%2520Memory%26entry.906535625%3DRuiqi%2520Wu%2520and%2520Xuanhua%2520He%2520and%2520Meng%2520Cheng%2520and%2520Tianyu%2520Yang%2520and%2520Yong%2520Zhang%2520and%2520Zhuoliang%2520Kang%2520and%2520Xunliang%2520Cai%2520and%2520Xiaoming%2520Wei%2520and%2520Chunle%2520Guo%2520and%2520Chongyi%2520Li%2520and%2520Ming-Ming%2520Cheng%26entry.1292438233%3DWe%2520propose%2520Infinite-World%252C%2520a%2520robust%2520interactive%2520world%2520model%2520capable%2520of%2520maintaining%2520coherent%2520visual%2520memory%2520over%25201000%252B%2520frames%2520in%2520complex%2520real-world%2520environments.%2520While%2520existing%2520world%2520models%2520can%2520be%2520efficiently%2520optimized%2520on%2520synthetic%2520data%2520with%2520perfect%2520ground-truth%252C%2520they%2520lack%2520an%2520effective%2520training%2520paradigm%2520for%2520real-world%2520videos%2520due%2520to%2520noisy%2520pose%2520estimations%2520and%2520the%2520scarcity%2520of%2520viewpoint%2520revisits.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520first%2520introduce%2520a%2520Hierarchical%2520Pose-free%2520Memory%2520Compressor%2520%2528HPMC%2529%2520that%2520recursively%2520distills%2520historical%2520latents%2520into%2520a%2520fixed-budget%2520representation.%2520By%2520jointly%2520optimizing%2520the%2520compressor%2520with%2520the%2520generative%2520backbone%252C%2520HPMC%2520enables%2520the%2520model%2520to%2520autonomously%2520anchor%2520generations%2520in%2520the%2520distant%2520past%2520with%2520bounded%2520computational%2520cost%252C%2520eliminating%2520the%2520need%2520for%2520explicit%2520geometric%2520priors.%2520Second%252C%2520we%2520propose%2520an%2520Uncertainty-aware%2520Action%2520Labeling%2520module%2520that%2520discretizes%2520continuous%2520motion%2520into%2520a%2520tri-state%2520logic.%2520This%2520strategy%2520maximizes%2520the%2520utilization%2520of%2520raw%2520video%2520data%2520while%2520shielding%2520the%2520deterministic%2520action%2520space%2520from%2520being%2520corrupted%2520by%2520noisy%2520trajectories%252C%2520ensuring%2520robust%2520action-response%2520learning.%2520Furthermore%252C%2520guided%2520by%2520insights%2520from%2520a%2520pilot%2520toy%2520study%252C%2520we%2520employ%2520a%2520Revisit-Dense%2520Finetuning%2520Strategy%2520using%2520a%2520compact%252C%252030-minute%2520dataset%2520to%2520efficiently%2520activate%2520the%2520model%2527s%2520long-range%2520loop-closure%2520capabilities.%2520Extensive%2520experiments%252C%2520including%2520objective%2520metrics%2520and%2520user%2520studies%252C%2520demonstrate%2520that%2520Infinite-World%2520achieves%2520superior%2520performance%2520in%2520visual%2520quality%252C%2520action%2520controllability%252C%2520and%2520spatial%2520consistency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02393v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infinite-World%3A%20Scaling%20Interactive%20World%20Models%20to%201000-Frame%20Horizons%20via%20Pose-Free%20Hierarchical%20Memory&entry.906535625=Ruiqi%20Wu%20and%20Xuanhua%20He%20and%20Meng%20Cheng%20and%20Tianyu%20Yang%20and%20Yong%20Zhang%20and%20Zhuoliang%20Kang%20and%20Xunliang%20Cai%20and%20Xiaoming%20Wei%20and%20Chunle%20Guo%20and%20Chongyi%20Li%20and%20Ming-Ming%20Cheng&entry.1292438233=We%20propose%20Infinite-World%2C%20a%20robust%20interactive%20world%20model%20capable%20of%20maintaining%20coherent%20visual%20memory%20over%201000%2B%20frames%20in%20complex%20real-world%20environments.%20While%20existing%20world%20models%20can%20be%20efficiently%20optimized%20on%20synthetic%20data%20with%20perfect%20ground-truth%2C%20they%20lack%20an%20effective%20training%20paradigm%20for%20real-world%20videos%20due%20to%20noisy%20pose%20estimations%20and%20the%20scarcity%20of%20viewpoint%20revisits.%20To%20bridge%20this%20gap%2C%20we%20first%20introduce%20a%20Hierarchical%20Pose-free%20Memory%20Compressor%20%28HPMC%29%20that%20recursively%20distills%20historical%20latents%20into%20a%20fixed-budget%20representation.%20By%20jointly%20optimizing%20the%20compressor%20with%20the%20generative%20backbone%2C%20HPMC%20enables%20the%20model%20to%20autonomously%20anchor%20generations%20in%20the%20distant%20past%20with%20bounded%20computational%20cost%2C%20eliminating%20the%20need%20for%20explicit%20geometric%20priors.%20Second%2C%20we%20propose%20an%20Uncertainty-aware%20Action%20Labeling%20module%20that%20discretizes%20continuous%20motion%20into%20a%20tri-state%20logic.%20This%20strategy%20maximizes%20the%20utilization%20of%20raw%20video%20data%20while%20shielding%20the%20deterministic%20action%20space%20from%20being%20corrupted%20by%20noisy%20trajectories%2C%20ensuring%20robust%20action-response%20learning.%20Furthermore%2C%20guided%20by%20insights%20from%20a%20pilot%20toy%20study%2C%20we%20employ%20a%20Revisit-Dense%20Finetuning%20Strategy%20using%20a%20compact%2C%2030-minute%20dataset%20to%20efficiently%20activate%20the%20model%27s%20long-range%20loop-closure%20capabilities.%20Extensive%20experiments%2C%20including%20objective%20metrics%20and%20user%20studies%2C%20demonstrate%20that%20Infinite-World%20achieves%20superior%20performance%20in%20visual%20quality%2C%20action%20controllability%2C%20and%20spatial%20consistency.&entry.1838667208=http%3A//arxiv.org/abs/2602.02393v1&entry.124074799=Read"},
{"title": "Extending RLVR to Open-Ended Tasks via Verifiable Multiple-Choice Reformulation", "author": "Mengyu Zhang and Siyu Ding and Weichong Yin and Yu Sun", "abstract": "Reinforcement Learning with Verifiable Rewards(RLVR) has demonstrated great potential in enhancing the reasoning capabilities of large language models (LLMs). However, its success has thus far been largely confined to the mathematical and programming domains with clear and automatically checkable outcomes. Reinforcement learning on open-ended tasks (e.g., creative writing and subjective Q&A) continues to rely on reward models due to the absence of verifiable solutions. This raises a key question: how can we extend RLVR to strengthen reasoning in open-ended tasks regardless of the absence of the unambiguous ground truth? To overcome this challenge, we introduce Verifiable Multiple-Choice Reformulation for Reinforcement Learning from Verifiable Rewards (VMR-RLVR), a novel training strategy that restructures open-ended data into verifiable multiple-choice formats, enabling effective training even in the absence of explicit ground truth. Experimental results on multiple benchmarks validate the effectiveness of our method in improving LLM performance on open-ended tasks. Notably, across seven open-ended benchmarks, our VMR-RLVR training delivers an average gain of 3.29 points over the RL with reward model.", "link": "http://arxiv.org/abs/2511.02463v2", "date": "2026-02-02", "relevancy": 2.4489, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5037}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4828}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extending%20RLVR%20to%20Open-Ended%20Tasks%20via%20Verifiable%20Multiple-Choice%20Reformulation&body=Title%3A%20Extending%20RLVR%20to%20Open-Ended%20Tasks%20via%20Verifiable%20Multiple-Choice%20Reformulation%0AAuthor%3A%20Mengyu%20Zhang%20and%20Siyu%20Ding%20and%20Weichong%20Yin%20and%20Yu%20Sun%0AAbstract%3A%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%28RLVR%29%20has%20demonstrated%20great%20potential%20in%20enhancing%20the%20reasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29.%20However%2C%20its%20success%20has%20thus%20far%20been%20largely%20confined%20to%20the%20mathematical%20and%20programming%20domains%20with%20clear%20and%20automatically%20checkable%20outcomes.%20Reinforcement%20learning%20on%20open-ended%20tasks%20%28e.g.%2C%20creative%20writing%20and%20subjective%20Q%26A%29%20continues%20to%20rely%20on%20reward%20models%20due%20to%20the%20absence%20of%20verifiable%20solutions.%20This%20raises%20a%20key%20question%3A%20how%20can%20we%20extend%20RLVR%20to%20strengthen%20reasoning%20in%20open-ended%20tasks%20regardless%20of%20the%20absence%20of%20the%20unambiguous%20ground%20truth%3F%20To%20overcome%20this%20challenge%2C%20we%20introduce%20Verifiable%20Multiple-Choice%20Reformulation%20for%20Reinforcement%20Learning%20from%20Verifiable%20Rewards%20%28VMR-RLVR%29%2C%20a%20novel%20training%20strategy%20that%20restructures%20open-ended%20data%20into%20verifiable%20multiple-choice%20formats%2C%20enabling%20effective%20training%20even%20in%20the%20absence%20of%20explicit%20ground%20truth.%20Experimental%20results%20on%20multiple%20benchmarks%20validate%20the%20effectiveness%20of%20our%20method%20in%20improving%20LLM%20performance%20on%20open-ended%20tasks.%20Notably%2C%20across%20seven%20open-ended%20benchmarks%2C%20our%20VMR-RLVR%20training%20delivers%20an%20average%20gain%20of%203.29%20points%20over%20the%20RL%20with%20reward%20model.%0ALink%3A%20http%3A//arxiv.org/abs/2511.02463v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtending%2520RLVR%2520to%2520Open-Ended%2520Tasks%2520via%2520Verifiable%2520Multiple-Choice%2520Reformulation%26entry.906535625%3DMengyu%2520Zhang%2520and%2520Siyu%2520Ding%2520and%2520Weichong%2520Yin%2520and%2520Yu%2520Sun%26entry.1292438233%3DReinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2528RLVR%2529%2520has%2520demonstrated%2520great%2520potential%2520in%2520enhancing%2520the%2520reasoning%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520However%252C%2520its%2520success%2520has%2520thus%2520far%2520been%2520largely%2520confined%2520to%2520the%2520mathematical%2520and%2520programming%2520domains%2520with%2520clear%2520and%2520automatically%2520checkable%2520outcomes.%2520Reinforcement%2520learning%2520on%2520open-ended%2520tasks%2520%2528e.g.%252C%2520creative%2520writing%2520and%2520subjective%2520Q%2526A%2529%2520continues%2520to%2520rely%2520on%2520reward%2520models%2520due%2520to%2520the%2520absence%2520of%2520verifiable%2520solutions.%2520This%2520raises%2520a%2520key%2520question%253A%2520how%2520can%2520we%2520extend%2520RLVR%2520to%2520strengthen%2520reasoning%2520in%2520open-ended%2520tasks%2520regardless%2520of%2520the%2520absence%2520of%2520the%2520unambiguous%2520ground%2520truth%253F%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520introduce%2520Verifiable%2520Multiple-Choice%2520Reformulation%2520for%2520Reinforcement%2520Learning%2520from%2520Verifiable%2520Rewards%2520%2528VMR-RLVR%2529%252C%2520a%2520novel%2520training%2520strategy%2520that%2520restructures%2520open-ended%2520data%2520into%2520verifiable%2520multiple-choice%2520formats%252C%2520enabling%2520effective%2520training%2520even%2520in%2520the%2520absence%2520of%2520explicit%2520ground%2520truth.%2520Experimental%2520results%2520on%2520multiple%2520benchmarks%2520validate%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520improving%2520LLM%2520performance%2520on%2520open-ended%2520tasks.%2520Notably%252C%2520across%2520seven%2520open-ended%2520benchmarks%252C%2520our%2520VMR-RLVR%2520training%2520delivers%2520an%2520average%2520gain%2520of%25203.29%2520points%2520over%2520the%2520RL%2520with%2520reward%2520model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.02463v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extending%20RLVR%20to%20Open-Ended%20Tasks%20via%20Verifiable%20Multiple-Choice%20Reformulation&entry.906535625=Mengyu%20Zhang%20and%20Siyu%20Ding%20and%20Weichong%20Yin%20and%20Yu%20Sun&entry.1292438233=Reinforcement%20Learning%20with%20Verifiable%20Rewards%28RLVR%29%20has%20demonstrated%20great%20potential%20in%20enhancing%20the%20reasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29.%20However%2C%20its%20success%20has%20thus%20far%20been%20largely%20confined%20to%20the%20mathematical%20and%20programming%20domains%20with%20clear%20and%20automatically%20checkable%20outcomes.%20Reinforcement%20learning%20on%20open-ended%20tasks%20%28e.g.%2C%20creative%20writing%20and%20subjective%20Q%26A%29%20continues%20to%20rely%20on%20reward%20models%20due%20to%20the%20absence%20of%20verifiable%20solutions.%20This%20raises%20a%20key%20question%3A%20how%20can%20we%20extend%20RLVR%20to%20strengthen%20reasoning%20in%20open-ended%20tasks%20regardless%20of%20the%20absence%20of%20the%20unambiguous%20ground%20truth%3F%20To%20overcome%20this%20challenge%2C%20we%20introduce%20Verifiable%20Multiple-Choice%20Reformulation%20for%20Reinforcement%20Learning%20from%20Verifiable%20Rewards%20%28VMR-RLVR%29%2C%20a%20novel%20training%20strategy%20that%20restructures%20open-ended%20data%20into%20verifiable%20multiple-choice%20formats%2C%20enabling%20effective%20training%20even%20in%20the%20absence%20of%20explicit%20ground%20truth.%20Experimental%20results%20on%20multiple%20benchmarks%20validate%20the%20effectiveness%20of%20our%20method%20in%20improving%20LLM%20performance%20on%20open-ended%20tasks.%20Notably%2C%20across%20seven%20open-ended%20benchmarks%2C%20our%20VMR-RLVR%20training%20delivers%20an%20average%20gain%20of%203.29%20points%20over%20the%20RL%20with%20reward%20model.&entry.1838667208=http%3A//arxiv.org/abs/2511.02463v2&entry.124074799=Read"},
{"title": "Dissecting Outlier Dynamics in LLM NVFP4 Pretraining", "author": "Peijie Dong and Ruibo Fan and Yuechen Tao and Di Mou and Wenhu Hu and Zhenheng Tang and Yinghao Yu and Jiamang Wang and Wenbo Su and Guodong Yang and Liping Zhang and Xiaowen Chu and Baochun Li and Bo Li", "abstract": "Training large language models using 4-bit arithmetic enhances throughput and memory efficiency. Yet, the limited dynamic range of FP4 increases sensitivity to outliers. While NVFP4 mitigates quantization error via hierarchical microscaling, a persistent loss gap remains compared to BF16. This study conducts a longitudinal analysis of outlier dynamics across architecture during NVFP4 pretraining, focusing on where they localize, why they occur, and how they evolve temporally. We find that, compared with Softmax Attention (SA), Linear Attention (LA) reduces per-tensor heavy tails but still exhibits persistent block-level spikes under block quantization. Our analysis attributes outliers to specific architectural components: Softmax in SA, gating in LA, and SwiGLU in FFN, with \"post-QK\" operations exhibiting higher sensitivity to quantization. Notably, outliers evolve from transient spikes early in training to a small set of persistent hot channels (i.e., channels with persistently large magnitudes) in later stages. Based on these findings, we introduce Hot-Channel Patch (HCP), an online compensation mechanism that identifies hot channels and reinjects residuals using hardware-efficient kernels. We then develop CHON, an NVFP4 training recipe integrating HCP with post-QK operation protection. On GLA-1.3B model trained for 60B tokens, CHON reduces the loss gap to BF16 from 0.94% to 0.58% while maintaining downstream accuracy.", "link": "http://arxiv.org/abs/2602.02047v1", "date": "2026-02-02", "relevancy": 2.4443, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4949}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.493}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dissecting%20Outlier%20Dynamics%20in%20LLM%20NVFP4%20Pretraining&body=Title%3A%20Dissecting%20Outlier%20Dynamics%20in%20LLM%20NVFP4%20Pretraining%0AAuthor%3A%20Peijie%20Dong%20and%20Ruibo%20Fan%20and%20Yuechen%20Tao%20and%20Di%20Mou%20and%20Wenhu%20Hu%20and%20Zhenheng%20Tang%20and%20Yinghao%20Yu%20and%20Jiamang%20Wang%20and%20Wenbo%20Su%20and%20Guodong%20Yang%20and%20Liping%20Zhang%20and%20Xiaowen%20Chu%20and%20Baochun%20Li%20and%20Bo%20Li%0AAbstract%3A%20Training%20large%20language%20models%20using%204-bit%20arithmetic%20enhances%20throughput%20and%20memory%20efficiency.%20Yet%2C%20the%20limited%20dynamic%20range%20of%20FP4%20increases%20sensitivity%20to%20outliers.%20While%20NVFP4%20mitigates%20quantization%20error%20via%20hierarchical%20microscaling%2C%20a%20persistent%20loss%20gap%20remains%20compared%20to%20BF16.%20This%20study%20conducts%20a%20longitudinal%20analysis%20of%20outlier%20dynamics%20across%20architecture%20during%20NVFP4%20pretraining%2C%20focusing%20on%20where%20they%20localize%2C%20why%20they%20occur%2C%20and%20how%20they%20evolve%20temporally.%20We%20find%20that%2C%20compared%20with%20Softmax%20Attention%20%28SA%29%2C%20Linear%20Attention%20%28LA%29%20reduces%20per-tensor%20heavy%20tails%20but%20still%20exhibits%20persistent%20block-level%20spikes%20under%20block%20quantization.%20Our%20analysis%20attributes%20outliers%20to%20specific%20architectural%20components%3A%20Softmax%20in%20SA%2C%20gating%20in%20LA%2C%20and%20SwiGLU%20in%20FFN%2C%20with%20%22post-QK%22%20operations%20exhibiting%20higher%20sensitivity%20to%20quantization.%20Notably%2C%20outliers%20evolve%20from%20transient%20spikes%20early%20in%20training%20to%20a%20small%20set%20of%20persistent%20hot%20channels%20%28i.e.%2C%20channels%20with%20persistently%20large%20magnitudes%29%20in%20later%20stages.%20Based%20on%20these%20findings%2C%20we%20introduce%20Hot-Channel%20Patch%20%28HCP%29%2C%20an%20online%20compensation%20mechanism%20that%20identifies%20hot%20channels%20and%20reinjects%20residuals%20using%20hardware-efficient%20kernels.%20We%20then%20develop%20CHON%2C%20an%20NVFP4%20training%20recipe%20integrating%20HCP%20with%20post-QK%20operation%20protection.%20On%20GLA-1.3B%20model%20trained%20for%2060B%20tokens%2C%20CHON%20reduces%20the%20loss%20gap%20to%20BF16%20from%200.94%25%20to%200.58%25%20while%20maintaining%20downstream%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02047v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDissecting%2520Outlier%2520Dynamics%2520in%2520LLM%2520NVFP4%2520Pretraining%26entry.906535625%3DPeijie%2520Dong%2520and%2520Ruibo%2520Fan%2520and%2520Yuechen%2520Tao%2520and%2520Di%2520Mou%2520and%2520Wenhu%2520Hu%2520and%2520Zhenheng%2520Tang%2520and%2520Yinghao%2520Yu%2520and%2520Jiamang%2520Wang%2520and%2520Wenbo%2520Su%2520and%2520Guodong%2520Yang%2520and%2520Liping%2520Zhang%2520and%2520Xiaowen%2520Chu%2520and%2520Baochun%2520Li%2520and%2520Bo%2520Li%26entry.1292438233%3DTraining%2520large%2520language%2520models%2520using%25204-bit%2520arithmetic%2520enhances%2520throughput%2520and%2520memory%2520efficiency.%2520Yet%252C%2520the%2520limited%2520dynamic%2520range%2520of%2520FP4%2520increases%2520sensitivity%2520to%2520outliers.%2520While%2520NVFP4%2520mitigates%2520quantization%2520error%2520via%2520hierarchical%2520microscaling%252C%2520a%2520persistent%2520loss%2520gap%2520remains%2520compared%2520to%2520BF16.%2520This%2520study%2520conducts%2520a%2520longitudinal%2520analysis%2520of%2520outlier%2520dynamics%2520across%2520architecture%2520during%2520NVFP4%2520pretraining%252C%2520focusing%2520on%2520where%2520they%2520localize%252C%2520why%2520they%2520occur%252C%2520and%2520how%2520they%2520evolve%2520temporally.%2520We%2520find%2520that%252C%2520compared%2520with%2520Softmax%2520Attention%2520%2528SA%2529%252C%2520Linear%2520Attention%2520%2528LA%2529%2520reduces%2520per-tensor%2520heavy%2520tails%2520but%2520still%2520exhibits%2520persistent%2520block-level%2520spikes%2520under%2520block%2520quantization.%2520Our%2520analysis%2520attributes%2520outliers%2520to%2520specific%2520architectural%2520components%253A%2520Softmax%2520in%2520SA%252C%2520gating%2520in%2520LA%252C%2520and%2520SwiGLU%2520in%2520FFN%252C%2520with%2520%2522post-QK%2522%2520operations%2520exhibiting%2520higher%2520sensitivity%2520to%2520quantization.%2520Notably%252C%2520outliers%2520evolve%2520from%2520transient%2520spikes%2520early%2520in%2520training%2520to%2520a%2520small%2520set%2520of%2520persistent%2520hot%2520channels%2520%2528i.e.%252C%2520channels%2520with%2520persistently%2520large%2520magnitudes%2529%2520in%2520later%2520stages.%2520Based%2520on%2520these%2520findings%252C%2520we%2520introduce%2520Hot-Channel%2520Patch%2520%2528HCP%2529%252C%2520an%2520online%2520compensation%2520mechanism%2520that%2520identifies%2520hot%2520channels%2520and%2520reinjects%2520residuals%2520using%2520hardware-efficient%2520kernels.%2520We%2520then%2520develop%2520CHON%252C%2520an%2520NVFP4%2520training%2520recipe%2520integrating%2520HCP%2520with%2520post-QK%2520operation%2520protection.%2520On%2520GLA-1.3B%2520model%2520trained%2520for%252060B%2520tokens%252C%2520CHON%2520reduces%2520the%2520loss%2520gap%2520to%2520BF16%2520from%25200.94%2525%2520to%25200.58%2525%2520while%2520maintaining%2520downstream%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02047v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dissecting%20Outlier%20Dynamics%20in%20LLM%20NVFP4%20Pretraining&entry.906535625=Peijie%20Dong%20and%20Ruibo%20Fan%20and%20Yuechen%20Tao%20and%20Di%20Mou%20and%20Wenhu%20Hu%20and%20Zhenheng%20Tang%20and%20Yinghao%20Yu%20and%20Jiamang%20Wang%20and%20Wenbo%20Su%20and%20Guodong%20Yang%20and%20Liping%20Zhang%20and%20Xiaowen%20Chu%20and%20Baochun%20Li%20and%20Bo%20Li&entry.1292438233=Training%20large%20language%20models%20using%204-bit%20arithmetic%20enhances%20throughput%20and%20memory%20efficiency.%20Yet%2C%20the%20limited%20dynamic%20range%20of%20FP4%20increases%20sensitivity%20to%20outliers.%20While%20NVFP4%20mitigates%20quantization%20error%20via%20hierarchical%20microscaling%2C%20a%20persistent%20loss%20gap%20remains%20compared%20to%20BF16.%20This%20study%20conducts%20a%20longitudinal%20analysis%20of%20outlier%20dynamics%20across%20architecture%20during%20NVFP4%20pretraining%2C%20focusing%20on%20where%20they%20localize%2C%20why%20they%20occur%2C%20and%20how%20they%20evolve%20temporally.%20We%20find%20that%2C%20compared%20with%20Softmax%20Attention%20%28SA%29%2C%20Linear%20Attention%20%28LA%29%20reduces%20per-tensor%20heavy%20tails%20but%20still%20exhibits%20persistent%20block-level%20spikes%20under%20block%20quantization.%20Our%20analysis%20attributes%20outliers%20to%20specific%20architectural%20components%3A%20Softmax%20in%20SA%2C%20gating%20in%20LA%2C%20and%20SwiGLU%20in%20FFN%2C%20with%20%22post-QK%22%20operations%20exhibiting%20higher%20sensitivity%20to%20quantization.%20Notably%2C%20outliers%20evolve%20from%20transient%20spikes%20early%20in%20training%20to%20a%20small%20set%20of%20persistent%20hot%20channels%20%28i.e.%2C%20channels%20with%20persistently%20large%20magnitudes%29%20in%20later%20stages.%20Based%20on%20these%20findings%2C%20we%20introduce%20Hot-Channel%20Patch%20%28HCP%29%2C%20an%20online%20compensation%20mechanism%20that%20identifies%20hot%20channels%20and%20reinjects%20residuals%20using%20hardware-efficient%20kernels.%20We%20then%20develop%20CHON%2C%20an%20NVFP4%20training%20recipe%20integrating%20HCP%20with%20post-QK%20operation%20protection.%20On%20GLA-1.3B%20model%20trained%20for%2060B%20tokens%2C%20CHON%20reduces%20the%20loss%20gap%20to%20BF16%20from%200.94%25%20to%200.58%25%20while%20maintaining%20downstream%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2602.02047v1&entry.124074799=Read"},
{"title": "OpenSeal: Good, Fast, and Cheap Construction of an Open-Source Southeast Asian LLM via Parallel Data", "author": "Tan Sang Nguyen and Muhammad Reza Qorib and Hwee Tou Ng", "abstract": "Large language models (LLMs) have proven to be effective tools for a wide range of natural language processing (NLP) applications. Although many LLMs are multilingual, most remain English-centric and perform poorly on low-resource languages. Recently, several Southeast Asia-focused LLMs have been developed, but none are truly open source, as they do not publicly disclose their training data. Truly open-source models are important for transparency and for enabling a deeper and more precise understanding of LLM internals and development, including biases, generalization, and multilinguality. Motivated by recent advances demonstrating the effectiveness of parallel data in improving multilingual performance, we conduct controlled and comprehensive experiments to study the effectiveness of parallel data in continual pretraining of LLMs. Our findings show that using only parallel data is the most effective way to extend an LLM to new languages. Using just 34.7B tokens of parallel data and 180 hours on 8x NVIDIA H200 GPUs, we built OpenSeal, the first truly open Southeast Asian LLM that rivals the performance of existing models of similar size.", "link": "http://arxiv.org/abs/2602.02266v1", "date": "2026-02-02", "relevancy": 2.4434, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4898}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4898}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenSeal%3A%20Good%2C%20Fast%2C%20and%20Cheap%20Construction%20of%20an%20Open-Source%20Southeast%20Asian%20LLM%20via%20Parallel%20Data&body=Title%3A%20OpenSeal%3A%20Good%2C%20Fast%2C%20and%20Cheap%20Construction%20of%20an%20Open-Source%20Southeast%20Asian%20LLM%20via%20Parallel%20Data%0AAuthor%3A%20Tan%20Sang%20Nguyen%20and%20Muhammad%20Reza%20Qorib%20and%20Hwee%20Tou%20Ng%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20proven%20to%20be%20effective%20tools%20for%20a%20wide%20range%20of%20natural%20language%20processing%20%28NLP%29%20applications.%20Although%20many%20LLMs%20are%20multilingual%2C%20most%20remain%20English-centric%20and%20perform%20poorly%20on%20low-resource%20languages.%20Recently%2C%20several%20Southeast%20Asia-focused%20LLMs%20have%20been%20developed%2C%20but%20none%20are%20truly%20open%20source%2C%20as%20they%20do%20not%20publicly%20disclose%20their%20training%20data.%20Truly%20open-source%20models%20are%20important%20for%20transparency%20and%20for%20enabling%20a%20deeper%20and%20more%20precise%20understanding%20of%20LLM%20internals%20and%20development%2C%20including%20biases%2C%20generalization%2C%20and%20multilinguality.%20Motivated%20by%20recent%20advances%20demonstrating%20the%20effectiveness%20of%20parallel%20data%20in%20improving%20multilingual%20performance%2C%20we%20conduct%20controlled%20and%20comprehensive%20experiments%20to%20study%20the%20effectiveness%20of%20parallel%20data%20in%20continual%20pretraining%20of%20LLMs.%20Our%20findings%20show%20that%20using%20only%20parallel%20data%20is%20the%20most%20effective%20way%20to%20extend%20an%20LLM%20to%20new%20languages.%20Using%20just%2034.7B%20tokens%20of%20parallel%20data%20and%20180%20hours%20on%208x%20NVIDIA%20H200%20GPUs%2C%20we%20built%20OpenSeal%2C%20the%20first%20truly%20open%20Southeast%20Asian%20LLM%20that%20rivals%20the%20performance%20of%20existing%20models%20of%20similar%20size.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenSeal%253A%2520Good%252C%2520Fast%252C%2520and%2520Cheap%2520Construction%2520of%2520an%2520Open-Source%2520Southeast%2520Asian%2520LLM%2520via%2520Parallel%2520Data%26entry.906535625%3DTan%2520Sang%2520Nguyen%2520and%2520Muhammad%2520Reza%2520Qorib%2520and%2520Hwee%2520Tou%2520Ng%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520proven%2520to%2520be%2520effective%2520tools%2520for%2520a%2520wide%2520range%2520of%2520natural%2520language%2520processing%2520%2528NLP%2529%2520applications.%2520Although%2520many%2520LLMs%2520are%2520multilingual%252C%2520most%2520remain%2520English-centric%2520and%2520perform%2520poorly%2520on%2520low-resource%2520languages.%2520Recently%252C%2520several%2520Southeast%2520Asia-focused%2520LLMs%2520have%2520been%2520developed%252C%2520but%2520none%2520are%2520truly%2520open%2520source%252C%2520as%2520they%2520do%2520not%2520publicly%2520disclose%2520their%2520training%2520data.%2520Truly%2520open-source%2520models%2520are%2520important%2520for%2520transparency%2520and%2520for%2520enabling%2520a%2520deeper%2520and%2520more%2520precise%2520understanding%2520of%2520LLM%2520internals%2520and%2520development%252C%2520including%2520biases%252C%2520generalization%252C%2520and%2520multilinguality.%2520Motivated%2520by%2520recent%2520advances%2520demonstrating%2520the%2520effectiveness%2520of%2520parallel%2520data%2520in%2520improving%2520multilingual%2520performance%252C%2520we%2520conduct%2520controlled%2520and%2520comprehensive%2520experiments%2520to%2520study%2520the%2520effectiveness%2520of%2520parallel%2520data%2520in%2520continual%2520pretraining%2520of%2520LLMs.%2520Our%2520findings%2520show%2520that%2520using%2520only%2520parallel%2520data%2520is%2520the%2520most%2520effective%2520way%2520to%2520extend%2520an%2520LLM%2520to%2520new%2520languages.%2520Using%2520just%252034.7B%2520tokens%2520of%2520parallel%2520data%2520and%2520180%2520hours%2520on%25208x%2520NVIDIA%2520H200%2520GPUs%252C%2520we%2520built%2520OpenSeal%252C%2520the%2520first%2520truly%2520open%2520Southeast%2520Asian%2520LLM%2520that%2520rivals%2520the%2520performance%2520of%2520existing%2520models%2520of%2520similar%2520size.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenSeal%3A%20Good%2C%20Fast%2C%20and%20Cheap%20Construction%20of%20an%20Open-Source%20Southeast%20Asian%20LLM%20via%20Parallel%20Data&entry.906535625=Tan%20Sang%20Nguyen%20and%20Muhammad%20Reza%20Qorib%20and%20Hwee%20Tou%20Ng&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20proven%20to%20be%20effective%20tools%20for%20a%20wide%20range%20of%20natural%20language%20processing%20%28NLP%29%20applications.%20Although%20many%20LLMs%20are%20multilingual%2C%20most%20remain%20English-centric%20and%20perform%20poorly%20on%20low-resource%20languages.%20Recently%2C%20several%20Southeast%20Asia-focused%20LLMs%20have%20been%20developed%2C%20but%20none%20are%20truly%20open%20source%2C%20as%20they%20do%20not%20publicly%20disclose%20their%20training%20data.%20Truly%20open-source%20models%20are%20important%20for%20transparency%20and%20for%20enabling%20a%20deeper%20and%20more%20precise%20understanding%20of%20LLM%20internals%20and%20development%2C%20including%20biases%2C%20generalization%2C%20and%20multilinguality.%20Motivated%20by%20recent%20advances%20demonstrating%20the%20effectiveness%20of%20parallel%20data%20in%20improving%20multilingual%20performance%2C%20we%20conduct%20controlled%20and%20comprehensive%20experiments%20to%20study%20the%20effectiveness%20of%20parallel%20data%20in%20continual%20pretraining%20of%20LLMs.%20Our%20findings%20show%20that%20using%20only%20parallel%20data%20is%20the%20most%20effective%20way%20to%20extend%20an%20LLM%20to%20new%20languages.%20Using%20just%2034.7B%20tokens%20of%20parallel%20data%20and%20180%20hours%20on%208x%20NVIDIA%20H200%20GPUs%2C%20we%20built%20OpenSeal%2C%20the%20first%20truly%20open%20Southeast%20Asian%20LLM%20that%20rivals%20the%20performance%20of%20existing%20models%20of%20similar%20size.&entry.1838667208=http%3A//arxiv.org/abs/2602.02266v1&entry.124074799=Read"},
{"title": "Game-Time: Evaluating Temporal Dynamics in Spoken Language Models", "author": "Kai-Wei Chang and En-Pei Hu and Chun-Yi Kuan and Wenze Ren and Wei-Chih Chen and Guan-Ting Lin and Yu Tsao and Shao-Hua Sun and Hung-yi Lee and James Glass", "abstract": "Conversational Spoken Language Models (SLMs) are emerging as a promising paradigm for real-time speech interaction. However, their capacity of temporal dynamics, including the ability to manage timing, tempo and simultaneous speaking, remains a critical and unevaluated challenge for conversational fluency. To address this gap, we introduce the Game-Time Benchmark, a framework to systematically assess these temporal capabilities. Inspired by how humans learn a language through language activities, Game-Time consists of basic instruction-following tasks and advanced tasks with temporal constraints, such as tempo adherence and synchronized responses. Our evaluation of diverse SLM architectures reveals a clear performance disparity: while state-of-the-art models handle basic tasks well, many contemporary systems still struggle with fundamental instruction-following. More critically, nearly all models degrade substantially under temporal constraints, exposing persistent weaknesses in time awareness and full-duplex interaction. The Game-Time Benchmark provides a foundation for guiding future research toward more temporally-aware conversational AI. Demos and datasets are available on our project website https://ga642381.github.io/Game-Time.", "link": "http://arxiv.org/abs/2509.26388v2", "date": "2026-02-02", "relevancy": 2.4427, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5007}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4851}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Game-Time%3A%20Evaluating%20Temporal%20Dynamics%20in%20Spoken%20Language%20Models&body=Title%3A%20Game-Time%3A%20Evaluating%20Temporal%20Dynamics%20in%20Spoken%20Language%20Models%0AAuthor%3A%20Kai-Wei%20Chang%20and%20En-Pei%20Hu%20and%20Chun-Yi%20Kuan%20and%20Wenze%20Ren%20and%20Wei-Chih%20Chen%20and%20Guan-Ting%20Lin%20and%20Yu%20Tsao%20and%20Shao-Hua%20Sun%20and%20Hung-yi%20Lee%20and%20James%20Glass%0AAbstract%3A%20Conversational%20Spoken%20Language%20Models%20%28SLMs%29%20are%20emerging%20as%20a%20promising%20paradigm%20for%20real-time%20speech%20interaction.%20However%2C%20their%20capacity%20of%20temporal%20dynamics%2C%20including%20the%20ability%20to%20manage%20timing%2C%20tempo%20and%20simultaneous%20speaking%2C%20remains%20a%20critical%20and%20unevaluated%20challenge%20for%20conversational%20fluency.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20Game-Time%20Benchmark%2C%20a%20framework%20to%20systematically%20assess%20these%20temporal%20capabilities.%20Inspired%20by%20how%20humans%20learn%20a%20language%20through%20language%20activities%2C%20Game-Time%20consists%20of%20basic%20instruction-following%20tasks%20and%20advanced%20tasks%20with%20temporal%20constraints%2C%20such%20as%20tempo%20adherence%20and%20synchronized%20responses.%20Our%20evaluation%20of%20diverse%20SLM%20architectures%20reveals%20a%20clear%20performance%20disparity%3A%20while%20state-of-the-art%20models%20handle%20basic%20tasks%20well%2C%20many%20contemporary%20systems%20still%20struggle%20with%20fundamental%20instruction-following.%20More%20critically%2C%20nearly%20all%20models%20degrade%20substantially%20under%20temporal%20constraints%2C%20exposing%20persistent%20weaknesses%20in%20time%20awareness%20and%20full-duplex%20interaction.%20The%20Game-Time%20Benchmark%20provides%20a%20foundation%20for%20guiding%20future%20research%20toward%20more%20temporally-aware%20conversational%20AI.%20Demos%20and%20datasets%20are%20available%20on%20our%20project%20website%20https%3A//ga642381.github.io/Game-Time.%0ALink%3A%20http%3A//arxiv.org/abs/2509.26388v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGame-Time%253A%2520Evaluating%2520Temporal%2520Dynamics%2520in%2520Spoken%2520Language%2520Models%26entry.906535625%3DKai-Wei%2520Chang%2520and%2520En-Pei%2520Hu%2520and%2520Chun-Yi%2520Kuan%2520and%2520Wenze%2520Ren%2520and%2520Wei-Chih%2520Chen%2520and%2520Guan-Ting%2520Lin%2520and%2520Yu%2520Tsao%2520and%2520Shao-Hua%2520Sun%2520and%2520Hung-yi%2520Lee%2520and%2520James%2520Glass%26entry.1292438233%3DConversational%2520Spoken%2520Language%2520Models%2520%2528SLMs%2529%2520are%2520emerging%2520as%2520a%2520promising%2520paradigm%2520for%2520real-time%2520speech%2520interaction.%2520However%252C%2520their%2520capacity%2520of%2520temporal%2520dynamics%252C%2520including%2520the%2520ability%2520to%2520manage%2520timing%252C%2520tempo%2520and%2520simultaneous%2520speaking%252C%2520remains%2520a%2520critical%2520and%2520unevaluated%2520challenge%2520for%2520conversational%2520fluency.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520the%2520Game-Time%2520Benchmark%252C%2520a%2520framework%2520to%2520systematically%2520assess%2520these%2520temporal%2520capabilities.%2520Inspired%2520by%2520how%2520humans%2520learn%2520a%2520language%2520through%2520language%2520activities%252C%2520Game-Time%2520consists%2520of%2520basic%2520instruction-following%2520tasks%2520and%2520advanced%2520tasks%2520with%2520temporal%2520constraints%252C%2520such%2520as%2520tempo%2520adherence%2520and%2520synchronized%2520responses.%2520Our%2520evaluation%2520of%2520diverse%2520SLM%2520architectures%2520reveals%2520a%2520clear%2520performance%2520disparity%253A%2520while%2520state-of-the-art%2520models%2520handle%2520basic%2520tasks%2520well%252C%2520many%2520contemporary%2520systems%2520still%2520struggle%2520with%2520fundamental%2520instruction-following.%2520More%2520critically%252C%2520nearly%2520all%2520models%2520degrade%2520substantially%2520under%2520temporal%2520constraints%252C%2520exposing%2520persistent%2520weaknesses%2520in%2520time%2520awareness%2520and%2520full-duplex%2520interaction.%2520The%2520Game-Time%2520Benchmark%2520provides%2520a%2520foundation%2520for%2520guiding%2520future%2520research%2520toward%2520more%2520temporally-aware%2520conversational%2520AI.%2520Demos%2520and%2520datasets%2520are%2520available%2520on%2520our%2520project%2520website%2520https%253A//ga642381.github.io/Game-Time.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26388v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Game-Time%3A%20Evaluating%20Temporal%20Dynamics%20in%20Spoken%20Language%20Models&entry.906535625=Kai-Wei%20Chang%20and%20En-Pei%20Hu%20and%20Chun-Yi%20Kuan%20and%20Wenze%20Ren%20and%20Wei-Chih%20Chen%20and%20Guan-Ting%20Lin%20and%20Yu%20Tsao%20and%20Shao-Hua%20Sun%20and%20Hung-yi%20Lee%20and%20James%20Glass&entry.1292438233=Conversational%20Spoken%20Language%20Models%20%28SLMs%29%20are%20emerging%20as%20a%20promising%20paradigm%20for%20real-time%20speech%20interaction.%20However%2C%20their%20capacity%20of%20temporal%20dynamics%2C%20including%20the%20ability%20to%20manage%20timing%2C%20tempo%20and%20simultaneous%20speaking%2C%20remains%20a%20critical%20and%20unevaluated%20challenge%20for%20conversational%20fluency.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20Game-Time%20Benchmark%2C%20a%20framework%20to%20systematically%20assess%20these%20temporal%20capabilities.%20Inspired%20by%20how%20humans%20learn%20a%20language%20through%20language%20activities%2C%20Game-Time%20consists%20of%20basic%20instruction-following%20tasks%20and%20advanced%20tasks%20with%20temporal%20constraints%2C%20such%20as%20tempo%20adherence%20and%20synchronized%20responses.%20Our%20evaluation%20of%20diverse%20SLM%20architectures%20reveals%20a%20clear%20performance%20disparity%3A%20while%20state-of-the-art%20models%20handle%20basic%20tasks%20well%2C%20many%20contemporary%20systems%20still%20struggle%20with%20fundamental%20instruction-following.%20More%20critically%2C%20nearly%20all%20models%20degrade%20substantially%20under%20temporal%20constraints%2C%20exposing%20persistent%20weaknesses%20in%20time%20awareness%20and%20full-duplex%20interaction.%20The%20Game-Time%20Benchmark%20provides%20a%20foundation%20for%20guiding%20future%20research%20toward%20more%20temporally-aware%20conversational%20AI.%20Demos%20and%20datasets%20are%20available%20on%20our%20project%20website%20https%3A//ga642381.github.io/Game-Time.&entry.1838667208=http%3A//arxiv.org/abs/2509.26388v2&entry.124074799=Read"},
{"title": "The Verification Crisis: Expert Perceptions of GenAI Disinformation and the Case for Reproducible Provenance", "author": "Alexander Loth and Martin Kappes and Marc-Oliver Pahl", "abstract": "The growth of Generative Artificial Intelligence (GenAI) has shifted disinformation production from manual fabrication to automated, large-scale manipulation. This article presents findings from the first wave of a longitudinal expert perception survey (N=21) involving AI researchers, policymakers, and disinformation specialists. It examines the perceived severity of multimodal threats -- text, image, audio, and video -- and evaluates current mitigation strategies.\n  Results indicate that while deepfake video presents immediate \"shock\" value, large-scale text generation poses a systemic risk of \"epistemic fragmentation\" and \"synthetic consensus,\" particularly in the political domain. The survey reveals skepticism about technical detection tools, with experts favoring provenance standards and regulatory frameworks despite implementation barriers.\n  GenAI disinformation research requires reproducible methods. The current challenge is measurement: without standardized benchmarks and reproducibility checklists, tracking or countering synthetic media remains difficult. We propose treating information integrity as an infrastructure with rigor in data provenance and methodological reproducibility.", "link": "http://arxiv.org/abs/2602.02100v1", "date": "2026-02-02", "relevancy": 2.4406, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5482}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4667}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Verification%20Crisis%3A%20Expert%20Perceptions%20of%20GenAI%20Disinformation%20and%20the%20Case%20for%20Reproducible%20Provenance&body=Title%3A%20The%20Verification%20Crisis%3A%20Expert%20Perceptions%20of%20GenAI%20Disinformation%20and%20the%20Case%20for%20Reproducible%20Provenance%0AAuthor%3A%20Alexander%20Loth%20and%20Martin%20Kappes%20and%20Marc-Oliver%20Pahl%0AAbstract%3A%20The%20growth%20of%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20has%20shifted%20disinformation%20production%20from%20manual%20fabrication%20to%20automated%2C%20large-scale%20manipulation.%20This%20article%20presents%20findings%20from%20the%20first%20wave%20of%20a%20longitudinal%20expert%20perception%20survey%20%28N%3D21%29%20involving%20AI%20researchers%2C%20policymakers%2C%20and%20disinformation%20specialists.%20It%20examines%20the%20perceived%20severity%20of%20multimodal%20threats%20--%20text%2C%20image%2C%20audio%2C%20and%20video%20--%20and%20evaluates%20current%20mitigation%20strategies.%0A%20%20Results%20indicate%20that%20while%20deepfake%20video%20presents%20immediate%20%22shock%22%20value%2C%20large-scale%20text%20generation%20poses%20a%20systemic%20risk%20of%20%22epistemic%20fragmentation%22%20and%20%22synthetic%20consensus%2C%22%20particularly%20in%20the%20political%20domain.%20The%20survey%20reveals%20skepticism%20about%20technical%20detection%20tools%2C%20with%20experts%20favoring%20provenance%20standards%20and%20regulatory%20frameworks%20despite%20implementation%20barriers.%0A%20%20GenAI%20disinformation%20research%20requires%20reproducible%20methods.%20The%20current%20challenge%20is%20measurement%3A%20without%20standardized%20benchmarks%20and%20reproducibility%20checklists%2C%20tracking%20or%20countering%20synthetic%20media%20remains%20difficult.%20We%20propose%20treating%20information%20integrity%20as%20an%20infrastructure%20with%20rigor%20in%20data%20provenance%20and%20methodological%20reproducibility.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02100v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Verification%2520Crisis%253A%2520Expert%2520Perceptions%2520of%2520GenAI%2520Disinformation%2520and%2520the%2520Case%2520for%2520Reproducible%2520Provenance%26entry.906535625%3DAlexander%2520Loth%2520and%2520Martin%2520Kappes%2520and%2520Marc-Oliver%2520Pahl%26entry.1292438233%3DThe%2520growth%2520of%2520Generative%2520Artificial%2520Intelligence%2520%2528GenAI%2529%2520has%2520shifted%2520disinformation%2520production%2520from%2520manual%2520fabrication%2520to%2520automated%252C%2520large-scale%2520manipulation.%2520This%2520article%2520presents%2520findings%2520from%2520the%2520first%2520wave%2520of%2520a%2520longitudinal%2520expert%2520perception%2520survey%2520%2528N%253D21%2529%2520involving%2520AI%2520researchers%252C%2520policymakers%252C%2520and%2520disinformation%2520specialists.%2520It%2520examines%2520the%2520perceived%2520severity%2520of%2520multimodal%2520threats%2520--%2520text%252C%2520image%252C%2520audio%252C%2520and%2520video%2520--%2520and%2520evaluates%2520current%2520mitigation%2520strategies.%250A%2520%2520Results%2520indicate%2520that%2520while%2520deepfake%2520video%2520presents%2520immediate%2520%2522shock%2522%2520value%252C%2520large-scale%2520text%2520generation%2520poses%2520a%2520systemic%2520risk%2520of%2520%2522epistemic%2520fragmentation%2522%2520and%2520%2522synthetic%2520consensus%252C%2522%2520particularly%2520in%2520the%2520political%2520domain.%2520The%2520survey%2520reveals%2520skepticism%2520about%2520technical%2520detection%2520tools%252C%2520with%2520experts%2520favoring%2520provenance%2520standards%2520and%2520regulatory%2520frameworks%2520despite%2520implementation%2520barriers.%250A%2520%2520GenAI%2520disinformation%2520research%2520requires%2520reproducible%2520methods.%2520The%2520current%2520challenge%2520is%2520measurement%253A%2520without%2520standardized%2520benchmarks%2520and%2520reproducibility%2520checklists%252C%2520tracking%2520or%2520countering%2520synthetic%2520media%2520remains%2520difficult.%2520We%2520propose%2520treating%2520information%2520integrity%2520as%2520an%2520infrastructure%2520with%2520rigor%2520in%2520data%2520provenance%2520and%2520methodological%2520reproducibility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02100v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Verification%20Crisis%3A%20Expert%20Perceptions%20of%20GenAI%20Disinformation%20and%20the%20Case%20for%20Reproducible%20Provenance&entry.906535625=Alexander%20Loth%20and%20Martin%20Kappes%20and%20Marc-Oliver%20Pahl&entry.1292438233=The%20growth%20of%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20has%20shifted%20disinformation%20production%20from%20manual%20fabrication%20to%20automated%2C%20large-scale%20manipulation.%20This%20article%20presents%20findings%20from%20the%20first%20wave%20of%20a%20longitudinal%20expert%20perception%20survey%20%28N%3D21%29%20involving%20AI%20researchers%2C%20policymakers%2C%20and%20disinformation%20specialists.%20It%20examines%20the%20perceived%20severity%20of%20multimodal%20threats%20--%20text%2C%20image%2C%20audio%2C%20and%20video%20--%20and%20evaluates%20current%20mitigation%20strategies.%0A%20%20Results%20indicate%20that%20while%20deepfake%20video%20presents%20immediate%20%22shock%22%20value%2C%20large-scale%20text%20generation%20poses%20a%20systemic%20risk%20of%20%22epistemic%20fragmentation%22%20and%20%22synthetic%20consensus%2C%22%20particularly%20in%20the%20political%20domain.%20The%20survey%20reveals%20skepticism%20about%20technical%20detection%20tools%2C%20with%20experts%20favoring%20provenance%20standards%20and%20regulatory%20frameworks%20despite%20implementation%20barriers.%0A%20%20GenAI%20disinformation%20research%20requires%20reproducible%20methods.%20The%20current%20challenge%20is%20measurement%3A%20without%20standardized%20benchmarks%20and%20reproducibility%20checklists%2C%20tracking%20or%20countering%20synthetic%20media%20remains%20difficult.%20We%20propose%20treating%20information%20integrity%20as%20an%20infrastructure%20with%20rigor%20in%20data%20provenance%20and%20methodological%20reproducibility.&entry.1838667208=http%3A//arxiv.org/abs/2602.02100v1&entry.124074799=Read"},
{"title": "Misconception Diagnosis From Student-Tutor Dialogue: Generate, Retrieve, Rerank", "author": "Joshua Mitton and Prarthana Bhattacharyya and Digory Smith and Thomas Christie and Ralph Abboud and Simon Woodhead", "abstract": "Timely and accurate identification of student misconceptions is key to improving learning outcomes and pre-empting the compounding of student errors. However, this task is highly dependent on the effort and intuition of the teacher. In this work, we present a novel approach for detecting misconceptions from student-tutor dialogues using large language models (LLMs). First, we use a fine-tuned LLM to generate plausible misconceptions, and then retrieve the most promising candidates among these using embedding similarity with the input dialogue. These candidates are then assessed and re-ranked by another fine-tuned LLM to improve misconception relevance. Empirically, we evaluate our system on real dialogues from an educational tutoring platform. We consider multiple base LLM models including LLaMA, Qwen and Claude on zero-shot and fine-tuned settings. We find that our approach improves predictive performance over baseline models and that fine-tuning improves both generated misconception quality and can outperform larger closed-source models. Finally, we conduct ablation studies to both validate the importance of our generation and reranking steps on misconception generation quality.", "link": "http://arxiv.org/abs/2602.02414v1", "date": "2026-02-02", "relevancy": 2.4401, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.487}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Misconception%20Diagnosis%20From%20Student-Tutor%20Dialogue%3A%20Generate%2C%20Retrieve%2C%20Rerank&body=Title%3A%20Misconception%20Diagnosis%20From%20Student-Tutor%20Dialogue%3A%20Generate%2C%20Retrieve%2C%20Rerank%0AAuthor%3A%20Joshua%20Mitton%20and%20Prarthana%20Bhattacharyya%20and%20Digory%20Smith%20and%20Thomas%20Christie%20and%20Ralph%20Abboud%20and%20Simon%20Woodhead%0AAbstract%3A%20Timely%20and%20accurate%20identification%20of%20student%20misconceptions%20is%20key%20to%20improving%20learning%20outcomes%20and%20pre-empting%20the%20compounding%20of%20student%20errors.%20However%2C%20this%20task%20is%20highly%20dependent%20on%20the%20effort%20and%20intuition%20of%20the%20teacher.%20In%20this%20work%2C%20we%20present%20a%20novel%20approach%20for%20detecting%20misconceptions%20from%20student-tutor%20dialogues%20using%20large%20language%20models%20%28LLMs%29.%20First%2C%20we%20use%20a%20fine-tuned%20LLM%20to%20generate%20plausible%20misconceptions%2C%20and%20then%20retrieve%20the%20most%20promising%20candidates%20among%20these%20using%20embedding%20similarity%20with%20the%20input%20dialogue.%20These%20candidates%20are%20then%20assessed%20and%20re-ranked%20by%20another%20fine-tuned%20LLM%20to%20improve%20misconception%20relevance.%20Empirically%2C%20we%20evaluate%20our%20system%20on%20real%20dialogues%20from%20an%20educational%20tutoring%20platform.%20We%20consider%20multiple%20base%20LLM%20models%20including%20LLaMA%2C%20Qwen%20and%20Claude%20on%20zero-shot%20and%20fine-tuned%20settings.%20We%20find%20that%20our%20approach%20improves%20predictive%20performance%20over%20baseline%20models%20and%20that%20fine-tuning%20improves%20both%20generated%20misconception%20quality%20and%20can%20outperform%20larger%20closed-source%20models.%20Finally%2C%20we%20conduct%20ablation%20studies%20to%20both%20validate%20the%20importance%20of%20our%20generation%20and%20reranking%20steps%20on%20misconception%20generation%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMisconception%2520Diagnosis%2520From%2520Student-Tutor%2520Dialogue%253A%2520Generate%252C%2520Retrieve%252C%2520Rerank%26entry.906535625%3DJoshua%2520Mitton%2520and%2520Prarthana%2520Bhattacharyya%2520and%2520Digory%2520Smith%2520and%2520Thomas%2520Christie%2520and%2520Ralph%2520Abboud%2520and%2520Simon%2520Woodhead%26entry.1292438233%3DTimely%2520and%2520accurate%2520identification%2520of%2520student%2520misconceptions%2520is%2520key%2520to%2520improving%2520learning%2520outcomes%2520and%2520pre-empting%2520the%2520compounding%2520of%2520student%2520errors.%2520However%252C%2520this%2520task%2520is%2520highly%2520dependent%2520on%2520the%2520effort%2520and%2520intuition%2520of%2520the%2520teacher.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%2520approach%2520for%2520detecting%2520misconceptions%2520from%2520student-tutor%2520dialogues%2520using%2520large%2520language%2520models%2520%2528LLMs%2529.%2520First%252C%2520we%2520use%2520a%2520fine-tuned%2520LLM%2520to%2520generate%2520plausible%2520misconceptions%252C%2520and%2520then%2520retrieve%2520the%2520most%2520promising%2520candidates%2520among%2520these%2520using%2520embedding%2520similarity%2520with%2520the%2520input%2520dialogue.%2520These%2520candidates%2520are%2520then%2520assessed%2520and%2520re-ranked%2520by%2520another%2520fine-tuned%2520LLM%2520to%2520improve%2520misconception%2520relevance.%2520Empirically%252C%2520we%2520evaluate%2520our%2520system%2520on%2520real%2520dialogues%2520from%2520an%2520educational%2520tutoring%2520platform.%2520We%2520consider%2520multiple%2520base%2520LLM%2520models%2520including%2520LLaMA%252C%2520Qwen%2520and%2520Claude%2520on%2520zero-shot%2520and%2520fine-tuned%2520settings.%2520We%2520find%2520that%2520our%2520approach%2520improves%2520predictive%2520performance%2520over%2520baseline%2520models%2520and%2520that%2520fine-tuning%2520improves%2520both%2520generated%2520misconception%2520quality%2520and%2520can%2520outperform%2520larger%2520closed-source%2520models.%2520Finally%252C%2520we%2520conduct%2520ablation%2520studies%2520to%2520both%2520validate%2520the%2520importance%2520of%2520our%2520generation%2520and%2520reranking%2520steps%2520on%2520misconception%2520generation%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Misconception%20Diagnosis%20From%20Student-Tutor%20Dialogue%3A%20Generate%2C%20Retrieve%2C%20Rerank&entry.906535625=Joshua%20Mitton%20and%20Prarthana%20Bhattacharyya%20and%20Digory%20Smith%20and%20Thomas%20Christie%20and%20Ralph%20Abboud%20and%20Simon%20Woodhead&entry.1292438233=Timely%20and%20accurate%20identification%20of%20student%20misconceptions%20is%20key%20to%20improving%20learning%20outcomes%20and%20pre-empting%20the%20compounding%20of%20student%20errors.%20However%2C%20this%20task%20is%20highly%20dependent%20on%20the%20effort%20and%20intuition%20of%20the%20teacher.%20In%20this%20work%2C%20we%20present%20a%20novel%20approach%20for%20detecting%20misconceptions%20from%20student-tutor%20dialogues%20using%20large%20language%20models%20%28LLMs%29.%20First%2C%20we%20use%20a%20fine-tuned%20LLM%20to%20generate%20plausible%20misconceptions%2C%20and%20then%20retrieve%20the%20most%20promising%20candidates%20among%20these%20using%20embedding%20similarity%20with%20the%20input%20dialogue.%20These%20candidates%20are%20then%20assessed%20and%20re-ranked%20by%20another%20fine-tuned%20LLM%20to%20improve%20misconception%20relevance.%20Empirically%2C%20we%20evaluate%20our%20system%20on%20real%20dialogues%20from%20an%20educational%20tutoring%20platform.%20We%20consider%20multiple%20base%20LLM%20models%20including%20LLaMA%2C%20Qwen%20and%20Claude%20on%20zero-shot%20and%20fine-tuned%20settings.%20We%20find%20that%20our%20approach%20improves%20predictive%20performance%20over%20baseline%20models%20and%20that%20fine-tuning%20improves%20both%20generated%20misconception%20quality%20and%20can%20outperform%20larger%20closed-source%20models.%20Finally%2C%20we%20conduct%20ablation%20studies%20to%20both%20validate%20the%20importance%20of%20our%20generation%20and%20reranking%20steps%20on%20misconception%20generation%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2602.02414v1&entry.124074799=Read"},
{"title": "ECHO: Entropy-Confidence Hybrid Optimization for Test-Time Reinforcement Learning", "author": "Chu Zhao and Enneng Yang and Yuting Liu and Jianzhe Zhao and Guibing Guo", "abstract": "Test-time reinforcement learning generates multiple candidate answers via repeated rollouts and performs online updates using pseudo-labels constructed by majority voting. To reduce overhead and improve exploration, prior work introduces tree structured rollouts, which share reasoning prefixes and branch at key nodes to improve sampling efficiency. However, this paradigm still faces two challenges: (1) high entropy branching can trigger rollout collapse, where the branching budget concentrates on a few trajectories with consecutive high-entropy segments, rapidly reducing the number of effective branches; (2) early pseudo-labels are noisy and biased, which can induce self-reinforcing overfitting, causing the policy to sharpen prematurely and suppress exploration. To address these issues, we propose Entropy Confidence Hybrid Group Relative Policy Optimization (ECHO). During rollout, ECHO jointly leverages local entropy and group level confidence to adaptively control branch width, and further introduces online confidence-based pruning to terminate persistently low confidence branches, avoiding high entropy traps and mitigating collapse. During policy updates, ECHO employs confidence adaptive clipping and an entropy confidence hybrid advantage shaping approach to enhance training robustness and mitigate early stage bias. Experiments demonstrate that ECHO achieves consistent gains on multiple mathematical and visual reasoning benchmarks, and generalizes more effectively under a limited rollout budget.", "link": "http://arxiv.org/abs/2602.02150v1", "date": "2026-02-02", "relevancy": 2.4391, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5086}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4841}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ECHO%3A%20Entropy-Confidence%20Hybrid%20Optimization%20for%20Test-Time%20Reinforcement%20Learning&body=Title%3A%20ECHO%3A%20Entropy-Confidence%20Hybrid%20Optimization%20for%20Test-Time%20Reinforcement%20Learning%0AAuthor%3A%20Chu%20Zhao%20and%20Enneng%20Yang%20and%20Yuting%20Liu%20and%20Jianzhe%20Zhao%20and%20Guibing%20Guo%0AAbstract%3A%20Test-time%20reinforcement%20learning%20generates%20multiple%20candidate%20answers%20via%20repeated%20rollouts%20and%20performs%20online%20updates%20using%20pseudo-labels%20constructed%20by%20majority%20voting.%20To%20reduce%20overhead%20and%20improve%20exploration%2C%20prior%20work%20introduces%20tree%20structured%20rollouts%2C%20which%20share%20reasoning%20prefixes%20and%20branch%20at%20key%20nodes%20to%20improve%20sampling%20efficiency.%20However%2C%20this%20paradigm%20still%20faces%20two%20challenges%3A%20%281%29%20high%20entropy%20branching%20can%20trigger%20rollout%20collapse%2C%20where%20the%20branching%20budget%20concentrates%20on%20a%20few%20trajectories%20with%20consecutive%20high-entropy%20segments%2C%20rapidly%20reducing%20the%20number%20of%20effective%20branches%3B%20%282%29%20early%20pseudo-labels%20are%20noisy%20and%20biased%2C%20which%20can%20induce%20self-reinforcing%20overfitting%2C%20causing%20the%20policy%20to%20sharpen%20prematurely%20and%20suppress%20exploration.%20To%20address%20these%20issues%2C%20we%20propose%20Entropy%20Confidence%20Hybrid%20Group%20Relative%20Policy%20Optimization%20%28ECHO%29.%20During%20rollout%2C%20ECHO%20jointly%20leverages%20local%20entropy%20and%20group%20level%20confidence%20to%20adaptively%20control%20branch%20width%2C%20and%20further%20introduces%20online%20confidence-based%20pruning%20to%20terminate%20persistently%20low%20confidence%20branches%2C%20avoiding%20high%20entropy%20traps%20and%20mitigating%20collapse.%20During%20policy%20updates%2C%20ECHO%20employs%20confidence%20adaptive%20clipping%20and%20an%20entropy%20confidence%20hybrid%20advantage%20shaping%20approach%20to%20enhance%20training%20robustness%20and%20mitigate%20early%20stage%20bias.%20Experiments%20demonstrate%20that%20ECHO%20achieves%20consistent%20gains%20on%20multiple%20mathematical%20and%20visual%20reasoning%20benchmarks%2C%20and%20generalizes%20more%20effectively%20under%20a%20limited%20rollout%20budget.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DECHO%253A%2520Entropy-Confidence%2520Hybrid%2520Optimization%2520for%2520Test-Time%2520Reinforcement%2520Learning%26entry.906535625%3DChu%2520Zhao%2520and%2520Enneng%2520Yang%2520and%2520Yuting%2520Liu%2520and%2520Jianzhe%2520Zhao%2520and%2520Guibing%2520Guo%26entry.1292438233%3DTest-time%2520reinforcement%2520learning%2520generates%2520multiple%2520candidate%2520answers%2520via%2520repeated%2520rollouts%2520and%2520performs%2520online%2520updates%2520using%2520pseudo-labels%2520constructed%2520by%2520majority%2520voting.%2520To%2520reduce%2520overhead%2520and%2520improve%2520exploration%252C%2520prior%2520work%2520introduces%2520tree%2520structured%2520rollouts%252C%2520which%2520share%2520reasoning%2520prefixes%2520and%2520branch%2520at%2520key%2520nodes%2520to%2520improve%2520sampling%2520efficiency.%2520However%252C%2520this%2520paradigm%2520still%2520faces%2520two%2520challenges%253A%2520%25281%2529%2520high%2520entropy%2520branching%2520can%2520trigger%2520rollout%2520collapse%252C%2520where%2520the%2520branching%2520budget%2520concentrates%2520on%2520a%2520few%2520trajectories%2520with%2520consecutive%2520high-entropy%2520segments%252C%2520rapidly%2520reducing%2520the%2520number%2520of%2520effective%2520branches%253B%2520%25282%2529%2520early%2520pseudo-labels%2520are%2520noisy%2520and%2520biased%252C%2520which%2520can%2520induce%2520self-reinforcing%2520overfitting%252C%2520causing%2520the%2520policy%2520to%2520sharpen%2520prematurely%2520and%2520suppress%2520exploration.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520Entropy%2520Confidence%2520Hybrid%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528ECHO%2529.%2520During%2520rollout%252C%2520ECHO%2520jointly%2520leverages%2520local%2520entropy%2520and%2520group%2520level%2520confidence%2520to%2520adaptively%2520control%2520branch%2520width%252C%2520and%2520further%2520introduces%2520online%2520confidence-based%2520pruning%2520to%2520terminate%2520persistently%2520low%2520confidence%2520branches%252C%2520avoiding%2520high%2520entropy%2520traps%2520and%2520mitigating%2520collapse.%2520During%2520policy%2520updates%252C%2520ECHO%2520employs%2520confidence%2520adaptive%2520clipping%2520and%2520an%2520entropy%2520confidence%2520hybrid%2520advantage%2520shaping%2520approach%2520to%2520enhance%2520training%2520robustness%2520and%2520mitigate%2520early%2520stage%2520bias.%2520Experiments%2520demonstrate%2520that%2520ECHO%2520achieves%2520consistent%2520gains%2520on%2520multiple%2520mathematical%2520and%2520visual%2520reasoning%2520benchmarks%252C%2520and%2520generalizes%2520more%2520effectively%2520under%2520a%2520limited%2520rollout%2520budget.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECHO%3A%20Entropy-Confidence%20Hybrid%20Optimization%20for%20Test-Time%20Reinforcement%20Learning&entry.906535625=Chu%20Zhao%20and%20Enneng%20Yang%20and%20Yuting%20Liu%20and%20Jianzhe%20Zhao%20and%20Guibing%20Guo&entry.1292438233=Test-time%20reinforcement%20learning%20generates%20multiple%20candidate%20answers%20via%20repeated%20rollouts%20and%20performs%20online%20updates%20using%20pseudo-labels%20constructed%20by%20majority%20voting.%20To%20reduce%20overhead%20and%20improve%20exploration%2C%20prior%20work%20introduces%20tree%20structured%20rollouts%2C%20which%20share%20reasoning%20prefixes%20and%20branch%20at%20key%20nodes%20to%20improve%20sampling%20efficiency.%20However%2C%20this%20paradigm%20still%20faces%20two%20challenges%3A%20%281%29%20high%20entropy%20branching%20can%20trigger%20rollout%20collapse%2C%20where%20the%20branching%20budget%20concentrates%20on%20a%20few%20trajectories%20with%20consecutive%20high-entropy%20segments%2C%20rapidly%20reducing%20the%20number%20of%20effective%20branches%3B%20%282%29%20early%20pseudo-labels%20are%20noisy%20and%20biased%2C%20which%20can%20induce%20self-reinforcing%20overfitting%2C%20causing%20the%20policy%20to%20sharpen%20prematurely%20and%20suppress%20exploration.%20To%20address%20these%20issues%2C%20we%20propose%20Entropy%20Confidence%20Hybrid%20Group%20Relative%20Policy%20Optimization%20%28ECHO%29.%20During%20rollout%2C%20ECHO%20jointly%20leverages%20local%20entropy%20and%20group%20level%20confidence%20to%20adaptively%20control%20branch%20width%2C%20and%20further%20introduces%20online%20confidence-based%20pruning%20to%20terminate%20persistently%20low%20confidence%20branches%2C%20avoiding%20high%20entropy%20traps%20and%20mitigating%20collapse.%20During%20policy%20updates%2C%20ECHO%20employs%20confidence%20adaptive%20clipping%20and%20an%20entropy%20confidence%20hybrid%20advantage%20shaping%20approach%20to%20enhance%20training%20robustness%20and%20mitigate%20early%20stage%20bias.%20Experiments%20demonstrate%20that%20ECHO%20achieves%20consistent%20gains%20on%20multiple%20mathematical%20and%20visual%20reasoning%20benchmarks%2C%20and%20generalizes%20more%20effectively%20under%20a%20limited%20rollout%20budget.&entry.1838667208=http%3A//arxiv.org/abs/2602.02150v1&entry.124074799=Read"},
{"title": "ASIL: Augmented Structural Information Learning for Deep Graph Clustering in Hyperbolic Space", "author": "Li Sun and Zhenhao Huang and Yujie Wang and Hongbo Lv and Chunyang Liu and Hao Peng and Philip S. Yu", "abstract": "Graph clustering is a longstanding topic in machine learning. Recently, deep methods have achieved results but still require predefined cluster numbers K and struggle with imbalanced graphs. We study deep graph clustering without K considering realistic imbalance through structural information theory. In the literature, structural information is rarely used in deep clustering, and its classic discrete definition neglects node attributes while exhibiting prohibitive complexity. In this paper, we establish a differentiable structural information framework, generalizing the discrete formalism to the continuous realm. We design a hyperbolic model (LSEnet) to learn the neural partitioning tree in the Lorentz model. Theoretically, we demonstrate its capability in clustering without K and identifying minority clusters. Second, we refine hyperbolic representations to enhance graph semantics. Since tree contrastive learning is non-trivial and costs quadratic complexity, we advance our theory by discovering that structural entropy bounds the tree contrastive loss. Finally, we approach graph clustering through a novel augmented structural information learning (ASIL), which offers an efficient objective to integrate hyperbolic partitioning tree construction and contrastive learning. With a provable improvement in graph conductance, ASIL achieves effective debiased graph clustering in linear complexity. Extensive experiments show ASIL outperforms 20 strong baselines by an average of +12.42% in NMI on the Citeseer dataset.", "link": "http://arxiv.org/abs/2504.09970v2", "date": "2026-02-02", "relevancy": 2.4387, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5305}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4727}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ASIL%3A%20Augmented%20Structural%20Information%20Learning%20for%20Deep%20Graph%20Clustering%20in%20Hyperbolic%20Space&body=Title%3A%20ASIL%3A%20Augmented%20Structural%20Information%20Learning%20for%20Deep%20Graph%20Clustering%20in%20Hyperbolic%20Space%0AAuthor%3A%20Li%20Sun%20and%20Zhenhao%20Huang%20and%20Yujie%20Wang%20and%20Hongbo%20Lv%20and%20Chunyang%20Liu%20and%20Hao%20Peng%20and%20Philip%20S.%20Yu%0AAbstract%3A%20Graph%20clustering%20is%20a%20longstanding%20topic%20in%20machine%20learning.%20Recently%2C%20deep%20methods%20have%20achieved%20results%20but%20still%20require%20predefined%20cluster%20numbers%20K%20and%20struggle%20with%20imbalanced%20graphs.%20We%20study%20deep%20graph%20clustering%20without%20K%20considering%20realistic%20imbalance%20through%20structural%20information%20theory.%20In%20the%20literature%2C%20structural%20information%20is%20rarely%20used%20in%20deep%20clustering%2C%20and%20its%20classic%20discrete%20definition%20neglects%20node%20attributes%20while%20exhibiting%20prohibitive%20complexity.%20In%20this%20paper%2C%20we%20establish%20a%20differentiable%20structural%20information%20framework%2C%20generalizing%20the%20discrete%20formalism%20to%20the%20continuous%20realm.%20We%20design%20a%20hyperbolic%20model%20%28LSEnet%29%20to%20learn%20the%20neural%20partitioning%20tree%20in%20the%20Lorentz%20model.%20Theoretically%2C%20we%20demonstrate%20its%20capability%20in%20clustering%20without%20K%20and%20identifying%20minority%20clusters.%20Second%2C%20we%20refine%20hyperbolic%20representations%20to%20enhance%20graph%20semantics.%20Since%20tree%20contrastive%20learning%20is%20non-trivial%20and%20costs%20quadratic%20complexity%2C%20we%20advance%20our%20theory%20by%20discovering%20that%20structural%20entropy%20bounds%20the%20tree%20contrastive%20loss.%20Finally%2C%20we%20approach%20graph%20clustering%20through%20a%20novel%20augmented%20structural%20information%20learning%20%28ASIL%29%2C%20which%20offers%20an%20efficient%20objective%20to%20integrate%20hyperbolic%20partitioning%20tree%20construction%20and%20contrastive%20learning.%20With%20a%20provable%20improvement%20in%20graph%20conductance%2C%20ASIL%20achieves%20effective%20debiased%20graph%20clustering%20in%20linear%20complexity.%20Extensive%20experiments%20show%20ASIL%20outperforms%2020%20strong%20baselines%20by%20an%20average%20of%20%2B12.42%25%20in%20NMI%20on%20the%20Citeseer%20dataset.%0ALink%3A%20http%3A//arxiv.org/abs/2504.09970v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DASIL%253A%2520Augmented%2520Structural%2520Information%2520Learning%2520for%2520Deep%2520Graph%2520Clustering%2520in%2520Hyperbolic%2520Space%26entry.906535625%3DLi%2520Sun%2520and%2520Zhenhao%2520Huang%2520and%2520Yujie%2520Wang%2520and%2520Hongbo%2520Lv%2520and%2520Chunyang%2520Liu%2520and%2520Hao%2520Peng%2520and%2520Philip%2520S.%2520Yu%26entry.1292438233%3DGraph%2520clustering%2520is%2520a%2520longstanding%2520topic%2520in%2520machine%2520learning.%2520Recently%252C%2520deep%2520methods%2520have%2520achieved%2520results%2520but%2520still%2520require%2520predefined%2520cluster%2520numbers%2520K%2520and%2520struggle%2520with%2520imbalanced%2520graphs.%2520We%2520study%2520deep%2520graph%2520clustering%2520without%2520K%2520considering%2520realistic%2520imbalance%2520through%2520structural%2520information%2520theory.%2520In%2520the%2520literature%252C%2520structural%2520information%2520is%2520rarely%2520used%2520in%2520deep%2520clustering%252C%2520and%2520its%2520classic%2520discrete%2520definition%2520neglects%2520node%2520attributes%2520while%2520exhibiting%2520prohibitive%2520complexity.%2520In%2520this%2520paper%252C%2520we%2520establish%2520a%2520differentiable%2520structural%2520information%2520framework%252C%2520generalizing%2520the%2520discrete%2520formalism%2520to%2520the%2520continuous%2520realm.%2520We%2520design%2520a%2520hyperbolic%2520model%2520%2528LSEnet%2529%2520to%2520learn%2520the%2520neural%2520partitioning%2520tree%2520in%2520the%2520Lorentz%2520model.%2520Theoretically%252C%2520we%2520demonstrate%2520its%2520capability%2520in%2520clustering%2520without%2520K%2520and%2520identifying%2520minority%2520clusters.%2520Second%252C%2520we%2520refine%2520hyperbolic%2520representations%2520to%2520enhance%2520graph%2520semantics.%2520Since%2520tree%2520contrastive%2520learning%2520is%2520non-trivial%2520and%2520costs%2520quadratic%2520complexity%252C%2520we%2520advance%2520our%2520theory%2520by%2520discovering%2520that%2520structural%2520entropy%2520bounds%2520the%2520tree%2520contrastive%2520loss.%2520Finally%252C%2520we%2520approach%2520graph%2520clustering%2520through%2520a%2520novel%2520augmented%2520structural%2520information%2520learning%2520%2528ASIL%2529%252C%2520which%2520offers%2520an%2520efficient%2520objective%2520to%2520integrate%2520hyperbolic%2520partitioning%2520tree%2520construction%2520and%2520contrastive%2520learning.%2520With%2520a%2520provable%2520improvement%2520in%2520graph%2520conductance%252C%2520ASIL%2520achieves%2520effective%2520debiased%2520graph%2520clustering%2520in%2520linear%2520complexity.%2520Extensive%2520experiments%2520show%2520ASIL%2520outperforms%252020%2520strong%2520baselines%2520by%2520an%2520average%2520of%2520%252B12.42%2525%2520in%2520NMI%2520on%2520the%2520Citeseer%2520dataset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.09970v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ASIL%3A%20Augmented%20Structural%20Information%20Learning%20for%20Deep%20Graph%20Clustering%20in%20Hyperbolic%20Space&entry.906535625=Li%20Sun%20and%20Zhenhao%20Huang%20and%20Yujie%20Wang%20and%20Hongbo%20Lv%20and%20Chunyang%20Liu%20and%20Hao%20Peng%20and%20Philip%20S.%20Yu&entry.1292438233=Graph%20clustering%20is%20a%20longstanding%20topic%20in%20machine%20learning.%20Recently%2C%20deep%20methods%20have%20achieved%20results%20but%20still%20require%20predefined%20cluster%20numbers%20K%20and%20struggle%20with%20imbalanced%20graphs.%20We%20study%20deep%20graph%20clustering%20without%20K%20considering%20realistic%20imbalance%20through%20structural%20information%20theory.%20In%20the%20literature%2C%20structural%20information%20is%20rarely%20used%20in%20deep%20clustering%2C%20and%20its%20classic%20discrete%20definition%20neglects%20node%20attributes%20while%20exhibiting%20prohibitive%20complexity.%20In%20this%20paper%2C%20we%20establish%20a%20differentiable%20structural%20information%20framework%2C%20generalizing%20the%20discrete%20formalism%20to%20the%20continuous%20realm.%20We%20design%20a%20hyperbolic%20model%20%28LSEnet%29%20to%20learn%20the%20neural%20partitioning%20tree%20in%20the%20Lorentz%20model.%20Theoretically%2C%20we%20demonstrate%20its%20capability%20in%20clustering%20without%20K%20and%20identifying%20minority%20clusters.%20Second%2C%20we%20refine%20hyperbolic%20representations%20to%20enhance%20graph%20semantics.%20Since%20tree%20contrastive%20learning%20is%20non-trivial%20and%20costs%20quadratic%20complexity%2C%20we%20advance%20our%20theory%20by%20discovering%20that%20structural%20entropy%20bounds%20the%20tree%20contrastive%20loss.%20Finally%2C%20we%20approach%20graph%20clustering%20through%20a%20novel%20augmented%20structural%20information%20learning%20%28ASIL%29%2C%20which%20offers%20an%20efficient%20objective%20to%20integrate%20hyperbolic%20partitioning%20tree%20construction%20and%20contrastive%20learning.%20With%20a%20provable%20improvement%20in%20graph%20conductance%2C%20ASIL%20achieves%20effective%20debiased%20graph%20clustering%20in%20linear%20complexity.%20Extensive%20experiments%20show%20ASIL%20outperforms%2020%20strong%20baselines%20by%20an%20average%20of%20%2B12.42%25%20in%20NMI%20on%20the%20Citeseer%20dataset.&entry.1838667208=http%3A//arxiv.org/abs/2504.09970v2&entry.124074799=Read"},
{"title": "Rethinking Genomic Modeling Through Optical Character Recognition", "author": "Hongxin Xiang and Pengsen Ma and Yunkang Cao and Di Yu and Haowen Chen and Xinyu Yang and Xiangxiang Zeng", "abstract": "Recent genomic foundation models largely adopt large language model architectures that treat DNA as a one-dimensional token sequence. However, exhaustive sequential reading is structurally misaligned with sparse and discontinuous genomic semantics, leading to wasted computation on low-information background and preventing understanding-driven compression for long contexts. Here, we present OpticalDNA, a vision-based framework that reframes genomic modeling as Optical Character Recognition (OCR)-style document understanding. OpticalDNA renders DNA into structured visual layouts and trains an OCR-capable vision--language model with a \\emph{visual DNA encoder} and a \\emph{document decoder}, where the encoder produces compact, reconstructible visual tokens for high-fidelity compression. Building on this representation, OpticalDNA defines prompt-conditioned objectives over core genomic primitives-reading, region grounding, subsequence retrieval, and masked span completion-thereby learning layout-aware DNA representations that retain fine-grained genomic information under a reduced effective token budget. Across diverse genomic benchmarks, OpticalDNA consistently outperforms recent baselines; on sequences up to 450k bases, it achieves the best overall performance with nearly $20\\times$ fewer effective tokens, and surpasses models with up to $985\\times$ more activated parameters while tuning only 256k \\emph{trainable} parameters.", "link": "http://arxiv.org/abs/2602.02014v1", "date": "2026-02-02", "relevancy": 2.4308, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.622}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.622}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Genomic%20Modeling%20Through%20Optical%20Character%20Recognition&body=Title%3A%20Rethinking%20Genomic%20Modeling%20Through%20Optical%20Character%20Recognition%0AAuthor%3A%20Hongxin%20Xiang%20and%20Pengsen%20Ma%20and%20Yunkang%20Cao%20and%20Di%20Yu%20and%20Haowen%20Chen%20and%20Xinyu%20Yang%20and%20Xiangxiang%20Zeng%0AAbstract%3A%20Recent%20genomic%20foundation%20models%20largely%20adopt%20large%20language%20model%20architectures%20that%20treat%20DNA%20as%20a%20one-dimensional%20token%20sequence.%20However%2C%20exhaustive%20sequential%20reading%20is%20structurally%20misaligned%20with%20sparse%20and%20discontinuous%20genomic%20semantics%2C%20leading%20to%20wasted%20computation%20on%20low-information%20background%20and%20preventing%20understanding-driven%20compression%20for%20long%20contexts.%20Here%2C%20we%20present%20OpticalDNA%2C%20a%20vision-based%20framework%20that%20reframes%20genomic%20modeling%20as%20Optical%20Character%20Recognition%20%28OCR%29-style%20document%20understanding.%20OpticalDNA%20renders%20DNA%20into%20structured%20visual%20layouts%20and%20trains%20an%20OCR-capable%20vision--language%20model%20with%20a%20%5Cemph%7Bvisual%20DNA%20encoder%7D%20and%20a%20%5Cemph%7Bdocument%20decoder%7D%2C%20where%20the%20encoder%20produces%20compact%2C%20reconstructible%20visual%20tokens%20for%20high-fidelity%20compression.%20Building%20on%20this%20representation%2C%20OpticalDNA%20defines%20prompt-conditioned%20objectives%20over%20core%20genomic%20primitives-reading%2C%20region%20grounding%2C%20subsequence%20retrieval%2C%20and%20masked%20span%20completion-thereby%20learning%20layout-aware%20DNA%20representations%20that%20retain%20fine-grained%20genomic%20information%20under%20a%20reduced%20effective%20token%20budget.%20Across%20diverse%20genomic%20benchmarks%2C%20OpticalDNA%20consistently%20outperforms%20recent%20baselines%3B%20on%20sequences%20up%20to%20450k%20bases%2C%20it%20achieves%20the%20best%20overall%20performance%20with%20nearly%20%2420%5Ctimes%24%20fewer%20effective%20tokens%2C%20and%20surpasses%20models%20with%20up%20to%20%24985%5Ctimes%24%20more%20activated%20parameters%20while%20tuning%20only%20256k%20%5Cemph%7Btrainable%7D%20parameters.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02014v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Genomic%2520Modeling%2520Through%2520Optical%2520Character%2520Recognition%26entry.906535625%3DHongxin%2520Xiang%2520and%2520Pengsen%2520Ma%2520and%2520Yunkang%2520Cao%2520and%2520Di%2520Yu%2520and%2520Haowen%2520Chen%2520and%2520Xinyu%2520Yang%2520and%2520Xiangxiang%2520Zeng%26entry.1292438233%3DRecent%2520genomic%2520foundation%2520models%2520largely%2520adopt%2520large%2520language%2520model%2520architectures%2520that%2520treat%2520DNA%2520as%2520a%2520one-dimensional%2520token%2520sequence.%2520However%252C%2520exhaustive%2520sequential%2520reading%2520is%2520structurally%2520misaligned%2520with%2520sparse%2520and%2520discontinuous%2520genomic%2520semantics%252C%2520leading%2520to%2520wasted%2520computation%2520on%2520low-information%2520background%2520and%2520preventing%2520understanding-driven%2520compression%2520for%2520long%2520contexts.%2520Here%252C%2520we%2520present%2520OpticalDNA%252C%2520a%2520vision-based%2520framework%2520that%2520reframes%2520genomic%2520modeling%2520as%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529-style%2520document%2520understanding.%2520OpticalDNA%2520renders%2520DNA%2520into%2520structured%2520visual%2520layouts%2520and%2520trains%2520an%2520OCR-capable%2520vision--language%2520model%2520with%2520a%2520%255Cemph%257Bvisual%2520DNA%2520encoder%257D%2520and%2520a%2520%255Cemph%257Bdocument%2520decoder%257D%252C%2520where%2520the%2520encoder%2520produces%2520compact%252C%2520reconstructible%2520visual%2520tokens%2520for%2520high-fidelity%2520compression.%2520Building%2520on%2520this%2520representation%252C%2520OpticalDNA%2520defines%2520prompt-conditioned%2520objectives%2520over%2520core%2520genomic%2520primitives-reading%252C%2520region%2520grounding%252C%2520subsequence%2520retrieval%252C%2520and%2520masked%2520span%2520completion-thereby%2520learning%2520layout-aware%2520DNA%2520representations%2520that%2520retain%2520fine-grained%2520genomic%2520information%2520under%2520a%2520reduced%2520effective%2520token%2520budget.%2520Across%2520diverse%2520genomic%2520benchmarks%252C%2520OpticalDNA%2520consistently%2520outperforms%2520recent%2520baselines%253B%2520on%2520sequences%2520up%2520to%2520450k%2520bases%252C%2520it%2520achieves%2520the%2520best%2520overall%2520performance%2520with%2520nearly%2520%252420%255Ctimes%2524%2520fewer%2520effective%2520tokens%252C%2520and%2520surpasses%2520models%2520with%2520up%2520to%2520%2524985%255Ctimes%2524%2520more%2520activated%2520parameters%2520while%2520tuning%2520only%2520256k%2520%255Cemph%257Btrainable%257D%2520parameters.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02014v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Genomic%20Modeling%20Through%20Optical%20Character%20Recognition&entry.906535625=Hongxin%20Xiang%20and%20Pengsen%20Ma%20and%20Yunkang%20Cao%20and%20Di%20Yu%20and%20Haowen%20Chen%20and%20Xinyu%20Yang%20and%20Xiangxiang%20Zeng&entry.1292438233=Recent%20genomic%20foundation%20models%20largely%20adopt%20large%20language%20model%20architectures%20that%20treat%20DNA%20as%20a%20one-dimensional%20token%20sequence.%20However%2C%20exhaustive%20sequential%20reading%20is%20structurally%20misaligned%20with%20sparse%20and%20discontinuous%20genomic%20semantics%2C%20leading%20to%20wasted%20computation%20on%20low-information%20background%20and%20preventing%20understanding-driven%20compression%20for%20long%20contexts.%20Here%2C%20we%20present%20OpticalDNA%2C%20a%20vision-based%20framework%20that%20reframes%20genomic%20modeling%20as%20Optical%20Character%20Recognition%20%28OCR%29-style%20document%20understanding.%20OpticalDNA%20renders%20DNA%20into%20structured%20visual%20layouts%20and%20trains%20an%20OCR-capable%20vision--language%20model%20with%20a%20%5Cemph%7Bvisual%20DNA%20encoder%7D%20and%20a%20%5Cemph%7Bdocument%20decoder%7D%2C%20where%20the%20encoder%20produces%20compact%2C%20reconstructible%20visual%20tokens%20for%20high-fidelity%20compression.%20Building%20on%20this%20representation%2C%20OpticalDNA%20defines%20prompt-conditioned%20objectives%20over%20core%20genomic%20primitives-reading%2C%20region%20grounding%2C%20subsequence%20retrieval%2C%20and%20masked%20span%20completion-thereby%20learning%20layout-aware%20DNA%20representations%20that%20retain%20fine-grained%20genomic%20information%20under%20a%20reduced%20effective%20token%20budget.%20Across%20diverse%20genomic%20benchmarks%2C%20OpticalDNA%20consistently%20outperforms%20recent%20baselines%3B%20on%20sequences%20up%20to%20450k%20bases%2C%20it%20achieves%20the%20best%20overall%20performance%20with%20nearly%20%2420%5Ctimes%24%20fewer%20effective%20tokens%2C%20and%20surpasses%20models%20with%20up%20to%20%24985%5Ctimes%24%20more%20activated%20parameters%20while%20tuning%20only%20256k%20%5Cemph%7Btrainable%7D%20parameters.&entry.1838667208=http%3A//arxiv.org/abs/2602.02014v1&entry.124074799=Read"},
{"title": "Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models", "author": "Yu Zeng and Wenxuan Huang and Zhen Fang and Shuang Chen and Yufan Shen and Yishuo Cai and Xiaoman Wang and Zhenfei Yin and Lin Chen and Zehui Chen and Shiting Huang and Yiming Zhao and Yao Hu and Philip Torr and Wanli Ouyang and Shaosheng Cao", "abstract": "Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.", "link": "http://arxiv.org/abs/2602.02185v1", "date": "2026-02-02", "relevancy": 2.409, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6181}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6181}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-DeepResearch%20Benchmark%3A%20Rethinking%20Visual%20and%20Textual%20Search%20for%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Vision-DeepResearch%20Benchmark%3A%20Rethinking%20Visual%20and%20Textual%20Search%20for%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Yu%20Zeng%20and%20Wenxuan%20Huang%20and%20Zhen%20Fang%20and%20Shuang%20Chen%20and%20Yufan%20Shen%20and%20Yishuo%20Cai%20and%20Xiaoman%20Wang%20and%20Zhenfei%20Yin%20and%20Lin%20Chen%20and%20Zehui%20Chen%20and%20Shiting%20Huang%20and%20Yiming%20Zhao%20and%20Yao%20Hu%20and%20Philip%20Torr%20and%20Wanli%20Ouyang%20and%20Shaosheng%20Cao%0AAbstract%3A%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20advanced%20VQA%20and%20now%20support%20Vision-DeepResearch%20systems%20that%20use%20search%20engines%20for%20complex%20visual-textual%20fact-finding.%20However%2C%20evaluating%20these%20visual%20and%20textual%20search%20abilities%20is%20still%20difficult%2C%20and%20existing%20benchmarks%20have%20two%20major%20limitations.%20First%2C%20existing%20benchmarks%20are%20not%20visual%20search-centric%3A%20answers%20that%20should%20require%20visual%20search%20are%20often%20leaked%20through%20cross-textual%20cues%20in%20the%20text%20questions%20or%20can%20be%20inferred%20from%20the%20prior%20world%20knowledge%20in%20current%20MLLMs.%20Second%2C%20overly%20idealized%20evaluation%20scenario%3A%20On%20the%20image-search%20side%2C%20the%20required%20information%20can%20often%20be%20obtained%20via%20near-exact%20matching%20against%20the%20full%20image%2C%20while%20the%20text-search%20side%20is%20overly%20direct%20and%20insufficiently%20challenging.%20To%20address%20these%20issues%2C%20we%20construct%20the%20Vision-DeepResearch%20benchmark%20%28VDR-Bench%29%20comprising%202%2C000%20VQA%20instances.%20All%20questions%20are%20created%20via%20a%20careful%2C%20multi-stage%20curation%20pipeline%20and%20rigorous%20expert%20review%2C%20designed%20to%20assess%20the%20behavior%20of%20Vision-DeepResearch%20systems%20under%20realistic%20real-world%20conditions.%20Moreover%2C%20to%20address%20the%20insufficient%20visual%20retrieval%20capabilities%20of%20current%20MLLMs%2C%20we%20propose%20a%20simple%20multi-round%20cropped-search%20workflow.%20This%20strategy%20is%20shown%20to%20effectively%20improve%20model%20performance%20in%20realistic%20visual%20retrieval%20scenarios.%20Overall%2C%20our%20results%20provide%20practical%20guidance%20for%20the%20design%20of%20future%20multimodal%20deep-research%20systems.%20The%20code%20will%20be%20released%20in%20https%3A//github.com/Osilly/Vision-DeepResearch.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-DeepResearch%2520Benchmark%253A%2520Rethinking%2520Visual%2520and%2520Textual%2520Search%2520for%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DYu%2520Zeng%2520and%2520Wenxuan%2520Huang%2520and%2520Zhen%2520Fang%2520and%2520Shuang%2520Chen%2520and%2520Yufan%2520Shen%2520and%2520Yishuo%2520Cai%2520and%2520Xiaoman%2520Wang%2520and%2520Zhenfei%2520Yin%2520and%2520Lin%2520Chen%2520and%2520Zehui%2520Chen%2520and%2520Shiting%2520Huang%2520and%2520Yiming%2520Zhao%2520and%2520Yao%2520Hu%2520and%2520Philip%2520Torr%2520and%2520Wanli%2520Ouyang%2520and%2520Shaosheng%2520Cao%26entry.1292438233%3DMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520advanced%2520VQA%2520and%2520now%2520support%2520Vision-DeepResearch%2520systems%2520that%2520use%2520search%2520engines%2520for%2520complex%2520visual-textual%2520fact-finding.%2520However%252C%2520evaluating%2520these%2520visual%2520and%2520textual%2520search%2520abilities%2520is%2520still%2520difficult%252C%2520and%2520existing%2520benchmarks%2520have%2520two%2520major%2520limitations.%2520First%252C%2520existing%2520benchmarks%2520are%2520not%2520visual%2520search-centric%253A%2520answers%2520that%2520should%2520require%2520visual%2520search%2520are%2520often%2520leaked%2520through%2520cross-textual%2520cues%2520in%2520the%2520text%2520questions%2520or%2520can%2520be%2520inferred%2520from%2520the%2520prior%2520world%2520knowledge%2520in%2520current%2520MLLMs.%2520Second%252C%2520overly%2520idealized%2520evaluation%2520scenario%253A%2520On%2520the%2520image-search%2520side%252C%2520the%2520required%2520information%2520can%2520often%2520be%2520obtained%2520via%2520near-exact%2520matching%2520against%2520the%2520full%2520image%252C%2520while%2520the%2520text-search%2520side%2520is%2520overly%2520direct%2520and%2520insufficiently%2520challenging.%2520To%2520address%2520these%2520issues%252C%2520we%2520construct%2520the%2520Vision-DeepResearch%2520benchmark%2520%2528VDR-Bench%2529%2520comprising%25202%252C000%2520VQA%2520instances.%2520All%2520questions%2520are%2520created%2520via%2520a%2520careful%252C%2520multi-stage%2520curation%2520pipeline%2520and%2520rigorous%2520expert%2520review%252C%2520designed%2520to%2520assess%2520the%2520behavior%2520of%2520Vision-DeepResearch%2520systems%2520under%2520realistic%2520real-world%2520conditions.%2520Moreover%252C%2520to%2520address%2520the%2520insufficient%2520visual%2520retrieval%2520capabilities%2520of%2520current%2520MLLMs%252C%2520we%2520propose%2520a%2520simple%2520multi-round%2520cropped-search%2520workflow.%2520This%2520strategy%2520is%2520shown%2520to%2520effectively%2520improve%2520model%2520performance%2520in%2520realistic%2520visual%2520retrieval%2520scenarios.%2520Overall%252C%2520our%2520results%2520provide%2520practical%2520guidance%2520for%2520the%2520design%2520of%2520future%2520multimodal%2520deep-research%2520systems.%2520The%2520code%2520will%2520be%2520released%2520in%2520https%253A//github.com/Osilly/Vision-DeepResearch.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-DeepResearch%20Benchmark%3A%20Rethinking%20Visual%20and%20Textual%20Search%20for%20Multimodal%20Large%20Language%20Models&entry.906535625=Yu%20Zeng%20and%20Wenxuan%20Huang%20and%20Zhen%20Fang%20and%20Shuang%20Chen%20and%20Yufan%20Shen%20and%20Yishuo%20Cai%20and%20Xiaoman%20Wang%20and%20Zhenfei%20Yin%20and%20Lin%20Chen%20and%20Zehui%20Chen%20and%20Shiting%20Huang%20and%20Yiming%20Zhao%20and%20Yao%20Hu%20and%20Philip%20Torr%20and%20Wanli%20Ouyang%20and%20Shaosheng%20Cao&entry.1292438233=Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20advanced%20VQA%20and%20now%20support%20Vision-DeepResearch%20systems%20that%20use%20search%20engines%20for%20complex%20visual-textual%20fact-finding.%20However%2C%20evaluating%20these%20visual%20and%20textual%20search%20abilities%20is%20still%20difficult%2C%20and%20existing%20benchmarks%20have%20two%20major%20limitations.%20First%2C%20existing%20benchmarks%20are%20not%20visual%20search-centric%3A%20answers%20that%20should%20require%20visual%20search%20are%20often%20leaked%20through%20cross-textual%20cues%20in%20the%20text%20questions%20or%20can%20be%20inferred%20from%20the%20prior%20world%20knowledge%20in%20current%20MLLMs.%20Second%2C%20overly%20idealized%20evaluation%20scenario%3A%20On%20the%20image-search%20side%2C%20the%20required%20information%20can%20often%20be%20obtained%20via%20near-exact%20matching%20against%20the%20full%20image%2C%20while%20the%20text-search%20side%20is%20overly%20direct%20and%20insufficiently%20challenging.%20To%20address%20these%20issues%2C%20we%20construct%20the%20Vision-DeepResearch%20benchmark%20%28VDR-Bench%29%20comprising%202%2C000%20VQA%20instances.%20All%20questions%20are%20created%20via%20a%20careful%2C%20multi-stage%20curation%20pipeline%20and%20rigorous%20expert%20review%2C%20designed%20to%20assess%20the%20behavior%20of%20Vision-DeepResearch%20systems%20under%20realistic%20real-world%20conditions.%20Moreover%2C%20to%20address%20the%20insufficient%20visual%20retrieval%20capabilities%20of%20current%20MLLMs%2C%20we%20propose%20a%20simple%20multi-round%20cropped-search%20workflow.%20This%20strategy%20is%20shown%20to%20effectively%20improve%20model%20performance%20in%20realistic%20visual%20retrieval%20scenarios.%20Overall%2C%20our%20results%20provide%20practical%20guidance%20for%20the%20design%20of%20future%20multimodal%20deep-research%20systems.%20The%20code%20will%20be%20released%20in%20https%3A//github.com/Osilly/Vision-DeepResearch.&entry.1838667208=http%3A//arxiv.org/abs/2602.02185v1&entry.124074799=Read"},
{"title": "UM-Text: A Unified Multimodal Model for Image Understanding and Visual Text Editing", "author": "Lichen Ma and Xiaolong Fu and Gaojing Zhou and Zipeng Guo and Ting Zhu and Yichun Liu and Yu Shi and Jason Li and Junshi Huang", "abstract": "With the rapid advancement of image generation, visual text editing using natural language instructions has received increasing attention. The main challenge of this task is to fully understand the instruction and reference image, and thus generate visual text that is style-consistent with the image. Previous methods often involve complex steps of specifying the text content and attributes, such as font size, color, and layout, without considering the stylistic consistency with the reference image. To address this, we propose UM-Text, a unified multimodal model for context understanding and visual text editing by natural language instructions. Specifically, we introduce a Visual Language Model (VLM) to process the instruction and reference image, so that the text content and layout can be elaborately designed according to the context information. To generate an accurate and harmonious visual text image, we further propose the UM-Encoder to combine the embeddings of various condition information, where the combination is automatically configured by VLM according to the input instruction. During training, we propose a regional consistency loss to offer more effective supervision for glyph generation on both latent and RGB space, and design a tailored three-stage training strategy to further enhance model performance. In addition, we contribute the UM-DATA-200K, a large-scale visual text image dataset on diverse scenes for model training. Extensive qualitative and quantitative results on multiple public benchmarks demonstrate that our method achieves state-of-the-art performance.", "link": "http://arxiv.org/abs/2601.08321v2", "date": "2026-02-02", "relevancy": 2.408, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6492}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5927}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UM-Text%3A%20A%20Unified%20Multimodal%20Model%20for%20Image%20Understanding%20and%20Visual%20Text%20Editing&body=Title%3A%20UM-Text%3A%20A%20Unified%20Multimodal%20Model%20for%20Image%20Understanding%20and%20Visual%20Text%20Editing%0AAuthor%3A%20Lichen%20Ma%20and%20Xiaolong%20Fu%20and%20Gaojing%20Zhou%20and%20Zipeng%20Guo%20and%20Ting%20Zhu%20and%20Yichun%20Liu%20and%20Yu%20Shi%20and%20Jason%20Li%20and%20Junshi%20Huang%0AAbstract%3A%20With%20the%20rapid%20advancement%20of%20image%20generation%2C%20visual%20text%20editing%20using%20natural%20language%20instructions%20has%20received%20increasing%20attention.%20The%20main%20challenge%20of%20this%20task%20is%20to%20fully%20understand%20the%20instruction%20and%20reference%20image%2C%20and%20thus%20generate%20visual%20text%20that%20is%20style-consistent%20with%20the%20image.%20Previous%20methods%20often%20involve%20complex%20steps%20of%20specifying%20the%20text%20content%20and%20attributes%2C%20such%20as%20font%20size%2C%20color%2C%20and%20layout%2C%20without%20considering%20the%20stylistic%20consistency%20with%20the%20reference%20image.%20To%20address%20this%2C%20we%20propose%20UM-Text%2C%20a%20unified%20multimodal%20model%20for%20context%20understanding%20and%20visual%20text%20editing%20by%20natural%20language%20instructions.%20Specifically%2C%20we%20introduce%20a%20Visual%20Language%20Model%20%28VLM%29%20to%20process%20the%20instruction%20and%20reference%20image%2C%20so%20that%20the%20text%20content%20and%20layout%20can%20be%20elaborately%20designed%20according%20to%20the%20context%20information.%20To%20generate%20an%20accurate%20and%20harmonious%20visual%20text%20image%2C%20we%20further%20propose%20the%20UM-Encoder%20to%20combine%20the%20embeddings%20of%20various%20condition%20information%2C%20where%20the%20combination%20is%20automatically%20configured%20by%20VLM%20according%20to%20the%20input%20instruction.%20During%20training%2C%20we%20propose%20a%20regional%20consistency%20loss%20to%20offer%20more%20effective%20supervision%20for%20glyph%20generation%20on%20both%20latent%20and%20RGB%20space%2C%20and%20design%20a%20tailored%20three-stage%20training%20strategy%20to%20further%20enhance%20model%20performance.%20In%20addition%2C%20we%20contribute%20the%20UM-DATA-200K%2C%20a%20large-scale%20visual%20text%20image%20dataset%20on%20diverse%20scenes%20for%20model%20training.%20Extensive%20qualitative%20and%20quantitative%20results%20on%20multiple%20public%20benchmarks%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08321v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUM-Text%253A%2520A%2520Unified%2520Multimodal%2520Model%2520for%2520Image%2520Understanding%2520and%2520Visual%2520Text%2520Editing%26entry.906535625%3DLichen%2520Ma%2520and%2520Xiaolong%2520Fu%2520and%2520Gaojing%2520Zhou%2520and%2520Zipeng%2520Guo%2520and%2520Ting%2520Zhu%2520and%2520Yichun%2520Liu%2520and%2520Yu%2520Shi%2520and%2520Jason%2520Li%2520and%2520Junshi%2520Huang%26entry.1292438233%3DWith%2520the%2520rapid%2520advancement%2520of%2520image%2520generation%252C%2520visual%2520text%2520editing%2520using%2520natural%2520language%2520instructions%2520has%2520received%2520increasing%2520attention.%2520The%2520main%2520challenge%2520of%2520this%2520task%2520is%2520to%2520fully%2520understand%2520the%2520instruction%2520and%2520reference%2520image%252C%2520and%2520thus%2520generate%2520visual%2520text%2520that%2520is%2520style-consistent%2520with%2520the%2520image.%2520Previous%2520methods%2520often%2520involve%2520complex%2520steps%2520of%2520specifying%2520the%2520text%2520content%2520and%2520attributes%252C%2520such%2520as%2520font%2520size%252C%2520color%252C%2520and%2520layout%252C%2520without%2520considering%2520the%2520stylistic%2520consistency%2520with%2520the%2520reference%2520image.%2520To%2520address%2520this%252C%2520we%2520propose%2520UM-Text%252C%2520a%2520unified%2520multimodal%2520model%2520for%2520context%2520understanding%2520and%2520visual%2520text%2520editing%2520by%2520natural%2520language%2520instructions.%2520Specifically%252C%2520we%2520introduce%2520a%2520Visual%2520Language%2520Model%2520%2528VLM%2529%2520to%2520process%2520the%2520instruction%2520and%2520reference%2520image%252C%2520so%2520that%2520the%2520text%2520content%2520and%2520layout%2520can%2520be%2520elaborately%2520designed%2520according%2520to%2520the%2520context%2520information.%2520To%2520generate%2520an%2520accurate%2520and%2520harmonious%2520visual%2520text%2520image%252C%2520we%2520further%2520propose%2520the%2520UM-Encoder%2520to%2520combine%2520the%2520embeddings%2520of%2520various%2520condition%2520information%252C%2520where%2520the%2520combination%2520is%2520automatically%2520configured%2520by%2520VLM%2520according%2520to%2520the%2520input%2520instruction.%2520During%2520training%252C%2520we%2520propose%2520a%2520regional%2520consistency%2520loss%2520to%2520offer%2520more%2520effective%2520supervision%2520for%2520glyph%2520generation%2520on%2520both%2520latent%2520and%2520RGB%2520space%252C%2520and%2520design%2520a%2520tailored%2520three-stage%2520training%2520strategy%2520to%2520further%2520enhance%2520model%2520performance.%2520In%2520addition%252C%2520we%2520contribute%2520the%2520UM-DATA-200K%252C%2520a%2520large-scale%2520visual%2520text%2520image%2520dataset%2520on%2520diverse%2520scenes%2520for%2520model%2520training.%2520Extensive%2520qualitative%2520and%2520quantitative%2520results%2520on%2520multiple%2520public%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08321v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UM-Text%3A%20A%20Unified%20Multimodal%20Model%20for%20Image%20Understanding%20and%20Visual%20Text%20Editing&entry.906535625=Lichen%20Ma%20and%20Xiaolong%20Fu%20and%20Gaojing%20Zhou%20and%20Zipeng%20Guo%20and%20Ting%20Zhu%20and%20Yichun%20Liu%20and%20Yu%20Shi%20and%20Jason%20Li%20and%20Junshi%20Huang&entry.1292438233=With%20the%20rapid%20advancement%20of%20image%20generation%2C%20visual%20text%20editing%20using%20natural%20language%20instructions%20has%20received%20increasing%20attention.%20The%20main%20challenge%20of%20this%20task%20is%20to%20fully%20understand%20the%20instruction%20and%20reference%20image%2C%20and%20thus%20generate%20visual%20text%20that%20is%20style-consistent%20with%20the%20image.%20Previous%20methods%20often%20involve%20complex%20steps%20of%20specifying%20the%20text%20content%20and%20attributes%2C%20such%20as%20font%20size%2C%20color%2C%20and%20layout%2C%20without%20considering%20the%20stylistic%20consistency%20with%20the%20reference%20image.%20To%20address%20this%2C%20we%20propose%20UM-Text%2C%20a%20unified%20multimodal%20model%20for%20context%20understanding%20and%20visual%20text%20editing%20by%20natural%20language%20instructions.%20Specifically%2C%20we%20introduce%20a%20Visual%20Language%20Model%20%28VLM%29%20to%20process%20the%20instruction%20and%20reference%20image%2C%20so%20that%20the%20text%20content%20and%20layout%20can%20be%20elaborately%20designed%20according%20to%20the%20context%20information.%20To%20generate%20an%20accurate%20and%20harmonious%20visual%20text%20image%2C%20we%20further%20propose%20the%20UM-Encoder%20to%20combine%20the%20embeddings%20of%20various%20condition%20information%2C%20where%20the%20combination%20is%20automatically%20configured%20by%20VLM%20according%20to%20the%20input%20instruction.%20During%20training%2C%20we%20propose%20a%20regional%20consistency%20loss%20to%20offer%20more%20effective%20supervision%20for%20glyph%20generation%20on%20both%20latent%20and%20RGB%20space%2C%20and%20design%20a%20tailored%20three-stage%20training%20strategy%20to%20further%20enhance%20model%20performance.%20In%20addition%2C%20we%20contribute%20the%20UM-DATA-200K%2C%20a%20large-scale%20visual%20text%20image%20dataset%20on%20diverse%20scenes%20for%20model%20training.%20Extensive%20qualitative%20and%20quantitative%20results%20on%20multiple%20public%20benchmarks%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2601.08321v2&entry.124074799=Read"},
{"title": "Hippasus: Effective and Efficient Automatic Feature Augmentation for Machine Learning Tasks on Relational Data", "author": "Serafeim Papadias and Kostas Patroumpas and Dimitrios Skoutas", "abstract": "Machine learning models depend critically on feature quality, yet useful features are often scattered across multiple relational tables. Feature augmentation enriches a base table by discovering and integrating features from related tables through join operations. However, scaling this process to complex schemas with many tables and multi-hop paths remains challenging. Feature augmentation must address three core tasks: identify promising join paths that connect the base table to candidate tables, execute these joins to materialize augmented data, and select the most informative features from the results. Existing approaches face a fundamental tradeoff between effectiveness and efficiency: achieving high accuracy requires exploring many candidate paths, but exhaustive exploration is computationally prohibitive. Some methods compromise by considering only immediate neighbors, limiting their effectiveness, while others employ neural models that require expensive training data and introduce scalability limitations. We present Hippasus, a modular framework that achieves both goals through three key contributions. First, we combine lightweight statistical signals with semantic reasoning from Large Language Models to prune unpromising join paths before execution, focusing computational resources on high-quality candidates. Second, we employ optimized multi-way join algorithms and consolidate features from multiple paths, substantially reducing execution time. Third, we integrate LLM-based semantic understanding with statistical measures to select features that are both semantically meaningful and empirically predictive. Our experimental evaluation on publicly available datasets shows that Hippasus substantially improves feature augmentation accuracy by up to 26.8% over state-of-the-art baselines while also offering high runtime performance.", "link": "http://arxiv.org/abs/2602.02025v1", "date": "2026-02-02", "relevancy": 2.4077, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5006}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4767}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hippasus%3A%20Effective%20and%20Efficient%20Automatic%20Feature%20Augmentation%20for%20Machine%20Learning%20Tasks%20on%20Relational%20Data&body=Title%3A%20Hippasus%3A%20Effective%20and%20Efficient%20Automatic%20Feature%20Augmentation%20for%20Machine%20Learning%20Tasks%20on%20Relational%20Data%0AAuthor%3A%20Serafeim%20Papadias%20and%20Kostas%20Patroumpas%20and%20Dimitrios%20Skoutas%0AAbstract%3A%20Machine%20learning%20models%20depend%20critically%20on%20feature%20quality%2C%20yet%20useful%20features%20are%20often%20scattered%20across%20multiple%20relational%20tables.%20Feature%20augmentation%20enriches%20a%20base%20table%20by%20discovering%20and%20integrating%20features%20from%20related%20tables%20through%20join%20operations.%20However%2C%20scaling%20this%20process%20to%20complex%20schemas%20with%20many%20tables%20and%20multi-hop%20paths%20remains%20challenging.%20Feature%20augmentation%20must%20address%20three%20core%20tasks%3A%20identify%20promising%20join%20paths%20that%20connect%20the%20base%20table%20to%20candidate%20tables%2C%20execute%20these%20joins%20to%20materialize%20augmented%20data%2C%20and%20select%20the%20most%20informative%20features%20from%20the%20results.%20Existing%20approaches%20face%20a%20fundamental%20tradeoff%20between%20effectiveness%20and%20efficiency%3A%20achieving%20high%20accuracy%20requires%20exploring%20many%20candidate%20paths%2C%20but%20exhaustive%20exploration%20is%20computationally%20prohibitive.%20Some%20methods%20compromise%20by%20considering%20only%20immediate%20neighbors%2C%20limiting%20their%20effectiveness%2C%20while%20others%20employ%20neural%20models%20that%20require%20expensive%20training%20data%20and%20introduce%20scalability%20limitations.%20We%20present%20Hippasus%2C%20a%20modular%20framework%20that%20achieves%20both%20goals%20through%20three%20key%20contributions.%20First%2C%20we%20combine%20lightweight%20statistical%20signals%20with%20semantic%20reasoning%20from%20Large%20Language%20Models%20to%20prune%20unpromising%20join%20paths%20before%20execution%2C%20focusing%20computational%20resources%20on%20high-quality%20candidates.%20Second%2C%20we%20employ%20optimized%20multi-way%20join%20algorithms%20and%20consolidate%20features%20from%20multiple%20paths%2C%20substantially%20reducing%20execution%20time.%20Third%2C%20we%20integrate%20LLM-based%20semantic%20understanding%20with%20statistical%20measures%20to%20select%20features%20that%20are%20both%20semantically%20meaningful%20and%20empirically%20predictive.%20Our%20experimental%20evaluation%20on%20publicly%20available%20datasets%20shows%20that%20Hippasus%20substantially%20improves%20feature%20augmentation%20accuracy%20by%20up%20to%2026.8%25%20over%20state-of-the-art%20baselines%20while%20also%20offering%20high%20runtime%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02025v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHippasus%253A%2520Effective%2520and%2520Efficient%2520Automatic%2520Feature%2520Augmentation%2520for%2520Machine%2520Learning%2520Tasks%2520on%2520Relational%2520Data%26entry.906535625%3DSerafeim%2520Papadias%2520and%2520Kostas%2520Patroumpas%2520and%2520Dimitrios%2520Skoutas%26entry.1292438233%3DMachine%2520learning%2520models%2520depend%2520critically%2520on%2520feature%2520quality%252C%2520yet%2520useful%2520features%2520are%2520often%2520scattered%2520across%2520multiple%2520relational%2520tables.%2520Feature%2520augmentation%2520enriches%2520a%2520base%2520table%2520by%2520discovering%2520and%2520integrating%2520features%2520from%2520related%2520tables%2520through%2520join%2520operations.%2520However%252C%2520scaling%2520this%2520process%2520to%2520complex%2520schemas%2520with%2520many%2520tables%2520and%2520multi-hop%2520paths%2520remains%2520challenging.%2520Feature%2520augmentation%2520must%2520address%2520three%2520core%2520tasks%253A%2520identify%2520promising%2520join%2520paths%2520that%2520connect%2520the%2520base%2520table%2520to%2520candidate%2520tables%252C%2520execute%2520these%2520joins%2520to%2520materialize%2520augmented%2520data%252C%2520and%2520select%2520the%2520most%2520informative%2520features%2520from%2520the%2520results.%2520Existing%2520approaches%2520face%2520a%2520fundamental%2520tradeoff%2520between%2520effectiveness%2520and%2520efficiency%253A%2520achieving%2520high%2520accuracy%2520requires%2520exploring%2520many%2520candidate%2520paths%252C%2520but%2520exhaustive%2520exploration%2520is%2520computationally%2520prohibitive.%2520Some%2520methods%2520compromise%2520by%2520considering%2520only%2520immediate%2520neighbors%252C%2520limiting%2520their%2520effectiveness%252C%2520while%2520others%2520employ%2520neural%2520models%2520that%2520require%2520expensive%2520training%2520data%2520and%2520introduce%2520scalability%2520limitations.%2520We%2520present%2520Hippasus%252C%2520a%2520modular%2520framework%2520that%2520achieves%2520both%2520goals%2520through%2520three%2520key%2520contributions.%2520First%252C%2520we%2520combine%2520lightweight%2520statistical%2520signals%2520with%2520semantic%2520reasoning%2520from%2520Large%2520Language%2520Models%2520to%2520prune%2520unpromising%2520join%2520paths%2520before%2520execution%252C%2520focusing%2520computational%2520resources%2520on%2520high-quality%2520candidates.%2520Second%252C%2520we%2520employ%2520optimized%2520multi-way%2520join%2520algorithms%2520and%2520consolidate%2520features%2520from%2520multiple%2520paths%252C%2520substantially%2520reducing%2520execution%2520time.%2520Third%252C%2520we%2520integrate%2520LLM-based%2520semantic%2520understanding%2520with%2520statistical%2520measures%2520to%2520select%2520features%2520that%2520are%2520both%2520semantically%2520meaningful%2520and%2520empirically%2520predictive.%2520Our%2520experimental%2520evaluation%2520on%2520publicly%2520available%2520datasets%2520shows%2520that%2520Hippasus%2520substantially%2520improves%2520feature%2520augmentation%2520accuracy%2520by%2520up%2520to%252026.8%2525%2520over%2520state-of-the-art%2520baselines%2520while%2520also%2520offering%2520high%2520runtime%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02025v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hippasus%3A%20Effective%20and%20Efficient%20Automatic%20Feature%20Augmentation%20for%20Machine%20Learning%20Tasks%20on%20Relational%20Data&entry.906535625=Serafeim%20Papadias%20and%20Kostas%20Patroumpas%20and%20Dimitrios%20Skoutas&entry.1292438233=Machine%20learning%20models%20depend%20critically%20on%20feature%20quality%2C%20yet%20useful%20features%20are%20often%20scattered%20across%20multiple%20relational%20tables.%20Feature%20augmentation%20enriches%20a%20base%20table%20by%20discovering%20and%20integrating%20features%20from%20related%20tables%20through%20join%20operations.%20However%2C%20scaling%20this%20process%20to%20complex%20schemas%20with%20many%20tables%20and%20multi-hop%20paths%20remains%20challenging.%20Feature%20augmentation%20must%20address%20three%20core%20tasks%3A%20identify%20promising%20join%20paths%20that%20connect%20the%20base%20table%20to%20candidate%20tables%2C%20execute%20these%20joins%20to%20materialize%20augmented%20data%2C%20and%20select%20the%20most%20informative%20features%20from%20the%20results.%20Existing%20approaches%20face%20a%20fundamental%20tradeoff%20between%20effectiveness%20and%20efficiency%3A%20achieving%20high%20accuracy%20requires%20exploring%20many%20candidate%20paths%2C%20but%20exhaustive%20exploration%20is%20computationally%20prohibitive.%20Some%20methods%20compromise%20by%20considering%20only%20immediate%20neighbors%2C%20limiting%20their%20effectiveness%2C%20while%20others%20employ%20neural%20models%20that%20require%20expensive%20training%20data%20and%20introduce%20scalability%20limitations.%20We%20present%20Hippasus%2C%20a%20modular%20framework%20that%20achieves%20both%20goals%20through%20three%20key%20contributions.%20First%2C%20we%20combine%20lightweight%20statistical%20signals%20with%20semantic%20reasoning%20from%20Large%20Language%20Models%20to%20prune%20unpromising%20join%20paths%20before%20execution%2C%20focusing%20computational%20resources%20on%20high-quality%20candidates.%20Second%2C%20we%20employ%20optimized%20multi-way%20join%20algorithms%20and%20consolidate%20features%20from%20multiple%20paths%2C%20substantially%20reducing%20execution%20time.%20Third%2C%20we%20integrate%20LLM-based%20semantic%20understanding%20with%20statistical%20measures%20to%20select%20features%20that%20are%20both%20semantically%20meaningful%20and%20empirically%20predictive.%20Our%20experimental%20evaluation%20on%20publicly%20available%20datasets%20shows%20that%20Hippasus%20substantially%20improves%20feature%20augmentation%20accuracy%20by%20up%20to%2026.8%25%20over%20state-of-the-art%20baselines%20while%20also%20offering%20high%20runtime%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2602.02025v1&entry.124074799=Read"},
{"title": "Artificial Intelligence and Symmetries: Learning, Encoding, and Discovering Structure in Physical Data", "author": "Veronica Sanz", "abstract": "Symmetries play a central role in physics, organizing dynamics, constraining interactions, and determining the effective number of physical degrees of freedom. In parallel, modern artificial intelligence methods have demonstrated a remarkable ability to extract low-dimensional structure from high-dimensional data through representation learning. This review examines the interplay between these two perspectives, focusing on the extent to which symmetry-induced constraints can be identified, encoded, or diagnosed using machine learning techniques.\n  Rather than emphasizing architectures that enforce known symmetries by construction, we concentrate on data-driven approaches and latent representation learning, with particular attention to variational autoencoders. We discuss how symmetries and conservation laws reduce the intrinsic dimensionality of physical datasets, and how this reduction may manifest itself through self-organization of latent spaces in generative models trained to balance reconstruction and compression. We review recent results, including case studies from simple geometric systems and particle physics processes, and analyze the theoretical and practical limitations of inferring symmetry structure without explicit inductive bias.", "link": "http://arxiv.org/abs/2602.02351v1", "date": "2026-02-02", "relevancy": 2.4065, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4967}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4801}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artificial%20Intelligence%20and%20Symmetries%3A%20Learning%2C%20Encoding%2C%20and%20Discovering%20Structure%20in%20Physical%20Data&body=Title%3A%20Artificial%20Intelligence%20and%20Symmetries%3A%20Learning%2C%20Encoding%2C%20and%20Discovering%20Structure%20in%20Physical%20Data%0AAuthor%3A%20Veronica%20Sanz%0AAbstract%3A%20Symmetries%20play%20a%20central%20role%20in%20physics%2C%20organizing%20dynamics%2C%20constraining%20interactions%2C%20and%20determining%20the%20effective%20number%20of%20physical%20degrees%20of%20freedom.%20In%20parallel%2C%20modern%20artificial%20intelligence%20methods%20have%20demonstrated%20a%20remarkable%20ability%20to%20extract%20low-dimensional%20structure%20from%20high-dimensional%20data%20through%20representation%20learning.%20This%20review%20examines%20the%20interplay%20between%20these%20two%20perspectives%2C%20focusing%20on%20the%20extent%20to%20which%20symmetry-induced%20constraints%20can%20be%20identified%2C%20encoded%2C%20or%20diagnosed%20using%20machine%20learning%20techniques.%0A%20%20Rather%20than%20emphasizing%20architectures%20that%20enforce%20known%20symmetries%20by%20construction%2C%20we%20concentrate%20on%20data-driven%20approaches%20and%20latent%20representation%20learning%2C%20with%20particular%20attention%20to%20variational%20autoencoders.%20We%20discuss%20how%20symmetries%20and%20conservation%20laws%20reduce%20the%20intrinsic%20dimensionality%20of%20physical%20datasets%2C%20and%20how%20this%20reduction%20may%20manifest%20itself%20through%20self-organization%20of%20latent%20spaces%20in%20generative%20models%20trained%20to%20balance%20reconstruction%20and%20compression.%20We%20review%20recent%20results%2C%20including%20case%20studies%20from%20simple%20geometric%20systems%20and%20particle%20physics%20processes%2C%20and%20analyze%20the%20theoretical%20and%20practical%20limitations%20of%20inferring%20symmetry%20structure%20without%20explicit%20inductive%20bias.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02351v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtificial%2520Intelligence%2520and%2520Symmetries%253A%2520Learning%252C%2520Encoding%252C%2520and%2520Discovering%2520Structure%2520in%2520Physical%2520Data%26entry.906535625%3DVeronica%2520Sanz%26entry.1292438233%3DSymmetries%2520play%2520a%2520central%2520role%2520in%2520physics%252C%2520organizing%2520dynamics%252C%2520constraining%2520interactions%252C%2520and%2520determining%2520the%2520effective%2520number%2520of%2520physical%2520degrees%2520of%2520freedom.%2520In%2520parallel%252C%2520modern%2520artificial%2520intelligence%2520methods%2520have%2520demonstrated%2520a%2520remarkable%2520ability%2520to%2520extract%2520low-dimensional%2520structure%2520from%2520high-dimensional%2520data%2520through%2520representation%2520learning.%2520This%2520review%2520examines%2520the%2520interplay%2520between%2520these%2520two%2520perspectives%252C%2520focusing%2520on%2520the%2520extent%2520to%2520which%2520symmetry-induced%2520constraints%2520can%2520be%2520identified%252C%2520encoded%252C%2520or%2520diagnosed%2520using%2520machine%2520learning%2520techniques.%250A%2520%2520Rather%2520than%2520emphasizing%2520architectures%2520that%2520enforce%2520known%2520symmetries%2520by%2520construction%252C%2520we%2520concentrate%2520on%2520data-driven%2520approaches%2520and%2520latent%2520representation%2520learning%252C%2520with%2520particular%2520attention%2520to%2520variational%2520autoencoders.%2520We%2520discuss%2520how%2520symmetries%2520and%2520conservation%2520laws%2520reduce%2520the%2520intrinsic%2520dimensionality%2520of%2520physical%2520datasets%252C%2520and%2520how%2520this%2520reduction%2520may%2520manifest%2520itself%2520through%2520self-organization%2520of%2520latent%2520spaces%2520in%2520generative%2520models%2520trained%2520to%2520balance%2520reconstruction%2520and%2520compression.%2520We%2520review%2520recent%2520results%252C%2520including%2520case%2520studies%2520from%2520simple%2520geometric%2520systems%2520and%2520particle%2520physics%2520processes%252C%2520and%2520analyze%2520the%2520theoretical%2520and%2520practical%2520limitations%2520of%2520inferring%2520symmetry%2520structure%2520without%2520explicit%2520inductive%2520bias.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02351v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial%20Intelligence%20and%20Symmetries%3A%20Learning%2C%20Encoding%2C%20and%20Discovering%20Structure%20in%20Physical%20Data&entry.906535625=Veronica%20Sanz&entry.1292438233=Symmetries%20play%20a%20central%20role%20in%20physics%2C%20organizing%20dynamics%2C%20constraining%20interactions%2C%20and%20determining%20the%20effective%20number%20of%20physical%20degrees%20of%20freedom.%20In%20parallel%2C%20modern%20artificial%20intelligence%20methods%20have%20demonstrated%20a%20remarkable%20ability%20to%20extract%20low-dimensional%20structure%20from%20high-dimensional%20data%20through%20representation%20learning.%20This%20review%20examines%20the%20interplay%20between%20these%20two%20perspectives%2C%20focusing%20on%20the%20extent%20to%20which%20symmetry-induced%20constraints%20can%20be%20identified%2C%20encoded%2C%20or%20diagnosed%20using%20machine%20learning%20techniques.%0A%20%20Rather%20than%20emphasizing%20architectures%20that%20enforce%20known%20symmetries%20by%20construction%2C%20we%20concentrate%20on%20data-driven%20approaches%20and%20latent%20representation%20learning%2C%20with%20particular%20attention%20to%20variational%20autoencoders.%20We%20discuss%20how%20symmetries%20and%20conservation%20laws%20reduce%20the%20intrinsic%20dimensionality%20of%20physical%20datasets%2C%20and%20how%20this%20reduction%20may%20manifest%20itself%20through%20self-organization%20of%20latent%20spaces%20in%20generative%20models%20trained%20to%20balance%20reconstruction%20and%20compression.%20We%20review%20recent%20results%2C%20including%20case%20studies%20from%20simple%20geometric%20systems%20and%20particle%20physics%20processes%2C%20and%20analyze%20the%20theoretical%20and%20practical%20limitations%20of%20inferring%20symmetry%20structure%20without%20explicit%20inductive%20bias.&entry.1838667208=http%3A//arxiv.org/abs/2602.02351v1&entry.124074799=Read"},
{"title": "ReasonCACHE: Teaching LLMs To Reason Without Weight Updates", "author": "Sharut Gupta and Phillip Isola and Stefanie Jegelka and David Lopez-Paz and Kartik Ahuja and Mark Ibrahim and Mohammad Pezeshki", "abstract": "Can Large language models (LLMs) learn to reason without any weight update and only through in-context learning (ICL)? ICL is strikingly sample-efficient, often learning from only a handful of demonstrations, but complex reasoning tasks typically demand many training examples to learn from. However, naively scaling ICL by adding more demonstrations breaks down at this scale: attention costs grow quadratically, performance saturates or degrades with longer contexts, and the approach remains a shallow form of learning. Due to these limitations, practitioners predominantly rely on in-weight learning (IWL) to induce reasoning. In this work, we show that by using Prefix Tuning, LLMs can learn to reason without overloading the context window and without any weight updates. We introduce $\\textbf{ReasonCACHE}$, an instantiation of this mechanism that distills demonstrations into a fixed key-value cache. Empirically, across challenging reasoning benchmarks, including GPQA-Diamond, ReasonCACHE outperforms standard ICL and matches or surpasses IWL approaches. Further, it achieves this all while being more efficient across three key axes: data, inference cost, and trainable parameters. We also theoretically prove that ReasonCACHE can be strictly more expressive than low-rank weight update since the latter ties expressivity to input rank, whereas ReasonCACHE bypasses this constraint by directly injecting key-values into the attention mechanism. Together, our findings identify ReasonCACHE as a middle path between in-context and in-weight learning, providing a scalable algorithm for learning reasoning skills beyond the context window without modifying parameters. Our project page: https://reasoncache.github.io/", "link": "http://arxiv.org/abs/2602.02366v1", "date": "2026-02-02", "relevancy": 2.3992, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4845}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReasonCACHE%3A%20Teaching%20LLMs%20To%20Reason%20Without%20Weight%20Updates&body=Title%3A%20ReasonCACHE%3A%20Teaching%20LLMs%20To%20Reason%20Without%20Weight%20Updates%0AAuthor%3A%20Sharut%20Gupta%20and%20Phillip%20Isola%20and%20Stefanie%20Jegelka%20and%20David%20Lopez-Paz%20and%20Kartik%20Ahuja%20and%20Mark%20Ibrahim%20and%20Mohammad%20Pezeshki%0AAbstract%3A%20Can%20Large%20language%20models%20%28LLMs%29%20learn%20to%20reason%20without%20any%20weight%20update%20and%20only%20through%20in-context%20learning%20%28ICL%29%3F%20ICL%20is%20strikingly%20sample-efficient%2C%20often%20learning%20from%20only%20a%20handful%20of%20demonstrations%2C%20but%20complex%20reasoning%20tasks%20typically%20demand%20many%20training%20examples%20to%20learn%20from.%20However%2C%20naively%20scaling%20ICL%20by%20adding%20more%20demonstrations%20breaks%20down%20at%20this%20scale%3A%20attention%20costs%20grow%20quadratically%2C%20performance%20saturates%20or%20degrades%20with%20longer%20contexts%2C%20and%20the%20approach%20remains%20a%20shallow%20form%20of%20learning.%20Due%20to%20these%20limitations%2C%20practitioners%20predominantly%20rely%20on%20in-weight%20learning%20%28IWL%29%20to%20induce%20reasoning.%20In%20this%20work%2C%20we%20show%20that%20by%20using%20Prefix%20Tuning%2C%20LLMs%20can%20learn%20to%20reason%20without%20overloading%20the%20context%20window%20and%20without%20any%20weight%20updates.%20We%20introduce%20%24%5Ctextbf%7BReasonCACHE%7D%24%2C%20an%20instantiation%20of%20this%20mechanism%20that%20distills%20demonstrations%20into%20a%20fixed%20key-value%20cache.%20Empirically%2C%20across%20challenging%20reasoning%20benchmarks%2C%20including%20GPQA-Diamond%2C%20ReasonCACHE%20outperforms%20standard%20ICL%20and%20matches%20or%20surpasses%20IWL%20approaches.%20Further%2C%20it%20achieves%20this%20all%20while%20being%20more%20efficient%20across%20three%20key%20axes%3A%20data%2C%20inference%20cost%2C%20and%20trainable%20parameters.%20We%20also%20theoretically%20prove%20that%20ReasonCACHE%20can%20be%20strictly%20more%20expressive%20than%20low-rank%20weight%20update%20since%20the%20latter%20ties%20expressivity%20to%20input%20rank%2C%20whereas%20ReasonCACHE%20bypasses%20this%20constraint%20by%20directly%20injecting%20key-values%20into%20the%20attention%20mechanism.%20Together%2C%20our%20findings%20identify%20ReasonCACHE%20as%20a%20middle%20path%20between%20in-context%20and%20in-weight%20learning%2C%20providing%20a%20scalable%20algorithm%20for%20learning%20reasoning%20skills%20beyond%20the%20context%20window%20without%20modifying%20parameters.%20Our%20project%20page%3A%20https%3A//reasoncache.github.io/%0ALink%3A%20http%3A//arxiv.org/abs/2602.02366v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasonCACHE%253A%2520Teaching%2520LLMs%2520To%2520Reason%2520Without%2520Weight%2520Updates%26entry.906535625%3DSharut%2520Gupta%2520and%2520Phillip%2520Isola%2520and%2520Stefanie%2520Jegelka%2520and%2520David%2520Lopez-Paz%2520and%2520Kartik%2520Ahuja%2520and%2520Mark%2520Ibrahim%2520and%2520Mohammad%2520Pezeshki%26entry.1292438233%3DCan%2520Large%2520language%2520models%2520%2528LLMs%2529%2520learn%2520to%2520reason%2520without%2520any%2520weight%2520update%2520and%2520only%2520through%2520in-context%2520learning%2520%2528ICL%2529%253F%2520ICL%2520is%2520strikingly%2520sample-efficient%252C%2520often%2520learning%2520from%2520only%2520a%2520handful%2520of%2520demonstrations%252C%2520but%2520complex%2520reasoning%2520tasks%2520typically%2520demand%2520many%2520training%2520examples%2520to%2520learn%2520from.%2520However%252C%2520naively%2520scaling%2520ICL%2520by%2520adding%2520more%2520demonstrations%2520breaks%2520down%2520at%2520this%2520scale%253A%2520attention%2520costs%2520grow%2520quadratically%252C%2520performance%2520saturates%2520or%2520degrades%2520with%2520longer%2520contexts%252C%2520and%2520the%2520approach%2520remains%2520a%2520shallow%2520form%2520of%2520learning.%2520Due%2520to%2520these%2520limitations%252C%2520practitioners%2520predominantly%2520rely%2520on%2520in-weight%2520learning%2520%2528IWL%2529%2520to%2520induce%2520reasoning.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520by%2520using%2520Prefix%2520Tuning%252C%2520LLMs%2520can%2520learn%2520to%2520reason%2520without%2520overloading%2520the%2520context%2520window%2520and%2520without%2520any%2520weight%2520updates.%2520We%2520introduce%2520%2524%255Ctextbf%257BReasonCACHE%257D%2524%252C%2520an%2520instantiation%2520of%2520this%2520mechanism%2520that%2520distills%2520demonstrations%2520into%2520a%2520fixed%2520key-value%2520cache.%2520Empirically%252C%2520across%2520challenging%2520reasoning%2520benchmarks%252C%2520including%2520GPQA-Diamond%252C%2520ReasonCACHE%2520outperforms%2520standard%2520ICL%2520and%2520matches%2520or%2520surpasses%2520IWL%2520approaches.%2520Further%252C%2520it%2520achieves%2520this%2520all%2520while%2520being%2520more%2520efficient%2520across%2520three%2520key%2520axes%253A%2520data%252C%2520inference%2520cost%252C%2520and%2520trainable%2520parameters.%2520We%2520also%2520theoretically%2520prove%2520that%2520ReasonCACHE%2520can%2520be%2520strictly%2520more%2520expressive%2520than%2520low-rank%2520weight%2520update%2520since%2520the%2520latter%2520ties%2520expressivity%2520to%2520input%2520rank%252C%2520whereas%2520ReasonCACHE%2520bypasses%2520this%2520constraint%2520by%2520directly%2520injecting%2520key-values%2520into%2520the%2520attention%2520mechanism.%2520Together%252C%2520our%2520findings%2520identify%2520ReasonCACHE%2520as%2520a%2520middle%2520path%2520between%2520in-context%2520and%2520in-weight%2520learning%252C%2520providing%2520a%2520scalable%2520algorithm%2520for%2520learning%2520reasoning%2520skills%2520beyond%2520the%2520context%2520window%2520without%2520modifying%2520parameters.%2520Our%2520project%2520page%253A%2520https%253A//reasoncache.github.io/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02366v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReasonCACHE%3A%20Teaching%20LLMs%20To%20Reason%20Without%20Weight%20Updates&entry.906535625=Sharut%20Gupta%20and%20Phillip%20Isola%20and%20Stefanie%20Jegelka%20and%20David%20Lopez-Paz%20and%20Kartik%20Ahuja%20and%20Mark%20Ibrahim%20and%20Mohammad%20Pezeshki&entry.1292438233=Can%20Large%20language%20models%20%28LLMs%29%20learn%20to%20reason%20without%20any%20weight%20update%20and%20only%20through%20in-context%20learning%20%28ICL%29%3F%20ICL%20is%20strikingly%20sample-efficient%2C%20often%20learning%20from%20only%20a%20handful%20of%20demonstrations%2C%20but%20complex%20reasoning%20tasks%20typically%20demand%20many%20training%20examples%20to%20learn%20from.%20However%2C%20naively%20scaling%20ICL%20by%20adding%20more%20demonstrations%20breaks%20down%20at%20this%20scale%3A%20attention%20costs%20grow%20quadratically%2C%20performance%20saturates%20or%20degrades%20with%20longer%20contexts%2C%20and%20the%20approach%20remains%20a%20shallow%20form%20of%20learning.%20Due%20to%20these%20limitations%2C%20practitioners%20predominantly%20rely%20on%20in-weight%20learning%20%28IWL%29%20to%20induce%20reasoning.%20In%20this%20work%2C%20we%20show%20that%20by%20using%20Prefix%20Tuning%2C%20LLMs%20can%20learn%20to%20reason%20without%20overloading%20the%20context%20window%20and%20without%20any%20weight%20updates.%20We%20introduce%20%24%5Ctextbf%7BReasonCACHE%7D%24%2C%20an%20instantiation%20of%20this%20mechanism%20that%20distills%20demonstrations%20into%20a%20fixed%20key-value%20cache.%20Empirically%2C%20across%20challenging%20reasoning%20benchmarks%2C%20including%20GPQA-Diamond%2C%20ReasonCACHE%20outperforms%20standard%20ICL%20and%20matches%20or%20surpasses%20IWL%20approaches.%20Further%2C%20it%20achieves%20this%20all%20while%20being%20more%20efficient%20across%20three%20key%20axes%3A%20data%2C%20inference%20cost%2C%20and%20trainable%20parameters.%20We%20also%20theoretically%20prove%20that%20ReasonCACHE%20can%20be%20strictly%20more%20expressive%20than%20low-rank%20weight%20update%20since%20the%20latter%20ties%20expressivity%20to%20input%20rank%2C%20whereas%20ReasonCACHE%20bypasses%20this%20constraint%20by%20directly%20injecting%20key-values%20into%20the%20attention%20mechanism.%20Together%2C%20our%20findings%20identify%20ReasonCACHE%20as%20a%20middle%20path%20between%20in-context%20and%20in-weight%20learning%2C%20providing%20a%20scalable%20algorithm%20for%20learning%20reasoning%20skills%20beyond%20the%20context%20window%20without%20modifying%20parameters.%20Our%20project%20page%3A%20https%3A//reasoncache.github.io/&entry.1838667208=http%3A//arxiv.org/abs/2602.02366v1&entry.124074799=Read"},
{"title": "A Survey on Efficient Vision-Language-Action Models", "author": "Zhaoshu Yu and Bo Wang and Pengpeng Zeng and Haonan Zhang and Ji Zhang and Zheng Wang and Lianli Gao and Jingkuan Song and Nicu Sebe and Heng Tao Shen", "abstract": "Vision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. Despite their remarkable performance, foundational VLAs are hindered by the prohibitive computational and data demands inherent to their large-scale architectures. While a surge of recent research has focused on enhancing VLA efficiency, the field lacks a unified framework to consolidate these disparate advancements. To bridge this gap, this survey presents the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs) across the entire model-training-data pipeline. Specifically, we introduce a unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through a critical review of state-of-the-art methods within this framework, this survey not only establishes a foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts a roadmap for future research. We maintain a continuously updated project page to track our latest developments: https://evla-survey.github.io/.", "link": "http://arxiv.org/abs/2510.24795v2", "date": "2026-02-02", "relevancy": 2.398, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6105}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6105}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Efficient%20Vision-Language-Action%20Models&body=Title%3A%20A%20Survey%20on%20Efficient%20Vision-Language-Action%20Models%0AAuthor%3A%20Zhaoshu%20Yu%20and%20Bo%20Wang%20and%20Pengpeng%20Zeng%20and%20Haonan%20Zhang%20and%20Ji%20Zhang%20and%20Zheng%20Wang%20and%20Lianli%20Gao%20and%20Jingkuan%20Song%20and%20Nicu%20Sebe%20and%20Heng%20Tao%20Shen%0AAbstract%3A%20Vision-Language-Action%20models%20%28VLAs%29%20represent%20a%20significant%20frontier%20in%20embodied%20intelligence%2C%20aiming%20to%20bridge%20digital%20knowledge%20with%20physical-world%20interaction.%20Despite%20their%20remarkable%20performance%2C%20foundational%20VLAs%20are%20hindered%20by%20the%20prohibitive%20computational%20and%20data%20demands%20inherent%20to%20their%20large-scale%20architectures.%20While%20a%20surge%20of%20recent%20research%20has%20focused%20on%20enhancing%20VLA%20efficiency%2C%20the%20field%20lacks%20a%20unified%20framework%20to%20consolidate%20these%20disparate%20advancements.%20To%20bridge%20this%20gap%2C%20this%20survey%20presents%20the%20first%20comprehensive%20review%20of%20Efficient%20Vision-Language-Action%20models%20%28Efficient%20VLAs%29%20across%20the%20entire%20model-training-data%20pipeline.%20Specifically%2C%20we%20introduce%20a%20unified%20taxonomy%20to%20systematically%20organize%20the%20disparate%20efforts%20in%20this%20domain%2C%20categorizing%20current%20techniques%20into%20three%20core%20pillars%3A%20%281%29%20Efficient%20Model%20Design%2C%20focusing%20on%20efficient%20architectures%20and%20model%20compression%3B%20%282%29%20Efficient%20Training%2C%20which%20reduces%20computational%20burdens%20during%20model%20learning%3B%20and%20%283%29%20Efficient%20Data%20Collection%2C%20which%20addresses%20the%20bottlenecks%20in%20acquiring%20and%20utilizing%20robotic%20data.%20Through%20a%20critical%20review%20of%20state-of-the-art%20methods%20within%20this%20framework%2C%20this%20survey%20not%20only%20establishes%20a%20foundational%20reference%20for%20the%20community%20but%20also%20summarizes%20representative%20applications%2C%20delineates%20key%20challenges%2C%20and%20charts%20a%20roadmap%20for%20future%20research.%20We%20maintain%20a%20continuously%20updated%20project%20page%20to%20track%20our%20latest%20developments%3A%20https%3A//evla-survey.github.io/.%0ALink%3A%20http%3A//arxiv.org/abs/2510.24795v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Efficient%2520Vision-Language-Action%2520Models%26entry.906535625%3DZhaoshu%2520Yu%2520and%2520Bo%2520Wang%2520and%2520Pengpeng%2520Zeng%2520and%2520Haonan%2520Zhang%2520and%2520Ji%2520Zhang%2520and%2520Zheng%2520Wang%2520and%2520Lianli%2520Gao%2520and%2520Jingkuan%2520Song%2520and%2520Nicu%2520Sebe%2520and%2520Heng%2520Tao%2520Shen%26entry.1292438233%3DVision-Language-Action%2520models%2520%2528VLAs%2529%2520represent%2520a%2520significant%2520frontier%2520in%2520embodied%2520intelligence%252C%2520aiming%2520to%2520bridge%2520digital%2520knowledge%2520with%2520physical-world%2520interaction.%2520Despite%2520their%2520remarkable%2520performance%252C%2520foundational%2520VLAs%2520are%2520hindered%2520by%2520the%2520prohibitive%2520computational%2520and%2520data%2520demands%2520inherent%2520to%2520their%2520large-scale%2520architectures.%2520While%2520a%2520surge%2520of%2520recent%2520research%2520has%2520focused%2520on%2520enhancing%2520VLA%2520efficiency%252C%2520the%2520field%2520lacks%2520a%2520unified%2520framework%2520to%2520consolidate%2520these%2520disparate%2520advancements.%2520To%2520bridge%2520this%2520gap%252C%2520this%2520survey%2520presents%2520the%2520first%2520comprehensive%2520review%2520of%2520Efficient%2520Vision-Language-Action%2520models%2520%2528Efficient%2520VLAs%2529%2520across%2520the%2520entire%2520model-training-data%2520pipeline.%2520Specifically%252C%2520we%2520introduce%2520a%2520unified%2520taxonomy%2520to%2520systematically%2520organize%2520the%2520disparate%2520efforts%2520in%2520this%2520domain%252C%2520categorizing%2520current%2520techniques%2520into%2520three%2520core%2520pillars%253A%2520%25281%2529%2520Efficient%2520Model%2520Design%252C%2520focusing%2520on%2520efficient%2520architectures%2520and%2520model%2520compression%253B%2520%25282%2529%2520Efficient%2520Training%252C%2520which%2520reduces%2520computational%2520burdens%2520during%2520model%2520learning%253B%2520and%2520%25283%2529%2520Efficient%2520Data%2520Collection%252C%2520which%2520addresses%2520the%2520bottlenecks%2520in%2520acquiring%2520and%2520utilizing%2520robotic%2520data.%2520Through%2520a%2520critical%2520review%2520of%2520state-of-the-art%2520methods%2520within%2520this%2520framework%252C%2520this%2520survey%2520not%2520only%2520establishes%2520a%2520foundational%2520reference%2520for%2520the%2520community%2520but%2520also%2520summarizes%2520representative%2520applications%252C%2520delineates%2520key%2520challenges%252C%2520and%2520charts%2520a%2520roadmap%2520for%2520future%2520research.%2520We%2520maintain%2520a%2520continuously%2520updated%2520project%2520page%2520to%2520track%2520our%2520latest%2520developments%253A%2520https%253A//evla-survey.github.io/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.24795v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Efficient%20Vision-Language-Action%20Models&entry.906535625=Zhaoshu%20Yu%20and%20Bo%20Wang%20and%20Pengpeng%20Zeng%20and%20Haonan%20Zhang%20and%20Ji%20Zhang%20and%20Zheng%20Wang%20and%20Lianli%20Gao%20and%20Jingkuan%20Song%20and%20Nicu%20Sebe%20and%20Heng%20Tao%20Shen&entry.1292438233=Vision-Language-Action%20models%20%28VLAs%29%20represent%20a%20significant%20frontier%20in%20embodied%20intelligence%2C%20aiming%20to%20bridge%20digital%20knowledge%20with%20physical-world%20interaction.%20Despite%20their%20remarkable%20performance%2C%20foundational%20VLAs%20are%20hindered%20by%20the%20prohibitive%20computational%20and%20data%20demands%20inherent%20to%20their%20large-scale%20architectures.%20While%20a%20surge%20of%20recent%20research%20has%20focused%20on%20enhancing%20VLA%20efficiency%2C%20the%20field%20lacks%20a%20unified%20framework%20to%20consolidate%20these%20disparate%20advancements.%20To%20bridge%20this%20gap%2C%20this%20survey%20presents%20the%20first%20comprehensive%20review%20of%20Efficient%20Vision-Language-Action%20models%20%28Efficient%20VLAs%29%20across%20the%20entire%20model-training-data%20pipeline.%20Specifically%2C%20we%20introduce%20a%20unified%20taxonomy%20to%20systematically%20organize%20the%20disparate%20efforts%20in%20this%20domain%2C%20categorizing%20current%20techniques%20into%20three%20core%20pillars%3A%20%281%29%20Efficient%20Model%20Design%2C%20focusing%20on%20efficient%20architectures%20and%20model%20compression%3B%20%282%29%20Efficient%20Training%2C%20which%20reduces%20computational%20burdens%20during%20model%20learning%3B%20and%20%283%29%20Efficient%20Data%20Collection%2C%20which%20addresses%20the%20bottlenecks%20in%20acquiring%20and%20utilizing%20robotic%20data.%20Through%20a%20critical%20review%20of%20state-of-the-art%20methods%20within%20this%20framework%2C%20this%20survey%20not%20only%20establishes%20a%20foundational%20reference%20for%20the%20community%20but%20also%20summarizes%20representative%20applications%2C%20delineates%20key%20challenges%2C%20and%20charts%20a%20roadmap%20for%20future%20research.%20We%20maintain%20a%20continuously%20updated%20project%20page%20to%20track%20our%20latest%20developments%3A%20https%3A//evla-survey.github.io/.&entry.1838667208=http%3A//arxiv.org/abs/2510.24795v2&entry.124074799=Read"},
{"title": "ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding", "author": "Ye Chen and Yupeng Zhu and Xiongzhen Zhang and Zhewen Wan and Yingzhe Li and Wenjun Zhang and Bingbing Ni", "abstract": "Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches.", "link": "http://arxiv.org/abs/2602.01881v1", "date": "2026-02-02", "relevancy": 2.3791, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6002}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5971}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProxyImg%3A%20Towards%20Highly-Controllable%20Image%20Representation%20via%20Hierarchical%20Disentangled%20Proxy%20Embedding&body=Title%3A%20ProxyImg%3A%20Towards%20Highly-Controllable%20Image%20Representation%20via%20Hierarchical%20Disentangled%20Proxy%20Embedding%0AAuthor%3A%20Ye%20Chen%20and%20Yupeng%20Zhu%20and%20Xiongzhen%20Zhang%20and%20Zhewen%20Wan%20and%20Yingzhe%20Li%20and%20Wenjun%20Zhang%20and%20Bingbing%20Ni%0AAbstract%3A%20Prevailing%20image%20representation%20methods%2C%20including%20explicit%20representations%20such%20as%20raster%20images%20and%20Gaussian%20primitives%2C%20as%20well%20as%20implicit%20representations%20such%20as%20latent%20images%2C%20either%20suffer%20from%20representation%20redundancy%20that%20leads%20to%20heavy%20manual%20editing%20effort%2C%20or%20lack%20a%20direct%20mapping%20from%20latent%20variables%20to%20semantic%20instances%20or%20parts%2C%20making%20fine-grained%20manipulation%20difficult.%20These%20limitations%20hinder%20efficient%20and%20controllable%20image%20and%20video%20editing.%20To%20address%20these%20issues%2C%20we%20propose%20a%20hierarchical%20proxy-based%20parametric%20image%20representation%20that%20disentangles%20semantic%2C%20geometric%2C%20and%20textural%20attributes%20into%20independent%20and%20manipulable%20parameter%20spaces.%20Based%20on%20a%20semantic-aware%20decomposition%20of%20the%20input%20image%2C%20our%20representation%20constructs%20hierarchical%20proxy%20geometries%20through%20adaptive%20Bezier%20fitting%20and%20iterative%20internal%20region%20subdivision%20and%20meshing.%20Multi-scale%20implicit%20texture%20parameters%20are%20embedded%20into%20the%20resulting%20geometry-aware%20distributed%20proxy%20nodes%2C%20enabling%20continuous%20high-fidelity%20reconstruction%20in%20the%20pixel%20domain%20and%20instance-%20or%20part-independent%20semantic%20editing.%20In%20addition%2C%20we%20introduce%20a%20locality-adaptive%20feature%20indexing%20mechanism%20to%20ensure%20spatial%20texture%20coherence%2C%20which%20further%20supports%20high-quality%20background%20completion%20without%20relying%20on%20generative%20models.%20Extensive%20experiments%20on%20image%20reconstruction%20and%20editing%20benchmarks%2C%20including%20ImageNet%2C%20OIR-Bench%2C%20and%20HumanEdit%2C%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20rendering%20fidelity%20with%20significantly%20fewer%20parameters%2C%20while%20enabling%20intuitive%2C%20interactive%2C%20and%20physically%20plausible%20manipulation.%20Moreover%2C%20by%20integrating%20proxy%20nodes%20with%20Position-Based%20Dynamics%2C%20our%20framework%20supports%20real-time%20physics-driven%20animation%20using%20lightweight%20implicit%20rendering%2C%20achieving%20superior%20temporal%20consistency%20and%20visual%20realism%20compared%20with%20generative%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2602.01881v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProxyImg%253A%2520Towards%2520Highly-Controllable%2520Image%2520Representation%2520via%2520Hierarchical%2520Disentangled%2520Proxy%2520Embedding%26entry.906535625%3DYe%2520Chen%2520and%2520Yupeng%2520Zhu%2520and%2520Xiongzhen%2520Zhang%2520and%2520Zhewen%2520Wan%2520and%2520Yingzhe%2520Li%2520and%2520Wenjun%2520Zhang%2520and%2520Bingbing%2520Ni%26entry.1292438233%3DPrevailing%2520image%2520representation%2520methods%252C%2520including%2520explicit%2520representations%2520such%2520as%2520raster%2520images%2520and%2520Gaussian%2520primitives%252C%2520as%2520well%2520as%2520implicit%2520representations%2520such%2520as%2520latent%2520images%252C%2520either%2520suffer%2520from%2520representation%2520redundancy%2520that%2520leads%2520to%2520heavy%2520manual%2520editing%2520effort%252C%2520or%2520lack%2520a%2520direct%2520mapping%2520from%2520latent%2520variables%2520to%2520semantic%2520instances%2520or%2520parts%252C%2520making%2520fine-grained%2520manipulation%2520difficult.%2520These%2520limitations%2520hinder%2520efficient%2520and%2520controllable%2520image%2520and%2520video%2520editing.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520hierarchical%2520proxy-based%2520parametric%2520image%2520representation%2520that%2520disentangles%2520semantic%252C%2520geometric%252C%2520and%2520textural%2520attributes%2520into%2520independent%2520and%2520manipulable%2520parameter%2520spaces.%2520Based%2520on%2520a%2520semantic-aware%2520decomposition%2520of%2520the%2520input%2520image%252C%2520our%2520representation%2520constructs%2520hierarchical%2520proxy%2520geometries%2520through%2520adaptive%2520Bezier%2520fitting%2520and%2520iterative%2520internal%2520region%2520subdivision%2520and%2520meshing.%2520Multi-scale%2520implicit%2520texture%2520parameters%2520are%2520embedded%2520into%2520the%2520resulting%2520geometry-aware%2520distributed%2520proxy%2520nodes%252C%2520enabling%2520continuous%2520high-fidelity%2520reconstruction%2520in%2520the%2520pixel%2520domain%2520and%2520instance-%2520or%2520part-independent%2520semantic%2520editing.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520locality-adaptive%2520feature%2520indexing%2520mechanism%2520to%2520ensure%2520spatial%2520texture%2520coherence%252C%2520which%2520further%2520supports%2520high-quality%2520background%2520completion%2520without%2520relying%2520on%2520generative%2520models.%2520Extensive%2520experiments%2520on%2520image%2520reconstruction%2520and%2520editing%2520benchmarks%252C%2520including%2520ImageNet%252C%2520OIR-Bench%252C%2520and%2520HumanEdit%252C%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520rendering%2520fidelity%2520with%2520significantly%2520fewer%2520parameters%252C%2520while%2520enabling%2520intuitive%252C%2520interactive%252C%2520and%2520physically%2520plausible%2520manipulation.%2520Moreover%252C%2520by%2520integrating%2520proxy%2520nodes%2520with%2520Position-Based%2520Dynamics%252C%2520our%2520framework%2520supports%2520real-time%2520physics-driven%2520animation%2520using%2520lightweight%2520implicit%2520rendering%252C%2520achieving%2520superior%2520temporal%2520consistency%2520and%2520visual%2520realism%2520compared%2520with%2520generative%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.01881v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProxyImg%3A%20Towards%20Highly-Controllable%20Image%20Representation%20via%20Hierarchical%20Disentangled%20Proxy%20Embedding&entry.906535625=Ye%20Chen%20and%20Yupeng%20Zhu%20and%20Xiongzhen%20Zhang%20and%20Zhewen%20Wan%20and%20Yingzhe%20Li%20and%20Wenjun%20Zhang%20and%20Bingbing%20Ni&entry.1292438233=Prevailing%20image%20representation%20methods%2C%20including%20explicit%20representations%20such%20as%20raster%20images%20and%20Gaussian%20primitives%2C%20as%20well%20as%20implicit%20representations%20such%20as%20latent%20images%2C%20either%20suffer%20from%20representation%20redundancy%20that%20leads%20to%20heavy%20manual%20editing%20effort%2C%20or%20lack%20a%20direct%20mapping%20from%20latent%20variables%20to%20semantic%20instances%20or%20parts%2C%20making%20fine-grained%20manipulation%20difficult.%20These%20limitations%20hinder%20efficient%20and%20controllable%20image%20and%20video%20editing.%20To%20address%20these%20issues%2C%20we%20propose%20a%20hierarchical%20proxy-based%20parametric%20image%20representation%20that%20disentangles%20semantic%2C%20geometric%2C%20and%20textural%20attributes%20into%20independent%20and%20manipulable%20parameter%20spaces.%20Based%20on%20a%20semantic-aware%20decomposition%20of%20the%20input%20image%2C%20our%20representation%20constructs%20hierarchical%20proxy%20geometries%20through%20adaptive%20Bezier%20fitting%20and%20iterative%20internal%20region%20subdivision%20and%20meshing.%20Multi-scale%20implicit%20texture%20parameters%20are%20embedded%20into%20the%20resulting%20geometry-aware%20distributed%20proxy%20nodes%2C%20enabling%20continuous%20high-fidelity%20reconstruction%20in%20the%20pixel%20domain%20and%20instance-%20or%20part-independent%20semantic%20editing.%20In%20addition%2C%20we%20introduce%20a%20locality-adaptive%20feature%20indexing%20mechanism%20to%20ensure%20spatial%20texture%20coherence%2C%20which%20further%20supports%20high-quality%20background%20completion%20without%20relying%20on%20generative%20models.%20Extensive%20experiments%20on%20image%20reconstruction%20and%20editing%20benchmarks%2C%20including%20ImageNet%2C%20OIR-Bench%2C%20and%20HumanEdit%2C%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20rendering%20fidelity%20with%20significantly%20fewer%20parameters%2C%20while%20enabling%20intuitive%2C%20interactive%2C%20and%20physically%20plausible%20manipulation.%20Moreover%2C%20by%20integrating%20proxy%20nodes%20with%20Position-Based%20Dynamics%2C%20our%20framework%20supports%20real-time%20physics-driven%20animation%20using%20lightweight%20implicit%20rendering%2C%20achieving%20superior%20temporal%20consistency%20and%20visual%20realism%20compared%20with%20generative%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2602.01881v1&entry.124074799=Read"},
{"title": "MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models", "author": "Zheyuan Zhou and Liang Du and Zixun Sun and Xiaoyu Zhou and Ruimin Ye and Qihao Chen and Yinda Chen and Lemiao Qiu", "abstract": "Despite significant progress in Visual-Language-Action (VLA), in highly complex and dynamic environments that involve real-time unpredictable interactions (such as 3D open worlds and large-scale PvP games), existing approaches remain inefficient at extracting action-critical signals from redundant sensor streams. To tackle this, we introduce MAIN-VLA, a framework that explicitly Models the Abstraction of Intention and eNvironment to ground decision-making in deep semantic alignment rather than superficial pattern matching. Specifically, our Intention Abstraction (IA) extracts verbose linguistic instructions and their associated reasoning into compact, explicit semantic primitives, while the Environment Semantics Abstraction (ESA) projects overwhelming visual streams into a structured, topological affordance representation. Furthermore, aligning these two abstract modalities induces an emergent attention-concentration effect, enabling a parameter-free token-pruning strategy that filters out perceptual redundancy without degrading performance. Extensive experiments in open-world Minecraft and large-scale PvP environments (Game for Peace and Valorant) demonstrate that MAIN-VLA sets a new state-of-the-art, which achieves superior decision quality, stronger generalization, and cutting-edge inference efficiency.", "link": "http://arxiv.org/abs/2602.02212v1", "date": "2026-02-02", "relevancy": 2.3725, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6021}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6021}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAIN-VLA%3A%20Modeling%20Abstraction%20of%20Intention%20and%20eNvironment%20for%20Vision-Language-Action%20Models&body=Title%3A%20MAIN-VLA%3A%20Modeling%20Abstraction%20of%20Intention%20and%20eNvironment%20for%20Vision-Language-Action%20Models%0AAuthor%3A%20Zheyuan%20Zhou%20and%20Liang%20Du%20and%20Zixun%20Sun%20and%20Xiaoyu%20Zhou%20and%20Ruimin%20Ye%20and%20Qihao%20Chen%20and%20Yinda%20Chen%20and%20Lemiao%20Qiu%0AAbstract%3A%20Despite%20significant%20progress%20in%20Visual-Language-Action%20%28VLA%29%2C%20in%20highly%20complex%20and%20dynamic%20environments%20that%20involve%20real-time%20unpredictable%20interactions%20%28such%20as%203D%20open%20worlds%20and%20large-scale%20PvP%20games%29%2C%20existing%20approaches%20remain%20inefficient%20at%20extracting%20action-critical%20signals%20from%20redundant%20sensor%20streams.%20To%20tackle%20this%2C%20we%20introduce%20MAIN-VLA%2C%20a%20framework%20that%20explicitly%20Models%20the%20Abstraction%20of%20Intention%20and%20eNvironment%20to%20ground%20decision-making%20in%20deep%20semantic%20alignment%20rather%20than%20superficial%20pattern%20matching.%20Specifically%2C%20our%20Intention%20Abstraction%20%28IA%29%20extracts%20verbose%20linguistic%20instructions%20and%20their%20associated%20reasoning%20into%20compact%2C%20explicit%20semantic%20primitives%2C%20while%20the%20Environment%20Semantics%20Abstraction%20%28ESA%29%20projects%20overwhelming%20visual%20streams%20into%20a%20structured%2C%20topological%20affordance%20representation.%20Furthermore%2C%20aligning%20these%20two%20abstract%20modalities%20induces%20an%20emergent%20attention-concentration%20effect%2C%20enabling%20a%20parameter-free%20token-pruning%20strategy%20that%20filters%20out%20perceptual%20redundancy%20without%20degrading%20performance.%20Extensive%20experiments%20in%20open-world%20Minecraft%20and%20large-scale%20PvP%20environments%20%28Game%20for%20Peace%20and%20Valorant%29%20demonstrate%20that%20MAIN-VLA%20sets%20a%20new%20state-of-the-art%2C%20which%20achieves%20superior%20decision%20quality%2C%20stronger%20generalization%2C%20and%20cutting-edge%20inference%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02212v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAIN-VLA%253A%2520Modeling%2520Abstraction%2520of%2520Intention%2520and%2520eNvironment%2520for%2520Vision-Language-Action%2520Models%26entry.906535625%3DZheyuan%2520Zhou%2520and%2520Liang%2520Du%2520and%2520Zixun%2520Sun%2520and%2520Xiaoyu%2520Zhou%2520and%2520Ruimin%2520Ye%2520and%2520Qihao%2520Chen%2520and%2520Yinda%2520Chen%2520and%2520Lemiao%2520Qiu%26entry.1292438233%3DDespite%2520significant%2520progress%2520in%2520Visual-Language-Action%2520%2528VLA%2529%252C%2520in%2520highly%2520complex%2520and%2520dynamic%2520environments%2520that%2520involve%2520real-time%2520unpredictable%2520interactions%2520%2528such%2520as%25203D%2520open%2520worlds%2520and%2520large-scale%2520PvP%2520games%2529%252C%2520existing%2520approaches%2520remain%2520inefficient%2520at%2520extracting%2520action-critical%2520signals%2520from%2520redundant%2520sensor%2520streams.%2520To%2520tackle%2520this%252C%2520we%2520introduce%2520MAIN-VLA%252C%2520a%2520framework%2520that%2520explicitly%2520Models%2520the%2520Abstraction%2520of%2520Intention%2520and%2520eNvironment%2520to%2520ground%2520decision-making%2520in%2520deep%2520semantic%2520alignment%2520rather%2520than%2520superficial%2520pattern%2520matching.%2520Specifically%252C%2520our%2520Intention%2520Abstraction%2520%2528IA%2529%2520extracts%2520verbose%2520linguistic%2520instructions%2520and%2520their%2520associated%2520reasoning%2520into%2520compact%252C%2520explicit%2520semantic%2520primitives%252C%2520while%2520the%2520Environment%2520Semantics%2520Abstraction%2520%2528ESA%2529%2520projects%2520overwhelming%2520visual%2520streams%2520into%2520a%2520structured%252C%2520topological%2520affordance%2520representation.%2520Furthermore%252C%2520aligning%2520these%2520two%2520abstract%2520modalities%2520induces%2520an%2520emergent%2520attention-concentration%2520effect%252C%2520enabling%2520a%2520parameter-free%2520token-pruning%2520strategy%2520that%2520filters%2520out%2520perceptual%2520redundancy%2520without%2520degrading%2520performance.%2520Extensive%2520experiments%2520in%2520open-world%2520Minecraft%2520and%2520large-scale%2520PvP%2520environments%2520%2528Game%2520for%2520Peace%2520and%2520Valorant%2529%2520demonstrate%2520that%2520MAIN-VLA%2520sets%2520a%2520new%2520state-of-the-art%252C%2520which%2520achieves%2520superior%2520decision%2520quality%252C%2520stronger%2520generalization%252C%2520and%2520cutting-edge%2520inference%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02212v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAIN-VLA%3A%20Modeling%20Abstraction%20of%20Intention%20and%20eNvironment%20for%20Vision-Language-Action%20Models&entry.906535625=Zheyuan%20Zhou%20and%20Liang%20Du%20and%20Zixun%20Sun%20and%20Xiaoyu%20Zhou%20and%20Ruimin%20Ye%20and%20Qihao%20Chen%20and%20Yinda%20Chen%20and%20Lemiao%20Qiu&entry.1292438233=Despite%20significant%20progress%20in%20Visual-Language-Action%20%28VLA%29%2C%20in%20highly%20complex%20and%20dynamic%20environments%20that%20involve%20real-time%20unpredictable%20interactions%20%28such%20as%203D%20open%20worlds%20and%20large-scale%20PvP%20games%29%2C%20existing%20approaches%20remain%20inefficient%20at%20extracting%20action-critical%20signals%20from%20redundant%20sensor%20streams.%20To%20tackle%20this%2C%20we%20introduce%20MAIN-VLA%2C%20a%20framework%20that%20explicitly%20Models%20the%20Abstraction%20of%20Intention%20and%20eNvironment%20to%20ground%20decision-making%20in%20deep%20semantic%20alignment%20rather%20than%20superficial%20pattern%20matching.%20Specifically%2C%20our%20Intention%20Abstraction%20%28IA%29%20extracts%20verbose%20linguistic%20instructions%20and%20their%20associated%20reasoning%20into%20compact%2C%20explicit%20semantic%20primitives%2C%20while%20the%20Environment%20Semantics%20Abstraction%20%28ESA%29%20projects%20overwhelming%20visual%20streams%20into%20a%20structured%2C%20topological%20affordance%20representation.%20Furthermore%2C%20aligning%20these%20two%20abstract%20modalities%20induces%20an%20emergent%20attention-concentration%20effect%2C%20enabling%20a%20parameter-free%20token-pruning%20strategy%20that%20filters%20out%20perceptual%20redundancy%20without%20degrading%20performance.%20Extensive%20experiments%20in%20open-world%20Minecraft%20and%20large-scale%20PvP%20environments%20%28Game%20for%20Peace%20and%20Valorant%29%20demonstrate%20that%20MAIN-VLA%20sets%20a%20new%20state-of-the-art%2C%20which%20achieves%20superior%20decision%20quality%2C%20stronger%20generalization%2C%20and%20cutting-edge%20inference%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2602.02212v1&entry.124074799=Read"},
{"title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation", "author": "Xiang Li and Yupeng Zheng and Pengfei Li and Yilun Chen and Ya-Qin Zhang and Wenchao Ding", "abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS\u2020. With depth integration, DiScene\u2020 attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at https://github.com/getterupper/DiScene.", "link": "http://arxiv.org/abs/2602.02318v1", "date": "2026-02-02", "relevancy": 2.3699, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5995}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5911}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Indoor%20Occupancy%20Prediction%20via%20Sparse%20Query-Based%20Multi-Level%20Consistent%20Knowledge%20Distillation&body=Title%3A%20Enhancing%20Indoor%20Occupancy%20Prediction%20via%20Sparse%20Query-Based%20Multi-Level%20Consistent%20Knowledge%20Distillation%0AAuthor%3A%20Xiang%20Li%20and%20Yupeng%20Zheng%20and%20Pengfei%20Li%20and%20Yilun%20Chen%20and%20Ya-Qin%20Zhang%20and%20Wenchao%20Ding%0AAbstract%3A%20Occupancy%20prediction%20provides%20critical%20geometric%20and%20semantic%20understanding%20for%20robotics%20but%20faces%20efficiency-accuracy%20trade-offs.%20Current%20dense%20methods%20suffer%20computational%20waste%20on%20empty%20voxels%2C%20while%20sparse%20query-based%20approaches%20lack%20robustness%20in%20diverse%20and%20complex%20indoor%20scenes.%20In%20this%20paper%2C%20we%20propose%20DiScene%2C%20a%20novel%20sparse%20query-based%20framework%20that%20leverages%20multi-level%20distillation%20to%20achieve%20efficient%20and%20robust%20occupancy%20prediction.%20In%20particular%2C%20our%20method%20incorporates%20two%20key%20innovations%3A%20%281%29%20a%20Multi-level%20Consistent%20Knowledge%20Distillation%20strategy%2C%20which%20transfers%20hierarchical%20representations%20from%20large%20teacher%20models%20to%20lightweight%20students%20through%20coordinated%20alignment%20across%20four%20levels%2C%20including%20encoder-level%20feature%20alignment%2C%20query-level%20feature%20matching%2C%20prior-level%20spatial%20guidance%2C%20and%20anchor-level%20high-confidence%20knowledge%20transfer%20and%20%282%29%20a%20Teacher-Guided%20Initialization%20policy%2C%20employing%20optimized%20parameter%20warm-up%20to%20accelerate%20model%20convergence.%20Validated%20on%20the%20Occ-Scannet%20benchmark%2C%20DiScene%20achieves%2023.2%20FPS%20without%20depth%20priors%20while%20outperforming%20our%20baseline%20method%2C%20OPUS%2C%20by%2036.1%25%20and%20even%20better%20than%20the%20depth-enhanced%20version%2C%20OPUS%E2%80%A0.%20With%20depth%20integration%2C%20DiScene%E2%80%A0%20attains%20new%20SOTA%20performance%2C%20surpassing%20EmbodiedOcc%20by%203.7%25%20with%201.62%24%5Ctimes%24%20faster%20inference%20speed.%20Furthermore%2C%20experiments%20on%20the%20Occ3D-nuScenes%20benchmark%20and%20in-the-wild%20scenarios%20demonstrate%20the%20versatility%20of%20our%20approach%20in%20various%20environments.%20Code%20and%20models%20can%20be%20accessed%20at%20https%3A//github.com/getterupper/DiScene.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Indoor%2520Occupancy%2520Prediction%2520via%2520Sparse%2520Query-Based%2520Multi-Level%2520Consistent%2520Knowledge%2520Distillation%26entry.906535625%3DXiang%2520Li%2520and%2520Yupeng%2520Zheng%2520and%2520Pengfei%2520Li%2520and%2520Yilun%2520Chen%2520and%2520Ya-Qin%2520Zhang%2520and%2520Wenchao%2520Ding%26entry.1292438233%3DOccupancy%2520prediction%2520provides%2520critical%2520geometric%2520and%2520semantic%2520understanding%2520for%2520robotics%2520but%2520faces%2520efficiency-accuracy%2520trade-offs.%2520Current%2520dense%2520methods%2520suffer%2520computational%2520waste%2520on%2520empty%2520voxels%252C%2520while%2520sparse%2520query-based%2520approaches%2520lack%2520robustness%2520in%2520diverse%2520and%2520complex%2520indoor%2520scenes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520DiScene%252C%2520a%2520novel%2520sparse%2520query-based%2520framework%2520that%2520leverages%2520multi-level%2520distillation%2520to%2520achieve%2520efficient%2520and%2520robust%2520occupancy%2520prediction.%2520In%2520particular%252C%2520our%2520method%2520incorporates%2520two%2520key%2520innovations%253A%2520%25281%2529%2520a%2520Multi-level%2520Consistent%2520Knowledge%2520Distillation%2520strategy%252C%2520which%2520transfers%2520hierarchical%2520representations%2520from%2520large%2520teacher%2520models%2520to%2520lightweight%2520students%2520through%2520coordinated%2520alignment%2520across%2520four%2520levels%252C%2520including%2520encoder-level%2520feature%2520alignment%252C%2520query-level%2520feature%2520matching%252C%2520prior-level%2520spatial%2520guidance%252C%2520and%2520anchor-level%2520high-confidence%2520knowledge%2520transfer%2520and%2520%25282%2529%2520a%2520Teacher-Guided%2520Initialization%2520policy%252C%2520employing%2520optimized%2520parameter%2520warm-up%2520to%2520accelerate%2520model%2520convergence.%2520Validated%2520on%2520the%2520Occ-Scannet%2520benchmark%252C%2520DiScene%2520achieves%252023.2%2520FPS%2520without%2520depth%2520priors%2520while%2520outperforming%2520our%2520baseline%2520method%252C%2520OPUS%252C%2520by%252036.1%2525%2520and%2520even%2520better%2520than%2520the%2520depth-enhanced%2520version%252C%2520OPUS%25E2%2580%25A0.%2520With%2520depth%2520integration%252C%2520DiScene%25E2%2580%25A0%2520attains%2520new%2520SOTA%2520performance%252C%2520surpassing%2520EmbodiedOcc%2520by%25203.7%2525%2520with%25201.62%2524%255Ctimes%2524%2520faster%2520inference%2520speed.%2520Furthermore%252C%2520experiments%2520on%2520the%2520Occ3D-nuScenes%2520benchmark%2520and%2520in-the-wild%2520scenarios%2520demonstrate%2520the%2520versatility%2520of%2520our%2520approach%2520in%2520various%2520environments.%2520Code%2520and%2520models%2520can%2520be%2520accessed%2520at%2520https%253A//github.com/getterupper/DiScene.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Indoor%20Occupancy%20Prediction%20via%20Sparse%20Query-Based%20Multi-Level%20Consistent%20Knowledge%20Distillation&entry.906535625=Xiang%20Li%20and%20Yupeng%20Zheng%20and%20Pengfei%20Li%20and%20Yilun%20Chen%20and%20Ya-Qin%20Zhang%20and%20Wenchao%20Ding&entry.1292438233=Occupancy%20prediction%20provides%20critical%20geometric%20and%20semantic%20understanding%20for%20robotics%20but%20faces%20efficiency-accuracy%20trade-offs.%20Current%20dense%20methods%20suffer%20computational%20waste%20on%20empty%20voxels%2C%20while%20sparse%20query-based%20approaches%20lack%20robustness%20in%20diverse%20and%20complex%20indoor%20scenes.%20In%20this%20paper%2C%20we%20propose%20DiScene%2C%20a%20novel%20sparse%20query-based%20framework%20that%20leverages%20multi-level%20distillation%20to%20achieve%20efficient%20and%20robust%20occupancy%20prediction.%20In%20particular%2C%20our%20method%20incorporates%20two%20key%20innovations%3A%20%281%29%20a%20Multi-level%20Consistent%20Knowledge%20Distillation%20strategy%2C%20which%20transfers%20hierarchical%20representations%20from%20large%20teacher%20models%20to%20lightweight%20students%20through%20coordinated%20alignment%20across%20four%20levels%2C%20including%20encoder-level%20feature%20alignment%2C%20query-level%20feature%20matching%2C%20prior-level%20spatial%20guidance%2C%20and%20anchor-level%20high-confidence%20knowledge%20transfer%20and%20%282%29%20a%20Teacher-Guided%20Initialization%20policy%2C%20employing%20optimized%20parameter%20warm-up%20to%20accelerate%20model%20convergence.%20Validated%20on%20the%20Occ-Scannet%20benchmark%2C%20DiScene%20achieves%2023.2%20FPS%20without%20depth%20priors%20while%20outperforming%20our%20baseline%20method%2C%20OPUS%2C%20by%2036.1%25%20and%20even%20better%20than%20the%20depth-enhanced%20version%2C%20OPUS%E2%80%A0.%20With%20depth%20integration%2C%20DiScene%E2%80%A0%20attains%20new%20SOTA%20performance%2C%20surpassing%20EmbodiedOcc%20by%203.7%25%20with%201.62%24%5Ctimes%24%20faster%20inference%20speed.%20Furthermore%2C%20experiments%20on%20the%20Occ3D-nuScenes%20benchmark%20and%20in-the-wild%20scenarios%20demonstrate%20the%20versatility%20of%20our%20approach%20in%20various%20environments.%20Code%20and%20models%20can%20be%20accessed%20at%20https%3A//github.com/getterupper/DiScene.&entry.1838667208=http%3A//arxiv.org/abs/2602.02318v1&entry.124074799=Read"},
{"title": "Interpretable Tabular Foundation Models via In-Context Kernel Regression", "author": "Ratmir Miftachov and Bruno Charron and Simon Valentin", "abstract": "Tabular foundation models like TabPFN and TabICL achieve state-of-the-art performance through in-context learning, yet their architectures remain fundamentally opaque. We introduce KernelICL, a framework to enhance tabular foundation models with quantifiable sample-based interpretability. Building on the insight that in-context learning is akin to kernel regression, we make this mechanism explicit by replacing the final prediction layer with kernel functions (Gaussian, dot-product, kNN) so that every prediction is a transparent weighted average of training labels. We introduce a two-dimensional taxonomy that formally unifies standard kernel methods, modern neighbor-based approaches, and attention mechanisms under a single framework, and quantify inspectability via the perplexity of the weight distribution over training samples. On 55 TALENT benchmark datasets, KernelICL achieves performance on par with existing tabular foundation models, demonstrating that explicit kernel constraints on the final layer enable inspectable predictions without sacrificing performance.", "link": "http://arxiv.org/abs/2602.02162v1", "date": "2026-02-02", "relevancy": 2.3666, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4787}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4711}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Tabular%20Foundation%20Models%20via%20In-Context%20Kernel%20Regression&body=Title%3A%20Interpretable%20Tabular%20Foundation%20Models%20via%20In-Context%20Kernel%20Regression%0AAuthor%3A%20Ratmir%20Miftachov%20and%20Bruno%20Charron%20and%20Simon%20Valentin%0AAbstract%3A%20Tabular%20foundation%20models%20like%20TabPFN%20and%20TabICL%20achieve%20state-of-the-art%20performance%20through%20in-context%20learning%2C%20yet%20their%20architectures%20remain%20fundamentally%20opaque.%20We%20introduce%20KernelICL%2C%20a%20framework%20to%20enhance%20tabular%20foundation%20models%20with%20quantifiable%20sample-based%20interpretability.%20Building%20on%20the%20insight%20that%20in-context%20learning%20is%20akin%20to%20kernel%20regression%2C%20we%20make%20this%20mechanism%20explicit%20by%20replacing%20the%20final%20prediction%20layer%20with%20kernel%20functions%20%28Gaussian%2C%20dot-product%2C%20kNN%29%20so%20that%20every%20prediction%20is%20a%20transparent%20weighted%20average%20of%20training%20labels.%20We%20introduce%20a%20two-dimensional%20taxonomy%20that%20formally%20unifies%20standard%20kernel%20methods%2C%20modern%20neighbor-based%20approaches%2C%20and%20attention%20mechanisms%20under%20a%20single%20framework%2C%20and%20quantify%20inspectability%20via%20the%20perplexity%20of%20the%20weight%20distribution%20over%20training%20samples.%20On%2055%20TALENT%20benchmark%20datasets%2C%20KernelICL%20achieves%20performance%20on%20par%20with%20existing%20tabular%20foundation%20models%2C%20demonstrating%20that%20explicit%20kernel%20constraints%20on%20the%20final%20layer%20enable%20inspectable%20predictions%20without%20sacrificing%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02162v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Tabular%2520Foundation%2520Models%2520via%2520In-Context%2520Kernel%2520Regression%26entry.906535625%3DRatmir%2520Miftachov%2520and%2520Bruno%2520Charron%2520and%2520Simon%2520Valentin%26entry.1292438233%3DTabular%2520foundation%2520models%2520like%2520TabPFN%2520and%2520TabICL%2520achieve%2520state-of-the-art%2520performance%2520through%2520in-context%2520learning%252C%2520yet%2520their%2520architectures%2520remain%2520fundamentally%2520opaque.%2520We%2520introduce%2520KernelICL%252C%2520a%2520framework%2520to%2520enhance%2520tabular%2520foundation%2520models%2520with%2520quantifiable%2520sample-based%2520interpretability.%2520Building%2520on%2520the%2520insight%2520that%2520in-context%2520learning%2520is%2520akin%2520to%2520kernel%2520regression%252C%2520we%2520make%2520this%2520mechanism%2520explicit%2520by%2520replacing%2520the%2520final%2520prediction%2520layer%2520with%2520kernel%2520functions%2520%2528Gaussian%252C%2520dot-product%252C%2520kNN%2529%2520so%2520that%2520every%2520prediction%2520is%2520a%2520transparent%2520weighted%2520average%2520of%2520training%2520labels.%2520We%2520introduce%2520a%2520two-dimensional%2520taxonomy%2520that%2520formally%2520unifies%2520standard%2520kernel%2520methods%252C%2520modern%2520neighbor-based%2520approaches%252C%2520and%2520attention%2520mechanisms%2520under%2520a%2520single%2520framework%252C%2520and%2520quantify%2520inspectability%2520via%2520the%2520perplexity%2520of%2520the%2520weight%2520distribution%2520over%2520training%2520samples.%2520On%252055%2520TALENT%2520benchmark%2520datasets%252C%2520KernelICL%2520achieves%2520performance%2520on%2520par%2520with%2520existing%2520tabular%2520foundation%2520models%252C%2520demonstrating%2520that%2520explicit%2520kernel%2520constraints%2520on%2520the%2520final%2520layer%2520enable%2520inspectable%2520predictions%2520without%2520sacrificing%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02162v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Tabular%20Foundation%20Models%20via%20In-Context%20Kernel%20Regression&entry.906535625=Ratmir%20Miftachov%20and%20Bruno%20Charron%20and%20Simon%20Valentin&entry.1292438233=Tabular%20foundation%20models%20like%20TabPFN%20and%20TabICL%20achieve%20state-of-the-art%20performance%20through%20in-context%20learning%2C%20yet%20their%20architectures%20remain%20fundamentally%20opaque.%20We%20introduce%20KernelICL%2C%20a%20framework%20to%20enhance%20tabular%20foundation%20models%20with%20quantifiable%20sample-based%20interpretability.%20Building%20on%20the%20insight%20that%20in-context%20learning%20is%20akin%20to%20kernel%20regression%2C%20we%20make%20this%20mechanism%20explicit%20by%20replacing%20the%20final%20prediction%20layer%20with%20kernel%20functions%20%28Gaussian%2C%20dot-product%2C%20kNN%29%20so%20that%20every%20prediction%20is%20a%20transparent%20weighted%20average%20of%20training%20labels.%20We%20introduce%20a%20two-dimensional%20taxonomy%20that%20formally%20unifies%20standard%20kernel%20methods%2C%20modern%20neighbor-based%20approaches%2C%20and%20attention%20mechanisms%20under%20a%20single%20framework%2C%20and%20quantify%20inspectability%20via%20the%20perplexity%20of%20the%20weight%20distribution%20over%20training%20samples.%20On%2055%20TALENT%20benchmark%20datasets%2C%20KernelICL%20achieves%20performance%20on%20par%20with%20existing%20tabular%20foundation%20models%2C%20demonstrating%20that%20explicit%20kernel%20constraints%20on%20the%20final%20layer%20enable%20inspectable%20predictions%20without%20sacrificing%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2602.02162v1&entry.124074799=Read"},
{"title": "LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization", "author": "Zhenpeng Huang and Jiaqi Li and Zihan Jia and Xinhao Li and Desen Meng and Lingxue Song and Xi Chen and Liang Li and Limin Wang", "abstract": "We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.", "link": "http://arxiv.org/abs/2602.02341v1", "date": "2026-02-02", "relevancy": 2.3521, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5922}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5922}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongVPO%3A%20From%20Anchored%20Cues%20to%20Self-Reasoning%20for%20Long-Form%20Video%20Preference%20Optimization&body=Title%3A%20LongVPO%3A%20From%20Anchored%20Cues%20to%20Self-Reasoning%20for%20Long-Form%20Video%20Preference%20Optimization%0AAuthor%3A%20Zhenpeng%20Huang%20and%20Jiaqi%20Li%20and%20Zihan%20Jia%20and%20Xinhao%20Li%20and%20Desen%20Meng%20and%20Lingxue%20Song%20and%20Xi%20Chen%20and%20Liang%20Li%20and%20Limin%20Wang%0AAbstract%3A%20We%20present%20LongVPO%2C%20a%20novel%20two-stage%20Direct%20Preference%20Optimization%20framework%20that%20enables%20short-context%20vision-language%20models%20to%20robustly%20understand%20ultra-long%20videos%20without%20any%20long-video%20annotations.%20In%20Stage%201%2C%20we%20synthesize%20preference%20triples%20by%20anchoring%20questions%20to%20individual%20short%20clips%2C%20interleaving%20them%20with%20distractors%2C%20and%20applying%20visual-similarity%20and%20question-specificity%20filtering%20to%20mitigate%20positional%20bias%20and%20ensure%20unambiguous%20supervision.%20We%20also%20approximate%20the%20reference%20model%27s%20scoring%20over%20long%20contexts%20by%20evaluating%20only%20the%20anchor%20clip%2C%20reducing%20computational%20overhead.%20In%20Stage%202%2C%20we%20employ%20a%20recursive%20captioning%20pipeline%20on%20long%20videos%20to%20generate%20scene-level%20metadata%2C%20then%20use%20a%20large%20language%20model%20to%20craft%20multi-segment%20reasoning%20queries%20and%20dispreferred%20responses%2C%20aligning%20the%20model%27s%20preferences%20through%20multi-segment%20reasoning%20tasks.%20With%20only%2016K%20synthetic%20examples%20and%20no%20costly%20human%20labels%2C%20LongVPO%20outperforms%20the%20state-of-the-art%20open-source%20models%20on%20multiple%20long-video%20benchmarks%2C%20while%20maintaining%20strong%20short-video%20performance%20%28e.g.%2C%20on%20MVBench%29%2C%20offering%20a%20scalable%20paradigm%20for%20efficient%20long-form%20video%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02341v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongVPO%253A%2520From%2520Anchored%2520Cues%2520to%2520Self-Reasoning%2520for%2520Long-Form%2520Video%2520Preference%2520Optimization%26entry.906535625%3DZhenpeng%2520Huang%2520and%2520Jiaqi%2520Li%2520and%2520Zihan%2520Jia%2520and%2520Xinhao%2520Li%2520and%2520Desen%2520Meng%2520and%2520Lingxue%2520Song%2520and%2520Xi%2520Chen%2520and%2520Liang%2520Li%2520and%2520Limin%2520Wang%26entry.1292438233%3DWe%2520present%2520LongVPO%252C%2520a%2520novel%2520two-stage%2520Direct%2520Preference%2520Optimization%2520framework%2520that%2520enables%2520short-context%2520vision-language%2520models%2520to%2520robustly%2520understand%2520ultra-long%2520videos%2520without%2520any%2520long-video%2520annotations.%2520In%2520Stage%25201%252C%2520we%2520synthesize%2520preference%2520triples%2520by%2520anchoring%2520questions%2520to%2520individual%2520short%2520clips%252C%2520interleaving%2520them%2520with%2520distractors%252C%2520and%2520applying%2520visual-similarity%2520and%2520question-specificity%2520filtering%2520to%2520mitigate%2520positional%2520bias%2520and%2520ensure%2520unambiguous%2520supervision.%2520We%2520also%2520approximate%2520the%2520reference%2520model%2527s%2520scoring%2520over%2520long%2520contexts%2520by%2520evaluating%2520only%2520the%2520anchor%2520clip%252C%2520reducing%2520computational%2520overhead.%2520In%2520Stage%25202%252C%2520we%2520employ%2520a%2520recursive%2520captioning%2520pipeline%2520on%2520long%2520videos%2520to%2520generate%2520scene-level%2520metadata%252C%2520then%2520use%2520a%2520large%2520language%2520model%2520to%2520craft%2520multi-segment%2520reasoning%2520queries%2520and%2520dispreferred%2520responses%252C%2520aligning%2520the%2520model%2527s%2520preferences%2520through%2520multi-segment%2520reasoning%2520tasks.%2520With%2520only%252016K%2520synthetic%2520examples%2520and%2520no%2520costly%2520human%2520labels%252C%2520LongVPO%2520outperforms%2520the%2520state-of-the-art%2520open-source%2520models%2520on%2520multiple%2520long-video%2520benchmarks%252C%2520while%2520maintaining%2520strong%2520short-video%2520performance%2520%2528e.g.%252C%2520on%2520MVBench%2529%252C%2520offering%2520a%2520scalable%2520paradigm%2520for%2520efficient%2520long-form%2520video%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02341v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongVPO%3A%20From%20Anchored%20Cues%20to%20Self-Reasoning%20for%20Long-Form%20Video%20Preference%20Optimization&entry.906535625=Zhenpeng%20Huang%20and%20Jiaqi%20Li%20and%20Zihan%20Jia%20and%20Xinhao%20Li%20and%20Desen%20Meng%20and%20Lingxue%20Song%20and%20Xi%20Chen%20and%20Liang%20Li%20and%20Limin%20Wang&entry.1292438233=We%20present%20LongVPO%2C%20a%20novel%20two-stage%20Direct%20Preference%20Optimization%20framework%20that%20enables%20short-context%20vision-language%20models%20to%20robustly%20understand%20ultra-long%20videos%20without%20any%20long-video%20annotations.%20In%20Stage%201%2C%20we%20synthesize%20preference%20triples%20by%20anchoring%20questions%20to%20individual%20short%20clips%2C%20interleaving%20them%20with%20distractors%2C%20and%20applying%20visual-similarity%20and%20question-specificity%20filtering%20to%20mitigate%20positional%20bias%20and%20ensure%20unambiguous%20supervision.%20We%20also%20approximate%20the%20reference%20model%27s%20scoring%20over%20long%20contexts%20by%20evaluating%20only%20the%20anchor%20clip%2C%20reducing%20computational%20overhead.%20In%20Stage%202%2C%20we%20employ%20a%20recursive%20captioning%20pipeline%20on%20long%20videos%20to%20generate%20scene-level%20metadata%2C%20then%20use%20a%20large%20language%20model%20to%20craft%20multi-segment%20reasoning%20queries%20and%20dispreferred%20responses%2C%20aligning%20the%20model%27s%20preferences%20through%20multi-segment%20reasoning%20tasks.%20With%20only%2016K%20synthetic%20examples%20and%20no%20costly%20human%20labels%2C%20LongVPO%20outperforms%20the%20state-of-the-art%20open-source%20models%20on%20multiple%20long-video%20benchmarks%2C%20while%20maintaining%20strong%20short-video%20performance%20%28e.g.%2C%20on%20MVBench%29%2C%20offering%20a%20scalable%20paradigm%20for%20efficient%20long-form%20video%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2602.02341v1&entry.124074799=Read"},
{"title": "HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos", "author": "Yinhuai Wang and Qihan Zhao and Yuen Fui Lau and Runyi Yu and Hok Wai Tsui and Qifeng Chen and Jingbo Wang and Jiangmiao Pang and Ping Tan", "abstract": "Enabling humanoid robots to perform agile and adaptive interactive tasks has long been a core challenge in robotics. Current approaches are bottlenecked by either the scarcity of realistic interaction data or the need for meticulous, task-specific reward engineering, which limits their scalability. To narrow this gap, we present HumanX, a full-stack framework that compiles human video into generalizable, real-world interaction skills for humanoids, without task-specific rewards. HumanX integrates two co-designed components: XGen, a data generation pipeline that synthesizes diverse and physically plausible robot interaction data from video while supporting scalable data augmentation; and XMimic, a unified imitation learning framework that learns generalizable interaction skills. Evaluated across five distinct domains--basketball, football, badminton, cargo pickup, and reactive fighting--HumanX successfully acquires 10 different skills and transfers them zero-shot to a physical Unitree G1 humanoid. The learned capabilities include complex maneuvers such as pump-fake turnaround fadeaway jumpshots without any external perception, as well as interactive tasks like sustained human-robot passing sequences over 10 consecutive cycles--learned from a single video demonstration. Our experiments show that HumanX achieves over 8 times higher generalization success than prior methods, demonstrating a scalable and task-agnostic pathway for learning versatile, real-world robot interactive skills.", "link": "http://arxiv.org/abs/2602.02473v1", "date": "2026-02-02", "relevancy": 2.3456, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6011}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6004}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanX%3A%20Toward%20Agile%20and%20Generalizable%20Humanoid%20Interaction%20Skills%20from%20Human%20Videos&body=Title%3A%20HumanX%3A%20Toward%20Agile%20and%20Generalizable%20Humanoid%20Interaction%20Skills%20from%20Human%20Videos%0AAuthor%3A%20Yinhuai%20Wang%20and%20Qihan%20Zhao%20and%20Yuen%20Fui%20Lau%20and%20Runyi%20Yu%20and%20Hok%20Wai%20Tsui%20and%20Qifeng%20Chen%20and%20Jingbo%20Wang%20and%20Jiangmiao%20Pang%20and%20Ping%20Tan%0AAbstract%3A%20Enabling%20humanoid%20robots%20to%20perform%20agile%20and%20adaptive%20interactive%20tasks%20has%20long%20been%20a%20core%20challenge%20in%20robotics.%20Current%20approaches%20are%20bottlenecked%20by%20either%20the%20scarcity%20of%20realistic%20interaction%20data%20or%20the%20need%20for%20meticulous%2C%20task-specific%20reward%20engineering%2C%20which%20limits%20their%20scalability.%20To%20narrow%20this%20gap%2C%20we%20present%20HumanX%2C%20a%20full-stack%20framework%20that%20compiles%20human%20video%20into%20generalizable%2C%20real-world%20interaction%20skills%20for%20humanoids%2C%20without%20task-specific%20rewards.%20HumanX%20integrates%20two%20co-designed%20components%3A%20XGen%2C%20a%20data%20generation%20pipeline%20that%20synthesizes%20diverse%20and%20physically%20plausible%20robot%20interaction%20data%20from%20video%20while%20supporting%20scalable%20data%20augmentation%3B%20and%20XMimic%2C%20a%20unified%20imitation%20learning%20framework%20that%20learns%20generalizable%20interaction%20skills.%20Evaluated%20across%20five%20distinct%20domains--basketball%2C%20football%2C%20badminton%2C%20cargo%20pickup%2C%20and%20reactive%20fighting--HumanX%20successfully%20acquires%2010%20different%20skills%20and%20transfers%20them%20zero-shot%20to%20a%20physical%20Unitree%20G1%20humanoid.%20The%20learned%20capabilities%20include%20complex%20maneuvers%20such%20as%20pump-fake%20turnaround%20fadeaway%20jumpshots%20without%20any%20external%20perception%2C%20as%20well%20as%20interactive%20tasks%20like%20sustained%20human-robot%20passing%20sequences%20over%2010%20consecutive%20cycles--learned%20from%20a%20single%20video%20demonstration.%20Our%20experiments%20show%20that%20HumanX%20achieves%20over%208%20times%20higher%20generalization%20success%20than%20prior%20methods%2C%20demonstrating%20a%20scalable%20and%20task-agnostic%20pathway%20for%20learning%20versatile%2C%20real-world%20robot%20interactive%20skills.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanX%253A%2520Toward%2520Agile%2520and%2520Generalizable%2520Humanoid%2520Interaction%2520Skills%2520from%2520Human%2520Videos%26entry.906535625%3DYinhuai%2520Wang%2520and%2520Qihan%2520Zhao%2520and%2520Yuen%2520Fui%2520Lau%2520and%2520Runyi%2520Yu%2520and%2520Hok%2520Wai%2520Tsui%2520and%2520Qifeng%2520Chen%2520and%2520Jingbo%2520Wang%2520and%2520Jiangmiao%2520Pang%2520and%2520Ping%2520Tan%26entry.1292438233%3DEnabling%2520humanoid%2520robots%2520to%2520perform%2520agile%2520and%2520adaptive%2520interactive%2520tasks%2520has%2520long%2520been%2520a%2520core%2520challenge%2520in%2520robotics.%2520Current%2520approaches%2520are%2520bottlenecked%2520by%2520either%2520the%2520scarcity%2520of%2520realistic%2520interaction%2520data%2520or%2520the%2520need%2520for%2520meticulous%252C%2520task-specific%2520reward%2520engineering%252C%2520which%2520limits%2520their%2520scalability.%2520To%2520narrow%2520this%2520gap%252C%2520we%2520present%2520HumanX%252C%2520a%2520full-stack%2520framework%2520that%2520compiles%2520human%2520video%2520into%2520generalizable%252C%2520real-world%2520interaction%2520skills%2520for%2520humanoids%252C%2520without%2520task-specific%2520rewards.%2520HumanX%2520integrates%2520two%2520co-designed%2520components%253A%2520XGen%252C%2520a%2520data%2520generation%2520pipeline%2520that%2520synthesizes%2520diverse%2520and%2520physically%2520plausible%2520robot%2520interaction%2520data%2520from%2520video%2520while%2520supporting%2520scalable%2520data%2520augmentation%253B%2520and%2520XMimic%252C%2520a%2520unified%2520imitation%2520learning%2520framework%2520that%2520learns%2520generalizable%2520interaction%2520skills.%2520Evaluated%2520across%2520five%2520distinct%2520domains--basketball%252C%2520football%252C%2520badminton%252C%2520cargo%2520pickup%252C%2520and%2520reactive%2520fighting--HumanX%2520successfully%2520acquires%252010%2520different%2520skills%2520and%2520transfers%2520them%2520zero-shot%2520to%2520a%2520physical%2520Unitree%2520G1%2520humanoid.%2520The%2520learned%2520capabilities%2520include%2520complex%2520maneuvers%2520such%2520as%2520pump-fake%2520turnaround%2520fadeaway%2520jumpshots%2520without%2520any%2520external%2520perception%252C%2520as%2520well%2520as%2520interactive%2520tasks%2520like%2520sustained%2520human-robot%2520passing%2520sequences%2520over%252010%2520consecutive%2520cycles--learned%2520from%2520a%2520single%2520video%2520demonstration.%2520Our%2520experiments%2520show%2520that%2520HumanX%2520achieves%2520over%25208%2520times%2520higher%2520generalization%2520success%2520than%2520prior%2520methods%252C%2520demonstrating%2520a%2520scalable%2520and%2520task-agnostic%2520pathway%2520for%2520learning%2520versatile%252C%2520real-world%2520robot%2520interactive%2520skills.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanX%3A%20Toward%20Agile%20and%20Generalizable%20Humanoid%20Interaction%20Skills%20from%20Human%20Videos&entry.906535625=Yinhuai%20Wang%20and%20Qihan%20Zhao%20and%20Yuen%20Fui%20Lau%20and%20Runyi%20Yu%20and%20Hok%20Wai%20Tsui%20and%20Qifeng%20Chen%20and%20Jingbo%20Wang%20and%20Jiangmiao%20Pang%20and%20Ping%20Tan&entry.1292438233=Enabling%20humanoid%20robots%20to%20perform%20agile%20and%20adaptive%20interactive%20tasks%20has%20long%20been%20a%20core%20challenge%20in%20robotics.%20Current%20approaches%20are%20bottlenecked%20by%20either%20the%20scarcity%20of%20realistic%20interaction%20data%20or%20the%20need%20for%20meticulous%2C%20task-specific%20reward%20engineering%2C%20which%20limits%20their%20scalability.%20To%20narrow%20this%20gap%2C%20we%20present%20HumanX%2C%20a%20full-stack%20framework%20that%20compiles%20human%20video%20into%20generalizable%2C%20real-world%20interaction%20skills%20for%20humanoids%2C%20without%20task-specific%20rewards.%20HumanX%20integrates%20two%20co-designed%20components%3A%20XGen%2C%20a%20data%20generation%20pipeline%20that%20synthesizes%20diverse%20and%20physically%20plausible%20robot%20interaction%20data%20from%20video%20while%20supporting%20scalable%20data%20augmentation%3B%20and%20XMimic%2C%20a%20unified%20imitation%20learning%20framework%20that%20learns%20generalizable%20interaction%20skills.%20Evaluated%20across%20five%20distinct%20domains--basketball%2C%20football%2C%20badminton%2C%20cargo%20pickup%2C%20and%20reactive%20fighting--HumanX%20successfully%20acquires%2010%20different%20skills%20and%20transfers%20them%20zero-shot%20to%20a%20physical%20Unitree%20G1%20humanoid.%20The%20learned%20capabilities%20include%20complex%20maneuvers%20such%20as%20pump-fake%20turnaround%20fadeaway%20jumpshots%20without%20any%20external%20perception%2C%20as%20well%20as%20interactive%20tasks%20like%20sustained%20human-robot%20passing%20sequences%20over%2010%20consecutive%20cycles--learned%20from%20a%20single%20video%20demonstration.%20Our%20experiments%20show%20that%20HumanX%20achieves%20over%208%20times%20higher%20generalization%20success%20than%20prior%20methods%2C%20demonstrating%20a%20scalable%20and%20task-agnostic%20pathway%20for%20learning%20versatile%2C%20real-world%20robot%20interactive%20skills.&entry.1838667208=http%3A//arxiv.org/abs/2602.02473v1&entry.124074799=Read"},
{"title": "DeepGB-TB: A Risk-Balanced Cross-Attention Gradient-Boosted Convolutional Network for Rapid, Interpretable Tuberculosis Screening", "author": "Zhixiang Lu and Yulong Li and Feilong Tang and Zhengyong Jiang and Chong Li and Mian Zhou and Tenglong Li and Jionglong Su", "abstract": "Large-scale tuberculosis (TB) screening is limited by the high cost and operational complexity of traditional diagnostics, creating a need for artificial-intelligence solutions. We propose DeepGB-TB, a non-invasive system that instantly assigns TB risk scores using only cough audio and basic demographic data. The model couples a lightweight one-dimensional convolutional neural network for audio processing with a gradient-boosted decision tree for tabular features. Its principal innovation is a Cross-Modal Bidirectional Cross-Attention module (CM-BCA) that iteratively exchanges salient cues between modalities, emulating the way clinicians integrate symptoms and risk factors. To meet the clinical priority of minimizing missed cases, we design a Tuberculosis Risk-Balanced Loss (TRBL) that places stronger penalties on false-negative predictions, thereby reducing high-risk misclassifications. DeepGB-TB is evaluated on a diverse dataset of 1,105 patients collected across seven countries, achieving an AUROC of 0.903 and an F1-score of 0.851, representing a new state of the art. Its computational efficiency enables real-time, offline inference directly on common mobile devices, making it ideal for low-resource settings. Importantly, the system produces clinically validated explanations that promote trust and adoption by frontline health workers. By coupling AI innovation with public-health requirements for speed, affordability, and reliability, DeepGB-TB offers a tool for advancing global TB control.", "link": "http://arxiv.org/abs/2508.02741v2", "date": "2026-02-02", "relevancy": 2.3449, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4972}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4551}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepGB-TB%3A%20A%20Risk-Balanced%20Cross-Attention%20Gradient-Boosted%20Convolutional%20Network%20for%20Rapid%2C%20Interpretable%20Tuberculosis%20Screening&body=Title%3A%20DeepGB-TB%3A%20A%20Risk-Balanced%20Cross-Attention%20Gradient-Boosted%20Convolutional%20Network%20for%20Rapid%2C%20Interpretable%20Tuberculosis%20Screening%0AAuthor%3A%20Zhixiang%20Lu%20and%20Yulong%20Li%20and%20Feilong%20Tang%20and%20Zhengyong%20Jiang%20and%20Chong%20Li%20and%20Mian%20Zhou%20and%20Tenglong%20Li%20and%20Jionglong%20Su%0AAbstract%3A%20Large-scale%20tuberculosis%20%28TB%29%20screening%20is%20limited%20by%20the%20high%20cost%20and%20operational%20complexity%20of%20traditional%20diagnostics%2C%20creating%20a%20need%20for%20artificial-intelligence%20solutions.%20We%20propose%20DeepGB-TB%2C%20a%20non-invasive%20system%20that%20instantly%20assigns%20TB%20risk%20scores%20using%20only%20cough%20audio%20and%20basic%20demographic%20data.%20The%20model%20couples%20a%20lightweight%20one-dimensional%20convolutional%20neural%20network%20for%20audio%20processing%20with%20a%20gradient-boosted%20decision%20tree%20for%20tabular%20features.%20Its%20principal%20innovation%20is%20a%20Cross-Modal%20Bidirectional%20Cross-Attention%20module%20%28CM-BCA%29%20that%20iteratively%20exchanges%20salient%20cues%20between%20modalities%2C%20emulating%20the%20way%20clinicians%20integrate%20symptoms%20and%20risk%20factors.%20To%20meet%20the%20clinical%20priority%20of%20minimizing%20missed%20cases%2C%20we%20design%20a%20Tuberculosis%20Risk-Balanced%20Loss%20%28TRBL%29%20that%20places%20stronger%20penalties%20on%20false-negative%20predictions%2C%20thereby%20reducing%20high-risk%20misclassifications.%20DeepGB-TB%20is%20evaluated%20on%20a%20diverse%20dataset%20of%201%2C105%20patients%20collected%20across%20seven%20countries%2C%20achieving%20an%20AUROC%20of%200.903%20and%20an%20F1-score%20of%200.851%2C%20representing%20a%20new%20state%20of%20the%20art.%20Its%20computational%20efficiency%20enables%20real-time%2C%20offline%20inference%20directly%20on%20common%20mobile%20devices%2C%20making%20it%20ideal%20for%20low-resource%20settings.%20Importantly%2C%20the%20system%20produces%20clinically%20validated%20explanations%20that%20promote%20trust%20and%20adoption%20by%20frontline%20health%20workers.%20By%20coupling%20AI%20innovation%20with%20public-health%20requirements%20for%20speed%2C%20affordability%2C%20and%20reliability%2C%20DeepGB-TB%20offers%20a%20tool%20for%20advancing%20global%20TB%20control.%0ALink%3A%20http%3A//arxiv.org/abs/2508.02741v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepGB-TB%253A%2520A%2520Risk-Balanced%2520Cross-Attention%2520Gradient-Boosted%2520Convolutional%2520Network%2520for%2520Rapid%252C%2520Interpretable%2520Tuberculosis%2520Screening%26entry.906535625%3DZhixiang%2520Lu%2520and%2520Yulong%2520Li%2520and%2520Feilong%2520Tang%2520and%2520Zhengyong%2520Jiang%2520and%2520Chong%2520Li%2520and%2520Mian%2520Zhou%2520and%2520Tenglong%2520Li%2520and%2520Jionglong%2520Su%26entry.1292438233%3DLarge-scale%2520tuberculosis%2520%2528TB%2529%2520screening%2520is%2520limited%2520by%2520the%2520high%2520cost%2520and%2520operational%2520complexity%2520of%2520traditional%2520diagnostics%252C%2520creating%2520a%2520need%2520for%2520artificial-intelligence%2520solutions.%2520We%2520propose%2520DeepGB-TB%252C%2520a%2520non-invasive%2520system%2520that%2520instantly%2520assigns%2520TB%2520risk%2520scores%2520using%2520only%2520cough%2520audio%2520and%2520basic%2520demographic%2520data.%2520The%2520model%2520couples%2520a%2520lightweight%2520one-dimensional%2520convolutional%2520neural%2520network%2520for%2520audio%2520processing%2520with%2520a%2520gradient-boosted%2520decision%2520tree%2520for%2520tabular%2520features.%2520Its%2520principal%2520innovation%2520is%2520a%2520Cross-Modal%2520Bidirectional%2520Cross-Attention%2520module%2520%2528CM-BCA%2529%2520that%2520iteratively%2520exchanges%2520salient%2520cues%2520between%2520modalities%252C%2520emulating%2520the%2520way%2520clinicians%2520integrate%2520symptoms%2520and%2520risk%2520factors.%2520To%2520meet%2520the%2520clinical%2520priority%2520of%2520minimizing%2520missed%2520cases%252C%2520we%2520design%2520a%2520Tuberculosis%2520Risk-Balanced%2520Loss%2520%2528TRBL%2529%2520that%2520places%2520stronger%2520penalties%2520on%2520false-negative%2520predictions%252C%2520thereby%2520reducing%2520high-risk%2520misclassifications.%2520DeepGB-TB%2520is%2520evaluated%2520on%2520a%2520diverse%2520dataset%2520of%25201%252C105%2520patients%2520collected%2520across%2520seven%2520countries%252C%2520achieving%2520an%2520AUROC%2520of%25200.903%2520and%2520an%2520F1-score%2520of%25200.851%252C%2520representing%2520a%2520new%2520state%2520of%2520the%2520art.%2520Its%2520computational%2520efficiency%2520enables%2520real-time%252C%2520offline%2520inference%2520directly%2520on%2520common%2520mobile%2520devices%252C%2520making%2520it%2520ideal%2520for%2520low-resource%2520settings.%2520Importantly%252C%2520the%2520system%2520produces%2520clinically%2520validated%2520explanations%2520that%2520promote%2520trust%2520and%2520adoption%2520by%2520frontline%2520health%2520workers.%2520By%2520coupling%2520AI%2520innovation%2520with%2520public-health%2520requirements%2520for%2520speed%252C%2520affordability%252C%2520and%2520reliability%252C%2520DeepGB-TB%2520offers%2520a%2520tool%2520for%2520advancing%2520global%2520TB%2520control.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02741v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepGB-TB%3A%20A%20Risk-Balanced%20Cross-Attention%20Gradient-Boosted%20Convolutional%20Network%20for%20Rapid%2C%20Interpretable%20Tuberculosis%20Screening&entry.906535625=Zhixiang%20Lu%20and%20Yulong%20Li%20and%20Feilong%20Tang%20and%20Zhengyong%20Jiang%20and%20Chong%20Li%20and%20Mian%20Zhou%20and%20Tenglong%20Li%20and%20Jionglong%20Su&entry.1292438233=Large-scale%20tuberculosis%20%28TB%29%20screening%20is%20limited%20by%20the%20high%20cost%20and%20operational%20complexity%20of%20traditional%20diagnostics%2C%20creating%20a%20need%20for%20artificial-intelligence%20solutions.%20We%20propose%20DeepGB-TB%2C%20a%20non-invasive%20system%20that%20instantly%20assigns%20TB%20risk%20scores%20using%20only%20cough%20audio%20and%20basic%20demographic%20data.%20The%20model%20couples%20a%20lightweight%20one-dimensional%20convolutional%20neural%20network%20for%20audio%20processing%20with%20a%20gradient-boosted%20decision%20tree%20for%20tabular%20features.%20Its%20principal%20innovation%20is%20a%20Cross-Modal%20Bidirectional%20Cross-Attention%20module%20%28CM-BCA%29%20that%20iteratively%20exchanges%20salient%20cues%20between%20modalities%2C%20emulating%20the%20way%20clinicians%20integrate%20symptoms%20and%20risk%20factors.%20To%20meet%20the%20clinical%20priority%20of%20minimizing%20missed%20cases%2C%20we%20design%20a%20Tuberculosis%20Risk-Balanced%20Loss%20%28TRBL%29%20that%20places%20stronger%20penalties%20on%20false-negative%20predictions%2C%20thereby%20reducing%20high-risk%20misclassifications.%20DeepGB-TB%20is%20evaluated%20on%20a%20diverse%20dataset%20of%201%2C105%20patients%20collected%20across%20seven%20countries%2C%20achieving%20an%20AUROC%20of%200.903%20and%20an%20F1-score%20of%200.851%2C%20representing%20a%20new%20state%20of%20the%20art.%20Its%20computational%20efficiency%20enables%20real-time%2C%20offline%20inference%20directly%20on%20common%20mobile%20devices%2C%20making%20it%20ideal%20for%20low-resource%20settings.%20Importantly%2C%20the%20system%20produces%20clinically%20validated%20explanations%20that%20promote%20trust%20and%20adoption%20by%20frontline%20health%20workers.%20By%20coupling%20AI%20innovation%20with%20public-health%20requirements%20for%20speed%2C%20affordability%2C%20and%20reliability%2C%20DeepGB-TB%20offers%20a%20tool%20for%20advancing%20global%20TB%20control.&entry.1838667208=http%3A//arxiv.org/abs/2508.02741v2&entry.124074799=Read"},
{"title": "Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning", "author": "Manjie Xu and Isabella Yin and Xinyi Tu and Chi Zhang and Yixin Zhu", "abstract": "LLMs struggle with Semantic Inertia: the inability to inhibit pre-trained priors (e.g., \"Lava is Dangerous\") when dynamic, in-context rules contradict them. We probe this phenomenon using Baba Is You, where physical laws are mutable text rules, enabling precise evaluation of models' ability to override learned priors when rules change. We quantatively observe that larger models can exhibit inverse scaling: they perform worse than smaller models when natural language reasoning requires suppressing pre-trained associations (e.g., accepting \"Lava is Safe\"). Our analysis attributes this to natural language encoding, which entangles descriptive semantics and logical rules, leading to persistent hallucinations of familiar physics despite explicit contradictory rules. Here we show that representing dynamics as executable code, rather than descriptive text, reverses this trend and enables effective prior inhibition. We introduce Code-Grounded Vistas (LCV), which fine-tunes models on counterfactual pairs and identifies states with contradictory rules, thereby forcing attention to logical constraints rather than visual semantics. This training-time approach outperforms expensive inference-time search methods in both efficiency and accuracy. Our results demonstrate that representation fundamentally determines whether scaling improves or impairs contextual reasoning. This challenges the assumption that larger models are universally better, with implications for domains that require dynamic overriding of learned priors.", "link": "http://arxiv.org/abs/2601.18352v2", "date": "2026-02-02", "relevancy": 2.3446, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5834}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Code%20over%20Words%3A%20Overcoming%20Semantic%20Inertia%20via%20Code-Grounded%20Reasoning&body=Title%3A%20Code%20over%20Words%3A%20Overcoming%20Semantic%20Inertia%20via%20Code-Grounded%20Reasoning%0AAuthor%3A%20Manjie%20Xu%20and%20Isabella%20Yin%20and%20Xinyi%20Tu%20and%20Chi%20Zhang%20and%20Yixin%20Zhu%0AAbstract%3A%20LLMs%20struggle%20with%20Semantic%20Inertia%3A%20the%20inability%20to%20inhibit%20pre-trained%20priors%20%28e.g.%2C%20%22Lava%20is%20Dangerous%22%29%20when%20dynamic%2C%20in-context%20rules%20contradict%20them.%20We%20probe%20this%20phenomenon%20using%20Baba%20Is%20You%2C%20where%20physical%20laws%20are%20mutable%20text%20rules%2C%20enabling%20precise%20evaluation%20of%20models%27%20ability%20to%20override%20learned%20priors%20when%20rules%20change.%20We%20quantatively%20observe%20that%20larger%20models%20can%20exhibit%20inverse%20scaling%3A%20they%20perform%20worse%20than%20smaller%20models%20when%20natural%20language%20reasoning%20requires%20suppressing%20pre-trained%20associations%20%28e.g.%2C%20accepting%20%22Lava%20is%20Safe%22%29.%20Our%20analysis%20attributes%20this%20to%20natural%20language%20encoding%2C%20which%20entangles%20descriptive%20semantics%20and%20logical%20rules%2C%20leading%20to%20persistent%20hallucinations%20of%20familiar%20physics%20despite%20explicit%20contradictory%20rules.%20Here%20we%20show%20that%20representing%20dynamics%20as%20executable%20code%2C%20rather%20than%20descriptive%20text%2C%20reverses%20this%20trend%20and%20enables%20effective%20prior%20inhibition.%20We%20introduce%20Code-Grounded%20Vistas%20%28LCV%29%2C%20which%20fine-tunes%20models%20on%20counterfactual%20pairs%20and%20identifies%20states%20with%20contradictory%20rules%2C%20thereby%20forcing%20attention%20to%20logical%20constraints%20rather%20than%20visual%20semantics.%20This%20training-time%20approach%20outperforms%20expensive%20inference-time%20search%20methods%20in%20both%20efficiency%20and%20accuracy.%20Our%20results%20demonstrate%20that%20representation%20fundamentally%20determines%20whether%20scaling%20improves%20or%20impairs%20contextual%20reasoning.%20This%20challenges%20the%20assumption%20that%20larger%20models%20are%20universally%20better%2C%20with%20implications%20for%20domains%20that%20require%20dynamic%20overriding%20of%20learned%20priors.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18352v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCode%2520over%2520Words%253A%2520Overcoming%2520Semantic%2520Inertia%2520via%2520Code-Grounded%2520Reasoning%26entry.906535625%3DManjie%2520Xu%2520and%2520Isabella%2520Yin%2520and%2520Xinyi%2520Tu%2520and%2520Chi%2520Zhang%2520and%2520Yixin%2520Zhu%26entry.1292438233%3DLLMs%2520struggle%2520with%2520Semantic%2520Inertia%253A%2520the%2520inability%2520to%2520inhibit%2520pre-trained%2520priors%2520%2528e.g.%252C%2520%2522Lava%2520is%2520Dangerous%2522%2529%2520when%2520dynamic%252C%2520in-context%2520rules%2520contradict%2520them.%2520We%2520probe%2520this%2520phenomenon%2520using%2520Baba%2520Is%2520You%252C%2520where%2520physical%2520laws%2520are%2520mutable%2520text%2520rules%252C%2520enabling%2520precise%2520evaluation%2520of%2520models%2527%2520ability%2520to%2520override%2520learned%2520priors%2520when%2520rules%2520change.%2520We%2520quantatively%2520observe%2520that%2520larger%2520models%2520can%2520exhibit%2520inverse%2520scaling%253A%2520they%2520perform%2520worse%2520than%2520smaller%2520models%2520when%2520natural%2520language%2520reasoning%2520requires%2520suppressing%2520pre-trained%2520associations%2520%2528e.g.%252C%2520accepting%2520%2522Lava%2520is%2520Safe%2522%2529.%2520Our%2520analysis%2520attributes%2520this%2520to%2520natural%2520language%2520encoding%252C%2520which%2520entangles%2520descriptive%2520semantics%2520and%2520logical%2520rules%252C%2520leading%2520to%2520persistent%2520hallucinations%2520of%2520familiar%2520physics%2520despite%2520explicit%2520contradictory%2520rules.%2520Here%2520we%2520show%2520that%2520representing%2520dynamics%2520as%2520executable%2520code%252C%2520rather%2520than%2520descriptive%2520text%252C%2520reverses%2520this%2520trend%2520and%2520enables%2520effective%2520prior%2520inhibition.%2520We%2520introduce%2520Code-Grounded%2520Vistas%2520%2528LCV%2529%252C%2520which%2520fine-tunes%2520models%2520on%2520counterfactual%2520pairs%2520and%2520identifies%2520states%2520with%2520contradictory%2520rules%252C%2520thereby%2520forcing%2520attention%2520to%2520logical%2520constraints%2520rather%2520than%2520visual%2520semantics.%2520This%2520training-time%2520approach%2520outperforms%2520expensive%2520inference-time%2520search%2520methods%2520in%2520both%2520efficiency%2520and%2520accuracy.%2520Our%2520results%2520demonstrate%2520that%2520representation%2520fundamentally%2520determines%2520whether%2520scaling%2520improves%2520or%2520impairs%2520contextual%2520reasoning.%2520This%2520challenges%2520the%2520assumption%2520that%2520larger%2520models%2520are%2520universally%2520better%252C%2520with%2520implications%2520for%2520domains%2520that%2520require%2520dynamic%2520overriding%2520of%2520learned%2520priors.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18352v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Code%20over%20Words%3A%20Overcoming%20Semantic%20Inertia%20via%20Code-Grounded%20Reasoning&entry.906535625=Manjie%20Xu%20and%20Isabella%20Yin%20and%20Xinyi%20Tu%20and%20Chi%20Zhang%20and%20Yixin%20Zhu&entry.1292438233=LLMs%20struggle%20with%20Semantic%20Inertia%3A%20the%20inability%20to%20inhibit%20pre-trained%20priors%20%28e.g.%2C%20%22Lava%20is%20Dangerous%22%29%20when%20dynamic%2C%20in-context%20rules%20contradict%20them.%20We%20probe%20this%20phenomenon%20using%20Baba%20Is%20You%2C%20where%20physical%20laws%20are%20mutable%20text%20rules%2C%20enabling%20precise%20evaluation%20of%20models%27%20ability%20to%20override%20learned%20priors%20when%20rules%20change.%20We%20quantatively%20observe%20that%20larger%20models%20can%20exhibit%20inverse%20scaling%3A%20they%20perform%20worse%20than%20smaller%20models%20when%20natural%20language%20reasoning%20requires%20suppressing%20pre-trained%20associations%20%28e.g.%2C%20accepting%20%22Lava%20is%20Safe%22%29.%20Our%20analysis%20attributes%20this%20to%20natural%20language%20encoding%2C%20which%20entangles%20descriptive%20semantics%20and%20logical%20rules%2C%20leading%20to%20persistent%20hallucinations%20of%20familiar%20physics%20despite%20explicit%20contradictory%20rules.%20Here%20we%20show%20that%20representing%20dynamics%20as%20executable%20code%2C%20rather%20than%20descriptive%20text%2C%20reverses%20this%20trend%20and%20enables%20effective%20prior%20inhibition.%20We%20introduce%20Code-Grounded%20Vistas%20%28LCV%29%2C%20which%20fine-tunes%20models%20on%20counterfactual%20pairs%20and%20identifies%20states%20with%20contradictory%20rules%2C%20thereby%20forcing%20attention%20to%20logical%20constraints%20rather%20than%20visual%20semantics.%20This%20training-time%20approach%20outperforms%20expensive%20inference-time%20search%20methods%20in%20both%20efficiency%20and%20accuracy.%20Our%20results%20demonstrate%20that%20representation%20fundamentally%20determines%20whether%20scaling%20improves%20or%20impairs%20contextual%20reasoning.%20This%20challenges%20the%20assumption%20that%20larger%20models%20are%20universally%20better%2C%20with%20implications%20for%20domains%20that%20require%20dynamic%20overriding%20of%20learned%20priors.&entry.1838667208=http%3A//arxiv.org/abs/2601.18352v2&entry.124074799=Read"},
{"title": "CIEC: Coupling Implicit and Explicit Cues for Multimodal Weakly Supervised Manipulation Localization", "author": "Xinquan Yu and Wei Lu and Xiangyang Luo", "abstract": "To mitigate the threat of misinformation, multimodal manipulation localization has garnered growing attention. Consider that current methods rely on costly and time-consuming fine-grained annotations, such as patch/token-level annotations. This paper proposes a novel framework named Coupling Implicit and Explicit Cues (CIEC), which aims to achieve multimodal weakly-supervised manipulation localization for image-text pairs utilizing only coarse-grained image/sentence-level annotations. It comprises two branches, image-based and text-based weakly-supervised localization. For the former, we devise the Textual-guidance Refine Patch Selection (TRPS) module. It integrates forgery cues from both visual and textual perspectives to lock onto suspicious regions aided by spatial priors. Followed by the background silencing and spatial contrast constraints to suppress interference from irrelevant areas. For the latter, we devise the Visual-deviation Calibrated Token Grounding (VCTG) module. It focuses on meaningful content words and leverages relative visual bias to assist token localization. Followed by the asymmetric sparse and semantic consistency constraints to mitigate label noise and ensure reliability. Extensive experiments demonstrate the effectiveness of our CIEC, yielding results comparable to fully supervised methods on several evaluation metrics.", "link": "http://arxiv.org/abs/2602.02175v1", "date": "2026-02-02", "relevancy": 2.3431, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6349}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5567}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CIEC%3A%20Coupling%20Implicit%20and%20Explicit%20Cues%20for%20Multimodal%20Weakly%20Supervised%20Manipulation%20Localization&body=Title%3A%20CIEC%3A%20Coupling%20Implicit%20and%20Explicit%20Cues%20for%20Multimodal%20Weakly%20Supervised%20Manipulation%20Localization%0AAuthor%3A%20Xinquan%20Yu%20and%20Wei%20Lu%20and%20Xiangyang%20Luo%0AAbstract%3A%20To%20mitigate%20the%20threat%20of%20misinformation%2C%20multimodal%20manipulation%20localization%20has%20garnered%20growing%20attention.%20Consider%20that%20current%20methods%20rely%20on%20costly%20and%20time-consuming%20fine-grained%20annotations%2C%20such%20as%20patch/token-level%20annotations.%20This%20paper%20proposes%20a%20novel%20framework%20named%20Coupling%20Implicit%20and%20Explicit%20Cues%20%28CIEC%29%2C%20which%20aims%20to%20achieve%20multimodal%20weakly-supervised%20manipulation%20localization%20for%20image-text%20pairs%20utilizing%20only%20coarse-grained%20image/sentence-level%20annotations.%20It%20comprises%20two%20branches%2C%20image-based%20and%20text-based%20weakly-supervised%20localization.%20For%20the%20former%2C%20we%20devise%20the%20Textual-guidance%20Refine%20Patch%20Selection%20%28TRPS%29%20module.%20It%20integrates%20forgery%20cues%20from%20both%20visual%20and%20textual%20perspectives%20to%20lock%20onto%20suspicious%20regions%20aided%20by%20spatial%20priors.%20Followed%20by%20the%20background%20silencing%20and%20spatial%20contrast%20constraints%20to%20suppress%20interference%20from%20irrelevant%20areas.%20For%20the%20latter%2C%20we%20devise%20the%20Visual-deviation%20Calibrated%20Token%20Grounding%20%28VCTG%29%20module.%20It%20focuses%20on%20meaningful%20content%20words%20and%20leverages%20relative%20visual%20bias%20to%20assist%20token%20localization.%20Followed%20by%20the%20asymmetric%20sparse%20and%20semantic%20consistency%20constraints%20to%20mitigate%20label%20noise%20and%20ensure%20reliability.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20CIEC%2C%20yielding%20results%20comparable%20to%20fully%20supervised%20methods%20on%20several%20evaluation%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCIEC%253A%2520Coupling%2520Implicit%2520and%2520Explicit%2520Cues%2520for%2520Multimodal%2520Weakly%2520Supervised%2520Manipulation%2520Localization%26entry.906535625%3DXinquan%2520Yu%2520and%2520Wei%2520Lu%2520and%2520Xiangyang%2520Luo%26entry.1292438233%3DTo%2520mitigate%2520the%2520threat%2520of%2520misinformation%252C%2520multimodal%2520manipulation%2520localization%2520has%2520garnered%2520growing%2520attention.%2520Consider%2520that%2520current%2520methods%2520rely%2520on%2520costly%2520and%2520time-consuming%2520fine-grained%2520annotations%252C%2520such%2520as%2520patch/token-level%2520annotations.%2520This%2520paper%2520proposes%2520a%2520novel%2520framework%2520named%2520Coupling%2520Implicit%2520and%2520Explicit%2520Cues%2520%2528CIEC%2529%252C%2520which%2520aims%2520to%2520achieve%2520multimodal%2520weakly-supervised%2520manipulation%2520localization%2520for%2520image-text%2520pairs%2520utilizing%2520only%2520coarse-grained%2520image/sentence-level%2520annotations.%2520It%2520comprises%2520two%2520branches%252C%2520image-based%2520and%2520text-based%2520weakly-supervised%2520localization.%2520For%2520the%2520former%252C%2520we%2520devise%2520the%2520Textual-guidance%2520Refine%2520Patch%2520Selection%2520%2528TRPS%2529%2520module.%2520It%2520integrates%2520forgery%2520cues%2520from%2520both%2520visual%2520and%2520textual%2520perspectives%2520to%2520lock%2520onto%2520suspicious%2520regions%2520aided%2520by%2520spatial%2520priors.%2520Followed%2520by%2520the%2520background%2520silencing%2520and%2520spatial%2520contrast%2520constraints%2520to%2520suppress%2520interference%2520from%2520irrelevant%2520areas.%2520For%2520the%2520latter%252C%2520we%2520devise%2520the%2520Visual-deviation%2520Calibrated%2520Token%2520Grounding%2520%2528VCTG%2529%2520module.%2520It%2520focuses%2520on%2520meaningful%2520content%2520words%2520and%2520leverages%2520relative%2520visual%2520bias%2520to%2520assist%2520token%2520localization.%2520Followed%2520by%2520the%2520asymmetric%2520sparse%2520and%2520semantic%2520consistency%2520constraints%2520to%2520mitigate%2520label%2520noise%2520and%2520ensure%2520reliability.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520CIEC%252C%2520yielding%2520results%2520comparable%2520to%2520fully%2520supervised%2520methods%2520on%2520several%2520evaluation%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CIEC%3A%20Coupling%20Implicit%20and%20Explicit%20Cues%20for%20Multimodal%20Weakly%20Supervised%20Manipulation%20Localization&entry.906535625=Xinquan%20Yu%20and%20Wei%20Lu%20and%20Xiangyang%20Luo&entry.1292438233=To%20mitigate%20the%20threat%20of%20misinformation%2C%20multimodal%20manipulation%20localization%20has%20garnered%20growing%20attention.%20Consider%20that%20current%20methods%20rely%20on%20costly%20and%20time-consuming%20fine-grained%20annotations%2C%20such%20as%20patch/token-level%20annotations.%20This%20paper%20proposes%20a%20novel%20framework%20named%20Coupling%20Implicit%20and%20Explicit%20Cues%20%28CIEC%29%2C%20which%20aims%20to%20achieve%20multimodal%20weakly-supervised%20manipulation%20localization%20for%20image-text%20pairs%20utilizing%20only%20coarse-grained%20image/sentence-level%20annotations.%20It%20comprises%20two%20branches%2C%20image-based%20and%20text-based%20weakly-supervised%20localization.%20For%20the%20former%2C%20we%20devise%20the%20Textual-guidance%20Refine%20Patch%20Selection%20%28TRPS%29%20module.%20It%20integrates%20forgery%20cues%20from%20both%20visual%20and%20textual%20perspectives%20to%20lock%20onto%20suspicious%20regions%20aided%20by%20spatial%20priors.%20Followed%20by%20the%20background%20silencing%20and%20spatial%20contrast%20constraints%20to%20suppress%20interference%20from%20irrelevant%20areas.%20For%20the%20latter%2C%20we%20devise%20the%20Visual-deviation%20Calibrated%20Token%20Grounding%20%28VCTG%29%20module.%20It%20focuses%20on%20meaningful%20content%20words%20and%20leverages%20relative%20visual%20bias%20to%20assist%20token%20localization.%20Followed%20by%20the%20asymmetric%20sparse%20and%20semantic%20consistency%20constraints%20to%20mitigate%20label%20noise%20and%20ensure%20reliability.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20CIEC%2C%20yielding%20results%20comparable%20to%20fully%20supervised%20methods%20on%20several%20evaluation%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2602.02175v1&entry.124074799=Read"},
{"title": "SimMerge: Learning to Select Merge Operators from Similarity Signals", "author": "Oliver Bolton and  Aakanksha and Arash Ahmadian and Sara Hooker and Marzieh Fadaee and Beyza Ermis", "abstract": "Model merging combines multiple models into a single model with aggregated capabilities, making it a powerful tool for large language model (LLM) development. However, scaling model merging is challenging: performance depends on the choice of merge operator, model subset, and merge order, often requiring expensive merge-and-evaluate searches. In this work, we introduce SimMerge, a predictive merge-selection method that identifies high-performing merges using inexpensive, task-agnostic similarity signals between models. Given a small set of unlabeled probes, SimMerge extracts functional and structural features to predict the performance of candidate two-way merges, enabling merge operator, order and model subset selection without iterative evaluation. We show that SimMerge consistently outperforms the best fixed merge operator across 7B-parameter LLMs and generalizes to multi-way merges and 111B-parameter LLMs without retraining. We further introduce a bandit variant that supports adding new tasks and operators online. Our results suggest that learning how to merge enables scalable model composition when checkpoint catalogs are large and evaluation budgets are limited.", "link": "http://arxiv.org/abs/2601.09473v2", "date": "2026-02-02", "relevancy": 2.3348, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4839}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimMerge%3A%20Learning%20to%20Select%20Merge%20Operators%20from%20Similarity%20Signals&body=Title%3A%20SimMerge%3A%20Learning%20to%20Select%20Merge%20Operators%20from%20Similarity%20Signals%0AAuthor%3A%20Oliver%20Bolton%20and%20%20Aakanksha%20and%20Arash%20Ahmadian%20and%20Sara%20Hooker%20and%20Marzieh%20Fadaee%20and%20Beyza%20Ermis%0AAbstract%3A%20Model%20merging%20combines%20multiple%20models%20into%20a%20single%20model%20with%20aggregated%20capabilities%2C%20making%20it%20a%20powerful%20tool%20for%20large%20language%20model%20%28LLM%29%20development.%20However%2C%20scaling%20model%20merging%20is%20challenging%3A%20performance%20depends%20on%20the%20choice%20of%20merge%20operator%2C%20model%20subset%2C%20and%20merge%20order%2C%20often%20requiring%20expensive%20merge-and-evaluate%20searches.%20In%20this%20work%2C%20we%20introduce%20SimMerge%2C%20a%20predictive%20merge-selection%20method%20that%20identifies%20high-performing%20merges%20using%20inexpensive%2C%20task-agnostic%20similarity%20signals%20between%20models.%20Given%20a%20small%20set%20of%20unlabeled%20probes%2C%20SimMerge%20extracts%20functional%20and%20structural%20features%20to%20predict%20the%20performance%20of%20candidate%20two-way%20merges%2C%20enabling%20merge%20operator%2C%20order%20and%20model%20subset%20selection%20without%20iterative%20evaluation.%20We%20show%20that%20SimMerge%20consistently%20outperforms%20the%20best%20fixed%20merge%20operator%20across%207B-parameter%20LLMs%20and%20generalizes%20to%20multi-way%20merges%20and%20111B-parameter%20LLMs%20without%20retraining.%20We%20further%20introduce%20a%20bandit%20variant%20that%20supports%20adding%20new%20tasks%20and%20operators%20online.%20Our%20results%20suggest%20that%20learning%20how%20to%20merge%20enables%20scalable%20model%20composition%20when%20checkpoint%20catalogs%20are%20large%20and%20evaluation%20budgets%20are%20limited.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09473v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimMerge%253A%2520Learning%2520to%2520Select%2520Merge%2520Operators%2520from%2520Similarity%2520Signals%26entry.906535625%3DOliver%2520Bolton%2520and%2520%2520Aakanksha%2520and%2520Arash%2520Ahmadian%2520and%2520Sara%2520Hooker%2520and%2520Marzieh%2520Fadaee%2520and%2520Beyza%2520Ermis%26entry.1292438233%3DModel%2520merging%2520combines%2520multiple%2520models%2520into%2520a%2520single%2520model%2520with%2520aggregated%2520capabilities%252C%2520making%2520it%2520a%2520powerful%2520tool%2520for%2520large%2520language%2520model%2520%2528LLM%2529%2520development.%2520However%252C%2520scaling%2520model%2520merging%2520is%2520challenging%253A%2520performance%2520depends%2520on%2520the%2520choice%2520of%2520merge%2520operator%252C%2520model%2520subset%252C%2520and%2520merge%2520order%252C%2520often%2520requiring%2520expensive%2520merge-and-evaluate%2520searches.%2520In%2520this%2520work%252C%2520we%2520introduce%2520SimMerge%252C%2520a%2520predictive%2520merge-selection%2520method%2520that%2520identifies%2520high-performing%2520merges%2520using%2520inexpensive%252C%2520task-agnostic%2520similarity%2520signals%2520between%2520models.%2520Given%2520a%2520small%2520set%2520of%2520unlabeled%2520probes%252C%2520SimMerge%2520extracts%2520functional%2520and%2520structural%2520features%2520to%2520predict%2520the%2520performance%2520of%2520candidate%2520two-way%2520merges%252C%2520enabling%2520merge%2520operator%252C%2520order%2520and%2520model%2520subset%2520selection%2520without%2520iterative%2520evaluation.%2520We%2520show%2520that%2520SimMerge%2520consistently%2520outperforms%2520the%2520best%2520fixed%2520merge%2520operator%2520across%25207B-parameter%2520LLMs%2520and%2520generalizes%2520to%2520multi-way%2520merges%2520and%2520111B-parameter%2520LLMs%2520without%2520retraining.%2520We%2520further%2520introduce%2520a%2520bandit%2520variant%2520that%2520supports%2520adding%2520new%2520tasks%2520and%2520operators%2520online.%2520Our%2520results%2520suggest%2520that%2520learning%2520how%2520to%2520merge%2520enables%2520scalable%2520model%2520composition%2520when%2520checkpoint%2520catalogs%2520are%2520large%2520and%2520evaluation%2520budgets%2520are%2520limited.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09473v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimMerge%3A%20Learning%20to%20Select%20Merge%20Operators%20from%20Similarity%20Signals&entry.906535625=Oliver%20Bolton%20and%20%20Aakanksha%20and%20Arash%20Ahmadian%20and%20Sara%20Hooker%20and%20Marzieh%20Fadaee%20and%20Beyza%20Ermis&entry.1292438233=Model%20merging%20combines%20multiple%20models%20into%20a%20single%20model%20with%20aggregated%20capabilities%2C%20making%20it%20a%20powerful%20tool%20for%20large%20language%20model%20%28LLM%29%20development.%20However%2C%20scaling%20model%20merging%20is%20challenging%3A%20performance%20depends%20on%20the%20choice%20of%20merge%20operator%2C%20model%20subset%2C%20and%20merge%20order%2C%20often%20requiring%20expensive%20merge-and-evaluate%20searches.%20In%20this%20work%2C%20we%20introduce%20SimMerge%2C%20a%20predictive%20merge-selection%20method%20that%20identifies%20high-performing%20merges%20using%20inexpensive%2C%20task-agnostic%20similarity%20signals%20between%20models.%20Given%20a%20small%20set%20of%20unlabeled%20probes%2C%20SimMerge%20extracts%20functional%20and%20structural%20features%20to%20predict%20the%20performance%20of%20candidate%20two-way%20merges%2C%20enabling%20merge%20operator%2C%20order%20and%20model%20subset%20selection%20without%20iterative%20evaluation.%20We%20show%20that%20SimMerge%20consistently%20outperforms%20the%20best%20fixed%20merge%20operator%20across%207B-parameter%20LLMs%20and%20generalizes%20to%20multi-way%20merges%20and%20111B-parameter%20LLMs%20without%20retraining.%20We%20further%20introduce%20a%20bandit%20variant%20that%20supports%20adding%20new%20tasks%20and%20operators%20online.%20Our%20results%20suggest%20that%20learning%20how%20to%20merge%20enables%20scalable%20model%20composition%20when%20checkpoint%20catalogs%20are%20large%20and%20evaluation%20budgets%20are%20limited.&entry.1838667208=http%3A//arxiv.org/abs/2601.09473v2&entry.124074799=Read"},
{"title": "A Large-Scale Dataset for Molecular Structure-Language Description via a Rule-Regularized Method", "author": "Feiyang Cai and Guijuan He and Yi Hu and Jingjing Wang and Joshua Luo and Tianyu Zhu and Srikanth Pilla and Gang Li and Ling Liu and Feng Luo", "abstract": "Molecular function is largely determined by structure. Accurately aligning molecular structure with natural language is therefore essential for enabling large language models (LLMs) to reason about downstream chemical tasks. However, the substantial cost of human annotation makes it infeasible to construct large-scale, high-quality datasets of structure-grounded descriptions. In this work, we propose a fully automated annotation framework for generating precise molecular structure descriptions at scale. Our approach builds upon and extends a rule-based chemical nomenclature parser to interpret IUPAC names and construct enriched, structured XML metadata that explicitly encodes molecular structure. This metadata is then used to guide LLMs in producing accurate natural-language descriptions. Using this framework, we curate a large-scale dataset of approximately $163$k molecule-description pairs. A rigorous validation protocol combining LLM-based and expert human evaluation on a subset of $2,000$ molecules demonstrates a high description precision of $98.6\\%$. The resulting dataset provides a reliable foundation for future molecule-language alignment, and the proposed annotation method is readily extensible to larger datasets and broader chemical tasks that rely on structural descriptions.", "link": "http://arxiv.org/abs/2602.02320v1", "date": "2026-02-02", "relevancy": 2.3342, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4718}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4718}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Large-Scale%20Dataset%20for%20Molecular%20Structure-Language%20Description%20via%20a%20Rule-Regularized%20Method&body=Title%3A%20A%20Large-Scale%20Dataset%20for%20Molecular%20Structure-Language%20Description%20via%20a%20Rule-Regularized%20Method%0AAuthor%3A%20Feiyang%20Cai%20and%20Guijuan%20He%20and%20Yi%20Hu%20and%20Jingjing%20Wang%20and%20Joshua%20Luo%20and%20Tianyu%20Zhu%20and%20Srikanth%20Pilla%20and%20Gang%20Li%20and%20Ling%20Liu%20and%20Feng%20Luo%0AAbstract%3A%20Molecular%20function%20is%20largely%20determined%20by%20structure.%20Accurately%20aligning%20molecular%20structure%20with%20natural%20language%20is%20therefore%20essential%20for%20enabling%20large%20language%20models%20%28LLMs%29%20to%20reason%20about%20downstream%20chemical%20tasks.%20However%2C%20the%20substantial%20cost%20of%20human%20annotation%20makes%20it%20infeasible%20to%20construct%20large-scale%2C%20high-quality%20datasets%20of%20structure-grounded%20descriptions.%20In%20this%20work%2C%20we%20propose%20a%20fully%20automated%20annotation%20framework%20for%20generating%20precise%20molecular%20structure%20descriptions%20at%20scale.%20Our%20approach%20builds%20upon%20and%20extends%20a%20rule-based%20chemical%20nomenclature%20parser%20to%20interpret%20IUPAC%20names%20and%20construct%20enriched%2C%20structured%20XML%20metadata%20that%20explicitly%20encodes%20molecular%20structure.%20This%20metadata%20is%20then%20used%20to%20guide%20LLMs%20in%20producing%20accurate%20natural-language%20descriptions.%20Using%20this%20framework%2C%20we%20curate%20a%20large-scale%20dataset%20of%20approximately%20%24163%24k%20molecule-description%20pairs.%20A%20rigorous%20validation%20protocol%20combining%20LLM-based%20and%20expert%20human%20evaluation%20on%20a%20subset%20of%20%242%2C000%24%20molecules%20demonstrates%20a%20high%20description%20precision%20of%20%2498.6%5C%25%24.%20The%20resulting%20dataset%20provides%20a%20reliable%20foundation%20for%20future%20molecule-language%20alignment%2C%20and%20the%20proposed%20annotation%20method%20is%20readily%20extensible%20to%20larger%20datasets%20and%20broader%20chemical%20tasks%20that%20rely%20on%20structural%20descriptions.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Large-Scale%2520Dataset%2520for%2520Molecular%2520Structure-Language%2520Description%2520via%2520a%2520Rule-Regularized%2520Method%26entry.906535625%3DFeiyang%2520Cai%2520and%2520Guijuan%2520He%2520and%2520Yi%2520Hu%2520and%2520Jingjing%2520Wang%2520and%2520Joshua%2520Luo%2520and%2520Tianyu%2520Zhu%2520and%2520Srikanth%2520Pilla%2520and%2520Gang%2520Li%2520and%2520Ling%2520Liu%2520and%2520Feng%2520Luo%26entry.1292438233%3DMolecular%2520function%2520is%2520largely%2520determined%2520by%2520structure.%2520Accurately%2520aligning%2520molecular%2520structure%2520with%2520natural%2520language%2520is%2520therefore%2520essential%2520for%2520enabling%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520reason%2520about%2520downstream%2520chemical%2520tasks.%2520However%252C%2520the%2520substantial%2520cost%2520of%2520human%2520annotation%2520makes%2520it%2520infeasible%2520to%2520construct%2520large-scale%252C%2520high-quality%2520datasets%2520of%2520structure-grounded%2520descriptions.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520fully%2520automated%2520annotation%2520framework%2520for%2520generating%2520precise%2520molecular%2520structure%2520descriptions%2520at%2520scale.%2520Our%2520approach%2520builds%2520upon%2520and%2520extends%2520a%2520rule-based%2520chemical%2520nomenclature%2520parser%2520to%2520interpret%2520IUPAC%2520names%2520and%2520construct%2520enriched%252C%2520structured%2520XML%2520metadata%2520that%2520explicitly%2520encodes%2520molecular%2520structure.%2520This%2520metadata%2520is%2520then%2520used%2520to%2520guide%2520LLMs%2520in%2520producing%2520accurate%2520natural-language%2520descriptions.%2520Using%2520this%2520framework%252C%2520we%2520curate%2520a%2520large-scale%2520dataset%2520of%2520approximately%2520%2524163%2524k%2520molecule-description%2520pairs.%2520A%2520rigorous%2520validation%2520protocol%2520combining%2520LLM-based%2520and%2520expert%2520human%2520evaluation%2520on%2520a%2520subset%2520of%2520%25242%252C000%2524%2520molecules%2520demonstrates%2520a%2520high%2520description%2520precision%2520of%2520%252498.6%255C%2525%2524.%2520The%2520resulting%2520dataset%2520provides%2520a%2520reliable%2520foundation%2520for%2520future%2520molecule-language%2520alignment%252C%2520and%2520the%2520proposed%2520annotation%2520method%2520is%2520readily%2520extensible%2520to%2520larger%2520datasets%2520and%2520broader%2520chemical%2520tasks%2520that%2520rely%2520on%2520structural%2520descriptions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Large-Scale%20Dataset%20for%20Molecular%20Structure-Language%20Description%20via%20a%20Rule-Regularized%20Method&entry.906535625=Feiyang%20Cai%20and%20Guijuan%20He%20and%20Yi%20Hu%20and%20Jingjing%20Wang%20and%20Joshua%20Luo%20and%20Tianyu%20Zhu%20and%20Srikanth%20Pilla%20and%20Gang%20Li%20and%20Ling%20Liu%20and%20Feng%20Luo&entry.1292438233=Molecular%20function%20is%20largely%20determined%20by%20structure.%20Accurately%20aligning%20molecular%20structure%20with%20natural%20language%20is%20therefore%20essential%20for%20enabling%20large%20language%20models%20%28LLMs%29%20to%20reason%20about%20downstream%20chemical%20tasks.%20However%2C%20the%20substantial%20cost%20of%20human%20annotation%20makes%20it%20infeasible%20to%20construct%20large-scale%2C%20high-quality%20datasets%20of%20structure-grounded%20descriptions.%20In%20this%20work%2C%20we%20propose%20a%20fully%20automated%20annotation%20framework%20for%20generating%20precise%20molecular%20structure%20descriptions%20at%20scale.%20Our%20approach%20builds%20upon%20and%20extends%20a%20rule-based%20chemical%20nomenclature%20parser%20to%20interpret%20IUPAC%20names%20and%20construct%20enriched%2C%20structured%20XML%20metadata%20that%20explicitly%20encodes%20molecular%20structure.%20This%20metadata%20is%20then%20used%20to%20guide%20LLMs%20in%20producing%20accurate%20natural-language%20descriptions.%20Using%20this%20framework%2C%20we%20curate%20a%20large-scale%20dataset%20of%20approximately%20%24163%24k%20molecule-description%20pairs.%20A%20rigorous%20validation%20protocol%20combining%20LLM-based%20and%20expert%20human%20evaluation%20on%20a%20subset%20of%20%242%2C000%24%20molecules%20demonstrates%20a%20high%20description%20precision%20of%20%2498.6%5C%25%24.%20The%20resulting%20dataset%20provides%20a%20reliable%20foundation%20for%20future%20molecule-language%20alignment%2C%20and%20the%20proposed%20annotation%20method%20is%20readily%20extensible%20to%20larger%20datasets%20and%20broader%20chemical%20tasks%20that%20rely%20on%20structural%20descriptions.&entry.1838667208=http%3A//arxiv.org/abs/2602.02320v1&entry.124074799=Read"},
{"title": "Decoding Workload and Agreement From EEG During Spoken Dialogue With Conversational AI", "author": "Lucija Mihi\u0107 Zidar and Philipp Wicke and Praneel Bhatia and Rosa Lutz and Marius Klug and Thorsten O. Zander", "abstract": "Passive brain-computer interfaces offer a potential source of implicit feedback for alignment of large language models, but most mental state decoding has been done in controlled tasks. This paper investigates whether established EEG classifiers for mental workload and implicit agreement can be transferred to spoken human-AI dialogue. We introduce two conversational paradigms - a Spelling Bee task and a sentence completion task- and an end-to-end pipeline for transcribing, annotating, and aligning word-level conversational events with continuous EEG classifier output. In a pilot study, workload decoding showed interpretable trends during spoken interaction, supporting cross-paradigm transfer. For implicit agreement, we demonstrate continuous application and precise temporal alignment to conversational events, while identifying limitations related to construct transfer and asynchronous application of event-based classifiers. Overall, the results establish feasibility and constraints for integrating passive BCI signals into conversational AI systems.", "link": "http://arxiv.org/abs/2601.05825v2", "date": "2026-02-02", "relevancy": 2.3173, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4668}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4668}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20Workload%20and%20Agreement%20From%20EEG%20During%20Spoken%20Dialogue%20With%20Conversational%20AI&body=Title%3A%20Decoding%20Workload%20and%20Agreement%20From%20EEG%20During%20Spoken%20Dialogue%20With%20Conversational%20AI%0AAuthor%3A%20Lucija%20Mihi%C4%87%20Zidar%20and%20Philipp%20Wicke%20and%20Praneel%20Bhatia%20and%20Rosa%20Lutz%20and%20Marius%20Klug%20and%20Thorsten%20O.%20Zander%0AAbstract%3A%20Passive%20brain-computer%20interfaces%20offer%20a%20potential%20source%20of%20implicit%20feedback%20for%20alignment%20of%20large%20language%20models%2C%20but%20most%20mental%20state%20decoding%20has%20been%20done%20in%20controlled%20tasks.%20This%20paper%20investigates%20whether%20established%20EEG%20classifiers%20for%20mental%20workload%20and%20implicit%20agreement%20can%20be%20transferred%20to%20spoken%20human-AI%20dialogue.%20We%20introduce%20two%20conversational%20paradigms%20-%20a%20Spelling%20Bee%20task%20and%20a%20sentence%20completion%20task-%20and%20an%20end-to-end%20pipeline%20for%20transcribing%2C%20annotating%2C%20and%20aligning%20word-level%20conversational%20events%20with%20continuous%20EEG%20classifier%20output.%20In%20a%20pilot%20study%2C%20workload%20decoding%20showed%20interpretable%20trends%20during%20spoken%20interaction%2C%20supporting%20cross-paradigm%20transfer.%20For%20implicit%20agreement%2C%20we%20demonstrate%20continuous%20application%20and%20precise%20temporal%20alignment%20to%20conversational%20events%2C%20while%20identifying%20limitations%20related%20to%20construct%20transfer%20and%20asynchronous%20application%20of%20event-based%20classifiers.%20Overall%2C%20the%20results%20establish%20feasibility%20and%20constraints%20for%20integrating%20passive%20BCI%20signals%20into%20conversational%20AI%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05825v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520Workload%2520and%2520Agreement%2520From%2520EEG%2520During%2520Spoken%2520Dialogue%2520With%2520Conversational%2520AI%26entry.906535625%3DLucija%2520Mihi%25C4%2587%2520Zidar%2520and%2520Philipp%2520Wicke%2520and%2520Praneel%2520Bhatia%2520and%2520Rosa%2520Lutz%2520and%2520Marius%2520Klug%2520and%2520Thorsten%2520O.%2520Zander%26entry.1292438233%3DPassive%2520brain-computer%2520interfaces%2520offer%2520a%2520potential%2520source%2520of%2520implicit%2520feedback%2520for%2520alignment%2520of%2520large%2520language%2520models%252C%2520but%2520most%2520mental%2520state%2520decoding%2520has%2520been%2520done%2520in%2520controlled%2520tasks.%2520This%2520paper%2520investigates%2520whether%2520established%2520EEG%2520classifiers%2520for%2520mental%2520workload%2520and%2520implicit%2520agreement%2520can%2520be%2520transferred%2520to%2520spoken%2520human-AI%2520dialogue.%2520We%2520introduce%2520two%2520conversational%2520paradigms%2520-%2520a%2520Spelling%2520Bee%2520task%2520and%2520a%2520sentence%2520completion%2520task-%2520and%2520an%2520end-to-end%2520pipeline%2520for%2520transcribing%252C%2520annotating%252C%2520and%2520aligning%2520word-level%2520conversational%2520events%2520with%2520continuous%2520EEG%2520classifier%2520output.%2520In%2520a%2520pilot%2520study%252C%2520workload%2520decoding%2520showed%2520interpretable%2520trends%2520during%2520spoken%2520interaction%252C%2520supporting%2520cross-paradigm%2520transfer.%2520For%2520implicit%2520agreement%252C%2520we%2520demonstrate%2520continuous%2520application%2520and%2520precise%2520temporal%2520alignment%2520to%2520conversational%2520events%252C%2520while%2520identifying%2520limitations%2520related%2520to%2520construct%2520transfer%2520and%2520asynchronous%2520application%2520of%2520event-based%2520classifiers.%2520Overall%252C%2520the%2520results%2520establish%2520feasibility%2520and%2520constraints%2520for%2520integrating%2520passive%2520BCI%2520signals%2520into%2520conversational%2520AI%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05825v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20Workload%20and%20Agreement%20From%20EEG%20During%20Spoken%20Dialogue%20With%20Conversational%20AI&entry.906535625=Lucija%20Mihi%C4%87%20Zidar%20and%20Philipp%20Wicke%20and%20Praneel%20Bhatia%20and%20Rosa%20Lutz%20and%20Marius%20Klug%20and%20Thorsten%20O.%20Zander&entry.1292438233=Passive%20brain-computer%20interfaces%20offer%20a%20potential%20source%20of%20implicit%20feedback%20for%20alignment%20of%20large%20language%20models%2C%20but%20most%20mental%20state%20decoding%20has%20been%20done%20in%20controlled%20tasks.%20This%20paper%20investigates%20whether%20established%20EEG%20classifiers%20for%20mental%20workload%20and%20implicit%20agreement%20can%20be%20transferred%20to%20spoken%20human-AI%20dialogue.%20We%20introduce%20two%20conversational%20paradigms%20-%20a%20Spelling%20Bee%20task%20and%20a%20sentence%20completion%20task-%20and%20an%20end-to-end%20pipeline%20for%20transcribing%2C%20annotating%2C%20and%20aligning%20word-level%20conversational%20events%20with%20continuous%20EEG%20classifier%20output.%20In%20a%20pilot%20study%2C%20workload%20decoding%20showed%20interpretable%20trends%20during%20spoken%20interaction%2C%20supporting%20cross-paradigm%20transfer.%20For%20implicit%20agreement%2C%20we%20demonstrate%20continuous%20application%20and%20precise%20temporal%20alignment%20to%20conversational%20events%2C%20while%20identifying%20limitations%20related%20to%20construct%20transfer%20and%20asynchronous%20application%20of%20event-based%20classifiers.%20Overall%2C%20the%20results%20establish%20feasibility%20and%20constraints%20for%20integrating%20passive%20BCI%20signals%20into%20conversational%20AI%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.05825v2&entry.124074799=Read"},
{"title": "How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models", "author": "Parth Asawa and Alan Zhu and Abby O'Neill and Matei Zaharia and Alexandros G. Dimakis and Joseph E. Gonzalez", "abstract": "Frontier language models are deployed as black-box services, where model weights cannot be modified and customization is limited to prompting. We introduce Advisor Models, a method to train small open-weight models to generate dynamic, per-instance natural language advice that improves the capabilities of black-box frontier models. Advisor Models improve GPT-5's performance on RuleArena (Taxes) by 71%, reduce Gemini 3 Pro's steps taken in SWE agent tasks by 24.6%, and outperform static prompt optimizers in personalizing GPT-5 to user preferences (85-100% vs. 40-60%). We also find that advisors are transferable: an advisor trained with a low-cost student model still transfers improvements to a frontier model. Moreover, Advisor Models are robust: we observe no degradation on other benchmarks than the pipeline is trained on. Our method shows how to perform parametric optimization for black-box frontier models in a practical and cost-effective way.", "link": "http://arxiv.org/abs/2510.02453v2", "date": "2026-02-02", "relevancy": 2.3169, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5147}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4418}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Train%20Your%20Advisor%3A%20Steering%20Black-Box%20LLMs%20with%20Advisor%20Models&body=Title%3A%20How%20to%20Train%20Your%20Advisor%3A%20Steering%20Black-Box%20LLMs%20with%20Advisor%20Models%0AAuthor%3A%20Parth%20Asawa%20and%20Alan%20Zhu%20and%20Abby%20O%27Neill%20and%20Matei%20Zaharia%20and%20Alexandros%20G.%20Dimakis%20and%20Joseph%20E.%20Gonzalez%0AAbstract%3A%20Frontier%20language%20models%20are%20deployed%20as%20black-box%20services%2C%20where%20model%20weights%20cannot%20be%20modified%20and%20customization%20is%20limited%20to%20prompting.%20We%20introduce%20Advisor%20Models%2C%20a%20method%20to%20train%20small%20open-weight%20models%20to%20generate%20dynamic%2C%20per-instance%20natural%20language%20advice%20that%20improves%20the%20capabilities%20of%20black-box%20frontier%20models.%20Advisor%20Models%20improve%20GPT-5%27s%20performance%20on%20RuleArena%20%28Taxes%29%20by%2071%25%2C%20reduce%20Gemini%203%20Pro%27s%20steps%20taken%20in%20SWE%20agent%20tasks%20by%2024.6%25%2C%20and%20outperform%20static%20prompt%20optimizers%20in%20personalizing%20GPT-5%20to%20user%20preferences%20%2885-100%25%20vs.%2040-60%25%29.%20We%20also%20find%20that%20advisors%20are%20transferable%3A%20an%20advisor%20trained%20with%20a%20low-cost%20student%20model%20still%20transfers%20improvements%20to%20a%20frontier%20model.%20Moreover%2C%20Advisor%20Models%20are%20robust%3A%20we%20observe%20no%20degradation%20on%20other%20benchmarks%20than%20the%20pipeline%20is%20trained%20on.%20Our%20method%20shows%20how%20to%20perform%20parametric%20optimization%20for%20black-box%20frontier%20models%20in%20a%20practical%20and%20cost-effective%20way.%0ALink%3A%20http%3A//arxiv.org/abs/2510.02453v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Train%2520Your%2520Advisor%253A%2520Steering%2520Black-Box%2520LLMs%2520with%2520Advisor%2520Models%26entry.906535625%3DParth%2520Asawa%2520and%2520Alan%2520Zhu%2520and%2520Abby%2520O%2527Neill%2520and%2520Matei%2520Zaharia%2520and%2520Alexandros%2520G.%2520Dimakis%2520and%2520Joseph%2520E.%2520Gonzalez%26entry.1292438233%3DFrontier%2520language%2520models%2520are%2520deployed%2520as%2520black-box%2520services%252C%2520where%2520model%2520weights%2520cannot%2520be%2520modified%2520and%2520customization%2520is%2520limited%2520to%2520prompting.%2520We%2520introduce%2520Advisor%2520Models%252C%2520a%2520method%2520to%2520train%2520small%2520open-weight%2520models%2520to%2520generate%2520dynamic%252C%2520per-instance%2520natural%2520language%2520advice%2520that%2520improves%2520the%2520capabilities%2520of%2520black-box%2520frontier%2520models.%2520Advisor%2520Models%2520improve%2520GPT-5%2527s%2520performance%2520on%2520RuleArena%2520%2528Taxes%2529%2520by%252071%2525%252C%2520reduce%2520Gemini%25203%2520Pro%2527s%2520steps%2520taken%2520in%2520SWE%2520agent%2520tasks%2520by%252024.6%2525%252C%2520and%2520outperform%2520static%2520prompt%2520optimizers%2520in%2520personalizing%2520GPT-5%2520to%2520user%2520preferences%2520%252885-100%2525%2520vs.%252040-60%2525%2529.%2520We%2520also%2520find%2520that%2520advisors%2520are%2520transferable%253A%2520an%2520advisor%2520trained%2520with%2520a%2520low-cost%2520student%2520model%2520still%2520transfers%2520improvements%2520to%2520a%2520frontier%2520model.%2520Moreover%252C%2520Advisor%2520Models%2520are%2520robust%253A%2520we%2520observe%2520no%2520degradation%2520on%2520other%2520benchmarks%2520than%2520the%2520pipeline%2520is%2520trained%2520on.%2520Our%2520method%2520shows%2520how%2520to%2520perform%2520parametric%2520optimization%2520for%2520black-box%2520frontier%2520models%2520in%2520a%2520practical%2520and%2520cost-effective%2520way.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02453v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Train%20Your%20Advisor%3A%20Steering%20Black-Box%20LLMs%20with%20Advisor%20Models&entry.906535625=Parth%20Asawa%20and%20Alan%20Zhu%20and%20Abby%20O%27Neill%20and%20Matei%20Zaharia%20and%20Alexandros%20G.%20Dimakis%20and%20Joseph%20E.%20Gonzalez&entry.1292438233=Frontier%20language%20models%20are%20deployed%20as%20black-box%20services%2C%20where%20model%20weights%20cannot%20be%20modified%20and%20customization%20is%20limited%20to%20prompting.%20We%20introduce%20Advisor%20Models%2C%20a%20method%20to%20train%20small%20open-weight%20models%20to%20generate%20dynamic%2C%20per-instance%20natural%20language%20advice%20that%20improves%20the%20capabilities%20of%20black-box%20frontier%20models.%20Advisor%20Models%20improve%20GPT-5%27s%20performance%20on%20RuleArena%20%28Taxes%29%20by%2071%25%2C%20reduce%20Gemini%203%20Pro%27s%20steps%20taken%20in%20SWE%20agent%20tasks%20by%2024.6%25%2C%20and%20outperform%20static%20prompt%20optimizers%20in%20personalizing%20GPT-5%20to%20user%20preferences%20%2885-100%25%20vs.%2040-60%25%29.%20We%20also%20find%20that%20advisors%20are%20transferable%3A%20an%20advisor%20trained%20with%20a%20low-cost%20student%20model%20still%20transfers%20improvements%20to%20a%20frontier%20model.%20Moreover%2C%20Advisor%20Models%20are%20robust%3A%20we%20observe%20no%20degradation%20on%20other%20benchmarks%20than%20the%20pipeline%20is%20trained%20on.%20Our%20method%20shows%20how%20to%20perform%20parametric%20optimization%20for%20black-box%20frontier%20models%20in%20a%20practical%20and%20cost-effective%20way.&entry.1838667208=http%3A//arxiv.org/abs/2510.02453v2&entry.124074799=Read"},
{"title": "U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding", "author": "Anjie Le and Henan Liu and Yue Wang and Zhenyu Liu and Rongkun Zhu and Taohan Weng and Jinze Yu and Boyang Wang and Yalun Wu and Kaiwen Yan and Quanlin Sun and Meirui Jiang and Jialun Pei and Siya Liu and Haoyun Zheng and Zhoujun Li and Alison Noble and Jacques Souquet and Xiaoqing Guo and Manxi Lin and Hongcheng Guo", "abstract": "Ultrasound is a widely-used imaging modality critical to global healthcare, yet its interpretation remains challenging due to its varying image quality on operators, noises, and anatomical structures. Although large vision-language models (LVLMs) have demonstrated impressive multimodal capabilities across natural and medical domains, their performance on ultrasound remains largely unexplored. We introduce U2-BENCH, the first comprehensive benchmark to evaluate LVLMs on ultrasound understanding across classification, detection, regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning 15 anatomical regions and defines 8 clinically inspired tasks, such as diagnosis, view recognition, lesion localization, clinical value estimation, and report generation, across 50 ultrasound application scenarios. We evaluate 23 state-of-the-art LVLMs, both open- and closed-source, general-purpose and medical-specific. Our results reveal strong performance on image-level classification, but persistent challenges in spatial reasoning and clinical language generation. U2-BENCH establishes a rigorous and unified testbed to assess and accelerate LVLM research in the uniquely multimodal domain of medical ultrasound imaging.", "link": "http://arxiv.org/abs/2505.17779v3", "date": "2026-02-02", "relevancy": 2.3111, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20U2-BENCH%3A%20Benchmarking%20Large%20Vision-Language%20Models%20on%20Ultrasound%20Understanding&body=Title%3A%20U2-BENCH%3A%20Benchmarking%20Large%20Vision-Language%20Models%20on%20Ultrasound%20Understanding%0AAuthor%3A%20Anjie%20Le%20and%20Henan%20Liu%20and%20Yue%20Wang%20and%20Zhenyu%20Liu%20and%20Rongkun%20Zhu%20and%20Taohan%20Weng%20and%20Jinze%20Yu%20and%20Boyang%20Wang%20and%20Yalun%20Wu%20and%20Kaiwen%20Yan%20and%20Quanlin%20Sun%20and%20Meirui%20Jiang%20and%20Jialun%20Pei%20and%20Siya%20Liu%20and%20Haoyun%20Zheng%20and%20Zhoujun%20Li%20and%20Alison%20Noble%20and%20Jacques%20Souquet%20and%20Xiaoqing%20Guo%20and%20Manxi%20Lin%20and%20Hongcheng%20Guo%0AAbstract%3A%20Ultrasound%20is%20a%20widely-used%20imaging%20modality%20critical%20to%20global%20healthcare%2C%20yet%20its%20interpretation%20remains%20challenging%20due%20to%20its%20varying%20image%20quality%20on%20operators%2C%20noises%2C%20and%20anatomical%20structures.%20Although%20large%20vision-language%20models%20%28LVLMs%29%20have%20demonstrated%20impressive%20multimodal%20capabilities%20across%20natural%20and%20medical%20domains%2C%20their%20performance%20on%20ultrasound%20remains%20largely%20unexplored.%20We%20introduce%20U2-BENCH%2C%20the%20first%20comprehensive%20benchmark%20to%20evaluate%20LVLMs%20on%20ultrasound%20understanding%20across%20classification%2C%20detection%2C%20regression%2C%20and%20text%20generation%20tasks.%20U2-BENCH%20aggregates%207%2C241%20cases%20spanning%2015%20anatomical%20regions%20and%20defines%208%20clinically%20inspired%20tasks%2C%20such%20as%20diagnosis%2C%20view%20recognition%2C%20lesion%20localization%2C%20clinical%20value%20estimation%2C%20and%20report%20generation%2C%20across%2050%20ultrasound%20application%20scenarios.%20We%20evaluate%2023%20state-of-the-art%20LVLMs%2C%20both%20open-%20and%20closed-source%2C%20general-purpose%20and%20medical-specific.%20Our%20results%20reveal%20strong%20performance%20on%20image-level%20classification%2C%20but%20persistent%20challenges%20in%20spatial%20reasoning%20and%20clinical%20language%20generation.%20U2-BENCH%20establishes%20a%20rigorous%20and%20unified%20testbed%20to%20assess%20and%20accelerate%20LVLM%20research%20in%20the%20uniquely%20multimodal%20domain%20of%20medical%20ultrasound%20imaging.%0ALink%3A%20http%3A//arxiv.org/abs/2505.17779v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DU2-BENCH%253A%2520Benchmarking%2520Large%2520Vision-Language%2520Models%2520on%2520Ultrasound%2520Understanding%26entry.906535625%3DAnjie%2520Le%2520and%2520Henan%2520Liu%2520and%2520Yue%2520Wang%2520and%2520Zhenyu%2520Liu%2520and%2520Rongkun%2520Zhu%2520and%2520Taohan%2520Weng%2520and%2520Jinze%2520Yu%2520and%2520Boyang%2520Wang%2520and%2520Yalun%2520Wu%2520and%2520Kaiwen%2520Yan%2520and%2520Quanlin%2520Sun%2520and%2520Meirui%2520Jiang%2520and%2520Jialun%2520Pei%2520and%2520Siya%2520Liu%2520and%2520Haoyun%2520Zheng%2520and%2520Zhoujun%2520Li%2520and%2520Alison%2520Noble%2520and%2520Jacques%2520Souquet%2520and%2520Xiaoqing%2520Guo%2520and%2520Manxi%2520Lin%2520and%2520Hongcheng%2520Guo%26entry.1292438233%3DUltrasound%2520is%2520a%2520widely-used%2520imaging%2520modality%2520critical%2520to%2520global%2520healthcare%252C%2520yet%2520its%2520interpretation%2520remains%2520challenging%2520due%2520to%2520its%2520varying%2520image%2520quality%2520on%2520operators%252C%2520noises%252C%2520and%2520anatomical%2520structures.%2520Although%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520impressive%2520multimodal%2520capabilities%2520across%2520natural%2520and%2520medical%2520domains%252C%2520their%2520performance%2520on%2520ultrasound%2520remains%2520largely%2520unexplored.%2520We%2520introduce%2520U2-BENCH%252C%2520the%2520first%2520comprehensive%2520benchmark%2520to%2520evaluate%2520LVLMs%2520on%2520ultrasound%2520understanding%2520across%2520classification%252C%2520detection%252C%2520regression%252C%2520and%2520text%2520generation%2520tasks.%2520U2-BENCH%2520aggregates%25207%252C241%2520cases%2520spanning%252015%2520anatomical%2520regions%2520and%2520defines%25208%2520clinically%2520inspired%2520tasks%252C%2520such%2520as%2520diagnosis%252C%2520view%2520recognition%252C%2520lesion%2520localization%252C%2520clinical%2520value%2520estimation%252C%2520and%2520report%2520generation%252C%2520across%252050%2520ultrasound%2520application%2520scenarios.%2520We%2520evaluate%252023%2520state-of-the-art%2520LVLMs%252C%2520both%2520open-%2520and%2520closed-source%252C%2520general-purpose%2520and%2520medical-specific.%2520Our%2520results%2520reveal%2520strong%2520performance%2520on%2520image-level%2520classification%252C%2520but%2520persistent%2520challenges%2520in%2520spatial%2520reasoning%2520and%2520clinical%2520language%2520generation.%2520U2-BENCH%2520establishes%2520a%2520rigorous%2520and%2520unified%2520testbed%2520to%2520assess%2520and%2520accelerate%2520LVLM%2520research%2520in%2520the%2520uniquely%2520multimodal%2520domain%2520of%2520medical%2520ultrasound%2520imaging.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17779v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=U2-BENCH%3A%20Benchmarking%20Large%20Vision-Language%20Models%20on%20Ultrasound%20Understanding&entry.906535625=Anjie%20Le%20and%20Henan%20Liu%20and%20Yue%20Wang%20and%20Zhenyu%20Liu%20and%20Rongkun%20Zhu%20and%20Taohan%20Weng%20and%20Jinze%20Yu%20and%20Boyang%20Wang%20and%20Yalun%20Wu%20and%20Kaiwen%20Yan%20and%20Quanlin%20Sun%20and%20Meirui%20Jiang%20and%20Jialun%20Pei%20and%20Siya%20Liu%20and%20Haoyun%20Zheng%20and%20Zhoujun%20Li%20and%20Alison%20Noble%20and%20Jacques%20Souquet%20and%20Xiaoqing%20Guo%20and%20Manxi%20Lin%20and%20Hongcheng%20Guo&entry.1292438233=Ultrasound%20is%20a%20widely-used%20imaging%20modality%20critical%20to%20global%20healthcare%2C%20yet%20its%20interpretation%20remains%20challenging%20due%20to%20its%20varying%20image%20quality%20on%20operators%2C%20noises%2C%20and%20anatomical%20structures.%20Although%20large%20vision-language%20models%20%28LVLMs%29%20have%20demonstrated%20impressive%20multimodal%20capabilities%20across%20natural%20and%20medical%20domains%2C%20their%20performance%20on%20ultrasound%20remains%20largely%20unexplored.%20We%20introduce%20U2-BENCH%2C%20the%20first%20comprehensive%20benchmark%20to%20evaluate%20LVLMs%20on%20ultrasound%20understanding%20across%20classification%2C%20detection%2C%20regression%2C%20and%20text%20generation%20tasks.%20U2-BENCH%20aggregates%207%2C241%20cases%20spanning%2015%20anatomical%20regions%20and%20defines%208%20clinically%20inspired%20tasks%2C%20such%20as%20diagnosis%2C%20view%20recognition%2C%20lesion%20localization%2C%20clinical%20value%20estimation%2C%20and%20report%20generation%2C%20across%2050%20ultrasound%20application%20scenarios.%20We%20evaluate%2023%20state-of-the-art%20LVLMs%2C%20both%20open-%20and%20closed-source%2C%20general-purpose%20and%20medical-specific.%20Our%20results%20reveal%20strong%20performance%20on%20image-level%20classification%2C%20but%20persistent%20challenges%20in%20spatial%20reasoning%20and%20clinical%20language%20generation.%20U2-BENCH%20establishes%20a%20rigorous%20and%20unified%20testbed%20to%20assess%20and%20accelerate%20LVLM%20research%20in%20the%20uniquely%20multimodal%20domain%20of%20medical%20ultrasound%20imaging.&entry.1838667208=http%3A//arxiv.org/abs/2505.17779v3&entry.124074799=Read"},
{"title": "Towards Faithful Reasoning in Remote Sensing: A Perceptually-Grounded GeoSpatial Chain-of-Thought for Vision-Language Models", "author": "Jiaqi Liu and Lang Sun and Ronghao Fu and Bo Yang", "abstract": "Vision-Language Models (VLMs) in remote sensing often fail at complex analytical tasks, a limitation stemming from their end-to-end training paradigm that bypasses crucial reasoning steps and leads to unverifiable outputs. To address this limitation, we introduce the Perceptually-Grounded Geospatial Chain-of-Thought (Geo-CoT), a framework that models remote sensing analysis as a verifiable, multi-step process. We instill this analytical process through a two-stage alignment strategy, leveraging Geo-CoT380k, the first large-scale dataset of structured Geo-CoT rationales. This strategy first employs supervised fine-tuning (SFT) to instill the foundational cognitive architecture, then leverages Group Reward Policy Optimization (GRPO) to refine the model's reasoning policy towards factual correctness. The resulting model, RSThinker, outputs both a final answer and its justifying, verifiable analytical trace. This capability yields dominant performance, significantly outperforming state-of-the-art models across a comprehensive range of tasks. The public release of our Geo-CoT380k dataset and RSThinker model upon publication serves as a concrete pathway from opaque perception towards structured, verifiable reasoning for Earth Observation.", "link": "http://arxiv.org/abs/2509.22221v2", "date": "2026-02-02", "relevancy": 2.3091, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5853}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5853}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Faithful%20Reasoning%20in%20Remote%20Sensing%3A%20A%20Perceptually-Grounded%20GeoSpatial%20Chain-of-Thought%20for%20Vision-Language%20Models&body=Title%3A%20Towards%20Faithful%20Reasoning%20in%20Remote%20Sensing%3A%20A%20Perceptually-Grounded%20GeoSpatial%20Chain-of-Thought%20for%20Vision-Language%20Models%0AAuthor%3A%20Jiaqi%20Liu%20and%20Lang%20Sun%20and%20Ronghao%20Fu%20and%20Bo%20Yang%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20in%20remote%20sensing%20often%20fail%20at%20complex%20analytical%20tasks%2C%20a%20limitation%20stemming%20from%20their%20end-to-end%20training%20paradigm%20that%20bypasses%20crucial%20reasoning%20steps%20and%20leads%20to%20unverifiable%20outputs.%20To%20address%20this%20limitation%2C%20we%20introduce%20the%20Perceptually-Grounded%20Geospatial%20Chain-of-Thought%20%28Geo-CoT%29%2C%20a%20framework%20that%20models%20remote%20sensing%20analysis%20as%20a%20verifiable%2C%20multi-step%20process.%20We%20instill%20this%20analytical%20process%20through%20a%20two-stage%20alignment%20strategy%2C%20leveraging%20Geo-CoT380k%2C%20the%20first%20large-scale%20dataset%20of%20structured%20Geo-CoT%20rationales.%20This%20strategy%20first%20employs%20supervised%20fine-tuning%20%28SFT%29%20to%20instill%20the%20foundational%20cognitive%20architecture%2C%20then%20leverages%20Group%20Reward%20Policy%20Optimization%20%28GRPO%29%20to%20refine%20the%20model%27s%20reasoning%20policy%20towards%20factual%20correctness.%20The%20resulting%20model%2C%20RSThinker%2C%20outputs%20both%20a%20final%20answer%20and%20its%20justifying%2C%20verifiable%20analytical%20trace.%20This%20capability%20yields%20dominant%20performance%2C%20significantly%20outperforming%20state-of-the-art%20models%20across%20a%20comprehensive%20range%20of%20tasks.%20The%20public%20release%20of%20our%20Geo-CoT380k%20dataset%20and%20RSThinker%20model%20upon%20publication%20serves%20as%20a%20concrete%20pathway%20from%20opaque%20perception%20towards%20structured%2C%20verifiable%20reasoning%20for%20Earth%20Observation.%0ALink%3A%20http%3A//arxiv.org/abs/2509.22221v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Faithful%2520Reasoning%2520in%2520Remote%2520Sensing%253A%2520A%2520Perceptually-Grounded%2520GeoSpatial%2520Chain-of-Thought%2520for%2520Vision-Language%2520Models%26entry.906535625%3DJiaqi%2520Liu%2520and%2520Lang%2520Sun%2520and%2520Ronghao%2520Fu%2520and%2520Bo%2520Yang%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520in%2520remote%2520sensing%2520often%2520fail%2520at%2520complex%2520analytical%2520tasks%252C%2520a%2520limitation%2520stemming%2520from%2520their%2520end-to-end%2520training%2520paradigm%2520that%2520bypasses%2520crucial%2520reasoning%2520steps%2520and%2520leads%2520to%2520unverifiable%2520outputs.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520the%2520Perceptually-Grounded%2520Geospatial%2520Chain-of-Thought%2520%2528Geo-CoT%2529%252C%2520a%2520framework%2520that%2520models%2520remote%2520sensing%2520analysis%2520as%2520a%2520verifiable%252C%2520multi-step%2520process.%2520We%2520instill%2520this%2520analytical%2520process%2520through%2520a%2520two-stage%2520alignment%2520strategy%252C%2520leveraging%2520Geo-CoT380k%252C%2520the%2520first%2520large-scale%2520dataset%2520of%2520structured%2520Geo-CoT%2520rationales.%2520This%2520strategy%2520first%2520employs%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520to%2520instill%2520the%2520foundational%2520cognitive%2520architecture%252C%2520then%2520leverages%2520Group%2520Reward%2520Policy%2520Optimization%2520%2528GRPO%2529%2520to%2520refine%2520the%2520model%2527s%2520reasoning%2520policy%2520towards%2520factual%2520correctness.%2520The%2520resulting%2520model%252C%2520RSThinker%252C%2520outputs%2520both%2520a%2520final%2520answer%2520and%2520its%2520justifying%252C%2520verifiable%2520analytical%2520trace.%2520This%2520capability%2520yields%2520dominant%2520performance%252C%2520significantly%2520outperforming%2520state-of-the-art%2520models%2520across%2520a%2520comprehensive%2520range%2520of%2520tasks.%2520The%2520public%2520release%2520of%2520our%2520Geo-CoT380k%2520dataset%2520and%2520RSThinker%2520model%2520upon%2520publication%2520serves%2520as%2520a%2520concrete%2520pathway%2520from%2520opaque%2520perception%2520towards%2520structured%252C%2520verifiable%2520reasoning%2520for%2520Earth%2520Observation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22221v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Faithful%20Reasoning%20in%20Remote%20Sensing%3A%20A%20Perceptually-Grounded%20GeoSpatial%20Chain-of-Thought%20for%20Vision-Language%20Models&entry.906535625=Jiaqi%20Liu%20and%20Lang%20Sun%20and%20Ronghao%20Fu%20and%20Bo%20Yang&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20in%20remote%20sensing%20often%20fail%20at%20complex%20analytical%20tasks%2C%20a%20limitation%20stemming%20from%20their%20end-to-end%20training%20paradigm%20that%20bypasses%20crucial%20reasoning%20steps%20and%20leads%20to%20unverifiable%20outputs.%20To%20address%20this%20limitation%2C%20we%20introduce%20the%20Perceptually-Grounded%20Geospatial%20Chain-of-Thought%20%28Geo-CoT%29%2C%20a%20framework%20that%20models%20remote%20sensing%20analysis%20as%20a%20verifiable%2C%20multi-step%20process.%20We%20instill%20this%20analytical%20process%20through%20a%20two-stage%20alignment%20strategy%2C%20leveraging%20Geo-CoT380k%2C%20the%20first%20large-scale%20dataset%20of%20structured%20Geo-CoT%20rationales.%20This%20strategy%20first%20employs%20supervised%20fine-tuning%20%28SFT%29%20to%20instill%20the%20foundational%20cognitive%20architecture%2C%20then%20leverages%20Group%20Reward%20Policy%20Optimization%20%28GRPO%29%20to%20refine%20the%20model%27s%20reasoning%20policy%20towards%20factual%20correctness.%20The%20resulting%20model%2C%20RSThinker%2C%20outputs%20both%20a%20final%20answer%20and%20its%20justifying%2C%20verifiable%20analytical%20trace.%20This%20capability%20yields%20dominant%20performance%2C%20significantly%20outperforming%20state-of-the-art%20models%20across%20a%20comprehensive%20range%20of%20tasks.%20The%20public%20release%20of%20our%20Geo-CoT380k%20dataset%20and%20RSThinker%20model%20upon%20publication%20serves%20as%20a%20concrete%20pathway%20from%20opaque%20perception%20towards%20structured%2C%20verifiable%20reasoning%20for%20Earth%20Observation.&entry.1838667208=http%3A//arxiv.org/abs/2509.22221v2&entry.124074799=Read"},
{"title": "Language Family Matters: Evaluating LLM-Based ASR Across Linguistic Boundaries", "author": "Yuchen Zhang and Ravi Shekhar and Haralambos Mouratidis", "abstract": "Large Language Model (LLM)-powered Automatic Speech Recognition (ASR) systems achieve strong performance with limited resources by linking a frozen speech encoder to a pretrained LLM via a lightweight connector. Prior work trains a separate connector per language, overlooking linguistic relatedness. We propose an efficient and novel connector-sharing strategy based on linguistic family membership, enabling one connector per family, and empirically validate its effectiveness across two multilingual LLMs and two real-world corpora spanning curated and crowd-sourced speech. Our results show that family-based connectors reduce parameter count while improving generalization across domains, offering a practical and scalable strategy for multilingual ASR deployment.", "link": "http://arxiv.org/abs/2601.18899v2", "date": "2026-02-02", "relevancy": 2.3029, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Family%20Matters%3A%20Evaluating%20LLM-Based%20ASR%20Across%20Linguistic%20Boundaries&body=Title%3A%20Language%20Family%20Matters%3A%20Evaluating%20LLM-Based%20ASR%20Across%20Linguistic%20Boundaries%0AAuthor%3A%20Yuchen%20Zhang%20and%20Ravi%20Shekhar%20and%20Haralambos%20Mouratidis%0AAbstract%3A%20Large%20Language%20Model%20%28LLM%29-powered%20Automatic%20Speech%20Recognition%20%28ASR%29%20systems%20achieve%20strong%20performance%20with%20limited%20resources%20by%20linking%20a%20frozen%20speech%20encoder%20to%20a%20pretrained%20LLM%20via%20a%20lightweight%20connector.%20Prior%20work%20trains%20a%20separate%20connector%20per%20language%2C%20overlooking%20linguistic%20relatedness.%20We%20propose%20an%20efficient%20and%20novel%20connector-sharing%20strategy%20based%20on%20linguistic%20family%20membership%2C%20enabling%20one%20connector%20per%20family%2C%20and%20empirically%20validate%20its%20effectiveness%20across%20two%20multilingual%20LLMs%20and%20two%20real-world%20corpora%20spanning%20curated%20and%20crowd-sourced%20speech.%20Our%20results%20show%20that%20family-based%20connectors%20reduce%20parameter%20count%20while%20improving%20generalization%20across%20domains%2C%20offering%20a%20practical%20and%20scalable%20strategy%20for%20multilingual%20ASR%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18899v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Family%2520Matters%253A%2520Evaluating%2520LLM-Based%2520ASR%2520Across%2520Linguistic%2520Boundaries%26entry.906535625%3DYuchen%2520Zhang%2520and%2520Ravi%2520Shekhar%2520and%2520Haralambos%2520Mouratidis%26entry.1292438233%3DLarge%2520Language%2520Model%2520%2528LLM%2529-powered%2520Automatic%2520Speech%2520Recognition%2520%2528ASR%2529%2520systems%2520achieve%2520strong%2520performance%2520with%2520limited%2520resources%2520by%2520linking%2520a%2520frozen%2520speech%2520encoder%2520to%2520a%2520pretrained%2520LLM%2520via%2520a%2520lightweight%2520connector.%2520Prior%2520work%2520trains%2520a%2520separate%2520connector%2520per%2520language%252C%2520overlooking%2520linguistic%2520relatedness.%2520We%2520propose%2520an%2520efficient%2520and%2520novel%2520connector-sharing%2520strategy%2520based%2520on%2520linguistic%2520family%2520membership%252C%2520enabling%2520one%2520connector%2520per%2520family%252C%2520and%2520empirically%2520validate%2520its%2520effectiveness%2520across%2520two%2520multilingual%2520LLMs%2520and%2520two%2520real-world%2520corpora%2520spanning%2520curated%2520and%2520crowd-sourced%2520speech.%2520Our%2520results%2520show%2520that%2520family-based%2520connectors%2520reduce%2520parameter%2520count%2520while%2520improving%2520generalization%2520across%2520domains%252C%2520offering%2520a%2520practical%2520and%2520scalable%2520strategy%2520for%2520multilingual%2520ASR%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18899v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Family%20Matters%3A%20Evaluating%20LLM-Based%20ASR%20Across%20Linguistic%20Boundaries&entry.906535625=Yuchen%20Zhang%20and%20Ravi%20Shekhar%20and%20Haralambos%20Mouratidis&entry.1292438233=Large%20Language%20Model%20%28LLM%29-powered%20Automatic%20Speech%20Recognition%20%28ASR%29%20systems%20achieve%20strong%20performance%20with%20limited%20resources%20by%20linking%20a%20frozen%20speech%20encoder%20to%20a%20pretrained%20LLM%20via%20a%20lightweight%20connector.%20Prior%20work%20trains%20a%20separate%20connector%20per%20language%2C%20overlooking%20linguistic%20relatedness.%20We%20propose%20an%20efficient%20and%20novel%20connector-sharing%20strategy%20based%20on%20linguistic%20family%20membership%2C%20enabling%20one%20connector%20per%20family%2C%20and%20empirically%20validate%20its%20effectiveness%20across%20two%20multilingual%20LLMs%20and%20two%20real-world%20corpora%20spanning%20curated%20and%20crowd-sourced%20speech.%20Our%20results%20show%20that%20family-based%20connectors%20reduce%20parameter%20count%20while%20improving%20generalization%20across%20domains%2C%20offering%20a%20practical%20and%20scalable%20strategy%20for%20multilingual%20ASR%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2601.18899v2&entry.124074799=Read"},
{"title": "Mixture-of-Experts with Intermediate CTC Supervision for Accented Speech Recognition", "author": "Wonjun Lee and Hyounghun Kim and Gary Geunbae Lee", "abstract": "Accented speech remains a persistent challenge for automatic speech recognition (ASR), as most models are trained on data dominated by a few high-resource English varieties, leading to substantial performance degradation for other accents. Accent-agnostic approaches improve robustness yet struggle with heavily accented or unseen varieties, while accent-specific methods rely on limited and often noisy labels. We introduce Moe-Ctc, a Mixture-of-Experts architecture with intermediate CTC supervision that jointly promotes expert specialization and generalization. During training, accent-aware routing encourages experts to capture accent-specific patterns, which gradually transitions to label-free routing for inference. Each expert is equipped with its own CTC head to align routing with transcription quality, and a routing-augmented loss further stabilizes optimization. Experiments on the Mcv-Accent benchmark demonstrate consistent gains across both seen and unseen accents in low- and high-resource conditions, achieving up to 29.3% relative WER reduction over strong FastConformer baselines.", "link": "http://arxiv.org/abs/2602.01967v1", "date": "2026-02-02", "relevancy": 2.3009, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4788}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.467}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixture-of-Experts%20with%20Intermediate%20CTC%20Supervision%20for%20Accented%20Speech%20Recognition&body=Title%3A%20Mixture-of-Experts%20with%20Intermediate%20CTC%20Supervision%20for%20Accented%20Speech%20Recognition%0AAuthor%3A%20Wonjun%20Lee%20and%20Hyounghun%20Kim%20and%20Gary%20Geunbae%20Lee%0AAbstract%3A%20Accented%20speech%20remains%20a%20persistent%20challenge%20for%20automatic%20speech%20recognition%20%28ASR%29%2C%20as%20most%20models%20are%20trained%20on%20data%20dominated%20by%20a%20few%20high-resource%20English%20varieties%2C%20leading%20to%20substantial%20performance%20degradation%20for%20other%20accents.%20Accent-agnostic%20approaches%20improve%20robustness%20yet%20struggle%20with%20heavily%20accented%20or%20unseen%20varieties%2C%20while%20accent-specific%20methods%20rely%20on%20limited%20and%20often%20noisy%20labels.%20We%20introduce%20Moe-Ctc%2C%20a%20Mixture-of-Experts%20architecture%20with%20intermediate%20CTC%20supervision%20that%20jointly%20promotes%20expert%20specialization%20and%20generalization.%20During%20training%2C%20accent-aware%20routing%20encourages%20experts%20to%20capture%20accent-specific%20patterns%2C%20which%20gradually%20transitions%20to%20label-free%20routing%20for%20inference.%20Each%20expert%20is%20equipped%20with%20its%20own%20CTC%20head%20to%20align%20routing%20with%20transcription%20quality%2C%20and%20a%20routing-augmented%20loss%20further%20stabilizes%20optimization.%20Experiments%20on%20the%20Mcv-Accent%20benchmark%20demonstrate%20consistent%20gains%20across%20both%20seen%20and%20unseen%20accents%20in%20low-%20and%20high-resource%20conditions%2C%20achieving%20up%20to%2029.3%25%20relative%20WER%20reduction%20over%20strong%20FastConformer%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2602.01967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixture-of-Experts%2520with%2520Intermediate%2520CTC%2520Supervision%2520for%2520Accented%2520Speech%2520Recognition%26entry.906535625%3DWonjun%2520Lee%2520and%2520Hyounghun%2520Kim%2520and%2520Gary%2520Geunbae%2520Lee%26entry.1292438233%3DAccented%2520speech%2520remains%2520a%2520persistent%2520challenge%2520for%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%252C%2520as%2520most%2520models%2520are%2520trained%2520on%2520data%2520dominated%2520by%2520a%2520few%2520high-resource%2520English%2520varieties%252C%2520leading%2520to%2520substantial%2520performance%2520degradation%2520for%2520other%2520accents.%2520Accent-agnostic%2520approaches%2520improve%2520robustness%2520yet%2520struggle%2520with%2520heavily%2520accented%2520or%2520unseen%2520varieties%252C%2520while%2520accent-specific%2520methods%2520rely%2520on%2520limited%2520and%2520often%2520noisy%2520labels.%2520We%2520introduce%2520Moe-Ctc%252C%2520a%2520Mixture-of-Experts%2520architecture%2520with%2520intermediate%2520CTC%2520supervision%2520that%2520jointly%2520promotes%2520expert%2520specialization%2520and%2520generalization.%2520During%2520training%252C%2520accent-aware%2520routing%2520encourages%2520experts%2520to%2520capture%2520accent-specific%2520patterns%252C%2520which%2520gradually%2520transitions%2520to%2520label-free%2520routing%2520for%2520inference.%2520Each%2520expert%2520is%2520equipped%2520with%2520its%2520own%2520CTC%2520head%2520to%2520align%2520routing%2520with%2520transcription%2520quality%252C%2520and%2520a%2520routing-augmented%2520loss%2520further%2520stabilizes%2520optimization.%2520Experiments%2520on%2520the%2520Mcv-Accent%2520benchmark%2520demonstrate%2520consistent%2520gains%2520across%2520both%2520seen%2520and%2520unseen%2520accents%2520in%2520low-%2520and%2520high-resource%2520conditions%252C%2520achieving%2520up%2520to%252029.3%2525%2520relative%2520WER%2520reduction%2520over%2520strong%2520FastConformer%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.01967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixture-of-Experts%20with%20Intermediate%20CTC%20Supervision%20for%20Accented%20Speech%20Recognition&entry.906535625=Wonjun%20Lee%20and%20Hyounghun%20Kim%20and%20Gary%20Geunbae%20Lee&entry.1292438233=Accented%20speech%20remains%20a%20persistent%20challenge%20for%20automatic%20speech%20recognition%20%28ASR%29%2C%20as%20most%20models%20are%20trained%20on%20data%20dominated%20by%20a%20few%20high-resource%20English%20varieties%2C%20leading%20to%20substantial%20performance%20degradation%20for%20other%20accents.%20Accent-agnostic%20approaches%20improve%20robustness%20yet%20struggle%20with%20heavily%20accented%20or%20unseen%20varieties%2C%20while%20accent-specific%20methods%20rely%20on%20limited%20and%20often%20noisy%20labels.%20We%20introduce%20Moe-Ctc%2C%20a%20Mixture-of-Experts%20architecture%20with%20intermediate%20CTC%20supervision%20that%20jointly%20promotes%20expert%20specialization%20and%20generalization.%20During%20training%2C%20accent-aware%20routing%20encourages%20experts%20to%20capture%20accent-specific%20patterns%2C%20which%20gradually%20transitions%20to%20label-free%20routing%20for%20inference.%20Each%20expert%20is%20equipped%20with%20its%20own%20CTC%20head%20to%20align%20routing%20with%20transcription%20quality%2C%20and%20a%20routing-augmented%20loss%20further%20stabilizes%20optimization.%20Experiments%20on%20the%20Mcv-Accent%20benchmark%20demonstrate%20consistent%20gains%20across%20both%20seen%20and%20unseen%20accents%20in%20low-%20and%20high-resource%20conditions%2C%20achieving%20up%20to%2029.3%25%20relative%20WER%20reduction%20over%20strong%20FastConformer%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2602.01967v1&entry.124074799=Read"},
{"title": "LiFlow: Flow Matching for 3D LiDAR Scene Completion", "author": "Andrea Matteazzi and Dietmar Tutsch", "abstract": "In autonomous driving scenarios, the collected LiDAR point clouds can be challenged by occlusion and long-range sparsity, limiting the perception of autonomous driving systems. Scene completion methods can infer the missing parts of incomplete 3D LiDAR scenes. Recent methods adopt local point-level denoising diffusion probabilistic models, which require predicting Gaussian noise, leading to a mismatch between training and inference initial distributions. This paper introduces the first flow matching framework for 3D LiDAR scene completion, improving upon diffusion-based methods by ensuring consistent initial distributions between training and inference. The model employs a nearest neighbor flow matching loss and a Chamfer distance loss to enhance both local structure and global coverage in the alignment of point clouds. LiFlow achieves state-of-the-art performance across multiple metrics. Code: https://github.com/matteandre/LiFlow.", "link": "http://arxiv.org/abs/2602.02232v1", "date": "2026-02-02", "relevancy": 2.294, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6038}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5533}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiFlow%3A%20Flow%20Matching%20for%203D%20LiDAR%20Scene%20Completion&body=Title%3A%20LiFlow%3A%20Flow%20Matching%20for%203D%20LiDAR%20Scene%20Completion%0AAuthor%3A%20Andrea%20Matteazzi%20and%20Dietmar%20Tutsch%0AAbstract%3A%20In%20autonomous%20driving%20scenarios%2C%20the%20collected%20LiDAR%20point%20clouds%20can%20be%20challenged%20by%20occlusion%20and%20long-range%20sparsity%2C%20limiting%20the%20perception%20of%20autonomous%20driving%20systems.%20Scene%20completion%20methods%20can%20infer%20the%20missing%20parts%20of%20incomplete%203D%20LiDAR%20scenes.%20Recent%20methods%20adopt%20local%20point-level%20denoising%20diffusion%20probabilistic%20models%2C%20which%20require%20predicting%20Gaussian%20noise%2C%20leading%20to%20a%20mismatch%20between%20training%20and%20inference%20initial%20distributions.%20This%20paper%20introduces%20the%20first%20flow%20matching%20framework%20for%203D%20LiDAR%20scene%20completion%2C%20improving%20upon%20diffusion-based%20methods%20by%20ensuring%20consistent%20initial%20distributions%20between%20training%20and%20inference.%20The%20model%20employs%20a%20nearest%20neighbor%20flow%20matching%20loss%20and%20a%20Chamfer%20distance%20loss%20to%20enhance%20both%20local%20structure%20and%20global%20coverage%20in%20the%20alignment%20of%20point%20clouds.%20LiFlow%20achieves%20state-of-the-art%20performance%20across%20multiple%20metrics.%20Code%3A%20https%3A//github.com/matteandre/LiFlow.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiFlow%253A%2520Flow%2520Matching%2520for%25203D%2520LiDAR%2520Scene%2520Completion%26entry.906535625%3DAndrea%2520Matteazzi%2520and%2520Dietmar%2520Tutsch%26entry.1292438233%3DIn%2520autonomous%2520driving%2520scenarios%252C%2520the%2520collected%2520LiDAR%2520point%2520clouds%2520can%2520be%2520challenged%2520by%2520occlusion%2520and%2520long-range%2520sparsity%252C%2520limiting%2520the%2520perception%2520of%2520autonomous%2520driving%2520systems.%2520Scene%2520completion%2520methods%2520can%2520infer%2520the%2520missing%2520parts%2520of%2520incomplete%25203D%2520LiDAR%2520scenes.%2520Recent%2520methods%2520adopt%2520local%2520point-level%2520denoising%2520diffusion%2520probabilistic%2520models%252C%2520which%2520require%2520predicting%2520Gaussian%2520noise%252C%2520leading%2520to%2520a%2520mismatch%2520between%2520training%2520and%2520inference%2520initial%2520distributions.%2520This%2520paper%2520introduces%2520the%2520first%2520flow%2520matching%2520framework%2520for%25203D%2520LiDAR%2520scene%2520completion%252C%2520improving%2520upon%2520diffusion-based%2520methods%2520by%2520ensuring%2520consistent%2520initial%2520distributions%2520between%2520training%2520and%2520inference.%2520The%2520model%2520employs%2520a%2520nearest%2520neighbor%2520flow%2520matching%2520loss%2520and%2520a%2520Chamfer%2520distance%2520loss%2520to%2520enhance%2520both%2520local%2520structure%2520and%2520global%2520coverage%2520in%2520the%2520alignment%2520of%2520point%2520clouds.%2520LiFlow%2520achieves%2520state-of-the-art%2520performance%2520across%2520multiple%2520metrics.%2520Code%253A%2520https%253A//github.com/matteandre/LiFlow.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiFlow%3A%20Flow%20Matching%20for%203D%20LiDAR%20Scene%20Completion&entry.906535625=Andrea%20Matteazzi%20and%20Dietmar%20Tutsch&entry.1292438233=In%20autonomous%20driving%20scenarios%2C%20the%20collected%20LiDAR%20point%20clouds%20can%20be%20challenged%20by%20occlusion%20and%20long-range%20sparsity%2C%20limiting%20the%20perception%20of%20autonomous%20driving%20systems.%20Scene%20completion%20methods%20can%20infer%20the%20missing%20parts%20of%20incomplete%203D%20LiDAR%20scenes.%20Recent%20methods%20adopt%20local%20point-level%20denoising%20diffusion%20probabilistic%20models%2C%20which%20require%20predicting%20Gaussian%20noise%2C%20leading%20to%20a%20mismatch%20between%20training%20and%20inference%20initial%20distributions.%20This%20paper%20introduces%20the%20first%20flow%20matching%20framework%20for%203D%20LiDAR%20scene%20completion%2C%20improving%20upon%20diffusion-based%20methods%20by%20ensuring%20consistent%20initial%20distributions%20between%20training%20and%20inference.%20The%20model%20employs%20a%20nearest%20neighbor%20flow%20matching%20loss%20and%20a%20Chamfer%20distance%20loss%20to%20enhance%20both%20local%20structure%20and%20global%20coverage%20in%20the%20alignment%20of%20point%20clouds.%20LiFlow%20achieves%20state-of-the-art%20performance%20across%20multiple%20metrics.%20Code%3A%20https%3A//github.com/matteandre/LiFlow.&entry.1838667208=http%3A//arxiv.org/abs/2602.02232v1&entry.124074799=Read"},
{"title": "A Scalable Inter-edge Correlation Modeling in CopulaGNN for Link Sign Prediction", "author": "Jinkyu Sung and Myunggeum Jee and Joonseok Lee", "abstract": "Link sign prediction on a signed graph is a task to determine whether the relationship represented by an edge is positive or negative. Since the presence of negative edges violates the graph homophily assumption that adjacent nodes are similar, regular graph methods have not been applicable without auxiliary structures to handle them. We aim to directly model the latent statistical dependency among edges with the Gaussian copula and its corresponding correlation matrix, extending CopulaGNN (Ma et al., 2021). However, a naive modeling of edge-edge relations is computationally intractable even for a graph with moderate scale. To address this, we propose to 1) represent the correlation matrix as a Gramian of edge embeddings, significantly reducing the number of parameters, and 2) reformulate the conditional probability distribution to dramatically reduce the inference cost. We theoretically verify scalability of our method by proving its linear convergence. Also, our extensive experiments demonstrate that it achieves significantly faster convergence than baselines, maintaining competitive prediction performance to the state-of-the-art models.", "link": "http://arxiv.org/abs/2601.19175v3", "date": "2026-02-02", "relevancy": 2.2861, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4703}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4564}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Scalable%20Inter-edge%20Correlation%20Modeling%20in%20CopulaGNN%20for%20Link%20Sign%20Prediction&body=Title%3A%20A%20Scalable%20Inter-edge%20Correlation%20Modeling%20in%20CopulaGNN%20for%20Link%20Sign%20Prediction%0AAuthor%3A%20Jinkyu%20Sung%20and%20Myunggeum%20Jee%20and%20Joonseok%20Lee%0AAbstract%3A%20Link%20sign%20prediction%20on%20a%20signed%20graph%20is%20a%20task%20to%20determine%20whether%20the%20relationship%20represented%20by%20an%20edge%20is%20positive%20or%20negative.%20Since%20the%20presence%20of%20negative%20edges%20violates%20the%20graph%20homophily%20assumption%20that%20adjacent%20nodes%20are%20similar%2C%20regular%20graph%20methods%20have%20not%20been%20applicable%20without%20auxiliary%20structures%20to%20handle%20them.%20We%20aim%20to%20directly%20model%20the%20latent%20statistical%20dependency%20among%20edges%20with%20the%20Gaussian%20copula%20and%20its%20corresponding%20correlation%20matrix%2C%20extending%20CopulaGNN%20%28Ma%20et%20al.%2C%202021%29.%20However%2C%20a%20naive%20modeling%20of%20edge-edge%20relations%20is%20computationally%20intractable%20even%20for%20a%20graph%20with%20moderate%20scale.%20To%20address%20this%2C%20we%20propose%20to%201%29%20represent%20the%20correlation%20matrix%20as%20a%20Gramian%20of%20edge%20embeddings%2C%20significantly%20reducing%20the%20number%20of%20parameters%2C%20and%202%29%20reformulate%20the%20conditional%20probability%20distribution%20to%20dramatically%20reduce%20the%20inference%20cost.%20We%20theoretically%20verify%20scalability%20of%20our%20method%20by%20proving%20its%20linear%20convergence.%20Also%2C%20our%20extensive%20experiments%20demonstrate%20that%20it%20achieves%20significantly%20faster%20convergence%20than%20baselines%2C%20maintaining%20competitive%20prediction%20performance%20to%20the%20state-of-the-art%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19175v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Scalable%2520Inter-edge%2520Correlation%2520Modeling%2520in%2520CopulaGNN%2520for%2520Link%2520Sign%2520Prediction%26entry.906535625%3DJinkyu%2520Sung%2520and%2520Myunggeum%2520Jee%2520and%2520Joonseok%2520Lee%26entry.1292438233%3DLink%2520sign%2520prediction%2520on%2520a%2520signed%2520graph%2520is%2520a%2520task%2520to%2520determine%2520whether%2520the%2520relationship%2520represented%2520by%2520an%2520edge%2520is%2520positive%2520or%2520negative.%2520Since%2520the%2520presence%2520of%2520negative%2520edges%2520violates%2520the%2520graph%2520homophily%2520assumption%2520that%2520adjacent%2520nodes%2520are%2520similar%252C%2520regular%2520graph%2520methods%2520have%2520not%2520been%2520applicable%2520without%2520auxiliary%2520structures%2520to%2520handle%2520them.%2520We%2520aim%2520to%2520directly%2520model%2520the%2520latent%2520statistical%2520dependency%2520among%2520edges%2520with%2520the%2520Gaussian%2520copula%2520and%2520its%2520corresponding%2520correlation%2520matrix%252C%2520extending%2520CopulaGNN%2520%2528Ma%2520et%2520al.%252C%25202021%2529.%2520However%252C%2520a%2520naive%2520modeling%2520of%2520edge-edge%2520relations%2520is%2520computationally%2520intractable%2520even%2520for%2520a%2520graph%2520with%2520moderate%2520scale.%2520To%2520address%2520this%252C%2520we%2520propose%2520to%25201%2529%2520represent%2520the%2520correlation%2520matrix%2520as%2520a%2520Gramian%2520of%2520edge%2520embeddings%252C%2520significantly%2520reducing%2520the%2520number%2520of%2520parameters%252C%2520and%25202%2529%2520reformulate%2520the%2520conditional%2520probability%2520distribution%2520to%2520dramatically%2520reduce%2520the%2520inference%2520cost.%2520We%2520theoretically%2520verify%2520scalability%2520of%2520our%2520method%2520by%2520proving%2520its%2520linear%2520convergence.%2520Also%252C%2520our%2520extensive%2520experiments%2520demonstrate%2520that%2520it%2520achieves%2520significantly%2520faster%2520convergence%2520than%2520baselines%252C%2520maintaining%2520competitive%2520prediction%2520performance%2520to%2520the%2520state-of-the-art%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19175v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Scalable%20Inter-edge%20Correlation%20Modeling%20in%20CopulaGNN%20for%20Link%20Sign%20Prediction&entry.906535625=Jinkyu%20Sung%20and%20Myunggeum%20Jee%20and%20Joonseok%20Lee&entry.1292438233=Link%20sign%20prediction%20on%20a%20signed%20graph%20is%20a%20task%20to%20determine%20whether%20the%20relationship%20represented%20by%20an%20edge%20is%20positive%20or%20negative.%20Since%20the%20presence%20of%20negative%20edges%20violates%20the%20graph%20homophily%20assumption%20that%20adjacent%20nodes%20are%20similar%2C%20regular%20graph%20methods%20have%20not%20been%20applicable%20without%20auxiliary%20structures%20to%20handle%20them.%20We%20aim%20to%20directly%20model%20the%20latent%20statistical%20dependency%20among%20edges%20with%20the%20Gaussian%20copula%20and%20its%20corresponding%20correlation%20matrix%2C%20extending%20CopulaGNN%20%28Ma%20et%20al.%2C%202021%29.%20However%2C%20a%20naive%20modeling%20of%20edge-edge%20relations%20is%20computationally%20intractable%20even%20for%20a%20graph%20with%20moderate%20scale.%20To%20address%20this%2C%20we%20propose%20to%201%29%20represent%20the%20correlation%20matrix%20as%20a%20Gramian%20of%20edge%20embeddings%2C%20significantly%20reducing%20the%20number%20of%20parameters%2C%20and%202%29%20reformulate%20the%20conditional%20probability%20distribution%20to%20dramatically%20reduce%20the%20inference%20cost.%20We%20theoretically%20verify%20scalability%20of%20our%20method%20by%20proving%20its%20linear%20convergence.%20Also%2C%20our%20extensive%20experiments%20demonstrate%20that%20it%20achieves%20significantly%20faster%20convergence%20than%20baselines%2C%20maintaining%20competitive%20prediction%20performance%20to%20the%20state-of-the-art%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.19175v3&entry.124074799=Read"},
{"title": "Your AI-Generated Image Detector Can Secretly Achieve SOTA Accuracy, If Calibrated", "author": "Muli Yang and Gabriel James Goenawan and Henan Wang and Huaiyuan Qin and Chenghao Xu and Yanhua Yang and Fen Fang and Ying Sun and Joo-Hwee Lim and Hongyuan Zhu", "abstract": "Despite being trained on balanced datasets, existing AI-generated image detectors often exhibit systematic bias at test time, frequently misclassifying fake images as real. We hypothesize that this behavior stems from distributional shift in fake samples and implicit priors learned during training. Specifically, models tend to overfit to superficial artifacts that do not generalize well across different generation methods, leading to a misaligned decision threshold when faced with test-time distribution shift. To address this, we propose a theoretically grounded post-hoc calibration framework based on Bayesian decision theory. In particular, we introduce a learnable scalar correction to the model's logits, optimized on a small validation set from the target distribution while keeping the backbone frozen. This parametric adjustment compensates for distributional shift in model output, realigning the decision boundary even without requiring ground-truth labels. Experiments on challenging benchmarks show that our approach significantly improves robustness without retraining, offering a lightweight and principled solution for reliable and adaptive AI-generated image detection in the open world. Code is available at https://github.com/muliyangm/AIGI-Det-Calib.", "link": "http://arxiv.org/abs/2602.01973v1", "date": "2026-02-02", "relevancy": 2.0881, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5283}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5266}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5149}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Your%20AI-Generated%20Image%20Detector%20Can%20Secretly%20Achieve%20SOTA%20Accuracy%2C%20If%20Calibrated&body=Title%3A%20Your%20AI-Generated%20Image%20Detector%20Can%20Secretly%20Achieve%20SOTA%20Accuracy%2C%20If%20Calibrated%0AAuthor%3A%20Muli%20Yang%20and%20Gabriel%20James%20Goenawan%20and%20Henan%20Wang%20and%20Huaiyuan%20Qin%20and%20Chenghao%20Xu%20and%20Yanhua%20Yang%20and%20Fen%20Fang%20and%20Ying%20Sun%20and%20Joo-Hwee%20Lim%20and%20Hongyuan%20Zhu%0AAbstract%3A%20Despite%20being%20trained%20on%20balanced%20datasets%2C%20existing%20AI-generated%20image%20detectors%20often%20exhibit%20systematic%20bias%20at%20test%20time%2C%20frequently%20misclassifying%20fake%20images%20as%20real.%20We%20hypothesize%20that%20this%20behavior%20stems%20from%20distributional%20shift%20in%20fake%20samples%20and%20implicit%20priors%20learned%20during%20training.%20Specifically%2C%20models%20tend%20to%20overfit%20to%20superficial%20artifacts%20that%20do%20not%20generalize%20well%20across%20different%20generation%20methods%2C%20leading%20to%20a%20misaligned%20decision%20threshold%20when%20faced%20with%20test-time%20distribution%20shift.%20To%20address%20this%2C%20we%20propose%20a%20theoretically%20grounded%20post-hoc%20calibration%20framework%20based%20on%20Bayesian%20decision%20theory.%20In%20particular%2C%20we%20introduce%20a%20learnable%20scalar%20correction%20to%20the%20model%27s%20logits%2C%20optimized%20on%20a%20small%20validation%20set%20from%20the%20target%20distribution%20while%20keeping%20the%20backbone%20frozen.%20This%20parametric%20adjustment%20compensates%20for%20distributional%20shift%20in%20model%20output%2C%20realigning%20the%20decision%20boundary%20even%20without%20requiring%20ground-truth%20labels.%20Experiments%20on%20challenging%20benchmarks%20show%20that%20our%20approach%20significantly%20improves%20robustness%20without%20retraining%2C%20offering%20a%20lightweight%20and%20principled%20solution%20for%20reliable%20and%20adaptive%20AI-generated%20image%20detection%20in%20the%20open%20world.%20Code%20is%20available%20at%20https%3A//github.com/muliyangm/AIGI-Det-Calib.%0ALink%3A%20http%3A//arxiv.org/abs/2602.01973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYour%2520AI-Generated%2520Image%2520Detector%2520Can%2520Secretly%2520Achieve%2520SOTA%2520Accuracy%252C%2520If%2520Calibrated%26entry.906535625%3DMuli%2520Yang%2520and%2520Gabriel%2520James%2520Goenawan%2520and%2520Henan%2520Wang%2520and%2520Huaiyuan%2520Qin%2520and%2520Chenghao%2520Xu%2520and%2520Yanhua%2520Yang%2520and%2520Fen%2520Fang%2520and%2520Ying%2520Sun%2520and%2520Joo-Hwee%2520Lim%2520and%2520Hongyuan%2520Zhu%26entry.1292438233%3DDespite%2520being%2520trained%2520on%2520balanced%2520datasets%252C%2520existing%2520AI-generated%2520image%2520detectors%2520often%2520exhibit%2520systematic%2520bias%2520at%2520test%2520time%252C%2520frequently%2520misclassifying%2520fake%2520images%2520as%2520real.%2520We%2520hypothesize%2520that%2520this%2520behavior%2520stems%2520from%2520distributional%2520shift%2520in%2520fake%2520samples%2520and%2520implicit%2520priors%2520learned%2520during%2520training.%2520Specifically%252C%2520models%2520tend%2520to%2520overfit%2520to%2520superficial%2520artifacts%2520that%2520do%2520not%2520generalize%2520well%2520across%2520different%2520generation%2520methods%252C%2520leading%2520to%2520a%2520misaligned%2520decision%2520threshold%2520when%2520faced%2520with%2520test-time%2520distribution%2520shift.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520theoretically%2520grounded%2520post-hoc%2520calibration%2520framework%2520based%2520on%2520Bayesian%2520decision%2520theory.%2520In%2520particular%252C%2520we%2520introduce%2520a%2520learnable%2520scalar%2520correction%2520to%2520the%2520model%2527s%2520logits%252C%2520optimized%2520on%2520a%2520small%2520validation%2520set%2520from%2520the%2520target%2520distribution%2520while%2520keeping%2520the%2520backbone%2520frozen.%2520This%2520parametric%2520adjustment%2520compensates%2520for%2520distributional%2520shift%2520in%2520model%2520output%252C%2520realigning%2520the%2520decision%2520boundary%2520even%2520without%2520requiring%2520ground-truth%2520labels.%2520Experiments%2520on%2520challenging%2520benchmarks%2520show%2520that%2520our%2520approach%2520significantly%2520improves%2520robustness%2520without%2520retraining%252C%2520offering%2520a%2520lightweight%2520and%2520principled%2520solution%2520for%2520reliable%2520and%2520adaptive%2520AI-generated%2520image%2520detection%2520in%2520the%2520open%2520world.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/muliyangm/AIGI-Det-Calib.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.01973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Your%20AI-Generated%20Image%20Detector%20Can%20Secretly%20Achieve%20SOTA%20Accuracy%2C%20If%20Calibrated&entry.906535625=Muli%20Yang%20and%20Gabriel%20James%20Goenawan%20and%20Henan%20Wang%20and%20Huaiyuan%20Qin%20and%20Chenghao%20Xu%20and%20Yanhua%20Yang%20and%20Fen%20Fang%20and%20Ying%20Sun%20and%20Joo-Hwee%20Lim%20and%20Hongyuan%20Zhu&entry.1292438233=Despite%20being%20trained%20on%20balanced%20datasets%2C%20existing%20AI-generated%20image%20detectors%20often%20exhibit%20systematic%20bias%20at%20test%20time%2C%20frequently%20misclassifying%20fake%20images%20as%20real.%20We%20hypothesize%20that%20this%20behavior%20stems%20from%20distributional%20shift%20in%20fake%20samples%20and%20implicit%20priors%20learned%20during%20training.%20Specifically%2C%20models%20tend%20to%20overfit%20to%20superficial%20artifacts%20that%20do%20not%20generalize%20well%20across%20different%20generation%20methods%2C%20leading%20to%20a%20misaligned%20decision%20threshold%20when%20faced%20with%20test-time%20distribution%20shift.%20To%20address%20this%2C%20we%20propose%20a%20theoretically%20grounded%20post-hoc%20calibration%20framework%20based%20on%20Bayesian%20decision%20theory.%20In%20particular%2C%20we%20introduce%20a%20learnable%20scalar%20correction%20to%20the%20model%27s%20logits%2C%20optimized%20on%20a%20small%20validation%20set%20from%20the%20target%20distribution%20while%20keeping%20the%20backbone%20frozen.%20This%20parametric%20adjustment%20compensates%20for%20distributional%20shift%20in%20model%20output%2C%20realigning%20the%20decision%20boundary%20even%20without%20requiring%20ground-truth%20labels.%20Experiments%20on%20challenging%20benchmarks%20show%20that%20our%20approach%20significantly%20improves%20robustness%20without%20retraining%2C%20offering%20a%20lightweight%20and%20principled%20solution%20for%20reliable%20and%20adaptive%20AI-generated%20image%20detection%20in%20the%20open%20world.%20Code%20is%20available%20at%20https%3A//github.com/muliyangm/AIGI-Det-Calib.&entry.1838667208=http%3A//arxiv.org/abs/2602.01973v1&entry.124074799=Read"},
{"title": "Efficient Epistemic Uncertainty Estimation for Large Language Models via Knowledge Distillation", "author": "Seonghyeon Park and Jewon Yeom and Jaewon Sok and Jeongjae Park and Heejun Kim and Taesup Kim", "abstract": "Quantifying uncertainty in Large Language Models (LLMs) is essential for mitigating hallucinations and enabling risk-aware deployment in safety-critical tasks. However, estimating Epistemic Uncertainty(EU) via Deep Ensembles is computationally prohibitive at the scale of modern models. We propose a framework that leverages the small draft models to efficiently estimate token-level EU, bypassing the need for full-scale ensembling. Theoretically grounded in a Bias-Variance Decomposition, our approach approximates EU via Jensen-Shannon divergence among drafts (variance proxy) and KL divergence between the draft mixture and the target (bias proxy). To further ensure accuracy without significant overhead, we introduce Online Stochastic Distillation (OSD) to efficiently approximate target aggregation and the Data-Diverse Drafts (DDD) strategy to enhance draft diversity for better target approximation. Extensive experiments on GSM8K demonstrate that our method reduces the estimation error (RMSE) by up to 37% compared to baselines. Crucially, our approach achieves Hallucination Detection performance competitive with heavy perturbation-based methods like TokUR while incurring negligible inference costs, offering a practical solution for uncertainty-aware LLM deployment.", "link": "http://arxiv.org/abs/2602.01956v1", "date": "2026-02-02", "relevancy": 2.161, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6423}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5299}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Epistemic%20Uncertainty%20Estimation%20for%20Large%20Language%20Models%20via%20Knowledge%20Distillation&body=Title%3A%20Efficient%20Epistemic%20Uncertainty%20Estimation%20for%20Large%20Language%20Models%20via%20Knowledge%20Distillation%0AAuthor%3A%20Seonghyeon%20Park%20and%20Jewon%20Yeom%20and%20Jaewon%20Sok%20and%20Jeongjae%20Park%20and%20Heejun%20Kim%20and%20Taesup%20Kim%0AAbstract%3A%20Quantifying%20uncertainty%20in%20Large%20Language%20Models%20%28LLMs%29%20is%20essential%20for%20mitigating%20hallucinations%20and%20enabling%20risk-aware%20deployment%20in%20safety-critical%20tasks.%20However%2C%20estimating%20Epistemic%20Uncertainty%28EU%29%20via%20Deep%20Ensembles%20is%20computationally%20prohibitive%20at%20the%20scale%20of%20modern%20models.%20We%20propose%20a%20framework%20that%20leverages%20the%20small%20draft%20models%20to%20efficiently%20estimate%20token-level%20EU%2C%20bypassing%20the%20need%20for%20full-scale%20ensembling.%20Theoretically%20grounded%20in%20a%20Bias-Variance%20Decomposition%2C%20our%20approach%20approximates%20EU%20via%20Jensen-Shannon%20divergence%20among%20drafts%20%28variance%20proxy%29%20and%20KL%20divergence%20between%20the%20draft%20mixture%20and%20the%20target%20%28bias%20proxy%29.%20To%20further%20ensure%20accuracy%20without%20significant%20overhead%2C%20we%20introduce%20Online%20Stochastic%20Distillation%20%28OSD%29%20to%20efficiently%20approximate%20target%20aggregation%20and%20the%20Data-Diverse%20Drafts%20%28DDD%29%20strategy%20to%20enhance%20draft%20diversity%20for%20better%20target%20approximation.%20Extensive%20experiments%20on%20GSM8K%20demonstrate%20that%20our%20method%20reduces%20the%20estimation%20error%20%28RMSE%29%20by%20up%20to%2037%25%20compared%20to%20baselines.%20Crucially%2C%20our%20approach%20achieves%20Hallucination%20Detection%20performance%20competitive%20with%20heavy%20perturbation-based%20methods%20like%20TokUR%20while%20incurring%20negligible%20inference%20costs%2C%20offering%20a%20practical%20solution%20for%20uncertainty-aware%20LLM%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2602.01956v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Epistemic%2520Uncertainty%2520Estimation%2520for%2520Large%2520Language%2520Models%2520via%2520Knowledge%2520Distillation%26entry.906535625%3DSeonghyeon%2520Park%2520and%2520Jewon%2520Yeom%2520and%2520Jaewon%2520Sok%2520and%2520Jeongjae%2520Park%2520and%2520Heejun%2520Kim%2520and%2520Taesup%2520Kim%26entry.1292438233%3DQuantifying%2520uncertainty%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520essential%2520for%2520mitigating%2520hallucinations%2520and%2520enabling%2520risk-aware%2520deployment%2520in%2520safety-critical%2520tasks.%2520However%252C%2520estimating%2520Epistemic%2520Uncertainty%2528EU%2529%2520via%2520Deep%2520Ensembles%2520is%2520computationally%2520prohibitive%2520at%2520the%2520scale%2520of%2520modern%2520models.%2520We%2520propose%2520a%2520framework%2520that%2520leverages%2520the%2520small%2520draft%2520models%2520to%2520efficiently%2520estimate%2520token-level%2520EU%252C%2520bypassing%2520the%2520need%2520for%2520full-scale%2520ensembling.%2520Theoretically%2520grounded%2520in%2520a%2520Bias-Variance%2520Decomposition%252C%2520our%2520approach%2520approximates%2520EU%2520via%2520Jensen-Shannon%2520divergence%2520among%2520drafts%2520%2528variance%2520proxy%2529%2520and%2520KL%2520divergence%2520between%2520the%2520draft%2520mixture%2520and%2520the%2520target%2520%2528bias%2520proxy%2529.%2520To%2520further%2520ensure%2520accuracy%2520without%2520significant%2520overhead%252C%2520we%2520introduce%2520Online%2520Stochastic%2520Distillation%2520%2528OSD%2529%2520to%2520efficiently%2520approximate%2520target%2520aggregation%2520and%2520the%2520Data-Diverse%2520Drafts%2520%2528DDD%2529%2520strategy%2520to%2520enhance%2520draft%2520diversity%2520for%2520better%2520target%2520approximation.%2520Extensive%2520experiments%2520on%2520GSM8K%2520demonstrate%2520that%2520our%2520method%2520reduces%2520the%2520estimation%2520error%2520%2528RMSE%2529%2520by%2520up%2520to%252037%2525%2520compared%2520to%2520baselines.%2520Crucially%252C%2520our%2520approach%2520achieves%2520Hallucination%2520Detection%2520performance%2520competitive%2520with%2520heavy%2520perturbation-based%2520methods%2520like%2520TokUR%2520while%2520incurring%2520negligible%2520inference%2520costs%252C%2520offering%2520a%2520practical%2520solution%2520for%2520uncertainty-aware%2520LLM%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.01956v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Epistemic%20Uncertainty%20Estimation%20for%20Large%20Language%20Models%20via%20Knowledge%20Distillation&entry.906535625=Seonghyeon%20Park%20and%20Jewon%20Yeom%20and%20Jaewon%20Sok%20and%20Jeongjae%20Park%20and%20Heejun%20Kim%20and%20Taesup%20Kim&entry.1292438233=Quantifying%20uncertainty%20in%20Large%20Language%20Models%20%28LLMs%29%20is%20essential%20for%20mitigating%20hallucinations%20and%20enabling%20risk-aware%20deployment%20in%20safety-critical%20tasks.%20However%2C%20estimating%20Epistemic%20Uncertainty%28EU%29%20via%20Deep%20Ensembles%20is%20computationally%20prohibitive%20at%20the%20scale%20of%20modern%20models.%20We%20propose%20a%20framework%20that%20leverages%20the%20small%20draft%20models%20to%20efficiently%20estimate%20token-level%20EU%2C%20bypassing%20the%20need%20for%20full-scale%20ensembling.%20Theoretically%20grounded%20in%20a%20Bias-Variance%20Decomposition%2C%20our%20approach%20approximates%20EU%20via%20Jensen-Shannon%20divergence%20among%20drafts%20%28variance%20proxy%29%20and%20KL%20divergence%20between%20the%20draft%20mixture%20and%20the%20target%20%28bias%20proxy%29.%20To%20further%20ensure%20accuracy%20without%20significant%20overhead%2C%20we%20introduce%20Online%20Stochastic%20Distillation%20%28OSD%29%20to%20efficiently%20approximate%20target%20aggregation%20and%20the%20Data-Diverse%20Drafts%20%28DDD%29%20strategy%20to%20enhance%20draft%20diversity%20for%20better%20target%20approximation.%20Extensive%20experiments%20on%20GSM8K%20demonstrate%20that%20our%20method%20reduces%20the%20estimation%20error%20%28RMSE%29%20by%20up%20to%2037%25%20compared%20to%20baselines.%20Crucially%2C%20our%20approach%20achieves%20Hallucination%20Detection%20performance%20competitive%20with%20heavy%20perturbation-based%20methods%20like%20TokUR%20while%20incurring%20negligible%20inference%20costs%2C%20offering%20a%20practical%20solution%20for%20uncertainty-aware%20LLM%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2602.01956v1&entry.124074799=Read"},
{"title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents", "author": "Hang Yan and Xinyu Che and Fangzhi Xu and Qiushi Sun and Zichen Ding and Kanzhi Cheng and Jian Zhang and Tao Qin and Jun Liu and Qika Lin", "abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.", "link": "http://arxiv.org/abs/2602.02196v1", "date": "2026-02-02", "relevancy": 1.9266, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4885}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4858}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TIDE%3A%20Trajectory-based%20Diagnostic%20Evaluation%20of%20Test-Time%20Improvement%20in%20LLM%20Agents&body=Title%3A%20TIDE%3A%20Trajectory-based%20Diagnostic%20Evaluation%20of%20Test-Time%20Improvement%20in%20LLM%20Agents%0AAuthor%3A%20Hang%20Yan%20and%20Xinyu%20Che%20and%20Fangzhi%20Xu%20and%20Qiushi%20Sun%20and%20Zichen%20Ding%20and%20Kanzhi%20Cheng%20and%20Jian%20Zhang%20and%20Tao%20Qin%20and%20Jun%20Liu%20and%20Qika%20Lin%0AAbstract%3A%20Recent%20advances%20in%20autonomous%20LLM%20agents%20demonstrate%20their%20ability%20to%20improve%20performance%20through%20iterative%20interaction%20with%20the%20environment.%20We%20define%20this%20paradigm%20as%20Test-Time%20Improvement%20%28TTI%29.%20However%2C%20the%20mechanisms%20under%20how%20and%20why%20TTI%20succeed%20or%20fail%20remain%20poorly%20understood%2C%20and%20existing%20evaluation%20metrics%20fail%20to%20capture%20their%20task%20optimization%20efficiency%2C%20behavior%20adaptation%20after%20erroneous%20actions%2C%20and%20the%20specific%20utility%20of%20working%20memory%20for%20task%20completion.%20To%20address%20these%20gaps%2C%20we%20propose%20Test-time%20Improvement%20Diagnostic%20Evaluation%20%28TIDE%29%2C%20an%20agent-agnostic%20and%20environment-agnostic%20framework%20that%20decomposes%20TTI%20into%20three%20comprehensive%20and%20interconnected%20dimensions.%20The%20framework%20measures%20%281%29%20the%20overall%20temporal%20dynamics%20of%20task%20completion%20and%20%282%29%20identifies%20whether%20performance%20is%20primarily%20constrained%20by%20recursive%20looping%20behaviors%20or%20%283%29%20by%20burdensome%20accumulated%20memory.%20Through%20extensive%20experiments%20across%20diverse%20agents%20and%20environments%2C%20TIDE%20highlights%20that%20improving%20agent%20performance%20requires%20more%20than%20scaling%20internal%20reasoning%2C%20calling%20for%20explicitly%20optimizing%20the%20interaction%20dynamics%20between%20the%20agent%20and%20the%20environment.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02196v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTIDE%253A%2520Trajectory-based%2520Diagnostic%2520Evaluation%2520of%2520Test-Time%2520Improvement%2520in%2520LLM%2520Agents%26entry.906535625%3DHang%2520Yan%2520and%2520Xinyu%2520Che%2520and%2520Fangzhi%2520Xu%2520and%2520Qiushi%2520Sun%2520and%2520Zichen%2520Ding%2520and%2520Kanzhi%2520Cheng%2520and%2520Jian%2520Zhang%2520and%2520Tao%2520Qin%2520and%2520Jun%2520Liu%2520and%2520Qika%2520Lin%26entry.1292438233%3DRecent%2520advances%2520in%2520autonomous%2520LLM%2520agents%2520demonstrate%2520their%2520ability%2520to%2520improve%2520performance%2520through%2520iterative%2520interaction%2520with%2520the%2520environment.%2520We%2520define%2520this%2520paradigm%2520as%2520Test-Time%2520Improvement%2520%2528TTI%2529.%2520However%252C%2520the%2520mechanisms%2520under%2520how%2520and%2520why%2520TTI%2520succeed%2520or%2520fail%2520remain%2520poorly%2520understood%252C%2520and%2520existing%2520evaluation%2520metrics%2520fail%2520to%2520capture%2520their%2520task%2520optimization%2520efficiency%252C%2520behavior%2520adaptation%2520after%2520erroneous%2520actions%252C%2520and%2520the%2520specific%2520utility%2520of%2520working%2520memory%2520for%2520task%2520completion.%2520To%2520address%2520these%2520gaps%252C%2520we%2520propose%2520Test-time%2520Improvement%2520Diagnostic%2520Evaluation%2520%2528TIDE%2529%252C%2520an%2520agent-agnostic%2520and%2520environment-agnostic%2520framework%2520that%2520decomposes%2520TTI%2520into%2520three%2520comprehensive%2520and%2520interconnected%2520dimensions.%2520The%2520framework%2520measures%2520%25281%2529%2520the%2520overall%2520temporal%2520dynamics%2520of%2520task%2520completion%2520and%2520%25282%2529%2520identifies%2520whether%2520performance%2520is%2520primarily%2520constrained%2520by%2520recursive%2520looping%2520behaviors%2520or%2520%25283%2529%2520by%2520burdensome%2520accumulated%2520memory.%2520Through%2520extensive%2520experiments%2520across%2520diverse%2520agents%2520and%2520environments%252C%2520TIDE%2520highlights%2520that%2520improving%2520agent%2520performance%2520requires%2520more%2520than%2520scaling%2520internal%2520reasoning%252C%2520calling%2520for%2520explicitly%2520optimizing%2520the%2520interaction%2520dynamics%2520between%2520the%2520agent%2520and%2520the%2520environment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02196v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TIDE%3A%20Trajectory-based%20Diagnostic%20Evaluation%20of%20Test-Time%20Improvement%20in%20LLM%20Agents&entry.906535625=Hang%20Yan%20and%20Xinyu%20Che%20and%20Fangzhi%20Xu%20and%20Qiushi%20Sun%20and%20Zichen%20Ding%20and%20Kanzhi%20Cheng%20and%20Jian%20Zhang%20and%20Tao%20Qin%20and%20Jun%20Liu%20and%20Qika%20Lin&entry.1292438233=Recent%20advances%20in%20autonomous%20LLM%20agents%20demonstrate%20their%20ability%20to%20improve%20performance%20through%20iterative%20interaction%20with%20the%20environment.%20We%20define%20this%20paradigm%20as%20Test-Time%20Improvement%20%28TTI%29.%20However%2C%20the%20mechanisms%20under%20how%20and%20why%20TTI%20succeed%20or%20fail%20remain%20poorly%20understood%2C%20and%20existing%20evaluation%20metrics%20fail%20to%20capture%20their%20task%20optimization%20efficiency%2C%20behavior%20adaptation%20after%20erroneous%20actions%2C%20and%20the%20specific%20utility%20of%20working%20memory%20for%20task%20completion.%20To%20address%20these%20gaps%2C%20we%20propose%20Test-time%20Improvement%20Diagnostic%20Evaluation%20%28TIDE%29%2C%20an%20agent-agnostic%20and%20environment-agnostic%20framework%20that%20decomposes%20TTI%20into%20three%20comprehensive%20and%20interconnected%20dimensions.%20The%20framework%20measures%20%281%29%20the%20overall%20temporal%20dynamics%20of%20task%20completion%20and%20%282%29%20identifies%20whether%20performance%20is%20primarily%20constrained%20by%20recursive%20looping%20behaviors%20or%20%283%29%20by%20burdensome%20accumulated%20memory.%20Through%20extensive%20experiments%20across%20diverse%20agents%20and%20environments%2C%20TIDE%20highlights%20that%20improving%20agent%20performance%20requires%20more%20than%20scaling%20internal%20reasoning%2C%20calling%20for%20explicitly%20optimizing%20the%20interaction%20dynamics%20between%20the%20agent%20and%20the%20environment.&entry.1838667208=http%3A//arxiv.org/abs/2602.02196v1&entry.124074799=Read"},
{"title": "Trust Region Continual Learning as an Implicit Meta-Learner", "author": "Zekun Wang and Anant Gupta and Christopher J. MacLellan", "abstract": "Continual learning aims to acquire tasks sequentially without catastrophic forgetting, yet standard strategies face a core tradeoff: regularization-based methods (e.g., EWC) can overconstrain updates when task optima are weakly overlapping, while replay-based methods can retain performance but drift due to imperfect replay. We study a hybrid perspective: \\emph{trust region continual learning} that combines generative replay with a Fisher-metric trust region constraint. We show that, under local approximations, the resulting update admits a MAML-style interpretation with a single implicit inner step: replay supplies an old-task gradient signal (query-like), while the Fisher-weighted penalty provides an efficient offline curvature shaping (support-like). This yields an emergent meta-learning property in continual learning: the model becomes an initialization that rapidly \\emph{re-converges} to prior task optima after each task transition, without explicitly optimizing a bilevel objective. Empirically, on task-incremental diffusion image generation and continual diffusion-policy control, trust region continual learning achieves the best final performance and retention, and consistently recovers early-task performance faster than EWC, replay, and continual meta-learning baselines.", "link": "http://arxiv.org/abs/2602.02417v1", "date": "2026-02-02", "relevancy": 1.9632, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5025}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4914}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trust%20Region%20Continual%20Learning%20as%20an%20Implicit%20Meta-Learner&body=Title%3A%20Trust%20Region%20Continual%20Learning%20as%20an%20Implicit%20Meta-Learner%0AAuthor%3A%20Zekun%20Wang%20and%20Anant%20Gupta%20and%20Christopher%20J.%20MacLellan%0AAbstract%3A%20Continual%20learning%20aims%20to%20acquire%20tasks%20sequentially%20without%20catastrophic%20forgetting%2C%20yet%20standard%20strategies%20face%20a%20core%20tradeoff%3A%20regularization-based%20methods%20%28e.g.%2C%20EWC%29%20can%20overconstrain%20updates%20when%20task%20optima%20are%20weakly%20overlapping%2C%20while%20replay-based%20methods%20can%20retain%20performance%20but%20drift%20due%20to%20imperfect%20replay.%20We%20study%20a%20hybrid%20perspective%3A%20%5Cemph%7Btrust%20region%20continual%20learning%7D%20that%20combines%20generative%20replay%20with%20a%20Fisher-metric%20trust%20region%20constraint.%20We%20show%20that%2C%20under%20local%20approximations%2C%20the%20resulting%20update%20admits%20a%20MAML-style%20interpretation%20with%20a%20single%20implicit%20inner%20step%3A%20replay%20supplies%20an%20old-task%20gradient%20signal%20%28query-like%29%2C%20while%20the%20Fisher-weighted%20penalty%20provides%20an%20efficient%20offline%20curvature%20shaping%20%28support-like%29.%20This%20yields%20an%20emergent%20meta-learning%20property%20in%20continual%20learning%3A%20the%20model%20becomes%20an%20initialization%20that%20rapidly%20%5Cemph%7Bre-converges%7D%20to%20prior%20task%20optima%20after%20each%20task%20transition%2C%20without%20explicitly%20optimizing%20a%20bilevel%20objective.%20Empirically%2C%20on%20task-incremental%20diffusion%20image%20generation%20and%20continual%20diffusion-policy%20control%2C%20trust%20region%20continual%20learning%20achieves%20the%20best%20final%20performance%20and%20retention%2C%20and%20consistently%20recovers%20early-task%20performance%20faster%20than%20EWC%2C%20replay%2C%20and%20continual%20meta-learning%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02417v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrust%2520Region%2520Continual%2520Learning%2520as%2520an%2520Implicit%2520Meta-Learner%26entry.906535625%3DZekun%2520Wang%2520and%2520Anant%2520Gupta%2520and%2520Christopher%2520J.%2520MacLellan%26entry.1292438233%3DContinual%2520learning%2520aims%2520to%2520acquire%2520tasks%2520sequentially%2520without%2520catastrophic%2520forgetting%252C%2520yet%2520standard%2520strategies%2520face%2520a%2520core%2520tradeoff%253A%2520regularization-based%2520methods%2520%2528e.g.%252C%2520EWC%2529%2520can%2520overconstrain%2520updates%2520when%2520task%2520optima%2520are%2520weakly%2520overlapping%252C%2520while%2520replay-based%2520methods%2520can%2520retain%2520performance%2520but%2520drift%2520due%2520to%2520imperfect%2520replay.%2520We%2520study%2520a%2520hybrid%2520perspective%253A%2520%255Cemph%257Btrust%2520region%2520continual%2520learning%257D%2520that%2520combines%2520generative%2520replay%2520with%2520a%2520Fisher-metric%2520trust%2520region%2520constraint.%2520We%2520show%2520that%252C%2520under%2520local%2520approximations%252C%2520the%2520resulting%2520update%2520admits%2520a%2520MAML-style%2520interpretation%2520with%2520a%2520single%2520implicit%2520inner%2520step%253A%2520replay%2520supplies%2520an%2520old-task%2520gradient%2520signal%2520%2528query-like%2529%252C%2520while%2520the%2520Fisher-weighted%2520penalty%2520provides%2520an%2520efficient%2520offline%2520curvature%2520shaping%2520%2528support-like%2529.%2520This%2520yields%2520an%2520emergent%2520meta-learning%2520property%2520in%2520continual%2520learning%253A%2520the%2520model%2520becomes%2520an%2520initialization%2520that%2520rapidly%2520%255Cemph%257Bre-converges%257D%2520to%2520prior%2520task%2520optima%2520after%2520each%2520task%2520transition%252C%2520without%2520explicitly%2520optimizing%2520a%2520bilevel%2520objective.%2520Empirically%252C%2520on%2520task-incremental%2520diffusion%2520image%2520generation%2520and%2520continual%2520diffusion-policy%2520control%252C%2520trust%2520region%2520continual%2520learning%2520achieves%2520the%2520best%2520final%2520performance%2520and%2520retention%252C%2520and%2520consistently%2520recovers%2520early-task%2520performance%2520faster%2520than%2520EWC%252C%2520replay%252C%2520and%2520continual%2520meta-learning%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02417v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trust%20Region%20Continual%20Learning%20as%20an%20Implicit%20Meta-Learner&entry.906535625=Zekun%20Wang%20and%20Anant%20Gupta%20and%20Christopher%20J.%20MacLellan&entry.1292438233=Continual%20learning%20aims%20to%20acquire%20tasks%20sequentially%20without%20catastrophic%20forgetting%2C%20yet%20standard%20strategies%20face%20a%20core%20tradeoff%3A%20regularization-based%20methods%20%28e.g.%2C%20EWC%29%20can%20overconstrain%20updates%20when%20task%20optima%20are%20weakly%20overlapping%2C%20while%20replay-based%20methods%20can%20retain%20performance%20but%20drift%20due%20to%20imperfect%20replay.%20We%20study%20a%20hybrid%20perspective%3A%20%5Cemph%7Btrust%20region%20continual%20learning%7D%20that%20combines%20generative%20replay%20with%20a%20Fisher-metric%20trust%20region%20constraint.%20We%20show%20that%2C%20under%20local%20approximations%2C%20the%20resulting%20update%20admits%20a%20MAML-style%20interpretation%20with%20a%20single%20implicit%20inner%20step%3A%20replay%20supplies%20an%20old-task%20gradient%20signal%20%28query-like%29%2C%20while%20the%20Fisher-weighted%20penalty%20provides%20an%20efficient%20offline%20curvature%20shaping%20%28support-like%29.%20This%20yields%20an%20emergent%20meta-learning%20property%20in%20continual%20learning%3A%20the%20model%20becomes%20an%20initialization%20that%20rapidly%20%5Cemph%7Bre-converges%7D%20to%20prior%20task%20optima%20after%20each%20task%20transition%2C%20without%20explicitly%20optimizing%20a%20bilevel%20objective.%20Empirically%2C%20on%20task-incremental%20diffusion%20image%20generation%20and%20continual%20diffusion-policy%20control%2C%20trust%20region%20continual%20learning%20achieves%20the%20best%20final%20performance%20and%20retention%2C%20and%20consistently%20recovers%20early-task%20performance%20faster%20than%20EWC%2C%20replay%2C%20and%20continual%20meta-learning%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2602.02417v1&entry.124074799=Read"},
{"title": "Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models", "author": "Wei Liu and Peijie Yu and Michele Orini and Yali Du and Yulan He", "abstract": "The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence, distinguishing it from executional intelligence, which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.", "link": "http://arxiv.org/abs/2602.02039v1", "date": "2026-02-02", "relevancy": 2.0913, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5334}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5334}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hunt%20Instead%20of%20Wait%3A%20Evaluating%20Deep%20Data%20Research%20on%20Large%20Language%20Models&body=Title%3A%20Hunt%20Instead%20of%20Wait%3A%20Evaluating%20Deep%20Data%20Research%20on%20Large%20Language%20Models%0AAuthor%3A%20Wei%20Liu%20and%20Peijie%20Yu%20and%20Michele%20Orini%20and%20Yali%20Du%20and%20Yulan%20He%0AAbstract%3A%20The%20agency%20expected%20of%20Agentic%20Large%20Language%20Models%20goes%20beyond%20answering%20correctly%2C%20requiring%20autonomy%20to%20set%20goals%20and%20decide%20what%20to%20explore.%20We%20term%20this%20investigatory%20intelligence%2C%20distinguishing%20it%20from%20executional%20intelligence%2C%20which%20merely%20completes%20assigned%20tasks.%20Data%20Science%20provides%20a%20natural%20testbed%2C%20as%20real-world%20analysis%20starts%20from%20raw%20data%20rather%20than%20explicit%20queries%2C%20yet%20few%20benchmarks%20focus%20on%20it.%20To%20address%20this%2C%20we%20introduce%20Deep%20Data%20Research%20%28DDR%29%2C%20an%20open-ended%20task%20where%20LLMs%20autonomously%20extract%20key%20insights%20from%20databases%2C%20and%20DDR-Bench%2C%20a%20large-scale%2C%20checklist-based%20benchmark%20that%20enables%20verifiable%20evaluation.%20Results%20show%20that%20while%20frontier%20models%20display%20emerging%20agency%2C%20long-horizon%20exploration%20remains%20challenging.%20Our%20analysis%20highlights%20that%20effective%20investigatory%20intelligence%20depends%20not%20only%20on%20agent%20scaffolding%20or%20merely%20scaling%2C%20but%20also%20on%20intrinsic%20strategies%20of%20agentic%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02039v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHunt%2520Instead%2520of%2520Wait%253A%2520Evaluating%2520Deep%2520Data%2520Research%2520on%2520Large%2520Language%2520Models%26entry.906535625%3DWei%2520Liu%2520and%2520Peijie%2520Yu%2520and%2520Michele%2520Orini%2520and%2520Yali%2520Du%2520and%2520Yulan%2520He%26entry.1292438233%3DThe%2520agency%2520expected%2520of%2520Agentic%2520Large%2520Language%2520Models%2520goes%2520beyond%2520answering%2520correctly%252C%2520requiring%2520autonomy%2520to%2520set%2520goals%2520and%2520decide%2520what%2520to%2520explore.%2520We%2520term%2520this%2520investigatory%2520intelligence%252C%2520distinguishing%2520it%2520from%2520executional%2520intelligence%252C%2520which%2520merely%2520completes%2520assigned%2520tasks.%2520Data%2520Science%2520provides%2520a%2520natural%2520testbed%252C%2520as%2520real-world%2520analysis%2520starts%2520from%2520raw%2520data%2520rather%2520than%2520explicit%2520queries%252C%2520yet%2520few%2520benchmarks%2520focus%2520on%2520it.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Deep%2520Data%2520Research%2520%2528DDR%2529%252C%2520an%2520open-ended%2520task%2520where%2520LLMs%2520autonomously%2520extract%2520key%2520insights%2520from%2520databases%252C%2520and%2520DDR-Bench%252C%2520a%2520large-scale%252C%2520checklist-based%2520benchmark%2520that%2520enables%2520verifiable%2520evaluation.%2520Results%2520show%2520that%2520while%2520frontier%2520models%2520display%2520emerging%2520agency%252C%2520long-horizon%2520exploration%2520remains%2520challenging.%2520Our%2520analysis%2520highlights%2520that%2520effective%2520investigatory%2520intelligence%2520depends%2520not%2520only%2520on%2520agent%2520scaffolding%2520or%2520merely%2520scaling%252C%2520but%2520also%2520on%2520intrinsic%2520strategies%2520of%2520agentic%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02039v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hunt%20Instead%20of%20Wait%3A%20Evaluating%20Deep%20Data%20Research%20on%20Large%20Language%20Models&entry.906535625=Wei%20Liu%20and%20Peijie%20Yu%20and%20Michele%20Orini%20and%20Yali%20Du%20and%20Yulan%20He&entry.1292438233=The%20agency%20expected%20of%20Agentic%20Large%20Language%20Models%20goes%20beyond%20answering%20correctly%2C%20requiring%20autonomy%20to%20set%20goals%20and%20decide%20what%20to%20explore.%20We%20term%20this%20investigatory%20intelligence%2C%20distinguishing%20it%20from%20executional%20intelligence%2C%20which%20merely%20completes%20assigned%20tasks.%20Data%20Science%20provides%20a%20natural%20testbed%2C%20as%20real-world%20analysis%20starts%20from%20raw%20data%20rather%20than%20explicit%20queries%2C%20yet%20few%20benchmarks%20focus%20on%20it.%20To%20address%20this%2C%20we%20introduce%20Deep%20Data%20Research%20%28DDR%29%2C%20an%20open-ended%20task%20where%20LLMs%20autonomously%20extract%20key%20insights%20from%20databases%2C%20and%20DDR-Bench%2C%20a%20large-scale%2C%20checklist-based%20benchmark%20that%20enables%20verifiable%20evaluation.%20Results%20show%20that%20while%20frontier%20models%20display%20emerging%20agency%2C%20long-horizon%20exploration%20remains%20challenging.%20Our%20analysis%20highlights%20that%20effective%20investigatory%20intelligence%20depends%20not%20only%20on%20agent%20scaffolding%20or%20merely%20scaling%2C%20but%20also%20on%20intrinsic%20strategies%20of%20agentic%20models.&entry.1838667208=http%3A//arxiv.org/abs/2602.02039v1&entry.124074799=Read"},
{"title": "Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation", "author": "Zihao Wang and Yuzhou Chen and Shaogang Ren", "abstract": "Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model's role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.", "link": "http://arxiv.org/abs/2601.18623v2", "date": "2026-02-02", "relevancy": 1.2011, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6299}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5981}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Domain%20Shift%20in%20Diffusion%20Models%20for%20Cross-Modality%20Image%20Translation&body=Title%3A%20Adaptive%20Domain%20Shift%20in%20Diffusion%20Models%20for%20Cross-Modality%20Image%20Translation%0AAuthor%3A%20Zihao%20Wang%20and%20Yuzhou%20Chen%20and%20Shaogang%20Ren%0AAbstract%3A%20Cross-modal%20image%20translation%20remains%20brittle%20and%20inefficient.%20Standard%20diffusion%20approaches%20often%20rely%20on%20a%20single%2C%20global%20linear%20transfer%20between%20domains.%20We%20find%20that%20this%20shortcut%20forces%20the%20sampler%20to%20traverse%20off-manifold%2C%20high-cost%20regions%2C%20inflating%20the%20correction%20burden%20and%20inviting%20semantic%20drift.%20We%20refer%20to%20this%20shared%20failure%20mode%20as%20fixed-schedule%20domain%20transfer.%20In%20this%20paper%2C%20we%20embed%20domain-shift%20dynamics%20directly%20into%20the%20generative%20process.%20Our%20model%20predicts%20a%20spatially%20varying%20mixing%20field%20at%20every%20reverse%20step%20and%20injects%20an%20explicit%2C%20target-consistent%20restoration%20term%20into%20the%20drift.%20This%20in-step%20guidance%20keeps%20large%20updates%20on-manifold%20and%20shifts%20the%20model%27s%20role%20from%20global%20alignment%20to%20local%20residual%20correction.%20We%20provide%20a%20continuous-time%20formulation%20with%20an%20exact%20solution%20form%20and%20derive%20a%20practical%20first-order%20sampler%20that%20preserves%20marginal%20consistency.%20Empirically%2C%20across%20translation%20tasks%20in%20medical%20imaging%2C%20remote%20sensing%2C%20and%20electroluminescence%20semantic%20mapping%2C%20our%20framework%20improves%20structural%20fidelity%20and%20semantic%20consistency%20while%20converging%20in%20fewer%20denoising%20steps.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18623v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Domain%2520Shift%2520in%2520Diffusion%2520Models%2520for%2520Cross-Modality%2520Image%2520Translation%26entry.906535625%3DZihao%2520Wang%2520and%2520Yuzhou%2520Chen%2520and%2520Shaogang%2520Ren%26entry.1292438233%3DCross-modal%2520image%2520translation%2520remains%2520brittle%2520and%2520inefficient.%2520Standard%2520diffusion%2520approaches%2520often%2520rely%2520on%2520a%2520single%252C%2520global%2520linear%2520transfer%2520between%2520domains.%2520We%2520find%2520that%2520this%2520shortcut%2520forces%2520the%2520sampler%2520to%2520traverse%2520off-manifold%252C%2520high-cost%2520regions%252C%2520inflating%2520the%2520correction%2520burden%2520and%2520inviting%2520semantic%2520drift.%2520We%2520refer%2520to%2520this%2520shared%2520failure%2520mode%2520as%2520fixed-schedule%2520domain%2520transfer.%2520In%2520this%2520paper%252C%2520we%2520embed%2520domain-shift%2520dynamics%2520directly%2520into%2520the%2520generative%2520process.%2520Our%2520model%2520predicts%2520a%2520spatially%2520varying%2520mixing%2520field%2520at%2520every%2520reverse%2520step%2520and%2520injects%2520an%2520explicit%252C%2520target-consistent%2520restoration%2520term%2520into%2520the%2520drift.%2520This%2520in-step%2520guidance%2520keeps%2520large%2520updates%2520on-manifold%2520and%2520shifts%2520the%2520model%2527s%2520role%2520from%2520global%2520alignment%2520to%2520local%2520residual%2520correction.%2520We%2520provide%2520a%2520continuous-time%2520formulation%2520with%2520an%2520exact%2520solution%2520form%2520and%2520derive%2520a%2520practical%2520first-order%2520sampler%2520that%2520preserves%2520marginal%2520consistency.%2520Empirically%252C%2520across%2520translation%2520tasks%2520in%2520medical%2520imaging%252C%2520remote%2520sensing%252C%2520and%2520electroluminescence%2520semantic%2520mapping%252C%2520our%2520framework%2520improves%2520structural%2520fidelity%2520and%2520semantic%2520consistency%2520while%2520converging%2520in%2520fewer%2520denoising%2520steps.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18623v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Domain%20Shift%20in%20Diffusion%20Models%20for%20Cross-Modality%20Image%20Translation&entry.906535625=Zihao%20Wang%20and%20Yuzhou%20Chen%20and%20Shaogang%20Ren&entry.1292438233=Cross-modal%20image%20translation%20remains%20brittle%20and%20inefficient.%20Standard%20diffusion%20approaches%20often%20rely%20on%20a%20single%2C%20global%20linear%20transfer%20between%20domains.%20We%20find%20that%20this%20shortcut%20forces%20the%20sampler%20to%20traverse%20off-manifold%2C%20high-cost%20regions%2C%20inflating%20the%20correction%20burden%20and%20inviting%20semantic%20drift.%20We%20refer%20to%20this%20shared%20failure%20mode%20as%20fixed-schedule%20domain%20transfer.%20In%20this%20paper%2C%20we%20embed%20domain-shift%20dynamics%20directly%20into%20the%20generative%20process.%20Our%20model%20predicts%20a%20spatially%20varying%20mixing%20field%20at%20every%20reverse%20step%20and%20injects%20an%20explicit%2C%20target-consistent%20restoration%20term%20into%20the%20drift.%20This%20in-step%20guidance%20keeps%20large%20updates%20on-manifold%20and%20shifts%20the%20model%27s%20role%20from%20global%20alignment%20to%20local%20residual%20correction.%20We%20provide%20a%20continuous-time%20formulation%20with%20an%20exact%20solution%20form%20and%20derive%20a%20practical%20first-order%20sampler%20that%20preserves%20marginal%20consistency.%20Empirically%2C%20across%20translation%20tasks%20in%20medical%20imaging%2C%20remote%20sensing%2C%20and%20electroluminescence%20semantic%20mapping%2C%20our%20framework%20improves%20structural%20fidelity%20and%20semantic%20consistency%20while%20converging%20in%20fewer%20denoising%20steps.&entry.1838667208=http%3A//arxiv.org/abs/2601.18623v2&entry.124074799=Read"},
{"title": "SEDformer: Event-Synchronous Spiking Transformers for Irregular Telemetry Time Series Forecasting", "author": "Ziyu Zhou and Yuchen Fang and Weilin Ruan and Shiyu Wang and James Kwok and Yuxuan Liang", "abstract": "Telemetry streams from large-scale Internet-connected systems (e.g., IoT deployments and online platforms) naturally form an irregular multivariate time series (IMTS) whose accurate forecasting is operationally vital. A closer examination reveals a defining Sparsity-Event Duality (SED) property of IMTS, i.e., long stretches with sparse or no observations are punctuated by short, dense bursts where most semantic events (observations) occur. However, existing Graph- and Transformer-based forecasters ignore SED: pre-alignment to uniform grids with heavy padding violates sparsity by inflating sequences and forcing computation at non-informative steps, while relational recasting weakens event semantics by disrupting local temporal continuity. These limitations motivate a more faithful and natural modeling paradigm for IMTS that aligns with its SED property. We find that Spiking Neural Networks meet this requirement, as they communicate via sparse binary spikes and update in an event-driven manner, aligning naturally with the SED nature of IMTS. Therefore, we present SEDformer, an SED-enhanced Spiking Transformer for telemetry IMTS forecasting that couples: (1) a SED-based Spike Encoder converts raw observations into event synchronous spikes using an Event-Aligned LIF neuron, (2) an Event-Preserving Temporal Downsampling module compresses long gaps while retaining salient firings and (3) a stack of SED-based Spike Transformer blocks enable intra-series dependency modeling with a membrane-based linear attention driven by EA-LIF spiking features. Experiments on public telemetry IMTS datasets show that SEDformer attains state-of-the-art forecasting accuracy while reducing energy and memory usage, providing a natural and efficient path for modeling IMTS.", "link": "http://arxiv.org/abs/2602.02230v1", "date": "2026-02-02", "relevancy": 1.4423, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5073}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4734}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEDformer%3A%20Event-Synchronous%20Spiking%20Transformers%20for%20Irregular%20Telemetry%20Time%20Series%20Forecasting&body=Title%3A%20SEDformer%3A%20Event-Synchronous%20Spiking%20Transformers%20for%20Irregular%20Telemetry%20Time%20Series%20Forecasting%0AAuthor%3A%20Ziyu%20Zhou%20and%20Yuchen%20Fang%20and%20Weilin%20Ruan%20and%20Shiyu%20Wang%20and%20James%20Kwok%20and%20Yuxuan%20Liang%0AAbstract%3A%20Telemetry%20streams%20from%20large-scale%20Internet-connected%20systems%20%28e.g.%2C%20IoT%20deployments%20and%20online%20platforms%29%20naturally%20form%20an%20irregular%20multivariate%20time%20series%20%28IMTS%29%20whose%20accurate%20forecasting%20is%20operationally%20vital.%20A%20closer%20examination%20reveals%20a%20defining%20Sparsity-Event%20Duality%20%28SED%29%20property%20of%20IMTS%2C%20i.e.%2C%20long%20stretches%20with%20sparse%20or%20no%20observations%20are%20punctuated%20by%20short%2C%20dense%20bursts%20where%20most%20semantic%20events%20%28observations%29%20occur.%20However%2C%20existing%20Graph-%20and%20Transformer-based%20forecasters%20ignore%20SED%3A%20pre-alignment%20to%20uniform%20grids%20with%20heavy%20padding%20violates%20sparsity%20by%20inflating%20sequences%20and%20forcing%20computation%20at%20non-informative%20steps%2C%20while%20relational%20recasting%20weakens%20event%20semantics%20by%20disrupting%20local%20temporal%20continuity.%20These%20limitations%20motivate%20a%20more%20faithful%20and%20natural%20modeling%20paradigm%20for%20IMTS%20that%20aligns%20with%20its%20SED%20property.%20We%20find%20that%20Spiking%20Neural%20Networks%20meet%20this%20requirement%2C%20as%20they%20communicate%20via%20sparse%20binary%20spikes%20and%20update%20in%20an%20event-driven%20manner%2C%20aligning%20naturally%20with%20the%20SED%20nature%20of%20IMTS.%20Therefore%2C%20we%20present%20SEDformer%2C%20an%20SED-enhanced%20Spiking%20Transformer%20for%20telemetry%20IMTS%20forecasting%20that%20couples%3A%20%281%29%20a%20SED-based%20Spike%20Encoder%20converts%20raw%20observations%20into%20event%20synchronous%20spikes%20using%20an%20Event-Aligned%20LIF%20neuron%2C%20%282%29%20an%20Event-Preserving%20Temporal%20Downsampling%20module%20compresses%20long%20gaps%20while%20retaining%20salient%20firings%20and%20%283%29%20a%20stack%20of%20SED-based%20Spike%20Transformer%20blocks%20enable%20intra-series%20dependency%20modeling%20with%20a%20membrane-based%20linear%20attention%20driven%20by%20EA-LIF%20spiking%20features.%20Experiments%20on%20public%20telemetry%20IMTS%20datasets%20show%20that%20SEDformer%20attains%20state-of-the-art%20forecasting%20accuracy%20while%20reducing%20energy%20and%20memory%20usage%2C%20providing%20a%20natural%20and%20efficient%20path%20for%20modeling%20IMTS.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEDformer%253A%2520Event-Synchronous%2520Spiking%2520Transformers%2520for%2520Irregular%2520Telemetry%2520Time%2520Series%2520Forecasting%26entry.906535625%3DZiyu%2520Zhou%2520and%2520Yuchen%2520Fang%2520and%2520Weilin%2520Ruan%2520and%2520Shiyu%2520Wang%2520and%2520James%2520Kwok%2520and%2520Yuxuan%2520Liang%26entry.1292438233%3DTelemetry%2520streams%2520from%2520large-scale%2520Internet-connected%2520systems%2520%2528e.g.%252C%2520IoT%2520deployments%2520and%2520online%2520platforms%2529%2520naturally%2520form%2520an%2520irregular%2520multivariate%2520time%2520series%2520%2528IMTS%2529%2520whose%2520accurate%2520forecasting%2520is%2520operationally%2520vital.%2520A%2520closer%2520examination%2520reveals%2520a%2520defining%2520Sparsity-Event%2520Duality%2520%2528SED%2529%2520property%2520of%2520IMTS%252C%2520i.e.%252C%2520long%2520stretches%2520with%2520sparse%2520or%2520no%2520observations%2520are%2520punctuated%2520by%2520short%252C%2520dense%2520bursts%2520where%2520most%2520semantic%2520events%2520%2528observations%2529%2520occur.%2520However%252C%2520existing%2520Graph-%2520and%2520Transformer-based%2520forecasters%2520ignore%2520SED%253A%2520pre-alignment%2520to%2520uniform%2520grids%2520with%2520heavy%2520padding%2520violates%2520sparsity%2520by%2520inflating%2520sequences%2520and%2520forcing%2520computation%2520at%2520non-informative%2520steps%252C%2520while%2520relational%2520recasting%2520weakens%2520event%2520semantics%2520by%2520disrupting%2520local%2520temporal%2520continuity.%2520These%2520limitations%2520motivate%2520a%2520more%2520faithful%2520and%2520natural%2520modeling%2520paradigm%2520for%2520IMTS%2520that%2520aligns%2520with%2520its%2520SED%2520property.%2520We%2520find%2520that%2520Spiking%2520Neural%2520Networks%2520meet%2520this%2520requirement%252C%2520as%2520they%2520communicate%2520via%2520sparse%2520binary%2520spikes%2520and%2520update%2520in%2520an%2520event-driven%2520manner%252C%2520aligning%2520naturally%2520with%2520the%2520SED%2520nature%2520of%2520IMTS.%2520Therefore%252C%2520we%2520present%2520SEDformer%252C%2520an%2520SED-enhanced%2520Spiking%2520Transformer%2520for%2520telemetry%2520IMTS%2520forecasting%2520that%2520couples%253A%2520%25281%2529%2520a%2520SED-based%2520Spike%2520Encoder%2520converts%2520raw%2520observations%2520into%2520event%2520synchronous%2520spikes%2520using%2520an%2520Event-Aligned%2520LIF%2520neuron%252C%2520%25282%2529%2520an%2520Event-Preserving%2520Temporal%2520Downsampling%2520module%2520compresses%2520long%2520gaps%2520while%2520retaining%2520salient%2520firings%2520and%2520%25283%2529%2520a%2520stack%2520of%2520SED-based%2520Spike%2520Transformer%2520blocks%2520enable%2520intra-series%2520dependency%2520modeling%2520with%2520a%2520membrane-based%2520linear%2520attention%2520driven%2520by%2520EA-LIF%2520spiking%2520features.%2520Experiments%2520on%2520public%2520telemetry%2520IMTS%2520datasets%2520show%2520that%2520SEDformer%2520attains%2520state-of-the-art%2520forecasting%2520accuracy%2520while%2520reducing%2520energy%2520and%2520memory%2520usage%252C%2520providing%2520a%2520natural%2520and%2520efficient%2520path%2520for%2520modeling%2520IMTS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEDformer%3A%20Event-Synchronous%20Spiking%20Transformers%20for%20Irregular%20Telemetry%20Time%20Series%20Forecasting&entry.906535625=Ziyu%20Zhou%20and%20Yuchen%20Fang%20and%20Weilin%20Ruan%20and%20Shiyu%20Wang%20and%20James%20Kwok%20and%20Yuxuan%20Liang&entry.1292438233=Telemetry%20streams%20from%20large-scale%20Internet-connected%20systems%20%28e.g.%2C%20IoT%20deployments%20and%20online%20platforms%29%20naturally%20form%20an%20irregular%20multivariate%20time%20series%20%28IMTS%29%20whose%20accurate%20forecasting%20is%20operationally%20vital.%20A%20closer%20examination%20reveals%20a%20defining%20Sparsity-Event%20Duality%20%28SED%29%20property%20of%20IMTS%2C%20i.e.%2C%20long%20stretches%20with%20sparse%20or%20no%20observations%20are%20punctuated%20by%20short%2C%20dense%20bursts%20where%20most%20semantic%20events%20%28observations%29%20occur.%20However%2C%20existing%20Graph-%20and%20Transformer-based%20forecasters%20ignore%20SED%3A%20pre-alignment%20to%20uniform%20grids%20with%20heavy%20padding%20violates%20sparsity%20by%20inflating%20sequences%20and%20forcing%20computation%20at%20non-informative%20steps%2C%20while%20relational%20recasting%20weakens%20event%20semantics%20by%20disrupting%20local%20temporal%20continuity.%20These%20limitations%20motivate%20a%20more%20faithful%20and%20natural%20modeling%20paradigm%20for%20IMTS%20that%20aligns%20with%20its%20SED%20property.%20We%20find%20that%20Spiking%20Neural%20Networks%20meet%20this%20requirement%2C%20as%20they%20communicate%20via%20sparse%20binary%20spikes%20and%20update%20in%20an%20event-driven%20manner%2C%20aligning%20naturally%20with%20the%20SED%20nature%20of%20IMTS.%20Therefore%2C%20we%20present%20SEDformer%2C%20an%20SED-enhanced%20Spiking%20Transformer%20for%20telemetry%20IMTS%20forecasting%20that%20couples%3A%20%281%29%20a%20SED-based%20Spike%20Encoder%20converts%20raw%20observations%20into%20event%20synchronous%20spikes%20using%20an%20Event-Aligned%20LIF%20neuron%2C%20%282%29%20an%20Event-Preserving%20Temporal%20Downsampling%20module%20compresses%20long%20gaps%20while%20retaining%20salient%20firings%20and%20%283%29%20a%20stack%20of%20SED-based%20Spike%20Transformer%20blocks%20enable%20intra-series%20dependency%20modeling%20with%20a%20membrane-based%20linear%20attention%20driven%20by%20EA-LIF%20spiking%20features.%20Experiments%20on%20public%20telemetry%20IMTS%20datasets%20show%20that%20SEDformer%20attains%20state-of-the-art%20forecasting%20accuracy%20while%20reducing%20energy%20and%20memory%20usage%2C%20providing%20a%20natural%20and%20efficient%20path%20for%20modeling%20IMTS.&entry.1838667208=http%3A//arxiv.org/abs/2602.02230v1&entry.124074799=Read"},
{"title": "Unsupervised Physics-Informed Operator Learning through Multi-Stage Curriculum Training", "author": "Paolo Marcandelli and Natansh Mathur and Stefano Markidis and Martina Siena and Stefano Mariani", "abstract": "Solving partial differential equations remains a central challenge in scientific machine learning. Neural operators offer a promising route by learning mappings between function spaces and enabling resolution-independent inference, yet they typically require supervised data. Physics-informed neural networks address this limitation through unsupervised training with physical constraints but often suffer from unstable convergence and limited generalization capability. To overcome these issues, we introduce a multi-stage physics-informed training strategy that achieves convergence by progressively enforcing boundary conditions in the loss landscape and subsequently incorporating interior residuals. At each stage the optimizer is re-initialized, acting as a continuation mechanism that restores stability and prevents gradient stagnation. We further propose the Physics-Informed Spline Fourier Neural Operator (PhIS-FNO), combining Fourier layers with Hermite spline kernels for smooth residual evaluation. Across canonical benchmarks, PhIS-FNO attains a level of accuracy comparable to that of supervised learning, using labeled information only along a narrow boundary region, establishing staged, spline-based optimization as a robust paradigm for physics-informed operator learning.", "link": "http://arxiv.org/abs/2602.02264v1", "date": "2026-02-02", "relevancy": 1.9511, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5166}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4911}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Physics-Informed%20Operator%20Learning%20through%20Multi-Stage%20Curriculum%20Training&body=Title%3A%20Unsupervised%20Physics-Informed%20Operator%20Learning%20through%20Multi-Stage%20Curriculum%20Training%0AAuthor%3A%20Paolo%20Marcandelli%20and%20Natansh%20Mathur%20and%20Stefano%20Markidis%20and%20Martina%20Siena%20and%20Stefano%20Mariani%0AAbstract%3A%20Solving%20partial%20differential%20equations%20remains%20a%20central%20challenge%20in%20scientific%20machine%20learning.%20Neural%20operators%20offer%20a%20promising%20route%20by%20learning%20mappings%20between%20function%20spaces%20and%20enabling%20resolution-independent%20inference%2C%20yet%20they%20typically%20require%20supervised%20data.%20Physics-informed%20neural%20networks%20address%20this%20limitation%20through%20unsupervised%20training%20with%20physical%20constraints%20but%20often%20suffer%20from%20unstable%20convergence%20and%20limited%20generalization%20capability.%20To%20overcome%20these%20issues%2C%20we%20introduce%20a%20multi-stage%20physics-informed%20training%20strategy%20that%20achieves%20convergence%20by%20progressively%20enforcing%20boundary%20conditions%20in%20the%20loss%20landscape%20and%20subsequently%20incorporating%20interior%20residuals.%20At%20each%20stage%20the%20optimizer%20is%20re-initialized%2C%20acting%20as%20a%20continuation%20mechanism%20that%20restores%20stability%20and%20prevents%20gradient%20stagnation.%20We%20further%20propose%20the%20Physics-Informed%20Spline%20Fourier%20Neural%20Operator%20%28PhIS-FNO%29%2C%20combining%20Fourier%20layers%20with%20Hermite%20spline%20kernels%20for%20smooth%20residual%20evaluation.%20Across%20canonical%20benchmarks%2C%20PhIS-FNO%20attains%20a%20level%20of%20accuracy%20comparable%20to%20that%20of%20supervised%20learning%2C%20using%20labeled%20information%20only%20along%20a%20narrow%20boundary%20region%2C%20establishing%20staged%2C%20spline-based%20optimization%20as%20a%20robust%20paradigm%20for%20physics-informed%20operator%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02264v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Physics-Informed%2520Operator%2520Learning%2520through%2520Multi-Stage%2520Curriculum%2520Training%26entry.906535625%3DPaolo%2520Marcandelli%2520and%2520Natansh%2520Mathur%2520and%2520Stefano%2520Markidis%2520and%2520Martina%2520Siena%2520and%2520Stefano%2520Mariani%26entry.1292438233%3DSolving%2520partial%2520differential%2520equations%2520remains%2520a%2520central%2520challenge%2520in%2520scientific%2520machine%2520learning.%2520Neural%2520operators%2520offer%2520a%2520promising%2520route%2520by%2520learning%2520mappings%2520between%2520function%2520spaces%2520and%2520enabling%2520resolution-independent%2520inference%252C%2520yet%2520they%2520typically%2520require%2520supervised%2520data.%2520Physics-informed%2520neural%2520networks%2520address%2520this%2520limitation%2520through%2520unsupervised%2520training%2520with%2520physical%2520constraints%2520but%2520often%2520suffer%2520from%2520unstable%2520convergence%2520and%2520limited%2520generalization%2520capability.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520introduce%2520a%2520multi-stage%2520physics-informed%2520training%2520strategy%2520that%2520achieves%2520convergence%2520by%2520progressively%2520enforcing%2520boundary%2520conditions%2520in%2520the%2520loss%2520landscape%2520and%2520subsequently%2520incorporating%2520interior%2520residuals.%2520At%2520each%2520stage%2520the%2520optimizer%2520is%2520re-initialized%252C%2520acting%2520as%2520a%2520continuation%2520mechanism%2520that%2520restores%2520stability%2520and%2520prevents%2520gradient%2520stagnation.%2520We%2520further%2520propose%2520the%2520Physics-Informed%2520Spline%2520Fourier%2520Neural%2520Operator%2520%2528PhIS-FNO%2529%252C%2520combining%2520Fourier%2520layers%2520with%2520Hermite%2520spline%2520kernels%2520for%2520smooth%2520residual%2520evaluation.%2520Across%2520canonical%2520benchmarks%252C%2520PhIS-FNO%2520attains%2520a%2520level%2520of%2520accuracy%2520comparable%2520to%2520that%2520of%2520supervised%2520learning%252C%2520using%2520labeled%2520information%2520only%2520along%2520a%2520narrow%2520boundary%2520region%252C%2520establishing%2520staged%252C%2520spline-based%2520optimization%2520as%2520a%2520robust%2520paradigm%2520for%2520physics-informed%2520operator%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02264v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Physics-Informed%20Operator%20Learning%20through%20Multi-Stage%20Curriculum%20Training&entry.906535625=Paolo%20Marcandelli%20and%20Natansh%20Mathur%20and%20Stefano%20Markidis%20and%20Martina%20Siena%20and%20Stefano%20Mariani&entry.1292438233=Solving%20partial%20differential%20equations%20remains%20a%20central%20challenge%20in%20scientific%20machine%20learning.%20Neural%20operators%20offer%20a%20promising%20route%20by%20learning%20mappings%20between%20function%20spaces%20and%20enabling%20resolution-independent%20inference%2C%20yet%20they%20typically%20require%20supervised%20data.%20Physics-informed%20neural%20networks%20address%20this%20limitation%20through%20unsupervised%20training%20with%20physical%20constraints%20but%20often%20suffer%20from%20unstable%20convergence%20and%20limited%20generalization%20capability.%20To%20overcome%20these%20issues%2C%20we%20introduce%20a%20multi-stage%20physics-informed%20training%20strategy%20that%20achieves%20convergence%20by%20progressively%20enforcing%20boundary%20conditions%20in%20the%20loss%20landscape%20and%20subsequently%20incorporating%20interior%20residuals.%20At%20each%20stage%20the%20optimizer%20is%20re-initialized%2C%20acting%20as%20a%20continuation%20mechanism%20that%20restores%20stability%20and%20prevents%20gradient%20stagnation.%20We%20further%20propose%20the%20Physics-Informed%20Spline%20Fourier%20Neural%20Operator%20%28PhIS-FNO%29%2C%20combining%20Fourier%20layers%20with%20Hermite%20spline%20kernels%20for%20smooth%20residual%20evaluation.%20Across%20canonical%20benchmarks%2C%20PhIS-FNO%20attains%20a%20level%20of%20accuracy%20comparable%20to%20that%20of%20supervised%20learning%2C%20using%20labeled%20information%20only%20along%20a%20narrow%20boundary%20region%2C%20establishing%20staged%2C%20spline-based%20optimization%20as%20a%20robust%20paradigm%20for%20physics-informed%20operator%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2602.02264v1&entry.124074799=Read"},
{"title": "Statistical Learning Theory in Lean 4: Empirical Processes from Scratch", "author": "Yuanhe Zhang and Jason D. Lee and Fanghui Liu", "abstract": "We present the first comprehensive Lean 4 formalization of statistical learning theory (SLT) grounded in empirical process theory. Our end-to-end formal infrastructure implement the missing contents in latest Lean 4 Mathlib library, including a complete development of Gaussian Lipschitz concentration, the first formalization of Dudley's entropy integral theorem for sub-Gaussian processes, and an application to least-squares (sparse) regression with a sharp rate. The project was carried out using a human-AI collaborative workflow, in which humans design proof strategies and AI agents execute tactical proof construction, leading to the human-verified Lean 4 toolbox for SLT. Beyond implementation, the formalization process exposes and resolves implicit assumptions and missing details in standard SLT textbooks, enforcing a granular, line-by-line understanding of the theory. This work establishes a reusable formal foundation and opens the door for future developments in machine learning theory. The code is available at https://github.com/YuanheZ/lean-stat-learning-theory", "link": "http://arxiv.org/abs/2602.02285v1", "date": "2026-02-02", "relevancy": 1.7751, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4604}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4445}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Statistical%20Learning%20Theory%20in%20Lean%204%3A%20Empirical%20Processes%20from%20Scratch&body=Title%3A%20Statistical%20Learning%20Theory%20in%20Lean%204%3A%20Empirical%20Processes%20from%20Scratch%0AAuthor%3A%20Yuanhe%20Zhang%20and%20Jason%20D.%20Lee%20and%20Fanghui%20Liu%0AAbstract%3A%20We%20present%20the%20first%20comprehensive%20Lean%204%20formalization%20of%20statistical%20learning%20theory%20%28SLT%29%20grounded%20in%20empirical%20process%20theory.%20Our%20end-to-end%20formal%20infrastructure%20implement%20the%20missing%20contents%20in%20latest%20Lean%204%20Mathlib%20library%2C%20including%20a%20complete%20development%20of%20Gaussian%20Lipschitz%20concentration%2C%20the%20first%20formalization%20of%20Dudley%27s%20entropy%20integral%20theorem%20for%20sub-Gaussian%20processes%2C%20and%20an%20application%20to%20least-squares%20%28sparse%29%20regression%20with%20a%20sharp%20rate.%20The%20project%20was%20carried%20out%20using%20a%20human-AI%20collaborative%20workflow%2C%20in%20which%20humans%20design%20proof%20strategies%20and%20AI%20agents%20execute%20tactical%20proof%20construction%2C%20leading%20to%20the%20human-verified%20Lean%204%20toolbox%20for%20SLT.%20Beyond%20implementation%2C%20the%20formalization%20process%20exposes%20and%20resolves%20implicit%20assumptions%20and%20missing%20details%20in%20standard%20SLT%20textbooks%2C%20enforcing%20a%20granular%2C%20line-by-line%20understanding%20of%20the%20theory.%20This%20work%20establishes%20a%20reusable%20formal%20foundation%20and%20opens%20the%20door%20for%20future%20developments%20in%20machine%20learning%20theory.%20The%20code%20is%20available%20at%20https%3A//github.com/YuanheZ/lean-stat-learning-theory%0ALink%3A%20http%3A//arxiv.org/abs/2602.02285v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStatistical%2520Learning%2520Theory%2520in%2520Lean%25204%253A%2520Empirical%2520Processes%2520from%2520Scratch%26entry.906535625%3DYuanhe%2520Zhang%2520and%2520Jason%2520D.%2520Lee%2520and%2520Fanghui%2520Liu%26entry.1292438233%3DWe%2520present%2520the%2520first%2520comprehensive%2520Lean%25204%2520formalization%2520of%2520statistical%2520learning%2520theory%2520%2528SLT%2529%2520grounded%2520in%2520empirical%2520process%2520theory.%2520Our%2520end-to-end%2520formal%2520infrastructure%2520implement%2520the%2520missing%2520contents%2520in%2520latest%2520Lean%25204%2520Mathlib%2520library%252C%2520including%2520a%2520complete%2520development%2520of%2520Gaussian%2520Lipschitz%2520concentration%252C%2520the%2520first%2520formalization%2520of%2520Dudley%2527s%2520entropy%2520integral%2520theorem%2520for%2520sub-Gaussian%2520processes%252C%2520and%2520an%2520application%2520to%2520least-squares%2520%2528sparse%2529%2520regression%2520with%2520a%2520sharp%2520rate.%2520The%2520project%2520was%2520carried%2520out%2520using%2520a%2520human-AI%2520collaborative%2520workflow%252C%2520in%2520which%2520humans%2520design%2520proof%2520strategies%2520and%2520AI%2520agents%2520execute%2520tactical%2520proof%2520construction%252C%2520leading%2520to%2520the%2520human-verified%2520Lean%25204%2520toolbox%2520for%2520SLT.%2520Beyond%2520implementation%252C%2520the%2520formalization%2520process%2520exposes%2520and%2520resolves%2520implicit%2520assumptions%2520and%2520missing%2520details%2520in%2520standard%2520SLT%2520textbooks%252C%2520enforcing%2520a%2520granular%252C%2520line-by-line%2520understanding%2520of%2520the%2520theory.%2520This%2520work%2520establishes%2520a%2520reusable%2520formal%2520foundation%2520and%2520opens%2520the%2520door%2520for%2520future%2520developments%2520in%2520machine%2520learning%2520theory.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/YuanheZ/lean-stat-learning-theory%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02285v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Statistical%20Learning%20Theory%20in%20Lean%204%3A%20Empirical%20Processes%20from%20Scratch&entry.906535625=Yuanhe%20Zhang%20and%20Jason%20D.%20Lee%20and%20Fanghui%20Liu&entry.1292438233=We%20present%20the%20first%20comprehensive%20Lean%204%20formalization%20of%20statistical%20learning%20theory%20%28SLT%29%20grounded%20in%20empirical%20process%20theory.%20Our%20end-to-end%20formal%20infrastructure%20implement%20the%20missing%20contents%20in%20latest%20Lean%204%20Mathlib%20library%2C%20including%20a%20complete%20development%20of%20Gaussian%20Lipschitz%20concentration%2C%20the%20first%20formalization%20of%20Dudley%27s%20entropy%20integral%20theorem%20for%20sub-Gaussian%20processes%2C%20and%20an%20application%20to%20least-squares%20%28sparse%29%20regression%20with%20a%20sharp%20rate.%20The%20project%20was%20carried%20out%20using%20a%20human-AI%20collaborative%20workflow%2C%20in%20which%20humans%20design%20proof%20strategies%20and%20AI%20agents%20execute%20tactical%20proof%20construction%2C%20leading%20to%20the%20human-verified%20Lean%204%20toolbox%20for%20SLT.%20Beyond%20implementation%2C%20the%20formalization%20process%20exposes%20and%20resolves%20implicit%20assumptions%20and%20missing%20details%20in%20standard%20SLT%20textbooks%2C%20enforcing%20a%20granular%2C%20line-by-line%20understanding%20of%20the%20theory.%20This%20work%20establishes%20a%20reusable%20formal%20foundation%20and%20opens%20the%20door%20for%20future%20developments%20in%20machine%20learning%20theory.%20The%20code%20is%20available%20at%20https%3A//github.com/YuanheZ/lean-stat-learning-theory&entry.1838667208=http%3A//arxiv.org/abs/2602.02285v1&entry.124074799=Read"},
{"title": "Well-Posed KL-Regularized Control via Wasserstein and Kalman-Wasserstein KL Divergences", "author": "Viktor Stein and Adwait Datar and Nihat Ay", "abstract": "Kullback-Leibler divergence (KL) regularization is widely used in reinforcement learning, but it becomes infinite under support mismatch and can degenerate in low-noise limits. Utilizing a unified information-geometric framework, we introduce (Kalman)-Wasserstein-based KL analogues by replacing the Fisher-Rao geometry in the dynamical formulation of the KL with transport-based geometries, and we derive closed-form values for common distribution families. These divergences remain finite under support mismatch and yield a geometric interpretation of regularization heuristics used in Kalman ensemble methods. We demonstrate the utility of these divergences in KL-regularized optimal control. In the fully tractable setting of linear time-invariant systems with Gaussian process noise, the classical KL reduces to a quadratic control penalty that becomes singular as process noise vanishes. Our variants remove this singularity, yielding well-posed problems. On a double integrator and a cart-pole example, the resulting controls outperform KL-based regularization.", "link": "http://arxiv.org/abs/2602.02250v1", "date": "2026-02-02", "relevancy": 1.363, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4568}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4566}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Well-Posed%20KL-Regularized%20Control%20via%20Wasserstein%20and%20Kalman-Wasserstein%20KL%20Divergences&body=Title%3A%20Well-Posed%20KL-Regularized%20Control%20via%20Wasserstein%20and%20Kalman-Wasserstein%20KL%20Divergences%0AAuthor%3A%20Viktor%20Stein%20and%20Adwait%20Datar%20and%20Nihat%20Ay%0AAbstract%3A%20Kullback-Leibler%20divergence%20%28KL%29%20regularization%20is%20widely%20used%20in%20reinforcement%20learning%2C%20but%20it%20becomes%20infinite%20under%20support%20mismatch%20and%20can%20degenerate%20in%20low-noise%20limits.%20Utilizing%20a%20unified%20information-geometric%20framework%2C%20we%20introduce%20%28Kalman%29-Wasserstein-based%20KL%20analogues%20by%20replacing%20the%20Fisher-Rao%20geometry%20in%20the%20dynamical%20formulation%20of%20the%20KL%20with%20transport-based%20geometries%2C%20and%20we%20derive%20closed-form%20values%20for%20common%20distribution%20families.%20These%20divergences%20remain%20finite%20under%20support%20mismatch%20and%20yield%20a%20geometric%20interpretation%20of%20regularization%20heuristics%20used%20in%20Kalman%20ensemble%20methods.%20We%20demonstrate%20the%20utility%20of%20these%20divergences%20in%20KL-regularized%20optimal%20control.%20In%20the%20fully%20tractable%20setting%20of%20linear%20time-invariant%20systems%20with%20Gaussian%20process%20noise%2C%20the%20classical%20KL%20reduces%20to%20a%20quadratic%20control%20penalty%20that%20becomes%20singular%20as%20process%20noise%20vanishes.%20Our%20variants%20remove%20this%20singularity%2C%20yielding%20well-posed%20problems.%20On%20a%20double%20integrator%20and%20a%20cart-pole%20example%2C%20the%20resulting%20controls%20outperform%20KL-based%20regularization.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02250v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWell-Posed%2520KL-Regularized%2520Control%2520via%2520Wasserstein%2520and%2520Kalman-Wasserstein%2520KL%2520Divergences%26entry.906535625%3DViktor%2520Stein%2520and%2520Adwait%2520Datar%2520and%2520Nihat%2520Ay%26entry.1292438233%3DKullback-Leibler%2520divergence%2520%2528KL%2529%2520regularization%2520is%2520widely%2520used%2520in%2520reinforcement%2520learning%252C%2520but%2520it%2520becomes%2520infinite%2520under%2520support%2520mismatch%2520and%2520can%2520degenerate%2520in%2520low-noise%2520limits.%2520Utilizing%2520a%2520unified%2520information-geometric%2520framework%252C%2520we%2520introduce%2520%2528Kalman%2529-Wasserstein-based%2520KL%2520analogues%2520by%2520replacing%2520the%2520Fisher-Rao%2520geometry%2520in%2520the%2520dynamical%2520formulation%2520of%2520the%2520KL%2520with%2520transport-based%2520geometries%252C%2520and%2520we%2520derive%2520closed-form%2520values%2520for%2520common%2520distribution%2520families.%2520These%2520divergences%2520remain%2520finite%2520under%2520support%2520mismatch%2520and%2520yield%2520a%2520geometric%2520interpretation%2520of%2520regularization%2520heuristics%2520used%2520in%2520Kalman%2520ensemble%2520methods.%2520We%2520demonstrate%2520the%2520utility%2520of%2520these%2520divergences%2520in%2520KL-regularized%2520optimal%2520control.%2520In%2520the%2520fully%2520tractable%2520setting%2520of%2520linear%2520time-invariant%2520systems%2520with%2520Gaussian%2520process%2520noise%252C%2520the%2520classical%2520KL%2520reduces%2520to%2520a%2520quadratic%2520control%2520penalty%2520that%2520becomes%2520singular%2520as%2520process%2520noise%2520vanishes.%2520Our%2520variants%2520remove%2520this%2520singularity%252C%2520yielding%2520well-posed%2520problems.%2520On%2520a%2520double%2520integrator%2520and%2520a%2520cart-pole%2520example%252C%2520the%2520resulting%2520controls%2520outperform%2520KL-based%2520regularization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02250v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Well-Posed%20KL-Regularized%20Control%20via%20Wasserstein%20and%20Kalman-Wasserstein%20KL%20Divergences&entry.906535625=Viktor%20Stein%20and%20Adwait%20Datar%20and%20Nihat%20Ay&entry.1292438233=Kullback-Leibler%20divergence%20%28KL%29%20regularization%20is%20widely%20used%20in%20reinforcement%20learning%2C%20but%20it%20becomes%20infinite%20under%20support%20mismatch%20and%20can%20degenerate%20in%20low-noise%20limits.%20Utilizing%20a%20unified%20information-geometric%20framework%2C%20we%20introduce%20%28Kalman%29-Wasserstein-based%20KL%20analogues%20by%20replacing%20the%20Fisher-Rao%20geometry%20in%20the%20dynamical%20formulation%20of%20the%20KL%20with%20transport-based%20geometries%2C%20and%20we%20derive%20closed-form%20values%20for%20common%20distribution%20families.%20These%20divergences%20remain%20finite%20under%20support%20mismatch%20and%20yield%20a%20geometric%20interpretation%20of%20regularization%20heuristics%20used%20in%20Kalman%20ensemble%20methods.%20We%20demonstrate%20the%20utility%20of%20these%20divergences%20in%20KL-regularized%20optimal%20control.%20In%20the%20fully%20tractable%20setting%20of%20linear%20time-invariant%20systems%20with%20Gaussian%20process%20noise%2C%20the%20classical%20KL%20reduces%20to%20a%20quadratic%20control%20penalty%20that%20becomes%20singular%20as%20process%20noise%20vanishes.%20Our%20variants%20remove%20this%20singularity%2C%20yielding%20well-posed%20problems.%20On%20a%20double%20integrator%20and%20a%20cart-pole%20example%2C%20the%20resulting%20controls%20outperform%20KL-based%20regularization.&entry.1838667208=http%3A//arxiv.org/abs/2602.02250v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


