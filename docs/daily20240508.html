<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240507.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Switchable Decision: Dynamic Neural Generation Networks", "author": "Shujian Zhang and Korawat Tanwisuth and Chengyue Gong and Pengcheng He and Mingyuan Zhou", "abstract": "  Auto-regressive generation models achieve competitive performance across many\ndifferent NLP tasks such as summarization, question answering, and\nclassifications. However, they are also known for being slow in inference,\nwhich makes them challenging to deploy in real-time applications. We propose a\nswitchable decision to accelerate inference by dynamically assigning\ncomputation resources for each data instance. Automatically making decisions on\nwhere to skip and how to balance quality and computation cost with constrained\noptimization, our dynamic neural generation networks enforce the efficient\ninference path and determine the optimized trade-off. Experiments across\nquestion answering, summarization, and classification benchmarks show that our\nmethod benefits from less computation cost during inference while keeping the\nsame accuracy. Extensive experiments and ablation studies demonstrate that our\nmethod can be general, effective, and beneficial for many NLP tasks.\n", "link": "http://arxiv.org/abs/2405.04513v1", "date": "2024-05-07", "relevancy": 2.8742, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6382}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5466}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Switchable%20Decision%3A%20Dynamic%20Neural%20Generation%20Networks&body=Title%3A%20Switchable%20Decision%3A%20Dynamic%20Neural%20Generation%20Networks%0AAuthor%3A%20Shujian%20Zhang%20and%20Korawat%20Tanwisuth%20and%20Chengyue%20Gong%20and%20Pengcheng%20He%20and%20Mingyuan%20Zhou%0AAbstract%3A%20%20%20Auto-regressive%20generation%20models%20achieve%20competitive%20performance%20across%20many%0Adifferent%20NLP%20tasks%20such%20as%20summarization%2C%20question%20answering%2C%20and%0Aclassifications.%20However%2C%20they%20are%20also%20known%20for%20being%20slow%20in%20inference%2C%0Awhich%20makes%20them%20challenging%20to%20deploy%20in%20real-time%20applications.%20We%20propose%20a%0Aswitchable%20decision%20to%20accelerate%20inference%20by%20dynamically%20assigning%0Acomputation%20resources%20for%20each%20data%20instance.%20Automatically%20making%20decisions%20on%0Awhere%20to%20skip%20and%20how%20to%20balance%20quality%20and%20computation%20cost%20with%20constrained%0Aoptimization%2C%20our%20dynamic%20neural%20generation%20networks%20enforce%20the%20efficient%0Ainference%20path%20and%20determine%20the%20optimized%20trade-off.%20Experiments%20across%0Aquestion%20answering%2C%20summarization%2C%20and%20classification%20benchmarks%20show%20that%20our%0Amethod%20benefits%20from%20less%20computation%20cost%20during%20inference%20while%20keeping%20the%0Asame%20accuracy.%20Extensive%20experiments%20and%20ablation%20studies%20demonstrate%20that%20our%0Amethod%20can%20be%20general%2C%20effective%2C%20and%20beneficial%20for%20many%20NLP%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04513v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSwitchable%2520Decision%253A%2520Dynamic%2520Neural%2520Generation%2520Networks%26entry.906535625%3DShujian%2520Zhang%2520and%2520Korawat%2520Tanwisuth%2520and%2520Chengyue%2520Gong%2520and%2520Pengcheng%2520He%2520and%2520Mingyuan%2520Zhou%26entry.1292438233%3D%2520%2520Auto-regressive%2520generation%2520models%2520achieve%2520competitive%2520performance%2520across%2520many%250Adifferent%2520NLP%2520tasks%2520such%2520as%2520summarization%252C%2520question%2520answering%252C%2520and%250Aclassifications.%2520However%252C%2520they%2520are%2520also%2520known%2520for%2520being%2520slow%2520in%2520inference%252C%250Awhich%2520makes%2520them%2520challenging%2520to%2520deploy%2520in%2520real-time%2520applications.%2520We%2520propose%2520a%250Aswitchable%2520decision%2520to%2520accelerate%2520inference%2520by%2520dynamically%2520assigning%250Acomputation%2520resources%2520for%2520each%2520data%2520instance.%2520Automatically%2520making%2520decisions%2520on%250Awhere%2520to%2520skip%2520and%2520how%2520to%2520balance%2520quality%2520and%2520computation%2520cost%2520with%2520constrained%250Aoptimization%252C%2520our%2520dynamic%2520neural%2520generation%2520networks%2520enforce%2520the%2520efficient%250Ainference%2520path%2520and%2520determine%2520the%2520optimized%2520trade-off.%2520Experiments%2520across%250Aquestion%2520answering%252C%2520summarization%252C%2520and%2520classification%2520benchmarks%2520show%2520that%2520our%250Amethod%2520benefits%2520from%2520less%2520computation%2520cost%2520during%2520inference%2520while%2520keeping%2520the%250Asame%2520accuracy.%2520Extensive%2520experiments%2520and%2520ablation%2520studies%2520demonstrate%2520that%2520our%250Amethod%2520can%2520be%2520general%252C%2520effective%252C%2520and%2520beneficial%2520for%2520many%2520NLP%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04513v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Switchable%20Decision%3A%20Dynamic%20Neural%20Generation%20Networks&entry.906535625=Shujian%20Zhang%20and%20Korawat%20Tanwisuth%20and%20Chengyue%20Gong%20and%20Pengcheng%20He%20and%20Mingyuan%20Zhou&entry.1292438233=%20%20Auto-regressive%20generation%20models%20achieve%20competitive%20performance%20across%20many%0Adifferent%20NLP%20tasks%20such%20as%20summarization%2C%20question%20answering%2C%20and%0Aclassifications.%20However%2C%20they%20are%20also%20known%20for%20being%20slow%20in%20inference%2C%0Awhich%20makes%20them%20challenging%20to%20deploy%20in%20real-time%20applications.%20We%20propose%20a%0Aswitchable%20decision%20to%20accelerate%20inference%20by%20dynamically%20assigning%0Acomputation%20resources%20for%20each%20data%20instance.%20Automatically%20making%20decisions%20on%0Awhere%20to%20skip%20and%20how%20to%20balance%20quality%20and%20computation%20cost%20with%20constrained%0Aoptimization%2C%20our%20dynamic%20neural%20generation%20networks%20enforce%20the%20efficient%0Ainference%20path%20and%20determine%20the%20optimized%20trade-off.%20Experiments%20across%0Aquestion%20answering%2C%20summarization%2C%20and%20classification%20benchmarks%20show%20that%20our%0Amethod%20benefits%20from%20less%20computation%20cost%20during%20inference%20while%20keeping%20the%0Asame%20accuracy.%20Extensive%20experiments%20and%20ablation%20studies%20demonstrate%20that%20our%0Amethod%20can%20be%20general%2C%20effective%2C%20and%20beneficial%20for%20many%20NLP%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04513v1&entry.124074799=Read"},
{"title": "Non-rigid Structure-from-Motion: Temporally-smooth Procrustean Alignment\n  and Spatially-variant Deformation Modeling", "author": "Jiawei Shi and Hui Deng and Yuchao Dai", "abstract": "  Even though Non-rigid Structure-from-Motion (NRSfM) has been extensively\nstudied and great progress has been made, there are still key challenges that\nhinder their broad real-world applications: 1) the inherent motion/rotation\nambiguity requires either explicit camera motion recovery with extra constraint\nor complex Procrustean Alignment; 2) existing low-rank modeling of the global\nshape can over-penalize drastic deformations in the 3D shape sequence. This\npaper proposes to resolve the above issues from a spatial-temporal modeling\nperspective. First, we propose a novel Temporally-smooth Procrustean Alignment\nmodule that estimates 3D deforming shapes and adjusts the camera motion by\naligning the 3D shape sequence consecutively. Our new alignment module remedies\nthe requirement of complex reference 3D shape during alignment, which is more\nconductive to non-isotropic deformation modeling. Second, we propose a\nspatial-weighted approach to enforce the low-rank constraint adaptively at\ndifferent locations to accommodate drastic spatially-variant deformation\nreconstruction better. Our modeling outperform existing low-rank based methods,\nand extensive experiments across different datasets validate the effectiveness\nof our method.\n", "link": "http://arxiv.org/abs/2405.04309v1", "date": "2024-05-07", "relevancy": 2.8061, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5803}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5536}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-rigid%20Structure-from-Motion%3A%20Temporally-smooth%20Procrustean%20Alignment%0A%20%20and%20Spatially-variant%20Deformation%20Modeling&body=Title%3A%20Non-rigid%20Structure-from-Motion%3A%20Temporally-smooth%20Procrustean%20Alignment%0A%20%20and%20Spatially-variant%20Deformation%20Modeling%0AAuthor%3A%20Jiawei%20Shi%20and%20Hui%20Deng%20and%20Yuchao%20Dai%0AAbstract%3A%20%20%20Even%20though%20Non-rigid%20Structure-from-Motion%20%28NRSfM%29%20has%20been%20extensively%0Astudied%20and%20great%20progress%20has%20been%20made%2C%20there%20are%20still%20key%20challenges%20that%0Ahinder%20their%20broad%20real-world%20applications%3A%201%29%20the%20inherent%20motion/rotation%0Aambiguity%20requires%20either%20explicit%20camera%20motion%20recovery%20with%20extra%20constraint%0Aor%20complex%20Procrustean%20Alignment%3B%202%29%20existing%20low-rank%20modeling%20of%20the%20global%0Ashape%20can%20over-penalize%20drastic%20deformations%20in%20the%203D%20shape%20sequence.%20This%0Apaper%20proposes%20to%20resolve%20the%20above%20issues%20from%20a%20spatial-temporal%20modeling%0Aperspective.%20First%2C%20we%20propose%20a%20novel%20Temporally-smooth%20Procrustean%20Alignment%0Amodule%20that%20estimates%203D%20deforming%20shapes%20and%20adjusts%20the%20camera%20motion%20by%0Aaligning%20the%203D%20shape%20sequence%20consecutively.%20Our%20new%20alignment%20module%20remedies%0Athe%20requirement%20of%20complex%20reference%203D%20shape%20during%20alignment%2C%20which%20is%20more%0Aconductive%20to%20non-isotropic%20deformation%20modeling.%20Second%2C%20we%20propose%20a%0Aspatial-weighted%20approach%20to%20enforce%20the%20low-rank%20constraint%20adaptively%20at%0Adifferent%20locations%20to%20accommodate%20drastic%20spatially-variant%20deformation%0Areconstruction%20better.%20Our%20modeling%20outperform%20existing%20low-rank%20based%20methods%2C%0Aand%20extensive%20experiments%20across%20different%20datasets%20validate%20the%20effectiveness%0Aof%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-rigid%2520Structure-from-Motion%253A%2520Temporally-smooth%2520Procrustean%2520Alignment%250A%2520%2520and%2520Spatially-variant%2520Deformation%2520Modeling%26entry.906535625%3DJiawei%2520Shi%2520and%2520Hui%2520Deng%2520and%2520Yuchao%2520Dai%26entry.1292438233%3D%2520%2520Even%2520though%2520Non-rigid%2520Structure-from-Motion%2520%2528NRSfM%2529%2520has%2520been%2520extensively%250Astudied%2520and%2520great%2520progress%2520has%2520been%2520made%252C%2520there%2520are%2520still%2520key%2520challenges%2520that%250Ahinder%2520their%2520broad%2520real-world%2520applications%253A%25201%2529%2520the%2520inherent%2520motion/rotation%250Aambiguity%2520requires%2520either%2520explicit%2520camera%2520motion%2520recovery%2520with%2520extra%2520constraint%250Aor%2520complex%2520Procrustean%2520Alignment%253B%25202%2529%2520existing%2520low-rank%2520modeling%2520of%2520the%2520global%250Ashape%2520can%2520over-penalize%2520drastic%2520deformations%2520in%2520the%25203D%2520shape%2520sequence.%2520This%250Apaper%2520proposes%2520to%2520resolve%2520the%2520above%2520issues%2520from%2520a%2520spatial-temporal%2520modeling%250Aperspective.%2520First%252C%2520we%2520propose%2520a%2520novel%2520Temporally-smooth%2520Procrustean%2520Alignment%250Amodule%2520that%2520estimates%25203D%2520deforming%2520shapes%2520and%2520adjusts%2520the%2520camera%2520motion%2520by%250Aaligning%2520the%25203D%2520shape%2520sequence%2520consecutively.%2520Our%2520new%2520alignment%2520module%2520remedies%250Athe%2520requirement%2520of%2520complex%2520reference%25203D%2520shape%2520during%2520alignment%252C%2520which%2520is%2520more%250Aconductive%2520to%2520non-isotropic%2520deformation%2520modeling.%2520Second%252C%2520we%2520propose%2520a%250Aspatial-weighted%2520approach%2520to%2520enforce%2520the%2520low-rank%2520constraint%2520adaptively%2520at%250Adifferent%2520locations%2520to%2520accommodate%2520drastic%2520spatially-variant%2520deformation%250Areconstruction%2520better.%2520Our%2520modeling%2520outperform%2520existing%2520low-rank%2520based%2520methods%252C%250Aand%2520extensive%2520experiments%2520across%2520different%2520datasets%2520validate%2520the%2520effectiveness%250Aof%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-rigid%20Structure-from-Motion%3A%20Temporally-smooth%20Procrustean%20Alignment%0A%20%20and%20Spatially-variant%20Deformation%20Modeling&entry.906535625=Jiawei%20Shi%20and%20Hui%20Deng%20and%20Yuchao%20Dai&entry.1292438233=%20%20Even%20though%20Non-rigid%20Structure-from-Motion%20%28NRSfM%29%20has%20been%20extensively%0Astudied%20and%20great%20progress%20has%20been%20made%2C%20there%20are%20still%20key%20challenges%20that%0Ahinder%20their%20broad%20real-world%20applications%3A%201%29%20the%20inherent%20motion/rotation%0Aambiguity%20requires%20either%20explicit%20camera%20motion%20recovery%20with%20extra%20constraint%0Aor%20complex%20Procrustean%20Alignment%3B%202%29%20existing%20low-rank%20modeling%20of%20the%20global%0Ashape%20can%20over-penalize%20drastic%20deformations%20in%20the%203D%20shape%20sequence.%20This%0Apaper%20proposes%20to%20resolve%20the%20above%20issues%20from%20a%20spatial-temporal%20modeling%0Aperspective.%20First%2C%20we%20propose%20a%20novel%20Temporally-smooth%20Procrustean%20Alignment%0Amodule%20that%20estimates%203D%20deforming%20shapes%20and%20adjusts%20the%20camera%20motion%20by%0Aaligning%20the%203D%20shape%20sequence%20consecutively.%20Our%20new%20alignment%20module%20remedies%0Athe%20requirement%20of%20complex%20reference%203D%20shape%20during%20alignment%2C%20which%20is%20more%0Aconductive%20to%20non-isotropic%20deformation%20modeling.%20Second%2C%20we%20propose%20a%0Aspatial-weighted%20approach%20to%20enforce%20the%20low-rank%20constraint%20adaptively%20at%0Adifferent%20locations%20to%20accommodate%20drastic%20spatially-variant%20deformation%0Areconstruction%20better.%20Our%20modeling%20outperform%20existing%20low-rank%20based%20methods%2C%0Aand%20extensive%20experiments%20across%20different%20datasets%20validate%20the%20effectiveness%0Aof%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04309v1&entry.124074799=Read"},
{"title": "DriveWorld: 4D Pre-trained Scene Understanding via World Models for\n  Autonomous Driving", "author": "Chen Min and Dawei Zhao and Liang Xiao and Jian Zhao and Xinli Xu and Zheng Zhu and Lei Jin and Jianshu Li and Yulan Guo and Junliang Xing and Liping Jing and Yiming Nie and Bin Dai", "abstract": "  Vision-centric autonomous driving has recently raised wide attention due to\nits lower cost. Pre-training is essential for extracting a universal\nrepresentation. However, current vision-centric pre-training typically relies\non either 2D or 3D pre-text tasks, overlooking the temporal characteristics of\nautonomous driving as a 4D scene understanding task. In this paper, we address\nthis challenge by introducing a world model-based autonomous driving 4D\nrepresentation learning framework, dubbed \\emph{DriveWorld}, which is capable\nof pre-training from multi-camera driving videos in a spatio-temporal fashion.\nSpecifically, we propose a Memory State-Space Model for spatio-temporal\nmodelling, which consists of a Dynamic Memory Bank module for learning\ntemporal-aware latent dynamics to predict future changes and a Static Scene\nPropagation module for learning spatial-aware latent statics to offer\ncomprehensive scene contexts. We additionally introduce a Task Prompt to\ndecouple task-aware features for various downstream tasks. The experiments\ndemonstrate that DriveWorld delivers promising results on various autonomous\ndriving tasks. When pre-trained with the OpenScene dataset, DriveWorld achieves\na 7.5% increase in mAP for 3D object detection, a 3.0% increase in IoU for\nonline mapping, a 5.0% increase in AMOTA for multi-object tracking, a 0.1m\ndecrease in minADE for motion forecasting, a 3.0% increase in IoU for occupancy\nprediction, and a 0.34m reduction in average L2 error for planning.\n", "link": "http://arxiv.org/abs/2405.04390v1", "date": "2024-05-07", "relevancy": 2.7691, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5657}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5501}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DriveWorld%3A%204D%20Pre-trained%20Scene%20Understanding%20via%20World%20Models%20for%0A%20%20Autonomous%20Driving&body=Title%3A%20DriveWorld%3A%204D%20Pre-trained%20Scene%20Understanding%20via%20World%20Models%20for%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Chen%20Min%20and%20Dawei%20Zhao%20and%20Liang%20Xiao%20and%20Jian%20Zhao%20and%20Xinli%20Xu%20and%20Zheng%20Zhu%20and%20Lei%20Jin%20and%20Jianshu%20Li%20and%20Yulan%20Guo%20and%20Junliang%20Xing%20and%20Liping%20Jing%20and%20Yiming%20Nie%20and%20Bin%20Dai%0AAbstract%3A%20%20%20Vision-centric%20autonomous%20driving%20has%20recently%20raised%20wide%20attention%20due%20to%0Aits%20lower%20cost.%20Pre-training%20is%20essential%20for%20extracting%20a%20universal%0Arepresentation.%20However%2C%20current%20vision-centric%20pre-training%20typically%20relies%0Aon%20either%202D%20or%203D%20pre-text%20tasks%2C%20overlooking%20the%20temporal%20characteristics%20of%0Aautonomous%20driving%20as%20a%204D%20scene%20understanding%20task.%20In%20this%20paper%2C%20we%20address%0Athis%20challenge%20by%20introducing%20a%20world%20model-based%20autonomous%20driving%204D%0Arepresentation%20learning%20framework%2C%20dubbed%20%5Cemph%7BDriveWorld%7D%2C%20which%20is%20capable%0Aof%20pre-training%20from%20multi-camera%20driving%20videos%20in%20a%20spatio-temporal%20fashion.%0ASpecifically%2C%20we%20propose%20a%20Memory%20State-Space%20Model%20for%20spatio-temporal%0Amodelling%2C%20which%20consists%20of%20a%20Dynamic%20Memory%20Bank%20module%20for%20learning%0Atemporal-aware%20latent%20dynamics%20to%20predict%20future%20changes%20and%20a%20Static%20Scene%0APropagation%20module%20for%20learning%20spatial-aware%20latent%20statics%20to%20offer%0Acomprehensive%20scene%20contexts.%20We%20additionally%20introduce%20a%20Task%20Prompt%20to%0Adecouple%20task-aware%20features%20for%20various%20downstream%20tasks.%20The%20experiments%0Ademonstrate%20that%20DriveWorld%20delivers%20promising%20results%20on%20various%20autonomous%0Adriving%20tasks.%20When%20pre-trained%20with%20the%20OpenScene%20dataset%2C%20DriveWorld%20achieves%0Aa%207.5%25%20increase%20in%20mAP%20for%203D%20object%20detection%2C%20a%203.0%25%20increase%20in%20IoU%20for%0Aonline%20mapping%2C%20a%205.0%25%20increase%20in%20AMOTA%20for%20multi-object%20tracking%2C%20a%200.1m%0Adecrease%20in%20minADE%20for%20motion%20forecasting%2C%20a%203.0%25%20increase%20in%20IoU%20for%20occupancy%0Aprediction%2C%20and%20a%200.34m%20reduction%20in%20average%20L2%20error%20for%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriveWorld%253A%25204D%2520Pre-trained%2520Scene%2520Understanding%2520via%2520World%2520Models%2520for%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DChen%2520Min%2520and%2520Dawei%2520Zhao%2520and%2520Liang%2520Xiao%2520and%2520Jian%2520Zhao%2520and%2520Xinli%2520Xu%2520and%2520Zheng%2520Zhu%2520and%2520Lei%2520Jin%2520and%2520Jianshu%2520Li%2520and%2520Yulan%2520Guo%2520and%2520Junliang%2520Xing%2520and%2520Liping%2520Jing%2520and%2520Yiming%2520Nie%2520and%2520Bin%2520Dai%26entry.1292438233%3D%2520%2520Vision-centric%2520autonomous%2520driving%2520has%2520recently%2520raised%2520wide%2520attention%2520due%2520to%250Aits%2520lower%2520cost.%2520Pre-training%2520is%2520essential%2520for%2520extracting%2520a%2520universal%250Arepresentation.%2520However%252C%2520current%2520vision-centric%2520pre-training%2520typically%2520relies%250Aon%2520either%25202D%2520or%25203D%2520pre-text%2520tasks%252C%2520overlooking%2520the%2520temporal%2520characteristics%2520of%250Aautonomous%2520driving%2520as%2520a%25204D%2520scene%2520understanding%2520task.%2520In%2520this%2520paper%252C%2520we%2520address%250Athis%2520challenge%2520by%2520introducing%2520a%2520world%2520model-based%2520autonomous%2520driving%25204D%250Arepresentation%2520learning%2520framework%252C%2520dubbed%2520%255Cemph%257BDriveWorld%257D%252C%2520which%2520is%2520capable%250Aof%2520pre-training%2520from%2520multi-camera%2520driving%2520videos%2520in%2520a%2520spatio-temporal%2520fashion.%250ASpecifically%252C%2520we%2520propose%2520a%2520Memory%2520State-Space%2520Model%2520for%2520spatio-temporal%250Amodelling%252C%2520which%2520consists%2520of%2520a%2520Dynamic%2520Memory%2520Bank%2520module%2520for%2520learning%250Atemporal-aware%2520latent%2520dynamics%2520to%2520predict%2520future%2520changes%2520and%2520a%2520Static%2520Scene%250APropagation%2520module%2520for%2520learning%2520spatial-aware%2520latent%2520statics%2520to%2520offer%250Acomprehensive%2520scene%2520contexts.%2520We%2520additionally%2520introduce%2520a%2520Task%2520Prompt%2520to%250Adecouple%2520task-aware%2520features%2520for%2520various%2520downstream%2520tasks.%2520The%2520experiments%250Ademonstrate%2520that%2520DriveWorld%2520delivers%2520promising%2520results%2520on%2520various%2520autonomous%250Adriving%2520tasks.%2520When%2520pre-trained%2520with%2520the%2520OpenScene%2520dataset%252C%2520DriveWorld%2520achieves%250Aa%25207.5%2525%2520increase%2520in%2520mAP%2520for%25203D%2520object%2520detection%252C%2520a%25203.0%2525%2520increase%2520in%2520IoU%2520for%250Aonline%2520mapping%252C%2520a%25205.0%2525%2520increase%2520in%2520AMOTA%2520for%2520multi-object%2520tracking%252C%2520a%25200.1m%250Adecrease%2520in%2520minADE%2520for%2520motion%2520forecasting%252C%2520a%25203.0%2525%2520increase%2520in%2520IoU%2520for%2520occupancy%250Aprediction%252C%2520and%2520a%25200.34m%2520reduction%2520in%2520average%2520L2%2520error%2520for%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DriveWorld%3A%204D%20Pre-trained%20Scene%20Understanding%20via%20World%20Models%20for%0A%20%20Autonomous%20Driving&entry.906535625=Chen%20Min%20and%20Dawei%20Zhao%20and%20Liang%20Xiao%20and%20Jian%20Zhao%20and%20Xinli%20Xu%20and%20Zheng%20Zhu%20and%20Lei%20Jin%20and%20Jianshu%20Li%20and%20Yulan%20Guo%20and%20Junliang%20Xing%20and%20Liping%20Jing%20and%20Yiming%20Nie%20and%20Bin%20Dai&entry.1292438233=%20%20Vision-centric%20autonomous%20driving%20has%20recently%20raised%20wide%20attention%20due%20to%0Aits%20lower%20cost.%20Pre-training%20is%20essential%20for%20extracting%20a%20universal%0Arepresentation.%20However%2C%20current%20vision-centric%20pre-training%20typically%20relies%0Aon%20either%202D%20or%203D%20pre-text%20tasks%2C%20overlooking%20the%20temporal%20characteristics%20of%0Aautonomous%20driving%20as%20a%204D%20scene%20understanding%20task.%20In%20this%20paper%2C%20we%20address%0Athis%20challenge%20by%20introducing%20a%20world%20model-based%20autonomous%20driving%204D%0Arepresentation%20learning%20framework%2C%20dubbed%20%5Cemph%7BDriveWorld%7D%2C%20which%20is%20capable%0Aof%20pre-training%20from%20multi-camera%20driving%20videos%20in%20a%20spatio-temporal%20fashion.%0ASpecifically%2C%20we%20propose%20a%20Memory%20State-Space%20Model%20for%20spatio-temporal%0Amodelling%2C%20which%20consists%20of%20a%20Dynamic%20Memory%20Bank%20module%20for%20learning%0Atemporal-aware%20latent%20dynamics%20to%20predict%20future%20changes%20and%20a%20Static%20Scene%0APropagation%20module%20for%20learning%20spatial-aware%20latent%20statics%20to%20offer%0Acomprehensive%20scene%20contexts.%20We%20additionally%20introduce%20a%20Task%20Prompt%20to%0Adecouple%20task-aware%20features%20for%20various%20downstream%20tasks.%20The%20experiments%0Ademonstrate%20that%20DriveWorld%20delivers%20promising%20results%20on%20various%20autonomous%0Adriving%20tasks.%20When%20pre-trained%20with%20the%20OpenScene%20dataset%2C%20DriveWorld%20achieves%0Aa%207.5%25%20increase%20in%20mAP%20for%203D%20object%20detection%2C%20a%203.0%25%20increase%20in%20IoU%20for%0Aonline%20mapping%2C%20a%205.0%25%20increase%20in%20AMOTA%20for%20multi-object%20tracking%2C%20a%200.1m%0Adecrease%20in%20minADE%20for%20motion%20forecasting%2C%20a%203.0%25%20increase%20in%20IoU%20for%20occupancy%0Aprediction%2C%20and%20a%200.34m%20reduction%20in%20average%20L2%20error%20for%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04390v1&entry.124074799=Read"},
{"title": "Mozart's Touch: A Lightweight Multi-modal Music Generation Framework\n  Based on Pre-Trained Large Models", "author": "Tianze Xu and Jiajun Li and Xuesong Chen and Xinrui Yao and Shuchang Liu", "abstract": "  In recent years, AI-Generated Content (AIGC) has witnessed rapid\nadvancements, facilitating the generation of music, images, and other forms of\nartistic expression across various industries. However, researches on general\nmulti-modal music generation model remain scarce. To fill this gap, we propose\na multi-modal music generation framework Mozart's Touch. It could generate\naligned music with the cross-modality inputs, such as images, videos and text.\nMozart's Touch is composed of three main components: Multi-modal Captioning\nModule, Large Language Model (LLM) Understanding & Bridging Module, and Music\nGeneration Module. Unlike traditional approaches, Mozart's Touch requires no\ntraining or fine-tuning pre-trained models, offering efficiency and\ntransparency through clear, interpretable prompts. We also introduce\n\"LLM-Bridge\" method to resolve the heterogeneous representation problems\nbetween descriptive texts of different modalities. We conduct a series of\nobjective and subjective evaluations on the proposed model, and results\nindicate that our model surpasses the performance of current state-of-the-art\nmodels. Our codes and examples is availble at:\nhttps://github.com/WangTooNaive/MozartsTouch\n", "link": "http://arxiv.org/abs/2405.02801v2", "date": "2024-05-07", "relevancy": 2.7506, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5703}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5568}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mozart%27s%20Touch%3A%20A%20Lightweight%20Multi-modal%20Music%20Generation%20Framework%0A%20%20Based%20on%20Pre-Trained%20Large%20Models&body=Title%3A%20Mozart%27s%20Touch%3A%20A%20Lightweight%20Multi-modal%20Music%20Generation%20Framework%0A%20%20Based%20on%20Pre-Trained%20Large%20Models%0AAuthor%3A%20Tianze%20Xu%20and%20Jiajun%20Li%20and%20Xuesong%20Chen%20and%20Xinrui%20Yao%20and%20Shuchang%20Liu%0AAbstract%3A%20%20%20In%20recent%20years%2C%20AI-Generated%20Content%20%28AIGC%29%20has%20witnessed%20rapid%0Aadvancements%2C%20facilitating%20the%20generation%20of%20music%2C%20images%2C%20and%20other%20forms%20of%0Aartistic%20expression%20across%20various%20industries.%20However%2C%20researches%20on%20general%0Amulti-modal%20music%20generation%20model%20remain%20scarce.%20To%20fill%20this%20gap%2C%20we%20propose%0Aa%20multi-modal%20music%20generation%20framework%20Mozart%27s%20Touch.%20It%20could%20generate%0Aaligned%20music%20with%20the%20cross-modality%20inputs%2C%20such%20as%20images%2C%20videos%20and%20text.%0AMozart%27s%20Touch%20is%20composed%20of%20three%20main%20components%3A%20Multi-modal%20Captioning%0AModule%2C%20Large%20Language%20Model%20%28LLM%29%20Understanding%20%26%20Bridging%20Module%2C%20and%20Music%0AGeneration%20Module.%20Unlike%20traditional%20approaches%2C%20Mozart%27s%20Touch%20requires%20no%0Atraining%20or%20fine-tuning%20pre-trained%20models%2C%20offering%20efficiency%20and%0Atransparency%20through%20clear%2C%20interpretable%20prompts.%20We%20also%20introduce%0A%22LLM-Bridge%22%20method%20to%20resolve%20the%20heterogeneous%20representation%20problems%0Abetween%20descriptive%20texts%20of%20different%20modalities.%20We%20conduct%20a%20series%20of%0Aobjective%20and%20subjective%20evaluations%20on%20the%20proposed%20model%2C%20and%20results%0Aindicate%20that%20our%20model%20surpasses%20the%20performance%20of%20current%20state-of-the-art%0Amodels.%20Our%20codes%20and%20examples%20is%20availble%20at%3A%0Ahttps%3A//github.com/WangTooNaive/MozartsTouch%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02801v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMozart%2527s%2520Touch%253A%2520A%2520Lightweight%2520Multi-modal%2520Music%2520Generation%2520Framework%250A%2520%2520Based%2520on%2520Pre-Trained%2520Large%2520Models%26entry.906535625%3DTianze%2520Xu%2520and%2520Jiajun%2520Li%2520and%2520Xuesong%2520Chen%2520and%2520Xinrui%2520Yao%2520and%2520Shuchang%2520Liu%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520AI-Generated%2520Content%2520%2528AIGC%2529%2520has%2520witnessed%2520rapid%250Aadvancements%252C%2520facilitating%2520the%2520generation%2520of%2520music%252C%2520images%252C%2520and%2520other%2520forms%2520of%250Aartistic%2520expression%2520across%2520various%2520industries.%2520However%252C%2520researches%2520on%2520general%250Amulti-modal%2520music%2520generation%2520model%2520remain%2520scarce.%2520To%2520fill%2520this%2520gap%252C%2520we%2520propose%250Aa%2520multi-modal%2520music%2520generation%2520framework%2520Mozart%2527s%2520Touch.%2520It%2520could%2520generate%250Aaligned%2520music%2520with%2520the%2520cross-modality%2520inputs%252C%2520such%2520as%2520images%252C%2520videos%2520and%2520text.%250AMozart%2527s%2520Touch%2520is%2520composed%2520of%2520three%2520main%2520components%253A%2520Multi-modal%2520Captioning%250AModule%252C%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520Understanding%2520%2526%2520Bridging%2520Module%252C%2520and%2520Music%250AGeneration%2520Module.%2520Unlike%2520traditional%2520approaches%252C%2520Mozart%2527s%2520Touch%2520requires%2520no%250Atraining%2520or%2520fine-tuning%2520pre-trained%2520models%252C%2520offering%2520efficiency%2520and%250Atransparency%2520through%2520clear%252C%2520interpretable%2520prompts.%2520We%2520also%2520introduce%250A%2522LLM-Bridge%2522%2520method%2520to%2520resolve%2520the%2520heterogeneous%2520representation%2520problems%250Abetween%2520descriptive%2520texts%2520of%2520different%2520modalities.%2520We%2520conduct%2520a%2520series%2520of%250Aobjective%2520and%2520subjective%2520evaluations%2520on%2520the%2520proposed%2520model%252C%2520and%2520results%250Aindicate%2520that%2520our%2520model%2520surpasses%2520the%2520performance%2520of%2520current%2520state-of-the-art%250Amodels.%2520Our%2520codes%2520and%2520examples%2520is%2520availble%2520at%253A%250Ahttps%253A//github.com/WangTooNaive/MozartsTouch%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02801v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mozart%27s%20Touch%3A%20A%20Lightweight%20Multi-modal%20Music%20Generation%20Framework%0A%20%20Based%20on%20Pre-Trained%20Large%20Models&entry.906535625=Tianze%20Xu%20and%20Jiajun%20Li%20and%20Xuesong%20Chen%20and%20Xinrui%20Yao%20and%20Shuchang%20Liu&entry.1292438233=%20%20In%20recent%20years%2C%20AI-Generated%20Content%20%28AIGC%29%20has%20witnessed%20rapid%0Aadvancements%2C%20facilitating%20the%20generation%20of%20music%2C%20images%2C%20and%20other%20forms%20of%0Aartistic%20expression%20across%20various%20industries.%20However%2C%20researches%20on%20general%0Amulti-modal%20music%20generation%20model%20remain%20scarce.%20To%20fill%20this%20gap%2C%20we%20propose%0Aa%20multi-modal%20music%20generation%20framework%20Mozart%27s%20Touch.%20It%20could%20generate%0Aaligned%20music%20with%20the%20cross-modality%20inputs%2C%20such%20as%20images%2C%20videos%20and%20text.%0AMozart%27s%20Touch%20is%20composed%20of%20three%20main%20components%3A%20Multi-modal%20Captioning%0AModule%2C%20Large%20Language%20Model%20%28LLM%29%20Understanding%20%26%20Bridging%20Module%2C%20and%20Music%0AGeneration%20Module.%20Unlike%20traditional%20approaches%2C%20Mozart%27s%20Touch%20requires%20no%0Atraining%20or%20fine-tuning%20pre-trained%20models%2C%20offering%20efficiency%20and%0Atransparency%20through%20clear%2C%20interpretable%20prompts.%20We%20also%20introduce%0A%22LLM-Bridge%22%20method%20to%20resolve%20the%20heterogeneous%20representation%20problems%0Abetween%20descriptive%20texts%20of%20different%20modalities.%20We%20conduct%20a%20series%20of%0Aobjective%20and%20subjective%20evaluations%20on%20the%20proposed%20model%2C%20and%20results%0Aindicate%20that%20our%20model%20surpasses%20the%20performance%20of%20current%20state-of-the-art%0Amodels.%20Our%20codes%20and%20examples%20is%20availble%20at%3A%0Ahttps%3A//github.com/WangTooNaive/MozartsTouch%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02801v2&entry.124074799=Read"},
{"title": "NTIRE 2024 Quality Assessment of AI-Generated Content Challenge", "author": "Xiaohong Liu and Xiongkuo Min and Guangtao Zhai and Chunyi Li and Tengchuan Kou and Wei Sun and Haoning Wu and Yixuan Gao and Yuqin Cao and Zicheng Zhang and Xiele Wu and Radu Timofte and Fei Peng and Huiyuan Fu and Anlong Ming and Chuanming Wang and Huadong Ma and Shuai He and Zifei Dou and Shu Chen and Huacong Zhang and Haiyi Xie and Chengwei Wang and Baoying Chen and Jishen Zeng and Jianquan Yang and Weigang Wang and Xi Fang and Xiaoxin Lv and Jun Yan and Tianwu Zhi and Yabin Zhang and Yaohui Li and Yang Li and Jingwen Xu and Jianzhao Liu and Yiting Liao and Junlin Li and Zihao Yu and Yiting Lu and Xin Li and Hossein Motamednia and S. Farhad Hosseini-Benvidi and Fengbin Guan and Ahmad Mahmoudi-Aznaveh and Azadeh Mansouri and Ganzorig Gankhuyag and Kihwan Yoon and Yifang Xu and Haotian Fan and Fangyuan Kong and Shiling Zhao and Weifeng Dong and Haibing Yin and Li Zhu and Zhiling Wang and Bingchen Huang and Avinab Saha and Sandeep Mishra and Shashank Gupta and Rajesh Sureddi and Oindrila Saha and Luigi Celona and Simone Bianco and Paolo Napoletano and Raimondo Schettini and Junfeng Yang and Jing Fu and Wei Zhang and Wenzhi Cao and Limei Liu and Han Peng and Weijun Yuan and Zhan Li and Yihang Cheng and Yifan Deng and Haohui Li and Bowen Qu and Yao Li and Shuqing Luo and Shunzhou Wang and Wei Gao and Zihao Lu and Marcos V. Conde and Xinrui Wang and Zhibo Chen and Ruling Liao and Yan Ye and Qiulin Wang and Bing Li and Zhaokun Zhou and Miao Geng and Rui Chen and Xin Tao and Xiaoyu Liang and Shangkun Sun and Xingyuan Ma and Jiaze Li and Mengduo Yang and Haoran Xu and Jie Zhou and Shiding Zhu and Bohan Yu and Pengfei Chen and Xinrui Xu and Jiabin Shen and Zhichao Duan and Erfan Asadi and Jiahe Liu and Qi Yan and Youran Qu and Xiaohui Zeng and Lele Wang and Renjie Liao", "abstract": "  This paper reports on the NTIRE 2024 Quality Assessment of AI-Generated\nContent Challenge, which will be held in conjunction with the New Trends in\nImage Restoration and Enhancement Workshop (NTIRE) at CVPR 2024. This challenge\nis to address a major challenge in the field of image and video processing,\nnamely, Image Quality Assessment (IQA) and Video Quality Assessment (VQA) for\nAI-Generated Content (AIGC). The challenge is divided into the image track and\nthe video track. The image track uses the AIGIQA-20K, which contains 20,000\nAI-Generated Images (AIGIs) generated by 15 popular generative models. The\nimage track has a total of 318 registered participants. A total of 1,646\nsubmissions are received in the development phase, and 221 submissions are\nreceived in the test phase. Finally, 16 participating teams submitted their\nmodels and fact sheets. The video track uses the T2VQA-DB, which contains\n10,000 AI-Generated Videos (AIGVs) generated by 9 popular Text-to-Video (T2V)\nmodels. A total of 196 participants have registered in the video track. A total\nof 991 submissions are received in the development phase, and 185 submissions\nare received in the test phase. Finally, 12 participating teams submitted their\nmodels and fact sheets. Some methods have achieved better results than baseline\nmethods, and the winning methods in both tracks have demonstrated superior\nprediction performance on AIGC.\n", "link": "http://arxiv.org/abs/2404.16687v2", "date": "2024-05-07", "relevancy": 2.7345, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5765}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5643}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NTIRE%202024%20Quality%20Assessment%20of%20AI-Generated%20Content%20Challenge&body=Title%3A%20NTIRE%202024%20Quality%20Assessment%20of%20AI-Generated%20Content%20Challenge%0AAuthor%3A%20Xiaohong%20Liu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%20and%20Chunyi%20Li%20and%20Tengchuan%20Kou%20and%20Wei%20Sun%20and%20Haoning%20Wu%20and%20Yixuan%20Gao%20and%20Yuqin%20Cao%20and%20Zicheng%20Zhang%20and%20Xiele%20Wu%20and%20Radu%20Timofte%20and%20Fei%20Peng%20and%20Huiyuan%20Fu%20and%20Anlong%20Ming%20and%20Chuanming%20Wang%20and%20Huadong%20Ma%20and%20Shuai%20He%20and%20Zifei%20Dou%20and%20Shu%20Chen%20and%20Huacong%20Zhang%20and%20Haiyi%20Xie%20and%20Chengwei%20Wang%20and%20Baoying%20Chen%20and%20Jishen%20Zeng%20and%20Jianquan%20Yang%20and%20Weigang%20Wang%20and%20Xi%20Fang%20and%20Xiaoxin%20Lv%20and%20Jun%20Yan%20and%20Tianwu%20Zhi%20and%20Yabin%20Zhang%20and%20Yaohui%20Li%20and%20Yang%20Li%20and%20Jingwen%20Xu%20and%20Jianzhao%20Liu%20and%20Yiting%20Liao%20and%20Junlin%20Li%20and%20Zihao%20Yu%20and%20Yiting%20Lu%20and%20Xin%20Li%20and%20Hossein%20Motamednia%20and%20S.%20Farhad%20Hosseini-Benvidi%20and%20Fengbin%20Guan%20and%20Ahmad%20Mahmoudi-Aznaveh%20and%20Azadeh%20Mansouri%20and%20Ganzorig%20Gankhuyag%20and%20Kihwan%20Yoon%20and%20Yifang%20Xu%20and%20Haotian%20Fan%20and%20Fangyuan%20Kong%20and%20Shiling%20Zhao%20and%20Weifeng%20Dong%20and%20Haibing%20Yin%20and%20Li%20Zhu%20and%20Zhiling%20Wang%20and%20Bingchen%20Huang%20and%20Avinab%20Saha%20and%20Sandeep%20Mishra%20and%20Shashank%20Gupta%20and%20Rajesh%20Sureddi%20and%20Oindrila%20Saha%20and%20Luigi%20Celona%20and%20Simone%20Bianco%20and%20Paolo%20Napoletano%20and%20Raimondo%20Schettini%20and%20Junfeng%20Yang%20and%20Jing%20Fu%20and%20Wei%20Zhang%20and%20Wenzhi%20Cao%20and%20Limei%20Liu%20and%20Han%20Peng%20and%20Weijun%20Yuan%20and%20Zhan%20Li%20and%20Yihang%20Cheng%20and%20Yifan%20Deng%20and%20Haohui%20Li%20and%20Bowen%20Qu%20and%20Yao%20Li%20and%20Shuqing%20Luo%20and%20Shunzhou%20Wang%20and%20Wei%20Gao%20and%20Zihao%20Lu%20and%20Marcos%20V.%20Conde%20and%20Xinrui%20Wang%20and%20Zhibo%20Chen%20and%20Ruling%20Liao%20and%20Yan%20Ye%20and%20Qiulin%20Wang%20and%20Bing%20Li%20and%20Zhaokun%20Zhou%20and%20Miao%20Geng%20and%20Rui%20Chen%20and%20Xin%20Tao%20and%20Xiaoyu%20Liang%20and%20Shangkun%20Sun%20and%20Xingyuan%20Ma%20and%20Jiaze%20Li%20and%20Mengduo%20Yang%20and%20Haoran%20Xu%20and%20Jie%20Zhou%20and%20Shiding%20Zhu%20and%20Bohan%20Yu%20and%20Pengfei%20Chen%20and%20Xinrui%20Xu%20and%20Jiabin%20Shen%20and%20Zhichao%20Duan%20and%20Erfan%20Asadi%20and%20Jiahe%20Liu%20and%20Qi%20Yan%20and%20Youran%20Qu%20and%20Xiaohui%20Zeng%20and%20Lele%20Wang%20and%20Renjie%20Liao%0AAbstract%3A%20%20%20This%20paper%20reports%20on%20the%20NTIRE%202024%20Quality%20Assessment%20of%20AI-Generated%0AContent%20Challenge%2C%20which%20will%20be%20held%20in%20conjunction%20with%20the%20New%20Trends%20in%0AImage%20Restoration%20and%20Enhancement%20Workshop%20%28NTIRE%29%20at%20CVPR%202024.%20This%20challenge%0Ais%20to%20address%20a%20major%20challenge%20in%20the%20field%20of%20image%20and%20video%20processing%2C%0Anamely%2C%20Image%20Quality%20Assessment%20%28IQA%29%20and%20Video%20Quality%20Assessment%20%28VQA%29%20for%0AAI-Generated%20Content%20%28AIGC%29.%20The%20challenge%20is%20divided%20into%20the%20image%20track%20and%0Athe%20video%20track.%20The%20image%20track%20uses%20the%20AIGIQA-20K%2C%20which%20contains%2020%2C000%0AAI-Generated%20Images%20%28AIGIs%29%20generated%20by%2015%20popular%20generative%20models.%20The%0Aimage%20track%20has%20a%20total%20of%20318%20registered%20participants.%20A%20total%20of%201%2C646%0Asubmissions%20are%20received%20in%20the%20development%20phase%2C%20and%20221%20submissions%20are%0Areceived%20in%20the%20test%20phase.%20Finally%2C%2016%20participating%20teams%20submitted%20their%0Amodels%20and%20fact%20sheets.%20The%20video%20track%20uses%20the%20T2VQA-DB%2C%20which%20contains%0A10%2C000%20AI-Generated%20Videos%20%28AIGVs%29%20generated%20by%209%20popular%20Text-to-Video%20%28T2V%29%0Amodels.%20A%20total%20of%20196%20participants%20have%20registered%20in%20the%20video%20track.%20A%20total%0Aof%20991%20submissions%20are%20received%20in%20the%20development%20phase%2C%20and%20185%20submissions%0Aare%20received%20in%20the%20test%20phase.%20Finally%2C%2012%20participating%20teams%20submitted%20their%0Amodels%20and%20fact%20sheets.%20Some%20methods%20have%20achieved%20better%20results%20than%20baseline%0Amethods%2C%20and%20the%20winning%20methods%20in%20both%20tracks%20have%20demonstrated%20superior%0Aprediction%20performance%20on%20AIGC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16687v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNTIRE%25202024%2520Quality%2520Assessment%2520of%2520AI-Generated%2520Content%2520Challenge%26entry.906535625%3DXiaohong%2520Liu%2520and%2520Xiongkuo%2520Min%2520and%2520Guangtao%2520Zhai%2520and%2520Chunyi%2520Li%2520and%2520Tengchuan%2520Kou%2520and%2520Wei%2520Sun%2520and%2520Haoning%2520Wu%2520and%2520Yixuan%2520Gao%2520and%2520Yuqin%2520Cao%2520and%2520Zicheng%2520Zhang%2520and%2520Xiele%2520Wu%2520and%2520Radu%2520Timofte%2520and%2520Fei%2520Peng%2520and%2520Huiyuan%2520Fu%2520and%2520Anlong%2520Ming%2520and%2520Chuanming%2520Wang%2520and%2520Huadong%2520Ma%2520and%2520Shuai%2520He%2520and%2520Zifei%2520Dou%2520and%2520Shu%2520Chen%2520and%2520Huacong%2520Zhang%2520and%2520Haiyi%2520Xie%2520and%2520Chengwei%2520Wang%2520and%2520Baoying%2520Chen%2520and%2520Jishen%2520Zeng%2520and%2520Jianquan%2520Yang%2520and%2520Weigang%2520Wang%2520and%2520Xi%2520Fang%2520and%2520Xiaoxin%2520Lv%2520and%2520Jun%2520Yan%2520and%2520Tianwu%2520Zhi%2520and%2520Yabin%2520Zhang%2520and%2520Yaohui%2520Li%2520and%2520Yang%2520Li%2520and%2520Jingwen%2520Xu%2520and%2520Jianzhao%2520Liu%2520and%2520Yiting%2520Liao%2520and%2520Junlin%2520Li%2520and%2520Zihao%2520Yu%2520and%2520Yiting%2520Lu%2520and%2520Xin%2520Li%2520and%2520Hossein%2520Motamednia%2520and%2520S.%2520Farhad%2520Hosseini-Benvidi%2520and%2520Fengbin%2520Guan%2520and%2520Ahmad%2520Mahmoudi-Aznaveh%2520and%2520Azadeh%2520Mansouri%2520and%2520Ganzorig%2520Gankhuyag%2520and%2520Kihwan%2520Yoon%2520and%2520Yifang%2520Xu%2520and%2520Haotian%2520Fan%2520and%2520Fangyuan%2520Kong%2520and%2520Shiling%2520Zhao%2520and%2520Weifeng%2520Dong%2520and%2520Haibing%2520Yin%2520and%2520Li%2520Zhu%2520and%2520Zhiling%2520Wang%2520and%2520Bingchen%2520Huang%2520and%2520Avinab%2520Saha%2520and%2520Sandeep%2520Mishra%2520and%2520Shashank%2520Gupta%2520and%2520Rajesh%2520Sureddi%2520and%2520Oindrila%2520Saha%2520and%2520Luigi%2520Celona%2520and%2520Simone%2520Bianco%2520and%2520Paolo%2520Napoletano%2520and%2520Raimondo%2520Schettini%2520and%2520Junfeng%2520Yang%2520and%2520Jing%2520Fu%2520and%2520Wei%2520Zhang%2520and%2520Wenzhi%2520Cao%2520and%2520Limei%2520Liu%2520and%2520Han%2520Peng%2520and%2520Weijun%2520Yuan%2520and%2520Zhan%2520Li%2520and%2520Yihang%2520Cheng%2520and%2520Yifan%2520Deng%2520and%2520Haohui%2520Li%2520and%2520Bowen%2520Qu%2520and%2520Yao%2520Li%2520and%2520Shuqing%2520Luo%2520and%2520Shunzhou%2520Wang%2520and%2520Wei%2520Gao%2520and%2520Zihao%2520Lu%2520and%2520Marcos%2520V.%2520Conde%2520and%2520Xinrui%2520Wang%2520and%2520Zhibo%2520Chen%2520and%2520Ruling%2520Liao%2520and%2520Yan%2520Ye%2520and%2520Qiulin%2520Wang%2520and%2520Bing%2520Li%2520and%2520Zhaokun%2520Zhou%2520and%2520Miao%2520Geng%2520and%2520Rui%2520Chen%2520and%2520Xin%2520Tao%2520and%2520Xiaoyu%2520Liang%2520and%2520Shangkun%2520Sun%2520and%2520Xingyuan%2520Ma%2520and%2520Jiaze%2520Li%2520and%2520Mengduo%2520Yang%2520and%2520Haoran%2520Xu%2520and%2520Jie%2520Zhou%2520and%2520Shiding%2520Zhu%2520and%2520Bohan%2520Yu%2520and%2520Pengfei%2520Chen%2520and%2520Xinrui%2520Xu%2520and%2520Jiabin%2520Shen%2520and%2520Zhichao%2520Duan%2520and%2520Erfan%2520Asadi%2520and%2520Jiahe%2520Liu%2520and%2520Qi%2520Yan%2520and%2520Youran%2520Qu%2520and%2520Xiaohui%2520Zeng%2520and%2520Lele%2520Wang%2520and%2520Renjie%2520Liao%26entry.1292438233%3D%2520%2520This%2520paper%2520reports%2520on%2520the%2520NTIRE%25202024%2520Quality%2520Assessment%2520of%2520AI-Generated%250AContent%2520Challenge%252C%2520which%2520will%2520be%2520held%2520in%2520conjunction%2520with%2520the%2520New%2520Trends%2520in%250AImage%2520Restoration%2520and%2520Enhancement%2520Workshop%2520%2528NTIRE%2529%2520at%2520CVPR%25202024.%2520This%2520challenge%250Ais%2520to%2520address%2520a%2520major%2520challenge%2520in%2520the%2520field%2520of%2520image%2520and%2520video%2520processing%252C%250Anamely%252C%2520Image%2520Quality%2520Assessment%2520%2528IQA%2529%2520and%2520Video%2520Quality%2520Assessment%2520%2528VQA%2529%2520for%250AAI-Generated%2520Content%2520%2528AIGC%2529.%2520The%2520challenge%2520is%2520divided%2520into%2520the%2520image%2520track%2520and%250Athe%2520video%2520track.%2520The%2520image%2520track%2520uses%2520the%2520AIGIQA-20K%252C%2520which%2520contains%252020%252C000%250AAI-Generated%2520Images%2520%2528AIGIs%2529%2520generated%2520by%252015%2520popular%2520generative%2520models.%2520The%250Aimage%2520track%2520has%2520a%2520total%2520of%2520318%2520registered%2520participants.%2520A%2520total%2520of%25201%252C646%250Asubmissions%2520are%2520received%2520in%2520the%2520development%2520phase%252C%2520and%2520221%2520submissions%2520are%250Areceived%2520in%2520the%2520test%2520phase.%2520Finally%252C%252016%2520participating%2520teams%2520submitted%2520their%250Amodels%2520and%2520fact%2520sheets.%2520The%2520video%2520track%2520uses%2520the%2520T2VQA-DB%252C%2520which%2520contains%250A10%252C000%2520AI-Generated%2520Videos%2520%2528AIGVs%2529%2520generated%2520by%25209%2520popular%2520Text-to-Video%2520%2528T2V%2529%250Amodels.%2520A%2520total%2520of%2520196%2520participants%2520have%2520registered%2520in%2520the%2520video%2520track.%2520A%2520total%250Aof%2520991%2520submissions%2520are%2520received%2520in%2520the%2520development%2520phase%252C%2520and%2520185%2520submissions%250Aare%2520received%2520in%2520the%2520test%2520phase.%2520Finally%252C%252012%2520participating%2520teams%2520submitted%2520their%250Amodels%2520and%2520fact%2520sheets.%2520Some%2520methods%2520have%2520achieved%2520better%2520results%2520than%2520baseline%250Amethods%252C%2520and%2520the%2520winning%2520methods%2520in%2520both%2520tracks%2520have%2520demonstrated%2520superior%250Aprediction%2520performance%2520on%2520AIGC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16687v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NTIRE%202024%20Quality%20Assessment%20of%20AI-Generated%20Content%20Challenge&entry.906535625=Xiaohong%20Liu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%20and%20Chunyi%20Li%20and%20Tengchuan%20Kou%20and%20Wei%20Sun%20and%20Haoning%20Wu%20and%20Yixuan%20Gao%20and%20Yuqin%20Cao%20and%20Zicheng%20Zhang%20and%20Xiele%20Wu%20and%20Radu%20Timofte%20and%20Fei%20Peng%20and%20Huiyuan%20Fu%20and%20Anlong%20Ming%20and%20Chuanming%20Wang%20and%20Huadong%20Ma%20and%20Shuai%20He%20and%20Zifei%20Dou%20and%20Shu%20Chen%20and%20Huacong%20Zhang%20and%20Haiyi%20Xie%20and%20Chengwei%20Wang%20and%20Baoying%20Chen%20and%20Jishen%20Zeng%20and%20Jianquan%20Yang%20and%20Weigang%20Wang%20and%20Xi%20Fang%20and%20Xiaoxin%20Lv%20and%20Jun%20Yan%20and%20Tianwu%20Zhi%20and%20Yabin%20Zhang%20and%20Yaohui%20Li%20and%20Yang%20Li%20and%20Jingwen%20Xu%20and%20Jianzhao%20Liu%20and%20Yiting%20Liao%20and%20Junlin%20Li%20and%20Zihao%20Yu%20and%20Yiting%20Lu%20and%20Xin%20Li%20and%20Hossein%20Motamednia%20and%20S.%20Farhad%20Hosseini-Benvidi%20and%20Fengbin%20Guan%20and%20Ahmad%20Mahmoudi-Aznaveh%20and%20Azadeh%20Mansouri%20and%20Ganzorig%20Gankhuyag%20and%20Kihwan%20Yoon%20and%20Yifang%20Xu%20and%20Haotian%20Fan%20and%20Fangyuan%20Kong%20and%20Shiling%20Zhao%20and%20Weifeng%20Dong%20and%20Haibing%20Yin%20and%20Li%20Zhu%20and%20Zhiling%20Wang%20and%20Bingchen%20Huang%20and%20Avinab%20Saha%20and%20Sandeep%20Mishra%20and%20Shashank%20Gupta%20and%20Rajesh%20Sureddi%20and%20Oindrila%20Saha%20and%20Luigi%20Celona%20and%20Simone%20Bianco%20and%20Paolo%20Napoletano%20and%20Raimondo%20Schettini%20and%20Junfeng%20Yang%20and%20Jing%20Fu%20and%20Wei%20Zhang%20and%20Wenzhi%20Cao%20and%20Limei%20Liu%20and%20Han%20Peng%20and%20Weijun%20Yuan%20and%20Zhan%20Li%20and%20Yihang%20Cheng%20and%20Yifan%20Deng%20and%20Haohui%20Li%20and%20Bowen%20Qu%20and%20Yao%20Li%20and%20Shuqing%20Luo%20and%20Shunzhou%20Wang%20and%20Wei%20Gao%20and%20Zihao%20Lu%20and%20Marcos%20V.%20Conde%20and%20Xinrui%20Wang%20and%20Zhibo%20Chen%20and%20Ruling%20Liao%20and%20Yan%20Ye%20and%20Qiulin%20Wang%20and%20Bing%20Li%20and%20Zhaokun%20Zhou%20and%20Miao%20Geng%20and%20Rui%20Chen%20and%20Xin%20Tao%20and%20Xiaoyu%20Liang%20and%20Shangkun%20Sun%20and%20Xingyuan%20Ma%20and%20Jiaze%20Li%20and%20Mengduo%20Yang%20and%20Haoran%20Xu%20and%20Jie%20Zhou%20and%20Shiding%20Zhu%20and%20Bohan%20Yu%20and%20Pengfei%20Chen%20and%20Xinrui%20Xu%20and%20Jiabin%20Shen%20and%20Zhichao%20Duan%20and%20Erfan%20Asadi%20and%20Jiahe%20Liu%20and%20Qi%20Yan%20and%20Youran%20Qu%20and%20Xiaohui%20Zeng%20and%20Lele%20Wang%20and%20Renjie%20Liao&entry.1292438233=%20%20This%20paper%20reports%20on%20the%20NTIRE%202024%20Quality%20Assessment%20of%20AI-Generated%0AContent%20Challenge%2C%20which%20will%20be%20held%20in%20conjunction%20with%20the%20New%20Trends%20in%0AImage%20Restoration%20and%20Enhancement%20Workshop%20%28NTIRE%29%20at%20CVPR%202024.%20This%20challenge%0Ais%20to%20address%20a%20major%20challenge%20in%20the%20field%20of%20image%20and%20video%20processing%2C%0Anamely%2C%20Image%20Quality%20Assessment%20%28IQA%29%20and%20Video%20Quality%20Assessment%20%28VQA%29%20for%0AAI-Generated%20Content%20%28AIGC%29.%20The%20challenge%20is%20divided%20into%20the%20image%20track%20and%0Athe%20video%20track.%20The%20image%20track%20uses%20the%20AIGIQA-20K%2C%20which%20contains%2020%2C000%0AAI-Generated%20Images%20%28AIGIs%29%20generated%20by%2015%20popular%20generative%20models.%20The%0Aimage%20track%20has%20a%20total%20of%20318%20registered%20participants.%20A%20total%20of%201%2C646%0Asubmissions%20are%20received%20in%20the%20development%20phase%2C%20and%20221%20submissions%20are%0Areceived%20in%20the%20test%20phase.%20Finally%2C%2016%20participating%20teams%20submitted%20their%0Amodels%20and%20fact%20sheets.%20The%20video%20track%20uses%20the%20T2VQA-DB%2C%20which%20contains%0A10%2C000%20AI-Generated%20Videos%20%28AIGVs%29%20generated%20by%209%20popular%20Text-to-Video%20%28T2V%29%0Amodels.%20A%20total%20of%20196%20participants%20have%20registered%20in%20the%20video%20track.%20A%20total%0Aof%20991%20submissions%20are%20received%20in%20the%20development%20phase%2C%20and%20185%20submissions%0Aare%20received%20in%20the%20test%20phase.%20Finally%2C%2012%20participating%20teams%20submitted%20their%0Amodels%20and%20fact%20sheets.%20Some%20methods%20have%20achieved%20better%20results%20than%20baseline%0Amethods%2C%20and%20the%20winning%20methods%20in%20both%20tracks%20have%20demonstrated%20superior%0Aprediction%20performance%20on%20AIGC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16687v2&entry.124074799=Read"},
{"title": "Zero-Shot Stitching in Reinforcement Learning using Relative\n  Representations", "author": "Antonio Pio Ricciardi and Valentino Maiorca and Luca Moschella and Riccardo Marin and Emanuele Rodol\u00e0", "abstract": "  Visual Reinforcement Learning is a popular and powerful framework that takes\nfull advantage of the Deep Learning breakthrough. However, it is also known\nthat variations in the input (e.g., different colors of the panorama due to the\nseason of the year) or the task (e.g., changing the speed limit for a car to\nrespect) could require complete retraining of the agents. In this work, we\nleverage recent developments in unifying latent representations to demonstrate\nthat it is possible to combine the components of an agent, rather than retrain\nit from scratch. We build upon the recent relative representations framework\nand adapt it for Visual RL. This allows us to create completely new agents\ncapable of handling environment-task combinations never seen during training.\nOur work paves the road toward a more accessible and flexible use of\nreinforcement learning.\n", "link": "http://arxiv.org/abs/2404.12917v2", "date": "2024-05-07", "relevancy": 2.6956, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5519}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5399}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Stitching%20in%20Reinforcement%20Learning%20using%20Relative%0A%20%20Representations&body=Title%3A%20Zero-Shot%20Stitching%20in%20Reinforcement%20Learning%20using%20Relative%0A%20%20Representations%0AAuthor%3A%20Antonio%20Pio%20Ricciardi%20and%20Valentino%20Maiorca%20and%20Luca%20Moschella%20and%20Riccardo%20Marin%20and%20Emanuele%20Rodol%C3%A0%0AAbstract%3A%20%20%20Visual%20Reinforcement%20Learning%20is%20a%20popular%20and%20powerful%20framework%20that%20takes%0Afull%20advantage%20of%20the%20Deep%20Learning%20breakthrough.%20However%2C%20it%20is%20also%20known%0Athat%20variations%20in%20the%20input%20%28e.g.%2C%20different%20colors%20of%20the%20panorama%20due%20to%20the%0Aseason%20of%20the%20year%29%20or%20the%20task%20%28e.g.%2C%20changing%20the%20speed%20limit%20for%20a%20car%20to%0Arespect%29%20could%20require%20complete%20retraining%20of%20the%20agents.%20In%20this%20work%2C%20we%0Aleverage%20recent%20developments%20in%20unifying%20latent%20representations%20to%20demonstrate%0Athat%20it%20is%20possible%20to%20combine%20the%20components%20of%20an%20agent%2C%20rather%20than%20retrain%0Ait%20from%20scratch.%20We%20build%20upon%20the%20recent%20relative%20representations%20framework%0Aand%20adapt%20it%20for%20Visual%20RL.%20This%20allows%20us%20to%20create%20completely%20new%20agents%0Acapable%20of%20handling%20environment-task%20combinations%20never%20seen%20during%20training.%0AOur%20work%20paves%20the%20road%20toward%20a%20more%20accessible%20and%20flexible%20use%20of%0Areinforcement%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12917v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Stitching%2520in%2520Reinforcement%2520Learning%2520using%2520Relative%250A%2520%2520Representations%26entry.906535625%3DAntonio%2520Pio%2520Ricciardi%2520and%2520Valentino%2520Maiorca%2520and%2520Luca%2520Moschella%2520and%2520Riccardo%2520Marin%2520and%2520Emanuele%2520Rodol%25C3%25A0%26entry.1292438233%3D%2520%2520Visual%2520Reinforcement%2520Learning%2520is%2520a%2520popular%2520and%2520powerful%2520framework%2520that%2520takes%250Afull%2520advantage%2520of%2520the%2520Deep%2520Learning%2520breakthrough.%2520However%252C%2520it%2520is%2520also%2520known%250Athat%2520variations%2520in%2520the%2520input%2520%2528e.g.%252C%2520different%2520colors%2520of%2520the%2520panorama%2520due%2520to%2520the%250Aseason%2520of%2520the%2520year%2529%2520or%2520the%2520task%2520%2528e.g.%252C%2520changing%2520the%2520speed%2520limit%2520for%2520a%2520car%2520to%250Arespect%2529%2520could%2520require%2520complete%2520retraining%2520of%2520the%2520agents.%2520In%2520this%2520work%252C%2520we%250Aleverage%2520recent%2520developments%2520in%2520unifying%2520latent%2520representations%2520to%2520demonstrate%250Athat%2520it%2520is%2520possible%2520to%2520combine%2520the%2520components%2520of%2520an%2520agent%252C%2520rather%2520than%2520retrain%250Ait%2520from%2520scratch.%2520We%2520build%2520upon%2520the%2520recent%2520relative%2520representations%2520framework%250Aand%2520adapt%2520it%2520for%2520Visual%2520RL.%2520This%2520allows%2520us%2520to%2520create%2520completely%2520new%2520agents%250Acapable%2520of%2520handling%2520environment-task%2520combinations%2520never%2520seen%2520during%2520training.%250AOur%2520work%2520paves%2520the%2520road%2520toward%2520a%2520more%2520accessible%2520and%2520flexible%2520use%2520of%250Areinforcement%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.12917v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Stitching%20in%20Reinforcement%20Learning%20using%20Relative%0A%20%20Representations&entry.906535625=Antonio%20Pio%20Ricciardi%20and%20Valentino%20Maiorca%20and%20Luca%20Moschella%20and%20Riccardo%20Marin%20and%20Emanuele%20Rodol%C3%A0&entry.1292438233=%20%20Visual%20Reinforcement%20Learning%20is%20a%20popular%20and%20powerful%20framework%20that%20takes%0Afull%20advantage%20of%20the%20Deep%20Learning%20breakthrough.%20However%2C%20it%20is%20also%20known%0Athat%20variations%20in%20the%20input%20%28e.g.%2C%20different%20colors%20of%20the%20panorama%20due%20to%20the%0Aseason%20of%20the%20year%29%20or%20the%20task%20%28e.g.%2C%20changing%20the%20speed%20limit%20for%20a%20car%20to%0Arespect%29%20could%20require%20complete%20retraining%20of%20the%20agents.%20In%20this%20work%2C%20we%0Aleverage%20recent%20developments%20in%20unifying%20latent%20representations%20to%20demonstrate%0Athat%20it%20is%20possible%20to%20combine%20the%20components%20of%20an%20agent%2C%20rather%20than%20retrain%0Ait%20from%20scratch.%20We%20build%20upon%20the%20recent%20relative%20representations%20framework%0Aand%20adapt%20it%20for%20Visual%20RL.%20This%20allows%20us%20to%20create%20completely%20new%20agents%0Acapable%20of%20handling%20environment-task%20combinations%20never%20seen%20during%20training.%0AOur%20work%20paves%20the%20road%20toward%20a%20more%20accessible%20and%20flexible%20use%20of%0Areinforcement%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12917v2&entry.124074799=Read"},
{"title": "Class-Balanced and Reinforced Active Learning on Graphs", "author": "Chengcheng Yu and Jiapeng Zhu and Xiang Li", "abstract": "  Graph neural networks (GNNs) have demonstrated significant success in various\napplications, such as node classification, link prediction, and graph\nclassification. Active learning for GNNs aims to query the valuable samples\nfrom the unlabeled data for annotation to maximize the GNNs' performance at a\nlower cost. However, most existing algorithms for reinforced active learning in\nGNNs may lead to a highly imbalanced class distribution, especially in highly\nskewed class scenarios. GNNs trained with class-imbalanced labeled data are\nsusceptible to bias toward majority classes, and the lower performance of\nminority classes may lead to a decline in overall performance. To tackle this\nissue, we propose a novel class-balanced and reinforced active learning\nframework for GNNs, namely, GCBR. It learns an optimal policy to acquire\nclass-balanced and informative nodes for annotation, maximizing the performance\nof GNNs trained with selected labeled nodes. GCBR designs class-balance-aware\nstates, as well as a reward function that achieves trade-off between model\nperformance and class balance. The reinforcement learning algorithm Advantage\nActor-Critic (A2C) is employed to learn an optimal policy stably and\nefficiently. We further upgrade GCBR to GCBR++ by introducing a punishment\nmechanism to obtain a more class-balanced labeled set. Extensive experiments on\nmultiple datasets demonstrate the effectiveness of the proposed approaches,\nachieving superior performance over state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2402.10074v3", "date": "2024-05-07", "relevancy": 2.6523, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5395}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5273}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Class-Balanced%20and%20Reinforced%20Active%20Learning%20on%20Graphs&body=Title%3A%20Class-Balanced%20and%20Reinforced%20Active%20Learning%20on%20Graphs%0AAuthor%3A%20Chengcheng%20Yu%20and%20Jiapeng%20Zhu%20and%20Xiang%20Li%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20demonstrated%20significant%20success%20in%20various%0Aapplications%2C%20such%20as%20node%20classification%2C%20link%20prediction%2C%20and%20graph%0Aclassification.%20Active%20learning%20for%20GNNs%20aims%20to%20query%20the%20valuable%20samples%0Afrom%20the%20unlabeled%20data%20for%20annotation%20to%20maximize%20the%20GNNs%27%20performance%20at%20a%0Alower%20cost.%20However%2C%20most%20existing%20algorithms%20for%20reinforced%20active%20learning%20in%0AGNNs%20may%20lead%20to%20a%20highly%20imbalanced%20class%20distribution%2C%20especially%20in%20highly%0Askewed%20class%20scenarios.%20GNNs%20trained%20with%20class-imbalanced%20labeled%20data%20are%0Asusceptible%20to%20bias%20toward%20majority%20classes%2C%20and%20the%20lower%20performance%20of%0Aminority%20classes%20may%20lead%20to%20a%20decline%20in%20overall%20performance.%20To%20tackle%20this%0Aissue%2C%20we%20propose%20a%20novel%20class-balanced%20and%20reinforced%20active%20learning%0Aframework%20for%20GNNs%2C%20namely%2C%20GCBR.%20It%20learns%20an%20optimal%20policy%20to%20acquire%0Aclass-balanced%20and%20informative%20nodes%20for%20annotation%2C%20maximizing%20the%20performance%0Aof%20GNNs%20trained%20with%20selected%20labeled%20nodes.%20GCBR%20designs%20class-balance-aware%0Astates%2C%20as%20well%20as%20a%20reward%20function%20that%20achieves%20trade-off%20between%20model%0Aperformance%20and%20class%20balance.%20The%20reinforcement%20learning%20algorithm%20Advantage%0AActor-Critic%20%28A2C%29%20is%20employed%20to%20learn%20an%20optimal%20policy%20stably%20and%0Aefficiently.%20We%20further%20upgrade%20GCBR%20to%20GCBR%2B%2B%20by%20introducing%20a%20punishment%0Amechanism%20to%20obtain%20a%20more%20class-balanced%20labeled%20set.%20Extensive%20experiments%20on%0Amultiple%20datasets%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approaches%2C%0Aachieving%20superior%20performance%20over%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10074v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClass-Balanced%2520and%2520Reinforced%2520Active%2520Learning%2520on%2520Graphs%26entry.906535625%3DChengcheng%2520Yu%2520and%2520Jiapeng%2520Zhu%2520and%2520Xiang%2520Li%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520demonstrated%2520significant%2520success%2520in%2520various%250Aapplications%252C%2520such%2520as%2520node%2520classification%252C%2520link%2520prediction%252C%2520and%2520graph%250Aclassification.%2520Active%2520learning%2520for%2520GNNs%2520aims%2520to%2520query%2520the%2520valuable%2520samples%250Afrom%2520the%2520unlabeled%2520data%2520for%2520annotation%2520to%2520maximize%2520the%2520GNNs%2527%2520performance%2520at%2520a%250Alower%2520cost.%2520However%252C%2520most%2520existing%2520algorithms%2520for%2520reinforced%2520active%2520learning%2520in%250AGNNs%2520may%2520lead%2520to%2520a%2520highly%2520imbalanced%2520class%2520distribution%252C%2520especially%2520in%2520highly%250Askewed%2520class%2520scenarios.%2520GNNs%2520trained%2520with%2520class-imbalanced%2520labeled%2520data%2520are%250Asusceptible%2520to%2520bias%2520toward%2520majority%2520classes%252C%2520and%2520the%2520lower%2520performance%2520of%250Aminority%2520classes%2520may%2520lead%2520to%2520a%2520decline%2520in%2520overall%2520performance.%2520To%2520tackle%2520this%250Aissue%252C%2520we%2520propose%2520a%2520novel%2520class-balanced%2520and%2520reinforced%2520active%2520learning%250Aframework%2520for%2520GNNs%252C%2520namely%252C%2520GCBR.%2520It%2520learns%2520an%2520optimal%2520policy%2520to%2520acquire%250Aclass-balanced%2520and%2520informative%2520nodes%2520for%2520annotation%252C%2520maximizing%2520the%2520performance%250Aof%2520GNNs%2520trained%2520with%2520selected%2520labeled%2520nodes.%2520GCBR%2520designs%2520class-balance-aware%250Astates%252C%2520as%2520well%2520as%2520a%2520reward%2520function%2520that%2520achieves%2520trade-off%2520between%2520model%250Aperformance%2520and%2520class%2520balance.%2520The%2520reinforcement%2520learning%2520algorithm%2520Advantage%250AActor-Critic%2520%2528A2C%2529%2520is%2520employed%2520to%2520learn%2520an%2520optimal%2520policy%2520stably%2520and%250Aefficiently.%2520We%2520further%2520upgrade%2520GCBR%2520to%2520GCBR%252B%252B%2520by%2520introducing%2520a%2520punishment%250Amechanism%2520to%2520obtain%2520a%2520more%2520class-balanced%2520labeled%2520set.%2520Extensive%2520experiments%2520on%250Amultiple%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approaches%252C%250Aachieving%2520superior%2520performance%2520over%2520state-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10074v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Class-Balanced%20and%20Reinforced%20Active%20Learning%20on%20Graphs&entry.906535625=Chengcheng%20Yu%20and%20Jiapeng%20Zhu%20and%20Xiang%20Li&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20demonstrated%20significant%20success%20in%20various%0Aapplications%2C%20such%20as%20node%20classification%2C%20link%20prediction%2C%20and%20graph%0Aclassification.%20Active%20learning%20for%20GNNs%20aims%20to%20query%20the%20valuable%20samples%0Afrom%20the%20unlabeled%20data%20for%20annotation%20to%20maximize%20the%20GNNs%27%20performance%20at%20a%0Alower%20cost.%20However%2C%20most%20existing%20algorithms%20for%20reinforced%20active%20learning%20in%0AGNNs%20may%20lead%20to%20a%20highly%20imbalanced%20class%20distribution%2C%20especially%20in%20highly%0Askewed%20class%20scenarios.%20GNNs%20trained%20with%20class-imbalanced%20labeled%20data%20are%0Asusceptible%20to%20bias%20toward%20majority%20classes%2C%20and%20the%20lower%20performance%20of%0Aminority%20classes%20may%20lead%20to%20a%20decline%20in%20overall%20performance.%20To%20tackle%20this%0Aissue%2C%20we%20propose%20a%20novel%20class-balanced%20and%20reinforced%20active%20learning%0Aframework%20for%20GNNs%2C%20namely%2C%20GCBR.%20It%20learns%20an%20optimal%20policy%20to%20acquire%0Aclass-balanced%20and%20informative%20nodes%20for%20annotation%2C%20maximizing%20the%20performance%0Aof%20GNNs%20trained%20with%20selected%20labeled%20nodes.%20GCBR%20designs%20class-balance-aware%0Astates%2C%20as%20well%20as%20a%20reward%20function%20that%20achieves%20trade-off%20between%20model%0Aperformance%20and%20class%20balance.%20The%20reinforcement%20learning%20algorithm%20Advantage%0AActor-Critic%20%28A2C%29%20is%20employed%20to%20learn%20an%20optimal%20policy%20stably%20and%0Aefficiently.%20We%20further%20upgrade%20GCBR%20to%20GCBR%2B%2B%20by%20introducing%20a%20punishment%0Amechanism%20to%20obtain%20a%20more%20class-balanced%20labeled%20set.%20Extensive%20experiments%20on%0Amultiple%20datasets%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approaches%2C%0Aachieving%20superior%20performance%20over%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10074v3&entry.124074799=Read"},
{"title": "MonoPCC: Photometric-invariant Cycle Constraint for Monocular Depth\n  Estimation of Endoscopic Images", "author": "Zhiwei Wang and Ying Zhou and Shiquan He and Ting Li and Fan Huang and Qiang Ding and Xinxia Feng and Mei Liu and Qiang Li", "abstract": "  Photometric constraint is indispensable for self-supervised monocular depth\nestimation. It involves warping a source image onto a target view using\nestimated depth&pose, and then minimizing the difference between the warped and\ntarget images. However, the endoscopic built-in light causes significant\nbrightness fluctuations, and thus makes the photometric constraint unreliable.\nPrevious efforts only mitigate this relying on extra models to calibrate image\nbrightness. In this paper, we propose MonoPCC to address the brightness\ninconsistency radically by reshaping the photometric constraint into a cycle\nform. Instead of only warping the source image, MonoPCC constructs a closed\nloop consisting of two opposite forward-backward warping paths: from target to\nsource and then back to target. Thus, the target image finally receives an\nimage cycle-warped from itself, which naturally makes the constraint invariant\nto brightness changes. Moreover, MonoPCC transplants the source image's\nphase-frequency into the intermediate warped image to avoid structure lost, and\nalso stabilizes the training via an exponential moving average (EMA) strategy\nto avoid frequent changes in the forward warping. The comprehensive and\nextensive experimental results on four endoscopic datasets demonstrate that our\nproposed MonoPCC shows a great robustness to the brightness inconsistency, and\nexceeds other state-of-the-arts by reducing the absolute relative error by at\nleast 7.27%, 9.38%, 9.90% and 3.17%, respectively.\n", "link": "http://arxiv.org/abs/2404.16571v2", "date": "2024-05-07", "relevancy": 2.6353, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5394}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5264}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MonoPCC%3A%20Photometric-invariant%20Cycle%20Constraint%20for%20Monocular%20Depth%0A%20%20Estimation%20of%20Endoscopic%20Images&body=Title%3A%20MonoPCC%3A%20Photometric-invariant%20Cycle%20Constraint%20for%20Monocular%20Depth%0A%20%20Estimation%20of%20Endoscopic%20Images%0AAuthor%3A%20Zhiwei%20Wang%20and%20Ying%20Zhou%20and%20Shiquan%20He%20and%20Ting%20Li%20and%20Fan%20Huang%20and%20Qiang%20Ding%20and%20Xinxia%20Feng%20and%20Mei%20Liu%20and%20Qiang%20Li%0AAbstract%3A%20%20%20Photometric%20constraint%20is%20indispensable%20for%20self-supervised%20monocular%20depth%0Aestimation.%20It%20involves%20warping%20a%20source%20image%20onto%20a%20target%20view%20using%0Aestimated%20depth%26pose%2C%20and%20then%20minimizing%20the%20difference%20between%20the%20warped%20and%0Atarget%20images.%20However%2C%20the%20endoscopic%20built-in%20light%20causes%20significant%0Abrightness%20fluctuations%2C%20and%20thus%20makes%20the%20photometric%20constraint%20unreliable.%0APrevious%20efforts%20only%20mitigate%20this%20relying%20on%20extra%20models%20to%20calibrate%20image%0Abrightness.%20In%20this%20paper%2C%20we%20propose%20MonoPCC%20to%20address%20the%20brightness%0Ainconsistency%20radically%20by%20reshaping%20the%20photometric%20constraint%20into%20a%20cycle%0Aform.%20Instead%20of%20only%20warping%20the%20source%20image%2C%20MonoPCC%20constructs%20a%20closed%0Aloop%20consisting%20of%20two%20opposite%20forward-backward%20warping%20paths%3A%20from%20target%20to%0Asource%20and%20then%20back%20to%20target.%20Thus%2C%20the%20target%20image%20finally%20receives%20an%0Aimage%20cycle-warped%20from%20itself%2C%20which%20naturally%20makes%20the%20constraint%20invariant%0Ato%20brightness%20changes.%20Moreover%2C%20MonoPCC%20transplants%20the%20source%20image%27s%0Aphase-frequency%20into%20the%20intermediate%20warped%20image%20to%20avoid%20structure%20lost%2C%20and%0Aalso%20stabilizes%20the%20training%20via%20an%20exponential%20moving%20average%20%28EMA%29%20strategy%0Ato%20avoid%20frequent%20changes%20in%20the%20forward%20warping.%20The%20comprehensive%20and%0Aextensive%20experimental%20results%20on%20four%20endoscopic%20datasets%20demonstrate%20that%20our%0Aproposed%20MonoPCC%20shows%20a%20great%20robustness%20to%20the%20brightness%20inconsistency%2C%20and%0Aexceeds%20other%20state-of-the-arts%20by%20reducing%20the%20absolute%20relative%20error%20by%20at%0Aleast%207.27%25%2C%209.38%25%2C%209.90%25%20and%203.17%25%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16571v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonoPCC%253A%2520Photometric-invariant%2520Cycle%2520Constraint%2520for%2520Monocular%2520Depth%250A%2520%2520Estimation%2520of%2520Endoscopic%2520Images%26entry.906535625%3DZhiwei%2520Wang%2520and%2520Ying%2520Zhou%2520and%2520Shiquan%2520He%2520and%2520Ting%2520Li%2520and%2520Fan%2520Huang%2520and%2520Qiang%2520Ding%2520and%2520Xinxia%2520Feng%2520and%2520Mei%2520Liu%2520and%2520Qiang%2520Li%26entry.1292438233%3D%2520%2520Photometric%2520constraint%2520is%2520indispensable%2520for%2520self-supervised%2520monocular%2520depth%250Aestimation.%2520It%2520involves%2520warping%2520a%2520source%2520image%2520onto%2520a%2520target%2520view%2520using%250Aestimated%2520depth%2526pose%252C%2520and%2520then%2520minimizing%2520the%2520difference%2520between%2520the%2520warped%2520and%250Atarget%2520images.%2520However%252C%2520the%2520endoscopic%2520built-in%2520light%2520causes%2520significant%250Abrightness%2520fluctuations%252C%2520and%2520thus%2520makes%2520the%2520photometric%2520constraint%2520unreliable.%250APrevious%2520efforts%2520only%2520mitigate%2520this%2520relying%2520on%2520extra%2520models%2520to%2520calibrate%2520image%250Abrightness.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MonoPCC%2520to%2520address%2520the%2520brightness%250Ainconsistency%2520radically%2520by%2520reshaping%2520the%2520photometric%2520constraint%2520into%2520a%2520cycle%250Aform.%2520Instead%2520of%2520only%2520warping%2520the%2520source%2520image%252C%2520MonoPCC%2520constructs%2520a%2520closed%250Aloop%2520consisting%2520of%2520two%2520opposite%2520forward-backward%2520warping%2520paths%253A%2520from%2520target%2520to%250Asource%2520and%2520then%2520back%2520to%2520target.%2520Thus%252C%2520the%2520target%2520image%2520finally%2520receives%2520an%250Aimage%2520cycle-warped%2520from%2520itself%252C%2520which%2520naturally%2520makes%2520the%2520constraint%2520invariant%250Ato%2520brightness%2520changes.%2520Moreover%252C%2520MonoPCC%2520transplants%2520the%2520source%2520image%2527s%250Aphase-frequency%2520into%2520the%2520intermediate%2520warped%2520image%2520to%2520avoid%2520structure%2520lost%252C%2520and%250Aalso%2520stabilizes%2520the%2520training%2520via%2520an%2520exponential%2520moving%2520average%2520%2528EMA%2529%2520strategy%250Ato%2520avoid%2520frequent%2520changes%2520in%2520the%2520forward%2520warping.%2520The%2520comprehensive%2520and%250Aextensive%2520experimental%2520results%2520on%2520four%2520endoscopic%2520datasets%2520demonstrate%2520that%2520our%250Aproposed%2520MonoPCC%2520shows%2520a%2520great%2520robustness%2520to%2520the%2520brightness%2520inconsistency%252C%2520and%250Aexceeds%2520other%2520state-of-the-arts%2520by%2520reducing%2520the%2520absolute%2520relative%2520error%2520by%2520at%250Aleast%25207.27%2525%252C%25209.38%2525%252C%25209.90%2525%2520and%25203.17%2525%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16571v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonoPCC%3A%20Photometric-invariant%20Cycle%20Constraint%20for%20Monocular%20Depth%0A%20%20Estimation%20of%20Endoscopic%20Images&entry.906535625=Zhiwei%20Wang%20and%20Ying%20Zhou%20and%20Shiquan%20He%20and%20Ting%20Li%20and%20Fan%20Huang%20and%20Qiang%20Ding%20and%20Xinxia%20Feng%20and%20Mei%20Liu%20and%20Qiang%20Li&entry.1292438233=%20%20Photometric%20constraint%20is%20indispensable%20for%20self-supervised%20monocular%20depth%0Aestimation.%20It%20involves%20warping%20a%20source%20image%20onto%20a%20target%20view%20using%0Aestimated%20depth%26pose%2C%20and%20then%20minimizing%20the%20difference%20between%20the%20warped%20and%0Atarget%20images.%20However%2C%20the%20endoscopic%20built-in%20light%20causes%20significant%0Abrightness%20fluctuations%2C%20and%20thus%20makes%20the%20photometric%20constraint%20unreliable.%0APrevious%20efforts%20only%20mitigate%20this%20relying%20on%20extra%20models%20to%20calibrate%20image%0Abrightness.%20In%20this%20paper%2C%20we%20propose%20MonoPCC%20to%20address%20the%20brightness%0Ainconsistency%20radically%20by%20reshaping%20the%20photometric%20constraint%20into%20a%20cycle%0Aform.%20Instead%20of%20only%20warping%20the%20source%20image%2C%20MonoPCC%20constructs%20a%20closed%0Aloop%20consisting%20of%20two%20opposite%20forward-backward%20warping%20paths%3A%20from%20target%20to%0Asource%20and%20then%20back%20to%20target.%20Thus%2C%20the%20target%20image%20finally%20receives%20an%0Aimage%20cycle-warped%20from%20itself%2C%20which%20naturally%20makes%20the%20constraint%20invariant%0Ato%20brightness%20changes.%20Moreover%2C%20MonoPCC%20transplants%20the%20source%20image%27s%0Aphase-frequency%20into%20the%20intermediate%20warped%20image%20to%20avoid%20structure%20lost%2C%20and%0Aalso%20stabilizes%20the%20training%20via%20an%20exponential%20moving%20average%20%28EMA%29%20strategy%0Ato%20avoid%20frequent%20changes%20in%20the%20forward%20warping.%20The%20comprehensive%20and%0Aextensive%20experimental%20results%20on%20four%20endoscopic%20datasets%20demonstrate%20that%20our%0Aproposed%20MonoPCC%20shows%20a%20great%20robustness%20to%20the%20brightness%20inconsistency%2C%20and%0Aexceeds%20other%20state-of-the-arts%20by%20reducing%20the%20absolute%20relative%20error%20by%20at%0Aleast%207.27%25%2C%209.38%25%2C%209.90%25%20and%203.17%25%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16571v2&entry.124074799=Read"},
{"title": "Introducing a microstructure-embedded autoencoder approach for\n  reconstructing high-resolution solution field data from a reduced parametric\n  space", "author": "Rasoul Najafi Koopas and Shahed Rezaei and Natalie Rauter and Richard Ostwald and Rolf Lammering", "abstract": "  In this study, we develop a novel multi-fidelity deep learning approach that\ntransforms low-fidelity solution maps into high-fidelity ones by incorporating\nparametric space information into a standard autoencoder architecture. This\nmethod's integration of parametric space information significantly reduces the\nneed for training data to effectively predict high-fidelity solutions from\nlow-fidelity ones. In this study, we examine a two-dimensional steady-state\nheat transfer analysis within a highly heterogeneous materials microstructure.\nThe heat conductivity coefficients for two different materials are condensed\nfrom a 101 x 101 grid to smaller grids. We then solve the boundary value\nproblem on the coarsest grid using a pre-trained physics-informed neural\noperator network known as Finite Operator Learning (FOL). The resulting\nlow-fidelity solution is subsequently upscaled back to a 101 x 101 grid using a\nnewly designed enhanced autoencoder. The novelty of the developed enhanced\nautoencoder lies in the concatenation of heat conductivity maps of different\nresolutions to the decoder segment in distinct steps. Hence the developed\nalgorithm is named microstructure-embedded autoencoder (MEA). We compare the\nMEA outcomes with those from finite element methods, the standard U-Net, and\nvarious other upscaling techniques, including interpolation functions and\nfeedforward neural networks (FFNN). Our analysis shows that MEA outperforms\nthese methods in terms of computational efficiency and error on test cases. As\na result, the MEA serves as a potential supplement to neural operator networks,\neffectively upscaling low-fidelity solutions to high fidelity while preserving\ncritical details often lost in traditional upscaling methods, particularly at\nsharp interfaces like those seen with interpolation.\n", "link": "http://arxiv.org/abs/2405.01975v2", "date": "2024-05-07", "relevancy": 2.6199, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5586}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5147}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Introducing%20a%20microstructure-embedded%20autoencoder%20approach%20for%0A%20%20reconstructing%20high-resolution%20solution%20field%20data%20from%20a%20reduced%20parametric%0A%20%20space&body=Title%3A%20Introducing%20a%20microstructure-embedded%20autoencoder%20approach%20for%0A%20%20reconstructing%20high-resolution%20solution%20field%20data%20from%20a%20reduced%20parametric%0A%20%20space%0AAuthor%3A%20Rasoul%20Najafi%20Koopas%20and%20Shahed%20Rezaei%20and%20Natalie%20Rauter%20and%20Richard%20Ostwald%20and%20Rolf%20Lammering%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20develop%20a%20novel%20multi-fidelity%20deep%20learning%20approach%20that%0Atransforms%20low-fidelity%20solution%20maps%20into%20high-fidelity%20ones%20by%20incorporating%0Aparametric%20space%20information%20into%20a%20standard%20autoencoder%20architecture.%20This%0Amethod%27s%20integration%20of%20parametric%20space%20information%20significantly%20reduces%20the%0Aneed%20for%20training%20data%20to%20effectively%20predict%20high-fidelity%20solutions%20from%0Alow-fidelity%20ones.%20In%20this%20study%2C%20we%20examine%20a%20two-dimensional%20steady-state%0Aheat%20transfer%20analysis%20within%20a%20highly%20heterogeneous%20materials%20microstructure.%0AThe%20heat%20conductivity%20coefficients%20for%20two%20different%20materials%20are%20condensed%0Afrom%20a%20101%20x%20101%20grid%20to%20smaller%20grids.%20We%20then%20solve%20the%20boundary%20value%0Aproblem%20on%20the%20coarsest%20grid%20using%20a%20pre-trained%20physics-informed%20neural%0Aoperator%20network%20known%20as%20Finite%20Operator%20Learning%20%28FOL%29.%20The%20resulting%0Alow-fidelity%20solution%20is%20subsequently%20upscaled%20back%20to%20a%20101%20x%20101%20grid%20using%20a%0Anewly%20designed%20enhanced%20autoencoder.%20The%20novelty%20of%20the%20developed%20enhanced%0Aautoencoder%20lies%20in%20the%20concatenation%20of%20heat%20conductivity%20maps%20of%20different%0Aresolutions%20to%20the%20decoder%20segment%20in%20distinct%20steps.%20Hence%20the%20developed%0Aalgorithm%20is%20named%20microstructure-embedded%20autoencoder%20%28MEA%29.%20We%20compare%20the%0AMEA%20outcomes%20with%20those%20from%20finite%20element%20methods%2C%20the%20standard%20U-Net%2C%20and%0Avarious%20other%20upscaling%20techniques%2C%20including%20interpolation%20functions%20and%0Afeedforward%20neural%20networks%20%28FFNN%29.%20Our%20analysis%20shows%20that%20MEA%20outperforms%0Athese%20methods%20in%20terms%20of%20computational%20efficiency%20and%20error%20on%20test%20cases.%20As%0Aa%20result%2C%20the%20MEA%20serves%20as%20a%20potential%20supplement%20to%20neural%20operator%20networks%2C%0Aeffectively%20upscaling%20low-fidelity%20solutions%20to%20high%20fidelity%20while%20preserving%0Acritical%20details%20often%20lost%20in%20traditional%20upscaling%20methods%2C%20particularly%20at%0Asharp%20interfaces%20like%20those%20seen%20with%20interpolation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01975v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntroducing%2520a%2520microstructure-embedded%2520autoencoder%2520approach%2520for%250A%2520%2520reconstructing%2520high-resolution%2520solution%2520field%2520data%2520from%2520a%2520reduced%2520parametric%250A%2520%2520space%26entry.906535625%3DRasoul%2520Najafi%2520Koopas%2520and%2520Shahed%2520Rezaei%2520and%2520Natalie%2520Rauter%2520and%2520Richard%2520Ostwald%2520and%2520Rolf%2520Lammering%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520develop%2520a%2520novel%2520multi-fidelity%2520deep%2520learning%2520approach%2520that%250Atransforms%2520low-fidelity%2520solution%2520maps%2520into%2520high-fidelity%2520ones%2520by%2520incorporating%250Aparametric%2520space%2520information%2520into%2520a%2520standard%2520autoencoder%2520architecture.%2520This%250Amethod%2527s%2520integration%2520of%2520parametric%2520space%2520information%2520significantly%2520reduces%2520the%250Aneed%2520for%2520training%2520data%2520to%2520effectively%2520predict%2520high-fidelity%2520solutions%2520from%250Alow-fidelity%2520ones.%2520In%2520this%2520study%252C%2520we%2520examine%2520a%2520two-dimensional%2520steady-state%250Aheat%2520transfer%2520analysis%2520within%2520a%2520highly%2520heterogeneous%2520materials%2520microstructure.%250AThe%2520heat%2520conductivity%2520coefficients%2520for%2520two%2520different%2520materials%2520are%2520condensed%250Afrom%2520a%2520101%2520x%2520101%2520grid%2520to%2520smaller%2520grids.%2520We%2520then%2520solve%2520the%2520boundary%2520value%250Aproblem%2520on%2520the%2520coarsest%2520grid%2520using%2520a%2520pre-trained%2520physics-informed%2520neural%250Aoperator%2520network%2520known%2520as%2520Finite%2520Operator%2520Learning%2520%2528FOL%2529.%2520The%2520resulting%250Alow-fidelity%2520solution%2520is%2520subsequently%2520upscaled%2520back%2520to%2520a%2520101%2520x%2520101%2520grid%2520using%2520a%250Anewly%2520designed%2520enhanced%2520autoencoder.%2520The%2520novelty%2520of%2520the%2520developed%2520enhanced%250Aautoencoder%2520lies%2520in%2520the%2520concatenation%2520of%2520heat%2520conductivity%2520maps%2520of%2520different%250Aresolutions%2520to%2520the%2520decoder%2520segment%2520in%2520distinct%2520steps.%2520Hence%2520the%2520developed%250Aalgorithm%2520is%2520named%2520microstructure-embedded%2520autoencoder%2520%2528MEA%2529.%2520We%2520compare%2520the%250AMEA%2520outcomes%2520with%2520those%2520from%2520finite%2520element%2520methods%252C%2520the%2520standard%2520U-Net%252C%2520and%250Avarious%2520other%2520upscaling%2520techniques%252C%2520including%2520interpolation%2520functions%2520and%250Afeedforward%2520neural%2520networks%2520%2528FFNN%2529.%2520Our%2520analysis%2520shows%2520that%2520MEA%2520outperforms%250Athese%2520methods%2520in%2520terms%2520of%2520computational%2520efficiency%2520and%2520error%2520on%2520test%2520cases.%2520As%250Aa%2520result%252C%2520the%2520MEA%2520serves%2520as%2520a%2520potential%2520supplement%2520to%2520neural%2520operator%2520networks%252C%250Aeffectively%2520upscaling%2520low-fidelity%2520solutions%2520to%2520high%2520fidelity%2520while%2520preserving%250Acritical%2520details%2520often%2520lost%2520in%2520traditional%2520upscaling%2520methods%252C%2520particularly%2520at%250Asharp%2520interfaces%2520like%2520those%2520seen%2520with%2520interpolation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01975v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Introducing%20a%20microstructure-embedded%20autoencoder%20approach%20for%0A%20%20reconstructing%20high-resolution%20solution%20field%20data%20from%20a%20reduced%20parametric%0A%20%20space&entry.906535625=Rasoul%20Najafi%20Koopas%20and%20Shahed%20Rezaei%20and%20Natalie%20Rauter%20and%20Richard%20Ostwald%20and%20Rolf%20Lammering&entry.1292438233=%20%20In%20this%20study%2C%20we%20develop%20a%20novel%20multi-fidelity%20deep%20learning%20approach%20that%0Atransforms%20low-fidelity%20solution%20maps%20into%20high-fidelity%20ones%20by%20incorporating%0Aparametric%20space%20information%20into%20a%20standard%20autoencoder%20architecture.%20This%0Amethod%27s%20integration%20of%20parametric%20space%20information%20significantly%20reduces%20the%0Aneed%20for%20training%20data%20to%20effectively%20predict%20high-fidelity%20solutions%20from%0Alow-fidelity%20ones.%20In%20this%20study%2C%20we%20examine%20a%20two-dimensional%20steady-state%0Aheat%20transfer%20analysis%20within%20a%20highly%20heterogeneous%20materials%20microstructure.%0AThe%20heat%20conductivity%20coefficients%20for%20two%20different%20materials%20are%20condensed%0Afrom%20a%20101%20x%20101%20grid%20to%20smaller%20grids.%20We%20then%20solve%20the%20boundary%20value%0Aproblem%20on%20the%20coarsest%20grid%20using%20a%20pre-trained%20physics-informed%20neural%0Aoperator%20network%20known%20as%20Finite%20Operator%20Learning%20%28FOL%29.%20The%20resulting%0Alow-fidelity%20solution%20is%20subsequently%20upscaled%20back%20to%20a%20101%20x%20101%20grid%20using%20a%0Anewly%20designed%20enhanced%20autoencoder.%20The%20novelty%20of%20the%20developed%20enhanced%0Aautoencoder%20lies%20in%20the%20concatenation%20of%20heat%20conductivity%20maps%20of%20different%0Aresolutions%20to%20the%20decoder%20segment%20in%20distinct%20steps.%20Hence%20the%20developed%0Aalgorithm%20is%20named%20microstructure-embedded%20autoencoder%20%28MEA%29.%20We%20compare%20the%0AMEA%20outcomes%20with%20those%20from%20finite%20element%20methods%2C%20the%20standard%20U-Net%2C%20and%0Avarious%20other%20upscaling%20techniques%2C%20including%20interpolation%20functions%20and%0Afeedforward%20neural%20networks%20%28FFNN%29.%20Our%20analysis%20shows%20that%20MEA%20outperforms%0Athese%20methods%20in%20terms%20of%20computational%20efficiency%20and%20error%20on%20test%20cases.%20As%0Aa%20result%2C%20the%20MEA%20serves%20as%20a%20potential%20supplement%20to%20neural%20operator%20networks%2C%0Aeffectively%20upscaling%20low-fidelity%20solutions%20to%20high%20fidelity%20while%20preserving%0Acritical%20details%20often%20lost%20in%20traditional%20upscaling%20methods%2C%20particularly%20at%0Asharp%20interfaces%20like%20those%20seen%20with%20interpolation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01975v2&entry.124074799=Read"},
{"title": "Vidu: a Highly Consistent, Dynamic and Skilled Text-to-Video Generator\n  with Diffusion Models", "author": "Fan Bao and Chendong Xiang and Gang Yue and Guande He and Hongzhou Zhu and Kaiwen Zheng and Min Zhao and Shilong Liu and Yaole Wang and Jun Zhu", "abstract": "  We introduce Vidu, a high-performance text-to-video generator that is capable\nof producing 1080p videos up to 16 seconds in a single generation. Vidu is a\ndiffusion model with U-ViT as its backbone, which unlocks the scalability and\nthe capability for handling long videos. Vidu exhibits strong coherence and\ndynamism, and is capable of generating both realistic and imaginative videos,\nas well as understanding some professional photography techniques, on par with\nSora -- the most powerful reported text-to-video generator. Finally, we perform\ninitial experiments on other controllable video generation, including\ncanny-to-video generation, video prediction and subject-driven generation,\nwhich demonstrate promising results.\n", "link": "http://arxiv.org/abs/2405.04233v1", "date": "2024-05-07", "relevancy": 2.604, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.698}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6611}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vidu%3A%20a%20Highly%20Consistent%2C%20Dynamic%20and%20Skilled%20Text-to-Video%20Generator%0A%20%20with%20Diffusion%20Models&body=Title%3A%20Vidu%3A%20a%20Highly%20Consistent%2C%20Dynamic%20and%20Skilled%20Text-to-Video%20Generator%0A%20%20with%20Diffusion%20Models%0AAuthor%3A%20Fan%20Bao%20and%20Chendong%20Xiang%20and%20Gang%20Yue%20and%20Guande%20He%20and%20Hongzhou%20Zhu%20and%20Kaiwen%20Zheng%20and%20Min%20Zhao%20and%20Shilong%20Liu%20and%20Yaole%20Wang%20and%20Jun%20Zhu%0AAbstract%3A%20%20%20We%20introduce%20Vidu%2C%20a%20high-performance%20text-to-video%20generator%20that%20is%20capable%0Aof%20producing%201080p%20videos%20up%20to%2016%20seconds%20in%20a%20single%20generation.%20Vidu%20is%20a%0Adiffusion%20model%20with%20U-ViT%20as%20its%20backbone%2C%20which%20unlocks%20the%20scalability%20and%0Athe%20capability%20for%20handling%20long%20videos.%20Vidu%20exhibits%20strong%20coherence%20and%0Adynamism%2C%20and%20is%20capable%20of%20generating%20both%20realistic%20and%20imaginative%20videos%2C%0Aas%20well%20as%20understanding%20some%20professional%20photography%20techniques%2C%20on%20par%20with%0ASora%20--%20the%20most%20powerful%20reported%20text-to-video%20generator.%20Finally%2C%20we%20perform%0Ainitial%20experiments%20on%20other%20controllable%20video%20generation%2C%20including%0Acanny-to-video%20generation%2C%20video%20prediction%20and%20subject-driven%20generation%2C%0Awhich%20demonstrate%20promising%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04233v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVidu%253A%2520a%2520Highly%2520Consistent%252C%2520Dynamic%2520and%2520Skilled%2520Text-to-Video%2520Generator%250A%2520%2520with%2520Diffusion%2520Models%26entry.906535625%3DFan%2520Bao%2520and%2520Chendong%2520Xiang%2520and%2520Gang%2520Yue%2520and%2520Guande%2520He%2520and%2520Hongzhou%2520Zhu%2520and%2520Kaiwen%2520Zheng%2520and%2520Min%2520Zhao%2520and%2520Shilong%2520Liu%2520and%2520Yaole%2520Wang%2520and%2520Jun%2520Zhu%26entry.1292438233%3D%2520%2520We%2520introduce%2520Vidu%252C%2520a%2520high-performance%2520text-to-video%2520generator%2520that%2520is%2520capable%250Aof%2520producing%25201080p%2520videos%2520up%2520to%252016%2520seconds%2520in%2520a%2520single%2520generation.%2520Vidu%2520is%2520a%250Adiffusion%2520model%2520with%2520U-ViT%2520as%2520its%2520backbone%252C%2520which%2520unlocks%2520the%2520scalability%2520and%250Athe%2520capability%2520for%2520handling%2520long%2520videos.%2520Vidu%2520exhibits%2520strong%2520coherence%2520and%250Adynamism%252C%2520and%2520is%2520capable%2520of%2520generating%2520both%2520realistic%2520and%2520imaginative%2520videos%252C%250Aas%2520well%2520as%2520understanding%2520some%2520professional%2520photography%2520techniques%252C%2520on%2520par%2520with%250ASora%2520--%2520the%2520most%2520powerful%2520reported%2520text-to-video%2520generator.%2520Finally%252C%2520we%2520perform%250Ainitial%2520experiments%2520on%2520other%2520controllable%2520video%2520generation%252C%2520including%250Acanny-to-video%2520generation%252C%2520video%2520prediction%2520and%2520subject-driven%2520generation%252C%250Awhich%2520demonstrate%2520promising%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04233v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vidu%3A%20a%20Highly%20Consistent%2C%20Dynamic%20and%20Skilled%20Text-to-Video%20Generator%0A%20%20with%20Diffusion%20Models&entry.906535625=Fan%20Bao%20and%20Chendong%20Xiang%20and%20Gang%20Yue%20and%20Guande%20He%20and%20Hongzhou%20Zhu%20and%20Kaiwen%20Zheng%20and%20Min%20Zhao%20and%20Shilong%20Liu%20and%20Yaole%20Wang%20and%20Jun%20Zhu&entry.1292438233=%20%20We%20introduce%20Vidu%2C%20a%20high-performance%20text-to-video%20generator%20that%20is%20capable%0Aof%20producing%201080p%20videos%20up%20to%2016%20seconds%20in%20a%20single%20generation.%20Vidu%20is%20a%0Adiffusion%20model%20with%20U-ViT%20as%20its%20backbone%2C%20which%20unlocks%20the%20scalability%20and%0Athe%20capability%20for%20handling%20long%20videos.%20Vidu%20exhibits%20strong%20coherence%20and%0Adynamism%2C%20and%20is%20capable%20of%20generating%20both%20realistic%20and%20imaginative%20videos%2C%0Aas%20well%20as%20understanding%20some%20professional%20photography%20techniques%2C%20on%20par%20with%0ASora%20--%20the%20most%20powerful%20reported%20text-to-video%20generator.%20Finally%2C%20we%20perform%0Ainitial%20experiments%20on%20other%20controllable%20video%20generation%2C%20including%0Acanny-to-video%20generation%2C%20video%20prediction%20and%20subject-driven%20generation%2C%0Awhich%20demonstrate%20promising%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04233v1&entry.124074799=Read"},
{"title": "GraphGPT: Graph Instruction Tuning for Large Language Models", "author": "Jiabin Tang and Yuhao Yang and Wei Wei and Lei Shi and Lixin Su and Suqi Cheng and Dawei Yin and Chao Huang", "abstract": "  Graph Neural Networks (GNNs) have evolved to understand graph structures\nthrough recursive exchanges and aggregations among nodes. To enhance\nrobustness, self-supervised learning (SSL) has become a vital tool for data\naugmentation. Traditional methods often depend on fine-tuning with\ntask-specific labels, limiting their effectiveness when labeled data is scarce.\nOur research tackles this by advancing graph model generalization in zero-shot\nlearning environments. Inspired by the success of large language models (LLMs),\nwe aim to create a graph-oriented LLM capable of exceptional generalization\nacross various datasets and tasks without relying on downstream graph data. We\nintroduce the GraphGPT framework, which integrates LLMs with graph structural\nknowledge through graph instruction tuning. This framework includes a\ntext-graph grounding component to link textual and graph structures and a\ndual-stage instruction tuning approach with a lightweight graph-text alignment\nprojector. These innovations allow LLMs to comprehend complex graph structures\nand enhance adaptability across diverse datasets and tasks. Our framework\ndemonstrates superior generalization in both supervised and zero-shot graph\nlearning tasks, surpassing existing benchmarks. The open-sourced model\nimplementation of our GraphGPT is available at\nhttps://github.com/HKUDS/GraphGPT.\n", "link": "http://arxiv.org/abs/2310.13023v3", "date": "2024-05-07", "relevancy": 2.5976, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5434}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5124}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphGPT%3A%20Graph%20Instruction%20Tuning%20for%20Large%20Language%20Models&body=Title%3A%20GraphGPT%3A%20Graph%20Instruction%20Tuning%20for%20Large%20Language%20Models%0AAuthor%3A%20Jiabin%20Tang%20and%20Yuhao%20Yang%20and%20Wei%20Wei%20and%20Lei%20Shi%20and%20Lixin%20Su%20and%20Suqi%20Cheng%20and%20Dawei%20Yin%20and%20Chao%20Huang%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20evolved%20to%20understand%20graph%20structures%0Athrough%20recursive%20exchanges%20and%20aggregations%20among%20nodes.%20To%20enhance%0Arobustness%2C%20self-supervised%20learning%20%28SSL%29%20has%20become%20a%20vital%20tool%20for%20data%0Aaugmentation.%20Traditional%20methods%20often%20depend%20on%20fine-tuning%20with%0Atask-specific%20labels%2C%20limiting%20their%20effectiveness%20when%20labeled%20data%20is%20scarce.%0AOur%20research%20tackles%20this%20by%20advancing%20graph%20model%20generalization%20in%20zero-shot%0Alearning%20environments.%20Inspired%20by%20the%20success%20of%20large%20language%20models%20%28LLMs%29%2C%0Awe%20aim%20to%20create%20a%20graph-oriented%20LLM%20capable%20of%20exceptional%20generalization%0Aacross%20various%20datasets%20and%20tasks%20without%20relying%20on%20downstream%20graph%20data.%20We%0Aintroduce%20the%20GraphGPT%20framework%2C%20which%20integrates%20LLMs%20with%20graph%20structural%0Aknowledge%20through%20graph%20instruction%20tuning.%20This%20framework%20includes%20a%0Atext-graph%20grounding%20component%20to%20link%20textual%20and%20graph%20structures%20and%20a%0Adual-stage%20instruction%20tuning%20approach%20with%20a%20lightweight%20graph-text%20alignment%0Aprojector.%20These%20innovations%20allow%20LLMs%20to%20comprehend%20complex%20graph%20structures%0Aand%20enhance%20adaptability%20across%20diverse%20datasets%20and%20tasks.%20Our%20framework%0Ademonstrates%20superior%20generalization%20in%20both%20supervised%20and%20zero-shot%20graph%0Alearning%20tasks%2C%20surpassing%20existing%20benchmarks.%20The%20open-sourced%20model%0Aimplementation%20of%20our%20GraphGPT%20is%20available%20at%0Ahttps%3A//github.com/HKUDS/GraphGPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.13023v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphGPT%253A%2520Graph%2520Instruction%2520Tuning%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DJiabin%2520Tang%2520and%2520Yuhao%2520Yang%2520and%2520Wei%2520Wei%2520and%2520Lei%2520Shi%2520and%2520Lixin%2520Su%2520and%2520Suqi%2520Cheng%2520and%2520Dawei%2520Yin%2520and%2520Chao%2520Huang%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520evolved%2520to%2520understand%2520graph%2520structures%250Athrough%2520recursive%2520exchanges%2520and%2520aggregations%2520among%2520nodes.%2520To%2520enhance%250Arobustness%252C%2520self-supervised%2520learning%2520%2528SSL%2529%2520has%2520become%2520a%2520vital%2520tool%2520for%2520data%250Aaugmentation.%2520Traditional%2520methods%2520often%2520depend%2520on%2520fine-tuning%2520with%250Atask-specific%2520labels%252C%2520limiting%2520their%2520effectiveness%2520when%2520labeled%2520data%2520is%2520scarce.%250AOur%2520research%2520tackles%2520this%2520by%2520advancing%2520graph%2520model%2520generalization%2520in%2520zero-shot%250Alearning%2520environments.%2520Inspired%2520by%2520the%2520success%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%250Awe%2520aim%2520to%2520create%2520a%2520graph-oriented%2520LLM%2520capable%2520of%2520exceptional%2520generalization%250Aacross%2520various%2520datasets%2520and%2520tasks%2520without%2520relying%2520on%2520downstream%2520graph%2520data.%2520We%250Aintroduce%2520the%2520GraphGPT%2520framework%252C%2520which%2520integrates%2520LLMs%2520with%2520graph%2520structural%250Aknowledge%2520through%2520graph%2520instruction%2520tuning.%2520This%2520framework%2520includes%2520a%250Atext-graph%2520grounding%2520component%2520to%2520link%2520textual%2520and%2520graph%2520structures%2520and%2520a%250Adual-stage%2520instruction%2520tuning%2520approach%2520with%2520a%2520lightweight%2520graph-text%2520alignment%250Aprojector.%2520These%2520innovations%2520allow%2520LLMs%2520to%2520comprehend%2520complex%2520graph%2520structures%250Aand%2520enhance%2520adaptability%2520across%2520diverse%2520datasets%2520and%2520tasks.%2520Our%2520framework%250Ademonstrates%2520superior%2520generalization%2520in%2520both%2520supervised%2520and%2520zero-shot%2520graph%250Alearning%2520tasks%252C%2520surpassing%2520existing%2520benchmarks.%2520The%2520open-sourced%2520model%250Aimplementation%2520of%2520our%2520GraphGPT%2520is%2520available%2520at%250Ahttps%253A//github.com/HKUDS/GraphGPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.13023v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphGPT%3A%20Graph%20Instruction%20Tuning%20for%20Large%20Language%20Models&entry.906535625=Jiabin%20Tang%20and%20Yuhao%20Yang%20and%20Wei%20Wei%20and%20Lei%20Shi%20and%20Lixin%20Su%20and%20Suqi%20Cheng%20and%20Dawei%20Yin%20and%20Chao%20Huang&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20evolved%20to%20understand%20graph%20structures%0Athrough%20recursive%20exchanges%20and%20aggregations%20among%20nodes.%20To%20enhance%0Arobustness%2C%20self-supervised%20learning%20%28SSL%29%20has%20become%20a%20vital%20tool%20for%20data%0Aaugmentation.%20Traditional%20methods%20often%20depend%20on%20fine-tuning%20with%0Atask-specific%20labels%2C%20limiting%20their%20effectiveness%20when%20labeled%20data%20is%20scarce.%0AOur%20research%20tackles%20this%20by%20advancing%20graph%20model%20generalization%20in%20zero-shot%0Alearning%20environments.%20Inspired%20by%20the%20success%20of%20large%20language%20models%20%28LLMs%29%2C%0Awe%20aim%20to%20create%20a%20graph-oriented%20LLM%20capable%20of%20exceptional%20generalization%0Aacross%20various%20datasets%20and%20tasks%20without%20relying%20on%20downstream%20graph%20data.%20We%0Aintroduce%20the%20GraphGPT%20framework%2C%20which%20integrates%20LLMs%20with%20graph%20structural%0Aknowledge%20through%20graph%20instruction%20tuning.%20This%20framework%20includes%20a%0Atext-graph%20grounding%20component%20to%20link%20textual%20and%20graph%20structures%20and%20a%0Adual-stage%20instruction%20tuning%20approach%20with%20a%20lightweight%20graph-text%20alignment%0Aprojector.%20These%20innovations%20allow%20LLMs%20to%20comprehend%20complex%20graph%20structures%0Aand%20enhance%20adaptability%20across%20diverse%20datasets%20and%20tasks.%20Our%20framework%0Ademonstrates%20superior%20generalization%20in%20both%20supervised%20and%20zero-shot%20graph%0Alearning%20tasks%2C%20surpassing%20existing%20benchmarks.%20The%20open-sourced%20model%0Aimplementation%20of%20our%20GraphGPT%20is%20available%20at%0Ahttps%3A//github.com/HKUDS/GraphGPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.13023v3&entry.124074799=Read"},
{"title": "Zero Grads: Learning Local Surrogate Losses for Non-Differentiable\n  Graphics", "author": "Michael Fischer and Tobias Ritschel", "abstract": "  Gradient-based optimization is now ubiquitous across graphics, but\nunfortunately can not be applied to problems with undefined or zero gradients.\nTo circumvent this issue, the loss function can be manually replaced by a\n``surrogate'' that has similar minima but is differentiable. Our proposed\nframework, ZeroGrads, automates this process by learning a neural approximation\nof the objective function, which in turn can be used to differentiate through\narbitrary black-box graphics pipelines. We train the surrogate on an actively\nsmoothed version of the objective and encourage locality, focusing the\nsurrogate's capacity on what matters at the current training episode. The\nfitting is performed online, alongside the parameter optimization, and\nself-supervised, without pre-computed data or pre-trained models. As sampling\nthe objective is expensive (it requires a full rendering or simulator run), we\ndevise an efficient sampling scheme that allows for tractable run-times and\ncompetitive performance at little overhead. We demonstrate optimizing diverse\nnon-convex, non-differentiable black-box problems in graphics, such as\nvisibility in rendering, discrete parameter spaces in procedural modelling or\noptimal control in physics-driven animation. In contrast to other\nderivative-free algorithms, our approach scales well to higher dimensions,\nwhich we demonstrate on problems with up to 35k interlinked variables.\n", "link": "http://arxiv.org/abs/2308.05739v2", "date": "2024-05-07", "relevancy": 2.5615, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5189}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5091}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero%20Grads%3A%20Learning%20Local%20Surrogate%20Losses%20for%20Non-Differentiable%0A%20%20Graphics&body=Title%3A%20Zero%20Grads%3A%20Learning%20Local%20Surrogate%20Losses%20for%20Non-Differentiable%0A%20%20Graphics%0AAuthor%3A%20Michael%20Fischer%20and%20Tobias%20Ritschel%0AAbstract%3A%20%20%20Gradient-based%20optimization%20is%20now%20ubiquitous%20across%20graphics%2C%20but%0Aunfortunately%20can%20not%20be%20applied%20to%20problems%20with%20undefined%20or%20zero%20gradients.%0ATo%20circumvent%20this%20issue%2C%20the%20loss%20function%20can%20be%20manually%20replaced%20by%20a%0A%60%60surrogate%27%27%20that%20has%20similar%20minima%20but%20is%20differentiable.%20Our%20proposed%0Aframework%2C%20ZeroGrads%2C%20automates%20this%20process%20by%20learning%20a%20neural%20approximation%0Aof%20the%20objective%20function%2C%20which%20in%20turn%20can%20be%20used%20to%20differentiate%20through%0Aarbitrary%20black-box%20graphics%20pipelines.%20We%20train%20the%20surrogate%20on%20an%20actively%0Asmoothed%20version%20of%20the%20objective%20and%20encourage%20locality%2C%20focusing%20the%0Asurrogate%27s%20capacity%20on%20what%20matters%20at%20the%20current%20training%20episode.%20The%0Afitting%20is%20performed%20online%2C%20alongside%20the%20parameter%20optimization%2C%20and%0Aself-supervised%2C%20without%20pre-computed%20data%20or%20pre-trained%20models.%20As%20sampling%0Athe%20objective%20is%20expensive%20%28it%20requires%20a%20full%20rendering%20or%20simulator%20run%29%2C%20we%0Adevise%20an%20efficient%20sampling%20scheme%20that%20allows%20for%20tractable%20run-times%20and%0Acompetitive%20performance%20at%20little%20overhead.%20We%20demonstrate%20optimizing%20diverse%0Anon-convex%2C%20non-differentiable%20black-box%20problems%20in%20graphics%2C%20such%20as%0Avisibility%20in%20rendering%2C%20discrete%20parameter%20spaces%20in%20procedural%20modelling%20or%0Aoptimal%20control%20in%20physics-driven%20animation.%20In%20contrast%20to%20other%0Aderivative-free%20algorithms%2C%20our%20approach%20scales%20well%20to%20higher%20dimensions%2C%0Awhich%20we%20demonstrate%20on%20problems%20with%20up%20to%2035k%20interlinked%20variables.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.05739v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero%2520Grads%253A%2520Learning%2520Local%2520Surrogate%2520Losses%2520for%2520Non-Differentiable%250A%2520%2520Graphics%26entry.906535625%3DMichael%2520Fischer%2520and%2520Tobias%2520Ritschel%26entry.1292438233%3D%2520%2520Gradient-based%2520optimization%2520is%2520now%2520ubiquitous%2520across%2520graphics%252C%2520but%250Aunfortunately%2520can%2520not%2520be%2520applied%2520to%2520problems%2520with%2520undefined%2520or%2520zero%2520gradients.%250ATo%2520circumvent%2520this%2520issue%252C%2520the%2520loss%2520function%2520can%2520be%2520manually%2520replaced%2520by%2520a%250A%2560%2560surrogate%2527%2527%2520that%2520has%2520similar%2520minima%2520but%2520is%2520differentiable.%2520Our%2520proposed%250Aframework%252C%2520ZeroGrads%252C%2520automates%2520this%2520process%2520by%2520learning%2520a%2520neural%2520approximation%250Aof%2520the%2520objective%2520function%252C%2520which%2520in%2520turn%2520can%2520be%2520used%2520to%2520differentiate%2520through%250Aarbitrary%2520black-box%2520graphics%2520pipelines.%2520We%2520train%2520the%2520surrogate%2520on%2520an%2520actively%250Asmoothed%2520version%2520of%2520the%2520objective%2520and%2520encourage%2520locality%252C%2520focusing%2520the%250Asurrogate%2527s%2520capacity%2520on%2520what%2520matters%2520at%2520the%2520current%2520training%2520episode.%2520The%250Afitting%2520is%2520performed%2520online%252C%2520alongside%2520the%2520parameter%2520optimization%252C%2520and%250Aself-supervised%252C%2520without%2520pre-computed%2520data%2520or%2520pre-trained%2520models.%2520As%2520sampling%250Athe%2520objective%2520is%2520expensive%2520%2528it%2520requires%2520a%2520full%2520rendering%2520or%2520simulator%2520run%2529%252C%2520we%250Adevise%2520an%2520efficient%2520sampling%2520scheme%2520that%2520allows%2520for%2520tractable%2520run-times%2520and%250Acompetitive%2520performance%2520at%2520little%2520overhead.%2520We%2520demonstrate%2520optimizing%2520diverse%250Anon-convex%252C%2520non-differentiable%2520black-box%2520problems%2520in%2520graphics%252C%2520such%2520as%250Avisibility%2520in%2520rendering%252C%2520discrete%2520parameter%2520spaces%2520in%2520procedural%2520modelling%2520or%250Aoptimal%2520control%2520in%2520physics-driven%2520animation.%2520In%2520contrast%2520to%2520other%250Aderivative-free%2520algorithms%252C%2520our%2520approach%2520scales%2520well%2520to%2520higher%2520dimensions%252C%250Awhich%2520we%2520demonstrate%2520on%2520problems%2520with%2520up%2520to%252035k%2520interlinked%2520variables.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.05739v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero%20Grads%3A%20Learning%20Local%20Surrogate%20Losses%20for%20Non-Differentiable%0A%20%20Graphics&entry.906535625=Michael%20Fischer%20and%20Tobias%20Ritschel&entry.1292438233=%20%20Gradient-based%20optimization%20is%20now%20ubiquitous%20across%20graphics%2C%20but%0Aunfortunately%20can%20not%20be%20applied%20to%20problems%20with%20undefined%20or%20zero%20gradients.%0ATo%20circumvent%20this%20issue%2C%20the%20loss%20function%20can%20be%20manually%20replaced%20by%20a%0A%60%60surrogate%27%27%20that%20has%20similar%20minima%20but%20is%20differentiable.%20Our%20proposed%0Aframework%2C%20ZeroGrads%2C%20automates%20this%20process%20by%20learning%20a%20neural%20approximation%0Aof%20the%20objective%20function%2C%20which%20in%20turn%20can%20be%20used%20to%20differentiate%20through%0Aarbitrary%20black-box%20graphics%20pipelines.%20We%20train%20the%20surrogate%20on%20an%20actively%0Asmoothed%20version%20of%20the%20objective%20and%20encourage%20locality%2C%20focusing%20the%0Asurrogate%27s%20capacity%20on%20what%20matters%20at%20the%20current%20training%20episode.%20The%0Afitting%20is%20performed%20online%2C%20alongside%20the%20parameter%20optimization%2C%20and%0Aself-supervised%2C%20without%20pre-computed%20data%20or%20pre-trained%20models.%20As%20sampling%0Athe%20objective%20is%20expensive%20%28it%20requires%20a%20full%20rendering%20or%20simulator%20run%29%2C%20we%0Adevise%20an%20efficient%20sampling%20scheme%20that%20allows%20for%20tractable%20run-times%20and%0Acompetitive%20performance%20at%20little%20overhead.%20We%20demonstrate%20optimizing%20diverse%0Anon-convex%2C%20non-differentiable%20black-box%20problems%20in%20graphics%2C%20such%20as%0Avisibility%20in%20rendering%2C%20discrete%20parameter%20spaces%20in%20procedural%20modelling%20or%0Aoptimal%20control%20in%20physics-driven%20animation.%20In%20contrast%20to%20other%0Aderivative-free%20algorithms%2C%20our%20approach%20scales%20well%20to%20higher%20dimensions%2C%0Awhich%20we%20demonstrate%20on%20problems%20with%20up%20to%2035k%20interlinked%20variables.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.05739v2&entry.124074799=Read"},
{"title": "Breast Histopathology Image Retrieval by Attention-based Adversarially\n  Regularized Variational Graph Autoencoder with Contrastive Learning-Based\n  Feature Extraction", "author": "Nematollah Saeidi and Hossein Karshenas and Bijan Shoushtarian and Sepideh Hatamikia and Ramona Woitek and Amirreza Mahbod", "abstract": "  Breast cancer is a significant global health concern, particularly for women.\nEarly detection and appropriate treatment are crucial in mitigating its impact,\nwith histopathology examinations playing a vital role in swift diagnosis.\nHowever, these examinations often require a substantial workforce and\nexperienced medical experts for proper recognition and cancer grading.\nAutomated image retrieval systems have the potential to assist pathologists in\nidentifying cancerous tissues, thereby accelerating the diagnostic process.\nNevertheless, due to considerable variability among the tissue and cell\npatterns in histological images, proposing an accurate image retrieval model is\nvery challenging.\n  This work introduces a novel attention-based adversarially regularized\nvariational graph autoencoder model for breast histological image retrieval.\nAdditionally, we incorporated cluster-guided contrastive learning as the graph\nfeature extractor to boost the retrieval performance. We evaluated the proposed\nmodel's performance on two publicly available datasets of breast cancer\nhistological images and achieved superior or very competitive retrieval\nperformance, with average mAP scores of 96.5% for the BreakHis dataset and\n94.7% for the BACH dataset, and mVP scores of 91.9% and 91.3%, respectively.\n  Our proposed retrieval model has the potential to be used in clinical\nsettings to enhance diagnostic performance and ultimately benefit patients.\n", "link": "http://arxiv.org/abs/2405.04211v1", "date": "2024-05-07", "relevancy": 2.5349, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.536}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4932}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breast%20Histopathology%20Image%20Retrieval%20by%20Attention-based%20Adversarially%0A%20%20Regularized%20Variational%20Graph%20Autoencoder%20with%20Contrastive%20Learning-Based%0A%20%20Feature%20Extraction&body=Title%3A%20Breast%20Histopathology%20Image%20Retrieval%20by%20Attention-based%20Adversarially%0A%20%20Regularized%20Variational%20Graph%20Autoencoder%20with%20Contrastive%20Learning-Based%0A%20%20Feature%20Extraction%0AAuthor%3A%20Nematollah%20Saeidi%20and%20Hossein%20Karshenas%20and%20Bijan%20Shoushtarian%20and%20Sepideh%20Hatamikia%20and%20Ramona%20Woitek%20and%20Amirreza%20Mahbod%0AAbstract%3A%20%20%20Breast%20cancer%20is%20a%20significant%20global%20health%20concern%2C%20particularly%20for%20women.%0AEarly%20detection%20and%20appropriate%20treatment%20are%20crucial%20in%20mitigating%20its%20impact%2C%0Awith%20histopathology%20examinations%20playing%20a%20vital%20role%20in%20swift%20diagnosis.%0AHowever%2C%20these%20examinations%20often%20require%20a%20substantial%20workforce%20and%0Aexperienced%20medical%20experts%20for%20proper%20recognition%20and%20cancer%20grading.%0AAutomated%20image%20retrieval%20systems%20have%20the%20potential%20to%20assist%20pathologists%20in%0Aidentifying%20cancerous%20tissues%2C%20thereby%20accelerating%20the%20diagnostic%20process.%0ANevertheless%2C%20due%20to%20considerable%20variability%20among%20the%20tissue%20and%20cell%0Apatterns%20in%20histological%20images%2C%20proposing%20an%20accurate%20image%20retrieval%20model%20is%0Avery%20challenging.%0A%20%20This%20work%20introduces%20a%20novel%20attention-based%20adversarially%20regularized%0Avariational%20graph%20autoencoder%20model%20for%20breast%20histological%20image%20retrieval.%0AAdditionally%2C%20we%20incorporated%20cluster-guided%20contrastive%20learning%20as%20the%20graph%0Afeature%20extractor%20to%20boost%20the%20retrieval%20performance.%20We%20evaluated%20the%20proposed%0Amodel%27s%20performance%20on%20two%20publicly%20available%20datasets%20of%20breast%20cancer%0Ahistological%20images%20and%20achieved%20superior%20or%20very%20competitive%20retrieval%0Aperformance%2C%20with%20average%20mAP%20scores%20of%2096.5%25%20for%20the%20BreakHis%20dataset%20and%0A94.7%25%20for%20the%20BACH%20dataset%2C%20and%20mVP%20scores%20of%2091.9%25%20and%2091.3%25%2C%20respectively.%0A%20%20Our%20proposed%20retrieval%20model%20has%20the%20potential%20to%20be%20used%20in%20clinical%0Asettings%20to%20enhance%20diagnostic%20performance%20and%20ultimately%20benefit%20patients.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreast%2520Histopathology%2520Image%2520Retrieval%2520by%2520Attention-based%2520Adversarially%250A%2520%2520Regularized%2520Variational%2520Graph%2520Autoencoder%2520with%2520Contrastive%2520Learning-Based%250A%2520%2520Feature%2520Extraction%26entry.906535625%3DNematollah%2520Saeidi%2520and%2520Hossein%2520Karshenas%2520and%2520Bijan%2520Shoushtarian%2520and%2520Sepideh%2520Hatamikia%2520and%2520Ramona%2520Woitek%2520and%2520Amirreza%2520Mahbod%26entry.1292438233%3D%2520%2520Breast%2520cancer%2520is%2520a%2520significant%2520global%2520health%2520concern%252C%2520particularly%2520for%2520women.%250AEarly%2520detection%2520and%2520appropriate%2520treatment%2520are%2520crucial%2520in%2520mitigating%2520its%2520impact%252C%250Awith%2520histopathology%2520examinations%2520playing%2520a%2520vital%2520role%2520in%2520swift%2520diagnosis.%250AHowever%252C%2520these%2520examinations%2520often%2520require%2520a%2520substantial%2520workforce%2520and%250Aexperienced%2520medical%2520experts%2520for%2520proper%2520recognition%2520and%2520cancer%2520grading.%250AAutomated%2520image%2520retrieval%2520systems%2520have%2520the%2520potential%2520to%2520assist%2520pathologists%2520in%250Aidentifying%2520cancerous%2520tissues%252C%2520thereby%2520accelerating%2520the%2520diagnostic%2520process.%250ANevertheless%252C%2520due%2520to%2520considerable%2520variability%2520among%2520the%2520tissue%2520and%2520cell%250Apatterns%2520in%2520histological%2520images%252C%2520proposing%2520an%2520accurate%2520image%2520retrieval%2520model%2520is%250Avery%2520challenging.%250A%2520%2520This%2520work%2520introduces%2520a%2520novel%2520attention-based%2520adversarially%2520regularized%250Avariational%2520graph%2520autoencoder%2520model%2520for%2520breast%2520histological%2520image%2520retrieval.%250AAdditionally%252C%2520we%2520incorporated%2520cluster-guided%2520contrastive%2520learning%2520as%2520the%2520graph%250Afeature%2520extractor%2520to%2520boost%2520the%2520retrieval%2520performance.%2520We%2520evaluated%2520the%2520proposed%250Amodel%2527s%2520performance%2520on%2520two%2520publicly%2520available%2520datasets%2520of%2520breast%2520cancer%250Ahistological%2520images%2520and%2520achieved%2520superior%2520or%2520very%2520competitive%2520retrieval%250Aperformance%252C%2520with%2520average%2520mAP%2520scores%2520of%252096.5%2525%2520for%2520the%2520BreakHis%2520dataset%2520and%250A94.7%2525%2520for%2520the%2520BACH%2520dataset%252C%2520and%2520mVP%2520scores%2520of%252091.9%2525%2520and%252091.3%2525%252C%2520respectively.%250A%2520%2520Our%2520proposed%2520retrieval%2520model%2520has%2520the%2520potential%2520to%2520be%2520used%2520in%2520clinical%250Asettings%2520to%2520enhance%2520diagnostic%2520performance%2520and%2520ultimately%2520benefit%2520patients.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breast%20Histopathology%20Image%20Retrieval%20by%20Attention-based%20Adversarially%0A%20%20Regularized%20Variational%20Graph%20Autoencoder%20with%20Contrastive%20Learning-Based%0A%20%20Feature%20Extraction&entry.906535625=Nematollah%20Saeidi%20and%20Hossein%20Karshenas%20and%20Bijan%20Shoushtarian%20and%20Sepideh%20Hatamikia%20and%20Ramona%20Woitek%20and%20Amirreza%20Mahbod&entry.1292438233=%20%20Breast%20cancer%20is%20a%20significant%20global%20health%20concern%2C%20particularly%20for%20women.%0AEarly%20detection%20and%20appropriate%20treatment%20are%20crucial%20in%20mitigating%20its%20impact%2C%0Awith%20histopathology%20examinations%20playing%20a%20vital%20role%20in%20swift%20diagnosis.%0AHowever%2C%20these%20examinations%20often%20require%20a%20substantial%20workforce%20and%0Aexperienced%20medical%20experts%20for%20proper%20recognition%20and%20cancer%20grading.%0AAutomated%20image%20retrieval%20systems%20have%20the%20potential%20to%20assist%20pathologists%20in%0Aidentifying%20cancerous%20tissues%2C%20thereby%20accelerating%20the%20diagnostic%20process.%0ANevertheless%2C%20due%20to%20considerable%20variability%20among%20the%20tissue%20and%20cell%0Apatterns%20in%20histological%20images%2C%20proposing%20an%20accurate%20image%20retrieval%20model%20is%0Avery%20challenging.%0A%20%20This%20work%20introduces%20a%20novel%20attention-based%20adversarially%20regularized%0Avariational%20graph%20autoencoder%20model%20for%20breast%20histological%20image%20retrieval.%0AAdditionally%2C%20we%20incorporated%20cluster-guided%20contrastive%20learning%20as%20the%20graph%0Afeature%20extractor%20to%20boost%20the%20retrieval%20performance.%20We%20evaluated%20the%20proposed%0Amodel%27s%20performance%20on%20two%20publicly%20available%20datasets%20of%20breast%20cancer%0Ahistological%20images%20and%20achieved%20superior%20or%20very%20competitive%20retrieval%0Aperformance%2C%20with%20average%20mAP%20scores%20of%2096.5%25%20for%20the%20BreakHis%20dataset%20and%0A94.7%25%20for%20the%20BACH%20dataset%2C%20and%20mVP%20scores%20of%2091.9%25%20and%2091.3%25%2C%20respectively.%0A%20%20Our%20proposed%20retrieval%20model%20has%20the%20potential%20to%20be%20used%20in%20clinical%0Asettings%20to%20enhance%20diagnostic%20performance%20and%20ultimately%20benefit%20patients.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04211v1&entry.124074799=Read"},
{"title": "Learning To See But Forgetting To Follow: Visual Instruction Tuning\n  Makes LLMs More Prone To Jailbreak Attacks", "author": "Georgios Pantazopoulos and Amit Parekh and Malvina Nikandrou and Alessandro Suglia", "abstract": "  Augmenting Large Language Models (LLMs) with image-understanding capabilities\nhas resulted in a boom of high-performing Vision-Language models (VLMs). While\nstudying the alignment of LLMs to human values has received widespread\nattention, the safety of VLMs has not received the same attention. In this\npaper, we explore the impact of jailbreaking on three state-of-the-art VLMs,\neach using a distinct modeling approach. By comparing each VLM to their\nrespective LLM backbone, we find that each VLM is more susceptible to\njailbreaking. We consider this as an undesirable outcome from visual\ninstruction-tuning, which imposes a forgetting effect on an LLM's safety\nguardrails. Therefore, we provide recommendations for future work based on\nevaluation strategies that aim to highlight the weaknesses of a VLM, as well as\ntake safety measures into account during visual instruction tuning.\n", "link": "http://arxiv.org/abs/2405.04403v1", "date": "2024-05-07", "relevancy": 2.5274, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5207}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5079}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20To%20See%20But%20Forgetting%20To%20Follow%3A%20Visual%20Instruction%20Tuning%0A%20%20Makes%20LLMs%20More%20Prone%20To%20Jailbreak%20Attacks&body=Title%3A%20Learning%20To%20See%20But%20Forgetting%20To%20Follow%3A%20Visual%20Instruction%20Tuning%0A%20%20Makes%20LLMs%20More%20Prone%20To%20Jailbreak%20Attacks%0AAuthor%3A%20Georgios%20Pantazopoulos%20and%20Amit%20Parekh%20and%20Malvina%20Nikandrou%20and%20Alessandro%20Suglia%0AAbstract%3A%20%20%20Augmenting%20Large%20Language%20Models%20%28LLMs%29%20with%20image-understanding%20capabilities%0Ahas%20resulted%20in%20a%20boom%20of%20high-performing%20Vision-Language%20models%20%28VLMs%29.%20While%0Astudying%20the%20alignment%20of%20LLMs%20to%20human%20values%20has%20received%20widespread%0Aattention%2C%20the%20safety%20of%20VLMs%20has%20not%20received%20the%20same%20attention.%20In%20this%0Apaper%2C%20we%20explore%20the%20impact%20of%20jailbreaking%20on%20three%20state-of-the-art%20VLMs%2C%0Aeach%20using%20a%20distinct%20modeling%20approach.%20By%20comparing%20each%20VLM%20to%20their%0Arespective%20LLM%20backbone%2C%20we%20find%20that%20each%20VLM%20is%20more%20susceptible%20to%0Ajailbreaking.%20We%20consider%20this%20as%20an%20undesirable%20outcome%20from%20visual%0Ainstruction-tuning%2C%20which%20imposes%20a%20forgetting%20effect%20on%20an%20LLM%27s%20safety%0Aguardrails.%20Therefore%2C%20we%20provide%20recommendations%20for%20future%20work%20based%20on%0Aevaluation%20strategies%20that%20aim%20to%20highlight%20the%20weaknesses%20of%20a%20VLM%2C%20as%20well%20as%0Atake%20safety%20measures%20into%20account%20during%20visual%20instruction%20tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520To%2520See%2520But%2520Forgetting%2520To%2520Follow%253A%2520Visual%2520Instruction%2520Tuning%250A%2520%2520Makes%2520LLMs%2520More%2520Prone%2520To%2520Jailbreak%2520Attacks%26entry.906535625%3DGeorgios%2520Pantazopoulos%2520and%2520Amit%2520Parekh%2520and%2520Malvina%2520Nikandrou%2520and%2520Alessandro%2520Suglia%26entry.1292438233%3D%2520%2520Augmenting%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520image-understanding%2520capabilities%250Ahas%2520resulted%2520in%2520a%2520boom%2520of%2520high-performing%2520Vision-Language%2520models%2520%2528VLMs%2529.%2520While%250Astudying%2520the%2520alignment%2520of%2520LLMs%2520to%2520human%2520values%2520has%2520received%2520widespread%250Aattention%252C%2520the%2520safety%2520of%2520VLMs%2520has%2520not%2520received%2520the%2520same%2520attention.%2520In%2520this%250Apaper%252C%2520we%2520explore%2520the%2520impact%2520of%2520jailbreaking%2520on%2520three%2520state-of-the-art%2520VLMs%252C%250Aeach%2520using%2520a%2520distinct%2520modeling%2520approach.%2520By%2520comparing%2520each%2520VLM%2520to%2520their%250Arespective%2520LLM%2520backbone%252C%2520we%2520find%2520that%2520each%2520VLM%2520is%2520more%2520susceptible%2520to%250Ajailbreaking.%2520We%2520consider%2520this%2520as%2520an%2520undesirable%2520outcome%2520from%2520visual%250Ainstruction-tuning%252C%2520which%2520imposes%2520a%2520forgetting%2520effect%2520on%2520an%2520LLM%2527s%2520safety%250Aguardrails.%2520Therefore%252C%2520we%2520provide%2520recommendations%2520for%2520future%2520work%2520based%2520on%250Aevaluation%2520strategies%2520that%2520aim%2520to%2520highlight%2520the%2520weaknesses%2520of%2520a%2520VLM%252C%2520as%2520well%2520as%250Atake%2520safety%2520measures%2520into%2520account%2520during%2520visual%2520instruction%2520tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20To%20See%20But%20Forgetting%20To%20Follow%3A%20Visual%20Instruction%20Tuning%0A%20%20Makes%20LLMs%20More%20Prone%20To%20Jailbreak%20Attacks&entry.906535625=Georgios%20Pantazopoulos%20and%20Amit%20Parekh%20and%20Malvina%20Nikandrou%20and%20Alessandro%20Suglia&entry.1292438233=%20%20Augmenting%20Large%20Language%20Models%20%28LLMs%29%20with%20image-understanding%20capabilities%0Ahas%20resulted%20in%20a%20boom%20of%20high-performing%20Vision-Language%20models%20%28VLMs%29.%20While%0Astudying%20the%20alignment%20of%20LLMs%20to%20human%20values%20has%20received%20widespread%0Aattention%2C%20the%20safety%20of%20VLMs%20has%20not%20received%20the%20same%20attention.%20In%20this%0Apaper%2C%20we%20explore%20the%20impact%20of%20jailbreaking%20on%20three%20state-of-the-art%20VLMs%2C%0Aeach%20using%20a%20distinct%20modeling%20approach.%20By%20comparing%20each%20VLM%20to%20their%0Arespective%20LLM%20backbone%2C%20we%20find%20that%20each%20VLM%20is%20more%20susceptible%20to%0Ajailbreaking.%20We%20consider%20this%20as%20an%20undesirable%20outcome%20from%20visual%0Ainstruction-tuning%2C%20which%20imposes%20a%20forgetting%20effect%20on%20an%20LLM%27s%20safety%0Aguardrails.%20Therefore%2C%20we%20provide%20recommendations%20for%20future%20work%20based%20on%0Aevaluation%20strategies%20that%20aim%20to%20highlight%20the%20weaknesses%20of%20a%20VLM%2C%20as%20well%20as%0Atake%20safety%20measures%20into%20account%20during%20visual%20instruction%20tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04403v1&entry.124074799=Read"},
{"title": "Global Scale Self-Supervised Channel Charting with Sensor Fusion", "author": "Omid Esrafilian and Mohsen Ahadi and Florian Kaltenberger and David Gesbert", "abstract": "  The sensing and positioning capabilities foreseen in 6G have great potential\nfor technology advancements in various domains, such as future smart cities and\nindustrial use cases. Channel charting has emerged as a promising technology in\nrecent years for radio frequency-based sensing and localization. However, the\naccuracy of these techniques is yet far behind the numbers envisioned in 6G. To\nreduce this gap, in this paper, we propose a novel channel charting technique\ncapitalizing on the time of arrival measurements from surrounding Transmission\nReception Points (TRPs) along with their locations and leveraging sensor fusion\nin channel charting by incorporating laser scanner data during the training\nphase of our algorithm. The proposed algorithm remains self-supervised during\ntraining and test phases, requiring no geometrical models or user position\nground truth. Simulation results validate the achievement of a sub-meter level\nlocalization accuracy using our algorithm 90% of the time, outperforming the\nstate-of-the-art channel charting techniques and the traditional\ntriangulation-based approaches.\n", "link": "http://arxiv.org/abs/2405.04357v1", "date": "2024-05-07", "relevancy": 2.4862, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5141}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.498}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20Scale%20Self-Supervised%20Channel%20Charting%20with%20Sensor%20Fusion&body=Title%3A%20Global%20Scale%20Self-Supervised%20Channel%20Charting%20with%20Sensor%20Fusion%0AAuthor%3A%20Omid%20Esrafilian%20and%20Mohsen%20Ahadi%20and%20Florian%20Kaltenberger%20and%20David%20Gesbert%0AAbstract%3A%20%20%20The%20sensing%20and%20positioning%20capabilities%20foreseen%20in%206G%20have%20great%20potential%0Afor%20technology%20advancements%20in%20various%20domains%2C%20such%20as%20future%20smart%20cities%20and%0Aindustrial%20use%20cases.%20Channel%20charting%20has%20emerged%20as%20a%20promising%20technology%20in%0Arecent%20years%20for%20radio%20frequency-based%20sensing%20and%20localization.%20However%2C%20the%0Aaccuracy%20of%20these%20techniques%20is%20yet%20far%20behind%20the%20numbers%20envisioned%20in%206G.%20To%0Areduce%20this%20gap%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20channel%20charting%20technique%0Acapitalizing%20on%20the%20time%20of%20arrival%20measurements%20from%20surrounding%20Transmission%0AReception%20Points%20%28TRPs%29%20along%20with%20their%20locations%20and%20leveraging%20sensor%20fusion%0Ain%20channel%20charting%20by%20incorporating%20laser%20scanner%20data%20during%20the%20training%0Aphase%20of%20our%20algorithm.%20The%20proposed%20algorithm%20remains%20self-supervised%20during%0Atraining%20and%20test%20phases%2C%20requiring%20no%20geometrical%20models%20or%20user%20position%0Aground%20truth.%20Simulation%20results%20validate%20the%20achievement%20of%20a%20sub-meter%20level%0Alocalization%20accuracy%20using%20our%20algorithm%2090%25%20of%20the%20time%2C%20outperforming%20the%0Astate-of-the-art%20channel%20charting%20techniques%20and%20the%20traditional%0Atriangulation-based%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04357v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520Scale%2520Self-Supervised%2520Channel%2520Charting%2520with%2520Sensor%2520Fusion%26entry.906535625%3DOmid%2520Esrafilian%2520and%2520Mohsen%2520Ahadi%2520and%2520Florian%2520Kaltenberger%2520and%2520David%2520Gesbert%26entry.1292438233%3D%2520%2520The%2520sensing%2520and%2520positioning%2520capabilities%2520foreseen%2520in%25206G%2520have%2520great%2520potential%250Afor%2520technology%2520advancements%2520in%2520various%2520domains%252C%2520such%2520as%2520future%2520smart%2520cities%2520and%250Aindustrial%2520use%2520cases.%2520Channel%2520charting%2520has%2520emerged%2520as%2520a%2520promising%2520technology%2520in%250Arecent%2520years%2520for%2520radio%2520frequency-based%2520sensing%2520and%2520localization.%2520However%252C%2520the%250Aaccuracy%2520of%2520these%2520techniques%2520is%2520yet%2520far%2520behind%2520the%2520numbers%2520envisioned%2520in%25206G.%2520To%250Areduce%2520this%2520gap%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520channel%2520charting%2520technique%250Acapitalizing%2520on%2520the%2520time%2520of%2520arrival%2520measurements%2520from%2520surrounding%2520Transmission%250AReception%2520Points%2520%2528TRPs%2529%2520along%2520with%2520their%2520locations%2520and%2520leveraging%2520sensor%2520fusion%250Ain%2520channel%2520charting%2520by%2520incorporating%2520laser%2520scanner%2520data%2520during%2520the%2520training%250Aphase%2520of%2520our%2520algorithm.%2520The%2520proposed%2520algorithm%2520remains%2520self-supervised%2520during%250Atraining%2520and%2520test%2520phases%252C%2520requiring%2520no%2520geometrical%2520models%2520or%2520user%2520position%250Aground%2520truth.%2520Simulation%2520results%2520validate%2520the%2520achievement%2520of%2520a%2520sub-meter%2520level%250Alocalization%2520accuracy%2520using%2520our%2520algorithm%252090%2525%2520of%2520the%2520time%252C%2520outperforming%2520the%250Astate-of-the-art%2520channel%2520charting%2520techniques%2520and%2520the%2520traditional%250Atriangulation-based%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04357v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20Scale%20Self-Supervised%20Channel%20Charting%20with%20Sensor%20Fusion&entry.906535625=Omid%20Esrafilian%20and%20Mohsen%20Ahadi%20and%20Florian%20Kaltenberger%20and%20David%20Gesbert&entry.1292438233=%20%20The%20sensing%20and%20positioning%20capabilities%20foreseen%20in%206G%20have%20great%20potential%0Afor%20technology%20advancements%20in%20various%20domains%2C%20such%20as%20future%20smart%20cities%20and%0Aindustrial%20use%20cases.%20Channel%20charting%20has%20emerged%20as%20a%20promising%20technology%20in%0Arecent%20years%20for%20radio%20frequency-based%20sensing%20and%20localization.%20However%2C%20the%0Aaccuracy%20of%20these%20techniques%20is%20yet%20far%20behind%20the%20numbers%20envisioned%20in%206G.%20To%0Areduce%20this%20gap%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20channel%20charting%20technique%0Acapitalizing%20on%20the%20time%20of%20arrival%20measurements%20from%20surrounding%20Transmission%0AReception%20Points%20%28TRPs%29%20along%20with%20their%20locations%20and%20leveraging%20sensor%20fusion%0Ain%20channel%20charting%20by%20incorporating%20laser%20scanner%20data%20during%20the%20training%0Aphase%20of%20our%20algorithm.%20The%20proposed%20algorithm%20remains%20self-supervised%20during%0Atraining%20and%20test%20phases%2C%20requiring%20no%20geometrical%20models%20or%20user%20position%0Aground%20truth.%20Simulation%20results%20validate%20the%20achievement%20of%20a%20sub-meter%20level%0Alocalization%20accuracy%20using%20our%20algorithm%2090%25%20of%20the%20time%2C%20outperforming%20the%0Astate-of-the-art%20channel%20charting%20techniques%20and%20the%20traditional%0Atriangulation-based%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04357v1&entry.124074799=Read"},
{"title": "An Attention Based Pipeline for Identifying Pre-Cancer Lesions in Head\n  and Neck Clinical Images", "author": "Abdullah Alsalemi and Anza Shakeel and Mollie Clark and Syed Ali Khurram and Shan E Ahmed Raza", "abstract": "  Early detection of cancer can help improve patient prognosis by early\nintervention. Head and neck cancer is diagnosed in specialist centres after a\nsurgical biopsy, however, there is a potential for these to be missed leading\nto delayed diagnosis. To overcome these challenges, we present an attention\nbased pipeline that identifies suspected lesions, segments, and classifies them\nas non-dysplastic, dysplastic and cancerous lesions. We propose (a) a vision\ntransformer based Mask R-CNN network for lesion detection and segmentation of\nclinical images, and (b) Multiple Instance Learning (MIL) based scheme for\nclassification. Current results show that the segmentation model produces\nsegmentation masks and bounding boxes with up to 82% overlap accuracy score on\nunseen external test data and surpassing reviewed segmentation benchmarks.\nNext, a classification F1-score of 85% on the internal cohort test set. An app\nhas been developed to perform lesion segmentation taken via a smart device.\nFuture work involves employing endoscopic video data for precise early\ndetection and prognosis.\n", "link": "http://arxiv.org/abs/2405.01937v2", "date": "2024-05-07", "relevancy": 2.4652, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5046}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4989}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Attention%20Based%20Pipeline%20for%20Identifying%20Pre-Cancer%20Lesions%20in%20Head%0A%20%20and%20Neck%20Clinical%20Images&body=Title%3A%20An%20Attention%20Based%20Pipeline%20for%20Identifying%20Pre-Cancer%20Lesions%20in%20Head%0A%20%20and%20Neck%20Clinical%20Images%0AAuthor%3A%20Abdullah%20Alsalemi%20and%20Anza%20Shakeel%20and%20Mollie%20Clark%20and%20Syed%20Ali%20Khurram%20and%20Shan%20E%20Ahmed%20Raza%0AAbstract%3A%20%20%20Early%20detection%20of%20cancer%20can%20help%20improve%20patient%20prognosis%20by%20early%0Aintervention.%20Head%20and%20neck%20cancer%20is%20diagnosed%20in%20specialist%20centres%20after%20a%0Asurgical%20biopsy%2C%20however%2C%20there%20is%20a%20potential%20for%20these%20to%20be%20missed%20leading%0Ato%20delayed%20diagnosis.%20To%20overcome%20these%20challenges%2C%20we%20present%20an%20attention%0Abased%20pipeline%20that%20identifies%20suspected%20lesions%2C%20segments%2C%20and%20classifies%20them%0Aas%20non-dysplastic%2C%20dysplastic%20and%20cancerous%20lesions.%20We%20propose%20%28a%29%20a%20vision%0Atransformer%20based%20Mask%20R-CNN%20network%20for%20lesion%20detection%20and%20segmentation%20of%0Aclinical%20images%2C%20and%20%28b%29%20Multiple%20Instance%20Learning%20%28MIL%29%20based%20scheme%20for%0Aclassification.%20Current%20results%20show%20that%20the%20segmentation%20model%20produces%0Asegmentation%20masks%20and%20bounding%20boxes%20with%20up%20to%2082%25%20overlap%20accuracy%20score%20on%0Aunseen%20external%20test%20data%20and%20surpassing%20reviewed%20segmentation%20benchmarks.%0ANext%2C%20a%20classification%20F1-score%20of%2085%25%20on%20the%20internal%20cohort%20test%20set.%20An%20app%0Ahas%20been%20developed%20to%20perform%20lesion%20segmentation%20taken%20via%20a%20smart%20device.%0AFuture%20work%20involves%20employing%20endoscopic%20video%20data%20for%20precise%20early%0Adetection%20and%20prognosis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01937v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Attention%2520Based%2520Pipeline%2520for%2520Identifying%2520Pre-Cancer%2520Lesions%2520in%2520Head%250A%2520%2520and%2520Neck%2520Clinical%2520Images%26entry.906535625%3DAbdullah%2520Alsalemi%2520and%2520Anza%2520Shakeel%2520and%2520Mollie%2520Clark%2520and%2520Syed%2520Ali%2520Khurram%2520and%2520Shan%2520E%2520Ahmed%2520Raza%26entry.1292438233%3D%2520%2520Early%2520detection%2520of%2520cancer%2520can%2520help%2520improve%2520patient%2520prognosis%2520by%2520early%250Aintervention.%2520Head%2520and%2520neck%2520cancer%2520is%2520diagnosed%2520in%2520specialist%2520centres%2520after%2520a%250Asurgical%2520biopsy%252C%2520however%252C%2520there%2520is%2520a%2520potential%2520for%2520these%2520to%2520be%2520missed%2520leading%250Ato%2520delayed%2520diagnosis.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520present%2520an%2520attention%250Abased%2520pipeline%2520that%2520identifies%2520suspected%2520lesions%252C%2520segments%252C%2520and%2520classifies%2520them%250Aas%2520non-dysplastic%252C%2520dysplastic%2520and%2520cancerous%2520lesions.%2520We%2520propose%2520%2528a%2529%2520a%2520vision%250Atransformer%2520based%2520Mask%2520R-CNN%2520network%2520for%2520lesion%2520detection%2520and%2520segmentation%2520of%250Aclinical%2520images%252C%2520and%2520%2528b%2529%2520Multiple%2520Instance%2520Learning%2520%2528MIL%2529%2520based%2520scheme%2520for%250Aclassification.%2520Current%2520results%2520show%2520that%2520the%2520segmentation%2520model%2520produces%250Asegmentation%2520masks%2520and%2520bounding%2520boxes%2520with%2520up%2520to%252082%2525%2520overlap%2520accuracy%2520score%2520on%250Aunseen%2520external%2520test%2520data%2520and%2520surpassing%2520reviewed%2520segmentation%2520benchmarks.%250ANext%252C%2520a%2520classification%2520F1-score%2520of%252085%2525%2520on%2520the%2520internal%2520cohort%2520test%2520set.%2520An%2520app%250Ahas%2520been%2520developed%2520to%2520perform%2520lesion%2520segmentation%2520taken%2520via%2520a%2520smart%2520device.%250AFuture%2520work%2520involves%2520employing%2520endoscopic%2520video%2520data%2520for%2520precise%2520early%250Adetection%2520and%2520prognosis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01937v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Attention%20Based%20Pipeline%20for%20Identifying%20Pre-Cancer%20Lesions%20in%20Head%0A%20%20and%20Neck%20Clinical%20Images&entry.906535625=Abdullah%20Alsalemi%20and%20Anza%20Shakeel%20and%20Mollie%20Clark%20and%20Syed%20Ali%20Khurram%20and%20Shan%20E%20Ahmed%20Raza&entry.1292438233=%20%20Early%20detection%20of%20cancer%20can%20help%20improve%20patient%20prognosis%20by%20early%0Aintervention.%20Head%20and%20neck%20cancer%20is%20diagnosed%20in%20specialist%20centres%20after%20a%0Asurgical%20biopsy%2C%20however%2C%20there%20is%20a%20potential%20for%20these%20to%20be%20missed%20leading%0Ato%20delayed%20diagnosis.%20To%20overcome%20these%20challenges%2C%20we%20present%20an%20attention%0Abased%20pipeline%20that%20identifies%20suspected%20lesions%2C%20segments%2C%20and%20classifies%20them%0Aas%20non-dysplastic%2C%20dysplastic%20and%20cancerous%20lesions.%20We%20propose%20%28a%29%20a%20vision%0Atransformer%20based%20Mask%20R-CNN%20network%20for%20lesion%20detection%20and%20segmentation%20of%0Aclinical%20images%2C%20and%20%28b%29%20Multiple%20Instance%20Learning%20%28MIL%29%20based%20scheme%20for%0Aclassification.%20Current%20results%20show%20that%20the%20segmentation%20model%20produces%0Asegmentation%20masks%20and%20bounding%20boxes%20with%20up%20to%2082%25%20overlap%20accuracy%20score%20on%0Aunseen%20external%20test%20data%20and%20surpassing%20reviewed%20segmentation%20benchmarks.%0ANext%2C%20a%20classification%20F1-score%20of%2085%25%20on%20the%20internal%20cohort%20test%20set.%20An%20app%0Ahas%20been%20developed%20to%20perform%20lesion%20segmentation%20taken%20via%20a%20smart%20device.%0AFuture%20work%20involves%20employing%20endoscopic%20video%20data%20for%20precise%20early%0Adetection%20and%20prognosis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01937v2&entry.124074799=Read"},
{"title": "Leveraging tropical reef, bird and unrelated sounds for superior\n  transfer learning in marine bioacoustics", "author": "Ben Williams and Bart van Merri\u00ebnboer and Vincent Dumoulin and Jenny Hamer and Eleni Triantafillou and Abram B. Fleishman and Matthew McKown and Jill E. Munger and Aaron N. Rice and Ashlee Lillis and Clemency E. White and Catherine A. D. Hobbs and Tries B. Razak and Kate E. Jones and Tom Denton", "abstract": "  Machine learning has the potential to revolutionize passive acoustic\nmonitoring (PAM) for ecological assessments. However, high annotation and\ncompute costs limit the field's efficacy. Generalizable pretrained networks can\novercome these costs, but high-quality pretraining requires vast annotated\nlibraries, limiting its current applicability primarily to bird taxa. Here, we\nidentify the optimum pretraining strategy for a data-deficient domain using\ncoral reef bioacoustics. We assemble ReefSet, a large annotated library of reef\nsounds, though modest compared to bird libraries at 2% of the sample count.\nThrough testing few-shot transfer learning performance, we observe that\npretraining on bird audio provides notably superior generalizability compared\nto pretraining on ReefSet or unrelated audio alone. However, our key findings\nshow that cross-domain mixing which leverages bird, reef and unrelated audio\nduring pretraining maximizes reef generalizability. SurfPerch, our pretrained\nnetwork, provides a strong foundation for automated analysis of marine PAM data\nwith minimal annotation and compute costs.\n", "link": "http://arxiv.org/abs/2404.16436v2", "date": "2024-05-07", "relevancy": 2.4637, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5202}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4844}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20tropical%20reef%2C%20bird%20and%20unrelated%20sounds%20for%20superior%0A%20%20transfer%20learning%20in%20marine%20bioacoustics&body=Title%3A%20Leveraging%20tropical%20reef%2C%20bird%20and%20unrelated%20sounds%20for%20superior%0A%20%20transfer%20learning%20in%20marine%20bioacoustics%0AAuthor%3A%20Ben%20Williams%20and%20Bart%20van%20Merri%C3%ABnboer%20and%20Vincent%20Dumoulin%20and%20Jenny%20Hamer%20and%20Eleni%20Triantafillou%20and%20Abram%20B.%20Fleishman%20and%20Matthew%20McKown%20and%20Jill%20E.%20Munger%20and%20Aaron%20N.%20Rice%20and%20Ashlee%20Lillis%20and%20Clemency%20E.%20White%20and%20Catherine%20A.%20D.%20Hobbs%20and%20Tries%20B.%20Razak%20and%20Kate%20E.%20Jones%20and%20Tom%20Denton%0AAbstract%3A%20%20%20Machine%20learning%20has%20the%20potential%20to%20revolutionize%20passive%20acoustic%0Amonitoring%20%28PAM%29%20for%20ecological%20assessments.%20However%2C%20high%20annotation%20and%0Acompute%20costs%20limit%20the%20field%27s%20efficacy.%20Generalizable%20pretrained%20networks%20can%0Aovercome%20these%20costs%2C%20but%20high-quality%20pretraining%20requires%20vast%20annotated%0Alibraries%2C%20limiting%20its%20current%20applicability%20primarily%20to%20bird%20taxa.%20Here%2C%20we%0Aidentify%20the%20optimum%20pretraining%20strategy%20for%20a%20data-deficient%20domain%20using%0Acoral%20reef%20bioacoustics.%20We%20assemble%20ReefSet%2C%20a%20large%20annotated%20library%20of%20reef%0Asounds%2C%20though%20modest%20compared%20to%20bird%20libraries%20at%202%25%20of%20the%20sample%20count.%0AThrough%20testing%20few-shot%20transfer%20learning%20performance%2C%20we%20observe%20that%0Apretraining%20on%20bird%20audio%20provides%20notably%20superior%20generalizability%20compared%0Ato%20pretraining%20on%20ReefSet%20or%20unrelated%20audio%20alone.%20However%2C%20our%20key%20findings%0Ashow%20that%20cross-domain%20mixing%20which%20leverages%20bird%2C%20reef%20and%20unrelated%20audio%0Aduring%20pretraining%20maximizes%20reef%20generalizability.%20SurfPerch%2C%20our%20pretrained%0Anetwork%2C%20provides%20a%20strong%20foundation%20for%20automated%20analysis%20of%20marine%20PAM%20data%0Awith%20minimal%20annotation%20and%20compute%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16436v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520tropical%2520reef%252C%2520bird%2520and%2520unrelated%2520sounds%2520for%2520superior%250A%2520%2520transfer%2520learning%2520in%2520marine%2520bioacoustics%26entry.906535625%3DBen%2520Williams%2520and%2520Bart%2520van%2520Merri%25C3%25ABnboer%2520and%2520Vincent%2520Dumoulin%2520and%2520Jenny%2520Hamer%2520and%2520Eleni%2520Triantafillou%2520and%2520Abram%2520B.%2520Fleishman%2520and%2520Matthew%2520McKown%2520and%2520Jill%2520E.%2520Munger%2520and%2520Aaron%2520N.%2520Rice%2520and%2520Ashlee%2520Lillis%2520and%2520Clemency%2520E.%2520White%2520and%2520Catherine%2520A.%2520D.%2520Hobbs%2520and%2520Tries%2520B.%2520Razak%2520and%2520Kate%2520E.%2520Jones%2520and%2520Tom%2520Denton%26entry.1292438233%3D%2520%2520Machine%2520learning%2520has%2520the%2520potential%2520to%2520revolutionize%2520passive%2520acoustic%250Amonitoring%2520%2528PAM%2529%2520for%2520ecological%2520assessments.%2520However%252C%2520high%2520annotation%2520and%250Acompute%2520costs%2520limit%2520the%2520field%2527s%2520efficacy.%2520Generalizable%2520pretrained%2520networks%2520can%250Aovercome%2520these%2520costs%252C%2520but%2520high-quality%2520pretraining%2520requires%2520vast%2520annotated%250Alibraries%252C%2520limiting%2520its%2520current%2520applicability%2520primarily%2520to%2520bird%2520taxa.%2520Here%252C%2520we%250Aidentify%2520the%2520optimum%2520pretraining%2520strategy%2520for%2520a%2520data-deficient%2520domain%2520using%250Acoral%2520reef%2520bioacoustics.%2520We%2520assemble%2520ReefSet%252C%2520a%2520large%2520annotated%2520library%2520of%2520reef%250Asounds%252C%2520though%2520modest%2520compared%2520to%2520bird%2520libraries%2520at%25202%2525%2520of%2520the%2520sample%2520count.%250AThrough%2520testing%2520few-shot%2520transfer%2520learning%2520performance%252C%2520we%2520observe%2520that%250Apretraining%2520on%2520bird%2520audio%2520provides%2520notably%2520superior%2520generalizability%2520compared%250Ato%2520pretraining%2520on%2520ReefSet%2520or%2520unrelated%2520audio%2520alone.%2520However%252C%2520our%2520key%2520findings%250Ashow%2520that%2520cross-domain%2520mixing%2520which%2520leverages%2520bird%252C%2520reef%2520and%2520unrelated%2520audio%250Aduring%2520pretraining%2520maximizes%2520reef%2520generalizability.%2520SurfPerch%252C%2520our%2520pretrained%250Anetwork%252C%2520provides%2520a%2520strong%2520foundation%2520for%2520automated%2520analysis%2520of%2520marine%2520PAM%2520data%250Awith%2520minimal%2520annotation%2520and%2520compute%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16436v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20tropical%20reef%2C%20bird%20and%20unrelated%20sounds%20for%20superior%0A%20%20transfer%20learning%20in%20marine%20bioacoustics&entry.906535625=Ben%20Williams%20and%20Bart%20van%20Merri%C3%ABnboer%20and%20Vincent%20Dumoulin%20and%20Jenny%20Hamer%20and%20Eleni%20Triantafillou%20and%20Abram%20B.%20Fleishman%20and%20Matthew%20McKown%20and%20Jill%20E.%20Munger%20and%20Aaron%20N.%20Rice%20and%20Ashlee%20Lillis%20and%20Clemency%20E.%20White%20and%20Catherine%20A.%20D.%20Hobbs%20and%20Tries%20B.%20Razak%20and%20Kate%20E.%20Jones%20and%20Tom%20Denton&entry.1292438233=%20%20Machine%20learning%20has%20the%20potential%20to%20revolutionize%20passive%20acoustic%0Amonitoring%20%28PAM%29%20for%20ecological%20assessments.%20However%2C%20high%20annotation%20and%0Acompute%20costs%20limit%20the%20field%27s%20efficacy.%20Generalizable%20pretrained%20networks%20can%0Aovercome%20these%20costs%2C%20but%20high-quality%20pretraining%20requires%20vast%20annotated%0Alibraries%2C%20limiting%20its%20current%20applicability%20primarily%20to%20bird%20taxa.%20Here%2C%20we%0Aidentify%20the%20optimum%20pretraining%20strategy%20for%20a%20data-deficient%20domain%20using%0Acoral%20reef%20bioacoustics.%20We%20assemble%20ReefSet%2C%20a%20large%20annotated%20library%20of%20reef%0Asounds%2C%20though%20modest%20compared%20to%20bird%20libraries%20at%202%25%20of%20the%20sample%20count.%0AThrough%20testing%20few-shot%20transfer%20learning%20performance%2C%20we%20observe%20that%0Apretraining%20on%20bird%20audio%20provides%20notably%20superior%20generalizability%20compared%0Ato%20pretraining%20on%20ReefSet%20or%20unrelated%20audio%20alone.%20However%2C%20our%20key%20findings%0Ashow%20that%20cross-domain%20mixing%20which%20leverages%20bird%2C%20reef%20and%20unrelated%20audio%0Aduring%20pretraining%20maximizes%20reef%20generalizability.%20SurfPerch%2C%20our%20pretrained%0Anetwork%2C%20provides%20a%20strong%20foundation%20for%20automated%20analysis%20of%20marine%20PAM%20data%0Awith%20minimal%20annotation%20and%20compute%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16436v2&entry.124074799=Read"},
{"title": "AugmenTory: A Fast and Flexible Polygon Augmentation Library", "author": "Tanaz Ghahremani and Mohammad Hoseyni and Mohammad Javad Ahmadi and Pouria Mehrabi and Amirhossein Nikoofard", "abstract": "  Data augmentation is a key technique for addressing the challenge of limited\ndatasets, which have become a major component in the training procedures of\nimage processing. Techniques such as geometric transformations and color space\nadjustments have been thoroughly tested for their ability to artificially\nexpand training datasets and generate semi-realistic data for training\npurposes. Data augmentation is the most important key to addressing the\nchallenge of limited datasets, which have become a major component of image\nprocessing training procedures. Data augmentation techniques, such as geometric\ntransformations and color space adjustments, are thoroughly tested for their\nability to artificially expand training datasets and generate semi-realistic\ndata for training purposes. Polygons play a crucial role in instance\nsegmentation and have seen a surge in use across advanced models, such as\nYOLOv8. Despite their growing popularity, the lack of specialized libraries\nhampers the polygon-augmentation process. This paper introduces a novel\nsolution to this challenge, embodied in the newly developed AugmenTory library.\nNotably, AugmenTory offers reduced computational demands in both time and space\ncompared to existing methods. Additionally, the library includes a\npostprocessing thresholding feature. The AugmenTory package is publicly\navailable on GitHub, where interested users can access the source code:\nhttps://github.com/Smartory/AugmenTory\n", "link": "http://arxiv.org/abs/2405.04442v1", "date": "2024-05-07", "relevancy": 2.4337, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4936}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4842}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AugmenTory%3A%20A%20Fast%20and%20Flexible%20Polygon%20Augmentation%20Library&body=Title%3A%20AugmenTory%3A%20A%20Fast%20and%20Flexible%20Polygon%20Augmentation%20Library%0AAuthor%3A%20Tanaz%20Ghahremani%20and%20Mohammad%20Hoseyni%20and%20Mohammad%20Javad%20Ahmadi%20and%20Pouria%20Mehrabi%20and%20Amirhossein%20Nikoofard%0AAbstract%3A%20%20%20Data%20augmentation%20is%20a%20key%20technique%20for%20addressing%20the%20challenge%20of%20limited%0Adatasets%2C%20which%20have%20become%20a%20major%20component%20in%20the%20training%20procedures%20of%0Aimage%20processing.%20Techniques%20such%20as%20geometric%20transformations%20and%20color%20space%0Aadjustments%20have%20been%20thoroughly%20tested%20for%20their%20ability%20to%20artificially%0Aexpand%20training%20datasets%20and%20generate%20semi-realistic%20data%20for%20training%0Apurposes.%20Data%20augmentation%20is%20the%20most%20important%20key%20to%20addressing%20the%0Achallenge%20of%20limited%20datasets%2C%20which%20have%20become%20a%20major%20component%20of%20image%0Aprocessing%20training%20procedures.%20Data%20augmentation%20techniques%2C%20such%20as%20geometric%0Atransformations%20and%20color%20space%20adjustments%2C%20are%20thoroughly%20tested%20for%20their%0Aability%20to%20artificially%20expand%20training%20datasets%20and%20generate%20semi-realistic%0Adata%20for%20training%20purposes.%20Polygons%20play%20a%20crucial%20role%20in%20instance%0Asegmentation%20and%20have%20seen%20a%20surge%20in%20use%20across%20advanced%20models%2C%20such%20as%0AYOLOv8.%20Despite%20their%20growing%20popularity%2C%20the%20lack%20of%20specialized%20libraries%0Ahampers%20the%20polygon-augmentation%20process.%20This%20paper%20introduces%20a%20novel%0Asolution%20to%20this%20challenge%2C%20embodied%20in%20the%20newly%20developed%20AugmenTory%20library.%0ANotably%2C%20AugmenTory%20offers%20reduced%20computational%20demands%20in%20both%20time%20and%20space%0Acompared%20to%20existing%20methods.%20Additionally%2C%20the%20library%20includes%20a%0Apostprocessing%20thresholding%20feature.%20The%20AugmenTory%20package%20is%20publicly%0Aavailable%20on%20GitHub%2C%20where%20interested%20users%20can%20access%20the%20source%20code%3A%0Ahttps%3A//github.com/Smartory/AugmenTory%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAugmenTory%253A%2520A%2520Fast%2520and%2520Flexible%2520Polygon%2520Augmentation%2520Library%26entry.906535625%3DTanaz%2520Ghahremani%2520and%2520Mohammad%2520Hoseyni%2520and%2520Mohammad%2520Javad%2520Ahmadi%2520and%2520Pouria%2520Mehrabi%2520and%2520Amirhossein%2520Nikoofard%26entry.1292438233%3D%2520%2520Data%2520augmentation%2520is%2520a%2520key%2520technique%2520for%2520addressing%2520the%2520challenge%2520of%2520limited%250Adatasets%252C%2520which%2520have%2520become%2520a%2520major%2520component%2520in%2520the%2520training%2520procedures%2520of%250Aimage%2520processing.%2520Techniques%2520such%2520as%2520geometric%2520transformations%2520and%2520color%2520space%250Aadjustments%2520have%2520been%2520thoroughly%2520tested%2520for%2520their%2520ability%2520to%2520artificially%250Aexpand%2520training%2520datasets%2520and%2520generate%2520semi-realistic%2520data%2520for%2520training%250Apurposes.%2520Data%2520augmentation%2520is%2520the%2520most%2520important%2520key%2520to%2520addressing%2520the%250Achallenge%2520of%2520limited%2520datasets%252C%2520which%2520have%2520become%2520a%2520major%2520component%2520of%2520image%250Aprocessing%2520training%2520procedures.%2520Data%2520augmentation%2520techniques%252C%2520such%2520as%2520geometric%250Atransformations%2520and%2520color%2520space%2520adjustments%252C%2520are%2520thoroughly%2520tested%2520for%2520their%250Aability%2520to%2520artificially%2520expand%2520training%2520datasets%2520and%2520generate%2520semi-realistic%250Adata%2520for%2520training%2520purposes.%2520Polygons%2520play%2520a%2520crucial%2520role%2520in%2520instance%250Asegmentation%2520and%2520have%2520seen%2520a%2520surge%2520in%2520use%2520across%2520advanced%2520models%252C%2520such%2520as%250AYOLOv8.%2520Despite%2520their%2520growing%2520popularity%252C%2520the%2520lack%2520of%2520specialized%2520libraries%250Ahampers%2520the%2520polygon-augmentation%2520process.%2520This%2520paper%2520introduces%2520a%2520novel%250Asolution%2520to%2520this%2520challenge%252C%2520embodied%2520in%2520the%2520newly%2520developed%2520AugmenTory%2520library.%250ANotably%252C%2520AugmenTory%2520offers%2520reduced%2520computational%2520demands%2520in%2520both%2520time%2520and%2520space%250Acompared%2520to%2520existing%2520methods.%2520Additionally%252C%2520the%2520library%2520includes%2520a%250Apostprocessing%2520thresholding%2520feature.%2520The%2520AugmenTory%2520package%2520is%2520publicly%250Aavailable%2520on%2520GitHub%252C%2520where%2520interested%2520users%2520can%2520access%2520the%2520source%2520code%253A%250Ahttps%253A//github.com/Smartory/AugmenTory%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AugmenTory%3A%20A%20Fast%20and%20Flexible%20Polygon%20Augmentation%20Library&entry.906535625=Tanaz%20Ghahremani%20and%20Mohammad%20Hoseyni%20and%20Mohammad%20Javad%20Ahmadi%20and%20Pouria%20Mehrabi%20and%20Amirhossein%20Nikoofard&entry.1292438233=%20%20Data%20augmentation%20is%20a%20key%20technique%20for%20addressing%20the%20challenge%20of%20limited%0Adatasets%2C%20which%20have%20become%20a%20major%20component%20in%20the%20training%20procedures%20of%0Aimage%20processing.%20Techniques%20such%20as%20geometric%20transformations%20and%20color%20space%0Aadjustments%20have%20been%20thoroughly%20tested%20for%20their%20ability%20to%20artificially%0Aexpand%20training%20datasets%20and%20generate%20semi-realistic%20data%20for%20training%0Apurposes.%20Data%20augmentation%20is%20the%20most%20important%20key%20to%20addressing%20the%0Achallenge%20of%20limited%20datasets%2C%20which%20have%20become%20a%20major%20component%20of%20image%0Aprocessing%20training%20procedures.%20Data%20augmentation%20techniques%2C%20such%20as%20geometric%0Atransformations%20and%20color%20space%20adjustments%2C%20are%20thoroughly%20tested%20for%20their%0Aability%20to%20artificially%20expand%20training%20datasets%20and%20generate%20semi-realistic%0Adata%20for%20training%20purposes.%20Polygons%20play%20a%20crucial%20role%20in%20instance%0Asegmentation%20and%20have%20seen%20a%20surge%20in%20use%20across%20advanced%20models%2C%20such%20as%0AYOLOv8.%20Despite%20their%20growing%20popularity%2C%20the%20lack%20of%20specialized%20libraries%0Ahampers%20the%20polygon-augmentation%20process.%20This%20paper%20introduces%20a%20novel%0Asolution%20to%20this%20challenge%2C%20embodied%20in%20the%20newly%20developed%20AugmenTory%20library.%0ANotably%2C%20AugmenTory%20offers%20reduced%20computational%20demands%20in%20both%20time%20and%20space%0Acompared%20to%20existing%20methods.%20Additionally%2C%20the%20library%20includes%20a%0Apostprocessing%20thresholding%20feature.%20The%20AugmenTory%20package%20is%20publicly%0Aavailable%20on%20GitHub%2C%20where%20interested%20users%20can%20access%20the%20source%20code%3A%0Ahttps%3A//github.com/Smartory/AugmenTory%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04442v1&entry.124074799=Read"},
{"title": "DistGrid: Scalable Scene Reconstruction with Distributed\n  Multi-resolution Hash Grid", "author": "Sidun Liu and Peng Qiao and Zongxin Ye and Wenyu Li and Yong Dou", "abstract": "  Neural Radiance Field~(NeRF) achieves extremely high quality in object-scaled\nand indoor scene reconstruction. However, there exist some challenges when\nreconstructing large-scale scenes. MLP-based NeRFs suffer from limited network\ncapacity, while volume-based NeRFs are heavily memory-consuming when the scene\nresolution increases. Recent approaches propose to geographically partition the\nscene and learn each sub-region using an individual NeRF. Such partitioning\nstrategies help volume-based NeRF exceed the single GPU memory limit and scale\nto larger scenes. However, this approach requires multiple background NeRF to\nhandle out-of-partition rays, which leads to redundancy of learning. Inspired\nby the fact that the background of current partition is the foreground of\nadjacent partition, we propose a scalable scene reconstruction method based on\njoint Multi-resolution Hash Grids, named DistGrid. In this method, the scene is\ndivided into multiple closely-paved yet non-overlapped Axis-Aligned Bounding\nBoxes, and a novel segmented volume rendering method is proposed to handle\ncross-boundary rays, thereby eliminating the need for background NeRFs. The\nexperiments demonstrate that our method outperforms existing methods on all\nevaluated large-scale scenes, and provides visually plausible scene\nreconstruction. The scalability of our method on reconstruction quality is\nfurther evaluated qualitatively and quantitatively.\n", "link": "http://arxiv.org/abs/2405.04416v1", "date": "2024-05-07", "relevancy": 2.4293, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6241}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6056}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DistGrid%3A%20Scalable%20Scene%20Reconstruction%20with%20Distributed%0A%20%20Multi-resolution%20Hash%20Grid&body=Title%3A%20DistGrid%3A%20Scalable%20Scene%20Reconstruction%20with%20Distributed%0A%20%20Multi-resolution%20Hash%20Grid%0AAuthor%3A%20Sidun%20Liu%20and%20Peng%20Qiao%20and%20Zongxin%20Ye%20and%20Wenyu%20Li%20and%20Yong%20Dou%0AAbstract%3A%20%20%20Neural%20Radiance%20Field~%28NeRF%29%20achieves%20extremely%20high%20quality%20in%20object-scaled%0Aand%20indoor%20scene%20reconstruction.%20However%2C%20there%20exist%20some%20challenges%20when%0Areconstructing%20large-scale%20scenes.%20MLP-based%20NeRFs%20suffer%20from%20limited%20network%0Acapacity%2C%20while%20volume-based%20NeRFs%20are%20heavily%20memory-consuming%20when%20the%20scene%0Aresolution%20increases.%20Recent%20approaches%20propose%20to%20geographically%20partition%20the%0Ascene%20and%20learn%20each%20sub-region%20using%20an%20individual%20NeRF.%20Such%20partitioning%0Astrategies%20help%20volume-based%20NeRF%20exceed%20the%20single%20GPU%20memory%20limit%20and%20scale%0Ato%20larger%20scenes.%20However%2C%20this%20approach%20requires%20multiple%20background%20NeRF%20to%0Ahandle%20out-of-partition%20rays%2C%20which%20leads%20to%20redundancy%20of%20learning.%20Inspired%0Aby%20the%20fact%20that%20the%20background%20of%20current%20partition%20is%20the%20foreground%20of%0Aadjacent%20partition%2C%20we%20propose%20a%20scalable%20scene%20reconstruction%20method%20based%20on%0Ajoint%20Multi-resolution%20Hash%20Grids%2C%20named%20DistGrid.%20In%20this%20method%2C%20the%20scene%20is%0Adivided%20into%20multiple%20closely-paved%20yet%20non-overlapped%20Axis-Aligned%20Bounding%0ABoxes%2C%20and%20a%20novel%20segmented%20volume%20rendering%20method%20is%20proposed%20to%20handle%0Across-boundary%20rays%2C%20thereby%20eliminating%20the%20need%20for%20background%20NeRFs.%20The%0Aexperiments%20demonstrate%20that%20our%20method%20outperforms%20existing%20methods%20on%20all%0Aevaluated%20large-scale%20scenes%2C%20and%20provides%20visually%20plausible%20scene%0Areconstruction.%20The%20scalability%20of%20our%20method%20on%20reconstruction%20quality%20is%0Afurther%20evaluated%20qualitatively%20and%20quantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04416v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistGrid%253A%2520Scalable%2520Scene%2520Reconstruction%2520with%2520Distributed%250A%2520%2520Multi-resolution%2520Hash%2520Grid%26entry.906535625%3DSidun%2520Liu%2520and%2520Peng%2520Qiao%2520and%2520Zongxin%2520Ye%2520and%2520Wenyu%2520Li%2520and%2520Yong%2520Dou%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Field~%2528NeRF%2529%2520achieves%2520extremely%2520high%2520quality%2520in%2520object-scaled%250Aand%2520indoor%2520scene%2520reconstruction.%2520However%252C%2520there%2520exist%2520some%2520challenges%2520when%250Areconstructing%2520large-scale%2520scenes.%2520MLP-based%2520NeRFs%2520suffer%2520from%2520limited%2520network%250Acapacity%252C%2520while%2520volume-based%2520NeRFs%2520are%2520heavily%2520memory-consuming%2520when%2520the%2520scene%250Aresolution%2520increases.%2520Recent%2520approaches%2520propose%2520to%2520geographically%2520partition%2520the%250Ascene%2520and%2520learn%2520each%2520sub-region%2520using%2520an%2520individual%2520NeRF.%2520Such%2520partitioning%250Astrategies%2520help%2520volume-based%2520NeRF%2520exceed%2520the%2520single%2520GPU%2520memory%2520limit%2520and%2520scale%250Ato%2520larger%2520scenes.%2520However%252C%2520this%2520approach%2520requires%2520multiple%2520background%2520NeRF%2520to%250Ahandle%2520out-of-partition%2520rays%252C%2520which%2520leads%2520to%2520redundancy%2520of%2520learning.%2520Inspired%250Aby%2520the%2520fact%2520that%2520the%2520background%2520of%2520current%2520partition%2520is%2520the%2520foreground%2520of%250Aadjacent%2520partition%252C%2520we%2520propose%2520a%2520scalable%2520scene%2520reconstruction%2520method%2520based%2520on%250Ajoint%2520Multi-resolution%2520Hash%2520Grids%252C%2520named%2520DistGrid.%2520In%2520this%2520method%252C%2520the%2520scene%2520is%250Adivided%2520into%2520multiple%2520closely-paved%2520yet%2520non-overlapped%2520Axis-Aligned%2520Bounding%250ABoxes%252C%2520and%2520a%2520novel%2520segmented%2520volume%2520rendering%2520method%2520is%2520proposed%2520to%2520handle%250Across-boundary%2520rays%252C%2520thereby%2520eliminating%2520the%2520need%2520for%2520background%2520NeRFs.%2520The%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520methods%2520on%2520all%250Aevaluated%2520large-scale%2520scenes%252C%2520and%2520provides%2520visually%2520plausible%2520scene%250Areconstruction.%2520The%2520scalability%2520of%2520our%2520method%2520on%2520reconstruction%2520quality%2520is%250Afurther%2520evaluated%2520qualitatively%2520and%2520quantitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04416v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DistGrid%3A%20Scalable%20Scene%20Reconstruction%20with%20Distributed%0A%20%20Multi-resolution%20Hash%20Grid&entry.906535625=Sidun%20Liu%20and%20Peng%20Qiao%20and%20Zongxin%20Ye%20and%20Wenyu%20Li%20and%20Yong%20Dou&entry.1292438233=%20%20Neural%20Radiance%20Field~%28NeRF%29%20achieves%20extremely%20high%20quality%20in%20object-scaled%0Aand%20indoor%20scene%20reconstruction.%20However%2C%20there%20exist%20some%20challenges%20when%0Areconstructing%20large-scale%20scenes.%20MLP-based%20NeRFs%20suffer%20from%20limited%20network%0Acapacity%2C%20while%20volume-based%20NeRFs%20are%20heavily%20memory-consuming%20when%20the%20scene%0Aresolution%20increases.%20Recent%20approaches%20propose%20to%20geographically%20partition%20the%0Ascene%20and%20learn%20each%20sub-region%20using%20an%20individual%20NeRF.%20Such%20partitioning%0Astrategies%20help%20volume-based%20NeRF%20exceed%20the%20single%20GPU%20memory%20limit%20and%20scale%0Ato%20larger%20scenes.%20However%2C%20this%20approach%20requires%20multiple%20background%20NeRF%20to%0Ahandle%20out-of-partition%20rays%2C%20which%20leads%20to%20redundancy%20of%20learning.%20Inspired%0Aby%20the%20fact%20that%20the%20background%20of%20current%20partition%20is%20the%20foreground%20of%0Aadjacent%20partition%2C%20we%20propose%20a%20scalable%20scene%20reconstruction%20method%20based%20on%0Ajoint%20Multi-resolution%20Hash%20Grids%2C%20named%20DistGrid.%20In%20this%20method%2C%20the%20scene%20is%0Adivided%20into%20multiple%20closely-paved%20yet%20non-overlapped%20Axis-Aligned%20Bounding%0ABoxes%2C%20and%20a%20novel%20segmented%20volume%20rendering%20method%20is%20proposed%20to%20handle%0Across-boundary%20rays%2C%20thereby%20eliminating%20the%20need%20for%20background%20NeRFs.%20The%0Aexperiments%20demonstrate%20that%20our%20method%20outperforms%20existing%20methods%20on%20all%0Aevaluated%20large-scale%20scenes%2C%20and%20provides%20visually%20plausible%20scene%0Areconstruction.%20The%20scalability%20of%20our%20method%20on%20reconstruction%20quality%20is%0Afurther%20evaluated%20qualitatively%20and%20quantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04416v1&entry.124074799=Read"},
{"title": "Unveiling the optimization process of Physics Informed Neural Networks:\n  How accurate and competitive can PINNs be?", "author": "Jorge F. Urb\u00e1n and Petros Stefanou and Jos\u00e9 A. Pons", "abstract": "  This study investigates the potential accuracy boundaries of physics-informed\nneural networks, contrasting their approach with previous similar works and\ntraditional numerical methods. We find that selecting improved optimization\nalgorithms significantly enhances the accuracy of the results. Simple\nmodifications to the loss function may also improve precision, offering an\nadditional avenue for enhancement. Despite optimization algorithms having a\ngreater impact on convergence than adjustments to the loss function, practical\nconsiderations often favor tweaking the latter due to ease of implementation.\nOn a global scale, the integration of an enhanced optimizer and a marginally\nadjusted loss function enables a reduction in the loss function by several\norders of magnitude across diverse physical problems. Consequently, our results\nobtained using compact networks (typically comprising 2 or 3 layers of 20-30\nneurons) achieve accuracies comparable to finite difference schemes employing\nthousands of grid points. This study encourages the continued advancement of\nPINNs and associated optimization techniques for broader applications across\nvarious fields.\n", "link": "http://arxiv.org/abs/2405.04230v1", "date": "2024-05-07", "relevancy": 2.4184, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5038}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4744}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20the%20optimization%20process%20of%20Physics%20Informed%20Neural%20Networks%3A%0A%20%20How%20accurate%20and%20competitive%20can%20PINNs%20be%3F&body=Title%3A%20Unveiling%20the%20optimization%20process%20of%20Physics%20Informed%20Neural%20Networks%3A%0A%20%20How%20accurate%20and%20competitive%20can%20PINNs%20be%3F%0AAuthor%3A%20Jorge%20F.%20Urb%C3%A1n%20and%20Petros%20Stefanou%20and%20Jos%C3%A9%20A.%20Pons%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20potential%20accuracy%20boundaries%20of%20physics-informed%0Aneural%20networks%2C%20contrasting%20their%20approach%20with%20previous%20similar%20works%20and%0Atraditional%20numerical%20methods.%20We%20find%20that%20selecting%20improved%20optimization%0Aalgorithms%20significantly%20enhances%20the%20accuracy%20of%20the%20results.%20Simple%0Amodifications%20to%20the%20loss%20function%20may%20also%20improve%20precision%2C%20offering%20an%0Aadditional%20avenue%20for%20enhancement.%20Despite%20optimization%20algorithms%20having%20a%0Agreater%20impact%20on%20convergence%20than%20adjustments%20to%20the%20loss%20function%2C%20practical%0Aconsiderations%20often%20favor%20tweaking%20the%20latter%20due%20to%20ease%20of%20implementation.%0AOn%20a%20global%20scale%2C%20the%20integration%20of%20an%20enhanced%20optimizer%20and%20a%20marginally%0Aadjusted%20loss%20function%20enables%20a%20reduction%20in%20the%20loss%20function%20by%20several%0Aorders%20of%20magnitude%20across%20diverse%20physical%20problems.%20Consequently%2C%20our%20results%0Aobtained%20using%20compact%20networks%20%28typically%20comprising%202%20or%203%20layers%20of%2020-30%0Aneurons%29%20achieve%20accuracies%20comparable%20to%20finite%20difference%20schemes%20employing%0Athousands%20of%20grid%20points.%20This%20study%20encourages%20the%20continued%20advancement%20of%0APINNs%20and%20associated%20optimization%20techniques%20for%20broader%20applications%20across%0Avarious%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520the%2520optimization%2520process%2520of%2520Physics%2520Informed%2520Neural%2520Networks%253A%250A%2520%2520How%2520accurate%2520and%2520competitive%2520can%2520PINNs%2520be%253F%26entry.906535625%3DJorge%2520F.%2520Urb%25C3%25A1n%2520and%2520Petros%2520Stefanou%2520and%2520Jos%25C3%25A9%2520A.%2520Pons%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520potential%2520accuracy%2520boundaries%2520of%2520physics-informed%250Aneural%2520networks%252C%2520contrasting%2520their%2520approach%2520with%2520previous%2520similar%2520works%2520and%250Atraditional%2520numerical%2520methods.%2520We%2520find%2520that%2520selecting%2520improved%2520optimization%250Aalgorithms%2520significantly%2520enhances%2520the%2520accuracy%2520of%2520the%2520results.%2520Simple%250Amodifications%2520to%2520the%2520loss%2520function%2520may%2520also%2520improve%2520precision%252C%2520offering%2520an%250Aadditional%2520avenue%2520for%2520enhancement.%2520Despite%2520optimization%2520algorithms%2520having%2520a%250Agreater%2520impact%2520on%2520convergence%2520than%2520adjustments%2520to%2520the%2520loss%2520function%252C%2520practical%250Aconsiderations%2520often%2520favor%2520tweaking%2520the%2520latter%2520due%2520to%2520ease%2520of%2520implementation.%250AOn%2520a%2520global%2520scale%252C%2520the%2520integration%2520of%2520an%2520enhanced%2520optimizer%2520and%2520a%2520marginally%250Aadjusted%2520loss%2520function%2520enables%2520a%2520reduction%2520in%2520the%2520loss%2520function%2520by%2520several%250Aorders%2520of%2520magnitude%2520across%2520diverse%2520physical%2520problems.%2520Consequently%252C%2520our%2520results%250Aobtained%2520using%2520compact%2520networks%2520%2528typically%2520comprising%25202%2520or%25203%2520layers%2520of%252020-30%250Aneurons%2529%2520achieve%2520accuracies%2520comparable%2520to%2520finite%2520difference%2520schemes%2520employing%250Athousands%2520of%2520grid%2520points.%2520This%2520study%2520encourages%2520the%2520continued%2520advancement%2520of%250APINNs%2520and%2520associated%2520optimization%2520techniques%2520for%2520broader%2520applications%2520across%250Avarious%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20optimization%20process%20of%20Physics%20Informed%20Neural%20Networks%3A%0A%20%20How%20accurate%20and%20competitive%20can%20PINNs%20be%3F&entry.906535625=Jorge%20F.%20Urb%C3%A1n%20and%20Petros%20Stefanou%20and%20Jos%C3%A9%20A.%20Pons&entry.1292438233=%20%20This%20study%20investigates%20the%20potential%20accuracy%20boundaries%20of%20physics-informed%0Aneural%20networks%2C%20contrasting%20their%20approach%20with%20previous%20similar%20works%20and%0Atraditional%20numerical%20methods.%20We%20find%20that%20selecting%20improved%20optimization%0Aalgorithms%20significantly%20enhances%20the%20accuracy%20of%20the%20results.%20Simple%0Amodifications%20to%20the%20loss%20function%20may%20also%20improve%20precision%2C%20offering%20an%0Aadditional%20avenue%20for%20enhancement.%20Despite%20optimization%20algorithms%20having%20a%0Agreater%20impact%20on%20convergence%20than%20adjustments%20to%20the%20loss%20function%2C%20practical%0Aconsiderations%20often%20favor%20tweaking%20the%20latter%20due%20to%20ease%20of%20implementation.%0AOn%20a%20global%20scale%2C%20the%20integration%20of%20an%20enhanced%20optimizer%20and%20a%20marginally%0Aadjusted%20loss%20function%20enables%20a%20reduction%20in%20the%20loss%20function%20by%20several%0Aorders%20of%20magnitude%20across%20diverse%20physical%20problems.%20Consequently%2C%20our%20results%0Aobtained%20using%20compact%20networks%20%28typically%20comprising%202%20or%203%20layers%20of%2020-30%0Aneurons%29%20achieve%20accuracies%20comparable%20to%20finite%20difference%20schemes%20employing%0Athousands%20of%20grid%20points.%20This%20study%20encourages%20the%20continued%20advancement%20of%0APINNs%20and%20associated%20optimization%20techniques%20for%20broader%20applications%20across%0Avarious%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04230v1&entry.124074799=Read"},
{"title": "Enhancing Boundary Segmentation for Topological Accuracy with\n  Skeleton-based Methods", "author": "Chuni Liu and Boyuan Ma and Xiaojuan Ban and Yujie Xie and Hao Wang and Weihua Xue and Jingchao Ma and Ke Xu", "abstract": "  Topological consistency plays a crucial role in the task of boundary\nsegmentation for reticular images, such as cell membrane segmentation in neuron\nelectron microscopic images, grain boundary segmentation in material\nmicroscopic images and road segmentation in aerial images. In these fields,\ntopological changes in segmentation results have a serious impact on the\ndownstream tasks, which can even exceed the misalignment of the boundary\nitself. To enhance the topology accuracy in segmentation results, we propose\nthe Skea-Topo Aware loss, which is a novel loss function that takes into\naccount the shape of each object and topological significance of the pixels. It\nconsists of two components. First, a skeleton-aware weighted loss improves the\nsegmentation accuracy by better modeling the object geometry with skeletons.\nSecond, a boundary rectified term effectively identifies and emphasizes\ntopological critical pixels in the prediction errors using both foreground and\nbackground skeletons in the ground truth and predictions. Experiments prove\nthat our method improves topological consistency by up to 7 points in VI\ncompared to 13 state-of-art methods, based on objective and subjective\nassessments across three different boundary segmentation datasets. The code is\navailable at https://github.com/clovermini/Skea_topo.\n", "link": "http://arxiv.org/abs/2404.18539v2", "date": "2024-05-07", "relevancy": 2.4086, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4982}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4749}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Boundary%20Segmentation%20for%20Topological%20Accuracy%20with%0A%20%20Skeleton-based%20Methods&body=Title%3A%20Enhancing%20Boundary%20Segmentation%20for%20Topological%20Accuracy%20with%0A%20%20Skeleton-based%20Methods%0AAuthor%3A%20Chuni%20Liu%20and%20Boyuan%20Ma%20and%20Xiaojuan%20Ban%20and%20Yujie%20Xie%20and%20Hao%20Wang%20and%20Weihua%20Xue%20and%20Jingchao%20Ma%20and%20Ke%20Xu%0AAbstract%3A%20%20%20Topological%20consistency%20plays%20a%20crucial%20role%20in%20the%20task%20of%20boundary%0Asegmentation%20for%20reticular%20images%2C%20such%20as%20cell%20membrane%20segmentation%20in%20neuron%0Aelectron%20microscopic%20images%2C%20grain%20boundary%20segmentation%20in%20material%0Amicroscopic%20images%20and%20road%20segmentation%20in%20aerial%20images.%20In%20these%20fields%2C%0Atopological%20changes%20in%20segmentation%20results%20have%20a%20serious%20impact%20on%20the%0Adownstream%20tasks%2C%20which%20can%20even%20exceed%20the%20misalignment%20of%20the%20boundary%0Aitself.%20To%20enhance%20the%20topology%20accuracy%20in%20segmentation%20results%2C%20we%20propose%0Athe%20Skea-Topo%20Aware%20loss%2C%20which%20is%20a%20novel%20loss%20function%20that%20takes%20into%0Aaccount%20the%20shape%20of%20each%20object%20and%20topological%20significance%20of%20the%20pixels.%20It%0Aconsists%20of%20two%20components.%20First%2C%20a%20skeleton-aware%20weighted%20loss%20improves%20the%0Asegmentation%20accuracy%20by%20better%20modeling%20the%20object%20geometry%20with%20skeletons.%0ASecond%2C%20a%20boundary%20rectified%20term%20effectively%20identifies%20and%20emphasizes%0Atopological%20critical%20pixels%20in%20the%20prediction%20errors%20using%20both%20foreground%20and%0Abackground%20skeletons%20in%20the%20ground%20truth%20and%20predictions.%20Experiments%20prove%0Athat%20our%20method%20improves%20topological%20consistency%20by%20up%20to%207%20points%20in%20VI%0Acompared%20to%2013%20state-of-art%20methods%2C%20based%20on%20objective%20and%20subjective%0Aassessments%20across%20three%20different%20boundary%20segmentation%20datasets.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/clovermini/Skea_topo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18539v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Boundary%2520Segmentation%2520for%2520Topological%2520Accuracy%2520with%250A%2520%2520Skeleton-based%2520Methods%26entry.906535625%3DChuni%2520Liu%2520and%2520Boyuan%2520Ma%2520and%2520Xiaojuan%2520Ban%2520and%2520Yujie%2520Xie%2520and%2520Hao%2520Wang%2520and%2520Weihua%2520Xue%2520and%2520Jingchao%2520Ma%2520and%2520Ke%2520Xu%26entry.1292438233%3D%2520%2520Topological%2520consistency%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520task%2520of%2520boundary%250Asegmentation%2520for%2520reticular%2520images%252C%2520such%2520as%2520cell%2520membrane%2520segmentation%2520in%2520neuron%250Aelectron%2520microscopic%2520images%252C%2520grain%2520boundary%2520segmentation%2520in%2520material%250Amicroscopic%2520images%2520and%2520road%2520segmentation%2520in%2520aerial%2520images.%2520In%2520these%2520fields%252C%250Atopological%2520changes%2520in%2520segmentation%2520results%2520have%2520a%2520serious%2520impact%2520on%2520the%250Adownstream%2520tasks%252C%2520which%2520can%2520even%2520exceed%2520the%2520misalignment%2520of%2520the%2520boundary%250Aitself.%2520To%2520enhance%2520the%2520topology%2520accuracy%2520in%2520segmentation%2520results%252C%2520we%2520propose%250Athe%2520Skea-Topo%2520Aware%2520loss%252C%2520which%2520is%2520a%2520novel%2520loss%2520function%2520that%2520takes%2520into%250Aaccount%2520the%2520shape%2520of%2520each%2520object%2520and%2520topological%2520significance%2520of%2520the%2520pixels.%2520It%250Aconsists%2520of%2520two%2520components.%2520First%252C%2520a%2520skeleton-aware%2520weighted%2520loss%2520improves%2520the%250Asegmentation%2520accuracy%2520by%2520better%2520modeling%2520the%2520object%2520geometry%2520with%2520skeletons.%250ASecond%252C%2520a%2520boundary%2520rectified%2520term%2520effectively%2520identifies%2520and%2520emphasizes%250Atopological%2520critical%2520pixels%2520in%2520the%2520prediction%2520errors%2520using%2520both%2520foreground%2520and%250Abackground%2520skeletons%2520in%2520the%2520ground%2520truth%2520and%2520predictions.%2520Experiments%2520prove%250Athat%2520our%2520method%2520improves%2520topological%2520consistency%2520by%2520up%2520to%25207%2520points%2520in%2520VI%250Acompared%2520to%252013%2520state-of-art%2520methods%252C%2520based%2520on%2520objective%2520and%2520subjective%250Aassessments%2520across%2520three%2520different%2520boundary%2520segmentation%2520datasets.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/clovermini/Skea_topo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18539v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Boundary%20Segmentation%20for%20Topological%20Accuracy%20with%0A%20%20Skeleton-based%20Methods&entry.906535625=Chuni%20Liu%20and%20Boyuan%20Ma%20and%20Xiaojuan%20Ban%20and%20Yujie%20Xie%20and%20Hao%20Wang%20and%20Weihua%20Xue%20and%20Jingchao%20Ma%20and%20Ke%20Xu&entry.1292438233=%20%20Topological%20consistency%20plays%20a%20crucial%20role%20in%20the%20task%20of%20boundary%0Asegmentation%20for%20reticular%20images%2C%20such%20as%20cell%20membrane%20segmentation%20in%20neuron%0Aelectron%20microscopic%20images%2C%20grain%20boundary%20segmentation%20in%20material%0Amicroscopic%20images%20and%20road%20segmentation%20in%20aerial%20images.%20In%20these%20fields%2C%0Atopological%20changes%20in%20segmentation%20results%20have%20a%20serious%20impact%20on%20the%0Adownstream%20tasks%2C%20which%20can%20even%20exceed%20the%20misalignment%20of%20the%20boundary%0Aitself.%20To%20enhance%20the%20topology%20accuracy%20in%20segmentation%20results%2C%20we%20propose%0Athe%20Skea-Topo%20Aware%20loss%2C%20which%20is%20a%20novel%20loss%20function%20that%20takes%20into%0Aaccount%20the%20shape%20of%20each%20object%20and%20topological%20significance%20of%20the%20pixels.%20It%0Aconsists%20of%20two%20components.%20First%2C%20a%20skeleton-aware%20weighted%20loss%20improves%20the%0Asegmentation%20accuracy%20by%20better%20modeling%20the%20object%20geometry%20with%20skeletons.%0ASecond%2C%20a%20boundary%20rectified%20term%20effectively%20identifies%20and%20emphasizes%0Atopological%20critical%20pixels%20in%20the%20prediction%20errors%20using%20both%20foreground%20and%0Abackground%20skeletons%20in%20the%20ground%20truth%20and%20predictions.%20Experiments%20prove%0Athat%20our%20method%20improves%20topological%20consistency%20by%20up%20to%207%20points%20in%20VI%0Acompared%20to%2013%20state-of-art%20methods%2C%20based%20on%20objective%20and%20subjective%0Aassessments%20across%20three%20different%20boundary%20segmentation%20datasets.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/clovermini/Skea_topo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18539v2&entry.124074799=Read"},
{"title": "The Role of Pseudo-labels in Self-training Linear Classifiers on\n  High-dimensional Gaussian Mixture Data", "author": "Takashi Takahashi", "abstract": "  Self-training (ST) is a simple yet effective semi-supervised learning method.\nHowever, why and how ST improves generalization performance by using\npotentially erroneous pseudo-labels is still not well understood. To deepen the\nunderstanding of ST, we derive and analyze a sharp characterization of the\nbehavior of iterative ST when training a linear classifier by minimizing the\nridge-regularized convex loss on binary Gaussian mixtures, in the asymptotic\nlimit where input dimension and data size diverge proportionally. The results\nshow that ST improves generalization in different ways depending on the number\nof iterations. When the number of iterations is small, ST improves\ngeneralization performance by fitting the model to relatively reliable\npseudo-labels and updating the model parameters by a large amount at each\niteration. This suggests that ST works intuitively. On the other hand, with\nmany iterations, ST can gradually improve the direction of the classification\nplane by updating the model parameters incrementally, using soft labels and\nsmall regularization. It is argued that this is because the small update of ST\ncan extract information from the data in an almost noiseless way. However, in\nthe presence of label imbalance, the generalization performance of ST\nunderperforms supervised learning with true labels. To overcome this, two\nheuristics are proposed to enable ST to achieve nearly compatible performance\nwith supervised learning even with significant label imbalance.\n", "link": "http://arxiv.org/abs/2205.07739v3", "date": "2024-05-07", "relevancy": 2.3608, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4909}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4806}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Role%20of%20Pseudo-labels%20in%20Self-training%20Linear%20Classifiers%20on%0A%20%20High-dimensional%20Gaussian%20Mixture%20Data&body=Title%3A%20The%20Role%20of%20Pseudo-labels%20in%20Self-training%20Linear%20Classifiers%20on%0A%20%20High-dimensional%20Gaussian%20Mixture%20Data%0AAuthor%3A%20Takashi%20Takahashi%0AAbstract%3A%20%20%20Self-training%20%28ST%29%20is%20a%20simple%20yet%20effective%20semi-supervised%20learning%20method.%0AHowever%2C%20why%20and%20how%20ST%20improves%20generalization%20performance%20by%20using%0Apotentially%20erroneous%20pseudo-labels%20is%20still%20not%20well%20understood.%20To%20deepen%20the%0Aunderstanding%20of%20ST%2C%20we%20derive%20and%20analyze%20a%20sharp%20characterization%20of%20the%0Abehavior%20of%20iterative%20ST%20when%20training%20a%20linear%20classifier%20by%20minimizing%20the%0Aridge-regularized%20convex%20loss%20on%20binary%20Gaussian%20mixtures%2C%20in%20the%20asymptotic%0Alimit%20where%20input%20dimension%20and%20data%20size%20diverge%20proportionally.%20The%20results%0Ashow%20that%20ST%20improves%20generalization%20in%20different%20ways%20depending%20on%20the%20number%0Aof%20iterations.%20When%20the%20number%20of%20iterations%20is%20small%2C%20ST%20improves%0Ageneralization%20performance%20by%20fitting%20the%20model%20to%20relatively%20reliable%0Apseudo-labels%20and%20updating%20the%20model%20parameters%20by%20a%20large%20amount%20at%20each%0Aiteration.%20This%20suggests%20that%20ST%20works%20intuitively.%20On%20the%20other%20hand%2C%20with%0Amany%20iterations%2C%20ST%20can%20gradually%20improve%20the%20direction%20of%20the%20classification%0Aplane%20by%20updating%20the%20model%20parameters%20incrementally%2C%20using%20soft%20labels%20and%0Asmall%20regularization.%20It%20is%20argued%20that%20this%20is%20because%20the%20small%20update%20of%20ST%0Acan%20extract%20information%20from%20the%20data%20in%20an%20almost%20noiseless%20way.%20However%2C%20in%0Athe%20presence%20of%20label%20imbalance%2C%20the%20generalization%20performance%20of%20ST%0Aunderperforms%20supervised%20learning%20with%20true%20labels.%20To%20overcome%20this%2C%20two%0Aheuristics%20are%20proposed%20to%20enable%20ST%20to%20achieve%20nearly%20compatible%20performance%0Awith%20supervised%20learning%20even%20with%20significant%20label%20imbalance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.07739v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Role%2520of%2520Pseudo-labels%2520in%2520Self-training%2520Linear%2520Classifiers%2520on%250A%2520%2520High-dimensional%2520Gaussian%2520Mixture%2520Data%26entry.906535625%3DTakashi%2520Takahashi%26entry.1292438233%3D%2520%2520Self-training%2520%2528ST%2529%2520is%2520a%2520simple%2520yet%2520effective%2520semi-supervised%2520learning%2520method.%250AHowever%252C%2520why%2520and%2520how%2520ST%2520improves%2520generalization%2520performance%2520by%2520using%250Apotentially%2520erroneous%2520pseudo-labels%2520is%2520still%2520not%2520well%2520understood.%2520To%2520deepen%2520the%250Aunderstanding%2520of%2520ST%252C%2520we%2520derive%2520and%2520analyze%2520a%2520sharp%2520characterization%2520of%2520the%250Abehavior%2520of%2520iterative%2520ST%2520when%2520training%2520a%2520linear%2520classifier%2520by%2520minimizing%2520the%250Aridge-regularized%2520convex%2520loss%2520on%2520binary%2520Gaussian%2520mixtures%252C%2520in%2520the%2520asymptotic%250Alimit%2520where%2520input%2520dimension%2520and%2520data%2520size%2520diverge%2520proportionally.%2520The%2520results%250Ashow%2520that%2520ST%2520improves%2520generalization%2520in%2520different%2520ways%2520depending%2520on%2520the%2520number%250Aof%2520iterations.%2520When%2520the%2520number%2520of%2520iterations%2520is%2520small%252C%2520ST%2520improves%250Ageneralization%2520performance%2520by%2520fitting%2520the%2520model%2520to%2520relatively%2520reliable%250Apseudo-labels%2520and%2520updating%2520the%2520model%2520parameters%2520by%2520a%2520large%2520amount%2520at%2520each%250Aiteration.%2520This%2520suggests%2520that%2520ST%2520works%2520intuitively.%2520On%2520the%2520other%2520hand%252C%2520with%250Amany%2520iterations%252C%2520ST%2520can%2520gradually%2520improve%2520the%2520direction%2520of%2520the%2520classification%250Aplane%2520by%2520updating%2520the%2520model%2520parameters%2520incrementally%252C%2520using%2520soft%2520labels%2520and%250Asmall%2520regularization.%2520It%2520is%2520argued%2520that%2520this%2520is%2520because%2520the%2520small%2520update%2520of%2520ST%250Acan%2520extract%2520information%2520from%2520the%2520data%2520in%2520an%2520almost%2520noiseless%2520way.%2520However%252C%2520in%250Athe%2520presence%2520of%2520label%2520imbalance%252C%2520the%2520generalization%2520performance%2520of%2520ST%250Aunderperforms%2520supervised%2520learning%2520with%2520true%2520labels.%2520To%2520overcome%2520this%252C%2520two%250Aheuristics%2520are%2520proposed%2520to%2520enable%2520ST%2520to%2520achieve%2520nearly%2520compatible%2520performance%250Awith%2520supervised%2520learning%2520even%2520with%2520significant%2520label%2520imbalance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2205.07739v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Role%20of%20Pseudo-labels%20in%20Self-training%20Linear%20Classifiers%20on%0A%20%20High-dimensional%20Gaussian%20Mixture%20Data&entry.906535625=Takashi%20Takahashi&entry.1292438233=%20%20Self-training%20%28ST%29%20is%20a%20simple%20yet%20effective%20semi-supervised%20learning%20method.%0AHowever%2C%20why%20and%20how%20ST%20improves%20generalization%20performance%20by%20using%0Apotentially%20erroneous%20pseudo-labels%20is%20still%20not%20well%20understood.%20To%20deepen%20the%0Aunderstanding%20of%20ST%2C%20we%20derive%20and%20analyze%20a%20sharp%20characterization%20of%20the%0Abehavior%20of%20iterative%20ST%20when%20training%20a%20linear%20classifier%20by%20minimizing%20the%0Aridge-regularized%20convex%20loss%20on%20binary%20Gaussian%20mixtures%2C%20in%20the%20asymptotic%0Alimit%20where%20input%20dimension%20and%20data%20size%20diverge%20proportionally.%20The%20results%0Ashow%20that%20ST%20improves%20generalization%20in%20different%20ways%20depending%20on%20the%20number%0Aof%20iterations.%20When%20the%20number%20of%20iterations%20is%20small%2C%20ST%20improves%0Ageneralization%20performance%20by%20fitting%20the%20model%20to%20relatively%20reliable%0Apseudo-labels%20and%20updating%20the%20model%20parameters%20by%20a%20large%20amount%20at%20each%0Aiteration.%20This%20suggests%20that%20ST%20works%20intuitively.%20On%20the%20other%20hand%2C%20with%0Amany%20iterations%2C%20ST%20can%20gradually%20improve%20the%20direction%20of%20the%20classification%0Aplane%20by%20updating%20the%20model%20parameters%20incrementally%2C%20using%20soft%20labels%20and%0Asmall%20regularization.%20It%20is%20argued%20that%20this%20is%20because%20the%20small%20update%20of%20ST%0Acan%20extract%20information%20from%20the%20data%20in%20an%20almost%20noiseless%20way.%20However%2C%20in%0Athe%20presence%20of%20label%20imbalance%2C%20the%20generalization%20performance%20of%20ST%0Aunderperforms%20supervised%20learning%20with%20true%20labels.%20To%20overcome%20this%2C%20two%0Aheuristics%20are%20proposed%20to%20enable%20ST%20to%20achieve%20nearly%20compatible%20performance%0Awith%20supervised%20learning%20even%20with%20significant%20label%20imbalance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.07739v3&entry.124074799=Read"},
{"title": "Toward Deep Drum Source Separation", "author": "Alessandro Ilic Mezza and Riccardo Giampiccolo and Alberto Bernardini and Augusto Sarti", "abstract": "  In the past, the field of drum source separation faced significant challenges\ndue to limited data availability, hindering the adoption of cutting-edge deep\nlearning methods that have found success in other related audio applications.\nIn this manuscript, we introduce StemGMD, a large-scale audio dataset of\nisolated single-instrument drum stems. Each audio clip is synthesized from MIDI\nrecordings of expressive drums performances using ten real-sounding acoustic\ndrum kits. Totaling 1224 hours, StemGMD is the largest audio dataset of drums\nto date and the first to comprise isolated audio clips for every instrument in\na canonical nine-piece drum kit. We leverage StemGMD to develop LarsNet, a\nnovel deep drum source separation model. Through a bank of dedicated U-Nets,\nLarsNet can separate five stems from a stereo drum mixture faster than\nreal-time and is shown to significantly outperform state-of-the-art nonnegative\nspectro-temporal factorization methods.\n", "link": "http://arxiv.org/abs/2312.09663v2", "date": "2024-05-07", "relevancy": 2.3525, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4795}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4676}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Deep%20Drum%20Source%20Separation&body=Title%3A%20Toward%20Deep%20Drum%20Source%20Separation%0AAuthor%3A%20Alessandro%20Ilic%20Mezza%20and%20Riccardo%20Giampiccolo%20and%20Alberto%20Bernardini%20and%20Augusto%20Sarti%0AAbstract%3A%20%20%20In%20the%20past%2C%20the%20field%20of%20drum%20source%20separation%20faced%20significant%20challenges%0Adue%20to%20limited%20data%20availability%2C%20hindering%20the%20adoption%20of%20cutting-edge%20deep%0Alearning%20methods%20that%20have%20found%20success%20in%20other%20related%20audio%20applications.%0AIn%20this%20manuscript%2C%20we%20introduce%20StemGMD%2C%20a%20large-scale%20audio%20dataset%20of%0Aisolated%20single-instrument%20drum%20stems.%20Each%20audio%20clip%20is%20synthesized%20from%20MIDI%0Arecordings%20of%20expressive%20drums%20performances%20using%20ten%20real-sounding%20acoustic%0Adrum%20kits.%20Totaling%201224%20hours%2C%20StemGMD%20is%20the%20largest%20audio%20dataset%20of%20drums%0Ato%20date%20and%20the%20first%20to%20comprise%20isolated%20audio%20clips%20for%20every%20instrument%20in%0Aa%20canonical%20nine-piece%20drum%20kit.%20We%20leverage%20StemGMD%20to%20develop%20LarsNet%2C%20a%0Anovel%20deep%20drum%20source%20separation%20model.%20Through%20a%20bank%20of%20dedicated%20U-Nets%2C%0ALarsNet%20can%20separate%20five%20stems%20from%20a%20stereo%20drum%20mixture%20faster%20than%0Areal-time%20and%20is%20shown%20to%20significantly%20outperform%20state-of-the-art%20nonnegative%0Aspectro-temporal%20factorization%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09663v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Deep%2520Drum%2520Source%2520Separation%26entry.906535625%3DAlessandro%2520Ilic%2520Mezza%2520and%2520Riccardo%2520Giampiccolo%2520and%2520Alberto%2520Bernardini%2520and%2520Augusto%2520Sarti%26entry.1292438233%3D%2520%2520In%2520the%2520past%252C%2520the%2520field%2520of%2520drum%2520source%2520separation%2520faced%2520significant%2520challenges%250Adue%2520to%2520limited%2520data%2520availability%252C%2520hindering%2520the%2520adoption%2520of%2520cutting-edge%2520deep%250Alearning%2520methods%2520that%2520have%2520found%2520success%2520in%2520other%2520related%2520audio%2520applications.%250AIn%2520this%2520manuscript%252C%2520we%2520introduce%2520StemGMD%252C%2520a%2520large-scale%2520audio%2520dataset%2520of%250Aisolated%2520single-instrument%2520drum%2520stems.%2520Each%2520audio%2520clip%2520is%2520synthesized%2520from%2520MIDI%250Arecordings%2520of%2520expressive%2520drums%2520performances%2520using%2520ten%2520real-sounding%2520acoustic%250Adrum%2520kits.%2520Totaling%25201224%2520hours%252C%2520StemGMD%2520is%2520the%2520largest%2520audio%2520dataset%2520of%2520drums%250Ato%2520date%2520and%2520the%2520first%2520to%2520comprise%2520isolated%2520audio%2520clips%2520for%2520every%2520instrument%2520in%250Aa%2520canonical%2520nine-piece%2520drum%2520kit.%2520We%2520leverage%2520StemGMD%2520to%2520develop%2520LarsNet%252C%2520a%250Anovel%2520deep%2520drum%2520source%2520separation%2520model.%2520Through%2520a%2520bank%2520of%2520dedicated%2520U-Nets%252C%250ALarsNet%2520can%2520separate%2520five%2520stems%2520from%2520a%2520stereo%2520drum%2520mixture%2520faster%2520than%250Areal-time%2520and%2520is%2520shown%2520to%2520significantly%2520outperform%2520state-of-the-art%2520nonnegative%250Aspectro-temporal%2520factorization%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.09663v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Deep%20Drum%20Source%20Separation&entry.906535625=Alessandro%20Ilic%20Mezza%20and%20Riccardo%20Giampiccolo%20and%20Alberto%20Bernardini%20and%20Augusto%20Sarti&entry.1292438233=%20%20In%20the%20past%2C%20the%20field%20of%20drum%20source%20separation%20faced%20significant%20challenges%0Adue%20to%20limited%20data%20availability%2C%20hindering%20the%20adoption%20of%20cutting-edge%20deep%0Alearning%20methods%20that%20have%20found%20success%20in%20other%20related%20audio%20applications.%0AIn%20this%20manuscript%2C%20we%20introduce%20StemGMD%2C%20a%20large-scale%20audio%20dataset%20of%0Aisolated%20single-instrument%20drum%20stems.%20Each%20audio%20clip%20is%20synthesized%20from%20MIDI%0Arecordings%20of%20expressive%20drums%20performances%20using%20ten%20real-sounding%20acoustic%0Adrum%20kits.%20Totaling%201224%20hours%2C%20StemGMD%20is%20the%20largest%20audio%20dataset%20of%20drums%0Ato%20date%20and%20the%20first%20to%20comprise%20isolated%20audio%20clips%20for%20every%20instrument%20in%0Aa%20canonical%20nine-piece%20drum%20kit.%20We%20leverage%20StemGMD%20to%20develop%20LarsNet%2C%20a%0Anovel%20deep%20drum%20source%20separation%20model.%20Through%20a%20bank%20of%20dedicated%20U-Nets%2C%0ALarsNet%20can%20separate%20five%20stems%20from%20a%20stereo%20drum%20mixture%20faster%20than%0Areal-time%20and%20is%20shown%20to%20significantly%20outperform%20state-of-the-art%20nonnegative%0Aspectro-temporal%20factorization%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09663v2&entry.124074799=Read"},
{"title": "SDDGR: Stable Diffusion-based Deep Generative Replay for Class\n  Incremental Object Detection", "author": "Junsu Kim and Hoseong Cho and Jihyeon Kim and Yihalem Yimolal Tiruneh and Seungryul Baek", "abstract": "  In the field of class incremental learning (CIL), generative replay has\nbecome increasingly prominent as a method to mitigate the catastrophic\nforgetting, alongside the continuous improvements in generative models.\nHowever, its application in class incremental object detection (CIOD) has been\nsignificantly limited, primarily due to the complexities of scenes involving\nmultiple labels. In this paper, we propose a novel approach called stable\ndiffusion deep generative replay (SDDGR) for CIOD. Our method utilizes a\ndiffusion-based generative model with pre-trained text-to-diffusion networks to\ngenerate realistic and diverse synthetic images. SDDGR incorporates an\niterative refinement strategy to produce high-quality images encompassing old\nclasses. Additionally, we adopt an L2 knowledge distillation technique to\nimprove the retention of prior knowledge in synthetic images. Furthermore, our\napproach includes pseudo-labeling for old objects within new task images,\npreventing misclassification as background elements. Extensive experiments on\nthe COCO 2017 dataset demonstrate that SDDGR significantly outperforms existing\nalgorithms, achieving a new state-of-the-art in various CIOD scenarios. The\nsource code will be made available to the public.\n", "link": "http://arxiv.org/abs/2402.17323v2", "date": "2024-05-07", "relevancy": 2.3515, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5947}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5874}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SDDGR%3A%20Stable%20Diffusion-based%20Deep%20Generative%20Replay%20for%20Class%0A%20%20Incremental%20Object%20Detection&body=Title%3A%20SDDGR%3A%20Stable%20Diffusion-based%20Deep%20Generative%20Replay%20for%20Class%0A%20%20Incremental%20Object%20Detection%0AAuthor%3A%20Junsu%20Kim%20and%20Hoseong%20Cho%20and%20Jihyeon%20Kim%20and%20Yihalem%20Yimolal%20Tiruneh%20and%20Seungryul%20Baek%0AAbstract%3A%20%20%20In%20the%20field%20of%20class%20incremental%20learning%20%28CIL%29%2C%20generative%20replay%20has%0Abecome%20increasingly%20prominent%20as%20a%20method%20to%20mitigate%20the%20catastrophic%0Aforgetting%2C%20alongside%20the%20continuous%20improvements%20in%20generative%20models.%0AHowever%2C%20its%20application%20in%20class%20incremental%20object%20detection%20%28CIOD%29%20has%20been%0Asignificantly%20limited%2C%20primarily%20due%20to%20the%20complexities%20of%20scenes%20involving%0Amultiple%20labels.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20called%20stable%0Adiffusion%20deep%20generative%20replay%20%28SDDGR%29%20for%20CIOD.%20Our%20method%20utilizes%20a%0Adiffusion-based%20generative%20model%20with%20pre-trained%20text-to-diffusion%20networks%20to%0Agenerate%20realistic%20and%20diverse%20synthetic%20images.%20SDDGR%20incorporates%20an%0Aiterative%20refinement%20strategy%20to%20produce%20high-quality%20images%20encompassing%20old%0Aclasses.%20Additionally%2C%20we%20adopt%20an%20L2%20knowledge%20distillation%20technique%20to%0Aimprove%20the%20retention%20of%20prior%20knowledge%20in%20synthetic%20images.%20Furthermore%2C%20our%0Aapproach%20includes%20pseudo-labeling%20for%20old%20objects%20within%20new%20task%20images%2C%0Apreventing%20misclassification%20as%20background%20elements.%20Extensive%20experiments%20on%0Athe%20COCO%202017%20dataset%20demonstrate%20that%20SDDGR%20significantly%20outperforms%20existing%0Aalgorithms%2C%20achieving%20a%20new%20state-of-the-art%20in%20various%20CIOD%20scenarios.%20The%0Asource%20code%20will%20be%20made%20available%20to%20the%20public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17323v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSDDGR%253A%2520Stable%2520Diffusion-based%2520Deep%2520Generative%2520Replay%2520for%2520Class%250A%2520%2520Incremental%2520Object%2520Detection%26entry.906535625%3DJunsu%2520Kim%2520and%2520Hoseong%2520Cho%2520and%2520Jihyeon%2520Kim%2520and%2520Yihalem%2520Yimolal%2520Tiruneh%2520and%2520Seungryul%2520Baek%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520class%2520incremental%2520learning%2520%2528CIL%2529%252C%2520generative%2520replay%2520has%250Abecome%2520increasingly%2520prominent%2520as%2520a%2520method%2520to%2520mitigate%2520the%2520catastrophic%250Aforgetting%252C%2520alongside%2520the%2520continuous%2520improvements%2520in%2520generative%2520models.%250AHowever%252C%2520its%2520application%2520in%2520class%2520incremental%2520object%2520detection%2520%2528CIOD%2529%2520has%2520been%250Asignificantly%2520limited%252C%2520primarily%2520due%2520to%2520the%2520complexities%2520of%2520scenes%2520involving%250Amultiple%2520labels.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520called%2520stable%250Adiffusion%2520deep%2520generative%2520replay%2520%2528SDDGR%2529%2520for%2520CIOD.%2520Our%2520method%2520utilizes%2520a%250Adiffusion-based%2520generative%2520model%2520with%2520pre-trained%2520text-to-diffusion%2520networks%2520to%250Agenerate%2520realistic%2520and%2520diverse%2520synthetic%2520images.%2520SDDGR%2520incorporates%2520an%250Aiterative%2520refinement%2520strategy%2520to%2520produce%2520high-quality%2520images%2520encompassing%2520old%250Aclasses.%2520Additionally%252C%2520we%2520adopt%2520an%2520L2%2520knowledge%2520distillation%2520technique%2520to%250Aimprove%2520the%2520retention%2520of%2520prior%2520knowledge%2520in%2520synthetic%2520images.%2520Furthermore%252C%2520our%250Aapproach%2520includes%2520pseudo-labeling%2520for%2520old%2520objects%2520within%2520new%2520task%2520images%252C%250Apreventing%2520misclassification%2520as%2520background%2520elements.%2520Extensive%2520experiments%2520on%250Athe%2520COCO%25202017%2520dataset%2520demonstrate%2520that%2520SDDGR%2520significantly%2520outperforms%2520existing%250Aalgorithms%252C%2520achieving%2520a%2520new%2520state-of-the-art%2520in%2520various%2520CIOD%2520scenarios.%2520The%250Asource%2520code%2520will%2520be%2520made%2520available%2520to%2520the%2520public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17323v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SDDGR%3A%20Stable%20Diffusion-based%20Deep%20Generative%20Replay%20for%20Class%0A%20%20Incremental%20Object%20Detection&entry.906535625=Junsu%20Kim%20and%20Hoseong%20Cho%20and%20Jihyeon%20Kim%20and%20Yihalem%20Yimolal%20Tiruneh%20and%20Seungryul%20Baek&entry.1292438233=%20%20In%20the%20field%20of%20class%20incremental%20learning%20%28CIL%29%2C%20generative%20replay%20has%0Abecome%20increasingly%20prominent%20as%20a%20method%20to%20mitigate%20the%20catastrophic%0Aforgetting%2C%20alongside%20the%20continuous%20improvements%20in%20generative%20models.%0AHowever%2C%20its%20application%20in%20class%20incremental%20object%20detection%20%28CIOD%29%20has%20been%0Asignificantly%20limited%2C%20primarily%20due%20to%20the%20complexities%20of%20scenes%20involving%0Amultiple%20labels.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20called%20stable%0Adiffusion%20deep%20generative%20replay%20%28SDDGR%29%20for%20CIOD.%20Our%20method%20utilizes%20a%0Adiffusion-based%20generative%20model%20with%20pre-trained%20text-to-diffusion%20networks%20to%0Agenerate%20realistic%20and%20diverse%20synthetic%20images.%20SDDGR%20incorporates%20an%0Aiterative%20refinement%20strategy%20to%20produce%20high-quality%20images%20encompassing%20old%0Aclasses.%20Additionally%2C%20we%20adopt%20an%20L2%20knowledge%20distillation%20technique%20to%0Aimprove%20the%20retention%20of%20prior%20knowledge%20in%20synthetic%20images.%20Furthermore%2C%20our%0Aapproach%20includes%20pseudo-labeling%20for%20old%20objects%20within%20new%20task%20images%2C%0Apreventing%20misclassification%20as%20background%20elements.%20Extensive%20experiments%20on%0Athe%20COCO%202017%20dataset%20demonstrate%20that%20SDDGR%20significantly%20outperforms%20existing%0Aalgorithms%2C%20achieving%20a%20new%20state-of-the-art%20in%20various%20CIOD%20scenarios.%20The%0Asource%20code%20will%20be%20made%20available%20to%20the%20public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17323v2&entry.124074799=Read"},
{"title": "Bayesian Simultaneous Localization and Multi-Lane Tracking Using Onboard\n  Sensors and a SD Map", "author": "Yuxuan Xia and Erik Stenborg and Junsheng Fu and Gustaf Hendeby", "abstract": "  High-definition map with accurate lane-level information is crucial for\nautonomous driving, but the creation of these maps is a resource-intensive\nprocess. To this end, we present a cost-effective solution to create lane-level\nroadmaps using only the global navigation satellite system (GNSS) and a camera\non customer vehicles. Our proposed solution utilizes a prior\nstandard-definition (SD) map, GNSS measurements, visual odometry, and lane\nmarking edge detection points, to simultaneously estimate the vehicle's 6D\npose, its position within a SD map, and also the 3D geometry of traffic lines.\nThis is achieved using a Bayesian simultaneous localization and multi-object\ntracking filter, where the estimation of traffic lines is formulated as a\nmultiple extended object tracking problem, solved using a trajectory Poisson\nmulti-Bernoulli mixture (TPMBM) filter. In TPMBM filtering, traffic lines are\nmodeled using B-spline trajectories, and each trajectory is parameterized by a\nsequence of control points. The proposed solution has been evaluated using\nexperimental data collected by a test vehicle driving on highway. Preliminary\nresults show that the traffic line estimates, overlaid on the satellite image,\ngenerally align with the lane markings up to some lateral offsets.\n", "link": "http://arxiv.org/abs/2405.04290v1", "date": "2024-05-07", "relevancy": 2.3347, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6187}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5979}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Simultaneous%20Localization%20and%20Multi-Lane%20Tracking%20Using%20Onboard%0A%20%20Sensors%20and%20a%20SD%20Map&body=Title%3A%20Bayesian%20Simultaneous%20Localization%20and%20Multi-Lane%20Tracking%20Using%20Onboard%0A%20%20Sensors%20and%20a%20SD%20Map%0AAuthor%3A%20Yuxuan%20Xia%20and%20Erik%20Stenborg%20and%20Junsheng%20Fu%20and%20Gustaf%20Hendeby%0AAbstract%3A%20%20%20High-definition%20map%20with%20accurate%20lane-level%20information%20is%20crucial%20for%0Aautonomous%20driving%2C%20but%20the%20creation%20of%20these%20maps%20is%20a%20resource-intensive%0Aprocess.%20To%20this%20end%2C%20we%20present%20a%20cost-effective%20solution%20to%20create%20lane-level%0Aroadmaps%20using%20only%20the%20global%20navigation%20satellite%20system%20%28GNSS%29%20and%20a%20camera%0Aon%20customer%20vehicles.%20Our%20proposed%20solution%20utilizes%20a%20prior%0Astandard-definition%20%28SD%29%20map%2C%20GNSS%20measurements%2C%20visual%20odometry%2C%20and%20lane%0Amarking%20edge%20detection%20points%2C%20to%20simultaneously%20estimate%20the%20vehicle%27s%206D%0Apose%2C%20its%20position%20within%20a%20SD%20map%2C%20and%20also%20the%203D%20geometry%20of%20traffic%20lines.%0AThis%20is%20achieved%20using%20a%20Bayesian%20simultaneous%20localization%20and%20multi-object%0Atracking%20filter%2C%20where%20the%20estimation%20of%20traffic%20lines%20is%20formulated%20as%20a%0Amultiple%20extended%20object%20tracking%20problem%2C%20solved%20using%20a%20trajectory%20Poisson%0Amulti-Bernoulli%20mixture%20%28TPMBM%29%20filter.%20In%20TPMBM%20filtering%2C%20traffic%20lines%20are%0Amodeled%20using%20B-spline%20trajectories%2C%20and%20each%20trajectory%20is%20parameterized%20by%20a%0Asequence%20of%20control%20points.%20The%20proposed%20solution%20has%20been%20evaluated%20using%0Aexperimental%20data%20collected%20by%20a%20test%20vehicle%20driving%20on%20highway.%20Preliminary%0Aresults%20show%20that%20the%20traffic%20line%20estimates%2C%20overlaid%20on%20the%20satellite%20image%2C%0Agenerally%20align%20with%20the%20lane%20markings%20up%20to%20some%20lateral%20offsets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04290v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Simultaneous%2520Localization%2520and%2520Multi-Lane%2520Tracking%2520Using%2520Onboard%250A%2520%2520Sensors%2520and%2520a%2520SD%2520Map%26entry.906535625%3DYuxuan%2520Xia%2520and%2520Erik%2520Stenborg%2520and%2520Junsheng%2520Fu%2520and%2520Gustaf%2520Hendeby%26entry.1292438233%3D%2520%2520High-definition%2520map%2520with%2520accurate%2520lane-level%2520information%2520is%2520crucial%2520for%250Aautonomous%2520driving%252C%2520but%2520the%2520creation%2520of%2520these%2520maps%2520is%2520a%2520resource-intensive%250Aprocess.%2520To%2520this%2520end%252C%2520we%2520present%2520a%2520cost-effective%2520solution%2520to%2520create%2520lane-level%250Aroadmaps%2520using%2520only%2520the%2520global%2520navigation%2520satellite%2520system%2520%2528GNSS%2529%2520and%2520a%2520camera%250Aon%2520customer%2520vehicles.%2520Our%2520proposed%2520solution%2520utilizes%2520a%2520prior%250Astandard-definition%2520%2528SD%2529%2520map%252C%2520GNSS%2520measurements%252C%2520visual%2520odometry%252C%2520and%2520lane%250Amarking%2520edge%2520detection%2520points%252C%2520to%2520simultaneously%2520estimate%2520the%2520vehicle%2527s%25206D%250Apose%252C%2520its%2520position%2520within%2520a%2520SD%2520map%252C%2520and%2520also%2520the%25203D%2520geometry%2520of%2520traffic%2520lines.%250AThis%2520is%2520achieved%2520using%2520a%2520Bayesian%2520simultaneous%2520localization%2520and%2520multi-object%250Atracking%2520filter%252C%2520where%2520the%2520estimation%2520of%2520traffic%2520lines%2520is%2520formulated%2520as%2520a%250Amultiple%2520extended%2520object%2520tracking%2520problem%252C%2520solved%2520using%2520a%2520trajectory%2520Poisson%250Amulti-Bernoulli%2520mixture%2520%2528TPMBM%2529%2520filter.%2520In%2520TPMBM%2520filtering%252C%2520traffic%2520lines%2520are%250Amodeled%2520using%2520B-spline%2520trajectories%252C%2520and%2520each%2520trajectory%2520is%2520parameterized%2520by%2520a%250Asequence%2520of%2520control%2520points.%2520The%2520proposed%2520solution%2520has%2520been%2520evaluated%2520using%250Aexperimental%2520data%2520collected%2520by%2520a%2520test%2520vehicle%2520driving%2520on%2520highway.%2520Preliminary%250Aresults%2520show%2520that%2520the%2520traffic%2520line%2520estimates%252C%2520overlaid%2520on%2520the%2520satellite%2520image%252C%250Agenerally%2520align%2520with%2520the%2520lane%2520markings%2520up%2520to%2520some%2520lateral%2520offsets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04290v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Simultaneous%20Localization%20and%20Multi-Lane%20Tracking%20Using%20Onboard%0A%20%20Sensors%20and%20a%20SD%20Map&entry.906535625=Yuxuan%20Xia%20and%20Erik%20Stenborg%20and%20Junsheng%20Fu%20and%20Gustaf%20Hendeby&entry.1292438233=%20%20High-definition%20map%20with%20accurate%20lane-level%20information%20is%20crucial%20for%0Aautonomous%20driving%2C%20but%20the%20creation%20of%20these%20maps%20is%20a%20resource-intensive%0Aprocess.%20To%20this%20end%2C%20we%20present%20a%20cost-effective%20solution%20to%20create%20lane-level%0Aroadmaps%20using%20only%20the%20global%20navigation%20satellite%20system%20%28GNSS%29%20and%20a%20camera%0Aon%20customer%20vehicles.%20Our%20proposed%20solution%20utilizes%20a%20prior%0Astandard-definition%20%28SD%29%20map%2C%20GNSS%20measurements%2C%20visual%20odometry%2C%20and%20lane%0Amarking%20edge%20detection%20points%2C%20to%20simultaneously%20estimate%20the%20vehicle%27s%206D%0Apose%2C%20its%20position%20within%20a%20SD%20map%2C%20and%20also%20the%203D%20geometry%20of%20traffic%20lines.%0AThis%20is%20achieved%20using%20a%20Bayesian%20simultaneous%20localization%20and%20multi-object%0Atracking%20filter%2C%20where%20the%20estimation%20of%20traffic%20lines%20is%20formulated%20as%20a%0Amultiple%20extended%20object%20tracking%20problem%2C%20solved%20using%20a%20trajectory%20Poisson%0Amulti-Bernoulli%20mixture%20%28TPMBM%29%20filter.%20In%20TPMBM%20filtering%2C%20traffic%20lines%20are%0Amodeled%20using%20B-spline%20trajectories%2C%20and%20each%20trajectory%20is%20parameterized%20by%20a%0Asequence%20of%20control%20points.%20The%20proposed%20solution%20has%20been%20evaluated%20using%0Aexperimental%20data%20collected%20by%20a%20test%20vehicle%20driving%20on%20highway.%20Preliminary%0Aresults%20show%20that%20the%20traffic%20line%20estimates%2C%20overlaid%20on%20the%20satellite%20image%2C%0Agenerally%20align%20with%20the%20lane%20markings%20up%20to%20some%20lateral%20offsets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04290v1&entry.124074799=Read"},
{"title": "Scalable network reconstruction in subquadratic time", "author": "Tiago P. Peixoto", "abstract": "  Network reconstruction consists in determining the unobserved pairwise\ncouplings between $N$ nodes given only observational data on the resulting\nbehavior that is conditioned on those couplings -- typically a time-series or\nindependent samples from a graphical model. A major obstacle to the scalability\nof algorithms proposed for this problem is a seemingly unavoidable quadratic\ncomplexity of $\\Omega(N^2)$, corresponding to the requirement of each possible\npairwise coupling being contemplated at least once, despite the fact that most\nnetworks of interest are sparse, with a number of non-zero couplings that is\nonly $O(N)$. Here we present a general algorithm applicable to a broad range of\nreconstruction problems that significantly outperforms this quadratic baseline.\nOur algorithm relies on a stochastic second neighbor search (Dong et al., 2011)\nthat produces the best edge candidates with high probability, thus bypassing an\nexhaustive quadratic search. If we rely on the conjecture that the\nsecond-neighbor search finishes in log-linear time (Baron & Darling, 2020;\n2022), we demonstrate theoretically that our algorithm finishes in subquadratic\ntime, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\\log\nN)$, but with a more typical log-linear complexity of $O(N\\log^2N)$. In\npractice, we show that our algorithm achieves a performance that is many orders\nof magnitude faster than the quadratic baseline -- in a manner consistent with\nour theoretical analysis -- allows for easy parallelization, and thus enables\nthe reconstruction of networks with hundreds of thousands and even millions of\nnodes and edges.\n", "link": "http://arxiv.org/abs/2401.01404v5", "date": "2024-05-07", "relevancy": 2.3264, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4832}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4723}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20network%20reconstruction%20in%20subquadratic%20time&body=Title%3A%20Scalable%20network%20reconstruction%20in%20subquadratic%20time%0AAuthor%3A%20Tiago%20P.%20Peixoto%0AAbstract%3A%20%20%20Network%20reconstruction%20consists%20in%20determining%20the%20unobserved%20pairwise%0Acouplings%20between%20%24N%24%20nodes%20given%20only%20observational%20data%20on%20the%20resulting%0Abehavior%20that%20is%20conditioned%20on%20those%20couplings%20--%20typically%20a%20time-series%20or%0Aindependent%20samples%20from%20a%20graphical%20model.%20A%20major%20obstacle%20to%20the%20scalability%0Aof%20algorithms%20proposed%20for%20this%20problem%20is%20a%20seemingly%20unavoidable%20quadratic%0Acomplexity%20of%20%24%5COmega%28N%5E2%29%24%2C%20corresponding%20to%20the%20requirement%20of%20each%20possible%0Apairwise%20coupling%20being%20contemplated%20at%20least%20once%2C%20despite%20the%20fact%20that%20most%0Anetworks%20of%20interest%20are%20sparse%2C%20with%20a%20number%20of%20non-zero%20couplings%20that%20is%0Aonly%20%24O%28N%29%24.%20Here%20we%20present%20a%20general%20algorithm%20applicable%20to%20a%20broad%20range%20of%0Areconstruction%20problems%20that%20significantly%20outperforms%20this%20quadratic%20baseline.%0AOur%20algorithm%20relies%20on%20a%20stochastic%20second%20neighbor%20search%20%28Dong%20et%20al.%2C%202011%29%0Athat%20produces%20the%20best%20edge%20candidates%20with%20high%20probability%2C%20thus%20bypassing%20an%0Aexhaustive%20quadratic%20search.%20If%20we%20rely%20on%20the%20conjecture%20that%20the%0Asecond-neighbor%20search%20finishes%20in%20log-linear%20time%20%28Baron%20%26%20Darling%2C%202020%3B%0A2022%29%2C%20we%20demonstrate%20theoretically%20that%20our%20algorithm%20finishes%20in%20subquadratic%0Atime%2C%20with%20a%20data-dependent%20complexity%20loosely%20upper%20bounded%20by%20%24O%28N%5E%7B3/2%7D%5Clog%0AN%29%24%2C%20but%20with%20a%20more%20typical%20log-linear%20complexity%20of%20%24O%28N%5Clog%5E2N%29%24.%20In%0Apractice%2C%20we%20show%20that%20our%20algorithm%20achieves%20a%20performance%20that%20is%20many%20orders%0Aof%20magnitude%20faster%20than%20the%20quadratic%20baseline%20--%20in%20a%20manner%20consistent%20with%0Aour%20theoretical%20analysis%20--%20allows%20for%20easy%20parallelization%2C%20and%20thus%20enables%0Athe%20reconstruction%20of%20networks%20with%20hundreds%20of%20thousands%20and%20even%20millions%20of%0Anodes%20and%20edges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.01404v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520network%2520reconstruction%2520in%2520subquadratic%2520time%26entry.906535625%3DTiago%2520P.%2520Peixoto%26entry.1292438233%3D%2520%2520Network%2520reconstruction%2520consists%2520in%2520determining%2520the%2520unobserved%2520pairwise%250Acouplings%2520between%2520%2524N%2524%2520nodes%2520given%2520only%2520observational%2520data%2520on%2520the%2520resulting%250Abehavior%2520that%2520is%2520conditioned%2520on%2520those%2520couplings%2520--%2520typically%2520a%2520time-series%2520or%250Aindependent%2520samples%2520from%2520a%2520graphical%2520model.%2520A%2520major%2520obstacle%2520to%2520the%2520scalability%250Aof%2520algorithms%2520proposed%2520for%2520this%2520problem%2520is%2520a%2520seemingly%2520unavoidable%2520quadratic%250Acomplexity%2520of%2520%2524%255COmega%2528N%255E2%2529%2524%252C%2520corresponding%2520to%2520the%2520requirement%2520of%2520each%2520possible%250Apairwise%2520coupling%2520being%2520contemplated%2520at%2520least%2520once%252C%2520despite%2520the%2520fact%2520that%2520most%250Anetworks%2520of%2520interest%2520are%2520sparse%252C%2520with%2520a%2520number%2520of%2520non-zero%2520couplings%2520that%2520is%250Aonly%2520%2524O%2528N%2529%2524.%2520Here%2520we%2520present%2520a%2520general%2520algorithm%2520applicable%2520to%2520a%2520broad%2520range%2520of%250Areconstruction%2520problems%2520that%2520significantly%2520outperforms%2520this%2520quadratic%2520baseline.%250AOur%2520algorithm%2520relies%2520on%2520a%2520stochastic%2520second%2520neighbor%2520search%2520%2528Dong%2520et%2520al.%252C%25202011%2529%250Athat%2520produces%2520the%2520best%2520edge%2520candidates%2520with%2520high%2520probability%252C%2520thus%2520bypassing%2520an%250Aexhaustive%2520quadratic%2520search.%2520If%2520we%2520rely%2520on%2520the%2520conjecture%2520that%2520the%250Asecond-neighbor%2520search%2520finishes%2520in%2520log-linear%2520time%2520%2528Baron%2520%2526%2520Darling%252C%25202020%253B%250A2022%2529%252C%2520we%2520demonstrate%2520theoretically%2520that%2520our%2520algorithm%2520finishes%2520in%2520subquadratic%250Atime%252C%2520with%2520a%2520data-dependent%2520complexity%2520loosely%2520upper%2520bounded%2520by%2520%2524O%2528N%255E%257B3/2%257D%255Clog%250AN%2529%2524%252C%2520but%2520with%2520a%2520more%2520typical%2520log-linear%2520complexity%2520of%2520%2524O%2528N%255Clog%255E2N%2529%2524.%2520In%250Apractice%252C%2520we%2520show%2520that%2520our%2520algorithm%2520achieves%2520a%2520performance%2520that%2520is%2520many%2520orders%250Aof%2520magnitude%2520faster%2520than%2520the%2520quadratic%2520baseline%2520--%2520in%2520a%2520manner%2520consistent%2520with%250Aour%2520theoretical%2520analysis%2520--%2520allows%2520for%2520easy%2520parallelization%252C%2520and%2520thus%2520enables%250Athe%2520reconstruction%2520of%2520networks%2520with%2520hundreds%2520of%2520thousands%2520and%2520even%2520millions%2520of%250Anodes%2520and%2520edges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.01404v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20network%20reconstruction%20in%20subquadratic%20time&entry.906535625=Tiago%20P.%20Peixoto&entry.1292438233=%20%20Network%20reconstruction%20consists%20in%20determining%20the%20unobserved%20pairwise%0Acouplings%20between%20%24N%24%20nodes%20given%20only%20observational%20data%20on%20the%20resulting%0Abehavior%20that%20is%20conditioned%20on%20those%20couplings%20--%20typically%20a%20time-series%20or%0Aindependent%20samples%20from%20a%20graphical%20model.%20A%20major%20obstacle%20to%20the%20scalability%0Aof%20algorithms%20proposed%20for%20this%20problem%20is%20a%20seemingly%20unavoidable%20quadratic%0Acomplexity%20of%20%24%5COmega%28N%5E2%29%24%2C%20corresponding%20to%20the%20requirement%20of%20each%20possible%0Apairwise%20coupling%20being%20contemplated%20at%20least%20once%2C%20despite%20the%20fact%20that%20most%0Anetworks%20of%20interest%20are%20sparse%2C%20with%20a%20number%20of%20non-zero%20couplings%20that%20is%0Aonly%20%24O%28N%29%24.%20Here%20we%20present%20a%20general%20algorithm%20applicable%20to%20a%20broad%20range%20of%0Areconstruction%20problems%20that%20significantly%20outperforms%20this%20quadratic%20baseline.%0AOur%20algorithm%20relies%20on%20a%20stochastic%20second%20neighbor%20search%20%28Dong%20et%20al.%2C%202011%29%0Athat%20produces%20the%20best%20edge%20candidates%20with%20high%20probability%2C%20thus%20bypassing%20an%0Aexhaustive%20quadratic%20search.%20If%20we%20rely%20on%20the%20conjecture%20that%20the%0Asecond-neighbor%20search%20finishes%20in%20log-linear%20time%20%28Baron%20%26%20Darling%2C%202020%3B%0A2022%29%2C%20we%20demonstrate%20theoretically%20that%20our%20algorithm%20finishes%20in%20subquadratic%0Atime%2C%20with%20a%20data-dependent%20complexity%20loosely%20upper%20bounded%20by%20%24O%28N%5E%7B3/2%7D%5Clog%0AN%29%24%2C%20but%20with%20a%20more%20typical%20log-linear%20complexity%20of%20%24O%28N%5Clog%5E2N%29%24.%20In%0Apractice%2C%20we%20show%20that%20our%20algorithm%20achieves%20a%20performance%20that%20is%20many%20orders%0Aof%20magnitude%20faster%20than%20the%20quadratic%20baseline%20--%20in%20a%20manner%20consistent%20with%0Aour%20theoretical%20analysis%20--%20allows%20for%20easy%20parallelization%2C%20and%20thus%20enables%0Athe%20reconstruction%20of%20networks%20with%20hundreds%20of%20thousands%20and%20even%20millions%20of%0Anodes%20and%20edges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01404v5&entry.124074799=Read"},
{"title": "POV Learning: Individual Alignment of Multimodal Models using Human\n  Perception", "author": "Simon Werner and Katharina Christ and Laura Bernardy and Marion G. M\u00fcller and Achim Rettinger", "abstract": "  Aligning machine learning systems with human expectations is mostly attempted\nby training with manually vetted human behavioral samples, typically explicit\nfeedback. This is done on a population level since the context that is\ncapturing the subjective Point-Of-View (POV) of a concrete person in a specific\nsituational context is not retained in the data. However, we argue that\nalignment on an individual level can boost the subjective predictive\nperformance for the individual user interacting with the system considerably.\nSince perception differs for each person, the same situation is observed\ndifferently. Consequently, the basis for decision making and the subsequent\nreasoning processes and observable reactions differ. We hypothesize that\nindividual perception patterns can be used for improving the alignment on an\nindividual level. We test this, by integrating perception information into\nmachine learning systems and measuring their predictive performance\nwrt.~individual subjective assessments. For our empirical study, we collect a\nnovel data set of multimodal stimuli and corresponding eye tracking sequences\nfor the novel task of Perception-Guided Crossmodal Entailment and tackle it\nwith our Perception-Guided Multimodal Transformer. Our findings suggest that\nexploiting individual perception signals for the machine learning of subjective\nhuman assessments provides a valuable cue for individual alignment. It does not\nonly improve the overall predictive performance from the point-of-view of the\nindividual user but might also contribute to steering AI systems towards every\nperson's individual expectations and values.\n", "link": "http://arxiv.org/abs/2405.04443v1", "date": "2024-05-07", "relevancy": 2.3258, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6105}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5834}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20POV%20Learning%3A%20Individual%20Alignment%20of%20Multimodal%20Models%20using%20Human%0A%20%20Perception&body=Title%3A%20POV%20Learning%3A%20Individual%20Alignment%20of%20Multimodal%20Models%20using%20Human%0A%20%20Perception%0AAuthor%3A%20Simon%20Werner%20and%20Katharina%20Christ%20and%20Laura%20Bernardy%20and%20Marion%20G.%20M%C3%BCller%20and%20Achim%20Rettinger%0AAbstract%3A%20%20%20Aligning%20machine%20learning%20systems%20with%20human%20expectations%20is%20mostly%20attempted%0Aby%20training%20with%20manually%20vetted%20human%20behavioral%20samples%2C%20typically%20explicit%0Afeedback.%20This%20is%20done%20on%20a%20population%20level%20since%20the%20context%20that%20is%0Acapturing%20the%20subjective%20Point-Of-View%20%28POV%29%20of%20a%20concrete%20person%20in%20a%20specific%0Asituational%20context%20is%20not%20retained%20in%20the%20data.%20However%2C%20we%20argue%20that%0Aalignment%20on%20an%20individual%20level%20can%20boost%20the%20subjective%20predictive%0Aperformance%20for%20the%20individual%20user%20interacting%20with%20the%20system%20considerably.%0ASince%20perception%20differs%20for%20each%20person%2C%20the%20same%20situation%20is%20observed%0Adifferently.%20Consequently%2C%20the%20basis%20for%20decision%20making%20and%20the%20subsequent%0Areasoning%20processes%20and%20observable%20reactions%20differ.%20We%20hypothesize%20that%0Aindividual%20perception%20patterns%20can%20be%20used%20for%20improving%20the%20alignment%20on%20an%0Aindividual%20level.%20We%20test%20this%2C%20by%20integrating%20perception%20information%20into%0Amachine%20learning%20systems%20and%20measuring%20their%20predictive%20performance%0Awrt.~individual%20subjective%20assessments.%20For%20our%20empirical%20study%2C%20we%20collect%20a%0Anovel%20data%20set%20of%20multimodal%20stimuli%20and%20corresponding%20eye%20tracking%20sequences%0Afor%20the%20novel%20task%20of%20Perception-Guided%20Crossmodal%20Entailment%20and%20tackle%20it%0Awith%20our%20Perception-Guided%20Multimodal%20Transformer.%20Our%20findings%20suggest%20that%0Aexploiting%20individual%20perception%20signals%20for%20the%20machine%20learning%20of%20subjective%0Ahuman%20assessments%20provides%20a%20valuable%20cue%20for%20individual%20alignment.%20It%20does%20not%0Aonly%20improve%20the%20overall%20predictive%20performance%20from%20the%20point-of-view%20of%20the%0Aindividual%20user%20but%20might%20also%20contribute%20to%20steering%20AI%20systems%20towards%20every%0Aperson%27s%20individual%20expectations%20and%20values.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPOV%2520Learning%253A%2520Individual%2520Alignment%2520of%2520Multimodal%2520Models%2520using%2520Human%250A%2520%2520Perception%26entry.906535625%3DSimon%2520Werner%2520and%2520Katharina%2520Christ%2520and%2520Laura%2520Bernardy%2520and%2520Marion%2520G.%2520M%25C3%25BCller%2520and%2520Achim%2520Rettinger%26entry.1292438233%3D%2520%2520Aligning%2520machine%2520learning%2520systems%2520with%2520human%2520expectations%2520is%2520mostly%2520attempted%250Aby%2520training%2520with%2520manually%2520vetted%2520human%2520behavioral%2520samples%252C%2520typically%2520explicit%250Afeedback.%2520This%2520is%2520done%2520on%2520a%2520population%2520level%2520since%2520the%2520context%2520that%2520is%250Acapturing%2520the%2520subjective%2520Point-Of-View%2520%2528POV%2529%2520of%2520a%2520concrete%2520person%2520in%2520a%2520specific%250Asituational%2520context%2520is%2520not%2520retained%2520in%2520the%2520data.%2520However%252C%2520we%2520argue%2520that%250Aalignment%2520on%2520an%2520individual%2520level%2520can%2520boost%2520the%2520subjective%2520predictive%250Aperformance%2520for%2520the%2520individual%2520user%2520interacting%2520with%2520the%2520system%2520considerably.%250ASince%2520perception%2520differs%2520for%2520each%2520person%252C%2520the%2520same%2520situation%2520is%2520observed%250Adifferently.%2520Consequently%252C%2520the%2520basis%2520for%2520decision%2520making%2520and%2520the%2520subsequent%250Areasoning%2520processes%2520and%2520observable%2520reactions%2520differ.%2520We%2520hypothesize%2520that%250Aindividual%2520perception%2520patterns%2520can%2520be%2520used%2520for%2520improving%2520the%2520alignment%2520on%2520an%250Aindividual%2520level.%2520We%2520test%2520this%252C%2520by%2520integrating%2520perception%2520information%2520into%250Amachine%2520learning%2520systems%2520and%2520measuring%2520their%2520predictive%2520performance%250Awrt.~individual%2520subjective%2520assessments.%2520For%2520our%2520empirical%2520study%252C%2520we%2520collect%2520a%250Anovel%2520data%2520set%2520of%2520multimodal%2520stimuli%2520and%2520corresponding%2520eye%2520tracking%2520sequences%250Afor%2520the%2520novel%2520task%2520of%2520Perception-Guided%2520Crossmodal%2520Entailment%2520and%2520tackle%2520it%250Awith%2520our%2520Perception-Guided%2520Multimodal%2520Transformer.%2520Our%2520findings%2520suggest%2520that%250Aexploiting%2520individual%2520perception%2520signals%2520for%2520the%2520machine%2520learning%2520of%2520subjective%250Ahuman%2520assessments%2520provides%2520a%2520valuable%2520cue%2520for%2520individual%2520alignment.%2520It%2520does%2520not%250Aonly%2520improve%2520the%2520overall%2520predictive%2520performance%2520from%2520the%2520point-of-view%2520of%2520the%250Aindividual%2520user%2520but%2520might%2520also%2520contribute%2520to%2520steering%2520AI%2520systems%2520towards%2520every%250Aperson%2527s%2520individual%2520expectations%2520and%2520values.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=POV%20Learning%3A%20Individual%20Alignment%20of%20Multimodal%20Models%20using%20Human%0A%20%20Perception&entry.906535625=Simon%20Werner%20and%20Katharina%20Christ%20and%20Laura%20Bernardy%20and%20Marion%20G.%20M%C3%BCller%20and%20Achim%20Rettinger&entry.1292438233=%20%20Aligning%20machine%20learning%20systems%20with%20human%20expectations%20is%20mostly%20attempted%0Aby%20training%20with%20manually%20vetted%20human%20behavioral%20samples%2C%20typically%20explicit%0Afeedback.%20This%20is%20done%20on%20a%20population%20level%20since%20the%20context%20that%20is%0Acapturing%20the%20subjective%20Point-Of-View%20%28POV%29%20of%20a%20concrete%20person%20in%20a%20specific%0Asituational%20context%20is%20not%20retained%20in%20the%20data.%20However%2C%20we%20argue%20that%0Aalignment%20on%20an%20individual%20level%20can%20boost%20the%20subjective%20predictive%0Aperformance%20for%20the%20individual%20user%20interacting%20with%20the%20system%20considerably.%0ASince%20perception%20differs%20for%20each%20person%2C%20the%20same%20situation%20is%20observed%0Adifferently.%20Consequently%2C%20the%20basis%20for%20decision%20making%20and%20the%20subsequent%0Areasoning%20processes%20and%20observable%20reactions%20differ.%20We%20hypothesize%20that%0Aindividual%20perception%20patterns%20can%20be%20used%20for%20improving%20the%20alignment%20on%20an%0Aindividual%20level.%20We%20test%20this%2C%20by%20integrating%20perception%20information%20into%0Amachine%20learning%20systems%20and%20measuring%20their%20predictive%20performance%0Awrt.~individual%20subjective%20assessments.%20For%20our%20empirical%20study%2C%20we%20collect%20a%0Anovel%20data%20set%20of%20multimodal%20stimuli%20and%20corresponding%20eye%20tracking%20sequences%0Afor%20the%20novel%20task%20of%20Perception-Guided%20Crossmodal%20Entailment%20and%20tackle%20it%0Awith%20our%20Perception-Guided%20Multimodal%20Transformer.%20Our%20findings%20suggest%20that%0Aexploiting%20individual%20perception%20signals%20for%20the%20machine%20learning%20of%20subjective%0Ahuman%20assessments%20provides%20a%20valuable%20cue%20for%20individual%20alignment.%20It%20does%20not%0Aonly%20improve%20the%20overall%20predictive%20performance%20from%20the%20point-of-view%20of%20the%0Aindividual%20user%20but%20might%20also%20contribute%20to%20steering%20AI%20systems%20towards%20every%0Aperson%27s%20individual%20expectations%20and%20values.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04443v1&entry.124074799=Read"},
{"title": "Revisiting character-level adversarial attacks", "author": "Elias Abad Rocamora and Yongtao Wu and Fanghui Liu and Grigorios G. Chrysos and Volkan Cevher", "abstract": "  Adversarial attacks in Natural Language Processing apply perturbations in the\ncharacter or token levels. Token-level attacks, gaining prominence for their\nuse of gradient-based methods, are susceptible to altering sentence semantics,\nleading to invalid adversarial examples. While character-level attacks easily\nmaintain semantics, they have received less attention as they cannot easily\nadopt popular gradient-based methods, and are thought to be easy to defend.\nChallenging these beliefs, we introduce Charmer, an efficient query-based\nadversarial attack capable of achieving high attack success rate (ASR) while\ngenerating highly similar adversarial examples. Our method successfully targets\nboth small (BERT) and large (Llama 2) models. Specifically, on BERT with SST-2,\nCharmer improves the ASR in 4.84% points and the USE similarity in 8% points\nwith respect to the previous art. Our implementation is available in\nhttps://github.com/LIONS-EPFL/Charmer.\n", "link": "http://arxiv.org/abs/2405.04346v1", "date": "2024-05-07", "relevancy": 2.3249, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5107}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4533}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20character-level%20adversarial%20attacks&body=Title%3A%20Revisiting%20character-level%20adversarial%20attacks%0AAuthor%3A%20Elias%20Abad%20Rocamora%20and%20Yongtao%20Wu%20and%20Fanghui%20Liu%20and%20Grigorios%20G.%20Chrysos%20and%20Volkan%20Cevher%0AAbstract%3A%20%20%20Adversarial%20attacks%20in%20Natural%20Language%20Processing%20apply%20perturbations%20in%20the%0Acharacter%20or%20token%20levels.%20Token-level%20attacks%2C%20gaining%20prominence%20for%20their%0Ause%20of%20gradient-based%20methods%2C%20are%20susceptible%20to%20altering%20sentence%20semantics%2C%0Aleading%20to%20invalid%20adversarial%20examples.%20While%20character-level%20attacks%20easily%0Amaintain%20semantics%2C%20they%20have%20received%20less%20attention%20as%20they%20cannot%20easily%0Aadopt%20popular%20gradient-based%20methods%2C%20and%20are%20thought%20to%20be%20easy%20to%20defend.%0AChallenging%20these%20beliefs%2C%20we%20introduce%20Charmer%2C%20an%20efficient%20query-based%0Aadversarial%20attack%20capable%20of%20achieving%20high%20attack%20success%20rate%20%28ASR%29%20while%0Agenerating%20highly%20similar%20adversarial%20examples.%20Our%20method%20successfully%20targets%0Aboth%20small%20%28BERT%29%20and%20large%20%28Llama%202%29%20models.%20Specifically%2C%20on%20BERT%20with%20SST-2%2C%0ACharmer%20improves%20the%20ASR%20in%204.84%25%20points%20and%20the%20USE%20similarity%20in%208%25%20points%0Awith%20respect%20to%20the%20previous%20art.%20Our%20implementation%20is%20available%20in%0Ahttps%3A//github.com/LIONS-EPFL/Charmer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04346v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520character-level%2520adversarial%2520attacks%26entry.906535625%3DElias%2520Abad%2520Rocamora%2520and%2520Yongtao%2520Wu%2520and%2520Fanghui%2520Liu%2520and%2520Grigorios%2520G.%2520Chrysos%2520and%2520Volkan%2520Cevher%26entry.1292438233%3D%2520%2520Adversarial%2520attacks%2520in%2520Natural%2520Language%2520Processing%2520apply%2520perturbations%2520in%2520the%250Acharacter%2520or%2520token%2520levels.%2520Token-level%2520attacks%252C%2520gaining%2520prominence%2520for%2520their%250Ause%2520of%2520gradient-based%2520methods%252C%2520are%2520susceptible%2520to%2520altering%2520sentence%2520semantics%252C%250Aleading%2520to%2520invalid%2520adversarial%2520examples.%2520While%2520character-level%2520attacks%2520easily%250Amaintain%2520semantics%252C%2520they%2520have%2520received%2520less%2520attention%2520as%2520they%2520cannot%2520easily%250Aadopt%2520popular%2520gradient-based%2520methods%252C%2520and%2520are%2520thought%2520to%2520be%2520easy%2520to%2520defend.%250AChallenging%2520these%2520beliefs%252C%2520we%2520introduce%2520Charmer%252C%2520an%2520efficient%2520query-based%250Aadversarial%2520attack%2520capable%2520of%2520achieving%2520high%2520attack%2520success%2520rate%2520%2528ASR%2529%2520while%250Agenerating%2520highly%2520similar%2520adversarial%2520examples.%2520Our%2520method%2520successfully%2520targets%250Aboth%2520small%2520%2528BERT%2529%2520and%2520large%2520%2528Llama%25202%2529%2520models.%2520Specifically%252C%2520on%2520BERT%2520with%2520SST-2%252C%250ACharmer%2520improves%2520the%2520ASR%2520in%25204.84%2525%2520points%2520and%2520the%2520USE%2520similarity%2520in%25208%2525%2520points%250Awith%2520respect%2520to%2520the%2520previous%2520art.%2520Our%2520implementation%2520is%2520available%2520in%250Ahttps%253A//github.com/LIONS-EPFL/Charmer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04346v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20character-level%20adversarial%20attacks&entry.906535625=Elias%20Abad%20Rocamora%20and%20Yongtao%20Wu%20and%20Fanghui%20Liu%20and%20Grigorios%20G.%20Chrysos%20and%20Volkan%20Cevher&entry.1292438233=%20%20Adversarial%20attacks%20in%20Natural%20Language%20Processing%20apply%20perturbations%20in%20the%0Acharacter%20or%20token%20levels.%20Token-level%20attacks%2C%20gaining%20prominence%20for%20their%0Ause%20of%20gradient-based%20methods%2C%20are%20susceptible%20to%20altering%20sentence%20semantics%2C%0Aleading%20to%20invalid%20adversarial%20examples.%20While%20character-level%20attacks%20easily%0Amaintain%20semantics%2C%20they%20have%20received%20less%20attention%20as%20they%20cannot%20easily%0Aadopt%20popular%20gradient-based%20methods%2C%20and%20are%20thought%20to%20be%20easy%20to%20defend.%0AChallenging%20these%20beliefs%2C%20we%20introduce%20Charmer%2C%20an%20efficient%20query-based%0Aadversarial%20attack%20capable%20of%20achieving%20high%20attack%20success%20rate%20%28ASR%29%20while%0Agenerating%20highly%20similar%20adversarial%20examples.%20Our%20method%20successfully%20targets%0Aboth%20small%20%28BERT%29%20and%20large%20%28Llama%202%29%20models.%20Specifically%2C%20on%20BERT%20with%20SST-2%2C%0ACharmer%20improves%20the%20ASR%20in%204.84%25%20points%20and%20the%20USE%20similarity%20in%208%25%20points%0Awith%20respect%20to%20the%20previous%20art.%20Our%20implementation%20is%20available%20in%0Ahttps%3A//github.com/LIONS-EPFL/Charmer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04346v1&entry.124074799=Read"},
{"title": "TwinDiffusion: Enhancing Coherence and Efficiency in Panoramic Image\n  Generation with Diffusion Models", "author": "Teng Zhou and Yongchuan Tang", "abstract": "  Diffusion models have emerged as effective tools for generating diverse and\nhigh-quality content. However, their capability in high-resolution image\ngeneration, particularly for panoramic images, still faces challenges such as\nvisible seams and incoherent transitions. In this paper, we propose\nTwinDiffusion, an optimized framework designed to address these challenges\nthrough two key innovations: Crop Fusion for quality enhancement and Cross\nSampling for efficiency optimization. We introduce a training-free optimizing\nstage to refine the similarity of the adjacent image areas, as well as an\ninterleaving sampling strategy to yield dynamic patches during the cropping\nprocess. A comprehensive evaluation is conducted to compare TwinDiffusion with\nthe existing methods, considering factors including coherence, fidelity,\ncompatibility, and efficiency. The results demonstrate the superior performance\nof our approach in generating seamless and coherent panoramas, setting a new\nstandard in quality and efficiency for panoramic image generation.\n", "link": "http://arxiv.org/abs/2404.19475v2", "date": "2024-05-07", "relevancy": 2.317, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6192}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5742}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TwinDiffusion%3A%20Enhancing%20Coherence%20and%20Efficiency%20in%20Panoramic%20Image%0A%20%20Generation%20with%20Diffusion%20Models&body=Title%3A%20TwinDiffusion%3A%20Enhancing%20Coherence%20and%20Efficiency%20in%20Panoramic%20Image%0A%20%20Generation%20with%20Diffusion%20Models%0AAuthor%3A%20Teng%20Zhou%20and%20Yongchuan%20Tang%0AAbstract%3A%20%20%20Diffusion%20models%20have%20emerged%20as%20effective%20tools%20for%20generating%20diverse%20and%0Ahigh-quality%20content.%20However%2C%20their%20capability%20in%20high-resolution%20image%0Ageneration%2C%20particularly%20for%20panoramic%20images%2C%20still%20faces%20challenges%20such%20as%0Avisible%20seams%20and%20incoherent%20transitions.%20In%20this%20paper%2C%20we%20propose%0ATwinDiffusion%2C%20an%20optimized%20framework%20designed%20to%20address%20these%20challenges%0Athrough%20two%20key%20innovations%3A%20Crop%20Fusion%20for%20quality%20enhancement%20and%20Cross%0ASampling%20for%20efficiency%20optimization.%20We%20introduce%20a%20training-free%20optimizing%0Astage%20to%20refine%20the%20similarity%20of%20the%20adjacent%20image%20areas%2C%20as%20well%20as%20an%0Ainterleaving%20sampling%20strategy%20to%20yield%20dynamic%20patches%20during%20the%20cropping%0Aprocess.%20A%20comprehensive%20evaluation%20is%20conducted%20to%20compare%20TwinDiffusion%20with%0Athe%20existing%20methods%2C%20considering%20factors%20including%20coherence%2C%20fidelity%2C%0Acompatibility%2C%20and%20efficiency.%20The%20results%20demonstrate%20the%20superior%20performance%0Aof%20our%20approach%20in%20generating%20seamless%20and%20coherent%20panoramas%2C%20setting%20a%20new%0Astandard%20in%20quality%20and%20efficiency%20for%20panoramic%20image%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19475v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwinDiffusion%253A%2520Enhancing%2520Coherence%2520and%2520Efficiency%2520in%2520Panoramic%2520Image%250A%2520%2520Generation%2520with%2520Diffusion%2520Models%26entry.906535625%3DTeng%2520Zhou%2520and%2520Yongchuan%2520Tang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520emerged%2520as%2520effective%2520tools%2520for%2520generating%2520diverse%2520and%250Ahigh-quality%2520content.%2520However%252C%2520their%2520capability%2520in%2520high-resolution%2520image%250Ageneration%252C%2520particularly%2520for%2520panoramic%2520images%252C%2520still%2520faces%2520challenges%2520such%2520as%250Avisible%2520seams%2520and%2520incoherent%2520transitions.%2520In%2520this%2520paper%252C%2520we%2520propose%250ATwinDiffusion%252C%2520an%2520optimized%2520framework%2520designed%2520to%2520address%2520these%2520challenges%250Athrough%2520two%2520key%2520innovations%253A%2520Crop%2520Fusion%2520for%2520quality%2520enhancement%2520and%2520Cross%250ASampling%2520for%2520efficiency%2520optimization.%2520We%2520introduce%2520a%2520training-free%2520optimizing%250Astage%2520to%2520refine%2520the%2520similarity%2520of%2520the%2520adjacent%2520image%2520areas%252C%2520as%2520well%2520as%2520an%250Ainterleaving%2520sampling%2520strategy%2520to%2520yield%2520dynamic%2520patches%2520during%2520the%2520cropping%250Aprocess.%2520A%2520comprehensive%2520evaluation%2520is%2520conducted%2520to%2520compare%2520TwinDiffusion%2520with%250Athe%2520existing%2520methods%252C%2520considering%2520factors%2520including%2520coherence%252C%2520fidelity%252C%250Acompatibility%252C%2520and%2520efficiency.%2520The%2520results%2520demonstrate%2520the%2520superior%2520performance%250Aof%2520our%2520approach%2520in%2520generating%2520seamless%2520and%2520coherent%2520panoramas%252C%2520setting%2520a%2520new%250Astandard%2520in%2520quality%2520and%2520efficiency%2520for%2520panoramic%2520image%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19475v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TwinDiffusion%3A%20Enhancing%20Coherence%20and%20Efficiency%20in%20Panoramic%20Image%0A%20%20Generation%20with%20Diffusion%20Models&entry.906535625=Teng%20Zhou%20and%20Yongchuan%20Tang&entry.1292438233=%20%20Diffusion%20models%20have%20emerged%20as%20effective%20tools%20for%20generating%20diverse%20and%0Ahigh-quality%20content.%20However%2C%20their%20capability%20in%20high-resolution%20image%0Ageneration%2C%20particularly%20for%20panoramic%20images%2C%20still%20faces%20challenges%20such%20as%0Avisible%20seams%20and%20incoherent%20transitions.%20In%20this%20paper%2C%20we%20propose%0ATwinDiffusion%2C%20an%20optimized%20framework%20designed%20to%20address%20these%20challenges%0Athrough%20two%20key%20innovations%3A%20Crop%20Fusion%20for%20quality%20enhancement%20and%20Cross%0ASampling%20for%20efficiency%20optimization.%20We%20introduce%20a%20training-free%20optimizing%0Astage%20to%20refine%20the%20similarity%20of%20the%20adjacent%20image%20areas%2C%20as%20well%20as%20an%0Ainterleaving%20sampling%20strategy%20to%20yield%20dynamic%20patches%20during%20the%20cropping%0Aprocess.%20A%20comprehensive%20evaluation%20is%20conducted%20to%20compare%20TwinDiffusion%20with%0Athe%20existing%20methods%2C%20considering%20factors%20including%20coherence%2C%20fidelity%2C%0Acompatibility%2C%20and%20efficiency.%20The%20results%20demonstrate%20the%20superior%20performance%0Aof%20our%20approach%20in%20generating%20seamless%20and%20coherent%20panoramas%2C%20setting%20a%20new%0Astandard%20in%20quality%20and%20efficiency%20for%20panoramic%20image%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19475v2&entry.124074799=Read"},
{"title": "Artificial Intelligence-powered fossil shark tooth identification:\n  Unleashing the potential of Convolutional Neural Networks", "author": "Andrea Barucci and Giulia Ciacci and Pietro Li\u00f2 and Tiago Azevedo and Andrea Di Cencio and Marco Merella and Giovanni Bianucci and Giulia Bosio and Simone Casati and Alberto Collareta", "abstract": "  All fields of knowledge are being impacted by Artificial Intelligence. In\nparticular, the Deep Learning paradigm enables the development of data analysis\ntools that support subject matter experts in a variety of sectors, from physics\nup to the recognition of ancient languages. Palaeontology is now observing this\ntrend as well. This study explores the capability of Convolutional Neural\nNetworks (CNNs), a particular class of Deep Learning algorithms specifically\ncrafted for computer vision tasks, to classify images of isolated fossil shark\nteeth gathered from online datasets as well as from the authors$'$ experience\non Peruvian Miocene and Italian Pliocene fossil assemblages. The shark taxa\nthat are included in the final, composite dataset (which consists of more than\none thousand images) are representative of both extinct and extant genera,\nnamely, Carcharhinus, Carcharias, Carcharocles, Chlamydoselachus,\nCosmopolitodus, Galeocerdo, Hemipristis, Notorynchus, Prionace and Squatina. We\ndeveloped a CNN, named SharkNet-X, specifically tailored on our recognition\ntask, reaching a 5-fold cross validated mean accuracy of 0.85 to identify\nimages containing a single shark tooth. Furthermore, we elaborated a\nvisualization of the features extracted from images using the last dense layer\nof the CNN, achieved through the application of the clustering technique t-SNE.\nIn addition, in order to understand and explain the behaviour of the CNN while\ngiving a paleontological point of view on the results, we introduced the\nexplainability method SHAP. To the best of our knowledge, this is the first\ninstance in which this method is applied to the field of palaeontology. The\nmain goal of this work is to showcase how Deep Learning techniques can aid in\nidentifying isolated fossil shark teeth, paving the way for developing new\ninformation tools for automating the recognition and classification of fossils.\n", "link": "http://arxiv.org/abs/2405.04189v1", "date": "2024-05-07", "relevancy": 2.3123, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4743}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4644}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artificial%20Intelligence-powered%20fossil%20shark%20tooth%20identification%3A%0A%20%20Unleashing%20the%20potential%20of%20Convolutional%20Neural%20Networks&body=Title%3A%20Artificial%20Intelligence-powered%20fossil%20shark%20tooth%20identification%3A%0A%20%20Unleashing%20the%20potential%20of%20Convolutional%20Neural%20Networks%0AAuthor%3A%20Andrea%20Barucci%20and%20Giulia%20Ciacci%20and%20Pietro%20Li%C3%B2%20and%20Tiago%20Azevedo%20and%20Andrea%20Di%20Cencio%20and%20Marco%20Merella%20and%20Giovanni%20Bianucci%20and%20Giulia%20Bosio%20and%20Simone%20Casati%20and%20Alberto%20Collareta%0AAbstract%3A%20%20%20All%20fields%20of%20knowledge%20are%20being%20impacted%20by%20Artificial%20Intelligence.%20In%0Aparticular%2C%20the%20Deep%20Learning%20paradigm%20enables%20the%20development%20of%20data%20analysis%0Atools%20that%20support%20subject%20matter%20experts%20in%20a%20variety%20of%20sectors%2C%20from%20physics%0Aup%20to%20the%20recognition%20of%20ancient%20languages.%20Palaeontology%20is%20now%20observing%20this%0Atrend%20as%20well.%20This%20study%20explores%20the%20capability%20of%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%2C%20a%20particular%20class%20of%20Deep%20Learning%20algorithms%20specifically%0Acrafted%20for%20computer%20vision%20tasks%2C%20to%20classify%20images%20of%20isolated%20fossil%20shark%0Ateeth%20gathered%20from%20online%20datasets%20as%20well%20as%20from%20the%20authors%24%27%24%20experience%0Aon%20Peruvian%20Miocene%20and%20Italian%20Pliocene%20fossil%20assemblages.%20The%20shark%20taxa%0Athat%20are%20included%20in%20the%20final%2C%20composite%20dataset%20%28which%20consists%20of%20more%20than%0Aone%20thousand%20images%29%20are%20representative%20of%20both%20extinct%20and%20extant%20genera%2C%0Anamely%2C%20Carcharhinus%2C%20Carcharias%2C%20Carcharocles%2C%20Chlamydoselachus%2C%0ACosmopolitodus%2C%20Galeocerdo%2C%20Hemipristis%2C%20Notorynchus%2C%20Prionace%20and%20Squatina.%20We%0Adeveloped%20a%20CNN%2C%20named%20SharkNet-X%2C%20specifically%20tailored%20on%20our%20recognition%0Atask%2C%20reaching%20a%205-fold%20cross%20validated%20mean%20accuracy%20of%200.85%20to%20identify%0Aimages%20containing%20a%20single%20shark%20tooth.%20Furthermore%2C%20we%20elaborated%20a%0Avisualization%20of%20the%20features%20extracted%20from%20images%20using%20the%20last%20dense%20layer%0Aof%20the%20CNN%2C%20achieved%20through%20the%20application%20of%20the%20clustering%20technique%20t-SNE.%0AIn%20addition%2C%20in%20order%20to%20understand%20and%20explain%20the%20behaviour%20of%20the%20CNN%20while%0Agiving%20a%20paleontological%20point%20of%20view%20on%20the%20results%2C%20we%20introduced%20the%0Aexplainability%20method%20SHAP.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Ainstance%20in%20which%20this%20method%20is%20applied%20to%20the%20field%20of%20palaeontology.%20The%0Amain%20goal%20of%20this%20work%20is%20to%20showcase%20how%20Deep%20Learning%20techniques%20can%20aid%20in%0Aidentifying%20isolated%20fossil%20shark%20teeth%2C%20paving%20the%20way%20for%20developing%20new%0Ainformation%20tools%20for%20automating%20the%20recognition%20and%20classification%20of%20fossils.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtificial%2520Intelligence-powered%2520fossil%2520shark%2520tooth%2520identification%253A%250A%2520%2520Unleashing%2520the%2520potential%2520of%2520Convolutional%2520Neural%2520Networks%26entry.906535625%3DAndrea%2520Barucci%2520and%2520Giulia%2520Ciacci%2520and%2520Pietro%2520Li%25C3%25B2%2520and%2520Tiago%2520Azevedo%2520and%2520Andrea%2520Di%2520Cencio%2520and%2520Marco%2520Merella%2520and%2520Giovanni%2520Bianucci%2520and%2520Giulia%2520Bosio%2520and%2520Simone%2520Casati%2520and%2520Alberto%2520Collareta%26entry.1292438233%3D%2520%2520All%2520fields%2520of%2520knowledge%2520are%2520being%2520impacted%2520by%2520Artificial%2520Intelligence.%2520In%250Aparticular%252C%2520the%2520Deep%2520Learning%2520paradigm%2520enables%2520the%2520development%2520of%2520data%2520analysis%250Atools%2520that%2520support%2520subject%2520matter%2520experts%2520in%2520a%2520variety%2520of%2520sectors%252C%2520from%2520physics%250Aup%2520to%2520the%2520recognition%2520of%2520ancient%2520languages.%2520Palaeontology%2520is%2520now%2520observing%2520this%250Atrend%2520as%2520well.%2520This%2520study%2520explores%2520the%2520capability%2520of%2520Convolutional%2520Neural%250ANetworks%2520%2528CNNs%2529%252C%2520a%2520particular%2520class%2520of%2520Deep%2520Learning%2520algorithms%2520specifically%250Acrafted%2520for%2520computer%2520vision%2520tasks%252C%2520to%2520classify%2520images%2520of%2520isolated%2520fossil%2520shark%250Ateeth%2520gathered%2520from%2520online%2520datasets%2520as%2520well%2520as%2520from%2520the%2520authors%2524%2527%2524%2520experience%250Aon%2520Peruvian%2520Miocene%2520and%2520Italian%2520Pliocene%2520fossil%2520assemblages.%2520The%2520shark%2520taxa%250Athat%2520are%2520included%2520in%2520the%2520final%252C%2520composite%2520dataset%2520%2528which%2520consists%2520of%2520more%2520than%250Aone%2520thousand%2520images%2529%2520are%2520representative%2520of%2520both%2520extinct%2520and%2520extant%2520genera%252C%250Anamely%252C%2520Carcharhinus%252C%2520Carcharias%252C%2520Carcharocles%252C%2520Chlamydoselachus%252C%250ACosmopolitodus%252C%2520Galeocerdo%252C%2520Hemipristis%252C%2520Notorynchus%252C%2520Prionace%2520and%2520Squatina.%2520We%250Adeveloped%2520a%2520CNN%252C%2520named%2520SharkNet-X%252C%2520specifically%2520tailored%2520on%2520our%2520recognition%250Atask%252C%2520reaching%2520a%25205-fold%2520cross%2520validated%2520mean%2520accuracy%2520of%25200.85%2520to%2520identify%250Aimages%2520containing%2520a%2520single%2520shark%2520tooth.%2520Furthermore%252C%2520we%2520elaborated%2520a%250Avisualization%2520of%2520the%2520features%2520extracted%2520from%2520images%2520using%2520the%2520last%2520dense%2520layer%250Aof%2520the%2520CNN%252C%2520achieved%2520through%2520the%2520application%2520of%2520the%2520clustering%2520technique%2520t-SNE.%250AIn%2520addition%252C%2520in%2520order%2520to%2520understand%2520and%2520explain%2520the%2520behaviour%2520of%2520the%2520CNN%2520while%250Agiving%2520a%2520paleontological%2520point%2520of%2520view%2520on%2520the%2520results%252C%2520we%2520introduced%2520the%250Aexplainability%2520method%2520SHAP.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%250Ainstance%2520in%2520which%2520this%2520method%2520is%2520applied%2520to%2520the%2520field%2520of%2520palaeontology.%2520The%250Amain%2520goal%2520of%2520this%2520work%2520is%2520to%2520showcase%2520how%2520Deep%2520Learning%2520techniques%2520can%2520aid%2520in%250Aidentifying%2520isolated%2520fossil%2520shark%2520teeth%252C%2520paving%2520the%2520way%2520for%2520developing%2520new%250Ainformation%2520tools%2520for%2520automating%2520the%2520recognition%2520and%2520classification%2520of%2520fossils.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial%20Intelligence-powered%20fossil%20shark%20tooth%20identification%3A%0A%20%20Unleashing%20the%20potential%20of%20Convolutional%20Neural%20Networks&entry.906535625=Andrea%20Barucci%20and%20Giulia%20Ciacci%20and%20Pietro%20Li%C3%B2%20and%20Tiago%20Azevedo%20and%20Andrea%20Di%20Cencio%20and%20Marco%20Merella%20and%20Giovanni%20Bianucci%20and%20Giulia%20Bosio%20and%20Simone%20Casati%20and%20Alberto%20Collareta&entry.1292438233=%20%20All%20fields%20of%20knowledge%20are%20being%20impacted%20by%20Artificial%20Intelligence.%20In%0Aparticular%2C%20the%20Deep%20Learning%20paradigm%20enables%20the%20development%20of%20data%20analysis%0Atools%20that%20support%20subject%20matter%20experts%20in%20a%20variety%20of%20sectors%2C%20from%20physics%0Aup%20to%20the%20recognition%20of%20ancient%20languages.%20Palaeontology%20is%20now%20observing%20this%0Atrend%20as%20well.%20This%20study%20explores%20the%20capability%20of%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%2C%20a%20particular%20class%20of%20Deep%20Learning%20algorithms%20specifically%0Acrafted%20for%20computer%20vision%20tasks%2C%20to%20classify%20images%20of%20isolated%20fossil%20shark%0Ateeth%20gathered%20from%20online%20datasets%20as%20well%20as%20from%20the%20authors%24%27%24%20experience%0Aon%20Peruvian%20Miocene%20and%20Italian%20Pliocene%20fossil%20assemblages.%20The%20shark%20taxa%0Athat%20are%20included%20in%20the%20final%2C%20composite%20dataset%20%28which%20consists%20of%20more%20than%0Aone%20thousand%20images%29%20are%20representative%20of%20both%20extinct%20and%20extant%20genera%2C%0Anamely%2C%20Carcharhinus%2C%20Carcharias%2C%20Carcharocles%2C%20Chlamydoselachus%2C%0ACosmopolitodus%2C%20Galeocerdo%2C%20Hemipristis%2C%20Notorynchus%2C%20Prionace%20and%20Squatina.%20We%0Adeveloped%20a%20CNN%2C%20named%20SharkNet-X%2C%20specifically%20tailored%20on%20our%20recognition%0Atask%2C%20reaching%20a%205-fold%20cross%20validated%20mean%20accuracy%20of%200.85%20to%20identify%0Aimages%20containing%20a%20single%20shark%20tooth.%20Furthermore%2C%20we%20elaborated%20a%0Avisualization%20of%20the%20features%20extracted%20from%20images%20using%20the%20last%20dense%20layer%0Aof%20the%20CNN%2C%20achieved%20through%20the%20application%20of%20the%20clustering%20technique%20t-SNE.%0AIn%20addition%2C%20in%20order%20to%20understand%20and%20explain%20the%20behaviour%20of%20the%20CNN%20while%0Agiving%20a%20paleontological%20point%20of%20view%20on%20the%20results%2C%20we%20introduced%20the%0Aexplainability%20method%20SHAP.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Ainstance%20in%20which%20this%20method%20is%20applied%20to%20the%20field%20of%20palaeontology.%20The%0Amain%20goal%20of%20this%20work%20is%20to%20showcase%20how%20Deep%20Learning%20techniques%20can%20aid%20in%0Aidentifying%20isolated%20fossil%20shark%20teeth%2C%20paving%20the%20way%20for%20developing%20new%0Ainformation%20tools%20for%20automating%20the%20recognition%20and%20classification%20of%20fossils.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04189v1&entry.124074799=Read"},
{"title": "ViewFormer: Exploring Spatiotemporal Modeling for Multi-View 3D\n  Occupancy Perception via View-Guided Transformers", "author": "Jinke Li and Xiao He and Chonghua Zhou and Xiaoqiang Cheng and Yang Wen and Dan Zhang", "abstract": "  3D occupancy, an advanced perception technology for driving scenarios,\nrepresents the entire scene without distinguishing between foreground and\nbackground by quantifying the physical space into a grid map. The widely\nadopted projection-first deformable attention, efficient in transforming image\nfeatures into 3D representations, encounters challenges in aggregating\nmulti-view features due to sensor deployment constraints. To address this\nissue, we propose our learning-first view attention mechanism for effective\nmulti-view feature aggregation. Moreover, we showcase the scalability of our\nview attention across diverse multi-view 3D tasks, such as map construction and\n3D object detection. Leveraging the proposed view attention as well as an\nadditional multi-frame streaming temporal attention, we introduce ViewFormer, a\nvision-centric transformer-based framework for spatiotemporal feature\naggregation. To further explore occupancy-level flow representation, we present\nFlowOcc3D, a benchmark built on top of existing high-quality datasets.\nQualitative and quantitative analyses on this benchmark reveal the potential to\nrepresent fine-grained dynamic scenes. Extensive experiments show that our\napproach significantly outperforms prior state-of-the-art methods. The codes\nand benchmark will be released soon.\n", "link": "http://arxiv.org/abs/2405.04299v1", "date": "2024-05-07", "relevancy": 2.3034, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5806}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5741}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViewFormer%3A%20Exploring%20Spatiotemporal%20Modeling%20for%20Multi-View%203D%0A%20%20Occupancy%20Perception%20via%20View-Guided%20Transformers&body=Title%3A%20ViewFormer%3A%20Exploring%20Spatiotemporal%20Modeling%20for%20Multi-View%203D%0A%20%20Occupancy%20Perception%20via%20View-Guided%20Transformers%0AAuthor%3A%20Jinke%20Li%20and%20Xiao%20He%20and%20Chonghua%20Zhou%20and%20Xiaoqiang%20Cheng%20and%20Yang%20Wen%20and%20Dan%20Zhang%0AAbstract%3A%20%20%203D%20occupancy%2C%20an%20advanced%20perception%20technology%20for%20driving%20scenarios%2C%0Arepresents%20the%20entire%20scene%20without%20distinguishing%20between%20foreground%20and%0Abackground%20by%20quantifying%20the%20physical%20space%20into%20a%20grid%20map.%20The%20widely%0Aadopted%20projection-first%20deformable%20attention%2C%20efficient%20in%20transforming%20image%0Afeatures%20into%203D%20representations%2C%20encounters%20challenges%20in%20aggregating%0Amulti-view%20features%20due%20to%20sensor%20deployment%20constraints.%20To%20address%20this%0Aissue%2C%20we%20propose%20our%20learning-first%20view%20attention%20mechanism%20for%20effective%0Amulti-view%20feature%20aggregation.%20Moreover%2C%20we%20showcase%20the%20scalability%20of%20our%0Aview%20attention%20across%20diverse%20multi-view%203D%20tasks%2C%20such%20as%20map%20construction%20and%0A3D%20object%20detection.%20Leveraging%20the%20proposed%20view%20attention%20as%20well%20as%20an%0Aadditional%20multi-frame%20streaming%20temporal%20attention%2C%20we%20introduce%20ViewFormer%2C%20a%0Avision-centric%20transformer-based%20framework%20for%20spatiotemporal%20feature%0Aaggregation.%20To%20further%20explore%20occupancy-level%20flow%20representation%2C%20we%20present%0AFlowOcc3D%2C%20a%20benchmark%20built%20on%20top%20of%20existing%20high-quality%20datasets.%0AQualitative%20and%20quantitative%20analyses%20on%20this%20benchmark%20reveal%20the%20potential%20to%0Arepresent%20fine-grained%20dynamic%20scenes.%20Extensive%20experiments%20show%20that%20our%0Aapproach%20significantly%20outperforms%20prior%20state-of-the-art%20methods.%20The%20codes%0Aand%20benchmark%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04299v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViewFormer%253A%2520Exploring%2520Spatiotemporal%2520Modeling%2520for%2520Multi-View%25203D%250A%2520%2520Occupancy%2520Perception%2520via%2520View-Guided%2520Transformers%26entry.906535625%3DJinke%2520Li%2520and%2520Xiao%2520He%2520and%2520Chonghua%2520Zhou%2520and%2520Xiaoqiang%2520Cheng%2520and%2520Yang%2520Wen%2520and%2520Dan%2520Zhang%26entry.1292438233%3D%2520%25203D%2520occupancy%252C%2520an%2520advanced%2520perception%2520technology%2520for%2520driving%2520scenarios%252C%250Arepresents%2520the%2520entire%2520scene%2520without%2520distinguishing%2520between%2520foreground%2520and%250Abackground%2520by%2520quantifying%2520the%2520physical%2520space%2520into%2520a%2520grid%2520map.%2520The%2520widely%250Aadopted%2520projection-first%2520deformable%2520attention%252C%2520efficient%2520in%2520transforming%2520image%250Afeatures%2520into%25203D%2520representations%252C%2520encounters%2520challenges%2520in%2520aggregating%250Amulti-view%2520features%2520due%2520to%2520sensor%2520deployment%2520constraints.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520our%2520learning-first%2520view%2520attention%2520mechanism%2520for%2520effective%250Amulti-view%2520feature%2520aggregation.%2520Moreover%252C%2520we%2520showcase%2520the%2520scalability%2520of%2520our%250Aview%2520attention%2520across%2520diverse%2520multi-view%25203D%2520tasks%252C%2520such%2520as%2520map%2520construction%2520and%250A3D%2520object%2520detection.%2520Leveraging%2520the%2520proposed%2520view%2520attention%2520as%2520well%2520as%2520an%250Aadditional%2520multi-frame%2520streaming%2520temporal%2520attention%252C%2520we%2520introduce%2520ViewFormer%252C%2520a%250Avision-centric%2520transformer-based%2520framework%2520for%2520spatiotemporal%2520feature%250Aaggregation.%2520To%2520further%2520explore%2520occupancy-level%2520flow%2520representation%252C%2520we%2520present%250AFlowOcc3D%252C%2520a%2520benchmark%2520built%2520on%2520top%2520of%2520existing%2520high-quality%2520datasets.%250AQualitative%2520and%2520quantitative%2520analyses%2520on%2520this%2520benchmark%2520reveal%2520the%2520potential%2520to%250Arepresent%2520fine-grained%2520dynamic%2520scenes.%2520Extensive%2520experiments%2520show%2520that%2520our%250Aapproach%2520significantly%2520outperforms%2520prior%2520state-of-the-art%2520methods.%2520The%2520codes%250Aand%2520benchmark%2520will%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04299v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViewFormer%3A%20Exploring%20Spatiotemporal%20Modeling%20for%20Multi-View%203D%0A%20%20Occupancy%20Perception%20via%20View-Guided%20Transformers&entry.906535625=Jinke%20Li%20and%20Xiao%20He%20and%20Chonghua%20Zhou%20and%20Xiaoqiang%20Cheng%20and%20Yang%20Wen%20and%20Dan%20Zhang&entry.1292438233=%20%203D%20occupancy%2C%20an%20advanced%20perception%20technology%20for%20driving%20scenarios%2C%0Arepresents%20the%20entire%20scene%20without%20distinguishing%20between%20foreground%20and%0Abackground%20by%20quantifying%20the%20physical%20space%20into%20a%20grid%20map.%20The%20widely%0Aadopted%20projection-first%20deformable%20attention%2C%20efficient%20in%20transforming%20image%0Afeatures%20into%203D%20representations%2C%20encounters%20challenges%20in%20aggregating%0Amulti-view%20features%20due%20to%20sensor%20deployment%20constraints.%20To%20address%20this%0Aissue%2C%20we%20propose%20our%20learning-first%20view%20attention%20mechanism%20for%20effective%0Amulti-view%20feature%20aggregation.%20Moreover%2C%20we%20showcase%20the%20scalability%20of%20our%0Aview%20attention%20across%20diverse%20multi-view%203D%20tasks%2C%20such%20as%20map%20construction%20and%0A3D%20object%20detection.%20Leveraging%20the%20proposed%20view%20attention%20as%20well%20as%20an%0Aadditional%20multi-frame%20streaming%20temporal%20attention%2C%20we%20introduce%20ViewFormer%2C%20a%0Avision-centric%20transformer-based%20framework%20for%20spatiotemporal%20feature%0Aaggregation.%20To%20further%20explore%20occupancy-level%20flow%20representation%2C%20we%20present%0AFlowOcc3D%2C%20a%20benchmark%20built%20on%20top%20of%20existing%20high-quality%20datasets.%0AQualitative%20and%20quantitative%20analyses%20on%20this%20benchmark%20reveal%20the%20potential%20to%0Arepresent%20fine-grained%20dynamic%20scenes.%20Extensive%20experiments%20show%20that%20our%0Aapproach%20significantly%20outperforms%20prior%20state-of-the-art%20methods.%20The%20codes%0Aand%20benchmark%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04299v1&entry.124074799=Read"},
{"title": "Detecting AI-Generated Sentences in Realistic Human-AI Collaborative\n  Hybrid Texts: Challenges, Strategies, and Insights", "author": "Zijie Zeng and Shiqi Liu and Lele Sha and Zhuang Li and Kaixun Yang and Sannyuya Liu and Dragan Ga\u0161evi\u0107 and Guanliang Chen", "abstract": "  This study explores the challenge of sentence-level AI-generated text\ndetection within human-AI collaborative hybrid texts. Existing studies of\nAI-generated text detection for hybrid texts often rely on synthetic datasets.\nThese typically involve hybrid texts with a limited number of boundaries. We\ncontend that studies of detecting AI-generated content within hybrid texts\nshould cover different types of hybrid texts generated in realistic settings to\nbetter inform real-world applications. Therefore, our study utilizes the\nCoAuthor dataset, which includes diverse, realistic hybrid texts generated\nthrough the collaboration between human writers and an intelligent writing\nsystem in multi-turn interactions. We adopt a two-step, segmentation-based\npipeline: (i) detect segments within a given hybrid text where each segment\ncontains sentences of consistent authorship, and (ii) classify the authorship\nof each identified segment. Our empirical findings highlight (1) detecting\nAI-generated sentences in hybrid texts is overall a challenging task because\n(1.1) human writers' selecting and even editing AI-generated sentences based on\npersonal preferences adds difficulty in identifying the authorship of segments;\n(1.2) the frequent change of authorship between neighboring sentences within\nthe hybrid text creates difficulties for segment detectors in identifying\nauthorship-consistent segments; (1.3) the short length of text segments within\nhybrid texts provides limited stylistic cues for reliable authorship\ndetermination; (2) before embarking on the detection process, it is beneficial\nto assess the average length of segments within the hybrid text. This\nassessment aids in deciding whether (2.1) to employ a text segmentation-based\nstrategy for hybrid texts with longer segments, or (2.2) to adopt a direct\nsentence-by-sentence classification strategy for those with shorter segments.\n", "link": "http://arxiv.org/abs/2403.03506v2", "date": "2024-05-07", "relevancy": 2.2868, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4792}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4572}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20AI-Generated%20Sentences%20in%20Realistic%20Human-AI%20Collaborative%0A%20%20Hybrid%20Texts%3A%20Challenges%2C%20Strategies%2C%20and%20Insights&body=Title%3A%20Detecting%20AI-Generated%20Sentences%20in%20Realistic%20Human-AI%20Collaborative%0A%20%20Hybrid%20Texts%3A%20Challenges%2C%20Strategies%2C%20and%20Insights%0AAuthor%3A%20Zijie%20Zeng%20and%20Shiqi%20Liu%20and%20Lele%20Sha%20and%20Zhuang%20Li%20and%20Kaixun%20Yang%20and%20Sannyuya%20Liu%20and%20Dragan%20Ga%C5%A1evi%C4%87%20and%20Guanliang%20Chen%0AAbstract%3A%20%20%20This%20study%20explores%20the%20challenge%20of%20sentence-level%20AI-generated%20text%0Adetection%20within%20human-AI%20collaborative%20hybrid%20texts.%20Existing%20studies%20of%0AAI-generated%20text%20detection%20for%20hybrid%20texts%20often%20rely%20on%20synthetic%20datasets.%0AThese%20typically%20involve%20hybrid%20texts%20with%20a%20limited%20number%20of%20boundaries.%20We%0Acontend%20that%20studies%20of%20detecting%20AI-generated%20content%20within%20hybrid%20texts%0Ashould%20cover%20different%20types%20of%20hybrid%20texts%20generated%20in%20realistic%20settings%20to%0Abetter%20inform%20real-world%20applications.%20Therefore%2C%20our%20study%20utilizes%20the%0ACoAuthor%20dataset%2C%20which%20includes%20diverse%2C%20realistic%20hybrid%20texts%20generated%0Athrough%20the%20collaboration%20between%20human%20writers%20and%20an%20intelligent%20writing%0Asystem%20in%20multi-turn%20interactions.%20We%20adopt%20a%20two-step%2C%20segmentation-based%0Apipeline%3A%20%28i%29%20detect%20segments%20within%20a%20given%20hybrid%20text%20where%20each%20segment%0Acontains%20sentences%20of%20consistent%20authorship%2C%20and%20%28ii%29%20classify%20the%20authorship%0Aof%20each%20identified%20segment.%20Our%20empirical%20findings%20highlight%20%281%29%20detecting%0AAI-generated%20sentences%20in%20hybrid%20texts%20is%20overall%20a%20challenging%20task%20because%0A%281.1%29%20human%20writers%27%20selecting%20and%20even%20editing%20AI-generated%20sentences%20based%20on%0Apersonal%20preferences%20adds%20difficulty%20in%20identifying%20the%20authorship%20of%20segments%3B%0A%281.2%29%20the%20frequent%20change%20of%20authorship%20between%20neighboring%20sentences%20within%0Athe%20hybrid%20text%20creates%20difficulties%20for%20segment%20detectors%20in%20identifying%0Aauthorship-consistent%20segments%3B%20%281.3%29%20the%20short%20length%20of%20text%20segments%20within%0Ahybrid%20texts%20provides%20limited%20stylistic%20cues%20for%20reliable%20authorship%0Adetermination%3B%20%282%29%20before%20embarking%20on%20the%20detection%20process%2C%20it%20is%20beneficial%0Ato%20assess%20the%20average%20length%20of%20segments%20within%20the%20hybrid%20text.%20This%0Aassessment%20aids%20in%20deciding%20whether%20%282.1%29%20to%20employ%20a%20text%20segmentation-based%0Astrategy%20for%20hybrid%20texts%20with%20longer%20segments%2C%20or%20%282.2%29%20to%20adopt%20a%20direct%0Asentence-by-sentence%20classification%20strategy%20for%20those%20with%20shorter%20segments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03506v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520AI-Generated%2520Sentences%2520in%2520Realistic%2520Human-AI%2520Collaborative%250A%2520%2520Hybrid%2520Texts%253A%2520Challenges%252C%2520Strategies%252C%2520and%2520Insights%26entry.906535625%3DZijie%2520Zeng%2520and%2520Shiqi%2520Liu%2520and%2520Lele%2520Sha%2520and%2520Zhuang%2520Li%2520and%2520Kaixun%2520Yang%2520and%2520Sannyuya%2520Liu%2520and%2520Dragan%2520Ga%25C5%25A1evi%25C4%2587%2520and%2520Guanliang%2520Chen%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520the%2520challenge%2520of%2520sentence-level%2520AI-generated%2520text%250Adetection%2520within%2520human-AI%2520collaborative%2520hybrid%2520texts.%2520Existing%2520studies%2520of%250AAI-generated%2520text%2520detection%2520for%2520hybrid%2520texts%2520often%2520rely%2520on%2520synthetic%2520datasets.%250AThese%2520typically%2520involve%2520hybrid%2520texts%2520with%2520a%2520limited%2520number%2520of%2520boundaries.%2520We%250Acontend%2520that%2520studies%2520of%2520detecting%2520AI-generated%2520content%2520within%2520hybrid%2520texts%250Ashould%2520cover%2520different%2520types%2520of%2520hybrid%2520texts%2520generated%2520in%2520realistic%2520settings%2520to%250Abetter%2520inform%2520real-world%2520applications.%2520Therefore%252C%2520our%2520study%2520utilizes%2520the%250ACoAuthor%2520dataset%252C%2520which%2520includes%2520diverse%252C%2520realistic%2520hybrid%2520texts%2520generated%250Athrough%2520the%2520collaboration%2520between%2520human%2520writers%2520and%2520an%2520intelligent%2520writing%250Asystem%2520in%2520multi-turn%2520interactions.%2520We%2520adopt%2520a%2520two-step%252C%2520segmentation-based%250Apipeline%253A%2520%2528i%2529%2520detect%2520segments%2520within%2520a%2520given%2520hybrid%2520text%2520where%2520each%2520segment%250Acontains%2520sentences%2520of%2520consistent%2520authorship%252C%2520and%2520%2528ii%2529%2520classify%2520the%2520authorship%250Aof%2520each%2520identified%2520segment.%2520Our%2520empirical%2520findings%2520highlight%2520%25281%2529%2520detecting%250AAI-generated%2520sentences%2520in%2520hybrid%2520texts%2520is%2520overall%2520a%2520challenging%2520task%2520because%250A%25281.1%2529%2520human%2520writers%2527%2520selecting%2520and%2520even%2520editing%2520AI-generated%2520sentences%2520based%2520on%250Apersonal%2520preferences%2520adds%2520difficulty%2520in%2520identifying%2520the%2520authorship%2520of%2520segments%253B%250A%25281.2%2529%2520the%2520frequent%2520change%2520of%2520authorship%2520between%2520neighboring%2520sentences%2520within%250Athe%2520hybrid%2520text%2520creates%2520difficulties%2520for%2520segment%2520detectors%2520in%2520identifying%250Aauthorship-consistent%2520segments%253B%2520%25281.3%2529%2520the%2520short%2520length%2520of%2520text%2520segments%2520within%250Ahybrid%2520texts%2520provides%2520limited%2520stylistic%2520cues%2520for%2520reliable%2520authorship%250Adetermination%253B%2520%25282%2529%2520before%2520embarking%2520on%2520the%2520detection%2520process%252C%2520it%2520is%2520beneficial%250Ato%2520assess%2520the%2520average%2520length%2520of%2520segments%2520within%2520the%2520hybrid%2520text.%2520This%250Aassessment%2520aids%2520in%2520deciding%2520whether%2520%25282.1%2529%2520to%2520employ%2520a%2520text%2520segmentation-based%250Astrategy%2520for%2520hybrid%2520texts%2520with%2520longer%2520segments%252C%2520or%2520%25282.2%2529%2520to%2520adopt%2520a%2520direct%250Asentence-by-sentence%2520classification%2520strategy%2520for%2520those%2520with%2520shorter%2520segments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03506v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20AI-Generated%20Sentences%20in%20Realistic%20Human-AI%20Collaborative%0A%20%20Hybrid%20Texts%3A%20Challenges%2C%20Strategies%2C%20and%20Insights&entry.906535625=Zijie%20Zeng%20and%20Shiqi%20Liu%20and%20Lele%20Sha%20and%20Zhuang%20Li%20and%20Kaixun%20Yang%20and%20Sannyuya%20Liu%20and%20Dragan%20Ga%C5%A1evi%C4%87%20and%20Guanliang%20Chen&entry.1292438233=%20%20This%20study%20explores%20the%20challenge%20of%20sentence-level%20AI-generated%20text%0Adetection%20within%20human-AI%20collaborative%20hybrid%20texts.%20Existing%20studies%20of%0AAI-generated%20text%20detection%20for%20hybrid%20texts%20often%20rely%20on%20synthetic%20datasets.%0AThese%20typically%20involve%20hybrid%20texts%20with%20a%20limited%20number%20of%20boundaries.%20We%0Acontend%20that%20studies%20of%20detecting%20AI-generated%20content%20within%20hybrid%20texts%0Ashould%20cover%20different%20types%20of%20hybrid%20texts%20generated%20in%20realistic%20settings%20to%0Abetter%20inform%20real-world%20applications.%20Therefore%2C%20our%20study%20utilizes%20the%0ACoAuthor%20dataset%2C%20which%20includes%20diverse%2C%20realistic%20hybrid%20texts%20generated%0Athrough%20the%20collaboration%20between%20human%20writers%20and%20an%20intelligent%20writing%0Asystem%20in%20multi-turn%20interactions.%20We%20adopt%20a%20two-step%2C%20segmentation-based%0Apipeline%3A%20%28i%29%20detect%20segments%20within%20a%20given%20hybrid%20text%20where%20each%20segment%0Acontains%20sentences%20of%20consistent%20authorship%2C%20and%20%28ii%29%20classify%20the%20authorship%0Aof%20each%20identified%20segment.%20Our%20empirical%20findings%20highlight%20%281%29%20detecting%0AAI-generated%20sentences%20in%20hybrid%20texts%20is%20overall%20a%20challenging%20task%20because%0A%281.1%29%20human%20writers%27%20selecting%20and%20even%20editing%20AI-generated%20sentences%20based%20on%0Apersonal%20preferences%20adds%20difficulty%20in%20identifying%20the%20authorship%20of%20segments%3B%0A%281.2%29%20the%20frequent%20change%20of%20authorship%20between%20neighboring%20sentences%20within%0Athe%20hybrid%20text%20creates%20difficulties%20for%20segment%20detectors%20in%20identifying%0Aauthorship-consistent%20segments%3B%20%281.3%29%20the%20short%20length%20of%20text%20segments%20within%0Ahybrid%20texts%20provides%20limited%20stylistic%20cues%20for%20reliable%20authorship%0Adetermination%3B%20%282%29%20before%20embarking%20on%20the%20detection%20process%2C%20it%20is%20beneficial%0Ato%20assess%20the%20average%20length%20of%20segments%20within%20the%20hybrid%20text.%20This%0Aassessment%20aids%20in%20deciding%20whether%20%282.1%29%20to%20employ%20a%20text%20segmentation-based%0Astrategy%20for%20hybrid%20texts%20with%20longer%20segments%2C%20or%20%282.2%29%20to%20adopt%20a%20direct%0Asentence-by-sentence%20classification%20strategy%20for%20those%20with%20shorter%20segments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03506v2&entry.124074799=Read"},
{"title": "Human Image Generation: A Comprehensive Survey", "author": "Zhen Jia and Zhang Zhang and Liang Wang and Tieniu Tan", "abstract": "  Image and video synthesis has become a blooming topic in computer vision and\nmachine learning communities along with the developments of deep generative\nmodels, due to its great academic and application value. Many researchers have\nbeen devoted to synthesizing high-fidelity human images as one of the most\ncommonly seen object categories in daily lives, where a large number of studies\nare performed based on various models, task settings and applications. Thus, it\nis necessary to give a comprehensive overview on these variant methods on human\nimage generation. In this paper, we divide human image generation techniques\ninto three paradigms, i.e., data-driven methods, knowledge-guided methods and\nhybrid methods. For each paradigm, the most representative models and the\ncorresponding variants are presented, where the advantages and characteristics\nof different methods are summarized in terms of model architectures. Besides,\nthe main public human image datasets and evaluation metrics in the literature\nare summarized. Furthermore, due to the wide application potentials, the\ntypical downstream usages of synthesized human images are covered. Finally, the\nchallenges and potential opportunities of human image generation are discussed\nto shed light on future research.\n", "link": "http://arxiv.org/abs/2212.08896v2", "date": "2024-05-07", "relevancy": 2.276, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5946}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5662}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20Image%20Generation%3A%20A%20Comprehensive%20Survey&body=Title%3A%20Human%20Image%20Generation%3A%20A%20Comprehensive%20Survey%0AAuthor%3A%20Zhen%20Jia%20and%20Zhang%20Zhang%20and%20Liang%20Wang%20and%20Tieniu%20Tan%0AAbstract%3A%20%20%20Image%20and%20video%20synthesis%20has%20become%20a%20blooming%20topic%20in%20computer%20vision%20and%0Amachine%20learning%20communities%20along%20with%20the%20developments%20of%20deep%20generative%0Amodels%2C%20due%20to%20its%20great%20academic%20and%20application%20value.%20Many%20researchers%20have%0Abeen%20devoted%20to%20synthesizing%20high-fidelity%20human%20images%20as%20one%20of%20the%20most%0Acommonly%20seen%20object%20categories%20in%20daily%20lives%2C%20where%20a%20large%20number%20of%20studies%0Aare%20performed%20based%20on%20various%20models%2C%20task%20settings%20and%20applications.%20Thus%2C%20it%0Ais%20necessary%20to%20give%20a%20comprehensive%20overview%20on%20these%20variant%20methods%20on%20human%0Aimage%20generation.%20In%20this%20paper%2C%20we%20divide%20human%20image%20generation%20techniques%0Ainto%20three%20paradigms%2C%20i.e.%2C%20data-driven%20methods%2C%20knowledge-guided%20methods%20and%0Ahybrid%20methods.%20For%20each%20paradigm%2C%20the%20most%20representative%20models%20and%20the%0Acorresponding%20variants%20are%20presented%2C%20where%20the%20advantages%20and%20characteristics%0Aof%20different%20methods%20are%20summarized%20in%20terms%20of%20model%20architectures.%20Besides%2C%0Athe%20main%20public%20human%20image%20datasets%20and%20evaluation%20metrics%20in%20the%20literature%0Aare%20summarized.%20Furthermore%2C%20due%20to%20the%20wide%20application%20potentials%2C%20the%0Atypical%20downstream%20usages%20of%20synthesized%20human%20images%20are%20covered.%20Finally%2C%20the%0Achallenges%20and%20potential%20opportunities%20of%20human%20image%20generation%20are%20discussed%0Ato%20shed%20light%20on%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.08896v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520Image%2520Generation%253A%2520A%2520Comprehensive%2520Survey%26entry.906535625%3DZhen%2520Jia%2520and%2520Zhang%2520Zhang%2520and%2520Liang%2520Wang%2520and%2520Tieniu%2520Tan%26entry.1292438233%3D%2520%2520Image%2520and%2520video%2520synthesis%2520has%2520become%2520a%2520blooming%2520topic%2520in%2520computer%2520vision%2520and%250Amachine%2520learning%2520communities%2520along%2520with%2520the%2520developments%2520of%2520deep%2520generative%250Amodels%252C%2520due%2520to%2520its%2520great%2520academic%2520and%2520application%2520value.%2520Many%2520researchers%2520have%250Abeen%2520devoted%2520to%2520synthesizing%2520high-fidelity%2520human%2520images%2520as%2520one%2520of%2520the%2520most%250Acommonly%2520seen%2520object%2520categories%2520in%2520daily%2520lives%252C%2520where%2520a%2520large%2520number%2520of%2520studies%250Aare%2520performed%2520based%2520on%2520various%2520models%252C%2520task%2520settings%2520and%2520applications.%2520Thus%252C%2520it%250Ais%2520necessary%2520to%2520give%2520a%2520comprehensive%2520overview%2520on%2520these%2520variant%2520methods%2520on%2520human%250Aimage%2520generation.%2520In%2520this%2520paper%252C%2520we%2520divide%2520human%2520image%2520generation%2520techniques%250Ainto%2520three%2520paradigms%252C%2520i.e.%252C%2520data-driven%2520methods%252C%2520knowledge-guided%2520methods%2520and%250Ahybrid%2520methods.%2520For%2520each%2520paradigm%252C%2520the%2520most%2520representative%2520models%2520and%2520the%250Acorresponding%2520variants%2520are%2520presented%252C%2520where%2520the%2520advantages%2520and%2520characteristics%250Aof%2520different%2520methods%2520are%2520summarized%2520in%2520terms%2520of%2520model%2520architectures.%2520Besides%252C%250Athe%2520main%2520public%2520human%2520image%2520datasets%2520and%2520evaluation%2520metrics%2520in%2520the%2520literature%250Aare%2520summarized.%2520Furthermore%252C%2520due%2520to%2520the%2520wide%2520application%2520potentials%252C%2520the%250Atypical%2520downstream%2520usages%2520of%2520synthesized%2520human%2520images%2520are%2520covered.%2520Finally%252C%2520the%250Achallenges%2520and%2520potential%2520opportunities%2520of%2520human%2520image%2520generation%2520are%2520discussed%250Ato%2520shed%2520light%2520on%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.08896v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Image%20Generation%3A%20A%20Comprehensive%20Survey&entry.906535625=Zhen%20Jia%20and%20Zhang%20Zhang%20and%20Liang%20Wang%20and%20Tieniu%20Tan&entry.1292438233=%20%20Image%20and%20video%20synthesis%20has%20become%20a%20blooming%20topic%20in%20computer%20vision%20and%0Amachine%20learning%20communities%20along%20with%20the%20developments%20of%20deep%20generative%0Amodels%2C%20due%20to%20its%20great%20academic%20and%20application%20value.%20Many%20researchers%20have%0Abeen%20devoted%20to%20synthesizing%20high-fidelity%20human%20images%20as%20one%20of%20the%20most%0Acommonly%20seen%20object%20categories%20in%20daily%20lives%2C%20where%20a%20large%20number%20of%20studies%0Aare%20performed%20based%20on%20various%20models%2C%20task%20settings%20and%20applications.%20Thus%2C%20it%0Ais%20necessary%20to%20give%20a%20comprehensive%20overview%20on%20these%20variant%20methods%20on%20human%0Aimage%20generation.%20In%20this%20paper%2C%20we%20divide%20human%20image%20generation%20techniques%0Ainto%20three%20paradigms%2C%20i.e.%2C%20data-driven%20methods%2C%20knowledge-guided%20methods%20and%0Ahybrid%20methods.%20For%20each%20paradigm%2C%20the%20most%20representative%20models%20and%20the%0Acorresponding%20variants%20are%20presented%2C%20where%20the%20advantages%20and%20characteristics%0Aof%20different%20methods%20are%20summarized%20in%20terms%20of%20model%20architectures.%20Besides%2C%0Athe%20main%20public%20human%20image%20datasets%20and%20evaluation%20metrics%20in%20the%20literature%0Aare%20summarized.%20Furthermore%2C%20due%20to%20the%20wide%20application%20potentials%2C%20the%0Atypical%20downstream%20usages%20of%20synthesized%20human%20images%20are%20covered.%20Finally%2C%20the%0Achallenges%20and%20potential%20opportunities%20of%20human%20image%20generation%20are%20discussed%0Ato%20shed%20light%20on%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.08896v2&entry.124074799=Read"},
{"title": "Emergence of Collective Open-Ended Exploration from Decentralized\n  Meta-Reinforcement Learning", "author": "Richard Bornemann and Gautier Hamon and Eleni Nisioti and Cl\u00e9ment Moulin-Frier", "abstract": "  Recent works have proven that intricate cooperative behaviors can emerge in\nagents trained using meta reinforcement learning on open ended task\ndistributions using self-play. While the results are impressive, we argue that\nself-play and other centralized training techniques do not accurately reflect\nhow general collective exploration strategies emerge in the natural world:\nthrough decentralized training and over an open-ended distribution of tasks. In\nthis work we therefore investigate the emergence of collective exploration\nstrategies, where several agents meta-learn independent recurrent policies on\nan open ended distribution of tasks. To this end we introduce a novel\nenvironment with an open ended procedurally generated task space which\ndynamically combines multiple subtasks sampled from five diverse task types to\nform a vast distribution of task trees. We show that decentralized agents\ntrained in our environment exhibit strong generalization abilities when\nconfronted with novel objects at test time. Additionally, despite never being\nforced to cooperate during training the agents learn collective exploration\nstrategies which allow them to solve novel tasks never encountered during\ntraining. We further find that the agents learned collective exploration\nstrategies extend to an open ended task setting, allowing them to solve task\ntrees of twice the depth compared to the ones seen during training. Our open\nsource code as well as videos of the agents can be found on our companion\nwebsite.\n", "link": "http://arxiv.org/abs/2311.00651v3", "date": "2024-05-07", "relevancy": 2.2738, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.61}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5911}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergence%20of%20Collective%20Open-Ended%20Exploration%20from%20Decentralized%0A%20%20Meta-Reinforcement%20Learning&body=Title%3A%20Emergence%20of%20Collective%20Open-Ended%20Exploration%20from%20Decentralized%0A%20%20Meta-Reinforcement%20Learning%0AAuthor%3A%20Richard%20Bornemann%20and%20Gautier%20Hamon%20and%20Eleni%20Nisioti%20and%20Cl%C3%A9ment%20Moulin-Frier%0AAbstract%3A%20%20%20Recent%20works%20have%20proven%20that%20intricate%20cooperative%20behaviors%20can%20emerge%20in%0Aagents%20trained%20using%20meta%20reinforcement%20learning%20on%20open%20ended%20task%0Adistributions%20using%20self-play.%20While%20the%20results%20are%20impressive%2C%20we%20argue%20that%0Aself-play%20and%20other%20centralized%20training%20techniques%20do%20not%20accurately%20reflect%0Ahow%20general%20collective%20exploration%20strategies%20emerge%20in%20the%20natural%20world%3A%0Athrough%20decentralized%20training%20and%20over%20an%20open-ended%20distribution%20of%20tasks.%20In%0Athis%20work%20we%20therefore%20investigate%20the%20emergence%20of%20collective%20exploration%0Astrategies%2C%20where%20several%20agents%20meta-learn%20independent%20recurrent%20policies%20on%0Aan%20open%20ended%20distribution%20of%20tasks.%20To%20this%20end%20we%20introduce%20a%20novel%0Aenvironment%20with%20an%20open%20ended%20procedurally%20generated%20task%20space%20which%0Adynamically%20combines%20multiple%20subtasks%20sampled%20from%20five%20diverse%20task%20types%20to%0Aform%20a%20vast%20distribution%20of%20task%20trees.%20We%20show%20that%20decentralized%20agents%0Atrained%20in%20our%20environment%20exhibit%20strong%20generalization%20abilities%20when%0Aconfronted%20with%20novel%20objects%20at%20test%20time.%20Additionally%2C%20despite%20never%20being%0Aforced%20to%20cooperate%20during%20training%20the%20agents%20learn%20collective%20exploration%0Astrategies%20which%20allow%20them%20to%20solve%20novel%20tasks%20never%20encountered%20during%0Atraining.%20We%20further%20find%20that%20the%20agents%20learned%20collective%20exploration%0Astrategies%20extend%20to%20an%20open%20ended%20task%20setting%2C%20allowing%20them%20to%20solve%20task%0Atrees%20of%20twice%20the%20depth%20compared%20to%20the%20ones%20seen%20during%20training.%20Our%20open%0Asource%20code%20as%20well%20as%20videos%20of%20the%20agents%20can%20be%20found%20on%20our%20companion%0Awebsite.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.00651v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergence%2520of%2520Collective%2520Open-Ended%2520Exploration%2520from%2520Decentralized%250A%2520%2520Meta-Reinforcement%2520Learning%26entry.906535625%3DRichard%2520Bornemann%2520and%2520Gautier%2520Hamon%2520and%2520Eleni%2520Nisioti%2520and%2520Cl%25C3%25A9ment%2520Moulin-Frier%26entry.1292438233%3D%2520%2520Recent%2520works%2520have%2520proven%2520that%2520intricate%2520cooperative%2520behaviors%2520can%2520emerge%2520in%250Aagents%2520trained%2520using%2520meta%2520reinforcement%2520learning%2520on%2520open%2520ended%2520task%250Adistributions%2520using%2520self-play.%2520While%2520the%2520results%2520are%2520impressive%252C%2520we%2520argue%2520that%250Aself-play%2520and%2520other%2520centralized%2520training%2520techniques%2520do%2520not%2520accurately%2520reflect%250Ahow%2520general%2520collective%2520exploration%2520strategies%2520emerge%2520in%2520the%2520natural%2520world%253A%250Athrough%2520decentralized%2520training%2520and%2520over%2520an%2520open-ended%2520distribution%2520of%2520tasks.%2520In%250Athis%2520work%2520we%2520therefore%2520investigate%2520the%2520emergence%2520of%2520collective%2520exploration%250Astrategies%252C%2520where%2520several%2520agents%2520meta-learn%2520independent%2520recurrent%2520policies%2520on%250Aan%2520open%2520ended%2520distribution%2520of%2520tasks.%2520To%2520this%2520end%2520we%2520introduce%2520a%2520novel%250Aenvironment%2520with%2520an%2520open%2520ended%2520procedurally%2520generated%2520task%2520space%2520which%250Adynamically%2520combines%2520multiple%2520subtasks%2520sampled%2520from%2520five%2520diverse%2520task%2520types%2520to%250Aform%2520a%2520vast%2520distribution%2520of%2520task%2520trees.%2520We%2520show%2520that%2520decentralized%2520agents%250Atrained%2520in%2520our%2520environment%2520exhibit%2520strong%2520generalization%2520abilities%2520when%250Aconfronted%2520with%2520novel%2520objects%2520at%2520test%2520time.%2520Additionally%252C%2520despite%2520never%2520being%250Aforced%2520to%2520cooperate%2520during%2520training%2520the%2520agents%2520learn%2520collective%2520exploration%250Astrategies%2520which%2520allow%2520them%2520to%2520solve%2520novel%2520tasks%2520never%2520encountered%2520during%250Atraining.%2520We%2520further%2520find%2520that%2520the%2520agents%2520learned%2520collective%2520exploration%250Astrategies%2520extend%2520to%2520an%2520open%2520ended%2520task%2520setting%252C%2520allowing%2520them%2520to%2520solve%2520task%250Atrees%2520of%2520twice%2520the%2520depth%2520compared%2520to%2520the%2520ones%2520seen%2520during%2520training.%2520Our%2520open%250Asource%2520code%2520as%2520well%2520as%2520videos%2520of%2520the%2520agents%2520can%2520be%2520found%2520on%2520our%2520companion%250Awebsite.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.00651v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergence%20of%20Collective%20Open-Ended%20Exploration%20from%20Decentralized%0A%20%20Meta-Reinforcement%20Learning&entry.906535625=Richard%20Bornemann%20and%20Gautier%20Hamon%20and%20Eleni%20Nisioti%20and%20Cl%C3%A9ment%20Moulin-Frier&entry.1292438233=%20%20Recent%20works%20have%20proven%20that%20intricate%20cooperative%20behaviors%20can%20emerge%20in%0Aagents%20trained%20using%20meta%20reinforcement%20learning%20on%20open%20ended%20task%0Adistributions%20using%20self-play.%20While%20the%20results%20are%20impressive%2C%20we%20argue%20that%0Aself-play%20and%20other%20centralized%20training%20techniques%20do%20not%20accurately%20reflect%0Ahow%20general%20collective%20exploration%20strategies%20emerge%20in%20the%20natural%20world%3A%0Athrough%20decentralized%20training%20and%20over%20an%20open-ended%20distribution%20of%20tasks.%20In%0Athis%20work%20we%20therefore%20investigate%20the%20emergence%20of%20collective%20exploration%0Astrategies%2C%20where%20several%20agents%20meta-learn%20independent%20recurrent%20policies%20on%0Aan%20open%20ended%20distribution%20of%20tasks.%20To%20this%20end%20we%20introduce%20a%20novel%0Aenvironment%20with%20an%20open%20ended%20procedurally%20generated%20task%20space%20which%0Adynamically%20combines%20multiple%20subtasks%20sampled%20from%20five%20diverse%20task%20types%20to%0Aform%20a%20vast%20distribution%20of%20task%20trees.%20We%20show%20that%20decentralized%20agents%0Atrained%20in%20our%20environment%20exhibit%20strong%20generalization%20abilities%20when%0Aconfronted%20with%20novel%20objects%20at%20test%20time.%20Additionally%2C%20despite%20never%20being%0Aforced%20to%20cooperate%20during%20training%20the%20agents%20learn%20collective%20exploration%0Astrategies%20which%20allow%20them%20to%20solve%20novel%20tasks%20never%20encountered%20during%0Atraining.%20We%20further%20find%20that%20the%20agents%20learned%20collective%20exploration%0Astrategies%20extend%20to%20an%20open%20ended%20task%20setting%2C%20allowing%20them%20to%20solve%20task%0Atrees%20of%20twice%20the%20depth%20compared%20to%20the%20ones%20seen%20during%20training.%20Our%20open%0Asource%20code%20as%20well%20as%20videos%20of%20the%20agents%20can%20be%20found%20on%20our%20companion%0Awebsite.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.00651v3&entry.124074799=Read"},
{"title": "Towards Stability of Parameter-free Optimization", "author": "Yijiang Pang and Shuyang Yu and Bao Hoang and Jiayu Zhou", "abstract": "  Hyperparameter tuning, particularly the selection of an appropriate learning\nrate in adaptive gradient training methods, remains a challenge. To tackle this\nchallenge, in this paper, we propose a novel parameter-free optimizer, AdamG\n(Adam with the golden step size), designed to automatically adapt to diverse\noptimization problems without manual tuning. The core technique underlying\nAdamG is our golden step size derived for the AdaGrad-Norm algorithm, which is\nexpected to help AdaGrad-Norm preserve the tuning-free convergence and\napproximate the optimal step size in expectation w.r.t. various optimization\nscenarios. To better evaluate tuning-free performance, we propose a novel\nevaluation criterion, stability, to comprehensively assess the efficacy of\nparameter-free optimizers in addition to classical performance criteria.\nEmpirical results demonstrate that compared with other parameter-free\nbaselines, AdamG achieves superior performance, which is consistently on par\nwith Adam using a manually tuned learning rate across various optimization\ntasks.\n", "link": "http://arxiv.org/abs/2405.04376v1", "date": "2024-05-07", "relevancy": 2.2424, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4518}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.447}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Stability%20of%20Parameter-free%20Optimization&body=Title%3A%20Towards%20Stability%20of%20Parameter-free%20Optimization%0AAuthor%3A%20Yijiang%20Pang%20and%20Shuyang%20Yu%20and%20Bao%20Hoang%20and%20Jiayu%20Zhou%0AAbstract%3A%20%20%20Hyperparameter%20tuning%2C%20particularly%20the%20selection%20of%20an%20appropriate%20learning%0Arate%20in%20adaptive%20gradient%20training%20methods%2C%20remains%20a%20challenge.%20To%20tackle%20this%0Achallenge%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20parameter-free%20optimizer%2C%20AdamG%0A%28Adam%20with%20the%20golden%20step%20size%29%2C%20designed%20to%20automatically%20adapt%20to%20diverse%0Aoptimization%20problems%20without%20manual%20tuning.%20The%20core%20technique%20underlying%0AAdamG%20is%20our%20golden%20step%20size%20derived%20for%20the%20AdaGrad-Norm%20algorithm%2C%20which%20is%0Aexpected%20to%20help%20AdaGrad-Norm%20preserve%20the%20tuning-free%20convergence%20and%0Aapproximate%20the%20optimal%20step%20size%20in%20expectation%20w.r.t.%20various%20optimization%0Ascenarios.%20To%20better%20evaluate%20tuning-free%20performance%2C%20we%20propose%20a%20novel%0Aevaluation%20criterion%2C%20stability%2C%20to%20comprehensively%20assess%20the%20efficacy%20of%0Aparameter-free%20optimizers%20in%20addition%20to%20classical%20performance%20criteria.%0AEmpirical%20results%20demonstrate%20that%20compared%20with%20other%20parameter-free%0Abaselines%2C%20AdamG%20achieves%20superior%20performance%2C%20which%20is%20consistently%20on%20par%0Awith%20Adam%20using%20a%20manually%20tuned%20learning%20rate%20across%20various%20optimization%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04376v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Stability%2520of%2520Parameter-free%2520Optimization%26entry.906535625%3DYijiang%2520Pang%2520and%2520Shuyang%2520Yu%2520and%2520Bao%2520Hoang%2520and%2520Jiayu%2520Zhou%26entry.1292438233%3D%2520%2520Hyperparameter%2520tuning%252C%2520particularly%2520the%2520selection%2520of%2520an%2520appropriate%2520learning%250Arate%2520in%2520adaptive%2520gradient%2520training%2520methods%252C%2520remains%2520a%2520challenge.%2520To%2520tackle%2520this%250Achallenge%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520parameter-free%2520optimizer%252C%2520AdamG%250A%2528Adam%2520with%2520the%2520golden%2520step%2520size%2529%252C%2520designed%2520to%2520automatically%2520adapt%2520to%2520diverse%250Aoptimization%2520problems%2520without%2520manual%2520tuning.%2520The%2520core%2520technique%2520underlying%250AAdamG%2520is%2520our%2520golden%2520step%2520size%2520derived%2520for%2520the%2520AdaGrad-Norm%2520algorithm%252C%2520which%2520is%250Aexpected%2520to%2520help%2520AdaGrad-Norm%2520preserve%2520the%2520tuning-free%2520convergence%2520and%250Aapproximate%2520the%2520optimal%2520step%2520size%2520in%2520expectation%2520w.r.t.%2520various%2520optimization%250Ascenarios.%2520To%2520better%2520evaluate%2520tuning-free%2520performance%252C%2520we%2520propose%2520a%2520novel%250Aevaluation%2520criterion%252C%2520stability%252C%2520to%2520comprehensively%2520assess%2520the%2520efficacy%2520of%250Aparameter-free%2520optimizers%2520in%2520addition%2520to%2520classical%2520performance%2520criteria.%250AEmpirical%2520results%2520demonstrate%2520that%2520compared%2520with%2520other%2520parameter-free%250Abaselines%252C%2520AdamG%2520achieves%2520superior%2520performance%252C%2520which%2520is%2520consistently%2520on%2520par%250Awith%2520Adam%2520using%2520a%2520manually%2520tuned%2520learning%2520rate%2520across%2520various%2520optimization%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04376v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Stability%20of%20Parameter-free%20Optimization&entry.906535625=Yijiang%20Pang%20and%20Shuyang%20Yu%20and%20Bao%20Hoang%20and%20Jiayu%20Zhou&entry.1292438233=%20%20Hyperparameter%20tuning%2C%20particularly%20the%20selection%20of%20an%20appropriate%20learning%0Arate%20in%20adaptive%20gradient%20training%20methods%2C%20remains%20a%20challenge.%20To%20tackle%20this%0Achallenge%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20parameter-free%20optimizer%2C%20AdamG%0A%28Adam%20with%20the%20golden%20step%20size%29%2C%20designed%20to%20automatically%20adapt%20to%20diverse%0Aoptimization%20problems%20without%20manual%20tuning.%20The%20core%20technique%20underlying%0AAdamG%20is%20our%20golden%20step%20size%20derived%20for%20the%20AdaGrad-Norm%20algorithm%2C%20which%20is%0Aexpected%20to%20help%20AdaGrad-Norm%20preserve%20the%20tuning-free%20convergence%20and%0Aapproximate%20the%20optimal%20step%20size%20in%20expectation%20w.r.t.%20various%20optimization%0Ascenarios.%20To%20better%20evaluate%20tuning-free%20performance%2C%20we%20propose%20a%20novel%0Aevaluation%20criterion%2C%20stability%2C%20to%20comprehensively%20assess%20the%20efficacy%20of%0Aparameter-free%20optimizers%20in%20addition%20to%20classical%20performance%20criteria.%0AEmpirical%20results%20demonstrate%20that%20compared%20with%20other%20parameter-free%0Abaselines%2C%20AdamG%20achieves%20superior%20performance%2C%20which%20is%20consistently%20on%20par%0Awith%20Adam%20using%20a%20manually%20tuned%20learning%20rate%20across%20various%20optimization%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04376v1&entry.124074799=Read"},
{"title": "Iterative Reasoning Preference Optimization", "author": "Richard Yuanzhe Pang and Weizhe Yuan and Kyunghyun Cho and He He and Sainbayar Sukhbaatar and Jason Weston", "abstract": "  Iterative preference optimization methods have recently been shown to perform\nwell for general instruction tuning tasks, but typically make little\nimprovement on reasoning tasks (Yuan et al., 2024, Chen et al., 2024). In this\nwork we develop an iterative approach that optimizes the preference between\ncompeting generated Chain-of-Thought (CoT) candidates by optimizing for winning\nvs. losing reasoning steps that lead to the correct answer. We train using a\nmodified DPO loss (Rafailov et al., 2023) with an additional negative\nlog-likelihood term, which we find to be crucial. We show reasoning improves\nacross repeated iterations of this scheme. While only relying on examples in\nthe training set, our approach results in increasing accuracy on GSM8K, MATH,\nand ARC-Challenge for Llama-2-70B-Chat, outperforming other Llama-2-based\nmodels not relying on additionally sourced datasets. For example, we see a\nlarge improvement from 55.6% to 81.6% on GSM8K and an accuracy of 88.7% with\nmajority voting out of 32 samples.\n", "link": "http://arxiv.org/abs/2404.19733v2", "date": "2024-05-07", "relevancy": 2.2329, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4504}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4481}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterative%20Reasoning%20Preference%20Optimization&body=Title%3A%20Iterative%20Reasoning%20Preference%20Optimization%0AAuthor%3A%20Richard%20Yuanzhe%20Pang%20and%20Weizhe%20Yuan%20and%20Kyunghyun%20Cho%20and%20He%20He%20and%20Sainbayar%20Sukhbaatar%20and%20Jason%20Weston%0AAbstract%3A%20%20%20Iterative%20preference%20optimization%20methods%20have%20recently%20been%20shown%20to%20perform%0Awell%20for%20general%20instruction%20tuning%20tasks%2C%20but%20typically%20make%20little%0Aimprovement%20on%20reasoning%20tasks%20%28Yuan%20et%20al.%2C%202024%2C%20Chen%20et%20al.%2C%202024%29.%20In%20this%0Awork%20we%20develop%20an%20iterative%20approach%20that%20optimizes%20the%20preference%20between%0Acompeting%20generated%20Chain-of-Thought%20%28CoT%29%20candidates%20by%20optimizing%20for%20winning%0Avs.%20losing%20reasoning%20steps%20that%20lead%20to%20the%20correct%20answer.%20We%20train%20using%20a%0Amodified%20DPO%20loss%20%28Rafailov%20et%20al.%2C%202023%29%20with%20an%20additional%20negative%0Alog-likelihood%20term%2C%20which%20we%20find%20to%20be%20crucial.%20We%20show%20reasoning%20improves%0Aacross%20repeated%20iterations%20of%20this%20scheme.%20While%20only%20relying%20on%20examples%20in%0Athe%20training%20set%2C%20our%20approach%20results%20in%20increasing%20accuracy%20on%20GSM8K%2C%20MATH%2C%0Aand%20ARC-Challenge%20for%20Llama-2-70B-Chat%2C%20outperforming%20other%20Llama-2-based%0Amodels%20not%20relying%20on%20additionally%20sourced%20datasets.%20For%20example%2C%20we%20see%20a%0Alarge%20improvement%20from%2055.6%25%20to%2081.6%25%20on%20GSM8K%20and%20an%20accuracy%20of%2088.7%25%20with%0Amajority%20voting%20out%20of%2032%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19733v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterative%2520Reasoning%2520Preference%2520Optimization%26entry.906535625%3DRichard%2520Yuanzhe%2520Pang%2520and%2520Weizhe%2520Yuan%2520and%2520Kyunghyun%2520Cho%2520and%2520He%2520He%2520and%2520Sainbayar%2520Sukhbaatar%2520and%2520Jason%2520Weston%26entry.1292438233%3D%2520%2520Iterative%2520preference%2520optimization%2520methods%2520have%2520recently%2520been%2520shown%2520to%2520perform%250Awell%2520for%2520general%2520instruction%2520tuning%2520tasks%252C%2520but%2520typically%2520make%2520little%250Aimprovement%2520on%2520reasoning%2520tasks%2520%2528Yuan%2520et%2520al.%252C%25202024%252C%2520Chen%2520et%2520al.%252C%25202024%2529.%2520In%2520this%250Awork%2520we%2520develop%2520an%2520iterative%2520approach%2520that%2520optimizes%2520the%2520preference%2520between%250Acompeting%2520generated%2520Chain-of-Thought%2520%2528CoT%2529%2520candidates%2520by%2520optimizing%2520for%2520winning%250Avs.%2520losing%2520reasoning%2520steps%2520that%2520lead%2520to%2520the%2520correct%2520answer.%2520We%2520train%2520using%2520a%250Amodified%2520DPO%2520loss%2520%2528Rafailov%2520et%2520al.%252C%25202023%2529%2520with%2520an%2520additional%2520negative%250Alog-likelihood%2520term%252C%2520which%2520we%2520find%2520to%2520be%2520crucial.%2520We%2520show%2520reasoning%2520improves%250Aacross%2520repeated%2520iterations%2520of%2520this%2520scheme.%2520While%2520only%2520relying%2520on%2520examples%2520in%250Athe%2520training%2520set%252C%2520our%2520approach%2520results%2520in%2520increasing%2520accuracy%2520on%2520GSM8K%252C%2520MATH%252C%250Aand%2520ARC-Challenge%2520for%2520Llama-2-70B-Chat%252C%2520outperforming%2520other%2520Llama-2-based%250Amodels%2520not%2520relying%2520on%2520additionally%2520sourced%2520datasets.%2520For%2520example%252C%2520we%2520see%2520a%250Alarge%2520improvement%2520from%252055.6%2525%2520to%252081.6%2525%2520on%2520GSM8K%2520and%2520an%2520accuracy%2520of%252088.7%2525%2520with%250Amajority%2520voting%2520out%2520of%252032%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19733v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20Reasoning%20Preference%20Optimization&entry.906535625=Richard%20Yuanzhe%20Pang%20and%20Weizhe%20Yuan%20and%20Kyunghyun%20Cho%20and%20He%20He%20and%20Sainbayar%20Sukhbaatar%20and%20Jason%20Weston&entry.1292438233=%20%20Iterative%20preference%20optimization%20methods%20have%20recently%20been%20shown%20to%20perform%0Awell%20for%20general%20instruction%20tuning%20tasks%2C%20but%20typically%20make%20little%0Aimprovement%20on%20reasoning%20tasks%20%28Yuan%20et%20al.%2C%202024%2C%20Chen%20et%20al.%2C%202024%29.%20In%20this%0Awork%20we%20develop%20an%20iterative%20approach%20that%20optimizes%20the%20preference%20between%0Acompeting%20generated%20Chain-of-Thought%20%28CoT%29%20candidates%20by%20optimizing%20for%20winning%0Avs.%20losing%20reasoning%20steps%20that%20lead%20to%20the%20correct%20answer.%20We%20train%20using%20a%0Amodified%20DPO%20loss%20%28Rafailov%20et%20al.%2C%202023%29%20with%20an%20additional%20negative%0Alog-likelihood%20term%2C%20which%20we%20find%20to%20be%20crucial.%20We%20show%20reasoning%20improves%0Aacross%20repeated%20iterations%20of%20this%20scheme.%20While%20only%20relying%20on%20examples%20in%0Athe%20training%20set%2C%20our%20approach%20results%20in%20increasing%20accuracy%20on%20GSM8K%2C%20MATH%2C%0Aand%20ARC-Challenge%20for%20Llama-2-70B-Chat%2C%20outperforming%20other%20Llama-2-based%0Amodels%20not%20relying%20on%20additionally%20sourced%20datasets.%20For%20example%2C%20we%20see%20a%0Alarge%20improvement%20from%2055.6%25%20to%2081.6%25%20on%20GSM8K%20and%20an%20accuracy%20of%2088.7%25%20with%0Amajority%20voting%20out%20of%2032%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19733v2&entry.124074799=Read"},
{"title": "PoseINN: Realtime Visual-based Pose Regression and Localization with\n  Invertible Neural Networks", "author": "Zirui Zang and Ahmad Amine and Rahul Mangharam", "abstract": "  Estimating ego-pose from cameras is an important problem in robotics with\napplications ranging from mobile robotics to augmented reality. While SOTA\nmodels are becoming increasingly accurate, they can still be unwieldy due to\nhigh computational costs. In this paper, we propose to solve the problem by\nusing invertible neural networks (INN) to find the mapping between the latent\nspace of images and poses for a given scene. Our model achieves similar\nperformance to the SOTA while being faster to train and only requiring offline\nrendering of low-resolution synthetic data. By using normalizing flows, the\nproposed method also provides uncertainty estimation for the output. We also\ndemonstrated the efficiency of this method by deploying the model on a mobile\nrobot.\n", "link": "http://arxiv.org/abs/2404.13288v3", "date": "2024-05-07", "relevancy": 2.2245, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5706}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5673}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoseINN%3A%20Realtime%20Visual-based%20Pose%20Regression%20and%20Localization%20with%0A%20%20Invertible%20Neural%20Networks&body=Title%3A%20PoseINN%3A%20Realtime%20Visual-based%20Pose%20Regression%20and%20Localization%20with%0A%20%20Invertible%20Neural%20Networks%0AAuthor%3A%20Zirui%20Zang%20and%20Ahmad%20Amine%20and%20Rahul%20Mangharam%0AAbstract%3A%20%20%20Estimating%20ego-pose%20from%20cameras%20is%20an%20important%20problem%20in%20robotics%20with%0Aapplications%20ranging%20from%20mobile%20robotics%20to%20augmented%20reality.%20While%20SOTA%0Amodels%20are%20becoming%20increasingly%20accurate%2C%20they%20can%20still%20be%20unwieldy%20due%20to%0Ahigh%20computational%20costs.%20In%20this%20paper%2C%20we%20propose%20to%20solve%20the%20problem%20by%0Ausing%20invertible%20neural%20networks%20%28INN%29%20to%20find%20the%20mapping%20between%20the%20latent%0Aspace%20of%20images%20and%20poses%20for%20a%20given%20scene.%20Our%20model%20achieves%20similar%0Aperformance%20to%20the%20SOTA%20while%20being%20faster%20to%20train%20and%20only%20requiring%20offline%0Arendering%20of%20low-resolution%20synthetic%20data.%20By%20using%20normalizing%20flows%2C%20the%0Aproposed%20method%20also%20provides%20uncertainty%20estimation%20for%20the%20output.%20We%20also%0Ademonstrated%20the%20efficiency%20of%20this%20method%20by%20deploying%20the%20model%20on%20a%20mobile%0Arobot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13288v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoseINN%253A%2520Realtime%2520Visual-based%2520Pose%2520Regression%2520and%2520Localization%2520with%250A%2520%2520Invertible%2520Neural%2520Networks%26entry.906535625%3DZirui%2520Zang%2520and%2520Ahmad%2520Amine%2520and%2520Rahul%2520Mangharam%26entry.1292438233%3D%2520%2520Estimating%2520ego-pose%2520from%2520cameras%2520is%2520an%2520important%2520problem%2520in%2520robotics%2520with%250Aapplications%2520ranging%2520from%2520mobile%2520robotics%2520to%2520augmented%2520reality.%2520While%2520SOTA%250Amodels%2520are%2520becoming%2520increasingly%2520accurate%252C%2520they%2520can%2520still%2520be%2520unwieldy%2520due%2520to%250Ahigh%2520computational%2520costs.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520solve%2520the%2520problem%2520by%250Ausing%2520invertible%2520neural%2520networks%2520%2528INN%2529%2520to%2520find%2520the%2520mapping%2520between%2520the%2520latent%250Aspace%2520of%2520images%2520and%2520poses%2520for%2520a%2520given%2520scene.%2520Our%2520model%2520achieves%2520similar%250Aperformance%2520to%2520the%2520SOTA%2520while%2520being%2520faster%2520to%2520train%2520and%2520only%2520requiring%2520offline%250Arendering%2520of%2520low-resolution%2520synthetic%2520data.%2520By%2520using%2520normalizing%2520flows%252C%2520the%250Aproposed%2520method%2520also%2520provides%2520uncertainty%2520estimation%2520for%2520the%2520output.%2520We%2520also%250Ademonstrated%2520the%2520efficiency%2520of%2520this%2520method%2520by%2520deploying%2520the%2520model%2520on%2520a%2520mobile%250Arobot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.13288v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoseINN%3A%20Realtime%20Visual-based%20Pose%20Regression%20and%20Localization%20with%0A%20%20Invertible%20Neural%20Networks&entry.906535625=Zirui%20Zang%20and%20Ahmad%20Amine%20and%20Rahul%20Mangharam&entry.1292438233=%20%20Estimating%20ego-pose%20from%20cameras%20is%20an%20important%20problem%20in%20robotics%20with%0Aapplications%20ranging%20from%20mobile%20robotics%20to%20augmented%20reality.%20While%20SOTA%0Amodels%20are%20becoming%20increasingly%20accurate%2C%20they%20can%20still%20be%20unwieldy%20due%20to%0Ahigh%20computational%20costs.%20In%20this%20paper%2C%20we%20propose%20to%20solve%20the%20problem%20by%0Ausing%20invertible%20neural%20networks%20%28INN%29%20to%20find%20the%20mapping%20between%20the%20latent%0Aspace%20of%20images%20and%20poses%20for%20a%20given%20scene.%20Our%20model%20achieves%20similar%0Aperformance%20to%20the%20SOTA%20while%20being%20faster%20to%20train%20and%20only%20requiring%20offline%0Arendering%20of%20low-resolution%20synthetic%20data.%20By%20using%20normalizing%20flows%2C%20the%0Aproposed%20method%20also%20provides%20uncertainty%20estimation%20for%20the%20output.%20We%20also%0Ademonstrated%20the%20efficiency%20of%20this%20method%20by%20deploying%20the%20model%20on%20a%20mobile%0Arobot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13288v3&entry.124074799=Read"},
{"title": "CascadedGaze: Efficiency in Global Context Extraction for Image\n  Restoration", "author": "Amirhosein Ghasemabadi and Muhammad Kamran Janjua and Mohammad Salameh and Chunhua Zhou and Fengyu Sun and Di Niu", "abstract": "  Image restoration tasks traditionally rely on convolutional neural networks.\nHowever, given the local nature of the convolutional operator, they struggle to\ncapture global information. The promise of attention mechanisms in Transformers\nis to circumvent this problem, but it comes at the cost of intensive\ncomputational overhead. Many recent studies in image restoration have focused\non solving the challenge of balancing performance and computational cost via\nTransformer variants. In this paper, we present CascadedGaze Network (CGNet),\nan encoder-decoder architecture that employs Global Context Extractor (GCE), a\nnovel and efficient way to capture global information for image restoration.\nThe GCE module leverages small kernels across convolutional layers to learn\nglobal dependencies, without requiring self-attention. Extensive experimental\nresults show that our computationally efficient approach performs competitively\nto a range of state-of-the-art methods on synthetic image denoising and single\nimage deblurring tasks, and pushes the performance boundary further on the real\nimage denoising task.\n", "link": "http://arxiv.org/abs/2401.15235v2", "date": "2024-05-07", "relevancy": 2.2092, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5704}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5608}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CascadedGaze%3A%20Efficiency%20in%20Global%20Context%20Extraction%20for%20Image%0A%20%20Restoration&body=Title%3A%20CascadedGaze%3A%20Efficiency%20in%20Global%20Context%20Extraction%20for%20Image%0A%20%20Restoration%0AAuthor%3A%20Amirhosein%20Ghasemabadi%20and%20Muhammad%20Kamran%20Janjua%20and%20Mohammad%20Salameh%20and%20Chunhua%20Zhou%20and%20Fengyu%20Sun%20and%20Di%20Niu%0AAbstract%3A%20%20%20Image%20restoration%20tasks%20traditionally%20rely%20on%20convolutional%20neural%20networks.%0AHowever%2C%20given%20the%20local%20nature%20of%20the%20convolutional%20operator%2C%20they%20struggle%20to%0Acapture%20global%20information.%20The%20promise%20of%20attention%20mechanisms%20in%20Transformers%0Ais%20to%20circumvent%20this%20problem%2C%20but%20it%20comes%20at%20the%20cost%20of%20intensive%0Acomputational%20overhead.%20Many%20recent%20studies%20in%20image%20restoration%20have%20focused%0Aon%20solving%20the%20challenge%20of%20balancing%20performance%20and%20computational%20cost%20via%0ATransformer%20variants.%20In%20this%20paper%2C%20we%20present%20CascadedGaze%20Network%20%28CGNet%29%2C%0Aan%20encoder-decoder%20architecture%20that%20employs%20Global%20Context%20Extractor%20%28GCE%29%2C%20a%0Anovel%20and%20efficient%20way%20to%20capture%20global%20information%20for%20image%20restoration.%0AThe%20GCE%20module%20leverages%20small%20kernels%20across%20convolutional%20layers%20to%20learn%0Aglobal%20dependencies%2C%20without%20requiring%20self-attention.%20Extensive%20experimental%0Aresults%20show%20that%20our%20computationally%20efficient%20approach%20performs%20competitively%0Ato%20a%20range%20of%20state-of-the-art%20methods%20on%20synthetic%20image%20denoising%20and%20single%0Aimage%20deblurring%20tasks%2C%20and%20pushes%20the%20performance%20boundary%20further%20on%20the%20real%0Aimage%20denoising%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.15235v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCascadedGaze%253A%2520Efficiency%2520in%2520Global%2520Context%2520Extraction%2520for%2520Image%250A%2520%2520Restoration%26entry.906535625%3DAmirhosein%2520Ghasemabadi%2520and%2520Muhammad%2520Kamran%2520Janjua%2520and%2520Mohammad%2520Salameh%2520and%2520Chunhua%2520Zhou%2520and%2520Fengyu%2520Sun%2520and%2520Di%2520Niu%26entry.1292438233%3D%2520%2520Image%2520restoration%2520tasks%2520traditionally%2520rely%2520on%2520convolutional%2520neural%2520networks.%250AHowever%252C%2520given%2520the%2520local%2520nature%2520of%2520the%2520convolutional%2520operator%252C%2520they%2520struggle%2520to%250Acapture%2520global%2520information.%2520The%2520promise%2520of%2520attention%2520mechanisms%2520in%2520Transformers%250Ais%2520to%2520circumvent%2520this%2520problem%252C%2520but%2520it%2520comes%2520at%2520the%2520cost%2520of%2520intensive%250Acomputational%2520overhead.%2520Many%2520recent%2520studies%2520in%2520image%2520restoration%2520have%2520focused%250Aon%2520solving%2520the%2520challenge%2520of%2520balancing%2520performance%2520and%2520computational%2520cost%2520via%250ATransformer%2520variants.%2520In%2520this%2520paper%252C%2520we%2520present%2520CascadedGaze%2520Network%2520%2528CGNet%2529%252C%250Aan%2520encoder-decoder%2520architecture%2520that%2520employs%2520Global%2520Context%2520Extractor%2520%2528GCE%2529%252C%2520a%250Anovel%2520and%2520efficient%2520way%2520to%2520capture%2520global%2520information%2520for%2520image%2520restoration.%250AThe%2520GCE%2520module%2520leverages%2520small%2520kernels%2520across%2520convolutional%2520layers%2520to%2520learn%250Aglobal%2520dependencies%252C%2520without%2520requiring%2520self-attention.%2520Extensive%2520experimental%250Aresults%2520show%2520that%2520our%2520computationally%2520efficient%2520approach%2520performs%2520competitively%250Ato%2520a%2520range%2520of%2520state-of-the-art%2520methods%2520on%2520synthetic%2520image%2520denoising%2520and%2520single%250Aimage%2520deblurring%2520tasks%252C%2520and%2520pushes%2520the%2520performance%2520boundary%2520further%2520on%2520the%2520real%250Aimage%2520denoising%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.15235v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CascadedGaze%3A%20Efficiency%20in%20Global%20Context%20Extraction%20for%20Image%0A%20%20Restoration&entry.906535625=Amirhosein%20Ghasemabadi%20and%20Muhammad%20Kamran%20Janjua%20and%20Mohammad%20Salameh%20and%20Chunhua%20Zhou%20and%20Fengyu%20Sun%20and%20Di%20Niu&entry.1292438233=%20%20Image%20restoration%20tasks%20traditionally%20rely%20on%20convolutional%20neural%20networks.%0AHowever%2C%20given%20the%20local%20nature%20of%20the%20convolutional%20operator%2C%20they%20struggle%20to%0Acapture%20global%20information.%20The%20promise%20of%20attention%20mechanisms%20in%20Transformers%0Ais%20to%20circumvent%20this%20problem%2C%20but%20it%20comes%20at%20the%20cost%20of%20intensive%0Acomputational%20overhead.%20Many%20recent%20studies%20in%20image%20restoration%20have%20focused%0Aon%20solving%20the%20challenge%20of%20balancing%20performance%20and%20computational%20cost%20via%0ATransformer%20variants.%20In%20this%20paper%2C%20we%20present%20CascadedGaze%20Network%20%28CGNet%29%2C%0Aan%20encoder-decoder%20architecture%20that%20employs%20Global%20Context%20Extractor%20%28GCE%29%2C%20a%0Anovel%20and%20efficient%20way%20to%20capture%20global%20information%20for%20image%20restoration.%0AThe%20GCE%20module%20leverages%20small%20kernels%20across%20convolutional%20layers%20to%20learn%0Aglobal%20dependencies%2C%20without%20requiring%20self-attention.%20Extensive%20experimental%0Aresults%20show%20that%20our%20computationally%20efficient%20approach%20performs%20competitively%0Ato%20a%20range%20of%20state-of-the-art%20methods%20on%20synthetic%20image%20denoising%20and%20single%0Aimage%20deblurring%20tasks%2C%20and%20pushes%20the%20performance%20boundary%20further%20on%20the%20real%0Aimage%20denoising%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15235v2&entry.124074799=Read"},
{"title": "Audio-Visual Speech Representation Expert for Enhanced Talking Face\n  Video Generation and Evaluation", "author": "Dogucan Yaman and Fevziye Irem Eyiokur and Leonard B\u00e4rmann and Seymanur Akt\u0131 and Haz\u0131m Kemal Ekenel and Alexander Waibel", "abstract": "  In the task of talking face generation, the objective is to generate a face\nvideo with lips synchronized to the corresponding audio while preserving visual\ndetails and identity information. Current methods face the challenge of\nlearning accurate lip synchronization while avoiding detrimental effects on\nvisual quality, as well as robustly evaluating such synchronization. To tackle\nthese problems, we propose utilizing an audio-visual speech representation\nexpert (AV-HuBERT) for calculating lip synchronization loss during training.\nMoreover, leveraging AV-HuBERT's features, we introduce three novel lip\nsynchronization evaluation metrics, aiming to provide a comprehensive\nassessment of lip synchronization performance. Experimental results, along with\na detailed ablation study, demonstrate the effectiveness of our approach and\nthe utility of the proposed evaluation metrics.\n", "link": "http://arxiv.org/abs/2405.04327v1", "date": "2024-05-07", "relevancy": 2.2036, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5587}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5516}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio-Visual%20Speech%20Representation%20Expert%20for%20Enhanced%20Talking%20Face%0A%20%20Video%20Generation%20and%20Evaluation&body=Title%3A%20Audio-Visual%20Speech%20Representation%20Expert%20for%20Enhanced%20Talking%20Face%0A%20%20Video%20Generation%20and%20Evaluation%0AAuthor%3A%20Dogucan%20Yaman%20and%20Fevziye%20Irem%20Eyiokur%20and%20Leonard%20B%C3%A4rmann%20and%20Seymanur%20Akt%C4%B1%20and%20Haz%C4%B1m%20Kemal%20Ekenel%20and%20Alexander%20Waibel%0AAbstract%3A%20%20%20In%20the%20task%20of%20talking%20face%20generation%2C%20the%20objective%20is%20to%20generate%20a%20face%0Avideo%20with%20lips%20synchronized%20to%20the%20corresponding%20audio%20while%20preserving%20visual%0Adetails%20and%20identity%20information.%20Current%20methods%20face%20the%20challenge%20of%0Alearning%20accurate%20lip%20synchronization%20while%20avoiding%20detrimental%20effects%20on%0Avisual%20quality%2C%20as%20well%20as%20robustly%20evaluating%20such%20synchronization.%20To%20tackle%0Athese%20problems%2C%20we%20propose%20utilizing%20an%20audio-visual%20speech%20representation%0Aexpert%20%28AV-HuBERT%29%20for%20calculating%20lip%20synchronization%20loss%20during%20training.%0AMoreover%2C%20leveraging%20AV-HuBERT%27s%20features%2C%20we%20introduce%20three%20novel%20lip%0Asynchronization%20evaluation%20metrics%2C%20aiming%20to%20provide%20a%20comprehensive%0Aassessment%20of%20lip%20synchronization%20performance.%20Experimental%20results%2C%20along%20with%0Aa%20detailed%20ablation%20study%2C%20demonstrate%20the%20effectiveness%20of%20our%20approach%20and%0Athe%20utility%20of%20the%20proposed%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio-Visual%2520Speech%2520Representation%2520Expert%2520for%2520Enhanced%2520Talking%2520Face%250A%2520%2520Video%2520Generation%2520and%2520Evaluation%26entry.906535625%3DDogucan%2520Yaman%2520and%2520Fevziye%2520Irem%2520Eyiokur%2520and%2520Leonard%2520B%25C3%25A4rmann%2520and%2520Seymanur%2520Akt%25C4%25B1%2520and%2520Haz%25C4%25B1m%2520Kemal%2520Ekenel%2520and%2520Alexander%2520Waibel%26entry.1292438233%3D%2520%2520In%2520the%2520task%2520of%2520talking%2520face%2520generation%252C%2520the%2520objective%2520is%2520to%2520generate%2520a%2520face%250Avideo%2520with%2520lips%2520synchronized%2520to%2520the%2520corresponding%2520audio%2520while%2520preserving%2520visual%250Adetails%2520and%2520identity%2520information.%2520Current%2520methods%2520face%2520the%2520challenge%2520of%250Alearning%2520accurate%2520lip%2520synchronization%2520while%2520avoiding%2520detrimental%2520effects%2520on%250Avisual%2520quality%252C%2520as%2520well%2520as%2520robustly%2520evaluating%2520such%2520synchronization.%2520To%2520tackle%250Athese%2520problems%252C%2520we%2520propose%2520utilizing%2520an%2520audio-visual%2520speech%2520representation%250Aexpert%2520%2528AV-HuBERT%2529%2520for%2520calculating%2520lip%2520synchronization%2520loss%2520during%2520training.%250AMoreover%252C%2520leveraging%2520AV-HuBERT%2527s%2520features%252C%2520we%2520introduce%2520three%2520novel%2520lip%250Asynchronization%2520evaluation%2520metrics%252C%2520aiming%2520to%2520provide%2520a%2520comprehensive%250Aassessment%2520of%2520lip%2520synchronization%2520performance.%2520Experimental%2520results%252C%2520along%2520with%250Aa%2520detailed%2520ablation%2520study%252C%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520and%250Athe%2520utility%2520of%2520the%2520proposed%2520evaluation%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio-Visual%20Speech%20Representation%20Expert%20for%20Enhanced%20Talking%20Face%0A%20%20Video%20Generation%20and%20Evaluation&entry.906535625=Dogucan%20Yaman%20and%20Fevziye%20Irem%20Eyiokur%20and%20Leonard%20B%C3%A4rmann%20and%20Seymanur%20Akt%C4%B1%20and%20Haz%C4%B1m%20Kemal%20Ekenel%20and%20Alexander%20Waibel&entry.1292438233=%20%20In%20the%20task%20of%20talking%20face%20generation%2C%20the%20objective%20is%20to%20generate%20a%20face%0Avideo%20with%20lips%20synchronized%20to%20the%20corresponding%20audio%20while%20preserving%20visual%0Adetails%20and%20identity%20information.%20Current%20methods%20face%20the%20challenge%20of%0Alearning%20accurate%20lip%20synchronization%20while%20avoiding%20detrimental%20effects%20on%0Avisual%20quality%2C%20as%20well%20as%20robustly%20evaluating%20such%20synchronization.%20To%20tackle%0Athese%20problems%2C%20we%20propose%20utilizing%20an%20audio-visual%20speech%20representation%0Aexpert%20%28AV-HuBERT%29%20for%20calculating%20lip%20synchronization%20loss%20during%20training.%0AMoreover%2C%20leveraging%20AV-HuBERT%27s%20features%2C%20we%20introduce%20three%20novel%20lip%0Asynchronization%20evaluation%20metrics%2C%20aiming%20to%20provide%20a%20comprehensive%0Aassessment%20of%20lip%20synchronization%20performance.%20Experimental%20results%2C%20along%20with%0Aa%20detailed%20ablation%20study%2C%20demonstrate%20the%20effectiveness%20of%20our%20approach%20and%0Athe%20utility%20of%20the%20proposed%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04327v1&entry.124074799=Read"},
{"title": "FedStale: leveraging stale client updates in federated learning", "author": "Angelo Rodio and Giovanni Neglia", "abstract": "  Federated learning algorithms, such as FedAvg, are negatively affected by\ndata heterogeneity and partial client participation. To mitigate the latter\nproblem, global variance reduction methods, like FedVARP, leverage stale model\nupdates for non-participating clients. These methods are effective under\nhomogeneous client participation. Yet, this paper shows that, when some clients\nparticipate much less than others, aggregating updates with different levels of\nstaleness can detrimentally affect the training process. Motivated by this\nobservation, we introduce FedStale, a novel algorithm that updates the global\nmodel in each round through a convex combination of \"fresh\" updates from\nparticipating clients and \"stale\" updates from non-participating ones. By\nadjusting the weight in the convex combination, FedStale interpolates between\nFedAvg, which only uses fresh updates, and FedVARP, which treats fresh and\nstale updates equally. Our analysis of FedStale convergence yields the\nfollowing novel findings: i) it integrates and extends previous FedAvg and\nFedVARP analyses to heterogeneous client participation; ii) it underscores how\nthe least participating client influences convergence error; iii) it provides\npractical guidelines to best exploit stale updates, showing that their\nusefulness diminishes as data heterogeneity decreases and participation\nheterogeneity increases. Extensive experiments featuring diverse levels of\nclient data and participation heterogeneity not only confirm these findings but\nalso show that FedStale outperforms both FedAvg and FedVARP in many settings.\n", "link": "http://arxiv.org/abs/2405.04171v1", "date": "2024-05-07", "relevancy": 2.1974, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4584}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4386}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedStale%3A%20leveraging%20stale%20client%20updates%20in%20federated%20learning&body=Title%3A%20FedStale%3A%20leveraging%20stale%20client%20updates%20in%20federated%20learning%0AAuthor%3A%20Angelo%20Rodio%20and%20Giovanni%20Neglia%0AAbstract%3A%20%20%20Federated%20learning%20algorithms%2C%20such%20as%20FedAvg%2C%20are%20negatively%20affected%20by%0Adata%20heterogeneity%20and%20partial%20client%20participation.%20To%20mitigate%20the%20latter%0Aproblem%2C%20global%20variance%20reduction%20methods%2C%20like%20FedVARP%2C%20leverage%20stale%20model%0Aupdates%20for%20non-participating%20clients.%20These%20methods%20are%20effective%20under%0Ahomogeneous%20client%20participation.%20Yet%2C%20this%20paper%20shows%20that%2C%20when%20some%20clients%0Aparticipate%20much%20less%20than%20others%2C%20aggregating%20updates%20with%20different%20levels%20of%0Astaleness%20can%20detrimentally%20affect%20the%20training%20process.%20Motivated%20by%20this%0Aobservation%2C%20we%20introduce%20FedStale%2C%20a%20novel%20algorithm%20that%20updates%20the%20global%0Amodel%20in%20each%20round%20through%20a%20convex%20combination%20of%20%22fresh%22%20updates%20from%0Aparticipating%20clients%20and%20%22stale%22%20updates%20from%20non-participating%20ones.%20By%0Aadjusting%20the%20weight%20in%20the%20convex%20combination%2C%20FedStale%20interpolates%20between%0AFedAvg%2C%20which%20only%20uses%20fresh%20updates%2C%20and%20FedVARP%2C%20which%20treats%20fresh%20and%0Astale%20updates%20equally.%20Our%20analysis%20of%20FedStale%20convergence%20yields%20the%0Afollowing%20novel%20findings%3A%20i%29%20it%20integrates%20and%20extends%20previous%20FedAvg%20and%0AFedVARP%20analyses%20to%20heterogeneous%20client%20participation%3B%20ii%29%20it%20underscores%20how%0Athe%20least%20participating%20client%20influences%20convergence%20error%3B%20iii%29%20it%20provides%0Apractical%20guidelines%20to%20best%20exploit%20stale%20updates%2C%20showing%20that%20their%0Ausefulness%20diminishes%20as%20data%20heterogeneity%20decreases%20and%20participation%0Aheterogeneity%20increases.%20Extensive%20experiments%20featuring%20diverse%20levels%20of%0Aclient%20data%20and%20participation%20heterogeneity%20not%20only%20confirm%20these%20findings%20but%0Aalso%20show%20that%20FedStale%20outperforms%20both%20FedAvg%20and%20FedVARP%20in%20many%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedStale%253A%2520leveraging%2520stale%2520client%2520updates%2520in%2520federated%2520learning%26entry.906535625%3DAngelo%2520Rodio%2520and%2520Giovanni%2520Neglia%26entry.1292438233%3D%2520%2520Federated%2520learning%2520algorithms%252C%2520such%2520as%2520FedAvg%252C%2520are%2520negatively%2520affected%2520by%250Adata%2520heterogeneity%2520and%2520partial%2520client%2520participation.%2520To%2520mitigate%2520the%2520latter%250Aproblem%252C%2520global%2520variance%2520reduction%2520methods%252C%2520like%2520FedVARP%252C%2520leverage%2520stale%2520model%250Aupdates%2520for%2520non-participating%2520clients.%2520These%2520methods%2520are%2520effective%2520under%250Ahomogeneous%2520client%2520participation.%2520Yet%252C%2520this%2520paper%2520shows%2520that%252C%2520when%2520some%2520clients%250Aparticipate%2520much%2520less%2520than%2520others%252C%2520aggregating%2520updates%2520with%2520different%2520levels%2520of%250Astaleness%2520can%2520detrimentally%2520affect%2520the%2520training%2520process.%2520Motivated%2520by%2520this%250Aobservation%252C%2520we%2520introduce%2520FedStale%252C%2520a%2520novel%2520algorithm%2520that%2520updates%2520the%2520global%250Amodel%2520in%2520each%2520round%2520through%2520a%2520convex%2520combination%2520of%2520%2522fresh%2522%2520updates%2520from%250Aparticipating%2520clients%2520and%2520%2522stale%2522%2520updates%2520from%2520non-participating%2520ones.%2520By%250Aadjusting%2520the%2520weight%2520in%2520the%2520convex%2520combination%252C%2520FedStale%2520interpolates%2520between%250AFedAvg%252C%2520which%2520only%2520uses%2520fresh%2520updates%252C%2520and%2520FedVARP%252C%2520which%2520treats%2520fresh%2520and%250Astale%2520updates%2520equally.%2520Our%2520analysis%2520of%2520FedStale%2520convergence%2520yields%2520the%250Afollowing%2520novel%2520findings%253A%2520i%2529%2520it%2520integrates%2520and%2520extends%2520previous%2520FedAvg%2520and%250AFedVARP%2520analyses%2520to%2520heterogeneous%2520client%2520participation%253B%2520ii%2529%2520it%2520underscores%2520how%250Athe%2520least%2520participating%2520client%2520influences%2520convergence%2520error%253B%2520iii%2529%2520it%2520provides%250Apractical%2520guidelines%2520to%2520best%2520exploit%2520stale%2520updates%252C%2520showing%2520that%2520their%250Ausefulness%2520diminishes%2520as%2520data%2520heterogeneity%2520decreases%2520and%2520participation%250Aheterogeneity%2520increases.%2520Extensive%2520experiments%2520featuring%2520diverse%2520levels%2520of%250Aclient%2520data%2520and%2520participation%2520heterogeneity%2520not%2520only%2520confirm%2520these%2520findings%2520but%250Aalso%2520show%2520that%2520FedStale%2520outperforms%2520both%2520FedAvg%2520and%2520FedVARP%2520in%2520many%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedStale%3A%20leveraging%20stale%20client%20updates%20in%20federated%20learning&entry.906535625=Angelo%20Rodio%20and%20Giovanni%20Neglia&entry.1292438233=%20%20Federated%20learning%20algorithms%2C%20such%20as%20FedAvg%2C%20are%20negatively%20affected%20by%0Adata%20heterogeneity%20and%20partial%20client%20participation.%20To%20mitigate%20the%20latter%0Aproblem%2C%20global%20variance%20reduction%20methods%2C%20like%20FedVARP%2C%20leverage%20stale%20model%0Aupdates%20for%20non-participating%20clients.%20These%20methods%20are%20effective%20under%0Ahomogeneous%20client%20participation.%20Yet%2C%20this%20paper%20shows%20that%2C%20when%20some%20clients%0Aparticipate%20much%20less%20than%20others%2C%20aggregating%20updates%20with%20different%20levels%20of%0Astaleness%20can%20detrimentally%20affect%20the%20training%20process.%20Motivated%20by%20this%0Aobservation%2C%20we%20introduce%20FedStale%2C%20a%20novel%20algorithm%20that%20updates%20the%20global%0Amodel%20in%20each%20round%20through%20a%20convex%20combination%20of%20%22fresh%22%20updates%20from%0Aparticipating%20clients%20and%20%22stale%22%20updates%20from%20non-participating%20ones.%20By%0Aadjusting%20the%20weight%20in%20the%20convex%20combination%2C%20FedStale%20interpolates%20between%0AFedAvg%2C%20which%20only%20uses%20fresh%20updates%2C%20and%20FedVARP%2C%20which%20treats%20fresh%20and%0Astale%20updates%20equally.%20Our%20analysis%20of%20FedStale%20convergence%20yields%20the%0Afollowing%20novel%20findings%3A%20i%29%20it%20integrates%20and%20extends%20previous%20FedAvg%20and%0AFedVARP%20analyses%20to%20heterogeneous%20client%20participation%3B%20ii%29%20it%20underscores%20how%0Athe%20least%20participating%20client%20influences%20convergence%20error%3B%20iii%29%20it%20provides%0Apractical%20guidelines%20to%20best%20exploit%20stale%20updates%2C%20showing%20that%20their%0Ausefulness%20diminishes%20as%20data%20heterogeneity%20decreases%20and%20participation%0Aheterogeneity%20increases.%20Extensive%20experiments%20featuring%20diverse%20levels%20of%0Aclient%20data%20and%20participation%20heterogeneity%20not%20only%20confirm%20these%20findings%20but%0Aalso%20show%20that%20FedStale%20outperforms%20both%20FedAvg%20and%20FedVARP%20in%20many%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04171v1&entry.124074799=Read"},
{"title": "AccidentBlip2: Accident Detection With Multi-View MotionBlip2", "author": "Yihua Shao and Hongyi Cai and Xinwei Long and Weiyi Lang and Zhe Wang and Haoran Wu and Yan Wang and Jiayi Yin and Yang Yang and Yisheng Lv and Zhen Lei", "abstract": "  Intelligent vehicles have demonstrated excellent capabilities in many\ntransportation scenarios. The inference capabilities of neural networks using\ncameras limit the accuracy of accident detection in complex transportation\nsystems. This paper presents AccidentBlip2, a pure vision-based multi-modal\nlarge model Blip2 for accident detection. Our method first processes the\nmulti-view images through ViT-14g and sends the multi-view features into the\ncross-attention layer of Q-Former. Different from Blip2's Q-Former, our Motion\nQ-Former extends the self-attention layer with the temporal-attention layer. In\nthe inference process, the queries generated from previous frames are input\ninto Motion Q-Former to aggregate temporal information. Queries are updated\nwith an auto-regressive strategy and are sent to a MLP to detect whether there\nis an accident in the surrounding environment. Our AccidentBlip2 can be\nextended to a multi-vehicle cooperative system by deploying Motion Q-Former on\neach vehicle and simultaneously fusing the generated queries into the MLP for\nauto-regressive inference. Our approach outperforms existing video large\nlanguage models in detection accuracy in both single-vehicle and multi-vehicle\nsystems.\n", "link": "http://arxiv.org/abs/2404.12149v4", "date": "2024-05-07", "relevancy": 2.1807, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5771}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5452}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AccidentBlip2%3A%20Accident%20Detection%20With%20Multi-View%20MotionBlip2&body=Title%3A%20AccidentBlip2%3A%20Accident%20Detection%20With%20Multi-View%20MotionBlip2%0AAuthor%3A%20Yihua%20Shao%20and%20Hongyi%20Cai%20and%20Xinwei%20Long%20and%20Weiyi%20Lang%20and%20Zhe%20Wang%20and%20Haoran%20Wu%20and%20Yan%20Wang%20and%20Jiayi%20Yin%20and%20Yang%20Yang%20and%20Yisheng%20Lv%20and%20Zhen%20Lei%0AAbstract%3A%20%20%20Intelligent%20vehicles%20have%20demonstrated%20excellent%20capabilities%20in%20many%0Atransportation%20scenarios.%20The%20inference%20capabilities%20of%20neural%20networks%20using%0Acameras%20limit%20the%20accuracy%20of%20accident%20detection%20in%20complex%20transportation%0Asystems.%20This%20paper%20presents%20AccidentBlip2%2C%20a%20pure%20vision-based%20multi-modal%0Alarge%20model%20Blip2%20for%20accident%20detection.%20Our%20method%20first%20processes%20the%0Amulti-view%20images%20through%20ViT-14g%20and%20sends%20the%20multi-view%20features%20into%20the%0Across-attention%20layer%20of%20Q-Former.%20Different%20from%20Blip2%27s%20Q-Former%2C%20our%20Motion%0AQ-Former%20extends%20the%20self-attention%20layer%20with%20the%20temporal-attention%20layer.%20In%0Athe%20inference%20process%2C%20the%20queries%20generated%20from%20previous%20frames%20are%20input%0Ainto%20Motion%20Q-Former%20to%20aggregate%20temporal%20information.%20Queries%20are%20updated%0Awith%20an%20auto-regressive%20strategy%20and%20are%20sent%20to%20a%20MLP%20to%20detect%20whether%20there%0Ais%20an%20accident%20in%20the%20surrounding%20environment.%20Our%20AccidentBlip2%20can%20be%0Aextended%20to%20a%20multi-vehicle%20cooperative%20system%20by%20deploying%20Motion%20Q-Former%20on%0Aeach%20vehicle%20and%20simultaneously%20fusing%20the%20generated%20queries%20into%20the%20MLP%20for%0Aauto-regressive%20inference.%20Our%20approach%20outperforms%20existing%20video%20large%0Alanguage%20models%20in%20detection%20accuracy%20in%20both%20single-vehicle%20and%20multi-vehicle%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12149v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccidentBlip2%253A%2520Accident%2520Detection%2520With%2520Multi-View%2520MotionBlip2%26entry.906535625%3DYihua%2520Shao%2520and%2520Hongyi%2520Cai%2520and%2520Xinwei%2520Long%2520and%2520Weiyi%2520Lang%2520and%2520Zhe%2520Wang%2520and%2520Haoran%2520Wu%2520and%2520Yan%2520Wang%2520and%2520Jiayi%2520Yin%2520and%2520Yang%2520Yang%2520and%2520Yisheng%2520Lv%2520and%2520Zhen%2520Lei%26entry.1292438233%3D%2520%2520Intelligent%2520vehicles%2520have%2520demonstrated%2520excellent%2520capabilities%2520in%2520many%250Atransportation%2520scenarios.%2520The%2520inference%2520capabilities%2520of%2520neural%2520networks%2520using%250Acameras%2520limit%2520the%2520accuracy%2520of%2520accident%2520detection%2520in%2520complex%2520transportation%250Asystems.%2520This%2520paper%2520presents%2520AccidentBlip2%252C%2520a%2520pure%2520vision-based%2520multi-modal%250Alarge%2520model%2520Blip2%2520for%2520accident%2520detection.%2520Our%2520method%2520first%2520processes%2520the%250Amulti-view%2520images%2520through%2520ViT-14g%2520and%2520sends%2520the%2520multi-view%2520features%2520into%2520the%250Across-attention%2520layer%2520of%2520Q-Former.%2520Different%2520from%2520Blip2%2527s%2520Q-Former%252C%2520our%2520Motion%250AQ-Former%2520extends%2520the%2520self-attention%2520layer%2520with%2520the%2520temporal-attention%2520layer.%2520In%250Athe%2520inference%2520process%252C%2520the%2520queries%2520generated%2520from%2520previous%2520frames%2520are%2520input%250Ainto%2520Motion%2520Q-Former%2520to%2520aggregate%2520temporal%2520information.%2520Queries%2520are%2520updated%250Awith%2520an%2520auto-regressive%2520strategy%2520and%2520are%2520sent%2520to%2520a%2520MLP%2520to%2520detect%2520whether%2520there%250Ais%2520an%2520accident%2520in%2520the%2520surrounding%2520environment.%2520Our%2520AccidentBlip2%2520can%2520be%250Aextended%2520to%2520a%2520multi-vehicle%2520cooperative%2520system%2520by%2520deploying%2520Motion%2520Q-Former%2520on%250Aeach%2520vehicle%2520and%2520simultaneously%2520fusing%2520the%2520generated%2520queries%2520into%2520the%2520MLP%2520for%250Aauto-regressive%2520inference.%2520Our%2520approach%2520outperforms%2520existing%2520video%2520large%250Alanguage%2520models%2520in%2520detection%2520accuracy%2520in%2520both%2520single-vehicle%2520and%2520multi-vehicle%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.12149v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AccidentBlip2%3A%20Accident%20Detection%20With%20Multi-View%20MotionBlip2&entry.906535625=Yihua%20Shao%20and%20Hongyi%20Cai%20and%20Xinwei%20Long%20and%20Weiyi%20Lang%20and%20Zhe%20Wang%20and%20Haoran%20Wu%20and%20Yan%20Wang%20and%20Jiayi%20Yin%20and%20Yang%20Yang%20and%20Yisheng%20Lv%20and%20Zhen%20Lei&entry.1292438233=%20%20Intelligent%20vehicles%20have%20demonstrated%20excellent%20capabilities%20in%20many%0Atransportation%20scenarios.%20The%20inference%20capabilities%20of%20neural%20networks%20using%0Acameras%20limit%20the%20accuracy%20of%20accident%20detection%20in%20complex%20transportation%0Asystems.%20This%20paper%20presents%20AccidentBlip2%2C%20a%20pure%20vision-based%20multi-modal%0Alarge%20model%20Blip2%20for%20accident%20detection.%20Our%20method%20first%20processes%20the%0Amulti-view%20images%20through%20ViT-14g%20and%20sends%20the%20multi-view%20features%20into%20the%0Across-attention%20layer%20of%20Q-Former.%20Different%20from%20Blip2%27s%20Q-Former%2C%20our%20Motion%0AQ-Former%20extends%20the%20self-attention%20layer%20with%20the%20temporal-attention%20layer.%20In%0Athe%20inference%20process%2C%20the%20queries%20generated%20from%20previous%20frames%20are%20input%0Ainto%20Motion%20Q-Former%20to%20aggregate%20temporal%20information.%20Queries%20are%20updated%0Awith%20an%20auto-regressive%20strategy%20and%20are%20sent%20to%20a%20MLP%20to%20detect%20whether%20there%0Ais%20an%20accident%20in%20the%20surrounding%20environment.%20Our%20AccidentBlip2%20can%20be%0Aextended%20to%20a%20multi-vehicle%20cooperative%20system%20by%20deploying%20Motion%20Q-Former%20on%0Aeach%20vehicle%20and%20simultaneously%20fusing%20the%20generated%20queries%20into%20the%20MLP%20for%0Aauto-regressive%20inference.%20Our%20approach%20outperforms%20existing%20video%20large%0Alanguage%20models%20in%20detection%20accuracy%20in%20both%20single-vehicle%20and%20multi-vehicle%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12149v4&entry.124074799=Read"},
{"title": "Tactile-Augmented Radiance Fields", "author": "Yiming Dou and Fengyu Yang and Yi Liu and Antonio Loquercio and Andrew Owens", "abstract": "  We present a scene representation, which we call a tactile-augmented radiance\nfield (TaRF), that brings vision and touch into a shared 3D space. This\nrepresentation can be used to estimate the visual and tactile signals for a\ngiven 3D position within a scene. We capture a scene's TaRF from a collection\nof photos and sparsely sampled touch probes. Our approach makes use of two\ninsights: (i) common vision-based touch sensors are built on ordinary cameras\nand thus can be registered to images using methods from multi-view geometry,\nand (ii) visually and structurally similar regions of a scene share the same\ntactile features. We use these insights to register touch signals to a captured\nvisual scene, and to train a conditional diffusion model that, provided with an\nRGB-D image rendered from a neural radiance field, generates its corresponding\ntactile signal. To evaluate our approach, we collect a dataset of TaRFs. This\ndataset contains more touch samples than previous real-world datasets, and it\nprovides spatially aligned visual signals for each captured touch signal. We\ndemonstrate the accuracy of our cross-modal generative model and the utility of\nthe captured visual-tactile data on several downstream tasks. Project page:\nhttps://dou-yiming.github.io/TaRF\n", "link": "http://arxiv.org/abs/2405.04534v1", "date": "2024-05-07", "relevancy": 2.179, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.569}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5318}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tactile-Augmented%20Radiance%20Fields&body=Title%3A%20Tactile-Augmented%20Radiance%20Fields%0AAuthor%3A%20Yiming%20Dou%20and%20Fengyu%20Yang%20and%20Yi%20Liu%20and%20Antonio%20Loquercio%20and%20Andrew%20Owens%0AAbstract%3A%20%20%20We%20present%20a%20scene%20representation%2C%20which%20we%20call%20a%20tactile-augmented%20radiance%0Afield%20%28TaRF%29%2C%20that%20brings%20vision%20and%20touch%20into%20a%20shared%203D%20space.%20This%0Arepresentation%20can%20be%20used%20to%20estimate%20the%20visual%20and%20tactile%20signals%20for%20a%0Agiven%203D%20position%20within%20a%20scene.%20We%20capture%20a%20scene%27s%20TaRF%20from%20a%20collection%0Aof%20photos%20and%20sparsely%20sampled%20touch%20probes.%20Our%20approach%20makes%20use%20of%20two%0Ainsights%3A%20%28i%29%20common%20vision-based%20touch%20sensors%20are%20built%20on%20ordinary%20cameras%0Aand%20thus%20can%20be%20registered%20to%20images%20using%20methods%20from%20multi-view%20geometry%2C%0Aand%20%28ii%29%20visually%20and%20structurally%20similar%20regions%20of%20a%20scene%20share%20the%20same%0Atactile%20features.%20We%20use%20these%20insights%20to%20register%20touch%20signals%20to%20a%20captured%0Avisual%20scene%2C%20and%20to%20train%20a%20conditional%20diffusion%20model%20that%2C%20provided%20with%20an%0ARGB-D%20image%20rendered%20from%20a%20neural%20radiance%20field%2C%20generates%20its%20corresponding%0Atactile%20signal.%20To%20evaluate%20our%20approach%2C%20we%20collect%20a%20dataset%20of%20TaRFs.%20This%0Adataset%20contains%20more%20touch%20samples%20than%20previous%20real-world%20datasets%2C%20and%20it%0Aprovides%20spatially%20aligned%20visual%20signals%20for%20each%20captured%20touch%20signal.%20We%0Ademonstrate%20the%20accuracy%20of%20our%20cross-modal%20generative%20model%20and%20the%20utility%20of%0Athe%20captured%20visual-tactile%20data%20on%20several%20downstream%20tasks.%20Project%20page%3A%0Ahttps%3A//dou-yiming.github.io/TaRF%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04534v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTactile-Augmented%2520Radiance%2520Fields%26entry.906535625%3DYiming%2520Dou%2520and%2520Fengyu%2520Yang%2520and%2520Yi%2520Liu%2520and%2520Antonio%2520Loquercio%2520and%2520Andrew%2520Owens%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520scene%2520representation%252C%2520which%2520we%2520call%2520a%2520tactile-augmented%2520radiance%250Afield%2520%2528TaRF%2529%252C%2520that%2520brings%2520vision%2520and%2520touch%2520into%2520a%2520shared%25203D%2520space.%2520This%250Arepresentation%2520can%2520be%2520used%2520to%2520estimate%2520the%2520visual%2520and%2520tactile%2520signals%2520for%2520a%250Agiven%25203D%2520position%2520within%2520a%2520scene.%2520We%2520capture%2520a%2520scene%2527s%2520TaRF%2520from%2520a%2520collection%250Aof%2520photos%2520and%2520sparsely%2520sampled%2520touch%2520probes.%2520Our%2520approach%2520makes%2520use%2520of%2520two%250Ainsights%253A%2520%2528i%2529%2520common%2520vision-based%2520touch%2520sensors%2520are%2520built%2520on%2520ordinary%2520cameras%250Aand%2520thus%2520can%2520be%2520registered%2520to%2520images%2520using%2520methods%2520from%2520multi-view%2520geometry%252C%250Aand%2520%2528ii%2529%2520visually%2520and%2520structurally%2520similar%2520regions%2520of%2520a%2520scene%2520share%2520the%2520same%250Atactile%2520features.%2520We%2520use%2520these%2520insights%2520to%2520register%2520touch%2520signals%2520to%2520a%2520captured%250Avisual%2520scene%252C%2520and%2520to%2520train%2520a%2520conditional%2520diffusion%2520model%2520that%252C%2520provided%2520with%2520an%250ARGB-D%2520image%2520rendered%2520from%2520a%2520neural%2520radiance%2520field%252C%2520generates%2520its%2520corresponding%250Atactile%2520signal.%2520To%2520evaluate%2520our%2520approach%252C%2520we%2520collect%2520a%2520dataset%2520of%2520TaRFs.%2520This%250Adataset%2520contains%2520more%2520touch%2520samples%2520than%2520previous%2520real-world%2520datasets%252C%2520and%2520it%250Aprovides%2520spatially%2520aligned%2520visual%2520signals%2520for%2520each%2520captured%2520touch%2520signal.%2520We%250Ademonstrate%2520the%2520accuracy%2520of%2520our%2520cross-modal%2520generative%2520model%2520and%2520the%2520utility%2520of%250Athe%2520captured%2520visual-tactile%2520data%2520on%2520several%2520downstream%2520tasks.%2520Project%2520page%253A%250Ahttps%253A//dou-yiming.github.io/TaRF%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04534v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tactile-Augmented%20Radiance%20Fields&entry.906535625=Yiming%20Dou%20and%20Fengyu%20Yang%20and%20Yi%20Liu%20and%20Antonio%20Loquercio%20and%20Andrew%20Owens&entry.1292438233=%20%20We%20present%20a%20scene%20representation%2C%20which%20we%20call%20a%20tactile-augmented%20radiance%0Afield%20%28TaRF%29%2C%20that%20brings%20vision%20and%20touch%20into%20a%20shared%203D%20space.%20This%0Arepresentation%20can%20be%20used%20to%20estimate%20the%20visual%20and%20tactile%20signals%20for%20a%0Agiven%203D%20position%20within%20a%20scene.%20We%20capture%20a%20scene%27s%20TaRF%20from%20a%20collection%0Aof%20photos%20and%20sparsely%20sampled%20touch%20probes.%20Our%20approach%20makes%20use%20of%20two%0Ainsights%3A%20%28i%29%20common%20vision-based%20touch%20sensors%20are%20built%20on%20ordinary%20cameras%0Aand%20thus%20can%20be%20registered%20to%20images%20using%20methods%20from%20multi-view%20geometry%2C%0Aand%20%28ii%29%20visually%20and%20structurally%20similar%20regions%20of%20a%20scene%20share%20the%20same%0Atactile%20features.%20We%20use%20these%20insights%20to%20register%20touch%20signals%20to%20a%20captured%0Avisual%20scene%2C%20and%20to%20train%20a%20conditional%20diffusion%20model%20that%2C%20provided%20with%20an%0ARGB-D%20image%20rendered%20from%20a%20neural%20radiance%20field%2C%20generates%20its%20corresponding%0Atactile%20signal.%20To%20evaluate%20our%20approach%2C%20we%20collect%20a%20dataset%20of%20TaRFs.%20This%0Adataset%20contains%20more%20touch%20samples%20than%20previous%20real-world%20datasets%2C%20and%20it%0Aprovides%20spatially%20aligned%20visual%20signals%20for%20each%20captured%20touch%20signal.%20We%0Ademonstrate%20the%20accuracy%20of%20our%20cross-modal%20generative%20model%20and%20the%20utility%20of%0Athe%20captured%20visual-tactile%20data%20on%20several%20downstream%20tasks.%20Project%20page%3A%0Ahttps%3A//dou-yiming.github.io/TaRF%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04534v1&entry.124074799=Read"},
{"title": "Sora Detector: A Unified Hallucination Detection for Large Text-to-Video\n  Models", "author": "Zhixuan Chu and Lei Zhang and Yichen Sun and Siqiao Xue and Zhibo Wang and Zhan Qin and Kui Ren", "abstract": "  The rapid advancement in text-to-video (T2V) generative models has enabled\nthe synthesis of high-fidelity video content guided by textual descriptions.\nDespite this significant progress, these models are often susceptible to\nhallucination, generating contents that contradict the input text, which poses\na challenge to their reliability and practical deployment. To address this\ncritical issue, we introduce the SoraDetector, a novel unified framework\ndesigned to detect hallucinations across diverse large T2V models, including\nthe cutting-edge Sora model. Our framework is built upon a comprehensive\nanalysis of hallucination phenomena, categorizing them based on their\nmanifestation in the video content. Leveraging the state-of-the-art keyframe\nextraction techniques and multimodal large language models, SoraDetector first\nevaluates the consistency between extracted video content summary and textual\nprompts, then constructs static and dynamic knowledge graphs (KGs) from frames\nto detect hallucination both in single frames and across frames. Sora Detector\nprovides a robust and quantifiable measure of consistency, static and dynamic\nhallucination. In addition, we have developed the Sora Detector Agent to\nautomate the hallucination detection process and generate a complete video\nquality report for each input video. Lastly, we present a novel meta-evaluation\nbenchmark, T2VHaluBench, meticulously crafted to facilitate the evaluation of\nadvancements in T2V hallucination detection. Through extensive experiments on\nvideos generated by Sora and other large T2V models, we demonstrate the\nefficacy of our approach in accurately detecting hallucinations. The code and\ndataset can be accessed via GitHub.\n", "link": "http://arxiv.org/abs/2405.04180v1", "date": "2024-05-07", "relevancy": 2.1714, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.558}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5507}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sora%20Detector%3A%20A%20Unified%20Hallucination%20Detection%20for%20Large%20Text-to-Video%0A%20%20Models&body=Title%3A%20Sora%20Detector%3A%20A%20Unified%20Hallucination%20Detection%20for%20Large%20Text-to-Video%0A%20%20Models%0AAuthor%3A%20Zhixuan%20Chu%20and%20Lei%20Zhang%20and%20Yichen%20Sun%20and%20Siqiao%20Xue%20and%20Zhibo%20Wang%20and%20Zhan%20Qin%20and%20Kui%20Ren%0AAbstract%3A%20%20%20The%20rapid%20advancement%20in%20text-to-video%20%28T2V%29%20generative%20models%20has%20enabled%0Athe%20synthesis%20of%20high-fidelity%20video%20content%20guided%20by%20textual%20descriptions.%0ADespite%20this%20significant%20progress%2C%20these%20models%20are%20often%20susceptible%20to%0Ahallucination%2C%20generating%20contents%20that%20contradict%20the%20input%20text%2C%20which%20poses%0Aa%20challenge%20to%20their%20reliability%20and%20practical%20deployment.%20To%20address%20this%0Acritical%20issue%2C%20we%20introduce%20the%20SoraDetector%2C%20a%20novel%20unified%20framework%0Adesigned%20to%20detect%20hallucinations%20across%20diverse%20large%20T2V%20models%2C%20including%0Athe%20cutting-edge%20Sora%20model.%20Our%20framework%20is%20built%20upon%20a%20comprehensive%0Aanalysis%20of%20hallucination%20phenomena%2C%20categorizing%20them%20based%20on%20their%0Amanifestation%20in%20the%20video%20content.%20Leveraging%20the%20state-of-the-art%20keyframe%0Aextraction%20techniques%20and%20multimodal%20large%20language%20models%2C%20SoraDetector%20first%0Aevaluates%20the%20consistency%20between%20extracted%20video%20content%20summary%20and%20textual%0Aprompts%2C%20then%20constructs%20static%20and%20dynamic%20knowledge%20graphs%20%28KGs%29%20from%20frames%0Ato%20detect%20hallucination%20both%20in%20single%20frames%20and%20across%20frames.%20Sora%20Detector%0Aprovides%20a%20robust%20and%20quantifiable%20measure%20of%20consistency%2C%20static%20and%20dynamic%0Ahallucination.%20In%20addition%2C%20we%20have%20developed%20the%20Sora%20Detector%20Agent%20to%0Aautomate%20the%20hallucination%20detection%20process%20and%20generate%20a%20complete%20video%0Aquality%20report%20for%20each%20input%20video.%20Lastly%2C%20we%20present%20a%20novel%20meta-evaluation%0Abenchmark%2C%20T2VHaluBench%2C%20meticulously%20crafted%20to%20facilitate%20the%20evaluation%20of%0Aadvancements%20in%20T2V%20hallucination%20detection.%20Through%20extensive%20experiments%20on%0Avideos%20generated%20by%20Sora%20and%20other%20large%20T2V%20models%2C%20we%20demonstrate%20the%0Aefficacy%20of%20our%20approach%20in%20accurately%20detecting%20hallucinations.%20The%20code%20and%0Adataset%20can%20be%20accessed%20via%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSora%2520Detector%253A%2520A%2520Unified%2520Hallucination%2520Detection%2520for%2520Large%2520Text-to-Video%250A%2520%2520Models%26entry.906535625%3DZhixuan%2520Chu%2520and%2520Lei%2520Zhang%2520and%2520Yichen%2520Sun%2520and%2520Siqiao%2520Xue%2520and%2520Zhibo%2520Wang%2520and%2520Zhan%2520Qin%2520and%2520Kui%2520Ren%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520in%2520text-to-video%2520%2528T2V%2529%2520generative%2520models%2520has%2520enabled%250Athe%2520synthesis%2520of%2520high-fidelity%2520video%2520content%2520guided%2520by%2520textual%2520descriptions.%250ADespite%2520this%2520significant%2520progress%252C%2520these%2520models%2520are%2520often%2520susceptible%2520to%250Ahallucination%252C%2520generating%2520contents%2520that%2520contradict%2520the%2520input%2520text%252C%2520which%2520poses%250Aa%2520challenge%2520to%2520their%2520reliability%2520and%2520practical%2520deployment.%2520To%2520address%2520this%250Acritical%2520issue%252C%2520we%2520introduce%2520the%2520SoraDetector%252C%2520a%2520novel%2520unified%2520framework%250Adesigned%2520to%2520detect%2520hallucinations%2520across%2520diverse%2520large%2520T2V%2520models%252C%2520including%250Athe%2520cutting-edge%2520Sora%2520model.%2520Our%2520framework%2520is%2520built%2520upon%2520a%2520comprehensive%250Aanalysis%2520of%2520hallucination%2520phenomena%252C%2520categorizing%2520them%2520based%2520on%2520their%250Amanifestation%2520in%2520the%2520video%2520content.%2520Leveraging%2520the%2520state-of-the-art%2520keyframe%250Aextraction%2520techniques%2520and%2520multimodal%2520large%2520language%2520models%252C%2520SoraDetector%2520first%250Aevaluates%2520the%2520consistency%2520between%2520extracted%2520video%2520content%2520summary%2520and%2520textual%250Aprompts%252C%2520then%2520constructs%2520static%2520and%2520dynamic%2520knowledge%2520graphs%2520%2528KGs%2529%2520from%2520frames%250Ato%2520detect%2520hallucination%2520both%2520in%2520single%2520frames%2520and%2520across%2520frames.%2520Sora%2520Detector%250Aprovides%2520a%2520robust%2520and%2520quantifiable%2520measure%2520of%2520consistency%252C%2520static%2520and%2520dynamic%250Ahallucination.%2520In%2520addition%252C%2520we%2520have%2520developed%2520the%2520Sora%2520Detector%2520Agent%2520to%250Aautomate%2520the%2520hallucination%2520detection%2520process%2520and%2520generate%2520a%2520complete%2520video%250Aquality%2520report%2520for%2520each%2520input%2520video.%2520Lastly%252C%2520we%2520present%2520a%2520novel%2520meta-evaluation%250Abenchmark%252C%2520T2VHaluBench%252C%2520meticulously%2520crafted%2520to%2520facilitate%2520the%2520evaluation%2520of%250Aadvancements%2520in%2520T2V%2520hallucination%2520detection.%2520Through%2520extensive%2520experiments%2520on%250Avideos%2520generated%2520by%2520Sora%2520and%2520other%2520large%2520T2V%2520models%252C%2520we%2520demonstrate%2520the%250Aefficacy%2520of%2520our%2520approach%2520in%2520accurately%2520detecting%2520hallucinations.%2520The%2520code%2520and%250Adataset%2520can%2520be%2520accessed%2520via%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sora%20Detector%3A%20A%20Unified%20Hallucination%20Detection%20for%20Large%20Text-to-Video%0A%20%20Models&entry.906535625=Zhixuan%20Chu%20and%20Lei%20Zhang%20and%20Yichen%20Sun%20and%20Siqiao%20Xue%20and%20Zhibo%20Wang%20and%20Zhan%20Qin%20and%20Kui%20Ren&entry.1292438233=%20%20The%20rapid%20advancement%20in%20text-to-video%20%28T2V%29%20generative%20models%20has%20enabled%0Athe%20synthesis%20of%20high-fidelity%20video%20content%20guided%20by%20textual%20descriptions.%0ADespite%20this%20significant%20progress%2C%20these%20models%20are%20often%20susceptible%20to%0Ahallucination%2C%20generating%20contents%20that%20contradict%20the%20input%20text%2C%20which%20poses%0Aa%20challenge%20to%20their%20reliability%20and%20practical%20deployment.%20To%20address%20this%0Acritical%20issue%2C%20we%20introduce%20the%20SoraDetector%2C%20a%20novel%20unified%20framework%0Adesigned%20to%20detect%20hallucinations%20across%20diverse%20large%20T2V%20models%2C%20including%0Athe%20cutting-edge%20Sora%20model.%20Our%20framework%20is%20built%20upon%20a%20comprehensive%0Aanalysis%20of%20hallucination%20phenomena%2C%20categorizing%20them%20based%20on%20their%0Amanifestation%20in%20the%20video%20content.%20Leveraging%20the%20state-of-the-art%20keyframe%0Aextraction%20techniques%20and%20multimodal%20large%20language%20models%2C%20SoraDetector%20first%0Aevaluates%20the%20consistency%20between%20extracted%20video%20content%20summary%20and%20textual%0Aprompts%2C%20then%20constructs%20static%20and%20dynamic%20knowledge%20graphs%20%28KGs%29%20from%20frames%0Ato%20detect%20hallucination%20both%20in%20single%20frames%20and%20across%20frames.%20Sora%20Detector%0Aprovides%20a%20robust%20and%20quantifiable%20measure%20of%20consistency%2C%20static%20and%20dynamic%0Ahallucination.%20In%20addition%2C%20we%20have%20developed%20the%20Sora%20Detector%20Agent%20to%0Aautomate%20the%20hallucination%20detection%20process%20and%20generate%20a%20complete%20video%0Aquality%20report%20for%20each%20input%20video.%20Lastly%2C%20we%20present%20a%20novel%20meta-evaluation%0Abenchmark%2C%20T2VHaluBench%2C%20meticulously%20crafted%20to%20facilitate%20the%20evaluation%20of%0Aadvancements%20in%20T2V%20hallucination%20detection.%20Through%20extensive%20experiments%20on%0Avideos%20generated%20by%20Sora%20and%20other%20large%20T2V%20models%2C%20we%20demonstrate%20the%0Aefficacy%20of%20our%20approach%20in%20accurately%20detecting%20hallucinations.%20The%20code%20and%0Adataset%20can%20be%20accessed%20via%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04180v1&entry.124074799=Read"},
{"title": "Unified Dynamic Scanpath Predictors Outperform Individually Trained\n  Neural Models", "author": "Fares Abawi and Di Fu and Stefan Wermter", "abstract": "  Previous research on scanpath prediction has mainly focused on group models,\ndisregarding the fact that the scanpaths and attentional behaviors of\nindividuals are diverse. The disregard of these differences is especially\ndetrimental to social human-robot interaction, whereby robots commonly emulate\nhuman gaze based on heuristics or predefined patterns. However, human gaze\npatterns are heterogeneous and varying behaviors can significantly affect the\noutcomes of such human-robot interactions. To fill this gap, we developed a\ndeep learning-based social cue integration model for saliency prediction to\ninstead predict scanpaths in videos. Our model learned scanpaths by recursively\nintegrating fixation history and social cues through a gating mechanism and\nsequential attention. We evaluated our approach on gaze datasets of dynamic\nsocial scenes, observed under the free-viewing condition. The introduction of\nfixation history into our models makes it possible to train a single unified\nmodel rather than the resource-intensive approach of training individual models\nfor each set of scanpaths. We observed that the late neural integration\napproach surpasses early fusion when training models on a large dataset, in\ncomparison to a smaller dataset with a similar distribution. Results also\nindicate that a single unified model, trained on all the observers' scanpaths,\nperforms on par or better than individually trained models. We hypothesize that\nthis outcome is a result of the group saliency representations instilling\nuniversal attention in the model, while the supervisory signal and fixation\nhistory guide it to learn personalized attentional behaviors, providing the\nunified model a benefit over individual models due to its implicit\nrepresentation of universal attention.\n", "link": "http://arxiv.org/abs/2405.02929v2", "date": "2024-05-07", "relevancy": 2.1697, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5483}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5459}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Dynamic%20Scanpath%20Predictors%20Outperform%20Individually%20Trained%0A%20%20Neural%20Models&body=Title%3A%20Unified%20Dynamic%20Scanpath%20Predictors%20Outperform%20Individually%20Trained%0A%20%20Neural%20Models%0AAuthor%3A%20Fares%20Abawi%20and%20Di%20Fu%20and%20Stefan%20Wermter%0AAbstract%3A%20%20%20Previous%20research%20on%20scanpath%20prediction%20has%20mainly%20focused%20on%20group%20models%2C%0Adisregarding%20the%20fact%20that%20the%20scanpaths%20and%20attentional%20behaviors%20of%0Aindividuals%20are%20diverse.%20The%20disregard%20of%20these%20differences%20is%20especially%0Adetrimental%20to%20social%20human-robot%20interaction%2C%20whereby%20robots%20commonly%20emulate%0Ahuman%20gaze%20based%20on%20heuristics%20or%20predefined%20patterns.%20However%2C%20human%20gaze%0Apatterns%20are%20heterogeneous%20and%20varying%20behaviors%20can%20significantly%20affect%20the%0Aoutcomes%20of%20such%20human-robot%20interactions.%20To%20fill%20this%20gap%2C%20we%20developed%20a%0Adeep%20learning-based%20social%20cue%20integration%20model%20for%20saliency%20prediction%20to%0Ainstead%20predict%20scanpaths%20in%20videos.%20Our%20model%20learned%20scanpaths%20by%20recursively%0Aintegrating%20fixation%20history%20and%20social%20cues%20through%20a%20gating%20mechanism%20and%0Asequential%20attention.%20We%20evaluated%20our%20approach%20on%20gaze%20datasets%20of%20dynamic%0Asocial%20scenes%2C%20observed%20under%20the%20free-viewing%20condition.%20The%20introduction%20of%0Afixation%20history%20into%20our%20models%20makes%20it%20possible%20to%20train%20a%20single%20unified%0Amodel%20rather%20than%20the%20resource-intensive%20approach%20of%20training%20individual%20models%0Afor%20each%20set%20of%20scanpaths.%20We%20observed%20that%20the%20late%20neural%20integration%0Aapproach%20surpasses%20early%20fusion%20when%20training%20models%20on%20a%20large%20dataset%2C%20in%0Acomparison%20to%20a%20smaller%20dataset%20with%20a%20similar%20distribution.%20Results%20also%0Aindicate%20that%20a%20single%20unified%20model%2C%20trained%20on%20all%20the%20observers%27%20scanpaths%2C%0Aperforms%20on%20par%20or%20better%20than%20individually%20trained%20models.%20We%20hypothesize%20that%0Athis%20outcome%20is%20a%20result%20of%20the%20group%20saliency%20representations%20instilling%0Auniversal%20attention%20in%20the%20model%2C%20while%20the%20supervisory%20signal%20and%20fixation%0Ahistory%20guide%20it%20to%20learn%20personalized%20attentional%20behaviors%2C%20providing%20the%0Aunified%20model%20a%20benefit%20over%20individual%20models%20due%20to%20its%20implicit%0Arepresentation%20of%20universal%20attention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02929v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Dynamic%2520Scanpath%2520Predictors%2520Outperform%2520Individually%2520Trained%250A%2520%2520Neural%2520Models%26entry.906535625%3DFares%2520Abawi%2520and%2520Di%2520Fu%2520and%2520Stefan%2520Wermter%26entry.1292438233%3D%2520%2520Previous%2520research%2520on%2520scanpath%2520prediction%2520has%2520mainly%2520focused%2520on%2520group%2520models%252C%250Adisregarding%2520the%2520fact%2520that%2520the%2520scanpaths%2520and%2520attentional%2520behaviors%2520of%250Aindividuals%2520are%2520diverse.%2520The%2520disregard%2520of%2520these%2520differences%2520is%2520especially%250Adetrimental%2520to%2520social%2520human-robot%2520interaction%252C%2520whereby%2520robots%2520commonly%2520emulate%250Ahuman%2520gaze%2520based%2520on%2520heuristics%2520or%2520predefined%2520patterns.%2520However%252C%2520human%2520gaze%250Apatterns%2520are%2520heterogeneous%2520and%2520varying%2520behaviors%2520can%2520significantly%2520affect%2520the%250Aoutcomes%2520of%2520such%2520human-robot%2520interactions.%2520To%2520fill%2520this%2520gap%252C%2520we%2520developed%2520a%250Adeep%2520learning-based%2520social%2520cue%2520integration%2520model%2520for%2520saliency%2520prediction%2520to%250Ainstead%2520predict%2520scanpaths%2520in%2520videos.%2520Our%2520model%2520learned%2520scanpaths%2520by%2520recursively%250Aintegrating%2520fixation%2520history%2520and%2520social%2520cues%2520through%2520a%2520gating%2520mechanism%2520and%250Asequential%2520attention.%2520We%2520evaluated%2520our%2520approach%2520on%2520gaze%2520datasets%2520of%2520dynamic%250Asocial%2520scenes%252C%2520observed%2520under%2520the%2520free-viewing%2520condition.%2520The%2520introduction%2520of%250Afixation%2520history%2520into%2520our%2520models%2520makes%2520it%2520possible%2520to%2520train%2520a%2520single%2520unified%250Amodel%2520rather%2520than%2520the%2520resource-intensive%2520approach%2520of%2520training%2520individual%2520models%250Afor%2520each%2520set%2520of%2520scanpaths.%2520We%2520observed%2520that%2520the%2520late%2520neural%2520integration%250Aapproach%2520surpasses%2520early%2520fusion%2520when%2520training%2520models%2520on%2520a%2520large%2520dataset%252C%2520in%250Acomparison%2520to%2520a%2520smaller%2520dataset%2520with%2520a%2520similar%2520distribution.%2520Results%2520also%250Aindicate%2520that%2520a%2520single%2520unified%2520model%252C%2520trained%2520on%2520all%2520the%2520observers%2527%2520scanpaths%252C%250Aperforms%2520on%2520par%2520or%2520better%2520than%2520individually%2520trained%2520models.%2520We%2520hypothesize%2520that%250Athis%2520outcome%2520is%2520a%2520result%2520of%2520the%2520group%2520saliency%2520representations%2520instilling%250Auniversal%2520attention%2520in%2520the%2520model%252C%2520while%2520the%2520supervisory%2520signal%2520and%2520fixation%250Ahistory%2520guide%2520it%2520to%2520learn%2520personalized%2520attentional%2520behaviors%252C%2520providing%2520the%250Aunified%2520model%2520a%2520benefit%2520over%2520individual%2520models%2520due%2520to%2520its%2520implicit%250Arepresentation%2520of%2520universal%2520attention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02929v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Dynamic%20Scanpath%20Predictors%20Outperform%20Individually%20Trained%0A%20%20Neural%20Models&entry.906535625=Fares%20Abawi%20and%20Di%20Fu%20and%20Stefan%20Wermter&entry.1292438233=%20%20Previous%20research%20on%20scanpath%20prediction%20has%20mainly%20focused%20on%20group%20models%2C%0Adisregarding%20the%20fact%20that%20the%20scanpaths%20and%20attentional%20behaviors%20of%0Aindividuals%20are%20diverse.%20The%20disregard%20of%20these%20differences%20is%20especially%0Adetrimental%20to%20social%20human-robot%20interaction%2C%20whereby%20robots%20commonly%20emulate%0Ahuman%20gaze%20based%20on%20heuristics%20or%20predefined%20patterns.%20However%2C%20human%20gaze%0Apatterns%20are%20heterogeneous%20and%20varying%20behaviors%20can%20significantly%20affect%20the%0Aoutcomes%20of%20such%20human-robot%20interactions.%20To%20fill%20this%20gap%2C%20we%20developed%20a%0Adeep%20learning-based%20social%20cue%20integration%20model%20for%20saliency%20prediction%20to%0Ainstead%20predict%20scanpaths%20in%20videos.%20Our%20model%20learned%20scanpaths%20by%20recursively%0Aintegrating%20fixation%20history%20and%20social%20cues%20through%20a%20gating%20mechanism%20and%0Asequential%20attention.%20We%20evaluated%20our%20approach%20on%20gaze%20datasets%20of%20dynamic%0Asocial%20scenes%2C%20observed%20under%20the%20free-viewing%20condition.%20The%20introduction%20of%0Afixation%20history%20into%20our%20models%20makes%20it%20possible%20to%20train%20a%20single%20unified%0Amodel%20rather%20than%20the%20resource-intensive%20approach%20of%20training%20individual%20models%0Afor%20each%20set%20of%20scanpaths.%20We%20observed%20that%20the%20late%20neural%20integration%0Aapproach%20surpasses%20early%20fusion%20when%20training%20models%20on%20a%20large%20dataset%2C%20in%0Acomparison%20to%20a%20smaller%20dataset%20with%20a%20similar%20distribution.%20Results%20also%0Aindicate%20that%20a%20single%20unified%20model%2C%20trained%20on%20all%20the%20observers%27%20scanpaths%2C%0Aperforms%20on%20par%20or%20better%20than%20individually%20trained%20models.%20We%20hypothesize%20that%0Athis%20outcome%20is%20a%20result%20of%20the%20group%20saliency%20representations%20instilling%0Auniversal%20attention%20in%20the%20model%2C%20while%20the%20supervisory%20signal%20and%20fixation%0Ahistory%20guide%20it%20to%20learn%20personalized%20attentional%20behaviors%2C%20providing%20the%0Aunified%20model%20a%20benefit%20over%20individual%20models%20due%20to%20its%20implicit%0Arepresentation%20of%20universal%20attention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02929v2&entry.124074799=Read"},
{"title": "Choose What You Need: Disentangled Representation Learning for Scene\n  Text Recognition, Removal and Editing", "author": "Boqiang Zhang and Hongtao Xie and Zuan Gao and Yuxin Wang", "abstract": "  Scene text images contain not only style information (font, background) but\nalso content information (character, texture). Different scene text tasks need\ndifferent information, but previous representation learning methods use tightly\ncoupled features for all tasks, resulting in sub-optimal performance. We\npropose a Disentangled Representation Learning framework (DARLING) aimed at\ndisentangling these two types of features for improved adaptability in better\naddressing various downstream tasks (choose what you really need).\nSpecifically, we synthesize a dataset of image pairs with identical style but\ndifferent content. Based on the dataset, we decouple the two types of features\nby the supervision design. Clearly, we directly split the visual representation\ninto style and content features, the content features are supervised by a text\nrecognition loss, while an alignment loss aligns the style features in the\nimage pairs. Then, style features are employed in reconstructing the\ncounterpart image via an image decoder with a prompt that indicates the\ncounterpart's content. Such an operation effectively decouples the features\nbased on their distinctive properties. To the best of our knowledge, this is\nthe first time in the field of scene text that disentangles the inherent\nproperties of the text images. Our method achieves state-of-the-art performance\nin Scene Text Recognition, Removal, and Editing.\n", "link": "http://arxiv.org/abs/2405.04377v1", "date": "2024-05-07", "relevancy": 2.1653, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5516}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5346}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Choose%20What%20You%20Need%3A%20Disentangled%20Representation%20Learning%20for%20Scene%0A%20%20Text%20Recognition%2C%20Removal%20and%20Editing&body=Title%3A%20Choose%20What%20You%20Need%3A%20Disentangled%20Representation%20Learning%20for%20Scene%0A%20%20Text%20Recognition%2C%20Removal%20and%20Editing%0AAuthor%3A%20Boqiang%20Zhang%20and%20Hongtao%20Xie%20and%20Zuan%20Gao%20and%20Yuxin%20Wang%0AAbstract%3A%20%20%20Scene%20text%20images%20contain%20not%20only%20style%20information%20%28font%2C%20background%29%20but%0Aalso%20content%20information%20%28character%2C%20texture%29.%20Different%20scene%20text%20tasks%20need%0Adifferent%20information%2C%20but%20previous%20representation%20learning%20methods%20use%20tightly%0Acoupled%20features%20for%20all%20tasks%2C%20resulting%20in%20sub-optimal%20performance.%20We%0Apropose%20a%20Disentangled%20Representation%20Learning%20framework%20%28DARLING%29%20aimed%20at%0Adisentangling%20these%20two%20types%20of%20features%20for%20improved%20adaptability%20in%20better%0Aaddressing%20various%20downstream%20tasks%20%28choose%20what%20you%20really%20need%29.%0ASpecifically%2C%20we%20synthesize%20a%20dataset%20of%20image%20pairs%20with%20identical%20style%20but%0Adifferent%20content.%20Based%20on%20the%20dataset%2C%20we%20decouple%20the%20two%20types%20of%20features%0Aby%20the%20supervision%20design.%20Clearly%2C%20we%20directly%20split%20the%20visual%20representation%0Ainto%20style%20and%20content%20features%2C%20the%20content%20features%20are%20supervised%20by%20a%20text%0Arecognition%20loss%2C%20while%20an%20alignment%20loss%20aligns%20the%20style%20features%20in%20the%0Aimage%20pairs.%20Then%2C%20style%20features%20are%20employed%20in%20reconstructing%20the%0Acounterpart%20image%20via%20an%20image%20decoder%20with%20a%20prompt%20that%20indicates%20the%0Acounterpart%27s%20content.%20Such%20an%20operation%20effectively%20decouples%20the%20features%0Abased%20on%20their%20distinctive%20properties.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%0Athe%20first%20time%20in%20the%20field%20of%20scene%20text%20that%20disentangles%20the%20inherent%0Aproperties%20of%20the%20text%20images.%20Our%20method%20achieves%20state-of-the-art%20performance%0Ain%20Scene%20Text%20Recognition%2C%20Removal%2C%20and%20Editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChoose%2520What%2520You%2520Need%253A%2520Disentangled%2520Representation%2520Learning%2520for%2520Scene%250A%2520%2520Text%2520Recognition%252C%2520Removal%2520and%2520Editing%26entry.906535625%3DBoqiang%2520Zhang%2520and%2520Hongtao%2520Xie%2520and%2520Zuan%2520Gao%2520and%2520Yuxin%2520Wang%26entry.1292438233%3D%2520%2520Scene%2520text%2520images%2520contain%2520not%2520only%2520style%2520information%2520%2528font%252C%2520background%2529%2520but%250Aalso%2520content%2520information%2520%2528character%252C%2520texture%2529.%2520Different%2520scene%2520text%2520tasks%2520need%250Adifferent%2520information%252C%2520but%2520previous%2520representation%2520learning%2520methods%2520use%2520tightly%250Acoupled%2520features%2520for%2520all%2520tasks%252C%2520resulting%2520in%2520sub-optimal%2520performance.%2520We%250Apropose%2520a%2520Disentangled%2520Representation%2520Learning%2520framework%2520%2528DARLING%2529%2520aimed%2520at%250Adisentangling%2520these%2520two%2520types%2520of%2520features%2520for%2520improved%2520adaptability%2520in%2520better%250Aaddressing%2520various%2520downstream%2520tasks%2520%2528choose%2520what%2520you%2520really%2520need%2529.%250ASpecifically%252C%2520we%2520synthesize%2520a%2520dataset%2520of%2520image%2520pairs%2520with%2520identical%2520style%2520but%250Adifferent%2520content.%2520Based%2520on%2520the%2520dataset%252C%2520we%2520decouple%2520the%2520two%2520types%2520of%2520features%250Aby%2520the%2520supervision%2520design.%2520Clearly%252C%2520we%2520directly%2520split%2520the%2520visual%2520representation%250Ainto%2520style%2520and%2520content%2520features%252C%2520the%2520content%2520features%2520are%2520supervised%2520by%2520a%2520text%250Arecognition%2520loss%252C%2520while%2520an%2520alignment%2520loss%2520aligns%2520the%2520style%2520features%2520in%2520the%250Aimage%2520pairs.%2520Then%252C%2520style%2520features%2520are%2520employed%2520in%2520reconstructing%2520the%250Acounterpart%2520image%2520via%2520an%2520image%2520decoder%2520with%2520a%2520prompt%2520that%2520indicates%2520the%250Acounterpart%2527s%2520content.%2520Such%2520an%2520operation%2520effectively%2520decouples%2520the%2520features%250Abased%2520on%2520their%2520distinctive%2520properties.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%250Athe%2520first%2520time%2520in%2520the%2520field%2520of%2520scene%2520text%2520that%2520disentangles%2520the%2520inherent%250Aproperties%2520of%2520the%2520text%2520images.%2520Our%2520method%2520achieves%2520state-of-the-art%2520performance%250Ain%2520Scene%2520Text%2520Recognition%252C%2520Removal%252C%2520and%2520Editing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Choose%20What%20You%20Need%3A%20Disentangled%20Representation%20Learning%20for%20Scene%0A%20%20Text%20Recognition%2C%20Removal%20and%20Editing&entry.906535625=Boqiang%20Zhang%20and%20Hongtao%20Xie%20and%20Zuan%20Gao%20and%20Yuxin%20Wang&entry.1292438233=%20%20Scene%20text%20images%20contain%20not%20only%20style%20information%20%28font%2C%20background%29%20but%0Aalso%20content%20information%20%28character%2C%20texture%29.%20Different%20scene%20text%20tasks%20need%0Adifferent%20information%2C%20but%20previous%20representation%20learning%20methods%20use%20tightly%0Acoupled%20features%20for%20all%20tasks%2C%20resulting%20in%20sub-optimal%20performance.%20We%0Apropose%20a%20Disentangled%20Representation%20Learning%20framework%20%28DARLING%29%20aimed%20at%0Adisentangling%20these%20two%20types%20of%20features%20for%20improved%20adaptability%20in%20better%0Aaddressing%20various%20downstream%20tasks%20%28choose%20what%20you%20really%20need%29.%0ASpecifically%2C%20we%20synthesize%20a%20dataset%20of%20image%20pairs%20with%20identical%20style%20but%0Adifferent%20content.%20Based%20on%20the%20dataset%2C%20we%20decouple%20the%20two%20types%20of%20features%0Aby%20the%20supervision%20design.%20Clearly%2C%20we%20directly%20split%20the%20visual%20representation%0Ainto%20style%20and%20content%20features%2C%20the%20content%20features%20are%20supervised%20by%20a%20text%0Arecognition%20loss%2C%20while%20an%20alignment%20loss%20aligns%20the%20style%20features%20in%20the%0Aimage%20pairs.%20Then%2C%20style%20features%20are%20employed%20in%20reconstructing%20the%0Acounterpart%20image%20via%20an%20image%20decoder%20with%20a%20prompt%20that%20indicates%20the%0Acounterpart%27s%20content.%20Such%20an%20operation%20effectively%20decouples%20the%20features%0Abased%20on%20their%20distinctive%20properties.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%0Athe%20first%20time%20in%20the%20field%20of%20scene%20text%20that%20disentangles%20the%20inherent%0Aproperties%20of%20the%20text%20images.%20Our%20method%20achieves%20state-of-the-art%20performance%0Ain%20Scene%20Text%20Recognition%2C%20Removal%2C%20and%20Editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04377v1&entry.124074799=Read"},
{"title": "Group-aware Parameter-efficient Updating for Content-Adaptive Neural\n  Video Compression", "author": "Zhenghao Chen and Luping Zhou and Zhihao Hu and Dong Xu", "abstract": "  Content-adaptive compression is crucial for enhancing the adaptability of the\npre-trained neural codec for various contents. Although these methods have been\nvery practical in neural image compression (NIC), their application in neural\nvideo compression (NVC) is still limited due to two main aspects: 1), video\ncompression relies heavily on temporal redundancy, therefore updating just one\nor a few frames can lead to significant errors accumulating over time; 2), NVC\nframeworks are generally more complex, with many large components that are not\neasy to update quickly during encoding. To address the previously mentioned\nchallenges, we have developed a content-adaptive NVC technique called\nGroup-aware Parameter-Efficient Updating (GPU). Initially, to minimize error\naccumulation, we adopt a group-aware approach for updating encoder parameters.\nThis involves adopting a patch-based Group of Pictures (GoP) training strategy\nto segment a video into patch-based GoPs, which will be updated to facilitate a\nglobally optimized domain-transferable solution. Subsequently, we introduce a\nparameter-efficient delta-tuning strategy, which is achieved by integrating\nseveral light-weight adapters into each coding component of the encoding\nprocess by both serial and parallel configuration. Such architecture-agnostic\nmodules stimulate the components with large parameters, thereby reducing both\nthe update cost and the encoding time. We incorporate our GPU into the latest\nNVC framework and conduct comprehensive experiments, whose results showcase\noutstanding video compression efficiency across four video benchmarks and\nadaptability of one medical image benchmark.\n", "link": "http://arxiv.org/abs/2405.04274v1", "date": "2024-05-07", "relevancy": 2.1637, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.562}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5269}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Group-aware%20Parameter-efficient%20Updating%20for%20Content-Adaptive%20Neural%0A%20%20Video%20Compression&body=Title%3A%20Group-aware%20Parameter-efficient%20Updating%20for%20Content-Adaptive%20Neural%0A%20%20Video%20Compression%0AAuthor%3A%20Zhenghao%20Chen%20and%20Luping%20Zhou%20and%20Zhihao%20Hu%20and%20Dong%20Xu%0AAbstract%3A%20%20%20Content-adaptive%20compression%20is%20crucial%20for%20enhancing%20the%20adaptability%20of%20the%0Apre-trained%20neural%20codec%20for%20various%20contents.%20Although%20these%20methods%20have%20been%0Avery%20practical%20in%20neural%20image%20compression%20%28NIC%29%2C%20their%20application%20in%20neural%0Avideo%20compression%20%28NVC%29%20is%20still%20limited%20due%20to%20two%20main%20aspects%3A%201%29%2C%20video%0Acompression%20relies%20heavily%20on%20temporal%20redundancy%2C%20therefore%20updating%20just%20one%0Aor%20a%20few%20frames%20can%20lead%20to%20significant%20errors%20accumulating%20over%20time%3B%202%29%2C%20NVC%0Aframeworks%20are%20generally%20more%20complex%2C%20with%20many%20large%20components%20that%20are%20not%0Aeasy%20to%20update%20quickly%20during%20encoding.%20To%20address%20the%20previously%20mentioned%0Achallenges%2C%20we%20have%20developed%20a%20content-adaptive%20NVC%20technique%20called%0AGroup-aware%20Parameter-Efficient%20Updating%20%28GPU%29.%20Initially%2C%20to%20minimize%20error%0Aaccumulation%2C%20we%20adopt%20a%20group-aware%20approach%20for%20updating%20encoder%20parameters.%0AThis%20involves%20adopting%20a%20patch-based%20Group%20of%20Pictures%20%28GoP%29%20training%20strategy%0Ato%20segment%20a%20video%20into%20patch-based%20GoPs%2C%20which%20will%20be%20updated%20to%20facilitate%20a%0Aglobally%20optimized%20domain-transferable%20solution.%20Subsequently%2C%20we%20introduce%20a%0Aparameter-efficient%20delta-tuning%20strategy%2C%20which%20is%20achieved%20by%20integrating%0Aseveral%20light-weight%20adapters%20into%20each%20coding%20component%20of%20the%20encoding%0Aprocess%20by%20both%20serial%20and%20parallel%20configuration.%20Such%20architecture-agnostic%0Amodules%20stimulate%20the%20components%20with%20large%20parameters%2C%20thereby%20reducing%20both%0Athe%20update%20cost%20and%20the%20encoding%20time.%20We%20incorporate%20our%20GPU%20into%20the%20latest%0ANVC%20framework%20and%20conduct%20comprehensive%20experiments%2C%20whose%20results%20showcase%0Aoutstanding%20video%20compression%20efficiency%20across%20four%20video%20benchmarks%20and%0Aadaptability%20of%20one%20medical%20image%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04274v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGroup-aware%2520Parameter-efficient%2520Updating%2520for%2520Content-Adaptive%2520Neural%250A%2520%2520Video%2520Compression%26entry.906535625%3DZhenghao%2520Chen%2520and%2520Luping%2520Zhou%2520and%2520Zhihao%2520Hu%2520and%2520Dong%2520Xu%26entry.1292438233%3D%2520%2520Content-adaptive%2520compression%2520is%2520crucial%2520for%2520enhancing%2520the%2520adaptability%2520of%2520the%250Apre-trained%2520neural%2520codec%2520for%2520various%2520contents.%2520Although%2520these%2520methods%2520have%2520been%250Avery%2520practical%2520in%2520neural%2520image%2520compression%2520%2528NIC%2529%252C%2520their%2520application%2520in%2520neural%250Avideo%2520compression%2520%2528NVC%2529%2520is%2520still%2520limited%2520due%2520to%2520two%2520main%2520aspects%253A%25201%2529%252C%2520video%250Acompression%2520relies%2520heavily%2520on%2520temporal%2520redundancy%252C%2520therefore%2520updating%2520just%2520one%250Aor%2520a%2520few%2520frames%2520can%2520lead%2520to%2520significant%2520errors%2520accumulating%2520over%2520time%253B%25202%2529%252C%2520NVC%250Aframeworks%2520are%2520generally%2520more%2520complex%252C%2520with%2520many%2520large%2520components%2520that%2520are%2520not%250Aeasy%2520to%2520update%2520quickly%2520during%2520encoding.%2520To%2520address%2520the%2520previously%2520mentioned%250Achallenges%252C%2520we%2520have%2520developed%2520a%2520content-adaptive%2520NVC%2520technique%2520called%250AGroup-aware%2520Parameter-Efficient%2520Updating%2520%2528GPU%2529.%2520Initially%252C%2520to%2520minimize%2520error%250Aaccumulation%252C%2520we%2520adopt%2520a%2520group-aware%2520approach%2520for%2520updating%2520encoder%2520parameters.%250AThis%2520involves%2520adopting%2520a%2520patch-based%2520Group%2520of%2520Pictures%2520%2528GoP%2529%2520training%2520strategy%250Ato%2520segment%2520a%2520video%2520into%2520patch-based%2520GoPs%252C%2520which%2520will%2520be%2520updated%2520to%2520facilitate%2520a%250Aglobally%2520optimized%2520domain-transferable%2520solution.%2520Subsequently%252C%2520we%2520introduce%2520a%250Aparameter-efficient%2520delta-tuning%2520strategy%252C%2520which%2520is%2520achieved%2520by%2520integrating%250Aseveral%2520light-weight%2520adapters%2520into%2520each%2520coding%2520component%2520of%2520the%2520encoding%250Aprocess%2520by%2520both%2520serial%2520and%2520parallel%2520configuration.%2520Such%2520architecture-agnostic%250Amodules%2520stimulate%2520the%2520components%2520with%2520large%2520parameters%252C%2520thereby%2520reducing%2520both%250Athe%2520update%2520cost%2520and%2520the%2520encoding%2520time.%2520We%2520incorporate%2520our%2520GPU%2520into%2520the%2520latest%250ANVC%2520framework%2520and%2520conduct%2520comprehensive%2520experiments%252C%2520whose%2520results%2520showcase%250Aoutstanding%2520video%2520compression%2520efficiency%2520across%2520four%2520video%2520benchmarks%2520and%250Aadaptability%2520of%2520one%2520medical%2520image%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04274v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Group-aware%20Parameter-efficient%20Updating%20for%20Content-Adaptive%20Neural%0A%20%20Video%20Compression&entry.906535625=Zhenghao%20Chen%20and%20Luping%20Zhou%20and%20Zhihao%20Hu%20and%20Dong%20Xu&entry.1292438233=%20%20Content-adaptive%20compression%20is%20crucial%20for%20enhancing%20the%20adaptability%20of%20the%0Apre-trained%20neural%20codec%20for%20various%20contents.%20Although%20these%20methods%20have%20been%0Avery%20practical%20in%20neural%20image%20compression%20%28NIC%29%2C%20their%20application%20in%20neural%0Avideo%20compression%20%28NVC%29%20is%20still%20limited%20due%20to%20two%20main%20aspects%3A%201%29%2C%20video%0Acompression%20relies%20heavily%20on%20temporal%20redundancy%2C%20therefore%20updating%20just%20one%0Aor%20a%20few%20frames%20can%20lead%20to%20significant%20errors%20accumulating%20over%20time%3B%202%29%2C%20NVC%0Aframeworks%20are%20generally%20more%20complex%2C%20with%20many%20large%20components%20that%20are%20not%0Aeasy%20to%20update%20quickly%20during%20encoding.%20To%20address%20the%20previously%20mentioned%0Achallenges%2C%20we%20have%20developed%20a%20content-adaptive%20NVC%20technique%20called%0AGroup-aware%20Parameter-Efficient%20Updating%20%28GPU%29.%20Initially%2C%20to%20minimize%20error%0Aaccumulation%2C%20we%20adopt%20a%20group-aware%20approach%20for%20updating%20encoder%20parameters.%0AThis%20involves%20adopting%20a%20patch-based%20Group%20of%20Pictures%20%28GoP%29%20training%20strategy%0Ato%20segment%20a%20video%20into%20patch-based%20GoPs%2C%20which%20will%20be%20updated%20to%20facilitate%20a%0Aglobally%20optimized%20domain-transferable%20solution.%20Subsequently%2C%20we%20introduce%20a%0Aparameter-efficient%20delta-tuning%20strategy%2C%20which%20is%20achieved%20by%20integrating%0Aseveral%20light-weight%20adapters%20into%20each%20coding%20component%20of%20the%20encoding%0Aprocess%20by%20both%20serial%20and%20parallel%20configuration.%20Such%20architecture-agnostic%0Amodules%20stimulate%20the%20components%20with%20large%20parameters%2C%20thereby%20reducing%20both%0Athe%20update%20cost%20and%20the%20encoding%20time.%20We%20incorporate%20our%20GPU%20into%20the%20latest%0ANVC%20framework%20and%20conduct%20comprehensive%20experiments%2C%20whose%20results%20showcase%0Aoutstanding%20video%20compression%20efficiency%20across%20four%20video%20benchmarks%20and%0Aadaptability%20of%20one%20medical%20image%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04274v1&entry.124074799=Read"},
{"title": "Label-Agnostic Forgetting: A Supervision-Free Unlearning in Deep Models", "author": "Shaofei Shen and Chenhao Zhang and Yawen Zhao and Alina Bialkowski and Weitong Tony Chen and Miao Xu", "abstract": "  Machine unlearning aims to remove information derived from forgotten data\nwhile preserving that of the remaining dataset in a well-trained model. With\nthe increasing emphasis on data privacy, several approaches to machine\nunlearning have emerged. However, these methods typically rely on complete\nsupervision throughout the unlearning process. Unfortunately, obtaining such\nsupervision, whether for the forgetting or remaining data, can be impractical\ndue to the substantial cost associated with annotating real-world datasets.\nThis challenge prompts us to propose a supervision-free unlearning approach\nthat operates without the need for labels during the unlearning process.\nSpecifically, we introduce a variational approach to approximate the\ndistribution of representations for the remaining data. Leveraging this\napproximation, we adapt the original model to eliminate information from the\nforgotten data at the representation level. To further address the issue of\nlacking supervision information, which hinders alignment with ground truth, we\nintroduce a contrastive loss to facilitate the matching of representations\nbetween the remaining data and those of the original model, thus preserving\npredictive performance. Experimental results across various unlearning tasks\ndemonstrate the effectiveness of our proposed method, Label-Agnostic Forgetting\n(LAF) without using any labels, which achieves comparable performance to\nstate-of-the-art methods that rely on full supervision information.\nFurthermore, our approach excels in semi-supervised scenarios, leveraging\nlimited supervision information to outperform fully supervised baselines. This\nwork not only showcases the viability of supervision-free unlearning in deep\nmodels but also opens up a new possibility for future research in unlearning at\nthe representation level.\n", "link": "http://arxiv.org/abs/2404.00506v2", "date": "2024-05-07", "relevancy": 2.1532, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5699}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5416}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Label-Agnostic%20Forgetting%3A%20A%20Supervision-Free%20Unlearning%20in%20Deep%20Models&body=Title%3A%20Label-Agnostic%20Forgetting%3A%20A%20Supervision-Free%20Unlearning%20in%20Deep%20Models%0AAuthor%3A%20Shaofei%20Shen%20and%20Chenhao%20Zhang%20and%20Yawen%20Zhao%20and%20Alina%20Bialkowski%20and%20Weitong%20Tony%20Chen%20and%20Miao%20Xu%0AAbstract%3A%20%20%20Machine%20unlearning%20aims%20to%20remove%20information%20derived%20from%20forgotten%20data%0Awhile%20preserving%20that%20of%20the%20remaining%20dataset%20in%20a%20well-trained%20model.%20With%0Athe%20increasing%20emphasis%20on%20data%20privacy%2C%20several%20approaches%20to%20machine%0Aunlearning%20have%20emerged.%20However%2C%20these%20methods%20typically%20rely%20on%20complete%0Asupervision%20throughout%20the%20unlearning%20process.%20Unfortunately%2C%20obtaining%20such%0Asupervision%2C%20whether%20for%20the%20forgetting%20or%20remaining%20data%2C%20can%20be%20impractical%0Adue%20to%20the%20substantial%20cost%20associated%20with%20annotating%20real-world%20datasets.%0AThis%20challenge%20prompts%20us%20to%20propose%20a%20supervision-free%20unlearning%20approach%0Athat%20operates%20without%20the%20need%20for%20labels%20during%20the%20unlearning%20process.%0ASpecifically%2C%20we%20introduce%20a%20variational%20approach%20to%20approximate%20the%0Adistribution%20of%20representations%20for%20the%20remaining%20data.%20Leveraging%20this%0Aapproximation%2C%20we%20adapt%20the%20original%20model%20to%20eliminate%20information%20from%20the%0Aforgotten%20data%20at%20the%20representation%20level.%20To%20further%20address%20the%20issue%20of%0Alacking%20supervision%20information%2C%20which%20hinders%20alignment%20with%20ground%20truth%2C%20we%0Aintroduce%20a%20contrastive%20loss%20to%20facilitate%20the%20matching%20of%20representations%0Abetween%20the%20remaining%20data%20and%20those%20of%20the%20original%20model%2C%20thus%20preserving%0Apredictive%20performance.%20Experimental%20results%20across%20various%20unlearning%20tasks%0Ademonstrate%20the%20effectiveness%20of%20our%20proposed%20method%2C%20Label-Agnostic%20Forgetting%0A%28LAF%29%20without%20using%20any%20labels%2C%20which%20achieves%20comparable%20performance%20to%0Astate-of-the-art%20methods%20that%20rely%20on%20full%20supervision%20information.%0AFurthermore%2C%20our%20approach%20excels%20in%20semi-supervised%20scenarios%2C%20leveraging%0Alimited%20supervision%20information%20to%20outperform%20fully%20supervised%20baselines.%20This%0Awork%20not%20only%20showcases%20the%20viability%20of%20supervision-free%20unlearning%20in%20deep%0Amodels%20but%20also%20opens%20up%20a%20new%20possibility%20for%20future%20research%20in%20unlearning%20at%0Athe%20representation%20level.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00506v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLabel-Agnostic%2520Forgetting%253A%2520A%2520Supervision-Free%2520Unlearning%2520in%2520Deep%2520Models%26entry.906535625%3DShaofei%2520Shen%2520and%2520Chenhao%2520Zhang%2520and%2520Yawen%2520Zhao%2520and%2520Alina%2520Bialkowski%2520and%2520Weitong%2520Tony%2520Chen%2520and%2520Miao%2520Xu%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520aims%2520to%2520remove%2520information%2520derived%2520from%2520forgotten%2520data%250Awhile%2520preserving%2520that%2520of%2520the%2520remaining%2520dataset%2520in%2520a%2520well-trained%2520model.%2520With%250Athe%2520increasing%2520emphasis%2520on%2520data%2520privacy%252C%2520several%2520approaches%2520to%2520machine%250Aunlearning%2520have%2520emerged.%2520However%252C%2520these%2520methods%2520typically%2520rely%2520on%2520complete%250Asupervision%2520throughout%2520the%2520unlearning%2520process.%2520Unfortunately%252C%2520obtaining%2520such%250Asupervision%252C%2520whether%2520for%2520the%2520forgetting%2520or%2520remaining%2520data%252C%2520can%2520be%2520impractical%250Adue%2520to%2520the%2520substantial%2520cost%2520associated%2520with%2520annotating%2520real-world%2520datasets.%250AThis%2520challenge%2520prompts%2520us%2520to%2520propose%2520a%2520supervision-free%2520unlearning%2520approach%250Athat%2520operates%2520without%2520the%2520need%2520for%2520labels%2520during%2520the%2520unlearning%2520process.%250ASpecifically%252C%2520we%2520introduce%2520a%2520variational%2520approach%2520to%2520approximate%2520the%250Adistribution%2520of%2520representations%2520for%2520the%2520remaining%2520data.%2520Leveraging%2520this%250Aapproximation%252C%2520we%2520adapt%2520the%2520original%2520model%2520to%2520eliminate%2520information%2520from%2520the%250Aforgotten%2520data%2520at%2520the%2520representation%2520level.%2520To%2520further%2520address%2520the%2520issue%2520of%250Alacking%2520supervision%2520information%252C%2520which%2520hinders%2520alignment%2520with%2520ground%2520truth%252C%2520we%250Aintroduce%2520a%2520contrastive%2520loss%2520to%2520facilitate%2520the%2520matching%2520of%2520representations%250Abetween%2520the%2520remaining%2520data%2520and%2520those%2520of%2520the%2520original%2520model%252C%2520thus%2520preserving%250Apredictive%2520performance.%2520Experimental%2520results%2520across%2520various%2520unlearning%2520tasks%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method%252C%2520Label-Agnostic%2520Forgetting%250A%2528LAF%2529%2520without%2520using%2520any%2520labels%252C%2520which%2520achieves%2520comparable%2520performance%2520to%250Astate-of-the-art%2520methods%2520that%2520rely%2520on%2520full%2520supervision%2520information.%250AFurthermore%252C%2520our%2520approach%2520excels%2520in%2520semi-supervised%2520scenarios%252C%2520leveraging%250Alimited%2520supervision%2520information%2520to%2520outperform%2520fully%2520supervised%2520baselines.%2520This%250Awork%2520not%2520only%2520showcases%2520the%2520viability%2520of%2520supervision-free%2520unlearning%2520in%2520deep%250Amodels%2520but%2520also%2520opens%2520up%2520a%2520new%2520possibility%2520for%2520future%2520research%2520in%2520unlearning%2520at%250Athe%2520representation%2520level.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00506v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label-Agnostic%20Forgetting%3A%20A%20Supervision-Free%20Unlearning%20in%20Deep%20Models&entry.906535625=Shaofei%20Shen%20and%20Chenhao%20Zhang%20and%20Yawen%20Zhao%20and%20Alina%20Bialkowski%20and%20Weitong%20Tony%20Chen%20and%20Miao%20Xu&entry.1292438233=%20%20Machine%20unlearning%20aims%20to%20remove%20information%20derived%20from%20forgotten%20data%0Awhile%20preserving%20that%20of%20the%20remaining%20dataset%20in%20a%20well-trained%20model.%20With%0Athe%20increasing%20emphasis%20on%20data%20privacy%2C%20several%20approaches%20to%20machine%0Aunlearning%20have%20emerged.%20However%2C%20these%20methods%20typically%20rely%20on%20complete%0Asupervision%20throughout%20the%20unlearning%20process.%20Unfortunately%2C%20obtaining%20such%0Asupervision%2C%20whether%20for%20the%20forgetting%20or%20remaining%20data%2C%20can%20be%20impractical%0Adue%20to%20the%20substantial%20cost%20associated%20with%20annotating%20real-world%20datasets.%0AThis%20challenge%20prompts%20us%20to%20propose%20a%20supervision-free%20unlearning%20approach%0Athat%20operates%20without%20the%20need%20for%20labels%20during%20the%20unlearning%20process.%0ASpecifically%2C%20we%20introduce%20a%20variational%20approach%20to%20approximate%20the%0Adistribution%20of%20representations%20for%20the%20remaining%20data.%20Leveraging%20this%0Aapproximation%2C%20we%20adapt%20the%20original%20model%20to%20eliminate%20information%20from%20the%0Aforgotten%20data%20at%20the%20representation%20level.%20To%20further%20address%20the%20issue%20of%0Alacking%20supervision%20information%2C%20which%20hinders%20alignment%20with%20ground%20truth%2C%20we%0Aintroduce%20a%20contrastive%20loss%20to%20facilitate%20the%20matching%20of%20representations%0Abetween%20the%20remaining%20data%20and%20those%20of%20the%20original%20model%2C%20thus%20preserving%0Apredictive%20performance.%20Experimental%20results%20across%20various%20unlearning%20tasks%0Ademonstrate%20the%20effectiveness%20of%20our%20proposed%20method%2C%20Label-Agnostic%20Forgetting%0A%28LAF%29%20without%20using%20any%20labels%2C%20which%20achieves%20comparable%20performance%20to%0Astate-of-the-art%20methods%20that%20rely%20on%20full%20supervision%20information.%0AFurthermore%2C%20our%20approach%20excels%20in%20semi-supervised%20scenarios%2C%20leveraging%0Alimited%20supervision%20information%20to%20outperform%20fully%20supervised%20baselines.%20This%0Awork%20not%20only%20showcases%20the%20viability%20of%20supervision-free%20unlearning%20in%20deep%0Amodels%20but%20also%20opens%20up%20a%20new%20possibility%20for%20future%20research%20in%20unlearning%20at%0Athe%20representation%20level.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00506v2&entry.124074799=Read"},
{"title": "CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor", "author": "Shuyang Sun and Runjia Li and Philip Torr and Xiuye Gu and Siyang Li", "abstract": "  Existing open-vocabulary image segmentation methods require a fine-tuning\nstep on mask labels and/or image-text datasets. Mask labels are\nlabor-intensive, which limits the number of categories in segmentation\ndatasets. Consequently, the vocabulary capacity of pre-trained VLMs is severely\nreduced after fine-tuning. However, without fine-tuning, VLMs trained under\nweak image-text supervision tend to make suboptimal mask predictions. To\nalleviate these issues, we introduce a novel recurrent framework that\nprogressively filters out irrelevant texts and enhances mask quality without\ntraining efforts. The recurrent unit is a two-stage segmenter built upon a\nfrozen VLM. Thus, our model retains the VLM's broad vocabulary space and equips\nit with segmentation ability. Experiments show that our method outperforms not\nonly the training-free counterparts, but also those fine-tuned with millions of\ndata samples, and sets the new state-of-the-art records for both zero-shot\nsemantic and referring segmentation. Concretely, we improve the current record\nby 28.8, 16.0, and 6.9 mIoU on Pascal VOC, COCO Object, and Pascal Context.\n", "link": "http://arxiv.org/abs/2312.07661v3", "date": "2024-05-07", "relevancy": 2.1422, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5634}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5159}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP%20as%20RNN%3A%20Segment%20Countless%20Visual%20Concepts%20without%20Training%20Endeavor&body=Title%3A%20CLIP%20as%20RNN%3A%20Segment%20Countless%20Visual%20Concepts%20without%20Training%20Endeavor%0AAuthor%3A%20Shuyang%20Sun%20and%20Runjia%20Li%20and%20Philip%20Torr%20and%20Xiuye%20Gu%20and%20Siyang%20Li%0AAbstract%3A%20%20%20Existing%20open-vocabulary%20image%20segmentation%20methods%20require%20a%20fine-tuning%0Astep%20on%20mask%20labels%20and/or%20image-text%20datasets.%20Mask%20labels%20are%0Alabor-intensive%2C%20which%20limits%20the%20number%20of%20categories%20in%20segmentation%0Adatasets.%20Consequently%2C%20the%20vocabulary%20capacity%20of%20pre-trained%20VLMs%20is%20severely%0Areduced%20after%20fine-tuning.%20However%2C%20without%20fine-tuning%2C%20VLMs%20trained%20under%0Aweak%20image-text%20supervision%20tend%20to%20make%20suboptimal%20mask%20predictions.%20To%0Aalleviate%20these%20issues%2C%20we%20introduce%20a%20novel%20recurrent%20framework%20that%0Aprogressively%20filters%20out%20irrelevant%20texts%20and%20enhances%20mask%20quality%20without%0Atraining%20efforts.%20The%20recurrent%20unit%20is%20a%20two-stage%20segmenter%20built%20upon%20a%0Afrozen%20VLM.%20Thus%2C%20our%20model%20retains%20the%20VLM%27s%20broad%20vocabulary%20space%20and%20equips%0Ait%20with%20segmentation%20ability.%20Experiments%20show%20that%20our%20method%20outperforms%20not%0Aonly%20the%20training-free%20counterparts%2C%20but%20also%20those%20fine-tuned%20with%20millions%20of%0Adata%20samples%2C%20and%20sets%20the%20new%20state-of-the-art%20records%20for%20both%20zero-shot%0Asemantic%20and%20referring%20segmentation.%20Concretely%2C%20we%20improve%20the%20current%20record%0Aby%2028.8%2C%2016.0%2C%20and%206.9%20mIoU%20on%20Pascal%20VOC%2C%20COCO%20Object%2C%20and%20Pascal%20Context.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07661v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP%2520as%2520RNN%253A%2520Segment%2520Countless%2520Visual%2520Concepts%2520without%2520Training%2520Endeavor%26entry.906535625%3DShuyang%2520Sun%2520and%2520Runjia%2520Li%2520and%2520Philip%2520Torr%2520and%2520Xiuye%2520Gu%2520and%2520Siyang%2520Li%26entry.1292438233%3D%2520%2520Existing%2520open-vocabulary%2520image%2520segmentation%2520methods%2520require%2520a%2520fine-tuning%250Astep%2520on%2520mask%2520labels%2520and/or%2520image-text%2520datasets.%2520Mask%2520labels%2520are%250Alabor-intensive%252C%2520which%2520limits%2520the%2520number%2520of%2520categories%2520in%2520segmentation%250Adatasets.%2520Consequently%252C%2520the%2520vocabulary%2520capacity%2520of%2520pre-trained%2520VLMs%2520is%2520severely%250Areduced%2520after%2520fine-tuning.%2520However%252C%2520without%2520fine-tuning%252C%2520VLMs%2520trained%2520under%250Aweak%2520image-text%2520supervision%2520tend%2520to%2520make%2520suboptimal%2520mask%2520predictions.%2520To%250Aalleviate%2520these%2520issues%252C%2520we%2520introduce%2520a%2520novel%2520recurrent%2520framework%2520that%250Aprogressively%2520filters%2520out%2520irrelevant%2520texts%2520and%2520enhances%2520mask%2520quality%2520without%250Atraining%2520efforts.%2520The%2520recurrent%2520unit%2520is%2520a%2520two-stage%2520segmenter%2520built%2520upon%2520a%250Afrozen%2520VLM.%2520Thus%252C%2520our%2520model%2520retains%2520the%2520VLM%2527s%2520broad%2520vocabulary%2520space%2520and%2520equips%250Ait%2520with%2520segmentation%2520ability.%2520Experiments%2520show%2520that%2520our%2520method%2520outperforms%2520not%250Aonly%2520the%2520training-free%2520counterparts%252C%2520but%2520also%2520those%2520fine-tuned%2520with%2520millions%2520of%250Adata%2520samples%252C%2520and%2520sets%2520the%2520new%2520state-of-the-art%2520records%2520for%2520both%2520zero-shot%250Asemantic%2520and%2520referring%2520segmentation.%2520Concretely%252C%2520we%2520improve%2520the%2520current%2520record%250Aby%252028.8%252C%252016.0%252C%2520and%25206.9%2520mIoU%2520on%2520Pascal%2520VOC%252C%2520COCO%2520Object%252C%2520and%2520Pascal%2520Context.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07661v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP%20as%20RNN%3A%20Segment%20Countless%20Visual%20Concepts%20without%20Training%20Endeavor&entry.906535625=Shuyang%20Sun%20and%20Runjia%20Li%20and%20Philip%20Torr%20and%20Xiuye%20Gu%20and%20Siyang%20Li&entry.1292438233=%20%20Existing%20open-vocabulary%20image%20segmentation%20methods%20require%20a%20fine-tuning%0Astep%20on%20mask%20labels%20and/or%20image-text%20datasets.%20Mask%20labels%20are%0Alabor-intensive%2C%20which%20limits%20the%20number%20of%20categories%20in%20segmentation%0Adatasets.%20Consequently%2C%20the%20vocabulary%20capacity%20of%20pre-trained%20VLMs%20is%20severely%0Areduced%20after%20fine-tuning.%20However%2C%20without%20fine-tuning%2C%20VLMs%20trained%20under%0Aweak%20image-text%20supervision%20tend%20to%20make%20suboptimal%20mask%20predictions.%20To%0Aalleviate%20these%20issues%2C%20we%20introduce%20a%20novel%20recurrent%20framework%20that%0Aprogressively%20filters%20out%20irrelevant%20texts%20and%20enhances%20mask%20quality%20without%0Atraining%20efforts.%20The%20recurrent%20unit%20is%20a%20two-stage%20segmenter%20built%20upon%20a%0Afrozen%20VLM.%20Thus%2C%20our%20model%20retains%20the%20VLM%27s%20broad%20vocabulary%20space%20and%20equips%0Ait%20with%20segmentation%20ability.%20Experiments%20show%20that%20our%20method%20outperforms%20not%0Aonly%20the%20training-free%20counterparts%2C%20but%20also%20those%20fine-tuned%20with%20millions%20of%0Adata%20samples%2C%20and%20sets%20the%20new%20state-of-the-art%20records%20for%20both%20zero-shot%0Asemantic%20and%20referring%20segmentation.%20Concretely%2C%20we%20improve%20the%20current%20record%0Aby%2028.8%2C%2016.0%2C%20and%206.9%20mIoU%20on%20Pascal%20VOC%2C%20COCO%20Object%2C%20and%20Pascal%20Context.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07661v3&entry.124074799=Read"},
{"title": "CLIP-KD: An Empirical Study of CLIP Model Distillation", "author": "Chuanguang Yang and Zhulin An and Libo Huang and Junyu Bi and Xinqiang Yu and Han Yang and Boyu Diao and Yongjun Xu", "abstract": "  Contrastive Language-Image Pre-training (CLIP) has become a promising\nlanguage-supervised visual pre-training framework. This paper aims to distill\nsmall CLIP models supervised by a large teacher CLIP model. We propose several\ndistillation strategies, including relation, feature, gradient and contrastive\nparadigms, to examine the effectiveness of CLIP-Knowledge Distillation (KD). We\nshow that a simple feature mimicry with Mean Squared Error loss works\nsurprisingly well. Moreover, interactive contrastive learning across teacher\nand student encoders is also effective in performance improvement. We explain\nthat the success of CLIP-KD can be attributed to maximizing the feature\nsimilarity between teacher and student. The unified method is applied to\ndistill several student models trained on CC3M+12M. CLIP-KD improves student\nCLIP models consistently over zero-shot ImageNet classification and cross-modal\nretrieval benchmarks. When using ViT-L/14 pretrained on Laion-400M as the\nteacher, CLIP-KD achieves 57.5\\% and 55.4\\% zero-shot top-1 ImageNet accuracy\nover ViT-B/16 and ResNet-50, surpassing the original CLIP without KD by 20.5\\%\nand 20.1\\% margins, respectively. Our code is released on\nhttps://github.com/winycg/CLIP-KD.\n", "link": "http://arxiv.org/abs/2307.12732v2", "date": "2024-05-07", "relevancy": 2.126, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5679}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5078}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP-KD%3A%20An%20Empirical%20Study%20of%20CLIP%20Model%20Distillation&body=Title%3A%20CLIP-KD%3A%20An%20Empirical%20Study%20of%20CLIP%20Model%20Distillation%0AAuthor%3A%20Chuanguang%20Yang%20and%20Zhulin%20An%20and%20Libo%20Huang%20and%20Junyu%20Bi%20and%20Xinqiang%20Yu%20and%20Han%20Yang%20and%20Boyu%20Diao%20and%20Yongjun%20Xu%0AAbstract%3A%20%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20has%20become%20a%20promising%0Alanguage-supervised%20visual%20pre-training%20framework.%20This%20paper%20aims%20to%20distill%0Asmall%20CLIP%20models%20supervised%20by%20a%20large%20teacher%20CLIP%20model.%20We%20propose%20several%0Adistillation%20strategies%2C%20including%20relation%2C%20feature%2C%20gradient%20and%20contrastive%0Aparadigms%2C%20to%20examine%20the%20effectiveness%20of%20CLIP-Knowledge%20Distillation%20%28KD%29.%20We%0Ashow%20that%20a%20simple%20feature%20mimicry%20with%20Mean%20Squared%20Error%20loss%20works%0Asurprisingly%20well.%20Moreover%2C%20interactive%20contrastive%20learning%20across%20teacher%0Aand%20student%20encoders%20is%20also%20effective%20in%20performance%20improvement.%20We%20explain%0Athat%20the%20success%20of%20CLIP-KD%20can%20be%20attributed%20to%20maximizing%20the%20feature%0Asimilarity%20between%20teacher%20and%20student.%20The%20unified%20method%20is%20applied%20to%0Adistill%20several%20student%20models%20trained%20on%20CC3M%2B12M.%20CLIP-KD%20improves%20student%0ACLIP%20models%20consistently%20over%20zero-shot%20ImageNet%20classification%20and%20cross-modal%0Aretrieval%20benchmarks.%20When%20using%20ViT-L/14%20pretrained%20on%20Laion-400M%20as%20the%0Ateacher%2C%20CLIP-KD%20achieves%2057.5%5C%25%20and%2055.4%5C%25%20zero-shot%20top-1%20ImageNet%20accuracy%0Aover%20ViT-B/16%20and%20ResNet-50%2C%20surpassing%20the%20original%20CLIP%20without%20KD%20by%2020.5%5C%25%0Aand%2020.1%5C%25%20margins%2C%20respectively.%20Our%20code%20is%20released%20on%0Ahttps%3A//github.com/winycg/CLIP-KD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.12732v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP-KD%253A%2520An%2520Empirical%2520Study%2520of%2520CLIP%2520Model%2520Distillation%26entry.906535625%3DChuanguang%2520Yang%2520and%2520Zhulin%2520An%2520and%2520Libo%2520Huang%2520and%2520Junyu%2520Bi%2520and%2520Xinqiang%2520Yu%2520and%2520Han%2520Yang%2520and%2520Boyu%2520Diao%2520and%2520Yongjun%2520Xu%26entry.1292438233%3D%2520%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520has%2520become%2520a%2520promising%250Alanguage-supervised%2520visual%2520pre-training%2520framework.%2520This%2520paper%2520aims%2520to%2520distill%250Asmall%2520CLIP%2520models%2520supervised%2520by%2520a%2520large%2520teacher%2520CLIP%2520model.%2520We%2520propose%2520several%250Adistillation%2520strategies%252C%2520including%2520relation%252C%2520feature%252C%2520gradient%2520and%2520contrastive%250Aparadigms%252C%2520to%2520examine%2520the%2520effectiveness%2520of%2520CLIP-Knowledge%2520Distillation%2520%2528KD%2529.%2520We%250Ashow%2520that%2520a%2520simple%2520feature%2520mimicry%2520with%2520Mean%2520Squared%2520Error%2520loss%2520works%250Asurprisingly%2520well.%2520Moreover%252C%2520interactive%2520contrastive%2520learning%2520across%2520teacher%250Aand%2520student%2520encoders%2520is%2520also%2520effective%2520in%2520performance%2520improvement.%2520We%2520explain%250Athat%2520the%2520success%2520of%2520CLIP-KD%2520can%2520be%2520attributed%2520to%2520maximizing%2520the%2520feature%250Asimilarity%2520between%2520teacher%2520and%2520student.%2520The%2520unified%2520method%2520is%2520applied%2520to%250Adistill%2520several%2520student%2520models%2520trained%2520on%2520CC3M%252B12M.%2520CLIP-KD%2520improves%2520student%250ACLIP%2520models%2520consistently%2520over%2520zero-shot%2520ImageNet%2520classification%2520and%2520cross-modal%250Aretrieval%2520benchmarks.%2520When%2520using%2520ViT-L/14%2520pretrained%2520on%2520Laion-400M%2520as%2520the%250Ateacher%252C%2520CLIP-KD%2520achieves%252057.5%255C%2525%2520and%252055.4%255C%2525%2520zero-shot%2520top-1%2520ImageNet%2520accuracy%250Aover%2520ViT-B/16%2520and%2520ResNet-50%252C%2520surpassing%2520the%2520original%2520CLIP%2520without%2520KD%2520by%252020.5%255C%2525%250Aand%252020.1%255C%2525%2520margins%252C%2520respectively.%2520Our%2520code%2520is%2520released%2520on%250Ahttps%253A//github.com/winycg/CLIP-KD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.12732v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP-KD%3A%20An%20Empirical%20Study%20of%20CLIP%20Model%20Distillation&entry.906535625=Chuanguang%20Yang%20and%20Zhulin%20An%20and%20Libo%20Huang%20and%20Junyu%20Bi%20and%20Xinqiang%20Yu%20and%20Han%20Yang%20and%20Boyu%20Diao%20and%20Yongjun%20Xu&entry.1292438233=%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20has%20become%20a%20promising%0Alanguage-supervised%20visual%20pre-training%20framework.%20This%20paper%20aims%20to%20distill%0Asmall%20CLIP%20models%20supervised%20by%20a%20large%20teacher%20CLIP%20model.%20We%20propose%20several%0Adistillation%20strategies%2C%20including%20relation%2C%20feature%2C%20gradient%20and%20contrastive%0Aparadigms%2C%20to%20examine%20the%20effectiveness%20of%20CLIP-Knowledge%20Distillation%20%28KD%29.%20We%0Ashow%20that%20a%20simple%20feature%20mimicry%20with%20Mean%20Squared%20Error%20loss%20works%0Asurprisingly%20well.%20Moreover%2C%20interactive%20contrastive%20learning%20across%20teacher%0Aand%20student%20encoders%20is%20also%20effective%20in%20performance%20improvement.%20We%20explain%0Athat%20the%20success%20of%20CLIP-KD%20can%20be%20attributed%20to%20maximizing%20the%20feature%0Asimilarity%20between%20teacher%20and%20student.%20The%20unified%20method%20is%20applied%20to%0Adistill%20several%20student%20models%20trained%20on%20CC3M%2B12M.%20CLIP-KD%20improves%20student%0ACLIP%20models%20consistently%20over%20zero-shot%20ImageNet%20classification%20and%20cross-modal%0Aretrieval%20benchmarks.%20When%20using%20ViT-L/14%20pretrained%20on%20Laion-400M%20as%20the%0Ateacher%2C%20CLIP-KD%20achieves%2057.5%5C%25%20and%2055.4%5C%25%20zero-shot%20top-1%20ImageNet%20accuracy%0Aover%20ViT-B/16%20and%20ResNet-50%2C%20surpassing%20the%20original%20CLIP%20without%20KD%20by%2020.5%5C%25%0Aand%2020.1%5C%25%20margins%2C%20respectively.%20Our%20code%20is%20released%20on%0Ahttps%3A//github.com/winycg/CLIP-KD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.12732v2&entry.124074799=Read"},
{"title": "Cross-IQA: Unsupervised Learning for Image Quality Assessment", "author": "Zhen Zhang", "abstract": "  Automatic perception of image quality is a challenging problem that impacts\nbillions of Internet and social media users daily. To advance research in this\nfield, we propose a no-reference image quality assessment (NR-IQA) method\ntermed Cross-IQA based on vision transformer(ViT) model. The proposed Cross-IQA\nmethod can learn image quality features from unlabeled image data. We construct\nthe pretext task of synthesized image reconstruction to unsupervised extract\nthe image quality information based ViT block. The pretrained encoder of\nCross-IQA is used to fine-tune a linear regression model for score prediction.\nExperimental results show that Cross-IQA can achieve state-of-the-art\nperformance in assessing the low-frequency degradation information (e.g., color\nchange, blurring, etc.) of images compared with the classical full-reference\nIQA and NR-IQA under the same datasets.\n", "link": "http://arxiv.org/abs/2405.04311v1", "date": "2024-05-07", "relevancy": 2.1148, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.553}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5374}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-IQA%3A%20Unsupervised%20Learning%20for%20Image%20Quality%20Assessment&body=Title%3A%20Cross-IQA%3A%20Unsupervised%20Learning%20for%20Image%20Quality%20Assessment%0AAuthor%3A%20Zhen%20Zhang%0AAbstract%3A%20%20%20Automatic%20perception%20of%20image%20quality%20is%20a%20challenging%20problem%20that%20impacts%0Abillions%20of%20Internet%20and%20social%20media%20users%20daily.%20To%20advance%20research%20in%20this%0Afield%2C%20we%20propose%20a%20no-reference%20image%20quality%20assessment%20%28NR-IQA%29%20method%0Atermed%20Cross-IQA%20based%20on%20vision%20transformer%28ViT%29%20model.%20The%20proposed%20Cross-IQA%0Amethod%20can%20learn%20image%20quality%20features%20from%20unlabeled%20image%20data.%20We%20construct%0Athe%20pretext%20task%20of%20synthesized%20image%20reconstruction%20to%20unsupervised%20extract%0Athe%20image%20quality%20information%20based%20ViT%20block.%20The%20pretrained%20encoder%20of%0ACross-IQA%20is%20used%20to%20fine-tune%20a%20linear%20regression%20model%20for%20score%20prediction.%0AExperimental%20results%20show%20that%20Cross-IQA%20can%20achieve%20state-of-the-art%0Aperformance%20in%20assessing%20the%20low-frequency%20degradation%20information%20%28e.g.%2C%20color%0Achange%2C%20blurring%2C%20etc.%29%20of%20images%20compared%20with%20the%20classical%20full-reference%0AIQA%20and%20NR-IQA%20under%20the%20same%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04311v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-IQA%253A%2520Unsupervised%2520Learning%2520for%2520Image%2520Quality%2520Assessment%26entry.906535625%3DZhen%2520Zhang%26entry.1292438233%3D%2520%2520Automatic%2520perception%2520of%2520image%2520quality%2520is%2520a%2520challenging%2520problem%2520that%2520impacts%250Abillions%2520of%2520Internet%2520and%2520social%2520media%2520users%2520daily.%2520To%2520advance%2520research%2520in%2520this%250Afield%252C%2520we%2520propose%2520a%2520no-reference%2520image%2520quality%2520assessment%2520%2528NR-IQA%2529%2520method%250Atermed%2520Cross-IQA%2520based%2520on%2520vision%2520transformer%2528ViT%2529%2520model.%2520The%2520proposed%2520Cross-IQA%250Amethod%2520can%2520learn%2520image%2520quality%2520features%2520from%2520unlabeled%2520image%2520data.%2520We%2520construct%250Athe%2520pretext%2520task%2520of%2520synthesized%2520image%2520reconstruction%2520to%2520unsupervised%2520extract%250Athe%2520image%2520quality%2520information%2520based%2520ViT%2520block.%2520The%2520pretrained%2520encoder%2520of%250ACross-IQA%2520is%2520used%2520to%2520fine-tune%2520a%2520linear%2520regression%2520model%2520for%2520score%2520prediction.%250AExperimental%2520results%2520show%2520that%2520Cross-IQA%2520can%2520achieve%2520state-of-the-art%250Aperformance%2520in%2520assessing%2520the%2520low-frequency%2520degradation%2520information%2520%2528e.g.%252C%2520color%250Achange%252C%2520blurring%252C%2520etc.%2529%2520of%2520images%2520compared%2520with%2520the%2520classical%2520full-reference%250AIQA%2520and%2520NR-IQA%2520under%2520the%2520same%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04311v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-IQA%3A%20Unsupervised%20Learning%20for%20Image%20Quality%20Assessment&entry.906535625=Zhen%20Zhang&entry.1292438233=%20%20Automatic%20perception%20of%20image%20quality%20is%20a%20challenging%20problem%20that%20impacts%0Abillions%20of%20Internet%20and%20social%20media%20users%20daily.%20To%20advance%20research%20in%20this%0Afield%2C%20we%20propose%20a%20no-reference%20image%20quality%20assessment%20%28NR-IQA%29%20method%0Atermed%20Cross-IQA%20based%20on%20vision%20transformer%28ViT%29%20model.%20The%20proposed%20Cross-IQA%0Amethod%20can%20learn%20image%20quality%20features%20from%20unlabeled%20image%20data.%20We%20construct%0Athe%20pretext%20task%20of%20synthesized%20image%20reconstruction%20to%20unsupervised%20extract%0Athe%20image%20quality%20information%20based%20ViT%20block.%20The%20pretrained%20encoder%20of%0ACross-IQA%20is%20used%20to%20fine-tune%20a%20linear%20regression%20model%20for%20score%20prediction.%0AExperimental%20results%20show%20that%20Cross-IQA%20can%20achieve%20state-of-the-art%0Aperformance%20in%20assessing%20the%20low-frequency%20degradation%20information%20%28e.g.%2C%20color%0Achange%2C%20blurring%2C%20etc.%29%20of%20images%20compared%20with%20the%20classical%20full-reference%0AIQA%20and%20NR-IQA%20under%20the%20same%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04311v1&entry.124074799=Read"},
{"title": "G$ \\mathbf{^2} $VD Planner: Efficient Motion Planning With Grid-based\n  Generalized Voronoi Diagrams", "author": "Jian Wen and Xuebo Zhang and Qingchen Bi and Hui Liu and Jing Yuan and Yongchun Fang", "abstract": "  In this paper, an efficient motion planning approach with grid-based\ngeneralized Voronoi diagrams (G$ \\mathbf{^2} $VD) is newly proposed for mobile\nrobots. Different from existing approaches, the novelty of this work is\ntwofold: 1) a new state lattice-based path searching approach is proposed, in\nwhich the search space is reduced to a novel Voronoi corridor to further\nimprove the search efficiency; 2) an efficient quadratic programming-based path\nsmoothing approach is presented, wherein the clearance to obstacles is\nconsidered to improve the path clearance of hard-constrained path smoothing\napproaches. We validate the efficiency and smoothness of our approach in\nvarious challenging simulation scenarios and outdoor environments. It is shown\nthat the computational efficiency is improved by 17.1% in the path searching\nstage, and path smoothing with the proposed approach is 6.6 times faster than\nan advanced sparse-banded structure-based path smoothing approach and 53.3\ntimes faster than the popular timed-elastic-band planner. A video showing\noutdoor navigation on our campus is available at https://youtu.be/iMXGthgvp58.\n", "link": "http://arxiv.org/abs/2201.12981v4", "date": "2024-05-07", "relevancy": 2.1139, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.531}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5283}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G%24%20%5Cmathbf%7B%5E2%7D%20%24VD%20Planner%3A%20Efficient%20Motion%20Planning%20With%20Grid-based%0A%20%20Generalized%20Voronoi%20Diagrams&body=Title%3A%20G%24%20%5Cmathbf%7B%5E2%7D%20%24VD%20Planner%3A%20Efficient%20Motion%20Planning%20With%20Grid-based%0A%20%20Generalized%20Voronoi%20Diagrams%0AAuthor%3A%20Jian%20Wen%20and%20Xuebo%20Zhang%20and%20Qingchen%20Bi%20and%20Hui%20Liu%20and%20Jing%20Yuan%20and%20Yongchun%20Fang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20an%20efficient%20motion%20planning%20approach%20with%20grid-based%0Ageneralized%20Voronoi%20diagrams%20%28G%24%20%5Cmathbf%7B%5E2%7D%20%24VD%29%20is%20newly%20proposed%20for%20mobile%0Arobots.%20Different%20from%20existing%20approaches%2C%20the%20novelty%20of%20this%20work%20is%0Atwofold%3A%201%29%20a%20new%20state%20lattice-based%20path%20searching%20approach%20is%20proposed%2C%20in%0Awhich%20the%20search%20space%20is%20reduced%20to%20a%20novel%20Voronoi%20corridor%20to%20further%0Aimprove%20the%20search%20efficiency%3B%202%29%20an%20efficient%20quadratic%20programming-based%20path%0Asmoothing%20approach%20is%20presented%2C%20wherein%20the%20clearance%20to%20obstacles%20is%0Aconsidered%20to%20improve%20the%20path%20clearance%20of%20hard-constrained%20path%20smoothing%0Aapproaches.%20We%20validate%20the%20efficiency%20and%20smoothness%20of%20our%20approach%20in%0Avarious%20challenging%20simulation%20scenarios%20and%20outdoor%20environments.%20It%20is%20shown%0Athat%20the%20computational%20efficiency%20is%20improved%20by%2017.1%25%20in%20the%20path%20searching%0Astage%2C%20and%20path%20smoothing%20with%20the%20proposed%20approach%20is%206.6%20times%20faster%20than%0Aan%20advanced%20sparse-banded%20structure-based%20path%20smoothing%20approach%20and%2053.3%0Atimes%20faster%20than%20the%20popular%20timed-elastic-band%20planner.%20A%20video%20showing%0Aoutdoor%20navigation%20on%20our%20campus%20is%20available%20at%20https%3A//youtu.be/iMXGthgvp58.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2201.12981v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG%2524%2520%255Cmathbf%257B%255E2%257D%2520%2524VD%2520Planner%253A%2520Efficient%2520Motion%2520Planning%2520With%2520Grid-based%250A%2520%2520Generalized%2520Voronoi%2520Diagrams%26entry.906535625%3DJian%2520Wen%2520and%2520Xuebo%2520Zhang%2520and%2520Qingchen%2520Bi%2520and%2520Hui%2520Liu%2520and%2520Jing%2520Yuan%2520and%2520Yongchun%2520Fang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520an%2520efficient%2520motion%2520planning%2520approach%2520with%2520grid-based%250Ageneralized%2520Voronoi%2520diagrams%2520%2528G%2524%2520%255Cmathbf%257B%255E2%257D%2520%2524VD%2529%2520is%2520newly%2520proposed%2520for%2520mobile%250Arobots.%2520Different%2520from%2520existing%2520approaches%252C%2520the%2520novelty%2520of%2520this%2520work%2520is%250Atwofold%253A%25201%2529%2520a%2520new%2520state%2520lattice-based%2520path%2520searching%2520approach%2520is%2520proposed%252C%2520in%250Awhich%2520the%2520search%2520space%2520is%2520reduced%2520to%2520a%2520novel%2520Voronoi%2520corridor%2520to%2520further%250Aimprove%2520the%2520search%2520efficiency%253B%25202%2529%2520an%2520efficient%2520quadratic%2520programming-based%2520path%250Asmoothing%2520approach%2520is%2520presented%252C%2520wherein%2520the%2520clearance%2520to%2520obstacles%2520is%250Aconsidered%2520to%2520improve%2520the%2520path%2520clearance%2520of%2520hard-constrained%2520path%2520smoothing%250Aapproaches.%2520We%2520validate%2520the%2520efficiency%2520and%2520smoothness%2520of%2520our%2520approach%2520in%250Avarious%2520challenging%2520simulation%2520scenarios%2520and%2520outdoor%2520environments.%2520It%2520is%2520shown%250Athat%2520the%2520computational%2520efficiency%2520is%2520improved%2520by%252017.1%2525%2520in%2520the%2520path%2520searching%250Astage%252C%2520and%2520path%2520smoothing%2520with%2520the%2520proposed%2520approach%2520is%25206.6%2520times%2520faster%2520than%250Aan%2520advanced%2520sparse-banded%2520structure-based%2520path%2520smoothing%2520approach%2520and%252053.3%250Atimes%2520faster%2520than%2520the%2520popular%2520timed-elastic-band%2520planner.%2520A%2520video%2520showing%250Aoutdoor%2520navigation%2520on%2520our%2520campus%2520is%2520available%2520at%2520https%253A//youtu.be/iMXGthgvp58.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2201.12981v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G%24%20%5Cmathbf%7B%5E2%7D%20%24VD%20Planner%3A%20Efficient%20Motion%20Planning%20With%20Grid-based%0A%20%20Generalized%20Voronoi%20Diagrams&entry.906535625=Jian%20Wen%20and%20Xuebo%20Zhang%20and%20Qingchen%20Bi%20and%20Hui%20Liu%20and%20Jing%20Yuan%20and%20Yongchun%20Fang&entry.1292438233=%20%20In%20this%20paper%2C%20an%20efficient%20motion%20planning%20approach%20with%20grid-based%0Ageneralized%20Voronoi%20diagrams%20%28G%24%20%5Cmathbf%7B%5E2%7D%20%24VD%29%20is%20newly%20proposed%20for%20mobile%0Arobots.%20Different%20from%20existing%20approaches%2C%20the%20novelty%20of%20this%20work%20is%0Atwofold%3A%201%29%20a%20new%20state%20lattice-based%20path%20searching%20approach%20is%20proposed%2C%20in%0Awhich%20the%20search%20space%20is%20reduced%20to%20a%20novel%20Voronoi%20corridor%20to%20further%0Aimprove%20the%20search%20efficiency%3B%202%29%20an%20efficient%20quadratic%20programming-based%20path%0Asmoothing%20approach%20is%20presented%2C%20wherein%20the%20clearance%20to%20obstacles%20is%0Aconsidered%20to%20improve%20the%20path%20clearance%20of%20hard-constrained%20path%20smoothing%0Aapproaches.%20We%20validate%20the%20efficiency%20and%20smoothness%20of%20our%20approach%20in%0Avarious%20challenging%20simulation%20scenarios%20and%20outdoor%20environments.%20It%20is%20shown%0Athat%20the%20computational%20efficiency%20is%20improved%20by%2017.1%25%20in%20the%20path%20searching%0Astage%2C%20and%20path%20smoothing%20with%20the%20proposed%20approach%20is%206.6%20times%20faster%20than%0Aan%20advanced%20sparse-banded%20structure-based%20path%20smoothing%20approach%20and%2053.3%0Atimes%20faster%20than%20the%20popular%20timed-elastic-band%20planner.%20A%20video%20showing%0Aoutdoor%20navigation%20on%20our%20campus%20is%20available%20at%20https%3A//youtu.be/iMXGthgvp58.%0A&entry.1838667208=http%3A//arxiv.org/abs/2201.12981v4&entry.124074799=Read"},
{"title": "Paint-it: Text-to-Texture Synthesis via Deep Convolutional Texture Map\n  Optimization and Physically-Based Rendering", "author": "Kim Youwang and Tae-Hyun Oh and Gerard Pons-Moll", "abstract": "  We present Paint-it, a text-driven high-fidelity texture map synthesis method\nfor 3D meshes via neural re-parameterized texture optimization. Paint-it\nsynthesizes texture maps from a text description by\nsynthesis-through-optimization, exploiting the Score-Distillation Sampling\n(SDS). We observe that directly applying SDS yields undesirable texture quality\ndue to its noisy gradients. We reveal the importance of texture\nparameterization when using SDS. Specifically, we propose Deep Convolutional\nPhysically-Based Rendering (DC-PBR) parameterization, which re-parameterizes\nthe physically-based rendering (PBR) texture maps with randomly initialized\nconvolution-based neural kernels, instead of a standard pixel-based\nparameterization. We show that DC-PBR inherently schedules the optimization\ncurriculum according to texture frequency and naturally filters out the noisy\nsignals from SDS. In experiments, Paint-it obtains remarkable quality PBR\ntexture maps within 15 min., given only a text description. We demonstrate the\ngeneralizability and practicality of Paint-it by synthesizing high-quality\ntexture maps for large-scale mesh datasets and showing test-time applications\nsuch as relighting and material control using a popular graphics engine.\nProject page: https://kim-youwang.github.io/paint-it\n", "link": "http://arxiv.org/abs/2312.11360v2", "date": "2024-05-07", "relevancy": 2.1017, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5362}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5242}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Paint-it%3A%20Text-to-Texture%20Synthesis%20via%20Deep%20Convolutional%20Texture%20Map%0A%20%20Optimization%20and%20Physically-Based%20Rendering&body=Title%3A%20Paint-it%3A%20Text-to-Texture%20Synthesis%20via%20Deep%20Convolutional%20Texture%20Map%0A%20%20Optimization%20and%20Physically-Based%20Rendering%0AAuthor%3A%20Kim%20Youwang%20and%20Tae-Hyun%20Oh%20and%20Gerard%20Pons-Moll%0AAbstract%3A%20%20%20We%20present%20Paint-it%2C%20a%20text-driven%20high-fidelity%20texture%20map%20synthesis%20method%0Afor%203D%20meshes%20via%20neural%20re-parameterized%20texture%20optimization.%20Paint-it%0Asynthesizes%20texture%20maps%20from%20a%20text%20description%20by%0Asynthesis-through-optimization%2C%20exploiting%20the%20Score-Distillation%20Sampling%0A%28SDS%29.%20We%20observe%20that%20directly%20applying%20SDS%20yields%20undesirable%20texture%20quality%0Adue%20to%20its%20noisy%20gradients.%20We%20reveal%20the%20importance%20of%20texture%0Aparameterization%20when%20using%20SDS.%20Specifically%2C%20we%20propose%20Deep%20Convolutional%0APhysically-Based%20Rendering%20%28DC-PBR%29%20parameterization%2C%20which%20re-parameterizes%0Athe%20physically-based%20rendering%20%28PBR%29%20texture%20maps%20with%20randomly%20initialized%0Aconvolution-based%20neural%20kernels%2C%20instead%20of%20a%20standard%20pixel-based%0Aparameterization.%20We%20show%20that%20DC-PBR%20inherently%20schedules%20the%20optimization%0Acurriculum%20according%20to%20texture%20frequency%20and%20naturally%20filters%20out%20the%20noisy%0Asignals%20from%20SDS.%20In%20experiments%2C%20Paint-it%20obtains%20remarkable%20quality%20PBR%0Atexture%20maps%20within%2015%20min.%2C%20given%20only%20a%20text%20description.%20We%20demonstrate%20the%0Ageneralizability%20and%20practicality%20of%20Paint-it%20by%20synthesizing%20high-quality%0Atexture%20maps%20for%20large-scale%20mesh%20datasets%20and%20showing%20test-time%20applications%0Asuch%20as%20relighting%20and%20material%20control%20using%20a%20popular%20graphics%20engine.%0AProject%20page%3A%20https%3A//kim-youwang.github.io/paint-it%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.11360v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPaint-it%253A%2520Text-to-Texture%2520Synthesis%2520via%2520Deep%2520Convolutional%2520Texture%2520Map%250A%2520%2520Optimization%2520and%2520Physically-Based%2520Rendering%26entry.906535625%3DKim%2520Youwang%2520and%2520Tae-Hyun%2520Oh%2520and%2520Gerard%2520Pons-Moll%26entry.1292438233%3D%2520%2520We%2520present%2520Paint-it%252C%2520a%2520text-driven%2520high-fidelity%2520texture%2520map%2520synthesis%2520method%250Afor%25203D%2520meshes%2520via%2520neural%2520re-parameterized%2520texture%2520optimization.%2520Paint-it%250Asynthesizes%2520texture%2520maps%2520from%2520a%2520text%2520description%2520by%250Asynthesis-through-optimization%252C%2520exploiting%2520the%2520Score-Distillation%2520Sampling%250A%2528SDS%2529.%2520We%2520observe%2520that%2520directly%2520applying%2520SDS%2520yields%2520undesirable%2520texture%2520quality%250Adue%2520to%2520its%2520noisy%2520gradients.%2520We%2520reveal%2520the%2520importance%2520of%2520texture%250Aparameterization%2520when%2520using%2520SDS.%2520Specifically%252C%2520we%2520propose%2520Deep%2520Convolutional%250APhysically-Based%2520Rendering%2520%2528DC-PBR%2529%2520parameterization%252C%2520which%2520re-parameterizes%250Athe%2520physically-based%2520rendering%2520%2528PBR%2529%2520texture%2520maps%2520with%2520randomly%2520initialized%250Aconvolution-based%2520neural%2520kernels%252C%2520instead%2520of%2520a%2520standard%2520pixel-based%250Aparameterization.%2520We%2520show%2520that%2520DC-PBR%2520inherently%2520schedules%2520the%2520optimization%250Acurriculum%2520according%2520to%2520texture%2520frequency%2520and%2520naturally%2520filters%2520out%2520the%2520noisy%250Asignals%2520from%2520SDS.%2520In%2520experiments%252C%2520Paint-it%2520obtains%2520remarkable%2520quality%2520PBR%250Atexture%2520maps%2520within%252015%2520min.%252C%2520given%2520only%2520a%2520text%2520description.%2520We%2520demonstrate%2520the%250Ageneralizability%2520and%2520practicality%2520of%2520Paint-it%2520by%2520synthesizing%2520high-quality%250Atexture%2520maps%2520for%2520large-scale%2520mesh%2520datasets%2520and%2520showing%2520test-time%2520applications%250Asuch%2520as%2520relighting%2520and%2520material%2520control%2520using%2520a%2520popular%2520graphics%2520engine.%250AProject%2520page%253A%2520https%253A//kim-youwang.github.io/paint-it%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.11360v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Paint-it%3A%20Text-to-Texture%20Synthesis%20via%20Deep%20Convolutional%20Texture%20Map%0A%20%20Optimization%20and%20Physically-Based%20Rendering&entry.906535625=Kim%20Youwang%20and%20Tae-Hyun%20Oh%20and%20Gerard%20Pons-Moll&entry.1292438233=%20%20We%20present%20Paint-it%2C%20a%20text-driven%20high-fidelity%20texture%20map%20synthesis%20method%0Afor%203D%20meshes%20via%20neural%20re-parameterized%20texture%20optimization.%20Paint-it%0Asynthesizes%20texture%20maps%20from%20a%20text%20description%20by%0Asynthesis-through-optimization%2C%20exploiting%20the%20Score-Distillation%20Sampling%0A%28SDS%29.%20We%20observe%20that%20directly%20applying%20SDS%20yields%20undesirable%20texture%20quality%0Adue%20to%20its%20noisy%20gradients.%20We%20reveal%20the%20importance%20of%20texture%0Aparameterization%20when%20using%20SDS.%20Specifically%2C%20we%20propose%20Deep%20Convolutional%0APhysically-Based%20Rendering%20%28DC-PBR%29%20parameterization%2C%20which%20re-parameterizes%0Athe%20physically-based%20rendering%20%28PBR%29%20texture%20maps%20with%20randomly%20initialized%0Aconvolution-based%20neural%20kernels%2C%20instead%20of%20a%20standard%20pixel-based%0Aparameterization.%20We%20show%20that%20DC-PBR%20inherently%20schedules%20the%20optimization%0Acurriculum%20according%20to%20texture%20frequency%20and%20naturally%20filters%20out%20the%20noisy%0Asignals%20from%20SDS.%20In%20experiments%2C%20Paint-it%20obtains%20remarkable%20quality%20PBR%0Atexture%20maps%20within%2015%20min.%2C%20given%20only%20a%20text%20description.%20We%20demonstrate%20the%0Ageneralizability%20and%20practicality%20of%20Paint-it%20by%20synthesizing%20high-quality%0Atexture%20maps%20for%20large-scale%20mesh%20datasets%20and%20showing%20test-time%20applications%0Asuch%20as%20relighting%20and%20material%20control%20using%20a%20popular%20graphics%20engine.%0AProject%20page%3A%20https%3A//kim-youwang.github.io/paint-it%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.11360v2&entry.124074799=Read"},
{"title": "Depth-Wise Attention (DWAtt): A Layer Fusion Method for Data-Efficient\n  Classification", "author": "Muhammad ElNokrashy and Badr AlKhamissi and Mona Diab", "abstract": "  Language Models pretrained on large textual data have been shown to encode\ndifferent types of knowledge simultaneously. Traditionally, only the features\nfrom the last layer are used when adapting to new tasks or data. We put forward\nthat, when using or finetuning deep pretrained models, intermediate layer\nfeatures that may be relevant to the downstream task are buried too deep to be\nused efficiently in terms of needed samples or steps. To test this, we propose\na new layer fusion method: Depth-Wise Attention (DWAtt), to help re-surface\nsignals from non-final layers. We compare DWAtt to a basic concatenation-based\nlayer fusion method (Concat), and compare both to a deeper model baseline --\nall kept within a similar parameter budget. Our findings show that DWAtt and\nConcat are more step- and sample-efficient than the baseline, especially in the\nfew-shot setting. DWAtt outperforms Concat on larger data sizes. On CoNLL-03\nNER, layer fusion shows 3.68--9.73% F1 gain at different few-shot sizes. The\nlayer fusion models presented significantly outperform the baseline in various\ntraining scenarios with different data sizes, architectures, and training\nconstraints.\n", "link": "http://arxiv.org/abs/2209.15168v2", "date": "2024-05-07", "relevancy": 2.086, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5298}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5174}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth-Wise%20Attention%20%28DWAtt%29%3A%20A%20Layer%20Fusion%20Method%20for%20Data-Efficient%0A%20%20Classification&body=Title%3A%20Depth-Wise%20Attention%20%28DWAtt%29%3A%20A%20Layer%20Fusion%20Method%20for%20Data-Efficient%0A%20%20Classification%0AAuthor%3A%20Muhammad%20ElNokrashy%20and%20Badr%20AlKhamissi%20and%20Mona%20Diab%0AAbstract%3A%20%20%20Language%20Models%20pretrained%20on%20large%20textual%20data%20have%20been%20shown%20to%20encode%0Adifferent%20types%20of%20knowledge%20simultaneously.%20Traditionally%2C%20only%20the%20features%0Afrom%20the%20last%20layer%20are%20used%20when%20adapting%20to%20new%20tasks%20or%20data.%20We%20put%20forward%0Athat%2C%20when%20using%20or%20finetuning%20deep%20pretrained%20models%2C%20intermediate%20layer%0Afeatures%20that%20may%20be%20relevant%20to%20the%20downstream%20task%20are%20buried%20too%20deep%20to%20be%0Aused%20efficiently%20in%20terms%20of%20needed%20samples%20or%20steps.%20To%20test%20this%2C%20we%20propose%0Aa%20new%20layer%20fusion%20method%3A%20Depth-Wise%20Attention%20%28DWAtt%29%2C%20to%20help%20re-surface%0Asignals%20from%20non-final%20layers.%20We%20compare%20DWAtt%20to%20a%20basic%20concatenation-based%0Alayer%20fusion%20method%20%28Concat%29%2C%20and%20compare%20both%20to%20a%20deeper%20model%20baseline%20--%0Aall%20kept%20within%20a%20similar%20parameter%20budget.%20Our%20findings%20show%20that%20DWAtt%20and%0AConcat%20are%20more%20step-%20and%20sample-efficient%20than%20the%20baseline%2C%20especially%20in%20the%0Afew-shot%20setting.%20DWAtt%20outperforms%20Concat%20on%20larger%20data%20sizes.%20On%20CoNLL-03%0ANER%2C%20layer%20fusion%20shows%203.68--9.73%25%20F1%20gain%20at%20different%20few-shot%20sizes.%20The%0Alayer%20fusion%20models%20presented%20significantly%20outperform%20the%20baseline%20in%20various%0Atraining%20scenarios%20with%20different%20data%20sizes%2C%20architectures%2C%20and%20training%0Aconstraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.15168v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth-Wise%2520Attention%2520%2528DWAtt%2529%253A%2520A%2520Layer%2520Fusion%2520Method%2520for%2520Data-Efficient%250A%2520%2520Classification%26entry.906535625%3DMuhammad%2520ElNokrashy%2520and%2520Badr%2520AlKhamissi%2520and%2520Mona%2520Diab%26entry.1292438233%3D%2520%2520Language%2520Models%2520pretrained%2520on%2520large%2520textual%2520data%2520have%2520been%2520shown%2520to%2520encode%250Adifferent%2520types%2520of%2520knowledge%2520simultaneously.%2520Traditionally%252C%2520only%2520the%2520features%250Afrom%2520the%2520last%2520layer%2520are%2520used%2520when%2520adapting%2520to%2520new%2520tasks%2520or%2520data.%2520We%2520put%2520forward%250Athat%252C%2520when%2520using%2520or%2520finetuning%2520deep%2520pretrained%2520models%252C%2520intermediate%2520layer%250Afeatures%2520that%2520may%2520be%2520relevant%2520to%2520the%2520downstream%2520task%2520are%2520buried%2520too%2520deep%2520to%2520be%250Aused%2520efficiently%2520in%2520terms%2520of%2520needed%2520samples%2520or%2520steps.%2520To%2520test%2520this%252C%2520we%2520propose%250Aa%2520new%2520layer%2520fusion%2520method%253A%2520Depth-Wise%2520Attention%2520%2528DWAtt%2529%252C%2520to%2520help%2520re-surface%250Asignals%2520from%2520non-final%2520layers.%2520We%2520compare%2520DWAtt%2520to%2520a%2520basic%2520concatenation-based%250Alayer%2520fusion%2520method%2520%2528Concat%2529%252C%2520and%2520compare%2520both%2520to%2520a%2520deeper%2520model%2520baseline%2520--%250Aall%2520kept%2520within%2520a%2520similar%2520parameter%2520budget.%2520Our%2520findings%2520show%2520that%2520DWAtt%2520and%250AConcat%2520are%2520more%2520step-%2520and%2520sample-efficient%2520than%2520the%2520baseline%252C%2520especially%2520in%2520the%250Afew-shot%2520setting.%2520DWAtt%2520outperforms%2520Concat%2520on%2520larger%2520data%2520sizes.%2520On%2520CoNLL-03%250ANER%252C%2520layer%2520fusion%2520shows%25203.68--9.73%2525%2520F1%2520gain%2520at%2520different%2520few-shot%2520sizes.%2520The%250Alayer%2520fusion%2520models%2520presented%2520significantly%2520outperform%2520the%2520baseline%2520in%2520various%250Atraining%2520scenarios%2520with%2520different%2520data%2520sizes%252C%2520architectures%252C%2520and%2520training%250Aconstraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.15168v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth-Wise%20Attention%20%28DWAtt%29%3A%20A%20Layer%20Fusion%20Method%20for%20Data-Efficient%0A%20%20Classification&entry.906535625=Muhammad%20ElNokrashy%20and%20Badr%20AlKhamissi%20and%20Mona%20Diab&entry.1292438233=%20%20Language%20Models%20pretrained%20on%20large%20textual%20data%20have%20been%20shown%20to%20encode%0Adifferent%20types%20of%20knowledge%20simultaneously.%20Traditionally%2C%20only%20the%20features%0Afrom%20the%20last%20layer%20are%20used%20when%20adapting%20to%20new%20tasks%20or%20data.%20We%20put%20forward%0Athat%2C%20when%20using%20or%20finetuning%20deep%20pretrained%20models%2C%20intermediate%20layer%0Afeatures%20that%20may%20be%20relevant%20to%20the%20downstream%20task%20are%20buried%20too%20deep%20to%20be%0Aused%20efficiently%20in%20terms%20of%20needed%20samples%20or%20steps.%20To%20test%20this%2C%20we%20propose%0Aa%20new%20layer%20fusion%20method%3A%20Depth-Wise%20Attention%20%28DWAtt%29%2C%20to%20help%20re-surface%0Asignals%20from%20non-final%20layers.%20We%20compare%20DWAtt%20to%20a%20basic%20concatenation-based%0Alayer%20fusion%20method%20%28Concat%29%2C%20and%20compare%20both%20to%20a%20deeper%20model%20baseline%20--%0Aall%20kept%20within%20a%20similar%20parameter%20budget.%20Our%20findings%20show%20that%20DWAtt%20and%0AConcat%20are%20more%20step-%20and%20sample-efficient%20than%20the%20baseline%2C%20especially%20in%20the%0Afew-shot%20setting.%20DWAtt%20outperforms%20Concat%20on%20larger%20data%20sizes.%20On%20CoNLL-03%0ANER%2C%20layer%20fusion%20shows%203.68--9.73%25%20F1%20gain%20at%20different%20few-shot%20sizes.%20The%0Alayer%20fusion%20models%20presented%20significantly%20outperform%20the%20baseline%20in%20various%0Atraining%20scenarios%20with%20different%20data%20sizes%2C%20architectures%2C%20and%20training%0Aconstraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.15168v2&entry.124074799=Read"},
{"title": "Predicting Transonic Flowfields in Non-Homogeneous Unstructured Grids\n  Using Autoencoder Graph Convolutional Networks", "author": "Gabriele Immordino and Andrea Vaiuso and Andrea Da Ronch and Marcello Righi", "abstract": "  This paper focuses on addressing challenges posed by non-homogeneous\nunstructured grids, commonly used in Computational Fluid Dynamics (CFD). Their\nprevalence in CFD scenarios has motivated the exploration of innovative\napproaches for generating reduced-order models. The core of our approach\ncenters on geometric deep learning, specifically the utilization of graph\nconvolutional network (GCN). The novel Autoencoder GCN architecture enhances\nprediction accuracy by propagating information to distant nodes and emphasizing\ninfluential points. This architecture, with GCN layers and encoding/decoding\nmodules, reduces dimensionality based on pressure-gradient values. The\nautoencoder structure improves the network capability to identify key features,\ncontributing to a more robust and accurate predictive model. To validate the\nproposed methodology, we analyzed two different test cases: wing-only model and\nwing--body configuration. Precise reconstruction of steady-state distributed\nquantities within a two-dimensional parametric space underscores the\nreliability and versatility of the implemented approach.\n", "link": "http://arxiv.org/abs/2405.04396v1", "date": "2024-05-07", "relevancy": 2.0818, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5538}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5239}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Transonic%20Flowfields%20in%20Non-Homogeneous%20Unstructured%20Grids%0A%20%20Using%20Autoencoder%20Graph%20Convolutional%20Networks&body=Title%3A%20Predicting%20Transonic%20Flowfields%20in%20Non-Homogeneous%20Unstructured%20Grids%0A%20%20Using%20Autoencoder%20Graph%20Convolutional%20Networks%0AAuthor%3A%20Gabriele%20Immordino%20and%20Andrea%20Vaiuso%20and%20Andrea%20Da%20Ronch%20and%20Marcello%20Righi%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20addressing%20challenges%20posed%20by%20non-homogeneous%0Aunstructured%20grids%2C%20commonly%20used%20in%20Computational%20Fluid%20Dynamics%20%28CFD%29.%20Their%0Aprevalence%20in%20CFD%20scenarios%20has%20motivated%20the%20exploration%20of%20innovative%0Aapproaches%20for%20generating%20reduced-order%20models.%20The%20core%20of%20our%20approach%0Acenters%20on%20geometric%20deep%20learning%2C%20specifically%20the%20utilization%20of%20graph%0Aconvolutional%20network%20%28GCN%29.%20The%20novel%20Autoencoder%20GCN%20architecture%20enhances%0Aprediction%20accuracy%20by%20propagating%20information%20to%20distant%20nodes%20and%20emphasizing%0Ainfluential%20points.%20This%20architecture%2C%20with%20GCN%20layers%20and%20encoding/decoding%0Amodules%2C%20reduces%20dimensionality%20based%20on%20pressure-gradient%20values.%20The%0Aautoencoder%20structure%20improves%20the%20network%20capability%20to%20identify%20key%20features%2C%0Acontributing%20to%20a%20more%20robust%20and%20accurate%20predictive%20model.%20To%20validate%20the%0Aproposed%20methodology%2C%20we%20analyzed%20two%20different%20test%20cases%3A%20wing-only%20model%20and%0Awing--body%20configuration.%20Precise%20reconstruction%20of%20steady-state%20distributed%0Aquantities%20within%20a%20two-dimensional%20parametric%20space%20underscores%20the%0Areliability%20and%20versatility%20of%20the%20implemented%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Transonic%2520Flowfields%2520in%2520Non-Homogeneous%2520Unstructured%2520Grids%250A%2520%2520Using%2520Autoencoder%2520Graph%2520Convolutional%2520Networks%26entry.906535625%3DGabriele%2520Immordino%2520and%2520Andrea%2520Vaiuso%2520and%2520Andrea%2520Da%2520Ronch%2520and%2520Marcello%2520Righi%26entry.1292438233%3D%2520%2520This%2520paper%2520focuses%2520on%2520addressing%2520challenges%2520posed%2520by%2520non-homogeneous%250Aunstructured%2520grids%252C%2520commonly%2520used%2520in%2520Computational%2520Fluid%2520Dynamics%2520%2528CFD%2529.%2520Their%250Aprevalence%2520in%2520CFD%2520scenarios%2520has%2520motivated%2520the%2520exploration%2520of%2520innovative%250Aapproaches%2520for%2520generating%2520reduced-order%2520models.%2520The%2520core%2520of%2520our%2520approach%250Acenters%2520on%2520geometric%2520deep%2520learning%252C%2520specifically%2520the%2520utilization%2520of%2520graph%250Aconvolutional%2520network%2520%2528GCN%2529.%2520The%2520novel%2520Autoencoder%2520GCN%2520architecture%2520enhances%250Aprediction%2520accuracy%2520by%2520propagating%2520information%2520to%2520distant%2520nodes%2520and%2520emphasizing%250Ainfluential%2520points.%2520This%2520architecture%252C%2520with%2520GCN%2520layers%2520and%2520encoding/decoding%250Amodules%252C%2520reduces%2520dimensionality%2520based%2520on%2520pressure-gradient%2520values.%2520The%250Aautoencoder%2520structure%2520improves%2520the%2520network%2520capability%2520to%2520identify%2520key%2520features%252C%250Acontributing%2520to%2520a%2520more%2520robust%2520and%2520accurate%2520predictive%2520model.%2520To%2520validate%2520the%250Aproposed%2520methodology%252C%2520we%2520analyzed%2520two%2520different%2520test%2520cases%253A%2520wing-only%2520model%2520and%250Awing--body%2520configuration.%2520Precise%2520reconstruction%2520of%2520steady-state%2520distributed%250Aquantities%2520within%2520a%2520two-dimensional%2520parametric%2520space%2520underscores%2520the%250Areliability%2520and%2520versatility%2520of%2520the%2520implemented%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Transonic%20Flowfields%20in%20Non-Homogeneous%20Unstructured%20Grids%0A%20%20Using%20Autoencoder%20Graph%20Convolutional%20Networks&entry.906535625=Gabriele%20Immordino%20and%20Andrea%20Vaiuso%20and%20Andrea%20Da%20Ronch%20and%20Marcello%20Righi&entry.1292438233=%20%20This%20paper%20focuses%20on%20addressing%20challenges%20posed%20by%20non-homogeneous%0Aunstructured%20grids%2C%20commonly%20used%20in%20Computational%20Fluid%20Dynamics%20%28CFD%29.%20Their%0Aprevalence%20in%20CFD%20scenarios%20has%20motivated%20the%20exploration%20of%20innovative%0Aapproaches%20for%20generating%20reduced-order%20models.%20The%20core%20of%20our%20approach%0Acenters%20on%20geometric%20deep%20learning%2C%20specifically%20the%20utilization%20of%20graph%0Aconvolutional%20network%20%28GCN%29.%20The%20novel%20Autoencoder%20GCN%20architecture%20enhances%0Aprediction%20accuracy%20by%20propagating%20information%20to%20distant%20nodes%20and%20emphasizing%0Ainfluential%20points.%20This%20architecture%2C%20with%20GCN%20layers%20and%20encoding/decoding%0Amodules%2C%20reduces%20dimensionality%20based%20on%20pressure-gradient%20values.%20The%0Aautoencoder%20structure%20improves%20the%20network%20capability%20to%20identify%20key%20features%2C%0Acontributing%20to%20a%20more%20robust%20and%20accurate%20predictive%20model.%20To%20validate%20the%0Aproposed%20methodology%2C%20we%20analyzed%20two%20different%20test%20cases%3A%20wing-only%20model%20and%0Awing--body%20configuration.%20Precise%20reconstruction%20of%20steady-state%20distributed%0Aquantities%20within%20a%20two-dimensional%20parametric%20space%20underscores%20the%0Areliability%20and%20versatility%20of%20the%20implemented%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04396v1&entry.124074799=Read"},
{"title": "Solving the bongard-logo problem by modeling a probabilistic model", "author": "Ruizhuo Song and Beiming Yuan", "abstract": "  Abstract reasoning problems challenge the perceptual and cognitive abilities\nof AI algorithms, demanding deeper pattern discernment and inductive reasoning\nbeyond explicit image features. This study introduces PMoC, a tailored\nprobability model for the Bongard-Logo problem, achieving high reasoning\naccuracy by constructing independent probability models. Additionally, we\npresent Pose-Transformer, an enhanced Transformer-Encoder designed for complex\nabstract reasoning tasks, including Bongard-Logo, RAVEN, I-RAVEN, and PGM.\nPose-Transformer incorporates positional information learning, inspired by\ncapsule networks' pose matrices, enhancing its focus on local positional\nrelationships in image data processing. When integrated with PMoC, it further\nimproves reasoning accuracy. Our approach effectively addresses reasoning\ndifficulties associated with abstract entities' positional changes,\noutperforming previous models on the OIG, D3$\\times$3 subsets of RAVEN, and PGM\ndatabases. This research contributes to advancing AI's capabilities in abstract\nreasoning and cognitive pattern recognition.\n", "link": "http://arxiv.org/abs/2403.03173v4", "date": "2024-05-07", "relevancy": 2.0787, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5369}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5241}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20the%20bongard-logo%20problem%20by%20modeling%20a%20probabilistic%20model&body=Title%3A%20Solving%20the%20bongard-logo%20problem%20by%20modeling%20a%20probabilistic%20model%0AAuthor%3A%20Ruizhuo%20Song%20and%20Beiming%20Yuan%0AAbstract%3A%20%20%20Abstract%20reasoning%20problems%20challenge%20the%20perceptual%20and%20cognitive%20abilities%0Aof%20AI%20algorithms%2C%20demanding%20deeper%20pattern%20discernment%20and%20inductive%20reasoning%0Abeyond%20explicit%20image%20features.%20This%20study%20introduces%20PMoC%2C%20a%20tailored%0Aprobability%20model%20for%20the%20Bongard-Logo%20problem%2C%20achieving%20high%20reasoning%0Aaccuracy%20by%20constructing%20independent%20probability%20models.%20Additionally%2C%20we%0Apresent%20Pose-Transformer%2C%20an%20enhanced%20Transformer-Encoder%20designed%20for%20complex%0Aabstract%20reasoning%20tasks%2C%20including%20Bongard-Logo%2C%20RAVEN%2C%20I-RAVEN%2C%20and%20PGM.%0APose-Transformer%20incorporates%20positional%20information%20learning%2C%20inspired%20by%0Acapsule%20networks%27%20pose%20matrices%2C%20enhancing%20its%20focus%20on%20local%20positional%0Arelationships%20in%20image%20data%20processing.%20When%20integrated%20with%20PMoC%2C%20it%20further%0Aimproves%20reasoning%20accuracy.%20Our%20approach%20effectively%20addresses%20reasoning%0Adifficulties%20associated%20with%20abstract%20entities%27%20positional%20changes%2C%0Aoutperforming%20previous%20models%20on%20the%20OIG%2C%20D3%24%5Ctimes%243%20subsets%20of%20RAVEN%2C%20and%20PGM%0Adatabases.%20This%20research%20contributes%20to%20advancing%20AI%27s%20capabilities%20in%20abstract%0Areasoning%20and%20cognitive%20pattern%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03173v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520the%2520bongard-logo%2520problem%2520by%2520modeling%2520a%2520probabilistic%2520model%26entry.906535625%3DRuizhuo%2520Song%2520and%2520Beiming%2520Yuan%26entry.1292438233%3D%2520%2520Abstract%2520reasoning%2520problems%2520challenge%2520the%2520perceptual%2520and%2520cognitive%2520abilities%250Aof%2520AI%2520algorithms%252C%2520demanding%2520deeper%2520pattern%2520discernment%2520and%2520inductive%2520reasoning%250Abeyond%2520explicit%2520image%2520features.%2520This%2520study%2520introduces%2520PMoC%252C%2520a%2520tailored%250Aprobability%2520model%2520for%2520the%2520Bongard-Logo%2520problem%252C%2520achieving%2520high%2520reasoning%250Aaccuracy%2520by%2520constructing%2520independent%2520probability%2520models.%2520Additionally%252C%2520we%250Apresent%2520Pose-Transformer%252C%2520an%2520enhanced%2520Transformer-Encoder%2520designed%2520for%2520complex%250Aabstract%2520reasoning%2520tasks%252C%2520including%2520Bongard-Logo%252C%2520RAVEN%252C%2520I-RAVEN%252C%2520and%2520PGM.%250APose-Transformer%2520incorporates%2520positional%2520information%2520learning%252C%2520inspired%2520by%250Acapsule%2520networks%2527%2520pose%2520matrices%252C%2520enhancing%2520its%2520focus%2520on%2520local%2520positional%250Arelationships%2520in%2520image%2520data%2520processing.%2520When%2520integrated%2520with%2520PMoC%252C%2520it%2520further%250Aimproves%2520reasoning%2520accuracy.%2520Our%2520approach%2520effectively%2520addresses%2520reasoning%250Adifficulties%2520associated%2520with%2520abstract%2520entities%2527%2520positional%2520changes%252C%250Aoutperforming%2520previous%2520models%2520on%2520the%2520OIG%252C%2520D3%2524%255Ctimes%25243%2520subsets%2520of%2520RAVEN%252C%2520and%2520PGM%250Adatabases.%2520This%2520research%2520contributes%2520to%2520advancing%2520AI%2527s%2520capabilities%2520in%2520abstract%250Areasoning%2520and%2520cognitive%2520pattern%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03173v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20the%20bongard-logo%20problem%20by%20modeling%20a%20probabilistic%20model&entry.906535625=Ruizhuo%20Song%20and%20Beiming%20Yuan&entry.1292438233=%20%20Abstract%20reasoning%20problems%20challenge%20the%20perceptual%20and%20cognitive%20abilities%0Aof%20AI%20algorithms%2C%20demanding%20deeper%20pattern%20discernment%20and%20inductive%20reasoning%0Abeyond%20explicit%20image%20features.%20This%20study%20introduces%20PMoC%2C%20a%20tailored%0Aprobability%20model%20for%20the%20Bongard-Logo%20problem%2C%20achieving%20high%20reasoning%0Aaccuracy%20by%20constructing%20independent%20probability%20models.%20Additionally%2C%20we%0Apresent%20Pose-Transformer%2C%20an%20enhanced%20Transformer-Encoder%20designed%20for%20complex%0Aabstract%20reasoning%20tasks%2C%20including%20Bongard-Logo%2C%20RAVEN%2C%20I-RAVEN%2C%20and%20PGM.%0APose-Transformer%20incorporates%20positional%20information%20learning%2C%20inspired%20by%0Acapsule%20networks%27%20pose%20matrices%2C%20enhancing%20its%20focus%20on%20local%20positional%0Arelationships%20in%20image%20data%20processing.%20When%20integrated%20with%20PMoC%2C%20it%20further%0Aimproves%20reasoning%20accuracy.%20Our%20approach%20effectively%20addresses%20reasoning%0Adifficulties%20associated%20with%20abstract%20entities%27%20positional%20changes%2C%0Aoutperforming%20previous%20models%20on%20the%20OIG%2C%20D3%24%5Ctimes%243%20subsets%20of%20RAVEN%2C%20and%20PGM%0Adatabases.%20This%20research%20contributes%20to%20advancing%20AI%27s%20capabilities%20in%20abstract%0Areasoning%20and%20cognitive%20pattern%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03173v4&entry.124074799=Read"},
{"title": "S3Former: Self-supervised High-resolution Transformer for Solar PV\n  Profiling", "author": "Minh Tran and Adrian De Luis and Haitao Liao and Ying Huang and Roy McCann and Alan Mantooth and Jack Cothren and Ngan Le", "abstract": "  As the impact of climate change escalates, the global necessity to transition\nto sustainable energy sources becomes increasingly evident. Renewable energies\nhave emerged as a viable solution for users, with Photovoltaic energy being a\nfavored choice for small installations due to its reliability and efficiency.\nAccurate mapping of PV installations is crucial for understanding the extension\nof its adoption and informing energy policy. To meet this need, we introduce\nS3Former, designed to segment solar panels from aerial imagery and provide size\nand location information critical for analyzing the impact of such\ninstallations on the grid. Solar panel identification is challenging due to\nfactors such as varying weather conditions, roof characteristics, Ground\nSampling Distance variations and lack of appropriate initialization weights for\noptimized training. To tackle these complexities, S3Former features a Masked\nAttention Mask Transformer incorporating a self-supervised learning pretrained\nbackbone. Specifically, our model leverages low-level and high-level features\nextracted from the backbone and incorporates an instance query mechanism\nincorporated on the Transformer architecture to enhance the localization of\nsolar PV installations. We introduce a self-supervised learning phase (pretext\ntask) to improve the initialization weights on the backbone of S3Former. We\nevaluated S3Former using diverse datasets, demonstrate improvement\nstate-of-the-art models.\n", "link": "http://arxiv.org/abs/2405.04489v1", "date": "2024-05-07", "relevancy": 2.0718, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5271}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5243}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S3Former%3A%20Self-supervised%20High-resolution%20Transformer%20for%20Solar%20PV%0A%20%20Profiling&body=Title%3A%20S3Former%3A%20Self-supervised%20High-resolution%20Transformer%20for%20Solar%20PV%0A%20%20Profiling%0AAuthor%3A%20Minh%20Tran%20and%20Adrian%20De%20Luis%20and%20Haitao%20Liao%20and%20Ying%20Huang%20and%20Roy%20McCann%20and%20Alan%20Mantooth%20and%20Jack%20Cothren%20and%20Ngan%20Le%0AAbstract%3A%20%20%20As%20the%20impact%20of%20climate%20change%20escalates%2C%20the%20global%20necessity%20to%20transition%0Ato%20sustainable%20energy%20sources%20becomes%20increasingly%20evident.%20Renewable%20energies%0Ahave%20emerged%20as%20a%20viable%20solution%20for%20users%2C%20with%20Photovoltaic%20energy%20being%20a%0Afavored%20choice%20for%20small%20installations%20due%20to%20its%20reliability%20and%20efficiency.%0AAccurate%20mapping%20of%20PV%20installations%20is%20crucial%20for%20understanding%20the%20extension%0Aof%20its%20adoption%20and%20informing%20energy%20policy.%20To%20meet%20this%20need%2C%20we%20introduce%0AS3Former%2C%20designed%20to%20segment%20solar%20panels%20from%20aerial%20imagery%20and%20provide%20size%0Aand%20location%20information%20critical%20for%20analyzing%20the%20impact%20of%20such%0Ainstallations%20on%20the%20grid.%20Solar%20panel%20identification%20is%20challenging%20due%20to%0Afactors%20such%20as%20varying%20weather%20conditions%2C%20roof%20characteristics%2C%20Ground%0ASampling%20Distance%20variations%20and%20lack%20of%20appropriate%20initialization%20weights%20for%0Aoptimized%20training.%20To%20tackle%20these%20complexities%2C%20S3Former%20features%20a%20Masked%0AAttention%20Mask%20Transformer%20incorporating%20a%20self-supervised%20learning%20pretrained%0Abackbone.%20Specifically%2C%20our%20model%20leverages%20low-level%20and%20high-level%20features%0Aextracted%20from%20the%20backbone%20and%20incorporates%20an%20instance%20query%20mechanism%0Aincorporated%20on%20the%20Transformer%20architecture%20to%20enhance%20the%20localization%20of%0Asolar%20PV%20installations.%20We%20introduce%20a%20self-supervised%20learning%20phase%20%28pretext%0Atask%29%20to%20improve%20the%20initialization%20weights%20on%20the%20backbone%20of%20S3Former.%20We%0Aevaluated%20S3Former%20using%20diverse%20datasets%2C%20demonstrate%20improvement%0Astate-of-the-art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS3Former%253A%2520Self-supervised%2520High-resolution%2520Transformer%2520for%2520Solar%2520PV%250A%2520%2520Profiling%26entry.906535625%3DMinh%2520Tran%2520and%2520Adrian%2520De%2520Luis%2520and%2520Haitao%2520Liao%2520and%2520Ying%2520Huang%2520and%2520Roy%2520McCann%2520and%2520Alan%2520Mantooth%2520and%2520Jack%2520Cothren%2520and%2520Ngan%2520Le%26entry.1292438233%3D%2520%2520As%2520the%2520impact%2520of%2520climate%2520change%2520escalates%252C%2520the%2520global%2520necessity%2520to%2520transition%250Ato%2520sustainable%2520energy%2520sources%2520becomes%2520increasingly%2520evident.%2520Renewable%2520energies%250Ahave%2520emerged%2520as%2520a%2520viable%2520solution%2520for%2520users%252C%2520with%2520Photovoltaic%2520energy%2520being%2520a%250Afavored%2520choice%2520for%2520small%2520installations%2520due%2520to%2520its%2520reliability%2520and%2520efficiency.%250AAccurate%2520mapping%2520of%2520PV%2520installations%2520is%2520crucial%2520for%2520understanding%2520the%2520extension%250Aof%2520its%2520adoption%2520and%2520informing%2520energy%2520policy.%2520To%2520meet%2520this%2520need%252C%2520we%2520introduce%250AS3Former%252C%2520designed%2520to%2520segment%2520solar%2520panels%2520from%2520aerial%2520imagery%2520and%2520provide%2520size%250Aand%2520location%2520information%2520critical%2520for%2520analyzing%2520the%2520impact%2520of%2520such%250Ainstallations%2520on%2520the%2520grid.%2520Solar%2520panel%2520identification%2520is%2520challenging%2520due%2520to%250Afactors%2520such%2520as%2520varying%2520weather%2520conditions%252C%2520roof%2520characteristics%252C%2520Ground%250ASampling%2520Distance%2520variations%2520and%2520lack%2520of%2520appropriate%2520initialization%2520weights%2520for%250Aoptimized%2520training.%2520To%2520tackle%2520these%2520complexities%252C%2520S3Former%2520features%2520a%2520Masked%250AAttention%2520Mask%2520Transformer%2520incorporating%2520a%2520self-supervised%2520learning%2520pretrained%250Abackbone.%2520Specifically%252C%2520our%2520model%2520leverages%2520low-level%2520and%2520high-level%2520features%250Aextracted%2520from%2520the%2520backbone%2520and%2520incorporates%2520an%2520instance%2520query%2520mechanism%250Aincorporated%2520on%2520the%2520Transformer%2520architecture%2520to%2520enhance%2520the%2520localization%2520of%250Asolar%2520PV%2520installations.%2520We%2520introduce%2520a%2520self-supervised%2520learning%2520phase%2520%2528pretext%250Atask%2529%2520to%2520improve%2520the%2520initialization%2520weights%2520on%2520the%2520backbone%2520of%2520S3Former.%2520We%250Aevaluated%2520S3Former%2520using%2520diverse%2520datasets%252C%2520demonstrate%2520improvement%250Astate-of-the-art%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S3Former%3A%20Self-supervised%20High-resolution%20Transformer%20for%20Solar%20PV%0A%20%20Profiling&entry.906535625=Minh%20Tran%20and%20Adrian%20De%20Luis%20and%20Haitao%20Liao%20and%20Ying%20Huang%20and%20Roy%20McCann%20and%20Alan%20Mantooth%20and%20Jack%20Cothren%20and%20Ngan%20Le&entry.1292438233=%20%20As%20the%20impact%20of%20climate%20change%20escalates%2C%20the%20global%20necessity%20to%20transition%0Ato%20sustainable%20energy%20sources%20becomes%20increasingly%20evident.%20Renewable%20energies%0Ahave%20emerged%20as%20a%20viable%20solution%20for%20users%2C%20with%20Photovoltaic%20energy%20being%20a%0Afavored%20choice%20for%20small%20installations%20due%20to%20its%20reliability%20and%20efficiency.%0AAccurate%20mapping%20of%20PV%20installations%20is%20crucial%20for%20understanding%20the%20extension%0Aof%20its%20adoption%20and%20informing%20energy%20policy.%20To%20meet%20this%20need%2C%20we%20introduce%0AS3Former%2C%20designed%20to%20segment%20solar%20panels%20from%20aerial%20imagery%20and%20provide%20size%0Aand%20location%20information%20critical%20for%20analyzing%20the%20impact%20of%20such%0Ainstallations%20on%20the%20grid.%20Solar%20panel%20identification%20is%20challenging%20due%20to%0Afactors%20such%20as%20varying%20weather%20conditions%2C%20roof%20characteristics%2C%20Ground%0ASampling%20Distance%20variations%20and%20lack%20of%20appropriate%20initialization%20weights%20for%0Aoptimized%20training.%20To%20tackle%20these%20complexities%2C%20S3Former%20features%20a%20Masked%0AAttention%20Mask%20Transformer%20incorporating%20a%20self-supervised%20learning%20pretrained%0Abackbone.%20Specifically%2C%20our%20model%20leverages%20low-level%20and%20high-level%20features%0Aextracted%20from%20the%20backbone%20and%20incorporates%20an%20instance%20query%20mechanism%0Aincorporated%20on%20the%20Transformer%20architecture%20to%20enhance%20the%20localization%20of%0Asolar%20PV%20installations.%20We%20introduce%20a%20self-supervised%20learning%20phase%20%28pretext%0Atask%29%20to%20improve%20the%20initialization%20weights%20on%20the%20backbone%20of%20S3Former.%20We%0Aevaluated%20S3Former%20using%20diverse%20datasets%2C%20demonstrate%20improvement%0Astate-of-the-art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04489v1&entry.124074799=Read"},
{"title": "Inf-DiT: Upsampling Any-Resolution Image with Memory-Efficient Diffusion\n  Transformer", "author": "Zhuoyi Yang and Heyang Jiang and Wenyi Hong and Jiayan Teng and Wendi Zheng and Yuxiao Dong and Ming Ding and Jie Tang", "abstract": "  Diffusion models have shown remarkable performance in image generation in\nrecent years. However, due to a quadratic increase in memory during generating\nultra-high-resolution images (e.g. 4096*4096), the resolution of generated\nimages is often limited to 1024*1024. In this work. we propose a unidirectional\nblock attention mechanism that can adaptively adjust the memory overhead during\nthe inference process and handle global dependencies. Building on this module,\nwe adopt the DiT structure for upsampling and develop an infinite\nsuper-resolution model capable of upsampling images of various shapes and\nresolutions. Comprehensive experiments show that our model achieves SOTA\nperformance in generating ultra-high-resolution images in both machine and\nhuman evaluation. Compared to commonly used UNet structures, our model can save\nmore than 5x memory when generating 4096*4096 images. The project URL is\nhttps://github.com/THUDM/Inf-DiT.\n", "link": "http://arxiv.org/abs/2405.04312v1", "date": "2024-05-07", "relevancy": 2.0667, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.7041}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6798}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inf-DiT%3A%20Upsampling%20Any-Resolution%20Image%20with%20Memory-Efficient%20Diffusion%0A%20%20Transformer&body=Title%3A%20Inf-DiT%3A%20Upsampling%20Any-Resolution%20Image%20with%20Memory-Efficient%20Diffusion%0A%20%20Transformer%0AAuthor%3A%20Zhuoyi%20Yang%20and%20Heyang%20Jiang%20and%20Wenyi%20Hong%20and%20Jiayan%20Teng%20and%20Wendi%20Zheng%20and%20Yuxiao%20Dong%20and%20Ming%20Ding%20and%20Jie%20Tang%0AAbstract%3A%20%20%20Diffusion%20models%20have%20shown%20remarkable%20performance%20in%20image%20generation%20in%0Arecent%20years.%20However%2C%20due%20to%20a%20quadratic%20increase%20in%20memory%20during%20generating%0Aultra-high-resolution%20images%20%28e.g.%204096%2A4096%29%2C%20the%20resolution%20of%20generated%0Aimages%20is%20often%20limited%20to%201024%2A1024.%20In%20this%20work.%20we%20propose%20a%20unidirectional%0Ablock%20attention%20mechanism%20that%20can%20adaptively%20adjust%20the%20memory%20overhead%20during%0Athe%20inference%20process%20and%20handle%20global%20dependencies.%20Building%20on%20this%20module%2C%0Awe%20adopt%20the%20DiT%20structure%20for%20upsampling%20and%20develop%20an%20infinite%0Asuper-resolution%20model%20capable%20of%20upsampling%20images%20of%20various%20shapes%20and%0Aresolutions.%20Comprehensive%20experiments%20show%20that%20our%20model%20achieves%20SOTA%0Aperformance%20in%20generating%20ultra-high-resolution%20images%20in%20both%20machine%20and%0Ahuman%20evaluation.%20Compared%20to%20commonly%20used%20UNet%20structures%2C%20our%20model%20can%20save%0Amore%20than%205x%20memory%20when%20generating%204096%2A4096%20images.%20The%20project%20URL%20is%0Ahttps%3A//github.com/THUDM/Inf-DiT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04312v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInf-DiT%253A%2520Upsampling%2520Any-Resolution%2520Image%2520with%2520Memory-Efficient%2520Diffusion%250A%2520%2520Transformer%26entry.906535625%3DZhuoyi%2520Yang%2520and%2520Heyang%2520Jiang%2520and%2520Wenyi%2520Hong%2520and%2520Jiayan%2520Teng%2520and%2520Wendi%2520Zheng%2520and%2520Yuxiao%2520Dong%2520and%2520Ming%2520Ding%2520and%2520Jie%2520Tang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520shown%2520remarkable%2520performance%2520in%2520image%2520generation%2520in%250Arecent%2520years.%2520However%252C%2520due%2520to%2520a%2520quadratic%2520increase%2520in%2520memory%2520during%2520generating%250Aultra-high-resolution%2520images%2520%2528e.g.%25204096%252A4096%2529%252C%2520the%2520resolution%2520of%2520generated%250Aimages%2520is%2520often%2520limited%2520to%25201024%252A1024.%2520In%2520this%2520work.%2520we%2520propose%2520a%2520unidirectional%250Ablock%2520attention%2520mechanism%2520that%2520can%2520adaptively%2520adjust%2520the%2520memory%2520overhead%2520during%250Athe%2520inference%2520process%2520and%2520handle%2520global%2520dependencies.%2520Building%2520on%2520this%2520module%252C%250Awe%2520adopt%2520the%2520DiT%2520structure%2520for%2520upsampling%2520and%2520develop%2520an%2520infinite%250Asuper-resolution%2520model%2520capable%2520of%2520upsampling%2520images%2520of%2520various%2520shapes%2520and%250Aresolutions.%2520Comprehensive%2520experiments%2520show%2520that%2520our%2520model%2520achieves%2520SOTA%250Aperformance%2520in%2520generating%2520ultra-high-resolution%2520images%2520in%2520both%2520machine%2520and%250Ahuman%2520evaluation.%2520Compared%2520to%2520commonly%2520used%2520UNet%2520structures%252C%2520our%2520model%2520can%2520save%250Amore%2520than%25205x%2520memory%2520when%2520generating%25204096%252A4096%2520images.%2520The%2520project%2520URL%2520is%250Ahttps%253A//github.com/THUDM/Inf-DiT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04312v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inf-DiT%3A%20Upsampling%20Any-Resolution%20Image%20with%20Memory-Efficient%20Diffusion%0A%20%20Transformer&entry.906535625=Zhuoyi%20Yang%20and%20Heyang%20Jiang%20and%20Wenyi%20Hong%20and%20Jiayan%20Teng%20and%20Wendi%20Zheng%20and%20Yuxiao%20Dong%20and%20Ming%20Ding%20and%20Jie%20Tang&entry.1292438233=%20%20Diffusion%20models%20have%20shown%20remarkable%20performance%20in%20image%20generation%20in%0Arecent%20years.%20However%2C%20due%20to%20a%20quadratic%20increase%20in%20memory%20during%20generating%0Aultra-high-resolution%20images%20%28e.g.%204096%2A4096%29%2C%20the%20resolution%20of%20generated%0Aimages%20is%20often%20limited%20to%201024%2A1024.%20In%20this%20work.%20we%20propose%20a%20unidirectional%0Ablock%20attention%20mechanism%20that%20can%20adaptively%20adjust%20the%20memory%20overhead%20during%0Athe%20inference%20process%20and%20handle%20global%20dependencies.%20Building%20on%20this%20module%2C%0Awe%20adopt%20the%20DiT%20structure%20for%20upsampling%20and%20develop%20an%20infinite%0Asuper-resolution%20model%20capable%20of%20upsampling%20images%20of%20various%20shapes%20and%0Aresolutions.%20Comprehensive%20experiments%20show%20that%20our%20model%20achieves%20SOTA%0Aperformance%20in%20generating%20ultra-high-resolution%20images%20in%20both%20machine%20and%0Ahuman%20evaluation.%20Compared%20to%20commonly%20used%20UNet%20structures%2C%20our%20model%20can%20save%0Amore%20than%205x%20memory%20when%20generating%204096%2A4096%20images.%20The%20project%20URL%20is%0Ahttps%3A//github.com/THUDM/Inf-DiT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04312v1&entry.124074799=Read"},
{"title": "Deep Unlearning: Fast and Efficient Training-free Approach to Class\n  Forgetting", "author": "Sangamesh Kodge and Gobinda Saha and Kaushik Roy", "abstract": "  Machine unlearning is a prominent and challenging field, driven by regulatory\ndemands for user data deletion and heightened privacy awareness. Existing\napproaches involve retraining model or multiple finetuning steps for each\ndeletion request, often constrained by computational limits and restricted data\naccess. In this work, we introduce a novel class unlearning algorithm designed\nto strategically eliminate specific classes from the learned model. Our\nalgorithm first estimates the Retain and the Forget Spaces using Singular Value\nDecomposition on the layerwise activations for a small subset of samples from\nthe retain and unlearn classes, respectively. We then compute the shared\ninformation between these spaces and remove it from the forget space to isolate\nclass-discriminatory feature space. Finally, we obtain the unlearned model by\nupdating the weights to suppress the class discriminatory features from the\nactivation spaces. We demonstrate our algorithm's efficacy on ImageNet using a\nVision Transformer with only $\\sim 1.5\\%$ drop in retain accuracy compared to\nthe original model while maintaining under $1\\%$ accuracy on the unlearned\nclass samples. Further, our algorithm consistently performs well when subject\nto Membership Inference Attacks showing $7.8\\%$ improvement on average across a\nvariety of image classification datasets and network architectures, as compared\nto other baselines while being $\\sim 6 \\times$ more computationally efficient.\nOur code is available at https://github.com/sangamesh-kodge/class_forgetting.\n", "link": "http://arxiv.org/abs/2312.00761v3", "date": "2024-05-07", "relevancy": 2.062, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5191}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5181}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Unlearning%3A%20Fast%20and%20Efficient%20Training-free%20Approach%20to%20Class%0A%20%20Forgetting&body=Title%3A%20Deep%20Unlearning%3A%20Fast%20and%20Efficient%20Training-free%20Approach%20to%20Class%0A%20%20Forgetting%0AAuthor%3A%20Sangamesh%20Kodge%20and%20Gobinda%20Saha%20and%20Kaushik%20Roy%0AAbstract%3A%20%20%20Machine%20unlearning%20is%20a%20prominent%20and%20challenging%20field%2C%20driven%20by%20regulatory%0Ademands%20for%20user%20data%20deletion%20and%20heightened%20privacy%20awareness.%20Existing%0Aapproaches%20involve%20retraining%20model%20or%20multiple%20finetuning%20steps%20for%20each%0Adeletion%20request%2C%20often%20constrained%20by%20computational%20limits%20and%20restricted%20data%0Aaccess.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20class%20unlearning%20algorithm%20designed%0Ato%20strategically%20eliminate%20specific%20classes%20from%20the%20learned%20model.%20Our%0Aalgorithm%20first%20estimates%20the%20Retain%20and%20the%20Forget%20Spaces%20using%20Singular%20Value%0ADecomposition%20on%20the%20layerwise%20activations%20for%20a%20small%20subset%20of%20samples%20from%0Athe%20retain%20and%20unlearn%20classes%2C%20respectively.%20We%20then%20compute%20the%20shared%0Ainformation%20between%20these%20spaces%20and%20remove%20it%20from%20the%20forget%20space%20to%20isolate%0Aclass-discriminatory%20feature%20space.%20Finally%2C%20we%20obtain%20the%20unlearned%20model%20by%0Aupdating%20the%20weights%20to%20suppress%20the%20class%20discriminatory%20features%20from%20the%0Aactivation%20spaces.%20We%20demonstrate%20our%20algorithm%27s%20efficacy%20on%20ImageNet%20using%20a%0AVision%20Transformer%20with%20only%20%24%5Csim%201.5%5C%25%24%20drop%20in%20retain%20accuracy%20compared%20to%0Athe%20original%20model%20while%20maintaining%20under%20%241%5C%25%24%20accuracy%20on%20the%20unlearned%0Aclass%20samples.%20Further%2C%20our%20algorithm%20consistently%20performs%20well%20when%20subject%0Ato%20Membership%20Inference%20Attacks%20showing%20%247.8%5C%25%24%20improvement%20on%20average%20across%20a%0Avariety%20of%20image%20classification%20datasets%20and%20network%20architectures%2C%20as%20compared%0Ato%20other%20baselines%20while%20being%20%24%5Csim%206%20%5Ctimes%24%20more%20computationally%20efficient.%0AOur%20code%20is%20available%20at%20https%3A//github.com/sangamesh-kodge/class_forgetting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00761v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Unlearning%253A%2520Fast%2520and%2520Efficient%2520Training-free%2520Approach%2520to%2520Class%250A%2520%2520Forgetting%26entry.906535625%3DSangamesh%2520Kodge%2520and%2520Gobinda%2520Saha%2520and%2520Kaushik%2520Roy%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520is%2520a%2520prominent%2520and%2520challenging%2520field%252C%2520driven%2520by%2520regulatory%250Ademands%2520for%2520user%2520data%2520deletion%2520and%2520heightened%2520privacy%2520awareness.%2520Existing%250Aapproaches%2520involve%2520retraining%2520model%2520or%2520multiple%2520finetuning%2520steps%2520for%2520each%250Adeletion%2520request%252C%2520often%2520constrained%2520by%2520computational%2520limits%2520and%2520restricted%2520data%250Aaccess.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520class%2520unlearning%2520algorithm%2520designed%250Ato%2520strategically%2520eliminate%2520specific%2520classes%2520from%2520the%2520learned%2520model.%2520Our%250Aalgorithm%2520first%2520estimates%2520the%2520Retain%2520and%2520the%2520Forget%2520Spaces%2520using%2520Singular%2520Value%250ADecomposition%2520on%2520the%2520layerwise%2520activations%2520for%2520a%2520small%2520subset%2520of%2520samples%2520from%250Athe%2520retain%2520and%2520unlearn%2520classes%252C%2520respectively.%2520We%2520then%2520compute%2520the%2520shared%250Ainformation%2520between%2520these%2520spaces%2520and%2520remove%2520it%2520from%2520the%2520forget%2520space%2520to%2520isolate%250Aclass-discriminatory%2520feature%2520space.%2520Finally%252C%2520we%2520obtain%2520the%2520unlearned%2520model%2520by%250Aupdating%2520the%2520weights%2520to%2520suppress%2520the%2520class%2520discriminatory%2520features%2520from%2520the%250Aactivation%2520spaces.%2520We%2520demonstrate%2520our%2520algorithm%2527s%2520efficacy%2520on%2520ImageNet%2520using%2520a%250AVision%2520Transformer%2520with%2520only%2520%2524%255Csim%25201.5%255C%2525%2524%2520drop%2520in%2520retain%2520accuracy%2520compared%2520to%250Athe%2520original%2520model%2520while%2520maintaining%2520under%2520%25241%255C%2525%2524%2520accuracy%2520on%2520the%2520unlearned%250Aclass%2520samples.%2520Further%252C%2520our%2520algorithm%2520consistently%2520performs%2520well%2520when%2520subject%250Ato%2520Membership%2520Inference%2520Attacks%2520showing%2520%25247.8%255C%2525%2524%2520improvement%2520on%2520average%2520across%2520a%250Avariety%2520of%2520image%2520classification%2520datasets%2520and%2520network%2520architectures%252C%2520as%2520compared%250Ato%2520other%2520baselines%2520while%2520being%2520%2524%255Csim%25206%2520%255Ctimes%2524%2520more%2520computationally%2520efficient.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/sangamesh-kodge/class_forgetting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00761v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Unlearning%3A%20Fast%20and%20Efficient%20Training-free%20Approach%20to%20Class%0A%20%20Forgetting&entry.906535625=Sangamesh%20Kodge%20and%20Gobinda%20Saha%20and%20Kaushik%20Roy&entry.1292438233=%20%20Machine%20unlearning%20is%20a%20prominent%20and%20challenging%20field%2C%20driven%20by%20regulatory%0Ademands%20for%20user%20data%20deletion%20and%20heightened%20privacy%20awareness.%20Existing%0Aapproaches%20involve%20retraining%20model%20or%20multiple%20finetuning%20steps%20for%20each%0Adeletion%20request%2C%20often%20constrained%20by%20computational%20limits%20and%20restricted%20data%0Aaccess.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20class%20unlearning%20algorithm%20designed%0Ato%20strategically%20eliminate%20specific%20classes%20from%20the%20learned%20model.%20Our%0Aalgorithm%20first%20estimates%20the%20Retain%20and%20the%20Forget%20Spaces%20using%20Singular%20Value%0ADecomposition%20on%20the%20layerwise%20activations%20for%20a%20small%20subset%20of%20samples%20from%0Athe%20retain%20and%20unlearn%20classes%2C%20respectively.%20We%20then%20compute%20the%20shared%0Ainformation%20between%20these%20spaces%20and%20remove%20it%20from%20the%20forget%20space%20to%20isolate%0Aclass-discriminatory%20feature%20space.%20Finally%2C%20we%20obtain%20the%20unlearned%20model%20by%0Aupdating%20the%20weights%20to%20suppress%20the%20class%20discriminatory%20features%20from%20the%0Aactivation%20spaces.%20We%20demonstrate%20our%20algorithm%27s%20efficacy%20on%20ImageNet%20using%20a%0AVision%20Transformer%20with%20only%20%24%5Csim%201.5%5C%25%24%20drop%20in%20retain%20accuracy%20compared%20to%0Athe%20original%20model%20while%20maintaining%20under%20%241%5C%25%24%20accuracy%20on%20the%20unlearned%0Aclass%20samples.%20Further%2C%20our%20algorithm%20consistently%20performs%20well%20when%20subject%0Ato%20Membership%20Inference%20Attacks%20showing%20%247.8%5C%25%24%20improvement%20on%20average%20across%20a%0Avariety%20of%20image%20classification%20datasets%20and%20network%20architectures%2C%20as%20compared%0Ato%20other%20baselines%20while%20being%20%24%5Csim%206%20%5Ctimes%24%20more%20computationally%20efficient.%0AOur%20code%20is%20available%20at%20https%3A//github.com/sangamesh-kodge/class_forgetting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00761v3&entry.124074799=Read"},
{"title": "World Models for Autonomous Driving: An Initial Survey", "author": "Yanchen Guan and Haicheng Liao and Zhenning Li and Jia Hu and Runze Yuan and Yunjian Li and Guohui Zhang and Chengzhong Xu", "abstract": "  In the rapidly evolving landscape of autonomous driving, the capability to\naccurately predict future events and assess their implications is paramount for\nboth safety and efficiency, critically aiding the decision-making process.\nWorld models have emerged as a transformative approach, enabling autonomous\ndriving systems to synthesize and interpret vast amounts of sensor data,\nthereby predicting potential future scenarios and compensating for information\ngaps. This paper provides an initial review of the current state and\nprospective advancements of world models in autonomous driving, spanning their\ntheoretical underpinnings, practical applications, and the ongoing research\nefforts aimed at overcoming existing limitations. Highlighting the significant\nrole of world models in advancing autonomous driving technologies, this survey\naspires to serve as a foundational reference for the research community,\nfacilitating swift access to and comprehension of this burgeoning field, and\ninspiring continued innovation and exploration.\n", "link": "http://arxiv.org/abs/2403.02622v3", "date": "2024-05-07", "relevancy": 2.0599, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5207}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5163}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20World%20Models%20for%20Autonomous%20Driving%3A%20An%20Initial%20Survey&body=Title%3A%20World%20Models%20for%20Autonomous%20Driving%3A%20An%20Initial%20Survey%0AAuthor%3A%20Yanchen%20Guan%20and%20Haicheng%20Liao%20and%20Zhenning%20Li%20and%20Jia%20Hu%20and%20Runze%20Yuan%20and%20Yunjian%20Li%20and%20Guohui%20Zhang%20and%20Chengzhong%20Xu%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20landscape%20of%20autonomous%20driving%2C%20the%20capability%20to%0Aaccurately%20predict%20future%20events%20and%20assess%20their%20implications%20is%20paramount%20for%0Aboth%20safety%20and%20efficiency%2C%20critically%20aiding%20the%20decision-making%20process.%0AWorld%20models%20have%20emerged%20as%20a%20transformative%20approach%2C%20enabling%20autonomous%0Adriving%20systems%20to%20synthesize%20and%20interpret%20vast%20amounts%20of%20sensor%20data%2C%0Athereby%20predicting%20potential%20future%20scenarios%20and%20compensating%20for%20information%0Agaps.%20This%20paper%20provides%20an%20initial%20review%20of%20the%20current%20state%20and%0Aprospective%20advancements%20of%20world%20models%20in%20autonomous%20driving%2C%20spanning%20their%0Atheoretical%20underpinnings%2C%20practical%20applications%2C%20and%20the%20ongoing%20research%0Aefforts%20aimed%20at%20overcoming%20existing%20limitations.%20Highlighting%20the%20significant%0Arole%20of%20world%20models%20in%20advancing%20autonomous%20driving%20technologies%2C%20this%20survey%0Aaspires%20to%20serve%20as%20a%20foundational%20reference%20for%20the%20research%20community%2C%0Afacilitating%20swift%20access%20to%20and%20comprehension%20of%20this%20burgeoning%20field%2C%20and%0Ainspiring%20continued%20innovation%20and%20exploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02622v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWorld%2520Models%2520for%2520Autonomous%2520Driving%253A%2520An%2520Initial%2520Survey%26entry.906535625%3DYanchen%2520Guan%2520and%2520Haicheng%2520Liao%2520and%2520Zhenning%2520Li%2520and%2520Jia%2520Hu%2520and%2520Runze%2520Yuan%2520and%2520Yunjian%2520Li%2520and%2520Guohui%2520Zhang%2520and%2520Chengzhong%2520Xu%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520landscape%2520of%2520autonomous%2520driving%252C%2520the%2520capability%2520to%250Aaccurately%2520predict%2520future%2520events%2520and%2520assess%2520their%2520implications%2520is%2520paramount%2520for%250Aboth%2520safety%2520and%2520efficiency%252C%2520critically%2520aiding%2520the%2520decision-making%2520process.%250AWorld%2520models%2520have%2520emerged%2520as%2520a%2520transformative%2520approach%252C%2520enabling%2520autonomous%250Adriving%2520systems%2520to%2520synthesize%2520and%2520interpret%2520vast%2520amounts%2520of%2520sensor%2520data%252C%250Athereby%2520predicting%2520potential%2520future%2520scenarios%2520and%2520compensating%2520for%2520information%250Agaps.%2520This%2520paper%2520provides%2520an%2520initial%2520review%2520of%2520the%2520current%2520state%2520and%250Aprospective%2520advancements%2520of%2520world%2520models%2520in%2520autonomous%2520driving%252C%2520spanning%2520their%250Atheoretical%2520underpinnings%252C%2520practical%2520applications%252C%2520and%2520the%2520ongoing%2520research%250Aefforts%2520aimed%2520at%2520overcoming%2520existing%2520limitations.%2520Highlighting%2520the%2520significant%250Arole%2520of%2520world%2520models%2520in%2520advancing%2520autonomous%2520driving%2520technologies%252C%2520this%2520survey%250Aaspires%2520to%2520serve%2520as%2520a%2520foundational%2520reference%2520for%2520the%2520research%2520community%252C%250Afacilitating%2520swift%2520access%2520to%2520and%2520comprehension%2520of%2520this%2520burgeoning%2520field%252C%2520and%250Ainspiring%2520continued%2520innovation%2520and%2520exploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.02622v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=World%20Models%20for%20Autonomous%20Driving%3A%20An%20Initial%20Survey&entry.906535625=Yanchen%20Guan%20and%20Haicheng%20Liao%20and%20Zhenning%20Li%20and%20Jia%20Hu%20and%20Runze%20Yuan%20and%20Yunjian%20Li%20and%20Guohui%20Zhang%20and%20Chengzhong%20Xu&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20landscape%20of%20autonomous%20driving%2C%20the%20capability%20to%0Aaccurately%20predict%20future%20events%20and%20assess%20their%20implications%20is%20paramount%20for%0Aboth%20safety%20and%20efficiency%2C%20critically%20aiding%20the%20decision-making%20process.%0AWorld%20models%20have%20emerged%20as%20a%20transformative%20approach%2C%20enabling%20autonomous%0Adriving%20systems%20to%20synthesize%20and%20interpret%20vast%20amounts%20of%20sensor%20data%2C%0Athereby%20predicting%20potential%20future%20scenarios%20and%20compensating%20for%20information%0Agaps.%20This%20paper%20provides%20an%20initial%20review%20of%20the%20current%20state%20and%0Aprospective%20advancements%20of%20world%20models%20in%20autonomous%20driving%2C%20spanning%20their%0Atheoretical%20underpinnings%2C%20practical%20applications%2C%20and%20the%20ongoing%20research%0Aefforts%20aimed%20at%20overcoming%20existing%20limitations.%20Highlighting%20the%20significant%0Arole%20of%20world%20models%20in%20advancing%20autonomous%20driving%20technologies%2C%20this%20survey%0Aaspires%20to%20serve%20as%20a%20foundational%20reference%20for%20the%20research%20community%2C%0Afacilitating%20swift%20access%20to%20and%20comprehension%20of%20this%20burgeoning%20field%2C%20and%0Ainspiring%20continued%20innovation%20and%20exploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02622v3&entry.124074799=Read"},
{"title": "M3oE: Multi-Domain Multi-Task Mixture-of Experts Recommendation\n  Framework", "author": "Zijian Zhang and Shuchang Liu and Jiaao Yu and Qingpeng Cai and Xiangyu Zhao and Chunxu Zhang and Ziru Liu and Qidong Liu and Hongwei Zhao and Lantao Hu and Peng Jiang and Kun Gai", "abstract": "  Multi-domain recommendation and multi-task recommendation have demonstrated\ntheir effectiveness in leveraging common information from different domains and\nobjectives for comprehensive user modeling. Nonetheless, the practical\nrecommendation usually faces multiple domains and tasks simultaneously, which\ncannot be well-addressed by current methods. To this end, we introduce M3oE, an\nadaptive multi-domain multi-task mixture-of-experts recommendation framework.\nM3oE integrates multi-domain information, maps knowledge across domains and\ntasks, and optimizes multiple objectives. We leverage three mixture-of-experts\nmodules to learn common, domain-aspect, and task-aspect user preferences\nrespectively to address the complex dependencies among multiple domains and\ntasks in a disentangled manner. Additionally, we design a two-level fusion\nmechanism for precise control over feature extraction and fusion across diverse\ndomains and tasks. The framework's adaptability is further enhanced by applying\nAutoML technique, which allows dynamic structure optimization. To the best of\nthe authors' knowledge, our M3oE is the first effort to solve multi-domain\nmulti-task recommendation self-adaptively. Extensive experiments on two\nbenchmark datasets against diverse baselines demonstrate M3oE's superior\nperformance. The implementation code is available to ensure reproducibility.\n", "link": "http://arxiv.org/abs/2404.18465v2", "date": "2024-05-07", "relevancy": 2.0599, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5323}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5242}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M3oE%3A%20Multi-Domain%20Multi-Task%20Mixture-of%20Experts%20Recommendation%0A%20%20Framework&body=Title%3A%20M3oE%3A%20Multi-Domain%20Multi-Task%20Mixture-of%20Experts%20Recommendation%0A%20%20Framework%0AAuthor%3A%20Zijian%20Zhang%20and%20Shuchang%20Liu%20and%20Jiaao%20Yu%20and%20Qingpeng%20Cai%20and%20Xiangyu%20Zhao%20and%20Chunxu%20Zhang%20and%20Ziru%20Liu%20and%20Qidong%20Liu%20and%20Hongwei%20Zhao%20and%20Lantao%20Hu%20and%20Peng%20Jiang%20and%20Kun%20Gai%0AAbstract%3A%20%20%20Multi-domain%20recommendation%20and%20multi-task%20recommendation%20have%20demonstrated%0Atheir%20effectiveness%20in%20leveraging%20common%20information%20from%20different%20domains%20and%0Aobjectives%20for%20comprehensive%20user%20modeling.%20Nonetheless%2C%20the%20practical%0Arecommendation%20usually%20faces%20multiple%20domains%20and%20tasks%20simultaneously%2C%20which%0Acannot%20be%20well-addressed%20by%20current%20methods.%20To%20this%20end%2C%20we%20introduce%20M3oE%2C%20an%0Aadaptive%20multi-domain%20multi-task%20mixture-of-experts%20recommendation%20framework.%0AM3oE%20integrates%20multi-domain%20information%2C%20maps%20knowledge%20across%20domains%20and%0Atasks%2C%20and%20optimizes%20multiple%20objectives.%20We%20leverage%20three%20mixture-of-experts%0Amodules%20to%20learn%20common%2C%20domain-aspect%2C%20and%20task-aspect%20user%20preferences%0Arespectively%20to%20address%20the%20complex%20dependencies%20among%20multiple%20domains%20and%0Atasks%20in%20a%20disentangled%20manner.%20Additionally%2C%20we%20design%20a%20two-level%20fusion%0Amechanism%20for%20precise%20control%20over%20feature%20extraction%20and%20fusion%20across%20diverse%0Adomains%20and%20tasks.%20The%20framework%27s%20adaptability%20is%20further%20enhanced%20by%20applying%0AAutoML%20technique%2C%20which%20allows%20dynamic%20structure%20optimization.%20To%20the%20best%20of%0Athe%20authors%27%20knowledge%2C%20our%20M3oE%20is%20the%20first%20effort%20to%20solve%20multi-domain%0Amulti-task%20recommendation%20self-adaptively.%20Extensive%20experiments%20on%20two%0Abenchmark%20datasets%20against%20diverse%20baselines%20demonstrate%20M3oE%27s%20superior%0Aperformance.%20The%20implementation%20code%20is%20available%20to%20ensure%20reproducibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18465v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM3oE%253A%2520Multi-Domain%2520Multi-Task%2520Mixture-of%2520Experts%2520Recommendation%250A%2520%2520Framework%26entry.906535625%3DZijian%2520Zhang%2520and%2520Shuchang%2520Liu%2520and%2520Jiaao%2520Yu%2520and%2520Qingpeng%2520Cai%2520and%2520Xiangyu%2520Zhao%2520and%2520Chunxu%2520Zhang%2520and%2520Ziru%2520Liu%2520and%2520Qidong%2520Liu%2520and%2520Hongwei%2520Zhao%2520and%2520Lantao%2520Hu%2520and%2520Peng%2520Jiang%2520and%2520Kun%2520Gai%26entry.1292438233%3D%2520%2520Multi-domain%2520recommendation%2520and%2520multi-task%2520recommendation%2520have%2520demonstrated%250Atheir%2520effectiveness%2520in%2520leveraging%2520common%2520information%2520from%2520different%2520domains%2520and%250Aobjectives%2520for%2520comprehensive%2520user%2520modeling.%2520Nonetheless%252C%2520the%2520practical%250Arecommendation%2520usually%2520faces%2520multiple%2520domains%2520and%2520tasks%2520simultaneously%252C%2520which%250Acannot%2520be%2520well-addressed%2520by%2520current%2520methods.%2520To%2520this%2520end%252C%2520we%2520introduce%2520M3oE%252C%2520an%250Aadaptive%2520multi-domain%2520multi-task%2520mixture-of-experts%2520recommendation%2520framework.%250AM3oE%2520integrates%2520multi-domain%2520information%252C%2520maps%2520knowledge%2520across%2520domains%2520and%250Atasks%252C%2520and%2520optimizes%2520multiple%2520objectives.%2520We%2520leverage%2520three%2520mixture-of-experts%250Amodules%2520to%2520learn%2520common%252C%2520domain-aspect%252C%2520and%2520task-aspect%2520user%2520preferences%250Arespectively%2520to%2520address%2520the%2520complex%2520dependencies%2520among%2520multiple%2520domains%2520and%250Atasks%2520in%2520a%2520disentangled%2520manner.%2520Additionally%252C%2520we%2520design%2520a%2520two-level%2520fusion%250Amechanism%2520for%2520precise%2520control%2520over%2520feature%2520extraction%2520and%2520fusion%2520across%2520diverse%250Adomains%2520and%2520tasks.%2520The%2520framework%2527s%2520adaptability%2520is%2520further%2520enhanced%2520by%2520applying%250AAutoML%2520technique%252C%2520which%2520allows%2520dynamic%2520structure%2520optimization.%2520To%2520the%2520best%2520of%250Athe%2520authors%2527%2520knowledge%252C%2520our%2520M3oE%2520is%2520the%2520first%2520effort%2520to%2520solve%2520multi-domain%250Amulti-task%2520recommendation%2520self-adaptively.%2520Extensive%2520experiments%2520on%2520two%250Abenchmark%2520datasets%2520against%2520diverse%2520baselines%2520demonstrate%2520M3oE%2527s%2520superior%250Aperformance.%2520The%2520implementation%2520code%2520is%2520available%2520to%2520ensure%2520reproducibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18465v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M3oE%3A%20Multi-Domain%20Multi-Task%20Mixture-of%20Experts%20Recommendation%0A%20%20Framework&entry.906535625=Zijian%20Zhang%20and%20Shuchang%20Liu%20and%20Jiaao%20Yu%20and%20Qingpeng%20Cai%20and%20Xiangyu%20Zhao%20and%20Chunxu%20Zhang%20and%20Ziru%20Liu%20and%20Qidong%20Liu%20and%20Hongwei%20Zhao%20and%20Lantao%20Hu%20and%20Peng%20Jiang%20and%20Kun%20Gai&entry.1292438233=%20%20Multi-domain%20recommendation%20and%20multi-task%20recommendation%20have%20demonstrated%0Atheir%20effectiveness%20in%20leveraging%20common%20information%20from%20different%20domains%20and%0Aobjectives%20for%20comprehensive%20user%20modeling.%20Nonetheless%2C%20the%20practical%0Arecommendation%20usually%20faces%20multiple%20domains%20and%20tasks%20simultaneously%2C%20which%0Acannot%20be%20well-addressed%20by%20current%20methods.%20To%20this%20end%2C%20we%20introduce%20M3oE%2C%20an%0Aadaptive%20multi-domain%20multi-task%20mixture-of-experts%20recommendation%20framework.%0AM3oE%20integrates%20multi-domain%20information%2C%20maps%20knowledge%20across%20domains%20and%0Atasks%2C%20and%20optimizes%20multiple%20objectives.%20We%20leverage%20three%20mixture-of-experts%0Amodules%20to%20learn%20common%2C%20domain-aspect%2C%20and%20task-aspect%20user%20preferences%0Arespectively%20to%20address%20the%20complex%20dependencies%20among%20multiple%20domains%20and%0Atasks%20in%20a%20disentangled%20manner.%20Additionally%2C%20we%20design%20a%20two-level%20fusion%0Amechanism%20for%20precise%20control%20over%20feature%20extraction%20and%20fusion%20across%20diverse%0Adomains%20and%20tasks.%20The%20framework%27s%20adaptability%20is%20further%20enhanced%20by%20applying%0AAutoML%20technique%2C%20which%20allows%20dynamic%20structure%20optimization.%20To%20the%20best%20of%0Athe%20authors%27%20knowledge%2C%20our%20M3oE%20is%20the%20first%20effort%20to%20solve%20multi-domain%0Amulti-task%20recommendation%20self-adaptively.%20Extensive%20experiments%20on%20two%0Abenchmark%20datasets%20against%20diverse%20baselines%20demonstrate%20M3oE%27s%20superior%0Aperformance.%20The%20implementation%20code%20is%20available%20to%20ensure%20reproducibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18465v2&entry.124074799=Read"},
{"title": "TorchDriveEnv: A Reinforcement Learning Benchmark for Autonomous Driving\n  with Reactive, Realistic, and Diverse Non-Playable Characters", "author": "Jonathan Wilder Lavington and Ke Zhang and Vasileios Lioutas and Matthew Niedoba and Yunpeng Liu and Dylan Green and Saeid Naderiparizi and Xiaoxuan Liang and Setareh Dabiri and Adam \u015acibior and Berend Zwartsenberg and Frank Wood", "abstract": "  The training, testing, and deployment, of autonomous vehicles requires\nrealistic and efficient simulators. Moreover, because of the high variability\nbetween different problems presented in different autonomous systems, these\nsimulators need to be easy to use, and easy to modify. To address these\nproblems we introduce TorchDriveSim and its benchmark extension TorchDriveEnv.\nTorchDriveEnv is a lightweight reinforcement learning benchmark programmed\nentirely in Python, which can be modified to test a number of different factors\nin learned vehicle behavior, including the effect of varying kinematic models,\nagent types, and traffic control patterns. Most importantly unlike many replay\nbased simulation approaches, TorchDriveEnv is fully integrated with a state of\nthe art behavioral simulation API. This allows users to train and evaluate\ndriving models alongside data driven Non-Playable Characters (NPC) whose\ninitializations and driving behavior are reactive, realistic, and diverse. We\nillustrate the efficiency and simplicity of TorchDriveEnv by evaluating common\nreinforcement learning baselines in both training and validation environments.\nOur experiments show that TorchDriveEnv is easy to use, but difficult to solve.\n", "link": "http://arxiv.org/abs/2405.04491v1", "date": "2024-05-07", "relevancy": 2.0437, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5127}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5108}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TorchDriveEnv%3A%20A%20Reinforcement%20Learning%20Benchmark%20for%20Autonomous%20Driving%0A%20%20with%20Reactive%2C%20Realistic%2C%20and%20Diverse%20Non-Playable%20Characters&body=Title%3A%20TorchDriveEnv%3A%20A%20Reinforcement%20Learning%20Benchmark%20for%20Autonomous%20Driving%0A%20%20with%20Reactive%2C%20Realistic%2C%20and%20Diverse%20Non-Playable%20Characters%0AAuthor%3A%20Jonathan%20Wilder%20Lavington%20and%20Ke%20Zhang%20and%20Vasileios%20Lioutas%20and%20Matthew%20Niedoba%20and%20Yunpeng%20Liu%20and%20Dylan%20Green%20and%20Saeid%20Naderiparizi%20and%20Xiaoxuan%20Liang%20and%20Setareh%20Dabiri%20and%20Adam%20%C5%9Acibior%20and%20Berend%20Zwartsenberg%20and%20Frank%20Wood%0AAbstract%3A%20%20%20The%20training%2C%20testing%2C%20and%20deployment%2C%20of%20autonomous%20vehicles%20requires%0Arealistic%20and%20efficient%20simulators.%20Moreover%2C%20because%20of%20the%20high%20variability%0Abetween%20different%20problems%20presented%20in%20different%20autonomous%20systems%2C%20these%0Asimulators%20need%20to%20be%20easy%20to%20use%2C%20and%20easy%20to%20modify.%20To%20address%20these%0Aproblems%20we%20introduce%20TorchDriveSim%20and%20its%20benchmark%20extension%20TorchDriveEnv.%0ATorchDriveEnv%20is%20a%20lightweight%20reinforcement%20learning%20benchmark%20programmed%0Aentirely%20in%20Python%2C%20which%20can%20be%20modified%20to%20test%20a%20number%20of%20different%20factors%0Ain%20learned%20vehicle%20behavior%2C%20including%20the%20effect%20of%20varying%20kinematic%20models%2C%0Aagent%20types%2C%20and%20traffic%20control%20patterns.%20Most%20importantly%20unlike%20many%20replay%0Abased%20simulation%20approaches%2C%20TorchDriveEnv%20is%20fully%20integrated%20with%20a%20state%20of%0Athe%20art%20behavioral%20simulation%20API.%20This%20allows%20users%20to%20train%20and%20evaluate%0Adriving%20models%20alongside%20data%20driven%20Non-Playable%20Characters%20%28NPC%29%20whose%0Ainitializations%20and%20driving%20behavior%20are%20reactive%2C%20realistic%2C%20and%20diverse.%20We%0Aillustrate%20the%20efficiency%20and%20simplicity%20of%20TorchDriveEnv%20by%20evaluating%20common%0Areinforcement%20learning%20baselines%20in%20both%20training%20and%20validation%20environments.%0AOur%20experiments%20show%20that%20TorchDriveEnv%20is%20easy%20to%20use%2C%20but%20difficult%20to%20solve.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04491v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTorchDriveEnv%253A%2520A%2520Reinforcement%2520Learning%2520Benchmark%2520for%2520Autonomous%2520Driving%250A%2520%2520with%2520Reactive%252C%2520Realistic%252C%2520and%2520Diverse%2520Non-Playable%2520Characters%26entry.906535625%3DJonathan%2520Wilder%2520Lavington%2520and%2520Ke%2520Zhang%2520and%2520Vasileios%2520Lioutas%2520and%2520Matthew%2520Niedoba%2520and%2520Yunpeng%2520Liu%2520and%2520Dylan%2520Green%2520and%2520Saeid%2520Naderiparizi%2520and%2520Xiaoxuan%2520Liang%2520and%2520Setareh%2520Dabiri%2520and%2520Adam%2520%25C5%259Acibior%2520and%2520Berend%2520Zwartsenberg%2520and%2520Frank%2520Wood%26entry.1292438233%3D%2520%2520The%2520training%252C%2520testing%252C%2520and%2520deployment%252C%2520of%2520autonomous%2520vehicles%2520requires%250Arealistic%2520and%2520efficient%2520simulators.%2520Moreover%252C%2520because%2520of%2520the%2520high%2520variability%250Abetween%2520different%2520problems%2520presented%2520in%2520different%2520autonomous%2520systems%252C%2520these%250Asimulators%2520need%2520to%2520be%2520easy%2520to%2520use%252C%2520and%2520easy%2520to%2520modify.%2520To%2520address%2520these%250Aproblems%2520we%2520introduce%2520TorchDriveSim%2520and%2520its%2520benchmark%2520extension%2520TorchDriveEnv.%250ATorchDriveEnv%2520is%2520a%2520lightweight%2520reinforcement%2520learning%2520benchmark%2520programmed%250Aentirely%2520in%2520Python%252C%2520which%2520can%2520be%2520modified%2520to%2520test%2520a%2520number%2520of%2520different%2520factors%250Ain%2520learned%2520vehicle%2520behavior%252C%2520including%2520the%2520effect%2520of%2520varying%2520kinematic%2520models%252C%250Aagent%2520types%252C%2520and%2520traffic%2520control%2520patterns.%2520Most%2520importantly%2520unlike%2520many%2520replay%250Abased%2520simulation%2520approaches%252C%2520TorchDriveEnv%2520is%2520fully%2520integrated%2520with%2520a%2520state%2520of%250Athe%2520art%2520behavioral%2520simulation%2520API.%2520This%2520allows%2520users%2520to%2520train%2520and%2520evaluate%250Adriving%2520models%2520alongside%2520data%2520driven%2520Non-Playable%2520Characters%2520%2528NPC%2529%2520whose%250Ainitializations%2520and%2520driving%2520behavior%2520are%2520reactive%252C%2520realistic%252C%2520and%2520diverse.%2520We%250Aillustrate%2520the%2520efficiency%2520and%2520simplicity%2520of%2520TorchDriveEnv%2520by%2520evaluating%2520common%250Areinforcement%2520learning%2520baselines%2520in%2520both%2520training%2520and%2520validation%2520environments.%250AOur%2520experiments%2520show%2520that%2520TorchDriveEnv%2520is%2520easy%2520to%2520use%252C%2520but%2520difficult%2520to%2520solve.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04491v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TorchDriveEnv%3A%20A%20Reinforcement%20Learning%20Benchmark%20for%20Autonomous%20Driving%0A%20%20with%20Reactive%2C%20Realistic%2C%20and%20Diverse%20Non-Playable%20Characters&entry.906535625=Jonathan%20Wilder%20Lavington%20and%20Ke%20Zhang%20and%20Vasileios%20Lioutas%20and%20Matthew%20Niedoba%20and%20Yunpeng%20Liu%20and%20Dylan%20Green%20and%20Saeid%20Naderiparizi%20and%20Xiaoxuan%20Liang%20and%20Setareh%20Dabiri%20and%20Adam%20%C5%9Acibior%20and%20Berend%20Zwartsenberg%20and%20Frank%20Wood&entry.1292438233=%20%20The%20training%2C%20testing%2C%20and%20deployment%2C%20of%20autonomous%20vehicles%20requires%0Arealistic%20and%20efficient%20simulators.%20Moreover%2C%20because%20of%20the%20high%20variability%0Abetween%20different%20problems%20presented%20in%20different%20autonomous%20systems%2C%20these%0Asimulators%20need%20to%20be%20easy%20to%20use%2C%20and%20easy%20to%20modify.%20To%20address%20these%0Aproblems%20we%20introduce%20TorchDriveSim%20and%20its%20benchmark%20extension%20TorchDriveEnv.%0ATorchDriveEnv%20is%20a%20lightweight%20reinforcement%20learning%20benchmark%20programmed%0Aentirely%20in%20Python%2C%20which%20can%20be%20modified%20to%20test%20a%20number%20of%20different%20factors%0Ain%20learned%20vehicle%20behavior%2C%20including%20the%20effect%20of%20varying%20kinematic%20models%2C%0Aagent%20types%2C%20and%20traffic%20control%20patterns.%20Most%20importantly%20unlike%20many%20replay%0Abased%20simulation%20approaches%2C%20TorchDriveEnv%20is%20fully%20integrated%20with%20a%20state%20of%0Athe%20art%20behavioral%20simulation%20API.%20This%20allows%20users%20to%20train%20and%20evaluate%0Adriving%20models%20alongside%20data%20driven%20Non-Playable%20Characters%20%28NPC%29%20whose%0Ainitializations%20and%20driving%20behavior%20are%20reactive%2C%20realistic%2C%20and%20diverse.%20We%0Aillustrate%20the%20efficiency%20and%20simplicity%20of%20TorchDriveEnv%20by%20evaluating%20common%0Areinforcement%20learning%20baselines%20in%20both%20training%20and%20validation%20environments.%0AOur%20experiments%20show%20that%20TorchDriveEnv%20is%20easy%20to%20use%2C%20but%20difficult%20to%20solve.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04491v1&entry.124074799=Read"},
{"title": "A General Model for Detecting Learner Engagement: Implementation and\n  Evaluation", "author": "Somayeh Malekshahi and Javad M. Kheyridoost and Omid Fatemi", "abstract": "  Considering learner engagement has a mutual benefit for both learners and\ninstructors. Instructors can help learners increase their attention,\ninvolvement, motivation, and interest. On the other hand, instructors can\nimprove their instructional performance by evaluating the cumulative results of\nall learners and upgrading their training programs. This paper proposes a\ngeneral, lightweight model for selecting and processing features to detect\nlearners' engagement levels while preserving the sequential temporal\nrelationship over time. During training and testing, we analyzed the videos\nfrom the publicly available DAiSEE dataset to capture the dynamic essence of\nlearner engagement. We have also proposed an adaptation policy to find new\nlabels that utilize the affective states of this dataset related to education,\nthereby improving the models' judgment. The suggested model achieves an\naccuracy of 68.57\\% in a specific implementation and outperforms the studied\nstate-of-the-art models detecting learners' engagement levels.\n", "link": "http://arxiv.org/abs/2405.04251v1", "date": "2024-05-07", "relevancy": 2.0397, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5202}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5115}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20General%20Model%20for%20Detecting%20Learner%20Engagement%3A%20Implementation%20and%0A%20%20Evaluation&body=Title%3A%20A%20General%20Model%20for%20Detecting%20Learner%20Engagement%3A%20Implementation%20and%0A%20%20Evaluation%0AAuthor%3A%20Somayeh%20Malekshahi%20and%20Javad%20M.%20Kheyridoost%20and%20Omid%20Fatemi%0AAbstract%3A%20%20%20Considering%20learner%20engagement%20has%20a%20mutual%20benefit%20for%20both%20learners%20and%0Ainstructors.%20Instructors%20can%20help%20learners%20increase%20their%20attention%2C%0Ainvolvement%2C%20motivation%2C%20and%20interest.%20On%20the%20other%20hand%2C%20instructors%20can%0Aimprove%20their%20instructional%20performance%20by%20evaluating%20the%20cumulative%20results%20of%0Aall%20learners%20and%20upgrading%20their%20training%20programs.%20This%20paper%20proposes%20a%0Ageneral%2C%20lightweight%20model%20for%20selecting%20and%20processing%20features%20to%20detect%0Alearners%27%20engagement%20levels%20while%20preserving%20the%20sequential%20temporal%0Arelationship%20over%20time.%20During%20training%20and%20testing%2C%20we%20analyzed%20the%20videos%0Afrom%20the%20publicly%20available%20DAiSEE%20dataset%20to%20capture%20the%20dynamic%20essence%20of%0Alearner%20engagement.%20We%20have%20also%20proposed%20an%20adaptation%20policy%20to%20find%20new%0Alabels%20that%20utilize%20the%20affective%20states%20of%20this%20dataset%20related%20to%20education%2C%0Athereby%20improving%20the%20models%27%20judgment.%20The%20suggested%20model%20achieves%20an%0Aaccuracy%20of%2068.57%5C%25%20in%20a%20specific%20implementation%20and%20outperforms%20the%20studied%0Astate-of-the-art%20models%20detecting%20learners%27%20engagement%20levels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520General%2520Model%2520for%2520Detecting%2520Learner%2520Engagement%253A%2520Implementation%2520and%250A%2520%2520Evaluation%26entry.906535625%3DSomayeh%2520Malekshahi%2520and%2520Javad%2520M.%2520Kheyridoost%2520and%2520Omid%2520Fatemi%26entry.1292438233%3D%2520%2520Considering%2520learner%2520engagement%2520has%2520a%2520mutual%2520benefit%2520for%2520both%2520learners%2520and%250Ainstructors.%2520Instructors%2520can%2520help%2520learners%2520increase%2520their%2520attention%252C%250Ainvolvement%252C%2520motivation%252C%2520and%2520interest.%2520On%2520the%2520other%2520hand%252C%2520instructors%2520can%250Aimprove%2520their%2520instructional%2520performance%2520by%2520evaluating%2520the%2520cumulative%2520results%2520of%250Aall%2520learners%2520and%2520upgrading%2520their%2520training%2520programs.%2520This%2520paper%2520proposes%2520a%250Ageneral%252C%2520lightweight%2520model%2520for%2520selecting%2520and%2520processing%2520features%2520to%2520detect%250Alearners%2527%2520engagement%2520levels%2520while%2520preserving%2520the%2520sequential%2520temporal%250Arelationship%2520over%2520time.%2520During%2520training%2520and%2520testing%252C%2520we%2520analyzed%2520the%2520videos%250Afrom%2520the%2520publicly%2520available%2520DAiSEE%2520dataset%2520to%2520capture%2520the%2520dynamic%2520essence%2520of%250Alearner%2520engagement.%2520We%2520have%2520also%2520proposed%2520an%2520adaptation%2520policy%2520to%2520find%2520new%250Alabels%2520that%2520utilize%2520the%2520affective%2520states%2520of%2520this%2520dataset%2520related%2520to%2520education%252C%250Athereby%2520improving%2520the%2520models%2527%2520judgment.%2520The%2520suggested%2520model%2520achieves%2520an%250Aaccuracy%2520of%252068.57%255C%2525%2520in%2520a%2520specific%2520implementation%2520and%2520outperforms%2520the%2520studied%250Astate-of-the-art%2520models%2520detecting%2520learners%2527%2520engagement%2520levels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20General%20Model%20for%20Detecting%20Learner%20Engagement%3A%20Implementation%20and%0A%20%20Evaluation&entry.906535625=Somayeh%20Malekshahi%20and%20Javad%20M.%20Kheyridoost%20and%20Omid%20Fatemi&entry.1292438233=%20%20Considering%20learner%20engagement%20has%20a%20mutual%20benefit%20for%20both%20learners%20and%0Ainstructors.%20Instructors%20can%20help%20learners%20increase%20their%20attention%2C%0Ainvolvement%2C%20motivation%2C%20and%20interest.%20On%20the%20other%20hand%2C%20instructors%20can%0Aimprove%20their%20instructional%20performance%20by%20evaluating%20the%20cumulative%20results%20of%0Aall%20learners%20and%20upgrading%20their%20training%20programs.%20This%20paper%20proposes%20a%0Ageneral%2C%20lightweight%20model%20for%20selecting%20and%20processing%20features%20to%20detect%0Alearners%27%20engagement%20levels%20while%20preserving%20the%20sequential%20temporal%0Arelationship%20over%20time.%20During%20training%20and%20testing%2C%20we%20analyzed%20the%20videos%0Afrom%20the%20publicly%20available%20DAiSEE%20dataset%20to%20capture%20the%20dynamic%20essence%20of%0Alearner%20engagement.%20We%20have%20also%20proposed%20an%20adaptation%20policy%20to%20find%20new%0Alabels%20that%20utilize%20the%20affective%20states%20of%20this%20dataset%20related%20to%20education%2C%0Athereby%20improving%20the%20models%27%20judgment.%20The%20suggested%20model%20achieves%20an%0Aaccuracy%20of%2068.57%5C%25%20in%20a%20specific%20implementation%20and%20outperforms%20the%20studied%0Astate-of-the-art%20models%20detecting%20learners%27%20engagement%20levels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04251v1&entry.124074799=Read"},
{"title": "Sign2GPT: Leveraging Large Language Models for Gloss-Free Sign Language\n  Translation", "author": "Ryan Wong and Necati Cihan Camgoz and Richard Bowden", "abstract": "  Automatic Sign Language Translation requires the integration of both computer\nvision and natural language processing to effectively bridge the communication\ngap between sign and spoken languages. However, the deficiency in large-scale\ntraining data to support sign language translation means we need to leverage\nresources from spoken language. We introduce, Sign2GPT, a novel framework for\nsign language translation that utilizes large-scale pretrained vision and\nlanguage models via lightweight adapters for gloss-free sign language\ntranslation. The lightweight adapters are crucial for sign language\ntranslation, due to the constraints imposed by limited dataset sizes and the\ncomputational requirements when training with long sign videos. We also propose\na novel pretraining strategy that directs our encoder to learn sign\nrepresentations from automatically extracted pseudo-glosses without requiring\ngloss order information or annotations. We evaluate our approach on two public\nbenchmark sign language translation datasets, namely RWTH-PHOENIX-Weather 2014T\nand CSL-Daily, and improve on state-of-the-art gloss-free translation\nperformance with a significant margin.\n", "link": "http://arxiv.org/abs/2405.04164v1", "date": "2024-05-07", "relevancy": 2.0395, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5319}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5182}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sign2GPT%3A%20Leveraging%20Large%20Language%20Models%20for%20Gloss-Free%20Sign%20Language%0A%20%20Translation&body=Title%3A%20Sign2GPT%3A%20Leveraging%20Large%20Language%20Models%20for%20Gloss-Free%20Sign%20Language%0A%20%20Translation%0AAuthor%3A%20Ryan%20Wong%20and%20Necati%20Cihan%20Camgoz%20and%20Richard%20Bowden%0AAbstract%3A%20%20%20Automatic%20Sign%20Language%20Translation%20requires%20the%20integration%20of%20both%20computer%0Avision%20and%20natural%20language%20processing%20to%20effectively%20bridge%20the%20communication%0Agap%20between%20sign%20and%20spoken%20languages.%20However%2C%20the%20deficiency%20in%20large-scale%0Atraining%20data%20to%20support%20sign%20language%20translation%20means%20we%20need%20to%20leverage%0Aresources%20from%20spoken%20language.%20We%20introduce%2C%20Sign2GPT%2C%20a%20novel%20framework%20for%0Asign%20language%20translation%20that%20utilizes%20large-scale%20pretrained%20vision%20and%0Alanguage%20models%20via%20lightweight%20adapters%20for%20gloss-free%20sign%20language%0Atranslation.%20The%20lightweight%20adapters%20are%20crucial%20for%20sign%20language%0Atranslation%2C%20due%20to%20the%20constraints%20imposed%20by%20limited%20dataset%20sizes%20and%20the%0Acomputational%20requirements%20when%20training%20with%20long%20sign%20videos.%20We%20also%20propose%0Aa%20novel%20pretraining%20strategy%20that%20directs%20our%20encoder%20to%20learn%20sign%0Arepresentations%20from%20automatically%20extracted%20pseudo-glosses%20without%20requiring%0Agloss%20order%20information%20or%20annotations.%20We%20evaluate%20our%20approach%20on%20two%20public%0Abenchmark%20sign%20language%20translation%20datasets%2C%20namely%20RWTH-PHOENIX-Weather%202014T%0Aand%20CSL-Daily%2C%20and%20improve%20on%20state-of-the-art%20gloss-free%20translation%0Aperformance%20with%20a%20significant%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSign2GPT%253A%2520Leveraging%2520Large%2520Language%2520Models%2520for%2520Gloss-Free%2520Sign%2520Language%250A%2520%2520Translation%26entry.906535625%3DRyan%2520Wong%2520and%2520Necati%2520Cihan%2520Camgoz%2520and%2520Richard%2520Bowden%26entry.1292438233%3D%2520%2520Automatic%2520Sign%2520Language%2520Translation%2520requires%2520the%2520integration%2520of%2520both%2520computer%250Avision%2520and%2520natural%2520language%2520processing%2520to%2520effectively%2520bridge%2520the%2520communication%250Agap%2520between%2520sign%2520and%2520spoken%2520languages.%2520However%252C%2520the%2520deficiency%2520in%2520large-scale%250Atraining%2520data%2520to%2520support%2520sign%2520language%2520translation%2520means%2520we%2520need%2520to%2520leverage%250Aresources%2520from%2520spoken%2520language.%2520We%2520introduce%252C%2520Sign2GPT%252C%2520a%2520novel%2520framework%2520for%250Asign%2520language%2520translation%2520that%2520utilizes%2520large-scale%2520pretrained%2520vision%2520and%250Alanguage%2520models%2520via%2520lightweight%2520adapters%2520for%2520gloss-free%2520sign%2520language%250Atranslation.%2520The%2520lightweight%2520adapters%2520are%2520crucial%2520for%2520sign%2520language%250Atranslation%252C%2520due%2520to%2520the%2520constraints%2520imposed%2520by%2520limited%2520dataset%2520sizes%2520and%2520the%250Acomputational%2520requirements%2520when%2520training%2520with%2520long%2520sign%2520videos.%2520We%2520also%2520propose%250Aa%2520novel%2520pretraining%2520strategy%2520that%2520directs%2520our%2520encoder%2520to%2520learn%2520sign%250Arepresentations%2520from%2520automatically%2520extracted%2520pseudo-glosses%2520without%2520requiring%250Agloss%2520order%2520information%2520or%2520annotations.%2520We%2520evaluate%2520our%2520approach%2520on%2520two%2520public%250Abenchmark%2520sign%2520language%2520translation%2520datasets%252C%2520namely%2520RWTH-PHOENIX-Weather%25202014T%250Aand%2520CSL-Daily%252C%2520and%2520improve%2520on%2520state-of-the-art%2520gloss-free%2520translation%250Aperformance%2520with%2520a%2520significant%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sign2GPT%3A%20Leveraging%20Large%20Language%20Models%20for%20Gloss-Free%20Sign%20Language%0A%20%20Translation&entry.906535625=Ryan%20Wong%20and%20Necati%20Cihan%20Camgoz%20and%20Richard%20Bowden&entry.1292438233=%20%20Automatic%20Sign%20Language%20Translation%20requires%20the%20integration%20of%20both%20computer%0Avision%20and%20natural%20language%20processing%20to%20effectively%20bridge%20the%20communication%0Agap%20between%20sign%20and%20spoken%20languages.%20However%2C%20the%20deficiency%20in%20large-scale%0Atraining%20data%20to%20support%20sign%20language%20translation%20means%20we%20need%20to%20leverage%0Aresources%20from%20spoken%20language.%20We%20introduce%2C%20Sign2GPT%2C%20a%20novel%20framework%20for%0Asign%20language%20translation%20that%20utilizes%20large-scale%20pretrained%20vision%20and%0Alanguage%20models%20via%20lightweight%20adapters%20for%20gloss-free%20sign%20language%0Atranslation.%20The%20lightweight%20adapters%20are%20crucial%20for%20sign%20language%0Atranslation%2C%20due%20to%20the%20constraints%20imposed%20by%20limited%20dataset%20sizes%20and%20the%0Acomputational%20requirements%20when%20training%20with%20long%20sign%20videos.%20We%20also%20propose%0Aa%20novel%20pretraining%20strategy%20that%20directs%20our%20encoder%20to%20learn%20sign%0Arepresentations%20from%20automatically%20extracted%20pseudo-glosses%20without%20requiring%0Agloss%20order%20information%20or%20annotations.%20We%20evaluate%20our%20approach%20on%20two%20public%0Abenchmark%20sign%20language%20translation%20datasets%2C%20namely%20RWTH-PHOENIX-Weather%202014T%0Aand%20CSL-Daily%2C%20and%20improve%20on%20state-of-the-art%20gloss-free%20translation%0Aperformance%20with%20a%20significant%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04164v1&entry.124074799=Read"},
{"title": "Efficient Online Set-valued Classification with Bandit Feedback", "author": "Zhou Wang and Xingye Qiao", "abstract": "  Conformal prediction is a distribution-free method that wraps a given machine\nlearning model and returns a set of plausible labels that contain the true\nlabel with a prescribed coverage rate. In practice, the empirical coverage\nachieved highly relies on fully observed label information from data both in\nthe training phase for model fitting and the calibration phase for quantile\nestimation. This dependency poses a challenge in the context of online learning\nwith bandit feedback, where a learner only has access to the correctness of\nactions (i.e., pulled an arm) but not the full information of the true label.\nIn particular, when the pulled arm is incorrect, the learner only knows that\nthe pulled one is not the true class label, but does not know which label is\ntrue. Additionally, bandit feedback further results in a smaller labeled\ndataset for calibration, limited to instances with correct actions, thereby\naffecting the accuracy of quantile estimation. To address these limitations, we\npropose Bandit Class-specific Conformal Prediction (BCCP), offering coverage\nguarantees on a class-specific granularity. Using an unbiased estimation of an\nestimand involving the true label, BCCP trains the model and makes set-valued\ninferences through stochastic gradient descent. Our approach overcomes the\nchallenges of sparsely labeled data in each iteration and generalizes the\nreliability and applicability of conformal prediction to online decision-making\nenvironments.\n", "link": "http://arxiv.org/abs/2405.04393v1", "date": "2024-05-07", "relevancy": 2.0192, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5328}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5043}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Online%20Set-valued%20Classification%20with%20Bandit%20Feedback&body=Title%3A%20Efficient%20Online%20Set-valued%20Classification%20with%20Bandit%20Feedback%0AAuthor%3A%20Zhou%20Wang%20and%20Xingye%20Qiao%0AAbstract%3A%20%20%20Conformal%20prediction%20is%20a%20distribution-free%20method%20that%20wraps%20a%20given%20machine%0Alearning%20model%20and%20returns%20a%20set%20of%20plausible%20labels%20that%20contain%20the%20true%0Alabel%20with%20a%20prescribed%20coverage%20rate.%20In%20practice%2C%20the%20empirical%20coverage%0Aachieved%20highly%20relies%20on%20fully%20observed%20label%20information%20from%20data%20both%20in%0Athe%20training%20phase%20for%20model%20fitting%20and%20the%20calibration%20phase%20for%20quantile%0Aestimation.%20This%20dependency%20poses%20a%20challenge%20in%20the%20context%20of%20online%20learning%0Awith%20bandit%20feedback%2C%20where%20a%20learner%20only%20has%20access%20to%20the%20correctness%20of%0Aactions%20%28i.e.%2C%20pulled%20an%20arm%29%20but%20not%20the%20full%20information%20of%20the%20true%20label.%0AIn%20particular%2C%20when%20the%20pulled%20arm%20is%20incorrect%2C%20the%20learner%20only%20knows%20that%0Athe%20pulled%20one%20is%20not%20the%20true%20class%20label%2C%20but%20does%20not%20know%20which%20label%20is%0Atrue.%20Additionally%2C%20bandit%20feedback%20further%20results%20in%20a%20smaller%20labeled%0Adataset%20for%20calibration%2C%20limited%20to%20instances%20with%20correct%20actions%2C%20thereby%0Aaffecting%20the%20accuracy%20of%20quantile%20estimation.%20To%20address%20these%20limitations%2C%20we%0Apropose%20Bandit%20Class-specific%20Conformal%20Prediction%20%28BCCP%29%2C%20offering%20coverage%0Aguarantees%20on%20a%20class-specific%20granularity.%20Using%20an%20unbiased%20estimation%20of%20an%0Aestimand%20involving%20the%20true%20label%2C%20BCCP%20trains%20the%20model%20and%20makes%20set-valued%0Ainferences%20through%20stochastic%20gradient%20descent.%20Our%20approach%20overcomes%20the%0Achallenges%20of%20sparsely%20labeled%20data%20in%20each%20iteration%20and%20generalizes%20the%0Areliability%20and%20applicability%20of%20conformal%20prediction%20to%20online%20decision-making%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04393v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Online%2520Set-valued%2520Classification%2520with%2520Bandit%2520Feedback%26entry.906535625%3DZhou%2520Wang%2520and%2520Xingye%2520Qiao%26entry.1292438233%3D%2520%2520Conformal%2520prediction%2520is%2520a%2520distribution-free%2520method%2520that%2520wraps%2520a%2520given%2520machine%250Alearning%2520model%2520and%2520returns%2520a%2520set%2520of%2520plausible%2520labels%2520that%2520contain%2520the%2520true%250Alabel%2520with%2520a%2520prescribed%2520coverage%2520rate.%2520In%2520practice%252C%2520the%2520empirical%2520coverage%250Aachieved%2520highly%2520relies%2520on%2520fully%2520observed%2520label%2520information%2520from%2520data%2520both%2520in%250Athe%2520training%2520phase%2520for%2520model%2520fitting%2520and%2520the%2520calibration%2520phase%2520for%2520quantile%250Aestimation.%2520This%2520dependency%2520poses%2520a%2520challenge%2520in%2520the%2520context%2520of%2520online%2520learning%250Awith%2520bandit%2520feedback%252C%2520where%2520a%2520learner%2520only%2520has%2520access%2520to%2520the%2520correctness%2520of%250Aactions%2520%2528i.e.%252C%2520pulled%2520an%2520arm%2529%2520but%2520not%2520the%2520full%2520information%2520of%2520the%2520true%2520label.%250AIn%2520particular%252C%2520when%2520the%2520pulled%2520arm%2520is%2520incorrect%252C%2520the%2520learner%2520only%2520knows%2520that%250Athe%2520pulled%2520one%2520is%2520not%2520the%2520true%2520class%2520label%252C%2520but%2520does%2520not%2520know%2520which%2520label%2520is%250Atrue.%2520Additionally%252C%2520bandit%2520feedback%2520further%2520results%2520in%2520a%2520smaller%2520labeled%250Adataset%2520for%2520calibration%252C%2520limited%2520to%2520instances%2520with%2520correct%2520actions%252C%2520thereby%250Aaffecting%2520the%2520accuracy%2520of%2520quantile%2520estimation.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520Bandit%2520Class-specific%2520Conformal%2520Prediction%2520%2528BCCP%2529%252C%2520offering%2520coverage%250Aguarantees%2520on%2520a%2520class-specific%2520granularity.%2520Using%2520an%2520unbiased%2520estimation%2520of%2520an%250Aestimand%2520involving%2520the%2520true%2520label%252C%2520BCCP%2520trains%2520the%2520model%2520and%2520makes%2520set-valued%250Ainferences%2520through%2520stochastic%2520gradient%2520descent.%2520Our%2520approach%2520overcomes%2520the%250Achallenges%2520of%2520sparsely%2520labeled%2520data%2520in%2520each%2520iteration%2520and%2520generalizes%2520the%250Areliability%2520and%2520applicability%2520of%2520conformal%2520prediction%2520to%2520online%2520decision-making%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04393v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Online%20Set-valued%20Classification%20with%20Bandit%20Feedback&entry.906535625=Zhou%20Wang%20and%20Xingye%20Qiao&entry.1292438233=%20%20Conformal%20prediction%20is%20a%20distribution-free%20method%20that%20wraps%20a%20given%20machine%0Alearning%20model%20and%20returns%20a%20set%20of%20plausible%20labels%20that%20contain%20the%20true%0Alabel%20with%20a%20prescribed%20coverage%20rate.%20In%20practice%2C%20the%20empirical%20coverage%0Aachieved%20highly%20relies%20on%20fully%20observed%20label%20information%20from%20data%20both%20in%0Athe%20training%20phase%20for%20model%20fitting%20and%20the%20calibration%20phase%20for%20quantile%0Aestimation.%20This%20dependency%20poses%20a%20challenge%20in%20the%20context%20of%20online%20learning%0Awith%20bandit%20feedback%2C%20where%20a%20learner%20only%20has%20access%20to%20the%20correctness%20of%0Aactions%20%28i.e.%2C%20pulled%20an%20arm%29%20but%20not%20the%20full%20information%20of%20the%20true%20label.%0AIn%20particular%2C%20when%20the%20pulled%20arm%20is%20incorrect%2C%20the%20learner%20only%20knows%20that%0Athe%20pulled%20one%20is%20not%20the%20true%20class%20label%2C%20but%20does%20not%20know%20which%20label%20is%0Atrue.%20Additionally%2C%20bandit%20feedback%20further%20results%20in%20a%20smaller%20labeled%0Adataset%20for%20calibration%2C%20limited%20to%20instances%20with%20correct%20actions%2C%20thereby%0Aaffecting%20the%20accuracy%20of%20quantile%20estimation.%20To%20address%20these%20limitations%2C%20we%0Apropose%20Bandit%20Class-specific%20Conformal%20Prediction%20%28BCCP%29%2C%20offering%20coverage%0Aguarantees%20on%20a%20class-specific%20granularity.%20Using%20an%20unbiased%20estimation%20of%20an%0Aestimand%20involving%20the%20true%20label%2C%20BCCP%20trains%20the%20model%20and%20makes%20set-valued%0Ainferences%20through%20stochastic%20gradient%20descent.%20Our%20approach%20overcomes%20the%0Achallenges%20of%20sparsely%20labeled%20data%20in%20each%20iteration%20and%20generalizes%20the%0Areliability%20and%20applicability%20of%20conformal%20prediction%20to%20online%20decision-making%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04393v1&entry.124074799=Read"},
{"title": "A Novel Approach to Chest X-ray Lung Segmentation Using U-net and\n  Modified Convolutional Block Attention Module", "author": "Mohammad Ali Labbaf Khaniki and Mohammad Manthouri", "abstract": "  Lung segmentation in chest X-ray images is of paramount importance as it\nplays a crucial role in the diagnosis and treatment of various lung diseases.\nThis paper presents a novel approach for lung segmentation in chest X-ray\nimages by integrating U-net with attention mechanisms. The proposed method\nenhances the U-net architecture by incorporating a Convolutional Block\nAttention Module (CBAM), which unifies three distinct attention mechanisms:\nchannel attention, spatial attention, and pixel attention. The channel\nattention mechanism enables the model to concentrate on the most informative\nfeatures across various channels. The spatial attention mechanism enhances the\nmodel's precision in localization by focusing on significant spatial locations.\nLastly, the pixel attention mechanism empowers the model to focus on individual\npixels, further refining the model's focus and thereby improving the accuracy\nof segmentation. The adoption of the proposed CBAM in conjunction with the\nU-net architecture marks a significant advancement in the field of medical\nimaging, with potential implications for improving diagnostic precision and\npatient outcomes. The efficacy of this method is validated against contemporary\nstate-of-the-art techniques, showcasing its superiority in segmentation\nperformance.\n", "link": "http://arxiv.org/abs/2404.14322v2", "date": "2024-05-07", "relevancy": 2.0109, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5175}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4941}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Approach%20to%20Chest%20X-ray%20Lung%20Segmentation%20Using%20U-net%20and%0A%20%20Modified%20Convolutional%20Block%20Attention%20Module&body=Title%3A%20A%20Novel%20Approach%20to%20Chest%20X-ray%20Lung%20Segmentation%20Using%20U-net%20and%0A%20%20Modified%20Convolutional%20Block%20Attention%20Module%0AAuthor%3A%20Mohammad%20Ali%20Labbaf%20Khaniki%20and%20Mohammad%20Manthouri%0AAbstract%3A%20%20%20Lung%20segmentation%20in%20chest%20X-ray%20images%20is%20of%20paramount%20importance%20as%20it%0Aplays%20a%20crucial%20role%20in%20the%20diagnosis%20and%20treatment%20of%20various%20lung%20diseases.%0AThis%20paper%20presents%20a%20novel%20approach%20for%20lung%20segmentation%20in%20chest%20X-ray%0Aimages%20by%20integrating%20U-net%20with%20attention%20mechanisms.%20The%20proposed%20method%0Aenhances%20the%20U-net%20architecture%20by%20incorporating%20a%20Convolutional%20Block%0AAttention%20Module%20%28CBAM%29%2C%20which%20unifies%20three%20distinct%20attention%20mechanisms%3A%0Achannel%20attention%2C%20spatial%20attention%2C%20and%20pixel%20attention.%20The%20channel%0Aattention%20mechanism%20enables%20the%20model%20to%20concentrate%20on%20the%20most%20informative%0Afeatures%20across%20various%20channels.%20The%20spatial%20attention%20mechanism%20enhances%20the%0Amodel%27s%20precision%20in%20localization%20by%20focusing%20on%20significant%20spatial%20locations.%0ALastly%2C%20the%20pixel%20attention%20mechanism%20empowers%20the%20model%20to%20focus%20on%20individual%0Apixels%2C%20further%20refining%20the%20model%27s%20focus%20and%20thereby%20improving%20the%20accuracy%0Aof%20segmentation.%20The%20adoption%20of%20the%20proposed%20CBAM%20in%20conjunction%20with%20the%0AU-net%20architecture%20marks%20a%20significant%20advancement%20in%20the%20field%20of%20medical%0Aimaging%2C%20with%20potential%20implications%20for%20improving%20diagnostic%20precision%20and%0Apatient%20outcomes.%20The%20efficacy%20of%20this%20method%20is%20validated%20against%20contemporary%0Astate-of-the-art%20techniques%2C%20showcasing%20its%20superiority%20in%20segmentation%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14322v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Approach%2520to%2520Chest%2520X-ray%2520Lung%2520Segmentation%2520Using%2520U-net%2520and%250A%2520%2520Modified%2520Convolutional%2520Block%2520Attention%2520Module%26entry.906535625%3DMohammad%2520Ali%2520Labbaf%2520Khaniki%2520and%2520Mohammad%2520Manthouri%26entry.1292438233%3D%2520%2520Lung%2520segmentation%2520in%2520chest%2520X-ray%2520images%2520is%2520of%2520paramount%2520importance%2520as%2520it%250Aplays%2520a%2520crucial%2520role%2520in%2520the%2520diagnosis%2520and%2520treatment%2520of%2520various%2520lung%2520diseases.%250AThis%2520paper%2520presents%2520a%2520novel%2520approach%2520for%2520lung%2520segmentation%2520in%2520chest%2520X-ray%250Aimages%2520by%2520integrating%2520U-net%2520with%2520attention%2520mechanisms.%2520The%2520proposed%2520method%250Aenhances%2520the%2520U-net%2520architecture%2520by%2520incorporating%2520a%2520Convolutional%2520Block%250AAttention%2520Module%2520%2528CBAM%2529%252C%2520which%2520unifies%2520three%2520distinct%2520attention%2520mechanisms%253A%250Achannel%2520attention%252C%2520spatial%2520attention%252C%2520and%2520pixel%2520attention.%2520The%2520channel%250Aattention%2520mechanism%2520enables%2520the%2520model%2520to%2520concentrate%2520on%2520the%2520most%2520informative%250Afeatures%2520across%2520various%2520channels.%2520The%2520spatial%2520attention%2520mechanism%2520enhances%2520the%250Amodel%2527s%2520precision%2520in%2520localization%2520by%2520focusing%2520on%2520significant%2520spatial%2520locations.%250ALastly%252C%2520the%2520pixel%2520attention%2520mechanism%2520empowers%2520the%2520model%2520to%2520focus%2520on%2520individual%250Apixels%252C%2520further%2520refining%2520the%2520model%2527s%2520focus%2520and%2520thereby%2520improving%2520the%2520accuracy%250Aof%2520segmentation.%2520The%2520adoption%2520of%2520the%2520proposed%2520CBAM%2520in%2520conjunction%2520with%2520the%250AU-net%2520architecture%2520marks%2520a%2520significant%2520advancement%2520in%2520the%2520field%2520of%2520medical%250Aimaging%252C%2520with%2520potential%2520implications%2520for%2520improving%2520diagnostic%2520precision%2520and%250Apatient%2520outcomes.%2520The%2520efficacy%2520of%2520this%2520method%2520is%2520validated%2520against%2520contemporary%250Astate-of-the-art%2520techniques%252C%2520showcasing%2520its%2520superiority%2520in%2520segmentation%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14322v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Approach%20to%20Chest%20X-ray%20Lung%20Segmentation%20Using%20U-net%20and%0A%20%20Modified%20Convolutional%20Block%20Attention%20Module&entry.906535625=Mohammad%20Ali%20Labbaf%20Khaniki%20and%20Mohammad%20Manthouri&entry.1292438233=%20%20Lung%20segmentation%20in%20chest%20X-ray%20images%20is%20of%20paramount%20importance%20as%20it%0Aplays%20a%20crucial%20role%20in%20the%20diagnosis%20and%20treatment%20of%20various%20lung%20diseases.%0AThis%20paper%20presents%20a%20novel%20approach%20for%20lung%20segmentation%20in%20chest%20X-ray%0Aimages%20by%20integrating%20U-net%20with%20attention%20mechanisms.%20The%20proposed%20method%0Aenhances%20the%20U-net%20architecture%20by%20incorporating%20a%20Convolutional%20Block%0AAttention%20Module%20%28CBAM%29%2C%20which%20unifies%20three%20distinct%20attention%20mechanisms%3A%0Achannel%20attention%2C%20spatial%20attention%2C%20and%20pixel%20attention.%20The%20channel%0Aattention%20mechanism%20enables%20the%20model%20to%20concentrate%20on%20the%20most%20informative%0Afeatures%20across%20various%20channels.%20The%20spatial%20attention%20mechanism%20enhances%20the%0Amodel%27s%20precision%20in%20localization%20by%20focusing%20on%20significant%20spatial%20locations.%0ALastly%2C%20the%20pixel%20attention%20mechanism%20empowers%20the%20model%20to%20focus%20on%20individual%0Apixels%2C%20further%20refining%20the%20model%27s%20focus%20and%20thereby%20improving%20the%20accuracy%0Aof%20segmentation.%20The%20adoption%20of%20the%20proposed%20CBAM%20in%20conjunction%20with%20the%0AU-net%20architecture%20marks%20a%20significant%20advancement%20in%20the%20field%20of%20medical%0Aimaging%2C%20with%20potential%20implications%20for%20improving%20diagnostic%20precision%20and%0Apatient%20outcomes.%20The%20efficacy%20of%20this%20method%20is%20validated%20against%20contemporary%0Astate-of-the-art%20techniques%2C%20showcasing%20its%20superiority%20in%20segmentation%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14322v2&entry.124074799=Read"},
{"title": "Temporal and Heterogeneous Graph Neural Network for Remaining Useful\n  Life Prediction", "author": "Zhihao Wen and Yuan Fang and Pengcheng Wei and Fayao Liu and Zhenghua Chen and Min Wu", "abstract": "  Predicting Remaining Useful Life (RUL) plays a crucial role in the\nprognostics and health management of industrial systems that involve a variety\nof interrelated sensors. Given a constant stream of time series sensory data\nfrom such systems, deep learning models have risen to prominence at identifying\ncomplex, nonlinear temporal dependencies in these data. In addition to the\ntemporal dependencies of individual sensors, spatial dependencies emerge as\nimportant correlations among these sensors, which can be naturally modelled by\na temporal graph that describes time-varying spatial relationships. However,\nthe majority of existing studies have relied on capturing discrete snapshots of\nthis temporal graph, a coarse-grained approach that leads to loss of temporal\ninformation. Moreover, given the variety of heterogeneous sensors, it becomes\nvital that such inherent heterogeneity is leveraged for RUL prediction in\ntemporal sensor graphs. To capture the nuances of the temporal and spatial\nrelationships and heterogeneous characteristics in an interconnected graph of\nsensors, we introduce a novel model named Temporal and Heterogeneous Graph\nNeural Networks (THGNN). Specifically, THGNN aggregates historical data from\nneighboring nodes to accurately capture the temporal dynamics and spatial\ncorrelations within the stream of sensor data in a fine-grained manner.\nMoreover, the model leverages Feature-wise Linear Modulation (FiLM) to address\nthe diversity of sensor types, significantly improving the model's capacity to\nlearn the heterogeneity in the data sources. Finally, we have validated the\neffectiveness of our approach through comprehensive experiments. Our empirical\nfindings demonstrate significant advancements on the N-CMAPSS dataset,\nachieving improvements of up to 19.2% and 31.6% in terms of two different\nevaluation metrics over state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2405.04336v1", "date": "2024-05-07", "relevancy": 2.0029, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5088}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5036}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20and%20Heterogeneous%20Graph%20Neural%20Network%20for%20Remaining%20Useful%0A%20%20Life%20Prediction&body=Title%3A%20Temporal%20and%20Heterogeneous%20Graph%20Neural%20Network%20for%20Remaining%20Useful%0A%20%20Life%20Prediction%0AAuthor%3A%20Zhihao%20Wen%20and%20Yuan%20Fang%20and%20Pengcheng%20Wei%20and%20Fayao%20Liu%20and%20Zhenghua%20Chen%20and%20Min%20Wu%0AAbstract%3A%20%20%20Predicting%20Remaining%20Useful%20Life%20%28RUL%29%20plays%20a%20crucial%20role%20in%20the%0Aprognostics%20and%20health%20management%20of%20industrial%20systems%20that%20involve%20a%20variety%0Aof%20interrelated%20sensors.%20Given%20a%20constant%20stream%20of%20time%20series%20sensory%20data%0Afrom%20such%20systems%2C%20deep%20learning%20models%20have%20risen%20to%20prominence%20at%20identifying%0Acomplex%2C%20nonlinear%20temporal%20dependencies%20in%20these%20data.%20In%20addition%20to%20the%0Atemporal%20dependencies%20of%20individual%20sensors%2C%20spatial%20dependencies%20emerge%20as%0Aimportant%20correlations%20among%20these%20sensors%2C%20which%20can%20be%20naturally%20modelled%20by%0Aa%20temporal%20graph%20that%20describes%20time-varying%20spatial%20relationships.%20However%2C%0Athe%20majority%20of%20existing%20studies%20have%20relied%20on%20capturing%20discrete%20snapshots%20of%0Athis%20temporal%20graph%2C%20a%20coarse-grained%20approach%20that%20leads%20to%20loss%20of%20temporal%0Ainformation.%20Moreover%2C%20given%20the%20variety%20of%20heterogeneous%20sensors%2C%20it%20becomes%0Avital%20that%20such%20inherent%20heterogeneity%20is%20leveraged%20for%20RUL%20prediction%20in%0Atemporal%20sensor%20graphs.%20To%20capture%20the%20nuances%20of%20the%20temporal%20and%20spatial%0Arelationships%20and%20heterogeneous%20characteristics%20in%20an%20interconnected%20graph%20of%0Asensors%2C%20we%20introduce%20a%20novel%20model%20named%20Temporal%20and%20Heterogeneous%20Graph%0ANeural%20Networks%20%28THGNN%29.%20Specifically%2C%20THGNN%20aggregates%20historical%20data%20from%0Aneighboring%20nodes%20to%20accurately%20capture%20the%20temporal%20dynamics%20and%20spatial%0Acorrelations%20within%20the%20stream%20of%20sensor%20data%20in%20a%20fine-grained%20manner.%0AMoreover%2C%20the%20model%20leverages%20Feature-wise%20Linear%20Modulation%20%28FiLM%29%20to%20address%0Athe%20diversity%20of%20sensor%20types%2C%20significantly%20improving%20the%20model%27s%20capacity%20to%0Alearn%20the%20heterogeneity%20in%20the%20data%20sources.%20Finally%2C%20we%20have%20validated%20the%0Aeffectiveness%20of%20our%20approach%20through%20comprehensive%20experiments.%20Our%20empirical%0Afindings%20demonstrate%20significant%20advancements%20on%20the%20N-CMAPSS%20dataset%2C%0Aachieving%20improvements%20of%20up%20to%2019.2%25%20and%2031.6%25%20in%20terms%20of%20two%20different%0Aevaluation%20metrics%20over%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520and%2520Heterogeneous%2520Graph%2520Neural%2520Network%2520for%2520Remaining%2520Useful%250A%2520%2520Life%2520Prediction%26entry.906535625%3DZhihao%2520Wen%2520and%2520Yuan%2520Fang%2520and%2520Pengcheng%2520Wei%2520and%2520Fayao%2520Liu%2520and%2520Zhenghua%2520Chen%2520and%2520Min%2520Wu%26entry.1292438233%3D%2520%2520Predicting%2520Remaining%2520Useful%2520Life%2520%2528RUL%2529%2520plays%2520a%2520crucial%2520role%2520in%2520the%250Aprognostics%2520and%2520health%2520management%2520of%2520industrial%2520systems%2520that%2520involve%2520a%2520variety%250Aof%2520interrelated%2520sensors.%2520Given%2520a%2520constant%2520stream%2520of%2520time%2520series%2520sensory%2520data%250Afrom%2520such%2520systems%252C%2520deep%2520learning%2520models%2520have%2520risen%2520to%2520prominence%2520at%2520identifying%250Acomplex%252C%2520nonlinear%2520temporal%2520dependencies%2520in%2520these%2520data.%2520In%2520addition%2520to%2520the%250Atemporal%2520dependencies%2520of%2520individual%2520sensors%252C%2520spatial%2520dependencies%2520emerge%2520as%250Aimportant%2520correlations%2520among%2520these%2520sensors%252C%2520which%2520can%2520be%2520naturally%2520modelled%2520by%250Aa%2520temporal%2520graph%2520that%2520describes%2520time-varying%2520spatial%2520relationships.%2520However%252C%250Athe%2520majority%2520of%2520existing%2520studies%2520have%2520relied%2520on%2520capturing%2520discrete%2520snapshots%2520of%250Athis%2520temporal%2520graph%252C%2520a%2520coarse-grained%2520approach%2520that%2520leads%2520to%2520loss%2520of%2520temporal%250Ainformation.%2520Moreover%252C%2520given%2520the%2520variety%2520of%2520heterogeneous%2520sensors%252C%2520it%2520becomes%250Avital%2520that%2520such%2520inherent%2520heterogeneity%2520is%2520leveraged%2520for%2520RUL%2520prediction%2520in%250Atemporal%2520sensor%2520graphs.%2520To%2520capture%2520the%2520nuances%2520of%2520the%2520temporal%2520and%2520spatial%250Arelationships%2520and%2520heterogeneous%2520characteristics%2520in%2520an%2520interconnected%2520graph%2520of%250Asensors%252C%2520we%2520introduce%2520a%2520novel%2520model%2520named%2520Temporal%2520and%2520Heterogeneous%2520Graph%250ANeural%2520Networks%2520%2528THGNN%2529.%2520Specifically%252C%2520THGNN%2520aggregates%2520historical%2520data%2520from%250Aneighboring%2520nodes%2520to%2520accurately%2520capture%2520the%2520temporal%2520dynamics%2520and%2520spatial%250Acorrelations%2520within%2520the%2520stream%2520of%2520sensor%2520data%2520in%2520a%2520fine-grained%2520manner.%250AMoreover%252C%2520the%2520model%2520leverages%2520Feature-wise%2520Linear%2520Modulation%2520%2528FiLM%2529%2520to%2520address%250Athe%2520diversity%2520of%2520sensor%2520types%252C%2520significantly%2520improving%2520the%2520model%2527s%2520capacity%2520to%250Alearn%2520the%2520heterogeneity%2520in%2520the%2520data%2520sources.%2520Finally%252C%2520we%2520have%2520validated%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520through%2520comprehensive%2520experiments.%2520Our%2520empirical%250Afindings%2520demonstrate%2520significant%2520advancements%2520on%2520the%2520N-CMAPSS%2520dataset%252C%250Aachieving%2520improvements%2520of%2520up%2520to%252019.2%2525%2520and%252031.6%2525%2520in%2520terms%2520of%2520two%2520different%250Aevaluation%2520metrics%2520over%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20and%20Heterogeneous%20Graph%20Neural%20Network%20for%20Remaining%20Useful%0A%20%20Life%20Prediction&entry.906535625=Zhihao%20Wen%20and%20Yuan%20Fang%20and%20Pengcheng%20Wei%20and%20Fayao%20Liu%20and%20Zhenghua%20Chen%20and%20Min%20Wu&entry.1292438233=%20%20Predicting%20Remaining%20Useful%20Life%20%28RUL%29%20plays%20a%20crucial%20role%20in%20the%0Aprognostics%20and%20health%20management%20of%20industrial%20systems%20that%20involve%20a%20variety%0Aof%20interrelated%20sensors.%20Given%20a%20constant%20stream%20of%20time%20series%20sensory%20data%0Afrom%20such%20systems%2C%20deep%20learning%20models%20have%20risen%20to%20prominence%20at%20identifying%0Acomplex%2C%20nonlinear%20temporal%20dependencies%20in%20these%20data.%20In%20addition%20to%20the%0Atemporal%20dependencies%20of%20individual%20sensors%2C%20spatial%20dependencies%20emerge%20as%0Aimportant%20correlations%20among%20these%20sensors%2C%20which%20can%20be%20naturally%20modelled%20by%0Aa%20temporal%20graph%20that%20describes%20time-varying%20spatial%20relationships.%20However%2C%0Athe%20majority%20of%20existing%20studies%20have%20relied%20on%20capturing%20discrete%20snapshots%20of%0Athis%20temporal%20graph%2C%20a%20coarse-grained%20approach%20that%20leads%20to%20loss%20of%20temporal%0Ainformation.%20Moreover%2C%20given%20the%20variety%20of%20heterogeneous%20sensors%2C%20it%20becomes%0Avital%20that%20such%20inherent%20heterogeneity%20is%20leveraged%20for%20RUL%20prediction%20in%0Atemporal%20sensor%20graphs.%20To%20capture%20the%20nuances%20of%20the%20temporal%20and%20spatial%0Arelationships%20and%20heterogeneous%20characteristics%20in%20an%20interconnected%20graph%20of%0Asensors%2C%20we%20introduce%20a%20novel%20model%20named%20Temporal%20and%20Heterogeneous%20Graph%0ANeural%20Networks%20%28THGNN%29.%20Specifically%2C%20THGNN%20aggregates%20historical%20data%20from%0Aneighboring%20nodes%20to%20accurately%20capture%20the%20temporal%20dynamics%20and%20spatial%0Acorrelations%20within%20the%20stream%20of%20sensor%20data%20in%20a%20fine-grained%20manner.%0AMoreover%2C%20the%20model%20leverages%20Feature-wise%20Linear%20Modulation%20%28FiLM%29%20to%20address%0Athe%20diversity%20of%20sensor%20types%2C%20significantly%20improving%20the%20model%27s%20capacity%20to%0Alearn%20the%20heterogeneity%20in%20the%20data%20sources.%20Finally%2C%20we%20have%20validated%20the%0Aeffectiveness%20of%20our%20approach%20through%20comprehensive%20experiments.%20Our%20empirical%0Afindings%20demonstrate%20significant%20advancements%20on%20the%20N-CMAPSS%20dataset%2C%0Aachieving%20improvements%20of%20up%20to%2019.2%25%20and%2031.6%25%20in%20terms%20of%20two%20different%0Aevaluation%20metrics%20over%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04336v1&entry.124074799=Read"},
{"title": "Towards Generalizing to Unseen Domains with Few Labels", "author": "Chamuditha Jayanga Galappaththige and Sanoojan Baliah and Malitha Gunawardhana and Muhammad Haris Khan", "abstract": "  We approach the challenge of addressing semi-supervised domain generalization\n(SSDG). Specifically, our aim is to obtain a model that learns\ndomain-generalizable features by leveraging a limited subset of labelled data\nalongside a substantially larger pool of unlabeled data. Existing domain\ngeneralization (DG) methods which are unable to exploit unlabeled data perform\npoorly compared to semi-supervised learning (SSL) methods under SSDG setting.\nNevertheless, SSL methods have considerable room for performance improvement\nwhen compared to fully-supervised DG training. To tackle this underexplored,\nyet highly practical problem of SSDG, we make the following core contributions.\nFirst, we propose a feature-based conformity technique that matches the\nposterior distributions from the feature space with the pseudo-label from the\nmodel's output space. Second, we develop a semantics alignment loss to learn\nsemantically-compatible representations by regularizing the semantic structure\nin the feature space. Our method is plug-and-play and can be readily integrated\nwith different SSL-based SSDG baselines without introducing any additional\nparameters. Extensive experimental results across five challenging DG\nbenchmarks with four strong SSL baselines suggest that our method provides\nconsistent and notable gains in two different SSDG settings.\n", "link": "http://arxiv.org/abs/2403.11674v3", "date": "2024-05-07", "relevancy": 1.9906, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5245}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4897}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Generalizing%20to%20Unseen%20Domains%20with%20Few%20Labels&body=Title%3A%20Towards%20Generalizing%20to%20Unseen%20Domains%20with%20Few%20Labels%0AAuthor%3A%20Chamuditha%20Jayanga%20Galappaththige%20and%20Sanoojan%20Baliah%20and%20Malitha%20Gunawardhana%20and%20Muhammad%20Haris%20Khan%0AAbstract%3A%20%20%20We%20approach%20the%20challenge%20of%20addressing%20semi-supervised%20domain%20generalization%0A%28SSDG%29.%20Specifically%2C%20our%20aim%20is%20to%20obtain%20a%20model%20that%20learns%0Adomain-generalizable%20features%20by%20leveraging%20a%20limited%20subset%20of%20labelled%20data%0Aalongside%20a%20substantially%20larger%20pool%20of%20unlabeled%20data.%20Existing%20domain%0Ageneralization%20%28DG%29%20methods%20which%20are%20unable%20to%20exploit%20unlabeled%20data%20perform%0Apoorly%20compared%20to%20semi-supervised%20learning%20%28SSL%29%20methods%20under%20SSDG%20setting.%0ANevertheless%2C%20SSL%20methods%20have%20considerable%20room%20for%20performance%20improvement%0Awhen%20compared%20to%20fully-supervised%20DG%20training.%20To%20tackle%20this%20underexplored%2C%0Ayet%20highly%20practical%20problem%20of%20SSDG%2C%20we%20make%20the%20following%20core%20contributions.%0AFirst%2C%20we%20propose%20a%20feature-based%20conformity%20technique%20that%20matches%20the%0Aposterior%20distributions%20from%20the%20feature%20space%20with%20the%20pseudo-label%20from%20the%0Amodel%27s%20output%20space.%20Second%2C%20we%20develop%20a%20semantics%20alignment%20loss%20to%20learn%0Asemantically-compatible%20representations%20by%20regularizing%20the%20semantic%20structure%0Ain%20the%20feature%20space.%20Our%20method%20is%20plug-and-play%20and%20can%20be%20readily%20integrated%0Awith%20different%20SSL-based%20SSDG%20baselines%20without%20introducing%20any%20additional%0Aparameters.%20Extensive%20experimental%20results%20across%20five%20challenging%20DG%0Abenchmarks%20with%20four%20strong%20SSL%20baselines%20suggest%20that%20our%20method%20provides%0Aconsistent%20and%20notable%20gains%20in%20two%20different%20SSDG%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11674v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Generalizing%2520to%2520Unseen%2520Domains%2520with%2520Few%2520Labels%26entry.906535625%3DChamuditha%2520Jayanga%2520Galappaththige%2520and%2520Sanoojan%2520Baliah%2520and%2520Malitha%2520Gunawardhana%2520and%2520Muhammad%2520Haris%2520Khan%26entry.1292438233%3D%2520%2520We%2520approach%2520the%2520challenge%2520of%2520addressing%2520semi-supervised%2520domain%2520generalization%250A%2528SSDG%2529.%2520Specifically%252C%2520our%2520aim%2520is%2520to%2520obtain%2520a%2520model%2520that%2520learns%250Adomain-generalizable%2520features%2520by%2520leveraging%2520a%2520limited%2520subset%2520of%2520labelled%2520data%250Aalongside%2520a%2520substantially%2520larger%2520pool%2520of%2520unlabeled%2520data.%2520Existing%2520domain%250Ageneralization%2520%2528DG%2529%2520methods%2520which%2520are%2520unable%2520to%2520exploit%2520unlabeled%2520data%2520perform%250Apoorly%2520compared%2520to%2520semi-supervised%2520learning%2520%2528SSL%2529%2520methods%2520under%2520SSDG%2520setting.%250ANevertheless%252C%2520SSL%2520methods%2520have%2520considerable%2520room%2520for%2520performance%2520improvement%250Awhen%2520compared%2520to%2520fully-supervised%2520DG%2520training.%2520To%2520tackle%2520this%2520underexplored%252C%250Ayet%2520highly%2520practical%2520problem%2520of%2520SSDG%252C%2520we%2520make%2520the%2520following%2520core%2520contributions.%250AFirst%252C%2520we%2520propose%2520a%2520feature-based%2520conformity%2520technique%2520that%2520matches%2520the%250Aposterior%2520distributions%2520from%2520the%2520feature%2520space%2520with%2520the%2520pseudo-label%2520from%2520the%250Amodel%2527s%2520output%2520space.%2520Second%252C%2520we%2520develop%2520a%2520semantics%2520alignment%2520loss%2520to%2520learn%250Asemantically-compatible%2520representations%2520by%2520regularizing%2520the%2520semantic%2520structure%250Ain%2520the%2520feature%2520space.%2520Our%2520method%2520is%2520plug-and-play%2520and%2520can%2520be%2520readily%2520integrated%250Awith%2520different%2520SSL-based%2520SSDG%2520baselines%2520without%2520introducing%2520any%2520additional%250Aparameters.%2520Extensive%2520experimental%2520results%2520across%2520five%2520challenging%2520DG%250Abenchmarks%2520with%2520four%2520strong%2520SSL%2520baselines%2520suggest%2520that%2520our%2520method%2520provides%250Aconsistent%2520and%2520notable%2520gains%2520in%2520two%2520different%2520SSDG%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11674v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Generalizing%20to%20Unseen%20Domains%20with%20Few%20Labels&entry.906535625=Chamuditha%20Jayanga%20Galappaththige%20and%20Sanoojan%20Baliah%20and%20Malitha%20Gunawardhana%20and%20Muhammad%20Haris%20Khan&entry.1292438233=%20%20We%20approach%20the%20challenge%20of%20addressing%20semi-supervised%20domain%20generalization%0A%28SSDG%29.%20Specifically%2C%20our%20aim%20is%20to%20obtain%20a%20model%20that%20learns%0Adomain-generalizable%20features%20by%20leveraging%20a%20limited%20subset%20of%20labelled%20data%0Aalongside%20a%20substantially%20larger%20pool%20of%20unlabeled%20data.%20Existing%20domain%0Ageneralization%20%28DG%29%20methods%20which%20are%20unable%20to%20exploit%20unlabeled%20data%20perform%0Apoorly%20compared%20to%20semi-supervised%20learning%20%28SSL%29%20methods%20under%20SSDG%20setting.%0ANevertheless%2C%20SSL%20methods%20have%20considerable%20room%20for%20performance%20improvement%0Awhen%20compared%20to%20fully-supervised%20DG%20training.%20To%20tackle%20this%20underexplored%2C%0Ayet%20highly%20practical%20problem%20of%20SSDG%2C%20we%20make%20the%20following%20core%20contributions.%0AFirst%2C%20we%20propose%20a%20feature-based%20conformity%20technique%20that%20matches%20the%0Aposterior%20distributions%20from%20the%20feature%20space%20with%20the%20pseudo-label%20from%20the%0Amodel%27s%20output%20space.%20Second%2C%20we%20develop%20a%20semantics%20alignment%20loss%20to%20learn%0Asemantically-compatible%20representations%20by%20regularizing%20the%20semantic%20structure%0Ain%20the%20feature%20space.%20Our%20method%20is%20plug-and-play%20and%20can%20be%20readily%20integrated%0Awith%20different%20SSL-based%20SSDG%20baselines%20without%20introducing%20any%20additional%0Aparameters.%20Extensive%20experimental%20results%20across%20five%20challenging%20DG%0Abenchmarks%20with%20four%20strong%20SSL%20baselines%20suggest%20that%20our%20method%20provides%0Aconsistent%20and%20notable%20gains%20in%20two%20different%20SSDG%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11674v3&entry.124074799=Read"},
{"title": "Accelerating Convergence in Bayesian Few-Shot Classification", "author": "Tianjun Ke and Haoqun Cao and Feng Zhou", "abstract": "  Bayesian few-shot classification has been a focal point in the field of\nfew-shot learning. This paper seamlessly integrates mirror descent-based\nvariational inference into Gaussian process-based few-shot classification,\naddressing the challenge of non-conjugate inference. By leveraging\nnon-Euclidean geometry, mirror descent achieves accelerated convergence by\nproviding the steepest descent direction along the corresponding manifold. It\nalso exhibits the parameterization invariance property concerning the\nvariational distribution. Experimental results demonstrate competitive\nclassification accuracy, improved uncertainty quantification, and faster\nconvergence compared to baseline models. Additionally, we investigate the\nimpact of hyperparameters and components. Code is publicly available at\nhttps://github.com/keanson/MD-BSFC.\n", "link": "http://arxiv.org/abs/2405.01507v3", "date": "2024-05-07", "relevancy": 1.9618, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5195}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5013}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Convergence%20in%20Bayesian%20Few-Shot%20Classification&body=Title%3A%20Accelerating%20Convergence%20in%20Bayesian%20Few-Shot%20Classification%0AAuthor%3A%20Tianjun%20Ke%20and%20Haoqun%20Cao%20and%20Feng%20Zhou%0AAbstract%3A%20%20%20Bayesian%20few-shot%20classification%20has%20been%20a%20focal%20point%20in%20the%20field%20of%0Afew-shot%20learning.%20This%20paper%20seamlessly%20integrates%20mirror%20descent-based%0Avariational%20inference%20into%20Gaussian%20process-based%20few-shot%20classification%2C%0Aaddressing%20the%20challenge%20of%20non-conjugate%20inference.%20By%20leveraging%0Anon-Euclidean%20geometry%2C%20mirror%20descent%20achieves%20accelerated%20convergence%20by%0Aproviding%20the%20steepest%20descent%20direction%20along%20the%20corresponding%20manifold.%20It%0Aalso%20exhibits%20the%20parameterization%20invariance%20property%20concerning%20the%0Avariational%20distribution.%20Experimental%20results%20demonstrate%20competitive%0Aclassification%20accuracy%2C%20improved%20uncertainty%20quantification%2C%20and%20faster%0Aconvergence%20compared%20to%20baseline%20models.%20Additionally%2C%20we%20investigate%20the%0Aimpact%20of%20hyperparameters%20and%20components.%20Code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/keanson/MD-BSFC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01507v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Convergence%2520in%2520Bayesian%2520Few-Shot%2520Classification%26entry.906535625%3DTianjun%2520Ke%2520and%2520Haoqun%2520Cao%2520and%2520Feng%2520Zhou%26entry.1292438233%3D%2520%2520Bayesian%2520few-shot%2520classification%2520has%2520been%2520a%2520focal%2520point%2520in%2520the%2520field%2520of%250Afew-shot%2520learning.%2520This%2520paper%2520seamlessly%2520integrates%2520mirror%2520descent-based%250Avariational%2520inference%2520into%2520Gaussian%2520process-based%2520few-shot%2520classification%252C%250Aaddressing%2520the%2520challenge%2520of%2520non-conjugate%2520inference.%2520By%2520leveraging%250Anon-Euclidean%2520geometry%252C%2520mirror%2520descent%2520achieves%2520accelerated%2520convergence%2520by%250Aproviding%2520the%2520steepest%2520descent%2520direction%2520along%2520the%2520corresponding%2520manifold.%2520It%250Aalso%2520exhibits%2520the%2520parameterization%2520invariance%2520property%2520concerning%2520the%250Avariational%2520distribution.%2520Experimental%2520results%2520demonstrate%2520competitive%250Aclassification%2520accuracy%252C%2520improved%2520uncertainty%2520quantification%252C%2520and%2520faster%250Aconvergence%2520compared%2520to%2520baseline%2520models.%2520Additionally%252C%2520we%2520investigate%2520the%250Aimpact%2520of%2520hyperparameters%2520and%2520components.%2520Code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/keanson/MD-BSFC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01507v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Convergence%20in%20Bayesian%20Few-Shot%20Classification&entry.906535625=Tianjun%20Ke%20and%20Haoqun%20Cao%20and%20Feng%20Zhou&entry.1292438233=%20%20Bayesian%20few-shot%20classification%20has%20been%20a%20focal%20point%20in%20the%20field%20of%0Afew-shot%20learning.%20This%20paper%20seamlessly%20integrates%20mirror%20descent-based%0Avariational%20inference%20into%20Gaussian%20process-based%20few-shot%20classification%2C%0Aaddressing%20the%20challenge%20of%20non-conjugate%20inference.%20By%20leveraging%0Anon-Euclidean%20geometry%2C%20mirror%20descent%20achieves%20accelerated%20convergence%20by%0Aproviding%20the%20steepest%20descent%20direction%20along%20the%20corresponding%20manifold.%20It%0Aalso%20exhibits%20the%20parameterization%20invariance%20property%20concerning%20the%0Avariational%20distribution.%20Experimental%20results%20demonstrate%20competitive%0Aclassification%20accuracy%2C%20improved%20uncertainty%20quantification%2C%20and%20faster%0Aconvergence%20compared%20to%20baseline%20models.%20Additionally%2C%20we%20investigate%20the%0Aimpact%20of%20hyperparameters%20and%20components.%20Code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/keanson/MD-BSFC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01507v3&entry.124074799=Read"},
{"title": "BILTS: A novel bi-invariant local trajectory-shape descriptor for\n  rigid-body motion", "author": "Arno Verduyn and Erwin Aertbeli\u00ebn and Glenn Maes and Joris De Schutter and Maxim Vochten", "abstract": "  Measuring the similarity between motions and established motion models is\ncrucial for motion analysis, recognition, generation, and adaptation. To\nenhance similarity measurement across diverse contexts, invariant motion\ndescriptors have been proposed. However, for rigid-body motion, few invariant\ndescriptors exist that are bi-invariant, meaning invariant to both the body and\nworld reference frames used to describe the motion. Moreover, their robustness\nto singularities is limited. This paper introduces a novel Bi-Invariant Local\nTrajectory-Shape descriptor (BILTS) and a corresponding dissimilarity measure.\nMathematical relationships between BILTS and existing descriptors are derived,\nproviding new insights into their properties. The paper also includes an\nalgorithm to reproduce the motion from the BILTS descriptor, demonstrating its\nbidirectionality and usefulness for trajectory generation. Experimental\nvalidation using datasets of daily-life activities shows the higher robustness\nof the BILTS descriptor compared to the bi-invariant ISA descriptor. This\nhigher robustness supports the further application of bi-invariant descriptors\nfor motion recognition and generalization.\n", "link": "http://arxiv.org/abs/2405.04392v1", "date": "2024-05-07", "relevancy": 1.955, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5378}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.483}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BILTS%3A%20A%20novel%20bi-invariant%20local%20trajectory-shape%20descriptor%20for%0A%20%20rigid-body%20motion&body=Title%3A%20BILTS%3A%20A%20novel%20bi-invariant%20local%20trajectory-shape%20descriptor%20for%0A%20%20rigid-body%20motion%0AAuthor%3A%20Arno%20Verduyn%20and%20Erwin%20Aertbeli%C3%ABn%20and%20Glenn%20Maes%20and%20Joris%20De%20Schutter%20and%20Maxim%20Vochten%0AAbstract%3A%20%20%20Measuring%20the%20similarity%20between%20motions%20and%20established%20motion%20models%20is%0Acrucial%20for%20motion%20analysis%2C%20recognition%2C%20generation%2C%20and%20adaptation.%20To%0Aenhance%20similarity%20measurement%20across%20diverse%20contexts%2C%20invariant%20motion%0Adescriptors%20have%20been%20proposed.%20However%2C%20for%20rigid-body%20motion%2C%20few%20invariant%0Adescriptors%20exist%20that%20are%20bi-invariant%2C%20meaning%20invariant%20to%20both%20the%20body%20and%0Aworld%20reference%20frames%20used%20to%20describe%20the%20motion.%20Moreover%2C%20their%20robustness%0Ato%20singularities%20is%20limited.%20This%20paper%20introduces%20a%20novel%20Bi-Invariant%20Local%0ATrajectory-Shape%20descriptor%20%28BILTS%29%20and%20a%20corresponding%20dissimilarity%20measure.%0AMathematical%20relationships%20between%20BILTS%20and%20existing%20descriptors%20are%20derived%2C%0Aproviding%20new%20insights%20into%20their%20properties.%20The%20paper%20also%20includes%20an%0Aalgorithm%20to%20reproduce%20the%20motion%20from%20the%20BILTS%20descriptor%2C%20demonstrating%20its%0Abidirectionality%20and%20usefulness%20for%20trajectory%20generation.%20Experimental%0Avalidation%20using%20datasets%20of%20daily-life%20activities%20shows%20the%20higher%20robustness%0Aof%20the%20BILTS%20descriptor%20compared%20to%20the%20bi-invariant%20ISA%20descriptor.%20This%0Ahigher%20robustness%20supports%20the%20further%20application%20of%20bi-invariant%20descriptors%0Afor%20motion%20recognition%20and%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBILTS%253A%2520A%2520novel%2520bi-invariant%2520local%2520trajectory-shape%2520descriptor%2520for%250A%2520%2520rigid-body%2520motion%26entry.906535625%3DArno%2520Verduyn%2520and%2520Erwin%2520Aertbeli%25C3%25ABn%2520and%2520Glenn%2520Maes%2520and%2520Joris%2520De%2520Schutter%2520and%2520Maxim%2520Vochten%26entry.1292438233%3D%2520%2520Measuring%2520the%2520similarity%2520between%2520motions%2520and%2520established%2520motion%2520models%2520is%250Acrucial%2520for%2520motion%2520analysis%252C%2520recognition%252C%2520generation%252C%2520and%2520adaptation.%2520To%250Aenhance%2520similarity%2520measurement%2520across%2520diverse%2520contexts%252C%2520invariant%2520motion%250Adescriptors%2520have%2520been%2520proposed.%2520However%252C%2520for%2520rigid-body%2520motion%252C%2520few%2520invariant%250Adescriptors%2520exist%2520that%2520are%2520bi-invariant%252C%2520meaning%2520invariant%2520to%2520both%2520the%2520body%2520and%250Aworld%2520reference%2520frames%2520used%2520to%2520describe%2520the%2520motion.%2520Moreover%252C%2520their%2520robustness%250Ato%2520singularities%2520is%2520limited.%2520This%2520paper%2520introduces%2520a%2520novel%2520Bi-Invariant%2520Local%250ATrajectory-Shape%2520descriptor%2520%2528BILTS%2529%2520and%2520a%2520corresponding%2520dissimilarity%2520measure.%250AMathematical%2520relationships%2520between%2520BILTS%2520and%2520existing%2520descriptors%2520are%2520derived%252C%250Aproviding%2520new%2520insights%2520into%2520their%2520properties.%2520The%2520paper%2520also%2520includes%2520an%250Aalgorithm%2520to%2520reproduce%2520the%2520motion%2520from%2520the%2520BILTS%2520descriptor%252C%2520demonstrating%2520its%250Abidirectionality%2520and%2520usefulness%2520for%2520trajectory%2520generation.%2520Experimental%250Avalidation%2520using%2520datasets%2520of%2520daily-life%2520activities%2520shows%2520the%2520higher%2520robustness%250Aof%2520the%2520BILTS%2520descriptor%2520compared%2520to%2520the%2520bi-invariant%2520ISA%2520descriptor.%2520This%250Ahigher%2520robustness%2520supports%2520the%2520further%2520application%2520of%2520bi-invariant%2520descriptors%250Afor%2520motion%2520recognition%2520and%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BILTS%3A%20A%20novel%20bi-invariant%20local%20trajectory-shape%20descriptor%20for%0A%20%20rigid-body%20motion&entry.906535625=Arno%20Verduyn%20and%20Erwin%20Aertbeli%C3%ABn%20and%20Glenn%20Maes%20and%20Joris%20De%20Schutter%20and%20Maxim%20Vochten&entry.1292438233=%20%20Measuring%20the%20similarity%20between%20motions%20and%20established%20motion%20models%20is%0Acrucial%20for%20motion%20analysis%2C%20recognition%2C%20generation%2C%20and%20adaptation.%20To%0Aenhance%20similarity%20measurement%20across%20diverse%20contexts%2C%20invariant%20motion%0Adescriptors%20have%20been%20proposed.%20However%2C%20for%20rigid-body%20motion%2C%20few%20invariant%0Adescriptors%20exist%20that%20are%20bi-invariant%2C%20meaning%20invariant%20to%20both%20the%20body%20and%0Aworld%20reference%20frames%20used%20to%20describe%20the%20motion.%20Moreover%2C%20their%20robustness%0Ato%20singularities%20is%20limited.%20This%20paper%20introduces%20a%20novel%20Bi-Invariant%20Local%0ATrajectory-Shape%20descriptor%20%28BILTS%29%20and%20a%20corresponding%20dissimilarity%20measure.%0AMathematical%20relationships%20between%20BILTS%20and%20existing%20descriptors%20are%20derived%2C%0Aproviding%20new%20insights%20into%20their%20properties.%20The%20paper%20also%20includes%20an%0Aalgorithm%20to%20reproduce%20the%20motion%20from%20the%20BILTS%20descriptor%2C%20demonstrating%20its%0Abidirectionality%20and%20usefulness%20for%20trajectory%20generation.%20Experimental%0Avalidation%20using%20datasets%20of%20daily-life%20activities%20shows%20the%20higher%20robustness%0Aof%20the%20BILTS%20descriptor%20compared%20to%20the%20bi-invariant%20ISA%20descriptor.%20This%0Ahigher%20robustness%20supports%20the%20further%20application%20of%20bi-invariant%20descriptors%0Afor%20motion%20recognition%20and%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04392v1&entry.124074799=Read"},
{"title": "NOVA: NoC-based Vector Unit for Mapping Attention Layers on a CNN\n  Accelerator", "author": "Mohit Upadhyay and Rohan Juneja and Weng-Fai Wong and Li-Shiuan Peh", "abstract": "  Attention mechanisms are becoming increasingly popular, being used in neural\nnetwork models in multiple domains such as natural language processing (NLP)\nand vision applications, especially at the edge. However, attention layers are\ndifficult to map onto existing neuro accelerators since they have a much higher\ndensity of non-linear operations, which lead to inefficient utilization of\ntoday's vector units. This work introduces NOVA, a NoC-based Vector Unit that\ncan perform non-linear operations within the NoC of the accelerators, and can\nbe overlaid onto existing neuro accelerators to map attention layers at the\nedge. Our results show that the NOVA architecture is up to 37.8x more\npower-efficient than state-of-the-art hardware approximators when running\nexisting attention-based neural networks.\n", "link": "http://arxiv.org/abs/2405.04206v1", "date": "2024-05-07", "relevancy": 1.9536, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5108}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4742}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NOVA%3A%20NoC-based%20Vector%20Unit%20for%20Mapping%20Attention%20Layers%20on%20a%20CNN%0A%20%20Accelerator&body=Title%3A%20NOVA%3A%20NoC-based%20Vector%20Unit%20for%20Mapping%20Attention%20Layers%20on%20a%20CNN%0A%20%20Accelerator%0AAuthor%3A%20Mohit%20Upadhyay%20and%20Rohan%20Juneja%20and%20Weng-Fai%20Wong%20and%20Li-Shiuan%20Peh%0AAbstract%3A%20%20%20Attention%20mechanisms%20are%20becoming%20increasingly%20popular%2C%20being%20used%20in%20neural%0Anetwork%20models%20in%20multiple%20domains%20such%20as%20natural%20language%20processing%20%28NLP%29%0Aand%20vision%20applications%2C%20especially%20at%20the%20edge.%20However%2C%20attention%20layers%20are%0Adifficult%20to%20map%20onto%20existing%20neuro%20accelerators%20since%20they%20have%20a%20much%20higher%0Adensity%20of%20non-linear%20operations%2C%20which%20lead%20to%20inefficient%20utilization%20of%0Atoday%27s%20vector%20units.%20This%20work%20introduces%20NOVA%2C%20a%20NoC-based%20Vector%20Unit%20that%0Acan%20perform%20non-linear%20operations%20within%20the%20NoC%20of%20the%20accelerators%2C%20and%20can%0Abe%20overlaid%20onto%20existing%20neuro%20accelerators%20to%20map%20attention%20layers%20at%20the%0Aedge.%20Our%20results%20show%20that%20the%20NOVA%20architecture%20is%20up%20to%2037.8x%20more%0Apower-efficient%20than%20state-of-the-art%20hardware%20approximators%20when%20running%0Aexisting%20attention-based%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNOVA%253A%2520NoC-based%2520Vector%2520Unit%2520for%2520Mapping%2520Attention%2520Layers%2520on%2520a%2520CNN%250A%2520%2520Accelerator%26entry.906535625%3DMohit%2520Upadhyay%2520and%2520Rohan%2520Juneja%2520and%2520Weng-Fai%2520Wong%2520and%2520Li-Shiuan%2520Peh%26entry.1292438233%3D%2520%2520Attention%2520mechanisms%2520are%2520becoming%2520increasingly%2520popular%252C%2520being%2520used%2520in%2520neural%250Anetwork%2520models%2520in%2520multiple%2520domains%2520such%2520as%2520natural%2520language%2520processing%2520%2528NLP%2529%250Aand%2520vision%2520applications%252C%2520especially%2520at%2520the%2520edge.%2520However%252C%2520attention%2520layers%2520are%250Adifficult%2520to%2520map%2520onto%2520existing%2520neuro%2520accelerators%2520since%2520they%2520have%2520a%2520much%2520higher%250Adensity%2520of%2520non-linear%2520operations%252C%2520which%2520lead%2520to%2520inefficient%2520utilization%2520of%250Atoday%2527s%2520vector%2520units.%2520This%2520work%2520introduces%2520NOVA%252C%2520a%2520NoC-based%2520Vector%2520Unit%2520that%250Acan%2520perform%2520non-linear%2520operations%2520within%2520the%2520NoC%2520of%2520the%2520accelerators%252C%2520and%2520can%250Abe%2520overlaid%2520onto%2520existing%2520neuro%2520accelerators%2520to%2520map%2520attention%2520layers%2520at%2520the%250Aedge.%2520Our%2520results%2520show%2520that%2520the%2520NOVA%2520architecture%2520is%2520up%2520to%252037.8x%2520more%250Apower-efficient%2520than%2520state-of-the-art%2520hardware%2520approximators%2520when%2520running%250Aexisting%2520attention-based%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NOVA%3A%20NoC-based%20Vector%20Unit%20for%20Mapping%20Attention%20Layers%20on%20a%20CNN%0A%20%20Accelerator&entry.906535625=Mohit%20Upadhyay%20and%20Rohan%20Juneja%20and%20Weng-Fai%20Wong%20and%20Li-Shiuan%20Peh&entry.1292438233=%20%20Attention%20mechanisms%20are%20becoming%20increasingly%20popular%2C%20being%20used%20in%20neural%0Anetwork%20models%20in%20multiple%20domains%20such%20as%20natural%20language%20processing%20%28NLP%29%0Aand%20vision%20applications%2C%20especially%20at%20the%20edge.%20However%2C%20attention%20layers%20are%0Adifficult%20to%20map%20onto%20existing%20neuro%20accelerators%20since%20they%20have%20a%20much%20higher%0Adensity%20of%20non-linear%20operations%2C%20which%20lead%20to%20inefficient%20utilization%20of%0Atoday%27s%20vector%20units.%20This%20work%20introduces%20NOVA%2C%20a%20NoC-based%20Vector%20Unit%20that%0Acan%20perform%20non-linear%20operations%20within%20the%20NoC%20of%20the%20accelerators%2C%20and%20can%0Abe%20overlaid%20onto%20existing%20neuro%20accelerators%20to%20map%20attention%20layers%20at%20the%0Aedge.%20Our%20results%20show%20that%20the%20NOVA%20architecture%20is%20up%20to%2037.8x%20more%0Apower-efficient%20than%20state-of-the-art%20hardware%20approximators%20when%20running%0Aexisting%20attention-based%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04206v1&entry.124074799=Read"},
{"title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving", "author": "Yujun Lin and Haotian Tang and Shang Yang and Zhekai Zhang and Guangxuan Xiao and Chuang Gan and Song Han", "abstract": "  Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.\n", "link": "http://arxiv.org/abs/2405.04532v1", "date": "2024-05-07", "relevancy": 1.9524, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5051}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4964}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QServe%3A%20W4A8KV4%20Quantization%20and%20System%20Co-design%20for%20Efficient%20LLM%0A%20%20Serving&body=Title%3A%20QServe%3A%20W4A8KV4%20Quantization%20and%20System%20Co-design%20for%20Efficient%20LLM%0A%20%20Serving%0AAuthor%3A%20Yujun%20Lin%20and%20Haotian%20Tang%20and%20Shang%20Yang%20and%20Zhekai%20Zhang%20and%20Guangxuan%20Xiao%20and%20Chuang%20Gan%20and%20Song%20Han%0AAbstract%3A%20%20%20Quantization%20can%20accelerate%20large%20language%20model%20%28LLM%29%20inference.%20Going%0Abeyond%20INT8%20quantization%2C%20the%20research%20community%20is%20actively%20exploring%20even%0Alower%20precision%2C%20such%20as%20INT4.%20Nonetheless%2C%20state-of-the-art%20INT4%20quantization%0Atechniques%20only%20accelerate%20low-batch%2C%20edge%20LLM%20inference%2C%20failing%20to%20deliver%0Aperformance%20gains%20in%20large-batch%2C%20cloud-based%20LLM%20serving.%20We%20uncover%20a%0Acritical%20issue%3A%20existing%20INT4%20quantization%20methods%20suffer%20from%20significant%0Aruntime%20overhead%20%2820-90%25%29%20when%20dequantizing%20either%20weights%20or%20partial%20sums%20on%0AGPUs.%20To%20address%20this%20challenge%2C%20we%20introduce%20QoQ%2C%20a%20W4A8KV4%20quantization%0Aalgorithm%20with%204-bit%20weight%2C%208-bit%20activation%2C%20and%204-bit%20KV%20cache.%20QoQ%20stands%0Afor%20quattuor-octo-quattuor%2C%20which%20represents%204-8-4%20in%20Latin.%20QoQ%20is%20implemented%0Aby%20the%20QServe%20inference%20library%20that%20achieves%20measured%20speedup.%20The%20key%20insight%0Adriving%20QServe%20is%20that%20the%20efficiency%20of%20LLM%20serving%20on%20GPUs%20is%20critically%0Ainfluenced%20by%20operations%20on%20low-throughput%20CUDA%20cores.%20Building%20upon%20this%0Ainsight%2C%20in%20QoQ%20algorithm%2C%20we%20introduce%20progressive%20quantization%20that%20can%20allow%0Alow%20dequantization%20overhead%20in%20W4A8%20GEMM.%20Additionally%2C%20we%20develop%0ASmoothAttention%20to%20effectively%20mitigate%20the%20accuracy%20degradation%20incurred%20by%0A4-bit%20KV%20quantization.%20In%20the%20QServe%20system%2C%20we%20perform%20compute-aware%20weight%0Areordering%20and%20take%20advantage%20of%20register-level%20parallelism%20to%20reduce%0Adequantization%20latency.%20We%20also%20make%20fused%20attention%20memory-bound%2C%20harnessing%0Athe%20performance%20gain%20brought%20by%20KV4%20quantization.%20As%20a%20result%2C%20QServe%20improves%0Athe%20maximum%20achievable%20serving%20throughput%20of%20Llama-3-8B%20by%201.2x%20on%20A100%2C%201.4x%0Aon%20L40S%3B%20and%20Qwen1.5-72B%20by%202.4x%20on%20A100%2C%203.5x%20on%20L40S%2C%20compared%20to%0ATensorRT-LLM.%20Remarkably%2C%20QServe%20on%20L40S%20GPU%20can%20achieve%20even%20higher%20throughput%0Athan%20TensorRT-LLM%20on%20A100.%20Thus%2C%20QServe%20effectively%20reduces%20the%20dollar%20cost%20of%0ALLM%20serving%20by%203x.%20Code%20is%20available%20at%20https%3A//github.com/mit-han-lab/qserve.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQServe%253A%2520W4A8KV4%2520Quantization%2520and%2520System%2520Co-design%2520for%2520Efficient%2520LLM%250A%2520%2520Serving%26entry.906535625%3DYujun%2520Lin%2520and%2520Haotian%2520Tang%2520and%2520Shang%2520Yang%2520and%2520Zhekai%2520Zhang%2520and%2520Guangxuan%2520Xiao%2520and%2520Chuang%2520Gan%2520and%2520Song%2520Han%26entry.1292438233%3D%2520%2520Quantization%2520can%2520accelerate%2520large%2520language%2520model%2520%2528LLM%2529%2520inference.%2520Going%250Abeyond%2520INT8%2520quantization%252C%2520the%2520research%2520community%2520is%2520actively%2520exploring%2520even%250Alower%2520precision%252C%2520such%2520as%2520INT4.%2520Nonetheless%252C%2520state-of-the-art%2520INT4%2520quantization%250Atechniques%2520only%2520accelerate%2520low-batch%252C%2520edge%2520LLM%2520inference%252C%2520failing%2520to%2520deliver%250Aperformance%2520gains%2520in%2520large-batch%252C%2520cloud-based%2520LLM%2520serving.%2520We%2520uncover%2520a%250Acritical%2520issue%253A%2520existing%2520INT4%2520quantization%2520methods%2520suffer%2520from%2520significant%250Aruntime%2520overhead%2520%252820-90%2525%2529%2520when%2520dequantizing%2520either%2520weights%2520or%2520partial%2520sums%2520on%250AGPUs.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520QoQ%252C%2520a%2520W4A8KV4%2520quantization%250Aalgorithm%2520with%25204-bit%2520weight%252C%25208-bit%2520activation%252C%2520and%25204-bit%2520KV%2520cache.%2520QoQ%2520stands%250Afor%2520quattuor-octo-quattuor%252C%2520which%2520represents%25204-8-4%2520in%2520Latin.%2520QoQ%2520is%2520implemented%250Aby%2520the%2520QServe%2520inference%2520library%2520that%2520achieves%2520measured%2520speedup.%2520The%2520key%2520insight%250Adriving%2520QServe%2520is%2520that%2520the%2520efficiency%2520of%2520LLM%2520serving%2520on%2520GPUs%2520is%2520critically%250Ainfluenced%2520by%2520operations%2520on%2520low-throughput%2520CUDA%2520cores.%2520Building%2520upon%2520this%250Ainsight%252C%2520in%2520QoQ%2520algorithm%252C%2520we%2520introduce%2520progressive%2520quantization%2520that%2520can%2520allow%250Alow%2520dequantization%2520overhead%2520in%2520W4A8%2520GEMM.%2520Additionally%252C%2520we%2520develop%250ASmoothAttention%2520to%2520effectively%2520mitigate%2520the%2520accuracy%2520degradation%2520incurred%2520by%250A4-bit%2520KV%2520quantization.%2520In%2520the%2520QServe%2520system%252C%2520we%2520perform%2520compute-aware%2520weight%250Areordering%2520and%2520take%2520advantage%2520of%2520register-level%2520parallelism%2520to%2520reduce%250Adequantization%2520latency.%2520We%2520also%2520make%2520fused%2520attention%2520memory-bound%252C%2520harnessing%250Athe%2520performance%2520gain%2520brought%2520by%2520KV4%2520quantization.%2520As%2520a%2520result%252C%2520QServe%2520improves%250Athe%2520maximum%2520achievable%2520serving%2520throughput%2520of%2520Llama-3-8B%2520by%25201.2x%2520on%2520A100%252C%25201.4x%250Aon%2520L40S%253B%2520and%2520Qwen1.5-72B%2520by%25202.4x%2520on%2520A100%252C%25203.5x%2520on%2520L40S%252C%2520compared%2520to%250ATensorRT-LLM.%2520Remarkably%252C%2520QServe%2520on%2520L40S%2520GPU%2520can%2520achieve%2520even%2520higher%2520throughput%250Athan%2520TensorRT-LLM%2520on%2520A100.%2520Thus%252C%2520QServe%2520effectively%2520reduces%2520the%2520dollar%2520cost%2520of%250ALLM%2520serving%2520by%25203x.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/mit-han-lab/qserve.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QServe%3A%20W4A8KV4%20Quantization%20and%20System%20Co-design%20for%20Efficient%20LLM%0A%20%20Serving&entry.906535625=Yujun%20Lin%20and%20Haotian%20Tang%20and%20Shang%20Yang%20and%20Zhekai%20Zhang%20and%20Guangxuan%20Xiao%20and%20Chuang%20Gan%20and%20Song%20Han&entry.1292438233=%20%20Quantization%20can%20accelerate%20large%20language%20model%20%28LLM%29%20inference.%20Going%0Abeyond%20INT8%20quantization%2C%20the%20research%20community%20is%20actively%20exploring%20even%0Alower%20precision%2C%20such%20as%20INT4.%20Nonetheless%2C%20state-of-the-art%20INT4%20quantization%0Atechniques%20only%20accelerate%20low-batch%2C%20edge%20LLM%20inference%2C%20failing%20to%20deliver%0Aperformance%20gains%20in%20large-batch%2C%20cloud-based%20LLM%20serving.%20We%20uncover%20a%0Acritical%20issue%3A%20existing%20INT4%20quantization%20methods%20suffer%20from%20significant%0Aruntime%20overhead%20%2820-90%25%29%20when%20dequantizing%20either%20weights%20or%20partial%20sums%20on%0AGPUs.%20To%20address%20this%20challenge%2C%20we%20introduce%20QoQ%2C%20a%20W4A8KV4%20quantization%0Aalgorithm%20with%204-bit%20weight%2C%208-bit%20activation%2C%20and%204-bit%20KV%20cache.%20QoQ%20stands%0Afor%20quattuor-octo-quattuor%2C%20which%20represents%204-8-4%20in%20Latin.%20QoQ%20is%20implemented%0Aby%20the%20QServe%20inference%20library%20that%20achieves%20measured%20speedup.%20The%20key%20insight%0Adriving%20QServe%20is%20that%20the%20efficiency%20of%20LLM%20serving%20on%20GPUs%20is%20critically%0Ainfluenced%20by%20operations%20on%20low-throughput%20CUDA%20cores.%20Building%20upon%20this%0Ainsight%2C%20in%20QoQ%20algorithm%2C%20we%20introduce%20progressive%20quantization%20that%20can%20allow%0Alow%20dequantization%20overhead%20in%20W4A8%20GEMM.%20Additionally%2C%20we%20develop%0ASmoothAttention%20to%20effectively%20mitigate%20the%20accuracy%20degradation%20incurred%20by%0A4-bit%20KV%20quantization.%20In%20the%20QServe%20system%2C%20we%20perform%20compute-aware%20weight%0Areordering%20and%20take%20advantage%20of%20register-level%20parallelism%20to%20reduce%0Adequantization%20latency.%20We%20also%20make%20fused%20attention%20memory-bound%2C%20harnessing%0Athe%20performance%20gain%20brought%20by%20KV4%20quantization.%20As%20a%20result%2C%20QServe%20improves%0Athe%20maximum%20achievable%20serving%20throughput%20of%20Llama-3-8B%20by%201.2x%20on%20A100%2C%201.4x%0Aon%20L40S%3B%20and%20Qwen1.5-72B%20by%202.4x%20on%20A100%2C%203.5x%20on%20L40S%2C%20compared%20to%0ATensorRT-LLM.%20Remarkably%2C%20QServe%20on%20L40S%20GPU%20can%20achieve%20even%20higher%20throughput%0Athan%20TensorRT-LLM%20on%20A100.%20Thus%2C%20QServe%20effectively%20reduces%20the%20dollar%20cost%20of%0ALLM%20serving%20by%203x.%20Code%20is%20available%20at%20https%3A//github.com/mit-han-lab/qserve.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04532v1&entry.124074799=Read"},
{"title": "Dynamic Event-based Optical Identification and Communication", "author": "Axel von Arnim and Jules Lecomte and Naima Elosegui Borras and Stanislaw Wozniak and Angeliki Pantazi", "abstract": "  Optical identification is often done with spatial or temporal visual pattern\nrecognition and localization. Temporal pattern recognition, depending on the\ntechnology, involves a trade-off between communication frequency, range and\naccurate tracking. We propose a solution with light-emitting beacons that\nimproves this trade-off by exploiting fast event-based cameras and, for\ntracking, sparse neuromorphic optical flow computed with spiking neurons. The\nsystem is embedded in a simulated drone and evaluated in an asset monitoring\nuse case. It is robust to relative movements and enables simultaneous\ncommunication with, and tracking of, multiple moving beacons. Finally, in a\nhardware lab prototype, we demonstrate for the first time beacon tracking\nperformed simultaneously with state-of-the-art frequency communication in the\nkHz range.\n", "link": "http://arxiv.org/abs/2303.07169v4", "date": "2024-05-07", "relevancy": 1.9495, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5155}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4869}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Event-based%20Optical%20Identification%20and%20Communication&body=Title%3A%20Dynamic%20Event-based%20Optical%20Identification%20and%20Communication%0AAuthor%3A%20Axel%20von%20Arnim%20and%20Jules%20Lecomte%20and%20Naima%20Elosegui%20Borras%20and%20Stanislaw%20Wozniak%20and%20Angeliki%20Pantazi%0AAbstract%3A%20%20%20Optical%20identification%20is%20often%20done%20with%20spatial%20or%20temporal%20visual%20pattern%0Arecognition%20and%20localization.%20Temporal%20pattern%20recognition%2C%20depending%20on%20the%0Atechnology%2C%20involves%20a%20trade-off%20between%20communication%20frequency%2C%20range%20and%0Aaccurate%20tracking.%20We%20propose%20a%20solution%20with%20light-emitting%20beacons%20that%0Aimproves%20this%20trade-off%20by%20exploiting%20fast%20event-based%20cameras%20and%2C%20for%0Atracking%2C%20sparse%20neuromorphic%20optical%20flow%20computed%20with%20spiking%20neurons.%20The%0Asystem%20is%20embedded%20in%20a%20simulated%20drone%20and%20evaluated%20in%20an%20asset%20monitoring%0Ause%20case.%20It%20is%20robust%20to%20relative%20movements%20and%20enables%20simultaneous%0Acommunication%20with%2C%20and%20tracking%20of%2C%20multiple%20moving%20beacons.%20Finally%2C%20in%20a%0Ahardware%20lab%20prototype%2C%20we%20demonstrate%20for%20the%20first%20time%20beacon%20tracking%0Aperformed%20simultaneously%20with%20state-of-the-art%20frequency%20communication%20in%20the%0AkHz%20range.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.07169v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Event-based%2520Optical%2520Identification%2520and%2520Communication%26entry.906535625%3DAxel%2520von%2520Arnim%2520and%2520Jules%2520Lecomte%2520and%2520Naima%2520Elosegui%2520Borras%2520and%2520Stanislaw%2520Wozniak%2520and%2520Angeliki%2520Pantazi%26entry.1292438233%3D%2520%2520Optical%2520identification%2520is%2520often%2520done%2520with%2520spatial%2520or%2520temporal%2520visual%2520pattern%250Arecognition%2520and%2520localization.%2520Temporal%2520pattern%2520recognition%252C%2520depending%2520on%2520the%250Atechnology%252C%2520involves%2520a%2520trade-off%2520between%2520communication%2520frequency%252C%2520range%2520and%250Aaccurate%2520tracking.%2520We%2520propose%2520a%2520solution%2520with%2520light-emitting%2520beacons%2520that%250Aimproves%2520this%2520trade-off%2520by%2520exploiting%2520fast%2520event-based%2520cameras%2520and%252C%2520for%250Atracking%252C%2520sparse%2520neuromorphic%2520optical%2520flow%2520computed%2520with%2520spiking%2520neurons.%2520The%250Asystem%2520is%2520embedded%2520in%2520a%2520simulated%2520drone%2520and%2520evaluated%2520in%2520an%2520asset%2520monitoring%250Ause%2520case.%2520It%2520is%2520robust%2520to%2520relative%2520movements%2520and%2520enables%2520simultaneous%250Acommunication%2520with%252C%2520and%2520tracking%2520of%252C%2520multiple%2520moving%2520beacons.%2520Finally%252C%2520in%2520a%250Ahardware%2520lab%2520prototype%252C%2520we%2520demonstrate%2520for%2520the%2520first%2520time%2520beacon%2520tracking%250Aperformed%2520simultaneously%2520with%2520state-of-the-art%2520frequency%2520communication%2520in%2520the%250AkHz%2520range.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.07169v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Event-based%20Optical%20Identification%20and%20Communication&entry.906535625=Axel%20von%20Arnim%20and%20Jules%20Lecomte%20and%20Naima%20Elosegui%20Borras%20and%20Stanislaw%20Wozniak%20and%20Angeliki%20Pantazi&entry.1292438233=%20%20Optical%20identification%20is%20often%20done%20with%20spatial%20or%20temporal%20visual%20pattern%0Arecognition%20and%20localization.%20Temporal%20pattern%20recognition%2C%20depending%20on%20the%0Atechnology%2C%20involves%20a%20trade-off%20between%20communication%20frequency%2C%20range%20and%0Aaccurate%20tracking.%20We%20propose%20a%20solution%20with%20light-emitting%20beacons%20that%0Aimproves%20this%20trade-off%20by%20exploiting%20fast%20event-based%20cameras%20and%2C%20for%0Atracking%2C%20sparse%20neuromorphic%20optical%20flow%20computed%20with%20spiking%20neurons.%20The%0Asystem%20is%20embedded%20in%20a%20simulated%20drone%20and%20evaluated%20in%20an%20asset%20monitoring%0Ause%20case.%20It%20is%20robust%20to%20relative%20movements%20and%20enables%20simultaneous%0Acommunication%20with%2C%20and%20tracking%20of%2C%20multiple%20moving%20beacons.%20Finally%2C%20in%20a%0Ahardware%20lab%20prototype%2C%20we%20demonstrate%20for%20the%20first%20time%20beacon%20tracking%0Aperformed%20simultaneously%20with%20state-of-the-art%20frequency%20communication%20in%20the%0AkHz%20range.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.07169v4&entry.124074799=Read"},
{"title": "Learning Governing Equations of Unobserved States in Dynamical Systems", "author": "Gevik Grigorian and Sandip V. George and Simon Arridge", "abstract": "  Data-driven modelling and scientific machine learning have been responsible\nfor significant advances in determining suitable models to describe data.\nWithin dynamical systems, neural ordinary differential equations (ODEs), where\nthe system equations are set to be governed by a neural network, have become a\npopular tool for this challenge in recent years. However, less emphasis has\nbeen placed on systems that are only partially-observed. In this work, we\nemploy a hybrid neural ODE structure, where the system equations are governed\nby a combination of a neural network and domain-specific knowledge, together\nwith symbolic regression (SR), to learn governing equations of\npartially-observed dynamical systems. We test this approach on two case\nstudies: A 3-dimensional model of the Lotka-Volterra system and a 5-dimensional\nmodel of the Lorenz system. We demonstrate that the method is capable of\nsuccessfully learning the true underlying governing equations of unobserved\nstates within these systems, with robustness to measurement noise.\n", "link": "http://arxiv.org/abs/2404.18572v2", "date": "2024-05-07", "relevancy": 1.944, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5158}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4887}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Governing%20Equations%20of%20Unobserved%20States%20in%20Dynamical%20Systems&body=Title%3A%20Learning%20Governing%20Equations%20of%20Unobserved%20States%20in%20Dynamical%20Systems%0AAuthor%3A%20Gevik%20Grigorian%20and%20Sandip%20V.%20George%20and%20Simon%20Arridge%0AAbstract%3A%20%20%20Data-driven%20modelling%20and%20scientific%20machine%20learning%20have%20been%20responsible%0Afor%20significant%20advances%20in%20determining%20suitable%20models%20to%20describe%20data.%0AWithin%20dynamical%20systems%2C%20neural%20ordinary%20differential%20equations%20%28ODEs%29%2C%20where%0Athe%20system%20equations%20are%20set%20to%20be%20governed%20by%20a%20neural%20network%2C%20have%20become%20a%0Apopular%20tool%20for%20this%20challenge%20in%20recent%20years.%20However%2C%20less%20emphasis%20has%0Abeen%20placed%20on%20systems%20that%20are%20only%20partially-observed.%20In%20this%20work%2C%20we%0Aemploy%20a%20hybrid%20neural%20ODE%20structure%2C%20where%20the%20system%20equations%20are%20governed%0Aby%20a%20combination%20of%20a%20neural%20network%20and%20domain-specific%20knowledge%2C%20together%0Awith%20symbolic%20regression%20%28SR%29%2C%20to%20learn%20governing%20equations%20of%0Apartially-observed%20dynamical%20systems.%20We%20test%20this%20approach%20on%20two%20case%0Astudies%3A%20A%203-dimensional%20model%20of%20the%20Lotka-Volterra%20system%20and%20a%205-dimensional%0Amodel%20of%20the%20Lorenz%20system.%20We%20demonstrate%20that%20the%20method%20is%20capable%20of%0Asuccessfully%20learning%20the%20true%20underlying%20governing%20equations%20of%20unobserved%0Astates%20within%20these%20systems%2C%20with%20robustness%20to%20measurement%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18572v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Governing%2520Equations%2520of%2520Unobserved%2520States%2520in%2520Dynamical%2520Systems%26entry.906535625%3DGevik%2520Grigorian%2520and%2520Sandip%2520V.%2520George%2520and%2520Simon%2520Arridge%26entry.1292438233%3D%2520%2520Data-driven%2520modelling%2520and%2520scientific%2520machine%2520learning%2520have%2520been%2520responsible%250Afor%2520significant%2520advances%2520in%2520determining%2520suitable%2520models%2520to%2520describe%2520data.%250AWithin%2520dynamical%2520systems%252C%2520neural%2520ordinary%2520differential%2520equations%2520%2528ODEs%2529%252C%2520where%250Athe%2520system%2520equations%2520are%2520set%2520to%2520be%2520governed%2520by%2520a%2520neural%2520network%252C%2520have%2520become%2520a%250Apopular%2520tool%2520for%2520this%2520challenge%2520in%2520recent%2520years.%2520However%252C%2520less%2520emphasis%2520has%250Abeen%2520placed%2520on%2520systems%2520that%2520are%2520only%2520partially-observed.%2520In%2520this%2520work%252C%2520we%250Aemploy%2520a%2520hybrid%2520neural%2520ODE%2520structure%252C%2520where%2520the%2520system%2520equations%2520are%2520governed%250Aby%2520a%2520combination%2520of%2520a%2520neural%2520network%2520and%2520domain-specific%2520knowledge%252C%2520together%250Awith%2520symbolic%2520regression%2520%2528SR%2529%252C%2520to%2520learn%2520governing%2520equations%2520of%250Apartially-observed%2520dynamical%2520systems.%2520We%2520test%2520this%2520approach%2520on%2520two%2520case%250Astudies%253A%2520A%25203-dimensional%2520model%2520of%2520the%2520Lotka-Volterra%2520system%2520and%2520a%25205-dimensional%250Amodel%2520of%2520the%2520Lorenz%2520system.%2520We%2520demonstrate%2520that%2520the%2520method%2520is%2520capable%2520of%250Asuccessfully%2520learning%2520the%2520true%2520underlying%2520governing%2520equations%2520of%2520unobserved%250Astates%2520within%2520these%2520systems%252C%2520with%2520robustness%2520to%2520measurement%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18572v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Governing%20Equations%20of%20Unobserved%20States%20in%20Dynamical%20Systems&entry.906535625=Gevik%20Grigorian%20and%20Sandip%20V.%20George%20and%20Simon%20Arridge&entry.1292438233=%20%20Data-driven%20modelling%20and%20scientific%20machine%20learning%20have%20been%20responsible%0Afor%20significant%20advances%20in%20determining%20suitable%20models%20to%20describe%20data.%0AWithin%20dynamical%20systems%2C%20neural%20ordinary%20differential%20equations%20%28ODEs%29%2C%20where%0Athe%20system%20equations%20are%20set%20to%20be%20governed%20by%20a%20neural%20network%2C%20have%20become%20a%0Apopular%20tool%20for%20this%20challenge%20in%20recent%20years.%20However%2C%20less%20emphasis%20has%0Abeen%20placed%20on%20systems%20that%20are%20only%20partially-observed.%20In%20this%20work%2C%20we%0Aemploy%20a%20hybrid%20neural%20ODE%20structure%2C%20where%20the%20system%20equations%20are%20governed%0Aby%20a%20combination%20of%20a%20neural%20network%20and%20domain-specific%20knowledge%2C%20together%0Awith%20symbolic%20regression%20%28SR%29%2C%20to%20learn%20governing%20equations%20of%0Apartially-observed%20dynamical%20systems.%20We%20test%20this%20approach%20on%20two%20case%0Astudies%3A%20A%203-dimensional%20model%20of%20the%20Lotka-Volterra%20system%20and%20a%205-dimensional%0Amodel%20of%20the%20Lorenz%20system.%20We%20demonstrate%20that%20the%20method%20is%20capable%20of%0Asuccessfully%20learning%20the%20true%20underlying%20governing%20equations%20of%20unobserved%0Astates%20within%20these%20systems%2C%20with%20robustness%20to%20measurement%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18572v2&entry.124074799=Read"},
{"title": "A Survey on Neural Question Generation: Methods, Applications, and\n  Prospects", "author": "Shasha Guo and Lizi Liao and Cuiping Li and Tat-Seng Chua", "abstract": "  In this survey, we present a detailed examination of the advancements in\nNeural Question Generation (NQG), a field leveraging neural network techniques\nto generate relevant questions from diverse inputs like knowledge bases, texts,\nand images. The survey begins with an overview of NQG's background,\nencompassing the task's problem formulation, prevalent benchmark datasets,\nestablished evaluation metrics, and notable applications. It then methodically\nclassifies NQG approaches into three predominant categories: structured NQG,\nwhich utilizes organized data sources, unstructured NQG, focusing on more\nloosely structured inputs like texts or visual content, and hybrid NQG, drawing\non diverse input modalities. This classification is followed by an in-depth\nanalysis of the distinct neural network models tailored for each category,\ndiscussing their inherent strengths and potential limitations. The survey\nculminates with a forward-looking perspective on the trajectory of NQG,\nidentifying emergent research trends and prospective developmental paths.\nAccompanying this survey is a curated collection of related research papers,\ndatasets and codes, systematically organized on Github, providing an extensive\nreference for those delving into NQG.\n", "link": "http://arxiv.org/abs/2402.18267v2", "date": "2024-05-07", "relevancy": 1.9375, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4972}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4885}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Neural%20Question%20Generation%3A%20Methods%2C%20Applications%2C%20and%0A%20%20Prospects&body=Title%3A%20A%20Survey%20on%20Neural%20Question%20Generation%3A%20Methods%2C%20Applications%2C%20and%0A%20%20Prospects%0AAuthor%3A%20Shasha%20Guo%20and%20Lizi%20Liao%20and%20Cuiping%20Li%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20In%20this%20survey%2C%20we%20present%20a%20detailed%20examination%20of%20the%20advancements%20in%0ANeural%20Question%20Generation%20%28NQG%29%2C%20a%20field%20leveraging%20neural%20network%20techniques%0Ato%20generate%20relevant%20questions%20from%20diverse%20inputs%20like%20knowledge%20bases%2C%20texts%2C%0Aand%20images.%20The%20survey%20begins%20with%20an%20overview%20of%20NQG%27s%20background%2C%0Aencompassing%20the%20task%27s%20problem%20formulation%2C%20prevalent%20benchmark%20datasets%2C%0Aestablished%20evaluation%20metrics%2C%20and%20notable%20applications.%20It%20then%20methodically%0Aclassifies%20NQG%20approaches%20into%20three%20predominant%20categories%3A%20structured%20NQG%2C%0Awhich%20utilizes%20organized%20data%20sources%2C%20unstructured%20NQG%2C%20focusing%20on%20more%0Aloosely%20structured%20inputs%20like%20texts%20or%20visual%20content%2C%20and%20hybrid%20NQG%2C%20drawing%0Aon%20diverse%20input%20modalities.%20This%20classification%20is%20followed%20by%20an%20in-depth%0Aanalysis%20of%20the%20distinct%20neural%20network%20models%20tailored%20for%20each%20category%2C%0Adiscussing%20their%20inherent%20strengths%20and%20potential%20limitations.%20The%20survey%0Aculminates%20with%20a%20forward-looking%20perspective%20on%20the%20trajectory%20of%20NQG%2C%0Aidentifying%20emergent%20research%20trends%20and%20prospective%20developmental%20paths.%0AAccompanying%20this%20survey%20is%20a%20curated%20collection%20of%20related%20research%20papers%2C%0Adatasets%20and%20codes%2C%20systematically%20organized%20on%20Github%2C%20providing%20an%20extensive%0Areference%20for%20those%20delving%20into%20NQG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18267v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Neural%2520Question%2520Generation%253A%2520Methods%252C%2520Applications%252C%2520and%250A%2520%2520Prospects%26entry.906535625%3DShasha%2520Guo%2520and%2520Lizi%2520Liao%2520and%2520Cuiping%2520Li%2520and%2520Tat-Seng%2520Chua%26entry.1292438233%3D%2520%2520In%2520this%2520survey%252C%2520we%2520present%2520a%2520detailed%2520examination%2520of%2520the%2520advancements%2520in%250ANeural%2520Question%2520Generation%2520%2528NQG%2529%252C%2520a%2520field%2520leveraging%2520neural%2520network%2520techniques%250Ato%2520generate%2520relevant%2520questions%2520from%2520diverse%2520inputs%2520like%2520knowledge%2520bases%252C%2520texts%252C%250Aand%2520images.%2520The%2520survey%2520begins%2520with%2520an%2520overview%2520of%2520NQG%2527s%2520background%252C%250Aencompassing%2520the%2520task%2527s%2520problem%2520formulation%252C%2520prevalent%2520benchmark%2520datasets%252C%250Aestablished%2520evaluation%2520metrics%252C%2520and%2520notable%2520applications.%2520It%2520then%2520methodically%250Aclassifies%2520NQG%2520approaches%2520into%2520three%2520predominant%2520categories%253A%2520structured%2520NQG%252C%250Awhich%2520utilizes%2520organized%2520data%2520sources%252C%2520unstructured%2520NQG%252C%2520focusing%2520on%2520more%250Aloosely%2520structured%2520inputs%2520like%2520texts%2520or%2520visual%2520content%252C%2520and%2520hybrid%2520NQG%252C%2520drawing%250Aon%2520diverse%2520input%2520modalities.%2520This%2520classification%2520is%2520followed%2520by%2520an%2520in-depth%250Aanalysis%2520of%2520the%2520distinct%2520neural%2520network%2520models%2520tailored%2520for%2520each%2520category%252C%250Adiscussing%2520their%2520inherent%2520strengths%2520and%2520potential%2520limitations.%2520The%2520survey%250Aculminates%2520with%2520a%2520forward-looking%2520perspective%2520on%2520the%2520trajectory%2520of%2520NQG%252C%250Aidentifying%2520emergent%2520research%2520trends%2520and%2520prospective%2520developmental%2520paths.%250AAccompanying%2520this%2520survey%2520is%2520a%2520curated%2520collection%2520of%2520related%2520research%2520papers%252C%250Adatasets%2520and%2520codes%252C%2520systematically%2520organized%2520on%2520Github%252C%2520providing%2520an%2520extensive%250Areference%2520for%2520those%2520delving%2520into%2520NQG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18267v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Neural%20Question%20Generation%3A%20Methods%2C%20Applications%2C%20and%0A%20%20Prospects&entry.906535625=Shasha%20Guo%20and%20Lizi%20Liao%20and%20Cuiping%20Li%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20In%20this%20survey%2C%20we%20present%20a%20detailed%20examination%20of%20the%20advancements%20in%0ANeural%20Question%20Generation%20%28NQG%29%2C%20a%20field%20leveraging%20neural%20network%20techniques%0Ato%20generate%20relevant%20questions%20from%20diverse%20inputs%20like%20knowledge%20bases%2C%20texts%2C%0Aand%20images.%20The%20survey%20begins%20with%20an%20overview%20of%20NQG%27s%20background%2C%0Aencompassing%20the%20task%27s%20problem%20formulation%2C%20prevalent%20benchmark%20datasets%2C%0Aestablished%20evaluation%20metrics%2C%20and%20notable%20applications.%20It%20then%20methodically%0Aclassifies%20NQG%20approaches%20into%20three%20predominant%20categories%3A%20structured%20NQG%2C%0Awhich%20utilizes%20organized%20data%20sources%2C%20unstructured%20NQG%2C%20focusing%20on%20more%0Aloosely%20structured%20inputs%20like%20texts%20or%20visual%20content%2C%20and%20hybrid%20NQG%2C%20drawing%0Aon%20diverse%20input%20modalities.%20This%20classification%20is%20followed%20by%20an%20in-depth%0Aanalysis%20of%20the%20distinct%20neural%20network%20models%20tailored%20for%20each%20category%2C%0Adiscussing%20their%20inherent%20strengths%20and%20potential%20limitations.%20The%20survey%0Aculminates%20with%20a%20forward-looking%20perspective%20on%20the%20trajectory%20of%20NQG%2C%0Aidentifying%20emergent%20research%20trends%20and%20prospective%20developmental%20paths.%0AAccompanying%20this%20survey%20is%20a%20curated%20collection%20of%20related%20research%20papers%2C%0Adatasets%20and%20codes%2C%20systematically%20organized%20on%20Github%2C%20providing%20an%20extensive%0Areference%20for%20those%20delving%20into%20NQG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18267v2&entry.124074799=Read"},
{"title": "Edit-Your-Motion: Space-Time Diffusion Decoupling Learning for Video\n  Motion Editing", "author": "Yi Zuo and Lingling Li and Licheng Jiao and Fang Liu and Xu Liu and Wenping Ma and Shuyuan Yang and Yuwei Guo", "abstract": "  Existing diffusion-based video editing methods have achieved impressive\nresults in motion editing. Most of the existing methods focus on the motion\nalignment between the edited video and the reference video. However, these\nmethods do not constrain the background and object content of the video to\nremain unchanged, which makes it possible for users to generate unexpected\nvideos. In this paper, we propose a one-shot video motion editing method called\nEdit-Your-Motion that requires only a single text-video pair for training.\nSpecifically, we design the Detailed Prompt-Guided Learning Strategy (DPL) to\ndecouple spatio-temporal features in space-time diffusion models. DPL separates\nlearning object content and motion into two training stages. In the first\ntraining stage, we focus on learning the spatial features (the features of\nobject content) and breaking down the temporal relationships in the video\nframes by shuffling them. We further propose Recurrent-Causal Attention\n(RC-Attn) to learn the consistent content features of the object from unordered\nvideo frames. In the second training stage, we restore the temporal\nrelationship in video frames to learn the temporal feature (the features of the\nbackground and object's motion). We also adopt the Noise Constraint Loss to\nsmooth out inter-frame differences. Finally, in the inference stage, we inject\nthe content features of the source object into the editing branch through a\ntwo-branch structure (editing branch and reconstruction branch). With\nEdit-Your-Motion, users can edit the motion of objects in the source video to\ngenerate more exciting and diverse videos. Comprehensive qualitative\nexperiments, quantitative experiments and user preference studies demonstrate\nthat Edit-Your-Motion performs better than other methods.\n", "link": "http://arxiv.org/abs/2405.04496v1", "date": "2024-05-07", "relevancy": 1.9353, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7407}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6263}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edit-Your-Motion%3A%20Space-Time%20Diffusion%20Decoupling%20Learning%20for%20Video%0A%20%20Motion%20Editing&body=Title%3A%20Edit-Your-Motion%3A%20Space-Time%20Diffusion%20Decoupling%20Learning%20for%20Video%0A%20%20Motion%20Editing%0AAuthor%3A%20Yi%20Zuo%20and%20Lingling%20Li%20and%20Licheng%20Jiao%20and%20Fang%20Liu%20and%20Xu%20Liu%20and%20Wenping%20Ma%20and%20Shuyuan%20Yang%20and%20Yuwei%20Guo%0AAbstract%3A%20%20%20Existing%20diffusion-based%20video%20editing%20methods%20have%20achieved%20impressive%0Aresults%20in%20motion%20editing.%20Most%20of%20the%20existing%20methods%20focus%20on%20the%20motion%0Aalignment%20between%20the%20edited%20video%20and%20the%20reference%20video.%20However%2C%20these%0Amethods%20do%20not%20constrain%20the%20background%20and%20object%20content%20of%20the%20video%20to%0Aremain%20unchanged%2C%20which%20makes%20it%20possible%20for%20users%20to%20generate%20unexpected%0Avideos.%20In%20this%20paper%2C%20we%20propose%20a%20one-shot%20video%20motion%20editing%20method%20called%0AEdit-Your-Motion%20that%20requires%20only%20a%20single%20text-video%20pair%20for%20training.%0ASpecifically%2C%20we%20design%20the%20Detailed%20Prompt-Guided%20Learning%20Strategy%20%28DPL%29%20to%0Adecouple%20spatio-temporal%20features%20in%20space-time%20diffusion%20models.%20DPL%20separates%0Alearning%20object%20content%20and%20motion%20into%20two%20training%20stages.%20In%20the%20first%0Atraining%20stage%2C%20we%20focus%20on%20learning%20the%20spatial%20features%20%28the%20features%20of%0Aobject%20content%29%20and%20breaking%20down%20the%20temporal%20relationships%20in%20the%20video%0Aframes%20by%20shuffling%20them.%20We%20further%20propose%20Recurrent-Causal%20Attention%0A%28RC-Attn%29%20to%20learn%20the%20consistent%20content%20features%20of%20the%20object%20from%20unordered%0Avideo%20frames.%20In%20the%20second%20training%20stage%2C%20we%20restore%20the%20temporal%0Arelationship%20in%20video%20frames%20to%20learn%20the%20temporal%20feature%20%28the%20features%20of%20the%0Abackground%20and%20object%27s%20motion%29.%20We%20also%20adopt%20the%20Noise%20Constraint%20Loss%20to%0Asmooth%20out%20inter-frame%20differences.%20Finally%2C%20in%20the%20inference%20stage%2C%20we%20inject%0Athe%20content%20features%20of%20the%20source%20object%20into%20the%20editing%20branch%20through%20a%0Atwo-branch%20structure%20%28editing%20branch%20and%20reconstruction%20branch%29.%20With%0AEdit-Your-Motion%2C%20users%20can%20edit%20the%20motion%20of%20objects%20in%20the%20source%20video%20to%0Agenerate%20more%20exciting%20and%20diverse%20videos.%20Comprehensive%20qualitative%0Aexperiments%2C%20quantitative%20experiments%20and%20user%20preference%20studies%20demonstrate%0Athat%20Edit-Your-Motion%20performs%20better%20than%20other%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdit-Your-Motion%253A%2520Space-Time%2520Diffusion%2520Decoupling%2520Learning%2520for%2520Video%250A%2520%2520Motion%2520Editing%26entry.906535625%3DYi%2520Zuo%2520and%2520Lingling%2520Li%2520and%2520Licheng%2520Jiao%2520and%2520Fang%2520Liu%2520and%2520Xu%2520Liu%2520and%2520Wenping%2520Ma%2520and%2520Shuyuan%2520Yang%2520and%2520Yuwei%2520Guo%26entry.1292438233%3D%2520%2520Existing%2520diffusion-based%2520video%2520editing%2520methods%2520have%2520achieved%2520impressive%250Aresults%2520in%2520motion%2520editing.%2520Most%2520of%2520the%2520existing%2520methods%2520focus%2520on%2520the%2520motion%250Aalignment%2520between%2520the%2520edited%2520video%2520and%2520the%2520reference%2520video.%2520However%252C%2520these%250Amethods%2520do%2520not%2520constrain%2520the%2520background%2520and%2520object%2520content%2520of%2520the%2520video%2520to%250Aremain%2520unchanged%252C%2520which%2520makes%2520it%2520possible%2520for%2520users%2520to%2520generate%2520unexpected%250Avideos.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520one-shot%2520video%2520motion%2520editing%2520method%2520called%250AEdit-Your-Motion%2520that%2520requires%2520only%2520a%2520single%2520text-video%2520pair%2520for%2520training.%250ASpecifically%252C%2520we%2520design%2520the%2520Detailed%2520Prompt-Guided%2520Learning%2520Strategy%2520%2528DPL%2529%2520to%250Adecouple%2520spatio-temporal%2520features%2520in%2520space-time%2520diffusion%2520models.%2520DPL%2520separates%250Alearning%2520object%2520content%2520and%2520motion%2520into%2520two%2520training%2520stages.%2520In%2520the%2520first%250Atraining%2520stage%252C%2520we%2520focus%2520on%2520learning%2520the%2520spatial%2520features%2520%2528the%2520features%2520of%250Aobject%2520content%2529%2520and%2520breaking%2520down%2520the%2520temporal%2520relationships%2520in%2520the%2520video%250Aframes%2520by%2520shuffling%2520them.%2520We%2520further%2520propose%2520Recurrent-Causal%2520Attention%250A%2528RC-Attn%2529%2520to%2520learn%2520the%2520consistent%2520content%2520features%2520of%2520the%2520object%2520from%2520unordered%250Avideo%2520frames.%2520In%2520the%2520second%2520training%2520stage%252C%2520we%2520restore%2520the%2520temporal%250Arelationship%2520in%2520video%2520frames%2520to%2520learn%2520the%2520temporal%2520feature%2520%2528the%2520features%2520of%2520the%250Abackground%2520and%2520object%2527s%2520motion%2529.%2520We%2520also%2520adopt%2520the%2520Noise%2520Constraint%2520Loss%2520to%250Asmooth%2520out%2520inter-frame%2520differences.%2520Finally%252C%2520in%2520the%2520inference%2520stage%252C%2520we%2520inject%250Athe%2520content%2520features%2520of%2520the%2520source%2520object%2520into%2520the%2520editing%2520branch%2520through%2520a%250Atwo-branch%2520structure%2520%2528editing%2520branch%2520and%2520reconstruction%2520branch%2529.%2520With%250AEdit-Your-Motion%252C%2520users%2520can%2520edit%2520the%2520motion%2520of%2520objects%2520in%2520the%2520source%2520video%2520to%250Agenerate%2520more%2520exciting%2520and%2520diverse%2520videos.%2520Comprehensive%2520qualitative%250Aexperiments%252C%2520quantitative%2520experiments%2520and%2520user%2520preference%2520studies%2520demonstrate%250Athat%2520Edit-Your-Motion%2520performs%2520better%2520than%2520other%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edit-Your-Motion%3A%20Space-Time%20Diffusion%20Decoupling%20Learning%20for%20Video%0A%20%20Motion%20Editing&entry.906535625=Yi%20Zuo%20and%20Lingling%20Li%20and%20Licheng%20Jiao%20and%20Fang%20Liu%20and%20Xu%20Liu%20and%20Wenping%20Ma%20and%20Shuyuan%20Yang%20and%20Yuwei%20Guo&entry.1292438233=%20%20Existing%20diffusion-based%20video%20editing%20methods%20have%20achieved%20impressive%0Aresults%20in%20motion%20editing.%20Most%20of%20the%20existing%20methods%20focus%20on%20the%20motion%0Aalignment%20between%20the%20edited%20video%20and%20the%20reference%20video.%20However%2C%20these%0Amethods%20do%20not%20constrain%20the%20background%20and%20object%20content%20of%20the%20video%20to%0Aremain%20unchanged%2C%20which%20makes%20it%20possible%20for%20users%20to%20generate%20unexpected%0Avideos.%20In%20this%20paper%2C%20we%20propose%20a%20one-shot%20video%20motion%20editing%20method%20called%0AEdit-Your-Motion%20that%20requires%20only%20a%20single%20text-video%20pair%20for%20training.%0ASpecifically%2C%20we%20design%20the%20Detailed%20Prompt-Guided%20Learning%20Strategy%20%28DPL%29%20to%0Adecouple%20spatio-temporal%20features%20in%20space-time%20diffusion%20models.%20DPL%20separates%0Alearning%20object%20content%20and%20motion%20into%20two%20training%20stages.%20In%20the%20first%0Atraining%20stage%2C%20we%20focus%20on%20learning%20the%20spatial%20features%20%28the%20features%20of%0Aobject%20content%29%20and%20breaking%20down%20the%20temporal%20relationships%20in%20the%20video%0Aframes%20by%20shuffling%20them.%20We%20further%20propose%20Recurrent-Causal%20Attention%0A%28RC-Attn%29%20to%20learn%20the%20consistent%20content%20features%20of%20the%20object%20from%20unordered%0Avideo%20frames.%20In%20the%20second%20training%20stage%2C%20we%20restore%20the%20temporal%0Arelationship%20in%20video%20frames%20to%20learn%20the%20temporal%20feature%20%28the%20features%20of%20the%0Abackground%20and%20object%27s%20motion%29.%20We%20also%20adopt%20the%20Noise%20Constraint%20Loss%20to%0Asmooth%20out%20inter-frame%20differences.%20Finally%2C%20in%20the%20inference%20stage%2C%20we%20inject%0Athe%20content%20features%20of%20the%20source%20object%20into%20the%20editing%20branch%20through%20a%0Atwo-branch%20structure%20%28editing%20branch%20and%20reconstruction%20branch%29.%20With%0AEdit-Your-Motion%2C%20users%20can%20edit%20the%20motion%20of%20objects%20in%20the%20source%20video%20to%0Agenerate%20more%20exciting%20and%20diverse%20videos.%20Comprehensive%20qualitative%0Aexperiments%2C%20quantitative%20experiments%20and%20user%20preference%20studies%20demonstrate%0Athat%20Edit-Your-Motion%20performs%20better%20than%20other%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04496v1&entry.124074799=Read"},
{"title": "Exploring Correlations of Self-supervised Tasks for Graphs", "author": "Taoran Fang and Wei Zhou and Yifei Sun and Kaiqiao Han and Lvbin Ma and Yang Yang", "abstract": "  Graph self-supervised learning has sparked a research surge in training\ninformative representations without accessing any labeled data. However, our\nunderstanding of graph self-supervised learning remains limited, and the\ninherent relationships between various self-supervised tasks are still\nunexplored. Our paper aims to provide a fresh understanding of graph\nself-supervised learning based on task correlations. Specifically, we evaluate\nthe performance of the representations trained by one specific task on other\ntasks and define correlation values to quantify task correlations. Through this\nprocess, we unveil the task correlations between various self-supervised tasks\nand can measure their expressive capabilities, which are closely related to\ndownstream performance. By analyzing the correlation values between tasks\nacross various datasets, we reveal the complexity of task correlations and the\nlimitations of existing multi-task learning methods. To obtain more capable\nrepresentations, we propose Graph Task Correlation Modeling (GraphTCM) to\nillustrate the task correlations and utilize it to enhance graph\nself-supervised training. The experimental results indicate that our method\nsignificantly outperforms existing methods across various downstream tasks.\n", "link": "http://arxiv.org/abs/2405.04245v1", "date": "2024-05-07", "relevancy": 1.9327, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5395}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4576}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Correlations%20of%20Self-supervised%20Tasks%20for%20Graphs&body=Title%3A%20Exploring%20Correlations%20of%20Self-supervised%20Tasks%20for%20Graphs%0AAuthor%3A%20Taoran%20Fang%20and%20Wei%20Zhou%20and%20Yifei%20Sun%20and%20Kaiqiao%20Han%20and%20Lvbin%20Ma%20and%20Yang%20Yang%0AAbstract%3A%20%20%20Graph%20self-supervised%20learning%20has%20sparked%20a%20research%20surge%20in%20training%0Ainformative%20representations%20without%20accessing%20any%20labeled%20data.%20However%2C%20our%0Aunderstanding%20of%20graph%20self-supervised%20learning%20remains%20limited%2C%20and%20the%0Ainherent%20relationships%20between%20various%20self-supervised%20tasks%20are%20still%0Aunexplored.%20Our%20paper%20aims%20to%20provide%20a%20fresh%20understanding%20of%20graph%0Aself-supervised%20learning%20based%20on%20task%20correlations.%20Specifically%2C%20we%20evaluate%0Athe%20performance%20of%20the%20representations%20trained%20by%20one%20specific%20task%20on%20other%0Atasks%20and%20define%20correlation%20values%20to%20quantify%20task%20correlations.%20Through%20this%0Aprocess%2C%20we%20unveil%20the%20task%20correlations%20between%20various%20self-supervised%20tasks%0Aand%20can%20measure%20their%20expressive%20capabilities%2C%20which%20are%20closely%20related%20to%0Adownstream%20performance.%20By%20analyzing%20the%20correlation%20values%20between%20tasks%0Aacross%20various%20datasets%2C%20we%20reveal%20the%20complexity%20of%20task%20correlations%20and%20the%0Alimitations%20of%20existing%20multi-task%20learning%20methods.%20To%20obtain%20more%20capable%0Arepresentations%2C%20we%20propose%20Graph%20Task%20Correlation%20Modeling%20%28GraphTCM%29%20to%0Aillustrate%20the%20task%20correlations%20and%20utilize%20it%20to%20enhance%20graph%0Aself-supervised%20training.%20The%20experimental%20results%20indicate%20that%20our%20method%0Asignificantly%20outperforms%20existing%20methods%20across%20various%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04245v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Correlations%2520of%2520Self-supervised%2520Tasks%2520for%2520Graphs%26entry.906535625%3DTaoran%2520Fang%2520and%2520Wei%2520Zhou%2520and%2520Yifei%2520Sun%2520and%2520Kaiqiao%2520Han%2520and%2520Lvbin%2520Ma%2520and%2520Yang%2520Yang%26entry.1292438233%3D%2520%2520Graph%2520self-supervised%2520learning%2520has%2520sparked%2520a%2520research%2520surge%2520in%2520training%250Ainformative%2520representations%2520without%2520accessing%2520any%2520labeled%2520data.%2520However%252C%2520our%250Aunderstanding%2520of%2520graph%2520self-supervised%2520learning%2520remains%2520limited%252C%2520and%2520the%250Ainherent%2520relationships%2520between%2520various%2520self-supervised%2520tasks%2520are%2520still%250Aunexplored.%2520Our%2520paper%2520aims%2520to%2520provide%2520a%2520fresh%2520understanding%2520of%2520graph%250Aself-supervised%2520learning%2520based%2520on%2520task%2520correlations.%2520Specifically%252C%2520we%2520evaluate%250Athe%2520performance%2520of%2520the%2520representations%2520trained%2520by%2520one%2520specific%2520task%2520on%2520other%250Atasks%2520and%2520define%2520correlation%2520values%2520to%2520quantify%2520task%2520correlations.%2520Through%2520this%250Aprocess%252C%2520we%2520unveil%2520the%2520task%2520correlations%2520between%2520various%2520self-supervised%2520tasks%250Aand%2520can%2520measure%2520their%2520expressive%2520capabilities%252C%2520which%2520are%2520closely%2520related%2520to%250Adownstream%2520performance.%2520By%2520analyzing%2520the%2520correlation%2520values%2520between%2520tasks%250Aacross%2520various%2520datasets%252C%2520we%2520reveal%2520the%2520complexity%2520of%2520task%2520correlations%2520and%2520the%250Alimitations%2520of%2520existing%2520multi-task%2520learning%2520methods.%2520To%2520obtain%2520more%2520capable%250Arepresentations%252C%2520we%2520propose%2520Graph%2520Task%2520Correlation%2520Modeling%2520%2528GraphTCM%2529%2520to%250Aillustrate%2520the%2520task%2520correlations%2520and%2520utilize%2520it%2520to%2520enhance%2520graph%250Aself-supervised%2520training.%2520The%2520experimental%2520results%2520indicate%2520that%2520our%2520method%250Asignificantly%2520outperforms%2520existing%2520methods%2520across%2520various%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04245v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Correlations%20of%20Self-supervised%20Tasks%20for%20Graphs&entry.906535625=Taoran%20Fang%20and%20Wei%20Zhou%20and%20Yifei%20Sun%20and%20Kaiqiao%20Han%20and%20Lvbin%20Ma%20and%20Yang%20Yang&entry.1292438233=%20%20Graph%20self-supervised%20learning%20has%20sparked%20a%20research%20surge%20in%20training%0Ainformative%20representations%20without%20accessing%20any%20labeled%20data.%20However%2C%20our%0Aunderstanding%20of%20graph%20self-supervised%20learning%20remains%20limited%2C%20and%20the%0Ainherent%20relationships%20between%20various%20self-supervised%20tasks%20are%20still%0Aunexplored.%20Our%20paper%20aims%20to%20provide%20a%20fresh%20understanding%20of%20graph%0Aself-supervised%20learning%20based%20on%20task%20correlations.%20Specifically%2C%20we%20evaluate%0Athe%20performance%20of%20the%20representations%20trained%20by%20one%20specific%20task%20on%20other%0Atasks%20and%20define%20correlation%20values%20to%20quantify%20task%20correlations.%20Through%20this%0Aprocess%2C%20we%20unveil%20the%20task%20correlations%20between%20various%20self-supervised%20tasks%0Aand%20can%20measure%20their%20expressive%20capabilities%2C%20which%20are%20closely%20related%20to%0Adownstream%20performance.%20By%20analyzing%20the%20correlation%20values%20between%20tasks%0Aacross%20various%20datasets%2C%20we%20reveal%20the%20complexity%20of%20task%20correlations%20and%20the%0Alimitations%20of%20existing%20multi-task%20learning%20methods.%20To%20obtain%20more%20capable%0Arepresentations%2C%20we%20propose%20Graph%20Task%20Correlation%20Modeling%20%28GraphTCM%29%20to%0Aillustrate%20the%20task%20correlations%20and%20utilize%20it%20to%20enhance%20graph%0Aself-supervised%20training.%20The%20experimental%20results%20indicate%20that%20our%20method%0Asignificantly%20outperforms%20existing%20methods%20across%20various%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04245v1&entry.124074799=Read"},
{"title": "Approximate Bayesian Class-Conditional Models under Continuous\n  Representation Shift", "author": "Thomas L. Lee and Amos Storkey", "abstract": "  For models consisting of a classifier in some representation space, learning\nonline from a non-stationary data stream often necessitates changes in the\nrepresentation. So, the question arises of what is the best way to adapt the\nclassifier to shifts in representation. Current methods only slowly change the\nclassifier to representation shift, introducing noise into learning as the\nclassifier is misaligned to the representation. We propose DeepCCG, an\nempirical Bayesian approach to solve this problem. DeepCCG works by updating\nthe posterior of a class conditional Gaussian classifier such that the\nclassifier adapts in one step to representation shift. The use of a class\nconditional Gaussian classifier also enables DeepCCG to use a log conditional\nmarginal likelihood loss to update the representation. To perform the update to\nthe classifier and representation, DeepCCG maintains a fixed number of examples\nin memory and so a key part of DeepCCG is selecting what examples to store,\nchoosing the subset that minimises the KL divergence between the true posterior\nand the posterior induced by the subset. We explore the behaviour of DeepCCG in\nonline continual learning (CL), demonstrating that it performs well against a\nspectrum of online CL methods and that it reduces the change in performance due\nto representation shift.\n", "link": "http://arxiv.org/abs/2305.19076v2", "date": "2024-05-07", "relevancy": 1.9262, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5137}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4752}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Approximate%20Bayesian%20Class-Conditional%20Models%20under%20Continuous%0A%20%20Representation%20Shift&body=Title%3A%20Approximate%20Bayesian%20Class-Conditional%20Models%20under%20Continuous%0A%20%20Representation%20Shift%0AAuthor%3A%20Thomas%20L.%20Lee%20and%20Amos%20Storkey%0AAbstract%3A%20%20%20For%20models%20consisting%20of%20a%20classifier%20in%20some%20representation%20space%2C%20learning%0Aonline%20from%20a%20non-stationary%20data%20stream%20often%20necessitates%20changes%20in%20the%0Arepresentation.%20So%2C%20the%20question%20arises%20of%20what%20is%20the%20best%20way%20to%20adapt%20the%0Aclassifier%20to%20shifts%20in%20representation.%20Current%20methods%20only%20slowly%20change%20the%0Aclassifier%20to%20representation%20shift%2C%20introducing%20noise%20into%20learning%20as%20the%0Aclassifier%20is%20misaligned%20to%20the%20representation.%20We%20propose%20DeepCCG%2C%20an%0Aempirical%20Bayesian%20approach%20to%20solve%20this%20problem.%20DeepCCG%20works%20by%20updating%0Athe%20posterior%20of%20a%20class%20conditional%20Gaussian%20classifier%20such%20that%20the%0Aclassifier%20adapts%20in%20one%20step%20to%20representation%20shift.%20The%20use%20of%20a%20class%0Aconditional%20Gaussian%20classifier%20also%20enables%20DeepCCG%20to%20use%20a%20log%20conditional%0Amarginal%20likelihood%20loss%20to%20update%20the%20representation.%20To%20perform%20the%20update%20to%0Athe%20classifier%20and%20representation%2C%20DeepCCG%20maintains%20a%20fixed%20number%20of%20examples%0Ain%20memory%20and%20so%20a%20key%20part%20of%20DeepCCG%20is%20selecting%20what%20examples%20to%20store%2C%0Achoosing%20the%20subset%20that%20minimises%20the%20KL%20divergence%20between%20the%20true%20posterior%0Aand%20the%20posterior%20induced%20by%20the%20subset.%20We%20explore%20the%20behaviour%20of%20DeepCCG%20in%0Aonline%20continual%20learning%20%28CL%29%2C%20demonstrating%20that%20it%20performs%20well%20against%20a%0Aspectrum%20of%20online%20CL%20methods%20and%20that%20it%20reduces%20the%20change%20in%20performance%20due%0Ato%20representation%20shift.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.19076v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApproximate%2520Bayesian%2520Class-Conditional%2520Models%2520under%2520Continuous%250A%2520%2520Representation%2520Shift%26entry.906535625%3DThomas%2520L.%2520Lee%2520and%2520Amos%2520Storkey%26entry.1292438233%3D%2520%2520For%2520models%2520consisting%2520of%2520a%2520classifier%2520in%2520some%2520representation%2520space%252C%2520learning%250Aonline%2520from%2520a%2520non-stationary%2520data%2520stream%2520often%2520necessitates%2520changes%2520in%2520the%250Arepresentation.%2520So%252C%2520the%2520question%2520arises%2520of%2520what%2520is%2520the%2520best%2520way%2520to%2520adapt%2520the%250Aclassifier%2520to%2520shifts%2520in%2520representation.%2520Current%2520methods%2520only%2520slowly%2520change%2520the%250Aclassifier%2520to%2520representation%2520shift%252C%2520introducing%2520noise%2520into%2520learning%2520as%2520the%250Aclassifier%2520is%2520misaligned%2520to%2520the%2520representation.%2520We%2520propose%2520DeepCCG%252C%2520an%250Aempirical%2520Bayesian%2520approach%2520to%2520solve%2520this%2520problem.%2520DeepCCG%2520works%2520by%2520updating%250Athe%2520posterior%2520of%2520a%2520class%2520conditional%2520Gaussian%2520classifier%2520such%2520that%2520the%250Aclassifier%2520adapts%2520in%2520one%2520step%2520to%2520representation%2520shift.%2520The%2520use%2520of%2520a%2520class%250Aconditional%2520Gaussian%2520classifier%2520also%2520enables%2520DeepCCG%2520to%2520use%2520a%2520log%2520conditional%250Amarginal%2520likelihood%2520loss%2520to%2520update%2520the%2520representation.%2520To%2520perform%2520the%2520update%2520to%250Athe%2520classifier%2520and%2520representation%252C%2520DeepCCG%2520maintains%2520a%2520fixed%2520number%2520of%2520examples%250Ain%2520memory%2520and%2520so%2520a%2520key%2520part%2520of%2520DeepCCG%2520is%2520selecting%2520what%2520examples%2520to%2520store%252C%250Achoosing%2520the%2520subset%2520that%2520minimises%2520the%2520KL%2520divergence%2520between%2520the%2520true%2520posterior%250Aand%2520the%2520posterior%2520induced%2520by%2520the%2520subset.%2520We%2520explore%2520the%2520behaviour%2520of%2520DeepCCG%2520in%250Aonline%2520continual%2520learning%2520%2528CL%2529%252C%2520demonstrating%2520that%2520it%2520performs%2520well%2520against%2520a%250Aspectrum%2520of%2520online%2520CL%2520methods%2520and%2520that%2520it%2520reduces%2520the%2520change%2520in%2520performance%2520due%250Ato%2520representation%2520shift.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.19076v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Approximate%20Bayesian%20Class-Conditional%20Models%20under%20Continuous%0A%20%20Representation%20Shift&entry.906535625=Thomas%20L.%20Lee%20and%20Amos%20Storkey&entry.1292438233=%20%20For%20models%20consisting%20of%20a%20classifier%20in%20some%20representation%20space%2C%20learning%0Aonline%20from%20a%20non-stationary%20data%20stream%20often%20necessitates%20changes%20in%20the%0Arepresentation.%20So%2C%20the%20question%20arises%20of%20what%20is%20the%20best%20way%20to%20adapt%20the%0Aclassifier%20to%20shifts%20in%20representation.%20Current%20methods%20only%20slowly%20change%20the%0Aclassifier%20to%20representation%20shift%2C%20introducing%20noise%20into%20learning%20as%20the%0Aclassifier%20is%20misaligned%20to%20the%20representation.%20We%20propose%20DeepCCG%2C%20an%0Aempirical%20Bayesian%20approach%20to%20solve%20this%20problem.%20DeepCCG%20works%20by%20updating%0Athe%20posterior%20of%20a%20class%20conditional%20Gaussian%20classifier%20such%20that%20the%0Aclassifier%20adapts%20in%20one%20step%20to%20representation%20shift.%20The%20use%20of%20a%20class%0Aconditional%20Gaussian%20classifier%20also%20enables%20DeepCCG%20to%20use%20a%20log%20conditional%0Amarginal%20likelihood%20loss%20to%20update%20the%20representation.%20To%20perform%20the%20update%20to%0Athe%20classifier%20and%20representation%2C%20DeepCCG%20maintains%20a%20fixed%20number%20of%20examples%0Ain%20memory%20and%20so%20a%20key%20part%20of%20DeepCCG%20is%20selecting%20what%20examples%20to%20store%2C%0Achoosing%20the%20subset%20that%20minimises%20the%20KL%20divergence%20between%20the%20true%20posterior%0Aand%20the%20posterior%20induced%20by%20the%20subset.%20We%20explore%20the%20behaviour%20of%20DeepCCG%20in%0Aonline%20continual%20learning%20%28CL%29%2C%20demonstrating%20that%20it%20performs%20well%20against%20a%0Aspectrum%20of%20online%20CL%20methods%20and%20that%20it%20reduces%20the%20change%20in%20performance%20due%0Ato%20representation%20shift.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.19076v2&entry.124074799=Read"},
{"title": "Representation Learning of Daily Movement Data Using Text Encoders", "author": "Alexander Capstick and Tianyu Cui and Yu Chen and Payam Barnaghi", "abstract": "  Time-series representation learning is a key area of research for remote\nhealthcare monitoring applications. In this work, we focus on a dataset of\nrecordings of in-home activity from people living with Dementia. We design a\nrepresentation learning method based on converting activity to text strings\nthat can be encoded using a language model fine-tuned to transform data from\nthe same participants within a $30$-day window to similar embeddings in the\nvector space. This allows for clustering and vector searching over participants\nand days, and the identification of activity deviations to aid with\npersonalised delivery of care.\n", "link": "http://arxiv.org/abs/2405.04494v1", "date": "2024-05-07", "relevancy": 1.9254, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4934}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.48}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representation%20Learning%20of%20Daily%20Movement%20Data%20Using%20Text%20Encoders&body=Title%3A%20Representation%20Learning%20of%20Daily%20Movement%20Data%20Using%20Text%20Encoders%0AAuthor%3A%20Alexander%20Capstick%20and%20Tianyu%20Cui%20and%20Yu%20Chen%20and%20Payam%20Barnaghi%0AAbstract%3A%20%20%20Time-series%20representation%20learning%20is%20a%20key%20area%20of%20research%20for%20remote%0Ahealthcare%20monitoring%20applications.%20In%20this%20work%2C%20we%20focus%20on%20a%20dataset%20of%0Arecordings%20of%20in-home%20activity%20from%20people%20living%20with%20Dementia.%20We%20design%20a%0Arepresentation%20learning%20method%20based%20on%20converting%20activity%20to%20text%20strings%0Athat%20can%20be%20encoded%20using%20a%20language%20model%20fine-tuned%20to%20transform%20data%20from%0Athe%20same%20participants%20within%20a%20%2430%24-day%20window%20to%20similar%20embeddings%20in%20the%0Avector%20space.%20This%20allows%20for%20clustering%20and%20vector%20searching%20over%20participants%0Aand%20days%2C%20and%20the%20identification%20of%20activity%20deviations%20to%20aid%20with%0Apersonalised%20delivery%20of%20care.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresentation%2520Learning%2520of%2520Daily%2520Movement%2520Data%2520Using%2520Text%2520Encoders%26entry.906535625%3DAlexander%2520Capstick%2520and%2520Tianyu%2520Cui%2520and%2520Yu%2520Chen%2520and%2520Payam%2520Barnaghi%26entry.1292438233%3D%2520%2520Time-series%2520representation%2520learning%2520is%2520a%2520key%2520area%2520of%2520research%2520for%2520remote%250Ahealthcare%2520monitoring%2520applications.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520a%2520dataset%2520of%250Arecordings%2520of%2520in-home%2520activity%2520from%2520people%2520living%2520with%2520Dementia.%2520We%2520design%2520a%250Arepresentation%2520learning%2520method%2520based%2520on%2520converting%2520activity%2520to%2520text%2520strings%250Athat%2520can%2520be%2520encoded%2520using%2520a%2520language%2520model%2520fine-tuned%2520to%2520transform%2520data%2520from%250Athe%2520same%2520participants%2520within%2520a%2520%252430%2524-day%2520window%2520to%2520similar%2520embeddings%2520in%2520the%250Avector%2520space.%2520This%2520allows%2520for%2520clustering%2520and%2520vector%2520searching%2520over%2520participants%250Aand%2520days%252C%2520and%2520the%2520identification%2520of%2520activity%2520deviations%2520to%2520aid%2520with%250Apersonalised%2520delivery%2520of%2520care.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representation%20Learning%20of%20Daily%20Movement%20Data%20Using%20Text%20Encoders&entry.906535625=Alexander%20Capstick%20and%20Tianyu%20Cui%20and%20Yu%20Chen%20and%20Payam%20Barnaghi&entry.1292438233=%20%20Time-series%20representation%20learning%20is%20a%20key%20area%20of%20research%20for%20remote%0Ahealthcare%20monitoring%20applications.%20In%20this%20work%2C%20we%20focus%20on%20a%20dataset%20of%0Arecordings%20of%20in-home%20activity%20from%20people%20living%20with%20Dementia.%20We%20design%20a%0Arepresentation%20learning%20method%20based%20on%20converting%20activity%20to%20text%20strings%0Athat%20can%20be%20encoded%20using%20a%20language%20model%20fine-tuned%20to%20transform%20data%20from%0Athe%20same%20participants%20within%20a%20%2430%24-day%20window%20to%20similar%20embeddings%20in%20the%0Avector%20space.%20This%20allows%20for%20clustering%20and%20vector%20searching%20over%20participants%0Aand%20days%2C%20and%20the%20identification%20of%20activity%20deviations%20to%20aid%20with%0Apersonalised%20delivery%20of%20care.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04494v1&entry.124074799=Read"},
{"title": "TransformerFAM: Feedback attention is working memory", "author": "Dongseong Hwang and Weiran Wang and Zhuoyuan Huo and Khe Chai Sim and Pedro Moreno Mengibar", "abstract": "  While Transformers have revolutionized deep learning, their quadratic\nattention complexity hinders their ability to process infinitely long inputs.\nWe propose Feedback Attention Memory (FAM), a novel Transformer architecture\nthat leverages a feedback loop to enable the network to attend to its own\nlatent representations. This design fosters the emergence of working memory\nwithin the Transformer, allowing it to process indefinitely long sequences.\nTransformerFAM requires no additional weights, enabling seamless integration\nwith pre-trained models. Our experiments show that TransformerFAM significantly\nimproves Transformer performance on long-context tasks across various model\nsizes (1B, 8B, and 24B). These results showcase the potential to empower Large\nLanguage Models (LLMs) to process sequences of unlimited length.\n", "link": "http://arxiv.org/abs/2404.09173v3", "date": "2024-05-07", "relevancy": 1.918, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5168}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4783}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TransformerFAM%3A%20Feedback%20attention%20is%20working%20memory&body=Title%3A%20TransformerFAM%3A%20Feedback%20attention%20is%20working%20memory%0AAuthor%3A%20Dongseong%20Hwang%20and%20Weiran%20Wang%20and%20Zhuoyuan%20Huo%20and%20Khe%20Chai%20Sim%20and%20Pedro%20Moreno%20Mengibar%0AAbstract%3A%20%20%20While%20Transformers%20have%20revolutionized%20deep%20learning%2C%20their%20quadratic%0Aattention%20complexity%20hinders%20their%20ability%20to%20process%20infinitely%20long%20inputs.%0AWe%20propose%20Feedback%20Attention%20Memory%20%28FAM%29%2C%20a%20novel%20Transformer%20architecture%0Athat%20leverages%20a%20feedback%20loop%20to%20enable%20the%20network%20to%20attend%20to%20its%20own%0Alatent%20representations.%20This%20design%20fosters%20the%20emergence%20of%20working%20memory%0Awithin%20the%20Transformer%2C%20allowing%20it%20to%20process%20indefinitely%20long%20sequences.%0ATransformerFAM%20requires%20no%20additional%20weights%2C%20enabling%20seamless%20integration%0Awith%20pre-trained%20models.%20Our%20experiments%20show%20that%20TransformerFAM%20significantly%0Aimproves%20Transformer%20performance%20on%20long-context%20tasks%20across%20various%20model%0Asizes%20%281B%2C%208B%2C%20and%2024B%29.%20These%20results%20showcase%20the%20potential%20to%20empower%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20process%20sequences%20of%20unlimited%20length.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09173v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformerFAM%253A%2520Feedback%2520attention%2520is%2520working%2520memory%26entry.906535625%3DDongseong%2520Hwang%2520and%2520Weiran%2520Wang%2520and%2520Zhuoyuan%2520Huo%2520and%2520Khe%2520Chai%2520Sim%2520and%2520Pedro%2520Moreno%2520Mengibar%26entry.1292438233%3D%2520%2520While%2520Transformers%2520have%2520revolutionized%2520deep%2520learning%252C%2520their%2520quadratic%250Aattention%2520complexity%2520hinders%2520their%2520ability%2520to%2520process%2520infinitely%2520long%2520inputs.%250AWe%2520propose%2520Feedback%2520Attention%2520Memory%2520%2528FAM%2529%252C%2520a%2520novel%2520Transformer%2520architecture%250Athat%2520leverages%2520a%2520feedback%2520loop%2520to%2520enable%2520the%2520network%2520to%2520attend%2520to%2520its%2520own%250Alatent%2520representations.%2520This%2520design%2520fosters%2520the%2520emergence%2520of%2520working%2520memory%250Awithin%2520the%2520Transformer%252C%2520allowing%2520it%2520to%2520process%2520indefinitely%2520long%2520sequences.%250ATransformerFAM%2520requires%2520no%2520additional%2520weights%252C%2520enabling%2520seamless%2520integration%250Awith%2520pre-trained%2520models.%2520Our%2520experiments%2520show%2520that%2520TransformerFAM%2520significantly%250Aimproves%2520Transformer%2520performance%2520on%2520long-context%2520tasks%2520across%2520various%2520model%250Asizes%2520%25281B%252C%25208B%252C%2520and%252024B%2529.%2520These%2520results%2520showcase%2520the%2520potential%2520to%2520empower%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520to%2520process%2520sequences%2520of%2520unlimited%2520length.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09173v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TransformerFAM%3A%20Feedback%20attention%20is%20working%20memory&entry.906535625=Dongseong%20Hwang%20and%20Weiran%20Wang%20and%20Zhuoyuan%20Huo%20and%20Khe%20Chai%20Sim%20and%20Pedro%20Moreno%20Mengibar&entry.1292438233=%20%20While%20Transformers%20have%20revolutionized%20deep%20learning%2C%20their%20quadratic%0Aattention%20complexity%20hinders%20their%20ability%20to%20process%20infinitely%20long%20inputs.%0AWe%20propose%20Feedback%20Attention%20Memory%20%28FAM%29%2C%20a%20novel%20Transformer%20architecture%0Athat%20leverages%20a%20feedback%20loop%20to%20enable%20the%20network%20to%20attend%20to%20its%20own%0Alatent%20representations.%20This%20design%20fosters%20the%20emergence%20of%20working%20memory%0Awithin%20the%20Transformer%2C%20allowing%20it%20to%20process%20indefinitely%20long%20sequences.%0ATransformerFAM%20requires%20no%20additional%20weights%2C%20enabling%20seamless%20integration%0Awith%20pre-trained%20models.%20Our%20experiments%20show%20that%20TransformerFAM%20significantly%0Aimproves%20Transformer%20performance%20on%20long-context%20tasks%20across%20various%20model%0Asizes%20%281B%2C%208B%2C%20and%2024B%29.%20These%20results%20showcase%20the%20potential%20to%20empower%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20process%20sequences%20of%20unlimited%20length.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09173v3&entry.124074799=Read"},
{"title": "The Curse of Diversity in Ensemble-Based Exploration", "author": "Zhixuan Lin and Pierluca D'Oro and Evgenii Nikishin and Aaron Courville", "abstract": "  We uncover a surprising phenomenon in deep reinforcement learning: training a\ndiverse ensemble of data-sharing agents -- a well-established exploration\nstrategy -- can significantly impair the performance of the individual ensemble\nmembers when compared to standard single-agent training. Through careful\nanalysis, we attribute the degradation in performance to the low proportion of\nself-generated data in the shared training data for each ensemble member, as\nwell as the inefficiency of the individual ensemble members to learn from such\nhighly off-policy data. We thus name this phenomenon the curse of diversity. We\nfind that several intuitive solutions -- such as a larger replay buffer or a\nsmaller ensemble size -- either fail to consistently mitigate the performance\nloss or undermine the advantages of ensembling. Finally, we demonstrate the\npotential of representation learning to counteract the curse of diversity with\na novel method named Cross-Ensemble Representation Learning (CERL) in both\ndiscrete and continuous control domains. Our work offers valuable insights into\nan unexpected pitfall in ensemble-based exploration and raises important\ncaveats for future applications of similar approaches.\n", "link": "http://arxiv.org/abs/2405.04342v1", "date": "2024-05-07", "relevancy": 1.9012, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4857}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4744}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Curse%20of%20Diversity%20in%20Ensemble-Based%20Exploration&body=Title%3A%20The%20Curse%20of%20Diversity%20in%20Ensemble-Based%20Exploration%0AAuthor%3A%20Zhixuan%20Lin%20and%20Pierluca%20D%27Oro%20and%20Evgenii%20Nikishin%20and%20Aaron%20Courville%0AAbstract%3A%20%20%20We%20uncover%20a%20surprising%20phenomenon%20in%20deep%20reinforcement%20learning%3A%20training%20a%0Adiverse%20ensemble%20of%20data-sharing%20agents%20--%20a%20well-established%20exploration%0Astrategy%20--%20can%20significantly%20impair%20the%20performance%20of%20the%20individual%20ensemble%0Amembers%20when%20compared%20to%20standard%20single-agent%20training.%20Through%20careful%0Aanalysis%2C%20we%20attribute%20the%20degradation%20in%20performance%20to%20the%20low%20proportion%20of%0Aself-generated%20data%20in%20the%20shared%20training%20data%20for%20each%20ensemble%20member%2C%20as%0Awell%20as%20the%20inefficiency%20of%20the%20individual%20ensemble%20members%20to%20learn%20from%20such%0Ahighly%20off-policy%20data.%20We%20thus%20name%20this%20phenomenon%20the%20curse%20of%20diversity.%20We%0Afind%20that%20several%20intuitive%20solutions%20--%20such%20as%20a%20larger%20replay%20buffer%20or%20a%0Asmaller%20ensemble%20size%20--%20either%20fail%20to%20consistently%20mitigate%20the%20performance%0Aloss%20or%20undermine%20the%20advantages%20of%20ensembling.%20Finally%2C%20we%20demonstrate%20the%0Apotential%20of%20representation%20learning%20to%20counteract%20the%20curse%20of%20diversity%20with%0Aa%20novel%20method%20named%20Cross-Ensemble%20Representation%20Learning%20%28CERL%29%20in%20both%0Adiscrete%20and%20continuous%20control%20domains.%20Our%20work%20offers%20valuable%20insights%20into%0Aan%20unexpected%20pitfall%20in%20ensemble-based%20exploration%20and%20raises%20important%0Acaveats%20for%20future%20applications%20of%20similar%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04342v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Curse%2520of%2520Diversity%2520in%2520Ensemble-Based%2520Exploration%26entry.906535625%3DZhixuan%2520Lin%2520and%2520Pierluca%2520D%2527Oro%2520and%2520Evgenii%2520Nikishin%2520and%2520Aaron%2520Courville%26entry.1292438233%3D%2520%2520We%2520uncover%2520a%2520surprising%2520phenomenon%2520in%2520deep%2520reinforcement%2520learning%253A%2520training%2520a%250Adiverse%2520ensemble%2520of%2520data-sharing%2520agents%2520--%2520a%2520well-established%2520exploration%250Astrategy%2520--%2520can%2520significantly%2520impair%2520the%2520performance%2520of%2520the%2520individual%2520ensemble%250Amembers%2520when%2520compared%2520to%2520standard%2520single-agent%2520training.%2520Through%2520careful%250Aanalysis%252C%2520we%2520attribute%2520the%2520degradation%2520in%2520performance%2520to%2520the%2520low%2520proportion%2520of%250Aself-generated%2520data%2520in%2520the%2520shared%2520training%2520data%2520for%2520each%2520ensemble%2520member%252C%2520as%250Awell%2520as%2520the%2520inefficiency%2520of%2520the%2520individual%2520ensemble%2520members%2520to%2520learn%2520from%2520such%250Ahighly%2520off-policy%2520data.%2520We%2520thus%2520name%2520this%2520phenomenon%2520the%2520curse%2520of%2520diversity.%2520We%250Afind%2520that%2520several%2520intuitive%2520solutions%2520--%2520such%2520as%2520a%2520larger%2520replay%2520buffer%2520or%2520a%250Asmaller%2520ensemble%2520size%2520--%2520either%2520fail%2520to%2520consistently%2520mitigate%2520the%2520performance%250Aloss%2520or%2520undermine%2520the%2520advantages%2520of%2520ensembling.%2520Finally%252C%2520we%2520demonstrate%2520the%250Apotential%2520of%2520representation%2520learning%2520to%2520counteract%2520the%2520curse%2520of%2520diversity%2520with%250Aa%2520novel%2520method%2520named%2520Cross-Ensemble%2520Representation%2520Learning%2520%2528CERL%2529%2520in%2520both%250Adiscrete%2520and%2520continuous%2520control%2520domains.%2520Our%2520work%2520offers%2520valuable%2520insights%2520into%250Aan%2520unexpected%2520pitfall%2520in%2520ensemble-based%2520exploration%2520and%2520raises%2520important%250Acaveats%2520for%2520future%2520applications%2520of%2520similar%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04342v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Curse%20of%20Diversity%20in%20Ensemble-Based%20Exploration&entry.906535625=Zhixuan%20Lin%20and%20Pierluca%20D%27Oro%20and%20Evgenii%20Nikishin%20and%20Aaron%20Courville&entry.1292438233=%20%20We%20uncover%20a%20surprising%20phenomenon%20in%20deep%20reinforcement%20learning%3A%20training%20a%0Adiverse%20ensemble%20of%20data-sharing%20agents%20--%20a%20well-established%20exploration%0Astrategy%20--%20can%20significantly%20impair%20the%20performance%20of%20the%20individual%20ensemble%0Amembers%20when%20compared%20to%20standard%20single-agent%20training.%20Through%20careful%0Aanalysis%2C%20we%20attribute%20the%20degradation%20in%20performance%20to%20the%20low%20proportion%20of%0Aself-generated%20data%20in%20the%20shared%20training%20data%20for%20each%20ensemble%20member%2C%20as%0Awell%20as%20the%20inefficiency%20of%20the%20individual%20ensemble%20members%20to%20learn%20from%20such%0Ahighly%20off-policy%20data.%20We%20thus%20name%20this%20phenomenon%20the%20curse%20of%20diversity.%20We%0Afind%20that%20several%20intuitive%20solutions%20--%20such%20as%20a%20larger%20replay%20buffer%20or%20a%0Asmaller%20ensemble%20size%20--%20either%20fail%20to%20consistently%20mitigate%20the%20performance%0Aloss%20or%20undermine%20the%20advantages%20of%20ensembling.%20Finally%2C%20we%20demonstrate%20the%0Apotential%20of%20representation%20learning%20to%20counteract%20the%20curse%20of%20diversity%20with%0Aa%20novel%20method%20named%20Cross-Ensemble%20Representation%20Learning%20%28CERL%29%20in%20both%0Adiscrete%20and%20continuous%20control%20domains.%20Our%20work%20offers%20valuable%20insights%20into%0Aan%20unexpected%20pitfall%20in%20ensemble-based%20exploration%20and%20raises%20important%0Acaveats%20for%20future%20applications%20of%20similar%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04342v1&entry.124074799=Read"},
{"title": "On Using Admissible Bounds for Learning Forward Search Heuristics", "author": "Carlos N\u00fa\u00f1ez-Molina and Masataro Asai and Pablo Mesejo and Juan Fern\u00e1ndez-Olivares", "abstract": "  In recent years, there has been growing interest in utilizing modern machine\nlearning techniques to learn heuristic functions for forward search algorithms.\nDespite this, there has been little theoretical understanding of what they\nshould learn, how to train them, and why we do so. This lack of understanding\nhas resulted in the adoption of diverse training targets (suboptimal vs optimal\ncosts vs admissible heuristics) and loss functions (e.g., square vs absolute\nerrors) in the literature. In this work, we focus on how to effectively utilize\nthe information provided by admissible heuristics in heuristic learning. We\nargue that learning from poly-time admissible heuristics by minimizing mean\nsquare errors (MSE) is not the correct approach, since its result is merely a\nnoisy, inadmissible copy of an efficiently computable heuristic. Instead, we\npropose to model the learned heuristic as a truncated gaussian, where\nadmissible heuristics are used not as training targets but as lower bounds of\nthis distribution. This results in a different loss function from the MSE\ncommonly employed in the literature, which implicitly models the learned\nheuristic as a gaussian distribution. We conduct experiments where both MSE and\nour novel loss function are applied to learning a heuristic from optimal plan\ncosts. Results show that our proposed method converges faster during training\nand yields better heuristics.\n", "link": "http://arxiv.org/abs/2308.11905v3", "date": "2024-05-07", "relevancy": 1.9007, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4944}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4871}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Using%20Admissible%20Bounds%20for%20Learning%20Forward%20Search%20Heuristics&body=Title%3A%20On%20Using%20Admissible%20Bounds%20for%20Learning%20Forward%20Search%20Heuristics%0AAuthor%3A%20Carlos%20N%C3%BA%C3%B1ez-Molina%20and%20Masataro%20Asai%20and%20Pablo%20Mesejo%20and%20Juan%20Fern%C3%A1ndez-Olivares%0AAbstract%3A%20%20%20In%20recent%20years%2C%20there%20has%20been%20growing%20interest%20in%20utilizing%20modern%20machine%0Alearning%20techniques%20to%20learn%20heuristic%20functions%20for%20forward%20search%20algorithms.%0ADespite%20this%2C%20there%20has%20been%20little%20theoretical%20understanding%20of%20what%20they%0Ashould%20learn%2C%20how%20to%20train%20them%2C%20and%20why%20we%20do%20so.%20This%20lack%20of%20understanding%0Ahas%20resulted%20in%20the%20adoption%20of%20diverse%20training%20targets%20%28suboptimal%20vs%20optimal%0Acosts%20vs%20admissible%20heuristics%29%20and%20loss%20functions%20%28e.g.%2C%20square%20vs%20absolute%0Aerrors%29%20in%20the%20literature.%20In%20this%20work%2C%20we%20focus%20on%20how%20to%20effectively%20utilize%0Athe%20information%20provided%20by%20admissible%20heuristics%20in%20heuristic%20learning.%20We%0Aargue%20that%20learning%20from%20poly-time%20admissible%20heuristics%20by%20minimizing%20mean%0Asquare%20errors%20%28MSE%29%20is%20not%20the%20correct%20approach%2C%20since%20its%20result%20is%20merely%20a%0Anoisy%2C%20inadmissible%20copy%20of%20an%20efficiently%20computable%20heuristic.%20Instead%2C%20we%0Apropose%20to%20model%20the%20learned%20heuristic%20as%20a%20truncated%20gaussian%2C%20where%0Aadmissible%20heuristics%20are%20used%20not%20as%20training%20targets%20but%20as%20lower%20bounds%20of%0Athis%20distribution.%20This%20results%20in%20a%20different%20loss%20function%20from%20the%20MSE%0Acommonly%20employed%20in%20the%20literature%2C%20which%20implicitly%20models%20the%20learned%0Aheuristic%20as%20a%20gaussian%20distribution.%20We%20conduct%20experiments%20where%20both%20MSE%20and%0Aour%20novel%20loss%20function%20are%20applied%20to%20learning%20a%20heuristic%20from%20optimal%20plan%0Acosts.%20Results%20show%20that%20our%20proposed%20method%20converges%20faster%20during%20training%0Aand%20yields%20better%20heuristics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.11905v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Using%2520Admissible%2520Bounds%2520for%2520Learning%2520Forward%2520Search%2520Heuristics%26entry.906535625%3DCarlos%2520N%25C3%25BA%25C3%25B1ez-Molina%2520and%2520Masataro%2520Asai%2520and%2520Pablo%2520Mesejo%2520and%2520Juan%2520Fern%25C3%25A1ndez-Olivares%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520there%2520has%2520been%2520growing%2520interest%2520in%2520utilizing%2520modern%2520machine%250Alearning%2520techniques%2520to%2520learn%2520heuristic%2520functions%2520for%2520forward%2520search%2520algorithms.%250ADespite%2520this%252C%2520there%2520has%2520been%2520little%2520theoretical%2520understanding%2520of%2520what%2520they%250Ashould%2520learn%252C%2520how%2520to%2520train%2520them%252C%2520and%2520why%2520we%2520do%2520so.%2520This%2520lack%2520of%2520understanding%250Ahas%2520resulted%2520in%2520the%2520adoption%2520of%2520diverse%2520training%2520targets%2520%2528suboptimal%2520vs%2520optimal%250Acosts%2520vs%2520admissible%2520heuristics%2529%2520and%2520loss%2520functions%2520%2528e.g.%252C%2520square%2520vs%2520absolute%250Aerrors%2529%2520in%2520the%2520literature.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520how%2520to%2520effectively%2520utilize%250Athe%2520information%2520provided%2520by%2520admissible%2520heuristics%2520in%2520heuristic%2520learning.%2520We%250Aargue%2520that%2520learning%2520from%2520poly-time%2520admissible%2520heuristics%2520by%2520minimizing%2520mean%250Asquare%2520errors%2520%2528MSE%2529%2520is%2520not%2520the%2520correct%2520approach%252C%2520since%2520its%2520result%2520is%2520merely%2520a%250Anoisy%252C%2520inadmissible%2520copy%2520of%2520an%2520efficiently%2520computable%2520heuristic.%2520Instead%252C%2520we%250Apropose%2520to%2520model%2520the%2520learned%2520heuristic%2520as%2520a%2520truncated%2520gaussian%252C%2520where%250Aadmissible%2520heuristics%2520are%2520used%2520not%2520as%2520training%2520targets%2520but%2520as%2520lower%2520bounds%2520of%250Athis%2520distribution.%2520This%2520results%2520in%2520a%2520different%2520loss%2520function%2520from%2520the%2520MSE%250Acommonly%2520employed%2520in%2520the%2520literature%252C%2520which%2520implicitly%2520models%2520the%2520learned%250Aheuristic%2520as%2520a%2520gaussian%2520distribution.%2520We%2520conduct%2520experiments%2520where%2520both%2520MSE%2520and%250Aour%2520novel%2520loss%2520function%2520are%2520applied%2520to%2520learning%2520a%2520heuristic%2520from%2520optimal%2520plan%250Acosts.%2520Results%2520show%2520that%2520our%2520proposed%2520method%2520converges%2520faster%2520during%2520training%250Aand%2520yields%2520better%2520heuristics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.11905v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Using%20Admissible%20Bounds%20for%20Learning%20Forward%20Search%20Heuristics&entry.906535625=Carlos%20N%C3%BA%C3%B1ez-Molina%20and%20Masataro%20Asai%20and%20Pablo%20Mesejo%20and%20Juan%20Fern%C3%A1ndez-Olivares&entry.1292438233=%20%20In%20recent%20years%2C%20there%20has%20been%20growing%20interest%20in%20utilizing%20modern%20machine%0Alearning%20techniques%20to%20learn%20heuristic%20functions%20for%20forward%20search%20algorithms.%0ADespite%20this%2C%20there%20has%20been%20little%20theoretical%20understanding%20of%20what%20they%0Ashould%20learn%2C%20how%20to%20train%20them%2C%20and%20why%20we%20do%20so.%20This%20lack%20of%20understanding%0Ahas%20resulted%20in%20the%20adoption%20of%20diverse%20training%20targets%20%28suboptimal%20vs%20optimal%0Acosts%20vs%20admissible%20heuristics%29%20and%20loss%20functions%20%28e.g.%2C%20square%20vs%20absolute%0Aerrors%29%20in%20the%20literature.%20In%20this%20work%2C%20we%20focus%20on%20how%20to%20effectively%20utilize%0Athe%20information%20provided%20by%20admissible%20heuristics%20in%20heuristic%20learning.%20We%0Aargue%20that%20learning%20from%20poly-time%20admissible%20heuristics%20by%20minimizing%20mean%0Asquare%20errors%20%28MSE%29%20is%20not%20the%20correct%20approach%2C%20since%20its%20result%20is%20merely%20a%0Anoisy%2C%20inadmissible%20copy%20of%20an%20efficiently%20computable%20heuristic.%20Instead%2C%20we%0Apropose%20to%20model%20the%20learned%20heuristic%20as%20a%20truncated%20gaussian%2C%20where%0Aadmissible%20heuristics%20are%20used%20not%20as%20training%20targets%20but%20as%20lower%20bounds%20of%0Athis%20distribution.%20This%20results%20in%20a%20different%20loss%20function%20from%20the%20MSE%0Acommonly%20employed%20in%20the%20literature%2C%20which%20implicitly%20models%20the%20learned%0Aheuristic%20as%20a%20gaussian%20distribution.%20We%20conduct%20experiments%20where%20both%20MSE%20and%0Aour%20novel%20loss%20function%20are%20applied%20to%20learning%20a%20heuristic%20from%20optimal%20plan%0Acosts.%20Results%20show%20that%20our%20proposed%20method%20converges%20faster%20during%20training%0Aand%20yields%20better%20heuristics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.11905v3&entry.124074799=Read"},
{"title": "NL2Plan: Robust LLM-Driven Planning from Minimal Text Descriptions", "author": "Elliot Gestrin and Marco Kuhlmann and Jendrik Seipp", "abstract": "  Today's classical planners are powerful, but modeling input tasks in formats\nsuch as PDDL is tedious and error-prone. In contrast, planning with Large\nLanguage Models (LLMs) allows for almost any input text, but offers no\nguarantees on plan quality or even soundness. In an attempt to merge the best\nof these two approaches, some work has begun to use LLMs to automate parts of\nthe PDDL creation process. However, these methods still require various degrees\nof expert input. We present NL2Plan, the first domain-agnostic offline\nLLM-driven planning system. NL2Plan uses an LLM to incrementally extract the\nnecessary information from a short text prompt before creating a complete PDDL\ndescription of both the domain and the problem, which is finally solved by a\nclassical planner. We evaluate NL2Plan on four planning domains and find that\nit solves 10 out of 15 tasks - a clear improvement over a plain\nchain-of-thought reasoning LLM approach, which only solves 2 tasks. Moreover,\nin two out of the five failure cases, instead of returning an invalid plan,\nNL2Plan reports that it failed to solve the task. In addition to using NL2Plan\nin end-to-end mode, users can inspect and correct all of its intermediate\nresults, such as the PDDL representation, increasing explainability and making\nit an assistive tool for PDDL creation.\n", "link": "http://arxiv.org/abs/2405.04215v1", "date": "2024-05-07", "relevancy": 1.8922, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5163}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4736}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NL2Plan%3A%20Robust%20LLM-Driven%20Planning%20from%20Minimal%20Text%20Descriptions&body=Title%3A%20NL2Plan%3A%20Robust%20LLM-Driven%20Planning%20from%20Minimal%20Text%20Descriptions%0AAuthor%3A%20Elliot%20Gestrin%20and%20Marco%20Kuhlmann%20and%20Jendrik%20Seipp%0AAbstract%3A%20%20%20Today%27s%20classical%20planners%20are%20powerful%2C%20but%20modeling%20input%20tasks%20in%20formats%0Asuch%20as%20PDDL%20is%20tedious%20and%20error-prone.%20In%20contrast%2C%20planning%20with%20Large%0ALanguage%20Models%20%28LLMs%29%20allows%20for%20almost%20any%20input%20text%2C%20but%20offers%20no%0Aguarantees%20on%20plan%20quality%20or%20even%20soundness.%20In%20an%20attempt%20to%20merge%20the%20best%0Aof%20these%20two%20approaches%2C%20some%20work%20has%20begun%20to%20use%20LLMs%20to%20automate%20parts%20of%0Athe%20PDDL%20creation%20process.%20However%2C%20these%20methods%20still%20require%20various%20degrees%0Aof%20expert%20input.%20We%20present%20NL2Plan%2C%20the%20first%20domain-agnostic%20offline%0ALLM-driven%20planning%20system.%20NL2Plan%20uses%20an%20LLM%20to%20incrementally%20extract%20the%0Anecessary%20information%20from%20a%20short%20text%20prompt%20before%20creating%20a%20complete%20PDDL%0Adescription%20of%20both%20the%20domain%20and%20the%20problem%2C%20which%20is%20finally%20solved%20by%20a%0Aclassical%20planner.%20We%20evaluate%20NL2Plan%20on%20four%20planning%20domains%20and%20find%20that%0Ait%20solves%2010%20out%20of%2015%20tasks%20-%20a%20clear%20improvement%20over%20a%20plain%0Achain-of-thought%20reasoning%20LLM%20approach%2C%20which%20only%20solves%202%20tasks.%20Moreover%2C%0Ain%20two%20out%20of%20the%20five%20failure%20cases%2C%20instead%20of%20returning%20an%20invalid%20plan%2C%0ANL2Plan%20reports%20that%20it%20failed%20to%20solve%20the%20task.%20In%20addition%20to%20using%20NL2Plan%0Ain%20end-to-end%20mode%2C%20users%20can%20inspect%20and%20correct%20all%20of%20its%20intermediate%0Aresults%2C%20such%20as%20the%20PDDL%20representation%2C%20increasing%20explainability%20and%20making%0Ait%20an%20assistive%20tool%20for%20PDDL%20creation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04215v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNL2Plan%253A%2520Robust%2520LLM-Driven%2520Planning%2520from%2520Minimal%2520Text%2520Descriptions%26entry.906535625%3DElliot%2520Gestrin%2520and%2520Marco%2520Kuhlmann%2520and%2520Jendrik%2520Seipp%26entry.1292438233%3D%2520%2520Today%2527s%2520classical%2520planners%2520are%2520powerful%252C%2520but%2520modeling%2520input%2520tasks%2520in%2520formats%250Asuch%2520as%2520PDDL%2520is%2520tedious%2520and%2520error-prone.%2520In%2520contrast%252C%2520planning%2520with%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520allows%2520for%2520almost%2520any%2520input%2520text%252C%2520but%2520offers%2520no%250Aguarantees%2520on%2520plan%2520quality%2520or%2520even%2520soundness.%2520In%2520an%2520attempt%2520to%2520merge%2520the%2520best%250Aof%2520these%2520two%2520approaches%252C%2520some%2520work%2520has%2520begun%2520to%2520use%2520LLMs%2520to%2520automate%2520parts%2520of%250Athe%2520PDDL%2520creation%2520process.%2520However%252C%2520these%2520methods%2520still%2520require%2520various%2520degrees%250Aof%2520expert%2520input.%2520We%2520present%2520NL2Plan%252C%2520the%2520first%2520domain-agnostic%2520offline%250ALLM-driven%2520planning%2520system.%2520NL2Plan%2520uses%2520an%2520LLM%2520to%2520incrementally%2520extract%2520the%250Anecessary%2520information%2520from%2520a%2520short%2520text%2520prompt%2520before%2520creating%2520a%2520complete%2520PDDL%250Adescription%2520of%2520both%2520the%2520domain%2520and%2520the%2520problem%252C%2520which%2520is%2520finally%2520solved%2520by%2520a%250Aclassical%2520planner.%2520We%2520evaluate%2520NL2Plan%2520on%2520four%2520planning%2520domains%2520and%2520find%2520that%250Ait%2520solves%252010%2520out%2520of%252015%2520tasks%2520-%2520a%2520clear%2520improvement%2520over%2520a%2520plain%250Achain-of-thought%2520reasoning%2520LLM%2520approach%252C%2520which%2520only%2520solves%25202%2520tasks.%2520Moreover%252C%250Ain%2520two%2520out%2520of%2520the%2520five%2520failure%2520cases%252C%2520instead%2520of%2520returning%2520an%2520invalid%2520plan%252C%250ANL2Plan%2520reports%2520that%2520it%2520failed%2520to%2520solve%2520the%2520task.%2520In%2520addition%2520to%2520using%2520NL2Plan%250Ain%2520end-to-end%2520mode%252C%2520users%2520can%2520inspect%2520and%2520correct%2520all%2520of%2520its%2520intermediate%250Aresults%252C%2520such%2520as%2520the%2520PDDL%2520representation%252C%2520increasing%2520explainability%2520and%2520making%250Ait%2520an%2520assistive%2520tool%2520for%2520PDDL%2520creation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04215v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NL2Plan%3A%20Robust%20LLM-Driven%20Planning%20from%20Minimal%20Text%20Descriptions&entry.906535625=Elliot%20Gestrin%20and%20Marco%20Kuhlmann%20and%20Jendrik%20Seipp&entry.1292438233=%20%20Today%27s%20classical%20planners%20are%20powerful%2C%20but%20modeling%20input%20tasks%20in%20formats%0Asuch%20as%20PDDL%20is%20tedious%20and%20error-prone.%20In%20contrast%2C%20planning%20with%20Large%0ALanguage%20Models%20%28LLMs%29%20allows%20for%20almost%20any%20input%20text%2C%20but%20offers%20no%0Aguarantees%20on%20plan%20quality%20or%20even%20soundness.%20In%20an%20attempt%20to%20merge%20the%20best%0Aof%20these%20two%20approaches%2C%20some%20work%20has%20begun%20to%20use%20LLMs%20to%20automate%20parts%20of%0Athe%20PDDL%20creation%20process.%20However%2C%20these%20methods%20still%20require%20various%20degrees%0Aof%20expert%20input.%20We%20present%20NL2Plan%2C%20the%20first%20domain-agnostic%20offline%0ALLM-driven%20planning%20system.%20NL2Plan%20uses%20an%20LLM%20to%20incrementally%20extract%20the%0Anecessary%20information%20from%20a%20short%20text%20prompt%20before%20creating%20a%20complete%20PDDL%0Adescription%20of%20both%20the%20domain%20and%20the%20problem%2C%20which%20is%20finally%20solved%20by%20a%0Aclassical%20planner.%20We%20evaluate%20NL2Plan%20on%20four%20planning%20domains%20and%20find%20that%0Ait%20solves%2010%20out%20of%2015%20tasks%20-%20a%20clear%20improvement%20over%20a%20plain%0Achain-of-thought%20reasoning%20LLM%20approach%2C%20which%20only%20solves%202%20tasks.%20Moreover%2C%0Ain%20two%20out%20of%20the%20five%20failure%20cases%2C%20instead%20of%20returning%20an%20invalid%20plan%2C%0ANL2Plan%20reports%20that%20it%20failed%20to%20solve%20the%20task.%20In%20addition%20to%20using%20NL2Plan%0Ain%20end-to-end%20mode%2C%20users%20can%20inspect%20and%20correct%20all%20of%20its%20intermediate%0Aresults%2C%20such%20as%20the%20PDDL%20representation%2C%20increasing%20explainability%20and%20making%0Ait%20an%20assistive%20tool%20for%20PDDL%20creation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04215v1&entry.124074799=Read"},
{"title": "Molecular Identification via Molecular Fingerprint extraction from\n  Atomic Force Microscopy images", "author": "Manuel Gonz\u00e1lez Lastre and Pablo Pou and Miguel Wiche and Daniel Ebeling and Andre Schirmeisen and Rub\u00e9n P\u00e9rez", "abstract": "  Non--Contact Atomic Force Microscopy with CO--functionalized metal tips\n(referred to as HR-AFM) provides access to the internal structure of individual\nmolecules adsorbed on a surface with totally unprecedented resolution. Previous\nworks have shown that deep learning (DL) models can retrieve the chemical and\nstructural information encoded in a 3D stack of constant-height HR--AFM images,\nleading to molecular identification. In this work, we overcome their\nlimitations by using a well-established description of the molecular structure\nin terms of topological fingerprints, the 1024--bit Extended Connectivity\nChemical Fingerprints of radius 2 (ECFP4), that were developed for substructure\nand similarity searching. ECFPs provide local structural information of the\nmolecule, each bit correlating with a particular substructure within the\nmolecule. Our DL model is able to extract this optimized structural descriptor\nfrom the 3D HR--AFM stacks and use it, through virtual screening, to identify\nmolecules from their predicted ECFP4 with a retrieval accuracy on theoretical\nimages of 95.4\\%. Furthermore, this approach, unlike previous DL models,\nassigns a confidence score, the Tanimoto similarity, to each of the candidate\nmolecules, thus providing information on the reliability of the identification.\n  By construction, the number of times a certain substructure is present in the\nmolecule is lost during the hashing process, necessary to make them useful for\nmachine learning applications. We show that it is possible to complement the\nfingerprint-based virtual screening with global information provided by another\nDL model that predicts from the same HR--AFM stacks the chemical formula,\nboosting the identification accuracy up to a 97.6\\%. Finally, we perform a\nlimited test with experimental images, obtaining promising results towards the\napplication of this pipeline under real conditions\n", "link": "http://arxiv.org/abs/2405.04321v1", "date": "2024-05-07", "relevancy": 1.8919, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4783}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4705}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Molecular%20Identification%20via%20Molecular%20Fingerprint%20extraction%20from%0A%20%20Atomic%20Force%20Microscopy%20images&body=Title%3A%20Molecular%20Identification%20via%20Molecular%20Fingerprint%20extraction%20from%0A%20%20Atomic%20Force%20Microscopy%20images%0AAuthor%3A%20Manuel%20Gonz%C3%A1lez%20Lastre%20and%20Pablo%20Pou%20and%20Miguel%20Wiche%20and%20Daniel%20Ebeling%20and%20Andre%20Schirmeisen%20and%20Rub%C3%A9n%20P%C3%A9rez%0AAbstract%3A%20%20%20Non--Contact%20Atomic%20Force%20Microscopy%20with%20CO--functionalized%20metal%20tips%0A%28referred%20to%20as%20HR-AFM%29%20provides%20access%20to%20the%20internal%20structure%20of%20individual%0Amolecules%20adsorbed%20on%20a%20surface%20with%20totally%20unprecedented%20resolution.%20Previous%0Aworks%20have%20shown%20that%20deep%20learning%20%28DL%29%20models%20can%20retrieve%20the%20chemical%20and%0Astructural%20information%20encoded%20in%20a%203D%20stack%20of%20constant-height%20HR--AFM%20images%2C%0Aleading%20to%20molecular%20identification.%20In%20this%20work%2C%20we%20overcome%20their%0Alimitations%20by%20using%20a%20well-established%20description%20of%20the%20molecular%20structure%0Ain%20terms%20of%20topological%20fingerprints%2C%20the%201024--bit%20Extended%20Connectivity%0AChemical%20Fingerprints%20of%20radius%202%20%28ECFP4%29%2C%20that%20were%20developed%20for%20substructure%0Aand%20similarity%20searching.%20ECFPs%20provide%20local%20structural%20information%20of%20the%0Amolecule%2C%20each%20bit%20correlating%20with%20a%20particular%20substructure%20within%20the%0Amolecule.%20Our%20DL%20model%20is%20able%20to%20extract%20this%20optimized%20structural%20descriptor%0Afrom%20the%203D%20HR--AFM%20stacks%20and%20use%20it%2C%20through%20virtual%20screening%2C%20to%20identify%0Amolecules%20from%20their%20predicted%20ECFP4%20with%20a%20retrieval%20accuracy%20on%20theoretical%0Aimages%20of%2095.4%5C%25.%20Furthermore%2C%20this%20approach%2C%20unlike%20previous%20DL%20models%2C%0Aassigns%20a%20confidence%20score%2C%20the%20Tanimoto%20similarity%2C%20to%20each%20of%20the%20candidate%0Amolecules%2C%20thus%20providing%20information%20on%20the%20reliability%20of%20the%20identification.%0A%20%20By%20construction%2C%20the%20number%20of%20times%20a%20certain%20substructure%20is%20present%20in%20the%0Amolecule%20is%20lost%20during%20the%20hashing%20process%2C%20necessary%20to%20make%20them%20useful%20for%0Amachine%20learning%20applications.%20We%20show%20that%20it%20is%20possible%20to%20complement%20the%0Afingerprint-based%20virtual%20screening%20with%20global%20information%20provided%20by%20another%0ADL%20model%20that%20predicts%20from%20the%20same%20HR--AFM%20stacks%20the%20chemical%20formula%2C%0Aboosting%20the%20identification%20accuracy%20up%20to%20a%2097.6%5C%25.%20Finally%2C%20we%20perform%20a%0Alimited%20test%20with%20experimental%20images%2C%20obtaining%20promising%20results%20towards%20the%0Aapplication%20of%20this%20pipeline%20under%20real%20conditions%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04321v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMolecular%2520Identification%2520via%2520Molecular%2520Fingerprint%2520extraction%2520from%250A%2520%2520Atomic%2520Force%2520Microscopy%2520images%26entry.906535625%3DManuel%2520Gonz%25C3%25A1lez%2520Lastre%2520and%2520Pablo%2520Pou%2520and%2520Miguel%2520Wiche%2520and%2520Daniel%2520Ebeling%2520and%2520Andre%2520Schirmeisen%2520and%2520Rub%25C3%25A9n%2520P%25C3%25A9rez%26entry.1292438233%3D%2520%2520Non--Contact%2520Atomic%2520Force%2520Microscopy%2520with%2520CO--functionalized%2520metal%2520tips%250A%2528referred%2520to%2520as%2520HR-AFM%2529%2520provides%2520access%2520to%2520the%2520internal%2520structure%2520of%2520individual%250Amolecules%2520adsorbed%2520on%2520a%2520surface%2520with%2520totally%2520unprecedented%2520resolution.%2520Previous%250Aworks%2520have%2520shown%2520that%2520deep%2520learning%2520%2528DL%2529%2520models%2520can%2520retrieve%2520the%2520chemical%2520and%250Astructural%2520information%2520encoded%2520in%2520a%25203D%2520stack%2520of%2520constant-height%2520HR--AFM%2520images%252C%250Aleading%2520to%2520molecular%2520identification.%2520In%2520this%2520work%252C%2520we%2520overcome%2520their%250Alimitations%2520by%2520using%2520a%2520well-established%2520description%2520of%2520the%2520molecular%2520structure%250Ain%2520terms%2520of%2520topological%2520fingerprints%252C%2520the%25201024--bit%2520Extended%2520Connectivity%250AChemical%2520Fingerprints%2520of%2520radius%25202%2520%2528ECFP4%2529%252C%2520that%2520were%2520developed%2520for%2520substructure%250Aand%2520similarity%2520searching.%2520ECFPs%2520provide%2520local%2520structural%2520information%2520of%2520the%250Amolecule%252C%2520each%2520bit%2520correlating%2520with%2520a%2520particular%2520substructure%2520within%2520the%250Amolecule.%2520Our%2520DL%2520model%2520is%2520able%2520to%2520extract%2520this%2520optimized%2520structural%2520descriptor%250Afrom%2520the%25203D%2520HR--AFM%2520stacks%2520and%2520use%2520it%252C%2520through%2520virtual%2520screening%252C%2520to%2520identify%250Amolecules%2520from%2520their%2520predicted%2520ECFP4%2520with%2520a%2520retrieval%2520accuracy%2520on%2520theoretical%250Aimages%2520of%252095.4%255C%2525.%2520Furthermore%252C%2520this%2520approach%252C%2520unlike%2520previous%2520DL%2520models%252C%250Aassigns%2520a%2520confidence%2520score%252C%2520the%2520Tanimoto%2520similarity%252C%2520to%2520each%2520of%2520the%2520candidate%250Amolecules%252C%2520thus%2520providing%2520information%2520on%2520the%2520reliability%2520of%2520the%2520identification.%250A%2520%2520By%2520construction%252C%2520the%2520number%2520of%2520times%2520a%2520certain%2520substructure%2520is%2520present%2520in%2520the%250Amolecule%2520is%2520lost%2520during%2520the%2520hashing%2520process%252C%2520necessary%2520to%2520make%2520them%2520useful%2520for%250Amachine%2520learning%2520applications.%2520We%2520show%2520that%2520it%2520is%2520possible%2520to%2520complement%2520the%250Afingerprint-based%2520virtual%2520screening%2520with%2520global%2520information%2520provided%2520by%2520another%250ADL%2520model%2520that%2520predicts%2520from%2520the%2520same%2520HR--AFM%2520stacks%2520the%2520chemical%2520formula%252C%250Aboosting%2520the%2520identification%2520accuracy%2520up%2520to%2520a%252097.6%255C%2525.%2520Finally%252C%2520we%2520perform%2520a%250Alimited%2520test%2520with%2520experimental%2520images%252C%2520obtaining%2520promising%2520results%2520towards%2520the%250Aapplication%2520of%2520this%2520pipeline%2520under%2520real%2520conditions%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04321v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Molecular%20Identification%20via%20Molecular%20Fingerprint%20extraction%20from%0A%20%20Atomic%20Force%20Microscopy%20images&entry.906535625=Manuel%20Gonz%C3%A1lez%20Lastre%20and%20Pablo%20Pou%20and%20Miguel%20Wiche%20and%20Daniel%20Ebeling%20and%20Andre%20Schirmeisen%20and%20Rub%C3%A9n%20P%C3%A9rez&entry.1292438233=%20%20Non--Contact%20Atomic%20Force%20Microscopy%20with%20CO--functionalized%20metal%20tips%0A%28referred%20to%20as%20HR-AFM%29%20provides%20access%20to%20the%20internal%20structure%20of%20individual%0Amolecules%20adsorbed%20on%20a%20surface%20with%20totally%20unprecedented%20resolution.%20Previous%0Aworks%20have%20shown%20that%20deep%20learning%20%28DL%29%20models%20can%20retrieve%20the%20chemical%20and%0Astructural%20information%20encoded%20in%20a%203D%20stack%20of%20constant-height%20HR--AFM%20images%2C%0Aleading%20to%20molecular%20identification.%20In%20this%20work%2C%20we%20overcome%20their%0Alimitations%20by%20using%20a%20well-established%20description%20of%20the%20molecular%20structure%0Ain%20terms%20of%20topological%20fingerprints%2C%20the%201024--bit%20Extended%20Connectivity%0AChemical%20Fingerprints%20of%20radius%202%20%28ECFP4%29%2C%20that%20were%20developed%20for%20substructure%0Aand%20similarity%20searching.%20ECFPs%20provide%20local%20structural%20information%20of%20the%0Amolecule%2C%20each%20bit%20correlating%20with%20a%20particular%20substructure%20within%20the%0Amolecule.%20Our%20DL%20model%20is%20able%20to%20extract%20this%20optimized%20structural%20descriptor%0Afrom%20the%203D%20HR--AFM%20stacks%20and%20use%20it%2C%20through%20virtual%20screening%2C%20to%20identify%0Amolecules%20from%20their%20predicted%20ECFP4%20with%20a%20retrieval%20accuracy%20on%20theoretical%0Aimages%20of%2095.4%5C%25.%20Furthermore%2C%20this%20approach%2C%20unlike%20previous%20DL%20models%2C%0Aassigns%20a%20confidence%20score%2C%20the%20Tanimoto%20similarity%2C%20to%20each%20of%20the%20candidate%0Amolecules%2C%20thus%20providing%20information%20on%20the%20reliability%20of%20the%20identification.%0A%20%20By%20construction%2C%20the%20number%20of%20times%20a%20certain%20substructure%20is%20present%20in%20the%0Amolecule%20is%20lost%20during%20the%20hashing%20process%2C%20necessary%20to%20make%20them%20useful%20for%0Amachine%20learning%20applications.%20We%20show%20that%20it%20is%20possible%20to%20complement%20the%0Afingerprint-based%20virtual%20screening%20with%20global%20information%20provided%20by%20another%0ADL%20model%20that%20predicts%20from%20the%20same%20HR--AFM%20stacks%20the%20chemical%20formula%2C%0Aboosting%20the%20identification%20accuracy%20up%20to%20a%2097.6%5C%25.%20Finally%2C%20we%20perform%20a%0Alimited%20test%20with%20experimental%20images%2C%20obtaining%20promising%20results%20towards%20the%0Aapplication%20of%20this%20pipeline%20under%20real%20conditions%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04321v1&entry.124074799=Read"},
{"title": "A Significantly Better Class of Activation Functions Than ReLU Like\n  Activation Functions", "author": "Mathew Mithra Noel and Yug Oswal", "abstract": "  This paper introduces a significantly better class of activation functions\nthan the almost universally used ReLU like and Sigmoidal class of activation\nfunctions. Two new activation functions referred to as the Cone and\nParabolic-Cone that differ drastically from popular activation functions and\nsignificantly outperform these on the CIFAR-10 and Imagenette benchmmarks are\nproposed. The cone activation functions are positive only on a finite interval\nand are strictly negative except at the end-points of the interval, where they\nbecome zero. Thus the set of inputs that produce a positive output for a neuron\nwith cone activation functions is a hyperstrip and not a half-space as is the\nusual case. Since a hyper strip is the region between two parallel\nhyper-planes, it allows neurons to more finely divide the input feature space\ninto positive and negative classes than with infinitely wide half-spaces. In\nparticular the XOR function can be learn by a single neuron with cone-like\nactivation functions. Both the cone and parabolic-cone activation functions are\nshown to achieve higher accuracies with significantly fewer neurons on\nbenchmarks. The results presented in this paper indicate that many nonlinear\nreal-world datasets may be separated with fewer hyperstrips than half-spaces.\nThe Cone and Parabolic-Cone activation functions have larger derivatives than\nReLU and are shown to significantly speedup training.\n", "link": "http://arxiv.org/abs/2405.04459v1", "date": "2024-05-07", "relevancy": 1.879, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4945}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4596}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Significantly%20Better%20Class%20of%20Activation%20Functions%20Than%20ReLU%20Like%0A%20%20Activation%20Functions&body=Title%3A%20A%20Significantly%20Better%20Class%20of%20Activation%20Functions%20Than%20ReLU%20Like%0A%20%20Activation%20Functions%0AAuthor%3A%20Mathew%20Mithra%20Noel%20and%20Yug%20Oswal%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20significantly%20better%20class%20of%20activation%20functions%0Athan%20the%20almost%20universally%20used%20ReLU%20like%20and%20Sigmoidal%20class%20of%20activation%0Afunctions.%20Two%20new%20activation%20functions%20referred%20to%20as%20the%20Cone%20and%0AParabolic-Cone%20that%20differ%20drastically%20from%20popular%20activation%20functions%20and%0Asignificantly%20outperform%20these%20on%20the%20CIFAR-10%20and%20Imagenette%20benchmmarks%20are%0Aproposed.%20The%20cone%20activation%20functions%20are%20positive%20only%20on%20a%20finite%20interval%0Aand%20are%20strictly%20negative%20except%20at%20the%20end-points%20of%20the%20interval%2C%20where%20they%0Abecome%20zero.%20Thus%20the%20set%20of%20inputs%20that%20produce%20a%20positive%20output%20for%20a%20neuron%0Awith%20cone%20activation%20functions%20is%20a%20hyperstrip%20and%20not%20a%20half-space%20as%20is%20the%0Ausual%20case.%20Since%20a%20hyper%20strip%20is%20the%20region%20between%20two%20parallel%0Ahyper-planes%2C%20it%20allows%20neurons%20to%20more%20finely%20divide%20the%20input%20feature%20space%0Ainto%20positive%20and%20negative%20classes%20than%20with%20infinitely%20wide%20half-spaces.%20In%0Aparticular%20the%20XOR%20function%20can%20be%20learn%20by%20a%20single%20neuron%20with%20cone-like%0Aactivation%20functions.%20Both%20the%20cone%20and%20parabolic-cone%20activation%20functions%20are%0Ashown%20to%20achieve%20higher%20accuracies%20with%20significantly%20fewer%20neurons%20on%0Abenchmarks.%20The%20results%20presented%20in%20this%20paper%20indicate%20that%20many%20nonlinear%0Areal-world%20datasets%20may%20be%20separated%20with%20fewer%20hyperstrips%20than%20half-spaces.%0AThe%20Cone%20and%20Parabolic-Cone%20activation%20functions%20have%20larger%20derivatives%20than%0AReLU%20and%20are%20shown%20to%20significantly%20speedup%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Significantly%2520Better%2520Class%2520of%2520Activation%2520Functions%2520Than%2520ReLU%2520Like%250A%2520%2520Activation%2520Functions%26entry.906535625%3DMathew%2520Mithra%2520Noel%2520and%2520Yug%2520Oswal%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520significantly%2520better%2520class%2520of%2520activation%2520functions%250Athan%2520the%2520almost%2520universally%2520used%2520ReLU%2520like%2520and%2520Sigmoidal%2520class%2520of%2520activation%250Afunctions.%2520Two%2520new%2520activation%2520functions%2520referred%2520to%2520as%2520the%2520Cone%2520and%250AParabolic-Cone%2520that%2520differ%2520drastically%2520from%2520popular%2520activation%2520functions%2520and%250Asignificantly%2520outperform%2520these%2520on%2520the%2520CIFAR-10%2520and%2520Imagenette%2520benchmmarks%2520are%250Aproposed.%2520The%2520cone%2520activation%2520functions%2520are%2520positive%2520only%2520on%2520a%2520finite%2520interval%250Aand%2520are%2520strictly%2520negative%2520except%2520at%2520the%2520end-points%2520of%2520the%2520interval%252C%2520where%2520they%250Abecome%2520zero.%2520Thus%2520the%2520set%2520of%2520inputs%2520that%2520produce%2520a%2520positive%2520output%2520for%2520a%2520neuron%250Awith%2520cone%2520activation%2520functions%2520is%2520a%2520hyperstrip%2520and%2520not%2520a%2520half-space%2520as%2520is%2520the%250Ausual%2520case.%2520Since%2520a%2520hyper%2520strip%2520is%2520the%2520region%2520between%2520two%2520parallel%250Ahyper-planes%252C%2520it%2520allows%2520neurons%2520to%2520more%2520finely%2520divide%2520the%2520input%2520feature%2520space%250Ainto%2520positive%2520and%2520negative%2520classes%2520than%2520with%2520infinitely%2520wide%2520half-spaces.%2520In%250Aparticular%2520the%2520XOR%2520function%2520can%2520be%2520learn%2520by%2520a%2520single%2520neuron%2520with%2520cone-like%250Aactivation%2520functions.%2520Both%2520the%2520cone%2520and%2520parabolic-cone%2520activation%2520functions%2520are%250Ashown%2520to%2520achieve%2520higher%2520accuracies%2520with%2520significantly%2520fewer%2520neurons%2520on%250Abenchmarks.%2520The%2520results%2520presented%2520in%2520this%2520paper%2520indicate%2520that%2520many%2520nonlinear%250Areal-world%2520datasets%2520may%2520be%2520separated%2520with%2520fewer%2520hyperstrips%2520than%2520half-spaces.%250AThe%2520Cone%2520and%2520Parabolic-Cone%2520activation%2520functions%2520have%2520larger%2520derivatives%2520than%250AReLU%2520and%2520are%2520shown%2520to%2520significantly%2520speedup%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Significantly%20Better%20Class%20of%20Activation%20Functions%20Than%20ReLU%20Like%0A%20%20Activation%20Functions&entry.906535625=Mathew%20Mithra%20Noel%20and%20Yug%20Oswal&entry.1292438233=%20%20This%20paper%20introduces%20a%20significantly%20better%20class%20of%20activation%20functions%0Athan%20the%20almost%20universally%20used%20ReLU%20like%20and%20Sigmoidal%20class%20of%20activation%0Afunctions.%20Two%20new%20activation%20functions%20referred%20to%20as%20the%20Cone%20and%0AParabolic-Cone%20that%20differ%20drastically%20from%20popular%20activation%20functions%20and%0Asignificantly%20outperform%20these%20on%20the%20CIFAR-10%20and%20Imagenette%20benchmmarks%20are%0Aproposed.%20The%20cone%20activation%20functions%20are%20positive%20only%20on%20a%20finite%20interval%0Aand%20are%20strictly%20negative%20except%20at%20the%20end-points%20of%20the%20interval%2C%20where%20they%0Abecome%20zero.%20Thus%20the%20set%20of%20inputs%20that%20produce%20a%20positive%20output%20for%20a%20neuron%0Awith%20cone%20activation%20functions%20is%20a%20hyperstrip%20and%20not%20a%20half-space%20as%20is%20the%0Ausual%20case.%20Since%20a%20hyper%20strip%20is%20the%20region%20between%20two%20parallel%0Ahyper-planes%2C%20it%20allows%20neurons%20to%20more%20finely%20divide%20the%20input%20feature%20space%0Ainto%20positive%20and%20negative%20classes%20than%20with%20infinitely%20wide%20half-spaces.%20In%0Aparticular%20the%20XOR%20function%20can%20be%20learn%20by%20a%20single%20neuron%20with%20cone-like%0Aactivation%20functions.%20Both%20the%20cone%20and%20parabolic-cone%20activation%20functions%20are%0Ashown%20to%20achieve%20higher%20accuracies%20with%20significantly%20fewer%20neurons%20on%0Abenchmarks.%20The%20results%20presented%20in%20this%20paper%20indicate%20that%20many%20nonlinear%0Areal-world%20datasets%20may%20be%20separated%20with%20fewer%20hyperstrips%20than%20half-spaces.%0AThe%20Cone%20and%20Parabolic-Cone%20activation%20functions%20have%20larger%20derivatives%20than%0AReLU%20and%20are%20shown%20to%20significantly%20speedup%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04459v1&entry.124074799=Read"},
{"title": "Mitigating Clickbait: An Approach to Spoiler Generation Using Multitask\n  Learning", "author": "Sayantan Pal and Souvik Das and Rohini K. Srihari", "abstract": "  This study introduces 'clickbait spoiling', a novel technique designed to\ndetect, categorize, and generate spoilers as succinct text responses,\ncountering the curiosity induced by clickbait content. By leveraging a\nmulti-task learning framework, our model's generalization capabilities are\nsignificantly enhanced, effectively addressing the pervasive issue of\nclickbait. The crux of our research lies in generating appropriate spoilers, be\nit a phrase, an extended passage, or multiple, depending on the spoiler type\nrequired. Our methodology integrates two crucial techniques: a refined spoiler\ncategorization method and a modified version of the Question Answering (QA)\nmechanism, incorporated within a multi-task learning paradigm for optimized\nspoiler extraction from context. Notably, we have included fine-tuning methods\nfor models capable of handling longer sequences to accommodate the generation\nof extended spoilers. This research highlights the potential of sophisticated\ntext processing techniques in tackling the omnipresent issue of clickbait,\npromising an enhanced user experience in the digital realm.\n", "link": "http://arxiv.org/abs/2405.04292v1", "date": "2024-05-07", "relevancy": 1.8706, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.491}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4727}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Clickbait%3A%20An%20Approach%20to%20Spoiler%20Generation%20Using%20Multitask%0A%20%20Learning&body=Title%3A%20Mitigating%20Clickbait%3A%20An%20Approach%20to%20Spoiler%20Generation%20Using%20Multitask%0A%20%20Learning%0AAuthor%3A%20Sayantan%20Pal%20and%20Souvik%20Das%20and%20Rohini%20K.%20Srihari%0AAbstract%3A%20%20%20This%20study%20introduces%20%27clickbait%20spoiling%27%2C%20a%20novel%20technique%20designed%20to%0Adetect%2C%20categorize%2C%20and%20generate%20spoilers%20as%20succinct%20text%20responses%2C%0Acountering%20the%20curiosity%20induced%20by%20clickbait%20content.%20By%20leveraging%20a%0Amulti-task%20learning%20framework%2C%20our%20model%27s%20generalization%20capabilities%20are%0Asignificantly%20enhanced%2C%20effectively%20addressing%20the%20pervasive%20issue%20of%0Aclickbait.%20The%20crux%20of%20our%20research%20lies%20in%20generating%20appropriate%20spoilers%2C%20be%0Ait%20a%20phrase%2C%20an%20extended%20passage%2C%20or%20multiple%2C%20depending%20on%20the%20spoiler%20type%0Arequired.%20Our%20methodology%20integrates%20two%20crucial%20techniques%3A%20a%20refined%20spoiler%0Acategorization%20method%20and%20a%20modified%20version%20of%20the%20Question%20Answering%20%28QA%29%0Amechanism%2C%20incorporated%20within%20a%20multi-task%20learning%20paradigm%20for%20optimized%0Aspoiler%20extraction%20from%20context.%20Notably%2C%20we%20have%20included%20fine-tuning%20methods%0Afor%20models%20capable%20of%20handling%20longer%20sequences%20to%20accommodate%20the%20generation%0Aof%20extended%20spoilers.%20This%20research%20highlights%20the%20potential%20of%20sophisticated%0Atext%20processing%20techniques%20in%20tackling%20the%20omnipresent%20issue%20of%20clickbait%2C%0Apromising%20an%20enhanced%20user%20experience%20in%20the%20digital%20realm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Clickbait%253A%2520An%2520Approach%2520to%2520Spoiler%2520Generation%2520Using%2520Multitask%250A%2520%2520Learning%26entry.906535625%3DSayantan%2520Pal%2520and%2520Souvik%2520Das%2520and%2520Rohini%2520K.%2520Srihari%26entry.1292438233%3D%2520%2520This%2520study%2520introduces%2520%2527clickbait%2520spoiling%2527%252C%2520a%2520novel%2520technique%2520designed%2520to%250Adetect%252C%2520categorize%252C%2520and%2520generate%2520spoilers%2520as%2520succinct%2520text%2520responses%252C%250Acountering%2520the%2520curiosity%2520induced%2520by%2520clickbait%2520content.%2520By%2520leveraging%2520a%250Amulti-task%2520learning%2520framework%252C%2520our%2520model%2527s%2520generalization%2520capabilities%2520are%250Asignificantly%2520enhanced%252C%2520effectively%2520addressing%2520the%2520pervasive%2520issue%2520of%250Aclickbait.%2520The%2520crux%2520of%2520our%2520research%2520lies%2520in%2520generating%2520appropriate%2520spoilers%252C%2520be%250Ait%2520a%2520phrase%252C%2520an%2520extended%2520passage%252C%2520or%2520multiple%252C%2520depending%2520on%2520the%2520spoiler%2520type%250Arequired.%2520Our%2520methodology%2520integrates%2520two%2520crucial%2520techniques%253A%2520a%2520refined%2520spoiler%250Acategorization%2520method%2520and%2520a%2520modified%2520version%2520of%2520the%2520Question%2520Answering%2520%2528QA%2529%250Amechanism%252C%2520incorporated%2520within%2520a%2520multi-task%2520learning%2520paradigm%2520for%2520optimized%250Aspoiler%2520extraction%2520from%2520context.%2520Notably%252C%2520we%2520have%2520included%2520fine-tuning%2520methods%250Afor%2520models%2520capable%2520of%2520handling%2520longer%2520sequences%2520to%2520accommodate%2520the%2520generation%250Aof%2520extended%2520spoilers.%2520This%2520research%2520highlights%2520the%2520potential%2520of%2520sophisticated%250Atext%2520processing%2520techniques%2520in%2520tackling%2520the%2520omnipresent%2520issue%2520of%2520clickbait%252C%250Apromising%2520an%2520enhanced%2520user%2520experience%2520in%2520the%2520digital%2520realm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Clickbait%3A%20An%20Approach%20to%20Spoiler%20Generation%20Using%20Multitask%0A%20%20Learning&entry.906535625=Sayantan%20Pal%20and%20Souvik%20Das%20and%20Rohini%20K.%20Srihari&entry.1292438233=%20%20This%20study%20introduces%20%27clickbait%20spoiling%27%2C%20a%20novel%20technique%20designed%20to%0Adetect%2C%20categorize%2C%20and%20generate%20spoilers%20as%20succinct%20text%20responses%2C%0Acountering%20the%20curiosity%20induced%20by%20clickbait%20content.%20By%20leveraging%20a%0Amulti-task%20learning%20framework%2C%20our%20model%27s%20generalization%20capabilities%20are%0Asignificantly%20enhanced%2C%20effectively%20addressing%20the%20pervasive%20issue%20of%0Aclickbait.%20The%20crux%20of%20our%20research%20lies%20in%20generating%20appropriate%20spoilers%2C%20be%0Ait%20a%20phrase%2C%20an%20extended%20passage%2C%20or%20multiple%2C%20depending%20on%20the%20spoiler%20type%0Arequired.%20Our%20methodology%20integrates%20two%20crucial%20techniques%3A%20a%20refined%20spoiler%0Acategorization%20method%20and%20a%20modified%20version%20of%20the%20Question%20Answering%20%28QA%29%0Amechanism%2C%20incorporated%20within%20a%20multi-task%20learning%20paradigm%20for%20optimized%0Aspoiler%20extraction%20from%20context.%20Notably%2C%20we%20have%20included%20fine-tuning%20methods%0Afor%20models%20capable%20of%20handling%20longer%20sequences%20to%20accommodate%20the%20generation%0Aof%20extended%20spoilers.%20This%20research%20highlights%20the%20potential%20of%20sophisticated%0Atext%20processing%20techniques%20in%20tackling%20the%20omnipresent%20issue%20of%20clickbait%2C%0Apromising%20an%20enhanced%20user%20experience%20in%20the%20digital%20realm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04292v1&entry.124074799=Read"},
{"title": "Network reconstruction via the minimum description length principle", "author": "Tiago P. Peixoto", "abstract": "  A fundamental problem associated with the task of network reconstruction from\ndynamical or behavioral data consists in determining the most appropriate model\ncomplexity in a manner that prevents overfitting, and produces an inferred\nnetwork with a statistically justifiable number of edges. The status quo in\nthis context is based on $L_{1}$ regularization combined with cross-validation.\nHowever, besides its high computational cost, this commonplace approach\nunnecessarily ties the promotion of sparsity with weight \"shrinkage\". This\ncombination forces a trade-off between the bias introduced by shrinkage and the\nnetwork sparsity, which often results in substantial overfitting even after\ncross-validation. In this work, we propose an alternative nonparametric\nregularization scheme based on hierarchical Bayesian inference and weight\nquantization, which does not rely on weight shrinkage to promote sparsity. Our\napproach follows the minimum description length (MDL) principle, and uncovers\nthe weight distribution that allows for the most compression of the data, thus\navoiding overfitting without requiring cross-validation. The latter property\nrenders our approach substantially faster to employ, as it requires a single\nfit to the complete data. As a result, we have a principled and efficient\ninference scheme that can be used with a large variety of generative models,\nwithout requiring the number of edges to be known in advance. We also\ndemonstrate that our scheme yields systematically increased accuracy in the\nreconstruction of both artificial and empirical networks. We highlight the use\nof our method with the reconstruction of interaction networks between microbial\ncommunities from large-scale abundance samples involving in the order of\n$10^{4}$ to $10^{5}$ species, and demonstrate how the inferred model can be\nused to predict the outcome of interventions in the system.\n", "link": "http://arxiv.org/abs/2405.01015v2", "date": "2024-05-07", "relevancy": 1.8679, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.475}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4641}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Network%20reconstruction%20via%20the%20minimum%20description%20length%20principle&body=Title%3A%20Network%20reconstruction%20via%20the%20minimum%20description%20length%20principle%0AAuthor%3A%20Tiago%20P.%20Peixoto%0AAbstract%3A%20%20%20A%20fundamental%20problem%20associated%20with%20the%20task%20of%20network%20reconstruction%20from%0Adynamical%20or%20behavioral%20data%20consists%20in%20determining%20the%20most%20appropriate%20model%0Acomplexity%20in%20a%20manner%20that%20prevents%20overfitting%2C%20and%20produces%20an%20inferred%0Anetwork%20with%20a%20statistically%20justifiable%20number%20of%20edges.%20The%20status%20quo%20in%0Athis%20context%20is%20based%20on%20%24L_%7B1%7D%24%20regularization%20combined%20with%20cross-validation.%0AHowever%2C%20besides%20its%20high%20computational%20cost%2C%20this%20commonplace%20approach%0Aunnecessarily%20ties%20the%20promotion%20of%20sparsity%20with%20weight%20%22shrinkage%22.%20This%0Acombination%20forces%20a%20trade-off%20between%20the%20bias%20introduced%20by%20shrinkage%20and%20the%0Anetwork%20sparsity%2C%20which%20often%20results%20in%20substantial%20overfitting%20even%20after%0Across-validation.%20In%20this%20work%2C%20we%20propose%20an%20alternative%20nonparametric%0Aregularization%20scheme%20based%20on%20hierarchical%20Bayesian%20inference%20and%20weight%0Aquantization%2C%20which%20does%20not%20rely%20on%20weight%20shrinkage%20to%20promote%20sparsity.%20Our%0Aapproach%20follows%20the%20minimum%20description%20length%20%28MDL%29%20principle%2C%20and%20uncovers%0Athe%20weight%20distribution%20that%20allows%20for%20the%20most%20compression%20of%20the%20data%2C%20thus%0Aavoiding%20overfitting%20without%20requiring%20cross-validation.%20The%20latter%20property%0Arenders%20our%20approach%20substantially%20faster%20to%20employ%2C%20as%20it%20requires%20a%20single%0Afit%20to%20the%20complete%20data.%20As%20a%20result%2C%20we%20have%20a%20principled%20and%20efficient%0Ainference%20scheme%20that%20can%20be%20used%20with%20a%20large%20variety%20of%20generative%20models%2C%0Awithout%20requiring%20the%20number%20of%20edges%20to%20be%20known%20in%20advance.%20We%20also%0Ademonstrate%20that%20our%20scheme%20yields%20systematically%20increased%20accuracy%20in%20the%0Areconstruction%20of%20both%20artificial%20and%20empirical%20networks.%20We%20highlight%20the%20use%0Aof%20our%20method%20with%20the%20reconstruction%20of%20interaction%20networks%20between%20microbial%0Acommunities%20from%20large-scale%20abundance%20samples%20involving%20in%20the%20order%20of%0A%2410%5E%7B4%7D%24%20to%20%2410%5E%7B5%7D%24%20species%2C%20and%20demonstrate%20how%20the%20inferred%20model%20can%20be%0Aused%20to%20predict%20the%20outcome%20of%20interventions%20in%20the%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01015v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNetwork%2520reconstruction%2520via%2520the%2520minimum%2520description%2520length%2520principle%26entry.906535625%3DTiago%2520P.%2520Peixoto%26entry.1292438233%3D%2520%2520A%2520fundamental%2520problem%2520associated%2520with%2520the%2520task%2520of%2520network%2520reconstruction%2520from%250Adynamical%2520or%2520behavioral%2520data%2520consists%2520in%2520determining%2520the%2520most%2520appropriate%2520model%250Acomplexity%2520in%2520a%2520manner%2520that%2520prevents%2520overfitting%252C%2520and%2520produces%2520an%2520inferred%250Anetwork%2520with%2520a%2520statistically%2520justifiable%2520number%2520of%2520edges.%2520The%2520status%2520quo%2520in%250Athis%2520context%2520is%2520based%2520on%2520%2524L_%257B1%257D%2524%2520regularization%2520combined%2520with%2520cross-validation.%250AHowever%252C%2520besides%2520its%2520high%2520computational%2520cost%252C%2520this%2520commonplace%2520approach%250Aunnecessarily%2520ties%2520the%2520promotion%2520of%2520sparsity%2520with%2520weight%2520%2522shrinkage%2522.%2520This%250Acombination%2520forces%2520a%2520trade-off%2520between%2520the%2520bias%2520introduced%2520by%2520shrinkage%2520and%2520the%250Anetwork%2520sparsity%252C%2520which%2520often%2520results%2520in%2520substantial%2520overfitting%2520even%2520after%250Across-validation.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520alternative%2520nonparametric%250Aregularization%2520scheme%2520based%2520on%2520hierarchical%2520Bayesian%2520inference%2520and%2520weight%250Aquantization%252C%2520which%2520does%2520not%2520rely%2520on%2520weight%2520shrinkage%2520to%2520promote%2520sparsity.%2520Our%250Aapproach%2520follows%2520the%2520minimum%2520description%2520length%2520%2528MDL%2529%2520principle%252C%2520and%2520uncovers%250Athe%2520weight%2520distribution%2520that%2520allows%2520for%2520the%2520most%2520compression%2520of%2520the%2520data%252C%2520thus%250Aavoiding%2520overfitting%2520without%2520requiring%2520cross-validation.%2520The%2520latter%2520property%250Arenders%2520our%2520approach%2520substantially%2520faster%2520to%2520employ%252C%2520as%2520it%2520requires%2520a%2520single%250Afit%2520to%2520the%2520complete%2520data.%2520As%2520a%2520result%252C%2520we%2520have%2520a%2520principled%2520and%2520efficient%250Ainference%2520scheme%2520that%2520can%2520be%2520used%2520with%2520a%2520large%2520variety%2520of%2520generative%2520models%252C%250Awithout%2520requiring%2520the%2520number%2520of%2520edges%2520to%2520be%2520known%2520in%2520advance.%2520We%2520also%250Ademonstrate%2520that%2520our%2520scheme%2520yields%2520systematically%2520increased%2520accuracy%2520in%2520the%250Areconstruction%2520of%2520both%2520artificial%2520and%2520empirical%2520networks.%2520We%2520highlight%2520the%2520use%250Aof%2520our%2520method%2520with%2520the%2520reconstruction%2520of%2520interaction%2520networks%2520between%2520microbial%250Acommunities%2520from%2520large-scale%2520abundance%2520samples%2520involving%2520in%2520the%2520order%2520of%250A%252410%255E%257B4%257D%2524%2520to%2520%252410%255E%257B5%257D%2524%2520species%252C%2520and%2520demonstrate%2520how%2520the%2520inferred%2520model%2520can%2520be%250Aused%2520to%2520predict%2520the%2520outcome%2520of%2520interventions%2520in%2520the%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01015v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Network%20reconstruction%20via%20the%20minimum%20description%20length%20principle&entry.906535625=Tiago%20P.%20Peixoto&entry.1292438233=%20%20A%20fundamental%20problem%20associated%20with%20the%20task%20of%20network%20reconstruction%20from%0Adynamical%20or%20behavioral%20data%20consists%20in%20determining%20the%20most%20appropriate%20model%0Acomplexity%20in%20a%20manner%20that%20prevents%20overfitting%2C%20and%20produces%20an%20inferred%0Anetwork%20with%20a%20statistically%20justifiable%20number%20of%20edges.%20The%20status%20quo%20in%0Athis%20context%20is%20based%20on%20%24L_%7B1%7D%24%20regularization%20combined%20with%20cross-validation.%0AHowever%2C%20besides%20its%20high%20computational%20cost%2C%20this%20commonplace%20approach%0Aunnecessarily%20ties%20the%20promotion%20of%20sparsity%20with%20weight%20%22shrinkage%22.%20This%0Acombination%20forces%20a%20trade-off%20between%20the%20bias%20introduced%20by%20shrinkage%20and%20the%0Anetwork%20sparsity%2C%20which%20often%20results%20in%20substantial%20overfitting%20even%20after%0Across-validation.%20In%20this%20work%2C%20we%20propose%20an%20alternative%20nonparametric%0Aregularization%20scheme%20based%20on%20hierarchical%20Bayesian%20inference%20and%20weight%0Aquantization%2C%20which%20does%20not%20rely%20on%20weight%20shrinkage%20to%20promote%20sparsity.%20Our%0Aapproach%20follows%20the%20minimum%20description%20length%20%28MDL%29%20principle%2C%20and%20uncovers%0Athe%20weight%20distribution%20that%20allows%20for%20the%20most%20compression%20of%20the%20data%2C%20thus%0Aavoiding%20overfitting%20without%20requiring%20cross-validation.%20The%20latter%20property%0Arenders%20our%20approach%20substantially%20faster%20to%20employ%2C%20as%20it%20requires%20a%20single%0Afit%20to%20the%20complete%20data.%20As%20a%20result%2C%20we%20have%20a%20principled%20and%20efficient%0Ainference%20scheme%20that%20can%20be%20used%20with%20a%20large%20variety%20of%20generative%20models%2C%0Awithout%20requiring%20the%20number%20of%20edges%20to%20be%20known%20in%20advance.%20We%20also%0Ademonstrate%20that%20our%20scheme%20yields%20systematically%20increased%20accuracy%20in%20the%0Areconstruction%20of%20both%20artificial%20and%20empirical%20networks.%20We%20highlight%20the%20use%0Aof%20our%20method%20with%20the%20reconstruction%20of%20interaction%20networks%20between%20microbial%0Acommunities%20from%20large-scale%20abundance%20samples%20involving%20in%20the%20order%20of%0A%2410%5E%7B4%7D%24%20to%20%2410%5E%7B5%7D%24%20species%2C%20and%20demonstrate%20how%20the%20inferred%20model%20can%20be%0Aused%20to%20predict%20the%20outcome%20of%20interventions%20in%20the%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01015v2&entry.124074799=Read"},
{"title": "Open Implementation and Study of BEST-RQ for Speech Processing", "author": "Ryan Whetten and Titouan Parcollet and Marco Dinarelli and Yannick Est\u00e8ve", "abstract": "  Self-Supervised Learning (SSL) has proven to be useful in various speech\ntasks. However, these methods are generally very demanding in terms of data,\nmemory, and computational resources. BERT-based Speech pre-Training with\nRandom-projection Quantizer (BEST-RQ), is an SSL method that has shown great\nperformance on Automatic Speech Recognition (ASR) while being simpler than\nother SSL methods, such as wav2vec 2.0. Despite BEST-RQ's great performance,\ndetails are lacking in the original paper, such as the amount of GPU/TPU hours\nused in pre-training, and there is no official easy-to-use open-source\nimplementation. Furthermore, BEST-RQ has not been evaluated on other downstream\ntasks aside from ASR and speech translation. In this work, we describe a\nre-implementation of a Random-projection quantizer and perform a preliminary\nstudy with a comparison to wav2vec 2.0 on four downstream tasks. We discuss the\ndetails and differences of our implementation. We show that a random projection\nquantizer can achieve similar downstream performance as wav2vec 2.0 while\ndecreasing training time by over a factor of two.\n", "link": "http://arxiv.org/abs/2405.04296v1", "date": "2024-05-07", "relevancy": 1.8659, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4876}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4624}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open%20Implementation%20and%20Study%20of%20BEST-RQ%20for%20Speech%20Processing&body=Title%3A%20Open%20Implementation%20and%20Study%20of%20BEST-RQ%20for%20Speech%20Processing%0AAuthor%3A%20Ryan%20Whetten%20and%20Titouan%20Parcollet%20and%20Marco%20Dinarelli%20and%20Yannick%20Est%C3%A8ve%0AAbstract%3A%20%20%20Self-Supervised%20Learning%20%28SSL%29%20has%20proven%20to%20be%20useful%20in%20various%20speech%0Atasks.%20However%2C%20these%20methods%20are%20generally%20very%20demanding%20in%20terms%20of%20data%2C%0Amemory%2C%20and%20computational%20resources.%20BERT-based%20Speech%20pre-Training%20with%0ARandom-projection%20Quantizer%20%28BEST-RQ%29%2C%20is%20an%20SSL%20method%20that%20has%20shown%20great%0Aperformance%20on%20Automatic%20Speech%20Recognition%20%28ASR%29%20while%20being%20simpler%20than%0Aother%20SSL%20methods%2C%20such%20as%20wav2vec%202.0.%20Despite%20BEST-RQ%27s%20great%20performance%2C%0Adetails%20are%20lacking%20in%20the%20original%20paper%2C%20such%20as%20the%20amount%20of%20GPU/TPU%20hours%0Aused%20in%20pre-training%2C%20and%20there%20is%20no%20official%20easy-to-use%20open-source%0Aimplementation.%20Furthermore%2C%20BEST-RQ%20has%20not%20been%20evaluated%20on%20other%20downstream%0Atasks%20aside%20from%20ASR%20and%20speech%20translation.%20In%20this%20work%2C%20we%20describe%20a%0Are-implementation%20of%20a%20Random-projection%20quantizer%20and%20perform%20a%20preliminary%0Astudy%20with%20a%20comparison%20to%20wav2vec%202.0%20on%20four%20downstream%20tasks.%20We%20discuss%20the%0Adetails%20and%20differences%20of%20our%20implementation.%20We%20show%20that%20a%20random%20projection%0Aquantizer%20can%20achieve%20similar%20downstream%20performance%20as%20wav2vec%202.0%20while%0Adecreasing%20training%20time%20by%20over%20a%20factor%20of%20two.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04296v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen%2520Implementation%2520and%2520Study%2520of%2520BEST-RQ%2520for%2520Speech%2520Processing%26entry.906535625%3DRyan%2520Whetten%2520and%2520Titouan%2520Parcollet%2520and%2520Marco%2520Dinarelli%2520and%2520Yannick%2520Est%25C3%25A8ve%26entry.1292438233%3D%2520%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520has%2520proven%2520to%2520be%2520useful%2520in%2520various%2520speech%250Atasks.%2520However%252C%2520these%2520methods%2520are%2520generally%2520very%2520demanding%2520in%2520terms%2520of%2520data%252C%250Amemory%252C%2520and%2520computational%2520resources.%2520BERT-based%2520Speech%2520pre-Training%2520with%250ARandom-projection%2520Quantizer%2520%2528BEST-RQ%2529%252C%2520is%2520an%2520SSL%2520method%2520that%2520has%2520shown%2520great%250Aperformance%2520on%2520Automatic%2520Speech%2520Recognition%2520%2528ASR%2529%2520while%2520being%2520simpler%2520than%250Aother%2520SSL%2520methods%252C%2520such%2520as%2520wav2vec%25202.0.%2520Despite%2520BEST-RQ%2527s%2520great%2520performance%252C%250Adetails%2520are%2520lacking%2520in%2520the%2520original%2520paper%252C%2520such%2520as%2520the%2520amount%2520of%2520GPU/TPU%2520hours%250Aused%2520in%2520pre-training%252C%2520and%2520there%2520is%2520no%2520official%2520easy-to-use%2520open-source%250Aimplementation.%2520Furthermore%252C%2520BEST-RQ%2520has%2520not%2520been%2520evaluated%2520on%2520other%2520downstream%250Atasks%2520aside%2520from%2520ASR%2520and%2520speech%2520translation.%2520In%2520this%2520work%252C%2520we%2520describe%2520a%250Are-implementation%2520of%2520a%2520Random-projection%2520quantizer%2520and%2520perform%2520a%2520preliminary%250Astudy%2520with%2520a%2520comparison%2520to%2520wav2vec%25202.0%2520on%2520four%2520downstream%2520tasks.%2520We%2520discuss%2520the%250Adetails%2520and%2520differences%2520of%2520our%2520implementation.%2520We%2520show%2520that%2520a%2520random%2520projection%250Aquantizer%2520can%2520achieve%2520similar%2520downstream%2520performance%2520as%2520wav2vec%25202.0%2520while%250Adecreasing%2520training%2520time%2520by%2520over%2520a%2520factor%2520of%2520two.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04296v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open%20Implementation%20and%20Study%20of%20BEST-RQ%20for%20Speech%20Processing&entry.906535625=Ryan%20Whetten%20and%20Titouan%20Parcollet%20and%20Marco%20Dinarelli%20and%20Yannick%20Est%C3%A8ve&entry.1292438233=%20%20Self-Supervised%20Learning%20%28SSL%29%20has%20proven%20to%20be%20useful%20in%20various%20speech%0Atasks.%20However%2C%20these%20methods%20are%20generally%20very%20demanding%20in%20terms%20of%20data%2C%0Amemory%2C%20and%20computational%20resources.%20BERT-based%20Speech%20pre-Training%20with%0ARandom-projection%20Quantizer%20%28BEST-RQ%29%2C%20is%20an%20SSL%20method%20that%20has%20shown%20great%0Aperformance%20on%20Automatic%20Speech%20Recognition%20%28ASR%29%20while%20being%20simpler%20than%0Aother%20SSL%20methods%2C%20such%20as%20wav2vec%202.0.%20Despite%20BEST-RQ%27s%20great%20performance%2C%0Adetails%20are%20lacking%20in%20the%20original%20paper%2C%20such%20as%20the%20amount%20of%20GPU/TPU%20hours%0Aused%20in%20pre-training%2C%20and%20there%20is%20no%20official%20easy-to-use%20open-source%0Aimplementation.%20Furthermore%2C%20BEST-RQ%20has%20not%20been%20evaluated%20on%20other%20downstream%0Atasks%20aside%20from%20ASR%20and%20speech%20translation.%20In%20this%20work%2C%20we%20describe%20a%0Are-implementation%20of%20a%20Random-projection%20quantizer%20and%20perform%20a%20preliminary%0Astudy%20with%20a%20comparison%20to%20wav2vec%202.0%20on%20four%20downstream%20tasks.%20We%20discuss%20the%0Adetails%20and%20differences%20of%20our%20implementation.%20We%20show%20that%20a%20random%20projection%0Aquantizer%20can%20achieve%20similar%20downstream%20performance%20as%20wav2vec%202.0%20while%0Adecreasing%20training%20time%20by%20over%20a%20factor%20of%20two.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04296v1&entry.124074799=Read"},
{"title": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention", "author": "Ramya Prabhu and Ajay Nayak and Jayashree Mohan and Ramachandran Ramjee and Ashish Panwar", "abstract": "  Efficient use of GPU memory is essential for high throughput LLM inference.\nPrior systems reserved memory for the KV-cache ahead-of-time, resulting in\nwasted capacity due to internal fragmentation. Inspired by OS-based virtual\nmemory systems, vLLM proposed PagedAttention to enable dynamic memory\nallocation for KV-cache. This approach eliminates fragmentation, enabling\nhigh-throughput LLM serving with larger batch sizes. However, to be able to\nallocate physical memory dynamically, PagedAttention changes the layout of\nKV-cache from contiguous virtual memory to non-contiguous virtual memory. This\nchange requires attention kernels to be rewritten to support paging, and\nserving framework to implement a memory manager. Thus, the PagedAttention model\nleads to software complexity, portability issues, redundancy and inefficiency.\n  In this paper, we propose vAttention for dynamic KV-cache memory management.\nIn contrast to PagedAttention, vAttention retains KV-cache in contiguous\nvirtual memory and leverages low-level system support for demand paging, that\nalready exists, to enable on-demand physical memory allocation. Thus,\nvAttention unburdens the attention kernel developer from having to explicitly\nsupport paging and avoids re-implementation of memory management in the serving\nframework. We show that vAttention enables seamless dynamic memory management\nfor unchanged implementations of various attention kernels. vAttention also\ngenerates tokens up to 1.97x faster than vLLM, while processing input prompts\nup to 3.92x and 1.45x faster than the PagedAttention variants of FlashAttention\nand FlashInfer.\n", "link": "http://arxiv.org/abs/2405.04437v1", "date": "2024-05-07", "relevancy": 1.8627, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4979}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.443}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20vAttention%3A%20Dynamic%20Memory%20Management%20for%20Serving%20LLMs%20without%0A%20%20PagedAttention&body=Title%3A%20vAttention%3A%20Dynamic%20Memory%20Management%20for%20Serving%20LLMs%20without%0A%20%20PagedAttention%0AAuthor%3A%20Ramya%20Prabhu%20and%20Ajay%20Nayak%20and%20Jayashree%20Mohan%20and%20Ramachandran%20Ramjee%20and%20Ashish%20Panwar%0AAbstract%3A%20%20%20Efficient%20use%20of%20GPU%20memory%20is%20essential%20for%20high%20throughput%20LLM%20inference.%0APrior%20systems%20reserved%20memory%20for%20the%20KV-cache%20ahead-of-time%2C%20resulting%20in%0Awasted%20capacity%20due%20to%20internal%20fragmentation.%20Inspired%20by%20OS-based%20virtual%0Amemory%20systems%2C%20vLLM%20proposed%20PagedAttention%20to%20enable%20dynamic%20memory%0Aallocation%20for%20KV-cache.%20This%20approach%20eliminates%20fragmentation%2C%20enabling%0Ahigh-throughput%20LLM%20serving%20with%20larger%20batch%20sizes.%20However%2C%20to%20be%20able%20to%0Aallocate%20physical%20memory%20dynamically%2C%20PagedAttention%20changes%20the%20layout%20of%0AKV-cache%20from%20contiguous%20virtual%20memory%20to%20non-contiguous%20virtual%20memory.%20This%0Achange%20requires%20attention%20kernels%20to%20be%20rewritten%20to%20support%20paging%2C%20and%0Aserving%20framework%20to%20implement%20a%20memory%20manager.%20Thus%2C%20the%20PagedAttention%20model%0Aleads%20to%20software%20complexity%2C%20portability%20issues%2C%20redundancy%20and%20inefficiency.%0A%20%20In%20this%20paper%2C%20we%20propose%20vAttention%20for%20dynamic%20KV-cache%20memory%20management.%0AIn%20contrast%20to%20PagedAttention%2C%20vAttention%20retains%20KV-cache%20in%20contiguous%0Avirtual%20memory%20and%20leverages%20low-level%20system%20support%20for%20demand%20paging%2C%20that%0Aalready%20exists%2C%20to%20enable%20on-demand%20physical%20memory%20allocation.%20Thus%2C%0AvAttention%20unburdens%20the%20attention%20kernel%20developer%20from%20having%20to%20explicitly%0Asupport%20paging%20and%20avoids%20re-implementation%20of%20memory%20management%20in%20the%20serving%0Aframework.%20We%20show%20that%20vAttention%20enables%20seamless%20dynamic%20memory%20management%0Afor%20unchanged%20implementations%20of%20various%20attention%20kernels.%20vAttention%20also%0Agenerates%20tokens%20up%20to%201.97x%20faster%20than%20vLLM%2C%20while%20processing%20input%20prompts%0Aup%20to%203.92x%20and%201.45x%20faster%20than%20the%20PagedAttention%20variants%20of%20FlashAttention%0Aand%20FlashInfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DvAttention%253A%2520Dynamic%2520Memory%2520Management%2520for%2520Serving%2520LLMs%2520without%250A%2520%2520PagedAttention%26entry.906535625%3DRamya%2520Prabhu%2520and%2520Ajay%2520Nayak%2520and%2520Jayashree%2520Mohan%2520and%2520Ramachandran%2520Ramjee%2520and%2520Ashish%2520Panwar%26entry.1292438233%3D%2520%2520Efficient%2520use%2520of%2520GPU%2520memory%2520is%2520essential%2520for%2520high%2520throughput%2520LLM%2520inference.%250APrior%2520systems%2520reserved%2520memory%2520for%2520the%2520KV-cache%2520ahead-of-time%252C%2520resulting%2520in%250Awasted%2520capacity%2520due%2520to%2520internal%2520fragmentation.%2520Inspired%2520by%2520OS-based%2520virtual%250Amemory%2520systems%252C%2520vLLM%2520proposed%2520PagedAttention%2520to%2520enable%2520dynamic%2520memory%250Aallocation%2520for%2520KV-cache.%2520This%2520approach%2520eliminates%2520fragmentation%252C%2520enabling%250Ahigh-throughput%2520LLM%2520serving%2520with%2520larger%2520batch%2520sizes.%2520However%252C%2520to%2520be%2520able%2520to%250Aallocate%2520physical%2520memory%2520dynamically%252C%2520PagedAttention%2520changes%2520the%2520layout%2520of%250AKV-cache%2520from%2520contiguous%2520virtual%2520memory%2520to%2520non-contiguous%2520virtual%2520memory.%2520This%250Achange%2520requires%2520attention%2520kernels%2520to%2520be%2520rewritten%2520to%2520support%2520paging%252C%2520and%250Aserving%2520framework%2520to%2520implement%2520a%2520memory%2520manager.%2520Thus%252C%2520the%2520PagedAttention%2520model%250Aleads%2520to%2520software%2520complexity%252C%2520portability%2520issues%252C%2520redundancy%2520and%2520inefficiency.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520vAttention%2520for%2520dynamic%2520KV-cache%2520memory%2520management.%250AIn%2520contrast%2520to%2520PagedAttention%252C%2520vAttention%2520retains%2520KV-cache%2520in%2520contiguous%250Avirtual%2520memory%2520and%2520leverages%2520low-level%2520system%2520support%2520for%2520demand%2520paging%252C%2520that%250Aalready%2520exists%252C%2520to%2520enable%2520on-demand%2520physical%2520memory%2520allocation.%2520Thus%252C%250AvAttention%2520unburdens%2520the%2520attention%2520kernel%2520developer%2520from%2520having%2520to%2520explicitly%250Asupport%2520paging%2520and%2520avoids%2520re-implementation%2520of%2520memory%2520management%2520in%2520the%2520serving%250Aframework.%2520We%2520show%2520that%2520vAttention%2520enables%2520seamless%2520dynamic%2520memory%2520management%250Afor%2520unchanged%2520implementations%2520of%2520various%2520attention%2520kernels.%2520vAttention%2520also%250Agenerates%2520tokens%2520up%2520to%25201.97x%2520faster%2520than%2520vLLM%252C%2520while%2520processing%2520input%2520prompts%250Aup%2520to%25203.92x%2520and%25201.45x%2520faster%2520than%2520the%2520PagedAttention%2520variants%2520of%2520FlashAttention%250Aand%2520FlashInfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=vAttention%3A%20Dynamic%20Memory%20Management%20for%20Serving%20LLMs%20without%0A%20%20PagedAttention&entry.906535625=Ramya%20Prabhu%20and%20Ajay%20Nayak%20and%20Jayashree%20Mohan%20and%20Ramachandran%20Ramjee%20and%20Ashish%20Panwar&entry.1292438233=%20%20Efficient%20use%20of%20GPU%20memory%20is%20essential%20for%20high%20throughput%20LLM%20inference.%0APrior%20systems%20reserved%20memory%20for%20the%20KV-cache%20ahead-of-time%2C%20resulting%20in%0Awasted%20capacity%20due%20to%20internal%20fragmentation.%20Inspired%20by%20OS-based%20virtual%0Amemory%20systems%2C%20vLLM%20proposed%20PagedAttention%20to%20enable%20dynamic%20memory%0Aallocation%20for%20KV-cache.%20This%20approach%20eliminates%20fragmentation%2C%20enabling%0Ahigh-throughput%20LLM%20serving%20with%20larger%20batch%20sizes.%20However%2C%20to%20be%20able%20to%0Aallocate%20physical%20memory%20dynamically%2C%20PagedAttention%20changes%20the%20layout%20of%0AKV-cache%20from%20contiguous%20virtual%20memory%20to%20non-contiguous%20virtual%20memory.%20This%0Achange%20requires%20attention%20kernels%20to%20be%20rewritten%20to%20support%20paging%2C%20and%0Aserving%20framework%20to%20implement%20a%20memory%20manager.%20Thus%2C%20the%20PagedAttention%20model%0Aleads%20to%20software%20complexity%2C%20portability%20issues%2C%20redundancy%20and%20inefficiency.%0A%20%20In%20this%20paper%2C%20we%20propose%20vAttention%20for%20dynamic%20KV-cache%20memory%20management.%0AIn%20contrast%20to%20PagedAttention%2C%20vAttention%20retains%20KV-cache%20in%20contiguous%0Avirtual%20memory%20and%20leverages%20low-level%20system%20support%20for%20demand%20paging%2C%20that%0Aalready%20exists%2C%20to%20enable%20on-demand%20physical%20memory%20allocation.%20Thus%2C%0AvAttention%20unburdens%20the%20attention%20kernel%20developer%20from%20having%20to%20explicitly%0Asupport%20paging%20and%20avoids%20re-implementation%20of%20memory%20management%20in%20the%20serving%0Aframework.%20We%20show%20that%20vAttention%20enables%20seamless%20dynamic%20memory%20management%0Afor%20unchanged%20implementations%20of%20various%20attention%20kernels.%20vAttention%20also%0Agenerates%20tokens%20up%20to%201.97x%20faster%20than%20vLLM%2C%20while%20processing%20input%20prompts%0Aup%20to%203.92x%20and%201.45x%20faster%20than%20the%20PagedAttention%20variants%20of%20FlashAttention%0Aand%20FlashInfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04437v1&entry.124074799=Read"},
{"title": "Material Property Prediction using Graphs based on Generically Complete\n  Isometry Invariants", "author": "Jonathan Balasingham and Viktor Zamaraev and Vitaliy Kurlin", "abstract": "  The structure-property hypothesis says that the properties of all materials\nare determined by an underlying crystal structure. The main obstacle was the\nambiguity of conventional crystal representations based on incomplete or\ndiscontinuous descriptors that allow false negatives or false positives. This\nambiguity was resolved by the ultra-fast Pointwise Distance Distribution (PDD),\nwhich distinguished all periodic structures in the world's largest collection\nof real materials (Cambridge Structural Database). The state-of-the-art results\nin property predictions were previously achieved by graph neural networks based\non various graph representations of periodic crystals, including the Crystal\nGraph with vertices at all atoms in a crystal unit cell. This work adapts the\nPointwise Distance Distribution for a simpler graph whose vertex set is not\nlarger than the asymmetric unit of a crystal structure. The new Distribution\nGraph reduces mean-absolute-error by 0.6\\%-12\\% while having 44\\%-88\\% of the\nnumber of vertices when compared to the crystal graph when applied on the\nMaterials Project and Jarvis-DFT datasets using CGCNN and ALIGNN. Methods for\nhyper-parameters selection for the graph are backed by the theoretical results\nof the Pointwise Distance Distribution and are then experimentally justified.\n", "link": "http://arxiv.org/abs/2212.11246v3", "date": "2024-05-07", "relevancy": 1.8573, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5463}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4501}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Material%20Property%20Prediction%20using%20Graphs%20based%20on%20Generically%20Complete%0A%20%20Isometry%20Invariants&body=Title%3A%20Material%20Property%20Prediction%20using%20Graphs%20based%20on%20Generically%20Complete%0A%20%20Isometry%20Invariants%0AAuthor%3A%20Jonathan%20Balasingham%20and%20Viktor%20Zamaraev%20and%20Vitaliy%20Kurlin%0AAbstract%3A%20%20%20The%20structure-property%20hypothesis%20says%20that%20the%20properties%20of%20all%20materials%0Aare%20determined%20by%20an%20underlying%20crystal%20structure.%20The%20main%20obstacle%20was%20the%0Aambiguity%20of%20conventional%20crystal%20representations%20based%20on%20incomplete%20or%0Adiscontinuous%20descriptors%20that%20allow%20false%20negatives%20or%20false%20positives.%20This%0Aambiguity%20was%20resolved%20by%20the%20ultra-fast%20Pointwise%20Distance%20Distribution%20%28PDD%29%2C%0Awhich%20distinguished%20all%20periodic%20structures%20in%20the%20world%27s%20largest%20collection%0Aof%20real%20materials%20%28Cambridge%20Structural%20Database%29.%20The%20state-of-the-art%20results%0Ain%20property%20predictions%20were%20previously%20achieved%20by%20graph%20neural%20networks%20based%0Aon%20various%20graph%20representations%20of%20periodic%20crystals%2C%20including%20the%20Crystal%0AGraph%20with%20vertices%20at%20all%20atoms%20in%20a%20crystal%20unit%20cell.%20This%20work%20adapts%20the%0APointwise%20Distance%20Distribution%20for%20a%20simpler%20graph%20whose%20vertex%20set%20is%20not%0Alarger%20than%20the%20asymmetric%20unit%20of%20a%20crystal%20structure.%20The%20new%20Distribution%0AGraph%20reduces%20mean-absolute-error%20by%200.6%5C%25-12%5C%25%20while%20having%2044%5C%25-88%5C%25%20of%20the%0Anumber%20of%20vertices%20when%20compared%20to%20the%20crystal%20graph%20when%20applied%20on%20the%0AMaterials%20Project%20and%20Jarvis-DFT%20datasets%20using%20CGCNN%20and%20ALIGNN.%20Methods%20for%0Ahyper-parameters%20selection%20for%20the%20graph%20are%20backed%20by%20the%20theoretical%20results%0Aof%20the%20Pointwise%20Distance%20Distribution%20and%20are%20then%20experimentally%20justified.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.11246v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaterial%2520Property%2520Prediction%2520using%2520Graphs%2520based%2520on%2520Generically%2520Complete%250A%2520%2520Isometry%2520Invariants%26entry.906535625%3DJonathan%2520Balasingham%2520and%2520Viktor%2520Zamaraev%2520and%2520Vitaliy%2520Kurlin%26entry.1292438233%3D%2520%2520The%2520structure-property%2520hypothesis%2520says%2520that%2520the%2520properties%2520of%2520all%2520materials%250Aare%2520determined%2520by%2520an%2520underlying%2520crystal%2520structure.%2520The%2520main%2520obstacle%2520was%2520the%250Aambiguity%2520of%2520conventional%2520crystal%2520representations%2520based%2520on%2520incomplete%2520or%250Adiscontinuous%2520descriptors%2520that%2520allow%2520false%2520negatives%2520or%2520false%2520positives.%2520This%250Aambiguity%2520was%2520resolved%2520by%2520the%2520ultra-fast%2520Pointwise%2520Distance%2520Distribution%2520%2528PDD%2529%252C%250Awhich%2520distinguished%2520all%2520periodic%2520structures%2520in%2520the%2520world%2527s%2520largest%2520collection%250Aof%2520real%2520materials%2520%2528Cambridge%2520Structural%2520Database%2529.%2520The%2520state-of-the-art%2520results%250Ain%2520property%2520predictions%2520were%2520previously%2520achieved%2520by%2520graph%2520neural%2520networks%2520based%250Aon%2520various%2520graph%2520representations%2520of%2520periodic%2520crystals%252C%2520including%2520the%2520Crystal%250AGraph%2520with%2520vertices%2520at%2520all%2520atoms%2520in%2520a%2520crystal%2520unit%2520cell.%2520This%2520work%2520adapts%2520the%250APointwise%2520Distance%2520Distribution%2520for%2520a%2520simpler%2520graph%2520whose%2520vertex%2520set%2520is%2520not%250Alarger%2520than%2520the%2520asymmetric%2520unit%2520of%2520a%2520crystal%2520structure.%2520The%2520new%2520Distribution%250AGraph%2520reduces%2520mean-absolute-error%2520by%25200.6%255C%2525-12%255C%2525%2520while%2520having%252044%255C%2525-88%255C%2525%2520of%2520the%250Anumber%2520of%2520vertices%2520when%2520compared%2520to%2520the%2520crystal%2520graph%2520when%2520applied%2520on%2520the%250AMaterials%2520Project%2520and%2520Jarvis-DFT%2520datasets%2520using%2520CGCNN%2520and%2520ALIGNN.%2520Methods%2520for%250Ahyper-parameters%2520selection%2520for%2520the%2520graph%2520are%2520backed%2520by%2520the%2520theoretical%2520results%250Aof%2520the%2520Pointwise%2520Distance%2520Distribution%2520and%2520are%2520then%2520experimentally%2520justified.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.11246v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Material%20Property%20Prediction%20using%20Graphs%20based%20on%20Generically%20Complete%0A%20%20Isometry%20Invariants&entry.906535625=Jonathan%20Balasingham%20and%20Viktor%20Zamaraev%20and%20Vitaliy%20Kurlin&entry.1292438233=%20%20The%20structure-property%20hypothesis%20says%20that%20the%20properties%20of%20all%20materials%0Aare%20determined%20by%20an%20underlying%20crystal%20structure.%20The%20main%20obstacle%20was%20the%0Aambiguity%20of%20conventional%20crystal%20representations%20based%20on%20incomplete%20or%0Adiscontinuous%20descriptors%20that%20allow%20false%20negatives%20or%20false%20positives.%20This%0Aambiguity%20was%20resolved%20by%20the%20ultra-fast%20Pointwise%20Distance%20Distribution%20%28PDD%29%2C%0Awhich%20distinguished%20all%20periodic%20structures%20in%20the%20world%27s%20largest%20collection%0Aof%20real%20materials%20%28Cambridge%20Structural%20Database%29.%20The%20state-of-the-art%20results%0Ain%20property%20predictions%20were%20previously%20achieved%20by%20graph%20neural%20networks%20based%0Aon%20various%20graph%20representations%20of%20periodic%20crystals%2C%20including%20the%20Crystal%0AGraph%20with%20vertices%20at%20all%20atoms%20in%20a%20crystal%20unit%20cell.%20This%20work%20adapts%20the%0APointwise%20Distance%20Distribution%20for%20a%20simpler%20graph%20whose%20vertex%20set%20is%20not%0Alarger%20than%20the%20asymmetric%20unit%20of%20a%20crystal%20structure.%20The%20new%20Distribution%0AGraph%20reduces%20mean-absolute-error%20by%200.6%5C%25-12%5C%25%20while%20having%2044%5C%25-88%5C%25%20of%20the%0Anumber%20of%20vertices%20when%20compared%20to%20the%20crystal%20graph%20when%20applied%20on%20the%0AMaterials%20Project%20and%20Jarvis-DFT%20datasets%20using%20CGCNN%20and%20ALIGNN.%20Methods%20for%0Ahyper-parameters%20selection%20for%20the%20graph%20are%20backed%20by%20the%20theoretical%20results%0Aof%20the%20Pointwise%20Distance%20Distribution%20and%20are%20then%20experimentally%20justified.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.11246v3&entry.124074799=Read"},
{"title": "$\\textbf{Splat-MOVER}$: Multi-Stage, Open-Vocabulary Robotic\n  Manipulation via Editable Gaussian Splatting", "author": "Ola Shorinwa and Johnathan Tucker and Aliyah Smith and Aiden Swann and Timothy Chen and Roya Firoozi and Monroe Kennedy III and Mac Schwager", "abstract": "  We present Splat-MOVER, a modular robotics stack for open-vocabulary robotic\nmanipulation, which leverages the editability of Gaussian Splatting (GSplat)\nscene representations to enable multi-stage manipulation tasks. Splat-MOVER\nconsists of: (i) $\\textit{ASK-Splat}$, a GSplat representation that distills\nlatent codes for language semantics and grasp affordance into the 3D scene.\nASK-Splat enables geometric, semantic, and affordance understanding of 3D\nscenes, which is critical for many robotics tasks; (ii) $\\textit{SEE-Splat}$, a\nreal-time scene-editing module using 3D semantic masking and infilling to\nvisualize the motions of objects that result from robot interactions in the\nreal-world. SEE-Splat creates a \"digital twin\" of the evolving environment\nthroughout the manipulation task; and (iii) $\\textit{Grasp-Splat}$, a grasp\ngeneration module that uses ASK-Splat and SEE-Splat to propose candidate grasps\nfor open-world objects. ASK-Splat is trained in real-time from RGB images in a\nbrief scanning phase prior to operation, while SEE-Splat and Grasp-Splat run in\nreal-time during operation. We demonstrate the superior performance of\nSplat-MOVER in hardware experiments on a Kinova robot compared to two recent\nbaselines in four single-stage, open-vocabulary manipulation tasks, as well as\nin four multi-stage manipulation tasks using the edited scene to reflect scene\nchanges due to prior manipulation stages, which is not possible with the\nexisting baselines. Code for this project and a link to the project page will\nbe made available soon.\n", "link": "http://arxiv.org/abs/2405.04378v1", "date": "2024-05-07", "relevancy": 1.8538, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6538}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6246}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%5Ctextbf%7BSplat-MOVER%7D%24%3A%20Multi-Stage%2C%20Open-Vocabulary%20Robotic%0A%20%20Manipulation%20via%20Editable%20Gaussian%20Splatting&body=Title%3A%20%24%5Ctextbf%7BSplat-MOVER%7D%24%3A%20Multi-Stage%2C%20Open-Vocabulary%20Robotic%0A%20%20Manipulation%20via%20Editable%20Gaussian%20Splatting%0AAuthor%3A%20Ola%20Shorinwa%20and%20Johnathan%20Tucker%20and%20Aliyah%20Smith%20and%20Aiden%20Swann%20and%20Timothy%20Chen%20and%20Roya%20Firoozi%20and%20Monroe%20Kennedy%20III%20and%20Mac%20Schwager%0AAbstract%3A%20%20%20We%20present%20Splat-MOVER%2C%20a%20modular%20robotics%20stack%20for%20open-vocabulary%20robotic%0Amanipulation%2C%20which%20leverages%20the%20editability%20of%20Gaussian%20Splatting%20%28GSplat%29%0Ascene%20representations%20to%20enable%20multi-stage%20manipulation%20tasks.%20Splat-MOVER%0Aconsists%20of%3A%20%28i%29%20%24%5Ctextit%7BASK-Splat%7D%24%2C%20a%20GSplat%20representation%20that%20distills%0Alatent%20codes%20for%20language%20semantics%20and%20grasp%20affordance%20into%20the%203D%20scene.%0AASK-Splat%20enables%20geometric%2C%20semantic%2C%20and%20affordance%20understanding%20of%203D%0Ascenes%2C%20which%20is%20critical%20for%20many%20robotics%20tasks%3B%20%28ii%29%20%24%5Ctextit%7BSEE-Splat%7D%24%2C%20a%0Areal-time%20scene-editing%20module%20using%203D%20semantic%20masking%20and%20infilling%20to%0Avisualize%20the%20motions%20of%20objects%20that%20result%20from%20robot%20interactions%20in%20the%0Areal-world.%20SEE-Splat%20creates%20a%20%22digital%20twin%22%20of%20the%20evolving%20environment%0Athroughout%20the%20manipulation%20task%3B%20and%20%28iii%29%20%24%5Ctextit%7BGrasp-Splat%7D%24%2C%20a%20grasp%0Ageneration%20module%20that%20uses%20ASK-Splat%20and%20SEE-Splat%20to%20propose%20candidate%20grasps%0Afor%20open-world%20objects.%20ASK-Splat%20is%20trained%20in%20real-time%20from%20RGB%20images%20in%20a%0Abrief%20scanning%20phase%20prior%20to%20operation%2C%20while%20SEE-Splat%20and%20Grasp-Splat%20run%20in%0Areal-time%20during%20operation.%20We%20demonstrate%20the%20superior%20performance%20of%0ASplat-MOVER%20in%20hardware%20experiments%20on%20a%20Kinova%20robot%20compared%20to%20two%20recent%0Abaselines%20in%20four%20single-stage%2C%20open-vocabulary%20manipulation%20tasks%2C%20as%20well%20as%0Ain%20four%20multi-stage%20manipulation%20tasks%20using%20the%20edited%20scene%20to%20reflect%20scene%0Achanges%20due%20to%20prior%20manipulation%20stages%2C%20which%20is%20not%20possible%20with%20the%0Aexisting%20baselines.%20Code%20for%20this%20project%20and%20a%20link%20to%20the%20project%20page%20will%0Abe%20made%20available%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04378v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%255Ctextbf%257BSplat-MOVER%257D%2524%253A%2520Multi-Stage%252C%2520Open-Vocabulary%2520Robotic%250A%2520%2520Manipulation%2520via%2520Editable%2520Gaussian%2520Splatting%26entry.906535625%3DOla%2520Shorinwa%2520and%2520Johnathan%2520Tucker%2520and%2520Aliyah%2520Smith%2520and%2520Aiden%2520Swann%2520and%2520Timothy%2520Chen%2520and%2520Roya%2520Firoozi%2520and%2520Monroe%2520Kennedy%2520III%2520and%2520Mac%2520Schwager%26entry.1292438233%3D%2520%2520We%2520present%2520Splat-MOVER%252C%2520a%2520modular%2520robotics%2520stack%2520for%2520open-vocabulary%2520robotic%250Amanipulation%252C%2520which%2520leverages%2520the%2520editability%2520of%2520Gaussian%2520Splatting%2520%2528GSplat%2529%250Ascene%2520representations%2520to%2520enable%2520multi-stage%2520manipulation%2520tasks.%2520Splat-MOVER%250Aconsists%2520of%253A%2520%2528i%2529%2520%2524%255Ctextit%257BASK-Splat%257D%2524%252C%2520a%2520GSplat%2520representation%2520that%2520distills%250Alatent%2520codes%2520for%2520language%2520semantics%2520and%2520grasp%2520affordance%2520into%2520the%25203D%2520scene.%250AASK-Splat%2520enables%2520geometric%252C%2520semantic%252C%2520and%2520affordance%2520understanding%2520of%25203D%250Ascenes%252C%2520which%2520is%2520critical%2520for%2520many%2520robotics%2520tasks%253B%2520%2528ii%2529%2520%2524%255Ctextit%257BSEE-Splat%257D%2524%252C%2520a%250Areal-time%2520scene-editing%2520module%2520using%25203D%2520semantic%2520masking%2520and%2520infilling%2520to%250Avisualize%2520the%2520motions%2520of%2520objects%2520that%2520result%2520from%2520robot%2520interactions%2520in%2520the%250Areal-world.%2520SEE-Splat%2520creates%2520a%2520%2522digital%2520twin%2522%2520of%2520the%2520evolving%2520environment%250Athroughout%2520the%2520manipulation%2520task%253B%2520and%2520%2528iii%2529%2520%2524%255Ctextit%257BGrasp-Splat%257D%2524%252C%2520a%2520grasp%250Ageneration%2520module%2520that%2520uses%2520ASK-Splat%2520and%2520SEE-Splat%2520to%2520propose%2520candidate%2520grasps%250Afor%2520open-world%2520objects.%2520ASK-Splat%2520is%2520trained%2520in%2520real-time%2520from%2520RGB%2520images%2520in%2520a%250Abrief%2520scanning%2520phase%2520prior%2520to%2520operation%252C%2520while%2520SEE-Splat%2520and%2520Grasp-Splat%2520run%2520in%250Areal-time%2520during%2520operation.%2520We%2520demonstrate%2520the%2520superior%2520performance%2520of%250ASplat-MOVER%2520in%2520hardware%2520experiments%2520on%2520a%2520Kinova%2520robot%2520compared%2520to%2520two%2520recent%250Abaselines%2520in%2520four%2520single-stage%252C%2520open-vocabulary%2520manipulation%2520tasks%252C%2520as%2520well%2520as%250Ain%2520four%2520multi-stage%2520manipulation%2520tasks%2520using%2520the%2520edited%2520scene%2520to%2520reflect%2520scene%250Achanges%2520due%2520to%2520prior%2520manipulation%2520stages%252C%2520which%2520is%2520not%2520possible%2520with%2520the%250Aexisting%2520baselines.%2520Code%2520for%2520this%2520project%2520and%2520a%2520link%2520to%2520the%2520project%2520page%2520will%250Abe%2520made%2520available%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04378v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%5Ctextbf%7BSplat-MOVER%7D%24%3A%20Multi-Stage%2C%20Open-Vocabulary%20Robotic%0A%20%20Manipulation%20via%20Editable%20Gaussian%20Splatting&entry.906535625=Ola%20Shorinwa%20and%20Johnathan%20Tucker%20and%20Aliyah%20Smith%20and%20Aiden%20Swann%20and%20Timothy%20Chen%20and%20Roya%20Firoozi%20and%20Monroe%20Kennedy%20III%20and%20Mac%20Schwager&entry.1292438233=%20%20We%20present%20Splat-MOVER%2C%20a%20modular%20robotics%20stack%20for%20open-vocabulary%20robotic%0Amanipulation%2C%20which%20leverages%20the%20editability%20of%20Gaussian%20Splatting%20%28GSplat%29%0Ascene%20representations%20to%20enable%20multi-stage%20manipulation%20tasks.%20Splat-MOVER%0Aconsists%20of%3A%20%28i%29%20%24%5Ctextit%7BASK-Splat%7D%24%2C%20a%20GSplat%20representation%20that%20distills%0Alatent%20codes%20for%20language%20semantics%20and%20grasp%20affordance%20into%20the%203D%20scene.%0AASK-Splat%20enables%20geometric%2C%20semantic%2C%20and%20affordance%20understanding%20of%203D%0Ascenes%2C%20which%20is%20critical%20for%20many%20robotics%20tasks%3B%20%28ii%29%20%24%5Ctextit%7BSEE-Splat%7D%24%2C%20a%0Areal-time%20scene-editing%20module%20using%203D%20semantic%20masking%20and%20infilling%20to%0Avisualize%20the%20motions%20of%20objects%20that%20result%20from%20robot%20interactions%20in%20the%0Areal-world.%20SEE-Splat%20creates%20a%20%22digital%20twin%22%20of%20the%20evolving%20environment%0Athroughout%20the%20manipulation%20task%3B%20and%20%28iii%29%20%24%5Ctextit%7BGrasp-Splat%7D%24%2C%20a%20grasp%0Ageneration%20module%20that%20uses%20ASK-Splat%20and%20SEE-Splat%20to%20propose%20candidate%20grasps%0Afor%20open-world%20objects.%20ASK-Splat%20is%20trained%20in%20real-time%20from%20RGB%20images%20in%20a%0Abrief%20scanning%20phase%20prior%20to%20operation%2C%20while%20SEE-Splat%20and%20Grasp-Splat%20run%20in%0Areal-time%20during%20operation.%20We%20demonstrate%20the%20superior%20performance%20of%0ASplat-MOVER%20in%20hardware%20experiments%20on%20a%20Kinova%20robot%20compared%20to%20two%20recent%0Abaselines%20in%20four%20single-stage%2C%20open-vocabulary%20manipulation%20tasks%2C%20as%20well%20as%0Ain%20four%20multi-stage%20manipulation%20tasks%20using%20the%20edited%20scene%20to%20reflect%20scene%0Achanges%20due%20to%20prior%20manipulation%20stages%2C%20which%20is%20not%20possible%20with%20the%0Aexisting%20baselines.%20Code%20for%20this%20project%20and%20a%20link%20to%20the%20project%20page%20will%0Abe%20made%20available%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04378v1&entry.124074799=Read"},
{"title": "Topicwise Separable Sentence Retrieval for Medical Report Generation", "author": "Junting Zhao and Yang Zhou and Zhihao Chen and Huazhu Fu and Liang Wan", "abstract": "  Automated radiology reporting holds immense clinical potential in alleviating\nthe burdensome workload of radiologists and mitigating diagnostic bias.\nRecently, retrieval-based report generation methods have garnered increasing\nattention due to their inherent advantages in terms of the quality and\nconsistency of generated reports. However, due to the long-tail distribution of\nthe training data, these models tend to learn frequently occurring sentences\nand topics, overlooking the rare topics. Regrettably, in many cases, the\ndescriptions of rare topics often indicate critical findings that should be\nmentioned in the report. To address this problem, we introduce a Topicwise\nSeparable Sentence Retrieval (Teaser) for medical report generation. To ensure\ncomprehensive learning of both common and rare topics, we categorize queries\ninto common and rare types to learn differentiated topics, and then propose\nTopic Contrastive Loss to effectively align topics and queries in the latent\nspace. Moreover, we integrate an Abstractor module following the extraction of\nvisual features, which aids the topic decoder in gaining a deeper understanding\nof the visual observational intent. Experiments on the MIMIC-CXR and IU X-ray\ndatasets demonstrate that Teaser surpasses state-of-the-art models, while also\nvalidating its capability to effectively represent rare topics and establish\nmore dependable correspondences between queries and topics.\n", "link": "http://arxiv.org/abs/2405.04175v1", "date": "2024-05-07", "relevancy": 1.7981, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4629}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.449}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topicwise%20Separable%20Sentence%20Retrieval%20for%20Medical%20Report%20Generation&body=Title%3A%20Topicwise%20Separable%20Sentence%20Retrieval%20for%20Medical%20Report%20Generation%0AAuthor%3A%20Junting%20Zhao%20and%20Yang%20Zhou%20and%20Zhihao%20Chen%20and%20Huazhu%20Fu%20and%20Liang%20Wan%0AAbstract%3A%20%20%20Automated%20radiology%20reporting%20holds%20immense%20clinical%20potential%20in%20alleviating%0Athe%20burdensome%20workload%20of%20radiologists%20and%20mitigating%20diagnostic%20bias.%0ARecently%2C%20retrieval-based%20report%20generation%20methods%20have%20garnered%20increasing%0Aattention%20due%20to%20their%20inherent%20advantages%20in%20terms%20of%20the%20quality%20and%0Aconsistency%20of%20generated%20reports.%20However%2C%20due%20to%20the%20long-tail%20distribution%20of%0Athe%20training%20data%2C%20these%20models%20tend%20to%20learn%20frequently%20occurring%20sentences%0Aand%20topics%2C%20overlooking%20the%20rare%20topics.%20Regrettably%2C%20in%20many%20cases%2C%20the%0Adescriptions%20of%20rare%20topics%20often%20indicate%20critical%20findings%20that%20should%20be%0Amentioned%20in%20the%20report.%20To%20address%20this%20problem%2C%20we%20introduce%20a%20Topicwise%0ASeparable%20Sentence%20Retrieval%20%28Teaser%29%20for%20medical%20report%20generation.%20To%20ensure%0Acomprehensive%20learning%20of%20both%20common%20and%20rare%20topics%2C%20we%20categorize%20queries%0Ainto%20common%20and%20rare%20types%20to%20learn%20differentiated%20topics%2C%20and%20then%20propose%0ATopic%20Contrastive%20Loss%20to%20effectively%20align%20topics%20and%20queries%20in%20the%20latent%0Aspace.%20Moreover%2C%20we%20integrate%20an%20Abstractor%20module%20following%20the%20extraction%20of%0Avisual%20features%2C%20which%20aids%20the%20topic%20decoder%20in%20gaining%20a%20deeper%20understanding%0Aof%20the%20visual%20observational%20intent.%20Experiments%20on%20the%20MIMIC-CXR%20and%20IU%20X-ray%0Adatasets%20demonstrate%20that%20Teaser%20surpasses%20state-of-the-art%20models%2C%20while%20also%0Avalidating%20its%20capability%20to%20effectively%20represent%20rare%20topics%20and%20establish%0Amore%20dependable%20correspondences%20between%20queries%20and%20topics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopicwise%2520Separable%2520Sentence%2520Retrieval%2520for%2520Medical%2520Report%2520Generation%26entry.906535625%3DJunting%2520Zhao%2520and%2520Yang%2520Zhou%2520and%2520Zhihao%2520Chen%2520and%2520Huazhu%2520Fu%2520and%2520Liang%2520Wan%26entry.1292438233%3D%2520%2520Automated%2520radiology%2520reporting%2520holds%2520immense%2520clinical%2520potential%2520in%2520alleviating%250Athe%2520burdensome%2520workload%2520of%2520radiologists%2520and%2520mitigating%2520diagnostic%2520bias.%250ARecently%252C%2520retrieval-based%2520report%2520generation%2520methods%2520have%2520garnered%2520increasing%250Aattention%2520due%2520to%2520their%2520inherent%2520advantages%2520in%2520terms%2520of%2520the%2520quality%2520and%250Aconsistency%2520of%2520generated%2520reports.%2520However%252C%2520due%2520to%2520the%2520long-tail%2520distribution%2520of%250Athe%2520training%2520data%252C%2520these%2520models%2520tend%2520to%2520learn%2520frequently%2520occurring%2520sentences%250Aand%2520topics%252C%2520overlooking%2520the%2520rare%2520topics.%2520Regrettably%252C%2520in%2520many%2520cases%252C%2520the%250Adescriptions%2520of%2520rare%2520topics%2520often%2520indicate%2520critical%2520findings%2520that%2520should%2520be%250Amentioned%2520in%2520the%2520report.%2520To%2520address%2520this%2520problem%252C%2520we%2520introduce%2520a%2520Topicwise%250ASeparable%2520Sentence%2520Retrieval%2520%2528Teaser%2529%2520for%2520medical%2520report%2520generation.%2520To%2520ensure%250Acomprehensive%2520learning%2520of%2520both%2520common%2520and%2520rare%2520topics%252C%2520we%2520categorize%2520queries%250Ainto%2520common%2520and%2520rare%2520types%2520to%2520learn%2520differentiated%2520topics%252C%2520and%2520then%2520propose%250ATopic%2520Contrastive%2520Loss%2520to%2520effectively%2520align%2520topics%2520and%2520queries%2520in%2520the%2520latent%250Aspace.%2520Moreover%252C%2520we%2520integrate%2520an%2520Abstractor%2520module%2520following%2520the%2520extraction%2520of%250Avisual%2520features%252C%2520which%2520aids%2520the%2520topic%2520decoder%2520in%2520gaining%2520a%2520deeper%2520understanding%250Aof%2520the%2520visual%2520observational%2520intent.%2520Experiments%2520on%2520the%2520MIMIC-CXR%2520and%2520IU%2520X-ray%250Adatasets%2520demonstrate%2520that%2520Teaser%2520surpasses%2520state-of-the-art%2520models%252C%2520while%2520also%250Avalidating%2520its%2520capability%2520to%2520effectively%2520represent%2520rare%2520topics%2520and%2520establish%250Amore%2520dependable%2520correspondences%2520between%2520queries%2520and%2520topics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topicwise%20Separable%20Sentence%20Retrieval%20for%20Medical%20Report%20Generation&entry.906535625=Junting%20Zhao%20and%20Yang%20Zhou%20and%20Zhihao%20Chen%20and%20Huazhu%20Fu%20and%20Liang%20Wan&entry.1292438233=%20%20Automated%20radiology%20reporting%20holds%20immense%20clinical%20potential%20in%20alleviating%0Athe%20burdensome%20workload%20of%20radiologists%20and%20mitigating%20diagnostic%20bias.%0ARecently%2C%20retrieval-based%20report%20generation%20methods%20have%20garnered%20increasing%0Aattention%20due%20to%20their%20inherent%20advantages%20in%20terms%20of%20the%20quality%20and%0Aconsistency%20of%20generated%20reports.%20However%2C%20due%20to%20the%20long-tail%20distribution%20of%0Athe%20training%20data%2C%20these%20models%20tend%20to%20learn%20frequently%20occurring%20sentences%0Aand%20topics%2C%20overlooking%20the%20rare%20topics.%20Regrettably%2C%20in%20many%20cases%2C%20the%0Adescriptions%20of%20rare%20topics%20often%20indicate%20critical%20findings%20that%20should%20be%0Amentioned%20in%20the%20report.%20To%20address%20this%20problem%2C%20we%20introduce%20a%20Topicwise%0ASeparable%20Sentence%20Retrieval%20%28Teaser%29%20for%20medical%20report%20generation.%20To%20ensure%0Acomprehensive%20learning%20of%20both%20common%20and%20rare%20topics%2C%20we%20categorize%20queries%0Ainto%20common%20and%20rare%20types%20to%20learn%20differentiated%20topics%2C%20and%20then%20propose%0ATopic%20Contrastive%20Loss%20to%20effectively%20align%20topics%20and%20queries%20in%20the%20latent%0Aspace.%20Moreover%2C%20we%20integrate%20an%20Abstractor%20module%20following%20the%20extraction%20of%0Avisual%20features%2C%20which%20aids%20the%20topic%20decoder%20in%20gaining%20a%20deeper%20understanding%0Aof%20the%20visual%20observational%20intent.%20Experiments%20on%20the%20MIMIC-CXR%20and%20IU%20X-ray%0Adatasets%20demonstrate%20that%20Teaser%20surpasses%20state-of-the-art%20models%2C%20while%20also%0Avalidating%20its%20capability%20to%20effectively%20represent%20rare%20topics%20and%20establish%0Amore%20dependable%20correspondences%20between%20queries%20and%20topics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04175v1&entry.124074799=Read"},
{"title": "Accelerating Material Property Prediction using Generically Complete\n  Isometry Invariants", "author": "Jonathan Balasingham and Viktor Zamaraev and Vitaliy Kurlin", "abstract": "  Periodic material or crystal property prediction using machine learning has\ngrown popular in recent years as it provides a computationally efficient\nreplacement for classical simulation methods. A crucial first step for any of\nthese algorithms is the representation used for a periodic crystal. While\nsimilar objects like molecules and proteins have a finite number of atoms and\ntheir representation can be built based upon a finite point cloud\ninterpretation, periodic crystals are unbounded in size, making their\nrepresentation more challenging. In the present work, we adapt the Pointwise\nDistance Distribution (PDD), a continuous and generically complete isometry\ninvariant for periodic point sets, as a representation for our learning\nalgorithm. The PDD distinguished all (more than 660 thousand) periodic crystals\nin the Cambridge Structural Database as purely periodic sets of points without\natomic types. We develop a transformer model with a modified self-attention\nmechanism that combines PDD with compositional information via a spatial\nencoding method. This model is tested on the crystals of the Materials Project\nand Jarvis-DFT databases and shown to produce accuracy on par with\nstate-of-the-art methods while being several times faster in both training and\nprediction time.\n", "link": "http://arxiv.org/abs/2401.15089v2", "date": "2024-05-07", "relevancy": 1.4273, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5373}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4598}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Material%20Property%20Prediction%20using%20Generically%20Complete%0A%20%20Isometry%20Invariants&body=Title%3A%20Accelerating%20Material%20Property%20Prediction%20using%20Generically%20Complete%0A%20%20Isometry%20Invariants%0AAuthor%3A%20Jonathan%20Balasingham%20and%20Viktor%20Zamaraev%20and%20Vitaliy%20Kurlin%0AAbstract%3A%20%20%20Periodic%20material%20or%20crystal%20property%20prediction%20using%20machine%20learning%20has%0Agrown%20popular%20in%20recent%20years%20as%20it%20provides%20a%20computationally%20efficient%0Areplacement%20for%20classical%20simulation%20methods.%20A%20crucial%20first%20step%20for%20any%20of%0Athese%20algorithms%20is%20the%20representation%20used%20for%20a%20periodic%20crystal.%20While%0Asimilar%20objects%20like%20molecules%20and%20proteins%20have%20a%20finite%20number%20of%20atoms%20and%0Atheir%20representation%20can%20be%20built%20based%20upon%20a%20finite%20point%20cloud%0Ainterpretation%2C%20periodic%20crystals%20are%20unbounded%20in%20size%2C%20making%20their%0Arepresentation%20more%20challenging.%20In%20the%20present%20work%2C%20we%20adapt%20the%20Pointwise%0ADistance%20Distribution%20%28PDD%29%2C%20a%20continuous%20and%20generically%20complete%20isometry%0Ainvariant%20for%20periodic%20point%20sets%2C%20as%20a%20representation%20for%20our%20learning%0Aalgorithm.%20The%20PDD%20distinguished%20all%20%28more%20than%20660%20thousand%29%20periodic%20crystals%0Ain%20the%20Cambridge%20Structural%20Database%20as%20purely%20periodic%20sets%20of%20points%20without%0Aatomic%20types.%20We%20develop%20a%20transformer%20model%20with%20a%20modified%20self-attention%0Amechanism%20that%20combines%20PDD%20with%20compositional%20information%20via%20a%20spatial%0Aencoding%20method.%20This%20model%20is%20tested%20on%20the%20crystals%20of%20the%20Materials%20Project%0Aand%20Jarvis-DFT%20databases%20and%20shown%20to%20produce%20accuracy%20on%20par%20with%0Astate-of-the-art%20methods%20while%20being%20several%20times%20faster%20in%20both%20training%20and%0Aprediction%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.15089v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Material%2520Property%2520Prediction%2520using%2520Generically%2520Complete%250A%2520%2520Isometry%2520Invariants%26entry.906535625%3DJonathan%2520Balasingham%2520and%2520Viktor%2520Zamaraev%2520and%2520Vitaliy%2520Kurlin%26entry.1292438233%3D%2520%2520Periodic%2520material%2520or%2520crystal%2520property%2520prediction%2520using%2520machine%2520learning%2520has%250Agrown%2520popular%2520in%2520recent%2520years%2520as%2520it%2520provides%2520a%2520computationally%2520efficient%250Areplacement%2520for%2520classical%2520simulation%2520methods.%2520A%2520crucial%2520first%2520step%2520for%2520any%2520of%250Athese%2520algorithms%2520is%2520the%2520representation%2520used%2520for%2520a%2520periodic%2520crystal.%2520While%250Asimilar%2520objects%2520like%2520molecules%2520and%2520proteins%2520have%2520a%2520finite%2520number%2520of%2520atoms%2520and%250Atheir%2520representation%2520can%2520be%2520built%2520based%2520upon%2520a%2520finite%2520point%2520cloud%250Ainterpretation%252C%2520periodic%2520crystals%2520are%2520unbounded%2520in%2520size%252C%2520making%2520their%250Arepresentation%2520more%2520challenging.%2520In%2520the%2520present%2520work%252C%2520we%2520adapt%2520the%2520Pointwise%250ADistance%2520Distribution%2520%2528PDD%2529%252C%2520a%2520continuous%2520and%2520generically%2520complete%2520isometry%250Ainvariant%2520for%2520periodic%2520point%2520sets%252C%2520as%2520a%2520representation%2520for%2520our%2520learning%250Aalgorithm.%2520The%2520PDD%2520distinguished%2520all%2520%2528more%2520than%2520660%2520thousand%2529%2520periodic%2520crystals%250Ain%2520the%2520Cambridge%2520Structural%2520Database%2520as%2520purely%2520periodic%2520sets%2520of%2520points%2520without%250Aatomic%2520types.%2520We%2520develop%2520a%2520transformer%2520model%2520with%2520a%2520modified%2520self-attention%250Amechanism%2520that%2520combines%2520PDD%2520with%2520compositional%2520information%2520via%2520a%2520spatial%250Aencoding%2520method.%2520This%2520model%2520is%2520tested%2520on%2520the%2520crystals%2520of%2520the%2520Materials%2520Project%250Aand%2520Jarvis-DFT%2520databases%2520and%2520shown%2520to%2520produce%2520accuracy%2520on%2520par%2520with%250Astate-of-the-art%2520methods%2520while%2520being%2520several%2520times%2520faster%2520in%2520both%2520training%2520and%250Aprediction%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.15089v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Material%20Property%20Prediction%20using%20Generically%20Complete%0A%20%20Isometry%20Invariants&entry.906535625=Jonathan%20Balasingham%20and%20Viktor%20Zamaraev%20and%20Vitaliy%20Kurlin&entry.1292438233=%20%20Periodic%20material%20or%20crystal%20property%20prediction%20using%20machine%20learning%20has%0Agrown%20popular%20in%20recent%20years%20as%20it%20provides%20a%20computationally%20efficient%0Areplacement%20for%20classical%20simulation%20methods.%20A%20crucial%20first%20step%20for%20any%20of%0Athese%20algorithms%20is%20the%20representation%20used%20for%20a%20periodic%20crystal.%20While%0Asimilar%20objects%20like%20molecules%20and%20proteins%20have%20a%20finite%20number%20of%20atoms%20and%0Atheir%20representation%20can%20be%20built%20based%20upon%20a%20finite%20point%20cloud%0Ainterpretation%2C%20periodic%20crystals%20are%20unbounded%20in%20size%2C%20making%20their%0Arepresentation%20more%20challenging.%20In%20the%20present%20work%2C%20we%20adapt%20the%20Pointwise%0ADistance%20Distribution%20%28PDD%29%2C%20a%20continuous%20and%20generically%20complete%20isometry%0Ainvariant%20for%20periodic%20point%20sets%2C%20as%20a%20representation%20for%20our%20learning%0Aalgorithm.%20The%20PDD%20distinguished%20all%20%28more%20than%20660%20thousand%29%20periodic%20crystals%0Ain%20the%20Cambridge%20Structural%20Database%20as%20purely%20periodic%20sets%20of%20points%20without%0Aatomic%20types.%20We%20develop%20a%20transformer%20model%20with%20a%20modified%20self-attention%0Amechanism%20that%20combines%20PDD%20with%20compositional%20information%20via%20a%20spatial%0Aencoding%20method.%20This%20model%20is%20tested%20on%20the%20crystals%20of%20the%20Materials%20Project%0Aand%20Jarvis-DFT%20databases%20and%20shown%20to%20produce%20accuracy%20on%20par%20with%0Astate-of-the-art%20methods%20while%20being%20several%20times%20faster%20in%20both%20training%20and%0Aprediction%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15089v2&entry.124074799=Read"},
{"title": "Monkeypox disease recognition model based on improved SE-InceptionV3", "author": "Junzhuo Chen and Zonghan Lu and Shitong Kang", "abstract": "  In the wake of the global spread of monkeypox, accurate disease recognition\nhas become crucial. This study introduces an improved SE-InceptionV3 model,\nembedding the SENet module and incorporating L2 regularization into the\nInceptionV3 framework to enhance monkeypox disease detection. Utilizing the\nKaggle monkeypox dataset, which includes images of monkeypox and similar skin\nconditions, our model demonstrates a noteworthy accuracy of 96.71% on the test\nset, outperforming conventional methods and deep learning models. The SENet\nmodules channel attention mechanism significantly elevates feature\nrepresentation, while L2 regularization ensures robust generalization.\nExtensive experiments validate the models superiority in precision, recall, and\nF1 score, highlighting its effectiveness in differentiating monkeypox lesions\nin diverse and complex cases. The study not only provides insights into the\napplication of advanced CNN architectures in medical diagnostics but also opens\navenues for further research in model optimization and hyperparameter tuning\nfor enhanced disease recognition. https://github.com/jzc777/SE-inceptionV3-L2\n", "link": "http://arxiv.org/abs/2403.10087v2", "date": "2024-05-07", "relevancy": 1.3212, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4443}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4428}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Monkeypox%20disease%20recognition%20model%20based%20on%20improved%20SE-InceptionV3&body=Title%3A%20Monkeypox%20disease%20recognition%20model%20based%20on%20improved%20SE-InceptionV3%0AAuthor%3A%20Junzhuo%20Chen%20and%20Zonghan%20Lu%20and%20Shitong%20Kang%0AAbstract%3A%20%20%20In%20the%20wake%20of%20the%20global%20spread%20of%20monkeypox%2C%20accurate%20disease%20recognition%0Ahas%20become%20crucial.%20This%20study%20introduces%20an%20improved%20SE-InceptionV3%20model%2C%0Aembedding%20the%20SENet%20module%20and%20incorporating%20L2%20regularization%20into%20the%0AInceptionV3%20framework%20to%20enhance%20monkeypox%20disease%20detection.%20Utilizing%20the%0AKaggle%20monkeypox%20dataset%2C%20which%20includes%20images%20of%20monkeypox%20and%20similar%20skin%0Aconditions%2C%20our%20model%20demonstrates%20a%20noteworthy%20accuracy%20of%2096.71%25%20on%20the%20test%0Aset%2C%20outperforming%20conventional%20methods%20and%20deep%20learning%20models.%20The%20SENet%0Amodules%20channel%20attention%20mechanism%20significantly%20elevates%20feature%0Arepresentation%2C%20while%20L2%20regularization%20ensures%20robust%20generalization.%0AExtensive%20experiments%20validate%20the%20models%20superiority%20in%20precision%2C%20recall%2C%20and%0AF1%20score%2C%20highlighting%20its%20effectiveness%20in%20differentiating%20monkeypox%20lesions%0Ain%20diverse%20and%20complex%20cases.%20The%20study%20not%20only%20provides%20insights%20into%20the%0Aapplication%20of%20advanced%20CNN%20architectures%20in%20medical%20diagnostics%20but%20also%20opens%0Aavenues%20for%20further%20research%20in%20model%20optimization%20and%20hyperparameter%20tuning%0Afor%20enhanced%20disease%20recognition.%20https%3A//github.com/jzc777/SE-inceptionV3-L2%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10087v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonkeypox%2520disease%2520recognition%2520model%2520based%2520on%2520improved%2520SE-InceptionV3%26entry.906535625%3DJunzhuo%2520Chen%2520and%2520Zonghan%2520Lu%2520and%2520Shitong%2520Kang%26entry.1292438233%3D%2520%2520In%2520the%2520wake%2520of%2520the%2520global%2520spread%2520of%2520monkeypox%252C%2520accurate%2520disease%2520recognition%250Ahas%2520become%2520crucial.%2520This%2520study%2520introduces%2520an%2520improved%2520SE-InceptionV3%2520model%252C%250Aembedding%2520the%2520SENet%2520module%2520and%2520incorporating%2520L2%2520regularization%2520into%2520the%250AInceptionV3%2520framework%2520to%2520enhance%2520monkeypox%2520disease%2520detection.%2520Utilizing%2520the%250AKaggle%2520monkeypox%2520dataset%252C%2520which%2520includes%2520images%2520of%2520monkeypox%2520and%2520similar%2520skin%250Aconditions%252C%2520our%2520model%2520demonstrates%2520a%2520noteworthy%2520accuracy%2520of%252096.71%2525%2520on%2520the%2520test%250Aset%252C%2520outperforming%2520conventional%2520methods%2520and%2520deep%2520learning%2520models.%2520The%2520SENet%250Amodules%2520channel%2520attention%2520mechanism%2520significantly%2520elevates%2520feature%250Arepresentation%252C%2520while%2520L2%2520regularization%2520ensures%2520robust%2520generalization.%250AExtensive%2520experiments%2520validate%2520the%2520models%2520superiority%2520in%2520precision%252C%2520recall%252C%2520and%250AF1%2520score%252C%2520highlighting%2520its%2520effectiveness%2520in%2520differentiating%2520monkeypox%2520lesions%250Ain%2520diverse%2520and%2520complex%2520cases.%2520The%2520study%2520not%2520only%2520provides%2520insights%2520into%2520the%250Aapplication%2520of%2520advanced%2520CNN%2520architectures%2520in%2520medical%2520diagnostics%2520but%2520also%2520opens%250Aavenues%2520for%2520further%2520research%2520in%2520model%2520optimization%2520and%2520hyperparameter%2520tuning%250Afor%2520enhanced%2520disease%2520recognition.%2520https%253A//github.com/jzc777/SE-inceptionV3-L2%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10087v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monkeypox%20disease%20recognition%20model%20based%20on%20improved%20SE-InceptionV3&entry.906535625=Junzhuo%20Chen%20and%20Zonghan%20Lu%20and%20Shitong%20Kang&entry.1292438233=%20%20In%20the%20wake%20of%20the%20global%20spread%20of%20monkeypox%2C%20accurate%20disease%20recognition%0Ahas%20become%20crucial.%20This%20study%20introduces%20an%20improved%20SE-InceptionV3%20model%2C%0Aembedding%20the%20SENet%20module%20and%20incorporating%20L2%20regularization%20into%20the%0AInceptionV3%20framework%20to%20enhance%20monkeypox%20disease%20detection.%20Utilizing%20the%0AKaggle%20monkeypox%20dataset%2C%20which%20includes%20images%20of%20monkeypox%20and%20similar%20skin%0Aconditions%2C%20our%20model%20demonstrates%20a%20noteworthy%20accuracy%20of%2096.71%25%20on%20the%20test%0Aset%2C%20outperforming%20conventional%20methods%20and%20deep%20learning%20models.%20The%20SENet%0Amodules%20channel%20attention%20mechanism%20significantly%20elevates%20feature%0Arepresentation%2C%20while%20L2%20regularization%20ensures%20robust%20generalization.%0AExtensive%20experiments%20validate%20the%20models%20superiority%20in%20precision%2C%20recall%2C%20and%0AF1%20score%2C%20highlighting%20its%20effectiveness%20in%20differentiating%20monkeypox%20lesions%0Ain%20diverse%20and%20complex%20cases.%20The%20study%20not%20only%20provides%20insights%20into%20the%0Aapplication%20of%20advanced%20CNN%20architectures%20in%20medical%20diagnostics%20but%20also%20opens%0Aavenues%20for%20further%20research%20in%20model%20optimization%20and%20hyperparameter%20tuning%0Afor%20enhanced%20disease%20recognition.%20https%3A//github.com/jzc777/SE-inceptionV3-L2%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10087v2&entry.124074799=Read"},
{"title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts\n  Language Model", "author": " DeepSeek-AI", "abstract": "  We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model\ncharacterized by economical training and efficient inference. It comprises 236B\ntotal parameters, of which 21B are activated for each token, and supports a\ncontext length of 128K tokens. DeepSeek-V2 adopts innovative architectures\nincluding Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees\nefficient inference through significantly compressing the Key-Value (KV) cache\ninto a latent vector, while DeepSeekMoE enables training strong models at an\neconomical cost through sparse computation. Compared with DeepSeek 67B,\nDeepSeek-V2 achieves significantly stronger performance, and meanwhile saves\n42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum\ngeneration throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality\nand multi-source corpus consisting of 8.1T tokens, and further perform\nSupervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock\nits potential. Evaluation results show that, even with only 21B activated\nparameters, DeepSeek-V2 and its chat versions still achieve top-tier\nperformance among open-source models. The model checkpoints are available at\n\"https://github.com/deepseek-ai/DeepSeek-V2\".\n", "link": "http://arxiv.org/abs/2405.04434v1", "date": "2024-05-07", "relevancy": 1.4518, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4992}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4803}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepSeek-V2%3A%20A%20Strong%2C%20Economical%2C%20and%20Efficient%20Mixture-of-Experts%0A%20%20Language%20Model&body=Title%3A%20DeepSeek-V2%3A%20A%20Strong%2C%20Economical%2C%20and%20Efficient%20Mixture-of-Experts%0A%20%20Language%20Model%0AAuthor%3A%20%20DeepSeek-AI%0AAbstract%3A%20%20%20We%20present%20DeepSeek-V2%2C%20a%20strong%20Mixture-of-Experts%20%28MoE%29%20language%20model%0Acharacterized%20by%20economical%20training%20and%20efficient%20inference.%20It%20comprises%20236B%0Atotal%20parameters%2C%20of%20which%2021B%20are%20activated%20for%20each%20token%2C%20and%20supports%20a%0Acontext%20length%20of%20128K%20tokens.%20DeepSeek-V2%20adopts%20innovative%20architectures%0Aincluding%20Multi-head%20Latent%20Attention%20%28MLA%29%20and%20DeepSeekMoE.%20MLA%20guarantees%0Aefficient%20inference%20through%20significantly%20compressing%20the%20Key-Value%20%28KV%29%20cache%0Ainto%20a%20latent%20vector%2C%20while%20DeepSeekMoE%20enables%20training%20strong%20models%20at%20an%0Aeconomical%20cost%20through%20sparse%20computation.%20Compared%20with%20DeepSeek%2067B%2C%0ADeepSeek-V2%20achieves%20significantly%20stronger%20performance%2C%20and%20meanwhile%20saves%0A42.5%25%20of%20training%20costs%2C%20reduces%20the%20KV%20cache%20by%2093.3%25%2C%20and%20boosts%20the%20maximum%0Ageneration%20throughput%20to%205.76%20times.%20We%20pretrain%20DeepSeek-V2%20on%20a%20high-quality%0Aand%20multi-source%20corpus%20consisting%20of%208.1T%20tokens%2C%20and%20further%20perform%0ASupervised%20Fine-Tuning%20%28SFT%29%20and%20Reinforcement%20Learning%20%28RL%29%20to%20fully%20unlock%0Aits%20potential.%20Evaluation%20results%20show%20that%2C%20even%20with%20only%2021B%20activated%0Aparameters%2C%20DeepSeek-V2%20and%20its%20chat%20versions%20still%20achieve%20top-tier%0Aperformance%20among%20open-source%20models.%20The%20model%20checkpoints%20are%20available%20at%0A%22https%3A//github.com/deepseek-ai/DeepSeek-V2%22.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepSeek-V2%253A%2520A%2520Strong%252C%2520Economical%252C%2520and%2520Efficient%2520Mixture-of-Experts%250A%2520%2520Language%2520Model%26entry.906535625%3D%2520DeepSeek-AI%26entry.1292438233%3D%2520%2520We%2520present%2520DeepSeek-V2%252C%2520a%2520strong%2520Mixture-of-Experts%2520%2528MoE%2529%2520language%2520model%250Acharacterized%2520by%2520economical%2520training%2520and%2520efficient%2520inference.%2520It%2520comprises%2520236B%250Atotal%2520parameters%252C%2520of%2520which%252021B%2520are%2520activated%2520for%2520each%2520token%252C%2520and%2520supports%2520a%250Acontext%2520length%2520of%2520128K%2520tokens.%2520DeepSeek-V2%2520adopts%2520innovative%2520architectures%250Aincluding%2520Multi-head%2520Latent%2520Attention%2520%2528MLA%2529%2520and%2520DeepSeekMoE.%2520MLA%2520guarantees%250Aefficient%2520inference%2520through%2520significantly%2520compressing%2520the%2520Key-Value%2520%2528KV%2529%2520cache%250Ainto%2520a%2520latent%2520vector%252C%2520while%2520DeepSeekMoE%2520enables%2520training%2520strong%2520models%2520at%2520an%250Aeconomical%2520cost%2520through%2520sparse%2520computation.%2520Compared%2520with%2520DeepSeek%252067B%252C%250ADeepSeek-V2%2520achieves%2520significantly%2520stronger%2520performance%252C%2520and%2520meanwhile%2520saves%250A42.5%2525%2520of%2520training%2520costs%252C%2520reduces%2520the%2520KV%2520cache%2520by%252093.3%2525%252C%2520and%2520boosts%2520the%2520maximum%250Ageneration%2520throughput%2520to%25205.76%2520times.%2520We%2520pretrain%2520DeepSeek-V2%2520on%2520a%2520high-quality%250Aand%2520multi-source%2520corpus%2520consisting%2520of%25208.1T%2520tokens%252C%2520and%2520further%2520perform%250ASupervised%2520Fine-Tuning%2520%2528SFT%2529%2520and%2520Reinforcement%2520Learning%2520%2528RL%2529%2520to%2520fully%2520unlock%250Aits%2520potential.%2520Evaluation%2520results%2520show%2520that%252C%2520even%2520with%2520only%252021B%2520activated%250Aparameters%252C%2520DeepSeek-V2%2520and%2520its%2520chat%2520versions%2520still%2520achieve%2520top-tier%250Aperformance%2520among%2520open-source%2520models.%2520The%2520model%2520checkpoints%2520are%2520available%2520at%250A%2522https%253A//github.com/deepseek-ai/DeepSeek-V2%2522.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepSeek-V2%3A%20A%20Strong%2C%20Economical%2C%20and%20Efficient%20Mixture-of-Experts%0A%20%20Language%20Model&entry.906535625=%20DeepSeek-AI&entry.1292438233=%20%20We%20present%20DeepSeek-V2%2C%20a%20strong%20Mixture-of-Experts%20%28MoE%29%20language%20model%0Acharacterized%20by%20economical%20training%20and%20efficient%20inference.%20It%20comprises%20236B%0Atotal%20parameters%2C%20of%20which%2021B%20are%20activated%20for%20each%20token%2C%20and%20supports%20a%0Acontext%20length%20of%20128K%20tokens.%20DeepSeek-V2%20adopts%20innovative%20architectures%0Aincluding%20Multi-head%20Latent%20Attention%20%28MLA%29%20and%20DeepSeekMoE.%20MLA%20guarantees%0Aefficient%20inference%20through%20significantly%20compressing%20the%20Key-Value%20%28KV%29%20cache%0Ainto%20a%20latent%20vector%2C%20while%20DeepSeekMoE%20enables%20training%20strong%20models%20at%20an%0Aeconomical%20cost%20through%20sparse%20computation.%20Compared%20with%20DeepSeek%2067B%2C%0ADeepSeek-V2%20achieves%20significantly%20stronger%20performance%2C%20and%20meanwhile%20saves%0A42.5%25%20of%20training%20costs%2C%20reduces%20the%20KV%20cache%20by%2093.3%25%2C%20and%20boosts%20the%20maximum%0Ageneration%20throughput%20to%205.76%20times.%20We%20pretrain%20DeepSeek-V2%20on%20a%20high-quality%0Aand%20multi-source%20corpus%20consisting%20of%208.1T%20tokens%2C%20and%20further%20perform%0ASupervised%20Fine-Tuning%20%28SFT%29%20and%20Reinforcement%20Learning%20%28RL%29%20to%20fully%20unlock%0Aits%20potential.%20Evaluation%20results%20show%20that%2C%20even%20with%20only%2021B%20activated%0Aparameters%2C%20DeepSeek-V2%20and%20its%20chat%20versions%20still%20achieve%20top-tier%0Aperformance%20among%20open-source%20models.%20The%20model%20checkpoints%20are%20available%20at%0A%22https%3A//github.com/deepseek-ai/DeepSeek-V2%22.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04434v1&entry.124074799=Read"},
{"title": "xLSTM: Extended Long Short-Term Memory", "author": "Maximilian Beck and Korbinian P\u00f6ppel and Markus Spanring and Andreas Auer and Oleksandra Prudnikova and Michael Kopp and G\u00fcnter Klambauer and Johannes Brandstetter and Sepp Hochreiter", "abstract": "  In the 1990s, the constant error carousel and gating were introduced as the\ncentral ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have\nstood the test of time and contributed to numerous deep learning success\nstories, in particular they constituted the first Large Language Models (LLMs).\nHowever, the advent of the Transformer technology with parallelizable\nself-attention at its core marked the dawn of a new era, outpacing LSTMs at\nscale. We now raise a simple question: How far do we get in language modeling\nwhen scaling LSTMs to billions of parameters, leveraging the latest techniques\nfrom modern LLMs, but mitigating known limitations of LSTMs? Firstly, we\nintroduce exponential gating with appropriate normalization and stabilization\ntechniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM\nwith a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that\nis fully parallelizable with a matrix memory and a covariance update rule.\nIntegrating these LSTM extensions into residual block backbones yields xLSTM\nblocks that are then residually stacked into xLSTM architectures. Exponential\ngating and modified memory structures boost xLSTM capabilities to perform\nfavorably when compared to state-of-the-art Transformers and State Space\nModels, both in performance and scaling.\n", "link": "http://arxiv.org/abs/2405.04517v1", "date": "2024-05-07", "relevancy": 1.4555, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5066}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4862}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20xLSTM%3A%20Extended%20Long%20Short-Term%20Memory&body=Title%3A%20xLSTM%3A%20Extended%20Long%20Short-Term%20Memory%0AAuthor%3A%20Maximilian%20Beck%20and%20Korbinian%20P%C3%B6ppel%20and%20Markus%20Spanring%20and%20Andreas%20Auer%20and%20Oleksandra%20Prudnikova%20and%20Michael%20Kopp%20and%20G%C3%BCnter%20Klambauer%20and%20Johannes%20Brandstetter%20and%20Sepp%20Hochreiter%0AAbstract%3A%20%20%20In%20the%201990s%2C%20the%20constant%20error%20carousel%20and%20gating%20were%20introduced%20as%20the%0Acentral%20ideas%20of%20the%20Long%20Short-Term%20Memory%20%28LSTM%29.%20Since%20then%2C%20LSTMs%20have%0Astood%20the%20test%20of%20time%20and%20contributed%20to%20numerous%20deep%20learning%20success%0Astories%2C%20in%20particular%20they%20constituted%20the%20first%20Large%20Language%20Models%20%28LLMs%29.%0AHowever%2C%20the%20advent%20of%20the%20Transformer%20technology%20with%20parallelizable%0Aself-attention%20at%20its%20core%20marked%20the%20dawn%20of%20a%20new%20era%2C%20outpacing%20LSTMs%20at%0Ascale.%20We%20now%20raise%20a%20simple%20question%3A%20How%20far%20do%20we%20get%20in%20language%20modeling%0Awhen%20scaling%20LSTMs%20to%20billions%20of%20parameters%2C%20leveraging%20the%20latest%20techniques%0Afrom%20modern%20LLMs%2C%20but%20mitigating%20known%20limitations%20of%20LSTMs%3F%20Firstly%2C%20we%0Aintroduce%20exponential%20gating%20with%20appropriate%20normalization%20and%20stabilization%0Atechniques.%20Secondly%2C%20we%20modify%20the%20LSTM%20memory%20structure%2C%20obtaining%3A%20%28i%29%20sLSTM%0Awith%20a%20scalar%20memory%2C%20a%20scalar%20update%2C%20and%20new%20memory%20mixing%2C%20%28ii%29%20mLSTM%20that%0Ais%20fully%20parallelizable%20with%20a%20matrix%20memory%20and%20a%20covariance%20update%20rule.%0AIntegrating%20these%20LSTM%20extensions%20into%20residual%20block%20backbones%20yields%20xLSTM%0Ablocks%20that%20are%20then%20residually%20stacked%20into%20xLSTM%20architectures.%20Exponential%0Agating%20and%20modified%20memory%20structures%20boost%20xLSTM%20capabilities%20to%20perform%0Afavorably%20when%20compared%20to%20state-of-the-art%20Transformers%20and%20State%20Space%0AModels%2C%20both%20in%20performance%20and%20scaling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DxLSTM%253A%2520Extended%2520Long%2520Short-Term%2520Memory%26entry.906535625%3DMaximilian%2520Beck%2520and%2520Korbinian%2520P%25C3%25B6ppel%2520and%2520Markus%2520Spanring%2520and%2520Andreas%2520Auer%2520and%2520Oleksandra%2520Prudnikova%2520and%2520Michael%2520Kopp%2520and%2520G%25C3%25BCnter%2520Klambauer%2520and%2520Johannes%2520Brandstetter%2520and%2520Sepp%2520Hochreiter%26entry.1292438233%3D%2520%2520In%2520the%25201990s%252C%2520the%2520constant%2520error%2520carousel%2520and%2520gating%2520were%2520introduced%2520as%2520the%250Acentral%2520ideas%2520of%2520the%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529.%2520Since%2520then%252C%2520LSTMs%2520have%250Astood%2520the%2520test%2520of%2520time%2520and%2520contributed%2520to%2520numerous%2520deep%2520learning%2520success%250Astories%252C%2520in%2520particular%2520they%2520constituted%2520the%2520first%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%250AHowever%252C%2520the%2520advent%2520of%2520the%2520Transformer%2520technology%2520with%2520parallelizable%250Aself-attention%2520at%2520its%2520core%2520marked%2520the%2520dawn%2520of%2520a%2520new%2520era%252C%2520outpacing%2520LSTMs%2520at%250Ascale.%2520We%2520now%2520raise%2520a%2520simple%2520question%253A%2520How%2520far%2520do%2520we%2520get%2520in%2520language%2520modeling%250Awhen%2520scaling%2520LSTMs%2520to%2520billions%2520of%2520parameters%252C%2520leveraging%2520the%2520latest%2520techniques%250Afrom%2520modern%2520LLMs%252C%2520but%2520mitigating%2520known%2520limitations%2520of%2520LSTMs%253F%2520Firstly%252C%2520we%250Aintroduce%2520exponential%2520gating%2520with%2520appropriate%2520normalization%2520and%2520stabilization%250Atechniques.%2520Secondly%252C%2520we%2520modify%2520the%2520LSTM%2520memory%2520structure%252C%2520obtaining%253A%2520%2528i%2529%2520sLSTM%250Awith%2520a%2520scalar%2520memory%252C%2520a%2520scalar%2520update%252C%2520and%2520new%2520memory%2520mixing%252C%2520%2528ii%2529%2520mLSTM%2520that%250Ais%2520fully%2520parallelizable%2520with%2520a%2520matrix%2520memory%2520and%2520a%2520covariance%2520update%2520rule.%250AIntegrating%2520these%2520LSTM%2520extensions%2520into%2520residual%2520block%2520backbones%2520yields%2520xLSTM%250Ablocks%2520that%2520are%2520then%2520residually%2520stacked%2520into%2520xLSTM%2520architectures.%2520Exponential%250Agating%2520and%2520modified%2520memory%2520structures%2520boost%2520xLSTM%2520capabilities%2520to%2520perform%250Afavorably%2520when%2520compared%2520to%2520state-of-the-art%2520Transformers%2520and%2520State%2520Space%250AModels%252C%2520both%2520in%2520performance%2520and%2520scaling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=xLSTM%3A%20Extended%20Long%20Short-Term%20Memory&entry.906535625=Maximilian%20Beck%20and%20Korbinian%20P%C3%B6ppel%20and%20Markus%20Spanring%20and%20Andreas%20Auer%20and%20Oleksandra%20Prudnikova%20and%20Michael%20Kopp%20and%20G%C3%BCnter%20Klambauer%20and%20Johannes%20Brandstetter%20and%20Sepp%20Hochreiter&entry.1292438233=%20%20In%20the%201990s%2C%20the%20constant%20error%20carousel%20and%20gating%20were%20introduced%20as%20the%0Acentral%20ideas%20of%20the%20Long%20Short-Term%20Memory%20%28LSTM%29.%20Since%20then%2C%20LSTMs%20have%0Astood%20the%20test%20of%20time%20and%20contributed%20to%20numerous%20deep%20learning%20success%0Astories%2C%20in%20particular%20they%20constituted%20the%20first%20Large%20Language%20Models%20%28LLMs%29.%0AHowever%2C%20the%20advent%20of%20the%20Transformer%20technology%20with%20parallelizable%0Aself-attention%20at%20its%20core%20marked%20the%20dawn%20of%20a%20new%20era%2C%20outpacing%20LSTMs%20at%0Ascale.%20We%20now%20raise%20a%20simple%20question%3A%20How%20far%20do%20we%20get%20in%20language%20modeling%0Awhen%20scaling%20LSTMs%20to%20billions%20of%20parameters%2C%20leveraging%20the%20latest%20techniques%0Afrom%20modern%20LLMs%2C%20but%20mitigating%20known%20limitations%20of%20LSTMs%3F%20Firstly%2C%20we%0Aintroduce%20exponential%20gating%20with%20appropriate%20normalization%20and%20stabilization%0Atechniques.%20Secondly%2C%20we%20modify%20the%20LSTM%20memory%20structure%2C%20obtaining%3A%20%28i%29%20sLSTM%0Awith%20a%20scalar%20memory%2C%20a%20scalar%20update%2C%20and%20new%20memory%20mixing%2C%20%28ii%29%20mLSTM%20that%0Ais%20fully%20parallelizable%20with%20a%20matrix%20memory%20and%20a%20covariance%20update%20rule.%0AIntegrating%20these%20LSTM%20extensions%20into%20residual%20block%20backbones%20yields%20xLSTM%0Ablocks%20that%20are%20then%20residually%20stacked%20into%20xLSTM%20architectures.%20Exponential%0Agating%20and%20modified%20memory%20structures%20boost%20xLSTM%20capabilities%20to%20perform%0Afavorably%20when%20compared%20to%20state-of-the-art%20Transformers%20and%20State%20Space%0AModels%2C%20both%20in%20performance%20and%20scaling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04517v1&entry.124074799=Read"},
{"title": "Bridging the Synthetic-to-Authentic Gap: Distortion-Guided Unsupervised\n  Domain Adaptation for Blind Image Quality Assessment", "author": "Aobo Li and Jinjian Wu and Yongxu Liu and Leida Li", "abstract": "  The annotation of blind image quality assessment (BIQA) is labor-intensive\nand time-consuming, especially for authentic images. Training on synthetic data\nis expected to be beneficial, but synthetically trained models often suffer\nfrom poor generalization in real domains due to domain gaps. In this work, we\nmake a key observation that introducing more distortion types in the synthetic\ndataset may not improve or even be harmful to generalizing authentic image\nquality assessment. To solve this challenge, we propose distortion-guided\nunsupervised domain adaptation for BIQA (DGQA), a novel framework that\nleverages adaptive multi-domain selection via prior knowledge from distortion\nto match the data distribution between the source domains and the target\ndomain, thereby reducing negative transfer from the outlier source domains.\nExtensive experiments on two cross-domain settings (synthetic distortion to\nauthentic distortion and synthetic distortion to algorithmic distortion) have\ndemonstrated the effectiveness of our proposed DGQA. Besides, DGQA is\northogonal to existing model-based BIQA methods, and can be used in combination\nwith such models to improve performance with less training data.\n", "link": "http://arxiv.org/abs/2405.04167v1", "date": "2024-05-07", "relevancy": 1.6264, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5489}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.541}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20the%20Synthetic-to-Authentic%20Gap%3A%20Distortion-Guided%20Unsupervised%0A%20%20Domain%20Adaptation%20for%20Blind%20Image%20Quality%20Assessment&body=Title%3A%20Bridging%20the%20Synthetic-to-Authentic%20Gap%3A%20Distortion-Guided%20Unsupervised%0A%20%20Domain%20Adaptation%20for%20Blind%20Image%20Quality%20Assessment%0AAuthor%3A%20Aobo%20Li%20and%20Jinjian%20Wu%20and%20Yongxu%20Liu%20and%20Leida%20Li%0AAbstract%3A%20%20%20The%20annotation%20of%20blind%20image%20quality%20assessment%20%28BIQA%29%20is%20labor-intensive%0Aand%20time-consuming%2C%20especially%20for%20authentic%20images.%20Training%20on%20synthetic%20data%0Ais%20expected%20to%20be%20beneficial%2C%20but%20synthetically%20trained%20models%20often%20suffer%0Afrom%20poor%20generalization%20in%20real%20domains%20due%20to%20domain%20gaps.%20In%20this%20work%2C%20we%0Amake%20a%20key%20observation%20that%20introducing%20more%20distortion%20types%20in%20the%20synthetic%0Adataset%20may%20not%20improve%20or%20even%20be%20harmful%20to%20generalizing%20authentic%20image%0Aquality%20assessment.%20To%20solve%20this%20challenge%2C%20we%20propose%20distortion-guided%0Aunsupervised%20domain%20adaptation%20for%20BIQA%20%28DGQA%29%2C%20a%20novel%20framework%20that%0Aleverages%20adaptive%20multi-domain%20selection%20via%20prior%20knowledge%20from%20distortion%0Ato%20match%20the%20data%20distribution%20between%20the%20source%20domains%20and%20the%20target%0Adomain%2C%20thereby%20reducing%20negative%20transfer%20from%20the%20outlier%20source%20domains.%0AExtensive%20experiments%20on%20two%20cross-domain%20settings%20%28synthetic%20distortion%20to%0Aauthentic%20distortion%20and%20synthetic%20distortion%20to%20algorithmic%20distortion%29%20have%0Ademonstrated%20the%20effectiveness%20of%20our%20proposed%20DGQA.%20Besides%2C%20DGQA%20is%0Aorthogonal%20to%20existing%20model-based%20BIQA%20methods%2C%20and%20can%20be%20used%20in%20combination%0Awith%20such%20models%20to%20improve%20performance%20with%20less%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04167v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520the%2520Synthetic-to-Authentic%2520Gap%253A%2520Distortion-Guided%2520Unsupervised%250A%2520%2520Domain%2520Adaptation%2520for%2520Blind%2520Image%2520Quality%2520Assessment%26entry.906535625%3DAobo%2520Li%2520and%2520Jinjian%2520Wu%2520and%2520Yongxu%2520Liu%2520and%2520Leida%2520Li%26entry.1292438233%3D%2520%2520The%2520annotation%2520of%2520blind%2520image%2520quality%2520assessment%2520%2528BIQA%2529%2520is%2520labor-intensive%250Aand%2520time-consuming%252C%2520especially%2520for%2520authentic%2520images.%2520Training%2520on%2520synthetic%2520data%250Ais%2520expected%2520to%2520be%2520beneficial%252C%2520but%2520synthetically%2520trained%2520models%2520often%2520suffer%250Afrom%2520poor%2520generalization%2520in%2520real%2520domains%2520due%2520to%2520domain%2520gaps.%2520In%2520this%2520work%252C%2520we%250Amake%2520a%2520key%2520observation%2520that%2520introducing%2520more%2520distortion%2520types%2520in%2520the%2520synthetic%250Adataset%2520may%2520not%2520improve%2520or%2520even%2520be%2520harmful%2520to%2520generalizing%2520authentic%2520image%250Aquality%2520assessment.%2520To%2520solve%2520this%2520challenge%252C%2520we%2520propose%2520distortion-guided%250Aunsupervised%2520domain%2520adaptation%2520for%2520BIQA%2520%2528DGQA%2529%252C%2520a%2520novel%2520framework%2520that%250Aleverages%2520adaptive%2520multi-domain%2520selection%2520via%2520prior%2520knowledge%2520from%2520distortion%250Ato%2520match%2520the%2520data%2520distribution%2520between%2520the%2520source%2520domains%2520and%2520the%2520target%250Adomain%252C%2520thereby%2520reducing%2520negative%2520transfer%2520from%2520the%2520outlier%2520source%2520domains.%250AExtensive%2520experiments%2520on%2520two%2520cross-domain%2520settings%2520%2528synthetic%2520distortion%2520to%250Aauthentic%2520distortion%2520and%2520synthetic%2520distortion%2520to%2520algorithmic%2520distortion%2529%2520have%250Ademonstrated%2520the%2520effectiveness%2520of%2520our%2520proposed%2520DGQA.%2520Besides%252C%2520DGQA%2520is%250Aorthogonal%2520to%2520existing%2520model-based%2520BIQA%2520methods%252C%2520and%2520can%2520be%2520used%2520in%2520combination%250Awith%2520such%2520models%2520to%2520improve%2520performance%2520with%2520less%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04167v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20the%20Synthetic-to-Authentic%20Gap%3A%20Distortion-Guided%20Unsupervised%0A%20%20Domain%20Adaptation%20for%20Blind%20Image%20Quality%20Assessment&entry.906535625=Aobo%20Li%20and%20Jinjian%20Wu%20and%20Yongxu%20Liu%20and%20Leida%20Li&entry.1292438233=%20%20The%20annotation%20of%20blind%20image%20quality%20assessment%20%28BIQA%29%20is%20labor-intensive%0Aand%20time-consuming%2C%20especially%20for%20authentic%20images.%20Training%20on%20synthetic%20data%0Ais%20expected%20to%20be%20beneficial%2C%20but%20synthetically%20trained%20models%20often%20suffer%0Afrom%20poor%20generalization%20in%20real%20domains%20due%20to%20domain%20gaps.%20In%20this%20work%2C%20we%0Amake%20a%20key%20observation%20that%20introducing%20more%20distortion%20types%20in%20the%20synthetic%0Adataset%20may%20not%20improve%20or%20even%20be%20harmful%20to%20generalizing%20authentic%20image%0Aquality%20assessment.%20To%20solve%20this%20challenge%2C%20we%20propose%20distortion-guided%0Aunsupervised%20domain%20adaptation%20for%20BIQA%20%28DGQA%29%2C%20a%20novel%20framework%20that%0Aleverages%20adaptive%20multi-domain%20selection%20via%20prior%20knowledge%20from%20distortion%0Ato%20match%20the%20data%20distribution%20between%20the%20source%20domains%20and%20the%20target%0Adomain%2C%20thereby%20reducing%20negative%20transfer%20from%20the%20outlier%20source%20domains.%0AExtensive%20experiments%20on%20two%20cross-domain%20settings%20%28synthetic%20distortion%20to%0Aauthentic%20distortion%20and%20synthetic%20distortion%20to%20algorithmic%20distortion%29%20have%0Ademonstrated%20the%20effectiveness%20of%20our%20proposed%20DGQA.%20Besides%2C%20DGQA%20is%0Aorthogonal%20to%20existing%20model-based%20BIQA%20methods%2C%20and%20can%20be%20used%20in%20combination%0Awith%20such%20models%20to%20improve%20performance%20with%20less%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04167v1&entry.124074799=Read"},
{"title": "Chain of Thought Empowers Transformers to Solve Inherently Serial\n  Problems", "author": "Zhiyuan Li and Hong Liu and Denny Zhou and Tengyu Ma", "abstract": "  Instructing the model to generate a sequence of intermediate steps, a.k.a., a\nchain of thought (CoT), is a highly effective method to improve the accuracy of\nlarge language models (LLMs) on arithmetics and symbolic reasoning tasks.\nHowever, the mechanism behind CoT remains unclear. This work provides a\ntheoretical understanding of the power of CoT for decoder-only transformers\nthrough the lens of expressiveness. Conceptually, CoT empowers the model with\nthe ability to perform inherently serial computation, which is otherwise\nlacking in transformers, especially when depth is low. Given input length $n$,\nprevious works have shown that constant-depth transformers with finite\nprecision $\\mathsf{poly}(n)$ embedding size can only solve problems in\n$\\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper\nbound for constant-depth transformers with constant-bit precision, which can\nonly solve problems in $\\mathsf{AC}^0$, a proper subset of $ \\mathsf{TC}^0$.\nHowever, with $T$ steps of CoT, constant-depth transformers using constant-bit\nprecision and $O(\\log n)$ embedding size can solve any problem solvable by\nboolean circuits of size $T$. Empirically, enabling CoT dramatically improves\nthe accuracy for tasks that are hard for parallel computation, including the\ncomposition of permutation groups, iterated squaring, and circuit value\nproblems, especially for low-depth transformers.\n", "link": "http://arxiv.org/abs/2402.12875v2", "date": "2024-05-07", "relevancy": 1.8503, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.506}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.457}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chain%20of%20Thought%20Empowers%20Transformers%20to%20Solve%20Inherently%20Serial%0A%20%20Problems&body=Title%3A%20Chain%20of%20Thought%20Empowers%20Transformers%20to%20Solve%20Inherently%20Serial%0A%20%20Problems%0AAuthor%3A%20Zhiyuan%20Li%20and%20Hong%20Liu%20and%20Denny%20Zhou%20and%20Tengyu%20Ma%0AAbstract%3A%20%20%20Instructing%20the%20model%20to%20generate%20a%20sequence%20of%20intermediate%20steps%2C%20a.k.a.%2C%20a%0Achain%20of%20thought%20%28CoT%29%2C%20is%20a%20highly%20effective%20method%20to%20improve%20the%20accuracy%20of%0Alarge%20language%20models%20%28LLMs%29%20on%20arithmetics%20and%20symbolic%20reasoning%20tasks.%0AHowever%2C%20the%20mechanism%20behind%20CoT%20remains%20unclear.%20This%20work%20provides%20a%0Atheoretical%20understanding%20of%20the%20power%20of%20CoT%20for%20decoder-only%20transformers%0Athrough%20the%20lens%20of%20expressiveness.%20Conceptually%2C%20CoT%20empowers%20the%20model%20with%0Athe%20ability%20to%20perform%20inherently%20serial%20computation%2C%20which%20is%20otherwise%0Alacking%20in%20transformers%2C%20especially%20when%20depth%20is%20low.%20Given%20input%20length%20%24n%24%2C%0Aprevious%20works%20have%20shown%20that%20constant-depth%20transformers%20with%20finite%0Aprecision%20%24%5Cmathsf%7Bpoly%7D%28n%29%24%20embedding%20size%20can%20only%20solve%20problems%20in%0A%24%5Cmathsf%7BTC%7D%5E0%24%20without%20CoT.%20We%20first%20show%20an%20even%20tighter%20expressiveness%20upper%0Abound%20for%20constant-depth%20transformers%20with%20constant-bit%20precision%2C%20which%20can%0Aonly%20solve%20problems%20in%20%24%5Cmathsf%7BAC%7D%5E0%24%2C%20a%20proper%20subset%20of%20%24%20%5Cmathsf%7BTC%7D%5E0%24.%0AHowever%2C%20with%20%24T%24%20steps%20of%20CoT%2C%20constant-depth%20transformers%20using%20constant-bit%0Aprecision%20and%20%24O%28%5Clog%20n%29%24%20embedding%20size%20can%20solve%20any%20problem%20solvable%20by%0Aboolean%20circuits%20of%20size%20%24T%24.%20Empirically%2C%20enabling%20CoT%20dramatically%20improves%0Athe%20accuracy%20for%20tasks%20that%20are%20hard%20for%20parallel%20computation%2C%20including%20the%0Acomposition%20of%20permutation%20groups%2C%20iterated%20squaring%2C%20and%20circuit%20value%0Aproblems%2C%20especially%20for%20low-depth%20transformers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12875v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChain%2520of%2520Thought%2520Empowers%2520Transformers%2520to%2520Solve%2520Inherently%2520Serial%250A%2520%2520Problems%26entry.906535625%3DZhiyuan%2520Li%2520and%2520Hong%2520Liu%2520and%2520Denny%2520Zhou%2520and%2520Tengyu%2520Ma%26entry.1292438233%3D%2520%2520Instructing%2520the%2520model%2520to%2520generate%2520a%2520sequence%2520of%2520intermediate%2520steps%252C%2520a.k.a.%252C%2520a%250Achain%2520of%2520thought%2520%2528CoT%2529%252C%2520is%2520a%2520highly%2520effective%2520method%2520to%2520improve%2520the%2520accuracy%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520on%2520arithmetics%2520and%2520symbolic%2520reasoning%2520tasks.%250AHowever%252C%2520the%2520mechanism%2520behind%2520CoT%2520remains%2520unclear.%2520This%2520work%2520provides%2520a%250Atheoretical%2520understanding%2520of%2520the%2520power%2520of%2520CoT%2520for%2520decoder-only%2520transformers%250Athrough%2520the%2520lens%2520of%2520expressiveness.%2520Conceptually%252C%2520CoT%2520empowers%2520the%2520model%2520with%250Athe%2520ability%2520to%2520perform%2520inherently%2520serial%2520computation%252C%2520which%2520is%2520otherwise%250Alacking%2520in%2520transformers%252C%2520especially%2520when%2520depth%2520is%2520low.%2520Given%2520input%2520length%2520%2524n%2524%252C%250Aprevious%2520works%2520have%2520shown%2520that%2520constant-depth%2520transformers%2520with%2520finite%250Aprecision%2520%2524%255Cmathsf%257Bpoly%257D%2528n%2529%2524%2520embedding%2520size%2520can%2520only%2520solve%2520problems%2520in%250A%2524%255Cmathsf%257BTC%257D%255E0%2524%2520without%2520CoT.%2520We%2520first%2520show%2520an%2520even%2520tighter%2520expressiveness%2520upper%250Abound%2520for%2520constant-depth%2520transformers%2520with%2520constant-bit%2520precision%252C%2520which%2520can%250Aonly%2520solve%2520problems%2520in%2520%2524%255Cmathsf%257BAC%257D%255E0%2524%252C%2520a%2520proper%2520subset%2520of%2520%2524%2520%255Cmathsf%257BTC%257D%255E0%2524.%250AHowever%252C%2520with%2520%2524T%2524%2520steps%2520of%2520CoT%252C%2520constant-depth%2520transformers%2520using%2520constant-bit%250Aprecision%2520and%2520%2524O%2528%255Clog%2520n%2529%2524%2520embedding%2520size%2520can%2520solve%2520any%2520problem%2520solvable%2520by%250Aboolean%2520circuits%2520of%2520size%2520%2524T%2524.%2520Empirically%252C%2520enabling%2520CoT%2520dramatically%2520improves%250Athe%2520accuracy%2520for%2520tasks%2520that%2520are%2520hard%2520for%2520parallel%2520computation%252C%2520including%2520the%250Acomposition%2520of%2520permutation%2520groups%252C%2520iterated%2520squaring%252C%2520and%2520circuit%2520value%250Aproblems%252C%2520especially%2520for%2520low-depth%2520transformers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12875v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chain%20of%20Thought%20Empowers%20Transformers%20to%20Solve%20Inherently%20Serial%0A%20%20Problems&entry.906535625=Zhiyuan%20Li%20and%20Hong%20Liu%20and%20Denny%20Zhou%20and%20Tengyu%20Ma&entry.1292438233=%20%20Instructing%20the%20model%20to%20generate%20a%20sequence%20of%20intermediate%20steps%2C%20a.k.a.%2C%20a%0Achain%20of%20thought%20%28CoT%29%2C%20is%20a%20highly%20effective%20method%20to%20improve%20the%20accuracy%20of%0Alarge%20language%20models%20%28LLMs%29%20on%20arithmetics%20and%20symbolic%20reasoning%20tasks.%0AHowever%2C%20the%20mechanism%20behind%20CoT%20remains%20unclear.%20This%20work%20provides%20a%0Atheoretical%20understanding%20of%20the%20power%20of%20CoT%20for%20decoder-only%20transformers%0Athrough%20the%20lens%20of%20expressiveness.%20Conceptually%2C%20CoT%20empowers%20the%20model%20with%0Athe%20ability%20to%20perform%20inherently%20serial%20computation%2C%20which%20is%20otherwise%0Alacking%20in%20transformers%2C%20especially%20when%20depth%20is%20low.%20Given%20input%20length%20%24n%24%2C%0Aprevious%20works%20have%20shown%20that%20constant-depth%20transformers%20with%20finite%0Aprecision%20%24%5Cmathsf%7Bpoly%7D%28n%29%24%20embedding%20size%20can%20only%20solve%20problems%20in%0A%24%5Cmathsf%7BTC%7D%5E0%24%20without%20CoT.%20We%20first%20show%20an%20even%20tighter%20expressiveness%20upper%0Abound%20for%20constant-depth%20transformers%20with%20constant-bit%20precision%2C%20which%20can%0Aonly%20solve%20problems%20in%20%24%5Cmathsf%7BAC%7D%5E0%24%2C%20a%20proper%20subset%20of%20%24%20%5Cmathsf%7BTC%7D%5E0%24.%0AHowever%2C%20with%20%24T%24%20steps%20of%20CoT%2C%20constant-depth%20transformers%20using%20constant-bit%0Aprecision%20and%20%24O%28%5Clog%20n%29%24%20embedding%20size%20can%20solve%20any%20problem%20solvable%20by%0Aboolean%20circuits%20of%20size%20%24T%24.%20Empirically%2C%20enabling%20CoT%20dramatically%20improves%0Athe%20accuracy%20for%20tasks%20that%20are%20hard%20for%20parallel%20computation%2C%20including%20the%0Acomposition%20of%20permutation%20groups%2C%20iterated%20squaring%2C%20and%20circuit%20value%0Aproblems%2C%20especially%20for%20low-depth%20transformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12875v2&entry.124074799=Read"},
{"title": "Natural Language Counterfactuals through Representation Surgery", "author": "Matan Avitan and Ryan Cotterell and Yoav Goldberg and Shauli Ravfogel", "abstract": "  Interventions targeting the representation space of language models (LMs)\nhave emerged as an effective means to influence model behavior. Such methods\nare employed, for example, to eliminate or alter the encoding of demographic\ninformation such as gender within the model's representations and, in so doing,\ncreate a counterfactual representation. However, because the intervention\noperates within the representation space, understanding precisely what aspects\nof the text it modifies poses a challenge. In this paper, we give a method to\nconvert representation counterfactuals into string counterfactuals. We\ndemonstrate that this approach enables us to analyze the linguistic alterations\ncorresponding to a given representation space intervention and to interpret the\nfeatures utilized to encode a specific concept. Moreover, the resulting\ncounterfactuals can be used to mitigate bias in classification through data\naugmentation.\n", "link": "http://arxiv.org/abs/2402.11355v3", "date": "2024-05-07", "relevancy": 1.3127, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4452}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4361}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Natural%20Language%20Counterfactuals%20through%20Representation%20Surgery&body=Title%3A%20Natural%20Language%20Counterfactuals%20through%20Representation%20Surgery%0AAuthor%3A%20Matan%20Avitan%20and%20Ryan%20Cotterell%20and%20Yoav%20Goldberg%20and%20Shauli%20Ravfogel%0AAbstract%3A%20%20%20Interventions%20targeting%20the%20representation%20space%20of%20language%20models%20%28LMs%29%0Ahave%20emerged%20as%20an%20effective%20means%20to%20influence%20model%20behavior.%20Such%20methods%0Aare%20employed%2C%20for%20example%2C%20to%20eliminate%20or%20alter%20the%20encoding%20of%20demographic%0Ainformation%20such%20as%20gender%20within%20the%20model%27s%20representations%20and%2C%20in%20so%20doing%2C%0Acreate%20a%20counterfactual%20representation.%20However%2C%20because%20the%20intervention%0Aoperates%20within%20the%20representation%20space%2C%20understanding%20precisely%20what%20aspects%0Aof%20the%20text%20it%20modifies%20poses%20a%20challenge.%20In%20this%20paper%2C%20we%20give%20a%20method%20to%0Aconvert%20representation%20counterfactuals%20into%20string%20counterfactuals.%20We%0Ademonstrate%20that%20this%20approach%20enables%20us%20to%20analyze%20the%20linguistic%20alterations%0Acorresponding%20to%20a%20given%20representation%20space%20intervention%20and%20to%20interpret%20the%0Afeatures%20utilized%20to%20encode%20a%20specific%20concept.%20Moreover%2C%20the%20resulting%0Acounterfactuals%20can%20be%20used%20to%20mitigate%20bias%20in%20classification%20through%20data%0Aaugmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11355v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNatural%2520Language%2520Counterfactuals%2520through%2520Representation%2520Surgery%26entry.906535625%3DMatan%2520Avitan%2520and%2520Ryan%2520Cotterell%2520and%2520Yoav%2520Goldberg%2520and%2520Shauli%2520Ravfogel%26entry.1292438233%3D%2520%2520Interventions%2520targeting%2520the%2520representation%2520space%2520of%2520language%2520models%2520%2528LMs%2529%250Ahave%2520emerged%2520as%2520an%2520effective%2520means%2520to%2520influence%2520model%2520behavior.%2520Such%2520methods%250Aare%2520employed%252C%2520for%2520example%252C%2520to%2520eliminate%2520or%2520alter%2520the%2520encoding%2520of%2520demographic%250Ainformation%2520such%2520as%2520gender%2520within%2520the%2520model%2527s%2520representations%2520and%252C%2520in%2520so%2520doing%252C%250Acreate%2520a%2520counterfactual%2520representation.%2520However%252C%2520because%2520the%2520intervention%250Aoperates%2520within%2520the%2520representation%2520space%252C%2520understanding%2520precisely%2520what%2520aspects%250Aof%2520the%2520text%2520it%2520modifies%2520poses%2520a%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520give%2520a%2520method%2520to%250Aconvert%2520representation%2520counterfactuals%2520into%2520string%2520counterfactuals.%2520We%250Ademonstrate%2520that%2520this%2520approach%2520enables%2520us%2520to%2520analyze%2520the%2520linguistic%2520alterations%250Acorresponding%2520to%2520a%2520given%2520representation%2520space%2520intervention%2520and%2520to%2520interpret%2520the%250Afeatures%2520utilized%2520to%2520encode%2520a%2520specific%2520concept.%2520Moreover%252C%2520the%2520resulting%250Acounterfactuals%2520can%2520be%2520used%2520to%2520mitigate%2520bias%2520in%2520classification%2520through%2520data%250Aaugmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11355v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Natural%20Language%20Counterfactuals%20through%20Representation%20Surgery&entry.906535625=Matan%20Avitan%20and%20Ryan%20Cotterell%20and%20Yoav%20Goldberg%20and%20Shauli%20Ravfogel&entry.1292438233=%20%20Interventions%20targeting%20the%20representation%20space%20of%20language%20models%20%28LMs%29%0Ahave%20emerged%20as%20an%20effective%20means%20to%20influence%20model%20behavior.%20Such%20methods%0Aare%20employed%2C%20for%20example%2C%20to%20eliminate%20or%20alter%20the%20encoding%20of%20demographic%0Ainformation%20such%20as%20gender%20within%20the%20model%27s%20representations%20and%2C%20in%20so%20doing%2C%0Acreate%20a%20counterfactual%20representation.%20However%2C%20because%20the%20intervention%0Aoperates%20within%20the%20representation%20space%2C%20understanding%20precisely%20what%20aspects%0Aof%20the%20text%20it%20modifies%20poses%20a%20challenge.%20In%20this%20paper%2C%20we%20give%20a%20method%20to%0Aconvert%20representation%20counterfactuals%20into%20string%20counterfactuals.%20We%0Ademonstrate%20that%20this%20approach%20enables%20us%20to%20analyze%20the%20linguistic%20alterations%0Acorresponding%20to%20a%20given%20representation%20space%20intervention%20and%20to%20interpret%20the%0Afeatures%20utilized%20to%20encode%20a%20specific%20concept.%20Moreover%2C%20the%20resulting%0Acounterfactuals%20can%20be%20used%20to%20mitigate%20bias%20in%20classification%20through%20data%0Aaugmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11355v3&entry.124074799=Read"},
{"title": "Physics-data hybrid dynamic model of a multi-axis manipulator for\n  sensorless dexterous manipulation and high-performance motion planning", "author": "Wu-Te Yang and Jyun-Ming Liao and Pei-Chun Lin", "abstract": "  We report on the development of an implementable physics-data hybrid dynamic\nmodel for an articulated manipulator to plan and operate in various scenarios.\nMeanwhile, the physics-based and data-driven dynamic models are studied in this\nresearch to select the best model for planning. The physics-based model is\nconstructed using the Lagrangian method, and the loss terms include inertia\nloss, viscous loss, and friction loss. As for the data-driven model, three\nmethods are explored, including DNN, LSTM, and XGBoost. Our modeling results\ndemonstrate that, after comprehensive hyperparameter optimization, the XGBoost\narchitecture outperforms DNN and LSTM in accurately representing manipulator\ndynamics. The hybrid model with physics-based and data-driven terms has the\nbest performance among all models based on the RMSE criteria, and it only needs\nabout 24k of training data. In addition, we developed a virtual force sensor of\na manipulator using the observed external torque derived from the dynamic model\nand designed a motion planner through the physics-data hybrid dynamic model.\nThe external torque contributes to forces and torque on the end effector,\nfacilitating interaction with the surroundings, while the internal torque\ngoverns manipulator motion dynamics and compensates for internal losses. By\nestimating external torque via the difference between measured joint torque and\ninternal losses, we implement a sensorless control strategy which is\ndemonstrated through a peg-in-hole task. Lastly, a learning-based motion\nplanner based on the hybrid dynamic model assists in planning time-efficient\ntrajectories for the manipulator. This comprehensive approach underscores the\nefficacy of integrating physics-based and data-driven models for advanced\nmanipulator control and planning in industrial environments.\n", "link": "http://arxiv.org/abs/2405.04503v1", "date": "2024-05-07", "relevancy": 1.6411, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5934}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5438}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-data%20hybrid%20dynamic%20model%20of%20a%20multi-axis%20manipulator%20for%0A%20%20sensorless%20dexterous%20manipulation%20and%20high-performance%20motion%20planning&body=Title%3A%20Physics-data%20hybrid%20dynamic%20model%20of%20a%20multi-axis%20manipulator%20for%0A%20%20sensorless%20dexterous%20manipulation%20and%20high-performance%20motion%20planning%0AAuthor%3A%20Wu-Te%20Yang%20and%20Jyun-Ming%20Liao%20and%20Pei-Chun%20Lin%0AAbstract%3A%20%20%20We%20report%20on%20the%20development%20of%20an%20implementable%20physics-data%20hybrid%20dynamic%0Amodel%20for%20an%20articulated%20manipulator%20to%20plan%20and%20operate%20in%20various%20scenarios.%0AMeanwhile%2C%20the%20physics-based%20and%20data-driven%20dynamic%20models%20are%20studied%20in%20this%0Aresearch%20to%20select%20the%20best%20model%20for%20planning.%20The%20physics-based%20model%20is%0Aconstructed%20using%20the%20Lagrangian%20method%2C%20and%20the%20loss%20terms%20include%20inertia%0Aloss%2C%20viscous%20loss%2C%20and%20friction%20loss.%20As%20for%20the%20data-driven%20model%2C%20three%0Amethods%20are%20explored%2C%20including%20DNN%2C%20LSTM%2C%20and%20XGBoost.%20Our%20modeling%20results%0Ademonstrate%20that%2C%20after%20comprehensive%20hyperparameter%20optimization%2C%20the%20XGBoost%0Aarchitecture%20outperforms%20DNN%20and%20LSTM%20in%20accurately%20representing%20manipulator%0Adynamics.%20The%20hybrid%20model%20with%20physics-based%20and%20data-driven%20terms%20has%20the%0Abest%20performance%20among%20all%20models%20based%20on%20the%20RMSE%20criteria%2C%20and%20it%20only%20needs%0Aabout%2024k%20of%20training%20data.%20In%20addition%2C%20we%20developed%20a%20virtual%20force%20sensor%20of%0Aa%20manipulator%20using%20the%20observed%20external%20torque%20derived%20from%20the%20dynamic%20model%0Aand%20designed%20a%20motion%20planner%20through%20the%20physics-data%20hybrid%20dynamic%20model.%0AThe%20external%20torque%20contributes%20to%20forces%20and%20torque%20on%20the%20end%20effector%2C%0Afacilitating%20interaction%20with%20the%20surroundings%2C%20while%20the%20internal%20torque%0Agoverns%20manipulator%20motion%20dynamics%20and%20compensates%20for%20internal%20losses.%20By%0Aestimating%20external%20torque%20via%20the%20difference%20between%20measured%20joint%20torque%20and%0Ainternal%20losses%2C%20we%20implement%20a%20sensorless%20control%20strategy%20which%20is%0Ademonstrated%20through%20a%20peg-in-hole%20task.%20Lastly%2C%20a%20learning-based%20motion%0Aplanner%20based%20on%20the%20hybrid%20dynamic%20model%20assists%20in%20planning%20time-efficient%0Atrajectories%20for%20the%20manipulator.%20This%20comprehensive%20approach%20underscores%20the%0Aefficacy%20of%20integrating%20physics-based%20and%20data-driven%20models%20for%20advanced%0Amanipulator%20control%20and%20planning%20in%20industrial%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-data%2520hybrid%2520dynamic%2520model%2520of%2520a%2520multi-axis%2520manipulator%2520for%250A%2520%2520sensorless%2520dexterous%2520manipulation%2520and%2520high-performance%2520motion%2520planning%26entry.906535625%3DWu-Te%2520Yang%2520and%2520Jyun-Ming%2520Liao%2520and%2520Pei-Chun%2520Lin%26entry.1292438233%3D%2520%2520We%2520report%2520on%2520the%2520development%2520of%2520an%2520implementable%2520physics-data%2520hybrid%2520dynamic%250Amodel%2520for%2520an%2520articulated%2520manipulator%2520to%2520plan%2520and%2520operate%2520in%2520various%2520scenarios.%250AMeanwhile%252C%2520the%2520physics-based%2520and%2520data-driven%2520dynamic%2520models%2520are%2520studied%2520in%2520this%250Aresearch%2520to%2520select%2520the%2520best%2520model%2520for%2520planning.%2520The%2520physics-based%2520model%2520is%250Aconstructed%2520using%2520the%2520Lagrangian%2520method%252C%2520and%2520the%2520loss%2520terms%2520include%2520inertia%250Aloss%252C%2520viscous%2520loss%252C%2520and%2520friction%2520loss.%2520As%2520for%2520the%2520data-driven%2520model%252C%2520three%250Amethods%2520are%2520explored%252C%2520including%2520DNN%252C%2520LSTM%252C%2520and%2520XGBoost.%2520Our%2520modeling%2520results%250Ademonstrate%2520that%252C%2520after%2520comprehensive%2520hyperparameter%2520optimization%252C%2520the%2520XGBoost%250Aarchitecture%2520outperforms%2520DNN%2520and%2520LSTM%2520in%2520accurately%2520representing%2520manipulator%250Adynamics.%2520The%2520hybrid%2520model%2520with%2520physics-based%2520and%2520data-driven%2520terms%2520has%2520the%250Abest%2520performance%2520among%2520all%2520models%2520based%2520on%2520the%2520RMSE%2520criteria%252C%2520and%2520it%2520only%2520needs%250Aabout%252024k%2520of%2520training%2520data.%2520In%2520addition%252C%2520we%2520developed%2520a%2520virtual%2520force%2520sensor%2520of%250Aa%2520manipulator%2520using%2520the%2520observed%2520external%2520torque%2520derived%2520from%2520the%2520dynamic%2520model%250Aand%2520designed%2520a%2520motion%2520planner%2520through%2520the%2520physics-data%2520hybrid%2520dynamic%2520model.%250AThe%2520external%2520torque%2520contributes%2520to%2520forces%2520and%2520torque%2520on%2520the%2520end%2520effector%252C%250Afacilitating%2520interaction%2520with%2520the%2520surroundings%252C%2520while%2520the%2520internal%2520torque%250Agoverns%2520manipulator%2520motion%2520dynamics%2520and%2520compensates%2520for%2520internal%2520losses.%2520By%250Aestimating%2520external%2520torque%2520via%2520the%2520difference%2520between%2520measured%2520joint%2520torque%2520and%250Ainternal%2520losses%252C%2520we%2520implement%2520a%2520sensorless%2520control%2520strategy%2520which%2520is%250Ademonstrated%2520through%2520a%2520peg-in-hole%2520task.%2520Lastly%252C%2520a%2520learning-based%2520motion%250Aplanner%2520based%2520on%2520the%2520hybrid%2520dynamic%2520model%2520assists%2520in%2520planning%2520time-efficient%250Atrajectories%2520for%2520the%2520manipulator.%2520This%2520comprehensive%2520approach%2520underscores%2520the%250Aefficacy%2520of%2520integrating%2520physics-based%2520and%2520data-driven%2520models%2520for%2520advanced%250Amanipulator%2520control%2520and%2520planning%2520in%2520industrial%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-data%20hybrid%20dynamic%20model%20of%20a%20multi-axis%20manipulator%20for%0A%20%20sensorless%20dexterous%20manipulation%20and%20high-performance%20motion%20planning&entry.906535625=Wu-Te%20Yang%20and%20Jyun-Ming%20Liao%20and%20Pei-Chun%20Lin&entry.1292438233=%20%20We%20report%20on%20the%20development%20of%20an%20implementable%20physics-data%20hybrid%20dynamic%0Amodel%20for%20an%20articulated%20manipulator%20to%20plan%20and%20operate%20in%20various%20scenarios.%0AMeanwhile%2C%20the%20physics-based%20and%20data-driven%20dynamic%20models%20are%20studied%20in%20this%0Aresearch%20to%20select%20the%20best%20model%20for%20planning.%20The%20physics-based%20model%20is%0Aconstructed%20using%20the%20Lagrangian%20method%2C%20and%20the%20loss%20terms%20include%20inertia%0Aloss%2C%20viscous%20loss%2C%20and%20friction%20loss.%20As%20for%20the%20data-driven%20model%2C%20three%0Amethods%20are%20explored%2C%20including%20DNN%2C%20LSTM%2C%20and%20XGBoost.%20Our%20modeling%20results%0Ademonstrate%20that%2C%20after%20comprehensive%20hyperparameter%20optimization%2C%20the%20XGBoost%0Aarchitecture%20outperforms%20DNN%20and%20LSTM%20in%20accurately%20representing%20manipulator%0Adynamics.%20The%20hybrid%20model%20with%20physics-based%20and%20data-driven%20terms%20has%20the%0Abest%20performance%20among%20all%20models%20based%20on%20the%20RMSE%20criteria%2C%20and%20it%20only%20needs%0Aabout%2024k%20of%20training%20data.%20In%20addition%2C%20we%20developed%20a%20virtual%20force%20sensor%20of%0Aa%20manipulator%20using%20the%20observed%20external%20torque%20derived%20from%20the%20dynamic%20model%0Aand%20designed%20a%20motion%20planner%20through%20the%20physics-data%20hybrid%20dynamic%20model.%0AThe%20external%20torque%20contributes%20to%20forces%20and%20torque%20on%20the%20end%20effector%2C%0Afacilitating%20interaction%20with%20the%20surroundings%2C%20while%20the%20internal%20torque%0Agoverns%20manipulator%20motion%20dynamics%20and%20compensates%20for%20internal%20losses.%20By%0Aestimating%20external%20torque%20via%20the%20difference%20between%20measured%20joint%20torque%20and%0Ainternal%20losses%2C%20we%20implement%20a%20sensorless%20control%20strategy%20which%20is%0Ademonstrated%20through%20a%20peg-in-hole%20task.%20Lastly%2C%20a%20learning-based%20motion%0Aplanner%20based%20on%20the%20hybrid%20dynamic%20model%20assists%20in%20planning%20time-efficient%0Atrajectories%20for%20the%20manipulator.%20This%20comprehensive%20approach%20underscores%20the%0Aefficacy%20of%20integrating%20physics-based%20and%20data-driven%20models%20for%20advanced%0Amanipulator%20control%20and%20planning%20in%20industrial%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04503v1&entry.124074799=Read"},
{"title": "On the Foundations of Earth and Climate Foundation Models", "author": "Xiao Xiang Zhu and Zhitong Xiong and Yi Wang and Adam J. Stewart and Konrad Heidler and Yuanyuan Wang and Zhenghang Yuan and Thomas Dujardin and Qingsong Xu and Yilei Shi", "abstract": "  Foundation models have enormous potential in advancing Earth and climate\nsciences, however, current approaches may not be optimal as they focus on a few\nbasic features of a desirable Earth and climate foundation model. Crafting the\nideal Earth foundation model, we define eleven features which would allow such\na foundation model to be beneficial for any geoscientific downstream\napplication in an environmental- and human-centric manner.We further shed light\non the way forward to achieve the ideal model and to evaluate Earth foundation\nmodels. What comes after foundation models? Energy efficient adaptation,\nadversarial defenses, and interpretability are among the emerging directions.\n", "link": "http://arxiv.org/abs/2405.04285v1", "date": "2024-05-07", "relevancy": 1.1788, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.395}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.3927}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Foundations%20of%20Earth%20and%20Climate%20Foundation%20Models&body=Title%3A%20On%20the%20Foundations%20of%20Earth%20and%20Climate%20Foundation%20Models%0AAuthor%3A%20Xiao%20Xiang%20Zhu%20and%20Zhitong%20Xiong%20and%20Yi%20Wang%20and%20Adam%20J.%20Stewart%20and%20Konrad%20Heidler%20and%20Yuanyuan%20Wang%20and%20Zhenghang%20Yuan%20and%20Thomas%20Dujardin%20and%20Qingsong%20Xu%20and%20Yilei%20Shi%0AAbstract%3A%20%20%20Foundation%20models%20have%20enormous%20potential%20in%20advancing%20Earth%20and%20climate%0Asciences%2C%20however%2C%20current%20approaches%20may%20not%20be%20optimal%20as%20they%20focus%20on%20a%20few%0Abasic%20features%20of%20a%20desirable%20Earth%20and%20climate%20foundation%20model.%20Crafting%20the%0Aideal%20Earth%20foundation%20model%2C%20we%20define%20eleven%20features%20which%20would%20allow%20such%0Aa%20foundation%20model%20to%20be%20beneficial%20for%20any%20geoscientific%20downstream%0Aapplication%20in%20an%20environmental-%20and%20human-centric%20manner.We%20further%20shed%20light%0Aon%20the%20way%20forward%20to%20achieve%20the%20ideal%20model%20and%20to%20evaluate%20Earth%20foundation%0Amodels.%20What%20comes%20after%20foundation%20models%3F%20Energy%20efficient%20adaptation%2C%0Aadversarial%20defenses%2C%20and%20interpretability%20are%20among%20the%20emerging%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04285v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Foundations%2520of%2520Earth%2520and%2520Climate%2520Foundation%2520Models%26entry.906535625%3DXiao%2520Xiang%2520Zhu%2520and%2520Zhitong%2520Xiong%2520and%2520Yi%2520Wang%2520and%2520Adam%2520J.%2520Stewart%2520and%2520Konrad%2520Heidler%2520and%2520Yuanyuan%2520Wang%2520and%2520Zhenghang%2520Yuan%2520and%2520Thomas%2520Dujardin%2520and%2520Qingsong%2520Xu%2520and%2520Yilei%2520Shi%26entry.1292438233%3D%2520%2520Foundation%2520models%2520have%2520enormous%2520potential%2520in%2520advancing%2520Earth%2520and%2520climate%250Asciences%252C%2520however%252C%2520current%2520approaches%2520may%2520not%2520be%2520optimal%2520as%2520they%2520focus%2520on%2520a%2520few%250Abasic%2520features%2520of%2520a%2520desirable%2520Earth%2520and%2520climate%2520foundation%2520model.%2520Crafting%2520the%250Aideal%2520Earth%2520foundation%2520model%252C%2520we%2520define%2520eleven%2520features%2520which%2520would%2520allow%2520such%250Aa%2520foundation%2520model%2520to%2520be%2520beneficial%2520for%2520any%2520geoscientific%2520downstream%250Aapplication%2520in%2520an%2520environmental-%2520and%2520human-centric%2520manner.We%2520further%2520shed%2520light%250Aon%2520the%2520way%2520forward%2520to%2520achieve%2520the%2520ideal%2520model%2520and%2520to%2520evaluate%2520Earth%2520foundation%250Amodels.%2520What%2520comes%2520after%2520foundation%2520models%253F%2520Energy%2520efficient%2520adaptation%252C%250Aadversarial%2520defenses%252C%2520and%2520interpretability%2520are%2520among%2520the%2520emerging%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04285v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Foundations%20of%20Earth%20and%20Climate%20Foundation%20Models&entry.906535625=Xiao%20Xiang%20Zhu%20and%20Zhitong%20Xiong%20and%20Yi%20Wang%20and%20Adam%20J.%20Stewart%20and%20Konrad%20Heidler%20and%20Yuanyuan%20Wang%20and%20Zhenghang%20Yuan%20and%20Thomas%20Dujardin%20and%20Qingsong%20Xu%20and%20Yilei%20Shi&entry.1292438233=%20%20Foundation%20models%20have%20enormous%20potential%20in%20advancing%20Earth%20and%20climate%0Asciences%2C%20however%2C%20current%20approaches%20may%20not%20be%20optimal%20as%20they%20focus%20on%20a%20few%0Abasic%20features%20of%20a%20desirable%20Earth%20and%20climate%20foundation%20model.%20Crafting%20the%0Aideal%20Earth%20foundation%20model%2C%20we%20define%20eleven%20features%20which%20would%20allow%20such%0Aa%20foundation%20model%20to%20be%20beneficial%20for%20any%20geoscientific%20downstream%0Aapplication%20in%20an%20environmental-%20and%20human-centric%20manner.We%20further%20shed%20light%0Aon%20the%20way%20forward%20to%20achieve%20the%20ideal%20model%20and%20to%20evaluate%20Earth%20foundation%0Amodels.%20What%20comes%20after%20foundation%20models%3F%20Energy%20efficient%20adaptation%2C%0Aadversarial%20defenses%2C%20and%20interpretability%20are%20among%20the%20emerging%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04285v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


