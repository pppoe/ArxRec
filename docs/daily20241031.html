<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241030.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "PointRecon: Online Point-based 3D Reconstruction via Ray-based 2D-3D\n  Matching", "author": "Chen Ziwen and Zexiang Xu and Li Fuxin", "abstract": "  We propose a novel online, point-based 3D reconstruction method from posed\nmonocular RGB videos. Our model maintains a global point cloud representation\nof the scene, continuously updating the features and 3D locations of points as\nnew images are observed. It expands the point cloud with newly detected points\nwhile carefully removing redundancies. The point cloud updates and depth\npredictions for new points are achieved through a novel ray-based 2D-3D feature\nmatching technique, which is robust against errors in previous point position\npredictions. In contrast to offline methods, our approach processes\ninfinite-length sequences and provides real-time updates. Additionally, the\npoint cloud imposes no pre-defined resolution or scene size constraints, and\nits unified global representation ensures view consistency across perspectives.\nExperiments on the ScanNet dataset show that our method achieves\nstate-of-the-art quality among online MVS approaches. Project page:\nhttps://arthurhero.github.io/projects/pointrecon\n", "link": "http://arxiv.org/abs/2410.23245v1", "date": "2024-10-30", "relevancy": 3.1989, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6545}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6397}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PointRecon%3A%20Online%20Point-based%203D%20Reconstruction%20via%20Ray-based%202D-3D%0A%20%20Matching&body=Title%3A%20PointRecon%3A%20Online%20Point-based%203D%20Reconstruction%20via%20Ray-based%202D-3D%0A%20%20Matching%0AAuthor%3A%20Chen%20Ziwen%20and%20Zexiang%20Xu%20and%20Li%20Fuxin%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20online%2C%20point-based%203D%20reconstruction%20method%20from%20posed%0Amonocular%20RGB%20videos.%20Our%20model%20maintains%20a%20global%20point%20cloud%20representation%0Aof%20the%20scene%2C%20continuously%20updating%20the%20features%20and%203D%20locations%20of%20points%20as%0Anew%20images%20are%20observed.%20It%20expands%20the%20point%20cloud%20with%20newly%20detected%20points%0Awhile%20carefully%20removing%20redundancies.%20The%20point%20cloud%20updates%20and%20depth%0Apredictions%20for%20new%20points%20are%20achieved%20through%20a%20novel%20ray-based%202D-3D%20feature%0Amatching%20technique%2C%20which%20is%20robust%20against%20errors%20in%20previous%20point%20position%0Apredictions.%20In%20contrast%20to%20offline%20methods%2C%20our%20approach%20processes%0Ainfinite-length%20sequences%20and%20provides%20real-time%20updates.%20Additionally%2C%20the%0Apoint%20cloud%20imposes%20no%20pre-defined%20resolution%20or%20scene%20size%20constraints%2C%20and%0Aits%20unified%20global%20representation%20ensures%20view%20consistency%20across%20perspectives.%0AExperiments%20on%20the%20ScanNet%20dataset%20show%20that%20our%20method%20achieves%0Astate-of-the-art%20quality%20among%20online%20MVS%20approaches.%20Project%20page%3A%0Ahttps%3A//arthurhero.github.io/projects/pointrecon%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23245v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointRecon%253A%2520Online%2520Point-based%25203D%2520Reconstruction%2520via%2520Ray-based%25202D-3D%250A%2520%2520Matching%26entry.906535625%3DChen%2520Ziwen%2520and%2520Zexiang%2520Xu%2520and%2520Li%2520Fuxin%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520online%252C%2520point-based%25203D%2520reconstruction%2520method%2520from%2520posed%250Amonocular%2520RGB%2520videos.%2520Our%2520model%2520maintains%2520a%2520global%2520point%2520cloud%2520representation%250Aof%2520the%2520scene%252C%2520continuously%2520updating%2520the%2520features%2520and%25203D%2520locations%2520of%2520points%2520as%250Anew%2520images%2520are%2520observed.%2520It%2520expands%2520the%2520point%2520cloud%2520with%2520newly%2520detected%2520points%250Awhile%2520carefully%2520removing%2520redundancies.%2520The%2520point%2520cloud%2520updates%2520and%2520depth%250Apredictions%2520for%2520new%2520points%2520are%2520achieved%2520through%2520a%2520novel%2520ray-based%25202D-3D%2520feature%250Amatching%2520technique%252C%2520which%2520is%2520robust%2520against%2520errors%2520in%2520previous%2520point%2520position%250Apredictions.%2520In%2520contrast%2520to%2520offline%2520methods%252C%2520our%2520approach%2520processes%250Ainfinite-length%2520sequences%2520and%2520provides%2520real-time%2520updates.%2520Additionally%252C%2520the%250Apoint%2520cloud%2520imposes%2520no%2520pre-defined%2520resolution%2520or%2520scene%2520size%2520constraints%252C%2520and%250Aits%2520unified%2520global%2520representation%2520ensures%2520view%2520consistency%2520across%2520perspectives.%250AExperiments%2520on%2520the%2520ScanNet%2520dataset%2520show%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520quality%2520among%2520online%2520MVS%2520approaches.%2520Project%2520page%253A%250Ahttps%253A//arthurhero.github.io/projects/pointrecon%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23245v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointRecon%3A%20Online%20Point-based%203D%20Reconstruction%20via%20Ray-based%202D-3D%0A%20%20Matching&entry.906535625=Chen%20Ziwen%20and%20Zexiang%20Xu%20and%20Li%20Fuxin&entry.1292438233=%20%20We%20propose%20a%20novel%20online%2C%20point-based%203D%20reconstruction%20method%20from%20posed%0Amonocular%20RGB%20videos.%20Our%20model%20maintains%20a%20global%20point%20cloud%20representation%0Aof%20the%20scene%2C%20continuously%20updating%20the%20features%20and%203D%20locations%20of%20points%20as%0Anew%20images%20are%20observed.%20It%20expands%20the%20point%20cloud%20with%20newly%20detected%20points%0Awhile%20carefully%20removing%20redundancies.%20The%20point%20cloud%20updates%20and%20depth%0Apredictions%20for%20new%20points%20are%20achieved%20through%20a%20novel%20ray-based%202D-3D%20feature%0Amatching%20technique%2C%20which%20is%20robust%20against%20errors%20in%20previous%20point%20position%0Apredictions.%20In%20contrast%20to%20offline%20methods%2C%20our%20approach%20processes%0Ainfinite-length%20sequences%20and%20provides%20real-time%20updates.%20Additionally%2C%20the%0Apoint%20cloud%20imposes%20no%20pre-defined%20resolution%20or%20scene%20size%20constraints%2C%20and%0Aits%20unified%20global%20representation%20ensures%20view%20consistency%20across%20perspectives.%0AExperiments%20on%20the%20ScanNet%20dataset%20show%20that%20our%20method%20achieves%0Astate-of-the-art%20quality%20among%20online%20MVS%20approaches.%20Project%20page%3A%0Ahttps%3A//arthurhero.github.io/projects/pointrecon%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23245v1&entry.124074799=Read"},
{"title": "DisC-GS: Discontinuity-aware Gaussian Splatting", "author": "Haoxuan Qu and Zhuoling Li and Hossein Rahmani and Yujun Cai and Jun Liu", "abstract": "  Recently, Gaussian Splatting, a method that represents a 3D scene as a\ncollection of Gaussian distributions, has gained significant attention in\naddressing the task of novel view synthesis. In this paper, we highlight a\nfundamental limitation of Gaussian Splatting: its inability to accurately\nrender discontinuities and boundaries in images due to the continuous nature of\nGaussian distributions. To address this issue, we propose a novel framework\nenabling Gaussian Splatting to perform discontinuity-aware image rendering.\nAdditionally, we introduce a B\\'ezier-boundary gradient approximation strategy\nwithin our framework to keep the \"differentiability\" of the proposed\ndiscontinuity-aware rendering process. Extensive experiments demonstrate the\nefficacy of our framework.\n", "link": "http://arxiv.org/abs/2405.15196v2", "date": "2024-10-30", "relevancy": 3.1613, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6424}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6279}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DisC-GS%3A%20Discontinuity-aware%20Gaussian%20Splatting&body=Title%3A%20DisC-GS%3A%20Discontinuity-aware%20Gaussian%20Splatting%0AAuthor%3A%20Haoxuan%20Qu%20and%20Zhuoling%20Li%20and%20Hossein%20Rahmani%20and%20Yujun%20Cai%20and%20Jun%20Liu%0AAbstract%3A%20%20%20Recently%2C%20Gaussian%20Splatting%2C%20a%20method%20that%20represents%20a%203D%20scene%20as%20a%0Acollection%20of%20Gaussian%20distributions%2C%20has%20gained%20significant%20attention%20in%0Aaddressing%20the%20task%20of%20novel%20view%20synthesis.%20In%20this%20paper%2C%20we%20highlight%20a%0Afundamental%20limitation%20of%20Gaussian%20Splatting%3A%20its%20inability%20to%20accurately%0Arender%20discontinuities%20and%20boundaries%20in%20images%20due%20to%20the%20continuous%20nature%20of%0AGaussian%20distributions.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20framework%0Aenabling%20Gaussian%20Splatting%20to%20perform%20discontinuity-aware%20image%20rendering.%0AAdditionally%2C%20we%20introduce%20a%20B%5C%27ezier-boundary%20gradient%20approximation%20strategy%0Awithin%20our%20framework%20to%20keep%20the%20%22differentiability%22%20of%20the%20proposed%0Adiscontinuity-aware%20rendering%20process.%20Extensive%20experiments%20demonstrate%20the%0Aefficacy%20of%20our%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15196v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisC-GS%253A%2520Discontinuity-aware%2520Gaussian%2520Splatting%26entry.906535625%3DHaoxuan%2520Qu%2520and%2520Zhuoling%2520Li%2520and%2520Hossein%2520Rahmani%2520and%2520Yujun%2520Cai%2520and%2520Jun%2520Liu%26entry.1292438233%3D%2520%2520Recently%252C%2520Gaussian%2520Splatting%252C%2520a%2520method%2520that%2520represents%2520a%25203D%2520scene%2520as%2520a%250Acollection%2520of%2520Gaussian%2520distributions%252C%2520has%2520gained%2520significant%2520attention%2520in%250Aaddressing%2520the%2520task%2520of%2520novel%2520view%2520synthesis.%2520In%2520this%2520paper%252C%2520we%2520highlight%2520a%250Afundamental%2520limitation%2520of%2520Gaussian%2520Splatting%253A%2520its%2520inability%2520to%2520accurately%250Arender%2520discontinuities%2520and%2520boundaries%2520in%2520images%2520due%2520to%2520the%2520continuous%2520nature%2520of%250AGaussian%2520distributions.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520framework%250Aenabling%2520Gaussian%2520Splatting%2520to%2520perform%2520discontinuity-aware%2520image%2520rendering.%250AAdditionally%252C%2520we%2520introduce%2520a%2520B%255C%2527ezier-boundary%2520gradient%2520approximation%2520strategy%250Awithin%2520our%2520framework%2520to%2520keep%2520the%2520%2522differentiability%2522%2520of%2520the%2520proposed%250Adiscontinuity-aware%2520rendering%2520process.%2520Extensive%2520experiments%2520demonstrate%2520the%250Aefficacy%2520of%2520our%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15196v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DisC-GS%3A%20Discontinuity-aware%20Gaussian%20Splatting&entry.906535625=Haoxuan%20Qu%20and%20Zhuoling%20Li%20and%20Hossein%20Rahmani%20and%20Yujun%20Cai%20and%20Jun%20Liu&entry.1292438233=%20%20Recently%2C%20Gaussian%20Splatting%2C%20a%20method%20that%20represents%20a%203D%20scene%20as%20a%0Acollection%20of%20Gaussian%20distributions%2C%20has%20gained%20significant%20attention%20in%0Aaddressing%20the%20task%20of%20novel%20view%20synthesis.%20In%20this%20paper%2C%20we%20highlight%20a%0Afundamental%20limitation%20of%20Gaussian%20Splatting%3A%20its%20inability%20to%20accurately%0Arender%20discontinuities%20and%20boundaries%20in%20images%20due%20to%20the%20continuous%20nature%20of%0AGaussian%20distributions.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20framework%0Aenabling%20Gaussian%20Splatting%20to%20perform%20discontinuity-aware%20image%20rendering.%0AAdditionally%2C%20we%20introduce%20a%20B%5C%27ezier-boundary%20gradient%20approximation%20strategy%0Awithin%20our%20framework%20to%20keep%20the%20%22differentiability%22%20of%20the%20proposed%0Adiscontinuity-aware%20rendering%20process.%20Extensive%20experiments%20demonstrate%20the%0Aefficacy%20of%20our%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15196v2&entry.124074799=Read"},
{"title": "Is Your LiDAR Placement Optimized for 3D Scene Understanding?", "author": "Ye Li and Lingdong Kong and Hanjiang Hu and Xiaohao Xu and Xiaonan Huang", "abstract": "  The reliability of driving perception systems under unprecedented conditions\nis crucial for practical usage. Latest advancements have prompted increasing\ninterest in multi-LiDAR perception. However, prevailing driving datasets\npredominantly utilize single-LiDAR systems and collect data devoid of adverse\nconditions, failing to capture the complexities of real-world environments\naccurately. Addressing these gaps, we proposed Place3D, a full-cycle pipeline\nthat encompasses LiDAR placement optimization, data generation, and downstream\nevaluations. Our framework makes three appealing contributions. 1) To identify\nthe most effective configurations for multi-LiDAR systems, we introduce the\nSurrogate Metric of the Semantic Occupancy Grids (M-SOG) to evaluate LiDAR\nplacement quality. 2) Leveraging the M-SOG metric, we propose a novel\noptimization strategy to refine multi-LiDAR placements. 3) Centered around the\ntheme of multi-condition multi-LiDAR perception, we collect a 280,000-frame\ndataset from both clean and adverse conditions. Extensive experiments\ndemonstrate that LiDAR placements optimized using our approach outperform\nvarious baselines. We showcase exceptional results in both LiDAR semantic\nsegmentation and 3D object detection tasks, under diverse weather and sensor\nfailure conditions.\n", "link": "http://arxiv.org/abs/2403.17009v2", "date": "2024-10-30", "relevancy": 3.1524, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6399}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6258}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Your%20LiDAR%20Placement%20Optimized%20for%203D%20Scene%20Understanding%3F&body=Title%3A%20Is%20Your%20LiDAR%20Placement%20Optimized%20for%203D%20Scene%20Understanding%3F%0AAuthor%3A%20Ye%20Li%20and%20Lingdong%20Kong%20and%20Hanjiang%20Hu%20and%20Xiaohao%20Xu%20and%20Xiaonan%20Huang%0AAbstract%3A%20%20%20The%20reliability%20of%20driving%20perception%20systems%20under%20unprecedented%20conditions%0Ais%20crucial%20for%20practical%20usage.%20Latest%20advancements%20have%20prompted%20increasing%0Ainterest%20in%20multi-LiDAR%20perception.%20However%2C%20prevailing%20driving%20datasets%0Apredominantly%20utilize%20single-LiDAR%20systems%20and%20collect%20data%20devoid%20of%20adverse%0Aconditions%2C%20failing%20to%20capture%20the%20complexities%20of%20real-world%20environments%0Aaccurately.%20Addressing%20these%20gaps%2C%20we%20proposed%20Place3D%2C%20a%20full-cycle%20pipeline%0Athat%20encompasses%20LiDAR%20placement%20optimization%2C%20data%20generation%2C%20and%20downstream%0Aevaluations.%20Our%20framework%20makes%20three%20appealing%20contributions.%201%29%20To%20identify%0Athe%20most%20effective%20configurations%20for%20multi-LiDAR%20systems%2C%20we%20introduce%20the%0ASurrogate%20Metric%20of%20the%20Semantic%20Occupancy%20Grids%20%28M-SOG%29%20to%20evaluate%20LiDAR%0Aplacement%20quality.%202%29%20Leveraging%20the%20M-SOG%20metric%2C%20we%20propose%20a%20novel%0Aoptimization%20strategy%20to%20refine%20multi-LiDAR%20placements.%203%29%20Centered%20around%20the%0Atheme%20of%20multi-condition%20multi-LiDAR%20perception%2C%20we%20collect%20a%20280%2C000-frame%0Adataset%20from%20both%20clean%20and%20adverse%20conditions.%20Extensive%20experiments%0Ademonstrate%20that%20LiDAR%20placements%20optimized%20using%20our%20approach%20outperform%0Avarious%20baselines.%20We%20showcase%20exceptional%20results%20in%20both%20LiDAR%20semantic%0Asegmentation%20and%203D%20object%20detection%20tasks%2C%20under%20diverse%20weather%20and%20sensor%0Afailure%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17009v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Your%2520LiDAR%2520Placement%2520Optimized%2520for%25203D%2520Scene%2520Understanding%253F%26entry.906535625%3DYe%2520Li%2520and%2520Lingdong%2520Kong%2520and%2520Hanjiang%2520Hu%2520and%2520Xiaohao%2520Xu%2520and%2520Xiaonan%2520Huang%26entry.1292438233%3D%2520%2520The%2520reliability%2520of%2520driving%2520perception%2520systems%2520under%2520unprecedented%2520conditions%250Ais%2520crucial%2520for%2520practical%2520usage.%2520Latest%2520advancements%2520have%2520prompted%2520increasing%250Ainterest%2520in%2520multi-LiDAR%2520perception.%2520However%252C%2520prevailing%2520driving%2520datasets%250Apredominantly%2520utilize%2520single-LiDAR%2520systems%2520and%2520collect%2520data%2520devoid%2520of%2520adverse%250Aconditions%252C%2520failing%2520to%2520capture%2520the%2520complexities%2520of%2520real-world%2520environments%250Aaccurately.%2520Addressing%2520these%2520gaps%252C%2520we%2520proposed%2520Place3D%252C%2520a%2520full-cycle%2520pipeline%250Athat%2520encompasses%2520LiDAR%2520placement%2520optimization%252C%2520data%2520generation%252C%2520and%2520downstream%250Aevaluations.%2520Our%2520framework%2520makes%2520three%2520appealing%2520contributions.%25201%2529%2520To%2520identify%250Athe%2520most%2520effective%2520configurations%2520for%2520multi-LiDAR%2520systems%252C%2520we%2520introduce%2520the%250ASurrogate%2520Metric%2520of%2520the%2520Semantic%2520Occupancy%2520Grids%2520%2528M-SOG%2529%2520to%2520evaluate%2520LiDAR%250Aplacement%2520quality.%25202%2529%2520Leveraging%2520the%2520M-SOG%2520metric%252C%2520we%2520propose%2520a%2520novel%250Aoptimization%2520strategy%2520to%2520refine%2520multi-LiDAR%2520placements.%25203%2529%2520Centered%2520around%2520the%250Atheme%2520of%2520multi-condition%2520multi-LiDAR%2520perception%252C%2520we%2520collect%2520a%2520280%252C000-frame%250Adataset%2520from%2520both%2520clean%2520and%2520adverse%2520conditions.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520LiDAR%2520placements%2520optimized%2520using%2520our%2520approach%2520outperform%250Avarious%2520baselines.%2520We%2520showcase%2520exceptional%2520results%2520in%2520both%2520LiDAR%2520semantic%250Asegmentation%2520and%25203D%2520object%2520detection%2520tasks%252C%2520under%2520diverse%2520weather%2520and%2520sensor%250Afailure%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17009v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Your%20LiDAR%20Placement%20Optimized%20for%203D%20Scene%20Understanding%3F&entry.906535625=Ye%20Li%20and%20Lingdong%20Kong%20and%20Hanjiang%20Hu%20and%20Xiaohao%20Xu%20and%20Xiaonan%20Huang&entry.1292438233=%20%20The%20reliability%20of%20driving%20perception%20systems%20under%20unprecedented%20conditions%0Ais%20crucial%20for%20practical%20usage.%20Latest%20advancements%20have%20prompted%20increasing%0Ainterest%20in%20multi-LiDAR%20perception.%20However%2C%20prevailing%20driving%20datasets%0Apredominantly%20utilize%20single-LiDAR%20systems%20and%20collect%20data%20devoid%20of%20adverse%0Aconditions%2C%20failing%20to%20capture%20the%20complexities%20of%20real-world%20environments%0Aaccurately.%20Addressing%20these%20gaps%2C%20we%20proposed%20Place3D%2C%20a%20full-cycle%20pipeline%0Athat%20encompasses%20LiDAR%20placement%20optimization%2C%20data%20generation%2C%20and%20downstream%0Aevaluations.%20Our%20framework%20makes%20three%20appealing%20contributions.%201%29%20To%20identify%0Athe%20most%20effective%20configurations%20for%20multi-LiDAR%20systems%2C%20we%20introduce%20the%0ASurrogate%20Metric%20of%20the%20Semantic%20Occupancy%20Grids%20%28M-SOG%29%20to%20evaluate%20LiDAR%0Aplacement%20quality.%202%29%20Leveraging%20the%20M-SOG%20metric%2C%20we%20propose%20a%20novel%0Aoptimization%20strategy%20to%20refine%20multi-LiDAR%20placements.%203%29%20Centered%20around%20the%0Atheme%20of%20multi-condition%20multi-LiDAR%20perception%2C%20we%20collect%20a%20280%2C000-frame%0Adataset%20from%20both%20clean%20and%20adverse%20conditions.%20Extensive%20experiments%0Ademonstrate%20that%20LiDAR%20placements%20optimized%20using%20our%20approach%20outperform%0Avarious%20baselines.%20We%20showcase%20exceptional%20results%20in%20both%20LiDAR%20semantic%0Asegmentation%20and%203D%20object%20detection%20tasks%2C%20under%20diverse%20weather%20and%20sensor%0Afailure%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17009v2&entry.124074799=Read"},
{"title": "ELMGS: Enhancing memory and computation scaLability through coMpression\n  for 3D Gaussian Splatting", "author": "Muhammad Salman Ali and Sung-Ho Bae and Enzo Tartaglione", "abstract": "  3D models have recently been popularized by the potentiality of end-to-end\ntraining offered first by Neural Radiance Fields and most recently by 3D\nGaussian Splatting models. The latter has the big advantage of naturally\nproviding fast training convergence and high editability. However, as the\nresearch around these is still in its infancy, there is still a gap in the\nliterature regarding the model's scalability. In this work, we propose an\napproach enabling both memory and computation scalability of such models. More\nspecifically, we propose an iterative pruning strategy that removes redundant\ninformation encoded in the model. We also enhance compressibility for the model\nby including in the optimization strategy a differentiable quantization and\nentropy coding estimator. Our results on popular benchmarks showcase the\neffectiveness of the proposed approach and open the road to the broad\ndeployability of such a solution even on resource-constrained devices.\n", "link": "http://arxiv.org/abs/2410.23213v1", "date": "2024-10-30", "relevancy": 3.142, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6451}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6216}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ELMGS%3A%20Enhancing%20memory%20and%20computation%20scaLability%20through%20coMpression%0A%20%20for%203D%20Gaussian%20Splatting&body=Title%3A%20ELMGS%3A%20Enhancing%20memory%20and%20computation%20scaLability%20through%20coMpression%0A%20%20for%203D%20Gaussian%20Splatting%0AAuthor%3A%20Muhammad%20Salman%20Ali%20and%20Sung-Ho%20Bae%20and%20Enzo%20Tartaglione%0AAbstract%3A%20%20%203D%20models%20have%20recently%20been%20popularized%20by%20the%20potentiality%20of%20end-to-end%0Atraining%20offered%20first%20by%20Neural%20Radiance%20Fields%20and%20most%20recently%20by%203D%0AGaussian%20Splatting%20models.%20The%20latter%20has%20the%20big%20advantage%20of%20naturally%0Aproviding%20fast%20training%20convergence%20and%20high%20editability.%20However%2C%20as%20the%0Aresearch%20around%20these%20is%20still%20in%20its%20infancy%2C%20there%20is%20still%20a%20gap%20in%20the%0Aliterature%20regarding%20the%20model%27s%20scalability.%20In%20this%20work%2C%20we%20propose%20an%0Aapproach%20enabling%20both%20memory%20and%20computation%20scalability%20of%20such%20models.%20More%0Aspecifically%2C%20we%20propose%20an%20iterative%20pruning%20strategy%20that%20removes%20redundant%0Ainformation%20encoded%20in%20the%20model.%20We%20also%20enhance%20compressibility%20for%20the%20model%0Aby%20including%20in%20the%20optimization%20strategy%20a%20differentiable%20quantization%20and%0Aentropy%20coding%20estimator.%20Our%20results%20on%20popular%20benchmarks%20showcase%20the%0Aeffectiveness%20of%20the%20proposed%20approach%20and%20open%20the%20road%20to%20the%20broad%0Adeployability%20of%20such%20a%20solution%20even%20on%20resource-constrained%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DELMGS%253A%2520Enhancing%2520memory%2520and%2520computation%2520scaLability%2520through%2520coMpression%250A%2520%2520for%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DMuhammad%2520Salman%2520Ali%2520and%2520Sung-Ho%2520Bae%2520and%2520Enzo%2520Tartaglione%26entry.1292438233%3D%2520%25203D%2520models%2520have%2520recently%2520been%2520popularized%2520by%2520the%2520potentiality%2520of%2520end-to-end%250Atraining%2520offered%2520first%2520by%2520Neural%2520Radiance%2520Fields%2520and%2520most%2520recently%2520by%25203D%250AGaussian%2520Splatting%2520models.%2520The%2520latter%2520has%2520the%2520big%2520advantage%2520of%2520naturally%250Aproviding%2520fast%2520training%2520convergence%2520and%2520high%2520editability.%2520However%252C%2520as%2520the%250Aresearch%2520around%2520these%2520is%2520still%2520in%2520its%2520infancy%252C%2520there%2520is%2520still%2520a%2520gap%2520in%2520the%250Aliterature%2520regarding%2520the%2520model%2527s%2520scalability.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%250Aapproach%2520enabling%2520both%2520memory%2520and%2520computation%2520scalability%2520of%2520such%2520models.%2520More%250Aspecifically%252C%2520we%2520propose%2520an%2520iterative%2520pruning%2520strategy%2520that%2520removes%2520redundant%250Ainformation%2520encoded%2520in%2520the%2520model.%2520We%2520also%2520enhance%2520compressibility%2520for%2520the%2520model%250Aby%2520including%2520in%2520the%2520optimization%2520strategy%2520a%2520differentiable%2520quantization%2520and%250Aentropy%2520coding%2520estimator.%2520Our%2520results%2520on%2520popular%2520benchmarks%2520showcase%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520approach%2520and%2520open%2520the%2520road%2520to%2520the%2520broad%250Adeployability%2520of%2520such%2520a%2520solution%2520even%2520on%2520resource-constrained%2520devices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ELMGS%3A%20Enhancing%20memory%20and%20computation%20scaLability%20through%20coMpression%0A%20%20for%203D%20Gaussian%20Splatting&entry.906535625=Muhammad%20Salman%20Ali%20and%20Sung-Ho%20Bae%20and%20Enzo%20Tartaglione&entry.1292438233=%20%203D%20models%20have%20recently%20been%20popularized%20by%20the%20potentiality%20of%20end-to-end%0Atraining%20offered%20first%20by%20Neural%20Radiance%20Fields%20and%20most%20recently%20by%203D%0AGaussian%20Splatting%20models.%20The%20latter%20has%20the%20big%20advantage%20of%20naturally%0Aproviding%20fast%20training%20convergence%20and%20high%20editability.%20However%2C%20as%20the%0Aresearch%20around%20these%20is%20still%20in%20its%20infancy%2C%20there%20is%20still%20a%20gap%20in%20the%0Aliterature%20regarding%20the%20model%27s%20scalability.%20In%20this%20work%2C%20we%20propose%20an%0Aapproach%20enabling%20both%20memory%20and%20computation%20scalability%20of%20such%20models.%20More%0Aspecifically%2C%20we%20propose%20an%20iterative%20pruning%20strategy%20that%20removes%20redundant%0Ainformation%20encoded%20in%20the%20model.%20We%20also%20enhance%20compressibility%20for%20the%20model%0Aby%20including%20in%20the%20optimization%20strategy%20a%20differentiable%20quantization%20and%0Aentropy%20coding%20estimator.%20Our%20results%20on%20popular%20benchmarks%20showcase%20the%0Aeffectiveness%20of%20the%20proposed%20approach%20and%20open%20the%20road%20to%20the%20broad%0Adeployability%20of%20such%20a%20solution%20even%20on%20resource-constrained%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23213v1&entry.124074799=Read"},
{"title": "TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal\n  Foundation Models", "author": "Ziyao Shangguan and Chuhan Li and Yuxuan Ding and Yanan Zheng and Yilun Zhao and Tesca Fitzgerald and Arman Cohan", "abstract": "  Existing benchmarks often highlight the remarkable performance achieved by\nstate-of-the-art Multimodal Foundation Models (MFMs) in leveraging temporal\ncontext for video understanding. However, how well do the models truly perform\nvisual temporal reasoning? Our study of existing benchmarks shows that this\ncapability of MFMs is likely overestimated as many questions can be solved by\nusing a single, few, or out-of-order frames. To systematically examine current\nvisual temporal reasoning tasks, we propose three principles with corresponding\nmetrics: (1) Multi-Frame Gain, (2) Frame Order Sensitivity, and (3) Frame\nInformation Disparity. Following these principles, we introduce TOMATO,\nTemporal Reasoning Multimodal Evaluation, a novel benchmark crafted to\nrigorously assess MFMs' temporal reasoning capabilities in video understanding.\nTOMATO comprises 1,484 carefully curated, human-annotated questions spanning\nsix tasks (i.e., action count, direction, rotation, shape & trend, velocity &\nfrequency, and visual cues), applied to 1,417 videos, including 805\nself-recorded and -generated videos, that encompass human-centric, real-world,\nand simulated scenarios. Our comprehensive evaluation reveals a human-model\nperformance gap of 57.3% with the best-performing model. Moreover, our in-depth\nanalysis uncovers more fundamental limitations beyond this gap in current MFMs.\nWhile they can accurately recognize events in isolated frames, they fail to\ninterpret these frames as a continuous sequence. We believe TOMATO will serve\nas a crucial testbed for evaluating the next-generation MFMs and as a call to\nthe community to develop AI systems capable of comprehending human world\ndynamics through the video modality.\n", "link": "http://arxiv.org/abs/2410.23266v1", "date": "2024-10-30", "relevancy": 2.9658, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5999}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5999}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TOMATO%3A%20Assessing%20Visual%20Temporal%20Reasoning%20Capabilities%20in%20Multimodal%0A%20%20Foundation%20Models&body=Title%3A%20TOMATO%3A%20Assessing%20Visual%20Temporal%20Reasoning%20Capabilities%20in%20Multimodal%0A%20%20Foundation%20Models%0AAuthor%3A%20Ziyao%20Shangguan%20and%20Chuhan%20Li%20and%20Yuxuan%20Ding%20and%20Yanan%20Zheng%20and%20Yilun%20Zhao%20and%20Tesca%20Fitzgerald%20and%20Arman%20Cohan%0AAbstract%3A%20%20%20Existing%20benchmarks%20often%20highlight%20the%20remarkable%20performance%20achieved%20by%0Astate-of-the-art%20Multimodal%20Foundation%20Models%20%28MFMs%29%20in%20leveraging%20temporal%0Acontext%20for%20video%20understanding.%20However%2C%20how%20well%20do%20the%20models%20truly%20perform%0Avisual%20temporal%20reasoning%3F%20Our%20study%20of%20existing%20benchmarks%20shows%20that%20this%0Acapability%20of%20MFMs%20is%20likely%20overestimated%20as%20many%20questions%20can%20be%20solved%20by%0Ausing%20a%20single%2C%20few%2C%20or%20out-of-order%20frames.%20To%20systematically%20examine%20current%0Avisual%20temporal%20reasoning%20tasks%2C%20we%20propose%20three%20principles%20with%20corresponding%0Ametrics%3A%20%281%29%20Multi-Frame%20Gain%2C%20%282%29%20Frame%20Order%20Sensitivity%2C%20and%20%283%29%20Frame%0AInformation%20Disparity.%20Following%20these%20principles%2C%20we%20introduce%20TOMATO%2C%0ATemporal%20Reasoning%20Multimodal%20Evaluation%2C%20a%20novel%20benchmark%20crafted%20to%0Arigorously%20assess%20MFMs%27%20temporal%20reasoning%20capabilities%20in%20video%20understanding.%0ATOMATO%20comprises%201%2C484%20carefully%20curated%2C%20human-annotated%20questions%20spanning%0Asix%20tasks%20%28i.e.%2C%20action%20count%2C%20direction%2C%20rotation%2C%20shape%20%26%20trend%2C%20velocity%20%26%0Afrequency%2C%20and%20visual%20cues%29%2C%20applied%20to%201%2C417%20videos%2C%20including%20805%0Aself-recorded%20and%20-generated%20videos%2C%20that%20encompass%20human-centric%2C%20real-world%2C%0Aand%20simulated%20scenarios.%20Our%20comprehensive%20evaluation%20reveals%20a%20human-model%0Aperformance%20gap%20of%2057.3%25%20with%20the%20best-performing%20model.%20Moreover%2C%20our%20in-depth%0Aanalysis%20uncovers%20more%20fundamental%20limitations%20beyond%20this%20gap%20in%20current%20MFMs.%0AWhile%20they%20can%20accurately%20recognize%20events%20in%20isolated%20frames%2C%20they%20fail%20to%0Ainterpret%20these%20frames%20as%20a%20continuous%20sequence.%20We%20believe%20TOMATO%20will%20serve%0Aas%20a%20crucial%20testbed%20for%20evaluating%20the%20next-generation%20MFMs%20and%20as%20a%20call%20to%0Athe%20community%20to%20develop%20AI%20systems%20capable%20of%20comprehending%20human%20world%0Adynamics%20through%20the%20video%20modality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTOMATO%253A%2520Assessing%2520Visual%2520Temporal%2520Reasoning%2520Capabilities%2520in%2520Multimodal%250A%2520%2520Foundation%2520Models%26entry.906535625%3DZiyao%2520Shangguan%2520and%2520Chuhan%2520Li%2520and%2520Yuxuan%2520Ding%2520and%2520Yanan%2520Zheng%2520and%2520Yilun%2520Zhao%2520and%2520Tesca%2520Fitzgerald%2520and%2520Arman%2520Cohan%26entry.1292438233%3D%2520%2520Existing%2520benchmarks%2520often%2520highlight%2520the%2520remarkable%2520performance%2520achieved%2520by%250Astate-of-the-art%2520Multimodal%2520Foundation%2520Models%2520%2528MFMs%2529%2520in%2520leveraging%2520temporal%250Acontext%2520for%2520video%2520understanding.%2520However%252C%2520how%2520well%2520do%2520the%2520models%2520truly%2520perform%250Avisual%2520temporal%2520reasoning%253F%2520Our%2520study%2520of%2520existing%2520benchmarks%2520shows%2520that%2520this%250Acapability%2520of%2520MFMs%2520is%2520likely%2520overestimated%2520as%2520many%2520questions%2520can%2520be%2520solved%2520by%250Ausing%2520a%2520single%252C%2520few%252C%2520or%2520out-of-order%2520frames.%2520To%2520systematically%2520examine%2520current%250Avisual%2520temporal%2520reasoning%2520tasks%252C%2520we%2520propose%2520three%2520principles%2520with%2520corresponding%250Ametrics%253A%2520%25281%2529%2520Multi-Frame%2520Gain%252C%2520%25282%2529%2520Frame%2520Order%2520Sensitivity%252C%2520and%2520%25283%2529%2520Frame%250AInformation%2520Disparity.%2520Following%2520these%2520principles%252C%2520we%2520introduce%2520TOMATO%252C%250ATemporal%2520Reasoning%2520Multimodal%2520Evaluation%252C%2520a%2520novel%2520benchmark%2520crafted%2520to%250Arigorously%2520assess%2520MFMs%2527%2520temporal%2520reasoning%2520capabilities%2520in%2520video%2520understanding.%250ATOMATO%2520comprises%25201%252C484%2520carefully%2520curated%252C%2520human-annotated%2520questions%2520spanning%250Asix%2520tasks%2520%2528i.e.%252C%2520action%2520count%252C%2520direction%252C%2520rotation%252C%2520shape%2520%2526%2520trend%252C%2520velocity%2520%2526%250Afrequency%252C%2520and%2520visual%2520cues%2529%252C%2520applied%2520to%25201%252C417%2520videos%252C%2520including%2520805%250Aself-recorded%2520and%2520-generated%2520videos%252C%2520that%2520encompass%2520human-centric%252C%2520real-world%252C%250Aand%2520simulated%2520scenarios.%2520Our%2520comprehensive%2520evaluation%2520reveals%2520a%2520human-model%250Aperformance%2520gap%2520of%252057.3%2525%2520with%2520the%2520best-performing%2520model.%2520Moreover%252C%2520our%2520in-depth%250Aanalysis%2520uncovers%2520more%2520fundamental%2520limitations%2520beyond%2520this%2520gap%2520in%2520current%2520MFMs.%250AWhile%2520they%2520can%2520accurately%2520recognize%2520events%2520in%2520isolated%2520frames%252C%2520they%2520fail%2520to%250Ainterpret%2520these%2520frames%2520as%2520a%2520continuous%2520sequence.%2520We%2520believe%2520TOMATO%2520will%2520serve%250Aas%2520a%2520crucial%2520testbed%2520for%2520evaluating%2520the%2520next-generation%2520MFMs%2520and%2520as%2520a%2520call%2520to%250Athe%2520community%2520to%2520develop%2520AI%2520systems%2520capable%2520of%2520comprehending%2520human%2520world%250Adynamics%2520through%2520the%2520video%2520modality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TOMATO%3A%20Assessing%20Visual%20Temporal%20Reasoning%20Capabilities%20in%20Multimodal%0A%20%20Foundation%20Models&entry.906535625=Ziyao%20Shangguan%20and%20Chuhan%20Li%20and%20Yuxuan%20Ding%20and%20Yanan%20Zheng%20and%20Yilun%20Zhao%20and%20Tesca%20Fitzgerald%20and%20Arman%20Cohan&entry.1292438233=%20%20Existing%20benchmarks%20often%20highlight%20the%20remarkable%20performance%20achieved%20by%0Astate-of-the-art%20Multimodal%20Foundation%20Models%20%28MFMs%29%20in%20leveraging%20temporal%0Acontext%20for%20video%20understanding.%20However%2C%20how%20well%20do%20the%20models%20truly%20perform%0Avisual%20temporal%20reasoning%3F%20Our%20study%20of%20existing%20benchmarks%20shows%20that%20this%0Acapability%20of%20MFMs%20is%20likely%20overestimated%20as%20many%20questions%20can%20be%20solved%20by%0Ausing%20a%20single%2C%20few%2C%20or%20out-of-order%20frames.%20To%20systematically%20examine%20current%0Avisual%20temporal%20reasoning%20tasks%2C%20we%20propose%20three%20principles%20with%20corresponding%0Ametrics%3A%20%281%29%20Multi-Frame%20Gain%2C%20%282%29%20Frame%20Order%20Sensitivity%2C%20and%20%283%29%20Frame%0AInformation%20Disparity.%20Following%20these%20principles%2C%20we%20introduce%20TOMATO%2C%0ATemporal%20Reasoning%20Multimodal%20Evaluation%2C%20a%20novel%20benchmark%20crafted%20to%0Arigorously%20assess%20MFMs%27%20temporal%20reasoning%20capabilities%20in%20video%20understanding.%0ATOMATO%20comprises%201%2C484%20carefully%20curated%2C%20human-annotated%20questions%20spanning%0Asix%20tasks%20%28i.e.%2C%20action%20count%2C%20direction%2C%20rotation%2C%20shape%20%26%20trend%2C%20velocity%20%26%0Afrequency%2C%20and%20visual%20cues%29%2C%20applied%20to%201%2C417%20videos%2C%20including%20805%0Aself-recorded%20and%20-generated%20videos%2C%20that%20encompass%20human-centric%2C%20real-world%2C%0Aand%20simulated%20scenarios.%20Our%20comprehensive%20evaluation%20reveals%20a%20human-model%0Aperformance%20gap%20of%2057.3%25%20with%20the%20best-performing%20model.%20Moreover%2C%20our%20in-depth%0Aanalysis%20uncovers%20more%20fundamental%20limitations%20beyond%20this%20gap%20in%20current%20MFMs.%0AWhile%20they%20can%20accurately%20recognize%20events%20in%20isolated%20frames%2C%20they%20fail%20to%0Ainterpret%20these%20frames%20as%20a%20continuous%20sequence.%20We%20believe%20TOMATO%20will%20serve%0Aas%20a%20crucial%20testbed%20for%20evaluating%20the%20next-generation%20MFMs%20and%20as%20a%20call%20to%0Athe%20community%20to%20develop%20AI%20systems%20capable%20of%20comprehending%20human%20world%0Adynamics%20through%20the%20video%20modality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23266v1&entry.124074799=Read"},
{"title": "A Hitchhikers Guide to Fine-Grained Face Forgery Detection Using Common\n  Sense Reasoning", "author": "Niki Maria Foteinopoulou and Enjie Ghorbel and Djamila Aouada", "abstract": "  Explainability in artificial intelligence is crucial for restoring trust,\nparticularly in areas like face forgery detection, where viewers often struggle\nto distinguish between real and fabricated content. Vision and Large Language\nModels (VLLM) bridge computer vision and natural language, offering numerous\napplications driven by strong common-sense reasoning. Despite their success in\nvarious tasks, the potential of vision and language remains underexplored in\nface forgery detection, where they hold promise for enhancing explainability by\nleveraging the intrinsic reasoning capabilities of language to analyse\nfine-grained manipulation areas. As such, there is a need for a methodology\nthat converts face forgery detection to a Visual Question Answering (VQA) task\nto systematically and fairly evaluate these capabilities. Previous efforts for\nunified benchmarks in deepfake detection have focused on the simpler binary\ntask, overlooking evaluation protocols for fine-grained detection and\ntext-generative models. We propose a multi-staged approach that diverges from\nthe traditional binary decision paradigm to address this gap. In the first\nstage, we assess the models' performance on the binary task and their\nsensitivity to given instructions using several prompts. In the second stage,\nwe delve deeper into fine-grained detection by identifying areas of\nmanipulation in a multiple-choice VQA setting. In the third stage, we convert\nthe fine-grained detection to an open-ended question and compare several\nmatching strategies for the multi-label classification task. Finally, we\nqualitatively evaluate the fine-grained responses of the VLLMs included in the\nbenchmark. We apply our benchmark to several popular models, providing a\ndetailed comparison of binary, multiple-choice, and open-ended VQA evaluation\nacross seven datasets.\n\\url{https://nickyfot.github.io/hitchhickersguide.github.io/}\n", "link": "http://arxiv.org/abs/2410.00485v2", "date": "2024-10-30", "relevancy": 2.8819, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5798}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5798}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hitchhikers%20Guide%20to%20Fine-Grained%20Face%20Forgery%20Detection%20Using%20Common%0A%20%20Sense%20Reasoning&body=Title%3A%20A%20Hitchhikers%20Guide%20to%20Fine-Grained%20Face%20Forgery%20Detection%20Using%20Common%0A%20%20Sense%20Reasoning%0AAuthor%3A%20Niki%20Maria%20Foteinopoulou%20and%20Enjie%20Ghorbel%20and%20Djamila%20Aouada%0AAbstract%3A%20%20%20Explainability%20in%20artificial%20intelligence%20is%20crucial%20for%20restoring%20trust%2C%0Aparticularly%20in%20areas%20like%20face%20forgery%20detection%2C%20where%20viewers%20often%20struggle%0Ato%20distinguish%20between%20real%20and%20fabricated%20content.%20Vision%20and%20Large%20Language%0AModels%20%28VLLM%29%20bridge%20computer%20vision%20and%20natural%20language%2C%20offering%20numerous%0Aapplications%20driven%20by%20strong%20common-sense%20reasoning.%20Despite%20their%20success%20in%0Avarious%20tasks%2C%20the%20potential%20of%20vision%20and%20language%20remains%20underexplored%20in%0Aface%20forgery%20detection%2C%20where%20they%20hold%20promise%20for%20enhancing%20explainability%20by%0Aleveraging%20the%20intrinsic%20reasoning%20capabilities%20of%20language%20to%20analyse%0Afine-grained%20manipulation%20areas.%20As%20such%2C%20there%20is%20a%20need%20for%20a%20methodology%0Athat%20converts%20face%20forgery%20detection%20to%20a%20Visual%20Question%20Answering%20%28VQA%29%20task%0Ato%20systematically%20and%20fairly%20evaluate%20these%20capabilities.%20Previous%20efforts%20for%0Aunified%20benchmarks%20in%20deepfake%20detection%20have%20focused%20on%20the%20simpler%20binary%0Atask%2C%20overlooking%20evaluation%20protocols%20for%20fine-grained%20detection%20and%0Atext-generative%20models.%20We%20propose%20a%20multi-staged%20approach%20that%20diverges%20from%0Athe%20traditional%20binary%20decision%20paradigm%20to%20address%20this%20gap.%20In%20the%20first%0Astage%2C%20we%20assess%20the%20models%27%20performance%20on%20the%20binary%20task%20and%20their%0Asensitivity%20to%20given%20instructions%20using%20several%20prompts.%20In%20the%20second%20stage%2C%0Awe%20delve%20deeper%20into%20fine-grained%20detection%20by%20identifying%20areas%20of%0Amanipulation%20in%20a%20multiple-choice%20VQA%20setting.%20In%20the%20third%20stage%2C%20we%20convert%0Athe%20fine-grained%20detection%20to%20an%20open-ended%20question%20and%20compare%20several%0Amatching%20strategies%20for%20the%20multi-label%20classification%20task.%20Finally%2C%20we%0Aqualitatively%20evaluate%20the%20fine-grained%20responses%20of%20the%20VLLMs%20included%20in%20the%0Abenchmark.%20We%20apply%20our%20benchmark%20to%20several%20popular%20models%2C%20providing%20a%0Adetailed%20comparison%20of%20binary%2C%20multiple-choice%2C%20and%20open-ended%20VQA%20evaluation%0Aacross%20seven%20datasets.%0A%5Curl%7Bhttps%3A//nickyfot.github.io/hitchhickersguide.github.io/%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.00485v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hitchhikers%2520Guide%2520to%2520Fine-Grained%2520Face%2520Forgery%2520Detection%2520Using%2520Common%250A%2520%2520Sense%2520Reasoning%26entry.906535625%3DNiki%2520Maria%2520Foteinopoulou%2520and%2520Enjie%2520Ghorbel%2520and%2520Djamila%2520Aouada%26entry.1292438233%3D%2520%2520Explainability%2520in%2520artificial%2520intelligence%2520is%2520crucial%2520for%2520restoring%2520trust%252C%250Aparticularly%2520in%2520areas%2520like%2520face%2520forgery%2520detection%252C%2520where%2520viewers%2520often%2520struggle%250Ato%2520distinguish%2520between%2520real%2520and%2520fabricated%2520content.%2520Vision%2520and%2520Large%2520Language%250AModels%2520%2528VLLM%2529%2520bridge%2520computer%2520vision%2520and%2520natural%2520language%252C%2520offering%2520numerous%250Aapplications%2520driven%2520by%2520strong%2520common-sense%2520reasoning.%2520Despite%2520their%2520success%2520in%250Avarious%2520tasks%252C%2520the%2520potential%2520of%2520vision%2520and%2520language%2520remains%2520underexplored%2520in%250Aface%2520forgery%2520detection%252C%2520where%2520they%2520hold%2520promise%2520for%2520enhancing%2520explainability%2520by%250Aleveraging%2520the%2520intrinsic%2520reasoning%2520capabilities%2520of%2520language%2520to%2520analyse%250Afine-grained%2520manipulation%2520areas.%2520As%2520such%252C%2520there%2520is%2520a%2520need%2520for%2520a%2520methodology%250Athat%2520converts%2520face%2520forgery%2520detection%2520to%2520a%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520task%250Ato%2520systematically%2520and%2520fairly%2520evaluate%2520these%2520capabilities.%2520Previous%2520efforts%2520for%250Aunified%2520benchmarks%2520in%2520deepfake%2520detection%2520have%2520focused%2520on%2520the%2520simpler%2520binary%250Atask%252C%2520overlooking%2520evaluation%2520protocols%2520for%2520fine-grained%2520detection%2520and%250Atext-generative%2520models.%2520We%2520propose%2520a%2520multi-staged%2520approach%2520that%2520diverges%2520from%250Athe%2520traditional%2520binary%2520decision%2520paradigm%2520to%2520address%2520this%2520gap.%2520In%2520the%2520first%250Astage%252C%2520we%2520assess%2520the%2520models%2527%2520performance%2520on%2520the%2520binary%2520task%2520and%2520their%250Asensitivity%2520to%2520given%2520instructions%2520using%2520several%2520prompts.%2520In%2520the%2520second%2520stage%252C%250Awe%2520delve%2520deeper%2520into%2520fine-grained%2520detection%2520by%2520identifying%2520areas%2520of%250Amanipulation%2520in%2520a%2520multiple-choice%2520VQA%2520setting.%2520In%2520the%2520third%2520stage%252C%2520we%2520convert%250Athe%2520fine-grained%2520detection%2520to%2520an%2520open-ended%2520question%2520and%2520compare%2520several%250Amatching%2520strategies%2520for%2520the%2520multi-label%2520classification%2520task.%2520Finally%252C%2520we%250Aqualitatively%2520evaluate%2520the%2520fine-grained%2520responses%2520of%2520the%2520VLLMs%2520included%2520in%2520the%250Abenchmark.%2520We%2520apply%2520our%2520benchmark%2520to%2520several%2520popular%2520models%252C%2520providing%2520a%250Adetailed%2520comparison%2520of%2520binary%252C%2520multiple-choice%252C%2520and%2520open-ended%2520VQA%2520evaluation%250Aacross%2520seven%2520datasets.%250A%255Curl%257Bhttps%253A//nickyfot.github.io/hitchhickersguide.github.io/%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.00485v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hitchhikers%20Guide%20to%20Fine-Grained%20Face%20Forgery%20Detection%20Using%20Common%0A%20%20Sense%20Reasoning&entry.906535625=Niki%20Maria%20Foteinopoulou%20and%20Enjie%20Ghorbel%20and%20Djamila%20Aouada&entry.1292438233=%20%20Explainability%20in%20artificial%20intelligence%20is%20crucial%20for%20restoring%20trust%2C%0Aparticularly%20in%20areas%20like%20face%20forgery%20detection%2C%20where%20viewers%20often%20struggle%0Ato%20distinguish%20between%20real%20and%20fabricated%20content.%20Vision%20and%20Large%20Language%0AModels%20%28VLLM%29%20bridge%20computer%20vision%20and%20natural%20language%2C%20offering%20numerous%0Aapplications%20driven%20by%20strong%20common-sense%20reasoning.%20Despite%20their%20success%20in%0Avarious%20tasks%2C%20the%20potential%20of%20vision%20and%20language%20remains%20underexplored%20in%0Aface%20forgery%20detection%2C%20where%20they%20hold%20promise%20for%20enhancing%20explainability%20by%0Aleveraging%20the%20intrinsic%20reasoning%20capabilities%20of%20language%20to%20analyse%0Afine-grained%20manipulation%20areas.%20As%20such%2C%20there%20is%20a%20need%20for%20a%20methodology%0Athat%20converts%20face%20forgery%20detection%20to%20a%20Visual%20Question%20Answering%20%28VQA%29%20task%0Ato%20systematically%20and%20fairly%20evaluate%20these%20capabilities.%20Previous%20efforts%20for%0Aunified%20benchmarks%20in%20deepfake%20detection%20have%20focused%20on%20the%20simpler%20binary%0Atask%2C%20overlooking%20evaluation%20protocols%20for%20fine-grained%20detection%20and%0Atext-generative%20models.%20We%20propose%20a%20multi-staged%20approach%20that%20diverges%20from%0Athe%20traditional%20binary%20decision%20paradigm%20to%20address%20this%20gap.%20In%20the%20first%0Astage%2C%20we%20assess%20the%20models%27%20performance%20on%20the%20binary%20task%20and%20their%0Asensitivity%20to%20given%20instructions%20using%20several%20prompts.%20In%20the%20second%20stage%2C%0Awe%20delve%20deeper%20into%20fine-grained%20detection%20by%20identifying%20areas%20of%0Amanipulation%20in%20a%20multiple-choice%20VQA%20setting.%20In%20the%20third%20stage%2C%20we%20convert%0Athe%20fine-grained%20detection%20to%20an%20open-ended%20question%20and%20compare%20several%0Amatching%20strategies%20for%20the%20multi-label%20classification%20task.%20Finally%2C%20we%0Aqualitatively%20evaluate%20the%20fine-grained%20responses%20of%20the%20VLLMs%20included%20in%20the%0Abenchmark.%20We%20apply%20our%20benchmark%20to%20several%20popular%20models%2C%20providing%20a%0Adetailed%20comparison%20of%20binary%2C%20multiple-choice%2C%20and%20open-ended%20VQA%20evaluation%0Aacross%20seven%20datasets.%0A%5Curl%7Bhttps%3A//nickyfot.github.io/hitchhickersguide.github.io/%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.00485v2&entry.124074799=Read"},
{"title": "Nested ResNet: A Vision-Based Method for Detecting the Sensing Area of a\n  Drop-in Gamma Probe", "author": "Songyu Xu and Yicheng Hu and Jionglong Su and Daniel Elson and Baoru Huang", "abstract": "  Purpose: Drop-in gamma probes are widely used in robotic-assisted minimally\ninvasive surgery (RAMIS) for lymph node detection. However, these devices only\nprovide audio feedback on signal intensity, lacking the visual feedback\nnecessary for precise localisation. Previous work attempted to predict the\nsensing area location using laparoscopic images, but the prediction accuracy\nwas unsatisfactory. Improvements are needed in the deep learning-based\nregression approach.\n  Methods: We introduce a three-branch deep learning framework to predict the\nsensing area of the probe. Specifically, we utilise the stereo laparoscopic\nimages as input for the main branch and develop a Nested ResNet architecture.\nThe framework also incorporates depth estimation via transfer learning and\norientation guidance through probe axis sampling. The combined features from\neach branch enhanced the accuracy of the prediction.\n  Results: Our approach has been evaluated on a publicly available dataset,\ndemonstrating superior performance over previous methods. In particular, our\nmethod resulted in a 22.10\\% decrease in 2D mean error and a 41.67\\% reduction\nin 3D mean error. Additionally, qualitative comparisons further demonstrated\nthe improved precision of our approach.\n  Conclusion: With extensive evaluation, our solution significantly enhances\nthe accuracy and reliability of sensing area predictions. This advancement\nenables visual feedback during the use of the drop-in gamma probe in surgery,\nproviding surgeons with more accurate and reliable localisation.}\n", "link": "http://arxiv.org/abs/2410.23154v1", "date": "2024-10-30", "relevancy": 2.679, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5473}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.53}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.53}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nested%20ResNet%3A%20A%20Vision-Based%20Method%20for%20Detecting%20the%20Sensing%20Area%20of%20a%0A%20%20Drop-in%20Gamma%20Probe&body=Title%3A%20Nested%20ResNet%3A%20A%20Vision-Based%20Method%20for%20Detecting%20the%20Sensing%20Area%20of%20a%0A%20%20Drop-in%20Gamma%20Probe%0AAuthor%3A%20Songyu%20Xu%20and%20Yicheng%20Hu%20and%20Jionglong%20Su%20and%20Daniel%20Elson%20and%20Baoru%20Huang%0AAbstract%3A%20%20%20Purpose%3A%20Drop-in%20gamma%20probes%20are%20widely%20used%20in%20robotic-assisted%20minimally%0Ainvasive%20surgery%20%28RAMIS%29%20for%20lymph%20node%20detection.%20However%2C%20these%20devices%20only%0Aprovide%20audio%20feedback%20on%20signal%20intensity%2C%20lacking%20the%20visual%20feedback%0Anecessary%20for%20precise%20localisation.%20Previous%20work%20attempted%20to%20predict%20the%0Asensing%20area%20location%20using%20laparoscopic%20images%2C%20but%20the%20prediction%20accuracy%0Awas%20unsatisfactory.%20Improvements%20are%20needed%20in%20the%20deep%20learning-based%0Aregression%20approach.%0A%20%20Methods%3A%20We%20introduce%20a%20three-branch%20deep%20learning%20framework%20to%20predict%20the%0Asensing%20area%20of%20the%20probe.%20Specifically%2C%20we%20utilise%20the%20stereo%20laparoscopic%0Aimages%20as%20input%20for%20the%20main%20branch%20and%20develop%20a%20Nested%20ResNet%20architecture.%0AThe%20framework%20also%20incorporates%20depth%20estimation%20via%20transfer%20learning%20and%0Aorientation%20guidance%20through%20probe%20axis%20sampling.%20The%20combined%20features%20from%0Aeach%20branch%20enhanced%20the%20accuracy%20of%20the%20prediction.%0A%20%20Results%3A%20Our%20approach%20has%20been%20evaluated%20on%20a%20publicly%20available%20dataset%2C%0Ademonstrating%20superior%20performance%20over%20previous%20methods.%20In%20particular%2C%20our%0Amethod%20resulted%20in%20a%2022.10%5C%25%20decrease%20in%202D%20mean%20error%20and%20a%2041.67%5C%25%20reduction%0Ain%203D%20mean%20error.%20Additionally%2C%20qualitative%20comparisons%20further%20demonstrated%0Athe%20improved%20precision%20of%20our%20approach.%0A%20%20Conclusion%3A%20With%20extensive%20evaluation%2C%20our%20solution%20significantly%20enhances%0Athe%20accuracy%20and%20reliability%20of%20sensing%20area%20predictions.%20This%20advancement%0Aenables%20visual%20feedback%20during%20the%20use%20of%20the%20drop-in%20gamma%20probe%20in%20surgery%2C%0Aproviding%20surgeons%20with%20more%20accurate%20and%20reliable%20localisation.%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23154v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNested%2520ResNet%253A%2520A%2520Vision-Based%2520Method%2520for%2520Detecting%2520the%2520Sensing%2520Area%2520of%2520a%250A%2520%2520Drop-in%2520Gamma%2520Probe%26entry.906535625%3DSongyu%2520Xu%2520and%2520Yicheng%2520Hu%2520and%2520Jionglong%2520Su%2520and%2520Daniel%2520Elson%2520and%2520Baoru%2520Huang%26entry.1292438233%3D%2520%2520Purpose%253A%2520Drop-in%2520gamma%2520probes%2520are%2520widely%2520used%2520in%2520robotic-assisted%2520minimally%250Ainvasive%2520surgery%2520%2528RAMIS%2529%2520for%2520lymph%2520node%2520detection.%2520However%252C%2520these%2520devices%2520only%250Aprovide%2520audio%2520feedback%2520on%2520signal%2520intensity%252C%2520lacking%2520the%2520visual%2520feedback%250Anecessary%2520for%2520precise%2520localisation.%2520Previous%2520work%2520attempted%2520to%2520predict%2520the%250Asensing%2520area%2520location%2520using%2520laparoscopic%2520images%252C%2520but%2520the%2520prediction%2520accuracy%250Awas%2520unsatisfactory.%2520Improvements%2520are%2520needed%2520in%2520the%2520deep%2520learning-based%250Aregression%2520approach.%250A%2520%2520Methods%253A%2520We%2520introduce%2520a%2520three-branch%2520deep%2520learning%2520framework%2520to%2520predict%2520the%250Asensing%2520area%2520of%2520the%2520probe.%2520Specifically%252C%2520we%2520utilise%2520the%2520stereo%2520laparoscopic%250Aimages%2520as%2520input%2520for%2520the%2520main%2520branch%2520and%2520develop%2520a%2520Nested%2520ResNet%2520architecture.%250AThe%2520framework%2520also%2520incorporates%2520depth%2520estimation%2520via%2520transfer%2520learning%2520and%250Aorientation%2520guidance%2520through%2520probe%2520axis%2520sampling.%2520The%2520combined%2520features%2520from%250Aeach%2520branch%2520enhanced%2520the%2520accuracy%2520of%2520the%2520prediction.%250A%2520%2520Results%253A%2520Our%2520approach%2520has%2520been%2520evaluated%2520on%2520a%2520publicly%2520available%2520dataset%252C%250Ademonstrating%2520superior%2520performance%2520over%2520previous%2520methods.%2520In%2520particular%252C%2520our%250Amethod%2520resulted%2520in%2520a%252022.10%255C%2525%2520decrease%2520in%25202D%2520mean%2520error%2520and%2520a%252041.67%255C%2525%2520reduction%250Ain%25203D%2520mean%2520error.%2520Additionally%252C%2520qualitative%2520comparisons%2520further%2520demonstrated%250Athe%2520improved%2520precision%2520of%2520our%2520approach.%250A%2520%2520Conclusion%253A%2520With%2520extensive%2520evaluation%252C%2520our%2520solution%2520significantly%2520enhances%250Athe%2520accuracy%2520and%2520reliability%2520of%2520sensing%2520area%2520predictions.%2520This%2520advancement%250Aenables%2520visual%2520feedback%2520during%2520the%2520use%2520of%2520the%2520drop-in%2520gamma%2520probe%2520in%2520surgery%252C%250Aproviding%2520surgeons%2520with%2520more%2520accurate%2520and%2520reliable%2520localisation.%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23154v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nested%20ResNet%3A%20A%20Vision-Based%20Method%20for%20Detecting%20the%20Sensing%20Area%20of%20a%0A%20%20Drop-in%20Gamma%20Probe&entry.906535625=Songyu%20Xu%20and%20Yicheng%20Hu%20and%20Jionglong%20Su%20and%20Daniel%20Elson%20and%20Baoru%20Huang&entry.1292438233=%20%20Purpose%3A%20Drop-in%20gamma%20probes%20are%20widely%20used%20in%20robotic-assisted%20minimally%0Ainvasive%20surgery%20%28RAMIS%29%20for%20lymph%20node%20detection.%20However%2C%20these%20devices%20only%0Aprovide%20audio%20feedback%20on%20signal%20intensity%2C%20lacking%20the%20visual%20feedback%0Anecessary%20for%20precise%20localisation.%20Previous%20work%20attempted%20to%20predict%20the%0Asensing%20area%20location%20using%20laparoscopic%20images%2C%20but%20the%20prediction%20accuracy%0Awas%20unsatisfactory.%20Improvements%20are%20needed%20in%20the%20deep%20learning-based%0Aregression%20approach.%0A%20%20Methods%3A%20We%20introduce%20a%20three-branch%20deep%20learning%20framework%20to%20predict%20the%0Asensing%20area%20of%20the%20probe.%20Specifically%2C%20we%20utilise%20the%20stereo%20laparoscopic%0Aimages%20as%20input%20for%20the%20main%20branch%20and%20develop%20a%20Nested%20ResNet%20architecture.%0AThe%20framework%20also%20incorporates%20depth%20estimation%20via%20transfer%20learning%20and%0Aorientation%20guidance%20through%20probe%20axis%20sampling.%20The%20combined%20features%20from%0Aeach%20branch%20enhanced%20the%20accuracy%20of%20the%20prediction.%0A%20%20Results%3A%20Our%20approach%20has%20been%20evaluated%20on%20a%20publicly%20available%20dataset%2C%0Ademonstrating%20superior%20performance%20over%20previous%20methods.%20In%20particular%2C%20our%0Amethod%20resulted%20in%20a%2022.10%5C%25%20decrease%20in%202D%20mean%20error%20and%20a%2041.67%5C%25%20reduction%0Ain%203D%20mean%20error.%20Additionally%2C%20qualitative%20comparisons%20further%20demonstrated%0Athe%20improved%20precision%20of%20our%20approach.%0A%20%20Conclusion%3A%20With%20extensive%20evaluation%2C%20our%20solution%20significantly%20enhances%0Athe%20accuracy%20and%20reliability%20of%20sensing%20area%20predictions.%20This%20advancement%0Aenables%20visual%20feedback%20during%20the%20use%20of%20the%20drop-in%20gamma%20probe%20in%20surgery%2C%0Aproviding%20surgeons%20with%20more%20accurate%20and%20reliable%20localisation.%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23154v1&entry.124074799=Read"},
{"title": "OpenSatMap: A Fine-grained High-resolution Satellite Dataset for\n  Large-scale Map Construction", "author": "Hongbo Zhao and Lue Fan and Yuntao Chen and Haochen Wang and yuran Yang and Xiaojuan Jin and Yixin Zhang and Gaofeng Meng and Zhaoxiang Zhang", "abstract": "  In this paper, we propose OpenSatMap, a fine-grained, high-resolution\nsatellite dataset for large-scale map construction. Map construction is one of\nthe foundations of the transportation industry, such as navigation and\nautonomous driving. Extracting road structures from satellite images is an\nefficient way to construct large-scale maps. However, existing satellite\ndatasets provide only coarse semantic-level labels with a relatively low\nresolution (up to level 19), impeding the advancement of this field. In\ncontrast, the proposed OpenSatMap (1) has fine-grained instance-level\nannotations; (2) consists of high-resolution images (level 20); (3) is\ncurrently the largest one of its kind; (4) collects data with high diversity.\nMoreover, OpenSatMap covers and aligns with the popular nuScenes dataset and\nArgoverse 2 dataset to potentially advance autonomous driving technologies. By\npublishing and maintaining the dataset, we provide a high-quality benchmark for\nsatellite-based map construction and downstream tasks like autonomous driving.\n", "link": "http://arxiv.org/abs/2410.23278v1", "date": "2024-10-30", "relevancy": 2.6417, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5773}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5272}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenSatMap%3A%20A%20Fine-grained%20High-resolution%20Satellite%20Dataset%20for%0A%20%20Large-scale%20Map%20Construction&body=Title%3A%20OpenSatMap%3A%20A%20Fine-grained%20High-resolution%20Satellite%20Dataset%20for%0A%20%20Large-scale%20Map%20Construction%0AAuthor%3A%20Hongbo%20Zhao%20and%20Lue%20Fan%20and%20Yuntao%20Chen%20and%20Haochen%20Wang%20and%20yuran%20Yang%20and%20Xiaojuan%20Jin%20and%20Yixin%20Zhang%20and%20Gaofeng%20Meng%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20OpenSatMap%2C%20a%20fine-grained%2C%20high-resolution%0Asatellite%20dataset%20for%20large-scale%20map%20construction.%20Map%20construction%20is%20one%20of%0Athe%20foundations%20of%20the%20transportation%20industry%2C%20such%20as%20navigation%20and%0Aautonomous%20driving.%20Extracting%20road%20structures%20from%20satellite%20images%20is%20an%0Aefficient%20way%20to%20construct%20large-scale%20maps.%20However%2C%20existing%20satellite%0Adatasets%20provide%20only%20coarse%20semantic-level%20labels%20with%20a%20relatively%20low%0Aresolution%20%28up%20to%20level%2019%29%2C%20impeding%20the%20advancement%20of%20this%20field.%20In%0Acontrast%2C%20the%20proposed%20OpenSatMap%20%281%29%20has%20fine-grained%20instance-level%0Aannotations%3B%20%282%29%20consists%20of%20high-resolution%20images%20%28level%2020%29%3B%20%283%29%20is%0Acurrently%20the%20largest%20one%20of%20its%20kind%3B%20%284%29%20collects%20data%20with%20high%20diversity.%0AMoreover%2C%20OpenSatMap%20covers%20and%20aligns%20with%20the%20popular%20nuScenes%20dataset%20and%0AArgoverse%202%20dataset%20to%20potentially%20advance%20autonomous%20driving%20technologies.%20By%0Apublishing%20and%20maintaining%20the%20dataset%2C%20we%20provide%20a%20high-quality%20benchmark%20for%0Asatellite-based%20map%20construction%20and%20downstream%20tasks%20like%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23278v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenSatMap%253A%2520A%2520Fine-grained%2520High-resolution%2520Satellite%2520Dataset%2520for%250A%2520%2520Large-scale%2520Map%2520Construction%26entry.906535625%3DHongbo%2520Zhao%2520and%2520Lue%2520Fan%2520and%2520Yuntao%2520Chen%2520and%2520Haochen%2520Wang%2520and%2520yuran%2520Yang%2520and%2520Xiaojuan%2520Jin%2520and%2520Yixin%2520Zhang%2520and%2520Gaofeng%2520Meng%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520OpenSatMap%252C%2520a%2520fine-grained%252C%2520high-resolution%250Asatellite%2520dataset%2520for%2520large-scale%2520map%2520construction.%2520Map%2520construction%2520is%2520one%2520of%250Athe%2520foundations%2520of%2520the%2520transportation%2520industry%252C%2520such%2520as%2520navigation%2520and%250Aautonomous%2520driving.%2520Extracting%2520road%2520structures%2520from%2520satellite%2520images%2520is%2520an%250Aefficient%2520way%2520to%2520construct%2520large-scale%2520maps.%2520However%252C%2520existing%2520satellite%250Adatasets%2520provide%2520only%2520coarse%2520semantic-level%2520labels%2520with%2520a%2520relatively%2520low%250Aresolution%2520%2528up%2520to%2520level%252019%2529%252C%2520impeding%2520the%2520advancement%2520of%2520this%2520field.%2520In%250Acontrast%252C%2520the%2520proposed%2520OpenSatMap%2520%25281%2529%2520has%2520fine-grained%2520instance-level%250Aannotations%253B%2520%25282%2529%2520consists%2520of%2520high-resolution%2520images%2520%2528level%252020%2529%253B%2520%25283%2529%2520is%250Acurrently%2520the%2520largest%2520one%2520of%2520its%2520kind%253B%2520%25284%2529%2520collects%2520data%2520with%2520high%2520diversity.%250AMoreover%252C%2520OpenSatMap%2520covers%2520and%2520aligns%2520with%2520the%2520popular%2520nuScenes%2520dataset%2520and%250AArgoverse%25202%2520dataset%2520to%2520potentially%2520advance%2520autonomous%2520driving%2520technologies.%2520By%250Apublishing%2520and%2520maintaining%2520the%2520dataset%252C%2520we%2520provide%2520a%2520high-quality%2520benchmark%2520for%250Asatellite-based%2520map%2520construction%2520and%2520downstream%2520tasks%2520like%2520autonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23278v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenSatMap%3A%20A%20Fine-grained%20High-resolution%20Satellite%20Dataset%20for%0A%20%20Large-scale%20Map%20Construction&entry.906535625=Hongbo%20Zhao%20and%20Lue%20Fan%20and%20Yuntao%20Chen%20and%20Haochen%20Wang%20and%20yuran%20Yang%20and%20Xiaojuan%20Jin%20and%20Yixin%20Zhang%20and%20Gaofeng%20Meng%20and%20Zhaoxiang%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20OpenSatMap%2C%20a%20fine-grained%2C%20high-resolution%0Asatellite%20dataset%20for%20large-scale%20map%20construction.%20Map%20construction%20is%20one%20of%0Athe%20foundations%20of%20the%20transportation%20industry%2C%20such%20as%20navigation%20and%0Aautonomous%20driving.%20Extracting%20road%20structures%20from%20satellite%20images%20is%20an%0Aefficient%20way%20to%20construct%20large-scale%20maps.%20However%2C%20existing%20satellite%0Adatasets%20provide%20only%20coarse%20semantic-level%20labels%20with%20a%20relatively%20low%0Aresolution%20%28up%20to%20level%2019%29%2C%20impeding%20the%20advancement%20of%20this%20field.%20In%0Acontrast%2C%20the%20proposed%20OpenSatMap%20%281%29%20has%20fine-grained%20instance-level%0Aannotations%3B%20%282%29%20consists%20of%20high-resolution%20images%20%28level%2020%29%3B%20%283%29%20is%0Acurrently%20the%20largest%20one%20of%20its%20kind%3B%20%284%29%20collects%20data%20with%20high%20diversity.%0AMoreover%2C%20OpenSatMap%20covers%20and%20aligns%20with%20the%20popular%20nuScenes%20dataset%20and%0AArgoverse%202%20dataset%20to%20potentially%20advance%20autonomous%20driving%20technologies.%20By%0Apublishing%20and%20maintaining%20the%20dataset%2C%20we%20provide%20a%20high-quality%20benchmark%20for%0Asatellite-based%20map%20construction%20and%20downstream%20tasks%20like%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23278v1&entry.124074799=Read"},
{"title": "LGU-SLAM: Learnable Gaussian Uncertainty Matching with Deformable\n  Correlation Sampling for Deep Visual SLAM", "author": "Yucheng Huang and Luping Ji and Hudong Liu and Mao Ye", "abstract": "  Deep visual Simultaneous Localization and Mapping (SLAM) techniques, e.g.,\nDROID, have made significant advancements by leveraging deep visual odometry on\ndense flow fields. In general, they heavily rely on global visual similarity\nmatching. However, the ambiguous similarity interference in uncertain regions\ncould often lead to excessive noise in correspondences, ultimately misleading\nSLAM in geometric modeling. To address this issue, we propose a Learnable\nGaussian Uncertainty (LGU) matching. It mainly focuses on precise\ncorrespondence construction. In our scheme, a learnable 2D Gaussian uncertainty\nmodel is designed to associate matching-frame pairs. It could generate\ninput-dependent Gaussian distributions for each correspondence map.\nAdditionally, a multi-scale deformable correlation sampling strategy is devised\nto adaptively fine-tune the sampling of each direction by a priori look-up\nranges, enabling reliable correlation construction. Furthermore, a KAN-bias GRU\ncomponent is adopted to improve a temporal iterative enhancement for\naccomplishing sophisticated spatio-temporal modeling with limited parameters.\nThe extensive experiments on real-world and synthetic datasets are conducted to\nvalidate the effectiveness and superiority of our method.\n", "link": "http://arxiv.org/abs/2410.23231v1", "date": "2024-10-30", "relevancy": 2.6355, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6957}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6454}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LGU-SLAM%3A%20Learnable%20Gaussian%20Uncertainty%20Matching%20with%20Deformable%0A%20%20Correlation%20Sampling%20for%20Deep%20Visual%20SLAM&body=Title%3A%20LGU-SLAM%3A%20Learnable%20Gaussian%20Uncertainty%20Matching%20with%20Deformable%0A%20%20Correlation%20Sampling%20for%20Deep%20Visual%20SLAM%0AAuthor%3A%20Yucheng%20Huang%20and%20Luping%20Ji%20and%20Hudong%20Liu%20and%20Mao%20Ye%0AAbstract%3A%20%20%20Deep%20visual%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20techniques%2C%20e.g.%2C%0ADROID%2C%20have%20made%20significant%20advancements%20by%20leveraging%20deep%20visual%20odometry%20on%0Adense%20flow%20fields.%20In%20general%2C%20they%20heavily%20rely%20on%20global%20visual%20similarity%0Amatching.%20However%2C%20the%20ambiguous%20similarity%20interference%20in%20uncertain%20regions%0Acould%20often%20lead%20to%20excessive%20noise%20in%20correspondences%2C%20ultimately%20misleading%0ASLAM%20in%20geometric%20modeling.%20To%20address%20this%20issue%2C%20we%20propose%20a%20Learnable%0AGaussian%20Uncertainty%20%28LGU%29%20matching.%20It%20mainly%20focuses%20on%20precise%0Acorrespondence%20construction.%20In%20our%20scheme%2C%20a%20learnable%202D%20Gaussian%20uncertainty%0Amodel%20is%20designed%20to%20associate%20matching-frame%20pairs.%20It%20could%20generate%0Ainput-dependent%20Gaussian%20distributions%20for%20each%20correspondence%20map.%0AAdditionally%2C%20a%20multi-scale%20deformable%20correlation%20sampling%20strategy%20is%20devised%0Ato%20adaptively%20fine-tune%20the%20sampling%20of%20each%20direction%20by%20a%20priori%20look-up%0Aranges%2C%20enabling%20reliable%20correlation%20construction.%20Furthermore%2C%20a%20KAN-bias%20GRU%0Acomponent%20is%20adopted%20to%20improve%20a%20temporal%20iterative%20enhancement%20for%0Aaccomplishing%20sophisticated%20spatio-temporal%20modeling%20with%20limited%20parameters.%0AThe%20extensive%20experiments%20on%20real-world%20and%20synthetic%20datasets%20are%20conducted%20to%0Avalidate%20the%20effectiveness%20and%20superiority%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23231v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLGU-SLAM%253A%2520Learnable%2520Gaussian%2520Uncertainty%2520Matching%2520with%2520Deformable%250A%2520%2520Correlation%2520Sampling%2520for%2520Deep%2520Visual%2520SLAM%26entry.906535625%3DYucheng%2520Huang%2520and%2520Luping%2520Ji%2520and%2520Hudong%2520Liu%2520and%2520Mao%2520Ye%26entry.1292438233%3D%2520%2520Deep%2520visual%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520techniques%252C%2520e.g.%252C%250ADROID%252C%2520have%2520made%2520significant%2520advancements%2520by%2520leveraging%2520deep%2520visual%2520odometry%2520on%250Adense%2520flow%2520fields.%2520In%2520general%252C%2520they%2520heavily%2520rely%2520on%2520global%2520visual%2520similarity%250Amatching.%2520However%252C%2520the%2520ambiguous%2520similarity%2520interference%2520in%2520uncertain%2520regions%250Acould%2520often%2520lead%2520to%2520excessive%2520noise%2520in%2520correspondences%252C%2520ultimately%2520misleading%250ASLAM%2520in%2520geometric%2520modeling.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520Learnable%250AGaussian%2520Uncertainty%2520%2528LGU%2529%2520matching.%2520It%2520mainly%2520focuses%2520on%2520precise%250Acorrespondence%2520construction.%2520In%2520our%2520scheme%252C%2520a%2520learnable%25202D%2520Gaussian%2520uncertainty%250Amodel%2520is%2520designed%2520to%2520associate%2520matching-frame%2520pairs.%2520It%2520could%2520generate%250Ainput-dependent%2520Gaussian%2520distributions%2520for%2520each%2520correspondence%2520map.%250AAdditionally%252C%2520a%2520multi-scale%2520deformable%2520correlation%2520sampling%2520strategy%2520is%2520devised%250Ato%2520adaptively%2520fine-tune%2520the%2520sampling%2520of%2520each%2520direction%2520by%2520a%2520priori%2520look-up%250Aranges%252C%2520enabling%2520reliable%2520correlation%2520construction.%2520Furthermore%252C%2520a%2520KAN-bias%2520GRU%250Acomponent%2520is%2520adopted%2520to%2520improve%2520a%2520temporal%2520iterative%2520enhancement%2520for%250Aaccomplishing%2520sophisticated%2520spatio-temporal%2520modeling%2520with%2520limited%2520parameters.%250AThe%2520extensive%2520experiments%2520on%2520real-world%2520and%2520synthetic%2520datasets%2520are%2520conducted%2520to%250Avalidate%2520the%2520effectiveness%2520and%2520superiority%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23231v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LGU-SLAM%3A%20Learnable%20Gaussian%20Uncertainty%20Matching%20with%20Deformable%0A%20%20Correlation%20Sampling%20for%20Deep%20Visual%20SLAM&entry.906535625=Yucheng%20Huang%20and%20Luping%20Ji%20and%20Hudong%20Liu%20and%20Mao%20Ye&entry.1292438233=%20%20Deep%20visual%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20techniques%2C%20e.g.%2C%0ADROID%2C%20have%20made%20significant%20advancements%20by%20leveraging%20deep%20visual%20odometry%20on%0Adense%20flow%20fields.%20In%20general%2C%20they%20heavily%20rely%20on%20global%20visual%20similarity%0Amatching.%20However%2C%20the%20ambiguous%20similarity%20interference%20in%20uncertain%20regions%0Acould%20often%20lead%20to%20excessive%20noise%20in%20correspondences%2C%20ultimately%20misleading%0ASLAM%20in%20geometric%20modeling.%20To%20address%20this%20issue%2C%20we%20propose%20a%20Learnable%0AGaussian%20Uncertainty%20%28LGU%29%20matching.%20It%20mainly%20focuses%20on%20precise%0Acorrespondence%20construction.%20In%20our%20scheme%2C%20a%20learnable%202D%20Gaussian%20uncertainty%0Amodel%20is%20designed%20to%20associate%20matching-frame%20pairs.%20It%20could%20generate%0Ainput-dependent%20Gaussian%20distributions%20for%20each%20correspondence%20map.%0AAdditionally%2C%20a%20multi-scale%20deformable%20correlation%20sampling%20strategy%20is%20devised%0Ato%20adaptively%20fine-tune%20the%20sampling%20of%20each%20direction%20by%20a%20priori%20look-up%0Aranges%2C%20enabling%20reliable%20correlation%20construction.%20Furthermore%2C%20a%20KAN-bias%20GRU%0Acomponent%20is%20adopted%20to%20improve%20a%20temporal%20iterative%20enhancement%20for%0Aaccomplishing%20sophisticated%20spatio-temporal%20modeling%20with%20limited%20parameters.%0AThe%20extensive%20experiments%20on%20real-world%20and%20synthetic%20datasets%20are%20conducted%20to%0Avalidate%20the%20effectiveness%20and%20superiority%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23231v1&entry.124074799=Read"},
{"title": "HEX: Hierarchical Emergence Exploitation in Self-Supervised Algorithms", "author": "Kiran Kokilepersaud and Seulgi Kim and Mohit Prabhushankar and Ghassan AlRegib", "abstract": "  In this paper, we propose an algorithm that can be used on top of a wide\nvariety of self-supervised (SSL) approaches to take advantage of hierarchical\nstructures that emerge during training. SSL approaches typically work through\nsome invariance term to ensure consistency between similar samples and a\nregularization term to prevent global dimensional collapse. Dimensional\ncollapse refers to data representations spanning a lower-dimensional subspace.\nRecent work has demonstrated that the representation space of these algorithms\ngradually reflects a semantic hierarchical structure as training progresses.\nData samples of the same hierarchical grouping tend to exhibit greater\ndimensional collapse locally compared to the dataset as a whole due to sharing\nfeatures in common with each other. Ideally, SSL algorithms would take\nadvantage of this hierarchical emergence to have an additional regularization\nterm to account for this local dimensional collapse effect. However, the\nconstruction of existing SSL algorithms does not account for this property. To\naddress this, we propose an adaptive algorithm that performs a weighted\ndecomposition of the denominator of the InfoNCE loss into two terms: local\nhierarchical and global collapse regularization respectively. This\ndecomposition is based on an adaptive threshold that gradually lowers to\nreflect the emerging hierarchical structure of the representation space\nthroughout training. It is based on an analysis of the cosine similarity\ndistribution of samples in a batch. We demonstrate that this hierarchical\nemergence exploitation (HEX) approach can be integrated across a wide variety\nof SSL algorithms. Empirically, we show performance improvements of up to 5.6%\nrelative improvement over baseline SSL approaches on classification accuracy on\nImagenet with 100 epochs of training.\n", "link": "http://arxiv.org/abs/2410.23200v1", "date": "2024-10-30", "relevancy": 2.6346, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5538}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5188}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HEX%3A%20Hierarchical%20Emergence%20Exploitation%20in%20Self-Supervised%20Algorithms&body=Title%3A%20HEX%3A%20Hierarchical%20Emergence%20Exploitation%20in%20Self-Supervised%20Algorithms%0AAuthor%3A%20Kiran%20Kokilepersaud%20and%20Seulgi%20Kim%20and%20Mohit%20Prabhushankar%20and%20Ghassan%20AlRegib%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20an%20algorithm%20that%20can%20be%20used%20on%20top%20of%20a%20wide%0Avariety%20of%20self-supervised%20%28SSL%29%20approaches%20to%20take%20advantage%20of%20hierarchical%0Astructures%20that%20emerge%20during%20training.%20SSL%20approaches%20typically%20work%20through%0Asome%20invariance%20term%20to%20ensure%20consistency%20between%20similar%20samples%20and%20a%0Aregularization%20term%20to%20prevent%20global%20dimensional%20collapse.%20Dimensional%0Acollapse%20refers%20to%20data%20representations%20spanning%20a%20lower-dimensional%20subspace.%0ARecent%20work%20has%20demonstrated%20that%20the%20representation%20space%20of%20these%20algorithms%0Agradually%20reflects%20a%20semantic%20hierarchical%20structure%20as%20training%20progresses.%0AData%20samples%20of%20the%20same%20hierarchical%20grouping%20tend%20to%20exhibit%20greater%0Adimensional%20collapse%20locally%20compared%20to%20the%20dataset%20as%20a%20whole%20due%20to%20sharing%0Afeatures%20in%20common%20with%20each%20other.%20Ideally%2C%20SSL%20algorithms%20would%20take%0Aadvantage%20of%20this%20hierarchical%20emergence%20to%20have%20an%20additional%20regularization%0Aterm%20to%20account%20for%20this%20local%20dimensional%20collapse%20effect.%20However%2C%20the%0Aconstruction%20of%20existing%20SSL%20algorithms%20does%20not%20account%20for%20this%20property.%20To%0Aaddress%20this%2C%20we%20propose%20an%20adaptive%20algorithm%20that%20performs%20a%20weighted%0Adecomposition%20of%20the%20denominator%20of%20the%20InfoNCE%20loss%20into%20two%20terms%3A%20local%0Ahierarchical%20and%20global%20collapse%20regularization%20respectively.%20This%0Adecomposition%20is%20based%20on%20an%20adaptive%20threshold%20that%20gradually%20lowers%20to%0Areflect%20the%20emerging%20hierarchical%20structure%20of%20the%20representation%20space%0Athroughout%20training.%20It%20is%20based%20on%20an%20analysis%20of%20the%20cosine%20similarity%0Adistribution%20of%20samples%20in%20a%20batch.%20We%20demonstrate%20that%20this%20hierarchical%0Aemergence%20exploitation%20%28HEX%29%20approach%20can%20be%20integrated%20across%20a%20wide%20variety%0Aof%20SSL%20algorithms.%20Empirically%2C%20we%20show%20performance%20improvements%20of%20up%20to%205.6%25%0Arelative%20improvement%20over%20baseline%20SSL%20approaches%20on%20classification%20accuracy%20on%0AImagenet%20with%20100%20epochs%20of%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHEX%253A%2520Hierarchical%2520Emergence%2520Exploitation%2520in%2520Self-Supervised%2520Algorithms%26entry.906535625%3DKiran%2520Kokilepersaud%2520and%2520Seulgi%2520Kim%2520and%2520Mohit%2520Prabhushankar%2520and%2520Ghassan%2520AlRegib%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520algorithm%2520that%2520can%2520be%2520used%2520on%2520top%2520of%2520a%2520wide%250Avariety%2520of%2520self-supervised%2520%2528SSL%2529%2520approaches%2520to%2520take%2520advantage%2520of%2520hierarchical%250Astructures%2520that%2520emerge%2520during%2520training.%2520SSL%2520approaches%2520typically%2520work%2520through%250Asome%2520invariance%2520term%2520to%2520ensure%2520consistency%2520between%2520similar%2520samples%2520and%2520a%250Aregularization%2520term%2520to%2520prevent%2520global%2520dimensional%2520collapse.%2520Dimensional%250Acollapse%2520refers%2520to%2520data%2520representations%2520spanning%2520a%2520lower-dimensional%2520subspace.%250ARecent%2520work%2520has%2520demonstrated%2520that%2520the%2520representation%2520space%2520of%2520these%2520algorithms%250Agradually%2520reflects%2520a%2520semantic%2520hierarchical%2520structure%2520as%2520training%2520progresses.%250AData%2520samples%2520of%2520the%2520same%2520hierarchical%2520grouping%2520tend%2520to%2520exhibit%2520greater%250Adimensional%2520collapse%2520locally%2520compared%2520to%2520the%2520dataset%2520as%2520a%2520whole%2520due%2520to%2520sharing%250Afeatures%2520in%2520common%2520with%2520each%2520other.%2520Ideally%252C%2520SSL%2520algorithms%2520would%2520take%250Aadvantage%2520of%2520this%2520hierarchical%2520emergence%2520to%2520have%2520an%2520additional%2520regularization%250Aterm%2520to%2520account%2520for%2520this%2520local%2520dimensional%2520collapse%2520effect.%2520However%252C%2520the%250Aconstruction%2520of%2520existing%2520SSL%2520algorithms%2520does%2520not%2520account%2520for%2520this%2520property.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520an%2520adaptive%2520algorithm%2520that%2520performs%2520a%2520weighted%250Adecomposition%2520of%2520the%2520denominator%2520of%2520the%2520InfoNCE%2520loss%2520into%2520two%2520terms%253A%2520local%250Ahierarchical%2520and%2520global%2520collapse%2520regularization%2520respectively.%2520This%250Adecomposition%2520is%2520based%2520on%2520an%2520adaptive%2520threshold%2520that%2520gradually%2520lowers%2520to%250Areflect%2520the%2520emerging%2520hierarchical%2520structure%2520of%2520the%2520representation%2520space%250Athroughout%2520training.%2520It%2520is%2520based%2520on%2520an%2520analysis%2520of%2520the%2520cosine%2520similarity%250Adistribution%2520of%2520samples%2520in%2520a%2520batch.%2520We%2520demonstrate%2520that%2520this%2520hierarchical%250Aemergence%2520exploitation%2520%2528HEX%2529%2520approach%2520can%2520be%2520integrated%2520across%2520a%2520wide%2520variety%250Aof%2520SSL%2520algorithms.%2520Empirically%252C%2520we%2520show%2520performance%2520improvements%2520of%2520up%2520to%25205.6%2525%250Arelative%2520improvement%2520over%2520baseline%2520SSL%2520approaches%2520on%2520classification%2520accuracy%2520on%250AImagenet%2520with%2520100%2520epochs%2520of%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HEX%3A%20Hierarchical%20Emergence%20Exploitation%20in%20Self-Supervised%20Algorithms&entry.906535625=Kiran%20Kokilepersaud%20and%20Seulgi%20Kim%20and%20Mohit%20Prabhushankar%20and%20Ghassan%20AlRegib&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20an%20algorithm%20that%20can%20be%20used%20on%20top%20of%20a%20wide%0Avariety%20of%20self-supervised%20%28SSL%29%20approaches%20to%20take%20advantage%20of%20hierarchical%0Astructures%20that%20emerge%20during%20training.%20SSL%20approaches%20typically%20work%20through%0Asome%20invariance%20term%20to%20ensure%20consistency%20between%20similar%20samples%20and%20a%0Aregularization%20term%20to%20prevent%20global%20dimensional%20collapse.%20Dimensional%0Acollapse%20refers%20to%20data%20representations%20spanning%20a%20lower-dimensional%20subspace.%0ARecent%20work%20has%20demonstrated%20that%20the%20representation%20space%20of%20these%20algorithms%0Agradually%20reflects%20a%20semantic%20hierarchical%20structure%20as%20training%20progresses.%0AData%20samples%20of%20the%20same%20hierarchical%20grouping%20tend%20to%20exhibit%20greater%0Adimensional%20collapse%20locally%20compared%20to%20the%20dataset%20as%20a%20whole%20due%20to%20sharing%0Afeatures%20in%20common%20with%20each%20other.%20Ideally%2C%20SSL%20algorithms%20would%20take%0Aadvantage%20of%20this%20hierarchical%20emergence%20to%20have%20an%20additional%20regularization%0Aterm%20to%20account%20for%20this%20local%20dimensional%20collapse%20effect.%20However%2C%20the%0Aconstruction%20of%20existing%20SSL%20algorithms%20does%20not%20account%20for%20this%20property.%20To%0Aaddress%20this%2C%20we%20propose%20an%20adaptive%20algorithm%20that%20performs%20a%20weighted%0Adecomposition%20of%20the%20denominator%20of%20the%20InfoNCE%20loss%20into%20two%20terms%3A%20local%0Ahierarchical%20and%20global%20collapse%20regularization%20respectively.%20This%0Adecomposition%20is%20based%20on%20an%20adaptive%20threshold%20that%20gradually%20lowers%20to%0Areflect%20the%20emerging%20hierarchical%20structure%20of%20the%20representation%20space%0Athroughout%20training.%20It%20is%20based%20on%20an%20analysis%20of%20the%20cosine%20similarity%0Adistribution%20of%20samples%20in%20a%20batch.%20We%20demonstrate%20that%20this%20hierarchical%0Aemergence%20exploitation%20%28HEX%29%20approach%20can%20be%20integrated%20across%20a%20wide%20variety%0Aof%20SSL%20algorithms.%20Empirically%2C%20we%20show%20performance%20improvements%20of%20up%20to%205.6%25%0Arelative%20improvement%20over%20baseline%20SSL%20approaches%20on%20classification%20accuracy%20on%0AImagenet%20with%20100%20epochs%20of%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23200v1&entry.124074799=Read"},
{"title": "Exploring Design Choices for Building Language-Specific LLMs", "author": "Atula Tejaswi and Nilesh Gupta and Eunsol Choi", "abstract": "  Despite rapid progress in large language models (LLMs), their performance on\na vast majority of languages remains unsatisfactory. In this paper, we study\nbuilding language-specific LLMs by adapting monolingual and multilingual LLMs.\nWe conduct systematic experiments on how design choices (base model selection,\nvocabulary extension, and continued pretraining) impact the adapted LLM, both\nin terms of efficiency (how many tokens are needed to encode the same amount of\ninformation) and end task performance. We find that (1) the initial performance\nof LLM does not always correlate with the final performance after the\nadaptation. Adapting an English-centric models can yield better results than\nadapting multilingual models despite their worse initial performance on\nlow-resource languages. (2) Efficiency can easily improved with simple\nvocabulary extension and continued pretraining in most LLMs we study, and (3)\nThe optimal adaptation method (choice of the base model, new vocabulary size,\ntraining data, initialization strategy) is highly language-dependent, and the\nsimplest embedding initialization works well across various experimental\nsettings. Together, our work lays foundations on efficiently building\nlanguage-specific LLMs by adapting existing LLMs.\n", "link": "http://arxiv.org/abs/2406.14670v2", "date": "2024-10-30", "relevancy": 2.6116, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5335}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5335}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Design%20Choices%20for%20Building%20Language-Specific%20LLMs&body=Title%3A%20Exploring%20Design%20Choices%20for%20Building%20Language-Specific%20LLMs%0AAuthor%3A%20Atula%20Tejaswi%20and%20Nilesh%20Gupta%20and%20Eunsol%20Choi%0AAbstract%3A%20%20%20Despite%20rapid%20progress%20in%20large%20language%20models%20%28LLMs%29%2C%20their%20performance%20on%0Aa%20vast%20majority%20of%20languages%20remains%20unsatisfactory.%20In%20this%20paper%2C%20we%20study%0Abuilding%20language-specific%20LLMs%20by%20adapting%20monolingual%20and%20multilingual%20LLMs.%0AWe%20conduct%20systematic%20experiments%20on%20how%20design%20choices%20%28base%20model%20selection%2C%0Avocabulary%20extension%2C%20and%20continued%20pretraining%29%20impact%20the%20adapted%20LLM%2C%20both%0Ain%20terms%20of%20efficiency%20%28how%20many%20tokens%20are%20needed%20to%20encode%20the%20same%20amount%20of%0Ainformation%29%20and%20end%20task%20performance.%20We%20find%20that%20%281%29%20the%20initial%20performance%0Aof%20LLM%20does%20not%20always%20correlate%20with%20the%20final%20performance%20after%20the%0Aadaptation.%20Adapting%20an%20English-centric%20models%20can%20yield%20better%20results%20than%0Aadapting%20multilingual%20models%20despite%20their%20worse%20initial%20performance%20on%0Alow-resource%20languages.%20%282%29%20Efficiency%20can%20easily%20improved%20with%20simple%0Avocabulary%20extension%20and%20continued%20pretraining%20in%20most%20LLMs%20we%20study%2C%20and%20%283%29%0AThe%20optimal%20adaptation%20method%20%28choice%20of%20the%20base%20model%2C%20new%20vocabulary%20size%2C%0Atraining%20data%2C%20initialization%20strategy%29%20is%20highly%20language-dependent%2C%20and%20the%0Asimplest%20embedding%20initialization%20works%20well%20across%20various%20experimental%0Asettings.%20Together%2C%20our%20work%20lays%20foundations%20on%20efficiently%20building%0Alanguage-specific%20LLMs%20by%20adapting%20existing%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14670v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Design%2520Choices%2520for%2520Building%2520Language-Specific%2520LLMs%26entry.906535625%3DAtula%2520Tejaswi%2520and%2520Nilesh%2520Gupta%2520and%2520Eunsol%2520Choi%26entry.1292438233%3D%2520%2520Despite%2520rapid%2520progress%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520their%2520performance%2520on%250Aa%2520vast%2520majority%2520of%2520languages%2520remains%2520unsatisfactory.%2520In%2520this%2520paper%252C%2520we%2520study%250Abuilding%2520language-specific%2520LLMs%2520by%2520adapting%2520monolingual%2520and%2520multilingual%2520LLMs.%250AWe%2520conduct%2520systematic%2520experiments%2520on%2520how%2520design%2520choices%2520%2528base%2520model%2520selection%252C%250Avocabulary%2520extension%252C%2520and%2520continued%2520pretraining%2529%2520impact%2520the%2520adapted%2520LLM%252C%2520both%250Ain%2520terms%2520of%2520efficiency%2520%2528how%2520many%2520tokens%2520are%2520needed%2520to%2520encode%2520the%2520same%2520amount%2520of%250Ainformation%2529%2520and%2520end%2520task%2520performance.%2520We%2520find%2520that%2520%25281%2529%2520the%2520initial%2520performance%250Aof%2520LLM%2520does%2520not%2520always%2520correlate%2520with%2520the%2520final%2520performance%2520after%2520the%250Aadaptation.%2520Adapting%2520an%2520English-centric%2520models%2520can%2520yield%2520better%2520results%2520than%250Aadapting%2520multilingual%2520models%2520despite%2520their%2520worse%2520initial%2520performance%2520on%250Alow-resource%2520languages.%2520%25282%2529%2520Efficiency%2520can%2520easily%2520improved%2520with%2520simple%250Avocabulary%2520extension%2520and%2520continued%2520pretraining%2520in%2520most%2520LLMs%2520we%2520study%252C%2520and%2520%25283%2529%250AThe%2520optimal%2520adaptation%2520method%2520%2528choice%2520of%2520the%2520base%2520model%252C%2520new%2520vocabulary%2520size%252C%250Atraining%2520data%252C%2520initialization%2520strategy%2529%2520is%2520highly%2520language-dependent%252C%2520and%2520the%250Asimplest%2520embedding%2520initialization%2520works%2520well%2520across%2520various%2520experimental%250Asettings.%2520Together%252C%2520our%2520work%2520lays%2520foundations%2520on%2520efficiently%2520building%250Alanguage-specific%2520LLMs%2520by%2520adapting%2520existing%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14670v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Design%20Choices%20for%20Building%20Language-Specific%20LLMs&entry.906535625=Atula%20Tejaswi%20and%20Nilesh%20Gupta%20and%20Eunsol%20Choi&entry.1292438233=%20%20Despite%20rapid%20progress%20in%20large%20language%20models%20%28LLMs%29%2C%20their%20performance%20on%0Aa%20vast%20majority%20of%20languages%20remains%20unsatisfactory.%20In%20this%20paper%2C%20we%20study%0Abuilding%20language-specific%20LLMs%20by%20adapting%20monolingual%20and%20multilingual%20LLMs.%0AWe%20conduct%20systematic%20experiments%20on%20how%20design%20choices%20%28base%20model%20selection%2C%0Avocabulary%20extension%2C%20and%20continued%20pretraining%29%20impact%20the%20adapted%20LLM%2C%20both%0Ain%20terms%20of%20efficiency%20%28how%20many%20tokens%20are%20needed%20to%20encode%20the%20same%20amount%20of%0Ainformation%29%20and%20end%20task%20performance.%20We%20find%20that%20%281%29%20the%20initial%20performance%0Aof%20LLM%20does%20not%20always%20correlate%20with%20the%20final%20performance%20after%20the%0Aadaptation.%20Adapting%20an%20English-centric%20models%20can%20yield%20better%20results%20than%0Aadapting%20multilingual%20models%20despite%20their%20worse%20initial%20performance%20on%0Alow-resource%20languages.%20%282%29%20Efficiency%20can%20easily%20improved%20with%20simple%0Avocabulary%20extension%20and%20continued%20pretraining%20in%20most%20LLMs%20we%20study%2C%20and%20%283%29%0AThe%20optimal%20adaptation%20method%20%28choice%20of%20the%20base%20model%2C%20new%20vocabulary%20size%2C%0Atraining%20data%2C%20initialization%20strategy%29%20is%20highly%20language-dependent%2C%20and%20the%0Asimplest%20embedding%20initialization%20works%20well%20across%20various%20experimental%0Asettings.%20Together%2C%20our%20work%20lays%20foundations%20on%20efficiently%20building%0Alanguage-specific%20LLMs%20by%20adapting%20existing%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14670v2&entry.124074799=Read"},
{"title": "FAIR-TAT: Improving Model Fairness Using Targeted Adversarial Training", "author": "Tejaswini Medi and Steffen Jung and Margret Keuper", "abstract": "  Deep neural networks are susceptible to adversarial attacks and common\ncorruptions, which undermine their robustness. In order to enhance model\nresilience against such challenges, Adversarial Training (AT) has emerged as a\nprominent solution. Nevertheless, adversarial robustness is often attained at\nthe expense of model fairness during AT, i.e., disparity in class-wise\nrobustness of the model. While distinctive classes become more robust towards\nsuch adversaries, hard to detect classes suffer. Recently, research has focused\non improving model fairness specifically for perturbed images, overlooking the\naccuracy of the most likely non-perturbed data. Additionally, despite their\nrobustness against the adversaries encountered during model training,\nstate-of-the-art adversarial trained models have difficulty maintaining\nrobustness and fairness when confronted with diverse adversarial threats or\ncommon corruptions. In this work, we address the above concerns by introducing\na novel approach called Fair Targeted Adversarial Training (FAIR-TAT). We show\nthat using targeted adversarial attacks for adversarial training (instead of\nuntargeted attacks) can allow for more favorable trade-offs with respect to\nadversarial fairness. Empirical results validate the efficacy of our approach.\n", "link": "http://arxiv.org/abs/2410.23142v1", "date": "2024-10-30", "relevancy": 2.5864, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5233}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5208}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FAIR-TAT%3A%20Improving%20Model%20Fairness%20Using%20Targeted%20Adversarial%20Training&body=Title%3A%20FAIR-TAT%3A%20Improving%20Model%20Fairness%20Using%20Targeted%20Adversarial%20Training%0AAuthor%3A%20Tejaswini%20Medi%20and%20Steffen%20Jung%20and%20Margret%20Keuper%0AAbstract%3A%20%20%20Deep%20neural%20networks%20are%20susceptible%20to%20adversarial%20attacks%20and%20common%0Acorruptions%2C%20which%20undermine%20their%20robustness.%20In%20order%20to%20enhance%20model%0Aresilience%20against%20such%20challenges%2C%20Adversarial%20Training%20%28AT%29%20has%20emerged%20as%20a%0Aprominent%20solution.%20Nevertheless%2C%20adversarial%20robustness%20is%20often%20attained%20at%0Athe%20expense%20of%20model%20fairness%20during%20AT%2C%20i.e.%2C%20disparity%20in%20class-wise%0Arobustness%20of%20the%20model.%20While%20distinctive%20classes%20become%20more%20robust%20towards%0Asuch%20adversaries%2C%20hard%20to%20detect%20classes%20suffer.%20Recently%2C%20research%20has%20focused%0Aon%20improving%20model%20fairness%20specifically%20for%20perturbed%20images%2C%20overlooking%20the%0Aaccuracy%20of%20the%20most%20likely%20non-perturbed%20data.%20Additionally%2C%20despite%20their%0Arobustness%20against%20the%20adversaries%20encountered%20during%20model%20training%2C%0Astate-of-the-art%20adversarial%20trained%20models%20have%20difficulty%20maintaining%0Arobustness%20and%20fairness%20when%20confronted%20with%20diverse%20adversarial%20threats%20or%0Acommon%20corruptions.%20In%20this%20work%2C%20we%20address%20the%20above%20concerns%20by%20introducing%0Aa%20novel%20approach%20called%20Fair%20Targeted%20Adversarial%20Training%20%28FAIR-TAT%29.%20We%20show%0Athat%20using%20targeted%20adversarial%20attacks%20for%20adversarial%20training%20%28instead%20of%0Auntargeted%20attacks%29%20can%20allow%20for%20more%20favorable%20trade-offs%20with%20respect%20to%0Aadversarial%20fairness.%20Empirical%20results%20validate%20the%20efficacy%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23142v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFAIR-TAT%253A%2520Improving%2520Model%2520Fairness%2520Using%2520Targeted%2520Adversarial%2520Training%26entry.906535625%3DTejaswini%2520Medi%2520and%2520Steffen%2520Jung%2520and%2520Margret%2520Keuper%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520are%2520susceptible%2520to%2520adversarial%2520attacks%2520and%2520common%250Acorruptions%252C%2520which%2520undermine%2520their%2520robustness.%2520In%2520order%2520to%2520enhance%2520model%250Aresilience%2520against%2520such%2520challenges%252C%2520Adversarial%2520Training%2520%2528AT%2529%2520has%2520emerged%2520as%2520a%250Aprominent%2520solution.%2520Nevertheless%252C%2520adversarial%2520robustness%2520is%2520often%2520attained%2520at%250Athe%2520expense%2520of%2520model%2520fairness%2520during%2520AT%252C%2520i.e.%252C%2520disparity%2520in%2520class-wise%250Arobustness%2520of%2520the%2520model.%2520While%2520distinctive%2520classes%2520become%2520more%2520robust%2520towards%250Asuch%2520adversaries%252C%2520hard%2520to%2520detect%2520classes%2520suffer.%2520Recently%252C%2520research%2520has%2520focused%250Aon%2520improving%2520model%2520fairness%2520specifically%2520for%2520perturbed%2520images%252C%2520overlooking%2520the%250Aaccuracy%2520of%2520the%2520most%2520likely%2520non-perturbed%2520data.%2520Additionally%252C%2520despite%2520their%250Arobustness%2520against%2520the%2520adversaries%2520encountered%2520during%2520model%2520training%252C%250Astate-of-the-art%2520adversarial%2520trained%2520models%2520have%2520difficulty%2520maintaining%250Arobustness%2520and%2520fairness%2520when%2520confronted%2520with%2520diverse%2520adversarial%2520threats%2520or%250Acommon%2520corruptions.%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520above%2520concerns%2520by%2520introducing%250Aa%2520novel%2520approach%2520called%2520Fair%2520Targeted%2520Adversarial%2520Training%2520%2528FAIR-TAT%2529.%2520We%2520show%250Athat%2520using%2520targeted%2520adversarial%2520attacks%2520for%2520adversarial%2520training%2520%2528instead%2520of%250Auntargeted%2520attacks%2529%2520can%2520allow%2520for%2520more%2520favorable%2520trade-offs%2520with%2520respect%2520to%250Aadversarial%2520fairness.%2520Empirical%2520results%2520validate%2520the%2520efficacy%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23142v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FAIR-TAT%3A%20Improving%20Model%20Fairness%20Using%20Targeted%20Adversarial%20Training&entry.906535625=Tejaswini%20Medi%20and%20Steffen%20Jung%20and%20Margret%20Keuper&entry.1292438233=%20%20Deep%20neural%20networks%20are%20susceptible%20to%20adversarial%20attacks%20and%20common%0Acorruptions%2C%20which%20undermine%20their%20robustness.%20In%20order%20to%20enhance%20model%0Aresilience%20against%20such%20challenges%2C%20Adversarial%20Training%20%28AT%29%20has%20emerged%20as%20a%0Aprominent%20solution.%20Nevertheless%2C%20adversarial%20robustness%20is%20often%20attained%20at%0Athe%20expense%20of%20model%20fairness%20during%20AT%2C%20i.e.%2C%20disparity%20in%20class-wise%0Arobustness%20of%20the%20model.%20While%20distinctive%20classes%20become%20more%20robust%20towards%0Asuch%20adversaries%2C%20hard%20to%20detect%20classes%20suffer.%20Recently%2C%20research%20has%20focused%0Aon%20improving%20model%20fairness%20specifically%20for%20perturbed%20images%2C%20overlooking%20the%0Aaccuracy%20of%20the%20most%20likely%20non-perturbed%20data.%20Additionally%2C%20despite%20their%0Arobustness%20against%20the%20adversaries%20encountered%20during%20model%20training%2C%0Astate-of-the-art%20adversarial%20trained%20models%20have%20difficulty%20maintaining%0Arobustness%20and%20fairness%20when%20confronted%20with%20diverse%20adversarial%20threats%20or%0Acommon%20corruptions.%20In%20this%20work%2C%20we%20address%20the%20above%20concerns%20by%20introducing%0Aa%20novel%20approach%20called%20Fair%20Targeted%20Adversarial%20Training%20%28FAIR-TAT%29.%20We%20show%0Athat%20using%20targeted%20adversarial%20attacks%20for%20adversarial%20training%20%28instead%20of%0Auntargeted%20attacks%29%20can%20allow%20for%20more%20favorable%20trade-offs%20with%20respect%20to%0Aadversarial%20fairness.%20Empirical%20results%20validate%20the%20efficacy%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23142v1&entry.124074799=Read"},
{"title": "Continuous Spatio-Temporal Memory Networks for 4D Cardiac Cine MRI\n  Segmentation", "author": "Meng Ye and Bingyu Xin and Leon Axel and Dimitris Metaxas", "abstract": "  Current cardiac cine magnetic resonance image (cMR) studies focus on the end\ndiastole (ED) and end systole (ES) phases, while ignoring the abundant temporal\ninformation in the whole image sequence. This is because whole sequence\nsegmentation is currently a tedious process and inaccurate. Conventional whole\nsequence segmentation approaches first estimate the motion field between\nframes, which is then used to propagate the mask along the temporal axis.\nHowever, the mask propagation results could be prone to error, especially for\nthe basal and apex slices, where through-plane motion leads to significant\nmorphology and structural change during the cardiac cycle. Inspired by recent\nadvances in video object segmentation (VOS), based on spatio-temporal memory\n(STM) networks, we propose a continuous STM (CSTM) network for semi-supervised\nwhole heart and whole sequence cMR segmentation. Our CSTM network takes full\nadvantage of the spatial, scale, temporal and through-plane continuity prior of\nthe underlying heart anatomy structures, to achieve accurate and fast 4D\nsegmentation. Results of extensive experiments across multiple cMR datasets\nshow that our method can improve the 4D cMR segmentation performance,\nespecially for the hard-to-segment regions.\n", "link": "http://arxiv.org/abs/2410.23191v1", "date": "2024-10-30", "relevancy": 2.5725, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5175}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.513}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20Spatio-Temporal%20Memory%20Networks%20for%204D%20Cardiac%20Cine%20MRI%0A%20%20Segmentation&body=Title%3A%20Continuous%20Spatio-Temporal%20Memory%20Networks%20for%204D%20Cardiac%20Cine%20MRI%0A%20%20Segmentation%0AAuthor%3A%20Meng%20Ye%20and%20Bingyu%20Xin%20and%20Leon%20Axel%20and%20Dimitris%20Metaxas%0AAbstract%3A%20%20%20Current%20cardiac%20cine%20magnetic%20resonance%20image%20%28cMR%29%20studies%20focus%20on%20the%20end%0Adiastole%20%28ED%29%20and%20end%20systole%20%28ES%29%20phases%2C%20while%20ignoring%20the%20abundant%20temporal%0Ainformation%20in%20the%20whole%20image%20sequence.%20This%20is%20because%20whole%20sequence%0Asegmentation%20is%20currently%20a%20tedious%20process%20and%20inaccurate.%20Conventional%20whole%0Asequence%20segmentation%20approaches%20first%20estimate%20the%20motion%20field%20between%0Aframes%2C%20which%20is%20then%20used%20to%20propagate%20the%20mask%20along%20the%20temporal%20axis.%0AHowever%2C%20the%20mask%20propagation%20results%20could%20be%20prone%20to%20error%2C%20especially%20for%0Athe%20basal%20and%20apex%20slices%2C%20where%20through-plane%20motion%20leads%20to%20significant%0Amorphology%20and%20structural%20change%20during%20the%20cardiac%20cycle.%20Inspired%20by%20recent%0Aadvances%20in%20video%20object%20segmentation%20%28VOS%29%2C%20based%20on%20spatio-temporal%20memory%0A%28STM%29%20networks%2C%20we%20propose%20a%20continuous%20STM%20%28CSTM%29%20network%20for%20semi-supervised%0Awhole%20heart%20and%20whole%20sequence%20cMR%20segmentation.%20Our%20CSTM%20network%20takes%20full%0Aadvantage%20of%20the%20spatial%2C%20scale%2C%20temporal%20and%20through-plane%20continuity%20prior%20of%0Athe%20underlying%20heart%20anatomy%20structures%2C%20to%20achieve%20accurate%20and%20fast%204D%0Asegmentation.%20Results%20of%20extensive%20experiments%20across%20multiple%20cMR%20datasets%0Ashow%20that%20our%20method%20can%20improve%20the%204D%20cMR%20segmentation%20performance%2C%0Aespecially%20for%20the%20hard-to-segment%20regions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520Spatio-Temporal%2520Memory%2520Networks%2520for%25204D%2520Cardiac%2520Cine%2520MRI%250A%2520%2520Segmentation%26entry.906535625%3DMeng%2520Ye%2520and%2520Bingyu%2520Xin%2520and%2520Leon%2520Axel%2520and%2520Dimitris%2520Metaxas%26entry.1292438233%3D%2520%2520Current%2520cardiac%2520cine%2520magnetic%2520resonance%2520image%2520%2528cMR%2529%2520studies%2520focus%2520on%2520the%2520end%250Adiastole%2520%2528ED%2529%2520and%2520end%2520systole%2520%2528ES%2529%2520phases%252C%2520while%2520ignoring%2520the%2520abundant%2520temporal%250Ainformation%2520in%2520the%2520whole%2520image%2520sequence.%2520This%2520is%2520because%2520whole%2520sequence%250Asegmentation%2520is%2520currently%2520a%2520tedious%2520process%2520and%2520inaccurate.%2520Conventional%2520whole%250Asequence%2520segmentation%2520approaches%2520first%2520estimate%2520the%2520motion%2520field%2520between%250Aframes%252C%2520which%2520is%2520then%2520used%2520to%2520propagate%2520the%2520mask%2520along%2520the%2520temporal%2520axis.%250AHowever%252C%2520the%2520mask%2520propagation%2520results%2520could%2520be%2520prone%2520to%2520error%252C%2520especially%2520for%250Athe%2520basal%2520and%2520apex%2520slices%252C%2520where%2520through-plane%2520motion%2520leads%2520to%2520significant%250Amorphology%2520and%2520structural%2520change%2520during%2520the%2520cardiac%2520cycle.%2520Inspired%2520by%2520recent%250Aadvances%2520in%2520video%2520object%2520segmentation%2520%2528VOS%2529%252C%2520based%2520on%2520spatio-temporal%2520memory%250A%2528STM%2529%2520networks%252C%2520we%2520propose%2520a%2520continuous%2520STM%2520%2528CSTM%2529%2520network%2520for%2520semi-supervised%250Awhole%2520heart%2520and%2520whole%2520sequence%2520cMR%2520segmentation.%2520Our%2520CSTM%2520network%2520takes%2520full%250Aadvantage%2520of%2520the%2520spatial%252C%2520scale%252C%2520temporal%2520and%2520through-plane%2520continuity%2520prior%2520of%250Athe%2520underlying%2520heart%2520anatomy%2520structures%252C%2520to%2520achieve%2520accurate%2520and%2520fast%25204D%250Asegmentation.%2520Results%2520of%2520extensive%2520experiments%2520across%2520multiple%2520cMR%2520datasets%250Ashow%2520that%2520our%2520method%2520can%2520improve%2520the%25204D%2520cMR%2520segmentation%2520performance%252C%250Aespecially%2520for%2520the%2520hard-to-segment%2520regions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Spatio-Temporal%20Memory%20Networks%20for%204D%20Cardiac%20Cine%20MRI%0A%20%20Segmentation&entry.906535625=Meng%20Ye%20and%20Bingyu%20Xin%20and%20Leon%20Axel%20and%20Dimitris%20Metaxas&entry.1292438233=%20%20Current%20cardiac%20cine%20magnetic%20resonance%20image%20%28cMR%29%20studies%20focus%20on%20the%20end%0Adiastole%20%28ED%29%20and%20end%20systole%20%28ES%29%20phases%2C%20while%20ignoring%20the%20abundant%20temporal%0Ainformation%20in%20the%20whole%20image%20sequence.%20This%20is%20because%20whole%20sequence%0Asegmentation%20is%20currently%20a%20tedious%20process%20and%20inaccurate.%20Conventional%20whole%0Asequence%20segmentation%20approaches%20first%20estimate%20the%20motion%20field%20between%0Aframes%2C%20which%20is%20then%20used%20to%20propagate%20the%20mask%20along%20the%20temporal%20axis.%0AHowever%2C%20the%20mask%20propagation%20results%20could%20be%20prone%20to%20error%2C%20especially%20for%0Athe%20basal%20and%20apex%20slices%2C%20where%20through-plane%20motion%20leads%20to%20significant%0Amorphology%20and%20structural%20change%20during%20the%20cardiac%20cycle.%20Inspired%20by%20recent%0Aadvances%20in%20video%20object%20segmentation%20%28VOS%29%2C%20based%20on%20spatio-temporal%20memory%0A%28STM%29%20networks%2C%20we%20propose%20a%20continuous%20STM%20%28CSTM%29%20network%20for%20semi-supervised%0Awhole%20heart%20and%20whole%20sequence%20cMR%20segmentation.%20Our%20CSTM%20network%20takes%20full%0Aadvantage%20of%20the%20spatial%2C%20scale%2C%20temporal%20and%20through-plane%20continuity%20prior%20of%0Athe%20underlying%20heart%20anatomy%20structures%2C%20to%20achieve%20accurate%20and%20fast%204D%0Asegmentation.%20Results%20of%20extensive%20experiments%20across%20multiple%20cMR%20datasets%0Ashow%20that%20our%20method%20can%20improve%20the%204D%20cMR%20segmentation%20performance%2C%0Aespecially%20for%20the%20hard-to-segment%20regions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23191v1&entry.124074799=Read"},
{"title": "Grounding by Trying: LLMs with Reinforcement Learning-Enhanced Retrieval", "author": "Sheryl Hsu and Omar Khattab and Chelsea Finn and Archit Sharma", "abstract": "  The hallucinations of large language models (LLMs) are increasingly mitigated\nby allowing LLMs to search for information and to ground their answers in real\nsources. Unfortunately, LLMs often struggle with posing the right search\nqueries, especially when dealing with complex or otherwise indirect topics.\nObserving that LLMs can learn to search for relevant facts by $\\textit{trying}$\ndifferent queries and learning to up-weight queries that successfully produce\nrelevant results, we introduce $\\underline{Le}$arning to $\\underline{Re}$trieve\nby $\\underline{T}$rying (LeReT), a reinforcement learning framework that\nexplores search queries and uses preference-based optimization to improve their\nquality. \\methodclass can improve the absolute retrieval accuracy by up to 29\\%\nand the downstream generator evaluations by 17\\%. The simplicity and\nflexibility of LeReT allows it to be applied to arbitrary off-the-shelf\nretrievers and makes it a promising technique for improving general LLM\npipelines. Project website: http://sherylhsu.com/LeReT/.\n", "link": "http://arxiv.org/abs/2410.23214v1", "date": "2024-10-30", "relevancy": 2.4834, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.501}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.501}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounding%20by%20Trying%3A%20LLMs%20with%20Reinforcement%20Learning-Enhanced%20Retrieval&body=Title%3A%20Grounding%20by%20Trying%3A%20LLMs%20with%20Reinforcement%20Learning-Enhanced%20Retrieval%0AAuthor%3A%20Sheryl%20Hsu%20and%20Omar%20Khattab%20and%20Chelsea%20Finn%20and%20Archit%20Sharma%0AAbstract%3A%20%20%20The%20hallucinations%20of%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20mitigated%0Aby%20allowing%20LLMs%20to%20search%20for%20information%20and%20to%20ground%20their%20answers%20in%20real%0Asources.%20Unfortunately%2C%20LLMs%20often%20struggle%20with%20posing%20the%20right%20search%0Aqueries%2C%20especially%20when%20dealing%20with%20complex%20or%20otherwise%20indirect%20topics.%0AObserving%20that%20LLMs%20can%20learn%20to%20search%20for%20relevant%20facts%20by%20%24%5Ctextit%7Btrying%7D%24%0Adifferent%20queries%20and%20learning%20to%20up-weight%20queries%20that%20successfully%20produce%0Arelevant%20results%2C%20we%20introduce%20%24%5Cunderline%7BLe%7D%24arning%20to%20%24%5Cunderline%7BRe%7D%24trieve%0Aby%20%24%5Cunderline%7BT%7D%24rying%20%28LeReT%29%2C%20a%20reinforcement%20learning%20framework%20that%0Aexplores%20search%20queries%20and%20uses%20preference-based%20optimization%20to%20improve%20their%0Aquality.%20%5Cmethodclass%20can%20improve%20the%20absolute%20retrieval%20accuracy%20by%20up%20to%2029%5C%25%0Aand%20the%20downstream%20generator%20evaluations%20by%2017%5C%25.%20The%20simplicity%20and%0Aflexibility%20of%20LeReT%20allows%20it%20to%20be%20applied%20to%20arbitrary%20off-the-shelf%0Aretrievers%20and%20makes%20it%20a%20promising%20technique%20for%20improving%20general%20LLM%0Apipelines.%20Project%20website%3A%20http%3A//sherylhsu.com/LeReT/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounding%2520by%2520Trying%253A%2520LLMs%2520with%2520Reinforcement%2520Learning-Enhanced%2520Retrieval%26entry.906535625%3DSheryl%2520Hsu%2520and%2520Omar%2520Khattab%2520and%2520Chelsea%2520Finn%2520and%2520Archit%2520Sharma%26entry.1292438233%3D%2520%2520The%2520hallucinations%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520mitigated%250Aby%2520allowing%2520LLMs%2520to%2520search%2520for%2520information%2520and%2520to%2520ground%2520their%2520answers%2520in%2520real%250Asources.%2520Unfortunately%252C%2520LLMs%2520often%2520struggle%2520with%2520posing%2520the%2520right%2520search%250Aqueries%252C%2520especially%2520when%2520dealing%2520with%2520complex%2520or%2520otherwise%2520indirect%2520topics.%250AObserving%2520that%2520LLMs%2520can%2520learn%2520to%2520search%2520for%2520relevant%2520facts%2520by%2520%2524%255Ctextit%257Btrying%257D%2524%250Adifferent%2520queries%2520and%2520learning%2520to%2520up-weight%2520queries%2520that%2520successfully%2520produce%250Arelevant%2520results%252C%2520we%2520introduce%2520%2524%255Cunderline%257BLe%257D%2524arning%2520to%2520%2524%255Cunderline%257BRe%257D%2524trieve%250Aby%2520%2524%255Cunderline%257BT%257D%2524rying%2520%2528LeReT%2529%252C%2520a%2520reinforcement%2520learning%2520framework%2520that%250Aexplores%2520search%2520queries%2520and%2520uses%2520preference-based%2520optimization%2520to%2520improve%2520their%250Aquality.%2520%255Cmethodclass%2520can%2520improve%2520the%2520absolute%2520retrieval%2520accuracy%2520by%2520up%2520to%252029%255C%2525%250Aand%2520the%2520downstream%2520generator%2520evaluations%2520by%252017%255C%2525.%2520The%2520simplicity%2520and%250Aflexibility%2520of%2520LeReT%2520allows%2520it%2520to%2520be%2520applied%2520to%2520arbitrary%2520off-the-shelf%250Aretrievers%2520and%2520makes%2520it%2520a%2520promising%2520technique%2520for%2520improving%2520general%2520LLM%250Apipelines.%2520Project%2520website%253A%2520http%253A//sherylhsu.com/LeReT/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounding%20by%20Trying%3A%20LLMs%20with%20Reinforcement%20Learning-Enhanced%20Retrieval&entry.906535625=Sheryl%20Hsu%20and%20Omar%20Khattab%20and%20Chelsea%20Finn%20and%20Archit%20Sharma&entry.1292438233=%20%20The%20hallucinations%20of%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20mitigated%0Aby%20allowing%20LLMs%20to%20search%20for%20information%20and%20to%20ground%20their%20answers%20in%20real%0Asources.%20Unfortunately%2C%20LLMs%20often%20struggle%20with%20posing%20the%20right%20search%0Aqueries%2C%20especially%20when%20dealing%20with%20complex%20or%20otherwise%20indirect%20topics.%0AObserving%20that%20LLMs%20can%20learn%20to%20search%20for%20relevant%20facts%20by%20%24%5Ctextit%7Btrying%7D%24%0Adifferent%20queries%20and%20learning%20to%20up-weight%20queries%20that%20successfully%20produce%0Arelevant%20results%2C%20we%20introduce%20%24%5Cunderline%7BLe%7D%24arning%20to%20%24%5Cunderline%7BRe%7D%24trieve%0Aby%20%24%5Cunderline%7BT%7D%24rying%20%28LeReT%29%2C%20a%20reinforcement%20learning%20framework%20that%0Aexplores%20search%20queries%20and%20uses%20preference-based%20optimization%20to%20improve%20their%0Aquality.%20%5Cmethodclass%20can%20improve%20the%20absolute%20retrieval%20accuracy%20by%20up%20to%2029%5C%25%0Aand%20the%20downstream%20generator%20evaluations%20by%2017%5C%25.%20The%20simplicity%20and%0Aflexibility%20of%20LeReT%20allows%20it%20to%20be%20applied%20to%20arbitrary%20off-the-shelf%0Aretrievers%20and%20makes%20it%20a%20promising%20technique%20for%20improving%20general%20LLM%0Apipelines.%20Project%20website%3A%20http%3A//sherylhsu.com/LeReT/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23214v1&entry.124074799=Read"},
{"title": "Guiding Through Complexity: What Makes Good Supervision for Hard\n  Reasoning Tasks?", "author": "Xuan He and Da Yin and Nanyun Peng", "abstract": "  How can \"weak teacher models\" such as average human annotators or existing AI\nsystems, effectively supervise LLMs to improve performance on hard reasoning\ntasks, especially those that challenge and requires expertise or daily practice\nfrom the teacher models? In this paper, we seek for empirical answers to this\nquestion by investigating various data-driven strategies that offer supervision\ndata at different quality levels upon tasks of varying complexity. Two\nintuitive strategies emerge for teacher models to provide supervision during\nalignment training: 1) using lower-quality supervision from complete tasks that\nmatch the difficulty of the target reasoning tasks, and 2) leveraging\nhigher-quality supervision from easier subtasks that are less challenging.\nInterestingly, we find that even when the outcome error rate for hard task\nsupervision is high (e.g., 90\\%), training on such data can outperform\nperfectly correct supervision on easier subtasks on multiple hard math\nbenchmarks. We further identify a more critical factor influencing training\nperformance: step-wise error rates, which indicate the severity of errors in\nsolutions. Specifically, training on hard task supervision with the same\noutcome error rates but disparate step-wise error rates can lead to a 30\\%\naccuracy gap on MATH benchmark. Our results also reveal that supplementing hard\ntask supervision with the corresponding subtask supervision can yield notable\nperformance improvements than simply combining rephrased hard full task\nsupervision, suggesting new avenues for data augmentation. Data and code are\nreleased at \\url{https://github.com/hexuan21/Weak-to-Strong}.\n", "link": "http://arxiv.org/abs/2410.20533v2", "date": "2024-10-30", "relevancy": 2.4792, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.498}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guiding%20Through%20Complexity%3A%20What%20Makes%20Good%20Supervision%20for%20Hard%0A%20%20Reasoning%20Tasks%3F&body=Title%3A%20Guiding%20Through%20Complexity%3A%20What%20Makes%20Good%20Supervision%20for%20Hard%0A%20%20Reasoning%20Tasks%3F%0AAuthor%3A%20Xuan%20He%20and%20Da%20Yin%20and%20Nanyun%20Peng%0AAbstract%3A%20%20%20How%20can%20%22weak%20teacher%20models%22%20such%20as%20average%20human%20annotators%20or%20existing%20AI%0Asystems%2C%20effectively%20supervise%20LLMs%20to%20improve%20performance%20on%20hard%20reasoning%0Atasks%2C%20especially%20those%20that%20challenge%20and%20requires%20expertise%20or%20daily%20practice%0Afrom%20the%20teacher%20models%3F%20In%20this%20paper%2C%20we%20seek%20for%20empirical%20answers%20to%20this%0Aquestion%20by%20investigating%20various%20data-driven%20strategies%20that%20offer%20supervision%0Adata%20at%20different%20quality%20levels%20upon%20tasks%20of%20varying%20complexity.%20Two%0Aintuitive%20strategies%20emerge%20for%20teacher%20models%20to%20provide%20supervision%20during%0Aalignment%20training%3A%201%29%20using%20lower-quality%20supervision%20from%20complete%20tasks%20that%0Amatch%20the%20difficulty%20of%20the%20target%20reasoning%20tasks%2C%20and%202%29%20leveraging%0Ahigher-quality%20supervision%20from%20easier%20subtasks%20that%20are%20less%20challenging.%0AInterestingly%2C%20we%20find%20that%20even%20when%20the%20outcome%20error%20rate%20for%20hard%20task%0Asupervision%20is%20high%20%28e.g.%2C%2090%5C%25%29%2C%20training%20on%20such%20data%20can%20outperform%0Aperfectly%20correct%20supervision%20on%20easier%20subtasks%20on%20multiple%20hard%20math%0Abenchmarks.%20We%20further%20identify%20a%20more%20critical%20factor%20influencing%20training%0Aperformance%3A%20step-wise%20error%20rates%2C%20which%20indicate%20the%20severity%20of%20errors%20in%0Asolutions.%20Specifically%2C%20training%20on%20hard%20task%20supervision%20with%20the%20same%0Aoutcome%20error%20rates%20but%20disparate%20step-wise%20error%20rates%20can%20lead%20to%20a%2030%5C%25%0Aaccuracy%20gap%20on%20MATH%20benchmark.%20Our%20results%20also%20reveal%20that%20supplementing%20hard%0Atask%20supervision%20with%20the%20corresponding%20subtask%20supervision%20can%20yield%20notable%0Aperformance%20improvements%20than%20simply%20combining%20rephrased%20hard%20full%20task%0Asupervision%2C%20suggesting%20new%20avenues%20for%20data%20augmentation.%20Data%20and%20code%20are%0Areleased%20at%20%5Curl%7Bhttps%3A//github.com/hexuan21/Weak-to-Strong%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20533v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuiding%2520Through%2520Complexity%253A%2520What%2520Makes%2520Good%2520Supervision%2520for%2520Hard%250A%2520%2520Reasoning%2520Tasks%253F%26entry.906535625%3DXuan%2520He%2520and%2520Da%2520Yin%2520and%2520Nanyun%2520Peng%26entry.1292438233%3D%2520%2520How%2520can%2520%2522weak%2520teacher%2520models%2522%2520such%2520as%2520average%2520human%2520annotators%2520or%2520existing%2520AI%250Asystems%252C%2520effectively%2520supervise%2520LLMs%2520to%2520improve%2520performance%2520on%2520hard%2520reasoning%250Atasks%252C%2520especially%2520those%2520that%2520challenge%2520and%2520requires%2520expertise%2520or%2520daily%2520practice%250Afrom%2520the%2520teacher%2520models%253F%2520In%2520this%2520paper%252C%2520we%2520seek%2520for%2520empirical%2520answers%2520to%2520this%250Aquestion%2520by%2520investigating%2520various%2520data-driven%2520strategies%2520that%2520offer%2520supervision%250Adata%2520at%2520different%2520quality%2520levels%2520upon%2520tasks%2520of%2520varying%2520complexity.%2520Two%250Aintuitive%2520strategies%2520emerge%2520for%2520teacher%2520models%2520to%2520provide%2520supervision%2520during%250Aalignment%2520training%253A%25201%2529%2520using%2520lower-quality%2520supervision%2520from%2520complete%2520tasks%2520that%250Amatch%2520the%2520difficulty%2520of%2520the%2520target%2520reasoning%2520tasks%252C%2520and%25202%2529%2520leveraging%250Ahigher-quality%2520supervision%2520from%2520easier%2520subtasks%2520that%2520are%2520less%2520challenging.%250AInterestingly%252C%2520we%2520find%2520that%2520even%2520when%2520the%2520outcome%2520error%2520rate%2520for%2520hard%2520task%250Asupervision%2520is%2520high%2520%2528e.g.%252C%252090%255C%2525%2529%252C%2520training%2520on%2520such%2520data%2520can%2520outperform%250Aperfectly%2520correct%2520supervision%2520on%2520easier%2520subtasks%2520on%2520multiple%2520hard%2520math%250Abenchmarks.%2520We%2520further%2520identify%2520a%2520more%2520critical%2520factor%2520influencing%2520training%250Aperformance%253A%2520step-wise%2520error%2520rates%252C%2520which%2520indicate%2520the%2520severity%2520of%2520errors%2520in%250Asolutions.%2520Specifically%252C%2520training%2520on%2520hard%2520task%2520supervision%2520with%2520the%2520same%250Aoutcome%2520error%2520rates%2520but%2520disparate%2520step-wise%2520error%2520rates%2520can%2520lead%2520to%2520a%252030%255C%2525%250Aaccuracy%2520gap%2520on%2520MATH%2520benchmark.%2520Our%2520results%2520also%2520reveal%2520that%2520supplementing%2520hard%250Atask%2520supervision%2520with%2520the%2520corresponding%2520subtask%2520supervision%2520can%2520yield%2520notable%250Aperformance%2520improvements%2520than%2520simply%2520combining%2520rephrased%2520hard%2520full%2520task%250Asupervision%252C%2520suggesting%2520new%2520avenues%2520for%2520data%2520augmentation.%2520Data%2520and%2520code%2520are%250Areleased%2520at%2520%255Curl%257Bhttps%253A//github.com/hexuan21/Weak-to-Strong%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20533v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guiding%20Through%20Complexity%3A%20What%20Makes%20Good%20Supervision%20for%20Hard%0A%20%20Reasoning%20Tasks%3F&entry.906535625=Xuan%20He%20and%20Da%20Yin%20and%20Nanyun%20Peng&entry.1292438233=%20%20How%20can%20%22weak%20teacher%20models%22%20such%20as%20average%20human%20annotators%20or%20existing%20AI%0Asystems%2C%20effectively%20supervise%20LLMs%20to%20improve%20performance%20on%20hard%20reasoning%0Atasks%2C%20especially%20those%20that%20challenge%20and%20requires%20expertise%20or%20daily%20practice%0Afrom%20the%20teacher%20models%3F%20In%20this%20paper%2C%20we%20seek%20for%20empirical%20answers%20to%20this%0Aquestion%20by%20investigating%20various%20data-driven%20strategies%20that%20offer%20supervision%0Adata%20at%20different%20quality%20levels%20upon%20tasks%20of%20varying%20complexity.%20Two%0Aintuitive%20strategies%20emerge%20for%20teacher%20models%20to%20provide%20supervision%20during%0Aalignment%20training%3A%201%29%20using%20lower-quality%20supervision%20from%20complete%20tasks%20that%0Amatch%20the%20difficulty%20of%20the%20target%20reasoning%20tasks%2C%20and%202%29%20leveraging%0Ahigher-quality%20supervision%20from%20easier%20subtasks%20that%20are%20less%20challenging.%0AInterestingly%2C%20we%20find%20that%20even%20when%20the%20outcome%20error%20rate%20for%20hard%20task%0Asupervision%20is%20high%20%28e.g.%2C%2090%5C%25%29%2C%20training%20on%20such%20data%20can%20outperform%0Aperfectly%20correct%20supervision%20on%20easier%20subtasks%20on%20multiple%20hard%20math%0Abenchmarks.%20We%20further%20identify%20a%20more%20critical%20factor%20influencing%20training%0Aperformance%3A%20step-wise%20error%20rates%2C%20which%20indicate%20the%20severity%20of%20errors%20in%0Asolutions.%20Specifically%2C%20training%20on%20hard%20task%20supervision%20with%20the%20same%0Aoutcome%20error%20rates%20but%20disparate%20step-wise%20error%20rates%20can%20lead%20to%20a%2030%5C%25%0Aaccuracy%20gap%20on%20MATH%20benchmark.%20Our%20results%20also%20reveal%20that%20supplementing%20hard%0Atask%20supervision%20with%20the%20corresponding%20subtask%20supervision%20can%20yield%20notable%0Aperformance%20improvements%20than%20simply%20combining%20rephrased%20hard%20full%20task%0Asupervision%2C%20suggesting%20new%20avenues%20for%20data%20augmentation.%20Data%20and%20code%20are%0Areleased%20at%20%5Curl%7Bhttps%3A//github.com/hexuan21/Weak-to-Strong%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20533v2&entry.124074799=Read"},
{"title": "SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video\n  Generation", "author": "Yining Hong and Beide Liu and Maxine Wu and Yuanhao Zhai and Kai-Wei Chang and Lingjie Li and Kevin Lin and Chung-Ching Lin and Jianfeng Wang and Zhengyuan Yang and Yingnian Wu and Lijuan Wang", "abstract": "  Human beings are endowed with a complementary learning system, which bridges\nthe slow learning of general world dynamics with fast storage of episodic\nmemory from a new experience. Previous video generation models, however,\nprimarily focus on slow learning by pre-training on vast amounts of data,\noverlooking the fast learning phase crucial for episodic memory storage. This\noversight leads to inconsistencies across temporally distant frames when\ngenerating longer videos, as these frames fall beyond the model's context\nwindow. To this end, we introduce SlowFast-VGen, a novel dual-speed learning\nsystem for action-driven long video generation. Our approach incorporates a\nmasked conditional video diffusion model for the slow learning of world\ndynamics, alongside an inference-time fast learning strategy based on a\ntemporal LoRA module. Specifically, the fast learning process updates its\ntemporal LoRA parameters based on local inputs and outputs, thereby efficiently\nstoring episodic memory in its parameters. We further propose a slow-fast\nlearning loop algorithm that seamlessly integrates the inner fast learning loop\ninto the outer slow learning loop, enabling the recall of prior multi-episode\nexperiences for context-aware skill learning. To facilitate the slow learning\nof an approximate world model, we collect a large-scale dataset of 200k videos\nwith language action annotations, covering a wide range of scenarios. Extensive\nexperiments show that SlowFast-VGen outperforms baselines across various\nmetrics for action-driven video generation, achieving an FVD score of 514\ncompared to 782, and maintaining consistency in longer videos, with an average\nof 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm\nsignificantly enhances performances on long-horizon planning tasks as well.\nProject Website: https://slowfast-vgen.github.io\n", "link": "http://arxiv.org/abs/2410.23277v1", "date": "2024-10-30", "relevancy": 2.4733, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6259}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6138}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SlowFast-VGen%3A%20Slow-Fast%20Learning%20for%20Action-Driven%20Long%20Video%0A%20%20Generation&body=Title%3A%20SlowFast-VGen%3A%20Slow-Fast%20Learning%20for%20Action-Driven%20Long%20Video%0A%20%20Generation%0AAuthor%3A%20Yining%20Hong%20and%20Beide%20Liu%20and%20Maxine%20Wu%20and%20Yuanhao%20Zhai%20and%20Kai-Wei%20Chang%20and%20Lingjie%20Li%20and%20Kevin%20Lin%20and%20Chung-Ching%20Lin%20and%20Jianfeng%20Wang%20and%20Zhengyuan%20Yang%20and%20Yingnian%20Wu%20and%20Lijuan%20Wang%0AAbstract%3A%20%20%20Human%20beings%20are%20endowed%20with%20a%20complementary%20learning%20system%2C%20which%20bridges%0Athe%20slow%20learning%20of%20general%20world%20dynamics%20with%20fast%20storage%20of%20episodic%0Amemory%20from%20a%20new%20experience.%20Previous%20video%20generation%20models%2C%20however%2C%0Aprimarily%20focus%20on%20slow%20learning%20by%20pre-training%20on%20vast%20amounts%20of%20data%2C%0Aoverlooking%20the%20fast%20learning%20phase%20crucial%20for%20episodic%20memory%20storage.%20This%0Aoversight%20leads%20to%20inconsistencies%20across%20temporally%20distant%20frames%20when%0Agenerating%20longer%20videos%2C%20as%20these%20frames%20fall%20beyond%20the%20model%27s%20context%0Awindow.%20To%20this%20end%2C%20we%20introduce%20SlowFast-VGen%2C%20a%20novel%20dual-speed%20learning%0Asystem%20for%20action-driven%20long%20video%20generation.%20Our%20approach%20incorporates%20a%0Amasked%20conditional%20video%20diffusion%20model%20for%20the%20slow%20learning%20of%20world%0Adynamics%2C%20alongside%20an%20inference-time%20fast%20learning%20strategy%20based%20on%20a%0Atemporal%20LoRA%20module.%20Specifically%2C%20the%20fast%20learning%20process%20updates%20its%0Atemporal%20LoRA%20parameters%20based%20on%20local%20inputs%20and%20outputs%2C%20thereby%20efficiently%0Astoring%20episodic%20memory%20in%20its%20parameters.%20We%20further%20propose%20a%20slow-fast%0Alearning%20loop%20algorithm%20that%20seamlessly%20integrates%20the%20inner%20fast%20learning%20loop%0Ainto%20the%20outer%20slow%20learning%20loop%2C%20enabling%20the%20recall%20of%20prior%20multi-episode%0Aexperiences%20for%20context-aware%20skill%20learning.%20To%20facilitate%20the%20slow%20learning%0Aof%20an%20approximate%20world%20model%2C%20we%20collect%20a%20large-scale%20dataset%20of%20200k%20videos%0Awith%20language%20action%20annotations%2C%20covering%20a%20wide%20range%20of%20scenarios.%20Extensive%0Aexperiments%20show%20that%20SlowFast-VGen%20outperforms%20baselines%20across%20various%0Ametrics%20for%20action-driven%20video%20generation%2C%20achieving%20an%20FVD%20score%20of%20514%0Acompared%20to%20782%2C%20and%20maintaining%20consistency%20in%20longer%20videos%2C%20with%20an%20average%0Aof%200.37%20scene%20cuts%20versus%200.89.%20The%20slow-fast%20learning%20loop%20algorithm%0Asignificantly%20enhances%20performances%20on%20long-horizon%20planning%20tasks%20as%20well.%0AProject%20Website%3A%20https%3A//slowfast-vgen.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23277v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlowFast-VGen%253A%2520Slow-Fast%2520Learning%2520for%2520Action-Driven%2520Long%2520Video%250A%2520%2520Generation%26entry.906535625%3DYining%2520Hong%2520and%2520Beide%2520Liu%2520and%2520Maxine%2520Wu%2520and%2520Yuanhao%2520Zhai%2520and%2520Kai-Wei%2520Chang%2520and%2520Lingjie%2520Li%2520and%2520Kevin%2520Lin%2520and%2520Chung-Ching%2520Lin%2520and%2520Jianfeng%2520Wang%2520and%2520Zhengyuan%2520Yang%2520and%2520Yingnian%2520Wu%2520and%2520Lijuan%2520Wang%26entry.1292438233%3D%2520%2520Human%2520beings%2520are%2520endowed%2520with%2520a%2520complementary%2520learning%2520system%252C%2520which%2520bridges%250Athe%2520slow%2520learning%2520of%2520general%2520world%2520dynamics%2520with%2520fast%2520storage%2520of%2520episodic%250Amemory%2520from%2520a%2520new%2520experience.%2520Previous%2520video%2520generation%2520models%252C%2520however%252C%250Aprimarily%2520focus%2520on%2520slow%2520learning%2520by%2520pre-training%2520on%2520vast%2520amounts%2520of%2520data%252C%250Aoverlooking%2520the%2520fast%2520learning%2520phase%2520crucial%2520for%2520episodic%2520memory%2520storage.%2520This%250Aoversight%2520leads%2520to%2520inconsistencies%2520across%2520temporally%2520distant%2520frames%2520when%250Agenerating%2520longer%2520videos%252C%2520as%2520these%2520frames%2520fall%2520beyond%2520the%2520model%2527s%2520context%250Awindow.%2520To%2520this%2520end%252C%2520we%2520introduce%2520SlowFast-VGen%252C%2520a%2520novel%2520dual-speed%2520learning%250Asystem%2520for%2520action-driven%2520long%2520video%2520generation.%2520Our%2520approach%2520incorporates%2520a%250Amasked%2520conditional%2520video%2520diffusion%2520model%2520for%2520the%2520slow%2520learning%2520of%2520world%250Adynamics%252C%2520alongside%2520an%2520inference-time%2520fast%2520learning%2520strategy%2520based%2520on%2520a%250Atemporal%2520LoRA%2520module.%2520Specifically%252C%2520the%2520fast%2520learning%2520process%2520updates%2520its%250Atemporal%2520LoRA%2520parameters%2520based%2520on%2520local%2520inputs%2520and%2520outputs%252C%2520thereby%2520efficiently%250Astoring%2520episodic%2520memory%2520in%2520its%2520parameters.%2520We%2520further%2520propose%2520a%2520slow-fast%250Alearning%2520loop%2520algorithm%2520that%2520seamlessly%2520integrates%2520the%2520inner%2520fast%2520learning%2520loop%250Ainto%2520the%2520outer%2520slow%2520learning%2520loop%252C%2520enabling%2520the%2520recall%2520of%2520prior%2520multi-episode%250Aexperiences%2520for%2520context-aware%2520skill%2520learning.%2520To%2520facilitate%2520the%2520slow%2520learning%250Aof%2520an%2520approximate%2520world%2520model%252C%2520we%2520collect%2520a%2520large-scale%2520dataset%2520of%2520200k%2520videos%250Awith%2520language%2520action%2520annotations%252C%2520covering%2520a%2520wide%2520range%2520of%2520scenarios.%2520Extensive%250Aexperiments%2520show%2520that%2520SlowFast-VGen%2520outperforms%2520baselines%2520across%2520various%250Ametrics%2520for%2520action-driven%2520video%2520generation%252C%2520achieving%2520an%2520FVD%2520score%2520of%2520514%250Acompared%2520to%2520782%252C%2520and%2520maintaining%2520consistency%2520in%2520longer%2520videos%252C%2520with%2520an%2520average%250Aof%25200.37%2520scene%2520cuts%2520versus%25200.89.%2520The%2520slow-fast%2520learning%2520loop%2520algorithm%250Asignificantly%2520enhances%2520performances%2520on%2520long-horizon%2520planning%2520tasks%2520as%2520well.%250AProject%2520Website%253A%2520https%253A//slowfast-vgen.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23277v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SlowFast-VGen%3A%20Slow-Fast%20Learning%20for%20Action-Driven%20Long%20Video%0A%20%20Generation&entry.906535625=Yining%20Hong%20and%20Beide%20Liu%20and%20Maxine%20Wu%20and%20Yuanhao%20Zhai%20and%20Kai-Wei%20Chang%20and%20Lingjie%20Li%20and%20Kevin%20Lin%20and%20Chung-Ching%20Lin%20and%20Jianfeng%20Wang%20and%20Zhengyuan%20Yang%20and%20Yingnian%20Wu%20and%20Lijuan%20Wang&entry.1292438233=%20%20Human%20beings%20are%20endowed%20with%20a%20complementary%20learning%20system%2C%20which%20bridges%0Athe%20slow%20learning%20of%20general%20world%20dynamics%20with%20fast%20storage%20of%20episodic%0Amemory%20from%20a%20new%20experience.%20Previous%20video%20generation%20models%2C%20however%2C%0Aprimarily%20focus%20on%20slow%20learning%20by%20pre-training%20on%20vast%20amounts%20of%20data%2C%0Aoverlooking%20the%20fast%20learning%20phase%20crucial%20for%20episodic%20memory%20storage.%20This%0Aoversight%20leads%20to%20inconsistencies%20across%20temporally%20distant%20frames%20when%0Agenerating%20longer%20videos%2C%20as%20these%20frames%20fall%20beyond%20the%20model%27s%20context%0Awindow.%20To%20this%20end%2C%20we%20introduce%20SlowFast-VGen%2C%20a%20novel%20dual-speed%20learning%0Asystem%20for%20action-driven%20long%20video%20generation.%20Our%20approach%20incorporates%20a%0Amasked%20conditional%20video%20diffusion%20model%20for%20the%20slow%20learning%20of%20world%0Adynamics%2C%20alongside%20an%20inference-time%20fast%20learning%20strategy%20based%20on%20a%0Atemporal%20LoRA%20module.%20Specifically%2C%20the%20fast%20learning%20process%20updates%20its%0Atemporal%20LoRA%20parameters%20based%20on%20local%20inputs%20and%20outputs%2C%20thereby%20efficiently%0Astoring%20episodic%20memory%20in%20its%20parameters.%20We%20further%20propose%20a%20slow-fast%0Alearning%20loop%20algorithm%20that%20seamlessly%20integrates%20the%20inner%20fast%20learning%20loop%0Ainto%20the%20outer%20slow%20learning%20loop%2C%20enabling%20the%20recall%20of%20prior%20multi-episode%0Aexperiences%20for%20context-aware%20skill%20learning.%20To%20facilitate%20the%20slow%20learning%0Aof%20an%20approximate%20world%20model%2C%20we%20collect%20a%20large-scale%20dataset%20of%20200k%20videos%0Awith%20language%20action%20annotations%2C%20covering%20a%20wide%20range%20of%20scenarios.%20Extensive%0Aexperiments%20show%20that%20SlowFast-VGen%20outperforms%20baselines%20across%20various%0Ametrics%20for%20action-driven%20video%20generation%2C%20achieving%20an%20FVD%20score%20of%20514%0Acompared%20to%20782%2C%20and%20maintaining%20consistency%20in%20longer%20videos%2C%20with%20an%20average%0Aof%200.37%20scene%20cuts%20versus%200.89.%20The%20slow-fast%20learning%20loop%20algorithm%0Asignificantly%20enhances%20performances%20on%20long-horizon%20planning%20tasks%20as%20well.%0AProject%20Website%3A%20https%3A//slowfast-vgen.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23277v1&entry.124074799=Read"},
{"title": "Depth Anywhere: Enhancing 360 Monocular Depth Estimation via Perspective\n  Distillation and Unlabeled Data Augmentation", "author": "Ning-Hsu Wang and Yu-Lun Liu", "abstract": "  Accurately estimating depth in 360-degree imagery is crucial for virtual\nreality, autonomous navigation, and immersive media applications. Existing\ndepth estimation methods designed for perspective-view imagery fail when\napplied to 360-degree images due to different camera projections and\ndistortions, whereas 360-degree methods perform inferior due to the lack of\nlabeled data pairs. We propose a new depth estimation framework that utilizes\nunlabeled 360-degree data effectively. Our approach uses state-of-the-art\nperspective depth estimation models as teacher models to generate pseudo labels\nthrough a six-face cube projection technique, enabling efficient labeling of\ndepth in 360-degree images. This method leverages the increasing availability\nof large datasets. Our approach includes two main stages: offline mask\ngeneration for invalid regions and an online semi-supervised joint training\nregime. We tested our approach on benchmark datasets such as Matterport3D and\nStanford2D3D, showing significant improvements in depth estimation accuracy,\nparticularly in zero-shot scenarios. Our proposed training pipeline can enhance\nany 360 monocular depth estimator and demonstrates effective knowledge transfer\nacross different camera projections and data types. See our project page for\nresults: https://albert100121.github.io/Depth-Anywhere/\n", "link": "http://arxiv.org/abs/2406.12849v2", "date": "2024-10-30", "relevancy": 2.4715, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6349}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6145}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth%20Anywhere%3A%20Enhancing%20360%20Monocular%20Depth%20Estimation%20via%20Perspective%0A%20%20Distillation%20and%20Unlabeled%20Data%20Augmentation&body=Title%3A%20Depth%20Anywhere%3A%20Enhancing%20360%20Monocular%20Depth%20Estimation%20via%20Perspective%0A%20%20Distillation%20and%20Unlabeled%20Data%20Augmentation%0AAuthor%3A%20Ning-Hsu%20Wang%20and%20Yu-Lun%20Liu%0AAbstract%3A%20%20%20Accurately%20estimating%20depth%20in%20360-degree%20imagery%20is%20crucial%20for%20virtual%0Areality%2C%20autonomous%20navigation%2C%20and%20immersive%20media%20applications.%20Existing%0Adepth%20estimation%20methods%20designed%20for%20perspective-view%20imagery%20fail%20when%0Aapplied%20to%20360-degree%20images%20due%20to%20different%20camera%20projections%20and%0Adistortions%2C%20whereas%20360-degree%20methods%20perform%20inferior%20due%20to%20the%20lack%20of%0Alabeled%20data%20pairs.%20We%20propose%20a%20new%20depth%20estimation%20framework%20that%20utilizes%0Aunlabeled%20360-degree%20data%20effectively.%20Our%20approach%20uses%20state-of-the-art%0Aperspective%20depth%20estimation%20models%20as%20teacher%20models%20to%20generate%20pseudo%20labels%0Athrough%20a%20six-face%20cube%20projection%20technique%2C%20enabling%20efficient%20labeling%20of%0Adepth%20in%20360-degree%20images.%20This%20method%20leverages%20the%20increasing%20availability%0Aof%20large%20datasets.%20Our%20approach%20includes%20two%20main%20stages%3A%20offline%20mask%0Ageneration%20for%20invalid%20regions%20and%20an%20online%20semi-supervised%20joint%20training%0Aregime.%20We%20tested%20our%20approach%20on%20benchmark%20datasets%20such%20as%20Matterport3D%20and%0AStanford2D3D%2C%20showing%20significant%20improvements%20in%20depth%20estimation%20accuracy%2C%0Aparticularly%20in%20zero-shot%20scenarios.%20Our%20proposed%20training%20pipeline%20can%20enhance%0Aany%20360%20monocular%20depth%20estimator%20and%20demonstrates%20effective%20knowledge%20transfer%0Aacross%20different%20camera%20projections%20and%20data%20types.%20See%20our%20project%20page%20for%0Aresults%3A%20https%3A//albert100121.github.io/Depth-Anywhere/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12849v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth%2520Anywhere%253A%2520Enhancing%2520360%2520Monocular%2520Depth%2520Estimation%2520via%2520Perspective%250A%2520%2520Distillation%2520and%2520Unlabeled%2520Data%2520Augmentation%26entry.906535625%3DNing-Hsu%2520Wang%2520and%2520Yu-Lun%2520Liu%26entry.1292438233%3D%2520%2520Accurately%2520estimating%2520depth%2520in%2520360-degree%2520imagery%2520is%2520crucial%2520for%2520virtual%250Areality%252C%2520autonomous%2520navigation%252C%2520and%2520immersive%2520media%2520applications.%2520Existing%250Adepth%2520estimation%2520methods%2520designed%2520for%2520perspective-view%2520imagery%2520fail%2520when%250Aapplied%2520to%2520360-degree%2520images%2520due%2520to%2520different%2520camera%2520projections%2520and%250Adistortions%252C%2520whereas%2520360-degree%2520methods%2520perform%2520inferior%2520due%2520to%2520the%2520lack%2520of%250Alabeled%2520data%2520pairs.%2520We%2520propose%2520a%2520new%2520depth%2520estimation%2520framework%2520that%2520utilizes%250Aunlabeled%2520360-degree%2520data%2520effectively.%2520Our%2520approach%2520uses%2520state-of-the-art%250Aperspective%2520depth%2520estimation%2520models%2520as%2520teacher%2520models%2520to%2520generate%2520pseudo%2520labels%250Athrough%2520a%2520six-face%2520cube%2520projection%2520technique%252C%2520enabling%2520efficient%2520labeling%2520of%250Adepth%2520in%2520360-degree%2520images.%2520This%2520method%2520leverages%2520the%2520increasing%2520availability%250Aof%2520large%2520datasets.%2520Our%2520approach%2520includes%2520two%2520main%2520stages%253A%2520offline%2520mask%250Ageneration%2520for%2520invalid%2520regions%2520and%2520an%2520online%2520semi-supervised%2520joint%2520training%250Aregime.%2520We%2520tested%2520our%2520approach%2520on%2520benchmark%2520datasets%2520such%2520as%2520Matterport3D%2520and%250AStanford2D3D%252C%2520showing%2520significant%2520improvements%2520in%2520depth%2520estimation%2520accuracy%252C%250Aparticularly%2520in%2520zero-shot%2520scenarios.%2520Our%2520proposed%2520training%2520pipeline%2520can%2520enhance%250Aany%2520360%2520monocular%2520depth%2520estimator%2520and%2520demonstrates%2520effective%2520knowledge%2520transfer%250Aacross%2520different%2520camera%2520projections%2520and%2520data%2520types.%2520See%2520our%2520project%2520page%2520for%250Aresults%253A%2520https%253A//albert100121.github.io/Depth-Anywhere/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12849v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20Anywhere%3A%20Enhancing%20360%20Monocular%20Depth%20Estimation%20via%20Perspective%0A%20%20Distillation%20and%20Unlabeled%20Data%20Augmentation&entry.906535625=Ning-Hsu%20Wang%20and%20Yu-Lun%20Liu&entry.1292438233=%20%20Accurately%20estimating%20depth%20in%20360-degree%20imagery%20is%20crucial%20for%20virtual%0Areality%2C%20autonomous%20navigation%2C%20and%20immersive%20media%20applications.%20Existing%0Adepth%20estimation%20methods%20designed%20for%20perspective-view%20imagery%20fail%20when%0Aapplied%20to%20360-degree%20images%20due%20to%20different%20camera%20projections%20and%0Adistortions%2C%20whereas%20360-degree%20methods%20perform%20inferior%20due%20to%20the%20lack%20of%0Alabeled%20data%20pairs.%20We%20propose%20a%20new%20depth%20estimation%20framework%20that%20utilizes%0Aunlabeled%20360-degree%20data%20effectively.%20Our%20approach%20uses%20state-of-the-art%0Aperspective%20depth%20estimation%20models%20as%20teacher%20models%20to%20generate%20pseudo%20labels%0Athrough%20a%20six-face%20cube%20projection%20technique%2C%20enabling%20efficient%20labeling%20of%0Adepth%20in%20360-degree%20images.%20This%20method%20leverages%20the%20increasing%20availability%0Aof%20large%20datasets.%20Our%20approach%20includes%20two%20main%20stages%3A%20offline%20mask%0Ageneration%20for%20invalid%20regions%20and%20an%20online%20semi-supervised%20joint%20training%0Aregime.%20We%20tested%20our%20approach%20on%20benchmark%20datasets%20such%20as%20Matterport3D%20and%0AStanford2D3D%2C%20showing%20significant%20improvements%20in%20depth%20estimation%20accuracy%2C%0Aparticularly%20in%20zero-shot%20scenarios.%20Our%20proposed%20training%20pipeline%20can%20enhance%0Aany%20360%20monocular%20depth%20estimator%20and%20demonstrates%20effective%20knowledge%20transfer%0Aacross%20different%20camera%20projections%20and%20data%20types.%20See%20our%20project%20page%20for%0Aresults%3A%20https%3A//albert100121.github.io/Depth-Anywhere/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12849v2&entry.124074799=Read"},
{"title": "The Persistence of Neural Collapse Despite Low-Rank Bias: An Analytic\n  Perspective Through Unconstrained Features", "author": "Connall Garrod and Jonathan P. Keating", "abstract": "  Modern deep neural networks have been observed to exhibit a simple structure\nin their final layer features and weights, commonly referred to as neural\ncollapse. This phenomenon has also been noted in layers beyond the final one,\nan extension known as deep neural collapse. Recent findings indicate that such\na structure is generally not optimal in the deep unconstrained feature model,\nan approximation of an expressive network. This is attributed to a low-rank\nbias induced by regularization, which favors solutions with lower-rank than\nthose typically associated with deep neural collapse. In this work, we extend\nthese observations to the cross-entropy loss and analyze how the low-rank bias\ninfluences various solutions. Additionally, we explore how this bias induces\nspecific structures in the singular values of the weights at global optima.\nFurthermore, we examine the loss surface of these models and provide evidence\nthat the frequent observation of deep neural collapse in practice, despite its\nsuboptimality, may result from its higher degeneracy on the loss surface.\n", "link": "http://arxiv.org/abs/2410.23169v1", "date": "2024-10-30", "relevancy": 2.4255, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4821}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Persistence%20of%20Neural%20Collapse%20Despite%20Low-Rank%20Bias%3A%20An%20Analytic%0A%20%20Perspective%20Through%20Unconstrained%20Features&body=Title%3A%20The%20Persistence%20of%20Neural%20Collapse%20Despite%20Low-Rank%20Bias%3A%20An%20Analytic%0A%20%20Perspective%20Through%20Unconstrained%20Features%0AAuthor%3A%20Connall%20Garrod%20and%20Jonathan%20P.%20Keating%0AAbstract%3A%20%20%20Modern%20deep%20neural%20networks%20have%20been%20observed%20to%20exhibit%20a%20simple%20structure%0Ain%20their%20final%20layer%20features%20and%20weights%2C%20commonly%20referred%20to%20as%20neural%0Acollapse.%20This%20phenomenon%20has%20also%20been%20noted%20in%20layers%20beyond%20the%20final%20one%2C%0Aan%20extension%20known%20as%20deep%20neural%20collapse.%20Recent%20findings%20indicate%20that%20such%0Aa%20structure%20is%20generally%20not%20optimal%20in%20the%20deep%20unconstrained%20feature%20model%2C%0Aan%20approximation%20of%20an%20expressive%20network.%20This%20is%20attributed%20to%20a%20low-rank%0Abias%20induced%20by%20regularization%2C%20which%20favors%20solutions%20with%20lower-rank%20than%0Athose%20typically%20associated%20with%20deep%20neural%20collapse.%20In%20this%20work%2C%20we%20extend%0Athese%20observations%20to%20the%20cross-entropy%20loss%20and%20analyze%20how%20the%20low-rank%20bias%0Ainfluences%20various%20solutions.%20Additionally%2C%20we%20explore%20how%20this%20bias%20induces%0Aspecific%20structures%20in%20the%20singular%20values%20of%20the%20weights%20at%20global%20optima.%0AFurthermore%2C%20we%20examine%20the%20loss%20surface%20of%20these%20models%20and%20provide%20evidence%0Athat%20the%20frequent%20observation%20of%20deep%20neural%20collapse%20in%20practice%2C%20despite%20its%0Asuboptimality%2C%20may%20result%20from%20its%20higher%20degeneracy%20on%20the%20loss%20surface.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23169v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Persistence%2520of%2520Neural%2520Collapse%2520Despite%2520Low-Rank%2520Bias%253A%2520An%2520Analytic%250A%2520%2520Perspective%2520Through%2520Unconstrained%2520Features%26entry.906535625%3DConnall%2520Garrod%2520and%2520Jonathan%2520P.%2520Keating%26entry.1292438233%3D%2520%2520Modern%2520deep%2520neural%2520networks%2520have%2520been%2520observed%2520to%2520exhibit%2520a%2520simple%2520structure%250Ain%2520their%2520final%2520layer%2520features%2520and%2520weights%252C%2520commonly%2520referred%2520to%2520as%2520neural%250Acollapse.%2520This%2520phenomenon%2520has%2520also%2520been%2520noted%2520in%2520layers%2520beyond%2520the%2520final%2520one%252C%250Aan%2520extension%2520known%2520as%2520deep%2520neural%2520collapse.%2520Recent%2520findings%2520indicate%2520that%2520such%250Aa%2520structure%2520is%2520generally%2520not%2520optimal%2520in%2520the%2520deep%2520unconstrained%2520feature%2520model%252C%250Aan%2520approximation%2520of%2520an%2520expressive%2520network.%2520This%2520is%2520attributed%2520to%2520a%2520low-rank%250Abias%2520induced%2520by%2520regularization%252C%2520which%2520favors%2520solutions%2520with%2520lower-rank%2520than%250Athose%2520typically%2520associated%2520with%2520deep%2520neural%2520collapse.%2520In%2520this%2520work%252C%2520we%2520extend%250Athese%2520observations%2520to%2520the%2520cross-entropy%2520loss%2520and%2520analyze%2520how%2520the%2520low-rank%2520bias%250Ainfluences%2520various%2520solutions.%2520Additionally%252C%2520we%2520explore%2520how%2520this%2520bias%2520induces%250Aspecific%2520structures%2520in%2520the%2520singular%2520values%2520of%2520the%2520weights%2520at%2520global%2520optima.%250AFurthermore%252C%2520we%2520examine%2520the%2520loss%2520surface%2520of%2520these%2520models%2520and%2520provide%2520evidence%250Athat%2520the%2520frequent%2520observation%2520of%2520deep%2520neural%2520collapse%2520in%2520practice%252C%2520despite%2520its%250Asuboptimality%252C%2520may%2520result%2520from%2520its%2520higher%2520degeneracy%2520on%2520the%2520loss%2520surface.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23169v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Persistence%20of%20Neural%20Collapse%20Despite%20Low-Rank%20Bias%3A%20An%20Analytic%0A%20%20Perspective%20Through%20Unconstrained%20Features&entry.906535625=Connall%20Garrod%20and%20Jonathan%20P.%20Keating&entry.1292438233=%20%20Modern%20deep%20neural%20networks%20have%20been%20observed%20to%20exhibit%20a%20simple%20structure%0Ain%20their%20final%20layer%20features%20and%20weights%2C%20commonly%20referred%20to%20as%20neural%0Acollapse.%20This%20phenomenon%20has%20also%20been%20noted%20in%20layers%20beyond%20the%20final%20one%2C%0Aan%20extension%20known%20as%20deep%20neural%20collapse.%20Recent%20findings%20indicate%20that%20such%0Aa%20structure%20is%20generally%20not%20optimal%20in%20the%20deep%20unconstrained%20feature%20model%2C%0Aan%20approximation%20of%20an%20expressive%20network.%20This%20is%20attributed%20to%20a%20low-rank%0Abias%20induced%20by%20regularization%2C%20which%20favors%20solutions%20with%20lower-rank%20than%0Athose%20typically%20associated%20with%20deep%20neural%20collapse.%20In%20this%20work%2C%20we%20extend%0Athese%20observations%20to%20the%20cross-entropy%20loss%20and%20analyze%20how%20the%20low-rank%20bias%0Ainfluences%20various%20solutions.%20Additionally%2C%20we%20explore%20how%20this%20bias%20induces%0Aspecific%20structures%20in%20the%20singular%20values%20of%20the%20weights%20at%20global%20optima.%0AFurthermore%2C%20we%20examine%20the%20loss%20surface%20of%20these%20models%20and%20provide%20evidence%0Athat%20the%20frequent%20observation%20of%20deep%20neural%20collapse%20in%20practice%2C%20despite%20its%0Asuboptimality%2C%20may%20result%20from%20its%20higher%20degeneracy%20on%20the%20loss%20surface.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23169v1&entry.124074799=Read"},
{"title": "Towards Unifying Understanding and Generation in the Era of Vision\n  Foundation Models: A Survey from the Autoregression Perspective", "author": "Shenghao Xie and Wenqiang Zu and Mingyang Zhao and Duo Su and Shilong Liu and Ruohua Shi and Guoqi Li and Shanghang Zhang and Lei Ma", "abstract": "  Autoregression in large language models (LLMs) has shown impressive\nscalability by unifying all language tasks into the next token prediction\nparadigm. Recently, there is a growing interest in extending this success to\nvision foundation models. In this survey, we review the recent advances and\ndiscuss future directions for autoregressive vision foundation models. First,\nwe present the trend for next generation of vision foundation models, i.e.,\nunifying both understanding and generation in vision tasks. We then analyze the\nlimitations of existing vision foundation models, and present a formal\ndefinition of autoregression with its advantages. Later, we categorize\nautoregressive vision foundation models from their vision tokenizers and\nautoregression backbones. Finally, we discuss several promising research\nchallenges and directions. To the best of our knowledge, this is the first\nsurvey to comprehensively summarize autoregressive vision foundation models\nunder the trend of unifying understanding and generation. A collection of\nrelated resources is available at https://github.com/EmmaSRH/ARVFM.\n", "link": "http://arxiv.org/abs/2410.22217v2", "date": "2024-10-30", "relevancy": 2.4077, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6176}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6176}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Unifying%20Understanding%20and%20Generation%20in%20the%20Era%20of%20Vision%0A%20%20Foundation%20Models%3A%20A%20Survey%20from%20the%20Autoregression%20Perspective&body=Title%3A%20Towards%20Unifying%20Understanding%20and%20Generation%20in%20the%20Era%20of%20Vision%0A%20%20Foundation%20Models%3A%20A%20Survey%20from%20the%20Autoregression%20Perspective%0AAuthor%3A%20Shenghao%20Xie%20and%20Wenqiang%20Zu%20and%20Mingyang%20Zhao%20and%20Duo%20Su%20and%20Shilong%20Liu%20and%20Ruohua%20Shi%20and%20Guoqi%20Li%20and%20Shanghang%20Zhang%20and%20Lei%20Ma%0AAbstract%3A%20%20%20Autoregression%20in%20large%20language%20models%20%28LLMs%29%20has%20shown%20impressive%0Ascalability%20by%20unifying%20all%20language%20tasks%20into%20the%20next%20token%20prediction%0Aparadigm.%20Recently%2C%20there%20is%20a%20growing%20interest%20in%20extending%20this%20success%20to%0Avision%20foundation%20models.%20In%20this%20survey%2C%20we%20review%20the%20recent%20advances%20and%0Adiscuss%20future%20directions%20for%20autoregressive%20vision%20foundation%20models.%20First%2C%0Awe%20present%20the%20trend%20for%20next%20generation%20of%20vision%20foundation%20models%2C%20i.e.%2C%0Aunifying%20both%20understanding%20and%20generation%20in%20vision%20tasks.%20We%20then%20analyze%20the%0Alimitations%20of%20existing%20vision%20foundation%20models%2C%20and%20present%20a%20formal%0Adefinition%20of%20autoregression%20with%20its%20advantages.%20Later%2C%20we%20categorize%0Aautoregressive%20vision%20foundation%20models%20from%20their%20vision%20tokenizers%20and%0Aautoregression%20backbones.%20Finally%2C%20we%20discuss%20several%20promising%20research%0Achallenges%20and%20directions.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Asurvey%20to%20comprehensively%20summarize%20autoregressive%20vision%20foundation%20models%0Aunder%20the%20trend%20of%20unifying%20understanding%20and%20generation.%20A%20collection%20of%0Arelated%20resources%20is%20available%20at%20https%3A//github.com/EmmaSRH/ARVFM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22217v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Unifying%2520Understanding%2520and%2520Generation%2520in%2520the%2520Era%2520of%2520Vision%250A%2520%2520Foundation%2520Models%253A%2520A%2520Survey%2520from%2520the%2520Autoregression%2520Perspective%26entry.906535625%3DShenghao%2520Xie%2520and%2520Wenqiang%2520Zu%2520and%2520Mingyang%2520Zhao%2520and%2520Duo%2520Su%2520and%2520Shilong%2520Liu%2520and%2520Ruohua%2520Shi%2520and%2520Guoqi%2520Li%2520and%2520Shanghang%2520Zhang%2520and%2520Lei%2520Ma%26entry.1292438233%3D%2520%2520Autoregression%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520shown%2520impressive%250Ascalability%2520by%2520unifying%2520all%2520language%2520tasks%2520into%2520the%2520next%2520token%2520prediction%250Aparadigm.%2520Recently%252C%2520there%2520is%2520a%2520growing%2520interest%2520in%2520extending%2520this%2520success%2520to%250Avision%2520foundation%2520models.%2520In%2520this%2520survey%252C%2520we%2520review%2520the%2520recent%2520advances%2520and%250Adiscuss%2520future%2520directions%2520for%2520autoregressive%2520vision%2520foundation%2520models.%2520First%252C%250Awe%2520present%2520the%2520trend%2520for%2520next%2520generation%2520of%2520vision%2520foundation%2520models%252C%2520i.e.%252C%250Aunifying%2520both%2520understanding%2520and%2520generation%2520in%2520vision%2520tasks.%2520We%2520then%2520analyze%2520the%250Alimitations%2520of%2520existing%2520vision%2520foundation%2520models%252C%2520and%2520present%2520a%2520formal%250Adefinition%2520of%2520autoregression%2520with%2520its%2520advantages.%2520Later%252C%2520we%2520categorize%250Aautoregressive%2520vision%2520foundation%2520models%2520from%2520their%2520vision%2520tokenizers%2520and%250Aautoregression%2520backbones.%2520Finally%252C%2520we%2520discuss%2520several%2520promising%2520research%250Achallenges%2520and%2520directions.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%250Asurvey%2520to%2520comprehensively%2520summarize%2520autoregressive%2520vision%2520foundation%2520models%250Aunder%2520the%2520trend%2520of%2520unifying%2520understanding%2520and%2520generation.%2520A%2520collection%2520of%250Arelated%2520resources%2520is%2520available%2520at%2520https%253A//github.com/EmmaSRH/ARVFM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22217v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Unifying%20Understanding%20and%20Generation%20in%20the%20Era%20of%20Vision%0A%20%20Foundation%20Models%3A%20A%20Survey%20from%20the%20Autoregression%20Perspective&entry.906535625=Shenghao%20Xie%20and%20Wenqiang%20Zu%20and%20Mingyang%20Zhao%20and%20Duo%20Su%20and%20Shilong%20Liu%20and%20Ruohua%20Shi%20and%20Guoqi%20Li%20and%20Shanghang%20Zhang%20and%20Lei%20Ma&entry.1292438233=%20%20Autoregression%20in%20large%20language%20models%20%28LLMs%29%20has%20shown%20impressive%0Ascalability%20by%20unifying%20all%20language%20tasks%20into%20the%20next%20token%20prediction%0Aparadigm.%20Recently%2C%20there%20is%20a%20growing%20interest%20in%20extending%20this%20success%20to%0Avision%20foundation%20models.%20In%20this%20survey%2C%20we%20review%20the%20recent%20advances%20and%0Adiscuss%20future%20directions%20for%20autoregressive%20vision%20foundation%20models.%20First%2C%0Awe%20present%20the%20trend%20for%20next%20generation%20of%20vision%20foundation%20models%2C%20i.e.%2C%0Aunifying%20both%20understanding%20and%20generation%20in%20vision%20tasks.%20We%20then%20analyze%20the%0Alimitations%20of%20existing%20vision%20foundation%20models%2C%20and%20present%20a%20formal%0Adefinition%20of%20autoregression%20with%20its%20advantages.%20Later%2C%20we%20categorize%0Aautoregressive%20vision%20foundation%20models%20from%20their%20vision%20tokenizers%20and%0Aautoregression%20backbones.%20Finally%2C%20we%20discuss%20several%20promising%20research%0Achallenges%20and%20directions.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Asurvey%20to%20comprehensively%20summarize%20autoregressive%20vision%20foundation%20models%0Aunder%20the%20trend%20of%20unifying%20understanding%20and%20generation.%20A%20collection%20of%0Arelated%20resources%20is%20available%20at%20https%3A//github.com/EmmaSRH/ARVFM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22217v2&entry.124074799=Read"},
{"title": "RelationBooth: Towards Relation-Aware Customized Object Generation", "author": "Qingyu Shi and Lu Qi and Jianzong Wu and Jinbin Bai and Jingbo Wang and Yunhai Tong and Xiangtai Li and Ming-Husang Yang", "abstract": "  Customized image generation is crucial for delivering personalized content\nbased on user-provided image prompts, aligning large-scale text-to-image\ndiffusion models with individual needs. However, existing models often overlook\nthe relationships between customized objects in generated images. Instead, this\nwork addresses that gap by focusing on relation-aware customized image\ngeneration, which aims to preserve the identities from image prompts while\nmaintaining the predicate relations described in text prompts. Specifically, we\nintroduce RelationBooth, a framework that disentangles identity and relation\nlearning through a well-curated dataset. Our training data consists of\nrelation-specific images, independent object images containing identity\ninformation, and text prompts to guide relation generation. Then, we propose\ntwo key modules to tackle the two main challenges: generating accurate and\nnatural relations, especially when significant pose adjustments are required,\nand avoiding object confusion in cases of overlap. First, we introduce a\nkeypoint matching loss that effectively guides the model in adjusting object\nposes closely tied to their relationships. Second, we incorporate local\nfeatures from the image prompts to better distinguish between objects,\npreventing confusion in overlapping cases. Extensive results on three\nbenchmarks demonstrate the superiority of RelationBooth in generating precise\nrelations while preserving object identities across a diverse set of objects\nand relations. The source code and trained models will be made available to the\npublic.\n", "link": "http://arxiv.org/abs/2410.23280v1", "date": "2024-10-30", "relevancy": 2.3892, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6191}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5834}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RelationBooth%3A%20Towards%20Relation-Aware%20Customized%20Object%20Generation&body=Title%3A%20RelationBooth%3A%20Towards%20Relation-Aware%20Customized%20Object%20Generation%0AAuthor%3A%20Qingyu%20Shi%20and%20Lu%20Qi%20and%20Jianzong%20Wu%20and%20Jinbin%20Bai%20and%20Jingbo%20Wang%20and%20Yunhai%20Tong%20and%20Xiangtai%20Li%20and%20Ming-Husang%20Yang%0AAbstract%3A%20%20%20Customized%20image%20generation%20is%20crucial%20for%20delivering%20personalized%20content%0Abased%20on%20user-provided%20image%20prompts%2C%20aligning%20large-scale%20text-to-image%0Adiffusion%20models%20with%20individual%20needs.%20However%2C%20existing%20models%20often%20overlook%0Athe%20relationships%20between%20customized%20objects%20in%20generated%20images.%20Instead%2C%20this%0Awork%20addresses%20that%20gap%20by%20focusing%20on%20relation-aware%20customized%20image%0Ageneration%2C%20which%20aims%20to%20preserve%20the%20identities%20from%20image%20prompts%20while%0Amaintaining%20the%20predicate%20relations%20described%20in%20text%20prompts.%20Specifically%2C%20we%0Aintroduce%20RelationBooth%2C%20a%20framework%20that%20disentangles%20identity%20and%20relation%0Alearning%20through%20a%20well-curated%20dataset.%20Our%20training%20data%20consists%20of%0Arelation-specific%20images%2C%20independent%20object%20images%20containing%20identity%0Ainformation%2C%20and%20text%20prompts%20to%20guide%20relation%20generation.%20Then%2C%20we%20propose%0Atwo%20key%20modules%20to%20tackle%20the%20two%20main%20challenges%3A%20generating%20accurate%20and%0Anatural%20relations%2C%20especially%20when%20significant%20pose%20adjustments%20are%20required%2C%0Aand%20avoiding%20object%20confusion%20in%20cases%20of%20overlap.%20First%2C%20we%20introduce%20a%0Akeypoint%20matching%20loss%20that%20effectively%20guides%20the%20model%20in%20adjusting%20object%0Aposes%20closely%20tied%20to%20their%20relationships.%20Second%2C%20we%20incorporate%20local%0Afeatures%20from%20the%20image%20prompts%20to%20better%20distinguish%20between%20objects%2C%0Apreventing%20confusion%20in%20overlapping%20cases.%20Extensive%20results%20on%20three%0Abenchmarks%20demonstrate%20the%20superiority%20of%20RelationBooth%20in%20generating%20precise%0Arelations%20while%20preserving%20object%20identities%20across%20a%20diverse%20set%20of%20objects%0Aand%20relations.%20The%20source%20code%20and%20trained%20models%20will%20be%20made%20available%20to%20the%0Apublic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23280v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelationBooth%253A%2520Towards%2520Relation-Aware%2520Customized%2520Object%2520Generation%26entry.906535625%3DQingyu%2520Shi%2520and%2520Lu%2520Qi%2520and%2520Jianzong%2520Wu%2520and%2520Jinbin%2520Bai%2520and%2520Jingbo%2520Wang%2520and%2520Yunhai%2520Tong%2520and%2520Xiangtai%2520Li%2520and%2520Ming-Husang%2520Yang%26entry.1292438233%3D%2520%2520Customized%2520image%2520generation%2520is%2520crucial%2520for%2520delivering%2520personalized%2520content%250Abased%2520on%2520user-provided%2520image%2520prompts%252C%2520aligning%2520large-scale%2520text-to-image%250Adiffusion%2520models%2520with%2520individual%2520needs.%2520However%252C%2520existing%2520models%2520often%2520overlook%250Athe%2520relationships%2520between%2520customized%2520objects%2520in%2520generated%2520images.%2520Instead%252C%2520this%250Awork%2520addresses%2520that%2520gap%2520by%2520focusing%2520on%2520relation-aware%2520customized%2520image%250Ageneration%252C%2520which%2520aims%2520to%2520preserve%2520the%2520identities%2520from%2520image%2520prompts%2520while%250Amaintaining%2520the%2520predicate%2520relations%2520described%2520in%2520text%2520prompts.%2520Specifically%252C%2520we%250Aintroduce%2520RelationBooth%252C%2520a%2520framework%2520that%2520disentangles%2520identity%2520and%2520relation%250Alearning%2520through%2520a%2520well-curated%2520dataset.%2520Our%2520training%2520data%2520consists%2520of%250Arelation-specific%2520images%252C%2520independent%2520object%2520images%2520containing%2520identity%250Ainformation%252C%2520and%2520text%2520prompts%2520to%2520guide%2520relation%2520generation.%2520Then%252C%2520we%2520propose%250Atwo%2520key%2520modules%2520to%2520tackle%2520the%2520two%2520main%2520challenges%253A%2520generating%2520accurate%2520and%250Anatural%2520relations%252C%2520especially%2520when%2520significant%2520pose%2520adjustments%2520are%2520required%252C%250Aand%2520avoiding%2520object%2520confusion%2520in%2520cases%2520of%2520overlap.%2520First%252C%2520we%2520introduce%2520a%250Akeypoint%2520matching%2520loss%2520that%2520effectively%2520guides%2520the%2520model%2520in%2520adjusting%2520object%250Aposes%2520closely%2520tied%2520to%2520their%2520relationships.%2520Second%252C%2520we%2520incorporate%2520local%250Afeatures%2520from%2520the%2520image%2520prompts%2520to%2520better%2520distinguish%2520between%2520objects%252C%250Apreventing%2520confusion%2520in%2520overlapping%2520cases.%2520Extensive%2520results%2520on%2520three%250Abenchmarks%2520demonstrate%2520the%2520superiority%2520of%2520RelationBooth%2520in%2520generating%2520precise%250Arelations%2520while%2520preserving%2520object%2520identities%2520across%2520a%2520diverse%2520set%2520of%2520objects%250Aand%2520relations.%2520The%2520source%2520code%2520and%2520trained%2520models%2520will%2520be%2520made%2520available%2520to%2520the%250Apublic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23280v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RelationBooth%3A%20Towards%20Relation-Aware%20Customized%20Object%20Generation&entry.906535625=Qingyu%20Shi%20and%20Lu%20Qi%20and%20Jianzong%20Wu%20and%20Jinbin%20Bai%20and%20Jingbo%20Wang%20and%20Yunhai%20Tong%20and%20Xiangtai%20Li%20and%20Ming-Husang%20Yang&entry.1292438233=%20%20Customized%20image%20generation%20is%20crucial%20for%20delivering%20personalized%20content%0Abased%20on%20user-provided%20image%20prompts%2C%20aligning%20large-scale%20text-to-image%0Adiffusion%20models%20with%20individual%20needs.%20However%2C%20existing%20models%20often%20overlook%0Athe%20relationships%20between%20customized%20objects%20in%20generated%20images.%20Instead%2C%20this%0Awork%20addresses%20that%20gap%20by%20focusing%20on%20relation-aware%20customized%20image%0Ageneration%2C%20which%20aims%20to%20preserve%20the%20identities%20from%20image%20prompts%20while%0Amaintaining%20the%20predicate%20relations%20described%20in%20text%20prompts.%20Specifically%2C%20we%0Aintroduce%20RelationBooth%2C%20a%20framework%20that%20disentangles%20identity%20and%20relation%0Alearning%20through%20a%20well-curated%20dataset.%20Our%20training%20data%20consists%20of%0Arelation-specific%20images%2C%20independent%20object%20images%20containing%20identity%0Ainformation%2C%20and%20text%20prompts%20to%20guide%20relation%20generation.%20Then%2C%20we%20propose%0Atwo%20key%20modules%20to%20tackle%20the%20two%20main%20challenges%3A%20generating%20accurate%20and%0Anatural%20relations%2C%20especially%20when%20significant%20pose%20adjustments%20are%20required%2C%0Aand%20avoiding%20object%20confusion%20in%20cases%20of%20overlap.%20First%2C%20we%20introduce%20a%0Akeypoint%20matching%20loss%20that%20effectively%20guides%20the%20model%20in%20adjusting%20object%0Aposes%20closely%20tied%20to%20their%20relationships.%20Second%2C%20we%20incorporate%20local%0Afeatures%20from%20the%20image%20prompts%20to%20better%20distinguish%20between%20objects%2C%0Apreventing%20confusion%20in%20overlapping%20cases.%20Extensive%20results%20on%20three%0Abenchmarks%20demonstrate%20the%20superiority%20of%20RelationBooth%20in%20generating%20precise%0Arelations%20while%20preserving%20object%20identities%20across%20a%20diverse%20set%20of%20objects%0Aand%20relations.%20The%20source%20code%20and%20trained%20models%20will%20be%20made%20available%20to%20the%0Apublic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23280v1&entry.124074799=Read"},
{"title": "Unbounded: A Generative Infinite Game of Character Life Simulation", "author": "Jialu Li and Yuanzhen Li and Neal Wadhwa and Yael Pritch and David E. Jacobs and Michael Rubinstein and Mohit Bansal and Nataniel Ruiz", "abstract": "  We introduce the concept of a generative infinite game, a video game that\ntranscends the traditional boundaries of finite, hard-coded systems by using\ngenerative models. Inspired by James P. Carse's distinction between finite and\ninfinite games, we leverage recent advances in generative AI to create\nUnbounded: a game of character life simulation that is fully encapsulated in\ngenerative models. Specifically, Unbounded draws inspiration from sandbox life\nsimulations and allows you to interact with your autonomous virtual character\nin a virtual world by feeding, playing with and guiding it - with open-ended\nmechanics generated by an LLM, some of which can be emergent. In order to\ndevelop Unbounded, we propose technical innovations in both the LLM and visual\ngeneration domains. Specifically, we present: (1) a specialized, distilled\nlarge language model (LLM) that dynamically generates game mechanics,\nnarratives, and character interactions in real-time, and (2) a new dynamic\nregional image prompt Adapter (IP-Adapter) for vision models that ensures\nconsistent yet flexible visual generation of a character across multiple\nenvironments. We evaluate our system through both qualitative and quantitative\nanalysis, showing significant improvements in character life simulation, user\ninstruction following, narrative coherence, and visual consistency for both\ncharacters and the environments compared to traditional related approaches.\n", "link": "http://arxiv.org/abs/2410.18975v2", "date": "2024-10-30", "relevancy": 2.3658, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6453}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5712}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unbounded%3A%20A%20Generative%20Infinite%20Game%20of%20Character%20Life%20Simulation&body=Title%3A%20Unbounded%3A%20A%20Generative%20Infinite%20Game%20of%20Character%20Life%20Simulation%0AAuthor%3A%20Jialu%20Li%20and%20Yuanzhen%20Li%20and%20Neal%20Wadhwa%20and%20Yael%20Pritch%20and%20David%20E.%20Jacobs%20and%20Michael%20Rubinstein%20and%20Mohit%20Bansal%20and%20Nataniel%20Ruiz%0AAbstract%3A%20%20%20We%20introduce%20the%20concept%20of%20a%20generative%20infinite%20game%2C%20a%20video%20game%20that%0Atranscends%20the%20traditional%20boundaries%20of%20finite%2C%20hard-coded%20systems%20by%20using%0Agenerative%20models.%20Inspired%20by%20James%20P.%20Carse%27s%20distinction%20between%20finite%20and%0Ainfinite%20games%2C%20we%20leverage%20recent%20advances%20in%20generative%20AI%20to%20create%0AUnbounded%3A%20a%20game%20of%20character%20life%20simulation%20that%20is%20fully%20encapsulated%20in%0Agenerative%20models.%20Specifically%2C%20Unbounded%20draws%20inspiration%20from%20sandbox%20life%0Asimulations%20and%20allows%20you%20to%20interact%20with%20your%20autonomous%20virtual%20character%0Ain%20a%20virtual%20world%20by%20feeding%2C%20playing%20with%20and%20guiding%20it%20-%20with%20open-ended%0Amechanics%20generated%20by%20an%20LLM%2C%20some%20of%20which%20can%20be%20emergent.%20In%20order%20to%0Adevelop%20Unbounded%2C%20we%20propose%20technical%20innovations%20in%20both%20the%20LLM%20and%20visual%0Ageneration%20domains.%20Specifically%2C%20we%20present%3A%20%281%29%20a%20specialized%2C%20distilled%0Alarge%20language%20model%20%28LLM%29%20that%20dynamically%20generates%20game%20mechanics%2C%0Anarratives%2C%20and%20character%20interactions%20in%20real-time%2C%20and%20%282%29%20a%20new%20dynamic%0Aregional%20image%20prompt%20Adapter%20%28IP-Adapter%29%20for%20vision%20models%20that%20ensures%0Aconsistent%20yet%20flexible%20visual%20generation%20of%20a%20character%20across%20multiple%0Aenvironments.%20We%20evaluate%20our%20system%20through%20both%20qualitative%20and%20quantitative%0Aanalysis%2C%20showing%20significant%20improvements%20in%20character%20life%20simulation%2C%20user%0Ainstruction%20following%2C%20narrative%20coherence%2C%20and%20visual%20consistency%20for%20both%0Acharacters%20and%20the%20environments%20compared%20to%20traditional%20related%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18975v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnbounded%253A%2520A%2520Generative%2520Infinite%2520Game%2520of%2520Character%2520Life%2520Simulation%26entry.906535625%3DJialu%2520Li%2520and%2520Yuanzhen%2520Li%2520and%2520Neal%2520Wadhwa%2520and%2520Yael%2520Pritch%2520and%2520David%2520E.%2520Jacobs%2520and%2520Michael%2520Rubinstein%2520and%2520Mohit%2520Bansal%2520and%2520Nataniel%2520Ruiz%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520concept%2520of%2520a%2520generative%2520infinite%2520game%252C%2520a%2520video%2520game%2520that%250Atranscends%2520the%2520traditional%2520boundaries%2520of%2520finite%252C%2520hard-coded%2520systems%2520by%2520using%250Agenerative%2520models.%2520Inspired%2520by%2520James%2520P.%2520Carse%2527s%2520distinction%2520between%2520finite%2520and%250Ainfinite%2520games%252C%2520we%2520leverage%2520recent%2520advances%2520in%2520generative%2520AI%2520to%2520create%250AUnbounded%253A%2520a%2520game%2520of%2520character%2520life%2520simulation%2520that%2520is%2520fully%2520encapsulated%2520in%250Agenerative%2520models.%2520Specifically%252C%2520Unbounded%2520draws%2520inspiration%2520from%2520sandbox%2520life%250Asimulations%2520and%2520allows%2520you%2520to%2520interact%2520with%2520your%2520autonomous%2520virtual%2520character%250Ain%2520a%2520virtual%2520world%2520by%2520feeding%252C%2520playing%2520with%2520and%2520guiding%2520it%2520-%2520with%2520open-ended%250Amechanics%2520generated%2520by%2520an%2520LLM%252C%2520some%2520of%2520which%2520can%2520be%2520emergent.%2520In%2520order%2520to%250Adevelop%2520Unbounded%252C%2520we%2520propose%2520technical%2520innovations%2520in%2520both%2520the%2520LLM%2520and%2520visual%250Ageneration%2520domains.%2520Specifically%252C%2520we%2520present%253A%2520%25281%2529%2520a%2520specialized%252C%2520distilled%250Alarge%2520language%2520model%2520%2528LLM%2529%2520that%2520dynamically%2520generates%2520game%2520mechanics%252C%250Anarratives%252C%2520and%2520character%2520interactions%2520in%2520real-time%252C%2520and%2520%25282%2529%2520a%2520new%2520dynamic%250Aregional%2520image%2520prompt%2520Adapter%2520%2528IP-Adapter%2529%2520for%2520vision%2520models%2520that%2520ensures%250Aconsistent%2520yet%2520flexible%2520visual%2520generation%2520of%2520a%2520character%2520across%2520multiple%250Aenvironments.%2520We%2520evaluate%2520our%2520system%2520through%2520both%2520qualitative%2520and%2520quantitative%250Aanalysis%252C%2520showing%2520significant%2520improvements%2520in%2520character%2520life%2520simulation%252C%2520user%250Ainstruction%2520following%252C%2520narrative%2520coherence%252C%2520and%2520visual%2520consistency%2520for%2520both%250Acharacters%2520and%2520the%2520environments%2520compared%2520to%2520traditional%2520related%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18975v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unbounded%3A%20A%20Generative%20Infinite%20Game%20of%20Character%20Life%20Simulation&entry.906535625=Jialu%20Li%20and%20Yuanzhen%20Li%20and%20Neal%20Wadhwa%20and%20Yael%20Pritch%20and%20David%20E.%20Jacobs%20and%20Michael%20Rubinstein%20and%20Mohit%20Bansal%20and%20Nataniel%20Ruiz&entry.1292438233=%20%20We%20introduce%20the%20concept%20of%20a%20generative%20infinite%20game%2C%20a%20video%20game%20that%0Atranscends%20the%20traditional%20boundaries%20of%20finite%2C%20hard-coded%20systems%20by%20using%0Agenerative%20models.%20Inspired%20by%20James%20P.%20Carse%27s%20distinction%20between%20finite%20and%0Ainfinite%20games%2C%20we%20leverage%20recent%20advances%20in%20generative%20AI%20to%20create%0AUnbounded%3A%20a%20game%20of%20character%20life%20simulation%20that%20is%20fully%20encapsulated%20in%0Agenerative%20models.%20Specifically%2C%20Unbounded%20draws%20inspiration%20from%20sandbox%20life%0Asimulations%20and%20allows%20you%20to%20interact%20with%20your%20autonomous%20virtual%20character%0Ain%20a%20virtual%20world%20by%20feeding%2C%20playing%20with%20and%20guiding%20it%20-%20with%20open-ended%0Amechanics%20generated%20by%20an%20LLM%2C%20some%20of%20which%20can%20be%20emergent.%20In%20order%20to%0Adevelop%20Unbounded%2C%20we%20propose%20technical%20innovations%20in%20both%20the%20LLM%20and%20visual%0Ageneration%20domains.%20Specifically%2C%20we%20present%3A%20%281%29%20a%20specialized%2C%20distilled%0Alarge%20language%20model%20%28LLM%29%20that%20dynamically%20generates%20game%20mechanics%2C%0Anarratives%2C%20and%20character%20interactions%20in%20real-time%2C%20and%20%282%29%20a%20new%20dynamic%0Aregional%20image%20prompt%20Adapter%20%28IP-Adapter%29%20for%20vision%20models%20that%20ensures%0Aconsistent%20yet%20flexible%20visual%20generation%20of%20a%20character%20across%20multiple%0Aenvironments.%20We%20evaluate%20our%20system%20through%20both%20qualitative%20and%20quantitative%0Aanalysis%2C%20showing%20significant%20improvements%20in%20character%20life%20simulation%2C%20user%0Ainstruction%20following%2C%20narrative%20coherence%2C%20and%20visual%20consistency%20for%20both%0Acharacters%20and%20the%20environments%20compared%20to%20traditional%20related%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18975v2&entry.124074799=Read"},
{"title": "ReferEverything: Towards Segmenting Everything We Can Speak of in Videos", "author": "Anurag Bagchi and Zhipeng Bao and Yu-Xiong Wang and Pavel Tokmakov and Martial Hebert", "abstract": "  We present REM, a framework for segmenting a wide range of concepts in video\nthat can be described through natural language. Our method capitalizes on\nvisual-language representations learned by video diffusion models on\nInternet-scale datasets. A key insight of our approach is preserving as much of\nthe generative model's original representation as possible, while fine-tuning\nit on narrow-domain Referral Object Segmentation datasets. As a result, our\nframework can accurately segment and track rare and unseen objects, despite\nbeing trained on object masks from a limited set of categories. Additionally,\nit can generalize to non-object dynamic concepts, such as waves crashing in the\nocean, as demonstrated in our newly introduced benchmark for Referral Video\nProcess Segmentation (Ref-VPS). Our experiments show that REM performs on par\nwith state-of-the-art approaches on in-domain datasets, like Ref-DAVIS, while\noutperforming them by up to twelve points in terms of region similarity on\nout-of-domain data, leveraging the power of Internet-scale pre-training.\n", "link": "http://arxiv.org/abs/2410.23287v1", "date": "2024-10-30", "relevancy": 2.2867, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5763}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5763}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReferEverything%3A%20Towards%20Segmenting%20Everything%20We%20Can%20Speak%20of%20in%20Videos&body=Title%3A%20ReferEverything%3A%20Towards%20Segmenting%20Everything%20We%20Can%20Speak%20of%20in%20Videos%0AAuthor%3A%20Anurag%20Bagchi%20and%20Zhipeng%20Bao%20and%20Yu-Xiong%20Wang%20and%20Pavel%20Tokmakov%20and%20Martial%20Hebert%0AAbstract%3A%20%20%20We%20present%20REM%2C%20a%20framework%20for%20segmenting%20a%20wide%20range%20of%20concepts%20in%20video%0Athat%20can%20be%20described%20through%20natural%20language.%20Our%20method%20capitalizes%20on%0Avisual-language%20representations%20learned%20by%20video%20diffusion%20models%20on%0AInternet-scale%20datasets.%20A%20key%20insight%20of%20our%20approach%20is%20preserving%20as%20much%20of%0Athe%20generative%20model%27s%20original%20representation%20as%20possible%2C%20while%20fine-tuning%0Ait%20on%20narrow-domain%20Referral%20Object%20Segmentation%20datasets.%20As%20a%20result%2C%20our%0Aframework%20can%20accurately%20segment%20and%20track%20rare%20and%20unseen%20objects%2C%20despite%0Abeing%20trained%20on%20object%20masks%20from%20a%20limited%20set%20of%20categories.%20Additionally%2C%0Ait%20can%20generalize%20to%20non-object%20dynamic%20concepts%2C%20such%20as%20waves%20crashing%20in%20the%0Aocean%2C%20as%20demonstrated%20in%20our%20newly%20introduced%20benchmark%20for%20Referral%20Video%0AProcess%20Segmentation%20%28Ref-VPS%29.%20Our%20experiments%20show%20that%20REM%20performs%20on%20par%0Awith%20state-of-the-art%20approaches%20on%20in-domain%20datasets%2C%20like%20Ref-DAVIS%2C%20while%0Aoutperforming%20them%20by%20up%20to%20twelve%20points%20in%20terms%20of%20region%20similarity%20on%0Aout-of-domain%20data%2C%20leveraging%20the%20power%20of%20Internet-scale%20pre-training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23287v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReferEverything%253A%2520Towards%2520Segmenting%2520Everything%2520We%2520Can%2520Speak%2520of%2520in%2520Videos%26entry.906535625%3DAnurag%2520Bagchi%2520and%2520Zhipeng%2520Bao%2520and%2520Yu-Xiong%2520Wang%2520and%2520Pavel%2520Tokmakov%2520and%2520Martial%2520Hebert%26entry.1292438233%3D%2520%2520We%2520present%2520REM%252C%2520a%2520framework%2520for%2520segmenting%2520a%2520wide%2520range%2520of%2520concepts%2520in%2520video%250Athat%2520can%2520be%2520described%2520through%2520natural%2520language.%2520Our%2520method%2520capitalizes%2520on%250Avisual-language%2520representations%2520learned%2520by%2520video%2520diffusion%2520models%2520on%250AInternet-scale%2520datasets.%2520A%2520key%2520insight%2520of%2520our%2520approach%2520is%2520preserving%2520as%2520much%2520of%250Athe%2520generative%2520model%2527s%2520original%2520representation%2520as%2520possible%252C%2520while%2520fine-tuning%250Ait%2520on%2520narrow-domain%2520Referral%2520Object%2520Segmentation%2520datasets.%2520As%2520a%2520result%252C%2520our%250Aframework%2520can%2520accurately%2520segment%2520and%2520track%2520rare%2520and%2520unseen%2520objects%252C%2520despite%250Abeing%2520trained%2520on%2520object%2520masks%2520from%2520a%2520limited%2520set%2520of%2520categories.%2520Additionally%252C%250Ait%2520can%2520generalize%2520to%2520non-object%2520dynamic%2520concepts%252C%2520such%2520as%2520waves%2520crashing%2520in%2520the%250Aocean%252C%2520as%2520demonstrated%2520in%2520our%2520newly%2520introduced%2520benchmark%2520for%2520Referral%2520Video%250AProcess%2520Segmentation%2520%2528Ref-VPS%2529.%2520Our%2520experiments%2520show%2520that%2520REM%2520performs%2520on%2520par%250Awith%2520state-of-the-art%2520approaches%2520on%2520in-domain%2520datasets%252C%2520like%2520Ref-DAVIS%252C%2520while%250Aoutperforming%2520them%2520by%2520up%2520to%2520twelve%2520points%2520in%2520terms%2520of%2520region%2520similarity%2520on%250Aout-of-domain%2520data%252C%2520leveraging%2520the%2520power%2520of%2520Internet-scale%2520pre-training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23287v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReferEverything%3A%20Towards%20Segmenting%20Everything%20We%20Can%20Speak%20of%20in%20Videos&entry.906535625=Anurag%20Bagchi%20and%20Zhipeng%20Bao%20and%20Yu-Xiong%20Wang%20and%20Pavel%20Tokmakov%20and%20Martial%20Hebert&entry.1292438233=%20%20We%20present%20REM%2C%20a%20framework%20for%20segmenting%20a%20wide%20range%20of%20concepts%20in%20video%0Athat%20can%20be%20described%20through%20natural%20language.%20Our%20method%20capitalizes%20on%0Avisual-language%20representations%20learned%20by%20video%20diffusion%20models%20on%0AInternet-scale%20datasets.%20A%20key%20insight%20of%20our%20approach%20is%20preserving%20as%20much%20of%0Athe%20generative%20model%27s%20original%20representation%20as%20possible%2C%20while%20fine-tuning%0Ait%20on%20narrow-domain%20Referral%20Object%20Segmentation%20datasets.%20As%20a%20result%2C%20our%0Aframework%20can%20accurately%20segment%20and%20track%20rare%20and%20unseen%20objects%2C%20despite%0Abeing%20trained%20on%20object%20masks%20from%20a%20limited%20set%20of%20categories.%20Additionally%2C%0Ait%20can%20generalize%20to%20non-object%20dynamic%20concepts%2C%20such%20as%20waves%20crashing%20in%20the%0Aocean%2C%20as%20demonstrated%20in%20our%20newly%20introduced%20benchmark%20for%20Referral%20Video%0AProcess%20Segmentation%20%28Ref-VPS%29.%20Our%20experiments%20show%20that%20REM%20performs%20on%20par%0Awith%20state-of-the-art%20approaches%20on%20in-domain%20datasets%2C%20like%20Ref-DAVIS%2C%20while%0Aoutperforming%20them%20by%20up%20to%20twelve%20points%20in%20terms%20of%20region%20similarity%20on%0Aout-of-domain%20data%2C%20leveraging%20the%20power%20of%20Internet-scale%20pre-training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23287v1&entry.124074799=Read"},
{"title": "Revisiting MAE pre-training for 3D medical image segmentation", "author": "Tassilo Wald and Constantin Ulrich and Stanislav Lukyanenko and Andrei Goncharov and Alberto Paderno and Leander Maerkisch and Paul F. J\u00e4ger and Klaus Maier-Hein", "abstract": "  Self-Supervised Learning (SSL) presents an exciting opportunity to unlock the\npotential of vast, untapped clinical datasets, for various downstream\napplications that suffer from the scarcity of labeled data. While SSL has\nrevolutionized fields like natural language processing and computer vision,\ntheir adoption in 3D medical image computing has been limited by three key\npitfalls: Small pre-training dataset sizes, architectures inadequate for 3D\nmedical image analysis, and insufficient evaluation practices. We address these\nissues by i) leveraging a large-scale dataset of 44k 3D brain MRI volumes and\nii) using a Residual Encoder U-Net architecture within the state-of-the-art\nnnU-Net framework. iii) A robust development framework, incorporating 5\ndevelopment and 8 testing brain MRI segmentation datasets, allowed\nperformance-driven design decisions to optimize the simple concept of Masked\nAuto Encoders (MAEs) for 3D CNNs. The resulting model not only surpasses\nprevious SSL methods but also outperforms the strong nnU-Net baseline by an\naverage of approximately 3 Dice points. Furthermore, our model demonstrates\nexceptional stability, achieving the highest average rank of 2 out of 7\nmethods, compared to the second-best method's mean rank of 3.\n", "link": "http://arxiv.org/abs/2410.23132v1", "date": "2024-10-30", "relevancy": 2.281, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6234}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5631}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20MAE%20pre-training%20for%203D%20medical%20image%20segmentation&body=Title%3A%20Revisiting%20MAE%20pre-training%20for%203D%20medical%20image%20segmentation%0AAuthor%3A%20Tassilo%20Wald%20and%20Constantin%20Ulrich%20and%20Stanislav%20Lukyanenko%20and%20Andrei%20Goncharov%20and%20Alberto%20Paderno%20and%20Leander%20Maerkisch%20and%20Paul%20F.%20J%C3%A4ger%20and%20Klaus%20Maier-Hein%0AAbstract%3A%20%20%20Self-Supervised%20Learning%20%28SSL%29%20presents%20an%20exciting%20opportunity%20to%20unlock%20the%0Apotential%20of%20vast%2C%20untapped%20clinical%20datasets%2C%20for%20various%20downstream%0Aapplications%20that%20suffer%20from%20the%20scarcity%20of%20labeled%20data.%20While%20SSL%20has%0Arevolutionized%20fields%20like%20natural%20language%20processing%20and%20computer%20vision%2C%0Atheir%20adoption%20in%203D%20medical%20image%20computing%20has%20been%20limited%20by%20three%20key%0Apitfalls%3A%20Small%20pre-training%20dataset%20sizes%2C%20architectures%20inadequate%20for%203D%0Amedical%20image%20analysis%2C%20and%20insufficient%20evaluation%20practices.%20We%20address%20these%0Aissues%20by%20i%29%20leveraging%20a%20large-scale%20dataset%20of%2044k%203D%20brain%20MRI%20volumes%20and%0Aii%29%20using%20a%20Residual%20Encoder%20U-Net%20architecture%20within%20the%20state-of-the-art%0AnnU-Net%20framework.%20iii%29%20A%20robust%20development%20framework%2C%20incorporating%205%0Adevelopment%20and%208%20testing%20brain%20MRI%20segmentation%20datasets%2C%20allowed%0Aperformance-driven%20design%20decisions%20to%20optimize%20the%20simple%20concept%20of%20Masked%0AAuto%20Encoders%20%28MAEs%29%20for%203D%20CNNs.%20The%20resulting%20model%20not%20only%20surpasses%0Aprevious%20SSL%20methods%20but%20also%20outperforms%20the%20strong%20nnU-Net%20baseline%20by%20an%0Aaverage%20of%20approximately%203%20Dice%20points.%20Furthermore%2C%20our%20model%20demonstrates%0Aexceptional%20stability%2C%20achieving%20the%20highest%20average%20rank%20of%202%20out%20of%207%0Amethods%2C%20compared%20to%20the%20second-best%20method%27s%20mean%20rank%20of%203.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23132v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520MAE%2520pre-training%2520for%25203D%2520medical%2520image%2520segmentation%26entry.906535625%3DTassilo%2520Wald%2520and%2520Constantin%2520Ulrich%2520and%2520Stanislav%2520Lukyanenko%2520and%2520Andrei%2520Goncharov%2520and%2520Alberto%2520Paderno%2520and%2520Leander%2520Maerkisch%2520and%2520Paul%2520F.%2520J%25C3%25A4ger%2520and%2520Klaus%2520Maier-Hein%26entry.1292438233%3D%2520%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520presents%2520an%2520exciting%2520opportunity%2520to%2520unlock%2520the%250Apotential%2520of%2520vast%252C%2520untapped%2520clinical%2520datasets%252C%2520for%2520various%2520downstream%250Aapplications%2520that%2520suffer%2520from%2520the%2520scarcity%2520of%2520labeled%2520data.%2520While%2520SSL%2520has%250Arevolutionized%2520fields%2520like%2520natural%2520language%2520processing%2520and%2520computer%2520vision%252C%250Atheir%2520adoption%2520in%25203D%2520medical%2520image%2520computing%2520has%2520been%2520limited%2520by%2520three%2520key%250Apitfalls%253A%2520Small%2520pre-training%2520dataset%2520sizes%252C%2520architectures%2520inadequate%2520for%25203D%250Amedical%2520image%2520analysis%252C%2520and%2520insufficient%2520evaluation%2520practices.%2520We%2520address%2520these%250Aissues%2520by%2520i%2529%2520leveraging%2520a%2520large-scale%2520dataset%2520of%252044k%25203D%2520brain%2520MRI%2520volumes%2520and%250Aii%2529%2520using%2520a%2520Residual%2520Encoder%2520U-Net%2520architecture%2520within%2520the%2520state-of-the-art%250AnnU-Net%2520framework.%2520iii%2529%2520A%2520robust%2520development%2520framework%252C%2520incorporating%25205%250Adevelopment%2520and%25208%2520testing%2520brain%2520MRI%2520segmentation%2520datasets%252C%2520allowed%250Aperformance-driven%2520design%2520decisions%2520to%2520optimize%2520the%2520simple%2520concept%2520of%2520Masked%250AAuto%2520Encoders%2520%2528MAEs%2529%2520for%25203D%2520CNNs.%2520The%2520resulting%2520model%2520not%2520only%2520surpasses%250Aprevious%2520SSL%2520methods%2520but%2520also%2520outperforms%2520the%2520strong%2520nnU-Net%2520baseline%2520by%2520an%250Aaverage%2520of%2520approximately%25203%2520Dice%2520points.%2520Furthermore%252C%2520our%2520model%2520demonstrates%250Aexceptional%2520stability%252C%2520achieving%2520the%2520highest%2520average%2520rank%2520of%25202%2520out%2520of%25207%250Amethods%252C%2520compared%2520to%2520the%2520second-best%2520method%2527s%2520mean%2520rank%2520of%25203.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23132v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20MAE%20pre-training%20for%203D%20medical%20image%20segmentation&entry.906535625=Tassilo%20Wald%20and%20Constantin%20Ulrich%20and%20Stanislav%20Lukyanenko%20and%20Andrei%20Goncharov%20and%20Alberto%20Paderno%20and%20Leander%20Maerkisch%20and%20Paul%20F.%20J%C3%A4ger%20and%20Klaus%20Maier-Hein&entry.1292438233=%20%20Self-Supervised%20Learning%20%28SSL%29%20presents%20an%20exciting%20opportunity%20to%20unlock%20the%0Apotential%20of%20vast%2C%20untapped%20clinical%20datasets%2C%20for%20various%20downstream%0Aapplications%20that%20suffer%20from%20the%20scarcity%20of%20labeled%20data.%20While%20SSL%20has%0Arevolutionized%20fields%20like%20natural%20language%20processing%20and%20computer%20vision%2C%0Atheir%20adoption%20in%203D%20medical%20image%20computing%20has%20been%20limited%20by%20three%20key%0Apitfalls%3A%20Small%20pre-training%20dataset%20sizes%2C%20architectures%20inadequate%20for%203D%0Amedical%20image%20analysis%2C%20and%20insufficient%20evaluation%20practices.%20We%20address%20these%0Aissues%20by%20i%29%20leveraging%20a%20large-scale%20dataset%20of%2044k%203D%20brain%20MRI%20volumes%20and%0Aii%29%20using%20a%20Residual%20Encoder%20U-Net%20architecture%20within%20the%20state-of-the-art%0AnnU-Net%20framework.%20iii%29%20A%20robust%20development%20framework%2C%20incorporating%205%0Adevelopment%20and%208%20testing%20brain%20MRI%20segmentation%20datasets%2C%20allowed%0Aperformance-driven%20design%20decisions%20to%20optimize%20the%20simple%20concept%20of%20Masked%0AAuto%20Encoders%20%28MAEs%29%20for%203D%20CNNs.%20The%20resulting%20model%20not%20only%20surpasses%0Aprevious%20SSL%20methods%20but%20also%20outperforms%20the%20strong%20nnU-Net%20baseline%20by%20an%0Aaverage%20of%20approximately%203%20Dice%20points.%20Furthermore%2C%20our%20model%20demonstrates%0Aexceptional%20stability%2C%20achieving%20the%20highest%20average%20rank%20of%202%20out%20of%207%0Amethods%2C%20compared%20to%20the%20second-best%20method%27s%20mean%20rank%20of%203.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23132v1&entry.124074799=Read"},
{"title": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "author": "Jyh-Jing Hwang and Runsheng Xu and Hubert Lin and Wei-Chih Hung and Jingwei Ji and Kristy Choi and Di Huang and Tong He and Paul Covington and Benjamin Sapp and James Guo and Dragomir Anguelov and Mingxing Tan", "abstract": "  We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving.\nBuilt on a multi-modal large language model foundation, EMMA directly maps raw\ncamera sensor data into various driving-specific outputs, including planner\ntrajectories, perception objects, and road graph elements. EMMA maximizes the\nutility of world knowledge from the pre-trained large language models, by\nrepresenting all non-sensor inputs (e.g. navigation instructions and ego\nvehicle status) and outputs (e.g. trajectories and 3D locations) as natural\nlanguage text. This approach allows EMMA to jointly process various driving\ntasks in a unified language space, and generate the outputs for each task using\ntask-specific prompts. Empirically, we demonstrate EMMA's effectiveness by\nachieving state-of-the-art performance in motion planning on nuScenes as well\nas competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also\nyields competitive results for camera-primary 3D object detection on the Waymo\nOpen Dataset (WOD). We show that co-training EMMA with planner trajectories,\nobject detection, and road graph tasks yields improvements across all three\ndomains, highlighting EMMA's potential as a generalist model for autonomous\ndriving applications. However, EMMA also exhibits certain limitations: it can\nprocess only a small amount of image frames, does not incorporate accurate 3D\nsensing modalities like LiDAR or radar and is computationally expensive. We\nhope that our results will inspire further research to mitigate these issues\nand to further evolve the state of the art in autonomous driving model\narchitectures.\n", "link": "http://arxiv.org/abs/2410.23262v1", "date": "2024-10-30", "relevancy": 2.2338, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5677}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5588}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMMA%3A%20End-to-End%20Multimodal%20Model%20for%20Autonomous%20Driving&body=Title%3A%20EMMA%3A%20End-to-End%20Multimodal%20Model%20for%20Autonomous%20Driving%0AAuthor%3A%20Jyh-Jing%20Hwang%20and%20Runsheng%20Xu%20and%20Hubert%20Lin%20and%20Wei-Chih%20Hung%20and%20Jingwei%20Ji%20and%20Kristy%20Choi%20and%20Di%20Huang%20and%20Tong%20He%20and%20Paul%20Covington%20and%20Benjamin%20Sapp%20and%20James%20Guo%20and%20Dragomir%20Anguelov%20and%20Mingxing%20Tan%0AAbstract%3A%20%20%20We%20introduce%20EMMA%2C%20an%20End-to-end%20Multimodal%20Model%20for%20Autonomous%20driving.%0ABuilt%20on%20a%20multi-modal%20large%20language%20model%20foundation%2C%20EMMA%20directly%20maps%20raw%0Acamera%20sensor%20data%20into%20various%20driving-specific%20outputs%2C%20including%20planner%0Atrajectories%2C%20perception%20objects%2C%20and%20road%20graph%20elements.%20EMMA%20maximizes%20the%0Autility%20of%20world%20knowledge%20from%20the%20pre-trained%20large%20language%20models%2C%20by%0Arepresenting%20all%20non-sensor%20inputs%20%28e.g.%20navigation%20instructions%20and%20ego%0Avehicle%20status%29%20and%20outputs%20%28e.g.%20trajectories%20and%203D%20locations%29%20as%20natural%0Alanguage%20text.%20This%20approach%20allows%20EMMA%20to%20jointly%20process%20various%20driving%0Atasks%20in%20a%20unified%20language%20space%2C%20and%20generate%20the%20outputs%20for%20each%20task%20using%0Atask-specific%20prompts.%20Empirically%2C%20we%20demonstrate%20EMMA%27s%20effectiveness%20by%0Aachieving%20state-of-the-art%20performance%20in%20motion%20planning%20on%20nuScenes%20as%20well%0Aas%20competitive%20results%20on%20the%20Waymo%20Open%20Motion%20Dataset%20%28WOMD%29.%20EMMA%20also%0Ayields%20competitive%20results%20for%20camera-primary%203D%20object%20detection%20on%20the%20Waymo%0AOpen%20Dataset%20%28WOD%29.%20We%20show%20that%20co-training%20EMMA%20with%20planner%20trajectories%2C%0Aobject%20detection%2C%20and%20road%20graph%20tasks%20yields%20improvements%20across%20all%20three%0Adomains%2C%20highlighting%20EMMA%27s%20potential%20as%20a%20generalist%20model%20for%20autonomous%0Adriving%20applications.%20However%2C%20EMMA%20also%20exhibits%20certain%20limitations%3A%20it%20can%0Aprocess%20only%20a%20small%20amount%20of%20image%20frames%2C%20does%20not%20incorporate%20accurate%203D%0Asensing%20modalities%20like%20LiDAR%20or%20radar%20and%20is%20computationally%20expensive.%20We%0Ahope%20that%20our%20results%20will%20inspire%20further%20research%20to%20mitigate%20these%20issues%0Aand%20to%20further%20evolve%20the%20state%20of%20the%20art%20in%20autonomous%20driving%20model%0Aarchitectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMMA%253A%2520End-to-End%2520Multimodal%2520Model%2520for%2520Autonomous%2520Driving%26entry.906535625%3DJyh-Jing%2520Hwang%2520and%2520Runsheng%2520Xu%2520and%2520Hubert%2520Lin%2520and%2520Wei-Chih%2520Hung%2520and%2520Jingwei%2520Ji%2520and%2520Kristy%2520Choi%2520and%2520Di%2520Huang%2520and%2520Tong%2520He%2520and%2520Paul%2520Covington%2520and%2520Benjamin%2520Sapp%2520and%2520James%2520Guo%2520and%2520Dragomir%2520Anguelov%2520and%2520Mingxing%2520Tan%26entry.1292438233%3D%2520%2520We%2520introduce%2520EMMA%252C%2520an%2520End-to-end%2520Multimodal%2520Model%2520for%2520Autonomous%2520driving.%250ABuilt%2520on%2520a%2520multi-modal%2520large%2520language%2520model%2520foundation%252C%2520EMMA%2520directly%2520maps%2520raw%250Acamera%2520sensor%2520data%2520into%2520various%2520driving-specific%2520outputs%252C%2520including%2520planner%250Atrajectories%252C%2520perception%2520objects%252C%2520and%2520road%2520graph%2520elements.%2520EMMA%2520maximizes%2520the%250Autility%2520of%2520world%2520knowledge%2520from%2520the%2520pre-trained%2520large%2520language%2520models%252C%2520by%250Arepresenting%2520all%2520non-sensor%2520inputs%2520%2528e.g.%2520navigation%2520instructions%2520and%2520ego%250Avehicle%2520status%2529%2520and%2520outputs%2520%2528e.g.%2520trajectories%2520and%25203D%2520locations%2529%2520as%2520natural%250Alanguage%2520text.%2520This%2520approach%2520allows%2520EMMA%2520to%2520jointly%2520process%2520various%2520driving%250Atasks%2520in%2520a%2520unified%2520language%2520space%252C%2520and%2520generate%2520the%2520outputs%2520for%2520each%2520task%2520using%250Atask-specific%2520prompts.%2520Empirically%252C%2520we%2520demonstrate%2520EMMA%2527s%2520effectiveness%2520by%250Aachieving%2520state-of-the-art%2520performance%2520in%2520motion%2520planning%2520on%2520nuScenes%2520as%2520well%250Aas%2520competitive%2520results%2520on%2520the%2520Waymo%2520Open%2520Motion%2520Dataset%2520%2528WOMD%2529.%2520EMMA%2520also%250Ayields%2520competitive%2520results%2520for%2520camera-primary%25203D%2520object%2520detection%2520on%2520the%2520Waymo%250AOpen%2520Dataset%2520%2528WOD%2529.%2520We%2520show%2520that%2520co-training%2520EMMA%2520with%2520planner%2520trajectories%252C%250Aobject%2520detection%252C%2520and%2520road%2520graph%2520tasks%2520yields%2520improvements%2520across%2520all%2520three%250Adomains%252C%2520highlighting%2520EMMA%2527s%2520potential%2520as%2520a%2520generalist%2520model%2520for%2520autonomous%250Adriving%2520applications.%2520However%252C%2520EMMA%2520also%2520exhibits%2520certain%2520limitations%253A%2520it%2520can%250Aprocess%2520only%2520a%2520small%2520amount%2520of%2520image%2520frames%252C%2520does%2520not%2520incorporate%2520accurate%25203D%250Asensing%2520modalities%2520like%2520LiDAR%2520or%2520radar%2520and%2520is%2520computationally%2520expensive.%2520We%250Ahope%2520that%2520our%2520results%2520will%2520inspire%2520further%2520research%2520to%2520mitigate%2520these%2520issues%250Aand%2520to%2520further%2520evolve%2520the%2520state%2520of%2520the%2520art%2520in%2520autonomous%2520driving%2520model%250Aarchitectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMMA%3A%20End-to-End%20Multimodal%20Model%20for%20Autonomous%20Driving&entry.906535625=Jyh-Jing%20Hwang%20and%20Runsheng%20Xu%20and%20Hubert%20Lin%20and%20Wei-Chih%20Hung%20and%20Jingwei%20Ji%20and%20Kristy%20Choi%20and%20Di%20Huang%20and%20Tong%20He%20and%20Paul%20Covington%20and%20Benjamin%20Sapp%20and%20James%20Guo%20and%20Dragomir%20Anguelov%20and%20Mingxing%20Tan&entry.1292438233=%20%20We%20introduce%20EMMA%2C%20an%20End-to-end%20Multimodal%20Model%20for%20Autonomous%20driving.%0ABuilt%20on%20a%20multi-modal%20large%20language%20model%20foundation%2C%20EMMA%20directly%20maps%20raw%0Acamera%20sensor%20data%20into%20various%20driving-specific%20outputs%2C%20including%20planner%0Atrajectories%2C%20perception%20objects%2C%20and%20road%20graph%20elements.%20EMMA%20maximizes%20the%0Autility%20of%20world%20knowledge%20from%20the%20pre-trained%20large%20language%20models%2C%20by%0Arepresenting%20all%20non-sensor%20inputs%20%28e.g.%20navigation%20instructions%20and%20ego%0Avehicle%20status%29%20and%20outputs%20%28e.g.%20trajectories%20and%203D%20locations%29%20as%20natural%0Alanguage%20text.%20This%20approach%20allows%20EMMA%20to%20jointly%20process%20various%20driving%0Atasks%20in%20a%20unified%20language%20space%2C%20and%20generate%20the%20outputs%20for%20each%20task%20using%0Atask-specific%20prompts.%20Empirically%2C%20we%20demonstrate%20EMMA%27s%20effectiveness%20by%0Aachieving%20state-of-the-art%20performance%20in%20motion%20planning%20on%20nuScenes%20as%20well%0Aas%20competitive%20results%20on%20the%20Waymo%20Open%20Motion%20Dataset%20%28WOMD%29.%20EMMA%20also%0Ayields%20competitive%20results%20for%20camera-primary%203D%20object%20detection%20on%20the%20Waymo%0AOpen%20Dataset%20%28WOD%29.%20We%20show%20that%20co-training%20EMMA%20with%20planner%20trajectories%2C%0Aobject%20detection%2C%20and%20road%20graph%20tasks%20yields%20improvements%20across%20all%20three%0Adomains%2C%20highlighting%20EMMA%27s%20potential%20as%20a%20generalist%20model%20for%20autonomous%0Adriving%20applications.%20However%2C%20EMMA%20also%20exhibits%20certain%20limitations%3A%20it%20can%0Aprocess%20only%20a%20small%20amount%20of%20image%20frames%2C%20does%20not%20incorporate%20accurate%203D%0Asensing%20modalities%20like%20LiDAR%20or%20radar%20and%20is%20computationally%20expensive.%20We%0Ahope%20that%20our%20results%20will%20inspire%20further%20research%20to%20mitigate%20these%20issues%0Aand%20to%20further%20evolve%20the%20state%20of%20the%20art%20in%20autonomous%20driving%20model%0Aarchitectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23262v1&entry.124074799=Read"},
{"title": "Source Code Foundation Models are Transferable Binary Analysis Knowledge\n  Bases", "author": "Zian Su and Xiangzhe Xu and Ziyang Huang and Kaiyuan Zhang and Xiangyu Zhang", "abstract": "  Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of\nbinary and source code, aiming to lift binary code to human-readable content\nrelevant to source code, thereby bridging the binary-source semantic gap.\nRecent advancements in uni-modal code model pre-training, particularly in\ngenerative Source Code Foundation Models (SCFMs) and binary understanding\nmodels, have laid the groundwork for transfer learning applicable to HOBRE.\nHowever, existing approaches for HOBRE rely heavily on uni-modal models like\nSCFMs for supervised fine-tuning or general LLMs for prompting, resulting in\nsub-optimal performance. Inspired by recent progress in large multi-modal\nmodels, we propose that it is possible to harness the strengths of uni-modal\ncode models from both sides to bridge the semantic gap effectively. In this\npaper, we introduce a novel probe-and-recover framework that incorporates a\nbinary-source encoder-decoder model and black-box LLMs for binary analysis. Our\napproach leverages the pre-trained knowledge within SCFMs to synthesize\nrelevant, symbol-rich code fragments as context. This additional context\nenables black-box LLMs to enhance recovery accuracy. We demonstrate significant\nimprovements in zero-shot binary summarization and binary function name\nrecovery, with a 10.3% relative gain in CHRF and a 16.7% relative gain in a\nGPT4-based metric for summarization, as well as a 6.7% and 7.4% absolute\nincrease in token-level precision and recall for name recovery, respectively.\nThese results highlight the effectiveness of our approach in automating and\nimproving binary code analysis.\n", "link": "http://arxiv.org/abs/2405.19581v2", "date": "2024-10-30", "relevancy": 2.2167, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Source%20Code%20Foundation%20Models%20are%20Transferable%20Binary%20Analysis%20Knowledge%0A%20%20Bases&body=Title%3A%20Source%20Code%20Foundation%20Models%20are%20Transferable%20Binary%20Analysis%20Knowledge%0A%20%20Bases%0AAuthor%3A%20Zian%20Su%20and%20Xiangzhe%20Xu%20and%20Ziyang%20Huang%20and%20Kaiyuan%20Zhang%20and%20Xiangyu%20Zhang%0AAbstract%3A%20%20%20Human-Oriented%20Binary%20Reverse%20Engineering%20%28HOBRE%29%20lies%20at%20the%20intersection%20of%0Abinary%20and%20source%20code%2C%20aiming%20to%20lift%20binary%20code%20to%20human-readable%20content%0Arelevant%20to%20source%20code%2C%20thereby%20bridging%20the%20binary-source%20semantic%20gap.%0ARecent%20advancements%20in%20uni-modal%20code%20model%20pre-training%2C%20particularly%20in%0Agenerative%20Source%20Code%20Foundation%20Models%20%28SCFMs%29%20and%20binary%20understanding%0Amodels%2C%20have%20laid%20the%20groundwork%20for%20transfer%20learning%20applicable%20to%20HOBRE.%0AHowever%2C%20existing%20approaches%20for%20HOBRE%20rely%20heavily%20on%20uni-modal%20models%20like%0ASCFMs%20for%20supervised%20fine-tuning%20or%20general%20LLMs%20for%20prompting%2C%20resulting%20in%0Asub-optimal%20performance.%20Inspired%20by%20recent%20progress%20in%20large%20multi-modal%0Amodels%2C%20we%20propose%20that%20it%20is%20possible%20to%20harness%20the%20strengths%20of%20uni-modal%0Acode%20models%20from%20both%20sides%20to%20bridge%20the%20semantic%20gap%20effectively.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20probe-and-recover%20framework%20that%20incorporates%20a%0Abinary-source%20encoder-decoder%20model%20and%20black-box%20LLMs%20for%20binary%20analysis.%20Our%0Aapproach%20leverages%20the%20pre-trained%20knowledge%20within%20SCFMs%20to%20synthesize%0Arelevant%2C%20symbol-rich%20code%20fragments%20as%20context.%20This%20additional%20context%0Aenables%20black-box%20LLMs%20to%20enhance%20recovery%20accuracy.%20We%20demonstrate%20significant%0Aimprovements%20in%20zero-shot%20binary%20summarization%20and%20binary%20function%20name%0Arecovery%2C%20with%20a%2010.3%25%20relative%20gain%20in%20CHRF%20and%20a%2016.7%25%20relative%20gain%20in%20a%0AGPT4-based%20metric%20for%20summarization%2C%20as%20well%20as%20a%206.7%25%20and%207.4%25%20absolute%0Aincrease%20in%20token-level%20precision%20and%20recall%20for%20name%20recovery%2C%20respectively.%0AThese%20results%20highlight%20the%20effectiveness%20of%20our%20approach%20in%20automating%20and%0Aimproving%20binary%20code%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19581v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSource%2520Code%2520Foundation%2520Models%2520are%2520Transferable%2520Binary%2520Analysis%2520Knowledge%250A%2520%2520Bases%26entry.906535625%3DZian%2520Su%2520and%2520Xiangzhe%2520Xu%2520and%2520Ziyang%2520Huang%2520and%2520Kaiyuan%2520Zhang%2520and%2520Xiangyu%2520Zhang%26entry.1292438233%3D%2520%2520Human-Oriented%2520Binary%2520Reverse%2520Engineering%2520%2528HOBRE%2529%2520lies%2520at%2520the%2520intersection%2520of%250Abinary%2520and%2520source%2520code%252C%2520aiming%2520to%2520lift%2520binary%2520code%2520to%2520human-readable%2520content%250Arelevant%2520to%2520source%2520code%252C%2520thereby%2520bridging%2520the%2520binary-source%2520semantic%2520gap.%250ARecent%2520advancements%2520in%2520uni-modal%2520code%2520model%2520pre-training%252C%2520particularly%2520in%250Agenerative%2520Source%2520Code%2520Foundation%2520Models%2520%2528SCFMs%2529%2520and%2520binary%2520understanding%250Amodels%252C%2520have%2520laid%2520the%2520groundwork%2520for%2520transfer%2520learning%2520applicable%2520to%2520HOBRE.%250AHowever%252C%2520existing%2520approaches%2520for%2520HOBRE%2520rely%2520heavily%2520on%2520uni-modal%2520models%2520like%250ASCFMs%2520for%2520supervised%2520fine-tuning%2520or%2520general%2520LLMs%2520for%2520prompting%252C%2520resulting%2520in%250Asub-optimal%2520performance.%2520Inspired%2520by%2520recent%2520progress%2520in%2520large%2520multi-modal%250Amodels%252C%2520we%2520propose%2520that%2520it%2520is%2520possible%2520to%2520harness%2520the%2520strengths%2520of%2520uni-modal%250Acode%2520models%2520from%2520both%2520sides%2520to%2520bridge%2520the%2520semantic%2520gap%2520effectively.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520novel%2520probe-and-recover%2520framework%2520that%2520incorporates%2520a%250Abinary-source%2520encoder-decoder%2520model%2520and%2520black-box%2520LLMs%2520for%2520binary%2520analysis.%2520Our%250Aapproach%2520leverages%2520the%2520pre-trained%2520knowledge%2520within%2520SCFMs%2520to%2520synthesize%250Arelevant%252C%2520symbol-rich%2520code%2520fragments%2520as%2520context.%2520This%2520additional%2520context%250Aenables%2520black-box%2520LLMs%2520to%2520enhance%2520recovery%2520accuracy.%2520We%2520demonstrate%2520significant%250Aimprovements%2520in%2520zero-shot%2520binary%2520summarization%2520and%2520binary%2520function%2520name%250Arecovery%252C%2520with%2520a%252010.3%2525%2520relative%2520gain%2520in%2520CHRF%2520and%2520a%252016.7%2525%2520relative%2520gain%2520in%2520a%250AGPT4-based%2520metric%2520for%2520summarization%252C%2520as%2520well%2520as%2520a%25206.7%2525%2520and%25207.4%2525%2520absolute%250Aincrease%2520in%2520token-level%2520precision%2520and%2520recall%2520for%2520name%2520recovery%252C%2520respectively.%250AThese%2520results%2520highlight%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520automating%2520and%250Aimproving%2520binary%2520code%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19581v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Source%20Code%20Foundation%20Models%20are%20Transferable%20Binary%20Analysis%20Knowledge%0A%20%20Bases&entry.906535625=Zian%20Su%20and%20Xiangzhe%20Xu%20and%20Ziyang%20Huang%20and%20Kaiyuan%20Zhang%20and%20Xiangyu%20Zhang&entry.1292438233=%20%20Human-Oriented%20Binary%20Reverse%20Engineering%20%28HOBRE%29%20lies%20at%20the%20intersection%20of%0Abinary%20and%20source%20code%2C%20aiming%20to%20lift%20binary%20code%20to%20human-readable%20content%0Arelevant%20to%20source%20code%2C%20thereby%20bridging%20the%20binary-source%20semantic%20gap.%0ARecent%20advancements%20in%20uni-modal%20code%20model%20pre-training%2C%20particularly%20in%0Agenerative%20Source%20Code%20Foundation%20Models%20%28SCFMs%29%20and%20binary%20understanding%0Amodels%2C%20have%20laid%20the%20groundwork%20for%20transfer%20learning%20applicable%20to%20HOBRE.%0AHowever%2C%20existing%20approaches%20for%20HOBRE%20rely%20heavily%20on%20uni-modal%20models%20like%0ASCFMs%20for%20supervised%20fine-tuning%20or%20general%20LLMs%20for%20prompting%2C%20resulting%20in%0Asub-optimal%20performance.%20Inspired%20by%20recent%20progress%20in%20large%20multi-modal%0Amodels%2C%20we%20propose%20that%20it%20is%20possible%20to%20harness%20the%20strengths%20of%20uni-modal%0Acode%20models%20from%20both%20sides%20to%20bridge%20the%20semantic%20gap%20effectively.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20probe-and-recover%20framework%20that%20incorporates%20a%0Abinary-source%20encoder-decoder%20model%20and%20black-box%20LLMs%20for%20binary%20analysis.%20Our%0Aapproach%20leverages%20the%20pre-trained%20knowledge%20within%20SCFMs%20to%20synthesize%0Arelevant%2C%20symbol-rich%20code%20fragments%20as%20context.%20This%20additional%20context%0Aenables%20black-box%20LLMs%20to%20enhance%20recovery%20accuracy.%20We%20demonstrate%20significant%0Aimprovements%20in%20zero-shot%20binary%20summarization%20and%20binary%20function%20name%0Arecovery%2C%20with%20a%2010.3%25%20relative%20gain%20in%20CHRF%20and%20a%2016.7%25%20relative%20gain%20in%20a%0AGPT4-based%20metric%20for%20summarization%2C%20as%20well%20as%20a%206.7%25%20and%207.4%25%20absolute%0Aincrease%20in%20token-level%20precision%20and%20recall%20for%20name%20recovery%2C%20respectively.%0AThese%20results%20highlight%20the%20effectiveness%20of%20our%20approach%20in%20automating%20and%0Aimproving%20binary%20code%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19581v2&entry.124074799=Read"},
{"title": "bit2bit: 1-bit quanta video reconstruction via self-supervised photon\n  prediction", "author": "Yehe Liu and Alexander Krull and Hector Basevi and Ales Leonardis and Michael W. Jenkins", "abstract": "  Quanta image sensors, such as SPAD arrays, are an emerging sensor technology,\nproducing 1-bit arrays representing photon detection events over exposures as\nshort as a few nanoseconds. In practice, raw data are post-processed using\nheavy spatiotemporal binning to create more useful and interpretable images at\nthe cost of degrading spatiotemporal resolution. In this work, we propose\nbit2bit, a new method for reconstructing high-quality image stacks at the\noriginal spatiotemporal resolution from sparse binary quanta image data.\nInspired by recent work on Poisson denoising, we developed an algorithm that\ncreates a dense image sequence from sparse binary photon data by predicting the\nphoton arrival location probability distribution. However, due to the binary\nnature of the data, we show that the assumption of a Poisson distribution is\ninadequate. Instead, we model the process with a Bernoulli lattice process from\nthe truncated Poisson. This leads to the proposal of a novel self-supervised\nsolution based on a masked loss function. We evaluate our method using both\nsimulated and real data. On simulated data from a conventional video, we\nachieve 34.35 mean PSNR with extremely photon-sparse binary input (<0.06\nphotons per pixel per frame). We also present a novel dataset containing a wide\nrange of real SPAD high-speed videos under various challenging imaging\nconditions. The scenes cover strong/weak ambient light, strong motion,\nultra-fast events, etc., which will be made available to the community, on\nwhich we demonstrate the promise of our approach. Both reconstruction quality\nand throughput substantially surpass the state-of-the-art methods (e.g., Quanta\nBurst Photography (QBP)). Our approach significantly enhances the visualization\nand usability of the data, enabling the application of existing analysis\ntechniques.\n", "link": "http://arxiv.org/abs/2410.23247v1", "date": "2024-10-30", "relevancy": 2.2066, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5567}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5556}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20bit2bit%3A%201-bit%20quanta%20video%20reconstruction%20via%20self-supervised%20photon%0A%20%20prediction&body=Title%3A%20bit2bit%3A%201-bit%20quanta%20video%20reconstruction%20via%20self-supervised%20photon%0A%20%20prediction%0AAuthor%3A%20Yehe%20Liu%20and%20Alexander%20Krull%20and%20Hector%20Basevi%20and%20Ales%20Leonardis%20and%20Michael%20W.%20Jenkins%0AAbstract%3A%20%20%20Quanta%20image%20sensors%2C%20such%20as%20SPAD%20arrays%2C%20are%20an%20emerging%20sensor%20technology%2C%0Aproducing%201-bit%20arrays%20representing%20photon%20detection%20events%20over%20exposures%20as%0Ashort%20as%20a%20few%20nanoseconds.%20In%20practice%2C%20raw%20data%20are%20post-processed%20using%0Aheavy%20spatiotemporal%20binning%20to%20create%20more%20useful%20and%20interpretable%20images%20at%0Athe%20cost%20of%20degrading%20spatiotemporal%20resolution.%20In%20this%20work%2C%20we%20propose%0Abit2bit%2C%20a%20new%20method%20for%20reconstructing%20high-quality%20image%20stacks%20at%20the%0Aoriginal%20spatiotemporal%20resolution%20from%20sparse%20binary%20quanta%20image%20data.%0AInspired%20by%20recent%20work%20on%20Poisson%20denoising%2C%20we%20developed%20an%20algorithm%20that%0Acreates%20a%20dense%20image%20sequence%20from%20sparse%20binary%20photon%20data%20by%20predicting%20the%0Aphoton%20arrival%20location%20probability%20distribution.%20However%2C%20due%20to%20the%20binary%0Anature%20of%20the%20data%2C%20we%20show%20that%20the%20assumption%20of%20a%20Poisson%20distribution%20is%0Ainadequate.%20Instead%2C%20we%20model%20the%20process%20with%20a%20Bernoulli%20lattice%20process%20from%0Athe%20truncated%20Poisson.%20This%20leads%20to%20the%20proposal%20of%20a%20novel%20self-supervised%0Asolution%20based%20on%20a%20masked%20loss%20function.%20We%20evaluate%20our%20method%20using%20both%0Asimulated%20and%20real%20data.%20On%20simulated%20data%20from%20a%20conventional%20video%2C%20we%0Aachieve%2034.35%20mean%20PSNR%20with%20extremely%20photon-sparse%20binary%20input%20%28%3C0.06%0Aphotons%20per%20pixel%20per%20frame%29.%20We%20also%20present%20a%20novel%20dataset%20containing%20a%20wide%0Arange%20of%20real%20SPAD%20high-speed%20videos%20under%20various%20challenging%20imaging%0Aconditions.%20The%20scenes%20cover%20strong/weak%20ambient%20light%2C%20strong%20motion%2C%0Aultra-fast%20events%2C%20etc.%2C%20which%20will%20be%20made%20available%20to%20the%20community%2C%20on%0Awhich%20we%20demonstrate%20the%20promise%20of%20our%20approach.%20Both%20reconstruction%20quality%0Aand%20throughput%20substantially%20surpass%20the%20state-of-the-art%20methods%20%28e.g.%2C%20Quanta%0ABurst%20Photography%20%28QBP%29%29.%20Our%20approach%20significantly%20enhances%20the%20visualization%0Aand%20usability%20of%20the%20data%2C%20enabling%20the%20application%20of%20existing%20analysis%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23247v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dbit2bit%253A%25201-bit%2520quanta%2520video%2520reconstruction%2520via%2520self-supervised%2520photon%250A%2520%2520prediction%26entry.906535625%3DYehe%2520Liu%2520and%2520Alexander%2520Krull%2520and%2520Hector%2520Basevi%2520and%2520Ales%2520Leonardis%2520and%2520Michael%2520W.%2520Jenkins%26entry.1292438233%3D%2520%2520Quanta%2520image%2520sensors%252C%2520such%2520as%2520SPAD%2520arrays%252C%2520are%2520an%2520emerging%2520sensor%2520technology%252C%250Aproducing%25201-bit%2520arrays%2520representing%2520photon%2520detection%2520events%2520over%2520exposures%2520as%250Ashort%2520as%2520a%2520few%2520nanoseconds.%2520In%2520practice%252C%2520raw%2520data%2520are%2520post-processed%2520using%250Aheavy%2520spatiotemporal%2520binning%2520to%2520create%2520more%2520useful%2520and%2520interpretable%2520images%2520at%250Athe%2520cost%2520of%2520degrading%2520spatiotemporal%2520resolution.%2520In%2520this%2520work%252C%2520we%2520propose%250Abit2bit%252C%2520a%2520new%2520method%2520for%2520reconstructing%2520high-quality%2520image%2520stacks%2520at%2520the%250Aoriginal%2520spatiotemporal%2520resolution%2520from%2520sparse%2520binary%2520quanta%2520image%2520data.%250AInspired%2520by%2520recent%2520work%2520on%2520Poisson%2520denoising%252C%2520we%2520developed%2520an%2520algorithm%2520that%250Acreates%2520a%2520dense%2520image%2520sequence%2520from%2520sparse%2520binary%2520photon%2520data%2520by%2520predicting%2520the%250Aphoton%2520arrival%2520location%2520probability%2520distribution.%2520However%252C%2520due%2520to%2520the%2520binary%250Anature%2520of%2520the%2520data%252C%2520we%2520show%2520that%2520the%2520assumption%2520of%2520a%2520Poisson%2520distribution%2520is%250Ainadequate.%2520Instead%252C%2520we%2520model%2520the%2520process%2520with%2520a%2520Bernoulli%2520lattice%2520process%2520from%250Athe%2520truncated%2520Poisson.%2520This%2520leads%2520to%2520the%2520proposal%2520of%2520a%2520novel%2520self-supervised%250Asolution%2520based%2520on%2520a%2520masked%2520loss%2520function.%2520We%2520evaluate%2520our%2520method%2520using%2520both%250Asimulated%2520and%2520real%2520data.%2520On%2520simulated%2520data%2520from%2520a%2520conventional%2520video%252C%2520we%250Aachieve%252034.35%2520mean%2520PSNR%2520with%2520extremely%2520photon-sparse%2520binary%2520input%2520%2528%253C0.06%250Aphotons%2520per%2520pixel%2520per%2520frame%2529.%2520We%2520also%2520present%2520a%2520novel%2520dataset%2520containing%2520a%2520wide%250Arange%2520of%2520real%2520SPAD%2520high-speed%2520videos%2520under%2520various%2520challenging%2520imaging%250Aconditions.%2520The%2520scenes%2520cover%2520strong/weak%2520ambient%2520light%252C%2520strong%2520motion%252C%250Aultra-fast%2520events%252C%2520etc.%252C%2520which%2520will%2520be%2520made%2520available%2520to%2520the%2520community%252C%2520on%250Awhich%2520we%2520demonstrate%2520the%2520promise%2520of%2520our%2520approach.%2520Both%2520reconstruction%2520quality%250Aand%2520throughput%2520substantially%2520surpass%2520the%2520state-of-the-art%2520methods%2520%2528e.g.%252C%2520Quanta%250ABurst%2520Photography%2520%2528QBP%2529%2529.%2520Our%2520approach%2520significantly%2520enhances%2520the%2520visualization%250Aand%2520usability%2520of%2520the%2520data%252C%2520enabling%2520the%2520application%2520of%2520existing%2520analysis%250Atechniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23247v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=bit2bit%3A%201-bit%20quanta%20video%20reconstruction%20via%20self-supervised%20photon%0A%20%20prediction&entry.906535625=Yehe%20Liu%20and%20Alexander%20Krull%20and%20Hector%20Basevi%20and%20Ales%20Leonardis%20and%20Michael%20W.%20Jenkins&entry.1292438233=%20%20Quanta%20image%20sensors%2C%20such%20as%20SPAD%20arrays%2C%20are%20an%20emerging%20sensor%20technology%2C%0Aproducing%201-bit%20arrays%20representing%20photon%20detection%20events%20over%20exposures%20as%0Ashort%20as%20a%20few%20nanoseconds.%20In%20practice%2C%20raw%20data%20are%20post-processed%20using%0Aheavy%20spatiotemporal%20binning%20to%20create%20more%20useful%20and%20interpretable%20images%20at%0Athe%20cost%20of%20degrading%20spatiotemporal%20resolution.%20In%20this%20work%2C%20we%20propose%0Abit2bit%2C%20a%20new%20method%20for%20reconstructing%20high-quality%20image%20stacks%20at%20the%0Aoriginal%20spatiotemporal%20resolution%20from%20sparse%20binary%20quanta%20image%20data.%0AInspired%20by%20recent%20work%20on%20Poisson%20denoising%2C%20we%20developed%20an%20algorithm%20that%0Acreates%20a%20dense%20image%20sequence%20from%20sparse%20binary%20photon%20data%20by%20predicting%20the%0Aphoton%20arrival%20location%20probability%20distribution.%20However%2C%20due%20to%20the%20binary%0Anature%20of%20the%20data%2C%20we%20show%20that%20the%20assumption%20of%20a%20Poisson%20distribution%20is%0Ainadequate.%20Instead%2C%20we%20model%20the%20process%20with%20a%20Bernoulli%20lattice%20process%20from%0Athe%20truncated%20Poisson.%20This%20leads%20to%20the%20proposal%20of%20a%20novel%20self-supervised%0Asolution%20based%20on%20a%20masked%20loss%20function.%20We%20evaluate%20our%20method%20using%20both%0Asimulated%20and%20real%20data.%20On%20simulated%20data%20from%20a%20conventional%20video%2C%20we%0Aachieve%2034.35%20mean%20PSNR%20with%20extremely%20photon-sparse%20binary%20input%20%28%3C0.06%0Aphotons%20per%20pixel%20per%20frame%29.%20We%20also%20present%20a%20novel%20dataset%20containing%20a%20wide%0Arange%20of%20real%20SPAD%20high-speed%20videos%20under%20various%20challenging%20imaging%0Aconditions.%20The%20scenes%20cover%20strong/weak%20ambient%20light%2C%20strong%20motion%2C%0Aultra-fast%20events%2C%20etc.%2C%20which%20will%20be%20made%20available%20to%20the%20community%2C%20on%0Awhich%20we%20demonstrate%20the%20promise%20of%20our%20approach.%20Both%20reconstruction%20quality%0Aand%20throughput%20substantially%20surpass%20the%20state-of-the-art%20methods%20%28e.g.%2C%20Quanta%0ABurst%20Photography%20%28QBP%29%29.%20Our%20approach%20significantly%20enhances%20the%20visualization%0Aand%20usability%20of%20the%20data%2C%20enabling%20the%20application%20of%20existing%20analysis%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23247v1&entry.124074799=Read"},
{"title": "OS-ATLAS: A Foundation Action Model for Generalist GUI Agents", "author": "Zhiyong Wu and Zhenyu Wu and Fangzhi Xu and Yian Wang and Qiushi Sun and Chengyou Jia and Kanzhi Cheng and Zichen Ding and Liheng Chen and Paul Pu Liang and Yu Qiao", "abstract": "  Existing efforts in building GUI agents heavily rely on the availability of\nrobust commercial Vision-Language Models (VLMs) such as GPT-4o and\nGeminiProVision. Practitioners are often reluctant to use open-source VLMs due\nto their significant performance lag compared to their closed-source\ncounterparts, particularly in GUI grounding and Out-Of-Distribution (OOD)\nscenarios. To facilitate future research in this area, we developed OS-Atlas -\na foundational GUI action model that excels at GUI grounding and OOD agentic\ntasks through innovations in both data and modeling. We have invested\nsignificant engineering effort in developing an open-source toolkit for\nsynthesizing GUI grounding data across multiple platforms, including Windows,\nLinux, MacOS, Android, and the web. Leveraging this toolkit, we are releasing\nthe largest open-source cross-platform GUI grounding corpus to date, which\ncontains over 13 million GUI elements. This dataset, combined with innovations\nin model training, provides a solid foundation for OS-Atlas to understand GUI\nscreenshots and generalize to unseen interfaces. Through extensive evaluation\nacross six benchmarks spanning three different platforms (mobile, desktop, and\nweb), OS-Atlas demonstrates significant performance improvements over previous\nstate-of-the-art models. Our evaluation also uncovers valuable insights into\ncontinuously improving and scaling the agentic capabilities of open-source\nVLMs.\n", "link": "http://arxiv.org/abs/2410.23218v1", "date": "2024-10-30", "relevancy": 2.1858, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5751}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5569}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OS-ATLAS%3A%20A%20Foundation%20Action%20Model%20for%20Generalist%20GUI%20Agents&body=Title%3A%20OS-ATLAS%3A%20A%20Foundation%20Action%20Model%20for%20Generalist%20GUI%20Agents%0AAuthor%3A%20Zhiyong%20Wu%20and%20Zhenyu%20Wu%20and%20Fangzhi%20Xu%20and%20Yian%20Wang%20and%20Qiushi%20Sun%20and%20Chengyou%20Jia%20and%20Kanzhi%20Cheng%20and%20Zichen%20Ding%20and%20Liheng%20Chen%20and%20Paul%20Pu%20Liang%20and%20Yu%20Qiao%0AAbstract%3A%20%20%20Existing%20efforts%20in%20building%20GUI%20agents%20heavily%20rely%20on%20the%20availability%20of%0Arobust%20commercial%20Vision-Language%20Models%20%28VLMs%29%20such%20as%20GPT-4o%20and%0AGeminiProVision.%20Practitioners%20are%20often%20reluctant%20to%20use%20open-source%20VLMs%20due%0Ato%20their%20significant%20performance%20lag%20compared%20to%20their%20closed-source%0Acounterparts%2C%20particularly%20in%20GUI%20grounding%20and%20Out-Of-Distribution%20%28OOD%29%0Ascenarios.%20To%20facilitate%20future%20research%20in%20this%20area%2C%20we%20developed%20OS-Atlas%20-%0Aa%20foundational%20GUI%20action%20model%20that%20excels%20at%20GUI%20grounding%20and%20OOD%20agentic%0Atasks%20through%20innovations%20in%20both%20data%20and%20modeling.%20We%20have%20invested%0Asignificant%20engineering%20effort%20in%20developing%20an%20open-source%20toolkit%20for%0Asynthesizing%20GUI%20grounding%20data%20across%20multiple%20platforms%2C%20including%20Windows%2C%0ALinux%2C%20MacOS%2C%20Android%2C%20and%20the%20web.%20Leveraging%20this%20toolkit%2C%20we%20are%20releasing%0Athe%20largest%20open-source%20cross-platform%20GUI%20grounding%20corpus%20to%20date%2C%20which%0Acontains%20over%2013%20million%20GUI%20elements.%20This%20dataset%2C%20combined%20with%20innovations%0Ain%20model%20training%2C%20provides%20a%20solid%20foundation%20for%20OS-Atlas%20to%20understand%20GUI%0Ascreenshots%20and%20generalize%20to%20unseen%20interfaces.%20Through%20extensive%20evaluation%0Aacross%20six%20benchmarks%20spanning%20three%20different%20platforms%20%28mobile%2C%20desktop%2C%20and%0Aweb%29%2C%20OS-Atlas%20demonstrates%20significant%20performance%20improvements%20over%20previous%0Astate-of-the-art%20models.%20Our%20evaluation%20also%20uncovers%20valuable%20insights%20into%0Acontinuously%20improving%20and%20scaling%20the%20agentic%20capabilities%20of%20open-source%0AVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOS-ATLAS%253A%2520A%2520Foundation%2520Action%2520Model%2520for%2520Generalist%2520GUI%2520Agents%26entry.906535625%3DZhiyong%2520Wu%2520and%2520Zhenyu%2520Wu%2520and%2520Fangzhi%2520Xu%2520and%2520Yian%2520Wang%2520and%2520Qiushi%2520Sun%2520and%2520Chengyou%2520Jia%2520and%2520Kanzhi%2520Cheng%2520and%2520Zichen%2520Ding%2520and%2520Liheng%2520Chen%2520and%2520Paul%2520Pu%2520Liang%2520and%2520Yu%2520Qiao%26entry.1292438233%3D%2520%2520Existing%2520efforts%2520in%2520building%2520GUI%2520agents%2520heavily%2520rely%2520on%2520the%2520availability%2520of%250Arobust%2520commercial%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520such%2520as%2520GPT-4o%2520and%250AGeminiProVision.%2520Practitioners%2520are%2520often%2520reluctant%2520to%2520use%2520open-source%2520VLMs%2520due%250Ato%2520their%2520significant%2520performance%2520lag%2520compared%2520to%2520their%2520closed-source%250Acounterparts%252C%2520particularly%2520in%2520GUI%2520grounding%2520and%2520Out-Of-Distribution%2520%2528OOD%2529%250Ascenarios.%2520To%2520facilitate%2520future%2520research%2520in%2520this%2520area%252C%2520we%2520developed%2520OS-Atlas%2520-%250Aa%2520foundational%2520GUI%2520action%2520model%2520that%2520excels%2520at%2520GUI%2520grounding%2520and%2520OOD%2520agentic%250Atasks%2520through%2520innovations%2520in%2520both%2520data%2520and%2520modeling.%2520We%2520have%2520invested%250Asignificant%2520engineering%2520effort%2520in%2520developing%2520an%2520open-source%2520toolkit%2520for%250Asynthesizing%2520GUI%2520grounding%2520data%2520across%2520multiple%2520platforms%252C%2520including%2520Windows%252C%250ALinux%252C%2520MacOS%252C%2520Android%252C%2520and%2520the%2520web.%2520Leveraging%2520this%2520toolkit%252C%2520we%2520are%2520releasing%250Athe%2520largest%2520open-source%2520cross-platform%2520GUI%2520grounding%2520corpus%2520to%2520date%252C%2520which%250Acontains%2520over%252013%2520million%2520GUI%2520elements.%2520This%2520dataset%252C%2520combined%2520with%2520innovations%250Ain%2520model%2520training%252C%2520provides%2520a%2520solid%2520foundation%2520for%2520OS-Atlas%2520to%2520understand%2520GUI%250Ascreenshots%2520and%2520generalize%2520to%2520unseen%2520interfaces.%2520Through%2520extensive%2520evaluation%250Aacross%2520six%2520benchmarks%2520spanning%2520three%2520different%2520platforms%2520%2528mobile%252C%2520desktop%252C%2520and%250Aweb%2529%252C%2520OS-Atlas%2520demonstrates%2520significant%2520performance%2520improvements%2520over%2520previous%250Astate-of-the-art%2520models.%2520Our%2520evaluation%2520also%2520uncovers%2520valuable%2520insights%2520into%250Acontinuously%2520improving%2520and%2520scaling%2520the%2520agentic%2520capabilities%2520of%2520open-source%250AVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OS-ATLAS%3A%20A%20Foundation%20Action%20Model%20for%20Generalist%20GUI%20Agents&entry.906535625=Zhiyong%20Wu%20and%20Zhenyu%20Wu%20and%20Fangzhi%20Xu%20and%20Yian%20Wang%20and%20Qiushi%20Sun%20and%20Chengyou%20Jia%20and%20Kanzhi%20Cheng%20and%20Zichen%20Ding%20and%20Liheng%20Chen%20and%20Paul%20Pu%20Liang%20and%20Yu%20Qiao&entry.1292438233=%20%20Existing%20efforts%20in%20building%20GUI%20agents%20heavily%20rely%20on%20the%20availability%20of%0Arobust%20commercial%20Vision-Language%20Models%20%28VLMs%29%20such%20as%20GPT-4o%20and%0AGeminiProVision.%20Practitioners%20are%20often%20reluctant%20to%20use%20open-source%20VLMs%20due%0Ato%20their%20significant%20performance%20lag%20compared%20to%20their%20closed-source%0Acounterparts%2C%20particularly%20in%20GUI%20grounding%20and%20Out-Of-Distribution%20%28OOD%29%0Ascenarios.%20To%20facilitate%20future%20research%20in%20this%20area%2C%20we%20developed%20OS-Atlas%20-%0Aa%20foundational%20GUI%20action%20model%20that%20excels%20at%20GUI%20grounding%20and%20OOD%20agentic%0Atasks%20through%20innovations%20in%20both%20data%20and%20modeling.%20We%20have%20invested%0Asignificant%20engineering%20effort%20in%20developing%20an%20open-source%20toolkit%20for%0Asynthesizing%20GUI%20grounding%20data%20across%20multiple%20platforms%2C%20including%20Windows%2C%0ALinux%2C%20MacOS%2C%20Android%2C%20and%20the%20web.%20Leveraging%20this%20toolkit%2C%20we%20are%20releasing%0Athe%20largest%20open-source%20cross-platform%20GUI%20grounding%20corpus%20to%20date%2C%20which%0Acontains%20over%2013%20million%20GUI%20elements.%20This%20dataset%2C%20combined%20with%20innovations%0Ain%20model%20training%2C%20provides%20a%20solid%20foundation%20for%20OS-Atlas%20to%20understand%20GUI%0Ascreenshots%20and%20generalize%20to%20unseen%20interfaces.%20Through%20extensive%20evaluation%0Aacross%20six%20benchmarks%20spanning%20three%20different%20platforms%20%28mobile%2C%20desktop%2C%20and%0Aweb%29%2C%20OS-Atlas%20demonstrates%20significant%20performance%20improvements%20over%20previous%0Astate-of-the-art%20models.%20Our%20evaluation%20also%20uncovers%20valuable%20insights%20into%0Acontinuously%20improving%20and%20scaling%20the%20agentic%20capabilities%20of%20open-source%0AVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23218v1&entry.124074799=Read"},
{"title": "Lorentz-Equivariant Geometric Algebra Transformers for High-Energy\n  Physics", "author": "Jonas Spinner and Victor Bres\u00f3 and Pim de Haan and Tilman Plehn and Jesse Thaler and Johann Brehmer", "abstract": "  Extracting scientific understanding from particle-physics experiments\nrequires solving diverse learning problems with high precision and good data\nefficiency. We propose the Lorentz Geometric Algebra Transformer (L-GATr), a\nnew multi-purpose architecture for high-energy physics. L-GATr represents\nhigh-energy data in a geometric algebra over four-dimensional space-time and is\nequivariant under Lorentz transformations, the symmetry group of relativistic\nkinematics. At the same time, the architecture is a Transformer, which makes it\nversatile and scalable to large systems. L-GATr is first demonstrated on\nregression and classification tasks from particle physics. We then construct\nthe first Lorentz-equivariant generative model: a continuous normalizing flow\nbased on an L-GATr network, trained with Riemannian flow matching. Across our\nexperiments, L-GATr is on par with or outperforms strong domain-specific\nbaselines.\n", "link": "http://arxiv.org/abs/2405.14806v3", "date": "2024-10-30", "relevancy": 2.1748, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5851}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5439}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lorentz-Equivariant%20Geometric%20Algebra%20Transformers%20for%20High-Energy%0A%20%20Physics&body=Title%3A%20Lorentz-Equivariant%20Geometric%20Algebra%20Transformers%20for%20High-Energy%0A%20%20Physics%0AAuthor%3A%20Jonas%20Spinner%20and%20Victor%20Bres%C3%B3%20and%20Pim%20de%20Haan%20and%20Tilman%20Plehn%20and%20Jesse%20Thaler%20and%20Johann%20Brehmer%0AAbstract%3A%20%20%20Extracting%20scientific%20understanding%20from%20particle-physics%20experiments%0Arequires%20solving%20diverse%20learning%20problems%20with%20high%20precision%20and%20good%20data%0Aefficiency.%20We%20propose%20the%20Lorentz%20Geometric%20Algebra%20Transformer%20%28L-GATr%29%2C%20a%0Anew%20multi-purpose%20architecture%20for%20high-energy%20physics.%20L-GATr%20represents%0Ahigh-energy%20data%20in%20a%20geometric%20algebra%20over%20four-dimensional%20space-time%20and%20is%0Aequivariant%20under%20Lorentz%20transformations%2C%20the%20symmetry%20group%20of%20relativistic%0Akinematics.%20At%20the%20same%20time%2C%20the%20architecture%20is%20a%20Transformer%2C%20which%20makes%20it%0Aversatile%20and%20scalable%20to%20large%20systems.%20L-GATr%20is%20first%20demonstrated%20on%0Aregression%20and%20classification%20tasks%20from%20particle%20physics.%20We%20then%20construct%0Athe%20first%20Lorentz-equivariant%20generative%20model%3A%20a%20continuous%20normalizing%20flow%0Abased%20on%20an%20L-GATr%20network%2C%20trained%20with%20Riemannian%20flow%20matching.%20Across%20our%0Aexperiments%2C%20L-GATr%20is%20on%20par%20with%20or%20outperforms%20strong%20domain-specific%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14806v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLorentz-Equivariant%2520Geometric%2520Algebra%2520Transformers%2520for%2520High-Energy%250A%2520%2520Physics%26entry.906535625%3DJonas%2520Spinner%2520and%2520Victor%2520Bres%25C3%25B3%2520and%2520Pim%2520de%2520Haan%2520and%2520Tilman%2520Plehn%2520and%2520Jesse%2520Thaler%2520and%2520Johann%2520Brehmer%26entry.1292438233%3D%2520%2520Extracting%2520scientific%2520understanding%2520from%2520particle-physics%2520experiments%250Arequires%2520solving%2520diverse%2520learning%2520problems%2520with%2520high%2520precision%2520and%2520good%2520data%250Aefficiency.%2520We%2520propose%2520the%2520Lorentz%2520Geometric%2520Algebra%2520Transformer%2520%2528L-GATr%2529%252C%2520a%250Anew%2520multi-purpose%2520architecture%2520for%2520high-energy%2520physics.%2520L-GATr%2520represents%250Ahigh-energy%2520data%2520in%2520a%2520geometric%2520algebra%2520over%2520four-dimensional%2520space-time%2520and%2520is%250Aequivariant%2520under%2520Lorentz%2520transformations%252C%2520the%2520symmetry%2520group%2520of%2520relativistic%250Akinematics.%2520At%2520the%2520same%2520time%252C%2520the%2520architecture%2520is%2520a%2520Transformer%252C%2520which%2520makes%2520it%250Aversatile%2520and%2520scalable%2520to%2520large%2520systems.%2520L-GATr%2520is%2520first%2520demonstrated%2520on%250Aregression%2520and%2520classification%2520tasks%2520from%2520particle%2520physics.%2520We%2520then%2520construct%250Athe%2520first%2520Lorentz-equivariant%2520generative%2520model%253A%2520a%2520continuous%2520normalizing%2520flow%250Abased%2520on%2520an%2520L-GATr%2520network%252C%2520trained%2520with%2520Riemannian%2520flow%2520matching.%2520Across%2520our%250Aexperiments%252C%2520L-GATr%2520is%2520on%2520par%2520with%2520or%2520outperforms%2520strong%2520domain-specific%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14806v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lorentz-Equivariant%20Geometric%20Algebra%20Transformers%20for%20High-Energy%0A%20%20Physics&entry.906535625=Jonas%20Spinner%20and%20Victor%20Bres%C3%B3%20and%20Pim%20de%20Haan%20and%20Tilman%20Plehn%20and%20Jesse%20Thaler%20and%20Johann%20Brehmer&entry.1292438233=%20%20Extracting%20scientific%20understanding%20from%20particle-physics%20experiments%0Arequires%20solving%20diverse%20learning%20problems%20with%20high%20precision%20and%20good%20data%0Aefficiency.%20We%20propose%20the%20Lorentz%20Geometric%20Algebra%20Transformer%20%28L-GATr%29%2C%20a%0Anew%20multi-purpose%20architecture%20for%20high-energy%20physics.%20L-GATr%20represents%0Ahigh-energy%20data%20in%20a%20geometric%20algebra%20over%20four-dimensional%20space-time%20and%20is%0Aequivariant%20under%20Lorentz%20transformations%2C%20the%20symmetry%20group%20of%20relativistic%0Akinematics.%20At%20the%20same%20time%2C%20the%20architecture%20is%20a%20Transformer%2C%20which%20makes%20it%0Aversatile%20and%20scalable%20to%20large%20systems.%20L-GATr%20is%20first%20demonstrated%20on%0Aregression%20and%20classification%20tasks%20from%20particle%20physics.%20We%20then%20construct%0Athe%20first%20Lorentz-equivariant%20generative%20model%3A%20a%20continuous%20normalizing%20flow%0Abased%20on%20an%20L-GATr%20network%2C%20trained%20with%20Riemannian%20flow%20matching.%20Across%20our%0Aexperiments%2C%20L-GATr%20is%20on%20par%20with%20or%20outperforms%20strong%20domain-specific%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14806v3&entry.124074799=Read"},
{"title": "QWO: Speeding Up Permutation-Based Causal Discovery in LiGAMs", "author": "Mohammad Shahverdikondori and Ehsan Mokhtarian and Negar Kiyavash", "abstract": "  Causal discovery is essential for understanding relationships among variables\nof interest in many scientific domains. In this paper, we focus on\npermutation-based methods for learning causal graphs in Linear Gaussian Acyclic\nModels (LiGAMs), where the permutation encodes a causal ordering of the\nvariables. Existing methods in this setting are not scalable due to their high\ncomputational complexity. These methods are comprised of two main components:\n(i) constructing a specific DAG, $\\mathcal{G}^\\pi$, for a given permutation\n$\\pi$, which represents the best structure that can be learned from the\navailable data while adhering to $\\pi$, and (ii) searching over the space of\npermutations (i.e., causal orders) to minimize the number of edges in\n$\\mathcal{G}^\\pi$. We introduce QWO, a novel approach that significantly\nenhances the efficiency of computing $\\mathcal{G}^\\pi$ for a given permutation\n$\\pi$. QWO has a speed-up of $O(n^2)$ ($n$ is the number of variables) compared\nto the state-of-the-art BIC-based method, making it highly scalable. We show\nthat our method is theoretically sound and can be integrated into existing\nsearch strategies such as GRASP and hill-climbing-based methods to improve\ntheir performance.\n", "link": "http://arxiv.org/abs/2410.23155v1", "date": "2024-10-30", "relevancy": 2.1375, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4343}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4293}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QWO%3A%20Speeding%20Up%20Permutation-Based%20Causal%20Discovery%20in%20LiGAMs&body=Title%3A%20QWO%3A%20Speeding%20Up%20Permutation-Based%20Causal%20Discovery%20in%20LiGAMs%0AAuthor%3A%20Mohammad%20Shahverdikondori%20and%20Ehsan%20Mokhtarian%20and%20Negar%20Kiyavash%0AAbstract%3A%20%20%20Causal%20discovery%20is%20essential%20for%20understanding%20relationships%20among%20variables%0Aof%20interest%20in%20many%20scientific%20domains.%20In%20this%20paper%2C%20we%20focus%20on%0Apermutation-based%20methods%20for%20learning%20causal%20graphs%20in%20Linear%20Gaussian%20Acyclic%0AModels%20%28LiGAMs%29%2C%20where%20the%20permutation%20encodes%20a%20causal%20ordering%20of%20the%0Avariables.%20Existing%20methods%20in%20this%20setting%20are%20not%20scalable%20due%20to%20their%20high%0Acomputational%20complexity.%20These%20methods%20are%20comprised%20of%20two%20main%20components%3A%0A%28i%29%20constructing%20a%20specific%20DAG%2C%20%24%5Cmathcal%7BG%7D%5E%5Cpi%24%2C%20for%20a%20given%20permutation%0A%24%5Cpi%24%2C%20which%20represents%20the%20best%20structure%20that%20can%20be%20learned%20from%20the%0Aavailable%20data%20while%20adhering%20to%20%24%5Cpi%24%2C%20and%20%28ii%29%20searching%20over%20the%20space%20of%0Apermutations%20%28i.e.%2C%20causal%20orders%29%20to%20minimize%20the%20number%20of%20edges%20in%0A%24%5Cmathcal%7BG%7D%5E%5Cpi%24.%20We%20introduce%20QWO%2C%20a%20novel%20approach%20that%20significantly%0Aenhances%20the%20efficiency%20of%20computing%20%24%5Cmathcal%7BG%7D%5E%5Cpi%24%20for%20a%20given%20permutation%0A%24%5Cpi%24.%20QWO%20has%20a%20speed-up%20of%20%24O%28n%5E2%29%24%20%28%24n%24%20is%20the%20number%20of%20variables%29%20compared%0Ato%20the%20state-of-the-art%20BIC-based%20method%2C%20making%20it%20highly%20scalable.%20We%20show%0Athat%20our%20method%20is%20theoretically%20sound%20and%20can%20be%20integrated%20into%20existing%0Asearch%20strategies%20such%20as%20GRASP%20and%20hill-climbing-based%20methods%20to%20improve%0Atheir%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQWO%253A%2520Speeding%2520Up%2520Permutation-Based%2520Causal%2520Discovery%2520in%2520LiGAMs%26entry.906535625%3DMohammad%2520Shahverdikondori%2520and%2520Ehsan%2520Mokhtarian%2520and%2520Negar%2520Kiyavash%26entry.1292438233%3D%2520%2520Causal%2520discovery%2520is%2520essential%2520for%2520understanding%2520relationships%2520among%2520variables%250Aof%2520interest%2520in%2520many%2520scientific%2520domains.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%250Apermutation-based%2520methods%2520for%2520learning%2520causal%2520graphs%2520in%2520Linear%2520Gaussian%2520Acyclic%250AModels%2520%2528LiGAMs%2529%252C%2520where%2520the%2520permutation%2520encodes%2520a%2520causal%2520ordering%2520of%2520the%250Avariables.%2520Existing%2520methods%2520in%2520this%2520setting%2520are%2520not%2520scalable%2520due%2520to%2520their%2520high%250Acomputational%2520complexity.%2520These%2520methods%2520are%2520comprised%2520of%2520two%2520main%2520components%253A%250A%2528i%2529%2520constructing%2520a%2520specific%2520DAG%252C%2520%2524%255Cmathcal%257BG%257D%255E%255Cpi%2524%252C%2520for%2520a%2520given%2520permutation%250A%2524%255Cpi%2524%252C%2520which%2520represents%2520the%2520best%2520structure%2520that%2520can%2520be%2520learned%2520from%2520the%250Aavailable%2520data%2520while%2520adhering%2520to%2520%2524%255Cpi%2524%252C%2520and%2520%2528ii%2529%2520searching%2520over%2520the%2520space%2520of%250Apermutations%2520%2528i.e.%252C%2520causal%2520orders%2529%2520to%2520minimize%2520the%2520number%2520of%2520edges%2520in%250A%2524%255Cmathcal%257BG%257D%255E%255Cpi%2524.%2520We%2520introduce%2520QWO%252C%2520a%2520novel%2520approach%2520that%2520significantly%250Aenhances%2520the%2520efficiency%2520of%2520computing%2520%2524%255Cmathcal%257BG%257D%255E%255Cpi%2524%2520for%2520a%2520given%2520permutation%250A%2524%255Cpi%2524.%2520QWO%2520has%2520a%2520speed-up%2520of%2520%2524O%2528n%255E2%2529%2524%2520%2528%2524n%2524%2520is%2520the%2520number%2520of%2520variables%2529%2520compared%250Ato%2520the%2520state-of-the-art%2520BIC-based%2520method%252C%2520making%2520it%2520highly%2520scalable.%2520We%2520show%250Athat%2520our%2520method%2520is%2520theoretically%2520sound%2520and%2520can%2520be%2520integrated%2520into%2520existing%250Asearch%2520strategies%2520such%2520as%2520GRASP%2520and%2520hill-climbing-based%2520methods%2520to%2520improve%250Atheir%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QWO%3A%20Speeding%20Up%20Permutation-Based%20Causal%20Discovery%20in%20LiGAMs&entry.906535625=Mohammad%20Shahverdikondori%20and%20Ehsan%20Mokhtarian%20and%20Negar%20Kiyavash&entry.1292438233=%20%20Causal%20discovery%20is%20essential%20for%20understanding%20relationships%20among%20variables%0Aof%20interest%20in%20many%20scientific%20domains.%20In%20this%20paper%2C%20we%20focus%20on%0Apermutation-based%20methods%20for%20learning%20causal%20graphs%20in%20Linear%20Gaussian%20Acyclic%0AModels%20%28LiGAMs%29%2C%20where%20the%20permutation%20encodes%20a%20causal%20ordering%20of%20the%0Avariables.%20Existing%20methods%20in%20this%20setting%20are%20not%20scalable%20due%20to%20their%20high%0Acomputational%20complexity.%20These%20methods%20are%20comprised%20of%20two%20main%20components%3A%0A%28i%29%20constructing%20a%20specific%20DAG%2C%20%24%5Cmathcal%7BG%7D%5E%5Cpi%24%2C%20for%20a%20given%20permutation%0A%24%5Cpi%24%2C%20which%20represents%20the%20best%20structure%20that%20can%20be%20learned%20from%20the%0Aavailable%20data%20while%20adhering%20to%20%24%5Cpi%24%2C%20and%20%28ii%29%20searching%20over%20the%20space%20of%0Apermutations%20%28i.e.%2C%20causal%20orders%29%20to%20minimize%20the%20number%20of%20edges%20in%0A%24%5Cmathcal%7BG%7D%5E%5Cpi%24.%20We%20introduce%20QWO%2C%20a%20novel%20approach%20that%20significantly%0Aenhances%20the%20efficiency%20of%20computing%20%24%5Cmathcal%7BG%7D%5E%5Cpi%24%20for%20a%20given%20permutation%0A%24%5Cpi%24.%20QWO%20has%20a%20speed-up%20of%20%24O%28n%5E2%29%24%20%28%24n%24%20is%20the%20number%20of%20variables%29%20compared%0Ato%20the%20state-of-the-art%20BIC-based%20method%2C%20making%20it%20highly%20scalable.%20We%20show%0Athat%20our%20method%20is%20theoretically%20sound%20and%20can%20be%20integrated%20into%20existing%0Asearch%20strategies%20such%20as%20GRASP%20and%20hill-climbing-based%20methods%20to%20improve%0Atheir%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23155v1&entry.124074799=Read"},
{"title": "CoTran: An LLM-based Code Translator using Reinforcement Learning with\n  Feedback from Compiler and Symbolic Execution", "author": "Prithwish Jana and Piyush Jha and Haoyang Ju and Gautham Kishore and Aryan Mahajan and Vijay Ganesh", "abstract": "  In this paper, we present an LLM-based code translation method and an\nassociated tool called CoTran, that translates whole-programs from one\nhigh-level programming language to another. Existing LLM-based code translation\nmethods lack training to ensure that the translated code reliably compiles or\nbears substantial functional equivalence to the input code. In our work, we\nfine-tune an LLM using reinforcement learning, incorporating compiler feedback,\nand symbolic execution (symexec)-based testing feedback to assess functional\nequivalence between the input and output programs. The idea is to guide an LLM\nduring fine-tuning, via compiler and symexec-based testing feedback, by letting\nit know how far it is from producing perfect translations. We conduct extensive\nexperiments comparing CoTran with 14 other code translation tools, including\nhuman-written transpilers, LLM-based translation tools, and ChatGPT. Using a\nbenchmark of over \\num{57000} code pairs in Java and Python, we demonstrate\nthat CoTran outperforms the other tools on relevant metrics such as compilation\naccuracy (CompAcc) and functional equivalence accuracy (FEqAcc). For example,\nin Python-to-Java translation, CoTran achieves 48.68% FEqAcc and 76.98%\nCompAcc, whereas the nearest competing tool (PLBART-base) gets 38.26% and\n75.77% respectively. Additionally, CoTran, built on top of CodeT5, improves\nFEqAcc by +14.89% and CompAcc by +8.14% for Python-to-Java (resp., +12.94% and\n+4.30% for Java-to-Python).\n", "link": "http://arxiv.org/abs/2306.06755v4", "date": "2024-10-30", "relevancy": 2.1275, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4425}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.417}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoTran%3A%20An%20LLM-based%20Code%20Translator%20using%20Reinforcement%20Learning%20with%0A%20%20Feedback%20from%20Compiler%20and%20Symbolic%20Execution&body=Title%3A%20CoTran%3A%20An%20LLM-based%20Code%20Translator%20using%20Reinforcement%20Learning%20with%0A%20%20Feedback%20from%20Compiler%20and%20Symbolic%20Execution%0AAuthor%3A%20Prithwish%20Jana%20and%20Piyush%20Jha%20and%20Haoyang%20Ju%20and%20Gautham%20Kishore%20and%20Aryan%20Mahajan%20and%20Vijay%20Ganesh%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20an%20LLM-based%20code%20translation%20method%20and%20an%0Aassociated%20tool%20called%20CoTran%2C%20that%20translates%20whole-programs%20from%20one%0Ahigh-level%20programming%20language%20to%20another.%20Existing%20LLM-based%20code%20translation%0Amethods%20lack%20training%20to%20ensure%20that%20the%20translated%20code%20reliably%20compiles%20or%0Abears%20substantial%20functional%20equivalence%20to%20the%20input%20code.%20In%20our%20work%2C%20we%0Afine-tune%20an%20LLM%20using%20reinforcement%20learning%2C%20incorporating%20compiler%20feedback%2C%0Aand%20symbolic%20execution%20%28symexec%29-based%20testing%20feedback%20to%20assess%20functional%0Aequivalence%20between%20the%20input%20and%20output%20programs.%20The%20idea%20is%20to%20guide%20an%20LLM%0Aduring%20fine-tuning%2C%20via%20compiler%20and%20symexec-based%20testing%20feedback%2C%20by%20letting%0Ait%20know%20how%20far%20it%20is%20from%20producing%20perfect%20translations.%20We%20conduct%20extensive%0Aexperiments%20comparing%20CoTran%20with%2014%20other%20code%20translation%20tools%2C%20including%0Ahuman-written%20transpilers%2C%20LLM-based%20translation%20tools%2C%20and%20ChatGPT.%20Using%20a%0Abenchmark%20of%20over%20%5Cnum%7B57000%7D%20code%20pairs%20in%20Java%20and%20Python%2C%20we%20demonstrate%0Athat%20CoTran%20outperforms%20the%20other%20tools%20on%20relevant%20metrics%20such%20as%20compilation%0Aaccuracy%20%28CompAcc%29%20and%20functional%20equivalence%20accuracy%20%28FEqAcc%29.%20For%20example%2C%0Ain%20Python-to-Java%20translation%2C%20CoTran%20achieves%2048.68%25%20FEqAcc%20and%2076.98%25%0ACompAcc%2C%20whereas%20the%20nearest%20competing%20tool%20%28PLBART-base%29%20gets%2038.26%25%20and%0A75.77%25%20respectively.%20Additionally%2C%20CoTran%2C%20built%20on%20top%20of%20CodeT5%2C%20improves%0AFEqAcc%20by%20%2B14.89%25%20and%20CompAcc%20by%20%2B8.14%25%20for%20Python-to-Java%20%28resp.%2C%20%2B12.94%25%20and%0A%2B4.30%25%20for%20Java-to-Python%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.06755v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoTran%253A%2520An%2520LLM-based%2520Code%2520Translator%2520using%2520Reinforcement%2520Learning%2520with%250A%2520%2520Feedback%2520from%2520Compiler%2520and%2520Symbolic%2520Execution%26entry.906535625%3DPrithwish%2520Jana%2520and%2520Piyush%2520Jha%2520and%2520Haoyang%2520Ju%2520and%2520Gautham%2520Kishore%2520and%2520Aryan%2520Mahajan%2520and%2520Vijay%2520Ganesh%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520LLM-based%2520code%2520translation%2520method%2520and%2520an%250Aassociated%2520tool%2520called%2520CoTran%252C%2520that%2520translates%2520whole-programs%2520from%2520one%250Ahigh-level%2520programming%2520language%2520to%2520another.%2520Existing%2520LLM-based%2520code%2520translation%250Amethods%2520lack%2520training%2520to%2520ensure%2520that%2520the%2520translated%2520code%2520reliably%2520compiles%2520or%250Abears%2520substantial%2520functional%2520equivalence%2520to%2520the%2520input%2520code.%2520In%2520our%2520work%252C%2520we%250Afine-tune%2520an%2520LLM%2520using%2520reinforcement%2520learning%252C%2520incorporating%2520compiler%2520feedback%252C%250Aand%2520symbolic%2520execution%2520%2528symexec%2529-based%2520testing%2520feedback%2520to%2520assess%2520functional%250Aequivalence%2520between%2520the%2520input%2520and%2520output%2520programs.%2520The%2520idea%2520is%2520to%2520guide%2520an%2520LLM%250Aduring%2520fine-tuning%252C%2520via%2520compiler%2520and%2520symexec-based%2520testing%2520feedback%252C%2520by%2520letting%250Ait%2520know%2520how%2520far%2520it%2520is%2520from%2520producing%2520perfect%2520translations.%2520We%2520conduct%2520extensive%250Aexperiments%2520comparing%2520CoTran%2520with%252014%2520other%2520code%2520translation%2520tools%252C%2520including%250Ahuman-written%2520transpilers%252C%2520LLM-based%2520translation%2520tools%252C%2520and%2520ChatGPT.%2520Using%2520a%250Abenchmark%2520of%2520over%2520%255Cnum%257B57000%257D%2520code%2520pairs%2520in%2520Java%2520and%2520Python%252C%2520we%2520demonstrate%250Athat%2520CoTran%2520outperforms%2520the%2520other%2520tools%2520on%2520relevant%2520metrics%2520such%2520as%2520compilation%250Aaccuracy%2520%2528CompAcc%2529%2520and%2520functional%2520equivalence%2520accuracy%2520%2528FEqAcc%2529.%2520For%2520example%252C%250Ain%2520Python-to-Java%2520translation%252C%2520CoTran%2520achieves%252048.68%2525%2520FEqAcc%2520and%252076.98%2525%250ACompAcc%252C%2520whereas%2520the%2520nearest%2520competing%2520tool%2520%2528PLBART-base%2529%2520gets%252038.26%2525%2520and%250A75.77%2525%2520respectively.%2520Additionally%252C%2520CoTran%252C%2520built%2520on%2520top%2520of%2520CodeT5%252C%2520improves%250AFEqAcc%2520by%2520%252B14.89%2525%2520and%2520CompAcc%2520by%2520%252B8.14%2525%2520for%2520Python-to-Java%2520%2528resp.%252C%2520%252B12.94%2525%2520and%250A%252B4.30%2525%2520for%2520Java-to-Python%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.06755v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoTran%3A%20An%20LLM-based%20Code%20Translator%20using%20Reinforcement%20Learning%20with%0A%20%20Feedback%20from%20Compiler%20and%20Symbolic%20Execution&entry.906535625=Prithwish%20Jana%20and%20Piyush%20Jha%20and%20Haoyang%20Ju%20and%20Gautham%20Kishore%20and%20Aryan%20Mahajan%20and%20Vijay%20Ganesh&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20an%20LLM-based%20code%20translation%20method%20and%20an%0Aassociated%20tool%20called%20CoTran%2C%20that%20translates%20whole-programs%20from%20one%0Ahigh-level%20programming%20language%20to%20another.%20Existing%20LLM-based%20code%20translation%0Amethods%20lack%20training%20to%20ensure%20that%20the%20translated%20code%20reliably%20compiles%20or%0Abears%20substantial%20functional%20equivalence%20to%20the%20input%20code.%20In%20our%20work%2C%20we%0Afine-tune%20an%20LLM%20using%20reinforcement%20learning%2C%20incorporating%20compiler%20feedback%2C%0Aand%20symbolic%20execution%20%28symexec%29-based%20testing%20feedback%20to%20assess%20functional%0Aequivalence%20between%20the%20input%20and%20output%20programs.%20The%20idea%20is%20to%20guide%20an%20LLM%0Aduring%20fine-tuning%2C%20via%20compiler%20and%20symexec-based%20testing%20feedback%2C%20by%20letting%0Ait%20know%20how%20far%20it%20is%20from%20producing%20perfect%20translations.%20We%20conduct%20extensive%0Aexperiments%20comparing%20CoTran%20with%2014%20other%20code%20translation%20tools%2C%20including%0Ahuman-written%20transpilers%2C%20LLM-based%20translation%20tools%2C%20and%20ChatGPT.%20Using%20a%0Abenchmark%20of%20over%20%5Cnum%7B57000%7D%20code%20pairs%20in%20Java%20and%20Python%2C%20we%20demonstrate%0Athat%20CoTran%20outperforms%20the%20other%20tools%20on%20relevant%20metrics%20such%20as%20compilation%0Aaccuracy%20%28CompAcc%29%20and%20functional%20equivalence%20accuracy%20%28FEqAcc%29.%20For%20example%2C%0Ain%20Python-to-Java%20translation%2C%20CoTran%20achieves%2048.68%25%20FEqAcc%20and%2076.98%25%0ACompAcc%2C%20whereas%20the%20nearest%20competing%20tool%20%28PLBART-base%29%20gets%2038.26%25%20and%0A75.77%25%20respectively.%20Additionally%2C%20CoTran%2C%20built%20on%20top%20of%20CodeT5%2C%20improves%0AFEqAcc%20by%20%2B14.89%25%20and%20CompAcc%20by%20%2B8.14%25%20for%20Python-to-Java%20%28resp.%2C%20%2B12.94%25%20and%0A%2B4.30%25%20for%20Java-to-Python%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.06755v4&entry.124074799=Read"},
{"title": "Super-resolution in disordered media using neural networks", "author": "Alexander Christie and Matan Leibovich and Miguel Moscoso and Alexei Novikov and George Papanicolaou and Chrysoula Tsogka", "abstract": "  We propose a methodology that exploits large and diverse data sets to\naccurately estimate the ambient medium's Green's functions in strongly\nscattering media. Given these estimates, obtained with and without the use of\nneural networks, excellent imaging results are achieved, with a resolution that\nis better than that of a homogeneous medium. This phenomenon, also known as\nsuper-resolution, occurs because the ambient scattering medium effectively\nenhances the physical imaging aperture.\n", "link": "http://arxiv.org/abs/2410.21556v2", "date": "2024-10-30", "relevancy": 2.125, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5423}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5252}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Super-resolution%20in%20disordered%20media%20using%20neural%20networks&body=Title%3A%20Super-resolution%20in%20disordered%20media%20using%20neural%20networks%0AAuthor%3A%20Alexander%20Christie%20and%20Matan%20Leibovich%20and%20Miguel%20Moscoso%20and%20Alexei%20Novikov%20and%20George%20Papanicolaou%20and%20Chrysoula%20Tsogka%0AAbstract%3A%20%20%20We%20propose%20a%20methodology%20that%20exploits%20large%20and%20diverse%20data%20sets%20to%0Aaccurately%20estimate%20the%20ambient%20medium%27s%20Green%27s%20functions%20in%20strongly%0Ascattering%20media.%20Given%20these%20estimates%2C%20obtained%20with%20and%20without%20the%20use%20of%0Aneural%20networks%2C%20excellent%20imaging%20results%20are%20achieved%2C%20with%20a%20resolution%20that%0Ais%20better%20than%20that%20of%20a%20homogeneous%20medium.%20This%20phenomenon%2C%20also%20known%20as%0Asuper-resolution%2C%20occurs%20because%20the%20ambient%20scattering%20medium%20effectively%0Aenhances%20the%20physical%20imaging%20aperture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21556v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuper-resolution%2520in%2520disordered%2520media%2520using%2520neural%2520networks%26entry.906535625%3DAlexander%2520Christie%2520and%2520Matan%2520Leibovich%2520and%2520Miguel%2520Moscoso%2520and%2520Alexei%2520Novikov%2520and%2520George%2520Papanicolaou%2520and%2520Chrysoula%2520Tsogka%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520methodology%2520that%2520exploits%2520large%2520and%2520diverse%2520data%2520sets%2520to%250Aaccurately%2520estimate%2520the%2520ambient%2520medium%2527s%2520Green%2527s%2520functions%2520in%2520strongly%250Ascattering%2520media.%2520Given%2520these%2520estimates%252C%2520obtained%2520with%2520and%2520without%2520the%2520use%2520of%250Aneural%2520networks%252C%2520excellent%2520imaging%2520results%2520are%2520achieved%252C%2520with%2520a%2520resolution%2520that%250Ais%2520better%2520than%2520that%2520of%2520a%2520homogeneous%2520medium.%2520This%2520phenomenon%252C%2520also%2520known%2520as%250Asuper-resolution%252C%2520occurs%2520because%2520the%2520ambient%2520scattering%2520medium%2520effectively%250Aenhances%2520the%2520physical%2520imaging%2520aperture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21556v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Super-resolution%20in%20disordered%20media%20using%20neural%20networks&entry.906535625=Alexander%20Christie%20and%20Matan%20Leibovich%20and%20Miguel%20Moscoso%20and%20Alexei%20Novikov%20and%20George%20Papanicolaou%20and%20Chrysoula%20Tsogka&entry.1292438233=%20%20We%20propose%20a%20methodology%20that%20exploits%20large%20and%20diverse%20data%20sets%20to%0Aaccurately%20estimate%20the%20ambient%20medium%27s%20Green%27s%20functions%20in%20strongly%0Ascattering%20media.%20Given%20these%20estimates%2C%20obtained%20with%20and%20without%20the%20use%20of%0Aneural%20networks%2C%20excellent%20imaging%20results%20are%20achieved%2C%20with%20a%20resolution%20that%0Ais%20better%20than%20that%20of%20a%20homogeneous%20medium.%20This%20phenomenon%2C%20also%20known%20as%0Asuper-resolution%2C%20occurs%20because%20the%20ambient%20scattering%20medium%20effectively%0Aenhances%20the%20physical%20imaging%20aperture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21556v2&entry.124074799=Read"},
{"title": "TokenFormer: Rethinking Transformer Scaling with Tokenized Model\n  Parameters", "author": "Haiyang Wang and Yue Fan and Muhammad Ferjad Naeem and Yongqin Xian and Jan Eric Lenssen and Liwei Wang and Federico Tombari and Bernt Schiele", "abstract": "  Transformers have become the predominant architecture in foundation models\ndue to their excellent performance across various domains. However, the\nsubstantial cost of scaling these models remains a significant concern. This\nproblem arises primarily from their dependence on a fixed number of parameters\nwithin linear projections. When architectural modifications (e.g., channel\ndimensions) are introduced, the entire model typically requires retraining from\nscratch. As model sizes continue growing, this strategy results in increasingly\nhigh computational costs and becomes unsustainable. To overcome this problem,\nwe introduce TokenFormer, a natively scalable architecture that leverages the\nattention mechanism not only for computations among input tokens but also for\ninteractions between tokens and model parameters, thereby enhancing\narchitectural flexibility. By treating model parameters as tokens, we replace\nall the linear projections in Transformers with our token-parameter attention\nlayer, where input tokens act as queries and model parameters as keys and\nvalues. This reformulation allows for progressive and efficient scaling without\nnecessitating retraining from scratch. Our model scales from 124M to 1.4B\nparameters by incrementally adding new key-value parameter pairs, achieving\nperformance comparable to Transformers trained from scratch while greatly\nreducing training costs. Code and models are available at\n\\url{https://github.com/Haiyang-W/TokenFormer}.\n", "link": "http://arxiv.org/abs/2410.23168v1", "date": "2024-10-30", "relevancy": 2.1202, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5991}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.571}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TokenFormer%3A%20Rethinking%20Transformer%20Scaling%20with%20Tokenized%20Model%0A%20%20Parameters&body=Title%3A%20TokenFormer%3A%20Rethinking%20Transformer%20Scaling%20with%20Tokenized%20Model%0A%20%20Parameters%0AAuthor%3A%20Haiyang%20Wang%20and%20Yue%20Fan%20and%20Muhammad%20Ferjad%20Naeem%20and%20Yongqin%20Xian%20and%20Jan%20Eric%20Lenssen%20and%20Liwei%20Wang%20and%20Federico%20Tombari%20and%20Bernt%20Schiele%0AAbstract%3A%20%20%20Transformers%20have%20become%20the%20predominant%20architecture%20in%20foundation%20models%0Adue%20to%20their%20excellent%20performance%20across%20various%20domains.%20However%2C%20the%0Asubstantial%20cost%20of%20scaling%20these%20models%20remains%20a%20significant%20concern.%20This%0Aproblem%20arises%20primarily%20from%20their%20dependence%20on%20a%20fixed%20number%20of%20parameters%0Awithin%20linear%20projections.%20When%20architectural%20modifications%20%28e.g.%2C%20channel%0Adimensions%29%20are%20introduced%2C%20the%20entire%20model%20typically%20requires%20retraining%20from%0Ascratch.%20As%20model%20sizes%20continue%20growing%2C%20this%20strategy%20results%20in%20increasingly%0Ahigh%20computational%20costs%20and%20becomes%20unsustainable.%20To%20overcome%20this%20problem%2C%0Awe%20introduce%20TokenFormer%2C%20a%20natively%20scalable%20architecture%20that%20leverages%20the%0Aattention%20mechanism%20not%20only%20for%20computations%20among%20input%20tokens%20but%20also%20for%0Ainteractions%20between%20tokens%20and%20model%20parameters%2C%20thereby%20enhancing%0Aarchitectural%20flexibility.%20By%20treating%20model%20parameters%20as%20tokens%2C%20we%20replace%0Aall%20the%20linear%20projections%20in%20Transformers%20with%20our%20token-parameter%20attention%0Alayer%2C%20where%20input%20tokens%20act%20as%20queries%20and%20model%20parameters%20as%20keys%20and%0Avalues.%20This%20reformulation%20allows%20for%20progressive%20and%20efficient%20scaling%20without%0Anecessitating%20retraining%20from%20scratch.%20Our%20model%20scales%20from%20124M%20to%201.4B%0Aparameters%20by%20incrementally%20adding%20new%20key-value%20parameter%20pairs%2C%20achieving%0Aperformance%20comparable%20to%20Transformers%20trained%20from%20scratch%20while%20greatly%0Areducing%20training%20costs.%20Code%20and%20models%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Haiyang-W/TokenFormer%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23168v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenFormer%253A%2520Rethinking%2520Transformer%2520Scaling%2520with%2520Tokenized%2520Model%250A%2520%2520Parameters%26entry.906535625%3DHaiyang%2520Wang%2520and%2520Yue%2520Fan%2520and%2520Muhammad%2520Ferjad%2520Naeem%2520and%2520Yongqin%2520Xian%2520and%2520Jan%2520Eric%2520Lenssen%2520and%2520Liwei%2520Wang%2520and%2520Federico%2520Tombari%2520and%2520Bernt%2520Schiele%26entry.1292438233%3D%2520%2520Transformers%2520have%2520become%2520the%2520predominant%2520architecture%2520in%2520foundation%2520models%250Adue%2520to%2520their%2520excellent%2520performance%2520across%2520various%2520domains.%2520However%252C%2520the%250Asubstantial%2520cost%2520of%2520scaling%2520these%2520models%2520remains%2520a%2520significant%2520concern.%2520This%250Aproblem%2520arises%2520primarily%2520from%2520their%2520dependence%2520on%2520a%2520fixed%2520number%2520of%2520parameters%250Awithin%2520linear%2520projections.%2520When%2520architectural%2520modifications%2520%2528e.g.%252C%2520channel%250Adimensions%2529%2520are%2520introduced%252C%2520the%2520entire%2520model%2520typically%2520requires%2520retraining%2520from%250Ascratch.%2520As%2520model%2520sizes%2520continue%2520growing%252C%2520this%2520strategy%2520results%2520in%2520increasingly%250Ahigh%2520computational%2520costs%2520and%2520becomes%2520unsustainable.%2520To%2520overcome%2520this%2520problem%252C%250Awe%2520introduce%2520TokenFormer%252C%2520a%2520natively%2520scalable%2520architecture%2520that%2520leverages%2520the%250Aattention%2520mechanism%2520not%2520only%2520for%2520computations%2520among%2520input%2520tokens%2520but%2520also%2520for%250Ainteractions%2520between%2520tokens%2520and%2520model%2520parameters%252C%2520thereby%2520enhancing%250Aarchitectural%2520flexibility.%2520By%2520treating%2520model%2520parameters%2520as%2520tokens%252C%2520we%2520replace%250Aall%2520the%2520linear%2520projections%2520in%2520Transformers%2520with%2520our%2520token-parameter%2520attention%250Alayer%252C%2520where%2520input%2520tokens%2520act%2520as%2520queries%2520and%2520model%2520parameters%2520as%2520keys%2520and%250Avalues.%2520This%2520reformulation%2520allows%2520for%2520progressive%2520and%2520efficient%2520scaling%2520without%250Anecessitating%2520retraining%2520from%2520scratch.%2520Our%2520model%2520scales%2520from%2520124M%2520to%25201.4B%250Aparameters%2520by%2520incrementally%2520adding%2520new%2520key-value%2520parameter%2520pairs%252C%2520achieving%250Aperformance%2520comparable%2520to%2520Transformers%2520trained%2520from%2520scratch%2520while%2520greatly%250Areducing%2520training%2520costs.%2520Code%2520and%2520models%2520are%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/Haiyang-W/TokenFormer%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23168v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TokenFormer%3A%20Rethinking%20Transformer%20Scaling%20with%20Tokenized%20Model%0A%20%20Parameters&entry.906535625=Haiyang%20Wang%20and%20Yue%20Fan%20and%20Muhammad%20Ferjad%20Naeem%20and%20Yongqin%20Xian%20and%20Jan%20Eric%20Lenssen%20and%20Liwei%20Wang%20and%20Federico%20Tombari%20and%20Bernt%20Schiele&entry.1292438233=%20%20Transformers%20have%20become%20the%20predominant%20architecture%20in%20foundation%20models%0Adue%20to%20their%20excellent%20performance%20across%20various%20domains.%20However%2C%20the%0Asubstantial%20cost%20of%20scaling%20these%20models%20remains%20a%20significant%20concern.%20This%0Aproblem%20arises%20primarily%20from%20their%20dependence%20on%20a%20fixed%20number%20of%20parameters%0Awithin%20linear%20projections.%20When%20architectural%20modifications%20%28e.g.%2C%20channel%0Adimensions%29%20are%20introduced%2C%20the%20entire%20model%20typically%20requires%20retraining%20from%0Ascratch.%20As%20model%20sizes%20continue%20growing%2C%20this%20strategy%20results%20in%20increasingly%0Ahigh%20computational%20costs%20and%20becomes%20unsustainable.%20To%20overcome%20this%20problem%2C%0Awe%20introduce%20TokenFormer%2C%20a%20natively%20scalable%20architecture%20that%20leverages%20the%0Aattention%20mechanism%20not%20only%20for%20computations%20among%20input%20tokens%20but%20also%20for%0Ainteractions%20between%20tokens%20and%20model%20parameters%2C%20thereby%20enhancing%0Aarchitectural%20flexibility.%20By%20treating%20model%20parameters%20as%20tokens%2C%20we%20replace%0Aall%20the%20linear%20projections%20in%20Transformers%20with%20our%20token-parameter%20attention%0Alayer%2C%20where%20input%20tokens%20act%20as%20queries%20and%20model%20parameters%20as%20keys%20and%0Avalues.%20This%20reformulation%20allows%20for%20progressive%20and%20efficient%20scaling%20without%0Anecessitating%20retraining%20from%20scratch.%20Our%20model%20scales%20from%20124M%20to%201.4B%0Aparameters%20by%20incrementally%20adding%20new%20key-value%20parameter%20pairs%2C%20achieving%0Aperformance%20comparable%20to%20Transformers%20trained%20from%20scratch%20while%20greatly%0Areducing%20training%20costs.%20Code%20and%20models%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Haiyang-W/TokenFormer%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23168v1&entry.124074799=Read"},
{"title": "Weight Copy and Low-Rank Adaptation for Few-Shot Distillation of Vision\n  Transformers", "author": "Diana-Nicoleta Grigore and Mariana-Iuliana Georgescu and Jon Alvarez Justo and Tor Johansen and Andreea Iuliana Ionescu and Radu Tudor Ionescu", "abstract": "  Few-shot knowledge distillation recently emerged as a viable approach to\nharness the knowledge of large-scale pre-trained models, using limited data and\ncomputational resources. In this paper, we propose a novel few-shot feature\ndistillation approach for vision transformers. Our approach is based on two key\nsteps. Leveraging the fact that vision transformers have a consistent\ndepth-wise structure, we first copy the weights from intermittent layers of\nexisting pre-trained vision transformers (teachers) into shallower\narchitectures (students), where the intermittence factor controls the\ncomplexity of the student transformer with respect to its teacher. Next, we\nemploy an enhanced version of Low-Rank Adaptation (LoRA) to distill knowledge\ninto the student in a few-shot scenario, aiming to recover the information\nprocessing carried out by the skipped teacher layers. We present comprehensive\nexperiments with supervised and self-supervised transformers as teachers, on\nsix data sets from various domains (natural, medical and satellite images) and\ntasks (classification and segmentation). The empirical results confirm the\nsuperiority of our approach over state-of-the-art competitors. Moreover, the\nablation results demonstrate the usefulness of each component of the proposed\npipeline. We release our code at https://github.com/dianagrigore/WeCoLoRA.\n", "link": "http://arxiv.org/abs/2404.09326v3", "date": "2024-10-30", "relevancy": 2.1171, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5714}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5288}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weight%20Copy%20and%20Low-Rank%20Adaptation%20for%20Few-Shot%20Distillation%20of%20Vision%0A%20%20Transformers&body=Title%3A%20Weight%20Copy%20and%20Low-Rank%20Adaptation%20for%20Few-Shot%20Distillation%20of%20Vision%0A%20%20Transformers%0AAuthor%3A%20Diana-Nicoleta%20Grigore%20and%20Mariana-Iuliana%20Georgescu%20and%20Jon%20Alvarez%20Justo%20and%20Tor%20Johansen%20and%20Andreea%20Iuliana%20Ionescu%20and%20Radu%20Tudor%20Ionescu%0AAbstract%3A%20%20%20Few-shot%20knowledge%20distillation%20recently%20emerged%20as%20a%20viable%20approach%20to%0Aharness%20the%20knowledge%20of%20large-scale%20pre-trained%20models%2C%20using%20limited%20data%20and%0Acomputational%20resources.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20few-shot%20feature%0Adistillation%20approach%20for%20vision%20transformers.%20Our%20approach%20is%20based%20on%20two%20key%0Asteps.%20Leveraging%20the%20fact%20that%20vision%20transformers%20have%20a%20consistent%0Adepth-wise%20structure%2C%20we%20first%20copy%20the%20weights%20from%20intermittent%20layers%20of%0Aexisting%20pre-trained%20vision%20transformers%20%28teachers%29%20into%20shallower%0Aarchitectures%20%28students%29%2C%20where%20the%20intermittence%20factor%20controls%20the%0Acomplexity%20of%20the%20student%20transformer%20with%20respect%20to%20its%20teacher.%20Next%2C%20we%0Aemploy%20an%20enhanced%20version%20of%20Low-Rank%20Adaptation%20%28LoRA%29%20to%20distill%20knowledge%0Ainto%20the%20student%20in%20a%20few-shot%20scenario%2C%20aiming%20to%20recover%20the%20information%0Aprocessing%20carried%20out%20by%20the%20skipped%20teacher%20layers.%20We%20present%20comprehensive%0Aexperiments%20with%20supervised%20and%20self-supervised%20transformers%20as%20teachers%2C%20on%0Asix%20data%20sets%20from%20various%20domains%20%28natural%2C%20medical%20and%20satellite%20images%29%20and%0Atasks%20%28classification%20and%20segmentation%29.%20The%20empirical%20results%20confirm%20the%0Asuperiority%20of%20our%20approach%20over%20state-of-the-art%20competitors.%20Moreover%2C%20the%0Aablation%20results%20demonstrate%20the%20usefulness%20of%20each%20component%20of%20the%20proposed%0Apipeline.%20We%20release%20our%20code%20at%20https%3A//github.com/dianagrigore/WeCoLoRA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09326v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeight%2520Copy%2520and%2520Low-Rank%2520Adaptation%2520for%2520Few-Shot%2520Distillation%2520of%2520Vision%250A%2520%2520Transformers%26entry.906535625%3DDiana-Nicoleta%2520Grigore%2520and%2520Mariana-Iuliana%2520Georgescu%2520and%2520Jon%2520Alvarez%2520Justo%2520and%2520Tor%2520Johansen%2520and%2520Andreea%2520Iuliana%2520Ionescu%2520and%2520Radu%2520Tudor%2520Ionescu%26entry.1292438233%3D%2520%2520Few-shot%2520knowledge%2520distillation%2520recently%2520emerged%2520as%2520a%2520viable%2520approach%2520to%250Aharness%2520the%2520knowledge%2520of%2520large-scale%2520pre-trained%2520models%252C%2520using%2520limited%2520data%2520and%250Acomputational%2520resources.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520few-shot%2520feature%250Adistillation%2520approach%2520for%2520vision%2520transformers.%2520Our%2520approach%2520is%2520based%2520on%2520two%2520key%250Asteps.%2520Leveraging%2520the%2520fact%2520that%2520vision%2520transformers%2520have%2520a%2520consistent%250Adepth-wise%2520structure%252C%2520we%2520first%2520copy%2520the%2520weights%2520from%2520intermittent%2520layers%2520of%250Aexisting%2520pre-trained%2520vision%2520transformers%2520%2528teachers%2529%2520into%2520shallower%250Aarchitectures%2520%2528students%2529%252C%2520where%2520the%2520intermittence%2520factor%2520controls%2520the%250Acomplexity%2520of%2520the%2520student%2520transformer%2520with%2520respect%2520to%2520its%2520teacher.%2520Next%252C%2520we%250Aemploy%2520an%2520enhanced%2520version%2520of%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520to%2520distill%2520knowledge%250Ainto%2520the%2520student%2520in%2520a%2520few-shot%2520scenario%252C%2520aiming%2520to%2520recover%2520the%2520information%250Aprocessing%2520carried%2520out%2520by%2520the%2520skipped%2520teacher%2520layers.%2520We%2520present%2520comprehensive%250Aexperiments%2520with%2520supervised%2520and%2520self-supervised%2520transformers%2520as%2520teachers%252C%2520on%250Asix%2520data%2520sets%2520from%2520various%2520domains%2520%2528natural%252C%2520medical%2520and%2520satellite%2520images%2529%2520and%250Atasks%2520%2528classification%2520and%2520segmentation%2529.%2520The%2520empirical%2520results%2520confirm%2520the%250Asuperiority%2520of%2520our%2520approach%2520over%2520state-of-the-art%2520competitors.%2520Moreover%252C%2520the%250Aablation%2520results%2520demonstrate%2520the%2520usefulness%2520of%2520each%2520component%2520of%2520the%2520proposed%250Apipeline.%2520We%2520release%2520our%2520code%2520at%2520https%253A//github.com/dianagrigore/WeCoLoRA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09326v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weight%20Copy%20and%20Low-Rank%20Adaptation%20for%20Few-Shot%20Distillation%20of%20Vision%0A%20%20Transformers&entry.906535625=Diana-Nicoleta%20Grigore%20and%20Mariana-Iuliana%20Georgescu%20and%20Jon%20Alvarez%20Justo%20and%20Tor%20Johansen%20and%20Andreea%20Iuliana%20Ionescu%20and%20Radu%20Tudor%20Ionescu&entry.1292438233=%20%20Few-shot%20knowledge%20distillation%20recently%20emerged%20as%20a%20viable%20approach%20to%0Aharness%20the%20knowledge%20of%20large-scale%20pre-trained%20models%2C%20using%20limited%20data%20and%0Acomputational%20resources.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20few-shot%20feature%0Adistillation%20approach%20for%20vision%20transformers.%20Our%20approach%20is%20based%20on%20two%20key%0Asteps.%20Leveraging%20the%20fact%20that%20vision%20transformers%20have%20a%20consistent%0Adepth-wise%20structure%2C%20we%20first%20copy%20the%20weights%20from%20intermittent%20layers%20of%0Aexisting%20pre-trained%20vision%20transformers%20%28teachers%29%20into%20shallower%0Aarchitectures%20%28students%29%2C%20where%20the%20intermittence%20factor%20controls%20the%0Acomplexity%20of%20the%20student%20transformer%20with%20respect%20to%20its%20teacher.%20Next%2C%20we%0Aemploy%20an%20enhanced%20version%20of%20Low-Rank%20Adaptation%20%28LoRA%29%20to%20distill%20knowledge%0Ainto%20the%20student%20in%20a%20few-shot%20scenario%2C%20aiming%20to%20recover%20the%20information%0Aprocessing%20carried%20out%20by%20the%20skipped%20teacher%20layers.%20We%20present%20comprehensive%0Aexperiments%20with%20supervised%20and%20self-supervised%20transformers%20as%20teachers%2C%20on%0Asix%20data%20sets%20from%20various%20domains%20%28natural%2C%20medical%20and%20satellite%20images%29%20and%0Atasks%20%28classification%20and%20segmentation%29.%20The%20empirical%20results%20confirm%20the%0Asuperiority%20of%20our%20approach%20over%20state-of-the-art%20competitors.%20Moreover%2C%20the%0Aablation%20results%20demonstrate%20the%20usefulness%20of%20each%20component%20of%20the%20proposed%0Apipeline.%20We%20release%20our%20code%20at%20https%3A//github.com/dianagrigore/WeCoLoRA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09326v3&entry.124074799=Read"},
{"title": "Improved convergence rate of kNN graph Laplacians", "author": "Yixuan Tan and Xiuyuan Cheng", "abstract": "  In graph-based data analysis, $k$-nearest neighbor ($k$NN) graphs are widely\nused due to their adaptivity to local data densities. Allowing weighted edges\nin the graph, the kernelized graph affinity provides a more general type of\n$k$NN graph where the $k$NN distance is used to set the kernel bandwidth\nadaptively. In this work, we consider a general class of $k$NN graph where the\ngraph affinity is $W_{ij} = \\epsilon^{-d/2} \\; k_0 ( \\| x_i - x_j \\|^2 /\n\\epsilon \\phi( \\widehat{\\rho}(x_i), \\widehat{\\rho}(x_j) )^2 ) $, with\n$\\widehat{\\rho}(x)$ being the (rescaled) $k$NN distance at the point $x$,\n$\\phi$ a symmetric bi-variate function, and $k_0$ a non-negative function on\n$[0,\\infty)$. Under the manifold data setting, where $N$ i.i.d. samples $x_i$\nare drawn from a density $p$ on a $d$-dimensional unknown manifold embedded in\na high dimensional Euclidean space, we prove the point-wise convergence of the\n$k$NN graph Laplacian to the limiting manifold operator (depending on $p$) at\nthe rate of $O(N^{-2/(d+6)}\\,)$, up to a log factor, when $k_0$ and $\\phi$ have\n$C^3$ regularity and satisfy other technical conditions. This fast rate is\nobtained when $\\epsilon \\sim N^{-2/(d+6)}\\,$ and $k \\sim N^{6/(d+6)}\\,$, both\nat the optimal order to balance the theoretical bias and variance errors. When\n$k_0$ and $\\phi$ have lower regularities, including when $k_0$ is a compactly\nsupported function as in the standard $k$NN graph, the convergence rate\ndegenerates to $O(N^{-1/(d+4)}\\,)$. Our improved convergence rate is based on a\nrefined analysis of the $k$NN estimator, which can be of independent interest.\nWe validate our theory by numerical experiments on simulated data.\n", "link": "http://arxiv.org/abs/2410.23212v1", "date": "2024-10-30", "relevancy": 2.0897, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4406}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4076}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20convergence%20rate%20of%20kNN%20graph%20Laplacians&body=Title%3A%20Improved%20convergence%20rate%20of%20kNN%20graph%20Laplacians%0AAuthor%3A%20Yixuan%20Tan%20and%20Xiuyuan%20Cheng%0AAbstract%3A%20%20%20In%20graph-based%20data%20analysis%2C%20%24k%24-nearest%20neighbor%20%28%24k%24NN%29%20graphs%20are%20widely%0Aused%20due%20to%20their%20adaptivity%20to%20local%20data%20densities.%20Allowing%20weighted%20edges%0Ain%20the%20graph%2C%20the%20kernelized%20graph%20affinity%20provides%20a%20more%20general%20type%20of%0A%24k%24NN%20graph%20where%20the%20%24k%24NN%20distance%20is%20used%20to%20set%20the%20kernel%20bandwidth%0Aadaptively.%20In%20this%20work%2C%20we%20consider%20a%20general%20class%20of%20%24k%24NN%20graph%20where%20the%0Agraph%20affinity%20is%20%24W_%7Bij%7D%20%3D%20%5Cepsilon%5E%7B-d/2%7D%20%5C%3B%20k_0%20%28%20%5C%7C%20x_i%20-%20x_j%20%5C%7C%5E2%20/%0A%5Cepsilon%20%5Cphi%28%20%5Cwidehat%7B%5Crho%7D%28x_i%29%2C%20%5Cwidehat%7B%5Crho%7D%28x_j%29%20%29%5E2%20%29%20%24%2C%20with%0A%24%5Cwidehat%7B%5Crho%7D%28x%29%24%20being%20the%20%28rescaled%29%20%24k%24NN%20distance%20at%20the%20point%20%24x%24%2C%0A%24%5Cphi%24%20a%20symmetric%20bi-variate%20function%2C%20and%20%24k_0%24%20a%20non-negative%20function%20on%0A%24%5B0%2C%5Cinfty%29%24.%20Under%20the%20manifold%20data%20setting%2C%20where%20%24N%24%20i.i.d.%20samples%20%24x_i%24%0Aare%20drawn%20from%20a%20density%20%24p%24%20on%20a%20%24d%24-dimensional%20unknown%20manifold%20embedded%20in%0Aa%20high%20dimensional%20Euclidean%20space%2C%20we%20prove%20the%20point-wise%20convergence%20of%20the%0A%24k%24NN%20graph%20Laplacian%20to%20the%20limiting%20manifold%20operator%20%28depending%20on%20%24p%24%29%20at%0Athe%20rate%20of%20%24O%28N%5E%7B-2/%28d%2B6%29%7D%5C%2C%29%24%2C%20up%20to%20a%20log%20factor%2C%20when%20%24k_0%24%20and%20%24%5Cphi%24%20have%0A%24C%5E3%24%20regularity%20and%20satisfy%20other%20technical%20conditions.%20This%20fast%20rate%20is%0Aobtained%20when%20%24%5Cepsilon%20%5Csim%20N%5E%7B-2/%28d%2B6%29%7D%5C%2C%24%20and%20%24k%20%5Csim%20N%5E%7B6/%28d%2B6%29%7D%5C%2C%24%2C%20both%0Aat%20the%20optimal%20order%20to%20balance%20the%20theoretical%20bias%20and%20variance%20errors.%20When%0A%24k_0%24%20and%20%24%5Cphi%24%20have%20lower%20regularities%2C%20including%20when%20%24k_0%24%20is%20a%20compactly%0Asupported%20function%20as%20in%20the%20standard%20%24k%24NN%20graph%2C%20the%20convergence%20rate%0Adegenerates%20to%20%24O%28N%5E%7B-1/%28d%2B4%29%7D%5C%2C%29%24.%20Our%20improved%20convergence%20rate%20is%20based%20on%20a%0Arefined%20analysis%20of%20the%20%24k%24NN%20estimator%2C%20which%20can%20be%20of%20independent%20interest.%0AWe%20validate%20our%20theory%20by%20numerical%20experiments%20on%20simulated%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23212v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520convergence%2520rate%2520of%2520kNN%2520graph%2520Laplacians%26entry.906535625%3DYixuan%2520Tan%2520and%2520Xiuyuan%2520Cheng%26entry.1292438233%3D%2520%2520In%2520graph-based%2520data%2520analysis%252C%2520%2524k%2524-nearest%2520neighbor%2520%2528%2524k%2524NN%2529%2520graphs%2520are%2520widely%250Aused%2520due%2520to%2520their%2520adaptivity%2520to%2520local%2520data%2520densities.%2520Allowing%2520weighted%2520edges%250Ain%2520the%2520graph%252C%2520the%2520kernelized%2520graph%2520affinity%2520provides%2520a%2520more%2520general%2520type%2520of%250A%2524k%2524NN%2520graph%2520where%2520the%2520%2524k%2524NN%2520distance%2520is%2520used%2520to%2520set%2520the%2520kernel%2520bandwidth%250Aadaptively.%2520In%2520this%2520work%252C%2520we%2520consider%2520a%2520general%2520class%2520of%2520%2524k%2524NN%2520graph%2520where%2520the%250Agraph%2520affinity%2520is%2520%2524W_%257Bij%257D%2520%253D%2520%255Cepsilon%255E%257B-d/2%257D%2520%255C%253B%2520k_0%2520%2528%2520%255C%257C%2520x_i%2520-%2520x_j%2520%255C%257C%255E2%2520/%250A%255Cepsilon%2520%255Cphi%2528%2520%255Cwidehat%257B%255Crho%257D%2528x_i%2529%252C%2520%255Cwidehat%257B%255Crho%257D%2528x_j%2529%2520%2529%255E2%2520%2529%2520%2524%252C%2520with%250A%2524%255Cwidehat%257B%255Crho%257D%2528x%2529%2524%2520being%2520the%2520%2528rescaled%2529%2520%2524k%2524NN%2520distance%2520at%2520the%2520point%2520%2524x%2524%252C%250A%2524%255Cphi%2524%2520a%2520symmetric%2520bi-variate%2520function%252C%2520and%2520%2524k_0%2524%2520a%2520non-negative%2520function%2520on%250A%2524%255B0%252C%255Cinfty%2529%2524.%2520Under%2520the%2520manifold%2520data%2520setting%252C%2520where%2520%2524N%2524%2520i.i.d.%2520samples%2520%2524x_i%2524%250Aare%2520drawn%2520from%2520a%2520density%2520%2524p%2524%2520on%2520a%2520%2524d%2524-dimensional%2520unknown%2520manifold%2520embedded%2520in%250Aa%2520high%2520dimensional%2520Euclidean%2520space%252C%2520we%2520prove%2520the%2520point-wise%2520convergence%2520of%2520the%250A%2524k%2524NN%2520graph%2520Laplacian%2520to%2520the%2520limiting%2520manifold%2520operator%2520%2528depending%2520on%2520%2524p%2524%2529%2520at%250Athe%2520rate%2520of%2520%2524O%2528N%255E%257B-2/%2528d%252B6%2529%257D%255C%252C%2529%2524%252C%2520up%2520to%2520a%2520log%2520factor%252C%2520when%2520%2524k_0%2524%2520and%2520%2524%255Cphi%2524%2520have%250A%2524C%255E3%2524%2520regularity%2520and%2520satisfy%2520other%2520technical%2520conditions.%2520This%2520fast%2520rate%2520is%250Aobtained%2520when%2520%2524%255Cepsilon%2520%255Csim%2520N%255E%257B-2/%2528d%252B6%2529%257D%255C%252C%2524%2520and%2520%2524k%2520%255Csim%2520N%255E%257B6/%2528d%252B6%2529%257D%255C%252C%2524%252C%2520both%250Aat%2520the%2520optimal%2520order%2520to%2520balance%2520the%2520theoretical%2520bias%2520and%2520variance%2520errors.%2520When%250A%2524k_0%2524%2520and%2520%2524%255Cphi%2524%2520have%2520lower%2520regularities%252C%2520including%2520when%2520%2524k_0%2524%2520is%2520a%2520compactly%250Asupported%2520function%2520as%2520in%2520the%2520standard%2520%2524k%2524NN%2520graph%252C%2520the%2520convergence%2520rate%250Adegenerates%2520to%2520%2524O%2528N%255E%257B-1/%2528d%252B4%2529%257D%255C%252C%2529%2524.%2520Our%2520improved%2520convergence%2520rate%2520is%2520based%2520on%2520a%250Arefined%2520analysis%2520of%2520the%2520%2524k%2524NN%2520estimator%252C%2520which%2520can%2520be%2520of%2520independent%2520interest.%250AWe%2520validate%2520our%2520theory%2520by%2520numerical%2520experiments%2520on%2520simulated%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23212v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20convergence%20rate%20of%20kNN%20graph%20Laplacians&entry.906535625=Yixuan%20Tan%20and%20Xiuyuan%20Cheng&entry.1292438233=%20%20In%20graph-based%20data%20analysis%2C%20%24k%24-nearest%20neighbor%20%28%24k%24NN%29%20graphs%20are%20widely%0Aused%20due%20to%20their%20adaptivity%20to%20local%20data%20densities.%20Allowing%20weighted%20edges%0Ain%20the%20graph%2C%20the%20kernelized%20graph%20affinity%20provides%20a%20more%20general%20type%20of%0A%24k%24NN%20graph%20where%20the%20%24k%24NN%20distance%20is%20used%20to%20set%20the%20kernel%20bandwidth%0Aadaptively.%20In%20this%20work%2C%20we%20consider%20a%20general%20class%20of%20%24k%24NN%20graph%20where%20the%0Agraph%20affinity%20is%20%24W_%7Bij%7D%20%3D%20%5Cepsilon%5E%7B-d/2%7D%20%5C%3B%20k_0%20%28%20%5C%7C%20x_i%20-%20x_j%20%5C%7C%5E2%20/%0A%5Cepsilon%20%5Cphi%28%20%5Cwidehat%7B%5Crho%7D%28x_i%29%2C%20%5Cwidehat%7B%5Crho%7D%28x_j%29%20%29%5E2%20%29%20%24%2C%20with%0A%24%5Cwidehat%7B%5Crho%7D%28x%29%24%20being%20the%20%28rescaled%29%20%24k%24NN%20distance%20at%20the%20point%20%24x%24%2C%0A%24%5Cphi%24%20a%20symmetric%20bi-variate%20function%2C%20and%20%24k_0%24%20a%20non-negative%20function%20on%0A%24%5B0%2C%5Cinfty%29%24.%20Under%20the%20manifold%20data%20setting%2C%20where%20%24N%24%20i.i.d.%20samples%20%24x_i%24%0Aare%20drawn%20from%20a%20density%20%24p%24%20on%20a%20%24d%24-dimensional%20unknown%20manifold%20embedded%20in%0Aa%20high%20dimensional%20Euclidean%20space%2C%20we%20prove%20the%20point-wise%20convergence%20of%20the%0A%24k%24NN%20graph%20Laplacian%20to%20the%20limiting%20manifold%20operator%20%28depending%20on%20%24p%24%29%20at%0Athe%20rate%20of%20%24O%28N%5E%7B-2/%28d%2B6%29%7D%5C%2C%29%24%2C%20up%20to%20a%20log%20factor%2C%20when%20%24k_0%24%20and%20%24%5Cphi%24%20have%0A%24C%5E3%24%20regularity%20and%20satisfy%20other%20technical%20conditions.%20This%20fast%20rate%20is%0Aobtained%20when%20%24%5Cepsilon%20%5Csim%20N%5E%7B-2/%28d%2B6%29%7D%5C%2C%24%20and%20%24k%20%5Csim%20N%5E%7B6/%28d%2B6%29%7D%5C%2C%24%2C%20both%0Aat%20the%20optimal%20order%20to%20balance%20the%20theoretical%20bias%20and%20variance%20errors.%20When%0A%24k_0%24%20and%20%24%5Cphi%24%20have%20lower%20regularities%2C%20including%20when%20%24k_0%24%20is%20a%20compactly%0Asupported%20function%20as%20in%20the%20standard%20%24k%24NN%20graph%2C%20the%20convergence%20rate%0Adegenerates%20to%20%24O%28N%5E%7B-1/%28d%2B4%29%7D%5C%2C%29%24.%20Our%20improved%20convergence%20rate%20is%20based%20on%20a%0Arefined%20analysis%20of%20the%20%24k%24NN%20estimator%2C%20which%20can%20be%20of%20independent%20interest.%0AWe%20validate%20our%20theory%20by%20numerical%20experiments%20on%20simulated%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23212v1&entry.124074799=Read"},
{"title": "CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision\n  Language Models", "author": "Peng Xia and Ze Chen and Juanxi Tian and Yangrui Gong and Ruibo Hou and Yue Xu and Zhenbang Wu and Zhiyuan Fan and Yiyang Zhou and Kangyu Zhu and Wenhao Zheng and Zhaoyang Wang and Xiao Wang and Xuchao Zhang and Chetan Bansal and Marc Niethammer and Junzhou Huang and Hongtu Zhu and Yun Li and Jimeng Sun and Zongyuan Ge and Gang Li and James Zou and Huaxiu Yao", "abstract": "  Artificial intelligence has significantly impacted medical applications,\nparticularly with the advent of Medical Large Vision Language Models\n(Med-LVLMs), sparking optimism for the future of automated and personalized\nhealthcare. However, the trustworthiness of Med-LVLMs remains unverified,\nposing significant risks for future model deployment. In this paper, we\nintroduce CARES and aim to comprehensively evaluate the Trustworthiness of\nMed-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs\nacross five dimensions, including trustfulness, fairness, safety, privacy, and\nrobustness. CARES comprises about 41K question-answer pairs in both closed and\nopen-ended formats, covering 16 medical image modalities and 27 anatomical\nregions. Our analysis reveals that the models consistently exhibit concerns\nregarding trustworthiness, often displaying factual inaccuracies and failing to\nmaintain fairness across different demographic groups. Furthermore, they are\nvulnerable to attacks and demonstrate a lack of privacy awareness. We publicly\nrelease our benchmark and code in https://cares-ai.github.io/.\n", "link": "http://arxiv.org/abs/2406.06007v2", "date": "2024-10-30", "relevancy": 2.0856, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5674}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5122}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CARES%3A%20A%20Comprehensive%20Benchmark%20of%20Trustworthiness%20in%20Medical%20Vision%0A%20%20Language%20Models&body=Title%3A%20CARES%3A%20A%20Comprehensive%20Benchmark%20of%20Trustworthiness%20in%20Medical%20Vision%0A%20%20Language%20Models%0AAuthor%3A%20Peng%20Xia%20and%20Ze%20Chen%20and%20Juanxi%20Tian%20and%20Yangrui%20Gong%20and%20Ruibo%20Hou%20and%20Yue%20Xu%20and%20Zhenbang%20Wu%20and%20Zhiyuan%20Fan%20and%20Yiyang%20Zhou%20and%20Kangyu%20Zhu%20and%20Wenhao%20Zheng%20and%20Zhaoyang%20Wang%20and%20Xiao%20Wang%20and%20Xuchao%20Zhang%20and%20Chetan%20Bansal%20and%20Marc%20Niethammer%20and%20Junzhou%20Huang%20and%20Hongtu%20Zhu%20and%20Yun%20Li%20and%20Jimeng%20Sun%20and%20Zongyuan%20Ge%20and%20Gang%20Li%20and%20James%20Zou%20and%20Huaxiu%20Yao%0AAbstract%3A%20%20%20Artificial%20intelligence%20has%20significantly%20impacted%20medical%20applications%2C%0Aparticularly%20with%20the%20advent%20of%20Medical%20Large%20Vision%20Language%20Models%0A%28Med-LVLMs%29%2C%20sparking%20optimism%20for%20the%20future%20of%20automated%20and%20personalized%0Ahealthcare.%20However%2C%20the%20trustworthiness%20of%20Med-LVLMs%20remains%20unverified%2C%0Aposing%20significant%20risks%20for%20future%20model%20deployment.%20In%20this%20paper%2C%20we%0Aintroduce%20CARES%20and%20aim%20to%20comprehensively%20evaluate%20the%20Trustworthiness%20of%0AMed-LVLMs%20across%20the%20medical%20domain.%20We%20assess%20the%20trustworthiness%20of%20Med-LVLMs%0Aacross%20five%20dimensions%2C%20including%20trustfulness%2C%20fairness%2C%20safety%2C%20privacy%2C%20and%0Arobustness.%20CARES%20comprises%20about%2041K%20question-answer%20pairs%20in%20both%20closed%20and%0Aopen-ended%20formats%2C%20covering%2016%20medical%20image%20modalities%20and%2027%20anatomical%0Aregions.%20Our%20analysis%20reveals%20that%20the%20models%20consistently%20exhibit%20concerns%0Aregarding%20trustworthiness%2C%20often%20displaying%20factual%20inaccuracies%20and%20failing%20to%0Amaintain%20fairness%20across%20different%20demographic%20groups.%20Furthermore%2C%20they%20are%0Avulnerable%20to%20attacks%20and%20demonstrate%20a%20lack%20of%20privacy%20awareness.%20We%20publicly%0Arelease%20our%20benchmark%20and%20code%20in%20https%3A//cares-ai.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06007v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCARES%253A%2520A%2520Comprehensive%2520Benchmark%2520of%2520Trustworthiness%2520in%2520Medical%2520Vision%250A%2520%2520Language%2520Models%26entry.906535625%3DPeng%2520Xia%2520and%2520Ze%2520Chen%2520and%2520Juanxi%2520Tian%2520and%2520Yangrui%2520Gong%2520and%2520Ruibo%2520Hou%2520and%2520Yue%2520Xu%2520and%2520Zhenbang%2520Wu%2520and%2520Zhiyuan%2520Fan%2520and%2520Yiyang%2520Zhou%2520and%2520Kangyu%2520Zhu%2520and%2520Wenhao%2520Zheng%2520and%2520Zhaoyang%2520Wang%2520and%2520Xiao%2520Wang%2520and%2520Xuchao%2520Zhang%2520and%2520Chetan%2520Bansal%2520and%2520Marc%2520Niethammer%2520and%2520Junzhou%2520Huang%2520and%2520Hongtu%2520Zhu%2520and%2520Yun%2520Li%2520and%2520Jimeng%2520Sun%2520and%2520Zongyuan%2520Ge%2520and%2520Gang%2520Li%2520and%2520James%2520Zou%2520and%2520Huaxiu%2520Yao%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520has%2520significantly%2520impacted%2520medical%2520applications%252C%250Aparticularly%2520with%2520the%2520advent%2520of%2520Medical%2520Large%2520Vision%2520Language%2520Models%250A%2528Med-LVLMs%2529%252C%2520sparking%2520optimism%2520for%2520the%2520future%2520of%2520automated%2520and%2520personalized%250Ahealthcare.%2520However%252C%2520the%2520trustworthiness%2520of%2520Med-LVLMs%2520remains%2520unverified%252C%250Aposing%2520significant%2520risks%2520for%2520future%2520model%2520deployment.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520CARES%2520and%2520aim%2520to%2520comprehensively%2520evaluate%2520the%2520Trustworthiness%2520of%250AMed-LVLMs%2520across%2520the%2520medical%2520domain.%2520We%2520assess%2520the%2520trustworthiness%2520of%2520Med-LVLMs%250Aacross%2520five%2520dimensions%252C%2520including%2520trustfulness%252C%2520fairness%252C%2520safety%252C%2520privacy%252C%2520and%250Arobustness.%2520CARES%2520comprises%2520about%252041K%2520question-answer%2520pairs%2520in%2520both%2520closed%2520and%250Aopen-ended%2520formats%252C%2520covering%252016%2520medical%2520image%2520modalities%2520and%252027%2520anatomical%250Aregions.%2520Our%2520analysis%2520reveals%2520that%2520the%2520models%2520consistently%2520exhibit%2520concerns%250Aregarding%2520trustworthiness%252C%2520often%2520displaying%2520factual%2520inaccuracies%2520and%2520failing%2520to%250Amaintain%2520fairness%2520across%2520different%2520demographic%2520groups.%2520Furthermore%252C%2520they%2520are%250Avulnerable%2520to%2520attacks%2520and%2520demonstrate%2520a%2520lack%2520of%2520privacy%2520awareness.%2520We%2520publicly%250Arelease%2520our%2520benchmark%2520and%2520code%2520in%2520https%253A//cares-ai.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06007v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CARES%3A%20A%20Comprehensive%20Benchmark%20of%20Trustworthiness%20in%20Medical%20Vision%0A%20%20Language%20Models&entry.906535625=Peng%20Xia%20and%20Ze%20Chen%20and%20Juanxi%20Tian%20and%20Yangrui%20Gong%20and%20Ruibo%20Hou%20and%20Yue%20Xu%20and%20Zhenbang%20Wu%20and%20Zhiyuan%20Fan%20and%20Yiyang%20Zhou%20and%20Kangyu%20Zhu%20and%20Wenhao%20Zheng%20and%20Zhaoyang%20Wang%20and%20Xiao%20Wang%20and%20Xuchao%20Zhang%20and%20Chetan%20Bansal%20and%20Marc%20Niethammer%20and%20Junzhou%20Huang%20and%20Hongtu%20Zhu%20and%20Yun%20Li%20and%20Jimeng%20Sun%20and%20Zongyuan%20Ge%20and%20Gang%20Li%20and%20James%20Zou%20and%20Huaxiu%20Yao&entry.1292438233=%20%20Artificial%20intelligence%20has%20significantly%20impacted%20medical%20applications%2C%0Aparticularly%20with%20the%20advent%20of%20Medical%20Large%20Vision%20Language%20Models%0A%28Med-LVLMs%29%2C%20sparking%20optimism%20for%20the%20future%20of%20automated%20and%20personalized%0Ahealthcare.%20However%2C%20the%20trustworthiness%20of%20Med-LVLMs%20remains%20unverified%2C%0Aposing%20significant%20risks%20for%20future%20model%20deployment.%20In%20this%20paper%2C%20we%0Aintroduce%20CARES%20and%20aim%20to%20comprehensively%20evaluate%20the%20Trustworthiness%20of%0AMed-LVLMs%20across%20the%20medical%20domain.%20We%20assess%20the%20trustworthiness%20of%20Med-LVLMs%0Aacross%20five%20dimensions%2C%20including%20trustfulness%2C%20fairness%2C%20safety%2C%20privacy%2C%20and%0Arobustness.%20CARES%20comprises%20about%2041K%20question-answer%20pairs%20in%20both%20closed%20and%0Aopen-ended%20formats%2C%20covering%2016%20medical%20image%20modalities%20and%2027%20anatomical%0Aregions.%20Our%20analysis%20reveals%20that%20the%20models%20consistently%20exhibit%20concerns%0Aregarding%20trustworthiness%2C%20often%20displaying%20factual%20inaccuracies%20and%20failing%20to%0Amaintain%20fairness%20across%20different%20demographic%20groups.%20Furthermore%2C%20they%20are%0Avulnerable%20to%20attacks%20and%20demonstrate%20a%20lack%20of%20privacy%20awareness.%20We%20publicly%0Arelease%20our%20benchmark%20and%20code%20in%20https%3A//cares-ai.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06007v2&entry.124074799=Read"},
{"title": "Conditional Forecasting of Margin Calls using Dynamic Graph Neural\n  Networks", "author": "Matteo Citterio and Marco D'Errico and Gabriele Visentin", "abstract": "  We introduce a novel Dynamic Graph Neural Network (DGNN) architecture for\nsolving conditional $m$-steps ahead forecasting problems in temporal financial\nnetworks. The proposed DGNN is validated on simulated data from a temporal\nfinancial network model capturing stylized features of Interest Rate Swaps\n(IRSs) transaction networks, where financial entities trade swap contracts\ndynamically and the network topology evolves conditionally on a reference rate.\nThe proposed model is able to produce accurate conditional forecasts of net\nvariation margins up to a $21$-day horizon by leveraging conditional\ninformation under pre-determined stress test scenarios. Our work shows that the\nnetwork dynamics can be successfully incorporated into stress-testing\npractices, thus providing regulators and policymakers with a crucial tool for\nsystemic risk monitoring.\n", "link": "http://arxiv.org/abs/2410.23275v1", "date": "2024-10-30", "relevancy": 2.0747, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5601}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5021}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conditional%20Forecasting%20of%20Margin%20Calls%20using%20Dynamic%20Graph%20Neural%0A%20%20Networks&body=Title%3A%20Conditional%20Forecasting%20of%20Margin%20Calls%20using%20Dynamic%20Graph%20Neural%0A%20%20Networks%0AAuthor%3A%20Matteo%20Citterio%20and%20Marco%20D%27Errico%20and%20Gabriele%20Visentin%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20Dynamic%20Graph%20Neural%20Network%20%28DGNN%29%20architecture%20for%0Asolving%20conditional%20%24m%24-steps%20ahead%20forecasting%20problems%20in%20temporal%20financial%0Anetworks.%20The%20proposed%20DGNN%20is%20validated%20on%20simulated%20data%20from%20a%20temporal%0Afinancial%20network%20model%20capturing%20stylized%20features%20of%20Interest%20Rate%20Swaps%0A%28IRSs%29%20transaction%20networks%2C%20where%20financial%20entities%20trade%20swap%20contracts%0Adynamically%20and%20the%20network%20topology%20evolves%20conditionally%20on%20a%20reference%20rate.%0AThe%20proposed%20model%20is%20able%20to%20produce%20accurate%20conditional%20forecasts%20of%20net%0Avariation%20margins%20up%20to%20a%20%2421%24-day%20horizon%20by%20leveraging%20conditional%0Ainformation%20under%20pre-determined%20stress%20test%20scenarios.%20Our%20work%20shows%20that%20the%0Anetwork%20dynamics%20can%20be%20successfully%20incorporated%20into%20stress-testing%0Apractices%2C%20thus%20providing%20regulators%20and%20policymakers%20with%20a%20crucial%20tool%20for%0Asystemic%20risk%20monitoring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23275v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConditional%2520Forecasting%2520of%2520Margin%2520Calls%2520using%2520Dynamic%2520Graph%2520Neural%250A%2520%2520Networks%26entry.906535625%3DMatteo%2520Citterio%2520and%2520Marco%2520D%2527Errico%2520and%2520Gabriele%2520Visentin%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520Dynamic%2520Graph%2520Neural%2520Network%2520%2528DGNN%2529%2520architecture%2520for%250Asolving%2520conditional%2520%2524m%2524-steps%2520ahead%2520forecasting%2520problems%2520in%2520temporal%2520financial%250Anetworks.%2520The%2520proposed%2520DGNN%2520is%2520validated%2520on%2520simulated%2520data%2520from%2520a%2520temporal%250Afinancial%2520network%2520model%2520capturing%2520stylized%2520features%2520of%2520Interest%2520Rate%2520Swaps%250A%2528IRSs%2529%2520transaction%2520networks%252C%2520where%2520financial%2520entities%2520trade%2520swap%2520contracts%250Adynamically%2520and%2520the%2520network%2520topology%2520evolves%2520conditionally%2520on%2520a%2520reference%2520rate.%250AThe%2520proposed%2520model%2520is%2520able%2520to%2520produce%2520accurate%2520conditional%2520forecasts%2520of%2520net%250Avariation%2520margins%2520up%2520to%2520a%2520%252421%2524-day%2520horizon%2520by%2520leveraging%2520conditional%250Ainformation%2520under%2520pre-determined%2520stress%2520test%2520scenarios.%2520Our%2520work%2520shows%2520that%2520the%250Anetwork%2520dynamics%2520can%2520be%2520successfully%2520incorporated%2520into%2520stress-testing%250Apractices%252C%2520thus%2520providing%2520regulators%2520and%2520policymakers%2520with%2520a%2520crucial%2520tool%2520for%250Asystemic%2520risk%2520monitoring.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23275v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditional%20Forecasting%20of%20Margin%20Calls%20using%20Dynamic%20Graph%20Neural%0A%20%20Networks&entry.906535625=Matteo%20Citterio%20and%20Marco%20D%27Errico%20and%20Gabriele%20Visentin&entry.1292438233=%20%20We%20introduce%20a%20novel%20Dynamic%20Graph%20Neural%20Network%20%28DGNN%29%20architecture%20for%0Asolving%20conditional%20%24m%24-steps%20ahead%20forecasting%20problems%20in%20temporal%20financial%0Anetworks.%20The%20proposed%20DGNN%20is%20validated%20on%20simulated%20data%20from%20a%20temporal%0Afinancial%20network%20model%20capturing%20stylized%20features%20of%20Interest%20Rate%20Swaps%0A%28IRSs%29%20transaction%20networks%2C%20where%20financial%20entities%20trade%20swap%20contracts%0Adynamically%20and%20the%20network%20topology%20evolves%20conditionally%20on%20a%20reference%20rate.%0AThe%20proposed%20model%20is%20able%20to%20produce%20accurate%20conditional%20forecasts%20of%20net%0Avariation%20margins%20up%20to%20a%20%2421%24-day%20horizon%20by%20leveraging%20conditional%0Ainformation%20under%20pre-determined%20stress%20test%20scenarios.%20Our%20work%20shows%20that%20the%0Anetwork%20dynamics%20can%20be%20successfully%20incorporated%20into%20stress-testing%0Apractices%2C%20thus%20providing%20regulators%20and%20policymakers%20with%20a%20crucial%20tool%20for%0Asystemic%20risk%20monitoring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23275v1&entry.124074799=Read"},
{"title": "Derivative-enhanced Deep Operator Network", "author": "Yuan Qiu and Nolan Bridges and Peng Chen", "abstract": "  The deep operator networks (DeepONet), a class of neural operators that learn\nmappings between function spaces, have recently been developed as surrogate\nmodels for parametric partial differential equations (PDEs). In this work we\npropose a derivative-enhanced deep operator network (DE-DeepONet), which\nleverages derivative information to enhance the solution prediction accuracy\nand provides a more accurate approximation of solution-to-parameter\nderivatives, especially when training data are limited. DE-DeepONet explicitly\nincorporates linear dimension reduction of high dimensional parameter input\ninto DeepONet to reduce training cost and adds derivative loss in the loss\nfunction to reduce the number of required parameter-solution pairs. We further\ndemonstrate that the use of derivative loss can be extended to enhance other\nneural operators, such as the Fourier neural operator (FNO). Numerical\nexperiments validate the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2402.19242v2", "date": "2024-10-30", "relevancy": 2.046, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5708}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4752}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Derivative-enhanced%20Deep%20Operator%20Network&body=Title%3A%20Derivative-enhanced%20Deep%20Operator%20Network%0AAuthor%3A%20Yuan%20Qiu%20and%20Nolan%20Bridges%20and%20Peng%20Chen%0AAbstract%3A%20%20%20The%20deep%20operator%20networks%20%28DeepONet%29%2C%20a%20class%20of%20neural%20operators%20that%20learn%0Amappings%20between%20function%20spaces%2C%20have%20recently%20been%20developed%20as%20surrogate%0Amodels%20for%20parametric%20partial%20differential%20equations%20%28PDEs%29.%20In%20this%20work%20we%0Apropose%20a%20derivative-enhanced%20deep%20operator%20network%20%28DE-DeepONet%29%2C%20which%0Aleverages%20derivative%20information%20to%20enhance%20the%20solution%20prediction%20accuracy%0Aand%20provides%20a%20more%20accurate%20approximation%20of%20solution-to-parameter%0Aderivatives%2C%20especially%20when%20training%20data%20are%20limited.%20DE-DeepONet%20explicitly%0Aincorporates%20linear%20dimension%20reduction%20of%20high%20dimensional%20parameter%20input%0Ainto%20DeepONet%20to%20reduce%20training%20cost%20and%20adds%20derivative%20loss%20in%20the%20loss%0Afunction%20to%20reduce%20the%20number%20of%20required%20parameter-solution%20pairs.%20We%20further%0Ademonstrate%20that%20the%20use%20of%20derivative%20loss%20can%20be%20extended%20to%20enhance%20other%0Aneural%20operators%2C%20such%20as%20the%20Fourier%20neural%20operator%20%28FNO%29.%20Numerical%0Aexperiments%20validate%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDerivative-enhanced%2520Deep%2520Operator%2520Network%26entry.906535625%3DYuan%2520Qiu%2520and%2520Nolan%2520Bridges%2520and%2520Peng%2520Chen%26entry.1292438233%3D%2520%2520The%2520deep%2520operator%2520networks%2520%2528DeepONet%2529%252C%2520a%2520class%2520of%2520neural%2520operators%2520that%2520learn%250Amappings%2520between%2520function%2520spaces%252C%2520have%2520recently%2520been%2520developed%2520as%2520surrogate%250Amodels%2520for%2520parametric%2520partial%2520differential%2520equations%2520%2528PDEs%2529.%2520In%2520this%2520work%2520we%250Apropose%2520a%2520derivative-enhanced%2520deep%2520operator%2520network%2520%2528DE-DeepONet%2529%252C%2520which%250Aleverages%2520derivative%2520information%2520to%2520enhance%2520the%2520solution%2520prediction%2520accuracy%250Aand%2520provides%2520a%2520more%2520accurate%2520approximation%2520of%2520solution-to-parameter%250Aderivatives%252C%2520especially%2520when%2520training%2520data%2520are%2520limited.%2520DE-DeepONet%2520explicitly%250Aincorporates%2520linear%2520dimension%2520reduction%2520of%2520high%2520dimensional%2520parameter%2520input%250Ainto%2520DeepONet%2520to%2520reduce%2520training%2520cost%2520and%2520adds%2520derivative%2520loss%2520in%2520the%2520loss%250Afunction%2520to%2520reduce%2520the%2520number%2520of%2520required%2520parameter-solution%2520pairs.%2520We%2520further%250Ademonstrate%2520that%2520the%2520use%2520of%2520derivative%2520loss%2520can%2520be%2520extended%2520to%2520enhance%2520other%250Aneural%2520operators%252C%2520such%2520as%2520the%2520Fourier%2520neural%2520operator%2520%2528FNO%2529.%2520Numerical%250Aexperiments%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Derivative-enhanced%20Deep%20Operator%20Network&entry.906535625=Yuan%20Qiu%20and%20Nolan%20Bridges%20and%20Peng%20Chen&entry.1292438233=%20%20The%20deep%20operator%20networks%20%28DeepONet%29%2C%20a%20class%20of%20neural%20operators%20that%20learn%0Amappings%20between%20function%20spaces%2C%20have%20recently%20been%20developed%20as%20surrogate%0Amodels%20for%20parametric%20partial%20differential%20equations%20%28PDEs%29.%20In%20this%20work%20we%0Apropose%20a%20derivative-enhanced%20deep%20operator%20network%20%28DE-DeepONet%29%2C%20which%0Aleverages%20derivative%20information%20to%20enhance%20the%20solution%20prediction%20accuracy%0Aand%20provides%20a%20more%20accurate%20approximation%20of%20solution-to-parameter%0Aderivatives%2C%20especially%20when%20training%20data%20are%20limited.%20DE-DeepONet%20explicitly%0Aincorporates%20linear%20dimension%20reduction%20of%20high%20dimensional%20parameter%20input%0Ainto%20DeepONet%20to%20reduce%20training%20cost%20and%20adds%20derivative%20loss%20in%20the%20loss%0Afunction%20to%20reduce%20the%20number%20of%20required%20parameter-solution%20pairs.%20We%20further%0Ademonstrate%20that%20the%20use%20of%20derivative%20loss%20can%20be%20extended%20to%20enhance%20other%0Aneural%20operators%2C%20such%20as%20the%20Fourier%20neural%20operator%20%28FNO%29.%20Numerical%0Aexperiments%20validate%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19242v2&entry.124074799=Read"},
{"title": "Attribute-to-Delete: Machine Unlearning via Datamodel Matching", "author": "Kristian Georgiev and Roy Rinberg and Sung Min Park and Shivam Garg and Andrew Ilyas and Aleksander Madry and Seth Neel", "abstract": "  Machine unlearning -- efficiently removing the effect of a small \"forget set\"\nof training data on a pre-trained machine learning model -- has recently\nattracted significant research interest. Despite this interest, however, recent\nwork shows that existing machine unlearning techniques do not hold up to\nthorough evaluation in non-convex settings. In this work, we introduce a new\nmachine unlearning technique that exhibits strong empirical performance even in\nsuch challenging settings. Our starting point is the perspective that the goal\nof unlearning is to produce a model whose outputs are statistically\nindistinguishable from those of a model re-trained on all but the forget set.\nThis perspective naturally suggests a reduction from the unlearning problem to\nthat of data attribution, where the goal is to predict the effect of changing\nthe training set on a model's outputs. Thus motivated, we propose the following\nmeta-algorithm, which we call Datamodel Matching (DMM): given a trained model,\nwe (a) use data attribution to predict the output of the model if it were\nre-trained on all but the forget set points; then (b) fine-tune the pre-trained\nmodel to match these predicted outputs. In a simple convex setting, we show how\nthis approach provably outperforms a variety of iterative unlearning\nalgorithms. Empirically, we use a combination of existing evaluations and a new\nmetric based on the KL-divergence to show that even in non-convex settings, DMM\nachieves strong unlearning performance relative to existing algorithms. An\nadded benefit of DMM is that it is a meta-algorithm, in the sense that future\nadvances in data attribution translate directly into better unlearning\nalgorithms, pointing to a clear direction for future progress in unlearning.\n", "link": "http://arxiv.org/abs/2410.23232v1", "date": "2024-10-30", "relevancy": 2.044, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5432}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.514}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attribute-to-Delete%3A%20Machine%20Unlearning%20via%20Datamodel%20Matching&body=Title%3A%20Attribute-to-Delete%3A%20Machine%20Unlearning%20via%20Datamodel%20Matching%0AAuthor%3A%20Kristian%20Georgiev%20and%20Roy%20Rinberg%20and%20Sung%20Min%20Park%20and%20Shivam%20Garg%20and%20Andrew%20Ilyas%20and%20Aleksander%20Madry%20and%20Seth%20Neel%0AAbstract%3A%20%20%20Machine%20unlearning%20--%20efficiently%20removing%20the%20effect%20of%20a%20small%20%22forget%20set%22%0Aof%20training%20data%20on%20a%20pre-trained%20machine%20learning%20model%20--%20has%20recently%0Aattracted%20significant%20research%20interest.%20Despite%20this%20interest%2C%20however%2C%20recent%0Awork%20shows%20that%20existing%20machine%20unlearning%20techniques%20do%20not%20hold%20up%20to%0Athorough%20evaluation%20in%20non-convex%20settings.%20In%20this%20work%2C%20we%20introduce%20a%20new%0Amachine%20unlearning%20technique%20that%20exhibits%20strong%20empirical%20performance%20even%20in%0Asuch%20challenging%20settings.%20Our%20starting%20point%20is%20the%20perspective%20that%20the%20goal%0Aof%20unlearning%20is%20to%20produce%20a%20model%20whose%20outputs%20are%20statistically%0Aindistinguishable%20from%20those%20of%20a%20model%20re-trained%20on%20all%20but%20the%20forget%20set.%0AThis%20perspective%20naturally%20suggests%20a%20reduction%20from%20the%20unlearning%20problem%20to%0Athat%20of%20data%20attribution%2C%20where%20the%20goal%20is%20to%20predict%20the%20effect%20of%20changing%0Athe%20training%20set%20on%20a%20model%27s%20outputs.%20Thus%20motivated%2C%20we%20propose%20the%20following%0Ameta-algorithm%2C%20which%20we%20call%20Datamodel%20Matching%20%28DMM%29%3A%20given%20a%20trained%20model%2C%0Awe%20%28a%29%20use%20data%20attribution%20to%20predict%20the%20output%20of%20the%20model%20if%20it%20were%0Are-trained%20on%20all%20but%20the%20forget%20set%20points%3B%20then%20%28b%29%20fine-tune%20the%20pre-trained%0Amodel%20to%20match%20these%20predicted%20outputs.%20In%20a%20simple%20convex%20setting%2C%20we%20show%20how%0Athis%20approach%20provably%20outperforms%20a%20variety%20of%20iterative%20unlearning%0Aalgorithms.%20Empirically%2C%20we%20use%20a%20combination%20of%20existing%20evaluations%20and%20a%20new%0Ametric%20based%20on%20the%20KL-divergence%20to%20show%20that%20even%20in%20non-convex%20settings%2C%20DMM%0Aachieves%20strong%20unlearning%20performance%20relative%20to%20existing%20algorithms.%20An%0Aadded%20benefit%20of%20DMM%20is%20that%20it%20is%20a%20meta-algorithm%2C%20in%20the%20sense%20that%20future%0Aadvances%20in%20data%20attribution%20translate%20directly%20into%20better%20unlearning%0Aalgorithms%2C%20pointing%20to%20a%20clear%20direction%20for%20future%20progress%20in%20unlearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttribute-to-Delete%253A%2520Machine%2520Unlearning%2520via%2520Datamodel%2520Matching%26entry.906535625%3DKristian%2520Georgiev%2520and%2520Roy%2520Rinberg%2520and%2520Sung%2520Min%2520Park%2520and%2520Shivam%2520Garg%2520and%2520Andrew%2520Ilyas%2520and%2520Aleksander%2520Madry%2520and%2520Seth%2520Neel%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520--%2520efficiently%2520removing%2520the%2520effect%2520of%2520a%2520small%2520%2522forget%2520set%2522%250Aof%2520training%2520data%2520on%2520a%2520pre-trained%2520machine%2520learning%2520model%2520--%2520has%2520recently%250Aattracted%2520significant%2520research%2520interest.%2520Despite%2520this%2520interest%252C%2520however%252C%2520recent%250Awork%2520shows%2520that%2520existing%2520machine%2520unlearning%2520techniques%2520do%2520not%2520hold%2520up%2520to%250Athorough%2520evaluation%2520in%2520non-convex%2520settings.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520new%250Amachine%2520unlearning%2520technique%2520that%2520exhibits%2520strong%2520empirical%2520performance%2520even%2520in%250Asuch%2520challenging%2520settings.%2520Our%2520starting%2520point%2520is%2520the%2520perspective%2520that%2520the%2520goal%250Aof%2520unlearning%2520is%2520to%2520produce%2520a%2520model%2520whose%2520outputs%2520are%2520statistically%250Aindistinguishable%2520from%2520those%2520of%2520a%2520model%2520re-trained%2520on%2520all%2520but%2520the%2520forget%2520set.%250AThis%2520perspective%2520naturally%2520suggests%2520a%2520reduction%2520from%2520the%2520unlearning%2520problem%2520to%250Athat%2520of%2520data%2520attribution%252C%2520where%2520the%2520goal%2520is%2520to%2520predict%2520the%2520effect%2520of%2520changing%250Athe%2520training%2520set%2520on%2520a%2520model%2527s%2520outputs.%2520Thus%2520motivated%252C%2520we%2520propose%2520the%2520following%250Ameta-algorithm%252C%2520which%2520we%2520call%2520Datamodel%2520Matching%2520%2528DMM%2529%253A%2520given%2520a%2520trained%2520model%252C%250Awe%2520%2528a%2529%2520use%2520data%2520attribution%2520to%2520predict%2520the%2520output%2520of%2520the%2520model%2520if%2520it%2520were%250Are-trained%2520on%2520all%2520but%2520the%2520forget%2520set%2520points%253B%2520then%2520%2528b%2529%2520fine-tune%2520the%2520pre-trained%250Amodel%2520to%2520match%2520these%2520predicted%2520outputs.%2520In%2520a%2520simple%2520convex%2520setting%252C%2520we%2520show%2520how%250Athis%2520approach%2520provably%2520outperforms%2520a%2520variety%2520of%2520iterative%2520unlearning%250Aalgorithms.%2520Empirically%252C%2520we%2520use%2520a%2520combination%2520of%2520existing%2520evaluations%2520and%2520a%2520new%250Ametric%2520based%2520on%2520the%2520KL-divergence%2520to%2520show%2520that%2520even%2520in%2520non-convex%2520settings%252C%2520DMM%250Aachieves%2520strong%2520unlearning%2520performance%2520relative%2520to%2520existing%2520algorithms.%2520An%250Aadded%2520benefit%2520of%2520DMM%2520is%2520that%2520it%2520is%2520a%2520meta-algorithm%252C%2520in%2520the%2520sense%2520that%2520future%250Aadvances%2520in%2520data%2520attribution%2520translate%2520directly%2520into%2520better%2520unlearning%250Aalgorithms%252C%2520pointing%2520to%2520a%2520clear%2520direction%2520for%2520future%2520progress%2520in%2520unlearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attribute-to-Delete%3A%20Machine%20Unlearning%20via%20Datamodel%20Matching&entry.906535625=Kristian%20Georgiev%20and%20Roy%20Rinberg%20and%20Sung%20Min%20Park%20and%20Shivam%20Garg%20and%20Andrew%20Ilyas%20and%20Aleksander%20Madry%20and%20Seth%20Neel&entry.1292438233=%20%20Machine%20unlearning%20--%20efficiently%20removing%20the%20effect%20of%20a%20small%20%22forget%20set%22%0Aof%20training%20data%20on%20a%20pre-trained%20machine%20learning%20model%20--%20has%20recently%0Aattracted%20significant%20research%20interest.%20Despite%20this%20interest%2C%20however%2C%20recent%0Awork%20shows%20that%20existing%20machine%20unlearning%20techniques%20do%20not%20hold%20up%20to%0Athorough%20evaluation%20in%20non-convex%20settings.%20In%20this%20work%2C%20we%20introduce%20a%20new%0Amachine%20unlearning%20technique%20that%20exhibits%20strong%20empirical%20performance%20even%20in%0Asuch%20challenging%20settings.%20Our%20starting%20point%20is%20the%20perspective%20that%20the%20goal%0Aof%20unlearning%20is%20to%20produce%20a%20model%20whose%20outputs%20are%20statistically%0Aindistinguishable%20from%20those%20of%20a%20model%20re-trained%20on%20all%20but%20the%20forget%20set.%0AThis%20perspective%20naturally%20suggests%20a%20reduction%20from%20the%20unlearning%20problem%20to%0Athat%20of%20data%20attribution%2C%20where%20the%20goal%20is%20to%20predict%20the%20effect%20of%20changing%0Athe%20training%20set%20on%20a%20model%27s%20outputs.%20Thus%20motivated%2C%20we%20propose%20the%20following%0Ameta-algorithm%2C%20which%20we%20call%20Datamodel%20Matching%20%28DMM%29%3A%20given%20a%20trained%20model%2C%0Awe%20%28a%29%20use%20data%20attribution%20to%20predict%20the%20output%20of%20the%20model%20if%20it%20were%0Are-trained%20on%20all%20but%20the%20forget%20set%20points%3B%20then%20%28b%29%20fine-tune%20the%20pre-trained%0Amodel%20to%20match%20these%20predicted%20outputs.%20In%20a%20simple%20convex%20setting%2C%20we%20show%20how%0Athis%20approach%20provably%20outperforms%20a%20variety%20of%20iterative%20unlearning%0Aalgorithms.%20Empirically%2C%20we%20use%20a%20combination%20of%20existing%20evaluations%20and%20a%20new%0Ametric%20based%20on%20the%20KL-divergence%20to%20show%20that%20even%20in%20non-convex%20settings%2C%20DMM%0Aachieves%20strong%20unlearning%20performance%20relative%20to%20existing%20algorithms.%20An%0Aadded%20benefit%20of%20DMM%20is%20that%20it%20is%20a%20meta-algorithm%2C%20in%20the%20sense%20that%20future%0Aadvances%20in%20data%20attribution%20translate%20directly%20into%20better%20unlearning%0Aalgorithms%2C%20pointing%20to%20a%20clear%20direction%20for%20future%20progress%20in%20unlearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23232v1&entry.124074799=Read"},
{"title": "Compositional Segmentation of Cardiac Images Leveraging Metadata", "author": "Abbas Khan and Muhammad Asad and Martin Benning and Caroline Roney and Gregory Slabaugh", "abstract": "  Cardiac image segmentation is essential for automated cardiac function\nassessment and monitoring of changes in cardiac structures over time. Inspired\nby coarse-to-fine approaches in image analysis, we propose a novel multitask\ncompositional segmentation approach that can simultaneously localize the heart\nin a cardiac image and perform part-based segmentation of different regions of\ninterest. We demonstrate that this compositional approach achieves better\nresults than direct segmentation of the anatomies. Further, we propose a novel\nCross-Modal Feature Integration (CMFI) module to leverage the metadata related\nto cardiac imaging collected during image acquisition. We perform experiments\non two different modalities, MRI and ultrasound, using public datasets,\nMulti-disease, Multi-View, and Multi-Centre (M&Ms-2) and Multi-structure\nUltrasound Segmentation (CAMUS) data, to showcase the efficiency of the\nproposed compositional segmentation method and Cross-Modal Feature Integration\nmodule incorporating metadata within the proposed compositional segmentation\nnetwork. The source code is available:\nhttps://github.com/kabbas570/CompSeg-MetaData.\n", "link": "http://arxiv.org/abs/2410.23130v1", "date": "2024-10-30", "relevancy": 2.0267, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5161}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5033}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compositional%20Segmentation%20of%20Cardiac%20Images%20Leveraging%20Metadata&body=Title%3A%20Compositional%20Segmentation%20of%20Cardiac%20Images%20Leveraging%20Metadata%0AAuthor%3A%20Abbas%20Khan%20and%20Muhammad%20Asad%20and%20Martin%20Benning%20and%20Caroline%20Roney%20and%20Gregory%20Slabaugh%0AAbstract%3A%20%20%20Cardiac%20image%20segmentation%20is%20essential%20for%20automated%20cardiac%20function%0Aassessment%20and%20monitoring%20of%20changes%20in%20cardiac%20structures%20over%20time.%20Inspired%0Aby%20coarse-to-fine%20approaches%20in%20image%20analysis%2C%20we%20propose%20a%20novel%20multitask%0Acompositional%20segmentation%20approach%20that%20can%20simultaneously%20localize%20the%20heart%0Ain%20a%20cardiac%20image%20and%20perform%20part-based%20segmentation%20of%20different%20regions%20of%0Ainterest.%20We%20demonstrate%20that%20this%20compositional%20approach%20achieves%20better%0Aresults%20than%20direct%20segmentation%20of%20the%20anatomies.%20Further%2C%20we%20propose%20a%20novel%0ACross-Modal%20Feature%20Integration%20%28CMFI%29%20module%20to%20leverage%20the%20metadata%20related%0Ato%20cardiac%20imaging%20collected%20during%20image%20acquisition.%20We%20perform%20experiments%0Aon%20two%20different%20modalities%2C%20MRI%20and%20ultrasound%2C%20using%20public%20datasets%2C%0AMulti-disease%2C%20Multi-View%2C%20and%20Multi-Centre%20%28M%26Ms-2%29%20and%20Multi-structure%0AUltrasound%20Segmentation%20%28CAMUS%29%20data%2C%20to%20showcase%20the%20efficiency%20of%20the%0Aproposed%20compositional%20segmentation%20method%20and%20Cross-Modal%20Feature%20Integration%0Amodule%20incorporating%20metadata%20within%20the%20proposed%20compositional%20segmentation%0Anetwork.%20The%20source%20code%20is%20available%3A%0Ahttps%3A//github.com/kabbas570/CompSeg-MetaData.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompositional%2520Segmentation%2520of%2520Cardiac%2520Images%2520Leveraging%2520Metadata%26entry.906535625%3DAbbas%2520Khan%2520and%2520Muhammad%2520Asad%2520and%2520Martin%2520Benning%2520and%2520Caroline%2520Roney%2520and%2520Gregory%2520Slabaugh%26entry.1292438233%3D%2520%2520Cardiac%2520image%2520segmentation%2520is%2520essential%2520for%2520automated%2520cardiac%2520function%250Aassessment%2520and%2520monitoring%2520of%2520changes%2520in%2520cardiac%2520structures%2520over%2520time.%2520Inspired%250Aby%2520coarse-to-fine%2520approaches%2520in%2520image%2520analysis%252C%2520we%2520propose%2520a%2520novel%2520multitask%250Acompositional%2520segmentation%2520approach%2520that%2520can%2520simultaneously%2520localize%2520the%2520heart%250Ain%2520a%2520cardiac%2520image%2520and%2520perform%2520part-based%2520segmentation%2520of%2520different%2520regions%2520of%250Ainterest.%2520We%2520demonstrate%2520that%2520this%2520compositional%2520approach%2520achieves%2520better%250Aresults%2520than%2520direct%2520segmentation%2520of%2520the%2520anatomies.%2520Further%252C%2520we%2520propose%2520a%2520novel%250ACross-Modal%2520Feature%2520Integration%2520%2528CMFI%2529%2520module%2520to%2520leverage%2520the%2520metadata%2520related%250Ato%2520cardiac%2520imaging%2520collected%2520during%2520image%2520acquisition.%2520We%2520perform%2520experiments%250Aon%2520two%2520different%2520modalities%252C%2520MRI%2520and%2520ultrasound%252C%2520using%2520public%2520datasets%252C%250AMulti-disease%252C%2520Multi-View%252C%2520and%2520Multi-Centre%2520%2528M%2526Ms-2%2529%2520and%2520Multi-structure%250AUltrasound%2520Segmentation%2520%2528CAMUS%2529%2520data%252C%2520to%2520showcase%2520the%2520efficiency%2520of%2520the%250Aproposed%2520compositional%2520segmentation%2520method%2520and%2520Cross-Modal%2520Feature%2520Integration%250Amodule%2520incorporating%2520metadata%2520within%2520the%2520proposed%2520compositional%2520segmentation%250Anetwork.%2520The%2520source%2520code%2520is%2520available%253A%250Ahttps%253A//github.com/kabbas570/CompSeg-MetaData.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compositional%20Segmentation%20of%20Cardiac%20Images%20Leveraging%20Metadata&entry.906535625=Abbas%20Khan%20and%20Muhammad%20Asad%20and%20Martin%20Benning%20and%20Caroline%20Roney%20and%20Gregory%20Slabaugh&entry.1292438233=%20%20Cardiac%20image%20segmentation%20is%20essential%20for%20automated%20cardiac%20function%0Aassessment%20and%20monitoring%20of%20changes%20in%20cardiac%20structures%20over%20time.%20Inspired%0Aby%20coarse-to-fine%20approaches%20in%20image%20analysis%2C%20we%20propose%20a%20novel%20multitask%0Acompositional%20segmentation%20approach%20that%20can%20simultaneously%20localize%20the%20heart%0Ain%20a%20cardiac%20image%20and%20perform%20part-based%20segmentation%20of%20different%20regions%20of%0Ainterest.%20We%20demonstrate%20that%20this%20compositional%20approach%20achieves%20better%0Aresults%20than%20direct%20segmentation%20of%20the%20anatomies.%20Further%2C%20we%20propose%20a%20novel%0ACross-Modal%20Feature%20Integration%20%28CMFI%29%20module%20to%20leverage%20the%20metadata%20related%0Ato%20cardiac%20imaging%20collected%20during%20image%20acquisition.%20We%20perform%20experiments%0Aon%20two%20different%20modalities%2C%20MRI%20and%20ultrasound%2C%20using%20public%20datasets%2C%0AMulti-disease%2C%20Multi-View%2C%20and%20Multi-Centre%20%28M%26Ms-2%29%20and%20Multi-structure%0AUltrasound%20Segmentation%20%28CAMUS%29%20data%2C%20to%20showcase%20the%20efficiency%20of%20the%0Aproposed%20compositional%20segmentation%20method%20and%20Cross-Modal%20Feature%20Integration%0Amodule%20incorporating%20metadata%20within%20the%20proposed%20compositional%20segmentation%0Anetwork.%20The%20source%20code%20is%20available%3A%0Ahttps%3A//github.com/kabbas570/CompSeg-MetaData.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23130v1&entry.124074799=Read"},
{"title": "ProTransformer: Robustify Transformers via Plug-and-Play Paradigm", "author": "Zhichao Hou and Weizhi Gao and Yuchen Shen and Feiyi Wang and Xiaorui Liu", "abstract": "  Transformer-based architectures have dominated various areas of machine\nlearning in recent years. In this paper, we introduce a novel robust attention\nmechanism designed to enhance the resilience of transformer-based\narchitectures. Crucially, this technique can be integrated into existing\ntransformers as a plug-and-play layer, improving their robustness without the\nneed for additional training or fine-tuning. Through comprehensive experiments\nand ablation studies, we demonstrate that our ProTransformer significantly\nenhances the robustness of transformer models across a variety of prediction\ntasks, attack mechanisms, backbone architectures, and data domains. Notably,\nwithout further fine-tuning, the ProTransformer consistently improves the\nperformance of vanilla transformers by 19.5%, 28.3%, 16.1%, and 11.4% for BERT,\nALBERT, DistilBERT, and RoBERTa, respectively, under the classical TextFooler\nattack. Furthermore, ProTransformer shows promising resilience in large\nlanguage models (LLMs) against prompting-based attacks, improving the\nperformance of T5 and LLaMA by 24.8% and 17.8%, respectively, and enhancing\nVicuna by an average of 10.4% against the Jailbreaking attack. Beyond the\nlanguage domain, ProTransformer also demonstrates outstanding robustness in\nboth vision and graph domains.\n", "link": "http://arxiv.org/abs/2410.23182v1", "date": "2024-10-30", "relevancy": 2.0207, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.603}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5009}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProTransformer%3A%20Robustify%20Transformers%20via%20Plug-and-Play%20Paradigm&body=Title%3A%20ProTransformer%3A%20Robustify%20Transformers%20via%20Plug-and-Play%20Paradigm%0AAuthor%3A%20Zhichao%20Hou%20and%20Weizhi%20Gao%20and%20Yuchen%20Shen%20and%20Feiyi%20Wang%20and%20Xiaorui%20Liu%0AAbstract%3A%20%20%20Transformer-based%20architectures%20have%20dominated%20various%20areas%20of%20machine%0Alearning%20in%20recent%20years.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20robust%20attention%0Amechanism%20designed%20to%20enhance%20the%20resilience%20of%20transformer-based%0Aarchitectures.%20Crucially%2C%20this%20technique%20can%20be%20integrated%20into%20existing%0Atransformers%20as%20a%20plug-and-play%20layer%2C%20improving%20their%20robustness%20without%20the%0Aneed%20for%20additional%20training%20or%20fine-tuning.%20Through%20comprehensive%20experiments%0Aand%20ablation%20studies%2C%20we%20demonstrate%20that%20our%20ProTransformer%20significantly%0Aenhances%20the%20robustness%20of%20transformer%20models%20across%20a%20variety%20of%20prediction%0Atasks%2C%20attack%20mechanisms%2C%20backbone%20architectures%2C%20and%20data%20domains.%20Notably%2C%0Awithout%20further%20fine-tuning%2C%20the%20ProTransformer%20consistently%20improves%20the%0Aperformance%20of%20vanilla%20transformers%20by%2019.5%25%2C%2028.3%25%2C%2016.1%25%2C%20and%2011.4%25%20for%20BERT%2C%0AALBERT%2C%20DistilBERT%2C%20and%20RoBERTa%2C%20respectively%2C%20under%20the%20classical%20TextFooler%0Aattack.%20Furthermore%2C%20ProTransformer%20shows%20promising%20resilience%20in%20large%0Alanguage%20models%20%28LLMs%29%20against%20prompting-based%20attacks%2C%20improving%20the%0Aperformance%20of%20T5%20and%20LLaMA%20by%2024.8%25%20and%2017.8%25%2C%20respectively%2C%20and%20enhancing%0AVicuna%20by%20an%20average%20of%2010.4%25%20against%20the%20Jailbreaking%20attack.%20Beyond%20the%0Alanguage%20domain%2C%20ProTransformer%20also%20demonstrates%20outstanding%20robustness%20in%0Aboth%20vision%20and%20graph%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23182v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProTransformer%253A%2520Robustify%2520Transformers%2520via%2520Plug-and-Play%2520Paradigm%26entry.906535625%3DZhichao%2520Hou%2520and%2520Weizhi%2520Gao%2520and%2520Yuchen%2520Shen%2520and%2520Feiyi%2520Wang%2520and%2520Xiaorui%2520Liu%26entry.1292438233%3D%2520%2520Transformer-based%2520architectures%2520have%2520dominated%2520various%2520areas%2520of%2520machine%250Alearning%2520in%2520recent%2520years.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520robust%2520attention%250Amechanism%2520designed%2520to%2520enhance%2520the%2520resilience%2520of%2520transformer-based%250Aarchitectures.%2520Crucially%252C%2520this%2520technique%2520can%2520be%2520integrated%2520into%2520existing%250Atransformers%2520as%2520a%2520plug-and-play%2520layer%252C%2520improving%2520their%2520robustness%2520without%2520the%250Aneed%2520for%2520additional%2520training%2520or%2520fine-tuning.%2520Through%2520comprehensive%2520experiments%250Aand%2520ablation%2520studies%252C%2520we%2520demonstrate%2520that%2520our%2520ProTransformer%2520significantly%250Aenhances%2520the%2520robustness%2520of%2520transformer%2520models%2520across%2520a%2520variety%2520of%2520prediction%250Atasks%252C%2520attack%2520mechanisms%252C%2520backbone%2520architectures%252C%2520and%2520data%2520domains.%2520Notably%252C%250Awithout%2520further%2520fine-tuning%252C%2520the%2520ProTransformer%2520consistently%2520improves%2520the%250Aperformance%2520of%2520vanilla%2520transformers%2520by%252019.5%2525%252C%252028.3%2525%252C%252016.1%2525%252C%2520and%252011.4%2525%2520for%2520BERT%252C%250AALBERT%252C%2520DistilBERT%252C%2520and%2520RoBERTa%252C%2520respectively%252C%2520under%2520the%2520classical%2520TextFooler%250Aattack.%2520Furthermore%252C%2520ProTransformer%2520shows%2520promising%2520resilience%2520in%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520against%2520prompting-based%2520attacks%252C%2520improving%2520the%250Aperformance%2520of%2520T5%2520and%2520LLaMA%2520by%252024.8%2525%2520and%252017.8%2525%252C%2520respectively%252C%2520and%2520enhancing%250AVicuna%2520by%2520an%2520average%2520of%252010.4%2525%2520against%2520the%2520Jailbreaking%2520attack.%2520Beyond%2520the%250Alanguage%2520domain%252C%2520ProTransformer%2520also%2520demonstrates%2520outstanding%2520robustness%2520in%250Aboth%2520vision%2520and%2520graph%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23182v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProTransformer%3A%20Robustify%20Transformers%20via%20Plug-and-Play%20Paradigm&entry.906535625=Zhichao%20Hou%20and%20Weizhi%20Gao%20and%20Yuchen%20Shen%20and%20Feiyi%20Wang%20and%20Xiaorui%20Liu&entry.1292438233=%20%20Transformer-based%20architectures%20have%20dominated%20various%20areas%20of%20machine%0Alearning%20in%20recent%20years.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20robust%20attention%0Amechanism%20designed%20to%20enhance%20the%20resilience%20of%20transformer-based%0Aarchitectures.%20Crucially%2C%20this%20technique%20can%20be%20integrated%20into%20existing%0Atransformers%20as%20a%20plug-and-play%20layer%2C%20improving%20their%20robustness%20without%20the%0Aneed%20for%20additional%20training%20or%20fine-tuning.%20Through%20comprehensive%20experiments%0Aand%20ablation%20studies%2C%20we%20demonstrate%20that%20our%20ProTransformer%20significantly%0Aenhances%20the%20robustness%20of%20transformer%20models%20across%20a%20variety%20of%20prediction%0Atasks%2C%20attack%20mechanisms%2C%20backbone%20architectures%2C%20and%20data%20domains.%20Notably%2C%0Awithout%20further%20fine-tuning%2C%20the%20ProTransformer%20consistently%20improves%20the%0Aperformance%20of%20vanilla%20transformers%20by%2019.5%25%2C%2028.3%25%2C%2016.1%25%2C%20and%2011.4%25%20for%20BERT%2C%0AALBERT%2C%20DistilBERT%2C%20and%20RoBERTa%2C%20respectively%2C%20under%20the%20classical%20TextFooler%0Aattack.%20Furthermore%2C%20ProTransformer%20shows%20promising%20resilience%20in%20large%0Alanguage%20models%20%28LLMs%29%20against%20prompting-based%20attacks%2C%20improving%20the%0Aperformance%20of%20T5%20and%20LLaMA%20by%2024.8%25%20and%2017.8%25%2C%20respectively%2C%20and%20enhancing%0AVicuna%20by%20an%20average%20of%2010.4%25%20against%20the%20Jailbreaking%20attack.%20Beyond%20the%0Alanguage%20domain%2C%20ProTransformer%20also%20demonstrates%20outstanding%20robustness%20in%0Aboth%20vision%20and%20graph%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23182v1&entry.124074799=Read"},
{"title": "Aligning Audio-Visual Joint Representations with an Agentic Workflow", "author": "Shentong Mo and Yibing Song", "abstract": "  Visual content and accompanied audio signals naturally formulate a joint\nrepresentation to improve audio-visual (AV) related applications. While studies\ndevelop various AV representation learning frameworks, the importance of AV\ndata alignment is usually undermined for achieving high-quality representation.\nWe observe that an audio signal may contain background noise interference.\nAlso, non-synchronization may appear between audio and video streams. These\nnon-strict data alignment limits representation quality and downgrade\napplication performance. In this paper, we propose to improve AV joint\nrepresentations from a data-centric perspective by aligning audio signals to\nvisual data. Our alignment is conducted in an agentic workflow controlled by an\nLLM-based assistant named AVAgent. For each input AV data pair, our AVAgent\nuses a multi-modal LLM to convert audio and visual data into language\ndescriptions separately (i.e., tool use). Then, AVAgent reasons whether this\npaired data is aligned well and plans to edit the audio signal if needed (i.e.,\nplanning). The audio editing is executed by predefined actions that filter\nnoise or augment data. Moreover, we use a VLM to evaluate how modified audio\nsignals match the visual content and provide feedback to AVAgent (i.e.,\nreflection). The tool use, planning, and reflection steps operate cyclically to\nbecome an agentic workflow where audio signals are gradually aligned to visual\ncontent. To this end, existing methods can directly leverage the aligned AV\ndata via our agentic workflow to improve AV joint representations. The\nexperimental results comprehensively demonstrate the state-of-the-art\nperformance of the proposed approach against previous baselines in diverse\ndownstream tasks.\n", "link": "http://arxiv.org/abs/2410.23230v1", "date": "2024-10-30", "relevancy": 2.016, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5122}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4991}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Audio-Visual%20Joint%20Representations%20with%20an%20Agentic%20Workflow&body=Title%3A%20Aligning%20Audio-Visual%20Joint%20Representations%20with%20an%20Agentic%20Workflow%0AAuthor%3A%20Shentong%20Mo%20and%20Yibing%20Song%0AAbstract%3A%20%20%20Visual%20content%20and%20accompanied%20audio%20signals%20naturally%20formulate%20a%20joint%0Arepresentation%20to%20improve%20audio-visual%20%28AV%29%20related%20applications.%20While%20studies%0Adevelop%20various%20AV%20representation%20learning%20frameworks%2C%20the%20importance%20of%20AV%0Adata%20alignment%20is%20usually%20undermined%20for%20achieving%20high-quality%20representation.%0AWe%20observe%20that%20an%20audio%20signal%20may%20contain%20background%20noise%20interference.%0AAlso%2C%20non-synchronization%20may%20appear%20between%20audio%20and%20video%20streams.%20These%0Anon-strict%20data%20alignment%20limits%20representation%20quality%20and%20downgrade%0Aapplication%20performance.%20In%20this%20paper%2C%20we%20propose%20to%20improve%20AV%20joint%0Arepresentations%20from%20a%20data-centric%20perspective%20by%20aligning%20audio%20signals%20to%0Avisual%20data.%20Our%20alignment%20is%20conducted%20in%20an%20agentic%20workflow%20controlled%20by%20an%0ALLM-based%20assistant%20named%20AVAgent.%20For%20each%20input%20AV%20data%20pair%2C%20our%20AVAgent%0Auses%20a%20multi-modal%20LLM%20to%20convert%20audio%20and%20visual%20data%20into%20language%0Adescriptions%20separately%20%28i.e.%2C%20tool%20use%29.%20Then%2C%20AVAgent%20reasons%20whether%20this%0Apaired%20data%20is%20aligned%20well%20and%20plans%20to%20edit%20the%20audio%20signal%20if%20needed%20%28i.e.%2C%0Aplanning%29.%20The%20audio%20editing%20is%20executed%20by%20predefined%20actions%20that%20filter%0Anoise%20or%20augment%20data.%20Moreover%2C%20we%20use%20a%20VLM%20to%20evaluate%20how%20modified%20audio%0Asignals%20match%20the%20visual%20content%20and%20provide%20feedback%20to%20AVAgent%20%28i.e.%2C%0Areflection%29.%20The%20tool%20use%2C%20planning%2C%20and%20reflection%20steps%20operate%20cyclically%20to%0Abecome%20an%20agentic%20workflow%20where%20audio%20signals%20are%20gradually%20aligned%20to%20visual%0Acontent.%20To%20this%20end%2C%20existing%20methods%20can%20directly%20leverage%20the%20aligned%20AV%0Adata%20via%20our%20agentic%20workflow%20to%20improve%20AV%20joint%20representations.%20The%0Aexperimental%20results%20comprehensively%20demonstrate%20the%20state-of-the-art%0Aperformance%20of%20the%20proposed%20approach%20against%20previous%20baselines%20in%20diverse%0Adownstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Audio-Visual%2520Joint%2520Representations%2520with%2520an%2520Agentic%2520Workflow%26entry.906535625%3DShentong%2520Mo%2520and%2520Yibing%2520Song%26entry.1292438233%3D%2520%2520Visual%2520content%2520and%2520accompanied%2520audio%2520signals%2520naturally%2520formulate%2520a%2520joint%250Arepresentation%2520to%2520improve%2520audio-visual%2520%2528AV%2529%2520related%2520applications.%2520While%2520studies%250Adevelop%2520various%2520AV%2520representation%2520learning%2520frameworks%252C%2520the%2520importance%2520of%2520AV%250Adata%2520alignment%2520is%2520usually%2520undermined%2520for%2520achieving%2520high-quality%2520representation.%250AWe%2520observe%2520that%2520an%2520audio%2520signal%2520may%2520contain%2520background%2520noise%2520interference.%250AAlso%252C%2520non-synchronization%2520may%2520appear%2520between%2520audio%2520and%2520video%2520streams.%2520These%250Anon-strict%2520data%2520alignment%2520limits%2520representation%2520quality%2520and%2520downgrade%250Aapplication%2520performance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520improve%2520AV%2520joint%250Arepresentations%2520from%2520a%2520data-centric%2520perspective%2520by%2520aligning%2520audio%2520signals%2520to%250Avisual%2520data.%2520Our%2520alignment%2520is%2520conducted%2520in%2520an%2520agentic%2520workflow%2520controlled%2520by%2520an%250ALLM-based%2520assistant%2520named%2520AVAgent.%2520For%2520each%2520input%2520AV%2520data%2520pair%252C%2520our%2520AVAgent%250Auses%2520a%2520multi-modal%2520LLM%2520to%2520convert%2520audio%2520and%2520visual%2520data%2520into%2520language%250Adescriptions%2520separately%2520%2528i.e.%252C%2520tool%2520use%2529.%2520Then%252C%2520AVAgent%2520reasons%2520whether%2520this%250Apaired%2520data%2520is%2520aligned%2520well%2520and%2520plans%2520to%2520edit%2520the%2520audio%2520signal%2520if%2520needed%2520%2528i.e.%252C%250Aplanning%2529.%2520The%2520audio%2520editing%2520is%2520executed%2520by%2520predefined%2520actions%2520that%2520filter%250Anoise%2520or%2520augment%2520data.%2520Moreover%252C%2520we%2520use%2520a%2520VLM%2520to%2520evaluate%2520how%2520modified%2520audio%250Asignals%2520match%2520the%2520visual%2520content%2520and%2520provide%2520feedback%2520to%2520AVAgent%2520%2528i.e.%252C%250Areflection%2529.%2520The%2520tool%2520use%252C%2520planning%252C%2520and%2520reflection%2520steps%2520operate%2520cyclically%2520to%250Abecome%2520an%2520agentic%2520workflow%2520where%2520audio%2520signals%2520are%2520gradually%2520aligned%2520to%2520visual%250Acontent.%2520To%2520this%2520end%252C%2520existing%2520methods%2520can%2520directly%2520leverage%2520the%2520aligned%2520AV%250Adata%2520via%2520our%2520agentic%2520workflow%2520to%2520improve%2520AV%2520joint%2520representations.%2520The%250Aexperimental%2520results%2520comprehensively%2520demonstrate%2520the%2520state-of-the-art%250Aperformance%2520of%2520the%2520proposed%2520approach%2520against%2520previous%2520baselines%2520in%2520diverse%250Adownstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Audio-Visual%20Joint%20Representations%20with%20an%20Agentic%20Workflow&entry.906535625=Shentong%20Mo%20and%20Yibing%20Song&entry.1292438233=%20%20Visual%20content%20and%20accompanied%20audio%20signals%20naturally%20formulate%20a%20joint%0Arepresentation%20to%20improve%20audio-visual%20%28AV%29%20related%20applications.%20While%20studies%0Adevelop%20various%20AV%20representation%20learning%20frameworks%2C%20the%20importance%20of%20AV%0Adata%20alignment%20is%20usually%20undermined%20for%20achieving%20high-quality%20representation.%0AWe%20observe%20that%20an%20audio%20signal%20may%20contain%20background%20noise%20interference.%0AAlso%2C%20non-synchronization%20may%20appear%20between%20audio%20and%20video%20streams.%20These%0Anon-strict%20data%20alignment%20limits%20representation%20quality%20and%20downgrade%0Aapplication%20performance.%20In%20this%20paper%2C%20we%20propose%20to%20improve%20AV%20joint%0Arepresentations%20from%20a%20data-centric%20perspective%20by%20aligning%20audio%20signals%20to%0Avisual%20data.%20Our%20alignment%20is%20conducted%20in%20an%20agentic%20workflow%20controlled%20by%20an%0ALLM-based%20assistant%20named%20AVAgent.%20For%20each%20input%20AV%20data%20pair%2C%20our%20AVAgent%0Auses%20a%20multi-modal%20LLM%20to%20convert%20audio%20and%20visual%20data%20into%20language%0Adescriptions%20separately%20%28i.e.%2C%20tool%20use%29.%20Then%2C%20AVAgent%20reasons%20whether%20this%0Apaired%20data%20is%20aligned%20well%20and%20plans%20to%20edit%20the%20audio%20signal%20if%20needed%20%28i.e.%2C%0Aplanning%29.%20The%20audio%20editing%20is%20executed%20by%20predefined%20actions%20that%20filter%0Anoise%20or%20augment%20data.%20Moreover%2C%20we%20use%20a%20VLM%20to%20evaluate%20how%20modified%20audio%0Asignals%20match%20the%20visual%20content%20and%20provide%20feedback%20to%20AVAgent%20%28i.e.%2C%0Areflection%29.%20The%20tool%20use%2C%20planning%2C%20and%20reflection%20steps%20operate%20cyclically%20to%0Abecome%20an%20agentic%20workflow%20where%20audio%20signals%20are%20gradually%20aligned%20to%20visual%0Acontent.%20To%20this%20end%2C%20existing%20methods%20can%20directly%20leverage%20the%20aligned%20AV%0Adata%20via%20our%20agentic%20workflow%20to%20improve%20AV%20joint%20representations.%20The%0Aexperimental%20results%20comprehensively%20demonstrate%20the%20state-of-the-art%0Aperformance%20of%20the%20proposed%20approach%20against%20previous%20baselines%20in%20diverse%0Adownstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23230v1&entry.124074799=Read"},
{"title": "A Survey Analyzing Generalization in Deep Reinforcement Learning", "author": "Ezgi Korkmaz", "abstract": "  Reinforcement learning research obtained significant success and attention\nwith the utilization of deep neural networks to solve problems in high\ndimensional state or action spaces. While deep reinforcement learning policies\nare currently being deployed in many different fields from medical applications\nto large language models, there are still ongoing questions the field is trying\nto answer on the generalization capabilities of deep reinforcement learning\npolicies. In this paper, we will formalize and analyze generalization in deep\nreinforcement learning. We will explain the fundamental reasons why deep\nreinforcement learning policies encounter overfitting problems that limit their\ngeneralization capabilities. Furthermore, we will categorize and explain the\nmanifold solution approaches to increase generalization, and overcome\noverfitting in deep reinforcement learning policies. From exploration to\nadversarial analysis and from regularization to robustness our paper provides\nan analysis on a wide range of subfields within deep reinforcement learning\nwith a broad scope and in-depth view. We believe our study can provide a\ncompact guideline for the current advancements in deep reinforcement learning,\nand help to construct robust deep neural policies with higher generalization\nskills.\n", "link": "http://arxiv.org/abs/2401.02349v2", "date": "2024-10-30", "relevancy": 1.968, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4981}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4918}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20Analyzing%20Generalization%20in%20Deep%20Reinforcement%20Learning&body=Title%3A%20A%20Survey%20Analyzing%20Generalization%20in%20Deep%20Reinforcement%20Learning%0AAuthor%3A%20Ezgi%20Korkmaz%0AAbstract%3A%20%20%20Reinforcement%20learning%20research%20obtained%20significant%20success%20and%20attention%0Awith%20the%20utilization%20of%20deep%20neural%20networks%20to%20solve%20problems%20in%20high%0Adimensional%20state%20or%20action%20spaces.%20While%20deep%20reinforcement%20learning%20policies%0Aare%20currently%20being%20deployed%20in%20many%20different%20fields%20from%20medical%20applications%0Ato%20large%20language%20models%2C%20there%20are%20still%20ongoing%20questions%20the%20field%20is%20trying%0Ato%20answer%20on%20the%20generalization%20capabilities%20of%20deep%20reinforcement%20learning%0Apolicies.%20In%20this%20paper%2C%20we%20will%20formalize%20and%20analyze%20generalization%20in%20deep%0Areinforcement%20learning.%20We%20will%20explain%20the%20fundamental%20reasons%20why%20deep%0Areinforcement%20learning%20policies%20encounter%20overfitting%20problems%20that%20limit%20their%0Ageneralization%20capabilities.%20Furthermore%2C%20we%20will%20categorize%20and%20explain%20the%0Amanifold%20solution%20approaches%20to%20increase%20generalization%2C%20and%20overcome%0Aoverfitting%20in%20deep%20reinforcement%20learning%20policies.%20From%20exploration%20to%0Aadversarial%20analysis%20and%20from%20regularization%20to%20robustness%20our%20paper%20provides%0Aan%20analysis%20on%20a%20wide%20range%20of%20subfields%20within%20deep%20reinforcement%20learning%0Awith%20a%20broad%20scope%20and%20in-depth%20view.%20We%20believe%20our%20study%20can%20provide%20a%0Acompact%20guideline%20for%20the%20current%20advancements%20in%20deep%20reinforcement%20learning%2C%0Aand%20help%20to%20construct%20robust%20deep%20neural%20policies%20with%20higher%20generalization%0Askills.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.02349v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520Analyzing%2520Generalization%2520in%2520Deep%2520Reinforcement%2520Learning%26entry.906535625%3DEzgi%2520Korkmaz%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520research%2520obtained%2520significant%2520success%2520and%2520attention%250Awith%2520the%2520utilization%2520of%2520deep%2520neural%2520networks%2520to%2520solve%2520problems%2520in%2520high%250Adimensional%2520state%2520or%2520action%2520spaces.%2520While%2520deep%2520reinforcement%2520learning%2520policies%250Aare%2520currently%2520being%2520deployed%2520in%2520many%2520different%2520fields%2520from%2520medical%2520applications%250Ato%2520large%2520language%2520models%252C%2520there%2520are%2520still%2520ongoing%2520questions%2520the%2520field%2520is%2520trying%250Ato%2520answer%2520on%2520the%2520generalization%2520capabilities%2520of%2520deep%2520reinforcement%2520learning%250Apolicies.%2520In%2520this%2520paper%252C%2520we%2520will%2520formalize%2520and%2520analyze%2520generalization%2520in%2520deep%250Areinforcement%2520learning.%2520We%2520will%2520explain%2520the%2520fundamental%2520reasons%2520why%2520deep%250Areinforcement%2520learning%2520policies%2520encounter%2520overfitting%2520problems%2520that%2520limit%2520their%250Ageneralization%2520capabilities.%2520Furthermore%252C%2520we%2520will%2520categorize%2520and%2520explain%2520the%250Amanifold%2520solution%2520approaches%2520to%2520increase%2520generalization%252C%2520and%2520overcome%250Aoverfitting%2520in%2520deep%2520reinforcement%2520learning%2520policies.%2520From%2520exploration%2520to%250Aadversarial%2520analysis%2520and%2520from%2520regularization%2520to%2520robustness%2520our%2520paper%2520provides%250Aan%2520analysis%2520on%2520a%2520wide%2520range%2520of%2520subfields%2520within%2520deep%2520reinforcement%2520learning%250Awith%2520a%2520broad%2520scope%2520and%2520in-depth%2520view.%2520We%2520believe%2520our%2520study%2520can%2520provide%2520a%250Acompact%2520guideline%2520for%2520the%2520current%2520advancements%2520in%2520deep%2520reinforcement%2520learning%252C%250Aand%2520help%2520to%2520construct%2520robust%2520deep%2520neural%2520policies%2520with%2520higher%2520generalization%250Askills.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.02349v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20Analyzing%20Generalization%20in%20Deep%20Reinforcement%20Learning&entry.906535625=Ezgi%20Korkmaz&entry.1292438233=%20%20Reinforcement%20learning%20research%20obtained%20significant%20success%20and%20attention%0Awith%20the%20utilization%20of%20deep%20neural%20networks%20to%20solve%20problems%20in%20high%0Adimensional%20state%20or%20action%20spaces.%20While%20deep%20reinforcement%20learning%20policies%0Aare%20currently%20being%20deployed%20in%20many%20different%20fields%20from%20medical%20applications%0Ato%20large%20language%20models%2C%20there%20are%20still%20ongoing%20questions%20the%20field%20is%20trying%0Ato%20answer%20on%20the%20generalization%20capabilities%20of%20deep%20reinforcement%20learning%0Apolicies.%20In%20this%20paper%2C%20we%20will%20formalize%20and%20analyze%20generalization%20in%20deep%0Areinforcement%20learning.%20We%20will%20explain%20the%20fundamental%20reasons%20why%20deep%0Areinforcement%20learning%20policies%20encounter%20overfitting%20problems%20that%20limit%20their%0Ageneralization%20capabilities.%20Furthermore%2C%20we%20will%20categorize%20and%20explain%20the%0Amanifold%20solution%20approaches%20to%20increase%20generalization%2C%20and%20overcome%0Aoverfitting%20in%20deep%20reinforcement%20learning%20policies.%20From%20exploration%20to%0Aadversarial%20analysis%20and%20from%20regularization%20to%20robustness%20our%20paper%20provides%0Aan%20analysis%20on%20a%20wide%20range%20of%20subfields%20within%20deep%20reinforcement%20learning%0Awith%20a%20broad%20scope%20and%20in-depth%20view.%20We%20believe%20our%20study%20can%20provide%20a%0Acompact%20guideline%20for%20the%20current%20advancements%20in%20deep%20reinforcement%20learning%2C%0Aand%20help%20to%20construct%20robust%20deep%20neural%20policies%20with%20higher%20generalization%0Askills.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.02349v2&entry.124074799=Read"},
{"title": "U-DiTs: Downsample Tokens in U-Shaped Diffusion Transformers", "author": "Yuchuan Tian and Zhijun Tu and Hanting Chen and Jie Hu and Chao Xu and Yunhe Wang", "abstract": "  Diffusion Transformers (DiTs) introduce the transformer architecture to\ndiffusion tasks for latent-space image generation. With an isotropic\narchitecture that chains a series of transformer blocks, DiTs demonstrate\ncompetitive performance and good scalability; but meanwhile, the abandonment of\nU-Net by DiTs and their following improvements is worth rethinking. To this\nend, we conduct a simple toy experiment by comparing a U-Net architectured DiT\nwith an isotropic one. It turns out that the U-Net architecture only gain a\nslight advantage amid the U-Net inductive bias, indicating potential\nredundancies within the U-Net-style DiT. Inspired by the discovery that U-Net\nbackbone features are low-frequency-dominated, we perform token downsampling on\nthe query-key-value tuple for self-attention that bring further improvements\ndespite a considerable amount of reduction in computation. Based on\nself-attention with downsampled tokens, we propose a series of U-shaped DiTs\n(U-DiTs) in the paper and conduct extensive experiments to demonstrate the\nextraordinary performance of U-DiT models. The proposed U-DiT could outperform\nDiT-XL/2 with only 1/6 of its computation cost. Codes are available at\nhttps://github.com/YuchuanTian/U-DiT.\n", "link": "http://arxiv.org/abs/2405.02730v3", "date": "2024-10-30", "relevancy": 1.9639, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6918}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.617}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20U-DiTs%3A%20Downsample%20Tokens%20in%20U-Shaped%20Diffusion%20Transformers&body=Title%3A%20U-DiTs%3A%20Downsample%20Tokens%20in%20U-Shaped%20Diffusion%20Transformers%0AAuthor%3A%20Yuchuan%20Tian%20and%20Zhijun%20Tu%20and%20Hanting%20Chen%20and%20Jie%20Hu%20and%20Chao%20Xu%20and%20Yunhe%20Wang%0AAbstract%3A%20%20%20Diffusion%20Transformers%20%28DiTs%29%20introduce%20the%20transformer%20architecture%20to%0Adiffusion%20tasks%20for%20latent-space%20image%20generation.%20With%20an%20isotropic%0Aarchitecture%20that%20chains%20a%20series%20of%20transformer%20blocks%2C%20DiTs%20demonstrate%0Acompetitive%20performance%20and%20good%20scalability%3B%20but%20meanwhile%2C%20the%20abandonment%20of%0AU-Net%20by%20DiTs%20and%20their%20following%20improvements%20is%20worth%20rethinking.%20To%20this%0Aend%2C%20we%20conduct%20a%20simple%20toy%20experiment%20by%20comparing%20a%20U-Net%20architectured%20DiT%0Awith%20an%20isotropic%20one.%20It%20turns%20out%20that%20the%20U-Net%20architecture%20only%20gain%20a%0Aslight%20advantage%20amid%20the%20U-Net%20inductive%20bias%2C%20indicating%20potential%0Aredundancies%20within%20the%20U-Net-style%20DiT.%20Inspired%20by%20the%20discovery%20that%20U-Net%0Abackbone%20features%20are%20low-frequency-dominated%2C%20we%20perform%20token%20downsampling%20on%0Athe%20query-key-value%20tuple%20for%20self-attention%20that%20bring%20further%20improvements%0Adespite%20a%20considerable%20amount%20of%20reduction%20in%20computation.%20Based%20on%0Aself-attention%20with%20downsampled%20tokens%2C%20we%20propose%20a%20series%20of%20U-shaped%20DiTs%0A%28U-DiTs%29%20in%20the%20paper%20and%20conduct%20extensive%20experiments%20to%20demonstrate%20the%0Aextraordinary%20performance%20of%20U-DiT%20models.%20The%20proposed%20U-DiT%20could%20outperform%0ADiT-XL/2%20with%20only%201/6%20of%20its%20computation%20cost.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/YuchuanTian/U-DiT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02730v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DU-DiTs%253A%2520Downsample%2520Tokens%2520in%2520U-Shaped%2520Diffusion%2520Transformers%26entry.906535625%3DYuchuan%2520Tian%2520and%2520Zhijun%2520Tu%2520and%2520Hanting%2520Chen%2520and%2520Jie%2520Hu%2520and%2520Chao%2520Xu%2520and%2520Yunhe%2520Wang%26entry.1292438233%3D%2520%2520Diffusion%2520Transformers%2520%2528DiTs%2529%2520introduce%2520the%2520transformer%2520architecture%2520to%250Adiffusion%2520tasks%2520for%2520latent-space%2520image%2520generation.%2520With%2520an%2520isotropic%250Aarchitecture%2520that%2520chains%2520a%2520series%2520of%2520transformer%2520blocks%252C%2520DiTs%2520demonstrate%250Acompetitive%2520performance%2520and%2520good%2520scalability%253B%2520but%2520meanwhile%252C%2520the%2520abandonment%2520of%250AU-Net%2520by%2520DiTs%2520and%2520their%2520following%2520improvements%2520is%2520worth%2520rethinking.%2520To%2520this%250Aend%252C%2520we%2520conduct%2520a%2520simple%2520toy%2520experiment%2520by%2520comparing%2520a%2520U-Net%2520architectured%2520DiT%250Awith%2520an%2520isotropic%2520one.%2520It%2520turns%2520out%2520that%2520the%2520U-Net%2520architecture%2520only%2520gain%2520a%250Aslight%2520advantage%2520amid%2520the%2520U-Net%2520inductive%2520bias%252C%2520indicating%2520potential%250Aredundancies%2520within%2520the%2520U-Net-style%2520DiT.%2520Inspired%2520by%2520the%2520discovery%2520that%2520U-Net%250Abackbone%2520features%2520are%2520low-frequency-dominated%252C%2520we%2520perform%2520token%2520downsampling%2520on%250Athe%2520query-key-value%2520tuple%2520for%2520self-attention%2520that%2520bring%2520further%2520improvements%250Adespite%2520a%2520considerable%2520amount%2520of%2520reduction%2520in%2520computation.%2520Based%2520on%250Aself-attention%2520with%2520downsampled%2520tokens%252C%2520we%2520propose%2520a%2520series%2520of%2520U-shaped%2520DiTs%250A%2528U-DiTs%2529%2520in%2520the%2520paper%2520and%2520conduct%2520extensive%2520experiments%2520to%2520demonstrate%2520the%250Aextraordinary%2520performance%2520of%2520U-DiT%2520models.%2520The%2520proposed%2520U-DiT%2520could%2520outperform%250ADiT-XL/2%2520with%2520only%25201/6%2520of%2520its%2520computation%2520cost.%2520Codes%2520are%2520available%2520at%250Ahttps%253A//github.com/YuchuanTian/U-DiT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02730v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=U-DiTs%3A%20Downsample%20Tokens%20in%20U-Shaped%20Diffusion%20Transformers&entry.906535625=Yuchuan%20Tian%20and%20Zhijun%20Tu%20and%20Hanting%20Chen%20and%20Jie%20Hu%20and%20Chao%20Xu%20and%20Yunhe%20Wang&entry.1292438233=%20%20Diffusion%20Transformers%20%28DiTs%29%20introduce%20the%20transformer%20architecture%20to%0Adiffusion%20tasks%20for%20latent-space%20image%20generation.%20With%20an%20isotropic%0Aarchitecture%20that%20chains%20a%20series%20of%20transformer%20blocks%2C%20DiTs%20demonstrate%0Acompetitive%20performance%20and%20good%20scalability%3B%20but%20meanwhile%2C%20the%20abandonment%20of%0AU-Net%20by%20DiTs%20and%20their%20following%20improvements%20is%20worth%20rethinking.%20To%20this%0Aend%2C%20we%20conduct%20a%20simple%20toy%20experiment%20by%20comparing%20a%20U-Net%20architectured%20DiT%0Awith%20an%20isotropic%20one.%20It%20turns%20out%20that%20the%20U-Net%20architecture%20only%20gain%20a%0Aslight%20advantage%20amid%20the%20U-Net%20inductive%20bias%2C%20indicating%20potential%0Aredundancies%20within%20the%20U-Net-style%20DiT.%20Inspired%20by%20the%20discovery%20that%20U-Net%0Abackbone%20features%20are%20low-frequency-dominated%2C%20we%20perform%20token%20downsampling%20on%0Athe%20query-key-value%20tuple%20for%20self-attention%20that%20bring%20further%20improvements%0Adespite%20a%20considerable%20amount%20of%20reduction%20in%20computation.%20Based%20on%0Aself-attention%20with%20downsampled%20tokens%2C%20we%20propose%20a%20series%20of%20U-shaped%20DiTs%0A%28U-DiTs%29%20in%20the%20paper%20and%20conduct%20extensive%20experiments%20to%20demonstrate%20the%0Aextraordinary%20performance%20of%20U-DiT%20models.%20The%20proposed%20U-DiT%20could%20outperform%0ADiT-XL/2%20with%20only%201/6%20of%20its%20computation%20cost.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/YuchuanTian/U-DiT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02730v3&entry.124074799=Read"},
{"title": "Data Contamination Can Cross Language Barriers", "author": "Feng Yao and Yufan Zhuang and Zihao Sun and Sunan Xu and Animesh Kumar and Jingbo Shang", "abstract": "  The opacity in developing large language models (LLMs) is raising growing\nconcerns about the potential contamination of public benchmarks in the\npre-training data. Existing contamination detection methods are typically based\non the text overlap between training and evaluation data, which can be too\nsuperficial to reflect deeper forms of contamination. In this paper, we first\npresent a cross-lingual form of contamination that inflates LLMs' performance\nwhile evading current detection methods, deliberately injected by overfitting\nLLMs on the translated versions of benchmark test sets. Then, we propose\ngeneralization-based approaches to unmask such deeply concealed contamination.\nSpecifically, we examine the LLM's performance change after modifying the\noriginal benchmark by replacing the false answer choices with correct ones from\nother questions. Contaminated models can hardly generalize to such easier\nsituations, where the false choices can be \\emph{not even wrong}, as all\nchoices are correct in their memorization. Experimental results demonstrate\nthat cross-lingual contamination can easily fool existing detection methods,\nbut not ours. In addition, we discuss the potential utilization of\ncross-lingual contamination in interpreting LLMs' working mechanisms and in\npost-training LLMs for enhanced multilingual capabilities. The code and dataset\nwe use can be obtained from \\url{https://github.com/ShangDataLab/Deep-Contam}.\n", "link": "http://arxiv.org/abs/2406.13236v2", "date": "2024-10-30", "relevancy": 1.9531, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5296}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.48}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Contamination%20Can%20Cross%20Language%20Barriers&body=Title%3A%20Data%20Contamination%20Can%20Cross%20Language%20Barriers%0AAuthor%3A%20Feng%20Yao%20and%20Yufan%20Zhuang%20and%20Zihao%20Sun%20and%20Sunan%20Xu%20and%20Animesh%20Kumar%20and%20Jingbo%20Shang%0AAbstract%3A%20%20%20The%20opacity%20in%20developing%20large%20language%20models%20%28LLMs%29%20is%20raising%20growing%0Aconcerns%20about%20the%20potential%20contamination%20of%20public%20benchmarks%20in%20the%0Apre-training%20data.%20Existing%20contamination%20detection%20methods%20are%20typically%20based%0Aon%20the%20text%20overlap%20between%20training%20and%20evaluation%20data%2C%20which%20can%20be%20too%0Asuperficial%20to%20reflect%20deeper%20forms%20of%20contamination.%20In%20this%20paper%2C%20we%20first%0Apresent%20a%20cross-lingual%20form%20of%20contamination%20that%20inflates%20LLMs%27%20performance%0Awhile%20evading%20current%20detection%20methods%2C%20deliberately%20injected%20by%20overfitting%0ALLMs%20on%20the%20translated%20versions%20of%20benchmark%20test%20sets.%20Then%2C%20we%20propose%0Ageneralization-based%20approaches%20to%20unmask%20such%20deeply%20concealed%20contamination.%0ASpecifically%2C%20we%20examine%20the%20LLM%27s%20performance%20change%20after%20modifying%20the%0Aoriginal%20benchmark%20by%20replacing%20the%20false%20answer%20choices%20with%20correct%20ones%20from%0Aother%20questions.%20Contaminated%20models%20can%20hardly%20generalize%20to%20such%20easier%0Asituations%2C%20where%20the%20false%20choices%20can%20be%20%5Cemph%7Bnot%20even%20wrong%7D%2C%20as%20all%0Achoices%20are%20correct%20in%20their%20memorization.%20Experimental%20results%20demonstrate%0Athat%20cross-lingual%20contamination%20can%20easily%20fool%20existing%20detection%20methods%2C%0Abut%20not%20ours.%20In%20addition%2C%20we%20discuss%20the%20potential%20utilization%20of%0Across-lingual%20contamination%20in%20interpreting%20LLMs%27%20working%20mechanisms%20and%20in%0Apost-training%20LLMs%20for%20enhanced%20multilingual%20capabilities.%20The%20code%20and%20dataset%0Awe%20use%20can%20be%20obtained%20from%20%5Curl%7Bhttps%3A//github.com/ShangDataLab/Deep-Contam%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13236v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Contamination%2520Can%2520Cross%2520Language%2520Barriers%26entry.906535625%3DFeng%2520Yao%2520and%2520Yufan%2520Zhuang%2520and%2520Zihao%2520Sun%2520and%2520Sunan%2520Xu%2520and%2520Animesh%2520Kumar%2520and%2520Jingbo%2520Shang%26entry.1292438233%3D%2520%2520The%2520opacity%2520in%2520developing%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520raising%2520growing%250Aconcerns%2520about%2520the%2520potential%2520contamination%2520of%2520public%2520benchmarks%2520in%2520the%250Apre-training%2520data.%2520Existing%2520contamination%2520detection%2520methods%2520are%2520typically%2520based%250Aon%2520the%2520text%2520overlap%2520between%2520training%2520and%2520evaluation%2520data%252C%2520which%2520can%2520be%2520too%250Asuperficial%2520to%2520reflect%2520deeper%2520forms%2520of%2520contamination.%2520In%2520this%2520paper%252C%2520we%2520first%250Apresent%2520a%2520cross-lingual%2520form%2520of%2520contamination%2520that%2520inflates%2520LLMs%2527%2520performance%250Awhile%2520evading%2520current%2520detection%2520methods%252C%2520deliberately%2520injected%2520by%2520overfitting%250ALLMs%2520on%2520the%2520translated%2520versions%2520of%2520benchmark%2520test%2520sets.%2520Then%252C%2520we%2520propose%250Ageneralization-based%2520approaches%2520to%2520unmask%2520such%2520deeply%2520concealed%2520contamination.%250ASpecifically%252C%2520we%2520examine%2520the%2520LLM%2527s%2520performance%2520change%2520after%2520modifying%2520the%250Aoriginal%2520benchmark%2520by%2520replacing%2520the%2520false%2520answer%2520choices%2520with%2520correct%2520ones%2520from%250Aother%2520questions.%2520Contaminated%2520models%2520can%2520hardly%2520generalize%2520to%2520such%2520easier%250Asituations%252C%2520where%2520the%2520false%2520choices%2520can%2520be%2520%255Cemph%257Bnot%2520even%2520wrong%257D%252C%2520as%2520all%250Achoices%2520are%2520correct%2520in%2520their%2520memorization.%2520Experimental%2520results%2520demonstrate%250Athat%2520cross-lingual%2520contamination%2520can%2520easily%2520fool%2520existing%2520detection%2520methods%252C%250Abut%2520not%2520ours.%2520In%2520addition%252C%2520we%2520discuss%2520the%2520potential%2520utilization%2520of%250Across-lingual%2520contamination%2520in%2520interpreting%2520LLMs%2527%2520working%2520mechanisms%2520and%2520in%250Apost-training%2520LLMs%2520for%2520enhanced%2520multilingual%2520capabilities.%2520The%2520code%2520and%2520dataset%250Awe%2520use%2520can%2520be%2520obtained%2520from%2520%255Curl%257Bhttps%253A//github.com/ShangDataLab/Deep-Contam%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13236v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Contamination%20Can%20Cross%20Language%20Barriers&entry.906535625=Feng%20Yao%20and%20Yufan%20Zhuang%20and%20Zihao%20Sun%20and%20Sunan%20Xu%20and%20Animesh%20Kumar%20and%20Jingbo%20Shang&entry.1292438233=%20%20The%20opacity%20in%20developing%20large%20language%20models%20%28LLMs%29%20is%20raising%20growing%0Aconcerns%20about%20the%20potential%20contamination%20of%20public%20benchmarks%20in%20the%0Apre-training%20data.%20Existing%20contamination%20detection%20methods%20are%20typically%20based%0Aon%20the%20text%20overlap%20between%20training%20and%20evaluation%20data%2C%20which%20can%20be%20too%0Asuperficial%20to%20reflect%20deeper%20forms%20of%20contamination.%20In%20this%20paper%2C%20we%20first%0Apresent%20a%20cross-lingual%20form%20of%20contamination%20that%20inflates%20LLMs%27%20performance%0Awhile%20evading%20current%20detection%20methods%2C%20deliberately%20injected%20by%20overfitting%0ALLMs%20on%20the%20translated%20versions%20of%20benchmark%20test%20sets.%20Then%2C%20we%20propose%0Ageneralization-based%20approaches%20to%20unmask%20such%20deeply%20concealed%20contamination.%0ASpecifically%2C%20we%20examine%20the%20LLM%27s%20performance%20change%20after%20modifying%20the%0Aoriginal%20benchmark%20by%20replacing%20the%20false%20answer%20choices%20with%20correct%20ones%20from%0Aother%20questions.%20Contaminated%20models%20can%20hardly%20generalize%20to%20such%20easier%0Asituations%2C%20where%20the%20false%20choices%20can%20be%20%5Cemph%7Bnot%20even%20wrong%7D%2C%20as%20all%0Achoices%20are%20correct%20in%20their%20memorization.%20Experimental%20results%20demonstrate%0Athat%20cross-lingual%20contamination%20can%20easily%20fool%20existing%20detection%20methods%2C%0Abut%20not%20ours.%20In%20addition%2C%20we%20discuss%20the%20potential%20utilization%20of%0Across-lingual%20contamination%20in%20interpreting%20LLMs%27%20working%20mechanisms%20and%20in%0Apost-training%20LLMs%20for%20enhanced%20multilingual%20capabilities.%20The%20code%20and%20dataset%0Awe%20use%20can%20be%20obtained%20from%20%5Curl%7Bhttps%3A//github.com/ShangDataLab/Deep-Contam%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13236v2&entry.124074799=Read"},
{"title": "Entrywise error bounds for low-rank approximations of kernel matrices", "author": "Alexander Modell", "abstract": "  In this paper, we derive entrywise error bounds for low-rank approximations\nof kernel matrices obtained using the truncated eigen-decomposition (or\nsingular value decomposition). While this approximation is well-known to be\noptimal with respect to the spectral and Frobenius norm error, little is known\nabout the statistical behaviour of individual entries. Our error bounds fill\nthis gap. A key technical innovation is a delocalisation result for the\neigenvectors of the kernel matrix corresponding to small eigenvalues, which\ntakes inspiration from the field of Random Matrix Theory. Finally, we validate\nour theory with an empirical study of a collection of synthetic and real-world\ndatasets.\n", "link": "http://arxiv.org/abs/2405.14494v2", "date": "2024-10-30", "relevancy": 1.9502, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3944}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.3893}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entrywise%20error%20bounds%20for%20low-rank%20approximations%20of%20kernel%20matrices&body=Title%3A%20Entrywise%20error%20bounds%20for%20low-rank%20approximations%20of%20kernel%20matrices%0AAuthor%3A%20Alexander%20Modell%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20derive%20entrywise%20error%20bounds%20for%20low-rank%20approximations%0Aof%20kernel%20matrices%20obtained%20using%20the%20truncated%20eigen-decomposition%20%28or%0Asingular%20value%20decomposition%29.%20While%20this%20approximation%20is%20well-known%20to%20be%0Aoptimal%20with%20respect%20to%20the%20spectral%20and%20Frobenius%20norm%20error%2C%20little%20is%20known%0Aabout%20the%20statistical%20behaviour%20of%20individual%20entries.%20Our%20error%20bounds%20fill%0Athis%20gap.%20A%20key%20technical%20innovation%20is%20a%20delocalisation%20result%20for%20the%0Aeigenvectors%20of%20the%20kernel%20matrix%20corresponding%20to%20small%20eigenvalues%2C%20which%0Atakes%20inspiration%20from%20the%20field%20of%20Random%20Matrix%20Theory.%20Finally%2C%20we%20validate%0Aour%20theory%20with%20an%20empirical%20study%20of%20a%20collection%20of%20synthetic%20and%20real-world%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14494v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntrywise%2520error%2520bounds%2520for%2520low-rank%2520approximations%2520of%2520kernel%2520matrices%26entry.906535625%3DAlexander%2520Modell%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520derive%2520entrywise%2520error%2520bounds%2520for%2520low-rank%2520approximations%250Aof%2520kernel%2520matrices%2520obtained%2520using%2520the%2520truncated%2520eigen-decomposition%2520%2528or%250Asingular%2520value%2520decomposition%2529.%2520While%2520this%2520approximation%2520is%2520well-known%2520to%2520be%250Aoptimal%2520with%2520respect%2520to%2520the%2520spectral%2520and%2520Frobenius%2520norm%2520error%252C%2520little%2520is%2520known%250Aabout%2520the%2520statistical%2520behaviour%2520of%2520individual%2520entries.%2520Our%2520error%2520bounds%2520fill%250Athis%2520gap.%2520A%2520key%2520technical%2520innovation%2520is%2520a%2520delocalisation%2520result%2520for%2520the%250Aeigenvectors%2520of%2520the%2520kernel%2520matrix%2520corresponding%2520to%2520small%2520eigenvalues%252C%2520which%250Atakes%2520inspiration%2520from%2520the%2520field%2520of%2520Random%2520Matrix%2520Theory.%2520Finally%252C%2520we%2520validate%250Aour%2520theory%2520with%2520an%2520empirical%2520study%2520of%2520a%2520collection%2520of%2520synthetic%2520and%2520real-world%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14494v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entrywise%20error%20bounds%20for%20low-rank%20approximations%20of%20kernel%20matrices&entry.906535625=Alexander%20Modell&entry.1292438233=%20%20In%20this%20paper%2C%20we%20derive%20entrywise%20error%20bounds%20for%20low-rank%20approximations%0Aof%20kernel%20matrices%20obtained%20using%20the%20truncated%20eigen-decomposition%20%28or%0Asingular%20value%20decomposition%29.%20While%20this%20approximation%20is%20well-known%20to%20be%0Aoptimal%20with%20respect%20to%20the%20spectral%20and%20Frobenius%20norm%20error%2C%20little%20is%20known%0Aabout%20the%20statistical%20behaviour%20of%20individual%20entries.%20Our%20error%20bounds%20fill%0Athis%20gap.%20A%20key%20technical%20innovation%20is%20a%20delocalisation%20result%20for%20the%0Aeigenvectors%20of%20the%20kernel%20matrix%20corresponding%20to%20small%20eigenvalues%2C%20which%0Atakes%20inspiration%20from%20the%20field%20of%20Random%20Matrix%20Theory.%20Finally%2C%20we%20validate%0Aour%20theory%20with%20an%20empirical%20study%20of%20a%20collection%20of%20synthetic%20and%20real-world%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14494v2&entry.124074799=Read"},
{"title": "Levels of explanation -- implementation and evaluation of what and when\n  for different time-sensitive tasks", "author": "Shikhar Kumar and Omer Keidar and Yael Edan", "abstract": "  In this work, we focused on constructing and evaluating levels of\nexplanation(LOE) that address two basic aspect of HRI: 1. What information\nshould be communicated to the user by the robot? 2. When should the robot\ncommunicate this information? For constructing the LOE, we defined two terms,\nverbosity and explanation patterns, each with two levels (verbosity -- high and\nlow, explanation patterns -- dynamic and static). Based on these parameters,\nthree different LOE (high, medium, and low) were constructed and evaluated in a\nuser study with a telepresence robot. The user study was conducted for a\nsimulated telerobotic healthcare task with two different conditions related to\ntime sensitivity, as evaluated by two different user groups -- one that\nperformed the task within a time limit and the other with no time limit. We\nfound that the high LOE was preferred in terms of adequacy of explanation,\nnumber of collisions, number of incorrect movements, and number of\nclarifications when users performed the experiment in the without time limit\ncondition. We also found that both high and medium LOE did not have significant\ndifferences in completion time, the fluency of HRI, and trust in the robot.\nWhen users performed the experiment in the with time limit condition, high and\nmedium LOE had better task performances and were preferred to the low LOE in\nterms of completion time, fluency, adequacy of explanation, trust, number of\ncollisions, number of incorrect movements and number of clarifications. Future\ndirections for advancing LOE are discussed.\n", "link": "http://arxiv.org/abs/2410.23215v1", "date": "2024-10-30", "relevancy": 1.9341, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5137}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4775}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Levels%20of%20explanation%20--%20implementation%20and%20evaluation%20of%20what%20and%20when%0A%20%20for%20different%20time-sensitive%20tasks&body=Title%3A%20Levels%20of%20explanation%20--%20implementation%20and%20evaluation%20of%20what%20and%20when%0A%20%20for%20different%20time-sensitive%20tasks%0AAuthor%3A%20Shikhar%20Kumar%20and%20Omer%20Keidar%20and%20Yael%20Edan%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20focused%20on%20constructing%20and%20evaluating%20levels%20of%0Aexplanation%28LOE%29%20that%20address%20two%20basic%20aspect%20of%20HRI%3A%201.%20What%20information%0Ashould%20be%20communicated%20to%20the%20user%20by%20the%20robot%3F%202.%20When%20should%20the%20robot%0Acommunicate%20this%20information%3F%20For%20constructing%20the%20LOE%2C%20we%20defined%20two%20terms%2C%0Averbosity%20and%20explanation%20patterns%2C%20each%20with%20two%20levels%20%28verbosity%20--%20high%20and%0Alow%2C%20explanation%20patterns%20--%20dynamic%20and%20static%29.%20Based%20on%20these%20parameters%2C%0Athree%20different%20LOE%20%28high%2C%20medium%2C%20and%20low%29%20were%20constructed%20and%20evaluated%20in%20a%0Auser%20study%20with%20a%20telepresence%20robot.%20The%20user%20study%20was%20conducted%20for%20a%0Asimulated%20telerobotic%20healthcare%20task%20with%20two%20different%20conditions%20related%20to%0Atime%20sensitivity%2C%20as%20evaluated%20by%20two%20different%20user%20groups%20--%20one%20that%0Aperformed%20the%20task%20within%20a%20time%20limit%20and%20the%20other%20with%20no%20time%20limit.%20We%0Afound%20that%20the%20high%20LOE%20was%20preferred%20in%20terms%20of%20adequacy%20of%20explanation%2C%0Anumber%20of%20collisions%2C%20number%20of%20incorrect%20movements%2C%20and%20number%20of%0Aclarifications%20when%20users%20performed%20the%20experiment%20in%20the%20without%20time%20limit%0Acondition.%20We%20also%20found%20that%20both%20high%20and%20medium%20LOE%20did%20not%20have%20significant%0Adifferences%20in%20completion%20time%2C%20the%20fluency%20of%20HRI%2C%20and%20trust%20in%20the%20robot.%0AWhen%20users%20performed%20the%20experiment%20in%20the%20with%20time%20limit%20condition%2C%20high%20and%0Amedium%20LOE%20had%20better%20task%20performances%20and%20were%20preferred%20to%20the%20low%20LOE%20in%0Aterms%20of%20completion%20time%2C%20fluency%2C%20adequacy%20of%20explanation%2C%20trust%2C%20number%20of%0Acollisions%2C%20number%20of%20incorrect%20movements%20and%20number%20of%20clarifications.%20Future%0Adirections%20for%20advancing%20LOE%20are%20discussed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23215v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLevels%2520of%2520explanation%2520--%2520implementation%2520and%2520evaluation%2520of%2520what%2520and%2520when%250A%2520%2520for%2520different%2520time-sensitive%2520tasks%26entry.906535625%3DShikhar%2520Kumar%2520and%2520Omer%2520Keidar%2520and%2520Yael%2520Edan%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520focused%2520on%2520constructing%2520and%2520evaluating%2520levels%2520of%250Aexplanation%2528LOE%2529%2520that%2520address%2520two%2520basic%2520aspect%2520of%2520HRI%253A%25201.%2520What%2520information%250Ashould%2520be%2520communicated%2520to%2520the%2520user%2520by%2520the%2520robot%253F%25202.%2520When%2520should%2520the%2520robot%250Acommunicate%2520this%2520information%253F%2520For%2520constructing%2520the%2520LOE%252C%2520we%2520defined%2520two%2520terms%252C%250Averbosity%2520and%2520explanation%2520patterns%252C%2520each%2520with%2520two%2520levels%2520%2528verbosity%2520--%2520high%2520and%250Alow%252C%2520explanation%2520patterns%2520--%2520dynamic%2520and%2520static%2529.%2520Based%2520on%2520these%2520parameters%252C%250Athree%2520different%2520LOE%2520%2528high%252C%2520medium%252C%2520and%2520low%2529%2520were%2520constructed%2520and%2520evaluated%2520in%2520a%250Auser%2520study%2520with%2520a%2520telepresence%2520robot.%2520The%2520user%2520study%2520was%2520conducted%2520for%2520a%250Asimulated%2520telerobotic%2520healthcare%2520task%2520with%2520two%2520different%2520conditions%2520related%2520to%250Atime%2520sensitivity%252C%2520as%2520evaluated%2520by%2520two%2520different%2520user%2520groups%2520--%2520one%2520that%250Aperformed%2520the%2520task%2520within%2520a%2520time%2520limit%2520and%2520the%2520other%2520with%2520no%2520time%2520limit.%2520We%250Afound%2520that%2520the%2520high%2520LOE%2520was%2520preferred%2520in%2520terms%2520of%2520adequacy%2520of%2520explanation%252C%250Anumber%2520of%2520collisions%252C%2520number%2520of%2520incorrect%2520movements%252C%2520and%2520number%2520of%250Aclarifications%2520when%2520users%2520performed%2520the%2520experiment%2520in%2520the%2520without%2520time%2520limit%250Acondition.%2520We%2520also%2520found%2520that%2520both%2520high%2520and%2520medium%2520LOE%2520did%2520not%2520have%2520significant%250Adifferences%2520in%2520completion%2520time%252C%2520the%2520fluency%2520of%2520HRI%252C%2520and%2520trust%2520in%2520the%2520robot.%250AWhen%2520users%2520performed%2520the%2520experiment%2520in%2520the%2520with%2520time%2520limit%2520condition%252C%2520high%2520and%250Amedium%2520LOE%2520had%2520better%2520task%2520performances%2520and%2520were%2520preferred%2520to%2520the%2520low%2520LOE%2520in%250Aterms%2520of%2520completion%2520time%252C%2520fluency%252C%2520adequacy%2520of%2520explanation%252C%2520trust%252C%2520number%2520of%250Acollisions%252C%2520number%2520of%2520incorrect%2520movements%2520and%2520number%2520of%2520clarifications.%2520Future%250Adirections%2520for%2520advancing%2520LOE%2520are%2520discussed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23215v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Levels%20of%20explanation%20--%20implementation%20and%20evaluation%20of%20what%20and%20when%0A%20%20for%20different%20time-sensitive%20tasks&entry.906535625=Shikhar%20Kumar%20and%20Omer%20Keidar%20and%20Yael%20Edan&entry.1292438233=%20%20In%20this%20work%2C%20we%20focused%20on%20constructing%20and%20evaluating%20levels%20of%0Aexplanation%28LOE%29%20that%20address%20two%20basic%20aspect%20of%20HRI%3A%201.%20What%20information%0Ashould%20be%20communicated%20to%20the%20user%20by%20the%20robot%3F%202.%20When%20should%20the%20robot%0Acommunicate%20this%20information%3F%20For%20constructing%20the%20LOE%2C%20we%20defined%20two%20terms%2C%0Averbosity%20and%20explanation%20patterns%2C%20each%20with%20two%20levels%20%28verbosity%20--%20high%20and%0Alow%2C%20explanation%20patterns%20--%20dynamic%20and%20static%29.%20Based%20on%20these%20parameters%2C%0Athree%20different%20LOE%20%28high%2C%20medium%2C%20and%20low%29%20were%20constructed%20and%20evaluated%20in%20a%0Auser%20study%20with%20a%20telepresence%20robot.%20The%20user%20study%20was%20conducted%20for%20a%0Asimulated%20telerobotic%20healthcare%20task%20with%20two%20different%20conditions%20related%20to%0Atime%20sensitivity%2C%20as%20evaluated%20by%20two%20different%20user%20groups%20--%20one%20that%0Aperformed%20the%20task%20within%20a%20time%20limit%20and%20the%20other%20with%20no%20time%20limit.%20We%0Afound%20that%20the%20high%20LOE%20was%20preferred%20in%20terms%20of%20adequacy%20of%20explanation%2C%0Anumber%20of%20collisions%2C%20number%20of%20incorrect%20movements%2C%20and%20number%20of%0Aclarifications%20when%20users%20performed%20the%20experiment%20in%20the%20without%20time%20limit%0Acondition.%20We%20also%20found%20that%20both%20high%20and%20medium%20LOE%20did%20not%20have%20significant%0Adifferences%20in%20completion%20time%2C%20the%20fluency%20of%20HRI%2C%20and%20trust%20in%20the%20robot.%0AWhen%20users%20performed%20the%20experiment%20in%20the%20with%20time%20limit%20condition%2C%20high%20and%0Amedium%20LOE%20had%20better%20task%20performances%20and%20were%20preferred%20to%20the%20low%20LOE%20in%0Aterms%20of%20completion%20time%2C%20fluency%2C%20adequacy%20of%20explanation%2C%20trust%2C%20number%20of%0Acollisions%2C%20number%20of%20incorrect%20movements%20and%20number%20of%20clarifications.%20Future%0Adirections%20for%20advancing%20LOE%20are%20discussed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23215v1&entry.124074799=Read"},
{"title": "Position Coupling: Improving Length Generalization of Arithmetic\n  Transformers Using Task Structure", "author": "Hanseul Cho and Jaeyoung Cha and Pranjal Awasthi and Srinadh Bhojanapalli and Anupam Gupta and Chulhee Yun", "abstract": "  Even for simple arithmetic tasks like integer addition, it is challenging for\nTransformers to generalize to longer sequences than those encountered during\ntraining. To tackle this problem, we propose position coupling, a simple yet\neffective method that directly embeds the structure of the tasks into the\npositional encoding of a (decoder-only) Transformer. Taking a departure from\nthe vanilla absolute position mechanism assigning unique position IDs to each\nof the tokens, we assign the same position IDs to two or more \"relevant\"\ntokens; for integer addition tasks, we regard digits of the same significance\nas in the same position. On the empirical side, we show that with the proposed\nposition coupling, our models trained on 1 to 30-digit additions can generalize\nup to 200-digit additions (6.67x of the trained length). On the theoretical\nside, we prove that a 1-layer Transformer with coupled positions can solve the\naddition task involving exponentially many digits, whereas any 1-layer\nTransformer without positional information cannot entirely solve it. We also\ndemonstrate that position coupling can be applied to other algorithmic tasks\nsuch as Nx2 multiplication and a two-dimensional task.\n", "link": "http://arxiv.org/abs/2405.20671v2", "date": "2024-10-30", "relevancy": 1.9313, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5032}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4986}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Position%20Coupling%3A%20Improving%20Length%20Generalization%20of%20Arithmetic%0A%20%20Transformers%20Using%20Task%20Structure&body=Title%3A%20Position%20Coupling%3A%20Improving%20Length%20Generalization%20of%20Arithmetic%0A%20%20Transformers%20Using%20Task%20Structure%0AAuthor%3A%20Hanseul%20Cho%20and%20Jaeyoung%20Cha%20and%20Pranjal%20Awasthi%20and%20Srinadh%20Bhojanapalli%20and%20Anupam%20Gupta%20and%20Chulhee%20Yun%0AAbstract%3A%20%20%20Even%20for%20simple%20arithmetic%20tasks%20like%20integer%20addition%2C%20it%20is%20challenging%20for%0ATransformers%20to%20generalize%20to%20longer%20sequences%20than%20those%20encountered%20during%0Atraining.%20To%20tackle%20this%20problem%2C%20we%20propose%20position%20coupling%2C%20a%20simple%20yet%0Aeffective%20method%20that%20directly%20embeds%20the%20structure%20of%20the%20tasks%20into%20the%0Apositional%20encoding%20of%20a%20%28decoder-only%29%20Transformer.%20Taking%20a%20departure%20from%0Athe%20vanilla%20absolute%20position%20mechanism%20assigning%20unique%20position%20IDs%20to%20each%0Aof%20the%20tokens%2C%20we%20assign%20the%20same%20position%20IDs%20to%20two%20or%20more%20%22relevant%22%0Atokens%3B%20for%20integer%20addition%20tasks%2C%20we%20regard%20digits%20of%20the%20same%20significance%0Aas%20in%20the%20same%20position.%20On%20the%20empirical%20side%2C%20we%20show%20that%20with%20the%20proposed%0Aposition%20coupling%2C%20our%20models%20trained%20on%201%20to%2030-digit%20additions%20can%20generalize%0Aup%20to%20200-digit%20additions%20%286.67x%20of%20the%20trained%20length%29.%20On%20the%20theoretical%0Aside%2C%20we%20prove%20that%20a%201-layer%20Transformer%20with%20coupled%20positions%20can%20solve%20the%0Aaddition%20task%20involving%20exponentially%20many%20digits%2C%20whereas%20any%201-layer%0ATransformer%20without%20positional%20information%20cannot%20entirely%20solve%20it.%20We%20also%0Ademonstrate%20that%20position%20coupling%20can%20be%20applied%20to%20other%20algorithmic%20tasks%0Asuch%20as%20Nx2%20multiplication%20and%20a%20two-dimensional%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20671v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosition%2520Coupling%253A%2520Improving%2520Length%2520Generalization%2520of%2520Arithmetic%250A%2520%2520Transformers%2520Using%2520Task%2520Structure%26entry.906535625%3DHanseul%2520Cho%2520and%2520Jaeyoung%2520Cha%2520and%2520Pranjal%2520Awasthi%2520and%2520Srinadh%2520Bhojanapalli%2520and%2520Anupam%2520Gupta%2520and%2520Chulhee%2520Yun%26entry.1292438233%3D%2520%2520Even%2520for%2520simple%2520arithmetic%2520tasks%2520like%2520integer%2520addition%252C%2520it%2520is%2520challenging%2520for%250ATransformers%2520to%2520generalize%2520to%2520longer%2520sequences%2520than%2520those%2520encountered%2520during%250Atraining.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520propose%2520position%2520coupling%252C%2520a%2520simple%2520yet%250Aeffective%2520method%2520that%2520directly%2520embeds%2520the%2520structure%2520of%2520the%2520tasks%2520into%2520the%250Apositional%2520encoding%2520of%2520a%2520%2528decoder-only%2529%2520Transformer.%2520Taking%2520a%2520departure%2520from%250Athe%2520vanilla%2520absolute%2520position%2520mechanism%2520assigning%2520unique%2520position%2520IDs%2520to%2520each%250Aof%2520the%2520tokens%252C%2520we%2520assign%2520the%2520same%2520position%2520IDs%2520to%2520two%2520or%2520more%2520%2522relevant%2522%250Atokens%253B%2520for%2520integer%2520addition%2520tasks%252C%2520we%2520regard%2520digits%2520of%2520the%2520same%2520significance%250Aas%2520in%2520the%2520same%2520position.%2520On%2520the%2520empirical%2520side%252C%2520we%2520show%2520that%2520with%2520the%2520proposed%250Aposition%2520coupling%252C%2520our%2520models%2520trained%2520on%25201%2520to%252030-digit%2520additions%2520can%2520generalize%250Aup%2520to%2520200-digit%2520additions%2520%25286.67x%2520of%2520the%2520trained%2520length%2529.%2520On%2520the%2520theoretical%250Aside%252C%2520we%2520prove%2520that%2520a%25201-layer%2520Transformer%2520with%2520coupled%2520positions%2520can%2520solve%2520the%250Aaddition%2520task%2520involving%2520exponentially%2520many%2520digits%252C%2520whereas%2520any%25201-layer%250ATransformer%2520without%2520positional%2520information%2520cannot%2520entirely%2520solve%2520it.%2520We%2520also%250Ademonstrate%2520that%2520position%2520coupling%2520can%2520be%2520applied%2520to%2520other%2520algorithmic%2520tasks%250Asuch%2520as%2520Nx2%2520multiplication%2520and%2520a%2520two-dimensional%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20671v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Position%20Coupling%3A%20Improving%20Length%20Generalization%20of%20Arithmetic%0A%20%20Transformers%20Using%20Task%20Structure&entry.906535625=Hanseul%20Cho%20and%20Jaeyoung%20Cha%20and%20Pranjal%20Awasthi%20and%20Srinadh%20Bhojanapalli%20and%20Anupam%20Gupta%20and%20Chulhee%20Yun&entry.1292438233=%20%20Even%20for%20simple%20arithmetic%20tasks%20like%20integer%20addition%2C%20it%20is%20challenging%20for%0ATransformers%20to%20generalize%20to%20longer%20sequences%20than%20those%20encountered%20during%0Atraining.%20To%20tackle%20this%20problem%2C%20we%20propose%20position%20coupling%2C%20a%20simple%20yet%0Aeffective%20method%20that%20directly%20embeds%20the%20structure%20of%20the%20tasks%20into%20the%0Apositional%20encoding%20of%20a%20%28decoder-only%29%20Transformer.%20Taking%20a%20departure%20from%0Athe%20vanilla%20absolute%20position%20mechanism%20assigning%20unique%20position%20IDs%20to%20each%0Aof%20the%20tokens%2C%20we%20assign%20the%20same%20position%20IDs%20to%20two%20or%20more%20%22relevant%22%0Atokens%3B%20for%20integer%20addition%20tasks%2C%20we%20regard%20digits%20of%20the%20same%20significance%0Aas%20in%20the%20same%20position.%20On%20the%20empirical%20side%2C%20we%20show%20that%20with%20the%20proposed%0Aposition%20coupling%2C%20our%20models%20trained%20on%201%20to%2030-digit%20additions%20can%20generalize%0Aup%20to%20200-digit%20additions%20%286.67x%20of%20the%20trained%20length%29.%20On%20the%20theoretical%0Aside%2C%20we%20prove%20that%20a%201-layer%20Transformer%20with%20coupled%20positions%20can%20solve%20the%0Aaddition%20task%20involving%20exponentially%20many%20digits%2C%20whereas%20any%201-layer%0ATransformer%20without%20positional%20information%20cannot%20entirely%20solve%20it.%20We%20also%0Ademonstrate%20that%20position%20coupling%20can%20be%20applied%20to%20other%20algorithmic%20tasks%0Asuch%20as%20Nx2%20multiplication%20and%20a%20two-dimensional%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20671v2&entry.124074799=Read"},
{"title": "Uncertainty quantification for fast reconstruction methods using\n  augmented equivariant bootstrap: Application to radio interferometry", "author": "Mostafa Cherif and Tob\u00edas I. Liaudat and Jonathan Kern and Christophe Kervazo and J\u00e9r\u00f4me Bobin", "abstract": "  The advent of next-generation radio interferometers like the Square Kilometer\nArray promises to revolutionise our radio astronomy observational capabilities.\nThe unprecedented volume of data these devices generate requires fast and\naccurate image reconstruction algorithms to solve the ill-posed radio\ninterferometric imaging problem. Most state-of-the-art reconstruction methods\nlack trustworthy and scalable uncertainty quantification, which is critical for\nthe rigorous scientific interpretation of radio observations. We propose an\nunsupervised technique based on a conformalized version of a radio-augmented\nequivariant bootstrapping method, which allows us to quantify uncertainties for\nfast reconstruction methods. Noticeably, we rely on reconstructions from\nultra-fast unrolled algorithms. The proposed method brings more reliable\nuncertainty estimations to our problem than existing alternatives.\n", "link": "http://arxiv.org/abs/2410.23178v1", "date": "2024-10-30", "relevancy": 1.9302, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4952}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4798}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20quantification%20for%20fast%20reconstruction%20methods%20using%0A%20%20augmented%20equivariant%20bootstrap%3A%20Application%20to%20radio%20interferometry&body=Title%3A%20Uncertainty%20quantification%20for%20fast%20reconstruction%20methods%20using%0A%20%20augmented%20equivariant%20bootstrap%3A%20Application%20to%20radio%20interferometry%0AAuthor%3A%20Mostafa%20Cherif%20and%20Tob%C3%ADas%20I.%20Liaudat%20and%20Jonathan%20Kern%20and%20Christophe%20Kervazo%20and%20J%C3%A9r%C3%B4me%20Bobin%0AAbstract%3A%20%20%20The%20advent%20of%20next-generation%20radio%20interferometers%20like%20the%20Square%20Kilometer%0AArray%20promises%20to%20revolutionise%20our%20radio%20astronomy%20observational%20capabilities.%0AThe%20unprecedented%20volume%20of%20data%20these%20devices%20generate%20requires%20fast%20and%0Aaccurate%20image%20reconstruction%20algorithms%20to%20solve%20the%20ill-posed%20radio%0Ainterferometric%20imaging%20problem.%20Most%20state-of-the-art%20reconstruction%20methods%0Alack%20trustworthy%20and%20scalable%20uncertainty%20quantification%2C%20which%20is%20critical%20for%0Athe%20rigorous%20scientific%20interpretation%20of%20radio%20observations.%20We%20propose%20an%0Aunsupervised%20technique%20based%20on%20a%20conformalized%20version%20of%20a%20radio-augmented%0Aequivariant%20bootstrapping%20method%2C%20which%20allows%20us%20to%20quantify%20uncertainties%20for%0Afast%20reconstruction%20methods.%20Noticeably%2C%20we%20rely%20on%20reconstructions%20from%0Aultra-fast%20unrolled%20algorithms.%20The%20proposed%20method%20brings%20more%20reliable%0Auncertainty%20estimations%20to%20our%20problem%20than%20existing%20alternatives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520quantification%2520for%2520fast%2520reconstruction%2520methods%2520using%250A%2520%2520augmented%2520equivariant%2520bootstrap%253A%2520Application%2520to%2520radio%2520interferometry%26entry.906535625%3DMostafa%2520Cherif%2520and%2520Tob%25C3%25ADas%2520I.%2520Liaudat%2520and%2520Jonathan%2520Kern%2520and%2520Christophe%2520Kervazo%2520and%2520J%25C3%25A9r%25C3%25B4me%2520Bobin%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520next-generation%2520radio%2520interferometers%2520like%2520the%2520Square%2520Kilometer%250AArray%2520promises%2520to%2520revolutionise%2520our%2520radio%2520astronomy%2520observational%2520capabilities.%250AThe%2520unprecedented%2520volume%2520of%2520data%2520these%2520devices%2520generate%2520requires%2520fast%2520and%250Aaccurate%2520image%2520reconstruction%2520algorithms%2520to%2520solve%2520the%2520ill-posed%2520radio%250Ainterferometric%2520imaging%2520problem.%2520Most%2520state-of-the-art%2520reconstruction%2520methods%250Alack%2520trustworthy%2520and%2520scalable%2520uncertainty%2520quantification%252C%2520which%2520is%2520critical%2520for%250Athe%2520rigorous%2520scientific%2520interpretation%2520of%2520radio%2520observations.%2520We%2520propose%2520an%250Aunsupervised%2520technique%2520based%2520on%2520a%2520conformalized%2520version%2520of%2520a%2520radio-augmented%250Aequivariant%2520bootstrapping%2520method%252C%2520which%2520allows%2520us%2520to%2520quantify%2520uncertainties%2520for%250Afast%2520reconstruction%2520methods.%2520Noticeably%252C%2520we%2520rely%2520on%2520reconstructions%2520from%250Aultra-fast%2520unrolled%2520algorithms.%2520The%2520proposed%2520method%2520brings%2520more%2520reliable%250Auncertainty%2520estimations%2520to%2520our%2520problem%2520than%2520existing%2520alternatives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20quantification%20for%20fast%20reconstruction%20methods%20using%0A%20%20augmented%20equivariant%20bootstrap%3A%20Application%20to%20radio%20interferometry&entry.906535625=Mostafa%20Cherif%20and%20Tob%C3%ADas%20I.%20Liaudat%20and%20Jonathan%20Kern%20and%20Christophe%20Kervazo%20and%20J%C3%A9r%C3%B4me%20Bobin&entry.1292438233=%20%20The%20advent%20of%20next-generation%20radio%20interferometers%20like%20the%20Square%20Kilometer%0AArray%20promises%20to%20revolutionise%20our%20radio%20astronomy%20observational%20capabilities.%0AThe%20unprecedented%20volume%20of%20data%20these%20devices%20generate%20requires%20fast%20and%0Aaccurate%20image%20reconstruction%20algorithms%20to%20solve%20the%20ill-posed%20radio%0Ainterferometric%20imaging%20problem.%20Most%20state-of-the-art%20reconstruction%20methods%0Alack%20trustworthy%20and%20scalable%20uncertainty%20quantification%2C%20which%20is%20critical%20for%0Athe%20rigorous%20scientific%20interpretation%20of%20radio%20observations.%20We%20propose%20an%0Aunsupervised%20technique%20based%20on%20a%20conformalized%20version%20of%20a%20radio-augmented%0Aequivariant%20bootstrapping%20method%2C%20which%20allows%20us%20to%20quantify%20uncertainties%20for%0Afast%20reconstruction%20methods.%20Noticeably%2C%20we%20rely%20on%20reconstructions%20from%0Aultra-fast%20unrolled%20algorithms.%20The%20proposed%20method%20brings%20more%20reliable%0Auncertainty%20estimations%20to%20our%20problem%20than%20existing%20alternatives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23178v1&entry.124074799=Read"},
{"title": "(FL)$^2$: Overcoming Few Labels in Federated Semi-Supervised Learning", "author": "Seungjoo Lee and Thanh-Long V. Le and Jaemin Shin and Sung-Ju Lee", "abstract": "  Federated Learning (FL) is a distributed machine learning framework that\ntrains accurate global models while preserving clients' privacy-sensitive data.\nHowever, most FL approaches assume that clients possess labeled data, which is\noften not the case in practice. Federated Semi-Supervised Learning (FSSL)\naddresses this label deficiency problem, targeting situations where only the\nserver has a small amount of labeled data while clients do not. However, a\nsignificant performance gap exists between Centralized Semi-Supervised Learning\n(SSL) and FSSL. This gap arises from confirmation bias, which is more\npronounced in FSSL due to multiple local training epochs and the separation of\nlabeled and unlabeled data. We propose $(FL)^2$, a robust training method for\nunlabeled clients using sharpness-aware consistency regularization. We show\nthat regularizing the original pseudo-labeling loss is suboptimal, and hence we\ncarefully select unlabeled samples for regularization. We further introduce\nclient-specific adaptive thresholding and learning status-aware aggregation to\nadjust the training process based on the learning progress of each client. Our\nexperiments on three benchmark datasets demonstrate that our approach\nsignificantly improves performance and bridges the gap with SSL, particularly\nin scenarios with scarce labeled data.\n", "link": "http://arxiv.org/abs/2410.23227v1", "date": "2024-10-30", "relevancy": 1.9067, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4877}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4792}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%28FL%29%24%5E2%24%3A%20Overcoming%20Few%20Labels%20in%20Federated%20Semi-Supervised%20Learning&body=Title%3A%20%28FL%29%24%5E2%24%3A%20Overcoming%20Few%20Labels%20in%20Federated%20Semi-Supervised%20Learning%0AAuthor%3A%20Seungjoo%20Lee%20and%20Thanh-Long%20V.%20Le%20and%20Jaemin%20Shin%20and%20Sung-Ju%20Lee%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20a%20distributed%20machine%20learning%20framework%20that%0Atrains%20accurate%20global%20models%20while%20preserving%20clients%27%20privacy-sensitive%20data.%0AHowever%2C%20most%20FL%20approaches%20assume%20that%20clients%20possess%20labeled%20data%2C%20which%20is%0Aoften%20not%20the%20case%20in%20practice.%20Federated%20Semi-Supervised%20Learning%20%28FSSL%29%0Aaddresses%20this%20label%20deficiency%20problem%2C%20targeting%20situations%20where%20only%20the%0Aserver%20has%20a%20small%20amount%20of%20labeled%20data%20while%20clients%20do%20not.%20However%2C%20a%0Asignificant%20performance%20gap%20exists%20between%20Centralized%20Semi-Supervised%20Learning%0A%28SSL%29%20and%20FSSL.%20This%20gap%20arises%20from%20confirmation%20bias%2C%20which%20is%20more%0Apronounced%20in%20FSSL%20due%20to%20multiple%20local%20training%20epochs%20and%20the%20separation%20of%0Alabeled%20and%20unlabeled%20data.%20We%20propose%20%24%28FL%29%5E2%24%2C%20a%20robust%20training%20method%20for%0Aunlabeled%20clients%20using%20sharpness-aware%20consistency%20regularization.%20We%20show%0Athat%20regularizing%20the%20original%20pseudo-labeling%20loss%20is%20suboptimal%2C%20and%20hence%20we%0Acarefully%20select%20unlabeled%20samples%20for%20regularization.%20We%20further%20introduce%0Aclient-specific%20adaptive%20thresholding%20and%20learning%20status-aware%20aggregation%20to%0Aadjust%20the%20training%20process%20based%20on%20the%20learning%20progress%20of%20each%20client.%20Our%0Aexperiments%20on%20three%20benchmark%20datasets%20demonstrate%20that%20our%20approach%0Asignificantly%20improves%20performance%20and%20bridges%20the%20gap%20with%20SSL%2C%20particularly%0Ain%20scenarios%20with%20scarce%20labeled%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2528FL%2529%2524%255E2%2524%253A%2520Overcoming%2520Few%2520Labels%2520in%2520Federated%2520Semi-Supervised%2520Learning%26entry.906535625%3DSeungjoo%2520Lee%2520and%2520Thanh-Long%2520V.%2520Le%2520and%2520Jaemin%2520Shin%2520and%2520Sung-Ju%2520Lee%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520a%2520distributed%2520machine%2520learning%2520framework%2520that%250Atrains%2520accurate%2520global%2520models%2520while%2520preserving%2520clients%2527%2520privacy-sensitive%2520data.%250AHowever%252C%2520most%2520FL%2520approaches%2520assume%2520that%2520clients%2520possess%2520labeled%2520data%252C%2520which%2520is%250Aoften%2520not%2520the%2520case%2520in%2520practice.%2520Federated%2520Semi-Supervised%2520Learning%2520%2528FSSL%2529%250Aaddresses%2520this%2520label%2520deficiency%2520problem%252C%2520targeting%2520situations%2520where%2520only%2520the%250Aserver%2520has%2520a%2520small%2520amount%2520of%2520labeled%2520data%2520while%2520clients%2520do%2520not.%2520However%252C%2520a%250Asignificant%2520performance%2520gap%2520exists%2520between%2520Centralized%2520Semi-Supervised%2520Learning%250A%2528SSL%2529%2520and%2520FSSL.%2520This%2520gap%2520arises%2520from%2520confirmation%2520bias%252C%2520which%2520is%2520more%250Apronounced%2520in%2520FSSL%2520due%2520to%2520multiple%2520local%2520training%2520epochs%2520and%2520the%2520separation%2520of%250Alabeled%2520and%2520unlabeled%2520data.%2520We%2520propose%2520%2524%2528FL%2529%255E2%2524%252C%2520a%2520robust%2520training%2520method%2520for%250Aunlabeled%2520clients%2520using%2520sharpness-aware%2520consistency%2520regularization.%2520We%2520show%250Athat%2520regularizing%2520the%2520original%2520pseudo-labeling%2520loss%2520is%2520suboptimal%252C%2520and%2520hence%2520we%250Acarefully%2520select%2520unlabeled%2520samples%2520for%2520regularization.%2520We%2520further%2520introduce%250Aclient-specific%2520adaptive%2520thresholding%2520and%2520learning%2520status-aware%2520aggregation%2520to%250Aadjust%2520the%2520training%2520process%2520based%2520on%2520the%2520learning%2520progress%2520of%2520each%2520client.%2520Our%250Aexperiments%2520on%2520three%2520benchmark%2520datasets%2520demonstrate%2520that%2520our%2520approach%250Asignificantly%2520improves%2520performance%2520and%2520bridges%2520the%2520gap%2520with%2520SSL%252C%2520particularly%250Ain%2520scenarios%2520with%2520scarce%2520labeled%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%28FL%29%24%5E2%24%3A%20Overcoming%20Few%20Labels%20in%20Federated%20Semi-Supervised%20Learning&entry.906535625=Seungjoo%20Lee%20and%20Thanh-Long%20V.%20Le%20and%20Jaemin%20Shin%20and%20Sung-Ju%20Lee&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20a%20distributed%20machine%20learning%20framework%20that%0Atrains%20accurate%20global%20models%20while%20preserving%20clients%27%20privacy-sensitive%20data.%0AHowever%2C%20most%20FL%20approaches%20assume%20that%20clients%20possess%20labeled%20data%2C%20which%20is%0Aoften%20not%20the%20case%20in%20practice.%20Federated%20Semi-Supervised%20Learning%20%28FSSL%29%0Aaddresses%20this%20label%20deficiency%20problem%2C%20targeting%20situations%20where%20only%20the%0Aserver%20has%20a%20small%20amount%20of%20labeled%20data%20while%20clients%20do%20not.%20However%2C%20a%0Asignificant%20performance%20gap%20exists%20between%20Centralized%20Semi-Supervised%20Learning%0A%28SSL%29%20and%20FSSL.%20This%20gap%20arises%20from%20confirmation%20bias%2C%20which%20is%20more%0Apronounced%20in%20FSSL%20due%20to%20multiple%20local%20training%20epochs%20and%20the%20separation%20of%0Alabeled%20and%20unlabeled%20data.%20We%20propose%20%24%28FL%29%5E2%24%2C%20a%20robust%20training%20method%20for%0Aunlabeled%20clients%20using%20sharpness-aware%20consistency%20regularization.%20We%20show%0Athat%20regularizing%20the%20original%20pseudo-labeling%20loss%20is%20suboptimal%2C%20and%20hence%20we%0Acarefully%20select%20unlabeled%20samples%20for%20regularization.%20We%20further%20introduce%0Aclient-specific%20adaptive%20thresholding%20and%20learning%20status-aware%20aggregation%20to%0Aadjust%20the%20training%20process%20based%20on%20the%20learning%20progress%20of%20each%20client.%20Our%0Aexperiments%20on%20three%20benchmark%20datasets%20demonstrate%20that%20our%20approach%0Asignificantly%20improves%20performance%20and%20bridges%20the%20gap%20with%20SSL%2C%20particularly%0Ain%20scenarios%20with%20scarce%20labeled%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23227v1&entry.124074799=Read"},
{"title": "Does equivariance matter at scale?", "author": "Johann Brehmer and S\u00f6nke Behrends and Pim de Haan and Taco Cohen", "abstract": "  Given large data sets and sufficient compute, is it beneficial to design\nneural architectures for the structure and symmetries of each problem? Or is it\nmore efficient to learn them from data? We study empirically how equivariant\nand non-equivariant networks scale with compute and training samples. Focusing\non a benchmark problem of rigid-body interactions and on general-purpose\ntransformer architectures, we perform a series of experiments, varying the\nmodel size, training steps, and dataset size. We find evidence for three\nconclusions. First, equivariance improves data efficiency, but training\nnon-equivariant models with data augmentation can close this gap given\nsufficient epochs. Second, scaling with compute follows a power law, with\nequivariant models outperforming non-equivariant ones at each tested compute\nbudget. Finally, the optimal allocation of a compute budget onto model size and\ntraining duration differs between equivariant and non-equivariant models.\n", "link": "http://arxiv.org/abs/2410.23179v1", "date": "2024-10-30", "relevancy": 1.8959, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.567}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4622}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20equivariance%20matter%20at%20scale%3F&body=Title%3A%20Does%20equivariance%20matter%20at%20scale%3F%0AAuthor%3A%20Johann%20Brehmer%20and%20S%C3%B6nke%20Behrends%20and%20Pim%20de%20Haan%20and%20Taco%20Cohen%0AAbstract%3A%20%20%20Given%20large%20data%20sets%20and%20sufficient%20compute%2C%20is%20it%20beneficial%20to%20design%0Aneural%20architectures%20for%20the%20structure%20and%20symmetries%20of%20each%20problem%3F%20Or%20is%20it%0Amore%20efficient%20to%20learn%20them%20from%20data%3F%20We%20study%20empirically%20how%20equivariant%0Aand%20non-equivariant%20networks%20scale%20with%20compute%20and%20training%20samples.%20Focusing%0Aon%20a%20benchmark%20problem%20of%20rigid-body%20interactions%20and%20on%20general-purpose%0Atransformer%20architectures%2C%20we%20perform%20a%20series%20of%20experiments%2C%20varying%20the%0Amodel%20size%2C%20training%20steps%2C%20and%20dataset%20size.%20We%20find%20evidence%20for%20three%0Aconclusions.%20First%2C%20equivariance%20improves%20data%20efficiency%2C%20but%20training%0Anon-equivariant%20models%20with%20data%20augmentation%20can%20close%20this%20gap%20given%0Asufficient%20epochs.%20Second%2C%20scaling%20with%20compute%20follows%20a%20power%20law%2C%20with%0Aequivariant%20models%20outperforming%20non-equivariant%20ones%20at%20each%20tested%20compute%0Abudget.%20Finally%2C%20the%20optimal%20allocation%20of%20a%20compute%20budget%20onto%20model%20size%20and%0Atraining%20duration%20differs%20between%20equivariant%20and%20non-equivariant%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520equivariance%2520matter%2520at%2520scale%253F%26entry.906535625%3DJohann%2520Brehmer%2520and%2520S%25C3%25B6nke%2520Behrends%2520and%2520Pim%2520de%2520Haan%2520and%2520Taco%2520Cohen%26entry.1292438233%3D%2520%2520Given%2520large%2520data%2520sets%2520and%2520sufficient%2520compute%252C%2520is%2520it%2520beneficial%2520to%2520design%250Aneural%2520architectures%2520for%2520the%2520structure%2520and%2520symmetries%2520of%2520each%2520problem%253F%2520Or%2520is%2520it%250Amore%2520efficient%2520to%2520learn%2520them%2520from%2520data%253F%2520We%2520study%2520empirically%2520how%2520equivariant%250Aand%2520non-equivariant%2520networks%2520scale%2520with%2520compute%2520and%2520training%2520samples.%2520Focusing%250Aon%2520a%2520benchmark%2520problem%2520of%2520rigid-body%2520interactions%2520and%2520on%2520general-purpose%250Atransformer%2520architectures%252C%2520we%2520perform%2520a%2520series%2520of%2520experiments%252C%2520varying%2520the%250Amodel%2520size%252C%2520training%2520steps%252C%2520and%2520dataset%2520size.%2520We%2520find%2520evidence%2520for%2520three%250Aconclusions.%2520First%252C%2520equivariance%2520improves%2520data%2520efficiency%252C%2520but%2520training%250Anon-equivariant%2520models%2520with%2520data%2520augmentation%2520can%2520close%2520this%2520gap%2520given%250Asufficient%2520epochs.%2520Second%252C%2520scaling%2520with%2520compute%2520follows%2520a%2520power%2520law%252C%2520with%250Aequivariant%2520models%2520outperforming%2520non-equivariant%2520ones%2520at%2520each%2520tested%2520compute%250Abudget.%2520Finally%252C%2520the%2520optimal%2520allocation%2520of%2520a%2520compute%2520budget%2520onto%2520model%2520size%2520and%250Atraining%2520duration%2520differs%2520between%2520equivariant%2520and%2520non-equivariant%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20equivariance%20matter%20at%20scale%3F&entry.906535625=Johann%20Brehmer%20and%20S%C3%B6nke%20Behrends%20and%20Pim%20de%20Haan%20and%20Taco%20Cohen&entry.1292438233=%20%20Given%20large%20data%20sets%20and%20sufficient%20compute%2C%20is%20it%20beneficial%20to%20design%0Aneural%20architectures%20for%20the%20structure%20and%20symmetries%20of%20each%20problem%3F%20Or%20is%20it%0Amore%20efficient%20to%20learn%20them%20from%20data%3F%20We%20study%20empirically%20how%20equivariant%0Aand%20non-equivariant%20networks%20scale%20with%20compute%20and%20training%20samples.%20Focusing%0Aon%20a%20benchmark%20problem%20of%20rigid-body%20interactions%20and%20on%20general-purpose%0Atransformer%20architectures%2C%20we%20perform%20a%20series%20of%20experiments%2C%20varying%20the%0Amodel%20size%2C%20training%20steps%2C%20and%20dataset%20size.%20We%20find%20evidence%20for%20three%0Aconclusions.%20First%2C%20equivariance%20improves%20data%20efficiency%2C%20but%20training%0Anon-equivariant%20models%20with%20data%20augmentation%20can%20close%20this%20gap%20given%0Asufficient%20epochs.%20Second%2C%20scaling%20with%20compute%20follows%20a%20power%20law%2C%20with%0Aequivariant%20models%20outperforming%20non-equivariant%20ones%20at%20each%20tested%20compute%0Abudget.%20Finally%2C%20the%20optimal%20allocation%20of%20a%20compute%20budget%20onto%20model%20size%20and%0Atraining%20duration%20differs%20between%20equivariant%20and%20non-equivariant%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23179v1&entry.124074799=Read"},
{"title": "Multi-student Diffusion Distillation for Better One-step Generators", "author": "Yanke Song and Jonathan Lorraine and Weili Nie and Karsten Kreis and James Lucas", "abstract": "  Diffusion models achieve high-quality sample generation at the cost of a\nlengthy multistep inference procedure. To overcome this, diffusion distillation\ntechniques produce student generators capable of matching or surpassing the\nteacher in a single step. However, the student model's inference speed is\nlimited by the size of the teacher architecture, preventing real-time\ngeneration for computationally heavy applications. In this work, we introduce\nMulti-Student Distillation (MSD), a framework to distill a conditional teacher\ndiffusion model into multiple single-step generators. Each student generator is\nresponsible for a subset of the conditioning data, thereby obtaining higher\ngeneration quality for the same capacity. MSD trains multiple distilled\nstudents, allowing smaller sizes and, therefore, faster inference. Also, MSD\noffers a lightweight quality boost over single-student distillation with the\nsame architecture. We demonstrate MSD is effective by training multiple\nsame-sized or smaller students on single-step distillation using distribution\nmatching and adversarial distillation techniques. With smaller students, MSD\ngets competitive results with faster inference for single-step generation.\nUsing 4 same-sized students, MSD sets a new state-of-the-art for one-step image\ngeneration: FID 1.20 on ImageNet-64x64 and 8.20 on zero-shot COCO2014.\n", "link": "http://arxiv.org/abs/2410.23274v1", "date": "2024-10-30", "relevancy": 1.8922, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6875}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6205}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-student%20Diffusion%20Distillation%20for%20Better%20One-step%20Generators&body=Title%3A%20Multi-student%20Diffusion%20Distillation%20for%20Better%20One-step%20Generators%0AAuthor%3A%20Yanke%20Song%20and%20Jonathan%20Lorraine%20and%20Weili%20Nie%20and%20Karsten%20Kreis%20and%20James%20Lucas%0AAbstract%3A%20%20%20Diffusion%20models%20achieve%20high-quality%20sample%20generation%20at%20the%20cost%20of%20a%0Alengthy%20multistep%20inference%20procedure.%20To%20overcome%20this%2C%20diffusion%20distillation%0Atechniques%20produce%20student%20generators%20capable%20of%20matching%20or%20surpassing%20the%0Ateacher%20in%20a%20single%20step.%20However%2C%20the%20student%20model%27s%20inference%20speed%20is%0Alimited%20by%20the%20size%20of%20the%20teacher%20architecture%2C%20preventing%20real-time%0Ageneration%20for%20computationally%20heavy%20applications.%20In%20this%20work%2C%20we%20introduce%0AMulti-Student%20Distillation%20%28MSD%29%2C%20a%20framework%20to%20distill%20a%20conditional%20teacher%0Adiffusion%20model%20into%20multiple%20single-step%20generators.%20Each%20student%20generator%20is%0Aresponsible%20for%20a%20subset%20of%20the%20conditioning%20data%2C%20thereby%20obtaining%20higher%0Ageneration%20quality%20for%20the%20same%20capacity.%20MSD%20trains%20multiple%20distilled%0Astudents%2C%20allowing%20smaller%20sizes%20and%2C%20therefore%2C%20faster%20inference.%20Also%2C%20MSD%0Aoffers%20a%20lightweight%20quality%20boost%20over%20single-student%20distillation%20with%20the%0Asame%20architecture.%20We%20demonstrate%20MSD%20is%20effective%20by%20training%20multiple%0Asame-sized%20or%20smaller%20students%20on%20single-step%20distillation%20using%20distribution%0Amatching%20and%20adversarial%20distillation%20techniques.%20With%20smaller%20students%2C%20MSD%0Agets%20competitive%20results%20with%20faster%20inference%20for%20single-step%20generation.%0AUsing%204%20same-sized%20students%2C%20MSD%20sets%20a%20new%20state-of-the-art%20for%20one-step%20image%0Ageneration%3A%20FID%201.20%20on%20ImageNet-64x64%20and%208.20%20on%20zero-shot%20COCO2014.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23274v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-student%2520Diffusion%2520Distillation%2520for%2520Better%2520One-step%2520Generators%26entry.906535625%3DYanke%2520Song%2520and%2520Jonathan%2520Lorraine%2520and%2520Weili%2520Nie%2520and%2520Karsten%2520Kreis%2520and%2520James%2520Lucas%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520achieve%2520high-quality%2520sample%2520generation%2520at%2520the%2520cost%2520of%2520a%250Alengthy%2520multistep%2520inference%2520procedure.%2520To%2520overcome%2520this%252C%2520diffusion%2520distillation%250Atechniques%2520produce%2520student%2520generators%2520capable%2520of%2520matching%2520or%2520surpassing%2520the%250Ateacher%2520in%2520a%2520single%2520step.%2520However%252C%2520the%2520student%2520model%2527s%2520inference%2520speed%2520is%250Alimited%2520by%2520the%2520size%2520of%2520the%2520teacher%2520architecture%252C%2520preventing%2520real-time%250Ageneration%2520for%2520computationally%2520heavy%2520applications.%2520In%2520this%2520work%252C%2520we%2520introduce%250AMulti-Student%2520Distillation%2520%2528MSD%2529%252C%2520a%2520framework%2520to%2520distill%2520a%2520conditional%2520teacher%250Adiffusion%2520model%2520into%2520multiple%2520single-step%2520generators.%2520Each%2520student%2520generator%2520is%250Aresponsible%2520for%2520a%2520subset%2520of%2520the%2520conditioning%2520data%252C%2520thereby%2520obtaining%2520higher%250Ageneration%2520quality%2520for%2520the%2520same%2520capacity.%2520MSD%2520trains%2520multiple%2520distilled%250Astudents%252C%2520allowing%2520smaller%2520sizes%2520and%252C%2520therefore%252C%2520faster%2520inference.%2520Also%252C%2520MSD%250Aoffers%2520a%2520lightweight%2520quality%2520boost%2520over%2520single-student%2520distillation%2520with%2520the%250Asame%2520architecture.%2520We%2520demonstrate%2520MSD%2520is%2520effective%2520by%2520training%2520multiple%250Asame-sized%2520or%2520smaller%2520students%2520on%2520single-step%2520distillation%2520using%2520distribution%250Amatching%2520and%2520adversarial%2520distillation%2520techniques.%2520With%2520smaller%2520students%252C%2520MSD%250Agets%2520competitive%2520results%2520with%2520faster%2520inference%2520for%2520single-step%2520generation.%250AUsing%25204%2520same-sized%2520students%252C%2520MSD%2520sets%2520a%2520new%2520state-of-the-art%2520for%2520one-step%2520image%250Ageneration%253A%2520FID%25201.20%2520on%2520ImageNet-64x64%2520and%25208.20%2520on%2520zero-shot%2520COCO2014.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23274v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-student%20Diffusion%20Distillation%20for%20Better%20One-step%20Generators&entry.906535625=Yanke%20Song%20and%20Jonathan%20Lorraine%20and%20Weili%20Nie%20and%20Karsten%20Kreis%20and%20James%20Lucas&entry.1292438233=%20%20Diffusion%20models%20achieve%20high-quality%20sample%20generation%20at%20the%20cost%20of%20a%0Alengthy%20multistep%20inference%20procedure.%20To%20overcome%20this%2C%20diffusion%20distillation%0Atechniques%20produce%20student%20generators%20capable%20of%20matching%20or%20surpassing%20the%0Ateacher%20in%20a%20single%20step.%20However%2C%20the%20student%20model%27s%20inference%20speed%20is%0Alimited%20by%20the%20size%20of%20the%20teacher%20architecture%2C%20preventing%20real-time%0Ageneration%20for%20computationally%20heavy%20applications.%20In%20this%20work%2C%20we%20introduce%0AMulti-Student%20Distillation%20%28MSD%29%2C%20a%20framework%20to%20distill%20a%20conditional%20teacher%0Adiffusion%20model%20into%20multiple%20single-step%20generators.%20Each%20student%20generator%20is%0Aresponsible%20for%20a%20subset%20of%20the%20conditioning%20data%2C%20thereby%20obtaining%20higher%0Ageneration%20quality%20for%20the%20same%20capacity.%20MSD%20trains%20multiple%20distilled%0Astudents%2C%20allowing%20smaller%20sizes%20and%2C%20therefore%2C%20faster%20inference.%20Also%2C%20MSD%0Aoffers%20a%20lightweight%20quality%20boost%20over%20single-student%20distillation%20with%20the%0Asame%20architecture.%20We%20demonstrate%20MSD%20is%20effective%20by%20training%20multiple%0Asame-sized%20or%20smaller%20students%20on%20single-step%20distillation%20using%20distribution%0Amatching%20and%20adversarial%20distillation%20techniques.%20With%20smaller%20students%2C%20MSD%0Agets%20competitive%20results%20with%20faster%20inference%20for%20single-step%20generation.%0AUsing%204%20same-sized%20students%2C%20MSD%20sets%20a%20new%20state-of-the-art%20for%20one-step%20image%0Ageneration%3A%20FID%201.20%20on%20ImageNet-64x64%20and%208.20%20on%20zero-shot%20COCO2014.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23274v1&entry.124074799=Read"},
{"title": "COMAL: A Convergent Meta-Algorithm for Aligning LLMs with General\n  Preferences", "author": "Yixin Liu and Argyris Oikonomou and Weiqiang Zheng and Yang Cai and Arman Cohan", "abstract": "  Many alignment methods, including reinforcement learning from human feedback\n(RLHF), rely on the Bradley-Terry reward assumption, which is insufficient to\ncapture the full range of general human preferences. To achieve robust\nalignment with general preferences, we model the alignment problem as a\ntwo-player zero-sum game, where the Nash equilibrium policy guarantees a 50%\nwin rate against any competing policy. However, previous algorithms for finding\nthe Nash policy either diverge or converge to a Nash policy in a modified game,\neven in a simple synthetic setting, thereby failing to maintain the 50% win\nrate guarantee against all other policies. We propose a meta-algorithm,\nConvergent Meta Alignment Algorithm (COMAL), for language model alignment with\ngeneral preferences, inspired by convergent algorithms in game theory.\nTheoretically, we prove that our meta-algorithm converges to an exact Nash\npolicy in the last iterate. Additionally, our meta-algorithm is simple and can\nbe integrated with many existing methods designed for RLHF and preference\noptimization with minimal changes. Experimental results demonstrate the\neffectiveness of the proposed framework when combined with existing preference\npolicy optimization methods.\n", "link": "http://arxiv.org/abs/2410.23223v1", "date": "2024-10-30", "relevancy": 1.8707, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4743}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4643}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COMAL%3A%20A%20Convergent%20Meta-Algorithm%20for%20Aligning%20LLMs%20with%20General%0A%20%20Preferences&body=Title%3A%20COMAL%3A%20A%20Convergent%20Meta-Algorithm%20for%20Aligning%20LLMs%20with%20General%0A%20%20Preferences%0AAuthor%3A%20Yixin%20Liu%20and%20Argyris%20Oikonomou%20and%20Weiqiang%20Zheng%20and%20Yang%20Cai%20and%20Arman%20Cohan%0AAbstract%3A%20%20%20Many%20alignment%20methods%2C%20including%20reinforcement%20learning%20from%20human%20feedback%0A%28RLHF%29%2C%20rely%20on%20the%20Bradley-Terry%20reward%20assumption%2C%20which%20is%20insufficient%20to%0Acapture%20the%20full%20range%20of%20general%20human%20preferences.%20To%20achieve%20robust%0Aalignment%20with%20general%20preferences%2C%20we%20model%20the%20alignment%20problem%20as%20a%0Atwo-player%20zero-sum%20game%2C%20where%20the%20Nash%20equilibrium%20policy%20guarantees%20a%2050%25%0Awin%20rate%20against%20any%20competing%20policy.%20However%2C%20previous%20algorithms%20for%20finding%0Athe%20Nash%20policy%20either%20diverge%20or%20converge%20to%20a%20Nash%20policy%20in%20a%20modified%20game%2C%0Aeven%20in%20a%20simple%20synthetic%20setting%2C%20thereby%20failing%20to%20maintain%20the%2050%25%20win%0Arate%20guarantee%20against%20all%20other%20policies.%20We%20propose%20a%20meta-algorithm%2C%0AConvergent%20Meta%20Alignment%20Algorithm%20%28COMAL%29%2C%20for%20language%20model%20alignment%20with%0Ageneral%20preferences%2C%20inspired%20by%20convergent%20algorithms%20in%20game%20theory.%0ATheoretically%2C%20we%20prove%20that%20our%20meta-algorithm%20converges%20to%20an%20exact%20Nash%0Apolicy%20in%20the%20last%20iterate.%20Additionally%2C%20our%20meta-algorithm%20is%20simple%20and%20can%0Abe%20integrated%20with%20many%20existing%20methods%20designed%20for%20RLHF%20and%20preference%0Aoptimization%20with%20minimal%20changes.%20Experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20framework%20when%20combined%20with%20existing%20preference%0Apolicy%20optimization%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOMAL%253A%2520A%2520Convergent%2520Meta-Algorithm%2520for%2520Aligning%2520LLMs%2520with%2520General%250A%2520%2520Preferences%26entry.906535625%3DYixin%2520Liu%2520and%2520Argyris%2520Oikonomou%2520and%2520Weiqiang%2520Zheng%2520and%2520Yang%2520Cai%2520and%2520Arman%2520Cohan%26entry.1292438233%3D%2520%2520Many%2520alignment%2520methods%252C%2520including%2520reinforcement%2520learning%2520from%2520human%2520feedback%250A%2528RLHF%2529%252C%2520rely%2520on%2520the%2520Bradley-Terry%2520reward%2520assumption%252C%2520which%2520is%2520insufficient%2520to%250Acapture%2520the%2520full%2520range%2520of%2520general%2520human%2520preferences.%2520To%2520achieve%2520robust%250Aalignment%2520with%2520general%2520preferences%252C%2520we%2520model%2520the%2520alignment%2520problem%2520as%2520a%250Atwo-player%2520zero-sum%2520game%252C%2520where%2520the%2520Nash%2520equilibrium%2520policy%2520guarantees%2520a%252050%2525%250Awin%2520rate%2520against%2520any%2520competing%2520policy.%2520However%252C%2520previous%2520algorithms%2520for%2520finding%250Athe%2520Nash%2520policy%2520either%2520diverge%2520or%2520converge%2520to%2520a%2520Nash%2520policy%2520in%2520a%2520modified%2520game%252C%250Aeven%2520in%2520a%2520simple%2520synthetic%2520setting%252C%2520thereby%2520failing%2520to%2520maintain%2520the%252050%2525%2520win%250Arate%2520guarantee%2520against%2520all%2520other%2520policies.%2520We%2520propose%2520a%2520meta-algorithm%252C%250AConvergent%2520Meta%2520Alignment%2520Algorithm%2520%2528COMAL%2529%252C%2520for%2520language%2520model%2520alignment%2520with%250Ageneral%2520preferences%252C%2520inspired%2520by%2520convergent%2520algorithms%2520in%2520game%2520theory.%250ATheoretically%252C%2520we%2520prove%2520that%2520our%2520meta-algorithm%2520converges%2520to%2520an%2520exact%2520Nash%250Apolicy%2520in%2520the%2520last%2520iterate.%2520Additionally%252C%2520our%2520meta-algorithm%2520is%2520simple%2520and%2520can%250Abe%2520integrated%2520with%2520many%2520existing%2520methods%2520designed%2520for%2520RLHF%2520and%2520preference%250Aoptimization%2520with%2520minimal%2520changes.%2520Experimental%2520results%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520framework%2520when%2520combined%2520with%2520existing%2520preference%250Apolicy%2520optimization%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COMAL%3A%20A%20Convergent%20Meta-Algorithm%20for%20Aligning%20LLMs%20with%20General%0A%20%20Preferences&entry.906535625=Yixin%20Liu%20and%20Argyris%20Oikonomou%20and%20Weiqiang%20Zheng%20and%20Yang%20Cai%20and%20Arman%20Cohan&entry.1292438233=%20%20Many%20alignment%20methods%2C%20including%20reinforcement%20learning%20from%20human%20feedback%0A%28RLHF%29%2C%20rely%20on%20the%20Bradley-Terry%20reward%20assumption%2C%20which%20is%20insufficient%20to%0Acapture%20the%20full%20range%20of%20general%20human%20preferences.%20To%20achieve%20robust%0Aalignment%20with%20general%20preferences%2C%20we%20model%20the%20alignment%20problem%20as%20a%0Atwo-player%20zero-sum%20game%2C%20where%20the%20Nash%20equilibrium%20policy%20guarantees%20a%2050%25%0Awin%20rate%20against%20any%20competing%20policy.%20However%2C%20previous%20algorithms%20for%20finding%0Athe%20Nash%20policy%20either%20diverge%20or%20converge%20to%20a%20Nash%20policy%20in%20a%20modified%20game%2C%0Aeven%20in%20a%20simple%20synthetic%20setting%2C%20thereby%20failing%20to%20maintain%20the%2050%25%20win%0Arate%20guarantee%20against%20all%20other%20policies.%20We%20propose%20a%20meta-algorithm%2C%0AConvergent%20Meta%20Alignment%20Algorithm%20%28COMAL%29%2C%20for%20language%20model%20alignment%20with%0Ageneral%20preferences%2C%20inspired%20by%20convergent%20algorithms%20in%20game%20theory.%0ATheoretically%2C%20we%20prove%20that%20our%20meta-algorithm%20converges%20to%20an%20exact%20Nash%0Apolicy%20in%20the%20last%20iterate.%20Additionally%2C%20our%20meta-algorithm%20is%20simple%20and%20can%0Abe%20integrated%20with%20many%20existing%20methods%20designed%20for%20RLHF%20and%20preference%0Aoptimization%20with%20minimal%20changes.%20Experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20framework%20when%20combined%20with%20existing%20preference%0Apolicy%20optimization%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23223v1&entry.124074799=Read"},
{"title": "FoLDTree: A ULDA-Based Decision Tree Framework for Efficient Oblique\n  Splits and Feature Selection", "author": "Siyu Wang", "abstract": "  Traditional decision trees are limited by axis-orthogonal splits, which can\nperform poorly when true decision boundaries are oblique. While oblique\ndecision tree methods address this limitation, they often face high\ncomputational costs, difficulties with multi-class classification, and a lack\nof effective feature selection. In this paper, we introduce LDATree and\nFoLDTree, two novel frameworks that integrate Uncorrelated Linear Discriminant\nAnalysis (ULDA) and Forward ULDA into a decision tree structure. These methods\nenable efficient oblique splits, handle missing values, support feature\nselection, and provide both class labels and probabilities as model outputs.\nThrough evaluations on simulated and real-world datasets, LDATree and FoLDTree\nconsistently outperform axis-orthogonal and other oblique decision tree\nmethods, achieving accuracy levels comparable to the random forest. The results\nhighlight the potential of these frameworks as robust alternatives to\ntraditional single-tree methods.\n", "link": "http://arxiv.org/abs/2410.23147v1", "date": "2024-10-30", "relevancy": 1.8616, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4748}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4676}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FoLDTree%3A%20A%20ULDA-Based%20Decision%20Tree%20Framework%20for%20Efficient%20Oblique%0A%20%20Splits%20and%20Feature%20Selection&body=Title%3A%20FoLDTree%3A%20A%20ULDA-Based%20Decision%20Tree%20Framework%20for%20Efficient%20Oblique%0A%20%20Splits%20and%20Feature%20Selection%0AAuthor%3A%20Siyu%20Wang%0AAbstract%3A%20%20%20Traditional%20decision%20trees%20are%20limited%20by%20axis-orthogonal%20splits%2C%20which%20can%0Aperform%20poorly%20when%20true%20decision%20boundaries%20are%20oblique.%20While%20oblique%0Adecision%20tree%20methods%20address%20this%20limitation%2C%20they%20often%20face%20high%0Acomputational%20costs%2C%20difficulties%20with%20multi-class%20classification%2C%20and%20a%20lack%0Aof%20effective%20feature%20selection.%20In%20this%20paper%2C%20we%20introduce%20LDATree%20and%0AFoLDTree%2C%20two%20novel%20frameworks%20that%20integrate%20Uncorrelated%20Linear%20Discriminant%0AAnalysis%20%28ULDA%29%20and%20Forward%20ULDA%20into%20a%20decision%20tree%20structure.%20These%20methods%0Aenable%20efficient%20oblique%20splits%2C%20handle%20missing%20values%2C%20support%20feature%0Aselection%2C%20and%20provide%20both%20class%20labels%20and%20probabilities%20as%20model%20outputs.%0AThrough%20evaluations%20on%20simulated%20and%20real-world%20datasets%2C%20LDATree%20and%20FoLDTree%0Aconsistently%20outperform%20axis-orthogonal%20and%20other%20oblique%20decision%20tree%0Amethods%2C%20achieving%20accuracy%20levels%20comparable%20to%20the%20random%20forest.%20The%20results%0Ahighlight%20the%20potential%20of%20these%20frameworks%20as%20robust%20alternatives%20to%0Atraditional%20single-tree%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoLDTree%253A%2520A%2520ULDA-Based%2520Decision%2520Tree%2520Framework%2520for%2520Efficient%2520Oblique%250A%2520%2520Splits%2520and%2520Feature%2520Selection%26entry.906535625%3DSiyu%2520Wang%26entry.1292438233%3D%2520%2520Traditional%2520decision%2520trees%2520are%2520limited%2520by%2520axis-orthogonal%2520splits%252C%2520which%2520can%250Aperform%2520poorly%2520when%2520true%2520decision%2520boundaries%2520are%2520oblique.%2520While%2520oblique%250Adecision%2520tree%2520methods%2520address%2520this%2520limitation%252C%2520they%2520often%2520face%2520high%250Acomputational%2520costs%252C%2520difficulties%2520with%2520multi-class%2520classification%252C%2520and%2520a%2520lack%250Aof%2520effective%2520feature%2520selection.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520LDATree%2520and%250AFoLDTree%252C%2520two%2520novel%2520frameworks%2520that%2520integrate%2520Uncorrelated%2520Linear%2520Discriminant%250AAnalysis%2520%2528ULDA%2529%2520and%2520Forward%2520ULDA%2520into%2520a%2520decision%2520tree%2520structure.%2520These%2520methods%250Aenable%2520efficient%2520oblique%2520splits%252C%2520handle%2520missing%2520values%252C%2520support%2520feature%250Aselection%252C%2520and%2520provide%2520both%2520class%2520labels%2520and%2520probabilities%2520as%2520model%2520outputs.%250AThrough%2520evaluations%2520on%2520simulated%2520and%2520real-world%2520datasets%252C%2520LDATree%2520and%2520FoLDTree%250Aconsistently%2520outperform%2520axis-orthogonal%2520and%2520other%2520oblique%2520decision%2520tree%250Amethods%252C%2520achieving%2520accuracy%2520levels%2520comparable%2520to%2520the%2520random%2520forest.%2520The%2520results%250Ahighlight%2520the%2520potential%2520of%2520these%2520frameworks%2520as%2520robust%2520alternatives%2520to%250Atraditional%2520single-tree%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FoLDTree%3A%20A%20ULDA-Based%20Decision%20Tree%20Framework%20for%20Efficient%20Oblique%0A%20%20Splits%20and%20Feature%20Selection&entry.906535625=Siyu%20Wang&entry.1292438233=%20%20Traditional%20decision%20trees%20are%20limited%20by%20axis-orthogonal%20splits%2C%20which%20can%0Aperform%20poorly%20when%20true%20decision%20boundaries%20are%20oblique.%20While%20oblique%0Adecision%20tree%20methods%20address%20this%20limitation%2C%20they%20often%20face%20high%0Acomputational%20costs%2C%20difficulties%20with%20multi-class%20classification%2C%20and%20a%20lack%0Aof%20effective%20feature%20selection.%20In%20this%20paper%2C%20we%20introduce%20LDATree%20and%0AFoLDTree%2C%20two%20novel%20frameworks%20that%20integrate%20Uncorrelated%20Linear%20Discriminant%0AAnalysis%20%28ULDA%29%20and%20Forward%20ULDA%20into%20a%20decision%20tree%20structure.%20These%20methods%0Aenable%20efficient%20oblique%20splits%2C%20handle%20missing%20values%2C%20support%20feature%0Aselection%2C%20and%20provide%20both%20class%20labels%20and%20probabilities%20as%20model%20outputs.%0AThrough%20evaluations%20on%20simulated%20and%20real-world%20datasets%2C%20LDATree%20and%20FoLDTree%0Aconsistently%20outperform%20axis-orthogonal%20and%20other%20oblique%20decision%20tree%0Amethods%2C%20achieving%20accuracy%20levels%20comparable%20to%20the%20random%20forest.%20The%20results%0Ahighlight%20the%20potential%20of%20these%20frameworks%20as%20robust%20alternatives%20to%0Atraditional%20single-tree%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23147v1&entry.124074799=Read"},
{"title": "Bandits with Preference Feedback: A Stackelberg Game Perspective", "author": "Barna P\u00e1sztor and Parnian Kassraie and Andreas Krause", "abstract": "  Bandits with preference feedback present a powerful tool for optimizing\nunknown target functions when only pairwise comparisons are allowed instead of\ndirect value queries. This model allows for incorporating human feedback into\nonline inference and optimization and has been employed in systems for\nfine-tuning large language models. The problem is well understood in simplified\nsettings with linear target functions or over finite small domains that limit\npractical interest. Taking the next step, we consider infinite domains and\nnonlinear (kernelized) rewards. In this setting, selecting a pair of actions is\nquite challenging and requires balancing exploration and exploitation at two\nlevels: within the pair, and along the iterations of the algorithm. We propose\nMAXMINLCB, which emulates this trade-off as a zero-sum Stackelberg game, and\nchooses action pairs that are informative and yield favorable rewards.\nMAXMINLCB consistently outperforms existing algorithms and satisfies an\nanytime-valid rate-optimal regret guarantee. This is due to our novel\npreference-based confidence sequences for kernelized logistic estimators.\n", "link": "http://arxiv.org/abs/2406.16745v2", "date": "2024-10-30", "relevancy": 1.8584, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4934}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4709}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bandits%20with%20Preference%20Feedback%3A%20A%20Stackelberg%20Game%20Perspective&body=Title%3A%20Bandits%20with%20Preference%20Feedback%3A%20A%20Stackelberg%20Game%20Perspective%0AAuthor%3A%20Barna%20P%C3%A1sztor%20and%20Parnian%20Kassraie%20and%20Andreas%20Krause%0AAbstract%3A%20%20%20Bandits%20with%20preference%20feedback%20present%20a%20powerful%20tool%20for%20optimizing%0Aunknown%20target%20functions%20when%20only%20pairwise%20comparisons%20are%20allowed%20instead%20of%0Adirect%20value%20queries.%20This%20model%20allows%20for%20incorporating%20human%20feedback%20into%0Aonline%20inference%20and%20optimization%20and%20has%20been%20employed%20in%20systems%20for%0Afine-tuning%20large%20language%20models.%20The%20problem%20is%20well%20understood%20in%20simplified%0Asettings%20with%20linear%20target%20functions%20or%20over%20finite%20small%20domains%20that%20limit%0Apractical%20interest.%20Taking%20the%20next%20step%2C%20we%20consider%20infinite%20domains%20and%0Anonlinear%20%28kernelized%29%20rewards.%20In%20this%20setting%2C%20selecting%20a%20pair%20of%20actions%20is%0Aquite%20challenging%20and%20requires%20balancing%20exploration%20and%20exploitation%20at%20two%0Alevels%3A%20within%20the%20pair%2C%20and%20along%20the%20iterations%20of%20the%20algorithm.%20We%20propose%0AMAXMINLCB%2C%20which%20emulates%20this%20trade-off%20as%20a%20zero-sum%20Stackelberg%20game%2C%20and%0Achooses%20action%20pairs%20that%20are%20informative%20and%20yield%20favorable%20rewards.%0AMAXMINLCB%20consistently%20outperforms%20existing%20algorithms%20and%20satisfies%20an%0Aanytime-valid%20rate-optimal%20regret%20guarantee.%20This%20is%20due%20to%20our%20novel%0Apreference-based%20confidence%20sequences%20for%20kernelized%20logistic%20estimators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16745v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBandits%2520with%2520Preference%2520Feedback%253A%2520A%2520Stackelberg%2520Game%2520Perspective%26entry.906535625%3DBarna%2520P%25C3%25A1sztor%2520and%2520Parnian%2520Kassraie%2520and%2520Andreas%2520Krause%26entry.1292438233%3D%2520%2520Bandits%2520with%2520preference%2520feedback%2520present%2520a%2520powerful%2520tool%2520for%2520optimizing%250Aunknown%2520target%2520functions%2520when%2520only%2520pairwise%2520comparisons%2520are%2520allowed%2520instead%2520of%250Adirect%2520value%2520queries.%2520This%2520model%2520allows%2520for%2520incorporating%2520human%2520feedback%2520into%250Aonline%2520inference%2520and%2520optimization%2520and%2520has%2520been%2520employed%2520in%2520systems%2520for%250Afine-tuning%2520large%2520language%2520models.%2520The%2520problem%2520is%2520well%2520understood%2520in%2520simplified%250Asettings%2520with%2520linear%2520target%2520functions%2520or%2520over%2520finite%2520small%2520domains%2520that%2520limit%250Apractical%2520interest.%2520Taking%2520the%2520next%2520step%252C%2520we%2520consider%2520infinite%2520domains%2520and%250Anonlinear%2520%2528kernelized%2529%2520rewards.%2520In%2520this%2520setting%252C%2520selecting%2520a%2520pair%2520of%2520actions%2520is%250Aquite%2520challenging%2520and%2520requires%2520balancing%2520exploration%2520and%2520exploitation%2520at%2520two%250Alevels%253A%2520within%2520the%2520pair%252C%2520and%2520along%2520the%2520iterations%2520of%2520the%2520algorithm.%2520We%2520propose%250AMAXMINLCB%252C%2520which%2520emulates%2520this%2520trade-off%2520as%2520a%2520zero-sum%2520Stackelberg%2520game%252C%2520and%250Achooses%2520action%2520pairs%2520that%2520are%2520informative%2520and%2520yield%2520favorable%2520rewards.%250AMAXMINLCB%2520consistently%2520outperforms%2520existing%2520algorithms%2520and%2520satisfies%2520an%250Aanytime-valid%2520rate-optimal%2520regret%2520guarantee.%2520This%2520is%2520due%2520to%2520our%2520novel%250Apreference-based%2520confidence%2520sequences%2520for%2520kernelized%2520logistic%2520estimators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16745v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bandits%20with%20Preference%20Feedback%3A%20A%20Stackelberg%20Game%20Perspective&entry.906535625=Barna%20P%C3%A1sztor%20and%20Parnian%20Kassraie%20and%20Andreas%20Krause&entry.1292438233=%20%20Bandits%20with%20preference%20feedback%20present%20a%20powerful%20tool%20for%20optimizing%0Aunknown%20target%20functions%20when%20only%20pairwise%20comparisons%20are%20allowed%20instead%20of%0Adirect%20value%20queries.%20This%20model%20allows%20for%20incorporating%20human%20feedback%20into%0Aonline%20inference%20and%20optimization%20and%20has%20been%20employed%20in%20systems%20for%0Afine-tuning%20large%20language%20models.%20The%20problem%20is%20well%20understood%20in%20simplified%0Asettings%20with%20linear%20target%20functions%20or%20over%20finite%20small%20domains%20that%20limit%0Apractical%20interest.%20Taking%20the%20next%20step%2C%20we%20consider%20infinite%20domains%20and%0Anonlinear%20%28kernelized%29%20rewards.%20In%20this%20setting%2C%20selecting%20a%20pair%20of%20actions%20is%0Aquite%20challenging%20and%20requires%20balancing%20exploration%20and%20exploitation%20at%20two%0Alevels%3A%20within%20the%20pair%2C%20and%20along%20the%20iterations%20of%20the%20algorithm.%20We%20propose%0AMAXMINLCB%2C%20which%20emulates%20this%20trade-off%20as%20a%20zero-sum%20Stackelberg%20game%2C%20and%0Achooses%20action%20pairs%20that%20are%20informative%20and%20yield%20favorable%20rewards.%0AMAXMINLCB%20consistently%20outperforms%20existing%20algorithms%20and%20satisfies%20an%0Aanytime-valid%20rate-optimal%20regret%20guarantee.%20This%20is%20due%20to%20our%20novel%0Apreference-based%20confidence%20sequences%20for%20kernelized%20logistic%20estimators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16745v2&entry.124074799=Read"},
{"title": "Adam with model exponential moving average is effective for nonconvex\n  optimization", "author": "Kwangjun Ahn and Ashok Cutkosky", "abstract": "  In this work, we offer a theoretical analysis of two modern optimization\ntechniques for training large and complex models: (i) adaptive optimization\nalgorithms, such as Adam, and (ii) the model exponential moving average (EMA).\nSpecifically, we demonstrate that a clipped version of Adam with model EMA\nachieves the optimal convergence rates in various nonconvex optimization\nsettings, both smooth and nonsmooth. Moreover, when the scale varies\nsignificantly across different coordinates, we demonstrate that the\ncoordinate-wise adaptivity of Adam is provably advantageous. Notably, unlike\nprevious analyses of Adam, our analysis crucially relies on its core elements\n-- momentum and discounting factors -- as well as model EMA, motivating their\nwide applications in practice.\n", "link": "http://arxiv.org/abs/2405.18199v2", "date": "2024-10-30", "relevancy": 1.8489, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.479}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.453}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adam%20with%20model%20exponential%20moving%20average%20is%20effective%20for%20nonconvex%0A%20%20optimization&body=Title%3A%20Adam%20with%20model%20exponential%20moving%20average%20is%20effective%20for%20nonconvex%0A%20%20optimization%0AAuthor%3A%20Kwangjun%20Ahn%20and%20Ashok%20Cutkosky%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20offer%20a%20theoretical%20analysis%20of%20two%20modern%20optimization%0Atechniques%20for%20training%20large%20and%20complex%20models%3A%20%28i%29%20adaptive%20optimization%0Aalgorithms%2C%20such%20as%20Adam%2C%20and%20%28ii%29%20the%20model%20exponential%20moving%20average%20%28EMA%29.%0ASpecifically%2C%20we%20demonstrate%20that%20a%20clipped%20version%20of%20Adam%20with%20model%20EMA%0Aachieves%20the%20optimal%20convergence%20rates%20in%20various%20nonconvex%20optimization%0Asettings%2C%20both%20smooth%20and%20nonsmooth.%20Moreover%2C%20when%20the%20scale%20varies%0Asignificantly%20across%20different%20coordinates%2C%20we%20demonstrate%20that%20the%0Acoordinate-wise%20adaptivity%20of%20Adam%20is%20provably%20advantageous.%20Notably%2C%20unlike%0Aprevious%20analyses%20of%20Adam%2C%20our%20analysis%20crucially%20relies%20on%20its%20core%20elements%0A--%20momentum%20and%20discounting%20factors%20--%20as%20well%20as%20model%20EMA%2C%20motivating%20their%0Awide%20applications%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18199v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdam%2520with%2520model%2520exponential%2520moving%2520average%2520is%2520effective%2520for%2520nonconvex%250A%2520%2520optimization%26entry.906535625%3DKwangjun%2520Ahn%2520and%2520Ashok%2520Cutkosky%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520offer%2520a%2520theoretical%2520analysis%2520of%2520two%2520modern%2520optimization%250Atechniques%2520for%2520training%2520large%2520and%2520complex%2520models%253A%2520%2528i%2529%2520adaptive%2520optimization%250Aalgorithms%252C%2520such%2520as%2520Adam%252C%2520and%2520%2528ii%2529%2520the%2520model%2520exponential%2520moving%2520average%2520%2528EMA%2529.%250ASpecifically%252C%2520we%2520demonstrate%2520that%2520a%2520clipped%2520version%2520of%2520Adam%2520with%2520model%2520EMA%250Aachieves%2520the%2520optimal%2520convergence%2520rates%2520in%2520various%2520nonconvex%2520optimization%250Asettings%252C%2520both%2520smooth%2520and%2520nonsmooth.%2520Moreover%252C%2520when%2520the%2520scale%2520varies%250Asignificantly%2520across%2520different%2520coordinates%252C%2520we%2520demonstrate%2520that%2520the%250Acoordinate-wise%2520adaptivity%2520of%2520Adam%2520is%2520provably%2520advantageous.%2520Notably%252C%2520unlike%250Aprevious%2520analyses%2520of%2520Adam%252C%2520our%2520analysis%2520crucially%2520relies%2520on%2520its%2520core%2520elements%250A--%2520momentum%2520and%2520discounting%2520factors%2520--%2520as%2520well%2520as%2520model%2520EMA%252C%2520motivating%2520their%250Awide%2520applications%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18199v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adam%20with%20model%20exponential%20moving%20average%20is%20effective%20for%20nonconvex%0A%20%20optimization&entry.906535625=Kwangjun%20Ahn%20and%20Ashok%20Cutkosky&entry.1292438233=%20%20In%20this%20work%2C%20we%20offer%20a%20theoretical%20analysis%20of%20two%20modern%20optimization%0Atechniques%20for%20training%20large%20and%20complex%20models%3A%20%28i%29%20adaptive%20optimization%0Aalgorithms%2C%20such%20as%20Adam%2C%20and%20%28ii%29%20the%20model%20exponential%20moving%20average%20%28EMA%29.%0ASpecifically%2C%20we%20demonstrate%20that%20a%20clipped%20version%20of%20Adam%20with%20model%20EMA%0Aachieves%20the%20optimal%20convergence%20rates%20in%20various%20nonconvex%20optimization%0Asettings%2C%20both%20smooth%20and%20nonsmooth.%20Moreover%2C%20when%20the%20scale%20varies%0Asignificantly%20across%20different%20coordinates%2C%20we%20demonstrate%20that%20the%0Acoordinate-wise%20adaptivity%20of%20Adam%20is%20provably%20advantageous.%20Notably%2C%20unlike%0Aprevious%20analyses%20of%20Adam%2C%20our%20analysis%20crucially%20relies%20on%20its%20core%20elements%0A--%20momentum%20and%20discounting%20factors%20--%20as%20well%20as%20model%20EMA%2C%20motivating%20their%0Awide%20applications%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18199v2&entry.124074799=Read"},
{"title": "Breach By A Thousand Leaks: Unsafe Information Leakage in `Safe' AI\n  Responses", "author": "David Glukhov and Ziwen Han and Ilia Shumailov and Vardan Papyan and Nicolas Papernot", "abstract": "  Vulnerability of Frontier language models to misuse and jailbreaks has\nprompted the development of safety measures like filters and alignment training\nin an effort to ensure safety through robustness to adversarially crafted\nprompts. We assert that robustness is fundamentally insufficient for ensuring\nsafety goals, and current defenses and evaluation methods fail to account for\nrisks of dual-intent queries and their composition for malicious goals. To\nquantify these risks, we introduce a new safety evaluation framework based on\nimpermissible information leakage of model outputs and demonstrate how our\nproposed question-decomposition attack can extract dangerous knowledge from a\ncensored LLM more effectively than traditional jailbreaking. Underlying our\nproposed evaluation method is a novel information-theoretic threat model of\ninferential adversaries, distinguished from security adversaries, such as\njailbreaks, in that success is measured by inferring impermissible knowledge\nfrom victim outputs as opposed to forcing explicitly impermissible outputs from\nthe victim. Through our information-theoretic framework, we show that to ensure\nsafety against inferential adversaries, defense mechanisms must ensure\ninformation censorship, bounding the leakage of impermissible information.\nHowever, we prove that such defenses inevitably incur a safety-utility\ntrade-off.\n", "link": "http://arxiv.org/abs/2407.02551v2", "date": "2024-10-30", "relevancy": 1.822, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4644}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4537}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breach%20By%20A%20Thousand%20Leaks%3A%20Unsafe%20Information%20Leakage%20in%20%60Safe%27%20AI%0A%20%20Responses&body=Title%3A%20Breach%20By%20A%20Thousand%20Leaks%3A%20Unsafe%20Information%20Leakage%20in%20%60Safe%27%20AI%0A%20%20Responses%0AAuthor%3A%20David%20Glukhov%20and%20Ziwen%20Han%20and%20Ilia%20Shumailov%20and%20Vardan%20Papyan%20and%20Nicolas%20Papernot%0AAbstract%3A%20%20%20Vulnerability%20of%20Frontier%20language%20models%20to%20misuse%20and%20jailbreaks%20has%0Aprompted%20the%20development%20of%20safety%20measures%20like%20filters%20and%20alignment%20training%0Ain%20an%20effort%20to%20ensure%20safety%20through%20robustness%20to%20adversarially%20crafted%0Aprompts.%20We%20assert%20that%20robustness%20is%20fundamentally%20insufficient%20for%20ensuring%0Asafety%20goals%2C%20and%20current%20defenses%20and%20evaluation%20methods%20fail%20to%20account%20for%0Arisks%20of%20dual-intent%20queries%20and%20their%20composition%20for%20malicious%20goals.%20To%0Aquantify%20these%20risks%2C%20we%20introduce%20a%20new%20safety%20evaluation%20framework%20based%20on%0Aimpermissible%20information%20leakage%20of%20model%20outputs%20and%20demonstrate%20how%20our%0Aproposed%20question-decomposition%20attack%20can%20extract%20dangerous%20knowledge%20from%20a%0Acensored%20LLM%20more%20effectively%20than%20traditional%20jailbreaking.%20Underlying%20our%0Aproposed%20evaluation%20method%20is%20a%20novel%20information-theoretic%20threat%20model%20of%0Ainferential%20adversaries%2C%20distinguished%20from%20security%20adversaries%2C%20such%20as%0Ajailbreaks%2C%20in%20that%20success%20is%20measured%20by%20inferring%20impermissible%20knowledge%0Afrom%20victim%20outputs%20as%20opposed%20to%20forcing%20explicitly%20impermissible%20outputs%20from%0Athe%20victim.%20Through%20our%20information-theoretic%20framework%2C%20we%20show%20that%20to%20ensure%0Asafety%20against%20inferential%20adversaries%2C%20defense%20mechanisms%20must%20ensure%0Ainformation%20censorship%2C%20bounding%20the%20leakage%20of%20impermissible%20information.%0AHowever%2C%20we%20prove%20that%20such%20defenses%20inevitably%20incur%20a%20safety-utility%0Atrade-off.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02551v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreach%2520By%2520A%2520Thousand%2520Leaks%253A%2520Unsafe%2520Information%2520Leakage%2520in%2520%2560Safe%2527%2520AI%250A%2520%2520Responses%26entry.906535625%3DDavid%2520Glukhov%2520and%2520Ziwen%2520Han%2520and%2520Ilia%2520Shumailov%2520and%2520Vardan%2520Papyan%2520and%2520Nicolas%2520Papernot%26entry.1292438233%3D%2520%2520Vulnerability%2520of%2520Frontier%2520language%2520models%2520to%2520misuse%2520and%2520jailbreaks%2520has%250Aprompted%2520the%2520development%2520of%2520safety%2520measures%2520like%2520filters%2520and%2520alignment%2520training%250Ain%2520an%2520effort%2520to%2520ensure%2520safety%2520through%2520robustness%2520to%2520adversarially%2520crafted%250Aprompts.%2520We%2520assert%2520that%2520robustness%2520is%2520fundamentally%2520insufficient%2520for%2520ensuring%250Asafety%2520goals%252C%2520and%2520current%2520defenses%2520and%2520evaluation%2520methods%2520fail%2520to%2520account%2520for%250Arisks%2520of%2520dual-intent%2520queries%2520and%2520their%2520composition%2520for%2520malicious%2520goals.%2520To%250Aquantify%2520these%2520risks%252C%2520we%2520introduce%2520a%2520new%2520safety%2520evaluation%2520framework%2520based%2520on%250Aimpermissible%2520information%2520leakage%2520of%2520model%2520outputs%2520and%2520demonstrate%2520how%2520our%250Aproposed%2520question-decomposition%2520attack%2520can%2520extract%2520dangerous%2520knowledge%2520from%2520a%250Acensored%2520LLM%2520more%2520effectively%2520than%2520traditional%2520jailbreaking.%2520Underlying%2520our%250Aproposed%2520evaluation%2520method%2520is%2520a%2520novel%2520information-theoretic%2520threat%2520model%2520of%250Ainferential%2520adversaries%252C%2520distinguished%2520from%2520security%2520adversaries%252C%2520such%2520as%250Ajailbreaks%252C%2520in%2520that%2520success%2520is%2520measured%2520by%2520inferring%2520impermissible%2520knowledge%250Afrom%2520victim%2520outputs%2520as%2520opposed%2520to%2520forcing%2520explicitly%2520impermissible%2520outputs%2520from%250Athe%2520victim.%2520Through%2520our%2520information-theoretic%2520framework%252C%2520we%2520show%2520that%2520to%2520ensure%250Asafety%2520against%2520inferential%2520adversaries%252C%2520defense%2520mechanisms%2520must%2520ensure%250Ainformation%2520censorship%252C%2520bounding%2520the%2520leakage%2520of%2520impermissible%2520information.%250AHowever%252C%2520we%2520prove%2520that%2520such%2520defenses%2520inevitably%2520incur%2520a%2520safety-utility%250Atrade-off.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02551v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breach%20By%20A%20Thousand%20Leaks%3A%20Unsafe%20Information%20Leakage%20in%20%60Safe%27%20AI%0A%20%20Responses&entry.906535625=David%20Glukhov%20and%20Ziwen%20Han%20and%20Ilia%20Shumailov%20and%20Vardan%20Papyan%20and%20Nicolas%20Papernot&entry.1292438233=%20%20Vulnerability%20of%20Frontier%20language%20models%20to%20misuse%20and%20jailbreaks%20has%0Aprompted%20the%20development%20of%20safety%20measures%20like%20filters%20and%20alignment%20training%0Ain%20an%20effort%20to%20ensure%20safety%20through%20robustness%20to%20adversarially%20crafted%0Aprompts.%20We%20assert%20that%20robustness%20is%20fundamentally%20insufficient%20for%20ensuring%0Asafety%20goals%2C%20and%20current%20defenses%20and%20evaluation%20methods%20fail%20to%20account%20for%0Arisks%20of%20dual-intent%20queries%20and%20their%20composition%20for%20malicious%20goals.%20To%0Aquantify%20these%20risks%2C%20we%20introduce%20a%20new%20safety%20evaluation%20framework%20based%20on%0Aimpermissible%20information%20leakage%20of%20model%20outputs%20and%20demonstrate%20how%20our%0Aproposed%20question-decomposition%20attack%20can%20extract%20dangerous%20knowledge%20from%20a%0Acensored%20LLM%20more%20effectively%20than%20traditional%20jailbreaking.%20Underlying%20our%0Aproposed%20evaluation%20method%20is%20a%20novel%20information-theoretic%20threat%20model%20of%0Ainferential%20adversaries%2C%20distinguished%20from%20security%20adversaries%2C%20such%20as%0Ajailbreaks%2C%20in%20that%20success%20is%20measured%20by%20inferring%20impermissible%20knowledge%0Afrom%20victim%20outputs%20as%20opposed%20to%20forcing%20explicitly%20impermissible%20outputs%20from%0Athe%20victim.%20Through%20our%20information-theoretic%20framework%2C%20we%20show%20that%20to%20ensure%0Asafety%20against%20inferential%20adversaries%2C%20defense%20mechanisms%20must%20ensure%0Ainformation%20censorship%2C%20bounding%20the%20leakage%20of%20impermissible%20information.%0AHowever%2C%20we%20prove%20that%20such%20defenses%20inevitably%20incur%20a%20safety-utility%0Atrade-off.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02551v2&entry.124074799=Read"},
{"title": "Full-waveform earthquake source inversion using simulation-based\n  inference", "author": "A. A. Saoulis and D. Piras and A. Spurio Mancini and B. Joachimi and A. M. G. Ferreira", "abstract": "  This paper presents a novel framework for full-waveform seismic source\ninversion using simulation-based inference (SBI). Traditional probabilistic\napproaches often rely on simplifying assumptions about data errors, which we\nshow can lead to inaccurate uncertainty quantification. SBI addresses this\nlimitation by building an empirical probabilistic model of the data errors\nusing machine learning models, known as neural density estimators, which can\nthen be integrated into the Bayesian inference framework. We apply the SBI\nframework to point-source moment tensor inversions as well as joint moment\ntensor and time-location inversions. We construct a range of synthetic examples\nto explore the quality of the SBI solutions, as well as to compare the SBI\nresults with standard Gaussian likelihood-based Bayesian inversions. We then\ndemonstrate that under real seismic noise, common Gaussian likelihood\nassumptions for treating full-waveform data yield overconfident posterior\ndistributions that underestimate the moment tensor component uncertainties by\nup to a factor of 3. We contrast this with SBI, which produces well-calibrated\nposteriors that generally agree with the true seismic source parameters, and\noffers an order-of-magnitude reduction in the number of simulations required to\nperform inference compared to standard Monte Carlo techniques. Finally, we\napply our methodology to a pair of moderate magnitude earthquakes in the North\nAtlantic. We utilise seismic waveforms recorded by the recent UPFLOW ocean\nbottom seismometer array as well as by regional land stations in the Azores,\ncomparing full moment tensor and source-time location posteriors between SBI\nand a Gaussian likelihood approach. We find that our adaptation of SBI can be\ndirectly applied to real earthquake sources to efficiently produce high quality\nposterior distributions that significantly improve upon Gaussian likelihood\napproaches.\n", "link": "http://arxiv.org/abs/2410.23238v1", "date": "2024-10-30", "relevancy": 1.8218, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5256}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4519}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Full-waveform%20earthquake%20source%20inversion%20using%20simulation-based%0A%20%20inference&body=Title%3A%20Full-waveform%20earthquake%20source%20inversion%20using%20simulation-based%0A%20%20inference%0AAuthor%3A%20A.%20A.%20Saoulis%20and%20D.%20Piras%20and%20A.%20Spurio%20Mancini%20and%20B.%20Joachimi%20and%20A.%20M.%20G.%20Ferreira%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20framework%20for%20full-waveform%20seismic%20source%0Ainversion%20using%20simulation-based%20inference%20%28SBI%29.%20Traditional%20probabilistic%0Aapproaches%20often%20rely%20on%20simplifying%20assumptions%20about%20data%20errors%2C%20which%20we%0Ashow%20can%20lead%20to%20inaccurate%20uncertainty%20quantification.%20SBI%20addresses%20this%0Alimitation%20by%20building%20an%20empirical%20probabilistic%20model%20of%20the%20data%20errors%0Ausing%20machine%20learning%20models%2C%20known%20as%20neural%20density%20estimators%2C%20which%20can%0Athen%20be%20integrated%20into%20the%20Bayesian%20inference%20framework.%20We%20apply%20the%20SBI%0Aframework%20to%20point-source%20moment%20tensor%20inversions%20as%20well%20as%20joint%20moment%0Atensor%20and%20time-location%20inversions.%20We%20construct%20a%20range%20of%20synthetic%20examples%0Ato%20explore%20the%20quality%20of%20the%20SBI%20solutions%2C%20as%20well%20as%20to%20compare%20the%20SBI%0Aresults%20with%20standard%20Gaussian%20likelihood-based%20Bayesian%20inversions.%20We%20then%0Ademonstrate%20that%20under%20real%20seismic%20noise%2C%20common%20Gaussian%20likelihood%0Aassumptions%20for%20treating%20full-waveform%20data%20yield%20overconfident%20posterior%0Adistributions%20that%20underestimate%20the%20moment%20tensor%20component%20uncertainties%20by%0Aup%20to%20a%20factor%20of%203.%20We%20contrast%20this%20with%20SBI%2C%20which%20produces%20well-calibrated%0Aposteriors%20that%20generally%20agree%20with%20the%20true%20seismic%20source%20parameters%2C%20and%0Aoffers%20an%20order-of-magnitude%20reduction%20in%20the%20number%20of%20simulations%20required%20to%0Aperform%20inference%20compared%20to%20standard%20Monte%20Carlo%20techniques.%20Finally%2C%20we%0Aapply%20our%20methodology%20to%20a%20pair%20of%20moderate%20magnitude%20earthquakes%20in%20the%20North%0AAtlantic.%20We%20utilise%20seismic%20waveforms%20recorded%20by%20the%20recent%20UPFLOW%20ocean%0Abottom%20seismometer%20array%20as%20well%20as%20by%20regional%20land%20stations%20in%20the%20Azores%2C%0Acomparing%20full%20moment%20tensor%20and%20source-time%20location%20posteriors%20between%20SBI%0Aand%20a%20Gaussian%20likelihood%20approach.%20We%20find%20that%20our%20adaptation%20of%20SBI%20can%20be%0Adirectly%20applied%20to%20real%20earthquake%20sources%20to%20efficiently%20produce%20high%20quality%0Aposterior%20distributions%20that%20significantly%20improve%20upon%20Gaussian%20likelihood%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23238v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFull-waveform%2520earthquake%2520source%2520inversion%2520using%2520simulation-based%250A%2520%2520inference%26entry.906535625%3DA.%2520A.%2520Saoulis%2520and%2520D.%2520Piras%2520and%2520A.%2520Spurio%2520Mancini%2520and%2520B.%2520Joachimi%2520and%2520A.%2520M.%2520G.%2520Ferreira%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520framework%2520for%2520full-waveform%2520seismic%2520source%250Ainversion%2520using%2520simulation-based%2520inference%2520%2528SBI%2529.%2520Traditional%2520probabilistic%250Aapproaches%2520often%2520rely%2520on%2520simplifying%2520assumptions%2520about%2520data%2520errors%252C%2520which%2520we%250Ashow%2520can%2520lead%2520to%2520inaccurate%2520uncertainty%2520quantification.%2520SBI%2520addresses%2520this%250Alimitation%2520by%2520building%2520an%2520empirical%2520probabilistic%2520model%2520of%2520the%2520data%2520errors%250Ausing%2520machine%2520learning%2520models%252C%2520known%2520as%2520neural%2520density%2520estimators%252C%2520which%2520can%250Athen%2520be%2520integrated%2520into%2520the%2520Bayesian%2520inference%2520framework.%2520We%2520apply%2520the%2520SBI%250Aframework%2520to%2520point-source%2520moment%2520tensor%2520inversions%2520as%2520well%2520as%2520joint%2520moment%250Atensor%2520and%2520time-location%2520inversions.%2520We%2520construct%2520a%2520range%2520of%2520synthetic%2520examples%250Ato%2520explore%2520the%2520quality%2520of%2520the%2520SBI%2520solutions%252C%2520as%2520well%2520as%2520to%2520compare%2520the%2520SBI%250Aresults%2520with%2520standard%2520Gaussian%2520likelihood-based%2520Bayesian%2520inversions.%2520We%2520then%250Ademonstrate%2520that%2520under%2520real%2520seismic%2520noise%252C%2520common%2520Gaussian%2520likelihood%250Aassumptions%2520for%2520treating%2520full-waveform%2520data%2520yield%2520overconfident%2520posterior%250Adistributions%2520that%2520underestimate%2520the%2520moment%2520tensor%2520component%2520uncertainties%2520by%250Aup%2520to%2520a%2520factor%2520of%25203.%2520We%2520contrast%2520this%2520with%2520SBI%252C%2520which%2520produces%2520well-calibrated%250Aposteriors%2520that%2520generally%2520agree%2520with%2520the%2520true%2520seismic%2520source%2520parameters%252C%2520and%250Aoffers%2520an%2520order-of-magnitude%2520reduction%2520in%2520the%2520number%2520of%2520simulations%2520required%2520to%250Aperform%2520inference%2520compared%2520to%2520standard%2520Monte%2520Carlo%2520techniques.%2520Finally%252C%2520we%250Aapply%2520our%2520methodology%2520to%2520a%2520pair%2520of%2520moderate%2520magnitude%2520earthquakes%2520in%2520the%2520North%250AAtlantic.%2520We%2520utilise%2520seismic%2520waveforms%2520recorded%2520by%2520the%2520recent%2520UPFLOW%2520ocean%250Abottom%2520seismometer%2520array%2520as%2520well%2520as%2520by%2520regional%2520land%2520stations%2520in%2520the%2520Azores%252C%250Acomparing%2520full%2520moment%2520tensor%2520and%2520source-time%2520location%2520posteriors%2520between%2520SBI%250Aand%2520a%2520Gaussian%2520likelihood%2520approach.%2520We%2520find%2520that%2520our%2520adaptation%2520of%2520SBI%2520can%2520be%250Adirectly%2520applied%2520to%2520real%2520earthquake%2520sources%2520to%2520efficiently%2520produce%2520high%2520quality%250Aposterior%2520distributions%2520that%2520significantly%2520improve%2520upon%2520Gaussian%2520likelihood%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23238v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Full-waveform%20earthquake%20source%20inversion%20using%20simulation-based%0A%20%20inference&entry.906535625=A.%20A.%20Saoulis%20and%20D.%20Piras%20and%20A.%20Spurio%20Mancini%20and%20B.%20Joachimi%20and%20A.%20M.%20G.%20Ferreira&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20framework%20for%20full-waveform%20seismic%20source%0Ainversion%20using%20simulation-based%20inference%20%28SBI%29.%20Traditional%20probabilistic%0Aapproaches%20often%20rely%20on%20simplifying%20assumptions%20about%20data%20errors%2C%20which%20we%0Ashow%20can%20lead%20to%20inaccurate%20uncertainty%20quantification.%20SBI%20addresses%20this%0Alimitation%20by%20building%20an%20empirical%20probabilistic%20model%20of%20the%20data%20errors%0Ausing%20machine%20learning%20models%2C%20known%20as%20neural%20density%20estimators%2C%20which%20can%0Athen%20be%20integrated%20into%20the%20Bayesian%20inference%20framework.%20We%20apply%20the%20SBI%0Aframework%20to%20point-source%20moment%20tensor%20inversions%20as%20well%20as%20joint%20moment%0Atensor%20and%20time-location%20inversions.%20We%20construct%20a%20range%20of%20synthetic%20examples%0Ato%20explore%20the%20quality%20of%20the%20SBI%20solutions%2C%20as%20well%20as%20to%20compare%20the%20SBI%0Aresults%20with%20standard%20Gaussian%20likelihood-based%20Bayesian%20inversions.%20We%20then%0Ademonstrate%20that%20under%20real%20seismic%20noise%2C%20common%20Gaussian%20likelihood%0Aassumptions%20for%20treating%20full-waveform%20data%20yield%20overconfident%20posterior%0Adistributions%20that%20underestimate%20the%20moment%20tensor%20component%20uncertainties%20by%0Aup%20to%20a%20factor%20of%203.%20We%20contrast%20this%20with%20SBI%2C%20which%20produces%20well-calibrated%0Aposteriors%20that%20generally%20agree%20with%20the%20true%20seismic%20source%20parameters%2C%20and%0Aoffers%20an%20order-of-magnitude%20reduction%20in%20the%20number%20of%20simulations%20required%20to%0Aperform%20inference%20compared%20to%20standard%20Monte%20Carlo%20techniques.%20Finally%2C%20we%0Aapply%20our%20methodology%20to%20a%20pair%20of%20moderate%20magnitude%20earthquakes%20in%20the%20North%0AAtlantic.%20We%20utilise%20seismic%20waveforms%20recorded%20by%20the%20recent%20UPFLOW%20ocean%0Abottom%20seismometer%20array%20as%20well%20as%20by%20regional%20land%20stations%20in%20the%20Azores%2C%0Acomparing%20full%20moment%20tensor%20and%20source-time%20location%20posteriors%20between%20SBI%0Aand%20a%20Gaussian%20likelihood%20approach.%20We%20find%20that%20our%20adaptation%20of%20SBI%20can%20be%0Adirectly%20applied%20to%20real%20earthquake%20sources%20to%20efficiently%20produce%20high%20quality%0Aposterior%20distributions%20that%20significantly%20improve%20upon%20Gaussian%20likelihood%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23238v1&entry.124074799=Read"},
{"title": "A little less conversation, a little more action, please: Investigating\n  the physical common-sense of LLMs in a 3D embodied environment", "author": "Matteo G. Mecattaf and Ben Slater and Marko Te\u0161i\u0107 and Jonathan Prunty and Konstantinos Voudouris and Lucy G. Cheke", "abstract": "  As general-purpose tools, Large Language Models (LLMs) must often reason\nabout everyday physical environments. In a question-and-answer capacity,\nunderstanding the interactions of physical objects may be necessary to give\nappropriate responses. Moreover, LLMs are increasingly used as reasoning\nengines in agentic systems, designing and controlling their action sequences.\nThe vast majority of research has tackled this issue using static benchmarks,\ncomprised of text or image-based questions about the physical world. However,\nthese benchmarks do not capture the complexity and nuance of real-life physical\nprocesses. Here we advocate for a second, relatively unexplored, approach:\n'embodying' the LLMs by granting them control of an agent within a 3D\nenvironment. We present the first embodied and cognitively meaningful\nevaluation of physical common-sense reasoning in LLMs. Our framework allows\ndirect comparison of LLMs with other embodied agents, such as those based on\nDeep Reinforcement Learning, and human and non-human animals. We employ the\nAnimal-AI (AAI) environment, a simulated 3D virtual laboratory, to study\nphysical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a\nsuite of experiments that replicate laboratory studies with non-human animals,\nto study physical reasoning capabilities including distance estimation,\ntracking out-of-sight objects, and tool use. We demonstrate that\nstate-of-the-art multi-modal models with no finetuning can complete this style\nof task, allowing meaningful comparison to the entrants of the 2019 Animal-AI\nOlympics competition and to human children. Our results show that LLMs are\ncurrently outperformed by human children on these tasks. We argue that this\napproach allows the study of physical reasoning using ecologically valid\nexperiments drawn directly from cognitive science, improving the predictability\nand reliability of LLMs.\n", "link": "http://arxiv.org/abs/2410.23242v1", "date": "2024-10-30", "relevancy": 1.7934, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6102}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6086}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20little%20less%20conversation%2C%20a%20little%20more%20action%2C%20please%3A%20Investigating%0A%20%20the%20physical%20common-sense%20of%20LLMs%20in%20a%203D%20embodied%20environment&body=Title%3A%20A%20little%20less%20conversation%2C%20a%20little%20more%20action%2C%20please%3A%20Investigating%0A%20%20the%20physical%20common-sense%20of%20LLMs%20in%20a%203D%20embodied%20environment%0AAuthor%3A%20Matteo%20G.%20Mecattaf%20and%20Ben%20Slater%20and%20Marko%20Te%C5%A1i%C4%87%20and%20Jonathan%20Prunty%20and%20Konstantinos%20Voudouris%20and%20Lucy%20G.%20Cheke%0AAbstract%3A%20%20%20As%20general-purpose%20tools%2C%20Large%20Language%20Models%20%28LLMs%29%20must%20often%20reason%0Aabout%20everyday%20physical%20environments.%20In%20a%20question-and-answer%20capacity%2C%0Aunderstanding%20the%20interactions%20of%20physical%20objects%20may%20be%20necessary%20to%20give%0Aappropriate%20responses.%20Moreover%2C%20LLMs%20are%20increasingly%20used%20as%20reasoning%0Aengines%20in%20agentic%20systems%2C%20designing%20and%20controlling%20their%20action%20sequences.%0AThe%20vast%20majority%20of%20research%20has%20tackled%20this%20issue%20using%20static%20benchmarks%2C%0Acomprised%20of%20text%20or%20image-based%20questions%20about%20the%20physical%20world.%20However%2C%0Athese%20benchmarks%20do%20not%20capture%20the%20complexity%20and%20nuance%20of%20real-life%20physical%0Aprocesses.%20Here%20we%20advocate%20for%20a%20second%2C%20relatively%20unexplored%2C%20approach%3A%0A%27embodying%27%20the%20LLMs%20by%20granting%20them%20control%20of%20an%20agent%20within%20a%203D%0Aenvironment.%20We%20present%20the%20first%20embodied%20and%20cognitively%20meaningful%0Aevaluation%20of%20physical%20common-sense%20reasoning%20in%20LLMs.%20Our%20framework%20allows%0Adirect%20comparison%20of%20LLMs%20with%20other%20embodied%20agents%2C%20such%20as%20those%20based%20on%0ADeep%20Reinforcement%20Learning%2C%20and%20human%20and%20non-human%20animals.%20We%20employ%20the%0AAnimal-AI%20%28AAI%29%20environment%2C%20a%20simulated%203D%20virtual%20laboratory%2C%20to%20study%0Aphysical%20common-sense%20reasoning%20in%20LLMs.%20For%20this%2C%20we%20use%20the%20AAI%20Testbed%2C%20a%0Asuite%20of%20experiments%20that%20replicate%20laboratory%20studies%20with%20non-human%20animals%2C%0Ato%20study%20physical%20reasoning%20capabilities%20including%20distance%20estimation%2C%0Atracking%20out-of-sight%20objects%2C%20and%20tool%20use.%20We%20demonstrate%20that%0Astate-of-the-art%20multi-modal%20models%20with%20no%20finetuning%20can%20complete%20this%20style%0Aof%20task%2C%20allowing%20meaningful%20comparison%20to%20the%20entrants%20of%20the%202019%20Animal-AI%0AOlympics%20competition%20and%20to%20human%20children.%20Our%20results%20show%20that%20LLMs%20are%0Acurrently%20outperformed%20by%20human%20children%20on%20these%20tasks.%20We%20argue%20that%20this%0Aapproach%20allows%20the%20study%20of%20physical%20reasoning%20using%20ecologically%20valid%0Aexperiments%20drawn%20directly%20from%20cognitive%20science%2C%20improving%20the%20predictability%0Aand%20reliability%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520little%2520less%2520conversation%252C%2520a%2520little%2520more%2520action%252C%2520please%253A%2520Investigating%250A%2520%2520the%2520physical%2520common-sense%2520of%2520LLMs%2520in%2520a%25203D%2520embodied%2520environment%26entry.906535625%3DMatteo%2520G.%2520Mecattaf%2520and%2520Ben%2520Slater%2520and%2520Marko%2520Te%25C5%25A1i%25C4%2587%2520and%2520Jonathan%2520Prunty%2520and%2520Konstantinos%2520Voudouris%2520and%2520Lucy%2520G.%2520Cheke%26entry.1292438233%3D%2520%2520As%2520general-purpose%2520tools%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520must%2520often%2520reason%250Aabout%2520everyday%2520physical%2520environments.%2520In%2520a%2520question-and-answer%2520capacity%252C%250Aunderstanding%2520the%2520interactions%2520of%2520physical%2520objects%2520may%2520be%2520necessary%2520to%2520give%250Aappropriate%2520responses.%2520Moreover%252C%2520LLMs%2520are%2520increasingly%2520used%2520as%2520reasoning%250Aengines%2520in%2520agentic%2520systems%252C%2520designing%2520and%2520controlling%2520their%2520action%2520sequences.%250AThe%2520vast%2520majority%2520of%2520research%2520has%2520tackled%2520this%2520issue%2520using%2520static%2520benchmarks%252C%250Acomprised%2520of%2520text%2520or%2520image-based%2520questions%2520about%2520the%2520physical%2520world.%2520However%252C%250Athese%2520benchmarks%2520do%2520not%2520capture%2520the%2520complexity%2520and%2520nuance%2520of%2520real-life%2520physical%250Aprocesses.%2520Here%2520we%2520advocate%2520for%2520a%2520second%252C%2520relatively%2520unexplored%252C%2520approach%253A%250A%2527embodying%2527%2520the%2520LLMs%2520by%2520granting%2520them%2520control%2520of%2520an%2520agent%2520within%2520a%25203D%250Aenvironment.%2520We%2520present%2520the%2520first%2520embodied%2520and%2520cognitively%2520meaningful%250Aevaluation%2520of%2520physical%2520common-sense%2520reasoning%2520in%2520LLMs.%2520Our%2520framework%2520allows%250Adirect%2520comparison%2520of%2520LLMs%2520with%2520other%2520embodied%2520agents%252C%2520such%2520as%2520those%2520based%2520on%250ADeep%2520Reinforcement%2520Learning%252C%2520and%2520human%2520and%2520non-human%2520animals.%2520We%2520employ%2520the%250AAnimal-AI%2520%2528AAI%2529%2520environment%252C%2520a%2520simulated%25203D%2520virtual%2520laboratory%252C%2520to%2520study%250Aphysical%2520common-sense%2520reasoning%2520in%2520LLMs.%2520For%2520this%252C%2520we%2520use%2520the%2520AAI%2520Testbed%252C%2520a%250Asuite%2520of%2520experiments%2520that%2520replicate%2520laboratory%2520studies%2520with%2520non-human%2520animals%252C%250Ato%2520study%2520physical%2520reasoning%2520capabilities%2520including%2520distance%2520estimation%252C%250Atracking%2520out-of-sight%2520objects%252C%2520and%2520tool%2520use.%2520We%2520demonstrate%2520that%250Astate-of-the-art%2520multi-modal%2520models%2520with%2520no%2520finetuning%2520can%2520complete%2520this%2520style%250Aof%2520task%252C%2520allowing%2520meaningful%2520comparison%2520to%2520the%2520entrants%2520of%2520the%25202019%2520Animal-AI%250AOlympics%2520competition%2520and%2520to%2520human%2520children.%2520Our%2520results%2520show%2520that%2520LLMs%2520are%250Acurrently%2520outperformed%2520by%2520human%2520children%2520on%2520these%2520tasks.%2520We%2520argue%2520that%2520this%250Aapproach%2520allows%2520the%2520study%2520of%2520physical%2520reasoning%2520using%2520ecologically%2520valid%250Aexperiments%2520drawn%2520directly%2520from%2520cognitive%2520science%252C%2520improving%2520the%2520predictability%250Aand%2520reliability%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20little%20less%20conversation%2C%20a%20little%20more%20action%2C%20please%3A%20Investigating%0A%20%20the%20physical%20common-sense%20of%20LLMs%20in%20a%203D%20embodied%20environment&entry.906535625=Matteo%20G.%20Mecattaf%20and%20Ben%20Slater%20and%20Marko%20Te%C5%A1i%C4%87%20and%20Jonathan%20Prunty%20and%20Konstantinos%20Voudouris%20and%20Lucy%20G.%20Cheke&entry.1292438233=%20%20As%20general-purpose%20tools%2C%20Large%20Language%20Models%20%28LLMs%29%20must%20often%20reason%0Aabout%20everyday%20physical%20environments.%20In%20a%20question-and-answer%20capacity%2C%0Aunderstanding%20the%20interactions%20of%20physical%20objects%20may%20be%20necessary%20to%20give%0Aappropriate%20responses.%20Moreover%2C%20LLMs%20are%20increasingly%20used%20as%20reasoning%0Aengines%20in%20agentic%20systems%2C%20designing%20and%20controlling%20their%20action%20sequences.%0AThe%20vast%20majority%20of%20research%20has%20tackled%20this%20issue%20using%20static%20benchmarks%2C%0Acomprised%20of%20text%20or%20image-based%20questions%20about%20the%20physical%20world.%20However%2C%0Athese%20benchmarks%20do%20not%20capture%20the%20complexity%20and%20nuance%20of%20real-life%20physical%0Aprocesses.%20Here%20we%20advocate%20for%20a%20second%2C%20relatively%20unexplored%2C%20approach%3A%0A%27embodying%27%20the%20LLMs%20by%20granting%20them%20control%20of%20an%20agent%20within%20a%203D%0Aenvironment.%20We%20present%20the%20first%20embodied%20and%20cognitively%20meaningful%0Aevaluation%20of%20physical%20common-sense%20reasoning%20in%20LLMs.%20Our%20framework%20allows%0Adirect%20comparison%20of%20LLMs%20with%20other%20embodied%20agents%2C%20such%20as%20those%20based%20on%0ADeep%20Reinforcement%20Learning%2C%20and%20human%20and%20non-human%20animals.%20We%20employ%20the%0AAnimal-AI%20%28AAI%29%20environment%2C%20a%20simulated%203D%20virtual%20laboratory%2C%20to%20study%0Aphysical%20common-sense%20reasoning%20in%20LLMs.%20For%20this%2C%20we%20use%20the%20AAI%20Testbed%2C%20a%0Asuite%20of%20experiments%20that%20replicate%20laboratory%20studies%20with%20non-human%20animals%2C%0Ato%20study%20physical%20reasoning%20capabilities%20including%20distance%20estimation%2C%0Atracking%20out-of-sight%20objects%2C%20and%20tool%20use.%20We%20demonstrate%20that%0Astate-of-the-art%20multi-modal%20models%20with%20no%20finetuning%20can%20complete%20this%20style%0Aof%20task%2C%20allowing%20meaningful%20comparison%20to%20the%20entrants%20of%20the%202019%20Animal-AI%0AOlympics%20competition%20and%20to%20human%20children.%20Our%20results%20show%20that%20LLMs%20are%0Acurrently%20outperformed%20by%20human%20children%20on%20these%20tasks.%20We%20argue%20that%20this%0Aapproach%20allows%20the%20study%20of%20physical%20reasoning%20using%20ecologically%20valid%0Aexperiments%20drawn%20directly%20from%20cognitive%20science%2C%20improving%20the%20predictability%0Aand%20reliability%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23242v1&entry.124074799=Read"},
{"title": "Kinetix: Investigating the Training of General Agents through Open-Ended\n  Physics-Based Control Tasks", "author": "Michael Matthews and Michael Beukman and Chris Lu and Jakob Foerster", "abstract": "  While large models trained with self-supervised learning on offline datasets\nhave shown remarkable capabilities in text and image domains, achieving the\nsame generalisation for agents that act in sequential decision problems remains\nan open challenge. In this work, we take a step towards this goal by\nprocedurally generating tens of millions of 2D physics-based tasks and using\nthese to train a general reinforcement learning (RL) agent for physical\ncontrol. To this end, we introduce Kinetix: an open-ended space of\nphysics-based RL environments that can represent tasks ranging from robotic\nlocomotion and grasping to video games and classic RL environments, all within\na unified framework. Kinetix makes use of our novel hardware-accelerated\nphysics engine Jax2D that allows us to cheaply simulate billions of environment\nsteps during training. Our trained agent exhibits strong physical reasoning\ncapabilities, being able to zero-shot solve unseen human-designed environments.\nFurthermore, fine-tuning this general agent on tasks of interest shows\nsignificantly stronger performance than training an RL agent *tabula rasa*.\nThis includes solving some environments that standard RL training completely\nfails at. We believe this demonstrates the feasibility of large scale,\nmixed-quality pre-training for online RL and we hope that Kinetix will serve as\na useful framework to investigate this further.\n", "link": "http://arxiv.org/abs/2410.23208v1", "date": "2024-10-30", "relevancy": 1.7858, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6038}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5908}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kinetix%3A%20Investigating%20the%20Training%20of%20General%20Agents%20through%20Open-Ended%0A%20%20Physics-Based%20Control%20Tasks&body=Title%3A%20Kinetix%3A%20Investigating%20the%20Training%20of%20General%20Agents%20through%20Open-Ended%0A%20%20Physics-Based%20Control%20Tasks%0AAuthor%3A%20Michael%20Matthews%20and%20Michael%20Beukman%20and%20Chris%20Lu%20and%20Jakob%20Foerster%0AAbstract%3A%20%20%20While%20large%20models%20trained%20with%20self-supervised%20learning%20on%20offline%20datasets%0Ahave%20shown%20remarkable%20capabilities%20in%20text%20and%20image%20domains%2C%20achieving%20the%0Asame%20generalisation%20for%20agents%20that%20act%20in%20sequential%20decision%20problems%20remains%0Aan%20open%20challenge.%20In%20this%20work%2C%20we%20take%20a%20step%20towards%20this%20goal%20by%0Aprocedurally%20generating%20tens%20of%20millions%20of%202D%20physics-based%20tasks%20and%20using%0Athese%20to%20train%20a%20general%20reinforcement%20learning%20%28RL%29%20agent%20for%20physical%0Acontrol.%20To%20this%20end%2C%20we%20introduce%20Kinetix%3A%20an%20open-ended%20space%20of%0Aphysics-based%20RL%20environments%20that%20can%20represent%20tasks%20ranging%20from%20robotic%0Alocomotion%20and%20grasping%20to%20video%20games%20and%20classic%20RL%20environments%2C%20all%20within%0Aa%20unified%20framework.%20Kinetix%20makes%20use%20of%20our%20novel%20hardware-accelerated%0Aphysics%20engine%20Jax2D%20that%20allows%20us%20to%20cheaply%20simulate%20billions%20of%20environment%0Asteps%20during%20training.%20Our%20trained%20agent%20exhibits%20strong%20physical%20reasoning%0Acapabilities%2C%20being%20able%20to%20zero-shot%20solve%20unseen%20human-designed%20environments.%0AFurthermore%2C%20fine-tuning%20this%20general%20agent%20on%20tasks%20of%20interest%20shows%0Asignificantly%20stronger%20performance%20than%20training%20an%20RL%20agent%20%2Atabula%20rasa%2A.%0AThis%20includes%20solving%20some%20environments%20that%20standard%20RL%20training%20completely%0Afails%20at.%20We%20believe%20this%20demonstrates%20the%20feasibility%20of%20large%20scale%2C%0Amixed-quality%20pre-training%20for%20online%20RL%20and%20we%20hope%20that%20Kinetix%20will%20serve%20as%0Aa%20useful%20framework%20to%20investigate%20this%20further.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKinetix%253A%2520Investigating%2520the%2520Training%2520of%2520General%2520Agents%2520through%2520Open-Ended%250A%2520%2520Physics-Based%2520Control%2520Tasks%26entry.906535625%3DMichael%2520Matthews%2520and%2520Michael%2520Beukman%2520and%2520Chris%2520Lu%2520and%2520Jakob%2520Foerster%26entry.1292438233%3D%2520%2520While%2520large%2520models%2520trained%2520with%2520self-supervised%2520learning%2520on%2520offline%2520datasets%250Ahave%2520shown%2520remarkable%2520capabilities%2520in%2520text%2520and%2520image%2520domains%252C%2520achieving%2520the%250Asame%2520generalisation%2520for%2520agents%2520that%2520act%2520in%2520sequential%2520decision%2520problems%2520remains%250Aan%2520open%2520challenge.%2520In%2520this%2520work%252C%2520we%2520take%2520a%2520step%2520towards%2520this%2520goal%2520by%250Aprocedurally%2520generating%2520tens%2520of%2520millions%2520of%25202D%2520physics-based%2520tasks%2520and%2520using%250Athese%2520to%2520train%2520a%2520general%2520reinforcement%2520learning%2520%2528RL%2529%2520agent%2520for%2520physical%250Acontrol.%2520To%2520this%2520end%252C%2520we%2520introduce%2520Kinetix%253A%2520an%2520open-ended%2520space%2520of%250Aphysics-based%2520RL%2520environments%2520that%2520can%2520represent%2520tasks%2520ranging%2520from%2520robotic%250Alocomotion%2520and%2520grasping%2520to%2520video%2520games%2520and%2520classic%2520RL%2520environments%252C%2520all%2520within%250Aa%2520unified%2520framework.%2520Kinetix%2520makes%2520use%2520of%2520our%2520novel%2520hardware-accelerated%250Aphysics%2520engine%2520Jax2D%2520that%2520allows%2520us%2520to%2520cheaply%2520simulate%2520billions%2520of%2520environment%250Asteps%2520during%2520training.%2520Our%2520trained%2520agent%2520exhibits%2520strong%2520physical%2520reasoning%250Acapabilities%252C%2520being%2520able%2520to%2520zero-shot%2520solve%2520unseen%2520human-designed%2520environments.%250AFurthermore%252C%2520fine-tuning%2520this%2520general%2520agent%2520on%2520tasks%2520of%2520interest%2520shows%250Asignificantly%2520stronger%2520performance%2520than%2520training%2520an%2520RL%2520agent%2520%252Atabula%2520rasa%252A.%250AThis%2520includes%2520solving%2520some%2520environments%2520that%2520standard%2520RL%2520training%2520completely%250Afails%2520at.%2520We%2520believe%2520this%2520demonstrates%2520the%2520feasibility%2520of%2520large%2520scale%252C%250Amixed-quality%2520pre-training%2520for%2520online%2520RL%2520and%2520we%2520hope%2520that%2520Kinetix%2520will%2520serve%2520as%250Aa%2520useful%2520framework%2520to%2520investigate%2520this%2520further.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kinetix%3A%20Investigating%20the%20Training%20of%20General%20Agents%20through%20Open-Ended%0A%20%20Physics-Based%20Control%20Tasks&entry.906535625=Michael%20Matthews%20and%20Michael%20Beukman%20and%20Chris%20Lu%20and%20Jakob%20Foerster&entry.1292438233=%20%20While%20large%20models%20trained%20with%20self-supervised%20learning%20on%20offline%20datasets%0Ahave%20shown%20remarkable%20capabilities%20in%20text%20and%20image%20domains%2C%20achieving%20the%0Asame%20generalisation%20for%20agents%20that%20act%20in%20sequential%20decision%20problems%20remains%0Aan%20open%20challenge.%20In%20this%20work%2C%20we%20take%20a%20step%20towards%20this%20goal%20by%0Aprocedurally%20generating%20tens%20of%20millions%20of%202D%20physics-based%20tasks%20and%20using%0Athese%20to%20train%20a%20general%20reinforcement%20learning%20%28RL%29%20agent%20for%20physical%0Acontrol.%20To%20this%20end%2C%20we%20introduce%20Kinetix%3A%20an%20open-ended%20space%20of%0Aphysics-based%20RL%20environments%20that%20can%20represent%20tasks%20ranging%20from%20robotic%0Alocomotion%20and%20grasping%20to%20video%20games%20and%20classic%20RL%20environments%2C%20all%20within%0Aa%20unified%20framework.%20Kinetix%20makes%20use%20of%20our%20novel%20hardware-accelerated%0Aphysics%20engine%20Jax2D%20that%20allows%20us%20to%20cheaply%20simulate%20billions%20of%20environment%0Asteps%20during%20training.%20Our%20trained%20agent%20exhibits%20strong%20physical%20reasoning%0Acapabilities%2C%20being%20able%20to%20zero-shot%20solve%20unseen%20human-designed%20environments.%0AFurthermore%2C%20fine-tuning%20this%20general%20agent%20on%20tasks%20of%20interest%20shows%0Asignificantly%20stronger%20performance%20than%20training%20an%20RL%20agent%20%2Atabula%20rasa%2A.%0AThis%20includes%20solving%20some%20environments%20that%20standard%20RL%20training%20completely%0Afails%20at.%20We%20believe%20this%20demonstrates%20the%20feasibility%20of%20large%20scale%2C%0Amixed-quality%20pre-training%20for%20online%20RL%20and%20we%20hope%20that%20Kinetix%20will%20serve%20as%0Aa%20useful%20framework%20to%20investigate%20this%20further.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23208v1&entry.124074799=Read"},
{"title": "Directional anomaly detection", "author": "Oliver Urs Lenz and Matthijs van Leeuwen", "abstract": "  Semi-supervised anomaly detection is based on the principle that potential\nanomalies are those records that look different from normal training data.\nHowever, in some cases we are specifically interested in anomalies that\ncorrespond to high attribute values (or low, but not both). We present two\nasymmetrical distance measures that take this directionality into account: ramp\ndistance and signed distance. Through experiments on synthetic and real-life\ndatasets we show that ramp distance performs as well or better than the\nabsolute distance traditionally used in anomaly detection. While signed\ndistance also performs well on synthetic data, it performs substantially poorer\non real-life datasets. We argue that this reflects the fact that in practice,\ngood scores on some attributes should not be allowed to compensate for bad\nscores on others.\n", "link": "http://arxiv.org/abs/2410.23158v1", "date": "2024-10-30", "relevancy": 1.7649, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4542}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4349}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Directional%20anomaly%20detection&body=Title%3A%20Directional%20anomaly%20detection%0AAuthor%3A%20Oliver%20Urs%20Lenz%20and%20Matthijs%20van%20Leeuwen%0AAbstract%3A%20%20%20Semi-supervised%20anomaly%20detection%20is%20based%20on%20the%20principle%20that%20potential%0Aanomalies%20are%20those%20records%20that%20look%20different%20from%20normal%20training%20data.%0AHowever%2C%20in%20some%20cases%20we%20are%20specifically%20interested%20in%20anomalies%20that%0Acorrespond%20to%20high%20attribute%20values%20%28or%20low%2C%20but%20not%20both%29.%20We%20present%20two%0Aasymmetrical%20distance%20measures%20that%20take%20this%20directionality%20into%20account%3A%20ramp%0Adistance%20and%20signed%20distance.%20Through%20experiments%20on%20synthetic%20and%20real-life%0Adatasets%20we%20show%20that%20ramp%20distance%20performs%20as%20well%20or%20better%20than%20the%0Aabsolute%20distance%20traditionally%20used%20in%20anomaly%20detection.%20While%20signed%0Adistance%20also%20performs%20well%20on%20synthetic%20data%2C%20it%20performs%20substantially%20poorer%0Aon%20real-life%20datasets.%20We%20argue%20that%20this%20reflects%20the%20fact%20that%20in%20practice%2C%0Agood%20scores%20on%20some%20attributes%20should%20not%20be%20allowed%20to%20compensate%20for%20bad%0Ascores%20on%20others.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23158v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirectional%2520anomaly%2520detection%26entry.906535625%3DOliver%2520Urs%2520Lenz%2520and%2520Matthijs%2520van%2520Leeuwen%26entry.1292438233%3D%2520%2520Semi-supervised%2520anomaly%2520detection%2520is%2520based%2520on%2520the%2520principle%2520that%2520potential%250Aanomalies%2520are%2520those%2520records%2520that%2520look%2520different%2520from%2520normal%2520training%2520data.%250AHowever%252C%2520in%2520some%2520cases%2520we%2520are%2520specifically%2520interested%2520in%2520anomalies%2520that%250Acorrespond%2520to%2520high%2520attribute%2520values%2520%2528or%2520low%252C%2520but%2520not%2520both%2529.%2520We%2520present%2520two%250Aasymmetrical%2520distance%2520measures%2520that%2520take%2520this%2520directionality%2520into%2520account%253A%2520ramp%250Adistance%2520and%2520signed%2520distance.%2520Through%2520experiments%2520on%2520synthetic%2520and%2520real-life%250Adatasets%2520we%2520show%2520that%2520ramp%2520distance%2520performs%2520as%2520well%2520or%2520better%2520than%2520the%250Aabsolute%2520distance%2520traditionally%2520used%2520in%2520anomaly%2520detection.%2520While%2520signed%250Adistance%2520also%2520performs%2520well%2520on%2520synthetic%2520data%252C%2520it%2520performs%2520substantially%2520poorer%250Aon%2520real-life%2520datasets.%2520We%2520argue%2520that%2520this%2520reflects%2520the%2520fact%2520that%2520in%2520practice%252C%250Agood%2520scores%2520on%2520some%2520attributes%2520should%2520not%2520be%2520allowed%2520to%2520compensate%2520for%2520bad%250Ascores%2520on%2520others.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23158v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Directional%20anomaly%20detection&entry.906535625=Oliver%20Urs%20Lenz%20and%20Matthijs%20van%20Leeuwen&entry.1292438233=%20%20Semi-supervised%20anomaly%20detection%20is%20based%20on%20the%20principle%20that%20potential%0Aanomalies%20are%20those%20records%20that%20look%20different%20from%20normal%20training%20data.%0AHowever%2C%20in%20some%20cases%20we%20are%20specifically%20interested%20in%20anomalies%20that%0Acorrespond%20to%20high%20attribute%20values%20%28or%20low%2C%20but%20not%20both%29.%20We%20present%20two%0Aasymmetrical%20distance%20measures%20that%20take%20this%20directionality%20into%20account%3A%20ramp%0Adistance%20and%20signed%20distance.%20Through%20experiments%20on%20synthetic%20and%20real-life%0Adatasets%20we%20show%20that%20ramp%20distance%20performs%20as%20well%20or%20better%20than%20the%0Aabsolute%20distance%20traditionally%20used%20in%20anomaly%20detection.%20While%20signed%0Adistance%20also%20performs%20well%20on%20synthetic%20data%2C%20it%20performs%20substantially%20poorer%0Aon%20real-life%20datasets.%20We%20argue%20that%20this%20reflects%20the%20fact%20that%20in%20practice%2C%0Agood%20scores%20on%20some%20attributes%20should%20not%20be%20allowed%20to%20compensate%20for%20bad%0Ascores%20on%20others.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23158v1&entry.124074799=Read"},
{"title": "Keypoint Abstraction using Large Models for Object-Relative Imitation\n  Learning", "author": "Xiaolin Fang and Bo-Ruei Huang and Jiayuan Mao and Jasmine Shone and Joshua B. Tenenbaum and Tom\u00e1s Lozano-P\u00e9rez and Leslie Pack Kaelbling", "abstract": "  Generalization to novel object configurations and instances across diverse\ntasks and environments is a critical challenge in robotics. Keypoint-based\nrepresentations have been proven effective as a succinct representation for\ncapturing essential object features, and for establishing a reference frame in\naction prediction, enabling data-efficient learning of robot skills. However,\ntheir manual design nature and reliance on additional human labels limit their\nscalability. In this paper, we propose KALM, a framework that leverages large\npre-trained vision-language models (LMs) to automatically generate\ntask-relevant and cross-instance consistent keypoints. KALM distills robust and\nconsistent keypoints across views and objects by generating proposals using LMs\nand verifies them against a small set of robot demonstration data. Based on the\ngenerated keypoints, we can train keypoint-conditioned policy models that\npredict actions in keypoint-centric frames, enabling robots to generalize\neffectively across varying object poses, camera views, and object instances\nwith similar functional shapes. Our method demonstrates strong performance in\nthe real world, adapting to different tasks and environments from only a\nhandful of demonstrations while requiring no additional labels. Website:\nhttps://kalm-il.github.io/\n", "link": "http://arxiv.org/abs/2410.23254v1", "date": "2024-10-30", "relevancy": 1.7617, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.605}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5956}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Keypoint%20Abstraction%20using%20Large%20Models%20for%20Object-Relative%20Imitation%0A%20%20Learning&body=Title%3A%20Keypoint%20Abstraction%20using%20Large%20Models%20for%20Object-Relative%20Imitation%0A%20%20Learning%0AAuthor%3A%20Xiaolin%20Fang%20and%20Bo-Ruei%20Huang%20and%20Jiayuan%20Mao%20and%20Jasmine%20Shone%20and%20Joshua%20B.%20Tenenbaum%20and%20Tom%C3%A1s%20Lozano-P%C3%A9rez%20and%20Leslie%20Pack%20Kaelbling%0AAbstract%3A%20%20%20Generalization%20to%20novel%20object%20configurations%20and%20instances%20across%20diverse%0Atasks%20and%20environments%20is%20a%20critical%20challenge%20in%20robotics.%20Keypoint-based%0Arepresentations%20have%20been%20proven%20effective%20as%20a%20succinct%20representation%20for%0Acapturing%20essential%20object%20features%2C%20and%20for%20establishing%20a%20reference%20frame%20in%0Aaction%20prediction%2C%20enabling%20data-efficient%20learning%20of%20robot%20skills.%20However%2C%0Atheir%20manual%20design%20nature%20and%20reliance%20on%20additional%20human%20labels%20limit%20their%0Ascalability.%20In%20this%20paper%2C%20we%20propose%20KALM%2C%20a%20framework%20that%20leverages%20large%0Apre-trained%20vision-language%20models%20%28LMs%29%20to%20automatically%20generate%0Atask-relevant%20and%20cross-instance%20consistent%20keypoints.%20KALM%20distills%20robust%20and%0Aconsistent%20keypoints%20across%20views%20and%20objects%20by%20generating%20proposals%20using%20LMs%0Aand%20verifies%20them%20against%20a%20small%20set%20of%20robot%20demonstration%20data.%20Based%20on%20the%0Agenerated%20keypoints%2C%20we%20can%20train%20keypoint-conditioned%20policy%20models%20that%0Apredict%20actions%20in%20keypoint-centric%20frames%2C%20enabling%20robots%20to%20generalize%0Aeffectively%20across%20varying%20object%20poses%2C%20camera%20views%2C%20and%20object%20instances%0Awith%20similar%20functional%20shapes.%20Our%20method%20demonstrates%20strong%20performance%20in%0Athe%20real%20world%2C%20adapting%20to%20different%20tasks%20and%20environments%20from%20only%20a%0Ahandful%20of%20demonstrations%20while%20requiring%20no%20additional%20labels.%20Website%3A%0Ahttps%3A//kalm-il.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23254v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKeypoint%2520Abstraction%2520using%2520Large%2520Models%2520for%2520Object-Relative%2520Imitation%250A%2520%2520Learning%26entry.906535625%3DXiaolin%2520Fang%2520and%2520Bo-Ruei%2520Huang%2520and%2520Jiayuan%2520Mao%2520and%2520Jasmine%2520Shone%2520and%2520Joshua%2520B.%2520Tenenbaum%2520and%2520Tom%25C3%25A1s%2520Lozano-P%25C3%25A9rez%2520and%2520Leslie%2520Pack%2520Kaelbling%26entry.1292438233%3D%2520%2520Generalization%2520to%2520novel%2520object%2520configurations%2520and%2520instances%2520across%2520diverse%250Atasks%2520and%2520environments%2520is%2520a%2520critical%2520challenge%2520in%2520robotics.%2520Keypoint-based%250Arepresentations%2520have%2520been%2520proven%2520effective%2520as%2520a%2520succinct%2520representation%2520for%250Acapturing%2520essential%2520object%2520features%252C%2520and%2520for%2520establishing%2520a%2520reference%2520frame%2520in%250Aaction%2520prediction%252C%2520enabling%2520data-efficient%2520learning%2520of%2520robot%2520skills.%2520However%252C%250Atheir%2520manual%2520design%2520nature%2520and%2520reliance%2520on%2520additional%2520human%2520labels%2520limit%2520their%250Ascalability.%2520In%2520this%2520paper%252C%2520we%2520propose%2520KALM%252C%2520a%2520framework%2520that%2520leverages%2520large%250Apre-trained%2520vision-language%2520models%2520%2528LMs%2529%2520to%2520automatically%2520generate%250Atask-relevant%2520and%2520cross-instance%2520consistent%2520keypoints.%2520KALM%2520distills%2520robust%2520and%250Aconsistent%2520keypoints%2520across%2520views%2520and%2520objects%2520by%2520generating%2520proposals%2520using%2520LMs%250Aand%2520verifies%2520them%2520against%2520a%2520small%2520set%2520of%2520robot%2520demonstration%2520data.%2520Based%2520on%2520the%250Agenerated%2520keypoints%252C%2520we%2520can%2520train%2520keypoint-conditioned%2520policy%2520models%2520that%250Apredict%2520actions%2520in%2520keypoint-centric%2520frames%252C%2520enabling%2520robots%2520to%2520generalize%250Aeffectively%2520across%2520varying%2520object%2520poses%252C%2520camera%2520views%252C%2520and%2520object%2520instances%250Awith%2520similar%2520functional%2520shapes.%2520Our%2520method%2520demonstrates%2520strong%2520performance%2520in%250Athe%2520real%2520world%252C%2520adapting%2520to%2520different%2520tasks%2520and%2520environments%2520from%2520only%2520a%250Ahandful%2520of%2520demonstrations%2520while%2520requiring%2520no%2520additional%2520labels.%2520Website%253A%250Ahttps%253A//kalm-il.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23254v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Keypoint%20Abstraction%20using%20Large%20Models%20for%20Object-Relative%20Imitation%0A%20%20Learning&entry.906535625=Xiaolin%20Fang%20and%20Bo-Ruei%20Huang%20and%20Jiayuan%20Mao%20and%20Jasmine%20Shone%20and%20Joshua%20B.%20Tenenbaum%20and%20Tom%C3%A1s%20Lozano-P%C3%A9rez%20and%20Leslie%20Pack%20Kaelbling&entry.1292438233=%20%20Generalization%20to%20novel%20object%20configurations%20and%20instances%20across%20diverse%0Atasks%20and%20environments%20is%20a%20critical%20challenge%20in%20robotics.%20Keypoint-based%0Arepresentations%20have%20been%20proven%20effective%20as%20a%20succinct%20representation%20for%0Acapturing%20essential%20object%20features%2C%20and%20for%20establishing%20a%20reference%20frame%20in%0Aaction%20prediction%2C%20enabling%20data-efficient%20learning%20of%20robot%20skills.%20However%2C%0Atheir%20manual%20design%20nature%20and%20reliance%20on%20additional%20human%20labels%20limit%20their%0Ascalability.%20In%20this%20paper%2C%20we%20propose%20KALM%2C%20a%20framework%20that%20leverages%20large%0Apre-trained%20vision-language%20models%20%28LMs%29%20to%20automatically%20generate%0Atask-relevant%20and%20cross-instance%20consistent%20keypoints.%20KALM%20distills%20robust%20and%0Aconsistent%20keypoints%20across%20views%20and%20objects%20by%20generating%20proposals%20using%20LMs%0Aand%20verifies%20them%20against%20a%20small%20set%20of%20robot%20demonstration%20data.%20Based%20on%20the%0Agenerated%20keypoints%2C%20we%20can%20train%20keypoint-conditioned%20policy%20models%20that%0Apredict%20actions%20in%20keypoint-centric%20frames%2C%20enabling%20robots%20to%20generalize%0Aeffectively%20across%20varying%20object%20poses%2C%20camera%20views%2C%20and%20object%20instances%0Awith%20similar%20functional%20shapes.%20Our%20method%20demonstrates%20strong%20performance%20in%0Athe%20real%20world%2C%20adapting%20to%20different%20tasks%20and%20environments%20from%20only%20a%0Ahandful%20of%20demonstrations%20while%20requiring%20no%20additional%20labels.%20Website%3A%0Ahttps%3A//kalm-il.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23254v1&entry.124074799=Read"},
{"title": "StyleAdapter: A Unified Stylized Image Generation Model", "author": "Zhouxia Wang and Xintao Wang and Liangbin Xie and Zhongang Qi and Ying Shan and Wenping Wang and Ping Luo", "abstract": "  This work focuses on generating high-quality images with specific style of\nreference images and content of provided textual descriptions. Current leading\nalgorithms, i.e., DreamBooth and LoRA, require fine-tuning for each style,\nleading to time-consuming and computationally expensive processes. In this\nwork, we propose StyleAdapter, a unified stylized image generation model\ncapable of producing a variety of stylized images that match both the content\nof a given prompt and the style of reference images, without the need for\nper-style fine-tuning. It introduces a two-path cross-attention (TPCA) module\nto separately process style information and textual prompt, which cooperate\nwith a semantic suppressing vision model (SSVM) to suppress the semantic\ncontent of style images. In this way, it can ensure that the prompt maintains\ncontrol over the content of the generated images, while also mitigating the\nnegative impact of semantic information in style references. This results in\nthe content of the generated image adhering to the prompt, and its style\naligning with the style references. Besides, our StyleAdapter can be integrated\nwith existing controllable synthesis methods, such as T2I-adapter and\nControlNet, to attain a more controllable and stable generation process.\nExtensive experiments demonstrate the superiority of our method over previous\nworks.\n", "link": "http://arxiv.org/abs/2309.01770v2", "date": "2024-10-30", "relevancy": 1.7338, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5864}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5759}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StyleAdapter%3A%20A%20Unified%20Stylized%20Image%20Generation%20Model&body=Title%3A%20StyleAdapter%3A%20A%20Unified%20Stylized%20Image%20Generation%20Model%0AAuthor%3A%20Zhouxia%20Wang%20and%20Xintao%20Wang%20and%20Liangbin%20Xie%20and%20Zhongang%20Qi%20and%20Ying%20Shan%20and%20Wenping%20Wang%20and%20Ping%20Luo%0AAbstract%3A%20%20%20This%20work%20focuses%20on%20generating%20high-quality%20images%20with%20specific%20style%20of%0Areference%20images%20and%20content%20of%20provided%20textual%20descriptions.%20Current%20leading%0Aalgorithms%2C%20i.e.%2C%20DreamBooth%20and%20LoRA%2C%20require%20fine-tuning%20for%20each%20style%2C%0Aleading%20to%20time-consuming%20and%20computationally%20expensive%20processes.%20In%20this%0Awork%2C%20we%20propose%20StyleAdapter%2C%20a%20unified%20stylized%20image%20generation%20model%0Acapable%20of%20producing%20a%20variety%20of%20stylized%20images%20that%20match%20both%20the%20content%0Aof%20a%20given%20prompt%20and%20the%20style%20of%20reference%20images%2C%20without%20the%20need%20for%0Aper-style%20fine-tuning.%20It%20introduces%20a%20two-path%20cross-attention%20%28TPCA%29%20module%0Ato%20separately%20process%20style%20information%20and%20textual%20prompt%2C%20which%20cooperate%0Awith%20a%20semantic%20suppressing%20vision%20model%20%28SSVM%29%20to%20suppress%20the%20semantic%0Acontent%20of%20style%20images.%20In%20this%20way%2C%20it%20can%20ensure%20that%20the%20prompt%20maintains%0Acontrol%20over%20the%20content%20of%20the%20generated%20images%2C%20while%20also%20mitigating%20the%0Anegative%20impact%20of%20semantic%20information%20in%20style%20references.%20This%20results%20in%0Athe%20content%20of%20the%20generated%20image%20adhering%20to%20the%20prompt%2C%20and%20its%20style%0Aaligning%20with%20the%20style%20references.%20Besides%2C%20our%20StyleAdapter%20can%20be%20integrated%0Awith%20existing%20controllable%20synthesis%20methods%2C%20such%20as%20T2I-adapter%20and%0AControlNet%2C%20to%20attain%20a%20more%20controllable%20and%20stable%20generation%20process.%0AExtensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20method%20over%20previous%0Aworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.01770v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStyleAdapter%253A%2520A%2520Unified%2520Stylized%2520Image%2520Generation%2520Model%26entry.906535625%3DZhouxia%2520Wang%2520and%2520Xintao%2520Wang%2520and%2520Liangbin%2520Xie%2520and%2520Zhongang%2520Qi%2520and%2520Ying%2520Shan%2520and%2520Wenping%2520Wang%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520This%2520work%2520focuses%2520on%2520generating%2520high-quality%2520images%2520with%2520specific%2520style%2520of%250Areference%2520images%2520and%2520content%2520of%2520provided%2520textual%2520descriptions.%2520Current%2520leading%250Aalgorithms%252C%2520i.e.%252C%2520DreamBooth%2520and%2520LoRA%252C%2520require%2520fine-tuning%2520for%2520each%2520style%252C%250Aleading%2520to%2520time-consuming%2520and%2520computationally%2520expensive%2520processes.%2520In%2520this%250Awork%252C%2520we%2520propose%2520StyleAdapter%252C%2520a%2520unified%2520stylized%2520image%2520generation%2520model%250Acapable%2520of%2520producing%2520a%2520variety%2520of%2520stylized%2520images%2520that%2520match%2520both%2520the%2520content%250Aof%2520a%2520given%2520prompt%2520and%2520the%2520style%2520of%2520reference%2520images%252C%2520without%2520the%2520need%2520for%250Aper-style%2520fine-tuning.%2520It%2520introduces%2520a%2520two-path%2520cross-attention%2520%2528TPCA%2529%2520module%250Ato%2520separately%2520process%2520style%2520information%2520and%2520textual%2520prompt%252C%2520which%2520cooperate%250Awith%2520a%2520semantic%2520suppressing%2520vision%2520model%2520%2528SSVM%2529%2520to%2520suppress%2520the%2520semantic%250Acontent%2520of%2520style%2520images.%2520In%2520this%2520way%252C%2520it%2520can%2520ensure%2520that%2520the%2520prompt%2520maintains%250Acontrol%2520over%2520the%2520content%2520of%2520the%2520generated%2520images%252C%2520while%2520also%2520mitigating%2520the%250Anegative%2520impact%2520of%2520semantic%2520information%2520in%2520style%2520references.%2520This%2520results%2520in%250Athe%2520content%2520of%2520the%2520generated%2520image%2520adhering%2520to%2520the%2520prompt%252C%2520and%2520its%2520style%250Aaligning%2520with%2520the%2520style%2520references.%2520Besides%252C%2520our%2520StyleAdapter%2520can%2520be%2520integrated%250Awith%2520existing%2520controllable%2520synthesis%2520methods%252C%2520such%2520as%2520T2I-adapter%2520and%250AControlNet%252C%2520to%2520attain%2520a%2520more%2520controllable%2520and%2520stable%2520generation%2520process.%250AExtensive%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method%2520over%2520previous%250Aworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.01770v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StyleAdapter%3A%20A%20Unified%20Stylized%20Image%20Generation%20Model&entry.906535625=Zhouxia%20Wang%20and%20Xintao%20Wang%20and%20Liangbin%20Xie%20and%20Zhongang%20Qi%20and%20Ying%20Shan%20and%20Wenping%20Wang%20and%20Ping%20Luo&entry.1292438233=%20%20This%20work%20focuses%20on%20generating%20high-quality%20images%20with%20specific%20style%20of%0Areference%20images%20and%20content%20of%20provided%20textual%20descriptions.%20Current%20leading%0Aalgorithms%2C%20i.e.%2C%20DreamBooth%20and%20LoRA%2C%20require%20fine-tuning%20for%20each%20style%2C%0Aleading%20to%20time-consuming%20and%20computationally%20expensive%20processes.%20In%20this%0Awork%2C%20we%20propose%20StyleAdapter%2C%20a%20unified%20stylized%20image%20generation%20model%0Acapable%20of%20producing%20a%20variety%20of%20stylized%20images%20that%20match%20both%20the%20content%0Aof%20a%20given%20prompt%20and%20the%20style%20of%20reference%20images%2C%20without%20the%20need%20for%0Aper-style%20fine-tuning.%20It%20introduces%20a%20two-path%20cross-attention%20%28TPCA%29%20module%0Ato%20separately%20process%20style%20information%20and%20textual%20prompt%2C%20which%20cooperate%0Awith%20a%20semantic%20suppressing%20vision%20model%20%28SSVM%29%20to%20suppress%20the%20semantic%0Acontent%20of%20style%20images.%20In%20this%20way%2C%20it%20can%20ensure%20that%20the%20prompt%20maintains%0Acontrol%20over%20the%20content%20of%20the%20generated%20images%2C%20while%20also%20mitigating%20the%0Anegative%20impact%20of%20semantic%20information%20in%20style%20references.%20This%20results%20in%0Athe%20content%20of%20the%20generated%20image%20adhering%20to%20the%20prompt%2C%20and%20its%20style%0Aaligning%20with%20the%20style%20references.%20Besides%2C%20our%20StyleAdapter%20can%20be%20integrated%0Awith%20existing%20controllable%20synthesis%20methods%2C%20such%20as%20T2I-adapter%20and%0AControlNet%2C%20to%20attain%20a%20more%20controllable%20and%20stable%20generation%20process.%0AExtensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20method%20over%20previous%0Aworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.01770v2&entry.124074799=Read"},
{"title": "VisualPredicator: Learning Abstract World Models with Neuro-Symbolic\n  Predicates for Robot Planning", "author": "Yichao Liang and Nishanth Kumar and Hao Tang and Adrian Weller and Joshua B. Tenenbaum and Tom Silver and Jo\u00e3o F. Henriques and Kevin Ellis", "abstract": "  Broadly intelligent agents should form task-specific abstractions that\nselectively expose the essential elements of a task, while abstracting away the\ncomplexity of the raw sensorimotor space. In this work, we present\nNeuro-Symbolic Predicates, a first-order abstraction language that combines the\nstrengths of symbolic and neural knowledge representations. We outline an\nonline algorithm for inventing such predicates and learning abstract world\nmodels. We compare our approach to hierarchical reinforcement learning,\nvision-language model planning, and symbolic predicate invention approaches, on\nboth in- and out-of-distribution tasks across five simulated robotic domains.\nResults show that our approach offers better sample complexity, stronger\nout-of-distribution generalization, and improved interpretability.\n", "link": "http://arxiv.org/abs/2410.23156v1", "date": "2024-10-30", "relevancy": 1.7244, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6212}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5823}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisualPredicator%3A%20Learning%20Abstract%20World%20Models%20with%20Neuro-Symbolic%0A%20%20Predicates%20for%20Robot%20Planning&body=Title%3A%20VisualPredicator%3A%20Learning%20Abstract%20World%20Models%20with%20Neuro-Symbolic%0A%20%20Predicates%20for%20Robot%20Planning%0AAuthor%3A%20Yichao%20Liang%20and%20Nishanth%20Kumar%20and%20Hao%20Tang%20and%20Adrian%20Weller%20and%20Joshua%20B.%20Tenenbaum%20and%20Tom%20Silver%20and%20Jo%C3%A3o%20F.%20Henriques%20and%20Kevin%20Ellis%0AAbstract%3A%20%20%20Broadly%20intelligent%20agents%20should%20form%20task-specific%20abstractions%20that%0Aselectively%20expose%20the%20essential%20elements%20of%20a%20task%2C%20while%20abstracting%20away%20the%0Acomplexity%20of%20the%20raw%20sensorimotor%20space.%20In%20this%20work%2C%20we%20present%0ANeuro-Symbolic%20Predicates%2C%20a%20first-order%20abstraction%20language%20that%20combines%20the%0Astrengths%20of%20symbolic%20and%20neural%20knowledge%20representations.%20We%20outline%20an%0Aonline%20algorithm%20for%20inventing%20such%20predicates%20and%20learning%20abstract%20world%0Amodels.%20We%20compare%20our%20approach%20to%20hierarchical%20reinforcement%20learning%2C%0Avision-language%20model%20planning%2C%20and%20symbolic%20predicate%20invention%20approaches%2C%20on%0Aboth%20in-%20and%20out-of-distribution%20tasks%20across%20five%20simulated%20robotic%20domains.%0AResults%20show%20that%20our%20approach%20offers%20better%20sample%20complexity%2C%20stronger%0Aout-of-distribution%20generalization%2C%20and%20improved%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23156v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisualPredicator%253A%2520Learning%2520Abstract%2520World%2520Models%2520with%2520Neuro-Symbolic%250A%2520%2520Predicates%2520for%2520Robot%2520Planning%26entry.906535625%3DYichao%2520Liang%2520and%2520Nishanth%2520Kumar%2520and%2520Hao%2520Tang%2520and%2520Adrian%2520Weller%2520and%2520Joshua%2520B.%2520Tenenbaum%2520and%2520Tom%2520Silver%2520and%2520Jo%25C3%25A3o%2520F.%2520Henriques%2520and%2520Kevin%2520Ellis%26entry.1292438233%3D%2520%2520Broadly%2520intelligent%2520agents%2520should%2520form%2520task-specific%2520abstractions%2520that%250Aselectively%2520expose%2520the%2520essential%2520elements%2520of%2520a%2520task%252C%2520while%2520abstracting%2520away%2520the%250Acomplexity%2520of%2520the%2520raw%2520sensorimotor%2520space.%2520In%2520this%2520work%252C%2520we%2520present%250ANeuro-Symbolic%2520Predicates%252C%2520a%2520first-order%2520abstraction%2520language%2520that%2520combines%2520the%250Astrengths%2520of%2520symbolic%2520and%2520neural%2520knowledge%2520representations.%2520We%2520outline%2520an%250Aonline%2520algorithm%2520for%2520inventing%2520such%2520predicates%2520and%2520learning%2520abstract%2520world%250Amodels.%2520We%2520compare%2520our%2520approach%2520to%2520hierarchical%2520reinforcement%2520learning%252C%250Avision-language%2520model%2520planning%252C%2520and%2520symbolic%2520predicate%2520invention%2520approaches%252C%2520on%250Aboth%2520in-%2520and%2520out-of-distribution%2520tasks%2520across%2520five%2520simulated%2520robotic%2520domains.%250AResults%2520show%2520that%2520our%2520approach%2520offers%2520better%2520sample%2520complexity%252C%2520stronger%250Aout-of-distribution%2520generalization%252C%2520and%2520improved%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23156v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisualPredicator%3A%20Learning%20Abstract%20World%20Models%20with%20Neuro-Symbolic%0A%20%20Predicates%20for%20Robot%20Planning&entry.906535625=Yichao%20Liang%20and%20Nishanth%20Kumar%20and%20Hao%20Tang%20and%20Adrian%20Weller%20and%20Joshua%20B.%20Tenenbaum%20and%20Tom%20Silver%20and%20Jo%C3%A3o%20F.%20Henriques%20and%20Kevin%20Ellis&entry.1292438233=%20%20Broadly%20intelligent%20agents%20should%20form%20task-specific%20abstractions%20that%0Aselectively%20expose%20the%20essential%20elements%20of%20a%20task%2C%20while%20abstracting%20away%20the%0Acomplexity%20of%20the%20raw%20sensorimotor%20space.%20In%20this%20work%2C%20we%20present%0ANeuro-Symbolic%20Predicates%2C%20a%20first-order%20abstraction%20language%20that%20combines%20the%0Astrengths%20of%20symbolic%20and%20neural%20knowledge%20representations.%20We%20outline%20an%0Aonline%20algorithm%20for%20inventing%20such%20predicates%20and%20learning%20abstract%20world%0Amodels.%20We%20compare%20our%20approach%20to%20hierarchical%20reinforcement%20learning%2C%0Avision-language%20model%20planning%2C%20and%20symbolic%20predicate%20invention%20approaches%2C%20on%0Aboth%20in-%20and%20out-of-distribution%20tasks%20across%20five%20simulated%20robotic%20domains.%0AResults%20show%20that%20our%20approach%20offers%20better%20sample%20complexity%2C%20stronger%0Aout-of-distribution%20generalization%2C%20and%20improved%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23156v1&entry.124074799=Read"},
{"title": "A Monte Carlo Framework for Calibrated Uncertainty Estimation in\n  Sequence Prediction", "author": "Qidong Yang and Weicheng Zhu and Joseph Keslin and Laure Zanna and Tim G. J. Rudner and Carlos Fernandez-Granda", "abstract": "  Probabilistic prediction of sequences from images and other high-dimensional\ndata is a key challenge, particularly in risk-sensitive applications. In these\nsettings, it is often desirable to quantify the uncertainty associated with the\nprediction (instead of just determining the most likely sequence, as in\nlanguage modeling). In this paper, we propose a Monte Carlo framework to\nestimate probabilities and confidence intervals associated with the\ndistribution of a discrete sequence. Our framework uses a Monte Carlo\nsimulator, implemented as an autoregressively trained neural network, to sample\nsequences conditioned on an image input. We then use these samples to estimate\nthe probabilities and confidence intervals. Experiments on synthetic and real\ndata show that the framework produces accurate discriminative predictions, but\ncan suffer from miscalibration. In order to address this shortcoming, we\npropose a time-dependent regularization method, which is shown to produce\ncalibrated predictions.\n", "link": "http://arxiv.org/abs/2410.23272v1", "date": "2024-10-30", "relevancy": 1.7223, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.578}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.574}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Monte%20Carlo%20Framework%20for%20Calibrated%20Uncertainty%20Estimation%20in%0A%20%20Sequence%20Prediction&body=Title%3A%20A%20Monte%20Carlo%20Framework%20for%20Calibrated%20Uncertainty%20Estimation%20in%0A%20%20Sequence%20Prediction%0AAuthor%3A%20Qidong%20Yang%20and%20Weicheng%20Zhu%20and%20Joseph%20Keslin%20and%20Laure%20Zanna%20and%20Tim%20G.%20J.%20Rudner%20and%20Carlos%20Fernandez-Granda%0AAbstract%3A%20%20%20Probabilistic%20prediction%20of%20sequences%20from%20images%20and%20other%20high-dimensional%0Adata%20is%20a%20key%20challenge%2C%20particularly%20in%20risk-sensitive%20applications.%20In%20these%0Asettings%2C%20it%20is%20often%20desirable%20to%20quantify%20the%20uncertainty%20associated%20with%20the%0Aprediction%20%28instead%20of%20just%20determining%20the%20most%20likely%20sequence%2C%20as%20in%0Alanguage%20modeling%29.%20In%20this%20paper%2C%20we%20propose%20a%20Monte%20Carlo%20framework%20to%0Aestimate%20probabilities%20and%20confidence%20intervals%20associated%20with%20the%0Adistribution%20of%20a%20discrete%20sequence.%20Our%20framework%20uses%20a%20Monte%20Carlo%0Asimulator%2C%20implemented%20as%20an%20autoregressively%20trained%20neural%20network%2C%20to%20sample%0Asequences%20conditioned%20on%20an%20image%20input.%20We%20then%20use%20these%20samples%20to%20estimate%0Athe%20probabilities%20and%20confidence%20intervals.%20Experiments%20on%20synthetic%20and%20real%0Adata%20show%20that%20the%20framework%20produces%20accurate%20discriminative%20predictions%2C%20but%0Acan%20suffer%20from%20miscalibration.%20In%20order%20to%20address%20this%20shortcoming%2C%20we%0Apropose%20a%20time-dependent%20regularization%20method%2C%20which%20is%20shown%20to%20produce%0Acalibrated%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23272v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Monte%2520Carlo%2520Framework%2520for%2520Calibrated%2520Uncertainty%2520Estimation%2520in%250A%2520%2520Sequence%2520Prediction%26entry.906535625%3DQidong%2520Yang%2520and%2520Weicheng%2520Zhu%2520and%2520Joseph%2520Keslin%2520and%2520Laure%2520Zanna%2520and%2520Tim%2520G.%2520J.%2520Rudner%2520and%2520Carlos%2520Fernandez-Granda%26entry.1292438233%3D%2520%2520Probabilistic%2520prediction%2520of%2520sequences%2520from%2520images%2520and%2520other%2520high-dimensional%250Adata%2520is%2520a%2520key%2520challenge%252C%2520particularly%2520in%2520risk-sensitive%2520applications.%2520In%2520these%250Asettings%252C%2520it%2520is%2520often%2520desirable%2520to%2520quantify%2520the%2520uncertainty%2520associated%2520with%2520the%250Aprediction%2520%2528instead%2520of%2520just%2520determining%2520the%2520most%2520likely%2520sequence%252C%2520as%2520in%250Alanguage%2520modeling%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Monte%2520Carlo%2520framework%2520to%250Aestimate%2520probabilities%2520and%2520confidence%2520intervals%2520associated%2520with%2520the%250Adistribution%2520of%2520a%2520discrete%2520sequence.%2520Our%2520framework%2520uses%2520a%2520Monte%2520Carlo%250Asimulator%252C%2520implemented%2520as%2520an%2520autoregressively%2520trained%2520neural%2520network%252C%2520to%2520sample%250Asequences%2520conditioned%2520on%2520an%2520image%2520input.%2520We%2520then%2520use%2520these%2520samples%2520to%2520estimate%250Athe%2520probabilities%2520and%2520confidence%2520intervals.%2520Experiments%2520on%2520synthetic%2520and%2520real%250Adata%2520show%2520that%2520the%2520framework%2520produces%2520accurate%2520discriminative%2520predictions%252C%2520but%250Acan%2520suffer%2520from%2520miscalibration.%2520In%2520order%2520to%2520address%2520this%2520shortcoming%252C%2520we%250Apropose%2520a%2520time-dependent%2520regularization%2520method%252C%2520which%2520is%2520shown%2520to%2520produce%250Acalibrated%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23272v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Monte%20Carlo%20Framework%20for%20Calibrated%20Uncertainty%20Estimation%20in%0A%20%20Sequence%20Prediction&entry.906535625=Qidong%20Yang%20and%20Weicheng%20Zhu%20and%20Joseph%20Keslin%20and%20Laure%20Zanna%20and%20Tim%20G.%20J.%20Rudner%20and%20Carlos%20Fernandez-Granda&entry.1292438233=%20%20Probabilistic%20prediction%20of%20sequences%20from%20images%20and%20other%20high-dimensional%0Adata%20is%20a%20key%20challenge%2C%20particularly%20in%20risk-sensitive%20applications.%20In%20these%0Asettings%2C%20it%20is%20often%20desirable%20to%20quantify%20the%20uncertainty%20associated%20with%20the%0Aprediction%20%28instead%20of%20just%20determining%20the%20most%20likely%20sequence%2C%20as%20in%0Alanguage%20modeling%29.%20In%20this%20paper%2C%20we%20propose%20a%20Monte%20Carlo%20framework%20to%0Aestimate%20probabilities%20and%20confidence%20intervals%20associated%20with%20the%0Adistribution%20of%20a%20discrete%20sequence.%20Our%20framework%20uses%20a%20Monte%20Carlo%0Asimulator%2C%20implemented%20as%20an%20autoregressively%20trained%20neural%20network%2C%20to%20sample%0Asequences%20conditioned%20on%20an%20image%20input.%20We%20then%20use%20these%20samples%20to%20estimate%0Athe%20probabilities%20and%20confidence%20intervals.%20Experiments%20on%20synthetic%20and%20real%0Adata%20show%20that%20the%20framework%20produces%20accurate%20discriminative%20predictions%2C%20but%0Acan%20suffer%20from%20miscalibration.%20In%20order%20to%20address%20this%20shortcoming%2C%20we%0Apropose%20a%20time-dependent%20regularization%20method%2C%20which%20is%20shown%20to%20produce%0Acalibrated%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23272v1&entry.124074799=Read"},
{"title": "EMOTION: Expressive Motion Sequence Generation for Humanoid Robots with\n  In-Context Learning", "author": "Peide Huang and Yuhan Hu and Nataliya Nechyporenko and Daehwa Kim and Walter Talbott and Jian Zhang", "abstract": "  This paper introduces a framework, called EMOTION, for generating expressive\nmotion sequences in humanoid robots, enhancing their ability to engage in\nhumanlike non-verbal communication. Non-verbal cues such as facial expressions,\ngestures, and body movements play a crucial role in effective interpersonal\ninteractions. Despite the advancements in robotic behaviors, existing methods\noften fall short in mimicking the diversity and subtlety of human non-verbal\ncommunication. To address this gap, our approach leverages the in-context\nlearning capability of large language models (LLMs) to dynamically generate\nsocially appropriate gesture motion sequences for human-robot interaction. We\nuse this framework to generate 10 different expressive gestures and conduct\nonline user studies comparing the naturalness and understandability of the\nmotions generated by EMOTION and its human-feedback version, EMOTION++, against\nthose by human operators. The results demonstrate that our approach either\nmatches or surpasses human performance in generating understandable and natural\nrobot motions under certain scenarios. We also provide design implications for\nfuture research to consider a set of variables when generating expressive\nrobotic gestures.\n", "link": "http://arxiv.org/abs/2410.23234v1", "date": "2024-10-30", "relevancy": 1.7185, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.621}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5805}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMOTION%3A%20Expressive%20Motion%20Sequence%20Generation%20for%20Humanoid%20Robots%20with%0A%20%20In-Context%20Learning&body=Title%3A%20EMOTION%3A%20Expressive%20Motion%20Sequence%20Generation%20for%20Humanoid%20Robots%20with%0A%20%20In-Context%20Learning%0AAuthor%3A%20Peide%20Huang%20and%20Yuhan%20Hu%20and%20Nataliya%20Nechyporenko%20and%20Daehwa%20Kim%20and%20Walter%20Talbott%20and%20Jian%20Zhang%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20framework%2C%20called%20EMOTION%2C%20for%20generating%20expressive%0Amotion%20sequences%20in%20humanoid%20robots%2C%20enhancing%20their%20ability%20to%20engage%20in%0Ahumanlike%20non-verbal%20communication.%20Non-verbal%20cues%20such%20as%20facial%20expressions%2C%0Agestures%2C%20and%20body%20movements%20play%20a%20crucial%20role%20in%20effective%20interpersonal%0Ainteractions.%20Despite%20the%20advancements%20in%20robotic%20behaviors%2C%20existing%20methods%0Aoften%20fall%20short%20in%20mimicking%20the%20diversity%20and%20subtlety%20of%20human%20non-verbal%0Acommunication.%20To%20address%20this%20gap%2C%20our%20approach%20leverages%20the%20in-context%0Alearning%20capability%20of%20large%20language%20models%20%28LLMs%29%20to%20dynamically%20generate%0Asocially%20appropriate%20gesture%20motion%20sequences%20for%20human-robot%20interaction.%20We%0Ause%20this%20framework%20to%20generate%2010%20different%20expressive%20gestures%20and%20conduct%0Aonline%20user%20studies%20comparing%20the%20naturalness%20and%20understandability%20of%20the%0Amotions%20generated%20by%20EMOTION%20and%20its%20human-feedback%20version%2C%20EMOTION%2B%2B%2C%20against%0Athose%20by%20human%20operators.%20The%20results%20demonstrate%20that%20our%20approach%20either%0Amatches%20or%20surpasses%20human%20performance%20in%20generating%20understandable%20and%20natural%0Arobot%20motions%20under%20certain%20scenarios.%20We%20also%20provide%20design%20implications%20for%0Afuture%20research%20to%20consider%20a%20set%20of%20variables%20when%20generating%20expressive%0Arobotic%20gestures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23234v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMOTION%253A%2520Expressive%2520Motion%2520Sequence%2520Generation%2520for%2520Humanoid%2520Robots%2520with%250A%2520%2520In-Context%2520Learning%26entry.906535625%3DPeide%2520Huang%2520and%2520Yuhan%2520Hu%2520and%2520Nataliya%2520Nechyporenko%2520and%2520Daehwa%2520Kim%2520and%2520Walter%2520Talbott%2520and%2520Jian%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520framework%252C%2520called%2520EMOTION%252C%2520for%2520generating%2520expressive%250Amotion%2520sequences%2520in%2520humanoid%2520robots%252C%2520enhancing%2520their%2520ability%2520to%2520engage%2520in%250Ahumanlike%2520non-verbal%2520communication.%2520Non-verbal%2520cues%2520such%2520as%2520facial%2520expressions%252C%250Agestures%252C%2520and%2520body%2520movements%2520play%2520a%2520crucial%2520role%2520in%2520effective%2520interpersonal%250Ainteractions.%2520Despite%2520the%2520advancements%2520in%2520robotic%2520behaviors%252C%2520existing%2520methods%250Aoften%2520fall%2520short%2520in%2520mimicking%2520the%2520diversity%2520and%2520subtlety%2520of%2520human%2520non-verbal%250Acommunication.%2520To%2520address%2520this%2520gap%252C%2520our%2520approach%2520leverages%2520the%2520in-context%250Alearning%2520capability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520dynamically%2520generate%250Asocially%2520appropriate%2520gesture%2520motion%2520sequences%2520for%2520human-robot%2520interaction.%2520We%250Ause%2520this%2520framework%2520to%2520generate%252010%2520different%2520expressive%2520gestures%2520and%2520conduct%250Aonline%2520user%2520studies%2520comparing%2520the%2520naturalness%2520and%2520understandability%2520of%2520the%250Amotions%2520generated%2520by%2520EMOTION%2520and%2520its%2520human-feedback%2520version%252C%2520EMOTION%252B%252B%252C%2520against%250Athose%2520by%2520human%2520operators.%2520The%2520results%2520demonstrate%2520that%2520our%2520approach%2520either%250Amatches%2520or%2520surpasses%2520human%2520performance%2520in%2520generating%2520understandable%2520and%2520natural%250Arobot%2520motions%2520under%2520certain%2520scenarios.%2520We%2520also%2520provide%2520design%2520implications%2520for%250Afuture%2520research%2520to%2520consider%2520a%2520set%2520of%2520variables%2520when%2520generating%2520expressive%250Arobotic%2520gestures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23234v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMOTION%3A%20Expressive%20Motion%20Sequence%20Generation%20for%20Humanoid%20Robots%20with%0A%20%20In-Context%20Learning&entry.906535625=Peide%20Huang%20and%20Yuhan%20Hu%20and%20Nataliya%20Nechyporenko%20and%20Daehwa%20Kim%20and%20Walter%20Talbott%20and%20Jian%20Zhang&entry.1292438233=%20%20This%20paper%20introduces%20a%20framework%2C%20called%20EMOTION%2C%20for%20generating%20expressive%0Amotion%20sequences%20in%20humanoid%20robots%2C%20enhancing%20their%20ability%20to%20engage%20in%0Ahumanlike%20non-verbal%20communication.%20Non-verbal%20cues%20such%20as%20facial%20expressions%2C%0Agestures%2C%20and%20body%20movements%20play%20a%20crucial%20role%20in%20effective%20interpersonal%0Ainteractions.%20Despite%20the%20advancements%20in%20robotic%20behaviors%2C%20existing%20methods%0Aoften%20fall%20short%20in%20mimicking%20the%20diversity%20and%20subtlety%20of%20human%20non-verbal%0Acommunication.%20To%20address%20this%20gap%2C%20our%20approach%20leverages%20the%20in-context%0Alearning%20capability%20of%20large%20language%20models%20%28LLMs%29%20to%20dynamically%20generate%0Asocially%20appropriate%20gesture%20motion%20sequences%20for%20human-robot%20interaction.%20We%0Ause%20this%20framework%20to%20generate%2010%20different%20expressive%20gestures%20and%20conduct%0Aonline%20user%20studies%20comparing%20the%20naturalness%20and%20understandability%20of%20the%0Amotions%20generated%20by%20EMOTION%20and%20its%20human-feedback%20version%2C%20EMOTION%2B%2B%2C%20against%0Athose%20by%20human%20operators.%20The%20results%20demonstrate%20that%20our%20approach%20either%0Amatches%20or%20surpasses%20human%20performance%20in%20generating%20understandable%20and%20natural%0Arobot%20motions%20under%20certain%20scenarios.%20We%20also%20provide%20design%20implications%20for%0Afuture%20research%20to%20consider%20a%20set%20of%20variables%20when%20generating%20expressive%0Arobotic%20gestures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23234v1&entry.124074799=Read"},
{"title": "Very fast Bayesian Additive Regression Trees on GPU", "author": "Giacomo Petrillo", "abstract": "  Bayesian Additive Regression Trees (BART) is a nonparametric Bayesian\nregression technique based on an ensemble of decision trees. It is part of the\ntoolbox of many statisticians. The overall statistical quality of the\nregression is typically higher than other generic alternatives, and it requires\nless manual tuning, making it a good default choice. However, it is a niche\nmethod compared to its natural competitor XGBoost, due to the longer running\ntime, making sample sizes above 10,000-100,000 a nuisance. I present a\nGPU-enabled implementation of BART, faster by up to 200x relative to a single\nCPU core, making BART competitive in running time with XGBoost. This\nimplementation is available in the Python package bartz.\n", "link": "http://arxiv.org/abs/2410.23244v1", "date": "2024-10-30", "relevancy": 1.6943, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4259}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4234}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Very%20fast%20Bayesian%20Additive%20Regression%20Trees%20on%20GPU&body=Title%3A%20Very%20fast%20Bayesian%20Additive%20Regression%20Trees%20on%20GPU%0AAuthor%3A%20Giacomo%20Petrillo%0AAbstract%3A%20%20%20Bayesian%20Additive%20Regression%20Trees%20%28BART%29%20is%20a%20nonparametric%20Bayesian%0Aregression%20technique%20based%20on%20an%20ensemble%20of%20decision%20trees.%20It%20is%20part%20of%20the%0Atoolbox%20of%20many%20statisticians.%20The%20overall%20statistical%20quality%20of%20the%0Aregression%20is%20typically%20higher%20than%20other%20generic%20alternatives%2C%20and%20it%20requires%0Aless%20manual%20tuning%2C%20making%20it%20a%20good%20default%20choice.%20However%2C%20it%20is%20a%20niche%0Amethod%20compared%20to%20its%20natural%20competitor%20XGBoost%2C%20due%20to%20the%20longer%20running%0Atime%2C%20making%20sample%20sizes%20above%2010%2C000-100%2C000%20a%20nuisance.%20I%20present%20a%0AGPU-enabled%20implementation%20of%20BART%2C%20faster%20by%20up%20to%20200x%20relative%20to%20a%20single%0ACPU%20core%2C%20making%20BART%20competitive%20in%20running%20time%20with%20XGBoost.%20This%0Aimplementation%20is%20available%20in%20the%20Python%20package%20bartz.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVery%2520fast%2520Bayesian%2520Additive%2520Regression%2520Trees%2520on%2520GPU%26entry.906535625%3DGiacomo%2520Petrillo%26entry.1292438233%3D%2520%2520Bayesian%2520Additive%2520Regression%2520Trees%2520%2528BART%2529%2520is%2520a%2520nonparametric%2520Bayesian%250Aregression%2520technique%2520based%2520on%2520an%2520ensemble%2520of%2520decision%2520trees.%2520It%2520is%2520part%2520of%2520the%250Atoolbox%2520of%2520many%2520statisticians.%2520The%2520overall%2520statistical%2520quality%2520of%2520the%250Aregression%2520is%2520typically%2520higher%2520than%2520other%2520generic%2520alternatives%252C%2520and%2520it%2520requires%250Aless%2520manual%2520tuning%252C%2520making%2520it%2520a%2520good%2520default%2520choice.%2520However%252C%2520it%2520is%2520a%2520niche%250Amethod%2520compared%2520to%2520its%2520natural%2520competitor%2520XGBoost%252C%2520due%2520to%2520the%2520longer%2520running%250Atime%252C%2520making%2520sample%2520sizes%2520above%252010%252C000-100%252C000%2520a%2520nuisance.%2520I%2520present%2520a%250AGPU-enabled%2520implementation%2520of%2520BART%252C%2520faster%2520by%2520up%2520to%2520200x%2520relative%2520to%2520a%2520single%250ACPU%2520core%252C%2520making%2520BART%2520competitive%2520in%2520running%2520time%2520with%2520XGBoost.%2520This%250Aimplementation%2520is%2520available%2520in%2520the%2520Python%2520package%2520bartz.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Very%20fast%20Bayesian%20Additive%20Regression%20Trees%20on%20GPU&entry.906535625=Giacomo%20Petrillo&entry.1292438233=%20%20Bayesian%20Additive%20Regression%20Trees%20%28BART%29%20is%20a%20nonparametric%20Bayesian%0Aregression%20technique%20based%20on%20an%20ensemble%20of%20decision%20trees.%20It%20is%20part%20of%20the%0Atoolbox%20of%20many%20statisticians.%20The%20overall%20statistical%20quality%20of%20the%0Aregression%20is%20typically%20higher%20than%20other%20generic%20alternatives%2C%20and%20it%20requires%0Aless%20manual%20tuning%2C%20making%20it%20a%20good%20default%20choice.%20However%2C%20it%20is%20a%20niche%0Amethod%20compared%20to%20its%20natural%20competitor%20XGBoost%2C%20due%20to%20the%20longer%20running%0Atime%2C%20making%20sample%20sizes%20above%2010%2C000-100%2C000%20a%20nuisance.%20I%20present%20a%0AGPU-enabled%20implementation%20of%20BART%2C%20faster%20by%20up%20to%20200x%20relative%20to%20a%20single%0ACPU%20core%2C%20making%20BART%20competitive%20in%20running%20time%20with%20XGBoost.%20This%0Aimplementation%20is%20available%20in%20the%20Python%20package%20bartz.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23244v1&entry.124074799=Read"},
{"title": "Bridging the Human to Robot Dexterity Gap through Object-Oriented\n  Rewards", "author": "Irmak Guzey and Yinlong Dai and Georgy Savva and Raunaq Bhirangi and Lerrel Pinto", "abstract": "  Training robots directly from human videos is an emerging area in robotics\nand computer vision. While there has been notable progress with two-fingered\ngrippers, learning autonomous tasks for multi-fingered robot hands in this way\nremains challenging. A key reason for this difficulty is that a policy trained\non human hands may not directly transfer to a robot hand due to morphology\ndifferences. In this work, we present HuDOR, a technique that enables online\nfine-tuning of policies by directly computing rewards from human videos.\nImportantly, this reward function is built using object-oriented trajectories\nderived from off-the-shelf point trackers, providing meaningful learning\nsignals despite the morphology gap and visual differences between human and\nrobot hands. Given a single video of a human solving a task, such as gently\nopening a music box, HuDOR enables our four-fingered Allegro hand to learn the\ntask with just an hour of online interaction. Our experiments across four tasks\nshow that HuDOR achieves a 4x improvement over baselines. Code and videos are\navailable on our website, https://object-rewards.github.io.\n", "link": "http://arxiv.org/abs/2410.23289v1", "date": "2024-10-30", "relevancy": 1.6868, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5646}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5631}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20the%20Human%20to%20Robot%20Dexterity%20Gap%20through%20Object-Oriented%0A%20%20Rewards&body=Title%3A%20Bridging%20the%20Human%20to%20Robot%20Dexterity%20Gap%20through%20Object-Oriented%0A%20%20Rewards%0AAuthor%3A%20Irmak%20Guzey%20and%20Yinlong%20Dai%20and%20Georgy%20Savva%20and%20Raunaq%20Bhirangi%20and%20Lerrel%20Pinto%0AAbstract%3A%20%20%20Training%20robots%20directly%20from%20human%20videos%20is%20an%20emerging%20area%20in%20robotics%0Aand%20computer%20vision.%20While%20there%20has%20been%20notable%20progress%20with%20two-fingered%0Agrippers%2C%20learning%20autonomous%20tasks%20for%20multi-fingered%20robot%20hands%20in%20this%20way%0Aremains%20challenging.%20A%20key%20reason%20for%20this%20difficulty%20is%20that%20a%20policy%20trained%0Aon%20human%20hands%20may%20not%20directly%20transfer%20to%20a%20robot%20hand%20due%20to%20morphology%0Adifferences.%20In%20this%20work%2C%20we%20present%20HuDOR%2C%20a%20technique%20that%20enables%20online%0Afine-tuning%20of%20policies%20by%20directly%20computing%20rewards%20from%20human%20videos.%0AImportantly%2C%20this%20reward%20function%20is%20built%20using%20object-oriented%20trajectories%0Aderived%20from%20off-the-shelf%20point%20trackers%2C%20providing%20meaningful%20learning%0Asignals%20despite%20the%20morphology%20gap%20and%20visual%20differences%20between%20human%20and%0Arobot%20hands.%20Given%20a%20single%20video%20of%20a%20human%20solving%20a%20task%2C%20such%20as%20gently%0Aopening%20a%20music%20box%2C%20HuDOR%20enables%20our%20four-fingered%20Allegro%20hand%20to%20learn%20the%0Atask%20with%20just%20an%20hour%20of%20online%20interaction.%20Our%20experiments%20across%20four%20tasks%0Ashow%20that%20HuDOR%20achieves%20a%204x%20improvement%20over%20baselines.%20Code%20and%20videos%20are%0Aavailable%20on%20our%20website%2C%20https%3A//object-rewards.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23289v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520the%2520Human%2520to%2520Robot%2520Dexterity%2520Gap%2520through%2520Object-Oriented%250A%2520%2520Rewards%26entry.906535625%3DIrmak%2520Guzey%2520and%2520Yinlong%2520Dai%2520and%2520Georgy%2520Savva%2520and%2520Raunaq%2520Bhirangi%2520and%2520Lerrel%2520Pinto%26entry.1292438233%3D%2520%2520Training%2520robots%2520directly%2520from%2520human%2520videos%2520is%2520an%2520emerging%2520area%2520in%2520robotics%250Aand%2520computer%2520vision.%2520While%2520there%2520has%2520been%2520notable%2520progress%2520with%2520two-fingered%250Agrippers%252C%2520learning%2520autonomous%2520tasks%2520for%2520multi-fingered%2520robot%2520hands%2520in%2520this%2520way%250Aremains%2520challenging.%2520A%2520key%2520reason%2520for%2520this%2520difficulty%2520is%2520that%2520a%2520policy%2520trained%250Aon%2520human%2520hands%2520may%2520not%2520directly%2520transfer%2520to%2520a%2520robot%2520hand%2520due%2520to%2520morphology%250Adifferences.%2520In%2520this%2520work%252C%2520we%2520present%2520HuDOR%252C%2520a%2520technique%2520that%2520enables%2520online%250Afine-tuning%2520of%2520policies%2520by%2520directly%2520computing%2520rewards%2520from%2520human%2520videos.%250AImportantly%252C%2520this%2520reward%2520function%2520is%2520built%2520using%2520object-oriented%2520trajectories%250Aderived%2520from%2520off-the-shelf%2520point%2520trackers%252C%2520providing%2520meaningful%2520learning%250Asignals%2520despite%2520the%2520morphology%2520gap%2520and%2520visual%2520differences%2520between%2520human%2520and%250Arobot%2520hands.%2520Given%2520a%2520single%2520video%2520of%2520a%2520human%2520solving%2520a%2520task%252C%2520such%2520as%2520gently%250Aopening%2520a%2520music%2520box%252C%2520HuDOR%2520enables%2520our%2520four-fingered%2520Allegro%2520hand%2520to%2520learn%2520the%250Atask%2520with%2520just%2520an%2520hour%2520of%2520online%2520interaction.%2520Our%2520experiments%2520across%2520four%2520tasks%250Ashow%2520that%2520HuDOR%2520achieves%2520a%25204x%2520improvement%2520over%2520baselines.%2520Code%2520and%2520videos%2520are%250Aavailable%2520on%2520our%2520website%252C%2520https%253A//object-rewards.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23289v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20the%20Human%20to%20Robot%20Dexterity%20Gap%20through%20Object-Oriented%0A%20%20Rewards&entry.906535625=Irmak%20Guzey%20and%20Yinlong%20Dai%20and%20Georgy%20Savva%20and%20Raunaq%20Bhirangi%20and%20Lerrel%20Pinto&entry.1292438233=%20%20Training%20robots%20directly%20from%20human%20videos%20is%20an%20emerging%20area%20in%20robotics%0Aand%20computer%20vision.%20While%20there%20has%20been%20notable%20progress%20with%20two-fingered%0Agrippers%2C%20learning%20autonomous%20tasks%20for%20multi-fingered%20robot%20hands%20in%20this%20way%0Aremains%20challenging.%20A%20key%20reason%20for%20this%20difficulty%20is%20that%20a%20policy%20trained%0Aon%20human%20hands%20may%20not%20directly%20transfer%20to%20a%20robot%20hand%20due%20to%20morphology%0Adifferences.%20In%20this%20work%2C%20we%20present%20HuDOR%2C%20a%20technique%20that%20enables%20online%0Afine-tuning%20of%20policies%20by%20directly%20computing%20rewards%20from%20human%20videos.%0AImportantly%2C%20this%20reward%20function%20is%20built%20using%20object-oriented%20trajectories%0Aderived%20from%20off-the-shelf%20point%20trackers%2C%20providing%20meaningful%20learning%0Asignals%20despite%20the%20morphology%20gap%20and%20visual%20differences%20between%20human%20and%0Arobot%20hands.%20Given%20a%20single%20video%20of%20a%20human%20solving%20a%20task%2C%20such%20as%20gently%0Aopening%20a%20music%20box%2C%20HuDOR%20enables%20our%20four-fingered%20Allegro%20hand%20to%20learn%20the%0Atask%20with%20just%20an%20hour%20of%20online%20interaction.%20Our%20experiments%20across%20four%20tasks%0Ashow%20that%20HuDOR%20achieves%20a%204x%20improvement%20over%20baselines.%20Code%20and%20videos%20are%0Aavailable%20on%20our%20website%2C%20https%3A//object-rewards.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23289v1&entry.124074799=Read"},
{"title": "DiaMond: Dementia Diagnosis with Multi-Modal Vision Transformers Using\n  MRI and PET", "author": "Yitong Li and Morteza Ghahremani and Youssef Wally and Christian Wachinger", "abstract": "  Diagnosing dementia, particularly for Alzheimer's Disease (AD) and\nfrontotemporal dementia (FTD), is complex due to overlapping symptoms. While\nmagnetic resonance imaging (MRI) and positron emission tomography (PET) data\nare critical for the diagnosis, integrating these modalities in deep learning\nfaces challenges, often resulting in suboptimal performance compared to using\nsingle modalities. Moreover, the potential of multi-modal approaches in\ndifferential diagnosis, which holds significant clinical importance, remains\nlargely unexplored. We propose a novel framework, DiaMond, to address these\nissues with vision Transformers to effectively integrate MRI and PET. DiaMond\nis equipped with self-attention and a novel bi-attention mechanism that\nsynergistically combine MRI and PET, alongside a multi-modal normalization to\nreduce redundant dependency, thereby boosting the performance. DiaMond\nsignificantly outperforms existing multi-modal methods across various datasets,\nachieving a balanced accuracy of 92.4% in AD diagnosis, 65.2% for AD-MCI-CN\nclassification, and 76.5% in differential diagnosis of AD and FTD. We also\nvalidated the robustness of DiaMond in a comprehensive ablation study. The code\nis available at https://github.com/ai-med/DiaMond.\n", "link": "http://arxiv.org/abs/2410.23219v1", "date": "2024-10-30", "relevancy": 1.6406, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5763}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5123}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiaMond%3A%20Dementia%20Diagnosis%20with%20Multi-Modal%20Vision%20Transformers%20Using%0A%20%20MRI%20and%20PET&body=Title%3A%20DiaMond%3A%20Dementia%20Diagnosis%20with%20Multi-Modal%20Vision%20Transformers%20Using%0A%20%20MRI%20and%20PET%0AAuthor%3A%20Yitong%20Li%20and%20Morteza%20Ghahremani%20and%20Youssef%20Wally%20and%20Christian%20Wachinger%0AAbstract%3A%20%20%20Diagnosing%20dementia%2C%20particularly%20for%20Alzheimer%27s%20Disease%20%28AD%29%20and%0Afrontotemporal%20dementia%20%28FTD%29%2C%20is%20complex%20due%20to%20overlapping%20symptoms.%20While%0Amagnetic%20resonance%20imaging%20%28MRI%29%20and%20positron%20emission%20tomography%20%28PET%29%20data%0Aare%20critical%20for%20the%20diagnosis%2C%20integrating%20these%20modalities%20in%20deep%20learning%0Afaces%20challenges%2C%20often%20resulting%20in%20suboptimal%20performance%20compared%20to%20using%0Asingle%20modalities.%20Moreover%2C%20the%20potential%20of%20multi-modal%20approaches%20in%0Adifferential%20diagnosis%2C%20which%20holds%20significant%20clinical%20importance%2C%20remains%0Alargely%20unexplored.%20We%20propose%20a%20novel%20framework%2C%20DiaMond%2C%20to%20address%20these%0Aissues%20with%20vision%20Transformers%20to%20effectively%20integrate%20MRI%20and%20PET.%20DiaMond%0Ais%20equipped%20with%20self-attention%20and%20a%20novel%20bi-attention%20mechanism%20that%0Asynergistically%20combine%20MRI%20and%20PET%2C%20alongside%20a%20multi-modal%20normalization%20to%0Areduce%20redundant%20dependency%2C%20thereby%20boosting%20the%20performance.%20DiaMond%0Asignificantly%20outperforms%20existing%20multi-modal%20methods%20across%20various%20datasets%2C%0Aachieving%20a%20balanced%20accuracy%20of%2092.4%25%20in%20AD%20diagnosis%2C%2065.2%25%20for%20AD-MCI-CN%0Aclassification%2C%20and%2076.5%25%20in%20differential%20diagnosis%20of%20AD%20and%20FTD.%20We%20also%0Avalidated%20the%20robustness%20of%20DiaMond%20in%20a%20comprehensive%20ablation%20study.%20The%20code%0Ais%20available%20at%20https%3A//github.com/ai-med/DiaMond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23219v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiaMond%253A%2520Dementia%2520Diagnosis%2520with%2520Multi-Modal%2520Vision%2520Transformers%2520Using%250A%2520%2520MRI%2520and%2520PET%26entry.906535625%3DYitong%2520Li%2520and%2520Morteza%2520Ghahremani%2520and%2520Youssef%2520Wally%2520and%2520Christian%2520Wachinger%26entry.1292438233%3D%2520%2520Diagnosing%2520dementia%252C%2520particularly%2520for%2520Alzheimer%2527s%2520Disease%2520%2528AD%2529%2520and%250Afrontotemporal%2520dementia%2520%2528FTD%2529%252C%2520is%2520complex%2520due%2520to%2520overlapping%2520symptoms.%2520While%250Amagnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520and%2520positron%2520emission%2520tomography%2520%2528PET%2529%2520data%250Aare%2520critical%2520for%2520the%2520diagnosis%252C%2520integrating%2520these%2520modalities%2520in%2520deep%2520learning%250Afaces%2520challenges%252C%2520often%2520resulting%2520in%2520suboptimal%2520performance%2520compared%2520to%2520using%250Asingle%2520modalities.%2520Moreover%252C%2520the%2520potential%2520of%2520multi-modal%2520approaches%2520in%250Adifferential%2520diagnosis%252C%2520which%2520holds%2520significant%2520clinical%2520importance%252C%2520remains%250Alargely%2520unexplored.%2520We%2520propose%2520a%2520novel%2520framework%252C%2520DiaMond%252C%2520to%2520address%2520these%250Aissues%2520with%2520vision%2520Transformers%2520to%2520effectively%2520integrate%2520MRI%2520and%2520PET.%2520DiaMond%250Ais%2520equipped%2520with%2520self-attention%2520and%2520a%2520novel%2520bi-attention%2520mechanism%2520that%250Asynergistically%2520combine%2520MRI%2520and%2520PET%252C%2520alongside%2520a%2520multi-modal%2520normalization%2520to%250Areduce%2520redundant%2520dependency%252C%2520thereby%2520boosting%2520the%2520performance.%2520DiaMond%250Asignificantly%2520outperforms%2520existing%2520multi-modal%2520methods%2520across%2520various%2520datasets%252C%250Aachieving%2520a%2520balanced%2520accuracy%2520of%252092.4%2525%2520in%2520AD%2520diagnosis%252C%252065.2%2525%2520for%2520AD-MCI-CN%250Aclassification%252C%2520and%252076.5%2525%2520in%2520differential%2520diagnosis%2520of%2520AD%2520and%2520FTD.%2520We%2520also%250Avalidated%2520the%2520robustness%2520of%2520DiaMond%2520in%2520a%2520comprehensive%2520ablation%2520study.%2520The%2520code%250Ais%2520available%2520at%2520https%253A//github.com/ai-med/DiaMond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23219v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiaMond%3A%20Dementia%20Diagnosis%20with%20Multi-Modal%20Vision%20Transformers%20Using%0A%20%20MRI%20and%20PET&entry.906535625=Yitong%20Li%20and%20Morteza%20Ghahremani%20and%20Youssef%20Wally%20and%20Christian%20Wachinger&entry.1292438233=%20%20Diagnosing%20dementia%2C%20particularly%20for%20Alzheimer%27s%20Disease%20%28AD%29%20and%0Afrontotemporal%20dementia%20%28FTD%29%2C%20is%20complex%20due%20to%20overlapping%20symptoms.%20While%0Amagnetic%20resonance%20imaging%20%28MRI%29%20and%20positron%20emission%20tomography%20%28PET%29%20data%0Aare%20critical%20for%20the%20diagnosis%2C%20integrating%20these%20modalities%20in%20deep%20learning%0Afaces%20challenges%2C%20often%20resulting%20in%20suboptimal%20performance%20compared%20to%20using%0Asingle%20modalities.%20Moreover%2C%20the%20potential%20of%20multi-modal%20approaches%20in%0Adifferential%20diagnosis%2C%20which%20holds%20significant%20clinical%20importance%2C%20remains%0Alargely%20unexplored.%20We%20propose%20a%20novel%20framework%2C%20DiaMond%2C%20to%20address%20these%0Aissues%20with%20vision%20Transformers%20to%20effectively%20integrate%20MRI%20and%20PET.%20DiaMond%0Ais%20equipped%20with%20self-attention%20and%20a%20novel%20bi-attention%20mechanism%20that%0Asynergistically%20combine%20MRI%20and%20PET%2C%20alongside%20a%20multi-modal%20normalization%20to%0Areduce%20redundant%20dependency%2C%20thereby%20boosting%20the%20performance.%20DiaMond%0Asignificantly%20outperforms%20existing%20multi-modal%20methods%20across%20various%20datasets%2C%0Aachieving%20a%20balanced%20accuracy%20of%2092.4%25%20in%20AD%20diagnosis%2C%2065.2%25%20for%20AD-MCI-CN%0Aclassification%2C%20and%2076.5%25%20in%20differential%20diagnosis%20of%20AD%20and%20FTD.%20We%20also%0Avalidated%20the%20robustness%20of%20DiaMond%20in%20a%20comprehensive%20ablation%20study.%20The%20code%0Ais%20available%20at%20https%3A//github.com/ai-med/DiaMond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23219v1&entry.124074799=Read"},
{"title": "Quantum Boltzmann machine learning of ground-state energies", "author": "Dhrumil Patel and Daniel Koch and Saahil Patel and Mark M. Wilde", "abstract": "  Estimating the ground-state energy of Hamiltonians is a fundamental task for\nwhich it is believed that quantum computers can be helpful. Several approaches\nhave been proposed toward this goal, including algorithms based on quantum\nphase estimation and hybrid quantum-classical optimizers involving\nparameterized quantum circuits, the latter falling under the umbrella of the\nvariational quantum eigensolver. Here, we analyze the performance of quantum\nBoltzmann machines for this task, which is a less explored ansatz based on\nparameterized thermal states and which is not known to suffer from the\nbarren-plateau problem. We delineate a hybrid quantum-classical algorithm for\nthis task and rigorously prove that it converges to an\n$\\varepsilon$-approximate stationary point of the energy function optimized\nover parameter space, while using a number of parameterized-thermal-state\nsamples that is polynomial in $\\varepsilon^{-1}$, the number of parameters, and\nthe norm of the Hamiltonian being optimized. Our algorithm estimates the\ngradient of the energy function efficiently by means of a novel quantum circuit\nconstruction that combines classical sampling, Hamiltonian simulation, and the\nHadamard test, thus overcoming a key obstacle to quantum Boltzmann machine\nlearning that has been left open since [Amin et al., Phys. Rev. X 8, 021050\n(2018)]. Additionally supporting our main claims are calculations of the\ngradient and Hessian of the energy function, as well as an upper bound on the\nmatrix elements of the latter that is used in the convergence analysis.\n", "link": "http://arxiv.org/abs/2410.12935v2", "date": "2024-10-30", "relevancy": 1.628, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4464}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4037}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Boltzmann%20machine%20learning%20of%20ground-state%20energies&body=Title%3A%20Quantum%20Boltzmann%20machine%20learning%20of%20ground-state%20energies%0AAuthor%3A%20Dhrumil%20Patel%20and%20Daniel%20Koch%20and%20Saahil%20Patel%20and%20Mark%20M.%20Wilde%0AAbstract%3A%20%20%20Estimating%20the%20ground-state%20energy%20of%20Hamiltonians%20is%20a%20fundamental%20task%20for%0Awhich%20it%20is%20believed%20that%20quantum%20computers%20can%20be%20helpful.%20Several%20approaches%0Ahave%20been%20proposed%20toward%20this%20goal%2C%20including%20algorithms%20based%20on%20quantum%0Aphase%20estimation%20and%20hybrid%20quantum-classical%20optimizers%20involving%0Aparameterized%20quantum%20circuits%2C%20the%20latter%20falling%20under%20the%20umbrella%20of%20the%0Avariational%20quantum%20eigensolver.%20Here%2C%20we%20analyze%20the%20performance%20of%20quantum%0ABoltzmann%20machines%20for%20this%20task%2C%20which%20is%20a%20less%20explored%20ansatz%20based%20on%0Aparameterized%20thermal%20states%20and%20which%20is%20not%20known%20to%20suffer%20from%20the%0Abarren-plateau%20problem.%20We%20delineate%20a%20hybrid%20quantum-classical%20algorithm%20for%0Athis%20task%20and%20rigorously%20prove%20that%20it%20converges%20to%20an%0A%24%5Cvarepsilon%24-approximate%20stationary%20point%20of%20the%20energy%20function%20optimized%0Aover%20parameter%20space%2C%20while%20using%20a%20number%20of%20parameterized-thermal-state%0Asamples%20that%20is%20polynomial%20in%20%24%5Cvarepsilon%5E%7B-1%7D%24%2C%20the%20number%20of%20parameters%2C%20and%0Athe%20norm%20of%20the%20Hamiltonian%20being%20optimized.%20Our%20algorithm%20estimates%20the%0Agradient%20of%20the%20energy%20function%20efficiently%20by%20means%20of%20a%20novel%20quantum%20circuit%0Aconstruction%20that%20combines%20classical%20sampling%2C%20Hamiltonian%20simulation%2C%20and%20the%0AHadamard%20test%2C%20thus%20overcoming%20a%20key%20obstacle%20to%20quantum%20Boltzmann%20machine%0Alearning%20that%20has%20been%20left%20open%20since%20%5BAmin%20et%20al.%2C%20Phys.%20Rev.%20X%208%2C%20021050%0A%282018%29%5D.%20Additionally%20supporting%20our%20main%20claims%20are%20calculations%20of%20the%0Agradient%20and%20Hessian%20of%20the%20energy%20function%2C%20as%20well%20as%20an%20upper%20bound%20on%20the%0Amatrix%20elements%20of%20the%20latter%20that%20is%20used%20in%20the%20convergence%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12935v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Boltzmann%2520machine%2520learning%2520of%2520ground-state%2520energies%26entry.906535625%3DDhrumil%2520Patel%2520and%2520Daniel%2520Koch%2520and%2520Saahil%2520Patel%2520and%2520Mark%2520M.%2520Wilde%26entry.1292438233%3D%2520%2520Estimating%2520the%2520ground-state%2520energy%2520of%2520Hamiltonians%2520is%2520a%2520fundamental%2520task%2520for%250Awhich%2520it%2520is%2520believed%2520that%2520quantum%2520computers%2520can%2520be%2520helpful.%2520Several%2520approaches%250Ahave%2520been%2520proposed%2520toward%2520this%2520goal%252C%2520including%2520algorithms%2520based%2520on%2520quantum%250Aphase%2520estimation%2520and%2520hybrid%2520quantum-classical%2520optimizers%2520involving%250Aparameterized%2520quantum%2520circuits%252C%2520the%2520latter%2520falling%2520under%2520the%2520umbrella%2520of%2520the%250Avariational%2520quantum%2520eigensolver.%2520Here%252C%2520we%2520analyze%2520the%2520performance%2520of%2520quantum%250ABoltzmann%2520machines%2520for%2520this%2520task%252C%2520which%2520is%2520a%2520less%2520explored%2520ansatz%2520based%2520on%250Aparameterized%2520thermal%2520states%2520and%2520which%2520is%2520not%2520known%2520to%2520suffer%2520from%2520the%250Abarren-plateau%2520problem.%2520We%2520delineate%2520a%2520hybrid%2520quantum-classical%2520algorithm%2520for%250Athis%2520task%2520and%2520rigorously%2520prove%2520that%2520it%2520converges%2520to%2520an%250A%2524%255Cvarepsilon%2524-approximate%2520stationary%2520point%2520of%2520the%2520energy%2520function%2520optimized%250Aover%2520parameter%2520space%252C%2520while%2520using%2520a%2520number%2520of%2520parameterized-thermal-state%250Asamples%2520that%2520is%2520polynomial%2520in%2520%2524%255Cvarepsilon%255E%257B-1%257D%2524%252C%2520the%2520number%2520of%2520parameters%252C%2520and%250Athe%2520norm%2520of%2520the%2520Hamiltonian%2520being%2520optimized.%2520Our%2520algorithm%2520estimates%2520the%250Agradient%2520of%2520the%2520energy%2520function%2520efficiently%2520by%2520means%2520of%2520a%2520novel%2520quantum%2520circuit%250Aconstruction%2520that%2520combines%2520classical%2520sampling%252C%2520Hamiltonian%2520simulation%252C%2520and%2520the%250AHadamard%2520test%252C%2520thus%2520overcoming%2520a%2520key%2520obstacle%2520to%2520quantum%2520Boltzmann%2520machine%250Alearning%2520that%2520has%2520been%2520left%2520open%2520since%2520%255BAmin%2520et%2520al.%252C%2520Phys.%2520Rev.%2520X%25208%252C%2520021050%250A%25282018%2529%255D.%2520Additionally%2520supporting%2520our%2520main%2520claims%2520are%2520calculations%2520of%2520the%250Agradient%2520and%2520Hessian%2520of%2520the%2520energy%2520function%252C%2520as%2520well%2520as%2520an%2520upper%2520bound%2520on%2520the%250Amatrix%2520elements%2520of%2520the%2520latter%2520that%2520is%2520used%2520in%2520the%2520convergence%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12935v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Boltzmann%20machine%20learning%20of%20ground-state%20energies&entry.906535625=Dhrumil%20Patel%20and%20Daniel%20Koch%20and%20Saahil%20Patel%20and%20Mark%20M.%20Wilde&entry.1292438233=%20%20Estimating%20the%20ground-state%20energy%20of%20Hamiltonians%20is%20a%20fundamental%20task%20for%0Awhich%20it%20is%20believed%20that%20quantum%20computers%20can%20be%20helpful.%20Several%20approaches%0Ahave%20been%20proposed%20toward%20this%20goal%2C%20including%20algorithms%20based%20on%20quantum%0Aphase%20estimation%20and%20hybrid%20quantum-classical%20optimizers%20involving%0Aparameterized%20quantum%20circuits%2C%20the%20latter%20falling%20under%20the%20umbrella%20of%20the%0Avariational%20quantum%20eigensolver.%20Here%2C%20we%20analyze%20the%20performance%20of%20quantum%0ABoltzmann%20machines%20for%20this%20task%2C%20which%20is%20a%20less%20explored%20ansatz%20based%20on%0Aparameterized%20thermal%20states%20and%20which%20is%20not%20known%20to%20suffer%20from%20the%0Abarren-plateau%20problem.%20We%20delineate%20a%20hybrid%20quantum-classical%20algorithm%20for%0Athis%20task%20and%20rigorously%20prove%20that%20it%20converges%20to%20an%0A%24%5Cvarepsilon%24-approximate%20stationary%20point%20of%20the%20energy%20function%20optimized%0Aover%20parameter%20space%2C%20while%20using%20a%20number%20of%20parameterized-thermal-state%0Asamples%20that%20is%20polynomial%20in%20%24%5Cvarepsilon%5E%7B-1%7D%24%2C%20the%20number%20of%20parameters%2C%20and%0Athe%20norm%20of%20the%20Hamiltonian%20being%20optimized.%20Our%20algorithm%20estimates%20the%0Agradient%20of%20the%20energy%20function%20efficiently%20by%20means%20of%20a%20novel%20quantum%20circuit%0Aconstruction%20that%20combines%20classical%20sampling%2C%20Hamiltonian%20simulation%2C%20and%20the%0AHadamard%20test%2C%20thus%20overcoming%20a%20key%20obstacle%20to%20quantum%20Boltzmann%20machine%0Alearning%20that%20has%20been%20left%20open%20since%20%5BAmin%20et%20al.%2C%20Phys.%20Rev.%20X%208%2C%20021050%0A%282018%29%5D.%20Additionally%20supporting%20our%20main%20claims%20are%20calculations%20of%20the%0Agradient%20and%20Hessian%20of%20the%20energy%20function%2C%20as%20well%20as%20an%20upper%20bound%20on%20the%0Amatrix%20elements%20of%20the%20latter%20that%20is%20used%20in%20the%20convergence%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12935v2&entry.124074799=Read"},
{"title": "Flow Snapshot Neurons in Action: Deep Neural Networks Generalize to\n  Biological Motion Perception", "author": "Shuangpeng Han and Ziyu Wang and Mengmi Zhang", "abstract": "  Biological motion perception (BMP) refers to humans' ability to perceive and\nrecognize the actions of living beings solely from their motion patterns,\nsometimes as minimal as those depicted on point-light displays. While humans\nexcel at these tasks without any prior training, current AI models struggle\nwith poor generalization performance. To close this research gap, we propose\nthe Motion Perceiver (MP). MP solely relies on patch-level optical flows from\nvideo clips as inputs. During training, it learns prototypical flow snapshots\nthrough a competitive binding mechanism and integrates invariant motion\nrepresentations to predict action labels for the given video. During inference,\nwe evaluate the generalization ability of all AI models and humans on 62,656\nvideo stimuli spanning 24 BMP conditions using point-light displays in\nneuroscience. Remarkably, MP outperforms all existing AI models with a maximum\nimprovement of 29% in top-1 action recognition accuracy on these conditions.\nMoreover, we benchmark all AI models in point-light displays of two standard\nvideo datasets in computer vision. MP also demonstrates superior performance in\nthese cases. More interestingly, via psychophysics experiments, we found that\nMP recognizes biological movements in a way that aligns with human behaviors.\nOur data and code are available at\nhttps://github.com/ZhangLab-DeepNeuroCogLab/MotionPerceiver.\n", "link": "http://arxiv.org/abs/2405.16493v2", "date": "2024-10-30", "relevancy": 1.6213, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5569}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5437}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flow%20Snapshot%20Neurons%20in%20Action%3A%20Deep%20Neural%20Networks%20Generalize%20to%0A%20%20Biological%20Motion%20Perception&body=Title%3A%20Flow%20Snapshot%20Neurons%20in%20Action%3A%20Deep%20Neural%20Networks%20Generalize%20to%0A%20%20Biological%20Motion%20Perception%0AAuthor%3A%20Shuangpeng%20Han%20and%20Ziyu%20Wang%20and%20Mengmi%20Zhang%0AAbstract%3A%20%20%20Biological%20motion%20perception%20%28BMP%29%20refers%20to%20humans%27%20ability%20to%20perceive%20and%0Arecognize%20the%20actions%20of%20living%20beings%20solely%20from%20their%20motion%20patterns%2C%0Asometimes%20as%20minimal%20as%20those%20depicted%20on%20point-light%20displays.%20While%20humans%0Aexcel%20at%20these%20tasks%20without%20any%20prior%20training%2C%20current%20AI%20models%20struggle%0Awith%20poor%20generalization%20performance.%20To%20close%20this%20research%20gap%2C%20we%20propose%0Athe%20Motion%20Perceiver%20%28MP%29.%20MP%20solely%20relies%20on%20patch-level%20optical%20flows%20from%0Avideo%20clips%20as%20inputs.%20During%20training%2C%20it%20learns%20prototypical%20flow%20snapshots%0Athrough%20a%20competitive%20binding%20mechanism%20and%20integrates%20invariant%20motion%0Arepresentations%20to%20predict%20action%20labels%20for%20the%20given%20video.%20During%20inference%2C%0Awe%20evaluate%20the%20generalization%20ability%20of%20all%20AI%20models%20and%20humans%20on%2062%2C656%0Avideo%20stimuli%20spanning%2024%20BMP%20conditions%20using%20point-light%20displays%20in%0Aneuroscience.%20Remarkably%2C%20MP%20outperforms%20all%20existing%20AI%20models%20with%20a%20maximum%0Aimprovement%20of%2029%25%20in%20top-1%20action%20recognition%20accuracy%20on%20these%20conditions.%0AMoreover%2C%20we%20benchmark%20all%20AI%20models%20in%20point-light%20displays%20of%20two%20standard%0Avideo%20datasets%20in%20computer%20vision.%20MP%20also%20demonstrates%20superior%20performance%20in%0Athese%20cases.%20More%20interestingly%2C%20via%20psychophysics%20experiments%2C%20we%20found%20that%0AMP%20recognizes%20biological%20movements%20in%20a%20way%20that%20aligns%20with%20human%20behaviors.%0AOur%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/ZhangLab-DeepNeuroCogLab/MotionPerceiver.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16493v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlow%2520Snapshot%2520Neurons%2520in%2520Action%253A%2520Deep%2520Neural%2520Networks%2520Generalize%2520to%250A%2520%2520Biological%2520Motion%2520Perception%26entry.906535625%3DShuangpeng%2520Han%2520and%2520Ziyu%2520Wang%2520and%2520Mengmi%2520Zhang%26entry.1292438233%3D%2520%2520Biological%2520motion%2520perception%2520%2528BMP%2529%2520refers%2520to%2520humans%2527%2520ability%2520to%2520perceive%2520and%250Arecognize%2520the%2520actions%2520of%2520living%2520beings%2520solely%2520from%2520their%2520motion%2520patterns%252C%250Asometimes%2520as%2520minimal%2520as%2520those%2520depicted%2520on%2520point-light%2520displays.%2520While%2520humans%250Aexcel%2520at%2520these%2520tasks%2520without%2520any%2520prior%2520training%252C%2520current%2520AI%2520models%2520struggle%250Awith%2520poor%2520generalization%2520performance.%2520To%2520close%2520this%2520research%2520gap%252C%2520we%2520propose%250Athe%2520Motion%2520Perceiver%2520%2528MP%2529.%2520MP%2520solely%2520relies%2520on%2520patch-level%2520optical%2520flows%2520from%250Avideo%2520clips%2520as%2520inputs.%2520During%2520training%252C%2520it%2520learns%2520prototypical%2520flow%2520snapshots%250Athrough%2520a%2520competitive%2520binding%2520mechanism%2520and%2520integrates%2520invariant%2520motion%250Arepresentations%2520to%2520predict%2520action%2520labels%2520for%2520the%2520given%2520video.%2520During%2520inference%252C%250Awe%2520evaluate%2520the%2520generalization%2520ability%2520of%2520all%2520AI%2520models%2520and%2520humans%2520on%252062%252C656%250Avideo%2520stimuli%2520spanning%252024%2520BMP%2520conditions%2520using%2520point-light%2520displays%2520in%250Aneuroscience.%2520Remarkably%252C%2520MP%2520outperforms%2520all%2520existing%2520AI%2520models%2520with%2520a%2520maximum%250Aimprovement%2520of%252029%2525%2520in%2520top-1%2520action%2520recognition%2520accuracy%2520on%2520these%2520conditions.%250AMoreover%252C%2520we%2520benchmark%2520all%2520AI%2520models%2520in%2520point-light%2520displays%2520of%2520two%2520standard%250Avideo%2520datasets%2520in%2520computer%2520vision.%2520MP%2520also%2520demonstrates%2520superior%2520performance%2520in%250Athese%2520cases.%2520More%2520interestingly%252C%2520via%2520psychophysics%2520experiments%252C%2520we%2520found%2520that%250AMP%2520recognizes%2520biological%2520movements%2520in%2520a%2520way%2520that%2520aligns%2520with%2520human%2520behaviors.%250AOur%2520data%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/ZhangLab-DeepNeuroCogLab/MotionPerceiver.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16493v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flow%20Snapshot%20Neurons%20in%20Action%3A%20Deep%20Neural%20Networks%20Generalize%20to%0A%20%20Biological%20Motion%20Perception&entry.906535625=Shuangpeng%20Han%20and%20Ziyu%20Wang%20and%20Mengmi%20Zhang&entry.1292438233=%20%20Biological%20motion%20perception%20%28BMP%29%20refers%20to%20humans%27%20ability%20to%20perceive%20and%0Arecognize%20the%20actions%20of%20living%20beings%20solely%20from%20their%20motion%20patterns%2C%0Asometimes%20as%20minimal%20as%20those%20depicted%20on%20point-light%20displays.%20While%20humans%0Aexcel%20at%20these%20tasks%20without%20any%20prior%20training%2C%20current%20AI%20models%20struggle%0Awith%20poor%20generalization%20performance.%20To%20close%20this%20research%20gap%2C%20we%20propose%0Athe%20Motion%20Perceiver%20%28MP%29.%20MP%20solely%20relies%20on%20patch-level%20optical%20flows%20from%0Avideo%20clips%20as%20inputs.%20During%20training%2C%20it%20learns%20prototypical%20flow%20snapshots%0Athrough%20a%20competitive%20binding%20mechanism%20and%20integrates%20invariant%20motion%0Arepresentations%20to%20predict%20action%20labels%20for%20the%20given%20video.%20During%20inference%2C%0Awe%20evaluate%20the%20generalization%20ability%20of%20all%20AI%20models%20and%20humans%20on%2062%2C656%0Avideo%20stimuli%20spanning%2024%20BMP%20conditions%20using%20point-light%20displays%20in%0Aneuroscience.%20Remarkably%2C%20MP%20outperforms%20all%20existing%20AI%20models%20with%20a%20maximum%0Aimprovement%20of%2029%25%20in%20top-1%20action%20recognition%20accuracy%20on%20these%20conditions.%0AMoreover%2C%20we%20benchmark%20all%20AI%20models%20in%20point-light%20displays%20of%20two%20standard%0Avideo%20datasets%20in%20computer%20vision.%20MP%20also%20demonstrates%20superior%20performance%20in%0Athese%20cases.%20More%20interestingly%2C%20via%20psychophysics%20experiments%2C%20we%20found%20that%0AMP%20recognizes%20biological%20movements%20in%20a%20way%20that%20aligns%20with%20human%20behaviors.%0AOur%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/ZhangLab-DeepNeuroCogLab/MotionPerceiver.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16493v2&entry.124074799=Read"},
{"title": "Provable acceleration for diffusion models under minimal assumptions", "author": "Gen Li and Changxiao Cai", "abstract": "  While score-based diffusion models have achieved exceptional sampling\nquality, their sampling speeds are often limited by the high computational\nburden of score function evaluations. Despite the recent remarkable empirical\nadvances in speeding up the score-based samplers, theoretical understanding of\nacceleration techniques remains largely limited. To bridge this gap, we propose\na novel training-free acceleration scheme for stochastic samplers. Under\nminimal assumptions -- namely, $L^2$-accurate score estimates and a finite\nsecond-moment condition on the target distribution -- our accelerated sampler\nprovably achieves $\\varepsilon$-accuracy in total variation within\n$\\widetilde{O}(d^{5/4}/\\sqrt{\\varepsilon})$ iterations, thereby significantly\nimproving upon the $\\widetilde{O}(d/\\varepsilon)$ iteration complexity of\nstandard score-based samplers. Notably, our convergence theory does not rely on\nrestrictive assumptions on the target distribution or higher-order score\nestimation guarantees.\n", "link": "http://arxiv.org/abs/2410.23285v1", "date": "2024-10-30", "relevancy": 1.5881, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6025}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5205}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provable%20acceleration%20for%20diffusion%20models%20under%20minimal%20assumptions&body=Title%3A%20Provable%20acceleration%20for%20diffusion%20models%20under%20minimal%20assumptions%0AAuthor%3A%20Gen%20Li%20and%20Changxiao%20Cai%0AAbstract%3A%20%20%20While%20score-based%20diffusion%20models%20have%20achieved%20exceptional%20sampling%0Aquality%2C%20their%20sampling%20speeds%20are%20often%20limited%20by%20the%20high%20computational%0Aburden%20of%20score%20function%20evaluations.%20Despite%20the%20recent%20remarkable%20empirical%0Aadvances%20in%20speeding%20up%20the%20score-based%20samplers%2C%20theoretical%20understanding%20of%0Aacceleration%20techniques%20remains%20largely%20limited.%20To%20bridge%20this%20gap%2C%20we%20propose%0Aa%20novel%20training-free%20acceleration%20scheme%20for%20stochastic%20samplers.%20Under%0Aminimal%20assumptions%20--%20namely%2C%20%24L%5E2%24-accurate%20score%20estimates%20and%20a%20finite%0Asecond-moment%20condition%20on%20the%20target%20distribution%20--%20our%20accelerated%20sampler%0Aprovably%20achieves%20%24%5Cvarepsilon%24-accuracy%20in%20total%20variation%20within%0A%24%5Cwidetilde%7BO%7D%28d%5E%7B5/4%7D/%5Csqrt%7B%5Cvarepsilon%7D%29%24%20iterations%2C%20thereby%20significantly%0Aimproving%20upon%20the%20%24%5Cwidetilde%7BO%7D%28d/%5Cvarepsilon%29%24%20iteration%20complexity%20of%0Astandard%20score-based%20samplers.%20Notably%2C%20our%20convergence%20theory%20does%20not%20rely%20on%0Arestrictive%20assumptions%20on%20the%20target%20distribution%20or%20higher-order%20score%0Aestimation%20guarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23285v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvable%2520acceleration%2520for%2520diffusion%2520models%2520under%2520minimal%2520assumptions%26entry.906535625%3DGen%2520Li%2520and%2520Changxiao%2520Cai%26entry.1292438233%3D%2520%2520While%2520score-based%2520diffusion%2520models%2520have%2520achieved%2520exceptional%2520sampling%250Aquality%252C%2520their%2520sampling%2520speeds%2520are%2520often%2520limited%2520by%2520the%2520high%2520computational%250Aburden%2520of%2520score%2520function%2520evaluations.%2520Despite%2520the%2520recent%2520remarkable%2520empirical%250Aadvances%2520in%2520speeding%2520up%2520the%2520score-based%2520samplers%252C%2520theoretical%2520understanding%2520of%250Aacceleration%2520techniques%2520remains%2520largely%2520limited.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%250Aa%2520novel%2520training-free%2520acceleration%2520scheme%2520for%2520stochastic%2520samplers.%2520Under%250Aminimal%2520assumptions%2520--%2520namely%252C%2520%2524L%255E2%2524-accurate%2520score%2520estimates%2520and%2520a%2520finite%250Asecond-moment%2520condition%2520on%2520the%2520target%2520distribution%2520--%2520our%2520accelerated%2520sampler%250Aprovably%2520achieves%2520%2524%255Cvarepsilon%2524-accuracy%2520in%2520total%2520variation%2520within%250A%2524%255Cwidetilde%257BO%257D%2528d%255E%257B5/4%257D/%255Csqrt%257B%255Cvarepsilon%257D%2529%2524%2520iterations%252C%2520thereby%2520significantly%250Aimproving%2520upon%2520the%2520%2524%255Cwidetilde%257BO%257D%2528d/%255Cvarepsilon%2529%2524%2520iteration%2520complexity%2520of%250Astandard%2520score-based%2520samplers.%2520Notably%252C%2520our%2520convergence%2520theory%2520does%2520not%2520rely%2520on%250Arestrictive%2520assumptions%2520on%2520the%2520target%2520distribution%2520or%2520higher-order%2520score%250Aestimation%2520guarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23285v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provable%20acceleration%20for%20diffusion%20models%20under%20minimal%20assumptions&entry.906535625=Gen%20Li%20and%20Changxiao%20Cai&entry.1292438233=%20%20While%20score-based%20diffusion%20models%20have%20achieved%20exceptional%20sampling%0Aquality%2C%20their%20sampling%20speeds%20are%20often%20limited%20by%20the%20high%20computational%0Aburden%20of%20score%20function%20evaluations.%20Despite%20the%20recent%20remarkable%20empirical%0Aadvances%20in%20speeding%20up%20the%20score-based%20samplers%2C%20theoretical%20understanding%20of%0Aacceleration%20techniques%20remains%20largely%20limited.%20To%20bridge%20this%20gap%2C%20we%20propose%0Aa%20novel%20training-free%20acceleration%20scheme%20for%20stochastic%20samplers.%20Under%0Aminimal%20assumptions%20--%20namely%2C%20%24L%5E2%24-accurate%20score%20estimates%20and%20a%20finite%0Asecond-moment%20condition%20on%20the%20target%20distribution%20--%20our%20accelerated%20sampler%0Aprovably%20achieves%20%24%5Cvarepsilon%24-accuracy%20in%20total%20variation%20within%0A%24%5Cwidetilde%7BO%7D%28d%5E%7B5/4%7D/%5Csqrt%7B%5Cvarepsilon%7D%29%24%20iterations%2C%20thereby%20significantly%0Aimproving%20upon%20the%20%24%5Cwidetilde%7BO%7D%28d/%5Cvarepsilon%29%24%20iteration%20complexity%20of%0Astandard%20score-based%20samplers.%20Notably%2C%20our%20convergence%20theory%20does%20not%20rely%20on%0Arestrictive%20assumptions%20on%20the%20target%20distribution%20or%20higher-order%20score%0Aestimation%20guarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23285v1&entry.124074799=Read"},
{"title": "Fair Division with Market Values", "author": "Siddharth Barman and Soroush Ebadian and Mohamad Latifian and Nisarg Shah", "abstract": "  We introduce a model of fair division with market values, where indivisible\ngoods must be partitioned among agents with (additive) subjective valuations,\nand each good additionally has a market value. The market valuation can be\nviewed as a separate additive valuation that holds identically across all the\nagents. We seek allocations that are simultaneously fair with respect to the\nsubjective valuations and with respect to the market valuation.\n  We show that an allocation that satisfies stochastically-dominant\nenvy-freeness up to one good (SD-EF1) with respect to both the subjective\nvaluations and the market valuation does not always exist, but the weaker\nguarantee of EF1 with respect to the subjective valuations along with SD-EF1\nwith respect to the market valuation can be guaranteed. We also study a number\nof other guarantees such as Pareto optimality, EFX, and MMS. In addition, we\nexplore non-additive valuations and extend our model to cake-cutting. Along the\nway, we identify several tantalizing open questions.\n", "link": "http://arxiv.org/abs/2410.23137v1", "date": "2024-10-30", "relevancy": 1.5631, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.405}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.3826}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fair%20Division%20with%20Market%20Values&body=Title%3A%20Fair%20Division%20with%20Market%20Values%0AAuthor%3A%20Siddharth%20Barman%20and%20Soroush%20Ebadian%20and%20Mohamad%20Latifian%20and%20Nisarg%20Shah%0AAbstract%3A%20%20%20We%20introduce%20a%20model%20of%20fair%20division%20with%20market%20values%2C%20where%20indivisible%0Agoods%20must%20be%20partitioned%20among%20agents%20with%20%28additive%29%20subjective%20valuations%2C%0Aand%20each%20good%20additionally%20has%20a%20market%20value.%20The%20market%20valuation%20can%20be%0Aviewed%20as%20a%20separate%20additive%20valuation%20that%20holds%20identically%20across%20all%20the%0Aagents.%20We%20seek%20allocations%20that%20are%20simultaneously%20fair%20with%20respect%20to%20the%0Asubjective%20valuations%20and%20with%20respect%20to%20the%20market%20valuation.%0A%20%20We%20show%20that%20an%20allocation%20that%20satisfies%20stochastically-dominant%0Aenvy-freeness%20up%20to%20one%20good%20%28SD-EF1%29%20with%20respect%20to%20both%20the%20subjective%0Avaluations%20and%20the%20market%20valuation%20does%20not%20always%20exist%2C%20but%20the%20weaker%0Aguarantee%20of%20EF1%20with%20respect%20to%20the%20subjective%20valuations%20along%20with%20SD-EF1%0Awith%20respect%20to%20the%20market%20valuation%20can%20be%20guaranteed.%20We%20also%20study%20a%20number%0Aof%20other%20guarantees%20such%20as%20Pareto%20optimality%2C%20EFX%2C%20and%20MMS.%20In%20addition%2C%20we%0Aexplore%20non-additive%20valuations%20and%20extend%20our%20model%20to%20cake-cutting.%20Along%20the%0Away%2C%20we%20identify%20several%20tantalizing%20open%20questions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFair%2520Division%2520with%2520Market%2520Values%26entry.906535625%3DSiddharth%2520Barman%2520and%2520Soroush%2520Ebadian%2520and%2520Mohamad%2520Latifian%2520and%2520Nisarg%2520Shah%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520model%2520of%2520fair%2520division%2520with%2520market%2520values%252C%2520where%2520indivisible%250Agoods%2520must%2520be%2520partitioned%2520among%2520agents%2520with%2520%2528additive%2529%2520subjective%2520valuations%252C%250Aand%2520each%2520good%2520additionally%2520has%2520a%2520market%2520value.%2520The%2520market%2520valuation%2520can%2520be%250Aviewed%2520as%2520a%2520separate%2520additive%2520valuation%2520that%2520holds%2520identically%2520across%2520all%2520the%250Aagents.%2520We%2520seek%2520allocations%2520that%2520are%2520simultaneously%2520fair%2520with%2520respect%2520to%2520the%250Asubjective%2520valuations%2520and%2520with%2520respect%2520to%2520the%2520market%2520valuation.%250A%2520%2520We%2520show%2520that%2520an%2520allocation%2520that%2520satisfies%2520stochastically-dominant%250Aenvy-freeness%2520up%2520to%2520one%2520good%2520%2528SD-EF1%2529%2520with%2520respect%2520to%2520both%2520the%2520subjective%250Avaluations%2520and%2520the%2520market%2520valuation%2520does%2520not%2520always%2520exist%252C%2520but%2520the%2520weaker%250Aguarantee%2520of%2520EF1%2520with%2520respect%2520to%2520the%2520subjective%2520valuations%2520along%2520with%2520SD-EF1%250Awith%2520respect%2520to%2520the%2520market%2520valuation%2520can%2520be%2520guaranteed.%2520We%2520also%2520study%2520a%2520number%250Aof%2520other%2520guarantees%2520such%2520as%2520Pareto%2520optimality%252C%2520EFX%252C%2520and%2520MMS.%2520In%2520addition%252C%2520we%250Aexplore%2520non-additive%2520valuations%2520and%2520extend%2520our%2520model%2520to%2520cake-cutting.%2520Along%2520the%250Away%252C%2520we%2520identify%2520several%2520tantalizing%2520open%2520questions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fair%20Division%20with%20Market%20Values&entry.906535625=Siddharth%20Barman%20and%20Soroush%20Ebadian%20and%20Mohamad%20Latifian%20and%20Nisarg%20Shah&entry.1292438233=%20%20We%20introduce%20a%20model%20of%20fair%20division%20with%20market%20values%2C%20where%20indivisible%0Agoods%20must%20be%20partitioned%20among%20agents%20with%20%28additive%29%20subjective%20valuations%2C%0Aand%20each%20good%20additionally%20has%20a%20market%20value.%20The%20market%20valuation%20can%20be%0Aviewed%20as%20a%20separate%20additive%20valuation%20that%20holds%20identically%20across%20all%20the%0Aagents.%20We%20seek%20allocations%20that%20are%20simultaneously%20fair%20with%20respect%20to%20the%0Asubjective%20valuations%20and%20with%20respect%20to%20the%20market%20valuation.%0A%20%20We%20show%20that%20an%20allocation%20that%20satisfies%20stochastically-dominant%0Aenvy-freeness%20up%20to%20one%20good%20%28SD-EF1%29%20with%20respect%20to%20both%20the%20subjective%0Avaluations%20and%20the%20market%20valuation%20does%20not%20always%20exist%2C%20but%20the%20weaker%0Aguarantee%20of%20EF1%20with%20respect%20to%20the%20subjective%20valuations%20along%20with%20SD-EF1%0Awith%20respect%20to%20the%20market%20valuation%20can%20be%20guaranteed.%20We%20also%20study%20a%20number%0Aof%20other%20guarantees%20such%20as%20Pareto%20optimality%2C%20EFX%2C%20and%20MMS.%20In%20addition%2C%20we%0Aexplore%20non-additive%20valuations%20and%20extend%20our%20model%20to%20cake-cutting.%20Along%20the%0Away%2C%20we%20identify%20several%20tantalizing%20open%20questions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23137v1&entry.124074799=Read"},
{"title": "DisCo: Distributed Contact-Rich Trajectory Optimization for Forceful\n  Multi-Robot Collaboration", "author": "Ola Shorinwa and Matthew Devlin and Elliot W. Hawkes and Mac Schwager", "abstract": "  We present DisCo, a distributed algorithm for contact-rich, multi-robot\ntasks. DisCo is a distributed contact-implicit trajectory optimization\nalgorithm, which allows a group of robots to optimize a time sequence of forces\nto objects and to their environment to accomplish tasks such as collaborative\nmanipulation, robot team sports, and modular robot locomotion. We build our\nalgorithm on a variant of the Alternating Direction Method of Multipliers\n(ADMM), where each robot computes its own contact forces and contact-switching\nevents from a smaller single-robot, contact-implicit trajectory optimization\nproblem, while cooperating with other robots through dual variables, enforcing\nconstraints between robots. Each robot iterates between solving its local\nproblem, and communicating over a wireless mesh network to enforce these\nconsistency constraints with its neighbors, ultimately converging to a\ncoordinated plan for the group. The local problems solved by each robot are\nsignificantly less challenging than a centralized problem with all robots'\ncontact forces and switching events, improving the computational efficiency,\nwhile also preserving the privacy of some aspects of each robot's operation. We\ndemonstrate the effectiveness of our algorithm in simulations of collaborative\nmanipulation, multi-robot team sports scenarios, and in modular robot\nlocomotion, where DisCo achieves $3$x higher success rates with a 2.5x to 5x\nfaster computation time. Further, we provide results of hardware experiments on\na modular truss robot, with three collaborating truss nodes planning\nindividually while working together to produce a punctuated rolling-gate motion\nof the composite structure. Videos are available on the project page:\nhttps://disco-opt.github.io.\n", "link": "http://arxiv.org/abs/2410.23283v1", "date": "2024-10-30", "relevancy": 1.5547, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5732}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5436}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DisCo%3A%20Distributed%20Contact-Rich%20Trajectory%20Optimization%20for%20Forceful%0A%20%20Multi-Robot%20Collaboration&body=Title%3A%20DisCo%3A%20Distributed%20Contact-Rich%20Trajectory%20Optimization%20for%20Forceful%0A%20%20Multi-Robot%20Collaboration%0AAuthor%3A%20Ola%20Shorinwa%20and%20Matthew%20Devlin%20and%20Elliot%20W.%20Hawkes%20and%20Mac%20Schwager%0AAbstract%3A%20%20%20We%20present%20DisCo%2C%20a%20distributed%20algorithm%20for%20contact-rich%2C%20multi-robot%0Atasks.%20DisCo%20is%20a%20distributed%20contact-implicit%20trajectory%20optimization%0Aalgorithm%2C%20which%20allows%20a%20group%20of%20robots%20to%20optimize%20a%20time%20sequence%20of%20forces%0Ato%20objects%20and%20to%20their%20environment%20to%20accomplish%20tasks%20such%20as%20collaborative%0Amanipulation%2C%20robot%20team%20sports%2C%20and%20modular%20robot%20locomotion.%20We%20build%20our%0Aalgorithm%20on%20a%20variant%20of%20the%20Alternating%20Direction%20Method%20of%20Multipliers%0A%28ADMM%29%2C%20where%20each%20robot%20computes%20its%20own%20contact%20forces%20and%20contact-switching%0Aevents%20from%20a%20smaller%20single-robot%2C%20contact-implicit%20trajectory%20optimization%0Aproblem%2C%20while%20cooperating%20with%20other%20robots%20through%20dual%20variables%2C%20enforcing%0Aconstraints%20between%20robots.%20Each%20robot%20iterates%20between%20solving%20its%20local%0Aproblem%2C%20and%20communicating%20over%20a%20wireless%20mesh%20network%20to%20enforce%20these%0Aconsistency%20constraints%20with%20its%20neighbors%2C%20ultimately%20converging%20to%20a%0Acoordinated%20plan%20for%20the%20group.%20The%20local%20problems%20solved%20by%20each%20robot%20are%0Asignificantly%20less%20challenging%20than%20a%20centralized%20problem%20with%20all%20robots%27%0Acontact%20forces%20and%20switching%20events%2C%20improving%20the%20computational%20efficiency%2C%0Awhile%20also%20preserving%20the%20privacy%20of%20some%20aspects%20of%20each%20robot%27s%20operation.%20We%0Ademonstrate%20the%20effectiveness%20of%20our%20algorithm%20in%20simulations%20of%20collaborative%0Amanipulation%2C%20multi-robot%20team%20sports%20scenarios%2C%20and%20in%20modular%20robot%0Alocomotion%2C%20where%20DisCo%20achieves%20%243%24x%20higher%20success%20rates%20with%20a%202.5x%20to%205x%0Afaster%20computation%20time.%20Further%2C%20we%20provide%20results%20of%20hardware%20experiments%20on%0Aa%20modular%20truss%20robot%2C%20with%20three%20collaborating%20truss%20nodes%20planning%0Aindividually%20while%20working%20together%20to%20produce%20a%20punctuated%20rolling-gate%20motion%0Aof%20the%20composite%20structure.%20Videos%20are%20available%20on%20the%20project%20page%3A%0Ahttps%3A//disco-opt.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisCo%253A%2520Distributed%2520Contact-Rich%2520Trajectory%2520Optimization%2520for%2520Forceful%250A%2520%2520Multi-Robot%2520Collaboration%26entry.906535625%3DOla%2520Shorinwa%2520and%2520Matthew%2520Devlin%2520and%2520Elliot%2520W.%2520Hawkes%2520and%2520Mac%2520Schwager%26entry.1292438233%3D%2520%2520We%2520present%2520DisCo%252C%2520a%2520distributed%2520algorithm%2520for%2520contact-rich%252C%2520multi-robot%250Atasks.%2520DisCo%2520is%2520a%2520distributed%2520contact-implicit%2520trajectory%2520optimization%250Aalgorithm%252C%2520which%2520allows%2520a%2520group%2520of%2520robots%2520to%2520optimize%2520a%2520time%2520sequence%2520of%2520forces%250Ato%2520objects%2520and%2520to%2520their%2520environment%2520to%2520accomplish%2520tasks%2520such%2520as%2520collaborative%250Amanipulation%252C%2520robot%2520team%2520sports%252C%2520and%2520modular%2520robot%2520locomotion.%2520We%2520build%2520our%250Aalgorithm%2520on%2520a%2520variant%2520of%2520the%2520Alternating%2520Direction%2520Method%2520of%2520Multipliers%250A%2528ADMM%2529%252C%2520where%2520each%2520robot%2520computes%2520its%2520own%2520contact%2520forces%2520and%2520contact-switching%250Aevents%2520from%2520a%2520smaller%2520single-robot%252C%2520contact-implicit%2520trajectory%2520optimization%250Aproblem%252C%2520while%2520cooperating%2520with%2520other%2520robots%2520through%2520dual%2520variables%252C%2520enforcing%250Aconstraints%2520between%2520robots.%2520Each%2520robot%2520iterates%2520between%2520solving%2520its%2520local%250Aproblem%252C%2520and%2520communicating%2520over%2520a%2520wireless%2520mesh%2520network%2520to%2520enforce%2520these%250Aconsistency%2520constraints%2520with%2520its%2520neighbors%252C%2520ultimately%2520converging%2520to%2520a%250Acoordinated%2520plan%2520for%2520the%2520group.%2520The%2520local%2520problems%2520solved%2520by%2520each%2520robot%2520are%250Asignificantly%2520less%2520challenging%2520than%2520a%2520centralized%2520problem%2520with%2520all%2520robots%2527%250Acontact%2520forces%2520and%2520switching%2520events%252C%2520improving%2520the%2520computational%2520efficiency%252C%250Awhile%2520also%2520preserving%2520the%2520privacy%2520of%2520some%2520aspects%2520of%2520each%2520robot%2527s%2520operation.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520algorithm%2520in%2520simulations%2520of%2520collaborative%250Amanipulation%252C%2520multi-robot%2520team%2520sports%2520scenarios%252C%2520and%2520in%2520modular%2520robot%250Alocomotion%252C%2520where%2520DisCo%2520achieves%2520%25243%2524x%2520higher%2520success%2520rates%2520with%2520a%25202.5x%2520to%25205x%250Afaster%2520computation%2520time.%2520Further%252C%2520we%2520provide%2520results%2520of%2520hardware%2520experiments%2520on%250Aa%2520modular%2520truss%2520robot%252C%2520with%2520three%2520collaborating%2520truss%2520nodes%2520planning%250Aindividually%2520while%2520working%2520together%2520to%2520produce%2520a%2520punctuated%2520rolling-gate%2520motion%250Aof%2520the%2520composite%2520structure.%2520Videos%2520are%2520available%2520on%2520the%2520project%2520page%253A%250Ahttps%253A//disco-opt.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DisCo%3A%20Distributed%20Contact-Rich%20Trajectory%20Optimization%20for%20Forceful%0A%20%20Multi-Robot%20Collaboration&entry.906535625=Ola%20Shorinwa%20and%20Matthew%20Devlin%20and%20Elliot%20W.%20Hawkes%20and%20Mac%20Schwager&entry.1292438233=%20%20We%20present%20DisCo%2C%20a%20distributed%20algorithm%20for%20contact-rich%2C%20multi-robot%0Atasks.%20DisCo%20is%20a%20distributed%20contact-implicit%20trajectory%20optimization%0Aalgorithm%2C%20which%20allows%20a%20group%20of%20robots%20to%20optimize%20a%20time%20sequence%20of%20forces%0Ato%20objects%20and%20to%20their%20environment%20to%20accomplish%20tasks%20such%20as%20collaborative%0Amanipulation%2C%20robot%20team%20sports%2C%20and%20modular%20robot%20locomotion.%20We%20build%20our%0Aalgorithm%20on%20a%20variant%20of%20the%20Alternating%20Direction%20Method%20of%20Multipliers%0A%28ADMM%29%2C%20where%20each%20robot%20computes%20its%20own%20contact%20forces%20and%20contact-switching%0Aevents%20from%20a%20smaller%20single-robot%2C%20contact-implicit%20trajectory%20optimization%0Aproblem%2C%20while%20cooperating%20with%20other%20robots%20through%20dual%20variables%2C%20enforcing%0Aconstraints%20between%20robots.%20Each%20robot%20iterates%20between%20solving%20its%20local%0Aproblem%2C%20and%20communicating%20over%20a%20wireless%20mesh%20network%20to%20enforce%20these%0Aconsistency%20constraints%20with%20its%20neighbors%2C%20ultimately%20converging%20to%20a%0Acoordinated%20plan%20for%20the%20group.%20The%20local%20problems%20solved%20by%20each%20robot%20are%0Asignificantly%20less%20challenging%20than%20a%20centralized%20problem%20with%20all%20robots%27%0Acontact%20forces%20and%20switching%20events%2C%20improving%20the%20computational%20efficiency%2C%0Awhile%20also%20preserving%20the%20privacy%20of%20some%20aspects%20of%20each%20robot%27s%20operation.%20We%0Ademonstrate%20the%20effectiveness%20of%20our%20algorithm%20in%20simulations%20of%20collaborative%0Amanipulation%2C%20multi-robot%20team%20sports%20scenarios%2C%20and%20in%20modular%20robot%0Alocomotion%2C%20where%20DisCo%20achieves%20%243%24x%20higher%20success%20rates%20with%20a%202.5x%20to%205x%0Afaster%20computation%20time.%20Further%2C%20we%20provide%20results%20of%20hardware%20experiments%20on%0Aa%20modular%20truss%20robot%2C%20with%20three%20collaborating%20truss%20nodes%20planning%0Aindividually%20while%20working%20together%20to%20produce%20a%20punctuated%20rolling-gate%20motion%0Aof%20the%20composite%20structure.%20Videos%20are%20available%20on%20the%20project%20page%3A%0Ahttps%3A//disco-opt.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23283v1&entry.124074799=Read"},
{"title": "Instigating Cooperation among LLM Agents Using Adaptive Information\n  Modulation", "author": "Qiliang Chen and Sepehr Ilami and Nunzio Lore and Babak Heydari", "abstract": "  This paper introduces a novel framework combining LLM agents as proxies for\nhuman strategic behavior with reinforcement learning (RL) to engage these\nagents in evolving strategic interactions within team environments. Our\napproach extends traditional agent-based simulations by using strategic LLM\nagents (SLA) and introducing dynamic and adaptive governance through a\npro-social promoting RL agent (PPA) that modulates information access across\nagents in a network, optimizing social welfare and promoting pro-social\nbehavior. Through validation in iterative games, including the prisoner\ndilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.\nThe PPA agent effectively learns to adjust information transparency, resulting\nin enhanced cooperation rates. This framework offers significant insights into\nAI-mediated social dynamics, contributing to the deployment of AI in real-world\nteam settings.\n", "link": "http://arxiv.org/abs/2409.10372v3", "date": "2024-10-30", "relevancy": 1.5542, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5338}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5029}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instigating%20Cooperation%20among%20LLM%20Agents%20Using%20Adaptive%20Information%0A%20%20Modulation&body=Title%3A%20Instigating%20Cooperation%20among%20LLM%20Agents%20Using%20Adaptive%20Information%0A%20%20Modulation%0AAuthor%3A%20Qiliang%20Chen%20and%20Sepehr%20Ilami%20and%20Nunzio%20Lore%20and%20Babak%20Heydari%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20framework%20combining%20LLM%20agents%20as%20proxies%20for%0Ahuman%20strategic%20behavior%20with%20reinforcement%20learning%20%28RL%29%20to%20engage%20these%0Aagents%20in%20evolving%20strategic%20interactions%20within%20team%20environments.%20Our%0Aapproach%20extends%20traditional%20agent-based%20simulations%20by%20using%20strategic%20LLM%0Aagents%20%28SLA%29%20and%20introducing%20dynamic%20and%20adaptive%20governance%20through%20a%0Apro-social%20promoting%20RL%20agent%20%28PPA%29%20that%20modulates%20information%20access%20across%0Aagents%20in%20a%20network%2C%20optimizing%20social%20welfare%20and%20promoting%20pro-social%0Abehavior.%20Through%20validation%20in%20iterative%20games%2C%20including%20the%20prisoner%0Adilemma%2C%20we%20demonstrate%20that%20SLA%20agents%20exhibit%20nuanced%20strategic%20adaptations.%0AThe%20PPA%20agent%20effectively%20learns%20to%20adjust%20information%20transparency%2C%20resulting%0Ain%20enhanced%20cooperation%20rates.%20This%20framework%20offers%20significant%20insights%20into%0AAI-mediated%20social%20dynamics%2C%20contributing%20to%20the%20deployment%20of%20AI%20in%20real-world%0Ateam%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10372v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstigating%2520Cooperation%2520among%2520LLM%2520Agents%2520Using%2520Adaptive%2520Information%250A%2520%2520Modulation%26entry.906535625%3DQiliang%2520Chen%2520and%2520Sepehr%2520Ilami%2520and%2520Nunzio%2520Lore%2520and%2520Babak%2520Heydari%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520framework%2520combining%2520LLM%2520agents%2520as%2520proxies%2520for%250Ahuman%2520strategic%2520behavior%2520with%2520reinforcement%2520learning%2520%2528RL%2529%2520to%2520engage%2520these%250Aagents%2520in%2520evolving%2520strategic%2520interactions%2520within%2520team%2520environments.%2520Our%250Aapproach%2520extends%2520traditional%2520agent-based%2520simulations%2520by%2520using%2520strategic%2520LLM%250Aagents%2520%2528SLA%2529%2520and%2520introducing%2520dynamic%2520and%2520adaptive%2520governance%2520through%2520a%250Apro-social%2520promoting%2520RL%2520agent%2520%2528PPA%2529%2520that%2520modulates%2520information%2520access%2520across%250Aagents%2520in%2520a%2520network%252C%2520optimizing%2520social%2520welfare%2520and%2520promoting%2520pro-social%250Abehavior.%2520Through%2520validation%2520in%2520iterative%2520games%252C%2520including%2520the%2520prisoner%250Adilemma%252C%2520we%2520demonstrate%2520that%2520SLA%2520agents%2520exhibit%2520nuanced%2520strategic%2520adaptations.%250AThe%2520PPA%2520agent%2520effectively%2520learns%2520to%2520adjust%2520information%2520transparency%252C%2520resulting%250Ain%2520enhanced%2520cooperation%2520rates.%2520This%2520framework%2520offers%2520significant%2520insights%2520into%250AAI-mediated%2520social%2520dynamics%252C%2520contributing%2520to%2520the%2520deployment%2520of%2520AI%2520in%2520real-world%250Ateam%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10372v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instigating%20Cooperation%20among%20LLM%20Agents%20Using%20Adaptive%20Information%0A%20%20Modulation&entry.906535625=Qiliang%20Chen%20and%20Sepehr%20Ilami%20and%20Nunzio%20Lore%20and%20Babak%20Heydari&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20framework%20combining%20LLM%20agents%20as%20proxies%20for%0Ahuman%20strategic%20behavior%20with%20reinforcement%20learning%20%28RL%29%20to%20engage%20these%0Aagents%20in%20evolving%20strategic%20interactions%20within%20team%20environments.%20Our%0Aapproach%20extends%20traditional%20agent-based%20simulations%20by%20using%20strategic%20LLM%0Aagents%20%28SLA%29%20and%20introducing%20dynamic%20and%20adaptive%20governance%20through%20a%0Apro-social%20promoting%20RL%20agent%20%28PPA%29%20that%20modulates%20information%20access%20across%0Aagents%20in%20a%20network%2C%20optimizing%20social%20welfare%20and%20promoting%20pro-social%0Abehavior.%20Through%20validation%20in%20iterative%20games%2C%20including%20the%20prisoner%0Adilemma%2C%20we%20demonstrate%20that%20SLA%20agents%20exhibit%20nuanced%20strategic%20adaptations.%0AThe%20PPA%20agent%20effectively%20learns%20to%20adjust%20information%20transparency%2C%20resulting%0Ain%20enhanced%20cooperation%20rates.%20This%20framework%20offers%20significant%20insights%20into%0AAI-mediated%20social%20dynamics%2C%20contributing%20to%20the%20deployment%20of%20AI%20in%20real-world%0Ateam%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10372v3&entry.124074799=Read"},
{"title": "When can classical neural networks represent quantum states?", "author": "Tai-Hsuan Yang and Mehdi Soleimanifar and Thiago Bergamaschi and John Preskill", "abstract": "  A naive classical representation of an n-qubit state requires specifying\nexponentially many amplitudes in the computational basis. Past works have\ndemonstrated that classical neural networks can succinctly express these\namplitudes for many physically relevant states, leading to computationally\npowerful representations known as neural quantum states. What underpins the\nefficacy of such representations? We show that conditional correlations present\nin the measurement distribution of quantum states control the performance of\ntheir neural representations. Such conditional correlations are basis\ndependent, arise due to measurement-induced entanglement, and reveal features\nnot accessible through conventional few-body correlations often examined in\nstudies of phases of matter. By combining theoretical and numerical analysis,\nwe demonstrate how the state's entanglement and sign structure, along with the\nchoice of measurement basis, give rise to distinct patterns of short- or\nlong-range conditional correlations. Our findings provide a rigorous framework\nfor exploring the expressive power of neural quantum states.\n", "link": "http://arxiv.org/abs/2410.23152v1", "date": "2024-10-30", "relevancy": 1.5325, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4225}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.3776}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20can%20classical%20neural%20networks%20represent%20quantum%20states%3F&body=Title%3A%20When%20can%20classical%20neural%20networks%20represent%20quantum%20states%3F%0AAuthor%3A%20Tai-Hsuan%20Yang%20and%20Mehdi%20Soleimanifar%20and%20Thiago%20Bergamaschi%20and%20John%20Preskill%0AAbstract%3A%20%20%20A%20naive%20classical%20representation%20of%20an%20n-qubit%20state%20requires%20specifying%0Aexponentially%20many%20amplitudes%20in%20the%20computational%20basis.%20Past%20works%20have%0Ademonstrated%20that%20classical%20neural%20networks%20can%20succinctly%20express%20these%0Aamplitudes%20for%20many%20physically%20relevant%20states%2C%20leading%20to%20computationally%0Apowerful%20representations%20known%20as%20neural%20quantum%20states.%20What%20underpins%20the%0Aefficacy%20of%20such%20representations%3F%20We%20show%20that%20conditional%20correlations%20present%0Ain%20the%20measurement%20distribution%20of%20quantum%20states%20control%20the%20performance%20of%0Atheir%20neural%20representations.%20Such%20conditional%20correlations%20are%20basis%0Adependent%2C%20arise%20due%20to%20measurement-induced%20entanglement%2C%20and%20reveal%20features%0Anot%20accessible%20through%20conventional%20few-body%20correlations%20often%20examined%20in%0Astudies%20of%20phases%20of%20matter.%20By%20combining%20theoretical%20and%20numerical%20analysis%2C%0Awe%20demonstrate%20how%20the%20state%27s%20entanglement%20and%20sign%20structure%2C%20along%20with%20the%0Achoice%20of%20measurement%20basis%2C%20give%20rise%20to%20distinct%20patterns%20of%20short-%20or%0Along-range%20conditional%20correlations.%20Our%20findings%20provide%20a%20rigorous%20framework%0Afor%20exploring%20the%20expressive%20power%20of%20neural%20quantum%20states.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520can%2520classical%2520neural%2520networks%2520represent%2520quantum%2520states%253F%26entry.906535625%3DTai-Hsuan%2520Yang%2520and%2520Mehdi%2520Soleimanifar%2520and%2520Thiago%2520Bergamaschi%2520and%2520John%2520Preskill%26entry.1292438233%3D%2520%2520A%2520naive%2520classical%2520representation%2520of%2520an%2520n-qubit%2520state%2520requires%2520specifying%250Aexponentially%2520many%2520amplitudes%2520in%2520the%2520computational%2520basis.%2520Past%2520works%2520have%250Ademonstrated%2520that%2520classical%2520neural%2520networks%2520can%2520succinctly%2520express%2520these%250Aamplitudes%2520for%2520many%2520physically%2520relevant%2520states%252C%2520leading%2520to%2520computationally%250Apowerful%2520representations%2520known%2520as%2520neural%2520quantum%2520states.%2520What%2520underpins%2520the%250Aefficacy%2520of%2520such%2520representations%253F%2520We%2520show%2520that%2520conditional%2520correlations%2520present%250Ain%2520the%2520measurement%2520distribution%2520of%2520quantum%2520states%2520control%2520the%2520performance%2520of%250Atheir%2520neural%2520representations.%2520Such%2520conditional%2520correlations%2520are%2520basis%250Adependent%252C%2520arise%2520due%2520to%2520measurement-induced%2520entanglement%252C%2520and%2520reveal%2520features%250Anot%2520accessible%2520through%2520conventional%2520few-body%2520correlations%2520often%2520examined%2520in%250Astudies%2520of%2520phases%2520of%2520matter.%2520By%2520combining%2520theoretical%2520and%2520numerical%2520analysis%252C%250Awe%2520demonstrate%2520how%2520the%2520state%2527s%2520entanglement%2520and%2520sign%2520structure%252C%2520along%2520with%2520the%250Achoice%2520of%2520measurement%2520basis%252C%2520give%2520rise%2520to%2520distinct%2520patterns%2520of%2520short-%2520or%250Along-range%2520conditional%2520correlations.%2520Our%2520findings%2520provide%2520a%2520rigorous%2520framework%250Afor%2520exploring%2520the%2520expressive%2520power%2520of%2520neural%2520quantum%2520states.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20can%20classical%20neural%20networks%20represent%20quantum%20states%3F&entry.906535625=Tai-Hsuan%20Yang%20and%20Mehdi%20Soleimanifar%20and%20Thiago%20Bergamaschi%20and%20John%20Preskill&entry.1292438233=%20%20A%20naive%20classical%20representation%20of%20an%20n-qubit%20state%20requires%20specifying%0Aexponentially%20many%20amplitudes%20in%20the%20computational%20basis.%20Past%20works%20have%0Ademonstrated%20that%20classical%20neural%20networks%20can%20succinctly%20express%20these%0Aamplitudes%20for%20many%20physically%20relevant%20states%2C%20leading%20to%20computationally%0Apowerful%20representations%20known%20as%20neural%20quantum%20states.%20What%20underpins%20the%0Aefficacy%20of%20such%20representations%3F%20We%20show%20that%20conditional%20correlations%20present%0Ain%20the%20measurement%20distribution%20of%20quantum%20states%20control%20the%20performance%20of%0Atheir%20neural%20representations.%20Such%20conditional%20correlations%20are%20basis%0Adependent%2C%20arise%20due%20to%20measurement-induced%20entanglement%2C%20and%20reveal%20features%0Anot%20accessible%20through%20conventional%20few-body%20correlations%20often%20examined%20in%0Astudies%20of%20phases%20of%20matter.%20By%20combining%20theoretical%20and%20numerical%20analysis%2C%0Awe%20demonstrate%20how%20the%20state%27s%20entanglement%20and%20sign%20structure%2C%20along%20with%20the%0Achoice%20of%20measurement%20basis%2C%20give%20rise%20to%20distinct%20patterns%20of%20short-%20or%0Along-range%20conditional%20correlations.%20Our%20findings%20provide%20a%20rigorous%20framework%0Afor%20exploring%20the%20expressive%20power%20of%20neural%20quantum%20states.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23152v1&entry.124074799=Read"},
{"title": "A Neural Transformer Framework for Simultaneous Tasks of Segmentation,\n  Classification, and Caller Identification of Marmoset Vocalization", "author": "Bin Wu and Sakriani Sakti and Shinnosuke Takamichi and Satoshi Nakamura", "abstract": "  Marmoset, a highly vocalized primate, has become a popular animal model for\nstudying social-communicative behavior and its underlying mechanism. In the\nstudy of vocal communication, it is vital to know the caller identities, call\ncontents, and vocal exchanges. Previous work of a CNN has achieved a joint\nmodel for call segmentation, classification, and caller identification for\nmarmoset vocalizations. However, the CNN has limitations in modeling long-range\nacoustic patterns; the Transformer architecture that has been shown to\noutperform CNNs, utilizes the self-attention mechanism that efficiently\nsegregates information parallelly over long distances and captures the global\nstructure of marmoset vocalization. We propose using the Transformer to jointly\nsegment and classify the marmoset calls and identify the callers for each\nvocalization.\n", "link": "http://arxiv.org/abs/2410.23279v1", "date": "2024-10-30", "relevancy": 1.5285, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5315}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4985}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Neural%20Transformer%20Framework%20for%20Simultaneous%20Tasks%20of%20Segmentation%2C%0A%20%20Classification%2C%20and%20Caller%20Identification%20of%20Marmoset%20Vocalization&body=Title%3A%20A%20Neural%20Transformer%20Framework%20for%20Simultaneous%20Tasks%20of%20Segmentation%2C%0A%20%20Classification%2C%20and%20Caller%20Identification%20of%20Marmoset%20Vocalization%0AAuthor%3A%20Bin%20Wu%20and%20Sakriani%20Sakti%20and%20Shinnosuke%20Takamichi%20and%20Satoshi%20Nakamura%0AAbstract%3A%20%20%20Marmoset%2C%20a%20highly%20vocalized%20primate%2C%20has%20become%20a%20popular%20animal%20model%20for%0Astudying%20social-communicative%20behavior%20and%20its%20underlying%20mechanism.%20In%20the%0Astudy%20of%20vocal%20communication%2C%20it%20is%20vital%20to%20know%20the%20caller%20identities%2C%20call%0Acontents%2C%20and%20vocal%20exchanges.%20Previous%20work%20of%20a%20CNN%20has%20achieved%20a%20joint%0Amodel%20for%20call%20segmentation%2C%20classification%2C%20and%20caller%20identification%20for%0Amarmoset%20vocalizations.%20However%2C%20the%20CNN%20has%20limitations%20in%20modeling%20long-range%0Aacoustic%20patterns%3B%20the%20Transformer%20architecture%20that%20has%20been%20shown%20to%0Aoutperform%20CNNs%2C%20utilizes%20the%20self-attention%20mechanism%20that%20efficiently%0Asegregates%20information%20parallelly%20over%20long%20distances%20and%20captures%20the%20global%0Astructure%20of%20marmoset%20vocalization.%20We%20propose%20using%20the%20Transformer%20to%20jointly%0Asegment%20and%20classify%20the%20marmoset%20calls%20and%20identify%20the%20callers%20for%20each%0Avocalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Neural%2520Transformer%2520Framework%2520for%2520Simultaneous%2520Tasks%2520of%2520Segmentation%252C%250A%2520%2520Classification%252C%2520and%2520Caller%2520Identification%2520of%2520Marmoset%2520Vocalization%26entry.906535625%3DBin%2520Wu%2520and%2520Sakriani%2520Sakti%2520and%2520Shinnosuke%2520Takamichi%2520and%2520Satoshi%2520Nakamura%26entry.1292438233%3D%2520%2520Marmoset%252C%2520a%2520highly%2520vocalized%2520primate%252C%2520has%2520become%2520a%2520popular%2520animal%2520model%2520for%250Astudying%2520social-communicative%2520behavior%2520and%2520its%2520underlying%2520mechanism.%2520In%2520the%250Astudy%2520of%2520vocal%2520communication%252C%2520it%2520is%2520vital%2520to%2520know%2520the%2520caller%2520identities%252C%2520call%250Acontents%252C%2520and%2520vocal%2520exchanges.%2520Previous%2520work%2520of%2520a%2520CNN%2520has%2520achieved%2520a%2520joint%250Amodel%2520for%2520call%2520segmentation%252C%2520classification%252C%2520and%2520caller%2520identification%2520for%250Amarmoset%2520vocalizations.%2520However%252C%2520the%2520CNN%2520has%2520limitations%2520in%2520modeling%2520long-range%250Aacoustic%2520patterns%253B%2520the%2520Transformer%2520architecture%2520that%2520has%2520been%2520shown%2520to%250Aoutperform%2520CNNs%252C%2520utilizes%2520the%2520self-attention%2520mechanism%2520that%2520efficiently%250Asegregates%2520information%2520parallelly%2520over%2520long%2520distances%2520and%2520captures%2520the%2520global%250Astructure%2520of%2520marmoset%2520vocalization.%2520We%2520propose%2520using%2520the%2520Transformer%2520to%2520jointly%250Asegment%2520and%2520classify%2520the%2520marmoset%2520calls%2520and%2520identify%2520the%2520callers%2520for%2520each%250Avocalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Neural%20Transformer%20Framework%20for%20Simultaneous%20Tasks%20of%20Segmentation%2C%0A%20%20Classification%2C%20and%20Caller%20Identification%20of%20Marmoset%20Vocalization&entry.906535625=Bin%20Wu%20and%20Sakriani%20Sakti%20and%20Shinnosuke%20Takamichi%20and%20Satoshi%20Nakamura&entry.1292438233=%20%20Marmoset%2C%20a%20highly%20vocalized%20primate%2C%20has%20become%20a%20popular%20animal%20model%20for%0Astudying%20social-communicative%20behavior%20and%20its%20underlying%20mechanism.%20In%20the%0Astudy%20of%20vocal%20communication%2C%20it%20is%20vital%20to%20know%20the%20caller%20identities%2C%20call%0Acontents%2C%20and%20vocal%20exchanges.%20Previous%20work%20of%20a%20CNN%20has%20achieved%20a%20joint%0Amodel%20for%20call%20segmentation%2C%20classification%2C%20and%20caller%20identification%20for%0Amarmoset%20vocalizations.%20However%2C%20the%20CNN%20has%20limitations%20in%20modeling%20long-range%0Aacoustic%20patterns%3B%20the%20Transformer%20architecture%20that%20has%20been%20shown%20to%0Aoutperform%20CNNs%2C%20utilizes%20the%20self-attention%20mechanism%20that%20efficiently%0Asegregates%20information%20parallelly%20over%20long%20distances%20and%20captures%20the%20global%0Astructure%20of%20marmoset%20vocalization.%20We%20propose%20using%20the%20Transformer%20to%20jointly%0Asegment%20and%20classify%20the%20marmoset%20calls%20and%20identify%20the%20callers%20for%20each%0Avocalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23279v1&entry.124074799=Read"},
{"title": "Human Expertise in Algorithmic Prediction", "author": "Rohan Alur and Manish Raghavan and Devavrat Shah", "abstract": "  We introduce a novel framework for incorporating human expertise into\nalgorithmic predictions. Our approach leverages human judgment to distinguish\ninputs which are algorithmically indistinguishable, or \"look the same\" to\npredictive algorithms. We argue that this framing clarifies the problem of\nhuman-AI collaboration in prediction tasks, as experts often form judgments by\ndrawing on information which is not encoded in an algorithm's training data.\nAlgorithmic indistinguishability yields a natural test for assessing whether\nexperts incorporate this kind of \"side information\", and further provides a\nsimple but principled method for selectively incorporating human feedback into\nalgorithmic predictions. We show that this method provably improves the\nperformance of any feasible algorithmic predictor and precisely quantify this\nimprovement. We find empirically that although algorithms often outperform\ntheir human counterparts on average, human judgment can improve algorithmic\npredictions on specific instances (which can be identified ex-ante). In an\nX-ray classification task, we find that this subset constitutes nearly $30\\%$\nof the patient population. Our approach provides a natural way of uncovering\nthis heterogeneity and thus enabling effective human-AI collaboration.\n", "link": "http://arxiv.org/abs/2402.00793v3", "date": "2024-10-30", "relevancy": 1.5199, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5154}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5104}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20Expertise%20in%20Algorithmic%20Prediction&body=Title%3A%20Human%20Expertise%20in%20Algorithmic%20Prediction%0AAuthor%3A%20Rohan%20Alur%20and%20Manish%20Raghavan%20and%20Devavrat%20Shah%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20framework%20for%20incorporating%20human%20expertise%20into%0Aalgorithmic%20predictions.%20Our%20approach%20leverages%20human%20judgment%20to%20distinguish%0Ainputs%20which%20are%20algorithmically%20indistinguishable%2C%20or%20%22look%20the%20same%22%20to%0Apredictive%20algorithms.%20We%20argue%20that%20this%20framing%20clarifies%20the%20problem%20of%0Ahuman-AI%20collaboration%20in%20prediction%20tasks%2C%20as%20experts%20often%20form%20judgments%20by%0Adrawing%20on%20information%20which%20is%20not%20encoded%20in%20an%20algorithm%27s%20training%20data.%0AAlgorithmic%20indistinguishability%20yields%20a%20natural%20test%20for%20assessing%20whether%0Aexperts%20incorporate%20this%20kind%20of%20%22side%20information%22%2C%20and%20further%20provides%20a%0Asimple%20but%20principled%20method%20for%20selectively%20incorporating%20human%20feedback%20into%0Aalgorithmic%20predictions.%20We%20show%20that%20this%20method%20provably%20improves%20the%0Aperformance%20of%20any%20feasible%20algorithmic%20predictor%20and%20precisely%20quantify%20this%0Aimprovement.%20We%20find%20empirically%20that%20although%20algorithms%20often%20outperform%0Atheir%20human%20counterparts%20on%20average%2C%20human%20judgment%20can%20improve%20algorithmic%0Apredictions%20on%20specific%20instances%20%28which%20can%20be%20identified%20ex-ante%29.%20In%20an%0AX-ray%20classification%20task%2C%20we%20find%20that%20this%20subset%20constitutes%20nearly%20%2430%5C%25%24%0Aof%20the%20patient%20population.%20Our%20approach%20provides%20a%20natural%20way%20of%20uncovering%0Athis%20heterogeneity%20and%20thus%20enabling%20effective%20human-AI%20collaboration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00793v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520Expertise%2520in%2520Algorithmic%2520Prediction%26entry.906535625%3DRohan%2520Alur%2520and%2520Manish%2520Raghavan%2520and%2520Devavrat%2520Shah%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520framework%2520for%2520incorporating%2520human%2520expertise%2520into%250Aalgorithmic%2520predictions.%2520Our%2520approach%2520leverages%2520human%2520judgment%2520to%2520distinguish%250Ainputs%2520which%2520are%2520algorithmically%2520indistinguishable%252C%2520or%2520%2522look%2520the%2520same%2522%2520to%250Apredictive%2520algorithms.%2520We%2520argue%2520that%2520this%2520framing%2520clarifies%2520the%2520problem%2520of%250Ahuman-AI%2520collaboration%2520in%2520prediction%2520tasks%252C%2520as%2520experts%2520often%2520form%2520judgments%2520by%250Adrawing%2520on%2520information%2520which%2520is%2520not%2520encoded%2520in%2520an%2520algorithm%2527s%2520training%2520data.%250AAlgorithmic%2520indistinguishability%2520yields%2520a%2520natural%2520test%2520for%2520assessing%2520whether%250Aexperts%2520incorporate%2520this%2520kind%2520of%2520%2522side%2520information%2522%252C%2520and%2520further%2520provides%2520a%250Asimple%2520but%2520principled%2520method%2520for%2520selectively%2520incorporating%2520human%2520feedback%2520into%250Aalgorithmic%2520predictions.%2520We%2520show%2520that%2520this%2520method%2520provably%2520improves%2520the%250Aperformance%2520of%2520any%2520feasible%2520algorithmic%2520predictor%2520and%2520precisely%2520quantify%2520this%250Aimprovement.%2520We%2520find%2520empirically%2520that%2520although%2520algorithms%2520often%2520outperform%250Atheir%2520human%2520counterparts%2520on%2520average%252C%2520human%2520judgment%2520can%2520improve%2520algorithmic%250Apredictions%2520on%2520specific%2520instances%2520%2528which%2520can%2520be%2520identified%2520ex-ante%2529.%2520In%2520an%250AX-ray%2520classification%2520task%252C%2520we%2520find%2520that%2520this%2520subset%2520constitutes%2520nearly%2520%252430%255C%2525%2524%250Aof%2520the%2520patient%2520population.%2520Our%2520approach%2520provides%2520a%2520natural%2520way%2520of%2520uncovering%250Athis%2520heterogeneity%2520and%2520thus%2520enabling%2520effective%2520human-AI%2520collaboration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00793v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Expertise%20in%20Algorithmic%20Prediction&entry.906535625=Rohan%20Alur%20and%20Manish%20Raghavan%20and%20Devavrat%20Shah&entry.1292438233=%20%20We%20introduce%20a%20novel%20framework%20for%20incorporating%20human%20expertise%20into%0Aalgorithmic%20predictions.%20Our%20approach%20leverages%20human%20judgment%20to%20distinguish%0Ainputs%20which%20are%20algorithmically%20indistinguishable%2C%20or%20%22look%20the%20same%22%20to%0Apredictive%20algorithms.%20We%20argue%20that%20this%20framing%20clarifies%20the%20problem%20of%0Ahuman-AI%20collaboration%20in%20prediction%20tasks%2C%20as%20experts%20often%20form%20judgments%20by%0Adrawing%20on%20information%20which%20is%20not%20encoded%20in%20an%20algorithm%27s%20training%20data.%0AAlgorithmic%20indistinguishability%20yields%20a%20natural%20test%20for%20assessing%20whether%0Aexperts%20incorporate%20this%20kind%20of%20%22side%20information%22%2C%20and%20further%20provides%20a%0Asimple%20but%20principled%20method%20for%20selectively%20incorporating%20human%20feedback%20into%0Aalgorithmic%20predictions.%20We%20show%20that%20this%20method%20provably%20improves%20the%0Aperformance%20of%20any%20feasible%20algorithmic%20predictor%20and%20precisely%20quantify%20this%0Aimprovement.%20We%20find%20empirically%20that%20although%20algorithms%20often%20outperform%0Atheir%20human%20counterparts%20on%20average%2C%20human%20judgment%20can%20improve%20algorithmic%0Apredictions%20on%20specific%20instances%20%28which%20can%20be%20identified%20ex-ante%29.%20In%20an%0AX-ray%20classification%20task%2C%20we%20find%20that%20this%20subset%20constitutes%20nearly%20%2430%5C%25%24%0Aof%20the%20patient%20population.%20Our%20approach%20provides%20a%20natural%20way%20of%20uncovering%0Athis%20heterogeneity%20and%20thus%20enabling%20effective%20human-AI%20collaboration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00793v3&entry.124074799=Read"},
{"title": "Resource-aware Mixed-precision Quantization for Enhancing Deployability\n  of Transformers for Time-series Forecasting on Embedded FPGAs", "author": "Tianheng Ling and Chao Qian and Gregor Schiele", "abstract": "  This study addresses the deployment challenges of integer-only quantized\nTransformers on resource-constrained embedded FPGAs (Xilinx Spartan-7 XC7S15).\nWe enhanced the flexibility of our VHDL template by introducing a selectable\nresource type for storing intermediate results across model layers, thereby\nbreaking the deployment bottleneck by utilizing BRAM efficiently. Moreover, we\ndeveloped a resource-aware mixed-precision quantization approach that enables\nresearchers to explore hardware-level quantization strategies without requiring\nextensive expertise in Neural Architecture Search. This method provides\naccurate resource utilization estimates with a precision discrepancy as low as\n3%, compared to actual deployment metrics. Compared to previous work, our\napproach has successfully facilitated the deployment of model configurations\nutilizing mixed-precision quantization, thus overcoming the limitations\ninherent in five previously non-deployable configurations with uniform\nquantization bitwidths. Consequently, this research enhances the applicability\nof Transformers in embedded systems, facilitating a broader range of\nTransformer-powered applications on edge devices.\n", "link": "http://arxiv.org/abs/2410.03294v3", "date": "2024-10-30", "relevancy": 1.4873, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5137}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4999}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resource-aware%20Mixed-precision%20Quantization%20for%20Enhancing%20Deployability%0A%20%20of%20Transformers%20for%20Time-series%20Forecasting%20on%20Embedded%20FPGAs&body=Title%3A%20Resource-aware%20Mixed-precision%20Quantization%20for%20Enhancing%20Deployability%0A%20%20of%20Transformers%20for%20Time-series%20Forecasting%20on%20Embedded%20FPGAs%0AAuthor%3A%20Tianheng%20Ling%20and%20Chao%20Qian%20and%20Gregor%20Schiele%0AAbstract%3A%20%20%20This%20study%20addresses%20the%20deployment%20challenges%20of%20integer-only%20quantized%0ATransformers%20on%20resource-constrained%20embedded%20FPGAs%20%28Xilinx%20Spartan-7%20XC7S15%29.%0AWe%20enhanced%20the%20flexibility%20of%20our%20VHDL%20template%20by%20introducing%20a%20selectable%0Aresource%20type%20for%20storing%20intermediate%20results%20across%20model%20layers%2C%20thereby%0Abreaking%20the%20deployment%20bottleneck%20by%20utilizing%20BRAM%20efficiently.%20Moreover%2C%20we%0Adeveloped%20a%20resource-aware%20mixed-precision%20quantization%20approach%20that%20enables%0Aresearchers%20to%20explore%20hardware-level%20quantization%20strategies%20without%20requiring%0Aextensive%20expertise%20in%20Neural%20Architecture%20Search.%20This%20method%20provides%0Aaccurate%20resource%20utilization%20estimates%20with%20a%20precision%20discrepancy%20as%20low%20as%0A3%25%2C%20compared%20to%20actual%20deployment%20metrics.%20Compared%20to%20previous%20work%2C%20our%0Aapproach%20has%20successfully%20facilitated%20the%20deployment%20of%20model%20configurations%0Autilizing%20mixed-precision%20quantization%2C%20thus%20overcoming%20the%20limitations%0Ainherent%20in%20five%20previously%20non-deployable%20configurations%20with%20uniform%0Aquantization%20bitwidths.%20Consequently%2C%20this%20research%20enhances%20the%20applicability%0Aof%20Transformers%20in%20embedded%20systems%2C%20facilitating%20a%20broader%20range%20of%0ATransformer-powered%20applications%20on%20edge%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03294v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResource-aware%2520Mixed-precision%2520Quantization%2520for%2520Enhancing%2520Deployability%250A%2520%2520of%2520Transformers%2520for%2520Time-series%2520Forecasting%2520on%2520Embedded%2520FPGAs%26entry.906535625%3DTianheng%2520Ling%2520and%2520Chao%2520Qian%2520and%2520Gregor%2520Schiele%26entry.1292438233%3D%2520%2520This%2520study%2520addresses%2520the%2520deployment%2520challenges%2520of%2520integer-only%2520quantized%250ATransformers%2520on%2520resource-constrained%2520embedded%2520FPGAs%2520%2528Xilinx%2520Spartan-7%2520XC7S15%2529.%250AWe%2520enhanced%2520the%2520flexibility%2520of%2520our%2520VHDL%2520template%2520by%2520introducing%2520a%2520selectable%250Aresource%2520type%2520for%2520storing%2520intermediate%2520results%2520across%2520model%2520layers%252C%2520thereby%250Abreaking%2520the%2520deployment%2520bottleneck%2520by%2520utilizing%2520BRAM%2520efficiently.%2520Moreover%252C%2520we%250Adeveloped%2520a%2520resource-aware%2520mixed-precision%2520quantization%2520approach%2520that%2520enables%250Aresearchers%2520to%2520explore%2520hardware-level%2520quantization%2520strategies%2520without%2520requiring%250Aextensive%2520expertise%2520in%2520Neural%2520Architecture%2520Search.%2520This%2520method%2520provides%250Aaccurate%2520resource%2520utilization%2520estimates%2520with%2520a%2520precision%2520discrepancy%2520as%2520low%2520as%250A3%2525%252C%2520compared%2520to%2520actual%2520deployment%2520metrics.%2520Compared%2520to%2520previous%2520work%252C%2520our%250Aapproach%2520has%2520successfully%2520facilitated%2520the%2520deployment%2520of%2520model%2520configurations%250Autilizing%2520mixed-precision%2520quantization%252C%2520thus%2520overcoming%2520the%2520limitations%250Ainherent%2520in%2520five%2520previously%2520non-deployable%2520configurations%2520with%2520uniform%250Aquantization%2520bitwidths.%2520Consequently%252C%2520this%2520research%2520enhances%2520the%2520applicability%250Aof%2520Transformers%2520in%2520embedded%2520systems%252C%2520facilitating%2520a%2520broader%2520range%2520of%250ATransformer-powered%2520applications%2520on%2520edge%2520devices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03294v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resource-aware%20Mixed-precision%20Quantization%20for%20Enhancing%20Deployability%0A%20%20of%20Transformers%20for%20Time-series%20Forecasting%20on%20Embedded%20FPGAs&entry.906535625=Tianheng%20Ling%20and%20Chao%20Qian%20and%20Gregor%20Schiele&entry.1292438233=%20%20This%20study%20addresses%20the%20deployment%20challenges%20of%20integer-only%20quantized%0ATransformers%20on%20resource-constrained%20embedded%20FPGAs%20%28Xilinx%20Spartan-7%20XC7S15%29.%0AWe%20enhanced%20the%20flexibility%20of%20our%20VHDL%20template%20by%20introducing%20a%20selectable%0Aresource%20type%20for%20storing%20intermediate%20results%20across%20model%20layers%2C%20thereby%0Abreaking%20the%20deployment%20bottleneck%20by%20utilizing%20BRAM%20efficiently.%20Moreover%2C%20we%0Adeveloped%20a%20resource-aware%20mixed-precision%20quantization%20approach%20that%20enables%0Aresearchers%20to%20explore%20hardware-level%20quantization%20strategies%20without%20requiring%0Aextensive%20expertise%20in%20Neural%20Architecture%20Search.%20This%20method%20provides%0Aaccurate%20resource%20utilization%20estimates%20with%20a%20precision%20discrepancy%20as%20low%20as%0A3%25%2C%20compared%20to%20actual%20deployment%20metrics.%20Compared%20to%20previous%20work%2C%20our%0Aapproach%20has%20successfully%20facilitated%20the%20deployment%20of%20model%20configurations%0Autilizing%20mixed-precision%20quantization%2C%20thus%20overcoming%20the%20limitations%0Ainherent%20in%20five%20previously%20non-deployable%20configurations%20with%20uniform%0Aquantization%20bitwidths.%20Consequently%2C%20this%20research%20enhances%20the%20applicability%0Aof%20Transformers%20in%20embedded%20systems%2C%20facilitating%20a%20broader%20range%20of%0ATransformer-powered%20applications%20on%20edge%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03294v3&entry.124074799=Read"},
{"title": "A Theory of Synaptic Neural Balance: From Local to Global Order", "author": "Pierre Baldi and Antonios Alexos and Ian Domingo and Alireza Rahmansetayesh", "abstract": "  We develop a general theory of synaptic neural balance and how it can emerge\nor be enforced in neural networks. For a given regularizer, a neuron is said to\nbe in balance if the total cost of its input weights is equal to the total cost\nof its output weights. The basic example is provided by feedforward networks of\nReLU units trained with $L_2$ regularizers, which exhibit balance after proper\ntraining. The theory explains this phenomenon and extends it in several\ndirections. The first direction is the extension to bilinear and other\nactivation functions. The second direction is the extension to more general\nregularizers, including all $L_p$ regularizers. The third direction is the\nextension to non-layered architectures, recurrent architectures, convolutional\narchitectures, as well as architectures with mixed activation functions.\nGradient descent on the error function alone does not converge in general to a\nbalanced state, where every neuron is in balance, even when starting from a\nbalanced state. However, gradient descent on the regularized error function\nought to converge to a balanced state, and thus network balance can be used to\nassess learning progress. The theory is based on two local neuronal operations:\nscaling which is commutative, and balancing which is not commutative. Given any\ninitial set of weights, when local balancing operations are applied to each\nneuron in a stochastic manner, global order always emerges through the\nconvergence of the stochastic balancing algorithm to the same unique set of\nbalanced weights. The reason for this is the existence of an underlying\nstrictly convex optimization problem where the relevant variables are\nconstrained to a linear, only architecture-dependent, manifold. Simulations\nshow that balancing neurons prior to learning, or during learning in\nalternation with gradient descent steps, can improve learning speed and final\nperformance.\n", "link": "http://arxiv.org/abs/2405.09688v3", "date": "2024-10-30", "relevancy": 1.4672, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5012}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4893}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Theory%20of%20Synaptic%20Neural%20Balance%3A%20From%20Local%20to%20Global%20Order&body=Title%3A%20A%20Theory%20of%20Synaptic%20Neural%20Balance%3A%20From%20Local%20to%20Global%20Order%0AAuthor%3A%20Pierre%20Baldi%20and%20Antonios%20Alexos%20and%20Ian%20Domingo%20and%20Alireza%20Rahmansetayesh%0AAbstract%3A%20%20%20We%20develop%20a%20general%20theory%20of%20synaptic%20neural%20balance%20and%20how%20it%20can%20emerge%0Aor%20be%20enforced%20in%20neural%20networks.%20For%20a%20given%20regularizer%2C%20a%20neuron%20is%20said%20to%0Abe%20in%20balance%20if%20the%20total%20cost%20of%20its%20input%20weights%20is%20equal%20to%20the%20total%20cost%0Aof%20its%20output%20weights.%20The%20basic%20example%20is%20provided%20by%20feedforward%20networks%20of%0AReLU%20units%20trained%20with%20%24L_2%24%20regularizers%2C%20which%20exhibit%20balance%20after%20proper%0Atraining.%20The%20theory%20explains%20this%20phenomenon%20and%20extends%20it%20in%20several%0Adirections.%20The%20first%20direction%20is%20the%20extension%20to%20bilinear%20and%20other%0Aactivation%20functions.%20The%20second%20direction%20is%20the%20extension%20to%20more%20general%0Aregularizers%2C%20including%20all%20%24L_p%24%20regularizers.%20The%20third%20direction%20is%20the%0Aextension%20to%20non-layered%20architectures%2C%20recurrent%20architectures%2C%20convolutional%0Aarchitectures%2C%20as%20well%20as%20architectures%20with%20mixed%20activation%20functions.%0AGradient%20descent%20on%20the%20error%20function%20alone%20does%20not%20converge%20in%20general%20to%20a%0Abalanced%20state%2C%20where%20every%20neuron%20is%20in%20balance%2C%20even%20when%20starting%20from%20a%0Abalanced%20state.%20However%2C%20gradient%20descent%20on%20the%20regularized%20error%20function%0Aought%20to%20converge%20to%20a%20balanced%20state%2C%20and%20thus%20network%20balance%20can%20be%20used%20to%0Aassess%20learning%20progress.%20The%20theory%20is%20based%20on%20two%20local%20neuronal%20operations%3A%0Ascaling%20which%20is%20commutative%2C%20and%20balancing%20which%20is%20not%20commutative.%20Given%20any%0Ainitial%20set%20of%20weights%2C%20when%20local%20balancing%20operations%20are%20applied%20to%20each%0Aneuron%20in%20a%20stochastic%20manner%2C%20global%20order%20always%20emerges%20through%20the%0Aconvergence%20of%20the%20stochastic%20balancing%20algorithm%20to%20the%20same%20unique%20set%20of%0Abalanced%20weights.%20The%20reason%20for%20this%20is%20the%20existence%20of%20an%20underlying%0Astrictly%20convex%20optimization%20problem%20where%20the%20relevant%20variables%20are%0Aconstrained%20to%20a%20linear%2C%20only%20architecture-dependent%2C%20manifold.%20Simulations%0Ashow%20that%20balancing%20neurons%20prior%20to%20learning%2C%20or%20during%20learning%20in%0Aalternation%20with%20gradient%20descent%20steps%2C%20can%20improve%20learning%20speed%20and%20final%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09688v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Theory%2520of%2520Synaptic%2520Neural%2520Balance%253A%2520From%2520Local%2520to%2520Global%2520Order%26entry.906535625%3DPierre%2520Baldi%2520and%2520Antonios%2520Alexos%2520and%2520Ian%2520Domingo%2520and%2520Alireza%2520Rahmansetayesh%26entry.1292438233%3D%2520%2520We%2520develop%2520a%2520general%2520theory%2520of%2520synaptic%2520neural%2520balance%2520and%2520how%2520it%2520can%2520emerge%250Aor%2520be%2520enforced%2520in%2520neural%2520networks.%2520For%2520a%2520given%2520regularizer%252C%2520a%2520neuron%2520is%2520said%2520to%250Abe%2520in%2520balance%2520if%2520the%2520total%2520cost%2520of%2520its%2520input%2520weights%2520is%2520equal%2520to%2520the%2520total%2520cost%250Aof%2520its%2520output%2520weights.%2520The%2520basic%2520example%2520is%2520provided%2520by%2520feedforward%2520networks%2520of%250AReLU%2520units%2520trained%2520with%2520%2524L_2%2524%2520regularizers%252C%2520which%2520exhibit%2520balance%2520after%2520proper%250Atraining.%2520The%2520theory%2520explains%2520this%2520phenomenon%2520and%2520extends%2520it%2520in%2520several%250Adirections.%2520The%2520first%2520direction%2520is%2520the%2520extension%2520to%2520bilinear%2520and%2520other%250Aactivation%2520functions.%2520The%2520second%2520direction%2520is%2520the%2520extension%2520to%2520more%2520general%250Aregularizers%252C%2520including%2520all%2520%2524L_p%2524%2520regularizers.%2520The%2520third%2520direction%2520is%2520the%250Aextension%2520to%2520non-layered%2520architectures%252C%2520recurrent%2520architectures%252C%2520convolutional%250Aarchitectures%252C%2520as%2520well%2520as%2520architectures%2520with%2520mixed%2520activation%2520functions.%250AGradient%2520descent%2520on%2520the%2520error%2520function%2520alone%2520does%2520not%2520converge%2520in%2520general%2520to%2520a%250Abalanced%2520state%252C%2520where%2520every%2520neuron%2520is%2520in%2520balance%252C%2520even%2520when%2520starting%2520from%2520a%250Abalanced%2520state.%2520However%252C%2520gradient%2520descent%2520on%2520the%2520regularized%2520error%2520function%250Aought%2520to%2520converge%2520to%2520a%2520balanced%2520state%252C%2520and%2520thus%2520network%2520balance%2520can%2520be%2520used%2520to%250Aassess%2520learning%2520progress.%2520The%2520theory%2520is%2520based%2520on%2520two%2520local%2520neuronal%2520operations%253A%250Ascaling%2520which%2520is%2520commutative%252C%2520and%2520balancing%2520which%2520is%2520not%2520commutative.%2520Given%2520any%250Ainitial%2520set%2520of%2520weights%252C%2520when%2520local%2520balancing%2520operations%2520are%2520applied%2520to%2520each%250Aneuron%2520in%2520a%2520stochastic%2520manner%252C%2520global%2520order%2520always%2520emerges%2520through%2520the%250Aconvergence%2520of%2520the%2520stochastic%2520balancing%2520algorithm%2520to%2520the%2520same%2520unique%2520set%2520of%250Abalanced%2520weights.%2520The%2520reason%2520for%2520this%2520is%2520the%2520existence%2520of%2520an%2520underlying%250Astrictly%2520convex%2520optimization%2520problem%2520where%2520the%2520relevant%2520variables%2520are%250Aconstrained%2520to%2520a%2520linear%252C%2520only%2520architecture-dependent%252C%2520manifold.%2520Simulations%250Ashow%2520that%2520balancing%2520neurons%2520prior%2520to%2520learning%252C%2520or%2520during%2520learning%2520in%250Aalternation%2520with%2520gradient%2520descent%2520steps%252C%2520can%2520improve%2520learning%2520speed%2520and%2520final%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09688v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Theory%20of%20Synaptic%20Neural%20Balance%3A%20From%20Local%20to%20Global%20Order&entry.906535625=Pierre%20Baldi%20and%20Antonios%20Alexos%20and%20Ian%20Domingo%20and%20Alireza%20Rahmansetayesh&entry.1292438233=%20%20We%20develop%20a%20general%20theory%20of%20synaptic%20neural%20balance%20and%20how%20it%20can%20emerge%0Aor%20be%20enforced%20in%20neural%20networks.%20For%20a%20given%20regularizer%2C%20a%20neuron%20is%20said%20to%0Abe%20in%20balance%20if%20the%20total%20cost%20of%20its%20input%20weights%20is%20equal%20to%20the%20total%20cost%0Aof%20its%20output%20weights.%20The%20basic%20example%20is%20provided%20by%20feedforward%20networks%20of%0AReLU%20units%20trained%20with%20%24L_2%24%20regularizers%2C%20which%20exhibit%20balance%20after%20proper%0Atraining.%20The%20theory%20explains%20this%20phenomenon%20and%20extends%20it%20in%20several%0Adirections.%20The%20first%20direction%20is%20the%20extension%20to%20bilinear%20and%20other%0Aactivation%20functions.%20The%20second%20direction%20is%20the%20extension%20to%20more%20general%0Aregularizers%2C%20including%20all%20%24L_p%24%20regularizers.%20The%20third%20direction%20is%20the%0Aextension%20to%20non-layered%20architectures%2C%20recurrent%20architectures%2C%20convolutional%0Aarchitectures%2C%20as%20well%20as%20architectures%20with%20mixed%20activation%20functions.%0AGradient%20descent%20on%20the%20error%20function%20alone%20does%20not%20converge%20in%20general%20to%20a%0Abalanced%20state%2C%20where%20every%20neuron%20is%20in%20balance%2C%20even%20when%20starting%20from%20a%0Abalanced%20state.%20However%2C%20gradient%20descent%20on%20the%20regularized%20error%20function%0Aought%20to%20converge%20to%20a%20balanced%20state%2C%20and%20thus%20network%20balance%20can%20be%20used%20to%0Aassess%20learning%20progress.%20The%20theory%20is%20based%20on%20two%20local%20neuronal%20operations%3A%0Ascaling%20which%20is%20commutative%2C%20and%20balancing%20which%20is%20not%20commutative.%20Given%20any%0Ainitial%20set%20of%20weights%2C%20when%20local%20balancing%20operations%20are%20applied%20to%20each%0Aneuron%20in%20a%20stochastic%20manner%2C%20global%20order%20always%20emerges%20through%20the%0Aconvergence%20of%20the%20stochastic%20balancing%20algorithm%20to%20the%20same%20unique%20set%20of%0Abalanced%20weights.%20The%20reason%20for%20this%20is%20the%20existence%20of%20an%20underlying%0Astrictly%20convex%20optimization%20problem%20where%20the%20relevant%20variables%20are%0Aconstrained%20to%20a%20linear%2C%20only%20architecture-dependent%2C%20manifold.%20Simulations%0Ashow%20that%20balancing%20neurons%20prior%20to%20learning%2C%20or%20during%20learning%20in%0Aalternation%20with%20gradient%20descent%20steps%2C%20can%20improve%20learning%20speed%20and%20final%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09688v3&entry.124074799=Read"},
{"title": "Certification for Differentially Private Prediction in Gradient-Based\n  Training", "author": "Matthew Wicker and Philip Sosnin and Igor Shilov and Adrianna Janik and Mark N. M\u00fcller and Yves-Alexandre de Montjoye and Adrian Weller and Calvin Tsay", "abstract": "  Differential privacy upper-bounds the information leakage of machine learning\nmodels, yet providing meaningful privacy guarantees has proven to be\nchallenging in practice. The private prediction setting where model outputs are\nprivatized is being investigated as an alternate way to provide formal\nguarantees at prediction time. Most current private prediction algorithms,\nhowever, rely on global sensitivity for noise calibration, which often results\nin large amounts of noise being added to the predictions. Data-specific noise\ncalibration, such as smooth sensitivity, could significantly reduce the amount\nof noise added, but were so far infeasible to compute exactly for modern\nmachine learning models. In this work we provide a novel and practical approach\nbased on convex relaxation and bound propagation to compute a provable\nupper-bound for the local and smooth sensitivity of a prediction. This bound\nallows us to reduce the magnitude of noise added or improve privacy accounting\nin the private prediction setting. We validate our framework on datasets from\nfinancial services, medical image classification, and natural language\nprocessing and across models and find our approach to reduce the noise added by\nup to order of magnitude.\n", "link": "http://arxiv.org/abs/2406.13433v2", "date": "2024-10-30", "relevancy": 1.4457, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.485}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4789}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Certification%20for%20Differentially%20Private%20Prediction%20in%20Gradient-Based%0A%20%20Training&body=Title%3A%20Certification%20for%20Differentially%20Private%20Prediction%20in%20Gradient-Based%0A%20%20Training%0AAuthor%3A%20Matthew%20Wicker%20and%20Philip%20Sosnin%20and%20Igor%20Shilov%20and%20Adrianna%20Janik%20and%20Mark%20N.%20M%C3%BCller%20and%20Yves-Alexandre%20de%20Montjoye%20and%20Adrian%20Weller%20and%20Calvin%20Tsay%0AAbstract%3A%20%20%20Differential%20privacy%20upper-bounds%20the%20information%20leakage%20of%20machine%20learning%0Amodels%2C%20yet%20providing%20meaningful%20privacy%20guarantees%20has%20proven%20to%20be%0Achallenging%20in%20practice.%20The%20private%20prediction%20setting%20where%20model%20outputs%20are%0Aprivatized%20is%20being%20investigated%20as%20an%20alternate%20way%20to%20provide%20formal%0Aguarantees%20at%20prediction%20time.%20Most%20current%20private%20prediction%20algorithms%2C%0Ahowever%2C%20rely%20on%20global%20sensitivity%20for%20noise%20calibration%2C%20which%20often%20results%0Ain%20large%20amounts%20of%20noise%20being%20added%20to%20the%20predictions.%20Data-specific%20noise%0Acalibration%2C%20such%20as%20smooth%20sensitivity%2C%20could%20significantly%20reduce%20the%20amount%0Aof%20noise%20added%2C%20but%20were%20so%20far%20infeasible%20to%20compute%20exactly%20for%20modern%0Amachine%20learning%20models.%20In%20this%20work%20we%20provide%20a%20novel%20and%20practical%20approach%0Abased%20on%20convex%20relaxation%20and%20bound%20propagation%20to%20compute%20a%20provable%0Aupper-bound%20for%20the%20local%20and%20smooth%20sensitivity%20of%20a%20prediction.%20This%20bound%0Aallows%20us%20to%20reduce%20the%20magnitude%20of%20noise%20added%20or%20improve%20privacy%20accounting%0Ain%20the%20private%20prediction%20setting.%20We%20validate%20our%20framework%20on%20datasets%20from%0Afinancial%20services%2C%20medical%20image%20classification%2C%20and%20natural%20language%0Aprocessing%20and%20across%20models%20and%20find%20our%20approach%20to%20reduce%20the%20noise%20added%20by%0Aup%20to%20order%20of%20magnitude.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13433v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCertification%2520for%2520Differentially%2520Private%2520Prediction%2520in%2520Gradient-Based%250A%2520%2520Training%26entry.906535625%3DMatthew%2520Wicker%2520and%2520Philip%2520Sosnin%2520and%2520Igor%2520Shilov%2520and%2520Adrianna%2520Janik%2520and%2520Mark%2520N.%2520M%25C3%25BCller%2520and%2520Yves-Alexandre%2520de%2520Montjoye%2520and%2520Adrian%2520Weller%2520and%2520Calvin%2520Tsay%26entry.1292438233%3D%2520%2520Differential%2520privacy%2520upper-bounds%2520the%2520information%2520leakage%2520of%2520machine%2520learning%250Amodels%252C%2520yet%2520providing%2520meaningful%2520privacy%2520guarantees%2520has%2520proven%2520to%2520be%250Achallenging%2520in%2520practice.%2520The%2520private%2520prediction%2520setting%2520where%2520model%2520outputs%2520are%250Aprivatized%2520is%2520being%2520investigated%2520as%2520an%2520alternate%2520way%2520to%2520provide%2520formal%250Aguarantees%2520at%2520prediction%2520time.%2520Most%2520current%2520private%2520prediction%2520algorithms%252C%250Ahowever%252C%2520rely%2520on%2520global%2520sensitivity%2520for%2520noise%2520calibration%252C%2520which%2520often%2520results%250Ain%2520large%2520amounts%2520of%2520noise%2520being%2520added%2520to%2520the%2520predictions.%2520Data-specific%2520noise%250Acalibration%252C%2520such%2520as%2520smooth%2520sensitivity%252C%2520could%2520significantly%2520reduce%2520the%2520amount%250Aof%2520noise%2520added%252C%2520but%2520were%2520so%2520far%2520infeasible%2520to%2520compute%2520exactly%2520for%2520modern%250Amachine%2520learning%2520models.%2520In%2520this%2520work%2520we%2520provide%2520a%2520novel%2520and%2520practical%2520approach%250Abased%2520on%2520convex%2520relaxation%2520and%2520bound%2520propagation%2520to%2520compute%2520a%2520provable%250Aupper-bound%2520for%2520the%2520local%2520and%2520smooth%2520sensitivity%2520of%2520a%2520prediction.%2520This%2520bound%250Aallows%2520us%2520to%2520reduce%2520the%2520magnitude%2520of%2520noise%2520added%2520or%2520improve%2520privacy%2520accounting%250Ain%2520the%2520private%2520prediction%2520setting.%2520We%2520validate%2520our%2520framework%2520on%2520datasets%2520from%250Afinancial%2520services%252C%2520medical%2520image%2520classification%252C%2520and%2520natural%2520language%250Aprocessing%2520and%2520across%2520models%2520and%2520find%2520our%2520approach%2520to%2520reduce%2520the%2520noise%2520added%2520by%250Aup%2520to%2520order%2520of%2520magnitude.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13433v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Certification%20for%20Differentially%20Private%20Prediction%20in%20Gradient-Based%0A%20%20Training&entry.906535625=Matthew%20Wicker%20and%20Philip%20Sosnin%20and%20Igor%20Shilov%20and%20Adrianna%20Janik%20and%20Mark%20N.%20M%C3%BCller%20and%20Yves-Alexandre%20de%20Montjoye%20and%20Adrian%20Weller%20and%20Calvin%20Tsay&entry.1292438233=%20%20Differential%20privacy%20upper-bounds%20the%20information%20leakage%20of%20machine%20learning%0Amodels%2C%20yet%20providing%20meaningful%20privacy%20guarantees%20has%20proven%20to%20be%0Achallenging%20in%20practice.%20The%20private%20prediction%20setting%20where%20model%20outputs%20are%0Aprivatized%20is%20being%20investigated%20as%20an%20alternate%20way%20to%20provide%20formal%0Aguarantees%20at%20prediction%20time.%20Most%20current%20private%20prediction%20algorithms%2C%0Ahowever%2C%20rely%20on%20global%20sensitivity%20for%20noise%20calibration%2C%20which%20often%20results%0Ain%20large%20amounts%20of%20noise%20being%20added%20to%20the%20predictions.%20Data-specific%20noise%0Acalibration%2C%20such%20as%20smooth%20sensitivity%2C%20could%20significantly%20reduce%20the%20amount%0Aof%20noise%20added%2C%20but%20were%20so%20far%20infeasible%20to%20compute%20exactly%20for%20modern%0Amachine%20learning%20models.%20In%20this%20work%20we%20provide%20a%20novel%20and%20practical%20approach%0Abased%20on%20convex%20relaxation%20and%20bound%20propagation%20to%20compute%20a%20provable%0Aupper-bound%20for%20the%20local%20and%20smooth%20sensitivity%20of%20a%20prediction.%20This%20bound%0Aallows%20us%20to%20reduce%20the%20magnitude%20of%20noise%20added%20or%20improve%20privacy%20accounting%0Ain%20the%20private%20prediction%20setting.%20We%20validate%20our%20framework%20on%20datasets%20from%0Afinancial%20services%2C%20medical%20image%20classification%2C%20and%20natural%20language%0Aprocessing%20and%20across%20models%20and%20find%20our%20approach%20to%20reduce%20the%20noise%20added%20by%0Aup%20to%20order%20of%20magnitude.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13433v2&entry.124074799=Read"},
{"title": "FlexTSF: A Universal Forecasting Model for Time Series with Variable\n  Regularities", "author": "Jingge Xiao and Yile Chen and Gao Cong and Wolfgang Nejdl and Simon Gottschalk", "abstract": "  Developing a foundation model for time series forecasting across diverse\ndomains has attracted significant attention in recent years. Existing works\ntypically assume regularly sampled, well-structured data, limiting their\napplicability to more generalized scenarios where time series often contain\nmissing values, unequal sequence lengths, and irregular time intervals between\nmeasurements. To cover diverse domains and handle variable regularities, we\npropose FlexTSF, a universal time series forecasting model that possesses\nbetter generalization and natively support both regular and irregular time\nseries. FlexTSF produces forecasts in an autoregressive manner and incorporates\nthree novel designs: VT-Norm, a normalization strategy to ablate data domain\nbarriers, IVP Patcher, a patching module to learn representations from flexibly\nstructured time series, and LED attention, an attention mechanism to seamlessly\nintegrate these two and propagate forecasts with awareness of domain and time\ninformation. Experiments on 12 datasets show that FlexTSF outperforms\nstate-of-the-art forecasting models respectively designed for regular and\nirregular time series. Furthermore, after self-supervised pre-training, FlexTSF\nshows exceptional performance in both zero-shot and few-show settings for time\nseries forecasting.\n", "link": "http://arxiv.org/abs/2410.23160v1", "date": "2024-10-30", "relevancy": 1.4369, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4858}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4787}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexTSF%3A%20A%20Universal%20Forecasting%20Model%20for%20Time%20Series%20with%20Variable%0A%20%20Regularities&body=Title%3A%20FlexTSF%3A%20A%20Universal%20Forecasting%20Model%20for%20Time%20Series%20with%20Variable%0A%20%20Regularities%0AAuthor%3A%20Jingge%20Xiao%20and%20Yile%20Chen%20and%20Gao%20Cong%20and%20Wolfgang%20Nejdl%20and%20Simon%20Gottschalk%0AAbstract%3A%20%20%20Developing%20a%20foundation%20model%20for%20time%20series%20forecasting%20across%20diverse%0Adomains%20has%20attracted%20significant%20attention%20in%20recent%20years.%20Existing%20works%0Atypically%20assume%20regularly%20sampled%2C%20well-structured%20data%2C%20limiting%20their%0Aapplicability%20to%20more%20generalized%20scenarios%20where%20time%20series%20often%20contain%0Amissing%20values%2C%20unequal%20sequence%20lengths%2C%20and%20irregular%20time%20intervals%20between%0Ameasurements.%20To%20cover%20diverse%20domains%20and%20handle%20variable%20regularities%2C%20we%0Apropose%20FlexTSF%2C%20a%20universal%20time%20series%20forecasting%20model%20that%20possesses%0Abetter%20generalization%20and%20natively%20support%20both%20regular%20and%20irregular%20time%0Aseries.%20FlexTSF%20produces%20forecasts%20in%20an%20autoregressive%20manner%20and%20incorporates%0Athree%20novel%20designs%3A%20VT-Norm%2C%20a%20normalization%20strategy%20to%20ablate%20data%20domain%0Abarriers%2C%20IVP%20Patcher%2C%20a%20patching%20module%20to%20learn%20representations%20from%20flexibly%0Astructured%20time%20series%2C%20and%20LED%20attention%2C%20an%20attention%20mechanism%20to%20seamlessly%0Aintegrate%20these%20two%20and%20propagate%20forecasts%20with%20awareness%20of%20domain%20and%20time%0Ainformation.%20Experiments%20on%2012%20datasets%20show%20that%20FlexTSF%20outperforms%0Astate-of-the-art%20forecasting%20models%20respectively%20designed%20for%20regular%20and%0Airregular%20time%20series.%20Furthermore%2C%20after%20self-supervised%20pre-training%2C%20FlexTSF%0Ashows%20exceptional%20performance%20in%20both%20zero-shot%20and%20few-show%20settings%20for%20time%0Aseries%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23160v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexTSF%253A%2520A%2520Universal%2520Forecasting%2520Model%2520for%2520Time%2520Series%2520with%2520Variable%250A%2520%2520Regularities%26entry.906535625%3DJingge%2520Xiao%2520and%2520Yile%2520Chen%2520and%2520Gao%2520Cong%2520and%2520Wolfgang%2520Nejdl%2520and%2520Simon%2520Gottschalk%26entry.1292438233%3D%2520%2520Developing%2520a%2520foundation%2520model%2520for%2520time%2520series%2520forecasting%2520across%2520diverse%250Adomains%2520has%2520attracted%2520significant%2520attention%2520in%2520recent%2520years.%2520Existing%2520works%250Atypically%2520assume%2520regularly%2520sampled%252C%2520well-structured%2520data%252C%2520limiting%2520their%250Aapplicability%2520to%2520more%2520generalized%2520scenarios%2520where%2520time%2520series%2520often%2520contain%250Amissing%2520values%252C%2520unequal%2520sequence%2520lengths%252C%2520and%2520irregular%2520time%2520intervals%2520between%250Ameasurements.%2520To%2520cover%2520diverse%2520domains%2520and%2520handle%2520variable%2520regularities%252C%2520we%250Apropose%2520FlexTSF%252C%2520a%2520universal%2520time%2520series%2520forecasting%2520model%2520that%2520possesses%250Abetter%2520generalization%2520and%2520natively%2520support%2520both%2520regular%2520and%2520irregular%2520time%250Aseries.%2520FlexTSF%2520produces%2520forecasts%2520in%2520an%2520autoregressive%2520manner%2520and%2520incorporates%250Athree%2520novel%2520designs%253A%2520VT-Norm%252C%2520a%2520normalization%2520strategy%2520to%2520ablate%2520data%2520domain%250Abarriers%252C%2520IVP%2520Patcher%252C%2520a%2520patching%2520module%2520to%2520learn%2520representations%2520from%2520flexibly%250Astructured%2520time%2520series%252C%2520and%2520LED%2520attention%252C%2520an%2520attention%2520mechanism%2520to%2520seamlessly%250Aintegrate%2520these%2520two%2520and%2520propagate%2520forecasts%2520with%2520awareness%2520of%2520domain%2520and%2520time%250Ainformation.%2520Experiments%2520on%252012%2520datasets%2520show%2520that%2520FlexTSF%2520outperforms%250Astate-of-the-art%2520forecasting%2520models%2520respectively%2520designed%2520for%2520regular%2520and%250Airregular%2520time%2520series.%2520Furthermore%252C%2520after%2520self-supervised%2520pre-training%252C%2520FlexTSF%250Ashows%2520exceptional%2520performance%2520in%2520both%2520zero-shot%2520and%2520few-show%2520settings%2520for%2520time%250Aseries%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23160v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexTSF%3A%20A%20Universal%20Forecasting%20Model%20for%20Time%20Series%20with%20Variable%0A%20%20Regularities&entry.906535625=Jingge%20Xiao%20and%20Yile%20Chen%20and%20Gao%20Cong%20and%20Wolfgang%20Nejdl%20and%20Simon%20Gottschalk&entry.1292438233=%20%20Developing%20a%20foundation%20model%20for%20time%20series%20forecasting%20across%20diverse%0Adomains%20has%20attracted%20significant%20attention%20in%20recent%20years.%20Existing%20works%0Atypically%20assume%20regularly%20sampled%2C%20well-structured%20data%2C%20limiting%20their%0Aapplicability%20to%20more%20generalized%20scenarios%20where%20time%20series%20often%20contain%0Amissing%20values%2C%20unequal%20sequence%20lengths%2C%20and%20irregular%20time%20intervals%20between%0Ameasurements.%20To%20cover%20diverse%20domains%20and%20handle%20variable%20regularities%2C%20we%0Apropose%20FlexTSF%2C%20a%20universal%20time%20series%20forecasting%20model%20that%20possesses%0Abetter%20generalization%20and%20natively%20support%20both%20regular%20and%20irregular%20time%0Aseries.%20FlexTSF%20produces%20forecasts%20in%20an%20autoregressive%20manner%20and%20incorporates%0Athree%20novel%20designs%3A%20VT-Norm%2C%20a%20normalization%20strategy%20to%20ablate%20data%20domain%0Abarriers%2C%20IVP%20Patcher%2C%20a%20patching%20module%20to%20learn%20representations%20from%20flexibly%0Astructured%20time%20series%2C%20and%20LED%20attention%2C%20an%20attention%20mechanism%20to%20seamlessly%0Aintegrate%20these%20two%20and%20propagate%20forecasts%20with%20awareness%20of%20domain%20and%20time%0Ainformation.%20Experiments%20on%2012%20datasets%20show%20that%20FlexTSF%20outperforms%0Astate-of-the-art%20forecasting%20models%20respectively%20designed%20for%20regular%20and%0Airregular%20time%20series.%20Furthermore%2C%20after%20self-supervised%20pre-training%2C%20FlexTSF%0Ashows%20exceptional%20performance%20in%20both%20zero-shot%20and%20few-show%20settings%20for%20time%0Aseries%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23160v1&entry.124074799=Read"},
{"title": "ReasoningRec: Bridging Personalized Recommendations and\n  Human-Interpretable Explanations through LLM Reasoning", "author": "Millennium Bismay and Xiangjue Dong and James Caverlee", "abstract": "  This paper presents ReasoningRec, a reasoning-based recommendation framework\nthat leverages Large Language Models (LLMs) to bridge the gap between\nrecommendations and human-interpretable explanations. In contrast to\nconventional recommendation systems that rely on implicit user-item\ninteractions, ReasoningRec employs LLMs to model users and items, focusing on\npreferences, aversions, and explanatory reasoning. The framework utilizes a\nlarger LLM to generate synthetic explanations for user preferences,\nsubsequently used to fine-tune a smaller LLM for enhanced recommendation\naccuracy and human-interpretable explanation. Our experimental study\ninvestigates the impact of reasoning and contextual information on personalized\nrecommendations, revealing that the quality of contextual and personalized data\nsignificantly influences the LLM's capacity to generate plausible explanations.\nEmpirical evaluations demonstrate that ReasoningRec surpasses state-of-the-art\nmethods by up to 12.5\\% in recommendation prediction while concurrently\nproviding human-intelligible explanations. The code is available here:\nhttps://github.com/millenniumbismay/reasoningrec.\n", "link": "http://arxiv.org/abs/2410.23180v1", "date": "2024-10-30", "relevancy": 1.426, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4997}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4686}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReasoningRec%3A%20Bridging%20Personalized%20Recommendations%20and%0A%20%20Human-Interpretable%20Explanations%20through%20LLM%20Reasoning&body=Title%3A%20ReasoningRec%3A%20Bridging%20Personalized%20Recommendations%20and%0A%20%20Human-Interpretable%20Explanations%20through%20LLM%20Reasoning%0AAuthor%3A%20Millennium%20Bismay%20and%20Xiangjue%20Dong%20and%20James%20Caverlee%0AAbstract%3A%20%20%20This%20paper%20presents%20ReasoningRec%2C%20a%20reasoning-based%20recommendation%20framework%0Athat%20leverages%20Large%20Language%20Models%20%28LLMs%29%20to%20bridge%20the%20gap%20between%0Arecommendations%20and%20human-interpretable%20explanations.%20In%20contrast%20to%0Aconventional%20recommendation%20systems%20that%20rely%20on%20implicit%20user-item%0Ainteractions%2C%20ReasoningRec%20employs%20LLMs%20to%20model%20users%20and%20items%2C%20focusing%20on%0Apreferences%2C%20aversions%2C%20and%20explanatory%20reasoning.%20The%20framework%20utilizes%20a%0Alarger%20LLM%20to%20generate%20synthetic%20explanations%20for%20user%20preferences%2C%0Asubsequently%20used%20to%20fine-tune%20a%20smaller%20LLM%20for%20enhanced%20recommendation%0Aaccuracy%20and%20human-interpretable%20explanation.%20Our%20experimental%20study%0Ainvestigates%20the%20impact%20of%20reasoning%20and%20contextual%20information%20on%20personalized%0Arecommendations%2C%20revealing%20that%20the%20quality%20of%20contextual%20and%20personalized%20data%0Asignificantly%20influences%20the%20LLM%27s%20capacity%20to%20generate%20plausible%20explanations.%0AEmpirical%20evaluations%20demonstrate%20that%20ReasoningRec%20surpasses%20state-of-the-art%0Amethods%20by%20up%20to%2012.5%5C%25%20in%20recommendation%20prediction%20while%20concurrently%0Aproviding%20human-intelligible%20explanations.%20The%20code%20is%20available%20here%3A%0Ahttps%3A//github.com/millenniumbismay/reasoningrec.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoningRec%253A%2520Bridging%2520Personalized%2520Recommendations%2520and%250A%2520%2520Human-Interpretable%2520Explanations%2520through%2520LLM%2520Reasoning%26entry.906535625%3DMillennium%2520Bismay%2520and%2520Xiangjue%2520Dong%2520and%2520James%2520Caverlee%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520ReasoningRec%252C%2520a%2520reasoning-based%2520recommendation%2520framework%250Athat%2520leverages%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520bridge%2520the%2520gap%2520between%250Arecommendations%2520and%2520human-interpretable%2520explanations.%2520In%2520contrast%2520to%250Aconventional%2520recommendation%2520systems%2520that%2520rely%2520on%2520implicit%2520user-item%250Ainteractions%252C%2520ReasoningRec%2520employs%2520LLMs%2520to%2520model%2520users%2520and%2520items%252C%2520focusing%2520on%250Apreferences%252C%2520aversions%252C%2520and%2520explanatory%2520reasoning.%2520The%2520framework%2520utilizes%2520a%250Alarger%2520LLM%2520to%2520generate%2520synthetic%2520explanations%2520for%2520user%2520preferences%252C%250Asubsequently%2520used%2520to%2520fine-tune%2520a%2520smaller%2520LLM%2520for%2520enhanced%2520recommendation%250Aaccuracy%2520and%2520human-interpretable%2520explanation.%2520Our%2520experimental%2520study%250Ainvestigates%2520the%2520impact%2520of%2520reasoning%2520and%2520contextual%2520information%2520on%2520personalized%250Arecommendations%252C%2520revealing%2520that%2520the%2520quality%2520of%2520contextual%2520and%2520personalized%2520data%250Asignificantly%2520influences%2520the%2520LLM%2527s%2520capacity%2520to%2520generate%2520plausible%2520explanations.%250AEmpirical%2520evaluations%2520demonstrate%2520that%2520ReasoningRec%2520surpasses%2520state-of-the-art%250Amethods%2520by%2520up%2520to%252012.5%255C%2525%2520in%2520recommendation%2520prediction%2520while%2520concurrently%250Aproviding%2520human-intelligible%2520explanations.%2520The%2520code%2520is%2520available%2520here%253A%250Ahttps%253A//github.com/millenniumbismay/reasoningrec.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReasoningRec%3A%20Bridging%20Personalized%20Recommendations%20and%0A%20%20Human-Interpretable%20Explanations%20through%20LLM%20Reasoning&entry.906535625=Millennium%20Bismay%20and%20Xiangjue%20Dong%20and%20James%20Caverlee&entry.1292438233=%20%20This%20paper%20presents%20ReasoningRec%2C%20a%20reasoning-based%20recommendation%20framework%0Athat%20leverages%20Large%20Language%20Models%20%28LLMs%29%20to%20bridge%20the%20gap%20between%0Arecommendations%20and%20human-interpretable%20explanations.%20In%20contrast%20to%0Aconventional%20recommendation%20systems%20that%20rely%20on%20implicit%20user-item%0Ainteractions%2C%20ReasoningRec%20employs%20LLMs%20to%20model%20users%20and%20items%2C%20focusing%20on%0Apreferences%2C%20aversions%2C%20and%20explanatory%20reasoning.%20The%20framework%20utilizes%20a%0Alarger%20LLM%20to%20generate%20synthetic%20explanations%20for%20user%20preferences%2C%0Asubsequently%20used%20to%20fine-tune%20a%20smaller%20LLM%20for%20enhanced%20recommendation%0Aaccuracy%20and%20human-interpretable%20explanation.%20Our%20experimental%20study%0Ainvestigates%20the%20impact%20of%20reasoning%20and%20contextual%20information%20on%20personalized%0Arecommendations%2C%20revealing%20that%20the%20quality%20of%20contextual%20and%20personalized%20data%0Asignificantly%20influences%20the%20LLM%27s%20capacity%20to%20generate%20plausible%20explanations.%0AEmpirical%20evaluations%20demonstrate%20that%20ReasoningRec%20surpasses%20state-of-the-art%0Amethods%20by%20up%20to%2012.5%5C%25%20in%20recommendation%20prediction%20while%20concurrently%0Aproviding%20human-intelligible%20explanations.%20The%20code%20is%20available%20here%3A%0Ahttps%3A//github.com/millenniumbismay/reasoningrec.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23180v1&entry.124074799=Read"},
{"title": "Kronecker-Factored Approximate Curvature for Physics-Informed Neural\n  Networks", "author": "Felix Dangel and Johannes M\u00fcller and Marius Zeinhofer", "abstract": "  Physics-informed neural networks (PINNs) are infamous for being hard to\ntrain. Recently, second-order methods based on natural gradient and\nGauss-Newton methods have shown promising performance, improving the accuracy\nachieved by first-order methods by several orders of magnitude. While\npromising, the proposed methods only scale to networks with a few thousand\nparameters due to the high computational cost to evaluate, store, and invert\nthe curvature matrix. We propose Kronecker-factored approximate curvature\n(KFAC) for PINN losses that greatly reduces the computational cost and allows\nscaling to much larger networks. Our approach goes beyond the established KFAC\nfor traditional deep learning problems as it captures contributions from a\nPDE's differential operator that are crucial for optimization. To establish\nKFAC for such losses, we use Taylor-mode automatic differentiation to describe\nthe differential operator's computation graph as a forward network with shared\nweights. This allows us to apply KFAC thanks to a recently-developed general\nformulation for networks with weight sharing. Empirically, we find that our\nKFAC-based optimizers are competitive with expensive second-order methods on\nsmall problems, scale more favorably to higher-dimensional neural networks and\nPDEs, and consistently outperform first-order methods and LBFGS.\n", "link": "http://arxiv.org/abs/2405.15603v3", "date": "2024-10-30", "relevancy": 1.4191, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4771}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4698}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kronecker-Factored%20Approximate%20Curvature%20for%20Physics-Informed%20Neural%0A%20%20Networks&body=Title%3A%20Kronecker-Factored%20Approximate%20Curvature%20for%20Physics-Informed%20Neural%0A%20%20Networks%0AAuthor%3A%20Felix%20Dangel%20and%20Johannes%20M%C3%BCller%20and%20Marius%20Zeinhofer%0AAbstract%3A%20%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20are%20infamous%20for%20being%20hard%20to%0Atrain.%20Recently%2C%20second-order%20methods%20based%20on%20natural%20gradient%20and%0AGauss-Newton%20methods%20have%20shown%20promising%20performance%2C%20improving%20the%20accuracy%0Aachieved%20by%20first-order%20methods%20by%20several%20orders%20of%20magnitude.%20While%0Apromising%2C%20the%20proposed%20methods%20only%20scale%20to%20networks%20with%20a%20few%20thousand%0Aparameters%20due%20to%20the%20high%20computational%20cost%20to%20evaluate%2C%20store%2C%20and%20invert%0Athe%20curvature%20matrix.%20We%20propose%20Kronecker-factored%20approximate%20curvature%0A%28KFAC%29%20for%20PINN%20losses%20that%20greatly%20reduces%20the%20computational%20cost%20and%20allows%0Ascaling%20to%20much%20larger%20networks.%20Our%20approach%20goes%20beyond%20the%20established%20KFAC%0Afor%20traditional%20deep%20learning%20problems%20as%20it%20captures%20contributions%20from%20a%0APDE%27s%20differential%20operator%20that%20are%20crucial%20for%20optimization.%20To%20establish%0AKFAC%20for%20such%20losses%2C%20we%20use%20Taylor-mode%20automatic%20differentiation%20to%20describe%0Athe%20differential%20operator%27s%20computation%20graph%20as%20a%20forward%20network%20with%20shared%0Aweights.%20This%20allows%20us%20to%20apply%20KFAC%20thanks%20to%20a%20recently-developed%20general%0Aformulation%20for%20networks%20with%20weight%20sharing.%20Empirically%2C%20we%20find%20that%20our%0AKFAC-based%20optimizers%20are%20competitive%20with%20expensive%20second-order%20methods%20on%0Asmall%20problems%2C%20scale%20more%20favorably%20to%20higher-dimensional%20neural%20networks%20and%0APDEs%2C%20and%20consistently%20outperform%20first-order%20methods%20and%20LBFGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15603v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKronecker-Factored%2520Approximate%2520Curvature%2520for%2520Physics-Informed%2520Neural%250A%2520%2520Networks%26entry.906535625%3DFelix%2520Dangel%2520and%2520Johannes%2520M%25C3%25BCller%2520and%2520Marius%2520Zeinhofer%26entry.1292438233%3D%2520%2520Physics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520are%2520infamous%2520for%2520being%2520hard%2520to%250Atrain.%2520Recently%252C%2520second-order%2520methods%2520based%2520on%2520natural%2520gradient%2520and%250AGauss-Newton%2520methods%2520have%2520shown%2520promising%2520performance%252C%2520improving%2520the%2520accuracy%250Aachieved%2520by%2520first-order%2520methods%2520by%2520several%2520orders%2520of%2520magnitude.%2520While%250Apromising%252C%2520the%2520proposed%2520methods%2520only%2520scale%2520to%2520networks%2520with%2520a%2520few%2520thousand%250Aparameters%2520due%2520to%2520the%2520high%2520computational%2520cost%2520to%2520evaluate%252C%2520store%252C%2520and%2520invert%250Athe%2520curvature%2520matrix.%2520We%2520propose%2520Kronecker-factored%2520approximate%2520curvature%250A%2528KFAC%2529%2520for%2520PINN%2520losses%2520that%2520greatly%2520reduces%2520the%2520computational%2520cost%2520and%2520allows%250Ascaling%2520to%2520much%2520larger%2520networks.%2520Our%2520approach%2520goes%2520beyond%2520the%2520established%2520KFAC%250Afor%2520traditional%2520deep%2520learning%2520problems%2520as%2520it%2520captures%2520contributions%2520from%2520a%250APDE%2527s%2520differential%2520operator%2520that%2520are%2520crucial%2520for%2520optimization.%2520To%2520establish%250AKFAC%2520for%2520such%2520losses%252C%2520we%2520use%2520Taylor-mode%2520automatic%2520differentiation%2520to%2520describe%250Athe%2520differential%2520operator%2527s%2520computation%2520graph%2520as%2520a%2520forward%2520network%2520with%2520shared%250Aweights.%2520This%2520allows%2520us%2520to%2520apply%2520KFAC%2520thanks%2520to%2520a%2520recently-developed%2520general%250Aformulation%2520for%2520networks%2520with%2520weight%2520sharing.%2520Empirically%252C%2520we%2520find%2520that%2520our%250AKFAC-based%2520optimizers%2520are%2520competitive%2520with%2520expensive%2520second-order%2520methods%2520on%250Asmall%2520problems%252C%2520scale%2520more%2520favorably%2520to%2520higher-dimensional%2520neural%2520networks%2520and%250APDEs%252C%2520and%2520consistently%2520outperform%2520first-order%2520methods%2520and%2520LBFGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15603v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kronecker-Factored%20Approximate%20Curvature%20for%20Physics-Informed%20Neural%0A%20%20Networks&entry.906535625=Felix%20Dangel%20and%20Johannes%20M%C3%BCller%20and%20Marius%20Zeinhofer&entry.1292438233=%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20are%20infamous%20for%20being%20hard%20to%0Atrain.%20Recently%2C%20second-order%20methods%20based%20on%20natural%20gradient%20and%0AGauss-Newton%20methods%20have%20shown%20promising%20performance%2C%20improving%20the%20accuracy%0Aachieved%20by%20first-order%20methods%20by%20several%20orders%20of%20magnitude.%20While%0Apromising%2C%20the%20proposed%20methods%20only%20scale%20to%20networks%20with%20a%20few%20thousand%0Aparameters%20due%20to%20the%20high%20computational%20cost%20to%20evaluate%2C%20store%2C%20and%20invert%0Athe%20curvature%20matrix.%20We%20propose%20Kronecker-factored%20approximate%20curvature%0A%28KFAC%29%20for%20PINN%20losses%20that%20greatly%20reduces%20the%20computational%20cost%20and%20allows%0Ascaling%20to%20much%20larger%20networks.%20Our%20approach%20goes%20beyond%20the%20established%20KFAC%0Afor%20traditional%20deep%20learning%20problems%20as%20it%20captures%20contributions%20from%20a%0APDE%27s%20differential%20operator%20that%20are%20crucial%20for%20optimization.%20To%20establish%0AKFAC%20for%20such%20losses%2C%20we%20use%20Taylor-mode%20automatic%20differentiation%20to%20describe%0Athe%20differential%20operator%27s%20computation%20graph%20as%20a%20forward%20network%20with%20shared%0Aweights.%20This%20allows%20us%20to%20apply%20KFAC%20thanks%20to%20a%20recently-developed%20general%0Aformulation%20for%20networks%20with%20weight%20sharing.%20Empirically%2C%20we%20find%20that%20our%0AKFAC-based%20optimizers%20are%20competitive%20with%20expensive%20second-order%20methods%20on%0Asmall%20problems%2C%20scale%20more%20favorably%20to%20higher-dimensional%20neural%20networks%20and%0APDEs%2C%20and%20consistently%20outperform%20first-order%20methods%20and%20LBFGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15603v3&entry.124074799=Read"},
{"title": "Emergence of meta-stable clustering in mean-field transformer models", "author": "Giuseppe Bruno and Federico Pasqualotto and Andrea Agazzi", "abstract": "  We model the evolution of tokens within a deep stack of Transformer layers as\na continuous-time flow on the unit sphere, governed by a mean-field interacting\nparticle system, building on the framework introduced in (Geshkovski et al.,\n2023). Studying the corresponding mean-field Partial Differential Equation\n(PDE), which can be interpreted as a Wasserstein gradient flow, in this paper\nwe provide a mathematical investigation of the long-term behavior of this\nsystem, with a particular focus on the emergence and persistence of meta-stable\nphases and clustering phenomena, key elements in applications like next-token\nprediction. More specifically, we perform a perturbative analysis of the\nmean-field PDE around the iid uniform initialization and prove that, in the\nlimit of large number of tokens, the model remains close to a meta-stable\nmanifold of solutions with a given structure (e.g., periodicity). Further, the\nstructure characterizing the meta-stable manifold is explicitly identified, as\na function of the inverse temperature parameter of the model, by the index\nmaximizing a certain rescaling of Gegenbauer polynomials.\n", "link": "http://arxiv.org/abs/2410.23228v1", "date": "2024-10-30", "relevancy": 1.4154, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5313}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.465}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergence%20of%20meta-stable%20clustering%20in%20mean-field%20transformer%20models&body=Title%3A%20Emergence%20of%20meta-stable%20clustering%20in%20mean-field%20transformer%20models%0AAuthor%3A%20Giuseppe%20Bruno%20and%20Federico%20Pasqualotto%20and%20Andrea%20Agazzi%0AAbstract%3A%20%20%20We%20model%20the%20evolution%20of%20tokens%20within%20a%20deep%20stack%20of%20Transformer%20layers%20as%0Aa%20continuous-time%20flow%20on%20the%20unit%20sphere%2C%20governed%20by%20a%20mean-field%20interacting%0Aparticle%20system%2C%20building%20on%20the%20framework%20introduced%20in%20%28Geshkovski%20et%20al.%2C%0A2023%29.%20Studying%20the%20corresponding%20mean-field%20Partial%20Differential%20Equation%0A%28PDE%29%2C%20which%20can%20be%20interpreted%20as%20a%20Wasserstein%20gradient%20flow%2C%20in%20this%20paper%0Awe%20provide%20a%20mathematical%20investigation%20of%20the%20long-term%20behavior%20of%20this%0Asystem%2C%20with%20a%20particular%20focus%20on%20the%20emergence%20and%20persistence%20of%20meta-stable%0Aphases%20and%20clustering%20phenomena%2C%20key%20elements%20in%20applications%20like%20next-token%0Aprediction.%20More%20specifically%2C%20we%20perform%20a%20perturbative%20analysis%20of%20the%0Amean-field%20PDE%20around%20the%20iid%20uniform%20initialization%20and%20prove%20that%2C%20in%20the%0Alimit%20of%20large%20number%20of%20tokens%2C%20the%20model%20remains%20close%20to%20a%20meta-stable%0Amanifold%20of%20solutions%20with%20a%20given%20structure%20%28e.g.%2C%20periodicity%29.%20Further%2C%20the%0Astructure%20characterizing%20the%20meta-stable%20manifold%20is%20explicitly%20identified%2C%20as%0Aa%20function%20of%20the%20inverse%20temperature%20parameter%20of%20the%20model%2C%20by%20the%20index%0Amaximizing%20a%20certain%20rescaling%20of%20Gegenbauer%20polynomials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergence%2520of%2520meta-stable%2520clustering%2520in%2520mean-field%2520transformer%2520models%26entry.906535625%3DGiuseppe%2520Bruno%2520and%2520Federico%2520Pasqualotto%2520and%2520Andrea%2520Agazzi%26entry.1292438233%3D%2520%2520We%2520model%2520the%2520evolution%2520of%2520tokens%2520within%2520a%2520deep%2520stack%2520of%2520Transformer%2520layers%2520as%250Aa%2520continuous-time%2520flow%2520on%2520the%2520unit%2520sphere%252C%2520governed%2520by%2520a%2520mean-field%2520interacting%250Aparticle%2520system%252C%2520building%2520on%2520the%2520framework%2520introduced%2520in%2520%2528Geshkovski%2520et%2520al.%252C%250A2023%2529.%2520Studying%2520the%2520corresponding%2520mean-field%2520Partial%2520Differential%2520Equation%250A%2528PDE%2529%252C%2520which%2520can%2520be%2520interpreted%2520as%2520a%2520Wasserstein%2520gradient%2520flow%252C%2520in%2520this%2520paper%250Awe%2520provide%2520a%2520mathematical%2520investigation%2520of%2520the%2520long-term%2520behavior%2520of%2520this%250Asystem%252C%2520with%2520a%2520particular%2520focus%2520on%2520the%2520emergence%2520and%2520persistence%2520of%2520meta-stable%250Aphases%2520and%2520clustering%2520phenomena%252C%2520key%2520elements%2520in%2520applications%2520like%2520next-token%250Aprediction.%2520More%2520specifically%252C%2520we%2520perform%2520a%2520perturbative%2520analysis%2520of%2520the%250Amean-field%2520PDE%2520around%2520the%2520iid%2520uniform%2520initialization%2520and%2520prove%2520that%252C%2520in%2520the%250Alimit%2520of%2520large%2520number%2520of%2520tokens%252C%2520the%2520model%2520remains%2520close%2520to%2520a%2520meta-stable%250Amanifold%2520of%2520solutions%2520with%2520a%2520given%2520structure%2520%2528e.g.%252C%2520periodicity%2529.%2520Further%252C%2520the%250Astructure%2520characterizing%2520the%2520meta-stable%2520manifold%2520is%2520explicitly%2520identified%252C%2520as%250Aa%2520function%2520of%2520the%2520inverse%2520temperature%2520parameter%2520of%2520the%2520model%252C%2520by%2520the%2520index%250Amaximizing%2520a%2520certain%2520rescaling%2520of%2520Gegenbauer%2520polynomials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergence%20of%20meta-stable%20clustering%20in%20mean-field%20transformer%20models&entry.906535625=Giuseppe%20Bruno%20and%20Federico%20Pasqualotto%20and%20Andrea%20Agazzi&entry.1292438233=%20%20We%20model%20the%20evolution%20of%20tokens%20within%20a%20deep%20stack%20of%20Transformer%20layers%20as%0Aa%20continuous-time%20flow%20on%20the%20unit%20sphere%2C%20governed%20by%20a%20mean-field%20interacting%0Aparticle%20system%2C%20building%20on%20the%20framework%20introduced%20in%20%28Geshkovski%20et%20al.%2C%0A2023%29.%20Studying%20the%20corresponding%20mean-field%20Partial%20Differential%20Equation%0A%28PDE%29%2C%20which%20can%20be%20interpreted%20as%20a%20Wasserstein%20gradient%20flow%2C%20in%20this%20paper%0Awe%20provide%20a%20mathematical%20investigation%20of%20the%20long-term%20behavior%20of%20this%0Asystem%2C%20with%20a%20particular%20focus%20on%20the%20emergence%20and%20persistence%20of%20meta-stable%0Aphases%20and%20clustering%20phenomena%2C%20key%20elements%20in%20applications%20like%20next-token%0Aprediction.%20More%20specifically%2C%20we%20perform%20a%20perturbative%20analysis%20of%20the%0Amean-field%20PDE%20around%20the%20iid%20uniform%20initialization%20and%20prove%20that%2C%20in%20the%0Alimit%20of%20large%20number%20of%20tokens%2C%20the%20model%20remains%20close%20to%20a%20meta-stable%0Amanifold%20of%20solutions%20with%20a%20given%20structure%20%28e.g.%2C%20periodicity%29.%20Further%2C%20the%0Astructure%20characterizing%20the%20meta-stable%20manifold%20is%20explicitly%20identified%2C%20as%0Aa%20function%20of%20the%20inverse%20temperature%20parameter%20of%20the%20model%2C%20by%20the%20index%0Amaximizing%20a%20certain%20rescaling%20of%20Gegenbauer%20polynomials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23228v1&entry.124074799=Read"},
{"title": "Fourier Amplitude and Correlation Loss: Beyond Using L2 Loss for\n  Skillful Precipitation Nowcasting", "author": "Chiu-Wai Yan and Shi Quan Foo and Van Hoan Trinh and Dit-Yan Yeung and Ka-Hing Wong and Wai-Kin Wong", "abstract": "  Deep learning approaches have been widely adopted for precipitation\nnowcasting in recent years. Previous studies mainly focus on proposing new\nmodel architectures to improve pixel-wise metrics. However, they frequently\nresult in blurry predictions which provide limited utility to forecasting\noperations. In this work, we propose a new Fourier Amplitude and Correlation\nLoss (FACL) which consists of two novel loss terms: Fourier Amplitude Loss\n(FAL) and Fourier Correlation Loss (FCL). FAL regularizes the Fourier amplitude\nof the model prediction and FCL complements the missing phase information. The\ntwo loss terms work together to replace the traditional $L_2$ losses such as\nMSE and weighted MSE for the spatiotemporal prediction problem on signal-based\ndata. Our method is generic, parameter-free and efficient. Extensive\nexperiments using one synthetic dataset and three radar echo datasets\ndemonstrate that our method improves perceptual metrics and meteorology skill\nscores, with a small trade-off to pixel-wise accuracy and structural\nsimilarity. Moreover, to improve the error margin in meteorological skill\nscores such as Critical Success Index (CSI) and Fractions Skill Score (FSS), we\npropose and adopt the Regional Histogram Divergence (RHD), a distance metric\nthat considers the patch-wise similarity between signal-based imagery patterns\nwith tolerance to local transforms. Code is available at\nhttps://github.com/argenycw/FACL\n", "link": "http://arxiv.org/abs/2410.23159v1", "date": "2024-10-30", "relevancy": 1.4004, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4676}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4673}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fourier%20Amplitude%20and%20Correlation%20Loss%3A%20Beyond%20Using%20L2%20Loss%20for%0A%20%20Skillful%20Precipitation%20Nowcasting&body=Title%3A%20Fourier%20Amplitude%20and%20Correlation%20Loss%3A%20Beyond%20Using%20L2%20Loss%20for%0A%20%20Skillful%20Precipitation%20Nowcasting%0AAuthor%3A%20Chiu-Wai%20Yan%20and%20Shi%20Quan%20Foo%20and%20Van%20Hoan%20Trinh%20and%20Dit-Yan%20Yeung%20and%20Ka-Hing%20Wong%20and%20Wai-Kin%20Wong%0AAbstract%3A%20%20%20Deep%20learning%20approaches%20have%20been%20widely%20adopted%20for%20precipitation%0Anowcasting%20in%20recent%20years.%20Previous%20studies%20mainly%20focus%20on%20proposing%20new%0Amodel%20architectures%20to%20improve%20pixel-wise%20metrics.%20However%2C%20they%20frequently%0Aresult%20in%20blurry%20predictions%20which%20provide%20limited%20utility%20to%20forecasting%0Aoperations.%20In%20this%20work%2C%20we%20propose%20a%20new%20Fourier%20Amplitude%20and%20Correlation%0ALoss%20%28FACL%29%20which%20consists%20of%20two%20novel%20loss%20terms%3A%20Fourier%20Amplitude%20Loss%0A%28FAL%29%20and%20Fourier%20Correlation%20Loss%20%28FCL%29.%20FAL%20regularizes%20the%20Fourier%20amplitude%0Aof%20the%20model%20prediction%20and%20FCL%20complements%20the%20missing%20phase%20information.%20The%0Atwo%20loss%20terms%20work%20together%20to%20replace%20the%20traditional%20%24L_2%24%20losses%20such%20as%0AMSE%20and%20weighted%20MSE%20for%20the%20spatiotemporal%20prediction%20problem%20on%20signal-based%0Adata.%20Our%20method%20is%20generic%2C%20parameter-free%20and%20efficient.%20Extensive%0Aexperiments%20using%20one%20synthetic%20dataset%20and%20three%20radar%20echo%20datasets%0Ademonstrate%20that%20our%20method%20improves%20perceptual%20metrics%20and%20meteorology%20skill%0Ascores%2C%20with%20a%20small%20trade-off%20to%20pixel-wise%20accuracy%20and%20structural%0Asimilarity.%20Moreover%2C%20to%20improve%20the%20error%20margin%20in%20meteorological%20skill%0Ascores%20such%20as%20Critical%20Success%20Index%20%28CSI%29%20and%20Fractions%20Skill%20Score%20%28FSS%29%2C%20we%0Apropose%20and%20adopt%20the%20Regional%20Histogram%20Divergence%20%28RHD%29%2C%20a%20distance%20metric%0Athat%20considers%20the%20patch-wise%20similarity%20between%20signal-based%20imagery%20patterns%0Awith%20tolerance%20to%20local%20transforms.%20Code%20is%20available%20at%0Ahttps%3A//github.com/argenycw/FACL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFourier%2520Amplitude%2520and%2520Correlation%2520Loss%253A%2520Beyond%2520Using%2520L2%2520Loss%2520for%250A%2520%2520Skillful%2520Precipitation%2520Nowcasting%26entry.906535625%3DChiu-Wai%2520Yan%2520and%2520Shi%2520Quan%2520Foo%2520and%2520Van%2520Hoan%2520Trinh%2520and%2520Dit-Yan%2520Yeung%2520and%2520Ka-Hing%2520Wong%2520and%2520Wai-Kin%2520Wong%26entry.1292438233%3D%2520%2520Deep%2520learning%2520approaches%2520have%2520been%2520widely%2520adopted%2520for%2520precipitation%250Anowcasting%2520in%2520recent%2520years.%2520Previous%2520studies%2520mainly%2520focus%2520on%2520proposing%2520new%250Amodel%2520architectures%2520to%2520improve%2520pixel-wise%2520metrics.%2520However%252C%2520they%2520frequently%250Aresult%2520in%2520blurry%2520predictions%2520which%2520provide%2520limited%2520utility%2520to%2520forecasting%250Aoperations.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520Fourier%2520Amplitude%2520and%2520Correlation%250ALoss%2520%2528FACL%2529%2520which%2520consists%2520of%2520two%2520novel%2520loss%2520terms%253A%2520Fourier%2520Amplitude%2520Loss%250A%2528FAL%2529%2520and%2520Fourier%2520Correlation%2520Loss%2520%2528FCL%2529.%2520FAL%2520regularizes%2520the%2520Fourier%2520amplitude%250Aof%2520the%2520model%2520prediction%2520and%2520FCL%2520complements%2520the%2520missing%2520phase%2520information.%2520The%250Atwo%2520loss%2520terms%2520work%2520together%2520to%2520replace%2520the%2520traditional%2520%2524L_2%2524%2520losses%2520such%2520as%250AMSE%2520and%2520weighted%2520MSE%2520for%2520the%2520spatiotemporal%2520prediction%2520problem%2520on%2520signal-based%250Adata.%2520Our%2520method%2520is%2520generic%252C%2520parameter-free%2520and%2520efficient.%2520Extensive%250Aexperiments%2520using%2520one%2520synthetic%2520dataset%2520and%2520three%2520radar%2520echo%2520datasets%250Ademonstrate%2520that%2520our%2520method%2520improves%2520perceptual%2520metrics%2520and%2520meteorology%2520skill%250Ascores%252C%2520with%2520a%2520small%2520trade-off%2520to%2520pixel-wise%2520accuracy%2520and%2520structural%250Asimilarity.%2520Moreover%252C%2520to%2520improve%2520the%2520error%2520margin%2520in%2520meteorological%2520skill%250Ascores%2520such%2520as%2520Critical%2520Success%2520Index%2520%2528CSI%2529%2520and%2520Fractions%2520Skill%2520Score%2520%2528FSS%2529%252C%2520we%250Apropose%2520and%2520adopt%2520the%2520Regional%2520Histogram%2520Divergence%2520%2528RHD%2529%252C%2520a%2520distance%2520metric%250Athat%2520considers%2520the%2520patch-wise%2520similarity%2520between%2520signal-based%2520imagery%2520patterns%250Awith%2520tolerance%2520to%2520local%2520transforms.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/argenycw/FACL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fourier%20Amplitude%20and%20Correlation%20Loss%3A%20Beyond%20Using%20L2%20Loss%20for%0A%20%20Skillful%20Precipitation%20Nowcasting&entry.906535625=Chiu-Wai%20Yan%20and%20Shi%20Quan%20Foo%20and%20Van%20Hoan%20Trinh%20and%20Dit-Yan%20Yeung%20and%20Ka-Hing%20Wong%20and%20Wai-Kin%20Wong&entry.1292438233=%20%20Deep%20learning%20approaches%20have%20been%20widely%20adopted%20for%20precipitation%0Anowcasting%20in%20recent%20years.%20Previous%20studies%20mainly%20focus%20on%20proposing%20new%0Amodel%20architectures%20to%20improve%20pixel-wise%20metrics.%20However%2C%20they%20frequently%0Aresult%20in%20blurry%20predictions%20which%20provide%20limited%20utility%20to%20forecasting%0Aoperations.%20In%20this%20work%2C%20we%20propose%20a%20new%20Fourier%20Amplitude%20and%20Correlation%0ALoss%20%28FACL%29%20which%20consists%20of%20two%20novel%20loss%20terms%3A%20Fourier%20Amplitude%20Loss%0A%28FAL%29%20and%20Fourier%20Correlation%20Loss%20%28FCL%29.%20FAL%20regularizes%20the%20Fourier%20amplitude%0Aof%20the%20model%20prediction%20and%20FCL%20complements%20the%20missing%20phase%20information.%20The%0Atwo%20loss%20terms%20work%20together%20to%20replace%20the%20traditional%20%24L_2%24%20losses%20such%20as%0AMSE%20and%20weighted%20MSE%20for%20the%20spatiotemporal%20prediction%20problem%20on%20signal-based%0Adata.%20Our%20method%20is%20generic%2C%20parameter-free%20and%20efficient.%20Extensive%0Aexperiments%20using%20one%20synthetic%20dataset%20and%20three%20radar%20echo%20datasets%0Ademonstrate%20that%20our%20method%20improves%20perceptual%20metrics%20and%20meteorology%20skill%0Ascores%2C%20with%20a%20small%20trade-off%20to%20pixel-wise%20accuracy%20and%20structural%0Asimilarity.%20Moreover%2C%20to%20improve%20the%20error%20margin%20in%20meteorological%20skill%0Ascores%20such%20as%20Critical%20Success%20Index%20%28CSI%29%20and%20Fractions%20Skill%20Score%20%28FSS%29%2C%20we%0Apropose%20and%20adopt%20the%20Regional%20Histogram%20Divergence%20%28RHD%29%2C%20a%20distance%20metric%0Athat%20considers%20the%20patch-wise%20similarity%20between%20signal-based%20imagery%20patterns%0Awith%20tolerance%20to%20local%20transforms.%20Code%20is%20available%20at%0Ahttps%3A//github.com/argenycw/FACL%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23159v1&entry.124074799=Read"},
{"title": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching", "author": "Farnaz Niknia and Ping Wang and Zixu Wang and Aakash Agarwal and Adib S. Rezaei", "abstract": "  This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity.\n", "link": "http://arxiv.org/abs/2402.14576v3", "date": "2024-10-30", "relevancy": 1.3952, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.474}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4554}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention-Enhanced%20Prioritized%20Proximal%20Policy%20Optimization%20for%20Adaptive%0A%20%20Edge%20Caching&body=Title%3A%20Attention-Enhanced%20Prioritized%20Proximal%20Policy%20Optimization%20for%20Adaptive%0A%20%20Edge%20Caching%0AAuthor%3A%20Farnaz%20Niknia%20and%20Ping%20Wang%20and%20Zixu%20Wang%20and%20Aakash%20Agarwal%20and%20Adib%20S.%20Rezaei%0AAbstract%3A%20%20%20This%20paper%20tackles%20the%20growing%20issue%20of%20excessive%20data%20transmission%20in%0Anetworks.%20With%20increasing%20traffic%2C%20backhaul%20links%20and%20core%20networks%20are%20under%0Asignificant%20traffic%2C%20leading%20to%20the%20investigation%20of%20caching%20solutions%20at%20edge%0Arouters.%20Many%20existing%20studies%20utilize%20Markov%20Decision%20Processes%20%28MDP%29%20to%0Atackle%20caching%20problems%2C%20often%20assuming%20decision%20points%20at%20fixed%20intervals%3B%0Ahowever%2C%20real-world%20environments%20are%20characterized%20by%20random%20request%20arrivals.%0AAdditionally%2C%20critical%20file%20attributes%20such%20as%20lifetime%2C%20size%2C%20and%20priority%0Asignificantly%20impact%20the%20effectiveness%20of%20caching%20policies%2C%20yet%20existing%0Aresearch%20fails%20to%20integrate%20all%20these%20attributes%20in%20policy%20design.%20In%20this%0Awork%2C%20we%20model%20the%20caching%20problem%20using%20a%20Semi-Markov%20Decision%20Process%20%28SMDP%29%0Ato%20better%20capture%20the%20continuous-time%20nature%20of%20real-world%20applications%2C%0Aenabling%20caching%20decisions%20to%20be%20triggered%20by%20random%20file%20requests.%20We%20then%0Aintroduce%20a%20Proximal%20Policy%20Optimization%20%28PPO%29--based%20caching%20strategy%20that%0Afully%20considers%20file%20attributes%20like%20lifetime%2C%20size%2C%20and%20priority.%20Simulations%0Ashow%20that%20our%20method%20outperforms%20a%20recent%20Deep%20Reinforcement%20Learning-based%0Atechnique.%20To%20further%20advance%20our%20research%2C%20we%20improved%20the%20convergence%20rate%20of%0APPO%20by%20prioritizing%20transitions%20within%20the%20replay%20buffer%20through%20an%20attention%0Amechanism.%20This%20mechanism%20evaluates%20the%20similarity%20between%20the%20current%20state%0Aand%20all%20stored%20transitions%2C%20assigning%20higher%20priorities%20to%20transitions%20that%0Aexhibit%20greater%20similarity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14576v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention-Enhanced%2520Prioritized%2520Proximal%2520Policy%2520Optimization%2520for%2520Adaptive%250A%2520%2520Edge%2520Caching%26entry.906535625%3DFarnaz%2520Niknia%2520and%2520Ping%2520Wang%2520and%2520Zixu%2520Wang%2520and%2520Aakash%2520Agarwal%2520and%2520Adib%2520S.%2520Rezaei%26entry.1292438233%3D%2520%2520This%2520paper%2520tackles%2520the%2520growing%2520issue%2520of%2520excessive%2520data%2520transmission%2520in%250Anetworks.%2520With%2520increasing%2520traffic%252C%2520backhaul%2520links%2520and%2520core%2520networks%2520are%2520under%250Asignificant%2520traffic%252C%2520leading%2520to%2520the%2520investigation%2520of%2520caching%2520solutions%2520at%2520edge%250Arouters.%2520Many%2520existing%2520studies%2520utilize%2520Markov%2520Decision%2520Processes%2520%2528MDP%2529%2520to%250Atackle%2520caching%2520problems%252C%2520often%2520assuming%2520decision%2520points%2520at%2520fixed%2520intervals%253B%250Ahowever%252C%2520real-world%2520environments%2520are%2520characterized%2520by%2520random%2520request%2520arrivals.%250AAdditionally%252C%2520critical%2520file%2520attributes%2520such%2520as%2520lifetime%252C%2520size%252C%2520and%2520priority%250Asignificantly%2520impact%2520the%2520effectiveness%2520of%2520caching%2520policies%252C%2520yet%2520existing%250Aresearch%2520fails%2520to%2520integrate%2520all%2520these%2520attributes%2520in%2520policy%2520design.%2520In%2520this%250Awork%252C%2520we%2520model%2520the%2520caching%2520problem%2520using%2520a%2520Semi-Markov%2520Decision%2520Process%2520%2528SMDP%2529%250Ato%2520better%2520capture%2520the%2520continuous-time%2520nature%2520of%2520real-world%2520applications%252C%250Aenabling%2520caching%2520decisions%2520to%2520be%2520triggered%2520by%2520random%2520file%2520requests.%2520We%2520then%250Aintroduce%2520a%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529--based%2520caching%2520strategy%2520that%250Afully%2520considers%2520file%2520attributes%2520like%2520lifetime%252C%2520size%252C%2520and%2520priority.%2520Simulations%250Ashow%2520that%2520our%2520method%2520outperforms%2520a%2520recent%2520Deep%2520Reinforcement%2520Learning-based%250Atechnique.%2520To%2520further%2520advance%2520our%2520research%252C%2520we%2520improved%2520the%2520convergence%2520rate%2520of%250APPO%2520by%2520prioritizing%2520transitions%2520within%2520the%2520replay%2520buffer%2520through%2520an%2520attention%250Amechanism.%2520This%2520mechanism%2520evaluates%2520the%2520similarity%2520between%2520the%2520current%2520state%250Aand%2520all%2520stored%2520transitions%252C%2520assigning%2520higher%2520priorities%2520to%2520transitions%2520that%250Aexhibit%2520greater%2520similarity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14576v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention-Enhanced%20Prioritized%20Proximal%20Policy%20Optimization%20for%20Adaptive%0A%20%20Edge%20Caching&entry.906535625=Farnaz%20Niknia%20and%20Ping%20Wang%20and%20Zixu%20Wang%20and%20Aakash%20Agarwal%20and%20Adib%20S.%20Rezaei&entry.1292438233=%20%20This%20paper%20tackles%20the%20growing%20issue%20of%20excessive%20data%20transmission%20in%0Anetworks.%20With%20increasing%20traffic%2C%20backhaul%20links%20and%20core%20networks%20are%20under%0Asignificant%20traffic%2C%20leading%20to%20the%20investigation%20of%20caching%20solutions%20at%20edge%0Arouters.%20Many%20existing%20studies%20utilize%20Markov%20Decision%20Processes%20%28MDP%29%20to%0Atackle%20caching%20problems%2C%20often%20assuming%20decision%20points%20at%20fixed%20intervals%3B%0Ahowever%2C%20real-world%20environments%20are%20characterized%20by%20random%20request%20arrivals.%0AAdditionally%2C%20critical%20file%20attributes%20such%20as%20lifetime%2C%20size%2C%20and%20priority%0Asignificantly%20impact%20the%20effectiveness%20of%20caching%20policies%2C%20yet%20existing%0Aresearch%20fails%20to%20integrate%20all%20these%20attributes%20in%20policy%20design.%20In%20this%0Awork%2C%20we%20model%20the%20caching%20problem%20using%20a%20Semi-Markov%20Decision%20Process%20%28SMDP%29%0Ato%20better%20capture%20the%20continuous-time%20nature%20of%20real-world%20applications%2C%0Aenabling%20caching%20decisions%20to%20be%20triggered%20by%20random%20file%20requests.%20We%20then%0Aintroduce%20a%20Proximal%20Policy%20Optimization%20%28PPO%29--based%20caching%20strategy%20that%0Afully%20considers%20file%20attributes%20like%20lifetime%2C%20size%2C%20and%20priority.%20Simulations%0Ashow%20that%20our%20method%20outperforms%20a%20recent%20Deep%20Reinforcement%20Learning-based%0Atechnique.%20To%20further%20advance%20our%20research%2C%20we%20improved%20the%20convergence%20rate%20of%0APPO%20by%20prioritizing%20transitions%20within%20the%20replay%20buffer%20through%20an%20attention%0Amechanism.%20This%20mechanism%20evaluates%20the%20similarity%20between%20the%20current%20state%0Aand%20all%20stored%20transitions%2C%20assigning%20higher%20priorities%20to%20transitions%20that%0Aexhibit%20greater%20similarity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14576v3&entry.124074799=Read"},
{"title": "Impacts of floating-point non-associativity on reproducibility for HPC\n  and deep learning applications", "author": "Sanjif Shanmugavelu and Mathieu Taillefumier and Christopher Culver and Oscar Hernandez and Mark Coletti and Ada Sedova", "abstract": "  Run to run variability in parallel programs caused by floating-point\nnon-associativity has been known to significantly affect reproducibility in\niterative algorithms, due to accumulating errors. Non-reproducibility can\ncritically affect the efficiency and effectiveness of correctness testing for\nstochastic programs. Recently, the sensitivity of deep learning training and\ninference pipelines to floating-point non-associativity has been found to\nsometimes be extreme. It can prevent certification for commercial applications,\naccurate assessment of robustness and sensitivity, and bug detection. New\napproaches in scientific computing applications have coupled deep learning\nmodels with high-performance computing, leading to an aggravation of debugging\nand testing challenges. Here we perform an investigation of the statistical\nproperties of floating-point non-associativity within modern parallel\nprogramming models, and analyze performance and productivity impacts of\nreplacing atomic operations with deterministic alternatives on GPUs. We examine\nthe recently-added deterministic options in PyTorch within the context of GPU\ndeployment for deep learning, uncovering and quantifying the impacts of input\nparameters triggering run to run variability and reporting on the reliability\nand completeness of the documentation. Finally, we evaluate the strategy of\nexploiting automatic determinism that could be provided by deterministic\nhardware, using the Groq accelerator for inference portions of the deep\nlearning pipeline. We demonstrate the benefits that a hardware-based strategy\ncan provide within reproducibility and correctness efforts.\n", "link": "http://arxiv.org/abs/2408.05148v3", "date": "2024-10-30", "relevancy": 1.3945, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4765}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4634}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Impacts%20of%20floating-point%20non-associativity%20on%20reproducibility%20for%20HPC%0A%20%20and%20deep%20learning%20applications&body=Title%3A%20Impacts%20of%20floating-point%20non-associativity%20on%20reproducibility%20for%20HPC%0A%20%20and%20deep%20learning%20applications%0AAuthor%3A%20Sanjif%20Shanmugavelu%20and%20Mathieu%20Taillefumier%20and%20Christopher%20Culver%20and%20Oscar%20Hernandez%20and%20Mark%20Coletti%20and%20Ada%20Sedova%0AAbstract%3A%20%20%20Run%20to%20run%20variability%20in%20parallel%20programs%20caused%20by%20floating-point%0Anon-associativity%20has%20been%20known%20to%20significantly%20affect%20reproducibility%20in%0Aiterative%20algorithms%2C%20due%20to%20accumulating%20errors.%20Non-reproducibility%20can%0Acritically%20affect%20the%20efficiency%20and%20effectiveness%20of%20correctness%20testing%20for%0Astochastic%20programs.%20Recently%2C%20the%20sensitivity%20of%20deep%20learning%20training%20and%0Ainference%20pipelines%20to%20floating-point%20non-associativity%20has%20been%20found%20to%0Asometimes%20be%20extreme.%20It%20can%20prevent%20certification%20for%20commercial%20applications%2C%0Aaccurate%20assessment%20of%20robustness%20and%20sensitivity%2C%20and%20bug%20detection.%20New%0Aapproaches%20in%20scientific%20computing%20applications%20have%20coupled%20deep%20learning%0Amodels%20with%20high-performance%20computing%2C%20leading%20to%20an%20aggravation%20of%20debugging%0Aand%20testing%20challenges.%20Here%20we%20perform%20an%20investigation%20of%20the%20statistical%0Aproperties%20of%20floating-point%20non-associativity%20within%20modern%20parallel%0Aprogramming%20models%2C%20and%20analyze%20performance%20and%20productivity%20impacts%20of%0Areplacing%20atomic%20operations%20with%20deterministic%20alternatives%20on%20GPUs.%20We%20examine%0Athe%20recently-added%20deterministic%20options%20in%20PyTorch%20within%20the%20context%20of%20GPU%0Adeployment%20for%20deep%20learning%2C%20uncovering%20and%20quantifying%20the%20impacts%20of%20input%0Aparameters%20triggering%20run%20to%20run%20variability%20and%20reporting%20on%20the%20reliability%0Aand%20completeness%20of%20the%20documentation.%20Finally%2C%20we%20evaluate%20the%20strategy%20of%0Aexploiting%20automatic%20determinism%20that%20could%20be%20provided%20by%20deterministic%0Ahardware%2C%20using%20the%20Groq%20accelerator%20for%20inference%20portions%20of%20the%20deep%0Alearning%20pipeline.%20We%20demonstrate%20the%20benefits%20that%20a%20hardware-based%20strategy%0Acan%20provide%20within%20reproducibility%20and%20correctness%20efforts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05148v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImpacts%2520of%2520floating-point%2520non-associativity%2520on%2520reproducibility%2520for%2520HPC%250A%2520%2520and%2520deep%2520learning%2520applications%26entry.906535625%3DSanjif%2520Shanmugavelu%2520and%2520Mathieu%2520Taillefumier%2520and%2520Christopher%2520Culver%2520and%2520Oscar%2520Hernandez%2520and%2520Mark%2520Coletti%2520and%2520Ada%2520Sedova%26entry.1292438233%3D%2520%2520Run%2520to%2520run%2520variability%2520in%2520parallel%2520programs%2520caused%2520by%2520floating-point%250Anon-associativity%2520has%2520been%2520known%2520to%2520significantly%2520affect%2520reproducibility%2520in%250Aiterative%2520algorithms%252C%2520due%2520to%2520accumulating%2520errors.%2520Non-reproducibility%2520can%250Acritically%2520affect%2520the%2520efficiency%2520and%2520effectiveness%2520of%2520correctness%2520testing%2520for%250Astochastic%2520programs.%2520Recently%252C%2520the%2520sensitivity%2520of%2520deep%2520learning%2520training%2520and%250Ainference%2520pipelines%2520to%2520floating-point%2520non-associativity%2520has%2520been%2520found%2520to%250Asometimes%2520be%2520extreme.%2520It%2520can%2520prevent%2520certification%2520for%2520commercial%2520applications%252C%250Aaccurate%2520assessment%2520of%2520robustness%2520and%2520sensitivity%252C%2520and%2520bug%2520detection.%2520New%250Aapproaches%2520in%2520scientific%2520computing%2520applications%2520have%2520coupled%2520deep%2520learning%250Amodels%2520with%2520high-performance%2520computing%252C%2520leading%2520to%2520an%2520aggravation%2520of%2520debugging%250Aand%2520testing%2520challenges.%2520Here%2520we%2520perform%2520an%2520investigation%2520of%2520the%2520statistical%250Aproperties%2520of%2520floating-point%2520non-associativity%2520within%2520modern%2520parallel%250Aprogramming%2520models%252C%2520and%2520analyze%2520performance%2520and%2520productivity%2520impacts%2520of%250Areplacing%2520atomic%2520operations%2520with%2520deterministic%2520alternatives%2520on%2520GPUs.%2520We%2520examine%250Athe%2520recently-added%2520deterministic%2520options%2520in%2520PyTorch%2520within%2520the%2520context%2520of%2520GPU%250Adeployment%2520for%2520deep%2520learning%252C%2520uncovering%2520and%2520quantifying%2520the%2520impacts%2520of%2520input%250Aparameters%2520triggering%2520run%2520to%2520run%2520variability%2520and%2520reporting%2520on%2520the%2520reliability%250Aand%2520completeness%2520of%2520the%2520documentation.%2520Finally%252C%2520we%2520evaluate%2520the%2520strategy%2520of%250Aexploiting%2520automatic%2520determinism%2520that%2520could%2520be%2520provided%2520by%2520deterministic%250Ahardware%252C%2520using%2520the%2520Groq%2520accelerator%2520for%2520inference%2520portions%2520of%2520the%2520deep%250Alearning%2520pipeline.%2520We%2520demonstrate%2520the%2520benefits%2520that%2520a%2520hardware-based%2520strategy%250Acan%2520provide%2520within%2520reproducibility%2520and%2520correctness%2520efforts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05148v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Impacts%20of%20floating-point%20non-associativity%20on%20reproducibility%20for%20HPC%0A%20%20and%20deep%20learning%20applications&entry.906535625=Sanjif%20Shanmugavelu%20and%20Mathieu%20Taillefumier%20and%20Christopher%20Culver%20and%20Oscar%20Hernandez%20and%20Mark%20Coletti%20and%20Ada%20Sedova&entry.1292438233=%20%20Run%20to%20run%20variability%20in%20parallel%20programs%20caused%20by%20floating-point%0Anon-associativity%20has%20been%20known%20to%20significantly%20affect%20reproducibility%20in%0Aiterative%20algorithms%2C%20due%20to%20accumulating%20errors.%20Non-reproducibility%20can%0Acritically%20affect%20the%20efficiency%20and%20effectiveness%20of%20correctness%20testing%20for%0Astochastic%20programs.%20Recently%2C%20the%20sensitivity%20of%20deep%20learning%20training%20and%0Ainference%20pipelines%20to%20floating-point%20non-associativity%20has%20been%20found%20to%0Asometimes%20be%20extreme.%20It%20can%20prevent%20certification%20for%20commercial%20applications%2C%0Aaccurate%20assessment%20of%20robustness%20and%20sensitivity%2C%20and%20bug%20detection.%20New%0Aapproaches%20in%20scientific%20computing%20applications%20have%20coupled%20deep%20learning%0Amodels%20with%20high-performance%20computing%2C%20leading%20to%20an%20aggravation%20of%20debugging%0Aand%20testing%20challenges.%20Here%20we%20perform%20an%20investigation%20of%20the%20statistical%0Aproperties%20of%20floating-point%20non-associativity%20within%20modern%20parallel%0Aprogramming%20models%2C%20and%20analyze%20performance%20and%20productivity%20impacts%20of%0Areplacing%20atomic%20operations%20with%20deterministic%20alternatives%20on%20GPUs.%20We%20examine%0Athe%20recently-added%20deterministic%20options%20in%20PyTorch%20within%20the%20context%20of%20GPU%0Adeployment%20for%20deep%20learning%2C%20uncovering%20and%20quantifying%20the%20impacts%20of%20input%0Aparameters%20triggering%20run%20to%20run%20variability%20and%20reporting%20on%20the%20reliability%0Aand%20completeness%20of%20the%20documentation.%20Finally%2C%20we%20evaluate%20the%20strategy%20of%0Aexploiting%20automatic%20determinism%20that%20could%20be%20provided%20by%20deterministic%0Ahardware%2C%20using%20the%20Groq%20accelerator%20for%20inference%20portions%20of%20the%20deep%0Alearning%20pipeline.%20We%20demonstrate%20the%20benefits%20that%20a%20hardware-based%20strategy%0Acan%20provide%20within%20reproducibility%20and%20correctness%20efforts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05148v3&entry.124074799=Read"},
{"title": "Extracting thin film structures of energy materials using transformers", "author": "Chen Zhang and Valerie A. Niemann and Peter Benedek and Thomas F. Jaramillo and Mathieu Doucet", "abstract": "  Neutron-Transformer Reflectometry and Advanced Computation Engine (N-TRACE ),\na neural network model using transformer architecture, is introduced for\nneutron reflectometry data analysis. It offers fast, accurate initial parameter\nestimations and efficient refinements, improving efficiency and precision for\nreal-time data analysis of lithium-mediated nitrogen reduction for\nelectrochemical ammonia synthesis, with relevance to other chemical\ntransformations and batteries. Despite limitations in generalizing across\nsystems, it shows promises for the use of transformers as the basis for models\nthat could replace trial-and-error approaches to modeling reflectometry data.\n", "link": "http://arxiv.org/abs/2406.16741v2", "date": "2024-10-30", "relevancy": 1.3758, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5368}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4369}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extracting%20thin%20film%20structures%20of%20energy%20materials%20using%20transformers&body=Title%3A%20Extracting%20thin%20film%20structures%20of%20energy%20materials%20using%20transformers%0AAuthor%3A%20Chen%20Zhang%20and%20Valerie%20A.%20Niemann%20and%20Peter%20Benedek%20and%20Thomas%20F.%20Jaramillo%20and%20Mathieu%20Doucet%0AAbstract%3A%20%20%20Neutron-Transformer%20Reflectometry%20and%20Advanced%20Computation%20Engine%20%28N-TRACE%20%29%2C%0Aa%20neural%20network%20model%20using%20transformer%20architecture%2C%20is%20introduced%20for%0Aneutron%20reflectometry%20data%20analysis.%20It%20offers%20fast%2C%20accurate%20initial%20parameter%0Aestimations%20and%20efficient%20refinements%2C%20improving%20efficiency%20and%20precision%20for%0Areal-time%20data%20analysis%20of%20lithium-mediated%20nitrogen%20reduction%20for%0Aelectrochemical%20ammonia%20synthesis%2C%20with%20relevance%20to%20other%20chemical%0Atransformations%20and%20batteries.%20Despite%20limitations%20in%20generalizing%20across%0Asystems%2C%20it%20shows%20promises%20for%20the%20use%20of%20transformers%20as%20the%20basis%20for%20models%0Athat%20could%20replace%20trial-and-error%20approaches%20to%20modeling%20reflectometry%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16741v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtracting%2520thin%2520film%2520structures%2520of%2520energy%2520materials%2520using%2520transformers%26entry.906535625%3DChen%2520Zhang%2520and%2520Valerie%2520A.%2520Niemann%2520and%2520Peter%2520Benedek%2520and%2520Thomas%2520F.%2520Jaramillo%2520and%2520Mathieu%2520Doucet%26entry.1292438233%3D%2520%2520Neutron-Transformer%2520Reflectometry%2520and%2520Advanced%2520Computation%2520Engine%2520%2528N-TRACE%2520%2529%252C%250Aa%2520neural%2520network%2520model%2520using%2520transformer%2520architecture%252C%2520is%2520introduced%2520for%250Aneutron%2520reflectometry%2520data%2520analysis.%2520It%2520offers%2520fast%252C%2520accurate%2520initial%2520parameter%250Aestimations%2520and%2520efficient%2520refinements%252C%2520improving%2520efficiency%2520and%2520precision%2520for%250Areal-time%2520data%2520analysis%2520of%2520lithium-mediated%2520nitrogen%2520reduction%2520for%250Aelectrochemical%2520ammonia%2520synthesis%252C%2520with%2520relevance%2520to%2520other%2520chemical%250Atransformations%2520and%2520batteries.%2520Despite%2520limitations%2520in%2520generalizing%2520across%250Asystems%252C%2520it%2520shows%2520promises%2520for%2520the%2520use%2520of%2520transformers%2520as%2520the%2520basis%2520for%2520models%250Athat%2520could%2520replace%2520trial-and-error%2520approaches%2520to%2520modeling%2520reflectometry%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16741v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extracting%20thin%20film%20structures%20of%20energy%20materials%20using%20transformers&entry.906535625=Chen%20Zhang%20and%20Valerie%20A.%20Niemann%20and%20Peter%20Benedek%20and%20Thomas%20F.%20Jaramillo%20and%20Mathieu%20Doucet&entry.1292438233=%20%20Neutron-Transformer%20Reflectometry%20and%20Advanced%20Computation%20Engine%20%28N-TRACE%20%29%2C%0Aa%20neural%20network%20model%20using%20transformer%20architecture%2C%20is%20introduced%20for%0Aneutron%20reflectometry%20data%20analysis.%20It%20offers%20fast%2C%20accurate%20initial%20parameter%0Aestimations%20and%20efficient%20refinements%2C%20improving%20efficiency%20and%20precision%20for%0Areal-time%20data%20analysis%20of%20lithium-mediated%20nitrogen%20reduction%20for%0Aelectrochemical%20ammonia%20synthesis%2C%20with%20relevance%20to%20other%20chemical%0Atransformations%20and%20batteries.%20Despite%20limitations%20in%20generalizing%20across%0Asystems%2C%20it%20shows%20promises%20for%20the%20use%20of%20transformers%20as%20the%20basis%20for%20models%0Athat%20could%20replace%20trial-and-error%20approaches%20to%20modeling%20reflectometry%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16741v2&entry.124074799=Read"},
{"title": "HiBO: Hierarchical Bayesian Optimization via Adaptive Search Space\n  Partitioning", "author": "Wenxuan Li and Taiyi Wang and Eiko Yoneki", "abstract": "  Optimizing black-box functions in high-dimensional search spaces has been\nknown to be challenging for traditional Bayesian Optimization (BO). In this\npaper, we introduce HiBO, a novel hierarchical algorithm integrating\nglobal-level search space partitioning information into the acquisition\nstrategy of a local BO-based optimizer. HiBO employs a search-tree-based\nglobal-level navigator to adaptively split the search space into partitions\nwith different sampling potential. The local optimizer then utilizes this\nglobal-level information to guide its acquisition strategy towards most\npromising regions within the search space. A comprehensive set of evaluations\ndemonstrates that HiBO outperforms state-of-the-art methods in high-dimensional\nsynthetic benchmarks and presents significant practical effectiveness in the\nreal-world task of tuning configurations of database management systems\n(DBMSs).\n", "link": "http://arxiv.org/abs/2410.23148v1", "date": "2024-10-30", "relevancy": 1.3577, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4698}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4529}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiBO%3A%20Hierarchical%20Bayesian%20Optimization%20via%20Adaptive%20Search%20Space%0A%20%20Partitioning&body=Title%3A%20HiBO%3A%20Hierarchical%20Bayesian%20Optimization%20via%20Adaptive%20Search%20Space%0A%20%20Partitioning%0AAuthor%3A%20Wenxuan%20Li%20and%20Taiyi%20Wang%20and%20Eiko%20Yoneki%0AAbstract%3A%20%20%20Optimizing%20black-box%20functions%20in%20high-dimensional%20search%20spaces%20has%20been%0Aknown%20to%20be%20challenging%20for%20traditional%20Bayesian%20Optimization%20%28BO%29.%20In%20this%0Apaper%2C%20we%20introduce%20HiBO%2C%20a%20novel%20hierarchical%20algorithm%20integrating%0Aglobal-level%20search%20space%20partitioning%20information%20into%20the%20acquisition%0Astrategy%20of%20a%20local%20BO-based%20optimizer.%20HiBO%20employs%20a%20search-tree-based%0Aglobal-level%20navigator%20to%20adaptively%20split%20the%20search%20space%20into%20partitions%0Awith%20different%20sampling%20potential.%20The%20local%20optimizer%20then%20utilizes%20this%0Aglobal-level%20information%20to%20guide%20its%20acquisition%20strategy%20towards%20most%0Apromising%20regions%20within%20the%20search%20space.%20A%20comprehensive%20set%20of%20evaluations%0Ademonstrates%20that%20HiBO%20outperforms%20state-of-the-art%20methods%20in%20high-dimensional%0Asynthetic%20benchmarks%20and%20presents%20significant%20practical%20effectiveness%20in%20the%0Areal-world%20task%20of%20tuning%20configurations%20of%20database%20management%20systems%0A%28DBMSs%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23148v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiBO%253A%2520Hierarchical%2520Bayesian%2520Optimization%2520via%2520Adaptive%2520Search%2520Space%250A%2520%2520Partitioning%26entry.906535625%3DWenxuan%2520Li%2520and%2520Taiyi%2520Wang%2520and%2520Eiko%2520Yoneki%26entry.1292438233%3D%2520%2520Optimizing%2520black-box%2520functions%2520in%2520high-dimensional%2520search%2520spaces%2520has%2520been%250Aknown%2520to%2520be%2520challenging%2520for%2520traditional%2520Bayesian%2520Optimization%2520%2528BO%2529.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520HiBO%252C%2520a%2520novel%2520hierarchical%2520algorithm%2520integrating%250Aglobal-level%2520search%2520space%2520partitioning%2520information%2520into%2520the%2520acquisition%250Astrategy%2520of%2520a%2520local%2520BO-based%2520optimizer.%2520HiBO%2520employs%2520a%2520search-tree-based%250Aglobal-level%2520navigator%2520to%2520adaptively%2520split%2520the%2520search%2520space%2520into%2520partitions%250Awith%2520different%2520sampling%2520potential.%2520The%2520local%2520optimizer%2520then%2520utilizes%2520this%250Aglobal-level%2520information%2520to%2520guide%2520its%2520acquisition%2520strategy%2520towards%2520most%250Apromising%2520regions%2520within%2520the%2520search%2520space.%2520A%2520comprehensive%2520set%2520of%2520evaluations%250Ademonstrates%2520that%2520HiBO%2520outperforms%2520state-of-the-art%2520methods%2520in%2520high-dimensional%250Asynthetic%2520benchmarks%2520and%2520presents%2520significant%2520practical%2520effectiveness%2520in%2520the%250Areal-world%2520task%2520of%2520tuning%2520configurations%2520of%2520database%2520management%2520systems%250A%2528DBMSs%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23148v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiBO%3A%20Hierarchical%20Bayesian%20Optimization%20via%20Adaptive%20Search%20Space%0A%20%20Partitioning&entry.906535625=Wenxuan%20Li%20and%20Taiyi%20Wang%20and%20Eiko%20Yoneki&entry.1292438233=%20%20Optimizing%20black-box%20functions%20in%20high-dimensional%20search%20spaces%20has%20been%0Aknown%20to%20be%20challenging%20for%20traditional%20Bayesian%20Optimization%20%28BO%29.%20In%20this%0Apaper%2C%20we%20introduce%20HiBO%2C%20a%20novel%20hierarchical%20algorithm%20integrating%0Aglobal-level%20search%20space%20partitioning%20information%20into%20the%20acquisition%0Astrategy%20of%20a%20local%20BO-based%20optimizer.%20HiBO%20employs%20a%20search-tree-based%0Aglobal-level%20navigator%20to%20adaptively%20split%20the%20search%20space%20into%20partitions%0Awith%20different%20sampling%20potential.%20The%20local%20optimizer%20then%20utilizes%20this%0Aglobal-level%20information%20to%20guide%20its%20acquisition%20strategy%20towards%20most%0Apromising%20regions%20within%20the%20search%20space.%20A%20comprehensive%20set%20of%20evaluations%0Ademonstrates%20that%20HiBO%20outperforms%20state-of-the-art%20methods%20in%20high-dimensional%0Asynthetic%20benchmarks%20and%20presents%20significant%20practical%20effectiveness%20in%20the%0Areal-world%20task%20of%20tuning%20configurations%20of%20database%20management%20systems%0A%28DBMSs%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23148v1&entry.124074799=Read"},
{"title": "$100K or 100 Days: Trade-offs when Pre-Training with Academic Resources", "author": "Apoorv Khandelwal and Tian Yun and Nihal V. Nayak and Jack Merullo and Stephen H. Bach and Chen Sun and Ellie Pavlick", "abstract": "  Pre-training is notoriously compute-intensive and academic researchers are\nnotoriously under-resourced. It is, therefore, commonly assumed that academics\ncan't pre-train models. In this paper, we seek to clarify this assumption. We\nfirst survey academic researchers to learn about their available compute and\nthen empirically measure the time to replicate models on such resources. We\nintroduce a benchmark to measure the time to pre-train models on given GPUs and\nalso identify ideal settings for maximizing training speed. We run our\nbenchmark on a range of models and academic GPUs, spending 2,000 GPU-hours on\nour experiments. Our results reveal a brighter picture for academic\npre-training: for example, although Pythia-1B was originally trained on 64 GPUs\nfor 3 days, we find it is also possible to replicate this model (with the same\nhyper-parameters) in 3x fewer GPU-days: i.e. on 4 GPUs in 18 days. We conclude\nwith a cost-benefit analysis to help clarify the trade-offs between price and\npre-training time. We believe our benchmark will help academic researchers\nconduct experiments that require training larger models on more data. We fully\nrelease our codebase at: https://github.com/apoorvkh/academic-pretraining.\n", "link": "http://arxiv.org/abs/2410.23261v1", "date": "2024-10-30", "relevancy": 1.3517, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.475}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4461}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24100K%20or%20100%20Days%3A%20Trade-offs%20when%20Pre-Training%20with%20Academic%20Resources&body=Title%3A%20%24100K%20or%20100%20Days%3A%20Trade-offs%20when%20Pre-Training%20with%20Academic%20Resources%0AAuthor%3A%20Apoorv%20Khandelwal%20and%20Tian%20Yun%20and%20Nihal%20V.%20Nayak%20and%20Jack%20Merullo%20and%20Stephen%20H.%20Bach%20and%20Chen%20Sun%20and%20Ellie%20Pavlick%0AAbstract%3A%20%20%20Pre-training%20is%20notoriously%20compute-intensive%20and%20academic%20researchers%20are%0Anotoriously%20under-resourced.%20It%20is%2C%20therefore%2C%20commonly%20assumed%20that%20academics%0Acan%27t%20pre-train%20models.%20In%20this%20paper%2C%20we%20seek%20to%20clarify%20this%20assumption.%20We%0Afirst%20survey%20academic%20researchers%20to%20learn%20about%20their%20available%20compute%20and%0Athen%20empirically%20measure%20the%20time%20to%20replicate%20models%20on%20such%20resources.%20We%0Aintroduce%20a%20benchmark%20to%20measure%20the%20time%20to%20pre-train%20models%20on%20given%20GPUs%20and%0Aalso%20identify%20ideal%20settings%20for%20maximizing%20training%20speed.%20We%20run%20our%0Abenchmark%20on%20a%20range%20of%20models%20and%20academic%20GPUs%2C%20spending%202%2C000%20GPU-hours%20on%0Aour%20experiments.%20Our%20results%20reveal%20a%20brighter%20picture%20for%20academic%0Apre-training%3A%20for%20example%2C%20although%20Pythia-1B%20was%20originally%20trained%20on%2064%20GPUs%0Afor%203%20days%2C%20we%20find%20it%20is%20also%20possible%20to%20replicate%20this%20model%20%28with%20the%20same%0Ahyper-parameters%29%20in%203x%20fewer%20GPU-days%3A%20i.e.%20on%204%20GPUs%20in%2018%20days.%20We%20conclude%0Awith%20a%20cost-benefit%20analysis%20to%20help%20clarify%20the%20trade-offs%20between%20price%20and%0Apre-training%20time.%20We%20believe%20our%20benchmark%20will%20help%20academic%20researchers%0Aconduct%20experiments%20that%20require%20training%20larger%20models%20on%20more%20data.%20We%20fully%0Arelease%20our%20codebase%20at%3A%20https%3A//github.com/apoorvkh/academic-pretraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23261v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524100K%2520or%2520100%2520Days%253A%2520Trade-offs%2520when%2520Pre-Training%2520with%2520Academic%2520Resources%26entry.906535625%3DApoorv%2520Khandelwal%2520and%2520Tian%2520Yun%2520and%2520Nihal%2520V.%2520Nayak%2520and%2520Jack%2520Merullo%2520and%2520Stephen%2520H.%2520Bach%2520and%2520Chen%2520Sun%2520and%2520Ellie%2520Pavlick%26entry.1292438233%3D%2520%2520Pre-training%2520is%2520notoriously%2520compute-intensive%2520and%2520academic%2520researchers%2520are%250Anotoriously%2520under-resourced.%2520It%2520is%252C%2520therefore%252C%2520commonly%2520assumed%2520that%2520academics%250Acan%2527t%2520pre-train%2520models.%2520In%2520this%2520paper%252C%2520we%2520seek%2520to%2520clarify%2520this%2520assumption.%2520We%250Afirst%2520survey%2520academic%2520researchers%2520to%2520learn%2520about%2520their%2520available%2520compute%2520and%250Athen%2520empirically%2520measure%2520the%2520time%2520to%2520replicate%2520models%2520on%2520such%2520resources.%2520We%250Aintroduce%2520a%2520benchmark%2520to%2520measure%2520the%2520time%2520to%2520pre-train%2520models%2520on%2520given%2520GPUs%2520and%250Aalso%2520identify%2520ideal%2520settings%2520for%2520maximizing%2520training%2520speed.%2520We%2520run%2520our%250Abenchmark%2520on%2520a%2520range%2520of%2520models%2520and%2520academic%2520GPUs%252C%2520spending%25202%252C000%2520GPU-hours%2520on%250Aour%2520experiments.%2520Our%2520results%2520reveal%2520a%2520brighter%2520picture%2520for%2520academic%250Apre-training%253A%2520for%2520example%252C%2520although%2520Pythia-1B%2520was%2520originally%2520trained%2520on%252064%2520GPUs%250Afor%25203%2520days%252C%2520we%2520find%2520it%2520is%2520also%2520possible%2520to%2520replicate%2520this%2520model%2520%2528with%2520the%2520same%250Ahyper-parameters%2529%2520in%25203x%2520fewer%2520GPU-days%253A%2520i.e.%2520on%25204%2520GPUs%2520in%252018%2520days.%2520We%2520conclude%250Awith%2520a%2520cost-benefit%2520analysis%2520to%2520help%2520clarify%2520the%2520trade-offs%2520between%2520price%2520and%250Apre-training%2520time.%2520We%2520believe%2520our%2520benchmark%2520will%2520help%2520academic%2520researchers%250Aconduct%2520experiments%2520that%2520require%2520training%2520larger%2520models%2520on%2520more%2520data.%2520We%2520fully%250Arelease%2520our%2520codebase%2520at%253A%2520https%253A//github.com/apoorvkh/academic-pretraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23261v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24100K%20or%20100%20Days%3A%20Trade-offs%20when%20Pre-Training%20with%20Academic%20Resources&entry.906535625=Apoorv%20Khandelwal%20and%20Tian%20Yun%20and%20Nihal%20V.%20Nayak%20and%20Jack%20Merullo%20and%20Stephen%20H.%20Bach%20and%20Chen%20Sun%20and%20Ellie%20Pavlick&entry.1292438233=%20%20Pre-training%20is%20notoriously%20compute-intensive%20and%20academic%20researchers%20are%0Anotoriously%20under-resourced.%20It%20is%2C%20therefore%2C%20commonly%20assumed%20that%20academics%0Acan%27t%20pre-train%20models.%20In%20this%20paper%2C%20we%20seek%20to%20clarify%20this%20assumption.%20We%0Afirst%20survey%20academic%20researchers%20to%20learn%20about%20their%20available%20compute%20and%0Athen%20empirically%20measure%20the%20time%20to%20replicate%20models%20on%20such%20resources.%20We%0Aintroduce%20a%20benchmark%20to%20measure%20the%20time%20to%20pre-train%20models%20on%20given%20GPUs%20and%0Aalso%20identify%20ideal%20settings%20for%20maximizing%20training%20speed.%20We%20run%20our%0Abenchmark%20on%20a%20range%20of%20models%20and%20academic%20GPUs%2C%20spending%202%2C000%20GPU-hours%20on%0Aour%20experiments.%20Our%20results%20reveal%20a%20brighter%20picture%20for%20academic%0Apre-training%3A%20for%20example%2C%20although%20Pythia-1B%20was%20originally%20trained%20on%2064%20GPUs%0Afor%203%20days%2C%20we%20find%20it%20is%20also%20possible%20to%20replicate%20this%20model%20%28with%20the%20same%0Ahyper-parameters%29%20in%203x%20fewer%20GPU-days%3A%20i.e.%20on%204%20GPUs%20in%2018%20days.%20We%20conclude%0Awith%20a%20cost-benefit%20analysis%20to%20help%20clarify%20the%20trade-offs%20between%20price%20and%0Apre-training%20time.%20We%20believe%20our%20benchmark%20will%20help%20academic%20researchers%0Aconduct%20experiments%20that%20require%20training%20larger%20models%20on%20more%20data.%20We%20fully%0Arelease%20our%20codebase%20at%3A%20https%3A//github.com/apoorvkh/academic-pretraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23261v1&entry.124074799=Read"},
{"title": "Certified Robustness to Data Poisoning in Gradient-Based Training", "author": "Philip Sosnin and Mark N. M\u00fcller and Maximilian Baader and Calvin Tsay and Matthew Wicker", "abstract": "  Modern machine learning pipelines leverage large amounts of public data,\nmaking it infeasible to guarantee data quality and leaving models open to\npoisoning and backdoor attacks. Provably bounding model behavior under such\nattacks remains an open problem. In this work, we address this challenge by\ndeveloping the first framework providing provable guarantees on the behavior of\nmodels trained with potentially manipulated data without modifying the model or\nlearning algorithm. In particular, our framework certifies robustness against\nuntargeted and targeted poisoning, as well as backdoor attacks, for bounded and\nunbounded manipulations of the training inputs and labels. Our method leverages\nconvex relaxations to over-approximate the set of all possible parameter\nupdates for a given poisoning threat model, allowing us to bound the set of all\nreachable parameters for any gradient-based learning algorithm. Given this set\nof parameters, we provide bounds on worst-case behavior, including model\nperformance and backdoor success rate. We demonstrate our approach on multiple\nreal-world datasets from applications including energy consumption, medical\nimaging, and autonomous driving.\n", "link": "http://arxiv.org/abs/2406.05670v2", "date": "2024-10-30", "relevancy": 0.9538, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4855}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4732}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Certified%20Robustness%20to%20Data%20Poisoning%20in%20Gradient-Based%20Training&body=Title%3A%20Certified%20Robustness%20to%20Data%20Poisoning%20in%20Gradient-Based%20Training%0AAuthor%3A%20Philip%20Sosnin%20and%20Mark%20N.%20M%C3%BCller%20and%20Maximilian%20Baader%20and%20Calvin%20Tsay%20and%20Matthew%20Wicker%0AAbstract%3A%20%20%20Modern%20machine%20learning%20pipelines%20leverage%20large%20amounts%20of%20public%20data%2C%0Amaking%20it%20infeasible%20to%20guarantee%20data%20quality%20and%20leaving%20models%20open%20to%0Apoisoning%20and%20backdoor%20attacks.%20Provably%20bounding%20model%20behavior%20under%20such%0Aattacks%20remains%20an%20open%20problem.%20In%20this%20work%2C%20we%20address%20this%20challenge%20by%0Adeveloping%20the%20first%20framework%20providing%20provable%20guarantees%20on%20the%20behavior%20of%0Amodels%20trained%20with%20potentially%20manipulated%20data%20without%20modifying%20the%20model%20or%0Alearning%20algorithm.%20In%20particular%2C%20our%20framework%20certifies%20robustness%20against%0Auntargeted%20and%20targeted%20poisoning%2C%20as%20well%20as%20backdoor%20attacks%2C%20for%20bounded%20and%0Aunbounded%20manipulations%20of%20the%20training%20inputs%20and%20labels.%20Our%20method%20leverages%0Aconvex%20relaxations%20to%20over-approximate%20the%20set%20of%20all%20possible%20parameter%0Aupdates%20for%20a%20given%20poisoning%20threat%20model%2C%20allowing%20us%20to%20bound%20the%20set%20of%20all%0Areachable%20parameters%20for%20any%20gradient-based%20learning%20algorithm.%20Given%20this%20set%0Aof%20parameters%2C%20we%20provide%20bounds%20on%20worst-case%20behavior%2C%20including%20model%0Aperformance%20and%20backdoor%20success%20rate.%20We%20demonstrate%20our%20approach%20on%20multiple%0Areal-world%20datasets%20from%20applications%20including%20energy%20consumption%2C%20medical%0Aimaging%2C%20and%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05670v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCertified%2520Robustness%2520to%2520Data%2520Poisoning%2520in%2520Gradient-Based%2520Training%26entry.906535625%3DPhilip%2520Sosnin%2520and%2520Mark%2520N.%2520M%25C3%25BCller%2520and%2520Maximilian%2520Baader%2520and%2520Calvin%2520Tsay%2520and%2520Matthew%2520Wicker%26entry.1292438233%3D%2520%2520Modern%2520machine%2520learning%2520pipelines%2520leverage%2520large%2520amounts%2520of%2520public%2520data%252C%250Amaking%2520it%2520infeasible%2520to%2520guarantee%2520data%2520quality%2520and%2520leaving%2520models%2520open%2520to%250Apoisoning%2520and%2520backdoor%2520attacks.%2520Provably%2520bounding%2520model%2520behavior%2520under%2520such%250Aattacks%2520remains%2520an%2520open%2520problem.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520challenge%2520by%250Adeveloping%2520the%2520first%2520framework%2520providing%2520provable%2520guarantees%2520on%2520the%2520behavior%2520of%250Amodels%2520trained%2520with%2520potentially%2520manipulated%2520data%2520without%2520modifying%2520the%2520model%2520or%250Alearning%2520algorithm.%2520In%2520particular%252C%2520our%2520framework%2520certifies%2520robustness%2520against%250Auntargeted%2520and%2520targeted%2520poisoning%252C%2520as%2520well%2520as%2520backdoor%2520attacks%252C%2520for%2520bounded%2520and%250Aunbounded%2520manipulations%2520of%2520the%2520training%2520inputs%2520and%2520labels.%2520Our%2520method%2520leverages%250Aconvex%2520relaxations%2520to%2520over-approximate%2520the%2520set%2520of%2520all%2520possible%2520parameter%250Aupdates%2520for%2520a%2520given%2520poisoning%2520threat%2520model%252C%2520allowing%2520us%2520to%2520bound%2520the%2520set%2520of%2520all%250Areachable%2520parameters%2520for%2520any%2520gradient-based%2520learning%2520algorithm.%2520Given%2520this%2520set%250Aof%2520parameters%252C%2520we%2520provide%2520bounds%2520on%2520worst-case%2520behavior%252C%2520including%2520model%250Aperformance%2520and%2520backdoor%2520success%2520rate.%2520We%2520demonstrate%2520our%2520approach%2520on%2520multiple%250Areal-world%2520datasets%2520from%2520applications%2520including%2520energy%2520consumption%252C%2520medical%250Aimaging%252C%2520and%2520autonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05670v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Certified%20Robustness%20to%20Data%20Poisoning%20in%20Gradient-Based%20Training&entry.906535625=Philip%20Sosnin%20and%20Mark%20N.%20M%C3%BCller%20and%20Maximilian%20Baader%20and%20Calvin%20Tsay%20and%20Matthew%20Wicker&entry.1292438233=%20%20Modern%20machine%20learning%20pipelines%20leverage%20large%20amounts%20of%20public%20data%2C%0Amaking%20it%20infeasible%20to%20guarantee%20data%20quality%20and%20leaving%20models%20open%20to%0Apoisoning%20and%20backdoor%20attacks.%20Provably%20bounding%20model%20behavior%20under%20such%0Aattacks%20remains%20an%20open%20problem.%20In%20this%20work%2C%20we%20address%20this%20challenge%20by%0Adeveloping%20the%20first%20framework%20providing%20provable%20guarantees%20on%20the%20behavior%20of%0Amodels%20trained%20with%20potentially%20manipulated%20data%20without%20modifying%20the%20model%20or%0Alearning%20algorithm.%20In%20particular%2C%20our%20framework%20certifies%20robustness%20against%0Auntargeted%20and%20targeted%20poisoning%2C%20as%20well%20as%20backdoor%20attacks%2C%20for%20bounded%20and%0Aunbounded%20manipulations%20of%20the%20training%20inputs%20and%20labels.%20Our%20method%20leverages%0Aconvex%20relaxations%20to%20over-approximate%20the%20set%20of%20all%20possible%20parameter%0Aupdates%20for%20a%20given%20poisoning%20threat%20model%2C%20allowing%20us%20to%20bound%20the%20set%20of%20all%0Areachable%20parameters%20for%20any%20gradient-based%20learning%20algorithm.%20Given%20this%20set%0Aof%20parameters%2C%20we%20provide%20bounds%20on%20worst-case%20behavior%2C%20including%20model%0Aperformance%20and%20backdoor%20success%20rate.%20We%20demonstrate%20our%20approach%20on%20multiple%0Areal-world%20datasets%20from%20applications%20including%20energy%20consumption%2C%20medical%0Aimaging%2C%20and%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05670v2&entry.124074799=Read"},
{"title": "SciPIP: An LLM-based Scientific Paper Idea Proposer", "author": "Wenxiao Wang and Lihui Gu and Liye Zhang and Yunxiang Luo and Yi Dai and Chen Shen and Liang Xie and Binbin Lin and Xiaofei He and Jieping Ye", "abstract": "  The exponential growth of knowledge and the increasing complexity of\ninterdisciplinary research pose significant challenges for researchers,\nincluding information overload and difficulties in exploring novel ideas. The\nadvancements in large language models (LLMs), such as GPT-4, have shown great\npotential in enhancing idea proposals, but how to effectively utilize large\nmodels for reasonable idea proposal has not been thoroughly explored. This\npaper proposes a scientific paper idea proposer (SciPIP). Based on a\nuser-provided research background, SciPIP retrieves helpful papers from a\nliterature database while leveraging the capabilities of LLMs to generate more\nnovel and feasible ideas. To this end, 1) we construct a literature retrieval\ndatabase, extracting lots of papers' multi-dimension information for fast\naccess. Then, a literature retrieval method based on semantics, entity, and\ncitation co-occurrences is proposed to search relevant literature from multiple\naspects based on the user-provided background. 2) After literature retrieval,\nwe introduce dual-path idea proposal strategies, where one path infers\nsolutions from the retrieved literature and the other path generates original\nideas through model brainstorming. We then combine the two to achieve a good\nbalance between feasibility and originality. Through extensive experiments on\nthe natural language processing (NLP) field, we demonstrate that SciPIP can\nretrieve citations similar to those of existing top conference papers and\ngenerate many ideas consistent with them. Additionally, we evaluate the\noriginality of other ideas generated by SciPIP using large language models,\nfurther validating the effectiveness of our proposed method. The code and the\ndatabase are released at https://github.com/cheerss/SciPIP.\n", "link": "http://arxiv.org/abs/2410.23166v1", "date": "2024-10-30", "relevancy": 0.8746, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4568}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4368}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SciPIP%3A%20An%20LLM-based%20Scientific%20Paper%20Idea%20Proposer&body=Title%3A%20SciPIP%3A%20An%20LLM-based%20Scientific%20Paper%20Idea%20Proposer%0AAuthor%3A%20Wenxiao%20Wang%20and%20Lihui%20Gu%20and%20Liye%20Zhang%20and%20Yunxiang%20Luo%20and%20Yi%20Dai%20and%20Chen%20Shen%20and%20Liang%20Xie%20and%20Binbin%20Lin%20and%20Xiaofei%20He%20and%20Jieping%20Ye%0AAbstract%3A%20%20%20The%20exponential%20growth%20of%20knowledge%20and%20the%20increasing%20complexity%20of%0Ainterdisciplinary%20research%20pose%20significant%20challenges%20for%20researchers%2C%0Aincluding%20information%20overload%20and%20difficulties%20in%20exploring%20novel%20ideas.%20The%0Aadvancements%20in%20large%20language%20models%20%28LLMs%29%2C%20such%20as%20GPT-4%2C%20have%20shown%20great%0Apotential%20in%20enhancing%20idea%20proposals%2C%20but%20how%20to%20effectively%20utilize%20large%0Amodels%20for%20reasonable%20idea%20proposal%20has%20not%20been%20thoroughly%20explored.%20This%0Apaper%20proposes%20a%20scientific%20paper%20idea%20proposer%20%28SciPIP%29.%20Based%20on%20a%0Auser-provided%20research%20background%2C%20SciPIP%20retrieves%20helpful%20papers%20from%20a%0Aliterature%20database%20while%20leveraging%20the%20capabilities%20of%20LLMs%20to%20generate%20more%0Anovel%20and%20feasible%20ideas.%20To%20this%20end%2C%201%29%20we%20construct%20a%20literature%20retrieval%0Adatabase%2C%20extracting%20lots%20of%20papers%27%20multi-dimension%20information%20for%20fast%0Aaccess.%20Then%2C%20a%20literature%20retrieval%20method%20based%20on%20semantics%2C%20entity%2C%20and%0Acitation%20co-occurrences%20is%20proposed%20to%20search%20relevant%20literature%20from%20multiple%0Aaspects%20based%20on%20the%20user-provided%20background.%202%29%20After%20literature%20retrieval%2C%0Awe%20introduce%20dual-path%20idea%20proposal%20strategies%2C%20where%20one%20path%20infers%0Asolutions%20from%20the%20retrieved%20literature%20and%20the%20other%20path%20generates%20original%0Aideas%20through%20model%20brainstorming.%20We%20then%20combine%20the%20two%20to%20achieve%20a%20good%0Abalance%20between%20feasibility%20and%20originality.%20Through%20extensive%20experiments%20on%0Athe%20natural%20language%20processing%20%28NLP%29%20field%2C%20we%20demonstrate%20that%20SciPIP%20can%0Aretrieve%20citations%20similar%20to%20those%20of%20existing%20top%20conference%20papers%20and%0Agenerate%20many%20ideas%20consistent%20with%20them.%20Additionally%2C%20we%20evaluate%20the%0Aoriginality%20of%20other%20ideas%20generated%20by%20SciPIP%20using%20large%20language%20models%2C%0Afurther%20validating%20the%20effectiveness%20of%20our%20proposed%20method.%20The%20code%20and%20the%0Adatabase%20are%20released%20at%20https%3A//github.com/cheerss/SciPIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23166v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSciPIP%253A%2520An%2520LLM-based%2520Scientific%2520Paper%2520Idea%2520Proposer%26entry.906535625%3DWenxiao%2520Wang%2520and%2520Lihui%2520Gu%2520and%2520Liye%2520Zhang%2520and%2520Yunxiang%2520Luo%2520and%2520Yi%2520Dai%2520and%2520Chen%2520Shen%2520and%2520Liang%2520Xie%2520and%2520Binbin%2520Lin%2520and%2520Xiaofei%2520He%2520and%2520Jieping%2520Ye%26entry.1292438233%3D%2520%2520The%2520exponential%2520growth%2520of%2520knowledge%2520and%2520the%2520increasing%2520complexity%2520of%250Ainterdisciplinary%2520research%2520pose%2520significant%2520challenges%2520for%2520researchers%252C%250Aincluding%2520information%2520overload%2520and%2520difficulties%2520in%2520exploring%2520novel%2520ideas.%2520The%250Aadvancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520such%2520as%2520GPT-4%252C%2520have%2520shown%2520great%250Apotential%2520in%2520enhancing%2520idea%2520proposals%252C%2520but%2520how%2520to%2520effectively%2520utilize%2520large%250Amodels%2520for%2520reasonable%2520idea%2520proposal%2520has%2520not%2520been%2520thoroughly%2520explored.%2520This%250Apaper%2520proposes%2520a%2520scientific%2520paper%2520idea%2520proposer%2520%2528SciPIP%2529.%2520Based%2520on%2520a%250Auser-provided%2520research%2520background%252C%2520SciPIP%2520retrieves%2520helpful%2520papers%2520from%2520a%250Aliterature%2520database%2520while%2520leveraging%2520the%2520capabilities%2520of%2520LLMs%2520to%2520generate%2520more%250Anovel%2520and%2520feasible%2520ideas.%2520To%2520this%2520end%252C%25201%2529%2520we%2520construct%2520a%2520literature%2520retrieval%250Adatabase%252C%2520extracting%2520lots%2520of%2520papers%2527%2520multi-dimension%2520information%2520for%2520fast%250Aaccess.%2520Then%252C%2520a%2520literature%2520retrieval%2520method%2520based%2520on%2520semantics%252C%2520entity%252C%2520and%250Acitation%2520co-occurrences%2520is%2520proposed%2520to%2520search%2520relevant%2520literature%2520from%2520multiple%250Aaspects%2520based%2520on%2520the%2520user-provided%2520background.%25202%2529%2520After%2520literature%2520retrieval%252C%250Awe%2520introduce%2520dual-path%2520idea%2520proposal%2520strategies%252C%2520where%2520one%2520path%2520infers%250Asolutions%2520from%2520the%2520retrieved%2520literature%2520and%2520the%2520other%2520path%2520generates%2520original%250Aideas%2520through%2520model%2520brainstorming.%2520We%2520then%2520combine%2520the%2520two%2520to%2520achieve%2520a%2520good%250Abalance%2520between%2520feasibility%2520and%2520originality.%2520Through%2520extensive%2520experiments%2520on%250Athe%2520natural%2520language%2520processing%2520%2528NLP%2529%2520field%252C%2520we%2520demonstrate%2520that%2520SciPIP%2520can%250Aretrieve%2520citations%2520similar%2520to%2520those%2520of%2520existing%2520top%2520conference%2520papers%2520and%250Agenerate%2520many%2520ideas%2520consistent%2520with%2520them.%2520Additionally%252C%2520we%2520evaluate%2520the%250Aoriginality%2520of%2520other%2520ideas%2520generated%2520by%2520SciPIP%2520using%2520large%2520language%2520models%252C%250Afurther%2520validating%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method.%2520The%2520code%2520and%2520the%250Adatabase%2520are%2520released%2520at%2520https%253A//github.com/cheerss/SciPIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23166v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SciPIP%3A%20An%20LLM-based%20Scientific%20Paper%20Idea%20Proposer&entry.906535625=Wenxiao%20Wang%20and%20Lihui%20Gu%20and%20Liye%20Zhang%20and%20Yunxiang%20Luo%20and%20Yi%20Dai%20and%20Chen%20Shen%20and%20Liang%20Xie%20and%20Binbin%20Lin%20and%20Xiaofei%20He%20and%20Jieping%20Ye&entry.1292438233=%20%20The%20exponential%20growth%20of%20knowledge%20and%20the%20increasing%20complexity%20of%0Ainterdisciplinary%20research%20pose%20significant%20challenges%20for%20researchers%2C%0Aincluding%20information%20overload%20and%20difficulties%20in%20exploring%20novel%20ideas.%20The%0Aadvancements%20in%20large%20language%20models%20%28LLMs%29%2C%20such%20as%20GPT-4%2C%20have%20shown%20great%0Apotential%20in%20enhancing%20idea%20proposals%2C%20but%20how%20to%20effectively%20utilize%20large%0Amodels%20for%20reasonable%20idea%20proposal%20has%20not%20been%20thoroughly%20explored.%20This%0Apaper%20proposes%20a%20scientific%20paper%20idea%20proposer%20%28SciPIP%29.%20Based%20on%20a%0Auser-provided%20research%20background%2C%20SciPIP%20retrieves%20helpful%20papers%20from%20a%0Aliterature%20database%20while%20leveraging%20the%20capabilities%20of%20LLMs%20to%20generate%20more%0Anovel%20and%20feasible%20ideas.%20To%20this%20end%2C%201%29%20we%20construct%20a%20literature%20retrieval%0Adatabase%2C%20extracting%20lots%20of%20papers%27%20multi-dimension%20information%20for%20fast%0Aaccess.%20Then%2C%20a%20literature%20retrieval%20method%20based%20on%20semantics%2C%20entity%2C%20and%0Acitation%20co-occurrences%20is%20proposed%20to%20search%20relevant%20literature%20from%20multiple%0Aaspects%20based%20on%20the%20user-provided%20background.%202%29%20After%20literature%20retrieval%2C%0Awe%20introduce%20dual-path%20idea%20proposal%20strategies%2C%20where%20one%20path%20infers%0Asolutions%20from%20the%20retrieved%20literature%20and%20the%20other%20path%20generates%20original%0Aideas%20through%20model%20brainstorming.%20We%20then%20combine%20the%20two%20to%20achieve%20a%20good%0Abalance%20between%20feasibility%20and%20originality.%20Through%20extensive%20experiments%20on%0Athe%20natural%20language%20processing%20%28NLP%29%20field%2C%20we%20demonstrate%20that%20SciPIP%20can%0Aretrieve%20citations%20similar%20to%20those%20of%20existing%20top%20conference%20papers%20and%0Agenerate%20many%20ideas%20consistent%20with%20them.%20Additionally%2C%20we%20evaluate%20the%0Aoriginality%20of%20other%20ideas%20generated%20by%20SciPIP%20using%20large%20language%20models%2C%0Afurther%20validating%20the%20effectiveness%20of%20our%20proposed%20method.%20The%20code%20and%20the%0Adatabase%20are%20released%20at%20https%3A//github.com/cheerss/SciPIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23166v1&entry.124074799=Read"},
{"title": "Public Domain 12M: A Highly Aesthetic Image-Text Dataset with Novel\n  Governance Mechanisms", "author": "Jordan Meyer and Nick Padgett and Cullen Miller and Laura Exline", "abstract": "  We present Public Domain 12M (PD12M), a dataset of 12.4 million high-quality\npublic domain and CC0-licensed images with synthetic captions, designed for\ntraining text-to-image models. PD12M is the largest public domain image-text\ndataset to date, with sufficient size to train foundation models while\nminimizing copyright concerns. Through the Source.Plus platform, we also\nintroduce novel, community-driven dataset governance mechanisms that reduce\nharm and support reproducibility over time.\n", "link": "http://arxiv.org/abs/2410.23144v1", "date": "2024-10-30", "relevancy": 1.0094, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5171}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4986}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Public%20Domain%2012M%3A%20A%20Highly%20Aesthetic%20Image-Text%20Dataset%20with%20Novel%0A%20%20Governance%20Mechanisms&body=Title%3A%20Public%20Domain%2012M%3A%20A%20Highly%20Aesthetic%20Image-Text%20Dataset%20with%20Novel%0A%20%20Governance%20Mechanisms%0AAuthor%3A%20Jordan%20Meyer%20and%20Nick%20Padgett%20and%20Cullen%20Miller%20and%20Laura%20Exline%0AAbstract%3A%20%20%20We%20present%20Public%20Domain%2012M%20%28PD12M%29%2C%20a%20dataset%20of%2012.4%20million%20high-quality%0Apublic%20domain%20and%20CC0-licensed%20images%20with%20synthetic%20captions%2C%20designed%20for%0Atraining%20text-to-image%20models.%20PD12M%20is%20the%20largest%20public%20domain%20image-text%0Adataset%20to%20date%2C%20with%20sufficient%20size%20to%20train%20foundation%20models%20while%0Aminimizing%20copyright%20concerns.%20Through%20the%20Source.Plus%20platform%2C%20we%20also%0Aintroduce%20novel%2C%20community-driven%20dataset%20governance%20mechanisms%20that%20reduce%0Aharm%20and%20support%20reproducibility%20over%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23144v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPublic%2520Domain%252012M%253A%2520A%2520Highly%2520Aesthetic%2520Image-Text%2520Dataset%2520with%2520Novel%250A%2520%2520Governance%2520Mechanisms%26entry.906535625%3DJordan%2520Meyer%2520and%2520Nick%2520Padgett%2520and%2520Cullen%2520Miller%2520and%2520Laura%2520Exline%26entry.1292438233%3D%2520%2520We%2520present%2520Public%2520Domain%252012M%2520%2528PD12M%2529%252C%2520a%2520dataset%2520of%252012.4%2520million%2520high-quality%250Apublic%2520domain%2520and%2520CC0-licensed%2520images%2520with%2520synthetic%2520captions%252C%2520designed%2520for%250Atraining%2520text-to-image%2520models.%2520PD12M%2520is%2520the%2520largest%2520public%2520domain%2520image-text%250Adataset%2520to%2520date%252C%2520with%2520sufficient%2520size%2520to%2520train%2520foundation%2520models%2520while%250Aminimizing%2520copyright%2520concerns.%2520Through%2520the%2520Source.Plus%2520platform%252C%2520we%2520also%250Aintroduce%2520novel%252C%2520community-driven%2520dataset%2520governance%2520mechanisms%2520that%2520reduce%250Aharm%2520and%2520support%2520reproducibility%2520over%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23144v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Public%20Domain%2012M%3A%20A%20Highly%20Aesthetic%20Image-Text%20Dataset%20with%20Novel%0A%20%20Governance%20Mechanisms&entry.906535625=Jordan%20Meyer%20and%20Nick%20Padgett%20and%20Cullen%20Miller%20and%20Laura%20Exline&entry.1292438233=%20%20We%20present%20Public%20Domain%2012M%20%28PD12M%29%2C%20a%20dataset%20of%2012.4%20million%20high-quality%0Apublic%20domain%20and%20CC0-licensed%20images%20with%20synthetic%20captions%2C%20designed%20for%0Atraining%20text-to-image%20models.%20PD12M%20is%20the%20largest%20public%20domain%20image-text%0Adataset%20to%20date%2C%20with%20sufficient%20size%20to%20train%20foundation%20models%20while%0Aminimizing%20copyright%20concerns.%20Through%20the%20Source.Plus%20platform%2C%20we%20also%0Aintroduce%20novel%2C%20community-driven%20dataset%20governance%20mechanisms%20that%20reduce%0Aharm%20and%20support%20reproducibility%20over%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23144v1&entry.124074799=Read"},
{"title": "Partial Channel Dependence with Channel Masks for Time Series Foundation\n  Models", "author": "Seunghan Lee and Taeyoung Park and Kibok Lee", "abstract": "  Recent advancements in foundation models have been successfully extended to\nthe time series (TS) domain, facilitated by the emergence of large-scale TS\ndatasets. However, previous efforts have primarily focused on designing model\narchitectures to address explicit heterogeneity among datasets such as various\nnumbers of channels, while often overlooking implicit heterogeneity such as\nvarying dependencies between channels. In this work, we introduce the concept\nof partial channel dependence (PCD), which enables a more sophisticated\nadjustment of channel dependencies based on dataset-specific information. To\nachieve PCD, we propose a channel mask that captures the relationships between\nchannels within a dataset using two key components: 1) a correlation matrix\nthat encodes relative dependencies between channels, and 2) domain parameters\nthat learn the absolute dependencies specific to each dataset, refining the\ncorrelation matrix. We validate the effectiveness of PCD across four tasks in\nTS including forecasting, classification, imputation, and anomaly detection,\nunder diverse settings, including few-shot and zero-shot scenarios with both TS\nfoundation models and single-task models. Code is available at\nhttps://github.com/seunghan96/CM.\n", "link": "http://arxiv.org/abs/2410.23222v1", "date": "2024-10-30", "relevancy": 0.9461, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4748}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4743}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Partial%20Channel%20Dependence%20with%20Channel%20Masks%20for%20Time%20Series%20Foundation%0A%20%20Models&body=Title%3A%20Partial%20Channel%20Dependence%20with%20Channel%20Masks%20for%20Time%20Series%20Foundation%0A%20%20Models%0AAuthor%3A%20Seunghan%20Lee%20and%20Taeyoung%20Park%20and%20Kibok%20Lee%0AAbstract%3A%20%20%20Recent%20advancements%20in%20foundation%20models%20have%20been%20successfully%20extended%20to%0Athe%20time%20series%20%28TS%29%20domain%2C%20facilitated%20by%20the%20emergence%20of%20large-scale%20TS%0Adatasets.%20However%2C%20previous%20efforts%20have%20primarily%20focused%20on%20designing%20model%0Aarchitectures%20to%20address%20explicit%20heterogeneity%20among%20datasets%20such%20as%20various%0Anumbers%20of%20channels%2C%20while%20often%20overlooking%20implicit%20heterogeneity%20such%20as%0Avarying%20dependencies%20between%20channels.%20In%20this%20work%2C%20we%20introduce%20the%20concept%0Aof%20partial%20channel%20dependence%20%28PCD%29%2C%20which%20enables%20a%20more%20sophisticated%0Aadjustment%20of%20channel%20dependencies%20based%20on%20dataset-specific%20information.%20To%0Aachieve%20PCD%2C%20we%20propose%20a%20channel%20mask%20that%20captures%20the%20relationships%20between%0Achannels%20within%20a%20dataset%20using%20two%20key%20components%3A%201%29%20a%20correlation%20matrix%0Athat%20encodes%20relative%20dependencies%20between%20channels%2C%20and%202%29%20domain%20parameters%0Athat%20learn%20the%20absolute%20dependencies%20specific%20to%20each%20dataset%2C%20refining%20the%0Acorrelation%20matrix.%20We%20validate%20the%20effectiveness%20of%20PCD%20across%20four%20tasks%20in%0ATS%20including%20forecasting%2C%20classification%2C%20imputation%2C%20and%20anomaly%20detection%2C%0Aunder%20diverse%20settings%2C%20including%20few-shot%20and%20zero-shot%20scenarios%20with%20both%20TS%0Afoundation%20models%20and%20single-task%20models.%20Code%20is%20available%20at%0Ahttps%3A//github.com/seunghan96/CM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23222v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartial%2520Channel%2520Dependence%2520with%2520Channel%2520Masks%2520for%2520Time%2520Series%2520Foundation%250A%2520%2520Models%26entry.906535625%3DSeunghan%2520Lee%2520and%2520Taeyoung%2520Park%2520and%2520Kibok%2520Lee%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520foundation%2520models%2520have%2520been%2520successfully%2520extended%2520to%250Athe%2520time%2520series%2520%2528TS%2529%2520domain%252C%2520facilitated%2520by%2520the%2520emergence%2520of%2520large-scale%2520TS%250Adatasets.%2520However%252C%2520previous%2520efforts%2520have%2520primarily%2520focused%2520on%2520designing%2520model%250Aarchitectures%2520to%2520address%2520explicit%2520heterogeneity%2520among%2520datasets%2520such%2520as%2520various%250Anumbers%2520of%2520channels%252C%2520while%2520often%2520overlooking%2520implicit%2520heterogeneity%2520such%2520as%250Avarying%2520dependencies%2520between%2520channels.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520concept%250Aof%2520partial%2520channel%2520dependence%2520%2528PCD%2529%252C%2520which%2520enables%2520a%2520more%2520sophisticated%250Aadjustment%2520of%2520channel%2520dependencies%2520based%2520on%2520dataset-specific%2520information.%2520To%250Aachieve%2520PCD%252C%2520we%2520propose%2520a%2520channel%2520mask%2520that%2520captures%2520the%2520relationships%2520between%250Achannels%2520within%2520a%2520dataset%2520using%2520two%2520key%2520components%253A%25201%2529%2520a%2520correlation%2520matrix%250Athat%2520encodes%2520relative%2520dependencies%2520between%2520channels%252C%2520and%25202%2529%2520domain%2520parameters%250Athat%2520learn%2520the%2520absolute%2520dependencies%2520specific%2520to%2520each%2520dataset%252C%2520refining%2520the%250Acorrelation%2520matrix.%2520We%2520validate%2520the%2520effectiveness%2520of%2520PCD%2520across%2520four%2520tasks%2520in%250ATS%2520including%2520forecasting%252C%2520classification%252C%2520imputation%252C%2520and%2520anomaly%2520detection%252C%250Aunder%2520diverse%2520settings%252C%2520including%2520few-shot%2520and%2520zero-shot%2520scenarios%2520with%2520both%2520TS%250Afoundation%2520models%2520and%2520single-task%2520models.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/seunghan96/CM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23222v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Partial%20Channel%20Dependence%20with%20Channel%20Masks%20for%20Time%20Series%20Foundation%0A%20%20Models&entry.906535625=Seunghan%20Lee%20and%20Taeyoung%20Park%20and%20Kibok%20Lee&entry.1292438233=%20%20Recent%20advancements%20in%20foundation%20models%20have%20been%20successfully%20extended%20to%0Athe%20time%20series%20%28TS%29%20domain%2C%20facilitated%20by%20the%20emergence%20of%20large-scale%20TS%0Adatasets.%20However%2C%20previous%20efforts%20have%20primarily%20focused%20on%20designing%20model%0Aarchitectures%20to%20address%20explicit%20heterogeneity%20among%20datasets%20such%20as%20various%0Anumbers%20of%20channels%2C%20while%20often%20overlooking%20implicit%20heterogeneity%20such%20as%0Avarying%20dependencies%20between%20channels.%20In%20this%20work%2C%20we%20introduce%20the%20concept%0Aof%20partial%20channel%20dependence%20%28PCD%29%2C%20which%20enables%20a%20more%20sophisticated%0Aadjustment%20of%20channel%20dependencies%20based%20on%20dataset-specific%20information.%20To%0Aachieve%20PCD%2C%20we%20propose%20a%20channel%20mask%20that%20captures%20the%20relationships%20between%0Achannels%20within%20a%20dataset%20using%20two%20key%20components%3A%201%29%20a%20correlation%20matrix%0Athat%20encodes%20relative%20dependencies%20between%20channels%2C%20and%202%29%20domain%20parameters%0Athat%20learn%20the%20absolute%20dependencies%20specific%20to%20each%20dataset%2C%20refining%20the%0Acorrelation%20matrix.%20We%20validate%20the%20effectiveness%20of%20PCD%20across%20four%20tasks%20in%0ATS%20including%20forecasting%2C%20classification%2C%20imputation%2C%20and%20anomaly%20detection%2C%0Aunder%20diverse%20settings%2C%20including%20few-shot%20and%20zero-shot%20scenarios%20with%20both%20TS%0Afoundation%20models%20and%20single-task%20models.%20Code%20is%20available%20at%0Ahttps%3A//github.com/seunghan96/CM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23222v1&entry.124074799=Read"},
{"title": "The Good, the Bad, and the Ugly: The Role of AI Quality Disclosure in\n  Lie Detection", "author": "Haimanti Bhattacharya and Subhasish Dugar and Sanchaita Hazra and Bodhisattwa Prasad Majumder", "abstract": "  We investigate how low-quality AI advisors, lacking quality disclosures, can\nhelp spread text-based lies while seeming to help people detect lies.\nParticipants in our experiment discern truth from lies by evaluating\ntranscripts from a game show that mimicked deceptive social media exchanges on\ntopics with objective truths. We find that when relying on low-quality advisors\nwithout disclosures, participants' truth-detection rates fall below their own\nabilities, which recovered once the AI's true effectiveness was revealed.\nConversely, high-quality advisor enhances truth detection, regardless of\ndisclosure. We discover that participants' expectations about AI capabilities\ncontribute to their undue reliance on opaque, low-quality advisors.\n", "link": "http://arxiv.org/abs/2410.23143v1", "date": "2024-10-30", "relevancy": 1.2819, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4549}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4277}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Good%2C%20the%20Bad%2C%20and%20the%20Ugly%3A%20The%20Role%20of%20AI%20Quality%20Disclosure%20in%0A%20%20Lie%20Detection&body=Title%3A%20The%20Good%2C%20the%20Bad%2C%20and%20the%20Ugly%3A%20The%20Role%20of%20AI%20Quality%20Disclosure%20in%0A%20%20Lie%20Detection%0AAuthor%3A%20Haimanti%20Bhattacharya%20and%20Subhasish%20Dugar%20and%20Sanchaita%20Hazra%20and%20Bodhisattwa%20Prasad%20Majumder%0AAbstract%3A%20%20%20We%20investigate%20how%20low-quality%20AI%20advisors%2C%20lacking%20quality%20disclosures%2C%20can%0Ahelp%20spread%20text-based%20lies%20while%20seeming%20to%20help%20people%20detect%20lies.%0AParticipants%20in%20our%20experiment%20discern%20truth%20from%20lies%20by%20evaluating%0Atranscripts%20from%20a%20game%20show%20that%20mimicked%20deceptive%20social%20media%20exchanges%20on%0Atopics%20with%20objective%20truths.%20We%20find%20that%20when%20relying%20on%20low-quality%20advisors%0Awithout%20disclosures%2C%20participants%27%20truth-detection%20rates%20fall%20below%20their%20own%0Aabilities%2C%20which%20recovered%20once%20the%20AI%27s%20true%20effectiveness%20was%20revealed.%0AConversely%2C%20high-quality%20advisor%20enhances%20truth%20detection%2C%20regardless%20of%0Adisclosure.%20We%20discover%20that%20participants%27%20expectations%20about%20AI%20capabilities%0Acontribute%20to%20their%20undue%20reliance%20on%20opaque%2C%20low-quality%20advisors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Good%252C%2520the%2520Bad%252C%2520and%2520the%2520Ugly%253A%2520The%2520Role%2520of%2520AI%2520Quality%2520Disclosure%2520in%250A%2520%2520Lie%2520Detection%26entry.906535625%3DHaimanti%2520Bhattacharya%2520and%2520Subhasish%2520Dugar%2520and%2520Sanchaita%2520Hazra%2520and%2520Bodhisattwa%2520Prasad%2520Majumder%26entry.1292438233%3D%2520%2520We%2520investigate%2520how%2520low-quality%2520AI%2520advisors%252C%2520lacking%2520quality%2520disclosures%252C%2520can%250Ahelp%2520spread%2520text-based%2520lies%2520while%2520seeming%2520to%2520help%2520people%2520detect%2520lies.%250AParticipants%2520in%2520our%2520experiment%2520discern%2520truth%2520from%2520lies%2520by%2520evaluating%250Atranscripts%2520from%2520a%2520game%2520show%2520that%2520mimicked%2520deceptive%2520social%2520media%2520exchanges%2520on%250Atopics%2520with%2520objective%2520truths.%2520We%2520find%2520that%2520when%2520relying%2520on%2520low-quality%2520advisors%250Awithout%2520disclosures%252C%2520participants%2527%2520truth-detection%2520rates%2520fall%2520below%2520their%2520own%250Aabilities%252C%2520which%2520recovered%2520once%2520the%2520AI%2527s%2520true%2520effectiveness%2520was%2520revealed.%250AConversely%252C%2520high-quality%2520advisor%2520enhances%2520truth%2520detection%252C%2520regardless%2520of%250Adisclosure.%2520We%2520discover%2520that%2520participants%2527%2520expectations%2520about%2520AI%2520capabilities%250Acontribute%2520to%2520their%2520undue%2520reliance%2520on%2520opaque%252C%2520low-quality%2520advisors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Good%2C%20the%20Bad%2C%20and%20the%20Ugly%3A%20The%20Role%20of%20AI%20Quality%20Disclosure%20in%0A%20%20Lie%20Detection&entry.906535625=Haimanti%20Bhattacharya%20and%20Subhasish%20Dugar%20and%20Sanchaita%20Hazra%20and%20Bodhisattwa%20Prasad%20Majumder&entry.1292438233=%20%20We%20investigate%20how%20low-quality%20AI%20advisors%2C%20lacking%20quality%20disclosures%2C%20can%0Ahelp%20spread%20text-based%20lies%20while%20seeming%20to%20help%20people%20detect%20lies.%0AParticipants%20in%20our%20experiment%20discern%20truth%20from%20lies%20by%20evaluating%0Atranscripts%20from%20a%20game%20show%20that%20mimicked%20deceptive%20social%20media%20exchanges%20on%0Atopics%20with%20objective%20truths.%20We%20find%20that%20when%20relying%20on%20low-quality%20advisors%0Awithout%20disclosures%2C%20participants%27%20truth-detection%20rates%20fall%20below%20their%20own%0Aabilities%2C%20which%20recovered%20once%20the%20AI%27s%20true%20effectiveness%20was%20revealed.%0AConversely%2C%20high-quality%20advisor%20enhances%20truth%20detection%2C%20regardless%20of%0Adisclosure.%20We%20discover%20that%20participants%27%20expectations%20about%20AI%20capabilities%0Acontribute%20to%20their%20undue%20reliance%20on%20opaque%2C%20low-quality%20advisors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23143v1&entry.124074799=Read"},
{"title": "Joint Estimation of Conditional Mean and Covariance for Unbalanced\n  Panels", "author": "Damir Filipovic and Paul Schneider", "abstract": "  We propose a novel nonparametric kernel-based estimator of cross-sectional\nconditional mean and covariance matrices for large unbalanced panels. We show\nits consistency and provide finite-sample guarantees. In an empirical\napplication, we estimate conditional mean and covariance matrices for a large\nunbalanced panel of monthly stock excess returns given macroeconomic and\nfirm-specific covariates from 1962 to 2021.The estimator performs well with\nrespect to statistical measures. It is informative for empirical asset pricing,\ngenerating conditional mean-variance efficient portfolios with substantial\nout-of-sample Sharpe ratios far beyond equal-weighted benchmarks.\n", "link": "http://arxiv.org/abs/2410.21858v2", "date": "2024-10-30", "relevancy": 1.2068, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.433}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3963}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Estimation%20of%20Conditional%20Mean%20and%20Covariance%20for%20Unbalanced%0A%20%20Panels&body=Title%3A%20Joint%20Estimation%20of%20Conditional%20Mean%20and%20Covariance%20for%20Unbalanced%0A%20%20Panels%0AAuthor%3A%20Damir%20Filipovic%20and%20Paul%20Schneider%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20nonparametric%20kernel-based%20estimator%20of%20cross-sectional%0Aconditional%20mean%20and%20covariance%20matrices%20for%20large%20unbalanced%20panels.%20We%20show%0Aits%20consistency%20and%20provide%20finite-sample%20guarantees.%20In%20an%20empirical%0Aapplication%2C%20we%20estimate%20conditional%20mean%20and%20covariance%20matrices%20for%20a%20large%0Aunbalanced%20panel%20of%20monthly%20stock%20excess%20returns%20given%20macroeconomic%20and%0Afirm-specific%20covariates%20from%201962%20to%202021.The%20estimator%20performs%20well%20with%0Arespect%20to%20statistical%20measures.%20It%20is%20informative%20for%20empirical%20asset%20pricing%2C%0Agenerating%20conditional%20mean-variance%20efficient%20portfolios%20with%20substantial%0Aout-of-sample%20Sharpe%20ratios%20far%20beyond%20equal-weighted%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21858v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Estimation%2520of%2520Conditional%2520Mean%2520and%2520Covariance%2520for%2520Unbalanced%250A%2520%2520Panels%26entry.906535625%3DDamir%2520Filipovic%2520and%2520Paul%2520Schneider%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520nonparametric%2520kernel-based%2520estimator%2520of%2520cross-sectional%250Aconditional%2520mean%2520and%2520covariance%2520matrices%2520for%2520large%2520unbalanced%2520panels.%2520We%2520show%250Aits%2520consistency%2520and%2520provide%2520finite-sample%2520guarantees.%2520In%2520an%2520empirical%250Aapplication%252C%2520we%2520estimate%2520conditional%2520mean%2520and%2520covariance%2520matrices%2520for%2520a%2520large%250Aunbalanced%2520panel%2520of%2520monthly%2520stock%2520excess%2520returns%2520given%2520macroeconomic%2520and%250Afirm-specific%2520covariates%2520from%25201962%2520to%25202021.The%2520estimator%2520performs%2520well%2520with%250Arespect%2520to%2520statistical%2520measures.%2520It%2520is%2520informative%2520for%2520empirical%2520asset%2520pricing%252C%250Agenerating%2520conditional%2520mean-variance%2520efficient%2520portfolios%2520with%2520substantial%250Aout-of-sample%2520Sharpe%2520ratios%2520far%2520beyond%2520equal-weighted%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21858v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Estimation%20of%20Conditional%20Mean%20and%20Covariance%20for%20Unbalanced%0A%20%20Panels&entry.906535625=Damir%20Filipovic%20and%20Paul%20Schneider&entry.1292438233=%20%20We%20propose%20a%20novel%20nonparametric%20kernel-based%20estimator%20of%20cross-sectional%0Aconditional%20mean%20and%20covariance%20matrices%20for%20large%20unbalanced%20panels.%20We%20show%0Aits%20consistency%20and%20provide%20finite-sample%20guarantees.%20In%20an%20empirical%0Aapplication%2C%20we%20estimate%20conditional%20mean%20and%20covariance%20matrices%20for%20a%20large%0Aunbalanced%20panel%20of%20monthly%20stock%20excess%20returns%20given%20macroeconomic%20and%0Afirm-specific%20covariates%20from%201962%20to%202021.The%20estimator%20performs%20well%20with%0Arespect%20to%20statistical%20measures.%20It%20is%20informative%20for%20empirical%20asset%20pricing%2C%0Agenerating%20conditional%20mean-variance%20efficient%20portfolios%20with%20substantial%0Aout-of-sample%20Sharpe%20ratios%20far%20beyond%20equal-weighted%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21858v2&entry.124074799=Read"},
{"title": "Functional Gradient Flows for Constrained Sampling", "author": "Shiyue Zhang and Longlin Yu and Ziheng Cheng and Cheng Zhang", "abstract": "  Recently, through a unified gradient flow perspective of Markov chain Monte\nCarlo (MCMC) and variational inference (VI), particle-based variational\ninference methods (ParVIs) have been proposed that tend to combine the best of\nboth worlds. While typical ParVIs such as Stein Variational Gradient Descent\n(SVGD) approximate the gradient flow within a reproducing kernel Hilbert space\n(RKHS), many attempts have been made recently to replace RKHS with more\nexpressive function spaces, such as neural networks. While successful, these\nmethods are mainly designed for sampling from unconstrained domains. In this\npaper, we offer a general solution to constrained sampling by introducing a\nboundary condition for the gradient flow which would confine the particles\nwithin the specific domain. This allows us to propose a new functional gradient\nParVI method for constrained sampling, called constrained functional gradient\nflow (CFG), with provable continuous-time convergence in total variation (TV).\nWe also present novel numerical strategies to handle the boundary integral term\narising from the domain constraints. Our theory and experiments demonstrate the\neffectiveness of the proposed framework.\n", "link": "http://arxiv.org/abs/2410.23170v1", "date": "2024-10-30", "relevancy": 0.9919, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5237}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4831}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Functional%20Gradient%20Flows%20for%20Constrained%20Sampling&body=Title%3A%20Functional%20Gradient%20Flows%20for%20Constrained%20Sampling%0AAuthor%3A%20Shiyue%20Zhang%20and%20Longlin%20Yu%20and%20Ziheng%20Cheng%20and%20Cheng%20Zhang%0AAbstract%3A%20%20%20Recently%2C%20through%20a%20unified%20gradient%20flow%20perspective%20of%20Markov%20chain%20Monte%0ACarlo%20%28MCMC%29%20and%20variational%20inference%20%28VI%29%2C%20particle-based%20variational%0Ainference%20methods%20%28ParVIs%29%20have%20been%20proposed%20that%20tend%20to%20combine%20the%20best%20of%0Aboth%20worlds.%20While%20typical%20ParVIs%20such%20as%20Stein%20Variational%20Gradient%20Descent%0A%28SVGD%29%20approximate%20the%20gradient%20flow%20within%20a%20reproducing%20kernel%20Hilbert%20space%0A%28RKHS%29%2C%20many%20attempts%20have%20been%20made%20recently%20to%20replace%20RKHS%20with%20more%0Aexpressive%20function%20spaces%2C%20such%20as%20neural%20networks.%20While%20successful%2C%20these%0Amethods%20are%20mainly%20designed%20for%20sampling%20from%20unconstrained%20domains.%20In%20this%0Apaper%2C%20we%20offer%20a%20general%20solution%20to%20constrained%20sampling%20by%20introducing%20a%0Aboundary%20condition%20for%20the%20gradient%20flow%20which%20would%20confine%20the%20particles%0Awithin%20the%20specific%20domain.%20This%20allows%20us%20to%20propose%20a%20new%20functional%20gradient%0AParVI%20method%20for%20constrained%20sampling%2C%20called%20constrained%20functional%20gradient%0Aflow%20%28CFG%29%2C%20with%20provable%20continuous-time%20convergence%20in%20total%20variation%20%28TV%29.%0AWe%20also%20present%20novel%20numerical%20strategies%20to%20handle%20the%20boundary%20integral%20term%0Aarising%20from%20the%20domain%20constraints.%20Our%20theory%20and%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23170v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFunctional%2520Gradient%2520Flows%2520for%2520Constrained%2520Sampling%26entry.906535625%3DShiyue%2520Zhang%2520and%2520Longlin%2520Yu%2520and%2520Ziheng%2520Cheng%2520and%2520Cheng%2520Zhang%26entry.1292438233%3D%2520%2520Recently%252C%2520through%2520a%2520unified%2520gradient%2520flow%2520perspective%2520of%2520Markov%2520chain%2520Monte%250ACarlo%2520%2528MCMC%2529%2520and%2520variational%2520inference%2520%2528VI%2529%252C%2520particle-based%2520variational%250Ainference%2520methods%2520%2528ParVIs%2529%2520have%2520been%2520proposed%2520that%2520tend%2520to%2520combine%2520the%2520best%2520of%250Aboth%2520worlds.%2520While%2520typical%2520ParVIs%2520such%2520as%2520Stein%2520Variational%2520Gradient%2520Descent%250A%2528SVGD%2529%2520approximate%2520the%2520gradient%2520flow%2520within%2520a%2520reproducing%2520kernel%2520Hilbert%2520space%250A%2528RKHS%2529%252C%2520many%2520attempts%2520have%2520been%2520made%2520recently%2520to%2520replace%2520RKHS%2520with%2520more%250Aexpressive%2520function%2520spaces%252C%2520such%2520as%2520neural%2520networks.%2520While%2520successful%252C%2520these%250Amethods%2520are%2520mainly%2520designed%2520for%2520sampling%2520from%2520unconstrained%2520domains.%2520In%2520this%250Apaper%252C%2520we%2520offer%2520a%2520general%2520solution%2520to%2520constrained%2520sampling%2520by%2520introducing%2520a%250Aboundary%2520condition%2520for%2520the%2520gradient%2520flow%2520which%2520would%2520confine%2520the%2520particles%250Awithin%2520the%2520specific%2520domain.%2520This%2520allows%2520us%2520to%2520propose%2520a%2520new%2520functional%2520gradient%250AParVI%2520method%2520for%2520constrained%2520sampling%252C%2520called%2520constrained%2520functional%2520gradient%250Aflow%2520%2528CFG%2529%252C%2520with%2520provable%2520continuous-time%2520convergence%2520in%2520total%2520variation%2520%2528TV%2529.%250AWe%2520also%2520present%2520novel%2520numerical%2520strategies%2520to%2520handle%2520the%2520boundary%2520integral%2520term%250Aarising%2520from%2520the%2520domain%2520constraints.%2520Our%2520theory%2520and%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23170v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Functional%20Gradient%20Flows%20for%20Constrained%20Sampling&entry.906535625=Shiyue%20Zhang%20and%20Longlin%20Yu%20and%20Ziheng%20Cheng%20and%20Cheng%20Zhang&entry.1292438233=%20%20Recently%2C%20through%20a%20unified%20gradient%20flow%20perspective%20of%20Markov%20chain%20Monte%0ACarlo%20%28MCMC%29%20and%20variational%20inference%20%28VI%29%2C%20particle-based%20variational%0Ainference%20methods%20%28ParVIs%29%20have%20been%20proposed%20that%20tend%20to%20combine%20the%20best%20of%0Aboth%20worlds.%20While%20typical%20ParVIs%20such%20as%20Stein%20Variational%20Gradient%20Descent%0A%28SVGD%29%20approximate%20the%20gradient%20flow%20within%20a%20reproducing%20kernel%20Hilbert%20space%0A%28RKHS%29%2C%20many%20attempts%20have%20been%20made%20recently%20to%20replace%20RKHS%20with%20more%0Aexpressive%20function%20spaces%2C%20such%20as%20neural%20networks.%20While%20successful%2C%20these%0Amethods%20are%20mainly%20designed%20for%20sampling%20from%20unconstrained%20domains.%20In%20this%0Apaper%2C%20we%20offer%20a%20general%20solution%20to%20constrained%20sampling%20by%20introducing%20a%0Aboundary%20condition%20for%20the%20gradient%20flow%20which%20would%20confine%20the%20particles%0Awithin%20the%20specific%20domain.%20This%20allows%20us%20to%20propose%20a%20new%20functional%20gradient%0AParVI%20method%20for%20constrained%20sampling%2C%20called%20constrained%20functional%20gradient%0Aflow%20%28CFG%29%2C%20with%20provable%20continuous-time%20convergence%20in%20total%20variation%20%28TV%29.%0AWe%20also%20present%20novel%20numerical%20strategies%20to%20handle%20the%20boundary%20integral%20term%0Aarising%20from%20the%20domain%20constraints.%20Our%20theory%20and%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23170v1&entry.124074799=Read"},
{"title": "Proportional Fairness in Non-Centroid Clustering", "author": "Ioannis Caragiannis and Evi Micha and Nisarg Shah", "abstract": "  We revisit the recently developed framework of proportionally fair\nclustering, where the goal is to provide group fairness guarantees that become\nstronger for groups of data points (agents) that are large and cohesive. Prior\nwork applies this framework to centroid clustering, where the loss of an agent\nis its distance to the centroid assigned to its cluster. We expand the\nframework to non-centroid clustering, where the loss of an agent is a function\nof the other agents in its cluster, by adapting two proportional fairness\ncriteria -- the core and its relaxation, fully justified representation (FJR)\n-- to this setting.\n  We show that the core can be approximated only under structured loss\nfunctions, and even then, the best approximation we are able to establish,\nusing an adaptation of the GreedyCapture algorithm developed for centroid\nclustering [Chen et al., 2019; Micha and Shah, 2020], is unappealing for a\nnatural loss function. In contrast, we design a new (inefficient) algorithm,\nGreedyCohesiveClustering, which achieves the relaxation FJR exactly under\narbitrary loss functions, and show that the efficient GreedyCapture algorithm\nachieves a constant approximation of FJR. We also design an efficient auditing\nalgorithm, which estimates the FJR approximation of any given clustering\nsolution up to a constant factor. Our experiments on real data suggest that\ntraditional clustering algorithms are highly unfair, whereas GreedyCapture is\nconsiderably fairer and incurs only a modest loss in common clustering\nobjectives.\n", "link": "http://arxiv.org/abs/2410.23273v1", "date": "2024-10-30", "relevancy": 1.2555, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4283}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4161}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proportional%20Fairness%20in%20Non-Centroid%20Clustering&body=Title%3A%20Proportional%20Fairness%20in%20Non-Centroid%20Clustering%0AAuthor%3A%20Ioannis%20Caragiannis%20and%20Evi%20Micha%20and%20Nisarg%20Shah%0AAbstract%3A%20%20%20We%20revisit%20the%20recently%20developed%20framework%20of%20proportionally%20fair%0Aclustering%2C%20where%20the%20goal%20is%20to%20provide%20group%20fairness%20guarantees%20that%20become%0Astronger%20for%20groups%20of%20data%20points%20%28agents%29%20that%20are%20large%20and%20cohesive.%20Prior%0Awork%20applies%20this%20framework%20to%20centroid%20clustering%2C%20where%20the%20loss%20of%20an%20agent%0Ais%20its%20distance%20to%20the%20centroid%20assigned%20to%20its%20cluster.%20We%20expand%20the%0Aframework%20to%20non-centroid%20clustering%2C%20where%20the%20loss%20of%20an%20agent%20is%20a%20function%0Aof%20the%20other%20agents%20in%20its%20cluster%2C%20by%20adapting%20two%20proportional%20fairness%0Acriteria%20--%20the%20core%20and%20its%20relaxation%2C%20fully%20justified%20representation%20%28FJR%29%0A--%20to%20this%20setting.%0A%20%20We%20show%20that%20the%20core%20can%20be%20approximated%20only%20under%20structured%20loss%0Afunctions%2C%20and%20even%20then%2C%20the%20best%20approximation%20we%20are%20able%20to%20establish%2C%0Ausing%20an%20adaptation%20of%20the%20GreedyCapture%20algorithm%20developed%20for%20centroid%0Aclustering%20%5BChen%20et%20al.%2C%202019%3B%20Micha%20and%20Shah%2C%202020%5D%2C%20is%20unappealing%20for%20a%0Anatural%20loss%20function.%20In%20contrast%2C%20we%20design%20a%20new%20%28inefficient%29%20algorithm%2C%0AGreedyCohesiveClustering%2C%20which%20achieves%20the%20relaxation%20FJR%20exactly%20under%0Aarbitrary%20loss%20functions%2C%20and%20show%20that%20the%20efficient%20GreedyCapture%20algorithm%0Aachieves%20a%20constant%20approximation%20of%20FJR.%20We%20also%20design%20an%20efficient%20auditing%0Aalgorithm%2C%20which%20estimates%20the%20FJR%20approximation%20of%20any%20given%20clustering%0Asolution%20up%20to%20a%20constant%20factor.%20Our%20experiments%20on%20real%20data%20suggest%20that%0Atraditional%20clustering%20algorithms%20are%20highly%20unfair%2C%20whereas%20GreedyCapture%20is%0Aconsiderably%20fairer%20and%20incurs%20only%20a%20modest%20loss%20in%20common%20clustering%0Aobjectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProportional%2520Fairness%2520in%2520Non-Centroid%2520Clustering%26entry.906535625%3DIoannis%2520Caragiannis%2520and%2520Evi%2520Micha%2520and%2520Nisarg%2520Shah%26entry.1292438233%3D%2520%2520We%2520revisit%2520the%2520recently%2520developed%2520framework%2520of%2520proportionally%2520fair%250Aclustering%252C%2520where%2520the%2520goal%2520is%2520to%2520provide%2520group%2520fairness%2520guarantees%2520that%2520become%250Astronger%2520for%2520groups%2520of%2520data%2520points%2520%2528agents%2529%2520that%2520are%2520large%2520and%2520cohesive.%2520Prior%250Awork%2520applies%2520this%2520framework%2520to%2520centroid%2520clustering%252C%2520where%2520the%2520loss%2520of%2520an%2520agent%250Ais%2520its%2520distance%2520to%2520the%2520centroid%2520assigned%2520to%2520its%2520cluster.%2520We%2520expand%2520the%250Aframework%2520to%2520non-centroid%2520clustering%252C%2520where%2520the%2520loss%2520of%2520an%2520agent%2520is%2520a%2520function%250Aof%2520the%2520other%2520agents%2520in%2520its%2520cluster%252C%2520by%2520adapting%2520two%2520proportional%2520fairness%250Acriteria%2520--%2520the%2520core%2520and%2520its%2520relaxation%252C%2520fully%2520justified%2520representation%2520%2528FJR%2529%250A--%2520to%2520this%2520setting.%250A%2520%2520We%2520show%2520that%2520the%2520core%2520can%2520be%2520approximated%2520only%2520under%2520structured%2520loss%250Afunctions%252C%2520and%2520even%2520then%252C%2520the%2520best%2520approximation%2520we%2520are%2520able%2520to%2520establish%252C%250Ausing%2520an%2520adaptation%2520of%2520the%2520GreedyCapture%2520algorithm%2520developed%2520for%2520centroid%250Aclustering%2520%255BChen%2520et%2520al.%252C%25202019%253B%2520Micha%2520and%2520Shah%252C%25202020%255D%252C%2520is%2520unappealing%2520for%2520a%250Anatural%2520loss%2520function.%2520In%2520contrast%252C%2520we%2520design%2520a%2520new%2520%2528inefficient%2529%2520algorithm%252C%250AGreedyCohesiveClustering%252C%2520which%2520achieves%2520the%2520relaxation%2520FJR%2520exactly%2520under%250Aarbitrary%2520loss%2520functions%252C%2520and%2520show%2520that%2520the%2520efficient%2520GreedyCapture%2520algorithm%250Aachieves%2520a%2520constant%2520approximation%2520of%2520FJR.%2520We%2520also%2520design%2520an%2520efficient%2520auditing%250Aalgorithm%252C%2520which%2520estimates%2520the%2520FJR%2520approximation%2520of%2520any%2520given%2520clustering%250Asolution%2520up%2520to%2520a%2520constant%2520factor.%2520Our%2520experiments%2520on%2520real%2520data%2520suggest%2520that%250Atraditional%2520clustering%2520algorithms%2520are%2520highly%2520unfair%252C%2520whereas%2520GreedyCapture%2520is%250Aconsiderably%2520fairer%2520and%2520incurs%2520only%2520a%2520modest%2520loss%2520in%2520common%2520clustering%250Aobjectives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proportional%20Fairness%20in%20Non-Centroid%20Clustering&entry.906535625=Ioannis%20Caragiannis%20and%20Evi%20Micha%20and%20Nisarg%20Shah&entry.1292438233=%20%20We%20revisit%20the%20recently%20developed%20framework%20of%20proportionally%20fair%0Aclustering%2C%20where%20the%20goal%20is%20to%20provide%20group%20fairness%20guarantees%20that%20become%0Astronger%20for%20groups%20of%20data%20points%20%28agents%29%20that%20are%20large%20and%20cohesive.%20Prior%0Awork%20applies%20this%20framework%20to%20centroid%20clustering%2C%20where%20the%20loss%20of%20an%20agent%0Ais%20its%20distance%20to%20the%20centroid%20assigned%20to%20its%20cluster.%20We%20expand%20the%0Aframework%20to%20non-centroid%20clustering%2C%20where%20the%20loss%20of%20an%20agent%20is%20a%20function%0Aof%20the%20other%20agents%20in%20its%20cluster%2C%20by%20adapting%20two%20proportional%20fairness%0Acriteria%20--%20the%20core%20and%20its%20relaxation%2C%20fully%20justified%20representation%20%28FJR%29%0A--%20to%20this%20setting.%0A%20%20We%20show%20that%20the%20core%20can%20be%20approximated%20only%20under%20structured%20loss%0Afunctions%2C%20and%20even%20then%2C%20the%20best%20approximation%20we%20are%20able%20to%20establish%2C%0Ausing%20an%20adaptation%20of%20the%20GreedyCapture%20algorithm%20developed%20for%20centroid%0Aclustering%20%5BChen%20et%20al.%2C%202019%3B%20Micha%20and%20Shah%2C%202020%5D%2C%20is%20unappealing%20for%20a%0Anatural%20loss%20function.%20In%20contrast%2C%20we%20design%20a%20new%20%28inefficient%29%20algorithm%2C%0AGreedyCohesiveClustering%2C%20which%20achieves%20the%20relaxation%20FJR%20exactly%20under%0Aarbitrary%20loss%20functions%2C%20and%20show%20that%20the%20efficient%20GreedyCapture%20algorithm%0Aachieves%20a%20constant%20approximation%20of%20FJR.%20We%20also%20design%20an%20efficient%20auditing%0Aalgorithm%2C%20which%20estimates%20the%20FJR%20approximation%20of%20any%20given%20clustering%0Asolution%20up%20to%20a%20constant%20factor.%20Our%20experiments%20on%20real%20data%20suggest%20that%0Atraditional%20clustering%20algorithms%20are%20highly%20unfair%2C%20whereas%20GreedyCapture%20is%0Aconsiderably%20fairer%20and%20incurs%20only%20a%20modest%20loss%20in%20common%20clustering%0Aobjectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23273v1&entry.124074799=Read"},
{"title": "Aequitas Flow: Streamlining Fair ML Experimentation", "author": "S\u00e9rgio Jesus and Pedro Saleiro and In\u00eas Oliveira e Silva and Beatriz M. Jorge and Rita P. Ribeiro and Jo\u00e3o Gama and Pedro Bizarro and Rayid Ghani", "abstract": "  Aequitas Flow is an open-source framework and toolkit for end-to-end Fair\nMachine Learning (ML) experimentation, and benchmarking in Python. This package\nfills integration gaps that exist in other fair ML packages. In addition to the\nexisting audit capabilities in Aequitas, the Aequitas Flow module provides a\npipeline for fairness-aware model training, hyperparameter optimization, and\nevaluation, enabling easy-to-use and rapid experiments and analysis of results.\nAimed at ML practitioners and researchers, the framework offers implementations\nof methods, datasets, metrics, and standard interfaces for these components to\nimprove extensibility. By facilitating the development of fair ML practices,\nAequitas Flow hopes to enhance the incorporation of fairness concepts in AI\nsystems making AI systems more robust and fair.\n", "link": "http://arxiv.org/abs/2405.05809v2", "date": "2024-10-30", "relevancy": 0.8705, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.48}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4185}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aequitas%20Flow%3A%20Streamlining%20Fair%20ML%20Experimentation&body=Title%3A%20Aequitas%20Flow%3A%20Streamlining%20Fair%20ML%20Experimentation%0AAuthor%3A%20S%C3%A9rgio%20Jesus%20and%20Pedro%20Saleiro%20and%20In%C3%AAs%20Oliveira%20e%20Silva%20and%20Beatriz%20M.%20Jorge%20and%20Rita%20P.%20Ribeiro%20and%20Jo%C3%A3o%20Gama%20and%20Pedro%20Bizarro%20and%20Rayid%20Ghani%0AAbstract%3A%20%20%20Aequitas%20Flow%20is%20an%20open-source%20framework%20and%20toolkit%20for%20end-to-end%20Fair%0AMachine%20Learning%20%28ML%29%20experimentation%2C%20and%20benchmarking%20in%20Python.%20This%20package%0Afills%20integration%20gaps%20that%20exist%20in%20other%20fair%20ML%20packages.%20In%20addition%20to%20the%0Aexisting%20audit%20capabilities%20in%20Aequitas%2C%20the%20Aequitas%20Flow%20module%20provides%20a%0Apipeline%20for%20fairness-aware%20model%20training%2C%20hyperparameter%20optimization%2C%20and%0Aevaluation%2C%20enabling%20easy-to-use%20and%20rapid%20experiments%20and%20analysis%20of%20results.%0AAimed%20at%20ML%20practitioners%20and%20researchers%2C%20the%20framework%20offers%20implementations%0Aof%20methods%2C%20datasets%2C%20metrics%2C%20and%20standard%20interfaces%20for%20these%20components%20to%0Aimprove%20extensibility.%20By%20facilitating%20the%20development%20of%20fair%20ML%20practices%2C%0AAequitas%20Flow%20hopes%20to%20enhance%20the%20incorporation%20of%20fairness%20concepts%20in%20AI%0Asystems%20making%20AI%20systems%20more%20robust%20and%20fair.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05809v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAequitas%2520Flow%253A%2520Streamlining%2520Fair%2520ML%2520Experimentation%26entry.906535625%3DS%25C3%25A9rgio%2520Jesus%2520and%2520Pedro%2520Saleiro%2520and%2520In%25C3%25AAs%2520Oliveira%2520e%2520Silva%2520and%2520Beatriz%2520M.%2520Jorge%2520and%2520Rita%2520P.%2520Ribeiro%2520and%2520Jo%25C3%25A3o%2520Gama%2520and%2520Pedro%2520Bizarro%2520and%2520Rayid%2520Ghani%26entry.1292438233%3D%2520%2520Aequitas%2520Flow%2520is%2520an%2520open-source%2520framework%2520and%2520toolkit%2520for%2520end-to-end%2520Fair%250AMachine%2520Learning%2520%2528ML%2529%2520experimentation%252C%2520and%2520benchmarking%2520in%2520Python.%2520This%2520package%250Afills%2520integration%2520gaps%2520that%2520exist%2520in%2520other%2520fair%2520ML%2520packages.%2520In%2520addition%2520to%2520the%250Aexisting%2520audit%2520capabilities%2520in%2520Aequitas%252C%2520the%2520Aequitas%2520Flow%2520module%2520provides%2520a%250Apipeline%2520for%2520fairness-aware%2520model%2520training%252C%2520hyperparameter%2520optimization%252C%2520and%250Aevaluation%252C%2520enabling%2520easy-to-use%2520and%2520rapid%2520experiments%2520and%2520analysis%2520of%2520results.%250AAimed%2520at%2520ML%2520practitioners%2520and%2520researchers%252C%2520the%2520framework%2520offers%2520implementations%250Aof%2520methods%252C%2520datasets%252C%2520metrics%252C%2520and%2520standard%2520interfaces%2520for%2520these%2520components%2520to%250Aimprove%2520extensibility.%2520By%2520facilitating%2520the%2520development%2520of%2520fair%2520ML%2520practices%252C%250AAequitas%2520Flow%2520hopes%2520to%2520enhance%2520the%2520incorporation%2520of%2520fairness%2520concepts%2520in%2520AI%250Asystems%2520making%2520AI%2520systems%2520more%2520robust%2520and%2520fair.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05809v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aequitas%20Flow%3A%20Streamlining%20Fair%20ML%20Experimentation&entry.906535625=S%C3%A9rgio%20Jesus%20and%20Pedro%20Saleiro%20and%20In%C3%AAs%20Oliveira%20e%20Silva%20and%20Beatriz%20M.%20Jorge%20and%20Rita%20P.%20Ribeiro%20and%20Jo%C3%A3o%20Gama%20and%20Pedro%20Bizarro%20and%20Rayid%20Ghani&entry.1292438233=%20%20Aequitas%20Flow%20is%20an%20open-source%20framework%20and%20toolkit%20for%20end-to-end%20Fair%0AMachine%20Learning%20%28ML%29%20experimentation%2C%20and%20benchmarking%20in%20Python.%20This%20package%0Afills%20integration%20gaps%20that%20exist%20in%20other%20fair%20ML%20packages.%20In%20addition%20to%20the%0Aexisting%20audit%20capabilities%20in%20Aequitas%2C%20the%20Aequitas%20Flow%20module%20provides%20a%0Apipeline%20for%20fairness-aware%20model%20training%2C%20hyperparameter%20optimization%2C%20and%0Aevaluation%2C%20enabling%20easy-to-use%20and%20rapid%20experiments%20and%20analysis%20of%20results.%0AAimed%20at%20ML%20practitioners%20and%20researchers%2C%20the%20framework%20offers%20implementations%0Aof%20methods%2C%20datasets%2C%20metrics%2C%20and%20standard%20interfaces%20for%20these%20components%20to%0Aimprove%20extensibility.%20By%20facilitating%20the%20development%20of%20fair%20ML%20practices%2C%0AAequitas%20Flow%20hopes%20to%20enhance%20the%20incorporation%20of%20fairness%20concepts%20in%20AI%0Asystems%20making%20AI%20systems%20more%20robust%20and%20fair.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05809v2&entry.124074799=Read"},
{"title": "Robust Statistical Scaling of Outlier Scores: Improving the Quality of\n  Outlier Probabilities for Outliers (Extended Version)", "author": "Philipp R\u00f6chner and Henrique O. Marques and Ricardo J. G. B. Campello and Arthur Zimek and Franz Rothlauf", "abstract": "  Outlier detection algorithms typically assign an outlier score to each\nobservation in a dataset, indicating the degree to which an observation is an\noutlier. However, these scores are often not comparable across algorithms and\ncan be difficult for humans to interpret. Statistical scaling addresses this\nproblem by transforming outlier scores into outlier probabilities without using\nground-truth labels, thereby improving interpretability and comparability\nacross algorithms. However, the quality of this transformation can be different\nfor outliers and inliers. Missing outliers in scenarios where they are of\nparticular interest - such as healthcare, finance, or engineering - can be\ncostly or dangerous. Thus, ensuring good probabilities for outliers is\nessential. This paper argues that statistical scaling, as commonly used in the\nliterature, does not produce equally good probabilities for outliers as for\ninliers. Therefore, we propose robust statistical scaling, which uses robust\nestimators to improve the probabilities for outliers. We evaluate several\nvariants of our method against other outlier score transformations for\nreal-world datasets and outlier detection algorithms, where it can improve the\nprobabilities for outliers.\n", "link": "http://arxiv.org/abs/2408.15874v3", "date": "2024-10-30", "relevancy": 0.8622, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4445}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4353}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Statistical%20Scaling%20of%20Outlier%20Scores%3A%20Improving%20the%20Quality%20of%0A%20%20Outlier%20Probabilities%20for%20Outliers%20%28Extended%20Version%29&body=Title%3A%20Robust%20Statistical%20Scaling%20of%20Outlier%20Scores%3A%20Improving%20the%20Quality%20of%0A%20%20Outlier%20Probabilities%20for%20Outliers%20%28Extended%20Version%29%0AAuthor%3A%20Philipp%20R%C3%B6chner%20and%20Henrique%20O.%20Marques%20and%20Ricardo%20J.%20G.%20B.%20Campello%20and%20Arthur%20Zimek%20and%20Franz%20Rothlauf%0AAbstract%3A%20%20%20Outlier%20detection%20algorithms%20typically%20assign%20an%20outlier%20score%20to%20each%0Aobservation%20in%20a%20dataset%2C%20indicating%20the%20degree%20to%20which%20an%20observation%20is%20an%0Aoutlier.%20However%2C%20these%20scores%20are%20often%20not%20comparable%20across%20algorithms%20and%0Acan%20be%20difficult%20for%20humans%20to%20interpret.%20Statistical%20scaling%20addresses%20this%0Aproblem%20by%20transforming%20outlier%20scores%20into%20outlier%20probabilities%20without%20using%0Aground-truth%20labels%2C%20thereby%20improving%20interpretability%20and%20comparability%0Aacross%20algorithms.%20However%2C%20the%20quality%20of%20this%20transformation%20can%20be%20different%0Afor%20outliers%20and%20inliers.%20Missing%20outliers%20in%20scenarios%20where%20they%20are%20of%0Aparticular%20interest%20-%20such%20as%20healthcare%2C%20finance%2C%20or%20engineering%20-%20can%20be%0Acostly%20or%20dangerous.%20Thus%2C%20ensuring%20good%20probabilities%20for%20outliers%20is%0Aessential.%20This%20paper%20argues%20that%20statistical%20scaling%2C%20as%20commonly%20used%20in%20the%0Aliterature%2C%20does%20not%20produce%20equally%20good%20probabilities%20for%20outliers%20as%20for%0Ainliers.%20Therefore%2C%20we%20propose%20robust%20statistical%20scaling%2C%20which%20uses%20robust%0Aestimators%20to%20improve%20the%20probabilities%20for%20outliers.%20We%20evaluate%20several%0Avariants%20of%20our%20method%20against%20other%20outlier%20score%20transformations%20for%0Areal-world%20datasets%20and%20outlier%20detection%20algorithms%2C%20where%20it%20can%20improve%20the%0Aprobabilities%20for%20outliers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15874v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Statistical%2520Scaling%2520of%2520Outlier%2520Scores%253A%2520Improving%2520the%2520Quality%2520of%250A%2520%2520Outlier%2520Probabilities%2520for%2520Outliers%2520%2528Extended%2520Version%2529%26entry.906535625%3DPhilipp%2520R%25C3%25B6chner%2520and%2520Henrique%2520O.%2520Marques%2520and%2520Ricardo%2520J.%2520G.%2520B.%2520Campello%2520and%2520Arthur%2520Zimek%2520and%2520Franz%2520Rothlauf%26entry.1292438233%3D%2520%2520Outlier%2520detection%2520algorithms%2520typically%2520assign%2520an%2520outlier%2520score%2520to%2520each%250Aobservation%2520in%2520a%2520dataset%252C%2520indicating%2520the%2520degree%2520to%2520which%2520an%2520observation%2520is%2520an%250Aoutlier.%2520However%252C%2520these%2520scores%2520are%2520often%2520not%2520comparable%2520across%2520algorithms%2520and%250Acan%2520be%2520difficult%2520for%2520humans%2520to%2520interpret.%2520Statistical%2520scaling%2520addresses%2520this%250Aproblem%2520by%2520transforming%2520outlier%2520scores%2520into%2520outlier%2520probabilities%2520without%2520using%250Aground-truth%2520labels%252C%2520thereby%2520improving%2520interpretability%2520and%2520comparability%250Aacross%2520algorithms.%2520However%252C%2520the%2520quality%2520of%2520this%2520transformation%2520can%2520be%2520different%250Afor%2520outliers%2520and%2520inliers.%2520Missing%2520outliers%2520in%2520scenarios%2520where%2520they%2520are%2520of%250Aparticular%2520interest%2520-%2520such%2520as%2520healthcare%252C%2520finance%252C%2520or%2520engineering%2520-%2520can%2520be%250Acostly%2520or%2520dangerous.%2520Thus%252C%2520ensuring%2520good%2520probabilities%2520for%2520outliers%2520is%250Aessential.%2520This%2520paper%2520argues%2520that%2520statistical%2520scaling%252C%2520as%2520commonly%2520used%2520in%2520the%250Aliterature%252C%2520does%2520not%2520produce%2520equally%2520good%2520probabilities%2520for%2520outliers%2520as%2520for%250Ainliers.%2520Therefore%252C%2520we%2520propose%2520robust%2520statistical%2520scaling%252C%2520which%2520uses%2520robust%250Aestimators%2520to%2520improve%2520the%2520probabilities%2520for%2520outliers.%2520We%2520evaluate%2520several%250Avariants%2520of%2520our%2520method%2520against%2520other%2520outlier%2520score%2520transformations%2520for%250Areal-world%2520datasets%2520and%2520outlier%2520detection%2520algorithms%252C%2520where%2520it%2520can%2520improve%2520the%250Aprobabilities%2520for%2520outliers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15874v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Statistical%20Scaling%20of%20Outlier%20Scores%3A%20Improving%20the%20Quality%20of%0A%20%20Outlier%20Probabilities%20for%20Outliers%20%28Extended%20Version%29&entry.906535625=Philipp%20R%C3%B6chner%20and%20Henrique%20O.%20Marques%20and%20Ricardo%20J.%20G.%20B.%20Campello%20and%20Arthur%20Zimek%20and%20Franz%20Rothlauf&entry.1292438233=%20%20Outlier%20detection%20algorithms%20typically%20assign%20an%20outlier%20score%20to%20each%0Aobservation%20in%20a%20dataset%2C%20indicating%20the%20degree%20to%20which%20an%20observation%20is%20an%0Aoutlier.%20However%2C%20these%20scores%20are%20often%20not%20comparable%20across%20algorithms%20and%0Acan%20be%20difficult%20for%20humans%20to%20interpret.%20Statistical%20scaling%20addresses%20this%0Aproblem%20by%20transforming%20outlier%20scores%20into%20outlier%20probabilities%20without%20using%0Aground-truth%20labels%2C%20thereby%20improving%20interpretability%20and%20comparability%0Aacross%20algorithms.%20However%2C%20the%20quality%20of%20this%20transformation%20can%20be%20different%0Afor%20outliers%20and%20inliers.%20Missing%20outliers%20in%20scenarios%20where%20they%20are%20of%0Aparticular%20interest%20-%20such%20as%20healthcare%2C%20finance%2C%20or%20engineering%20-%20can%20be%0Acostly%20or%20dangerous.%20Thus%2C%20ensuring%20good%20probabilities%20for%20outliers%20is%0Aessential.%20This%20paper%20argues%20that%20statistical%20scaling%2C%20as%20commonly%20used%20in%20the%0Aliterature%2C%20does%20not%20produce%20equally%20good%20probabilities%20for%20outliers%20as%20for%0Ainliers.%20Therefore%2C%20we%20propose%20robust%20statistical%20scaling%2C%20which%20uses%20robust%0Aestimators%20to%20improve%20the%20probabilities%20for%20outliers.%20We%20evaluate%20several%0Avariants%20of%20our%20method%20against%20other%20outlier%20score%20transformations%20for%0Areal-world%20datasets%20and%20outlier%20detection%20algorithms%2C%20where%20it%20can%20improve%20the%0Aprobabilities%20for%20outliers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15874v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


