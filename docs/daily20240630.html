<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240627.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "FAGhead: Fully Animate Gaussian Head from Monocular Videos", "author": "Yixin Xuan and Xinyang Li and Gongxin Yao and Shiwei Zhou and Donghui Sun and Xiaoxin Chen and Yu Pan", "abstract": "  High-fidelity reconstruction of 3D human avatars has a wild application in\nvisual reality. In this paper, we introduce FAGhead, a method that enables\nfully controllable human portraits from monocular videos. We explicit the\ntraditional 3D morphable meshes (3DMM) and optimize the neutral 3D Gaussians to\nreconstruct with complex expressions. Furthermore, we employ a novel\nPoint-based Learnable Representation Field (PLRF) with learnable Gaussian point\npositions to enhance reconstruction performance. Meanwhile, to effectively\nmanage the edges of avatars, we introduced the alpha rendering to supervise the\nalpha value of each pixel. Extensive experimental results on the open-source\ndatasets and our capturing datasets demonstrate that our approach is able to\ngenerate high-fidelity 3D head avatars and fully control the expression and\npose of the virtual avatars, which is outperforming than existing works.\n", "link": "http://arxiv.org/abs/2406.19070v1", "date": "2024-06-27", "relevancy": 3.838, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.8098}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.8098}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FAGhead%3A%20Fully%20Animate%20Gaussian%20Head%20from%20Monocular%20Videos&body=Title%3A%20FAGhead%3A%20Fully%20Animate%20Gaussian%20Head%20from%20Monocular%20Videos%0AAuthor%3A%20Yixin%20Xuan%20and%20Xinyang%20Li%20and%20Gongxin%20Yao%20and%20Shiwei%20Zhou%20and%20Donghui%20Sun%20and%20Xiaoxin%20Chen%20and%20Yu%20Pan%0AAbstract%3A%20%20%20High-fidelity%20reconstruction%20of%203D%20human%20avatars%20has%20a%20wild%20application%20in%0Avisual%20reality.%20In%20this%20paper%2C%20we%20introduce%20FAGhead%2C%20a%20method%20that%20enables%0Afully%20controllable%20human%20portraits%20from%20monocular%20videos.%20We%20explicit%20the%0Atraditional%203D%20morphable%20meshes%20%283DMM%29%20and%20optimize%20the%20neutral%203D%20Gaussians%20to%0Areconstruct%20with%20complex%20expressions.%20Furthermore%2C%20we%20employ%20a%20novel%0APoint-based%20Learnable%20Representation%20Field%20%28PLRF%29%20with%20learnable%20Gaussian%20point%0Apositions%20to%20enhance%20reconstruction%20performance.%20Meanwhile%2C%20to%20effectively%0Amanage%20the%20edges%20of%20avatars%2C%20we%20introduced%20the%20alpha%20rendering%20to%20supervise%20the%0Aalpha%20value%20of%20each%20pixel.%20Extensive%20experimental%20results%20on%20the%20open-source%0Adatasets%20and%20our%20capturing%20datasets%20demonstrate%20that%20our%20approach%20is%20able%20to%0Agenerate%20high-fidelity%203D%20head%20avatars%20and%20fully%20control%20the%20expression%20and%0Apose%20of%20the%20virtual%20avatars%2C%20which%20is%20outperforming%20than%20existing%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19070v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFAGhead%253A%2520Fully%2520Animate%2520Gaussian%2520Head%2520from%2520Monocular%2520Videos%26entry.906535625%3DYixin%2520Xuan%2520and%2520Xinyang%2520Li%2520and%2520Gongxin%2520Yao%2520and%2520Shiwei%2520Zhou%2520and%2520Donghui%2520Sun%2520and%2520Xiaoxin%2520Chen%2520and%2520Yu%2520Pan%26entry.1292438233%3D%2520%2520High-fidelity%2520reconstruction%2520of%25203D%2520human%2520avatars%2520has%2520a%2520wild%2520application%2520in%250Avisual%2520reality.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520FAGhead%252C%2520a%2520method%2520that%2520enables%250Afully%2520controllable%2520human%2520portraits%2520from%2520monocular%2520videos.%2520We%2520explicit%2520the%250Atraditional%25203D%2520morphable%2520meshes%2520%25283DMM%2529%2520and%2520optimize%2520the%2520neutral%25203D%2520Gaussians%2520to%250Areconstruct%2520with%2520complex%2520expressions.%2520Furthermore%252C%2520we%2520employ%2520a%2520novel%250APoint-based%2520Learnable%2520Representation%2520Field%2520%2528PLRF%2529%2520with%2520learnable%2520Gaussian%2520point%250Apositions%2520to%2520enhance%2520reconstruction%2520performance.%2520Meanwhile%252C%2520to%2520effectively%250Amanage%2520the%2520edges%2520of%2520avatars%252C%2520we%2520introduced%2520the%2520alpha%2520rendering%2520to%2520supervise%2520the%250Aalpha%2520value%2520of%2520each%2520pixel.%2520Extensive%2520experimental%2520results%2520on%2520the%2520open-source%250Adatasets%2520and%2520our%2520capturing%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520is%2520able%2520to%250Agenerate%2520high-fidelity%25203D%2520head%2520avatars%2520and%2520fully%2520control%2520the%2520expression%2520and%250Apose%2520of%2520the%2520virtual%2520avatars%252C%2520which%2520is%2520outperforming%2520than%2520existing%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19070v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FAGhead%3A%20Fully%20Animate%20Gaussian%20Head%20from%20Monocular%20Videos&entry.906535625=Yixin%20Xuan%20and%20Xinyang%20Li%20and%20Gongxin%20Yao%20and%20Shiwei%20Zhou%20and%20Donghui%20Sun%20and%20Xiaoxin%20Chen%20and%20Yu%20Pan&entry.1292438233=%20%20High-fidelity%20reconstruction%20of%203D%20human%20avatars%20has%20a%20wild%20application%20in%0Avisual%20reality.%20In%20this%20paper%2C%20we%20introduce%20FAGhead%2C%20a%20method%20that%20enables%0Afully%20controllable%20human%20portraits%20from%20monocular%20videos.%20We%20explicit%20the%0Atraditional%203D%20morphable%20meshes%20%283DMM%29%20and%20optimize%20the%20neutral%203D%20Gaussians%20to%0Areconstruct%20with%20complex%20expressions.%20Furthermore%2C%20we%20employ%20a%20novel%0APoint-based%20Learnable%20Representation%20Field%20%28PLRF%29%20with%20learnable%20Gaussian%20point%0Apositions%20to%20enhance%20reconstruction%20performance.%20Meanwhile%2C%20to%20effectively%0Amanage%20the%20edges%20of%20avatars%2C%20we%20introduced%20the%20alpha%20rendering%20to%20supervise%20the%0Aalpha%20value%20of%20each%20pixel.%20Extensive%20experimental%20results%20on%20the%20open-source%0Adatasets%20and%20our%20capturing%20datasets%20demonstrate%20that%20our%20approach%20is%20able%20to%0Agenerate%20high-fidelity%203D%20head%20avatars%20and%20fully%20control%20the%20expression%20and%0Apose%20of%20the%20virtual%20avatars%2C%20which%20is%20outperforming%20than%20existing%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19070v1&entry.124074799=Read"},
{"title": "OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and\n  Understanding", "author": "Tao Zhang and Xiangtai Li and Hao Fei and Haobo Yuan and Shengqiong Wu and Shunping Ji and Chen Change Loy and Shuicheng Yan", "abstract": "  Current universal segmentation methods demonstrate strong capabilities in\npixel-level image and video understanding. However, they lack reasoning\nabilities and cannot be controlled via text instructions. In contrast, large\nvision-language multimodal models exhibit powerful vision-based conversation\nand reasoning capabilities but lack pixel-level understanding and have\ndifficulty accepting visual prompts for flexible user interaction. This paper\nproposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level\nvision understanding with reasoning abilities. It can accept various visual and\ntext prompts for flexible user interaction. Specifically, we use a universal\nsegmentation method as the visual encoder, integrating image information,\nperception priors, and visual prompts into visual tokens provided to the LLM.\nThe LLM is responsible for understanding the user's text instructions and\nproviding text responses and pixel-level segmentation results based on the\nvisual information. We propose perception prior embedding to better integrate\nperception priors with image features. OMG-LLaVA achieves image-level,\nobject-level, and pixel-level reasoning and understanding in a single model,\nmatching or surpassing the performance of specialized methods on multiple\nbenchmarks. Rather than using LLM to connect each specialist, our work aims at\nend-to-end training on one encoder, one decoder, and one LLM. The code and\nmodel have been released for further research.\n", "link": "http://arxiv.org/abs/2406.19389v1", "date": "2024-06-27", "relevancy": 2.7722, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5659}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5539}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OMG-LLaVA%3A%20Bridging%20Image-level%2C%20Object-level%2C%20Pixel-level%20Reasoning%20and%0A%20%20Understanding&body=Title%3A%20OMG-LLaVA%3A%20Bridging%20Image-level%2C%20Object-level%2C%20Pixel-level%20Reasoning%20and%0A%20%20Understanding%0AAuthor%3A%20Tao%20Zhang%20and%20Xiangtai%20Li%20and%20Hao%20Fei%20and%20Haobo%20Yuan%20and%20Shengqiong%20Wu%20and%20Shunping%20Ji%20and%20Chen%20Change%20Loy%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%20Current%20universal%20segmentation%20methods%20demonstrate%20strong%20capabilities%20in%0Apixel-level%20image%20and%20video%20understanding.%20However%2C%20they%20lack%20reasoning%0Aabilities%20and%20cannot%20be%20controlled%20via%20text%20instructions.%20In%20contrast%2C%20large%0Avision-language%20multimodal%20models%20exhibit%20powerful%20vision-based%20conversation%0Aand%20reasoning%20capabilities%20but%20lack%20pixel-level%20understanding%20and%20have%0Adifficulty%20accepting%20visual%20prompts%20for%20flexible%20user%20interaction.%20This%20paper%0Aproposes%20OMG-LLaVA%2C%20a%20new%20and%20elegant%20framework%20combining%20powerful%20pixel-level%0Avision%20understanding%20with%20reasoning%20abilities.%20It%20can%20accept%20various%20visual%20and%0Atext%20prompts%20for%20flexible%20user%20interaction.%20Specifically%2C%20we%20use%20a%20universal%0Asegmentation%20method%20as%20the%20visual%20encoder%2C%20integrating%20image%20information%2C%0Aperception%20priors%2C%20and%20visual%20prompts%20into%20visual%20tokens%20provided%20to%20the%20LLM.%0AThe%20LLM%20is%20responsible%20for%20understanding%20the%20user%27s%20text%20instructions%20and%0Aproviding%20text%20responses%20and%20pixel-level%20segmentation%20results%20based%20on%20the%0Avisual%20information.%20We%20propose%20perception%20prior%20embedding%20to%20better%20integrate%0Aperception%20priors%20with%20image%20features.%20OMG-LLaVA%20achieves%20image-level%2C%0Aobject-level%2C%20and%20pixel-level%20reasoning%20and%20understanding%20in%20a%20single%20model%2C%0Amatching%20or%20surpassing%20the%20performance%20of%20specialized%20methods%20on%20multiple%0Abenchmarks.%20Rather%20than%20using%20LLM%20to%20connect%20each%20specialist%2C%20our%20work%20aims%20at%0Aend-to-end%20training%20on%20one%20encoder%2C%20one%20decoder%2C%20and%20one%20LLM.%20The%20code%20and%0Amodel%20have%20been%20released%20for%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19389v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOMG-LLaVA%253A%2520Bridging%2520Image-level%252C%2520Object-level%252C%2520Pixel-level%2520Reasoning%2520and%250A%2520%2520Understanding%26entry.906535625%3DTao%2520Zhang%2520and%2520Xiangtai%2520Li%2520and%2520Hao%2520Fei%2520and%2520Haobo%2520Yuan%2520and%2520Shengqiong%2520Wu%2520and%2520Shunping%2520Ji%2520and%2520Chen%2520Change%2520Loy%2520and%2520Shuicheng%2520Yan%26entry.1292438233%3D%2520%2520Current%2520universal%2520segmentation%2520methods%2520demonstrate%2520strong%2520capabilities%2520in%250Apixel-level%2520image%2520and%2520video%2520understanding.%2520However%252C%2520they%2520lack%2520reasoning%250Aabilities%2520and%2520cannot%2520be%2520controlled%2520via%2520text%2520instructions.%2520In%2520contrast%252C%2520large%250Avision-language%2520multimodal%2520models%2520exhibit%2520powerful%2520vision-based%2520conversation%250Aand%2520reasoning%2520capabilities%2520but%2520lack%2520pixel-level%2520understanding%2520and%2520have%250Adifficulty%2520accepting%2520visual%2520prompts%2520for%2520flexible%2520user%2520interaction.%2520This%2520paper%250Aproposes%2520OMG-LLaVA%252C%2520a%2520new%2520and%2520elegant%2520framework%2520combining%2520powerful%2520pixel-level%250Avision%2520understanding%2520with%2520reasoning%2520abilities.%2520It%2520can%2520accept%2520various%2520visual%2520and%250Atext%2520prompts%2520for%2520flexible%2520user%2520interaction.%2520Specifically%252C%2520we%2520use%2520a%2520universal%250Asegmentation%2520method%2520as%2520the%2520visual%2520encoder%252C%2520integrating%2520image%2520information%252C%250Aperception%2520priors%252C%2520and%2520visual%2520prompts%2520into%2520visual%2520tokens%2520provided%2520to%2520the%2520LLM.%250AThe%2520LLM%2520is%2520responsible%2520for%2520understanding%2520the%2520user%2527s%2520text%2520instructions%2520and%250Aproviding%2520text%2520responses%2520and%2520pixel-level%2520segmentation%2520results%2520based%2520on%2520the%250Avisual%2520information.%2520We%2520propose%2520perception%2520prior%2520embedding%2520to%2520better%2520integrate%250Aperception%2520priors%2520with%2520image%2520features.%2520OMG-LLaVA%2520achieves%2520image-level%252C%250Aobject-level%252C%2520and%2520pixel-level%2520reasoning%2520and%2520understanding%2520in%2520a%2520single%2520model%252C%250Amatching%2520or%2520surpassing%2520the%2520performance%2520of%2520specialized%2520methods%2520on%2520multiple%250Abenchmarks.%2520Rather%2520than%2520using%2520LLM%2520to%2520connect%2520each%2520specialist%252C%2520our%2520work%2520aims%2520at%250Aend-to-end%2520training%2520on%2520one%2520encoder%252C%2520one%2520decoder%252C%2520and%2520one%2520LLM.%2520The%2520code%2520and%250Amodel%2520have%2520been%2520released%2520for%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19389v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OMG-LLaVA%3A%20Bridging%20Image-level%2C%20Object-level%2C%20Pixel-level%20Reasoning%20and%0A%20%20Understanding&entry.906535625=Tao%20Zhang%20and%20Xiangtai%20Li%20and%20Hao%20Fei%20and%20Haobo%20Yuan%20and%20Shengqiong%20Wu%20and%20Shunping%20Ji%20and%20Chen%20Change%20Loy%20and%20Shuicheng%20Yan&entry.1292438233=%20%20Current%20universal%20segmentation%20methods%20demonstrate%20strong%20capabilities%20in%0Apixel-level%20image%20and%20video%20understanding.%20However%2C%20they%20lack%20reasoning%0Aabilities%20and%20cannot%20be%20controlled%20via%20text%20instructions.%20In%20contrast%2C%20large%0Avision-language%20multimodal%20models%20exhibit%20powerful%20vision-based%20conversation%0Aand%20reasoning%20capabilities%20but%20lack%20pixel-level%20understanding%20and%20have%0Adifficulty%20accepting%20visual%20prompts%20for%20flexible%20user%20interaction.%20This%20paper%0Aproposes%20OMG-LLaVA%2C%20a%20new%20and%20elegant%20framework%20combining%20powerful%20pixel-level%0Avision%20understanding%20with%20reasoning%20abilities.%20It%20can%20accept%20various%20visual%20and%0Atext%20prompts%20for%20flexible%20user%20interaction.%20Specifically%2C%20we%20use%20a%20universal%0Asegmentation%20method%20as%20the%20visual%20encoder%2C%20integrating%20image%20information%2C%0Aperception%20priors%2C%20and%20visual%20prompts%20into%20visual%20tokens%20provided%20to%20the%20LLM.%0AThe%20LLM%20is%20responsible%20for%20understanding%20the%20user%27s%20text%20instructions%20and%0Aproviding%20text%20responses%20and%20pixel-level%20segmentation%20results%20based%20on%20the%0Avisual%20information.%20We%20propose%20perception%20prior%20embedding%20to%20better%20integrate%0Aperception%20priors%20with%20image%20features.%20OMG-LLaVA%20achieves%20image-level%2C%0Aobject-level%2C%20and%20pixel-level%20reasoning%20and%20understanding%20in%20a%20single%20model%2C%0Amatching%20or%20surpassing%20the%20performance%20of%20specialized%20methods%20on%20multiple%0Abenchmarks.%20Rather%20than%20using%20LLM%20to%20connect%20each%20specialist%2C%20our%20work%20aims%20at%0Aend-to-end%20training%20on%20one%20encoder%2C%20one%20decoder%2C%20and%20one%20LLM.%20The%20code%20and%0Amodel%20have%20been%20released%20for%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19389v1&entry.124074799=Read"},
{"title": "Local Manifold Learning for No-Reference Image Quality Assessment", "author": "Timin Gao and Wensheng Pan and Yan Zhang and Sicheng Zhao and Shengchuan Zhang and Xiawu Zheng and Ke Li and Liujuan Cao and Rongrong Ji", "abstract": "  Contrastive learning has considerably advanced the field of Image Quality\nAssessment (IQA), emerging as a widely adopted technique. The core mechanism of\ncontrastive learning involves minimizing the distance between quality-similar\n(positive) examples while maximizing the distance between quality-dissimilar\n(negative) examples. Despite its successes, current contrastive learning\nmethods often neglect the importance of preserving the local manifold\nstructure. This oversight can result in a high degree of similarity among hard\nexamples within the feature space, thereby impeding effective differentiation\nand assessment. To address this issue, we propose an innovative framework that\nintegrates local manifold learning with contrastive learning for No-Reference\nImage Quality Assessment (NR-IQA). Our method begins by sampling multiple crops\nfrom a given image, identifying the most visually salient crop. This crop is\nthen used to cluster other crops from the same image as the positive class,\nwhile crops from different images are treated as negative classes to increase\ninter-class distance. Uniquely, our approach also considers non-saliency crops\nfrom the same image as intra-class negative classes to preserve their\ndistinctiveness. Additionally, we employ a mutual learning framework, which\nfurther enhances the model's ability to adaptively learn and identify visual\nsaliency regions. Our approach demonstrates a better performance compared to\nstate-of-the-art methods in 7 standard datasets, achieving PLCC values of 0.942\n(compared to 0.908 in TID2013) and 0.914 (compared to 0.894 in LIVEC).\n", "link": "http://arxiv.org/abs/2406.19247v1", "date": "2024-06-27", "relevancy": 2.7609, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5664}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5623}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Manifold%20Learning%20for%20No-Reference%20Image%20Quality%20Assessment&body=Title%3A%20Local%20Manifold%20Learning%20for%20No-Reference%20Image%20Quality%20Assessment%0AAuthor%3A%20Timin%20Gao%20and%20Wensheng%20Pan%20and%20Yan%20Zhang%20and%20Sicheng%20Zhao%20and%20Shengchuan%20Zhang%20and%20Xiawu%20Zheng%20and%20Ke%20Li%20and%20Liujuan%20Cao%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20Contrastive%20learning%20has%20considerably%20advanced%20the%20field%20of%20Image%20Quality%0AAssessment%20%28IQA%29%2C%20emerging%20as%20a%20widely%20adopted%20technique.%20The%20core%20mechanism%20of%0Acontrastive%20learning%20involves%20minimizing%20the%20distance%20between%20quality-similar%0A%28positive%29%20examples%20while%20maximizing%20the%20distance%20between%20quality-dissimilar%0A%28negative%29%20examples.%20Despite%20its%20successes%2C%20current%20contrastive%20learning%0Amethods%20often%20neglect%20the%20importance%20of%20preserving%20the%20local%20manifold%0Astructure.%20This%20oversight%20can%20result%20in%20a%20high%20degree%20of%20similarity%20among%20hard%0Aexamples%20within%20the%20feature%20space%2C%20thereby%20impeding%20effective%20differentiation%0Aand%20assessment.%20To%20address%20this%20issue%2C%20we%20propose%20an%20innovative%20framework%20that%0Aintegrates%20local%20manifold%20learning%20with%20contrastive%20learning%20for%20No-Reference%0AImage%20Quality%20Assessment%20%28NR-IQA%29.%20Our%20method%20begins%20by%20sampling%20multiple%20crops%0Afrom%20a%20given%20image%2C%20identifying%20the%20most%20visually%20salient%20crop.%20This%20crop%20is%0Athen%20used%20to%20cluster%20other%20crops%20from%20the%20same%20image%20as%20the%20positive%20class%2C%0Awhile%20crops%20from%20different%20images%20are%20treated%20as%20negative%20classes%20to%20increase%0Ainter-class%20distance.%20Uniquely%2C%20our%20approach%20also%20considers%20non-saliency%20crops%0Afrom%20the%20same%20image%20as%20intra-class%20negative%20classes%20to%20preserve%20their%0Adistinctiveness.%20Additionally%2C%20we%20employ%20a%20mutual%20learning%20framework%2C%20which%0Afurther%20enhances%20the%20model%27s%20ability%20to%20adaptively%20learn%20and%20identify%20visual%0Asaliency%20regions.%20Our%20approach%20demonstrates%20a%20better%20performance%20compared%20to%0Astate-of-the-art%20methods%20in%207%20standard%20datasets%2C%20achieving%20PLCC%20values%20of%200.942%0A%28compared%20to%200.908%20in%20TID2013%29%20and%200.914%20%28compared%20to%200.894%20in%20LIVEC%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19247v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Manifold%2520Learning%2520for%2520No-Reference%2520Image%2520Quality%2520Assessment%26entry.906535625%3DTimin%2520Gao%2520and%2520Wensheng%2520Pan%2520and%2520Yan%2520Zhang%2520and%2520Sicheng%2520Zhao%2520and%2520Shengchuan%2520Zhang%2520and%2520Xiawu%2520Zheng%2520and%2520Ke%2520Li%2520and%2520Liujuan%2520Cao%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520Contrastive%2520learning%2520has%2520considerably%2520advanced%2520the%2520field%2520of%2520Image%2520Quality%250AAssessment%2520%2528IQA%2529%252C%2520emerging%2520as%2520a%2520widely%2520adopted%2520technique.%2520The%2520core%2520mechanism%2520of%250Acontrastive%2520learning%2520involves%2520minimizing%2520the%2520distance%2520between%2520quality-similar%250A%2528positive%2529%2520examples%2520while%2520maximizing%2520the%2520distance%2520between%2520quality-dissimilar%250A%2528negative%2529%2520examples.%2520Despite%2520its%2520successes%252C%2520current%2520contrastive%2520learning%250Amethods%2520often%2520neglect%2520the%2520importance%2520of%2520preserving%2520the%2520local%2520manifold%250Astructure.%2520This%2520oversight%2520can%2520result%2520in%2520a%2520high%2520degree%2520of%2520similarity%2520among%2520hard%250Aexamples%2520within%2520the%2520feature%2520space%252C%2520thereby%2520impeding%2520effective%2520differentiation%250Aand%2520assessment.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520an%2520innovative%2520framework%2520that%250Aintegrates%2520local%2520manifold%2520learning%2520with%2520contrastive%2520learning%2520for%2520No-Reference%250AImage%2520Quality%2520Assessment%2520%2528NR-IQA%2529.%2520Our%2520method%2520begins%2520by%2520sampling%2520multiple%2520crops%250Afrom%2520a%2520given%2520image%252C%2520identifying%2520the%2520most%2520visually%2520salient%2520crop.%2520This%2520crop%2520is%250Athen%2520used%2520to%2520cluster%2520other%2520crops%2520from%2520the%2520same%2520image%2520as%2520the%2520positive%2520class%252C%250Awhile%2520crops%2520from%2520different%2520images%2520are%2520treated%2520as%2520negative%2520classes%2520to%2520increase%250Ainter-class%2520distance.%2520Uniquely%252C%2520our%2520approach%2520also%2520considers%2520non-saliency%2520crops%250Afrom%2520the%2520same%2520image%2520as%2520intra-class%2520negative%2520classes%2520to%2520preserve%2520their%250Adistinctiveness.%2520Additionally%252C%2520we%2520employ%2520a%2520mutual%2520learning%2520framework%252C%2520which%250Afurther%2520enhances%2520the%2520model%2527s%2520ability%2520to%2520adaptively%2520learn%2520and%2520identify%2520visual%250Asaliency%2520regions.%2520Our%2520approach%2520demonstrates%2520a%2520better%2520performance%2520compared%2520to%250Astate-of-the-art%2520methods%2520in%25207%2520standard%2520datasets%252C%2520achieving%2520PLCC%2520values%2520of%25200.942%250A%2528compared%2520to%25200.908%2520in%2520TID2013%2529%2520and%25200.914%2520%2528compared%2520to%25200.894%2520in%2520LIVEC%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19247v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Manifold%20Learning%20for%20No-Reference%20Image%20Quality%20Assessment&entry.906535625=Timin%20Gao%20and%20Wensheng%20Pan%20and%20Yan%20Zhang%20and%20Sicheng%20Zhao%20and%20Shengchuan%20Zhang%20and%20Xiawu%20Zheng%20and%20Ke%20Li%20and%20Liujuan%20Cao%20and%20Rongrong%20Ji&entry.1292438233=%20%20Contrastive%20learning%20has%20considerably%20advanced%20the%20field%20of%20Image%20Quality%0AAssessment%20%28IQA%29%2C%20emerging%20as%20a%20widely%20adopted%20technique.%20The%20core%20mechanism%20of%0Acontrastive%20learning%20involves%20minimizing%20the%20distance%20between%20quality-similar%0A%28positive%29%20examples%20while%20maximizing%20the%20distance%20between%20quality-dissimilar%0A%28negative%29%20examples.%20Despite%20its%20successes%2C%20current%20contrastive%20learning%0Amethods%20often%20neglect%20the%20importance%20of%20preserving%20the%20local%20manifold%0Astructure.%20This%20oversight%20can%20result%20in%20a%20high%20degree%20of%20similarity%20among%20hard%0Aexamples%20within%20the%20feature%20space%2C%20thereby%20impeding%20effective%20differentiation%0Aand%20assessment.%20To%20address%20this%20issue%2C%20we%20propose%20an%20innovative%20framework%20that%0Aintegrates%20local%20manifold%20learning%20with%20contrastive%20learning%20for%20No-Reference%0AImage%20Quality%20Assessment%20%28NR-IQA%29.%20Our%20method%20begins%20by%20sampling%20multiple%20crops%0Afrom%20a%20given%20image%2C%20identifying%20the%20most%20visually%20salient%20crop.%20This%20crop%20is%0Athen%20used%20to%20cluster%20other%20crops%20from%20the%20same%20image%20as%20the%20positive%20class%2C%0Awhile%20crops%20from%20different%20images%20are%20treated%20as%20negative%20classes%20to%20increase%0Ainter-class%20distance.%20Uniquely%2C%20our%20approach%20also%20considers%20non-saliency%20crops%0Afrom%20the%20same%20image%20as%20intra-class%20negative%20classes%20to%20preserve%20their%0Adistinctiveness.%20Additionally%2C%20we%20employ%20a%20mutual%20learning%20framework%2C%20which%0Afurther%20enhances%20the%20model%27s%20ability%20to%20adaptively%20learn%20and%20identify%20visual%0Asaliency%20regions.%20Our%20approach%20demonstrates%20a%20better%20performance%20compared%20to%0Astate-of-the-art%20methods%20in%207%20standard%20datasets%2C%20achieving%20PLCC%20values%20of%200.942%0A%28compared%20to%200.908%20in%20TID2013%29%20and%200.914%20%28compared%20to%200.894%20in%20LIVEC%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19247v1&entry.124074799=Read"},
{"title": "SALVe: Semantic Alignment Verification for Floorplan Reconstruction from\n  Sparse Panoramas", "author": "John Lambert and Yuguang Li and Ivaylo Boyadzhiev and Lambert Wixson and Manjunath Narayana and Will Hutchcroft and James Hays and Frank Dellaert and Sing Bing Kang", "abstract": "  We propose a new system for automatic 2D floorplan reconstruction that is\nenabled by SALVe, our novel pairwise learned alignment verifier. The inputs to\nour system are sparsely located 360$^\\circ$ panoramas, whose semantic features\n(windows, doors, and openings) are inferred and used to hypothesize pairwise\nroom adjacency or overlap. SALVe initializes a pose graph, which is\nsubsequently optimized using GTSAM. Once the room poses are computed, room\nlayouts are inferred using HorizonNet, and the floorplan is constructed by\nstitching the most confident layout boundaries. We validate our system\nqualitatively and quantitatively as well as through ablation studies, showing\nthat it outperforms state-of-the-art SfM systems in completeness by over 200%,\nwithout sacrificing accuracy. Our results point to the significance of our\nwork: poses of 81% of panoramas are localized in the first 2 connected\ncomponents (CCs), and 89% in the first 3 CCs. Code and models are publicly\navailable at https://github.com/zillow/salve.\n", "link": "http://arxiv.org/abs/2406.19390v1", "date": "2024-06-27", "relevancy": 2.7322, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5638}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5594}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SALVe%3A%20Semantic%20Alignment%20Verification%20for%20Floorplan%20Reconstruction%20from%0A%20%20Sparse%20Panoramas&body=Title%3A%20SALVe%3A%20Semantic%20Alignment%20Verification%20for%20Floorplan%20Reconstruction%20from%0A%20%20Sparse%20Panoramas%0AAuthor%3A%20John%20Lambert%20and%20Yuguang%20Li%20and%20Ivaylo%20Boyadzhiev%20and%20Lambert%20Wixson%20and%20Manjunath%20Narayana%20and%20Will%20Hutchcroft%20and%20James%20Hays%20and%20Frank%20Dellaert%20and%20Sing%20Bing%20Kang%0AAbstract%3A%20%20%20We%20propose%20a%20new%20system%20for%20automatic%202D%20floorplan%20reconstruction%20that%20is%0Aenabled%20by%20SALVe%2C%20our%20novel%20pairwise%20learned%20alignment%20verifier.%20The%20inputs%20to%0Aour%20system%20are%20sparsely%20located%20360%24%5E%5Ccirc%24%20panoramas%2C%20whose%20semantic%20features%0A%28windows%2C%20doors%2C%20and%20openings%29%20are%20inferred%20and%20used%20to%20hypothesize%20pairwise%0Aroom%20adjacency%20or%20overlap.%20SALVe%20initializes%20a%20pose%20graph%2C%20which%20is%0Asubsequently%20optimized%20using%20GTSAM.%20Once%20the%20room%20poses%20are%20computed%2C%20room%0Alayouts%20are%20inferred%20using%20HorizonNet%2C%20and%20the%20floorplan%20is%20constructed%20by%0Astitching%20the%20most%20confident%20layout%20boundaries.%20We%20validate%20our%20system%0Aqualitatively%20and%20quantitatively%20as%20well%20as%20through%20ablation%20studies%2C%20showing%0Athat%20it%20outperforms%20state-of-the-art%20SfM%20systems%20in%20completeness%20by%20over%20200%25%2C%0Awithout%20sacrificing%20accuracy.%20Our%20results%20point%20to%20the%20significance%20of%20our%0Awork%3A%20poses%20of%2081%25%20of%20panoramas%20are%20localized%20in%20the%20first%202%20connected%0Acomponents%20%28CCs%29%2C%20and%2089%25%20in%20the%20first%203%20CCs.%20Code%20and%20models%20are%20publicly%0Aavailable%20at%20https%3A//github.com/zillow/salve.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSALVe%253A%2520Semantic%2520Alignment%2520Verification%2520for%2520Floorplan%2520Reconstruction%2520from%250A%2520%2520Sparse%2520Panoramas%26entry.906535625%3DJohn%2520Lambert%2520and%2520Yuguang%2520Li%2520and%2520Ivaylo%2520Boyadzhiev%2520and%2520Lambert%2520Wixson%2520and%2520Manjunath%2520Narayana%2520and%2520Will%2520Hutchcroft%2520and%2520James%2520Hays%2520and%2520Frank%2520Dellaert%2520and%2520Sing%2520Bing%2520Kang%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520system%2520for%2520automatic%25202D%2520floorplan%2520reconstruction%2520that%2520is%250Aenabled%2520by%2520SALVe%252C%2520our%2520novel%2520pairwise%2520learned%2520alignment%2520verifier.%2520The%2520inputs%2520to%250Aour%2520system%2520are%2520sparsely%2520located%2520360%2524%255E%255Ccirc%2524%2520panoramas%252C%2520whose%2520semantic%2520features%250A%2528windows%252C%2520doors%252C%2520and%2520openings%2529%2520are%2520inferred%2520and%2520used%2520to%2520hypothesize%2520pairwise%250Aroom%2520adjacency%2520or%2520overlap.%2520SALVe%2520initializes%2520a%2520pose%2520graph%252C%2520which%2520is%250Asubsequently%2520optimized%2520using%2520GTSAM.%2520Once%2520the%2520room%2520poses%2520are%2520computed%252C%2520room%250Alayouts%2520are%2520inferred%2520using%2520HorizonNet%252C%2520and%2520the%2520floorplan%2520is%2520constructed%2520by%250Astitching%2520the%2520most%2520confident%2520layout%2520boundaries.%2520We%2520validate%2520our%2520system%250Aqualitatively%2520and%2520quantitatively%2520as%2520well%2520as%2520through%2520ablation%2520studies%252C%2520showing%250Athat%2520it%2520outperforms%2520state-of-the-art%2520SfM%2520systems%2520in%2520completeness%2520by%2520over%2520200%2525%252C%250Awithout%2520sacrificing%2520accuracy.%2520Our%2520results%2520point%2520to%2520the%2520significance%2520of%2520our%250Awork%253A%2520poses%2520of%252081%2525%2520of%2520panoramas%2520are%2520localized%2520in%2520the%2520first%25202%2520connected%250Acomponents%2520%2528CCs%2529%252C%2520and%252089%2525%2520in%2520the%2520first%25203%2520CCs.%2520Code%2520and%2520models%2520are%2520publicly%250Aavailable%2520at%2520https%253A//github.com/zillow/salve.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SALVe%3A%20Semantic%20Alignment%20Verification%20for%20Floorplan%20Reconstruction%20from%0A%20%20Sparse%20Panoramas&entry.906535625=John%20Lambert%20and%20Yuguang%20Li%20and%20Ivaylo%20Boyadzhiev%20and%20Lambert%20Wixson%20and%20Manjunath%20Narayana%20and%20Will%20Hutchcroft%20and%20James%20Hays%20and%20Frank%20Dellaert%20and%20Sing%20Bing%20Kang&entry.1292438233=%20%20We%20propose%20a%20new%20system%20for%20automatic%202D%20floorplan%20reconstruction%20that%20is%0Aenabled%20by%20SALVe%2C%20our%20novel%20pairwise%20learned%20alignment%20verifier.%20The%20inputs%20to%0Aour%20system%20are%20sparsely%20located%20360%24%5E%5Ccirc%24%20panoramas%2C%20whose%20semantic%20features%0A%28windows%2C%20doors%2C%20and%20openings%29%20are%20inferred%20and%20used%20to%20hypothesize%20pairwise%0Aroom%20adjacency%20or%20overlap.%20SALVe%20initializes%20a%20pose%20graph%2C%20which%20is%0Asubsequently%20optimized%20using%20GTSAM.%20Once%20the%20room%20poses%20are%20computed%2C%20room%0Alayouts%20are%20inferred%20using%20HorizonNet%2C%20and%20the%20floorplan%20is%20constructed%20by%0Astitching%20the%20most%20confident%20layout%20boundaries.%20We%20validate%20our%20system%0Aqualitatively%20and%20quantitatively%20as%20well%20as%20through%20ablation%20studies%2C%20showing%0Athat%20it%20outperforms%20state-of-the-art%20SfM%20systems%20in%20completeness%20by%20over%20200%25%2C%0Awithout%20sacrificing%20accuracy.%20Our%20results%20point%20to%20the%20significance%20of%20our%0Awork%3A%20poses%20of%2081%25%20of%20panoramas%20are%20localized%20in%20the%20first%202%20connected%0Acomponents%20%28CCs%29%2C%20and%2089%25%20in%20the%20first%203%20CCs.%20Code%20and%20models%20are%20publicly%0Aavailable%20at%20https%3A//github.com/zillow/salve.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19390v1&entry.124074799=Read"},
{"title": "HUWSOD: Holistic Self-training for Unified Weakly Supervised Object\n  Detection", "author": "Liujuan Cao and Jianghang Lin and Zebo Hong and Yunhang Shen and Shaohui Lin and Chao Chen and Rongrong Ji", "abstract": "  Most WSOD methods rely on traditional object proposals to generate candidate\nregions and are confronted with unstable training, which easily gets stuck in a\npoor local optimum. In this paper, we introduce a unified, high-capacity weakly\nsupervised object detection (WSOD) network called HUWSOD, which utilizes a\ncomprehensive self-training framework without needing external modules or\nadditional supervision. HUWSOD innovatively incorporates a self-supervised\nproposal generator and an autoencoder proposal generator with a multi-rate\nresampling pyramid to replace traditional object proposals, enabling end-to-end\nWSOD training and inference. Additionally, we implement a holistic\nself-training scheme that refines detection scores and coordinates through\nstep-wise entropy minimization and consistency-constraint regularization,\nensuring consistent predictions across stochastic augmentations of the same\nimage. Extensive experiments on PASCAL VOC and MS COCO demonstrate that HUWSOD\ncompetes with state-of-the-art WSOD methods, eliminating the need for offline\nproposals and additional data. The peak performance of HUWSOD approaches that\nof fully-supervised Faster R-CNN. Our findings also indicate that randomly\ninitialized boxes, although significantly different from well-designed offline\nobject proposals, are effective for WSOD training.\n", "link": "http://arxiv.org/abs/2406.19394v1", "date": "2024-06-27", "relevancy": 2.715, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5834}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5277}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HUWSOD%3A%20Holistic%20Self-training%20for%20Unified%20Weakly%20Supervised%20Object%0A%20%20Detection&body=Title%3A%20HUWSOD%3A%20Holistic%20Self-training%20for%20Unified%20Weakly%20Supervised%20Object%0A%20%20Detection%0AAuthor%3A%20Liujuan%20Cao%20and%20Jianghang%20Lin%20and%20Zebo%20Hong%20and%20Yunhang%20Shen%20and%20Shaohui%20Lin%20and%20Chao%20Chen%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20Most%20WSOD%20methods%20rely%20on%20traditional%20object%20proposals%20to%20generate%20candidate%0Aregions%20and%20are%20confronted%20with%20unstable%20training%2C%20which%20easily%20gets%20stuck%20in%20a%0Apoor%20local%20optimum.%20In%20this%20paper%2C%20we%20introduce%20a%20unified%2C%20high-capacity%20weakly%0Asupervised%20object%20detection%20%28WSOD%29%20network%20called%20HUWSOD%2C%20which%20utilizes%20a%0Acomprehensive%20self-training%20framework%20without%20needing%20external%20modules%20or%0Aadditional%20supervision.%20HUWSOD%20innovatively%20incorporates%20a%20self-supervised%0Aproposal%20generator%20and%20an%20autoencoder%20proposal%20generator%20with%20a%20multi-rate%0Aresampling%20pyramid%20to%20replace%20traditional%20object%20proposals%2C%20enabling%20end-to-end%0AWSOD%20training%20and%20inference.%20Additionally%2C%20we%20implement%20a%20holistic%0Aself-training%20scheme%20that%20refines%20detection%20scores%20and%20coordinates%20through%0Astep-wise%20entropy%20minimization%20and%20consistency-constraint%20regularization%2C%0Aensuring%20consistent%20predictions%20across%20stochastic%20augmentations%20of%20the%20same%0Aimage.%20Extensive%20experiments%20on%20PASCAL%20VOC%20and%20MS%20COCO%20demonstrate%20that%20HUWSOD%0Acompetes%20with%20state-of-the-art%20WSOD%20methods%2C%20eliminating%20the%20need%20for%20offline%0Aproposals%20and%20additional%20data.%20The%20peak%20performance%20of%20HUWSOD%20approaches%20that%0Aof%20fully-supervised%20Faster%20R-CNN.%20Our%20findings%20also%20indicate%20that%20randomly%0Ainitialized%20boxes%2C%20although%20significantly%20different%20from%20well-designed%20offline%0Aobject%20proposals%2C%20are%20effective%20for%20WSOD%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHUWSOD%253A%2520Holistic%2520Self-training%2520for%2520Unified%2520Weakly%2520Supervised%2520Object%250A%2520%2520Detection%26entry.906535625%3DLiujuan%2520Cao%2520and%2520Jianghang%2520Lin%2520and%2520Zebo%2520Hong%2520and%2520Yunhang%2520Shen%2520and%2520Shaohui%2520Lin%2520and%2520Chao%2520Chen%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520Most%2520WSOD%2520methods%2520rely%2520on%2520traditional%2520object%2520proposals%2520to%2520generate%2520candidate%250Aregions%2520and%2520are%2520confronted%2520with%2520unstable%2520training%252C%2520which%2520easily%2520gets%2520stuck%2520in%2520a%250Apoor%2520local%2520optimum.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520unified%252C%2520high-capacity%2520weakly%250Asupervised%2520object%2520detection%2520%2528WSOD%2529%2520network%2520called%2520HUWSOD%252C%2520which%2520utilizes%2520a%250Acomprehensive%2520self-training%2520framework%2520without%2520needing%2520external%2520modules%2520or%250Aadditional%2520supervision.%2520HUWSOD%2520innovatively%2520incorporates%2520a%2520self-supervised%250Aproposal%2520generator%2520and%2520an%2520autoencoder%2520proposal%2520generator%2520with%2520a%2520multi-rate%250Aresampling%2520pyramid%2520to%2520replace%2520traditional%2520object%2520proposals%252C%2520enabling%2520end-to-end%250AWSOD%2520training%2520and%2520inference.%2520Additionally%252C%2520we%2520implement%2520a%2520holistic%250Aself-training%2520scheme%2520that%2520refines%2520detection%2520scores%2520and%2520coordinates%2520through%250Astep-wise%2520entropy%2520minimization%2520and%2520consistency-constraint%2520regularization%252C%250Aensuring%2520consistent%2520predictions%2520across%2520stochastic%2520augmentations%2520of%2520the%2520same%250Aimage.%2520Extensive%2520experiments%2520on%2520PASCAL%2520VOC%2520and%2520MS%2520COCO%2520demonstrate%2520that%2520HUWSOD%250Acompetes%2520with%2520state-of-the-art%2520WSOD%2520methods%252C%2520eliminating%2520the%2520need%2520for%2520offline%250Aproposals%2520and%2520additional%2520data.%2520The%2520peak%2520performance%2520of%2520HUWSOD%2520approaches%2520that%250Aof%2520fully-supervised%2520Faster%2520R-CNN.%2520Our%2520findings%2520also%2520indicate%2520that%2520randomly%250Ainitialized%2520boxes%252C%2520although%2520significantly%2520different%2520from%2520well-designed%2520offline%250Aobject%2520proposals%252C%2520are%2520effective%2520for%2520WSOD%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HUWSOD%3A%20Holistic%20Self-training%20for%20Unified%20Weakly%20Supervised%20Object%0A%20%20Detection&entry.906535625=Liujuan%20Cao%20and%20Jianghang%20Lin%20and%20Zebo%20Hong%20and%20Yunhang%20Shen%20and%20Shaohui%20Lin%20and%20Chao%20Chen%20and%20Rongrong%20Ji&entry.1292438233=%20%20Most%20WSOD%20methods%20rely%20on%20traditional%20object%20proposals%20to%20generate%20candidate%0Aregions%20and%20are%20confronted%20with%20unstable%20training%2C%20which%20easily%20gets%20stuck%20in%20a%0Apoor%20local%20optimum.%20In%20this%20paper%2C%20we%20introduce%20a%20unified%2C%20high-capacity%20weakly%0Asupervised%20object%20detection%20%28WSOD%29%20network%20called%20HUWSOD%2C%20which%20utilizes%20a%0Acomprehensive%20self-training%20framework%20without%20needing%20external%20modules%20or%0Aadditional%20supervision.%20HUWSOD%20innovatively%20incorporates%20a%20self-supervised%0Aproposal%20generator%20and%20an%20autoencoder%20proposal%20generator%20with%20a%20multi-rate%0Aresampling%20pyramid%20to%20replace%20traditional%20object%20proposals%2C%20enabling%20end-to-end%0AWSOD%20training%20and%20inference.%20Additionally%2C%20we%20implement%20a%20holistic%0Aself-training%20scheme%20that%20refines%20detection%20scores%20and%20coordinates%20through%0Astep-wise%20entropy%20minimization%20and%20consistency-constraint%20regularization%2C%0Aensuring%20consistent%20predictions%20across%20stochastic%20augmentations%20of%20the%20same%0Aimage.%20Extensive%20experiments%20on%20PASCAL%20VOC%20and%20MS%20COCO%20demonstrate%20that%20HUWSOD%0Acompetes%20with%20state-of-the-art%20WSOD%20methods%2C%20eliminating%20the%20need%20for%20offline%0Aproposals%20and%20additional%20data.%20The%20peak%20performance%20of%20HUWSOD%20approaches%20that%0Aof%20fully-supervised%20Faster%20R-CNN.%20Our%20findings%20also%20indicate%20that%20randomly%0Ainitialized%20boxes%2C%20although%20significantly%20different%20from%20well-designed%20offline%0Aobject%20proposals%2C%20are%20effective%20for%20WSOD%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19394v1&entry.124074799=Read"},
{"title": "BackMix: Mitigating Shortcut Learning in Echocardiography with Minimal\n  Supervision", "author": "Kit Mills Bransby and Arian Beqiri and Woo-Jin Cho Kim and Jorge Oliveira and Agisilaos Chartsias and Alberto Gomez", "abstract": "  Neural networks can learn spurious correlations that lead to the correct\nprediction in a validation set, but generalise poorly because the predictions\nare right for the wrong reason. This undesired learning of naive shortcuts\n(Clever Hans effect) can happen for example in echocardiogram view\nclassification when background cues (e.g. metadata) are biased towards a class\nand the model learns to focus on those background features instead of on the\nimage content. We propose a simple, yet effective random background\naugmentation method called BackMix, which samples random backgrounds from other\nexamples in the training set. By enforcing the background to be uncorrelated\nwith the outcome, the model learns to focus on the data within the ultrasound\nsector and becomes invariant to the regions outside this. We extend our method\nin a semi-supervised setting, finding that the positive effects of BackMix are\nmaintained with as few as 5% of segmentation labels. A loss weighting\nmechanism, wBackMix, is also proposed to increase the contribution of the\naugmented examples. We validate our method on both in-distribution and\nout-of-distribution datasets, demonstrating significant improvements in\nclassification accuracy, region focus and generalisability. Our source code is\navailable at: https://github.com/kitbransby/BackMix\n", "link": "http://arxiv.org/abs/2406.19148v1", "date": "2024-06-27", "relevancy": 2.7017, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5554}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5405}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BackMix%3A%20Mitigating%20Shortcut%20Learning%20in%20Echocardiography%20with%20Minimal%0A%20%20Supervision&body=Title%3A%20BackMix%3A%20Mitigating%20Shortcut%20Learning%20in%20Echocardiography%20with%20Minimal%0A%20%20Supervision%0AAuthor%3A%20Kit%20Mills%20Bransby%20and%20Arian%20Beqiri%20and%20Woo-Jin%20Cho%20Kim%20and%20Jorge%20Oliveira%20and%20Agisilaos%20Chartsias%20and%20Alberto%20Gomez%0AAbstract%3A%20%20%20Neural%20networks%20can%20learn%20spurious%20correlations%20that%20lead%20to%20the%20correct%0Aprediction%20in%20a%20validation%20set%2C%20but%20generalise%20poorly%20because%20the%20predictions%0Aare%20right%20for%20the%20wrong%20reason.%20This%20undesired%20learning%20of%20naive%20shortcuts%0A%28Clever%20Hans%20effect%29%20can%20happen%20for%20example%20in%20echocardiogram%20view%0Aclassification%20when%20background%20cues%20%28e.g.%20metadata%29%20are%20biased%20towards%20a%20class%0Aand%20the%20model%20learns%20to%20focus%20on%20those%20background%20features%20instead%20of%20on%20the%0Aimage%20content.%20We%20propose%20a%20simple%2C%20yet%20effective%20random%20background%0Aaugmentation%20method%20called%20BackMix%2C%20which%20samples%20random%20backgrounds%20from%20other%0Aexamples%20in%20the%20training%20set.%20By%20enforcing%20the%20background%20to%20be%20uncorrelated%0Awith%20the%20outcome%2C%20the%20model%20learns%20to%20focus%20on%20the%20data%20within%20the%20ultrasound%0Asector%20and%20becomes%20invariant%20to%20the%20regions%20outside%20this.%20We%20extend%20our%20method%0Ain%20a%20semi-supervised%20setting%2C%20finding%20that%20the%20positive%20effects%20of%20BackMix%20are%0Amaintained%20with%20as%20few%20as%205%25%20of%20segmentation%20labels.%20A%20loss%20weighting%0Amechanism%2C%20wBackMix%2C%20is%20also%20proposed%20to%20increase%20the%20contribution%20of%20the%0Aaugmented%20examples.%20We%20validate%20our%20method%20on%20both%20in-distribution%20and%0Aout-of-distribution%20datasets%2C%20demonstrating%20significant%20improvements%20in%0Aclassification%20accuracy%2C%20region%20focus%20and%20generalisability.%20Our%20source%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/kitbransby/BackMix%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19148v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBackMix%253A%2520Mitigating%2520Shortcut%2520Learning%2520in%2520Echocardiography%2520with%2520Minimal%250A%2520%2520Supervision%26entry.906535625%3DKit%2520Mills%2520Bransby%2520and%2520Arian%2520Beqiri%2520and%2520Woo-Jin%2520Cho%2520Kim%2520and%2520Jorge%2520Oliveira%2520and%2520Agisilaos%2520Chartsias%2520and%2520Alberto%2520Gomez%26entry.1292438233%3D%2520%2520Neural%2520networks%2520can%2520learn%2520spurious%2520correlations%2520that%2520lead%2520to%2520the%2520correct%250Aprediction%2520in%2520a%2520validation%2520set%252C%2520but%2520generalise%2520poorly%2520because%2520the%2520predictions%250Aare%2520right%2520for%2520the%2520wrong%2520reason.%2520This%2520undesired%2520learning%2520of%2520naive%2520shortcuts%250A%2528Clever%2520Hans%2520effect%2529%2520can%2520happen%2520for%2520example%2520in%2520echocardiogram%2520view%250Aclassification%2520when%2520background%2520cues%2520%2528e.g.%2520metadata%2529%2520are%2520biased%2520towards%2520a%2520class%250Aand%2520the%2520model%2520learns%2520to%2520focus%2520on%2520those%2520background%2520features%2520instead%2520of%2520on%2520the%250Aimage%2520content.%2520We%2520propose%2520a%2520simple%252C%2520yet%2520effective%2520random%2520background%250Aaugmentation%2520method%2520called%2520BackMix%252C%2520which%2520samples%2520random%2520backgrounds%2520from%2520other%250Aexamples%2520in%2520the%2520training%2520set.%2520By%2520enforcing%2520the%2520background%2520to%2520be%2520uncorrelated%250Awith%2520the%2520outcome%252C%2520the%2520model%2520learns%2520to%2520focus%2520on%2520the%2520data%2520within%2520the%2520ultrasound%250Asector%2520and%2520becomes%2520invariant%2520to%2520the%2520regions%2520outside%2520this.%2520We%2520extend%2520our%2520method%250Ain%2520a%2520semi-supervised%2520setting%252C%2520finding%2520that%2520the%2520positive%2520effects%2520of%2520BackMix%2520are%250Amaintained%2520with%2520as%2520few%2520as%25205%2525%2520of%2520segmentation%2520labels.%2520A%2520loss%2520weighting%250Amechanism%252C%2520wBackMix%252C%2520is%2520also%2520proposed%2520to%2520increase%2520the%2520contribution%2520of%2520the%250Aaugmented%2520examples.%2520We%2520validate%2520our%2520method%2520on%2520both%2520in-distribution%2520and%250Aout-of-distribution%2520datasets%252C%2520demonstrating%2520significant%2520improvements%2520in%250Aclassification%2520accuracy%252C%2520region%2520focus%2520and%2520generalisability.%2520Our%2520source%2520code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/kitbransby/BackMix%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19148v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BackMix%3A%20Mitigating%20Shortcut%20Learning%20in%20Echocardiography%20with%20Minimal%0A%20%20Supervision&entry.906535625=Kit%20Mills%20Bransby%20and%20Arian%20Beqiri%20and%20Woo-Jin%20Cho%20Kim%20and%20Jorge%20Oliveira%20and%20Agisilaos%20Chartsias%20and%20Alberto%20Gomez&entry.1292438233=%20%20Neural%20networks%20can%20learn%20spurious%20correlations%20that%20lead%20to%20the%20correct%0Aprediction%20in%20a%20validation%20set%2C%20but%20generalise%20poorly%20because%20the%20predictions%0Aare%20right%20for%20the%20wrong%20reason.%20This%20undesired%20learning%20of%20naive%20shortcuts%0A%28Clever%20Hans%20effect%29%20can%20happen%20for%20example%20in%20echocardiogram%20view%0Aclassification%20when%20background%20cues%20%28e.g.%20metadata%29%20are%20biased%20towards%20a%20class%0Aand%20the%20model%20learns%20to%20focus%20on%20those%20background%20features%20instead%20of%20on%20the%0Aimage%20content.%20We%20propose%20a%20simple%2C%20yet%20effective%20random%20background%0Aaugmentation%20method%20called%20BackMix%2C%20which%20samples%20random%20backgrounds%20from%20other%0Aexamples%20in%20the%20training%20set.%20By%20enforcing%20the%20background%20to%20be%20uncorrelated%0Awith%20the%20outcome%2C%20the%20model%20learns%20to%20focus%20on%20the%20data%20within%20the%20ultrasound%0Asector%20and%20becomes%20invariant%20to%20the%20regions%20outside%20this.%20We%20extend%20our%20method%0Ain%20a%20semi-supervised%20setting%2C%20finding%20that%20the%20positive%20effects%20of%20BackMix%20are%0Amaintained%20with%20as%20few%20as%205%25%20of%20segmentation%20labels.%20A%20loss%20weighting%0Amechanism%2C%20wBackMix%2C%20is%20also%20proposed%20to%20increase%20the%20contribution%20of%20the%0Aaugmented%20examples.%20We%20validate%20our%20method%20on%20both%20in-distribution%20and%0Aout-of-distribution%20datasets%2C%20demonstrating%20significant%20improvements%20in%0Aclassification%20accuracy%2C%20region%20focus%20and%20generalisability.%20Our%20source%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/kitbransby/BackMix%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19148v1&entry.124074799=Read"},
{"title": "CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI", "author": "Zi Wang and Fanwen Wang and Chen Qin and Jun Lyu and Ouyang Cheng and Shuo Wang and Yan Li and Mengyao Yu and Haoyu Zhang and Kunyuan Guo and Zhang Shi and Qirong Li and Ziqiang Xu and Yajing Zhang and Hao Li and Sha Hua and Binghua Chen and Longyu Sun and Mengting Sun and Qin Li and Ying-Hua Chu and Wenjia Bai and Jing Qin and Xiahai Zhuang and Claudia Prieto and Alistair Young and Michael Markl and He Wang and Lianming Wu and Guang Yang and Xiaobo Qu and Chengyan Wang", "abstract": "  Cardiac magnetic resonance imaging (MRI) has emerged as a clinically\ngold-standard technique for diagnosing cardiac diseases, thanks to its ability\nto provide diverse information with multiple modalities and anatomical views.\nAccelerated cardiac MRI is highly expected to achieve time-efficient and\npatient-friendly imaging, and then advanced image reconstruction approaches are\nrequired to recover high-quality, clinically interpretable images from\nundersampled measurements. However, the lack of publicly available cardiac MRI\nk-space dataset in terms of both quantity and diversity has severely hindered\nsubstantial technological progress, particularly for data-driven artificial\nintelligence. Here, we provide a standardized, diverse, and high-quality\nCMRxRecon2024 dataset to facilitate the technical development, fair evaluation,\nand clinical transfer of cardiac MRI reconstruction approaches, towards\npromoting the universal frameworks that enable fast and robust reconstructions\nacross different cardiac MRI protocols in clinical practice. To the best of our\nknowledge, the CMRxRecon2024 dataset is the largest and most diverse publicly\navailable cardiac k-space dataset. It is acquired from 330 healthy volunteers,\ncovering commonly used modalities, anatomical views, and acquisition\ntrajectories in clinical cardiac MRI workflows. Besides, an open platform with\ntutorials, benchmarks, and data processing tools is provided to facilitate data\nusage, advanced method development, and fair performance evaluation.\n", "link": "http://arxiv.org/abs/2406.19043v1", "date": "2024-06-27", "relevancy": 2.6559, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5422}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5422}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CMRxRecon2024%3A%20A%20Multi-Modality%2C%20Multi-View%20K-Space%20Dataset%20Boosting%0A%20%20Universal%20Machine%20Learning%20for%20Accelerated%20Cardiac%20MRI&body=Title%3A%20CMRxRecon2024%3A%20A%20Multi-Modality%2C%20Multi-View%20K-Space%20Dataset%20Boosting%0A%20%20Universal%20Machine%20Learning%20for%20Accelerated%20Cardiac%20MRI%0AAuthor%3A%20Zi%20Wang%20and%20Fanwen%20Wang%20and%20Chen%20Qin%20and%20Jun%20Lyu%20and%20Ouyang%20Cheng%20and%20Shuo%20Wang%20and%20Yan%20Li%20and%20Mengyao%20Yu%20and%20Haoyu%20Zhang%20and%20Kunyuan%20Guo%20and%20Zhang%20Shi%20and%20Qirong%20Li%20and%20Ziqiang%20Xu%20and%20Yajing%20Zhang%20and%20Hao%20Li%20and%20Sha%20Hua%20and%20Binghua%20Chen%20and%20Longyu%20Sun%20and%20Mengting%20Sun%20and%20Qin%20Li%20and%20Ying-Hua%20Chu%20and%20Wenjia%20Bai%20and%20Jing%20Qin%20and%20Xiahai%20Zhuang%20and%20Claudia%20Prieto%20and%20Alistair%20Young%20and%20Michael%20Markl%20and%20He%20Wang%20and%20Lianming%20Wu%20and%20Guang%20Yang%20and%20Xiaobo%20Qu%20and%20Chengyan%20Wang%0AAbstract%3A%20%20%20Cardiac%20magnetic%20resonance%20imaging%20%28MRI%29%20has%20emerged%20as%20a%20clinically%0Agold-standard%20technique%20for%20diagnosing%20cardiac%20diseases%2C%20thanks%20to%20its%20ability%0Ato%20provide%20diverse%20information%20with%20multiple%20modalities%20and%20anatomical%20views.%0AAccelerated%20cardiac%20MRI%20is%20highly%20expected%20to%20achieve%20time-efficient%20and%0Apatient-friendly%20imaging%2C%20and%20then%20advanced%20image%20reconstruction%20approaches%20are%0Arequired%20to%20recover%20high-quality%2C%20clinically%20interpretable%20images%20from%0Aundersampled%20measurements.%20However%2C%20the%20lack%20of%20publicly%20available%20cardiac%20MRI%0Ak-space%20dataset%20in%20terms%20of%20both%20quantity%20and%20diversity%20has%20severely%20hindered%0Asubstantial%20technological%20progress%2C%20particularly%20for%20data-driven%20artificial%0Aintelligence.%20Here%2C%20we%20provide%20a%20standardized%2C%20diverse%2C%20and%20high-quality%0ACMRxRecon2024%20dataset%20to%20facilitate%20the%20technical%20development%2C%20fair%20evaluation%2C%0Aand%20clinical%20transfer%20of%20cardiac%20MRI%20reconstruction%20approaches%2C%20towards%0Apromoting%20the%20universal%20frameworks%20that%20enable%20fast%20and%20robust%20reconstructions%0Aacross%20different%20cardiac%20MRI%20protocols%20in%20clinical%20practice.%20To%20the%20best%20of%20our%0Aknowledge%2C%20the%20CMRxRecon2024%20dataset%20is%20the%20largest%20and%20most%20diverse%20publicly%0Aavailable%20cardiac%20k-space%20dataset.%20It%20is%20acquired%20from%20330%20healthy%20volunteers%2C%0Acovering%20commonly%20used%20modalities%2C%20anatomical%20views%2C%20and%20acquisition%0Atrajectories%20in%20clinical%20cardiac%20MRI%20workflows.%20Besides%2C%20an%20open%20platform%20with%0Atutorials%2C%20benchmarks%2C%20and%20data%20processing%20tools%20is%20provided%20to%20facilitate%20data%0Ausage%2C%20advanced%20method%20development%2C%20and%20fair%20performance%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19043v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCMRxRecon2024%253A%2520A%2520Multi-Modality%252C%2520Multi-View%2520K-Space%2520Dataset%2520Boosting%250A%2520%2520Universal%2520Machine%2520Learning%2520for%2520Accelerated%2520Cardiac%2520MRI%26entry.906535625%3DZi%2520Wang%2520and%2520Fanwen%2520Wang%2520and%2520Chen%2520Qin%2520and%2520Jun%2520Lyu%2520and%2520Ouyang%2520Cheng%2520and%2520Shuo%2520Wang%2520and%2520Yan%2520Li%2520and%2520Mengyao%2520Yu%2520and%2520Haoyu%2520Zhang%2520and%2520Kunyuan%2520Guo%2520and%2520Zhang%2520Shi%2520and%2520Qirong%2520Li%2520and%2520Ziqiang%2520Xu%2520and%2520Yajing%2520Zhang%2520and%2520Hao%2520Li%2520and%2520Sha%2520Hua%2520and%2520Binghua%2520Chen%2520and%2520Longyu%2520Sun%2520and%2520Mengting%2520Sun%2520and%2520Qin%2520Li%2520and%2520Ying-Hua%2520Chu%2520and%2520Wenjia%2520Bai%2520and%2520Jing%2520Qin%2520and%2520Xiahai%2520Zhuang%2520and%2520Claudia%2520Prieto%2520and%2520Alistair%2520Young%2520and%2520Michael%2520Markl%2520and%2520He%2520Wang%2520and%2520Lianming%2520Wu%2520and%2520Guang%2520Yang%2520and%2520Xiaobo%2520Qu%2520and%2520Chengyan%2520Wang%26entry.1292438233%3D%2520%2520Cardiac%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520has%2520emerged%2520as%2520a%2520clinically%250Agold-standard%2520technique%2520for%2520diagnosing%2520cardiac%2520diseases%252C%2520thanks%2520to%2520its%2520ability%250Ato%2520provide%2520diverse%2520information%2520with%2520multiple%2520modalities%2520and%2520anatomical%2520views.%250AAccelerated%2520cardiac%2520MRI%2520is%2520highly%2520expected%2520to%2520achieve%2520time-efficient%2520and%250Apatient-friendly%2520imaging%252C%2520and%2520then%2520advanced%2520image%2520reconstruction%2520approaches%2520are%250Arequired%2520to%2520recover%2520high-quality%252C%2520clinically%2520interpretable%2520images%2520from%250Aundersampled%2520measurements.%2520However%252C%2520the%2520lack%2520of%2520publicly%2520available%2520cardiac%2520MRI%250Ak-space%2520dataset%2520in%2520terms%2520of%2520both%2520quantity%2520and%2520diversity%2520has%2520severely%2520hindered%250Asubstantial%2520technological%2520progress%252C%2520particularly%2520for%2520data-driven%2520artificial%250Aintelligence.%2520Here%252C%2520we%2520provide%2520a%2520standardized%252C%2520diverse%252C%2520and%2520high-quality%250ACMRxRecon2024%2520dataset%2520to%2520facilitate%2520the%2520technical%2520development%252C%2520fair%2520evaluation%252C%250Aand%2520clinical%2520transfer%2520of%2520cardiac%2520MRI%2520reconstruction%2520approaches%252C%2520towards%250Apromoting%2520the%2520universal%2520frameworks%2520that%2520enable%2520fast%2520and%2520robust%2520reconstructions%250Aacross%2520different%2520cardiac%2520MRI%2520protocols%2520in%2520clinical%2520practice.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520the%2520CMRxRecon2024%2520dataset%2520is%2520the%2520largest%2520and%2520most%2520diverse%2520publicly%250Aavailable%2520cardiac%2520k-space%2520dataset.%2520It%2520is%2520acquired%2520from%2520330%2520healthy%2520volunteers%252C%250Acovering%2520commonly%2520used%2520modalities%252C%2520anatomical%2520views%252C%2520and%2520acquisition%250Atrajectories%2520in%2520clinical%2520cardiac%2520MRI%2520workflows.%2520Besides%252C%2520an%2520open%2520platform%2520with%250Atutorials%252C%2520benchmarks%252C%2520and%2520data%2520processing%2520tools%2520is%2520provided%2520to%2520facilitate%2520data%250Ausage%252C%2520advanced%2520method%2520development%252C%2520and%2520fair%2520performance%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19043v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CMRxRecon2024%3A%20A%20Multi-Modality%2C%20Multi-View%20K-Space%20Dataset%20Boosting%0A%20%20Universal%20Machine%20Learning%20for%20Accelerated%20Cardiac%20MRI&entry.906535625=Zi%20Wang%20and%20Fanwen%20Wang%20and%20Chen%20Qin%20and%20Jun%20Lyu%20and%20Ouyang%20Cheng%20and%20Shuo%20Wang%20and%20Yan%20Li%20and%20Mengyao%20Yu%20and%20Haoyu%20Zhang%20and%20Kunyuan%20Guo%20and%20Zhang%20Shi%20and%20Qirong%20Li%20and%20Ziqiang%20Xu%20and%20Yajing%20Zhang%20and%20Hao%20Li%20and%20Sha%20Hua%20and%20Binghua%20Chen%20and%20Longyu%20Sun%20and%20Mengting%20Sun%20and%20Qin%20Li%20and%20Ying-Hua%20Chu%20and%20Wenjia%20Bai%20and%20Jing%20Qin%20and%20Xiahai%20Zhuang%20and%20Claudia%20Prieto%20and%20Alistair%20Young%20and%20Michael%20Markl%20and%20He%20Wang%20and%20Lianming%20Wu%20and%20Guang%20Yang%20and%20Xiaobo%20Qu%20and%20Chengyan%20Wang&entry.1292438233=%20%20Cardiac%20magnetic%20resonance%20imaging%20%28MRI%29%20has%20emerged%20as%20a%20clinically%0Agold-standard%20technique%20for%20diagnosing%20cardiac%20diseases%2C%20thanks%20to%20its%20ability%0Ato%20provide%20diverse%20information%20with%20multiple%20modalities%20and%20anatomical%20views.%0AAccelerated%20cardiac%20MRI%20is%20highly%20expected%20to%20achieve%20time-efficient%20and%0Apatient-friendly%20imaging%2C%20and%20then%20advanced%20image%20reconstruction%20approaches%20are%0Arequired%20to%20recover%20high-quality%2C%20clinically%20interpretable%20images%20from%0Aundersampled%20measurements.%20However%2C%20the%20lack%20of%20publicly%20available%20cardiac%20MRI%0Ak-space%20dataset%20in%20terms%20of%20both%20quantity%20and%20diversity%20has%20severely%20hindered%0Asubstantial%20technological%20progress%2C%20particularly%20for%20data-driven%20artificial%0Aintelligence.%20Here%2C%20we%20provide%20a%20standardized%2C%20diverse%2C%20and%20high-quality%0ACMRxRecon2024%20dataset%20to%20facilitate%20the%20technical%20development%2C%20fair%20evaluation%2C%0Aand%20clinical%20transfer%20of%20cardiac%20MRI%20reconstruction%20approaches%2C%20towards%0Apromoting%20the%20universal%20frameworks%20that%20enable%20fast%20and%20robust%20reconstructions%0Aacross%20different%20cardiac%20MRI%20protocols%20in%20clinical%20practice.%20To%20the%20best%20of%20our%0Aknowledge%2C%20the%20CMRxRecon2024%20dataset%20is%20the%20largest%20and%20most%20diverse%20publicly%0Aavailable%20cardiac%20k-space%20dataset.%20It%20is%20acquired%20from%20330%20healthy%20volunteers%2C%0Acovering%20commonly%20used%20modalities%2C%20anatomical%20views%2C%20and%20acquisition%0Atrajectories%20in%20clinical%20cardiac%20MRI%20workflows.%20Besides%2C%20an%20open%20platform%20with%0Atutorials%2C%20benchmarks%2C%20and%20data%20processing%20tools%20is%20provided%20to%20facilitate%20data%0Ausage%2C%20advanced%20method%20development%2C%20and%20fair%20performance%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19043v1&entry.124074799=Read"},
{"title": "Enhanced Data Transfer Cooperating with Artificial Triplets for Scene\n  Graph Generation", "author": "KuanChao Chu and Satoshi Yamazaki and Hideki Nakayama", "abstract": "  This work focuses on training dataset enhancement of informative relational\ntriplets for Scene Graph Generation (SGG). Due to the lack of effective\nsupervision, the current SGG model predictions perform poorly for informative\nrelational triplets with inadequate training samples. Therefore, we propose two\nnovel training dataset enhancement modules: Feature Space Triplet Augmentation\n(FSTA) and Soft Transfer. FSTA leverages a feature generator trained to\ngenerate representations of an object in relational triplets. The biased\nprediction based sampling in FSTA efficiently augments artificial triplets\nfocusing on the challenging ones. In addition, we introduce Soft Transfer,\nwhich assigns soft predicate labels to general relational triplets to make more\nsupervisions for informative predicate classes effectively. Experimental\nresults show that integrating FSTA and Soft Transfer achieve high levels of\nboth Recall and mean Recall in Visual Genome dataset. The mean of Recall and\nmean Recall is the highest among all the existing model-agnostic methods.\n", "link": "http://arxiv.org/abs/2406.19316v1", "date": "2024-06-27", "relevancy": 2.6062, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.525}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5202}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Data%20Transfer%20Cooperating%20with%20Artificial%20Triplets%20for%20Scene%0A%20%20Graph%20Generation&body=Title%3A%20Enhanced%20Data%20Transfer%20Cooperating%20with%20Artificial%20Triplets%20for%20Scene%0A%20%20Graph%20Generation%0AAuthor%3A%20KuanChao%20Chu%20and%20Satoshi%20Yamazaki%20and%20Hideki%20Nakayama%0AAbstract%3A%20%20%20This%20work%20focuses%20on%20training%20dataset%20enhancement%20of%20informative%20relational%0Atriplets%20for%20Scene%20Graph%20Generation%20%28SGG%29.%20Due%20to%20the%20lack%20of%20effective%0Asupervision%2C%20the%20current%20SGG%20model%20predictions%20perform%20poorly%20for%20informative%0Arelational%20triplets%20with%20inadequate%20training%20samples.%20Therefore%2C%20we%20propose%20two%0Anovel%20training%20dataset%20enhancement%20modules%3A%20Feature%20Space%20Triplet%20Augmentation%0A%28FSTA%29%20and%20Soft%20Transfer.%20FSTA%20leverages%20a%20feature%20generator%20trained%20to%0Agenerate%20representations%20of%20an%20object%20in%20relational%20triplets.%20The%20biased%0Aprediction%20based%20sampling%20in%20FSTA%20efficiently%20augments%20artificial%20triplets%0Afocusing%20on%20the%20challenging%20ones.%20In%20addition%2C%20we%20introduce%20Soft%20Transfer%2C%0Awhich%20assigns%20soft%20predicate%20labels%20to%20general%20relational%20triplets%20to%20make%20more%0Asupervisions%20for%20informative%20predicate%20classes%20effectively.%20Experimental%0Aresults%20show%20that%20integrating%20FSTA%20and%20Soft%20Transfer%20achieve%20high%20levels%20of%0Aboth%20Recall%20and%20mean%20Recall%20in%20Visual%20Genome%20dataset.%20The%20mean%20of%20Recall%20and%0Amean%20Recall%20is%20the%20highest%20among%20all%20the%20existing%20model-agnostic%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19316v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Data%2520Transfer%2520Cooperating%2520with%2520Artificial%2520Triplets%2520for%2520Scene%250A%2520%2520Graph%2520Generation%26entry.906535625%3DKuanChao%2520Chu%2520and%2520Satoshi%2520Yamazaki%2520and%2520Hideki%2520Nakayama%26entry.1292438233%3D%2520%2520This%2520work%2520focuses%2520on%2520training%2520dataset%2520enhancement%2520of%2520informative%2520relational%250Atriplets%2520for%2520Scene%2520Graph%2520Generation%2520%2528SGG%2529.%2520Due%2520to%2520the%2520lack%2520of%2520effective%250Asupervision%252C%2520the%2520current%2520SGG%2520model%2520predictions%2520perform%2520poorly%2520for%2520informative%250Arelational%2520triplets%2520with%2520inadequate%2520training%2520samples.%2520Therefore%252C%2520we%2520propose%2520two%250Anovel%2520training%2520dataset%2520enhancement%2520modules%253A%2520Feature%2520Space%2520Triplet%2520Augmentation%250A%2528FSTA%2529%2520and%2520Soft%2520Transfer.%2520FSTA%2520leverages%2520a%2520feature%2520generator%2520trained%2520to%250Agenerate%2520representations%2520of%2520an%2520object%2520in%2520relational%2520triplets.%2520The%2520biased%250Aprediction%2520based%2520sampling%2520in%2520FSTA%2520efficiently%2520augments%2520artificial%2520triplets%250Afocusing%2520on%2520the%2520challenging%2520ones.%2520In%2520addition%252C%2520we%2520introduce%2520Soft%2520Transfer%252C%250Awhich%2520assigns%2520soft%2520predicate%2520labels%2520to%2520general%2520relational%2520triplets%2520to%2520make%2520more%250Asupervisions%2520for%2520informative%2520predicate%2520classes%2520effectively.%2520Experimental%250Aresults%2520show%2520that%2520integrating%2520FSTA%2520and%2520Soft%2520Transfer%2520achieve%2520high%2520levels%2520of%250Aboth%2520Recall%2520and%2520mean%2520Recall%2520in%2520Visual%2520Genome%2520dataset.%2520The%2520mean%2520of%2520Recall%2520and%250Amean%2520Recall%2520is%2520the%2520highest%2520among%2520all%2520the%2520existing%2520model-agnostic%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19316v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Data%20Transfer%20Cooperating%20with%20Artificial%20Triplets%20for%20Scene%0A%20%20Graph%20Generation&entry.906535625=KuanChao%20Chu%20and%20Satoshi%20Yamazaki%20and%20Hideki%20Nakayama&entry.1292438233=%20%20This%20work%20focuses%20on%20training%20dataset%20enhancement%20of%20informative%20relational%0Atriplets%20for%20Scene%20Graph%20Generation%20%28SGG%29.%20Due%20to%20the%20lack%20of%20effective%0Asupervision%2C%20the%20current%20SGG%20model%20predictions%20perform%20poorly%20for%20informative%0Arelational%20triplets%20with%20inadequate%20training%20samples.%20Therefore%2C%20we%20propose%20two%0Anovel%20training%20dataset%20enhancement%20modules%3A%20Feature%20Space%20Triplet%20Augmentation%0A%28FSTA%29%20and%20Soft%20Transfer.%20FSTA%20leverages%20a%20feature%20generator%20trained%20to%0Agenerate%20representations%20of%20an%20object%20in%20relational%20triplets.%20The%20biased%0Aprediction%20based%20sampling%20in%20FSTA%20efficiently%20augments%20artificial%20triplets%0Afocusing%20on%20the%20challenging%20ones.%20In%20addition%2C%20we%20introduce%20Soft%20Transfer%2C%0Awhich%20assigns%20soft%20predicate%20labels%20to%20general%20relational%20triplets%20to%20make%20more%0Asupervisions%20for%20informative%20predicate%20classes%20effectively.%20Experimental%0Aresults%20show%20that%20integrating%20FSTA%20and%20Soft%20Transfer%20achieve%20high%20levels%20of%0Aboth%20Recall%20and%20mean%20Recall%20in%20Visual%20Genome%20dataset.%20The%20mean%20of%20Recall%20and%0Amean%20Recall%20is%20the%20highest%20among%20all%20the%20existing%20model-agnostic%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19316v1&entry.124074799=Read"},
{"title": "T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for\n  Memory-Efficient Embeddings", "author": "Bj\u00f6rn Deiseroth and Manuel Brack and Patrick Schramowski and Kristian Kersting and Samuel Weinbach", "abstract": "  Tokenizers are crucial for encoding information in Large Language Models, but\ntheir development has recently stagnated, and they contain inherent weaknesses.\nMajor limitations include computational overhead, ineffective vocabulary use,\nand unnecessarily large embedding and head layers. Additionally, their\nperformance is biased towards a reference corpus, leading to reduced\neffectiveness for underrepresented languages.\n  To remedy these issues, we propose T-FREE, which directly embeds words\nthrough sparse activation patterns over character triplets, and does not\nrequire a reference corpus. T-FREE inherently exploits morphological\nsimilarities and allows for strong compression of embedding layers. In our\nexhaustive experimental evaluation, we achieve competitive downstream\nperformance with a parameter reduction of more than 85% on these layers.\nFurther, T-FREE shows significant improvements in cross-lingual transfer\nlearning.\n", "link": "http://arxiv.org/abs/2406.19223v1", "date": "2024-06-27", "relevancy": 2.5904, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5562}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5213}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T-FREE%3A%20Tokenizer-Free%20Generative%20LLMs%20via%20Sparse%20Representations%20for%0A%20%20Memory-Efficient%20Embeddings&body=Title%3A%20T-FREE%3A%20Tokenizer-Free%20Generative%20LLMs%20via%20Sparse%20Representations%20for%0A%20%20Memory-Efficient%20Embeddings%0AAuthor%3A%20Bj%C3%B6rn%20Deiseroth%20and%20Manuel%20Brack%20and%20Patrick%20Schramowski%20and%20Kristian%20Kersting%20and%20Samuel%20Weinbach%0AAbstract%3A%20%20%20Tokenizers%20are%20crucial%20for%20encoding%20information%20in%20Large%20Language%20Models%2C%20but%0Atheir%20development%20has%20recently%20stagnated%2C%20and%20they%20contain%20inherent%20weaknesses.%0AMajor%20limitations%20include%20computational%20overhead%2C%20ineffective%20vocabulary%20use%2C%0Aand%20unnecessarily%20large%20embedding%20and%20head%20layers.%20Additionally%2C%20their%0Aperformance%20is%20biased%20towards%20a%20reference%20corpus%2C%20leading%20to%20reduced%0Aeffectiveness%20for%20underrepresented%20languages.%0A%20%20To%20remedy%20these%20issues%2C%20we%20propose%20T-FREE%2C%20which%20directly%20embeds%20words%0Athrough%20sparse%20activation%20patterns%20over%20character%20triplets%2C%20and%20does%20not%0Arequire%20a%20reference%20corpus.%20T-FREE%20inherently%20exploits%20morphological%0Asimilarities%20and%20allows%20for%20strong%20compression%20of%20embedding%20layers.%20In%20our%0Aexhaustive%20experimental%20evaluation%2C%20we%20achieve%20competitive%20downstream%0Aperformance%20with%20a%20parameter%20reduction%20of%20more%20than%2085%25%20on%20these%20layers.%0AFurther%2C%20T-FREE%20shows%20significant%20improvements%20in%20cross-lingual%20transfer%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT-FREE%253A%2520Tokenizer-Free%2520Generative%2520LLMs%2520via%2520Sparse%2520Representations%2520for%250A%2520%2520Memory-Efficient%2520Embeddings%26entry.906535625%3DBj%25C3%25B6rn%2520Deiseroth%2520and%2520Manuel%2520Brack%2520and%2520Patrick%2520Schramowski%2520and%2520Kristian%2520Kersting%2520and%2520Samuel%2520Weinbach%26entry.1292438233%3D%2520%2520Tokenizers%2520are%2520crucial%2520for%2520encoding%2520information%2520in%2520Large%2520Language%2520Models%252C%2520but%250Atheir%2520development%2520has%2520recently%2520stagnated%252C%2520and%2520they%2520contain%2520inherent%2520weaknesses.%250AMajor%2520limitations%2520include%2520computational%2520overhead%252C%2520ineffective%2520vocabulary%2520use%252C%250Aand%2520unnecessarily%2520large%2520embedding%2520and%2520head%2520layers.%2520Additionally%252C%2520their%250Aperformance%2520is%2520biased%2520towards%2520a%2520reference%2520corpus%252C%2520leading%2520to%2520reduced%250Aeffectiveness%2520for%2520underrepresented%2520languages.%250A%2520%2520To%2520remedy%2520these%2520issues%252C%2520we%2520propose%2520T-FREE%252C%2520which%2520directly%2520embeds%2520words%250Athrough%2520sparse%2520activation%2520patterns%2520over%2520character%2520triplets%252C%2520and%2520does%2520not%250Arequire%2520a%2520reference%2520corpus.%2520T-FREE%2520inherently%2520exploits%2520morphological%250Asimilarities%2520and%2520allows%2520for%2520strong%2520compression%2520of%2520embedding%2520layers.%2520In%2520our%250Aexhaustive%2520experimental%2520evaluation%252C%2520we%2520achieve%2520competitive%2520downstream%250Aperformance%2520with%2520a%2520parameter%2520reduction%2520of%2520more%2520than%252085%2525%2520on%2520these%2520layers.%250AFurther%252C%2520T-FREE%2520shows%2520significant%2520improvements%2520in%2520cross-lingual%2520transfer%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-FREE%3A%20Tokenizer-Free%20Generative%20LLMs%20via%20Sparse%20Representations%20for%0A%20%20Memory-Efficient%20Embeddings&entry.906535625=Bj%C3%B6rn%20Deiseroth%20and%20Manuel%20Brack%20and%20Patrick%20Schramowski%20and%20Kristian%20Kersting%20and%20Samuel%20Weinbach&entry.1292438233=%20%20Tokenizers%20are%20crucial%20for%20encoding%20information%20in%20Large%20Language%20Models%2C%20but%0Atheir%20development%20has%20recently%20stagnated%2C%20and%20they%20contain%20inherent%20weaknesses.%0AMajor%20limitations%20include%20computational%20overhead%2C%20ineffective%20vocabulary%20use%2C%0Aand%20unnecessarily%20large%20embedding%20and%20head%20layers.%20Additionally%2C%20their%0Aperformance%20is%20biased%20towards%20a%20reference%20corpus%2C%20leading%20to%20reduced%0Aeffectiveness%20for%20underrepresented%20languages.%0A%20%20To%20remedy%20these%20issues%2C%20we%20propose%20T-FREE%2C%20which%20directly%20embeds%20words%0Athrough%20sparse%20activation%20patterns%20over%20character%20triplets%2C%20and%20does%20not%0Arequire%20a%20reference%20corpus.%20T-FREE%20inherently%20exploits%20morphological%0Asimilarities%20and%20allows%20for%20strong%20compression%20of%20embedding%20layers.%20In%20our%0Aexhaustive%20experimental%20evaluation%2C%20we%20achieve%20competitive%20downstream%0Aperformance%20with%20a%20parameter%20reduction%20of%20more%20than%2085%25%20on%20these%20layers.%0AFurther%2C%20T-FREE%20shows%20significant%20improvements%20in%20cross-lingual%20transfer%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19223v1&entry.124074799=Read"},
{"title": "SRC-Net: Bi-Temporal Spatial Relationship Concerned Network for Change\n  Detection", "author": "Hongjia Chen and Xin Xu and Fangling Pu", "abstract": "  Change detection (CD) in remote sensing imagery is a crucial task with\napplications in environmental monitoring, urban development, and disaster\nmanagement. CD involves utilizing bi-temporal images to identify changes over\ntime. The bi-temporal spatial relationships between features at the same\nlocation at different times play a key role in this process. However, existing\nchange detection networks often do not fully leverage these spatial\nrelationships during bi-temporal feature extraction and fusion. In this work,\nwe propose SRC-Net: a bi-temporal spatial relationship concerned network for\nCD. The proposed SRC-Net includes a Perception and Interaction Module that\nincorporates spatial relationships and establishes a cross-branch perception\nmechanism to enhance the precision and robustness of feature extraction.\nAdditionally, a Patch-Mode joint Feature Fusion Module is introduced to address\ninformation loss in current methods. It considers different change modes and\nconcerns about spatial relationships, resulting in more expressive fusion\nfeatures. Furthermore, we construct a novel network using these two\nrelationship concerned modules and conducted experiments on the LEVIR-CD and\nWHU Building datasets. The experimental results demonstrate that our network\noutperforms state-of-the-art (SOTA) methods while maintaining a modest\nparameter count. We believe our approach sets a new paradigm for change\ndetection and will inspire further advancements in the field. The code and\nmodels are publicly available at https://github.com/Chnja/SRCNet.\n", "link": "http://arxiv.org/abs/2406.05668v2", "date": "2024-06-27", "relevancy": 2.5555, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5202}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5068}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SRC-Net%3A%20Bi-Temporal%20Spatial%20Relationship%20Concerned%20Network%20for%20Change%0A%20%20Detection&body=Title%3A%20SRC-Net%3A%20Bi-Temporal%20Spatial%20Relationship%20Concerned%20Network%20for%20Change%0A%20%20Detection%0AAuthor%3A%20Hongjia%20Chen%20and%20Xin%20Xu%20and%20Fangling%20Pu%0AAbstract%3A%20%20%20Change%20detection%20%28CD%29%20in%20remote%20sensing%20imagery%20is%20a%20crucial%20task%20with%0Aapplications%20in%20environmental%20monitoring%2C%20urban%20development%2C%20and%20disaster%0Amanagement.%20CD%20involves%20utilizing%20bi-temporal%20images%20to%20identify%20changes%20over%0Atime.%20The%20bi-temporal%20spatial%20relationships%20between%20features%20at%20the%20same%0Alocation%20at%20different%20times%20play%20a%20key%20role%20in%20this%20process.%20However%2C%20existing%0Achange%20detection%20networks%20often%20do%20not%20fully%20leverage%20these%20spatial%0Arelationships%20during%20bi-temporal%20feature%20extraction%20and%20fusion.%20In%20this%20work%2C%0Awe%20propose%20SRC-Net%3A%20a%20bi-temporal%20spatial%20relationship%20concerned%20network%20for%0ACD.%20The%20proposed%20SRC-Net%20includes%20a%20Perception%20and%20Interaction%20Module%20that%0Aincorporates%20spatial%20relationships%20and%20establishes%20a%20cross-branch%20perception%0Amechanism%20to%20enhance%20the%20precision%20and%20robustness%20of%20feature%20extraction.%0AAdditionally%2C%20a%20Patch-Mode%20joint%20Feature%20Fusion%20Module%20is%20introduced%20to%20address%0Ainformation%20loss%20in%20current%20methods.%20It%20considers%20different%20change%20modes%20and%0Aconcerns%20about%20spatial%20relationships%2C%20resulting%20in%20more%20expressive%20fusion%0Afeatures.%20Furthermore%2C%20we%20construct%20a%20novel%20network%20using%20these%20two%0Arelationship%20concerned%20modules%20and%20conducted%20experiments%20on%20the%20LEVIR-CD%20and%0AWHU%20Building%20datasets.%20The%20experimental%20results%20demonstrate%20that%20our%20network%0Aoutperforms%20state-of-the-art%20%28SOTA%29%20methods%20while%20maintaining%20a%20modest%0Aparameter%20count.%20We%20believe%20our%20approach%20sets%20a%20new%20paradigm%20for%20change%0Adetection%20and%20will%20inspire%20further%20advancements%20in%20the%20field.%20The%20code%20and%0Amodels%20are%20publicly%20available%20at%20https%3A//github.com/Chnja/SRCNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05668v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSRC-Net%253A%2520Bi-Temporal%2520Spatial%2520Relationship%2520Concerned%2520Network%2520for%2520Change%250A%2520%2520Detection%26entry.906535625%3DHongjia%2520Chen%2520and%2520Xin%2520Xu%2520and%2520Fangling%2520Pu%26entry.1292438233%3D%2520%2520Change%2520detection%2520%2528CD%2529%2520in%2520remote%2520sensing%2520imagery%2520is%2520a%2520crucial%2520task%2520with%250Aapplications%2520in%2520environmental%2520monitoring%252C%2520urban%2520development%252C%2520and%2520disaster%250Amanagement.%2520CD%2520involves%2520utilizing%2520bi-temporal%2520images%2520to%2520identify%2520changes%2520over%250Atime.%2520The%2520bi-temporal%2520spatial%2520relationships%2520between%2520features%2520at%2520the%2520same%250Alocation%2520at%2520different%2520times%2520play%2520a%2520key%2520role%2520in%2520this%2520process.%2520However%252C%2520existing%250Achange%2520detection%2520networks%2520often%2520do%2520not%2520fully%2520leverage%2520these%2520spatial%250Arelationships%2520during%2520bi-temporal%2520feature%2520extraction%2520and%2520fusion.%2520In%2520this%2520work%252C%250Awe%2520propose%2520SRC-Net%253A%2520a%2520bi-temporal%2520spatial%2520relationship%2520concerned%2520network%2520for%250ACD.%2520The%2520proposed%2520SRC-Net%2520includes%2520a%2520Perception%2520and%2520Interaction%2520Module%2520that%250Aincorporates%2520spatial%2520relationships%2520and%2520establishes%2520a%2520cross-branch%2520perception%250Amechanism%2520to%2520enhance%2520the%2520precision%2520and%2520robustness%2520of%2520feature%2520extraction.%250AAdditionally%252C%2520a%2520Patch-Mode%2520joint%2520Feature%2520Fusion%2520Module%2520is%2520introduced%2520to%2520address%250Ainformation%2520loss%2520in%2520current%2520methods.%2520It%2520considers%2520different%2520change%2520modes%2520and%250Aconcerns%2520about%2520spatial%2520relationships%252C%2520resulting%2520in%2520more%2520expressive%2520fusion%250Afeatures.%2520Furthermore%252C%2520we%2520construct%2520a%2520novel%2520network%2520using%2520these%2520two%250Arelationship%2520concerned%2520modules%2520and%2520conducted%2520experiments%2520on%2520the%2520LEVIR-CD%2520and%250AWHU%2520Building%2520datasets.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520our%2520network%250Aoutperforms%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520while%2520maintaining%2520a%2520modest%250Aparameter%2520count.%2520We%2520believe%2520our%2520approach%2520sets%2520a%2520new%2520paradigm%2520for%2520change%250Adetection%2520and%2520will%2520inspire%2520further%2520advancements%2520in%2520the%2520field.%2520The%2520code%2520and%250Amodels%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/Chnja/SRCNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05668v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SRC-Net%3A%20Bi-Temporal%20Spatial%20Relationship%20Concerned%20Network%20for%20Change%0A%20%20Detection&entry.906535625=Hongjia%20Chen%20and%20Xin%20Xu%20and%20Fangling%20Pu&entry.1292438233=%20%20Change%20detection%20%28CD%29%20in%20remote%20sensing%20imagery%20is%20a%20crucial%20task%20with%0Aapplications%20in%20environmental%20monitoring%2C%20urban%20development%2C%20and%20disaster%0Amanagement.%20CD%20involves%20utilizing%20bi-temporal%20images%20to%20identify%20changes%20over%0Atime.%20The%20bi-temporal%20spatial%20relationships%20between%20features%20at%20the%20same%0Alocation%20at%20different%20times%20play%20a%20key%20role%20in%20this%20process.%20However%2C%20existing%0Achange%20detection%20networks%20often%20do%20not%20fully%20leverage%20these%20spatial%0Arelationships%20during%20bi-temporal%20feature%20extraction%20and%20fusion.%20In%20this%20work%2C%0Awe%20propose%20SRC-Net%3A%20a%20bi-temporal%20spatial%20relationship%20concerned%20network%20for%0ACD.%20The%20proposed%20SRC-Net%20includes%20a%20Perception%20and%20Interaction%20Module%20that%0Aincorporates%20spatial%20relationships%20and%20establishes%20a%20cross-branch%20perception%0Amechanism%20to%20enhance%20the%20precision%20and%20robustness%20of%20feature%20extraction.%0AAdditionally%2C%20a%20Patch-Mode%20joint%20Feature%20Fusion%20Module%20is%20introduced%20to%20address%0Ainformation%20loss%20in%20current%20methods.%20It%20considers%20different%20change%20modes%20and%0Aconcerns%20about%20spatial%20relationships%2C%20resulting%20in%20more%20expressive%20fusion%0Afeatures.%20Furthermore%2C%20we%20construct%20a%20novel%20network%20using%20these%20two%0Arelationship%20concerned%20modules%20and%20conducted%20experiments%20on%20the%20LEVIR-CD%20and%0AWHU%20Building%20datasets.%20The%20experimental%20results%20demonstrate%20that%20our%20network%0Aoutperforms%20state-of-the-art%20%28SOTA%29%20methods%20while%20maintaining%20a%20modest%0Aparameter%20count.%20We%20believe%20our%20approach%20sets%20a%20new%20paradigm%20for%20change%0Adetection%20and%20will%20inspire%20further%20advancements%20in%20the%20field.%20The%20code%20and%0Amodels%20are%20publicly%20available%20at%20https%3A//github.com/Chnja/SRCNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05668v2&entry.124074799=Read"},
{"title": "Dimensions underlying the representational alignment of deep neural\n  networks with humans", "author": "Florian P. Mahner and Lukas Muttenthaler and Umut G\u00fc\u00e7l\u00fc and Martin N. Hebart", "abstract": "  Determining the similarities and differences between humans and artificial\nintelligence is an important goal both in machine learning and cognitive\nneuroscience. However, similarities in representations only inform us about the\ndegree of alignment, not the factors that determine it. Drawing upon recent\ndevelopments in cognitive science, we propose a generic framework for yielding\ncomparable representations in humans and deep neural networks (DNN). Applying\nthis framework to humans and a DNN model of natural images revealed a\nlow-dimensional DNN embedding of both visual and semantic dimensions. In\ncontrast to humans, DNNs exhibited a clear dominance of visual over semantic\nfeatures, indicating divergent strategies for representing images. While\nin-silico experiments showed seemingly-consistent interpretability of DNN\ndimensions, a direct comparison between human and DNN representations revealed\nsubstantial differences in how they process images. By making representations\ndirectly comparable, our results reveal important challenges for\nrepresentational alignment, offering a means for improving their comparability.\n", "link": "http://arxiv.org/abs/2406.19087v1", "date": "2024-06-27", "relevancy": 2.5552, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5195}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5101}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dimensions%20underlying%20the%20representational%20alignment%20of%20deep%20neural%0A%20%20networks%20with%20humans&body=Title%3A%20Dimensions%20underlying%20the%20representational%20alignment%20of%20deep%20neural%0A%20%20networks%20with%20humans%0AAuthor%3A%20Florian%20P.%20Mahner%20and%20Lukas%20Muttenthaler%20and%20Umut%20G%C3%BC%C3%A7l%C3%BC%20and%20Martin%20N.%20Hebart%0AAbstract%3A%20%20%20Determining%20the%20similarities%20and%20differences%20between%20humans%20and%20artificial%0Aintelligence%20is%20an%20important%20goal%20both%20in%20machine%20learning%20and%20cognitive%0Aneuroscience.%20However%2C%20similarities%20in%20representations%20only%20inform%20us%20about%20the%0Adegree%20of%20alignment%2C%20not%20the%20factors%20that%20determine%20it.%20Drawing%20upon%20recent%0Adevelopments%20in%20cognitive%20science%2C%20we%20propose%20a%20generic%20framework%20for%20yielding%0Acomparable%20representations%20in%20humans%20and%20deep%20neural%20networks%20%28DNN%29.%20Applying%0Athis%20framework%20to%20humans%20and%20a%20DNN%20model%20of%20natural%20images%20revealed%20a%0Alow-dimensional%20DNN%20embedding%20of%20both%20visual%20and%20semantic%20dimensions.%20In%0Acontrast%20to%20humans%2C%20DNNs%20exhibited%20a%20clear%20dominance%20of%20visual%20over%20semantic%0Afeatures%2C%20indicating%20divergent%20strategies%20for%20representing%20images.%20While%0Ain-silico%20experiments%20showed%20seemingly-consistent%20interpretability%20of%20DNN%0Adimensions%2C%20a%20direct%20comparison%20between%20human%20and%20DNN%20representations%20revealed%0Asubstantial%20differences%20in%20how%20they%20process%20images.%20By%20making%20representations%0Adirectly%20comparable%2C%20our%20results%20reveal%20important%20challenges%20for%0Arepresentational%20alignment%2C%20offering%20a%20means%20for%20improving%20their%20comparability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19087v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDimensions%2520underlying%2520the%2520representational%2520alignment%2520of%2520deep%2520neural%250A%2520%2520networks%2520with%2520humans%26entry.906535625%3DFlorian%2520P.%2520Mahner%2520and%2520Lukas%2520Muttenthaler%2520and%2520Umut%2520G%25C3%25BC%25C3%25A7l%25C3%25BC%2520and%2520Martin%2520N.%2520Hebart%26entry.1292438233%3D%2520%2520Determining%2520the%2520similarities%2520and%2520differences%2520between%2520humans%2520and%2520artificial%250Aintelligence%2520is%2520an%2520important%2520goal%2520both%2520in%2520machine%2520learning%2520and%2520cognitive%250Aneuroscience.%2520However%252C%2520similarities%2520in%2520representations%2520only%2520inform%2520us%2520about%2520the%250Adegree%2520of%2520alignment%252C%2520not%2520the%2520factors%2520that%2520determine%2520it.%2520Drawing%2520upon%2520recent%250Adevelopments%2520in%2520cognitive%2520science%252C%2520we%2520propose%2520a%2520generic%2520framework%2520for%2520yielding%250Acomparable%2520representations%2520in%2520humans%2520and%2520deep%2520neural%2520networks%2520%2528DNN%2529.%2520Applying%250Athis%2520framework%2520to%2520humans%2520and%2520a%2520DNN%2520model%2520of%2520natural%2520images%2520revealed%2520a%250Alow-dimensional%2520DNN%2520embedding%2520of%2520both%2520visual%2520and%2520semantic%2520dimensions.%2520In%250Acontrast%2520to%2520humans%252C%2520DNNs%2520exhibited%2520a%2520clear%2520dominance%2520of%2520visual%2520over%2520semantic%250Afeatures%252C%2520indicating%2520divergent%2520strategies%2520for%2520representing%2520images.%2520While%250Ain-silico%2520experiments%2520showed%2520seemingly-consistent%2520interpretability%2520of%2520DNN%250Adimensions%252C%2520a%2520direct%2520comparison%2520between%2520human%2520and%2520DNN%2520representations%2520revealed%250Asubstantial%2520differences%2520in%2520how%2520they%2520process%2520images.%2520By%2520making%2520representations%250Adirectly%2520comparable%252C%2520our%2520results%2520reveal%2520important%2520challenges%2520for%250Arepresentational%2520alignment%252C%2520offering%2520a%2520means%2520for%2520improving%2520their%2520comparability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19087v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dimensions%20underlying%20the%20representational%20alignment%20of%20deep%20neural%0A%20%20networks%20with%20humans&entry.906535625=Florian%20P.%20Mahner%20and%20Lukas%20Muttenthaler%20and%20Umut%20G%C3%BC%C3%A7l%C3%BC%20and%20Martin%20N.%20Hebart&entry.1292438233=%20%20Determining%20the%20similarities%20and%20differences%20between%20humans%20and%20artificial%0Aintelligence%20is%20an%20important%20goal%20both%20in%20machine%20learning%20and%20cognitive%0Aneuroscience.%20However%2C%20similarities%20in%20representations%20only%20inform%20us%20about%20the%0Adegree%20of%20alignment%2C%20not%20the%20factors%20that%20determine%20it.%20Drawing%20upon%20recent%0Adevelopments%20in%20cognitive%20science%2C%20we%20propose%20a%20generic%20framework%20for%20yielding%0Acomparable%20representations%20in%20humans%20and%20deep%20neural%20networks%20%28DNN%29.%20Applying%0Athis%20framework%20to%20humans%20and%20a%20DNN%20model%20of%20natural%20images%20revealed%20a%0Alow-dimensional%20DNN%20embedding%20of%20both%20visual%20and%20semantic%20dimensions.%20In%0Acontrast%20to%20humans%2C%20DNNs%20exhibited%20a%20clear%20dominance%20of%20visual%20over%20semantic%0Afeatures%2C%20indicating%20divergent%20strategies%20for%20representing%20images.%20While%0Ain-silico%20experiments%20showed%20seemingly-consistent%20interpretability%20of%20DNN%0Adimensions%2C%20a%20direct%20comparison%20between%20human%20and%20DNN%20representations%20revealed%0Asubstantial%20differences%20in%20how%20they%20process%20images.%20By%20making%20representations%0Adirectly%20comparable%2C%20our%20results%20reveal%20important%20challenges%20for%0Arepresentational%20alignment%2C%20offering%20a%20means%20for%20improving%20their%20comparability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19087v1&entry.124074799=Read"},
{"title": "S4: Self-Supervised Sensing Across the Spectrum", "author": "Jayanth Shenoy and Xingjian Davis Zhang and Shlok Mehrotra and Bill Tao and Rem Yang and Han Zhao and Deepak Vasisht", "abstract": "  Satellite image time series (SITS) segmentation is crucial for many\napplications like environmental monitoring, land cover mapping and agricultural\ncrop type classification. However, training models for SITS segmentation\nremains a challenging task due to the lack of abundant training data, which\nrequires fine grained annotation. We propose S4 a new self-supervised\npre-training approach that significantly reduces the requirement for labeled\ntraining data by utilizing two new insights: (a) Satellites capture images in\ndifferent parts of the spectrum such as radio frequencies, and visible\nfrequencies. (b) Satellite imagery is geo-registered allowing for fine-grained\nspatial alignment. We use these insights to formulate pre-training tasks in S4.\nWe also curate m2s2-SITS, a large-scale dataset of unlabeled,\nspatially-aligned, multi-modal and geographic specific SITS that serves as\nrepresentative pre-training data for S4. Finally, we evaluate S4 on multiple\nSITS segmentation datasets and demonstrate its efficacy against competing\nbaselines while using limited labeled data.\n", "link": "http://arxiv.org/abs/2405.01656v2", "date": "2024-06-27", "relevancy": 2.5475, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5502}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5017}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S4%3A%20Self-Supervised%20Sensing%20Across%20the%20Spectrum&body=Title%3A%20S4%3A%20Self-Supervised%20Sensing%20Across%20the%20Spectrum%0AAuthor%3A%20Jayanth%20Shenoy%20and%20Xingjian%20Davis%20Zhang%20and%20Shlok%20Mehrotra%20and%20Bill%20Tao%20and%20Rem%20Yang%20and%20Han%20Zhao%20and%20Deepak%20Vasisht%0AAbstract%3A%20%20%20Satellite%20image%20time%20series%20%28SITS%29%20segmentation%20is%20crucial%20for%20many%0Aapplications%20like%20environmental%20monitoring%2C%20land%20cover%20mapping%20and%20agricultural%0Acrop%20type%20classification.%20However%2C%20training%20models%20for%20SITS%20segmentation%0Aremains%20a%20challenging%20task%20due%20to%20the%20lack%20of%20abundant%20training%20data%2C%20which%0Arequires%20fine%20grained%20annotation.%20We%20propose%20S4%20a%20new%20self-supervised%0Apre-training%20approach%20that%20significantly%20reduces%20the%20requirement%20for%20labeled%0Atraining%20data%20by%20utilizing%20two%20new%20insights%3A%20%28a%29%20Satellites%20capture%20images%20in%0Adifferent%20parts%20of%20the%20spectrum%20such%20as%20radio%20frequencies%2C%20and%20visible%0Afrequencies.%20%28b%29%20Satellite%20imagery%20is%20geo-registered%20allowing%20for%20fine-grained%0Aspatial%20alignment.%20We%20use%20these%20insights%20to%20formulate%20pre-training%20tasks%20in%20S4.%0AWe%20also%20curate%20m2s2-SITS%2C%20a%20large-scale%20dataset%20of%20unlabeled%2C%0Aspatially-aligned%2C%20multi-modal%20and%20geographic%20specific%20SITS%20that%20serves%20as%0Arepresentative%20pre-training%20data%20for%20S4.%20Finally%2C%20we%20evaluate%20S4%20on%20multiple%0ASITS%20segmentation%20datasets%20and%20demonstrate%20its%20efficacy%20against%20competing%0Abaselines%20while%20using%20limited%20labeled%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01656v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS4%253A%2520Self-Supervised%2520Sensing%2520Across%2520the%2520Spectrum%26entry.906535625%3DJayanth%2520Shenoy%2520and%2520Xingjian%2520Davis%2520Zhang%2520and%2520Shlok%2520Mehrotra%2520and%2520Bill%2520Tao%2520and%2520Rem%2520Yang%2520and%2520Han%2520Zhao%2520and%2520Deepak%2520Vasisht%26entry.1292438233%3D%2520%2520Satellite%2520image%2520time%2520series%2520%2528SITS%2529%2520segmentation%2520is%2520crucial%2520for%2520many%250Aapplications%2520like%2520environmental%2520monitoring%252C%2520land%2520cover%2520mapping%2520and%2520agricultural%250Acrop%2520type%2520classification.%2520However%252C%2520training%2520models%2520for%2520SITS%2520segmentation%250Aremains%2520a%2520challenging%2520task%2520due%2520to%2520the%2520lack%2520of%2520abundant%2520training%2520data%252C%2520which%250Arequires%2520fine%2520grained%2520annotation.%2520We%2520propose%2520S4%2520a%2520new%2520self-supervised%250Apre-training%2520approach%2520that%2520significantly%2520reduces%2520the%2520requirement%2520for%2520labeled%250Atraining%2520data%2520by%2520utilizing%2520two%2520new%2520insights%253A%2520%2528a%2529%2520Satellites%2520capture%2520images%2520in%250Adifferent%2520parts%2520of%2520the%2520spectrum%2520such%2520as%2520radio%2520frequencies%252C%2520and%2520visible%250Afrequencies.%2520%2528b%2529%2520Satellite%2520imagery%2520is%2520geo-registered%2520allowing%2520for%2520fine-grained%250Aspatial%2520alignment.%2520We%2520use%2520these%2520insights%2520to%2520formulate%2520pre-training%2520tasks%2520in%2520S4.%250AWe%2520also%2520curate%2520m2s2-SITS%252C%2520a%2520large-scale%2520dataset%2520of%2520unlabeled%252C%250Aspatially-aligned%252C%2520multi-modal%2520and%2520geographic%2520specific%2520SITS%2520that%2520serves%2520as%250Arepresentative%2520pre-training%2520data%2520for%2520S4.%2520Finally%252C%2520we%2520evaluate%2520S4%2520on%2520multiple%250ASITS%2520segmentation%2520datasets%2520and%2520demonstrate%2520its%2520efficacy%2520against%2520competing%250Abaselines%2520while%2520using%2520limited%2520labeled%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01656v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S4%3A%20Self-Supervised%20Sensing%20Across%20the%20Spectrum&entry.906535625=Jayanth%20Shenoy%20and%20Xingjian%20Davis%20Zhang%20and%20Shlok%20Mehrotra%20and%20Bill%20Tao%20and%20Rem%20Yang%20and%20Han%20Zhao%20and%20Deepak%20Vasisht&entry.1292438233=%20%20Satellite%20image%20time%20series%20%28SITS%29%20segmentation%20is%20crucial%20for%20many%0Aapplications%20like%20environmental%20monitoring%2C%20land%20cover%20mapping%20and%20agricultural%0Acrop%20type%20classification.%20However%2C%20training%20models%20for%20SITS%20segmentation%0Aremains%20a%20challenging%20task%20due%20to%20the%20lack%20of%20abundant%20training%20data%2C%20which%0Arequires%20fine%20grained%20annotation.%20We%20propose%20S4%20a%20new%20self-supervised%0Apre-training%20approach%20that%20significantly%20reduces%20the%20requirement%20for%20labeled%0Atraining%20data%20by%20utilizing%20two%20new%20insights%3A%20%28a%29%20Satellites%20capture%20images%20in%0Adifferent%20parts%20of%20the%20spectrum%20such%20as%20radio%20frequencies%2C%20and%20visible%0Afrequencies.%20%28b%29%20Satellite%20imagery%20is%20geo-registered%20allowing%20for%20fine-grained%0Aspatial%20alignment.%20We%20use%20these%20insights%20to%20formulate%20pre-training%20tasks%20in%20S4.%0AWe%20also%20curate%20m2s2-SITS%2C%20a%20large-scale%20dataset%20of%20unlabeled%2C%0Aspatially-aligned%2C%20multi-modal%20and%20geographic%20specific%20SITS%20that%20serves%20as%0Arepresentative%20pre-training%20data%20for%20S4.%20Finally%2C%20we%20evaluate%20S4%20on%20multiple%0ASITS%20segmentation%20datasets%20and%20demonstrate%20its%20efficacy%20against%20competing%0Abaselines%20while%20using%20limited%20labeled%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01656v2&entry.124074799=Read"},
{"title": "Decentralized Stochastic Subgradient Methods for Nonsmooth Nonconvex\n  Optimization", "author": "Siyuan Zhang and Nachuan Xiao and Xin Liu", "abstract": "  In this paper, we concentrate on decentralized optimization problems with\nnonconvex and nonsmooth objective functions, especially on the decentralized\ntraining of nonsmooth neural networks. We introduce a unified framework to\nanalyze the global convergence of decentralized stochastic subgradient-based\nmethods. We prove the global convergence of our proposed framework under mild\nconditions, by establishing that the generated sequence asymptotically\napproximates the trajectories of its associated differential inclusion.\nFurthermore, we establish that our proposed framework covers a wide range of\nexisting efficient decentralized subgradient-based methods, including\ndecentralized stochastic subgradient descent (DSGD), DSGD with\ngradient-tracking technique (DSGD-T), and DSGD with momentum (DSGD-M). In\naddition, we introduce the sign map to regularize the update directions in\nDSGD-M, and show it is enclosed in our proposed framework. Consequently, our\nconvergence results establish, for the first time, global convergence of these\nmethods when applied to nonsmooth nonconvex objectives. Preliminary numerical\nexperiments demonstrate that our proposed framework yields highly efficient\ndecentralized subgradient-based methods with convergence guarantees in the\ntraining of nonsmooth neural networks.\n", "link": "http://arxiv.org/abs/2403.11565v2", "date": "2024-06-27", "relevancy": 2.5273, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5156}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5026}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decentralized%20Stochastic%20Subgradient%20Methods%20for%20Nonsmooth%20Nonconvex%0A%20%20Optimization&body=Title%3A%20Decentralized%20Stochastic%20Subgradient%20Methods%20for%20Nonsmooth%20Nonconvex%0A%20%20Optimization%0AAuthor%3A%20Siyuan%20Zhang%20and%20Nachuan%20Xiao%20and%20Xin%20Liu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20concentrate%20on%20decentralized%20optimization%20problems%20with%0Anonconvex%20and%20nonsmooth%20objective%20functions%2C%20especially%20on%20the%20decentralized%0Atraining%20of%20nonsmooth%20neural%20networks.%20We%20introduce%20a%20unified%20framework%20to%0Aanalyze%20the%20global%20convergence%20of%20decentralized%20stochastic%20subgradient-based%0Amethods.%20We%20prove%20the%20global%20convergence%20of%20our%20proposed%20framework%20under%20mild%0Aconditions%2C%20by%20establishing%20that%20the%20generated%20sequence%20asymptotically%0Aapproximates%20the%20trajectories%20of%20its%20associated%20differential%20inclusion.%0AFurthermore%2C%20we%20establish%20that%20our%20proposed%20framework%20covers%20a%20wide%20range%20of%0Aexisting%20efficient%20decentralized%20subgradient-based%20methods%2C%20including%0Adecentralized%20stochastic%20subgradient%20descent%20%28DSGD%29%2C%20DSGD%20with%0Agradient-tracking%20technique%20%28DSGD-T%29%2C%20and%20DSGD%20with%20momentum%20%28DSGD-M%29.%20In%0Aaddition%2C%20we%20introduce%20the%20sign%20map%20to%20regularize%20the%20update%20directions%20in%0ADSGD-M%2C%20and%20show%20it%20is%20enclosed%20in%20our%20proposed%20framework.%20Consequently%2C%20our%0Aconvergence%20results%20establish%2C%20for%20the%20first%20time%2C%20global%20convergence%20of%20these%0Amethods%20when%20applied%20to%20nonsmooth%20nonconvex%20objectives.%20Preliminary%20numerical%0Aexperiments%20demonstrate%20that%20our%20proposed%20framework%20yields%20highly%20efficient%0Adecentralized%20subgradient-based%20methods%20with%20convergence%20guarantees%20in%20the%0Atraining%20of%20nonsmooth%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11565v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecentralized%2520Stochastic%2520Subgradient%2520Methods%2520for%2520Nonsmooth%2520Nonconvex%250A%2520%2520Optimization%26entry.906535625%3DSiyuan%2520Zhang%2520and%2520Nachuan%2520Xiao%2520and%2520Xin%2520Liu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520concentrate%2520on%2520decentralized%2520optimization%2520problems%2520with%250Anonconvex%2520and%2520nonsmooth%2520objective%2520functions%252C%2520especially%2520on%2520the%2520decentralized%250Atraining%2520of%2520nonsmooth%2520neural%2520networks.%2520We%2520introduce%2520a%2520unified%2520framework%2520to%250Aanalyze%2520the%2520global%2520convergence%2520of%2520decentralized%2520stochastic%2520subgradient-based%250Amethods.%2520We%2520prove%2520the%2520global%2520convergence%2520of%2520our%2520proposed%2520framework%2520under%2520mild%250Aconditions%252C%2520by%2520establishing%2520that%2520the%2520generated%2520sequence%2520asymptotically%250Aapproximates%2520the%2520trajectories%2520of%2520its%2520associated%2520differential%2520inclusion.%250AFurthermore%252C%2520we%2520establish%2520that%2520our%2520proposed%2520framework%2520covers%2520a%2520wide%2520range%2520of%250Aexisting%2520efficient%2520decentralized%2520subgradient-based%2520methods%252C%2520including%250Adecentralized%2520stochastic%2520subgradient%2520descent%2520%2528DSGD%2529%252C%2520DSGD%2520with%250Agradient-tracking%2520technique%2520%2528DSGD-T%2529%252C%2520and%2520DSGD%2520with%2520momentum%2520%2528DSGD-M%2529.%2520In%250Aaddition%252C%2520we%2520introduce%2520the%2520sign%2520map%2520to%2520regularize%2520the%2520update%2520directions%2520in%250ADSGD-M%252C%2520and%2520show%2520it%2520is%2520enclosed%2520in%2520our%2520proposed%2520framework.%2520Consequently%252C%2520our%250Aconvergence%2520results%2520establish%252C%2520for%2520the%2520first%2520time%252C%2520global%2520convergence%2520of%2520these%250Amethods%2520when%2520applied%2520to%2520nonsmooth%2520nonconvex%2520objectives.%2520Preliminary%2520numerical%250Aexperiments%2520demonstrate%2520that%2520our%2520proposed%2520framework%2520yields%2520highly%2520efficient%250Adecentralized%2520subgradient-based%2520methods%2520with%2520convergence%2520guarantees%2520in%2520the%250Atraining%2520of%2520nonsmooth%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11565v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decentralized%20Stochastic%20Subgradient%20Methods%20for%20Nonsmooth%20Nonconvex%0A%20%20Optimization&entry.906535625=Siyuan%20Zhang%20and%20Nachuan%20Xiao%20and%20Xin%20Liu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20concentrate%20on%20decentralized%20optimization%20problems%20with%0Anonconvex%20and%20nonsmooth%20objective%20functions%2C%20especially%20on%20the%20decentralized%0Atraining%20of%20nonsmooth%20neural%20networks.%20We%20introduce%20a%20unified%20framework%20to%0Aanalyze%20the%20global%20convergence%20of%20decentralized%20stochastic%20subgradient-based%0Amethods.%20We%20prove%20the%20global%20convergence%20of%20our%20proposed%20framework%20under%20mild%0Aconditions%2C%20by%20establishing%20that%20the%20generated%20sequence%20asymptotically%0Aapproximates%20the%20trajectories%20of%20its%20associated%20differential%20inclusion.%0AFurthermore%2C%20we%20establish%20that%20our%20proposed%20framework%20covers%20a%20wide%20range%20of%0Aexisting%20efficient%20decentralized%20subgradient-based%20methods%2C%20including%0Adecentralized%20stochastic%20subgradient%20descent%20%28DSGD%29%2C%20DSGD%20with%0Agradient-tracking%20technique%20%28DSGD-T%29%2C%20and%20DSGD%20with%20momentum%20%28DSGD-M%29.%20In%0Aaddition%2C%20we%20introduce%20the%20sign%20map%20to%20regularize%20the%20update%20directions%20in%0ADSGD-M%2C%20and%20show%20it%20is%20enclosed%20in%20our%20proposed%20framework.%20Consequently%2C%20our%0Aconvergence%20results%20establish%2C%20for%20the%20first%20time%2C%20global%20convergence%20of%20these%0Amethods%20when%20applied%20to%20nonsmooth%20nonconvex%20objectives.%20Preliminary%20numerical%0Aexperiments%20demonstrate%20that%20our%20proposed%20framework%20yields%20highly%20efficient%0Adecentralized%20subgradient-based%20methods%20with%20convergence%20guarantees%20in%20the%0Atraining%20of%20nonsmooth%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11565v2&entry.124074799=Read"},
{"title": "STAL3D: Unsupervised Domain Adaptation for 3D Object Detection via\n  Collaborating Self-Training and Adversarial Learning", "author": "Yanan Zhang and Chao Zhou and Di Huang", "abstract": "  Existing 3D object detection suffers from expensive annotation costs and poor\ntransferability to unknown data due to the domain gap, Unsupervised Domain\nAdaptation (UDA) aims to generalize detection models trained in labeled source\ndomains to perform robustly on unexplored target domains, providing a promising\nsolution for cross-domain 3D object detection. Although Self-Training (ST)\nbased cross-domain 3D detection methods with the assistance of pseudo-labeling\ntechniques have achieved remarkable progress, they still face the issue of\nlow-quality pseudo-labels when there are significant domain disparities due to\nthe absence of a process for feature distribution alignment. While Adversarial\nLearning (AL) based methods can effectively align the feature distributions of\nthe source and target domains, the inability to obtain labels in the target\ndomain forces the adoption of asymmetric optimization losses, resulting in a\nchallenging issue of source domain bias. To overcome these limitations, we\npropose a novel unsupervised domain adaptation framework for 3D object\ndetection via collaborating ST and AL, dubbed as STAL3D, unleashing the\ncomplementary advantages of pseudo labels and feature distribution alignment.\nAdditionally, a Background Suppression Adversarial Learning (BS-AL) module and\na Scale Filtering Module (SFM) are designed tailored for 3D cross-domain\nscenes, effectively alleviating the issues of the large proportion of\nbackground interference and source domain size bias. Our STAL3D achieves\nstate-of-the-art performance on multiple cross-domain tasks and even surpasses\nthe Oracle results on Waymo $\\rightarrow$ KITTI and Waymo $\\rightarrow$\nKITTI-rain.\n", "link": "http://arxiv.org/abs/2406.19362v1", "date": "2024-06-27", "relevancy": 2.5265, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6496}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6284}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STAL3D%3A%20Unsupervised%20Domain%20Adaptation%20for%203D%20Object%20Detection%20via%0A%20%20Collaborating%20Self-Training%20and%20Adversarial%20Learning&body=Title%3A%20STAL3D%3A%20Unsupervised%20Domain%20Adaptation%20for%203D%20Object%20Detection%20via%0A%20%20Collaborating%20Self-Training%20and%20Adversarial%20Learning%0AAuthor%3A%20Yanan%20Zhang%20and%20Chao%20Zhou%20and%20Di%20Huang%0AAbstract%3A%20%20%20Existing%203D%20object%20detection%20suffers%20from%20expensive%20annotation%20costs%20and%20poor%0Atransferability%20to%20unknown%20data%20due%20to%20the%20domain%20gap%2C%20Unsupervised%20Domain%0AAdaptation%20%28UDA%29%20aims%20to%20generalize%20detection%20models%20trained%20in%20labeled%20source%0Adomains%20to%20perform%20robustly%20on%20unexplored%20target%20domains%2C%20providing%20a%20promising%0Asolution%20for%20cross-domain%203D%20object%20detection.%20Although%20Self-Training%20%28ST%29%0Abased%20cross-domain%203D%20detection%20methods%20with%20the%20assistance%20of%20pseudo-labeling%0Atechniques%20have%20achieved%20remarkable%20progress%2C%20they%20still%20face%20the%20issue%20of%0Alow-quality%20pseudo-labels%20when%20there%20are%20significant%20domain%20disparities%20due%20to%0Athe%20absence%20of%20a%20process%20for%20feature%20distribution%20alignment.%20While%20Adversarial%0ALearning%20%28AL%29%20based%20methods%20can%20effectively%20align%20the%20feature%20distributions%20of%0Athe%20source%20and%20target%20domains%2C%20the%20inability%20to%20obtain%20labels%20in%20the%20target%0Adomain%20forces%20the%20adoption%20of%20asymmetric%20optimization%20losses%2C%20resulting%20in%20a%0Achallenging%20issue%20of%20source%20domain%20bias.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20a%20novel%20unsupervised%20domain%20adaptation%20framework%20for%203D%20object%0Adetection%20via%20collaborating%20ST%20and%20AL%2C%20dubbed%20as%20STAL3D%2C%20unleashing%20the%0Acomplementary%20advantages%20of%20pseudo%20labels%20and%20feature%20distribution%20alignment.%0AAdditionally%2C%20a%20Background%20Suppression%20Adversarial%20Learning%20%28BS-AL%29%20module%20and%0Aa%20Scale%20Filtering%20Module%20%28SFM%29%20are%20designed%20tailored%20for%203D%20cross-domain%0Ascenes%2C%20effectively%20alleviating%20the%20issues%20of%20the%20large%20proportion%20of%0Abackground%20interference%20and%20source%20domain%20size%20bias.%20Our%20STAL3D%20achieves%0Astate-of-the-art%20performance%20on%20multiple%20cross-domain%20tasks%20and%20even%20surpasses%0Athe%20Oracle%20results%20on%20Waymo%20%24%5Crightarrow%24%20KITTI%20and%20Waymo%20%24%5Crightarrow%24%0AKITTI-rain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19362v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTAL3D%253A%2520Unsupervised%2520Domain%2520Adaptation%2520for%25203D%2520Object%2520Detection%2520via%250A%2520%2520Collaborating%2520Self-Training%2520and%2520Adversarial%2520Learning%26entry.906535625%3DYanan%2520Zhang%2520and%2520Chao%2520Zhou%2520and%2520Di%2520Huang%26entry.1292438233%3D%2520%2520Existing%25203D%2520object%2520detection%2520suffers%2520from%2520expensive%2520annotation%2520costs%2520and%2520poor%250Atransferability%2520to%2520unknown%2520data%2520due%2520to%2520the%2520domain%2520gap%252C%2520Unsupervised%2520Domain%250AAdaptation%2520%2528UDA%2529%2520aims%2520to%2520generalize%2520detection%2520models%2520trained%2520in%2520labeled%2520source%250Adomains%2520to%2520perform%2520robustly%2520on%2520unexplored%2520target%2520domains%252C%2520providing%2520a%2520promising%250Asolution%2520for%2520cross-domain%25203D%2520object%2520detection.%2520Although%2520Self-Training%2520%2528ST%2529%250Abased%2520cross-domain%25203D%2520detection%2520methods%2520with%2520the%2520assistance%2520of%2520pseudo-labeling%250Atechniques%2520have%2520achieved%2520remarkable%2520progress%252C%2520they%2520still%2520face%2520the%2520issue%2520of%250Alow-quality%2520pseudo-labels%2520when%2520there%2520are%2520significant%2520domain%2520disparities%2520due%2520to%250Athe%2520absence%2520of%2520a%2520process%2520for%2520feature%2520distribution%2520alignment.%2520While%2520Adversarial%250ALearning%2520%2528AL%2529%2520based%2520methods%2520can%2520effectively%2520align%2520the%2520feature%2520distributions%2520of%250Athe%2520source%2520and%2520target%2520domains%252C%2520the%2520inability%2520to%2520obtain%2520labels%2520in%2520the%2520target%250Adomain%2520forces%2520the%2520adoption%2520of%2520asymmetric%2520optimization%2520losses%252C%2520resulting%2520in%2520a%250Achallenging%2520issue%2520of%2520source%2520domain%2520bias.%2520To%2520overcome%2520these%2520limitations%252C%2520we%250Apropose%2520a%2520novel%2520unsupervised%2520domain%2520adaptation%2520framework%2520for%25203D%2520object%250Adetection%2520via%2520collaborating%2520ST%2520and%2520AL%252C%2520dubbed%2520as%2520STAL3D%252C%2520unleashing%2520the%250Acomplementary%2520advantages%2520of%2520pseudo%2520labels%2520and%2520feature%2520distribution%2520alignment.%250AAdditionally%252C%2520a%2520Background%2520Suppression%2520Adversarial%2520Learning%2520%2528BS-AL%2529%2520module%2520and%250Aa%2520Scale%2520Filtering%2520Module%2520%2528SFM%2529%2520are%2520designed%2520tailored%2520for%25203D%2520cross-domain%250Ascenes%252C%2520effectively%2520alleviating%2520the%2520issues%2520of%2520the%2520large%2520proportion%2520of%250Abackground%2520interference%2520and%2520source%2520domain%2520size%2520bias.%2520Our%2520STAL3D%2520achieves%250Astate-of-the-art%2520performance%2520on%2520multiple%2520cross-domain%2520tasks%2520and%2520even%2520surpasses%250Athe%2520Oracle%2520results%2520on%2520Waymo%2520%2524%255Crightarrow%2524%2520KITTI%2520and%2520Waymo%2520%2524%255Crightarrow%2524%250AKITTI-rain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19362v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STAL3D%3A%20Unsupervised%20Domain%20Adaptation%20for%203D%20Object%20Detection%20via%0A%20%20Collaborating%20Self-Training%20and%20Adversarial%20Learning&entry.906535625=Yanan%20Zhang%20and%20Chao%20Zhou%20and%20Di%20Huang&entry.1292438233=%20%20Existing%203D%20object%20detection%20suffers%20from%20expensive%20annotation%20costs%20and%20poor%0Atransferability%20to%20unknown%20data%20due%20to%20the%20domain%20gap%2C%20Unsupervised%20Domain%0AAdaptation%20%28UDA%29%20aims%20to%20generalize%20detection%20models%20trained%20in%20labeled%20source%0Adomains%20to%20perform%20robustly%20on%20unexplored%20target%20domains%2C%20providing%20a%20promising%0Asolution%20for%20cross-domain%203D%20object%20detection.%20Although%20Self-Training%20%28ST%29%0Abased%20cross-domain%203D%20detection%20methods%20with%20the%20assistance%20of%20pseudo-labeling%0Atechniques%20have%20achieved%20remarkable%20progress%2C%20they%20still%20face%20the%20issue%20of%0Alow-quality%20pseudo-labels%20when%20there%20are%20significant%20domain%20disparities%20due%20to%0Athe%20absence%20of%20a%20process%20for%20feature%20distribution%20alignment.%20While%20Adversarial%0ALearning%20%28AL%29%20based%20methods%20can%20effectively%20align%20the%20feature%20distributions%20of%0Athe%20source%20and%20target%20domains%2C%20the%20inability%20to%20obtain%20labels%20in%20the%20target%0Adomain%20forces%20the%20adoption%20of%20asymmetric%20optimization%20losses%2C%20resulting%20in%20a%0Achallenging%20issue%20of%20source%20domain%20bias.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20a%20novel%20unsupervised%20domain%20adaptation%20framework%20for%203D%20object%0Adetection%20via%20collaborating%20ST%20and%20AL%2C%20dubbed%20as%20STAL3D%2C%20unleashing%20the%0Acomplementary%20advantages%20of%20pseudo%20labels%20and%20feature%20distribution%20alignment.%0AAdditionally%2C%20a%20Background%20Suppression%20Adversarial%20Learning%20%28BS-AL%29%20module%20and%0Aa%20Scale%20Filtering%20Module%20%28SFM%29%20are%20designed%20tailored%20for%203D%20cross-domain%0Ascenes%2C%20effectively%20alleviating%20the%20issues%20of%20the%20large%20proportion%20of%0Abackground%20interference%20and%20source%20domain%20size%20bias.%20Our%20STAL3D%20achieves%0Astate-of-the-art%20performance%20on%20multiple%20cross-domain%20tasks%20and%20even%20surpasses%0Athe%20Oracle%20results%20on%20Waymo%20%24%5Crightarrow%24%20KITTI%20and%20Waymo%20%24%5Crightarrow%24%0AKITTI-rain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19362v1&entry.124074799=Read"},
{"title": "GSplit: Scaling Graph Neural Network Training on Large Graphs via\n  Split-Parallelism", "author": "Sandeep Polisetty and Juelin Liu and Kobi Falus and Yi Ren Fung and Seung-Hwan Lim and Hui Guan and Marco Serafini", "abstract": "  Graph neural networks (GNNs), an emerging class of machine learning models\nfor graphs, have gained popularity for their superior performance in various\ngraph analytical tasks. Mini-batch training is commonly used to train GNNs on\nlarge graphs, and data parallelism is the standard approach to scale mini-batch\ntraining across multiple GPUs. One of the major performance costs in GNN\ntraining is the loading of input features, which prevents GPUs from being fully\nutilized. In this paper, we argue that this problem is exacerbated by\nredundancies that are inherent to the data parallel approach. To address this\nissue, we introduce a hybrid parallel mini-batch training paradigm called split\nparallelism. Split parallelism avoids redundant data loads and splits the\nsampling and training of each mini-batch across multiple GPUs online, at each\niteration, using a lightweight splitting algorithm. We implement split\nparallelism in GSplit and show that it outperforms state-of-the-art mini-batch\ntraining systems like DGL, Quiver, and $P^3$.\n", "link": "http://arxiv.org/abs/2303.13775v2", "date": "2024-06-27", "relevancy": 2.4855, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5258}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4889}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSplit%3A%20Scaling%20Graph%20Neural%20Network%20Training%20on%20Large%20Graphs%20via%0A%20%20Split-Parallelism&body=Title%3A%20GSplit%3A%20Scaling%20Graph%20Neural%20Network%20Training%20on%20Large%20Graphs%20via%0A%20%20Split-Parallelism%0AAuthor%3A%20Sandeep%20Polisetty%20and%20Juelin%20Liu%20and%20Kobi%20Falus%20and%20Yi%20Ren%20Fung%20and%20Seung-Hwan%20Lim%20and%20Hui%20Guan%20and%20Marco%20Serafini%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%2C%20an%20emerging%20class%20of%20machine%20learning%20models%0Afor%20graphs%2C%20have%20gained%20popularity%20for%20their%20superior%20performance%20in%20various%0Agraph%20analytical%20tasks.%20Mini-batch%20training%20is%20commonly%20used%20to%20train%20GNNs%20on%0Alarge%20graphs%2C%20and%20data%20parallelism%20is%20the%20standard%20approach%20to%20scale%20mini-batch%0Atraining%20across%20multiple%20GPUs.%20One%20of%20the%20major%20performance%20costs%20in%20GNN%0Atraining%20is%20the%20loading%20of%20input%20features%2C%20which%20prevents%20GPUs%20from%20being%20fully%0Autilized.%20In%20this%20paper%2C%20we%20argue%20that%20this%20problem%20is%20exacerbated%20by%0Aredundancies%20that%20are%20inherent%20to%20the%20data%20parallel%20approach.%20To%20address%20this%0Aissue%2C%20we%20introduce%20a%20hybrid%20parallel%20mini-batch%20training%20paradigm%20called%20split%0Aparallelism.%20Split%20parallelism%20avoids%20redundant%20data%20loads%20and%20splits%20the%0Asampling%20and%20training%20of%20each%20mini-batch%20across%20multiple%20GPUs%20online%2C%20at%20each%0Aiteration%2C%20using%20a%20lightweight%20splitting%20algorithm.%20We%20implement%20split%0Aparallelism%20in%20GSplit%20and%20show%20that%20it%20outperforms%20state-of-the-art%20mini-batch%0Atraining%20systems%20like%20DGL%2C%20Quiver%2C%20and%20%24P%5E3%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.13775v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSplit%253A%2520Scaling%2520Graph%2520Neural%2520Network%2520Training%2520on%2520Large%2520Graphs%2520via%250A%2520%2520Split-Parallelism%26entry.906535625%3DSandeep%2520Polisetty%2520and%2520Juelin%2520Liu%2520and%2520Kobi%2520Falus%2520and%2520Yi%2520Ren%2520Fung%2520and%2520Seung-Hwan%2520Lim%2520and%2520Hui%2520Guan%2520and%2520Marco%2520Serafini%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%252C%2520an%2520emerging%2520class%2520of%2520machine%2520learning%2520models%250Afor%2520graphs%252C%2520have%2520gained%2520popularity%2520for%2520their%2520superior%2520performance%2520in%2520various%250Agraph%2520analytical%2520tasks.%2520Mini-batch%2520training%2520is%2520commonly%2520used%2520to%2520train%2520GNNs%2520on%250Alarge%2520graphs%252C%2520and%2520data%2520parallelism%2520is%2520the%2520standard%2520approach%2520to%2520scale%2520mini-batch%250Atraining%2520across%2520multiple%2520GPUs.%2520One%2520of%2520the%2520major%2520performance%2520costs%2520in%2520GNN%250Atraining%2520is%2520the%2520loading%2520of%2520input%2520features%252C%2520which%2520prevents%2520GPUs%2520from%2520being%2520fully%250Autilized.%2520In%2520this%2520paper%252C%2520we%2520argue%2520that%2520this%2520problem%2520is%2520exacerbated%2520by%250Aredundancies%2520that%2520are%2520inherent%2520to%2520the%2520data%2520parallel%2520approach.%2520To%2520address%2520this%250Aissue%252C%2520we%2520introduce%2520a%2520hybrid%2520parallel%2520mini-batch%2520training%2520paradigm%2520called%2520split%250Aparallelism.%2520Split%2520parallelism%2520avoids%2520redundant%2520data%2520loads%2520and%2520splits%2520the%250Asampling%2520and%2520training%2520of%2520each%2520mini-batch%2520across%2520multiple%2520GPUs%2520online%252C%2520at%2520each%250Aiteration%252C%2520using%2520a%2520lightweight%2520splitting%2520algorithm.%2520We%2520implement%2520split%250Aparallelism%2520in%2520GSplit%2520and%2520show%2520that%2520it%2520outperforms%2520state-of-the-art%2520mini-batch%250Atraining%2520systems%2520like%2520DGL%252C%2520Quiver%252C%2520and%2520%2524P%255E3%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.13775v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSplit%3A%20Scaling%20Graph%20Neural%20Network%20Training%20on%20Large%20Graphs%20via%0A%20%20Split-Parallelism&entry.906535625=Sandeep%20Polisetty%20and%20Juelin%20Liu%20and%20Kobi%20Falus%20and%20Yi%20Ren%20Fung%20and%20Seung-Hwan%20Lim%20and%20Hui%20Guan%20and%20Marco%20Serafini&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%2C%20an%20emerging%20class%20of%20machine%20learning%20models%0Afor%20graphs%2C%20have%20gained%20popularity%20for%20their%20superior%20performance%20in%20various%0Agraph%20analytical%20tasks.%20Mini-batch%20training%20is%20commonly%20used%20to%20train%20GNNs%20on%0Alarge%20graphs%2C%20and%20data%20parallelism%20is%20the%20standard%20approach%20to%20scale%20mini-batch%0Atraining%20across%20multiple%20GPUs.%20One%20of%20the%20major%20performance%20costs%20in%20GNN%0Atraining%20is%20the%20loading%20of%20input%20features%2C%20which%20prevents%20GPUs%20from%20being%20fully%0Autilized.%20In%20this%20paper%2C%20we%20argue%20that%20this%20problem%20is%20exacerbated%20by%0Aredundancies%20that%20are%20inherent%20to%20the%20data%20parallel%20approach.%20To%20address%20this%0Aissue%2C%20we%20introduce%20a%20hybrid%20parallel%20mini-batch%20training%20paradigm%20called%20split%0Aparallelism.%20Split%20parallelism%20avoids%20redundant%20data%20loads%20and%20splits%20the%0Asampling%20and%20training%20of%20each%20mini-batch%20across%20multiple%20GPUs%20online%2C%20at%20each%0Aiteration%2C%20using%20a%20lightweight%20splitting%20algorithm.%20We%20implement%20split%0Aparallelism%20in%20GSplit%20and%20show%20that%20it%20outperforms%20state-of-the-art%20mini-batch%0Atraining%20systems%20like%20DGL%2C%20Quiver%2C%20and%20%24P%5E3%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.13775v2&entry.124074799=Read"},
{"title": "Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping", "author": "Haoyu Wang and Guozheng Ma and Ziqiao Meng and Zeyu Qin and Li Shen and Zhong Zhang and Bingzhe Wu and Liu Liu and Yatao Bian and Tingyang Xu and Xueqian Wang and Peilin Zhao", "abstract": "  Self-alignment is an effective way to reduce the cost of human annotation\nwhile ensuring promising model capability. However, most current methods\ncomplete the data collection and training steps in a single round, which may\noverlook the continuously improving ability of self-aligned models. This gives\nrise to a key query: What if we do multi-time bootstrapping self-alignment?\nDoes this strategy enhance model performance or lead to rapid degradation? In\nthis paper, our pioneering exploration delves into the impact of bootstrapping\nself-alignment on large language models. Our findings reveal that bootstrapping\nself-alignment markedly surpasses the single-round approach, by guaranteeing\ndata diversity from in-context learning. To further exploit the capabilities of\nbootstrapping, we investigate and adjust the training order of data, which\nyields improved performance of the model. Drawing on these findings, we propose\nStep-On-Feet Tuning (SOFT) which leverages model's continuously enhanced\nfew-shot ability to boost zero or one-shot performance. Based on easy-to-hard\ntraining recipe, we propose SOFT+ which further boost self-alignment's\nperformance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across\nvarious classification and generation tasks, highlighting the potential of\nbootstrapping self-alignment on continually enhancing model alignment\nperformance.\n", "link": "http://arxiv.org/abs/2402.07610v3", "date": "2024-06-27", "relevancy": 2.4793, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5092}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4963}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Step-On-Feet%20Tuning%3A%20Scaling%20Self-Alignment%20of%20LLMs%20via%20Bootstrapping&body=Title%3A%20Step-On-Feet%20Tuning%3A%20Scaling%20Self-Alignment%20of%20LLMs%20via%20Bootstrapping%0AAuthor%3A%20Haoyu%20Wang%20and%20Guozheng%20Ma%20and%20Ziqiao%20Meng%20and%20Zeyu%20Qin%20and%20Li%20Shen%20and%20Zhong%20Zhang%20and%20Bingzhe%20Wu%20and%20Liu%20Liu%20and%20Yatao%20Bian%20and%20Tingyang%20Xu%20and%20Xueqian%20Wang%20and%20Peilin%20Zhao%0AAbstract%3A%20%20%20Self-alignment%20is%20an%20effective%20way%20to%20reduce%20the%20cost%20of%20human%20annotation%0Awhile%20ensuring%20promising%20model%20capability.%20However%2C%20most%20current%20methods%0Acomplete%20the%20data%20collection%20and%20training%20steps%20in%20a%20single%20round%2C%20which%20may%0Aoverlook%20the%20continuously%20improving%20ability%20of%20self-aligned%20models.%20This%20gives%0Arise%20to%20a%20key%20query%3A%20What%20if%20we%20do%20multi-time%20bootstrapping%20self-alignment%3F%0ADoes%20this%20strategy%20enhance%20model%20performance%20or%20lead%20to%20rapid%20degradation%3F%20In%0Athis%20paper%2C%20our%20pioneering%20exploration%20delves%20into%20the%20impact%20of%20bootstrapping%0Aself-alignment%20on%20large%20language%20models.%20Our%20findings%20reveal%20that%20bootstrapping%0Aself-alignment%20markedly%20surpasses%20the%20single-round%20approach%2C%20by%20guaranteeing%0Adata%20diversity%20from%20in-context%20learning.%20To%20further%20exploit%20the%20capabilities%20of%0Abootstrapping%2C%20we%20investigate%20and%20adjust%20the%20training%20order%20of%20data%2C%20which%0Ayields%20improved%20performance%20of%20the%20model.%20Drawing%20on%20these%20findings%2C%20we%20propose%0AStep-On-Feet%20Tuning%20%28SOFT%29%20which%20leverages%20model%27s%20continuously%20enhanced%0Afew-shot%20ability%20to%20boost%20zero%20or%20one-shot%20performance.%20Based%20on%20easy-to-hard%0Atraining%20recipe%2C%20we%20propose%20SOFT%2B%20which%20further%20boost%20self-alignment%27s%0Aperformance.%20Our%20experiments%20demonstrate%20the%20efficiency%20of%20SOFT%20%28SOFT%2B%29%20across%0Avarious%20classification%20and%20generation%20tasks%2C%20highlighting%20the%20potential%20of%0Abootstrapping%20self-alignment%20on%20continually%20enhancing%20model%20alignment%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07610v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStep-On-Feet%2520Tuning%253A%2520Scaling%2520Self-Alignment%2520of%2520LLMs%2520via%2520Bootstrapping%26entry.906535625%3DHaoyu%2520Wang%2520and%2520Guozheng%2520Ma%2520and%2520Ziqiao%2520Meng%2520and%2520Zeyu%2520Qin%2520and%2520Li%2520Shen%2520and%2520Zhong%2520Zhang%2520and%2520Bingzhe%2520Wu%2520and%2520Liu%2520Liu%2520and%2520Yatao%2520Bian%2520and%2520Tingyang%2520Xu%2520and%2520Xueqian%2520Wang%2520and%2520Peilin%2520Zhao%26entry.1292438233%3D%2520%2520Self-alignment%2520is%2520an%2520effective%2520way%2520to%2520reduce%2520the%2520cost%2520of%2520human%2520annotation%250Awhile%2520ensuring%2520promising%2520model%2520capability.%2520However%252C%2520most%2520current%2520methods%250Acomplete%2520the%2520data%2520collection%2520and%2520training%2520steps%2520in%2520a%2520single%2520round%252C%2520which%2520may%250Aoverlook%2520the%2520continuously%2520improving%2520ability%2520of%2520self-aligned%2520models.%2520This%2520gives%250Arise%2520to%2520a%2520key%2520query%253A%2520What%2520if%2520we%2520do%2520multi-time%2520bootstrapping%2520self-alignment%253F%250ADoes%2520this%2520strategy%2520enhance%2520model%2520performance%2520or%2520lead%2520to%2520rapid%2520degradation%253F%2520In%250Athis%2520paper%252C%2520our%2520pioneering%2520exploration%2520delves%2520into%2520the%2520impact%2520of%2520bootstrapping%250Aself-alignment%2520on%2520large%2520language%2520models.%2520Our%2520findings%2520reveal%2520that%2520bootstrapping%250Aself-alignment%2520markedly%2520surpasses%2520the%2520single-round%2520approach%252C%2520by%2520guaranteeing%250Adata%2520diversity%2520from%2520in-context%2520learning.%2520To%2520further%2520exploit%2520the%2520capabilities%2520of%250Abootstrapping%252C%2520we%2520investigate%2520and%2520adjust%2520the%2520training%2520order%2520of%2520data%252C%2520which%250Ayields%2520improved%2520performance%2520of%2520the%2520model.%2520Drawing%2520on%2520these%2520findings%252C%2520we%2520propose%250AStep-On-Feet%2520Tuning%2520%2528SOFT%2529%2520which%2520leverages%2520model%2527s%2520continuously%2520enhanced%250Afew-shot%2520ability%2520to%2520boost%2520zero%2520or%2520one-shot%2520performance.%2520Based%2520on%2520easy-to-hard%250Atraining%2520recipe%252C%2520we%2520propose%2520SOFT%252B%2520which%2520further%2520boost%2520self-alignment%2527s%250Aperformance.%2520Our%2520experiments%2520demonstrate%2520the%2520efficiency%2520of%2520SOFT%2520%2528SOFT%252B%2529%2520across%250Avarious%2520classification%2520and%2520generation%2520tasks%252C%2520highlighting%2520the%2520potential%2520of%250Abootstrapping%2520self-alignment%2520on%2520continually%2520enhancing%2520model%2520alignment%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07610v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Step-On-Feet%20Tuning%3A%20Scaling%20Self-Alignment%20of%20LLMs%20via%20Bootstrapping&entry.906535625=Haoyu%20Wang%20and%20Guozheng%20Ma%20and%20Ziqiao%20Meng%20and%20Zeyu%20Qin%20and%20Li%20Shen%20and%20Zhong%20Zhang%20and%20Bingzhe%20Wu%20and%20Liu%20Liu%20and%20Yatao%20Bian%20and%20Tingyang%20Xu%20and%20Xueqian%20Wang%20and%20Peilin%20Zhao&entry.1292438233=%20%20Self-alignment%20is%20an%20effective%20way%20to%20reduce%20the%20cost%20of%20human%20annotation%0Awhile%20ensuring%20promising%20model%20capability.%20However%2C%20most%20current%20methods%0Acomplete%20the%20data%20collection%20and%20training%20steps%20in%20a%20single%20round%2C%20which%20may%0Aoverlook%20the%20continuously%20improving%20ability%20of%20self-aligned%20models.%20This%20gives%0Arise%20to%20a%20key%20query%3A%20What%20if%20we%20do%20multi-time%20bootstrapping%20self-alignment%3F%0ADoes%20this%20strategy%20enhance%20model%20performance%20or%20lead%20to%20rapid%20degradation%3F%20In%0Athis%20paper%2C%20our%20pioneering%20exploration%20delves%20into%20the%20impact%20of%20bootstrapping%0Aself-alignment%20on%20large%20language%20models.%20Our%20findings%20reveal%20that%20bootstrapping%0Aself-alignment%20markedly%20surpasses%20the%20single-round%20approach%2C%20by%20guaranteeing%0Adata%20diversity%20from%20in-context%20learning.%20To%20further%20exploit%20the%20capabilities%20of%0Abootstrapping%2C%20we%20investigate%20and%20adjust%20the%20training%20order%20of%20data%2C%20which%0Ayields%20improved%20performance%20of%20the%20model.%20Drawing%20on%20these%20findings%2C%20we%20propose%0AStep-On-Feet%20Tuning%20%28SOFT%29%20which%20leverages%20model%27s%20continuously%20enhanced%0Afew-shot%20ability%20to%20boost%20zero%20or%20one-shot%20performance.%20Based%20on%20easy-to-hard%0Atraining%20recipe%2C%20we%20propose%20SOFT%2B%20which%20further%20boost%20self-alignment%27s%0Aperformance.%20Our%20experiments%20demonstrate%20the%20efficiency%20of%20SOFT%20%28SOFT%2B%29%20across%0Avarious%20classification%20and%20generation%20tasks%2C%20highlighting%20the%20potential%20of%0Abootstrapping%20self-alignment%20on%20continually%20enhancing%20model%20alignment%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07610v3&entry.124074799=Read"},
{"title": "WsiCaption: Multiple Instance Generation of Pathology Reports for\n  Gigapixel Whole-Slide Images", "author": "Pingyi Chen and Honglin Li and Chenglu Zhu and Sunyi Zheng and Zhongyi Shui and Lin Yang", "abstract": "  Whole slide images are the foundation of digital pathology for the diagnosis\nand treatment of carcinomas. Writing pathology reports is laborious and\nerror-prone for inexperienced pathologists. To reduce the workload and improve\nclinical automation, we investigate how to generate pathology reports given\nwhole slide images. On the data end, we curated the largest WSI-text dataset\n(PathText). In specific, we collected nearly 10000 high-quality WSI-text pairs\nfor visual-language models by recognizing and cleaning pathology reports which\nnarrate diagnostic slides in TCGA. On the model end, we propose the multiple\ninstance generative model (MI-Gen) which can produce pathology reports for\ngigapixel WSIs. We benchmark our model on the largest subset of TCGA-PathoText.\nExperimental results show our model can generate pathology reports which\ncontain multiple clinical clues and achieve competitive performance on certain\nslide-level tasks. We observe that simple semantic extraction from the\npathology reports can achieve the best performance (0.838 of F1 score) on BRCA\nsubtyping surpassing previous state-of-the-art approaches. Our collected\ndataset and related code are available.\n", "link": "http://arxiv.org/abs/2311.16480v4", "date": "2024-06-27", "relevancy": 2.4062, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4968}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4743}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WsiCaption%3A%20Multiple%20Instance%20Generation%20of%20Pathology%20Reports%20for%0A%20%20Gigapixel%20Whole-Slide%20Images&body=Title%3A%20WsiCaption%3A%20Multiple%20Instance%20Generation%20of%20Pathology%20Reports%20for%0A%20%20Gigapixel%20Whole-Slide%20Images%0AAuthor%3A%20Pingyi%20Chen%20and%20Honglin%20Li%20and%20Chenglu%20Zhu%20and%20Sunyi%20Zheng%20and%20Zhongyi%20Shui%20and%20Lin%20Yang%0AAbstract%3A%20%20%20Whole%20slide%20images%20are%20the%20foundation%20of%20digital%20pathology%20for%20the%20diagnosis%0Aand%20treatment%20of%20carcinomas.%20Writing%20pathology%20reports%20is%20laborious%20and%0Aerror-prone%20for%20inexperienced%20pathologists.%20To%20reduce%20the%20workload%20and%20improve%0Aclinical%20automation%2C%20we%20investigate%20how%20to%20generate%20pathology%20reports%20given%0Awhole%20slide%20images.%20On%20the%20data%20end%2C%20we%20curated%20the%20largest%20WSI-text%20dataset%0A%28PathText%29.%20In%20specific%2C%20we%20collected%20nearly%2010000%20high-quality%20WSI-text%20pairs%0Afor%20visual-language%20models%20by%20recognizing%20and%20cleaning%20pathology%20reports%20which%0Anarrate%20diagnostic%20slides%20in%20TCGA.%20On%20the%20model%20end%2C%20we%20propose%20the%20multiple%0Ainstance%20generative%20model%20%28MI-Gen%29%20which%20can%20produce%20pathology%20reports%20for%0Agigapixel%20WSIs.%20We%20benchmark%20our%20model%20on%20the%20largest%20subset%20of%20TCGA-PathoText.%0AExperimental%20results%20show%20our%20model%20can%20generate%20pathology%20reports%20which%0Acontain%20multiple%20clinical%20clues%20and%20achieve%20competitive%20performance%20on%20certain%0Aslide-level%20tasks.%20We%20observe%20that%20simple%20semantic%20extraction%20from%20the%0Apathology%20reports%20can%20achieve%20the%20best%20performance%20%280.838%20of%20F1%20score%29%20on%20BRCA%0Asubtyping%20surpassing%20previous%20state-of-the-art%20approaches.%20Our%20collected%0Adataset%20and%20related%20code%20are%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16480v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWsiCaption%253A%2520Multiple%2520Instance%2520Generation%2520of%2520Pathology%2520Reports%2520for%250A%2520%2520Gigapixel%2520Whole-Slide%2520Images%26entry.906535625%3DPingyi%2520Chen%2520and%2520Honglin%2520Li%2520and%2520Chenglu%2520Zhu%2520and%2520Sunyi%2520Zheng%2520and%2520Zhongyi%2520Shui%2520and%2520Lin%2520Yang%26entry.1292438233%3D%2520%2520Whole%2520slide%2520images%2520are%2520the%2520foundation%2520of%2520digital%2520pathology%2520for%2520the%2520diagnosis%250Aand%2520treatment%2520of%2520carcinomas.%2520Writing%2520pathology%2520reports%2520is%2520laborious%2520and%250Aerror-prone%2520for%2520inexperienced%2520pathologists.%2520To%2520reduce%2520the%2520workload%2520and%2520improve%250Aclinical%2520automation%252C%2520we%2520investigate%2520how%2520to%2520generate%2520pathology%2520reports%2520given%250Awhole%2520slide%2520images.%2520On%2520the%2520data%2520end%252C%2520we%2520curated%2520the%2520largest%2520WSI-text%2520dataset%250A%2528PathText%2529.%2520In%2520specific%252C%2520we%2520collected%2520nearly%252010000%2520high-quality%2520WSI-text%2520pairs%250Afor%2520visual-language%2520models%2520by%2520recognizing%2520and%2520cleaning%2520pathology%2520reports%2520which%250Anarrate%2520diagnostic%2520slides%2520in%2520TCGA.%2520On%2520the%2520model%2520end%252C%2520we%2520propose%2520the%2520multiple%250Ainstance%2520generative%2520model%2520%2528MI-Gen%2529%2520which%2520can%2520produce%2520pathology%2520reports%2520for%250Agigapixel%2520WSIs.%2520We%2520benchmark%2520our%2520model%2520on%2520the%2520largest%2520subset%2520of%2520TCGA-PathoText.%250AExperimental%2520results%2520show%2520our%2520model%2520can%2520generate%2520pathology%2520reports%2520which%250Acontain%2520multiple%2520clinical%2520clues%2520and%2520achieve%2520competitive%2520performance%2520on%2520certain%250Aslide-level%2520tasks.%2520We%2520observe%2520that%2520simple%2520semantic%2520extraction%2520from%2520the%250Apathology%2520reports%2520can%2520achieve%2520the%2520best%2520performance%2520%25280.838%2520of%2520F1%2520score%2529%2520on%2520BRCA%250Asubtyping%2520surpassing%2520previous%2520state-of-the-art%2520approaches.%2520Our%2520collected%250Adataset%2520and%2520related%2520code%2520are%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.16480v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WsiCaption%3A%20Multiple%20Instance%20Generation%20of%20Pathology%20Reports%20for%0A%20%20Gigapixel%20Whole-Slide%20Images&entry.906535625=Pingyi%20Chen%20and%20Honglin%20Li%20and%20Chenglu%20Zhu%20and%20Sunyi%20Zheng%20and%20Zhongyi%20Shui%20and%20Lin%20Yang&entry.1292438233=%20%20Whole%20slide%20images%20are%20the%20foundation%20of%20digital%20pathology%20for%20the%20diagnosis%0Aand%20treatment%20of%20carcinomas.%20Writing%20pathology%20reports%20is%20laborious%20and%0Aerror-prone%20for%20inexperienced%20pathologists.%20To%20reduce%20the%20workload%20and%20improve%0Aclinical%20automation%2C%20we%20investigate%20how%20to%20generate%20pathology%20reports%20given%0Awhole%20slide%20images.%20On%20the%20data%20end%2C%20we%20curated%20the%20largest%20WSI-text%20dataset%0A%28PathText%29.%20In%20specific%2C%20we%20collected%20nearly%2010000%20high-quality%20WSI-text%20pairs%0Afor%20visual-language%20models%20by%20recognizing%20and%20cleaning%20pathology%20reports%20which%0Anarrate%20diagnostic%20slides%20in%20TCGA.%20On%20the%20model%20end%2C%20we%20propose%20the%20multiple%0Ainstance%20generative%20model%20%28MI-Gen%29%20which%20can%20produce%20pathology%20reports%20for%0Agigapixel%20WSIs.%20We%20benchmark%20our%20model%20on%20the%20largest%20subset%20of%20TCGA-PathoText.%0AExperimental%20results%20show%20our%20model%20can%20generate%20pathology%20reports%20which%0Acontain%20multiple%20clinical%20clues%20and%20achieve%20competitive%20performance%20on%20certain%0Aslide-level%20tasks.%20We%20observe%20that%20simple%20semantic%20extraction%20from%20the%0Apathology%20reports%20can%20achieve%20the%20best%20performance%20%280.838%20of%20F1%20score%29%20on%20BRCA%0Asubtyping%20surpassing%20previous%20state-of-the-art%20approaches.%20Our%20collected%0Adataset%20and%20related%20code%20are%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16480v4&entry.124074799=Read"},
{"title": "Heterophily-Aware Graph Attention Network", "author": "Junfu Wang and Yuanfang Guo and Liang Yang and Yunhong Wang", "abstract": "  Graph Neural Networks (GNNs) have shown remarkable success in graph\nrepresentation learning. Unfortunately, current weight assignment schemes in\nstandard GNNs, such as the calculation based on node degrees or pair-wise\nrepresentations, can hardly be effective in processing the networks with\nheterophily, in which the connected nodes usually possess different labels or\nfeatures. Existing heterophilic GNNs tend to ignore the modeling of heterophily\nof each edge, which is also a vital part in tackling the heterophily problem.\nIn this paper, we firstly propose a heterophily-aware attention scheme and\nreveal the benefits of modeling the edge heterophily, i.e., if a GNN assigns\ndifferent weights to edges according to different heterophilic types, it can\nlearn effective local attention patterns, which enable nodes to acquire\nappropriate information from distinct neighbors. Then, we propose a novel\nHeterophily-Aware Graph Attention Network (HA-GAT) by fully exploring and\nutilizing the local distribution as the underlying heterophily, to handle the\nnetworks with different homophily ratios. To demonstrate the effectiveness of\nthe proposed HA-GAT, we analyze the proposed heterophily-aware attention scheme\nand local distribution exploration, by seeking for an interpretation from their\nmechanism. Extensive results demonstrate that our HA-GAT achieves\nstate-of-the-art performances on eight datasets with different homophily ratios\nin both the supervised and semi-supervised node classification tasks.\n", "link": "http://arxiv.org/abs/2302.03228v2", "date": "2024-06-27", "relevancy": 2.3855, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.492}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4777}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterophily-Aware%20Graph%20Attention%20Network&body=Title%3A%20Heterophily-Aware%20Graph%20Attention%20Network%0AAuthor%3A%20Junfu%20Wang%20and%20Yuanfang%20Guo%20and%20Liang%20Yang%20and%20Yunhong%20Wang%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20remarkable%20success%20in%20graph%0Arepresentation%20learning.%20Unfortunately%2C%20current%20weight%20assignment%20schemes%20in%0Astandard%20GNNs%2C%20such%20as%20the%20calculation%20based%20on%20node%20degrees%20or%20pair-wise%0Arepresentations%2C%20can%20hardly%20be%20effective%20in%20processing%20the%20networks%20with%0Aheterophily%2C%20in%20which%20the%20connected%20nodes%20usually%20possess%20different%20labels%20or%0Afeatures.%20Existing%20heterophilic%20GNNs%20tend%20to%20ignore%20the%20modeling%20of%20heterophily%0Aof%20each%20edge%2C%20which%20is%20also%20a%20vital%20part%20in%20tackling%20the%20heterophily%20problem.%0AIn%20this%20paper%2C%20we%20firstly%20propose%20a%20heterophily-aware%20attention%20scheme%20and%0Areveal%20the%20benefits%20of%20modeling%20the%20edge%20heterophily%2C%20i.e.%2C%20if%20a%20GNN%20assigns%0Adifferent%20weights%20to%20edges%20according%20to%20different%20heterophilic%20types%2C%20it%20can%0Alearn%20effective%20local%20attention%20patterns%2C%20which%20enable%20nodes%20to%20acquire%0Aappropriate%20information%20from%20distinct%20neighbors.%20Then%2C%20we%20propose%20a%20novel%0AHeterophily-Aware%20Graph%20Attention%20Network%20%28HA-GAT%29%20by%20fully%20exploring%20and%0Autilizing%20the%20local%20distribution%20as%20the%20underlying%20heterophily%2C%20to%20handle%20the%0Anetworks%20with%20different%20homophily%20ratios.%20To%20demonstrate%20the%20effectiveness%20of%0Athe%20proposed%20HA-GAT%2C%20we%20analyze%20the%20proposed%20heterophily-aware%20attention%20scheme%0Aand%20local%20distribution%20exploration%2C%20by%20seeking%20for%20an%20interpretation%20from%20their%0Amechanism.%20Extensive%20results%20demonstrate%20that%20our%20HA-GAT%20achieves%0Astate-of-the-art%20performances%20on%20eight%20datasets%20with%20different%20homophily%20ratios%0Ain%20both%20the%20supervised%20and%20semi-supervised%20node%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.03228v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterophily-Aware%2520Graph%2520Attention%2520Network%26entry.906535625%3DJunfu%2520Wang%2520and%2520Yuanfang%2520Guo%2520and%2520Liang%2520Yang%2520and%2520Yunhong%2520Wang%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520shown%2520remarkable%2520success%2520in%2520graph%250Arepresentation%2520learning.%2520Unfortunately%252C%2520current%2520weight%2520assignment%2520schemes%2520in%250Astandard%2520GNNs%252C%2520such%2520as%2520the%2520calculation%2520based%2520on%2520node%2520degrees%2520or%2520pair-wise%250Arepresentations%252C%2520can%2520hardly%2520be%2520effective%2520in%2520processing%2520the%2520networks%2520with%250Aheterophily%252C%2520in%2520which%2520the%2520connected%2520nodes%2520usually%2520possess%2520different%2520labels%2520or%250Afeatures.%2520Existing%2520heterophilic%2520GNNs%2520tend%2520to%2520ignore%2520the%2520modeling%2520of%2520heterophily%250Aof%2520each%2520edge%252C%2520which%2520is%2520also%2520a%2520vital%2520part%2520in%2520tackling%2520the%2520heterophily%2520problem.%250AIn%2520this%2520paper%252C%2520we%2520firstly%2520propose%2520a%2520heterophily-aware%2520attention%2520scheme%2520and%250Areveal%2520the%2520benefits%2520of%2520modeling%2520the%2520edge%2520heterophily%252C%2520i.e.%252C%2520if%2520a%2520GNN%2520assigns%250Adifferent%2520weights%2520to%2520edges%2520according%2520to%2520different%2520heterophilic%2520types%252C%2520it%2520can%250Alearn%2520effective%2520local%2520attention%2520patterns%252C%2520which%2520enable%2520nodes%2520to%2520acquire%250Aappropriate%2520information%2520from%2520distinct%2520neighbors.%2520Then%252C%2520we%2520propose%2520a%2520novel%250AHeterophily-Aware%2520Graph%2520Attention%2520Network%2520%2528HA-GAT%2529%2520by%2520fully%2520exploring%2520and%250Autilizing%2520the%2520local%2520distribution%2520as%2520the%2520underlying%2520heterophily%252C%2520to%2520handle%2520the%250Anetworks%2520with%2520different%2520homophily%2520ratios.%2520To%2520demonstrate%2520the%2520effectiveness%2520of%250Athe%2520proposed%2520HA-GAT%252C%2520we%2520analyze%2520the%2520proposed%2520heterophily-aware%2520attention%2520scheme%250Aand%2520local%2520distribution%2520exploration%252C%2520by%2520seeking%2520for%2520an%2520interpretation%2520from%2520their%250Amechanism.%2520Extensive%2520results%2520demonstrate%2520that%2520our%2520HA-GAT%2520achieves%250Astate-of-the-art%2520performances%2520on%2520eight%2520datasets%2520with%2520different%2520homophily%2520ratios%250Ain%2520both%2520the%2520supervised%2520and%2520semi-supervised%2520node%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.03228v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterophily-Aware%20Graph%20Attention%20Network&entry.906535625=Junfu%20Wang%20and%20Yuanfang%20Guo%20and%20Liang%20Yang%20and%20Yunhong%20Wang&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20remarkable%20success%20in%20graph%0Arepresentation%20learning.%20Unfortunately%2C%20current%20weight%20assignment%20schemes%20in%0Astandard%20GNNs%2C%20such%20as%20the%20calculation%20based%20on%20node%20degrees%20or%20pair-wise%0Arepresentations%2C%20can%20hardly%20be%20effective%20in%20processing%20the%20networks%20with%0Aheterophily%2C%20in%20which%20the%20connected%20nodes%20usually%20possess%20different%20labels%20or%0Afeatures.%20Existing%20heterophilic%20GNNs%20tend%20to%20ignore%20the%20modeling%20of%20heterophily%0Aof%20each%20edge%2C%20which%20is%20also%20a%20vital%20part%20in%20tackling%20the%20heterophily%20problem.%0AIn%20this%20paper%2C%20we%20firstly%20propose%20a%20heterophily-aware%20attention%20scheme%20and%0Areveal%20the%20benefits%20of%20modeling%20the%20edge%20heterophily%2C%20i.e.%2C%20if%20a%20GNN%20assigns%0Adifferent%20weights%20to%20edges%20according%20to%20different%20heterophilic%20types%2C%20it%20can%0Alearn%20effective%20local%20attention%20patterns%2C%20which%20enable%20nodes%20to%20acquire%0Aappropriate%20information%20from%20distinct%20neighbors.%20Then%2C%20we%20propose%20a%20novel%0AHeterophily-Aware%20Graph%20Attention%20Network%20%28HA-GAT%29%20by%20fully%20exploring%20and%0Autilizing%20the%20local%20distribution%20as%20the%20underlying%20heterophily%2C%20to%20handle%20the%0Anetworks%20with%20different%20homophily%20ratios.%20To%20demonstrate%20the%20effectiveness%20of%0Athe%20proposed%20HA-GAT%2C%20we%20analyze%20the%20proposed%20heterophily-aware%20attention%20scheme%0Aand%20local%20distribution%20exploration%2C%20by%20seeking%20for%20an%20interpretation%20from%20their%0Amechanism.%20Extensive%20results%20demonstrate%20that%20our%20HA-GAT%20achieves%0Astate-of-the-art%20performances%20on%20eight%20datasets%20with%20different%20homophily%20ratios%0Ain%20both%20the%20supervised%20and%20semi-supervised%20node%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.03228v2&entry.124074799=Read"},
{"title": "Efficient World Models with Context-Aware Tokenization", "author": "Vincent Micheli and Eloi Alonso and Fran\u00e7ois Fleuret", "abstract": "  Scaling up deep Reinforcement Learning (RL) methods presents a significant\nchallenge. Following developments in generative modelling, model-based RL\npositions itself as a strong contender. Recent advances in sequence modelling\nhave led to effective transformer-based world models, albeit at the price of\nheavy computations due to the long sequences of tokens required to accurately\nsimulate environments. In this work, we propose $\\Delta$-IRIS, a new agent with\na world model architecture composed of a discrete autoencoder that encodes\nstochastic deltas between time steps and an autoregressive transformer that\npredicts future deltas by summarizing the current state of the world with\ncontinuous tokens. In the Crafter benchmark, $\\Delta$-IRIS sets a new state of\nthe art at multiple frame budgets, while being an order of magnitude faster to\ntrain than previous attention-based approaches. We release our code and models\nat https://github.com/vmicheli/delta-iris.\n", "link": "http://arxiv.org/abs/2406.19320v1", "date": "2024-06-27", "relevancy": 2.3457, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5946}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5895}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20World%20Models%20with%20Context-Aware%20Tokenization&body=Title%3A%20Efficient%20World%20Models%20with%20Context-Aware%20Tokenization%0AAuthor%3A%20Vincent%20Micheli%20and%20Eloi%20Alonso%20and%20Fran%C3%A7ois%20Fleuret%0AAbstract%3A%20%20%20Scaling%20up%20deep%20Reinforcement%20Learning%20%28RL%29%20methods%20presents%20a%20significant%0Achallenge.%20Following%20developments%20in%20generative%20modelling%2C%20model-based%20RL%0Apositions%20itself%20as%20a%20strong%20contender.%20Recent%20advances%20in%20sequence%20modelling%0Ahave%20led%20to%20effective%20transformer-based%20world%20models%2C%20albeit%20at%20the%20price%20of%0Aheavy%20computations%20due%20to%20the%20long%20sequences%20of%20tokens%20required%20to%20accurately%0Asimulate%20environments.%20In%20this%20work%2C%20we%20propose%20%24%5CDelta%24-IRIS%2C%20a%20new%20agent%20with%0Aa%20world%20model%20architecture%20composed%20of%20a%20discrete%20autoencoder%20that%20encodes%0Astochastic%20deltas%20between%20time%20steps%20and%20an%20autoregressive%20transformer%20that%0Apredicts%20future%20deltas%20by%20summarizing%20the%20current%20state%20of%20the%20world%20with%0Acontinuous%20tokens.%20In%20the%20Crafter%20benchmark%2C%20%24%5CDelta%24-IRIS%20sets%20a%20new%20state%20of%0Athe%20art%20at%20multiple%20frame%20budgets%2C%20while%20being%20an%20order%20of%20magnitude%20faster%20to%0Atrain%20than%20previous%20attention-based%20approaches.%20We%20release%20our%20code%20and%20models%0Aat%20https%3A//github.com/vmicheli/delta-iris.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520World%2520Models%2520with%2520Context-Aware%2520Tokenization%26entry.906535625%3DVincent%2520Micheli%2520and%2520Eloi%2520Alonso%2520and%2520Fran%25C3%25A7ois%2520Fleuret%26entry.1292438233%3D%2520%2520Scaling%2520up%2520deep%2520Reinforcement%2520Learning%2520%2528RL%2529%2520methods%2520presents%2520a%2520significant%250Achallenge.%2520Following%2520developments%2520in%2520generative%2520modelling%252C%2520model-based%2520RL%250Apositions%2520itself%2520as%2520a%2520strong%2520contender.%2520Recent%2520advances%2520in%2520sequence%2520modelling%250Ahave%2520led%2520to%2520effective%2520transformer-based%2520world%2520models%252C%2520albeit%2520at%2520the%2520price%2520of%250Aheavy%2520computations%2520due%2520to%2520the%2520long%2520sequences%2520of%2520tokens%2520required%2520to%2520accurately%250Asimulate%2520environments.%2520In%2520this%2520work%252C%2520we%2520propose%2520%2524%255CDelta%2524-IRIS%252C%2520a%2520new%2520agent%2520with%250Aa%2520world%2520model%2520architecture%2520composed%2520of%2520a%2520discrete%2520autoencoder%2520that%2520encodes%250Astochastic%2520deltas%2520between%2520time%2520steps%2520and%2520an%2520autoregressive%2520transformer%2520that%250Apredicts%2520future%2520deltas%2520by%2520summarizing%2520the%2520current%2520state%2520of%2520the%2520world%2520with%250Acontinuous%2520tokens.%2520In%2520the%2520Crafter%2520benchmark%252C%2520%2524%255CDelta%2524-IRIS%2520sets%2520a%2520new%2520state%2520of%250Athe%2520art%2520at%2520multiple%2520frame%2520budgets%252C%2520while%2520being%2520an%2520order%2520of%2520magnitude%2520faster%2520to%250Atrain%2520than%2520previous%2520attention-based%2520approaches.%2520We%2520release%2520our%2520code%2520and%2520models%250Aat%2520https%253A//github.com/vmicheli/delta-iris.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20World%20Models%20with%20Context-Aware%20Tokenization&entry.906535625=Vincent%20Micheli%20and%20Eloi%20Alonso%20and%20Fran%C3%A7ois%20Fleuret&entry.1292438233=%20%20Scaling%20up%20deep%20Reinforcement%20Learning%20%28RL%29%20methods%20presents%20a%20significant%0Achallenge.%20Following%20developments%20in%20generative%20modelling%2C%20model-based%20RL%0Apositions%20itself%20as%20a%20strong%20contender.%20Recent%20advances%20in%20sequence%20modelling%0Ahave%20led%20to%20effective%20transformer-based%20world%20models%2C%20albeit%20at%20the%20price%20of%0Aheavy%20computations%20due%20to%20the%20long%20sequences%20of%20tokens%20required%20to%20accurately%0Asimulate%20environments.%20In%20this%20work%2C%20we%20propose%20%24%5CDelta%24-IRIS%2C%20a%20new%20agent%20with%0Aa%20world%20model%20architecture%20composed%20of%20a%20discrete%20autoencoder%20that%20encodes%0Astochastic%20deltas%20between%20time%20steps%20and%20an%20autoregressive%20transformer%20that%0Apredicts%20future%20deltas%20by%20summarizing%20the%20current%20state%20of%20the%20world%20with%0Acontinuous%20tokens.%20In%20the%20Crafter%20benchmark%2C%20%24%5CDelta%24-IRIS%20sets%20a%20new%20state%20of%0Athe%20art%20at%20multiple%20frame%20budgets%2C%20while%20being%20an%20order%20of%20magnitude%20faster%20to%0Atrain%20than%20previous%20attention-based%20approaches.%20We%20release%20our%20code%20and%20models%0Aat%20https%3A//github.com/vmicheli/delta-iris.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19320v1&entry.124074799=Read"},
{"title": "BiCo-Fusion: Bidirectional Complementary LiDAR-Camera Fusion for\n  Semantic- and Spatial-Aware 3D Object Detection", "author": "Yang Song and Lin Wang", "abstract": "  3D object detection is an important task that has been widely applied in\nautonomous driving. Recently, fusing multi-modal inputs, i.e., LiDAR and camera\ndata, to perform this task has become a new trend. Existing methods, however,\neither ignore the sparsity of Lidar features or fail to preserve the original\nspatial structure of LiDAR and the semantic density of camera features\nsimultaneously due to the modality gap. To address issues, this letter proposes\na novel bidirectional complementary Lidar-camera fusion framework, called\nBiCo-Fusion that can achieve robust semantic- and spatial-aware 3D object\ndetection. The key insight is to mutually fuse the multi-modal features to\nenhance the semantics of LiDAR features and the spatial awareness of the camera\nfeatures and adaptatively select features from both modalities to build a\nunified 3D representation. Specifically, we introduce Pre-Fusion consisting of\na Voxel Enhancement Module (VEM) to enhance the semantics of voxel features\nfrom 2D camera features and Image Enhancement Module (IEM) to enhance the\nspatial characteristics of camera features from 3D voxel features. Both VEM and\nIEM are bidirectionally updated to effectively reduce the modality gap. We then\nintroduce Unified Fusion to adaptively weight to select features from the\nenchanted Lidar and camera features to build a unified 3D representation.\nExtensive experiments demonstrate the superiority of our BiCo-Fusion against\nthe prior arts. Project page: https://t-ys.github.io/BiCo-Fusion/.\n", "link": "http://arxiv.org/abs/2406.19048v1", "date": "2024-06-27", "relevancy": 2.3428, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5888}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5866}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiCo-Fusion%3A%20Bidirectional%20Complementary%20LiDAR-Camera%20Fusion%20for%0A%20%20Semantic-%20and%20Spatial-Aware%203D%20Object%20Detection&body=Title%3A%20BiCo-Fusion%3A%20Bidirectional%20Complementary%20LiDAR-Camera%20Fusion%20for%0A%20%20Semantic-%20and%20Spatial-Aware%203D%20Object%20Detection%0AAuthor%3A%20Yang%20Song%20and%20Lin%20Wang%0AAbstract%3A%20%20%203D%20object%20detection%20is%20an%20important%20task%20that%20has%20been%20widely%20applied%20in%0Aautonomous%20driving.%20Recently%2C%20fusing%20multi-modal%20inputs%2C%20i.e.%2C%20LiDAR%20and%20camera%0Adata%2C%20to%20perform%20this%20task%20has%20become%20a%20new%20trend.%20Existing%20methods%2C%20however%2C%0Aeither%20ignore%20the%20sparsity%20of%20Lidar%20features%20or%20fail%20to%20preserve%20the%20original%0Aspatial%20structure%20of%20LiDAR%20and%20the%20semantic%20density%20of%20camera%20features%0Asimultaneously%20due%20to%20the%20modality%20gap.%20To%20address%20issues%2C%20this%20letter%20proposes%0Aa%20novel%20bidirectional%20complementary%20Lidar-camera%20fusion%20framework%2C%20called%0ABiCo-Fusion%20that%20can%20achieve%20robust%20semantic-%20and%20spatial-aware%203D%20object%0Adetection.%20The%20key%20insight%20is%20to%20mutually%20fuse%20the%20multi-modal%20features%20to%0Aenhance%20the%20semantics%20of%20LiDAR%20features%20and%20the%20spatial%20awareness%20of%20the%20camera%0Afeatures%20and%20adaptatively%20select%20features%20from%20both%20modalities%20to%20build%20a%0Aunified%203D%20representation.%20Specifically%2C%20we%20introduce%20Pre-Fusion%20consisting%20of%0Aa%20Voxel%20Enhancement%20Module%20%28VEM%29%20to%20enhance%20the%20semantics%20of%20voxel%20features%0Afrom%202D%20camera%20features%20and%20Image%20Enhancement%20Module%20%28IEM%29%20to%20enhance%20the%0Aspatial%20characteristics%20of%20camera%20features%20from%203D%20voxel%20features.%20Both%20VEM%20and%0AIEM%20are%20bidirectionally%20updated%20to%20effectively%20reduce%20the%20modality%20gap.%20We%20then%0Aintroduce%20Unified%20Fusion%20to%20adaptively%20weight%20to%20select%20features%20from%20the%0Aenchanted%20Lidar%20and%20camera%20features%20to%20build%20a%20unified%203D%20representation.%0AExtensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20BiCo-Fusion%20against%0Athe%20prior%20arts.%20Project%20page%3A%20https%3A//t-ys.github.io/BiCo-Fusion/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19048v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiCo-Fusion%253A%2520Bidirectional%2520Complementary%2520LiDAR-Camera%2520Fusion%2520for%250A%2520%2520Semantic-%2520and%2520Spatial-Aware%25203D%2520Object%2520Detection%26entry.906535625%3DYang%2520Song%2520and%2520Lin%2520Wang%26entry.1292438233%3D%2520%25203D%2520object%2520detection%2520is%2520an%2520important%2520task%2520that%2520has%2520been%2520widely%2520applied%2520in%250Aautonomous%2520driving.%2520Recently%252C%2520fusing%2520multi-modal%2520inputs%252C%2520i.e.%252C%2520LiDAR%2520and%2520camera%250Adata%252C%2520to%2520perform%2520this%2520task%2520has%2520become%2520a%2520new%2520trend.%2520Existing%2520methods%252C%2520however%252C%250Aeither%2520ignore%2520the%2520sparsity%2520of%2520Lidar%2520features%2520or%2520fail%2520to%2520preserve%2520the%2520original%250Aspatial%2520structure%2520of%2520LiDAR%2520and%2520the%2520semantic%2520density%2520of%2520camera%2520features%250Asimultaneously%2520due%2520to%2520the%2520modality%2520gap.%2520To%2520address%2520issues%252C%2520this%2520letter%2520proposes%250Aa%2520novel%2520bidirectional%2520complementary%2520Lidar-camera%2520fusion%2520framework%252C%2520called%250ABiCo-Fusion%2520that%2520can%2520achieve%2520robust%2520semantic-%2520and%2520spatial-aware%25203D%2520object%250Adetection.%2520The%2520key%2520insight%2520is%2520to%2520mutually%2520fuse%2520the%2520multi-modal%2520features%2520to%250Aenhance%2520the%2520semantics%2520of%2520LiDAR%2520features%2520and%2520the%2520spatial%2520awareness%2520of%2520the%2520camera%250Afeatures%2520and%2520adaptatively%2520select%2520features%2520from%2520both%2520modalities%2520to%2520build%2520a%250Aunified%25203D%2520representation.%2520Specifically%252C%2520we%2520introduce%2520Pre-Fusion%2520consisting%2520of%250Aa%2520Voxel%2520Enhancement%2520Module%2520%2528VEM%2529%2520to%2520enhance%2520the%2520semantics%2520of%2520voxel%2520features%250Afrom%25202D%2520camera%2520features%2520and%2520Image%2520Enhancement%2520Module%2520%2528IEM%2529%2520to%2520enhance%2520the%250Aspatial%2520characteristics%2520of%2520camera%2520features%2520from%25203D%2520voxel%2520features.%2520Both%2520VEM%2520and%250AIEM%2520are%2520bidirectionally%2520updated%2520to%2520effectively%2520reduce%2520the%2520modality%2520gap.%2520We%2520then%250Aintroduce%2520Unified%2520Fusion%2520to%2520adaptively%2520weight%2520to%2520select%2520features%2520from%2520the%250Aenchanted%2520Lidar%2520and%2520camera%2520features%2520to%2520build%2520a%2520unified%25203D%2520representation.%250AExtensive%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520our%2520BiCo-Fusion%2520against%250Athe%2520prior%2520arts.%2520Project%2520page%253A%2520https%253A//t-ys.github.io/BiCo-Fusion/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19048v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiCo-Fusion%3A%20Bidirectional%20Complementary%20LiDAR-Camera%20Fusion%20for%0A%20%20Semantic-%20and%20Spatial-Aware%203D%20Object%20Detection&entry.906535625=Yang%20Song%20and%20Lin%20Wang&entry.1292438233=%20%203D%20object%20detection%20is%20an%20important%20task%20that%20has%20been%20widely%20applied%20in%0Aautonomous%20driving.%20Recently%2C%20fusing%20multi-modal%20inputs%2C%20i.e.%2C%20LiDAR%20and%20camera%0Adata%2C%20to%20perform%20this%20task%20has%20become%20a%20new%20trend.%20Existing%20methods%2C%20however%2C%0Aeither%20ignore%20the%20sparsity%20of%20Lidar%20features%20or%20fail%20to%20preserve%20the%20original%0Aspatial%20structure%20of%20LiDAR%20and%20the%20semantic%20density%20of%20camera%20features%0Asimultaneously%20due%20to%20the%20modality%20gap.%20To%20address%20issues%2C%20this%20letter%20proposes%0Aa%20novel%20bidirectional%20complementary%20Lidar-camera%20fusion%20framework%2C%20called%0ABiCo-Fusion%20that%20can%20achieve%20robust%20semantic-%20and%20spatial-aware%203D%20object%0Adetection.%20The%20key%20insight%20is%20to%20mutually%20fuse%20the%20multi-modal%20features%20to%0Aenhance%20the%20semantics%20of%20LiDAR%20features%20and%20the%20spatial%20awareness%20of%20the%20camera%0Afeatures%20and%20adaptatively%20select%20features%20from%20both%20modalities%20to%20build%20a%0Aunified%203D%20representation.%20Specifically%2C%20we%20introduce%20Pre-Fusion%20consisting%20of%0Aa%20Voxel%20Enhancement%20Module%20%28VEM%29%20to%20enhance%20the%20semantics%20of%20voxel%20features%0Afrom%202D%20camera%20features%20and%20Image%20Enhancement%20Module%20%28IEM%29%20to%20enhance%20the%0Aspatial%20characteristics%20of%20camera%20features%20from%203D%20voxel%20features.%20Both%20VEM%20and%0AIEM%20are%20bidirectionally%20updated%20to%20effectively%20reduce%20the%20modality%20gap.%20We%20then%0Aintroduce%20Unified%20Fusion%20to%20adaptively%20weight%20to%20select%20features%20from%20the%0Aenchanted%20Lidar%20and%20camera%20features%20to%20build%20a%20unified%203D%20representation.%0AExtensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20BiCo-Fusion%20against%0Athe%20prior%20arts.%20Project%20page%3A%20https%3A//t-ys.github.io/BiCo-Fusion/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19048v1&entry.124074799=Read"},
{"title": "Fibottention: Inceptive Visual Representation Learning with Diverse\n  Attention Across Heads", "author": "Ali Khaleghi Rahimian and Manish Kumar Govind and Subhajit Maity and Dominick Reilly and Christian K\u00fcmmerle and Srijan Das and Aritra Dutta", "abstract": "  Visual perception tasks are predominantly solved by Vision Transformer (ViT)\narchitectures, which, despite their effectiveness, encounter a computational\nbottleneck due to the quadratic complexity of computing self-attention. This\ninefficiency is largely due to the self-attention heads capturing redundant\ntoken interactions, reflecting inherent redundancy within visual data. Many\nworks have aimed to reduce the computational complexity of self-attention in\nViTs, leading to the development of efficient and sparse transformer\narchitectures. In this paper, viewing through the efficiency lens, we realized\nthat introducing any sparse self-attention strategy in ViTs can keep the\ncomputational overhead low. However, these strategies are sub-optimal as they\noften fail to capture fine-grained visual details. This observation leads us to\npropose a general, efficient, sparse architecture, named Fibottention, for\napproximating self-attention with superlinear complexity that is built upon\nFibonacci sequences. The key strategies in Fibottention include: it excludes\nproximate tokens to reduce redundancy, employs structured sparsity by design to\ndecrease computational demands, and incorporates inception-like diversity\nacross attention heads. This diversity ensures the capture of complementary\ninformation through non-overlapping token interactions, optimizing both\nperformance and resource utilization in ViTs for visual representation\nlearning. We embed our Fibottention mechanism into multiple state-of-the-art\ntransformer architectures dedicated to visual tasks. Leveraging only 2-6% of\nthe elements in the self-attention heads, Fibottention in conjunction with ViT\nand its variants, consistently achieves significant performance boosts compared\nto standard ViTs in nine datasets across three domains $\\unicode{x2013}$ image\nclassification, video understanding, and robot learning tasks.\n", "link": "http://arxiv.org/abs/2406.19391v1", "date": "2024-06-27", "relevancy": 2.3406, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.627}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5691}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fibottention%3A%20Inceptive%20Visual%20Representation%20Learning%20with%20Diverse%0A%20%20Attention%20Across%20Heads&body=Title%3A%20Fibottention%3A%20Inceptive%20Visual%20Representation%20Learning%20with%20Diverse%0A%20%20Attention%20Across%20Heads%0AAuthor%3A%20Ali%20Khaleghi%20Rahimian%20and%20Manish%20Kumar%20Govind%20and%20Subhajit%20Maity%20and%20Dominick%20Reilly%20and%20Christian%20K%C3%BCmmerle%20and%20Srijan%20Das%20and%20Aritra%20Dutta%0AAbstract%3A%20%20%20Visual%20perception%20tasks%20are%20predominantly%20solved%20by%20Vision%20Transformer%20%28ViT%29%0Aarchitectures%2C%20which%2C%20despite%20their%20effectiveness%2C%20encounter%20a%20computational%0Abottleneck%20due%20to%20the%20quadratic%20complexity%20of%20computing%20self-attention.%20This%0Ainefficiency%20is%20largely%20due%20to%20the%20self-attention%20heads%20capturing%20redundant%0Atoken%20interactions%2C%20reflecting%20inherent%20redundancy%20within%20visual%20data.%20Many%0Aworks%20have%20aimed%20to%20reduce%20the%20computational%20complexity%20of%20self-attention%20in%0AViTs%2C%20leading%20to%20the%20development%20of%20efficient%20and%20sparse%20transformer%0Aarchitectures.%20In%20this%20paper%2C%20viewing%20through%20the%20efficiency%20lens%2C%20we%20realized%0Athat%20introducing%20any%20sparse%20self-attention%20strategy%20in%20ViTs%20can%20keep%20the%0Acomputational%20overhead%20low.%20However%2C%20these%20strategies%20are%20sub-optimal%20as%20they%0Aoften%20fail%20to%20capture%20fine-grained%20visual%20details.%20This%20observation%20leads%20us%20to%0Apropose%20a%20general%2C%20efficient%2C%20sparse%20architecture%2C%20named%20Fibottention%2C%20for%0Aapproximating%20self-attention%20with%20superlinear%20complexity%20that%20is%20built%20upon%0AFibonacci%20sequences.%20The%20key%20strategies%20in%20Fibottention%20include%3A%20it%20excludes%0Aproximate%20tokens%20to%20reduce%20redundancy%2C%20employs%20structured%20sparsity%20by%20design%20to%0Adecrease%20computational%20demands%2C%20and%20incorporates%20inception-like%20diversity%0Aacross%20attention%20heads.%20This%20diversity%20ensures%20the%20capture%20of%20complementary%0Ainformation%20through%20non-overlapping%20token%20interactions%2C%20optimizing%20both%0Aperformance%20and%20resource%20utilization%20in%20ViTs%20for%20visual%20representation%0Alearning.%20We%20embed%20our%20Fibottention%20mechanism%20into%20multiple%20state-of-the-art%0Atransformer%20architectures%20dedicated%20to%20visual%20tasks.%20Leveraging%20only%202-6%25%20of%0Athe%20elements%20in%20the%20self-attention%20heads%2C%20Fibottention%20in%20conjunction%20with%20ViT%0Aand%20its%20variants%2C%20consistently%20achieves%20significant%20performance%20boosts%20compared%0Ato%20standard%20ViTs%20in%20nine%20datasets%20across%20three%20domains%20%24%5Cunicode%7Bx2013%7D%24%20image%0Aclassification%2C%20video%20understanding%2C%20and%20robot%20learning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19391v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFibottention%253A%2520Inceptive%2520Visual%2520Representation%2520Learning%2520with%2520Diverse%250A%2520%2520Attention%2520Across%2520Heads%26entry.906535625%3DAli%2520Khaleghi%2520Rahimian%2520and%2520Manish%2520Kumar%2520Govind%2520and%2520Subhajit%2520Maity%2520and%2520Dominick%2520Reilly%2520and%2520Christian%2520K%25C3%25BCmmerle%2520and%2520Srijan%2520Das%2520and%2520Aritra%2520Dutta%26entry.1292438233%3D%2520%2520Visual%2520perception%2520tasks%2520are%2520predominantly%2520solved%2520by%2520Vision%2520Transformer%2520%2528ViT%2529%250Aarchitectures%252C%2520which%252C%2520despite%2520their%2520effectiveness%252C%2520encounter%2520a%2520computational%250Abottleneck%2520due%2520to%2520the%2520quadratic%2520complexity%2520of%2520computing%2520self-attention.%2520This%250Ainefficiency%2520is%2520largely%2520due%2520to%2520the%2520self-attention%2520heads%2520capturing%2520redundant%250Atoken%2520interactions%252C%2520reflecting%2520inherent%2520redundancy%2520within%2520visual%2520data.%2520Many%250Aworks%2520have%2520aimed%2520to%2520reduce%2520the%2520computational%2520complexity%2520of%2520self-attention%2520in%250AViTs%252C%2520leading%2520to%2520the%2520development%2520of%2520efficient%2520and%2520sparse%2520transformer%250Aarchitectures.%2520In%2520this%2520paper%252C%2520viewing%2520through%2520the%2520efficiency%2520lens%252C%2520we%2520realized%250Athat%2520introducing%2520any%2520sparse%2520self-attention%2520strategy%2520in%2520ViTs%2520can%2520keep%2520the%250Acomputational%2520overhead%2520low.%2520However%252C%2520these%2520strategies%2520are%2520sub-optimal%2520as%2520they%250Aoften%2520fail%2520to%2520capture%2520fine-grained%2520visual%2520details.%2520This%2520observation%2520leads%2520us%2520to%250Apropose%2520a%2520general%252C%2520efficient%252C%2520sparse%2520architecture%252C%2520named%2520Fibottention%252C%2520for%250Aapproximating%2520self-attention%2520with%2520superlinear%2520complexity%2520that%2520is%2520built%2520upon%250AFibonacci%2520sequences.%2520The%2520key%2520strategies%2520in%2520Fibottention%2520include%253A%2520it%2520excludes%250Aproximate%2520tokens%2520to%2520reduce%2520redundancy%252C%2520employs%2520structured%2520sparsity%2520by%2520design%2520to%250Adecrease%2520computational%2520demands%252C%2520and%2520incorporates%2520inception-like%2520diversity%250Aacross%2520attention%2520heads.%2520This%2520diversity%2520ensures%2520the%2520capture%2520of%2520complementary%250Ainformation%2520through%2520non-overlapping%2520token%2520interactions%252C%2520optimizing%2520both%250Aperformance%2520and%2520resource%2520utilization%2520in%2520ViTs%2520for%2520visual%2520representation%250Alearning.%2520We%2520embed%2520our%2520Fibottention%2520mechanism%2520into%2520multiple%2520state-of-the-art%250Atransformer%2520architectures%2520dedicated%2520to%2520visual%2520tasks.%2520Leveraging%2520only%25202-6%2525%2520of%250Athe%2520elements%2520in%2520the%2520self-attention%2520heads%252C%2520Fibottention%2520in%2520conjunction%2520with%2520ViT%250Aand%2520its%2520variants%252C%2520consistently%2520achieves%2520significant%2520performance%2520boosts%2520compared%250Ato%2520standard%2520ViTs%2520in%2520nine%2520datasets%2520across%2520three%2520domains%2520%2524%255Cunicode%257Bx2013%257D%2524%2520image%250Aclassification%252C%2520video%2520understanding%252C%2520and%2520robot%2520learning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19391v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fibottention%3A%20Inceptive%20Visual%20Representation%20Learning%20with%20Diverse%0A%20%20Attention%20Across%20Heads&entry.906535625=Ali%20Khaleghi%20Rahimian%20and%20Manish%20Kumar%20Govind%20and%20Subhajit%20Maity%20and%20Dominick%20Reilly%20and%20Christian%20K%C3%BCmmerle%20and%20Srijan%20Das%20and%20Aritra%20Dutta&entry.1292438233=%20%20Visual%20perception%20tasks%20are%20predominantly%20solved%20by%20Vision%20Transformer%20%28ViT%29%0Aarchitectures%2C%20which%2C%20despite%20their%20effectiveness%2C%20encounter%20a%20computational%0Abottleneck%20due%20to%20the%20quadratic%20complexity%20of%20computing%20self-attention.%20This%0Ainefficiency%20is%20largely%20due%20to%20the%20self-attention%20heads%20capturing%20redundant%0Atoken%20interactions%2C%20reflecting%20inherent%20redundancy%20within%20visual%20data.%20Many%0Aworks%20have%20aimed%20to%20reduce%20the%20computational%20complexity%20of%20self-attention%20in%0AViTs%2C%20leading%20to%20the%20development%20of%20efficient%20and%20sparse%20transformer%0Aarchitectures.%20In%20this%20paper%2C%20viewing%20through%20the%20efficiency%20lens%2C%20we%20realized%0Athat%20introducing%20any%20sparse%20self-attention%20strategy%20in%20ViTs%20can%20keep%20the%0Acomputational%20overhead%20low.%20However%2C%20these%20strategies%20are%20sub-optimal%20as%20they%0Aoften%20fail%20to%20capture%20fine-grained%20visual%20details.%20This%20observation%20leads%20us%20to%0Apropose%20a%20general%2C%20efficient%2C%20sparse%20architecture%2C%20named%20Fibottention%2C%20for%0Aapproximating%20self-attention%20with%20superlinear%20complexity%20that%20is%20built%20upon%0AFibonacci%20sequences.%20The%20key%20strategies%20in%20Fibottention%20include%3A%20it%20excludes%0Aproximate%20tokens%20to%20reduce%20redundancy%2C%20employs%20structured%20sparsity%20by%20design%20to%0Adecrease%20computational%20demands%2C%20and%20incorporates%20inception-like%20diversity%0Aacross%20attention%20heads.%20This%20diversity%20ensures%20the%20capture%20of%20complementary%0Ainformation%20through%20non-overlapping%20token%20interactions%2C%20optimizing%20both%0Aperformance%20and%20resource%20utilization%20in%20ViTs%20for%20visual%20representation%0Alearning.%20We%20embed%20our%20Fibottention%20mechanism%20into%20multiple%20state-of-the-art%0Atransformer%20architectures%20dedicated%20to%20visual%20tasks.%20Leveraging%20only%202-6%25%20of%0Athe%20elements%20in%20the%20self-attention%20heads%2C%20Fibottention%20in%20conjunction%20with%20ViT%0Aand%20its%20variants%2C%20consistently%20achieves%20significant%20performance%20boosts%20compared%0Ato%20standard%20ViTs%20in%20nine%20datasets%20across%20three%20domains%20%24%5Cunicode%7Bx2013%7D%24%20image%0Aclassification%2C%20video%20understanding%2C%20and%20robot%20learning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19391v1&entry.124074799=Read"},
{"title": "BT-Adapter: Video Conversation is Feasible Without Video Instruction\n  Tuning", "author": "Ruyang Liu and Chen Li and Yixiao Ge and Ying Shan and Thomas H. Li and Ge Li", "abstract": "  The recent progress in Large Language Models (LLM) has spurred various\nadvancements in image-language conversation agents, while how to build a\nproficient video-based dialogue system is still under exploration. Considering\nthe extensive scale of LLM and visual backbone, minimal GPU memory is left for\nfacilitating effective temporal modeling, which is crucial for comprehending\nand providing feedback on videos. To this end, we propose Branching Temporal\nAdapter (BT-Adapter), a novel method for extending image-language pretrained\nmodels into the video domain. Specifically, BT-Adapter serves as a plug-and-use\ntemporal modeling branch alongside the pretrained visual encoder, which is\ntuned while keeping the backbone frozen. Just pretrained once, BT-Adapter can\nbe seamlessly integrated into all image conversation models using this version\nof CLIP, enabling video conversations without the need for video instructions.\nBesides, we develop a unique asymmetric token masking strategy inside the\nbranch with tailor-made training tasks for BT-Adapter, facilitating faster\nconvergence and better results. Thanks to BT-Adapter, we are able to empower\nexisting multimodal dialogue models with strong video understanding\ncapabilities without incurring excessive GPU costs. Without bells and whistles,\nBT-Adapter achieves (1) state-of-the-art zero-shot results on various video\ntasks using thousands of fewer GPU hours. (2) better performance than current\nvideo chatbots without any video instruction tuning. (3) state-of-the-art\nresults of video chatting using video instruction tuning, outperforming\nprevious SOTAs by a large margin.\n", "link": "http://arxiv.org/abs/2309.15785v2", "date": "2024-06-27", "relevancy": 2.3383, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6211}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5781}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BT-Adapter%3A%20Video%20Conversation%20is%20Feasible%20Without%20Video%20Instruction%0A%20%20Tuning&body=Title%3A%20BT-Adapter%3A%20Video%20Conversation%20is%20Feasible%20Without%20Video%20Instruction%0A%20%20Tuning%0AAuthor%3A%20Ruyang%20Liu%20and%20Chen%20Li%20and%20Yixiao%20Ge%20and%20Ying%20Shan%20and%20Thomas%20H.%20Li%20and%20Ge%20Li%0AAbstract%3A%20%20%20The%20recent%20progress%20in%20Large%20Language%20Models%20%28LLM%29%20has%20spurred%20various%0Aadvancements%20in%20image-language%20conversation%20agents%2C%20while%20how%20to%20build%20a%0Aproficient%20video-based%20dialogue%20system%20is%20still%20under%20exploration.%20Considering%0Athe%20extensive%20scale%20of%20LLM%20and%20visual%20backbone%2C%20minimal%20GPU%20memory%20is%20left%20for%0Afacilitating%20effective%20temporal%20modeling%2C%20which%20is%20crucial%20for%20comprehending%0Aand%20providing%20feedback%20on%20videos.%20To%20this%20end%2C%20we%20propose%20Branching%20Temporal%0AAdapter%20%28BT-Adapter%29%2C%20a%20novel%20method%20for%20extending%20image-language%20pretrained%0Amodels%20into%20the%20video%20domain.%20Specifically%2C%20BT-Adapter%20serves%20as%20a%20plug-and-use%0Atemporal%20modeling%20branch%20alongside%20the%20pretrained%20visual%20encoder%2C%20which%20is%0Atuned%20while%20keeping%20the%20backbone%20frozen.%20Just%20pretrained%20once%2C%20BT-Adapter%20can%0Abe%20seamlessly%20integrated%20into%20all%20image%20conversation%20models%20using%20this%20version%0Aof%20CLIP%2C%20enabling%20video%20conversations%20without%20the%20need%20for%20video%20instructions.%0ABesides%2C%20we%20develop%20a%20unique%20asymmetric%20token%20masking%20strategy%20inside%20the%0Abranch%20with%20tailor-made%20training%20tasks%20for%20BT-Adapter%2C%20facilitating%20faster%0Aconvergence%20and%20better%20results.%20Thanks%20to%20BT-Adapter%2C%20we%20are%20able%20to%20empower%0Aexisting%20multimodal%20dialogue%20models%20with%20strong%20video%20understanding%0Acapabilities%20without%20incurring%20excessive%20GPU%20costs.%20Without%20bells%20and%20whistles%2C%0ABT-Adapter%20achieves%20%281%29%20state-of-the-art%20zero-shot%20results%20on%20various%20video%0Atasks%20using%20thousands%20of%20fewer%20GPU%20hours.%20%282%29%20better%20performance%20than%20current%0Avideo%20chatbots%20without%20any%20video%20instruction%20tuning.%20%283%29%20state-of-the-art%0Aresults%20of%20video%20chatting%20using%20video%20instruction%20tuning%2C%20outperforming%0Aprevious%20SOTAs%20by%20a%20large%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.15785v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBT-Adapter%253A%2520Video%2520Conversation%2520is%2520Feasible%2520Without%2520Video%2520Instruction%250A%2520%2520Tuning%26entry.906535625%3DRuyang%2520Liu%2520and%2520Chen%2520Li%2520and%2520Yixiao%2520Ge%2520and%2520Ying%2520Shan%2520and%2520Thomas%2520H.%2520Li%2520and%2520Ge%2520Li%26entry.1292438233%3D%2520%2520The%2520recent%2520progress%2520in%2520Large%2520Language%2520Models%2520%2528LLM%2529%2520has%2520spurred%2520various%250Aadvancements%2520in%2520image-language%2520conversation%2520agents%252C%2520while%2520how%2520to%2520build%2520a%250Aproficient%2520video-based%2520dialogue%2520system%2520is%2520still%2520under%2520exploration.%2520Considering%250Athe%2520extensive%2520scale%2520of%2520LLM%2520and%2520visual%2520backbone%252C%2520minimal%2520GPU%2520memory%2520is%2520left%2520for%250Afacilitating%2520effective%2520temporal%2520modeling%252C%2520which%2520is%2520crucial%2520for%2520comprehending%250Aand%2520providing%2520feedback%2520on%2520videos.%2520To%2520this%2520end%252C%2520we%2520propose%2520Branching%2520Temporal%250AAdapter%2520%2528BT-Adapter%2529%252C%2520a%2520novel%2520method%2520for%2520extending%2520image-language%2520pretrained%250Amodels%2520into%2520the%2520video%2520domain.%2520Specifically%252C%2520BT-Adapter%2520serves%2520as%2520a%2520plug-and-use%250Atemporal%2520modeling%2520branch%2520alongside%2520the%2520pretrained%2520visual%2520encoder%252C%2520which%2520is%250Atuned%2520while%2520keeping%2520the%2520backbone%2520frozen.%2520Just%2520pretrained%2520once%252C%2520BT-Adapter%2520can%250Abe%2520seamlessly%2520integrated%2520into%2520all%2520image%2520conversation%2520models%2520using%2520this%2520version%250Aof%2520CLIP%252C%2520enabling%2520video%2520conversations%2520without%2520the%2520need%2520for%2520video%2520instructions.%250ABesides%252C%2520we%2520develop%2520a%2520unique%2520asymmetric%2520token%2520masking%2520strategy%2520inside%2520the%250Abranch%2520with%2520tailor-made%2520training%2520tasks%2520for%2520BT-Adapter%252C%2520facilitating%2520faster%250Aconvergence%2520and%2520better%2520results.%2520Thanks%2520to%2520BT-Adapter%252C%2520we%2520are%2520able%2520to%2520empower%250Aexisting%2520multimodal%2520dialogue%2520models%2520with%2520strong%2520video%2520understanding%250Acapabilities%2520without%2520incurring%2520excessive%2520GPU%2520costs.%2520Without%2520bells%2520and%2520whistles%252C%250ABT-Adapter%2520achieves%2520%25281%2529%2520state-of-the-art%2520zero-shot%2520results%2520on%2520various%2520video%250Atasks%2520using%2520thousands%2520of%2520fewer%2520GPU%2520hours.%2520%25282%2529%2520better%2520performance%2520than%2520current%250Avideo%2520chatbots%2520without%2520any%2520video%2520instruction%2520tuning.%2520%25283%2529%2520state-of-the-art%250Aresults%2520of%2520video%2520chatting%2520using%2520video%2520instruction%2520tuning%252C%2520outperforming%250Aprevious%2520SOTAs%2520by%2520a%2520large%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.15785v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BT-Adapter%3A%20Video%20Conversation%20is%20Feasible%20Without%20Video%20Instruction%0A%20%20Tuning&entry.906535625=Ruyang%20Liu%20and%20Chen%20Li%20and%20Yixiao%20Ge%20and%20Ying%20Shan%20and%20Thomas%20H.%20Li%20and%20Ge%20Li&entry.1292438233=%20%20The%20recent%20progress%20in%20Large%20Language%20Models%20%28LLM%29%20has%20spurred%20various%0Aadvancements%20in%20image-language%20conversation%20agents%2C%20while%20how%20to%20build%20a%0Aproficient%20video-based%20dialogue%20system%20is%20still%20under%20exploration.%20Considering%0Athe%20extensive%20scale%20of%20LLM%20and%20visual%20backbone%2C%20minimal%20GPU%20memory%20is%20left%20for%0Afacilitating%20effective%20temporal%20modeling%2C%20which%20is%20crucial%20for%20comprehending%0Aand%20providing%20feedback%20on%20videos.%20To%20this%20end%2C%20we%20propose%20Branching%20Temporal%0AAdapter%20%28BT-Adapter%29%2C%20a%20novel%20method%20for%20extending%20image-language%20pretrained%0Amodels%20into%20the%20video%20domain.%20Specifically%2C%20BT-Adapter%20serves%20as%20a%20plug-and-use%0Atemporal%20modeling%20branch%20alongside%20the%20pretrained%20visual%20encoder%2C%20which%20is%0Atuned%20while%20keeping%20the%20backbone%20frozen.%20Just%20pretrained%20once%2C%20BT-Adapter%20can%0Abe%20seamlessly%20integrated%20into%20all%20image%20conversation%20models%20using%20this%20version%0Aof%20CLIP%2C%20enabling%20video%20conversations%20without%20the%20need%20for%20video%20instructions.%0ABesides%2C%20we%20develop%20a%20unique%20asymmetric%20token%20masking%20strategy%20inside%20the%0Abranch%20with%20tailor-made%20training%20tasks%20for%20BT-Adapter%2C%20facilitating%20faster%0Aconvergence%20and%20better%20results.%20Thanks%20to%20BT-Adapter%2C%20we%20are%20able%20to%20empower%0Aexisting%20multimodal%20dialogue%20models%20with%20strong%20video%20understanding%0Acapabilities%20without%20incurring%20excessive%20GPU%20costs.%20Without%20bells%20and%20whistles%2C%0ABT-Adapter%20achieves%20%281%29%20state-of-the-art%20zero-shot%20results%20on%20various%20video%0Atasks%20using%20thousands%20of%20fewer%20GPU%20hours.%20%282%29%20better%20performance%20than%20current%0Avideo%20chatbots%20without%20any%20video%20instruction%20tuning.%20%283%29%20state-of-the-art%0Aresults%20of%20video%20chatting%20using%20video%20instruction%20tuning%2C%20outperforming%0Aprevious%20SOTAs%20by%20a%20large%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.15785v2&entry.124074799=Read"},
{"title": "Multimodal Visual-haptic pose estimation in the presence of transient\n  occlusion", "author": "Michael Zechmair and Yannick Morel", "abstract": "  Human-robot collaboration requires the establishment of methods to guarantee\nthe safety of participating operators. A necessary part of this process is\nensuring reliable human pose estimation. Established vision-based modalities\nencounter problems when under conditions of occlusion. This article describes\nthe combination of two perception modalities for pose estimation in\nenvironments containing such transient occlusion. We first introduce a\nvision-based pose estimation method, based on a deep Predictive Coding (PC)\nmodel featuring robustness to partial occlusion. Next, capacitive sensing\nhardware capable of detecting various objects is introduced. The sensor is\ncompact enough to be mounted on the exterior of any given robotic system. The\ntechnology is particularly well-suited to detection of capacitive material,\nsuch as living tissue. Pose estimation from the two individual sensing\nmodalities is combined using a modified Luenberger observer model. We\ndemonstrate that the results offer better performance than either sensor alone.\nThe efficacy of the system is demonstrated on an environment containing a robot\narm and a human, showing the ability to estimate the pose of a human forearm\nunder varying levels of occlusion.\n", "link": "http://arxiv.org/abs/2406.19323v1", "date": "2024-06-27", "relevancy": 2.3211, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6489}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5762}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Visual-haptic%20pose%20estimation%20in%20the%20presence%20of%20transient%0A%20%20occlusion&body=Title%3A%20Multimodal%20Visual-haptic%20pose%20estimation%20in%20the%20presence%20of%20transient%0A%20%20occlusion%0AAuthor%3A%20Michael%20Zechmair%20and%20Yannick%20Morel%0AAbstract%3A%20%20%20Human-robot%20collaboration%20requires%20the%20establishment%20of%20methods%20to%20guarantee%0Athe%20safety%20of%20participating%20operators.%20A%20necessary%20part%20of%20this%20process%20is%0Aensuring%20reliable%20human%20pose%20estimation.%20Established%20vision-based%20modalities%0Aencounter%20problems%20when%20under%20conditions%20of%20occlusion.%20This%20article%20describes%0Athe%20combination%20of%20two%20perception%20modalities%20for%20pose%20estimation%20in%0Aenvironments%20containing%20such%20transient%20occlusion.%20We%20first%20introduce%20a%0Avision-based%20pose%20estimation%20method%2C%20based%20on%20a%20deep%20Predictive%20Coding%20%28PC%29%0Amodel%20featuring%20robustness%20to%20partial%20occlusion.%20Next%2C%20capacitive%20sensing%0Ahardware%20capable%20of%20detecting%20various%20objects%20is%20introduced.%20The%20sensor%20is%0Acompact%20enough%20to%20be%20mounted%20on%20the%20exterior%20of%20any%20given%20robotic%20system.%20The%0Atechnology%20is%20particularly%20well-suited%20to%20detection%20of%20capacitive%20material%2C%0Asuch%20as%20living%20tissue.%20Pose%20estimation%20from%20the%20two%20individual%20sensing%0Amodalities%20is%20combined%20using%20a%20modified%20Luenberger%20observer%20model.%20We%0Ademonstrate%20that%20the%20results%20offer%20better%20performance%20than%20either%20sensor%20alone.%0AThe%20efficacy%20of%20the%20system%20is%20demonstrated%20on%20an%20environment%20containing%20a%20robot%0Aarm%20and%20a%20human%2C%20showing%20the%20ability%20to%20estimate%20the%20pose%20of%20a%20human%20forearm%0Aunder%20varying%20levels%20of%20occlusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Visual-haptic%2520pose%2520estimation%2520in%2520the%2520presence%2520of%2520transient%250A%2520%2520occlusion%26entry.906535625%3DMichael%2520Zechmair%2520and%2520Yannick%2520Morel%26entry.1292438233%3D%2520%2520Human-robot%2520collaboration%2520requires%2520the%2520establishment%2520of%2520methods%2520to%2520guarantee%250Athe%2520safety%2520of%2520participating%2520operators.%2520A%2520necessary%2520part%2520of%2520this%2520process%2520is%250Aensuring%2520reliable%2520human%2520pose%2520estimation.%2520Established%2520vision-based%2520modalities%250Aencounter%2520problems%2520when%2520under%2520conditions%2520of%2520occlusion.%2520This%2520article%2520describes%250Athe%2520combination%2520of%2520two%2520perception%2520modalities%2520for%2520pose%2520estimation%2520in%250Aenvironments%2520containing%2520such%2520transient%2520occlusion.%2520We%2520first%2520introduce%2520a%250Avision-based%2520pose%2520estimation%2520method%252C%2520based%2520on%2520a%2520deep%2520Predictive%2520Coding%2520%2528PC%2529%250Amodel%2520featuring%2520robustness%2520to%2520partial%2520occlusion.%2520Next%252C%2520capacitive%2520sensing%250Ahardware%2520capable%2520of%2520detecting%2520various%2520objects%2520is%2520introduced.%2520The%2520sensor%2520is%250Acompact%2520enough%2520to%2520be%2520mounted%2520on%2520the%2520exterior%2520of%2520any%2520given%2520robotic%2520system.%2520The%250Atechnology%2520is%2520particularly%2520well-suited%2520to%2520detection%2520of%2520capacitive%2520material%252C%250Asuch%2520as%2520living%2520tissue.%2520Pose%2520estimation%2520from%2520the%2520two%2520individual%2520sensing%250Amodalities%2520is%2520combined%2520using%2520a%2520modified%2520Luenberger%2520observer%2520model.%2520We%250Ademonstrate%2520that%2520the%2520results%2520offer%2520better%2520performance%2520than%2520either%2520sensor%2520alone.%250AThe%2520efficacy%2520of%2520the%2520system%2520is%2520demonstrated%2520on%2520an%2520environment%2520containing%2520a%2520robot%250Aarm%2520and%2520a%2520human%252C%2520showing%2520the%2520ability%2520to%2520estimate%2520the%2520pose%2520of%2520a%2520human%2520forearm%250Aunder%2520varying%2520levels%2520of%2520occlusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Visual-haptic%20pose%20estimation%20in%20the%20presence%20of%20transient%0A%20%20occlusion&entry.906535625=Michael%20Zechmair%20and%20Yannick%20Morel&entry.1292438233=%20%20Human-robot%20collaboration%20requires%20the%20establishment%20of%20methods%20to%20guarantee%0Athe%20safety%20of%20participating%20operators.%20A%20necessary%20part%20of%20this%20process%20is%0Aensuring%20reliable%20human%20pose%20estimation.%20Established%20vision-based%20modalities%0Aencounter%20problems%20when%20under%20conditions%20of%20occlusion.%20This%20article%20describes%0Athe%20combination%20of%20two%20perception%20modalities%20for%20pose%20estimation%20in%0Aenvironments%20containing%20such%20transient%20occlusion.%20We%20first%20introduce%20a%0Avision-based%20pose%20estimation%20method%2C%20based%20on%20a%20deep%20Predictive%20Coding%20%28PC%29%0Amodel%20featuring%20robustness%20to%20partial%20occlusion.%20Next%2C%20capacitive%20sensing%0Ahardware%20capable%20of%20detecting%20various%20objects%20is%20introduced.%20The%20sensor%20is%0Acompact%20enough%20to%20be%20mounted%20on%20the%20exterior%20of%20any%20given%20robotic%20system.%20The%0Atechnology%20is%20particularly%20well-suited%20to%20detection%20of%20capacitive%20material%2C%0Asuch%20as%20living%20tissue.%20Pose%20estimation%20from%20the%20two%20individual%20sensing%0Amodalities%20is%20combined%20using%20a%20modified%20Luenberger%20observer%20model.%20We%0Ademonstrate%20that%20the%20results%20offer%20better%20performance%20than%20either%20sensor%20alone.%0AThe%20efficacy%20of%20the%20system%20is%20demonstrated%20on%20an%20environment%20containing%20a%20robot%0Aarm%20and%20a%20human%2C%20showing%20the%20ability%20to%20estimate%20the%20pose%20of%20a%20human%20forearm%0Aunder%20varying%20levels%20of%20occlusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19323v1&entry.124074799=Read"},
{"title": "D-GRIL: End-to-End Topological Learning with 2-parameter Persistence", "author": "Soham Mukherjee and Shreyas N. Samaga and Cheng Xin and Steve Oudot and Tamal K. Dey", "abstract": "  End-to-end topological learning using 1-parameter persistence is well-known.\nWe show that the framework can be enhanced using 2-parameter persistence by\nadopting a recently introduced 2-parameter persistence based vectorization\ntechnique called GRIL. We establish a theoretical foundation of differentiating\nGRIL producing D-GRIL. We show that D-GRIL can be used to learn a bifiltration\nfunction on standard benchmark graph datasets. Further, we exhibit that this\nframework can be applied in the context of bio-activity prediction in drug\ndiscovery.\n", "link": "http://arxiv.org/abs/2406.07100v2", "date": "2024-06-27", "relevancy": 2.3194, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4757}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4715}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D-GRIL%3A%20End-to-End%20Topological%20Learning%20with%202-parameter%20Persistence&body=Title%3A%20D-GRIL%3A%20End-to-End%20Topological%20Learning%20with%202-parameter%20Persistence%0AAuthor%3A%20Soham%20Mukherjee%20and%20Shreyas%20N.%20Samaga%20and%20Cheng%20Xin%20and%20Steve%20Oudot%20and%20Tamal%20K.%20Dey%0AAbstract%3A%20%20%20End-to-end%20topological%20learning%20using%201-parameter%20persistence%20is%20well-known.%0AWe%20show%20that%20the%20framework%20can%20be%20enhanced%20using%202-parameter%20persistence%20by%0Aadopting%20a%20recently%20introduced%202-parameter%20persistence%20based%20vectorization%0Atechnique%20called%20GRIL.%20We%20establish%20a%20theoretical%20foundation%20of%20differentiating%0AGRIL%20producing%20D-GRIL.%20We%20show%20that%20D-GRIL%20can%20be%20used%20to%20learn%20a%20bifiltration%0Afunction%20on%20standard%20benchmark%20graph%20datasets.%20Further%2C%20we%20exhibit%20that%20this%0Aframework%20can%20be%20applied%20in%20the%20context%20of%20bio-activity%20prediction%20in%20drug%0Adiscovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07100v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD-GRIL%253A%2520End-to-End%2520Topological%2520Learning%2520with%25202-parameter%2520Persistence%26entry.906535625%3DSoham%2520Mukherjee%2520and%2520Shreyas%2520N.%2520Samaga%2520and%2520Cheng%2520Xin%2520and%2520Steve%2520Oudot%2520and%2520Tamal%2520K.%2520Dey%26entry.1292438233%3D%2520%2520End-to-end%2520topological%2520learning%2520using%25201-parameter%2520persistence%2520is%2520well-known.%250AWe%2520show%2520that%2520the%2520framework%2520can%2520be%2520enhanced%2520using%25202-parameter%2520persistence%2520by%250Aadopting%2520a%2520recently%2520introduced%25202-parameter%2520persistence%2520based%2520vectorization%250Atechnique%2520called%2520GRIL.%2520We%2520establish%2520a%2520theoretical%2520foundation%2520of%2520differentiating%250AGRIL%2520producing%2520D-GRIL.%2520We%2520show%2520that%2520D-GRIL%2520can%2520be%2520used%2520to%2520learn%2520a%2520bifiltration%250Afunction%2520on%2520standard%2520benchmark%2520graph%2520datasets.%2520Further%252C%2520we%2520exhibit%2520that%2520this%250Aframework%2520can%2520be%2520applied%2520in%2520the%2520context%2520of%2520bio-activity%2520prediction%2520in%2520drug%250Adiscovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07100v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D-GRIL%3A%20End-to-End%20Topological%20Learning%20with%202-parameter%20Persistence&entry.906535625=Soham%20Mukherjee%20and%20Shreyas%20N.%20Samaga%20and%20Cheng%20Xin%20and%20Steve%20Oudot%20and%20Tamal%20K.%20Dey&entry.1292438233=%20%20End-to-end%20topological%20learning%20using%201-parameter%20persistence%20is%20well-known.%0AWe%20show%20that%20the%20framework%20can%20be%20enhanced%20using%202-parameter%20persistence%20by%0Aadopting%20a%20recently%20introduced%202-parameter%20persistence%20based%20vectorization%0Atechnique%20called%20GRIL.%20We%20establish%20a%20theoretical%20foundation%20of%20differentiating%0AGRIL%20producing%20D-GRIL.%20We%20show%20that%20D-GRIL%20can%20be%20used%20to%20learn%20a%20bifiltration%0Afunction%20on%20standard%20benchmark%20graph%20datasets.%20Further%2C%20we%20exhibit%20that%20this%0Aframework%20can%20be%20applied%20in%20the%20context%20of%20bio-activity%20prediction%20in%20drug%0Adiscovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07100v2&entry.124074799=Read"},
{"title": "Self-Supervised Detection of Perfect and Partial Input-Dependent\n  Symmetries", "author": "Alonso Urbano and David W. Romero", "abstract": "  Group equivariance can overly constrain models if the symmetries in the group\ndiffer from those observed in data. While common methods address this by\ndetermining the appropriate level of symmetry at the dataset level, they are\nlimited to supervised settings and ignore scenarios in which multiple levels of\nsymmetry co-exist in the same dataset. In this paper, we propose a method able\nto detect the level of symmetry of each input without the need for labels. Our\nframework is general enough to accommodate different families of both\ncontinuous and discrete symmetry distributions, such as arbitrary unimodal,\nsymmetric distributions and discrete groups. We validate the effectiveness of\nour approach on synthetic datasets with different per-class levels of\nsymmetries, and demonstrate practical applications such as the detection of\nout-of-distribution symmetries. Our code is publicly available at\nhttps://github.com/aurban0/ssl-sym.\n", "link": "http://arxiv.org/abs/2312.12223v3", "date": "2024-06-27", "relevancy": 2.3069, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4946}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.453}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Detection%20of%20Perfect%20and%20Partial%20Input-Dependent%0A%20%20Symmetries&body=Title%3A%20Self-Supervised%20Detection%20of%20Perfect%20and%20Partial%20Input-Dependent%0A%20%20Symmetries%0AAuthor%3A%20Alonso%20Urbano%20and%20David%20W.%20Romero%0AAbstract%3A%20%20%20Group%20equivariance%20can%20overly%20constrain%20models%20if%20the%20symmetries%20in%20the%20group%0Adiffer%20from%20those%20observed%20in%20data.%20While%20common%20methods%20address%20this%20by%0Adetermining%20the%20appropriate%20level%20of%20symmetry%20at%20the%20dataset%20level%2C%20they%20are%0Alimited%20to%20supervised%20settings%20and%20ignore%20scenarios%20in%20which%20multiple%20levels%20of%0Asymmetry%20co-exist%20in%20the%20same%20dataset.%20In%20this%20paper%2C%20we%20propose%20a%20method%20able%0Ato%20detect%20the%20level%20of%20symmetry%20of%20each%20input%20without%20the%20need%20for%20labels.%20Our%0Aframework%20is%20general%20enough%20to%20accommodate%20different%20families%20of%20both%0Acontinuous%20and%20discrete%20symmetry%20distributions%2C%20such%20as%20arbitrary%20unimodal%2C%0Asymmetric%20distributions%20and%20discrete%20groups.%20We%20validate%20the%20effectiveness%20of%0Aour%20approach%20on%20synthetic%20datasets%20with%20different%20per-class%20levels%20of%0Asymmetries%2C%20and%20demonstrate%20practical%20applications%20such%20as%20the%20detection%20of%0Aout-of-distribution%20symmetries.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/aurban0/ssl-sym.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12223v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Detection%2520of%2520Perfect%2520and%2520Partial%2520Input-Dependent%250A%2520%2520Symmetries%26entry.906535625%3DAlonso%2520Urbano%2520and%2520David%2520W.%2520Romero%26entry.1292438233%3D%2520%2520Group%2520equivariance%2520can%2520overly%2520constrain%2520models%2520if%2520the%2520symmetries%2520in%2520the%2520group%250Adiffer%2520from%2520those%2520observed%2520in%2520data.%2520While%2520common%2520methods%2520address%2520this%2520by%250Adetermining%2520the%2520appropriate%2520level%2520of%2520symmetry%2520at%2520the%2520dataset%2520level%252C%2520they%2520are%250Alimited%2520to%2520supervised%2520settings%2520and%2520ignore%2520scenarios%2520in%2520which%2520multiple%2520levels%2520of%250Asymmetry%2520co-exist%2520in%2520the%2520same%2520dataset.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520method%2520able%250Ato%2520detect%2520the%2520level%2520of%2520symmetry%2520of%2520each%2520input%2520without%2520the%2520need%2520for%2520labels.%2520Our%250Aframework%2520is%2520general%2520enough%2520to%2520accommodate%2520different%2520families%2520of%2520both%250Acontinuous%2520and%2520discrete%2520symmetry%2520distributions%252C%2520such%2520as%2520arbitrary%2520unimodal%252C%250Asymmetric%2520distributions%2520and%2520discrete%2520groups.%2520We%2520validate%2520the%2520effectiveness%2520of%250Aour%2520approach%2520on%2520synthetic%2520datasets%2520with%2520different%2520per-class%2520levels%2520of%250Asymmetries%252C%2520and%2520demonstrate%2520practical%2520applications%2520such%2520as%2520the%2520detection%2520of%250Aout-of-distribution%2520symmetries.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/aurban0/ssl-sym.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.12223v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Detection%20of%20Perfect%20and%20Partial%20Input-Dependent%0A%20%20Symmetries&entry.906535625=Alonso%20Urbano%20and%20David%20W.%20Romero&entry.1292438233=%20%20Group%20equivariance%20can%20overly%20constrain%20models%20if%20the%20symmetries%20in%20the%20group%0Adiffer%20from%20those%20observed%20in%20data.%20While%20common%20methods%20address%20this%20by%0Adetermining%20the%20appropriate%20level%20of%20symmetry%20at%20the%20dataset%20level%2C%20they%20are%0Alimited%20to%20supervised%20settings%20and%20ignore%20scenarios%20in%20which%20multiple%20levels%20of%0Asymmetry%20co-exist%20in%20the%20same%20dataset.%20In%20this%20paper%2C%20we%20propose%20a%20method%20able%0Ato%20detect%20the%20level%20of%20symmetry%20of%20each%20input%20without%20the%20need%20for%20labels.%20Our%0Aframework%20is%20general%20enough%20to%20accommodate%20different%20families%20of%20both%0Acontinuous%20and%20discrete%20symmetry%20distributions%2C%20such%20as%20arbitrary%20unimodal%2C%0Asymmetric%20distributions%20and%20discrete%20groups.%20We%20validate%20the%20effectiveness%20of%0Aour%20approach%20on%20synthetic%20datasets%20with%20different%20per-class%20levels%20of%0Asymmetries%2C%20and%20demonstrate%20practical%20applications%20such%20as%20the%20detection%20of%0Aout-of-distribution%20symmetries.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/aurban0/ssl-sym.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12223v3&entry.124074799=Read"},
{"title": "Looking 3D: Anomaly Detection with 2D-3D Alignment", "author": "Ankan Bhunia and Changjian Li and Hakan Bilen", "abstract": "  Automatic anomaly detection based on visual cues holds practical significance\nin various domains, such as manufacturing and product quality assessment. This\npaper introduces a new conditional anomaly detection problem, which involves\nidentifying anomalies in a query image by comparing it to a reference shape. To\naddress this challenge, we have created a large dataset, BrokenChairs-180K,\nconsisting of around 180K images, with diverse anomalies, geometries, and\ntextures paired with 8,143 reference 3D shapes. To tackle this task, we have\nproposed a novel transformer-based approach that explicitly learns the\ncorrespondence between the query image and reference 3D shape via feature\nalignment and leverages a customized attention mechanism for anomaly detection.\nOur approach has been rigorously evaluated through comprehensive experiments,\nserving as a benchmark for future research in this domain.\n", "link": "http://arxiv.org/abs/2406.19393v1", "date": "2024-06-27", "relevancy": 2.2961, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5783}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5741}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Looking%203D%3A%20Anomaly%20Detection%20with%202D-3D%20Alignment&body=Title%3A%20Looking%203D%3A%20Anomaly%20Detection%20with%202D-3D%20Alignment%0AAuthor%3A%20Ankan%20Bhunia%20and%20Changjian%20Li%20and%20Hakan%20Bilen%0AAbstract%3A%20%20%20Automatic%20anomaly%20detection%20based%20on%20visual%20cues%20holds%20practical%20significance%0Ain%20various%20domains%2C%20such%20as%20manufacturing%20and%20product%20quality%20assessment.%20This%0Apaper%20introduces%20a%20new%20conditional%20anomaly%20detection%20problem%2C%20which%20involves%0Aidentifying%20anomalies%20in%20a%20query%20image%20by%20comparing%20it%20to%20a%20reference%20shape.%20To%0Aaddress%20this%20challenge%2C%20we%20have%20created%20a%20large%20dataset%2C%20BrokenChairs-180K%2C%0Aconsisting%20of%20around%20180K%20images%2C%20with%20diverse%20anomalies%2C%20geometries%2C%20and%0Atextures%20paired%20with%208%2C143%20reference%203D%20shapes.%20To%20tackle%20this%20task%2C%20we%20have%0Aproposed%20a%20novel%20transformer-based%20approach%20that%20explicitly%20learns%20the%0Acorrespondence%20between%20the%20query%20image%20and%20reference%203D%20shape%20via%20feature%0Aalignment%20and%20leverages%20a%20customized%20attention%20mechanism%20for%20anomaly%20detection.%0AOur%20approach%20has%20been%20rigorously%20evaluated%20through%20comprehensive%20experiments%2C%0Aserving%20as%20a%20benchmark%20for%20future%20research%20in%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19393v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLooking%25203D%253A%2520Anomaly%2520Detection%2520with%25202D-3D%2520Alignment%26entry.906535625%3DAnkan%2520Bhunia%2520and%2520Changjian%2520Li%2520and%2520Hakan%2520Bilen%26entry.1292438233%3D%2520%2520Automatic%2520anomaly%2520detection%2520based%2520on%2520visual%2520cues%2520holds%2520practical%2520significance%250Ain%2520various%2520domains%252C%2520such%2520as%2520manufacturing%2520and%2520product%2520quality%2520assessment.%2520This%250Apaper%2520introduces%2520a%2520new%2520conditional%2520anomaly%2520detection%2520problem%252C%2520which%2520involves%250Aidentifying%2520anomalies%2520in%2520a%2520query%2520image%2520by%2520comparing%2520it%2520to%2520a%2520reference%2520shape.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520have%2520created%2520a%2520large%2520dataset%252C%2520BrokenChairs-180K%252C%250Aconsisting%2520of%2520around%2520180K%2520images%252C%2520with%2520diverse%2520anomalies%252C%2520geometries%252C%2520and%250Atextures%2520paired%2520with%25208%252C143%2520reference%25203D%2520shapes.%2520To%2520tackle%2520this%2520task%252C%2520we%2520have%250Aproposed%2520a%2520novel%2520transformer-based%2520approach%2520that%2520explicitly%2520learns%2520the%250Acorrespondence%2520between%2520the%2520query%2520image%2520and%2520reference%25203D%2520shape%2520via%2520feature%250Aalignment%2520and%2520leverages%2520a%2520customized%2520attention%2520mechanism%2520for%2520anomaly%2520detection.%250AOur%2520approach%2520has%2520been%2520rigorously%2520evaluated%2520through%2520comprehensive%2520experiments%252C%250Aserving%2520as%2520a%2520benchmark%2520for%2520future%2520research%2520in%2520this%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19393v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Looking%203D%3A%20Anomaly%20Detection%20with%202D-3D%20Alignment&entry.906535625=Ankan%20Bhunia%20and%20Changjian%20Li%20and%20Hakan%20Bilen&entry.1292438233=%20%20Automatic%20anomaly%20detection%20based%20on%20visual%20cues%20holds%20practical%20significance%0Ain%20various%20domains%2C%20such%20as%20manufacturing%20and%20product%20quality%20assessment.%20This%0Apaper%20introduces%20a%20new%20conditional%20anomaly%20detection%20problem%2C%20which%20involves%0Aidentifying%20anomalies%20in%20a%20query%20image%20by%20comparing%20it%20to%20a%20reference%20shape.%20To%0Aaddress%20this%20challenge%2C%20we%20have%20created%20a%20large%20dataset%2C%20BrokenChairs-180K%2C%0Aconsisting%20of%20around%20180K%20images%2C%20with%20diverse%20anomalies%2C%20geometries%2C%20and%0Atextures%20paired%20with%208%2C143%20reference%203D%20shapes.%20To%20tackle%20this%20task%2C%20we%20have%0Aproposed%20a%20novel%20transformer-based%20approach%20that%20explicitly%20learns%20the%0Acorrespondence%20between%20the%20query%20image%20and%20reference%203D%20shape%20via%20feature%0Aalignment%20and%20leverages%20a%20customized%20attention%20mechanism%20for%20anomaly%20detection.%0AOur%20approach%20has%20been%20rigorously%20evaluated%20through%20comprehensive%20experiments%2C%0Aserving%20as%20a%20benchmark%20for%20future%20research%20in%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19393v1&entry.124074799=Read"},
{"title": "ProtoGMM: Multi-prototype Gaussian-Mixture-based Domain Adaptation Model\n  for Semantic Segmentation", "author": "Nazanin Moradinasab and Laura S. Shankman and Rebecca A. Deaton and Gary K. Owens and Donald E. Brown", "abstract": "  Domain adaptive semantic segmentation aims to generate accurate and dense\npredictions for an unlabeled target domain by leveraging a supervised model\ntrained on a labeled source domain. The prevalent self-training approach\ninvolves retraining the dense discriminative classifier of $p(class|pixel\nfeature)$ using the pseudo-labels from the target domain. While many methods\nfocus on mitigating the issue of noisy pseudo-labels, they often overlook the\nunderlying data distribution p(pixel feature|class) in both the source and\ntarget domains. To address this limitation, we propose the multi-prototype\nGaussian-Mixture-based (ProtoGMM) model, which incorporates the GMM into\ncontrastive losses to perform guided contrastive learning. Contrastive losses\nare commonly executed in the literature using memory banks, which can lead to\nclass biases due to underrepresented classes. Furthermore, memory banks often\nhave fixed capacities, potentially restricting the model's ability to capture\ndiverse representations of the target/source domains. An alternative approach\nis to use global class prototypes (i.e. averaged features per category).\nHowever, the global prototypes are based on the unimodal distribution\nassumption per class, disregarding within-class variation. To address these\nchallenges, we propose the ProtoGMM model. This novel approach involves\nestimating the underlying multi-prototype source distribution by utilizing the\nGMM on the feature space of the source samples. The components of the GMM model\nact as representative prototypes. To achieve increased intra-class semantic\nsimilarity, decreased inter-class similarity, and domain alignment between the\nsource and target domains, we employ multi-prototype contrastive learning\nbetween source distribution and target samples. The experiments show the\neffectiveness of our method on UDA benchmarks.\n", "link": "http://arxiv.org/abs/2406.19225v1", "date": "2024-06-27", "relevancy": 2.2865, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5858}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5801}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProtoGMM%3A%20Multi-prototype%20Gaussian-Mixture-based%20Domain%20Adaptation%20Model%0A%20%20for%20Semantic%20Segmentation&body=Title%3A%20ProtoGMM%3A%20Multi-prototype%20Gaussian-Mixture-based%20Domain%20Adaptation%20Model%0A%20%20for%20Semantic%20Segmentation%0AAuthor%3A%20Nazanin%20Moradinasab%20and%20Laura%20S.%20Shankman%20and%20Rebecca%20A.%20Deaton%20and%20Gary%20K.%20Owens%20and%20Donald%20E.%20Brown%0AAbstract%3A%20%20%20Domain%20adaptive%20semantic%20segmentation%20aims%20to%20generate%20accurate%20and%20dense%0Apredictions%20for%20an%20unlabeled%20target%20domain%20by%20leveraging%20a%20supervised%20model%0Atrained%20on%20a%20labeled%20source%20domain.%20The%20prevalent%20self-training%20approach%0Ainvolves%20retraining%20the%20dense%20discriminative%20classifier%20of%20%24p%28class%7Cpixel%0Afeature%29%24%20using%20the%20pseudo-labels%20from%20the%20target%20domain.%20While%20many%20methods%0Afocus%20on%20mitigating%20the%20issue%20of%20noisy%20pseudo-labels%2C%20they%20often%20overlook%20the%0Aunderlying%20data%20distribution%20p%28pixel%20feature%7Cclass%29%20in%20both%20the%20source%20and%0Atarget%20domains.%20To%20address%20this%20limitation%2C%20we%20propose%20the%20multi-prototype%0AGaussian-Mixture-based%20%28ProtoGMM%29%20model%2C%20which%20incorporates%20the%20GMM%20into%0Acontrastive%20losses%20to%20perform%20guided%20contrastive%20learning.%20Contrastive%20losses%0Aare%20commonly%20executed%20in%20the%20literature%20using%20memory%20banks%2C%20which%20can%20lead%20to%0Aclass%20biases%20due%20to%20underrepresented%20classes.%20Furthermore%2C%20memory%20banks%20often%0Ahave%20fixed%20capacities%2C%20potentially%20restricting%20the%20model%27s%20ability%20to%20capture%0Adiverse%20representations%20of%20the%20target/source%20domains.%20An%20alternative%20approach%0Ais%20to%20use%20global%20class%20prototypes%20%28i.e.%20averaged%20features%20per%20category%29.%0AHowever%2C%20the%20global%20prototypes%20are%20based%20on%20the%20unimodal%20distribution%0Aassumption%20per%20class%2C%20disregarding%20within-class%20variation.%20To%20address%20these%0Achallenges%2C%20we%20propose%20the%20ProtoGMM%20model.%20This%20novel%20approach%20involves%0Aestimating%20the%20underlying%20multi-prototype%20source%20distribution%20by%20utilizing%20the%0AGMM%20on%20the%20feature%20space%20of%20the%20source%20samples.%20The%20components%20of%20the%20GMM%20model%0Aact%20as%20representative%20prototypes.%20To%20achieve%20increased%20intra-class%20semantic%0Asimilarity%2C%20decreased%20inter-class%20similarity%2C%20and%20domain%20alignment%20between%20the%0Asource%20and%20target%20domains%2C%20we%20employ%20multi-prototype%20contrastive%20learning%0Abetween%20source%20distribution%20and%20target%20samples.%20The%20experiments%20show%20the%0Aeffectiveness%20of%20our%20method%20on%20UDA%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProtoGMM%253A%2520Multi-prototype%2520Gaussian-Mixture-based%2520Domain%2520Adaptation%2520Model%250A%2520%2520for%2520Semantic%2520Segmentation%26entry.906535625%3DNazanin%2520Moradinasab%2520and%2520Laura%2520S.%2520Shankman%2520and%2520Rebecca%2520A.%2520Deaton%2520and%2520Gary%2520K.%2520Owens%2520and%2520Donald%2520E.%2520Brown%26entry.1292438233%3D%2520%2520Domain%2520adaptive%2520semantic%2520segmentation%2520aims%2520to%2520generate%2520accurate%2520and%2520dense%250Apredictions%2520for%2520an%2520unlabeled%2520target%2520domain%2520by%2520leveraging%2520a%2520supervised%2520model%250Atrained%2520on%2520a%2520labeled%2520source%2520domain.%2520The%2520prevalent%2520self-training%2520approach%250Ainvolves%2520retraining%2520the%2520dense%2520discriminative%2520classifier%2520of%2520%2524p%2528class%257Cpixel%250Afeature%2529%2524%2520using%2520the%2520pseudo-labels%2520from%2520the%2520target%2520domain.%2520While%2520many%2520methods%250Afocus%2520on%2520mitigating%2520the%2520issue%2520of%2520noisy%2520pseudo-labels%252C%2520they%2520often%2520overlook%2520the%250Aunderlying%2520data%2520distribution%2520p%2528pixel%2520feature%257Cclass%2529%2520in%2520both%2520the%2520source%2520and%250Atarget%2520domains.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520the%2520multi-prototype%250AGaussian-Mixture-based%2520%2528ProtoGMM%2529%2520model%252C%2520which%2520incorporates%2520the%2520GMM%2520into%250Acontrastive%2520losses%2520to%2520perform%2520guided%2520contrastive%2520learning.%2520Contrastive%2520losses%250Aare%2520commonly%2520executed%2520in%2520the%2520literature%2520using%2520memory%2520banks%252C%2520which%2520can%2520lead%2520to%250Aclass%2520biases%2520due%2520to%2520underrepresented%2520classes.%2520Furthermore%252C%2520memory%2520banks%2520often%250Ahave%2520fixed%2520capacities%252C%2520potentially%2520restricting%2520the%2520model%2527s%2520ability%2520to%2520capture%250Adiverse%2520representations%2520of%2520the%2520target/source%2520domains.%2520An%2520alternative%2520approach%250Ais%2520to%2520use%2520global%2520class%2520prototypes%2520%2528i.e.%2520averaged%2520features%2520per%2520category%2529.%250AHowever%252C%2520the%2520global%2520prototypes%2520are%2520based%2520on%2520the%2520unimodal%2520distribution%250Aassumption%2520per%2520class%252C%2520disregarding%2520within-class%2520variation.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520the%2520ProtoGMM%2520model.%2520This%2520novel%2520approach%2520involves%250Aestimating%2520the%2520underlying%2520multi-prototype%2520source%2520distribution%2520by%2520utilizing%2520the%250AGMM%2520on%2520the%2520feature%2520space%2520of%2520the%2520source%2520samples.%2520The%2520components%2520of%2520the%2520GMM%2520model%250Aact%2520as%2520representative%2520prototypes.%2520To%2520achieve%2520increased%2520intra-class%2520semantic%250Asimilarity%252C%2520decreased%2520inter-class%2520similarity%252C%2520and%2520domain%2520alignment%2520between%2520the%250Asource%2520and%2520target%2520domains%252C%2520we%2520employ%2520multi-prototype%2520contrastive%2520learning%250Abetween%2520source%2520distribution%2520and%2520target%2520samples.%2520The%2520experiments%2520show%2520the%250Aeffectiveness%2520of%2520our%2520method%2520on%2520UDA%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProtoGMM%3A%20Multi-prototype%20Gaussian-Mixture-based%20Domain%20Adaptation%20Model%0A%20%20for%20Semantic%20Segmentation&entry.906535625=Nazanin%20Moradinasab%20and%20Laura%20S.%20Shankman%20and%20Rebecca%20A.%20Deaton%20and%20Gary%20K.%20Owens%20and%20Donald%20E.%20Brown&entry.1292438233=%20%20Domain%20adaptive%20semantic%20segmentation%20aims%20to%20generate%20accurate%20and%20dense%0Apredictions%20for%20an%20unlabeled%20target%20domain%20by%20leveraging%20a%20supervised%20model%0Atrained%20on%20a%20labeled%20source%20domain.%20The%20prevalent%20self-training%20approach%0Ainvolves%20retraining%20the%20dense%20discriminative%20classifier%20of%20%24p%28class%7Cpixel%0Afeature%29%24%20using%20the%20pseudo-labels%20from%20the%20target%20domain.%20While%20many%20methods%0Afocus%20on%20mitigating%20the%20issue%20of%20noisy%20pseudo-labels%2C%20they%20often%20overlook%20the%0Aunderlying%20data%20distribution%20p%28pixel%20feature%7Cclass%29%20in%20both%20the%20source%20and%0Atarget%20domains.%20To%20address%20this%20limitation%2C%20we%20propose%20the%20multi-prototype%0AGaussian-Mixture-based%20%28ProtoGMM%29%20model%2C%20which%20incorporates%20the%20GMM%20into%0Acontrastive%20losses%20to%20perform%20guided%20contrastive%20learning.%20Contrastive%20losses%0Aare%20commonly%20executed%20in%20the%20literature%20using%20memory%20banks%2C%20which%20can%20lead%20to%0Aclass%20biases%20due%20to%20underrepresented%20classes.%20Furthermore%2C%20memory%20banks%20often%0Ahave%20fixed%20capacities%2C%20potentially%20restricting%20the%20model%27s%20ability%20to%20capture%0Adiverse%20representations%20of%20the%20target/source%20domains.%20An%20alternative%20approach%0Ais%20to%20use%20global%20class%20prototypes%20%28i.e.%20averaged%20features%20per%20category%29.%0AHowever%2C%20the%20global%20prototypes%20are%20based%20on%20the%20unimodal%20distribution%0Aassumption%20per%20class%2C%20disregarding%20within-class%20variation.%20To%20address%20these%0Achallenges%2C%20we%20propose%20the%20ProtoGMM%20model.%20This%20novel%20approach%20involves%0Aestimating%20the%20underlying%20multi-prototype%20source%20distribution%20by%20utilizing%20the%0AGMM%20on%20the%20feature%20space%20of%20the%20source%20samples.%20The%20components%20of%20the%20GMM%20model%0Aact%20as%20representative%20prototypes.%20To%20achieve%20increased%20intra-class%20semantic%0Asimilarity%2C%20decreased%20inter-class%20similarity%2C%20and%20domain%20alignment%20between%20the%0Asource%20and%20target%20domains%2C%20we%20employ%20multi-prototype%20contrastive%20learning%0Abetween%20source%20distribution%20and%20target%20samples.%20The%20experiments%20show%20the%0Aeffectiveness%20of%20our%20method%20on%20UDA%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19225v1&entry.124074799=Read"},
{"title": "CORE4D: A 4D Human-Object-Human Interaction Dataset for Collaborative\n  Object REarrangement", "author": "Chengwen Zhang and Yun Liu and Ruofan Xing and Bingda Tang and Li Yi", "abstract": "  Understanding how humans cooperatively rearrange household objects is\ncritical for VR/AR and human-robot interaction. However, in-depth studies on\nmodeling these behaviors are under-researched due to the lack of relevant\ndatasets. We fill this gap by presenting CORE4D, a novel large-scale 4D\nhuman-object-human interaction dataset focusing on collaborative object\nrearrangement, which encompasses diverse compositions of various object\ngeometries, collaboration modes, and 3D scenes. With 1K human-object-human\nmotion sequences captured in the real world, we enrich CORE4D by contributing\nan iterative collaboration retargeting strategy to augment motions to a variety\nof novel objects. Leveraging this approach, CORE4D comprises a total of 11K\ncollaboration sequences spanning 3K real and virtual object shapes. Benefiting\nfrom extensive motion patterns provided by CORE4D, we benchmark two tasks\naiming at generating human-object interaction: human-object motion forecasting\nand interaction synthesis. Extensive experiments demonstrate the effectiveness\nof our collaboration retargeting strategy and indicate that CORE4D has posed\nnew challenges to existing human-object interaction generation methodologies.\nOur dataset and code are available at\nhttps://github.com/leolyliu/CORE4D-Instructions.\n", "link": "http://arxiv.org/abs/2406.19353v1", "date": "2024-06-27", "relevancy": 2.2732, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.587}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5692}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CORE4D%3A%20A%204D%20Human-Object-Human%20Interaction%20Dataset%20for%20Collaborative%0A%20%20Object%20REarrangement&body=Title%3A%20CORE4D%3A%20A%204D%20Human-Object-Human%20Interaction%20Dataset%20for%20Collaborative%0A%20%20Object%20REarrangement%0AAuthor%3A%20Chengwen%20Zhang%20and%20Yun%20Liu%20and%20Ruofan%20Xing%20and%20Bingda%20Tang%20and%20Li%20Yi%0AAbstract%3A%20%20%20Understanding%20how%20humans%20cooperatively%20rearrange%20household%20objects%20is%0Acritical%20for%20VR/AR%20and%20human-robot%20interaction.%20However%2C%20in-depth%20studies%20on%0Amodeling%20these%20behaviors%20are%20under-researched%20due%20to%20the%20lack%20of%20relevant%0Adatasets.%20We%20fill%20this%20gap%20by%20presenting%20CORE4D%2C%20a%20novel%20large-scale%204D%0Ahuman-object-human%20interaction%20dataset%20focusing%20on%20collaborative%20object%0Arearrangement%2C%20which%20encompasses%20diverse%20compositions%20of%20various%20object%0Ageometries%2C%20collaboration%20modes%2C%20and%203D%20scenes.%20With%201K%20human-object-human%0Amotion%20sequences%20captured%20in%20the%20real%20world%2C%20we%20enrich%20CORE4D%20by%20contributing%0Aan%20iterative%20collaboration%20retargeting%20strategy%20to%20augment%20motions%20to%20a%20variety%0Aof%20novel%20objects.%20Leveraging%20this%20approach%2C%20CORE4D%20comprises%20a%20total%20of%2011K%0Acollaboration%20sequences%20spanning%203K%20real%20and%20virtual%20object%20shapes.%20Benefiting%0Afrom%20extensive%20motion%20patterns%20provided%20by%20CORE4D%2C%20we%20benchmark%20two%20tasks%0Aaiming%20at%20generating%20human-object%20interaction%3A%20human-object%20motion%20forecasting%0Aand%20interaction%20synthesis.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%0Aof%20our%20collaboration%20retargeting%20strategy%20and%20indicate%20that%20CORE4D%20has%20posed%0Anew%20challenges%20to%20existing%20human-object%20interaction%20generation%20methodologies.%0AOur%20dataset%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/leolyliu/CORE4D-Instructions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19353v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCORE4D%253A%2520A%25204D%2520Human-Object-Human%2520Interaction%2520Dataset%2520for%2520Collaborative%250A%2520%2520Object%2520REarrangement%26entry.906535625%3DChengwen%2520Zhang%2520and%2520Yun%2520Liu%2520and%2520Ruofan%2520Xing%2520and%2520Bingda%2520Tang%2520and%2520Li%2520Yi%26entry.1292438233%3D%2520%2520Understanding%2520how%2520humans%2520cooperatively%2520rearrange%2520household%2520objects%2520is%250Acritical%2520for%2520VR/AR%2520and%2520human-robot%2520interaction.%2520However%252C%2520in-depth%2520studies%2520on%250Amodeling%2520these%2520behaviors%2520are%2520under-researched%2520due%2520to%2520the%2520lack%2520of%2520relevant%250Adatasets.%2520We%2520fill%2520this%2520gap%2520by%2520presenting%2520CORE4D%252C%2520a%2520novel%2520large-scale%25204D%250Ahuman-object-human%2520interaction%2520dataset%2520focusing%2520on%2520collaborative%2520object%250Arearrangement%252C%2520which%2520encompasses%2520diverse%2520compositions%2520of%2520various%2520object%250Ageometries%252C%2520collaboration%2520modes%252C%2520and%25203D%2520scenes.%2520With%25201K%2520human-object-human%250Amotion%2520sequences%2520captured%2520in%2520the%2520real%2520world%252C%2520we%2520enrich%2520CORE4D%2520by%2520contributing%250Aan%2520iterative%2520collaboration%2520retargeting%2520strategy%2520to%2520augment%2520motions%2520to%2520a%2520variety%250Aof%2520novel%2520objects.%2520Leveraging%2520this%2520approach%252C%2520CORE4D%2520comprises%2520a%2520total%2520of%252011K%250Acollaboration%2520sequences%2520spanning%25203K%2520real%2520and%2520virtual%2520object%2520shapes.%2520Benefiting%250Afrom%2520extensive%2520motion%2520patterns%2520provided%2520by%2520CORE4D%252C%2520we%2520benchmark%2520two%2520tasks%250Aaiming%2520at%2520generating%2520human-object%2520interaction%253A%2520human-object%2520motion%2520forecasting%250Aand%2520interaction%2520synthesis.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%250Aof%2520our%2520collaboration%2520retargeting%2520strategy%2520and%2520indicate%2520that%2520CORE4D%2520has%2520posed%250Anew%2520challenges%2520to%2520existing%2520human-object%2520interaction%2520generation%2520methodologies.%250AOur%2520dataset%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/leolyliu/CORE4D-Instructions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19353v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CORE4D%3A%20A%204D%20Human-Object-Human%20Interaction%20Dataset%20for%20Collaborative%0A%20%20Object%20REarrangement&entry.906535625=Chengwen%20Zhang%20and%20Yun%20Liu%20and%20Ruofan%20Xing%20and%20Bingda%20Tang%20and%20Li%20Yi&entry.1292438233=%20%20Understanding%20how%20humans%20cooperatively%20rearrange%20household%20objects%20is%0Acritical%20for%20VR/AR%20and%20human-robot%20interaction.%20However%2C%20in-depth%20studies%20on%0Amodeling%20these%20behaviors%20are%20under-researched%20due%20to%20the%20lack%20of%20relevant%0Adatasets.%20We%20fill%20this%20gap%20by%20presenting%20CORE4D%2C%20a%20novel%20large-scale%204D%0Ahuman-object-human%20interaction%20dataset%20focusing%20on%20collaborative%20object%0Arearrangement%2C%20which%20encompasses%20diverse%20compositions%20of%20various%20object%0Ageometries%2C%20collaboration%20modes%2C%20and%203D%20scenes.%20With%201K%20human-object-human%0Amotion%20sequences%20captured%20in%20the%20real%20world%2C%20we%20enrich%20CORE4D%20by%20contributing%0Aan%20iterative%20collaboration%20retargeting%20strategy%20to%20augment%20motions%20to%20a%20variety%0Aof%20novel%20objects.%20Leveraging%20this%20approach%2C%20CORE4D%20comprises%20a%20total%20of%2011K%0Acollaboration%20sequences%20spanning%203K%20real%20and%20virtual%20object%20shapes.%20Benefiting%0Afrom%20extensive%20motion%20patterns%20provided%20by%20CORE4D%2C%20we%20benchmark%20two%20tasks%0Aaiming%20at%20generating%20human-object%20interaction%3A%20human-object%20motion%20forecasting%0Aand%20interaction%20synthesis.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%0Aof%20our%20collaboration%20retargeting%20strategy%20and%20indicate%20that%20CORE4D%20has%20posed%0Anew%20challenges%20to%20existing%20human-object%20interaction%20generation%20methodologies.%0AOur%20dataset%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/leolyliu/CORE4D-Instructions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19353v1&entry.124074799=Read"},
{"title": "SpatialBot: Precise Spatial Understanding with Vision Language Models", "author": "Wenxiao Cai and Yaroslav Ponomarenko and Jianhao Yuan and Xiaoqi Li and Wankou Yang and Hao Dong and Bo Zhao", "abstract": "  Vision Language Models (VLMs) have achieved impressive performance in 2D\nimage understanding, however they are still struggling with spatial\nunderstanding which is the foundation of Embodied AI. In this paper, we propose\nSpatialBot for better spatial understanding by feeding both RGB and depth\nimages. Additionally, we have constructed the SpatialQA dataset, which involves\nmulti-level depth-related questions to train VLMs for depth understanding.\nFinally, we present SpatialBench to comprehensively evaluate VLMs' capabilities\nin spatial understanding at different levels. Extensive experiments on our\nspatial-understanding benchmark, general VLM benchmarks and Embodied AI tasks,\ndemonstrate the remarkable improvements of SpatialBot trained on SpatialQA. The\nmodel, code and data are available at https://github.com/BAAI-DCAI/SpatialBot.\n", "link": "http://arxiv.org/abs/2406.13642v2", "date": "2024-06-27", "relevancy": 2.2452, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5747}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5593}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpatialBot%3A%20Precise%20Spatial%20Understanding%20with%20Vision%20Language%20Models&body=Title%3A%20SpatialBot%3A%20Precise%20Spatial%20Understanding%20with%20Vision%20Language%20Models%0AAuthor%3A%20Wenxiao%20Cai%20and%20Yaroslav%20Ponomarenko%20and%20Jianhao%20Yuan%20and%20Xiaoqi%20Li%20and%20Wankou%20Yang%20and%20Hao%20Dong%20and%20Bo%20Zhao%0AAbstract%3A%20%20%20Vision%20Language%20Models%20%28VLMs%29%20have%20achieved%20impressive%20performance%20in%202D%0Aimage%20understanding%2C%20however%20they%20are%20still%20struggling%20with%20spatial%0Aunderstanding%20which%20is%20the%20foundation%20of%20Embodied%20AI.%20In%20this%20paper%2C%20we%20propose%0ASpatialBot%20for%20better%20spatial%20understanding%20by%20feeding%20both%20RGB%20and%20depth%0Aimages.%20Additionally%2C%20we%20have%20constructed%20the%20SpatialQA%20dataset%2C%20which%20involves%0Amulti-level%20depth-related%20questions%20to%20train%20VLMs%20for%20depth%20understanding.%0AFinally%2C%20we%20present%20SpatialBench%20to%20comprehensively%20evaluate%20VLMs%27%20capabilities%0Ain%20spatial%20understanding%20at%20different%20levels.%20Extensive%20experiments%20on%20our%0Aspatial-understanding%20benchmark%2C%20general%20VLM%20benchmarks%20and%20Embodied%20AI%20tasks%2C%0Ademonstrate%20the%20remarkable%20improvements%20of%20SpatialBot%20trained%20on%20SpatialQA.%20The%0Amodel%2C%20code%20and%20data%20are%20available%20at%20https%3A//github.com/BAAI-DCAI/SpatialBot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13642v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatialBot%253A%2520Precise%2520Spatial%2520Understanding%2520with%2520Vision%2520Language%2520Models%26entry.906535625%3DWenxiao%2520Cai%2520and%2520Yaroslav%2520Ponomarenko%2520and%2520Jianhao%2520Yuan%2520and%2520Xiaoqi%2520Li%2520and%2520Wankou%2520Yang%2520and%2520Hao%2520Dong%2520and%2520Bo%2520Zhao%26entry.1292438233%3D%2520%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520achieved%2520impressive%2520performance%2520in%25202D%250Aimage%2520understanding%252C%2520however%2520they%2520are%2520still%2520struggling%2520with%2520spatial%250Aunderstanding%2520which%2520is%2520the%2520foundation%2520of%2520Embodied%2520AI.%2520In%2520this%2520paper%252C%2520we%2520propose%250ASpatialBot%2520for%2520better%2520spatial%2520understanding%2520by%2520feeding%2520both%2520RGB%2520and%2520depth%250Aimages.%2520Additionally%252C%2520we%2520have%2520constructed%2520the%2520SpatialQA%2520dataset%252C%2520which%2520involves%250Amulti-level%2520depth-related%2520questions%2520to%2520train%2520VLMs%2520for%2520depth%2520understanding.%250AFinally%252C%2520we%2520present%2520SpatialBench%2520to%2520comprehensively%2520evaluate%2520VLMs%2527%2520capabilities%250Ain%2520spatial%2520understanding%2520at%2520different%2520levels.%2520Extensive%2520experiments%2520on%2520our%250Aspatial-understanding%2520benchmark%252C%2520general%2520VLM%2520benchmarks%2520and%2520Embodied%2520AI%2520tasks%252C%250Ademonstrate%2520the%2520remarkable%2520improvements%2520of%2520SpatialBot%2520trained%2520on%2520SpatialQA.%2520The%250Amodel%252C%2520code%2520and%2520data%2520are%2520available%2520at%2520https%253A//github.com/BAAI-DCAI/SpatialBot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13642v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpatialBot%3A%20Precise%20Spatial%20Understanding%20with%20Vision%20Language%20Models&entry.906535625=Wenxiao%20Cai%20and%20Yaroslav%20Ponomarenko%20and%20Jianhao%20Yuan%20and%20Xiaoqi%20Li%20and%20Wankou%20Yang%20and%20Hao%20Dong%20and%20Bo%20Zhao&entry.1292438233=%20%20Vision%20Language%20Models%20%28VLMs%29%20have%20achieved%20impressive%20performance%20in%202D%0Aimage%20understanding%2C%20however%20they%20are%20still%20struggling%20with%20spatial%0Aunderstanding%20which%20is%20the%20foundation%20of%20Embodied%20AI.%20In%20this%20paper%2C%20we%20propose%0ASpatialBot%20for%20better%20spatial%20understanding%20by%20feeding%20both%20RGB%20and%20depth%0Aimages.%20Additionally%2C%20we%20have%20constructed%20the%20SpatialQA%20dataset%2C%20which%20involves%0Amulti-level%20depth-related%20questions%20to%20train%20VLMs%20for%20depth%20understanding.%0AFinally%2C%20we%20present%20SpatialBench%20to%20comprehensively%20evaluate%20VLMs%27%20capabilities%0Ain%20spatial%20understanding%20at%20different%20levels.%20Extensive%20experiments%20on%20our%0Aspatial-understanding%20benchmark%2C%20general%20VLM%20benchmarks%20and%20Embodied%20AI%20tasks%2C%0Ademonstrate%20the%20remarkable%20improvements%20of%20SpatialBot%20trained%20on%20SpatialQA.%20The%0Amodel%2C%20code%20and%20data%20are%20available%20at%20https%3A//github.com/BAAI-DCAI/SpatialBot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13642v2&entry.124074799=Read"},
{"title": "Learning Visual Conditioning Tokens to Correct Domain Shift for Fully\n  Test-time Adaptation", "author": "Yushun Tang and Shuoshuo Chen and Zhehan Kan and Yi Zhang and Qinghai Guo and Zhihai He", "abstract": "  Fully test-time adaptation aims to adapt the network model based on\nsequential analysis of input samples during the inference stage to address the\ncross-domain performance degradation problem of deep neural networks. This work\nis based on the following interesting finding: in transformer-based image\nclassification, the class token at the first transformer encoder layer can be\nlearned to capture the domain-specific characteristics of target samples during\ntest-time adaptation. This learned token, when combined with input image patch\nembeddings, is able to gradually remove the domain-specific information from\nthe feature representations of input samples during the transformer encoding\nprocess, thereby significantly improving the test-time adaptation performance\nof the source model across different domains. We refer to this class token as\nvisual conditioning token (VCT). To successfully learn the VCT, we propose a\nbi-level learning approach to capture the long-term variations of\ndomain-specific characteristics while accommodating local variations of\ninstance-specific characteristics. Experimental results on the benchmark\ndatasets demonstrate that our proposed bi-level visual conditioning token\nlearning method is able to achieve significantly improved test-time adaptation\nperformance by up to 1.9%.\n", "link": "http://arxiv.org/abs/2406.19341v1", "date": "2024-06-27", "relevancy": 2.24, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5673}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5649}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Visual%20Conditioning%20Tokens%20to%20Correct%20Domain%20Shift%20for%20Fully%0A%20%20Test-time%20Adaptation&body=Title%3A%20Learning%20Visual%20Conditioning%20Tokens%20to%20Correct%20Domain%20Shift%20for%20Fully%0A%20%20Test-time%20Adaptation%0AAuthor%3A%20Yushun%20Tang%20and%20Shuoshuo%20Chen%20and%20Zhehan%20Kan%20and%20Yi%20Zhang%20and%20Qinghai%20Guo%20and%20Zhihai%20He%0AAbstract%3A%20%20%20Fully%20test-time%20adaptation%20aims%20to%20adapt%20the%20network%20model%20based%20on%0Asequential%20analysis%20of%20input%20samples%20during%20the%20inference%20stage%20to%20address%20the%0Across-domain%20performance%20degradation%20problem%20of%20deep%20neural%20networks.%20This%20work%0Ais%20based%20on%20the%20following%20interesting%20finding%3A%20in%20transformer-based%20image%0Aclassification%2C%20the%20class%20token%20at%20the%20first%20transformer%20encoder%20layer%20can%20be%0Alearned%20to%20capture%20the%20domain-specific%20characteristics%20of%20target%20samples%20during%0Atest-time%20adaptation.%20This%20learned%20token%2C%20when%20combined%20with%20input%20image%20patch%0Aembeddings%2C%20is%20able%20to%20gradually%20remove%20the%20domain-specific%20information%20from%0Athe%20feature%20representations%20of%20input%20samples%20during%20the%20transformer%20encoding%0Aprocess%2C%20thereby%20significantly%20improving%20the%20test-time%20adaptation%20performance%0Aof%20the%20source%20model%20across%20different%20domains.%20We%20refer%20to%20this%20class%20token%20as%0Avisual%20conditioning%20token%20%28VCT%29.%20To%20successfully%20learn%20the%20VCT%2C%20we%20propose%20a%0Abi-level%20learning%20approach%20to%20capture%20the%20long-term%20variations%20of%0Adomain-specific%20characteristics%20while%20accommodating%20local%20variations%20of%0Ainstance-specific%20characteristics.%20Experimental%20results%20on%20the%20benchmark%0Adatasets%20demonstrate%20that%20our%20proposed%20bi-level%20visual%20conditioning%20token%0Alearning%20method%20is%20able%20to%20achieve%20significantly%20improved%20test-time%20adaptation%0Aperformance%20by%20up%20to%201.9%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19341v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Visual%2520Conditioning%2520Tokens%2520to%2520Correct%2520Domain%2520Shift%2520for%2520Fully%250A%2520%2520Test-time%2520Adaptation%26entry.906535625%3DYushun%2520Tang%2520and%2520Shuoshuo%2520Chen%2520and%2520Zhehan%2520Kan%2520and%2520Yi%2520Zhang%2520and%2520Qinghai%2520Guo%2520and%2520Zhihai%2520He%26entry.1292438233%3D%2520%2520Fully%2520test-time%2520adaptation%2520aims%2520to%2520adapt%2520the%2520network%2520model%2520based%2520on%250Asequential%2520analysis%2520of%2520input%2520samples%2520during%2520the%2520inference%2520stage%2520to%2520address%2520the%250Across-domain%2520performance%2520degradation%2520problem%2520of%2520deep%2520neural%2520networks.%2520This%2520work%250Ais%2520based%2520on%2520the%2520following%2520interesting%2520finding%253A%2520in%2520transformer-based%2520image%250Aclassification%252C%2520the%2520class%2520token%2520at%2520the%2520first%2520transformer%2520encoder%2520layer%2520can%2520be%250Alearned%2520to%2520capture%2520the%2520domain-specific%2520characteristics%2520of%2520target%2520samples%2520during%250Atest-time%2520adaptation.%2520This%2520learned%2520token%252C%2520when%2520combined%2520with%2520input%2520image%2520patch%250Aembeddings%252C%2520is%2520able%2520to%2520gradually%2520remove%2520the%2520domain-specific%2520information%2520from%250Athe%2520feature%2520representations%2520of%2520input%2520samples%2520during%2520the%2520transformer%2520encoding%250Aprocess%252C%2520thereby%2520significantly%2520improving%2520the%2520test-time%2520adaptation%2520performance%250Aof%2520the%2520source%2520model%2520across%2520different%2520domains.%2520We%2520refer%2520to%2520this%2520class%2520token%2520as%250Avisual%2520conditioning%2520token%2520%2528VCT%2529.%2520To%2520successfully%2520learn%2520the%2520VCT%252C%2520we%2520propose%2520a%250Abi-level%2520learning%2520approach%2520to%2520capture%2520the%2520long-term%2520variations%2520of%250Adomain-specific%2520characteristics%2520while%2520accommodating%2520local%2520variations%2520of%250Ainstance-specific%2520characteristics.%2520Experimental%2520results%2520on%2520the%2520benchmark%250Adatasets%2520demonstrate%2520that%2520our%2520proposed%2520bi-level%2520visual%2520conditioning%2520token%250Alearning%2520method%2520is%2520able%2520to%2520achieve%2520significantly%2520improved%2520test-time%2520adaptation%250Aperformance%2520by%2520up%2520to%25201.9%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19341v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Visual%20Conditioning%20Tokens%20to%20Correct%20Domain%20Shift%20for%20Fully%0A%20%20Test-time%20Adaptation&entry.906535625=Yushun%20Tang%20and%20Shuoshuo%20Chen%20and%20Zhehan%20Kan%20and%20Yi%20Zhang%20and%20Qinghai%20Guo%20and%20Zhihai%20He&entry.1292438233=%20%20Fully%20test-time%20adaptation%20aims%20to%20adapt%20the%20network%20model%20based%20on%0Asequential%20analysis%20of%20input%20samples%20during%20the%20inference%20stage%20to%20address%20the%0Across-domain%20performance%20degradation%20problem%20of%20deep%20neural%20networks.%20This%20work%0Ais%20based%20on%20the%20following%20interesting%20finding%3A%20in%20transformer-based%20image%0Aclassification%2C%20the%20class%20token%20at%20the%20first%20transformer%20encoder%20layer%20can%20be%0Alearned%20to%20capture%20the%20domain-specific%20characteristics%20of%20target%20samples%20during%0Atest-time%20adaptation.%20This%20learned%20token%2C%20when%20combined%20with%20input%20image%20patch%0Aembeddings%2C%20is%20able%20to%20gradually%20remove%20the%20domain-specific%20information%20from%0Athe%20feature%20representations%20of%20input%20samples%20during%20the%20transformer%20encoding%0Aprocess%2C%20thereby%20significantly%20improving%20the%20test-time%20adaptation%20performance%0Aof%20the%20source%20model%20across%20different%20domains.%20We%20refer%20to%20this%20class%20token%20as%0Avisual%20conditioning%20token%20%28VCT%29.%20To%20successfully%20learn%20the%20VCT%2C%20we%20propose%20a%0Abi-level%20learning%20approach%20to%20capture%20the%20long-term%20variations%20of%0Adomain-specific%20characteristics%20while%20accommodating%20local%20variations%20of%0Ainstance-specific%20characteristics.%20Experimental%20results%20on%20the%20benchmark%0Adatasets%20demonstrate%20that%20our%20proposed%20bi-level%20visual%20conditioning%20token%0Alearning%20method%20is%20able%20to%20achieve%20significantly%20improved%20test-time%20adaptation%0Aperformance%20by%20up%20to%201.9%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19341v1&entry.124074799=Read"},
{"title": "Taming Data and Transformers for Audio Generation", "author": "Moayed Haji-Ali and Willi Menapace and Aliaksandr Siarohin and Guha Balakrishnan and Sergey Tulyakov and Vicente Ordonez", "abstract": "  Generating ambient sounds and effects is a challenging problem due to data\nscarcity and often insufficient caption quality, making it difficult to employ\nlarge-scale generative models for the task. In this work, we tackle the problem\nby introducing two new models. First, we propose AutoCap, a high-quality and\nefficient automatic audio captioning model. We show that by leveraging metadata\navailable with the audio modality, we can substantially improve the quality of\ncaptions. AutoCap reaches CIDEr score of 83.2, marking a 3.2% improvement from\nthe best available captioning model at four times faster inference speed. We\nthen use AutoCap to caption clips from existing datasets, obtaining 761,000\naudio clips with high-quality captions, forming the largest available\naudio-text dataset. Second, we propose GenAu, a scalable transformer-based\naudio generation architecture that we scale up to 1.25B parameters and train\nwith our new dataset. When compared to state-of-the-art audio generators, GenAu\nobtains significant improvements of 15.7% in FAD score, 22.7% in IS, and 13.5%\nin CLAP score, indicating significantly improved quality of generated audio\ncompared to previous works. This shows that the quality of data is often as\nimportant as its quantity. Besides, since AutoCap is fully automatic, new audio\nsamples can be added to the training dataset, unlocking the training of even\nlarger generative models for audio synthesis.\n", "link": "http://arxiv.org/abs/2406.19388v1", "date": "2024-06-27", "relevancy": 2.2358, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5954}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5679}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taming%20Data%20and%20Transformers%20for%20Audio%20Generation&body=Title%3A%20Taming%20Data%20and%20Transformers%20for%20Audio%20Generation%0AAuthor%3A%20Moayed%20Haji-Ali%20and%20Willi%20Menapace%20and%20Aliaksandr%20Siarohin%20and%20Guha%20Balakrishnan%20and%20Sergey%20Tulyakov%20and%20Vicente%20Ordonez%0AAbstract%3A%20%20%20Generating%20ambient%20sounds%20and%20effects%20is%20a%20challenging%20problem%20due%20to%20data%0Ascarcity%20and%20often%20insufficient%20caption%20quality%2C%20making%20it%20difficult%20to%20employ%0Alarge-scale%20generative%20models%20for%20the%20task.%20In%20this%20work%2C%20we%20tackle%20the%20problem%0Aby%20introducing%20two%20new%20models.%20First%2C%20we%20propose%20AutoCap%2C%20a%20high-quality%20and%0Aefficient%20automatic%20audio%20captioning%20model.%20We%20show%20that%20by%20leveraging%20metadata%0Aavailable%20with%20the%20audio%20modality%2C%20we%20can%20substantially%20improve%20the%20quality%20of%0Acaptions.%20AutoCap%20reaches%20CIDEr%20score%20of%2083.2%2C%20marking%20a%203.2%25%20improvement%20from%0Athe%20best%20available%20captioning%20model%20at%20four%20times%20faster%20inference%20speed.%20We%0Athen%20use%20AutoCap%20to%20caption%20clips%20from%20existing%20datasets%2C%20obtaining%20761%2C000%0Aaudio%20clips%20with%20high-quality%20captions%2C%20forming%20the%20largest%20available%0Aaudio-text%20dataset.%20Second%2C%20we%20propose%20GenAu%2C%20a%20scalable%20transformer-based%0Aaudio%20generation%20architecture%20that%20we%20scale%20up%20to%201.25B%20parameters%20and%20train%0Awith%20our%20new%20dataset.%20When%20compared%20to%20state-of-the-art%20audio%20generators%2C%20GenAu%0Aobtains%20significant%20improvements%20of%2015.7%25%20in%20FAD%20score%2C%2022.7%25%20in%20IS%2C%20and%2013.5%25%0Ain%20CLAP%20score%2C%20indicating%20significantly%20improved%20quality%20of%20generated%20audio%0Acompared%20to%20previous%20works.%20This%20shows%20that%20the%20quality%20of%20data%20is%20often%20as%0Aimportant%20as%20its%20quantity.%20Besides%2C%20since%20AutoCap%20is%20fully%20automatic%2C%20new%20audio%0Asamples%20can%20be%20added%20to%20the%20training%20dataset%2C%20unlocking%20the%20training%20of%20even%0Alarger%20generative%20models%20for%20audio%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19388v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaming%2520Data%2520and%2520Transformers%2520for%2520Audio%2520Generation%26entry.906535625%3DMoayed%2520Haji-Ali%2520and%2520Willi%2520Menapace%2520and%2520Aliaksandr%2520Siarohin%2520and%2520Guha%2520Balakrishnan%2520and%2520Sergey%2520Tulyakov%2520and%2520Vicente%2520Ordonez%26entry.1292438233%3D%2520%2520Generating%2520ambient%2520sounds%2520and%2520effects%2520is%2520a%2520challenging%2520problem%2520due%2520to%2520data%250Ascarcity%2520and%2520often%2520insufficient%2520caption%2520quality%252C%2520making%2520it%2520difficult%2520to%2520employ%250Alarge-scale%2520generative%2520models%2520for%2520the%2520task.%2520In%2520this%2520work%252C%2520we%2520tackle%2520the%2520problem%250Aby%2520introducing%2520two%2520new%2520models.%2520First%252C%2520we%2520propose%2520AutoCap%252C%2520a%2520high-quality%2520and%250Aefficient%2520automatic%2520audio%2520captioning%2520model.%2520We%2520show%2520that%2520by%2520leveraging%2520metadata%250Aavailable%2520with%2520the%2520audio%2520modality%252C%2520we%2520can%2520substantially%2520improve%2520the%2520quality%2520of%250Acaptions.%2520AutoCap%2520reaches%2520CIDEr%2520score%2520of%252083.2%252C%2520marking%2520a%25203.2%2525%2520improvement%2520from%250Athe%2520best%2520available%2520captioning%2520model%2520at%2520four%2520times%2520faster%2520inference%2520speed.%2520We%250Athen%2520use%2520AutoCap%2520to%2520caption%2520clips%2520from%2520existing%2520datasets%252C%2520obtaining%2520761%252C000%250Aaudio%2520clips%2520with%2520high-quality%2520captions%252C%2520forming%2520the%2520largest%2520available%250Aaudio-text%2520dataset.%2520Second%252C%2520we%2520propose%2520GenAu%252C%2520a%2520scalable%2520transformer-based%250Aaudio%2520generation%2520architecture%2520that%2520we%2520scale%2520up%2520to%25201.25B%2520parameters%2520and%2520train%250Awith%2520our%2520new%2520dataset.%2520When%2520compared%2520to%2520state-of-the-art%2520audio%2520generators%252C%2520GenAu%250Aobtains%2520significant%2520improvements%2520of%252015.7%2525%2520in%2520FAD%2520score%252C%252022.7%2525%2520in%2520IS%252C%2520and%252013.5%2525%250Ain%2520CLAP%2520score%252C%2520indicating%2520significantly%2520improved%2520quality%2520of%2520generated%2520audio%250Acompared%2520to%2520previous%2520works.%2520This%2520shows%2520that%2520the%2520quality%2520of%2520data%2520is%2520often%2520as%250Aimportant%2520as%2520its%2520quantity.%2520Besides%252C%2520since%2520AutoCap%2520is%2520fully%2520automatic%252C%2520new%2520audio%250Asamples%2520can%2520be%2520added%2520to%2520the%2520training%2520dataset%252C%2520unlocking%2520the%2520training%2520of%2520even%250Alarger%2520generative%2520models%2520for%2520audio%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19388v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taming%20Data%20and%20Transformers%20for%20Audio%20Generation&entry.906535625=Moayed%20Haji-Ali%20and%20Willi%20Menapace%20and%20Aliaksandr%20Siarohin%20and%20Guha%20Balakrishnan%20and%20Sergey%20Tulyakov%20and%20Vicente%20Ordonez&entry.1292438233=%20%20Generating%20ambient%20sounds%20and%20effects%20is%20a%20challenging%20problem%20due%20to%20data%0Ascarcity%20and%20often%20insufficient%20caption%20quality%2C%20making%20it%20difficult%20to%20employ%0Alarge-scale%20generative%20models%20for%20the%20task.%20In%20this%20work%2C%20we%20tackle%20the%20problem%0Aby%20introducing%20two%20new%20models.%20First%2C%20we%20propose%20AutoCap%2C%20a%20high-quality%20and%0Aefficient%20automatic%20audio%20captioning%20model.%20We%20show%20that%20by%20leveraging%20metadata%0Aavailable%20with%20the%20audio%20modality%2C%20we%20can%20substantially%20improve%20the%20quality%20of%0Acaptions.%20AutoCap%20reaches%20CIDEr%20score%20of%2083.2%2C%20marking%20a%203.2%25%20improvement%20from%0Athe%20best%20available%20captioning%20model%20at%20four%20times%20faster%20inference%20speed.%20We%0Athen%20use%20AutoCap%20to%20caption%20clips%20from%20existing%20datasets%2C%20obtaining%20761%2C000%0Aaudio%20clips%20with%20high-quality%20captions%2C%20forming%20the%20largest%20available%0Aaudio-text%20dataset.%20Second%2C%20we%20propose%20GenAu%2C%20a%20scalable%20transformer-based%0Aaudio%20generation%20architecture%20that%20we%20scale%20up%20to%201.25B%20parameters%20and%20train%0Awith%20our%20new%20dataset.%20When%20compared%20to%20state-of-the-art%20audio%20generators%2C%20GenAu%0Aobtains%20significant%20improvements%20of%2015.7%25%20in%20FAD%20score%2C%2022.7%25%20in%20IS%2C%20and%2013.5%25%0Ain%20CLAP%20score%2C%20indicating%20significantly%20improved%20quality%20of%20generated%20audio%0Acompared%20to%20previous%20works.%20This%20shows%20that%20the%20quality%20of%20data%20is%20often%20as%0Aimportant%20as%20its%20quantity.%20Besides%2C%20since%20AutoCap%20is%20fully%20automatic%2C%20new%20audio%0Asamples%20can%20be%20added%20to%20the%20training%20dataset%2C%20unlocking%20the%20training%20of%20even%0Alarger%20generative%20models%20for%20audio%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19388v1&entry.124074799=Read"},
{"title": "Enhancing Video-Language Representations with Structural Spatio-Temporal\n  Alignment", "author": "Hao Fei and Shengqiong Wu and Meishan Zhang and Min Zhang and Tat-Seng Chua and Shuicheng Yan", "abstract": "  While pre-training large-scale video-language models (VLMs) has shown\nremarkable potential for various downstream video-language tasks, existing VLMs\ncan still suffer from certain commonly seen limitations, e.g., coarse-grained\ncross-modal aligning , under-modeling of temporal dynamics, detached\nvideo-language view. In this work, we target enhancing VLMs with a fine-grained\nstructural spatio-temporal alignment learning method (namely Finsta). First of\nall, we represent the input texts and videos with fine-grained scene graph (SG)\nstructures, both of which are further unified into a holistic SG (HSG) for\nbridging two modalities. Then, an SG-based framework is built, where the\ntextual SG (TSG) is encoded with a graph Transformer, while the video dynamic\nSG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for\nspatial and temporal feature propagation. A spatial-temporal Gaussian\ndifferential graph Transformer is further devised to strengthen the sense of\nthe changes in objects across spatial and temporal dimensions. Next, based on\nthe fine-grained structural features of TSG and DSG, we perform object-centered\nspatial alignment and predicate-centered temporal alignment respectively,\nenhancing the video-language grounding in both the spatiality and temporality.\nWe design our method as a plug&play system, which can be integrated into\nexisting well-trained VLMs for further representation augmentation, without\ntraining from scratch or relying on SG annotations in downstream applications.\nOn 6 representative VL modeling tasks over 12 datasets in both standard and\nlong-form video scenarios, Finsta consistently improves the existing 13\nstrong-performing VLMs persistently, and refreshes the current state-of-the-art\nend task performance significantly in both the fine-tuning and zero-shot\nsettings.\n", "link": "http://arxiv.org/abs/2406.19255v1", "date": "2024-06-27", "relevancy": 2.2041, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5699}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5591}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Video-Language%20Representations%20with%20Structural%20Spatio-Temporal%0A%20%20Alignment&body=Title%3A%20Enhancing%20Video-Language%20Representations%20with%20Structural%20Spatio-Temporal%0A%20%20Alignment%0AAuthor%3A%20Hao%20Fei%20and%20Shengqiong%20Wu%20and%20Meishan%20Zhang%20and%20Min%20Zhang%20and%20Tat-Seng%20Chua%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%20While%20pre-training%20large-scale%20video-language%20models%20%28VLMs%29%20has%20shown%0Aremarkable%20potential%20for%20various%20downstream%20video-language%20tasks%2C%20existing%20VLMs%0Acan%20still%20suffer%20from%20certain%20commonly%20seen%20limitations%2C%20e.g.%2C%20coarse-grained%0Across-modal%20aligning%20%2C%20under-modeling%20of%20temporal%20dynamics%2C%20detached%0Avideo-language%20view.%20In%20this%20work%2C%20we%20target%20enhancing%20VLMs%20with%20a%20fine-grained%0Astructural%20spatio-temporal%20alignment%20learning%20method%20%28namely%20Finsta%29.%20First%20of%0Aall%2C%20we%20represent%20the%20input%20texts%20and%20videos%20with%20fine-grained%20scene%20graph%20%28SG%29%0Astructures%2C%20both%20of%20which%20are%20further%20unified%20into%20a%20holistic%20SG%20%28HSG%29%20for%0Abridging%20two%20modalities.%20Then%2C%20an%20SG-based%20framework%20is%20built%2C%20where%20the%0Atextual%20SG%20%28TSG%29%20is%20encoded%20with%20a%20graph%20Transformer%2C%20while%20the%20video%20dynamic%0ASG%20%28DSG%29%20and%20the%20HSG%20are%20modeled%20with%20a%20novel%20recurrent%20graph%20Transformer%20for%0Aspatial%20and%20temporal%20feature%20propagation.%20A%20spatial-temporal%20Gaussian%0Adifferential%20graph%20Transformer%20is%20further%20devised%20to%20strengthen%20the%20sense%20of%0Athe%20changes%20in%20objects%20across%20spatial%20and%20temporal%20dimensions.%20Next%2C%20based%20on%0Athe%20fine-grained%20structural%20features%20of%20TSG%20and%20DSG%2C%20we%20perform%20object-centered%0Aspatial%20alignment%20and%20predicate-centered%20temporal%20alignment%20respectively%2C%0Aenhancing%20the%20video-language%20grounding%20in%20both%20the%20spatiality%20and%20temporality.%0AWe%20design%20our%20method%20as%20a%20plug%26play%20system%2C%20which%20can%20be%20integrated%20into%0Aexisting%20well-trained%20VLMs%20for%20further%20representation%20augmentation%2C%20without%0Atraining%20from%20scratch%20or%20relying%20on%20SG%20annotations%20in%20downstream%20applications.%0AOn%206%20representative%20VL%20modeling%20tasks%20over%2012%20datasets%20in%20both%20standard%20and%0Along-form%20video%20scenarios%2C%20Finsta%20consistently%20improves%20the%20existing%2013%0Astrong-performing%20VLMs%20persistently%2C%20and%20refreshes%20the%20current%20state-of-the-art%0Aend%20task%20performance%20significantly%20in%20both%20the%20fine-tuning%20and%20zero-shot%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Video-Language%2520Representations%2520with%2520Structural%2520Spatio-Temporal%250A%2520%2520Alignment%26entry.906535625%3DHao%2520Fei%2520and%2520Shengqiong%2520Wu%2520and%2520Meishan%2520Zhang%2520and%2520Min%2520Zhang%2520and%2520Tat-Seng%2520Chua%2520and%2520Shuicheng%2520Yan%26entry.1292438233%3D%2520%2520While%2520pre-training%2520large-scale%2520video-language%2520models%2520%2528VLMs%2529%2520has%2520shown%250Aremarkable%2520potential%2520for%2520various%2520downstream%2520video-language%2520tasks%252C%2520existing%2520VLMs%250Acan%2520still%2520suffer%2520from%2520certain%2520commonly%2520seen%2520limitations%252C%2520e.g.%252C%2520coarse-grained%250Across-modal%2520aligning%2520%252C%2520under-modeling%2520of%2520temporal%2520dynamics%252C%2520detached%250Avideo-language%2520view.%2520In%2520this%2520work%252C%2520we%2520target%2520enhancing%2520VLMs%2520with%2520a%2520fine-grained%250Astructural%2520spatio-temporal%2520alignment%2520learning%2520method%2520%2528namely%2520Finsta%2529.%2520First%2520of%250Aall%252C%2520we%2520represent%2520the%2520input%2520texts%2520and%2520videos%2520with%2520fine-grained%2520scene%2520graph%2520%2528SG%2529%250Astructures%252C%2520both%2520of%2520which%2520are%2520further%2520unified%2520into%2520a%2520holistic%2520SG%2520%2528HSG%2529%2520for%250Abridging%2520two%2520modalities.%2520Then%252C%2520an%2520SG-based%2520framework%2520is%2520built%252C%2520where%2520the%250Atextual%2520SG%2520%2528TSG%2529%2520is%2520encoded%2520with%2520a%2520graph%2520Transformer%252C%2520while%2520the%2520video%2520dynamic%250ASG%2520%2528DSG%2529%2520and%2520the%2520HSG%2520are%2520modeled%2520with%2520a%2520novel%2520recurrent%2520graph%2520Transformer%2520for%250Aspatial%2520and%2520temporal%2520feature%2520propagation.%2520A%2520spatial-temporal%2520Gaussian%250Adifferential%2520graph%2520Transformer%2520is%2520further%2520devised%2520to%2520strengthen%2520the%2520sense%2520of%250Athe%2520changes%2520in%2520objects%2520across%2520spatial%2520and%2520temporal%2520dimensions.%2520Next%252C%2520based%2520on%250Athe%2520fine-grained%2520structural%2520features%2520of%2520TSG%2520and%2520DSG%252C%2520we%2520perform%2520object-centered%250Aspatial%2520alignment%2520and%2520predicate-centered%2520temporal%2520alignment%2520respectively%252C%250Aenhancing%2520the%2520video-language%2520grounding%2520in%2520both%2520the%2520spatiality%2520and%2520temporality.%250AWe%2520design%2520our%2520method%2520as%2520a%2520plug%2526play%2520system%252C%2520which%2520can%2520be%2520integrated%2520into%250Aexisting%2520well-trained%2520VLMs%2520for%2520further%2520representation%2520augmentation%252C%2520without%250Atraining%2520from%2520scratch%2520or%2520relying%2520on%2520SG%2520annotations%2520in%2520downstream%2520applications.%250AOn%25206%2520representative%2520VL%2520modeling%2520tasks%2520over%252012%2520datasets%2520in%2520both%2520standard%2520and%250Along-form%2520video%2520scenarios%252C%2520Finsta%2520consistently%2520improves%2520the%2520existing%252013%250Astrong-performing%2520VLMs%2520persistently%252C%2520and%2520refreshes%2520the%2520current%2520state-of-the-art%250Aend%2520task%2520performance%2520significantly%2520in%2520both%2520the%2520fine-tuning%2520and%2520zero-shot%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Video-Language%20Representations%20with%20Structural%20Spatio-Temporal%0A%20%20Alignment&entry.906535625=Hao%20Fei%20and%20Shengqiong%20Wu%20and%20Meishan%20Zhang%20and%20Min%20Zhang%20and%20Tat-Seng%20Chua%20and%20Shuicheng%20Yan&entry.1292438233=%20%20While%20pre-training%20large-scale%20video-language%20models%20%28VLMs%29%20has%20shown%0Aremarkable%20potential%20for%20various%20downstream%20video-language%20tasks%2C%20existing%20VLMs%0Acan%20still%20suffer%20from%20certain%20commonly%20seen%20limitations%2C%20e.g.%2C%20coarse-grained%0Across-modal%20aligning%20%2C%20under-modeling%20of%20temporal%20dynamics%2C%20detached%0Avideo-language%20view.%20In%20this%20work%2C%20we%20target%20enhancing%20VLMs%20with%20a%20fine-grained%0Astructural%20spatio-temporal%20alignment%20learning%20method%20%28namely%20Finsta%29.%20First%20of%0Aall%2C%20we%20represent%20the%20input%20texts%20and%20videos%20with%20fine-grained%20scene%20graph%20%28SG%29%0Astructures%2C%20both%20of%20which%20are%20further%20unified%20into%20a%20holistic%20SG%20%28HSG%29%20for%0Abridging%20two%20modalities.%20Then%2C%20an%20SG-based%20framework%20is%20built%2C%20where%20the%0Atextual%20SG%20%28TSG%29%20is%20encoded%20with%20a%20graph%20Transformer%2C%20while%20the%20video%20dynamic%0ASG%20%28DSG%29%20and%20the%20HSG%20are%20modeled%20with%20a%20novel%20recurrent%20graph%20Transformer%20for%0Aspatial%20and%20temporal%20feature%20propagation.%20A%20spatial-temporal%20Gaussian%0Adifferential%20graph%20Transformer%20is%20further%20devised%20to%20strengthen%20the%20sense%20of%0Athe%20changes%20in%20objects%20across%20spatial%20and%20temporal%20dimensions.%20Next%2C%20based%20on%0Athe%20fine-grained%20structural%20features%20of%20TSG%20and%20DSG%2C%20we%20perform%20object-centered%0Aspatial%20alignment%20and%20predicate-centered%20temporal%20alignment%20respectively%2C%0Aenhancing%20the%20video-language%20grounding%20in%20both%20the%20spatiality%20and%20temporality.%0AWe%20design%20our%20method%20as%20a%20plug%26play%20system%2C%20which%20can%20be%20integrated%20into%0Aexisting%20well-trained%20VLMs%20for%20further%20representation%20augmentation%2C%20without%0Atraining%20from%20scratch%20or%20relying%20on%20SG%20annotations%20in%20downstream%20applications.%0AOn%206%20representative%20VL%20modeling%20tasks%20over%2012%20datasets%20in%20both%20standard%20and%0Along-form%20video%20scenarios%2C%20Finsta%20consistently%20improves%20the%20existing%2013%0Astrong-performing%20VLMs%20persistently%2C%20and%20refreshes%20the%20current%20state-of-the-art%0Aend%20task%20performance%20significantly%20in%20both%20the%20fine-tuning%20and%20zero-shot%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19255v1&entry.124074799=Read"},
{"title": "REVEAL-IT: REinforcement learning with Visibility of Evolving Agent\n  poLicy for InTerpretability", "author": "Shuang Ao and Simon Khan and Haris Aziz and Flora D. Salim", "abstract": "  Understanding the agent's learning process, particularly the factors that\ncontribute to its success or failure post-training, is crucial for\ncomprehending the rationale behind the agent's decision-making process. Prior\nmethods clarify the learning process by creating a structural causal model\n(SCM) or visually representing the distribution of value functions.\nNevertheless, these approaches have constraints as they exclusively function in\n2D-environments or with uncomplicated transition dynamics. Understanding the\nagent's learning process in complicated environments or tasks is more\nchallenging. In this paper, we propose REVEAL-IT, a novel framework for\nexplaining the learning process of an agent in complex environments. Initially,\nwe visualize the policy structure and the agent's learning process for various\ntraining tasks. By visualizing these findings, we can understand how much a\nparticular training task or stage affects the agent's performance in test.\nThen, a GNN-based explainer learns to highlight the most important section of\nthe policy, providing a more clear and robust explanation of the agent's\nlearning process. The experiments demonstrate that explanations derived from\nthis framework can effectively help in the optimization of the training tasks,\nresulting in improved learning efficiency and final performance.\n", "link": "http://arxiv.org/abs/2406.14214v3", "date": "2024-06-27", "relevancy": 2.192, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.551}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5487}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REVEAL-IT%3A%20REinforcement%20learning%20with%20Visibility%20of%20Evolving%20Agent%0A%20%20poLicy%20for%20InTerpretability&body=Title%3A%20REVEAL-IT%3A%20REinforcement%20learning%20with%20Visibility%20of%20Evolving%20Agent%0A%20%20poLicy%20for%20InTerpretability%0AAuthor%3A%20Shuang%20Ao%20and%20Simon%20Khan%20and%20Haris%20Aziz%20and%20Flora%20D.%20Salim%0AAbstract%3A%20%20%20Understanding%20the%20agent%27s%20learning%20process%2C%20particularly%20the%20factors%20that%0Acontribute%20to%20its%20success%20or%20failure%20post-training%2C%20is%20crucial%20for%0Acomprehending%20the%20rationale%20behind%20the%20agent%27s%20decision-making%20process.%20Prior%0Amethods%20clarify%20the%20learning%20process%20by%20creating%20a%20structural%20causal%20model%0A%28SCM%29%20or%20visually%20representing%20the%20distribution%20of%20value%20functions.%0ANevertheless%2C%20these%20approaches%20have%20constraints%20as%20they%20exclusively%20function%20in%0A2D-environments%20or%20with%20uncomplicated%20transition%20dynamics.%20Understanding%20the%0Aagent%27s%20learning%20process%20in%20complicated%20environments%20or%20tasks%20is%20more%0Achallenging.%20In%20this%20paper%2C%20we%20propose%20REVEAL-IT%2C%20a%20novel%20framework%20for%0Aexplaining%20the%20learning%20process%20of%20an%20agent%20in%20complex%20environments.%20Initially%2C%0Awe%20visualize%20the%20policy%20structure%20and%20the%20agent%27s%20learning%20process%20for%20various%0Atraining%20tasks.%20By%20visualizing%20these%20findings%2C%20we%20can%20understand%20how%20much%20a%0Aparticular%20training%20task%20or%20stage%20affects%20the%20agent%27s%20performance%20in%20test.%0AThen%2C%20a%20GNN-based%20explainer%20learns%20to%20highlight%20the%20most%20important%20section%20of%0Athe%20policy%2C%20providing%20a%20more%20clear%20and%20robust%20explanation%20of%20the%20agent%27s%0Alearning%20process.%20The%20experiments%20demonstrate%20that%20explanations%20derived%20from%0Athis%20framework%20can%20effectively%20help%20in%20the%20optimization%20of%20the%20training%20tasks%2C%0Aresulting%20in%20improved%20learning%20efficiency%20and%20final%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14214v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREVEAL-IT%253A%2520REinforcement%2520learning%2520with%2520Visibility%2520of%2520Evolving%2520Agent%250A%2520%2520poLicy%2520for%2520InTerpretability%26entry.906535625%3DShuang%2520Ao%2520and%2520Simon%2520Khan%2520and%2520Haris%2520Aziz%2520and%2520Flora%2520D.%2520Salim%26entry.1292438233%3D%2520%2520Understanding%2520the%2520agent%2527s%2520learning%2520process%252C%2520particularly%2520the%2520factors%2520that%250Acontribute%2520to%2520its%2520success%2520or%2520failure%2520post-training%252C%2520is%2520crucial%2520for%250Acomprehending%2520the%2520rationale%2520behind%2520the%2520agent%2527s%2520decision-making%2520process.%2520Prior%250Amethods%2520clarify%2520the%2520learning%2520process%2520by%2520creating%2520a%2520structural%2520causal%2520model%250A%2528SCM%2529%2520or%2520visually%2520representing%2520the%2520distribution%2520of%2520value%2520functions.%250ANevertheless%252C%2520these%2520approaches%2520have%2520constraints%2520as%2520they%2520exclusively%2520function%2520in%250A2D-environments%2520or%2520with%2520uncomplicated%2520transition%2520dynamics.%2520Understanding%2520the%250Aagent%2527s%2520learning%2520process%2520in%2520complicated%2520environments%2520or%2520tasks%2520is%2520more%250Achallenging.%2520In%2520this%2520paper%252C%2520we%2520propose%2520REVEAL-IT%252C%2520a%2520novel%2520framework%2520for%250Aexplaining%2520the%2520learning%2520process%2520of%2520an%2520agent%2520in%2520complex%2520environments.%2520Initially%252C%250Awe%2520visualize%2520the%2520policy%2520structure%2520and%2520the%2520agent%2527s%2520learning%2520process%2520for%2520various%250Atraining%2520tasks.%2520By%2520visualizing%2520these%2520findings%252C%2520we%2520can%2520understand%2520how%2520much%2520a%250Aparticular%2520training%2520task%2520or%2520stage%2520affects%2520the%2520agent%2527s%2520performance%2520in%2520test.%250AThen%252C%2520a%2520GNN-based%2520explainer%2520learns%2520to%2520highlight%2520the%2520most%2520important%2520section%2520of%250Athe%2520policy%252C%2520providing%2520a%2520more%2520clear%2520and%2520robust%2520explanation%2520of%2520the%2520agent%2527s%250Alearning%2520process.%2520The%2520experiments%2520demonstrate%2520that%2520explanations%2520derived%2520from%250Athis%2520framework%2520can%2520effectively%2520help%2520in%2520the%2520optimization%2520of%2520the%2520training%2520tasks%252C%250Aresulting%2520in%2520improved%2520learning%2520efficiency%2520and%2520final%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14214v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REVEAL-IT%3A%20REinforcement%20learning%20with%20Visibility%20of%20Evolving%20Agent%0A%20%20poLicy%20for%20InTerpretability&entry.906535625=Shuang%20Ao%20and%20Simon%20Khan%20and%20Haris%20Aziz%20and%20Flora%20D.%20Salim&entry.1292438233=%20%20Understanding%20the%20agent%27s%20learning%20process%2C%20particularly%20the%20factors%20that%0Acontribute%20to%20its%20success%20or%20failure%20post-training%2C%20is%20crucial%20for%0Acomprehending%20the%20rationale%20behind%20the%20agent%27s%20decision-making%20process.%20Prior%0Amethods%20clarify%20the%20learning%20process%20by%20creating%20a%20structural%20causal%20model%0A%28SCM%29%20or%20visually%20representing%20the%20distribution%20of%20value%20functions.%0ANevertheless%2C%20these%20approaches%20have%20constraints%20as%20they%20exclusively%20function%20in%0A2D-environments%20or%20with%20uncomplicated%20transition%20dynamics.%20Understanding%20the%0Aagent%27s%20learning%20process%20in%20complicated%20environments%20or%20tasks%20is%20more%0Achallenging.%20In%20this%20paper%2C%20we%20propose%20REVEAL-IT%2C%20a%20novel%20framework%20for%0Aexplaining%20the%20learning%20process%20of%20an%20agent%20in%20complex%20environments.%20Initially%2C%0Awe%20visualize%20the%20policy%20structure%20and%20the%20agent%27s%20learning%20process%20for%20various%0Atraining%20tasks.%20By%20visualizing%20these%20findings%2C%20we%20can%20understand%20how%20much%20a%0Aparticular%20training%20task%20or%20stage%20affects%20the%20agent%27s%20performance%20in%20test.%0AThen%2C%20a%20GNN-based%20explainer%20learns%20to%20highlight%20the%20most%20important%20section%20of%0Athe%20policy%2C%20providing%20a%20more%20clear%20and%20robust%20explanation%20of%20the%20agent%27s%0Alearning%20process.%20The%20experiments%20demonstrate%20that%20explanations%20derived%20from%0Athis%20framework%20can%20effectively%20help%20in%20the%20optimization%20of%20the%20training%20tasks%2C%0Aresulting%20in%20improved%20learning%20efficiency%20and%20final%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14214v3&entry.124074799=Read"},
{"title": "Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept\n  Space", "author": "Core Francisco Park and Maya Okawa and Andrew Lee and Ekdeep Singh Lubana and Hidenori Tanaka", "abstract": "  Modern generative models demonstrate impressive capabilities, likely stemming\nfrom an ability to identify and manipulate abstract concepts underlying their\ntraining data. However, fundamental questions remain: what determines the\nconcepts a model learns, the order in which it learns them, and its ability to\nmanipulate those concepts? To address these questions, we propose analyzing a\nmodel's learning dynamics via a framework we call the concept space, where each\naxis represents an independent concept underlying the data generating process.\nBy characterizing learning dynamics in this space, we identify how the speed at\nwhich a concept is learned, and hence the order of concept learning, is\ncontrolled by properties of the data we term concept signal. Further, we\nobserve moments of sudden turns in the direction of a model's learning dynamics\nin concept space. Surprisingly, these points precisely correspond to the\nemergence of hidden capabilities, i.e., where latent interventions show the\nmodel possesses the capability to manipulate a concept, but these capabilities\ncannot yet be elicited via naive input prompting. While our results focus on\nsynthetically defined toy datasets, we hypothesize a general claim on emergence\nof hidden capabilities may hold: generative models possess latent capabilities\nthat emerge suddenly and consistently during training, though a model might not\nexhibit these capabilities under naive input prompting.\n", "link": "http://arxiv.org/abs/2406.19370v1", "date": "2024-06-27", "relevancy": 2.1654, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5824}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5122}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergence%20of%20Hidden%20Capabilities%3A%20Exploring%20Learning%20Dynamics%20in%20Concept%0A%20%20Space&body=Title%3A%20Emergence%20of%20Hidden%20Capabilities%3A%20Exploring%20Learning%20Dynamics%20in%20Concept%0A%20%20Space%0AAuthor%3A%20Core%20Francisco%20Park%20and%20Maya%20Okawa%20and%20Andrew%20Lee%20and%20Ekdeep%20Singh%20Lubana%20and%20Hidenori%20Tanaka%0AAbstract%3A%20%20%20Modern%20generative%20models%20demonstrate%20impressive%20capabilities%2C%20likely%20stemming%0Afrom%20an%20ability%20to%20identify%20and%20manipulate%20abstract%20concepts%20underlying%20their%0Atraining%20data.%20However%2C%20fundamental%20questions%20remain%3A%20what%20determines%20the%0Aconcepts%20a%20model%20learns%2C%20the%20order%20in%20which%20it%20learns%20them%2C%20and%20its%20ability%20to%0Amanipulate%20those%20concepts%3F%20To%20address%20these%20questions%2C%20we%20propose%20analyzing%20a%0Amodel%27s%20learning%20dynamics%20via%20a%20framework%20we%20call%20the%20concept%20space%2C%20where%20each%0Aaxis%20represents%20an%20independent%20concept%20underlying%20the%20data%20generating%20process.%0ABy%20characterizing%20learning%20dynamics%20in%20this%20space%2C%20we%20identify%20how%20the%20speed%20at%0Awhich%20a%20concept%20is%20learned%2C%20and%20hence%20the%20order%20of%20concept%20learning%2C%20is%0Acontrolled%20by%20properties%20of%20the%20data%20we%20term%20concept%20signal.%20Further%2C%20we%0Aobserve%20moments%20of%20sudden%20turns%20in%20the%20direction%20of%20a%20model%27s%20learning%20dynamics%0Ain%20concept%20space.%20Surprisingly%2C%20these%20points%20precisely%20correspond%20to%20the%0Aemergence%20of%20hidden%20capabilities%2C%20i.e.%2C%20where%20latent%20interventions%20show%20the%0Amodel%20possesses%20the%20capability%20to%20manipulate%20a%20concept%2C%20but%20these%20capabilities%0Acannot%20yet%20be%20elicited%20via%20naive%20input%20prompting.%20While%20our%20results%20focus%20on%0Asynthetically%20defined%20toy%20datasets%2C%20we%20hypothesize%20a%20general%20claim%20on%20emergence%0Aof%20hidden%20capabilities%20may%20hold%3A%20generative%20models%20possess%20latent%20capabilities%0Athat%20emerge%20suddenly%20and%20consistently%20during%20training%2C%20though%20a%20model%20might%20not%0Aexhibit%20these%20capabilities%20under%20naive%20input%20prompting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19370v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergence%2520of%2520Hidden%2520Capabilities%253A%2520Exploring%2520Learning%2520Dynamics%2520in%2520Concept%250A%2520%2520Space%26entry.906535625%3DCore%2520Francisco%2520Park%2520and%2520Maya%2520Okawa%2520and%2520Andrew%2520Lee%2520and%2520Ekdeep%2520Singh%2520Lubana%2520and%2520Hidenori%2520Tanaka%26entry.1292438233%3D%2520%2520Modern%2520generative%2520models%2520demonstrate%2520impressive%2520capabilities%252C%2520likely%2520stemming%250Afrom%2520an%2520ability%2520to%2520identify%2520and%2520manipulate%2520abstract%2520concepts%2520underlying%2520their%250Atraining%2520data.%2520However%252C%2520fundamental%2520questions%2520remain%253A%2520what%2520determines%2520the%250Aconcepts%2520a%2520model%2520learns%252C%2520the%2520order%2520in%2520which%2520it%2520learns%2520them%252C%2520and%2520its%2520ability%2520to%250Amanipulate%2520those%2520concepts%253F%2520To%2520address%2520these%2520questions%252C%2520we%2520propose%2520analyzing%2520a%250Amodel%2527s%2520learning%2520dynamics%2520via%2520a%2520framework%2520we%2520call%2520the%2520concept%2520space%252C%2520where%2520each%250Aaxis%2520represents%2520an%2520independent%2520concept%2520underlying%2520the%2520data%2520generating%2520process.%250ABy%2520characterizing%2520learning%2520dynamics%2520in%2520this%2520space%252C%2520we%2520identify%2520how%2520the%2520speed%2520at%250Awhich%2520a%2520concept%2520is%2520learned%252C%2520and%2520hence%2520the%2520order%2520of%2520concept%2520learning%252C%2520is%250Acontrolled%2520by%2520properties%2520of%2520the%2520data%2520we%2520term%2520concept%2520signal.%2520Further%252C%2520we%250Aobserve%2520moments%2520of%2520sudden%2520turns%2520in%2520the%2520direction%2520of%2520a%2520model%2527s%2520learning%2520dynamics%250Ain%2520concept%2520space.%2520Surprisingly%252C%2520these%2520points%2520precisely%2520correspond%2520to%2520the%250Aemergence%2520of%2520hidden%2520capabilities%252C%2520i.e.%252C%2520where%2520latent%2520interventions%2520show%2520the%250Amodel%2520possesses%2520the%2520capability%2520to%2520manipulate%2520a%2520concept%252C%2520but%2520these%2520capabilities%250Acannot%2520yet%2520be%2520elicited%2520via%2520naive%2520input%2520prompting.%2520While%2520our%2520results%2520focus%2520on%250Asynthetically%2520defined%2520toy%2520datasets%252C%2520we%2520hypothesize%2520a%2520general%2520claim%2520on%2520emergence%250Aof%2520hidden%2520capabilities%2520may%2520hold%253A%2520generative%2520models%2520possess%2520latent%2520capabilities%250Athat%2520emerge%2520suddenly%2520and%2520consistently%2520during%2520training%252C%2520though%2520a%2520model%2520might%2520not%250Aexhibit%2520these%2520capabilities%2520under%2520naive%2520input%2520prompting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19370v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergence%20of%20Hidden%20Capabilities%3A%20Exploring%20Learning%20Dynamics%20in%20Concept%0A%20%20Space&entry.906535625=Core%20Francisco%20Park%20and%20Maya%20Okawa%20and%20Andrew%20Lee%20and%20Ekdeep%20Singh%20Lubana%20and%20Hidenori%20Tanaka&entry.1292438233=%20%20Modern%20generative%20models%20demonstrate%20impressive%20capabilities%2C%20likely%20stemming%0Afrom%20an%20ability%20to%20identify%20and%20manipulate%20abstract%20concepts%20underlying%20their%0Atraining%20data.%20However%2C%20fundamental%20questions%20remain%3A%20what%20determines%20the%0Aconcepts%20a%20model%20learns%2C%20the%20order%20in%20which%20it%20learns%20them%2C%20and%20its%20ability%20to%0Amanipulate%20those%20concepts%3F%20To%20address%20these%20questions%2C%20we%20propose%20analyzing%20a%0Amodel%27s%20learning%20dynamics%20via%20a%20framework%20we%20call%20the%20concept%20space%2C%20where%20each%0Aaxis%20represents%20an%20independent%20concept%20underlying%20the%20data%20generating%20process.%0ABy%20characterizing%20learning%20dynamics%20in%20this%20space%2C%20we%20identify%20how%20the%20speed%20at%0Awhich%20a%20concept%20is%20learned%2C%20and%20hence%20the%20order%20of%20concept%20learning%2C%20is%0Acontrolled%20by%20properties%20of%20the%20data%20we%20term%20concept%20signal.%20Further%2C%20we%0Aobserve%20moments%20of%20sudden%20turns%20in%20the%20direction%20of%20a%20model%27s%20learning%20dynamics%0Ain%20concept%20space.%20Surprisingly%2C%20these%20points%20precisely%20correspond%20to%20the%0Aemergence%20of%20hidden%20capabilities%2C%20i.e.%2C%20where%20latent%20interventions%20show%20the%0Amodel%20possesses%20the%20capability%20to%20manipulate%20a%20concept%2C%20but%20these%20capabilities%0Acannot%20yet%20be%20elicited%20via%20naive%20input%20prompting.%20While%20our%20results%20focus%20on%0Asynthetically%20defined%20toy%20datasets%2C%20we%20hypothesize%20a%20general%20claim%20on%20emergence%0Aof%20hidden%20capabilities%20may%20hold%3A%20generative%20models%20possess%20latent%20capabilities%0Athat%20emerge%20suddenly%20and%20consistently%20during%20training%2C%20though%20a%20model%20might%20not%0Aexhibit%20these%20capabilities%20under%20naive%20input%20prompting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19370v1&entry.124074799=Read"},
{"title": "Enhancing Continual Learning in Visual Question Answering with\n  Modality-Aware Feature Distillation", "author": "Malvina Nikandrou and Georgios Pantazopoulos and Ioannis Konstas and Alessandro Suglia", "abstract": "  Continual learning focuses on incrementally training a model on a sequence of\ntasks with the aim of learning new tasks while minimizing performance drop on\nprevious tasks. Existing approaches at the intersection of Continual Learning\nand Visual Question Answering (VQA) do not study how the multimodal nature of\nthe input affects the learning dynamics of a model. In this paper, we\ndemonstrate that each modality evolves at different rates across a continuum of\ntasks and that this behavior occurs in established encoder-only models as well\nas modern recipes for developing Vision & Language (VL) models. Motivated by\nthis observation, we propose a modality-aware feature distillation (MAFED)\napproach which outperforms existing baselines across models of varying scale in\nthree multimodal continual learning settings. Furthermore, we provide ablations\nshowcasing that modality-aware distillation complements experience replay.\nOverall, our results emphasize the importance of addressing modality-specific\ndynamics to prevent forgetting in multimodal continual learning.\n", "link": "http://arxiv.org/abs/2406.19297v1", "date": "2024-06-27", "relevancy": 2.144, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5479}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5477}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Continual%20Learning%20in%20Visual%20Question%20Answering%20with%0A%20%20Modality-Aware%20Feature%20Distillation&body=Title%3A%20Enhancing%20Continual%20Learning%20in%20Visual%20Question%20Answering%20with%0A%20%20Modality-Aware%20Feature%20Distillation%0AAuthor%3A%20Malvina%20Nikandrou%20and%20Georgios%20Pantazopoulos%20and%20Ioannis%20Konstas%20and%20Alessandro%20Suglia%0AAbstract%3A%20%20%20Continual%20learning%20focuses%20on%20incrementally%20training%20a%20model%20on%20a%20sequence%20of%0Atasks%20with%20the%20aim%20of%20learning%20new%20tasks%20while%20minimizing%20performance%20drop%20on%0Aprevious%20tasks.%20Existing%20approaches%20at%20the%20intersection%20of%20Continual%20Learning%0Aand%20Visual%20Question%20Answering%20%28VQA%29%20do%20not%20study%20how%20the%20multimodal%20nature%20of%0Athe%20input%20affects%20the%20learning%20dynamics%20of%20a%20model.%20In%20this%20paper%2C%20we%0Ademonstrate%20that%20each%20modality%20evolves%20at%20different%20rates%20across%20a%20continuum%20of%0Atasks%20and%20that%20this%20behavior%20occurs%20in%20established%20encoder-only%20models%20as%20well%0Aas%20modern%20recipes%20for%20developing%20Vision%20%26%20Language%20%28VL%29%20models.%20Motivated%20by%0Athis%20observation%2C%20we%20propose%20a%20modality-aware%20feature%20distillation%20%28MAFED%29%0Aapproach%20which%20outperforms%20existing%20baselines%20across%20models%20of%20varying%20scale%20in%0Athree%20multimodal%20continual%20learning%20settings.%20Furthermore%2C%20we%20provide%20ablations%0Ashowcasing%20that%20modality-aware%20distillation%20complements%20experience%20replay.%0AOverall%2C%20our%20results%20emphasize%20the%20importance%20of%20addressing%20modality-specific%0Adynamics%20to%20prevent%20forgetting%20in%20multimodal%20continual%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19297v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Continual%2520Learning%2520in%2520Visual%2520Question%2520Answering%2520with%250A%2520%2520Modality-Aware%2520Feature%2520Distillation%26entry.906535625%3DMalvina%2520Nikandrou%2520and%2520Georgios%2520Pantazopoulos%2520and%2520Ioannis%2520Konstas%2520and%2520Alessandro%2520Suglia%26entry.1292438233%3D%2520%2520Continual%2520learning%2520focuses%2520on%2520incrementally%2520training%2520a%2520model%2520on%2520a%2520sequence%2520of%250Atasks%2520with%2520the%2520aim%2520of%2520learning%2520new%2520tasks%2520while%2520minimizing%2520performance%2520drop%2520on%250Aprevious%2520tasks.%2520Existing%2520approaches%2520at%2520the%2520intersection%2520of%2520Continual%2520Learning%250Aand%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520do%2520not%2520study%2520how%2520the%2520multimodal%2520nature%2520of%250Athe%2520input%2520affects%2520the%2520learning%2520dynamics%2520of%2520a%2520model.%2520In%2520this%2520paper%252C%2520we%250Ademonstrate%2520that%2520each%2520modality%2520evolves%2520at%2520different%2520rates%2520across%2520a%2520continuum%2520of%250Atasks%2520and%2520that%2520this%2520behavior%2520occurs%2520in%2520established%2520encoder-only%2520models%2520as%2520well%250Aas%2520modern%2520recipes%2520for%2520developing%2520Vision%2520%2526%2520Language%2520%2528VL%2529%2520models.%2520Motivated%2520by%250Athis%2520observation%252C%2520we%2520propose%2520a%2520modality-aware%2520feature%2520distillation%2520%2528MAFED%2529%250Aapproach%2520which%2520outperforms%2520existing%2520baselines%2520across%2520models%2520of%2520varying%2520scale%2520in%250Athree%2520multimodal%2520continual%2520learning%2520settings.%2520Furthermore%252C%2520we%2520provide%2520ablations%250Ashowcasing%2520that%2520modality-aware%2520distillation%2520complements%2520experience%2520replay.%250AOverall%252C%2520our%2520results%2520emphasize%2520the%2520importance%2520of%2520addressing%2520modality-specific%250Adynamics%2520to%2520prevent%2520forgetting%2520in%2520multimodal%2520continual%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19297v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Continual%20Learning%20in%20Visual%20Question%20Answering%20with%0A%20%20Modality-Aware%20Feature%20Distillation&entry.906535625=Malvina%20Nikandrou%20and%20Georgios%20Pantazopoulos%20and%20Ioannis%20Konstas%20and%20Alessandro%20Suglia&entry.1292438233=%20%20Continual%20learning%20focuses%20on%20incrementally%20training%20a%20model%20on%20a%20sequence%20of%0Atasks%20with%20the%20aim%20of%20learning%20new%20tasks%20while%20minimizing%20performance%20drop%20on%0Aprevious%20tasks.%20Existing%20approaches%20at%20the%20intersection%20of%20Continual%20Learning%0Aand%20Visual%20Question%20Answering%20%28VQA%29%20do%20not%20study%20how%20the%20multimodal%20nature%20of%0Athe%20input%20affects%20the%20learning%20dynamics%20of%20a%20model.%20In%20this%20paper%2C%20we%0Ademonstrate%20that%20each%20modality%20evolves%20at%20different%20rates%20across%20a%20continuum%20of%0Atasks%20and%20that%20this%20behavior%20occurs%20in%20established%20encoder-only%20models%20as%20well%0Aas%20modern%20recipes%20for%20developing%20Vision%20%26%20Language%20%28VL%29%20models.%20Motivated%20by%0Athis%20observation%2C%20we%20propose%20a%20modality-aware%20feature%20distillation%20%28MAFED%29%0Aapproach%20which%20outperforms%20existing%20baselines%20across%20models%20of%20varying%20scale%20in%0Athree%20multimodal%20continual%20learning%20settings.%20Furthermore%2C%20we%20provide%20ablations%0Ashowcasing%20that%20modality-aware%20distillation%20complements%20experience%20replay.%0AOverall%2C%20our%20results%20emphasize%20the%20importance%20of%20addressing%20modality-specific%0Adynamics%20to%20prevent%20forgetting%20in%20multimodal%20continual%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19297v1&entry.124074799=Read"},
{"title": "Mapping Land Naturalness from Sentinel-2 using Deep Contextual and\n  Geographical Priors", "author": "Burak Ekim and Michael Schmitt", "abstract": "  In recent decades, the causes and consequences of climate change have\naccelerated, affecting our planet on an unprecedented scale. This change is\nclosely tied to the ways in which humans alter their surroundings. As our\nactions continue to impact natural areas, using satellite images to observe and\nmeasure these effects has become crucial for understanding and combating\nclimate change. Aiming to map land naturalness on the continuum of modern human\npressure, we have developed a multi-modal supervised deep learning framework\nthat addresses the unique challenges of satellite data and the task at hand. We\nincorporate contextual and geographical priors, represented by corresponding\ncoordinate information and broader contextual information, including and\nsurrounding the immediate patch to be predicted. Our framework improves the\nmodel's predictive performance in mapping land naturalness from Sentinel-2\ndata, a type of multi-spectral optical satellite imagery. Recognizing that our\nprotective measures are only as effective as our understanding of the\necosystem, quantifying naturalness serves as a crucial step toward enhancing\nour environmental stewardship.\n", "link": "http://arxiv.org/abs/2406.19302v1", "date": "2024-06-27", "relevancy": 2.1223, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5565}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5452}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mapping%20Land%20Naturalness%20from%20Sentinel-2%20using%20Deep%20Contextual%20and%0A%20%20Geographical%20Priors&body=Title%3A%20Mapping%20Land%20Naturalness%20from%20Sentinel-2%20using%20Deep%20Contextual%20and%0A%20%20Geographical%20Priors%0AAuthor%3A%20Burak%20Ekim%20and%20Michael%20Schmitt%0AAbstract%3A%20%20%20In%20recent%20decades%2C%20the%20causes%20and%20consequences%20of%20climate%20change%20have%0Aaccelerated%2C%20affecting%20our%20planet%20on%20an%20unprecedented%20scale.%20This%20change%20is%0Aclosely%20tied%20to%20the%20ways%20in%20which%20humans%20alter%20their%20surroundings.%20As%20our%0Aactions%20continue%20to%20impact%20natural%20areas%2C%20using%20satellite%20images%20to%20observe%20and%0Ameasure%20these%20effects%20has%20become%20crucial%20for%20understanding%20and%20combating%0Aclimate%20change.%20Aiming%20to%20map%20land%20naturalness%20on%20the%20continuum%20of%20modern%20human%0Apressure%2C%20we%20have%20developed%20a%20multi-modal%20supervised%20deep%20learning%20framework%0Athat%20addresses%20the%20unique%20challenges%20of%20satellite%20data%20and%20the%20task%20at%20hand.%20We%0Aincorporate%20contextual%20and%20geographical%20priors%2C%20represented%20by%20corresponding%0Acoordinate%20information%20and%20broader%20contextual%20information%2C%20including%20and%0Asurrounding%20the%20immediate%20patch%20to%20be%20predicted.%20Our%20framework%20improves%20the%0Amodel%27s%20predictive%20performance%20in%20mapping%20land%20naturalness%20from%20Sentinel-2%0Adata%2C%20a%20type%20of%20multi-spectral%20optical%20satellite%20imagery.%20Recognizing%20that%20our%0Aprotective%20measures%20are%20only%20as%20effective%20as%20our%20understanding%20of%20the%0Aecosystem%2C%20quantifying%20naturalness%20serves%20as%20a%20crucial%20step%20toward%20enhancing%0Aour%20environmental%20stewardship.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapping%2520Land%2520Naturalness%2520from%2520Sentinel-2%2520using%2520Deep%2520Contextual%2520and%250A%2520%2520Geographical%2520Priors%26entry.906535625%3DBurak%2520Ekim%2520and%2520Michael%2520Schmitt%26entry.1292438233%3D%2520%2520In%2520recent%2520decades%252C%2520the%2520causes%2520and%2520consequences%2520of%2520climate%2520change%2520have%250Aaccelerated%252C%2520affecting%2520our%2520planet%2520on%2520an%2520unprecedented%2520scale.%2520This%2520change%2520is%250Aclosely%2520tied%2520to%2520the%2520ways%2520in%2520which%2520humans%2520alter%2520their%2520surroundings.%2520As%2520our%250Aactions%2520continue%2520to%2520impact%2520natural%2520areas%252C%2520using%2520satellite%2520images%2520to%2520observe%2520and%250Ameasure%2520these%2520effects%2520has%2520become%2520crucial%2520for%2520understanding%2520and%2520combating%250Aclimate%2520change.%2520Aiming%2520to%2520map%2520land%2520naturalness%2520on%2520the%2520continuum%2520of%2520modern%2520human%250Apressure%252C%2520we%2520have%2520developed%2520a%2520multi-modal%2520supervised%2520deep%2520learning%2520framework%250Athat%2520addresses%2520the%2520unique%2520challenges%2520of%2520satellite%2520data%2520and%2520the%2520task%2520at%2520hand.%2520We%250Aincorporate%2520contextual%2520and%2520geographical%2520priors%252C%2520represented%2520by%2520corresponding%250Acoordinate%2520information%2520and%2520broader%2520contextual%2520information%252C%2520including%2520and%250Asurrounding%2520the%2520immediate%2520patch%2520to%2520be%2520predicted.%2520Our%2520framework%2520improves%2520the%250Amodel%2527s%2520predictive%2520performance%2520in%2520mapping%2520land%2520naturalness%2520from%2520Sentinel-2%250Adata%252C%2520a%2520type%2520of%2520multi-spectral%2520optical%2520satellite%2520imagery.%2520Recognizing%2520that%2520our%250Aprotective%2520measures%2520are%2520only%2520as%2520effective%2520as%2520our%2520understanding%2520of%2520the%250Aecosystem%252C%2520quantifying%2520naturalness%2520serves%2520as%2520a%2520crucial%2520step%2520toward%2520enhancing%250Aour%2520environmental%2520stewardship.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mapping%20Land%20Naturalness%20from%20Sentinel-2%20using%20Deep%20Contextual%20and%0A%20%20Geographical%20Priors&entry.906535625=Burak%20Ekim%20and%20Michael%20Schmitt&entry.1292438233=%20%20In%20recent%20decades%2C%20the%20causes%20and%20consequences%20of%20climate%20change%20have%0Aaccelerated%2C%20affecting%20our%20planet%20on%20an%20unprecedented%20scale.%20This%20change%20is%0Aclosely%20tied%20to%20the%20ways%20in%20which%20humans%20alter%20their%20surroundings.%20As%20our%0Aactions%20continue%20to%20impact%20natural%20areas%2C%20using%20satellite%20images%20to%20observe%20and%0Ameasure%20these%20effects%20has%20become%20crucial%20for%20understanding%20and%20combating%0Aclimate%20change.%20Aiming%20to%20map%20land%20naturalness%20on%20the%20continuum%20of%20modern%20human%0Apressure%2C%20we%20have%20developed%20a%20multi-modal%20supervised%20deep%20learning%20framework%0Athat%20addresses%20the%20unique%20challenges%20of%20satellite%20data%20and%20the%20task%20at%20hand.%20We%0Aincorporate%20contextual%20and%20geographical%20priors%2C%20represented%20by%20corresponding%0Acoordinate%20information%20and%20broader%20contextual%20information%2C%20including%20and%0Asurrounding%20the%20immediate%20patch%20to%20be%20predicted.%20Our%20framework%20improves%20the%0Amodel%27s%20predictive%20performance%20in%20mapping%20land%20naturalness%20from%20Sentinel-2%0Adata%2C%20a%20type%20of%20multi-spectral%20optical%20satellite%20imagery.%20Recognizing%20that%20our%0Aprotective%20measures%20are%20only%20as%20effective%20as%20our%20understanding%20of%20the%0Aecosystem%2C%20quantifying%20naturalness%20serves%20as%20a%20crucial%20step%20toward%20enhancing%0Aour%20environmental%20stewardship.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19302v1&entry.124074799=Read"},
{"title": "Human Modelling and Pose Estimation Overview", "author": "Pawel Knap", "abstract": "  Human modelling and pose estimation stands at the crossroads of Computer\nVision, Computer Graphics, and Machine Learning. This paper presents a thorough\ninvestigation of this interdisciplinary field, examining various algorithms,\nmethodologies, and practical applications. It explores the diverse range of\nsensor technologies relevant to this domain and delves into a wide array of\napplication areas. Additionally, we discuss the challenges and advancements in\n2D and 3D human modelling methodologies, along with popular datasets, metrics,\nand future research directions. The main contribution of this paper lies in its\nup-to-date comparison of state-of-the-art (SOTA) human pose estimation\nalgorithms in both 2D and 3D domains. By providing this comprehensive overview,\nthe paper aims to enhance understanding of 3D human modelling and pose\nestimation, offering insights into current SOTA achievements, challenges, and\nfuture prospects within the field.\n", "link": "http://arxiv.org/abs/2406.19290v1", "date": "2024-06-27", "relevancy": 2.1097, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.548}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5458}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20Modelling%20and%20Pose%20Estimation%20Overview&body=Title%3A%20Human%20Modelling%20and%20Pose%20Estimation%20Overview%0AAuthor%3A%20Pawel%20Knap%0AAbstract%3A%20%20%20Human%20modelling%20and%20pose%20estimation%20stands%20at%20the%20crossroads%20of%20Computer%0AVision%2C%20Computer%20Graphics%2C%20and%20Machine%20Learning.%20This%20paper%20presents%20a%20thorough%0Ainvestigation%20of%20this%20interdisciplinary%20field%2C%20examining%20various%20algorithms%2C%0Amethodologies%2C%20and%20practical%20applications.%20It%20explores%20the%20diverse%20range%20of%0Asensor%20technologies%20relevant%20to%20this%20domain%20and%20delves%20into%20a%20wide%20array%20of%0Aapplication%20areas.%20Additionally%2C%20we%20discuss%20the%20challenges%20and%20advancements%20in%0A2D%20and%203D%20human%20modelling%20methodologies%2C%20along%20with%20popular%20datasets%2C%20metrics%2C%0Aand%20future%20research%20directions.%20The%20main%20contribution%20of%20this%20paper%20lies%20in%20its%0Aup-to-date%20comparison%20of%20state-of-the-art%20%28SOTA%29%20human%20pose%20estimation%0Aalgorithms%20in%20both%202D%20and%203D%20domains.%20By%20providing%20this%20comprehensive%20overview%2C%0Athe%20paper%20aims%20to%20enhance%20understanding%20of%203D%20human%20modelling%20and%20pose%0Aestimation%2C%20offering%20insights%20into%20current%20SOTA%20achievements%2C%20challenges%2C%20and%0Afuture%20prospects%20within%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19290v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520Modelling%2520and%2520Pose%2520Estimation%2520Overview%26entry.906535625%3DPawel%2520Knap%26entry.1292438233%3D%2520%2520Human%2520modelling%2520and%2520pose%2520estimation%2520stands%2520at%2520the%2520crossroads%2520of%2520Computer%250AVision%252C%2520Computer%2520Graphics%252C%2520and%2520Machine%2520Learning.%2520This%2520paper%2520presents%2520a%2520thorough%250Ainvestigation%2520of%2520this%2520interdisciplinary%2520field%252C%2520examining%2520various%2520algorithms%252C%250Amethodologies%252C%2520and%2520practical%2520applications.%2520It%2520explores%2520the%2520diverse%2520range%2520of%250Asensor%2520technologies%2520relevant%2520to%2520this%2520domain%2520and%2520delves%2520into%2520a%2520wide%2520array%2520of%250Aapplication%2520areas.%2520Additionally%252C%2520we%2520discuss%2520the%2520challenges%2520and%2520advancements%2520in%250A2D%2520and%25203D%2520human%2520modelling%2520methodologies%252C%2520along%2520with%2520popular%2520datasets%252C%2520metrics%252C%250Aand%2520future%2520research%2520directions.%2520The%2520main%2520contribution%2520of%2520this%2520paper%2520lies%2520in%2520its%250Aup-to-date%2520comparison%2520of%2520state-of-the-art%2520%2528SOTA%2529%2520human%2520pose%2520estimation%250Aalgorithms%2520in%2520both%25202D%2520and%25203D%2520domains.%2520By%2520providing%2520this%2520comprehensive%2520overview%252C%250Athe%2520paper%2520aims%2520to%2520enhance%2520understanding%2520of%25203D%2520human%2520modelling%2520and%2520pose%250Aestimation%252C%2520offering%2520insights%2520into%2520current%2520SOTA%2520achievements%252C%2520challenges%252C%2520and%250Afuture%2520prospects%2520within%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19290v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Modelling%20and%20Pose%20Estimation%20Overview&entry.906535625=Pawel%20Knap&entry.1292438233=%20%20Human%20modelling%20and%20pose%20estimation%20stands%20at%20the%20crossroads%20of%20Computer%0AVision%2C%20Computer%20Graphics%2C%20and%20Machine%20Learning.%20This%20paper%20presents%20a%20thorough%0Ainvestigation%20of%20this%20interdisciplinary%20field%2C%20examining%20various%20algorithms%2C%0Amethodologies%2C%20and%20practical%20applications.%20It%20explores%20the%20diverse%20range%20of%0Asensor%20technologies%20relevant%20to%20this%20domain%20and%20delves%20into%20a%20wide%20array%20of%0Aapplication%20areas.%20Additionally%2C%20we%20discuss%20the%20challenges%20and%20advancements%20in%0A2D%20and%203D%20human%20modelling%20methodologies%2C%20along%20with%20popular%20datasets%2C%20metrics%2C%0Aand%20future%20research%20directions.%20The%20main%20contribution%20of%20this%20paper%20lies%20in%20its%0Aup-to-date%20comparison%20of%20state-of-the-art%20%28SOTA%29%20human%20pose%20estimation%0Aalgorithms%20in%20both%202D%20and%203D%20domains.%20By%20providing%20this%20comprehensive%20overview%2C%0Athe%20paper%20aims%20to%20enhance%20understanding%20of%203D%20human%20modelling%20and%20pose%0Aestimation%2C%20offering%20insights%20into%20current%20SOTA%20achievements%2C%20challenges%2C%20and%0Afuture%20prospects%20within%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19290v1&entry.124074799=Read"},
{"title": "Dancing in the Shadows: Harnessing Ambiguity for Fairer Classifiers", "author": "Ainhize Barrainkua and Paula Gordaliza and Jose A. Lozano and Novi Quadrianto", "abstract": "  This paper introduces a novel approach to bolster algorithmic fairness in\nscenarios where sensitive information is only partially known. In particular,\nwe propose to leverage instances with uncertain identity with regards to the\nsensitive attribute to train a conventional machine learning classifier. The\nenhanced fairness observed in the final predictions of this classifier\nhighlights the promising potential of prioritizing ambiguity (i.e.,\nnon-normativity) as a means to improve fairness guarantees in real-world\nclassification tasks.\n", "link": "http://arxiv.org/abs/2406.19066v1", "date": "2024-06-27", "relevancy": 2.1022, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5464}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5119}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dancing%20in%20the%20Shadows%3A%20Harnessing%20Ambiguity%20for%20Fairer%20Classifiers&body=Title%3A%20Dancing%20in%20the%20Shadows%3A%20Harnessing%20Ambiguity%20for%20Fairer%20Classifiers%0AAuthor%3A%20Ainhize%20Barrainkua%20and%20Paula%20Gordaliza%20and%20Jose%20A.%20Lozano%20and%20Novi%20Quadrianto%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20approach%20to%20bolster%20algorithmic%20fairness%20in%0Ascenarios%20where%20sensitive%20information%20is%20only%20partially%20known.%20In%20particular%2C%0Awe%20propose%20to%20leverage%20instances%20with%20uncertain%20identity%20with%20regards%20to%20the%0Asensitive%20attribute%20to%20train%20a%20conventional%20machine%20learning%20classifier.%20The%0Aenhanced%20fairness%20observed%20in%20the%20final%20predictions%20of%20this%20classifier%0Ahighlights%20the%20promising%20potential%20of%20prioritizing%20ambiguity%20%28i.e.%2C%0Anon-normativity%29%20as%20a%20means%20to%20improve%20fairness%20guarantees%20in%20real-world%0Aclassification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDancing%2520in%2520the%2520Shadows%253A%2520Harnessing%2520Ambiguity%2520for%2520Fairer%2520Classifiers%26entry.906535625%3DAinhize%2520Barrainkua%2520and%2520Paula%2520Gordaliza%2520and%2520Jose%2520A.%2520Lozano%2520and%2520Novi%2520Quadrianto%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520to%2520bolster%2520algorithmic%2520fairness%2520in%250Ascenarios%2520where%2520sensitive%2520information%2520is%2520only%2520partially%2520known.%2520In%2520particular%252C%250Awe%2520propose%2520to%2520leverage%2520instances%2520with%2520uncertain%2520identity%2520with%2520regards%2520to%2520the%250Asensitive%2520attribute%2520to%2520train%2520a%2520conventional%2520machine%2520learning%2520classifier.%2520The%250Aenhanced%2520fairness%2520observed%2520in%2520the%2520final%2520predictions%2520of%2520this%2520classifier%250Ahighlights%2520the%2520promising%2520potential%2520of%2520prioritizing%2520ambiguity%2520%2528i.e.%252C%250Anon-normativity%2529%2520as%2520a%2520means%2520to%2520improve%2520fairness%2520guarantees%2520in%2520real-world%250Aclassification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dancing%20in%20the%20Shadows%3A%20Harnessing%20Ambiguity%20for%20Fairer%20Classifiers&entry.906535625=Ainhize%20Barrainkua%20and%20Paula%20Gordaliza%20and%20Jose%20A.%20Lozano%20and%20Novi%20Quadrianto&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20approach%20to%20bolster%20algorithmic%20fairness%20in%0Ascenarios%20where%20sensitive%20information%20is%20only%20partially%20known.%20In%20particular%2C%0Awe%20propose%20to%20leverage%20instances%20with%20uncertain%20identity%20with%20regards%20to%20the%0Asensitive%20attribute%20to%20train%20a%20conventional%20machine%20learning%20classifier.%20The%0Aenhanced%20fairness%20observed%20in%20the%20final%20predictions%20of%20this%20classifier%0Ahighlights%20the%20promising%20potential%20of%20prioritizing%20ambiguity%20%28i.e.%2C%0Anon-normativity%29%20as%20a%20means%20to%20improve%20fairness%20guarantees%20in%20real-world%0Aclassification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19066v1&entry.124074799=Read"},
{"title": "SimpleFusion: A Simple Fusion Framework for Infrared and Visible Images", "author": "Ming Chen and Yuxuan Cheng and Xinwei He and Xinyue Wang and Yan Aze and Jinhai Xiang", "abstract": "  Integrating visible and infrared images into one high-quality image, also\nknown as visible and infrared image fusion, is a challenging yet critical task\nfor many downstream vision tasks. Most existing works utilize pretrained deep\nneural networks or design sophisticated frameworks with strong priors for this\ntask, which may be unsuitable or lack flexibility. This paper presents\nSimpleFusion, a simple yet effective framework for visible and infrared image\nfusion. Our framework follows the decompose-and-fusion paradigm, where the\nvisible and the infrared images are decomposed into reflectance and\nillumination components via Retinex theory and followed by the fusion of these\ncorresponding elements. The whole framework is designed with two plain\nconvolutional neural networks without downsampling, which can perform image\ndecomposition and fusion efficiently. Moreover, we introduce decomposition loss\nand a detail-to-semantic loss to preserve the complementary information between\nthe two modalities for fusion. We conduct extensive experiments on the\nchallenging benchmarks, verifying the superiority of our method over previous\nstate-of-the-arts. Code is available at\n\\href{https://github.com/hxwxss/SimpleFusion-A-Simple-Fusion-Framework-for-Infrared-and-Visible-Images}{https://github.com/hxwxss/SimpleFusion-A-Simple-Fusion-Framework-for-Infrared-and-Visible-Images}\n", "link": "http://arxiv.org/abs/2406.19055v1", "date": "2024-06-27", "relevancy": 2.0988, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5311}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5303}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimpleFusion%3A%20A%20Simple%20Fusion%20Framework%20for%20Infrared%20and%20Visible%20Images&body=Title%3A%20SimpleFusion%3A%20A%20Simple%20Fusion%20Framework%20for%20Infrared%20and%20Visible%20Images%0AAuthor%3A%20Ming%20Chen%20and%20Yuxuan%20Cheng%20and%20Xinwei%20He%20and%20Xinyue%20Wang%20and%20Yan%20Aze%20and%20Jinhai%20Xiang%0AAbstract%3A%20%20%20Integrating%20visible%20and%20infrared%20images%20into%20one%20high-quality%20image%2C%20also%0Aknown%20as%20visible%20and%20infrared%20image%20fusion%2C%20is%20a%20challenging%20yet%20critical%20task%0Afor%20many%20downstream%20vision%20tasks.%20Most%20existing%20works%20utilize%20pretrained%20deep%0Aneural%20networks%20or%20design%20sophisticated%20frameworks%20with%20strong%20priors%20for%20this%0Atask%2C%20which%20may%20be%20unsuitable%20or%20lack%20flexibility.%20This%20paper%20presents%0ASimpleFusion%2C%20a%20simple%20yet%20effective%20framework%20for%20visible%20and%20infrared%20image%0Afusion.%20Our%20framework%20follows%20the%20decompose-and-fusion%20paradigm%2C%20where%20the%0Avisible%20and%20the%20infrared%20images%20are%20decomposed%20into%20reflectance%20and%0Aillumination%20components%20via%20Retinex%20theory%20and%20followed%20by%20the%20fusion%20of%20these%0Acorresponding%20elements.%20The%20whole%20framework%20is%20designed%20with%20two%20plain%0Aconvolutional%20neural%20networks%20without%20downsampling%2C%20which%20can%20perform%20image%0Adecomposition%20and%20fusion%20efficiently.%20Moreover%2C%20we%20introduce%20decomposition%20loss%0Aand%20a%20detail-to-semantic%20loss%20to%20preserve%20the%20complementary%20information%20between%0Athe%20two%20modalities%20for%20fusion.%20We%20conduct%20extensive%20experiments%20on%20the%0Achallenging%20benchmarks%2C%20verifying%20the%20superiority%20of%20our%20method%20over%20previous%0Astate-of-the-arts.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/hxwxss/SimpleFusion-A-Simple-Fusion-Framework-for-Infrared-and-Visible-Images%7D%7Bhttps%3A//github.com/hxwxss/SimpleFusion-A-Simple-Fusion-Framework-for-Infrared-and-Visible-Images%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19055v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimpleFusion%253A%2520A%2520Simple%2520Fusion%2520Framework%2520for%2520Infrared%2520and%2520Visible%2520Images%26entry.906535625%3DMing%2520Chen%2520and%2520Yuxuan%2520Cheng%2520and%2520Xinwei%2520He%2520and%2520Xinyue%2520Wang%2520and%2520Yan%2520Aze%2520and%2520Jinhai%2520Xiang%26entry.1292438233%3D%2520%2520Integrating%2520visible%2520and%2520infrared%2520images%2520into%2520one%2520high-quality%2520image%252C%2520also%250Aknown%2520as%2520visible%2520and%2520infrared%2520image%2520fusion%252C%2520is%2520a%2520challenging%2520yet%2520critical%2520task%250Afor%2520many%2520downstream%2520vision%2520tasks.%2520Most%2520existing%2520works%2520utilize%2520pretrained%2520deep%250Aneural%2520networks%2520or%2520design%2520sophisticated%2520frameworks%2520with%2520strong%2520priors%2520for%2520this%250Atask%252C%2520which%2520may%2520be%2520unsuitable%2520or%2520lack%2520flexibility.%2520This%2520paper%2520presents%250ASimpleFusion%252C%2520a%2520simple%2520yet%2520effective%2520framework%2520for%2520visible%2520and%2520infrared%2520image%250Afusion.%2520Our%2520framework%2520follows%2520the%2520decompose-and-fusion%2520paradigm%252C%2520where%2520the%250Avisible%2520and%2520the%2520infrared%2520images%2520are%2520decomposed%2520into%2520reflectance%2520and%250Aillumination%2520components%2520via%2520Retinex%2520theory%2520and%2520followed%2520by%2520the%2520fusion%2520of%2520these%250Acorresponding%2520elements.%2520The%2520whole%2520framework%2520is%2520designed%2520with%2520two%2520plain%250Aconvolutional%2520neural%2520networks%2520without%2520downsampling%252C%2520which%2520can%2520perform%2520image%250Adecomposition%2520and%2520fusion%2520efficiently.%2520Moreover%252C%2520we%2520introduce%2520decomposition%2520loss%250Aand%2520a%2520detail-to-semantic%2520loss%2520to%2520preserve%2520the%2520complementary%2520information%2520between%250Athe%2520two%2520modalities%2520for%2520fusion.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520the%250Achallenging%2520benchmarks%252C%2520verifying%2520the%2520superiority%2520of%2520our%2520method%2520over%2520previous%250Astate-of-the-arts.%2520Code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/hxwxss/SimpleFusion-A-Simple-Fusion-Framework-for-Infrared-and-Visible-Images%257D%257Bhttps%253A//github.com/hxwxss/SimpleFusion-A-Simple-Fusion-Framework-for-Infrared-and-Visible-Images%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19055v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimpleFusion%3A%20A%20Simple%20Fusion%20Framework%20for%20Infrared%20and%20Visible%20Images&entry.906535625=Ming%20Chen%20and%20Yuxuan%20Cheng%20and%20Xinwei%20He%20and%20Xinyue%20Wang%20and%20Yan%20Aze%20and%20Jinhai%20Xiang&entry.1292438233=%20%20Integrating%20visible%20and%20infrared%20images%20into%20one%20high-quality%20image%2C%20also%0Aknown%20as%20visible%20and%20infrared%20image%20fusion%2C%20is%20a%20challenging%20yet%20critical%20task%0Afor%20many%20downstream%20vision%20tasks.%20Most%20existing%20works%20utilize%20pretrained%20deep%0Aneural%20networks%20or%20design%20sophisticated%20frameworks%20with%20strong%20priors%20for%20this%0Atask%2C%20which%20may%20be%20unsuitable%20or%20lack%20flexibility.%20This%20paper%20presents%0ASimpleFusion%2C%20a%20simple%20yet%20effective%20framework%20for%20visible%20and%20infrared%20image%0Afusion.%20Our%20framework%20follows%20the%20decompose-and-fusion%20paradigm%2C%20where%20the%0Avisible%20and%20the%20infrared%20images%20are%20decomposed%20into%20reflectance%20and%0Aillumination%20components%20via%20Retinex%20theory%20and%20followed%20by%20the%20fusion%20of%20these%0Acorresponding%20elements.%20The%20whole%20framework%20is%20designed%20with%20two%20plain%0Aconvolutional%20neural%20networks%20without%20downsampling%2C%20which%20can%20perform%20image%0Adecomposition%20and%20fusion%20efficiently.%20Moreover%2C%20we%20introduce%20decomposition%20loss%0Aand%20a%20detail-to-semantic%20loss%20to%20preserve%20the%20complementary%20information%20between%0Athe%20two%20modalities%20for%20fusion.%20We%20conduct%20extensive%20experiments%20on%20the%0Achallenging%20benchmarks%2C%20verifying%20the%20superiority%20of%20our%20method%20over%20previous%0Astate-of-the-arts.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/hxwxss/SimpleFusion-A-Simple-Fusion-Framework-for-Infrared-and-Visible-Images%7D%7Bhttps%3A//github.com/hxwxss/SimpleFusion-A-Simple-Fusion-Framework-for-Infrared-and-Visible-Images%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19055v1&entry.124074799=Read"},
{"title": "RAVEN: Multitask Retrieval Augmented Vision-Language Learning", "author": "Varun Nagaraj Rao and Siddharth Choudhary and Aditya Deshpande and Ravi Kumar Satzoda and Srikar Appalaraju", "abstract": "  The scaling of large language models to encode all the world's knowledge in\nmodel parameters is unsustainable and has exacerbated resource barriers.\nRetrieval-Augmented Generation (RAG) presents a potential solution, yet its\napplication to vision-language models (VLMs) is under explored. Existing\nmethods focus on models designed for single tasks. Furthermore, they're limited\nby the need for resource intensive pre training, additional parameter\nrequirements, unaddressed modality prioritization and lack of clear benefit\nover non-retrieval baselines. This paper introduces RAVEN, a multitask\nretrieval augmented VLM framework that enhances base VLMs through efficient,\ntask specific fine-tuning. By integrating retrieval augmented samples without\nthe need for additional retrieval-specific parameters, we show that the model\nacquires retrieval properties that are effective across multiple tasks. Our\nresults and extensive ablations across retrieved modalities for the image\ncaptioning and VQA tasks indicate significant performance improvements compared\nto non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a\n+3\\% accuracy on specific VQA question types. This underscores the efficacy of\napplying RAG approaches to VLMs, marking a stride toward more efficient and\naccessible multimodal learning.\n", "link": "http://arxiv.org/abs/2406.19150v1", "date": "2024-06-27", "relevancy": 2.095, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5574}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5009}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAVEN%3A%20Multitask%20Retrieval%20Augmented%20Vision-Language%20Learning&body=Title%3A%20RAVEN%3A%20Multitask%20Retrieval%20Augmented%20Vision-Language%20Learning%0AAuthor%3A%20Varun%20Nagaraj%20Rao%20and%20Siddharth%20Choudhary%20and%20Aditya%20Deshpande%20and%20Ravi%20Kumar%20Satzoda%20and%20Srikar%20Appalaraju%0AAbstract%3A%20%20%20The%20scaling%20of%20large%20language%20models%20to%20encode%20all%20the%20world%27s%20knowledge%20in%0Amodel%20parameters%20is%20unsustainable%20and%20has%20exacerbated%20resource%20barriers.%0ARetrieval-Augmented%20Generation%20%28RAG%29%20presents%20a%20potential%20solution%2C%20yet%20its%0Aapplication%20to%20vision-language%20models%20%28VLMs%29%20is%20under%20explored.%20Existing%0Amethods%20focus%20on%20models%20designed%20for%20single%20tasks.%20Furthermore%2C%20they%27re%20limited%0Aby%20the%20need%20for%20resource%20intensive%20pre%20training%2C%20additional%20parameter%0Arequirements%2C%20unaddressed%20modality%20prioritization%20and%20lack%20of%20clear%20benefit%0Aover%20non-retrieval%20baselines.%20This%20paper%20introduces%20RAVEN%2C%20a%20multitask%0Aretrieval%20augmented%20VLM%20framework%20that%20enhances%20base%20VLMs%20through%20efficient%2C%0Atask%20specific%20fine-tuning.%20By%20integrating%20retrieval%20augmented%20samples%20without%0Athe%20need%20for%20additional%20retrieval-specific%20parameters%2C%20we%20show%20that%20the%20model%0Aacquires%20retrieval%20properties%20that%20are%20effective%20across%20multiple%20tasks.%20Our%0Aresults%20and%20extensive%20ablations%20across%20retrieved%20modalities%20for%20the%20image%0Acaptioning%20and%20VQA%20tasks%20indicate%20significant%20performance%20improvements%20compared%0Ato%20non%20retrieved%20baselines%20%2B1%20CIDEr%20on%20MSCOCO%2C%20%2B4%20CIDEr%20on%20NoCaps%20and%20nearly%20a%0A%2B3%5C%25%20accuracy%20on%20specific%20VQA%20question%20types.%20This%20underscores%20the%20efficacy%20of%0Aapplying%20RAG%20approaches%20to%20VLMs%2C%20marking%20a%20stride%20toward%20more%20efficient%20and%0Aaccessible%20multimodal%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAVEN%253A%2520Multitask%2520Retrieval%2520Augmented%2520Vision-Language%2520Learning%26entry.906535625%3DVarun%2520Nagaraj%2520Rao%2520and%2520Siddharth%2520Choudhary%2520and%2520Aditya%2520Deshpande%2520and%2520Ravi%2520Kumar%2520Satzoda%2520and%2520Srikar%2520Appalaraju%26entry.1292438233%3D%2520%2520The%2520scaling%2520of%2520large%2520language%2520models%2520to%2520encode%2520all%2520the%2520world%2527s%2520knowledge%2520in%250Amodel%2520parameters%2520is%2520unsustainable%2520and%2520has%2520exacerbated%2520resource%2520barriers.%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520presents%2520a%2520potential%2520solution%252C%2520yet%2520its%250Aapplication%2520to%2520vision-language%2520models%2520%2528VLMs%2529%2520is%2520under%2520explored.%2520Existing%250Amethods%2520focus%2520on%2520models%2520designed%2520for%2520single%2520tasks.%2520Furthermore%252C%2520they%2527re%2520limited%250Aby%2520the%2520need%2520for%2520resource%2520intensive%2520pre%2520training%252C%2520additional%2520parameter%250Arequirements%252C%2520unaddressed%2520modality%2520prioritization%2520and%2520lack%2520of%2520clear%2520benefit%250Aover%2520non-retrieval%2520baselines.%2520This%2520paper%2520introduces%2520RAVEN%252C%2520a%2520multitask%250Aretrieval%2520augmented%2520VLM%2520framework%2520that%2520enhances%2520base%2520VLMs%2520through%2520efficient%252C%250Atask%2520specific%2520fine-tuning.%2520By%2520integrating%2520retrieval%2520augmented%2520samples%2520without%250Athe%2520need%2520for%2520additional%2520retrieval-specific%2520parameters%252C%2520we%2520show%2520that%2520the%2520model%250Aacquires%2520retrieval%2520properties%2520that%2520are%2520effective%2520across%2520multiple%2520tasks.%2520Our%250Aresults%2520and%2520extensive%2520ablations%2520across%2520retrieved%2520modalities%2520for%2520the%2520image%250Acaptioning%2520and%2520VQA%2520tasks%2520indicate%2520significant%2520performance%2520improvements%2520compared%250Ato%2520non%2520retrieved%2520baselines%2520%252B1%2520CIDEr%2520on%2520MSCOCO%252C%2520%252B4%2520CIDEr%2520on%2520NoCaps%2520and%2520nearly%2520a%250A%252B3%255C%2525%2520accuracy%2520on%2520specific%2520VQA%2520question%2520types.%2520This%2520underscores%2520the%2520efficacy%2520of%250Aapplying%2520RAG%2520approaches%2520to%2520VLMs%252C%2520marking%2520a%2520stride%2520toward%2520more%2520efficient%2520and%250Aaccessible%2520multimodal%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAVEN%3A%20Multitask%20Retrieval%20Augmented%20Vision-Language%20Learning&entry.906535625=Varun%20Nagaraj%20Rao%20and%20Siddharth%20Choudhary%20and%20Aditya%20Deshpande%20and%20Ravi%20Kumar%20Satzoda%20and%20Srikar%20Appalaraju&entry.1292438233=%20%20The%20scaling%20of%20large%20language%20models%20to%20encode%20all%20the%20world%27s%20knowledge%20in%0Amodel%20parameters%20is%20unsustainable%20and%20has%20exacerbated%20resource%20barriers.%0ARetrieval-Augmented%20Generation%20%28RAG%29%20presents%20a%20potential%20solution%2C%20yet%20its%0Aapplication%20to%20vision-language%20models%20%28VLMs%29%20is%20under%20explored.%20Existing%0Amethods%20focus%20on%20models%20designed%20for%20single%20tasks.%20Furthermore%2C%20they%27re%20limited%0Aby%20the%20need%20for%20resource%20intensive%20pre%20training%2C%20additional%20parameter%0Arequirements%2C%20unaddressed%20modality%20prioritization%20and%20lack%20of%20clear%20benefit%0Aover%20non-retrieval%20baselines.%20This%20paper%20introduces%20RAVEN%2C%20a%20multitask%0Aretrieval%20augmented%20VLM%20framework%20that%20enhances%20base%20VLMs%20through%20efficient%2C%0Atask%20specific%20fine-tuning.%20By%20integrating%20retrieval%20augmented%20samples%20without%0Athe%20need%20for%20additional%20retrieval-specific%20parameters%2C%20we%20show%20that%20the%20model%0Aacquires%20retrieval%20properties%20that%20are%20effective%20across%20multiple%20tasks.%20Our%0Aresults%20and%20extensive%20ablations%20across%20retrieved%20modalities%20for%20the%20image%0Acaptioning%20and%20VQA%20tasks%20indicate%20significant%20performance%20improvements%20compared%0Ato%20non%20retrieved%20baselines%20%2B1%20CIDEr%20on%20MSCOCO%2C%20%2B4%20CIDEr%20on%20NoCaps%20and%20nearly%20a%0A%2B3%5C%25%20accuracy%20on%20specific%20VQA%20question%20types.%20This%20underscores%20the%20efficacy%20of%0Aapplying%20RAG%20approaches%20to%20VLMs%2C%20marking%20a%20stride%20toward%20more%20efficient%20and%0Aaccessible%20multimodal%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19150v1&entry.124074799=Read"},
{"title": "Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping", "author": "Hang Jung Ling and Salom\u00e9 Bru and Julia Puig and Florian Vix\u00e8ge and Simon Mendez and Franck Nicoud and Pierre-Yves Courand and Olivier Bernard and Damien Garcia", "abstract": "  Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify\ncolor Doppler in cardiac imaging. In this study, we propose novel alternatives\nto the traditional iVFM optimization scheme by utilizing physics-informed\nneural networks (PINNs) and a physics-guided nnU-Net-based supervised approach.\nWhen evaluated on simulated color Doppler images derived from a\npatient-specific computational fluid dynamics model and in vivo Doppler\nacquisitions, both approaches demonstrate comparable reconstruction performance\nto the original iVFM algorithm. The efficiency of PINNs is boosted through\ndual-stage optimization and pre-optimized weights. On the other hand, the\nnnU-Net method excels in generalizability and real-time capabilities. Notably,\nnnU-Net shows superior robustness on sparse and truncated Doppler data while\nmaintaining independence from explicit boundary conditions. Overall, our\nresults highlight the effectiveness of these methods in reconstructing\nintraventricular vector blood flow. The study also suggests potential\napplications of PINNs in ultrafast color Doppler imaging and the incorporation\nof fluid dynamics equations to derive biomarkers for cardiovascular diseases\nbased on blood flow.\n", "link": "http://arxiv.org/abs/2403.13040v2", "date": "2024-06-27", "relevancy": 2.0945, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5344}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5223}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Guided%20Neural%20Networks%20for%20Intraventricular%20Vector%20Flow%20Mapping&body=Title%3A%20Physics-Guided%20Neural%20Networks%20for%20Intraventricular%20Vector%20Flow%20Mapping%0AAuthor%3A%20Hang%20Jung%20Ling%20and%20Salom%C3%A9%20Bru%20and%20Julia%20Puig%20and%20Florian%20Vix%C3%A8ge%20and%20Simon%20Mendez%20and%20Franck%20Nicoud%20and%20Pierre-Yves%20Courand%20and%20Olivier%20Bernard%20and%20Damien%20Garcia%0AAbstract%3A%20%20%20Intraventricular%20vector%20flow%20mapping%20%28iVFM%29%20seeks%20to%20enhance%20and%20quantify%0Acolor%20Doppler%20in%20cardiac%20imaging.%20In%20this%20study%2C%20we%20propose%20novel%20alternatives%0Ato%20the%20traditional%20iVFM%20optimization%20scheme%20by%20utilizing%20physics-informed%0Aneural%20networks%20%28PINNs%29%20and%20a%20physics-guided%20nnU-Net-based%20supervised%20approach.%0AWhen%20evaluated%20on%20simulated%20color%20Doppler%20images%20derived%20from%20a%0Apatient-specific%20computational%20fluid%20dynamics%20model%20and%20in%20vivo%20Doppler%0Aacquisitions%2C%20both%20approaches%20demonstrate%20comparable%20reconstruction%20performance%0Ato%20the%20original%20iVFM%20algorithm.%20The%20efficiency%20of%20PINNs%20is%20boosted%20through%0Adual-stage%20optimization%20and%20pre-optimized%20weights.%20On%20the%20other%20hand%2C%20the%0AnnU-Net%20method%20excels%20in%20generalizability%20and%20real-time%20capabilities.%20Notably%2C%0AnnU-Net%20shows%20superior%20robustness%20on%20sparse%20and%20truncated%20Doppler%20data%20while%0Amaintaining%20independence%20from%20explicit%20boundary%20conditions.%20Overall%2C%20our%0Aresults%20highlight%20the%20effectiveness%20of%20these%20methods%20in%20reconstructing%0Aintraventricular%20vector%20blood%20flow.%20The%20study%20also%20suggests%20potential%0Aapplications%20of%20PINNs%20in%20ultrafast%20color%20Doppler%20imaging%20and%20the%20incorporation%0Aof%20fluid%20dynamics%20equations%20to%20derive%20biomarkers%20for%20cardiovascular%20diseases%0Abased%20on%20blood%20flow.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13040v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Guided%2520Neural%2520Networks%2520for%2520Intraventricular%2520Vector%2520Flow%2520Mapping%26entry.906535625%3DHang%2520Jung%2520Ling%2520and%2520Salom%25C3%25A9%2520Bru%2520and%2520Julia%2520Puig%2520and%2520Florian%2520Vix%25C3%25A8ge%2520and%2520Simon%2520Mendez%2520and%2520Franck%2520Nicoud%2520and%2520Pierre-Yves%2520Courand%2520and%2520Olivier%2520Bernard%2520and%2520Damien%2520Garcia%26entry.1292438233%3D%2520%2520Intraventricular%2520vector%2520flow%2520mapping%2520%2528iVFM%2529%2520seeks%2520to%2520enhance%2520and%2520quantify%250Acolor%2520Doppler%2520in%2520cardiac%2520imaging.%2520In%2520this%2520study%252C%2520we%2520propose%2520novel%2520alternatives%250Ato%2520the%2520traditional%2520iVFM%2520optimization%2520scheme%2520by%2520utilizing%2520physics-informed%250Aneural%2520networks%2520%2528PINNs%2529%2520and%2520a%2520physics-guided%2520nnU-Net-based%2520supervised%2520approach.%250AWhen%2520evaluated%2520on%2520simulated%2520color%2520Doppler%2520images%2520derived%2520from%2520a%250Apatient-specific%2520computational%2520fluid%2520dynamics%2520model%2520and%2520in%2520vivo%2520Doppler%250Aacquisitions%252C%2520both%2520approaches%2520demonstrate%2520comparable%2520reconstruction%2520performance%250Ato%2520the%2520original%2520iVFM%2520algorithm.%2520The%2520efficiency%2520of%2520PINNs%2520is%2520boosted%2520through%250Adual-stage%2520optimization%2520and%2520pre-optimized%2520weights.%2520On%2520the%2520other%2520hand%252C%2520the%250AnnU-Net%2520method%2520excels%2520in%2520generalizability%2520and%2520real-time%2520capabilities.%2520Notably%252C%250AnnU-Net%2520shows%2520superior%2520robustness%2520on%2520sparse%2520and%2520truncated%2520Doppler%2520data%2520while%250Amaintaining%2520independence%2520from%2520explicit%2520boundary%2520conditions.%2520Overall%252C%2520our%250Aresults%2520highlight%2520the%2520effectiveness%2520of%2520these%2520methods%2520in%2520reconstructing%250Aintraventricular%2520vector%2520blood%2520flow.%2520The%2520study%2520also%2520suggests%2520potential%250Aapplications%2520of%2520PINNs%2520in%2520ultrafast%2520color%2520Doppler%2520imaging%2520and%2520the%2520incorporation%250Aof%2520fluid%2520dynamics%2520equations%2520to%2520derive%2520biomarkers%2520for%2520cardiovascular%2520diseases%250Abased%2520on%2520blood%2520flow.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13040v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Guided%20Neural%20Networks%20for%20Intraventricular%20Vector%20Flow%20Mapping&entry.906535625=Hang%20Jung%20Ling%20and%20Salom%C3%A9%20Bru%20and%20Julia%20Puig%20and%20Florian%20Vix%C3%A8ge%20and%20Simon%20Mendez%20and%20Franck%20Nicoud%20and%20Pierre-Yves%20Courand%20and%20Olivier%20Bernard%20and%20Damien%20Garcia&entry.1292438233=%20%20Intraventricular%20vector%20flow%20mapping%20%28iVFM%29%20seeks%20to%20enhance%20and%20quantify%0Acolor%20Doppler%20in%20cardiac%20imaging.%20In%20this%20study%2C%20we%20propose%20novel%20alternatives%0Ato%20the%20traditional%20iVFM%20optimization%20scheme%20by%20utilizing%20physics-informed%0Aneural%20networks%20%28PINNs%29%20and%20a%20physics-guided%20nnU-Net-based%20supervised%20approach.%0AWhen%20evaluated%20on%20simulated%20color%20Doppler%20images%20derived%20from%20a%0Apatient-specific%20computational%20fluid%20dynamics%20model%20and%20in%20vivo%20Doppler%0Aacquisitions%2C%20both%20approaches%20demonstrate%20comparable%20reconstruction%20performance%0Ato%20the%20original%20iVFM%20algorithm.%20The%20efficiency%20of%20PINNs%20is%20boosted%20through%0Adual-stage%20optimization%20and%20pre-optimized%20weights.%20On%20the%20other%20hand%2C%20the%0AnnU-Net%20method%20excels%20in%20generalizability%20and%20real-time%20capabilities.%20Notably%2C%0AnnU-Net%20shows%20superior%20robustness%20on%20sparse%20and%20truncated%20Doppler%20data%20while%0Amaintaining%20independence%20from%20explicit%20boundary%20conditions.%20Overall%2C%20our%0Aresults%20highlight%20the%20effectiveness%20of%20these%20methods%20in%20reconstructing%0Aintraventricular%20vector%20blood%20flow.%20The%20study%20also%20suggests%20potential%0Aapplications%20of%20PINNs%20in%20ultrafast%20color%20Doppler%20imaging%20and%20the%20incorporation%0Aof%20fluid%20dynamics%20equations%20to%20derive%20biomarkers%20for%20cardiovascular%20diseases%0Abased%20on%20blood%20flow.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13040v2&entry.124074799=Read"},
{"title": "FDLite: A Single Stage Lightweight Face Detector Network", "author": "Yogesh Aggarwal and Prithwijit Guha", "abstract": "  Face detection is frequently attempted by using heavy pre-trained backbone\nnetworks like ResNet-50/101/152 and VGG16/19. Few recent works have also\nproposed lightweight detectors with customized backbones, novel loss functions\nand efficient training strategies. The novelty of this work lies in the design\nof a lightweight detector while training with only the commonly used loss\nfunctions and learning strategies. The proposed face detector grossly follows\nthe established RetinaFace architecture. The first contribution of this work is\nthe design of a customized lightweight backbone network (BLite) having 0.167M\nparameters with 0.52 GFLOPs. The second contribution is the use of two\nindependent multi-task losses. The proposed lightweight face detector (FDLite)\nhas 0.26M parameters with 0.94 GFLOPs. The network is trained on the WIDER FACE\ndataset. FDLite is observed to achieve 92.3\\%, 89.8\\%, and 82.2\\% Average\nPrecision (AP) on the easy, medium, and hard subsets of the WIDER FACE\nvalidation dataset, respectively.\n", "link": "http://arxiv.org/abs/2406.19107v1", "date": "2024-06-27", "relevancy": 2.0736, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5387}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5144}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FDLite%3A%20A%20Single%20Stage%20Lightweight%20Face%20Detector%20Network&body=Title%3A%20FDLite%3A%20A%20Single%20Stage%20Lightweight%20Face%20Detector%20Network%0AAuthor%3A%20Yogesh%20Aggarwal%20and%20Prithwijit%20Guha%0AAbstract%3A%20%20%20Face%20detection%20is%20frequently%20attempted%20by%20using%20heavy%20pre-trained%20backbone%0Anetworks%20like%20ResNet-50/101/152%20and%20VGG16/19.%20Few%20recent%20works%20have%20also%0Aproposed%20lightweight%20detectors%20with%20customized%20backbones%2C%20novel%20loss%20functions%0Aand%20efficient%20training%20strategies.%20The%20novelty%20of%20this%20work%20lies%20in%20the%20design%0Aof%20a%20lightweight%20detector%20while%20training%20with%20only%20the%20commonly%20used%20loss%0Afunctions%20and%20learning%20strategies.%20The%20proposed%20face%20detector%20grossly%20follows%0Athe%20established%20RetinaFace%20architecture.%20The%20first%20contribution%20of%20this%20work%20is%0Athe%20design%20of%20a%20customized%20lightweight%20backbone%20network%20%28BLite%29%20having%200.167M%0Aparameters%20with%200.52%20GFLOPs.%20The%20second%20contribution%20is%20the%20use%20of%20two%0Aindependent%20multi-task%20losses.%20The%20proposed%20lightweight%20face%20detector%20%28FDLite%29%0Ahas%200.26M%20parameters%20with%200.94%20GFLOPs.%20The%20network%20is%20trained%20on%20the%20WIDER%20FACE%0Adataset.%20FDLite%20is%20observed%20to%20achieve%2092.3%5C%25%2C%2089.8%5C%25%2C%20and%2082.2%5C%25%20Average%0APrecision%20%28AP%29%20on%20the%20easy%2C%20medium%2C%20and%20hard%20subsets%20of%20the%20WIDER%20FACE%0Avalidation%20dataset%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFDLite%253A%2520A%2520Single%2520Stage%2520Lightweight%2520Face%2520Detector%2520Network%26entry.906535625%3DYogesh%2520Aggarwal%2520and%2520Prithwijit%2520Guha%26entry.1292438233%3D%2520%2520Face%2520detection%2520is%2520frequently%2520attempted%2520by%2520using%2520heavy%2520pre-trained%2520backbone%250Anetworks%2520like%2520ResNet-50/101/152%2520and%2520VGG16/19.%2520Few%2520recent%2520works%2520have%2520also%250Aproposed%2520lightweight%2520detectors%2520with%2520customized%2520backbones%252C%2520novel%2520loss%2520functions%250Aand%2520efficient%2520training%2520strategies.%2520The%2520novelty%2520of%2520this%2520work%2520lies%2520in%2520the%2520design%250Aof%2520a%2520lightweight%2520detector%2520while%2520training%2520with%2520only%2520the%2520commonly%2520used%2520loss%250Afunctions%2520and%2520learning%2520strategies.%2520The%2520proposed%2520face%2520detector%2520grossly%2520follows%250Athe%2520established%2520RetinaFace%2520architecture.%2520The%2520first%2520contribution%2520of%2520this%2520work%2520is%250Athe%2520design%2520of%2520a%2520customized%2520lightweight%2520backbone%2520network%2520%2528BLite%2529%2520having%25200.167M%250Aparameters%2520with%25200.52%2520GFLOPs.%2520The%2520second%2520contribution%2520is%2520the%2520use%2520of%2520two%250Aindependent%2520multi-task%2520losses.%2520The%2520proposed%2520lightweight%2520face%2520detector%2520%2528FDLite%2529%250Ahas%25200.26M%2520parameters%2520with%25200.94%2520GFLOPs.%2520The%2520network%2520is%2520trained%2520on%2520the%2520WIDER%2520FACE%250Adataset.%2520FDLite%2520is%2520observed%2520to%2520achieve%252092.3%255C%2525%252C%252089.8%255C%2525%252C%2520and%252082.2%255C%2525%2520Average%250APrecision%2520%2528AP%2529%2520on%2520the%2520easy%252C%2520medium%252C%2520and%2520hard%2520subsets%2520of%2520the%2520WIDER%2520FACE%250Avalidation%2520dataset%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FDLite%3A%20A%20Single%20Stage%20Lightweight%20Face%20Detector%20Network&entry.906535625=Yogesh%20Aggarwal%20and%20Prithwijit%20Guha&entry.1292438233=%20%20Face%20detection%20is%20frequently%20attempted%20by%20using%20heavy%20pre-trained%20backbone%0Anetworks%20like%20ResNet-50/101/152%20and%20VGG16/19.%20Few%20recent%20works%20have%20also%0Aproposed%20lightweight%20detectors%20with%20customized%20backbones%2C%20novel%20loss%20functions%0Aand%20efficient%20training%20strategies.%20The%20novelty%20of%20this%20work%20lies%20in%20the%20design%0Aof%20a%20lightweight%20detector%20while%20training%20with%20only%20the%20commonly%20used%20loss%0Afunctions%20and%20learning%20strategies.%20The%20proposed%20face%20detector%20grossly%20follows%0Athe%20established%20RetinaFace%20architecture.%20The%20first%20contribution%20of%20this%20work%20is%0Athe%20design%20of%20a%20customized%20lightweight%20backbone%20network%20%28BLite%29%20having%200.167M%0Aparameters%20with%200.52%20GFLOPs.%20The%20second%20contribution%20is%20the%20use%20of%20two%0Aindependent%20multi-task%20losses.%20The%20proposed%20lightweight%20face%20detector%20%28FDLite%29%0Ahas%200.26M%20parameters%20with%200.94%20GFLOPs.%20The%20network%20is%20trained%20on%20the%20WIDER%20FACE%0Adataset.%20FDLite%20is%20observed%20to%20achieve%2092.3%5C%25%2C%2089.8%5C%25%2C%20and%2082.2%5C%25%20Average%0APrecision%20%28AP%29%20on%20the%20easy%2C%20medium%2C%20and%20hard%20subsets%20of%20the%20WIDER%20FACE%0Avalidation%20dataset%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19107v1&entry.124074799=Read"},
{"title": "PNeRV: A Polynomial Neural Representation for Videos", "author": "Sonam Gupta and Snehal Singh Tomar and Grigorios G Chrysos and Sukhendu Das and A. N. Rajagopalan", "abstract": "  Extracting Implicit Neural Representations (INRs) on video data poses unique\nchallenges due to the additional temporal dimension. In the context of videos,\nINRs have predominantly relied on a frame-only parameterization, which\nsacrifices the spatiotemporal continuity observed in pixel-level (spatial)\nrepresentations. To mitigate this, we introduce Polynomial Neural\nRepresentation for Videos (PNeRV), a parameter-wise efficient, patch-wise INR\nfor videos that preserves spatiotemporal continuity. PNeRV leverages the\nmodeling capabilities of Polynomial Neural Networks to perform the modulation\nof a continuous spatial (patch) signal with a continuous time (frame) signal.\nWe further propose a custom Hierarchical Patch-wise Spatial Sampling Scheme\nthat ensures spatial continuity while retaining parameter efficiency. We also\nemploy a carefully designed Positional Embedding methodology to further enhance\nPNeRV's performance. Our extensive experimentation demonstrates that PNeRV\noutperforms the baselines in conventional Implicit Neural Representation tasks\nlike compression along with downstream applications that require spatiotemporal\ncontinuity in the underlying representation. PNeRV not only addresses the\nchallenges posed by video data in the realm of INRs but also opens new avenues\nfor advanced video processing and analysis.\n", "link": "http://arxiv.org/abs/2406.19299v1", "date": "2024-06-27", "relevancy": 2.0722, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5359}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5054}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PNeRV%3A%20A%20Polynomial%20Neural%20Representation%20for%20Videos&body=Title%3A%20PNeRV%3A%20A%20Polynomial%20Neural%20Representation%20for%20Videos%0AAuthor%3A%20Sonam%20Gupta%20and%20Snehal%20Singh%20Tomar%20and%20Grigorios%20G%20Chrysos%20and%20Sukhendu%20Das%20and%20A.%20N.%20Rajagopalan%0AAbstract%3A%20%20%20Extracting%20Implicit%20Neural%20Representations%20%28INRs%29%20on%20video%20data%20poses%20unique%0Achallenges%20due%20to%20the%20additional%20temporal%20dimension.%20In%20the%20context%20of%20videos%2C%0AINRs%20have%20predominantly%20relied%20on%20a%20frame-only%20parameterization%2C%20which%0Asacrifices%20the%20spatiotemporal%20continuity%20observed%20in%20pixel-level%20%28spatial%29%0Arepresentations.%20To%20mitigate%20this%2C%20we%20introduce%20Polynomial%20Neural%0ARepresentation%20for%20Videos%20%28PNeRV%29%2C%20a%20parameter-wise%20efficient%2C%20patch-wise%20INR%0Afor%20videos%20that%20preserves%20spatiotemporal%20continuity.%20PNeRV%20leverages%20the%0Amodeling%20capabilities%20of%20Polynomial%20Neural%20Networks%20to%20perform%20the%20modulation%0Aof%20a%20continuous%20spatial%20%28patch%29%20signal%20with%20a%20continuous%20time%20%28frame%29%20signal.%0AWe%20further%20propose%20a%20custom%20Hierarchical%20Patch-wise%20Spatial%20Sampling%20Scheme%0Athat%20ensures%20spatial%20continuity%20while%20retaining%20parameter%20efficiency.%20We%20also%0Aemploy%20a%20carefully%20designed%20Positional%20Embedding%20methodology%20to%20further%20enhance%0APNeRV%27s%20performance.%20Our%20extensive%20experimentation%20demonstrates%20that%20PNeRV%0Aoutperforms%20the%20baselines%20in%20conventional%20Implicit%20Neural%20Representation%20tasks%0Alike%20compression%20along%20with%20downstream%20applications%20that%20require%20spatiotemporal%0Acontinuity%20in%20the%20underlying%20representation.%20PNeRV%20not%20only%20addresses%20the%0Achallenges%20posed%20by%20video%20data%20in%20the%20realm%20of%20INRs%20but%20also%20opens%20new%20avenues%0Afor%20advanced%20video%20processing%20and%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19299v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPNeRV%253A%2520A%2520Polynomial%2520Neural%2520Representation%2520for%2520Videos%26entry.906535625%3DSonam%2520Gupta%2520and%2520Snehal%2520Singh%2520Tomar%2520and%2520Grigorios%2520G%2520Chrysos%2520and%2520Sukhendu%2520Das%2520and%2520A.%2520N.%2520Rajagopalan%26entry.1292438233%3D%2520%2520Extracting%2520Implicit%2520Neural%2520Representations%2520%2528INRs%2529%2520on%2520video%2520data%2520poses%2520unique%250Achallenges%2520due%2520to%2520the%2520additional%2520temporal%2520dimension.%2520In%2520the%2520context%2520of%2520videos%252C%250AINRs%2520have%2520predominantly%2520relied%2520on%2520a%2520frame-only%2520parameterization%252C%2520which%250Asacrifices%2520the%2520spatiotemporal%2520continuity%2520observed%2520in%2520pixel-level%2520%2528spatial%2529%250Arepresentations.%2520To%2520mitigate%2520this%252C%2520we%2520introduce%2520Polynomial%2520Neural%250ARepresentation%2520for%2520Videos%2520%2528PNeRV%2529%252C%2520a%2520parameter-wise%2520efficient%252C%2520patch-wise%2520INR%250Afor%2520videos%2520that%2520preserves%2520spatiotemporal%2520continuity.%2520PNeRV%2520leverages%2520the%250Amodeling%2520capabilities%2520of%2520Polynomial%2520Neural%2520Networks%2520to%2520perform%2520the%2520modulation%250Aof%2520a%2520continuous%2520spatial%2520%2528patch%2529%2520signal%2520with%2520a%2520continuous%2520time%2520%2528frame%2529%2520signal.%250AWe%2520further%2520propose%2520a%2520custom%2520Hierarchical%2520Patch-wise%2520Spatial%2520Sampling%2520Scheme%250Athat%2520ensures%2520spatial%2520continuity%2520while%2520retaining%2520parameter%2520efficiency.%2520We%2520also%250Aemploy%2520a%2520carefully%2520designed%2520Positional%2520Embedding%2520methodology%2520to%2520further%2520enhance%250APNeRV%2527s%2520performance.%2520Our%2520extensive%2520experimentation%2520demonstrates%2520that%2520PNeRV%250Aoutperforms%2520the%2520baselines%2520in%2520conventional%2520Implicit%2520Neural%2520Representation%2520tasks%250Alike%2520compression%2520along%2520with%2520downstream%2520applications%2520that%2520require%2520spatiotemporal%250Acontinuity%2520in%2520the%2520underlying%2520representation.%2520PNeRV%2520not%2520only%2520addresses%2520the%250Achallenges%2520posed%2520by%2520video%2520data%2520in%2520the%2520realm%2520of%2520INRs%2520but%2520also%2520opens%2520new%2520avenues%250Afor%2520advanced%2520video%2520processing%2520and%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19299v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PNeRV%3A%20A%20Polynomial%20Neural%20Representation%20for%20Videos&entry.906535625=Sonam%20Gupta%20and%20Snehal%20Singh%20Tomar%20and%20Grigorios%20G%20Chrysos%20and%20Sukhendu%20Das%20and%20A.%20N.%20Rajagopalan&entry.1292438233=%20%20Extracting%20Implicit%20Neural%20Representations%20%28INRs%29%20on%20video%20data%20poses%20unique%0Achallenges%20due%20to%20the%20additional%20temporal%20dimension.%20In%20the%20context%20of%20videos%2C%0AINRs%20have%20predominantly%20relied%20on%20a%20frame-only%20parameterization%2C%20which%0Asacrifices%20the%20spatiotemporal%20continuity%20observed%20in%20pixel-level%20%28spatial%29%0Arepresentations.%20To%20mitigate%20this%2C%20we%20introduce%20Polynomial%20Neural%0ARepresentation%20for%20Videos%20%28PNeRV%29%2C%20a%20parameter-wise%20efficient%2C%20patch-wise%20INR%0Afor%20videos%20that%20preserves%20spatiotemporal%20continuity.%20PNeRV%20leverages%20the%0Amodeling%20capabilities%20of%20Polynomial%20Neural%20Networks%20to%20perform%20the%20modulation%0Aof%20a%20continuous%20spatial%20%28patch%29%20signal%20with%20a%20continuous%20time%20%28frame%29%20signal.%0AWe%20further%20propose%20a%20custom%20Hierarchical%20Patch-wise%20Spatial%20Sampling%20Scheme%0Athat%20ensures%20spatial%20continuity%20while%20retaining%20parameter%20efficiency.%20We%20also%0Aemploy%20a%20carefully%20designed%20Positional%20Embedding%20methodology%20to%20further%20enhance%0APNeRV%27s%20performance.%20Our%20extensive%20experimentation%20demonstrates%20that%20PNeRV%0Aoutperforms%20the%20baselines%20in%20conventional%20Implicit%20Neural%20Representation%20tasks%0Alike%20compression%20along%20with%20downstream%20applications%20that%20require%20spatiotemporal%0Acontinuity%20in%20the%20underlying%20representation.%20PNeRV%20not%20only%20addresses%20the%0Achallenges%20posed%20by%20video%20data%20in%20the%20realm%20of%20INRs%20but%20also%20opens%20new%20avenues%0Afor%20advanced%20video%20processing%20and%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19299v1&entry.124074799=Read"},
{"title": "Token-level Direct Preference Optimization", "author": "Yongcheng Zeng and Guoqing Liu and Weiyu Ma and Ning Yang and Haifeng Zhang and Jun Wang", "abstract": "  Fine-tuning pre-trained Large Language Models (LLMs) is essential to align\nthem with human values and intentions. This process often utilizes methods like\npairwise comparisons and KL divergence against a reference LLM, focusing on the\nevaluation of full answers generated by the models. However, the generation of\nthese responses occurs in a token level, following a sequential,\nauto-regressive fashion. In this paper, we introduce Token-level Direct\nPreference Optimization (TDPO), a novel approach to align LLMs with human\npreferences by optimizing policy at the token level. Unlike previous methods,\nwhich face challenges in divergence efficiency, TDPO incorporates forward KL\ndivergence constraints for each token, improving alignment and diversity.\nUtilizing the Bradley-Terry model for a token-based reward system, TDPO\nenhances the regulation of KL divergence, while preserving simplicity without\nthe need for explicit reward modeling. Experimental results across various text\ntasks demonstrate TDPO's superior performance in balancing alignment with\ngeneration diversity. Notably, fine-tuning with TDPO strikes a better balance\nthan DPO in the controlled sentiment generation and single-turn dialogue\ndatasets, and significantly improves the quality of generated responses\ncompared to both DPO and PPO-based RLHF methods. Our code is open-sourced at\nhttps://github.com/Vance0124/Token-level-Direct-Preference-Optimization.\n", "link": "http://arxiv.org/abs/2404.11999v4", "date": "2024-06-27", "relevancy": 2.0587, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5554}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.491}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token-level%20Direct%20Preference%20Optimization&body=Title%3A%20Token-level%20Direct%20Preference%20Optimization%0AAuthor%3A%20Yongcheng%20Zeng%20and%20Guoqing%20Liu%20and%20Weiyu%20Ma%20and%20Ning%20Yang%20and%20Haifeng%20Zhang%20and%20Jun%20Wang%0AAbstract%3A%20%20%20Fine-tuning%20pre-trained%20Large%20Language%20Models%20%28LLMs%29%20is%20essential%20to%20align%0Athem%20with%20human%20values%20and%20intentions.%20This%20process%20often%20utilizes%20methods%20like%0Apairwise%20comparisons%20and%20KL%20divergence%20against%20a%20reference%20LLM%2C%20focusing%20on%20the%0Aevaluation%20of%20full%20answers%20generated%20by%20the%20models.%20However%2C%20the%20generation%20of%0Athese%20responses%20occurs%20in%20a%20token%20level%2C%20following%20a%20sequential%2C%0Aauto-regressive%20fashion.%20In%20this%20paper%2C%20we%20introduce%20Token-level%20Direct%0APreference%20Optimization%20%28TDPO%29%2C%20a%20novel%20approach%20to%20align%20LLMs%20with%20human%0Apreferences%20by%20optimizing%20policy%20at%20the%20token%20level.%20Unlike%20previous%20methods%2C%0Awhich%20face%20challenges%20in%20divergence%20efficiency%2C%20TDPO%20incorporates%20forward%20KL%0Adivergence%20constraints%20for%20each%20token%2C%20improving%20alignment%20and%20diversity.%0AUtilizing%20the%20Bradley-Terry%20model%20for%20a%20token-based%20reward%20system%2C%20TDPO%0Aenhances%20the%20regulation%20of%20KL%20divergence%2C%20while%20preserving%20simplicity%20without%0Athe%20need%20for%20explicit%20reward%20modeling.%20Experimental%20results%20across%20various%20text%0Atasks%20demonstrate%20TDPO%27s%20superior%20performance%20in%20balancing%20alignment%20with%0Ageneration%20diversity.%20Notably%2C%20fine-tuning%20with%20TDPO%20strikes%20a%20better%20balance%0Athan%20DPO%20in%20the%20controlled%20sentiment%20generation%20and%20single-turn%20dialogue%0Adatasets%2C%20and%20significantly%20improves%20the%20quality%20of%20generated%20responses%0Acompared%20to%20both%20DPO%20and%20PPO-based%20RLHF%20methods.%20Our%20code%20is%20open-sourced%20at%0Ahttps%3A//github.com/Vance0124/Token-level-Direct-Preference-Optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11999v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken-level%2520Direct%2520Preference%2520Optimization%26entry.906535625%3DYongcheng%2520Zeng%2520and%2520Guoqing%2520Liu%2520and%2520Weiyu%2520Ma%2520and%2520Ning%2520Yang%2520and%2520Haifeng%2520Zhang%2520and%2520Jun%2520Wang%26entry.1292438233%3D%2520%2520Fine-tuning%2520pre-trained%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520essential%2520to%2520align%250Athem%2520with%2520human%2520values%2520and%2520intentions.%2520This%2520process%2520often%2520utilizes%2520methods%2520like%250Apairwise%2520comparisons%2520and%2520KL%2520divergence%2520against%2520a%2520reference%2520LLM%252C%2520focusing%2520on%2520the%250Aevaluation%2520of%2520full%2520answers%2520generated%2520by%2520the%2520models.%2520However%252C%2520the%2520generation%2520of%250Athese%2520responses%2520occurs%2520in%2520a%2520token%2520level%252C%2520following%2520a%2520sequential%252C%250Aauto-regressive%2520fashion.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Token-level%2520Direct%250APreference%2520Optimization%2520%2528TDPO%2529%252C%2520a%2520novel%2520approach%2520to%2520align%2520LLMs%2520with%2520human%250Apreferences%2520by%2520optimizing%2520policy%2520at%2520the%2520token%2520level.%2520Unlike%2520previous%2520methods%252C%250Awhich%2520face%2520challenges%2520in%2520divergence%2520efficiency%252C%2520TDPO%2520incorporates%2520forward%2520KL%250Adivergence%2520constraints%2520for%2520each%2520token%252C%2520improving%2520alignment%2520and%2520diversity.%250AUtilizing%2520the%2520Bradley-Terry%2520model%2520for%2520a%2520token-based%2520reward%2520system%252C%2520TDPO%250Aenhances%2520the%2520regulation%2520of%2520KL%2520divergence%252C%2520while%2520preserving%2520simplicity%2520without%250Athe%2520need%2520for%2520explicit%2520reward%2520modeling.%2520Experimental%2520results%2520across%2520various%2520text%250Atasks%2520demonstrate%2520TDPO%2527s%2520superior%2520performance%2520in%2520balancing%2520alignment%2520with%250Ageneration%2520diversity.%2520Notably%252C%2520fine-tuning%2520with%2520TDPO%2520strikes%2520a%2520better%2520balance%250Athan%2520DPO%2520in%2520the%2520controlled%2520sentiment%2520generation%2520and%2520single-turn%2520dialogue%250Adatasets%252C%2520and%2520significantly%2520improves%2520the%2520quality%2520of%2520generated%2520responses%250Acompared%2520to%2520both%2520DPO%2520and%2520PPO-based%2520RLHF%2520methods.%2520Our%2520code%2520is%2520open-sourced%2520at%250Ahttps%253A//github.com/Vance0124/Token-level-Direct-Preference-Optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11999v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token-level%20Direct%20Preference%20Optimization&entry.906535625=Yongcheng%20Zeng%20and%20Guoqing%20Liu%20and%20Weiyu%20Ma%20and%20Ning%20Yang%20and%20Haifeng%20Zhang%20and%20Jun%20Wang&entry.1292438233=%20%20Fine-tuning%20pre-trained%20Large%20Language%20Models%20%28LLMs%29%20is%20essential%20to%20align%0Athem%20with%20human%20values%20and%20intentions.%20This%20process%20often%20utilizes%20methods%20like%0Apairwise%20comparisons%20and%20KL%20divergence%20against%20a%20reference%20LLM%2C%20focusing%20on%20the%0Aevaluation%20of%20full%20answers%20generated%20by%20the%20models.%20However%2C%20the%20generation%20of%0Athese%20responses%20occurs%20in%20a%20token%20level%2C%20following%20a%20sequential%2C%0Aauto-regressive%20fashion.%20In%20this%20paper%2C%20we%20introduce%20Token-level%20Direct%0APreference%20Optimization%20%28TDPO%29%2C%20a%20novel%20approach%20to%20align%20LLMs%20with%20human%0Apreferences%20by%20optimizing%20policy%20at%20the%20token%20level.%20Unlike%20previous%20methods%2C%0Awhich%20face%20challenges%20in%20divergence%20efficiency%2C%20TDPO%20incorporates%20forward%20KL%0Adivergence%20constraints%20for%20each%20token%2C%20improving%20alignment%20and%20diversity.%0AUtilizing%20the%20Bradley-Terry%20model%20for%20a%20token-based%20reward%20system%2C%20TDPO%0Aenhances%20the%20regulation%20of%20KL%20divergence%2C%20while%20preserving%20simplicity%20without%0Athe%20need%20for%20explicit%20reward%20modeling.%20Experimental%20results%20across%20various%20text%0Atasks%20demonstrate%20TDPO%27s%20superior%20performance%20in%20balancing%20alignment%20with%0Ageneration%20diversity.%20Notably%2C%20fine-tuning%20with%20TDPO%20strikes%20a%20better%20balance%0Athan%20DPO%20in%20the%20controlled%20sentiment%20generation%20and%20single-turn%20dialogue%0Adatasets%2C%20and%20significantly%20improves%20the%20quality%20of%20generated%20responses%0Acompared%20to%20both%20DPO%20and%20PPO-based%20RLHF%20methods.%20Our%20code%20is%20open-sourced%20at%0Ahttps%3A//github.com/Vance0124/Token-level-Direct-Preference-Optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11999v4&entry.124074799=Read"},
{"title": "Hack Me If You Can: Aggregating AutoEncoders for Countering Persistent\n  Access Threats Within Highly Imbalanced Data", "author": "Sidahmed Benabderrahmane and Ngoc Hoang and Petko Valtchev and James Cheney and Talal Rahwan", "abstract": "  Advanced Persistent Threats (APTs) are sophisticated, targeted cyberattacks\ndesigned to gain unauthorized access to systems and remain undetected for\nextended periods. To evade detection, APT cyberattacks deceive defense layers\nwith breaches and exploits, thereby complicating exposure by traditional\nanomaly detection-based security methods. The challenge of detecting APTs with\nmachine learning is compounded by the rarity of relevant datasets and the\nsignificant imbalance in the data, which makes the detection process highly\nburdensome. We present AE-APT, a deep learning-based tool for APT detection\nthat features a family of AutoEncoder methods ranging from a basic one to a\nTransformer-based one. We evaluated our tool on a suite of provenance trace\ndatabases produced by the DARPA Transparent Computing program, where APT-like\nattacks constitute as little as 0.004% of the data. The datasets span multiple\noperating systems, including Android, Linux, BSD, and Windows, and cover two\nattack scenarios. The outcomes showed that AE-APT has significantly higher\ndetection rates compared to its competitors, indicating superior performance in\ndetecting and ranking anomalies.\n", "link": "http://arxiv.org/abs/2406.19220v1", "date": "2024-06-27", "relevancy": 2.0513, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5278}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5068}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hack%20Me%20If%20You%20Can%3A%20Aggregating%20AutoEncoders%20for%20Countering%20Persistent%0A%20%20Access%20Threats%20Within%20Highly%20Imbalanced%20Data&body=Title%3A%20Hack%20Me%20If%20You%20Can%3A%20Aggregating%20AutoEncoders%20for%20Countering%20Persistent%0A%20%20Access%20Threats%20Within%20Highly%20Imbalanced%20Data%0AAuthor%3A%20Sidahmed%20Benabderrahmane%20and%20Ngoc%20Hoang%20and%20Petko%20Valtchev%20and%20James%20Cheney%20and%20Talal%20Rahwan%0AAbstract%3A%20%20%20Advanced%20Persistent%20Threats%20%28APTs%29%20are%20sophisticated%2C%20targeted%20cyberattacks%0Adesigned%20to%20gain%20unauthorized%20access%20to%20systems%20and%20remain%20undetected%20for%0Aextended%20periods.%20To%20evade%20detection%2C%20APT%20cyberattacks%20deceive%20defense%20layers%0Awith%20breaches%20and%20exploits%2C%20thereby%20complicating%20exposure%20by%20traditional%0Aanomaly%20detection-based%20security%20methods.%20The%20challenge%20of%20detecting%20APTs%20with%0Amachine%20learning%20is%20compounded%20by%20the%20rarity%20of%20relevant%20datasets%20and%20the%0Asignificant%20imbalance%20in%20the%20data%2C%20which%20makes%20the%20detection%20process%20highly%0Aburdensome.%20We%20present%20AE-APT%2C%20a%20deep%20learning-based%20tool%20for%20APT%20detection%0Athat%20features%20a%20family%20of%20AutoEncoder%20methods%20ranging%20from%20a%20basic%20one%20to%20a%0ATransformer-based%20one.%20We%20evaluated%20our%20tool%20on%20a%20suite%20of%20provenance%20trace%0Adatabases%20produced%20by%20the%20DARPA%20Transparent%20Computing%20program%2C%20where%20APT-like%0Aattacks%20constitute%20as%20little%20as%200.004%25%20of%20the%20data.%20The%20datasets%20span%20multiple%0Aoperating%20systems%2C%20including%20Android%2C%20Linux%2C%20BSD%2C%20and%20Windows%2C%20and%20cover%20two%0Aattack%20scenarios.%20The%20outcomes%20showed%20that%20AE-APT%20has%20significantly%20higher%0Adetection%20rates%20compared%20to%20its%20competitors%2C%20indicating%20superior%20performance%20in%0Adetecting%20and%20ranking%20anomalies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHack%2520Me%2520If%2520You%2520Can%253A%2520Aggregating%2520AutoEncoders%2520for%2520Countering%2520Persistent%250A%2520%2520Access%2520Threats%2520Within%2520Highly%2520Imbalanced%2520Data%26entry.906535625%3DSidahmed%2520Benabderrahmane%2520and%2520Ngoc%2520Hoang%2520and%2520Petko%2520Valtchev%2520and%2520James%2520Cheney%2520and%2520Talal%2520Rahwan%26entry.1292438233%3D%2520%2520Advanced%2520Persistent%2520Threats%2520%2528APTs%2529%2520are%2520sophisticated%252C%2520targeted%2520cyberattacks%250Adesigned%2520to%2520gain%2520unauthorized%2520access%2520to%2520systems%2520and%2520remain%2520undetected%2520for%250Aextended%2520periods.%2520To%2520evade%2520detection%252C%2520APT%2520cyberattacks%2520deceive%2520defense%2520layers%250Awith%2520breaches%2520and%2520exploits%252C%2520thereby%2520complicating%2520exposure%2520by%2520traditional%250Aanomaly%2520detection-based%2520security%2520methods.%2520The%2520challenge%2520of%2520detecting%2520APTs%2520with%250Amachine%2520learning%2520is%2520compounded%2520by%2520the%2520rarity%2520of%2520relevant%2520datasets%2520and%2520the%250Asignificant%2520imbalance%2520in%2520the%2520data%252C%2520which%2520makes%2520the%2520detection%2520process%2520highly%250Aburdensome.%2520We%2520present%2520AE-APT%252C%2520a%2520deep%2520learning-based%2520tool%2520for%2520APT%2520detection%250Athat%2520features%2520a%2520family%2520of%2520AutoEncoder%2520methods%2520ranging%2520from%2520a%2520basic%2520one%2520to%2520a%250ATransformer-based%2520one.%2520We%2520evaluated%2520our%2520tool%2520on%2520a%2520suite%2520of%2520provenance%2520trace%250Adatabases%2520produced%2520by%2520the%2520DARPA%2520Transparent%2520Computing%2520program%252C%2520where%2520APT-like%250Aattacks%2520constitute%2520as%2520little%2520as%25200.004%2525%2520of%2520the%2520data.%2520The%2520datasets%2520span%2520multiple%250Aoperating%2520systems%252C%2520including%2520Android%252C%2520Linux%252C%2520BSD%252C%2520and%2520Windows%252C%2520and%2520cover%2520two%250Aattack%2520scenarios.%2520The%2520outcomes%2520showed%2520that%2520AE-APT%2520has%2520significantly%2520higher%250Adetection%2520rates%2520compared%2520to%2520its%2520competitors%252C%2520indicating%2520superior%2520performance%2520in%250Adetecting%2520and%2520ranking%2520anomalies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hack%20Me%20If%20You%20Can%3A%20Aggregating%20AutoEncoders%20for%20Countering%20Persistent%0A%20%20Access%20Threats%20Within%20Highly%20Imbalanced%20Data&entry.906535625=Sidahmed%20Benabderrahmane%20and%20Ngoc%20Hoang%20and%20Petko%20Valtchev%20and%20James%20Cheney%20and%20Talal%20Rahwan&entry.1292438233=%20%20Advanced%20Persistent%20Threats%20%28APTs%29%20are%20sophisticated%2C%20targeted%20cyberattacks%0Adesigned%20to%20gain%20unauthorized%20access%20to%20systems%20and%20remain%20undetected%20for%0Aextended%20periods.%20To%20evade%20detection%2C%20APT%20cyberattacks%20deceive%20defense%20layers%0Awith%20breaches%20and%20exploits%2C%20thereby%20complicating%20exposure%20by%20traditional%0Aanomaly%20detection-based%20security%20methods.%20The%20challenge%20of%20detecting%20APTs%20with%0Amachine%20learning%20is%20compounded%20by%20the%20rarity%20of%20relevant%20datasets%20and%20the%0Asignificant%20imbalance%20in%20the%20data%2C%20which%20makes%20the%20detection%20process%20highly%0Aburdensome.%20We%20present%20AE-APT%2C%20a%20deep%20learning-based%20tool%20for%20APT%20detection%0Athat%20features%20a%20family%20of%20AutoEncoder%20methods%20ranging%20from%20a%20basic%20one%20to%20a%0ATransformer-based%20one.%20We%20evaluated%20our%20tool%20on%20a%20suite%20of%20provenance%20trace%0Adatabases%20produced%20by%20the%20DARPA%20Transparent%20Computing%20program%2C%20where%20APT-like%0Aattacks%20constitute%20as%20little%20as%200.004%25%20of%20the%20data.%20The%20datasets%20span%20multiple%0Aoperating%20systems%2C%20including%20Android%2C%20Linux%2C%20BSD%2C%20and%20Windows%2C%20and%20cover%20two%0Aattack%20scenarios.%20The%20outcomes%20showed%20that%20AE-APT%20has%20significantly%20higher%0Adetection%20rates%20compared%20to%20its%20competitors%2C%20indicating%20superior%20performance%20in%0Adetecting%20and%20ranking%20anomalies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19220v1&entry.124074799=Read"},
{"title": "Towards Semantic Equivalence of Tokenization in Multimodal LLM", "author": "Shengqiong Wu and Hao Fei and Xiangtai Li and Jiayi Ji and Hanwang Zhang and Tat-Seng Chua and Shuicheng Yan", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated exceptional\ncapabilities in processing vision-language tasks. One of the crux of MLLMs lies\nin vision tokenization, which involves efficiently transforming input visual\nsignals into feature representations that are most beneficial for LLMs.\nHowever, existing vision tokenizers, essential for semantic alignment between\nvision and language, remain problematic. Existing methods aggressively fragment\nvisual input, corrupting the visual semantic integrity. To address this, this\npaper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok),\nwhich groups visual features into semantic units via a dynamic clustering\nalgorithm, flexibly determining the number of tokens based on image complexity.\nThe resulting vision tokens effectively preserve semantic integrity and capture\nboth low-frequency and high-frequency visual features. The proposed MLLM\n(Setokim) equipped with SeTok significantly demonstrates superior performance\nacross various tasks, as evidenced by our experimental results. The project\npage is at https://chocowu.github.io/SeTok-web/.\n", "link": "http://arxiv.org/abs/2406.05127v2", "date": "2024-06-27", "relevancy": 2.0455, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5221}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5058}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Semantic%20Equivalence%20of%20Tokenization%20in%20Multimodal%20LLM&body=Title%3A%20Towards%20Semantic%20Equivalence%20of%20Tokenization%20in%20Multimodal%20LLM%0AAuthor%3A%20Shengqiong%20Wu%20and%20Hao%20Fei%20and%20Xiangtai%20Li%20and%20Jiayi%20Ji%20and%20Hanwang%20Zhang%20and%20Tat-Seng%20Chua%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20exceptional%0Acapabilities%20in%20processing%20vision-language%20tasks.%20One%20of%20the%20crux%20of%20MLLMs%20lies%0Ain%20vision%20tokenization%2C%20which%20involves%20efficiently%20transforming%20input%20visual%0Asignals%20into%20feature%20representations%20that%20are%20most%20beneficial%20for%20LLMs.%0AHowever%2C%20existing%20vision%20tokenizers%2C%20essential%20for%20semantic%20alignment%20between%0Avision%20and%20language%2C%20remain%20problematic.%20Existing%20methods%20aggressively%20fragment%0Avisual%20input%2C%20corrupting%20the%20visual%20semantic%20integrity.%20To%20address%20this%2C%20this%0Apaper%20proposes%20a%20novel%20dynamic%20Semantic-Equivalent%20Vision%20Tokenizer%20%28SeTok%29%2C%0Awhich%20groups%20visual%20features%20into%20semantic%20units%20via%20a%20dynamic%20clustering%0Aalgorithm%2C%20flexibly%20determining%20the%20number%20of%20tokens%20based%20on%20image%20complexity.%0AThe%20resulting%20vision%20tokens%20effectively%20preserve%20semantic%20integrity%20and%20capture%0Aboth%20low-frequency%20and%20high-frequency%20visual%20features.%20The%20proposed%20MLLM%0A%28Setokim%29%20equipped%20with%20SeTok%20significantly%20demonstrates%20superior%20performance%0Aacross%20various%20tasks%2C%20as%20evidenced%20by%20our%20experimental%20results.%20The%20project%0Apage%20is%20at%20https%3A//chocowu.github.io/SeTok-web/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05127v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Semantic%2520Equivalence%2520of%2520Tokenization%2520in%2520Multimodal%2520LLM%26entry.906535625%3DShengqiong%2520Wu%2520and%2520Hao%2520Fei%2520and%2520Xiangtai%2520Li%2520and%2520Jiayi%2520Ji%2520and%2520Hanwang%2520Zhang%2520and%2520Tat-Seng%2520Chua%2520and%2520Shuicheng%2520Yan%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520exceptional%250Acapabilities%2520in%2520processing%2520vision-language%2520tasks.%2520One%2520of%2520the%2520crux%2520of%2520MLLMs%2520lies%250Ain%2520vision%2520tokenization%252C%2520which%2520involves%2520efficiently%2520transforming%2520input%2520visual%250Asignals%2520into%2520feature%2520representations%2520that%2520are%2520most%2520beneficial%2520for%2520LLMs.%250AHowever%252C%2520existing%2520vision%2520tokenizers%252C%2520essential%2520for%2520semantic%2520alignment%2520between%250Avision%2520and%2520language%252C%2520remain%2520problematic.%2520Existing%2520methods%2520aggressively%2520fragment%250Avisual%2520input%252C%2520corrupting%2520the%2520visual%2520semantic%2520integrity.%2520To%2520address%2520this%252C%2520this%250Apaper%2520proposes%2520a%2520novel%2520dynamic%2520Semantic-Equivalent%2520Vision%2520Tokenizer%2520%2528SeTok%2529%252C%250Awhich%2520groups%2520visual%2520features%2520into%2520semantic%2520units%2520via%2520a%2520dynamic%2520clustering%250Aalgorithm%252C%2520flexibly%2520determining%2520the%2520number%2520of%2520tokens%2520based%2520on%2520image%2520complexity.%250AThe%2520resulting%2520vision%2520tokens%2520effectively%2520preserve%2520semantic%2520integrity%2520and%2520capture%250Aboth%2520low-frequency%2520and%2520high-frequency%2520visual%2520features.%2520The%2520proposed%2520MLLM%250A%2528Setokim%2529%2520equipped%2520with%2520SeTok%2520significantly%2520demonstrates%2520superior%2520performance%250Aacross%2520various%2520tasks%252C%2520as%2520evidenced%2520by%2520our%2520experimental%2520results.%2520The%2520project%250Apage%2520is%2520at%2520https%253A//chocowu.github.io/SeTok-web/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05127v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Semantic%20Equivalence%20of%20Tokenization%20in%20Multimodal%20LLM&entry.906535625=Shengqiong%20Wu%20and%20Hao%20Fei%20and%20Xiangtai%20Li%20and%20Jiayi%20Ji%20and%20Hanwang%20Zhang%20and%20Tat-Seng%20Chua%20and%20Shuicheng%20Yan&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20exceptional%0Acapabilities%20in%20processing%20vision-language%20tasks.%20One%20of%20the%20crux%20of%20MLLMs%20lies%0Ain%20vision%20tokenization%2C%20which%20involves%20efficiently%20transforming%20input%20visual%0Asignals%20into%20feature%20representations%20that%20are%20most%20beneficial%20for%20LLMs.%0AHowever%2C%20existing%20vision%20tokenizers%2C%20essential%20for%20semantic%20alignment%20between%0Avision%20and%20language%2C%20remain%20problematic.%20Existing%20methods%20aggressively%20fragment%0Avisual%20input%2C%20corrupting%20the%20visual%20semantic%20integrity.%20To%20address%20this%2C%20this%0Apaper%20proposes%20a%20novel%20dynamic%20Semantic-Equivalent%20Vision%20Tokenizer%20%28SeTok%29%2C%0Awhich%20groups%20visual%20features%20into%20semantic%20units%20via%20a%20dynamic%20clustering%0Aalgorithm%2C%20flexibly%20determining%20the%20number%20of%20tokens%20based%20on%20image%20complexity.%0AThe%20resulting%20vision%20tokens%20effectively%20preserve%20semantic%20integrity%20and%20capture%0Aboth%20low-frequency%20and%20high-frequency%20visual%20features.%20The%20proposed%20MLLM%0A%28Setokim%29%20equipped%20with%20SeTok%20significantly%20demonstrates%20superior%20performance%0Aacross%20various%20tasks%2C%20as%20evidenced%20by%20our%20experimental%20results.%20The%20project%0Apage%20is%20at%20https%3A//chocowu.github.io/SeTok-web/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05127v2&entry.124074799=Read"},
{"title": "Physics-informed and Unsupervised Riemannian Domain Adaptation for\n  Machine Learning on Heterogeneous EEG Datasets", "author": "Apolline Mellot and Antoine Collas and Sylvain Chevallier and Denis Engemann and Alexandre Gramfort", "abstract": "  Combining electroencephalogram (EEG) datasets for supervised machine learning\n(ML) is challenging due to session, subject, and device variability. ML\nalgorithms typically require identical features at train and test time,\ncomplicating analysis due to varying sensor numbers and positions across\ndatasets. Simple channel selection discards valuable data, leading to poorer\nperformance, especially with datasets sharing few channels. To address this, we\npropose an unsupervised approach leveraging EEG signal physics. We map EEG\nchannels to fixed positions using field interpolation, facilitating source-free\ndomain adaptation. Leveraging Riemannian geometry classification pipelines and\ntransfer learning steps, our method demonstrates robust performance in\nbrain-computer interface (BCI) tasks and potential biomarker applications.\nComparative analysis against a statistical-based approach known as\nDimensionality Transcending, a signal-based imputation called ComImp,\nsource-dependent methods, as well as common channel selection and spherical\nspline interpolation, was conducted with leave-one-dataset-out validation on\nsix public BCI datasets for a right-hand/left-hand classification task.\nNumerical experiments show that in the presence of few shared channels in train\nand test, the field interpolation consistently outperforms other methods,\ndemonstrating enhanced classification performance across all datasets. When\nmore channels are shared, field interpolation was found to be competitive with\nother methods and faster to compute than source-dependent methods.\n", "link": "http://arxiv.org/abs/2403.15415v2", "date": "2024-06-27", "relevancy": 2.0426, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5437}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5057}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-informed%20and%20Unsupervised%20Riemannian%20Domain%20Adaptation%20for%0A%20%20Machine%20Learning%20on%20Heterogeneous%20EEG%20Datasets&body=Title%3A%20Physics-informed%20and%20Unsupervised%20Riemannian%20Domain%20Adaptation%20for%0A%20%20Machine%20Learning%20on%20Heterogeneous%20EEG%20Datasets%0AAuthor%3A%20Apolline%20Mellot%20and%20Antoine%20Collas%20and%20Sylvain%20Chevallier%20and%20Denis%20Engemann%20and%20Alexandre%20Gramfort%0AAbstract%3A%20%20%20Combining%20electroencephalogram%20%28EEG%29%20datasets%20for%20supervised%20machine%20learning%0A%28ML%29%20is%20challenging%20due%20to%20session%2C%20subject%2C%20and%20device%20variability.%20ML%0Aalgorithms%20typically%20require%20identical%20features%20at%20train%20and%20test%20time%2C%0Acomplicating%20analysis%20due%20to%20varying%20sensor%20numbers%20and%20positions%20across%0Adatasets.%20Simple%20channel%20selection%20discards%20valuable%20data%2C%20leading%20to%20poorer%0Aperformance%2C%20especially%20with%20datasets%20sharing%20few%20channels.%20To%20address%20this%2C%20we%0Apropose%20an%20unsupervised%20approach%20leveraging%20EEG%20signal%20physics.%20We%20map%20EEG%0Achannels%20to%20fixed%20positions%20using%20field%20interpolation%2C%20facilitating%20source-free%0Adomain%20adaptation.%20Leveraging%20Riemannian%20geometry%20classification%20pipelines%20and%0Atransfer%20learning%20steps%2C%20our%20method%20demonstrates%20robust%20performance%20in%0Abrain-computer%20interface%20%28BCI%29%20tasks%20and%20potential%20biomarker%20applications.%0AComparative%20analysis%20against%20a%20statistical-based%20approach%20known%20as%0ADimensionality%20Transcending%2C%20a%20signal-based%20imputation%20called%20ComImp%2C%0Asource-dependent%20methods%2C%20as%20well%20as%20common%20channel%20selection%20and%20spherical%0Aspline%20interpolation%2C%20was%20conducted%20with%20leave-one-dataset-out%20validation%20on%0Asix%20public%20BCI%20datasets%20for%20a%20right-hand/left-hand%20classification%20task.%0ANumerical%20experiments%20show%20that%20in%20the%20presence%20of%20few%20shared%20channels%20in%20train%0Aand%20test%2C%20the%20field%20interpolation%20consistently%20outperforms%20other%20methods%2C%0Ademonstrating%20enhanced%20classification%20performance%20across%20all%20datasets.%20When%0Amore%20channels%20are%20shared%2C%20field%20interpolation%20was%20found%20to%20be%20competitive%20with%0Aother%20methods%20and%20faster%20to%20compute%20than%20source-dependent%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15415v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-informed%2520and%2520Unsupervised%2520Riemannian%2520Domain%2520Adaptation%2520for%250A%2520%2520Machine%2520Learning%2520on%2520Heterogeneous%2520EEG%2520Datasets%26entry.906535625%3DApolline%2520Mellot%2520and%2520Antoine%2520Collas%2520and%2520Sylvain%2520Chevallier%2520and%2520Denis%2520Engemann%2520and%2520Alexandre%2520Gramfort%26entry.1292438233%3D%2520%2520Combining%2520electroencephalogram%2520%2528EEG%2529%2520datasets%2520for%2520supervised%2520machine%2520learning%250A%2528ML%2529%2520is%2520challenging%2520due%2520to%2520session%252C%2520subject%252C%2520and%2520device%2520variability.%2520ML%250Aalgorithms%2520typically%2520require%2520identical%2520features%2520at%2520train%2520and%2520test%2520time%252C%250Acomplicating%2520analysis%2520due%2520to%2520varying%2520sensor%2520numbers%2520and%2520positions%2520across%250Adatasets.%2520Simple%2520channel%2520selection%2520discards%2520valuable%2520data%252C%2520leading%2520to%2520poorer%250Aperformance%252C%2520especially%2520with%2520datasets%2520sharing%2520few%2520channels.%2520To%2520address%2520this%252C%2520we%250Apropose%2520an%2520unsupervised%2520approach%2520leveraging%2520EEG%2520signal%2520physics.%2520We%2520map%2520EEG%250Achannels%2520to%2520fixed%2520positions%2520using%2520field%2520interpolation%252C%2520facilitating%2520source-free%250Adomain%2520adaptation.%2520Leveraging%2520Riemannian%2520geometry%2520classification%2520pipelines%2520and%250Atransfer%2520learning%2520steps%252C%2520our%2520method%2520demonstrates%2520robust%2520performance%2520in%250Abrain-computer%2520interface%2520%2528BCI%2529%2520tasks%2520and%2520potential%2520biomarker%2520applications.%250AComparative%2520analysis%2520against%2520a%2520statistical-based%2520approach%2520known%2520as%250ADimensionality%2520Transcending%252C%2520a%2520signal-based%2520imputation%2520called%2520ComImp%252C%250Asource-dependent%2520methods%252C%2520as%2520well%2520as%2520common%2520channel%2520selection%2520and%2520spherical%250Aspline%2520interpolation%252C%2520was%2520conducted%2520with%2520leave-one-dataset-out%2520validation%2520on%250Asix%2520public%2520BCI%2520datasets%2520for%2520a%2520right-hand/left-hand%2520classification%2520task.%250ANumerical%2520experiments%2520show%2520that%2520in%2520the%2520presence%2520of%2520few%2520shared%2520channels%2520in%2520train%250Aand%2520test%252C%2520the%2520field%2520interpolation%2520consistently%2520outperforms%2520other%2520methods%252C%250Ademonstrating%2520enhanced%2520classification%2520performance%2520across%2520all%2520datasets.%2520When%250Amore%2520channels%2520are%2520shared%252C%2520field%2520interpolation%2520was%2520found%2520to%2520be%2520competitive%2520with%250Aother%2520methods%2520and%2520faster%2520to%2520compute%2520than%2520source-dependent%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15415v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-informed%20and%20Unsupervised%20Riemannian%20Domain%20Adaptation%20for%0A%20%20Machine%20Learning%20on%20Heterogeneous%20EEG%20Datasets&entry.906535625=Apolline%20Mellot%20and%20Antoine%20Collas%20and%20Sylvain%20Chevallier%20and%20Denis%20Engemann%20and%20Alexandre%20Gramfort&entry.1292438233=%20%20Combining%20electroencephalogram%20%28EEG%29%20datasets%20for%20supervised%20machine%20learning%0A%28ML%29%20is%20challenging%20due%20to%20session%2C%20subject%2C%20and%20device%20variability.%20ML%0Aalgorithms%20typically%20require%20identical%20features%20at%20train%20and%20test%20time%2C%0Acomplicating%20analysis%20due%20to%20varying%20sensor%20numbers%20and%20positions%20across%0Adatasets.%20Simple%20channel%20selection%20discards%20valuable%20data%2C%20leading%20to%20poorer%0Aperformance%2C%20especially%20with%20datasets%20sharing%20few%20channels.%20To%20address%20this%2C%20we%0Apropose%20an%20unsupervised%20approach%20leveraging%20EEG%20signal%20physics.%20We%20map%20EEG%0Achannels%20to%20fixed%20positions%20using%20field%20interpolation%2C%20facilitating%20source-free%0Adomain%20adaptation.%20Leveraging%20Riemannian%20geometry%20classification%20pipelines%20and%0Atransfer%20learning%20steps%2C%20our%20method%20demonstrates%20robust%20performance%20in%0Abrain-computer%20interface%20%28BCI%29%20tasks%20and%20potential%20biomarker%20applications.%0AComparative%20analysis%20against%20a%20statistical-based%20approach%20known%20as%0ADimensionality%20Transcending%2C%20a%20signal-based%20imputation%20called%20ComImp%2C%0Asource-dependent%20methods%2C%20as%20well%20as%20common%20channel%20selection%20and%20spherical%0Aspline%20interpolation%2C%20was%20conducted%20with%20leave-one-dataset-out%20validation%20on%0Asix%20public%20BCI%20datasets%20for%20a%20right-hand/left-hand%20classification%20task.%0ANumerical%20experiments%20show%20that%20in%20the%20presence%20of%20few%20shared%20channels%20in%20train%0Aand%20test%2C%20the%20field%20interpolation%20consistently%20outperforms%20other%20methods%2C%0Ademonstrating%20enhanced%20classification%20performance%20across%20all%20datasets.%20When%0Amore%20channels%20are%20shared%2C%20field%20interpolation%20was%20found%20to%20be%20competitive%20with%0Aother%20methods%20and%20faster%20to%20compute%20than%20source-dependent%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15415v2&entry.124074799=Read"},
{"title": "Super-resolution imaging using super-oscillatory diffractive neural\n  networks", "author": "Hang Chen and Sheng Gao and Zejia Zhao and Zhengyang Duan and Haiou Zhang and Gordon Wetzstein and Xing Lin", "abstract": "  Optical super-oscillation enables far-field super-resolution imaging beyond\ndiffraction limits. However, the existing super-oscillatory lens for the\nspatial super-resolution imaging system still confronts critical limitations in\nperformance due to the lack of a more advanced design method and the limited\ndesign degree of freedom. Here, we propose an optical super-oscillatory\ndiffractive neural network, i.e., SODNN, that can achieve super-resolved\nspatial resolution for imaging beyond the diffraction limit with superior\nperformance over existing methods. SODNN is constructed by utilizing\ndiffractive layers to implement optical interconnections and imaging samples or\nbiological sensors to implement nonlinearity, which modulates the incident\noptical field to create optical super-oscillation effects in 3D space and\ngenerate the super-resolved focal spots. By optimizing diffractive layers with\n3D optical field constraints under an incident wavelength size of $\\lambda$, we\nachieved a super-oscillatory spot with a full width at half maximum of\n0.407$\\lambda$ in the far field distance over 400$\\lambda$ without side-lobes\nover the field of view, having a long depth of field over 10$\\lambda$.\nFurthermore, the SODNN implements a multi-wavelength and multi-focus spot array\nthat effectively avoids chromatic aberrations. Our research work will inspire\nthe development of intelligent optical instruments to facilitate the\napplications of imaging, sensing, perception, etc.\n", "link": "http://arxiv.org/abs/2406.19126v1", "date": "2024-06-27", "relevancy": 2.0217, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5526}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5004}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Super-resolution%20imaging%20using%20super-oscillatory%20diffractive%20neural%0A%20%20networks&body=Title%3A%20Super-resolution%20imaging%20using%20super-oscillatory%20diffractive%20neural%0A%20%20networks%0AAuthor%3A%20Hang%20Chen%20and%20Sheng%20Gao%20and%20Zejia%20Zhao%20and%20Zhengyang%20Duan%20and%20Haiou%20Zhang%20and%20Gordon%20Wetzstein%20and%20Xing%20Lin%0AAbstract%3A%20%20%20Optical%20super-oscillation%20enables%20far-field%20super-resolution%20imaging%20beyond%0Adiffraction%20limits.%20However%2C%20the%20existing%20super-oscillatory%20lens%20for%20the%0Aspatial%20super-resolution%20imaging%20system%20still%20confronts%20critical%20limitations%20in%0Aperformance%20due%20to%20the%20lack%20of%20a%20more%20advanced%20design%20method%20and%20the%20limited%0Adesign%20degree%20of%20freedom.%20Here%2C%20we%20propose%20an%20optical%20super-oscillatory%0Adiffractive%20neural%20network%2C%20i.e.%2C%20SODNN%2C%20that%20can%20achieve%20super-resolved%0Aspatial%20resolution%20for%20imaging%20beyond%20the%20diffraction%20limit%20with%20superior%0Aperformance%20over%20existing%20methods.%20SODNN%20is%20constructed%20by%20utilizing%0Adiffractive%20layers%20to%20implement%20optical%20interconnections%20and%20imaging%20samples%20or%0Abiological%20sensors%20to%20implement%20nonlinearity%2C%20which%20modulates%20the%20incident%0Aoptical%20field%20to%20create%20optical%20super-oscillation%20effects%20in%203D%20space%20and%0Agenerate%20the%20super-resolved%20focal%20spots.%20By%20optimizing%20diffractive%20layers%20with%0A3D%20optical%20field%20constraints%20under%20an%20incident%20wavelength%20size%20of%20%24%5Clambda%24%2C%20we%0Aachieved%20a%20super-oscillatory%20spot%20with%20a%20full%20width%20at%20half%20maximum%20of%0A0.407%24%5Clambda%24%20in%20the%20far%20field%20distance%20over%20400%24%5Clambda%24%20without%20side-lobes%0Aover%20the%20field%20of%20view%2C%20having%20a%20long%20depth%20of%20field%20over%2010%24%5Clambda%24.%0AFurthermore%2C%20the%20SODNN%20implements%20a%20multi-wavelength%20and%20multi-focus%20spot%20array%0Athat%20effectively%20avoids%20chromatic%20aberrations.%20Our%20research%20work%20will%20inspire%0Athe%20development%20of%20intelligent%20optical%20instruments%20to%20facilitate%20the%0Aapplications%20of%20imaging%2C%20sensing%2C%20perception%2C%20etc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19126v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuper-resolution%2520imaging%2520using%2520super-oscillatory%2520diffractive%2520neural%250A%2520%2520networks%26entry.906535625%3DHang%2520Chen%2520and%2520Sheng%2520Gao%2520and%2520Zejia%2520Zhao%2520and%2520Zhengyang%2520Duan%2520and%2520Haiou%2520Zhang%2520and%2520Gordon%2520Wetzstein%2520and%2520Xing%2520Lin%26entry.1292438233%3D%2520%2520Optical%2520super-oscillation%2520enables%2520far-field%2520super-resolution%2520imaging%2520beyond%250Adiffraction%2520limits.%2520However%252C%2520the%2520existing%2520super-oscillatory%2520lens%2520for%2520the%250Aspatial%2520super-resolution%2520imaging%2520system%2520still%2520confronts%2520critical%2520limitations%2520in%250Aperformance%2520due%2520to%2520the%2520lack%2520of%2520a%2520more%2520advanced%2520design%2520method%2520and%2520the%2520limited%250Adesign%2520degree%2520of%2520freedom.%2520Here%252C%2520we%2520propose%2520an%2520optical%2520super-oscillatory%250Adiffractive%2520neural%2520network%252C%2520i.e.%252C%2520SODNN%252C%2520that%2520can%2520achieve%2520super-resolved%250Aspatial%2520resolution%2520for%2520imaging%2520beyond%2520the%2520diffraction%2520limit%2520with%2520superior%250Aperformance%2520over%2520existing%2520methods.%2520SODNN%2520is%2520constructed%2520by%2520utilizing%250Adiffractive%2520layers%2520to%2520implement%2520optical%2520interconnections%2520and%2520imaging%2520samples%2520or%250Abiological%2520sensors%2520to%2520implement%2520nonlinearity%252C%2520which%2520modulates%2520the%2520incident%250Aoptical%2520field%2520to%2520create%2520optical%2520super-oscillation%2520effects%2520in%25203D%2520space%2520and%250Agenerate%2520the%2520super-resolved%2520focal%2520spots.%2520By%2520optimizing%2520diffractive%2520layers%2520with%250A3D%2520optical%2520field%2520constraints%2520under%2520an%2520incident%2520wavelength%2520size%2520of%2520%2524%255Clambda%2524%252C%2520we%250Aachieved%2520a%2520super-oscillatory%2520spot%2520with%2520a%2520full%2520width%2520at%2520half%2520maximum%2520of%250A0.407%2524%255Clambda%2524%2520in%2520the%2520far%2520field%2520distance%2520over%2520400%2524%255Clambda%2524%2520without%2520side-lobes%250Aover%2520the%2520field%2520of%2520view%252C%2520having%2520a%2520long%2520depth%2520of%2520field%2520over%252010%2524%255Clambda%2524.%250AFurthermore%252C%2520the%2520SODNN%2520implements%2520a%2520multi-wavelength%2520and%2520multi-focus%2520spot%2520array%250Athat%2520effectively%2520avoids%2520chromatic%2520aberrations.%2520Our%2520research%2520work%2520will%2520inspire%250Athe%2520development%2520of%2520intelligent%2520optical%2520instruments%2520to%2520facilitate%2520the%250Aapplications%2520of%2520imaging%252C%2520sensing%252C%2520perception%252C%2520etc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19126v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Super-resolution%20imaging%20using%20super-oscillatory%20diffractive%20neural%0A%20%20networks&entry.906535625=Hang%20Chen%20and%20Sheng%20Gao%20and%20Zejia%20Zhao%20and%20Zhengyang%20Duan%20and%20Haiou%20Zhang%20and%20Gordon%20Wetzstein%20and%20Xing%20Lin&entry.1292438233=%20%20Optical%20super-oscillation%20enables%20far-field%20super-resolution%20imaging%20beyond%0Adiffraction%20limits.%20However%2C%20the%20existing%20super-oscillatory%20lens%20for%20the%0Aspatial%20super-resolution%20imaging%20system%20still%20confronts%20critical%20limitations%20in%0Aperformance%20due%20to%20the%20lack%20of%20a%20more%20advanced%20design%20method%20and%20the%20limited%0Adesign%20degree%20of%20freedom.%20Here%2C%20we%20propose%20an%20optical%20super-oscillatory%0Adiffractive%20neural%20network%2C%20i.e.%2C%20SODNN%2C%20that%20can%20achieve%20super-resolved%0Aspatial%20resolution%20for%20imaging%20beyond%20the%20diffraction%20limit%20with%20superior%0Aperformance%20over%20existing%20methods.%20SODNN%20is%20constructed%20by%20utilizing%0Adiffractive%20layers%20to%20implement%20optical%20interconnections%20and%20imaging%20samples%20or%0Abiological%20sensors%20to%20implement%20nonlinearity%2C%20which%20modulates%20the%20incident%0Aoptical%20field%20to%20create%20optical%20super-oscillation%20effects%20in%203D%20space%20and%0Agenerate%20the%20super-resolved%20focal%20spots.%20By%20optimizing%20diffractive%20layers%20with%0A3D%20optical%20field%20constraints%20under%20an%20incident%20wavelength%20size%20of%20%24%5Clambda%24%2C%20we%0Aachieved%20a%20super-oscillatory%20spot%20with%20a%20full%20width%20at%20half%20maximum%20of%0A0.407%24%5Clambda%24%20in%20the%20far%20field%20distance%20over%20400%24%5Clambda%24%20without%20side-lobes%0Aover%20the%20field%20of%20view%2C%20having%20a%20long%20depth%20of%20field%20over%2010%24%5Clambda%24.%0AFurthermore%2C%20the%20SODNN%20implements%20a%20multi-wavelength%20and%20multi-focus%20spot%20array%0Athat%20effectively%20avoids%20chromatic%20aberrations.%20Our%20research%20work%20will%20inspire%0Athe%20development%20of%20intelligent%20optical%20instruments%20to%20facilitate%20the%0Aapplications%20of%20imaging%2C%20sensing%2C%20perception%2C%20etc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19126v1&entry.124074799=Read"},
{"title": "Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens\n  Grounding", "author": "Yue Fan and Lei Ding and Ching-Chen Kuo and Shan Jiang and Yang Zhao and Xinze Guan and Jie Yang and Yi Zhang and Xin Eric Wang", "abstract": "  Graphical User Interfaces (GUIs) are central to our interaction with digital\ndevices. Recently, growing efforts have been made to build models for various\nGUI understanding tasks. However, these efforts largely overlook an important\nGUI-referring task: screen reading based on user-indicated points, which we\nname the Screen Point-and-Read (SPR) task. This task is predominantly handled\nby rigid accessible screen reading tools, in great need of new models driven by\nadvancements in Multimodal Large Language Models (MLLMs). In this paper, we\npropose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,\nto address the SPR task. Based on the input point coordinate and the\ncorresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout\nTree. Based on the tree, our ToL agent not only comprehends the content of the\nindicated area but also articulates the layout and spatial relationships\nbetween elements. Such layout information is crucial for accurately\ninterpreting information on the screen, distinguishing our ToL agent from other\nscreen reading tools. We also thoroughly evaluate the ToL agent against other\nbaselines on a newly proposed SPR benchmark, which includes GUIs from mobile,\nweb, and operating systems. Last but not least, we test the ToL agent on mobile\nGUI navigation tasks, demonstrating its utility in identifying incorrect\nactions along the path of agent execution trajectories. Code and data:\nscreen-point-and-read.github.io\n", "link": "http://arxiv.org/abs/2406.19263v1", "date": "2024-06-27", "relevancy": 2.0199, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5123}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5005}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Read%20Anywhere%20Pointed%3A%20Layout-aware%20GUI%20Screen%20Reading%20with%20Tree-of-Lens%0A%20%20Grounding&body=Title%3A%20Read%20Anywhere%20Pointed%3A%20Layout-aware%20GUI%20Screen%20Reading%20with%20Tree-of-Lens%0A%20%20Grounding%0AAuthor%3A%20Yue%20Fan%20and%20Lei%20Ding%20and%20Ching-Chen%20Kuo%20and%20Shan%20Jiang%20and%20Yang%20Zhao%20and%20Xinze%20Guan%20and%20Jie%20Yang%20and%20Yi%20Zhang%20and%20Xin%20Eric%20Wang%0AAbstract%3A%20%20%20Graphical%20User%20Interfaces%20%28GUIs%29%20are%20central%20to%20our%20interaction%20with%20digital%0Adevices.%20Recently%2C%20growing%20efforts%20have%20been%20made%20to%20build%20models%20for%20various%0AGUI%20understanding%20tasks.%20However%2C%20these%20efforts%20largely%20overlook%20an%20important%0AGUI-referring%20task%3A%20screen%20reading%20based%20on%20user-indicated%20points%2C%20which%20we%0Aname%20the%20Screen%20Point-and-Read%20%28SPR%29%20task.%20This%20task%20is%20predominantly%20handled%0Aby%20rigid%20accessible%20screen%20reading%20tools%2C%20in%20great%20need%20of%20new%20models%20driven%20by%0Aadvancements%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20In%20this%20paper%2C%20we%0Apropose%20a%20Tree-of-Lens%20%28ToL%29%20agent%2C%20utilizing%20a%20novel%20ToL%20grounding%20mechanism%2C%0Ato%20address%20the%20SPR%20task.%20Based%20on%20the%20input%20point%20coordinate%20and%20the%0Acorresponding%20GUI%20screenshot%2C%20our%20ToL%20agent%20constructs%20a%20Hierarchical%20Layout%0ATree.%20Based%20on%20the%20tree%2C%20our%20ToL%20agent%20not%20only%20comprehends%20the%20content%20of%20the%0Aindicated%20area%20but%20also%20articulates%20the%20layout%20and%20spatial%20relationships%0Abetween%20elements.%20Such%20layout%20information%20is%20crucial%20for%20accurately%0Ainterpreting%20information%20on%20the%20screen%2C%20distinguishing%20our%20ToL%20agent%20from%20other%0Ascreen%20reading%20tools.%20We%20also%20thoroughly%20evaluate%20the%20ToL%20agent%20against%20other%0Abaselines%20on%20a%20newly%20proposed%20SPR%20benchmark%2C%20which%20includes%20GUIs%20from%20mobile%2C%0Aweb%2C%20and%20operating%20systems.%20Last%20but%20not%20least%2C%20we%20test%20the%20ToL%20agent%20on%20mobile%0AGUI%20navigation%20tasks%2C%20demonstrating%20its%20utility%20in%20identifying%20incorrect%0Aactions%20along%20the%20path%20of%20agent%20execution%20trajectories.%20Code%20and%20data%3A%0Ascreen-point-and-read.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRead%2520Anywhere%2520Pointed%253A%2520Layout-aware%2520GUI%2520Screen%2520Reading%2520with%2520Tree-of-Lens%250A%2520%2520Grounding%26entry.906535625%3DYue%2520Fan%2520and%2520Lei%2520Ding%2520and%2520Ching-Chen%2520Kuo%2520and%2520Shan%2520Jiang%2520and%2520Yang%2520Zhao%2520and%2520Xinze%2520Guan%2520and%2520Jie%2520Yang%2520and%2520Yi%2520Zhang%2520and%2520Xin%2520Eric%2520Wang%26entry.1292438233%3D%2520%2520Graphical%2520User%2520Interfaces%2520%2528GUIs%2529%2520are%2520central%2520to%2520our%2520interaction%2520with%2520digital%250Adevices.%2520Recently%252C%2520growing%2520efforts%2520have%2520been%2520made%2520to%2520build%2520models%2520for%2520various%250AGUI%2520understanding%2520tasks.%2520However%252C%2520these%2520efforts%2520largely%2520overlook%2520an%2520important%250AGUI-referring%2520task%253A%2520screen%2520reading%2520based%2520on%2520user-indicated%2520points%252C%2520which%2520we%250Aname%2520the%2520Screen%2520Point-and-Read%2520%2528SPR%2529%2520task.%2520This%2520task%2520is%2520predominantly%2520handled%250Aby%2520rigid%2520accessible%2520screen%2520reading%2520tools%252C%2520in%2520great%2520need%2520of%2520new%2520models%2520driven%2520by%250Aadvancements%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520Tree-of-Lens%2520%2528ToL%2529%2520agent%252C%2520utilizing%2520a%2520novel%2520ToL%2520grounding%2520mechanism%252C%250Ato%2520address%2520the%2520SPR%2520task.%2520Based%2520on%2520the%2520input%2520point%2520coordinate%2520and%2520the%250Acorresponding%2520GUI%2520screenshot%252C%2520our%2520ToL%2520agent%2520constructs%2520a%2520Hierarchical%2520Layout%250ATree.%2520Based%2520on%2520the%2520tree%252C%2520our%2520ToL%2520agent%2520not%2520only%2520comprehends%2520the%2520content%2520of%2520the%250Aindicated%2520area%2520but%2520also%2520articulates%2520the%2520layout%2520and%2520spatial%2520relationships%250Abetween%2520elements.%2520Such%2520layout%2520information%2520is%2520crucial%2520for%2520accurately%250Ainterpreting%2520information%2520on%2520the%2520screen%252C%2520distinguishing%2520our%2520ToL%2520agent%2520from%2520other%250Ascreen%2520reading%2520tools.%2520We%2520also%2520thoroughly%2520evaluate%2520the%2520ToL%2520agent%2520against%2520other%250Abaselines%2520on%2520a%2520newly%2520proposed%2520SPR%2520benchmark%252C%2520which%2520includes%2520GUIs%2520from%2520mobile%252C%250Aweb%252C%2520and%2520operating%2520systems.%2520Last%2520but%2520not%2520least%252C%2520we%2520test%2520the%2520ToL%2520agent%2520on%2520mobile%250AGUI%2520navigation%2520tasks%252C%2520demonstrating%2520its%2520utility%2520in%2520identifying%2520incorrect%250Aactions%2520along%2520the%2520path%2520of%2520agent%2520execution%2520trajectories.%2520Code%2520and%2520data%253A%250Ascreen-point-and-read.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Read%20Anywhere%20Pointed%3A%20Layout-aware%20GUI%20Screen%20Reading%20with%20Tree-of-Lens%0A%20%20Grounding&entry.906535625=Yue%20Fan%20and%20Lei%20Ding%20and%20Ching-Chen%20Kuo%20and%20Shan%20Jiang%20and%20Yang%20Zhao%20and%20Xinze%20Guan%20and%20Jie%20Yang%20and%20Yi%20Zhang%20and%20Xin%20Eric%20Wang&entry.1292438233=%20%20Graphical%20User%20Interfaces%20%28GUIs%29%20are%20central%20to%20our%20interaction%20with%20digital%0Adevices.%20Recently%2C%20growing%20efforts%20have%20been%20made%20to%20build%20models%20for%20various%0AGUI%20understanding%20tasks.%20However%2C%20these%20efforts%20largely%20overlook%20an%20important%0AGUI-referring%20task%3A%20screen%20reading%20based%20on%20user-indicated%20points%2C%20which%20we%0Aname%20the%20Screen%20Point-and-Read%20%28SPR%29%20task.%20This%20task%20is%20predominantly%20handled%0Aby%20rigid%20accessible%20screen%20reading%20tools%2C%20in%20great%20need%20of%20new%20models%20driven%20by%0Aadvancements%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20In%20this%20paper%2C%20we%0Apropose%20a%20Tree-of-Lens%20%28ToL%29%20agent%2C%20utilizing%20a%20novel%20ToL%20grounding%20mechanism%2C%0Ato%20address%20the%20SPR%20task.%20Based%20on%20the%20input%20point%20coordinate%20and%20the%0Acorresponding%20GUI%20screenshot%2C%20our%20ToL%20agent%20constructs%20a%20Hierarchical%20Layout%0ATree.%20Based%20on%20the%20tree%2C%20our%20ToL%20agent%20not%20only%20comprehends%20the%20content%20of%20the%0Aindicated%20area%20but%20also%20articulates%20the%20layout%20and%20spatial%20relationships%0Abetween%20elements.%20Such%20layout%20information%20is%20crucial%20for%20accurately%0Ainterpreting%20information%20on%20the%20screen%2C%20distinguishing%20our%20ToL%20agent%20from%20other%0Ascreen%20reading%20tools.%20We%20also%20thoroughly%20evaluate%20the%20ToL%20agent%20against%20other%0Abaselines%20on%20a%20newly%20proposed%20SPR%20benchmark%2C%20which%20includes%20GUIs%20from%20mobile%2C%0Aweb%2C%20and%20operating%20systems.%20Last%20but%20not%20least%2C%20we%20test%20the%20ToL%20agent%20on%20mobile%0AGUI%20navigation%20tasks%2C%20demonstrating%20its%20utility%20in%20identifying%20incorrect%0Aactions%20along%20the%20path%20of%20agent%20execution%20trajectories.%20Code%20and%20data%3A%0Ascreen-point-and-read.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19263v1&entry.124074799=Read"},
{"title": "Leveraging Contrastive Learning for Enhanced Node Representations in\n  Tokenized Graph Transformers", "author": "Jinsong Chen and Hanpeng Liu and John E. Hopcroft and Kun He", "abstract": "  While tokenized graph Transformers have demonstrated strong performance in\nnode classification tasks, their reliance on a limited subset of nodes with\nhigh similarity scores for constructing token sequences overlooks valuable\ninformation from other nodes, hindering their ability to fully harness graph\ninformation for learning optimal node representations. To address this\nlimitation, we propose a novel graph Transformer called GCFormer. Unlike\nprevious approaches, GCFormer develops a hybrid token generator to create two\ntypes of token sequences, positive and negative, to capture diverse graph\ninformation. And a tailored Transformer-based backbone is adopted to learn\nmeaningful node representations from these generated token sequences.\nAdditionally, GCFormer introduces contrastive learning to extract valuable\ninformation from both positive and negative token sequences, enhancing the\nquality of learned node representations. Extensive experimental results across\nvarious datasets, including homophily and heterophily graphs, demonstrate the\nsuperiority of GCFormer in node classification, when compared to representative\ngraph neural networks (GNNs) and graph Transformers.\n", "link": "http://arxiv.org/abs/2406.19258v1", "date": "2024-06-27", "relevancy": 2.0132, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5295}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5027}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Contrastive%20Learning%20for%20Enhanced%20Node%20Representations%20in%0A%20%20Tokenized%20Graph%20Transformers&body=Title%3A%20Leveraging%20Contrastive%20Learning%20for%20Enhanced%20Node%20Representations%20in%0A%20%20Tokenized%20Graph%20Transformers%0AAuthor%3A%20Jinsong%20Chen%20and%20Hanpeng%20Liu%20and%20John%20E.%20Hopcroft%20and%20Kun%20He%0AAbstract%3A%20%20%20While%20tokenized%20graph%20Transformers%20have%20demonstrated%20strong%20performance%20in%0Anode%20classification%20tasks%2C%20their%20reliance%20on%20a%20limited%20subset%20of%20nodes%20with%0Ahigh%20similarity%20scores%20for%20constructing%20token%20sequences%20overlooks%20valuable%0Ainformation%20from%20other%20nodes%2C%20hindering%20their%20ability%20to%20fully%20harness%20graph%0Ainformation%20for%20learning%20optimal%20node%20representations.%20To%20address%20this%0Alimitation%2C%20we%20propose%20a%20novel%20graph%20Transformer%20called%20GCFormer.%20Unlike%0Aprevious%20approaches%2C%20GCFormer%20develops%20a%20hybrid%20token%20generator%20to%20create%20two%0Atypes%20of%20token%20sequences%2C%20positive%20and%20negative%2C%20to%20capture%20diverse%20graph%0Ainformation.%20And%20a%20tailored%20Transformer-based%20backbone%20is%20adopted%20to%20learn%0Ameaningful%20node%20representations%20from%20these%20generated%20token%20sequences.%0AAdditionally%2C%20GCFormer%20introduces%20contrastive%20learning%20to%20extract%20valuable%0Ainformation%20from%20both%20positive%20and%20negative%20token%20sequences%2C%20enhancing%20the%0Aquality%20of%20learned%20node%20representations.%20Extensive%20experimental%20results%20across%0Avarious%20datasets%2C%20including%20homophily%20and%20heterophily%20graphs%2C%20demonstrate%20the%0Asuperiority%20of%20GCFormer%20in%20node%20classification%2C%20when%20compared%20to%20representative%0Agraph%20neural%20networks%20%28GNNs%29%20and%20graph%20Transformers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Contrastive%2520Learning%2520for%2520Enhanced%2520Node%2520Representations%2520in%250A%2520%2520Tokenized%2520Graph%2520Transformers%26entry.906535625%3DJinsong%2520Chen%2520and%2520Hanpeng%2520Liu%2520and%2520John%2520E.%2520Hopcroft%2520and%2520Kun%2520He%26entry.1292438233%3D%2520%2520While%2520tokenized%2520graph%2520Transformers%2520have%2520demonstrated%2520strong%2520performance%2520in%250Anode%2520classification%2520tasks%252C%2520their%2520reliance%2520on%2520a%2520limited%2520subset%2520of%2520nodes%2520with%250Ahigh%2520similarity%2520scores%2520for%2520constructing%2520token%2520sequences%2520overlooks%2520valuable%250Ainformation%2520from%2520other%2520nodes%252C%2520hindering%2520their%2520ability%2520to%2520fully%2520harness%2520graph%250Ainformation%2520for%2520learning%2520optimal%2520node%2520representations.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520a%2520novel%2520graph%2520Transformer%2520called%2520GCFormer.%2520Unlike%250Aprevious%2520approaches%252C%2520GCFormer%2520develops%2520a%2520hybrid%2520token%2520generator%2520to%2520create%2520two%250Atypes%2520of%2520token%2520sequences%252C%2520positive%2520and%2520negative%252C%2520to%2520capture%2520diverse%2520graph%250Ainformation.%2520And%2520a%2520tailored%2520Transformer-based%2520backbone%2520is%2520adopted%2520to%2520learn%250Ameaningful%2520node%2520representations%2520from%2520these%2520generated%2520token%2520sequences.%250AAdditionally%252C%2520GCFormer%2520introduces%2520contrastive%2520learning%2520to%2520extract%2520valuable%250Ainformation%2520from%2520both%2520positive%2520and%2520negative%2520token%2520sequences%252C%2520enhancing%2520the%250Aquality%2520of%2520learned%2520node%2520representations.%2520Extensive%2520experimental%2520results%2520across%250Avarious%2520datasets%252C%2520including%2520homophily%2520and%2520heterophily%2520graphs%252C%2520demonstrate%2520the%250Asuperiority%2520of%2520GCFormer%2520in%2520node%2520classification%252C%2520when%2520compared%2520to%2520representative%250Agraph%2520neural%2520networks%2520%2528GNNs%2529%2520and%2520graph%2520Transformers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Contrastive%20Learning%20for%20Enhanced%20Node%20Representations%20in%0A%20%20Tokenized%20Graph%20Transformers&entry.906535625=Jinsong%20Chen%20and%20Hanpeng%20Liu%20and%20John%20E.%20Hopcroft%20and%20Kun%20He&entry.1292438233=%20%20While%20tokenized%20graph%20Transformers%20have%20demonstrated%20strong%20performance%20in%0Anode%20classification%20tasks%2C%20their%20reliance%20on%20a%20limited%20subset%20of%20nodes%20with%0Ahigh%20similarity%20scores%20for%20constructing%20token%20sequences%20overlooks%20valuable%0Ainformation%20from%20other%20nodes%2C%20hindering%20their%20ability%20to%20fully%20harness%20graph%0Ainformation%20for%20learning%20optimal%20node%20representations.%20To%20address%20this%0Alimitation%2C%20we%20propose%20a%20novel%20graph%20Transformer%20called%20GCFormer.%20Unlike%0Aprevious%20approaches%2C%20GCFormer%20develops%20a%20hybrid%20token%20generator%20to%20create%20two%0Atypes%20of%20token%20sequences%2C%20positive%20and%20negative%2C%20to%20capture%20diverse%20graph%0Ainformation.%20And%20a%20tailored%20Transformer-based%20backbone%20is%20adopted%20to%20learn%0Ameaningful%20node%20representations%20from%20these%20generated%20token%20sequences.%0AAdditionally%2C%20GCFormer%20introduces%20contrastive%20learning%20to%20extract%20valuable%0Ainformation%20from%20both%20positive%20and%20negative%20token%20sequences%2C%20enhancing%20the%0Aquality%20of%20learned%20node%20representations.%20Extensive%20experimental%20results%20across%0Avarious%20datasets%2C%20including%20homophily%20and%20heterophily%20graphs%2C%20demonstrate%20the%0Asuperiority%20of%20GCFormer%20in%20node%20classification%2C%20when%20compared%20to%20representative%0Agraph%20neural%20networks%20%28GNNs%29%20and%20graph%20Transformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19258v1&entry.124074799=Read"},
{"title": "LiverUSRecon: Automatic 3D Reconstruction and Volumetry of the Liver\n  with a Few Partial Ultrasound Scans", "author": "Kaushalya Sivayogaraj and Sahan T. Guruge and Udari Liyanage and Jeevani Udupihille and Saroj Jayasinghe and Gerard Fernando and Ranga Rodrigo and M. Rukshani Liyanaarachchi", "abstract": "  3D reconstruction of the liver for volumetry is important for qualitative\nanalysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,\nalthough advantageous due to less acquisition time and safety, is challenging\ndue to the inherent noisiness in US scans, blurry boundaries, and partial liver\nvisibility. We address these challenges by using the segmentation masks of a\nfew incomplete sagittal-plane US scans of the liver in conjunction with a\nstatistical shape model (SSM) built using a set of CT scans of the liver. We\ncompute the shape parameters needed to warp this canonical SSM to fit the US\nscans through a parametric regression network. The resulting 3D liver\nreconstruction is accurate and leads to automatic liver volume calculation. We\nevaluate the accuracy of the estimated liver volumes with respect to CT\nsegmentation volumes using RMSE. Our volume computation is statistically much\ncloser to the volume estimated using CT scans than the volume computed using\nChilds' method by radiologists: p-value of 0.094 (>0.05) says that there is no\nsignificant difference between CT segmentation volumes and ours in contrast to\nChilds' method. We validate our method using investigations (ablation studies)\non the US image resolution, the number of CT scans used for SSM, the number of\nprincipal components, and the number of input US scans. To the best of our\nknowledge, this is the first automatic liver volumetry system using a few\nincomplete US scans given a set of CT scans of livers for SSM.\n", "link": "http://arxiv.org/abs/2406.19336v1", "date": "2024-06-27", "relevancy": 1.9979, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5145}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4973}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiverUSRecon%3A%20Automatic%203D%20Reconstruction%20and%20Volumetry%20of%20the%20Liver%0A%20%20with%20a%20Few%20Partial%20Ultrasound%20Scans&body=Title%3A%20LiverUSRecon%3A%20Automatic%203D%20Reconstruction%20and%20Volumetry%20of%20the%20Liver%0A%20%20with%20a%20Few%20Partial%20Ultrasound%20Scans%0AAuthor%3A%20Kaushalya%20Sivayogaraj%20and%20Sahan%20T.%20Guruge%20and%20Udari%20Liyanage%20and%20Jeevani%20Udupihille%20and%20Saroj%20Jayasinghe%20and%20Gerard%20Fernando%20and%20Ranga%20Rodrigo%20and%20M.%20Rukshani%20Liyanaarachchi%0AAbstract%3A%20%20%203D%20reconstruction%20of%20the%20liver%20for%20volumetry%20is%20important%20for%20qualitative%0Aanalysis%20and%20disease%20diagnosis.%20Liver%20volumetry%20using%20ultrasound%20%28US%29%20scans%2C%0Aalthough%20advantageous%20due%20to%20less%20acquisition%20time%20and%20safety%2C%20is%20challenging%0Adue%20to%20the%20inherent%20noisiness%20in%20US%20scans%2C%20blurry%20boundaries%2C%20and%20partial%20liver%0Avisibility.%20We%20address%20these%20challenges%20by%20using%20the%20segmentation%20masks%20of%20a%0Afew%20incomplete%20sagittal-plane%20US%20scans%20of%20the%20liver%20in%20conjunction%20with%20a%0Astatistical%20shape%20model%20%28SSM%29%20built%20using%20a%20set%20of%20CT%20scans%20of%20the%20liver.%20We%0Acompute%20the%20shape%20parameters%20needed%20to%20warp%20this%20canonical%20SSM%20to%20fit%20the%20US%0Ascans%20through%20a%20parametric%20regression%20network.%20The%20resulting%203D%20liver%0Areconstruction%20is%20accurate%20and%20leads%20to%20automatic%20liver%20volume%20calculation.%20We%0Aevaluate%20the%20accuracy%20of%20the%20estimated%20liver%20volumes%20with%20respect%20to%20CT%0Asegmentation%20volumes%20using%20RMSE.%20Our%20volume%20computation%20is%20statistically%20much%0Acloser%20to%20the%20volume%20estimated%20using%20CT%20scans%20than%20the%20volume%20computed%20using%0AChilds%27%20method%20by%20radiologists%3A%20p-value%20of%200.094%20%28%3E0.05%29%20says%20that%20there%20is%20no%0Asignificant%20difference%20between%20CT%20segmentation%20volumes%20and%20ours%20in%20contrast%20to%0AChilds%27%20method.%20We%20validate%20our%20method%20using%20investigations%20%28ablation%20studies%29%0Aon%20the%20US%20image%20resolution%2C%20the%20number%20of%20CT%20scans%20used%20for%20SSM%2C%20the%20number%20of%0Aprincipal%20components%2C%20and%20the%20number%20of%20input%20US%20scans.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20automatic%20liver%20volumetry%20system%20using%20a%20few%0Aincomplete%20US%20scans%20given%20a%20set%20of%20CT%20scans%20of%20livers%20for%20SSM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiverUSRecon%253A%2520Automatic%25203D%2520Reconstruction%2520and%2520Volumetry%2520of%2520the%2520Liver%250A%2520%2520with%2520a%2520Few%2520Partial%2520Ultrasound%2520Scans%26entry.906535625%3DKaushalya%2520Sivayogaraj%2520and%2520Sahan%2520T.%2520Guruge%2520and%2520Udari%2520Liyanage%2520and%2520Jeevani%2520Udupihille%2520and%2520Saroj%2520Jayasinghe%2520and%2520Gerard%2520Fernando%2520and%2520Ranga%2520Rodrigo%2520and%2520M.%2520Rukshani%2520Liyanaarachchi%26entry.1292438233%3D%2520%25203D%2520reconstruction%2520of%2520the%2520liver%2520for%2520volumetry%2520is%2520important%2520for%2520qualitative%250Aanalysis%2520and%2520disease%2520diagnosis.%2520Liver%2520volumetry%2520using%2520ultrasound%2520%2528US%2529%2520scans%252C%250Aalthough%2520advantageous%2520due%2520to%2520less%2520acquisition%2520time%2520and%2520safety%252C%2520is%2520challenging%250Adue%2520to%2520the%2520inherent%2520noisiness%2520in%2520US%2520scans%252C%2520blurry%2520boundaries%252C%2520and%2520partial%2520liver%250Avisibility.%2520We%2520address%2520these%2520challenges%2520by%2520using%2520the%2520segmentation%2520masks%2520of%2520a%250Afew%2520incomplete%2520sagittal-plane%2520US%2520scans%2520of%2520the%2520liver%2520in%2520conjunction%2520with%2520a%250Astatistical%2520shape%2520model%2520%2528SSM%2529%2520built%2520using%2520a%2520set%2520of%2520CT%2520scans%2520of%2520the%2520liver.%2520We%250Acompute%2520the%2520shape%2520parameters%2520needed%2520to%2520warp%2520this%2520canonical%2520SSM%2520to%2520fit%2520the%2520US%250Ascans%2520through%2520a%2520parametric%2520regression%2520network.%2520The%2520resulting%25203D%2520liver%250Areconstruction%2520is%2520accurate%2520and%2520leads%2520to%2520automatic%2520liver%2520volume%2520calculation.%2520We%250Aevaluate%2520the%2520accuracy%2520of%2520the%2520estimated%2520liver%2520volumes%2520with%2520respect%2520to%2520CT%250Asegmentation%2520volumes%2520using%2520RMSE.%2520Our%2520volume%2520computation%2520is%2520statistically%2520much%250Acloser%2520to%2520the%2520volume%2520estimated%2520using%2520CT%2520scans%2520than%2520the%2520volume%2520computed%2520using%250AChilds%2527%2520method%2520by%2520radiologists%253A%2520p-value%2520of%25200.094%2520%2528%253E0.05%2529%2520says%2520that%2520there%2520is%2520no%250Asignificant%2520difference%2520between%2520CT%2520segmentation%2520volumes%2520and%2520ours%2520in%2520contrast%2520to%250AChilds%2527%2520method.%2520We%2520validate%2520our%2520method%2520using%2520investigations%2520%2528ablation%2520studies%2529%250Aon%2520the%2520US%2520image%2520resolution%252C%2520the%2520number%2520of%2520CT%2520scans%2520used%2520for%2520SSM%252C%2520the%2520number%2520of%250Aprincipal%2520components%252C%2520and%2520the%2520number%2520of%2520input%2520US%2520scans.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520automatic%2520liver%2520volumetry%2520system%2520using%2520a%2520few%250Aincomplete%2520US%2520scans%2520given%2520a%2520set%2520of%2520CT%2520scans%2520of%2520livers%2520for%2520SSM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiverUSRecon%3A%20Automatic%203D%20Reconstruction%20and%20Volumetry%20of%20the%20Liver%0A%20%20with%20a%20Few%20Partial%20Ultrasound%20Scans&entry.906535625=Kaushalya%20Sivayogaraj%20and%20Sahan%20T.%20Guruge%20and%20Udari%20Liyanage%20and%20Jeevani%20Udupihille%20and%20Saroj%20Jayasinghe%20and%20Gerard%20Fernando%20and%20Ranga%20Rodrigo%20and%20M.%20Rukshani%20Liyanaarachchi&entry.1292438233=%20%203D%20reconstruction%20of%20the%20liver%20for%20volumetry%20is%20important%20for%20qualitative%0Aanalysis%20and%20disease%20diagnosis.%20Liver%20volumetry%20using%20ultrasound%20%28US%29%20scans%2C%0Aalthough%20advantageous%20due%20to%20less%20acquisition%20time%20and%20safety%2C%20is%20challenging%0Adue%20to%20the%20inherent%20noisiness%20in%20US%20scans%2C%20blurry%20boundaries%2C%20and%20partial%20liver%0Avisibility.%20We%20address%20these%20challenges%20by%20using%20the%20segmentation%20masks%20of%20a%0Afew%20incomplete%20sagittal-plane%20US%20scans%20of%20the%20liver%20in%20conjunction%20with%20a%0Astatistical%20shape%20model%20%28SSM%29%20built%20using%20a%20set%20of%20CT%20scans%20of%20the%20liver.%20We%0Acompute%20the%20shape%20parameters%20needed%20to%20warp%20this%20canonical%20SSM%20to%20fit%20the%20US%0Ascans%20through%20a%20parametric%20regression%20network.%20The%20resulting%203D%20liver%0Areconstruction%20is%20accurate%20and%20leads%20to%20automatic%20liver%20volume%20calculation.%20We%0Aevaluate%20the%20accuracy%20of%20the%20estimated%20liver%20volumes%20with%20respect%20to%20CT%0Asegmentation%20volumes%20using%20RMSE.%20Our%20volume%20computation%20is%20statistically%20much%0Acloser%20to%20the%20volume%20estimated%20using%20CT%20scans%20than%20the%20volume%20computed%20using%0AChilds%27%20method%20by%20radiologists%3A%20p-value%20of%200.094%20%28%3E0.05%29%20says%20that%20there%20is%20no%0Asignificant%20difference%20between%20CT%20segmentation%20volumes%20and%20ours%20in%20contrast%20to%0AChilds%27%20method.%20We%20validate%20our%20method%20using%20investigations%20%28ablation%20studies%29%0Aon%20the%20US%20image%20resolution%2C%20the%20number%20of%20CT%20scans%20used%20for%20SSM%2C%20the%20number%20of%0Aprincipal%20components%2C%20and%20the%20number%20of%20input%20US%20scans.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20automatic%20liver%20volumetry%20system%20using%20a%20few%0Aincomplete%20US%20scans%20given%20a%20set%20of%20CT%20scans%20of%20livers%20for%20SSM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19336v1&entry.124074799=Read"},
{"title": "Optimistic Information Directed Sampling", "author": "Gergely Neu and Matteo Papini and Ludovic Schwartz", "abstract": "  We study the problem of online learning in contextual bandit problems where\nthe loss function is assumed to belong to a known parametric function class. We\npropose a new analytic framework for this setting that bridges the Bayesian\ntheory of information-directed sampling due to Russo and Van Roy (2018) and the\nworst-case theory of Foster, Kakade, Qian, and Rakhlin (2021) based on the\ndecision-estimation coefficient. Drawing from both lines of work, we propose a\nalgorithmic template called Optimistic Information-Directed Sampling and show\nthat it can achieve instance-dependent regret guarantees similar to the ones\nachievable by the classic Bayesian IDS method, but with the major advantage of\nnot requiring any Bayesian assumptions. The key technical innovation of our\nanalysis is introducing an optimistic surrogate model for the regret and using\nit to define a frequentist version of the Information Ratio of Russo and Van\nRoy (2018), and a less conservative version of the Decision Estimation\nCoefficient of Foster et al. (2021). Keywords: Contextual bandits,\ninformation-directed sampling, decision estimation coefficient, first-order\nregret bounds.\n", "link": "http://arxiv.org/abs/2402.15411v2", "date": "2024-06-27", "relevancy": 1.9686, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5253}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4872}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimistic%20Information%20Directed%20Sampling&body=Title%3A%20Optimistic%20Information%20Directed%20Sampling%0AAuthor%3A%20Gergely%20Neu%20and%20Matteo%20Papini%20and%20Ludovic%20Schwartz%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20online%20learning%20in%20contextual%20bandit%20problems%20where%0Athe%20loss%20function%20is%20assumed%20to%20belong%20to%20a%20known%20parametric%20function%20class.%20We%0Apropose%20a%20new%20analytic%20framework%20for%20this%20setting%20that%20bridges%20the%20Bayesian%0Atheory%20of%20information-directed%20sampling%20due%20to%20Russo%20and%20Van%20Roy%20%282018%29%20and%20the%0Aworst-case%20theory%20of%20Foster%2C%20Kakade%2C%20Qian%2C%20and%20Rakhlin%20%282021%29%20based%20on%20the%0Adecision-estimation%20coefficient.%20Drawing%20from%20both%20lines%20of%20work%2C%20we%20propose%20a%0Aalgorithmic%20template%20called%20Optimistic%20Information-Directed%20Sampling%20and%20show%0Athat%20it%20can%20achieve%20instance-dependent%20regret%20guarantees%20similar%20to%20the%20ones%0Aachievable%20by%20the%20classic%20Bayesian%20IDS%20method%2C%20but%20with%20the%20major%20advantage%20of%0Anot%20requiring%20any%20Bayesian%20assumptions.%20The%20key%20technical%20innovation%20of%20our%0Aanalysis%20is%20introducing%20an%20optimistic%20surrogate%20model%20for%20the%20regret%20and%20using%0Ait%20to%20define%20a%20frequentist%20version%20of%20the%20Information%20Ratio%20of%20Russo%20and%20Van%0ARoy%20%282018%29%2C%20and%20a%20less%20conservative%20version%20of%20the%20Decision%20Estimation%0ACoefficient%20of%20Foster%20et%20al.%20%282021%29.%20Keywords%3A%20Contextual%20bandits%2C%0Ainformation-directed%20sampling%2C%20decision%20estimation%20coefficient%2C%20first-order%0Aregret%20bounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15411v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimistic%2520Information%2520Directed%2520Sampling%26entry.906535625%3DGergely%2520Neu%2520and%2520Matteo%2520Papini%2520and%2520Ludovic%2520Schwartz%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520online%2520learning%2520in%2520contextual%2520bandit%2520problems%2520where%250Athe%2520loss%2520function%2520is%2520assumed%2520to%2520belong%2520to%2520a%2520known%2520parametric%2520function%2520class.%2520We%250Apropose%2520a%2520new%2520analytic%2520framework%2520for%2520this%2520setting%2520that%2520bridges%2520the%2520Bayesian%250Atheory%2520of%2520information-directed%2520sampling%2520due%2520to%2520Russo%2520and%2520Van%2520Roy%2520%25282018%2529%2520and%2520the%250Aworst-case%2520theory%2520of%2520Foster%252C%2520Kakade%252C%2520Qian%252C%2520and%2520Rakhlin%2520%25282021%2529%2520based%2520on%2520the%250Adecision-estimation%2520coefficient.%2520Drawing%2520from%2520both%2520lines%2520of%2520work%252C%2520we%2520propose%2520a%250Aalgorithmic%2520template%2520called%2520Optimistic%2520Information-Directed%2520Sampling%2520and%2520show%250Athat%2520it%2520can%2520achieve%2520instance-dependent%2520regret%2520guarantees%2520similar%2520to%2520the%2520ones%250Aachievable%2520by%2520the%2520classic%2520Bayesian%2520IDS%2520method%252C%2520but%2520with%2520the%2520major%2520advantage%2520of%250Anot%2520requiring%2520any%2520Bayesian%2520assumptions.%2520The%2520key%2520technical%2520innovation%2520of%2520our%250Aanalysis%2520is%2520introducing%2520an%2520optimistic%2520surrogate%2520model%2520for%2520the%2520regret%2520and%2520using%250Ait%2520to%2520define%2520a%2520frequentist%2520version%2520of%2520the%2520Information%2520Ratio%2520of%2520Russo%2520and%2520Van%250ARoy%2520%25282018%2529%252C%2520and%2520a%2520less%2520conservative%2520version%2520of%2520the%2520Decision%2520Estimation%250ACoefficient%2520of%2520Foster%2520et%2520al.%2520%25282021%2529.%2520Keywords%253A%2520Contextual%2520bandits%252C%250Ainformation-directed%2520sampling%252C%2520decision%2520estimation%2520coefficient%252C%2520first-order%250Aregret%2520bounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15411v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimistic%20Information%20Directed%20Sampling&entry.906535625=Gergely%20Neu%20and%20Matteo%20Papini%20and%20Ludovic%20Schwartz&entry.1292438233=%20%20We%20study%20the%20problem%20of%20online%20learning%20in%20contextual%20bandit%20problems%20where%0Athe%20loss%20function%20is%20assumed%20to%20belong%20to%20a%20known%20parametric%20function%20class.%20We%0Apropose%20a%20new%20analytic%20framework%20for%20this%20setting%20that%20bridges%20the%20Bayesian%0Atheory%20of%20information-directed%20sampling%20due%20to%20Russo%20and%20Van%20Roy%20%282018%29%20and%20the%0Aworst-case%20theory%20of%20Foster%2C%20Kakade%2C%20Qian%2C%20and%20Rakhlin%20%282021%29%20based%20on%20the%0Adecision-estimation%20coefficient.%20Drawing%20from%20both%20lines%20of%20work%2C%20we%20propose%20a%0Aalgorithmic%20template%20called%20Optimistic%20Information-Directed%20Sampling%20and%20show%0Athat%20it%20can%20achieve%20instance-dependent%20regret%20guarantees%20similar%20to%20the%20ones%0Aachievable%20by%20the%20classic%20Bayesian%20IDS%20method%2C%20but%20with%20the%20major%20advantage%20of%0Anot%20requiring%20any%20Bayesian%20assumptions.%20The%20key%20technical%20innovation%20of%20our%0Aanalysis%20is%20introducing%20an%20optimistic%20surrogate%20model%20for%20the%20regret%20and%20using%0Ait%20to%20define%20a%20frequentist%20version%20of%20the%20Information%20Ratio%20of%20Russo%20and%20Van%0ARoy%20%282018%29%2C%20and%20a%20less%20conservative%20version%20of%20the%20Decision%20Estimation%0ACoefficient%20of%20Foster%20et%20al.%20%282021%29.%20Keywords%3A%20Contextual%20bandits%2C%0Ainformation-directed%20sampling%2C%20decision%20estimation%20coefficient%2C%20first-order%0Aregret%20bounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15411v2&entry.124074799=Read"},
{"title": "SimTxtSeg: Weakly-Supervised Medical Image Segmentation with Simple Text\n  Cues", "author": "Yuxin Xie and Tao Zhou and Yi Zhou and Geng Chen", "abstract": "  Weakly-supervised medical image segmentation is a challenging task that aims\nto reduce the annotation cost while keep the segmentation performance. In this\npaper, we present a novel framework, SimTxtSeg, that leverages simple text cues\nto generate high-quality pseudo-labels and study the cross-modal fusion in\ntraining segmentation models, simultaneously. Our contribution consists of two\nkey components: an effective Textual-to-Visual Cue Converter that produces\nvisual prompts from text prompts on medical images, and a text-guided\nsegmentation model with Text-Vision Hybrid Attention that fuses text and image\nfeatures. We evaluate our framework on two medical image segmentation tasks:\ncolonic polyp segmentation and MRI brain tumor segmentation, and achieve\nconsistent state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2406.19364v1", "date": "2024-06-27", "relevancy": 1.9684, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5072}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4926}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimTxtSeg%3A%20Weakly-Supervised%20Medical%20Image%20Segmentation%20with%20Simple%20Text%0A%20%20Cues&body=Title%3A%20SimTxtSeg%3A%20Weakly-Supervised%20Medical%20Image%20Segmentation%20with%20Simple%20Text%0A%20%20Cues%0AAuthor%3A%20Yuxin%20Xie%20and%20Tao%20Zhou%20and%20Yi%20Zhou%20and%20Geng%20Chen%0AAbstract%3A%20%20%20Weakly-supervised%20medical%20image%20segmentation%20is%20a%20challenging%20task%20that%20aims%0Ato%20reduce%20the%20annotation%20cost%20while%20keep%20the%20segmentation%20performance.%20In%20this%0Apaper%2C%20we%20present%20a%20novel%20framework%2C%20SimTxtSeg%2C%20that%20leverages%20simple%20text%20cues%0Ato%20generate%20high-quality%20pseudo-labels%20and%20study%20the%20cross-modal%20fusion%20in%0Atraining%20segmentation%20models%2C%20simultaneously.%20Our%20contribution%20consists%20of%20two%0Akey%20components%3A%20an%20effective%20Textual-to-Visual%20Cue%20Converter%20that%20produces%0Avisual%20prompts%20from%20text%20prompts%20on%20medical%20images%2C%20and%20a%20text-guided%0Asegmentation%20model%20with%20Text-Vision%20Hybrid%20Attention%20that%20fuses%20text%20and%20image%0Afeatures.%20We%20evaluate%20our%20framework%20on%20two%20medical%20image%20segmentation%20tasks%3A%0Acolonic%20polyp%20segmentation%20and%20MRI%20brain%20tumor%20segmentation%2C%20and%20achieve%0Aconsistent%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19364v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimTxtSeg%253A%2520Weakly-Supervised%2520Medical%2520Image%2520Segmentation%2520with%2520Simple%2520Text%250A%2520%2520Cues%26entry.906535625%3DYuxin%2520Xie%2520and%2520Tao%2520Zhou%2520and%2520Yi%2520Zhou%2520and%2520Geng%2520Chen%26entry.1292438233%3D%2520%2520Weakly-supervised%2520medical%2520image%2520segmentation%2520is%2520a%2520challenging%2520task%2520that%2520aims%250Ato%2520reduce%2520the%2520annotation%2520cost%2520while%2520keep%2520the%2520segmentation%2520performance.%2520In%2520this%250Apaper%252C%2520we%2520present%2520a%2520novel%2520framework%252C%2520SimTxtSeg%252C%2520that%2520leverages%2520simple%2520text%2520cues%250Ato%2520generate%2520high-quality%2520pseudo-labels%2520and%2520study%2520the%2520cross-modal%2520fusion%2520in%250Atraining%2520segmentation%2520models%252C%2520simultaneously.%2520Our%2520contribution%2520consists%2520of%2520two%250Akey%2520components%253A%2520an%2520effective%2520Textual-to-Visual%2520Cue%2520Converter%2520that%2520produces%250Avisual%2520prompts%2520from%2520text%2520prompts%2520on%2520medical%2520images%252C%2520and%2520a%2520text-guided%250Asegmentation%2520model%2520with%2520Text-Vision%2520Hybrid%2520Attention%2520that%2520fuses%2520text%2520and%2520image%250Afeatures.%2520We%2520evaluate%2520our%2520framework%2520on%2520two%2520medical%2520image%2520segmentation%2520tasks%253A%250Acolonic%2520polyp%2520segmentation%2520and%2520MRI%2520brain%2520tumor%2520segmentation%252C%2520and%2520achieve%250Aconsistent%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19364v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimTxtSeg%3A%20Weakly-Supervised%20Medical%20Image%20Segmentation%20with%20Simple%20Text%0A%20%20Cues&entry.906535625=Yuxin%20Xie%20and%20Tao%20Zhou%20and%20Yi%20Zhou%20and%20Geng%20Chen&entry.1292438233=%20%20Weakly-supervised%20medical%20image%20segmentation%20is%20a%20challenging%20task%20that%20aims%0Ato%20reduce%20the%20annotation%20cost%20while%20keep%20the%20segmentation%20performance.%20In%20this%0Apaper%2C%20we%20present%20a%20novel%20framework%2C%20SimTxtSeg%2C%20that%20leverages%20simple%20text%20cues%0Ato%20generate%20high-quality%20pseudo-labels%20and%20study%20the%20cross-modal%20fusion%20in%0Atraining%20segmentation%20models%2C%20simultaneously.%20Our%20contribution%20consists%20of%20two%0Akey%20components%3A%20an%20effective%20Textual-to-Visual%20Cue%20Converter%20that%20produces%0Avisual%20prompts%20from%20text%20prompts%20on%20medical%20images%2C%20and%20a%20text-guided%0Asegmentation%20model%20with%20Text-Vision%20Hybrid%20Attention%20that%20fuses%20text%20and%20image%0Afeatures.%20We%20evaluate%20our%20framework%20on%20two%20medical%20image%20segmentation%20tasks%3A%0Acolonic%20polyp%20segmentation%20and%20MRI%20brain%20tumor%20segmentation%2C%20and%20achieve%0Aconsistent%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19364v1&entry.124074799=Read"},
{"title": "Shortcut Learning in Medical Image Segmentation", "author": "Manxi Lin and Nina Weng and Kamil Mikolaj and Zahra Bashir and Morten Bo S\u00f8ndergaard Svendsen and Martin Tolsgaard and Anders Nymark Christensen and Aasa Feragen", "abstract": "  Shortcut learning is a phenomenon where machine learning models prioritize\nlearning simple, potentially misleading cues from data that do not generalize\nwell beyond the training set. While existing research primarily investigates\nthis in the realm of image classification, this study extends the exploration\nof shortcut learning into medical image segmentation. We demonstrate that\nclinical annotations such as calipers, and the combination of zero-padded\nconvolutions and center-cropped training sets in the dataset can inadvertently\nserve as shortcuts, impacting segmentation accuracy. We identify and evaluate\nthe shortcut learning on two different but common medical image segmentation\ntasks. In addition, we suggest strategies to mitigate the influence of shortcut\nlearning and improve the generalizability of the segmentation models. By\nuncovering the presence and implications of shortcuts in medical image\nsegmentation, we provide insights and methodologies for evaluating and\novercoming this pervasive challenge and call for attention in the community for\nshortcuts in segmentation. Our code is public at\nhttps://github.com/nina-weng/shortcut_skinseg .\n", "link": "http://arxiv.org/abs/2403.06748v2", "date": "2024-06-27", "relevancy": 1.967, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4984}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4954}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shortcut%20Learning%20in%20Medical%20Image%20Segmentation&body=Title%3A%20Shortcut%20Learning%20in%20Medical%20Image%20Segmentation%0AAuthor%3A%20Manxi%20Lin%20and%20Nina%20Weng%20and%20Kamil%20Mikolaj%20and%20Zahra%20Bashir%20and%20Morten%20Bo%20S%C3%B8ndergaard%20Svendsen%20and%20Martin%20Tolsgaard%20and%20Anders%20Nymark%20Christensen%20and%20Aasa%20Feragen%0AAbstract%3A%20%20%20Shortcut%20learning%20is%20a%20phenomenon%20where%20machine%20learning%20models%20prioritize%0Alearning%20simple%2C%20potentially%20misleading%20cues%20from%20data%20that%20do%20not%20generalize%0Awell%20beyond%20the%20training%20set.%20While%20existing%20research%20primarily%20investigates%0Athis%20in%20the%20realm%20of%20image%20classification%2C%20this%20study%20extends%20the%20exploration%0Aof%20shortcut%20learning%20into%20medical%20image%20segmentation.%20We%20demonstrate%20that%0Aclinical%20annotations%20such%20as%20calipers%2C%20and%20the%20combination%20of%20zero-padded%0Aconvolutions%20and%20center-cropped%20training%20sets%20in%20the%20dataset%20can%20inadvertently%0Aserve%20as%20shortcuts%2C%20impacting%20segmentation%20accuracy.%20We%20identify%20and%20evaluate%0Athe%20shortcut%20learning%20on%20two%20different%20but%20common%20medical%20image%20segmentation%0Atasks.%20In%20addition%2C%20we%20suggest%20strategies%20to%20mitigate%20the%20influence%20of%20shortcut%0Alearning%20and%20improve%20the%20generalizability%20of%20the%20segmentation%20models.%20By%0Auncovering%20the%20presence%20and%20implications%20of%20shortcuts%20in%20medical%20image%0Asegmentation%2C%20we%20provide%20insights%20and%20methodologies%20for%20evaluating%20and%0Aovercoming%20this%20pervasive%20challenge%20and%20call%20for%20attention%20in%20the%20community%20for%0Ashortcuts%20in%20segmentation.%20Our%20code%20is%20public%20at%0Ahttps%3A//github.com/nina-weng/shortcut_skinseg%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06748v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShortcut%2520Learning%2520in%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DManxi%2520Lin%2520and%2520Nina%2520Weng%2520and%2520Kamil%2520Mikolaj%2520and%2520Zahra%2520Bashir%2520and%2520Morten%2520Bo%2520S%25C3%25B8ndergaard%2520Svendsen%2520and%2520Martin%2520Tolsgaard%2520and%2520Anders%2520Nymark%2520Christensen%2520and%2520Aasa%2520Feragen%26entry.1292438233%3D%2520%2520Shortcut%2520learning%2520is%2520a%2520phenomenon%2520where%2520machine%2520learning%2520models%2520prioritize%250Alearning%2520simple%252C%2520potentially%2520misleading%2520cues%2520from%2520data%2520that%2520do%2520not%2520generalize%250Awell%2520beyond%2520the%2520training%2520set.%2520While%2520existing%2520research%2520primarily%2520investigates%250Athis%2520in%2520the%2520realm%2520of%2520image%2520classification%252C%2520this%2520study%2520extends%2520the%2520exploration%250Aof%2520shortcut%2520learning%2520into%2520medical%2520image%2520segmentation.%2520We%2520demonstrate%2520that%250Aclinical%2520annotations%2520such%2520as%2520calipers%252C%2520and%2520the%2520combination%2520of%2520zero-padded%250Aconvolutions%2520and%2520center-cropped%2520training%2520sets%2520in%2520the%2520dataset%2520can%2520inadvertently%250Aserve%2520as%2520shortcuts%252C%2520impacting%2520segmentation%2520accuracy.%2520We%2520identify%2520and%2520evaluate%250Athe%2520shortcut%2520learning%2520on%2520two%2520different%2520but%2520common%2520medical%2520image%2520segmentation%250Atasks.%2520In%2520addition%252C%2520we%2520suggest%2520strategies%2520to%2520mitigate%2520the%2520influence%2520of%2520shortcut%250Alearning%2520and%2520improve%2520the%2520generalizability%2520of%2520the%2520segmentation%2520models.%2520By%250Auncovering%2520the%2520presence%2520and%2520implications%2520of%2520shortcuts%2520in%2520medical%2520image%250Asegmentation%252C%2520we%2520provide%2520insights%2520and%2520methodologies%2520for%2520evaluating%2520and%250Aovercoming%2520this%2520pervasive%2520challenge%2520and%2520call%2520for%2520attention%2520in%2520the%2520community%2520for%250Ashortcuts%2520in%2520segmentation.%2520Our%2520code%2520is%2520public%2520at%250Ahttps%253A//github.com/nina-weng/shortcut_skinseg%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06748v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shortcut%20Learning%20in%20Medical%20Image%20Segmentation&entry.906535625=Manxi%20Lin%20and%20Nina%20Weng%20and%20Kamil%20Mikolaj%20and%20Zahra%20Bashir%20and%20Morten%20Bo%20S%C3%B8ndergaard%20Svendsen%20and%20Martin%20Tolsgaard%20and%20Anders%20Nymark%20Christensen%20and%20Aasa%20Feragen&entry.1292438233=%20%20Shortcut%20learning%20is%20a%20phenomenon%20where%20machine%20learning%20models%20prioritize%0Alearning%20simple%2C%20potentially%20misleading%20cues%20from%20data%20that%20do%20not%20generalize%0Awell%20beyond%20the%20training%20set.%20While%20existing%20research%20primarily%20investigates%0Athis%20in%20the%20realm%20of%20image%20classification%2C%20this%20study%20extends%20the%20exploration%0Aof%20shortcut%20learning%20into%20medical%20image%20segmentation.%20We%20demonstrate%20that%0Aclinical%20annotations%20such%20as%20calipers%2C%20and%20the%20combination%20of%20zero-padded%0Aconvolutions%20and%20center-cropped%20training%20sets%20in%20the%20dataset%20can%20inadvertently%0Aserve%20as%20shortcuts%2C%20impacting%20segmentation%20accuracy.%20We%20identify%20and%20evaluate%0Athe%20shortcut%20learning%20on%20two%20different%20but%20common%20medical%20image%20segmentation%0Atasks.%20In%20addition%2C%20we%20suggest%20strategies%20to%20mitigate%20the%20influence%20of%20shortcut%0Alearning%20and%20improve%20the%20generalizability%20of%20the%20segmentation%20models.%20By%0Auncovering%20the%20presence%20and%20implications%20of%20shortcuts%20in%20medical%20image%0Asegmentation%2C%20we%20provide%20insights%20and%20methodologies%20for%20evaluating%20and%0Aovercoming%20this%20pervasive%20challenge%20and%20call%20for%20attention%20in%20the%20community%20for%0Ashortcuts%20in%20segmentation.%20Our%20code%20is%20public%20at%0Ahttps%3A//github.com/nina-weng/shortcut_skinseg%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06748v2&entry.124074799=Read"},
{"title": "Adaptive Stochastic Weight Averaging", "author": "Caglar Demir and Arnab Sharma and Axel-Cyrille Ngonga Ngomo", "abstract": "  Ensemble models often improve generalization performances in challenging\ntasks. Yet, traditional techniques based on prediction averaging incur three\nwell-known disadvantages: the computational overhead of training multiple\nmodels, increased latency, and memory requirements at test time. To address\nthese issues, the Stochastic Weight Averaging (SWA) technique maintains a\nrunning average of model parameters from a specific epoch onward. Despite its\npotential benefits, maintaining a running average of parameters can hinder\ngeneralization, as an underlying running model begins to overfit. Conversely,\nan inadequately chosen starting point can render SWA more susceptible to\nunderfitting compared to an underlying running model. In this work, we propose\nAdaptive Stochastic Weight Averaging (ASWA) technique that updates a running\naverage of model parameters, only when generalization performance is improved\non the validation dataset. Hence, ASWA can be seen as a combination of SWA with\nthe early stopping technique, where the former accepts all updates on a\nparameter ensemble model and the latter rejects any update on an underlying\nrunning model. We conducted extensive experiments ranging from image\nclassification to multi-hop reasoning over knowledge graphs. Our experiments\nover 11 benchmark datasets with 7 baseline models suggest that ASWA leads to a\nstatistically better generalization across models and datasets\n", "link": "http://arxiv.org/abs/2406.19092v1", "date": "2024-06-27", "relevancy": 1.9549, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4945}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4889}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Stochastic%20Weight%20Averaging&body=Title%3A%20Adaptive%20Stochastic%20Weight%20Averaging%0AAuthor%3A%20Caglar%20Demir%20and%20Arnab%20Sharma%20and%20Axel-Cyrille%20Ngonga%20Ngomo%0AAbstract%3A%20%20%20Ensemble%20models%20often%20improve%20generalization%20performances%20in%20challenging%0Atasks.%20Yet%2C%20traditional%20techniques%20based%20on%20prediction%20averaging%20incur%20three%0Awell-known%20disadvantages%3A%20the%20computational%20overhead%20of%20training%20multiple%0Amodels%2C%20increased%20latency%2C%20and%20memory%20requirements%20at%20test%20time.%20To%20address%0Athese%20issues%2C%20the%20Stochastic%20Weight%20Averaging%20%28SWA%29%20technique%20maintains%20a%0Arunning%20average%20of%20model%20parameters%20from%20a%20specific%20epoch%20onward.%20Despite%20its%0Apotential%20benefits%2C%20maintaining%20a%20running%20average%20of%20parameters%20can%20hinder%0Ageneralization%2C%20as%20an%20underlying%20running%20model%20begins%20to%20overfit.%20Conversely%2C%0Aan%20inadequately%20chosen%20starting%20point%20can%20render%20SWA%20more%20susceptible%20to%0Aunderfitting%20compared%20to%20an%20underlying%20running%20model.%20In%20this%20work%2C%20we%20propose%0AAdaptive%20Stochastic%20Weight%20Averaging%20%28ASWA%29%20technique%20that%20updates%20a%20running%0Aaverage%20of%20model%20parameters%2C%20only%20when%20generalization%20performance%20is%20improved%0Aon%20the%20validation%20dataset.%20Hence%2C%20ASWA%20can%20be%20seen%20as%20a%20combination%20of%20SWA%20with%0Athe%20early%20stopping%20technique%2C%20where%20the%20former%20accepts%20all%20updates%20on%20a%0Aparameter%20ensemble%20model%20and%20the%20latter%20rejects%20any%20update%20on%20an%20underlying%0Arunning%20model.%20We%20conducted%20extensive%20experiments%20ranging%20from%20image%0Aclassification%20to%20multi-hop%20reasoning%20over%20knowledge%20graphs.%20Our%20experiments%0Aover%2011%20benchmark%20datasets%20with%207%20baseline%20models%20suggest%20that%20ASWA%20leads%20to%20a%0Astatistically%20better%20generalization%20across%20models%20and%20datasets%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Stochastic%2520Weight%2520Averaging%26entry.906535625%3DCaglar%2520Demir%2520and%2520Arnab%2520Sharma%2520and%2520Axel-Cyrille%2520Ngonga%2520Ngomo%26entry.1292438233%3D%2520%2520Ensemble%2520models%2520often%2520improve%2520generalization%2520performances%2520in%2520challenging%250Atasks.%2520Yet%252C%2520traditional%2520techniques%2520based%2520on%2520prediction%2520averaging%2520incur%2520three%250Awell-known%2520disadvantages%253A%2520the%2520computational%2520overhead%2520of%2520training%2520multiple%250Amodels%252C%2520increased%2520latency%252C%2520and%2520memory%2520requirements%2520at%2520test%2520time.%2520To%2520address%250Athese%2520issues%252C%2520the%2520Stochastic%2520Weight%2520Averaging%2520%2528SWA%2529%2520technique%2520maintains%2520a%250Arunning%2520average%2520of%2520model%2520parameters%2520from%2520a%2520specific%2520epoch%2520onward.%2520Despite%2520its%250Apotential%2520benefits%252C%2520maintaining%2520a%2520running%2520average%2520of%2520parameters%2520can%2520hinder%250Ageneralization%252C%2520as%2520an%2520underlying%2520running%2520model%2520begins%2520to%2520overfit.%2520Conversely%252C%250Aan%2520inadequately%2520chosen%2520starting%2520point%2520can%2520render%2520SWA%2520more%2520susceptible%2520to%250Aunderfitting%2520compared%2520to%2520an%2520underlying%2520running%2520model.%2520In%2520this%2520work%252C%2520we%2520propose%250AAdaptive%2520Stochastic%2520Weight%2520Averaging%2520%2528ASWA%2529%2520technique%2520that%2520updates%2520a%2520running%250Aaverage%2520of%2520model%2520parameters%252C%2520only%2520when%2520generalization%2520performance%2520is%2520improved%250Aon%2520the%2520validation%2520dataset.%2520Hence%252C%2520ASWA%2520can%2520be%2520seen%2520as%2520a%2520combination%2520of%2520SWA%2520with%250Athe%2520early%2520stopping%2520technique%252C%2520where%2520the%2520former%2520accepts%2520all%2520updates%2520on%2520a%250Aparameter%2520ensemble%2520model%2520and%2520the%2520latter%2520rejects%2520any%2520update%2520on%2520an%2520underlying%250Arunning%2520model.%2520We%2520conducted%2520extensive%2520experiments%2520ranging%2520from%2520image%250Aclassification%2520to%2520multi-hop%2520reasoning%2520over%2520knowledge%2520graphs.%2520Our%2520experiments%250Aover%252011%2520benchmark%2520datasets%2520with%25207%2520baseline%2520models%2520suggest%2520that%2520ASWA%2520leads%2520to%2520a%250Astatistically%2520better%2520generalization%2520across%2520models%2520and%2520datasets%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Stochastic%20Weight%20Averaging&entry.906535625=Caglar%20Demir%20and%20Arnab%20Sharma%20and%20Axel-Cyrille%20Ngonga%20Ngomo&entry.1292438233=%20%20Ensemble%20models%20often%20improve%20generalization%20performances%20in%20challenging%0Atasks.%20Yet%2C%20traditional%20techniques%20based%20on%20prediction%20averaging%20incur%20three%0Awell-known%20disadvantages%3A%20the%20computational%20overhead%20of%20training%20multiple%0Amodels%2C%20increased%20latency%2C%20and%20memory%20requirements%20at%20test%20time.%20To%20address%0Athese%20issues%2C%20the%20Stochastic%20Weight%20Averaging%20%28SWA%29%20technique%20maintains%20a%0Arunning%20average%20of%20model%20parameters%20from%20a%20specific%20epoch%20onward.%20Despite%20its%0Apotential%20benefits%2C%20maintaining%20a%20running%20average%20of%20parameters%20can%20hinder%0Ageneralization%2C%20as%20an%20underlying%20running%20model%20begins%20to%20overfit.%20Conversely%2C%0Aan%20inadequately%20chosen%20starting%20point%20can%20render%20SWA%20more%20susceptible%20to%0Aunderfitting%20compared%20to%20an%20underlying%20running%20model.%20In%20this%20work%2C%20we%20propose%0AAdaptive%20Stochastic%20Weight%20Averaging%20%28ASWA%29%20technique%20that%20updates%20a%20running%0Aaverage%20of%20model%20parameters%2C%20only%20when%20generalization%20performance%20is%20improved%0Aon%20the%20validation%20dataset.%20Hence%2C%20ASWA%20can%20be%20seen%20as%20a%20combination%20of%20SWA%20with%0Athe%20early%20stopping%20technique%2C%20where%20the%20former%20accepts%20all%20updates%20on%20a%0Aparameter%20ensemble%20model%20and%20the%20latter%20rejects%20any%20update%20on%20an%20underlying%0Arunning%20model.%20We%20conducted%20extensive%20experiments%20ranging%20from%20image%0Aclassification%20to%20multi-hop%20reasoning%20over%20knowledge%20graphs.%20Our%20experiments%0Aover%2011%20benchmark%20datasets%20with%207%20baseline%20models%20suggest%20that%20ASWA%20leads%20to%20a%0Astatistically%20better%20generalization%20across%20models%20and%20datasets%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19092v1&entry.124074799=Read"},
{"title": "Long-term drought prediction using deep neural networks based on\n  geospatial weather data", "author": "Vsevolod Grabar and Alexander Marusov and Yury Maximov and Nazar Sotiriadi and Alexander Bulkin and Alexey Zaytsev", "abstract": "  The problem of high-quality drought forecasting up to a year in advance is\ncritical for agriculture planning and insurance. Yet, it is still unsolved with\nreasonable accuracy due to data complexity and aridity stochasticity. We tackle\ndrought data by introducing an end-to-end approach that adopts a\nspatio-temporal neural network model with accessible open monthly climate data\nas the input.\n  Our systematic research employs diverse proposed models and five distinct\nenvironmental regions as a testbed to evaluate the efficacy of the Palmer\nDrought Severity Index (PDSI) prediction. Key aggregated findings are the\nexceptional performance of a Transformer model, EarthFormer, in making accurate\nshort-term (up to six months) forecasts. At the same time, the Convolutional\nLSTM excels in longer-term forecasting. Both models achieved high ROC AUC\nscores: 0.948 for one month ahead and 0.617 for twelve months ahead forecasts,\nbecoming closer to perfect ROC-AUC by $54\\%$ and $16\\%$, respectively, c.t.\nclassic approaches.\n", "link": "http://arxiv.org/abs/2309.06212v4", "date": "2024-06-27", "relevancy": 1.9508, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5174}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4863}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-term%20drought%20prediction%20using%20deep%20neural%20networks%20based%20on%0A%20%20geospatial%20weather%20data&body=Title%3A%20Long-term%20drought%20prediction%20using%20deep%20neural%20networks%20based%20on%0A%20%20geospatial%20weather%20data%0AAuthor%3A%20Vsevolod%20Grabar%20and%20Alexander%20Marusov%20and%20Yury%20Maximov%20and%20Nazar%20Sotiriadi%20and%20Alexander%20Bulkin%20and%20Alexey%20Zaytsev%0AAbstract%3A%20%20%20The%20problem%20of%20high-quality%20drought%20forecasting%20up%20to%20a%20year%20in%20advance%20is%0Acritical%20for%20agriculture%20planning%20and%20insurance.%20Yet%2C%20it%20is%20still%20unsolved%20with%0Areasonable%20accuracy%20due%20to%20data%20complexity%20and%20aridity%20stochasticity.%20We%20tackle%0Adrought%20data%20by%20introducing%20an%20end-to-end%20approach%20that%20adopts%20a%0Aspatio-temporal%20neural%20network%20model%20with%20accessible%20open%20monthly%20climate%20data%0Aas%20the%20input.%0A%20%20Our%20systematic%20research%20employs%20diverse%20proposed%20models%20and%20five%20distinct%0Aenvironmental%20regions%20as%20a%20testbed%20to%20evaluate%20the%20efficacy%20of%20the%20Palmer%0ADrought%20Severity%20Index%20%28PDSI%29%20prediction.%20Key%20aggregated%20findings%20are%20the%0Aexceptional%20performance%20of%20a%20Transformer%20model%2C%20EarthFormer%2C%20in%20making%20accurate%0Ashort-term%20%28up%20to%20six%20months%29%20forecasts.%20At%20the%20same%20time%2C%20the%20Convolutional%0ALSTM%20excels%20in%20longer-term%20forecasting.%20Both%20models%20achieved%20high%20ROC%20AUC%0Ascores%3A%200.948%20for%20one%20month%20ahead%20and%200.617%20for%20twelve%20months%20ahead%20forecasts%2C%0Abecoming%20closer%20to%20perfect%20ROC-AUC%20by%20%2454%5C%25%24%20and%20%2416%5C%25%24%2C%20respectively%2C%20c.t.%0Aclassic%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.06212v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-term%2520drought%2520prediction%2520using%2520deep%2520neural%2520networks%2520based%2520on%250A%2520%2520geospatial%2520weather%2520data%26entry.906535625%3DVsevolod%2520Grabar%2520and%2520Alexander%2520Marusov%2520and%2520Yury%2520Maximov%2520and%2520Nazar%2520Sotiriadi%2520and%2520Alexander%2520Bulkin%2520and%2520Alexey%2520Zaytsev%26entry.1292438233%3D%2520%2520The%2520problem%2520of%2520high-quality%2520drought%2520forecasting%2520up%2520to%2520a%2520year%2520in%2520advance%2520is%250Acritical%2520for%2520agriculture%2520planning%2520and%2520insurance.%2520Yet%252C%2520it%2520is%2520still%2520unsolved%2520with%250Areasonable%2520accuracy%2520due%2520to%2520data%2520complexity%2520and%2520aridity%2520stochasticity.%2520We%2520tackle%250Adrought%2520data%2520by%2520introducing%2520an%2520end-to-end%2520approach%2520that%2520adopts%2520a%250Aspatio-temporal%2520neural%2520network%2520model%2520with%2520accessible%2520open%2520monthly%2520climate%2520data%250Aas%2520the%2520input.%250A%2520%2520Our%2520systematic%2520research%2520employs%2520diverse%2520proposed%2520models%2520and%2520five%2520distinct%250Aenvironmental%2520regions%2520as%2520a%2520testbed%2520to%2520evaluate%2520the%2520efficacy%2520of%2520the%2520Palmer%250ADrought%2520Severity%2520Index%2520%2528PDSI%2529%2520prediction.%2520Key%2520aggregated%2520findings%2520are%2520the%250Aexceptional%2520performance%2520of%2520a%2520Transformer%2520model%252C%2520EarthFormer%252C%2520in%2520making%2520accurate%250Ashort-term%2520%2528up%2520to%2520six%2520months%2529%2520forecasts.%2520At%2520the%2520same%2520time%252C%2520the%2520Convolutional%250ALSTM%2520excels%2520in%2520longer-term%2520forecasting.%2520Both%2520models%2520achieved%2520high%2520ROC%2520AUC%250Ascores%253A%25200.948%2520for%2520one%2520month%2520ahead%2520and%25200.617%2520for%2520twelve%2520months%2520ahead%2520forecasts%252C%250Abecoming%2520closer%2520to%2520perfect%2520ROC-AUC%2520by%2520%252454%255C%2525%2524%2520and%2520%252416%255C%2525%2524%252C%2520respectively%252C%2520c.t.%250Aclassic%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.06212v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-term%20drought%20prediction%20using%20deep%20neural%20networks%20based%20on%0A%20%20geospatial%20weather%20data&entry.906535625=Vsevolod%20Grabar%20and%20Alexander%20Marusov%20and%20Yury%20Maximov%20and%20Nazar%20Sotiriadi%20and%20Alexander%20Bulkin%20and%20Alexey%20Zaytsev&entry.1292438233=%20%20The%20problem%20of%20high-quality%20drought%20forecasting%20up%20to%20a%20year%20in%20advance%20is%0Acritical%20for%20agriculture%20planning%20and%20insurance.%20Yet%2C%20it%20is%20still%20unsolved%20with%0Areasonable%20accuracy%20due%20to%20data%20complexity%20and%20aridity%20stochasticity.%20We%20tackle%0Adrought%20data%20by%20introducing%20an%20end-to-end%20approach%20that%20adopts%20a%0Aspatio-temporal%20neural%20network%20model%20with%20accessible%20open%20monthly%20climate%20data%0Aas%20the%20input.%0A%20%20Our%20systematic%20research%20employs%20diverse%20proposed%20models%20and%20five%20distinct%0Aenvironmental%20regions%20as%20a%20testbed%20to%20evaluate%20the%20efficacy%20of%20the%20Palmer%0ADrought%20Severity%20Index%20%28PDSI%29%20prediction.%20Key%20aggregated%20findings%20are%20the%0Aexceptional%20performance%20of%20a%20Transformer%20model%2C%20EarthFormer%2C%20in%20making%20accurate%0Ashort-term%20%28up%20to%20six%20months%29%20forecasts.%20At%20the%20same%20time%2C%20the%20Convolutional%0ALSTM%20excels%20in%20longer-term%20forecasting.%20Both%20models%20achieved%20high%20ROC%20AUC%0Ascores%3A%200.948%20for%20one%20month%20ahead%20and%200.617%20for%20twelve%20months%20ahead%20forecasts%2C%0Abecoming%20closer%20to%20perfect%20ROC-AUC%20by%20%2454%5C%25%24%20and%20%2416%5C%25%24%2C%20respectively%2C%20c.t.%0Aclassic%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.06212v4&entry.124074799=Read"},
{"title": "Local to Global: Learning Dynamics and Effect of Initialization for\n  Transformers", "author": "Ashok Vardhan Makkuva and Marco Bondaschi and Chanakya Ekbote and Adway Girish and Alliot Nagle and Hyeji Kim and Michael Gastpar", "abstract": "  In recent years, transformer-based models have revolutionized deep learning,\nparticularly in sequence modeling. To better understand this phenomenon, there\nis a growing interest in using Markov input processes to study transformers.\nHowever, our current understanding in this regard remains limited with many\nfundamental questions about how transformers learn Markov chains still\nunanswered. In this paper, we address this by focusing on first-order Markov\nchains and single-layer transformers, providing a comprehensive\ncharacterization of the learning dynamics in this context. Specifically, we\nprove that transformer parameters trained on next-token prediction loss can\neither converge to global or local minima, contingent on the initialization and\nthe Markovian data properties, and we characterize the precise conditions under\nwhich this occurs. To the best of our knowledge, this is the first result of\nits kind highlighting the role of initialization. We further demonstrate that\nour theoretical findings are corroborated by empirical evidence. Based on these\ninsights, we provide guidelines for the initialization of transformer\nparameters and demonstrate their effectiveness. Finally, we outline several\nopen problems in this arena. Code is available at:\nhttps://github.com/Bond1995/Markov.\n", "link": "http://arxiv.org/abs/2406.03072v2", "date": "2024-06-27", "relevancy": 1.9405, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5405}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4752}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20to%20Global%3A%20Learning%20Dynamics%20and%20Effect%20of%20Initialization%20for%0A%20%20Transformers&body=Title%3A%20Local%20to%20Global%3A%20Learning%20Dynamics%20and%20Effect%20of%20Initialization%20for%0A%20%20Transformers%0AAuthor%3A%20Ashok%20Vardhan%20Makkuva%20and%20Marco%20Bondaschi%20and%20Chanakya%20Ekbote%20and%20Adway%20Girish%20and%20Alliot%20Nagle%20and%20Hyeji%20Kim%20and%20Michael%20Gastpar%0AAbstract%3A%20%20%20In%20recent%20years%2C%20transformer-based%20models%20have%20revolutionized%20deep%20learning%2C%0Aparticularly%20in%20sequence%20modeling.%20To%20better%20understand%20this%20phenomenon%2C%20there%0Ais%20a%20growing%20interest%20in%20using%20Markov%20input%20processes%20to%20study%20transformers.%0AHowever%2C%20our%20current%20understanding%20in%20this%20regard%20remains%20limited%20with%20many%0Afundamental%20questions%20about%20how%20transformers%20learn%20Markov%20chains%20still%0Aunanswered.%20In%20this%20paper%2C%20we%20address%20this%20by%20focusing%20on%20first-order%20Markov%0Achains%20and%20single-layer%20transformers%2C%20providing%20a%20comprehensive%0Acharacterization%20of%20the%20learning%20dynamics%20in%20this%20context.%20Specifically%2C%20we%0Aprove%20that%20transformer%20parameters%20trained%20on%20next-token%20prediction%20loss%20can%0Aeither%20converge%20to%20global%20or%20local%20minima%2C%20contingent%20on%20the%20initialization%20and%0Athe%20Markovian%20data%20properties%2C%20and%20we%20characterize%20the%20precise%20conditions%20under%0Awhich%20this%20occurs.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20result%20of%0Aits%20kind%20highlighting%20the%20role%20of%20initialization.%20We%20further%20demonstrate%20that%0Aour%20theoretical%20findings%20are%20corroborated%20by%20empirical%20evidence.%20Based%20on%20these%0Ainsights%2C%20we%20provide%20guidelines%20for%20the%20initialization%20of%20transformer%0Aparameters%20and%20demonstrate%20their%20effectiveness.%20Finally%2C%20we%20outline%20several%0Aopen%20problems%20in%20this%20arena.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/Bond1995/Markov.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03072v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520to%2520Global%253A%2520Learning%2520Dynamics%2520and%2520Effect%2520of%2520Initialization%2520for%250A%2520%2520Transformers%26entry.906535625%3DAshok%2520Vardhan%2520Makkuva%2520and%2520Marco%2520Bondaschi%2520and%2520Chanakya%2520Ekbote%2520and%2520Adway%2520Girish%2520and%2520Alliot%2520Nagle%2520and%2520Hyeji%2520Kim%2520and%2520Michael%2520Gastpar%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520transformer-based%2520models%2520have%2520revolutionized%2520deep%2520learning%252C%250Aparticularly%2520in%2520sequence%2520modeling.%2520To%2520better%2520understand%2520this%2520phenomenon%252C%2520there%250Ais%2520a%2520growing%2520interest%2520in%2520using%2520Markov%2520input%2520processes%2520to%2520study%2520transformers.%250AHowever%252C%2520our%2520current%2520understanding%2520in%2520this%2520regard%2520remains%2520limited%2520with%2520many%250Afundamental%2520questions%2520about%2520how%2520transformers%2520learn%2520Markov%2520chains%2520still%250Aunanswered.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520by%2520focusing%2520on%2520first-order%2520Markov%250Achains%2520and%2520single-layer%2520transformers%252C%2520providing%2520a%2520comprehensive%250Acharacterization%2520of%2520the%2520learning%2520dynamics%2520in%2520this%2520context.%2520Specifically%252C%2520we%250Aprove%2520that%2520transformer%2520parameters%2520trained%2520on%2520next-token%2520prediction%2520loss%2520can%250Aeither%2520converge%2520to%2520global%2520or%2520local%2520minima%252C%2520contingent%2520on%2520the%2520initialization%2520and%250Athe%2520Markovian%2520data%2520properties%252C%2520and%2520we%2520characterize%2520the%2520precise%2520conditions%2520under%250Awhich%2520this%2520occurs.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520result%2520of%250Aits%2520kind%2520highlighting%2520the%2520role%2520of%2520initialization.%2520We%2520further%2520demonstrate%2520that%250Aour%2520theoretical%2520findings%2520are%2520corroborated%2520by%2520empirical%2520evidence.%2520Based%2520on%2520these%250Ainsights%252C%2520we%2520provide%2520guidelines%2520for%2520the%2520initialization%2520of%2520transformer%250Aparameters%2520and%2520demonstrate%2520their%2520effectiveness.%2520Finally%252C%2520we%2520outline%2520several%250Aopen%2520problems%2520in%2520this%2520arena.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/Bond1995/Markov.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03072v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20to%20Global%3A%20Learning%20Dynamics%20and%20Effect%20of%20Initialization%20for%0A%20%20Transformers&entry.906535625=Ashok%20Vardhan%20Makkuva%20and%20Marco%20Bondaschi%20and%20Chanakya%20Ekbote%20and%20Adway%20Girish%20and%20Alliot%20Nagle%20and%20Hyeji%20Kim%20and%20Michael%20Gastpar&entry.1292438233=%20%20In%20recent%20years%2C%20transformer-based%20models%20have%20revolutionized%20deep%20learning%2C%0Aparticularly%20in%20sequence%20modeling.%20To%20better%20understand%20this%20phenomenon%2C%20there%0Ais%20a%20growing%20interest%20in%20using%20Markov%20input%20processes%20to%20study%20transformers.%0AHowever%2C%20our%20current%20understanding%20in%20this%20regard%20remains%20limited%20with%20many%0Afundamental%20questions%20about%20how%20transformers%20learn%20Markov%20chains%20still%0Aunanswered.%20In%20this%20paper%2C%20we%20address%20this%20by%20focusing%20on%20first-order%20Markov%0Achains%20and%20single-layer%20transformers%2C%20providing%20a%20comprehensive%0Acharacterization%20of%20the%20learning%20dynamics%20in%20this%20context.%20Specifically%2C%20we%0Aprove%20that%20transformer%20parameters%20trained%20on%20next-token%20prediction%20loss%20can%0Aeither%20converge%20to%20global%20or%20local%20minima%2C%20contingent%20on%20the%20initialization%20and%0Athe%20Markovian%20data%20properties%2C%20and%20we%20characterize%20the%20precise%20conditions%20under%0Awhich%20this%20occurs.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20result%20of%0Aits%20kind%20highlighting%20the%20role%20of%20initialization.%20We%20further%20demonstrate%20that%0Aour%20theoretical%20findings%20are%20corroborated%20by%20empirical%20evidence.%20Based%20on%20these%0Ainsights%2C%20we%20provide%20guidelines%20for%20the%20initialization%20of%20transformer%0Aparameters%20and%20demonstrate%20their%20effectiveness.%20Finally%2C%20we%20outline%20several%0Aopen%20problems%20in%20this%20arena.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/Bond1995/Markov.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03072v2&entry.124074799=Read"},
{"title": "FedMap: Iterative Magnitude-Based Pruning for Communication-Efficient\n  Federated Learning", "author": "Alexander Herzog and Robbie Southam and Ioannis Mavromatis and Aftab Khan", "abstract": "  Federated Learning (FL) is a distributed machine learning approach that\nenables training on decentralized data while preserving privacy. However, FL\nsystems often involve resource-constrained client devices with limited\ncomputational power, memory, storage, and bandwidth. This paper introduces\nFedMap, a novel method that aims to enhance the communication efficiency of FL\ndeployments by collaboratively learning an increasingly sparse global model\nthrough iterative, unstructured pruning. Importantly, FedMap trains a global\nmodel from scratch, unlike other methods reported in the literature, making it\nideal for privacy-critical use cases such as in the medical and finance\ndomains, where suitable pre-training data is often limited. FedMap adapts\niterative magnitude-based pruning to the FL setting, ensuring all clients prune\nand refine the same subset of the global model parameters, therefore gradually\nreducing the global model size and communication overhead. The iterative nature\nof FedMap, forming subsequent models as subsets of predecessors, avoids\nparameter reactivation issues seen in prior work, resulting in stable\nperformance. In this paper we provide an extensive evaluation of FedMap across\ndiverse settings, datasets, model architectures, and hyperparameters, assessing\nperformance in both IID and non-IID environments. Comparative analysis against\nthe baseline approach demonstrates FedMap's ability to achieve more stable\nclient model performance. For IID scenarios, FedMap achieves over $90$\\%\npruning without significant performance degradation. In non-IID settings, it\nachieves at least $~80$\\% pruning while maintaining accuracy. FedMap offers a\npromising solution to alleviate communication bottlenecks in FL systems while\nretaining model accuracy.\n", "link": "http://arxiv.org/abs/2406.19050v1", "date": "2024-06-27", "relevancy": 1.9402, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5136}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4655}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedMap%3A%20Iterative%20Magnitude-Based%20Pruning%20for%20Communication-Efficient%0A%20%20Federated%20Learning&body=Title%3A%20FedMap%3A%20Iterative%20Magnitude-Based%20Pruning%20for%20Communication-Efficient%0A%20%20Federated%20Learning%0AAuthor%3A%20Alexander%20Herzog%20and%20Robbie%20Southam%20and%20Ioannis%20Mavromatis%20and%20Aftab%20Khan%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20a%20distributed%20machine%20learning%20approach%20that%0Aenables%20training%20on%20decentralized%20data%20while%20preserving%20privacy.%20However%2C%20FL%0Asystems%20often%20involve%20resource-constrained%20client%20devices%20with%20limited%0Acomputational%20power%2C%20memory%2C%20storage%2C%20and%20bandwidth.%20This%20paper%20introduces%0AFedMap%2C%20a%20novel%20method%20that%20aims%20to%20enhance%20the%20communication%20efficiency%20of%20FL%0Adeployments%20by%20collaboratively%20learning%20an%20increasingly%20sparse%20global%20model%0Athrough%20iterative%2C%20unstructured%20pruning.%20Importantly%2C%20FedMap%20trains%20a%20global%0Amodel%20from%20scratch%2C%20unlike%20other%20methods%20reported%20in%20the%20literature%2C%20making%20it%0Aideal%20for%20privacy-critical%20use%20cases%20such%20as%20in%20the%20medical%20and%20finance%0Adomains%2C%20where%20suitable%20pre-training%20data%20is%20often%20limited.%20FedMap%20adapts%0Aiterative%20magnitude-based%20pruning%20to%20the%20FL%20setting%2C%20ensuring%20all%20clients%20prune%0Aand%20refine%20the%20same%20subset%20of%20the%20global%20model%20parameters%2C%20therefore%20gradually%0Areducing%20the%20global%20model%20size%20and%20communication%20overhead.%20The%20iterative%20nature%0Aof%20FedMap%2C%20forming%20subsequent%20models%20as%20subsets%20of%20predecessors%2C%20avoids%0Aparameter%20reactivation%20issues%20seen%20in%20prior%20work%2C%20resulting%20in%20stable%0Aperformance.%20In%20this%20paper%20we%20provide%20an%20extensive%20evaluation%20of%20FedMap%20across%0Adiverse%20settings%2C%20datasets%2C%20model%20architectures%2C%20and%20hyperparameters%2C%20assessing%0Aperformance%20in%20both%20IID%20and%20non-IID%20environments.%20Comparative%20analysis%20against%0Athe%20baseline%20approach%20demonstrates%20FedMap%27s%20ability%20to%20achieve%20more%20stable%0Aclient%20model%20performance.%20For%20IID%20scenarios%2C%20FedMap%20achieves%20over%20%2490%24%5C%25%0Apruning%20without%20significant%20performance%20degradation.%20In%20non-IID%20settings%2C%20it%0Aachieves%20at%20least%20%24~80%24%5C%25%20pruning%20while%20maintaining%20accuracy.%20FedMap%20offers%20a%0Apromising%20solution%20to%20alleviate%20communication%20bottlenecks%20in%20FL%20systems%20while%0Aretaining%20model%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedMap%253A%2520Iterative%2520Magnitude-Based%2520Pruning%2520for%2520Communication-Efficient%250A%2520%2520Federated%2520Learning%26entry.906535625%3DAlexander%2520Herzog%2520and%2520Robbie%2520Southam%2520and%2520Ioannis%2520Mavromatis%2520and%2520Aftab%2520Khan%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520a%2520distributed%2520machine%2520learning%2520approach%2520that%250Aenables%2520training%2520on%2520decentralized%2520data%2520while%2520preserving%2520privacy.%2520However%252C%2520FL%250Asystems%2520often%2520involve%2520resource-constrained%2520client%2520devices%2520with%2520limited%250Acomputational%2520power%252C%2520memory%252C%2520storage%252C%2520and%2520bandwidth.%2520This%2520paper%2520introduces%250AFedMap%252C%2520a%2520novel%2520method%2520that%2520aims%2520to%2520enhance%2520the%2520communication%2520efficiency%2520of%2520FL%250Adeployments%2520by%2520collaboratively%2520learning%2520an%2520increasingly%2520sparse%2520global%2520model%250Athrough%2520iterative%252C%2520unstructured%2520pruning.%2520Importantly%252C%2520FedMap%2520trains%2520a%2520global%250Amodel%2520from%2520scratch%252C%2520unlike%2520other%2520methods%2520reported%2520in%2520the%2520literature%252C%2520making%2520it%250Aideal%2520for%2520privacy-critical%2520use%2520cases%2520such%2520as%2520in%2520the%2520medical%2520and%2520finance%250Adomains%252C%2520where%2520suitable%2520pre-training%2520data%2520is%2520often%2520limited.%2520FedMap%2520adapts%250Aiterative%2520magnitude-based%2520pruning%2520to%2520the%2520FL%2520setting%252C%2520ensuring%2520all%2520clients%2520prune%250Aand%2520refine%2520the%2520same%2520subset%2520of%2520the%2520global%2520model%2520parameters%252C%2520therefore%2520gradually%250Areducing%2520the%2520global%2520model%2520size%2520and%2520communication%2520overhead.%2520The%2520iterative%2520nature%250Aof%2520FedMap%252C%2520forming%2520subsequent%2520models%2520as%2520subsets%2520of%2520predecessors%252C%2520avoids%250Aparameter%2520reactivation%2520issues%2520seen%2520in%2520prior%2520work%252C%2520resulting%2520in%2520stable%250Aperformance.%2520In%2520this%2520paper%2520we%2520provide%2520an%2520extensive%2520evaluation%2520of%2520FedMap%2520across%250Adiverse%2520settings%252C%2520datasets%252C%2520model%2520architectures%252C%2520and%2520hyperparameters%252C%2520assessing%250Aperformance%2520in%2520both%2520IID%2520and%2520non-IID%2520environments.%2520Comparative%2520analysis%2520against%250Athe%2520baseline%2520approach%2520demonstrates%2520FedMap%2527s%2520ability%2520to%2520achieve%2520more%2520stable%250Aclient%2520model%2520performance.%2520For%2520IID%2520scenarios%252C%2520FedMap%2520achieves%2520over%2520%252490%2524%255C%2525%250Apruning%2520without%2520significant%2520performance%2520degradation.%2520In%2520non-IID%2520settings%252C%2520it%250Aachieves%2520at%2520least%2520%2524~80%2524%255C%2525%2520pruning%2520while%2520maintaining%2520accuracy.%2520FedMap%2520offers%2520a%250Apromising%2520solution%2520to%2520alleviate%2520communication%2520bottlenecks%2520in%2520FL%2520systems%2520while%250Aretaining%2520model%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedMap%3A%20Iterative%20Magnitude-Based%20Pruning%20for%20Communication-Efficient%0A%20%20Federated%20Learning&entry.906535625=Alexander%20Herzog%20and%20Robbie%20Southam%20and%20Ioannis%20Mavromatis%20and%20Aftab%20Khan&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20a%20distributed%20machine%20learning%20approach%20that%0Aenables%20training%20on%20decentralized%20data%20while%20preserving%20privacy.%20However%2C%20FL%0Asystems%20often%20involve%20resource-constrained%20client%20devices%20with%20limited%0Acomputational%20power%2C%20memory%2C%20storage%2C%20and%20bandwidth.%20This%20paper%20introduces%0AFedMap%2C%20a%20novel%20method%20that%20aims%20to%20enhance%20the%20communication%20efficiency%20of%20FL%0Adeployments%20by%20collaboratively%20learning%20an%20increasingly%20sparse%20global%20model%0Athrough%20iterative%2C%20unstructured%20pruning.%20Importantly%2C%20FedMap%20trains%20a%20global%0Amodel%20from%20scratch%2C%20unlike%20other%20methods%20reported%20in%20the%20literature%2C%20making%20it%0Aideal%20for%20privacy-critical%20use%20cases%20such%20as%20in%20the%20medical%20and%20finance%0Adomains%2C%20where%20suitable%20pre-training%20data%20is%20often%20limited.%20FedMap%20adapts%0Aiterative%20magnitude-based%20pruning%20to%20the%20FL%20setting%2C%20ensuring%20all%20clients%20prune%0Aand%20refine%20the%20same%20subset%20of%20the%20global%20model%20parameters%2C%20therefore%20gradually%0Areducing%20the%20global%20model%20size%20and%20communication%20overhead.%20The%20iterative%20nature%0Aof%20FedMap%2C%20forming%20subsequent%20models%20as%20subsets%20of%20predecessors%2C%20avoids%0Aparameter%20reactivation%20issues%20seen%20in%20prior%20work%2C%20resulting%20in%20stable%0Aperformance.%20In%20this%20paper%20we%20provide%20an%20extensive%20evaluation%20of%20FedMap%20across%0Adiverse%20settings%2C%20datasets%2C%20model%20architectures%2C%20and%20hyperparameters%2C%20assessing%0Aperformance%20in%20both%20IID%20and%20non-IID%20environments.%20Comparative%20analysis%20against%0Athe%20baseline%20approach%20demonstrates%20FedMap%27s%20ability%20to%20achieve%20more%20stable%0Aclient%20model%20performance.%20For%20IID%20scenarios%2C%20FedMap%20achieves%20over%20%2490%24%5C%25%0Apruning%20without%20significant%20performance%20degradation.%20In%20non-IID%20settings%2C%20it%0Aachieves%20at%20least%20%24~80%24%5C%25%20pruning%20while%20maintaining%20accuracy.%20FedMap%20offers%20a%0Apromising%20solution%20to%20alleviate%20communication%20bottlenecks%20in%20FL%20systems%20while%0Aretaining%20model%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19050v1&entry.124074799=Read"},
{"title": "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for\n  Retrieval-Augmented Generation", "author": "Jia Fu and Xiaoting Qin and Fangkai Yang and Lu Wang and Jue Zhang and Qingwei Lin and Yubo Chen and Dongmei Zhang and Saravan Rajmohan and Qi Zhang", "abstract": "  Recent advancements in Large Language Models have transformed ML/AI\ndevelopment, necessitating a reevaluation of AutoML principles for the\nRetrieval-Augmented Generation (RAG) systems. To address the challenges of\nhyper-parameter optimization and online adaptation in RAG, we propose the\nAutoRAG-HP framework, which formulates the hyper-parameter tuning as an online\nmulti-armed bandit (MAB) problem and introduces a novel two-level Hierarchical\nMAB (Hier-MAB) method for efficient exploration of large search spaces. We\nconduct extensive experiments on tuning hyper-parameters, such as top-k\nretrieved documents, prompt compression ratio, and embedding methods, using the\nALCE-ASQA and Natural Questions datasets. Our evaluation from jointly\noptimization all three hyper-parameters demonstrate that MAB-based online\nlearning methods can achieve Recall@5 $\\approx 0.8$ for scenarios with\nprominent gradients in search space, using only $\\sim20\\%$ of the LLM API calls\nrequired by the Grid Search approach. Additionally, the proposed Hier-MAB\napproach outperforms other baselines in more challenging optimization\nscenarios. The code will be made available at https://aka.ms/autorag.\n", "link": "http://arxiv.org/abs/2406.19251v1", "date": "2024-06-27", "relevancy": 1.9359, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4869}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4823}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoRAG-HP%3A%20Automatic%20Online%20Hyper-Parameter%20Tuning%20for%0A%20%20Retrieval-Augmented%20Generation&body=Title%3A%20AutoRAG-HP%3A%20Automatic%20Online%20Hyper-Parameter%20Tuning%20for%0A%20%20Retrieval-Augmented%20Generation%0AAuthor%3A%20Jia%20Fu%20and%20Xiaoting%20Qin%20and%20Fangkai%20Yang%20and%20Lu%20Wang%20and%20Jue%20Zhang%20and%20Qingwei%20Lin%20and%20Yubo%20Chen%20and%20Dongmei%20Zhang%20and%20Saravan%20Rajmohan%20and%20Qi%20Zhang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Language%20Models%20have%20transformed%20ML/AI%0Adevelopment%2C%20necessitating%20a%20reevaluation%20of%20AutoML%20principles%20for%20the%0ARetrieval-Augmented%20Generation%20%28RAG%29%20systems.%20To%20address%20the%20challenges%20of%0Ahyper-parameter%20optimization%20and%20online%20adaptation%20in%20RAG%2C%20we%20propose%20the%0AAutoRAG-HP%20framework%2C%20which%20formulates%20the%20hyper-parameter%20tuning%20as%20an%20online%0Amulti-armed%20bandit%20%28MAB%29%20problem%20and%20introduces%20a%20novel%20two-level%20Hierarchical%0AMAB%20%28Hier-MAB%29%20method%20for%20efficient%20exploration%20of%20large%20search%20spaces.%20We%0Aconduct%20extensive%20experiments%20on%20tuning%20hyper-parameters%2C%20such%20as%20top-k%0Aretrieved%20documents%2C%20prompt%20compression%20ratio%2C%20and%20embedding%20methods%2C%20using%20the%0AALCE-ASQA%20and%20Natural%20Questions%20datasets.%20Our%20evaluation%20from%20jointly%0Aoptimization%20all%20three%20hyper-parameters%20demonstrate%20that%20MAB-based%20online%0Alearning%20methods%20can%20achieve%20Recall%405%20%24%5Capprox%200.8%24%20for%20scenarios%20with%0Aprominent%20gradients%20in%20search%20space%2C%20using%20only%20%24%5Csim20%5C%25%24%20of%20the%20LLM%20API%20calls%0Arequired%20by%20the%20Grid%20Search%20approach.%20Additionally%2C%20the%20proposed%20Hier-MAB%0Aapproach%20outperforms%20other%20baselines%20in%20more%20challenging%20optimization%0Ascenarios.%20The%20code%20will%20be%20made%20available%20at%20https%3A//aka.ms/autorag.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoRAG-HP%253A%2520Automatic%2520Online%2520Hyper-Parameter%2520Tuning%2520for%250A%2520%2520Retrieval-Augmented%2520Generation%26entry.906535625%3DJia%2520Fu%2520and%2520Xiaoting%2520Qin%2520and%2520Fangkai%2520Yang%2520and%2520Lu%2520Wang%2520and%2520Jue%2520Zhang%2520and%2520Qingwei%2520Lin%2520and%2520Yubo%2520Chen%2520and%2520Dongmei%2520Zhang%2520and%2520Saravan%2520Rajmohan%2520and%2520Qi%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Language%2520Models%2520have%2520transformed%2520ML/AI%250Adevelopment%252C%2520necessitating%2520a%2520reevaluation%2520of%2520AutoML%2520principles%2520for%2520the%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520systems.%2520To%2520address%2520the%2520challenges%2520of%250Ahyper-parameter%2520optimization%2520and%2520online%2520adaptation%2520in%2520RAG%252C%2520we%2520propose%2520the%250AAutoRAG-HP%2520framework%252C%2520which%2520formulates%2520the%2520hyper-parameter%2520tuning%2520as%2520an%2520online%250Amulti-armed%2520bandit%2520%2528MAB%2529%2520problem%2520and%2520introduces%2520a%2520novel%2520two-level%2520Hierarchical%250AMAB%2520%2528Hier-MAB%2529%2520method%2520for%2520efficient%2520exploration%2520of%2520large%2520search%2520spaces.%2520We%250Aconduct%2520extensive%2520experiments%2520on%2520tuning%2520hyper-parameters%252C%2520such%2520as%2520top-k%250Aretrieved%2520documents%252C%2520prompt%2520compression%2520ratio%252C%2520and%2520embedding%2520methods%252C%2520using%2520the%250AALCE-ASQA%2520and%2520Natural%2520Questions%2520datasets.%2520Our%2520evaluation%2520from%2520jointly%250Aoptimization%2520all%2520three%2520hyper-parameters%2520demonstrate%2520that%2520MAB-based%2520online%250Alearning%2520methods%2520can%2520achieve%2520Recall%25405%2520%2524%255Capprox%25200.8%2524%2520for%2520scenarios%2520with%250Aprominent%2520gradients%2520in%2520search%2520space%252C%2520using%2520only%2520%2524%255Csim20%255C%2525%2524%2520of%2520the%2520LLM%2520API%2520calls%250Arequired%2520by%2520the%2520Grid%2520Search%2520approach.%2520Additionally%252C%2520the%2520proposed%2520Hier-MAB%250Aapproach%2520outperforms%2520other%2520baselines%2520in%2520more%2520challenging%2520optimization%250Ascenarios.%2520The%2520code%2520will%2520be%2520made%2520available%2520at%2520https%253A//aka.ms/autorag.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoRAG-HP%3A%20Automatic%20Online%20Hyper-Parameter%20Tuning%20for%0A%20%20Retrieval-Augmented%20Generation&entry.906535625=Jia%20Fu%20and%20Xiaoting%20Qin%20and%20Fangkai%20Yang%20and%20Lu%20Wang%20and%20Jue%20Zhang%20and%20Qingwei%20Lin%20and%20Yubo%20Chen%20and%20Dongmei%20Zhang%20and%20Saravan%20Rajmohan%20and%20Qi%20Zhang&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Language%20Models%20have%20transformed%20ML/AI%0Adevelopment%2C%20necessitating%20a%20reevaluation%20of%20AutoML%20principles%20for%20the%0ARetrieval-Augmented%20Generation%20%28RAG%29%20systems.%20To%20address%20the%20challenges%20of%0Ahyper-parameter%20optimization%20and%20online%20adaptation%20in%20RAG%2C%20we%20propose%20the%0AAutoRAG-HP%20framework%2C%20which%20formulates%20the%20hyper-parameter%20tuning%20as%20an%20online%0Amulti-armed%20bandit%20%28MAB%29%20problem%20and%20introduces%20a%20novel%20two-level%20Hierarchical%0AMAB%20%28Hier-MAB%29%20method%20for%20efficient%20exploration%20of%20large%20search%20spaces.%20We%0Aconduct%20extensive%20experiments%20on%20tuning%20hyper-parameters%2C%20such%20as%20top-k%0Aretrieved%20documents%2C%20prompt%20compression%20ratio%2C%20and%20embedding%20methods%2C%20using%20the%0AALCE-ASQA%20and%20Natural%20Questions%20datasets.%20Our%20evaluation%20from%20jointly%0Aoptimization%20all%20three%20hyper-parameters%20demonstrate%20that%20MAB-based%20online%0Alearning%20methods%20can%20achieve%20Recall%405%20%24%5Capprox%200.8%24%20for%20scenarios%20with%0Aprominent%20gradients%20in%20search%20space%2C%20using%20only%20%24%5Csim20%5C%25%24%20of%20the%20LLM%20API%20calls%0Arequired%20by%20the%20Grid%20Search%20approach.%20Additionally%2C%20the%20proposed%20Hier-MAB%0Aapproach%20outperforms%20other%20baselines%20in%20more%20challenging%20optimization%0Ascenarios.%20The%20code%20will%20be%20made%20available%20at%20https%3A//aka.ms/autorag.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19251v1&entry.124074799=Read"},
{"title": "The Remarkable Robustness of LLMs: Stages of Inference?", "author": "Vedang Lad and Wes Gurnee and Max Tegmark", "abstract": "  We demonstrate and investigate the remarkable robustness of Large Language\nModels by deleting and swapping adjacent layers. We find that deleting and\nswapping interventions retain 72-95\\% of the original model's prediction\naccuracy without fine-tuning, whereas models with more layers exhibit more\nrobustness. Based on the results of the layer-wise intervention and further\nexperiments, we hypothesize the existence of four universal stages of inference\nacross eight different models: detokenization, feature engineering, prediction\nensembling, and residual sharpening. The first stage integrates local\ninformation, lifting raw token representations into higher-level contextual\nrepresentations. Next is the iterative refinement of task and entity-specific\nfeatures. Then, the second half of the model begins with a phase transition,\nwhere hidden representations align more with the vocabulary space due to\nspecialized model components. Finally, the last layer sharpens the following\ntoken distribution by eliminating obsolete features that add noise to the\nprediction.\n", "link": "http://arxiv.org/abs/2406.19384v1", "date": "2024-06-27", "relevancy": 1.9281, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4922}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4886}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Remarkable%20Robustness%20of%20LLMs%3A%20Stages%20of%20Inference%3F&body=Title%3A%20The%20Remarkable%20Robustness%20of%20LLMs%3A%20Stages%20of%20Inference%3F%0AAuthor%3A%20Vedang%20Lad%20and%20Wes%20Gurnee%20and%20Max%20Tegmark%0AAbstract%3A%20%20%20We%20demonstrate%20and%20investigate%20the%20remarkable%20robustness%20of%20Large%20Language%0AModels%20by%20deleting%20and%20swapping%20adjacent%20layers.%20We%20find%20that%20deleting%20and%0Aswapping%20interventions%20retain%2072-95%5C%25%20of%20the%20original%20model%27s%20prediction%0Aaccuracy%20without%20fine-tuning%2C%20whereas%20models%20with%20more%20layers%20exhibit%20more%0Arobustness.%20Based%20on%20the%20results%20of%20the%20layer-wise%20intervention%20and%20further%0Aexperiments%2C%20we%20hypothesize%20the%20existence%20of%20four%20universal%20stages%20of%20inference%0Aacross%20eight%20different%20models%3A%20detokenization%2C%20feature%20engineering%2C%20prediction%0Aensembling%2C%20and%20residual%20sharpening.%20The%20first%20stage%20integrates%20local%0Ainformation%2C%20lifting%20raw%20token%20representations%20into%20higher-level%20contextual%0Arepresentations.%20Next%20is%20the%20iterative%20refinement%20of%20task%20and%20entity-specific%0Afeatures.%20Then%2C%20the%20second%20half%20of%20the%20model%20begins%20with%20a%20phase%20transition%2C%0Awhere%20hidden%20representations%20align%20more%20with%20the%20vocabulary%20space%20due%20to%0Aspecialized%20model%20components.%20Finally%2C%20the%20last%20layer%20sharpens%20the%20following%0Atoken%20distribution%20by%20eliminating%20obsolete%20features%20that%20add%20noise%20to%20the%0Aprediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19384v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Remarkable%2520Robustness%2520of%2520LLMs%253A%2520Stages%2520of%2520Inference%253F%26entry.906535625%3DVedang%2520Lad%2520and%2520Wes%2520Gurnee%2520and%2520Max%2520Tegmark%26entry.1292438233%3D%2520%2520We%2520demonstrate%2520and%2520investigate%2520the%2520remarkable%2520robustness%2520of%2520Large%2520Language%250AModels%2520by%2520deleting%2520and%2520swapping%2520adjacent%2520layers.%2520We%2520find%2520that%2520deleting%2520and%250Aswapping%2520interventions%2520retain%252072-95%255C%2525%2520of%2520the%2520original%2520model%2527s%2520prediction%250Aaccuracy%2520without%2520fine-tuning%252C%2520whereas%2520models%2520with%2520more%2520layers%2520exhibit%2520more%250Arobustness.%2520Based%2520on%2520the%2520results%2520of%2520the%2520layer-wise%2520intervention%2520and%2520further%250Aexperiments%252C%2520we%2520hypothesize%2520the%2520existence%2520of%2520four%2520universal%2520stages%2520of%2520inference%250Aacross%2520eight%2520different%2520models%253A%2520detokenization%252C%2520feature%2520engineering%252C%2520prediction%250Aensembling%252C%2520and%2520residual%2520sharpening.%2520The%2520first%2520stage%2520integrates%2520local%250Ainformation%252C%2520lifting%2520raw%2520token%2520representations%2520into%2520higher-level%2520contextual%250Arepresentations.%2520Next%2520is%2520the%2520iterative%2520refinement%2520of%2520task%2520and%2520entity-specific%250Afeatures.%2520Then%252C%2520the%2520second%2520half%2520of%2520the%2520model%2520begins%2520with%2520a%2520phase%2520transition%252C%250Awhere%2520hidden%2520representations%2520align%2520more%2520with%2520the%2520vocabulary%2520space%2520due%2520to%250Aspecialized%2520model%2520components.%2520Finally%252C%2520the%2520last%2520layer%2520sharpens%2520the%2520following%250Atoken%2520distribution%2520by%2520eliminating%2520obsolete%2520features%2520that%2520add%2520noise%2520to%2520the%250Aprediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19384v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Remarkable%20Robustness%20of%20LLMs%3A%20Stages%20of%20Inference%3F&entry.906535625=Vedang%20Lad%20and%20Wes%20Gurnee%20and%20Max%20Tegmark&entry.1292438233=%20%20We%20demonstrate%20and%20investigate%20the%20remarkable%20robustness%20of%20Large%20Language%0AModels%20by%20deleting%20and%20swapping%20adjacent%20layers.%20We%20find%20that%20deleting%20and%0Aswapping%20interventions%20retain%2072-95%5C%25%20of%20the%20original%20model%27s%20prediction%0Aaccuracy%20without%20fine-tuning%2C%20whereas%20models%20with%20more%20layers%20exhibit%20more%0Arobustness.%20Based%20on%20the%20results%20of%20the%20layer-wise%20intervention%20and%20further%0Aexperiments%2C%20we%20hypothesize%20the%20existence%20of%20four%20universal%20stages%20of%20inference%0Aacross%20eight%20different%20models%3A%20detokenization%2C%20feature%20engineering%2C%20prediction%0Aensembling%2C%20and%20residual%20sharpening.%20The%20first%20stage%20integrates%20local%0Ainformation%2C%20lifting%20raw%20token%20representations%20into%20higher-level%20contextual%0Arepresentations.%20Next%20is%20the%20iterative%20refinement%20of%20task%20and%20entity-specific%0Afeatures.%20Then%2C%20the%20second%20half%20of%20the%20model%20begins%20with%20a%20phase%20transition%2C%0Awhere%20hidden%20representations%20align%20more%20with%20the%20vocabulary%20space%20due%20to%0Aspecialized%20model%20components.%20Finally%2C%20the%20last%20layer%20sharpens%20the%20following%0Atoken%20distribution%20by%20eliminating%20obsolete%20features%20that%20add%20noise%20to%20the%0Aprediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19384v1&entry.124074799=Read"},
{"title": "Application of ASV for Voice Identification after VC and Duration\n  Predictor Improvement in TTS Models", "author": "Borodin Kirill Nikolayevich and Kudryavtsev Vasiliy Dmitrievich and Mkrtchian Grach Maratovich and Gorodnichev Mikhail Genadievich and Korzh Dmitrii Sergeevich", "abstract": "  One of the most crucial components in the field of biometric security is the\nautomatic speaker verification system, which is based on the speaker's voice.\nIt is possible to utilise ASVs in isolation or in conjunction with other AI\nmodels. In the contemporary era, the quality and quantity of neural networks\nare increasing exponentially. Concurrently, there is a growing number of\nsystems that aim to manipulate data through the use of voice conversion and\ntext-to-speech models. The field of voice biometrics forgery is aided by a\nnumber of challenges, including SSTC, ASVSpoof, and SingFake.\n  This paper presents a system for automatic speaker verification. The primary\nobjective of our model is the extraction of embeddings from the target\nspeaker's audio in order to obtain information about important characteristics\nof his voice, such as pitch, energy, and the duration of phonemes. This\ninformation is used in our multivoice TTS pipeline, which is currently under\ndevelopment. However, this model was employed within the SSTC challenge to\nverify users whose voice had undergone voice conversion, where it demonstrated\nan EER of 20.669.\n", "link": "http://arxiv.org/abs/2406.19243v1", "date": "2024-06-27", "relevancy": 1.9255, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5046}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4822}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Application%20of%20ASV%20for%20Voice%20Identification%20after%20VC%20and%20Duration%0A%20%20Predictor%20Improvement%20in%20TTS%20Models&body=Title%3A%20Application%20of%20ASV%20for%20Voice%20Identification%20after%20VC%20and%20Duration%0A%20%20Predictor%20Improvement%20in%20TTS%20Models%0AAuthor%3A%20Borodin%20Kirill%20Nikolayevich%20and%20Kudryavtsev%20Vasiliy%20Dmitrievich%20and%20Mkrtchian%20Grach%20Maratovich%20and%20Gorodnichev%20Mikhail%20Genadievich%20and%20Korzh%20Dmitrii%20Sergeevich%0AAbstract%3A%20%20%20One%20of%20the%20most%20crucial%20components%20in%20the%20field%20of%20biometric%20security%20is%20the%0Aautomatic%20speaker%20verification%20system%2C%20which%20is%20based%20on%20the%20speaker%27s%20voice.%0AIt%20is%20possible%20to%20utilise%20ASVs%20in%20isolation%20or%20in%20conjunction%20with%20other%20AI%0Amodels.%20In%20the%20contemporary%20era%2C%20the%20quality%20and%20quantity%20of%20neural%20networks%0Aare%20increasing%20exponentially.%20Concurrently%2C%20there%20is%20a%20growing%20number%20of%0Asystems%20that%20aim%20to%20manipulate%20data%20through%20the%20use%20of%20voice%20conversion%20and%0Atext-to-speech%20models.%20The%20field%20of%20voice%20biometrics%20forgery%20is%20aided%20by%20a%0Anumber%20of%20challenges%2C%20including%20SSTC%2C%20ASVSpoof%2C%20and%20SingFake.%0A%20%20This%20paper%20presents%20a%20system%20for%20automatic%20speaker%20verification.%20The%20primary%0Aobjective%20of%20our%20model%20is%20the%20extraction%20of%20embeddings%20from%20the%20target%0Aspeaker%27s%20audio%20in%20order%20to%20obtain%20information%20about%20important%20characteristics%0Aof%20his%20voice%2C%20such%20as%20pitch%2C%20energy%2C%20and%20the%20duration%20of%20phonemes.%20This%0Ainformation%20is%20used%20in%20our%20multivoice%20TTS%20pipeline%2C%20which%20is%20currently%20under%0Adevelopment.%20However%2C%20this%20model%20was%20employed%20within%20the%20SSTC%20challenge%20to%0Averify%20users%20whose%20voice%20had%20undergone%20voice%20conversion%2C%20where%20it%20demonstrated%0Aan%20EER%20of%2020.669.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19243v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplication%2520of%2520ASV%2520for%2520Voice%2520Identification%2520after%2520VC%2520and%2520Duration%250A%2520%2520Predictor%2520Improvement%2520in%2520TTS%2520Models%26entry.906535625%3DBorodin%2520Kirill%2520Nikolayevich%2520and%2520Kudryavtsev%2520Vasiliy%2520Dmitrievich%2520and%2520Mkrtchian%2520Grach%2520Maratovich%2520and%2520Gorodnichev%2520Mikhail%2520Genadievich%2520and%2520Korzh%2520Dmitrii%2520Sergeevich%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520most%2520crucial%2520components%2520in%2520the%2520field%2520of%2520biometric%2520security%2520is%2520the%250Aautomatic%2520speaker%2520verification%2520system%252C%2520which%2520is%2520based%2520on%2520the%2520speaker%2527s%2520voice.%250AIt%2520is%2520possible%2520to%2520utilise%2520ASVs%2520in%2520isolation%2520or%2520in%2520conjunction%2520with%2520other%2520AI%250Amodels.%2520In%2520the%2520contemporary%2520era%252C%2520the%2520quality%2520and%2520quantity%2520of%2520neural%2520networks%250Aare%2520increasing%2520exponentially.%2520Concurrently%252C%2520there%2520is%2520a%2520growing%2520number%2520of%250Asystems%2520that%2520aim%2520to%2520manipulate%2520data%2520through%2520the%2520use%2520of%2520voice%2520conversion%2520and%250Atext-to-speech%2520models.%2520The%2520field%2520of%2520voice%2520biometrics%2520forgery%2520is%2520aided%2520by%2520a%250Anumber%2520of%2520challenges%252C%2520including%2520SSTC%252C%2520ASVSpoof%252C%2520and%2520SingFake.%250A%2520%2520This%2520paper%2520presents%2520a%2520system%2520for%2520automatic%2520speaker%2520verification.%2520The%2520primary%250Aobjective%2520of%2520our%2520model%2520is%2520the%2520extraction%2520of%2520embeddings%2520from%2520the%2520target%250Aspeaker%2527s%2520audio%2520in%2520order%2520to%2520obtain%2520information%2520about%2520important%2520characteristics%250Aof%2520his%2520voice%252C%2520such%2520as%2520pitch%252C%2520energy%252C%2520and%2520the%2520duration%2520of%2520phonemes.%2520This%250Ainformation%2520is%2520used%2520in%2520our%2520multivoice%2520TTS%2520pipeline%252C%2520which%2520is%2520currently%2520under%250Adevelopment.%2520However%252C%2520this%2520model%2520was%2520employed%2520within%2520the%2520SSTC%2520challenge%2520to%250Averify%2520users%2520whose%2520voice%2520had%2520undergone%2520voice%2520conversion%252C%2520where%2520it%2520demonstrated%250Aan%2520EER%2520of%252020.669.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19243v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Application%20of%20ASV%20for%20Voice%20Identification%20after%20VC%20and%20Duration%0A%20%20Predictor%20Improvement%20in%20TTS%20Models&entry.906535625=Borodin%20Kirill%20Nikolayevich%20and%20Kudryavtsev%20Vasiliy%20Dmitrievich%20and%20Mkrtchian%20Grach%20Maratovich%20and%20Gorodnichev%20Mikhail%20Genadievich%20and%20Korzh%20Dmitrii%20Sergeevich&entry.1292438233=%20%20One%20of%20the%20most%20crucial%20components%20in%20the%20field%20of%20biometric%20security%20is%20the%0Aautomatic%20speaker%20verification%20system%2C%20which%20is%20based%20on%20the%20speaker%27s%20voice.%0AIt%20is%20possible%20to%20utilise%20ASVs%20in%20isolation%20or%20in%20conjunction%20with%20other%20AI%0Amodels.%20In%20the%20contemporary%20era%2C%20the%20quality%20and%20quantity%20of%20neural%20networks%0Aare%20increasing%20exponentially.%20Concurrently%2C%20there%20is%20a%20growing%20number%20of%0Asystems%20that%20aim%20to%20manipulate%20data%20through%20the%20use%20of%20voice%20conversion%20and%0Atext-to-speech%20models.%20The%20field%20of%20voice%20biometrics%20forgery%20is%20aided%20by%20a%0Anumber%20of%20challenges%2C%20including%20SSTC%2C%20ASVSpoof%2C%20and%20SingFake.%0A%20%20This%20paper%20presents%20a%20system%20for%20automatic%20speaker%20verification.%20The%20primary%0Aobjective%20of%20our%20model%20is%20the%20extraction%20of%20embeddings%20from%20the%20target%0Aspeaker%27s%20audio%20in%20order%20to%20obtain%20information%20about%20important%20characteristics%0Aof%20his%20voice%2C%20such%20as%20pitch%2C%20energy%2C%20and%20the%20duration%20of%20phonemes.%20This%0Ainformation%20is%20used%20in%20our%20multivoice%20TTS%20pipeline%2C%20which%20is%20currently%20under%0Adevelopment.%20However%2C%20this%20model%20was%20employed%20within%20the%20SSTC%20challenge%20to%0Averify%20users%20whose%20voice%20had%20undergone%20voice%20conversion%2C%20where%20it%20demonstrated%0Aan%20EER%20of%2020.669.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19243v1&entry.124074799=Read"},
{"title": "Efficient Interaction-Aware Interval Analysis of Neural Network Feedback\n  Loops", "author": "Saber Jafarpour and Akash Harapanahalli and Samuel Coogan", "abstract": "  In this paper, we propose a computationally efficient framework for interval\nreachability of systems with neural network controllers. Our approach leverages\ninclusion functions for the open-loop system and the neural network controller\nto embed the closed-loop system into a larger-dimensional embedding system,\nwhere a single trajectory over-approximates the original system's behavior\nunder uncertainty. We propose two methods for constructing closed-loop\nembedding systems, which account for the interactions between the system and\nthe controller in different ways. The interconnection-based approach considers\nthe worst-case evolution of each coordinate separately by substituting the\nneural network inclusion function into the open-loop inclusion function. The\ninteraction-based approach uses novel Jacobian-based inclusion functions to\ncapture the first-order interactions between the open-loop system and the\ncontroller by leveraging state-of-the-art neural network verifiers. Finally, we\nimplement our approach in a Python framework called ReachMM to demonstrate its\nefficiency and scalability on benchmarks and examples ranging to $200$ state\ndimensions.\n", "link": "http://arxiv.org/abs/2307.14938v3", "date": "2024-06-27", "relevancy": 1.9104, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5268}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4697}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Interaction-Aware%20Interval%20Analysis%20of%20Neural%20Network%20Feedback%0A%20%20Loops&body=Title%3A%20Efficient%20Interaction-Aware%20Interval%20Analysis%20of%20Neural%20Network%20Feedback%0A%20%20Loops%0AAuthor%3A%20Saber%20Jafarpour%20and%20Akash%20Harapanahalli%20and%20Samuel%20Coogan%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20computationally%20efficient%20framework%20for%20interval%0Areachability%20of%20systems%20with%20neural%20network%20controllers.%20Our%20approach%20leverages%0Ainclusion%20functions%20for%20the%20open-loop%20system%20and%20the%20neural%20network%20controller%0Ato%20embed%20the%20closed-loop%20system%20into%20a%20larger-dimensional%20embedding%20system%2C%0Awhere%20a%20single%20trajectory%20over-approximates%20the%20original%20system%27s%20behavior%0Aunder%20uncertainty.%20We%20propose%20two%20methods%20for%20constructing%20closed-loop%0Aembedding%20systems%2C%20which%20account%20for%20the%20interactions%20between%20the%20system%20and%0Athe%20controller%20in%20different%20ways.%20The%20interconnection-based%20approach%20considers%0Athe%20worst-case%20evolution%20of%20each%20coordinate%20separately%20by%20substituting%20the%0Aneural%20network%20inclusion%20function%20into%20the%20open-loop%20inclusion%20function.%20The%0Ainteraction-based%20approach%20uses%20novel%20Jacobian-based%20inclusion%20functions%20to%0Acapture%20the%20first-order%20interactions%20between%20the%20open-loop%20system%20and%20the%0Acontroller%20by%20leveraging%20state-of-the-art%20neural%20network%20verifiers.%20Finally%2C%20we%0Aimplement%20our%20approach%20in%20a%20Python%20framework%20called%20ReachMM%20to%20demonstrate%20its%0Aefficiency%20and%20scalability%20on%20benchmarks%20and%20examples%20ranging%20to%20%24200%24%20state%0Adimensions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.14938v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Interaction-Aware%2520Interval%2520Analysis%2520of%2520Neural%2520Network%2520Feedback%250A%2520%2520Loops%26entry.906535625%3DSaber%2520Jafarpour%2520and%2520Akash%2520Harapanahalli%2520and%2520Samuel%2520Coogan%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520computationally%2520efficient%2520framework%2520for%2520interval%250Areachability%2520of%2520systems%2520with%2520neural%2520network%2520controllers.%2520Our%2520approach%2520leverages%250Ainclusion%2520functions%2520for%2520the%2520open-loop%2520system%2520and%2520the%2520neural%2520network%2520controller%250Ato%2520embed%2520the%2520closed-loop%2520system%2520into%2520a%2520larger-dimensional%2520embedding%2520system%252C%250Awhere%2520a%2520single%2520trajectory%2520over-approximates%2520the%2520original%2520system%2527s%2520behavior%250Aunder%2520uncertainty.%2520We%2520propose%2520two%2520methods%2520for%2520constructing%2520closed-loop%250Aembedding%2520systems%252C%2520which%2520account%2520for%2520the%2520interactions%2520between%2520the%2520system%2520and%250Athe%2520controller%2520in%2520different%2520ways.%2520The%2520interconnection-based%2520approach%2520considers%250Athe%2520worst-case%2520evolution%2520of%2520each%2520coordinate%2520separately%2520by%2520substituting%2520the%250Aneural%2520network%2520inclusion%2520function%2520into%2520the%2520open-loop%2520inclusion%2520function.%2520The%250Ainteraction-based%2520approach%2520uses%2520novel%2520Jacobian-based%2520inclusion%2520functions%2520to%250Acapture%2520the%2520first-order%2520interactions%2520between%2520the%2520open-loop%2520system%2520and%2520the%250Acontroller%2520by%2520leveraging%2520state-of-the-art%2520neural%2520network%2520verifiers.%2520Finally%252C%2520we%250Aimplement%2520our%2520approach%2520in%2520a%2520Python%2520framework%2520called%2520ReachMM%2520to%2520demonstrate%2520its%250Aefficiency%2520and%2520scalability%2520on%2520benchmarks%2520and%2520examples%2520ranging%2520to%2520%2524200%2524%2520state%250Adimensions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.14938v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Interaction-Aware%20Interval%20Analysis%20of%20Neural%20Network%20Feedback%0A%20%20Loops&entry.906535625=Saber%20Jafarpour%20and%20Akash%20Harapanahalli%20and%20Samuel%20Coogan&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20computationally%20efficient%20framework%20for%20interval%0Areachability%20of%20systems%20with%20neural%20network%20controllers.%20Our%20approach%20leverages%0Ainclusion%20functions%20for%20the%20open-loop%20system%20and%20the%20neural%20network%20controller%0Ato%20embed%20the%20closed-loop%20system%20into%20a%20larger-dimensional%20embedding%20system%2C%0Awhere%20a%20single%20trajectory%20over-approximates%20the%20original%20system%27s%20behavior%0Aunder%20uncertainty.%20We%20propose%20two%20methods%20for%20constructing%20closed-loop%0Aembedding%20systems%2C%20which%20account%20for%20the%20interactions%20between%20the%20system%20and%0Athe%20controller%20in%20different%20ways.%20The%20interconnection-based%20approach%20considers%0Athe%20worst-case%20evolution%20of%20each%20coordinate%20separately%20by%20substituting%20the%0Aneural%20network%20inclusion%20function%20into%20the%20open-loop%20inclusion%20function.%20The%0Ainteraction-based%20approach%20uses%20novel%20Jacobian-based%20inclusion%20functions%20to%0Acapture%20the%20first-order%20interactions%20between%20the%20open-loop%20system%20and%20the%0Acontroller%20by%20leveraging%20state-of-the-art%20neural%20network%20verifiers.%20Finally%2C%20we%0Aimplement%20our%20approach%20in%20a%20Python%20framework%20called%20ReachMM%20to%20demonstrate%20its%0Aefficiency%20and%20scalability%20on%20benchmarks%20and%20examples%20ranging%20to%20%24200%24%20state%0Adimensions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.14938v3&entry.124074799=Read"},
{"title": "Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite\n  Kernel Strategy in One-Class Classification", "author": "Muhammad Uzair Zahid and Aysen Degerli and Fahad Sohrab and Serkan Kiranyaz and Tahir Hamid and Rashid Mazhar and Moncef Gabbouj", "abstract": "  Early detection of myocardial infarction (MI), a critical condition arising\nfrom coronary artery disease (CAD), is vital to prevent further myocardial\ndamage. This study introduces a novel method for early MI detection using a\none-class classification (OCC) algorithm in echocardiography. Our study\novercomes the challenge of limited echocardiography data availability by\nadopting a novel approach based on Multi-modal Subspace Support Vector Data\nDescription. The proposed technique involves a specialized MI detection\nframework employing multi-view echocardiography incorporating a composite\nkernel in the non-linear projection trick, fusing Gaussian and Laplacian\nsigmoid functions. Additionally, we enhance the update strategy of the\nprojection matrices by adapting maximization for both or one of the modalities\nin the optimization process. Our method boosts MI detection capability by\nefficiently transforming features extracted from echocardiography data into an\noptimized lower-dimensional subspace. The OCC model trained specifically on\ntarget class instances from the comprehensive HMC-QU dataset that includes\nmultiple echocardiography views indicates a marked improvement in MI detection\naccuracy. Our findings reveal that our proposed multi-view approach achieves a\ngeometric mean of 71.24%, signifying a substantial advancement in\nechocardiography-based MI diagnosis and offering more precise and efficient\ndiagnostic tools.\n", "link": "http://arxiv.org/abs/2402.06530v3", "date": "2024-06-27", "relevancy": 1.8997, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4851}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4847}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Refining%20Myocardial%20Infarction%20Detection%3A%20A%20Novel%20Multi-Modal%20Composite%0A%20%20Kernel%20Strategy%20in%20One-Class%20Classification&body=Title%3A%20Refining%20Myocardial%20Infarction%20Detection%3A%20A%20Novel%20Multi-Modal%20Composite%0A%20%20Kernel%20Strategy%20in%20One-Class%20Classification%0AAuthor%3A%20Muhammad%20Uzair%20Zahid%20and%20Aysen%20Degerli%20and%20Fahad%20Sohrab%20and%20Serkan%20Kiranyaz%20and%20Tahir%20Hamid%20and%20Rashid%20Mazhar%20and%20Moncef%20Gabbouj%0AAbstract%3A%20%20%20Early%20detection%20of%20myocardial%20infarction%20%28MI%29%2C%20a%20critical%20condition%20arising%0Afrom%20coronary%20artery%20disease%20%28CAD%29%2C%20is%20vital%20to%20prevent%20further%20myocardial%0Adamage.%20This%20study%20introduces%20a%20novel%20method%20for%20early%20MI%20detection%20using%20a%0Aone-class%20classification%20%28OCC%29%20algorithm%20in%20echocardiography.%20Our%20study%0Aovercomes%20the%20challenge%20of%20limited%20echocardiography%20data%20availability%20by%0Aadopting%20a%20novel%20approach%20based%20on%20Multi-modal%20Subspace%20Support%20Vector%20Data%0ADescription.%20The%20proposed%20technique%20involves%20a%20specialized%20MI%20detection%0Aframework%20employing%20multi-view%20echocardiography%20incorporating%20a%20composite%0Akernel%20in%20the%20non-linear%20projection%20trick%2C%20fusing%20Gaussian%20and%20Laplacian%0Asigmoid%20functions.%20Additionally%2C%20we%20enhance%20the%20update%20strategy%20of%20the%0Aprojection%20matrices%20by%20adapting%20maximization%20for%20both%20or%20one%20of%20the%20modalities%0Ain%20the%20optimization%20process.%20Our%20method%20boosts%20MI%20detection%20capability%20by%0Aefficiently%20transforming%20features%20extracted%20from%20echocardiography%20data%20into%20an%0Aoptimized%20lower-dimensional%20subspace.%20The%20OCC%20model%20trained%20specifically%20on%0Atarget%20class%20instances%20from%20the%20comprehensive%20HMC-QU%20dataset%20that%20includes%0Amultiple%20echocardiography%20views%20indicates%20a%20marked%20improvement%20in%20MI%20detection%0Aaccuracy.%20Our%20findings%20reveal%20that%20our%20proposed%20multi-view%20approach%20achieves%20a%0Ageometric%20mean%20of%2071.24%25%2C%20signifying%20a%20substantial%20advancement%20in%0Aechocardiography-based%20MI%20diagnosis%20and%20offering%20more%20precise%20and%20efficient%0Adiagnostic%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06530v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefining%2520Myocardial%2520Infarction%2520Detection%253A%2520A%2520Novel%2520Multi-Modal%2520Composite%250A%2520%2520Kernel%2520Strategy%2520in%2520One-Class%2520Classification%26entry.906535625%3DMuhammad%2520Uzair%2520Zahid%2520and%2520Aysen%2520Degerli%2520and%2520Fahad%2520Sohrab%2520and%2520Serkan%2520Kiranyaz%2520and%2520Tahir%2520Hamid%2520and%2520Rashid%2520Mazhar%2520and%2520Moncef%2520Gabbouj%26entry.1292438233%3D%2520%2520Early%2520detection%2520of%2520myocardial%2520infarction%2520%2528MI%2529%252C%2520a%2520critical%2520condition%2520arising%250Afrom%2520coronary%2520artery%2520disease%2520%2528CAD%2529%252C%2520is%2520vital%2520to%2520prevent%2520further%2520myocardial%250Adamage.%2520This%2520study%2520introduces%2520a%2520novel%2520method%2520for%2520early%2520MI%2520detection%2520using%2520a%250Aone-class%2520classification%2520%2528OCC%2529%2520algorithm%2520in%2520echocardiography.%2520Our%2520study%250Aovercomes%2520the%2520challenge%2520of%2520limited%2520echocardiography%2520data%2520availability%2520by%250Aadopting%2520a%2520novel%2520approach%2520based%2520on%2520Multi-modal%2520Subspace%2520Support%2520Vector%2520Data%250ADescription.%2520The%2520proposed%2520technique%2520involves%2520a%2520specialized%2520MI%2520detection%250Aframework%2520employing%2520multi-view%2520echocardiography%2520incorporating%2520a%2520composite%250Akernel%2520in%2520the%2520non-linear%2520projection%2520trick%252C%2520fusing%2520Gaussian%2520and%2520Laplacian%250Asigmoid%2520functions.%2520Additionally%252C%2520we%2520enhance%2520the%2520update%2520strategy%2520of%2520the%250Aprojection%2520matrices%2520by%2520adapting%2520maximization%2520for%2520both%2520or%2520one%2520of%2520the%2520modalities%250Ain%2520the%2520optimization%2520process.%2520Our%2520method%2520boosts%2520MI%2520detection%2520capability%2520by%250Aefficiently%2520transforming%2520features%2520extracted%2520from%2520echocardiography%2520data%2520into%2520an%250Aoptimized%2520lower-dimensional%2520subspace.%2520The%2520OCC%2520model%2520trained%2520specifically%2520on%250Atarget%2520class%2520instances%2520from%2520the%2520comprehensive%2520HMC-QU%2520dataset%2520that%2520includes%250Amultiple%2520echocardiography%2520views%2520indicates%2520a%2520marked%2520improvement%2520in%2520MI%2520detection%250Aaccuracy.%2520Our%2520findings%2520reveal%2520that%2520our%2520proposed%2520multi-view%2520approach%2520achieves%2520a%250Ageometric%2520mean%2520of%252071.24%2525%252C%2520signifying%2520a%2520substantial%2520advancement%2520in%250Aechocardiography-based%2520MI%2520diagnosis%2520and%2520offering%2520more%2520precise%2520and%2520efficient%250Adiagnostic%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.06530v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Refining%20Myocardial%20Infarction%20Detection%3A%20A%20Novel%20Multi-Modal%20Composite%0A%20%20Kernel%20Strategy%20in%20One-Class%20Classification&entry.906535625=Muhammad%20Uzair%20Zahid%20and%20Aysen%20Degerli%20and%20Fahad%20Sohrab%20and%20Serkan%20Kiranyaz%20and%20Tahir%20Hamid%20and%20Rashid%20Mazhar%20and%20Moncef%20Gabbouj&entry.1292438233=%20%20Early%20detection%20of%20myocardial%20infarction%20%28MI%29%2C%20a%20critical%20condition%20arising%0Afrom%20coronary%20artery%20disease%20%28CAD%29%2C%20is%20vital%20to%20prevent%20further%20myocardial%0Adamage.%20This%20study%20introduces%20a%20novel%20method%20for%20early%20MI%20detection%20using%20a%0Aone-class%20classification%20%28OCC%29%20algorithm%20in%20echocardiography.%20Our%20study%0Aovercomes%20the%20challenge%20of%20limited%20echocardiography%20data%20availability%20by%0Aadopting%20a%20novel%20approach%20based%20on%20Multi-modal%20Subspace%20Support%20Vector%20Data%0ADescription.%20The%20proposed%20technique%20involves%20a%20specialized%20MI%20detection%0Aframework%20employing%20multi-view%20echocardiography%20incorporating%20a%20composite%0Akernel%20in%20the%20non-linear%20projection%20trick%2C%20fusing%20Gaussian%20and%20Laplacian%0Asigmoid%20functions.%20Additionally%2C%20we%20enhance%20the%20update%20strategy%20of%20the%0Aprojection%20matrices%20by%20adapting%20maximization%20for%20both%20or%20one%20of%20the%20modalities%0Ain%20the%20optimization%20process.%20Our%20method%20boosts%20MI%20detection%20capability%20by%0Aefficiently%20transforming%20features%20extracted%20from%20echocardiography%20data%20into%20an%0Aoptimized%20lower-dimensional%20subspace.%20The%20OCC%20model%20trained%20specifically%20on%0Atarget%20class%20instances%20from%20the%20comprehensive%20HMC-QU%20dataset%20that%20includes%0Amultiple%20echocardiography%20views%20indicates%20a%20marked%20improvement%20in%20MI%20detection%0Aaccuracy.%20Our%20findings%20reveal%20that%20our%20proposed%20multi-view%20approach%20achieves%20a%0Ageometric%20mean%20of%2071.24%25%2C%20signifying%20a%20substantial%20advancement%20in%0Aechocardiography-based%20MI%20diagnosis%20and%20offering%20more%20precise%20and%20efficient%0Adiagnostic%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06530v3&entry.124074799=Read"},
{"title": "VDebugger: Harnessing Execution Feedback for Debugging Visual Programs", "author": "Xueqing Wu and Zongyu Lin and Songyan Zhao and Te-Lin Wu and Pan Lu and Nanyun Peng and Kai-Wei Chang", "abstract": "  Visual programs are executable code generated by large language models to\naddress visual reasoning problems. They decompose complex questions into\nmultiple reasoning steps and invoke specialized models for each step to solve\nthe problems. However, these programs are prone to logic errors, with our\npreliminary evaluation showing that 58% of the total errors are caused by\nprogram logic errors. Debugging complex visual programs remains a major\nbottleneck for visual reasoning. To address this, we introduce VDebugger, a\nnovel critic-refiner framework trained to localize and debug visual programs by\ntracking execution step by step. VDebugger identifies and corrects program\nerrors leveraging detailed execution feedback, improving interpretability and\naccuracy. The training data is generated through an automated pipeline that\ninjects errors into correct visual programs using a novel mask-best decoding\ntechnique. Evaluations on six datasets demonstrate VDebugger's effectiveness,\nshowing performance improvements of up to 3.2% in downstream task accuracy.\nFurther studies show VDebugger's ability to generalize to unseen tasks,\nbringing a notable improvement of 2.3% on the unseen COVR task. Code, data and\nmodels are made publicly available at https://github.com/shirley-wu/vdebugger/\n", "link": "http://arxiv.org/abs/2406.13444v2", "date": "2024-06-27", "relevancy": 1.8954, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4811}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4703}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VDebugger%3A%20Harnessing%20Execution%20Feedback%20for%20Debugging%20Visual%20Programs&body=Title%3A%20VDebugger%3A%20Harnessing%20Execution%20Feedback%20for%20Debugging%20Visual%20Programs%0AAuthor%3A%20Xueqing%20Wu%20and%20Zongyu%20Lin%20and%20Songyan%20Zhao%20and%20Te-Lin%20Wu%20and%20Pan%20Lu%20and%20Nanyun%20Peng%20and%20Kai-Wei%20Chang%0AAbstract%3A%20%20%20Visual%20programs%20are%20executable%20code%20generated%20by%20large%20language%20models%20to%0Aaddress%20visual%20reasoning%20problems.%20They%20decompose%20complex%20questions%20into%0Amultiple%20reasoning%20steps%20and%20invoke%20specialized%20models%20for%20each%20step%20to%20solve%0Athe%20problems.%20However%2C%20these%20programs%20are%20prone%20to%20logic%20errors%2C%20with%20our%0Apreliminary%20evaluation%20showing%20that%2058%25%20of%20the%20total%20errors%20are%20caused%20by%0Aprogram%20logic%20errors.%20Debugging%20complex%20visual%20programs%20remains%20a%20major%0Abottleneck%20for%20visual%20reasoning.%20To%20address%20this%2C%20we%20introduce%20VDebugger%2C%20a%0Anovel%20critic-refiner%20framework%20trained%20to%20localize%20and%20debug%20visual%20programs%20by%0Atracking%20execution%20step%20by%20step.%20VDebugger%20identifies%20and%20corrects%20program%0Aerrors%20leveraging%20detailed%20execution%20feedback%2C%20improving%20interpretability%20and%0Aaccuracy.%20The%20training%20data%20is%20generated%20through%20an%20automated%20pipeline%20that%0Ainjects%20errors%20into%20correct%20visual%20programs%20using%20a%20novel%20mask-best%20decoding%0Atechnique.%20Evaluations%20on%20six%20datasets%20demonstrate%20VDebugger%27s%20effectiveness%2C%0Ashowing%20performance%20improvements%20of%20up%20to%203.2%25%20in%20downstream%20task%20accuracy.%0AFurther%20studies%20show%20VDebugger%27s%20ability%20to%20generalize%20to%20unseen%20tasks%2C%0Abringing%20a%20notable%20improvement%20of%202.3%25%20on%20the%20unseen%20COVR%20task.%20Code%2C%20data%20and%0Amodels%20are%20made%20publicly%20available%20at%20https%3A//github.com/shirley-wu/vdebugger/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13444v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVDebugger%253A%2520Harnessing%2520Execution%2520Feedback%2520for%2520Debugging%2520Visual%2520Programs%26entry.906535625%3DXueqing%2520Wu%2520and%2520Zongyu%2520Lin%2520and%2520Songyan%2520Zhao%2520and%2520Te-Lin%2520Wu%2520and%2520Pan%2520Lu%2520and%2520Nanyun%2520Peng%2520and%2520Kai-Wei%2520Chang%26entry.1292438233%3D%2520%2520Visual%2520programs%2520are%2520executable%2520code%2520generated%2520by%2520large%2520language%2520models%2520to%250Aaddress%2520visual%2520reasoning%2520problems.%2520They%2520decompose%2520complex%2520questions%2520into%250Amultiple%2520reasoning%2520steps%2520and%2520invoke%2520specialized%2520models%2520for%2520each%2520step%2520to%2520solve%250Athe%2520problems.%2520However%252C%2520these%2520programs%2520are%2520prone%2520to%2520logic%2520errors%252C%2520with%2520our%250Apreliminary%2520evaluation%2520showing%2520that%252058%2525%2520of%2520the%2520total%2520errors%2520are%2520caused%2520by%250Aprogram%2520logic%2520errors.%2520Debugging%2520complex%2520visual%2520programs%2520remains%2520a%2520major%250Abottleneck%2520for%2520visual%2520reasoning.%2520To%2520address%2520this%252C%2520we%2520introduce%2520VDebugger%252C%2520a%250Anovel%2520critic-refiner%2520framework%2520trained%2520to%2520localize%2520and%2520debug%2520visual%2520programs%2520by%250Atracking%2520execution%2520step%2520by%2520step.%2520VDebugger%2520identifies%2520and%2520corrects%2520program%250Aerrors%2520leveraging%2520detailed%2520execution%2520feedback%252C%2520improving%2520interpretability%2520and%250Aaccuracy.%2520The%2520training%2520data%2520is%2520generated%2520through%2520an%2520automated%2520pipeline%2520that%250Ainjects%2520errors%2520into%2520correct%2520visual%2520programs%2520using%2520a%2520novel%2520mask-best%2520decoding%250Atechnique.%2520Evaluations%2520on%2520six%2520datasets%2520demonstrate%2520VDebugger%2527s%2520effectiveness%252C%250Ashowing%2520performance%2520improvements%2520of%2520up%2520to%25203.2%2525%2520in%2520downstream%2520task%2520accuracy.%250AFurther%2520studies%2520show%2520VDebugger%2527s%2520ability%2520to%2520generalize%2520to%2520unseen%2520tasks%252C%250Abringing%2520a%2520notable%2520improvement%2520of%25202.3%2525%2520on%2520the%2520unseen%2520COVR%2520task.%2520Code%252C%2520data%2520and%250Amodels%2520are%2520made%2520publicly%2520available%2520at%2520https%253A//github.com/shirley-wu/vdebugger/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13444v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VDebugger%3A%20Harnessing%20Execution%20Feedback%20for%20Debugging%20Visual%20Programs&entry.906535625=Xueqing%20Wu%20and%20Zongyu%20Lin%20and%20Songyan%20Zhao%20and%20Te-Lin%20Wu%20and%20Pan%20Lu%20and%20Nanyun%20Peng%20and%20Kai-Wei%20Chang&entry.1292438233=%20%20Visual%20programs%20are%20executable%20code%20generated%20by%20large%20language%20models%20to%0Aaddress%20visual%20reasoning%20problems.%20They%20decompose%20complex%20questions%20into%0Amultiple%20reasoning%20steps%20and%20invoke%20specialized%20models%20for%20each%20step%20to%20solve%0Athe%20problems.%20However%2C%20these%20programs%20are%20prone%20to%20logic%20errors%2C%20with%20our%0Apreliminary%20evaluation%20showing%20that%2058%25%20of%20the%20total%20errors%20are%20caused%20by%0Aprogram%20logic%20errors.%20Debugging%20complex%20visual%20programs%20remains%20a%20major%0Abottleneck%20for%20visual%20reasoning.%20To%20address%20this%2C%20we%20introduce%20VDebugger%2C%20a%0Anovel%20critic-refiner%20framework%20trained%20to%20localize%20and%20debug%20visual%20programs%20by%0Atracking%20execution%20step%20by%20step.%20VDebugger%20identifies%20and%20corrects%20program%0Aerrors%20leveraging%20detailed%20execution%20feedback%2C%20improving%20interpretability%20and%0Aaccuracy.%20The%20training%20data%20is%20generated%20through%20an%20automated%20pipeline%20that%0Ainjects%20errors%20into%20correct%20visual%20programs%20using%20a%20novel%20mask-best%20decoding%0Atechnique.%20Evaluations%20on%20six%20datasets%20demonstrate%20VDebugger%27s%20effectiveness%2C%0Ashowing%20performance%20improvements%20of%20up%20to%203.2%25%20in%20downstream%20task%20accuracy.%0AFurther%20studies%20show%20VDebugger%27s%20ability%20to%20generalize%20to%20unseen%20tasks%2C%0Abringing%20a%20notable%20improvement%20of%202.3%25%20on%20the%20unseen%20COVR%20task.%20Code%2C%20data%20and%0Amodels%20are%20made%20publicly%20available%20at%20https%3A//github.com/shirley-wu/vdebugger/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13444v2&entry.124074799=Read"},
{"title": "Dataset Size Recovery from LoRA Weights", "author": "Mohammad Salama and Jonathan Kahana and Eliahu Horwitz and Yedid Hoshen", "abstract": "  Model inversion and membership inference attacks aim to reconstruct and\nverify the data which a model was trained on. However, they are not guaranteed\nto find all training samples as they do not know the size of the training set.\nIn this paper, we introduce a new task: dataset size recovery, that aims to\ndetermine the number of samples used to train a model, directly from its\nweights. We then propose DSiRe, a method for recovering the number of images\nused to fine-tune a model, in the common case where fine-tuning uses LoRA. We\ndiscover that both the norm and the spectrum of the LoRA matrices are closely\nlinked to the fine-tuning dataset size; we leverage this finding to propose a\nsimple yet effective prediction algorithm. To evaluate dataset size recovery of\nLoRA weights, we develop and release a new benchmark, LoRA-WiSE, consisting of\nover 25000 weight snapshots from more than 2000 diverse LoRA fine-tuned models.\nOur best classifier can predict the number of fine-tuning images with a mean\nabsolute error of 0.36 images, establishing the feasibility of this attack.\n", "link": "http://arxiv.org/abs/2406.19395v1", "date": "2024-06-27", "relevancy": 1.894, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4827}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4677}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dataset%20Size%20Recovery%20from%20LoRA%20Weights&body=Title%3A%20Dataset%20Size%20Recovery%20from%20LoRA%20Weights%0AAuthor%3A%20Mohammad%20Salama%20and%20Jonathan%20Kahana%20and%20Eliahu%20Horwitz%20and%20Yedid%20Hoshen%0AAbstract%3A%20%20%20Model%20inversion%20and%20membership%20inference%20attacks%20aim%20to%20reconstruct%20and%0Averify%20the%20data%20which%20a%20model%20was%20trained%20on.%20However%2C%20they%20are%20not%20guaranteed%0Ato%20find%20all%20training%20samples%20as%20they%20do%20not%20know%20the%20size%20of%20the%20training%20set.%0AIn%20this%20paper%2C%20we%20introduce%20a%20new%20task%3A%20dataset%20size%20recovery%2C%20that%20aims%20to%0Adetermine%20the%20number%20of%20samples%20used%20to%20train%20a%20model%2C%20directly%20from%20its%0Aweights.%20We%20then%20propose%20DSiRe%2C%20a%20method%20for%20recovering%20the%20number%20of%20images%0Aused%20to%20fine-tune%20a%20model%2C%20in%20the%20common%20case%20where%20fine-tuning%20uses%20LoRA.%20We%0Adiscover%20that%20both%20the%20norm%20and%20the%20spectrum%20of%20the%20LoRA%20matrices%20are%20closely%0Alinked%20to%20the%20fine-tuning%20dataset%20size%3B%20we%20leverage%20this%20finding%20to%20propose%20a%0Asimple%20yet%20effective%20prediction%20algorithm.%20To%20evaluate%20dataset%20size%20recovery%20of%0ALoRA%20weights%2C%20we%20develop%20and%20release%20a%20new%20benchmark%2C%20LoRA-WiSE%2C%20consisting%20of%0Aover%2025000%20weight%20snapshots%20from%20more%20than%202000%20diverse%20LoRA%20fine-tuned%20models.%0AOur%20best%20classifier%20can%20predict%20the%20number%20of%20fine-tuning%20images%20with%20a%20mean%0Aabsolute%20error%20of%200.36%20images%2C%20establishing%20the%20feasibility%20of%20this%20attack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19395v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDataset%2520Size%2520Recovery%2520from%2520LoRA%2520Weights%26entry.906535625%3DMohammad%2520Salama%2520and%2520Jonathan%2520Kahana%2520and%2520Eliahu%2520Horwitz%2520and%2520Yedid%2520Hoshen%26entry.1292438233%3D%2520%2520Model%2520inversion%2520and%2520membership%2520inference%2520attacks%2520aim%2520to%2520reconstruct%2520and%250Averify%2520the%2520data%2520which%2520a%2520model%2520was%2520trained%2520on.%2520However%252C%2520they%2520are%2520not%2520guaranteed%250Ato%2520find%2520all%2520training%2520samples%2520as%2520they%2520do%2520not%2520know%2520the%2520size%2520of%2520the%2520training%2520set.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520task%253A%2520dataset%2520size%2520recovery%252C%2520that%2520aims%2520to%250Adetermine%2520the%2520number%2520of%2520samples%2520used%2520to%2520train%2520a%2520model%252C%2520directly%2520from%2520its%250Aweights.%2520We%2520then%2520propose%2520DSiRe%252C%2520a%2520method%2520for%2520recovering%2520the%2520number%2520of%2520images%250Aused%2520to%2520fine-tune%2520a%2520model%252C%2520in%2520the%2520common%2520case%2520where%2520fine-tuning%2520uses%2520LoRA.%2520We%250Adiscover%2520that%2520both%2520the%2520norm%2520and%2520the%2520spectrum%2520of%2520the%2520LoRA%2520matrices%2520are%2520closely%250Alinked%2520to%2520the%2520fine-tuning%2520dataset%2520size%253B%2520we%2520leverage%2520this%2520finding%2520to%2520propose%2520a%250Asimple%2520yet%2520effective%2520prediction%2520algorithm.%2520To%2520evaluate%2520dataset%2520size%2520recovery%2520of%250ALoRA%2520weights%252C%2520we%2520develop%2520and%2520release%2520a%2520new%2520benchmark%252C%2520LoRA-WiSE%252C%2520consisting%2520of%250Aover%252025000%2520weight%2520snapshots%2520from%2520more%2520than%25202000%2520diverse%2520LoRA%2520fine-tuned%2520models.%250AOur%2520best%2520classifier%2520can%2520predict%2520the%2520number%2520of%2520fine-tuning%2520images%2520with%2520a%2520mean%250Aabsolute%2520error%2520of%25200.36%2520images%252C%2520establishing%2520the%2520feasibility%2520of%2520this%2520attack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19395v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dataset%20Size%20Recovery%20from%20LoRA%20Weights&entry.906535625=Mohammad%20Salama%20and%20Jonathan%20Kahana%20and%20Eliahu%20Horwitz%20and%20Yedid%20Hoshen&entry.1292438233=%20%20Model%20inversion%20and%20membership%20inference%20attacks%20aim%20to%20reconstruct%20and%0Averify%20the%20data%20which%20a%20model%20was%20trained%20on.%20However%2C%20they%20are%20not%20guaranteed%0Ato%20find%20all%20training%20samples%20as%20they%20do%20not%20know%20the%20size%20of%20the%20training%20set.%0AIn%20this%20paper%2C%20we%20introduce%20a%20new%20task%3A%20dataset%20size%20recovery%2C%20that%20aims%20to%0Adetermine%20the%20number%20of%20samples%20used%20to%20train%20a%20model%2C%20directly%20from%20its%0Aweights.%20We%20then%20propose%20DSiRe%2C%20a%20method%20for%20recovering%20the%20number%20of%20images%0Aused%20to%20fine-tune%20a%20model%2C%20in%20the%20common%20case%20where%20fine-tuning%20uses%20LoRA.%20We%0Adiscover%20that%20both%20the%20norm%20and%20the%20spectrum%20of%20the%20LoRA%20matrices%20are%20closely%0Alinked%20to%20the%20fine-tuning%20dataset%20size%3B%20we%20leverage%20this%20finding%20to%20propose%20a%0Asimple%20yet%20effective%20prediction%20algorithm.%20To%20evaluate%20dataset%20size%20recovery%20of%0ALoRA%20weights%2C%20we%20develop%20and%20release%20a%20new%20benchmark%2C%20LoRA-WiSE%2C%20consisting%20of%0Aover%2025000%20weight%20snapshots%20from%20more%20than%202000%20diverse%20LoRA%20fine-tuned%20models.%0AOur%20best%20classifier%20can%20predict%20the%20number%20of%20fine-tuning%20images%20with%20a%20mean%0Aabsolute%20error%20of%200.36%20images%2C%20establishing%20the%20feasibility%20of%20this%20attack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19395v1&entry.124074799=Read"},
{"title": "Improving the Expressiveness of $K$-hop Message-Passing GNNs by\n  Injecting Contextualized Substructure Information", "author": "Tianjun Yao and Yiongxu Wang and Kun Zhang and Shangsong Liang", "abstract": "  Graph neural networks (GNNs) have become the \\textit{de facto} standard for\nrepresentational learning in graphs, and have achieved state-of-the-art\nperformance in many graph-related tasks; however, it has been shown that the\nexpressive power of standard GNNs are equivalent maximally to 1-dimensional\nWeisfeiler-Lehman (1-WL) Test. Recently, there is a line of works aiming to\nenhance the expressive power of graph neural networks. One line of such works\naim at developing $K$-hop message-passing GNNs where node representation is\nupdated by aggregating information from not only direct neighbors but all\nneighbors within $K$-hop of the node. Another line of works leverages subgraph\ninformation to enhance the expressive power which is proven to be strictly more\npowerful than 1-WL test. In this work, we discuss the limitation of $K$-hop\nmessage-passing GNNs and propose \\textit{substructure encoding function} to\nuplift the expressive power of any $K$-hop message-passing GNN. We further\ninject contextualized substructure information to enhance the expressiveness of\n$K$-hop message-passing GNNs. Our method is provably more powerful than\nprevious works on $K$-hop graph neural networks and 1-WL subgraph GNNs, which\nis a specific type of subgraph based GNN models, and not less powerful than\n3-WL. Empirically, our proposed method set new state-of-the-art performance or\nachieves comparable performance for a variety of datasets. Our code is\navailable at \\url{https://github.com/tianyao-aka/Expresive_K_hop_GNNs}.\n", "link": "http://arxiv.org/abs/2406.19244v1", "date": "2024-06-27", "relevancy": 1.8936, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.499}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4552}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20the%20Expressiveness%20of%20%24K%24-hop%20Message-Passing%20GNNs%20by%0A%20%20Injecting%20Contextualized%20Substructure%20Information&body=Title%3A%20Improving%20the%20Expressiveness%20of%20%24K%24-hop%20Message-Passing%20GNNs%20by%0A%20%20Injecting%20Contextualized%20Substructure%20Information%0AAuthor%3A%20Tianjun%20Yao%20and%20Yiongxu%20Wang%20and%20Kun%20Zhang%20and%20Shangsong%20Liang%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20become%20the%20%5Ctextit%7Bde%20facto%7D%20standard%20for%0Arepresentational%20learning%20in%20graphs%2C%20and%20have%20achieved%20state-of-the-art%0Aperformance%20in%20many%20graph-related%20tasks%3B%20however%2C%20it%20has%20been%20shown%20that%20the%0Aexpressive%20power%20of%20standard%20GNNs%20are%20equivalent%20maximally%20to%201-dimensional%0AWeisfeiler-Lehman%20%281-WL%29%20Test.%20Recently%2C%20there%20is%20a%20line%20of%20works%20aiming%20to%0Aenhance%20the%20expressive%20power%20of%20graph%20neural%20networks.%20One%20line%20of%20such%20works%0Aaim%20at%20developing%20%24K%24-hop%20message-passing%20GNNs%20where%20node%20representation%20is%0Aupdated%20by%20aggregating%20information%20from%20not%20only%20direct%20neighbors%20but%20all%0Aneighbors%20within%20%24K%24-hop%20of%20the%20node.%20Another%20line%20of%20works%20leverages%20subgraph%0Ainformation%20to%20enhance%20the%20expressive%20power%20which%20is%20proven%20to%20be%20strictly%20more%0Apowerful%20than%201-WL%20test.%20In%20this%20work%2C%20we%20discuss%20the%20limitation%20of%20%24K%24-hop%0Amessage-passing%20GNNs%20and%20propose%20%5Ctextit%7Bsubstructure%20encoding%20function%7D%20to%0Auplift%20the%20expressive%20power%20of%20any%20%24K%24-hop%20message-passing%20GNN.%20We%20further%0Ainject%20contextualized%20substructure%20information%20to%20enhance%20the%20expressiveness%20of%0A%24K%24-hop%20message-passing%20GNNs.%20Our%20method%20is%20provably%20more%20powerful%20than%0Aprevious%20works%20on%20%24K%24-hop%20graph%20neural%20networks%20and%201-WL%20subgraph%20GNNs%2C%20which%0Ais%20a%20specific%20type%20of%20subgraph%20based%20GNN%20models%2C%20and%20not%20less%20powerful%20than%0A3-WL.%20Empirically%2C%20our%20proposed%20method%20set%20new%20state-of-the-art%20performance%20or%0Aachieves%20comparable%20performance%20for%20a%20variety%20of%20datasets.%20Our%20code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/tianyao-aka/Expresive_K_hop_GNNs%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520the%2520Expressiveness%2520of%2520%2524K%2524-hop%2520Message-Passing%2520GNNs%2520by%250A%2520%2520Injecting%2520Contextualized%2520Substructure%2520Information%26entry.906535625%3DTianjun%2520Yao%2520and%2520Yiongxu%2520Wang%2520and%2520Kun%2520Zhang%2520and%2520Shangsong%2520Liang%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520become%2520the%2520%255Ctextit%257Bde%2520facto%257D%2520standard%2520for%250Arepresentational%2520learning%2520in%2520graphs%252C%2520and%2520have%2520achieved%2520state-of-the-art%250Aperformance%2520in%2520many%2520graph-related%2520tasks%253B%2520however%252C%2520it%2520has%2520been%2520shown%2520that%2520the%250Aexpressive%2520power%2520of%2520standard%2520GNNs%2520are%2520equivalent%2520maximally%2520to%25201-dimensional%250AWeisfeiler-Lehman%2520%25281-WL%2529%2520Test.%2520Recently%252C%2520there%2520is%2520a%2520line%2520of%2520works%2520aiming%2520to%250Aenhance%2520the%2520expressive%2520power%2520of%2520graph%2520neural%2520networks.%2520One%2520line%2520of%2520such%2520works%250Aaim%2520at%2520developing%2520%2524K%2524-hop%2520message-passing%2520GNNs%2520where%2520node%2520representation%2520is%250Aupdated%2520by%2520aggregating%2520information%2520from%2520not%2520only%2520direct%2520neighbors%2520but%2520all%250Aneighbors%2520within%2520%2524K%2524-hop%2520of%2520the%2520node.%2520Another%2520line%2520of%2520works%2520leverages%2520subgraph%250Ainformation%2520to%2520enhance%2520the%2520expressive%2520power%2520which%2520is%2520proven%2520to%2520be%2520strictly%2520more%250Apowerful%2520than%25201-WL%2520test.%2520In%2520this%2520work%252C%2520we%2520discuss%2520the%2520limitation%2520of%2520%2524K%2524-hop%250Amessage-passing%2520GNNs%2520and%2520propose%2520%255Ctextit%257Bsubstructure%2520encoding%2520function%257D%2520to%250Auplift%2520the%2520expressive%2520power%2520of%2520any%2520%2524K%2524-hop%2520message-passing%2520GNN.%2520We%2520further%250Ainject%2520contextualized%2520substructure%2520information%2520to%2520enhance%2520the%2520expressiveness%2520of%250A%2524K%2524-hop%2520message-passing%2520GNNs.%2520Our%2520method%2520is%2520provably%2520more%2520powerful%2520than%250Aprevious%2520works%2520on%2520%2524K%2524-hop%2520graph%2520neural%2520networks%2520and%25201-WL%2520subgraph%2520GNNs%252C%2520which%250Ais%2520a%2520specific%2520type%2520of%2520subgraph%2520based%2520GNN%2520models%252C%2520and%2520not%2520less%2520powerful%2520than%250A3-WL.%2520Empirically%252C%2520our%2520proposed%2520method%2520set%2520new%2520state-of-the-art%2520performance%2520or%250Aachieves%2520comparable%2520performance%2520for%2520a%2520variety%2520of%2520datasets.%2520Our%2520code%2520is%250Aavailable%2520at%2520%255Curl%257Bhttps%253A//github.com/tianyao-aka/Expresive_K_hop_GNNs%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20the%20Expressiveness%20of%20%24K%24-hop%20Message-Passing%20GNNs%20by%0A%20%20Injecting%20Contextualized%20Substructure%20Information&entry.906535625=Tianjun%20Yao%20and%20Yiongxu%20Wang%20and%20Kun%20Zhang%20and%20Shangsong%20Liang&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20become%20the%20%5Ctextit%7Bde%20facto%7D%20standard%20for%0Arepresentational%20learning%20in%20graphs%2C%20and%20have%20achieved%20state-of-the-art%0Aperformance%20in%20many%20graph-related%20tasks%3B%20however%2C%20it%20has%20been%20shown%20that%20the%0Aexpressive%20power%20of%20standard%20GNNs%20are%20equivalent%20maximally%20to%201-dimensional%0AWeisfeiler-Lehman%20%281-WL%29%20Test.%20Recently%2C%20there%20is%20a%20line%20of%20works%20aiming%20to%0Aenhance%20the%20expressive%20power%20of%20graph%20neural%20networks.%20One%20line%20of%20such%20works%0Aaim%20at%20developing%20%24K%24-hop%20message-passing%20GNNs%20where%20node%20representation%20is%0Aupdated%20by%20aggregating%20information%20from%20not%20only%20direct%20neighbors%20but%20all%0Aneighbors%20within%20%24K%24-hop%20of%20the%20node.%20Another%20line%20of%20works%20leverages%20subgraph%0Ainformation%20to%20enhance%20the%20expressive%20power%20which%20is%20proven%20to%20be%20strictly%20more%0Apowerful%20than%201-WL%20test.%20In%20this%20work%2C%20we%20discuss%20the%20limitation%20of%20%24K%24-hop%0Amessage-passing%20GNNs%20and%20propose%20%5Ctextit%7Bsubstructure%20encoding%20function%7D%20to%0Auplift%20the%20expressive%20power%20of%20any%20%24K%24-hop%20message-passing%20GNN.%20We%20further%0Ainject%20contextualized%20substructure%20information%20to%20enhance%20the%20expressiveness%20of%0A%24K%24-hop%20message-passing%20GNNs.%20Our%20method%20is%20provably%20more%20powerful%20than%0Aprevious%20works%20on%20%24K%24-hop%20graph%20neural%20networks%20and%201-WL%20subgraph%20GNNs%2C%20which%0Ais%20a%20specific%20type%20of%20subgraph%20based%20GNN%20models%2C%20and%20not%20less%20powerful%20than%0A3-WL.%20Empirically%2C%20our%20proposed%20method%20set%20new%20state-of-the-art%20performance%20or%0Aachieves%20comparable%20performance%20for%20a%20variety%20of%20datasets.%20Our%20code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/tianyao-aka/Expresive_K_hop_GNNs%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19244v1&entry.124074799=Read"},
{"title": "ALMA: a mathematics-driven approach for determining tuning parameters in\n  generalized LASSO problems, with applications to MRI", "author": "Gianluca Giacchi and Isidoros Iakovidis and Bastien Milani and Matthias Stuber and Micah Murray and Benedetta Franceschiello", "abstract": "  Magnetic Resonance Imaging (MRI) is a powerful technique employed for\nnon-invasive in vivo visualization of internal structures. Sparsity is often\ndeployed to accelerate the signal acquisition or overcome the presence of\nmotion artifacts, improving the quality of image reconstruction. Image\nreconstruction algorithms use TV-regularized LASSO (Total Variation-regularized\nLASSO) to retrieve the missing information of undersampled signals, by cleaning\nthe data of noise and while optimizing sparsity. A tuning parameter moderates\nthe balance between these two aspects; its choice affecting the quality of the\nreconstructions. Currently, there is a lack of general deterministic techniques\nto choose these parameters, which are oftentimes manually selected and thus\nhinder the reliability of the reconstructions. Here, we present ALMA (Algorithm\nfor Lagrange Multipliers Approximation), an iterative mathematics-inspired\ntechnique that computes tuning parameters for generalized LASSO problems during\nMRI reconstruction. We analyze quantitatively the performance of these\nparameters for imaging reconstructions via TV-LASSO in an MRI context on\nphantoms. Although our study concentrates on TV-LASSO, the techniques developed\nhere hold significant promise for a wide array of applications. ALMA is not\nonly adaptable to more generalized LASSO problems but is also robust to\naccommodate other forms of regularization beyond total variation. Moreover, it\nextends effectively to handle non-Cartesian sampling trajectories, broadening\nits utility in complex data reconstruction scenarios. More generally, ALMA\nprovides a powerful tool for numerically solving constrained optimization\nproblems across various disciplines, offering a versatile and impactful\nsolution for advanced computational challenges.\n", "link": "http://arxiv.org/abs/2406.19239v1", "date": "2024-06-27", "relevancy": 1.8934, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4804}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4767}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALMA%3A%20a%20mathematics-driven%20approach%20for%20determining%20tuning%20parameters%20in%0A%20%20generalized%20LASSO%20problems%2C%20with%20applications%20to%20MRI&body=Title%3A%20ALMA%3A%20a%20mathematics-driven%20approach%20for%20determining%20tuning%20parameters%20in%0A%20%20generalized%20LASSO%20problems%2C%20with%20applications%20to%20MRI%0AAuthor%3A%20Gianluca%20Giacchi%20and%20Isidoros%20Iakovidis%20and%20Bastien%20Milani%20and%20Matthias%20Stuber%20and%20Micah%20Murray%20and%20Benedetta%20Franceschiello%0AAbstract%3A%20%20%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20is%20a%20powerful%20technique%20employed%20for%0Anon-invasive%20in%20vivo%20visualization%20of%20internal%20structures.%20Sparsity%20is%20often%0Adeployed%20to%20accelerate%20the%20signal%20acquisition%20or%20overcome%20the%20presence%20of%0Amotion%20artifacts%2C%20improving%20the%20quality%20of%20image%20reconstruction.%20Image%0Areconstruction%20algorithms%20use%20TV-regularized%20LASSO%20%28Total%20Variation-regularized%0ALASSO%29%20to%20retrieve%20the%20missing%20information%20of%20undersampled%20signals%2C%20by%20cleaning%0Athe%20data%20of%20noise%20and%20while%20optimizing%20sparsity.%20A%20tuning%20parameter%20moderates%0Athe%20balance%20between%20these%20two%20aspects%3B%20its%20choice%20affecting%20the%20quality%20of%20the%0Areconstructions.%20Currently%2C%20there%20is%20a%20lack%20of%20general%20deterministic%20techniques%0Ato%20choose%20these%20parameters%2C%20which%20are%20oftentimes%20manually%20selected%20and%20thus%0Ahinder%20the%20reliability%20of%20the%20reconstructions.%20Here%2C%20we%20present%20ALMA%20%28Algorithm%0Afor%20Lagrange%20Multipliers%20Approximation%29%2C%20an%20iterative%20mathematics-inspired%0Atechnique%20that%20computes%20tuning%20parameters%20for%20generalized%20LASSO%20problems%20during%0AMRI%20reconstruction.%20We%20analyze%20quantitatively%20the%20performance%20of%20these%0Aparameters%20for%20imaging%20reconstructions%20via%20TV-LASSO%20in%20an%20MRI%20context%20on%0Aphantoms.%20Although%20our%20study%20concentrates%20on%20TV-LASSO%2C%20the%20techniques%20developed%0Ahere%20hold%20significant%20promise%20for%20a%20wide%20array%20of%20applications.%20ALMA%20is%20not%0Aonly%20adaptable%20to%20more%20generalized%20LASSO%20problems%20but%20is%20also%20robust%20to%0Aaccommodate%20other%20forms%20of%20regularization%20beyond%20total%20variation.%20Moreover%2C%20it%0Aextends%20effectively%20to%20handle%20non-Cartesian%20sampling%20trajectories%2C%20broadening%0Aits%20utility%20in%20complex%20data%20reconstruction%20scenarios.%20More%20generally%2C%20ALMA%0Aprovides%20a%20powerful%20tool%20for%20numerically%20solving%20constrained%20optimization%0Aproblems%20across%20various%20disciplines%2C%20offering%20a%20versatile%20and%20impactful%0Asolution%20for%20advanced%20computational%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALMA%253A%2520a%2520mathematics-driven%2520approach%2520for%2520determining%2520tuning%2520parameters%2520in%250A%2520%2520generalized%2520LASSO%2520problems%252C%2520with%2520applications%2520to%2520MRI%26entry.906535625%3DGianluca%2520Giacchi%2520and%2520Isidoros%2520Iakovidis%2520and%2520Bastien%2520Milani%2520and%2520Matthias%2520Stuber%2520and%2520Micah%2520Murray%2520and%2520Benedetta%2520Franceschiello%26entry.1292438233%3D%2520%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520is%2520a%2520powerful%2520technique%2520employed%2520for%250Anon-invasive%2520in%2520vivo%2520visualization%2520of%2520internal%2520structures.%2520Sparsity%2520is%2520often%250Adeployed%2520to%2520accelerate%2520the%2520signal%2520acquisition%2520or%2520overcome%2520the%2520presence%2520of%250Amotion%2520artifacts%252C%2520improving%2520the%2520quality%2520of%2520image%2520reconstruction.%2520Image%250Areconstruction%2520algorithms%2520use%2520TV-regularized%2520LASSO%2520%2528Total%2520Variation-regularized%250ALASSO%2529%2520to%2520retrieve%2520the%2520missing%2520information%2520of%2520undersampled%2520signals%252C%2520by%2520cleaning%250Athe%2520data%2520of%2520noise%2520and%2520while%2520optimizing%2520sparsity.%2520A%2520tuning%2520parameter%2520moderates%250Athe%2520balance%2520between%2520these%2520two%2520aspects%253B%2520its%2520choice%2520affecting%2520the%2520quality%2520of%2520the%250Areconstructions.%2520Currently%252C%2520there%2520is%2520a%2520lack%2520of%2520general%2520deterministic%2520techniques%250Ato%2520choose%2520these%2520parameters%252C%2520which%2520are%2520oftentimes%2520manually%2520selected%2520and%2520thus%250Ahinder%2520the%2520reliability%2520of%2520the%2520reconstructions.%2520Here%252C%2520we%2520present%2520ALMA%2520%2528Algorithm%250Afor%2520Lagrange%2520Multipliers%2520Approximation%2529%252C%2520an%2520iterative%2520mathematics-inspired%250Atechnique%2520that%2520computes%2520tuning%2520parameters%2520for%2520generalized%2520LASSO%2520problems%2520during%250AMRI%2520reconstruction.%2520We%2520analyze%2520quantitatively%2520the%2520performance%2520of%2520these%250Aparameters%2520for%2520imaging%2520reconstructions%2520via%2520TV-LASSO%2520in%2520an%2520MRI%2520context%2520on%250Aphantoms.%2520Although%2520our%2520study%2520concentrates%2520on%2520TV-LASSO%252C%2520the%2520techniques%2520developed%250Ahere%2520hold%2520significant%2520promise%2520for%2520a%2520wide%2520array%2520of%2520applications.%2520ALMA%2520is%2520not%250Aonly%2520adaptable%2520to%2520more%2520generalized%2520LASSO%2520problems%2520but%2520is%2520also%2520robust%2520to%250Aaccommodate%2520other%2520forms%2520of%2520regularization%2520beyond%2520total%2520variation.%2520Moreover%252C%2520it%250Aextends%2520effectively%2520to%2520handle%2520non-Cartesian%2520sampling%2520trajectories%252C%2520broadening%250Aits%2520utility%2520in%2520complex%2520data%2520reconstruction%2520scenarios.%2520More%2520generally%252C%2520ALMA%250Aprovides%2520a%2520powerful%2520tool%2520for%2520numerically%2520solving%2520constrained%2520optimization%250Aproblems%2520across%2520various%2520disciplines%252C%2520offering%2520a%2520versatile%2520and%2520impactful%250Asolution%2520for%2520advanced%2520computational%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALMA%3A%20a%20mathematics-driven%20approach%20for%20determining%20tuning%20parameters%20in%0A%20%20generalized%20LASSO%20problems%2C%20with%20applications%20to%20MRI&entry.906535625=Gianluca%20Giacchi%20and%20Isidoros%20Iakovidis%20and%20Bastien%20Milani%20and%20Matthias%20Stuber%20and%20Micah%20Murray%20and%20Benedetta%20Franceschiello&entry.1292438233=%20%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20is%20a%20powerful%20technique%20employed%20for%0Anon-invasive%20in%20vivo%20visualization%20of%20internal%20structures.%20Sparsity%20is%20often%0Adeployed%20to%20accelerate%20the%20signal%20acquisition%20or%20overcome%20the%20presence%20of%0Amotion%20artifacts%2C%20improving%20the%20quality%20of%20image%20reconstruction.%20Image%0Areconstruction%20algorithms%20use%20TV-regularized%20LASSO%20%28Total%20Variation-regularized%0ALASSO%29%20to%20retrieve%20the%20missing%20information%20of%20undersampled%20signals%2C%20by%20cleaning%0Athe%20data%20of%20noise%20and%20while%20optimizing%20sparsity.%20A%20tuning%20parameter%20moderates%0Athe%20balance%20between%20these%20two%20aspects%3B%20its%20choice%20affecting%20the%20quality%20of%20the%0Areconstructions.%20Currently%2C%20there%20is%20a%20lack%20of%20general%20deterministic%20techniques%0Ato%20choose%20these%20parameters%2C%20which%20are%20oftentimes%20manually%20selected%20and%20thus%0Ahinder%20the%20reliability%20of%20the%20reconstructions.%20Here%2C%20we%20present%20ALMA%20%28Algorithm%0Afor%20Lagrange%20Multipliers%20Approximation%29%2C%20an%20iterative%20mathematics-inspired%0Atechnique%20that%20computes%20tuning%20parameters%20for%20generalized%20LASSO%20problems%20during%0AMRI%20reconstruction.%20We%20analyze%20quantitatively%20the%20performance%20of%20these%0Aparameters%20for%20imaging%20reconstructions%20via%20TV-LASSO%20in%20an%20MRI%20context%20on%0Aphantoms.%20Although%20our%20study%20concentrates%20on%20TV-LASSO%2C%20the%20techniques%20developed%0Ahere%20hold%20significant%20promise%20for%20a%20wide%20array%20of%20applications.%20ALMA%20is%20not%0Aonly%20adaptable%20to%20more%20generalized%20LASSO%20problems%20but%20is%20also%20robust%20to%0Aaccommodate%20other%20forms%20of%20regularization%20beyond%20total%20variation.%20Moreover%2C%20it%0Aextends%20effectively%20to%20handle%20non-Cartesian%20sampling%20trajectories%2C%20broadening%0Aits%20utility%20in%20complex%20data%20reconstruction%20scenarios.%20More%20generally%2C%20ALMA%0Aprovides%20a%20powerful%20tool%20for%20numerically%20solving%20constrained%20optimization%0Aproblems%20across%20various%20disciplines%2C%20offering%20a%20versatile%20and%20impactful%0Asolution%20for%20advanced%20computational%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19239v1&entry.124074799=Read"},
{"title": "Single Image Estimation of Cell Migration Direction by Deep Circular\n  Regression", "author": "Lennart Bruns and Lucas Lamparter and Milos Galic and Xiaoyi Jiang", "abstract": "  In this paper we study the problem of estimating the migration direction of\ncells based on a single image. To the best of our knowledge, there is only one\nrelated work that uses a classification CNN for four classes (quadrants). This\napproach does not allow detailed directional resolution. We solve the single\nimage estimation problem using deep circular regression with special attention\nto cycle-sensitive methods. On two databases we achieve an average accuracy of\n$\\sim$17 degrees, which is a significant improvement over the previous work.\n", "link": "http://arxiv.org/abs/2406.19162v1", "date": "2024-06-27", "relevancy": 1.8718, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4878}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4546}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single%20Image%20Estimation%20of%20Cell%20Migration%20Direction%20by%20Deep%20Circular%0A%20%20Regression&body=Title%3A%20Single%20Image%20Estimation%20of%20Cell%20Migration%20Direction%20by%20Deep%20Circular%0A%20%20Regression%0AAuthor%3A%20Lennart%20Bruns%20and%20Lucas%20Lamparter%20and%20Milos%20Galic%20and%20Xiaoyi%20Jiang%0AAbstract%3A%20%20%20In%20this%20paper%20we%20study%20the%20problem%20of%20estimating%20the%20migration%20direction%20of%0Acells%20based%20on%20a%20single%20image.%20To%20the%20best%20of%20our%20knowledge%2C%20there%20is%20only%20one%0Arelated%20work%20that%20uses%20a%20classification%20CNN%20for%20four%20classes%20%28quadrants%29.%20This%0Aapproach%20does%20not%20allow%20detailed%20directional%20resolution.%20We%20solve%20the%20single%0Aimage%20estimation%20problem%20using%20deep%20circular%20regression%20with%20special%20attention%0Ato%20cycle-sensitive%20methods.%20On%20two%20databases%20we%20achieve%20an%20average%20accuracy%20of%0A%24%5Csim%2417%20degrees%2C%20which%20is%20a%20significant%20improvement%20over%20the%20previous%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19162v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle%2520Image%2520Estimation%2520of%2520Cell%2520Migration%2520Direction%2520by%2520Deep%2520Circular%250A%2520%2520Regression%26entry.906535625%3DLennart%2520Bruns%2520and%2520Lucas%2520Lamparter%2520and%2520Milos%2520Galic%2520and%2520Xiaoyi%2520Jiang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520study%2520the%2520problem%2520of%2520estimating%2520the%2520migration%2520direction%2520of%250Acells%2520based%2520on%2520a%2520single%2520image.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520there%2520is%2520only%2520one%250Arelated%2520work%2520that%2520uses%2520a%2520classification%2520CNN%2520for%2520four%2520classes%2520%2528quadrants%2529.%2520This%250Aapproach%2520does%2520not%2520allow%2520detailed%2520directional%2520resolution.%2520We%2520solve%2520the%2520single%250Aimage%2520estimation%2520problem%2520using%2520deep%2520circular%2520regression%2520with%2520special%2520attention%250Ato%2520cycle-sensitive%2520methods.%2520On%2520two%2520databases%2520we%2520achieve%2520an%2520average%2520accuracy%2520of%250A%2524%255Csim%252417%2520degrees%252C%2520which%2520is%2520a%2520significant%2520improvement%2520over%2520the%2520previous%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19162v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single%20Image%20Estimation%20of%20Cell%20Migration%20Direction%20by%20Deep%20Circular%0A%20%20Regression&entry.906535625=Lennart%20Bruns%20and%20Lucas%20Lamparter%20and%20Milos%20Galic%20and%20Xiaoyi%20Jiang&entry.1292438233=%20%20In%20this%20paper%20we%20study%20the%20problem%20of%20estimating%20the%20migration%20direction%20of%0Acells%20based%20on%20a%20single%20image.%20To%20the%20best%20of%20our%20knowledge%2C%20there%20is%20only%20one%0Arelated%20work%20that%20uses%20a%20classification%20CNN%20for%20four%20classes%20%28quadrants%29.%20This%0Aapproach%20does%20not%20allow%20detailed%20directional%20resolution.%20We%20solve%20the%20single%0Aimage%20estimation%20problem%20using%20deep%20circular%20regression%20with%20special%20attention%0Ato%20cycle-sensitive%20methods.%20On%20two%20databases%20we%20achieve%20an%20average%20accuracy%20of%0A%24%5Csim%2417%20degrees%2C%20which%20is%20a%20significant%20improvement%20over%20the%20previous%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19162v1&entry.124074799=Read"},
{"title": "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a\n  supervised-friendly fashion", "author": "Yannis Flet-Berliac and Nathan Grinsztajn and Florian Strub and Eugene Choi and Chris Cremer and Arash Ahmadian and Yash Chandak and Mohammad Gheshlaghi Azar and Olivier Pietquin and Matthieu Geist", "abstract": "  Reinforcement Learning (RL) has been used to finetune Large Language Models\n(LLMs) using a reward model trained from preference data, to better align with\nhuman judgment. The recently introduced direct alignment methods, which are\noften simpler, more stable, and computationally lighter, can more directly\nachieve this. However, these approaches cannot optimize arbitrary rewards, and\nthe preference-based ones are not the only rewards of interest for LLMs (eg.,\nunit tests for code generation or textual entailment for summarization, among\nothers). RL-finetuning is usually done with a variation of policy gradient,\nwhich calls for on-policy or near-on-policy samples, requiring costly\ngenerations. We introduce Contrastive Policy Gradient, or CoPG, a simple and\nmathematically principled new RL algorithm that can estimate the optimal policy\neven from off-policy data. It can be seen as an off-policy policy gradient\napproach that does not rely on important sampling techniques and highlights the\nimportance of using (the right) state baseline. We show this approach to\ngeneralize the direct alignment method IPO (identity preference optimization)\nand classic policy gradient. We experiment with the proposed CoPG on a toy\nbandit problem to illustrate its properties, as well as for finetuning LLMs on\na summarization task, using a learned reward function considered as ground\ntruth for the purpose of the experiments.\n", "link": "http://arxiv.org/abs/2406.19185v1", "date": "2024-06-27", "relevancy": 1.8657, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4716}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4664}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Policy%20Gradient%3A%20Aligning%20LLMs%20on%20sequence-level%20scores%20in%20a%0A%20%20supervised-friendly%20fashion&body=Title%3A%20Contrastive%20Policy%20Gradient%3A%20Aligning%20LLMs%20on%20sequence-level%20scores%20in%20a%0A%20%20supervised-friendly%20fashion%0AAuthor%3A%20Yannis%20Flet-Berliac%20and%20Nathan%20Grinsztajn%20and%20Florian%20Strub%20and%20Eugene%20Choi%20and%20Chris%20Cremer%20and%20Arash%20Ahmadian%20and%20Yash%20Chandak%20and%20Mohammad%20Gheshlaghi%20Azar%20and%20Olivier%20Pietquin%20and%20Matthieu%20Geist%0AAbstract%3A%20%20%20Reinforcement%20Learning%20%28RL%29%20has%20been%20used%20to%20finetune%20Large%20Language%20Models%0A%28LLMs%29%20using%20a%20reward%20model%20trained%20from%20preference%20data%2C%20to%20better%20align%20with%0Ahuman%20judgment.%20The%20recently%20introduced%20direct%20alignment%20methods%2C%20which%20are%0Aoften%20simpler%2C%20more%20stable%2C%20and%20computationally%20lighter%2C%20can%20more%20directly%0Aachieve%20this.%20However%2C%20these%20approaches%20cannot%20optimize%20arbitrary%20rewards%2C%20and%0Athe%20preference-based%20ones%20are%20not%20the%20only%20rewards%20of%20interest%20for%20LLMs%20%28eg.%2C%0Aunit%20tests%20for%20code%20generation%20or%20textual%20entailment%20for%20summarization%2C%20among%0Aothers%29.%20RL-finetuning%20is%20usually%20done%20with%20a%20variation%20of%20policy%20gradient%2C%0Awhich%20calls%20for%20on-policy%20or%20near-on-policy%20samples%2C%20requiring%20costly%0Agenerations.%20We%20introduce%20Contrastive%20Policy%20Gradient%2C%20or%20CoPG%2C%20a%20simple%20and%0Amathematically%20principled%20new%20RL%20algorithm%20that%20can%20estimate%20the%20optimal%20policy%0Aeven%20from%20off-policy%20data.%20It%20can%20be%20seen%20as%20an%20off-policy%20policy%20gradient%0Aapproach%20that%20does%20not%20rely%20on%20important%20sampling%20techniques%20and%20highlights%20the%0Aimportance%20of%20using%20%28the%20right%29%20state%20baseline.%20We%20show%20this%20approach%20to%0Ageneralize%20the%20direct%20alignment%20method%20IPO%20%28identity%20preference%20optimization%29%0Aand%20classic%20policy%20gradient.%20We%20experiment%20with%20the%20proposed%20CoPG%20on%20a%20toy%0Abandit%20problem%20to%20illustrate%20its%20properties%2C%20as%20well%20as%20for%20finetuning%20LLMs%20on%0Aa%20summarization%20task%2C%20using%20a%20learned%20reward%20function%20considered%20as%20ground%0Atruth%20for%20the%20purpose%20of%20the%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Policy%2520Gradient%253A%2520Aligning%2520LLMs%2520on%2520sequence-level%2520scores%2520in%2520a%250A%2520%2520supervised-friendly%2520fashion%26entry.906535625%3DYannis%2520Flet-Berliac%2520and%2520Nathan%2520Grinsztajn%2520and%2520Florian%2520Strub%2520and%2520Eugene%2520Choi%2520and%2520Chris%2520Cremer%2520and%2520Arash%2520Ahmadian%2520and%2520Yash%2520Chandak%2520and%2520Mohammad%2520Gheshlaghi%2520Azar%2520and%2520Olivier%2520Pietquin%2520and%2520Matthieu%2520Geist%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520%2528RL%2529%2520has%2520been%2520used%2520to%2520finetune%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520using%2520a%2520reward%2520model%2520trained%2520from%2520preference%2520data%252C%2520to%2520better%2520align%2520with%250Ahuman%2520judgment.%2520The%2520recently%2520introduced%2520direct%2520alignment%2520methods%252C%2520which%2520are%250Aoften%2520simpler%252C%2520more%2520stable%252C%2520and%2520computationally%2520lighter%252C%2520can%2520more%2520directly%250Aachieve%2520this.%2520However%252C%2520these%2520approaches%2520cannot%2520optimize%2520arbitrary%2520rewards%252C%2520and%250Athe%2520preference-based%2520ones%2520are%2520not%2520the%2520only%2520rewards%2520of%2520interest%2520for%2520LLMs%2520%2528eg.%252C%250Aunit%2520tests%2520for%2520code%2520generation%2520or%2520textual%2520entailment%2520for%2520summarization%252C%2520among%250Aothers%2529.%2520RL-finetuning%2520is%2520usually%2520done%2520with%2520a%2520variation%2520of%2520policy%2520gradient%252C%250Awhich%2520calls%2520for%2520on-policy%2520or%2520near-on-policy%2520samples%252C%2520requiring%2520costly%250Agenerations.%2520We%2520introduce%2520Contrastive%2520Policy%2520Gradient%252C%2520or%2520CoPG%252C%2520a%2520simple%2520and%250Amathematically%2520principled%2520new%2520RL%2520algorithm%2520that%2520can%2520estimate%2520the%2520optimal%2520policy%250Aeven%2520from%2520off-policy%2520data.%2520It%2520can%2520be%2520seen%2520as%2520an%2520off-policy%2520policy%2520gradient%250Aapproach%2520that%2520does%2520not%2520rely%2520on%2520important%2520sampling%2520techniques%2520and%2520highlights%2520the%250Aimportance%2520of%2520using%2520%2528the%2520right%2529%2520state%2520baseline.%2520We%2520show%2520this%2520approach%2520to%250Ageneralize%2520the%2520direct%2520alignment%2520method%2520IPO%2520%2528identity%2520preference%2520optimization%2529%250Aand%2520classic%2520policy%2520gradient.%2520We%2520experiment%2520with%2520the%2520proposed%2520CoPG%2520on%2520a%2520toy%250Abandit%2520problem%2520to%2520illustrate%2520its%2520properties%252C%2520as%2520well%2520as%2520for%2520finetuning%2520LLMs%2520on%250Aa%2520summarization%2520task%252C%2520using%2520a%2520learned%2520reward%2520function%2520considered%2520as%2520ground%250Atruth%2520for%2520the%2520purpose%2520of%2520the%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Policy%20Gradient%3A%20Aligning%20LLMs%20on%20sequence-level%20scores%20in%20a%0A%20%20supervised-friendly%20fashion&entry.906535625=Yannis%20Flet-Berliac%20and%20Nathan%20Grinsztajn%20and%20Florian%20Strub%20and%20Eugene%20Choi%20and%20Chris%20Cremer%20and%20Arash%20Ahmadian%20and%20Yash%20Chandak%20and%20Mohammad%20Gheshlaghi%20Azar%20and%20Olivier%20Pietquin%20and%20Matthieu%20Geist&entry.1292438233=%20%20Reinforcement%20Learning%20%28RL%29%20has%20been%20used%20to%20finetune%20Large%20Language%20Models%0A%28LLMs%29%20using%20a%20reward%20model%20trained%20from%20preference%20data%2C%20to%20better%20align%20with%0Ahuman%20judgment.%20The%20recently%20introduced%20direct%20alignment%20methods%2C%20which%20are%0Aoften%20simpler%2C%20more%20stable%2C%20and%20computationally%20lighter%2C%20can%20more%20directly%0Aachieve%20this.%20However%2C%20these%20approaches%20cannot%20optimize%20arbitrary%20rewards%2C%20and%0Athe%20preference-based%20ones%20are%20not%20the%20only%20rewards%20of%20interest%20for%20LLMs%20%28eg.%2C%0Aunit%20tests%20for%20code%20generation%20or%20textual%20entailment%20for%20summarization%2C%20among%0Aothers%29.%20RL-finetuning%20is%20usually%20done%20with%20a%20variation%20of%20policy%20gradient%2C%0Awhich%20calls%20for%20on-policy%20or%20near-on-policy%20samples%2C%20requiring%20costly%0Agenerations.%20We%20introduce%20Contrastive%20Policy%20Gradient%2C%20or%20CoPG%2C%20a%20simple%20and%0Amathematically%20principled%20new%20RL%20algorithm%20that%20can%20estimate%20the%20optimal%20policy%0Aeven%20from%20off-policy%20data.%20It%20can%20be%20seen%20as%20an%20off-policy%20policy%20gradient%0Aapproach%20that%20does%20not%20rely%20on%20important%20sampling%20techniques%20and%20highlights%20the%0Aimportance%20of%20using%20%28the%20right%29%20state%20baseline.%20We%20show%20this%20approach%20to%0Ageneralize%20the%20direct%20alignment%20method%20IPO%20%28identity%20preference%20optimization%29%0Aand%20classic%20policy%20gradient.%20We%20experiment%20with%20the%20proposed%20CoPG%20on%20a%20toy%0Abandit%20problem%20to%20illustrate%20its%20properties%2C%20as%20well%20as%20for%20finetuning%20LLMs%20on%0Aa%20summarization%20task%2C%20using%20a%20learned%20reward%20function%20considered%20as%20ground%0Atruth%20for%20the%20purpose%20of%20the%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19185v1&entry.124074799=Read"},
{"title": "NTFormer: A Composite Node Tokenized Graph Transformer for Node\n  Classification", "author": "Jinsong Chen and Siyu Jiang and Kun He", "abstract": "  Recently, the emerging graph Transformers have made significant advancements\nfor node classification on graphs. In most graph Transformers, a crucial step\ninvolves transforming the input graph into token sequences as the model input,\nenabling Transformer to effectively learn the node representations. However, we\nobserve that existing methods only express partial graph information of nodes\nthrough single-type token generation. Consequently, they require tailored\nstrategies to encode additional graph-specific features into the Transformer to\nensure the quality of node representation learning, limiting the model\nflexibility to handle diverse graphs. To this end, we propose a new graph\nTransformer called NTFormer to address this issue. NTFormer introduces a novel\ntoken generator called Node2Par, which constructs various token sequences using\ndifferent token elements for each node. This flexibility allows Node2Par to\ngenerate valuable token sequences from different perspectives, ensuring\ncomprehensive expression of rich graph features. Benefiting from the merits of\nNode2Par, NTFormer only leverages a Transformer-based backbone without\ngraph-specific modifications to learn node representations, eliminating the\nneed for graph-specific modifications. Extensive experiments conducted on\nvarious benchmark datasets containing homophily and heterophily graphs with\ndifferent scales demonstrate the superiority of NTFormer over representative\ngraph Transformers and graph neural networks for node classification.\n", "link": "http://arxiv.org/abs/2406.19249v1", "date": "2024-06-27", "relevancy": 1.8489, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4886}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4689}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NTFormer%3A%20A%20Composite%20Node%20Tokenized%20Graph%20Transformer%20for%20Node%0A%20%20Classification&body=Title%3A%20NTFormer%3A%20A%20Composite%20Node%20Tokenized%20Graph%20Transformer%20for%20Node%0A%20%20Classification%0AAuthor%3A%20Jinsong%20Chen%20and%20Siyu%20Jiang%20and%20Kun%20He%0AAbstract%3A%20%20%20Recently%2C%20the%20emerging%20graph%20Transformers%20have%20made%20significant%20advancements%0Afor%20node%20classification%20on%20graphs.%20In%20most%20graph%20Transformers%2C%20a%20crucial%20step%0Ainvolves%20transforming%20the%20input%20graph%20into%20token%20sequences%20as%20the%20model%20input%2C%0Aenabling%20Transformer%20to%20effectively%20learn%20the%20node%20representations.%20However%2C%20we%0Aobserve%20that%20existing%20methods%20only%20express%20partial%20graph%20information%20of%20nodes%0Athrough%20single-type%20token%20generation.%20Consequently%2C%20they%20require%20tailored%0Astrategies%20to%20encode%20additional%20graph-specific%20features%20into%20the%20Transformer%20to%0Aensure%20the%20quality%20of%20node%20representation%20learning%2C%20limiting%20the%20model%0Aflexibility%20to%20handle%20diverse%20graphs.%20To%20this%20end%2C%20we%20propose%20a%20new%20graph%0ATransformer%20called%20NTFormer%20to%20address%20this%20issue.%20NTFormer%20introduces%20a%20novel%0Atoken%20generator%20called%20Node2Par%2C%20which%20constructs%20various%20token%20sequences%20using%0Adifferent%20token%20elements%20for%20each%20node.%20This%20flexibility%20allows%20Node2Par%20to%0Agenerate%20valuable%20token%20sequences%20from%20different%20perspectives%2C%20ensuring%0Acomprehensive%20expression%20of%20rich%20graph%20features.%20Benefiting%20from%20the%20merits%20of%0ANode2Par%2C%20NTFormer%20only%20leverages%20a%20Transformer-based%20backbone%20without%0Agraph-specific%20modifications%20to%20learn%20node%20representations%2C%20eliminating%20the%0Aneed%20for%20graph-specific%20modifications.%20Extensive%20experiments%20conducted%20on%0Avarious%20benchmark%20datasets%20containing%20homophily%20and%20heterophily%20graphs%20with%0Adifferent%20scales%20demonstrate%20the%20superiority%20of%20NTFormer%20over%20representative%0Agraph%20Transformers%20and%20graph%20neural%20networks%20for%20node%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNTFormer%253A%2520A%2520Composite%2520Node%2520Tokenized%2520Graph%2520Transformer%2520for%2520Node%250A%2520%2520Classification%26entry.906535625%3DJinsong%2520Chen%2520and%2520Siyu%2520Jiang%2520and%2520Kun%2520He%26entry.1292438233%3D%2520%2520Recently%252C%2520the%2520emerging%2520graph%2520Transformers%2520have%2520made%2520significant%2520advancements%250Afor%2520node%2520classification%2520on%2520graphs.%2520In%2520most%2520graph%2520Transformers%252C%2520a%2520crucial%2520step%250Ainvolves%2520transforming%2520the%2520input%2520graph%2520into%2520token%2520sequences%2520as%2520the%2520model%2520input%252C%250Aenabling%2520Transformer%2520to%2520effectively%2520learn%2520the%2520node%2520representations.%2520However%252C%2520we%250Aobserve%2520that%2520existing%2520methods%2520only%2520express%2520partial%2520graph%2520information%2520of%2520nodes%250Athrough%2520single-type%2520token%2520generation.%2520Consequently%252C%2520they%2520require%2520tailored%250Astrategies%2520to%2520encode%2520additional%2520graph-specific%2520features%2520into%2520the%2520Transformer%2520to%250Aensure%2520the%2520quality%2520of%2520node%2520representation%2520learning%252C%2520limiting%2520the%2520model%250Aflexibility%2520to%2520handle%2520diverse%2520graphs.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520new%2520graph%250ATransformer%2520called%2520NTFormer%2520to%2520address%2520this%2520issue.%2520NTFormer%2520introduces%2520a%2520novel%250Atoken%2520generator%2520called%2520Node2Par%252C%2520which%2520constructs%2520various%2520token%2520sequences%2520using%250Adifferent%2520token%2520elements%2520for%2520each%2520node.%2520This%2520flexibility%2520allows%2520Node2Par%2520to%250Agenerate%2520valuable%2520token%2520sequences%2520from%2520different%2520perspectives%252C%2520ensuring%250Acomprehensive%2520expression%2520of%2520rich%2520graph%2520features.%2520Benefiting%2520from%2520the%2520merits%2520of%250ANode2Par%252C%2520NTFormer%2520only%2520leverages%2520a%2520Transformer-based%2520backbone%2520without%250Agraph-specific%2520modifications%2520to%2520learn%2520node%2520representations%252C%2520eliminating%2520the%250Aneed%2520for%2520graph-specific%2520modifications.%2520Extensive%2520experiments%2520conducted%2520on%250Avarious%2520benchmark%2520datasets%2520containing%2520homophily%2520and%2520heterophily%2520graphs%2520with%250Adifferent%2520scales%2520demonstrate%2520the%2520superiority%2520of%2520NTFormer%2520over%2520representative%250Agraph%2520Transformers%2520and%2520graph%2520neural%2520networks%2520for%2520node%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NTFormer%3A%20A%20Composite%20Node%20Tokenized%20Graph%20Transformer%20for%20Node%0A%20%20Classification&entry.906535625=Jinsong%20Chen%20and%20Siyu%20Jiang%20and%20Kun%20He&entry.1292438233=%20%20Recently%2C%20the%20emerging%20graph%20Transformers%20have%20made%20significant%20advancements%0Afor%20node%20classification%20on%20graphs.%20In%20most%20graph%20Transformers%2C%20a%20crucial%20step%0Ainvolves%20transforming%20the%20input%20graph%20into%20token%20sequences%20as%20the%20model%20input%2C%0Aenabling%20Transformer%20to%20effectively%20learn%20the%20node%20representations.%20However%2C%20we%0Aobserve%20that%20existing%20methods%20only%20express%20partial%20graph%20information%20of%20nodes%0Athrough%20single-type%20token%20generation.%20Consequently%2C%20they%20require%20tailored%0Astrategies%20to%20encode%20additional%20graph-specific%20features%20into%20the%20Transformer%20to%0Aensure%20the%20quality%20of%20node%20representation%20learning%2C%20limiting%20the%20model%0Aflexibility%20to%20handle%20diverse%20graphs.%20To%20this%20end%2C%20we%20propose%20a%20new%20graph%0ATransformer%20called%20NTFormer%20to%20address%20this%20issue.%20NTFormer%20introduces%20a%20novel%0Atoken%20generator%20called%20Node2Par%2C%20which%20constructs%20various%20token%20sequences%20using%0Adifferent%20token%20elements%20for%20each%20node.%20This%20flexibility%20allows%20Node2Par%20to%0Agenerate%20valuable%20token%20sequences%20from%20different%20perspectives%2C%20ensuring%0Acomprehensive%20expression%20of%20rich%20graph%20features.%20Benefiting%20from%20the%20merits%20of%0ANode2Par%2C%20NTFormer%20only%20leverages%20a%20Transformer-based%20backbone%20without%0Agraph-specific%20modifications%20to%20learn%20node%20representations%2C%20eliminating%20the%0Aneed%20for%20graph-specific%20modifications.%20Extensive%20experiments%20conducted%20on%0Avarious%20benchmark%20datasets%20containing%20homophily%20and%20heterophily%20graphs%20with%0Adifferent%20scales%20demonstrate%20the%20superiority%20of%20NTFormer%20over%20representative%0Agraph%20Transformers%20and%20graph%20neural%20networks%20for%20node%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19249v1&entry.124074799=Read"},
{"title": "Systematically Exploring the Landscape of Grasp Affordances via\n  Behavioral Manifolds", "author": "Michael Zechmair and Yannick Morel", "abstract": "  The use of machine learning to investigate grasp affordances has received\nextensive attention over the past several decades. The existing literature\nprovides a robust basis to build upon, though a number of aspects may be\nimproved. Results commonly work in terms of grasp configuration, with little\nconsideration for the manner in which the grasp may be (re-)produced from a\nreachability and trajectory planning perspective. In addition, the majority of\nexisting learning approaches focus of producing a single viable grasp, offering\nlittle transparency on how the result was reached, or insights on its\nrobustness. We propose a different perspective on grasp affordance learning,\nexplicitly accounting for grasp synthesis; that is, the manner in which\nmanipulator kinematics are used to allow materialization of grasps. The\napproach allows to explicitly map the grasp policy space in terms of generated\ngrasp types and associated grasp quality. Results of numerical simulations\nillustrate merit of the method and highlight the manner in which it may promote\na greater degree of explainability for otherwise intransparent reinforcement\nprocesses.\n", "link": "http://arxiv.org/abs/2405.04188v2", "date": "2024-06-27", "relevancy": 1.8427, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.621}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6157}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Systematically%20Exploring%20the%20Landscape%20of%20Grasp%20Affordances%20via%0A%20%20Behavioral%20Manifolds&body=Title%3A%20Systematically%20Exploring%20the%20Landscape%20of%20Grasp%20Affordances%20via%0A%20%20Behavioral%20Manifolds%0AAuthor%3A%20Michael%20Zechmair%20and%20Yannick%20Morel%0AAbstract%3A%20%20%20The%20use%20of%20machine%20learning%20to%20investigate%20grasp%20affordances%20has%20received%0Aextensive%20attention%20over%20the%20past%20several%20decades.%20The%20existing%20literature%0Aprovides%20a%20robust%20basis%20to%20build%20upon%2C%20though%20a%20number%20of%20aspects%20may%20be%0Aimproved.%20Results%20commonly%20work%20in%20terms%20of%20grasp%20configuration%2C%20with%20little%0Aconsideration%20for%20the%20manner%20in%20which%20the%20grasp%20may%20be%20%28re-%29produced%20from%20a%0Areachability%20and%20trajectory%20planning%20perspective.%20In%20addition%2C%20the%20majority%20of%0Aexisting%20learning%20approaches%20focus%20of%20producing%20a%20single%20viable%20grasp%2C%20offering%0Alittle%20transparency%20on%20how%20the%20result%20was%20reached%2C%20or%20insights%20on%20its%0Arobustness.%20We%20propose%20a%20different%20perspective%20on%20grasp%20affordance%20learning%2C%0Aexplicitly%20accounting%20for%20grasp%20synthesis%3B%20that%20is%2C%20the%20manner%20in%20which%0Amanipulator%20kinematics%20are%20used%20to%20allow%20materialization%20of%20grasps.%20The%0Aapproach%20allows%20to%20explicitly%20map%20the%20grasp%20policy%20space%20in%20terms%20of%20generated%0Agrasp%20types%20and%20associated%20grasp%20quality.%20Results%20of%20numerical%20simulations%0Aillustrate%20merit%20of%20the%20method%20and%20highlight%20the%20manner%20in%20which%20it%20may%20promote%0Aa%20greater%20degree%20of%20explainability%20for%20otherwise%20intransparent%20reinforcement%0Aprocesses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04188v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSystematically%2520Exploring%2520the%2520Landscape%2520of%2520Grasp%2520Affordances%2520via%250A%2520%2520Behavioral%2520Manifolds%26entry.906535625%3DMichael%2520Zechmair%2520and%2520Yannick%2520Morel%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520machine%2520learning%2520to%2520investigate%2520grasp%2520affordances%2520has%2520received%250Aextensive%2520attention%2520over%2520the%2520past%2520several%2520decades.%2520The%2520existing%2520literature%250Aprovides%2520a%2520robust%2520basis%2520to%2520build%2520upon%252C%2520though%2520a%2520number%2520of%2520aspects%2520may%2520be%250Aimproved.%2520Results%2520commonly%2520work%2520in%2520terms%2520of%2520grasp%2520configuration%252C%2520with%2520little%250Aconsideration%2520for%2520the%2520manner%2520in%2520which%2520the%2520grasp%2520may%2520be%2520%2528re-%2529produced%2520from%2520a%250Areachability%2520and%2520trajectory%2520planning%2520perspective.%2520In%2520addition%252C%2520the%2520majority%2520of%250Aexisting%2520learning%2520approaches%2520focus%2520of%2520producing%2520a%2520single%2520viable%2520grasp%252C%2520offering%250Alittle%2520transparency%2520on%2520how%2520the%2520result%2520was%2520reached%252C%2520or%2520insights%2520on%2520its%250Arobustness.%2520We%2520propose%2520a%2520different%2520perspective%2520on%2520grasp%2520affordance%2520learning%252C%250Aexplicitly%2520accounting%2520for%2520grasp%2520synthesis%253B%2520that%2520is%252C%2520the%2520manner%2520in%2520which%250Amanipulator%2520kinematics%2520are%2520used%2520to%2520allow%2520materialization%2520of%2520grasps.%2520The%250Aapproach%2520allows%2520to%2520explicitly%2520map%2520the%2520grasp%2520policy%2520space%2520in%2520terms%2520of%2520generated%250Agrasp%2520types%2520and%2520associated%2520grasp%2520quality.%2520Results%2520of%2520numerical%2520simulations%250Aillustrate%2520merit%2520of%2520the%2520method%2520and%2520highlight%2520the%2520manner%2520in%2520which%2520it%2520may%2520promote%250Aa%2520greater%2520degree%2520of%2520explainability%2520for%2520otherwise%2520intransparent%2520reinforcement%250Aprocesses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04188v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Systematically%20Exploring%20the%20Landscape%20of%20Grasp%20Affordances%20via%0A%20%20Behavioral%20Manifolds&entry.906535625=Michael%20Zechmair%20and%20Yannick%20Morel&entry.1292438233=%20%20The%20use%20of%20machine%20learning%20to%20investigate%20grasp%20affordances%20has%20received%0Aextensive%20attention%20over%20the%20past%20several%20decades.%20The%20existing%20literature%0Aprovides%20a%20robust%20basis%20to%20build%20upon%2C%20though%20a%20number%20of%20aspects%20may%20be%0Aimproved.%20Results%20commonly%20work%20in%20terms%20of%20grasp%20configuration%2C%20with%20little%0Aconsideration%20for%20the%20manner%20in%20which%20the%20grasp%20may%20be%20%28re-%29produced%20from%20a%0Areachability%20and%20trajectory%20planning%20perspective.%20In%20addition%2C%20the%20majority%20of%0Aexisting%20learning%20approaches%20focus%20of%20producing%20a%20single%20viable%20grasp%2C%20offering%0Alittle%20transparency%20on%20how%20the%20result%20was%20reached%2C%20or%20insights%20on%20its%0Arobustness.%20We%20propose%20a%20different%20perspective%20on%20grasp%20affordance%20learning%2C%0Aexplicitly%20accounting%20for%20grasp%20synthesis%3B%20that%20is%2C%20the%20manner%20in%20which%0Amanipulator%20kinematics%20are%20used%20to%20allow%20materialization%20of%20grasps.%20The%0Aapproach%20allows%20to%20explicitly%20map%20the%20grasp%20policy%20space%20in%20terms%20of%20generated%0Agrasp%20types%20and%20associated%20grasp%20quality.%20Results%20of%20numerical%20simulations%0Aillustrate%20merit%20of%20the%20method%20and%20highlight%20the%20manner%20in%20which%20it%20may%20promote%0Aa%20greater%20degree%20of%20explainability%20for%20otherwise%20intransparent%20reinforcement%0Aprocesses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04188v2&entry.124074799=Read"},
{"title": "Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL", "author": "Zijin Hong and Zheng Yuan and Qinggang Zhang and Hao Chen and Junnan Dong and Feiran Huang and Xiao Huang", "abstract": "  Generating accurate SQL according to natural language questions (text-to-SQL)\nis a long-standing challenge due to the complexities involved in user question\nunderstanding, database schema comprehension, and SQL generation. Conventional\ntext-to-SQL systems, comprising human engineering and deep neural networks,\nhave made substantial progress. Subsequently, pre-trained language models\n(PLMs) have been developed and utilized for text-to-SQL tasks, achieving\npromising performance. As modern databases become more complex, the\ncorresponding user questions also grow more challenging, leading PLMs with\nlimited comprehension capabilities to produce incorrect SQL. This necessitates\nmore sophisticated and tailored optimization methods for PLMs, which, in turn,\nrestricts the applications of PLM-based systems. Most recently, large language\nmodels (LLMs) have demonstrated significant capabilities in natural language\nunderstanding as the model scale remains increasing. Therefore, integrating the\nLLM-based implementation can bring unique opportunities, improvements, and\nsolutions to text-to-SQL research. In this survey, we present a comprehensive\nreview of LLM-based text-to-SQL. Specifically, we propose a brief overview of\nthe technical challenges and the evolutionary process of text-to-SQL. Then, we\nprovide a detailed introduction to the datasets and metrics designed to\nevaluate text-to-SQL systems. After that, we present a systematic analysis of\nrecent advances in LLM-based text-to-SQL. Finally, we discuss the remaining\nchallenges in this field and propose expectations for future research\ndirections.\n", "link": "http://arxiv.org/abs/2406.08426v2", "date": "2024-06-27", "relevancy": 1.8386, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4672}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4556}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Next-Generation%20Database%20Interfaces%3A%20A%20Survey%20of%20LLM-based%20Text-to-SQL&body=Title%3A%20Next-Generation%20Database%20Interfaces%3A%20A%20Survey%20of%20LLM-based%20Text-to-SQL%0AAuthor%3A%20Zijin%20Hong%20and%20Zheng%20Yuan%20and%20Qinggang%20Zhang%20and%20Hao%20Chen%20and%20Junnan%20Dong%20and%20Feiran%20Huang%20and%20Xiao%20Huang%0AAbstract%3A%20%20%20Generating%20accurate%20SQL%20according%20to%20natural%20language%20questions%20%28text-to-SQL%29%0Ais%20a%20long-standing%20challenge%20due%20to%20the%20complexities%20involved%20in%20user%20question%0Aunderstanding%2C%20database%20schema%20comprehension%2C%20and%20SQL%20generation.%20Conventional%0Atext-to-SQL%20systems%2C%20comprising%20human%20engineering%20and%20deep%20neural%20networks%2C%0Ahave%20made%20substantial%20progress.%20Subsequently%2C%20pre-trained%20language%20models%0A%28PLMs%29%20have%20been%20developed%20and%20utilized%20for%20text-to-SQL%20tasks%2C%20achieving%0Apromising%20performance.%20As%20modern%20databases%20become%20more%20complex%2C%20the%0Acorresponding%20user%20questions%20also%20grow%20more%20challenging%2C%20leading%20PLMs%20with%0Alimited%20comprehension%20capabilities%20to%20produce%20incorrect%20SQL.%20This%20necessitates%0Amore%20sophisticated%20and%20tailored%20optimization%20methods%20for%20PLMs%2C%20which%2C%20in%20turn%2C%0Arestricts%20the%20applications%20of%20PLM-based%20systems.%20Most%20recently%2C%20large%20language%0Amodels%20%28LLMs%29%20have%20demonstrated%20significant%20capabilities%20in%20natural%20language%0Aunderstanding%20as%20the%20model%20scale%20remains%20increasing.%20Therefore%2C%20integrating%20the%0ALLM-based%20implementation%20can%20bring%20unique%20opportunities%2C%20improvements%2C%20and%0Asolutions%20to%20text-to-SQL%20research.%20In%20this%20survey%2C%20we%20present%20a%20comprehensive%0Areview%20of%20LLM-based%20text-to-SQL.%20Specifically%2C%20we%20propose%20a%20brief%20overview%20of%0Athe%20technical%20challenges%20and%20the%20evolutionary%20process%20of%20text-to-SQL.%20Then%2C%20we%0Aprovide%20a%20detailed%20introduction%20to%20the%20datasets%20and%20metrics%20designed%20to%0Aevaluate%20text-to-SQL%20systems.%20After%20that%2C%20we%20present%20a%20systematic%20analysis%20of%0Arecent%20advances%20in%20LLM-based%20text-to-SQL.%20Finally%2C%20we%20discuss%20the%20remaining%0Achallenges%20in%20this%20field%20and%20propose%20expectations%20for%20future%20research%0Adirections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08426v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNext-Generation%2520Database%2520Interfaces%253A%2520A%2520Survey%2520of%2520LLM-based%2520Text-to-SQL%26entry.906535625%3DZijin%2520Hong%2520and%2520Zheng%2520Yuan%2520and%2520Qinggang%2520Zhang%2520and%2520Hao%2520Chen%2520and%2520Junnan%2520Dong%2520and%2520Feiran%2520Huang%2520and%2520Xiao%2520Huang%26entry.1292438233%3D%2520%2520Generating%2520accurate%2520SQL%2520according%2520to%2520natural%2520language%2520questions%2520%2528text-to-SQL%2529%250Ais%2520a%2520long-standing%2520challenge%2520due%2520to%2520the%2520complexities%2520involved%2520in%2520user%2520question%250Aunderstanding%252C%2520database%2520schema%2520comprehension%252C%2520and%2520SQL%2520generation.%2520Conventional%250Atext-to-SQL%2520systems%252C%2520comprising%2520human%2520engineering%2520and%2520deep%2520neural%2520networks%252C%250Ahave%2520made%2520substantial%2520progress.%2520Subsequently%252C%2520pre-trained%2520language%2520models%250A%2528PLMs%2529%2520have%2520been%2520developed%2520and%2520utilized%2520for%2520text-to-SQL%2520tasks%252C%2520achieving%250Apromising%2520performance.%2520As%2520modern%2520databases%2520become%2520more%2520complex%252C%2520the%250Acorresponding%2520user%2520questions%2520also%2520grow%2520more%2520challenging%252C%2520leading%2520PLMs%2520with%250Alimited%2520comprehension%2520capabilities%2520to%2520produce%2520incorrect%2520SQL.%2520This%2520necessitates%250Amore%2520sophisticated%2520and%2520tailored%2520optimization%2520methods%2520for%2520PLMs%252C%2520which%252C%2520in%2520turn%252C%250Arestricts%2520the%2520applications%2520of%2520PLM-based%2520systems.%2520Most%2520recently%252C%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520have%2520demonstrated%2520significant%2520capabilities%2520in%2520natural%2520language%250Aunderstanding%2520as%2520the%2520model%2520scale%2520remains%2520increasing.%2520Therefore%252C%2520integrating%2520the%250ALLM-based%2520implementation%2520can%2520bring%2520unique%2520opportunities%252C%2520improvements%252C%2520and%250Asolutions%2520to%2520text-to-SQL%2520research.%2520In%2520this%2520survey%252C%2520we%2520present%2520a%2520comprehensive%250Areview%2520of%2520LLM-based%2520text-to-SQL.%2520Specifically%252C%2520we%2520propose%2520a%2520brief%2520overview%2520of%250Athe%2520technical%2520challenges%2520and%2520the%2520evolutionary%2520process%2520of%2520text-to-SQL.%2520Then%252C%2520we%250Aprovide%2520a%2520detailed%2520introduction%2520to%2520the%2520datasets%2520and%2520metrics%2520designed%2520to%250Aevaluate%2520text-to-SQL%2520systems.%2520After%2520that%252C%2520we%2520present%2520a%2520systematic%2520analysis%2520of%250Arecent%2520advances%2520in%2520LLM-based%2520text-to-SQL.%2520Finally%252C%2520we%2520discuss%2520the%2520remaining%250Achallenges%2520in%2520this%2520field%2520and%2520propose%2520expectations%2520for%2520future%2520research%250Adirections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08426v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Next-Generation%20Database%20Interfaces%3A%20A%20Survey%20of%20LLM-based%20Text-to-SQL&entry.906535625=Zijin%20Hong%20and%20Zheng%20Yuan%20and%20Qinggang%20Zhang%20and%20Hao%20Chen%20and%20Junnan%20Dong%20and%20Feiran%20Huang%20and%20Xiao%20Huang&entry.1292438233=%20%20Generating%20accurate%20SQL%20according%20to%20natural%20language%20questions%20%28text-to-SQL%29%0Ais%20a%20long-standing%20challenge%20due%20to%20the%20complexities%20involved%20in%20user%20question%0Aunderstanding%2C%20database%20schema%20comprehension%2C%20and%20SQL%20generation.%20Conventional%0Atext-to-SQL%20systems%2C%20comprising%20human%20engineering%20and%20deep%20neural%20networks%2C%0Ahave%20made%20substantial%20progress.%20Subsequently%2C%20pre-trained%20language%20models%0A%28PLMs%29%20have%20been%20developed%20and%20utilized%20for%20text-to-SQL%20tasks%2C%20achieving%0Apromising%20performance.%20As%20modern%20databases%20become%20more%20complex%2C%20the%0Acorresponding%20user%20questions%20also%20grow%20more%20challenging%2C%20leading%20PLMs%20with%0Alimited%20comprehension%20capabilities%20to%20produce%20incorrect%20SQL.%20This%20necessitates%0Amore%20sophisticated%20and%20tailored%20optimization%20methods%20for%20PLMs%2C%20which%2C%20in%20turn%2C%0Arestricts%20the%20applications%20of%20PLM-based%20systems.%20Most%20recently%2C%20large%20language%0Amodels%20%28LLMs%29%20have%20demonstrated%20significant%20capabilities%20in%20natural%20language%0Aunderstanding%20as%20the%20model%20scale%20remains%20increasing.%20Therefore%2C%20integrating%20the%0ALLM-based%20implementation%20can%20bring%20unique%20opportunities%2C%20improvements%2C%20and%0Asolutions%20to%20text-to-SQL%20research.%20In%20this%20survey%2C%20we%20present%20a%20comprehensive%0Areview%20of%20LLM-based%20text-to-SQL.%20Specifically%2C%20we%20propose%20a%20brief%20overview%20of%0Athe%20technical%20challenges%20and%20the%20evolutionary%20process%20of%20text-to-SQL.%20Then%2C%20we%0Aprovide%20a%20detailed%20introduction%20to%20the%20datasets%20and%20metrics%20designed%20to%0Aevaluate%20text-to-SQL%20systems.%20After%20that%2C%20we%20present%20a%20systematic%20analysis%20of%0Arecent%20advances%20in%20LLM-based%20text-to-SQL.%20Finally%2C%20we%20discuss%20the%20remaining%0Achallenges%20in%20this%20field%20and%20propose%20expectations%20for%20future%20research%0Adirections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08426v2&entry.124074799=Read"},
{"title": "Estimating Long-term Heterogeneous Dose-response Curve: Generalization\n  Bound Leveraging Optimal Transport Weights", "author": "Zeqin Yang and Weilin Chen and Ruichu Cai and Yuguang Yan and Zhifeng Hao and Zhipeng Yu and Zhichao Zou and Zhen Peng and Jiecheng Guo", "abstract": "  Long-term causal effect estimation is a significant but challenging problem\nin many applications. Existing methods rely on ideal assumptions to estimate\nlong-term average effects, e.g., no unobserved confounders or a binary\ntreatment,while in numerous real-world applications, these assumptions could be\nviolated and average effects are unable to provide individual-level\nsuggestions.In this paper,we address a more general problem of estimating the\nlong-term heterogeneous dose-response curve (HDRC) while accounting for\nunobserved confounders. Specifically, to remove unobserved confounding in\nobservational data, we introduce an optimal transport weighting framework to\nalign the observational data to the experimental data with theoretical\nguarantees. Furthermore,to accurately predict the heterogeneous effects of\ncontinuous treatment, we establish a generalization bound on counterfactual\nprediction error by leveraging the reweighted distribution induced by optimal\ntransport. Finally, we develop an HDRC estimator building upon the above\ntheoretical foundations. Extensive experimental studies conducted on multiple\nsynthetic and semi-synthetic datasets demonstrate the effectiveness of our\nproposed method.\n", "link": "http://arxiv.org/abs/2406.19195v1", "date": "2024-06-27", "relevancy": 1.8065, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4556}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4552}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20Long-term%20Heterogeneous%20Dose-response%20Curve%3A%20Generalization%0A%20%20Bound%20Leveraging%20Optimal%20Transport%20Weights&body=Title%3A%20Estimating%20Long-term%20Heterogeneous%20Dose-response%20Curve%3A%20Generalization%0A%20%20Bound%20Leveraging%20Optimal%20Transport%20Weights%0AAuthor%3A%20Zeqin%20Yang%20and%20Weilin%20Chen%20and%20Ruichu%20Cai%20and%20Yuguang%20Yan%20and%20Zhifeng%20Hao%20and%20Zhipeng%20Yu%20and%20Zhichao%20Zou%20and%20Zhen%20Peng%20and%20Jiecheng%20Guo%0AAbstract%3A%20%20%20Long-term%20causal%20effect%20estimation%20is%20a%20significant%20but%20challenging%20problem%0Ain%20many%20applications.%20Existing%20methods%20rely%20on%20ideal%20assumptions%20to%20estimate%0Along-term%20average%20effects%2C%20e.g.%2C%20no%20unobserved%20confounders%20or%20a%20binary%0Atreatment%2Cwhile%20in%20numerous%20real-world%20applications%2C%20these%20assumptions%20could%20be%0Aviolated%20and%20average%20effects%20are%20unable%20to%20provide%20individual-level%0Asuggestions.In%20this%20paper%2Cwe%20address%20a%20more%20general%20problem%20of%20estimating%20the%0Along-term%20heterogeneous%20dose-response%20curve%20%28HDRC%29%20while%20accounting%20for%0Aunobserved%20confounders.%20Specifically%2C%20to%20remove%20unobserved%20confounding%20in%0Aobservational%20data%2C%20we%20introduce%20an%20optimal%20transport%20weighting%20framework%20to%0Aalign%20the%20observational%20data%20to%20the%20experimental%20data%20with%20theoretical%0Aguarantees.%20Furthermore%2Cto%20accurately%20predict%20the%20heterogeneous%20effects%20of%0Acontinuous%20treatment%2C%20we%20establish%20a%20generalization%20bound%20on%20counterfactual%0Aprediction%20error%20by%20leveraging%20the%20reweighted%20distribution%20induced%20by%20optimal%0Atransport.%20Finally%2C%20we%20develop%20an%20HDRC%20estimator%20building%20upon%20the%20above%0Atheoretical%20foundations.%20Extensive%20experimental%20studies%20conducted%20on%20multiple%0Asynthetic%20and%20semi-synthetic%20datasets%20demonstrate%20the%20effectiveness%20of%20our%0Aproposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520Long-term%2520Heterogeneous%2520Dose-response%2520Curve%253A%2520Generalization%250A%2520%2520Bound%2520Leveraging%2520Optimal%2520Transport%2520Weights%26entry.906535625%3DZeqin%2520Yang%2520and%2520Weilin%2520Chen%2520and%2520Ruichu%2520Cai%2520and%2520Yuguang%2520Yan%2520and%2520Zhifeng%2520Hao%2520and%2520Zhipeng%2520Yu%2520and%2520Zhichao%2520Zou%2520and%2520Zhen%2520Peng%2520and%2520Jiecheng%2520Guo%26entry.1292438233%3D%2520%2520Long-term%2520causal%2520effect%2520estimation%2520is%2520a%2520significant%2520but%2520challenging%2520problem%250Ain%2520many%2520applications.%2520Existing%2520methods%2520rely%2520on%2520ideal%2520assumptions%2520to%2520estimate%250Along-term%2520average%2520effects%252C%2520e.g.%252C%2520no%2520unobserved%2520confounders%2520or%2520a%2520binary%250Atreatment%252Cwhile%2520in%2520numerous%2520real-world%2520applications%252C%2520these%2520assumptions%2520could%2520be%250Aviolated%2520and%2520average%2520effects%2520are%2520unable%2520to%2520provide%2520individual-level%250Asuggestions.In%2520this%2520paper%252Cwe%2520address%2520a%2520more%2520general%2520problem%2520of%2520estimating%2520the%250Along-term%2520heterogeneous%2520dose-response%2520curve%2520%2528HDRC%2529%2520while%2520accounting%2520for%250Aunobserved%2520confounders.%2520Specifically%252C%2520to%2520remove%2520unobserved%2520confounding%2520in%250Aobservational%2520data%252C%2520we%2520introduce%2520an%2520optimal%2520transport%2520weighting%2520framework%2520to%250Aalign%2520the%2520observational%2520data%2520to%2520the%2520experimental%2520data%2520with%2520theoretical%250Aguarantees.%2520Furthermore%252Cto%2520accurately%2520predict%2520the%2520heterogeneous%2520effects%2520of%250Acontinuous%2520treatment%252C%2520we%2520establish%2520a%2520generalization%2520bound%2520on%2520counterfactual%250Aprediction%2520error%2520by%2520leveraging%2520the%2520reweighted%2520distribution%2520induced%2520by%2520optimal%250Atransport.%2520Finally%252C%2520we%2520develop%2520an%2520HDRC%2520estimator%2520building%2520upon%2520the%2520above%250Atheoretical%2520foundations.%2520Extensive%2520experimental%2520studies%2520conducted%2520on%2520multiple%250Asynthetic%2520and%2520semi-synthetic%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aproposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20Long-term%20Heterogeneous%20Dose-response%20Curve%3A%20Generalization%0A%20%20Bound%20Leveraging%20Optimal%20Transport%20Weights&entry.906535625=Zeqin%20Yang%20and%20Weilin%20Chen%20and%20Ruichu%20Cai%20and%20Yuguang%20Yan%20and%20Zhifeng%20Hao%20and%20Zhipeng%20Yu%20and%20Zhichao%20Zou%20and%20Zhen%20Peng%20and%20Jiecheng%20Guo&entry.1292438233=%20%20Long-term%20causal%20effect%20estimation%20is%20a%20significant%20but%20challenging%20problem%0Ain%20many%20applications.%20Existing%20methods%20rely%20on%20ideal%20assumptions%20to%20estimate%0Along-term%20average%20effects%2C%20e.g.%2C%20no%20unobserved%20confounders%20or%20a%20binary%0Atreatment%2Cwhile%20in%20numerous%20real-world%20applications%2C%20these%20assumptions%20could%20be%0Aviolated%20and%20average%20effects%20are%20unable%20to%20provide%20individual-level%0Asuggestions.In%20this%20paper%2Cwe%20address%20a%20more%20general%20problem%20of%20estimating%20the%0Along-term%20heterogeneous%20dose-response%20curve%20%28HDRC%29%20while%20accounting%20for%0Aunobserved%20confounders.%20Specifically%2C%20to%20remove%20unobserved%20confounding%20in%0Aobservational%20data%2C%20we%20introduce%20an%20optimal%20transport%20weighting%20framework%20to%0Aalign%20the%20observational%20data%20to%20the%20experimental%20data%20with%20theoretical%0Aguarantees.%20Furthermore%2Cto%20accurately%20predict%20the%20heterogeneous%20effects%20of%0Acontinuous%20treatment%2C%20we%20establish%20a%20generalization%20bound%20on%20counterfactual%0Aprediction%20error%20by%20leveraging%20the%20reweighted%20distribution%20induced%20by%20optimal%0Atransport.%20Finally%2C%20we%20develop%20an%20HDRC%20estimator%20building%20upon%20the%20above%0Atheoretical%20foundations.%20Extensive%20experimental%20studies%20conducted%20on%20multiple%0Asynthetic%20and%20semi-synthetic%20datasets%20demonstrate%20the%20effectiveness%20of%20our%0Aproposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19195v1&entry.124074799=Read"},
{"title": "TabReD: A Benchmark of Tabular Machine Learning in-the-Wild", "author": "Ivan Rubachev and Nikolay Kartashev and Yury Gorishniy and Artem Babenko", "abstract": "  Benchmarks that closely reflect downstream application scenarios are\nessential for the streamlined adoption of new research in tabular machine\nlearning (ML). In this work, we examine existing tabular benchmarks and find\ntwo common characteristics of industry-grade tabular data that are\nunderrepresented in the datasets available to the academic community. First,\ntabular data often changes over time in real-world deployment scenarios. This\nimpacts model performance and requires time-based train and test splits for\ncorrect model evaluation. Yet, existing academic tabular datasets often lack\ntimestamp metadata to enable such evaluation. Second, a considerable portion of\ndatasets in production settings stem from extensive data acquisition and\nfeature engineering pipelines. For each specific dataset, this can have a\ndifferent impact on the absolute and relative number of predictive,\nuninformative, and correlated features, which in turn can affect model\nselection. To fill the aforementioned gaps in academic benchmarks, we introduce\nTabReD -- a collection of eight industry-grade tabular datasets covering a wide\nrange of domains from finance to food delivery services. We assess a large\nnumber of tabular ML models in the feature-rich, temporally-evolving data\nsetting facilitated by TabReD. We demonstrate that evaluation on time-based\ndata splits leads to different methods ranking, compared to evaluation on\nrandom splits more common in academic benchmarks. Furthermore, on the TabReD\ndatasets, MLP-like architectures and GBDT show the best results, while more\nsophisticated DL models are yet to prove their effectiveness.\n", "link": "http://arxiv.org/abs/2406.19380v1", "date": "2024-06-27", "relevancy": 1.7993, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4542}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4471}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TabReD%3A%20A%20Benchmark%20of%20Tabular%20Machine%20Learning%20in-the-Wild&body=Title%3A%20TabReD%3A%20A%20Benchmark%20of%20Tabular%20Machine%20Learning%20in-the-Wild%0AAuthor%3A%20Ivan%20Rubachev%20and%20Nikolay%20Kartashev%20and%20Yury%20Gorishniy%20and%20Artem%20Babenko%0AAbstract%3A%20%20%20Benchmarks%20that%20closely%20reflect%20downstream%20application%20scenarios%20are%0Aessential%20for%20the%20streamlined%20adoption%20of%20new%20research%20in%20tabular%20machine%0Alearning%20%28ML%29.%20In%20this%20work%2C%20we%20examine%20existing%20tabular%20benchmarks%20and%20find%0Atwo%20common%20characteristics%20of%20industry-grade%20tabular%20data%20that%20are%0Aunderrepresented%20in%20the%20datasets%20available%20to%20the%20academic%20community.%20First%2C%0Atabular%20data%20often%20changes%20over%20time%20in%20real-world%20deployment%20scenarios.%20This%0Aimpacts%20model%20performance%20and%20requires%20time-based%20train%20and%20test%20splits%20for%0Acorrect%20model%20evaluation.%20Yet%2C%20existing%20academic%20tabular%20datasets%20often%20lack%0Atimestamp%20metadata%20to%20enable%20such%20evaluation.%20Second%2C%20a%20considerable%20portion%20of%0Adatasets%20in%20production%20settings%20stem%20from%20extensive%20data%20acquisition%20and%0Afeature%20engineering%20pipelines.%20For%20each%20specific%20dataset%2C%20this%20can%20have%20a%0Adifferent%20impact%20on%20the%20absolute%20and%20relative%20number%20of%20predictive%2C%0Auninformative%2C%20and%20correlated%20features%2C%20which%20in%20turn%20can%20affect%20model%0Aselection.%20To%20fill%20the%20aforementioned%20gaps%20in%20academic%20benchmarks%2C%20we%20introduce%0ATabReD%20--%20a%20collection%20of%20eight%20industry-grade%20tabular%20datasets%20covering%20a%20wide%0Arange%20of%20domains%20from%20finance%20to%20food%20delivery%20services.%20We%20assess%20a%20large%0Anumber%20of%20tabular%20ML%20models%20in%20the%20feature-rich%2C%20temporally-evolving%20data%0Asetting%20facilitated%20by%20TabReD.%20We%20demonstrate%20that%20evaluation%20on%20time-based%0Adata%20splits%20leads%20to%20different%20methods%20ranking%2C%20compared%20to%20evaluation%20on%0Arandom%20splits%20more%20common%20in%20academic%20benchmarks.%20Furthermore%2C%20on%20the%20TabReD%0Adatasets%2C%20MLP-like%20architectures%20and%20GBDT%20show%20the%20best%20results%2C%20while%20more%0Asophisticated%20DL%20models%20are%20yet%20to%20prove%20their%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTabReD%253A%2520A%2520Benchmark%2520of%2520Tabular%2520Machine%2520Learning%2520in-the-Wild%26entry.906535625%3DIvan%2520Rubachev%2520and%2520Nikolay%2520Kartashev%2520and%2520Yury%2520Gorishniy%2520and%2520Artem%2520Babenko%26entry.1292438233%3D%2520%2520Benchmarks%2520that%2520closely%2520reflect%2520downstream%2520application%2520scenarios%2520are%250Aessential%2520for%2520the%2520streamlined%2520adoption%2520of%2520new%2520research%2520in%2520tabular%2520machine%250Alearning%2520%2528ML%2529.%2520In%2520this%2520work%252C%2520we%2520examine%2520existing%2520tabular%2520benchmarks%2520and%2520find%250Atwo%2520common%2520characteristics%2520of%2520industry-grade%2520tabular%2520data%2520that%2520are%250Aunderrepresented%2520in%2520the%2520datasets%2520available%2520to%2520the%2520academic%2520community.%2520First%252C%250Atabular%2520data%2520often%2520changes%2520over%2520time%2520in%2520real-world%2520deployment%2520scenarios.%2520This%250Aimpacts%2520model%2520performance%2520and%2520requires%2520time-based%2520train%2520and%2520test%2520splits%2520for%250Acorrect%2520model%2520evaluation.%2520Yet%252C%2520existing%2520academic%2520tabular%2520datasets%2520often%2520lack%250Atimestamp%2520metadata%2520to%2520enable%2520such%2520evaluation.%2520Second%252C%2520a%2520considerable%2520portion%2520of%250Adatasets%2520in%2520production%2520settings%2520stem%2520from%2520extensive%2520data%2520acquisition%2520and%250Afeature%2520engineering%2520pipelines.%2520For%2520each%2520specific%2520dataset%252C%2520this%2520can%2520have%2520a%250Adifferent%2520impact%2520on%2520the%2520absolute%2520and%2520relative%2520number%2520of%2520predictive%252C%250Auninformative%252C%2520and%2520correlated%2520features%252C%2520which%2520in%2520turn%2520can%2520affect%2520model%250Aselection.%2520To%2520fill%2520the%2520aforementioned%2520gaps%2520in%2520academic%2520benchmarks%252C%2520we%2520introduce%250ATabReD%2520--%2520a%2520collection%2520of%2520eight%2520industry-grade%2520tabular%2520datasets%2520covering%2520a%2520wide%250Arange%2520of%2520domains%2520from%2520finance%2520to%2520food%2520delivery%2520services.%2520We%2520assess%2520a%2520large%250Anumber%2520of%2520tabular%2520ML%2520models%2520in%2520the%2520feature-rich%252C%2520temporally-evolving%2520data%250Asetting%2520facilitated%2520by%2520TabReD.%2520We%2520demonstrate%2520that%2520evaluation%2520on%2520time-based%250Adata%2520splits%2520leads%2520to%2520different%2520methods%2520ranking%252C%2520compared%2520to%2520evaluation%2520on%250Arandom%2520splits%2520more%2520common%2520in%2520academic%2520benchmarks.%2520Furthermore%252C%2520on%2520the%2520TabReD%250Adatasets%252C%2520MLP-like%2520architectures%2520and%2520GBDT%2520show%2520the%2520best%2520results%252C%2520while%2520more%250Asophisticated%2520DL%2520models%2520are%2520yet%2520to%2520prove%2520their%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TabReD%3A%20A%20Benchmark%20of%20Tabular%20Machine%20Learning%20in-the-Wild&entry.906535625=Ivan%20Rubachev%20and%20Nikolay%20Kartashev%20and%20Yury%20Gorishniy%20and%20Artem%20Babenko&entry.1292438233=%20%20Benchmarks%20that%20closely%20reflect%20downstream%20application%20scenarios%20are%0Aessential%20for%20the%20streamlined%20adoption%20of%20new%20research%20in%20tabular%20machine%0Alearning%20%28ML%29.%20In%20this%20work%2C%20we%20examine%20existing%20tabular%20benchmarks%20and%20find%0Atwo%20common%20characteristics%20of%20industry-grade%20tabular%20data%20that%20are%0Aunderrepresented%20in%20the%20datasets%20available%20to%20the%20academic%20community.%20First%2C%0Atabular%20data%20often%20changes%20over%20time%20in%20real-world%20deployment%20scenarios.%20This%0Aimpacts%20model%20performance%20and%20requires%20time-based%20train%20and%20test%20splits%20for%0Acorrect%20model%20evaluation.%20Yet%2C%20existing%20academic%20tabular%20datasets%20often%20lack%0Atimestamp%20metadata%20to%20enable%20such%20evaluation.%20Second%2C%20a%20considerable%20portion%20of%0Adatasets%20in%20production%20settings%20stem%20from%20extensive%20data%20acquisition%20and%0Afeature%20engineering%20pipelines.%20For%20each%20specific%20dataset%2C%20this%20can%20have%20a%0Adifferent%20impact%20on%20the%20absolute%20and%20relative%20number%20of%20predictive%2C%0Auninformative%2C%20and%20correlated%20features%2C%20which%20in%20turn%20can%20affect%20model%0Aselection.%20To%20fill%20the%20aforementioned%20gaps%20in%20academic%20benchmarks%2C%20we%20introduce%0ATabReD%20--%20a%20collection%20of%20eight%20industry-grade%20tabular%20datasets%20covering%20a%20wide%0Arange%20of%20domains%20from%20finance%20to%20food%20delivery%20services.%20We%20assess%20a%20large%0Anumber%20of%20tabular%20ML%20models%20in%20the%20feature-rich%2C%20temporally-evolving%20data%0Asetting%20facilitated%20by%20TabReD.%20We%20demonstrate%20that%20evaluation%20on%20time-based%0Adata%20splits%20leads%20to%20different%20methods%20ranking%2C%20compared%20to%20evaluation%20on%0Arandom%20splits%20more%20common%20in%20academic%20benchmarks.%20Furthermore%2C%20on%20the%20TabReD%0Adatasets%2C%20MLP-like%20architectures%20and%20GBDT%20show%20the%20best%20results%2C%20while%20more%0Asophisticated%20DL%20models%20are%20yet%20to%20prove%20their%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19380v1&entry.124074799=Read"},
{"title": "A Teacher Is Worth A Million Instructions", "author": "Nikhil Kothari and Ravindra Nayak and Shreyas Shetty and Amey Patil and Nikesh Garera", "abstract": "  Large Language Models(LLMs) have shown exceptional abilities, yet training\nthese models can be quite challenging. There is a strong dependence on the\nquality of data and finding the best instruction tuning set. Further, the\ninherent limitations in training methods create substantial difficulties to\ntrain relatively smaller models with 7B and 13B parameters. In our research, we\nsuggest an improved training method for these models by utilising knowledge\nfrom larger models, such as a mixture of experts (8x7B) architectures. The\nscale of these larger models allows them to capture a wide range of variations\nfrom data alone, making them effective teachers for smaller models. Moreover,\nwe implement a novel post-training domain alignment phase that employs\ndomain-specific expert models to boost domain-specific knowledge during\ntraining while preserving the model's ability to generalise. Fine-tuning\nMistral 7B and 2x7B with our method surpasses the performance of\nstate-of-the-art language models with more than 7B and 13B parameters:\nachieving up to $7.9$ in MT-Bench and $93.04\\%$ on AlpacaEval.\n", "link": "http://arxiv.org/abs/2406.19112v1", "date": "2024-06-27", "relevancy": 1.7771, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4573}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.437}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Teacher%20Is%20Worth%20A%20Million%20Instructions&body=Title%3A%20A%20Teacher%20Is%20Worth%20A%20Million%20Instructions%0AAuthor%3A%20Nikhil%20Kothari%20and%20Ravindra%20Nayak%20and%20Shreyas%20Shetty%20and%20Amey%20Patil%20and%20Nikesh%20Garera%0AAbstract%3A%20%20%20Large%20Language%20Models%28LLMs%29%20have%20shown%20exceptional%20abilities%2C%20yet%20training%0Athese%20models%20can%20be%20quite%20challenging.%20There%20is%20a%20strong%20dependence%20on%20the%0Aquality%20of%20data%20and%20finding%20the%20best%20instruction%20tuning%20set.%20Further%2C%20the%0Ainherent%20limitations%20in%20training%20methods%20create%20substantial%20difficulties%20to%0Atrain%20relatively%20smaller%20models%20with%207B%20and%2013B%20parameters.%20In%20our%20research%2C%20we%0Asuggest%20an%20improved%20training%20method%20for%20these%20models%20by%20utilising%20knowledge%0Afrom%20larger%20models%2C%20such%20as%20a%20mixture%20of%20experts%20%288x7B%29%20architectures.%20The%0Ascale%20of%20these%20larger%20models%20allows%20them%20to%20capture%20a%20wide%20range%20of%20variations%0Afrom%20data%20alone%2C%20making%20them%20effective%20teachers%20for%20smaller%20models.%20Moreover%2C%0Awe%20implement%20a%20novel%20post-training%20domain%20alignment%20phase%20that%20employs%0Adomain-specific%20expert%20models%20to%20boost%20domain-specific%20knowledge%20during%0Atraining%20while%20preserving%20the%20model%27s%20ability%20to%20generalise.%20Fine-tuning%0AMistral%207B%20and%202x7B%20with%20our%20method%20surpasses%20the%20performance%20of%0Astate-of-the-art%20language%20models%20with%20more%20than%207B%20and%2013B%20parameters%3A%0Aachieving%20up%20to%20%247.9%24%20in%20MT-Bench%20and%20%2493.04%5C%25%24%20on%20AlpacaEval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19112v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Teacher%2520Is%2520Worth%2520A%2520Million%2520Instructions%26entry.906535625%3DNikhil%2520Kothari%2520and%2520Ravindra%2520Nayak%2520and%2520Shreyas%2520Shetty%2520and%2520Amey%2520Patil%2520and%2520Nikesh%2520Garera%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2528LLMs%2529%2520have%2520shown%2520exceptional%2520abilities%252C%2520yet%2520training%250Athese%2520models%2520can%2520be%2520quite%2520challenging.%2520There%2520is%2520a%2520strong%2520dependence%2520on%2520the%250Aquality%2520of%2520data%2520and%2520finding%2520the%2520best%2520instruction%2520tuning%2520set.%2520Further%252C%2520the%250Ainherent%2520limitations%2520in%2520training%2520methods%2520create%2520substantial%2520difficulties%2520to%250Atrain%2520relatively%2520smaller%2520models%2520with%25207B%2520and%252013B%2520parameters.%2520In%2520our%2520research%252C%2520we%250Asuggest%2520an%2520improved%2520training%2520method%2520for%2520these%2520models%2520by%2520utilising%2520knowledge%250Afrom%2520larger%2520models%252C%2520such%2520as%2520a%2520mixture%2520of%2520experts%2520%25288x7B%2529%2520architectures.%2520The%250Ascale%2520of%2520these%2520larger%2520models%2520allows%2520them%2520to%2520capture%2520a%2520wide%2520range%2520of%2520variations%250Afrom%2520data%2520alone%252C%2520making%2520them%2520effective%2520teachers%2520for%2520smaller%2520models.%2520Moreover%252C%250Awe%2520implement%2520a%2520novel%2520post-training%2520domain%2520alignment%2520phase%2520that%2520employs%250Adomain-specific%2520expert%2520models%2520to%2520boost%2520domain-specific%2520knowledge%2520during%250Atraining%2520while%2520preserving%2520the%2520model%2527s%2520ability%2520to%2520generalise.%2520Fine-tuning%250AMistral%25207B%2520and%25202x7B%2520with%2520our%2520method%2520surpasses%2520the%2520performance%2520of%250Astate-of-the-art%2520language%2520models%2520with%2520more%2520than%25207B%2520and%252013B%2520parameters%253A%250Aachieving%2520up%2520to%2520%25247.9%2524%2520in%2520MT-Bench%2520and%2520%252493.04%255C%2525%2524%2520on%2520AlpacaEval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19112v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Teacher%20Is%20Worth%20A%20Million%20Instructions&entry.906535625=Nikhil%20Kothari%20and%20Ravindra%20Nayak%20and%20Shreyas%20Shetty%20and%20Amey%20Patil%20and%20Nikesh%20Garera&entry.1292438233=%20%20Large%20Language%20Models%28LLMs%29%20have%20shown%20exceptional%20abilities%2C%20yet%20training%0Athese%20models%20can%20be%20quite%20challenging.%20There%20is%20a%20strong%20dependence%20on%20the%0Aquality%20of%20data%20and%20finding%20the%20best%20instruction%20tuning%20set.%20Further%2C%20the%0Ainherent%20limitations%20in%20training%20methods%20create%20substantial%20difficulties%20to%0Atrain%20relatively%20smaller%20models%20with%207B%20and%2013B%20parameters.%20In%20our%20research%2C%20we%0Asuggest%20an%20improved%20training%20method%20for%20these%20models%20by%20utilising%20knowledge%0Afrom%20larger%20models%2C%20such%20as%20a%20mixture%20of%20experts%20%288x7B%29%20architectures.%20The%0Ascale%20of%20these%20larger%20models%20allows%20them%20to%20capture%20a%20wide%20range%20of%20variations%0Afrom%20data%20alone%2C%20making%20them%20effective%20teachers%20for%20smaller%20models.%20Moreover%2C%0Awe%20implement%20a%20novel%20post-training%20domain%20alignment%20phase%20that%20employs%0Adomain-specific%20expert%20models%20to%20boost%20domain-specific%20knowledge%20during%0Atraining%20while%20preserving%20the%20model%27s%20ability%20to%20generalise.%20Fine-tuning%0AMistral%207B%20and%202x7B%20with%20our%20method%20surpasses%20the%20performance%20of%0Astate-of-the-art%20language%20models%20with%20more%20than%207B%20and%2013B%20parameters%3A%0Aachieving%20up%20to%20%247.9%24%20in%20MT-Bench%20and%20%2493.04%5C%25%24%20on%20AlpacaEval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19112v1&entry.124074799=Read"},
{"title": "LiveBench: A Challenging, Contamination-Free LLM Benchmark", "author": "Colin White and Samuel Dooley and Manley Roberts and Arka Pal and Ben Feuer and Siddhartha Jain and Ravid Shwartz-Ziv and Neel Jain and Khalid Saifullah and Siddartha Naidu and Chinmay Hegde and Yann LeCun and Tom Goldstein and Willie Neiswanger and Micah Goldblum", "abstract": "  Test set contamination, wherein test data from a benchmark ends up in a newer\nmodel's training set, is a well-documented obstacle for fair LLM evaluation and\ncan quickly render benchmarks obsolete. To mitigate this, many recent\nbenchmarks crowdsource new prompts and evaluations from human or LLM judges;\nhowever, these can introduce significant biases, and break down when scoring\nhard questions. In this work, we introduce a new benchmark for LLMs designed to\nbe immune to both test set contamination and the pitfalls of LLM judging and\nhuman crowdsourcing. We release LiveBench, the first benchmark that (1)\ncontains frequently-updated questions from recent information sources, (2)\nscores answers automatically according to objective ground-truth values, and\n(3) contains a wide variety of challenging tasks, spanning math, coding,\nreasoning, language, instruction following, and data analysis. To achieve this,\nLiveBench contains questions that are based on recently-released math\ncompetitions, arXiv papers, news articles, and datasets, and it contains\nharder, contamination-free versions of tasks from previous benchmarks such as\nBig-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source\nmodels, as well as dozens of open-source models ranging from 0.5B to 110B in\nsize. LiveBench is difficult, with top models achieving below 65% accuracy. We\nrelease all questions, code, and model answers. Questions will be added and\nupdated on a monthly basis, and we will release new tasks and harder versions\nof tasks over time so that LiveBench can distinguish between the capabilities\nof LLMs as they improve in the future. We welcome community engagement and\ncollaboration for expanding the benchmark tasks and models.\n", "link": "http://arxiv.org/abs/2406.19314v1", "date": "2024-06-27", "relevancy": 1.7713, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4722}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4563}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiveBench%3A%20A%20Challenging%2C%20Contamination-Free%20LLM%20Benchmark&body=Title%3A%20LiveBench%3A%20A%20Challenging%2C%20Contamination-Free%20LLM%20Benchmark%0AAuthor%3A%20Colin%20White%20and%20Samuel%20Dooley%20and%20Manley%20Roberts%20and%20Arka%20Pal%20and%20Ben%20Feuer%20and%20Siddhartha%20Jain%20and%20Ravid%20Shwartz-Ziv%20and%20Neel%20Jain%20and%20Khalid%20Saifullah%20and%20Siddartha%20Naidu%20and%20Chinmay%20Hegde%20and%20Yann%20LeCun%20and%20Tom%20Goldstein%20and%20Willie%20Neiswanger%20and%20Micah%20Goldblum%0AAbstract%3A%20%20%20Test%20set%20contamination%2C%20wherein%20test%20data%20from%20a%20benchmark%20ends%20up%20in%20a%20newer%0Amodel%27s%20training%20set%2C%20is%20a%20well-documented%20obstacle%20for%20fair%20LLM%20evaluation%20and%0Acan%20quickly%20render%20benchmarks%20obsolete.%20To%20mitigate%20this%2C%20many%20recent%0Abenchmarks%20crowdsource%20new%20prompts%20and%20evaluations%20from%20human%20or%20LLM%20judges%3B%0Ahowever%2C%20these%20can%20introduce%20significant%20biases%2C%20and%20break%20down%20when%20scoring%0Ahard%20questions.%20In%20this%20work%2C%20we%20introduce%20a%20new%20benchmark%20for%20LLMs%20designed%20to%0Abe%20immune%20to%20both%20test%20set%20contamination%20and%20the%20pitfalls%20of%20LLM%20judging%20and%0Ahuman%20crowdsourcing.%20We%20release%20LiveBench%2C%20the%20first%20benchmark%20that%20%281%29%0Acontains%20frequently-updated%20questions%20from%20recent%20information%20sources%2C%20%282%29%0Ascores%20answers%20automatically%20according%20to%20objective%20ground-truth%20values%2C%20and%0A%283%29%20contains%20a%20wide%20variety%20of%20challenging%20tasks%2C%20spanning%20math%2C%20coding%2C%0Areasoning%2C%20language%2C%20instruction%20following%2C%20and%20data%20analysis.%20To%20achieve%20this%2C%0ALiveBench%20contains%20questions%20that%20are%20based%20on%20recently-released%20math%0Acompetitions%2C%20arXiv%20papers%2C%20news%20articles%2C%20and%20datasets%2C%20and%20it%20contains%0Aharder%2C%20contamination-free%20versions%20of%20tasks%20from%20previous%20benchmarks%20such%20as%0ABig-Bench%20Hard%2C%20AMPS%2C%20and%20IFEval.%20We%20evaluate%20many%20prominent%20closed-source%0Amodels%2C%20as%20well%20as%20dozens%20of%20open-source%20models%20ranging%20from%200.5B%20to%20110B%20in%0Asize.%20LiveBench%20is%20difficult%2C%20with%20top%20models%20achieving%20below%2065%25%20accuracy.%20We%0Arelease%20all%20questions%2C%20code%2C%20and%20model%20answers.%20Questions%20will%20be%20added%20and%0Aupdated%20on%20a%20monthly%20basis%2C%20and%20we%20will%20release%20new%20tasks%20and%20harder%20versions%0Aof%20tasks%20over%20time%20so%20that%20LiveBench%20can%20distinguish%20between%20the%20capabilities%0Aof%20LLMs%20as%20they%20improve%20in%20the%20future.%20We%20welcome%20community%20engagement%20and%0Acollaboration%20for%20expanding%20the%20benchmark%20tasks%20and%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiveBench%253A%2520A%2520Challenging%252C%2520Contamination-Free%2520LLM%2520Benchmark%26entry.906535625%3DColin%2520White%2520and%2520Samuel%2520Dooley%2520and%2520Manley%2520Roberts%2520and%2520Arka%2520Pal%2520and%2520Ben%2520Feuer%2520and%2520Siddhartha%2520Jain%2520and%2520Ravid%2520Shwartz-Ziv%2520and%2520Neel%2520Jain%2520and%2520Khalid%2520Saifullah%2520and%2520Siddartha%2520Naidu%2520and%2520Chinmay%2520Hegde%2520and%2520Yann%2520LeCun%2520and%2520Tom%2520Goldstein%2520and%2520Willie%2520Neiswanger%2520and%2520Micah%2520Goldblum%26entry.1292438233%3D%2520%2520Test%2520set%2520contamination%252C%2520wherein%2520test%2520data%2520from%2520a%2520benchmark%2520ends%2520up%2520in%2520a%2520newer%250Amodel%2527s%2520training%2520set%252C%2520is%2520a%2520well-documented%2520obstacle%2520for%2520fair%2520LLM%2520evaluation%2520and%250Acan%2520quickly%2520render%2520benchmarks%2520obsolete.%2520To%2520mitigate%2520this%252C%2520many%2520recent%250Abenchmarks%2520crowdsource%2520new%2520prompts%2520and%2520evaluations%2520from%2520human%2520or%2520LLM%2520judges%253B%250Ahowever%252C%2520these%2520can%2520introduce%2520significant%2520biases%252C%2520and%2520break%2520down%2520when%2520scoring%250Ahard%2520questions.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520new%2520benchmark%2520for%2520LLMs%2520designed%2520to%250Abe%2520immune%2520to%2520both%2520test%2520set%2520contamination%2520and%2520the%2520pitfalls%2520of%2520LLM%2520judging%2520and%250Ahuman%2520crowdsourcing.%2520We%2520release%2520LiveBench%252C%2520the%2520first%2520benchmark%2520that%2520%25281%2529%250Acontains%2520frequently-updated%2520questions%2520from%2520recent%2520information%2520sources%252C%2520%25282%2529%250Ascores%2520answers%2520automatically%2520according%2520to%2520objective%2520ground-truth%2520values%252C%2520and%250A%25283%2529%2520contains%2520a%2520wide%2520variety%2520of%2520challenging%2520tasks%252C%2520spanning%2520math%252C%2520coding%252C%250Areasoning%252C%2520language%252C%2520instruction%2520following%252C%2520and%2520data%2520analysis.%2520To%2520achieve%2520this%252C%250ALiveBench%2520contains%2520questions%2520that%2520are%2520based%2520on%2520recently-released%2520math%250Acompetitions%252C%2520arXiv%2520papers%252C%2520news%2520articles%252C%2520and%2520datasets%252C%2520and%2520it%2520contains%250Aharder%252C%2520contamination-free%2520versions%2520of%2520tasks%2520from%2520previous%2520benchmarks%2520such%2520as%250ABig-Bench%2520Hard%252C%2520AMPS%252C%2520and%2520IFEval.%2520We%2520evaluate%2520many%2520prominent%2520closed-source%250Amodels%252C%2520as%2520well%2520as%2520dozens%2520of%2520open-source%2520models%2520ranging%2520from%25200.5B%2520to%2520110B%2520in%250Asize.%2520LiveBench%2520is%2520difficult%252C%2520with%2520top%2520models%2520achieving%2520below%252065%2525%2520accuracy.%2520We%250Arelease%2520all%2520questions%252C%2520code%252C%2520and%2520model%2520answers.%2520Questions%2520will%2520be%2520added%2520and%250Aupdated%2520on%2520a%2520monthly%2520basis%252C%2520and%2520we%2520will%2520release%2520new%2520tasks%2520and%2520harder%2520versions%250Aof%2520tasks%2520over%2520time%2520so%2520that%2520LiveBench%2520can%2520distinguish%2520between%2520the%2520capabilities%250Aof%2520LLMs%2520as%2520they%2520improve%2520in%2520the%2520future.%2520We%2520welcome%2520community%2520engagement%2520and%250Acollaboration%2520for%2520expanding%2520the%2520benchmark%2520tasks%2520and%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiveBench%3A%20A%20Challenging%2C%20Contamination-Free%20LLM%20Benchmark&entry.906535625=Colin%20White%20and%20Samuel%20Dooley%20and%20Manley%20Roberts%20and%20Arka%20Pal%20and%20Ben%20Feuer%20and%20Siddhartha%20Jain%20and%20Ravid%20Shwartz-Ziv%20and%20Neel%20Jain%20and%20Khalid%20Saifullah%20and%20Siddartha%20Naidu%20and%20Chinmay%20Hegde%20and%20Yann%20LeCun%20and%20Tom%20Goldstein%20and%20Willie%20Neiswanger%20and%20Micah%20Goldblum&entry.1292438233=%20%20Test%20set%20contamination%2C%20wherein%20test%20data%20from%20a%20benchmark%20ends%20up%20in%20a%20newer%0Amodel%27s%20training%20set%2C%20is%20a%20well-documented%20obstacle%20for%20fair%20LLM%20evaluation%20and%0Acan%20quickly%20render%20benchmarks%20obsolete.%20To%20mitigate%20this%2C%20many%20recent%0Abenchmarks%20crowdsource%20new%20prompts%20and%20evaluations%20from%20human%20or%20LLM%20judges%3B%0Ahowever%2C%20these%20can%20introduce%20significant%20biases%2C%20and%20break%20down%20when%20scoring%0Ahard%20questions.%20In%20this%20work%2C%20we%20introduce%20a%20new%20benchmark%20for%20LLMs%20designed%20to%0Abe%20immune%20to%20both%20test%20set%20contamination%20and%20the%20pitfalls%20of%20LLM%20judging%20and%0Ahuman%20crowdsourcing.%20We%20release%20LiveBench%2C%20the%20first%20benchmark%20that%20%281%29%0Acontains%20frequently-updated%20questions%20from%20recent%20information%20sources%2C%20%282%29%0Ascores%20answers%20automatically%20according%20to%20objective%20ground-truth%20values%2C%20and%0A%283%29%20contains%20a%20wide%20variety%20of%20challenging%20tasks%2C%20spanning%20math%2C%20coding%2C%0Areasoning%2C%20language%2C%20instruction%20following%2C%20and%20data%20analysis.%20To%20achieve%20this%2C%0ALiveBench%20contains%20questions%20that%20are%20based%20on%20recently-released%20math%0Acompetitions%2C%20arXiv%20papers%2C%20news%20articles%2C%20and%20datasets%2C%20and%20it%20contains%0Aharder%2C%20contamination-free%20versions%20of%20tasks%20from%20previous%20benchmarks%20such%20as%0ABig-Bench%20Hard%2C%20AMPS%2C%20and%20IFEval.%20We%20evaluate%20many%20prominent%20closed-source%0Amodels%2C%20as%20well%20as%20dozens%20of%20open-source%20models%20ranging%20from%200.5B%20to%20110B%20in%0Asize.%20LiveBench%20is%20difficult%2C%20with%20top%20models%20achieving%20below%2065%25%20accuracy.%20We%0Arelease%20all%20questions%2C%20code%2C%20and%20model%20answers.%20Questions%20will%20be%20added%20and%0Aupdated%20on%20a%20monthly%20basis%2C%20and%20we%20will%20release%20new%20tasks%20and%20harder%20versions%0Aof%20tasks%20over%20time%20so%20that%20LiveBench%20can%20distinguish%20between%20the%20capabilities%0Aof%20LLMs%20as%20they%20improve%20in%20the%20future.%20We%20welcome%20community%20engagement%20and%0Acollaboration%20for%20expanding%20the%20benchmark%20tasks%20and%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19314v1&entry.124074799=Read"},
{"title": "Compositional Image Decomposition with Diffusion Models", "author": "Jocelin Su and Nan Liu and Yanbo Wang and Joshua B. Tenenbaum and Yilun Du", "abstract": "  Given an image of a natural scene, we are able to quickly decompose it into a\nset of components such as objects, lighting, shadows, and foreground. We can\nthen envision a scene where we combine certain components with those from other\nimages, for instance a set of objects from our bedroom and animals from a zoo\nunder the lighting conditions of a forest, even if we have never encountered\nsuch a scene before. In this paper, we present a method to decompose an image\ninto such compositional components. Our approach, Decomp Diffusion, is an\nunsupervised method which, when given a single image, infers a set of different\ncomponents in the image, each represented by a diffusion model. We demonstrate\nhow components can capture different factors of the scene, ranging from global\nscene descriptors like shadows or facial expression to local scene descriptors\nlike constituent objects. We further illustrate how inferred factors can be\nflexibly composed, even with factors inferred from other models, to generate a\nvariety of scenes sharply different than those seen in training time. Website\nand code at https://energy-based-model.github.io/decomp-diffusion.\n", "link": "http://arxiv.org/abs/2406.19298v1", "date": "2024-06-27", "relevancy": 1.758, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6149}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5903}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compositional%20Image%20Decomposition%20with%20Diffusion%20Models&body=Title%3A%20Compositional%20Image%20Decomposition%20with%20Diffusion%20Models%0AAuthor%3A%20Jocelin%20Su%20and%20Nan%20Liu%20and%20Yanbo%20Wang%20and%20Joshua%20B.%20Tenenbaum%20and%20Yilun%20Du%0AAbstract%3A%20%20%20Given%20an%20image%20of%20a%20natural%20scene%2C%20we%20are%20able%20to%20quickly%20decompose%20it%20into%20a%0Aset%20of%20components%20such%20as%20objects%2C%20lighting%2C%20shadows%2C%20and%20foreground.%20We%20can%0Athen%20envision%20a%20scene%20where%20we%20combine%20certain%20components%20with%20those%20from%20other%0Aimages%2C%20for%20instance%20a%20set%20of%20objects%20from%20our%20bedroom%20and%20animals%20from%20a%20zoo%0Aunder%20the%20lighting%20conditions%20of%20a%20forest%2C%20even%20if%20we%20have%20never%20encountered%0Asuch%20a%20scene%20before.%20In%20this%20paper%2C%20we%20present%20a%20method%20to%20decompose%20an%20image%0Ainto%20such%20compositional%20components.%20Our%20approach%2C%20Decomp%20Diffusion%2C%20is%20an%0Aunsupervised%20method%20which%2C%20when%20given%20a%20single%20image%2C%20infers%20a%20set%20of%20different%0Acomponents%20in%20the%20image%2C%20each%20represented%20by%20a%20diffusion%20model.%20We%20demonstrate%0Ahow%20components%20can%20capture%20different%20factors%20of%20the%20scene%2C%20ranging%20from%20global%0Ascene%20descriptors%20like%20shadows%20or%20facial%20expression%20to%20local%20scene%20descriptors%0Alike%20constituent%20objects.%20We%20further%20illustrate%20how%20inferred%20factors%20can%20be%0Aflexibly%20composed%2C%20even%20with%20factors%20inferred%20from%20other%20models%2C%20to%20generate%20a%0Avariety%20of%20scenes%20sharply%20different%20than%20those%20seen%20in%20training%20time.%20Website%0Aand%20code%20at%20https%3A//energy-based-model.github.io/decomp-diffusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19298v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompositional%2520Image%2520Decomposition%2520with%2520Diffusion%2520Models%26entry.906535625%3DJocelin%2520Su%2520and%2520Nan%2520Liu%2520and%2520Yanbo%2520Wang%2520and%2520Joshua%2520B.%2520Tenenbaum%2520and%2520Yilun%2520Du%26entry.1292438233%3D%2520%2520Given%2520an%2520image%2520of%2520a%2520natural%2520scene%252C%2520we%2520are%2520able%2520to%2520quickly%2520decompose%2520it%2520into%2520a%250Aset%2520of%2520components%2520such%2520as%2520objects%252C%2520lighting%252C%2520shadows%252C%2520and%2520foreground.%2520We%2520can%250Athen%2520envision%2520a%2520scene%2520where%2520we%2520combine%2520certain%2520components%2520with%2520those%2520from%2520other%250Aimages%252C%2520for%2520instance%2520a%2520set%2520of%2520objects%2520from%2520our%2520bedroom%2520and%2520animals%2520from%2520a%2520zoo%250Aunder%2520the%2520lighting%2520conditions%2520of%2520a%2520forest%252C%2520even%2520if%2520we%2520have%2520never%2520encountered%250Asuch%2520a%2520scene%2520before.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520method%2520to%2520decompose%2520an%2520image%250Ainto%2520such%2520compositional%2520components.%2520Our%2520approach%252C%2520Decomp%2520Diffusion%252C%2520is%2520an%250Aunsupervised%2520method%2520which%252C%2520when%2520given%2520a%2520single%2520image%252C%2520infers%2520a%2520set%2520of%2520different%250Acomponents%2520in%2520the%2520image%252C%2520each%2520represented%2520by%2520a%2520diffusion%2520model.%2520We%2520demonstrate%250Ahow%2520components%2520can%2520capture%2520different%2520factors%2520of%2520the%2520scene%252C%2520ranging%2520from%2520global%250Ascene%2520descriptors%2520like%2520shadows%2520or%2520facial%2520expression%2520to%2520local%2520scene%2520descriptors%250Alike%2520constituent%2520objects.%2520We%2520further%2520illustrate%2520how%2520inferred%2520factors%2520can%2520be%250Aflexibly%2520composed%252C%2520even%2520with%2520factors%2520inferred%2520from%2520other%2520models%252C%2520to%2520generate%2520a%250Avariety%2520of%2520scenes%2520sharply%2520different%2520than%2520those%2520seen%2520in%2520training%2520time.%2520Website%250Aand%2520code%2520at%2520https%253A//energy-based-model.github.io/decomp-diffusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19298v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compositional%20Image%20Decomposition%20with%20Diffusion%20Models&entry.906535625=Jocelin%20Su%20and%20Nan%20Liu%20and%20Yanbo%20Wang%20and%20Joshua%20B.%20Tenenbaum%20and%20Yilun%20Du&entry.1292438233=%20%20Given%20an%20image%20of%20a%20natural%20scene%2C%20we%20are%20able%20to%20quickly%20decompose%20it%20into%20a%0Aset%20of%20components%20such%20as%20objects%2C%20lighting%2C%20shadows%2C%20and%20foreground.%20We%20can%0Athen%20envision%20a%20scene%20where%20we%20combine%20certain%20components%20with%20those%20from%20other%0Aimages%2C%20for%20instance%20a%20set%20of%20objects%20from%20our%20bedroom%20and%20animals%20from%20a%20zoo%0Aunder%20the%20lighting%20conditions%20of%20a%20forest%2C%20even%20if%20we%20have%20never%20encountered%0Asuch%20a%20scene%20before.%20In%20this%20paper%2C%20we%20present%20a%20method%20to%20decompose%20an%20image%0Ainto%20such%20compositional%20components.%20Our%20approach%2C%20Decomp%20Diffusion%2C%20is%20an%0Aunsupervised%20method%20which%2C%20when%20given%20a%20single%20image%2C%20infers%20a%20set%20of%20different%0Acomponents%20in%20the%20image%2C%20each%20represented%20by%20a%20diffusion%20model.%20We%20demonstrate%0Ahow%20components%20can%20capture%20different%20factors%20of%20the%20scene%2C%20ranging%20from%20global%0Ascene%20descriptors%20like%20shadows%20or%20facial%20expression%20to%20local%20scene%20descriptors%0Alike%20constituent%20objects.%20We%20further%20illustrate%20how%20inferred%20factors%20can%20be%0Aflexibly%20composed%2C%20even%20with%20factors%20inferred%20from%20other%20models%2C%20to%20generate%20a%0Avariety%20of%20scenes%20sharply%20different%20than%20those%20seen%20in%20training%20time.%20Website%0Aand%20code%20at%20https%3A//energy-based-model.github.io/decomp-diffusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19298v1&entry.124074799=Read"},
{"title": "Accuracy on the wrong line: On the pitfalls of noisy data for\n  out-of-distribution generalisation", "author": "Amartya Sanyal and Yaxi Hu and Yaodong Yu and Yian Ma and Yixin Wang and Bernhard Sch\u00f6lkopf", "abstract": "  \"Accuracy-on-the-line\" is a widely observed phenomenon in machine learning,\nwhere a model's accuracy on in-distribution (ID) and out-of-distribution (OOD)\ndata is positively correlated across different hyperparameters and data\nconfigurations. But when does this useful relationship break down? In this\nwork, we explore its robustness. The key observation is that noisy data and the\npresence of nuisance features can be sufficient to shatter the\nAccuracy-on-the-line phenomenon. In these cases, ID and OOD accuracy can become\nnegatively correlated, leading to \"Accuracy-on-the-wrong-line\". This phenomenon\ncan also occur in the presence of spurious (shortcut) features, which tend to\novershadow the more complex signal (core, non-spurious) features, resulting in\na large nuisance feature space. Moreover, scaling to larger datasets does not\nmitigate this undesirable behavior and may even exacerbate it. We formally\nprove a lower bound on Out-of-distribution (OOD) error in a linear\nclassification model, characterizing the conditions on the noise and nuisance\nfeatures for a large OOD error. We finally demonstrate this phenomenon across\nboth synthetic and real datasets with noisy data and nuisance features.\n", "link": "http://arxiv.org/abs/2406.19049v1", "date": "2024-06-27", "relevancy": 1.7498, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4402}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4368}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accuracy%20on%20the%20wrong%20line%3A%20On%20the%20pitfalls%20of%20noisy%20data%20for%0A%20%20out-of-distribution%20generalisation&body=Title%3A%20Accuracy%20on%20the%20wrong%20line%3A%20On%20the%20pitfalls%20of%20noisy%20data%20for%0A%20%20out-of-distribution%20generalisation%0AAuthor%3A%20Amartya%20Sanyal%20and%20Yaxi%20Hu%20and%20Yaodong%20Yu%20and%20Yian%20Ma%20and%20Yixin%20Wang%20and%20Bernhard%20Sch%C3%B6lkopf%0AAbstract%3A%20%20%20%22Accuracy-on-the-line%22%20is%20a%20widely%20observed%20phenomenon%20in%20machine%20learning%2C%0Awhere%20a%20model%27s%20accuracy%20on%20in-distribution%20%28ID%29%20and%20out-of-distribution%20%28OOD%29%0Adata%20is%20positively%20correlated%20across%20different%20hyperparameters%20and%20data%0Aconfigurations.%20But%20when%20does%20this%20useful%20relationship%20break%20down%3F%20In%20this%0Awork%2C%20we%20explore%20its%20robustness.%20The%20key%20observation%20is%20that%20noisy%20data%20and%20the%0Apresence%20of%20nuisance%20features%20can%20be%20sufficient%20to%20shatter%20the%0AAccuracy-on-the-line%20phenomenon.%20In%20these%20cases%2C%20ID%20and%20OOD%20accuracy%20can%20become%0Anegatively%20correlated%2C%20leading%20to%20%22Accuracy-on-the-wrong-line%22.%20This%20phenomenon%0Acan%20also%20occur%20in%20the%20presence%20of%20spurious%20%28shortcut%29%20features%2C%20which%20tend%20to%0Aovershadow%20the%20more%20complex%20signal%20%28core%2C%20non-spurious%29%20features%2C%20resulting%20in%0Aa%20large%20nuisance%20feature%20space.%20Moreover%2C%20scaling%20to%20larger%20datasets%20does%20not%0Amitigate%20this%20undesirable%20behavior%20and%20may%20even%20exacerbate%20it.%20We%20formally%0Aprove%20a%20lower%20bound%20on%20Out-of-distribution%20%28OOD%29%20error%20in%20a%20linear%0Aclassification%20model%2C%20characterizing%20the%20conditions%20on%20the%20noise%20and%20nuisance%0Afeatures%20for%20a%20large%20OOD%20error.%20We%20finally%20demonstrate%20this%20phenomenon%20across%0Aboth%20synthetic%20and%20real%20datasets%20with%20noisy%20data%20and%20nuisance%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccuracy%2520on%2520the%2520wrong%2520line%253A%2520On%2520the%2520pitfalls%2520of%2520noisy%2520data%2520for%250A%2520%2520out-of-distribution%2520generalisation%26entry.906535625%3DAmartya%2520Sanyal%2520and%2520Yaxi%2520Hu%2520and%2520Yaodong%2520Yu%2520and%2520Yian%2520Ma%2520and%2520Yixin%2520Wang%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%26entry.1292438233%3D%2520%2520%2522Accuracy-on-the-line%2522%2520is%2520a%2520widely%2520observed%2520phenomenon%2520in%2520machine%2520learning%252C%250Awhere%2520a%2520model%2527s%2520accuracy%2520on%2520in-distribution%2520%2528ID%2529%2520and%2520out-of-distribution%2520%2528OOD%2529%250Adata%2520is%2520positively%2520correlated%2520across%2520different%2520hyperparameters%2520and%2520data%250Aconfigurations.%2520But%2520when%2520does%2520this%2520useful%2520relationship%2520break%2520down%253F%2520In%2520this%250Awork%252C%2520we%2520explore%2520its%2520robustness.%2520The%2520key%2520observation%2520is%2520that%2520noisy%2520data%2520and%2520the%250Apresence%2520of%2520nuisance%2520features%2520can%2520be%2520sufficient%2520to%2520shatter%2520the%250AAccuracy-on-the-line%2520phenomenon.%2520In%2520these%2520cases%252C%2520ID%2520and%2520OOD%2520accuracy%2520can%2520become%250Anegatively%2520correlated%252C%2520leading%2520to%2520%2522Accuracy-on-the-wrong-line%2522.%2520This%2520phenomenon%250Acan%2520also%2520occur%2520in%2520the%2520presence%2520of%2520spurious%2520%2528shortcut%2529%2520features%252C%2520which%2520tend%2520to%250Aovershadow%2520the%2520more%2520complex%2520signal%2520%2528core%252C%2520non-spurious%2529%2520features%252C%2520resulting%2520in%250Aa%2520large%2520nuisance%2520feature%2520space.%2520Moreover%252C%2520scaling%2520to%2520larger%2520datasets%2520does%2520not%250Amitigate%2520this%2520undesirable%2520behavior%2520and%2520may%2520even%2520exacerbate%2520it.%2520We%2520formally%250Aprove%2520a%2520lower%2520bound%2520on%2520Out-of-distribution%2520%2528OOD%2529%2520error%2520in%2520a%2520linear%250Aclassification%2520model%252C%2520characterizing%2520the%2520conditions%2520on%2520the%2520noise%2520and%2520nuisance%250Afeatures%2520for%2520a%2520large%2520OOD%2520error.%2520We%2520finally%2520demonstrate%2520this%2520phenomenon%2520across%250Aboth%2520synthetic%2520and%2520real%2520datasets%2520with%2520noisy%2520data%2520and%2520nuisance%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accuracy%20on%20the%20wrong%20line%3A%20On%20the%20pitfalls%20of%20noisy%20data%20for%0A%20%20out-of-distribution%20generalisation&entry.906535625=Amartya%20Sanyal%20and%20Yaxi%20Hu%20and%20Yaodong%20Yu%20and%20Yian%20Ma%20and%20Yixin%20Wang%20and%20Bernhard%20Sch%C3%B6lkopf&entry.1292438233=%20%20%22Accuracy-on-the-line%22%20is%20a%20widely%20observed%20phenomenon%20in%20machine%20learning%2C%0Awhere%20a%20model%27s%20accuracy%20on%20in-distribution%20%28ID%29%20and%20out-of-distribution%20%28OOD%29%0Adata%20is%20positively%20correlated%20across%20different%20hyperparameters%20and%20data%0Aconfigurations.%20But%20when%20does%20this%20useful%20relationship%20break%20down%3F%20In%20this%0Awork%2C%20we%20explore%20its%20robustness.%20The%20key%20observation%20is%20that%20noisy%20data%20and%20the%0Apresence%20of%20nuisance%20features%20can%20be%20sufficient%20to%20shatter%20the%0AAccuracy-on-the-line%20phenomenon.%20In%20these%20cases%2C%20ID%20and%20OOD%20accuracy%20can%20become%0Anegatively%20correlated%2C%20leading%20to%20%22Accuracy-on-the-wrong-line%22.%20This%20phenomenon%0Acan%20also%20occur%20in%20the%20presence%20of%20spurious%20%28shortcut%29%20features%2C%20which%20tend%20to%0Aovershadow%20the%20more%20complex%20signal%20%28core%2C%20non-spurious%29%20features%2C%20resulting%20in%0Aa%20large%20nuisance%20feature%20space.%20Moreover%2C%20scaling%20to%20larger%20datasets%20does%20not%0Amitigate%20this%20undesirable%20behavior%20and%20may%20even%20exacerbate%20it.%20We%20formally%0Aprove%20a%20lower%20bound%20on%20Out-of-distribution%20%28OOD%29%20error%20in%20a%20linear%0Aclassification%20model%2C%20characterizing%20the%20conditions%20on%20the%20noise%20and%20nuisance%0Afeatures%20for%20a%20large%20OOD%20error.%20We%20finally%20demonstrate%20this%20phenomenon%20across%0Aboth%20synthetic%20and%20real%20datasets%20with%20noisy%20data%20and%20nuisance%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19049v1&entry.124074799=Read"},
{"title": "GPTFUZZER: Red Teaming Large Language Models with Auto-Generated\n  Jailbreak Prompts", "author": "Jiahao Yu and Xingwei Lin and Zheng Yu and Xinyu Xing", "abstract": "  Large language models (LLMs) have recently experienced tremendous popularity\nand are widely used from casual conversations to AI-driven programming.\nHowever, despite their considerable success, LLMs are not entirely reliable and\ncan give detailed guidance on how to conduct harmful or illegal activities.\nWhile safety measures can reduce the risk of such outputs, adversarial\njailbreak attacks can still exploit LLMs to produce harmful content. These\njailbreak templates are typically manually crafted, making large-scale testing\nchallenging.\n  In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzing\nframework inspired by the AFL fuzzing framework. Instead of manual engineering,\nGPTFuzz automates the generation of jailbreak templates for red-teaming LLMs.\nAt its core, GPTFuzz starts with human-written templates as initial seeds, then\nmutates them to produce new templates. We detail three key components of\nGPTFuzz: a seed selection strategy for balancing efficiency and variability,\nmutate operators for creating semantically equivalent or similar sentences, and\na judgment model to assess the success of a jailbreak attack.\n  We evaluate GPTFuzz against various commercial and open-source LLMs,\nincluding ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Our\nresults indicate that GPTFuzz consistently produces jailbreak templates with a\nhigh success rate, surpassing human-crafted templates. Remarkably, GPTFuzz\nachieves over 90% attack success rates against ChatGPT and Llama-2 models, even\nwith suboptimal initial seed templates. We anticipate that GPTFuzz will be\ninstrumental for researchers and practitioners in examining LLM robustness and\nwill encourage further exploration into enhancing LLM safety.\n", "link": "http://arxiv.org/abs/2309.10253v4", "date": "2024-06-27", "relevancy": 1.7389, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4387}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4376}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPTFUZZER%3A%20Red%20Teaming%20Large%20Language%20Models%20with%20Auto-Generated%0A%20%20Jailbreak%20Prompts&body=Title%3A%20GPTFUZZER%3A%20Red%20Teaming%20Large%20Language%20Models%20with%20Auto-Generated%0A%20%20Jailbreak%20Prompts%0AAuthor%3A%20Jiahao%20Yu%20and%20Xingwei%20Lin%20and%20Zheng%20Yu%20and%20Xinyu%20Xing%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20recently%20experienced%20tremendous%20popularity%0Aand%20are%20widely%20used%20from%20casual%20conversations%20to%20AI-driven%20programming.%0AHowever%2C%20despite%20their%20considerable%20success%2C%20LLMs%20are%20not%20entirely%20reliable%20and%0Acan%20give%20detailed%20guidance%20on%20how%20to%20conduct%20harmful%20or%20illegal%20activities.%0AWhile%20safety%20measures%20can%20reduce%20the%20risk%20of%20such%20outputs%2C%20adversarial%0Ajailbreak%20attacks%20can%20still%20exploit%20LLMs%20to%20produce%20harmful%20content.%20These%0Ajailbreak%20templates%20are%20typically%20manually%20crafted%2C%20making%20large-scale%20testing%0Achallenging.%0A%20%20In%20this%20paper%2C%20we%20introduce%20GPTFuzz%2C%20a%20novel%20black-box%20jailbreak%20fuzzing%0Aframework%20inspired%20by%20the%20AFL%20fuzzing%20framework.%20Instead%20of%20manual%20engineering%2C%0AGPTFuzz%20automates%20the%20generation%20of%20jailbreak%20templates%20for%20red-teaming%20LLMs.%0AAt%20its%20core%2C%20GPTFuzz%20starts%20with%20human-written%20templates%20as%20initial%20seeds%2C%20then%0Amutates%20them%20to%20produce%20new%20templates.%20We%20detail%20three%20key%20components%20of%0AGPTFuzz%3A%20a%20seed%20selection%20strategy%20for%20balancing%20efficiency%20and%20variability%2C%0Amutate%20operators%20for%20creating%20semantically%20equivalent%20or%20similar%20sentences%2C%20and%0Aa%20judgment%20model%20to%20assess%20the%20success%20of%20a%20jailbreak%20attack.%0A%20%20We%20evaluate%20GPTFuzz%20against%20various%20commercial%20and%20open-source%20LLMs%2C%0Aincluding%20ChatGPT%2C%20LLaMa-2%2C%20and%20Vicuna%2C%20under%20diverse%20attack%20scenarios.%20Our%0Aresults%20indicate%20that%20GPTFuzz%20consistently%20produces%20jailbreak%20templates%20with%20a%0Ahigh%20success%20rate%2C%20surpassing%20human-crafted%20templates.%20Remarkably%2C%20GPTFuzz%0Aachieves%20over%2090%25%20attack%20success%20rates%20against%20ChatGPT%20and%20Llama-2%20models%2C%20even%0Awith%20suboptimal%20initial%20seed%20templates.%20We%20anticipate%20that%20GPTFuzz%20will%20be%0Ainstrumental%20for%20researchers%20and%20practitioners%20in%20examining%20LLM%20robustness%20and%0Awill%20encourage%20further%20exploration%20into%20enhancing%20LLM%20safety.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.10253v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPTFUZZER%253A%2520Red%2520Teaming%2520Large%2520Language%2520Models%2520with%2520Auto-Generated%250A%2520%2520Jailbreak%2520Prompts%26entry.906535625%3DJiahao%2520Yu%2520and%2520Xingwei%2520Lin%2520and%2520Zheng%2520Yu%2520and%2520Xinyu%2520Xing%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520recently%2520experienced%2520tremendous%2520popularity%250Aand%2520are%2520widely%2520used%2520from%2520casual%2520conversations%2520to%2520AI-driven%2520programming.%250AHowever%252C%2520despite%2520their%2520considerable%2520success%252C%2520LLMs%2520are%2520not%2520entirely%2520reliable%2520and%250Acan%2520give%2520detailed%2520guidance%2520on%2520how%2520to%2520conduct%2520harmful%2520or%2520illegal%2520activities.%250AWhile%2520safety%2520measures%2520can%2520reduce%2520the%2520risk%2520of%2520such%2520outputs%252C%2520adversarial%250Ajailbreak%2520attacks%2520can%2520still%2520exploit%2520LLMs%2520to%2520produce%2520harmful%2520content.%2520These%250Ajailbreak%2520templates%2520are%2520typically%2520manually%2520crafted%252C%2520making%2520large-scale%2520testing%250Achallenging.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520GPTFuzz%252C%2520a%2520novel%2520black-box%2520jailbreak%2520fuzzing%250Aframework%2520inspired%2520by%2520the%2520AFL%2520fuzzing%2520framework.%2520Instead%2520of%2520manual%2520engineering%252C%250AGPTFuzz%2520automates%2520the%2520generation%2520of%2520jailbreak%2520templates%2520for%2520red-teaming%2520LLMs.%250AAt%2520its%2520core%252C%2520GPTFuzz%2520starts%2520with%2520human-written%2520templates%2520as%2520initial%2520seeds%252C%2520then%250Amutates%2520them%2520to%2520produce%2520new%2520templates.%2520We%2520detail%2520three%2520key%2520components%2520of%250AGPTFuzz%253A%2520a%2520seed%2520selection%2520strategy%2520for%2520balancing%2520efficiency%2520and%2520variability%252C%250Amutate%2520operators%2520for%2520creating%2520semantically%2520equivalent%2520or%2520similar%2520sentences%252C%2520and%250Aa%2520judgment%2520model%2520to%2520assess%2520the%2520success%2520of%2520a%2520jailbreak%2520attack.%250A%2520%2520We%2520evaluate%2520GPTFuzz%2520against%2520various%2520commercial%2520and%2520open-source%2520LLMs%252C%250Aincluding%2520ChatGPT%252C%2520LLaMa-2%252C%2520and%2520Vicuna%252C%2520under%2520diverse%2520attack%2520scenarios.%2520Our%250Aresults%2520indicate%2520that%2520GPTFuzz%2520consistently%2520produces%2520jailbreak%2520templates%2520with%2520a%250Ahigh%2520success%2520rate%252C%2520surpassing%2520human-crafted%2520templates.%2520Remarkably%252C%2520GPTFuzz%250Aachieves%2520over%252090%2525%2520attack%2520success%2520rates%2520against%2520ChatGPT%2520and%2520Llama-2%2520models%252C%2520even%250Awith%2520suboptimal%2520initial%2520seed%2520templates.%2520We%2520anticipate%2520that%2520GPTFuzz%2520will%2520be%250Ainstrumental%2520for%2520researchers%2520and%2520practitioners%2520in%2520examining%2520LLM%2520robustness%2520and%250Awill%2520encourage%2520further%2520exploration%2520into%2520enhancing%2520LLM%2520safety.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.10253v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPTFUZZER%3A%20Red%20Teaming%20Large%20Language%20Models%20with%20Auto-Generated%0A%20%20Jailbreak%20Prompts&entry.906535625=Jiahao%20Yu%20and%20Xingwei%20Lin%20and%20Zheng%20Yu%20and%20Xinyu%20Xing&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20recently%20experienced%20tremendous%20popularity%0Aand%20are%20widely%20used%20from%20casual%20conversations%20to%20AI-driven%20programming.%0AHowever%2C%20despite%20their%20considerable%20success%2C%20LLMs%20are%20not%20entirely%20reliable%20and%0Acan%20give%20detailed%20guidance%20on%20how%20to%20conduct%20harmful%20or%20illegal%20activities.%0AWhile%20safety%20measures%20can%20reduce%20the%20risk%20of%20such%20outputs%2C%20adversarial%0Ajailbreak%20attacks%20can%20still%20exploit%20LLMs%20to%20produce%20harmful%20content.%20These%0Ajailbreak%20templates%20are%20typically%20manually%20crafted%2C%20making%20large-scale%20testing%0Achallenging.%0A%20%20In%20this%20paper%2C%20we%20introduce%20GPTFuzz%2C%20a%20novel%20black-box%20jailbreak%20fuzzing%0Aframework%20inspired%20by%20the%20AFL%20fuzzing%20framework.%20Instead%20of%20manual%20engineering%2C%0AGPTFuzz%20automates%20the%20generation%20of%20jailbreak%20templates%20for%20red-teaming%20LLMs.%0AAt%20its%20core%2C%20GPTFuzz%20starts%20with%20human-written%20templates%20as%20initial%20seeds%2C%20then%0Amutates%20them%20to%20produce%20new%20templates.%20We%20detail%20three%20key%20components%20of%0AGPTFuzz%3A%20a%20seed%20selection%20strategy%20for%20balancing%20efficiency%20and%20variability%2C%0Amutate%20operators%20for%20creating%20semantically%20equivalent%20or%20similar%20sentences%2C%20and%0Aa%20judgment%20model%20to%20assess%20the%20success%20of%20a%20jailbreak%20attack.%0A%20%20We%20evaluate%20GPTFuzz%20against%20various%20commercial%20and%20open-source%20LLMs%2C%0Aincluding%20ChatGPT%2C%20LLaMa-2%2C%20and%20Vicuna%2C%20under%20diverse%20attack%20scenarios.%20Our%0Aresults%20indicate%20that%20GPTFuzz%20consistently%20produces%20jailbreak%20templates%20with%20a%0Ahigh%20success%20rate%2C%20surpassing%20human-crafted%20templates.%20Remarkably%2C%20GPTFuzz%0Aachieves%20over%2090%25%20attack%20success%20rates%20against%20ChatGPT%20and%20Llama-2%20models%2C%20even%0Awith%20suboptimal%20initial%20seed%20templates.%20We%20anticipate%20that%20GPTFuzz%20will%20be%0Ainstrumental%20for%20researchers%20and%20practitioners%20in%20examining%20LLM%20robustness%20and%0Awill%20encourage%20further%20exploration%20into%20enhancing%20LLM%20safety.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.10253v4&entry.124074799=Read"},
{"title": "Averaging log-likelihoods in direct alignment", "author": "Nathan Grinsztajn and Yannis Flet-Berliac and Mohammad Gheshlaghi Azar and Florian Strub and Bill Wu and Eugene Choi and Chris Cremer and Arash Ahmadian and Yash Chandak and Olivier Pietquin and Matthieu Geist", "abstract": "  To better align Large Language Models (LLMs) with human judgment,\nReinforcement Learning from Human Feedback (RLHF) learns a reward model and\nthen optimizes it using regularized RL. Recently, direct alignment methods were\nintroduced to learn such a fine-tuned model directly from a preference dataset\nwithout computing a proxy reward function. These methods are built upon\ncontrastive losses involving the log-likelihood of (dis)preferred completions\naccording to the trained model. However, completions have various lengths, and\nthe log-likelihood is not length-invariant. On the other side, the\ncross-entropy loss used in supervised training is length-invariant, as batches\nare typically averaged token-wise. To reconcile these approaches, we introduce\na principled approach for making direct alignment length-invariant. Formally,\nwe introduce a new averaging operator, to be composed with the optimality\noperator giving the best policy for the underlying RL problem. It translates\ninto averaging the log-likelihood within the loss. We empirically study the\neffect of such averaging, observing a trade-off between the length of\ngenerations and their scores.\n", "link": "http://arxiv.org/abs/2406.19188v1", "date": "2024-06-27", "relevancy": 1.7221, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4349}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4313}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Averaging%20log-likelihoods%20in%20direct%20alignment&body=Title%3A%20Averaging%20log-likelihoods%20in%20direct%20alignment%0AAuthor%3A%20Nathan%20Grinsztajn%20and%20Yannis%20Flet-Berliac%20and%20Mohammad%20Gheshlaghi%20Azar%20and%20Florian%20Strub%20and%20Bill%20Wu%20and%20Eugene%20Choi%20and%20Chris%20Cremer%20and%20Arash%20Ahmadian%20and%20Yash%20Chandak%20and%20Olivier%20Pietquin%20and%20Matthieu%20Geist%0AAbstract%3A%20%20%20To%20better%20align%20Large%20Language%20Models%20%28LLMs%29%20with%20human%20judgment%2C%0AReinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20learns%20a%20reward%20model%20and%0Athen%20optimizes%20it%20using%20regularized%20RL.%20Recently%2C%20direct%20alignment%20methods%20were%0Aintroduced%20to%20learn%20such%20a%20fine-tuned%20model%20directly%20from%20a%20preference%20dataset%0Awithout%20computing%20a%20proxy%20reward%20function.%20These%20methods%20are%20built%20upon%0Acontrastive%20losses%20involving%20the%20log-likelihood%20of%20%28dis%29preferred%20completions%0Aaccording%20to%20the%20trained%20model.%20However%2C%20completions%20have%20various%20lengths%2C%20and%0Athe%20log-likelihood%20is%20not%20length-invariant.%20On%20the%20other%20side%2C%20the%0Across-entropy%20loss%20used%20in%20supervised%20training%20is%20length-invariant%2C%20as%20batches%0Aare%20typically%20averaged%20token-wise.%20To%20reconcile%20these%20approaches%2C%20we%20introduce%0Aa%20principled%20approach%20for%20making%20direct%20alignment%20length-invariant.%20Formally%2C%0Awe%20introduce%20a%20new%20averaging%20operator%2C%20to%20be%20composed%20with%20the%20optimality%0Aoperator%20giving%20the%20best%20policy%20for%20the%20underlying%20RL%20problem.%20It%20translates%0Ainto%20averaging%20the%20log-likelihood%20within%20the%20loss.%20We%20empirically%20study%20the%0Aeffect%20of%20such%20averaging%2C%20observing%20a%20trade-off%20between%20the%20length%20of%0Agenerations%20and%20their%20scores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAveraging%2520log-likelihoods%2520in%2520direct%2520alignment%26entry.906535625%3DNathan%2520Grinsztajn%2520and%2520Yannis%2520Flet-Berliac%2520and%2520Mohammad%2520Gheshlaghi%2520Azar%2520and%2520Florian%2520Strub%2520and%2520Bill%2520Wu%2520and%2520Eugene%2520Choi%2520and%2520Chris%2520Cremer%2520and%2520Arash%2520Ahmadian%2520and%2520Yash%2520Chandak%2520and%2520Olivier%2520Pietquin%2520and%2520Matthieu%2520Geist%26entry.1292438233%3D%2520%2520To%2520better%2520align%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520human%2520judgment%252C%250AReinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520learns%2520a%2520reward%2520model%2520and%250Athen%2520optimizes%2520it%2520using%2520regularized%2520RL.%2520Recently%252C%2520direct%2520alignment%2520methods%2520were%250Aintroduced%2520to%2520learn%2520such%2520a%2520fine-tuned%2520model%2520directly%2520from%2520a%2520preference%2520dataset%250Awithout%2520computing%2520a%2520proxy%2520reward%2520function.%2520These%2520methods%2520are%2520built%2520upon%250Acontrastive%2520losses%2520involving%2520the%2520log-likelihood%2520of%2520%2528dis%2529preferred%2520completions%250Aaccording%2520to%2520the%2520trained%2520model.%2520However%252C%2520completions%2520have%2520various%2520lengths%252C%2520and%250Athe%2520log-likelihood%2520is%2520not%2520length-invariant.%2520On%2520the%2520other%2520side%252C%2520the%250Across-entropy%2520loss%2520used%2520in%2520supervised%2520training%2520is%2520length-invariant%252C%2520as%2520batches%250Aare%2520typically%2520averaged%2520token-wise.%2520To%2520reconcile%2520these%2520approaches%252C%2520we%2520introduce%250Aa%2520principled%2520approach%2520for%2520making%2520direct%2520alignment%2520length-invariant.%2520Formally%252C%250Awe%2520introduce%2520a%2520new%2520averaging%2520operator%252C%2520to%2520be%2520composed%2520with%2520the%2520optimality%250Aoperator%2520giving%2520the%2520best%2520policy%2520for%2520the%2520underlying%2520RL%2520problem.%2520It%2520translates%250Ainto%2520averaging%2520the%2520log-likelihood%2520within%2520the%2520loss.%2520We%2520empirically%2520study%2520the%250Aeffect%2520of%2520such%2520averaging%252C%2520observing%2520a%2520trade-off%2520between%2520the%2520length%2520of%250Agenerations%2520and%2520their%2520scores.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Averaging%20log-likelihoods%20in%20direct%20alignment&entry.906535625=Nathan%20Grinsztajn%20and%20Yannis%20Flet-Berliac%20and%20Mohammad%20Gheshlaghi%20Azar%20and%20Florian%20Strub%20and%20Bill%20Wu%20and%20Eugene%20Choi%20and%20Chris%20Cremer%20and%20Arash%20Ahmadian%20and%20Yash%20Chandak%20and%20Olivier%20Pietquin%20and%20Matthieu%20Geist&entry.1292438233=%20%20To%20better%20align%20Large%20Language%20Models%20%28LLMs%29%20with%20human%20judgment%2C%0AReinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20learns%20a%20reward%20model%20and%0Athen%20optimizes%20it%20using%20regularized%20RL.%20Recently%2C%20direct%20alignment%20methods%20were%0Aintroduced%20to%20learn%20such%20a%20fine-tuned%20model%20directly%20from%20a%20preference%20dataset%0Awithout%20computing%20a%20proxy%20reward%20function.%20These%20methods%20are%20built%20upon%0Acontrastive%20losses%20involving%20the%20log-likelihood%20of%20%28dis%29preferred%20completions%0Aaccording%20to%20the%20trained%20model.%20However%2C%20completions%20have%20various%20lengths%2C%20and%0Athe%20log-likelihood%20is%20not%20length-invariant.%20On%20the%20other%20side%2C%20the%0Across-entropy%20loss%20used%20in%20supervised%20training%20is%20length-invariant%2C%20as%20batches%0Aare%20typically%20averaged%20token-wise.%20To%20reconcile%20these%20approaches%2C%20we%20introduce%0Aa%20principled%20approach%20for%20making%20direct%20alignment%20length-invariant.%20Formally%2C%0Awe%20introduce%20a%20new%20averaging%20operator%2C%20to%20be%20composed%20with%20the%20optimality%0Aoperator%20giving%20the%20best%20policy%20for%20the%20underlying%20RL%20problem.%20It%20translates%0Ainto%20averaging%20the%20log-likelihood%20within%20the%20loss.%20We%20empirically%20study%20the%0Aeffect%20of%20such%20averaging%2C%20observing%20a%20trade-off%20between%20the%20length%20of%0Agenerations%20and%20their%20scores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19188v1&entry.124074799=Read"},
{"title": "The Price of Adaptivity in Stochastic Convex Optimization", "author": "Yair Carmon and Oliver Hinder", "abstract": "  We prove impossibility results for adaptivity in non-smooth stochastic convex\noptimization. Given a set of problem parameters we wish to adapt to, we define\na \"price of adaptivity\" (PoA) that, roughly speaking, measures the\nmultiplicative increase in suboptimality due to uncertainty in these\nparameters. When the initial distance to the optimum is unknown but a gradient\nnorm bound is known, we show that the PoA is at least logarithmic for expected\nsuboptimality, and double-logarithmic for median suboptimality. When there is\nuncertainty in both distance and gradient norm, we show that the PoA must be\npolynomial in the level of uncertainty. Our lower bounds nearly match existing\nupper bounds, and establish that there is no parameter-free lunch.\n  En route, we also establish tight upper and lower bounds for\n(known-parameter) high-probability stochastic convex optimization with\nheavy-tailed and bounded noise, respectively.\n", "link": "http://arxiv.org/abs/2402.10898v3", "date": "2024-06-27", "relevancy": 1.7128, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4452}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4378}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Price%20of%20Adaptivity%20in%20Stochastic%20Convex%20Optimization&body=Title%3A%20The%20Price%20of%20Adaptivity%20in%20Stochastic%20Convex%20Optimization%0AAuthor%3A%20Yair%20Carmon%20and%20Oliver%20Hinder%0AAbstract%3A%20%20%20We%20prove%20impossibility%20results%20for%20adaptivity%20in%20non-smooth%20stochastic%20convex%0Aoptimization.%20Given%20a%20set%20of%20problem%20parameters%20we%20wish%20to%20adapt%20to%2C%20we%20define%0Aa%20%22price%20of%20adaptivity%22%20%28PoA%29%20that%2C%20roughly%20speaking%2C%20measures%20the%0Amultiplicative%20increase%20in%20suboptimality%20due%20to%20uncertainty%20in%20these%0Aparameters.%20When%20the%20initial%20distance%20to%20the%20optimum%20is%20unknown%20but%20a%20gradient%0Anorm%20bound%20is%20known%2C%20we%20show%20that%20the%20PoA%20is%20at%20least%20logarithmic%20for%20expected%0Asuboptimality%2C%20and%20double-logarithmic%20for%20median%20suboptimality.%20When%20there%20is%0Auncertainty%20in%20both%20distance%20and%20gradient%20norm%2C%20we%20show%20that%20the%20PoA%20must%20be%0Apolynomial%20in%20the%20level%20of%20uncertainty.%20Our%20lower%20bounds%20nearly%20match%20existing%0Aupper%20bounds%2C%20and%20establish%20that%20there%20is%20no%20parameter-free%20lunch.%0A%20%20En%20route%2C%20we%20also%20establish%20tight%20upper%20and%20lower%20bounds%20for%0A%28known-parameter%29%20high-probability%20stochastic%20convex%20optimization%20with%0Aheavy-tailed%20and%20bounded%20noise%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10898v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Price%2520of%2520Adaptivity%2520in%2520Stochastic%2520Convex%2520Optimization%26entry.906535625%3DYair%2520Carmon%2520and%2520Oliver%2520Hinder%26entry.1292438233%3D%2520%2520We%2520prove%2520impossibility%2520results%2520for%2520adaptivity%2520in%2520non-smooth%2520stochastic%2520convex%250Aoptimization.%2520Given%2520a%2520set%2520of%2520problem%2520parameters%2520we%2520wish%2520to%2520adapt%2520to%252C%2520we%2520define%250Aa%2520%2522price%2520of%2520adaptivity%2522%2520%2528PoA%2529%2520that%252C%2520roughly%2520speaking%252C%2520measures%2520the%250Amultiplicative%2520increase%2520in%2520suboptimality%2520due%2520to%2520uncertainty%2520in%2520these%250Aparameters.%2520When%2520the%2520initial%2520distance%2520to%2520the%2520optimum%2520is%2520unknown%2520but%2520a%2520gradient%250Anorm%2520bound%2520is%2520known%252C%2520we%2520show%2520that%2520the%2520PoA%2520is%2520at%2520least%2520logarithmic%2520for%2520expected%250Asuboptimality%252C%2520and%2520double-logarithmic%2520for%2520median%2520suboptimality.%2520When%2520there%2520is%250Auncertainty%2520in%2520both%2520distance%2520and%2520gradient%2520norm%252C%2520we%2520show%2520that%2520the%2520PoA%2520must%2520be%250Apolynomial%2520in%2520the%2520level%2520of%2520uncertainty.%2520Our%2520lower%2520bounds%2520nearly%2520match%2520existing%250Aupper%2520bounds%252C%2520and%2520establish%2520that%2520there%2520is%2520no%2520parameter-free%2520lunch.%250A%2520%2520En%2520route%252C%2520we%2520also%2520establish%2520tight%2520upper%2520and%2520lower%2520bounds%2520for%250A%2528known-parameter%2529%2520high-probability%2520stochastic%2520convex%2520optimization%2520with%250Aheavy-tailed%2520and%2520bounded%2520noise%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10898v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Price%20of%20Adaptivity%20in%20Stochastic%20Convex%20Optimization&entry.906535625=Yair%20Carmon%20and%20Oliver%20Hinder&entry.1292438233=%20%20We%20prove%20impossibility%20results%20for%20adaptivity%20in%20non-smooth%20stochastic%20convex%0Aoptimization.%20Given%20a%20set%20of%20problem%20parameters%20we%20wish%20to%20adapt%20to%2C%20we%20define%0Aa%20%22price%20of%20adaptivity%22%20%28PoA%29%20that%2C%20roughly%20speaking%2C%20measures%20the%0Amultiplicative%20increase%20in%20suboptimality%20due%20to%20uncertainty%20in%20these%0Aparameters.%20When%20the%20initial%20distance%20to%20the%20optimum%20is%20unknown%20but%20a%20gradient%0Anorm%20bound%20is%20known%2C%20we%20show%20that%20the%20PoA%20is%20at%20least%20logarithmic%20for%20expected%0Asuboptimality%2C%20and%20double-logarithmic%20for%20median%20suboptimality.%20When%20there%20is%0Auncertainty%20in%20both%20distance%20and%20gradient%20norm%2C%20we%20show%20that%20the%20PoA%20must%20be%0Apolynomial%20in%20the%20level%20of%20uncertainty.%20Our%20lower%20bounds%20nearly%20match%20existing%0Aupper%20bounds%2C%20and%20establish%20that%20there%20is%20no%20parameter-free%20lunch.%0A%20%20En%20route%2C%20we%20also%20establish%20tight%20upper%20and%20lower%20bounds%20for%0A%28known-parameter%29%20high-probability%20stochastic%20convex%20optimization%20with%0Aheavy-tailed%20and%20bounded%20noise%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10898v3&entry.124074799=Read"},
{"title": "Robust Pushing: Exploiting Quasi-static Belief Dynamics and\n  Contact-informed Optimization", "author": "Julius Jankowski and Lara Bruderm\u00fcller and Nick Hawes and Sylvain Calinon", "abstract": "  Non-prehensile manipulation such as pushing is typically subject to\nuncertain, non-smooth dynamics. However, modeling the uncertainty of the\ndynamics typically results in intractable belief dynamics, making\ndata-efficient planning under uncertainty difficult. This article focuses on\nthe problem of efficiently generating robust open-loop pushing plans. First, we\ninvestigate how the belief over object configurations propagates through\nquasi-static contact dynamics. We exploit the simplified dynamics to predict\nthe variance of the object configuration without sampling from a perturbation\ndistribution. In a sampling-based trajectory optimization algorithm, the gain\nof the variance is constrained in order to enforce robustness of the plan.\nSecond, we propose an informed trajectory sampling mechanism for drawing robot\ntrajectories that are likely to make contact with the object. This sampling\nmechanism is shown to significantly improve chances of finding robust\nsolutions, especially when making-and-breaking contacts is required. We\ndemonstrate that the proposed approach is able to synthesize bi-manual pushing\ntrajectories, resulting in successful long-horizon pushing maneuvers without\nexteroceptive feedback such as vision or tactile feedback. We furthermore\ndeploy the proposed approach in a model-predictive control scheme,\ndemonstrating additional robustness against unmodeled perturbations.\n", "link": "http://arxiv.org/abs/2404.02795v2", "date": "2024-06-27", "relevancy": 1.7032, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6055}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5829}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Pushing%3A%20Exploiting%20Quasi-static%20Belief%20Dynamics%20and%0A%20%20Contact-informed%20Optimization&body=Title%3A%20Robust%20Pushing%3A%20Exploiting%20Quasi-static%20Belief%20Dynamics%20and%0A%20%20Contact-informed%20Optimization%0AAuthor%3A%20Julius%20Jankowski%20and%20Lara%20Bruderm%C3%BCller%20and%20Nick%20Hawes%20and%20Sylvain%20Calinon%0AAbstract%3A%20%20%20Non-prehensile%20manipulation%20such%20as%20pushing%20is%20typically%20subject%20to%0Auncertain%2C%20non-smooth%20dynamics.%20However%2C%20modeling%20the%20uncertainty%20of%20the%0Adynamics%20typically%20results%20in%20intractable%20belief%20dynamics%2C%20making%0Adata-efficient%20planning%20under%20uncertainty%20difficult.%20This%20article%20focuses%20on%0Athe%20problem%20of%20efficiently%20generating%20robust%20open-loop%20pushing%20plans.%20First%2C%20we%0Ainvestigate%20how%20the%20belief%20over%20object%20configurations%20propagates%20through%0Aquasi-static%20contact%20dynamics.%20We%20exploit%20the%20simplified%20dynamics%20to%20predict%0Athe%20variance%20of%20the%20object%20configuration%20without%20sampling%20from%20a%20perturbation%0Adistribution.%20In%20a%20sampling-based%20trajectory%20optimization%20algorithm%2C%20the%20gain%0Aof%20the%20variance%20is%20constrained%20in%20order%20to%20enforce%20robustness%20of%20the%20plan.%0ASecond%2C%20we%20propose%20an%20informed%20trajectory%20sampling%20mechanism%20for%20drawing%20robot%0Atrajectories%20that%20are%20likely%20to%20make%20contact%20with%20the%20object.%20This%20sampling%0Amechanism%20is%20shown%20to%20significantly%20improve%20chances%20of%20finding%20robust%0Asolutions%2C%20especially%20when%20making-and-breaking%20contacts%20is%20required.%20We%0Ademonstrate%20that%20the%20proposed%20approach%20is%20able%20to%20synthesize%20bi-manual%20pushing%0Atrajectories%2C%20resulting%20in%20successful%20long-horizon%20pushing%20maneuvers%20without%0Aexteroceptive%20feedback%20such%20as%20vision%20or%20tactile%20feedback.%20We%20furthermore%0Adeploy%20the%20proposed%20approach%20in%20a%20model-predictive%20control%20scheme%2C%0Ademonstrating%20additional%20robustness%20against%20unmodeled%20perturbations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02795v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Pushing%253A%2520Exploiting%2520Quasi-static%2520Belief%2520Dynamics%2520and%250A%2520%2520Contact-informed%2520Optimization%26entry.906535625%3DJulius%2520Jankowski%2520and%2520Lara%2520Bruderm%25C3%25BCller%2520and%2520Nick%2520Hawes%2520and%2520Sylvain%2520Calinon%26entry.1292438233%3D%2520%2520Non-prehensile%2520manipulation%2520such%2520as%2520pushing%2520is%2520typically%2520subject%2520to%250Auncertain%252C%2520non-smooth%2520dynamics.%2520However%252C%2520modeling%2520the%2520uncertainty%2520of%2520the%250Adynamics%2520typically%2520results%2520in%2520intractable%2520belief%2520dynamics%252C%2520making%250Adata-efficient%2520planning%2520under%2520uncertainty%2520difficult.%2520This%2520article%2520focuses%2520on%250Athe%2520problem%2520of%2520efficiently%2520generating%2520robust%2520open-loop%2520pushing%2520plans.%2520First%252C%2520we%250Ainvestigate%2520how%2520the%2520belief%2520over%2520object%2520configurations%2520propagates%2520through%250Aquasi-static%2520contact%2520dynamics.%2520We%2520exploit%2520the%2520simplified%2520dynamics%2520to%2520predict%250Athe%2520variance%2520of%2520the%2520object%2520configuration%2520without%2520sampling%2520from%2520a%2520perturbation%250Adistribution.%2520In%2520a%2520sampling-based%2520trajectory%2520optimization%2520algorithm%252C%2520the%2520gain%250Aof%2520the%2520variance%2520is%2520constrained%2520in%2520order%2520to%2520enforce%2520robustness%2520of%2520the%2520plan.%250ASecond%252C%2520we%2520propose%2520an%2520informed%2520trajectory%2520sampling%2520mechanism%2520for%2520drawing%2520robot%250Atrajectories%2520that%2520are%2520likely%2520to%2520make%2520contact%2520with%2520the%2520object.%2520This%2520sampling%250Amechanism%2520is%2520shown%2520to%2520significantly%2520improve%2520chances%2520of%2520finding%2520robust%250Asolutions%252C%2520especially%2520when%2520making-and-breaking%2520contacts%2520is%2520required.%2520We%250Ademonstrate%2520that%2520the%2520proposed%2520approach%2520is%2520able%2520to%2520synthesize%2520bi-manual%2520pushing%250Atrajectories%252C%2520resulting%2520in%2520successful%2520long-horizon%2520pushing%2520maneuvers%2520without%250Aexteroceptive%2520feedback%2520such%2520as%2520vision%2520or%2520tactile%2520feedback.%2520We%2520furthermore%250Adeploy%2520the%2520proposed%2520approach%2520in%2520a%2520model-predictive%2520control%2520scheme%252C%250Ademonstrating%2520additional%2520robustness%2520against%2520unmodeled%2520perturbations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02795v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Pushing%3A%20Exploiting%20Quasi-static%20Belief%20Dynamics%20and%0A%20%20Contact-informed%20Optimization&entry.906535625=Julius%20Jankowski%20and%20Lara%20Bruderm%C3%BCller%20and%20Nick%20Hawes%20and%20Sylvain%20Calinon&entry.1292438233=%20%20Non-prehensile%20manipulation%20such%20as%20pushing%20is%20typically%20subject%20to%0Auncertain%2C%20non-smooth%20dynamics.%20However%2C%20modeling%20the%20uncertainty%20of%20the%0Adynamics%20typically%20results%20in%20intractable%20belief%20dynamics%2C%20making%0Adata-efficient%20planning%20under%20uncertainty%20difficult.%20This%20article%20focuses%20on%0Athe%20problem%20of%20efficiently%20generating%20robust%20open-loop%20pushing%20plans.%20First%2C%20we%0Ainvestigate%20how%20the%20belief%20over%20object%20configurations%20propagates%20through%0Aquasi-static%20contact%20dynamics.%20We%20exploit%20the%20simplified%20dynamics%20to%20predict%0Athe%20variance%20of%20the%20object%20configuration%20without%20sampling%20from%20a%20perturbation%0Adistribution.%20In%20a%20sampling-based%20trajectory%20optimization%20algorithm%2C%20the%20gain%0Aof%20the%20variance%20is%20constrained%20in%20order%20to%20enforce%20robustness%20of%20the%20plan.%0ASecond%2C%20we%20propose%20an%20informed%20trajectory%20sampling%20mechanism%20for%20drawing%20robot%0Atrajectories%20that%20are%20likely%20to%20make%20contact%20with%20the%20object.%20This%20sampling%0Amechanism%20is%20shown%20to%20significantly%20improve%20chances%20of%20finding%20robust%0Asolutions%2C%20especially%20when%20making-and-breaking%20contacts%20is%20required.%20We%0Ademonstrate%20that%20the%20proposed%20approach%20is%20able%20to%20synthesize%20bi-manual%20pushing%0Atrajectories%2C%20resulting%20in%20successful%20long-horizon%20pushing%20maneuvers%20without%0Aexteroceptive%20feedback%20such%20as%20vision%20or%20tactile%20feedback.%20We%20furthermore%0Adeploy%20the%20proposed%20approach%20in%20a%20model-predictive%20control%20scheme%2C%0Ademonstrating%20additional%20robustness%20against%20unmodeled%20perturbations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02795v2&entry.124074799=Read"},
{"title": "scTree: Discovering Cellular Hierarchies in the Presence of Batch\n  Effects in scRNA-seq Data", "author": "Moritz Vandenhirtz and Florian Barkmann and Laura Manduchi and Julia E. Vogt and Valentina Boeva", "abstract": "  We propose a novel method, scTree, for single-cell Tree Variational\nAutoencoders, extending a hierarchical clustering approach to single-cell RNA\nsequencing data. scTree corrects for batch effects while simultaneously\nlearning a tree-structured data representation. This VAE-based method allows\nfor a more in-depth understanding of complex cellular landscapes independently\nof the biasing effects of batches. We show empirically on seven datasets that\nscTree discovers the underlying clusters of the data and the hierarchical\nrelations between them, as well as outperforms established baseline methods\nacross these datasets. Additionally, we analyze the learned hierarchy to\nunderstand its biological relevance, thus underpinning the importance of\nintegrating batch correction directly into the clustering procedure.\n", "link": "http://arxiv.org/abs/2406.19300v1", "date": "2024-06-27", "relevancy": 1.6951, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4396}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4181}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20scTree%3A%20Discovering%20Cellular%20Hierarchies%20in%20the%20Presence%20of%20Batch%0A%20%20Effects%20in%20scRNA-seq%20Data&body=Title%3A%20scTree%3A%20Discovering%20Cellular%20Hierarchies%20in%20the%20Presence%20of%20Batch%0A%20%20Effects%20in%20scRNA-seq%20Data%0AAuthor%3A%20Moritz%20Vandenhirtz%20and%20Florian%20Barkmann%20and%20Laura%20Manduchi%20and%20Julia%20E.%20Vogt%20and%20Valentina%20Boeva%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20method%2C%20scTree%2C%20for%20single-cell%20Tree%20Variational%0AAutoencoders%2C%20extending%20a%20hierarchical%20clustering%20approach%20to%20single-cell%20RNA%0Asequencing%20data.%20scTree%20corrects%20for%20batch%20effects%20while%20simultaneously%0Alearning%20a%20tree-structured%20data%20representation.%20This%20VAE-based%20method%20allows%0Afor%20a%20more%20in-depth%20understanding%20of%20complex%20cellular%20landscapes%20independently%0Aof%20the%20biasing%20effects%20of%20batches.%20We%20show%20empirically%20on%20seven%20datasets%20that%0AscTree%20discovers%20the%20underlying%20clusters%20of%20the%20data%20and%20the%20hierarchical%0Arelations%20between%20them%2C%20as%20well%20as%20outperforms%20established%20baseline%20methods%0Aacross%20these%20datasets.%20Additionally%2C%20we%20analyze%20the%20learned%20hierarchy%20to%0Aunderstand%20its%20biological%20relevance%2C%20thus%20underpinning%20the%20importance%20of%0Aintegrating%20batch%20correction%20directly%20into%20the%20clustering%20procedure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DscTree%253A%2520Discovering%2520Cellular%2520Hierarchies%2520in%2520the%2520Presence%2520of%2520Batch%250A%2520%2520Effects%2520in%2520scRNA-seq%2520Data%26entry.906535625%3DMoritz%2520Vandenhirtz%2520and%2520Florian%2520Barkmann%2520and%2520Laura%2520Manduchi%2520and%2520Julia%2520E.%2520Vogt%2520and%2520Valentina%2520Boeva%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520method%252C%2520scTree%252C%2520for%2520single-cell%2520Tree%2520Variational%250AAutoencoders%252C%2520extending%2520a%2520hierarchical%2520clustering%2520approach%2520to%2520single-cell%2520RNA%250Asequencing%2520data.%2520scTree%2520corrects%2520for%2520batch%2520effects%2520while%2520simultaneously%250Alearning%2520a%2520tree-structured%2520data%2520representation.%2520This%2520VAE-based%2520method%2520allows%250Afor%2520a%2520more%2520in-depth%2520understanding%2520of%2520complex%2520cellular%2520landscapes%2520independently%250Aof%2520the%2520biasing%2520effects%2520of%2520batches.%2520We%2520show%2520empirically%2520on%2520seven%2520datasets%2520that%250AscTree%2520discovers%2520the%2520underlying%2520clusters%2520of%2520the%2520data%2520and%2520the%2520hierarchical%250Arelations%2520between%2520them%252C%2520as%2520well%2520as%2520outperforms%2520established%2520baseline%2520methods%250Aacross%2520these%2520datasets.%2520Additionally%252C%2520we%2520analyze%2520the%2520learned%2520hierarchy%2520to%250Aunderstand%2520its%2520biological%2520relevance%252C%2520thus%2520underpinning%2520the%2520importance%2520of%250Aintegrating%2520batch%2520correction%2520directly%2520into%2520the%2520clustering%2520procedure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=scTree%3A%20Discovering%20Cellular%20Hierarchies%20in%20the%20Presence%20of%20Batch%0A%20%20Effects%20in%20scRNA-seq%20Data&entry.906535625=Moritz%20Vandenhirtz%20and%20Florian%20Barkmann%20and%20Laura%20Manduchi%20and%20Julia%20E.%20Vogt%20and%20Valentina%20Boeva&entry.1292438233=%20%20We%20propose%20a%20novel%20method%2C%20scTree%2C%20for%20single-cell%20Tree%20Variational%0AAutoencoders%2C%20extending%20a%20hierarchical%20clustering%20approach%20to%20single-cell%20RNA%0Asequencing%20data.%20scTree%20corrects%20for%20batch%20effects%20while%20simultaneously%0Alearning%20a%20tree-structured%20data%20representation.%20This%20VAE-based%20method%20allows%0Afor%20a%20more%20in-depth%20understanding%20of%20complex%20cellular%20landscapes%20independently%0Aof%20the%20biasing%20effects%20of%20batches.%20We%20show%20empirically%20on%20seven%20datasets%20that%0AscTree%20discovers%20the%20underlying%20clusters%20of%20the%20data%20and%20the%20hierarchical%0Arelations%20between%20them%2C%20as%20well%20as%20outperforms%20established%20baseline%20methods%0Aacross%20these%20datasets.%20Additionally%2C%20we%20analyze%20the%20learned%20hierarchy%20to%0Aunderstand%20its%20biological%20relevance%2C%20thus%20underpinning%20the%20importance%20of%0Aintegrating%20batch%20correction%20directly%20into%20the%20clustering%20procedure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19300v1&entry.124074799=Read"},
{"title": "Mapping the Potential of Explainable AI for Fairness Along the AI\n  Lifecycle", "author": "Luca Deck and Astrid Schom\u00e4cker and Timo Speith and Jakob Sch\u00f6ffer and Lena K\u00e4stner and Niklas K\u00fchl", "abstract": "  The widespread use of artificial intelligence (AI) systems across various\ndomains is increasingly surfacing issues related to algorithmic fairness,\nespecially in high-stakes scenarios. Thus, critical considerations of how\nfairness in AI systems might be improved -- and what measures are available to\naid this process -- are overdue. Many researchers and policymakers see\nexplainable AI (XAI) as a promising way to increase fairness in AI systems.\nHowever, there is a wide variety of XAI methods and fairness conceptions\nexpressing different desiderata, and the precise connections between XAI and\nfairness remain largely nebulous. Besides, different measures to increase\nalgorithmic fairness might be applicable at different points throughout an AI\nsystem's lifecycle. Yet, there currently is no coherent mapping of fairness\ndesiderata along the AI lifecycle. In this paper, we we distill eight fairness\ndesiderata, map them along the AI lifecycle, and discuss how XAI could help\naddress each of them. We hope to provide orientation for practical applications\nand to inspire XAI research specifically focused on these fairness desiderata.\n", "link": "http://arxiv.org/abs/2404.18736v4", "date": "2024-06-27", "relevancy": 1.6813, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4405}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4205}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mapping%20the%20Potential%20of%20Explainable%20AI%20for%20Fairness%20Along%20the%20AI%0A%20%20Lifecycle&body=Title%3A%20Mapping%20the%20Potential%20of%20Explainable%20AI%20for%20Fairness%20Along%20the%20AI%0A%20%20Lifecycle%0AAuthor%3A%20Luca%20Deck%20and%20Astrid%20Schom%C3%A4cker%20and%20Timo%20Speith%20and%20Jakob%20Sch%C3%B6ffer%20and%20Lena%20K%C3%A4stner%20and%20Niklas%20K%C3%BChl%0AAbstract%3A%20%20%20The%20widespread%20use%20of%20artificial%20intelligence%20%28AI%29%20systems%20across%20various%0Adomains%20is%20increasingly%20surfacing%20issues%20related%20to%20algorithmic%20fairness%2C%0Aespecially%20in%20high-stakes%20scenarios.%20Thus%2C%20critical%20considerations%20of%20how%0Afairness%20in%20AI%20systems%20might%20be%20improved%20--%20and%20what%20measures%20are%20available%20to%0Aaid%20this%20process%20--%20are%20overdue.%20Many%20researchers%20and%20policymakers%20see%0Aexplainable%20AI%20%28XAI%29%20as%20a%20promising%20way%20to%20increase%20fairness%20in%20AI%20systems.%0AHowever%2C%20there%20is%20a%20wide%20variety%20of%20XAI%20methods%20and%20fairness%20conceptions%0Aexpressing%20different%20desiderata%2C%20and%20the%20precise%20connections%20between%20XAI%20and%0Afairness%20remain%20largely%20nebulous.%20Besides%2C%20different%20measures%20to%20increase%0Aalgorithmic%20fairness%20might%20be%20applicable%20at%20different%20points%20throughout%20an%20AI%0Asystem%27s%20lifecycle.%20Yet%2C%20there%20currently%20is%20no%20coherent%20mapping%20of%20fairness%0Adesiderata%20along%20the%20AI%20lifecycle.%20In%20this%20paper%2C%20we%20we%20distill%20eight%20fairness%0Adesiderata%2C%20map%20them%20along%20the%20AI%20lifecycle%2C%20and%20discuss%20how%20XAI%20could%20help%0Aaddress%20each%20of%20them.%20We%20hope%20to%20provide%20orientation%20for%20practical%20applications%0Aand%20to%20inspire%20XAI%20research%20specifically%20focused%20on%20these%20fairness%20desiderata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18736v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapping%2520the%2520Potential%2520of%2520Explainable%2520AI%2520for%2520Fairness%2520Along%2520the%2520AI%250A%2520%2520Lifecycle%26entry.906535625%3DLuca%2520Deck%2520and%2520Astrid%2520Schom%25C3%25A4cker%2520and%2520Timo%2520Speith%2520and%2520Jakob%2520Sch%25C3%25B6ffer%2520and%2520Lena%2520K%25C3%25A4stner%2520and%2520Niklas%2520K%25C3%25BChl%26entry.1292438233%3D%2520%2520The%2520widespread%2520use%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520systems%2520across%2520various%250Adomains%2520is%2520increasingly%2520surfacing%2520issues%2520related%2520to%2520algorithmic%2520fairness%252C%250Aespecially%2520in%2520high-stakes%2520scenarios.%2520Thus%252C%2520critical%2520considerations%2520of%2520how%250Afairness%2520in%2520AI%2520systems%2520might%2520be%2520improved%2520--%2520and%2520what%2520measures%2520are%2520available%2520to%250Aaid%2520this%2520process%2520--%2520are%2520overdue.%2520Many%2520researchers%2520and%2520policymakers%2520see%250Aexplainable%2520AI%2520%2528XAI%2529%2520as%2520a%2520promising%2520way%2520to%2520increase%2520fairness%2520in%2520AI%2520systems.%250AHowever%252C%2520there%2520is%2520a%2520wide%2520variety%2520of%2520XAI%2520methods%2520and%2520fairness%2520conceptions%250Aexpressing%2520different%2520desiderata%252C%2520and%2520the%2520precise%2520connections%2520between%2520XAI%2520and%250Afairness%2520remain%2520largely%2520nebulous.%2520Besides%252C%2520different%2520measures%2520to%2520increase%250Aalgorithmic%2520fairness%2520might%2520be%2520applicable%2520at%2520different%2520points%2520throughout%2520an%2520AI%250Asystem%2527s%2520lifecycle.%2520Yet%252C%2520there%2520currently%2520is%2520no%2520coherent%2520mapping%2520of%2520fairness%250Adesiderata%2520along%2520the%2520AI%2520lifecycle.%2520In%2520this%2520paper%252C%2520we%2520we%2520distill%2520eight%2520fairness%250Adesiderata%252C%2520map%2520them%2520along%2520the%2520AI%2520lifecycle%252C%2520and%2520discuss%2520how%2520XAI%2520could%2520help%250Aaddress%2520each%2520of%2520them.%2520We%2520hope%2520to%2520provide%2520orientation%2520for%2520practical%2520applications%250Aand%2520to%2520inspire%2520XAI%2520research%2520specifically%2520focused%2520on%2520these%2520fairness%2520desiderata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18736v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mapping%20the%20Potential%20of%20Explainable%20AI%20for%20Fairness%20Along%20the%20AI%0A%20%20Lifecycle&entry.906535625=Luca%20Deck%20and%20Astrid%20Schom%C3%A4cker%20and%20Timo%20Speith%20and%20Jakob%20Sch%C3%B6ffer%20and%20Lena%20K%C3%A4stner%20and%20Niklas%20K%C3%BChl&entry.1292438233=%20%20The%20widespread%20use%20of%20artificial%20intelligence%20%28AI%29%20systems%20across%20various%0Adomains%20is%20increasingly%20surfacing%20issues%20related%20to%20algorithmic%20fairness%2C%0Aespecially%20in%20high-stakes%20scenarios.%20Thus%2C%20critical%20considerations%20of%20how%0Afairness%20in%20AI%20systems%20might%20be%20improved%20--%20and%20what%20measures%20are%20available%20to%0Aaid%20this%20process%20--%20are%20overdue.%20Many%20researchers%20and%20policymakers%20see%0Aexplainable%20AI%20%28XAI%29%20as%20a%20promising%20way%20to%20increase%20fairness%20in%20AI%20systems.%0AHowever%2C%20there%20is%20a%20wide%20variety%20of%20XAI%20methods%20and%20fairness%20conceptions%0Aexpressing%20different%20desiderata%2C%20and%20the%20precise%20connections%20between%20XAI%20and%0Afairness%20remain%20largely%20nebulous.%20Besides%2C%20different%20measures%20to%20increase%0Aalgorithmic%20fairness%20might%20be%20applicable%20at%20different%20points%20throughout%20an%20AI%0Asystem%27s%20lifecycle.%20Yet%2C%20there%20currently%20is%20no%20coherent%20mapping%20of%20fairness%0Adesiderata%20along%20the%20AI%20lifecycle.%20In%20this%20paper%2C%20we%20we%20distill%20eight%20fairness%0Adesiderata%2C%20map%20them%20along%20the%20AI%20lifecycle%2C%20and%20discuss%20how%20XAI%20could%20help%0Aaddress%20each%20of%20them.%20We%20hope%20to%20provide%20orientation%20for%20practical%20applications%0Aand%20to%20inspire%20XAI%20research%20specifically%20focused%20on%20these%20fairness%20desiderata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18736v4&entry.124074799=Read"},
{"title": "Evidential Concept Embedding Models: Towards Reliable Concept\n  Explanations for Skin Disease Diagnosis", "author": "Yibo Gao and Zheyao Gao and Xin Gao and Yuanye Liu and Bomin Wang and Xiahai Zhuang", "abstract": "  Due to the high stakes in medical decision-making, there is a compelling\ndemand for interpretable deep learning methods in medical image analysis.\nConcept Bottleneck Models (CBM) have emerged as an active interpretable\nframework incorporating human-interpretable concepts into decision-making.\nHowever, their concept predictions may lack reliability when applied to\nclinical diagnosis, impeding concept explanations' quality. To address this, we\npropose an evidential Concept Embedding Model (evi-CEM), which employs\nevidential learning to model the concept uncertainty. Additionally, we offer to\nleverage the concept uncertainty to rectify concept misalignments that arise\nwhen training CBMs using vision-language models without complete concept\nsupervision. With the proposed methods, we can enhance concept explanations'\nreliability for both supervised and label-efficient settings. Furthermore, we\nintroduce concept uncertainty for effective test-time intervention. Our\nevaluation demonstrates that evi-CEM achieves superior performance in terms of\nconcept prediction, and the proposed concept rectification effectively\nmitigates concept misalignments for label-efficient training. Our code is\navailable at https://github.com/obiyoag/evi-CEM.\n", "link": "http://arxiv.org/abs/2406.19130v1", "date": "2024-06-27", "relevancy": 1.6793, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.583}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5606}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evidential%20Concept%20Embedding%20Models%3A%20Towards%20Reliable%20Concept%0A%20%20Explanations%20for%20Skin%20Disease%20Diagnosis&body=Title%3A%20Evidential%20Concept%20Embedding%20Models%3A%20Towards%20Reliable%20Concept%0A%20%20Explanations%20for%20Skin%20Disease%20Diagnosis%0AAuthor%3A%20Yibo%20Gao%20and%20Zheyao%20Gao%20and%20Xin%20Gao%20and%20Yuanye%20Liu%20and%20Bomin%20Wang%20and%20Xiahai%20Zhuang%0AAbstract%3A%20%20%20Due%20to%20the%20high%20stakes%20in%20medical%20decision-making%2C%20there%20is%20a%20compelling%0Ademand%20for%20interpretable%20deep%20learning%20methods%20in%20medical%20image%20analysis.%0AConcept%20Bottleneck%20Models%20%28CBM%29%20have%20emerged%20as%20an%20active%20interpretable%0Aframework%20incorporating%20human-interpretable%20concepts%20into%20decision-making.%0AHowever%2C%20their%20concept%20predictions%20may%20lack%20reliability%20when%20applied%20to%0Aclinical%20diagnosis%2C%20impeding%20concept%20explanations%27%20quality.%20To%20address%20this%2C%20we%0Apropose%20an%20evidential%20Concept%20Embedding%20Model%20%28evi-CEM%29%2C%20which%20employs%0Aevidential%20learning%20to%20model%20the%20concept%20uncertainty.%20Additionally%2C%20we%20offer%20to%0Aleverage%20the%20concept%20uncertainty%20to%20rectify%20concept%20misalignments%20that%20arise%0Awhen%20training%20CBMs%20using%20vision-language%20models%20without%20complete%20concept%0Asupervision.%20With%20the%20proposed%20methods%2C%20we%20can%20enhance%20concept%20explanations%27%0Areliability%20for%20both%20supervised%20and%20label-efficient%20settings.%20Furthermore%2C%20we%0Aintroduce%20concept%20uncertainty%20for%20effective%20test-time%20intervention.%20Our%0Aevaluation%20demonstrates%20that%20evi-CEM%20achieves%20superior%20performance%20in%20terms%20of%0Aconcept%20prediction%2C%20and%20the%20proposed%20concept%20rectification%20effectively%0Amitigates%20concept%20misalignments%20for%20label-efficient%20training.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/obiyoag/evi-CEM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvidential%2520Concept%2520Embedding%2520Models%253A%2520Towards%2520Reliable%2520Concept%250A%2520%2520Explanations%2520for%2520Skin%2520Disease%2520Diagnosis%26entry.906535625%3DYibo%2520Gao%2520and%2520Zheyao%2520Gao%2520and%2520Xin%2520Gao%2520and%2520Yuanye%2520Liu%2520and%2520Bomin%2520Wang%2520and%2520Xiahai%2520Zhuang%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520high%2520stakes%2520in%2520medical%2520decision-making%252C%2520there%2520is%2520a%2520compelling%250Ademand%2520for%2520interpretable%2520deep%2520learning%2520methods%2520in%2520medical%2520image%2520analysis.%250AConcept%2520Bottleneck%2520Models%2520%2528CBM%2529%2520have%2520emerged%2520as%2520an%2520active%2520interpretable%250Aframework%2520incorporating%2520human-interpretable%2520concepts%2520into%2520decision-making.%250AHowever%252C%2520their%2520concept%2520predictions%2520may%2520lack%2520reliability%2520when%2520applied%2520to%250Aclinical%2520diagnosis%252C%2520impeding%2520concept%2520explanations%2527%2520quality.%2520To%2520address%2520this%252C%2520we%250Apropose%2520an%2520evidential%2520Concept%2520Embedding%2520Model%2520%2528evi-CEM%2529%252C%2520which%2520employs%250Aevidential%2520learning%2520to%2520model%2520the%2520concept%2520uncertainty.%2520Additionally%252C%2520we%2520offer%2520to%250Aleverage%2520the%2520concept%2520uncertainty%2520to%2520rectify%2520concept%2520misalignments%2520that%2520arise%250Awhen%2520training%2520CBMs%2520using%2520vision-language%2520models%2520without%2520complete%2520concept%250Asupervision.%2520With%2520the%2520proposed%2520methods%252C%2520we%2520can%2520enhance%2520concept%2520explanations%2527%250Areliability%2520for%2520both%2520supervised%2520and%2520label-efficient%2520settings.%2520Furthermore%252C%2520we%250Aintroduce%2520concept%2520uncertainty%2520for%2520effective%2520test-time%2520intervention.%2520Our%250Aevaluation%2520demonstrates%2520that%2520evi-CEM%2520achieves%2520superior%2520performance%2520in%2520terms%2520of%250Aconcept%2520prediction%252C%2520and%2520the%2520proposed%2520concept%2520rectification%2520effectively%250Amitigates%2520concept%2520misalignments%2520for%2520label-efficient%2520training.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/obiyoag/evi-CEM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evidential%20Concept%20Embedding%20Models%3A%20Towards%20Reliable%20Concept%0A%20%20Explanations%20for%20Skin%20Disease%20Diagnosis&entry.906535625=Yibo%20Gao%20and%20Zheyao%20Gao%20and%20Xin%20Gao%20and%20Yuanye%20Liu%20and%20Bomin%20Wang%20and%20Xiahai%20Zhuang&entry.1292438233=%20%20Due%20to%20the%20high%20stakes%20in%20medical%20decision-making%2C%20there%20is%20a%20compelling%0Ademand%20for%20interpretable%20deep%20learning%20methods%20in%20medical%20image%20analysis.%0AConcept%20Bottleneck%20Models%20%28CBM%29%20have%20emerged%20as%20an%20active%20interpretable%0Aframework%20incorporating%20human-interpretable%20concepts%20into%20decision-making.%0AHowever%2C%20their%20concept%20predictions%20may%20lack%20reliability%20when%20applied%20to%0Aclinical%20diagnosis%2C%20impeding%20concept%20explanations%27%20quality.%20To%20address%20this%2C%20we%0Apropose%20an%20evidential%20Concept%20Embedding%20Model%20%28evi-CEM%29%2C%20which%20employs%0Aevidential%20learning%20to%20model%20the%20concept%20uncertainty.%20Additionally%2C%20we%20offer%20to%0Aleverage%20the%20concept%20uncertainty%20to%20rectify%20concept%20misalignments%20that%20arise%0Awhen%20training%20CBMs%20using%20vision-language%20models%20without%20complete%20concept%0Asupervision.%20With%20the%20proposed%20methods%2C%20we%20can%20enhance%20concept%20explanations%27%0Areliability%20for%20both%20supervised%20and%20label-efficient%20settings.%20Furthermore%2C%20we%0Aintroduce%20concept%20uncertainty%20for%20effective%20test-time%20intervention.%20Our%0Aevaluation%20demonstrates%20that%20evi-CEM%20achieves%20superior%20performance%20in%20terms%20of%0Aconcept%20prediction%2C%20and%20the%20proposed%20concept%20rectification%20effectively%0Amitigates%20concept%20misalignments%20for%20label-efficient%20training.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/obiyoag/evi-CEM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19130v1&entry.124074799=Read"},
{"title": "Automatic infant 2D pose estimation from videos: comparing seven deep\n  neural network methods", "author": "Filipe Gama and Matej Misar and Lukas Navara and Sergiu T. Popescu and Matej Hoffmann", "abstract": "  Automatic markerless estimation of infant posture and motion from ordinary\nvideos carries great potential for movement studies \"in the wild\", facilitating\nunderstanding of motor development and massively increasing the chances of\nearly diagnosis of disorders. There is rapid development of human pose\nestimation methods in computer vision thanks to advances in deep learning and\nmachine learning. However, these methods are trained on datasets featuring\nadults in different contexts. This work tests and compares seven popular\nmethods (AlphaPose, DeepLabCut/DeeperCut, Detectron2, HRNet,\nMediaPipe/BlazePose, OpenPose, and ViTPose) on videos of infants in supine\nposition. Surprisingly, all methods except DeepLabCut and MediaPipe have\ncompetitive performance without additional finetuning, with ViTPose performing\nbest. Next to standard performance metrics (object keypoint similarity, average\nprecision and recall), we introduce errors expressed in the neck-mid-hip ratio\nand additionally study missed and redundant detections and the reliability of\nthe internal confidence ratings of the different methods, which are relevant\nfor downstream tasks. Among the networks with competitive performance, only\nAlphaPose could run close to real time (27 fps) on our machine. We provide\ndocumented Docker containers or instructions for all the methods we used, our\nanalysis scripts, and processed data at https://hub.docker.com/u/humanoidsctu\nand https://osf.io/x465b/.\n", "link": "http://arxiv.org/abs/2406.17382v2", "date": "2024-06-27", "relevancy": 1.6782, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.581}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5332}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20infant%202D%20pose%20estimation%20from%20videos%3A%20comparing%20seven%20deep%0A%20%20neural%20network%20methods&body=Title%3A%20Automatic%20infant%202D%20pose%20estimation%20from%20videos%3A%20comparing%20seven%20deep%0A%20%20neural%20network%20methods%0AAuthor%3A%20Filipe%20Gama%20and%20Matej%20Misar%20and%20Lukas%20Navara%20and%20Sergiu%20T.%20Popescu%20and%20Matej%20Hoffmann%0AAbstract%3A%20%20%20Automatic%20markerless%20estimation%20of%20infant%20posture%20and%20motion%20from%20ordinary%0Avideos%20carries%20great%20potential%20for%20movement%20studies%20%22in%20the%20wild%22%2C%20facilitating%0Aunderstanding%20of%20motor%20development%20and%20massively%20increasing%20the%20chances%20of%0Aearly%20diagnosis%20of%20disorders.%20There%20is%20rapid%20development%20of%20human%20pose%0Aestimation%20methods%20in%20computer%20vision%20thanks%20to%20advances%20in%20deep%20learning%20and%0Amachine%20learning.%20However%2C%20these%20methods%20are%20trained%20on%20datasets%20featuring%0Aadults%20in%20different%20contexts.%20This%20work%20tests%20and%20compares%20seven%20popular%0Amethods%20%28AlphaPose%2C%20DeepLabCut/DeeperCut%2C%20Detectron2%2C%20HRNet%2C%0AMediaPipe/BlazePose%2C%20OpenPose%2C%20and%20ViTPose%29%20on%20videos%20of%20infants%20in%20supine%0Aposition.%20Surprisingly%2C%20all%20methods%20except%20DeepLabCut%20and%20MediaPipe%20have%0Acompetitive%20performance%20without%20additional%20finetuning%2C%20with%20ViTPose%20performing%0Abest.%20Next%20to%20standard%20performance%20metrics%20%28object%20keypoint%20similarity%2C%20average%0Aprecision%20and%20recall%29%2C%20we%20introduce%20errors%20expressed%20in%20the%20neck-mid-hip%20ratio%0Aand%20additionally%20study%20missed%20and%20redundant%20detections%20and%20the%20reliability%20of%0Athe%20internal%20confidence%20ratings%20of%20the%20different%20methods%2C%20which%20are%20relevant%0Afor%20downstream%20tasks.%20Among%20the%20networks%20with%20competitive%20performance%2C%20only%0AAlphaPose%20could%20run%20close%20to%20real%20time%20%2827%20fps%29%20on%20our%20machine.%20We%20provide%0Adocumented%20Docker%20containers%20or%20instructions%20for%20all%20the%20methods%20we%20used%2C%20our%0Aanalysis%20scripts%2C%20and%20processed%20data%20at%20https%3A//hub.docker.com/u/humanoidsctu%0Aand%20https%3A//osf.io/x465b/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17382v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520infant%25202D%2520pose%2520estimation%2520from%2520videos%253A%2520comparing%2520seven%2520deep%250A%2520%2520neural%2520network%2520methods%26entry.906535625%3DFilipe%2520Gama%2520and%2520Matej%2520Misar%2520and%2520Lukas%2520Navara%2520and%2520Sergiu%2520T.%2520Popescu%2520and%2520Matej%2520Hoffmann%26entry.1292438233%3D%2520%2520Automatic%2520markerless%2520estimation%2520of%2520infant%2520posture%2520and%2520motion%2520from%2520ordinary%250Avideos%2520carries%2520great%2520potential%2520for%2520movement%2520studies%2520%2522in%2520the%2520wild%2522%252C%2520facilitating%250Aunderstanding%2520of%2520motor%2520development%2520and%2520massively%2520increasing%2520the%2520chances%2520of%250Aearly%2520diagnosis%2520of%2520disorders.%2520There%2520is%2520rapid%2520development%2520of%2520human%2520pose%250Aestimation%2520methods%2520in%2520computer%2520vision%2520thanks%2520to%2520advances%2520in%2520deep%2520learning%2520and%250Amachine%2520learning.%2520However%252C%2520these%2520methods%2520are%2520trained%2520on%2520datasets%2520featuring%250Aadults%2520in%2520different%2520contexts.%2520This%2520work%2520tests%2520and%2520compares%2520seven%2520popular%250Amethods%2520%2528AlphaPose%252C%2520DeepLabCut/DeeperCut%252C%2520Detectron2%252C%2520HRNet%252C%250AMediaPipe/BlazePose%252C%2520OpenPose%252C%2520and%2520ViTPose%2529%2520on%2520videos%2520of%2520infants%2520in%2520supine%250Aposition.%2520Surprisingly%252C%2520all%2520methods%2520except%2520DeepLabCut%2520and%2520MediaPipe%2520have%250Acompetitive%2520performance%2520without%2520additional%2520finetuning%252C%2520with%2520ViTPose%2520performing%250Abest.%2520Next%2520to%2520standard%2520performance%2520metrics%2520%2528object%2520keypoint%2520similarity%252C%2520average%250Aprecision%2520and%2520recall%2529%252C%2520we%2520introduce%2520errors%2520expressed%2520in%2520the%2520neck-mid-hip%2520ratio%250Aand%2520additionally%2520study%2520missed%2520and%2520redundant%2520detections%2520and%2520the%2520reliability%2520of%250Athe%2520internal%2520confidence%2520ratings%2520of%2520the%2520different%2520methods%252C%2520which%2520are%2520relevant%250Afor%2520downstream%2520tasks.%2520Among%2520the%2520networks%2520with%2520competitive%2520performance%252C%2520only%250AAlphaPose%2520could%2520run%2520close%2520to%2520real%2520time%2520%252827%2520fps%2529%2520on%2520our%2520machine.%2520We%2520provide%250Adocumented%2520Docker%2520containers%2520or%2520instructions%2520for%2520all%2520the%2520methods%2520we%2520used%252C%2520our%250Aanalysis%2520scripts%252C%2520and%2520processed%2520data%2520at%2520https%253A//hub.docker.com/u/humanoidsctu%250Aand%2520https%253A//osf.io/x465b/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17382v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20infant%202D%20pose%20estimation%20from%20videos%3A%20comparing%20seven%20deep%0A%20%20neural%20network%20methods&entry.906535625=Filipe%20Gama%20and%20Matej%20Misar%20and%20Lukas%20Navara%20and%20Sergiu%20T.%20Popescu%20and%20Matej%20Hoffmann&entry.1292438233=%20%20Automatic%20markerless%20estimation%20of%20infant%20posture%20and%20motion%20from%20ordinary%0Avideos%20carries%20great%20potential%20for%20movement%20studies%20%22in%20the%20wild%22%2C%20facilitating%0Aunderstanding%20of%20motor%20development%20and%20massively%20increasing%20the%20chances%20of%0Aearly%20diagnosis%20of%20disorders.%20There%20is%20rapid%20development%20of%20human%20pose%0Aestimation%20methods%20in%20computer%20vision%20thanks%20to%20advances%20in%20deep%20learning%20and%0Amachine%20learning.%20However%2C%20these%20methods%20are%20trained%20on%20datasets%20featuring%0Aadults%20in%20different%20contexts.%20This%20work%20tests%20and%20compares%20seven%20popular%0Amethods%20%28AlphaPose%2C%20DeepLabCut/DeeperCut%2C%20Detectron2%2C%20HRNet%2C%0AMediaPipe/BlazePose%2C%20OpenPose%2C%20and%20ViTPose%29%20on%20videos%20of%20infants%20in%20supine%0Aposition.%20Surprisingly%2C%20all%20methods%20except%20DeepLabCut%20and%20MediaPipe%20have%0Acompetitive%20performance%20without%20additional%20finetuning%2C%20with%20ViTPose%20performing%0Abest.%20Next%20to%20standard%20performance%20metrics%20%28object%20keypoint%20similarity%2C%20average%0Aprecision%20and%20recall%29%2C%20we%20introduce%20errors%20expressed%20in%20the%20neck-mid-hip%20ratio%0Aand%20additionally%20study%20missed%20and%20redundant%20detections%20and%20the%20reliability%20of%0Athe%20internal%20confidence%20ratings%20of%20the%20different%20methods%2C%20which%20are%20relevant%0Afor%20downstream%20tasks.%20Among%20the%20networks%20with%20competitive%20performance%2C%20only%0AAlphaPose%20could%20run%20close%20to%20real%20time%20%2827%20fps%29%20on%20our%20machine.%20We%20provide%0Adocumented%20Docker%20containers%20or%20instructions%20for%20all%20the%20methods%20we%20used%2C%20our%0Aanalysis%20scripts%2C%20and%20processed%20data%20at%20https%3A//hub.docker.com/u/humanoidsctu%0Aand%20https%3A//osf.io/x465b/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17382v2&entry.124074799=Read"},
{"title": "Human-Aware Vision-and-Language Navigation: Bridging Simulation to\n  Reality with Dynamic Human Interactions", "author": "Minghan Li and Heng Li and Zhi-Qi Cheng and Yifei Dong and Yuxuan Zhou and Jun-Yan He and Qi Dai and Teruko Mitamura and Alexander G. Hauptmann", "abstract": "  Vision-and-Language Navigation (VLN) aims to develop embodied agents that\nnavigate based on human instructions. However, current VLN frameworks often\nrely on static environments and optimal expert supervision, limiting their\nreal-world applicability. To address this, we introduce Human-Aware\nVision-and-Language Navigation (HA-VLN), extending traditional VLN by\nincorporating dynamic human activities and relaxing key assumptions. We propose\nthe Human-Aware 3D (HA3D) simulator, which combines dynamic human activities\nwith the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R)\ndataset, extending R2R with human activity descriptions. To tackle HA-VLN\nchallenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and\nNon-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing\ncross-modal fusion and diverse training strategies for effective navigation in\ndynamic human environments. A comprehensive evaluation, including metrics\nconsidering human activities, and systematic analysis of HA-VLN's unique\nchallenges, underscores the need for further research to enhance HA-VLN agents'\nreal-world robustness and adaptability. Ultimately, this work provides\nbenchmarks and insights for future research on embodied AI and Sim2Real\ntransfer, paving the way for more realistic and applicable VLN systems in\nhuman-populated environments.\n", "link": "http://arxiv.org/abs/2406.19236v1", "date": "2024-06-27", "relevancy": 1.6711, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.559}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5583}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Aware%20Vision-and-Language%20Navigation%3A%20Bridging%20Simulation%20to%0A%20%20Reality%20with%20Dynamic%20Human%20Interactions&body=Title%3A%20Human-Aware%20Vision-and-Language%20Navigation%3A%20Bridging%20Simulation%20to%0A%20%20Reality%20with%20Dynamic%20Human%20Interactions%0AAuthor%3A%20Minghan%20Li%20and%20Heng%20Li%20and%20Zhi-Qi%20Cheng%20and%20Yifei%20Dong%20and%20Yuxuan%20Zhou%20and%20Jun-Yan%20He%20and%20Qi%20Dai%20and%20Teruko%20Mitamura%20and%20Alexander%20G.%20Hauptmann%0AAbstract%3A%20%20%20Vision-and-Language%20Navigation%20%28VLN%29%20aims%20to%20develop%20embodied%20agents%20that%0Anavigate%20based%20on%20human%20instructions.%20However%2C%20current%20VLN%20frameworks%20often%0Arely%20on%20static%20environments%20and%20optimal%20expert%20supervision%2C%20limiting%20their%0Areal-world%20applicability.%20To%20address%20this%2C%20we%20introduce%20Human-Aware%0AVision-and-Language%20Navigation%20%28HA-VLN%29%2C%20extending%20traditional%20VLN%20by%0Aincorporating%20dynamic%20human%20activities%20and%20relaxing%20key%20assumptions.%20We%20propose%0Athe%20Human-Aware%203D%20%28HA3D%29%20simulator%2C%20which%20combines%20dynamic%20human%20activities%0Awith%20the%20Matterport3D%20dataset%2C%20and%20the%20Human-Aware%20Room-to-Room%20%28HA-R2R%29%0Adataset%2C%20extending%20R2R%20with%20human%20activity%20descriptions.%20To%20tackle%20HA-VLN%0Achallenges%2C%20we%20present%20the%20Expert-Supervised%20Cross-Modal%20%28VLN-CM%29%20and%0ANon-Expert-Supervised%20Decision%20Transformer%20%28VLN-DT%29%20agents%2C%20utilizing%0Across-modal%20fusion%20and%20diverse%20training%20strategies%20for%20effective%20navigation%20in%0Adynamic%20human%20environments.%20A%20comprehensive%20evaluation%2C%20including%20metrics%0Aconsidering%20human%20activities%2C%20and%20systematic%20analysis%20of%20HA-VLN%27s%20unique%0Achallenges%2C%20underscores%20the%20need%20for%20further%20research%20to%20enhance%20HA-VLN%20agents%27%0Areal-world%20robustness%20and%20adaptability.%20Ultimately%2C%20this%20work%20provides%0Abenchmarks%20and%20insights%20for%20future%20research%20on%20embodied%20AI%20and%20Sim2Real%0Atransfer%2C%20paving%20the%20way%20for%20more%20realistic%20and%20applicable%20VLN%20systems%20in%0Ahuman-populated%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Aware%2520Vision-and-Language%2520Navigation%253A%2520Bridging%2520Simulation%2520to%250A%2520%2520Reality%2520with%2520Dynamic%2520Human%2520Interactions%26entry.906535625%3DMinghan%2520Li%2520and%2520Heng%2520Li%2520and%2520Zhi-Qi%2520Cheng%2520and%2520Yifei%2520Dong%2520and%2520Yuxuan%2520Zhou%2520and%2520Jun-Yan%2520He%2520and%2520Qi%2520Dai%2520and%2520Teruko%2520Mitamura%2520and%2520Alexander%2520G.%2520Hauptmann%26entry.1292438233%3D%2520%2520Vision-and-Language%2520Navigation%2520%2528VLN%2529%2520aims%2520to%2520develop%2520embodied%2520agents%2520that%250Anavigate%2520based%2520on%2520human%2520instructions.%2520However%252C%2520current%2520VLN%2520frameworks%2520often%250Arely%2520on%2520static%2520environments%2520and%2520optimal%2520expert%2520supervision%252C%2520limiting%2520their%250Areal-world%2520applicability.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Human-Aware%250AVision-and-Language%2520Navigation%2520%2528HA-VLN%2529%252C%2520extending%2520traditional%2520VLN%2520by%250Aincorporating%2520dynamic%2520human%2520activities%2520and%2520relaxing%2520key%2520assumptions.%2520We%2520propose%250Athe%2520Human-Aware%25203D%2520%2528HA3D%2529%2520simulator%252C%2520which%2520combines%2520dynamic%2520human%2520activities%250Awith%2520the%2520Matterport3D%2520dataset%252C%2520and%2520the%2520Human-Aware%2520Room-to-Room%2520%2528HA-R2R%2529%250Adataset%252C%2520extending%2520R2R%2520with%2520human%2520activity%2520descriptions.%2520To%2520tackle%2520HA-VLN%250Achallenges%252C%2520we%2520present%2520the%2520Expert-Supervised%2520Cross-Modal%2520%2528VLN-CM%2529%2520and%250ANon-Expert-Supervised%2520Decision%2520Transformer%2520%2528VLN-DT%2529%2520agents%252C%2520utilizing%250Across-modal%2520fusion%2520and%2520diverse%2520training%2520strategies%2520for%2520effective%2520navigation%2520in%250Adynamic%2520human%2520environments.%2520A%2520comprehensive%2520evaluation%252C%2520including%2520metrics%250Aconsidering%2520human%2520activities%252C%2520and%2520systematic%2520analysis%2520of%2520HA-VLN%2527s%2520unique%250Achallenges%252C%2520underscores%2520the%2520need%2520for%2520further%2520research%2520to%2520enhance%2520HA-VLN%2520agents%2527%250Areal-world%2520robustness%2520and%2520adaptability.%2520Ultimately%252C%2520this%2520work%2520provides%250Abenchmarks%2520and%2520insights%2520for%2520future%2520research%2520on%2520embodied%2520AI%2520and%2520Sim2Real%250Atransfer%252C%2520paving%2520the%2520way%2520for%2520more%2520realistic%2520and%2520applicable%2520VLN%2520systems%2520in%250Ahuman-populated%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Aware%20Vision-and-Language%20Navigation%3A%20Bridging%20Simulation%20to%0A%20%20Reality%20with%20Dynamic%20Human%20Interactions&entry.906535625=Minghan%20Li%20and%20Heng%20Li%20and%20Zhi-Qi%20Cheng%20and%20Yifei%20Dong%20and%20Yuxuan%20Zhou%20and%20Jun-Yan%20He%20and%20Qi%20Dai%20and%20Teruko%20Mitamura%20and%20Alexander%20G.%20Hauptmann&entry.1292438233=%20%20Vision-and-Language%20Navigation%20%28VLN%29%20aims%20to%20develop%20embodied%20agents%20that%0Anavigate%20based%20on%20human%20instructions.%20However%2C%20current%20VLN%20frameworks%20often%0Arely%20on%20static%20environments%20and%20optimal%20expert%20supervision%2C%20limiting%20their%0Areal-world%20applicability.%20To%20address%20this%2C%20we%20introduce%20Human-Aware%0AVision-and-Language%20Navigation%20%28HA-VLN%29%2C%20extending%20traditional%20VLN%20by%0Aincorporating%20dynamic%20human%20activities%20and%20relaxing%20key%20assumptions.%20We%20propose%0Athe%20Human-Aware%203D%20%28HA3D%29%20simulator%2C%20which%20combines%20dynamic%20human%20activities%0Awith%20the%20Matterport3D%20dataset%2C%20and%20the%20Human-Aware%20Room-to-Room%20%28HA-R2R%29%0Adataset%2C%20extending%20R2R%20with%20human%20activity%20descriptions.%20To%20tackle%20HA-VLN%0Achallenges%2C%20we%20present%20the%20Expert-Supervised%20Cross-Modal%20%28VLN-CM%29%20and%0ANon-Expert-Supervised%20Decision%20Transformer%20%28VLN-DT%29%20agents%2C%20utilizing%0Across-modal%20fusion%20and%20diverse%20training%20strategies%20for%20effective%20navigation%20in%0Adynamic%20human%20environments.%20A%20comprehensive%20evaluation%2C%20including%20metrics%0Aconsidering%20human%20activities%2C%20and%20systematic%20analysis%20of%20HA-VLN%27s%20unique%0Achallenges%2C%20underscores%20the%20need%20for%20further%20research%20to%20enhance%20HA-VLN%20agents%27%0Areal-world%20robustness%20and%20adaptability.%20Ultimately%2C%20this%20work%20provides%0Abenchmarks%20and%20insights%20for%20future%20research%20on%20embodied%20AI%20and%20Sim2Real%0Atransfer%2C%20paving%20the%20way%20for%20more%20realistic%20and%20applicable%20VLN%20systems%20in%0Ahuman-populated%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19236v1&entry.124074799=Read"},
{"title": "HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale", "author": "Junying Chen and Ruyi Ouyang and Anningzhe Gao and Shunian Chen and Guiming Hardy Chen and Xidong Wang and Ruifei Zhang and Zhenyang Cai and Ke Ji and Guangjun Yu and Xiang Wan and Benyou Wang", "abstract": "  The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.\n", "link": "http://arxiv.org/abs/2406.19280v1", "date": "2024-06-27", "relevancy": 1.6697, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5882}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5545}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HuatuoGPT-Vision%2C%20Towards%20Injecting%20Medical%20Visual%20Knowledge%20into%0A%20%20Multimodal%20LLMs%20at%20Scale&body=Title%3A%20HuatuoGPT-Vision%2C%20Towards%20Injecting%20Medical%20Visual%20Knowledge%20into%0A%20%20Multimodal%20LLMs%20at%20Scale%0AAuthor%3A%20Junying%20Chen%20and%20Ruyi%20Ouyang%20and%20Anningzhe%20Gao%20and%20Shunian%20Chen%20and%20Guiming%20Hardy%20Chen%20and%20Xidong%20Wang%20and%20Ruifei%20Zhang%20and%20Zhenyang%20Cai%20and%20Ke%20Ji%20and%20Guangjun%20Yu%20and%20Xiang%20Wan%20and%20Benyou%20Wang%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20such%20as%0AGPT-4V%2C%20has%20led%20to%20significant%20advancements.%20However%2C%20these%20models%20still%20face%0Achallenges%20in%20medical%20multimodal%20capabilities%20due%20to%20limitations%20in%20the%0Aquantity%20and%20quality%20of%20medical%20vision-text%20data%2C%20stemming%20from%20data%20privacy%0Aconcerns%20and%20high%20annotation%20costs.%20While%20pioneering%20approaches%20utilize%0APubMed%27s%20large-scale%2C%20de-identified%20medical%20image-text%20pairs%20to%20address%20these%0Alimitations%2C%20they%20still%20fall%20short%20due%20to%20inherent%20data%20noise.%20To%20tackle%20this%2C%0Awe%20refined%20medical%20image-text%20pairs%20from%20PubMed%20and%20employed%20MLLMs%20%28GPT-4V%29%20in%0Aan%20%27unblinded%27%20capacity%20to%20denoise%20and%20reformat%20the%20data%2C%20resulting%20in%20the%0Acreation%20of%20the%20PubMedVision%20dataset%20with%201.3%20million%20medical%20VQA%20samples.%20Our%0Avalidation%20demonstrates%20that%3A%20%281%29%20PubMedVision%20can%20significantly%20enhance%20the%0Amedical%20multimodal%20capabilities%20of%20current%20MLLMs%2C%20showing%20significant%0Aimprovement%20in%20benchmarks%20including%20the%20MMMU%20Health%20%26%20Medicine%20track%3B%20%282%29%0Amanual%20checks%20by%20medical%20experts%20and%20empirical%20results%20validate%20the%20superior%0Adata%20quality%20of%20our%20dataset%20compared%20to%20other%20data%20construction%20methods.%20Using%0APubMedVision%2C%20we%20train%20a%2034B%20medical%20MLLM%20HuatuoGPT-Vision%2C%20which%20shows%0Asuperior%20performance%20in%20medical%20multimodal%20scenarios%20among%20open-source%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19280v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuatuoGPT-Vision%252C%2520Towards%2520Injecting%2520Medical%2520Visual%2520Knowledge%2520into%250A%2520%2520Multimodal%2520LLMs%2520at%2520Scale%26entry.906535625%3DJunying%2520Chen%2520and%2520Ruyi%2520Ouyang%2520and%2520Anningzhe%2520Gao%2520and%2520Shunian%2520Chen%2520and%2520Guiming%2520Hardy%2520Chen%2520and%2520Xidong%2520Wang%2520and%2520Ruifei%2520Zhang%2520and%2520Zhenyang%2520Cai%2520and%2520Ke%2520Ji%2520and%2520Guangjun%2520Yu%2520and%2520Xiang%2520Wan%2520and%2520Benyou%2520Wang%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520such%2520as%250AGPT-4V%252C%2520has%2520led%2520to%2520significant%2520advancements.%2520However%252C%2520these%2520models%2520still%2520face%250Achallenges%2520in%2520medical%2520multimodal%2520capabilities%2520due%2520to%2520limitations%2520in%2520the%250Aquantity%2520and%2520quality%2520of%2520medical%2520vision-text%2520data%252C%2520stemming%2520from%2520data%2520privacy%250Aconcerns%2520and%2520high%2520annotation%2520costs.%2520While%2520pioneering%2520approaches%2520utilize%250APubMed%2527s%2520large-scale%252C%2520de-identified%2520medical%2520image-text%2520pairs%2520to%2520address%2520these%250Alimitations%252C%2520they%2520still%2520fall%2520short%2520due%2520to%2520inherent%2520data%2520noise.%2520To%2520tackle%2520this%252C%250Awe%2520refined%2520medical%2520image-text%2520pairs%2520from%2520PubMed%2520and%2520employed%2520MLLMs%2520%2528GPT-4V%2529%2520in%250Aan%2520%2527unblinded%2527%2520capacity%2520to%2520denoise%2520and%2520reformat%2520the%2520data%252C%2520resulting%2520in%2520the%250Acreation%2520of%2520the%2520PubMedVision%2520dataset%2520with%25201.3%2520million%2520medical%2520VQA%2520samples.%2520Our%250Avalidation%2520demonstrates%2520that%253A%2520%25281%2529%2520PubMedVision%2520can%2520significantly%2520enhance%2520the%250Amedical%2520multimodal%2520capabilities%2520of%2520current%2520MLLMs%252C%2520showing%2520significant%250Aimprovement%2520in%2520benchmarks%2520including%2520the%2520MMMU%2520Health%2520%2526%2520Medicine%2520track%253B%2520%25282%2529%250Amanual%2520checks%2520by%2520medical%2520experts%2520and%2520empirical%2520results%2520validate%2520the%2520superior%250Adata%2520quality%2520of%2520our%2520dataset%2520compared%2520to%2520other%2520data%2520construction%2520methods.%2520Using%250APubMedVision%252C%2520we%2520train%2520a%252034B%2520medical%2520MLLM%2520HuatuoGPT-Vision%252C%2520which%2520shows%250Asuperior%2520performance%2520in%2520medical%2520multimodal%2520scenarios%2520among%2520open-source%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19280v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HuatuoGPT-Vision%2C%20Towards%20Injecting%20Medical%20Visual%20Knowledge%20into%0A%20%20Multimodal%20LLMs%20at%20Scale&entry.906535625=Junying%20Chen%20and%20Ruyi%20Ouyang%20and%20Anningzhe%20Gao%20and%20Shunian%20Chen%20and%20Guiming%20Hardy%20Chen%20and%20Xidong%20Wang%20and%20Ruifei%20Zhang%20and%20Zhenyang%20Cai%20and%20Ke%20Ji%20and%20Guangjun%20Yu%20and%20Xiang%20Wan%20and%20Benyou%20Wang&entry.1292438233=%20%20The%20rapid%20development%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20such%20as%0AGPT-4V%2C%20has%20led%20to%20significant%20advancements.%20However%2C%20these%20models%20still%20face%0Achallenges%20in%20medical%20multimodal%20capabilities%20due%20to%20limitations%20in%20the%0Aquantity%20and%20quality%20of%20medical%20vision-text%20data%2C%20stemming%20from%20data%20privacy%0Aconcerns%20and%20high%20annotation%20costs.%20While%20pioneering%20approaches%20utilize%0APubMed%27s%20large-scale%2C%20de-identified%20medical%20image-text%20pairs%20to%20address%20these%0Alimitations%2C%20they%20still%20fall%20short%20due%20to%20inherent%20data%20noise.%20To%20tackle%20this%2C%0Awe%20refined%20medical%20image-text%20pairs%20from%20PubMed%20and%20employed%20MLLMs%20%28GPT-4V%29%20in%0Aan%20%27unblinded%27%20capacity%20to%20denoise%20and%20reformat%20the%20data%2C%20resulting%20in%20the%0Acreation%20of%20the%20PubMedVision%20dataset%20with%201.3%20million%20medical%20VQA%20samples.%20Our%0Avalidation%20demonstrates%20that%3A%20%281%29%20PubMedVision%20can%20significantly%20enhance%20the%0Amedical%20multimodal%20capabilities%20of%20current%20MLLMs%2C%20showing%20significant%0Aimprovement%20in%20benchmarks%20including%20the%20MMMU%20Health%20%26%20Medicine%20track%3B%20%282%29%0Amanual%20checks%20by%20medical%20experts%20and%20empirical%20results%20validate%20the%20superior%0Adata%20quality%20of%20our%20dataset%20compared%20to%20other%20data%20construction%20methods.%20Using%0APubMedVision%2C%20we%20train%20a%2034B%20medical%20MLLM%20HuatuoGPT-Vision%2C%20which%20shows%0Asuperior%20performance%20in%20medical%20multimodal%20scenarios%20among%20open-source%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19280v1&entry.124074799=Read"},
{"title": "VLSM-Adapter: Finetuning Vision-Language Segmentation Efficiently with\n  Lightweight Blocks", "author": "Manish Dhakal and Rabin Adhikari and Safal Thapaliya and Bishesh Khanal", "abstract": "  Foundation Vision-Language Models (VLMs) trained using large-scale\nopen-domain images and text pairs have recently been adapted to develop\nVision-Language Segmentation Models (VLSMs) that allow providing text prompts\nduring inference to guide image segmentation. If robust and powerful VLSMs can\nbe built for medical images, it could aid medical professionals in many\nclinical tasks where they must spend substantial time delineating the target\nstructure of interest. VLSMs for medical images resort to fine-tuning base VLM\nor VLSM pretrained on open-domain natural image datasets due to fewer annotated\nmedical image datasets; this fine-tuning is resource-consuming and expensive as\nit usually requires updating all or a significant fraction of the pretrained\nparameters. Recently, lightweight blocks called adapters have been proposed in\nVLMs that keep the pretrained model frozen and only train adapters during\nfine-tuning, substantially reducing the computing resources required. We\nintroduce a novel adapter, VLSM-Adapter, that can fine-tune pretrained\nvision-language segmentation models using transformer encoders. Our experiments\nin widely used CLIP-based segmentation models show that with only 3 million\ntrainable parameters, the VLSM-Adapter outperforms state-of-the-art and is\ncomparable to the upper bound end-to-end fine-tuning. The source code is\navailable at: https://github.com/naamiinepal/vlsm-adapter.\n", "link": "http://arxiv.org/abs/2405.06196v2", "date": "2024-06-27", "relevancy": 1.6581, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6002}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4995}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLSM-Adapter%3A%20Finetuning%20Vision-Language%20Segmentation%20Efficiently%20with%0A%20%20Lightweight%20Blocks&body=Title%3A%20VLSM-Adapter%3A%20Finetuning%20Vision-Language%20Segmentation%20Efficiently%20with%0A%20%20Lightweight%20Blocks%0AAuthor%3A%20Manish%20Dhakal%20and%20Rabin%20Adhikari%20and%20Safal%20Thapaliya%20and%20Bishesh%20Khanal%0AAbstract%3A%20%20%20Foundation%20Vision-Language%20Models%20%28VLMs%29%20trained%20using%20large-scale%0Aopen-domain%20images%20and%20text%20pairs%20have%20recently%20been%20adapted%20to%20develop%0AVision-Language%20Segmentation%20Models%20%28VLSMs%29%20that%20allow%20providing%20text%20prompts%0Aduring%20inference%20to%20guide%20image%20segmentation.%20If%20robust%20and%20powerful%20VLSMs%20can%0Abe%20built%20for%20medical%20images%2C%20it%20could%20aid%20medical%20professionals%20in%20many%0Aclinical%20tasks%20where%20they%20must%20spend%20substantial%20time%20delineating%20the%20target%0Astructure%20of%20interest.%20VLSMs%20for%20medical%20images%20resort%20to%20fine-tuning%20base%20VLM%0Aor%20VLSM%20pretrained%20on%20open-domain%20natural%20image%20datasets%20due%20to%20fewer%20annotated%0Amedical%20image%20datasets%3B%20this%20fine-tuning%20is%20resource-consuming%20and%20expensive%20as%0Ait%20usually%20requires%20updating%20all%20or%20a%20significant%20fraction%20of%20the%20pretrained%0Aparameters.%20Recently%2C%20lightweight%20blocks%20called%20adapters%20have%20been%20proposed%20in%0AVLMs%20that%20keep%20the%20pretrained%20model%20frozen%20and%20only%20train%20adapters%20during%0Afine-tuning%2C%20substantially%20reducing%20the%20computing%20resources%20required.%20We%0Aintroduce%20a%20novel%20adapter%2C%20VLSM-Adapter%2C%20that%20can%20fine-tune%20pretrained%0Avision-language%20segmentation%20models%20using%20transformer%20encoders.%20Our%20experiments%0Ain%20widely%20used%20CLIP-based%20segmentation%20models%20show%20that%20with%20only%203%20million%0Atrainable%20parameters%2C%20the%20VLSM-Adapter%20outperforms%20state-of-the-art%20and%20is%0Acomparable%20to%20the%20upper%20bound%20end-to-end%20fine-tuning.%20The%20source%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/naamiinepal/vlsm-adapter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06196v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLSM-Adapter%253A%2520Finetuning%2520Vision-Language%2520Segmentation%2520Efficiently%2520with%250A%2520%2520Lightweight%2520Blocks%26entry.906535625%3DManish%2520Dhakal%2520and%2520Rabin%2520Adhikari%2520and%2520Safal%2520Thapaliya%2520and%2520Bishesh%2520Khanal%26entry.1292438233%3D%2520%2520Foundation%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520trained%2520using%2520large-scale%250Aopen-domain%2520images%2520and%2520text%2520pairs%2520have%2520recently%2520been%2520adapted%2520to%2520develop%250AVision-Language%2520Segmentation%2520Models%2520%2528VLSMs%2529%2520that%2520allow%2520providing%2520text%2520prompts%250Aduring%2520inference%2520to%2520guide%2520image%2520segmentation.%2520If%2520robust%2520and%2520powerful%2520VLSMs%2520can%250Abe%2520built%2520for%2520medical%2520images%252C%2520it%2520could%2520aid%2520medical%2520professionals%2520in%2520many%250Aclinical%2520tasks%2520where%2520they%2520must%2520spend%2520substantial%2520time%2520delineating%2520the%2520target%250Astructure%2520of%2520interest.%2520VLSMs%2520for%2520medical%2520images%2520resort%2520to%2520fine-tuning%2520base%2520VLM%250Aor%2520VLSM%2520pretrained%2520on%2520open-domain%2520natural%2520image%2520datasets%2520due%2520to%2520fewer%2520annotated%250Amedical%2520image%2520datasets%253B%2520this%2520fine-tuning%2520is%2520resource-consuming%2520and%2520expensive%2520as%250Ait%2520usually%2520requires%2520updating%2520all%2520or%2520a%2520significant%2520fraction%2520of%2520the%2520pretrained%250Aparameters.%2520Recently%252C%2520lightweight%2520blocks%2520called%2520adapters%2520have%2520been%2520proposed%2520in%250AVLMs%2520that%2520keep%2520the%2520pretrained%2520model%2520frozen%2520and%2520only%2520train%2520adapters%2520during%250Afine-tuning%252C%2520substantially%2520reducing%2520the%2520computing%2520resources%2520required.%2520We%250Aintroduce%2520a%2520novel%2520adapter%252C%2520VLSM-Adapter%252C%2520that%2520can%2520fine-tune%2520pretrained%250Avision-language%2520segmentation%2520models%2520using%2520transformer%2520encoders.%2520Our%2520experiments%250Ain%2520widely%2520used%2520CLIP-based%2520segmentation%2520models%2520show%2520that%2520with%2520only%25203%2520million%250Atrainable%2520parameters%252C%2520the%2520VLSM-Adapter%2520outperforms%2520state-of-the-art%2520and%2520is%250Acomparable%2520to%2520the%2520upper%2520bound%2520end-to-end%2520fine-tuning.%2520The%2520source%2520code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/naamiinepal/vlsm-adapter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06196v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLSM-Adapter%3A%20Finetuning%20Vision-Language%20Segmentation%20Efficiently%20with%0A%20%20Lightweight%20Blocks&entry.906535625=Manish%20Dhakal%20and%20Rabin%20Adhikari%20and%20Safal%20Thapaliya%20and%20Bishesh%20Khanal&entry.1292438233=%20%20Foundation%20Vision-Language%20Models%20%28VLMs%29%20trained%20using%20large-scale%0Aopen-domain%20images%20and%20text%20pairs%20have%20recently%20been%20adapted%20to%20develop%0AVision-Language%20Segmentation%20Models%20%28VLSMs%29%20that%20allow%20providing%20text%20prompts%0Aduring%20inference%20to%20guide%20image%20segmentation.%20If%20robust%20and%20powerful%20VLSMs%20can%0Abe%20built%20for%20medical%20images%2C%20it%20could%20aid%20medical%20professionals%20in%20many%0Aclinical%20tasks%20where%20they%20must%20spend%20substantial%20time%20delineating%20the%20target%0Astructure%20of%20interest.%20VLSMs%20for%20medical%20images%20resort%20to%20fine-tuning%20base%20VLM%0Aor%20VLSM%20pretrained%20on%20open-domain%20natural%20image%20datasets%20due%20to%20fewer%20annotated%0Amedical%20image%20datasets%3B%20this%20fine-tuning%20is%20resource-consuming%20and%20expensive%20as%0Ait%20usually%20requires%20updating%20all%20or%20a%20significant%20fraction%20of%20the%20pretrained%0Aparameters.%20Recently%2C%20lightweight%20blocks%20called%20adapters%20have%20been%20proposed%20in%0AVLMs%20that%20keep%20the%20pretrained%20model%20frozen%20and%20only%20train%20adapters%20during%0Afine-tuning%2C%20substantially%20reducing%20the%20computing%20resources%20required.%20We%0Aintroduce%20a%20novel%20adapter%2C%20VLSM-Adapter%2C%20that%20can%20fine-tune%20pretrained%0Avision-language%20segmentation%20models%20using%20transformer%20encoders.%20Our%20experiments%0Ain%20widely%20used%20CLIP-based%20segmentation%20models%20show%20that%20with%20only%203%20million%0Atrainable%20parameters%2C%20the%20VLSM-Adapter%20outperforms%20state-of-the-art%20and%20is%0Acomparable%20to%20the%20upper%20bound%20end-to-end%20fine-tuning.%20The%20source%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/naamiinepal/vlsm-adapter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06196v2&entry.124074799=Read"},
{"title": "DocKylin: A Large Multimodal Model for Visual Document Understanding\n  with Efficient Visual Slimming", "author": "Jiaxin Zhang and Wentao Yang and Songxuan Lai and Zecheng Xie and Lianwen Jin", "abstract": "  Current multimodal large language models (MLLMs) face significant challenges\nin visual document understanding (VDU) tasks due to the high resolution, dense\ntext, and complex layouts typical of document images. These characteristics\ndemand a high level of detail perception ability from MLLMs. While increasing\ninput resolution improves detail perception, it also leads to longer sequences\nof visual tokens, increasing computational costs and straining the models'\nability to handle long contexts. To address these challenges, we introduce\nDocKylin, a document-centric MLLM that performs visual content slimming at both\nthe pixel and token levels, thereby reducing token sequence length in VDU\nscenarios. DocKylin utilizes an Adaptive Pixel Slimming (APS) preprocessing\nmodule to perform pixel-level slimming, increasing the proportion of\ninformative pixels. Moreover, DocKylin incorporates a novel Dynamic Token\nSlimming (DTS) module to conduct token-level slimming, filtering essential\ntokens and removing others to create a compressed, adaptive visual sequence.\nExperiments demonstrate DocKylin's promising performance across various VDU\nbenchmarks. Notably, both the proposed APS and DTS are parameter-free,\nfacilitating easy integration into existing MLLMs, and our experiments indicate\ntheir potential for broader applications.\n", "link": "http://arxiv.org/abs/2406.19101v1", "date": "2024-06-27", "relevancy": 1.6532, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5541}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5524}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DocKylin%3A%20A%20Large%20Multimodal%20Model%20for%20Visual%20Document%20Understanding%0A%20%20with%20Efficient%20Visual%20Slimming&body=Title%3A%20DocKylin%3A%20A%20Large%20Multimodal%20Model%20for%20Visual%20Document%20Understanding%0A%20%20with%20Efficient%20Visual%20Slimming%0AAuthor%3A%20Jiaxin%20Zhang%20and%20Wentao%20Yang%20and%20Songxuan%20Lai%20and%20Zecheng%20Xie%20and%20Lianwen%20Jin%0AAbstract%3A%20%20%20Current%20multimodal%20large%20language%20models%20%28MLLMs%29%20face%20significant%20challenges%0Ain%20visual%20document%20understanding%20%28VDU%29%20tasks%20due%20to%20the%20high%20resolution%2C%20dense%0Atext%2C%20and%20complex%20layouts%20typical%20of%20document%20images.%20These%20characteristics%0Ademand%20a%20high%20level%20of%20detail%20perception%20ability%20from%20MLLMs.%20While%20increasing%0Ainput%20resolution%20improves%20detail%20perception%2C%20it%20also%20leads%20to%20longer%20sequences%0Aof%20visual%20tokens%2C%20increasing%20computational%20costs%20and%20straining%20the%20models%27%0Aability%20to%20handle%20long%20contexts.%20To%20address%20these%20challenges%2C%20we%20introduce%0ADocKylin%2C%20a%20document-centric%20MLLM%20that%20performs%20visual%20content%20slimming%20at%20both%0Athe%20pixel%20and%20token%20levels%2C%20thereby%20reducing%20token%20sequence%20length%20in%20VDU%0Ascenarios.%20DocKylin%20utilizes%20an%20Adaptive%20Pixel%20Slimming%20%28APS%29%20preprocessing%0Amodule%20to%20perform%20pixel-level%20slimming%2C%20increasing%20the%20proportion%20of%0Ainformative%20pixels.%20Moreover%2C%20DocKylin%20incorporates%20a%20novel%20Dynamic%20Token%0ASlimming%20%28DTS%29%20module%20to%20conduct%20token-level%20slimming%2C%20filtering%20essential%0Atokens%20and%20removing%20others%20to%20create%20a%20compressed%2C%20adaptive%20visual%20sequence.%0AExperiments%20demonstrate%20DocKylin%27s%20promising%20performance%20across%20various%20VDU%0Abenchmarks.%20Notably%2C%20both%20the%20proposed%20APS%20and%20DTS%20are%20parameter-free%2C%0Afacilitating%20easy%20integration%20into%20existing%20MLLMs%2C%20and%20our%20experiments%20indicate%0Atheir%20potential%20for%20broader%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocKylin%253A%2520A%2520Large%2520Multimodal%2520Model%2520for%2520Visual%2520Document%2520Understanding%250A%2520%2520with%2520Efficient%2520Visual%2520Slimming%26entry.906535625%3DJiaxin%2520Zhang%2520and%2520Wentao%2520Yang%2520and%2520Songxuan%2520Lai%2520and%2520Zecheng%2520Xie%2520and%2520Lianwen%2520Jin%26entry.1292438233%3D%2520%2520Current%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520face%2520significant%2520challenges%250Ain%2520visual%2520document%2520understanding%2520%2528VDU%2529%2520tasks%2520due%2520to%2520the%2520high%2520resolution%252C%2520dense%250Atext%252C%2520and%2520complex%2520layouts%2520typical%2520of%2520document%2520images.%2520These%2520characteristics%250Ademand%2520a%2520high%2520level%2520of%2520detail%2520perception%2520ability%2520from%2520MLLMs.%2520While%2520increasing%250Ainput%2520resolution%2520improves%2520detail%2520perception%252C%2520it%2520also%2520leads%2520to%2520longer%2520sequences%250Aof%2520visual%2520tokens%252C%2520increasing%2520computational%2520costs%2520and%2520straining%2520the%2520models%2527%250Aability%2520to%2520handle%2520long%2520contexts.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250ADocKylin%252C%2520a%2520document-centric%2520MLLM%2520that%2520performs%2520visual%2520content%2520slimming%2520at%2520both%250Athe%2520pixel%2520and%2520token%2520levels%252C%2520thereby%2520reducing%2520token%2520sequence%2520length%2520in%2520VDU%250Ascenarios.%2520DocKylin%2520utilizes%2520an%2520Adaptive%2520Pixel%2520Slimming%2520%2528APS%2529%2520preprocessing%250Amodule%2520to%2520perform%2520pixel-level%2520slimming%252C%2520increasing%2520the%2520proportion%2520of%250Ainformative%2520pixels.%2520Moreover%252C%2520DocKylin%2520incorporates%2520a%2520novel%2520Dynamic%2520Token%250ASlimming%2520%2528DTS%2529%2520module%2520to%2520conduct%2520token-level%2520slimming%252C%2520filtering%2520essential%250Atokens%2520and%2520removing%2520others%2520to%2520create%2520a%2520compressed%252C%2520adaptive%2520visual%2520sequence.%250AExperiments%2520demonstrate%2520DocKylin%2527s%2520promising%2520performance%2520across%2520various%2520VDU%250Abenchmarks.%2520Notably%252C%2520both%2520the%2520proposed%2520APS%2520and%2520DTS%2520are%2520parameter-free%252C%250Afacilitating%2520easy%2520integration%2520into%2520existing%2520MLLMs%252C%2520and%2520our%2520experiments%2520indicate%250Atheir%2520potential%2520for%2520broader%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DocKylin%3A%20A%20Large%20Multimodal%20Model%20for%20Visual%20Document%20Understanding%0A%20%20with%20Efficient%20Visual%20Slimming&entry.906535625=Jiaxin%20Zhang%20and%20Wentao%20Yang%20and%20Songxuan%20Lai%20and%20Zecheng%20Xie%20and%20Lianwen%20Jin&entry.1292438233=%20%20Current%20multimodal%20large%20language%20models%20%28MLLMs%29%20face%20significant%20challenges%0Ain%20visual%20document%20understanding%20%28VDU%29%20tasks%20due%20to%20the%20high%20resolution%2C%20dense%0Atext%2C%20and%20complex%20layouts%20typical%20of%20document%20images.%20These%20characteristics%0Ademand%20a%20high%20level%20of%20detail%20perception%20ability%20from%20MLLMs.%20While%20increasing%0Ainput%20resolution%20improves%20detail%20perception%2C%20it%20also%20leads%20to%20longer%20sequences%0Aof%20visual%20tokens%2C%20increasing%20computational%20costs%20and%20straining%20the%20models%27%0Aability%20to%20handle%20long%20contexts.%20To%20address%20these%20challenges%2C%20we%20introduce%0ADocKylin%2C%20a%20document-centric%20MLLM%20that%20performs%20visual%20content%20slimming%20at%20both%0Athe%20pixel%20and%20token%20levels%2C%20thereby%20reducing%20token%20sequence%20length%20in%20VDU%0Ascenarios.%20DocKylin%20utilizes%20an%20Adaptive%20Pixel%20Slimming%20%28APS%29%20preprocessing%0Amodule%20to%20perform%20pixel-level%20slimming%2C%20increasing%20the%20proportion%20of%0Ainformative%20pixels.%20Moreover%2C%20DocKylin%20incorporates%20a%20novel%20Dynamic%20Token%0ASlimming%20%28DTS%29%20module%20to%20conduct%20token-level%20slimming%2C%20filtering%20essential%0Atokens%20and%20removing%20others%20to%20create%20a%20compressed%2C%20adaptive%20visual%20sequence.%0AExperiments%20demonstrate%20DocKylin%27s%20promising%20performance%20across%20various%20VDU%0Abenchmarks.%20Notably%2C%20both%20the%20proposed%20APS%20and%20DTS%20are%20parameter-free%2C%0Afacilitating%20easy%20integration%20into%20existing%20MLLMs%2C%20and%20our%20experiments%20indicate%0Atheir%20potential%20for%20broader%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19101v1&entry.124074799=Read"},
{"title": "Towards Learning Abductive Reasoning using VSA Distributed\n  Representations", "author": "Giacomo Camposampiero and Michael Hersche and Aleksandar Terzi\u0107 and Roger Wattenhofer and Abu Sebastian and Abbas Rahimi", "abstract": "  We introduce the Abductive Rule Learner with Context-awareness (ARLC), a\nmodel that solves abstract reasoning tasks based on Learn-VRF. ARLC features a\nnovel and more broadly applicable training objective for abductive reasoning,\nresulting in better interpretability and higher accuracy when solving Raven's\nprogressive matrices (RPM). ARLC allows both programming domain knowledge and\nlearning the rules underlying a data distribution. We evaluate ARLC on the\nI-RAVEN dataset, showcasing state-of-the-art accuracy across both\nin-distribution and out-of-distribution (unseen attribute-rule pairs) tests.\nARLC surpasses neuro-symbolic and connectionist baselines, including large\nlanguage models, despite having orders of magnitude fewer parameters. We show\nARLC's robustness to post-programming training by incrementally learning from\nexamples on top of programmed knowledge, which only improves its performance\nand does not result in catastrophic forgetting of the programmed solution. We\nvalidate ARLC's seamless transfer learning from a 2x2 RPM constellation to\nunseen constellations. Our code is available at\nhttps://github.com/IBM/abductive-rule-learner-with-context-awareness.\n", "link": "http://arxiv.org/abs/2406.19121v1", "date": "2024-06-27", "relevancy": 1.4846, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5176}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.501}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Learning%20Abductive%20Reasoning%20using%20VSA%20Distributed%0A%20%20Representations&body=Title%3A%20Towards%20Learning%20Abductive%20Reasoning%20using%20VSA%20Distributed%0A%20%20Representations%0AAuthor%3A%20Giacomo%20Camposampiero%20and%20Michael%20Hersche%20and%20Aleksandar%20Terzi%C4%87%20and%20Roger%20Wattenhofer%20and%20Abu%20Sebastian%20and%20Abbas%20Rahimi%0AAbstract%3A%20%20%20We%20introduce%20the%20Abductive%20Rule%20Learner%20with%20Context-awareness%20%28ARLC%29%2C%20a%0Amodel%20that%20solves%20abstract%20reasoning%20tasks%20based%20on%20Learn-VRF.%20ARLC%20features%20a%0Anovel%20and%20more%20broadly%20applicable%20training%20objective%20for%20abductive%20reasoning%2C%0Aresulting%20in%20better%20interpretability%20and%20higher%20accuracy%20when%20solving%20Raven%27s%0Aprogressive%20matrices%20%28RPM%29.%20ARLC%20allows%20both%20programming%20domain%20knowledge%20and%0Alearning%20the%20rules%20underlying%20a%20data%20distribution.%20We%20evaluate%20ARLC%20on%20the%0AI-RAVEN%20dataset%2C%20showcasing%20state-of-the-art%20accuracy%20across%20both%0Ain-distribution%20and%20out-of-distribution%20%28unseen%20attribute-rule%20pairs%29%20tests.%0AARLC%20surpasses%20neuro-symbolic%20and%20connectionist%20baselines%2C%20including%20large%0Alanguage%20models%2C%20despite%20having%20orders%20of%20magnitude%20fewer%20parameters.%20We%20show%0AARLC%27s%20robustness%20to%20post-programming%20training%20by%20incrementally%20learning%20from%0Aexamples%20on%20top%20of%20programmed%20knowledge%2C%20which%20only%20improves%20its%20performance%0Aand%20does%20not%20result%20in%20catastrophic%20forgetting%20of%20the%20programmed%20solution.%20We%0Avalidate%20ARLC%27s%20seamless%20transfer%20learning%20from%20a%202x2%20RPM%20constellation%20to%0Aunseen%20constellations.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/IBM/abductive-rule-learner-with-context-awareness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19121v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Learning%2520Abductive%2520Reasoning%2520using%2520VSA%2520Distributed%250A%2520%2520Representations%26entry.906535625%3DGiacomo%2520Camposampiero%2520and%2520Michael%2520Hersche%2520and%2520Aleksandar%2520Terzi%25C4%2587%2520and%2520Roger%2520Wattenhofer%2520and%2520Abu%2520Sebastian%2520and%2520Abbas%2520Rahimi%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Abductive%2520Rule%2520Learner%2520with%2520Context-awareness%2520%2528ARLC%2529%252C%2520a%250Amodel%2520that%2520solves%2520abstract%2520reasoning%2520tasks%2520based%2520on%2520Learn-VRF.%2520ARLC%2520features%2520a%250Anovel%2520and%2520more%2520broadly%2520applicable%2520training%2520objective%2520for%2520abductive%2520reasoning%252C%250Aresulting%2520in%2520better%2520interpretability%2520and%2520higher%2520accuracy%2520when%2520solving%2520Raven%2527s%250Aprogressive%2520matrices%2520%2528RPM%2529.%2520ARLC%2520allows%2520both%2520programming%2520domain%2520knowledge%2520and%250Alearning%2520the%2520rules%2520underlying%2520a%2520data%2520distribution.%2520We%2520evaluate%2520ARLC%2520on%2520the%250AI-RAVEN%2520dataset%252C%2520showcasing%2520state-of-the-art%2520accuracy%2520across%2520both%250Ain-distribution%2520and%2520out-of-distribution%2520%2528unseen%2520attribute-rule%2520pairs%2529%2520tests.%250AARLC%2520surpasses%2520neuro-symbolic%2520and%2520connectionist%2520baselines%252C%2520including%2520large%250Alanguage%2520models%252C%2520despite%2520having%2520orders%2520of%2520magnitude%2520fewer%2520parameters.%2520We%2520show%250AARLC%2527s%2520robustness%2520to%2520post-programming%2520training%2520by%2520incrementally%2520learning%2520from%250Aexamples%2520on%2520top%2520of%2520programmed%2520knowledge%252C%2520which%2520only%2520improves%2520its%2520performance%250Aand%2520does%2520not%2520result%2520in%2520catastrophic%2520forgetting%2520of%2520the%2520programmed%2520solution.%2520We%250Avalidate%2520ARLC%2527s%2520seamless%2520transfer%2520learning%2520from%2520a%25202x2%2520RPM%2520constellation%2520to%250Aunseen%2520constellations.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/IBM/abductive-rule-learner-with-context-awareness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19121v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Learning%20Abductive%20Reasoning%20using%20VSA%20Distributed%0A%20%20Representations&entry.906535625=Giacomo%20Camposampiero%20and%20Michael%20Hersche%20and%20Aleksandar%20Terzi%C4%87%20and%20Roger%20Wattenhofer%20and%20Abu%20Sebastian%20and%20Abbas%20Rahimi&entry.1292438233=%20%20We%20introduce%20the%20Abductive%20Rule%20Learner%20with%20Context-awareness%20%28ARLC%29%2C%20a%0Amodel%20that%20solves%20abstract%20reasoning%20tasks%20based%20on%20Learn-VRF.%20ARLC%20features%20a%0Anovel%20and%20more%20broadly%20applicable%20training%20objective%20for%20abductive%20reasoning%2C%0Aresulting%20in%20better%20interpretability%20and%20higher%20accuracy%20when%20solving%20Raven%27s%0Aprogressive%20matrices%20%28RPM%29.%20ARLC%20allows%20both%20programming%20domain%20knowledge%20and%0Alearning%20the%20rules%20underlying%20a%20data%20distribution.%20We%20evaluate%20ARLC%20on%20the%0AI-RAVEN%20dataset%2C%20showcasing%20state-of-the-art%20accuracy%20across%20both%0Ain-distribution%20and%20out-of-distribution%20%28unseen%20attribute-rule%20pairs%29%20tests.%0AARLC%20surpasses%20neuro-symbolic%20and%20connectionist%20baselines%2C%20including%20large%0Alanguage%20models%2C%20despite%20having%20orders%20of%20magnitude%20fewer%20parameters.%20We%20show%0AARLC%27s%20robustness%20to%20post-programming%20training%20by%20incrementally%20learning%20from%0Aexamples%20on%20top%20of%20programmed%20knowledge%2C%20which%20only%20improves%20its%20performance%0Aand%20does%20not%20result%20in%20catastrophic%20forgetting%20of%20the%20programmed%20solution.%20We%0Avalidate%20ARLC%27s%20seamless%20transfer%20learning%20from%20a%202x2%20RPM%20constellation%20to%0Aunseen%20constellations.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/IBM/abductive-rule-learner-with-context-awareness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19121v1&entry.124074799=Read"},
{"title": "AI Data Readiness Inspector (AIDRIN) for Quantitative Assessment of Data\n  Readiness for AI", "author": "Kaveen Hiniduma and Suren Byna and Jean Luca Bez and Ravi Madduri", "abstract": "  \"Garbage In Garbage Out\" is a universally agreed quote by computer scientists\nfrom various domains, including Artificial Intelligence (AI). As data is the\nfuel for AI, models trained on low-quality, biased data are often ineffective.\nComputer scientists who use AI invest a considerable amount of time and effort\nin preparing the data for AI. However, there are no standard methods or\nframeworks for assessing the \"readiness\" of data for AI. To provide a\nquantifiable assessment of the readiness of data for AI processes, we define\nparameters of AI data readiness and introduce AIDRIN (AI Data Readiness\nInspector). AIDRIN is a framework covering a broad range of readiness\ndimensions available in the literature that aid in evaluating the readiness of\ndata quantitatively and qualitatively. AIDRIN uses metrics in traditional data\nquality assessment such as completeness, outliers, and duplicates for data\nevaluation. Furthermore, AIDRIN uses metrics specific to assess data for AI,\nsuch as feature importance, feature correlations, class imbalance, fairness,\nprivacy, and FAIR (Findability, Accessibility, Interoperability, and\nReusability) principle compliance. AIDRIN provides visualizations and reports\nto assist data scientists in further investigating the readiness of data. The\nAIDRIN framework enhances the efficiency of the machine learning pipeline to\nmake informed decisions on data readiness for AI applications.\n", "link": "http://arxiv.org/abs/2406.19256v1", "date": "2024-06-27", "relevancy": 1.5332, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4032}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.382}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20Data%20Readiness%20Inspector%20%28AIDRIN%29%20for%20Quantitative%20Assessment%20of%20Data%0A%20%20Readiness%20for%20AI&body=Title%3A%20AI%20Data%20Readiness%20Inspector%20%28AIDRIN%29%20for%20Quantitative%20Assessment%20of%20Data%0A%20%20Readiness%20for%20AI%0AAuthor%3A%20Kaveen%20Hiniduma%20and%20Suren%20Byna%20and%20Jean%20Luca%20Bez%20and%20Ravi%20Madduri%0AAbstract%3A%20%20%20%22Garbage%20In%20Garbage%20Out%22%20is%20a%20universally%20agreed%20quote%20by%20computer%20scientists%0Afrom%20various%20domains%2C%20including%20Artificial%20Intelligence%20%28AI%29.%20As%20data%20is%20the%0Afuel%20for%20AI%2C%20models%20trained%20on%20low-quality%2C%20biased%20data%20are%20often%20ineffective.%0AComputer%20scientists%20who%20use%20AI%20invest%20a%20considerable%20amount%20of%20time%20and%20effort%0Ain%20preparing%20the%20data%20for%20AI.%20However%2C%20there%20are%20no%20standard%20methods%20or%0Aframeworks%20for%20assessing%20the%20%22readiness%22%20of%20data%20for%20AI.%20To%20provide%20a%0Aquantifiable%20assessment%20of%20the%20readiness%20of%20data%20for%20AI%20processes%2C%20we%20define%0Aparameters%20of%20AI%20data%20readiness%20and%20introduce%20AIDRIN%20%28AI%20Data%20Readiness%0AInspector%29.%20AIDRIN%20is%20a%20framework%20covering%20a%20broad%20range%20of%20readiness%0Adimensions%20available%20in%20the%20literature%20that%20aid%20in%20evaluating%20the%20readiness%20of%0Adata%20quantitatively%20and%20qualitatively.%20AIDRIN%20uses%20metrics%20in%20traditional%20data%0Aquality%20assessment%20such%20as%20completeness%2C%20outliers%2C%20and%20duplicates%20for%20data%0Aevaluation.%20Furthermore%2C%20AIDRIN%20uses%20metrics%20specific%20to%20assess%20data%20for%20AI%2C%0Asuch%20as%20feature%20importance%2C%20feature%20correlations%2C%20class%20imbalance%2C%20fairness%2C%0Aprivacy%2C%20and%20FAIR%20%28Findability%2C%20Accessibility%2C%20Interoperability%2C%20and%0AReusability%29%20principle%20compliance.%20AIDRIN%20provides%20visualizations%20and%20reports%0Ato%20assist%20data%20scientists%20in%20further%20investigating%20the%20readiness%20of%20data.%20The%0AAIDRIN%20framework%20enhances%20the%20efficiency%20of%20the%20machine%20learning%20pipeline%20to%0Amake%20informed%20decisions%20on%20data%20readiness%20for%20AI%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19256v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520Data%2520Readiness%2520Inspector%2520%2528AIDRIN%2529%2520for%2520Quantitative%2520Assessment%2520of%2520Data%250A%2520%2520Readiness%2520for%2520AI%26entry.906535625%3DKaveen%2520Hiniduma%2520and%2520Suren%2520Byna%2520and%2520Jean%2520Luca%2520Bez%2520and%2520Ravi%2520Madduri%26entry.1292438233%3D%2520%2520%2522Garbage%2520In%2520Garbage%2520Out%2522%2520is%2520a%2520universally%2520agreed%2520quote%2520by%2520computer%2520scientists%250Afrom%2520various%2520domains%252C%2520including%2520Artificial%2520Intelligence%2520%2528AI%2529.%2520As%2520data%2520is%2520the%250Afuel%2520for%2520AI%252C%2520models%2520trained%2520on%2520low-quality%252C%2520biased%2520data%2520are%2520often%2520ineffective.%250AComputer%2520scientists%2520who%2520use%2520AI%2520invest%2520a%2520considerable%2520amount%2520of%2520time%2520and%2520effort%250Ain%2520preparing%2520the%2520data%2520for%2520AI.%2520However%252C%2520there%2520are%2520no%2520standard%2520methods%2520or%250Aframeworks%2520for%2520assessing%2520the%2520%2522readiness%2522%2520of%2520data%2520for%2520AI.%2520To%2520provide%2520a%250Aquantifiable%2520assessment%2520of%2520the%2520readiness%2520of%2520data%2520for%2520AI%2520processes%252C%2520we%2520define%250Aparameters%2520of%2520AI%2520data%2520readiness%2520and%2520introduce%2520AIDRIN%2520%2528AI%2520Data%2520Readiness%250AInspector%2529.%2520AIDRIN%2520is%2520a%2520framework%2520covering%2520a%2520broad%2520range%2520of%2520readiness%250Adimensions%2520available%2520in%2520the%2520literature%2520that%2520aid%2520in%2520evaluating%2520the%2520readiness%2520of%250Adata%2520quantitatively%2520and%2520qualitatively.%2520AIDRIN%2520uses%2520metrics%2520in%2520traditional%2520data%250Aquality%2520assessment%2520such%2520as%2520completeness%252C%2520outliers%252C%2520and%2520duplicates%2520for%2520data%250Aevaluation.%2520Furthermore%252C%2520AIDRIN%2520uses%2520metrics%2520specific%2520to%2520assess%2520data%2520for%2520AI%252C%250Asuch%2520as%2520feature%2520importance%252C%2520feature%2520correlations%252C%2520class%2520imbalance%252C%2520fairness%252C%250Aprivacy%252C%2520and%2520FAIR%2520%2528Findability%252C%2520Accessibility%252C%2520Interoperability%252C%2520and%250AReusability%2529%2520principle%2520compliance.%2520AIDRIN%2520provides%2520visualizations%2520and%2520reports%250Ato%2520assist%2520data%2520scientists%2520in%2520further%2520investigating%2520the%2520readiness%2520of%2520data.%2520The%250AAIDRIN%2520framework%2520enhances%2520the%2520efficiency%2520of%2520the%2520machine%2520learning%2520pipeline%2520to%250Amake%2520informed%2520decisions%2520on%2520data%2520readiness%2520for%2520AI%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19256v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20Data%20Readiness%20Inspector%20%28AIDRIN%29%20for%20Quantitative%20Assessment%20of%20Data%0A%20%20Readiness%20for%20AI&entry.906535625=Kaveen%20Hiniduma%20and%20Suren%20Byna%20and%20Jean%20Luca%20Bez%20and%20Ravi%20Madduri&entry.1292438233=%20%20%22Garbage%20In%20Garbage%20Out%22%20is%20a%20universally%20agreed%20quote%20by%20computer%20scientists%0Afrom%20various%20domains%2C%20including%20Artificial%20Intelligence%20%28AI%29.%20As%20data%20is%20the%0Afuel%20for%20AI%2C%20models%20trained%20on%20low-quality%2C%20biased%20data%20are%20often%20ineffective.%0AComputer%20scientists%20who%20use%20AI%20invest%20a%20considerable%20amount%20of%20time%20and%20effort%0Ain%20preparing%20the%20data%20for%20AI.%20However%2C%20there%20are%20no%20standard%20methods%20or%0Aframeworks%20for%20assessing%20the%20%22readiness%22%20of%20data%20for%20AI.%20To%20provide%20a%0Aquantifiable%20assessment%20of%20the%20readiness%20of%20data%20for%20AI%20processes%2C%20we%20define%0Aparameters%20of%20AI%20data%20readiness%20and%20introduce%20AIDRIN%20%28AI%20Data%20Readiness%0AInspector%29.%20AIDRIN%20is%20a%20framework%20covering%20a%20broad%20range%20of%20readiness%0Adimensions%20available%20in%20the%20literature%20that%20aid%20in%20evaluating%20the%20readiness%20of%0Adata%20quantitatively%20and%20qualitatively.%20AIDRIN%20uses%20metrics%20in%20traditional%20data%0Aquality%20assessment%20such%20as%20completeness%2C%20outliers%2C%20and%20duplicates%20for%20data%0Aevaluation.%20Furthermore%2C%20AIDRIN%20uses%20metrics%20specific%20to%20assess%20data%20for%20AI%2C%0Asuch%20as%20feature%20importance%2C%20feature%20correlations%2C%20class%20imbalance%2C%20fairness%2C%0Aprivacy%2C%20and%20FAIR%20%28Findability%2C%20Accessibility%2C%20Interoperability%2C%20and%0AReusability%29%20principle%20compliance.%20AIDRIN%20provides%20visualizations%20and%20reports%0Ato%20assist%20data%20scientists%20in%20further%20investigating%20the%20readiness%20of%20data.%20The%0AAIDRIN%20framework%20enhances%20the%20efficiency%20of%20the%20machine%20learning%20pipeline%20to%0Amake%20informed%20decisions%20on%20data%20readiness%20for%20AI%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19256v1&entry.124074799=Read"},
{"title": "MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution", "author": "Wei Tao and Yucheng Zhou and Yanlin Wang and Wenqiang Zhang and Hongyu Zhang and Yu Cheng", "abstract": "  In software development, resolving the emergent issues within GitHub\nrepositories is a complex challenge that involves not only the incorporation of\nnew code but also the maintenance of existing code. Large Language Models\n(LLMs) have shown promise in code generation but face difficulties in resolving\nGithub issues, particularly at the repository level. To overcome this\nchallenge, we empirically study the reason why LLMs fail to resolve GitHub\nissues and analyze the major factors. Motivated by the empirical findings, we\npropose a novel LLM-based Multi-Agent framework for GitHub Issue reSolution,\nMAGIS, consisting of four agents customized for software evolution: Manager,\nRepository Custodian, Developer, and Quality Assurance Engineer agents. This\nframework leverages the collaboration of various agents in the planning and\ncoding process to unlock the potential of LLMs to resolve GitHub issues. In\nexperiments, we employ the SWE-bench benchmark to compare MAGIS with popular\nLLMs, including GPT-3.5, GPT-4, and Claude-2. MAGIS can resolve 13.94% GitHub\nissues, significantly outperforming the baselines. Specifically, MAGIS achieves\nan eight-fold increase in resolved ratio over the direct application of GPT-4,\nthe advanced LLM.\n", "link": "http://arxiv.org/abs/2403.17927v2", "date": "2024-06-27", "relevancy": 1.4181, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4752}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4715}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAGIS%3A%20LLM-Based%20Multi-Agent%20Framework%20for%20GitHub%20Issue%20Resolution&body=Title%3A%20MAGIS%3A%20LLM-Based%20Multi-Agent%20Framework%20for%20GitHub%20Issue%20Resolution%0AAuthor%3A%20Wei%20Tao%20and%20Yucheng%20Zhou%20and%20Yanlin%20Wang%20and%20Wenqiang%20Zhang%20and%20Hongyu%20Zhang%20and%20Yu%20Cheng%0AAbstract%3A%20%20%20In%20software%20development%2C%20resolving%20the%20emergent%20issues%20within%20GitHub%0Arepositories%20is%20a%20complex%20challenge%20that%20involves%20not%20only%20the%20incorporation%20of%0Anew%20code%20but%20also%20the%20maintenance%20of%20existing%20code.%20Large%20Language%20Models%0A%28LLMs%29%20have%20shown%20promise%20in%20code%20generation%20but%20face%20difficulties%20in%20resolving%0AGithub%20issues%2C%20particularly%20at%20the%20repository%20level.%20To%20overcome%20this%0Achallenge%2C%20we%20empirically%20study%20the%20reason%20why%20LLMs%20fail%20to%20resolve%20GitHub%0Aissues%20and%20analyze%20the%20major%20factors.%20Motivated%20by%20the%20empirical%20findings%2C%20we%0Apropose%20a%20novel%20LLM-based%20Multi-Agent%20framework%20for%20GitHub%20Issue%20reSolution%2C%0AMAGIS%2C%20consisting%20of%20four%20agents%20customized%20for%20software%20evolution%3A%20Manager%2C%0ARepository%20Custodian%2C%20Developer%2C%20and%20Quality%20Assurance%20Engineer%20agents.%20This%0Aframework%20leverages%20the%20collaboration%20of%20various%20agents%20in%20the%20planning%20and%0Acoding%20process%20to%20unlock%20the%20potential%20of%20LLMs%20to%20resolve%20GitHub%20issues.%20In%0Aexperiments%2C%20we%20employ%20the%20SWE-bench%20benchmark%20to%20compare%20MAGIS%20with%20popular%0ALLMs%2C%20including%20GPT-3.5%2C%20GPT-4%2C%20and%20Claude-2.%20MAGIS%20can%20resolve%2013.94%25%20GitHub%0Aissues%2C%20significantly%20outperforming%20the%20baselines.%20Specifically%2C%20MAGIS%20achieves%0Aan%20eight-fold%20increase%20in%20resolved%20ratio%20over%20the%20direct%20application%20of%20GPT-4%2C%0Athe%20advanced%20LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17927v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAGIS%253A%2520LLM-Based%2520Multi-Agent%2520Framework%2520for%2520GitHub%2520Issue%2520Resolution%26entry.906535625%3DWei%2520Tao%2520and%2520Yucheng%2520Zhou%2520and%2520Yanlin%2520Wang%2520and%2520Wenqiang%2520Zhang%2520and%2520Hongyu%2520Zhang%2520and%2520Yu%2520Cheng%26entry.1292438233%3D%2520%2520In%2520software%2520development%252C%2520resolving%2520the%2520emergent%2520issues%2520within%2520GitHub%250Arepositories%2520is%2520a%2520complex%2520challenge%2520that%2520involves%2520not%2520only%2520the%2520incorporation%2520of%250Anew%2520code%2520but%2520also%2520the%2520maintenance%2520of%2520existing%2520code.%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520have%2520shown%2520promise%2520in%2520code%2520generation%2520but%2520face%2520difficulties%2520in%2520resolving%250AGithub%2520issues%252C%2520particularly%2520at%2520the%2520repository%2520level.%2520To%2520overcome%2520this%250Achallenge%252C%2520we%2520empirically%2520study%2520the%2520reason%2520why%2520LLMs%2520fail%2520to%2520resolve%2520GitHub%250Aissues%2520and%2520analyze%2520the%2520major%2520factors.%2520Motivated%2520by%2520the%2520empirical%2520findings%252C%2520we%250Apropose%2520a%2520novel%2520LLM-based%2520Multi-Agent%2520framework%2520for%2520GitHub%2520Issue%2520reSolution%252C%250AMAGIS%252C%2520consisting%2520of%2520four%2520agents%2520customized%2520for%2520software%2520evolution%253A%2520Manager%252C%250ARepository%2520Custodian%252C%2520Developer%252C%2520and%2520Quality%2520Assurance%2520Engineer%2520agents.%2520This%250Aframework%2520leverages%2520the%2520collaboration%2520of%2520various%2520agents%2520in%2520the%2520planning%2520and%250Acoding%2520process%2520to%2520unlock%2520the%2520potential%2520of%2520LLMs%2520to%2520resolve%2520GitHub%2520issues.%2520In%250Aexperiments%252C%2520we%2520employ%2520the%2520SWE-bench%2520benchmark%2520to%2520compare%2520MAGIS%2520with%2520popular%250ALLMs%252C%2520including%2520GPT-3.5%252C%2520GPT-4%252C%2520and%2520Claude-2.%2520MAGIS%2520can%2520resolve%252013.94%2525%2520GitHub%250Aissues%252C%2520significantly%2520outperforming%2520the%2520baselines.%2520Specifically%252C%2520MAGIS%2520achieves%250Aan%2520eight-fold%2520increase%2520in%2520resolved%2520ratio%2520over%2520the%2520direct%2520application%2520of%2520GPT-4%252C%250Athe%2520advanced%2520LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17927v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAGIS%3A%20LLM-Based%20Multi-Agent%20Framework%20for%20GitHub%20Issue%20Resolution&entry.906535625=Wei%20Tao%20and%20Yucheng%20Zhou%20and%20Yanlin%20Wang%20and%20Wenqiang%20Zhang%20and%20Hongyu%20Zhang%20and%20Yu%20Cheng&entry.1292438233=%20%20In%20software%20development%2C%20resolving%20the%20emergent%20issues%20within%20GitHub%0Arepositories%20is%20a%20complex%20challenge%20that%20involves%20not%20only%20the%20incorporation%20of%0Anew%20code%20but%20also%20the%20maintenance%20of%20existing%20code.%20Large%20Language%20Models%0A%28LLMs%29%20have%20shown%20promise%20in%20code%20generation%20but%20face%20difficulties%20in%20resolving%0AGithub%20issues%2C%20particularly%20at%20the%20repository%20level.%20To%20overcome%20this%0Achallenge%2C%20we%20empirically%20study%20the%20reason%20why%20LLMs%20fail%20to%20resolve%20GitHub%0Aissues%20and%20analyze%20the%20major%20factors.%20Motivated%20by%20the%20empirical%20findings%2C%20we%0Apropose%20a%20novel%20LLM-based%20Multi-Agent%20framework%20for%20GitHub%20Issue%20reSolution%2C%0AMAGIS%2C%20consisting%20of%20four%20agents%20customized%20for%20software%20evolution%3A%20Manager%2C%0ARepository%20Custodian%2C%20Developer%2C%20and%20Quality%20Assurance%20Engineer%20agents.%20This%0Aframework%20leverages%20the%20collaboration%20of%20various%20agents%20in%20the%20planning%20and%0Acoding%20process%20to%20unlock%20the%20potential%20of%20LLMs%20to%20resolve%20GitHub%20issues.%20In%0Aexperiments%2C%20we%20employ%20the%20SWE-bench%20benchmark%20to%20compare%20MAGIS%20with%20popular%0ALLMs%2C%20including%20GPT-3.5%2C%20GPT-4%2C%20and%20Claude-2.%20MAGIS%20can%20resolve%2013.94%25%20GitHub%0Aissues%2C%20significantly%20outperforming%20the%20baselines.%20Specifically%2C%20MAGIS%20achieves%0Aan%20eight-fold%20increase%20in%20resolved%20ratio%20over%20the%20direct%20application%20of%20GPT-4%2C%0Athe%20advanced%20LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17927v2&entry.124074799=Read"},
{"title": "From Artificial Needles to Real Haystacks: Improving Retrieval\n  Capabilities in LLMs by Finetuning on Synthetic Data", "author": "Zheyang Xiong and Vasilis Papageorgiou and Kangwook Lee and Dimitris Papailiopoulos", "abstract": "  Recent studies have shown that Large Language Models (LLMs) struggle to\naccurately retrieve information and maintain reasoning capabilities when\nprocessing long-context inputs. To address these limitations, we propose a\nfinetuning approach utilizing a carefully designed synthetic dataset comprising\nnumerical key-value retrieval tasks. Our experiments on models like GPT-3.5\nTurbo and Mistral 7B demonstrate that finetuning LLMs on this dataset\nsignificantly improves LLMs' information retrieval and reasoning capabilities\nin longer-context settings. We present an analysis of the finetuned models,\nillustrating the transfer of skills from synthetic to real task evaluations\n(e.g., $10.5\\%$ improvement on $20$ documents MDQA at position $10$ for GPT-3.5\nTurbo). We also find that finetuned LLMs' performance on general benchmarks\nremains almost constant while LLMs finetuned on other baseline long-context\naugmentation data can encourage hallucination (e.g., on TriviaQA, Mistral 7B\nfinetuned on our synthetic data cause no performance drop while other baseline\ndata can cause a drop that ranges from $2.33\\%$ to $6.19\\%$). Our study\nhighlights the potential of finetuning on synthetic data for improving the\nperformance of LLMs on longer-context tasks.\n", "link": "http://arxiv.org/abs/2406.19292v1", "date": "2024-06-27", "relevancy": 1.5292, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5274}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5065}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Artificial%20Needles%20to%20Real%20Haystacks%3A%20Improving%20Retrieval%0A%20%20Capabilities%20in%20LLMs%20by%20Finetuning%20on%20Synthetic%20Data&body=Title%3A%20From%20Artificial%20Needles%20to%20Real%20Haystacks%3A%20Improving%20Retrieval%0A%20%20Capabilities%20in%20LLMs%20by%20Finetuning%20on%20Synthetic%20Data%0AAuthor%3A%20Zheyang%20Xiong%20and%20Vasilis%20Papageorgiou%20and%20Kangwook%20Lee%20and%20Dimitris%20Papailiopoulos%0AAbstract%3A%20%20%20Recent%20studies%20have%20shown%20that%20Large%20Language%20Models%20%28LLMs%29%20struggle%20to%0Aaccurately%20retrieve%20information%20and%20maintain%20reasoning%20capabilities%20when%0Aprocessing%20long-context%20inputs.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Afinetuning%20approach%20utilizing%20a%20carefully%20designed%20synthetic%20dataset%20comprising%0Anumerical%20key-value%20retrieval%20tasks.%20Our%20experiments%20on%20models%20like%20GPT-3.5%0ATurbo%20and%20Mistral%207B%20demonstrate%20that%20finetuning%20LLMs%20on%20this%20dataset%0Asignificantly%20improves%20LLMs%27%20information%20retrieval%20and%20reasoning%20capabilities%0Ain%20longer-context%20settings.%20We%20present%20an%20analysis%20of%20the%20finetuned%20models%2C%0Aillustrating%20the%20transfer%20of%20skills%20from%20synthetic%20to%20real%20task%20evaluations%0A%28e.g.%2C%20%2410.5%5C%25%24%20improvement%20on%20%2420%24%20documents%20MDQA%20at%20position%20%2410%24%20for%20GPT-3.5%0ATurbo%29.%20We%20also%20find%20that%20finetuned%20LLMs%27%20performance%20on%20general%20benchmarks%0Aremains%20almost%20constant%20while%20LLMs%20finetuned%20on%20other%20baseline%20long-context%0Aaugmentation%20data%20can%20encourage%20hallucination%20%28e.g.%2C%20on%20TriviaQA%2C%20Mistral%207B%0Afinetuned%20on%20our%20synthetic%20data%20cause%20no%20performance%20drop%20while%20other%20baseline%0Adata%20can%20cause%20a%20drop%20that%20ranges%20from%20%242.33%5C%25%24%20to%20%246.19%5C%25%24%29.%20Our%20study%0Ahighlights%20the%20potential%20of%20finetuning%20on%20synthetic%20data%20for%20improving%20the%0Aperformance%20of%20LLMs%20on%20longer-context%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Artificial%2520Needles%2520to%2520Real%2520Haystacks%253A%2520Improving%2520Retrieval%250A%2520%2520Capabilities%2520in%2520LLMs%2520by%2520Finetuning%2520on%2520Synthetic%2520Data%26entry.906535625%3DZheyang%2520Xiong%2520and%2520Vasilis%2520Papageorgiou%2520and%2520Kangwook%2520Lee%2520and%2520Dimitris%2520Papailiopoulos%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520shown%2520that%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520struggle%2520to%250Aaccurately%2520retrieve%2520information%2520and%2520maintain%2520reasoning%2520capabilities%2520when%250Aprocessing%2520long-context%2520inputs.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%250Afinetuning%2520approach%2520utilizing%2520a%2520carefully%2520designed%2520synthetic%2520dataset%2520comprising%250Anumerical%2520key-value%2520retrieval%2520tasks.%2520Our%2520experiments%2520on%2520models%2520like%2520GPT-3.5%250ATurbo%2520and%2520Mistral%25207B%2520demonstrate%2520that%2520finetuning%2520LLMs%2520on%2520this%2520dataset%250Asignificantly%2520improves%2520LLMs%2527%2520information%2520retrieval%2520and%2520reasoning%2520capabilities%250Ain%2520longer-context%2520settings.%2520We%2520present%2520an%2520analysis%2520of%2520the%2520finetuned%2520models%252C%250Aillustrating%2520the%2520transfer%2520of%2520skills%2520from%2520synthetic%2520to%2520real%2520task%2520evaluations%250A%2528e.g.%252C%2520%252410.5%255C%2525%2524%2520improvement%2520on%2520%252420%2524%2520documents%2520MDQA%2520at%2520position%2520%252410%2524%2520for%2520GPT-3.5%250ATurbo%2529.%2520We%2520also%2520find%2520that%2520finetuned%2520LLMs%2527%2520performance%2520on%2520general%2520benchmarks%250Aremains%2520almost%2520constant%2520while%2520LLMs%2520finetuned%2520on%2520other%2520baseline%2520long-context%250Aaugmentation%2520data%2520can%2520encourage%2520hallucination%2520%2528e.g.%252C%2520on%2520TriviaQA%252C%2520Mistral%25207B%250Afinetuned%2520on%2520our%2520synthetic%2520data%2520cause%2520no%2520performance%2520drop%2520while%2520other%2520baseline%250Adata%2520can%2520cause%2520a%2520drop%2520that%2520ranges%2520from%2520%25242.33%255C%2525%2524%2520to%2520%25246.19%255C%2525%2524%2529.%2520Our%2520study%250Ahighlights%2520the%2520potential%2520of%2520finetuning%2520on%2520synthetic%2520data%2520for%2520improving%2520the%250Aperformance%2520of%2520LLMs%2520on%2520longer-context%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Artificial%20Needles%20to%20Real%20Haystacks%3A%20Improving%20Retrieval%0A%20%20Capabilities%20in%20LLMs%20by%20Finetuning%20on%20Synthetic%20Data&entry.906535625=Zheyang%20Xiong%20and%20Vasilis%20Papageorgiou%20and%20Kangwook%20Lee%20and%20Dimitris%20Papailiopoulos&entry.1292438233=%20%20Recent%20studies%20have%20shown%20that%20Large%20Language%20Models%20%28LLMs%29%20struggle%20to%0Aaccurately%20retrieve%20information%20and%20maintain%20reasoning%20capabilities%20when%0Aprocessing%20long-context%20inputs.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Afinetuning%20approach%20utilizing%20a%20carefully%20designed%20synthetic%20dataset%20comprising%0Anumerical%20key-value%20retrieval%20tasks.%20Our%20experiments%20on%20models%20like%20GPT-3.5%0ATurbo%20and%20Mistral%207B%20demonstrate%20that%20finetuning%20LLMs%20on%20this%20dataset%0Asignificantly%20improves%20LLMs%27%20information%20retrieval%20and%20reasoning%20capabilities%0Ain%20longer-context%20settings.%20We%20present%20an%20analysis%20of%20the%20finetuned%20models%2C%0Aillustrating%20the%20transfer%20of%20skills%20from%20synthetic%20to%20real%20task%20evaluations%0A%28e.g.%2C%20%2410.5%5C%25%24%20improvement%20on%20%2420%24%20documents%20MDQA%20at%20position%20%2410%24%20for%20GPT-3.5%0ATurbo%29.%20We%20also%20find%20that%20finetuned%20LLMs%27%20performance%20on%20general%20benchmarks%0Aremains%20almost%20constant%20while%20LLMs%20finetuned%20on%20other%20baseline%20long-context%0Aaugmentation%20data%20can%20encourage%20hallucination%20%28e.g.%2C%20on%20TriviaQA%2C%20Mistral%207B%0Afinetuned%20on%20our%20synthetic%20data%20cause%20no%20performance%20drop%20while%20other%20baseline%0Adata%20can%20cause%20a%20drop%20that%20ranges%20from%20%242.33%5C%25%24%20to%20%246.19%5C%25%24%29.%20Our%20study%0Ahighlights%20the%20potential%20of%20finetuning%20on%20synthetic%20data%20for%20improving%20the%0Aperformance%20of%20LLMs%20on%20longer-context%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19292v1&entry.124074799=Read"},
{"title": "Seeing Is Believing: Black-Box Membership Inference Attacks Against\n  Retrieval Augmented Generation", "author": "Yuying Li and Gaoyang Liu and Yang Yang and Chen Wang", "abstract": "  Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that\nenhances Large Language Models (LLMs) by retrieving relevant knowledge from an\nexternal, non-parametric database. This approach aims to mitigate common LLM\nissues such as hallucinations and outdated knowledge. Although existing\nresearch has demonstrated security and privacy vulnerabilities within RAG\nsystems, making them susceptible to attacks like jailbreaks and prompt\ninjections, the security of the RAG system's external databases remains largely\nunderexplored. In this paper, we employ Membership Inference Attacks (MIA) to\ndetermine whether a sample is part of the knowledge database of a RAG system,\nusing only black-box API access. Our core hypothesis posits that if a sample is\na member, it will exhibit significant similarity to the text generated by the\nRAG system. To test this, we compute the cosine similarity and the model's\nperplexity to establish a membership score, thereby building robust features.\nWe then introduce two novel attack strategies: a Threshold-based Attack and a\nMachine Learning-based Attack, designed to accurately identify membership.\nExperimental validation of our methods has achieved a ROC AUC of 82%.\n", "link": "http://arxiv.org/abs/2406.19234v1", "date": "2024-06-27", "relevancy": 1.3945, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4756}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.464}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20Is%20Believing%3A%20Black-Box%20Membership%20Inference%20Attacks%20Against%0A%20%20Retrieval%20Augmented%20Generation&body=Title%3A%20Seeing%20Is%20Believing%3A%20Black-Box%20Membership%20Inference%20Attacks%20Against%0A%20%20Retrieval%20Augmented%20Generation%0AAuthor%3A%20Yuying%20Li%20and%20Gaoyang%20Liu%20and%20Yang%20Yang%20and%20Chen%20Wang%0AAbstract%3A%20%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20is%20a%20state-of-the-art%20technique%20that%0Aenhances%20Large%20Language%20Models%20%28LLMs%29%20by%20retrieving%20relevant%20knowledge%20from%20an%0Aexternal%2C%20non-parametric%20database.%20This%20approach%20aims%20to%20mitigate%20common%20LLM%0Aissues%20such%20as%20hallucinations%20and%20outdated%20knowledge.%20Although%20existing%0Aresearch%20has%20demonstrated%20security%20and%20privacy%20vulnerabilities%20within%20RAG%0Asystems%2C%20making%20them%20susceptible%20to%20attacks%20like%20jailbreaks%20and%20prompt%0Ainjections%2C%20the%20security%20of%20the%20RAG%20system%27s%20external%20databases%20remains%20largely%0Aunderexplored.%20In%20this%20paper%2C%20we%20employ%20Membership%20Inference%20Attacks%20%28MIA%29%20to%0Adetermine%20whether%20a%20sample%20is%20part%20of%20the%20knowledge%20database%20of%20a%20RAG%20system%2C%0Ausing%20only%20black-box%20API%20access.%20Our%20core%20hypothesis%20posits%20that%20if%20a%20sample%20is%0Aa%20member%2C%20it%20will%20exhibit%20significant%20similarity%20to%20the%20text%20generated%20by%20the%0ARAG%20system.%20To%20test%20this%2C%20we%20compute%20the%20cosine%20similarity%20and%20the%20model%27s%0Aperplexity%20to%20establish%20a%20membership%20score%2C%20thereby%20building%20robust%20features.%0AWe%20then%20introduce%20two%20novel%20attack%20strategies%3A%20a%20Threshold-based%20Attack%20and%20a%0AMachine%20Learning-based%20Attack%2C%20designed%20to%20accurately%20identify%20membership.%0AExperimental%20validation%20of%20our%20methods%20has%20achieved%20a%20ROC%20AUC%20of%2082%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19234v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520Is%2520Believing%253A%2520Black-Box%2520Membership%2520Inference%2520Attacks%2520Against%250A%2520%2520Retrieval%2520Augmented%2520Generation%26entry.906535625%3DYuying%2520Li%2520and%2520Gaoyang%2520Liu%2520and%2520Yang%2520Yang%2520and%2520Chen%2520Wang%26entry.1292438233%3D%2520%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520is%2520a%2520state-of-the-art%2520technique%2520that%250Aenhances%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520by%2520retrieving%2520relevant%2520knowledge%2520from%2520an%250Aexternal%252C%2520non-parametric%2520database.%2520This%2520approach%2520aims%2520to%2520mitigate%2520common%2520LLM%250Aissues%2520such%2520as%2520hallucinations%2520and%2520outdated%2520knowledge.%2520Although%2520existing%250Aresearch%2520has%2520demonstrated%2520security%2520and%2520privacy%2520vulnerabilities%2520within%2520RAG%250Asystems%252C%2520making%2520them%2520susceptible%2520to%2520attacks%2520like%2520jailbreaks%2520and%2520prompt%250Ainjections%252C%2520the%2520security%2520of%2520the%2520RAG%2520system%2527s%2520external%2520databases%2520remains%2520largely%250Aunderexplored.%2520In%2520this%2520paper%252C%2520we%2520employ%2520Membership%2520Inference%2520Attacks%2520%2528MIA%2529%2520to%250Adetermine%2520whether%2520a%2520sample%2520is%2520part%2520of%2520the%2520knowledge%2520database%2520of%2520a%2520RAG%2520system%252C%250Ausing%2520only%2520black-box%2520API%2520access.%2520Our%2520core%2520hypothesis%2520posits%2520that%2520if%2520a%2520sample%2520is%250Aa%2520member%252C%2520it%2520will%2520exhibit%2520significant%2520similarity%2520to%2520the%2520text%2520generated%2520by%2520the%250ARAG%2520system.%2520To%2520test%2520this%252C%2520we%2520compute%2520the%2520cosine%2520similarity%2520and%2520the%2520model%2527s%250Aperplexity%2520to%2520establish%2520a%2520membership%2520score%252C%2520thereby%2520building%2520robust%2520features.%250AWe%2520then%2520introduce%2520two%2520novel%2520attack%2520strategies%253A%2520a%2520Threshold-based%2520Attack%2520and%2520a%250AMachine%2520Learning-based%2520Attack%252C%2520designed%2520to%2520accurately%2520identify%2520membership.%250AExperimental%2520validation%2520of%2520our%2520methods%2520has%2520achieved%2520a%2520ROC%2520AUC%2520of%252082%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19234v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20Is%20Believing%3A%20Black-Box%20Membership%20Inference%20Attacks%20Against%0A%20%20Retrieval%20Augmented%20Generation&entry.906535625=Yuying%20Li%20and%20Gaoyang%20Liu%20and%20Yang%20Yang%20and%20Chen%20Wang&entry.1292438233=%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20is%20a%20state-of-the-art%20technique%20that%0Aenhances%20Large%20Language%20Models%20%28LLMs%29%20by%20retrieving%20relevant%20knowledge%20from%20an%0Aexternal%2C%20non-parametric%20database.%20This%20approach%20aims%20to%20mitigate%20common%20LLM%0Aissues%20such%20as%20hallucinations%20and%20outdated%20knowledge.%20Although%20existing%0Aresearch%20has%20demonstrated%20security%20and%20privacy%20vulnerabilities%20within%20RAG%0Asystems%2C%20making%20them%20susceptible%20to%20attacks%20like%20jailbreaks%20and%20prompt%0Ainjections%2C%20the%20security%20of%20the%20RAG%20system%27s%20external%20databases%20remains%20largely%0Aunderexplored.%20In%20this%20paper%2C%20we%20employ%20Membership%20Inference%20Attacks%20%28MIA%29%20to%0Adetermine%20whether%20a%20sample%20is%20part%20of%20the%20knowledge%20database%20of%20a%20RAG%20system%2C%0Ausing%20only%20black-box%20API%20access.%20Our%20core%20hypothesis%20posits%20that%20if%20a%20sample%20is%0Aa%20member%2C%20it%20will%20exhibit%20significant%20similarity%20to%20the%20text%20generated%20by%20the%0ARAG%20system.%20To%20test%20this%2C%20we%20compute%20the%20cosine%20similarity%20and%20the%20model%27s%0Aperplexity%20to%20establish%20a%20membership%20score%2C%20thereby%20building%20robust%20features.%0AWe%20then%20introduce%20two%20novel%20attack%20strategies%3A%20a%20Threshold-based%20Attack%20and%20a%0AMachine%20Learning-based%20Attack%2C%20designed%20to%20accurately%20identify%20membership.%0AExperimental%20validation%20of%20our%20methods%20has%20achieved%20a%20ROC%20AUC%20of%2082%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19234v1&entry.124074799=Read"},
{"title": "Subtractive Training for Music Stem Insertion using Latent Diffusion\n  Models", "author": "Ivan Villa-Renteria and Mason L. Wang and Zachary Shah and Zhe Li and Soohyun Kim and Neelesh Ramachandran and Mert Pilanci", "abstract": "  We present Subtractive Training, a simple and novel method for synthesizing\nindividual musical instrument stems given other instruments as context. This\nmethod pairs a dataset of complete music mixes with 1) a variant of the dataset\nlacking a specific stem, and 2) LLM-generated instructions describing how the\nmissing stem should be reintroduced. We then fine-tune a pretrained\ntext-to-audio diffusion model to generate the missing instrument stem, guided\nby both the existing stems and the text instruction. Our results demonstrate\nSubtractive Training's efficacy in creating authentic drum stems that\nseamlessly blend with the existing tracks. We also show that we can use the\ntext instruction to control the generation of the inserted stem in terms of\nrhythm, dynamics, and genre, allowing us to modify the style of a single\ninstrument in a full song while keeping the remaining instruments the same.\nLastly, we extend this technique to MIDI formats, successfully generating\ncompatible bass, drum, and guitar parts for incomplete arrangements.\n", "link": "http://arxiv.org/abs/2406.19328v1", "date": "2024-06-27", "relevancy": 1.0304, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5278}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5149}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subtractive%20Training%20for%20Music%20Stem%20Insertion%20using%20Latent%20Diffusion%0A%20%20Models&body=Title%3A%20Subtractive%20Training%20for%20Music%20Stem%20Insertion%20using%20Latent%20Diffusion%0A%20%20Models%0AAuthor%3A%20Ivan%20Villa-Renteria%20and%20Mason%20L.%20Wang%20and%20Zachary%20Shah%20and%20Zhe%20Li%20and%20Soohyun%20Kim%20and%20Neelesh%20Ramachandran%20and%20Mert%20Pilanci%0AAbstract%3A%20%20%20We%20present%20Subtractive%20Training%2C%20a%20simple%20and%20novel%20method%20for%20synthesizing%0Aindividual%20musical%20instrument%20stems%20given%20other%20instruments%20as%20context.%20This%0Amethod%20pairs%20a%20dataset%20of%20complete%20music%20mixes%20with%201%29%20a%20variant%20of%20the%20dataset%0Alacking%20a%20specific%20stem%2C%20and%202%29%20LLM-generated%20instructions%20describing%20how%20the%0Amissing%20stem%20should%20be%20reintroduced.%20We%20then%20fine-tune%20a%20pretrained%0Atext-to-audio%20diffusion%20model%20to%20generate%20the%20missing%20instrument%20stem%2C%20guided%0Aby%20both%20the%20existing%20stems%20and%20the%20text%20instruction.%20Our%20results%20demonstrate%0ASubtractive%20Training%27s%20efficacy%20in%20creating%20authentic%20drum%20stems%20that%0Aseamlessly%20blend%20with%20the%20existing%20tracks.%20We%20also%20show%20that%20we%20can%20use%20the%0Atext%20instruction%20to%20control%20the%20generation%20of%20the%20inserted%20stem%20in%20terms%20of%0Arhythm%2C%20dynamics%2C%20and%20genre%2C%20allowing%20us%20to%20modify%20the%20style%20of%20a%20single%0Ainstrument%20in%20a%20full%20song%20while%20keeping%20the%20remaining%20instruments%20the%20same.%0ALastly%2C%20we%20extend%20this%20technique%20to%20MIDI%20formats%2C%20successfully%20generating%0Acompatible%20bass%2C%20drum%2C%20and%20guitar%20parts%20for%20incomplete%20arrangements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19328v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubtractive%2520Training%2520for%2520Music%2520Stem%2520Insertion%2520using%2520Latent%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DIvan%2520Villa-Renteria%2520and%2520Mason%2520L.%2520Wang%2520and%2520Zachary%2520Shah%2520and%2520Zhe%2520Li%2520and%2520Soohyun%2520Kim%2520and%2520Neelesh%2520Ramachandran%2520and%2520Mert%2520Pilanci%26entry.1292438233%3D%2520%2520We%2520present%2520Subtractive%2520Training%252C%2520a%2520simple%2520and%2520novel%2520method%2520for%2520synthesizing%250Aindividual%2520musical%2520instrument%2520stems%2520given%2520other%2520instruments%2520as%2520context.%2520This%250Amethod%2520pairs%2520a%2520dataset%2520of%2520complete%2520music%2520mixes%2520with%25201%2529%2520a%2520variant%2520of%2520the%2520dataset%250Alacking%2520a%2520specific%2520stem%252C%2520and%25202%2529%2520LLM-generated%2520instructions%2520describing%2520how%2520the%250Amissing%2520stem%2520should%2520be%2520reintroduced.%2520We%2520then%2520fine-tune%2520a%2520pretrained%250Atext-to-audio%2520diffusion%2520model%2520to%2520generate%2520the%2520missing%2520instrument%2520stem%252C%2520guided%250Aby%2520both%2520the%2520existing%2520stems%2520and%2520the%2520text%2520instruction.%2520Our%2520results%2520demonstrate%250ASubtractive%2520Training%2527s%2520efficacy%2520in%2520creating%2520authentic%2520drum%2520stems%2520that%250Aseamlessly%2520blend%2520with%2520the%2520existing%2520tracks.%2520We%2520also%2520show%2520that%2520we%2520can%2520use%2520the%250Atext%2520instruction%2520to%2520control%2520the%2520generation%2520of%2520the%2520inserted%2520stem%2520in%2520terms%2520of%250Arhythm%252C%2520dynamics%252C%2520and%2520genre%252C%2520allowing%2520us%2520to%2520modify%2520the%2520style%2520of%2520a%2520single%250Ainstrument%2520in%2520a%2520full%2520song%2520while%2520keeping%2520the%2520remaining%2520instruments%2520the%2520same.%250ALastly%252C%2520we%2520extend%2520this%2520technique%2520to%2520MIDI%2520formats%252C%2520successfully%2520generating%250Acompatible%2520bass%252C%2520drum%252C%2520and%2520guitar%2520parts%2520for%2520incomplete%2520arrangements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19328v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subtractive%20Training%20for%20Music%20Stem%20Insertion%20using%20Latent%20Diffusion%0A%20%20Models&entry.906535625=Ivan%20Villa-Renteria%20and%20Mason%20L.%20Wang%20and%20Zachary%20Shah%20and%20Zhe%20Li%20and%20Soohyun%20Kim%20and%20Neelesh%20Ramachandran%20and%20Mert%20Pilanci&entry.1292438233=%20%20We%20present%20Subtractive%20Training%2C%20a%20simple%20and%20novel%20method%20for%20synthesizing%0Aindividual%20musical%20instrument%20stems%20given%20other%20instruments%20as%20context.%20This%0Amethod%20pairs%20a%20dataset%20of%20complete%20music%20mixes%20with%201%29%20a%20variant%20of%20the%20dataset%0Alacking%20a%20specific%20stem%2C%20and%202%29%20LLM-generated%20instructions%20describing%20how%20the%0Amissing%20stem%20should%20be%20reintroduced.%20We%20then%20fine-tune%20a%20pretrained%0Atext-to-audio%20diffusion%20model%20to%20generate%20the%20missing%20instrument%20stem%2C%20guided%0Aby%20both%20the%20existing%20stems%20and%20the%20text%20instruction.%20Our%20results%20demonstrate%0ASubtractive%20Training%27s%20efficacy%20in%20creating%20authentic%20drum%20stems%20that%0Aseamlessly%20blend%20with%20the%20existing%20tracks.%20We%20also%20show%20that%20we%20can%20use%20the%0Atext%20instruction%20to%20control%20the%20generation%20of%20the%20inserted%20stem%20in%20terms%20of%0Arhythm%2C%20dynamics%2C%20and%20genre%2C%20allowing%20us%20to%20modify%20the%20style%20of%20a%20single%0Ainstrument%20in%20a%20full%20song%20while%20keeping%20the%20remaining%20instruments%20the%20same.%0ALastly%2C%20we%20extend%20this%20technique%20to%20MIDI%20formats%2C%20successfully%20generating%0Acompatible%20bass%2C%20drum%2C%20and%20guitar%20parts%20for%20incomplete%20arrangements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19328v1&entry.124074799=Read"},
{"title": "CHESS: Contextual Harnessing for Efficient SQL Synthesis", "author": "Shayan Talaei and Mohammadreza Pourreza and Yu-Chen Chang and Azalia Mirhoseini and Amin Saberi", "abstract": "  Utilizing large language models (LLMs) for transforming natural language\nquestions into SQL queries (text-to-SQL) is a promising yet challenging\napproach, particularly when applied to real-world databases with complex and\nextensive schemas. In particular, effectively incorporating data catalogs and\ndatabase values for SQL generation remains an obstacle, leading to suboptimal\nsolutions. We address this problem by proposing a new pipeline that effectively\nretrieves relevant data and context, selects an efficient schema, and\nsynthesizes correct and efficient SQL queries. To increase retrieval precision,\nour pipeline introduces a hierarchical retrieval method leveraging\nmodel-generated keywords, locality-sensitive hashing indexing, and vector\ndatabases. Additionally, we have developed an adaptive schema pruning technique\nthat adjusts based on the complexity of the problem and the model's context\nsize. Our approach generalizes to both frontier proprietary models like GPT-4\nand open-source models such as Llama-3-70B. Through a series of ablation\nstudies, we demonstrate the effectiveness of each component of our pipeline and\nits impact on the end-to-end performance. Our method achieves new\nstate-of-the-art performance on the cross-domain challenging BIRD dataset.\n", "link": "http://arxiv.org/abs/2405.16755v2", "date": "2024-06-27", "relevancy": 1.4479, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5096}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4775}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CHESS%3A%20Contextual%20Harnessing%20for%20Efficient%20SQL%20Synthesis&body=Title%3A%20CHESS%3A%20Contextual%20Harnessing%20for%20Efficient%20SQL%20Synthesis%0AAuthor%3A%20Shayan%20Talaei%20and%20Mohammadreza%20Pourreza%20and%20Yu-Chen%20Chang%20and%20Azalia%20Mirhoseini%20and%20Amin%20Saberi%0AAbstract%3A%20%20%20Utilizing%20large%20language%20models%20%28LLMs%29%20for%20transforming%20natural%20language%0Aquestions%20into%20SQL%20queries%20%28text-to-SQL%29%20is%20a%20promising%20yet%20challenging%0Aapproach%2C%20particularly%20when%20applied%20to%20real-world%20databases%20with%20complex%20and%0Aextensive%20schemas.%20In%20particular%2C%20effectively%20incorporating%20data%20catalogs%20and%0Adatabase%20values%20for%20SQL%20generation%20remains%20an%20obstacle%2C%20leading%20to%20suboptimal%0Asolutions.%20We%20address%20this%20problem%20by%20proposing%20a%20new%20pipeline%20that%20effectively%0Aretrieves%20relevant%20data%20and%20context%2C%20selects%20an%20efficient%20schema%2C%20and%0Asynthesizes%20correct%20and%20efficient%20SQL%20queries.%20To%20increase%20retrieval%20precision%2C%0Aour%20pipeline%20introduces%20a%20hierarchical%20retrieval%20method%20leveraging%0Amodel-generated%20keywords%2C%20locality-sensitive%20hashing%20indexing%2C%20and%20vector%0Adatabases.%20Additionally%2C%20we%20have%20developed%20an%20adaptive%20schema%20pruning%20technique%0Athat%20adjusts%20based%20on%20the%20complexity%20of%20the%20problem%20and%20the%20model%27s%20context%0Asize.%20Our%20approach%20generalizes%20to%20both%20frontier%20proprietary%20models%20like%20GPT-4%0Aand%20open-source%20models%20such%20as%20Llama-3-70B.%20Through%20a%20series%20of%20ablation%0Astudies%2C%20we%20demonstrate%20the%20effectiveness%20of%20each%20component%20of%20our%20pipeline%20and%0Aits%20impact%20on%20the%20end-to-end%20performance.%20Our%20method%20achieves%20new%0Astate-of-the-art%20performance%20on%20the%20cross-domain%20challenging%20BIRD%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16755v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCHESS%253A%2520Contextual%2520Harnessing%2520for%2520Efficient%2520SQL%2520Synthesis%26entry.906535625%3DShayan%2520Talaei%2520and%2520Mohammadreza%2520Pourreza%2520and%2520Yu-Chen%2520Chang%2520and%2520Azalia%2520Mirhoseini%2520and%2520Amin%2520Saberi%26entry.1292438233%3D%2520%2520Utilizing%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520transforming%2520natural%2520language%250Aquestions%2520into%2520SQL%2520queries%2520%2528text-to-SQL%2529%2520is%2520a%2520promising%2520yet%2520challenging%250Aapproach%252C%2520particularly%2520when%2520applied%2520to%2520real-world%2520databases%2520with%2520complex%2520and%250Aextensive%2520schemas.%2520In%2520particular%252C%2520effectively%2520incorporating%2520data%2520catalogs%2520and%250Adatabase%2520values%2520for%2520SQL%2520generation%2520remains%2520an%2520obstacle%252C%2520leading%2520to%2520suboptimal%250Asolutions.%2520We%2520address%2520this%2520problem%2520by%2520proposing%2520a%2520new%2520pipeline%2520that%2520effectively%250Aretrieves%2520relevant%2520data%2520and%2520context%252C%2520selects%2520an%2520efficient%2520schema%252C%2520and%250Asynthesizes%2520correct%2520and%2520efficient%2520SQL%2520queries.%2520To%2520increase%2520retrieval%2520precision%252C%250Aour%2520pipeline%2520introduces%2520a%2520hierarchical%2520retrieval%2520method%2520leveraging%250Amodel-generated%2520keywords%252C%2520locality-sensitive%2520hashing%2520indexing%252C%2520and%2520vector%250Adatabases.%2520Additionally%252C%2520we%2520have%2520developed%2520an%2520adaptive%2520schema%2520pruning%2520technique%250Athat%2520adjusts%2520based%2520on%2520the%2520complexity%2520of%2520the%2520problem%2520and%2520the%2520model%2527s%2520context%250Asize.%2520Our%2520approach%2520generalizes%2520to%2520both%2520frontier%2520proprietary%2520models%2520like%2520GPT-4%250Aand%2520open-source%2520models%2520such%2520as%2520Llama-3-70B.%2520Through%2520a%2520series%2520of%2520ablation%250Astudies%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520each%2520component%2520of%2520our%2520pipeline%2520and%250Aits%2520impact%2520on%2520the%2520end-to-end%2520performance.%2520Our%2520method%2520achieves%2520new%250Astate-of-the-art%2520performance%2520on%2520the%2520cross-domain%2520challenging%2520BIRD%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16755v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CHESS%3A%20Contextual%20Harnessing%20for%20Efficient%20SQL%20Synthesis&entry.906535625=Shayan%20Talaei%20and%20Mohammadreza%20Pourreza%20and%20Yu-Chen%20Chang%20and%20Azalia%20Mirhoseini%20and%20Amin%20Saberi&entry.1292438233=%20%20Utilizing%20large%20language%20models%20%28LLMs%29%20for%20transforming%20natural%20language%0Aquestions%20into%20SQL%20queries%20%28text-to-SQL%29%20is%20a%20promising%20yet%20challenging%0Aapproach%2C%20particularly%20when%20applied%20to%20real-world%20databases%20with%20complex%20and%0Aextensive%20schemas.%20In%20particular%2C%20effectively%20incorporating%20data%20catalogs%20and%0Adatabase%20values%20for%20SQL%20generation%20remains%20an%20obstacle%2C%20leading%20to%20suboptimal%0Asolutions.%20We%20address%20this%20problem%20by%20proposing%20a%20new%20pipeline%20that%20effectively%0Aretrieves%20relevant%20data%20and%20context%2C%20selects%20an%20efficient%20schema%2C%20and%0Asynthesizes%20correct%20and%20efficient%20SQL%20queries.%20To%20increase%20retrieval%20precision%2C%0Aour%20pipeline%20introduces%20a%20hierarchical%20retrieval%20method%20leveraging%0Amodel-generated%20keywords%2C%20locality-sensitive%20hashing%20indexing%2C%20and%20vector%0Adatabases.%20Additionally%2C%20we%20have%20developed%20an%20adaptive%20schema%20pruning%20technique%0Athat%20adjusts%20based%20on%20the%20complexity%20of%20the%20problem%20and%20the%20model%27s%20context%0Asize.%20Our%20approach%20generalizes%20to%20both%20frontier%20proprietary%20models%20like%20GPT-4%0Aand%20open-source%20models%20such%20as%20Llama-3-70B.%20Through%20a%20series%20of%20ablation%0Astudies%2C%20we%20demonstrate%20the%20effectiveness%20of%20each%20component%20of%20our%20pipeline%20and%0Aits%20impact%20on%20the%20end-to-end%20performance.%20Our%20method%20achieves%20new%0Astate-of-the-art%20performance%20on%20the%20cross-domain%20challenging%20BIRD%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16755v2&entry.124074799=Read"},
{"title": "Towards Reducing Data Acquisition and Labeling for Defect Detection\n  using Simulated Data", "author": "Lukas Malte Kemeter and Rasmus Hvingelby and Paulina Sierak and Tobias Sch\u00f6n and Bishwajit Gosswam", "abstract": "  In many manufacturing settings, annotating data for machine learning and\ncomputer vision is costly, but synthetic data can be generated at significantly\nlower cost. Substituting the real-world data with synthetic data is therefore\nappealing for many machine learning applications that require large amounts of\ntraining data. However, relying solely on synthetic data is frequently\ninadequate for effectively training models that perform well on real-world\ndata, primarily due to domain shifts between the synthetic and real-world data.\nWe discuss approaches for dealing with such a domain shift when detecting\ndefects in X-ray scans of aluminium wheels. Using both simulated and real-world\nX-ray images, we train an object detection model with different strategies to\nidentify the training approach that generates the best detection results while\nminimising the demand for annotated real-world training samples. Our\npreliminary findings suggest that the sim-2-real domain adaptation approach is\nmore cost-efficient than a fully supervised oracle - if the total number of\navailable annotated samples is fixed. Given a certain number of labeled\nreal-world samples, training on a mix of synthetic and unlabeled real-world\ndata achieved comparable or even better detection results at significantly\nlower cost. We argue that future research into the cost-efficiency of different\ntraining strategies is important for a better understanding of how to allocate\nbudget in applied machine learning projects.\n", "link": "http://arxiv.org/abs/2406.19175v1", "date": "2024-06-27", "relevancy": 1.0094, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5319}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4929}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Reducing%20Data%20Acquisition%20and%20Labeling%20for%20Defect%20Detection%0A%20%20using%20Simulated%20Data&body=Title%3A%20Towards%20Reducing%20Data%20Acquisition%20and%20Labeling%20for%20Defect%20Detection%0A%20%20using%20Simulated%20Data%0AAuthor%3A%20Lukas%20Malte%20Kemeter%20and%20Rasmus%20Hvingelby%20and%20Paulina%20Sierak%20and%20Tobias%20Sch%C3%B6n%20and%20Bishwajit%20Gosswam%0AAbstract%3A%20%20%20In%20many%20manufacturing%20settings%2C%20annotating%20data%20for%20machine%20learning%20and%0Acomputer%20vision%20is%20costly%2C%20but%20synthetic%20data%20can%20be%20generated%20at%20significantly%0Alower%20cost.%20Substituting%20the%20real-world%20data%20with%20synthetic%20data%20is%20therefore%0Aappealing%20for%20many%20machine%20learning%20applications%20that%20require%20large%20amounts%20of%0Atraining%20data.%20However%2C%20relying%20solely%20on%20synthetic%20data%20is%20frequently%0Ainadequate%20for%20effectively%20training%20models%20that%20perform%20well%20on%20real-world%0Adata%2C%20primarily%20due%20to%20domain%20shifts%20between%20the%20synthetic%20and%20real-world%20data.%0AWe%20discuss%20approaches%20for%20dealing%20with%20such%20a%20domain%20shift%20when%20detecting%0Adefects%20in%20X-ray%20scans%20of%20aluminium%20wheels.%20Using%20both%20simulated%20and%20real-world%0AX-ray%20images%2C%20we%20train%20an%20object%20detection%20model%20with%20different%20strategies%20to%0Aidentify%20the%20training%20approach%20that%20generates%20the%20best%20detection%20results%20while%0Aminimising%20the%20demand%20for%20annotated%20real-world%20training%20samples.%20Our%0Apreliminary%20findings%20suggest%20that%20the%20sim-2-real%20domain%20adaptation%20approach%20is%0Amore%20cost-efficient%20than%20a%20fully%20supervised%20oracle%20-%20if%20the%20total%20number%20of%0Aavailable%20annotated%20samples%20is%20fixed.%20Given%20a%20certain%20number%20of%20labeled%0Areal-world%20samples%2C%20training%20on%20a%20mix%20of%20synthetic%20and%20unlabeled%20real-world%0Adata%20achieved%20comparable%20or%20even%20better%20detection%20results%20at%20significantly%0Alower%20cost.%20We%20argue%20that%20future%20research%20into%20the%20cost-efficiency%20of%20different%0Atraining%20strategies%20is%20important%20for%20a%20better%20understanding%20of%20how%20to%20allocate%0Abudget%20in%20applied%20machine%20learning%20projects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Reducing%2520Data%2520Acquisition%2520and%2520Labeling%2520for%2520Defect%2520Detection%250A%2520%2520using%2520Simulated%2520Data%26entry.906535625%3DLukas%2520Malte%2520Kemeter%2520and%2520Rasmus%2520Hvingelby%2520and%2520Paulina%2520Sierak%2520and%2520Tobias%2520Sch%25C3%25B6n%2520and%2520Bishwajit%2520Gosswam%26entry.1292438233%3D%2520%2520In%2520many%2520manufacturing%2520settings%252C%2520annotating%2520data%2520for%2520machine%2520learning%2520and%250Acomputer%2520vision%2520is%2520costly%252C%2520but%2520synthetic%2520data%2520can%2520be%2520generated%2520at%2520significantly%250Alower%2520cost.%2520Substituting%2520the%2520real-world%2520data%2520with%2520synthetic%2520data%2520is%2520therefore%250Aappealing%2520for%2520many%2520machine%2520learning%2520applications%2520that%2520require%2520large%2520amounts%2520of%250Atraining%2520data.%2520However%252C%2520relying%2520solely%2520on%2520synthetic%2520data%2520is%2520frequently%250Ainadequate%2520for%2520effectively%2520training%2520models%2520that%2520perform%2520well%2520on%2520real-world%250Adata%252C%2520primarily%2520due%2520to%2520domain%2520shifts%2520between%2520the%2520synthetic%2520and%2520real-world%2520data.%250AWe%2520discuss%2520approaches%2520for%2520dealing%2520with%2520such%2520a%2520domain%2520shift%2520when%2520detecting%250Adefects%2520in%2520X-ray%2520scans%2520of%2520aluminium%2520wheels.%2520Using%2520both%2520simulated%2520and%2520real-world%250AX-ray%2520images%252C%2520we%2520train%2520an%2520object%2520detection%2520model%2520with%2520different%2520strategies%2520to%250Aidentify%2520the%2520training%2520approach%2520that%2520generates%2520the%2520best%2520detection%2520results%2520while%250Aminimising%2520the%2520demand%2520for%2520annotated%2520real-world%2520training%2520samples.%2520Our%250Apreliminary%2520findings%2520suggest%2520that%2520the%2520sim-2-real%2520domain%2520adaptation%2520approach%2520is%250Amore%2520cost-efficient%2520than%2520a%2520fully%2520supervised%2520oracle%2520-%2520if%2520the%2520total%2520number%2520of%250Aavailable%2520annotated%2520samples%2520is%2520fixed.%2520Given%2520a%2520certain%2520number%2520of%2520labeled%250Areal-world%2520samples%252C%2520training%2520on%2520a%2520mix%2520of%2520synthetic%2520and%2520unlabeled%2520real-world%250Adata%2520achieved%2520comparable%2520or%2520even%2520better%2520detection%2520results%2520at%2520significantly%250Alower%2520cost.%2520We%2520argue%2520that%2520future%2520research%2520into%2520the%2520cost-efficiency%2520of%2520different%250Atraining%2520strategies%2520is%2520important%2520for%2520a%2520better%2520understanding%2520of%2520how%2520to%2520allocate%250Abudget%2520in%2520applied%2520machine%2520learning%2520projects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Reducing%20Data%20Acquisition%20and%20Labeling%20for%20Defect%20Detection%0A%20%20using%20Simulated%20Data&entry.906535625=Lukas%20Malte%20Kemeter%20and%20Rasmus%20Hvingelby%20and%20Paulina%20Sierak%20and%20Tobias%20Sch%C3%B6n%20and%20Bishwajit%20Gosswam&entry.1292438233=%20%20In%20many%20manufacturing%20settings%2C%20annotating%20data%20for%20machine%20learning%20and%0Acomputer%20vision%20is%20costly%2C%20but%20synthetic%20data%20can%20be%20generated%20at%20significantly%0Alower%20cost.%20Substituting%20the%20real-world%20data%20with%20synthetic%20data%20is%20therefore%0Aappealing%20for%20many%20machine%20learning%20applications%20that%20require%20large%20amounts%20of%0Atraining%20data.%20However%2C%20relying%20solely%20on%20synthetic%20data%20is%20frequently%0Ainadequate%20for%20effectively%20training%20models%20that%20perform%20well%20on%20real-world%0Adata%2C%20primarily%20due%20to%20domain%20shifts%20between%20the%20synthetic%20and%20real-world%20data.%0AWe%20discuss%20approaches%20for%20dealing%20with%20such%20a%20domain%20shift%20when%20detecting%0Adefects%20in%20X-ray%20scans%20of%20aluminium%20wheels.%20Using%20both%20simulated%20and%20real-world%0AX-ray%20images%2C%20we%20train%20an%20object%20detection%20model%20with%20different%20strategies%20to%0Aidentify%20the%20training%20approach%20that%20generates%20the%20best%20detection%20results%20while%0Aminimising%20the%20demand%20for%20annotated%20real-world%20training%20samples.%20Our%0Apreliminary%20findings%20suggest%20that%20the%20sim-2-real%20domain%20adaptation%20approach%20is%0Amore%20cost-efficient%20than%20a%20fully%20supervised%20oracle%20-%20if%20the%20total%20number%20of%0Aavailable%20annotated%20samples%20is%20fixed.%20Given%20a%20certain%20number%20of%20labeled%0Areal-world%20samples%2C%20training%20on%20a%20mix%20of%20synthetic%20and%20unlabeled%20real-world%0Adata%20achieved%20comparable%20or%20even%20better%20detection%20results%20at%20significantly%0Alower%20cost.%20We%20argue%20that%20future%20research%20into%20the%20cost-efficiency%20of%20different%0Atraining%20strategies%20is%20important%20for%20a%20better%20understanding%20of%20how%20to%20allocate%0Abudget%20in%20applied%20machine%20learning%20projects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19175v1&entry.124074799=Read"},
{"title": "Cosserat Rod Modeling and Validation for a Soft Continuum Robot with\n  Self-Controllable Variable Curvature", "author": "Xinran Wang and Nicolas Rojas", "abstract": "  This paper introduces a Cosserat rod based mathematical model for modeling a\nself-controllable variable curvature soft continuum robot. This soft continuum\nrobot has a hollow inner channel and was developed with the ability to perform\nvariable curvature utilizing a growing spine. The growing spine is able to grow\nand retract while modifies its stiffness through milli-size particle (glass\nbubble) granular jamming. This soft continuum robot can then perform continuous\ncurvature variation, unlike previous approaches whose curvature variation is\ndiscrete and depends on the number of locking mechanisms or manual\nconfigurations. The robot poses an emergent modeling problem due to the\nvariable stiffness growing spine which is addressed in this paper. We\ninvestigate the property of growing spine stiffness and incorporate it into the\nCosserat rod model by implementing a combined stiffness approach. We conduct\nexperiments with the soft continuum robot in various configurations and\ncompared the results with our developed mathematical model. The results show\nthat the mathematical model based on the adapted Cosserat rod matches the\nexperimental results with only a 3.3\\% error with respect to the length of the\nsoft continuum robot.\n", "link": "http://arxiv.org/abs/2402.12315v2", "date": "2024-06-27", "relevancy": 1.3668, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5025}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4469}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cosserat%20Rod%20Modeling%20and%20Validation%20for%20a%20Soft%20Continuum%20Robot%20with%0A%20%20Self-Controllable%20Variable%20Curvature&body=Title%3A%20Cosserat%20Rod%20Modeling%20and%20Validation%20for%20a%20Soft%20Continuum%20Robot%20with%0A%20%20Self-Controllable%20Variable%20Curvature%0AAuthor%3A%20Xinran%20Wang%20and%20Nicolas%20Rojas%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20Cosserat%20rod%20based%20mathematical%20model%20for%20modeling%20a%0Aself-controllable%20variable%20curvature%20soft%20continuum%20robot.%20This%20soft%20continuum%0Arobot%20has%20a%20hollow%20inner%20channel%20and%20was%20developed%20with%20the%20ability%20to%20perform%0Avariable%20curvature%20utilizing%20a%20growing%20spine.%20The%20growing%20spine%20is%20able%20to%20grow%0Aand%20retract%20while%20modifies%20its%20stiffness%20through%20milli-size%20particle%20%28glass%0Abubble%29%20granular%20jamming.%20This%20soft%20continuum%20robot%20can%20then%20perform%20continuous%0Acurvature%20variation%2C%20unlike%20previous%20approaches%20whose%20curvature%20variation%20is%0Adiscrete%20and%20depends%20on%20the%20number%20of%20locking%20mechanisms%20or%20manual%0Aconfigurations.%20The%20robot%20poses%20an%20emergent%20modeling%20problem%20due%20to%20the%0Avariable%20stiffness%20growing%20spine%20which%20is%20addressed%20in%20this%20paper.%20We%0Ainvestigate%20the%20property%20of%20growing%20spine%20stiffness%20and%20incorporate%20it%20into%20the%0ACosserat%20rod%20model%20by%20implementing%20a%20combined%20stiffness%20approach.%20We%20conduct%0Aexperiments%20with%20the%20soft%20continuum%20robot%20in%20various%20configurations%20and%0Acompared%20the%20results%20with%20our%20developed%20mathematical%20model.%20The%20results%20show%0Athat%20the%20mathematical%20model%20based%20on%20the%20adapted%20Cosserat%20rod%20matches%20the%0Aexperimental%20results%20with%20only%20a%203.3%5C%25%20error%20with%20respect%20to%20the%20length%20of%20the%0Asoft%20continuum%20robot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12315v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCosserat%2520Rod%2520Modeling%2520and%2520Validation%2520for%2520a%2520Soft%2520Continuum%2520Robot%2520with%250A%2520%2520Self-Controllable%2520Variable%2520Curvature%26entry.906535625%3DXinran%2520Wang%2520and%2520Nicolas%2520Rojas%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520Cosserat%2520rod%2520based%2520mathematical%2520model%2520for%2520modeling%2520a%250Aself-controllable%2520variable%2520curvature%2520soft%2520continuum%2520robot.%2520This%2520soft%2520continuum%250Arobot%2520has%2520a%2520hollow%2520inner%2520channel%2520and%2520was%2520developed%2520with%2520the%2520ability%2520to%2520perform%250Avariable%2520curvature%2520utilizing%2520a%2520growing%2520spine.%2520The%2520growing%2520spine%2520is%2520able%2520to%2520grow%250Aand%2520retract%2520while%2520modifies%2520its%2520stiffness%2520through%2520milli-size%2520particle%2520%2528glass%250Abubble%2529%2520granular%2520jamming.%2520This%2520soft%2520continuum%2520robot%2520can%2520then%2520perform%2520continuous%250Acurvature%2520variation%252C%2520unlike%2520previous%2520approaches%2520whose%2520curvature%2520variation%2520is%250Adiscrete%2520and%2520depends%2520on%2520the%2520number%2520of%2520locking%2520mechanisms%2520or%2520manual%250Aconfigurations.%2520The%2520robot%2520poses%2520an%2520emergent%2520modeling%2520problem%2520due%2520to%2520the%250Avariable%2520stiffness%2520growing%2520spine%2520which%2520is%2520addressed%2520in%2520this%2520paper.%2520We%250Ainvestigate%2520the%2520property%2520of%2520growing%2520spine%2520stiffness%2520and%2520incorporate%2520it%2520into%2520the%250ACosserat%2520rod%2520model%2520by%2520implementing%2520a%2520combined%2520stiffness%2520approach.%2520We%2520conduct%250Aexperiments%2520with%2520the%2520soft%2520continuum%2520robot%2520in%2520various%2520configurations%2520and%250Acompared%2520the%2520results%2520with%2520our%2520developed%2520mathematical%2520model.%2520The%2520results%2520show%250Athat%2520the%2520mathematical%2520model%2520based%2520on%2520the%2520adapted%2520Cosserat%2520rod%2520matches%2520the%250Aexperimental%2520results%2520with%2520only%2520a%25203.3%255C%2525%2520error%2520with%2520respect%2520to%2520the%2520length%2520of%2520the%250Asoft%2520continuum%2520robot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12315v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cosserat%20Rod%20Modeling%20and%20Validation%20for%20a%20Soft%20Continuum%20Robot%20with%0A%20%20Self-Controllable%20Variable%20Curvature&entry.906535625=Xinran%20Wang%20and%20Nicolas%20Rojas&entry.1292438233=%20%20This%20paper%20introduces%20a%20Cosserat%20rod%20based%20mathematical%20model%20for%20modeling%20a%0Aself-controllable%20variable%20curvature%20soft%20continuum%20robot.%20This%20soft%20continuum%0Arobot%20has%20a%20hollow%20inner%20channel%20and%20was%20developed%20with%20the%20ability%20to%20perform%0Avariable%20curvature%20utilizing%20a%20growing%20spine.%20The%20growing%20spine%20is%20able%20to%20grow%0Aand%20retract%20while%20modifies%20its%20stiffness%20through%20milli-size%20particle%20%28glass%0Abubble%29%20granular%20jamming.%20This%20soft%20continuum%20robot%20can%20then%20perform%20continuous%0Acurvature%20variation%2C%20unlike%20previous%20approaches%20whose%20curvature%20variation%20is%0Adiscrete%20and%20depends%20on%20the%20number%20of%20locking%20mechanisms%20or%20manual%0Aconfigurations.%20The%20robot%20poses%20an%20emergent%20modeling%20problem%20due%20to%20the%0Avariable%20stiffness%20growing%20spine%20which%20is%20addressed%20in%20this%20paper.%20We%0Ainvestigate%20the%20property%20of%20growing%20spine%20stiffness%20and%20incorporate%20it%20into%20the%0ACosserat%20rod%20model%20by%20implementing%20a%20combined%20stiffness%20approach.%20We%20conduct%0Aexperiments%20with%20the%20soft%20continuum%20robot%20in%20various%20configurations%20and%0Acompared%20the%20results%20with%20our%20developed%20mathematical%20model.%20The%20results%20show%0Athat%20the%20mathematical%20model%20based%20on%20the%20adapted%20Cosserat%20rod%20matches%20the%0Aexperimental%20results%20with%20only%20a%203.3%5C%25%20error%20with%20respect%20to%20the%20length%20of%20the%0Asoft%20continuum%20robot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12315v2&entry.124074799=Read"},
{"title": "Fundamental Problems With Model Editing: How Should Rational Belief\n  Revision Work in LLMs?", "author": "Peter Hase and Thomas Hofweber and Xiang Zhou and Elias Stengel-Eskin and Mohit Bansal", "abstract": "  The model editing problem concerns how language models should learn new facts\nabout the world over time. While empirical research on model editing has drawn\nwidespread attention, the conceptual foundations of model editing remain shaky\n-- perhaps unsurprisingly, since model editing is essentially belief revision,\na storied problem in philosophy that has eluded succinct solutions for decades.\nModel editing nonetheless demands a solution, since we need to be able to\ncontrol the knowledge within language models. With this goal in mind, this\npaper critiques the standard formulation of the model editing problem and\nproposes a formal testbed for model editing research. We first describe 12 open\nproblems with model editing, based on challenges with (1) defining the problem,\n(2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the\nfirst place. Many of these challenges are extremely difficult to address, e.g.\ndetermining far-reaching consequences of edits, labeling probabilistic\nentailments between facts, and updating beliefs of agent simulators. Next, we\nintroduce a semi-synthetic dataset for model editing based on Wikidata, where\nwe can evaluate edits against labels given by an idealized Bayesian agent. This\nenables us to say exactly how belief revision in language models falls short of\na desirable epistemic standard. We encourage further research exploring\nsettings where such a gold standard can be compared against. Our code is\npublicly available at: https://github.com/peterbhase/LLM-belief-revision\n", "link": "http://arxiv.org/abs/2406.19354v1", "date": "2024-06-27", "relevancy": 0.8982, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4528}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4508}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fundamental%20Problems%20With%20Model%20Editing%3A%20How%20Should%20Rational%20Belief%0A%20%20Revision%20Work%20in%20LLMs%3F&body=Title%3A%20Fundamental%20Problems%20With%20Model%20Editing%3A%20How%20Should%20Rational%20Belief%0A%20%20Revision%20Work%20in%20LLMs%3F%0AAuthor%3A%20Peter%20Hase%20and%20Thomas%20Hofweber%20and%20Xiang%20Zhou%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20The%20model%20editing%20problem%20concerns%20how%20language%20models%20should%20learn%20new%20facts%0Aabout%20the%20world%20over%20time.%20While%20empirical%20research%20on%20model%20editing%20has%20drawn%0Awidespread%20attention%2C%20the%20conceptual%20foundations%20of%20model%20editing%20remain%20shaky%0A--%20perhaps%20unsurprisingly%2C%20since%20model%20editing%20is%20essentially%20belief%20revision%2C%0Aa%20storied%20problem%20in%20philosophy%20that%20has%20eluded%20succinct%20solutions%20for%20decades.%0AModel%20editing%20nonetheless%20demands%20a%20solution%2C%20since%20we%20need%20to%20be%20able%20to%0Acontrol%20the%20knowledge%20within%20language%20models.%20With%20this%20goal%20in%20mind%2C%20this%0Apaper%20critiques%20the%20standard%20formulation%20of%20the%20model%20editing%20problem%20and%0Aproposes%20a%20formal%20testbed%20for%20model%20editing%20research.%20We%20first%20describe%2012%20open%0Aproblems%20with%20model%20editing%2C%20based%20on%20challenges%20with%20%281%29%20defining%20the%20problem%2C%0A%282%29%20developing%20benchmarks%2C%20and%20%283%29%20assuming%20LLMs%20have%20editable%20beliefs%20in%20the%0Afirst%20place.%20Many%20of%20these%20challenges%20are%20extremely%20difficult%20to%20address%2C%20e.g.%0Adetermining%20far-reaching%20consequences%20of%20edits%2C%20labeling%20probabilistic%0Aentailments%20between%20facts%2C%20and%20updating%20beliefs%20of%20agent%20simulators.%20Next%2C%20we%0Aintroduce%20a%20semi-synthetic%20dataset%20for%20model%20editing%20based%20on%20Wikidata%2C%20where%0Awe%20can%20evaluate%20edits%20against%20labels%20given%20by%20an%20idealized%20Bayesian%20agent.%20This%0Aenables%20us%20to%20say%20exactly%20how%20belief%20revision%20in%20language%20models%20falls%20short%20of%0Aa%20desirable%20epistemic%20standard.%20We%20encourage%20further%20research%20exploring%0Asettings%20where%20such%20a%20gold%20standard%20can%20be%20compared%20against.%20Our%20code%20is%0Apublicly%20available%20at%3A%20https%3A//github.com/peterbhase/LLM-belief-revision%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19354v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFundamental%2520Problems%2520With%2520Model%2520Editing%253A%2520How%2520Should%2520Rational%2520Belief%250A%2520%2520Revision%2520Work%2520in%2520LLMs%253F%26entry.906535625%3DPeter%2520Hase%2520and%2520Thomas%2520Hofweber%2520and%2520Xiang%2520Zhou%2520and%2520Elias%2520Stengel-Eskin%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520The%2520model%2520editing%2520problem%2520concerns%2520how%2520language%2520models%2520should%2520learn%2520new%2520facts%250Aabout%2520the%2520world%2520over%2520time.%2520While%2520empirical%2520research%2520on%2520model%2520editing%2520has%2520drawn%250Awidespread%2520attention%252C%2520the%2520conceptual%2520foundations%2520of%2520model%2520editing%2520remain%2520shaky%250A--%2520perhaps%2520unsurprisingly%252C%2520since%2520model%2520editing%2520is%2520essentially%2520belief%2520revision%252C%250Aa%2520storied%2520problem%2520in%2520philosophy%2520that%2520has%2520eluded%2520succinct%2520solutions%2520for%2520decades.%250AModel%2520editing%2520nonetheless%2520demands%2520a%2520solution%252C%2520since%2520we%2520need%2520to%2520be%2520able%2520to%250Acontrol%2520the%2520knowledge%2520within%2520language%2520models.%2520With%2520this%2520goal%2520in%2520mind%252C%2520this%250Apaper%2520critiques%2520the%2520standard%2520formulation%2520of%2520the%2520model%2520editing%2520problem%2520and%250Aproposes%2520a%2520formal%2520testbed%2520for%2520model%2520editing%2520research.%2520We%2520first%2520describe%252012%2520open%250Aproblems%2520with%2520model%2520editing%252C%2520based%2520on%2520challenges%2520with%2520%25281%2529%2520defining%2520the%2520problem%252C%250A%25282%2529%2520developing%2520benchmarks%252C%2520and%2520%25283%2529%2520assuming%2520LLMs%2520have%2520editable%2520beliefs%2520in%2520the%250Afirst%2520place.%2520Many%2520of%2520these%2520challenges%2520are%2520extremely%2520difficult%2520to%2520address%252C%2520e.g.%250Adetermining%2520far-reaching%2520consequences%2520of%2520edits%252C%2520labeling%2520probabilistic%250Aentailments%2520between%2520facts%252C%2520and%2520updating%2520beliefs%2520of%2520agent%2520simulators.%2520Next%252C%2520we%250Aintroduce%2520a%2520semi-synthetic%2520dataset%2520for%2520model%2520editing%2520based%2520on%2520Wikidata%252C%2520where%250Awe%2520can%2520evaluate%2520edits%2520against%2520labels%2520given%2520by%2520an%2520idealized%2520Bayesian%2520agent.%2520This%250Aenables%2520us%2520to%2520say%2520exactly%2520how%2520belief%2520revision%2520in%2520language%2520models%2520falls%2520short%2520of%250Aa%2520desirable%2520epistemic%2520standard.%2520We%2520encourage%2520further%2520research%2520exploring%250Asettings%2520where%2520such%2520a%2520gold%2520standard%2520can%2520be%2520compared%2520against.%2520Our%2520code%2520is%250Apublicly%2520available%2520at%253A%2520https%253A//github.com/peterbhase/LLM-belief-revision%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19354v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fundamental%20Problems%20With%20Model%20Editing%3A%20How%20Should%20Rational%20Belief%0A%20%20Revision%20Work%20in%20LLMs%3F&entry.906535625=Peter%20Hase%20and%20Thomas%20Hofweber%20and%20Xiang%20Zhou%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal&entry.1292438233=%20%20The%20model%20editing%20problem%20concerns%20how%20language%20models%20should%20learn%20new%20facts%0Aabout%20the%20world%20over%20time.%20While%20empirical%20research%20on%20model%20editing%20has%20drawn%0Awidespread%20attention%2C%20the%20conceptual%20foundations%20of%20model%20editing%20remain%20shaky%0A--%20perhaps%20unsurprisingly%2C%20since%20model%20editing%20is%20essentially%20belief%20revision%2C%0Aa%20storied%20problem%20in%20philosophy%20that%20has%20eluded%20succinct%20solutions%20for%20decades.%0AModel%20editing%20nonetheless%20demands%20a%20solution%2C%20since%20we%20need%20to%20be%20able%20to%0Acontrol%20the%20knowledge%20within%20language%20models.%20With%20this%20goal%20in%20mind%2C%20this%0Apaper%20critiques%20the%20standard%20formulation%20of%20the%20model%20editing%20problem%20and%0Aproposes%20a%20formal%20testbed%20for%20model%20editing%20research.%20We%20first%20describe%2012%20open%0Aproblems%20with%20model%20editing%2C%20based%20on%20challenges%20with%20%281%29%20defining%20the%20problem%2C%0A%282%29%20developing%20benchmarks%2C%20and%20%283%29%20assuming%20LLMs%20have%20editable%20beliefs%20in%20the%0Afirst%20place.%20Many%20of%20these%20challenges%20are%20extremely%20difficult%20to%20address%2C%20e.g.%0Adetermining%20far-reaching%20consequences%20of%20edits%2C%20labeling%20probabilistic%0Aentailments%20between%20facts%2C%20and%20updating%20beliefs%20of%20agent%20simulators.%20Next%2C%20we%0Aintroduce%20a%20semi-synthetic%20dataset%20for%20model%20editing%20based%20on%20Wikidata%2C%20where%0Awe%20can%20evaluate%20edits%20against%20labels%20given%20by%20an%20idealized%20Bayesian%20agent.%20This%0Aenables%20us%20to%20say%20exactly%20how%20belief%20revision%20in%20language%20models%20falls%20short%20of%0Aa%20desirable%20epistemic%20standard.%20We%20encourage%20further%20research%20exploring%0Asettings%20where%20such%20a%20gold%20standard%20can%20be%20compared%20against.%20Our%20code%20is%0Apublicly%20available%20at%3A%20https%3A//github.com/peterbhase/LLM-belief-revision%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19354v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


