<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240428.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Image Copy-Move Forgery Detection via Deep PatchMatch and Pairwise\n  Ranking Learning", "author": "Yuanman Li and Yingjie He and Changsheng Chen and Li Dong and Bin Li and Jiantao Zhou and Xia Li", "abstract": "  Recent advances in deep learning algorithms have shown impressive progress in\nimage copy-move forgery detection (CMFD). However, these algorithms lack\ngeneralizability in practical scenarios where the copied regions are not\npresent in the training images, or the cloned regions are part of the\nbackground. Additionally, these algorithms utilize convolution operations to\ndistinguish source and target regions, leading to unsatisfactory results when\nthe target regions blend well with the background. To address these\nlimitations, this study proposes a novel end-to-end CMFD framework that\nintegrates the strengths of conventional and deep learning methods.\nSpecifically, the study develops a deep cross-scale PatchMatch (PM) method that\nis customized for CMFD to locate copy-move regions. Unlike existing deep\nmodels, our approach utilizes features extracted from high-resolution scales to\nseek explicit and reliable point-to-point matching between source and target\nregions. Furthermore, we propose a novel pairwise rank learning framework to\nseparate source and target regions. By leveraging the strong prior of\npoint-to-point matches, the framework can identify subtle differences and\neffectively discriminate between source and target regions, even when the\ntarget regions blend well with the background. Our framework is fully\ndifferentiable and can be trained end-to-end. Comprehensive experimental\nresults highlight the remarkable generalizability of our scheme across various\ncopy-move scenarios, significantly outperforming existing methods.\n", "link": "http://arxiv.org/abs/2404.17310v1", "date": "2024-04-26", "relevancy": 2.8004, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6089}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5471}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5242}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Image%20Copy-Move%20Forgery%20Detection%20via%20Deep%20PatchMatch%20and%20Pairwise%0A%20%20Ranking%20Learning&body=Title%3A%20Image%20Copy-Move%20Forgery%20Detection%20via%20Deep%20PatchMatch%20and%20Pairwise%0A%20%20Ranking%20Learning%0AAuthor%3A%20Yuanman%20Li%20and%20Yingjie%20He%20and%20Changsheng%20Chen%20and%20Li%20Dong%20and%20Bin%20Li%20and%20Jiantao%20Zhou%20and%20Xia%20Li%0AAbstract%3A%20%20%20Recent%20advances%20in%20deep%20learning%20algorithms%20have%20shown%20impressive%20progress%20in%0Aimage%20copy-move%20forgery%20detection%20%28CMFD%29.%20However%2C%20these%20algorithms%20lack%0Ageneralizability%20in%20practical%20scenarios%20where%20the%20copied%20regions%20are%20not%0Apresent%20in%20the%20training%20images%2C%20or%20the%20cloned%20regions%20are%20part%20of%20the%0Abackground.%20Additionally%2C%20these%20algorithms%20utilize%20convolution%20operations%20to%0Adistinguish%20source%20and%20target%20regions%2C%20leading%20to%20unsatisfactory%20results%20when%0Athe%20target%20regions%20blend%20well%20with%20the%20background.%20To%20address%20these%0Alimitations%2C%20this%20study%20proposes%20a%20novel%20end-to-end%20CMFD%20framework%20that%0Aintegrates%20the%20strengths%20of%20conventional%20and%20deep%20learning%20methods.%0ASpecifically%2C%20the%20study%20develops%20a%20deep%20cross-scale%20PatchMatch%20%28PM%29%20method%20that%0Ais%20customized%20for%20CMFD%20to%20locate%20copy-move%20regions.%20Unlike%20existing%20deep%0Amodels%2C%20our%20approach%20utilizes%20features%20extracted%20from%20high-resolution%20scales%20to%0Aseek%20explicit%20and%20reliable%20point-to-point%20matching%20between%20source%20and%20target%0Aregions.%20Furthermore%2C%20we%20propose%20a%20novel%20pairwise%20rank%20learning%20framework%20to%0Aseparate%20source%20and%20target%20regions.%20By%20leveraging%20the%20strong%20prior%20of%0Apoint-to-point%20matches%2C%20the%20framework%20can%20identify%20subtle%20differences%20and%0Aeffectively%20discriminate%20between%20source%20and%20target%20regions%2C%20even%20when%20the%0Atarget%20regions%20blend%20well%20with%20the%20background.%20Our%20framework%20is%20fully%0Adifferentiable%20and%20can%20be%20trained%20end-to-end.%20Comprehensive%20experimental%0Aresults%20highlight%20the%20remarkable%20generalizability%20of%20our%20scheme%20across%20various%0Acopy-move%20scenarios%2C%20significantly%20outperforming%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17310v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Copy-Move%20Forgery%20Detection%20via%20Deep%20PatchMatch%20and%20Pairwise%0A%20%20Ranking%20Learning&entry.906535625=Yuanman%20Li%20and%20Yingjie%20He%20and%20Changsheng%20Chen%20and%20Li%20Dong%20and%20Bin%20Li%20and%20Jiantao%20Zhou%20and%20Xia%20Li&entry.1292438233=%20%20Recent%20advances%20in%20deep%20learning%20algorithms%20have%20shown%20impressive%20progress%20in%0Aimage%20copy-move%20forgery%20detection%20%28CMFD%29.%20However%2C%20these%20algorithms%20lack%0Ageneralizability%20in%20practical%20scenarios%20where%20the%20copied%20regions%20are%20not%0Apresent%20in%20the%20training%20images%2C%20or%20the%20cloned%20regions%20are%20part%20of%20the%0Abackground.%20Additionally%2C%20these%20algorithms%20utilize%20convolution%20operations%20to%0Adistinguish%20source%20and%20target%20regions%2C%20leading%20to%20unsatisfactory%20results%20when%0Athe%20target%20regions%20blend%20well%20with%20the%20background.%20To%20address%20these%0Alimitations%2C%20this%20study%20proposes%20a%20novel%20end-to-end%20CMFD%20framework%20that%0Aintegrates%20the%20strengths%20of%20conventional%20and%20deep%20learning%20methods.%0ASpecifically%2C%20the%20study%20develops%20a%20deep%20cross-scale%20PatchMatch%20%28PM%29%20method%20that%0Ais%20customized%20for%20CMFD%20to%20locate%20copy-move%20regions.%20Unlike%20existing%20deep%0Amodels%2C%20our%20approach%20utilizes%20features%20extracted%20from%20high-resolution%20scales%20to%0Aseek%20explicit%20and%20reliable%20point-to-point%20matching%20between%20source%20and%20target%0Aregions.%20Furthermore%2C%20we%20propose%20a%20novel%20pairwise%20rank%20learning%20framework%20to%0Aseparate%20source%20and%20target%20regions.%20By%20leveraging%20the%20strong%20prior%20of%0Apoint-to-point%20matches%2C%20the%20framework%20can%20identify%20subtle%20differences%20and%0Aeffectively%20discriminate%20between%20source%20and%20target%20regions%2C%20even%20when%20the%0Atarget%20regions%20blend%20well%20with%20the%20background.%20Our%20framework%20is%20fully%0Adifferentiable%20and%20can%20be%20trained%20end-to-end.%20Comprehensive%20experimental%0Aresults%20highlight%20the%20remarkable%20generalizability%20of%20our%20scheme%20across%20various%0Acopy-move%20scenarios%2C%20significantly%20outperforming%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17310v1&entry.124074799=Read"},
{"title": "Probing Conceptual Understanding of Large Visual-Language Models", "author": "Madeline Schiappa and Raiyaan Abdullah and Shehreen Azad and Jared Claypoole and Michael Cogswell and Ajay Divakaran and Yogesh Rawat", "abstract": "  In recent years large visual-language (V+L) models have achieved great\nsuccess in various downstream tasks. However, it is not well studied whether\nthese models have a conceptual grasp of the visual content. In this work we\nfocus on conceptual understanding of these large V+L models. To facilitate this\nstudy, we propose novel benchmarking datasets for probing three different\naspects of content understanding, 1) \\textit{relations}, 2)\n\\textit{composition}, and 3) \\textit{context}. Our probes are grounded in\ncognitive science and help determine if a V+L model can, for example, determine\nif snow garnished with a man is implausible, or if it can identify beach\nfurniture by knowing it is located on a beach. We experimented with many recent\nstate-of-the-art V+L models and observe that these models mostly \\textit{fail\nto demonstrate} a conceptual understanding. This study reveals several\ninteresting insights such as that \\textit{cross-attention} helps learning\nconceptual understanding, and that CNNs are better with \\textit{texture and\npatterns}, while Transformers are better at \\textit{color and shape}. We\nfurther utilize some of these insights and investigate a \\textit{simple\nfinetuning technique} that rewards the three conceptual understanding measures\nwith promising initial results. The proposed benchmarks will drive the\ncommunity to delve deeper into conceptual understanding and foster advancements\nin the capabilities of large V+L models. The code and dataset is available at:\n\\url{https://tinyurl.com/vlm-robustness}\n", "link": "http://arxiv.org/abs/2304.03659v3", "date": "2024-04-26", "relevancy": 2.7119, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5483}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5434}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5355}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Probing%20Conceptual%20Understanding%20of%20Large%20Visual-Language%20Models&body=Title%3A%20Probing%20Conceptual%20Understanding%20of%20Large%20Visual-Language%20Models%0AAuthor%3A%20Madeline%20Schiappa%20and%20Raiyaan%20Abdullah%20and%20Shehreen%20Azad%20and%20Jared%20Claypoole%20and%20Michael%20Cogswell%20and%20Ajay%20Divakaran%20and%20Yogesh%20Rawat%0AAbstract%3A%20%20%20In%20recent%20years%20large%20visual-language%20%28V%2BL%29%20models%20have%20achieved%20great%0Asuccess%20in%20various%20downstream%20tasks.%20However%2C%20it%20is%20not%20well%20studied%20whether%0Athese%20models%20have%20a%20conceptual%20grasp%20of%20the%20visual%20content.%20In%20this%20work%20we%0Afocus%20on%20conceptual%20understanding%20of%20these%20large%20V%2BL%20models.%20To%20facilitate%20this%0Astudy%2C%20we%20propose%20novel%20benchmarking%20datasets%20for%20probing%20three%20different%0Aaspects%20of%20content%20understanding%2C%201%29%20%5Ctextit%7Brelations%7D%2C%202%29%0A%5Ctextit%7Bcomposition%7D%2C%20and%203%29%20%5Ctextit%7Bcontext%7D.%20Our%20probes%20are%20grounded%20in%0Acognitive%20science%20and%20help%20determine%20if%20a%20V%2BL%20model%20can%2C%20for%20example%2C%20determine%0Aif%20snow%20garnished%20with%20a%20man%20is%20implausible%2C%20or%20if%20it%20can%20identify%20beach%0Afurniture%20by%20knowing%20it%20is%20located%20on%20a%20beach.%20We%20experimented%20with%20many%20recent%0Astate-of-the-art%20V%2BL%20models%20and%20observe%20that%20these%20models%20mostly%20%5Ctextit%7Bfail%0Ato%20demonstrate%7D%20a%20conceptual%20understanding.%20This%20study%20reveals%20several%0Ainteresting%20insights%20such%20as%20that%20%5Ctextit%7Bcross-attention%7D%20helps%20learning%0Aconceptual%20understanding%2C%20and%20that%20CNNs%20are%20better%20with%20%5Ctextit%7Btexture%20and%0Apatterns%7D%2C%20while%20Transformers%20are%20better%20at%20%5Ctextit%7Bcolor%20and%20shape%7D.%20We%0Afurther%20utilize%20some%20of%20these%20insights%20and%20investigate%20a%20%5Ctextit%7Bsimple%0Afinetuning%20technique%7D%20that%20rewards%20the%20three%20conceptual%20understanding%20measures%0Awith%20promising%20initial%20results.%20The%20proposed%20benchmarks%20will%20drive%20the%0Acommunity%20to%20delve%20deeper%20into%20conceptual%20understanding%20and%20foster%20advancements%0Ain%20the%20capabilities%20of%20large%20V%2BL%20models.%20The%20code%20and%20dataset%20is%20available%20at%3A%0A%5Curl%7Bhttps%3A//tinyurl.com/vlm-robustness%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.03659v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20Conceptual%20Understanding%20of%20Large%20Visual-Language%20Models&entry.906535625=Madeline%20Schiappa%20and%20Raiyaan%20Abdullah%20and%20Shehreen%20Azad%20and%20Jared%20Claypoole%20and%20Michael%20Cogswell%20and%20Ajay%20Divakaran%20and%20Yogesh%20Rawat&entry.1292438233=%20%20In%20recent%20years%20large%20visual-language%20%28V%2BL%29%20models%20have%20achieved%20great%0Asuccess%20in%20various%20downstream%20tasks.%20However%2C%20it%20is%20not%20well%20studied%20whether%0Athese%20models%20have%20a%20conceptual%20grasp%20of%20the%20visual%20content.%20In%20this%20work%20we%0Afocus%20on%20conceptual%20understanding%20of%20these%20large%20V%2BL%20models.%20To%20facilitate%20this%0Astudy%2C%20we%20propose%20novel%20benchmarking%20datasets%20for%20probing%20three%20different%0Aaspects%20of%20content%20understanding%2C%201%29%20%5Ctextit%7Brelations%7D%2C%202%29%0A%5Ctextit%7Bcomposition%7D%2C%20and%203%29%20%5Ctextit%7Bcontext%7D.%20Our%20probes%20are%20grounded%20in%0Acognitive%20science%20and%20help%20determine%20if%20a%20V%2BL%20model%20can%2C%20for%20example%2C%20determine%0Aif%20snow%20garnished%20with%20a%20man%20is%20implausible%2C%20or%20if%20it%20can%20identify%20beach%0Afurniture%20by%20knowing%20it%20is%20located%20on%20a%20beach.%20We%20experimented%20with%20many%20recent%0Astate-of-the-art%20V%2BL%20models%20and%20observe%20that%20these%20models%20mostly%20%5Ctextit%7Bfail%0Ato%20demonstrate%7D%20a%20conceptual%20understanding.%20This%20study%20reveals%20several%0Ainteresting%20insights%20such%20as%20that%20%5Ctextit%7Bcross-attention%7D%20helps%20learning%0Aconceptual%20understanding%2C%20and%20that%20CNNs%20are%20better%20with%20%5Ctextit%7Btexture%20and%0Apatterns%7D%2C%20while%20Transformers%20are%20better%20at%20%5Ctextit%7Bcolor%20and%20shape%7D.%20We%0Afurther%20utilize%20some%20of%20these%20insights%20and%20investigate%20a%20%5Ctextit%7Bsimple%0Afinetuning%20technique%7D%20that%20rewards%20the%20three%20conceptual%20understanding%20measures%0Awith%20promising%20initial%20results.%20The%20proposed%20benchmarks%20will%20drive%20the%0Acommunity%20to%20delve%20deeper%20into%20conceptual%20understanding%20and%20foster%20advancements%0Ain%20the%20capabilities%20of%20large%20V%2BL%20models.%20The%20code%20and%20dataset%20is%20available%20at%3A%0A%5Curl%7Bhttps%3A//tinyurl.com/vlm-robustness%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.03659v3&entry.124074799=Read"},
{"title": "Learning CNN on ViT: A Hybrid Model to Explicitly Class-specific\n  Boundaries for Domain Adaptation", "author": "Ba Hung Ngo and Nhat-Tuong Do-Tran and Tuan-Ngoc Nguyen and Hae-Gon Jeon and Tae Jong Choi", "abstract": "  Most domain adaptation (DA) methods are based on either a convolutional\nneural networks (CNNs) or a vision transformers (ViTs). They align the\ndistribution differences between domains as encoders without considering their\nunique characteristics. For instance, ViT excels in accuracy due to its\nsuperior ability to capture global representations, while CNN has an advantage\nin capturing local representations. This fact has led us to design a hybrid\nmethod to fully take advantage of both ViT and CNN, called Explicitly\nClass-specific Boundaries (ECB). ECB learns CNN on ViT to combine their\ndistinct strengths. In particular, we leverage ViT's properties to explicitly\nfind class-specific decision boundaries by maximizing the discrepancy between\nthe outputs of the two classifiers to detect target samples far from the source\nsupport. In contrast, the CNN encoder clusters target features based on the\npreviously defined class-specific boundaries by minimizing the discrepancy\nbetween the probabilities of the two classifiers. Finally, ViT and CNN mutually\nexchange knowledge to improve the quality of pseudo labels and reduce the\nknowledge discrepancies of these models. Compared to conventional DA methods,\nour ECB achieves superior performance, which verifies its effectiveness in this\nhybrid model. The project website can be found\nhttps://dotrannhattuong.github.io/ECB/website.\n", "link": "http://arxiv.org/abs/2403.18360v3", "date": "2024-04-26", "relevancy": 2.637, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5408}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5261}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5152}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20CNN%20on%20ViT%3A%20A%20Hybrid%20Model%20to%20Explicitly%20Class-specific%0A%20%20Boundaries%20for%20Domain%20Adaptation&body=Title%3A%20Learning%20CNN%20on%20ViT%3A%20A%20Hybrid%20Model%20to%20Explicitly%20Class-specific%0A%20%20Boundaries%20for%20Domain%20Adaptation%0AAuthor%3A%20Ba%20Hung%20Ngo%20and%20Nhat-Tuong%20Do-Tran%20and%20Tuan-Ngoc%20Nguyen%20and%20Hae-Gon%20Jeon%20and%20Tae%20Jong%20Choi%0AAbstract%3A%20%20%20Most%20domain%20adaptation%20%28DA%29%20methods%20are%20based%20on%20either%20a%20convolutional%0Aneural%20networks%20%28CNNs%29%20or%20a%20vision%20transformers%20%28ViTs%29.%20They%20align%20the%0Adistribution%20differences%20between%20domains%20as%20encoders%20without%20considering%20their%0Aunique%20characteristics.%20For%20instance%2C%20ViT%20excels%20in%20accuracy%20due%20to%20its%0Asuperior%20ability%20to%20capture%20global%20representations%2C%20while%20CNN%20has%20an%20advantage%0Ain%20capturing%20local%20representations.%20This%20fact%20has%20led%20us%20to%20design%20a%20hybrid%0Amethod%20to%20fully%20take%20advantage%20of%20both%20ViT%20and%20CNN%2C%20called%20Explicitly%0AClass-specific%20Boundaries%20%28ECB%29.%20ECB%20learns%20CNN%20on%20ViT%20to%20combine%20their%0Adistinct%20strengths.%20In%20particular%2C%20we%20leverage%20ViT%27s%20properties%20to%20explicitly%0Afind%20class-specific%20decision%20boundaries%20by%20maximizing%20the%20discrepancy%20between%0Athe%20outputs%20of%20the%20two%20classifiers%20to%20detect%20target%20samples%20far%20from%20the%20source%0Asupport.%20In%20contrast%2C%20the%20CNN%20encoder%20clusters%20target%20features%20based%20on%20the%0Apreviously%20defined%20class-specific%20boundaries%20by%20minimizing%20the%20discrepancy%0Abetween%20the%20probabilities%20of%20the%20two%20classifiers.%20Finally%2C%20ViT%20and%20CNN%20mutually%0Aexchange%20knowledge%20to%20improve%20the%20quality%20of%20pseudo%20labels%20and%20reduce%20the%0Aknowledge%20discrepancies%20of%20these%20models.%20Compared%20to%20conventional%20DA%20methods%2C%0Aour%20ECB%20achieves%20superior%20performance%2C%20which%20verifies%20its%20effectiveness%20in%20this%0Ahybrid%20model.%20The%20project%20website%20can%20be%20found%0Ahttps%3A//dotrannhattuong.github.io/ECB/website.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18360v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20CNN%20on%20ViT%3A%20A%20Hybrid%20Model%20to%20Explicitly%20Class-specific%0A%20%20Boundaries%20for%20Domain%20Adaptation&entry.906535625=Ba%20Hung%20Ngo%20and%20Nhat-Tuong%20Do-Tran%20and%20Tuan-Ngoc%20Nguyen%20and%20Hae-Gon%20Jeon%20and%20Tae%20Jong%20Choi&entry.1292438233=%20%20Most%20domain%20adaptation%20%28DA%29%20methods%20are%20based%20on%20either%20a%20convolutional%0Aneural%20networks%20%28CNNs%29%20or%20a%20vision%20transformers%20%28ViTs%29.%20They%20align%20the%0Adistribution%20differences%20between%20domains%20as%20encoders%20without%20considering%20their%0Aunique%20characteristics.%20For%20instance%2C%20ViT%20excels%20in%20accuracy%20due%20to%20its%0Asuperior%20ability%20to%20capture%20global%20representations%2C%20while%20CNN%20has%20an%20advantage%0Ain%20capturing%20local%20representations.%20This%20fact%20has%20led%20us%20to%20design%20a%20hybrid%0Amethod%20to%20fully%20take%20advantage%20of%20both%20ViT%20and%20CNN%2C%20called%20Explicitly%0AClass-specific%20Boundaries%20%28ECB%29.%20ECB%20learns%20CNN%20on%20ViT%20to%20combine%20their%0Adistinct%20strengths.%20In%20particular%2C%20we%20leverage%20ViT%27s%20properties%20to%20explicitly%0Afind%20class-specific%20decision%20boundaries%20by%20maximizing%20the%20discrepancy%20between%0Athe%20outputs%20of%20the%20two%20classifiers%20to%20detect%20target%20samples%20far%20from%20the%20source%0Asupport.%20In%20contrast%2C%20the%20CNN%20encoder%20clusters%20target%20features%20based%20on%20the%0Apreviously%20defined%20class-specific%20boundaries%20by%20minimizing%20the%20discrepancy%0Abetween%20the%20probabilities%20of%20the%20two%20classifiers.%20Finally%2C%20ViT%20and%20CNN%20mutually%0Aexchange%20knowledge%20to%20improve%20the%20quality%20of%20pseudo%20labels%20and%20reduce%20the%0Aknowledge%20discrepancies%20of%20these%20models.%20Compared%20to%20conventional%20DA%20methods%2C%0Aour%20ECB%20achieves%20superior%20performance%2C%20which%20verifies%20its%20effectiveness%20in%20this%0Ahybrid%20model.%20The%20project%20website%20can%20be%20found%0Ahttps%3A//dotrannhattuong.github.io/ECB/website.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18360v3&entry.124074799=Read"},
{"title": "FENet: Focusing Enhanced Network for Lane Detection", "author": "Liman Wang and Hanyang Zhong", "abstract": "  Inspired by human driving focus, this research pioneers networks augmented\nwith Focusing Sampling, Partial Field of View Evaluation, Enhanced FPN\narchitecture and Directional IoU Loss - targeted innovations addressing\nobstacles to precise lane detection for autonomous driving. Experiments\ndemonstrate our Focusing Sampling strategy, emphasizing vital distant details\nunlike uniform approaches, significantly boosts both benchmark and practical\ncurved/distant lane recognition accuracy essential for safety. While FENetV1\nachieves state-of-the-art conventional metric performance via enhancements\nisolating perspective-aware contexts mimicking driver vision, FENetV2 proves\nmost reliable on the proposed Partial Field analysis. Hence we specifically\nrecommend V2 for practical lane navigation despite fractional degradation on\nstandard entire-image measures. Future directions include collecting on-road\ndata and integrating complementary dual frameworks to further breakthroughs\nguided by human perception principles. The Code is available at\nhttps://github.com/HanyangZhong/FENet.\n", "link": "http://arxiv.org/abs/2312.17163v5", "date": "2024-04-26", "relevancy": 2.5915, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.53}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5276}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4973}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FENet%3A%20Focusing%20Enhanced%20Network%20for%20Lane%20Detection&body=Title%3A%20FENet%3A%20Focusing%20Enhanced%20Network%20for%20Lane%20Detection%0AAuthor%3A%20Liman%20Wang%20and%20Hanyang%20Zhong%0AAbstract%3A%20%20%20Inspired%20by%20human%20driving%20focus%2C%20this%20research%20pioneers%20networks%20augmented%0Awith%20Focusing%20Sampling%2C%20Partial%20Field%20of%20View%20Evaluation%2C%20Enhanced%20FPN%0Aarchitecture%20and%20Directional%20IoU%20Loss%20-%20targeted%20innovations%20addressing%0Aobstacles%20to%20precise%20lane%20detection%20for%20autonomous%20driving.%20Experiments%0Ademonstrate%20our%20Focusing%20Sampling%20strategy%2C%20emphasizing%20vital%20distant%20details%0Aunlike%20uniform%20approaches%2C%20significantly%20boosts%20both%20benchmark%20and%20practical%0Acurved/distant%20lane%20recognition%20accuracy%20essential%20for%20safety.%20While%20FENetV1%0Aachieves%20state-of-the-art%20conventional%20metric%20performance%20via%20enhancements%0Aisolating%20perspective-aware%20contexts%20mimicking%20driver%20vision%2C%20FENetV2%20proves%0Amost%20reliable%20on%20the%20proposed%20Partial%20Field%20analysis.%20Hence%20we%20specifically%0Arecommend%20V2%20for%20practical%20lane%20navigation%20despite%20fractional%20degradation%20on%0Astandard%20entire-image%20measures.%20Future%20directions%20include%20collecting%20on-road%0Adata%20and%20integrating%20complementary%20dual%20frameworks%20to%20further%20breakthroughs%0Aguided%20by%20human%20perception%20principles.%20The%20Code%20is%20available%20at%0Ahttps%3A//github.com/HanyangZhong/FENet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.17163v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FENet%3A%20Focusing%20Enhanced%20Network%20for%20Lane%20Detection&entry.906535625=Liman%20Wang%20and%20Hanyang%20Zhong&entry.1292438233=%20%20Inspired%20by%20human%20driving%20focus%2C%20this%20research%20pioneers%20networks%20augmented%0Awith%20Focusing%20Sampling%2C%20Partial%20Field%20of%20View%20Evaluation%2C%20Enhanced%20FPN%0Aarchitecture%20and%20Directional%20IoU%20Loss%20-%20targeted%20innovations%20addressing%0Aobstacles%20to%20precise%20lane%20detection%20for%20autonomous%20driving.%20Experiments%0Ademonstrate%20our%20Focusing%20Sampling%20strategy%2C%20emphasizing%20vital%20distant%20details%0Aunlike%20uniform%20approaches%2C%20significantly%20boosts%20both%20benchmark%20and%20practical%0Acurved/distant%20lane%20recognition%20accuracy%20essential%20for%20safety.%20While%20FENetV1%0Aachieves%20state-of-the-art%20conventional%20metric%20performance%20via%20enhancements%0Aisolating%20perspective-aware%20contexts%20mimicking%20driver%20vision%2C%20FENetV2%20proves%0Amost%20reliable%20on%20the%20proposed%20Partial%20Field%20analysis.%20Hence%20we%20specifically%0Arecommend%20V2%20for%20practical%20lane%20navigation%20despite%20fractional%20degradation%20on%0Astandard%20entire-image%20measures.%20Future%20directions%20include%20collecting%20on-road%0Adata%20and%20integrating%20complementary%20dual%20frameworks%20to%20further%20breakthroughs%0Aguided%20by%20human%20perception%20principles.%20The%20Code%20is%20available%20at%0Ahttps%3A//github.com/HanyangZhong/FENet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.17163v5&entry.124074799=Read"},
{"title": "Domain Adaptive and Fine-grained Anomaly Detection for Single-cell\n  Sequencing Data and Beyond", "author": "Kaichen Xu and Yueyang Ding and Suyang Hou and Weiqiang Zhan and Nisang Chen and Jun Wang and Xiaobo Sun", "abstract": "  Fined-grained anomalous cell detection from affected tissues is critical for\nclinical diagnosis and pathological research. Single-cell sequencing data\nprovide unprecedented opportunities for this task. However, current anomaly\ndetection methods struggle to handle domain shifts prevalent in multi-sample\nand multi-domain single-cell sequencing data, leading to suboptimal\nperformance. Moreover, these methods fall short of distinguishing anomalous\ncells into pathologically distinct subtypes. In response, we propose ACSleuth,\na novel, reconstruction deviation-guided generative framework that integrates\nthe detection, domain adaptation, and fine-grained annotating of anomalous\ncells into a methodologically cohesive workflow. Notably, we present the first\ntheoretical analysis of using reconstruction deviations output by generative\nmodels for anomaly detection in lieu of domain shifts. This analysis informs us\nto develop a novel and superior maximum mean discrepancy-based anomaly scorer\nin ACSleuth. Extensive benchmarks over various single-cell data and other types\nof tabular data demonstrate ACSleuth's superiority over the state-of-the-art\nmethods in identifying and subtyping anomalies in multi-sample and multi-domain\ncontexts. Our code is available at https://github.com/Catchxu/ACsleuth.\n", "link": "http://arxiv.org/abs/2404.17454v1", "date": "2024-04-26", "relevancy": 2.4712, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5115}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4871}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4842}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Domain%20Adaptive%20and%20Fine-grained%20Anomaly%20Detection%20for%20Single-cell%0A%20%20Sequencing%20Data%20and%20Beyond&body=Title%3A%20Domain%20Adaptive%20and%20Fine-grained%20Anomaly%20Detection%20for%20Single-cell%0A%20%20Sequencing%20Data%20and%20Beyond%0AAuthor%3A%20Kaichen%20Xu%20and%20Yueyang%20Ding%20and%20Suyang%20Hou%20and%20Weiqiang%20Zhan%20and%20Nisang%20Chen%20and%20Jun%20Wang%20and%20Xiaobo%20Sun%0AAbstract%3A%20%20%20Fined-grained%20anomalous%20cell%20detection%20from%20affected%20tissues%20is%20critical%20for%0Aclinical%20diagnosis%20and%20pathological%20research.%20Single-cell%20sequencing%20data%0Aprovide%20unprecedented%20opportunities%20for%20this%20task.%20However%2C%20current%20anomaly%0Adetection%20methods%20struggle%20to%20handle%20domain%20shifts%20prevalent%20in%20multi-sample%0Aand%20multi-domain%20single-cell%20sequencing%20data%2C%20leading%20to%20suboptimal%0Aperformance.%20Moreover%2C%20these%20methods%20fall%20short%20of%20distinguishing%20anomalous%0Acells%20into%20pathologically%20distinct%20subtypes.%20In%20response%2C%20we%20propose%20ACSleuth%2C%0Aa%20novel%2C%20reconstruction%20deviation-guided%20generative%20framework%20that%20integrates%0Athe%20detection%2C%20domain%20adaptation%2C%20and%20fine-grained%20annotating%20of%20anomalous%0Acells%20into%20a%20methodologically%20cohesive%20workflow.%20Notably%2C%20we%20present%20the%20first%0Atheoretical%20analysis%20of%20using%20reconstruction%20deviations%20output%20by%20generative%0Amodels%20for%20anomaly%20detection%20in%20lieu%20of%20domain%20shifts.%20This%20analysis%20informs%20us%0Ato%20develop%20a%20novel%20and%20superior%20maximum%20mean%20discrepancy-based%20anomaly%20scorer%0Ain%20ACSleuth.%20Extensive%20benchmarks%20over%20various%20single-cell%20data%20and%20other%20types%0Aof%20tabular%20data%20demonstrate%20ACSleuth%27s%20superiority%20over%20the%20state-of-the-art%0Amethods%20in%20identifying%20and%20subtyping%20anomalies%20in%20multi-sample%20and%20multi-domain%0Acontexts.%20Our%20code%20is%20available%20at%20https%3A//github.com/Catchxu/ACsleuth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17454v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Adaptive%20and%20Fine-grained%20Anomaly%20Detection%20for%20Single-cell%0A%20%20Sequencing%20Data%20and%20Beyond&entry.906535625=Kaichen%20Xu%20and%20Yueyang%20Ding%20and%20Suyang%20Hou%20and%20Weiqiang%20Zhan%20and%20Nisang%20Chen%20and%20Jun%20Wang%20and%20Xiaobo%20Sun&entry.1292438233=%20%20Fined-grained%20anomalous%20cell%20detection%20from%20affected%20tissues%20is%20critical%20for%0Aclinical%20diagnosis%20and%20pathological%20research.%20Single-cell%20sequencing%20data%0Aprovide%20unprecedented%20opportunities%20for%20this%20task.%20However%2C%20current%20anomaly%0Adetection%20methods%20struggle%20to%20handle%20domain%20shifts%20prevalent%20in%20multi-sample%0Aand%20multi-domain%20single-cell%20sequencing%20data%2C%20leading%20to%20suboptimal%0Aperformance.%20Moreover%2C%20these%20methods%20fall%20short%20of%20distinguishing%20anomalous%0Acells%20into%20pathologically%20distinct%20subtypes.%20In%20response%2C%20we%20propose%20ACSleuth%2C%0Aa%20novel%2C%20reconstruction%20deviation-guided%20generative%20framework%20that%20integrates%0Athe%20detection%2C%20domain%20adaptation%2C%20and%20fine-grained%20annotating%20of%20anomalous%0Acells%20into%20a%20methodologically%20cohesive%20workflow.%20Notably%2C%20we%20present%20the%20first%0Atheoretical%20analysis%20of%20using%20reconstruction%20deviations%20output%20by%20generative%0Amodels%20for%20anomaly%20detection%20in%20lieu%20of%20domain%20shifts.%20This%20analysis%20informs%20us%0Ato%20develop%20a%20novel%20and%20superior%20maximum%20mean%20discrepancy-based%20anomaly%20scorer%0Ain%20ACSleuth.%20Extensive%20benchmarks%20over%20various%20single-cell%20data%20and%20other%20types%0Aof%20tabular%20data%20demonstrate%20ACSleuth%27s%20superiority%20over%20the%20state-of-the-art%0Amethods%20in%20identifying%20and%20subtyping%20anomalies%20in%20multi-sample%20and%20multi-domain%0Acontexts.%20Our%20code%20is%20available%20at%20https%3A//github.com/Catchxu/ACsleuth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17454v1&entry.124074799=Read"},
{"title": "Automatic Target-Less Camera-LiDAR Calibration From Motion and Deep\n  Point Correspondences", "author": "K\u00fcrsat Petek and Niclas V\u00f6disch and Johannes Meyer and Daniele Cattaneo and Abhinav Valada and Wolfram Burgard", "abstract": "  Sensor setups of robotic platforms commonly include both camera and LiDAR as\nthey provide complementary information. However, fusing these two modalities\ntypically requires a highly accurate calibration between them. In this paper,\nwe propose MDPCalib which is a novel method for camera-LiDAR calibration that\nrequires neither human supervision nor any specific target objects. Instead, we\nutilize sensor motion estimates from visual and LiDAR odometry as well as deep\nlearning-based 2D-pixel-to-3D-point correspondences that are obtained without\nin-domain retraining. We represent the camera-LiDAR calibration as a graph\noptimization problem and minimize the costs induced by constraints from sensor\nmotion and point correspondences. In extensive experiments, we demonstrate that\nour approach yields highly accurate extrinsic calibration parameters and is\nrobust to random initialization. Additionally, our approach generalizes to a\nwide range of sensor setups, which we demonstrate by employing it on various\nrobotic platforms including a self-driving perception car, a quadruped robot,\nand a UAV. To make our calibration method publicly accessible, we release the\ncode on our project website at http://calibration.cs.uni-freiburg.de.\n", "link": "http://arxiv.org/abs/2404.17298v1", "date": "2024-04-26", "relevancy": 2.455, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6185}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6179}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5913}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Automatic%20Target-Less%20Camera-LiDAR%20Calibration%20From%20Motion%20and%20Deep%0A%20%20Point%20Correspondences&body=Title%3A%20Automatic%20Target-Less%20Camera-LiDAR%20Calibration%20From%20Motion%20and%20Deep%0A%20%20Point%20Correspondences%0AAuthor%3A%20K%C3%BCrsat%20Petek%20and%20Niclas%20V%C3%B6disch%20and%20Johannes%20Meyer%20and%20Daniele%20Cattaneo%20and%20Abhinav%20Valada%20and%20Wolfram%20Burgard%0AAbstract%3A%20%20%20Sensor%20setups%20of%20robotic%20platforms%20commonly%20include%20both%20camera%20and%20LiDAR%20as%0Athey%20provide%20complementary%20information.%20However%2C%20fusing%20these%20two%20modalities%0Atypically%20requires%20a%20highly%20accurate%20calibration%20between%20them.%20In%20this%20paper%2C%0Awe%20propose%20MDPCalib%20which%20is%20a%20novel%20method%20for%20camera-LiDAR%20calibration%20that%0Arequires%20neither%20human%20supervision%20nor%20any%20specific%20target%20objects.%20Instead%2C%20we%0Autilize%20sensor%20motion%20estimates%20from%20visual%20and%20LiDAR%20odometry%20as%20well%20as%20deep%0Alearning-based%202D-pixel-to-3D-point%20correspondences%20that%20are%20obtained%20without%0Ain-domain%20retraining.%20We%20represent%20the%20camera-LiDAR%20calibration%20as%20a%20graph%0Aoptimization%20problem%20and%20minimize%20the%20costs%20induced%20by%20constraints%20from%20sensor%0Amotion%20and%20point%20correspondences.%20In%20extensive%20experiments%2C%20we%20demonstrate%20that%0Aour%20approach%20yields%20highly%20accurate%20extrinsic%20calibration%20parameters%20and%20is%0Arobust%20to%20random%20initialization.%20Additionally%2C%20our%20approach%20generalizes%20to%20a%0Awide%20range%20of%20sensor%20setups%2C%20which%20we%20demonstrate%20by%20employing%20it%20on%20various%0Arobotic%20platforms%20including%20a%20self-driving%20perception%20car%2C%20a%20quadruped%20robot%2C%0Aand%20a%20UAV.%20To%20make%20our%20calibration%20method%20publicly%20accessible%2C%20we%20release%20the%0Acode%20on%20our%20project%20website%20at%20http%3A//calibration.cs.uni-freiburg.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17298v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Target-Less%20Camera-LiDAR%20Calibration%20From%20Motion%20and%20Deep%0A%20%20Point%20Correspondences&entry.906535625=K%C3%BCrsat%20Petek%20and%20Niclas%20V%C3%B6disch%20and%20Johannes%20Meyer%20and%20Daniele%20Cattaneo%20and%20Abhinav%20Valada%20and%20Wolfram%20Burgard&entry.1292438233=%20%20Sensor%20setups%20of%20robotic%20platforms%20commonly%20include%20both%20camera%20and%20LiDAR%20as%0Athey%20provide%20complementary%20information.%20However%2C%20fusing%20these%20two%20modalities%0Atypically%20requires%20a%20highly%20accurate%20calibration%20between%20them.%20In%20this%20paper%2C%0Awe%20propose%20MDPCalib%20which%20is%20a%20novel%20method%20for%20camera-LiDAR%20calibration%20that%0Arequires%20neither%20human%20supervision%20nor%20any%20specific%20target%20objects.%20Instead%2C%20we%0Autilize%20sensor%20motion%20estimates%20from%20visual%20and%20LiDAR%20odometry%20as%20well%20as%20deep%0Alearning-based%202D-pixel-to-3D-point%20correspondences%20that%20are%20obtained%20without%0Ain-domain%20retraining.%20We%20represent%20the%20camera-LiDAR%20calibration%20as%20a%20graph%0Aoptimization%20problem%20and%20minimize%20the%20costs%20induced%20by%20constraints%20from%20sensor%0Amotion%20and%20point%20correspondences.%20In%20extensive%20experiments%2C%20we%20demonstrate%20that%0Aour%20approach%20yields%20highly%20accurate%20extrinsic%20calibration%20parameters%20and%20is%0Arobust%20to%20random%20initialization.%20Additionally%2C%20our%20approach%20generalizes%20to%20a%0Awide%20range%20of%20sensor%20setups%2C%20which%20we%20demonstrate%20by%20employing%20it%20on%20various%0Arobotic%20platforms%20including%20a%20self-driving%20perception%20car%2C%20a%20quadruped%20robot%2C%0Aand%20a%20UAV.%20To%20make%20our%20calibration%20method%20publicly%20accessible%2C%20we%20release%20the%0Acode%20on%20our%20project%20website%20at%20http%3A//calibration.cs.uni-freiburg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17298v1&entry.124074799=Read"},
{"title": "Tabular Data Contrastive Learning via Class-Conditioned and\n  Feature-Correlation Based Augmentation", "author": "Wei Cui and Rasa Hosseinzadeh and Junwei Ma and Tongzi Wu and Yi Sui and Keyvan Golestan", "abstract": "  Contrastive learning is a model pre-training technique by first creating\nsimilar views of the original data, and then encouraging the data and its\ncorresponding views to be close in the embedding space. Contrastive learning\nhas witnessed success in image and natural language data, thanks to the\ndomain-specific augmentation techniques that are both intuitive and effective.\nNonetheless, in tabular domain, the predominant augmentation technique for\ncreating views is through corrupting tabular entries via swapping values, which\nis not as sound or effective. We propose a simple yet powerful improvement to\nthis augmentation technique: corrupting tabular data conditioned on class\nidentity. Specifically, when corrupting a specific tabular entry from an anchor\nrow, instead of randomly sampling a value in the same feature column from the\nentire table uniformly, we only sample from rows that are identified to be\nwithin the same class as the anchor row. We assume the semi-supervised learning\nsetting, and adopt the pseudo labeling technique for obtaining class identities\nover all table rows. We also explore the novel idea of selecting features to be\ncorrupted based on feature correlation structures. Extensive experiments show\nthat the proposed approach consistently outperforms the conventional corruption\nmethod for tabular data classification tasks. Our code is available at\nhttps://github.com/willtop/Tabular-Class-Conditioned-SSL.\n", "link": "http://arxiv.org/abs/2404.17489v1", "date": "2024-04-26", "relevancy": 2.4213, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5163}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4855}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.451}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Tabular%20Data%20Contrastive%20Learning%20via%20Class-Conditioned%20and%0A%20%20Feature-Correlation%20Based%20Augmentation&body=Title%3A%20Tabular%20Data%20Contrastive%20Learning%20via%20Class-Conditioned%20and%0A%20%20Feature-Correlation%20Based%20Augmentation%0AAuthor%3A%20Wei%20Cui%20and%20Rasa%20Hosseinzadeh%20and%20Junwei%20Ma%20and%20Tongzi%20Wu%20and%20Yi%20Sui%20and%20Keyvan%20Golestan%0AAbstract%3A%20%20%20Contrastive%20learning%20is%20a%20model%20pre-training%20technique%20by%20first%20creating%0Asimilar%20views%20of%20the%20original%20data%2C%20and%20then%20encouraging%20the%20data%20and%20its%0Acorresponding%20views%20to%20be%20close%20in%20the%20embedding%20space.%20Contrastive%20learning%0Ahas%20witnessed%20success%20in%20image%20and%20natural%20language%20data%2C%20thanks%20to%20the%0Adomain-specific%20augmentation%20techniques%20that%20are%20both%20intuitive%20and%20effective.%0ANonetheless%2C%20in%20tabular%20domain%2C%20the%20predominant%20augmentation%20technique%20for%0Acreating%20views%20is%20through%20corrupting%20tabular%20entries%20via%20swapping%20values%2C%20which%0Ais%20not%20as%20sound%20or%20effective.%20We%20propose%20a%20simple%20yet%20powerful%20improvement%20to%0Athis%20augmentation%20technique%3A%20corrupting%20tabular%20data%20conditioned%20on%20class%0Aidentity.%20Specifically%2C%20when%20corrupting%20a%20specific%20tabular%20entry%20from%20an%20anchor%0Arow%2C%20instead%20of%20randomly%20sampling%20a%20value%20in%20the%20same%20feature%20column%20from%20the%0Aentire%20table%20uniformly%2C%20we%20only%20sample%20from%20rows%20that%20are%20identified%20to%20be%0Awithin%20the%20same%20class%20as%20the%20anchor%20row.%20We%20assume%20the%20semi-supervised%20learning%0Asetting%2C%20and%20adopt%20the%20pseudo%20labeling%20technique%20for%20obtaining%20class%20identities%0Aover%20all%20table%20rows.%20We%20also%20explore%20the%20novel%20idea%20of%20selecting%20features%20to%20be%0Acorrupted%20based%20on%20feature%20correlation%20structures.%20Extensive%20experiments%20show%0Athat%20the%20proposed%20approach%20consistently%20outperforms%20the%20conventional%20corruption%0Amethod%20for%20tabular%20data%20classification%20tasks.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/willtop/Tabular-Class-Conditioned-SSL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17489v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tabular%20Data%20Contrastive%20Learning%20via%20Class-Conditioned%20and%0A%20%20Feature-Correlation%20Based%20Augmentation&entry.906535625=Wei%20Cui%20and%20Rasa%20Hosseinzadeh%20and%20Junwei%20Ma%20and%20Tongzi%20Wu%20and%20Yi%20Sui%20and%20Keyvan%20Golestan&entry.1292438233=%20%20Contrastive%20learning%20is%20a%20model%20pre-training%20technique%20by%20first%20creating%0Asimilar%20views%20of%20the%20original%20data%2C%20and%20then%20encouraging%20the%20data%20and%20its%0Acorresponding%20views%20to%20be%20close%20in%20the%20embedding%20space.%20Contrastive%20learning%0Ahas%20witnessed%20success%20in%20image%20and%20natural%20language%20data%2C%20thanks%20to%20the%0Adomain-specific%20augmentation%20techniques%20that%20are%20both%20intuitive%20and%20effective.%0ANonetheless%2C%20in%20tabular%20domain%2C%20the%20predominant%20augmentation%20technique%20for%0Acreating%20views%20is%20through%20corrupting%20tabular%20entries%20via%20swapping%20values%2C%20which%0Ais%20not%20as%20sound%20or%20effective.%20We%20propose%20a%20simple%20yet%20powerful%20improvement%20to%0Athis%20augmentation%20technique%3A%20corrupting%20tabular%20data%20conditioned%20on%20class%0Aidentity.%20Specifically%2C%20when%20corrupting%20a%20specific%20tabular%20entry%20from%20an%20anchor%0Arow%2C%20instead%20of%20randomly%20sampling%20a%20value%20in%20the%20same%20feature%20column%20from%20the%0Aentire%20table%20uniformly%2C%20we%20only%20sample%20from%20rows%20that%20are%20identified%20to%20be%0Awithin%20the%20same%20class%20as%20the%20anchor%20row.%20We%20assume%20the%20semi-supervised%20learning%0Asetting%2C%20and%20adopt%20the%20pseudo%20labeling%20technique%20for%20obtaining%20class%20identities%0Aover%20all%20table%20rows.%20We%20also%20explore%20the%20novel%20idea%20of%20selecting%20features%20to%20be%0Acorrupted%20based%20on%20feature%20correlation%20structures.%20Extensive%20experiments%20show%0Athat%20the%20proposed%20approach%20consistently%20outperforms%20the%20conventional%20corruption%0Amethod%20for%20tabular%20data%20classification%20tasks.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/willtop/Tabular-Class-Conditioned-SSL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17489v1&entry.124074799=Read"},
{"title": "RegWSI: Whole Slide Image Registration using Combined Deep Feature- and\n  Intensity-Based Methods: Winner of the ACROBAT 2023 Challenge", "author": "Marek Wodzinski and Niccol\u00f2 Marini and Manfredo Atzori and Henning M\u00fcller", "abstract": "  The automatic registration of differently stained whole slide images (WSIs)\nis crucial for improving diagnosis and prognosis by fusing complementary\ninformation emerging from different visible structures. It is also useful to\nquickly transfer annotations between consecutive or restained slides, thus\nsignificantly reducing the annotation time and associated costs. Nevertheless,\nthe slide preparation is different for each stain and the tissue undergoes\ncomplex and large deformations. Therefore, a robust, efficient, and accurate\nregistration method is highly desired by the scientific community and hospitals\nspecializing in digital pathology. We propose a two-step hybrid method\nconsisting of (i) deep learning- and feature-based initial alignment algorithm,\nand (ii) intensity-based nonrigid registration using the instance optimization.\nThe proposed method does not require any fine-tuning to a particular dataset\nand can be used directly for any desired tissue type and stain. The method\nscored 1st place in the ACROBAT 2023 challenge. We evaluated using three open\ndatasets: (i) ANHIR, (ii) ACROBAT, and (iii) HyReCo, and performed several\nablation studies concerning the resolution used for registration and the\ninitial alignment robustness and stability. The method achieves the most\naccurate results for the ACROBAT dataset, the cell-level registration accuracy\nfor the restained slides from the HyReCo dataset, and is among the best methods\nevaluated on the ANHIR dataset. The method does not require any fine-tuning to\na new datasets and can be used out-of-the-box for other types of microscopic\nimages. The method is incorporated into the DeeperHistReg framework, allowing\nothers to directly use it to register, transform, and save the WSIs at any\ndesired pyramid level. The proposed method is a significant contribution to the\nWSI registration, thus advancing the field of digital pathology.\n", "link": "http://arxiv.org/abs/2404.13108v2", "date": "2024-04-26", "relevancy": 2.339, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4868}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4709}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4457}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RegWSI%3A%20Whole%20Slide%20Image%20Registration%20using%20Combined%20Deep%20Feature-%20and%0A%20%20Intensity-Based%20Methods%3A%20Winner%20of%20the%20ACROBAT%202023%20Challenge&body=Title%3A%20RegWSI%3A%20Whole%20Slide%20Image%20Registration%20using%20Combined%20Deep%20Feature-%20and%0A%20%20Intensity-Based%20Methods%3A%20Winner%20of%20the%20ACROBAT%202023%20Challenge%0AAuthor%3A%20Marek%20Wodzinski%20and%20Niccol%C3%B2%20Marini%20and%20Manfredo%20Atzori%20and%20Henning%20M%C3%BCller%0AAbstract%3A%20%20%20The%20automatic%20registration%20of%20differently%20stained%20whole%20slide%20images%20%28WSIs%29%0Ais%20crucial%20for%20improving%20diagnosis%20and%20prognosis%20by%20fusing%20complementary%0Ainformation%20emerging%20from%20different%20visible%20structures.%20It%20is%20also%20useful%20to%0Aquickly%20transfer%20annotations%20between%20consecutive%20or%20restained%20slides%2C%20thus%0Asignificantly%20reducing%20the%20annotation%20time%20and%20associated%20costs.%20Nevertheless%2C%0Athe%20slide%20preparation%20is%20different%20for%20each%20stain%20and%20the%20tissue%20undergoes%0Acomplex%20and%20large%20deformations.%20Therefore%2C%20a%20robust%2C%20efficient%2C%20and%20accurate%0Aregistration%20method%20is%20highly%20desired%20by%20the%20scientific%20community%20and%20hospitals%0Aspecializing%20in%20digital%20pathology.%20We%20propose%20a%20two-step%20hybrid%20method%0Aconsisting%20of%20%28i%29%20deep%20learning-%20and%20feature-based%20initial%20alignment%20algorithm%2C%0Aand%20%28ii%29%20intensity-based%20nonrigid%20registration%20using%20the%20instance%20optimization.%0AThe%20proposed%20method%20does%20not%20require%20any%20fine-tuning%20to%20a%20particular%20dataset%0Aand%20can%20be%20used%20directly%20for%20any%20desired%20tissue%20type%20and%20stain.%20The%20method%0Ascored%201st%20place%20in%20the%20ACROBAT%202023%20challenge.%20We%20evaluated%20using%20three%20open%0Adatasets%3A%20%28i%29%20ANHIR%2C%20%28ii%29%20ACROBAT%2C%20and%20%28iii%29%20HyReCo%2C%20and%20performed%20several%0Aablation%20studies%20concerning%20the%20resolution%20used%20for%20registration%20and%20the%0Ainitial%20alignment%20robustness%20and%20stability.%20The%20method%20achieves%20the%20most%0Aaccurate%20results%20for%20the%20ACROBAT%20dataset%2C%20the%20cell-level%20registration%20accuracy%0Afor%20the%20restained%20slides%20from%20the%20HyReCo%20dataset%2C%20and%20is%20among%20the%20best%20methods%0Aevaluated%20on%20the%20ANHIR%20dataset.%20The%20method%20does%20not%20require%20any%20fine-tuning%20to%0Aa%20new%20datasets%20and%20can%20be%20used%20out-of-the-box%20for%20other%20types%20of%20microscopic%0Aimages.%20The%20method%20is%20incorporated%20into%20the%20DeeperHistReg%20framework%2C%20allowing%0Aothers%20to%20directly%20use%20it%20to%20register%2C%20transform%2C%20and%20save%20the%20WSIs%20at%20any%0Adesired%20pyramid%20level.%20The%20proposed%20method%20is%20a%20significant%20contribution%20to%20the%0AWSI%20registration%2C%20thus%20advancing%20the%20field%20of%20digital%20pathology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13108v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RegWSI%3A%20Whole%20Slide%20Image%20Registration%20using%20Combined%20Deep%20Feature-%20and%0A%20%20Intensity-Based%20Methods%3A%20Winner%20of%20the%20ACROBAT%202023%20Challenge&entry.906535625=Marek%20Wodzinski%20and%20Niccol%C3%B2%20Marini%20and%20Manfredo%20Atzori%20and%20Henning%20M%C3%BCller&entry.1292438233=%20%20The%20automatic%20registration%20of%20differently%20stained%20whole%20slide%20images%20%28WSIs%29%0Ais%20crucial%20for%20improving%20diagnosis%20and%20prognosis%20by%20fusing%20complementary%0Ainformation%20emerging%20from%20different%20visible%20structures.%20It%20is%20also%20useful%20to%0Aquickly%20transfer%20annotations%20between%20consecutive%20or%20restained%20slides%2C%20thus%0Asignificantly%20reducing%20the%20annotation%20time%20and%20associated%20costs.%20Nevertheless%2C%0Athe%20slide%20preparation%20is%20different%20for%20each%20stain%20and%20the%20tissue%20undergoes%0Acomplex%20and%20large%20deformations.%20Therefore%2C%20a%20robust%2C%20efficient%2C%20and%20accurate%0Aregistration%20method%20is%20highly%20desired%20by%20the%20scientific%20community%20and%20hospitals%0Aspecializing%20in%20digital%20pathology.%20We%20propose%20a%20two-step%20hybrid%20method%0Aconsisting%20of%20%28i%29%20deep%20learning-%20and%20feature-based%20initial%20alignment%20algorithm%2C%0Aand%20%28ii%29%20intensity-based%20nonrigid%20registration%20using%20the%20instance%20optimization.%0AThe%20proposed%20method%20does%20not%20require%20any%20fine-tuning%20to%20a%20particular%20dataset%0Aand%20can%20be%20used%20directly%20for%20any%20desired%20tissue%20type%20and%20stain.%20The%20method%0Ascored%201st%20place%20in%20the%20ACROBAT%202023%20challenge.%20We%20evaluated%20using%20three%20open%0Adatasets%3A%20%28i%29%20ANHIR%2C%20%28ii%29%20ACROBAT%2C%20and%20%28iii%29%20HyReCo%2C%20and%20performed%20several%0Aablation%20studies%20concerning%20the%20resolution%20used%20for%20registration%20and%20the%0Ainitial%20alignment%20robustness%20and%20stability.%20The%20method%20achieves%20the%20most%0Aaccurate%20results%20for%20the%20ACROBAT%20dataset%2C%20the%20cell-level%20registration%20accuracy%0Afor%20the%20restained%20slides%20from%20the%20HyReCo%20dataset%2C%20and%20is%20among%20the%20best%20methods%0Aevaluated%20on%20the%20ANHIR%20dataset.%20The%20method%20does%20not%20require%20any%20fine-tuning%20to%0Aa%20new%20datasets%20and%20can%20be%20used%20out-of-the-box%20for%20other%20types%20of%20microscopic%0Aimages.%20The%20method%20is%20incorporated%20into%20the%20DeeperHistReg%20framework%2C%20allowing%0Aothers%20to%20directly%20use%20it%20to%20register%2C%20transform%2C%20and%20save%20the%20WSIs%20at%20any%0Adesired%20pyramid%20level.%20The%20proposed%20method%20is%20a%20significant%20contribution%20to%20the%0AWSI%20registration%2C%20thus%20advancing%20the%20field%20of%20digital%20pathology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13108v2&entry.124074799=Read"},
{"title": "Bridging the Fairness Divide: Achieving Group and Individual Fairness in\n  Graph Neural Networks", "author": "Duna Zhan and Dongliang Guo and Pengsheng Ji and Sheng Li", "abstract": "  Graph neural networks (GNNs) have emerged as a powerful tool for analyzing\nand learning from complex data structured as graphs, demonstrating remarkable\neffectiveness in various applications, such as social network analysis,\nrecommendation systems, and drug discovery. However, despite their impressive\nperformance, the fairness problem has increasingly gained attention as a\ncrucial aspect to consider. Existing research in graph learning focuses on\neither group fairness or individual fairness. However, since each concept\nprovides unique insights into fairness from distinct perspectives, integrating\nthem into a fair graph neural network system is crucial. To the best of our\nknowledge, no study has yet to comprehensively tackle both individual and group\nfairness simultaneously. In this paper, we propose a new concept of individual\nfairness within groups and a novel framework named Fairness for Group and\nIndividual (FairGI), which considers both group fairness and individual\nfairness within groups in the context of graph learning. FairGI employs the\nsimilarity matrix of individuals to achieve individual fairness within groups,\nwhile leveraging adversarial learning to address group fairness in terms of\nboth Equal Opportunity and Statistical Parity. The experimental results\ndemonstrate that our approach not only outperforms other state-of-the-art\nmodels in terms of group fairness and individual fairness within groups, but\nalso exhibits excellent performance in population-level individual fairness,\nwhile maintaining comparable prediction accuracy.\n", "link": "http://arxiv.org/abs/2404.17511v1", "date": "2024-04-26", "relevancy": 2.3387, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4883}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4733}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4416}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bridging%20the%20Fairness%20Divide%3A%20Achieving%20Group%20and%20Individual%20Fairness%20in%0A%20%20Graph%20Neural%20Networks&body=Title%3A%20Bridging%20the%20Fairness%20Divide%3A%20Achieving%20Group%20and%20Individual%20Fairness%20in%0A%20%20Graph%20Neural%20Networks%0AAuthor%3A%20Duna%20Zhan%20and%20Dongliang%20Guo%20and%20Pengsheng%20Ji%20and%20Sheng%20Li%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20emerged%20as%20a%20powerful%20tool%20for%20analyzing%0Aand%20learning%20from%20complex%20data%20structured%20as%20graphs%2C%20demonstrating%20remarkable%0Aeffectiveness%20in%20various%20applications%2C%20such%20as%20social%20network%20analysis%2C%0Arecommendation%20systems%2C%20and%20drug%20discovery.%20However%2C%20despite%20their%20impressive%0Aperformance%2C%20the%20fairness%20problem%20has%20increasingly%20gained%20attention%20as%20a%0Acrucial%20aspect%20to%20consider.%20Existing%20research%20in%20graph%20learning%20focuses%20on%0Aeither%20group%20fairness%20or%20individual%20fairness.%20However%2C%20since%20each%20concept%0Aprovides%20unique%20insights%20into%20fairness%20from%20distinct%20perspectives%2C%20integrating%0Athem%20into%20a%20fair%20graph%20neural%20network%20system%20is%20crucial.%20To%20the%20best%20of%20our%0Aknowledge%2C%20no%20study%20has%20yet%20to%20comprehensively%20tackle%20both%20individual%20and%20group%0Afairness%20simultaneously.%20In%20this%20paper%2C%20we%20propose%20a%20new%20concept%20of%20individual%0Afairness%20within%20groups%20and%20a%20novel%20framework%20named%20Fairness%20for%20Group%20and%0AIndividual%20%28FairGI%29%2C%20which%20considers%20both%20group%20fairness%20and%20individual%0Afairness%20within%20groups%20in%20the%20context%20of%20graph%20learning.%20FairGI%20employs%20the%0Asimilarity%20matrix%20of%20individuals%20to%20achieve%20individual%20fairness%20within%20groups%2C%0Awhile%20leveraging%20adversarial%20learning%20to%20address%20group%20fairness%20in%20terms%20of%0Aboth%20Equal%20Opportunity%20and%20Statistical%20Parity.%20The%20experimental%20results%0Ademonstrate%20that%20our%20approach%20not%20only%20outperforms%20other%20state-of-the-art%0Amodels%20in%20terms%20of%20group%20fairness%20and%20individual%20fairness%20within%20groups%2C%20but%0Aalso%20exhibits%20excellent%20performance%20in%20population-level%20individual%20fairness%2C%0Awhile%20maintaining%20comparable%20prediction%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17511v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20the%20Fairness%20Divide%3A%20Achieving%20Group%20and%20Individual%20Fairness%20in%0A%20%20Graph%20Neural%20Networks&entry.906535625=Duna%20Zhan%20and%20Dongliang%20Guo%20and%20Pengsheng%20Ji%20and%20Sheng%20Li&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20emerged%20as%20a%20powerful%20tool%20for%20analyzing%0Aand%20learning%20from%20complex%20data%20structured%20as%20graphs%2C%20demonstrating%20remarkable%0Aeffectiveness%20in%20various%20applications%2C%20such%20as%20social%20network%20analysis%2C%0Arecommendation%20systems%2C%20and%20drug%20discovery.%20However%2C%20despite%20their%20impressive%0Aperformance%2C%20the%20fairness%20problem%20has%20increasingly%20gained%20attention%20as%20a%0Acrucial%20aspect%20to%20consider.%20Existing%20research%20in%20graph%20learning%20focuses%20on%0Aeither%20group%20fairness%20or%20individual%20fairness.%20However%2C%20since%20each%20concept%0Aprovides%20unique%20insights%20into%20fairness%20from%20distinct%20perspectives%2C%20integrating%0Athem%20into%20a%20fair%20graph%20neural%20network%20system%20is%20crucial.%20To%20the%20best%20of%20our%0Aknowledge%2C%20no%20study%20has%20yet%20to%20comprehensively%20tackle%20both%20individual%20and%20group%0Afairness%20simultaneously.%20In%20this%20paper%2C%20we%20propose%20a%20new%20concept%20of%20individual%0Afairness%20within%20groups%20and%20a%20novel%20framework%20named%20Fairness%20for%20Group%20and%0AIndividual%20%28FairGI%29%2C%20which%20considers%20both%20group%20fairness%20and%20individual%0Afairness%20within%20groups%20in%20the%20context%20of%20graph%20learning.%20FairGI%20employs%20the%0Asimilarity%20matrix%20of%20individuals%20to%20achieve%20individual%20fairness%20within%20groups%2C%0Awhile%20leveraging%20adversarial%20learning%20to%20address%20group%20fairness%20in%20terms%20of%0Aboth%20Equal%20Opportunity%20and%20Statistical%20Parity.%20The%20experimental%20results%0Ademonstrate%20that%20our%20approach%20not%20only%20outperforms%20other%20state-of-the-art%0Amodels%20in%20terms%20of%20group%20fairness%20and%20individual%20fairness%20within%20groups%2C%20but%0Aalso%20exhibits%20excellent%20performance%20in%20population-level%20individual%20fairness%2C%0Awhile%20maintaining%20comparable%20prediction%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17511v1&entry.124074799=Read"},
{"title": "Tunnel Try-on: Excavating Spatial-temporal Tunnels for High-quality\n  Virtual Try-on in Videos", "author": "Zhengze Xu and Mengting Chen and Zhao Wang and Linyu Xing and Zhonghua Zhai and Nong Sang and Jinsong Lan and Shuai Xiao and Changxin Gao", "abstract": "  Video try-on is a challenging task and has not been well tackled in previous\nworks. The main obstacle lies in preserving the details of the clothing and\nmodeling the coherent motions simultaneously. Faced with those difficulties, we\naddress video try-on by proposing a diffusion-based framework named \"Tunnel\nTry-on.\" The core idea is excavating a \"focus tunnel\" in the input video that\ngives close-up shots around the clothing regions. We zoom in on the region in\nthe tunnel to better preserve the fine details of the clothing. To generate\ncoherent motions, we first leverage the Kalman filter to construct smooth crops\nin the focus tunnel and inject the position embedding of the tunnel into\nattention layers to improve the continuity of the generated videos. In\naddition, we develop an environment encoder to extract the context information\noutside the tunnels as supplementary cues. Equipped with these techniques,\nTunnel Try-on keeps the fine details of the clothing and synthesizes stable and\nsmooth videos. Demonstrating significant advancements, Tunnel Try-on could be\nregarded as the first attempt toward the commercial-level application of\nvirtual try-on in videos.\n", "link": "http://arxiv.org/abs/2404.17571v1", "date": "2024-04-26", "relevancy": 2.3301, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6336}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5799}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5647}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Tunnel%20Try-on%3A%20Excavating%20Spatial-temporal%20Tunnels%20for%20High-quality%0A%20%20Virtual%20Try-on%20in%20Videos&body=Title%3A%20Tunnel%20Try-on%3A%20Excavating%20Spatial-temporal%20Tunnels%20for%20High-quality%0A%20%20Virtual%20Try-on%20in%20Videos%0AAuthor%3A%20Zhengze%20Xu%20and%20Mengting%20Chen%20and%20Zhao%20Wang%20and%20Linyu%20Xing%20and%20Zhonghua%20Zhai%20and%20Nong%20Sang%20and%20Jinsong%20Lan%20and%20Shuai%20Xiao%20and%20Changxin%20Gao%0AAbstract%3A%20%20%20Video%20try-on%20is%20a%20challenging%20task%20and%20has%20not%20been%20well%20tackled%20in%20previous%0Aworks.%20The%20main%20obstacle%20lies%20in%20preserving%20the%20details%20of%20the%20clothing%20and%0Amodeling%20the%20coherent%20motions%20simultaneously.%20Faced%20with%20those%20difficulties%2C%20we%0Aaddress%20video%20try-on%20by%20proposing%20a%20diffusion-based%20framework%20named%20%22Tunnel%0ATry-on.%22%20The%20core%20idea%20is%20excavating%20a%20%22focus%20tunnel%22%20in%20the%20input%20video%20that%0Agives%20close-up%20shots%20around%20the%20clothing%20regions.%20We%20zoom%20in%20on%20the%20region%20in%0Athe%20tunnel%20to%20better%20preserve%20the%20fine%20details%20of%20the%20clothing.%20To%20generate%0Acoherent%20motions%2C%20we%20first%20leverage%20the%20Kalman%20filter%20to%20construct%20smooth%20crops%0Ain%20the%20focus%20tunnel%20and%20inject%20the%20position%20embedding%20of%20the%20tunnel%20into%0Aattention%20layers%20to%20improve%20the%20continuity%20of%20the%20generated%20videos.%20In%0Aaddition%2C%20we%20develop%20an%20environment%20encoder%20to%20extract%20the%20context%20information%0Aoutside%20the%20tunnels%20as%20supplementary%20cues.%20Equipped%20with%20these%20techniques%2C%0ATunnel%20Try-on%20keeps%20the%20fine%20details%20of%20the%20clothing%20and%20synthesizes%20stable%20and%0Asmooth%20videos.%20Demonstrating%20significant%20advancements%2C%20Tunnel%20Try-on%20could%20be%0Aregarded%20as%20the%20first%20attempt%20toward%20the%20commercial-level%20application%20of%0Avirtual%20try-on%20in%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17571v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tunnel%20Try-on%3A%20Excavating%20Spatial-temporal%20Tunnels%20for%20High-quality%0A%20%20Virtual%20Try-on%20in%20Videos&entry.906535625=Zhengze%20Xu%20and%20Mengting%20Chen%20and%20Zhao%20Wang%20and%20Linyu%20Xing%20and%20Zhonghua%20Zhai%20and%20Nong%20Sang%20and%20Jinsong%20Lan%20and%20Shuai%20Xiao%20and%20Changxin%20Gao&entry.1292438233=%20%20Video%20try-on%20is%20a%20challenging%20task%20and%20has%20not%20been%20well%20tackled%20in%20previous%0Aworks.%20The%20main%20obstacle%20lies%20in%20preserving%20the%20details%20of%20the%20clothing%20and%0Amodeling%20the%20coherent%20motions%20simultaneously.%20Faced%20with%20those%20difficulties%2C%20we%0Aaddress%20video%20try-on%20by%20proposing%20a%20diffusion-based%20framework%20named%20%22Tunnel%0ATry-on.%22%20The%20core%20idea%20is%20excavating%20a%20%22focus%20tunnel%22%20in%20the%20input%20video%20that%0Agives%20close-up%20shots%20around%20the%20clothing%20regions.%20We%20zoom%20in%20on%20the%20region%20in%0Athe%20tunnel%20to%20better%20preserve%20the%20fine%20details%20of%20the%20clothing.%20To%20generate%0Acoherent%20motions%2C%20we%20first%20leverage%20the%20Kalman%20filter%20to%20construct%20smooth%20crops%0Ain%20the%20focus%20tunnel%20and%20inject%20the%20position%20embedding%20of%20the%20tunnel%20into%0Aattention%20layers%20to%20improve%20the%20continuity%20of%20the%20generated%20videos.%20In%0Aaddition%2C%20we%20develop%20an%20environment%20encoder%20to%20extract%20the%20context%20information%0Aoutside%20the%20tunnels%20as%20supplementary%20cues.%20Equipped%20with%20these%20techniques%2C%0ATunnel%20Try-on%20keeps%20the%20fine%20details%20of%20the%20clothing%20and%20synthesizes%20stable%20and%0Asmooth%20videos.%20Demonstrating%20significant%20advancements%2C%20Tunnel%20Try-on%20could%20be%0Aregarded%20as%20the%20first%20attempt%20toward%20the%20commercial-level%20application%20of%0Avirtual%20try-on%20in%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17571v1&entry.124074799=Read"},
{"title": "Cost-Sensitive Uncertainty-Based Failure Recognition for Object\n  Detection", "author": "Moussa Kassem Sbeyti and Michelle Karg and Christian Wirth and Nadja Klein and Sahin Albayrak", "abstract": "  Object detectors in real-world applications often fail to detect objects due\nto varying factors such as weather conditions and noisy input. Therefore, a\nprocess that mitigates false detections is crucial for both safety and\naccuracy. While uncertainty-based thresholding shows promise, previous works\ndemonstrate an imperfect correlation between uncertainty and detection errors.\nThis hinders ideal thresholding, prompting us to further investigate the\ncorrelation and associated cost with different types of uncertainty. We\ntherefore propose a cost-sensitive framework for object detection tailored to\nuser-defined budgets on the two types of errors, missing and false detections.\nWe derive minimum thresholding requirements to prevent performance degradation\nand define metrics to assess the applicability of uncertainty for failure\nrecognition. Furthermore, we automate and optimize the thresholding process to\nmaximize the failure recognition rate w.r.t. the specified budget. Evaluation\non three autonomous driving datasets demonstrates that our approach\nsignificantly enhances safety, particularly in challenging scenarios.\nLeveraging localization aleatoric uncertainty and softmax-based entropy only,\nour method boosts the failure recognition rate by 36-60\\% compared to\nconventional approaches. Code is available at\nhttps://mos-ks.github.io/publications.\n", "link": "http://arxiv.org/abs/2404.17427v1", "date": "2024-04-26", "relevancy": 2.3152, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5988}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5948}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5548}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cost-Sensitive%20Uncertainty-Based%20Failure%20Recognition%20for%20Object%0A%20%20Detection&body=Title%3A%20Cost-Sensitive%20Uncertainty-Based%20Failure%20Recognition%20for%20Object%0A%20%20Detection%0AAuthor%3A%20Moussa%20Kassem%20Sbeyti%20and%20Michelle%20Karg%20and%20Christian%20Wirth%20and%20Nadja%20Klein%20and%20Sahin%20Albayrak%0AAbstract%3A%20%20%20Object%20detectors%20in%20real-world%20applications%20often%20fail%20to%20detect%20objects%20due%0Ato%20varying%20factors%20such%20as%20weather%20conditions%20and%20noisy%20input.%20Therefore%2C%20a%0Aprocess%20that%20mitigates%20false%20detections%20is%20crucial%20for%20both%20safety%20and%0Aaccuracy.%20While%20uncertainty-based%20thresholding%20shows%20promise%2C%20previous%20works%0Ademonstrate%20an%20imperfect%20correlation%20between%20uncertainty%20and%20detection%20errors.%0AThis%20hinders%20ideal%20thresholding%2C%20prompting%20us%20to%20further%20investigate%20the%0Acorrelation%20and%20associated%20cost%20with%20different%20types%20of%20uncertainty.%20We%0Atherefore%20propose%20a%20cost-sensitive%20framework%20for%20object%20detection%20tailored%20to%0Auser-defined%20budgets%20on%20the%20two%20types%20of%20errors%2C%20missing%20and%20false%20detections.%0AWe%20derive%20minimum%20thresholding%20requirements%20to%20prevent%20performance%20degradation%0Aand%20define%20metrics%20to%20assess%20the%20applicability%20of%20uncertainty%20for%20failure%0Arecognition.%20Furthermore%2C%20we%20automate%20and%20optimize%20the%20thresholding%20process%20to%0Amaximize%20the%20failure%20recognition%20rate%20w.r.t.%20the%20specified%20budget.%20Evaluation%0Aon%20three%20autonomous%20driving%20datasets%20demonstrates%20that%20our%20approach%0Asignificantly%20enhances%20safety%2C%20particularly%20in%20challenging%20scenarios.%0ALeveraging%20localization%20aleatoric%20uncertainty%20and%20softmax-based%20entropy%20only%2C%0Aour%20method%20boosts%20the%20failure%20recognition%20rate%20by%2036-60%5C%25%20compared%20to%0Aconventional%20approaches.%20Code%20is%20available%20at%0Ahttps%3A//mos-ks.github.io/publications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17427v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cost-Sensitive%20Uncertainty-Based%20Failure%20Recognition%20for%20Object%0A%20%20Detection&entry.906535625=Moussa%20Kassem%20Sbeyti%20and%20Michelle%20Karg%20and%20Christian%20Wirth%20and%20Nadja%20Klein%20and%20Sahin%20Albayrak&entry.1292438233=%20%20Object%20detectors%20in%20real-world%20applications%20often%20fail%20to%20detect%20objects%20due%0Ato%20varying%20factors%20such%20as%20weather%20conditions%20and%20noisy%20input.%20Therefore%2C%20a%0Aprocess%20that%20mitigates%20false%20detections%20is%20crucial%20for%20both%20safety%20and%0Aaccuracy.%20While%20uncertainty-based%20thresholding%20shows%20promise%2C%20previous%20works%0Ademonstrate%20an%20imperfect%20correlation%20between%20uncertainty%20and%20detection%20errors.%0AThis%20hinders%20ideal%20thresholding%2C%20prompting%20us%20to%20further%20investigate%20the%0Acorrelation%20and%20associated%20cost%20with%20different%20types%20of%20uncertainty.%20We%0Atherefore%20propose%20a%20cost-sensitive%20framework%20for%20object%20detection%20tailored%20to%0Auser-defined%20budgets%20on%20the%20two%20types%20of%20errors%2C%20missing%20and%20false%20detections.%0AWe%20derive%20minimum%20thresholding%20requirements%20to%20prevent%20performance%20degradation%0Aand%20define%20metrics%20to%20assess%20the%20applicability%20of%20uncertainty%20for%20failure%0Arecognition.%20Furthermore%2C%20we%20automate%20and%20optimize%20the%20thresholding%20process%20to%0Amaximize%20the%20failure%20recognition%20rate%20w.r.t.%20the%20specified%20budget.%20Evaluation%0Aon%20three%20autonomous%20driving%20datasets%20demonstrates%20that%20our%20approach%0Asignificantly%20enhances%20safety%2C%20particularly%20in%20challenging%20scenarios.%0ALeveraging%20localization%20aleatoric%20uncertainty%20and%20softmax-based%20entropy%20only%2C%0Aour%20method%20boosts%20the%20failure%20recognition%20rate%20by%2036-60%5C%25%20compared%20to%0Aconventional%20approaches.%20Code%20is%20available%20at%0Ahttps%3A//mos-ks.github.io/publications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17427v1&entry.124074799=Read"},
{"title": "Boosting Defect Detection in Manufacturing using Tensor Convolutional\n  Neural Networks", "author": "Pablo Martin-Ramiro and Unai Sainz de la Maza and Sukhbinder Singh and Roman Orus and Samuel Mugel", "abstract": "  Defect detection is one of the most important yet challenging tasks in the\nquality control stage in the manufacturing sector. In this work, we introduce a\nTensor Convolutional Neural Network (T-CNN) and examine its performance on a\nreal defect detection application in one of the components of the ultrasonic\nsensors produced at Robert Bosch's manufacturing plants. Our quantum-inspired\nT-CNN operates on a reduced model parameter space to substantially improve the\ntraining speed and performance of an equivalent CNN model without sacrificing\naccuracy. More specifically, we demonstrate how T-CNNs are able to reach the\nsame performance as classical CNNs as measured by quality metrics, with up to\nfifteen times fewer parameters and 4% to 19% faster training times. Our results\ndemonstrate that the T-CNN greatly outperforms the results of traditional human\nvisual inspection, providing value in a current real application in\nmanufacturing.\n", "link": "http://arxiv.org/abs/2401.01373v2", "date": "2024-04-26", "relevancy": 2.2989, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4625}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4613}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4555}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Boosting%20Defect%20Detection%20in%20Manufacturing%20using%20Tensor%20Convolutional%0A%20%20Neural%20Networks&body=Title%3A%20Boosting%20Defect%20Detection%20in%20Manufacturing%20using%20Tensor%20Convolutional%0A%20%20Neural%20Networks%0AAuthor%3A%20Pablo%20Martin-Ramiro%20and%20Unai%20Sainz%20de%20la%20Maza%20and%20Sukhbinder%20Singh%20and%20Roman%20Orus%20and%20Samuel%20Mugel%0AAbstract%3A%20%20%20Defect%20detection%20is%20one%20of%20the%20most%20important%20yet%20challenging%20tasks%20in%20the%0Aquality%20control%20stage%20in%20the%20manufacturing%20sector.%20In%20this%20work%2C%20we%20introduce%20a%0ATensor%20Convolutional%20Neural%20Network%20%28T-CNN%29%20and%20examine%20its%20performance%20on%20a%0Areal%20defect%20detection%20application%20in%20one%20of%20the%20components%20of%20the%20ultrasonic%0Asensors%20produced%20at%20Robert%20Bosch%27s%20manufacturing%20plants.%20Our%20quantum-inspired%0AT-CNN%20operates%20on%20a%20reduced%20model%20parameter%20space%20to%20substantially%20improve%20the%0Atraining%20speed%20and%20performance%20of%20an%20equivalent%20CNN%20model%20without%20sacrificing%0Aaccuracy.%20More%20specifically%2C%20we%20demonstrate%20how%20T-CNNs%20are%20able%20to%20reach%20the%0Asame%20performance%20as%20classical%20CNNs%20as%20measured%20by%20quality%20metrics%2C%20with%20up%20to%0Afifteen%20times%20fewer%20parameters%20and%204%25%20to%2019%25%20faster%20training%20times.%20Our%20results%0Ademonstrate%20that%20the%20T-CNN%20greatly%20outperforms%20the%20results%20of%20traditional%20human%0Avisual%20inspection%2C%20providing%20value%20in%20a%20current%20real%20application%20in%0Amanufacturing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.01373v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Defect%20Detection%20in%20Manufacturing%20using%20Tensor%20Convolutional%0A%20%20Neural%20Networks&entry.906535625=Pablo%20Martin-Ramiro%20and%20Unai%20Sainz%20de%20la%20Maza%20and%20Sukhbinder%20Singh%20and%20Roman%20Orus%20and%20Samuel%20Mugel&entry.1292438233=%20%20Defect%20detection%20is%20one%20of%20the%20most%20important%20yet%20challenging%20tasks%20in%20the%0Aquality%20control%20stage%20in%20the%20manufacturing%20sector.%20In%20this%20work%2C%20we%20introduce%20a%0ATensor%20Convolutional%20Neural%20Network%20%28T-CNN%29%20and%20examine%20its%20performance%20on%20a%0Areal%20defect%20detection%20application%20in%20one%20of%20the%20components%20of%20the%20ultrasonic%0Asensors%20produced%20at%20Robert%20Bosch%27s%20manufacturing%20plants.%20Our%20quantum-inspired%0AT-CNN%20operates%20on%20a%20reduced%20model%20parameter%20space%20to%20substantially%20improve%20the%0Atraining%20speed%20and%20performance%20of%20an%20equivalent%20CNN%20model%20without%20sacrificing%0Aaccuracy.%20More%20specifically%2C%20we%20demonstrate%20how%20T-CNNs%20are%20able%20to%20reach%20the%0Asame%20performance%20as%20classical%20CNNs%20as%20measured%20by%20quality%20metrics%2C%20with%20up%20to%0Afifteen%20times%20fewer%20parameters%20and%204%25%20to%2019%25%20faster%20training%20times.%20Our%20results%0Ademonstrate%20that%20the%20T-CNN%20greatly%20outperforms%20the%20results%20of%20traditional%20human%0Avisual%20inspection%2C%20providing%20value%20in%20a%20current%20real%20application%20in%0Amanufacturing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01373v2&entry.124074799=Read"},
{"title": "M3BAT: Unsupervised Domain Adaptation for Multimodal Mobile Sensing with\n  Multi-Branch Adversarial Training", "author": "Lakmal Meegahapola and Hamza Hassoune and Daniel Gatica-Perez", "abstract": "  Over the years, multimodal mobile sensing has been used extensively for\ninferences regarding health and well being, behavior, and context. However, a\nsignificant challenge hindering the widespread deployment of such models in\nreal world scenarios is the issue of distribution shift. This is the phenomenon\nwhere the distribution of data in the training set differs from the\ndistribution of data in the real world, the deployment environment. While\nextensively explored in computer vision and natural language processing, and\nwhile prior research in mobile sensing briefly addresses this concern, current\nwork primarily focuses on models dealing with a single modality of data, such\nas audio or accelerometer readings, and consequently, there is little research\non unsupervised domain adaptation when dealing with multimodal sensor data. To\naddress this gap, we did extensive experiments with domain adversarial neural\nnetworks (DANN) showing that they can effectively handle distribution shifts in\nmultimodal sensor data. Moreover, we proposed a novel improvement over DANN,\ncalled M3BAT, unsupervised domain adaptation for multimodal mobile sensing with\nmulti-branch adversarial training, to account for the multimodality of sensor\ndata during domain adaptation with multiple branches. Through extensive\nexperiments conducted on two multimodal mobile sensing datasets, three\ninference tasks, and 14 source-target domain pairs, including both regression\nand classification, we demonstrate that our approach performs effectively on\nunseen domains. Compared to directly deploying a model trained in the source\ndomain to the target domain, the model shows performance increases up to 12%\nAUC (area under the receiver operating characteristics curves) on\nclassification tasks, and up to 0.13 MAE (mean absolute error) on regression\ntasks.\n", "link": "http://arxiv.org/abs/2404.17391v1", "date": "2024-04-26", "relevancy": 2.2925, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5908}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5823}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5518}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20M3BAT%3A%20Unsupervised%20Domain%20Adaptation%20for%20Multimodal%20Mobile%20Sensing%20with%0A%20%20Multi-Branch%20Adversarial%20Training&body=Title%3A%20M3BAT%3A%20Unsupervised%20Domain%20Adaptation%20for%20Multimodal%20Mobile%20Sensing%20with%0A%20%20Multi-Branch%20Adversarial%20Training%0AAuthor%3A%20Lakmal%20Meegahapola%20and%20Hamza%20Hassoune%20and%20Daniel%20Gatica-Perez%0AAbstract%3A%20%20%20Over%20the%20years%2C%20multimodal%20mobile%20sensing%20has%20been%20used%20extensively%20for%0Ainferences%20regarding%20health%20and%20well%20being%2C%20behavior%2C%20and%20context.%20However%2C%20a%0Asignificant%20challenge%20hindering%20the%20widespread%20deployment%20of%20such%20models%20in%0Areal%20world%20scenarios%20is%20the%20issue%20of%20distribution%20shift.%20This%20is%20the%20phenomenon%0Awhere%20the%20distribution%20of%20data%20in%20the%20training%20set%20differs%20from%20the%0Adistribution%20of%20data%20in%20the%20real%20world%2C%20the%20deployment%20environment.%20While%0Aextensively%20explored%20in%20computer%20vision%20and%20natural%20language%20processing%2C%20and%0Awhile%20prior%20research%20in%20mobile%20sensing%20briefly%20addresses%20this%20concern%2C%20current%0Awork%20primarily%20focuses%20on%20models%20dealing%20with%20a%20single%20modality%20of%20data%2C%20such%0Aas%20audio%20or%20accelerometer%20readings%2C%20and%20consequently%2C%20there%20is%20little%20research%0Aon%20unsupervised%20domain%20adaptation%20when%20dealing%20with%20multimodal%20sensor%20data.%20To%0Aaddress%20this%20gap%2C%20we%20did%20extensive%20experiments%20with%20domain%20adversarial%20neural%0Anetworks%20%28DANN%29%20showing%20that%20they%20can%20effectively%20handle%20distribution%20shifts%20in%0Amultimodal%20sensor%20data.%20Moreover%2C%20we%20proposed%20a%20novel%20improvement%20over%20DANN%2C%0Acalled%20M3BAT%2C%20unsupervised%20domain%20adaptation%20for%20multimodal%20mobile%20sensing%20with%0Amulti-branch%20adversarial%20training%2C%20to%20account%20for%20the%20multimodality%20of%20sensor%0Adata%20during%20domain%20adaptation%20with%20multiple%20branches.%20Through%20extensive%0Aexperiments%20conducted%20on%20two%20multimodal%20mobile%20sensing%20datasets%2C%20three%0Ainference%20tasks%2C%20and%2014%20source-target%20domain%20pairs%2C%20including%20both%20regression%0Aand%20classification%2C%20we%20demonstrate%20that%20our%20approach%20performs%20effectively%20on%0Aunseen%20domains.%20Compared%20to%20directly%20deploying%20a%20model%20trained%20in%20the%20source%0Adomain%20to%20the%20target%20domain%2C%20the%20model%20shows%20performance%20increases%20up%20to%2012%25%0AAUC%20%28area%20under%20the%20receiver%20operating%20characteristics%20curves%29%20on%0Aclassification%20tasks%2C%20and%20up%20to%200.13%20MAE%20%28mean%20absolute%20error%29%20on%20regression%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17391v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M3BAT%3A%20Unsupervised%20Domain%20Adaptation%20for%20Multimodal%20Mobile%20Sensing%20with%0A%20%20Multi-Branch%20Adversarial%20Training&entry.906535625=Lakmal%20Meegahapola%20and%20Hamza%20Hassoune%20and%20Daniel%20Gatica-Perez&entry.1292438233=%20%20Over%20the%20years%2C%20multimodal%20mobile%20sensing%20has%20been%20used%20extensively%20for%0Ainferences%20regarding%20health%20and%20well%20being%2C%20behavior%2C%20and%20context.%20However%2C%20a%0Asignificant%20challenge%20hindering%20the%20widespread%20deployment%20of%20such%20models%20in%0Areal%20world%20scenarios%20is%20the%20issue%20of%20distribution%20shift.%20This%20is%20the%20phenomenon%0Awhere%20the%20distribution%20of%20data%20in%20the%20training%20set%20differs%20from%20the%0Adistribution%20of%20data%20in%20the%20real%20world%2C%20the%20deployment%20environment.%20While%0Aextensively%20explored%20in%20computer%20vision%20and%20natural%20language%20processing%2C%20and%0Awhile%20prior%20research%20in%20mobile%20sensing%20briefly%20addresses%20this%20concern%2C%20current%0Awork%20primarily%20focuses%20on%20models%20dealing%20with%20a%20single%20modality%20of%20data%2C%20such%0Aas%20audio%20or%20accelerometer%20readings%2C%20and%20consequently%2C%20there%20is%20little%20research%0Aon%20unsupervised%20domain%20adaptation%20when%20dealing%20with%20multimodal%20sensor%20data.%20To%0Aaddress%20this%20gap%2C%20we%20did%20extensive%20experiments%20with%20domain%20adversarial%20neural%0Anetworks%20%28DANN%29%20showing%20that%20they%20can%20effectively%20handle%20distribution%20shifts%20in%0Amultimodal%20sensor%20data.%20Moreover%2C%20we%20proposed%20a%20novel%20improvement%20over%20DANN%2C%0Acalled%20M3BAT%2C%20unsupervised%20domain%20adaptation%20for%20multimodal%20mobile%20sensing%20with%0Amulti-branch%20adversarial%20training%2C%20to%20account%20for%20the%20multimodality%20of%20sensor%0Adata%20during%20domain%20adaptation%20with%20multiple%20branches.%20Through%20extensive%0Aexperiments%20conducted%20on%20two%20multimodal%20mobile%20sensing%20datasets%2C%20three%0Ainference%20tasks%2C%20and%2014%20source-target%20domain%20pairs%2C%20including%20both%20regression%0Aand%20classification%2C%20we%20demonstrate%20that%20our%20approach%20performs%20effectively%20on%0Aunseen%20domains.%20Compared%20to%20directly%20deploying%20a%20model%20trained%20in%20the%20source%0Adomain%20to%20the%20target%20domain%2C%20the%20model%20shows%20performance%20increases%20up%20to%2012%25%0AAUC%20%28area%20under%20the%20receiver%20operating%20characteristics%20curves%29%20on%0Aclassification%20tasks%2C%20and%20up%20to%200.13%20MAE%20%28mean%20absolute%20error%29%20on%20regression%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17391v1&entry.124074799=Read"},
{"title": "Guardians of the Quantum GAN", "author": "Archisman Ghosh and Debarshi Kundu and Avimita Chatterjee and Swaroop Ghosh", "abstract": "  Quantum Generative Adversarial Networks (qGANs) are at the forefront of\nimage-generating quantum machine learning models. To accommodate the growing\ndemand for Noisy Intermediate-Scale Quantum (NISQ) devices to train and infer\nquantum machine learning models, the number of third-party vendors offering\nquantum hardware as a service is expected to rise. This expansion introduces\nthe risk of untrusted vendors potentially stealing proprietary information from\nthe quantum machine learning models. To address this concern we propose a novel\nwatermarking technique that exploits the noise signature embedded during the\ntraining phase of qGANs as a non-invasive watermark. The watermark is\nidentifiable in the images generated by the qGAN allowing us to trace the\nspecific quantum hardware used during training hence providing strong proof of\nownership. To further enhance the security robustness, we propose the training\nof qGANs on a sequence of multiple quantum hardware, embedding a complex\nwatermark comprising the noise signatures of all the training hardware that is\ndifficult for adversaries to replicate. We also develop a machine learning\nclassifier to extract this watermark robustly, thereby identifying the training\nhardware (or the suite of hardware) from the images generated by the qGAN\nvalidating the authenticity of the model. We note that the watermark signature\nis robust against inferencing on hardware different than the hardware that was\nused for training. We obtain watermark extraction accuracy of 100% and ~90% for\ntraining the qGAN on individual and multiple quantum hardware setups (and\ninferencing on different hardware), respectively. Since parameter evolution\nduring training is strongly modulated by quantum noise, the proposed watermark\ncan be extended to other quantum machine learning models as well.\n", "link": "http://arxiv.org/abs/2404.16156v2", "date": "2024-04-26", "relevancy": 2.2817, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4651}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4525}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4514}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Guardians%20of%20the%20Quantum%20GAN&body=Title%3A%20Guardians%20of%20the%20Quantum%20GAN%0AAuthor%3A%20Archisman%20Ghosh%20and%20Debarshi%20Kundu%20and%20Avimita%20Chatterjee%20and%20Swaroop%20Ghosh%0AAbstract%3A%20%20%20Quantum%20Generative%20Adversarial%20Networks%20%28qGANs%29%20are%20at%20the%20forefront%20of%0Aimage-generating%20quantum%20machine%20learning%20models.%20To%20accommodate%20the%20growing%0Ademand%20for%20Noisy%20Intermediate-Scale%20Quantum%20%28NISQ%29%20devices%20to%20train%20and%20infer%0Aquantum%20machine%20learning%20models%2C%20the%20number%20of%20third-party%20vendors%20offering%0Aquantum%20hardware%20as%20a%20service%20is%20expected%20to%20rise.%20This%20expansion%20introduces%0Athe%20risk%20of%20untrusted%20vendors%20potentially%20stealing%20proprietary%20information%20from%0Athe%20quantum%20machine%20learning%20models.%20To%20address%20this%20concern%20we%20propose%20a%20novel%0Awatermarking%20technique%20that%20exploits%20the%20noise%20signature%20embedded%20during%20the%0Atraining%20phase%20of%20qGANs%20as%20a%20non-invasive%20watermark.%20The%20watermark%20is%0Aidentifiable%20in%20the%20images%20generated%20by%20the%20qGAN%20allowing%20us%20to%20trace%20the%0Aspecific%20quantum%20hardware%20used%20during%20training%20hence%20providing%20strong%20proof%20of%0Aownership.%20To%20further%20enhance%20the%20security%20robustness%2C%20we%20propose%20the%20training%0Aof%20qGANs%20on%20a%20sequence%20of%20multiple%20quantum%20hardware%2C%20embedding%20a%20complex%0Awatermark%20comprising%20the%20noise%20signatures%20of%20all%20the%20training%20hardware%20that%20is%0Adifficult%20for%20adversaries%20to%20replicate.%20We%20also%20develop%20a%20machine%20learning%0Aclassifier%20to%20extract%20this%20watermark%20robustly%2C%20thereby%20identifying%20the%20training%0Ahardware%20%28or%20the%20suite%20of%20hardware%29%20from%20the%20images%20generated%20by%20the%20qGAN%0Avalidating%20the%20authenticity%20of%20the%20model.%20We%20note%20that%20the%20watermark%20signature%0Ais%20robust%20against%20inferencing%20on%20hardware%20different%20than%20the%20hardware%20that%20was%0Aused%20for%20training.%20We%20obtain%20watermark%20extraction%20accuracy%20of%20100%25%20and%20~90%25%20for%0Atraining%20the%20qGAN%20on%20individual%20and%20multiple%20quantum%20hardware%20setups%20%28and%0Ainferencing%20on%20different%20hardware%29%2C%20respectively.%20Since%20parameter%20evolution%0Aduring%20training%20is%20strongly%20modulated%20by%20quantum%20noise%2C%20the%20proposed%20watermark%0Acan%20be%20extended%20to%20other%20quantum%20machine%20learning%20models%20as%20well.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16156v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guardians%20of%20the%20Quantum%20GAN&entry.906535625=Archisman%20Ghosh%20and%20Debarshi%20Kundu%20and%20Avimita%20Chatterjee%20and%20Swaroop%20Ghosh&entry.1292438233=%20%20Quantum%20Generative%20Adversarial%20Networks%20%28qGANs%29%20are%20at%20the%20forefront%20of%0Aimage-generating%20quantum%20machine%20learning%20models.%20To%20accommodate%20the%20growing%0Ademand%20for%20Noisy%20Intermediate-Scale%20Quantum%20%28NISQ%29%20devices%20to%20train%20and%20infer%0Aquantum%20machine%20learning%20models%2C%20the%20number%20of%20third-party%20vendors%20offering%0Aquantum%20hardware%20as%20a%20service%20is%20expected%20to%20rise.%20This%20expansion%20introduces%0Athe%20risk%20of%20untrusted%20vendors%20potentially%20stealing%20proprietary%20information%20from%0Athe%20quantum%20machine%20learning%20models.%20To%20address%20this%20concern%20we%20propose%20a%20novel%0Awatermarking%20technique%20that%20exploits%20the%20noise%20signature%20embedded%20during%20the%0Atraining%20phase%20of%20qGANs%20as%20a%20non-invasive%20watermark.%20The%20watermark%20is%0Aidentifiable%20in%20the%20images%20generated%20by%20the%20qGAN%20allowing%20us%20to%20trace%20the%0Aspecific%20quantum%20hardware%20used%20during%20training%20hence%20providing%20strong%20proof%20of%0Aownership.%20To%20further%20enhance%20the%20security%20robustness%2C%20we%20propose%20the%20training%0Aof%20qGANs%20on%20a%20sequence%20of%20multiple%20quantum%20hardware%2C%20embedding%20a%20complex%0Awatermark%20comprising%20the%20noise%20signatures%20of%20all%20the%20training%20hardware%20that%20is%0Adifficult%20for%20adversaries%20to%20replicate.%20We%20also%20develop%20a%20machine%20learning%0Aclassifier%20to%20extract%20this%20watermark%20robustly%2C%20thereby%20identifying%20the%20training%0Ahardware%20%28or%20the%20suite%20of%20hardware%29%20from%20the%20images%20generated%20by%20the%20qGAN%0Avalidating%20the%20authenticity%20of%20the%20model.%20We%20note%20that%20the%20watermark%20signature%0Ais%20robust%20against%20inferencing%20on%20hardware%20different%20than%20the%20hardware%20that%20was%0Aused%20for%20training.%20We%20obtain%20watermark%20extraction%20accuracy%20of%20100%25%20and%20~90%25%20for%0Atraining%20the%20qGAN%20on%20individual%20and%20multiple%20quantum%20hardware%20setups%20%28and%0Ainferencing%20on%20different%20hardware%29%2C%20respectively.%20Since%20parameter%20evolution%0Aduring%20training%20is%20strongly%20modulated%20by%20quantum%20noise%2C%20the%20proposed%20watermark%0Acan%20be%20extended%20to%20other%20quantum%20machine%20learning%20models%20as%20well.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16156v2&entry.124074799=Read"},
{"title": "Benchmarking the Fairness of Image Upsampling Methods", "author": "Mike Laszkiewicz and Imant Daunhawer and Julia E. Vogt and Asja Fischer and Johannes Lederer", "abstract": "  Recent years have witnessed a rapid development of deep generative models for\ncreating synthetic media, such as images and videos. While the practical\napplications of these models in everyday tasks are enticing, it is crucial to\nassess the inherent risks regarding their fairness. In this work, we introduce\na comprehensive framework for benchmarking the performance and fairness of\nconditional generative models. We develop a set of\nmetrics$\\unicode{x2013}$inspired by their supervised fairness\ncounterparts$\\unicode{x2013}$to evaluate the models on their fairness and\ndiversity. Focusing on the specific application of image upsampling, we create\na benchmark covering a wide variety of modern upsampling methods. As part of\nthe benchmark, we introduce UnfairFace, a subset of FairFace that replicates\nthe racial distribution of common large-scale face datasets. Our empirical\nstudy highlights the importance of using an unbiased training set and reveals\nvariations in how the algorithms respond to dataset imbalances. Alarmingly, we\nfind that none of the considered methods produces statistically fair and\ndiverse results. All experiments can be reproduced using our provided\nrepository.\n", "link": "http://arxiv.org/abs/2401.13555v2", "date": "2024-04-26", "relevancy": 2.2686, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5785}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5596}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5588}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20the%20Fairness%20of%20Image%20Upsampling%20Methods&body=Title%3A%20Benchmarking%20the%20Fairness%20of%20Image%20Upsampling%20Methods%0AAuthor%3A%20Mike%20Laszkiewicz%20and%20Imant%20Daunhawer%20and%20Julia%20E.%20Vogt%20and%20Asja%20Fischer%20and%20Johannes%20Lederer%0AAbstract%3A%20%20%20Recent%20years%20have%20witnessed%20a%20rapid%20development%20of%20deep%20generative%20models%20for%0Acreating%20synthetic%20media%2C%20such%20as%20images%20and%20videos.%20While%20the%20practical%0Aapplications%20of%20these%20models%20in%20everyday%20tasks%20are%20enticing%2C%20it%20is%20crucial%20to%0Aassess%20the%20inherent%20risks%20regarding%20their%20fairness.%20In%20this%20work%2C%20we%20introduce%0Aa%20comprehensive%20framework%20for%20benchmarking%20the%20performance%20and%20fairness%20of%0Aconditional%20generative%20models.%20We%20develop%20a%20set%20of%0Ametrics%24%5Cunicode%7Bx2013%7D%24inspired%20by%20their%20supervised%20fairness%0Acounterparts%24%5Cunicode%7Bx2013%7D%24to%20evaluate%20the%20models%20on%20their%20fairness%20and%0Adiversity.%20Focusing%20on%20the%20specific%20application%20of%20image%20upsampling%2C%20we%20create%0Aa%20benchmark%20covering%20a%20wide%20variety%20of%20modern%20upsampling%20methods.%20As%20part%20of%0Athe%20benchmark%2C%20we%20introduce%20UnfairFace%2C%20a%20subset%20of%20FairFace%20that%20replicates%0Athe%20racial%20distribution%20of%20common%20large-scale%20face%20datasets.%20Our%20empirical%0Astudy%20highlights%20the%20importance%20of%20using%20an%20unbiased%20training%20set%20and%20reveals%0Avariations%20in%20how%20the%20algorithms%20respond%20to%20dataset%20imbalances.%20Alarmingly%2C%20we%0Afind%20that%20none%20of%20the%20considered%20methods%20produces%20statistically%20fair%20and%0Adiverse%20results.%20All%20experiments%20can%20be%20reproduced%20using%20our%20provided%0Arepository.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.13555v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20the%20Fairness%20of%20Image%20Upsampling%20Methods&entry.906535625=Mike%20Laszkiewicz%20and%20Imant%20Daunhawer%20and%20Julia%20E.%20Vogt%20and%20Asja%20Fischer%20and%20Johannes%20Lederer&entry.1292438233=%20%20Recent%20years%20have%20witnessed%20a%20rapid%20development%20of%20deep%20generative%20models%20for%0Acreating%20synthetic%20media%2C%20such%20as%20images%20and%20videos.%20While%20the%20practical%0Aapplications%20of%20these%20models%20in%20everyday%20tasks%20are%20enticing%2C%20it%20is%20crucial%20to%0Aassess%20the%20inherent%20risks%20regarding%20their%20fairness.%20In%20this%20work%2C%20we%20introduce%0Aa%20comprehensive%20framework%20for%20benchmarking%20the%20performance%20and%20fairness%20of%0Aconditional%20generative%20models.%20We%20develop%20a%20set%20of%0Ametrics%24%5Cunicode%7Bx2013%7D%24inspired%20by%20their%20supervised%20fairness%0Acounterparts%24%5Cunicode%7Bx2013%7D%24to%20evaluate%20the%20models%20on%20their%20fairness%20and%0Adiversity.%20Focusing%20on%20the%20specific%20application%20of%20image%20upsampling%2C%20we%20create%0Aa%20benchmark%20covering%20a%20wide%20variety%20of%20modern%20upsampling%20methods.%20As%20part%20of%0Athe%20benchmark%2C%20we%20introduce%20UnfairFace%2C%20a%20subset%20of%20FairFace%20that%20replicates%0Athe%20racial%20distribution%20of%20common%20large-scale%20face%20datasets.%20Our%20empirical%0Astudy%20highlights%20the%20importance%20of%20using%20an%20unbiased%20training%20set%20and%20reveals%0Avariations%20in%20how%20the%20algorithms%20respond%20to%20dataset%20imbalances.%20Alarmingly%2C%20we%0Afind%20that%20none%20of%20the%20considered%20methods%20produces%20statistically%20fair%20and%0Adiverse%20results.%20All%20experiments%20can%20be%20reproduced%20using%20our%20provided%0Arepository.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13555v2&entry.124074799=Read"},
{"title": "Masked Two-channel Decoupling Framework for Incomplete Multi-view Weak\n  Multi-label Learning", "author": "Chengliang Liu and Jie Wen and Yabo Liu and Chao Huang and Zhihao Wu and Xiaoling Luo and Yong Xu", "abstract": "  Multi-view learning has become a popular research topic in recent years, but\nresearch on the cross-application of classic multi-label classification and\nmulti-view learning is still in its early stages. In this paper, we focus on\nthe complex yet highly realistic task of incomplete multi-view weak multi-label\nlearning and propose a masked two-channel decoupling framework based on deep\nneural networks to solve this problem. The core innovation of our method lies\nin decoupling the single-channel view-level representation, which is common in\ndeep multi-view learning methods, into a shared representation and a\nview-proprietary representation. We also design a cross-channel contrastive\nloss to enhance the semantic property of the two channels. Additionally, we\nexploit supervised information to design a label-guided graph regularization\nloss, helping the extracted embedding features preserve the geometric structure\namong samples. Inspired by the success of masking mechanisms in image and text\nanalysis, we develop a random fragment masking strategy for vector features to\nimprove the learning ability of encoders. Finally, it is important to emphasize\nthat our model is fully adaptable to arbitrary view and label absences while\nalso performing well on the ideal full data. We have conducted sufficient and\nconvincing experiments to confirm the effectiveness and advancement of our\nmodel.\n", "link": "http://arxiv.org/abs/2404.17340v1", "date": "2024-04-26", "relevancy": 2.2613, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.577}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5612}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5553}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Masked%20Two-channel%20Decoupling%20Framework%20for%20Incomplete%20Multi-view%20Weak%0A%20%20Multi-label%20Learning&body=Title%3A%20Masked%20Two-channel%20Decoupling%20Framework%20for%20Incomplete%20Multi-view%20Weak%0A%20%20Multi-label%20Learning%0AAuthor%3A%20Chengliang%20Liu%20and%20Jie%20Wen%20and%20Yabo%20Liu%20and%20Chao%20Huang%20and%20Zhihao%20Wu%20and%20Xiaoling%20Luo%20and%20Yong%20Xu%0AAbstract%3A%20%20%20Multi-view%20learning%20has%20become%20a%20popular%20research%20topic%20in%20recent%20years%2C%20but%0Aresearch%20on%20the%20cross-application%20of%20classic%20multi-label%20classification%20and%0Amulti-view%20learning%20is%20still%20in%20its%20early%20stages.%20In%20this%20paper%2C%20we%20focus%20on%0Athe%20complex%20yet%20highly%20realistic%20task%20of%20incomplete%20multi-view%20weak%20multi-label%0Alearning%20and%20propose%20a%20masked%20two-channel%20decoupling%20framework%20based%20on%20deep%0Aneural%20networks%20to%20solve%20this%20problem.%20The%20core%20innovation%20of%20our%20method%20lies%0Ain%20decoupling%20the%20single-channel%20view-level%20representation%2C%20which%20is%20common%20in%0Adeep%20multi-view%20learning%20methods%2C%20into%20a%20shared%20representation%20and%20a%0Aview-proprietary%20representation.%20We%20also%20design%20a%20cross-channel%20contrastive%0Aloss%20to%20enhance%20the%20semantic%20property%20of%20the%20two%20channels.%20Additionally%2C%20we%0Aexploit%20supervised%20information%20to%20design%20a%20label-guided%20graph%20regularization%0Aloss%2C%20helping%20the%20extracted%20embedding%20features%20preserve%20the%20geometric%20structure%0Aamong%20samples.%20Inspired%20by%20the%20success%20of%20masking%20mechanisms%20in%20image%20and%20text%0Aanalysis%2C%20we%20develop%20a%20random%20fragment%20masking%20strategy%20for%20vector%20features%20to%0Aimprove%20the%20learning%20ability%20of%20encoders.%20Finally%2C%20it%20is%20important%20to%20emphasize%0Athat%20our%20model%20is%20fully%20adaptable%20to%20arbitrary%20view%20and%20label%20absences%20while%0Aalso%20performing%20well%20on%20the%20ideal%20full%20data.%20We%20have%20conducted%20sufficient%20and%0Aconvincing%20experiments%20to%20confirm%20the%20effectiveness%20and%20advancement%20of%20our%0Amodel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17340v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Two-channel%20Decoupling%20Framework%20for%20Incomplete%20Multi-view%20Weak%0A%20%20Multi-label%20Learning&entry.906535625=Chengliang%20Liu%20and%20Jie%20Wen%20and%20Yabo%20Liu%20and%20Chao%20Huang%20and%20Zhihao%20Wu%20and%20Xiaoling%20Luo%20and%20Yong%20Xu&entry.1292438233=%20%20Multi-view%20learning%20has%20become%20a%20popular%20research%20topic%20in%20recent%20years%2C%20but%0Aresearch%20on%20the%20cross-application%20of%20classic%20multi-label%20classification%20and%0Amulti-view%20learning%20is%20still%20in%20its%20early%20stages.%20In%20this%20paper%2C%20we%20focus%20on%0Athe%20complex%20yet%20highly%20realistic%20task%20of%20incomplete%20multi-view%20weak%20multi-label%0Alearning%20and%20propose%20a%20masked%20two-channel%20decoupling%20framework%20based%20on%20deep%0Aneural%20networks%20to%20solve%20this%20problem.%20The%20core%20innovation%20of%20our%20method%20lies%0Ain%20decoupling%20the%20single-channel%20view-level%20representation%2C%20which%20is%20common%20in%0Adeep%20multi-view%20learning%20methods%2C%20into%20a%20shared%20representation%20and%20a%0Aview-proprietary%20representation.%20We%20also%20design%20a%20cross-channel%20contrastive%0Aloss%20to%20enhance%20the%20semantic%20property%20of%20the%20two%20channels.%20Additionally%2C%20we%0Aexploit%20supervised%20information%20to%20design%20a%20label-guided%20graph%20regularization%0Aloss%2C%20helping%20the%20extracted%20embedding%20features%20preserve%20the%20geometric%20structure%0Aamong%20samples.%20Inspired%20by%20the%20success%20of%20masking%20mechanisms%20in%20image%20and%20text%0Aanalysis%2C%20we%20develop%20a%20random%20fragment%20masking%20strategy%20for%20vector%20features%20to%0Aimprove%20the%20learning%20ability%20of%20encoders.%20Finally%2C%20it%20is%20important%20to%20emphasize%0Athat%20our%20model%20is%20fully%20adaptable%20to%20arbitrary%20view%20and%20label%20absences%20while%0Aalso%20performing%20well%20on%20the%20ideal%20full%20data.%20We%20have%20conducted%20sufficient%20and%0Aconvincing%20experiments%20to%20confirm%20the%20effectiveness%20and%20advancement%20of%20our%0Amodel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17340v1&entry.124074799=Read"},
{"title": "TextGaze: Gaze-Controllable Face Generation with Natural Language", "author": "Hengfei Wang and Zhongqun Zhang and Yihua Cheng and Hyung Jin Chang", "abstract": "  Generating face image with specific gaze information has attracted\nconsiderable attention. Existing approaches typically input gaze values\ndirectly for face generation, which is unnatural and requires annotated gaze\ndatasets for training, thereby limiting its application. In this paper, we\npresent a novel gaze-controllable face generation task. Our approach inputs\ntextual descriptions that describe human gaze and head behavior and generates\ncorresponding face images. Our work first introduces a text-of-gaze dataset\ncontaining over 90k text descriptions spanning a dense distribution of gaze and\nhead poses. We further propose a gaze-controllable text-to-face method. Our\nmethod contains a sketch-conditioned face diffusion module and a model-based\nsketch diffusion module. We define a face sketch based on facial landmarks and\neye segmentation map. The face diffusion module generates face images from the\nface sketch, and the sketch diffusion module employs a 3D face model to\ngenerate face sketch from text description. Experiments on the FFHQ dataset\nshow the effectiveness of our method. We will release our dataset and code for\nfuture research.\n", "link": "http://arxiv.org/abs/2404.17486v1", "date": "2024-04-26", "relevancy": 2.2461, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5686}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5667}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5535}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TextGaze%3A%20Gaze-Controllable%20Face%20Generation%20with%20Natural%20Language&body=Title%3A%20TextGaze%3A%20Gaze-Controllable%20Face%20Generation%20with%20Natural%20Language%0AAuthor%3A%20Hengfei%20Wang%20and%20Zhongqun%20Zhang%20and%20Yihua%20Cheng%20and%20Hyung%20Jin%20Chang%0AAbstract%3A%20%20%20Generating%20face%20image%20with%20specific%20gaze%20information%20has%20attracted%0Aconsiderable%20attention.%20Existing%20approaches%20typically%20input%20gaze%20values%0Adirectly%20for%20face%20generation%2C%20which%20is%20unnatural%20and%20requires%20annotated%20gaze%0Adatasets%20for%20training%2C%20thereby%20limiting%20its%20application.%20In%20this%20paper%2C%20we%0Apresent%20a%20novel%20gaze-controllable%20face%20generation%20task.%20Our%20approach%20inputs%0Atextual%20descriptions%20that%20describe%20human%20gaze%20and%20head%20behavior%20and%20generates%0Acorresponding%20face%20images.%20Our%20work%20first%20introduces%20a%20text-of-gaze%20dataset%0Acontaining%20over%2090k%20text%20descriptions%20spanning%20a%20dense%20distribution%20of%20gaze%20and%0Ahead%20poses.%20We%20further%20propose%20a%20gaze-controllable%20text-to-face%20method.%20Our%0Amethod%20contains%20a%20sketch-conditioned%20face%20diffusion%20module%20and%20a%20model-based%0Asketch%20diffusion%20module.%20We%20define%20a%20face%20sketch%20based%20on%20facial%20landmarks%20and%0Aeye%20segmentation%20map.%20The%20face%20diffusion%20module%20generates%20face%20images%20from%20the%0Aface%20sketch%2C%20and%20the%20sketch%20diffusion%20module%20employs%20a%203D%20face%20model%20to%0Agenerate%20face%20sketch%20from%20text%20description.%20Experiments%20on%20the%20FFHQ%20dataset%0Ashow%20the%20effectiveness%20of%20our%20method.%20We%20will%20release%20our%20dataset%20and%20code%20for%0Afuture%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17486v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextGaze%3A%20Gaze-Controllable%20Face%20Generation%20with%20Natural%20Language&entry.906535625=Hengfei%20Wang%20and%20Zhongqun%20Zhang%20and%20Yihua%20Cheng%20and%20Hyung%20Jin%20Chang&entry.1292438233=%20%20Generating%20face%20image%20with%20specific%20gaze%20information%20has%20attracted%0Aconsiderable%20attention.%20Existing%20approaches%20typically%20input%20gaze%20values%0Adirectly%20for%20face%20generation%2C%20which%20is%20unnatural%20and%20requires%20annotated%20gaze%0Adatasets%20for%20training%2C%20thereby%20limiting%20its%20application.%20In%20this%20paper%2C%20we%0Apresent%20a%20novel%20gaze-controllable%20face%20generation%20task.%20Our%20approach%20inputs%0Atextual%20descriptions%20that%20describe%20human%20gaze%20and%20head%20behavior%20and%20generates%0Acorresponding%20face%20images.%20Our%20work%20first%20introduces%20a%20text-of-gaze%20dataset%0Acontaining%20over%2090k%20text%20descriptions%20spanning%20a%20dense%20distribution%20of%20gaze%20and%0Ahead%20poses.%20We%20further%20propose%20a%20gaze-controllable%20text-to-face%20method.%20Our%0Amethod%20contains%20a%20sketch-conditioned%20face%20diffusion%20module%20and%20a%20model-based%0Asketch%20diffusion%20module.%20We%20define%20a%20face%20sketch%20based%20on%20facial%20landmarks%20and%0Aeye%20segmentation%20map.%20The%20face%20diffusion%20module%20generates%20face%20images%20from%20the%0Aface%20sketch%2C%20and%20the%20sketch%20diffusion%20module%20employs%20a%203D%20face%20model%20to%0Agenerate%20face%20sketch%20from%20text%20description.%20Experiments%20on%20the%20FFHQ%20dataset%0Ashow%20the%20effectiveness%20of%20our%20method.%20We%20will%20release%20our%20dataset%20and%20code%20for%0Afuture%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17486v1&entry.124074799=Read"},
{"title": "Geometry-aware Reconstruction and Fusion-refined Rendering for\n  Generalizable Neural Radiance Fields", "author": "Tianqi Liu and Xinyi Ye and Min Shi and Zihao Huang and Zhiyu Pan and Zhan Peng and Zhiguo Cao", "abstract": "  Generalizable NeRF aims to synthesize novel views for unseen scenes. Common\npractices involve constructing variance-based cost volumes for geometry\nreconstruction and encoding 3D descriptors for decoding novel views. However,\nexisting methods show limited generalization ability in challenging conditions\ndue to inaccurate geometry, sub-optimal descriptors, and decoding strategies.\nWe address these issues point by point. First, we find the variance-based cost\nvolume exhibits failure patterns as the features of pixels corresponding to the\nsame point can be inconsistent across different views due to occlusions or\nreflections. We introduce an Adaptive Cost Aggregation (ACA) approach to\namplify the contribution of consistent pixel pairs and suppress inconsistent\nones. Unlike previous methods that solely fuse 2D features into descriptors,\nour approach introduces a Spatial-View Aggregator (SVA) to incorporate 3D\ncontext into descriptors through spatial and inter-view interaction. When\ndecoding the descriptors, we observe the two existing decoding strategies excel\nin different areas, which are complementary. A Consistency-Aware Fusion (CAF)\nstrategy is proposed to leverage the advantages of both. We incorporate the\nabove ACA, SVA, and CAF into a coarse-to-fine framework, termed Geometry-aware\nReconstruction and Fusion-refined Rendering (GeFu). GeFu attains\nstate-of-the-art performance across multiple datasets. Code is available at\nhttps://github.com/TQTQliu/GeFu .\n", "link": "http://arxiv.org/abs/2404.17528v1", "date": "2024-04-26", "relevancy": 2.2451, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5775}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5532}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5409}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Geometry-aware%20Reconstruction%20and%20Fusion-refined%20Rendering%20for%0A%20%20Generalizable%20Neural%20Radiance%20Fields&body=Title%3A%20Geometry-aware%20Reconstruction%20and%20Fusion-refined%20Rendering%20for%0A%20%20Generalizable%20Neural%20Radiance%20Fields%0AAuthor%3A%20Tianqi%20Liu%20and%20Xinyi%20Ye%20and%20Min%20Shi%20and%20Zihao%20Huang%20and%20Zhiyu%20Pan%20and%20Zhan%20Peng%20and%20Zhiguo%20Cao%0AAbstract%3A%20%20%20Generalizable%20NeRF%20aims%20to%20synthesize%20novel%20views%20for%20unseen%20scenes.%20Common%0Apractices%20involve%20constructing%20variance-based%20cost%20volumes%20for%20geometry%0Areconstruction%20and%20encoding%203D%20descriptors%20for%20decoding%20novel%20views.%20However%2C%0Aexisting%20methods%20show%20limited%20generalization%20ability%20in%20challenging%20conditions%0Adue%20to%20inaccurate%20geometry%2C%20sub-optimal%20descriptors%2C%20and%20decoding%20strategies.%0AWe%20address%20these%20issues%20point%20by%20point.%20First%2C%20we%20find%20the%20variance-based%20cost%0Avolume%20exhibits%20failure%20patterns%20as%20the%20features%20of%20pixels%20corresponding%20to%20the%0Asame%20point%20can%20be%20inconsistent%20across%20different%20views%20due%20to%20occlusions%20or%0Areflections.%20We%20introduce%20an%20Adaptive%20Cost%20Aggregation%20%28ACA%29%20approach%20to%0Aamplify%20the%20contribution%20of%20consistent%20pixel%20pairs%20and%20suppress%20inconsistent%0Aones.%20Unlike%20previous%20methods%20that%20solely%20fuse%202D%20features%20into%20descriptors%2C%0Aour%20approach%20introduces%20a%20Spatial-View%20Aggregator%20%28SVA%29%20to%20incorporate%203D%0Acontext%20into%20descriptors%20through%20spatial%20and%20inter-view%20interaction.%20When%0Adecoding%20the%20descriptors%2C%20we%20observe%20the%20two%20existing%20decoding%20strategies%20excel%0Ain%20different%20areas%2C%20which%20are%20complementary.%20A%20Consistency-Aware%20Fusion%20%28CAF%29%0Astrategy%20is%20proposed%20to%20leverage%20the%20advantages%20of%20both.%20We%20incorporate%20the%0Aabove%20ACA%2C%20SVA%2C%20and%20CAF%20into%20a%20coarse-to-fine%20framework%2C%20termed%20Geometry-aware%0AReconstruction%20and%20Fusion-refined%20Rendering%20%28GeFu%29.%20GeFu%20attains%0Astate-of-the-art%20performance%20across%20multiple%20datasets.%20Code%20is%20available%20at%0Ahttps%3A//github.com/TQTQliu/GeFu%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17528v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-aware%20Reconstruction%20and%20Fusion-refined%20Rendering%20for%0A%20%20Generalizable%20Neural%20Radiance%20Fields&entry.906535625=Tianqi%20Liu%20and%20Xinyi%20Ye%20and%20Min%20Shi%20and%20Zihao%20Huang%20and%20Zhiyu%20Pan%20and%20Zhan%20Peng%20and%20Zhiguo%20Cao&entry.1292438233=%20%20Generalizable%20NeRF%20aims%20to%20synthesize%20novel%20views%20for%20unseen%20scenes.%20Common%0Apractices%20involve%20constructing%20variance-based%20cost%20volumes%20for%20geometry%0Areconstruction%20and%20encoding%203D%20descriptors%20for%20decoding%20novel%20views.%20However%2C%0Aexisting%20methods%20show%20limited%20generalization%20ability%20in%20challenging%20conditions%0Adue%20to%20inaccurate%20geometry%2C%20sub-optimal%20descriptors%2C%20and%20decoding%20strategies.%0AWe%20address%20these%20issues%20point%20by%20point.%20First%2C%20we%20find%20the%20variance-based%20cost%0Avolume%20exhibits%20failure%20patterns%20as%20the%20features%20of%20pixels%20corresponding%20to%20the%0Asame%20point%20can%20be%20inconsistent%20across%20different%20views%20due%20to%20occlusions%20or%0Areflections.%20We%20introduce%20an%20Adaptive%20Cost%20Aggregation%20%28ACA%29%20approach%20to%0Aamplify%20the%20contribution%20of%20consistent%20pixel%20pairs%20and%20suppress%20inconsistent%0Aones.%20Unlike%20previous%20methods%20that%20solely%20fuse%202D%20features%20into%20descriptors%2C%0Aour%20approach%20introduces%20a%20Spatial-View%20Aggregator%20%28SVA%29%20to%20incorporate%203D%0Acontext%20into%20descriptors%20through%20spatial%20and%20inter-view%20interaction.%20When%0Adecoding%20the%20descriptors%2C%20we%20observe%20the%20two%20existing%20decoding%20strategies%20excel%0Ain%20different%20areas%2C%20which%20are%20complementary.%20A%20Consistency-Aware%20Fusion%20%28CAF%29%0Astrategy%20is%20proposed%20to%20leverage%20the%20advantages%20of%20both.%20We%20incorporate%20the%0Aabove%20ACA%2C%20SVA%2C%20and%20CAF%20into%20a%20coarse-to-fine%20framework%2C%20termed%20Geometry-aware%0AReconstruction%20and%20Fusion-refined%20Rendering%20%28GeFu%29.%20GeFu%20attains%0Astate-of-the-art%20performance%20across%20multiple%20datasets.%20Code%20is%20available%20at%0Ahttps%3A//github.com/TQTQliu/GeFu%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17528v1&entry.124074799=Read"},
{"title": "CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans and\n  Radiology Reports for Full-Body Scenarios", "author": "Jingyang Lin and Yingda Xia and Jianpeng Zhang and Ke Yan and Le Lu and Jiebo Luo and Ling Zhang", "abstract": "  Medical Vision-Language Pretraining (Med-VLP) establishes a connection\nbetween visual content from medical images and the relevant textual\ndescriptions. Existing Med-VLP methods primarily focus on 2D images depicting a\nsingle body part, notably chest X-rays. In this paper, we extend the scope of\nMed-VLP to encompass 3D images, specifically targeting full-body scenarios, by\nusing a multimodal dataset of CT images and reports. Compared with the 2D\ncounterpart, 3D VLP is required to effectively capture essential semantics from\nsignificantly sparser representation in 3D imaging. In this paper, we introduce\nCT-GLIP (Grounded Language-Image Pretraining with CT scans), a novel method\nthat constructs organ-level image-text pairs to enhance multimodal contrastive\nlearning, aligning grounded visual features with precise diagnostic text.\nAdditionally, we developed an abnormality dictionary to augment contrastive\nlearning with diverse contrastive pairs. Our method, trained on a multimodal CT\ndataset comprising 44,011 organ-level vision-text pairs from 17,702 patients\nacross 104 organs, demonstrates it can identify organs and abnormalities in a\nzero-shot manner using natural languages. The performance of CT-GLIP is\nvalidated on a separate test set of 1,130 patients, focusing on the 16 most\nfrequent abnormalities across 7 organs. The experimental results show our\nmodel's superior performance over the standard CLIP framework across zero-shot\nand fine-tuning scenarios, using both CNN and ViT architectures.\n", "link": "http://arxiv.org/abs/2404.15272v2", "date": "2024-04-26", "relevancy": 2.2269, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6232}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5361}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4985}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CT-GLIP%3A%203D%20Grounded%20Language-Image%20Pretraining%20with%20CT%20Scans%20and%0A%20%20Radiology%20Reports%20for%20Full-Body%20Scenarios&body=Title%3A%20CT-GLIP%3A%203D%20Grounded%20Language-Image%20Pretraining%20with%20CT%20Scans%20and%0A%20%20Radiology%20Reports%20for%20Full-Body%20Scenarios%0AAuthor%3A%20Jingyang%20Lin%20and%20Yingda%20Xia%20and%20Jianpeng%20Zhang%20and%20Ke%20Yan%20and%20Le%20Lu%20and%20Jiebo%20Luo%20and%20Ling%20Zhang%0AAbstract%3A%20%20%20Medical%20Vision-Language%20Pretraining%20%28Med-VLP%29%20establishes%20a%20connection%0Abetween%20visual%20content%20from%20medical%20images%20and%20the%20relevant%20textual%0Adescriptions.%20Existing%20Med-VLP%20methods%20primarily%20focus%20on%202D%20images%20depicting%20a%0Asingle%20body%20part%2C%20notably%20chest%20X-rays.%20In%20this%20paper%2C%20we%20extend%20the%20scope%20of%0AMed-VLP%20to%20encompass%203D%20images%2C%20specifically%20targeting%20full-body%20scenarios%2C%20by%0Ausing%20a%20multimodal%20dataset%20of%20CT%20images%20and%20reports.%20Compared%20with%20the%202D%0Acounterpart%2C%203D%20VLP%20is%20required%20to%20effectively%20capture%20essential%20semantics%20from%0Asignificantly%20sparser%20representation%20in%203D%20imaging.%20In%20this%20paper%2C%20we%20introduce%0ACT-GLIP%20%28Grounded%20Language-Image%20Pretraining%20with%20CT%20scans%29%2C%20a%20novel%20method%0Athat%20constructs%20organ-level%20image-text%20pairs%20to%20enhance%20multimodal%20contrastive%0Alearning%2C%20aligning%20grounded%20visual%20features%20with%20precise%20diagnostic%20text.%0AAdditionally%2C%20we%20developed%20an%20abnormality%20dictionary%20to%20augment%20contrastive%0Alearning%20with%20diverse%20contrastive%20pairs.%20Our%20method%2C%20trained%20on%20a%20multimodal%20CT%0Adataset%20comprising%2044%2C011%20organ-level%20vision-text%20pairs%20from%2017%2C702%20patients%0Aacross%20104%20organs%2C%20demonstrates%20it%20can%20identify%20organs%20and%20abnormalities%20in%20a%0Azero-shot%20manner%20using%20natural%20languages.%20The%20performance%20of%20CT-GLIP%20is%0Avalidated%20on%20a%20separate%20test%20set%20of%201%2C130%20patients%2C%20focusing%20on%20the%2016%20most%0Afrequent%20abnormalities%20across%207%20organs.%20The%20experimental%20results%20show%20our%0Amodel%27s%20superior%20performance%20over%20the%20standard%20CLIP%20framework%20across%20zero-shot%0Aand%20fine-tuning%20scenarios%2C%20using%20both%20CNN%20and%20ViT%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15272v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CT-GLIP%3A%203D%20Grounded%20Language-Image%20Pretraining%20with%20CT%20Scans%20and%0A%20%20Radiology%20Reports%20for%20Full-Body%20Scenarios&entry.906535625=Jingyang%20Lin%20and%20Yingda%20Xia%20and%20Jianpeng%20Zhang%20and%20Ke%20Yan%20and%20Le%20Lu%20and%20Jiebo%20Luo%20and%20Ling%20Zhang&entry.1292438233=%20%20Medical%20Vision-Language%20Pretraining%20%28Med-VLP%29%20establishes%20a%20connection%0Abetween%20visual%20content%20from%20medical%20images%20and%20the%20relevant%20textual%0Adescriptions.%20Existing%20Med-VLP%20methods%20primarily%20focus%20on%202D%20images%20depicting%20a%0Asingle%20body%20part%2C%20notably%20chest%20X-rays.%20In%20this%20paper%2C%20we%20extend%20the%20scope%20of%0AMed-VLP%20to%20encompass%203D%20images%2C%20specifically%20targeting%20full-body%20scenarios%2C%20by%0Ausing%20a%20multimodal%20dataset%20of%20CT%20images%20and%20reports.%20Compared%20with%20the%202D%0Acounterpart%2C%203D%20VLP%20is%20required%20to%20effectively%20capture%20essential%20semantics%20from%0Asignificantly%20sparser%20representation%20in%203D%20imaging.%20In%20this%20paper%2C%20we%20introduce%0ACT-GLIP%20%28Grounded%20Language-Image%20Pretraining%20with%20CT%20scans%29%2C%20a%20novel%20method%0Athat%20constructs%20organ-level%20image-text%20pairs%20to%20enhance%20multimodal%20contrastive%0Alearning%2C%20aligning%20grounded%20visual%20features%20with%20precise%20diagnostic%20text.%0AAdditionally%2C%20we%20developed%20an%20abnormality%20dictionary%20to%20augment%20contrastive%0Alearning%20with%20diverse%20contrastive%20pairs.%20Our%20method%2C%20trained%20on%20a%20multimodal%20CT%0Adataset%20comprising%2044%2C011%20organ-level%20vision-text%20pairs%20from%2017%2C702%20patients%0Aacross%20104%20organs%2C%20demonstrates%20it%20can%20identify%20organs%20and%20abnormalities%20in%20a%0Azero-shot%20manner%20using%20natural%20languages.%20The%20performance%20of%20CT-GLIP%20is%0Avalidated%20on%20a%20separate%20test%20set%20of%201%2C130%20patients%2C%20focusing%20on%20the%2016%20most%0Afrequent%20abnormalities%20across%207%20organs.%20The%20experimental%20results%20show%20our%0Amodel%27s%20superior%20performance%20over%20the%20standard%20CLIP%20framework%20across%20zero-shot%0Aand%20fine-tuning%20scenarios%2C%20using%20both%20CNN%20and%20ViT%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15272v2&entry.124074799=Read"},
{"title": "Spatial-frequency Dual-Domain Feature Fusion Network for Low-Light\n  Remote Sensing Image Enhancement", "author": "Zishu Yao and Guodong Fan and Jinfu Fan and Min Gan and C. L. Philip Chen", "abstract": "  Low-light remote sensing images generally feature high resolution and high\nspatial complexity, with continuously distributed surface features in space.\nThis continuity in scenes leads to extensive long-range correlations in spatial\ndomains within remote sensing images. Convolutional Neural Networks, which rely\non local correlations for long-distance modeling, struggle to establish\nlong-range correlations in such images. On the other hand, transformer-based\nmethods that focus on global information face high computational complexities\nwhen processing high-resolution remote sensing images. From another\nperspective, Fourier transform can compute global information without\nintroducing a large number of parameters, enabling the network to more\nefficiently capture the overall image structure and establish long-range\ncorrelations. Therefore, we propose a Dual-Domain Feature Fusion Network (DFFN)\nfor low-light remote sensing image enhancement. Specifically, this challenging\ntask of low-light enhancement is divided into two more manageable sub-tasks:\nthe first phase learns amplitude information to restore image brightness, and\nthe second phase learns phase information to refine details. To facilitate\ninformation exchange between the two phases, we designed an information fusion\naffine block that combines data from different phases and scales. Additionally,\nwe have constructed two dark light remote sensing datasets to address the\ncurrent lack of datasets in dark light remote sensing image enhancement.\nExtensive evaluations show that our method outperforms existing\nstate-of-the-art methods. The code is available at\nhttps://github.com/iijjlk/DFFN.\n", "link": "http://arxiv.org/abs/2404.17400v1", "date": "2024-04-26", "relevancy": 2.2116, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5708}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5504}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5482}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Spatial-frequency%20Dual-Domain%20Feature%20Fusion%20Network%20for%20Low-Light%0A%20%20Remote%20Sensing%20Image%20Enhancement&body=Title%3A%20Spatial-frequency%20Dual-Domain%20Feature%20Fusion%20Network%20for%20Low-Light%0A%20%20Remote%20Sensing%20Image%20Enhancement%0AAuthor%3A%20Zishu%20Yao%20and%20Guodong%20Fan%20and%20Jinfu%20Fan%20and%20Min%20Gan%20and%20C.%20L.%20Philip%20Chen%0AAbstract%3A%20%20%20Low-light%20remote%20sensing%20images%20generally%20feature%20high%20resolution%20and%20high%0Aspatial%20complexity%2C%20with%20continuously%20distributed%20surface%20features%20in%20space.%0AThis%20continuity%20in%20scenes%20leads%20to%20extensive%20long-range%20correlations%20in%20spatial%0Adomains%20within%20remote%20sensing%20images.%20Convolutional%20Neural%20Networks%2C%20which%20rely%0Aon%20local%20correlations%20for%20long-distance%20modeling%2C%20struggle%20to%20establish%0Along-range%20correlations%20in%20such%20images.%20On%20the%20other%20hand%2C%20transformer-based%0Amethods%20that%20focus%20on%20global%20information%20face%20high%20computational%20complexities%0Awhen%20processing%20high-resolution%20remote%20sensing%20images.%20From%20another%0Aperspective%2C%20Fourier%20transform%20can%20compute%20global%20information%20without%0Aintroducing%20a%20large%20number%20of%20parameters%2C%20enabling%20the%20network%20to%20more%0Aefficiently%20capture%20the%20overall%20image%20structure%20and%20establish%20long-range%0Acorrelations.%20Therefore%2C%20we%20propose%20a%20Dual-Domain%20Feature%20Fusion%20Network%20%28DFFN%29%0Afor%20low-light%20remote%20sensing%20image%20enhancement.%20Specifically%2C%20this%20challenging%0Atask%20of%20low-light%20enhancement%20is%20divided%20into%20two%20more%20manageable%20sub-tasks%3A%0Athe%20first%20phase%20learns%20amplitude%20information%20to%20restore%20image%20brightness%2C%20and%0Athe%20second%20phase%20learns%20phase%20information%20to%20refine%20details.%20To%20facilitate%0Ainformation%20exchange%20between%20the%20two%20phases%2C%20we%20designed%20an%20information%20fusion%0Aaffine%20block%20that%20combines%20data%20from%20different%20phases%20and%20scales.%20Additionally%2C%0Awe%20have%20constructed%20two%20dark%20light%20remote%20sensing%20datasets%20to%20address%20the%0Acurrent%20lack%20of%20datasets%20in%20dark%20light%20remote%20sensing%20image%20enhancement.%0AExtensive%20evaluations%20show%20that%20our%20method%20outperforms%20existing%0Astate-of-the-art%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/iijjlk/DFFN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17400v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial-frequency%20Dual-Domain%20Feature%20Fusion%20Network%20for%20Low-Light%0A%20%20Remote%20Sensing%20Image%20Enhancement&entry.906535625=Zishu%20Yao%20and%20Guodong%20Fan%20and%20Jinfu%20Fan%20and%20Min%20Gan%20and%20C.%20L.%20Philip%20Chen&entry.1292438233=%20%20Low-light%20remote%20sensing%20images%20generally%20feature%20high%20resolution%20and%20high%0Aspatial%20complexity%2C%20with%20continuously%20distributed%20surface%20features%20in%20space.%0AThis%20continuity%20in%20scenes%20leads%20to%20extensive%20long-range%20correlations%20in%20spatial%0Adomains%20within%20remote%20sensing%20images.%20Convolutional%20Neural%20Networks%2C%20which%20rely%0Aon%20local%20correlations%20for%20long-distance%20modeling%2C%20struggle%20to%20establish%0Along-range%20correlations%20in%20such%20images.%20On%20the%20other%20hand%2C%20transformer-based%0Amethods%20that%20focus%20on%20global%20information%20face%20high%20computational%20complexities%0Awhen%20processing%20high-resolution%20remote%20sensing%20images.%20From%20another%0Aperspective%2C%20Fourier%20transform%20can%20compute%20global%20information%20without%0Aintroducing%20a%20large%20number%20of%20parameters%2C%20enabling%20the%20network%20to%20more%0Aefficiently%20capture%20the%20overall%20image%20structure%20and%20establish%20long-range%0Acorrelations.%20Therefore%2C%20we%20propose%20a%20Dual-Domain%20Feature%20Fusion%20Network%20%28DFFN%29%0Afor%20low-light%20remote%20sensing%20image%20enhancement.%20Specifically%2C%20this%20challenging%0Atask%20of%20low-light%20enhancement%20is%20divided%20into%20two%20more%20manageable%20sub-tasks%3A%0Athe%20first%20phase%20learns%20amplitude%20information%20to%20restore%20image%20brightness%2C%20and%0Athe%20second%20phase%20learns%20phase%20information%20to%20refine%20details.%20To%20facilitate%0Ainformation%20exchange%20between%20the%20two%20phases%2C%20we%20designed%20an%20information%20fusion%0Aaffine%20block%20that%20combines%20data%20from%20different%20phases%20and%20scales.%20Additionally%2C%0Awe%20have%20constructed%20two%20dark%20light%20remote%20sensing%20datasets%20to%20address%20the%0Acurrent%20lack%20of%20datasets%20in%20dark%20light%20remote%20sensing%20image%20enhancement.%0AExtensive%20evaluations%20show%20that%20our%20method%20outperforms%20existing%0Astate-of-the-art%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/iijjlk/DFFN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17400v1&entry.124074799=Read"},
{"title": "Federated Transfer Component Analysis Towards Effective VNF Profiling", "author": "Xunzheng ZhangB and Shadi Moazzeni and Juan Marcelo Parra-Ullauri and Reza Nejabati and Dimitra Simeonidou", "abstract": "  The increasing concerns of knowledge transfer and data privacy challenge the\ntraditional gather-and-analyse paradigm in networks. Specifically, the\nintelligent orchestration of Virtual Network Functions (VNFs) requires\nunderstanding and profiling the resource consumption. However, profiling all\nkinds of VNFs is time-consuming. It is important to consider transferring the\nwell-profiled VNF knowledge to other lack-profiled VNF types while keeping data\nprivate. To this end, this paper proposes a Federated Transfer Component\nAnalysis (FTCA) method between the source and target VNFs. FTCA first trains\nGenerative Adversarial Networks (GANs) based on the source VNF profiling data,\nand the trained GANs model is sent to the target VNF domain. Then, FTCA\nrealizes federated domain adaptation by using the generated source VNF data and\nless target VNF profiling data, while keeping the raw data locally. Experiments\nshow that the proposed FTCA can effectively predict the required resources for\nthe target VNF. Specifically, the RMSE index of the regression model decreases\nby 38.5% and the R-squared metric advances up to 68.6%.\n", "link": "http://arxiv.org/abs/2404.17553v1", "date": "2024-04-26", "relevancy": 2.2072, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4495}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4398}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.435}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Federated%20Transfer%20Component%20Analysis%20Towards%20Effective%20VNF%20Profiling&body=Title%3A%20Federated%20Transfer%20Component%20Analysis%20Towards%20Effective%20VNF%20Profiling%0AAuthor%3A%20Xunzheng%20ZhangB%20and%20Shadi%20Moazzeni%20and%20Juan%20Marcelo%20Parra-Ullauri%20and%20Reza%20Nejabati%20and%20Dimitra%20Simeonidou%0AAbstract%3A%20%20%20The%20increasing%20concerns%20of%20knowledge%20transfer%20and%20data%20privacy%20challenge%20the%0Atraditional%20gather-and-analyse%20paradigm%20in%20networks.%20Specifically%2C%20the%0Aintelligent%20orchestration%20of%20Virtual%20Network%20Functions%20%28VNFs%29%20requires%0Aunderstanding%20and%20profiling%20the%20resource%20consumption.%20However%2C%20profiling%20all%0Akinds%20of%20VNFs%20is%20time-consuming.%20It%20is%20important%20to%20consider%20transferring%20the%0Awell-profiled%20VNF%20knowledge%20to%20other%20lack-profiled%20VNF%20types%20while%20keeping%20data%0Aprivate.%20To%20this%20end%2C%20this%20paper%20proposes%20a%20Federated%20Transfer%20Component%0AAnalysis%20%28FTCA%29%20method%20between%20the%20source%20and%20target%20VNFs.%20FTCA%20first%20trains%0AGenerative%20Adversarial%20Networks%20%28GANs%29%20based%20on%20the%20source%20VNF%20profiling%20data%2C%0Aand%20the%20trained%20GANs%20model%20is%20sent%20to%20the%20target%20VNF%20domain.%20Then%2C%20FTCA%0Arealizes%20federated%20domain%20adaptation%20by%20using%20the%20generated%20source%20VNF%20data%20and%0Aless%20target%20VNF%20profiling%20data%2C%20while%20keeping%20the%20raw%20data%20locally.%20Experiments%0Ashow%20that%20the%20proposed%20FTCA%20can%20effectively%20predict%20the%20required%20resources%20for%0Athe%20target%20VNF.%20Specifically%2C%20the%20RMSE%20index%20of%20the%20regression%20model%20decreases%0Aby%2038.5%25%20and%20the%20R-squared%20metric%20advances%20up%20to%2068.6%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17553v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Transfer%20Component%20Analysis%20Towards%20Effective%20VNF%20Profiling&entry.906535625=Xunzheng%20ZhangB%20and%20Shadi%20Moazzeni%20and%20Juan%20Marcelo%20Parra-Ullauri%20and%20Reza%20Nejabati%20and%20Dimitra%20Simeonidou&entry.1292438233=%20%20The%20increasing%20concerns%20of%20knowledge%20transfer%20and%20data%20privacy%20challenge%20the%0Atraditional%20gather-and-analyse%20paradigm%20in%20networks.%20Specifically%2C%20the%0Aintelligent%20orchestration%20of%20Virtual%20Network%20Functions%20%28VNFs%29%20requires%0Aunderstanding%20and%20profiling%20the%20resource%20consumption.%20However%2C%20profiling%20all%0Akinds%20of%20VNFs%20is%20time-consuming.%20It%20is%20important%20to%20consider%20transferring%20the%0Awell-profiled%20VNF%20knowledge%20to%20other%20lack-profiled%20VNF%20types%20while%20keeping%20data%0Aprivate.%20To%20this%20end%2C%20this%20paper%20proposes%20a%20Federated%20Transfer%20Component%0AAnalysis%20%28FTCA%29%20method%20between%20the%20source%20and%20target%20VNFs.%20FTCA%20first%20trains%0AGenerative%20Adversarial%20Networks%20%28GANs%29%20based%20on%20the%20source%20VNF%20profiling%20data%2C%0Aand%20the%20trained%20GANs%20model%20is%20sent%20to%20the%20target%20VNF%20domain.%20Then%2C%20FTCA%0Arealizes%20federated%20domain%20adaptation%20by%20using%20the%20generated%20source%20VNF%20data%20and%0Aless%20target%20VNF%20profiling%20data%2C%20while%20keeping%20the%20raw%20data%20locally.%20Experiments%0Ashow%20that%20the%20proposed%20FTCA%20can%20effectively%20predict%20the%20required%20resources%20for%0Athe%20target%20VNF.%20Specifically%2C%20the%20RMSE%20index%20of%20the%20regression%20model%20decreases%0Aby%2038.5%25%20and%20the%20R-squared%20metric%20advances%20up%20to%2068.6%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17553v1&entry.124074799=Read"},
{"title": "Learning text-to-video retrieval from image captioning", "author": "Lucas Ventura and Cordelia Schmid and G\u00fcl Varol", "abstract": "  We describe a protocol to study text-to-video retrieval training with\nunlabeled videos, where we assume (i) no access to labels for any videos, i.e.,\nno access to the set of ground-truth captions, but (ii) access to labeled\nimages in the form of text. Using image expert models is a realistic scenario\ngiven that annotating images is cheaper therefore scalable, in contrast to\nexpensive video labeling schemes. Recently, zero-shot image experts such as\nCLIP have established a new strong baseline for video understanding tasks. In\nthis paper, we make use of this progress and instantiate the image experts from\ntwo types of models: a text-to-image retrieval model to provide an initial\nbackbone, and image captioning models to provide supervision signal into\nunlabeled videos. We show that automatically labeling video frames with image\ncaptioning allows text-to-video retrieval training. This process adapts the\nfeatures to the target domain at no manual annotation cost, consequently\noutperforming the strong zero-shot CLIP baseline. During training, we sample\ncaptions from multiple video frames that best match the visual content, and\nperform a temporal pooling over frame representations by scoring frames\naccording to their relevance to each caption. We conduct extensive ablations to\nprovide insights and demonstrate the effectiveness of this simple framework by\noutperforming the CLIP zero-shot baselines on text-to-video retrieval on three\nstandard datasets, namely ActivityNet, MSR-VTT, and MSVD.\n", "link": "http://arxiv.org/abs/2404.17498v1", "date": "2024-04-26", "relevancy": 2.2029, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5796}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5563}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5336}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20text-to-video%20retrieval%20from%20image%20captioning&body=Title%3A%20Learning%20text-to-video%20retrieval%20from%20image%20captioning%0AAuthor%3A%20Lucas%20Ventura%20and%20Cordelia%20Schmid%20and%20G%C3%BCl%20Varol%0AAbstract%3A%20%20%20We%20describe%20a%20protocol%20to%20study%20text-to-video%20retrieval%20training%20with%0Aunlabeled%20videos%2C%20where%20we%20assume%20%28i%29%20no%20access%20to%20labels%20for%20any%20videos%2C%20i.e.%2C%0Ano%20access%20to%20the%20set%20of%20ground-truth%20captions%2C%20but%20%28ii%29%20access%20to%20labeled%0Aimages%20in%20the%20form%20of%20text.%20Using%20image%20expert%20models%20is%20a%20realistic%20scenario%0Agiven%20that%20annotating%20images%20is%20cheaper%20therefore%20scalable%2C%20in%20contrast%20to%0Aexpensive%20video%20labeling%20schemes.%20Recently%2C%20zero-shot%20image%20experts%20such%20as%0ACLIP%20have%20established%20a%20new%20strong%20baseline%20for%20video%20understanding%20tasks.%20In%0Athis%20paper%2C%20we%20make%20use%20of%20this%20progress%20and%20instantiate%20the%20image%20experts%20from%0Atwo%20types%20of%20models%3A%20a%20text-to-image%20retrieval%20model%20to%20provide%20an%20initial%0Abackbone%2C%20and%20image%20captioning%20models%20to%20provide%20supervision%20signal%20into%0Aunlabeled%20videos.%20We%20show%20that%20automatically%20labeling%20video%20frames%20with%20image%0Acaptioning%20allows%20text-to-video%20retrieval%20training.%20This%20process%20adapts%20the%0Afeatures%20to%20the%20target%20domain%20at%20no%20manual%20annotation%20cost%2C%20consequently%0Aoutperforming%20the%20strong%20zero-shot%20CLIP%20baseline.%20During%20training%2C%20we%20sample%0Acaptions%20from%20multiple%20video%20frames%20that%20best%20match%20the%20visual%20content%2C%20and%0Aperform%20a%20temporal%20pooling%20over%20frame%20representations%20by%20scoring%20frames%0Aaccording%20to%20their%20relevance%20to%20each%20caption.%20We%20conduct%20extensive%20ablations%20to%0Aprovide%20insights%20and%20demonstrate%20the%20effectiveness%20of%20this%20simple%20framework%20by%0Aoutperforming%20the%20CLIP%20zero-shot%20baselines%20on%20text-to-video%20retrieval%20on%20three%0Astandard%20datasets%2C%20namely%20ActivityNet%2C%20MSR-VTT%2C%20and%20MSVD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17498v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20text-to-video%20retrieval%20from%20image%20captioning&entry.906535625=Lucas%20Ventura%20and%20Cordelia%20Schmid%20and%20G%C3%BCl%20Varol&entry.1292438233=%20%20We%20describe%20a%20protocol%20to%20study%20text-to-video%20retrieval%20training%20with%0Aunlabeled%20videos%2C%20where%20we%20assume%20%28i%29%20no%20access%20to%20labels%20for%20any%20videos%2C%20i.e.%2C%0Ano%20access%20to%20the%20set%20of%20ground-truth%20captions%2C%20but%20%28ii%29%20access%20to%20labeled%0Aimages%20in%20the%20form%20of%20text.%20Using%20image%20expert%20models%20is%20a%20realistic%20scenario%0Agiven%20that%20annotating%20images%20is%20cheaper%20therefore%20scalable%2C%20in%20contrast%20to%0Aexpensive%20video%20labeling%20schemes.%20Recently%2C%20zero-shot%20image%20experts%20such%20as%0ACLIP%20have%20established%20a%20new%20strong%20baseline%20for%20video%20understanding%20tasks.%20In%0Athis%20paper%2C%20we%20make%20use%20of%20this%20progress%20and%20instantiate%20the%20image%20experts%20from%0Atwo%20types%20of%20models%3A%20a%20text-to-image%20retrieval%20model%20to%20provide%20an%20initial%0Abackbone%2C%20and%20image%20captioning%20models%20to%20provide%20supervision%20signal%20into%0Aunlabeled%20videos.%20We%20show%20that%20automatically%20labeling%20video%20frames%20with%20image%0Acaptioning%20allows%20text-to-video%20retrieval%20training.%20This%20process%20adapts%20the%0Afeatures%20to%20the%20target%20domain%20at%20no%20manual%20annotation%20cost%2C%20consequently%0Aoutperforming%20the%20strong%20zero-shot%20CLIP%20baseline.%20During%20training%2C%20we%20sample%0Acaptions%20from%20multiple%20video%20frames%20that%20best%20match%20the%20visual%20content%2C%20and%0Aperform%20a%20temporal%20pooling%20over%20frame%20representations%20by%20scoring%20frames%0Aaccording%20to%20their%20relevance%20to%20each%20caption.%20We%20conduct%20extensive%20ablations%20to%0Aprovide%20insights%20and%20demonstrate%20the%20effectiveness%20of%20this%20simple%20framework%20by%0Aoutperforming%20the%20CLIP%20zero-shot%20baselines%20on%20text-to-video%20retrieval%20on%20three%0Astandard%20datasets%2C%20namely%20ActivityNet%2C%20MSR-VTT%2C%20and%20MSVD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17498v1&entry.124074799=Read"},
{"title": "One-Shot Image Restoration", "author": "Deborah Pereg", "abstract": "  Image restoration, or inverse problems in image processing, has long been an\nextensively studied topic. In recent years supervised learning approaches have\nbecome a popular strategy attempting to tackle this task. Unfortunately, most\nsupervised learning-based methods are highly demanding in terms of\ncomputational resources and training data (sample complexity). In addition,\ntrained models are sensitive to domain changes, such as varying acquisition\nsystems, signal sampling rates, resolution and contrast. In this work, we try\nto answer a fundamental question: Can supervised learning models generalize\nwell solely by learning from one image or even part of an image? If so, then\nwhat is the minimal amount of patches required to achieve acceptable\ngeneralization? To this end, we focus on an efficient patch-based learning\nframework that requires a single image input-output pair for training.\nExperimental results demonstrate the applicability, robustness and\ncomputational efficiency of the proposed approach for supervised image\ndeblurring and super-resolution. Our results showcase significant improvement\nof learning models' sample efficiency, generalization and time complexity, that\ncan hopefully be leveraged for future real-time applications, and applied to\nother signals and modalities.\n", "link": "http://arxiv.org/abs/2404.17426v1", "date": "2024-04-26", "relevancy": 2.2021, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5619}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5575}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5364}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20One-Shot%20Image%20Restoration&body=Title%3A%20One-Shot%20Image%20Restoration%0AAuthor%3A%20Deborah%20Pereg%0AAbstract%3A%20%20%20Image%20restoration%2C%20or%20inverse%20problems%20in%20image%20processing%2C%20has%20long%20been%20an%0Aextensively%20studied%20topic.%20In%20recent%20years%20supervised%20learning%20approaches%20have%0Abecome%20a%20popular%20strategy%20attempting%20to%20tackle%20this%20task.%20Unfortunately%2C%20most%0Asupervised%20learning-based%20methods%20are%20highly%20demanding%20in%20terms%20of%0Acomputational%20resources%20and%20training%20data%20%28sample%20complexity%29.%20In%20addition%2C%0Atrained%20models%20are%20sensitive%20to%20domain%20changes%2C%20such%20as%20varying%20acquisition%0Asystems%2C%20signal%20sampling%20rates%2C%20resolution%20and%20contrast.%20In%20this%20work%2C%20we%20try%0Ato%20answer%20a%20fundamental%20question%3A%20Can%20supervised%20learning%20models%20generalize%0Awell%20solely%20by%20learning%20from%20one%20image%20or%20even%20part%20of%20an%20image%3F%20If%20so%2C%20then%0Awhat%20is%20the%20minimal%20amount%20of%20patches%20required%20to%20achieve%20acceptable%0Ageneralization%3F%20To%20this%20end%2C%20we%20focus%20on%20an%20efficient%20patch-based%20learning%0Aframework%20that%20requires%20a%20single%20image%20input-output%20pair%20for%20training.%0AExperimental%20results%20demonstrate%20the%20applicability%2C%20robustness%20and%0Acomputational%20efficiency%20of%20the%20proposed%20approach%20for%20supervised%20image%0Adeblurring%20and%20super-resolution.%20Our%20results%20showcase%20significant%20improvement%0Aof%20learning%20models%27%20sample%20efficiency%2C%20generalization%20and%20time%20complexity%2C%20that%0Acan%20hopefully%20be%20leveraged%20for%20future%20real-time%20applications%2C%20and%20applied%20to%0Aother%20signals%20and%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17426v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-Shot%20Image%20Restoration&entry.906535625=Deborah%20Pereg&entry.1292438233=%20%20Image%20restoration%2C%20or%20inverse%20problems%20in%20image%20processing%2C%20has%20long%20been%20an%0Aextensively%20studied%20topic.%20In%20recent%20years%20supervised%20learning%20approaches%20have%0Abecome%20a%20popular%20strategy%20attempting%20to%20tackle%20this%20task.%20Unfortunately%2C%20most%0Asupervised%20learning-based%20methods%20are%20highly%20demanding%20in%20terms%20of%0Acomputational%20resources%20and%20training%20data%20%28sample%20complexity%29.%20In%20addition%2C%0Atrained%20models%20are%20sensitive%20to%20domain%20changes%2C%20such%20as%20varying%20acquisition%0Asystems%2C%20signal%20sampling%20rates%2C%20resolution%20and%20contrast.%20In%20this%20work%2C%20we%20try%0Ato%20answer%20a%20fundamental%20question%3A%20Can%20supervised%20learning%20models%20generalize%0Awell%20solely%20by%20learning%20from%20one%20image%20or%20even%20part%20of%20an%20image%3F%20If%20so%2C%20then%0Awhat%20is%20the%20minimal%20amount%20of%20patches%20required%20to%20achieve%20acceptable%0Ageneralization%3F%20To%20this%20end%2C%20we%20focus%20on%20an%20efficient%20patch-based%20learning%0Aframework%20that%20requires%20a%20single%20image%20input-output%20pair%20for%20training.%0AExperimental%20results%20demonstrate%20the%20applicability%2C%20robustness%20and%0Acomputational%20efficiency%20of%20the%20proposed%20approach%20for%20supervised%20image%0Adeblurring%20and%20super-resolution.%20Our%20results%20showcase%20significant%20improvement%0Aof%20learning%20models%27%20sample%20efficiency%2C%20generalization%20and%20time%20complexity%2C%20that%0Acan%20hopefully%20be%20leveraged%20for%20future%20real-time%20applications%2C%20and%20applied%20to%0Aother%20signals%20and%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17426v1&entry.124074799=Read"},
{"title": "UniRGB-IR: A Unified Framework for Visible-Infrared Downstream Tasks via\n  Adapter Tuning", "author": "Maoxun Yuan and Bo Cui and Tianyi Zhao and Xingxing Wei", "abstract": "  Semantic analysis on visible (RGB) and infrared (IR) images has gained\nattention for its ability to be more accurate and robust under low-illumination\nand complex weather conditions. Due to the lack of pre-trained foundation\nmodels on the large-scale infrared image datasets, existing methods prefer to\ndesign task-specific frameworks and directly fine-tune them with pre-trained\nfoundation models on their RGB-IR semantic relevance datasets, which results in\npoor scalability and limited generalization. In this work, we propose a\nscalable and efficient framework called UniRGB-IR to unify RGB-IR downstream\ntasks, in which a novel adapter is developed to efficiently introduce richer\nRGB-IR features into the pre-trained RGB-based foundation model. Specifically,\nour framework consists of a vision transformer (ViT) foundation model, a\nMulti-modal Feature Pool (MFP) module and a Supplementary Feature Injector\n(SFI) module. The MFP and SFI modules cooperate with each other as an adpater\nto effectively complement the ViT features with the contextual multi-scale\nfeatures. During training process, we freeze the entire foundation model to\ninherit prior knowledge and only optimize the MFP and SFI modules. Furthermore,\nto verify the effectiveness of our framework, we utilize the ViT-Base as the\npre-trained foundation model to perform extensive experiments. Experimental\nresults on various RGB-IR downstream tasks demonstrate that our method can\nachieve state-of-the-art performance. The source code and results are available\nat https://github.com/PoTsui99/UniRGB-IR.git.\n", "link": "http://arxiv.org/abs/2404.17360v1", "date": "2024-04-26", "relevancy": 2.1997, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5635}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5415}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5371}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20UniRGB-IR%3A%20A%20Unified%20Framework%20for%20Visible-Infrared%20Downstream%20Tasks%20via%0A%20%20Adapter%20Tuning&body=Title%3A%20UniRGB-IR%3A%20A%20Unified%20Framework%20for%20Visible-Infrared%20Downstream%20Tasks%20via%0A%20%20Adapter%20Tuning%0AAuthor%3A%20Maoxun%20Yuan%20and%20Bo%20Cui%20and%20Tianyi%20Zhao%20and%20Xingxing%20Wei%0AAbstract%3A%20%20%20Semantic%20analysis%20on%20visible%20%28RGB%29%20and%20infrared%20%28IR%29%20images%20has%20gained%0Aattention%20for%20its%20ability%20to%20be%20more%20accurate%20and%20robust%20under%20low-illumination%0Aand%20complex%20weather%20conditions.%20Due%20to%20the%20lack%20of%20pre-trained%20foundation%0Amodels%20on%20the%20large-scale%20infrared%20image%20datasets%2C%20existing%20methods%20prefer%20to%0Adesign%20task-specific%20frameworks%20and%20directly%20fine-tune%20them%20with%20pre-trained%0Afoundation%20models%20on%20their%20RGB-IR%20semantic%20relevance%20datasets%2C%20which%20results%20in%0Apoor%20scalability%20and%20limited%20generalization.%20In%20this%20work%2C%20we%20propose%20a%0Ascalable%20and%20efficient%20framework%20called%20UniRGB-IR%20to%20unify%20RGB-IR%20downstream%0Atasks%2C%20in%20which%20a%20novel%20adapter%20is%20developed%20to%20efficiently%20introduce%20richer%0ARGB-IR%20features%20into%20the%20pre-trained%20RGB-based%20foundation%20model.%20Specifically%2C%0Aour%20framework%20consists%20of%20a%20vision%20transformer%20%28ViT%29%20foundation%20model%2C%20a%0AMulti-modal%20Feature%20Pool%20%28MFP%29%20module%20and%20a%20Supplementary%20Feature%20Injector%0A%28SFI%29%20module.%20The%20MFP%20and%20SFI%20modules%20cooperate%20with%20each%20other%20as%20an%20adpater%0Ato%20effectively%20complement%20the%20ViT%20features%20with%20the%20contextual%20multi-scale%0Afeatures.%20During%20training%20process%2C%20we%20freeze%20the%20entire%20foundation%20model%20to%0Ainherit%20prior%20knowledge%20and%20only%20optimize%20the%20MFP%20and%20SFI%20modules.%20Furthermore%2C%0Ato%20verify%20the%20effectiveness%20of%20our%20framework%2C%20we%20utilize%20the%20ViT-Base%20as%20the%0Apre-trained%20foundation%20model%20to%20perform%20extensive%20experiments.%20Experimental%0Aresults%20on%20various%20RGB-IR%20downstream%20tasks%20demonstrate%20that%20our%20method%20can%0Aachieve%20state-of-the-art%20performance.%20The%20source%20code%20and%20results%20are%20available%0Aat%20https%3A//github.com/PoTsui99/UniRGB-IR.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17360v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniRGB-IR%3A%20A%20Unified%20Framework%20for%20Visible-Infrared%20Downstream%20Tasks%20via%0A%20%20Adapter%20Tuning&entry.906535625=Maoxun%20Yuan%20and%20Bo%20Cui%20and%20Tianyi%20Zhao%20and%20Xingxing%20Wei&entry.1292438233=%20%20Semantic%20analysis%20on%20visible%20%28RGB%29%20and%20infrared%20%28IR%29%20images%20has%20gained%0Aattention%20for%20its%20ability%20to%20be%20more%20accurate%20and%20robust%20under%20low-illumination%0Aand%20complex%20weather%20conditions.%20Due%20to%20the%20lack%20of%20pre-trained%20foundation%0Amodels%20on%20the%20large-scale%20infrared%20image%20datasets%2C%20existing%20methods%20prefer%20to%0Adesign%20task-specific%20frameworks%20and%20directly%20fine-tune%20them%20with%20pre-trained%0Afoundation%20models%20on%20their%20RGB-IR%20semantic%20relevance%20datasets%2C%20which%20results%20in%0Apoor%20scalability%20and%20limited%20generalization.%20In%20this%20work%2C%20we%20propose%20a%0Ascalable%20and%20efficient%20framework%20called%20UniRGB-IR%20to%20unify%20RGB-IR%20downstream%0Atasks%2C%20in%20which%20a%20novel%20adapter%20is%20developed%20to%20efficiently%20introduce%20richer%0ARGB-IR%20features%20into%20the%20pre-trained%20RGB-based%20foundation%20model.%20Specifically%2C%0Aour%20framework%20consists%20of%20a%20vision%20transformer%20%28ViT%29%20foundation%20model%2C%20a%0AMulti-modal%20Feature%20Pool%20%28MFP%29%20module%20and%20a%20Supplementary%20Feature%20Injector%0A%28SFI%29%20module.%20The%20MFP%20and%20SFI%20modules%20cooperate%20with%20each%20other%20as%20an%20adpater%0Ato%20effectively%20complement%20the%20ViT%20features%20with%20the%20contextual%20multi-scale%0Afeatures.%20During%20training%20process%2C%20we%20freeze%20the%20entire%20foundation%20model%20to%0Ainherit%20prior%20knowledge%20and%20only%20optimize%20the%20MFP%20and%20SFI%20modules.%20Furthermore%2C%0Ato%20verify%20the%20effectiveness%20of%20our%20framework%2C%20we%20utilize%20the%20ViT-Base%20as%20the%0Apre-trained%20foundation%20model%20to%20perform%20extensive%20experiments.%20Experimental%0Aresults%20on%20various%20RGB-IR%20downstream%20tasks%20demonstrate%20that%20our%20method%20can%0Aachieve%20state-of-the-art%20performance.%20The%20source%20code%20and%20results%20are%20available%0Aat%20https%3A//github.com/PoTsui99/UniRGB-IR.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17360v1&entry.124074799=Read"},
{"title": "A Cognitive-Driven Trajectory Prediction Model for Autonomous Driving in\n  Mixed Autonomy Environment", "author": "Haicheng Liao and Zhenning Li and Chengyue Wang and Bonan Wang and Hanlin Kong and Yanchen Guan and Guofa Li and Zhiyong Cui and Chengzhong Xu", "abstract": "  As autonomous driving technology progresses, the need for precise trajectory\nprediction models becomes paramount. This paper introduces an innovative model\nthat infuses cognitive insights into trajectory prediction, focusing on\nperceived safety and dynamic decision-making. Distinct from traditional\napproaches, our model excels in analyzing interactions and behavior patterns in\nmixed autonomy traffic scenarios. It represents a significant leap forward,\nachieving marked performance improvements on several key datasets.\nSpecifically, it surpasses existing benchmarks with gains of 16.2% on the Next\nGeneration Simulation (NGSIM), 27.4% on the Highway Drone (HighD), and 19.8% on\nthe Macao Connected Autonomous Driving (MoCAD) dataset. Our proposed model\nshows exceptional proficiency in handling corner cases, essential for\nreal-world applications. Moreover, its robustness is evident in scenarios with\nmissing or limited data, outperforming most of the state-of-the-art baselines.\nThis adaptability and resilience position our model as a viable tool for\nreal-world autonomous driving systems, heralding a new standard in vehicle\ntrajectory prediction for enhanced safety and efficiency.\n", "link": "http://arxiv.org/abs/2404.17520v1", "date": "2024-04-26", "relevancy": 2.1986, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5974}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5572}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.523}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Cognitive-Driven%20Trajectory%20Prediction%20Model%20for%20Autonomous%20Driving%20in%0A%20%20Mixed%20Autonomy%20Environment&body=Title%3A%20A%20Cognitive-Driven%20Trajectory%20Prediction%20Model%20for%20Autonomous%20Driving%20in%0A%20%20Mixed%20Autonomy%20Environment%0AAuthor%3A%20Haicheng%20Liao%20and%20Zhenning%20Li%20and%20Chengyue%20Wang%20and%20Bonan%20Wang%20and%20Hanlin%20Kong%20and%20Yanchen%20Guan%20and%20Guofa%20Li%20and%20Zhiyong%20Cui%20and%20Chengzhong%20Xu%0AAbstract%3A%20%20%20As%20autonomous%20driving%20technology%20progresses%2C%20the%20need%20for%20precise%20trajectory%0Aprediction%20models%20becomes%20paramount.%20This%20paper%20introduces%20an%20innovative%20model%0Athat%20infuses%20cognitive%20insights%20into%20trajectory%20prediction%2C%20focusing%20on%0Aperceived%20safety%20and%20dynamic%20decision-making.%20Distinct%20from%20traditional%0Aapproaches%2C%20our%20model%20excels%20in%20analyzing%20interactions%20and%20behavior%20patterns%20in%0Amixed%20autonomy%20traffic%20scenarios.%20It%20represents%20a%20significant%20leap%20forward%2C%0Aachieving%20marked%20performance%20improvements%20on%20several%20key%20datasets.%0ASpecifically%2C%20it%20surpasses%20existing%20benchmarks%20with%20gains%20of%2016.2%25%20on%20the%20Next%0AGeneration%20Simulation%20%28NGSIM%29%2C%2027.4%25%20on%20the%20Highway%20Drone%20%28HighD%29%2C%20and%2019.8%25%20on%0Athe%20Macao%20Connected%20Autonomous%20Driving%20%28MoCAD%29%20dataset.%20Our%20proposed%20model%0Ashows%20exceptional%20proficiency%20in%20handling%20corner%20cases%2C%20essential%20for%0Areal-world%20applications.%20Moreover%2C%20its%20robustness%20is%20evident%20in%20scenarios%20with%0Amissing%20or%20limited%20data%2C%20outperforming%20most%20of%20the%20state-of-the-art%20baselines.%0AThis%20adaptability%20and%20resilience%20position%20our%20model%20as%20a%20viable%20tool%20for%0Areal-world%20autonomous%20driving%20systems%2C%20heralding%20a%20new%20standard%20in%20vehicle%0Atrajectory%20prediction%20for%20enhanced%20safety%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17520v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Cognitive-Driven%20Trajectory%20Prediction%20Model%20for%20Autonomous%20Driving%20in%0A%20%20Mixed%20Autonomy%20Environment&entry.906535625=Haicheng%20Liao%20and%20Zhenning%20Li%20and%20Chengyue%20Wang%20and%20Bonan%20Wang%20and%20Hanlin%20Kong%20and%20Yanchen%20Guan%20and%20Guofa%20Li%20and%20Zhiyong%20Cui%20and%20Chengzhong%20Xu&entry.1292438233=%20%20As%20autonomous%20driving%20technology%20progresses%2C%20the%20need%20for%20precise%20trajectory%0Aprediction%20models%20becomes%20paramount.%20This%20paper%20introduces%20an%20innovative%20model%0Athat%20infuses%20cognitive%20insights%20into%20trajectory%20prediction%2C%20focusing%20on%0Aperceived%20safety%20and%20dynamic%20decision-making.%20Distinct%20from%20traditional%0Aapproaches%2C%20our%20model%20excels%20in%20analyzing%20interactions%20and%20behavior%20patterns%20in%0Amixed%20autonomy%20traffic%20scenarios.%20It%20represents%20a%20significant%20leap%20forward%2C%0Aachieving%20marked%20performance%20improvements%20on%20several%20key%20datasets.%0ASpecifically%2C%20it%20surpasses%20existing%20benchmarks%20with%20gains%20of%2016.2%25%20on%20the%20Next%0AGeneration%20Simulation%20%28NGSIM%29%2C%2027.4%25%20on%20the%20Highway%20Drone%20%28HighD%29%2C%20and%2019.8%25%20on%0Athe%20Macao%20Connected%20Autonomous%20Driving%20%28MoCAD%29%20dataset.%20Our%20proposed%20model%0Ashows%20exceptional%20proficiency%20in%20handling%20corner%20cases%2C%20essential%20for%0Areal-world%20applications.%20Moreover%2C%20its%20robustness%20is%20evident%20in%20scenarios%20with%0Amissing%20or%20limited%20data%2C%20outperforming%20most%20of%20the%20state-of-the-art%20baselines.%0AThis%20adaptability%20and%20resilience%20position%20our%20model%20as%20a%20viable%20tool%20for%0Areal-world%20autonomous%20driving%20systems%2C%20heralding%20a%20new%20standard%20in%20vehicle%0Atrajectory%20prediction%20for%20enhanced%20safety%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17520v1&entry.124074799=Read"},
{"title": "The LuViRA Dataset: Synchronized Vision, Radio, and Audio Sensors for\n  Indoor Localization", "author": "Ilayda Yaman and Guoda Tian and Martin Larsson and Patrik Persson and Michiel Sandra and Alexander D\u00fcrr and Erik Tegler and Nikhil Challa and Henrik Garde and Fredrik Tufvesson and Kalle \u00c5str\u00f6m and Ove Edfors and Steffen Malkowsky and Liang Liu", "abstract": "  We present a synchronized multisensory dataset for accurate and robust indoor\nlocalization: the Lund University Vision, Radio, and Audio (LuViRA) Dataset.\nThe dataset includes color images, corresponding depth maps, inertial\nmeasurement unit (IMU) readings, channel response between a 5G massive\nmultiple-input and multiple-output (MIMO) testbed and user equipment, audio\nrecorded by 12 microphones, and accurate six degrees of freedom (6DOF) pose\nground truth of 0.5 mm. We synchronize these sensors to ensure that all data is\nrecorded simultaneously. A camera, speaker, and transmit antenna are placed on\ntop of a slowly moving service robot, and 89 trajectories are recorded. Each\ntrajectory includes 20 to 50 seconds of recorded sensor data and ground truth\nlabels. Data from different sensors can be used separately or jointly to\nperform localization tasks, and data from the motion capture (mocap) system is\nused to verify the results obtained by the localization algorithms. The main\naim of this dataset is to enable research on sensor fusion with the most\ncommonly used sensors for localization tasks. Moreover, the full dataset or\nsome parts of it can also be used for other research areas such as channel\nestimation, image classification, etc. Our dataset is available at:\nhttps://github.com/ilaydayaman/LuViRA_Dataset\n", "link": "http://arxiv.org/abs/2302.05309v3", "date": "2024-04-26", "relevancy": 2.1851, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5919}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5141}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5128}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20LuViRA%20Dataset%3A%20Synchronized%20Vision%2C%20Radio%2C%20and%20Audio%20Sensors%20for%0A%20%20Indoor%20Localization&body=Title%3A%20The%20LuViRA%20Dataset%3A%20Synchronized%20Vision%2C%20Radio%2C%20and%20Audio%20Sensors%20for%0A%20%20Indoor%20Localization%0AAuthor%3A%20Ilayda%20Yaman%20and%20Guoda%20Tian%20and%20Martin%20Larsson%20and%20Patrik%20Persson%20and%20Michiel%20Sandra%20and%20Alexander%20D%C3%BCrr%20and%20Erik%20Tegler%20and%20Nikhil%20Challa%20and%20Henrik%20Garde%20and%20Fredrik%20Tufvesson%20and%20Kalle%20%C3%85str%C3%B6m%20and%20Ove%20Edfors%20and%20Steffen%20Malkowsky%20and%20Liang%20Liu%0AAbstract%3A%20%20%20We%20present%20a%20synchronized%20multisensory%20dataset%20for%20accurate%20and%20robust%20indoor%0Alocalization%3A%20the%20Lund%20University%20Vision%2C%20Radio%2C%20and%20Audio%20%28LuViRA%29%20Dataset.%0AThe%20dataset%20includes%20color%20images%2C%20corresponding%20depth%20maps%2C%20inertial%0Ameasurement%20unit%20%28IMU%29%20readings%2C%20channel%20response%20between%20a%205G%20massive%0Amultiple-input%20and%20multiple-output%20%28MIMO%29%20testbed%20and%20user%20equipment%2C%20audio%0Arecorded%20by%2012%20microphones%2C%20and%20accurate%20six%20degrees%20of%20freedom%20%286DOF%29%20pose%0Aground%20truth%20of%200.5%20mm.%20We%20synchronize%20these%20sensors%20to%20ensure%20that%20all%20data%20is%0Arecorded%20simultaneously.%20A%20camera%2C%20speaker%2C%20and%20transmit%20antenna%20are%20placed%20on%0Atop%20of%20a%20slowly%20moving%20service%20robot%2C%20and%2089%20trajectories%20are%20recorded.%20Each%0Atrajectory%20includes%2020%20to%2050%20seconds%20of%20recorded%20sensor%20data%20and%20ground%20truth%0Alabels.%20Data%20from%20different%20sensors%20can%20be%20used%20separately%20or%20jointly%20to%0Aperform%20localization%20tasks%2C%20and%20data%20from%20the%20motion%20capture%20%28mocap%29%20system%20is%0Aused%20to%20verify%20the%20results%20obtained%20by%20the%20localization%20algorithms.%20The%20main%0Aaim%20of%20this%20dataset%20is%20to%20enable%20research%20on%20sensor%20fusion%20with%20the%20most%0Acommonly%20used%20sensors%20for%20localization%20tasks.%20Moreover%2C%20the%20full%20dataset%20or%0Asome%20parts%20of%20it%20can%20also%20be%20used%20for%20other%20research%20areas%20such%20as%20channel%0Aestimation%2C%20image%20classification%2C%20etc.%20Our%20dataset%20is%20available%20at%3A%0Ahttps%3A//github.com/ilaydayaman/LuViRA_Dataset%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.05309v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20LuViRA%20Dataset%3A%20Synchronized%20Vision%2C%20Radio%2C%20and%20Audio%20Sensors%20for%0A%20%20Indoor%20Localization&entry.906535625=Ilayda%20Yaman%20and%20Guoda%20Tian%20and%20Martin%20Larsson%20and%20Patrik%20Persson%20and%20Michiel%20Sandra%20and%20Alexander%20D%C3%BCrr%20and%20Erik%20Tegler%20and%20Nikhil%20Challa%20and%20Henrik%20Garde%20and%20Fredrik%20Tufvesson%20and%20Kalle%20%C3%85str%C3%B6m%20and%20Ove%20Edfors%20and%20Steffen%20Malkowsky%20and%20Liang%20Liu&entry.1292438233=%20%20We%20present%20a%20synchronized%20multisensory%20dataset%20for%20accurate%20and%20robust%20indoor%0Alocalization%3A%20the%20Lund%20University%20Vision%2C%20Radio%2C%20and%20Audio%20%28LuViRA%29%20Dataset.%0AThe%20dataset%20includes%20color%20images%2C%20corresponding%20depth%20maps%2C%20inertial%0Ameasurement%20unit%20%28IMU%29%20readings%2C%20channel%20response%20between%20a%205G%20massive%0Amultiple-input%20and%20multiple-output%20%28MIMO%29%20testbed%20and%20user%20equipment%2C%20audio%0Arecorded%20by%2012%20microphones%2C%20and%20accurate%20six%20degrees%20of%20freedom%20%286DOF%29%20pose%0Aground%20truth%20of%200.5%20mm.%20We%20synchronize%20these%20sensors%20to%20ensure%20that%20all%20data%20is%0Arecorded%20simultaneously.%20A%20camera%2C%20speaker%2C%20and%20transmit%20antenna%20are%20placed%20on%0Atop%20of%20a%20slowly%20moving%20service%20robot%2C%20and%2089%20trajectories%20are%20recorded.%20Each%0Atrajectory%20includes%2020%20to%2050%20seconds%20of%20recorded%20sensor%20data%20and%20ground%20truth%0Alabels.%20Data%20from%20different%20sensors%20can%20be%20used%20separately%20or%20jointly%20to%0Aperform%20localization%20tasks%2C%20and%20data%20from%20the%20motion%20capture%20%28mocap%29%20system%20is%0Aused%20to%20verify%20the%20results%20obtained%20by%20the%20localization%20algorithms.%20The%20main%0Aaim%20of%20this%20dataset%20is%20to%20enable%20research%20on%20sensor%20fusion%20with%20the%20most%0Acommonly%20used%20sensors%20for%20localization%20tasks.%20Moreover%2C%20the%20full%20dataset%20or%0Asome%20parts%20of%20it%20can%20also%20be%20used%20for%20other%20research%20areas%20such%20as%20channel%0Aestimation%2C%20image%20classification%2C%20etc.%20Our%20dataset%20is%20available%20at%3A%0Ahttps%3A//github.com/ilaydayaman/LuViRA_Dataset%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.05309v3&entry.124074799=Read"},
{"title": "On the Road to Clarity: Exploring Explainable AI for World Models in a\n  Driver Assistance System", "author": "Mohamed Roshdi and Julian Petzold and Mostafa Wahby and Hussein Ebrahim and Mladen Berekovic and Heiko Hamann", "abstract": "  In Autonomous Driving (AD) transparency and safety are paramount, as mistakes\nare costly. However, neural networks used in AD systems are generally\nconsidered black boxes. As a countermeasure, we have methods of explainable AI\n(XAI), such as feature relevance estimation and dimensionality reduction.\nCoarse graining techniques can also help reduce dimensionality and find\ninterpretable global patterns. A specific coarse graining method is\nRenormalization Groups from statistical physics. It has previously been applied\nto Restricted Boltzmann Machines (RBMs) to interpret unsupervised learning. We\nrefine this technique by building a transparent backbone model for\nconvolutional variational autoencoders (VAE) that allows mapping latent values\nto input features and has performance comparable to trained black box VAEs.\nMoreover, we propose a custom feature map visualization technique to analyze\nthe internal convolutional layers in the VAE to explain internal causes of poor\nreconstruction that may lead to dangerous traffic scenarios in AD applications.\nIn a second key contribution, we propose explanation and evaluation techniques\nfor the internal dynamics and feature relevance of prediction networks. We test\na long short-term memory (LSTM) network in the computer vision domain to\nevaluate the predictability and in future applications potentially safety of\nprediction models. We showcase our methods by analyzing a VAE-LSTM world model\nthat predicts pedestrian perception in an urban traffic situation.\n", "link": "http://arxiv.org/abs/2404.17350v1", "date": "2024-04-26", "relevancy": 2.1829, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5658}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5481}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5353}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20the%20Road%20to%20Clarity%3A%20Exploring%20Explainable%20AI%20for%20World%20Models%20in%20a%0A%20%20Driver%20Assistance%20System&body=Title%3A%20On%20the%20Road%20to%20Clarity%3A%20Exploring%20Explainable%20AI%20for%20World%20Models%20in%20a%0A%20%20Driver%20Assistance%20System%0AAuthor%3A%20Mohamed%20Roshdi%20and%20Julian%20Petzold%20and%20Mostafa%20Wahby%20and%20Hussein%20Ebrahim%20and%20Mladen%20Berekovic%20and%20Heiko%20Hamann%0AAbstract%3A%20%20%20In%20Autonomous%20Driving%20%28AD%29%20transparency%20and%20safety%20are%20paramount%2C%20as%20mistakes%0Aare%20costly.%20However%2C%20neural%20networks%20used%20in%20AD%20systems%20are%20generally%0Aconsidered%20black%20boxes.%20As%20a%20countermeasure%2C%20we%20have%20methods%20of%20explainable%20AI%0A%28XAI%29%2C%20such%20as%20feature%20relevance%20estimation%20and%20dimensionality%20reduction.%0ACoarse%20graining%20techniques%20can%20also%20help%20reduce%20dimensionality%20and%20find%0Ainterpretable%20global%20patterns.%20A%20specific%20coarse%20graining%20method%20is%0ARenormalization%20Groups%20from%20statistical%20physics.%20It%20has%20previously%20been%20applied%0Ato%20Restricted%20Boltzmann%20Machines%20%28RBMs%29%20to%20interpret%20unsupervised%20learning.%20We%0Arefine%20this%20technique%20by%20building%20a%20transparent%20backbone%20model%20for%0Aconvolutional%20variational%20autoencoders%20%28VAE%29%20that%20allows%20mapping%20latent%20values%0Ato%20input%20features%20and%20has%20performance%20comparable%20to%20trained%20black%20box%20VAEs.%0AMoreover%2C%20we%20propose%20a%20custom%20feature%20map%20visualization%20technique%20to%20analyze%0Athe%20internal%20convolutional%20layers%20in%20the%20VAE%20to%20explain%20internal%20causes%20of%20poor%0Areconstruction%20that%20may%20lead%20to%20dangerous%20traffic%20scenarios%20in%20AD%20applications.%0AIn%20a%20second%20key%20contribution%2C%20we%20propose%20explanation%20and%20evaluation%20techniques%0Afor%20the%20internal%20dynamics%20and%20feature%20relevance%20of%20prediction%20networks.%20We%20test%0Aa%20long%20short-term%20memory%20%28LSTM%29%20network%20in%20the%20computer%20vision%20domain%20to%0Aevaluate%20the%20predictability%20and%20in%20future%20applications%20potentially%20safety%20of%0Aprediction%20models.%20We%20showcase%20our%20methods%20by%20analyzing%20a%20VAE-LSTM%20world%20model%0Athat%20predicts%20pedestrian%20perception%20in%20an%20urban%20traffic%20situation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17350v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Road%20to%20Clarity%3A%20Exploring%20Explainable%20AI%20for%20World%20Models%20in%20a%0A%20%20Driver%20Assistance%20System&entry.906535625=Mohamed%20Roshdi%20and%20Julian%20Petzold%20and%20Mostafa%20Wahby%20and%20Hussein%20Ebrahim%20and%20Mladen%20Berekovic%20and%20Heiko%20Hamann&entry.1292438233=%20%20In%20Autonomous%20Driving%20%28AD%29%20transparency%20and%20safety%20are%20paramount%2C%20as%20mistakes%0Aare%20costly.%20However%2C%20neural%20networks%20used%20in%20AD%20systems%20are%20generally%0Aconsidered%20black%20boxes.%20As%20a%20countermeasure%2C%20we%20have%20methods%20of%20explainable%20AI%0A%28XAI%29%2C%20such%20as%20feature%20relevance%20estimation%20and%20dimensionality%20reduction.%0ACoarse%20graining%20techniques%20can%20also%20help%20reduce%20dimensionality%20and%20find%0Ainterpretable%20global%20patterns.%20A%20specific%20coarse%20graining%20method%20is%0ARenormalization%20Groups%20from%20statistical%20physics.%20It%20has%20previously%20been%20applied%0Ato%20Restricted%20Boltzmann%20Machines%20%28RBMs%29%20to%20interpret%20unsupervised%20learning.%20We%0Arefine%20this%20technique%20by%20building%20a%20transparent%20backbone%20model%20for%0Aconvolutional%20variational%20autoencoders%20%28VAE%29%20that%20allows%20mapping%20latent%20values%0Ato%20input%20features%20and%20has%20performance%20comparable%20to%20trained%20black%20box%20VAEs.%0AMoreover%2C%20we%20propose%20a%20custom%20feature%20map%20visualization%20technique%20to%20analyze%0Athe%20internal%20convolutional%20layers%20in%20the%20VAE%20to%20explain%20internal%20causes%20of%20poor%0Areconstruction%20that%20may%20lead%20to%20dangerous%20traffic%20scenarios%20in%20AD%20applications.%0AIn%20a%20second%20key%20contribution%2C%20we%20propose%20explanation%20and%20evaluation%20techniques%0Afor%20the%20internal%20dynamics%20and%20feature%20relevance%20of%20prediction%20networks.%20We%20test%0Aa%20long%20short-term%20memory%20%28LSTM%29%20network%20in%20the%20computer%20vision%20domain%20to%0Aevaluate%20the%20predictability%20and%20in%20future%20applications%20potentially%20safety%20of%0Aprediction%20models.%20We%20showcase%20our%20methods%20by%20analyzing%20a%20VAE-LSTM%20world%20model%0Athat%20predicts%20pedestrian%20perception%20in%20an%20urban%20traffic%20situation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17350v1&entry.124074799=Read"},
{"title": "If It's Not Enough, Make It So: Reducing Authentic Data Demand in Face\n  Recognition through Synthetic Faces", "author": "Andrea Atzori and Fadi Boutros and Naser Damer and Gianni Fenu and Mirko Marras", "abstract": "  Recent advances in deep face recognition have spurred a growing demand for\nlarge, diverse, and manually annotated face datasets. Acquiring authentic,\nhigh-quality data for face recognition has proven to be a challenge, primarily\ndue to privacy concerns. Large face datasets are primarily sourced from\nweb-based images, lacking explicit user consent. In this paper, we examine\nwhether and how synthetic face data can be used to train effective face\nrecognition models with reduced reliance on authentic images, thereby\nmitigating data collection concerns. First, we explored the performance gap\namong recent state-of-the-art face recognition models, trained with synthetic\ndata only and authentic (scarce) data only. Then, we deepened our analysis by\ntraining a state-of-the-art backbone with various combinations of synthetic and\nauthentic data, gaining insights into optimizing the limited use of the latter\nfor verification accuracy. Finally, we assessed the effectiveness of data\naugmentation approaches on synthetic and authentic data, with the same goal in\nmind. Our results highlighted the effectiveness of FR trained on combined\ndatasets, particularly when combined with appropriate augmentation techniques.\n", "link": "http://arxiv.org/abs/2404.03537v4", "date": "2024-04-26", "relevancy": 2.1628, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5607}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5465}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5269}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20If%20It%27s%20Not%20Enough%2C%20Make%20It%20So%3A%20Reducing%20Authentic%20Data%20Demand%20in%20Face%0A%20%20Recognition%20through%20Synthetic%20Faces&body=Title%3A%20If%20It%27s%20Not%20Enough%2C%20Make%20It%20So%3A%20Reducing%20Authentic%20Data%20Demand%20in%20Face%0A%20%20Recognition%20through%20Synthetic%20Faces%0AAuthor%3A%20Andrea%20Atzori%20and%20Fadi%20Boutros%20and%20Naser%20Damer%20and%20Gianni%20Fenu%20and%20Mirko%20Marras%0AAbstract%3A%20%20%20Recent%20advances%20in%20deep%20face%20recognition%20have%20spurred%20a%20growing%20demand%20for%0Alarge%2C%20diverse%2C%20and%20manually%20annotated%20face%20datasets.%20Acquiring%20authentic%2C%0Ahigh-quality%20data%20for%20face%20recognition%20has%20proven%20to%20be%20a%20challenge%2C%20primarily%0Adue%20to%20privacy%20concerns.%20Large%20face%20datasets%20are%20primarily%20sourced%20from%0Aweb-based%20images%2C%20lacking%20explicit%20user%20consent.%20In%20this%20paper%2C%20we%20examine%0Awhether%20and%20how%20synthetic%20face%20data%20can%20be%20used%20to%20train%20effective%20face%0Arecognition%20models%20with%20reduced%20reliance%20on%20authentic%20images%2C%20thereby%0Amitigating%20data%20collection%20concerns.%20First%2C%20we%20explored%20the%20performance%20gap%0Aamong%20recent%20state-of-the-art%20face%20recognition%20models%2C%20trained%20with%20synthetic%0Adata%20only%20and%20authentic%20%28scarce%29%20data%20only.%20Then%2C%20we%20deepened%20our%20analysis%20by%0Atraining%20a%20state-of-the-art%20backbone%20with%20various%20combinations%20of%20synthetic%20and%0Aauthentic%20data%2C%20gaining%20insights%20into%20optimizing%20the%20limited%20use%20of%20the%20latter%0Afor%20verification%20accuracy.%20Finally%2C%20we%20assessed%20the%20effectiveness%20of%20data%0Aaugmentation%20approaches%20on%20synthetic%20and%20authentic%20data%2C%20with%20the%20same%20goal%20in%0Amind.%20Our%20results%20highlighted%20the%20effectiveness%20of%20FR%20trained%20on%20combined%0Adatasets%2C%20particularly%20when%20combined%20with%20appropriate%20augmentation%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03537v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=If%20It%27s%20Not%20Enough%2C%20Make%20It%20So%3A%20Reducing%20Authentic%20Data%20Demand%20in%20Face%0A%20%20Recognition%20through%20Synthetic%20Faces&entry.906535625=Andrea%20Atzori%20and%20Fadi%20Boutros%20and%20Naser%20Damer%20and%20Gianni%20Fenu%20and%20Mirko%20Marras&entry.1292438233=%20%20Recent%20advances%20in%20deep%20face%20recognition%20have%20spurred%20a%20growing%20demand%20for%0Alarge%2C%20diverse%2C%20and%20manually%20annotated%20face%20datasets.%20Acquiring%20authentic%2C%0Ahigh-quality%20data%20for%20face%20recognition%20has%20proven%20to%20be%20a%20challenge%2C%20primarily%0Adue%20to%20privacy%20concerns.%20Large%20face%20datasets%20are%20primarily%20sourced%20from%0Aweb-based%20images%2C%20lacking%20explicit%20user%20consent.%20In%20this%20paper%2C%20we%20examine%0Awhether%20and%20how%20synthetic%20face%20data%20can%20be%20used%20to%20train%20effective%20face%0Arecognition%20models%20with%20reduced%20reliance%20on%20authentic%20images%2C%20thereby%0Amitigating%20data%20collection%20concerns.%20First%2C%20we%20explored%20the%20performance%20gap%0Aamong%20recent%20state-of-the-art%20face%20recognition%20models%2C%20trained%20with%20synthetic%0Adata%20only%20and%20authentic%20%28scarce%29%20data%20only.%20Then%2C%20we%20deepened%20our%20analysis%20by%0Atraining%20a%20state-of-the-art%20backbone%20with%20various%20combinations%20of%20synthetic%20and%0Aauthentic%20data%2C%20gaining%20insights%20into%20optimizing%20the%20limited%20use%20of%20the%20latter%0Afor%20verification%20accuracy.%20Finally%2C%20we%20assessed%20the%20effectiveness%20of%20data%0Aaugmentation%20approaches%20on%20synthetic%20and%20authentic%20data%2C%20with%20the%20same%20goal%20in%0Amind.%20Our%20results%20highlighted%20the%20effectiveness%20of%20FR%20trained%20on%20combined%0Adatasets%2C%20particularly%20when%20combined%20with%20appropriate%20augmentation%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03537v4&entry.124074799=Read"},
{"title": "Image Clustering via the Principle of Rate Reduction in the Age of\n  Pretrained Models", "author": "Tianzhe Chu and Shengbang Tong and Tianjiao Ding and Xili Dai and Benjamin David Haeffele and Ren\u00e9 Vidal and Yi Ma", "abstract": "  The advent of large pre-trained models has brought about a paradigm shift in\nboth visual representation learning and natural language processing. However,\nclustering unlabeled images, as a fundamental and classic machine learning\nproblem, still lacks an effective solution, particularly for large-scale\ndatasets. In this paper, we propose a novel image clustering pipeline that\nleverages the powerful feature representation of large pre-trained models such\nas CLIP and cluster images effectively and efficiently at scale. We first\ndeveloped a novel algorithm to estimate the number of clusters in a given\ndataset. We then show that the pre-trained features are significantly more\nstructured by further optimizing the rate reduction objective. The resulting\nfeatures may significantly improve the clustering accuracy, e.g., from 57\\% to\n66\\% on ImageNet-1k. Furthermore, by leveraging CLIP's multimodality bridge\nbetween image and text, we develop a simple yet effective self-labeling\nalgorithm that produces meaningful captions for the clusters. Through extensive\nexperiments, we show that our pipeline works well on standard datasets such as\nCIFAR-10, CIFAR-100, and ImageNet-1k. It also extends to datasets that are not\ncurated for clustering, such as LAION-Aesthetics and WikiArts. We released the\ncode in https://github.com/LeslieTrue/CPP.\n", "link": "http://arxiv.org/abs/2306.05272v5", "date": "2024-04-26", "relevancy": 2.1572, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5615}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5276}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5132}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Image%20Clustering%20via%20the%20Principle%20of%20Rate%20Reduction%20in%20the%20Age%20of%0A%20%20Pretrained%20Models&body=Title%3A%20Image%20Clustering%20via%20the%20Principle%20of%20Rate%20Reduction%20in%20the%20Age%20of%0A%20%20Pretrained%20Models%0AAuthor%3A%20Tianzhe%20Chu%20and%20Shengbang%20Tong%20and%20Tianjiao%20Ding%20and%20Xili%20Dai%20and%20Benjamin%20David%20Haeffele%20and%20Ren%C3%A9%20Vidal%20and%20Yi%20Ma%0AAbstract%3A%20%20%20The%20advent%20of%20large%20pre-trained%20models%20has%20brought%20about%20a%20paradigm%20shift%20in%0Aboth%20visual%20representation%20learning%20and%20natural%20language%20processing.%20However%2C%0Aclustering%20unlabeled%20images%2C%20as%20a%20fundamental%20and%20classic%20machine%20learning%0Aproblem%2C%20still%20lacks%20an%20effective%20solution%2C%20particularly%20for%20large-scale%0Adatasets.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20image%20clustering%20pipeline%20that%0Aleverages%20the%20powerful%20feature%20representation%20of%20large%20pre-trained%20models%20such%0Aas%20CLIP%20and%20cluster%20images%20effectively%20and%20efficiently%20at%20scale.%20We%20first%0Adeveloped%20a%20novel%20algorithm%20to%20estimate%20the%20number%20of%20clusters%20in%20a%20given%0Adataset.%20We%20then%20show%20that%20the%20pre-trained%20features%20are%20significantly%20more%0Astructured%20by%20further%20optimizing%20the%20rate%20reduction%20objective.%20The%20resulting%0Afeatures%20may%20significantly%20improve%20the%20clustering%20accuracy%2C%20e.g.%2C%20from%2057%5C%25%20to%0A66%5C%25%20on%20ImageNet-1k.%20Furthermore%2C%20by%20leveraging%20CLIP%27s%20multimodality%20bridge%0Abetween%20image%20and%20text%2C%20we%20develop%20a%20simple%20yet%20effective%20self-labeling%0Aalgorithm%20that%20produces%20meaningful%20captions%20for%20the%20clusters.%20Through%20extensive%0Aexperiments%2C%20we%20show%20that%20our%20pipeline%20works%20well%20on%20standard%20datasets%20such%20as%0ACIFAR-10%2C%20CIFAR-100%2C%20and%20ImageNet-1k.%20It%20also%20extends%20to%20datasets%20that%20are%20not%0Acurated%20for%20clustering%2C%20such%20as%20LAION-Aesthetics%20and%20WikiArts.%20We%20released%20the%0Acode%20in%20https%3A//github.com/LeslieTrue/CPP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.05272v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Clustering%20via%20the%20Principle%20of%20Rate%20Reduction%20in%20the%20Age%20of%0A%20%20Pretrained%20Models&entry.906535625=Tianzhe%20Chu%20and%20Shengbang%20Tong%20and%20Tianjiao%20Ding%20and%20Xili%20Dai%20and%20Benjamin%20David%20Haeffele%20and%20Ren%C3%A9%20Vidal%20and%20Yi%20Ma&entry.1292438233=%20%20The%20advent%20of%20large%20pre-trained%20models%20has%20brought%20about%20a%20paradigm%20shift%20in%0Aboth%20visual%20representation%20learning%20and%20natural%20language%20processing.%20However%2C%0Aclustering%20unlabeled%20images%2C%20as%20a%20fundamental%20and%20classic%20machine%20learning%0Aproblem%2C%20still%20lacks%20an%20effective%20solution%2C%20particularly%20for%20large-scale%0Adatasets.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20image%20clustering%20pipeline%20that%0Aleverages%20the%20powerful%20feature%20representation%20of%20large%20pre-trained%20models%20such%0Aas%20CLIP%20and%20cluster%20images%20effectively%20and%20efficiently%20at%20scale.%20We%20first%0Adeveloped%20a%20novel%20algorithm%20to%20estimate%20the%20number%20of%20clusters%20in%20a%20given%0Adataset.%20We%20then%20show%20that%20the%20pre-trained%20features%20are%20significantly%20more%0Astructured%20by%20further%20optimizing%20the%20rate%20reduction%20objective.%20The%20resulting%0Afeatures%20may%20significantly%20improve%20the%20clustering%20accuracy%2C%20e.g.%2C%20from%2057%5C%25%20to%0A66%5C%25%20on%20ImageNet-1k.%20Furthermore%2C%20by%20leveraging%20CLIP%27s%20multimodality%20bridge%0Abetween%20image%20and%20text%2C%20we%20develop%20a%20simple%20yet%20effective%20self-labeling%0Aalgorithm%20that%20produces%20meaningful%20captions%20for%20the%20clusters.%20Through%20extensive%0Aexperiments%2C%20we%20show%20that%20our%20pipeline%20works%20well%20on%20standard%20datasets%20such%20as%0ACIFAR-10%2C%20CIFAR-100%2C%20and%20ImageNet-1k.%20It%20also%20extends%20to%20datasets%20that%20are%20not%0Acurated%20for%20clustering%2C%20such%20as%20LAION-Aesthetics%20and%20WikiArts.%20We%20released%20the%0Acode%20in%20https%3A//github.com/LeslieTrue/CPP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.05272v5&entry.124074799=Read"},
{"title": "LEAF: Unveiling Two Sides of the Same Coin in Semi-supervised Facial\n  Expression Recognition", "author": "Fan Zhang and Zhi-Qi Cheng and Jian Zhao and Xiaojiang Peng and Xuelong Li", "abstract": "  Semi-supervised learning has emerged as a promising approach to tackle the\nchallenge of label scarcity in facial expression recognition (FER) task.\nHowever, current state-of-the-art methods primarily focus on one side of the\ncoin, i.e., generating high-quality pseudo-labels, while overlooking the other\nside: enhancing expression-relevant representations. In this paper, we unveil\nboth sides of the coin by proposing a unified framework termed hierarchicaL\ndEcoupling And Fusing (LEAF) to coordinate expression-relevant representations\nand pseudo-labels for semi-supervised FER. LEAF introduces a hierarchical\nexpression-aware aggregation strategy that operates at three levels: semantic,\ninstance, and category. (1) At the semantic and instance levels, LEAF decouples\nrepresentations into expression-agnostic and expression-relevant components,\nand adaptively fuses them using learnable gating weights. (2) At the category\nlevel, LEAF assigns ambiguous pseudo-labels by decoupling predictions into\npositive and negative parts, and employs a consistency loss to ensure agreement\nbetween two augmented views of the same image. Extensive experiments on\nbenchmark datasets demonstrate that by unveiling and harmonizing both sides of\nthe coin, LEAF outperforms state-of-the-art semi-supervised FER methods,\neffectively leveraging both labeled and unlabeled data. Moreover, the proposed\nexpression-aware aggregation strategy can be seamlessly integrated into\nexisting semi-supervised frameworks, leading to significant performance gains.\nOur code is available at https://anonymous.4open.science/r/LEAF-BC57/.\n", "link": "http://arxiv.org/abs/2404.15041v2", "date": "2024-04-26", "relevancy": 2.138, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5474}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5377}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5262}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LEAF%3A%20Unveiling%20Two%20Sides%20of%20the%20Same%20Coin%20in%20Semi-supervised%20Facial%0A%20%20Expression%20Recognition&body=Title%3A%20LEAF%3A%20Unveiling%20Two%20Sides%20of%20the%20Same%20Coin%20in%20Semi-supervised%20Facial%0A%20%20Expression%20Recognition%0AAuthor%3A%20Fan%20Zhang%20and%20Zhi-Qi%20Cheng%20and%20Jian%20Zhao%20and%20Xiaojiang%20Peng%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Semi-supervised%20learning%20has%20emerged%20as%20a%20promising%20approach%20to%20tackle%20the%0Achallenge%20of%20label%20scarcity%20in%20facial%20expression%20recognition%20%28FER%29%20task.%0AHowever%2C%20current%20state-of-the-art%20methods%20primarily%20focus%20on%20one%20side%20of%20the%0Acoin%2C%20i.e.%2C%20generating%20high-quality%20pseudo-labels%2C%20while%20overlooking%20the%20other%0Aside%3A%20enhancing%20expression-relevant%20representations.%20In%20this%20paper%2C%20we%20unveil%0Aboth%20sides%20of%20the%20coin%20by%20proposing%20a%20unified%20framework%20termed%20hierarchicaL%0AdEcoupling%20And%20Fusing%20%28LEAF%29%20to%20coordinate%20expression-relevant%20representations%0Aand%20pseudo-labels%20for%20semi-supervised%20FER.%20LEAF%20introduces%20a%20hierarchical%0Aexpression-aware%20aggregation%20strategy%20that%20operates%20at%20three%20levels%3A%20semantic%2C%0Ainstance%2C%20and%20category.%20%281%29%20At%20the%20semantic%20and%20instance%20levels%2C%20LEAF%20decouples%0Arepresentations%20into%20expression-agnostic%20and%20expression-relevant%20components%2C%0Aand%20adaptively%20fuses%20them%20using%20learnable%20gating%20weights.%20%282%29%20At%20the%20category%0Alevel%2C%20LEAF%20assigns%20ambiguous%20pseudo-labels%20by%20decoupling%20predictions%20into%0Apositive%20and%20negative%20parts%2C%20and%20employs%20a%20consistency%20loss%20to%20ensure%20agreement%0Abetween%20two%20augmented%20views%20of%20the%20same%20image.%20Extensive%20experiments%20on%0Abenchmark%20datasets%20demonstrate%20that%20by%20unveiling%20and%20harmonizing%20both%20sides%20of%0Athe%20coin%2C%20LEAF%20outperforms%20state-of-the-art%20semi-supervised%20FER%20methods%2C%0Aeffectively%20leveraging%20both%20labeled%20and%20unlabeled%20data.%20Moreover%2C%20the%20proposed%0Aexpression-aware%20aggregation%20strategy%20can%20be%20seamlessly%20integrated%20into%0Aexisting%20semi-supervised%20frameworks%2C%20leading%20to%20significant%20performance%20gains.%0AOur%20code%20is%20available%20at%20https%3A//anonymous.4open.science/r/LEAF-BC57/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15041v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEAF%3A%20Unveiling%20Two%20Sides%20of%20the%20Same%20Coin%20in%20Semi-supervised%20Facial%0A%20%20Expression%20Recognition&entry.906535625=Fan%20Zhang%20and%20Zhi-Qi%20Cheng%20and%20Jian%20Zhao%20and%20Xiaojiang%20Peng%20and%20Xuelong%20Li&entry.1292438233=%20%20Semi-supervised%20learning%20has%20emerged%20as%20a%20promising%20approach%20to%20tackle%20the%0Achallenge%20of%20label%20scarcity%20in%20facial%20expression%20recognition%20%28FER%29%20task.%0AHowever%2C%20current%20state-of-the-art%20methods%20primarily%20focus%20on%20one%20side%20of%20the%0Acoin%2C%20i.e.%2C%20generating%20high-quality%20pseudo-labels%2C%20while%20overlooking%20the%20other%0Aside%3A%20enhancing%20expression-relevant%20representations.%20In%20this%20paper%2C%20we%20unveil%0Aboth%20sides%20of%20the%20coin%20by%20proposing%20a%20unified%20framework%20termed%20hierarchicaL%0AdEcoupling%20And%20Fusing%20%28LEAF%29%20to%20coordinate%20expression-relevant%20representations%0Aand%20pseudo-labels%20for%20semi-supervised%20FER.%20LEAF%20introduces%20a%20hierarchical%0Aexpression-aware%20aggregation%20strategy%20that%20operates%20at%20three%20levels%3A%20semantic%2C%0Ainstance%2C%20and%20category.%20%281%29%20At%20the%20semantic%20and%20instance%20levels%2C%20LEAF%20decouples%0Arepresentations%20into%20expression-agnostic%20and%20expression-relevant%20components%2C%0Aand%20adaptively%20fuses%20them%20using%20learnable%20gating%20weights.%20%282%29%20At%20the%20category%0Alevel%2C%20LEAF%20assigns%20ambiguous%20pseudo-labels%20by%20decoupling%20predictions%20into%0Apositive%20and%20negative%20parts%2C%20and%20employs%20a%20consistency%20loss%20to%20ensure%20agreement%0Abetween%20two%20augmented%20views%20of%20the%20same%20image.%20Extensive%20experiments%20on%0Abenchmark%20datasets%20demonstrate%20that%20by%20unveiling%20and%20harmonizing%20both%20sides%20of%0Athe%20coin%2C%20LEAF%20outperforms%20state-of-the-art%20semi-supervised%20FER%20methods%2C%0Aeffectively%20leveraging%20both%20labeled%20and%20unlabeled%20data.%20Moreover%2C%20the%20proposed%0Aexpression-aware%20aggregation%20strategy%20can%20be%20seamlessly%20integrated%20into%0Aexisting%20semi-supervised%20frameworks%2C%20leading%20to%20significant%20performance%20gains.%0AOur%20code%20is%20available%20at%20https%3A//anonymous.4open.science/r/LEAF-BC57/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15041v2&entry.124074799=Read"},
{"title": "Efficient Deep Spiking Multi-Layer Perceptrons with Multiplication-Free\n  Inference", "author": "Boyan Li and Luziwei Leng and Shuaijie Shen and Kaixuan Zhang and Jianguo Zhang and Jianxing Liao and Ran Cheng", "abstract": "  Advancements in adapting deep convolution architectures for Spiking Neural\nNetworks (SNNs) have significantly enhanced image classification performance\nand reduced computational burdens. However, the inability of\nMultiplication-Free Inference (MFI) to align with attention and transformer\nmechanisms, which are critical to superior performance on high-resolution\nvision tasks, imposing limitations on these gains. To address this, our\nresearch explores a new pathway, drawing inspiration from the progress made in\nMulti-Layer Perceptrons (MLPs). We propose an innovative spiking MLP\narchitecture that uses batch normalization to retain MFI compatibility and\nintroducing a spiking patch encoding layer to enhance local feature extraction\ncapabilities. As a result, we establish an efficient multi-stage spiking MLP\nnetwork that blends effectively global receptive fields with local feature\nextraction for comprehensive spike-based computation. Without relying on\npre-training or sophisticated SNN training techniques, our network secures a\ntop-1 accuracy of 66.39% on the ImageNet-1K dataset, surpassing the directly\ntrained spiking ResNet-34 by 2.67%. Furthermore, we curtail computational\ncosts, model parameters, and simulation steps. An expanded version of our\nnetwork compares with the performance of the spiking VGG-16 network with a\n71.64% top-1 accuracy, all while operating with a model capacity 2.1 times\nsmaller. Our findings highlight the potential of our deep SNN architecture in\neffectively integrating global and local learning abilities. Interestingly, the\ntrained receptive field in our network mirrors the activity patterns of\ncortical cells. Source codes are publicly accessible at\nhttps://github.com/EMI-Group/mixer-snn.\n", "link": "http://arxiv.org/abs/2306.12465v3", "date": "2024-04-26", "relevancy": 2.1248, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5699}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5222}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4962}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Efficient%20Deep%20Spiking%20Multi-Layer%20Perceptrons%20with%20Multiplication-Free%0A%20%20Inference&body=Title%3A%20Efficient%20Deep%20Spiking%20Multi-Layer%20Perceptrons%20with%20Multiplication-Free%0A%20%20Inference%0AAuthor%3A%20Boyan%20Li%20and%20Luziwei%20Leng%20and%20Shuaijie%20Shen%20and%20Kaixuan%20Zhang%20and%20Jianguo%20Zhang%20and%20Jianxing%20Liao%20and%20Ran%20Cheng%0AAbstract%3A%20%20%20Advancements%20in%20adapting%20deep%20convolution%20architectures%20for%20Spiking%20Neural%0ANetworks%20%28SNNs%29%20have%20significantly%20enhanced%20image%20classification%20performance%0Aand%20reduced%20computational%20burdens.%20However%2C%20the%20inability%20of%0AMultiplication-Free%20Inference%20%28MFI%29%20to%20align%20with%20attention%20and%20transformer%0Amechanisms%2C%20which%20are%20critical%20to%20superior%20performance%20on%20high-resolution%0Avision%20tasks%2C%20imposing%20limitations%20on%20these%20gains.%20To%20address%20this%2C%20our%0Aresearch%20explores%20a%20new%20pathway%2C%20drawing%20inspiration%20from%20the%20progress%20made%20in%0AMulti-Layer%20Perceptrons%20%28MLPs%29.%20We%20propose%20an%20innovative%20spiking%20MLP%0Aarchitecture%20that%20uses%20batch%20normalization%20to%20retain%20MFI%20compatibility%20and%0Aintroducing%20a%20spiking%20patch%20encoding%20layer%20to%20enhance%20local%20feature%20extraction%0Acapabilities.%20As%20a%20result%2C%20we%20establish%20an%20efficient%20multi-stage%20spiking%20MLP%0Anetwork%20that%20blends%20effectively%20global%20receptive%20fields%20with%20local%20feature%0Aextraction%20for%20comprehensive%20spike-based%20computation.%20Without%20relying%20on%0Apre-training%20or%20sophisticated%20SNN%20training%20techniques%2C%20our%20network%20secures%20a%0Atop-1%20accuracy%20of%2066.39%25%20on%20the%20ImageNet-1K%20dataset%2C%20surpassing%20the%20directly%0Atrained%20spiking%20ResNet-34%20by%202.67%25.%20Furthermore%2C%20we%20curtail%20computational%0Acosts%2C%20model%20parameters%2C%20and%20simulation%20steps.%20An%20expanded%20version%20of%20our%0Anetwork%20compares%20with%20the%20performance%20of%20the%20spiking%20VGG-16%20network%20with%20a%0A71.64%25%20top-1%20accuracy%2C%20all%20while%20operating%20with%20a%20model%20capacity%202.1%20times%0Asmaller.%20Our%20findings%20highlight%20the%20potential%20of%20our%20deep%20SNN%20architecture%20in%0Aeffectively%20integrating%20global%20and%20local%20learning%20abilities.%20Interestingly%2C%20the%0Atrained%20receptive%20field%20in%20our%20network%20mirrors%20the%20activity%20patterns%20of%0Acortical%20cells.%20Source%20codes%20are%20publicly%20accessible%20at%0Ahttps%3A//github.com/EMI-Group/mixer-snn.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.12465v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Deep%20Spiking%20Multi-Layer%20Perceptrons%20with%20Multiplication-Free%0A%20%20Inference&entry.906535625=Boyan%20Li%20and%20Luziwei%20Leng%20and%20Shuaijie%20Shen%20and%20Kaixuan%20Zhang%20and%20Jianguo%20Zhang%20and%20Jianxing%20Liao%20and%20Ran%20Cheng&entry.1292438233=%20%20Advancements%20in%20adapting%20deep%20convolution%20architectures%20for%20Spiking%20Neural%0ANetworks%20%28SNNs%29%20have%20significantly%20enhanced%20image%20classification%20performance%0Aand%20reduced%20computational%20burdens.%20However%2C%20the%20inability%20of%0AMultiplication-Free%20Inference%20%28MFI%29%20to%20align%20with%20attention%20and%20transformer%0Amechanisms%2C%20which%20are%20critical%20to%20superior%20performance%20on%20high-resolution%0Avision%20tasks%2C%20imposing%20limitations%20on%20these%20gains.%20To%20address%20this%2C%20our%0Aresearch%20explores%20a%20new%20pathway%2C%20drawing%20inspiration%20from%20the%20progress%20made%20in%0AMulti-Layer%20Perceptrons%20%28MLPs%29.%20We%20propose%20an%20innovative%20spiking%20MLP%0Aarchitecture%20that%20uses%20batch%20normalization%20to%20retain%20MFI%20compatibility%20and%0Aintroducing%20a%20spiking%20patch%20encoding%20layer%20to%20enhance%20local%20feature%20extraction%0Acapabilities.%20As%20a%20result%2C%20we%20establish%20an%20efficient%20multi-stage%20spiking%20MLP%0Anetwork%20that%20blends%20effectively%20global%20receptive%20fields%20with%20local%20feature%0Aextraction%20for%20comprehensive%20spike-based%20computation.%20Without%20relying%20on%0Apre-training%20or%20sophisticated%20SNN%20training%20techniques%2C%20our%20network%20secures%20a%0Atop-1%20accuracy%20of%2066.39%25%20on%20the%20ImageNet-1K%20dataset%2C%20surpassing%20the%20directly%0Atrained%20spiking%20ResNet-34%20by%202.67%25.%20Furthermore%2C%20we%20curtail%20computational%0Acosts%2C%20model%20parameters%2C%20and%20simulation%20steps.%20An%20expanded%20version%20of%20our%0Anetwork%20compares%20with%20the%20performance%20of%20the%20spiking%20VGG-16%20network%20with%20a%0A71.64%25%20top-1%20accuracy%2C%20all%20while%20operating%20with%20a%20model%20capacity%202.1%20times%0Asmaller.%20Our%20findings%20highlight%20the%20potential%20of%20our%20deep%20SNN%20architecture%20in%0Aeffectively%20integrating%20global%20and%20local%20learning%20abilities.%20Interestingly%2C%20the%0Atrained%20receptive%20field%20in%20our%20network%20mirrors%20the%20activity%20patterns%20of%0Acortical%20cells.%20Source%20codes%20are%20publicly%20accessible%20at%0Ahttps%3A//github.com/EMI-Group/mixer-snn.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.12465v3&entry.124074799=Read"},
{"title": "MaPa: Text-driven Photorealistic Material Painting for 3D Shapes", "author": "Shangzhan Zhang and Sida Peng and Tao Xu and Yuanbo Yang and Tianrun Chen and Nan Xue and Yujun Shen and Hujun Bao and Ruizhen Hu and Xiaowei Zhou", "abstract": "  This paper aims to generate materials for 3D meshes from text descriptions.\nUnlike existing methods that synthesize texture maps, we propose to generate\nsegment-wise procedural material graphs as the appearance representation, which\nsupports high-quality rendering and provides substantial flexibility in\nediting. Instead of relying on extensive paired data, i.e., 3D meshes with\nmaterial graphs and corresponding text descriptions, to train a material graph\ngenerative model, we propose to leverage the pre-trained 2D diffusion model as\na bridge to connect the text and material graphs. Specifically, our approach\ndecomposes a shape into a set of segments and designs a segment-controlled\ndiffusion model to synthesize 2D images that are aligned with mesh parts. Based\non generated images, we initialize parameters of material graphs and fine-tune\nthem through the differentiable rendering module to produce materials in\naccordance with the textual description. Extensive experiments demonstrate the\nsuperior performance of our framework in photorealism, resolution, and\neditability over existing methods. Project page:\nhttps://zhanghe3z.github.io/MaPa/\n", "link": "http://arxiv.org/abs/2404.17569v1", "date": "2024-04-26", "relevancy": 2.109, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5375}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5266}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5173}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MaPa%3A%20Text-driven%20Photorealistic%20Material%20Painting%20for%203D%20Shapes&body=Title%3A%20MaPa%3A%20Text-driven%20Photorealistic%20Material%20Painting%20for%203D%20Shapes%0AAuthor%3A%20Shangzhan%20Zhang%20and%20Sida%20Peng%20and%20Tao%20Xu%20and%20Yuanbo%20Yang%20and%20Tianrun%20Chen%20and%20Nan%20Xue%20and%20Yujun%20Shen%20and%20Hujun%20Bao%20and%20Ruizhen%20Hu%20and%20Xiaowei%20Zhou%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20generate%20materials%20for%203D%20meshes%20from%20text%20descriptions.%0AUnlike%20existing%20methods%20that%20synthesize%20texture%20maps%2C%20we%20propose%20to%20generate%0Asegment-wise%20procedural%20material%20graphs%20as%20the%20appearance%20representation%2C%20which%0Asupports%20high-quality%20rendering%20and%20provides%20substantial%20flexibility%20in%0Aediting.%20Instead%20of%20relying%20on%20extensive%20paired%20data%2C%20i.e.%2C%203D%20meshes%20with%0Amaterial%20graphs%20and%20corresponding%20text%20descriptions%2C%20to%20train%20a%20material%20graph%0Agenerative%20model%2C%20we%20propose%20to%20leverage%20the%20pre-trained%202D%20diffusion%20model%20as%0Aa%20bridge%20to%20connect%20the%20text%20and%20material%20graphs.%20Specifically%2C%20our%20approach%0Adecomposes%20a%20shape%20into%20a%20set%20of%20segments%20and%20designs%20a%20segment-controlled%0Adiffusion%20model%20to%20synthesize%202D%20images%20that%20are%20aligned%20with%20mesh%20parts.%20Based%0Aon%20generated%20images%2C%20we%20initialize%20parameters%20of%20material%20graphs%20and%20fine-tune%0Athem%20through%20the%20differentiable%20rendering%20module%20to%20produce%20materials%20in%0Aaccordance%20with%20the%20textual%20description.%20Extensive%20experiments%20demonstrate%20the%0Asuperior%20performance%20of%20our%20framework%20in%20photorealism%2C%20resolution%2C%20and%0Aeditability%20over%20existing%20methods.%20Project%20page%3A%0Ahttps%3A//zhanghe3z.github.io/MaPa/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17569v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaPa%3A%20Text-driven%20Photorealistic%20Material%20Painting%20for%203D%20Shapes&entry.906535625=Shangzhan%20Zhang%20and%20Sida%20Peng%20and%20Tao%20Xu%20and%20Yuanbo%20Yang%20and%20Tianrun%20Chen%20and%20Nan%20Xue%20and%20Yujun%20Shen%20and%20Hujun%20Bao%20and%20Ruizhen%20Hu%20and%20Xiaowei%20Zhou&entry.1292438233=%20%20This%20paper%20aims%20to%20generate%20materials%20for%203D%20meshes%20from%20text%20descriptions.%0AUnlike%20existing%20methods%20that%20synthesize%20texture%20maps%2C%20we%20propose%20to%20generate%0Asegment-wise%20procedural%20material%20graphs%20as%20the%20appearance%20representation%2C%20which%0Asupports%20high-quality%20rendering%20and%20provides%20substantial%20flexibility%20in%0Aediting.%20Instead%20of%20relying%20on%20extensive%20paired%20data%2C%20i.e.%2C%203D%20meshes%20with%0Amaterial%20graphs%20and%20corresponding%20text%20descriptions%2C%20to%20train%20a%20material%20graph%0Agenerative%20model%2C%20we%20propose%20to%20leverage%20the%20pre-trained%202D%20diffusion%20model%20as%0Aa%20bridge%20to%20connect%20the%20text%20and%20material%20graphs.%20Specifically%2C%20our%20approach%0Adecomposes%20a%20shape%20into%20a%20set%20of%20segments%20and%20designs%20a%20segment-controlled%0Adiffusion%20model%20to%20synthesize%202D%20images%20that%20are%20aligned%20with%20mesh%20parts.%20Based%0Aon%20generated%20images%2C%20we%20initialize%20parameters%20of%20material%20graphs%20and%20fine-tune%0Athem%20through%20the%20differentiable%20rendering%20module%20to%20produce%20materials%20in%0Aaccordance%20with%20the%20textual%20description.%20Extensive%20experiments%20demonstrate%20the%0Asuperior%20performance%20of%20our%20framework%20in%20photorealism%2C%20resolution%2C%20and%0Aeditability%20over%20existing%20methods.%20Project%20page%3A%0Ahttps%3A//zhanghe3z.github.io/MaPa/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17569v1&entry.124074799=Read"},
{"title": "Gait Recognition in Large-scale Free Environment via Single LiDAR", "author": "Xiao Han and Yiming Ren and Peishan Cong and Yujing Sun and Jingya Wang and Lan Xu and Yuexin Ma", "abstract": "  Human gait recognition is crucial in multimedia, enabling identification\nthrough walking patterns without direct interaction, enhancing the integration\nacross various media forms in real-world applications like smart homes,\nhealthcare and non-intrusive security. LiDAR's ability to capture depth makes\nit pivotal for robotic perception and holds promise for real-world gait\nrecognition. In this paper, based on a single LiDAR, we present the\nHierarchical Multi-representation Feature Interaction Network (HMRNet) for\nrobust gait recognition. Prevailing LiDAR-based gait datasets primarily derive\nfrom controlled settings with predefined trajectory, remaining a gap with\nreal-world scenarios. To facilitate LiDAR-based gait recognition research, we\nintroduce FreeGait, a comprehensive gait dataset from large-scale,\nunconstrained settings, enriched with multi-modal and varied 2D/3D data.\nNotably, our approach achieves state-of-the-art performance on prior dataset\n(SUSTech1K) and on FreeGait. Code and dataset will be released upon publication\nof this paper.\n", "link": "http://arxiv.org/abs/2211.12371v2", "date": "2024-04-26", "relevancy": 2.0922, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5495}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5205}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.515}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Gait%20Recognition%20in%20Large-scale%20Free%20Environment%20via%20Single%20LiDAR&body=Title%3A%20Gait%20Recognition%20in%20Large-scale%20Free%20Environment%20via%20Single%20LiDAR%0AAuthor%3A%20Xiao%20Han%20and%20Yiming%20Ren%20and%20Peishan%20Cong%20and%20Yujing%20Sun%20and%20Jingya%20Wang%20and%20Lan%20Xu%20and%20Yuexin%20Ma%0AAbstract%3A%20%20%20Human%20gait%20recognition%20is%20crucial%20in%20multimedia%2C%20enabling%20identification%0Athrough%20walking%20patterns%20without%20direct%20interaction%2C%20enhancing%20the%20integration%0Aacross%20various%20media%20forms%20in%20real-world%20applications%20like%20smart%20homes%2C%0Ahealthcare%20and%20non-intrusive%20security.%20LiDAR%27s%20ability%20to%20capture%20depth%20makes%0Ait%20pivotal%20for%20robotic%20perception%20and%20holds%20promise%20for%20real-world%20gait%0Arecognition.%20In%20this%20paper%2C%20based%20on%20a%20single%20LiDAR%2C%20we%20present%20the%0AHierarchical%20Multi-representation%20Feature%20Interaction%20Network%20%28HMRNet%29%20for%0Arobust%20gait%20recognition.%20Prevailing%20LiDAR-based%20gait%20datasets%20primarily%20derive%0Afrom%20controlled%20settings%20with%20predefined%20trajectory%2C%20remaining%20a%20gap%20with%0Areal-world%20scenarios.%20To%20facilitate%20LiDAR-based%20gait%20recognition%20research%2C%20we%0Aintroduce%20FreeGait%2C%20a%20comprehensive%20gait%20dataset%20from%20large-scale%2C%0Aunconstrained%20settings%2C%20enriched%20with%20multi-modal%20and%20varied%202D/3D%20data.%0ANotably%2C%20our%20approach%20achieves%20state-of-the-art%20performance%20on%20prior%20dataset%0A%28SUSTech1K%29%20and%20on%20FreeGait.%20Code%20and%20dataset%20will%20be%20released%20upon%20publication%0Aof%20this%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.12371v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gait%20Recognition%20in%20Large-scale%20Free%20Environment%20via%20Single%20LiDAR&entry.906535625=Xiao%20Han%20and%20Yiming%20Ren%20and%20Peishan%20Cong%20and%20Yujing%20Sun%20and%20Jingya%20Wang%20and%20Lan%20Xu%20and%20Yuexin%20Ma&entry.1292438233=%20%20Human%20gait%20recognition%20is%20crucial%20in%20multimedia%2C%20enabling%20identification%0Athrough%20walking%20patterns%20without%20direct%20interaction%2C%20enhancing%20the%20integration%0Aacross%20various%20media%20forms%20in%20real-world%20applications%20like%20smart%20homes%2C%0Ahealthcare%20and%20non-intrusive%20security.%20LiDAR%27s%20ability%20to%20capture%20depth%20makes%0Ait%20pivotal%20for%20robotic%20perception%20and%20holds%20promise%20for%20real-world%20gait%0Arecognition.%20In%20this%20paper%2C%20based%20on%20a%20single%20LiDAR%2C%20we%20present%20the%0AHierarchical%20Multi-representation%20Feature%20Interaction%20Network%20%28HMRNet%29%20for%0Arobust%20gait%20recognition.%20Prevailing%20LiDAR-based%20gait%20datasets%20primarily%20derive%0Afrom%20controlled%20settings%20with%20predefined%20trajectory%2C%20remaining%20a%20gap%20with%0Areal-world%20scenarios.%20To%20facilitate%20LiDAR-based%20gait%20recognition%20research%2C%20we%0Aintroduce%20FreeGait%2C%20a%20comprehensive%20gait%20dataset%20from%20large-scale%2C%0Aunconstrained%20settings%2C%20enriched%20with%20multi-modal%20and%20varied%202D/3D%20data.%0ANotably%2C%20our%20approach%20achieves%20state-of-the-art%20performance%20on%20prior%20dataset%0A%28SUSTech1K%29%20and%20on%20FreeGait.%20Code%20and%20dataset%20will%20be%20released%20upon%20publication%0Aof%20this%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.12371v2&entry.124074799=Read"},
{"title": "Audio-Visual Person Verification based on Recursive Fusion of Joint\n  Cross-Attention", "author": "R. Gnana Praveen and Jahangir Alam", "abstract": "  Person or identity verification has been recently gaining a lot of attention\nusing audio-visual fusion as faces and voices share close associations with\neach other. Conventional approaches based on audio-visual fusion rely on\nscore-level or early feature-level fusion techniques. Though existing\napproaches showed improvement over unimodal systems, the potential of\naudio-visual fusion for person verification is not fully exploited. In this\npaper, we have investigated the prospect of effectively capturing both the\nintra- and inter-modal relationships across audio and visual modalities, which\ncan play a crucial role in significantly improving the fusion performance over\nunimodal systems. In particular, we introduce a recursive fusion of a joint\ncross-attentional model, where a joint audio-visual feature representation is\nemployed in the cross-attention framework in a recursive fashion to\nprogressively refine the feature representations that can efficiently capture\nthe intra-and inter-modal relationships. To further enhance the audio-visual\nfeature representations, we have also explored BLSTMs to improve the temporal\nmodeling of audio-visual feature representations. Extensive experiments are\nconducted on the Voxceleb1 dataset to evaluate the proposed model. Results\nindicate that the proposed model shows promising improvement in fusion\nperformance by adeptly capturing the intra-and inter-modal relationships across\naudio and visual modalities.\n", "link": "http://arxiv.org/abs/2403.04654v3", "date": "2024-04-26", "relevancy": 2.061, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5186}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5129}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5127}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Audio-Visual%20Person%20Verification%20based%20on%20Recursive%20Fusion%20of%20Joint%0A%20%20Cross-Attention&body=Title%3A%20Audio-Visual%20Person%20Verification%20based%20on%20Recursive%20Fusion%20of%20Joint%0A%20%20Cross-Attention%0AAuthor%3A%20R.%20Gnana%20Praveen%20and%20Jahangir%20Alam%0AAbstract%3A%20%20%20Person%20or%20identity%20verification%20has%20been%20recently%20gaining%20a%20lot%20of%20attention%0Ausing%20audio-visual%20fusion%20as%20faces%20and%20voices%20share%20close%20associations%20with%0Aeach%20other.%20Conventional%20approaches%20based%20on%20audio-visual%20fusion%20rely%20on%0Ascore-level%20or%20early%20feature-level%20fusion%20techniques.%20Though%20existing%0Aapproaches%20showed%20improvement%20over%20unimodal%20systems%2C%20the%20potential%20of%0Aaudio-visual%20fusion%20for%20person%20verification%20is%20not%20fully%20exploited.%20In%20this%0Apaper%2C%20we%20have%20investigated%20the%20prospect%20of%20effectively%20capturing%20both%20the%0Aintra-%20and%20inter-modal%20relationships%20across%20audio%20and%20visual%20modalities%2C%20which%0Acan%20play%20a%20crucial%20role%20in%20significantly%20improving%20the%20fusion%20performance%20over%0Aunimodal%20systems.%20In%20particular%2C%20we%20introduce%20a%20recursive%20fusion%20of%20a%20joint%0Across-attentional%20model%2C%20where%20a%20joint%20audio-visual%20feature%20representation%20is%0Aemployed%20in%20the%20cross-attention%20framework%20in%20a%20recursive%20fashion%20to%0Aprogressively%20refine%20the%20feature%20representations%20that%20can%20efficiently%20capture%0Athe%20intra-and%20inter-modal%20relationships.%20To%20further%20enhance%20the%20audio-visual%0Afeature%20representations%2C%20we%20have%20also%20explored%20BLSTMs%20to%20improve%20the%20temporal%0Amodeling%20of%20audio-visual%20feature%20representations.%20Extensive%20experiments%20are%0Aconducted%20on%20the%20Voxceleb1%20dataset%20to%20evaluate%20the%20proposed%20model.%20Results%0Aindicate%20that%20the%20proposed%20model%20shows%20promising%20improvement%20in%20fusion%0Aperformance%20by%20adeptly%20capturing%20the%20intra-and%20inter-modal%20relationships%20across%0Aaudio%20and%20visual%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04654v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio-Visual%20Person%20Verification%20based%20on%20Recursive%20Fusion%20of%20Joint%0A%20%20Cross-Attention&entry.906535625=R.%20Gnana%20Praveen%20and%20Jahangir%20Alam&entry.1292438233=%20%20Person%20or%20identity%20verification%20has%20been%20recently%20gaining%20a%20lot%20of%20attention%0Ausing%20audio-visual%20fusion%20as%20faces%20and%20voices%20share%20close%20associations%20with%0Aeach%20other.%20Conventional%20approaches%20based%20on%20audio-visual%20fusion%20rely%20on%0Ascore-level%20or%20early%20feature-level%20fusion%20techniques.%20Though%20existing%0Aapproaches%20showed%20improvement%20over%20unimodal%20systems%2C%20the%20potential%20of%0Aaudio-visual%20fusion%20for%20person%20verification%20is%20not%20fully%20exploited.%20In%20this%0Apaper%2C%20we%20have%20investigated%20the%20prospect%20of%20effectively%20capturing%20both%20the%0Aintra-%20and%20inter-modal%20relationships%20across%20audio%20and%20visual%20modalities%2C%20which%0Acan%20play%20a%20crucial%20role%20in%20significantly%20improving%20the%20fusion%20performance%20over%0Aunimodal%20systems.%20In%20particular%2C%20we%20introduce%20a%20recursive%20fusion%20of%20a%20joint%0Across-attentional%20model%2C%20where%20a%20joint%20audio-visual%20feature%20representation%20is%0Aemployed%20in%20the%20cross-attention%20framework%20in%20a%20recursive%20fashion%20to%0Aprogressively%20refine%20the%20feature%20representations%20that%20can%20efficiently%20capture%0Athe%20intra-and%20inter-modal%20relationships.%20To%20further%20enhance%20the%20audio-visual%0Afeature%20representations%2C%20we%20have%20also%20explored%20BLSTMs%20to%20improve%20the%20temporal%0Amodeling%20of%20audio-visual%20feature%20representations.%20Extensive%20experiments%20are%0Aconducted%20on%20the%20Voxceleb1%20dataset%20to%20evaluate%20the%20proposed%20model.%20Results%0Aindicate%20that%20the%20proposed%20model%20shows%20promising%20improvement%20in%20fusion%0Aperformance%20by%20adeptly%20capturing%20the%20intra-and%20inter-modal%20relationships%20across%0Aaudio%20and%20visual%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04654v3&entry.124074799=Read"},
{"title": "A Semi-Automatic Approach to Create Large Gender- and Age-Balanced\n  Speaker Corpora: Usefulness of Speaker Diarization & Identification", "author": "R\u00e9mi Uro and David Doukhan and Albert Rilliard and La\u00ebtitia Larcher and Anissa-Claire Adgharouamane and Marie Tahon and Antoine Laurent", "abstract": "  This paper presents a semi-automatic approach to create a diachronic corpus\nof voices balanced for speaker's age, gender, and recording period, according\nto 32 categories (2 genders, 4 age ranges and 4 recording periods). Corpora\nwere selected at French National Institute of Audiovisual (INA) to obtain at\nleast 30 speakers per category (a total of 960 speakers; only 874 have be found\nyet). For each speaker, speech excerpts were extracted from audiovisual\ndocuments using an automatic pipeline consisting of speech detection,\nbackground music and overlapped speech removal and speaker diarization, used to\npresent clean speaker segments to human annotators identifying target speakers.\nThis pipeline proved highly effective, cutting down manual processing by a\nfactor of ten. Evaluation of the quality of the automatic processing and of the\nfinal output is provided. It shows the automatic processing compare to\nup-to-date process, and that the output provides high quality speech for most\nof the selected excerpts. This method shows promise for creating large corpora\nof known target speakers.\n", "link": "http://arxiv.org/abs/2404.17552v1", "date": "2024-04-26", "relevancy": 2.0571, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4217}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4104}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4023}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Semi-Automatic%20Approach%20to%20Create%20Large%20Gender-%20and%20Age-Balanced%0A%20%20Speaker%20Corpora%3A%20Usefulness%20of%20Speaker%20Diarization%20%26%20Identification&body=Title%3A%20A%20Semi-Automatic%20Approach%20to%20Create%20Large%20Gender-%20and%20Age-Balanced%0A%20%20Speaker%20Corpora%3A%20Usefulness%20of%20Speaker%20Diarization%20%26%20Identification%0AAuthor%3A%20R%C3%A9mi%20Uro%20and%20David%20Doukhan%20and%20Albert%20Rilliard%20and%20La%C3%ABtitia%20Larcher%20and%20Anissa-Claire%20Adgharouamane%20and%20Marie%20Tahon%20and%20Antoine%20Laurent%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20semi-automatic%20approach%20to%20create%20a%20diachronic%20corpus%0Aof%20voices%20balanced%20for%20speaker%27s%20age%2C%20gender%2C%20and%20recording%20period%2C%20according%0Ato%2032%20categories%20%282%20genders%2C%204%20age%20ranges%20and%204%20recording%20periods%29.%20Corpora%0Awere%20selected%20at%20French%20National%20Institute%20of%20Audiovisual%20%28INA%29%20to%20obtain%20at%0Aleast%2030%20speakers%20per%20category%20%28a%20total%20of%20960%20speakers%3B%20only%20874%20have%20be%20found%0Ayet%29.%20For%20each%20speaker%2C%20speech%20excerpts%20were%20extracted%20from%20audiovisual%0Adocuments%20using%20an%20automatic%20pipeline%20consisting%20of%20speech%20detection%2C%0Abackground%20music%20and%20overlapped%20speech%20removal%20and%20speaker%20diarization%2C%20used%20to%0Apresent%20clean%20speaker%20segments%20to%20human%20annotators%20identifying%20target%20speakers.%0AThis%20pipeline%20proved%20highly%20effective%2C%20cutting%20down%20manual%20processing%20by%20a%0Afactor%20of%20ten.%20Evaluation%20of%20the%20quality%20of%20the%20automatic%20processing%20and%20of%20the%0Afinal%20output%20is%20provided.%20It%20shows%20the%20automatic%20processing%20compare%20to%0Aup-to-date%20process%2C%20and%20that%20the%20output%20provides%20high%20quality%20speech%20for%20most%0Aof%20the%20selected%20excerpts.%20This%20method%20shows%20promise%20for%20creating%20large%20corpora%0Aof%20known%20target%20speakers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17552v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Semi-Automatic%20Approach%20to%20Create%20Large%20Gender-%20and%20Age-Balanced%0A%20%20Speaker%20Corpora%3A%20Usefulness%20of%20Speaker%20Diarization%20%26%20Identification&entry.906535625=R%C3%A9mi%20Uro%20and%20David%20Doukhan%20and%20Albert%20Rilliard%20and%20La%C3%ABtitia%20Larcher%20and%20Anissa-Claire%20Adgharouamane%20and%20Marie%20Tahon%20and%20Antoine%20Laurent&entry.1292438233=%20%20This%20paper%20presents%20a%20semi-automatic%20approach%20to%20create%20a%20diachronic%20corpus%0Aof%20voices%20balanced%20for%20speaker%27s%20age%2C%20gender%2C%20and%20recording%20period%2C%20according%0Ato%2032%20categories%20%282%20genders%2C%204%20age%20ranges%20and%204%20recording%20periods%29.%20Corpora%0Awere%20selected%20at%20French%20National%20Institute%20of%20Audiovisual%20%28INA%29%20to%20obtain%20at%0Aleast%2030%20speakers%20per%20category%20%28a%20total%20of%20960%20speakers%3B%20only%20874%20have%20be%20found%0Ayet%29.%20For%20each%20speaker%2C%20speech%20excerpts%20were%20extracted%20from%20audiovisual%0Adocuments%20using%20an%20automatic%20pipeline%20consisting%20of%20speech%20detection%2C%0Abackground%20music%20and%20overlapped%20speech%20removal%20and%20speaker%20diarization%2C%20used%20to%0Apresent%20clean%20speaker%20segments%20to%20human%20annotators%20identifying%20target%20speakers.%0AThis%20pipeline%20proved%20highly%20effective%2C%20cutting%20down%20manual%20processing%20by%20a%0Afactor%20of%20ten.%20Evaluation%20of%20the%20quality%20of%20the%20automatic%20processing%20and%20of%20the%0Afinal%20output%20is%20provided.%20It%20shows%20the%20automatic%20processing%20compare%20to%0Aup-to-date%20process%2C%20and%20that%20the%20output%20provides%20high%20quality%20speech%20for%20most%0Aof%20the%20selected%20excerpts.%20This%20method%20shows%20promise%20for%20creating%20large%20corpora%0Aof%20known%20target%20speakers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17552v1&entry.124074799=Read"},
{"title": "4D Millimeter-Wave Radar in Autonomous Driving: A Survey", "author": "Zeyu Han and Jiahao Wang and Zikun Xu and Shuocheng Yang and Lei He and Shaobing Xu and Jianqiang Wang and Keqiang Li", "abstract": "  The 4D millimeter-wave (mmWave) radar, proficient in measuring the range,\nazimuth, elevation, and velocity of targets, has attracted considerable\ninterest within the autonomous driving community. This is attributed to its\nrobustness in extreme environments and the velocity and elevation measurement\ncapabilities. However, despite the rapid advancement in research related to its\nsensing theory and application, there is a conspicuous absence of comprehensive\nsurveys on the subject of 4D mmWave radar. In an effort to bridge this gap and\nstimulate future research, this paper presents an exhaustive survey on the\nutilization of 4D mmWave radar in autonomous driving. Initially, the paper\nprovides reviews on the theoretical background and progress of 4D mmWave\nradars, encompassing aspects such as the signal processing workflow, resolution\nimprovement approaches, and extrinsic calibration process. Learning-based radar\ndata quality improvement methods are present following. Then, this paper\nintroduces relevant datasets and application algorithms in autonomous driving\nperception, localization and mapping tasks. Finally, this paper concludes by\nforecasting future trends in the realm of 4D mmWave radar in autonomous\ndriving. To the best of our knowledge, this is the first survey specifically\ndedicated to the 4D mmWave radar in autonomous driving.\n", "link": "http://arxiv.org/abs/2306.04242v4", "date": "2024-04-26", "relevancy": 2.0418, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5408}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4923}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4874}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%204D%20Millimeter-Wave%20Radar%20in%20Autonomous%20Driving%3A%20A%20Survey&body=Title%3A%204D%20Millimeter-Wave%20Radar%20in%20Autonomous%20Driving%3A%20A%20Survey%0AAuthor%3A%20Zeyu%20Han%20and%20Jiahao%20Wang%20and%20Zikun%20Xu%20and%20Shuocheng%20Yang%20and%20Lei%20He%20and%20Shaobing%20Xu%20and%20Jianqiang%20Wang%20and%20Keqiang%20Li%0AAbstract%3A%20%20%20The%204D%20millimeter-wave%20%28mmWave%29%20radar%2C%20proficient%20in%20measuring%20the%20range%2C%0Aazimuth%2C%20elevation%2C%20and%20velocity%20of%20targets%2C%20has%20attracted%20considerable%0Ainterest%20within%20the%20autonomous%20driving%20community.%20This%20is%20attributed%20to%20its%0Arobustness%20in%20extreme%20environments%20and%20the%20velocity%20and%20elevation%20measurement%0Acapabilities.%20However%2C%20despite%20the%20rapid%20advancement%20in%20research%20related%20to%20its%0Asensing%20theory%20and%20application%2C%20there%20is%20a%20conspicuous%20absence%20of%20comprehensive%0Asurveys%20on%20the%20subject%20of%204D%20mmWave%20radar.%20In%20an%20effort%20to%20bridge%20this%20gap%20and%0Astimulate%20future%20research%2C%20this%20paper%20presents%20an%20exhaustive%20survey%20on%20the%0Autilization%20of%204D%20mmWave%20radar%20in%20autonomous%20driving.%20Initially%2C%20the%20paper%0Aprovides%20reviews%20on%20the%20theoretical%20background%20and%20progress%20of%204D%20mmWave%0Aradars%2C%20encompassing%20aspects%20such%20as%20the%20signal%20processing%20workflow%2C%20resolution%0Aimprovement%20approaches%2C%20and%20extrinsic%20calibration%20process.%20Learning-based%20radar%0Adata%20quality%20improvement%20methods%20are%20present%20following.%20Then%2C%20this%20paper%0Aintroduces%20relevant%20datasets%20and%20application%20algorithms%20in%20autonomous%20driving%0Aperception%2C%20localization%20and%20mapping%20tasks.%20Finally%2C%20this%20paper%20concludes%20by%0Aforecasting%20future%20trends%20in%20the%20realm%20of%204D%20mmWave%20radar%20in%20autonomous%0Adriving.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20survey%20specifically%0Adedicated%20to%20the%204D%20mmWave%20radar%20in%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.04242v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4D%20Millimeter-Wave%20Radar%20in%20Autonomous%20Driving%3A%20A%20Survey&entry.906535625=Zeyu%20Han%20and%20Jiahao%20Wang%20and%20Zikun%20Xu%20and%20Shuocheng%20Yang%20and%20Lei%20He%20and%20Shaobing%20Xu%20and%20Jianqiang%20Wang%20and%20Keqiang%20Li&entry.1292438233=%20%20The%204D%20millimeter-wave%20%28mmWave%29%20radar%2C%20proficient%20in%20measuring%20the%20range%2C%0Aazimuth%2C%20elevation%2C%20and%20velocity%20of%20targets%2C%20has%20attracted%20considerable%0Ainterest%20within%20the%20autonomous%20driving%20community.%20This%20is%20attributed%20to%20its%0Arobustness%20in%20extreme%20environments%20and%20the%20velocity%20and%20elevation%20measurement%0Acapabilities.%20However%2C%20despite%20the%20rapid%20advancement%20in%20research%20related%20to%20its%0Asensing%20theory%20and%20application%2C%20there%20is%20a%20conspicuous%20absence%20of%20comprehensive%0Asurveys%20on%20the%20subject%20of%204D%20mmWave%20radar.%20In%20an%20effort%20to%20bridge%20this%20gap%20and%0Astimulate%20future%20research%2C%20this%20paper%20presents%20an%20exhaustive%20survey%20on%20the%0Autilization%20of%204D%20mmWave%20radar%20in%20autonomous%20driving.%20Initially%2C%20the%20paper%0Aprovides%20reviews%20on%20the%20theoretical%20background%20and%20progress%20of%204D%20mmWave%0Aradars%2C%20encompassing%20aspects%20such%20as%20the%20signal%20processing%20workflow%2C%20resolution%0Aimprovement%20approaches%2C%20and%20extrinsic%20calibration%20process.%20Learning-based%20radar%0Adata%20quality%20improvement%20methods%20are%20present%20following.%20Then%2C%20this%20paper%0Aintroduces%20relevant%20datasets%20and%20application%20algorithms%20in%20autonomous%20driving%0Aperception%2C%20localization%20and%20mapping%20tasks.%20Finally%2C%20this%20paper%20concludes%20by%0Aforecasting%20future%20trends%20in%20the%20realm%20of%204D%20mmWave%20radar%20in%20autonomous%0Adriving.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20survey%20specifically%0Adedicated%20to%20the%204D%20mmWave%20radar%20in%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.04242v4&entry.124074799=Read"},
{"title": "Q-Learning to navigate turbulence without a map", "author": "Marco Rando and Martin James and Alessandro Verri and Lorenzo Rosasco and Agnese Seminara", "abstract": "  We consider the problem of olfactory searches in a turbulent environment. We\nfocus on agents that respond solely to odor stimuli, with no access to spatial\nperception nor prior information about the odor location. We ask whether\nnavigation strategies to a target can be learned robustly within a sequential\ndecision making framework. We develop a reinforcement learning algorithm using\na small set of interpretable olfactory states and train it with realistic\nturbulent odor cues. By introducing a temporal memory, we demonstrate that two\nsalient features of odor traces, discretized in few olfactory states, are\nsufficient to learn navigation in a realistic odor plume. Performance is\ndictated by the sparse nature of turbulent plumes. An optimal memory exists\nwhich ignores blanks within the plume and activates a recovery strategy outside\nthe plume. We obtain the best performance by letting agents learn their\nrecovery strategy and show that it is mostly casting cross wind, similar to\nbehavior observed in flying insects. The optimal strategy is robust to\nsubstantial changes in the odor plumes, suggesting minor parameter tuning may\nbe sufficient to adapt to different environments.\n", "link": "http://arxiv.org/abs/2404.17495v1", "date": "2024-04-26", "relevancy": 2.0405, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5323}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.503}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4908}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Q-Learning%20to%20navigate%20turbulence%20without%20a%20map&body=Title%3A%20Q-Learning%20to%20navigate%20turbulence%20without%20a%20map%0AAuthor%3A%20Marco%20Rando%20and%20Martin%20James%20and%20Alessandro%20Verri%20and%20Lorenzo%20Rosasco%20and%20Agnese%20Seminara%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20olfactory%20searches%20in%20a%20turbulent%20environment.%20We%0Afocus%20on%20agents%20that%20respond%20solely%20to%20odor%20stimuli%2C%20with%20no%20access%20to%20spatial%0Aperception%20nor%20prior%20information%20about%20the%20odor%20location.%20We%20ask%20whether%0Anavigation%20strategies%20to%20a%20target%20can%20be%20learned%20robustly%20within%20a%20sequential%0Adecision%20making%20framework.%20We%20develop%20a%20reinforcement%20learning%20algorithm%20using%0Aa%20small%20set%20of%20interpretable%20olfactory%20states%20and%20train%20it%20with%20realistic%0Aturbulent%20odor%20cues.%20By%20introducing%20a%20temporal%20memory%2C%20we%20demonstrate%20that%20two%0Asalient%20features%20of%20odor%20traces%2C%20discretized%20in%20few%20olfactory%20states%2C%20are%0Asufficient%20to%20learn%20navigation%20in%20a%20realistic%20odor%20plume.%20Performance%20is%0Adictated%20by%20the%20sparse%20nature%20of%20turbulent%20plumes.%20An%20optimal%20memory%20exists%0Awhich%20ignores%20blanks%20within%20the%20plume%20and%20activates%20a%20recovery%20strategy%20outside%0Athe%20plume.%20We%20obtain%20the%20best%20performance%20by%20letting%20agents%20learn%20their%0Arecovery%20strategy%20and%20show%20that%20it%20is%20mostly%20casting%20cross%20wind%2C%20similar%20to%0Abehavior%20observed%20in%20flying%20insects.%20The%20optimal%20strategy%20is%20robust%20to%0Asubstantial%20changes%20in%20the%20odor%20plumes%2C%20suggesting%20minor%20parameter%20tuning%20may%0Abe%20sufficient%20to%20adapt%20to%20different%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17495v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q-Learning%20to%20navigate%20turbulence%20without%20a%20map&entry.906535625=Marco%20Rando%20and%20Martin%20James%20and%20Alessandro%20Verri%20and%20Lorenzo%20Rosasco%20and%20Agnese%20Seminara&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20olfactory%20searches%20in%20a%20turbulent%20environment.%20We%0Afocus%20on%20agents%20that%20respond%20solely%20to%20odor%20stimuli%2C%20with%20no%20access%20to%20spatial%0Aperception%20nor%20prior%20information%20about%20the%20odor%20location.%20We%20ask%20whether%0Anavigation%20strategies%20to%20a%20target%20can%20be%20learned%20robustly%20within%20a%20sequential%0Adecision%20making%20framework.%20We%20develop%20a%20reinforcement%20learning%20algorithm%20using%0Aa%20small%20set%20of%20interpretable%20olfactory%20states%20and%20train%20it%20with%20realistic%0Aturbulent%20odor%20cues.%20By%20introducing%20a%20temporal%20memory%2C%20we%20demonstrate%20that%20two%0Asalient%20features%20of%20odor%20traces%2C%20discretized%20in%20few%20olfactory%20states%2C%20are%0Asufficient%20to%20learn%20navigation%20in%20a%20realistic%20odor%20plume.%20Performance%20is%0Adictated%20by%20the%20sparse%20nature%20of%20turbulent%20plumes.%20An%20optimal%20memory%20exists%0Awhich%20ignores%20blanks%20within%20the%20plume%20and%20activates%20a%20recovery%20strategy%20outside%0Athe%20plume.%20We%20obtain%20the%20best%20performance%20by%20letting%20agents%20learn%20their%0Arecovery%20strategy%20and%20show%20that%20it%20is%20mostly%20casting%20cross%20wind%2C%20similar%20to%0Abehavior%20observed%20in%20flying%20insects.%20The%20optimal%20strategy%20is%20robust%20to%0Asubstantial%20changes%20in%20the%20odor%20plumes%2C%20suggesting%20minor%20parameter%20tuning%20may%0Abe%20sufficient%20to%20adapt%20to%20different%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17495v1&entry.124074799=Read"},
{"title": "Open World Learning Graph Convolution for Latency Estimation in Routing\n  Networks", "author": "Yifei Jin and Marios Daoutis and Sarunas Girdzijauskas and Aristides Gionis", "abstract": "  Accurate routing network status estimation is a key component in Software\nDefined Networking. However, existing deep-learning-based methods for modeling\nnetwork routing are not able to extrapolate towards unseen feature\ndistributions. Nor are they able to handle scaled and drifted network\nattributes in test sets that include open-world inputs. To deal with these\nchallenges, we propose a novel approach for modeling network routing, using\nGraph Neural Networks. Our method can also be used for network-latency\nestimation. Supported by a domain-knowledge-assisted graph formulation, our\nmodel shares a stable performance across different network sizes and\nconfigurations of routing networks, while at the same time being able to\nextrapolate towards unseen sizes, configurations, and user behavior. We show\nthat our model outperforms most conventional deep-learning-based models, in\nterms of prediction accuracy, computational resources, inference speed, as well\nas ability to generalize towards open-world input.\n", "link": "http://arxiv.org/abs/2207.14643v2", "date": "2024-04-26", "relevancy": 2.0344, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.523}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5017}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.497}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Open%20World%20Learning%20Graph%20Convolution%20for%20Latency%20Estimation%20in%20Routing%0A%20%20Networks&body=Title%3A%20Open%20World%20Learning%20Graph%20Convolution%20for%20Latency%20Estimation%20in%20Routing%0A%20%20Networks%0AAuthor%3A%20Yifei%20Jin%20and%20Marios%20Daoutis%20and%20Sarunas%20Girdzijauskas%20and%20Aristides%20Gionis%0AAbstract%3A%20%20%20Accurate%20routing%20network%20status%20estimation%20is%20a%20key%20component%20in%20Software%0ADefined%20Networking.%20However%2C%20existing%20deep-learning-based%20methods%20for%20modeling%0Anetwork%20routing%20are%20not%20able%20to%20extrapolate%20towards%20unseen%20feature%0Adistributions.%20Nor%20are%20they%20able%20to%20handle%20scaled%20and%20drifted%20network%0Aattributes%20in%20test%20sets%20that%20include%20open-world%20inputs.%20To%20deal%20with%20these%0Achallenges%2C%20we%20propose%20a%20novel%20approach%20for%20modeling%20network%20routing%2C%20using%0AGraph%20Neural%20Networks.%20Our%20method%20can%20also%20be%20used%20for%20network-latency%0Aestimation.%20Supported%20by%20a%20domain-knowledge-assisted%20graph%20formulation%2C%20our%0Amodel%20shares%20a%20stable%20performance%20across%20different%20network%20sizes%20and%0Aconfigurations%20of%20routing%20networks%2C%20while%20at%20the%20same%20time%20being%20able%20to%0Aextrapolate%20towards%20unseen%20sizes%2C%20configurations%2C%20and%20user%20behavior.%20We%20show%0Athat%20our%20model%20outperforms%20most%20conventional%20deep-learning-based%20models%2C%20in%0Aterms%20of%20prediction%20accuracy%2C%20computational%20resources%2C%20inference%20speed%2C%20as%20well%0Aas%20ability%20to%20generalize%20towards%20open-world%20input.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.14643v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open%20World%20Learning%20Graph%20Convolution%20for%20Latency%20Estimation%20in%20Routing%0A%20%20Networks&entry.906535625=Yifei%20Jin%20and%20Marios%20Daoutis%20and%20Sarunas%20Girdzijauskas%20and%20Aristides%20Gionis&entry.1292438233=%20%20Accurate%20routing%20network%20status%20estimation%20is%20a%20key%20component%20in%20Software%0ADefined%20Networking.%20However%2C%20existing%20deep-learning-based%20methods%20for%20modeling%0Anetwork%20routing%20are%20not%20able%20to%20extrapolate%20towards%20unseen%20feature%0Adistributions.%20Nor%20are%20they%20able%20to%20handle%20scaled%20and%20drifted%20network%0Aattributes%20in%20test%20sets%20that%20include%20open-world%20inputs.%20To%20deal%20with%20these%0Achallenges%2C%20we%20propose%20a%20novel%20approach%20for%20modeling%20network%20routing%2C%20using%0AGraph%20Neural%20Networks.%20Our%20method%20can%20also%20be%20used%20for%20network-latency%0Aestimation.%20Supported%20by%20a%20domain-knowledge-assisted%20graph%20formulation%2C%20our%0Amodel%20shares%20a%20stable%20performance%20across%20different%20network%20sizes%20and%0Aconfigurations%20of%20routing%20networks%2C%20while%20at%20the%20same%20time%20being%20able%20to%0Aextrapolate%20towards%20unseen%20sizes%2C%20configurations%2C%20and%20user%20behavior.%20We%20show%0Athat%20our%20model%20outperforms%20most%20conventional%20deep-learning-based%20models%2C%20in%0Aterms%20of%20prediction%20accuracy%2C%20computational%20resources%2C%20inference%20speed%2C%20as%20well%0Aas%20ability%20to%20generalize%20towards%20open-world%20input.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.14643v2&entry.124074799=Read"},
{"title": "Adaptive speed planning for Unmanned Vehicle Based on Deep Reinforcement\n  Learning", "author": "Hao Liu and Yi Shen and Wenjing Zhou and Yuelin Zou and Chang Zhou and Shuyao He", "abstract": "  In order to solve the problem of frequent deceleration of unmanned vehicles\nwhen approaching obstacles, this article uses a Deep Q-Network (DQN) and its\nextension, the Double Deep Q-Network (DDQN), to develop a local navigation\nsystem that adapts to obstacles while maintaining optimal speed planning. By\nintegrating improved reward functions and obstacle angle determination methods,\nthe system demonstrates significant enhancements in maneuvering capabilities\nwithout frequent decelerations. Experiments conducted in simulated environments\nwith varying obstacle densities confirm the effectiveness of the proposed\nmethod in achieving more stable and efficient path planning.\n", "link": "http://arxiv.org/abs/2404.17379v1", "date": "2024-04-26", "relevancy": 2.0248, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5127}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.504}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4957}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adaptive%20speed%20planning%20for%20Unmanned%20Vehicle%20Based%20on%20Deep%20Reinforcement%0A%20%20Learning&body=Title%3A%20Adaptive%20speed%20planning%20for%20Unmanned%20Vehicle%20Based%20on%20Deep%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Hao%20Liu%20and%20Yi%20Shen%20and%20Wenjing%20Zhou%20and%20Yuelin%20Zou%20and%20Chang%20Zhou%20and%20Shuyao%20He%0AAbstract%3A%20%20%20In%20order%20to%20solve%20the%20problem%20of%20frequent%20deceleration%20of%20unmanned%20vehicles%0Awhen%20approaching%20obstacles%2C%20this%20article%20uses%20a%20Deep%20Q-Network%20%28DQN%29%20and%20its%0Aextension%2C%20the%20Double%20Deep%20Q-Network%20%28DDQN%29%2C%20to%20develop%20a%20local%20navigation%0Asystem%20that%20adapts%20to%20obstacles%20while%20maintaining%20optimal%20speed%20planning.%20By%0Aintegrating%20improved%20reward%20functions%20and%20obstacle%20angle%20determination%20methods%2C%0Athe%20system%20demonstrates%20significant%20enhancements%20in%20maneuvering%20capabilities%0Awithout%20frequent%20decelerations.%20Experiments%20conducted%20in%20simulated%20environments%0Awith%20varying%20obstacle%20densities%20confirm%20the%20effectiveness%20of%20the%20proposed%0Amethod%20in%20achieving%20more%20stable%20and%20efficient%20path%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17379v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20speed%20planning%20for%20Unmanned%20Vehicle%20Based%20on%20Deep%20Reinforcement%0A%20%20Learning&entry.906535625=Hao%20Liu%20and%20Yi%20Shen%20and%20Wenjing%20Zhou%20and%20Yuelin%20Zou%20and%20Chang%20Zhou%20and%20Shuyao%20He&entry.1292438233=%20%20In%20order%20to%20solve%20the%20problem%20of%20frequent%20deceleration%20of%20unmanned%20vehicles%0Awhen%20approaching%20obstacles%2C%20this%20article%20uses%20a%20Deep%20Q-Network%20%28DQN%29%20and%20its%0Aextension%2C%20the%20Double%20Deep%20Q-Network%20%28DDQN%29%2C%20to%20develop%20a%20local%20navigation%0Asystem%20that%20adapts%20to%20obstacles%20while%20maintaining%20optimal%20speed%20planning.%20By%0Aintegrating%20improved%20reward%20functions%20and%20obstacle%20angle%20determination%20methods%2C%0Athe%20system%20demonstrates%20significant%20enhancements%20in%20maneuvering%20capabilities%0Awithout%20frequent%20decelerations.%20Experiments%20conducted%20in%20simulated%20environments%0Awith%20varying%20obstacle%20densities%20confirm%20the%20effectiveness%20of%20the%20proposed%0Amethod%20in%20achieving%20more%20stable%20and%20efficient%20path%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17379v1&entry.124074799=Read"},
{"title": "Kinodynamic Motion Planning via Funnel Control for Underactuated\n  Unmanned Surface Vehicles", "author": "D\u017eenan Lapandi\u0107 and Christos K. Verginis and Dimos V. Dimarogonas and Bo Wahlberg", "abstract": "  We develop an algorithm to control an underactuated unmanned surface vehicle\n(USV) using kinodynamic motion planning with funnel control (KDF). KDF has two\nkey components: motion planning used to generate trajectories with respect to\nkinodynamic constraints, and funnel control, also referred to as prescribed\nperformance control, which enables trajectory tracking in the presence of\nuncertain dynamics and disturbances. We extend prescribed performance control\nto address the challenges posed by underactuation and control-input saturation\npresent on the USV. The proposed scheme guarantees stability under user-defined\nprescribed performance functions where model parameters and exogenous\ndisturbances are unknown. Furthermore, we present an optimization problem to\nobtain smooth, collision-free trajectories while respecting kinodynamic\nconstraints. We deploy the algorithm on a USV and verify its efficiency in\nreal-world open-water experiments.\n", "link": "http://arxiv.org/abs/2308.00130v2", "date": "2024-04-26", "relevancy": 2.0211, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5082}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5062}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5032}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Kinodynamic%20Motion%20Planning%20via%20Funnel%20Control%20for%20Underactuated%0A%20%20Unmanned%20Surface%20Vehicles&body=Title%3A%20Kinodynamic%20Motion%20Planning%20via%20Funnel%20Control%20for%20Underactuated%0A%20%20Unmanned%20Surface%20Vehicles%0AAuthor%3A%20D%C5%BEenan%20Lapandi%C4%87%20and%20Christos%20K.%20Verginis%20and%20Dimos%20V.%20Dimarogonas%20and%20Bo%20Wahlberg%0AAbstract%3A%20%20%20We%20develop%20an%20algorithm%20to%20control%20an%20underactuated%20unmanned%20surface%20vehicle%0A%28USV%29%20using%20kinodynamic%20motion%20planning%20with%20funnel%20control%20%28KDF%29.%20KDF%20has%20two%0Akey%20components%3A%20motion%20planning%20used%20to%20generate%20trajectories%20with%20respect%20to%0Akinodynamic%20constraints%2C%20and%20funnel%20control%2C%20also%20referred%20to%20as%20prescribed%0Aperformance%20control%2C%20which%20enables%20trajectory%20tracking%20in%20the%20presence%20of%0Auncertain%20dynamics%20and%20disturbances.%20We%20extend%20prescribed%20performance%20control%0Ato%20address%20the%20challenges%20posed%20by%20underactuation%20and%20control-input%20saturation%0Apresent%20on%20the%20USV.%20The%20proposed%20scheme%20guarantees%20stability%20under%20user-defined%0Aprescribed%20performance%20functions%20where%20model%20parameters%20and%20exogenous%0Adisturbances%20are%20unknown.%20Furthermore%2C%20we%20present%20an%20optimization%20problem%20to%0Aobtain%20smooth%2C%20collision-free%20trajectories%20while%20respecting%20kinodynamic%0Aconstraints.%20We%20deploy%20the%20algorithm%20on%20a%20USV%20and%20verify%20its%20efficiency%20in%0Areal-world%20open-water%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.00130v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kinodynamic%20Motion%20Planning%20via%20Funnel%20Control%20for%20Underactuated%0A%20%20Unmanned%20Surface%20Vehicles&entry.906535625=D%C5%BEenan%20Lapandi%C4%87%20and%20Christos%20K.%20Verginis%20and%20Dimos%20V.%20Dimarogonas%20and%20Bo%20Wahlberg&entry.1292438233=%20%20We%20develop%20an%20algorithm%20to%20control%20an%20underactuated%20unmanned%20surface%20vehicle%0A%28USV%29%20using%20kinodynamic%20motion%20planning%20with%20funnel%20control%20%28KDF%29.%20KDF%20has%20two%0Akey%20components%3A%20motion%20planning%20used%20to%20generate%20trajectories%20with%20respect%20to%0Akinodynamic%20constraints%2C%20and%20funnel%20control%2C%20also%20referred%20to%20as%20prescribed%0Aperformance%20control%2C%20which%20enables%20trajectory%20tracking%20in%20the%20presence%20of%0Auncertain%20dynamics%20and%20disturbances.%20We%20extend%20prescribed%20performance%20control%0Ato%20address%20the%20challenges%20posed%20by%20underactuation%20and%20control-input%20saturation%0Apresent%20on%20the%20USV.%20The%20proposed%20scheme%20guarantees%20stability%20under%20user-defined%0Aprescribed%20performance%20functions%20where%20model%20parameters%20and%20exogenous%0Adisturbances%20are%20unknown.%20Furthermore%2C%20we%20present%20an%20optimization%20problem%20to%0Aobtain%20smooth%2C%20collision-free%20trajectories%20while%20respecting%20kinodynamic%0Aconstraints.%20We%20deploy%20the%20algorithm%20on%20a%20USV%20and%20verify%20its%20efficiency%20in%0Areal-world%20open-water%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.00130v2&entry.124074799=Read"},
{"title": "Conformal Prediction with Learned Features", "author": "Shayan Kiyani and George Pappas and Hamed Hassani", "abstract": "  In this paper, we focus on the problem of conformal prediction with\nconditional guarantees. Prior work has shown that it is impossible to construct\nnontrivial prediction sets with full conditional coverage guarantees. A wealth\nof research has considered relaxations of full conditional guarantees, relying\non some predefined uncertainty structures. Departing from this line of\nthinking, we propose Partition Learning Conformal Prediction (PLCP), a\nframework to improve conditional validity of prediction sets through learning\nuncertainty-guided features from the calibration data. We implement PLCP\nefficiently with alternating gradient descent, utilizing off-the-shelf machine\nlearning models. We further analyze PLCP theoretically and provide conditional\nguarantees for infinite and finite sample sizes. Finally, our experimental\nresults over four real-world and synthetic datasets show the superior\nperformance of PLCP compared to state-of-the-art methods in terms of coverage\nand length in both classification and regression scenarios.\n", "link": "http://arxiv.org/abs/2404.17487v1", "date": "2024-04-26", "relevancy": 1.9989, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5036}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4999}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4958}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Conformal%20Prediction%20with%20Learned%20Features&body=Title%3A%20Conformal%20Prediction%20with%20Learned%20Features%0AAuthor%3A%20Shayan%20Kiyani%20and%20George%20Pappas%20and%20Hamed%20Hassani%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20focus%20on%20the%20problem%20of%20conformal%20prediction%20with%0Aconditional%20guarantees.%20Prior%20work%20has%20shown%20that%20it%20is%20impossible%20to%20construct%0Anontrivial%20prediction%20sets%20with%20full%20conditional%20coverage%20guarantees.%20A%20wealth%0Aof%20research%20has%20considered%20relaxations%20of%20full%20conditional%20guarantees%2C%20relying%0Aon%20some%20predefined%20uncertainty%20structures.%20Departing%20from%20this%20line%20of%0Athinking%2C%20we%20propose%20Partition%20Learning%20Conformal%20Prediction%20%28PLCP%29%2C%20a%0Aframework%20to%20improve%20conditional%20validity%20of%20prediction%20sets%20through%20learning%0Auncertainty-guided%20features%20from%20the%20calibration%20data.%20We%20implement%20PLCP%0Aefficiently%20with%20alternating%20gradient%20descent%2C%20utilizing%20off-the-shelf%20machine%0Alearning%20models.%20We%20further%20analyze%20PLCP%20theoretically%20and%20provide%20conditional%0Aguarantees%20for%20infinite%20and%20finite%20sample%20sizes.%20Finally%2C%20our%20experimental%0Aresults%20over%20four%20real-world%20and%20synthetic%20datasets%20show%20the%20superior%0Aperformance%20of%20PLCP%20compared%20to%20state-of-the-art%20methods%20in%20terms%20of%20coverage%0Aand%20length%20in%20both%20classification%20and%20regression%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17487v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Prediction%20with%20Learned%20Features&entry.906535625=Shayan%20Kiyani%20and%20George%20Pappas%20and%20Hamed%20Hassani&entry.1292438233=%20%20In%20this%20paper%2C%20we%20focus%20on%20the%20problem%20of%20conformal%20prediction%20with%0Aconditional%20guarantees.%20Prior%20work%20has%20shown%20that%20it%20is%20impossible%20to%20construct%0Anontrivial%20prediction%20sets%20with%20full%20conditional%20coverage%20guarantees.%20A%20wealth%0Aof%20research%20has%20considered%20relaxations%20of%20full%20conditional%20guarantees%2C%20relying%0Aon%20some%20predefined%20uncertainty%20structures.%20Departing%20from%20this%20line%20of%0Athinking%2C%20we%20propose%20Partition%20Learning%20Conformal%20Prediction%20%28PLCP%29%2C%20a%0Aframework%20to%20improve%20conditional%20validity%20of%20prediction%20sets%20through%20learning%0Auncertainty-guided%20features%20from%20the%20calibration%20data.%20We%20implement%20PLCP%0Aefficiently%20with%20alternating%20gradient%20descent%2C%20utilizing%20off-the-shelf%20machine%0Alearning%20models.%20We%20further%20analyze%20PLCP%20theoretically%20and%20provide%20conditional%0Aguarantees%20for%20infinite%20and%20finite%20sample%20sizes.%20Finally%2C%20our%20experimental%0Aresults%20over%20four%20real-world%20and%20synthetic%20datasets%20show%20the%20superior%0Aperformance%20of%20PLCP%20compared%20to%20state-of-the-art%20methods%20in%20terms%20of%20coverage%0Aand%20length%20in%20both%20classification%20and%20regression%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17487v1&entry.124074799=Read"},
{"title": "PromptCIR: Blind Compressed Image Restoration with Prompt Learning", "author": "Bingchen Li and Xin Li and Yiting Lu and Ruoyu Feng and Mengxi Guo and Shijie Zhao and Li Zhang and Zhibo Chen", "abstract": "  Blind Compressed Image Restoration (CIR) has garnered significant attention\ndue to its practical applications. It aims to mitigate compression artifacts\ncaused by unknown quality factors, particularly with JPEG codecs. Existing\nworks on blind CIR often seek assistance from a quality factor prediction\nnetwork to facilitate their network to restore compressed images. However, the\npredicted numerical quality factor lacks spatial information, preventing\nnetwork adaptability toward image contents. Recent studies in\nprompt-learning-based image restoration have showcased the potential of prompts\nto generalize across varied degradation types and degrees. This motivated us to\ndesign a prompt-learning-based compressed image restoration network, dubbed\nPromptCIR, which can effectively restore images from various compress levels.\nSpecifically, PromptCIR exploits prompts to encode compression information\nimplicitly, where prompts directly interact with soft weights generated from\nimage features, thus providing dynamic content-aware and distortion-aware\nguidance for the restoration process. The light-weight prompts enable our\nmethod to adapt to different compression levels, while introducing minimal\nparameter overhead. Overall, PromptCIR leverages the powerful transformer-based\nbackbone with the dynamic prompt module to proficiently handle blind CIR tasks,\nwinning first place in the NTIRE 2024 challenge of blind compressed image\nenhancement track. Extensive experiments have validated the effectiveness of\nour proposed PromptCIR. The code is available at\nhttps://github.com/lbc12345/PromptCIR-NTIRE24.\n", "link": "http://arxiv.org/abs/2404.17433v1", "date": "2024-04-26", "relevancy": 1.9967, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5135}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4912}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4835}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PromptCIR%3A%20Blind%20Compressed%20Image%20Restoration%20with%20Prompt%20Learning&body=Title%3A%20PromptCIR%3A%20Blind%20Compressed%20Image%20Restoration%20with%20Prompt%20Learning%0AAuthor%3A%20Bingchen%20Li%20and%20Xin%20Li%20and%20Yiting%20Lu%20and%20Ruoyu%20Feng%20and%20Mengxi%20Guo%20and%20Shijie%20Zhao%20and%20Li%20Zhang%20and%20Zhibo%20Chen%0AAbstract%3A%20%20%20Blind%20Compressed%20Image%20Restoration%20%28CIR%29%20has%20garnered%20significant%20attention%0Adue%20to%20its%20practical%20applications.%20It%20aims%20to%20mitigate%20compression%20artifacts%0Acaused%20by%20unknown%20quality%20factors%2C%20particularly%20with%20JPEG%20codecs.%20Existing%0Aworks%20on%20blind%20CIR%20often%20seek%20assistance%20from%20a%20quality%20factor%20prediction%0Anetwork%20to%20facilitate%20their%20network%20to%20restore%20compressed%20images.%20However%2C%20the%0Apredicted%20numerical%20quality%20factor%20lacks%20spatial%20information%2C%20preventing%0Anetwork%20adaptability%20toward%20image%20contents.%20Recent%20studies%20in%0Aprompt-learning-based%20image%20restoration%20have%20showcased%20the%20potential%20of%20prompts%0Ato%20generalize%20across%20varied%20degradation%20types%20and%20degrees.%20This%20motivated%20us%20to%0Adesign%20a%20prompt-learning-based%20compressed%20image%20restoration%20network%2C%20dubbed%0APromptCIR%2C%20which%20can%20effectively%20restore%20images%20from%20various%20compress%20levels.%0ASpecifically%2C%20PromptCIR%20exploits%20prompts%20to%20encode%20compression%20information%0Aimplicitly%2C%20where%20prompts%20directly%20interact%20with%20soft%20weights%20generated%20from%0Aimage%20features%2C%20thus%20providing%20dynamic%20content-aware%20and%20distortion-aware%0Aguidance%20for%20the%20restoration%20process.%20The%20light-weight%20prompts%20enable%20our%0Amethod%20to%20adapt%20to%20different%20compression%20levels%2C%20while%20introducing%20minimal%0Aparameter%20overhead.%20Overall%2C%20PromptCIR%20leverages%20the%20powerful%20transformer-based%0Abackbone%20with%20the%20dynamic%20prompt%20module%20to%20proficiently%20handle%20blind%20CIR%20tasks%2C%0Awinning%20first%20place%20in%20the%20NTIRE%202024%20challenge%20of%20blind%20compressed%20image%0Aenhancement%20track.%20Extensive%20experiments%20have%20validated%20the%20effectiveness%20of%0Aour%20proposed%20PromptCIR.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/lbc12345/PromptCIR-NTIRE24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17433v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PromptCIR%3A%20Blind%20Compressed%20Image%20Restoration%20with%20Prompt%20Learning&entry.906535625=Bingchen%20Li%20and%20Xin%20Li%20and%20Yiting%20Lu%20and%20Ruoyu%20Feng%20and%20Mengxi%20Guo%20and%20Shijie%20Zhao%20and%20Li%20Zhang%20and%20Zhibo%20Chen&entry.1292438233=%20%20Blind%20Compressed%20Image%20Restoration%20%28CIR%29%20has%20garnered%20significant%20attention%0Adue%20to%20its%20practical%20applications.%20It%20aims%20to%20mitigate%20compression%20artifacts%0Acaused%20by%20unknown%20quality%20factors%2C%20particularly%20with%20JPEG%20codecs.%20Existing%0Aworks%20on%20blind%20CIR%20often%20seek%20assistance%20from%20a%20quality%20factor%20prediction%0Anetwork%20to%20facilitate%20their%20network%20to%20restore%20compressed%20images.%20However%2C%20the%0Apredicted%20numerical%20quality%20factor%20lacks%20spatial%20information%2C%20preventing%0Anetwork%20adaptability%20toward%20image%20contents.%20Recent%20studies%20in%0Aprompt-learning-based%20image%20restoration%20have%20showcased%20the%20potential%20of%20prompts%0Ato%20generalize%20across%20varied%20degradation%20types%20and%20degrees.%20This%20motivated%20us%20to%0Adesign%20a%20prompt-learning-based%20compressed%20image%20restoration%20network%2C%20dubbed%0APromptCIR%2C%20which%20can%20effectively%20restore%20images%20from%20various%20compress%20levels.%0ASpecifically%2C%20PromptCIR%20exploits%20prompts%20to%20encode%20compression%20information%0Aimplicitly%2C%20where%20prompts%20directly%20interact%20with%20soft%20weights%20generated%20from%0Aimage%20features%2C%20thus%20providing%20dynamic%20content-aware%20and%20distortion-aware%0Aguidance%20for%20the%20restoration%20process.%20The%20light-weight%20prompts%20enable%20our%0Amethod%20to%20adapt%20to%20different%20compression%20levels%2C%20while%20introducing%20minimal%0Aparameter%20overhead.%20Overall%2C%20PromptCIR%20leverages%20the%20powerful%20transformer-based%0Abackbone%20with%20the%20dynamic%20prompt%20module%20to%20proficiently%20handle%20blind%20CIR%20tasks%2C%0Awinning%20first%20place%20in%20the%20NTIRE%202024%20challenge%20of%20blind%20compressed%20image%0Aenhancement%20track.%20Extensive%20experiments%20have%20validated%20the%20effectiveness%20of%0Aour%20proposed%20PromptCIR.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/lbc12345/PromptCIR-NTIRE24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17433v1&entry.124074799=Read"},
{"title": "Frequency-Guided Multi-Level Human Action Anomaly Detection with\n  Normalizing Flows", "author": "Shun Maeda and Chunzhi Gu and Jun Yu and Shogo Tokai and Shangce Gao and Chao Zhang", "abstract": "  We introduce the task of human action anomaly detection (HAAD), which aims to\nidentify anomalous motions in an unsupervised manner given only the\npre-determined normal category of training action samples. Compared to prior\nhuman-related anomaly detection tasks which primarily focus on unusual events\nfrom videos, HAAD involves the learning of specific action labels to recognize\nsemantically anomalous human behaviors. To address this task, we propose a\nnormalizing flow (NF)-based detection framework where the sample likelihood is\neffectively leveraged to indicate anomalies. As action anomalies often occur in\nsome specific body parts, in addition to the full-body action feature learning,\nwe incorporate extra encoding streams into our framework for a finer modeling\nof body subsets. Our framework is thus multi-level to jointly discover global\nand local motion anomalies. Furthermore, to show awareness of the potentially\njittery data during recording, we resort to discrete cosine transformation by\nconverting the action samples from the temporal to the frequency domain to\nmitigate the issue of data instability. Extensive experimental results on two\nhuman action datasets demonstrate that our method outperforms the baselines\nformed by adapting state-of-the-art human activity AD approaches to our task of\nHAAD.\n", "link": "http://arxiv.org/abs/2404.17381v1", "date": "2024-04-26", "relevancy": 1.9913, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5451}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4722}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4608}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Frequency-Guided%20Multi-Level%20Human%20Action%20Anomaly%20Detection%20with%0A%20%20Normalizing%20Flows&body=Title%3A%20Frequency-Guided%20Multi-Level%20Human%20Action%20Anomaly%20Detection%20with%0A%20%20Normalizing%20Flows%0AAuthor%3A%20Shun%20Maeda%20and%20Chunzhi%20Gu%20and%20Jun%20Yu%20and%20Shogo%20Tokai%20and%20Shangce%20Gao%20and%20Chao%20Zhang%0AAbstract%3A%20%20%20We%20introduce%20the%20task%20of%20human%20action%20anomaly%20detection%20%28HAAD%29%2C%20which%20aims%20to%0Aidentify%20anomalous%20motions%20in%20an%20unsupervised%20manner%20given%20only%20the%0Apre-determined%20normal%20category%20of%20training%20action%20samples.%20Compared%20to%20prior%0Ahuman-related%20anomaly%20detection%20tasks%20which%20primarily%20focus%20on%20unusual%20events%0Afrom%20videos%2C%20HAAD%20involves%20the%20learning%20of%20specific%20action%20labels%20to%20recognize%0Asemantically%20anomalous%20human%20behaviors.%20To%20address%20this%20task%2C%20we%20propose%20a%0Anormalizing%20flow%20%28NF%29-based%20detection%20framework%20where%20the%20sample%20likelihood%20is%0Aeffectively%20leveraged%20to%20indicate%20anomalies.%20As%20action%20anomalies%20often%20occur%20in%0Asome%20specific%20body%20parts%2C%20in%20addition%20to%20the%20full-body%20action%20feature%20learning%2C%0Awe%20incorporate%20extra%20encoding%20streams%20into%20our%20framework%20for%20a%20finer%20modeling%0Aof%20body%20subsets.%20Our%20framework%20is%20thus%20multi-level%20to%20jointly%20discover%20global%0Aand%20local%20motion%20anomalies.%20Furthermore%2C%20to%20show%20awareness%20of%20the%20potentially%0Ajittery%20data%20during%20recording%2C%20we%20resort%20to%20discrete%20cosine%20transformation%20by%0Aconverting%20the%20action%20samples%20from%20the%20temporal%20to%20the%20frequency%20domain%20to%0Amitigate%20the%20issue%20of%20data%20instability.%20Extensive%20experimental%20results%20on%20two%0Ahuman%20action%20datasets%20demonstrate%20that%20our%20method%20outperforms%20the%20baselines%0Aformed%20by%20adapting%20state-of-the-art%20human%20activity%20AD%20approaches%20to%20our%20task%20of%0AHAAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17381v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frequency-Guided%20Multi-Level%20Human%20Action%20Anomaly%20Detection%20with%0A%20%20Normalizing%20Flows&entry.906535625=Shun%20Maeda%20and%20Chunzhi%20Gu%20and%20Jun%20Yu%20and%20Shogo%20Tokai%20and%20Shangce%20Gao%20and%20Chao%20Zhang&entry.1292438233=%20%20We%20introduce%20the%20task%20of%20human%20action%20anomaly%20detection%20%28HAAD%29%2C%20which%20aims%20to%0Aidentify%20anomalous%20motions%20in%20an%20unsupervised%20manner%20given%20only%20the%0Apre-determined%20normal%20category%20of%20training%20action%20samples.%20Compared%20to%20prior%0Ahuman-related%20anomaly%20detection%20tasks%20which%20primarily%20focus%20on%20unusual%20events%0Afrom%20videos%2C%20HAAD%20involves%20the%20learning%20of%20specific%20action%20labels%20to%20recognize%0Asemantically%20anomalous%20human%20behaviors.%20To%20address%20this%20task%2C%20we%20propose%20a%0Anormalizing%20flow%20%28NF%29-based%20detection%20framework%20where%20the%20sample%20likelihood%20is%0Aeffectively%20leveraged%20to%20indicate%20anomalies.%20As%20action%20anomalies%20often%20occur%20in%0Asome%20specific%20body%20parts%2C%20in%20addition%20to%20the%20full-body%20action%20feature%20learning%2C%0Awe%20incorporate%20extra%20encoding%20streams%20into%20our%20framework%20for%20a%20finer%20modeling%0Aof%20body%20subsets.%20Our%20framework%20is%20thus%20multi-level%20to%20jointly%20discover%20global%0Aand%20local%20motion%20anomalies.%20Furthermore%2C%20to%20show%20awareness%20of%20the%20potentially%0Ajittery%20data%20during%20recording%2C%20we%20resort%20to%20discrete%20cosine%20transformation%20by%0Aconverting%20the%20action%20samples%20from%20the%20temporal%20to%20the%20frequency%20domain%20to%0Amitigate%20the%20issue%20of%20data%20instability.%20Extensive%20experimental%20results%20on%20two%0Ahuman%20action%20datasets%20demonstrate%20that%20our%20method%20outperforms%20the%20baselines%0Aformed%20by%20adapting%20state-of-the-art%20human%20activity%20AD%20approaches%20to%20our%20task%20of%0AHAAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17381v1&entry.124074799=Read"},
{"title": "ChangeBind: A Hybrid Change Encoder for Remote Sensing Change Detection", "author": "Mubashir Noman and Mustansar Fiaz and Hisham Cholakkal", "abstract": "  Change detection (CD) is a fundamental task in remote sensing (RS) which aims\nto detect the semantic changes between the same geographical regions at\ndifferent time stamps. Existing convolutional neural networks (CNNs) based\napproaches often struggle to capture long-range dependencies. Whereas recent\ntransformer-based methods are prone to the dominant global representation and\nmay limit their capabilities to capture the subtle change regions due to the\ncomplexity of the objects in the scene. To address these limitations, we\npropose an effective Siamese-based framework to encode the semantic changes\noccurring in the bi-temporal RS images. The main focus of our design is to\nintroduce a change encoder that leverages local and global feature\nrepresentations to capture both subtle and large change feature information\nfrom multi-scale features to precisely estimate the change regions. Our\nexperimental study on two challenging CD datasets reveals the merits of our\napproach and obtains state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2404.17565v1", "date": "2024-04-26", "relevancy": 1.9783, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5012}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4966}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4871}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ChangeBind%3A%20A%20Hybrid%20Change%20Encoder%20for%20Remote%20Sensing%20Change%20Detection&body=Title%3A%20ChangeBind%3A%20A%20Hybrid%20Change%20Encoder%20for%20Remote%20Sensing%20Change%20Detection%0AAuthor%3A%20Mubashir%20Noman%20and%20Mustansar%20Fiaz%20and%20Hisham%20Cholakkal%0AAbstract%3A%20%20%20Change%20detection%20%28CD%29%20is%20a%20fundamental%20task%20in%20remote%20sensing%20%28RS%29%20which%20aims%0Ato%20detect%20the%20semantic%20changes%20between%20the%20same%20geographical%20regions%20at%0Adifferent%20time%20stamps.%20Existing%20convolutional%20neural%20networks%20%28CNNs%29%20based%0Aapproaches%20often%20struggle%20to%20capture%20long-range%20dependencies.%20Whereas%20recent%0Atransformer-based%20methods%20are%20prone%20to%20the%20dominant%20global%20representation%20and%0Amay%20limit%20their%20capabilities%20to%20capture%20the%20subtle%20change%20regions%20due%20to%20the%0Acomplexity%20of%20the%20objects%20in%20the%20scene.%20To%20address%20these%20limitations%2C%20we%0Apropose%20an%20effective%20Siamese-based%20framework%20to%20encode%20the%20semantic%20changes%0Aoccurring%20in%20the%20bi-temporal%20RS%20images.%20The%20main%20focus%20of%20our%20design%20is%20to%0Aintroduce%20a%20change%20encoder%20that%20leverages%20local%20and%20global%20feature%0Arepresentations%20to%20capture%20both%20subtle%20and%20large%20change%20feature%20information%0Afrom%20multi-scale%20features%20to%20precisely%20estimate%20the%20change%20regions.%20Our%0Aexperimental%20study%20on%20two%20challenging%20CD%20datasets%20reveals%20the%20merits%20of%20our%0Aapproach%20and%20obtains%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17565v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChangeBind%3A%20A%20Hybrid%20Change%20Encoder%20for%20Remote%20Sensing%20Change%20Detection&entry.906535625=Mubashir%20Noman%20and%20Mustansar%20Fiaz%20and%20Hisham%20Cholakkal&entry.1292438233=%20%20Change%20detection%20%28CD%29%20is%20a%20fundamental%20task%20in%20remote%20sensing%20%28RS%29%20which%20aims%0Ato%20detect%20the%20semantic%20changes%20between%20the%20same%20geographical%20regions%20at%0Adifferent%20time%20stamps.%20Existing%20convolutional%20neural%20networks%20%28CNNs%29%20based%0Aapproaches%20often%20struggle%20to%20capture%20long-range%20dependencies.%20Whereas%20recent%0Atransformer-based%20methods%20are%20prone%20to%20the%20dominant%20global%20representation%20and%0Amay%20limit%20their%20capabilities%20to%20capture%20the%20subtle%20change%20regions%20due%20to%20the%0Acomplexity%20of%20the%20objects%20in%20the%20scene.%20To%20address%20these%20limitations%2C%20we%0Apropose%20an%20effective%20Siamese-based%20framework%20to%20encode%20the%20semantic%20changes%0Aoccurring%20in%20the%20bi-temporal%20RS%20images.%20The%20main%20focus%20of%20our%20design%20is%20to%0Aintroduce%20a%20change%20encoder%20that%20leverages%20local%20and%20global%20feature%0Arepresentations%20to%20capture%20both%20subtle%20and%20large%20change%20feature%20information%0Afrom%20multi-scale%20features%20to%20precisely%20estimate%20the%20change%20regions.%20Our%0Aexperimental%20study%20on%20two%20challenging%20CD%20datasets%20reveals%20the%20merits%20of%20our%0Aapproach%20and%20obtains%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17565v1&entry.124074799=Read"},
{"title": "Laplace-HDC: Understanding the geometry of binary hyperdimensional\n  computing", "author": "Saeid Pourmand and Wyatt D. Whiting and Alireza Aghasi and Nicholas F. Marshall", "abstract": "  This paper studies the geometry of binary hyperdimensional computing (HDC), a\ncomputational scheme in which data are encoded using high-dimensional binary\nvectors. We establish a result about the similarity structure induced by the\nHDC binding operator and show that the Laplace kernel naturally arises in this\nsetting, motivating our new encoding method Laplace-HDC, which improves upon\nprevious methods. We describe how our results indicate limitations of binary\nHDC in encoding spatial information from images and discuss potential\nsolutions, including using Haar convolutional features and the definition of a\ntranslation-equivariant HDC encoding. Several numerical experiments\nhighlighting the improved accuracy of Laplace-HDC in contrast to alternative\nmethods are presented. We also numerically study other aspects of the proposed\nframework such as robustness and the underlying translation-equivariant\nencoding.\n", "link": "http://arxiv.org/abs/2404.10759v2", "date": "2024-04-26", "relevancy": 1.9592, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5047}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4801}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4767}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Laplace-HDC%3A%20Understanding%20the%20geometry%20of%20binary%20hyperdimensional%0A%20%20computing&body=Title%3A%20Laplace-HDC%3A%20Understanding%20the%20geometry%20of%20binary%20hyperdimensional%0A%20%20computing%0AAuthor%3A%20Saeid%20Pourmand%20and%20Wyatt%20D.%20Whiting%20and%20Alireza%20Aghasi%20and%20Nicholas%20F.%20Marshall%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20geometry%20of%20binary%20hyperdimensional%20computing%20%28HDC%29%2C%20a%0Acomputational%20scheme%20in%20which%20data%20are%20encoded%20using%20high-dimensional%20binary%0Avectors.%20We%20establish%20a%20result%20about%20the%20similarity%20structure%20induced%20by%20the%0AHDC%20binding%20operator%20and%20show%20that%20the%20Laplace%20kernel%20naturally%20arises%20in%20this%0Asetting%2C%20motivating%20our%20new%20encoding%20method%20Laplace-HDC%2C%20which%20improves%20upon%0Aprevious%20methods.%20We%20describe%20how%20our%20results%20indicate%20limitations%20of%20binary%0AHDC%20in%20encoding%20spatial%20information%20from%20images%20and%20discuss%20potential%0Asolutions%2C%20including%20using%20Haar%20convolutional%20features%20and%20the%20definition%20of%20a%0Atranslation-equivariant%20HDC%20encoding.%20Several%20numerical%20experiments%0Ahighlighting%20the%20improved%20accuracy%20of%20Laplace-HDC%20in%20contrast%20to%20alternative%0Amethods%20are%20presented.%20We%20also%20numerically%20study%20other%20aspects%20of%20the%20proposed%0Aframework%20such%20as%20robustness%20and%20the%20underlying%20translation-equivariant%0Aencoding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10759v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Laplace-HDC%3A%20Understanding%20the%20geometry%20of%20binary%20hyperdimensional%0A%20%20computing&entry.906535625=Saeid%20Pourmand%20and%20Wyatt%20D.%20Whiting%20and%20Alireza%20Aghasi%20and%20Nicholas%20F.%20Marshall&entry.1292438233=%20%20This%20paper%20studies%20the%20geometry%20of%20binary%20hyperdimensional%20computing%20%28HDC%29%2C%20a%0Acomputational%20scheme%20in%20which%20data%20are%20encoded%20using%20high-dimensional%20binary%0Avectors.%20We%20establish%20a%20result%20about%20the%20similarity%20structure%20induced%20by%20the%0AHDC%20binding%20operator%20and%20show%20that%20the%20Laplace%20kernel%20naturally%20arises%20in%20this%0Asetting%2C%20motivating%20our%20new%20encoding%20method%20Laplace-HDC%2C%20which%20improves%20upon%0Aprevious%20methods.%20We%20describe%20how%20our%20results%20indicate%20limitations%20of%20binary%0AHDC%20in%20encoding%20spatial%20information%20from%20images%20and%20discuss%20potential%0Asolutions%2C%20including%20using%20Haar%20convolutional%20features%20and%20the%20definition%20of%20a%0Atranslation-equivariant%20HDC%20encoding.%20Several%20numerical%20experiments%0Ahighlighting%20the%20improved%20accuracy%20of%20Laplace-HDC%20in%20contrast%20to%20alternative%0Amethods%20are%20presented.%20We%20also%20numerically%20study%20other%20aspects%20of%20the%20proposed%0Aframework%20such%20as%20robustness%20and%20the%20underlying%20translation-equivariant%0Aencoding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10759v2&entry.124074799=Read"},
{"title": "Redefining Safety for Autonomous Vehicles", "author": "Philip Koopman and William Widen", "abstract": "  Existing definitions and associated conceptual frameworks for computer-based\nsystem safety should be revisited in light of real-world experiences from\ndeploying autonomous vehicles. Current terminology used by industry safety\nstandards emphasizes mitigation of risk from specifically identified hazards,\nand carries assumptions based on human-supervised vehicle operation. Operation\nwithout a human driver dramatically increases the scope of safety concerns,\nespecially due to operation in an open world environment, a requirement to\nself-enforce operational limits, participation in an ad hoc sociotechnical\nsystem of systems, and a requirement to conform to both legal and ethical\nconstraints. Existing standards and terminology only partially address these\nnew challenges. We propose updated definitions for core system safety concepts\nthat encompass these additional considerations as a starting point for evolving\nsafe-ty approaches to address these additional safety challenges. These results\nmight additionally inform framing safety terminology for other autonomous\nsystem applications.\n", "link": "http://arxiv.org/abs/2404.16768v2", "date": "2024-04-26", "relevancy": 1.9552, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5111}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4741}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4724}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Redefining%20Safety%20for%20Autonomous%20Vehicles&body=Title%3A%20Redefining%20Safety%20for%20Autonomous%20Vehicles%0AAuthor%3A%20Philip%20Koopman%20and%20William%20Widen%0AAbstract%3A%20%20%20Existing%20definitions%20and%20associated%20conceptual%20frameworks%20for%20computer-based%0Asystem%20safety%20should%20be%20revisited%20in%20light%20of%20real-world%20experiences%20from%0Adeploying%20autonomous%20vehicles.%20Current%20terminology%20used%20by%20industry%20safety%0Astandards%20emphasizes%20mitigation%20of%20risk%20from%20specifically%20identified%20hazards%2C%0Aand%20carries%20assumptions%20based%20on%20human-supervised%20vehicle%20operation.%20Operation%0Awithout%20a%20human%20driver%20dramatically%20increases%20the%20scope%20of%20safety%20concerns%2C%0Aespecially%20due%20to%20operation%20in%20an%20open%20world%20environment%2C%20a%20requirement%20to%0Aself-enforce%20operational%20limits%2C%20participation%20in%20an%20ad%20hoc%20sociotechnical%0Asystem%20of%20systems%2C%20and%20a%20requirement%20to%20conform%20to%20both%20legal%20and%20ethical%0Aconstraints.%20Existing%20standards%20and%20terminology%20only%20partially%20address%20these%0Anew%20challenges.%20We%20propose%20updated%20definitions%20for%20core%20system%20safety%20concepts%0Athat%20encompass%20these%20additional%20considerations%20as%20a%20starting%20point%20for%20evolving%0Asafe-ty%20approaches%20to%20address%20these%20additional%20safety%20challenges.%20These%20results%0Amight%20additionally%20inform%20framing%20safety%20terminology%20for%20other%20autonomous%0Asystem%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16768v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Redefining%20Safety%20for%20Autonomous%20Vehicles&entry.906535625=Philip%20Koopman%20and%20William%20Widen&entry.1292438233=%20%20Existing%20definitions%20and%20associated%20conceptual%20frameworks%20for%20computer-based%0Asystem%20safety%20should%20be%20revisited%20in%20light%20of%20real-world%20experiences%20from%0Adeploying%20autonomous%20vehicles.%20Current%20terminology%20used%20by%20industry%20safety%0Astandards%20emphasizes%20mitigation%20of%20risk%20from%20specifically%20identified%20hazards%2C%0Aand%20carries%20assumptions%20based%20on%20human-supervised%20vehicle%20operation.%20Operation%0Awithout%20a%20human%20driver%20dramatically%20increases%20the%20scope%20of%20safety%20concerns%2C%0Aespecially%20due%20to%20operation%20in%20an%20open%20world%20environment%2C%20a%20requirement%20to%0Aself-enforce%20operational%20limits%2C%20participation%20in%20an%20ad%20hoc%20sociotechnical%0Asystem%20of%20systems%2C%20and%20a%20requirement%20to%20conform%20to%20both%20legal%20and%20ethical%0Aconstraints.%20Existing%20standards%20and%20terminology%20only%20partially%20address%20these%0Anew%20challenges.%20We%20propose%20updated%20definitions%20for%20core%20system%20safety%20concepts%0Athat%20encompass%20these%20additional%20considerations%20as%20a%20starting%20point%20for%20evolving%0Asafe-ty%20approaches%20to%20address%20these%20additional%20safety%20challenges.%20These%20results%0Amight%20additionally%20inform%20framing%20safety%20terminology%20for%20other%20autonomous%0Asystem%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16768v2&entry.124074799=Read"},
{"title": "Online Policy Learning and Inference by Matrix Completion", "author": "Congyuan Duan and Jingyang Li and Dong Xia", "abstract": "  Making online decisions can be challenging when features are sparse and\northogonal to historical ones, especially when the optimal policy is learned\nthrough collaborative filtering. We formulate the problem as a matrix\ncompletion bandit (MCB), where the expected reward under each arm is\ncharacterized by an unknown low-rank matrix. The $\\epsilon$-greedy bandit and\nthe online gradient descent algorithm are explored. Policy learning and regret\nperformance are studied under a specific schedule for exploration probabilities\nand step sizes. A faster decaying exploration probability yields smaller regret\nbut learns the optimal policy less accurately. We investigate an online\ndebiasing method based on inverse propensity weighting (IPW) and a general\nframework for online policy inference. The IPW-based estimators are\nasymptotically normal under mild arm-optimality conditions. Numerical\nsimulations corroborate our theoretical findings. Our methods are applied to\nthe San Francisco parking pricing project data, revealing intriguing\ndiscoveries and outperforming the benchmark policy.\n", "link": "http://arxiv.org/abs/2404.17398v1", "date": "2024-04-26", "relevancy": 1.9518, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5606}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4884}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4585}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Online%20Policy%20Learning%20and%20Inference%20by%20Matrix%20Completion&body=Title%3A%20Online%20Policy%20Learning%20and%20Inference%20by%20Matrix%20Completion%0AAuthor%3A%20Congyuan%20Duan%20and%20Jingyang%20Li%20and%20Dong%20Xia%0AAbstract%3A%20%20%20Making%20online%20decisions%20can%20be%20challenging%20when%20features%20are%20sparse%20and%0Aorthogonal%20to%20historical%20ones%2C%20especially%20when%20the%20optimal%20policy%20is%20learned%0Athrough%20collaborative%20filtering.%20We%20formulate%20the%20problem%20as%20a%20matrix%0Acompletion%20bandit%20%28MCB%29%2C%20where%20the%20expected%20reward%20under%20each%20arm%20is%0Acharacterized%20by%20an%20unknown%20low-rank%20matrix.%20The%20%24%5Cepsilon%24-greedy%20bandit%20and%0Athe%20online%20gradient%20descent%20algorithm%20are%20explored.%20Policy%20learning%20and%20regret%0Aperformance%20are%20studied%20under%20a%20specific%20schedule%20for%20exploration%20probabilities%0Aand%20step%20sizes.%20A%20faster%20decaying%20exploration%20probability%20yields%20smaller%20regret%0Abut%20learns%20the%20optimal%20policy%20less%20accurately.%20We%20investigate%20an%20online%0Adebiasing%20method%20based%20on%20inverse%20propensity%20weighting%20%28IPW%29%20and%20a%20general%0Aframework%20for%20online%20policy%20inference.%20The%20IPW-based%20estimators%20are%0Aasymptotically%20normal%20under%20mild%20arm-optimality%20conditions.%20Numerical%0Asimulations%20corroborate%20our%20theoretical%20findings.%20Our%20methods%20are%20applied%20to%0Athe%20San%20Francisco%20parking%20pricing%20project%20data%2C%20revealing%20intriguing%0Adiscoveries%20and%20outperforming%20the%20benchmark%20policy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17398v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Policy%20Learning%20and%20Inference%20by%20Matrix%20Completion&entry.906535625=Congyuan%20Duan%20and%20Jingyang%20Li%20and%20Dong%20Xia&entry.1292438233=%20%20Making%20online%20decisions%20can%20be%20challenging%20when%20features%20are%20sparse%20and%0Aorthogonal%20to%20historical%20ones%2C%20especially%20when%20the%20optimal%20policy%20is%20learned%0Athrough%20collaborative%20filtering.%20We%20formulate%20the%20problem%20as%20a%20matrix%0Acompletion%20bandit%20%28MCB%29%2C%20where%20the%20expected%20reward%20under%20each%20arm%20is%0Acharacterized%20by%20an%20unknown%20low-rank%20matrix.%20The%20%24%5Cepsilon%24-greedy%20bandit%20and%0Athe%20online%20gradient%20descent%20algorithm%20are%20explored.%20Policy%20learning%20and%20regret%0Aperformance%20are%20studied%20under%20a%20specific%20schedule%20for%20exploration%20probabilities%0Aand%20step%20sizes.%20A%20faster%20decaying%20exploration%20probability%20yields%20smaller%20regret%0Abut%20learns%20the%20optimal%20policy%20less%20accurately.%20We%20investigate%20an%20online%0Adebiasing%20method%20based%20on%20inverse%20propensity%20weighting%20%28IPW%29%20and%20a%20general%0Aframework%20for%20online%20policy%20inference.%20The%20IPW-based%20estimators%20are%0Aasymptotically%20normal%20under%20mild%20arm-optimality%20conditions.%20Numerical%0Asimulations%20corroborate%20our%20theoretical%20findings.%20Our%20methods%20are%20applied%20to%0Athe%20San%20Francisco%20parking%20pricing%20project%20data%2C%20revealing%20intriguing%0Adiscoveries%20and%20outperforming%20the%20benchmark%20policy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17398v1&entry.124074799=Read"},
{"title": "Optimizing ZX-Diagrams with Deep Reinforcement Learning", "author": "Maximilian N\u00e4gele and Florian Marquardt", "abstract": "  ZX-diagrams are a powerful graphical language for the description of quantum\nprocesses with applications in fundamental quantum mechanics, quantum circuit\noptimization, tensor network simulation, and many more. The utility of\nZX-diagrams relies on a set of local transformation rules that can be applied\nto them without changing the underlying quantum process they describe. These\nrules can be exploited to optimize the structure of ZX-diagrams for a range of\napplications. However, finding an optimal sequence of transformation rules is\ngenerally an open problem. In this work, we bring together ZX-diagrams with\nreinforcement learning, a machine learning technique designed to discover an\noptimal sequence of actions in a decision-making problem and show that a\ntrained reinforcement learning agent can significantly outperform other\noptimization techniques like a greedy strategy or simulated annealing. The use\nof graph neural networks to encode the policy of the agent enables\ngeneralization to diagrams much bigger than seen during the training phase.\n", "link": "http://arxiv.org/abs/2311.18588v2", "date": "2024-04-26", "relevancy": 1.9497, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5106}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.483}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4826}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Optimizing%20ZX-Diagrams%20with%20Deep%20Reinforcement%20Learning&body=Title%3A%20Optimizing%20ZX-Diagrams%20with%20Deep%20Reinforcement%20Learning%0AAuthor%3A%20Maximilian%20N%C3%A4gele%20and%20Florian%20Marquardt%0AAbstract%3A%20%20%20ZX-diagrams%20are%20a%20powerful%20graphical%20language%20for%20the%20description%20of%20quantum%0Aprocesses%20with%20applications%20in%20fundamental%20quantum%20mechanics%2C%20quantum%20circuit%0Aoptimization%2C%20tensor%20network%20simulation%2C%20and%20many%20more.%20The%20utility%20of%0AZX-diagrams%20relies%20on%20a%20set%20of%20local%20transformation%20rules%20that%20can%20be%20applied%0Ato%20them%20without%20changing%20the%20underlying%20quantum%20process%20they%20describe.%20These%0Arules%20can%20be%20exploited%20to%20optimize%20the%20structure%20of%20ZX-diagrams%20for%20a%20range%20of%0Aapplications.%20However%2C%20finding%20an%20optimal%20sequence%20of%20transformation%20rules%20is%0Agenerally%20an%20open%20problem.%20In%20this%20work%2C%20we%20bring%20together%20ZX-diagrams%20with%0Areinforcement%20learning%2C%20a%20machine%20learning%20technique%20designed%20to%20discover%20an%0Aoptimal%20sequence%20of%20actions%20in%20a%20decision-making%20problem%20and%20show%20that%20a%0Atrained%20reinforcement%20learning%20agent%20can%20significantly%20outperform%20other%0Aoptimization%20techniques%20like%20a%20greedy%20strategy%20or%20simulated%20annealing.%20The%20use%0Aof%20graph%20neural%20networks%20to%20encode%20the%20policy%20of%20the%20agent%20enables%0Ageneralization%20to%20diagrams%20much%20bigger%20than%20seen%20during%20the%20training%20phase.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18588v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20ZX-Diagrams%20with%20Deep%20Reinforcement%20Learning&entry.906535625=Maximilian%20N%C3%A4gele%20and%20Florian%20Marquardt&entry.1292438233=%20%20ZX-diagrams%20are%20a%20powerful%20graphical%20language%20for%20the%20description%20of%20quantum%0Aprocesses%20with%20applications%20in%20fundamental%20quantum%20mechanics%2C%20quantum%20circuit%0Aoptimization%2C%20tensor%20network%20simulation%2C%20and%20many%20more.%20The%20utility%20of%0AZX-diagrams%20relies%20on%20a%20set%20of%20local%20transformation%20rules%20that%20can%20be%20applied%0Ato%20them%20without%20changing%20the%20underlying%20quantum%20process%20they%20describe.%20These%0Arules%20can%20be%20exploited%20to%20optimize%20the%20structure%20of%20ZX-diagrams%20for%20a%20range%20of%0Aapplications.%20However%2C%20finding%20an%20optimal%20sequence%20of%20transformation%20rules%20is%0Agenerally%20an%20open%20problem.%20In%20this%20work%2C%20we%20bring%20together%20ZX-diagrams%20with%0Areinforcement%20learning%2C%20a%20machine%20learning%20technique%20designed%20to%20discover%20an%0Aoptimal%20sequence%20of%20actions%20in%20a%20decision-making%20problem%20and%20show%20that%20a%0Atrained%20reinforcement%20learning%20agent%20can%20significantly%20outperform%20other%0Aoptimization%20techniques%20like%20a%20greedy%20strategy%20or%20simulated%20annealing.%20The%20use%0Aof%20graph%20neural%20networks%20to%20encode%20the%20policy%20of%20the%20agent%20enables%0Ageneralization%20to%20diagrams%20much%20bigger%20than%20seen%20during%20the%20training%20phase.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18588v2&entry.124074799=Read"},
{"title": "Should Teleoperation Be like Driving in a Car? Comparison of\n  Teleoperation HMIs", "author": "Maria-Magdalena Wolf and Richard Taupitz and Frank Diermeyer", "abstract": "  Since Automated Driving Systems are not expected to operate flawlessly,\nAutomated Vehicles will require human assistance in certain situations. For\nthis reason, teleoperation offers the opportunity for a human to be remotely\nconnected to the vehicle and assist it. The Remote Operator can provide\nextensive support by directly controlling the vehicle, eliminating the need for\nAutomated Driving functions. However, due to the physical disconnection to the\nvehicle, monitoring and controlling is challenging compared to driving in the\nvehicle. Therefore, this work follows the approach of simplifying the task for\nthe Remote Operator by separating the path and velocity input. In a study using\na miniature vehicle, different operator-vehicle interactions and input devices\nwere compared based on collisions, task completion time, usability and\nworkload. The evaluation revealed significant differences between the three\nimplemented prototypes using a steering wheel, mouse and keyboard or a\ntouchscreen. The separate input of path and velocity via mouse and keyboard or\ntouchscreen is preferred but is slower compared to parallel input via steering\nwheel.\n", "link": "http://arxiv.org/abs/2404.13697v2", "date": "2024-04-26", "relevancy": 1.9295, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.542}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4474}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4208}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Should%20Teleoperation%20Be%20like%20Driving%20in%20a%20Car%3F%20Comparison%20of%0A%20%20Teleoperation%20HMIs&body=Title%3A%20Should%20Teleoperation%20Be%20like%20Driving%20in%20a%20Car%3F%20Comparison%20of%0A%20%20Teleoperation%20HMIs%0AAuthor%3A%20Maria-Magdalena%20Wolf%20and%20Richard%20Taupitz%20and%20Frank%20Diermeyer%0AAbstract%3A%20%20%20Since%20Automated%20Driving%20Systems%20are%20not%20expected%20to%20operate%20flawlessly%2C%0AAutomated%20Vehicles%20will%20require%20human%20assistance%20in%20certain%20situations.%20For%0Athis%20reason%2C%20teleoperation%20offers%20the%20opportunity%20for%20a%20human%20to%20be%20remotely%0Aconnected%20to%20the%20vehicle%20and%20assist%20it.%20The%20Remote%20Operator%20can%20provide%0Aextensive%20support%20by%20directly%20controlling%20the%20vehicle%2C%20eliminating%20the%20need%20for%0AAutomated%20Driving%20functions.%20However%2C%20due%20to%20the%20physical%20disconnection%20to%20the%0Avehicle%2C%20monitoring%20and%20controlling%20is%20challenging%20compared%20to%20driving%20in%20the%0Avehicle.%20Therefore%2C%20this%20work%20follows%20the%20approach%20of%20simplifying%20the%20task%20for%0Athe%20Remote%20Operator%20by%20separating%20the%20path%20and%20velocity%20input.%20In%20a%20study%20using%0Aa%20miniature%20vehicle%2C%20different%20operator-vehicle%20interactions%20and%20input%20devices%0Awere%20compared%20based%20on%20collisions%2C%20task%20completion%20time%2C%20usability%20and%0Aworkload.%20The%20evaluation%20revealed%20significant%20differences%20between%20the%20three%0Aimplemented%20prototypes%20using%20a%20steering%20wheel%2C%20mouse%20and%20keyboard%20or%20a%0Atouchscreen.%20The%20separate%20input%20of%20path%20and%20velocity%20via%20mouse%20and%20keyboard%20or%0Atouchscreen%20is%20preferred%20but%20is%20slower%20compared%20to%20parallel%20input%20via%20steering%0Awheel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13697v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Should%20Teleoperation%20Be%20like%20Driving%20in%20a%20Car%3F%20Comparison%20of%0A%20%20Teleoperation%20HMIs&entry.906535625=Maria-Magdalena%20Wolf%20and%20Richard%20Taupitz%20and%20Frank%20Diermeyer&entry.1292438233=%20%20Since%20Automated%20Driving%20Systems%20are%20not%20expected%20to%20operate%20flawlessly%2C%0AAutomated%20Vehicles%20will%20require%20human%20assistance%20in%20certain%20situations.%20For%0Athis%20reason%2C%20teleoperation%20offers%20the%20opportunity%20for%20a%20human%20to%20be%20remotely%0Aconnected%20to%20the%20vehicle%20and%20assist%20it.%20The%20Remote%20Operator%20can%20provide%0Aextensive%20support%20by%20directly%20controlling%20the%20vehicle%2C%20eliminating%20the%20need%20for%0AAutomated%20Driving%20functions.%20However%2C%20due%20to%20the%20physical%20disconnection%20to%20the%0Avehicle%2C%20monitoring%20and%20controlling%20is%20challenging%20compared%20to%20driving%20in%20the%0Avehicle.%20Therefore%2C%20this%20work%20follows%20the%20approach%20of%20simplifying%20the%20task%20for%0Athe%20Remote%20Operator%20by%20separating%20the%20path%20and%20velocity%20input.%20In%20a%20study%20using%0Aa%20miniature%20vehicle%2C%20different%20operator-vehicle%20interactions%20and%20input%20devices%0Awere%20compared%20based%20on%20collisions%2C%20task%20completion%20time%2C%20usability%20and%0Aworkload.%20The%20evaluation%20revealed%20significant%20differences%20between%20the%20three%0Aimplemented%20prototypes%20using%20a%20steering%20wheel%2C%20mouse%20and%20keyboard%20or%20a%0Atouchscreen.%20The%20separate%20input%20of%20path%20and%20velocity%20via%20mouse%20and%20keyboard%20or%0Atouchscreen%20is%20preferred%20but%20is%20slower%20compared%20to%20parallel%20input%20via%20steering%0Awheel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13697v2&entry.124074799=Read"},
{"title": "Quantum Adjoint Convolutional Layers for Effective Data Representation", "author": "Ren-Xin Zhao and Shi Wang and Yaonan Wang", "abstract": "  Quantum Convolutional Layer (QCL) is considered as one of the core of Quantum\nConvolutional Neural Networks (QCNNs) due to its efficient data feature\nextraction capability. However, the current principle of QCL is not as\nmathematically understandable as Classical Convolutional Layer (CCL) due to its\nblack-box structure. Moreover, classical data mapping in many QCLs is\ninefficient. To this end, firstly, the Quantum Adjoint Convolution Operation\n(QACO) consisting of a quantum amplitude encoding and its inverse is\ntheoretically shown to be equivalent to the quantum normalization of the\nconvolution operation based on the Frobenius inner product while achieving an\nefficient characterization of the data. Subsequently, QACO is extended into a\nQuantum Adjoint Convolutional Layer (QACL) by Quantum Phase Estimation (QPE) to\ncompute all Frobenius inner products in parallel. At last, comparative\nsimulation experiments are carried out on PennyLane and TensorFlow platforms,\nmainly for the two cases of kernel fixed and unfixed in QACL. The results\ndemonstrate that QACL with the insight of special quantum properties for the\nsame images, provides higher training accuracy in MNIST and Fashion MNIST\nclassification experiments, but sacrifices the learning performance to some\nextent. Predictably, our research lays the foundation for the development of\nefficient and interpretable quantum convolutional networks and also advances\nthe field of quantum machine vision.\n", "link": "http://arxiv.org/abs/2404.17378v1", "date": "2024-04-26", "relevancy": 1.9128, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4952}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4703}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4554}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Quantum%20Adjoint%20Convolutional%20Layers%20for%20Effective%20Data%20Representation&body=Title%3A%20Quantum%20Adjoint%20Convolutional%20Layers%20for%20Effective%20Data%20Representation%0AAuthor%3A%20Ren-Xin%20Zhao%20and%20Shi%20Wang%20and%20Yaonan%20Wang%0AAbstract%3A%20%20%20Quantum%20Convolutional%20Layer%20%28QCL%29%20is%20considered%20as%20one%20of%20the%20core%20of%20Quantum%0AConvolutional%20Neural%20Networks%20%28QCNNs%29%20due%20to%20its%20efficient%20data%20feature%0Aextraction%20capability.%20However%2C%20the%20current%20principle%20of%20QCL%20is%20not%20as%0Amathematically%20understandable%20as%20Classical%20Convolutional%20Layer%20%28CCL%29%20due%20to%20its%0Ablack-box%20structure.%20Moreover%2C%20classical%20data%20mapping%20in%20many%20QCLs%20is%0Ainefficient.%20To%20this%20end%2C%20firstly%2C%20the%20Quantum%20Adjoint%20Convolution%20Operation%0A%28QACO%29%20consisting%20of%20a%20quantum%20amplitude%20encoding%20and%20its%20inverse%20is%0Atheoretically%20shown%20to%20be%20equivalent%20to%20the%20quantum%20normalization%20of%20the%0Aconvolution%20operation%20based%20on%20the%20Frobenius%20inner%20product%20while%20achieving%20an%0Aefficient%20characterization%20of%20the%20data.%20Subsequently%2C%20QACO%20is%20extended%20into%20a%0AQuantum%20Adjoint%20Convolutional%20Layer%20%28QACL%29%20by%20Quantum%20Phase%20Estimation%20%28QPE%29%20to%0Acompute%20all%20Frobenius%20inner%20products%20in%20parallel.%20At%20last%2C%20comparative%0Asimulation%20experiments%20are%20carried%20out%20on%20PennyLane%20and%20TensorFlow%20platforms%2C%0Amainly%20for%20the%20two%20cases%20of%20kernel%20fixed%20and%20unfixed%20in%20QACL.%20The%20results%0Ademonstrate%20that%20QACL%20with%20the%20insight%20of%20special%20quantum%20properties%20for%20the%0Asame%20images%2C%20provides%20higher%20training%20accuracy%20in%20MNIST%20and%20Fashion%20MNIST%0Aclassification%20experiments%2C%20but%20sacrifices%20the%20learning%20performance%20to%20some%0Aextent.%20Predictably%2C%20our%20research%20lays%20the%20foundation%20for%20the%20development%20of%0Aefficient%20and%20interpretable%20quantum%20convolutional%20networks%20and%20also%20advances%0Athe%20field%20of%20quantum%20machine%20vision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17378v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Adjoint%20Convolutional%20Layers%20for%20Effective%20Data%20Representation&entry.906535625=Ren-Xin%20Zhao%20and%20Shi%20Wang%20and%20Yaonan%20Wang&entry.1292438233=%20%20Quantum%20Convolutional%20Layer%20%28QCL%29%20is%20considered%20as%20one%20of%20the%20core%20of%20Quantum%0AConvolutional%20Neural%20Networks%20%28QCNNs%29%20due%20to%20its%20efficient%20data%20feature%0Aextraction%20capability.%20However%2C%20the%20current%20principle%20of%20QCL%20is%20not%20as%0Amathematically%20understandable%20as%20Classical%20Convolutional%20Layer%20%28CCL%29%20due%20to%20its%0Ablack-box%20structure.%20Moreover%2C%20classical%20data%20mapping%20in%20many%20QCLs%20is%0Ainefficient.%20To%20this%20end%2C%20firstly%2C%20the%20Quantum%20Adjoint%20Convolution%20Operation%0A%28QACO%29%20consisting%20of%20a%20quantum%20amplitude%20encoding%20and%20its%20inverse%20is%0Atheoretically%20shown%20to%20be%20equivalent%20to%20the%20quantum%20normalization%20of%20the%0Aconvolution%20operation%20based%20on%20the%20Frobenius%20inner%20product%20while%20achieving%20an%0Aefficient%20characterization%20of%20the%20data.%20Subsequently%2C%20QACO%20is%20extended%20into%20a%0AQuantum%20Adjoint%20Convolutional%20Layer%20%28QACL%29%20by%20Quantum%20Phase%20Estimation%20%28QPE%29%20to%0Acompute%20all%20Frobenius%20inner%20products%20in%20parallel.%20At%20last%2C%20comparative%0Asimulation%20experiments%20are%20carried%20out%20on%20PennyLane%20and%20TensorFlow%20platforms%2C%0Amainly%20for%20the%20two%20cases%20of%20kernel%20fixed%20and%20unfixed%20in%20QACL.%20The%20results%0Ademonstrate%20that%20QACL%20with%20the%20insight%20of%20special%20quantum%20properties%20for%20the%0Asame%20images%2C%20provides%20higher%20training%20accuracy%20in%20MNIST%20and%20Fashion%20MNIST%0Aclassification%20experiments%2C%20but%20sacrifices%20the%20learning%20performance%20to%20some%0Aextent.%20Predictably%2C%20our%20research%20lays%20the%20foundation%20for%20the%20development%20of%0Aefficient%20and%20interpretable%20quantum%20convolutional%20networks%20and%20also%20advances%0Athe%20field%20of%20quantum%20machine%20vision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17378v1&entry.124074799=Read"},
{"title": "Learning Performance-Improving Code Edits", "author": "Alexander Shypula and Aman Madaan and Yimeng Zeng and Uri Alon and Jacob Gardner and Milad Hashemi and Graham Neubig and Parthasarathy Ranganathan and Osbert Bastani and Amir Yazdanbakhsh", "abstract": "  With the decline of Moore's law, optimizing program performance has become a\nmajor focus of software research. However, high-level optimizations such as API\nand algorithm changes remain elusive due to the difficulty of understanding the\nsemantics of code. Simultaneously, pretrained large language models (LLMs) have\ndemonstrated strong capabilities at solving a wide range of programming tasks.\nTo that end, we introduce a framework for adapting LLMs to high-level program\noptimization. First, we curate a dataset of performance-improving edits made by\nhuman programmers of over 77,000 competitive C++ programming submission pairs,\naccompanied by extensive unit tests. A major challenge is the significant\nvariability of measuring performance on commodity hardware, which can lead to\nspurious \"improvements.\" To isolate and reliably evaluate the impact of program\noptimizations, we design an environment based on the gem5 full system\nsimulator, the de facto simulator used in academia and industry. Next, we\npropose a broad range of adaptation strategies for code optimization; for\nprompting, these include retrieval-based few-shot prompting and\nchain-of-thought, and for finetuning, these include performance-conditioned\ngeneration and synthetic data augmentation based on self-play. A combination of\nthese techniques achieves a mean speedup of 6.86 with eight generations, higher\nthan average optimizations from individual programmers (3.66). Using our\nmodel's fastest generations, we set a new upper limit on the fastest speedup\npossible for our dataset at 9.64 compared to using the fastest human\nsubmissions available (9.56).\n", "link": "http://arxiv.org/abs/2302.07867v5", "date": "2024-04-26", "relevancy": 1.9093, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4821}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4757}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4732}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Performance-Improving%20Code%20Edits&body=Title%3A%20Learning%20Performance-Improving%20Code%20Edits%0AAuthor%3A%20Alexander%20Shypula%20and%20Aman%20Madaan%20and%20Yimeng%20Zeng%20and%20Uri%20Alon%20and%20Jacob%20Gardner%20and%20Milad%20Hashemi%20and%20Graham%20Neubig%20and%20Parthasarathy%20Ranganathan%20and%20Osbert%20Bastani%20and%20Amir%20Yazdanbakhsh%0AAbstract%3A%20%20%20With%20the%20decline%20of%20Moore%27s%20law%2C%20optimizing%20program%20performance%20has%20become%20a%0Amajor%20focus%20of%20software%20research.%20However%2C%20high-level%20optimizations%20such%20as%20API%0Aand%20algorithm%20changes%20remain%20elusive%20due%20to%20the%20difficulty%20of%20understanding%20the%0Asemantics%20of%20code.%20Simultaneously%2C%20pretrained%20large%20language%20models%20%28LLMs%29%20have%0Ademonstrated%20strong%20capabilities%20at%20solving%20a%20wide%20range%20of%20programming%20tasks.%0ATo%20that%20end%2C%20we%20introduce%20a%20framework%20for%20adapting%20LLMs%20to%20high-level%20program%0Aoptimization.%20First%2C%20we%20curate%20a%20dataset%20of%20performance-improving%20edits%20made%20by%0Ahuman%20programmers%20of%20over%2077%2C000%20competitive%20C%2B%2B%20programming%20submission%20pairs%2C%0Aaccompanied%20by%20extensive%20unit%20tests.%20A%20major%20challenge%20is%20the%20significant%0Avariability%20of%20measuring%20performance%20on%20commodity%20hardware%2C%20which%20can%20lead%20to%0Aspurious%20%22improvements.%22%20To%20isolate%20and%20reliably%20evaluate%20the%20impact%20of%20program%0Aoptimizations%2C%20we%20design%20an%20environment%20based%20on%20the%20gem5%20full%20system%0Asimulator%2C%20the%20de%20facto%20simulator%20used%20in%20academia%20and%20industry.%20Next%2C%20we%0Apropose%20a%20broad%20range%20of%20adaptation%20strategies%20for%20code%20optimization%3B%20for%0Aprompting%2C%20these%20include%20retrieval-based%20few-shot%20prompting%20and%0Achain-of-thought%2C%20and%20for%20finetuning%2C%20these%20include%20performance-conditioned%0Ageneration%20and%20synthetic%20data%20augmentation%20based%20on%20self-play.%20A%20combination%20of%0Athese%20techniques%20achieves%20a%20mean%20speedup%20of%206.86%20with%20eight%20generations%2C%20higher%0Athan%20average%20optimizations%20from%20individual%20programmers%20%283.66%29.%20Using%20our%0Amodel%27s%20fastest%20generations%2C%20we%20set%20a%20new%20upper%20limit%20on%20the%20fastest%20speedup%0Apossible%20for%20our%20dataset%20at%209.64%20compared%20to%20using%20the%20fastest%20human%0Asubmissions%20available%20%289.56%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.07867v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Performance-Improving%20Code%20Edits&entry.906535625=Alexander%20Shypula%20and%20Aman%20Madaan%20and%20Yimeng%20Zeng%20and%20Uri%20Alon%20and%20Jacob%20Gardner%20and%20Milad%20Hashemi%20and%20Graham%20Neubig%20and%20Parthasarathy%20Ranganathan%20and%20Osbert%20Bastani%20and%20Amir%20Yazdanbakhsh&entry.1292438233=%20%20With%20the%20decline%20of%20Moore%27s%20law%2C%20optimizing%20program%20performance%20has%20become%20a%0Amajor%20focus%20of%20software%20research.%20However%2C%20high-level%20optimizations%20such%20as%20API%0Aand%20algorithm%20changes%20remain%20elusive%20due%20to%20the%20difficulty%20of%20understanding%20the%0Asemantics%20of%20code.%20Simultaneously%2C%20pretrained%20large%20language%20models%20%28LLMs%29%20have%0Ademonstrated%20strong%20capabilities%20at%20solving%20a%20wide%20range%20of%20programming%20tasks.%0ATo%20that%20end%2C%20we%20introduce%20a%20framework%20for%20adapting%20LLMs%20to%20high-level%20program%0Aoptimization.%20First%2C%20we%20curate%20a%20dataset%20of%20performance-improving%20edits%20made%20by%0Ahuman%20programmers%20of%20over%2077%2C000%20competitive%20C%2B%2B%20programming%20submission%20pairs%2C%0Aaccompanied%20by%20extensive%20unit%20tests.%20A%20major%20challenge%20is%20the%20significant%0Avariability%20of%20measuring%20performance%20on%20commodity%20hardware%2C%20which%20can%20lead%20to%0Aspurious%20%22improvements.%22%20To%20isolate%20and%20reliably%20evaluate%20the%20impact%20of%20program%0Aoptimizations%2C%20we%20design%20an%20environment%20based%20on%20the%20gem5%20full%20system%0Asimulator%2C%20the%20de%20facto%20simulator%20used%20in%20academia%20and%20industry.%20Next%2C%20we%0Apropose%20a%20broad%20range%20of%20adaptation%20strategies%20for%20code%20optimization%3B%20for%0Aprompting%2C%20these%20include%20retrieval-based%20few-shot%20prompting%20and%0Achain-of-thought%2C%20and%20for%20finetuning%2C%20these%20include%20performance-conditioned%0Ageneration%20and%20synthetic%20data%20augmentation%20based%20on%20self-play.%20A%20combination%20of%0Athese%20techniques%20achieves%20a%20mean%20speedup%20of%206.86%20with%20eight%20generations%2C%20higher%0Athan%20average%20optimizations%20from%20individual%20programmers%20%283.66%29.%20Using%20our%0Amodel%27s%20fastest%20generations%2C%20we%20set%20a%20new%20upper%20limit%20on%20the%20fastest%20speedup%0Apossible%20for%20our%20dataset%20at%209.64%20compared%20to%20using%20the%20fastest%20human%0Asubmissions%20available%20%289.56%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.07867v5&entry.124074799=Read"},
{"title": "SEGSRNet for Stereo-Endoscopic Image Super-Resolution and Surgical\n  Instrument Segmentation", "author": "Mansoor Hayat and Supavadee Aramvith and Titipat Achakulvisut", "abstract": "  SEGSRNet addresses the challenge of precisely identifying surgical\ninstruments in low-resolution stereo endoscopic images, a common issue in\nmedical imaging and robotic surgery. Our innovative framework enhances image\nclarity and segmentation accuracy by applying state-of-the-art super-resolution\ntechniques before segmentation. This ensures higher-quality inputs for more\nprecise segmentation. SEGSRNet combines advanced feature extraction and\nattention mechanisms with spatial processing to sharpen image details, which is\nsignificant for accurate tool identification in medical images. Our proposed\nmodel outperforms current models including Dice, IoU, PSNR, and SSIM, SEGSRNet\nwhere it produces clearer and more accurate images for stereo endoscopic\nsurgical imaging. SEGSRNet can provide image resolution and precise\nsegmentation which can significantly enhance surgical accuracy and patient care\noutcomes.\n", "link": "http://arxiv.org/abs/2404.13330v2", "date": "2024-04-26", "relevancy": 1.8968, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4829}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4692}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4675}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SEGSRNet%20for%20Stereo-Endoscopic%20Image%20Super-Resolution%20and%20Surgical%0A%20%20Instrument%20Segmentation&body=Title%3A%20SEGSRNet%20for%20Stereo-Endoscopic%20Image%20Super-Resolution%20and%20Surgical%0A%20%20Instrument%20Segmentation%0AAuthor%3A%20Mansoor%20Hayat%20and%20Supavadee%20Aramvith%20and%20Titipat%20Achakulvisut%0AAbstract%3A%20%20%20SEGSRNet%20addresses%20the%20challenge%20of%20precisely%20identifying%20surgical%0Ainstruments%20in%20low-resolution%20stereo%20endoscopic%20images%2C%20a%20common%20issue%20in%0Amedical%20imaging%20and%20robotic%20surgery.%20Our%20innovative%20framework%20enhances%20image%0Aclarity%20and%20segmentation%20accuracy%20by%20applying%20state-of-the-art%20super-resolution%0Atechniques%20before%20segmentation.%20This%20ensures%20higher-quality%20inputs%20for%20more%0Aprecise%20segmentation.%20SEGSRNet%20combines%20advanced%20feature%20extraction%20and%0Aattention%20mechanisms%20with%20spatial%20processing%20to%20sharpen%20image%20details%2C%20which%20is%0Asignificant%20for%20accurate%20tool%20identification%20in%20medical%20images.%20Our%20proposed%0Amodel%20outperforms%20current%20models%20including%20Dice%2C%20IoU%2C%20PSNR%2C%20and%20SSIM%2C%20SEGSRNet%0Awhere%20it%20produces%20clearer%20and%20more%20accurate%20images%20for%20stereo%20endoscopic%0Asurgical%20imaging.%20SEGSRNet%20can%20provide%20image%20resolution%20and%20precise%0Asegmentation%20which%20can%20significantly%20enhance%20surgical%20accuracy%20and%20patient%20care%0Aoutcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13330v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEGSRNet%20for%20Stereo-Endoscopic%20Image%20Super-Resolution%20and%20Surgical%0A%20%20Instrument%20Segmentation&entry.906535625=Mansoor%20Hayat%20and%20Supavadee%20Aramvith%20and%20Titipat%20Achakulvisut&entry.1292438233=%20%20SEGSRNet%20addresses%20the%20challenge%20of%20precisely%20identifying%20surgical%0Ainstruments%20in%20low-resolution%20stereo%20endoscopic%20images%2C%20a%20common%20issue%20in%0Amedical%20imaging%20and%20robotic%20surgery.%20Our%20innovative%20framework%20enhances%20image%0Aclarity%20and%20segmentation%20accuracy%20by%20applying%20state-of-the-art%20super-resolution%0Atechniques%20before%20segmentation.%20This%20ensures%20higher-quality%20inputs%20for%20more%0Aprecise%20segmentation.%20SEGSRNet%20combines%20advanced%20feature%20extraction%20and%0Aattention%20mechanisms%20with%20spatial%20processing%20to%20sharpen%20image%20details%2C%20which%20is%0Asignificant%20for%20accurate%20tool%20identification%20in%20medical%20images.%20Our%20proposed%0Amodel%20outperforms%20current%20models%20including%20Dice%2C%20IoU%2C%20PSNR%2C%20and%20SSIM%2C%20SEGSRNet%0Awhere%20it%20produces%20clearer%20and%20more%20accurate%20images%20for%20stereo%20endoscopic%0Asurgical%20imaging.%20SEGSRNet%20can%20provide%20image%20resolution%20and%20precise%0Asegmentation%20which%20can%20significantly%20enhance%20surgical%20accuracy%20and%20patient%20care%0Aoutcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13330v2&entry.124074799=Read"},
{"title": "CoCar NextGen: a Multi-Purpose Platform for Connected Autonomous Driving\n  Research", "author": "Marc Heinrich and Maximilian Zipfl and Marc Uecker and Sven Ochs and Martin Gontscharow and Tobias Fleck and Jens Doll and Philip Sch\u00f6rner and Christian Hubschneider and Marc Ren\u00e9 Zofka and Alexander Viehl and J. Marius Z\u00f6llner", "abstract": "  Real world testing is of vital importance to the success of automated\ndriving. While many players in the business design purpose build testing\nvehicles, we designed and build a modular platform that offers high flexibility\nfor any kind of scenario. CoCar NextGen is equipped with next generation\nhardware that addresses all future use cases. Its extensive, redundant sensor\nsetup allows to develop cross-domain data driven approaches that manage the\ntransfer to other sensor setups. Together with the possibility of being\ndeployed on public roads, this creates a unique research platform that supports\nthe road to automated driving on SAE Level 5.\n", "link": "http://arxiv.org/abs/2404.17550v1", "date": "2024-04-26", "relevancy": 1.8858, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4978}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4804}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4415}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CoCar%20NextGen%3A%20a%20Multi-Purpose%20Platform%20for%20Connected%20Autonomous%20Driving%0A%20%20Research&body=Title%3A%20CoCar%20NextGen%3A%20a%20Multi-Purpose%20Platform%20for%20Connected%20Autonomous%20Driving%0A%20%20Research%0AAuthor%3A%20Marc%20Heinrich%20and%20Maximilian%20Zipfl%20and%20Marc%20Uecker%20and%20Sven%20Ochs%20and%20Martin%20Gontscharow%20and%20Tobias%20Fleck%20and%20Jens%20Doll%20and%20Philip%20Sch%C3%B6rner%20and%20Christian%20Hubschneider%20and%20Marc%20Ren%C3%A9%20Zofka%20and%20Alexander%20Viehl%20and%20J.%20Marius%20Z%C3%B6llner%0AAbstract%3A%20%20%20Real%20world%20testing%20is%20of%20vital%20importance%20to%20the%20success%20of%20automated%0Adriving.%20While%20many%20players%20in%20the%20business%20design%20purpose%20build%20testing%0Avehicles%2C%20we%20designed%20and%20build%20a%20modular%20platform%20that%20offers%20high%20flexibility%0Afor%20any%20kind%20of%20scenario.%20CoCar%20NextGen%20is%20equipped%20with%20next%20generation%0Ahardware%20that%20addresses%20all%20future%20use%20cases.%20Its%20extensive%2C%20redundant%20sensor%0Asetup%20allows%20to%20develop%20cross-domain%20data%20driven%20approaches%20that%20manage%20the%0Atransfer%20to%20other%20sensor%20setups.%20Together%20with%20the%20possibility%20of%20being%0Adeployed%20on%20public%20roads%2C%20this%20creates%20a%20unique%20research%20platform%20that%20supports%0Athe%20road%20to%20automated%20driving%20on%20SAE%20Level%205.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17550v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoCar%20NextGen%3A%20a%20Multi-Purpose%20Platform%20for%20Connected%20Autonomous%20Driving%0A%20%20Research&entry.906535625=Marc%20Heinrich%20and%20Maximilian%20Zipfl%20and%20Marc%20Uecker%20and%20Sven%20Ochs%20and%20Martin%20Gontscharow%20and%20Tobias%20Fleck%20and%20Jens%20Doll%20and%20Philip%20Sch%C3%B6rner%20and%20Christian%20Hubschneider%20and%20Marc%20Ren%C3%A9%20Zofka%20and%20Alexander%20Viehl%20and%20J.%20Marius%20Z%C3%B6llner&entry.1292438233=%20%20Real%20world%20testing%20is%20of%20vital%20importance%20to%20the%20success%20of%20automated%0Adriving.%20While%20many%20players%20in%20the%20business%20design%20purpose%20build%20testing%0Avehicles%2C%20we%20designed%20and%20build%20a%20modular%20platform%20that%20offers%20high%20flexibility%0Afor%20any%20kind%20of%20scenario.%20CoCar%20NextGen%20is%20equipped%20with%20next%20generation%0Ahardware%20that%20addresses%20all%20future%20use%20cases.%20Its%20extensive%2C%20redundant%20sensor%0Asetup%20allows%20to%20develop%20cross-domain%20data%20driven%20approaches%20that%20manage%20the%0Atransfer%20to%20other%20sensor%20setups.%20Together%20with%20the%20possibility%20of%20being%0Adeployed%20on%20public%20roads%2C%20this%20creates%20a%20unique%20research%20platform%20that%20supports%0Athe%20road%20to%20automated%20driving%20on%20SAE%20Level%205.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17550v1&entry.124074799=Read"},
{"title": "A Deep Dive into Effects of Structural Bias on CMA-ES Performance along\n  Affine Trajectories", "author": "Niki van Stein and Sarah L. Thomson and Anna V. Kononova", "abstract": "  To guide the design of better iterative optimisation heuristics, it is\nimperative to understand how inherent structural biases within algorithm\ncomponents affect the performance on a wide variety of search landscapes. This\nstudy explores the impact of structural bias in the modular Covariance Matrix\nAdaptation Evolution Strategy (modCMA), focusing on the roles of various\nmodulars within the algorithm. Through an extensive investigation involving\n435,456 configurations of modCMA, we identified key modules that significantly\ninfluence structural bias of various classes. Our analysis utilized the\nDeep-BIAS toolbox for structural bias detection and classification,\ncomplemented by SHAP analysis for quantifying module contributions. The\nperformance of these configurations was tested on a sequence of\naffine-recombined functions, maintaining fixed optimum locations while\ngradually varying the landscape features. Our results demonstrate an interplay\nbetween module-induced structural bias and algorithm performance across\ndifferent landscape characteristics.\n", "link": "http://arxiv.org/abs/2404.17323v1", "date": "2024-04-26", "relevancy": 1.8751, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4929}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4738}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4541}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Deep%20Dive%20into%20Effects%20of%20Structural%20Bias%20on%20CMA-ES%20Performance%20along%0A%20%20Affine%20Trajectories&body=Title%3A%20A%20Deep%20Dive%20into%20Effects%20of%20Structural%20Bias%20on%20CMA-ES%20Performance%20along%0A%20%20Affine%20Trajectories%0AAuthor%3A%20Niki%20van%20Stein%20and%20Sarah%20L.%20Thomson%20and%20Anna%20V.%20Kononova%0AAbstract%3A%20%20%20To%20guide%20the%20design%20of%20better%20iterative%20optimisation%20heuristics%2C%20it%20is%0Aimperative%20to%20understand%20how%20inherent%20structural%20biases%20within%20algorithm%0Acomponents%20affect%20the%20performance%20on%20a%20wide%20variety%20of%20search%20landscapes.%20This%0Astudy%20explores%20the%20impact%20of%20structural%20bias%20in%20the%20modular%20Covariance%20Matrix%0AAdaptation%20Evolution%20Strategy%20%28modCMA%29%2C%20focusing%20on%20the%20roles%20of%20various%0Amodulars%20within%20the%20algorithm.%20Through%20an%20extensive%20investigation%20involving%0A435%2C456%20configurations%20of%20modCMA%2C%20we%20identified%20key%20modules%20that%20significantly%0Ainfluence%20structural%20bias%20of%20various%20classes.%20Our%20analysis%20utilized%20the%0ADeep-BIAS%20toolbox%20for%20structural%20bias%20detection%20and%20classification%2C%0Acomplemented%20by%20SHAP%20analysis%20for%20quantifying%20module%20contributions.%20The%0Aperformance%20of%20these%20configurations%20was%20tested%20on%20a%20sequence%20of%0Aaffine-recombined%20functions%2C%20maintaining%20fixed%20optimum%20locations%20while%0Agradually%20varying%20the%20landscape%20features.%20Our%20results%20demonstrate%20an%20interplay%0Abetween%20module-induced%20structural%20bias%20and%20algorithm%20performance%20across%0Adifferent%20landscape%20characteristics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17323v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Deep%20Dive%20into%20Effects%20of%20Structural%20Bias%20on%20CMA-ES%20Performance%20along%0A%20%20Affine%20Trajectories&entry.906535625=Niki%20van%20Stein%20and%20Sarah%20L.%20Thomson%20and%20Anna%20V.%20Kononova&entry.1292438233=%20%20To%20guide%20the%20design%20of%20better%20iterative%20optimisation%20heuristics%2C%20it%20is%0Aimperative%20to%20understand%20how%20inherent%20structural%20biases%20within%20algorithm%0Acomponents%20affect%20the%20performance%20on%20a%20wide%20variety%20of%20search%20landscapes.%20This%0Astudy%20explores%20the%20impact%20of%20structural%20bias%20in%20the%20modular%20Covariance%20Matrix%0AAdaptation%20Evolution%20Strategy%20%28modCMA%29%2C%20focusing%20on%20the%20roles%20of%20various%0Amodulars%20within%20the%20algorithm.%20Through%20an%20extensive%20investigation%20involving%0A435%2C456%20configurations%20of%20modCMA%2C%20we%20identified%20key%20modules%20that%20significantly%0Ainfluence%20structural%20bias%20of%20various%20classes.%20Our%20analysis%20utilized%20the%0ADeep-BIAS%20toolbox%20for%20structural%20bias%20detection%20and%20classification%2C%0Acomplemented%20by%20SHAP%20analysis%20for%20quantifying%20module%20contributions.%20The%0Aperformance%20of%20these%20configurations%20was%20tested%20on%20a%20sequence%20of%0Aaffine-recombined%20functions%2C%20maintaining%20fixed%20optimum%20locations%20while%0Agradually%20varying%20the%20landscape%20features.%20Our%20results%20demonstrate%20an%20interplay%0Abetween%20module-induced%20structural%20bias%20and%20algorithm%20performance%20across%0Adifferent%20landscape%20characteristics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17323v1&entry.124074799=Read"},
{"title": "Uniform Generalization Bounds on Data-Dependent Hypothesis Sets via\n  PAC-Bayesian Theory on Random Sets", "author": "Benjamin Dupuis and Paul Viallard and George Deligiannidis and Umut Simsekli", "abstract": "  We propose data-dependent uniform generalization bounds by approaching the\nproblem from a PAC-Bayesian perspective. We first apply the PAC-Bayesian\nframework on `random sets' in a rigorous way, where the training algorithm is\nassumed to output a data-dependent hypothesis set after observing the training\ndata. This approach allows us to prove data-dependent bounds, which can be\napplicable in numerous contexts. To highlight the power of our approach, we\nconsider two main applications. First, we propose a PAC-Bayesian formulation of\nthe recently developed fractal-dimension-based generalization bounds. The\nderived results are shown to be tighter and they unify the existing results\naround one simple proof technique. Second, we prove uniform bounds over the\ntrajectories of continuous Langevin dynamics and stochastic gradient Langevin\ndynamics. These results provide novel information about the generalization\nproperties of noisy algorithms.\n", "link": "http://arxiv.org/abs/2404.17442v1", "date": "2024-04-26", "relevancy": 1.8116, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4973}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.446}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.442}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Uniform%20Generalization%20Bounds%20on%20Data-Dependent%20Hypothesis%20Sets%20via%0A%20%20PAC-Bayesian%20Theory%20on%20Random%20Sets&body=Title%3A%20Uniform%20Generalization%20Bounds%20on%20Data-Dependent%20Hypothesis%20Sets%20via%0A%20%20PAC-Bayesian%20Theory%20on%20Random%20Sets%0AAuthor%3A%20Benjamin%20Dupuis%20and%20Paul%20Viallard%20and%20George%20Deligiannidis%20and%20Umut%20Simsekli%0AAbstract%3A%20%20%20We%20propose%20data-dependent%20uniform%20generalization%20bounds%20by%20approaching%20the%0Aproblem%20from%20a%20PAC-Bayesian%20perspective.%20We%20first%20apply%20the%20PAC-Bayesian%0Aframework%20on%20%60random%20sets%27%20in%20a%20rigorous%20way%2C%20where%20the%20training%20algorithm%20is%0Aassumed%20to%20output%20a%20data-dependent%20hypothesis%20set%20after%20observing%20the%20training%0Adata.%20This%20approach%20allows%20us%20to%20prove%20data-dependent%20bounds%2C%20which%20can%20be%0Aapplicable%20in%20numerous%20contexts.%20To%20highlight%20the%20power%20of%20our%20approach%2C%20we%0Aconsider%20two%20main%20applications.%20First%2C%20we%20propose%20a%20PAC-Bayesian%20formulation%20of%0Athe%20recently%20developed%20fractal-dimension-based%20generalization%20bounds.%20The%0Aderived%20results%20are%20shown%20to%20be%20tighter%20and%20they%20unify%20the%20existing%20results%0Aaround%20one%20simple%20proof%20technique.%20Second%2C%20we%20prove%20uniform%20bounds%20over%20the%0Atrajectories%20of%20continuous%20Langevin%20dynamics%20and%20stochastic%20gradient%20Langevin%0Adynamics.%20These%20results%20provide%20novel%20information%20about%20the%20generalization%0Aproperties%20of%20noisy%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17442v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uniform%20Generalization%20Bounds%20on%20Data-Dependent%20Hypothesis%20Sets%20via%0A%20%20PAC-Bayesian%20Theory%20on%20Random%20Sets&entry.906535625=Benjamin%20Dupuis%20and%20Paul%20Viallard%20and%20George%20Deligiannidis%20and%20Umut%20Simsekli&entry.1292438233=%20%20We%20propose%20data-dependent%20uniform%20generalization%20bounds%20by%20approaching%20the%0Aproblem%20from%20a%20PAC-Bayesian%20perspective.%20We%20first%20apply%20the%20PAC-Bayesian%0Aframework%20on%20%60random%20sets%27%20in%20a%20rigorous%20way%2C%20where%20the%20training%20algorithm%20is%0Aassumed%20to%20output%20a%20data-dependent%20hypothesis%20set%20after%20observing%20the%20training%0Adata.%20This%20approach%20allows%20us%20to%20prove%20data-dependent%20bounds%2C%20which%20can%20be%0Aapplicable%20in%20numerous%20contexts.%20To%20highlight%20the%20power%20of%20our%20approach%2C%20we%0Aconsider%20two%20main%20applications.%20First%2C%20we%20propose%20a%20PAC-Bayesian%20formulation%20of%0Athe%20recently%20developed%20fractal-dimension-based%20generalization%20bounds.%20The%0Aderived%20results%20are%20shown%20to%20be%20tighter%20and%20they%20unify%20the%20existing%20results%0Aaround%20one%20simple%20proof%20technique.%20Second%2C%20we%20prove%20uniform%20bounds%20over%20the%0Atrajectories%20of%20continuous%20Langevin%20dynamics%20and%20stochastic%20gradient%20Langevin%0Adynamics.%20These%20results%20provide%20novel%20information%20about%20the%20generalization%0Aproperties%20of%20noisy%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17442v1&entry.124074799=Read"},
{"title": "Multi-layer random features and the approximation power of neural\n  networks", "author": "Rustem Takhanov", "abstract": "  A neural architecture with randomly initialized weights, in the infinite\nwidth limit, is equivalent to a Gaussian Random Field whose covariance function\nis the so-called Neural Network Gaussian Process kernel (NNGP). We prove that a\nreproducing kernel Hilbert space (RKHS) defined by the NNGP contains only\nfunctions that can be approximated by the architecture. To achieve a certain\napproximation error the required number of neurons in each layer is defined by\nthe RKHS norm of the target function. Moreover, the approximation can be\nconstructed from a supervised dataset by a random multi-layer representation of\nan input vector, together with training of the last layer's weights.\n  For a 2-layer NN and a domain equal to an $n-1$-dimensional sphere in\n${\\mathbb R}^n$, we compare the number of neurons required by Barron's theorem\nand by the multi-layer features construction. We show that if eigenvalues of\nthe integral operator of the NNGP decay slower than $k^{-n-\\frac{2}{3}}$ where\n$k$ is an order of an eigenvalue, then our theorem guarantees a more succinct\nneural network approximation than Barron's theorem. We also make some\ncomputational experiments to verify our theoretical findings. Our experiments\nshow that realistic neural networks easily learn target functions even when\nboth theorems do not give any guarantees.\n", "link": "http://arxiv.org/abs/2404.17461v1", "date": "2024-04-26", "relevancy": 1.8103, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4898}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4455}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4448}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-layer%20random%20features%20and%20the%20approximation%20power%20of%20neural%0A%20%20networks&body=Title%3A%20Multi-layer%20random%20features%20and%20the%20approximation%20power%20of%20neural%0A%20%20networks%0AAuthor%3A%20Rustem%20Takhanov%0AAbstract%3A%20%20%20A%20neural%20architecture%20with%20randomly%20initialized%20weights%2C%20in%20the%20infinite%0Awidth%20limit%2C%20is%20equivalent%20to%20a%20Gaussian%20Random%20Field%20whose%20covariance%20function%0Ais%20the%20so-called%20Neural%20Network%20Gaussian%20Process%20kernel%20%28NNGP%29.%20We%20prove%20that%20a%0Areproducing%20kernel%20Hilbert%20space%20%28RKHS%29%20defined%20by%20the%20NNGP%20contains%20only%0Afunctions%20that%20can%20be%20approximated%20by%20the%20architecture.%20To%20achieve%20a%20certain%0Aapproximation%20error%20the%20required%20number%20of%20neurons%20in%20each%20layer%20is%20defined%20by%0Athe%20RKHS%20norm%20of%20the%20target%20function.%20Moreover%2C%20the%20approximation%20can%20be%0Aconstructed%20from%20a%20supervised%20dataset%20by%20a%20random%20multi-layer%20representation%20of%0Aan%20input%20vector%2C%20together%20with%20training%20of%20the%20last%20layer%27s%20weights.%0A%20%20For%20a%202-layer%20NN%20and%20a%20domain%20equal%20to%20an%20%24n-1%24-dimensional%20sphere%20in%0A%24%7B%5Cmathbb%20R%7D%5En%24%2C%20we%20compare%20the%20number%20of%20neurons%20required%20by%20Barron%27s%20theorem%0Aand%20by%20the%20multi-layer%20features%20construction.%20We%20show%20that%20if%20eigenvalues%20of%0Athe%20integral%20operator%20of%20the%20NNGP%20decay%20slower%20than%20%24k%5E%7B-n-%5Cfrac%7B2%7D%7B3%7D%7D%24%20where%0A%24k%24%20is%20an%20order%20of%20an%20eigenvalue%2C%20then%20our%20theorem%20guarantees%20a%20more%20succinct%0Aneural%20network%20approximation%20than%20Barron%27s%20theorem.%20We%20also%20make%20some%0Acomputational%20experiments%20to%20verify%20our%20theoretical%20findings.%20Our%20experiments%0Ashow%20that%20realistic%20neural%20networks%20easily%20learn%20target%20functions%20even%20when%0Aboth%20theorems%20do%20not%20give%20any%20guarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17461v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-layer%20random%20features%20and%20the%20approximation%20power%20of%20neural%0A%20%20networks&entry.906535625=Rustem%20Takhanov&entry.1292438233=%20%20A%20neural%20architecture%20with%20randomly%20initialized%20weights%2C%20in%20the%20infinite%0Awidth%20limit%2C%20is%20equivalent%20to%20a%20Gaussian%20Random%20Field%20whose%20covariance%20function%0Ais%20the%20so-called%20Neural%20Network%20Gaussian%20Process%20kernel%20%28NNGP%29.%20We%20prove%20that%20a%0Areproducing%20kernel%20Hilbert%20space%20%28RKHS%29%20defined%20by%20the%20NNGP%20contains%20only%0Afunctions%20that%20can%20be%20approximated%20by%20the%20architecture.%20To%20achieve%20a%20certain%0Aapproximation%20error%20the%20required%20number%20of%20neurons%20in%20each%20layer%20is%20defined%20by%0Athe%20RKHS%20norm%20of%20the%20target%20function.%20Moreover%2C%20the%20approximation%20can%20be%0Aconstructed%20from%20a%20supervised%20dataset%20by%20a%20random%20multi-layer%20representation%20of%0Aan%20input%20vector%2C%20together%20with%20training%20of%20the%20last%20layer%27s%20weights.%0A%20%20For%20a%202-layer%20NN%20and%20a%20domain%20equal%20to%20an%20%24n-1%24-dimensional%20sphere%20in%0A%24%7B%5Cmathbb%20R%7D%5En%24%2C%20we%20compare%20the%20number%20of%20neurons%20required%20by%20Barron%27s%20theorem%0Aand%20by%20the%20multi-layer%20features%20construction.%20We%20show%20that%20if%20eigenvalues%20of%0Athe%20integral%20operator%20of%20the%20NNGP%20decay%20slower%20than%20%24k%5E%7B-n-%5Cfrac%7B2%7D%7B3%7D%7D%24%20where%0A%24k%24%20is%20an%20order%20of%20an%20eigenvalue%2C%20then%20our%20theorem%20guarantees%20a%20more%20succinct%0Aneural%20network%20approximation%20than%20Barron%27s%20theorem.%20We%20also%20make%20some%0Acomputational%20experiments%20to%20verify%20our%20theoretical%20findings.%20Our%20experiments%0Ashow%20that%20realistic%20neural%20networks%20easily%20learn%20target%20functions%20even%20when%0Aboth%20theorems%20do%20not%20give%20any%20guarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17461v1&entry.124074799=Read"},
{"title": "DeepClean: Machine Unlearning on the Cheap by Resetting Privacy\n  Sensitive Weights using the Fisher Diagonal", "author": "Jiaeli Shi and Najah Ghalyan and Kostis Gourgoulias and John Buford and Sean Moran", "abstract": "  Machine learning models trained on sensitive or private data can\ninadvertently memorize and leak that information. Machine unlearning seeks to\nretroactively remove such details from model weights to protect privacy. We\ncontribute a lightweight unlearning algorithm that leverages the Fisher\nInformation Matrix (FIM) for selective forgetting. Prior work in this area\nrequires full retraining or large matrix inversions, which are computationally\nexpensive. Our key insight is that the diagonal elements of the FIM, which\nmeasure the sensitivity of log-likelihood to changes in weights, contain\nsufficient information for effective forgetting. Specifically, we compute the\nFIM diagonal over two subsets -- the data to retain and forget -- for all\ntrainable weights. This diagonal representation approximates the complete FIM\nwhile dramatically reducing computation. We then use it to selectively update\nweights to maximize forgetting of the sensitive subset while minimizing impact\non the retained subset. Experiments show that our algorithm can successfully\nforget any randomly selected subsets of training data across neural network\narchitectures. By leveraging the FIM diagonal, our approach provides an\ninterpretable, lightweight, and efficient solution for machine unlearning with\npractical privacy benefits.\n", "link": "http://arxiv.org/abs/2311.10448v2", "date": "2024-04-26", "relevancy": 1.8091, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4552}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4527}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4506}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DeepClean%3A%20Machine%20Unlearning%20on%20the%20Cheap%20by%20Resetting%20Privacy%0A%20%20Sensitive%20Weights%20using%20the%20Fisher%20Diagonal&body=Title%3A%20DeepClean%3A%20Machine%20Unlearning%20on%20the%20Cheap%20by%20Resetting%20Privacy%0A%20%20Sensitive%20Weights%20using%20the%20Fisher%20Diagonal%0AAuthor%3A%20Jiaeli%20Shi%20and%20Najah%20Ghalyan%20and%20Kostis%20Gourgoulias%20and%20John%20Buford%20and%20Sean%20Moran%0AAbstract%3A%20%20%20Machine%20learning%20models%20trained%20on%20sensitive%20or%20private%20data%20can%0Ainadvertently%20memorize%20and%20leak%20that%20information.%20Machine%20unlearning%20seeks%20to%0Aretroactively%20remove%20such%20details%20from%20model%20weights%20to%20protect%20privacy.%20We%0Acontribute%20a%20lightweight%20unlearning%20algorithm%20that%20leverages%20the%20Fisher%0AInformation%20Matrix%20%28FIM%29%20for%20selective%20forgetting.%20Prior%20work%20in%20this%20area%0Arequires%20full%20retraining%20or%20large%20matrix%20inversions%2C%20which%20are%20computationally%0Aexpensive.%20Our%20key%20insight%20is%20that%20the%20diagonal%20elements%20of%20the%20FIM%2C%20which%0Ameasure%20the%20sensitivity%20of%20log-likelihood%20to%20changes%20in%20weights%2C%20contain%0Asufficient%20information%20for%20effective%20forgetting.%20Specifically%2C%20we%20compute%20the%0AFIM%20diagonal%20over%20two%20subsets%20--%20the%20data%20to%20retain%20and%20forget%20--%20for%20all%0Atrainable%20weights.%20This%20diagonal%20representation%20approximates%20the%20complete%20FIM%0Awhile%20dramatically%20reducing%20computation.%20We%20then%20use%20it%20to%20selectively%20update%0Aweights%20to%20maximize%20forgetting%20of%20the%20sensitive%20subset%20while%20minimizing%20impact%0Aon%20the%20retained%20subset.%20Experiments%20show%20that%20our%20algorithm%20can%20successfully%0Aforget%20any%20randomly%20selected%20subsets%20of%20training%20data%20across%20neural%20network%0Aarchitectures.%20By%20leveraging%20the%20FIM%20diagonal%2C%20our%20approach%20provides%20an%0Ainterpretable%2C%20lightweight%2C%20and%20efficient%20solution%20for%20machine%20unlearning%20with%0Apractical%20privacy%20benefits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.10448v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepClean%3A%20Machine%20Unlearning%20on%20the%20Cheap%20by%20Resetting%20Privacy%0A%20%20Sensitive%20Weights%20using%20the%20Fisher%20Diagonal&entry.906535625=Jiaeli%20Shi%20and%20Najah%20Ghalyan%20and%20Kostis%20Gourgoulias%20and%20John%20Buford%20and%20Sean%20Moran&entry.1292438233=%20%20Machine%20learning%20models%20trained%20on%20sensitive%20or%20private%20data%20can%0Ainadvertently%20memorize%20and%20leak%20that%20information.%20Machine%20unlearning%20seeks%20to%0Aretroactively%20remove%20such%20details%20from%20model%20weights%20to%20protect%20privacy.%20We%0Acontribute%20a%20lightweight%20unlearning%20algorithm%20that%20leverages%20the%20Fisher%0AInformation%20Matrix%20%28FIM%29%20for%20selective%20forgetting.%20Prior%20work%20in%20this%20area%0Arequires%20full%20retraining%20or%20large%20matrix%20inversions%2C%20which%20are%20computationally%0Aexpensive.%20Our%20key%20insight%20is%20that%20the%20diagonal%20elements%20of%20the%20FIM%2C%20which%0Ameasure%20the%20sensitivity%20of%20log-likelihood%20to%20changes%20in%20weights%2C%20contain%0Asufficient%20information%20for%20effective%20forgetting.%20Specifically%2C%20we%20compute%20the%0AFIM%20diagonal%20over%20two%20subsets%20--%20the%20data%20to%20retain%20and%20forget%20--%20for%20all%0Atrainable%20weights.%20This%20diagonal%20representation%20approximates%20the%20complete%20FIM%0Awhile%20dramatically%20reducing%20computation.%20We%20then%20use%20it%20to%20selectively%20update%0Aweights%20to%20maximize%20forgetting%20of%20the%20sensitive%20subset%20while%20minimizing%20impact%0Aon%20the%20retained%20subset.%20Experiments%20show%20that%20our%20algorithm%20can%20successfully%0Aforget%20any%20randomly%20selected%20subsets%20of%20training%20data%20across%20neural%20network%0Aarchitectures.%20By%20leveraging%20the%20FIM%20diagonal%2C%20our%20approach%20provides%20an%0Ainterpretable%2C%20lightweight%2C%20and%20efficient%20solution%20for%20machine%20unlearning%20with%0Apractical%20privacy%20benefits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.10448v2&entry.124074799=Read"},
{"title": "Differentiable Pareto-Smoothed Weighting for High-Dimensional\n  Heterogeneous Treatment Effect Estimation", "author": "Yoichi Chikahara and Kansei Ushiyama", "abstract": "  There is a growing interest in estimating heterogeneous treatment effects\nacross individuals using their high-dimensional feature attributes. Achieving\nhigh performance in such high-dimensional heterogeneous treatment effect\nestimation is challenging because in this setup, it is usual that some features\ninduce sample selection bias while others do not but are predictive of\npotential outcomes. To avoid losing such predictive feature information,\nexisting methods learn separate feature representations using the inverse of\nprobability weighting (IPW). However, due to the numerically unstable IPW\nweights, they suffer from estimation bias under a finite sample setup. To\ndevelop a numerically robust estimator via weighted representation learning, we\npropose a differentiable Pareto-smoothed weighting framework that replaces\nextreme weight values in an end-to-end fashion. Experimental results show that\nby effectively correcting the weight values, our method outperforms the\nexisting ones, including traditional weighting schemes.\n", "link": "http://arxiv.org/abs/2404.17483v1", "date": "2024-04-26", "relevancy": 1.7972, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4591}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4515}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4386}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Differentiable%20Pareto-Smoothed%20Weighting%20for%20High-Dimensional%0A%20%20Heterogeneous%20Treatment%20Effect%20Estimation&body=Title%3A%20Differentiable%20Pareto-Smoothed%20Weighting%20for%20High-Dimensional%0A%20%20Heterogeneous%20Treatment%20Effect%20Estimation%0AAuthor%3A%20Yoichi%20Chikahara%20and%20Kansei%20Ushiyama%0AAbstract%3A%20%20%20There%20is%20a%20growing%20interest%20in%20estimating%20heterogeneous%20treatment%20effects%0Aacross%20individuals%20using%20their%20high-dimensional%20feature%20attributes.%20Achieving%0Ahigh%20performance%20in%20such%20high-dimensional%20heterogeneous%20treatment%20effect%0Aestimation%20is%20challenging%20because%20in%20this%20setup%2C%20it%20is%20usual%20that%20some%20features%0Ainduce%20sample%20selection%20bias%20while%20others%20do%20not%20but%20are%20predictive%20of%0Apotential%20outcomes.%20To%20avoid%20losing%20such%20predictive%20feature%20information%2C%0Aexisting%20methods%20learn%20separate%20feature%20representations%20using%20the%20inverse%20of%0Aprobability%20weighting%20%28IPW%29.%20However%2C%20due%20to%20the%20numerically%20unstable%20IPW%0Aweights%2C%20they%20suffer%20from%20estimation%20bias%20under%20a%20finite%20sample%20setup.%20To%0Adevelop%20a%20numerically%20robust%20estimator%20via%20weighted%20representation%20learning%2C%20we%0Apropose%20a%20differentiable%20Pareto-smoothed%20weighting%20framework%20that%20replaces%0Aextreme%20weight%20values%20in%20an%20end-to-end%20fashion.%20Experimental%20results%20show%20that%0Aby%20effectively%20correcting%20the%20weight%20values%2C%20our%20method%20outperforms%20the%0Aexisting%20ones%2C%20including%20traditional%20weighting%20schemes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17483v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20Pareto-Smoothed%20Weighting%20for%20High-Dimensional%0A%20%20Heterogeneous%20Treatment%20Effect%20Estimation&entry.906535625=Yoichi%20Chikahara%20and%20Kansei%20Ushiyama&entry.1292438233=%20%20There%20is%20a%20growing%20interest%20in%20estimating%20heterogeneous%20treatment%20effects%0Aacross%20individuals%20using%20their%20high-dimensional%20feature%20attributes.%20Achieving%0Ahigh%20performance%20in%20such%20high-dimensional%20heterogeneous%20treatment%20effect%0Aestimation%20is%20challenging%20because%20in%20this%20setup%2C%20it%20is%20usual%20that%20some%20features%0Ainduce%20sample%20selection%20bias%20while%20others%20do%20not%20but%20are%20predictive%20of%0Apotential%20outcomes.%20To%20avoid%20losing%20such%20predictive%20feature%20information%2C%0Aexisting%20methods%20learn%20separate%20feature%20representations%20using%20the%20inverse%20of%0Aprobability%20weighting%20%28IPW%29.%20However%2C%20due%20to%20the%20numerically%20unstable%20IPW%0Aweights%2C%20they%20suffer%20from%20estimation%20bias%20under%20a%20finite%20sample%20setup.%20To%0Adevelop%20a%20numerically%20robust%20estimator%20via%20weighted%20representation%20learning%2C%20we%0Apropose%20a%20differentiable%20Pareto-smoothed%20weighting%20framework%20that%20replaces%0Aextreme%20weight%20values%20in%20an%20end-to-end%20fashion.%20Experimental%20results%20show%20that%0Aby%20effectively%20correcting%20the%20weight%20values%2C%20our%20method%20outperforms%20the%0Aexisting%20ones%2C%20including%20traditional%20weighting%20schemes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17483v1&entry.124074799=Read"},
{"title": "Ag2Manip: Learning Novel Manipulation Skills with Agent-Agnostic Visual\n  and Action Representations", "author": "Puhao Li and Tengyu Liu and Yuyang Li and Muzhi Han and Haoran Geng and Shu Wang and Yixin Zhu and Song-Chun Zhu and Siyuan Huang", "abstract": "  Autonomous robotic systems capable of learning novel manipulation tasks are\npoised to transform industries from manufacturing to service automation.\nHowever, modern methods (e.g., VIP and R3M) still face significant hurdles,\nnotably the domain gap among robotic embodiments and the sparsity of successful\ntask executions within specific action spaces, resulting in misaligned and\nambiguous task representations. We introduce Ag2Manip (Agent-Agnostic\nrepresentations for Manipulation), a framework aimed at surmounting these\nchallenges through two key innovations: a novel agent-agnostic visual\nrepresentation derived from human manipulation videos, with the specifics of\nembodiments obscured to enhance generalizability; and an agent-agnostic action\nrepresentation abstracting a robot's kinematics to a universal agent proxy,\nemphasizing crucial interactions between end-effector and object. Ag2Manip's\nempirical validation across simulated benchmarks like FrankaKitchen, ManiSkill,\nand PartManip shows a 325% increase in performance, achieved without\ndomain-specific demonstrations. Ablation studies underline the essential\ncontributions of the visual and action representations to this success.\nExtending our evaluations to the real world, Ag2Manip significantly improves\nimitation learning success rates from 50% to 77.5%, demonstrating its\neffectiveness and generalizability across both simulated and physical\nenvironments.\n", "link": "http://arxiv.org/abs/2404.17521v1", "date": "2024-04-26", "relevancy": 1.7895, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6819}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5982}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5617}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Ag2Manip%3A%20Learning%20Novel%20Manipulation%20Skills%20with%20Agent-Agnostic%20Visual%0A%20%20and%20Action%20Representations&body=Title%3A%20Ag2Manip%3A%20Learning%20Novel%20Manipulation%20Skills%20with%20Agent-Agnostic%20Visual%0A%20%20and%20Action%20Representations%0AAuthor%3A%20Puhao%20Li%20and%20Tengyu%20Liu%20and%20Yuyang%20Li%20and%20Muzhi%20Han%20and%20Haoran%20Geng%20and%20Shu%20Wang%20and%20Yixin%20Zhu%20and%20Song-Chun%20Zhu%20and%20Siyuan%20Huang%0AAbstract%3A%20%20%20Autonomous%20robotic%20systems%20capable%20of%20learning%20novel%20manipulation%20tasks%20are%0Apoised%20to%20transform%20industries%20from%20manufacturing%20to%20service%20automation.%0AHowever%2C%20modern%20methods%20%28e.g.%2C%20VIP%20and%20R3M%29%20still%20face%20significant%20hurdles%2C%0Anotably%20the%20domain%20gap%20among%20robotic%20embodiments%20and%20the%20sparsity%20of%20successful%0Atask%20executions%20within%20specific%20action%20spaces%2C%20resulting%20in%20misaligned%20and%0Aambiguous%20task%20representations.%20We%20introduce%20Ag2Manip%20%28Agent-Agnostic%0Arepresentations%20for%20Manipulation%29%2C%20a%20framework%20aimed%20at%20surmounting%20these%0Achallenges%20through%20two%20key%20innovations%3A%20a%20novel%20agent-agnostic%20visual%0Arepresentation%20derived%20from%20human%20manipulation%20videos%2C%20with%20the%20specifics%20of%0Aembodiments%20obscured%20to%20enhance%20generalizability%3B%20and%20an%20agent-agnostic%20action%0Arepresentation%20abstracting%20a%20robot%27s%20kinematics%20to%20a%20universal%20agent%20proxy%2C%0Aemphasizing%20crucial%20interactions%20between%20end-effector%20and%20object.%20Ag2Manip%27s%0Aempirical%20validation%20across%20simulated%20benchmarks%20like%20FrankaKitchen%2C%20ManiSkill%2C%0Aand%20PartManip%20shows%20a%20325%25%20increase%20in%20performance%2C%20achieved%20without%0Adomain-specific%20demonstrations.%20Ablation%20studies%20underline%20the%20essential%0Acontributions%20of%20the%20visual%20and%20action%20representations%20to%20this%20success.%0AExtending%20our%20evaluations%20to%20the%20real%20world%2C%20Ag2Manip%20significantly%20improves%0Aimitation%20learning%20success%20rates%20from%2050%25%20to%2077.5%25%2C%20demonstrating%20its%0Aeffectiveness%20and%20generalizability%20across%20both%20simulated%20and%20physical%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17521v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ag2Manip%3A%20Learning%20Novel%20Manipulation%20Skills%20with%20Agent-Agnostic%20Visual%0A%20%20and%20Action%20Representations&entry.906535625=Puhao%20Li%20and%20Tengyu%20Liu%20and%20Yuyang%20Li%20and%20Muzhi%20Han%20and%20Haoran%20Geng%20and%20Shu%20Wang%20and%20Yixin%20Zhu%20and%20Song-Chun%20Zhu%20and%20Siyuan%20Huang&entry.1292438233=%20%20Autonomous%20robotic%20systems%20capable%20of%20learning%20novel%20manipulation%20tasks%20are%0Apoised%20to%20transform%20industries%20from%20manufacturing%20to%20service%20automation.%0AHowever%2C%20modern%20methods%20%28e.g.%2C%20VIP%20and%20R3M%29%20still%20face%20significant%20hurdles%2C%0Anotably%20the%20domain%20gap%20among%20robotic%20embodiments%20and%20the%20sparsity%20of%20successful%0Atask%20executions%20within%20specific%20action%20spaces%2C%20resulting%20in%20misaligned%20and%0Aambiguous%20task%20representations.%20We%20introduce%20Ag2Manip%20%28Agent-Agnostic%0Arepresentations%20for%20Manipulation%29%2C%20a%20framework%20aimed%20at%20surmounting%20these%0Achallenges%20through%20two%20key%20innovations%3A%20a%20novel%20agent-agnostic%20visual%0Arepresentation%20derived%20from%20human%20manipulation%20videos%2C%20with%20the%20specifics%20of%0Aembodiments%20obscured%20to%20enhance%20generalizability%3B%20and%20an%20agent-agnostic%20action%0Arepresentation%20abstracting%20a%20robot%27s%20kinematics%20to%20a%20universal%20agent%20proxy%2C%0Aemphasizing%20crucial%20interactions%20between%20end-effector%20and%20object.%20Ag2Manip%27s%0Aempirical%20validation%20across%20simulated%20benchmarks%20like%20FrankaKitchen%2C%20ManiSkill%2C%0Aand%20PartManip%20shows%20a%20325%25%20increase%20in%20performance%2C%20achieved%20without%0Adomain-specific%20demonstrations.%20Ablation%20studies%20underline%20the%20essential%0Acontributions%20of%20the%20visual%20and%20action%20representations%20to%20this%20success.%0AExtending%20our%20evaluations%20to%20the%20real%20world%2C%20Ag2Manip%20significantly%20improves%0Aimitation%20learning%20success%20rates%20from%2050%25%20to%2077.5%25%2C%20demonstrating%20its%0Aeffectiveness%20and%20generalizability%20across%20both%20simulated%20and%20physical%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17521v1&entry.124074799=Read"},
{"title": "Adversarial Estimation of Riesz Representers", "author": "Victor Chernozhukov and Whitney Newey and Rahul Singh and Vasilis Syrgkanis", "abstract": "  Many causal parameters are linear functionals of an underlying regression.\nThe Riesz representer is a key component in the asymptotic variance of a\nsemiparametrically estimated linear functional. We propose an adversarial\nframework to estimate the Riesz representer using general function spaces. We\nprove a nonasymptotic mean square rate in terms of an abstract quantity called\nthe critical radius, then specialize it for neural networks, random forests,\nand reproducing kernel Hilbert spaces as leading cases. Our estimators are\nhighly compatible with targeted and debiased machine learning with sample\nsplitting; our guarantees directly verify general conditions for inference that\nallow mis-specification. We also use our guarantees to prove inference without\nsample splitting, based on stability or complexity. Our estimators achieve\nnominal coverage in highly nonlinear simulations where some previous methods\nbreak down. They shed new light on the heterogeneous effects of matching\ngrants.\n", "link": "http://arxiv.org/abs/2101.00009v3", "date": "2024-04-26", "relevancy": 1.7833, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5112}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4387}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4268}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Estimation%20of%20Riesz%20Representers&body=Title%3A%20Adversarial%20Estimation%20of%20Riesz%20Representers%0AAuthor%3A%20Victor%20Chernozhukov%20and%20Whitney%20Newey%20and%20Rahul%20Singh%20and%20Vasilis%20Syrgkanis%0AAbstract%3A%20%20%20Many%20causal%20parameters%20are%20linear%20functionals%20of%20an%20underlying%20regression.%0AThe%20Riesz%20representer%20is%20a%20key%20component%20in%20the%20asymptotic%20variance%20of%20a%0Asemiparametrically%20estimated%20linear%20functional.%20We%20propose%20an%20adversarial%0Aframework%20to%20estimate%20the%20Riesz%20representer%20using%20general%20function%20spaces.%20We%0Aprove%20a%20nonasymptotic%20mean%20square%20rate%20in%20terms%20of%20an%20abstract%20quantity%20called%0Athe%20critical%20radius%2C%20then%20specialize%20it%20for%20neural%20networks%2C%20random%20forests%2C%0Aand%20reproducing%20kernel%20Hilbert%20spaces%20as%20leading%20cases.%20Our%20estimators%20are%0Ahighly%20compatible%20with%20targeted%20and%20debiased%20machine%20learning%20with%20sample%0Asplitting%3B%20our%20guarantees%20directly%20verify%20general%20conditions%20for%20inference%20that%0Aallow%20mis-specification.%20We%20also%20use%20our%20guarantees%20to%20prove%20inference%20without%0Asample%20splitting%2C%20based%20on%20stability%20or%20complexity.%20Our%20estimators%20achieve%0Anominal%20coverage%20in%20highly%20nonlinear%20simulations%20where%20some%20previous%20methods%0Abreak%20down.%20They%20shed%20new%20light%20on%20the%20heterogeneous%20effects%20of%20matching%0Agrants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2101.00009v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Estimation%20of%20Riesz%20Representers&entry.906535625=Victor%20Chernozhukov%20and%20Whitney%20Newey%20and%20Rahul%20Singh%20and%20Vasilis%20Syrgkanis&entry.1292438233=%20%20Many%20causal%20parameters%20are%20linear%20functionals%20of%20an%20underlying%20regression.%0AThe%20Riesz%20representer%20is%20a%20key%20component%20in%20the%20asymptotic%20variance%20of%20a%0Asemiparametrically%20estimated%20linear%20functional.%20We%20propose%20an%20adversarial%0Aframework%20to%20estimate%20the%20Riesz%20representer%20using%20general%20function%20spaces.%20We%0Aprove%20a%20nonasymptotic%20mean%20square%20rate%20in%20terms%20of%20an%20abstract%20quantity%20called%0Athe%20critical%20radius%2C%20then%20specialize%20it%20for%20neural%20networks%2C%20random%20forests%2C%0Aand%20reproducing%20kernel%20Hilbert%20spaces%20as%20leading%20cases.%20Our%20estimators%20are%0Ahighly%20compatible%20with%20targeted%20and%20debiased%20machine%20learning%20with%20sample%0Asplitting%3B%20our%20guarantees%20directly%20verify%20general%20conditions%20for%20inference%20that%0Aallow%20mis-specification.%20We%20also%20use%20our%20guarantees%20to%20prove%20inference%20without%0Asample%20splitting%2C%20based%20on%20stability%20or%20complexity.%20Our%20estimators%20achieve%0Anominal%20coverage%20in%20highly%20nonlinear%20simulations%20where%20some%20previous%20methods%0Abreak%20down.%20They%20shed%20new%20light%20on%20the%20heterogeneous%20effects%20of%20matching%0Agrants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2101.00009v3&entry.124074799=Read"},
{"title": "Multi-view Image Prompted Multi-view Diffusion for Improved 3D\n  Generation", "author": "Seungwook Kim and Yichun Shi and Kejie Li and Minsu Cho and Peng Wang", "abstract": "  Using image as prompts for 3D generation demonstrate particularly strong\nperformances compared to using text prompts alone, for images provide a more\nintuitive guidance for the 3D generation process. In this work, we delve into\nthe potential of using multiple image prompts, instead of a single image\nprompt, for 3D generation. Specifically, we build on ImageDream, a novel\nimage-prompt multi-view diffusion model, to support multi-view images as the\ninput prompt. Our method, dubbed MultiImageDream, reveals that transitioning\nfrom a single-image prompt to multiple-image prompts enhances the performance\nof multi-view and 3D object generation according to various quantitative\nevaluation metrics and qualitative assessments. This advancement is achieved\nwithout the necessity of fine-tuning the pre-trained ImageDream multi-view\ndiffusion model.\n", "link": "http://arxiv.org/abs/2404.17419v1", "date": "2024-04-26", "relevancy": 1.7828, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.617}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6046}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.581}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-view%20Image%20Prompted%20Multi-view%20Diffusion%20for%20Improved%203D%0A%20%20Generation&body=Title%3A%20Multi-view%20Image%20Prompted%20Multi-view%20Diffusion%20for%20Improved%203D%0A%20%20Generation%0AAuthor%3A%20Seungwook%20Kim%20and%20Yichun%20Shi%20and%20Kejie%20Li%20and%20Minsu%20Cho%20and%20Peng%20Wang%0AAbstract%3A%20%20%20Using%20image%20as%20prompts%20for%203D%20generation%20demonstrate%20particularly%20strong%0Aperformances%20compared%20to%20using%20text%20prompts%20alone%2C%20for%20images%20provide%20a%20more%0Aintuitive%20guidance%20for%20the%203D%20generation%20process.%20In%20this%20work%2C%20we%20delve%20into%0Athe%20potential%20of%20using%20multiple%20image%20prompts%2C%20instead%20of%20a%20single%20image%0Aprompt%2C%20for%203D%20generation.%20Specifically%2C%20we%20build%20on%20ImageDream%2C%20a%20novel%0Aimage-prompt%20multi-view%20diffusion%20model%2C%20to%20support%20multi-view%20images%20as%20the%0Ainput%20prompt.%20Our%20method%2C%20dubbed%20MultiImageDream%2C%20reveals%20that%20transitioning%0Afrom%20a%20single-image%20prompt%20to%20multiple-image%20prompts%20enhances%20the%20performance%0Aof%20multi-view%20and%203D%20object%20generation%20according%20to%20various%20quantitative%0Aevaluation%20metrics%20and%20qualitative%20assessments.%20This%20advancement%20is%20achieved%0Awithout%20the%20necessity%20of%20fine-tuning%20the%20pre-trained%20ImageDream%20multi-view%0Adiffusion%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17419v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-view%20Image%20Prompted%20Multi-view%20Diffusion%20for%20Improved%203D%0A%20%20Generation&entry.906535625=Seungwook%20Kim%20and%20Yichun%20Shi%20and%20Kejie%20Li%20and%20Minsu%20Cho%20and%20Peng%20Wang&entry.1292438233=%20%20Using%20image%20as%20prompts%20for%203D%20generation%20demonstrate%20particularly%20strong%0Aperformances%20compared%20to%20using%20text%20prompts%20alone%2C%20for%20images%20provide%20a%20more%0Aintuitive%20guidance%20for%20the%203D%20generation%20process.%20In%20this%20work%2C%20we%20delve%20into%0Athe%20potential%20of%20using%20multiple%20image%20prompts%2C%20instead%20of%20a%20single%20image%0Aprompt%2C%20for%203D%20generation.%20Specifically%2C%20we%20build%20on%20ImageDream%2C%20a%20novel%0Aimage-prompt%20multi-view%20diffusion%20model%2C%20to%20support%20multi-view%20images%20as%20the%0Ainput%20prompt.%20Our%20method%2C%20dubbed%20MultiImageDream%2C%20reveals%20that%20transitioning%0Afrom%20a%20single-image%20prompt%20to%20multiple-image%20prompts%20enhances%20the%20performance%0Aof%20multi-view%20and%203D%20object%20generation%20according%20to%20various%20quantitative%0Aevaluation%20metrics%20and%20qualitative%20assessments.%20This%20advancement%20is%20achieved%0Awithout%20the%20necessity%20of%20fine-tuning%20the%20pre-trained%20ImageDream%20multi-view%0Adiffusion%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17419v1&entry.124074799=Read"},
{"title": "MV-VTON: Multi-View Virtual Try-On with Diffusion Models", "author": "Haoyu Wang and Zhilu Zhang and Donglin Di and Shiliang Zhang and Wangmeng Zuo", "abstract": "  The goal of image-based virtual try-on is to generate an image of the target\nperson naturally wearing the given clothing. However, most existing methods\nsolely focus on the frontal try-on using the frontal clothing. When the views\nof the clothing and person are significantly inconsistent, particularly when\nthe person's view is non-frontal, the results are unsatisfactory. To address\nthis challenge, we introduce Multi-View Virtual Try-ON (MV-VTON), which aims to\nreconstruct the dressing results of a person from multiple views using the\ngiven clothes. On the one hand, given that single-view clothes provide\ninsufficient information for MV-VTON, we instead employ two images, i.e., the\nfrontal and back views of the clothing, to encompass the complete view as much\nas possible. On the other hand, the diffusion models that have demonstrated\nsuperior abilities are adopted to perform our MV-VTON. In particular, we\npropose a view-adaptive selection method where hard-selection and\nsoft-selection are applied to the global and local clothing feature extraction,\nrespectively. This ensures that the clothing features are roughly fit to the\nperson's view. Subsequently, we suggest a joint attention block to align and\nfuse clothing features with person features. Additionally, we collect a MV-VTON\ndataset, i.e., Multi-View Garment (MVG), in which each person has multiple\nphotos with diverse views and poses. Experiments show that the proposed method\nnot only achieves state-of-the-art results on MV-VTON task using our MVG\ndataset, but also has superiority on frontal-view virtual try-on task using\nVITON-HD and DressCode datasets. Codes and datasets will be publicly released\nat https://github.com/hywang2002/MV-VTON .\n", "link": "http://arxiv.org/abs/2404.17364v1", "date": "2024-04-26", "relevancy": 1.7793, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6706}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5719}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5686}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MV-VTON%3A%20Multi-View%20Virtual%20Try-On%20with%20Diffusion%20Models&body=Title%3A%20MV-VTON%3A%20Multi-View%20Virtual%20Try-On%20with%20Diffusion%20Models%0AAuthor%3A%20Haoyu%20Wang%20and%20Zhilu%20Zhang%20and%20Donglin%20Di%20and%20Shiliang%20Zhang%20and%20Wangmeng%20Zuo%0AAbstract%3A%20%20%20The%20goal%20of%20image-based%20virtual%20try-on%20is%20to%20generate%20an%20image%20of%20the%20target%0Aperson%20naturally%20wearing%20the%20given%20clothing.%20However%2C%20most%20existing%20methods%0Asolely%20focus%20on%20the%20frontal%20try-on%20using%20the%20frontal%20clothing.%20When%20the%20views%0Aof%20the%20clothing%20and%20person%20are%20significantly%20inconsistent%2C%20particularly%20when%0Athe%20person%27s%20view%20is%20non-frontal%2C%20the%20results%20are%20unsatisfactory.%20To%20address%0Athis%20challenge%2C%20we%20introduce%20Multi-View%20Virtual%20Try-ON%20%28MV-VTON%29%2C%20which%20aims%20to%0Areconstruct%20the%20dressing%20results%20of%20a%20person%20from%20multiple%20views%20using%20the%0Agiven%20clothes.%20On%20the%20one%20hand%2C%20given%20that%20single-view%20clothes%20provide%0Ainsufficient%20information%20for%20MV-VTON%2C%20we%20instead%20employ%20two%20images%2C%20i.e.%2C%20the%0Afrontal%20and%20back%20views%20of%20the%20clothing%2C%20to%20encompass%20the%20complete%20view%20as%20much%0Aas%20possible.%20On%20the%20other%20hand%2C%20the%20diffusion%20models%20that%20have%20demonstrated%0Asuperior%20abilities%20are%20adopted%20to%20perform%20our%20MV-VTON.%20In%20particular%2C%20we%0Apropose%20a%20view-adaptive%20selection%20method%20where%20hard-selection%20and%0Asoft-selection%20are%20applied%20to%20the%20global%20and%20local%20clothing%20feature%20extraction%2C%0Arespectively.%20This%20ensures%20that%20the%20clothing%20features%20are%20roughly%20fit%20to%20the%0Aperson%27s%20view.%20Subsequently%2C%20we%20suggest%20a%20joint%20attention%20block%20to%20align%20and%0Afuse%20clothing%20features%20with%20person%20features.%20Additionally%2C%20we%20collect%20a%20MV-VTON%0Adataset%2C%20i.e.%2C%20Multi-View%20Garment%20%28MVG%29%2C%20in%20which%20each%20person%20has%20multiple%0Aphotos%20with%20diverse%20views%20and%20poses.%20Experiments%20show%20that%20the%20proposed%20method%0Anot%20only%20achieves%20state-of-the-art%20results%20on%20MV-VTON%20task%20using%20our%20MVG%0Adataset%2C%20but%20also%20has%20superiority%20on%20frontal-view%20virtual%20try-on%20task%20using%0AVITON-HD%20and%20DressCode%20datasets.%20Codes%20and%20datasets%20will%20be%20publicly%20released%0Aat%20https%3A//github.com/hywang2002/MV-VTON%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17364v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MV-VTON%3A%20Multi-View%20Virtual%20Try-On%20with%20Diffusion%20Models&entry.906535625=Haoyu%20Wang%20and%20Zhilu%20Zhang%20and%20Donglin%20Di%20and%20Shiliang%20Zhang%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20The%20goal%20of%20image-based%20virtual%20try-on%20is%20to%20generate%20an%20image%20of%20the%20target%0Aperson%20naturally%20wearing%20the%20given%20clothing.%20However%2C%20most%20existing%20methods%0Asolely%20focus%20on%20the%20frontal%20try-on%20using%20the%20frontal%20clothing.%20When%20the%20views%0Aof%20the%20clothing%20and%20person%20are%20significantly%20inconsistent%2C%20particularly%20when%0Athe%20person%27s%20view%20is%20non-frontal%2C%20the%20results%20are%20unsatisfactory.%20To%20address%0Athis%20challenge%2C%20we%20introduce%20Multi-View%20Virtual%20Try-ON%20%28MV-VTON%29%2C%20which%20aims%20to%0Areconstruct%20the%20dressing%20results%20of%20a%20person%20from%20multiple%20views%20using%20the%0Agiven%20clothes.%20On%20the%20one%20hand%2C%20given%20that%20single-view%20clothes%20provide%0Ainsufficient%20information%20for%20MV-VTON%2C%20we%20instead%20employ%20two%20images%2C%20i.e.%2C%20the%0Afrontal%20and%20back%20views%20of%20the%20clothing%2C%20to%20encompass%20the%20complete%20view%20as%20much%0Aas%20possible.%20On%20the%20other%20hand%2C%20the%20diffusion%20models%20that%20have%20demonstrated%0Asuperior%20abilities%20are%20adopted%20to%20perform%20our%20MV-VTON.%20In%20particular%2C%20we%0Apropose%20a%20view-adaptive%20selection%20method%20where%20hard-selection%20and%0Asoft-selection%20are%20applied%20to%20the%20global%20and%20local%20clothing%20feature%20extraction%2C%0Arespectively.%20This%20ensures%20that%20the%20clothing%20features%20are%20roughly%20fit%20to%20the%0Aperson%27s%20view.%20Subsequently%2C%20we%20suggest%20a%20joint%20attention%20block%20to%20align%20and%0Afuse%20clothing%20features%20with%20person%20features.%20Additionally%2C%20we%20collect%20a%20MV-VTON%0Adataset%2C%20i.e.%2C%20Multi-View%20Garment%20%28MVG%29%2C%20in%20which%20each%20person%20has%20multiple%0Aphotos%20with%20diverse%20views%20and%20poses.%20Experiments%20show%20that%20the%20proposed%20method%0Anot%20only%20achieves%20state-of-the-art%20results%20on%20MV-VTON%20task%20using%20our%20MVG%0Adataset%2C%20but%20also%20has%20superiority%20on%20frontal-view%20virtual%20try-on%20task%20using%0AVITON-HD%20and%20DressCode%20datasets.%20Codes%20and%20datasets%20will%20be%20publicly%20released%0Aat%20https%3A//github.com/hywang2002/MV-VTON%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17364v1&entry.124074799=Read"},
{"title": "\"ChatGPT Is Here to Help, Not to Replace Anybody\" -- An Evaluation of\n  Students' Opinions On Integrating ChatGPT In CS Courses", "author": "Bruno Pereira Cipriano and Pedro Alves", "abstract": "  Large Language Models (LLMs) like GPT and Bard are capable of producing code\nbased on textual descriptions, with remarkable efficacy. Such technology will\nhave profound implications for computing education, raising concerns about\ncheating, excessive dependence, and a decline in computational thinking skills,\namong others. There has been extensive research on how teachers should handle\nthis challenge but it is also important to understand how students feel about\nthis paradigm shift. In this research, 52 first-year CS students were surveyed\nin order to assess their views on technologies with code-generation\ncapabilities, both from academic and professional perspectives. Our findings\nindicate that while students generally favor the academic use of GPT, they\ndon't over rely on it, only mildly asking for its help. Although most students\nbenefit from GPT, some struggle to use it effectively, urging the need for\nspecific GPT training. Opinions on GPT's impact on their professional lives\nvary, but there is a consensus on its importance in academic practice.\n", "link": "http://arxiv.org/abs/2404.17443v1", "date": "2024-04-26", "relevancy": 1.7768, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4723}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4267}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4178}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20%22ChatGPT%20Is%20Here%20to%20Help%2C%20Not%20to%20Replace%20Anybody%22%20--%20An%20Evaluation%20of%0A%20%20Students%27%20Opinions%20On%20Integrating%20ChatGPT%20In%20CS%20Courses&body=Title%3A%20%22ChatGPT%20Is%20Here%20to%20Help%2C%20Not%20to%20Replace%20Anybody%22%20--%20An%20Evaluation%20of%0A%20%20Students%27%20Opinions%20On%20Integrating%20ChatGPT%20In%20CS%20Courses%0AAuthor%3A%20Bruno%20Pereira%20Cipriano%20and%20Pedro%20Alves%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20like%20GPT%20and%20Bard%20are%20capable%20of%20producing%20code%0Abased%20on%20textual%20descriptions%2C%20with%20remarkable%20efficacy.%20Such%20technology%20will%0Ahave%20profound%20implications%20for%20computing%20education%2C%20raising%20concerns%20about%0Acheating%2C%20excessive%20dependence%2C%20and%20a%20decline%20in%20computational%20thinking%20skills%2C%0Aamong%20others.%20There%20has%20been%20extensive%20research%20on%20how%20teachers%20should%20handle%0Athis%20challenge%20but%20it%20is%20also%20important%20to%20understand%20how%20students%20feel%20about%0Athis%20paradigm%20shift.%20In%20this%20research%2C%2052%20first-year%20CS%20students%20were%20surveyed%0Ain%20order%20to%20assess%20their%20views%20on%20technologies%20with%20code-generation%0Acapabilities%2C%20both%20from%20academic%20and%20professional%20perspectives.%20Our%20findings%0Aindicate%20that%20while%20students%20generally%20favor%20the%20academic%20use%20of%20GPT%2C%20they%0Adon%27t%20over%20rely%20on%20it%2C%20only%20mildly%20asking%20for%20its%20help.%20Although%20most%20students%0Abenefit%20from%20GPT%2C%20some%20struggle%20to%20use%20it%20effectively%2C%20urging%20the%20need%20for%0Aspecific%20GPT%20training.%20Opinions%20on%20GPT%27s%20impact%20on%20their%20professional%20lives%0Avary%2C%20but%20there%20is%20a%20consensus%20on%20its%20importance%20in%20academic%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17443v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22ChatGPT%20Is%20Here%20to%20Help%2C%20Not%20to%20Replace%20Anybody%22%20--%20An%20Evaluation%20of%0A%20%20Students%27%20Opinions%20On%20Integrating%20ChatGPT%20In%20CS%20Courses&entry.906535625=Bruno%20Pereira%20Cipriano%20and%20Pedro%20Alves&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20like%20GPT%20and%20Bard%20are%20capable%20of%20producing%20code%0Abased%20on%20textual%20descriptions%2C%20with%20remarkable%20efficacy.%20Such%20technology%20will%0Ahave%20profound%20implications%20for%20computing%20education%2C%20raising%20concerns%20about%0Acheating%2C%20excessive%20dependence%2C%20and%20a%20decline%20in%20computational%20thinking%20skills%2C%0Aamong%20others.%20There%20has%20been%20extensive%20research%20on%20how%20teachers%20should%20handle%0Athis%20challenge%20but%20it%20is%20also%20important%20to%20understand%20how%20students%20feel%20about%0Athis%20paradigm%20shift.%20In%20this%20research%2C%2052%20first-year%20CS%20students%20were%20surveyed%0Ain%20order%20to%20assess%20their%20views%20on%20technologies%20with%20code-generation%0Acapabilities%2C%20both%20from%20academic%20and%20professional%20perspectives.%20Our%20findings%0Aindicate%20that%20while%20students%20generally%20favor%20the%20academic%20use%20of%20GPT%2C%20they%0Adon%27t%20over%20rely%20on%20it%2C%20only%20mildly%20asking%20for%20its%20help.%20Although%20most%20students%0Abenefit%20from%20GPT%2C%20some%20struggle%20to%20use%20it%20effectively%2C%20urging%20the%20need%20for%0Aspecific%20GPT%20training.%20Opinions%20on%20GPT%27s%20impact%20on%20their%20professional%20lives%0Avary%2C%20but%20there%20is%20a%20consensus%20on%20its%20importance%20in%20academic%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17443v1&entry.124074799=Read"},
{"title": "Low Cost Machine Vision for Insect Classification", "author": "Danja Brandt and Martin Tschaikner and Teodor Chiaburu and Henning Schmidt and Ilona Schrimpf and Alexandra Stadel and Ingeborg E. Beckers and Frank Hau\u00dfer", "abstract": "  Preserving the number and diversity of insects is one of our society's most\nimportant goals in the area of environmental sustainability. A prerequisite for\nthis is a systematic and up-scaled monitoring in order to detect correlations\nand identify countermeasures. Therefore, automatized monitoring using live\ntraps is important, but so far there is no system that provides image data of\nsufficient detailed information for entomological classification.\n  In this work, we present an imaging method as part of a multisensor system\ndeveloped as a low-cost, scalable, open-source system that is adaptable to\nclassical trap types. The image quality meets the requirements needed for\nclassification in the taxonomic tree. Therefore, illumination and resolution\nhave been optimized and motion artefacts have been suppressed. The system is\nevaluated exemplarily on a dataset consisting of 16 insect species of the same\nas well as different genus, family and order. We demonstrate that standard\nCNN-architectures like ResNet50 (pretrained on iNaturalist data) or MobileNet\nperform very well for the prediction task after re-training. Smaller custom\nmade CNNs also lead to promising results. Classification accuracy of $>96\\%$\nhas been achieved. Moreover, it was proved that image cropping of insects is\nnecessary for classification of species with high inter-class similarity.\n", "link": "http://arxiv.org/abs/2404.17488v1", "date": "2024-04-26", "relevancy": 1.7767, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.45}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4418}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4392}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Low%20Cost%20Machine%20Vision%20for%20Insect%20Classification&body=Title%3A%20Low%20Cost%20Machine%20Vision%20for%20Insect%20Classification%0AAuthor%3A%20Danja%20Brandt%20and%20Martin%20Tschaikner%20and%20Teodor%20Chiaburu%20and%20Henning%20Schmidt%20and%20Ilona%20Schrimpf%20and%20Alexandra%20Stadel%20and%20Ingeborg%20E.%20Beckers%20and%20Frank%20Hau%C3%9Fer%0AAbstract%3A%20%20%20Preserving%20the%20number%20and%20diversity%20of%20insects%20is%20one%20of%20our%20society%27s%20most%0Aimportant%20goals%20in%20the%20area%20of%20environmental%20sustainability.%20A%20prerequisite%20for%0Athis%20is%20a%20systematic%20and%20up-scaled%20monitoring%20in%20order%20to%20detect%20correlations%0Aand%20identify%20countermeasures.%20Therefore%2C%20automatized%20monitoring%20using%20live%0Atraps%20is%20important%2C%20but%20so%20far%20there%20is%20no%20system%20that%20provides%20image%20data%20of%0Asufficient%20detailed%20information%20for%20entomological%20classification.%0A%20%20In%20this%20work%2C%20we%20present%20an%20imaging%20method%20as%20part%20of%20a%20multisensor%20system%0Adeveloped%20as%20a%20low-cost%2C%20scalable%2C%20open-source%20system%20that%20is%20adaptable%20to%0Aclassical%20trap%20types.%20The%20image%20quality%20meets%20the%20requirements%20needed%20for%0Aclassification%20in%20the%20taxonomic%20tree.%20Therefore%2C%20illumination%20and%20resolution%0Ahave%20been%20optimized%20and%20motion%20artefacts%20have%20been%20suppressed.%20The%20system%20is%0Aevaluated%20exemplarily%20on%20a%20dataset%20consisting%20of%2016%20insect%20species%20of%20the%20same%0Aas%20well%20as%20different%20genus%2C%20family%20and%20order.%20We%20demonstrate%20that%20standard%0ACNN-architectures%20like%20ResNet50%20%28pretrained%20on%20iNaturalist%20data%29%20or%20MobileNet%0Aperform%20very%20well%20for%20the%20prediction%20task%20after%20re-training.%20Smaller%20custom%0Amade%20CNNs%20also%20lead%20to%20promising%20results.%20Classification%20accuracy%20of%20%24%3E96%5C%25%24%0Ahas%20been%20achieved.%20Moreover%2C%20it%20was%20proved%20that%20image%20cropping%20of%20insects%20is%0Anecessary%20for%20classification%20of%20species%20with%20high%20inter-class%20similarity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17488v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low%20Cost%20Machine%20Vision%20for%20Insect%20Classification&entry.906535625=Danja%20Brandt%20and%20Martin%20Tschaikner%20and%20Teodor%20Chiaburu%20and%20Henning%20Schmidt%20and%20Ilona%20Schrimpf%20and%20Alexandra%20Stadel%20and%20Ingeborg%20E.%20Beckers%20and%20Frank%20Hau%C3%9Fer&entry.1292438233=%20%20Preserving%20the%20number%20and%20diversity%20of%20insects%20is%20one%20of%20our%20society%27s%20most%0Aimportant%20goals%20in%20the%20area%20of%20environmental%20sustainability.%20A%20prerequisite%20for%0Athis%20is%20a%20systematic%20and%20up-scaled%20monitoring%20in%20order%20to%20detect%20correlations%0Aand%20identify%20countermeasures.%20Therefore%2C%20automatized%20monitoring%20using%20live%0Atraps%20is%20important%2C%20but%20so%20far%20there%20is%20no%20system%20that%20provides%20image%20data%20of%0Asufficient%20detailed%20information%20for%20entomological%20classification.%0A%20%20In%20this%20work%2C%20we%20present%20an%20imaging%20method%20as%20part%20of%20a%20multisensor%20system%0Adeveloped%20as%20a%20low-cost%2C%20scalable%2C%20open-source%20system%20that%20is%20adaptable%20to%0Aclassical%20trap%20types.%20The%20image%20quality%20meets%20the%20requirements%20needed%20for%0Aclassification%20in%20the%20taxonomic%20tree.%20Therefore%2C%20illumination%20and%20resolution%0Ahave%20been%20optimized%20and%20motion%20artefacts%20have%20been%20suppressed.%20The%20system%20is%0Aevaluated%20exemplarily%20on%20a%20dataset%20consisting%20of%2016%20insect%20species%20of%20the%20same%0Aas%20well%20as%20different%20genus%2C%20family%20and%20order.%20We%20demonstrate%20that%20standard%0ACNN-architectures%20like%20ResNet50%20%28pretrained%20on%20iNaturalist%20data%29%20or%20MobileNet%0Aperform%20very%20well%20for%20the%20prediction%20task%20after%20re-training.%20Smaller%20custom%0Amade%20CNNs%20also%20lead%20to%20promising%20results.%20Classification%20accuracy%20of%20%24%3E96%5C%25%24%0Ahas%20been%20achieved.%20Moreover%2C%20it%20was%20proved%20that%20image%20cropping%20of%20insects%20is%0Anecessary%20for%20classification%20of%20species%20with%20high%20inter-class%20similarity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17488v1&entry.124074799=Read"},
{"title": "A machine-learning approach to thunderstorm forecasting through\n  post-processing of simulation data", "author": "Kianusch Vahid Yousefnia and Tobias B\u00f6lle and Isabella Z\u00f6bisch and Thomas Gerz", "abstract": "  Thunderstorms pose a major hazard to society and economy, which calls for\nreliable thunderstorm forecasts. In this work, we introduce a Signature-based\nApproach of identifying Lightning Activity using MAchine learning (SALAMA), a\nfeedforward neural network model for identifying thunderstorm occurrence in\nnumerical weather prediction (NWP) data. The model is trained on\nconvection-resolving ensemble forecasts over Central Europe and lightning\nobservations. Given only a set of pixel-wise input parameters that are\nextracted from NWP data and related to thunderstorm development, SALAMA infers\nthe probability of thunderstorm occurrence in a reliably calibrated manner. For\nlead times up to eleven hours, we find a forecast skill superior to\nclassification based only on NWP reflectivity. Varying the spatiotemporal\ncriteria by which we associate lightning observations with NWP data, we show\nthat the time scale for skillful thunderstorm predictions increases linearly\nwith the spatial scale of the forecast.\n", "link": "http://arxiv.org/abs/2303.08736v3", "date": "2024-04-26", "relevancy": 1.7731, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4492}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.443}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4292}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20machine-learning%20approach%20to%20thunderstorm%20forecasting%20through%0A%20%20post-processing%20of%20simulation%20data&body=Title%3A%20A%20machine-learning%20approach%20to%20thunderstorm%20forecasting%20through%0A%20%20post-processing%20of%20simulation%20data%0AAuthor%3A%20Kianusch%20Vahid%20Yousefnia%20and%20Tobias%20B%C3%B6lle%20and%20Isabella%20Z%C3%B6bisch%20and%20Thomas%20Gerz%0AAbstract%3A%20%20%20Thunderstorms%20pose%20a%20major%20hazard%20to%20society%20and%20economy%2C%20which%20calls%20for%0Areliable%20thunderstorm%20forecasts.%20In%20this%20work%2C%20we%20introduce%20a%20Signature-based%0AApproach%20of%20identifying%20Lightning%20Activity%20using%20MAchine%20learning%20%28SALAMA%29%2C%20a%0Afeedforward%20neural%20network%20model%20for%20identifying%20thunderstorm%20occurrence%20in%0Anumerical%20weather%20prediction%20%28NWP%29%20data.%20The%20model%20is%20trained%20on%0Aconvection-resolving%20ensemble%20forecasts%20over%20Central%20Europe%20and%20lightning%0Aobservations.%20Given%20only%20a%20set%20of%20pixel-wise%20input%20parameters%20that%20are%0Aextracted%20from%20NWP%20data%20and%20related%20to%20thunderstorm%20development%2C%20SALAMA%20infers%0Athe%20probability%20of%20thunderstorm%20occurrence%20in%20a%20reliably%20calibrated%20manner.%20For%0Alead%20times%20up%20to%20eleven%20hours%2C%20we%20find%20a%20forecast%20skill%20superior%20to%0Aclassification%20based%20only%20on%20NWP%20reflectivity.%20Varying%20the%20spatiotemporal%0Acriteria%20by%20which%20we%20associate%20lightning%20observations%20with%20NWP%20data%2C%20we%20show%0Athat%20the%20time%20scale%20for%20skillful%20thunderstorm%20predictions%20increases%20linearly%0Awith%20the%20spatial%20scale%20of%20the%20forecast.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.08736v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20machine-learning%20approach%20to%20thunderstorm%20forecasting%20through%0A%20%20post-processing%20of%20simulation%20data&entry.906535625=Kianusch%20Vahid%20Yousefnia%20and%20Tobias%20B%C3%B6lle%20and%20Isabella%20Z%C3%B6bisch%20and%20Thomas%20Gerz&entry.1292438233=%20%20Thunderstorms%20pose%20a%20major%20hazard%20to%20society%20and%20economy%2C%20which%20calls%20for%0Areliable%20thunderstorm%20forecasts.%20In%20this%20work%2C%20we%20introduce%20a%20Signature-based%0AApproach%20of%20identifying%20Lightning%20Activity%20using%20MAchine%20learning%20%28SALAMA%29%2C%20a%0Afeedforward%20neural%20network%20model%20for%20identifying%20thunderstorm%20occurrence%20in%0Anumerical%20weather%20prediction%20%28NWP%29%20data.%20The%20model%20is%20trained%20on%0Aconvection-resolving%20ensemble%20forecasts%20over%20Central%20Europe%20and%20lightning%0Aobservations.%20Given%20only%20a%20set%20of%20pixel-wise%20input%20parameters%20that%20are%0Aextracted%20from%20NWP%20data%20and%20related%20to%20thunderstorm%20development%2C%20SALAMA%20infers%0Athe%20probability%20of%20thunderstorm%20occurrence%20in%20a%20reliably%20calibrated%20manner.%20For%0Alead%20times%20up%20to%20eleven%20hours%2C%20we%20find%20a%20forecast%20skill%20superior%20to%0Aclassification%20based%20only%20on%20NWP%20reflectivity.%20Varying%20the%20spatiotemporal%0Acriteria%20by%20which%20we%20associate%20lightning%20observations%20with%20NWP%20data%2C%20we%20show%0Athat%20the%20time%20scale%20for%20skillful%20thunderstorm%20predictions%20increases%20linearly%0Awith%20the%20spatial%20scale%20of%20the%20forecast.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.08736v3&entry.124074799=Read"},
{"title": "Part-Guided 3D RL for Sim2Real Articulated Object Manipulation", "author": "Pengwei Xie and Rui Chen and Siang Chen and Yuzhe Qin and Fanbo Xiang and Tianyu Sun and Jing Xu and Guijin Wang and Hao Su", "abstract": "  Manipulating unseen articulated objects through visual feedback is a critical\nbut challenging task for real robots. Existing learning-based solutions mainly\nfocus on visual affordance learning or other pre-trained visual models to guide\nmanipulation policies, which face challenges for novel instances in real-world\nscenarios. In this paper, we propose a novel part-guided 3D RL framework, which\ncan learn to manipulate articulated objects without demonstrations. We combine\nthe strengths of 2D segmentation and 3D RL to improve the efficiency of RL\npolicy training. To improve the stability of the policy on real robots, we\ndesign a Frame-consistent Uncertainty-aware Sampling (FUS) strategy to get a\ncondensed and hierarchical 3D representation. In addition, a single versatile\nRL policy can be trained on multiple articulated object manipulation tasks\nsimultaneously in simulation and shows great generalizability to novel\ncategories and instances. Experimental results demonstrate the effectiveness of\nour framework in both simulation and real-world settings. Our code is available\nat\nhttps://github.com/THU-VCLab/Part-Guided-3D-RL-for-Sim2Real-Articulated-Object-Manipulation.\n", "link": "http://arxiv.org/abs/2404.17302v1", "date": "2024-04-26", "relevancy": 1.7718, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6238}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6085}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5701}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Part-Guided%203D%20RL%20for%20Sim2Real%20Articulated%20Object%20Manipulation&body=Title%3A%20Part-Guided%203D%20RL%20for%20Sim2Real%20Articulated%20Object%20Manipulation%0AAuthor%3A%20Pengwei%20Xie%20and%20Rui%20Chen%20and%20Siang%20Chen%20and%20Yuzhe%20Qin%20and%20Fanbo%20Xiang%20and%20Tianyu%20Sun%20and%20Jing%20Xu%20and%20Guijin%20Wang%20and%20Hao%20Su%0AAbstract%3A%20%20%20Manipulating%20unseen%20articulated%20objects%20through%20visual%20feedback%20is%20a%20critical%0Abut%20challenging%20task%20for%20real%20robots.%20Existing%20learning-based%20solutions%20mainly%0Afocus%20on%20visual%20affordance%20learning%20or%20other%20pre-trained%20visual%20models%20to%20guide%0Amanipulation%20policies%2C%20which%20face%20challenges%20for%20novel%20instances%20in%20real-world%0Ascenarios.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20part-guided%203D%20RL%20framework%2C%20which%0Acan%20learn%20to%20manipulate%20articulated%20objects%20without%20demonstrations.%20We%20combine%0Athe%20strengths%20of%202D%20segmentation%20and%203D%20RL%20to%20improve%20the%20efficiency%20of%20RL%0Apolicy%20training.%20To%20improve%20the%20stability%20of%20the%20policy%20on%20real%20robots%2C%20we%0Adesign%20a%20Frame-consistent%20Uncertainty-aware%20Sampling%20%28FUS%29%20strategy%20to%20get%20a%0Acondensed%20and%20hierarchical%203D%20representation.%20In%20addition%2C%20a%20single%20versatile%0ARL%20policy%20can%20be%20trained%20on%20multiple%20articulated%20object%20manipulation%20tasks%0Asimultaneously%20in%20simulation%20and%20shows%20great%20generalizability%20to%20novel%0Acategories%20and%20instances.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%0Aour%20framework%20in%20both%20simulation%20and%20real-world%20settings.%20Our%20code%20is%20available%0Aat%0Ahttps%3A//github.com/THU-VCLab/Part-Guided-3D-RL-for-Sim2Real-Articulated-Object-Manipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17302v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Part-Guided%203D%20RL%20for%20Sim2Real%20Articulated%20Object%20Manipulation&entry.906535625=Pengwei%20Xie%20and%20Rui%20Chen%20and%20Siang%20Chen%20and%20Yuzhe%20Qin%20and%20Fanbo%20Xiang%20and%20Tianyu%20Sun%20and%20Jing%20Xu%20and%20Guijin%20Wang%20and%20Hao%20Su&entry.1292438233=%20%20Manipulating%20unseen%20articulated%20objects%20through%20visual%20feedback%20is%20a%20critical%0Abut%20challenging%20task%20for%20real%20robots.%20Existing%20learning-based%20solutions%20mainly%0Afocus%20on%20visual%20affordance%20learning%20or%20other%20pre-trained%20visual%20models%20to%20guide%0Amanipulation%20policies%2C%20which%20face%20challenges%20for%20novel%20instances%20in%20real-world%0Ascenarios.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20part-guided%203D%20RL%20framework%2C%20which%0Acan%20learn%20to%20manipulate%20articulated%20objects%20without%20demonstrations.%20We%20combine%0Athe%20strengths%20of%202D%20segmentation%20and%203D%20RL%20to%20improve%20the%20efficiency%20of%20RL%0Apolicy%20training.%20To%20improve%20the%20stability%20of%20the%20policy%20on%20real%20robots%2C%20we%0Adesign%20a%20Frame-consistent%20Uncertainty-aware%20Sampling%20%28FUS%29%20strategy%20to%20get%20a%0Acondensed%20and%20hierarchical%203D%20representation.%20In%20addition%2C%20a%20single%20versatile%0ARL%20policy%20can%20be%20trained%20on%20multiple%20articulated%20object%20manipulation%20tasks%0Asimultaneously%20in%20simulation%20and%20shows%20great%20generalizability%20to%20novel%0Acategories%20and%20instances.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%0Aour%20framework%20in%20both%20simulation%20and%20real-world%20settings.%20Our%20code%20is%20available%0Aat%0Ahttps%3A//github.com/THU-VCLab/Part-Guided-3D-RL-for-Sim2Real-Articulated-Object-Manipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17302v1&entry.124074799=Read"},
{"title": "Adversarial Consistency and the Uniqueness of the Adversarial Bayes\n  Classifier", "author": "Natalie S. Frank", "abstract": "  Adversarial training is a common technique for learning robust classifiers.\nPrior work showed that convex surrogate losses are not statistically consistent\nin the adversarial context -- or in other words, a minimizing sequence of the\nadversarial surrogate risk will not necessarily minimize the adversarial\nclassification error. We connect the consistency of adversarial surrogate\nlosses to properties of minimizers to the adversarial classification risk,\nknown as \\emph{adversarial Bayes classifiers}. Specifically, under reasonable\ndistributional assumptions, a convex loss is statistically consistent for\nadversarial learning iff the adversarial Bayes classifier satisfies a certain\nnotion of uniqueness.\n", "link": "http://arxiv.org/abs/2404.17358v1", "date": "2024-04-26", "relevancy": 1.7603, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.462}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4252}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4225}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Consistency%20and%20the%20Uniqueness%20of%20the%20Adversarial%20Bayes%0A%20%20Classifier&body=Title%3A%20Adversarial%20Consistency%20and%20the%20Uniqueness%20of%20the%20Adversarial%20Bayes%0A%20%20Classifier%0AAuthor%3A%20Natalie%20S.%20Frank%0AAbstract%3A%20%20%20Adversarial%20training%20is%20a%20common%20technique%20for%20learning%20robust%20classifiers.%0APrior%20work%20showed%20that%20convex%20surrogate%20losses%20are%20not%20statistically%20consistent%0Ain%20the%20adversarial%20context%20--%20or%20in%20other%20words%2C%20a%20minimizing%20sequence%20of%20the%0Aadversarial%20surrogate%20risk%20will%20not%20necessarily%20minimize%20the%20adversarial%0Aclassification%20error.%20We%20connect%20the%20consistency%20of%20adversarial%20surrogate%0Alosses%20to%20properties%20of%20minimizers%20to%20the%20adversarial%20classification%20risk%2C%0Aknown%20as%20%5Cemph%7Badversarial%20Bayes%20classifiers%7D.%20Specifically%2C%20under%20reasonable%0Adistributional%20assumptions%2C%20a%20convex%20loss%20is%20statistically%20consistent%20for%0Aadversarial%20learning%20iff%20the%20adversarial%20Bayes%20classifier%20satisfies%20a%20certain%0Anotion%20of%20uniqueness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17358v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Consistency%20and%20the%20Uniqueness%20of%20the%20Adversarial%20Bayes%0A%20%20Classifier&entry.906535625=Natalie%20S.%20Frank&entry.1292438233=%20%20Adversarial%20training%20is%20a%20common%20technique%20for%20learning%20robust%20classifiers.%0APrior%20work%20showed%20that%20convex%20surrogate%20losses%20are%20not%20statistically%20consistent%0Ain%20the%20adversarial%20context%20--%20or%20in%20other%20words%2C%20a%20minimizing%20sequence%20of%20the%0Aadversarial%20surrogate%20risk%20will%20not%20necessarily%20minimize%20the%20adversarial%0Aclassification%20error.%20We%20connect%20the%20consistency%20of%20adversarial%20surrogate%0Alosses%20to%20properties%20of%20minimizers%20to%20the%20adversarial%20classification%20risk%2C%0Aknown%20as%20%5Cemph%7Badversarial%20Bayes%20classifiers%7D.%20Specifically%2C%20under%20reasonable%0Adistributional%20assumptions%2C%20a%20convex%20loss%20is%20statistically%20consistent%20for%0Aadversarial%20learning%20iff%20the%20adversarial%20Bayes%20classifier%20satisfies%20a%20certain%0Anotion%20of%20uniqueness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17358v1&entry.124074799=Read"},
{"title": "MEIA: Towards Realistic Multimodal Interaction and Manipulation for\n  Embodied Robots", "author": "Yang Liu and Xinshuai Song and Kaixuan Jiang and Weixing Chen and Jingzhou Luo and Guanbin Li and Liang Lin", "abstract": "  With the surge in the development of large language models, embodied\nintelligence has attracted increasing attention. Nevertheless, prior works on\nembodied intelligence typically encode scene or historical memory in an\nunimodal manner, either visual or linguistic, which complicates the alignment\nof the model's action planning with embodied control. To overcome this\nlimitation, we introduce the Multimodal Embodied Interactive Agent (MEIA),\ncapable of translating high-level tasks expressed in natural language into a\nsequence of executable actions. Specifically, we propose a novel Multimodal\nEnvironment Memory (MEM) module, facilitating the integration of embodied\ncontrol with large models through the visual-language memory of scenes. This\ncapability enables MEIA to generate executable action plans based on diverse\nrequirements and the robot's capabilities. Furthermore, we construct an\nembodied question answering dataset based on a dynamic virtual cafe environment\nwith the help of the large language model. In this virtual environment, we\nconduct several experiments, utilizing multiple large models through zero-shot\nlearning, and carefully design scenarios for various situations. The\nexperimental results showcase the promising performance of our MEIA in various\nembodied interactive tasks.\n", "link": "http://arxiv.org/abs/2402.00290v2", "date": "2024-04-26", "relevancy": 1.7507, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6427}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6138}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5478}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MEIA%3A%20Towards%20Realistic%20Multimodal%20Interaction%20and%20Manipulation%20for%0A%20%20Embodied%20Robots&body=Title%3A%20MEIA%3A%20Towards%20Realistic%20Multimodal%20Interaction%20and%20Manipulation%20for%0A%20%20Embodied%20Robots%0AAuthor%3A%20Yang%20Liu%20and%20Xinshuai%20Song%20and%20Kaixuan%20Jiang%20and%20Weixing%20Chen%20and%20Jingzhou%20Luo%20and%20Guanbin%20Li%20and%20Liang%20Lin%0AAbstract%3A%20%20%20With%20the%20surge%20in%20the%20development%20of%20large%20language%20models%2C%20embodied%0Aintelligence%20has%20attracted%20increasing%20attention.%20Nevertheless%2C%20prior%20works%20on%0Aembodied%20intelligence%20typically%20encode%20scene%20or%20historical%20memory%20in%20an%0Aunimodal%20manner%2C%20either%20visual%20or%20linguistic%2C%20which%20complicates%20the%20alignment%0Aof%20the%20model%27s%20action%20planning%20with%20embodied%20control.%20To%20overcome%20this%0Alimitation%2C%20we%20introduce%20the%20Multimodal%20Embodied%20Interactive%20Agent%20%28MEIA%29%2C%0Acapable%20of%20translating%20high-level%20tasks%20expressed%20in%20natural%20language%20into%20a%0Asequence%20of%20executable%20actions.%20Specifically%2C%20we%20propose%20a%20novel%20Multimodal%0AEnvironment%20Memory%20%28MEM%29%20module%2C%20facilitating%20the%20integration%20of%20embodied%0Acontrol%20with%20large%20models%20through%20the%20visual-language%20memory%20of%20scenes.%20This%0Acapability%20enables%20MEIA%20to%20generate%20executable%20action%20plans%20based%20on%20diverse%0Arequirements%20and%20the%20robot%27s%20capabilities.%20Furthermore%2C%20we%20construct%20an%0Aembodied%20question%20answering%20dataset%20based%20on%20a%20dynamic%20virtual%20cafe%20environment%0Awith%20the%20help%20of%20the%20large%20language%20model.%20In%20this%20virtual%20environment%2C%20we%0Aconduct%20several%20experiments%2C%20utilizing%20multiple%20large%20models%20through%20zero-shot%0Alearning%2C%20and%20carefully%20design%20scenarios%20for%20various%20situations.%20The%0Aexperimental%20results%20showcase%20the%20promising%20performance%20of%20our%20MEIA%20in%20various%0Aembodied%20interactive%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00290v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEIA%3A%20Towards%20Realistic%20Multimodal%20Interaction%20and%20Manipulation%20for%0A%20%20Embodied%20Robots&entry.906535625=Yang%20Liu%20and%20Xinshuai%20Song%20and%20Kaixuan%20Jiang%20and%20Weixing%20Chen%20and%20Jingzhou%20Luo%20and%20Guanbin%20Li%20and%20Liang%20Lin&entry.1292438233=%20%20With%20the%20surge%20in%20the%20development%20of%20large%20language%20models%2C%20embodied%0Aintelligence%20has%20attracted%20increasing%20attention.%20Nevertheless%2C%20prior%20works%20on%0Aembodied%20intelligence%20typically%20encode%20scene%20or%20historical%20memory%20in%20an%0Aunimodal%20manner%2C%20either%20visual%20or%20linguistic%2C%20which%20complicates%20the%20alignment%0Aof%20the%20model%27s%20action%20planning%20with%20embodied%20control.%20To%20overcome%20this%0Alimitation%2C%20we%20introduce%20the%20Multimodal%20Embodied%20Interactive%20Agent%20%28MEIA%29%2C%0Acapable%20of%20translating%20high-level%20tasks%20expressed%20in%20natural%20language%20into%20a%0Asequence%20of%20executable%20actions.%20Specifically%2C%20we%20propose%20a%20novel%20Multimodal%0AEnvironment%20Memory%20%28MEM%29%20module%2C%20facilitating%20the%20integration%20of%20embodied%0Acontrol%20with%20large%20models%20through%20the%20visual-language%20memory%20of%20scenes.%20This%0Acapability%20enables%20MEIA%20to%20generate%20executable%20action%20plans%20based%20on%20diverse%0Arequirements%20and%20the%20robot%27s%20capabilities.%20Furthermore%2C%20we%20construct%20an%0Aembodied%20question%20answering%20dataset%20based%20on%20a%20dynamic%20virtual%20cafe%20environment%0Awith%20the%20help%20of%20the%20large%20language%20model.%20In%20this%20virtual%20environment%2C%20we%0Aconduct%20several%20experiments%2C%20utilizing%20multiple%20large%20models%20through%20zero-shot%0Alearning%2C%20and%20carefully%20design%20scenarios%20for%20various%20situations.%20The%0Aexperimental%20results%20showcase%20the%20promising%20performance%20of%20our%20MEIA%20in%20various%0Aembodied%20interactive%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00290v2&entry.124074799=Read"},
{"title": "Predicting Properties of Nodes via Community-Aware Features", "author": "Bogumi\u0142 Kami\u0144ski and Pawe\u0142 Pra\u0142at and Fran\u00e7ois Th\u00e9berge and Sebastian Zaj\u0105c", "abstract": "  This paper shows how information about the network's community structure can\nbe used to define node features with high predictive power for classification\ntasks. To do so, we define a family of community-aware node features and\ninvestigate their properties. Those features are designed to ensure that they\ncan be efficiently computed even for large graphs. We show that community-aware\nnode features contain information that cannot be completely recovered by\nclassical node features or node embeddings (both classical and structural) and\nbring value in node classification tasks. This is verified for various\nclassification tasks on synthetic and real-life networks.\n", "link": "http://arxiv.org/abs/2311.04730v2", "date": "2024-04-26", "relevancy": 1.7484, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4974}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4273}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4228}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Predicting%20Properties%20of%20Nodes%20via%20Community-Aware%20Features&body=Title%3A%20Predicting%20Properties%20of%20Nodes%20via%20Community-Aware%20Features%0AAuthor%3A%20Bogumi%C5%82%20Kami%C5%84ski%20and%20Pawe%C5%82%20Pra%C5%82at%20and%20Fran%C3%A7ois%20Th%C3%A9berge%20and%20Sebastian%20Zaj%C4%85c%0AAbstract%3A%20%20%20This%20paper%20shows%20how%20information%20about%20the%20network%27s%20community%20structure%20can%0Abe%20used%20to%20define%20node%20features%20with%20high%20predictive%20power%20for%20classification%0Atasks.%20To%20do%20so%2C%20we%20define%20a%20family%20of%20community-aware%20node%20features%20and%0Ainvestigate%20their%20properties.%20Those%20features%20are%20designed%20to%20ensure%20that%20they%0Acan%20be%20efficiently%20computed%20even%20for%20large%20graphs.%20We%20show%20that%20community-aware%0Anode%20features%20contain%20information%20that%20cannot%20be%20completely%20recovered%20by%0Aclassical%20node%20features%20or%20node%20embeddings%20%28both%20classical%20and%20structural%29%20and%0Abring%20value%20in%20node%20classification%20tasks.%20This%20is%20verified%20for%20various%0Aclassification%20tasks%20on%20synthetic%20and%20real-life%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04730v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Properties%20of%20Nodes%20via%20Community-Aware%20Features&entry.906535625=Bogumi%C5%82%20Kami%C5%84ski%20and%20Pawe%C5%82%20Pra%C5%82at%20and%20Fran%C3%A7ois%20Th%C3%A9berge%20and%20Sebastian%20Zaj%C4%85c&entry.1292438233=%20%20This%20paper%20shows%20how%20information%20about%20the%20network%27s%20community%20structure%20can%0Abe%20used%20to%20define%20node%20features%20with%20high%20predictive%20power%20for%20classification%0Atasks.%20To%20do%20so%2C%20we%20define%20a%20family%20of%20community-aware%20node%20features%20and%0Ainvestigate%20their%20properties.%20Those%20features%20are%20designed%20to%20ensure%20that%20they%0Acan%20be%20efficiently%20computed%20even%20for%20large%20graphs.%20We%20show%20that%20community-aware%0Anode%20features%20contain%20information%20that%20cannot%20be%20completely%20recovered%20by%0Aclassical%20node%20features%20or%20node%20embeddings%20%28both%20classical%20and%20structural%29%20and%0Abring%20value%20in%20node%20classification%20tasks.%20This%20is%20verified%20for%20various%0Aclassification%20tasks%20on%20synthetic%20and%20real-life%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04730v2&entry.124074799=Read"},
{"title": "A Continuous Relaxation for Discrete Bayesian Optimization", "author": "Richard Michael and Simon Bartels and Miguel Gonz\u00e1lez-Duque and Yevgen Zainchkovskyy and Jes Frellsen and S\u00f8ren Hauberg and Wouter Boomsma", "abstract": "  To optimize efficiently over discrete data and with only few available target\nobservations is a challenge in Bayesian optimization. We propose a continuous\nrelaxation of the objective function and show that inference and optimization\ncan be computationally tractable. We consider in particular the optimization\ndomain where very few observations and strict budgets exist; motivated by\noptimizing protein sequences for expensive to evaluate bio-chemical properties.\nThe advantages of our approach are two-fold: the problem is treated in the\ncontinuous setting, and available prior knowledge over sequences can be\nincorporated directly. More specifically, we utilize available and learned\ndistributions over the problem domain for a weighting of the Hellinger distance\nwhich yields a covariance function. We show that the resulting acquisition\nfunction can be optimized with both continuous or discrete optimization\nalgorithms and empirically assess our method on two bio-chemical sequence\noptimization tasks.\n", "link": "http://arxiv.org/abs/2404.17452v1", "date": "2024-04-26", "relevancy": 1.7354, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4803}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4254}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4237}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Continuous%20Relaxation%20for%20Discrete%20Bayesian%20Optimization&body=Title%3A%20A%20Continuous%20Relaxation%20for%20Discrete%20Bayesian%20Optimization%0AAuthor%3A%20Richard%20Michael%20and%20Simon%20Bartels%20and%20Miguel%20Gonz%C3%A1lez-Duque%20and%20Yevgen%20Zainchkovskyy%20and%20Jes%20Frellsen%20and%20S%C3%B8ren%20Hauberg%20and%20Wouter%20Boomsma%0AAbstract%3A%20%20%20To%20optimize%20efficiently%20over%20discrete%20data%20and%20with%20only%20few%20available%20target%0Aobservations%20is%20a%20challenge%20in%20Bayesian%20optimization.%20We%20propose%20a%20continuous%0Arelaxation%20of%20the%20objective%20function%20and%20show%20that%20inference%20and%20optimization%0Acan%20be%20computationally%20tractable.%20We%20consider%20in%20particular%20the%20optimization%0Adomain%20where%20very%20few%20observations%20and%20strict%20budgets%20exist%3B%20motivated%20by%0Aoptimizing%20protein%20sequences%20for%20expensive%20to%20evaluate%20bio-chemical%20properties.%0AThe%20advantages%20of%20our%20approach%20are%20two-fold%3A%20the%20problem%20is%20treated%20in%20the%0Acontinuous%20setting%2C%20and%20available%20prior%20knowledge%20over%20sequences%20can%20be%0Aincorporated%20directly.%20More%20specifically%2C%20we%20utilize%20available%20and%20learned%0Adistributions%20over%20the%20problem%20domain%20for%20a%20weighting%20of%20the%20Hellinger%20distance%0Awhich%20yields%20a%20covariance%20function.%20We%20show%20that%20the%20resulting%20acquisition%0Afunction%20can%20be%20optimized%20with%20both%20continuous%20or%20discrete%20optimization%0Aalgorithms%20and%20empirically%20assess%20our%20method%20on%20two%20bio-chemical%20sequence%0Aoptimization%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17452v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Continuous%20Relaxation%20for%20Discrete%20Bayesian%20Optimization&entry.906535625=Richard%20Michael%20and%20Simon%20Bartels%20and%20Miguel%20Gonz%C3%A1lez-Duque%20and%20Yevgen%20Zainchkovskyy%20and%20Jes%20Frellsen%20and%20S%C3%B8ren%20Hauberg%20and%20Wouter%20Boomsma&entry.1292438233=%20%20To%20optimize%20efficiently%20over%20discrete%20data%20and%20with%20only%20few%20available%20target%0Aobservations%20is%20a%20challenge%20in%20Bayesian%20optimization.%20We%20propose%20a%20continuous%0Arelaxation%20of%20the%20objective%20function%20and%20show%20that%20inference%20and%20optimization%0Acan%20be%20computationally%20tractable.%20We%20consider%20in%20particular%20the%20optimization%0Adomain%20where%20very%20few%20observations%20and%20strict%20budgets%20exist%3B%20motivated%20by%0Aoptimizing%20protein%20sequences%20for%20expensive%20to%20evaluate%20bio-chemical%20properties.%0AThe%20advantages%20of%20our%20approach%20are%20two-fold%3A%20the%20problem%20is%20treated%20in%20the%0Acontinuous%20setting%2C%20and%20available%20prior%20knowledge%20over%20sequences%20can%20be%0Aincorporated%20directly.%20More%20specifically%2C%20we%20utilize%20available%20and%20learned%0Adistributions%20over%20the%20problem%20domain%20for%20a%20weighting%20of%20the%20Hellinger%20distance%0Awhich%20yields%20a%20covariance%20function.%20We%20show%20that%20the%20resulting%20acquisition%0Afunction%20can%20be%20optimized%20with%20both%20continuous%20or%20discrete%20optimization%0Aalgorithms%20and%20empirically%20assess%20our%20method%20on%20two%20bio-chemical%20sequence%0Aoptimization%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17452v1&entry.124074799=Read"},
{"title": "Simultaneous Tri-Modal Medical Image Fusion and Super-Resolution using\n  Conditional Diffusion Model", "author": "Yushen Xu and Xiaosong Li and Yuchan Jie and Haishu Tan", "abstract": "  In clinical practice, tri-modal medical image fusion, compared to the\nexisting dual-modal technique, can provide a more comprehensive view of the\nlesions, aiding physicians in evaluating the disease's shape, location, and\nbiological activity. However, due to the limitations of imaging equipment and\nconsiderations for patient safety, the quality of medical images is usually\nlimited, leading to sub-optimal fusion performance, and affecting the depth of\nimage analysis by the physician. Thus, there is an urgent need for a technology\nthat can both enhance image resolution and integrate multi-modal information.\nAlthough current image processing methods can effectively address image fusion\nand super-resolution individually, solving both problems synchronously remains\nextremely challenging. In this paper, we propose TFS-Diff, a simultaneously\nrealize tri-modal medical image fusion and super-resolution model. Specially,\nTFS-Diff is based on the diffusion model generation of a random iterative\ndenoising process. We also develop a simple objective function and the proposed\nfusion super-resolution loss, effectively evaluates the uncertainty in the\nfusion and ensures the stability of the optimization process. And the channel\nattention module is proposed to effectively integrate key information from\ndifferent modalities for clinical diagnosis, avoiding information loss caused\nby multiple image processing. Extensive experiments on public Harvard datasets\nshow that TFS-Diff significantly surpass the existing state-of-the-art methods\nin both quantitative and visual evaluations. The source code will be available\nat GitHub.\n", "link": "http://arxiv.org/abs/2404.17357v1", "date": "2024-04-26", "relevancy": 1.7312, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.635}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.592}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5479}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Simultaneous%20Tri-Modal%20Medical%20Image%20Fusion%20and%20Super-Resolution%20using%0A%20%20Conditional%20Diffusion%20Model&body=Title%3A%20Simultaneous%20Tri-Modal%20Medical%20Image%20Fusion%20and%20Super-Resolution%20using%0A%20%20Conditional%20Diffusion%20Model%0AAuthor%3A%20Yushen%20Xu%20and%20Xiaosong%20Li%20and%20Yuchan%20Jie%20and%20Haishu%20Tan%0AAbstract%3A%20%20%20In%20clinical%20practice%2C%20tri-modal%20medical%20image%20fusion%2C%20compared%20to%20the%0Aexisting%20dual-modal%20technique%2C%20can%20provide%20a%20more%20comprehensive%20view%20of%20the%0Alesions%2C%20aiding%20physicians%20in%20evaluating%20the%20disease%27s%20shape%2C%20location%2C%20and%0Abiological%20activity.%20However%2C%20due%20to%20the%20limitations%20of%20imaging%20equipment%20and%0Aconsiderations%20for%20patient%20safety%2C%20the%20quality%20of%20medical%20images%20is%20usually%0Alimited%2C%20leading%20to%20sub-optimal%20fusion%20performance%2C%20and%20affecting%20the%20depth%20of%0Aimage%20analysis%20by%20the%20physician.%20Thus%2C%20there%20is%20an%20urgent%20need%20for%20a%20technology%0Athat%20can%20both%20enhance%20image%20resolution%20and%20integrate%20multi-modal%20information.%0AAlthough%20current%20image%20processing%20methods%20can%20effectively%20address%20image%20fusion%0Aand%20super-resolution%20individually%2C%20solving%20both%20problems%20synchronously%20remains%0Aextremely%20challenging.%20In%20this%20paper%2C%20we%20propose%20TFS-Diff%2C%20a%20simultaneously%0Arealize%20tri-modal%20medical%20image%20fusion%20and%20super-resolution%20model.%20Specially%2C%0ATFS-Diff%20is%20based%20on%20the%20diffusion%20model%20generation%20of%20a%20random%20iterative%0Adenoising%20process.%20We%20also%20develop%20a%20simple%20objective%20function%20and%20the%20proposed%0Afusion%20super-resolution%20loss%2C%20effectively%20evaluates%20the%20uncertainty%20in%20the%0Afusion%20and%20ensures%20the%20stability%20of%20the%20optimization%20process.%20And%20the%20channel%0Aattention%20module%20is%20proposed%20to%20effectively%20integrate%20key%20information%20from%0Adifferent%20modalities%20for%20clinical%20diagnosis%2C%20avoiding%20information%20loss%20caused%0Aby%20multiple%20image%20processing.%20Extensive%20experiments%20on%20public%20Harvard%20datasets%0Ashow%20that%20TFS-Diff%20significantly%20surpass%20the%20existing%20state-of-the-art%20methods%0Ain%20both%20quantitative%20and%20visual%20evaluations.%20The%20source%20code%20will%20be%20available%0Aat%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17357v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simultaneous%20Tri-Modal%20Medical%20Image%20Fusion%20and%20Super-Resolution%20using%0A%20%20Conditional%20Diffusion%20Model&entry.906535625=Yushen%20Xu%20and%20Xiaosong%20Li%20and%20Yuchan%20Jie%20and%20Haishu%20Tan&entry.1292438233=%20%20In%20clinical%20practice%2C%20tri-modal%20medical%20image%20fusion%2C%20compared%20to%20the%0Aexisting%20dual-modal%20technique%2C%20can%20provide%20a%20more%20comprehensive%20view%20of%20the%0Alesions%2C%20aiding%20physicians%20in%20evaluating%20the%20disease%27s%20shape%2C%20location%2C%20and%0Abiological%20activity.%20However%2C%20due%20to%20the%20limitations%20of%20imaging%20equipment%20and%0Aconsiderations%20for%20patient%20safety%2C%20the%20quality%20of%20medical%20images%20is%20usually%0Alimited%2C%20leading%20to%20sub-optimal%20fusion%20performance%2C%20and%20affecting%20the%20depth%20of%0Aimage%20analysis%20by%20the%20physician.%20Thus%2C%20there%20is%20an%20urgent%20need%20for%20a%20technology%0Athat%20can%20both%20enhance%20image%20resolution%20and%20integrate%20multi-modal%20information.%0AAlthough%20current%20image%20processing%20methods%20can%20effectively%20address%20image%20fusion%0Aand%20super-resolution%20individually%2C%20solving%20both%20problems%20synchronously%20remains%0Aextremely%20challenging.%20In%20this%20paper%2C%20we%20propose%20TFS-Diff%2C%20a%20simultaneously%0Arealize%20tri-modal%20medical%20image%20fusion%20and%20super-resolution%20model.%20Specially%2C%0ATFS-Diff%20is%20based%20on%20the%20diffusion%20model%20generation%20of%20a%20random%20iterative%0Adenoising%20process.%20We%20also%20develop%20a%20simple%20objective%20function%20and%20the%20proposed%0Afusion%20super-resolution%20loss%2C%20effectively%20evaluates%20the%20uncertainty%20in%20the%0Afusion%20and%20ensures%20the%20stability%20of%20the%20optimization%20process.%20And%20the%20channel%0Aattention%20module%20is%20proposed%20to%20effectively%20integrate%20key%20information%20from%0Adifferent%20modalities%20for%20clinical%20diagnosis%2C%20avoiding%20information%20loss%20caused%0Aby%20multiple%20image%20processing.%20Extensive%20experiments%20on%20public%20Harvard%20datasets%0Ashow%20that%20TFS-Diff%20significantly%20surpass%20the%20existing%20state-of-the-art%20methods%0Ain%20both%20quantitative%20and%20visual%20evaluations.%20The%20source%20code%20will%20be%20available%0Aat%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17357v1&entry.124074799=Read"},
{"title": "Structural-Based Uncertainty in Deep Learning Across Anatomical Scales:\n  Analysis in White Matter Lesion Segmentation", "author": "Nataliia Molchanova and Vatsal Raina and Andrey Malinin and Francesco La Rosa and Adrien Depeursinge and Mark Gales and Cristina Granziera and Henning Muller and Mara Graziani and Meritxell Bach Cuadra", "abstract": "  This paper explores uncertainty quantification (UQ) as an indicator of the\ntrustworthiness of automated deep-learning (DL) tools in the context of white\nmatter lesion (WML) segmentation from magnetic resonance imaging (MRI) scans of\nmultiple sclerosis (MS) patients. Our study focuses on two principal aspects of\nuncertainty in structured output segmentation tasks. Firstly, we postulate that\na good uncertainty measure should indicate predictions likely to be incorrect\nwith high uncertainty values. Second, we investigate the merit of quantifying\nuncertainty at different anatomical scales (voxel, lesion, or patient). We\nhypothesize that uncertainty at each scale is related to specific types of\nerrors. Our study aims to confirm this relationship by conducting separate\nanalyses for in-domain and out-of-domain settings. Our primary methodological\ncontributions are (i) the development of novel measures for quantifying\nuncertainty at lesion and patient scales, derived from structural prediction\ndiscrepancies, and (ii) the extension of an error retention curve analysis\nframework to facilitate the evaluation of UQ performance at both lesion and\npatient scales. The results from a multi-centric MRI dataset of 334 patients\ndemonstrate that our proposed measures more effectively capture model errors at\nthe lesion and patient scales compared to measures that average voxel-scale\nuncertainty values. We provide the UQ protocols code at\nhttps://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs.\n", "link": "http://arxiv.org/abs/2311.08931v2", "date": "2024-04-26", "relevancy": 1.7153, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.64}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5823}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4773}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Structural-Based%20Uncertainty%20in%20Deep%20Learning%20Across%20Anatomical%20Scales%3A%0A%20%20Analysis%20in%20White%20Matter%20Lesion%20Segmentation&body=Title%3A%20Structural-Based%20Uncertainty%20in%20Deep%20Learning%20Across%20Anatomical%20Scales%3A%0A%20%20Analysis%20in%20White%20Matter%20Lesion%20Segmentation%0AAuthor%3A%20Nataliia%20Molchanova%20and%20Vatsal%20Raina%20and%20Andrey%20Malinin%20and%20Francesco%20La%20Rosa%20and%20Adrien%20Depeursinge%20and%20Mark%20Gales%20and%20Cristina%20Granziera%20and%20Henning%20Muller%20and%20Mara%20Graziani%20and%20Meritxell%20Bach%20Cuadra%0AAbstract%3A%20%20%20This%20paper%20explores%20uncertainty%20quantification%20%28UQ%29%20as%20an%20indicator%20of%20the%0Atrustworthiness%20of%20automated%20deep-learning%20%28DL%29%20tools%20in%20the%20context%20of%20white%0Amatter%20lesion%20%28WML%29%20segmentation%20from%20magnetic%20resonance%20imaging%20%28MRI%29%20scans%20of%0Amultiple%20sclerosis%20%28MS%29%20patients.%20Our%20study%20focuses%20on%20two%20principal%20aspects%20of%0Auncertainty%20in%20structured%20output%20segmentation%20tasks.%20Firstly%2C%20we%20postulate%20that%0Aa%20good%20uncertainty%20measure%20should%20indicate%20predictions%20likely%20to%20be%20incorrect%0Awith%20high%20uncertainty%20values.%20Second%2C%20we%20investigate%20the%20merit%20of%20quantifying%0Auncertainty%20at%20different%20anatomical%20scales%20%28voxel%2C%20lesion%2C%20or%20patient%29.%20We%0Ahypothesize%20that%20uncertainty%20at%20each%20scale%20is%20related%20to%20specific%20types%20of%0Aerrors.%20Our%20study%20aims%20to%20confirm%20this%20relationship%20by%20conducting%20separate%0Aanalyses%20for%20in-domain%20and%20out-of-domain%20settings.%20Our%20primary%20methodological%0Acontributions%20are%20%28i%29%20the%20development%20of%20novel%20measures%20for%20quantifying%0Auncertainty%20at%20lesion%20and%20patient%20scales%2C%20derived%20from%20structural%20prediction%0Adiscrepancies%2C%20and%20%28ii%29%20the%20extension%20of%20an%20error%20retention%20curve%20analysis%0Aframework%20to%20facilitate%20the%20evaluation%20of%20UQ%20performance%20at%20both%20lesion%20and%0Apatient%20scales.%20The%20results%20from%20a%20multi-centric%20MRI%20dataset%20of%20334%20patients%0Ademonstrate%20that%20our%20proposed%20measures%20more%20effectively%20capture%20model%20errors%20at%0Athe%20lesion%20and%20patient%20scales%20compared%20to%20measures%20that%20average%20voxel-scale%0Auncertainty%20values.%20We%20provide%20the%20UQ%20protocols%20code%20at%0Ahttps%3A//github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.08931v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structural-Based%20Uncertainty%20in%20Deep%20Learning%20Across%20Anatomical%20Scales%3A%0A%20%20Analysis%20in%20White%20Matter%20Lesion%20Segmentation&entry.906535625=Nataliia%20Molchanova%20and%20Vatsal%20Raina%20and%20Andrey%20Malinin%20and%20Francesco%20La%20Rosa%20and%20Adrien%20Depeursinge%20and%20Mark%20Gales%20and%20Cristina%20Granziera%20and%20Henning%20Muller%20and%20Mara%20Graziani%20and%20Meritxell%20Bach%20Cuadra&entry.1292438233=%20%20This%20paper%20explores%20uncertainty%20quantification%20%28UQ%29%20as%20an%20indicator%20of%20the%0Atrustworthiness%20of%20automated%20deep-learning%20%28DL%29%20tools%20in%20the%20context%20of%20white%0Amatter%20lesion%20%28WML%29%20segmentation%20from%20magnetic%20resonance%20imaging%20%28MRI%29%20scans%20of%0Amultiple%20sclerosis%20%28MS%29%20patients.%20Our%20study%20focuses%20on%20two%20principal%20aspects%20of%0Auncertainty%20in%20structured%20output%20segmentation%20tasks.%20Firstly%2C%20we%20postulate%20that%0Aa%20good%20uncertainty%20measure%20should%20indicate%20predictions%20likely%20to%20be%20incorrect%0Awith%20high%20uncertainty%20values.%20Second%2C%20we%20investigate%20the%20merit%20of%20quantifying%0Auncertainty%20at%20different%20anatomical%20scales%20%28voxel%2C%20lesion%2C%20or%20patient%29.%20We%0Ahypothesize%20that%20uncertainty%20at%20each%20scale%20is%20related%20to%20specific%20types%20of%0Aerrors.%20Our%20study%20aims%20to%20confirm%20this%20relationship%20by%20conducting%20separate%0Aanalyses%20for%20in-domain%20and%20out-of-domain%20settings.%20Our%20primary%20methodological%0Acontributions%20are%20%28i%29%20the%20development%20of%20novel%20measures%20for%20quantifying%0Auncertainty%20at%20lesion%20and%20patient%20scales%2C%20derived%20from%20structural%20prediction%0Adiscrepancies%2C%20and%20%28ii%29%20the%20extension%20of%20an%20error%20retention%20curve%20analysis%0Aframework%20to%20facilitate%20the%20evaluation%20of%20UQ%20performance%20at%20both%20lesion%20and%0Apatient%20scales.%20The%20results%20from%20a%20multi-centric%20MRI%20dataset%20of%20334%20patients%0Ademonstrate%20that%20our%20proposed%20measures%20more%20effectively%20capture%20model%20errors%20at%0Athe%20lesion%20and%20patient%20scales%20compared%20to%20measures%20that%20average%20voxel-scale%0Auncertainty%20values.%20We%20provide%20the%20UQ%20protocols%20code%20at%0Ahttps%3A//github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.08931v2&entry.124074799=Read"},
{"title": "Raising the ClaSS of Streaming Time Series Segmentation", "author": "Arik Ermshaus and Patrick Sch\u00e4fer and Ulf Leser", "abstract": "  Ubiquitous sensors today emit high frequency streams of numerical\nmeasurements that reflect properties of human, animal, industrial, commercial,\nand natural processes. Shifts in such processes, e.g. caused by external events\nor internal state changes, manifest as changes in the recorded signals. The\ntask of streaming time series segmentation (STSS) is to partition the stream\ninto consecutive variable-sized segments that correspond to states of the\nobserved processes or entities. The partition operation itself must in\nperformance be able to cope with the input frequency of the signals. We\nintroduce ClaSS, a novel, efficient, and highly accurate algorithm for STSS.\nClaSS assesses the homogeneity of potential partitions using self-supervised\ntime series classification and applies statistical tests to detect significant\nchange points (CPs). In our experimental evaluation using two large benchmarks\nand six real-world data archives, we found ClaSS to be significantly more\nprecise than eight state-of-the-art competitors. Its space and time complexity\nis independent of segment sizes and linear only in the sliding window size. We\nalso provide ClaSS as a window operator with an average throughput of 1k data\npoints per second for the Apache Flink streaming engine.\n", "link": "http://arxiv.org/abs/2310.20431v3", "date": "2024-04-26", "relevancy": 1.707, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4373}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4211}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4144}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Raising%20the%20ClaSS%20of%20Streaming%20Time%20Series%20Segmentation&body=Title%3A%20Raising%20the%20ClaSS%20of%20Streaming%20Time%20Series%20Segmentation%0AAuthor%3A%20Arik%20Ermshaus%20and%20Patrick%20Sch%C3%A4fer%20and%20Ulf%20Leser%0AAbstract%3A%20%20%20Ubiquitous%20sensors%20today%20emit%20high%20frequency%20streams%20of%20numerical%0Ameasurements%20that%20reflect%20properties%20of%20human%2C%20animal%2C%20industrial%2C%20commercial%2C%0Aand%20natural%20processes.%20Shifts%20in%20such%20processes%2C%20e.g.%20caused%20by%20external%20events%0Aor%20internal%20state%20changes%2C%20manifest%20as%20changes%20in%20the%20recorded%20signals.%20The%0Atask%20of%20streaming%20time%20series%20segmentation%20%28STSS%29%20is%20to%20partition%20the%20stream%0Ainto%20consecutive%20variable-sized%20segments%20that%20correspond%20to%20states%20of%20the%0Aobserved%20processes%20or%20entities.%20The%20partition%20operation%20itself%20must%20in%0Aperformance%20be%20able%20to%20cope%20with%20the%20input%20frequency%20of%20the%20signals.%20We%0Aintroduce%20ClaSS%2C%20a%20novel%2C%20efficient%2C%20and%20highly%20accurate%20algorithm%20for%20STSS.%0AClaSS%20assesses%20the%20homogeneity%20of%20potential%20partitions%20using%20self-supervised%0Atime%20series%20classification%20and%20applies%20statistical%20tests%20to%20detect%20significant%0Achange%20points%20%28CPs%29.%20In%20our%20experimental%20evaluation%20using%20two%20large%20benchmarks%0Aand%20six%20real-world%20data%20archives%2C%20we%20found%20ClaSS%20to%20be%20significantly%20more%0Aprecise%20than%20eight%20state-of-the-art%20competitors.%20Its%20space%20and%20time%20complexity%0Ais%20independent%20of%20segment%20sizes%20and%20linear%20only%20in%20the%20sliding%20window%20size.%20We%0Aalso%20provide%20ClaSS%20as%20a%20window%20operator%20with%20an%20average%20throughput%20of%201k%20data%0Apoints%20per%20second%20for%20the%20Apache%20Flink%20streaming%20engine.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.20431v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Raising%20the%20ClaSS%20of%20Streaming%20Time%20Series%20Segmentation&entry.906535625=Arik%20Ermshaus%20and%20Patrick%20Sch%C3%A4fer%20and%20Ulf%20Leser&entry.1292438233=%20%20Ubiquitous%20sensors%20today%20emit%20high%20frequency%20streams%20of%20numerical%0Ameasurements%20that%20reflect%20properties%20of%20human%2C%20animal%2C%20industrial%2C%20commercial%2C%0Aand%20natural%20processes.%20Shifts%20in%20such%20processes%2C%20e.g.%20caused%20by%20external%20events%0Aor%20internal%20state%20changes%2C%20manifest%20as%20changes%20in%20the%20recorded%20signals.%20The%0Atask%20of%20streaming%20time%20series%20segmentation%20%28STSS%29%20is%20to%20partition%20the%20stream%0Ainto%20consecutive%20variable-sized%20segments%20that%20correspond%20to%20states%20of%20the%0Aobserved%20processes%20or%20entities.%20The%20partition%20operation%20itself%20must%20in%0Aperformance%20be%20able%20to%20cope%20with%20the%20input%20frequency%20of%20the%20signals.%20We%0Aintroduce%20ClaSS%2C%20a%20novel%2C%20efficient%2C%20and%20highly%20accurate%20algorithm%20for%20STSS.%0AClaSS%20assesses%20the%20homogeneity%20of%20potential%20partitions%20using%20self-supervised%0Atime%20series%20classification%20and%20applies%20statistical%20tests%20to%20detect%20significant%0Achange%20points%20%28CPs%29.%20In%20our%20experimental%20evaluation%20using%20two%20large%20benchmarks%0Aand%20six%20real-world%20data%20archives%2C%20we%20found%20ClaSS%20to%20be%20significantly%20more%0Aprecise%20than%20eight%20state-of-the-art%20competitors.%20Its%20space%20and%20time%20complexity%0Ais%20independent%20of%20segment%20sizes%20and%20linear%20only%20in%20the%20sliding%20window%20size.%20We%0Aalso%20provide%20ClaSS%20as%20a%20window%20operator%20with%20an%20average%20throughput%20of%201k%20data%0Apoints%20per%20second%20for%20the%20Apache%20Flink%20streaming%20engine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.20431v3&entry.124074799=Read"},
{"title": "Dense Road Surface Grip Map Prediction from Multimodal Image Data", "author": "Jyri Maanp\u00e4\u00e4 and Julius Pesonen and Heikki Hyyti and Iaroslav Melekhov and Juho Kannala and Petri Manninen and Antero Kukko and Juha Hyypp\u00e4", "abstract": "  Slippery road weather conditions are prevalent in many regions and cause a\nregular risk for traffic. Still, there has been less research on how autonomous\nvehicles could detect slippery driving conditions on the road to drive safely.\nIn this work, we propose a method to predict a dense grip map from the area in\nfront of the car, based on postprocessed multimodal sensor data. We trained a\nconvolutional neural network to predict pixelwise grip values from fused RGB\ncamera, thermal camera, and LiDAR reflectance images, based on weakly\nsupervised ground truth from an optical road weather sensor.\n  The experiments show that it is possible to predict dense grip values with\ngood accuracy from the used data modalities as the produced grip map follows\nboth ground truth measurements and local weather conditions, such as snowy\nareas on the road. The model using only the RGB camera or LiDAR reflectance\nmodality provided good baseline results for grip prediction accuracy while\nusing models fusing the RGB camera, thermal camera, and LiDAR modalities\nimproved the grip predictions significantly.\n", "link": "http://arxiv.org/abs/2404.17324v1", "date": "2024-04-26", "relevancy": 1.7056, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6226}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5698}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5113}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dense%20Road%20Surface%20Grip%20Map%20Prediction%20from%20Multimodal%20Image%20Data&body=Title%3A%20Dense%20Road%20Surface%20Grip%20Map%20Prediction%20from%20Multimodal%20Image%20Data%0AAuthor%3A%20Jyri%20Maanp%C3%A4%C3%A4%20and%20Julius%20Pesonen%20and%20Heikki%20Hyyti%20and%20Iaroslav%20Melekhov%20and%20Juho%20Kannala%20and%20Petri%20Manninen%20and%20Antero%20Kukko%20and%20Juha%20Hyypp%C3%A4%0AAbstract%3A%20%20%20Slippery%20road%20weather%20conditions%20are%20prevalent%20in%20many%20regions%20and%20cause%20a%0Aregular%20risk%20for%20traffic.%20Still%2C%20there%20has%20been%20less%20research%20on%20how%20autonomous%0Avehicles%20could%20detect%20slippery%20driving%20conditions%20on%20the%20road%20to%20drive%20safely.%0AIn%20this%20work%2C%20we%20propose%20a%20method%20to%20predict%20a%20dense%20grip%20map%20from%20the%20area%20in%0Afront%20of%20the%20car%2C%20based%20on%20postprocessed%20multimodal%20sensor%20data.%20We%20trained%20a%0Aconvolutional%20neural%20network%20to%20predict%20pixelwise%20grip%20values%20from%20fused%20RGB%0Acamera%2C%20thermal%20camera%2C%20and%20LiDAR%20reflectance%20images%2C%20based%20on%20weakly%0Asupervised%20ground%20truth%20from%20an%20optical%20road%20weather%20sensor.%0A%20%20The%20experiments%20show%20that%20it%20is%20possible%20to%20predict%20dense%20grip%20values%20with%0Agood%20accuracy%20from%20the%20used%20data%20modalities%20as%20the%20produced%20grip%20map%20follows%0Aboth%20ground%20truth%20measurements%20and%20local%20weather%20conditions%2C%20such%20as%20snowy%0Aareas%20on%20the%20road.%20The%20model%20using%20only%20the%20RGB%20camera%20or%20LiDAR%20reflectance%0Amodality%20provided%20good%20baseline%20results%20for%20grip%20prediction%20accuracy%20while%0Ausing%20models%20fusing%20the%20RGB%20camera%2C%20thermal%20camera%2C%20and%20LiDAR%20modalities%0Aimproved%20the%20grip%20predictions%20significantly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17324v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dense%20Road%20Surface%20Grip%20Map%20Prediction%20from%20Multimodal%20Image%20Data&entry.906535625=Jyri%20Maanp%C3%A4%C3%A4%20and%20Julius%20Pesonen%20and%20Heikki%20Hyyti%20and%20Iaroslav%20Melekhov%20and%20Juho%20Kannala%20and%20Petri%20Manninen%20and%20Antero%20Kukko%20and%20Juha%20Hyypp%C3%A4&entry.1292438233=%20%20Slippery%20road%20weather%20conditions%20are%20prevalent%20in%20many%20regions%20and%20cause%20a%0Aregular%20risk%20for%20traffic.%20Still%2C%20there%20has%20been%20less%20research%20on%20how%20autonomous%0Avehicles%20could%20detect%20slippery%20driving%20conditions%20on%20the%20road%20to%20drive%20safely.%0AIn%20this%20work%2C%20we%20propose%20a%20method%20to%20predict%20a%20dense%20grip%20map%20from%20the%20area%20in%0Afront%20of%20the%20car%2C%20based%20on%20postprocessed%20multimodal%20sensor%20data.%20We%20trained%20a%0Aconvolutional%20neural%20network%20to%20predict%20pixelwise%20grip%20values%20from%20fused%20RGB%0Acamera%2C%20thermal%20camera%2C%20and%20LiDAR%20reflectance%20images%2C%20based%20on%20weakly%0Asupervised%20ground%20truth%20from%20an%20optical%20road%20weather%20sensor.%0A%20%20The%20experiments%20show%20that%20it%20is%20possible%20to%20predict%20dense%20grip%20values%20with%0Agood%20accuracy%20from%20the%20used%20data%20modalities%20as%20the%20produced%20grip%20map%20follows%0Aboth%20ground%20truth%20measurements%20and%20local%20weather%20conditions%2C%20such%20as%20snowy%0Aareas%20on%20the%20road.%20The%20model%20using%20only%20the%20RGB%20camera%20or%20LiDAR%20reflectance%0Amodality%20provided%20good%20baseline%20results%20for%20grip%20prediction%20accuracy%20while%0Ausing%20models%20fusing%20the%20RGB%20camera%2C%20thermal%20camera%2C%20and%20LiDAR%20modalities%0Aimproved%20the%20grip%20predictions%20significantly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17324v1&entry.124074799=Read"},
{"title": "Sparse Reconstruction of Optical Doppler Tomography Based on State Space\n  Model", "author": "Zhenghong Li and Jiaxiang Ren and Wensheng Cheng and Congwu Du and Yingtian Pan and Haibin Ling", "abstract": "  Optical Doppler Tomography (ODT) is a blood flow imaging technique popularly\nused in bioengineering applications. The fundamental unit of ODT is the 1D\nfrequency response along the A-line (depth), named raw A-scan. A 2D ODT image\n(B-scan) is obtained by first sensing raw A-scans along the B-line (width), and\nthen constructing the B-scan from these raw A-scans via magnitude-phase\nanalysis and post-processing. To obtain a high-resolution B-scan with a precise\nflow map, densely sampled A-scans are required in current methods, causing both\ncomputational and storage burdens. To address this issue, in this paper we\npropose a novel sparse reconstruction framework with four main sequential\nsteps: 1) early magnitude-phase fusion that encourages rich interaction of the\ncomplementary information in magnitude and phase, 2) State Space Model\n(SSM)-based representation learning, inspired by recent successes in Mamba and\nVMamba, to naturally capture both the intra-A-scan sequential information and\nbetween-A-scan interactions, 3) an Inception-based Feedforward Network module\n(IncFFN) to further boost the SSM-module, and 4) a B-line Pixel Shuffle (BPS)\nlayer to effectively reconstruct the final results. In the experiments on\nreal-world animal data, our method shows clear effectiveness in reconstruction\naccuracy. As the first application of SSM for image reconstruction tasks, we\nexpect our work to inspire related explorations in not only efficient ODT\nimaging techniques but also generic image enhancement.\n", "link": "http://arxiv.org/abs/2404.17484v1", "date": "2024-04-26", "relevancy": 1.6917, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5997}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5553}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5495}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sparse%20Reconstruction%20of%20Optical%20Doppler%20Tomography%20Based%20on%20State%20Space%0A%20%20Model&body=Title%3A%20Sparse%20Reconstruction%20of%20Optical%20Doppler%20Tomography%20Based%20on%20State%20Space%0A%20%20Model%0AAuthor%3A%20Zhenghong%20Li%20and%20Jiaxiang%20Ren%20and%20Wensheng%20Cheng%20and%20Congwu%20Du%20and%20Yingtian%20Pan%20and%20Haibin%20Ling%0AAbstract%3A%20%20%20Optical%20Doppler%20Tomography%20%28ODT%29%20is%20a%20blood%20flow%20imaging%20technique%20popularly%0Aused%20in%20bioengineering%20applications.%20The%20fundamental%20unit%20of%20ODT%20is%20the%201D%0Afrequency%20response%20along%20the%20A-line%20%28depth%29%2C%20named%20raw%20A-scan.%20A%202D%20ODT%20image%0A%28B-scan%29%20is%20obtained%20by%20first%20sensing%20raw%20A-scans%20along%20the%20B-line%20%28width%29%2C%20and%0Athen%20constructing%20the%20B-scan%20from%20these%20raw%20A-scans%20via%20magnitude-phase%0Aanalysis%20and%20post-processing.%20To%20obtain%20a%20high-resolution%20B-scan%20with%20a%20precise%0Aflow%20map%2C%20densely%20sampled%20A-scans%20are%20required%20in%20current%20methods%2C%20causing%20both%0Acomputational%20and%20storage%20burdens.%20To%20address%20this%20issue%2C%20in%20this%20paper%20we%0Apropose%20a%20novel%20sparse%20reconstruction%20framework%20with%20four%20main%20sequential%0Asteps%3A%201%29%20early%20magnitude-phase%20fusion%20that%20encourages%20rich%20interaction%20of%20the%0Acomplementary%20information%20in%20magnitude%20and%20phase%2C%202%29%20State%20Space%20Model%0A%28SSM%29-based%20representation%20learning%2C%20inspired%20by%20recent%20successes%20in%20Mamba%20and%0AVMamba%2C%20to%20naturally%20capture%20both%20the%20intra-A-scan%20sequential%20information%20and%0Abetween-A-scan%20interactions%2C%203%29%20an%20Inception-based%20Feedforward%20Network%20module%0A%28IncFFN%29%20to%20further%20boost%20the%20SSM-module%2C%20and%204%29%20a%20B-line%20Pixel%20Shuffle%20%28BPS%29%0Alayer%20to%20effectively%20reconstruct%20the%20final%20results.%20In%20the%20experiments%20on%0Areal-world%20animal%20data%2C%20our%20method%20shows%20clear%20effectiveness%20in%20reconstruction%0Aaccuracy.%20As%20the%20first%20application%20of%20SSM%20for%20image%20reconstruction%20tasks%2C%20we%0Aexpect%20our%20work%20to%20inspire%20related%20explorations%20in%20not%20only%20efficient%20ODT%0Aimaging%20techniques%20but%20also%20generic%20image%20enhancement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17484v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Reconstruction%20of%20Optical%20Doppler%20Tomography%20Based%20on%20State%20Space%0A%20%20Model&entry.906535625=Zhenghong%20Li%20and%20Jiaxiang%20Ren%20and%20Wensheng%20Cheng%20and%20Congwu%20Du%20and%20Yingtian%20Pan%20and%20Haibin%20Ling&entry.1292438233=%20%20Optical%20Doppler%20Tomography%20%28ODT%29%20is%20a%20blood%20flow%20imaging%20technique%20popularly%0Aused%20in%20bioengineering%20applications.%20The%20fundamental%20unit%20of%20ODT%20is%20the%201D%0Afrequency%20response%20along%20the%20A-line%20%28depth%29%2C%20named%20raw%20A-scan.%20A%202D%20ODT%20image%0A%28B-scan%29%20is%20obtained%20by%20first%20sensing%20raw%20A-scans%20along%20the%20B-line%20%28width%29%2C%20and%0Athen%20constructing%20the%20B-scan%20from%20these%20raw%20A-scans%20via%20magnitude-phase%0Aanalysis%20and%20post-processing.%20To%20obtain%20a%20high-resolution%20B-scan%20with%20a%20precise%0Aflow%20map%2C%20densely%20sampled%20A-scans%20are%20required%20in%20current%20methods%2C%20causing%20both%0Acomputational%20and%20storage%20burdens.%20To%20address%20this%20issue%2C%20in%20this%20paper%20we%0Apropose%20a%20novel%20sparse%20reconstruction%20framework%20with%20four%20main%20sequential%0Asteps%3A%201%29%20early%20magnitude-phase%20fusion%20that%20encourages%20rich%20interaction%20of%20the%0Acomplementary%20information%20in%20magnitude%20and%20phase%2C%202%29%20State%20Space%20Model%0A%28SSM%29-based%20representation%20learning%2C%20inspired%20by%20recent%20successes%20in%20Mamba%20and%0AVMamba%2C%20to%20naturally%20capture%20both%20the%20intra-A-scan%20sequential%20information%20and%0Abetween-A-scan%20interactions%2C%203%29%20an%20Inception-based%20Feedforward%20Network%20module%0A%28IncFFN%29%20to%20further%20boost%20the%20SSM-module%2C%20and%204%29%20a%20B-line%20Pixel%20Shuffle%20%28BPS%29%0Alayer%20to%20effectively%20reconstruct%20the%20final%20results.%20In%20the%20experiments%20on%0Areal-world%20animal%20data%2C%20our%20method%20shows%20clear%20effectiveness%20in%20reconstruction%0Aaccuracy.%20As%20the%20first%20application%20of%20SSM%20for%20image%20reconstruction%20tasks%2C%20we%0Aexpect%20our%20work%20to%20inspire%20related%20explorations%20in%20not%20only%20efficient%20ODT%0Aimaging%20techniques%20but%20also%20generic%20image%20enhancement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17484v1&entry.124074799=Read"},
{"title": "Real-World Deployment of a Hierarchical Uncertainty-Aware Collaborative\n  Multiagent Planning System", "author": "Martina Stadler Kurtz and Samuel Prentice and Yasmin Veys and Long Quang and Carlos Nieto-Granda and Michael Novitzky and Ethan Stump and Nicholas Roy", "abstract": "  We would like to enable a collaborative multiagent team to navigate at long\nlength scales and under uncertainty in real-world environments. In practice,\nplanning complexity scales with the number of agents in the team, with the\nlength scale of the environment, and with environmental uncertainty. Enabling\ntractable planning requires developing abstract models that can represent\ncomplex, high-quality plans. However, such models often abstract away\ninformation needed to generate directly-executable plans for real-world agents\nin real-world environments, as planning in such detail, especially in the\npresence of real-world uncertainty, would be computationally intractable. In\nthis paper, we describe the deployment of a planning system that used a\nhierarchy of planners to execute collaborative multiagent navigation tasks in\nreal-world, unknown environments. By developing a planning system that was\nrobust to failures at every level of the planning hierarchy, we enabled the\nteam to complete collaborative navigation tasks, even in the presence of\nimperfect planning abstractions and real-world uncertainty. We deployed our\napproach on a Clearpath Husky-Jackal team navigating in a structured outdoor\nenvironment, and demonstrated that the system enabled the agents to\nsuccessfully execute collaborative plans.\n", "link": "http://arxiv.org/abs/2404.17438v1", "date": "2024-04-26", "relevancy": 1.679, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5732}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5486}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.537}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Real-World%20Deployment%20of%20a%20Hierarchical%20Uncertainty-Aware%20Collaborative%0A%20%20Multiagent%20Planning%20System&body=Title%3A%20Real-World%20Deployment%20of%20a%20Hierarchical%20Uncertainty-Aware%20Collaborative%0A%20%20Multiagent%20Planning%20System%0AAuthor%3A%20Martina%20Stadler%20Kurtz%20and%20Samuel%20Prentice%20and%20Yasmin%20Veys%20and%20Long%20Quang%20and%20Carlos%20Nieto-Granda%20and%20Michael%20Novitzky%20and%20Ethan%20Stump%20and%20Nicholas%20Roy%0AAbstract%3A%20%20%20We%20would%20like%20to%20enable%20a%20collaborative%20multiagent%20team%20to%20navigate%20at%20long%0Alength%20scales%20and%20under%20uncertainty%20in%20real-world%20environments.%20In%20practice%2C%0Aplanning%20complexity%20scales%20with%20the%20number%20of%20agents%20in%20the%20team%2C%20with%20the%0Alength%20scale%20of%20the%20environment%2C%20and%20with%20environmental%20uncertainty.%20Enabling%0Atractable%20planning%20requires%20developing%20abstract%20models%20that%20can%20represent%0Acomplex%2C%20high-quality%20plans.%20However%2C%20such%20models%20often%20abstract%20away%0Ainformation%20needed%20to%20generate%20directly-executable%20plans%20for%20real-world%20agents%0Ain%20real-world%20environments%2C%20as%20planning%20in%20such%20detail%2C%20especially%20in%20the%0Apresence%20of%20real-world%20uncertainty%2C%20would%20be%20computationally%20intractable.%20In%0Athis%20paper%2C%20we%20describe%20the%20deployment%20of%20a%20planning%20system%20that%20used%20a%0Ahierarchy%20of%20planners%20to%20execute%20collaborative%20multiagent%20navigation%20tasks%20in%0Areal-world%2C%20unknown%20environments.%20By%20developing%20a%20planning%20system%20that%20was%0Arobust%20to%20failures%20at%20every%20level%20of%20the%20planning%20hierarchy%2C%20we%20enabled%20the%0Ateam%20to%20complete%20collaborative%20navigation%20tasks%2C%20even%20in%20the%20presence%20of%0Aimperfect%20planning%20abstractions%20and%20real-world%20uncertainty.%20We%20deployed%20our%0Aapproach%20on%20a%20Clearpath%20Husky-Jackal%20team%20navigating%20in%20a%20structured%20outdoor%0Aenvironment%2C%20and%20demonstrated%20that%20the%20system%20enabled%20the%20agents%20to%0Asuccessfully%20execute%20collaborative%20plans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17438v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-World%20Deployment%20of%20a%20Hierarchical%20Uncertainty-Aware%20Collaborative%0A%20%20Multiagent%20Planning%20System&entry.906535625=Martina%20Stadler%20Kurtz%20and%20Samuel%20Prentice%20and%20Yasmin%20Veys%20and%20Long%20Quang%20and%20Carlos%20Nieto-Granda%20and%20Michael%20Novitzky%20and%20Ethan%20Stump%20and%20Nicholas%20Roy&entry.1292438233=%20%20We%20would%20like%20to%20enable%20a%20collaborative%20multiagent%20team%20to%20navigate%20at%20long%0Alength%20scales%20and%20under%20uncertainty%20in%20real-world%20environments.%20In%20practice%2C%0Aplanning%20complexity%20scales%20with%20the%20number%20of%20agents%20in%20the%20team%2C%20with%20the%0Alength%20scale%20of%20the%20environment%2C%20and%20with%20environmental%20uncertainty.%20Enabling%0Atractable%20planning%20requires%20developing%20abstract%20models%20that%20can%20represent%0Acomplex%2C%20high-quality%20plans.%20However%2C%20such%20models%20often%20abstract%20away%0Ainformation%20needed%20to%20generate%20directly-executable%20plans%20for%20real-world%20agents%0Ain%20real-world%20environments%2C%20as%20planning%20in%20such%20detail%2C%20especially%20in%20the%0Apresence%20of%20real-world%20uncertainty%2C%20would%20be%20computationally%20intractable.%20In%0Athis%20paper%2C%20we%20describe%20the%20deployment%20of%20a%20planning%20system%20that%20used%20a%0Ahierarchy%20of%20planners%20to%20execute%20collaborative%20multiagent%20navigation%20tasks%20in%0Areal-world%2C%20unknown%20environments.%20By%20developing%20a%20planning%20system%20that%20was%0Arobust%20to%20failures%20at%20every%20level%20of%20the%20planning%20hierarchy%2C%20we%20enabled%20the%0Ateam%20to%20complete%20collaborative%20navigation%20tasks%2C%20even%20in%20the%20presence%20of%0Aimperfect%20planning%20abstractions%20and%20real-world%20uncertainty.%20We%20deployed%20our%0Aapproach%20on%20a%20Clearpath%20Husky-Jackal%20team%20navigating%20in%20a%20structured%20outdoor%0Aenvironment%2C%20and%20demonstrated%20that%20the%20system%20enabled%20the%20agents%20to%0Asuccessfully%20execute%20collaborative%20plans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17438v1&entry.124074799=Read"},
{"title": "Permissible Knowledge Pooling", "author": "Huimin Dong", "abstract": "  Information pooling has been extensively formalised across various logical\nframeworks in distributed systems, characterized by diverse information-sharing\npatterns. These approaches generally adopt an intersection perspective,\naggregating all possible information, regardless of whether it is known or\nunknown to the agents. In contrast, this work adopts a unique stance,\nemphasising that sharing knowledge means distributing what is known, rather\nthan what remains uncertain. This paper introduces new modal logics for\nknowledge pooling and sharing, ranging from a novel language of knowledge\npooling to a dynamic mechanism for knowledge sharing. It also outlines their\naxiomatizations and discusses a potential framework for permissible knowledge\npooling.\n", "link": "http://arxiv.org/abs/2404.03418v2", "date": "2024-04-26", "relevancy": 1.6786, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4314}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4174}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4088}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Permissible%20Knowledge%20Pooling&body=Title%3A%20Permissible%20Knowledge%20Pooling%0AAuthor%3A%20Huimin%20Dong%0AAbstract%3A%20%20%20Information%20pooling%20has%20been%20extensively%20formalised%20across%20various%20logical%0Aframeworks%20in%20distributed%20systems%2C%20characterized%20by%20diverse%20information-sharing%0Apatterns.%20These%20approaches%20generally%20adopt%20an%20intersection%20perspective%2C%0Aaggregating%20all%20possible%20information%2C%20regardless%20of%20whether%20it%20is%20known%20or%0Aunknown%20to%20the%20agents.%20In%20contrast%2C%20this%20work%20adopts%20a%20unique%20stance%2C%0Aemphasising%20that%20sharing%20knowledge%20means%20distributing%20what%20is%20known%2C%20rather%0Athan%20what%20remains%20uncertain.%20This%20paper%20introduces%20new%20modal%20logics%20for%0Aknowledge%20pooling%20and%20sharing%2C%20ranging%20from%20a%20novel%20language%20of%20knowledge%0Apooling%20to%20a%20dynamic%20mechanism%20for%20knowledge%20sharing.%20It%20also%20outlines%20their%0Aaxiomatizations%20and%20discusses%20a%20potential%20framework%20for%20permissible%20knowledge%0Apooling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03418v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Permissible%20Knowledge%20Pooling&entry.906535625=Huimin%20Dong&entry.1292438233=%20%20Information%20pooling%20has%20been%20extensively%20formalised%20across%20various%20logical%0Aframeworks%20in%20distributed%20systems%2C%20characterized%20by%20diverse%20information-sharing%0Apatterns.%20These%20approaches%20generally%20adopt%20an%20intersection%20perspective%2C%0Aaggregating%20all%20possible%20information%2C%20regardless%20of%20whether%20it%20is%20known%20or%0Aunknown%20to%20the%20agents.%20In%20contrast%2C%20this%20work%20adopts%20a%20unique%20stance%2C%0Aemphasising%20that%20sharing%20knowledge%20means%20distributing%20what%20is%20known%2C%20rather%0Athan%20what%20remains%20uncertain.%20This%20paper%20introduces%20new%20modal%20logics%20for%0Aknowledge%20pooling%20and%20sharing%2C%20ranging%20from%20a%20novel%20language%20of%20knowledge%0Apooling%20to%20a%20dynamic%20mechanism%20for%20knowledge%20sharing.%20It%20also%20outlines%20their%0Aaxiomatizations%20and%20discusses%20a%20potential%20framework%20for%20permissible%20knowledge%0Apooling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03418v2&entry.124074799=Read"},
{"title": "ChemScraper: Leveraging PDF Graphics Instructions for Molecular Diagram\n  Parsing", "author": "Ayush Kumar Shah and Bryan Manrique Amador and Abhisek Dey and Ming Creekmore and Blake Ocampo and Scott Denmark and Richard Zanibbi", "abstract": "  Most molecular diagram parsers recover chemical structure from raster images\n(e.g., PNGs). However, many PDFs include commands giving explicit locations and\nshapes for characters, lines, and polygons. We present a new parser that uses\nthese born-digital PDF primitives as input. The parsing model is fast and\naccurate, and does not require GPUs, Optical Character Recognition (OCR), or\nvectorization. We use the parser to annotate raster images and then train a new\nmulti-task neural network for recognizing molecules in raster images. We\nevaluate our parsers using SMILES and standard benchmarks, along with a novel\nevaluation protocol comparing molecular graphs directly that supports automatic\nerror compilation and reveals errors missed by SMILES-based evaluation.\n", "link": "http://arxiv.org/abs/2311.12161v3", "date": "2024-04-26", "relevancy": 1.6776, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4234}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4207}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.406}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ChemScraper%3A%20Leveraging%20PDF%20Graphics%20Instructions%20for%20Molecular%20Diagram%0A%20%20Parsing&body=Title%3A%20ChemScraper%3A%20Leveraging%20PDF%20Graphics%20Instructions%20for%20Molecular%20Diagram%0A%20%20Parsing%0AAuthor%3A%20Ayush%20Kumar%20Shah%20and%20Bryan%20Manrique%20Amador%20and%20Abhisek%20Dey%20and%20Ming%20Creekmore%20and%20Blake%20Ocampo%20and%20Scott%20Denmark%20and%20Richard%20Zanibbi%0AAbstract%3A%20%20%20Most%20molecular%20diagram%20parsers%20recover%20chemical%20structure%20from%20raster%20images%0A%28e.g.%2C%20PNGs%29.%20However%2C%20many%20PDFs%20include%20commands%20giving%20explicit%20locations%20and%0Ashapes%20for%20characters%2C%20lines%2C%20and%20polygons.%20We%20present%20a%20new%20parser%20that%20uses%0Athese%20born-digital%20PDF%20primitives%20as%20input.%20The%20parsing%20model%20is%20fast%20and%0Aaccurate%2C%20and%20does%20not%20require%20GPUs%2C%20Optical%20Character%20Recognition%20%28OCR%29%2C%20or%0Avectorization.%20We%20use%20the%20parser%20to%20annotate%20raster%20images%20and%20then%20train%20a%20new%0Amulti-task%20neural%20network%20for%20recognizing%20molecules%20in%20raster%20images.%20We%0Aevaluate%20our%20parsers%20using%20SMILES%20and%20standard%20benchmarks%2C%20along%20with%20a%20novel%0Aevaluation%20protocol%20comparing%20molecular%20graphs%20directly%20that%20supports%20automatic%0Aerror%20compilation%20and%20reveals%20errors%20missed%20by%20SMILES-based%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12161v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChemScraper%3A%20Leveraging%20PDF%20Graphics%20Instructions%20for%20Molecular%20Diagram%0A%20%20Parsing&entry.906535625=Ayush%20Kumar%20Shah%20and%20Bryan%20Manrique%20Amador%20and%20Abhisek%20Dey%20and%20Ming%20Creekmore%20and%20Blake%20Ocampo%20and%20Scott%20Denmark%20and%20Richard%20Zanibbi&entry.1292438233=%20%20Most%20molecular%20diagram%20parsers%20recover%20chemical%20structure%20from%20raster%20images%0A%28e.g.%2C%20PNGs%29.%20However%2C%20many%20PDFs%20include%20commands%20giving%20explicit%20locations%20and%0Ashapes%20for%20characters%2C%20lines%2C%20and%20polygons.%20We%20present%20a%20new%20parser%20that%20uses%0Athese%20born-digital%20PDF%20primitives%20as%20input.%20The%20parsing%20model%20is%20fast%20and%0Aaccurate%2C%20and%20does%20not%20require%20GPUs%2C%20Optical%20Character%20Recognition%20%28OCR%29%2C%20or%0Avectorization.%20We%20use%20the%20parser%20to%20annotate%20raster%20images%20and%20then%20train%20a%20new%0Amulti-task%20neural%20network%20for%20recognizing%20molecules%20in%20raster%20images.%20We%0Aevaluate%20our%20parsers%20using%20SMILES%20and%20standard%20benchmarks%2C%20along%20with%20a%20novel%0Aevaluation%20protocol%20comparing%20molecular%20graphs%20directly%20that%20supports%20automatic%0Aerror%20compilation%20and%20reveals%20errors%20missed%20by%20SMILES-based%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12161v3&entry.124074799=Read"},
{"title": "LLIC: Large Receptive Field Transform Coding with Adaptive Weights for\n  Learned Image Compression", "author": "Wei Jiang and Peirong Ning and Jiayu Yang and Yongqi Zhai and Feng Gao and Ronggang Wang", "abstract": "  The Effective Receptive field (ERF) plays an important role in transform\ncoding, which determines how much redundancy can be removed at most during\ntransform and how many spatial priors can be utilized to synthesize textures\nduring inverse transform. Existing methods rely on stacks of small kernels,\nwhose ERF remains not large enough instead, or heavy non-local attention\nmechanisms, which limit the potential of high-resolution image coding. To\ntackle this issue, we propose Large Receptive Field Transform Coding with\nAdaptive Weights for Learned Image Compression (LLIC). Specifically, for the\nfirst time in the learned image compression community, we introduce a few large\nkernel-based depth-wise convolutions to reduce more redundancy while\nmaintaining modest complexity. Due to the wide range of image diversity, we\nfurther propose a mechanism to augment convolution adaptability through the\nself-conditioned generation of weights. The large kernels cooperate with\nnon-linear embedding and gate mechanisms for better expressiveness and lighter\npoint-wise interactions. Our investigation extends to refined training methods\nthat unlock the full potential of these large kernels. Moreover, to promote\nmore dynamic inter-channel interactions, we introduce an adaptive channel-wise\nbit allocation strategy that autonomously generates channel importance factors\nin a self-conditioned manner. To demonstrate the effectiveness of the proposed\ntransform coding, we align the entropy model to compare with existing transform\nmethods and obtain models LLIC-STF, LLIC-ELIC, LLIC-TCM. Extensive experiments\ndemonstrate our proposed LLIC models have significant improvements over\ncorresponding baselines and reduce BD-Rate by 9.49%, 9.47%, 10.94% on Kodak\nover VTM-17.0 Intra, respectively. Our LLIC models achieve state-of-the-art\nperformances and better trade-offs between performance and complexity.\n", "link": "http://arxiv.org/abs/2304.09571v7", "date": "2024-04-26", "relevancy": 1.6725, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5718}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5444}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5348}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LLIC%3A%20Large%20Receptive%20Field%20Transform%20Coding%20with%20Adaptive%20Weights%20for%0A%20%20Learned%20Image%20Compression&body=Title%3A%20LLIC%3A%20Large%20Receptive%20Field%20Transform%20Coding%20with%20Adaptive%20Weights%20for%0A%20%20Learned%20Image%20Compression%0AAuthor%3A%20Wei%20Jiang%20and%20Peirong%20Ning%20and%20Jiayu%20Yang%20and%20Yongqi%20Zhai%20and%20Feng%20Gao%20and%20Ronggang%20Wang%0AAbstract%3A%20%20%20The%20Effective%20Receptive%20field%20%28ERF%29%20plays%20an%20important%20role%20in%20transform%0Acoding%2C%20which%20determines%20how%20much%20redundancy%20can%20be%20removed%20at%20most%20during%0Atransform%20and%20how%20many%20spatial%20priors%20can%20be%20utilized%20to%20synthesize%20textures%0Aduring%20inverse%20transform.%20Existing%20methods%20rely%20on%20stacks%20of%20small%20kernels%2C%0Awhose%20ERF%20remains%20not%20large%20enough%20instead%2C%20or%20heavy%20non-local%20attention%0Amechanisms%2C%20which%20limit%20the%20potential%20of%20high-resolution%20image%20coding.%20To%0Atackle%20this%20issue%2C%20we%20propose%20Large%20Receptive%20Field%20Transform%20Coding%20with%0AAdaptive%20Weights%20for%20Learned%20Image%20Compression%20%28LLIC%29.%20Specifically%2C%20for%20the%0Afirst%20time%20in%20the%20learned%20image%20compression%20community%2C%20we%20introduce%20a%20few%20large%0Akernel-based%20depth-wise%20convolutions%20to%20reduce%20more%20redundancy%20while%0Amaintaining%20modest%20complexity.%20Due%20to%20the%20wide%20range%20of%20image%20diversity%2C%20we%0Afurther%20propose%20a%20mechanism%20to%20augment%20convolution%20adaptability%20through%20the%0Aself-conditioned%20generation%20of%20weights.%20The%20large%20kernels%20cooperate%20with%0Anon-linear%20embedding%20and%20gate%20mechanisms%20for%20better%20expressiveness%20and%20lighter%0Apoint-wise%20interactions.%20Our%20investigation%20extends%20to%20refined%20training%20methods%0Athat%20unlock%20the%20full%20potential%20of%20these%20large%20kernels.%20Moreover%2C%20to%20promote%0Amore%20dynamic%20inter-channel%20interactions%2C%20we%20introduce%20an%20adaptive%20channel-wise%0Abit%20allocation%20strategy%20that%20autonomously%20generates%20channel%20importance%20factors%0Ain%20a%20self-conditioned%20manner.%20To%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Atransform%20coding%2C%20we%20align%20the%20entropy%20model%20to%20compare%20with%20existing%20transform%0Amethods%20and%20obtain%20models%20LLIC-STF%2C%20LLIC-ELIC%2C%20LLIC-TCM.%20Extensive%20experiments%0Ademonstrate%20our%20proposed%20LLIC%20models%20have%20significant%20improvements%20over%0Acorresponding%20baselines%20and%20reduce%20BD-Rate%20by%209.49%25%2C%209.47%25%2C%2010.94%25%20on%20Kodak%0Aover%20VTM-17.0%20Intra%2C%20respectively.%20Our%20LLIC%20models%20achieve%20state-of-the-art%0Aperformances%20and%20better%20trade-offs%20between%20performance%20and%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.09571v7", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLIC%3A%20Large%20Receptive%20Field%20Transform%20Coding%20with%20Adaptive%20Weights%20for%0A%20%20Learned%20Image%20Compression&entry.906535625=Wei%20Jiang%20and%20Peirong%20Ning%20and%20Jiayu%20Yang%20and%20Yongqi%20Zhai%20and%20Feng%20Gao%20and%20Ronggang%20Wang&entry.1292438233=%20%20The%20Effective%20Receptive%20field%20%28ERF%29%20plays%20an%20important%20role%20in%20transform%0Acoding%2C%20which%20determines%20how%20much%20redundancy%20can%20be%20removed%20at%20most%20during%0Atransform%20and%20how%20many%20spatial%20priors%20can%20be%20utilized%20to%20synthesize%20textures%0Aduring%20inverse%20transform.%20Existing%20methods%20rely%20on%20stacks%20of%20small%20kernels%2C%0Awhose%20ERF%20remains%20not%20large%20enough%20instead%2C%20or%20heavy%20non-local%20attention%0Amechanisms%2C%20which%20limit%20the%20potential%20of%20high-resolution%20image%20coding.%20To%0Atackle%20this%20issue%2C%20we%20propose%20Large%20Receptive%20Field%20Transform%20Coding%20with%0AAdaptive%20Weights%20for%20Learned%20Image%20Compression%20%28LLIC%29.%20Specifically%2C%20for%20the%0Afirst%20time%20in%20the%20learned%20image%20compression%20community%2C%20we%20introduce%20a%20few%20large%0Akernel-based%20depth-wise%20convolutions%20to%20reduce%20more%20redundancy%20while%0Amaintaining%20modest%20complexity.%20Due%20to%20the%20wide%20range%20of%20image%20diversity%2C%20we%0Afurther%20propose%20a%20mechanism%20to%20augment%20convolution%20adaptability%20through%20the%0Aself-conditioned%20generation%20of%20weights.%20The%20large%20kernels%20cooperate%20with%0Anon-linear%20embedding%20and%20gate%20mechanisms%20for%20better%20expressiveness%20and%20lighter%0Apoint-wise%20interactions.%20Our%20investigation%20extends%20to%20refined%20training%20methods%0Athat%20unlock%20the%20full%20potential%20of%20these%20large%20kernels.%20Moreover%2C%20to%20promote%0Amore%20dynamic%20inter-channel%20interactions%2C%20we%20introduce%20an%20adaptive%20channel-wise%0Abit%20allocation%20strategy%20that%20autonomously%20generates%20channel%20importance%20factors%0Ain%20a%20self-conditioned%20manner.%20To%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Atransform%20coding%2C%20we%20align%20the%20entropy%20model%20to%20compare%20with%20existing%20transform%0Amethods%20and%20obtain%20models%20LLIC-STF%2C%20LLIC-ELIC%2C%20LLIC-TCM.%20Extensive%20experiments%0Ademonstrate%20our%20proposed%20LLIC%20models%20have%20significant%20improvements%20over%0Acorresponding%20baselines%20and%20reduce%20BD-Rate%20by%209.49%25%2C%209.47%25%2C%2010.94%25%20on%20Kodak%0Aover%20VTM-17.0%20Intra%2C%20respectively.%20Our%20LLIC%20models%20achieve%20state-of-the-art%0Aperformances%20and%20better%20trade-offs%20between%20performance%20and%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.09571v7&entry.124074799=Read"},
{"title": "Exploring the Distinctiveness and Fidelity of the Descriptions Generated\n  by Large Vision-Language Models", "author": "Yuhang Huang and Zihan Wu and Chongyang Gao and Jiawei Peng and Xu Yang", "abstract": "  Large Vision-Language Models (LVLMs) are gaining traction for their\nremarkable ability to process and integrate visual and textual data. Despite\ntheir popularity, the capacity of LVLMs to generate precise, fine-grained\ntextual descriptions has not been fully explored. This study addresses this gap\nby focusing on \\textit{distinctiveness} and \\textit{fidelity}, assessing how\nmodels like Open-Flamingo, IDEFICS, and MiniGPT-4 can distinguish between\nsimilar objects and accurately describe visual features. We proposed the\nTextual Retrieval-Augmented Classification (TRAC) framework, which, by\nleveraging its generative capabilities, allows us to delve deeper into\nanalyzing fine-grained visual description generation. This research provides\nvaluable insights into the generation quality of LVLMs, enhancing the\nunderstanding of multimodal language models. Notably, MiniGPT-4 stands out for\nits better ability to generate fine-grained descriptions, outperforming the\nother two models in this aspect. The code is provided at\n\\url{https://anonymous.4open.science/r/Explore_FGVDs-E277}.\n", "link": "http://arxiv.org/abs/2404.17534v1", "date": "2024-04-26", "relevancy": 1.6703, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.574}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5358}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5344}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Distinctiveness%20and%20Fidelity%20of%20the%20Descriptions%20Generated%0A%20%20by%20Large%20Vision-Language%20Models&body=Title%3A%20Exploring%20the%20Distinctiveness%20and%20Fidelity%20of%20the%20Descriptions%20Generated%0A%20%20by%20Large%20Vision-Language%20Models%0AAuthor%3A%20Yuhang%20Huang%20and%20Zihan%20Wu%20and%20Chongyang%20Gao%20and%20Jiawei%20Peng%20and%20Xu%20Yang%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20are%20gaining%20traction%20for%20their%0Aremarkable%20ability%20to%20process%20and%20integrate%20visual%20and%20textual%20data.%20Despite%0Atheir%20popularity%2C%20the%20capacity%20of%20LVLMs%20to%20generate%20precise%2C%20fine-grained%0Atextual%20descriptions%20has%20not%20been%20fully%20explored.%20This%20study%20addresses%20this%20gap%0Aby%20focusing%20on%20%5Ctextit%7Bdistinctiveness%7D%20and%20%5Ctextit%7Bfidelity%7D%2C%20assessing%20how%0Amodels%20like%20Open-Flamingo%2C%20IDEFICS%2C%20and%20MiniGPT-4%20can%20distinguish%20between%0Asimilar%20objects%20and%20accurately%20describe%20visual%20features.%20We%20proposed%20the%0ATextual%20Retrieval-Augmented%20Classification%20%28TRAC%29%20framework%2C%20which%2C%20by%0Aleveraging%20its%20generative%20capabilities%2C%20allows%20us%20to%20delve%20deeper%20into%0Aanalyzing%20fine-grained%20visual%20description%20generation.%20This%20research%20provides%0Avaluable%20insights%20into%20the%20generation%20quality%20of%20LVLMs%2C%20enhancing%20the%0Aunderstanding%20of%20multimodal%20language%20models.%20Notably%2C%20MiniGPT-4%20stands%20out%20for%0Aits%20better%20ability%20to%20generate%20fine-grained%20descriptions%2C%20outperforming%20the%0Aother%20two%20models%20in%20this%20aspect.%20The%20code%20is%20provided%20at%0A%5Curl%7Bhttps%3A//anonymous.4open.science/r/Explore_FGVDs-E277%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17534v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Distinctiveness%20and%20Fidelity%20of%20the%20Descriptions%20Generated%0A%20%20by%20Large%20Vision-Language%20Models&entry.906535625=Yuhang%20Huang%20and%20Zihan%20Wu%20and%20Chongyang%20Gao%20and%20Jiawei%20Peng%20and%20Xu%20Yang&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20are%20gaining%20traction%20for%20their%0Aremarkable%20ability%20to%20process%20and%20integrate%20visual%20and%20textual%20data.%20Despite%0Atheir%20popularity%2C%20the%20capacity%20of%20LVLMs%20to%20generate%20precise%2C%20fine-grained%0Atextual%20descriptions%20has%20not%20been%20fully%20explored.%20This%20study%20addresses%20this%20gap%0Aby%20focusing%20on%20%5Ctextit%7Bdistinctiveness%7D%20and%20%5Ctextit%7Bfidelity%7D%2C%20assessing%20how%0Amodels%20like%20Open-Flamingo%2C%20IDEFICS%2C%20and%20MiniGPT-4%20can%20distinguish%20between%0Asimilar%20objects%20and%20accurately%20describe%20visual%20features.%20We%20proposed%20the%0ATextual%20Retrieval-Augmented%20Classification%20%28TRAC%29%20framework%2C%20which%2C%20by%0Aleveraging%20its%20generative%20capabilities%2C%20allows%20us%20to%20delve%20deeper%20into%0Aanalyzing%20fine-grained%20visual%20description%20generation.%20This%20research%20provides%0Avaluable%20insights%20into%20the%20generation%20quality%20of%20LVLMs%2C%20enhancing%20the%0Aunderstanding%20of%20multimodal%20language%20models.%20Notably%2C%20MiniGPT-4%20stands%20out%20for%0Aits%20better%20ability%20to%20generate%20fine-grained%20descriptions%2C%20outperforming%20the%0Aother%20two%20models%20in%20this%20aspect.%20The%20code%20is%20provided%20at%0A%5Curl%7Bhttps%3A//anonymous.4open.science/r/Explore_FGVDs-E277%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17534v1&entry.124074799=Read"},
{"title": "Introducing cosmosGPT: Monolingual Training for Turkish Language Models", "author": "H. Toprak Kesgin and M. Kaan Yuce and Eren Dogan and M. Egemen Uzun and Atahan Uz and H. Emre Seyrek and Ahmed Zeer and M. Fatih Amasyali", "abstract": "  The number of open source language models that can produce Turkish is\nincreasing day by day, as in other languages. In order to create the basic\nversions of such models, the training of multilingual models is usually\ncontinued with Turkish corpora. The alternative is to train the model with only\nTurkish corpora. In this study, we first introduce the cosmosGPT models that we\ncreated with this alternative method. Then, we introduce new finetune datasets\nfor basic language models to fulfill user requests and new evaluation datasets\nfor measuring the capabilities of Turkish language models. Finally, a\ncomprehensive comparison of the adapted Turkish language models on different\ncapabilities is presented. The results show that the language models we built\nwith the monolingual corpus have promising performance despite being about 10\ntimes smaller than the others.\n", "link": "http://arxiv.org/abs/2404.17336v1", "date": "2024-04-26", "relevancy": 1.6573, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4199}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4139}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4016}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Introducing%20cosmosGPT%3A%20Monolingual%20Training%20for%20Turkish%20Language%20Models&body=Title%3A%20Introducing%20cosmosGPT%3A%20Monolingual%20Training%20for%20Turkish%20Language%20Models%0AAuthor%3A%20H.%20Toprak%20Kesgin%20and%20M.%20Kaan%20Yuce%20and%20Eren%20Dogan%20and%20M.%20Egemen%20Uzun%20and%20Atahan%20Uz%20and%20H.%20Emre%20Seyrek%20and%20Ahmed%20Zeer%20and%20M.%20Fatih%20Amasyali%0AAbstract%3A%20%20%20The%20number%20of%20open%20source%20language%20models%20that%20can%20produce%20Turkish%20is%0Aincreasing%20day%20by%20day%2C%20as%20in%20other%20languages.%20In%20order%20to%20create%20the%20basic%0Aversions%20of%20such%20models%2C%20the%20training%20of%20multilingual%20models%20is%20usually%0Acontinued%20with%20Turkish%20corpora.%20The%20alternative%20is%20to%20train%20the%20model%20with%20only%0ATurkish%20corpora.%20In%20this%20study%2C%20we%20first%20introduce%20the%20cosmosGPT%20models%20that%20we%0Acreated%20with%20this%20alternative%20method.%20Then%2C%20we%20introduce%20new%20finetune%20datasets%0Afor%20basic%20language%20models%20to%20fulfill%20user%20requests%20and%20new%20evaluation%20datasets%0Afor%20measuring%20the%20capabilities%20of%20Turkish%20language%20models.%20Finally%2C%20a%0Acomprehensive%20comparison%20of%20the%20adapted%20Turkish%20language%20models%20on%20different%0Acapabilities%20is%20presented.%20The%20results%20show%20that%20the%20language%20models%20we%20built%0Awith%20the%20monolingual%20corpus%20have%20promising%20performance%20despite%20being%20about%2010%0Atimes%20smaller%20than%20the%20others.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17336v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Introducing%20cosmosGPT%3A%20Monolingual%20Training%20for%20Turkish%20Language%20Models&entry.906535625=H.%20Toprak%20Kesgin%20and%20M.%20Kaan%20Yuce%20and%20Eren%20Dogan%20and%20M.%20Egemen%20Uzun%20and%20Atahan%20Uz%20and%20H.%20Emre%20Seyrek%20and%20Ahmed%20Zeer%20and%20M.%20Fatih%20Amasyali&entry.1292438233=%20%20The%20number%20of%20open%20source%20language%20models%20that%20can%20produce%20Turkish%20is%0Aincreasing%20day%20by%20day%2C%20as%20in%20other%20languages.%20In%20order%20to%20create%20the%20basic%0Aversions%20of%20such%20models%2C%20the%20training%20of%20multilingual%20models%20is%20usually%0Acontinued%20with%20Turkish%20corpora.%20The%20alternative%20is%20to%20train%20the%20model%20with%20only%0ATurkish%20corpora.%20In%20this%20study%2C%20we%20first%20introduce%20the%20cosmosGPT%20models%20that%20we%0Acreated%20with%20this%20alternative%20method.%20Then%2C%20we%20introduce%20new%20finetune%20datasets%0Afor%20basic%20language%20models%20to%20fulfill%20user%20requests%20and%20new%20evaluation%20datasets%0Afor%20measuring%20the%20capabilities%20of%20Turkish%20language%20models.%20Finally%2C%20a%0Acomprehensive%20comparison%20of%20the%20adapted%20Turkish%20language%20models%20on%20different%0Acapabilities%20is%20presented.%20The%20results%20show%20that%20the%20language%20models%20we%20built%0Awith%20the%20monolingual%20corpus%20have%20promising%20performance%20despite%20being%20about%2010%0Atimes%20smaller%20than%20the%20others.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17336v1&entry.124074799=Read"},
{"title": "Learning World Models With Hierarchical Temporal Abstractions: A\n  Probabilistic Perspective", "author": "Vaisakh Shaj", "abstract": "  Machines that can replicate human intelligence with type 2 reasoning\ncapabilities should be able to reason at multiple levels of spatio-temporal\nabstractions and scales using internal world models. Devising formalisms to\ndevelop such internal world models, which accurately reflect the causal\nhierarchies inherent in the dynamics of the real world, is a critical research\nchallenge in the domains of artificial intelligence and machine learning. This\nthesis identifies several limitations with the prevalent use of state space\nmodels (SSMs) as internal world models and propose two new probabilistic\nformalisms namely Hidden-Parameter SSMs and Multi-Time Scale SSMs to address\nthese drawbacks. The structure of graphical models in both formalisms\nfacilitates scalable exact probabilistic inference using belief propagation, as\nwell as end-to-end learning via backpropagation through time. This approach\npermits the development of scalable, adaptive hierarchical world models capable\nof representing nonstationary dynamics across multiple temporal abstractions\nand scales. Moreover, these probabilistic formalisms integrate the concept of\nuncertainty in world states, thus improving the system's capacity to emulate\nthe stochastic nature of the real world and quantify the confidence in its\npredictions. The thesis also discuss how these formalisms are in line with\nrelated neuroscience literature on Bayesian brain hypothesis and predicitive\nprocessing. Our experiments on various real and simulated robots demonstrate\nthat our formalisms can match and in many cases exceed the performance of\ncontemporary transformer variants in making long-range future predictions. We\nconclude the thesis by reflecting on the limitations of our current models and\nsuggesting directions for future research.\n", "link": "http://arxiv.org/abs/2404.16078v2", "date": "2024-04-26", "relevancy": 1.6483, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6239}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5287}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5266}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20World%20Models%20With%20Hierarchical%20Temporal%20Abstractions%3A%20A%0A%20%20Probabilistic%20Perspective&body=Title%3A%20Learning%20World%20Models%20With%20Hierarchical%20Temporal%20Abstractions%3A%20A%0A%20%20Probabilistic%20Perspective%0AAuthor%3A%20Vaisakh%20Shaj%0AAbstract%3A%20%20%20Machines%20that%20can%20replicate%20human%20intelligence%20with%20type%202%20reasoning%0Acapabilities%20should%20be%20able%20to%20reason%20at%20multiple%20levels%20of%20spatio-temporal%0Aabstractions%20and%20scales%20using%20internal%20world%20models.%20Devising%20formalisms%20to%0Adevelop%20such%20internal%20world%20models%2C%20which%20accurately%20reflect%20the%20causal%0Ahierarchies%20inherent%20in%20the%20dynamics%20of%20the%20real%20world%2C%20is%20a%20critical%20research%0Achallenge%20in%20the%20domains%20of%20artificial%20intelligence%20and%20machine%20learning.%20This%0Athesis%20identifies%20several%20limitations%20with%20the%20prevalent%20use%20of%20state%20space%0Amodels%20%28SSMs%29%20as%20internal%20world%20models%20and%20propose%20two%20new%20probabilistic%0Aformalisms%20namely%20Hidden-Parameter%20SSMs%20and%20Multi-Time%20Scale%20SSMs%20to%20address%0Athese%20drawbacks.%20The%20structure%20of%20graphical%20models%20in%20both%20formalisms%0Afacilitates%20scalable%20exact%20probabilistic%20inference%20using%20belief%20propagation%2C%20as%0Awell%20as%20end-to-end%20learning%20via%20backpropagation%20through%20time.%20This%20approach%0Apermits%20the%20development%20of%20scalable%2C%20adaptive%20hierarchical%20world%20models%20capable%0Aof%20representing%20nonstationary%20dynamics%20across%20multiple%20temporal%20abstractions%0Aand%20scales.%20Moreover%2C%20these%20probabilistic%20formalisms%20integrate%20the%20concept%20of%0Auncertainty%20in%20world%20states%2C%20thus%20improving%20the%20system%27s%20capacity%20to%20emulate%0Athe%20stochastic%20nature%20of%20the%20real%20world%20and%20quantify%20the%20confidence%20in%20its%0Apredictions.%20The%20thesis%20also%20discuss%20how%20these%20formalisms%20are%20in%20line%20with%0Arelated%20neuroscience%20literature%20on%20Bayesian%20brain%20hypothesis%20and%20predicitive%0Aprocessing.%20Our%20experiments%20on%20various%20real%20and%20simulated%20robots%20demonstrate%0Athat%20our%20formalisms%20can%20match%20and%20in%20many%20cases%20exceed%20the%20performance%20of%0Acontemporary%20transformer%20variants%20in%20making%20long-range%20future%20predictions.%20We%0Aconclude%20the%20thesis%20by%20reflecting%20on%20the%20limitations%20of%20our%20current%20models%20and%0Asuggesting%20directions%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16078v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20World%20Models%20With%20Hierarchical%20Temporal%20Abstractions%3A%20A%0A%20%20Probabilistic%20Perspective&entry.906535625=Vaisakh%20Shaj&entry.1292438233=%20%20Machines%20that%20can%20replicate%20human%20intelligence%20with%20type%202%20reasoning%0Acapabilities%20should%20be%20able%20to%20reason%20at%20multiple%20levels%20of%20spatio-temporal%0Aabstractions%20and%20scales%20using%20internal%20world%20models.%20Devising%20formalisms%20to%0Adevelop%20such%20internal%20world%20models%2C%20which%20accurately%20reflect%20the%20causal%0Ahierarchies%20inherent%20in%20the%20dynamics%20of%20the%20real%20world%2C%20is%20a%20critical%20research%0Achallenge%20in%20the%20domains%20of%20artificial%20intelligence%20and%20machine%20learning.%20This%0Athesis%20identifies%20several%20limitations%20with%20the%20prevalent%20use%20of%20state%20space%0Amodels%20%28SSMs%29%20as%20internal%20world%20models%20and%20propose%20two%20new%20probabilistic%0Aformalisms%20namely%20Hidden-Parameter%20SSMs%20and%20Multi-Time%20Scale%20SSMs%20to%20address%0Athese%20drawbacks.%20The%20structure%20of%20graphical%20models%20in%20both%20formalisms%0Afacilitates%20scalable%20exact%20probabilistic%20inference%20using%20belief%20propagation%2C%20as%0Awell%20as%20end-to-end%20learning%20via%20backpropagation%20through%20time.%20This%20approach%0Apermits%20the%20development%20of%20scalable%2C%20adaptive%20hierarchical%20world%20models%20capable%0Aof%20representing%20nonstationary%20dynamics%20across%20multiple%20temporal%20abstractions%0Aand%20scales.%20Moreover%2C%20these%20probabilistic%20formalisms%20integrate%20the%20concept%20of%0Auncertainty%20in%20world%20states%2C%20thus%20improving%20the%20system%27s%20capacity%20to%20emulate%0Athe%20stochastic%20nature%20of%20the%20real%20world%20and%20quantify%20the%20confidence%20in%20its%0Apredictions.%20The%20thesis%20also%20discuss%20how%20these%20formalisms%20are%20in%20line%20with%0Arelated%20neuroscience%20literature%20on%20Bayesian%20brain%20hypothesis%20and%20predicitive%0Aprocessing.%20Our%20experiments%20on%20various%20real%20and%20simulated%20robots%20demonstrate%0Athat%20our%20formalisms%20can%20match%20and%20in%20many%20cases%20exceed%20the%20performance%20of%0Acontemporary%20transformer%20variants%20in%20making%20long-range%20future%20predictions.%20We%0Aconclude%20the%20thesis%20by%20reflecting%20on%20the%20limitations%20of%20our%20current%20models%20and%0Asuggesting%20directions%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16078v2&entry.124074799=Read"},
{"title": "Fast Evaluation of Additive Kernels: Feature Arrangement, Fourier\n  Methods, and Kernel Derivatives", "author": "Theresa Wagner and Franziska Nestler and Martin Stoll", "abstract": "  One of the main computational bottlenecks when working with kernel based\nlearning is dealing with the large and typically dense kernel matrix.\nTechniques dealing with fast approximations of the matrix vector product for\nthese kernel matrices typically deteriorate in their performance if the feature\nvectors reside in higher-dimensional feature spaces. We here present a\ntechnique based on the non-equispaced fast Fourier transform (NFFT) with\nrigorous error analysis. We show that this approach is also well suited to\nallow the approximation of the matrix that arises when the kernel is\ndifferentiated with respect to the kernel hyperparameters; a problem often\nfound in the training phase of methods such as Gaussian processes. We also\nprovide an error analysis for this case. We illustrate the performance of the\nadditive kernel scheme with fast matrix vector products on a number of data\nsets. Our code is available at https://github.com/wagnertheresa/NFFTAddKer\n", "link": "http://arxiv.org/abs/2404.17344v1", "date": "2024-04-26", "relevancy": 1.647, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4142}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4124}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4101}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fast%20Evaluation%20of%20Additive%20Kernels%3A%20Feature%20Arrangement%2C%20Fourier%0A%20%20Methods%2C%20and%20Kernel%20Derivatives&body=Title%3A%20Fast%20Evaluation%20of%20Additive%20Kernels%3A%20Feature%20Arrangement%2C%20Fourier%0A%20%20Methods%2C%20and%20Kernel%20Derivatives%0AAuthor%3A%20Theresa%20Wagner%20and%20Franziska%20Nestler%20and%20Martin%20Stoll%0AAbstract%3A%20%20%20One%20of%20the%20main%20computational%20bottlenecks%20when%20working%20with%20kernel%20based%0Alearning%20is%20dealing%20with%20the%20large%20and%20typically%20dense%20kernel%20matrix.%0ATechniques%20dealing%20with%20fast%20approximations%20of%20the%20matrix%20vector%20product%20for%0Athese%20kernel%20matrices%20typically%20deteriorate%20in%20their%20performance%20if%20the%20feature%0Avectors%20reside%20in%20higher-dimensional%20feature%20spaces.%20We%20here%20present%20a%0Atechnique%20based%20on%20the%20non-equispaced%20fast%20Fourier%20transform%20%28NFFT%29%20with%0Arigorous%20error%20analysis.%20We%20show%20that%20this%20approach%20is%20also%20well%20suited%20to%0Aallow%20the%20approximation%20of%20the%20matrix%20that%20arises%20when%20the%20kernel%20is%0Adifferentiated%20with%20respect%20to%20the%20kernel%20hyperparameters%3B%20a%20problem%20often%0Afound%20in%20the%20training%20phase%20of%20methods%20such%20as%20Gaussian%20processes.%20We%20also%0Aprovide%20an%20error%20analysis%20for%20this%20case.%20We%20illustrate%20the%20performance%20of%20the%0Aadditive%20kernel%20scheme%20with%20fast%20matrix%20vector%20products%20on%20a%20number%20of%20data%0Asets.%20Our%20code%20is%20available%20at%20https%3A//github.com/wagnertheresa/NFFTAddKer%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17344v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Evaluation%20of%20Additive%20Kernels%3A%20Feature%20Arrangement%2C%20Fourier%0A%20%20Methods%2C%20and%20Kernel%20Derivatives&entry.906535625=Theresa%20Wagner%20and%20Franziska%20Nestler%20and%20Martin%20Stoll&entry.1292438233=%20%20One%20of%20the%20main%20computational%20bottlenecks%20when%20working%20with%20kernel%20based%0Alearning%20is%20dealing%20with%20the%20large%20and%20typically%20dense%20kernel%20matrix.%0ATechniques%20dealing%20with%20fast%20approximations%20of%20the%20matrix%20vector%20product%20for%0Athese%20kernel%20matrices%20typically%20deteriorate%20in%20their%20performance%20if%20the%20feature%0Avectors%20reside%20in%20higher-dimensional%20feature%20spaces.%20We%20here%20present%20a%0Atechnique%20based%20on%20the%20non-equispaced%20fast%20Fourier%20transform%20%28NFFT%29%20with%0Arigorous%20error%20analysis.%20We%20show%20that%20this%20approach%20is%20also%20well%20suited%20to%0Aallow%20the%20approximation%20of%20the%20matrix%20that%20arises%20when%20the%20kernel%20is%0Adifferentiated%20with%20respect%20to%20the%20kernel%20hyperparameters%3B%20a%20problem%20often%0Afound%20in%20the%20training%20phase%20of%20methods%20such%20as%20Gaussian%20processes.%20We%20also%0Aprovide%20an%20error%20analysis%20for%20this%20case.%20We%20illustrate%20the%20performance%20of%20the%0Aadditive%20kernel%20scheme%20with%20fast%20matrix%20vector%20products%20on%20a%20number%20of%20data%0Asets.%20Our%20code%20is%20available%20at%20https%3A//github.com/wagnertheresa/NFFTAddKer%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17344v1&entry.124074799=Read"},
{"title": "Evaluations of Machine Learning Privacy Defenses are Misleading", "author": "Michael Aerni and Jie Zhang and Florian Tram\u00e8r", "abstract": "  Empirical defenses for machine learning privacy forgo the provable guarantees\nof differential privacy in the hope of achieving higher utility while resisting\nrealistic adversaries. We identify severe pitfalls in existing empirical\nprivacy evaluations (based on membership inference attacks) that result in\nmisleading conclusions. In particular, we show that prior evaluations fail to\ncharacterize the privacy leakage of the most vulnerable samples, use weak\nattacks, and avoid comparisons with practical differential privacy baselines.\nIn 5 case studies of empirical privacy defenses, we find that prior evaluations\nunderestimate privacy leakage by an order of magnitude. Under our stronger\nevaluation, none of the empirical defenses we study are competitive with a\nproperly tuned, high-utility DP-SGD baseline (with vacuous provable\nguarantees).\n", "link": "http://arxiv.org/abs/2404.17399v1", "date": "2024-04-26", "relevancy": 1.6375, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4593}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4042}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3946}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Evaluations%20of%20Machine%20Learning%20Privacy%20Defenses%20are%20Misleading&body=Title%3A%20Evaluations%20of%20Machine%20Learning%20Privacy%20Defenses%20are%20Misleading%0AAuthor%3A%20Michael%20Aerni%20and%20Jie%20Zhang%20and%20Florian%20Tram%C3%A8r%0AAbstract%3A%20%20%20Empirical%20defenses%20for%20machine%20learning%20privacy%20forgo%20the%20provable%20guarantees%0Aof%20differential%20privacy%20in%20the%20hope%20of%20achieving%20higher%20utility%20while%20resisting%0Arealistic%20adversaries.%20We%20identify%20severe%20pitfalls%20in%20existing%20empirical%0Aprivacy%20evaluations%20%28based%20on%20membership%20inference%20attacks%29%20that%20result%20in%0Amisleading%20conclusions.%20In%20particular%2C%20we%20show%20that%20prior%20evaluations%20fail%20to%0Acharacterize%20the%20privacy%20leakage%20of%20the%20most%20vulnerable%20samples%2C%20use%20weak%0Aattacks%2C%20and%20avoid%20comparisons%20with%20practical%20differential%20privacy%20baselines.%0AIn%205%20case%20studies%20of%20empirical%20privacy%20defenses%2C%20we%20find%20that%20prior%20evaluations%0Aunderestimate%20privacy%20leakage%20by%20an%20order%20of%20magnitude.%20Under%20our%20stronger%0Aevaluation%2C%20none%20of%20the%20empirical%20defenses%20we%20study%20are%20competitive%20with%20a%0Aproperly%20tuned%2C%20high-utility%20DP-SGD%20baseline%20%28with%20vacuous%20provable%0Aguarantees%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17399v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluations%20of%20Machine%20Learning%20Privacy%20Defenses%20are%20Misleading&entry.906535625=Michael%20Aerni%20and%20Jie%20Zhang%20and%20Florian%20Tram%C3%A8r&entry.1292438233=%20%20Empirical%20defenses%20for%20machine%20learning%20privacy%20forgo%20the%20provable%20guarantees%0Aof%20differential%20privacy%20in%20the%20hope%20of%20achieving%20higher%20utility%20while%20resisting%0Arealistic%20adversaries.%20We%20identify%20severe%20pitfalls%20in%20existing%20empirical%0Aprivacy%20evaluations%20%28based%20on%20membership%20inference%20attacks%29%20that%20result%20in%0Amisleading%20conclusions.%20In%20particular%2C%20we%20show%20that%20prior%20evaluations%20fail%20to%0Acharacterize%20the%20privacy%20leakage%20of%20the%20most%20vulnerable%20samples%2C%20use%20weak%0Aattacks%2C%20and%20avoid%20comparisons%20with%20practical%20differential%20privacy%20baselines.%0AIn%205%20case%20studies%20of%20empirical%20privacy%20defenses%2C%20we%20find%20that%20prior%20evaluations%0Aunderestimate%20privacy%20leakage%20by%20an%20order%20of%20magnitude.%20Under%20our%20stronger%0Aevaluation%2C%20none%20of%20the%20empirical%20defenses%20we%20study%20are%20competitive%20with%20a%0Aproperly%20tuned%2C%20high-utility%20DP-SGD%20baseline%20%28with%20vacuous%20provable%0Aguarantees%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17399v1&entry.124074799=Read"},
{"title": "A Novel Spike Transformer Network for Depth Estimation from Event\n  Cameras via Cross-modality Knowledge Distillation", "author": "Xin Zhang and Liangxiu Han and Tam Sobeih and Lianghao Han and Darren Dancey", "abstract": "  Depth estimation is crucial for interpreting complex environments, especially\nin areas such as autonomous vehicle navigation and robotics. Nonetheless,\nobtaining accurate depth readings from event camera data remains a formidable\nchallenge. Event cameras operate differently from traditional digital cameras,\ncontinuously capturing data and generating asynchronous binary spikes that\nencode time, location, and light intensity. Yet, the unique sampling mechanisms\nof event cameras render standard image based algorithms inadequate for\nprocessing spike data. This necessitates the development of innovative,\nspike-aware algorithms tailored for event cameras, a task compounded by the\nirregularity, continuity, noise, and spatial and temporal characteristics\ninherent in spiking data.Harnessing the strong generalization capabilities of\ntransformer neural networks for spatiotemporal data, we propose a purely\nspike-driven spike transformer network for depth estimation from spiking camera\ndata. To address performance limitations with Spiking Neural Networks (SNN), we\nintroduce a novel single-stage cross-modality knowledge transfer framework\nleveraging knowledge from a large vision foundational model of artificial\nneural networks (ANN) (DINOv2) to enhance the performance of SNNs with limited\ndata. Our experimental results on both synthetic and real datasets show\nsubstantial improvements over existing models, with notable gains in Absolute\nRelative and Square Relative errors (49% and 39.77% improvements over the\nbenchmark model Spike-T, respectively). Besides accuracy, the proposed model\nalso demonstrates reduced power consumptions, a critical factor for practical\napplications.\n", "link": "http://arxiv.org/abs/2404.17335v1", "date": "2024-04-26", "relevancy": 1.637, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5477}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5435}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5427}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Spike%20Transformer%20Network%20for%20Depth%20Estimation%20from%20Event%0A%20%20Cameras%20via%20Cross-modality%20Knowledge%20Distillation&body=Title%3A%20A%20Novel%20Spike%20Transformer%20Network%20for%20Depth%20Estimation%20from%20Event%0A%20%20Cameras%20via%20Cross-modality%20Knowledge%20Distillation%0AAuthor%3A%20Xin%20Zhang%20and%20Liangxiu%20Han%20and%20Tam%20Sobeih%20and%20Lianghao%20Han%20and%20Darren%20Dancey%0AAbstract%3A%20%20%20Depth%20estimation%20is%20crucial%20for%20interpreting%20complex%20environments%2C%20especially%0Ain%20areas%20such%20as%20autonomous%20vehicle%20navigation%20and%20robotics.%20Nonetheless%2C%0Aobtaining%20accurate%20depth%20readings%20from%20event%20camera%20data%20remains%20a%20formidable%0Achallenge.%20Event%20cameras%20operate%20differently%20from%20traditional%20digital%20cameras%2C%0Acontinuously%20capturing%20data%20and%20generating%20asynchronous%20binary%20spikes%20that%0Aencode%20time%2C%20location%2C%20and%20light%20intensity.%20Yet%2C%20the%20unique%20sampling%20mechanisms%0Aof%20event%20cameras%20render%20standard%20image%20based%20algorithms%20inadequate%20for%0Aprocessing%20spike%20data.%20This%20necessitates%20the%20development%20of%20innovative%2C%0Aspike-aware%20algorithms%20tailored%20for%20event%20cameras%2C%20a%20task%20compounded%20by%20the%0Airregularity%2C%20continuity%2C%20noise%2C%20and%20spatial%20and%20temporal%20characteristics%0Ainherent%20in%20spiking%20data.Harnessing%20the%20strong%20generalization%20capabilities%20of%0Atransformer%20neural%20networks%20for%20spatiotemporal%20data%2C%20we%20propose%20a%20purely%0Aspike-driven%20spike%20transformer%20network%20for%20depth%20estimation%20from%20spiking%20camera%0Adata.%20To%20address%20performance%20limitations%20with%20Spiking%20Neural%20Networks%20%28SNN%29%2C%20we%0Aintroduce%20a%20novel%20single-stage%20cross-modality%20knowledge%20transfer%20framework%0Aleveraging%20knowledge%20from%20a%20large%20vision%20foundational%20model%20of%20artificial%0Aneural%20networks%20%28ANN%29%20%28DINOv2%29%20to%20enhance%20the%20performance%20of%20SNNs%20with%20limited%0Adata.%20Our%20experimental%20results%20on%20both%20synthetic%20and%20real%20datasets%20show%0Asubstantial%20improvements%20over%20existing%20models%2C%20with%20notable%20gains%20in%20Absolute%0ARelative%20and%20Square%20Relative%20errors%20%2849%25%20and%2039.77%25%20improvements%20over%20the%0Abenchmark%20model%20Spike-T%2C%20respectively%29.%20Besides%20accuracy%2C%20the%20proposed%20model%0Aalso%20demonstrates%20reduced%20power%20consumptions%2C%20a%20critical%20factor%20for%20practical%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17335v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Spike%20Transformer%20Network%20for%20Depth%20Estimation%20from%20Event%0A%20%20Cameras%20via%20Cross-modality%20Knowledge%20Distillation&entry.906535625=Xin%20Zhang%20and%20Liangxiu%20Han%20and%20Tam%20Sobeih%20and%20Lianghao%20Han%20and%20Darren%20Dancey&entry.1292438233=%20%20Depth%20estimation%20is%20crucial%20for%20interpreting%20complex%20environments%2C%20especially%0Ain%20areas%20such%20as%20autonomous%20vehicle%20navigation%20and%20robotics.%20Nonetheless%2C%0Aobtaining%20accurate%20depth%20readings%20from%20event%20camera%20data%20remains%20a%20formidable%0Achallenge.%20Event%20cameras%20operate%20differently%20from%20traditional%20digital%20cameras%2C%0Acontinuously%20capturing%20data%20and%20generating%20asynchronous%20binary%20spikes%20that%0Aencode%20time%2C%20location%2C%20and%20light%20intensity.%20Yet%2C%20the%20unique%20sampling%20mechanisms%0Aof%20event%20cameras%20render%20standard%20image%20based%20algorithms%20inadequate%20for%0Aprocessing%20spike%20data.%20This%20necessitates%20the%20development%20of%20innovative%2C%0Aspike-aware%20algorithms%20tailored%20for%20event%20cameras%2C%20a%20task%20compounded%20by%20the%0Airregularity%2C%20continuity%2C%20noise%2C%20and%20spatial%20and%20temporal%20characteristics%0Ainherent%20in%20spiking%20data.Harnessing%20the%20strong%20generalization%20capabilities%20of%0Atransformer%20neural%20networks%20for%20spatiotemporal%20data%2C%20we%20propose%20a%20purely%0Aspike-driven%20spike%20transformer%20network%20for%20depth%20estimation%20from%20spiking%20camera%0Adata.%20To%20address%20performance%20limitations%20with%20Spiking%20Neural%20Networks%20%28SNN%29%2C%20we%0Aintroduce%20a%20novel%20single-stage%20cross-modality%20knowledge%20transfer%20framework%0Aleveraging%20knowledge%20from%20a%20large%20vision%20foundational%20model%20of%20artificial%0Aneural%20networks%20%28ANN%29%20%28DINOv2%29%20to%20enhance%20the%20performance%20of%20SNNs%20with%20limited%0Adata.%20Our%20experimental%20results%20on%20both%20synthetic%20and%20real%20datasets%20show%0Asubstantial%20improvements%20over%20existing%20models%2C%20with%20notable%20gains%20in%20Absolute%0ARelative%20and%20Square%20Relative%20errors%20%2849%25%20and%2039.77%25%20improvements%20over%20the%0Abenchmark%20model%20Spike-T%2C%20respectively%29.%20Besides%20accuracy%2C%20the%20proposed%20model%0Aalso%20demonstrates%20reduced%20power%20consumptions%2C%20a%20critical%20factor%20for%20practical%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17335v1&entry.124074799=Read"},
{"title": "Situational Graphs for Robotic First Responders: an application to\n  dismantling drug labs", "author": "W. J. Meijer and A. C. Kemmeren and J. M. van Bruggen and T. Haije and J. E. Fransman and J. D. van Mil", "abstract": "  In this work, we support experts in the safety domain with safer dismantling\nof drug labs, by deploying robots for the initial inspection. Being able to act\non the discovered environment is key to enabling this (semi-)autonomous\ninspection, e.g. to open doors or take a closer at suspicious items. Our\napproach addresses this with a novel environmental representation, the\nBehavior-Oriented Situational Graph, where we extend on the classical\nsituational graph by merging a perception-driven backbone with prior actionable\nknowledge via a situational affordance schema. Linking situations to robot\nbehaviors facilitates both autonomous mission planning and situational\nunderstanding of the operator. Planning over the graph is easier and faster,\nsince it directly incorporates actionable information, which is critical for\nonline mission systems. Moreover, the representation allows the human operator\nto seamlessly transition between different levels of autonomy of the robot,\nfrom remote control to behavior execution to full autonomous exploration. We\ntest the effectiveness of our approach in a real-world drug lab scenario at a\nDutch police training facility using a mobile Spot robot and use the results to\niterate on the system design.\n", "link": "http://arxiv.org/abs/2404.17395v1", "date": "2024-04-26", "relevancy": 1.6331, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.7071}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5373}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4821}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Situational%20Graphs%20for%20Robotic%20First%20Responders%3A%20an%20application%20to%0A%20%20dismantling%20drug%20labs&body=Title%3A%20Situational%20Graphs%20for%20Robotic%20First%20Responders%3A%20an%20application%20to%0A%20%20dismantling%20drug%20labs%0AAuthor%3A%20W.%20J.%20Meijer%20and%20A.%20C.%20Kemmeren%20and%20J.%20M.%20van%20Bruggen%20and%20T.%20Haije%20and%20J.%20E.%20Fransman%20and%20J.%20D.%20van%20Mil%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20support%20experts%20in%20the%20safety%20domain%20with%20safer%20dismantling%0Aof%20drug%20labs%2C%20by%20deploying%20robots%20for%20the%20initial%20inspection.%20Being%20able%20to%20act%0Aon%20the%20discovered%20environment%20is%20key%20to%20enabling%20this%20%28semi-%29autonomous%0Ainspection%2C%20e.g.%20to%20open%20doors%20or%20take%20a%20closer%20at%20suspicious%20items.%20Our%0Aapproach%20addresses%20this%20with%20a%20novel%20environmental%20representation%2C%20the%0ABehavior-Oriented%20Situational%20Graph%2C%20where%20we%20extend%20on%20the%20classical%0Asituational%20graph%20by%20merging%20a%20perception-driven%20backbone%20with%20prior%20actionable%0Aknowledge%20via%20a%20situational%20affordance%20schema.%20Linking%20situations%20to%20robot%0Abehaviors%20facilitates%20both%20autonomous%20mission%20planning%20and%20situational%0Aunderstanding%20of%20the%20operator.%20Planning%20over%20the%20graph%20is%20easier%20and%20faster%2C%0Asince%20it%20directly%20incorporates%20actionable%20information%2C%20which%20is%20critical%20for%0Aonline%20mission%20systems.%20Moreover%2C%20the%20representation%20allows%20the%20human%20operator%0Ato%20seamlessly%20transition%20between%20different%20levels%20of%20autonomy%20of%20the%20robot%2C%0Afrom%20remote%20control%20to%20behavior%20execution%20to%20full%20autonomous%20exploration.%20We%0Atest%20the%20effectiveness%20of%20our%20approach%20in%20a%20real-world%20drug%20lab%20scenario%20at%20a%0ADutch%20police%20training%20facility%20using%20a%20mobile%20Spot%20robot%20and%20use%20the%20results%20to%0Aiterate%20on%20the%20system%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17395v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Situational%20Graphs%20for%20Robotic%20First%20Responders%3A%20an%20application%20to%0A%20%20dismantling%20drug%20labs&entry.906535625=W.%20J.%20Meijer%20and%20A.%20C.%20Kemmeren%20and%20J.%20M.%20van%20Bruggen%20and%20T.%20Haije%20and%20J.%20E.%20Fransman%20and%20J.%20D.%20van%20Mil&entry.1292438233=%20%20In%20this%20work%2C%20we%20support%20experts%20in%20the%20safety%20domain%20with%20safer%20dismantling%0Aof%20drug%20labs%2C%20by%20deploying%20robots%20for%20the%20initial%20inspection.%20Being%20able%20to%20act%0Aon%20the%20discovered%20environment%20is%20key%20to%20enabling%20this%20%28semi-%29autonomous%0Ainspection%2C%20e.g.%20to%20open%20doors%20or%20take%20a%20closer%20at%20suspicious%20items.%20Our%0Aapproach%20addresses%20this%20with%20a%20novel%20environmental%20representation%2C%20the%0ABehavior-Oriented%20Situational%20Graph%2C%20where%20we%20extend%20on%20the%20classical%0Asituational%20graph%20by%20merging%20a%20perception-driven%20backbone%20with%20prior%20actionable%0Aknowledge%20via%20a%20situational%20affordance%20schema.%20Linking%20situations%20to%20robot%0Abehaviors%20facilitates%20both%20autonomous%20mission%20planning%20and%20situational%0Aunderstanding%20of%20the%20operator.%20Planning%20over%20the%20graph%20is%20easier%20and%20faster%2C%0Asince%20it%20directly%20incorporates%20actionable%20information%2C%20which%20is%20critical%20for%0Aonline%20mission%20systems.%20Moreover%2C%20the%20representation%20allows%20the%20human%20operator%0Ato%20seamlessly%20transition%20between%20different%20levels%20of%20autonomy%20of%20the%20robot%2C%0Afrom%20remote%20control%20to%20behavior%20execution%20to%20full%20autonomous%20exploration.%20We%0Atest%20the%20effectiveness%20of%20our%20approach%20in%20a%20real-world%20drug%20lab%20scenario%20at%20a%0ADutch%20police%20training%20facility%20using%20a%20mobile%20Spot%20robot%20and%20use%20the%20results%20to%0Aiterate%20on%20the%20system%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17395v1&entry.124074799=Read"},
{"title": "Inhomogeneous illuminated image enhancement under extremely low\n  visibility condition", "author": "Libang Chen and Yikun Liu and Jianying Zhou", "abstract": "  Imaging through fog significantly impacts fields such as object detection and\nrecognition. In conditions of extremely low visibility, essential image\ninformation can be obscured, rendering standard extraction methods ineffective.\nTraditional digital processing techniques, such as histogram stretching, aim to\nmitigate fog effects by enhancing object light contrast diminished by\natmospheric scattering. However, these methods often experience reduce\neffectiveness under inhomogeneous illumination. This paper introduces a novel\napproach that adaptively filters background illumination under extremely low\nvisibility and preserve only the essential signal information. Additionally, we\nemploy a visual optimization strategy based on image gradients to eliminate\ngrayscale banding. Finally, the image is transformed to achieve high contrast\nand maintain fidelity to the original information through maximum histogram\nequalization. Our proposed method significantly enhances signal clarity in\nconditions of extremely low visibility and outperforms existing algorithms.\n", "link": "http://arxiv.org/abs/2404.17503v1", "date": "2024-04-26", "relevancy": 1.6258, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5532}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.543}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5279}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Inhomogeneous%20illuminated%20image%20enhancement%20under%20extremely%20low%0A%20%20visibility%20condition&body=Title%3A%20Inhomogeneous%20illuminated%20image%20enhancement%20under%20extremely%20low%0A%20%20visibility%20condition%0AAuthor%3A%20Libang%20Chen%20and%20Yikun%20Liu%20and%20Jianying%20Zhou%0AAbstract%3A%20%20%20Imaging%20through%20fog%20significantly%20impacts%20fields%20such%20as%20object%20detection%20and%0Arecognition.%20In%20conditions%20of%20extremely%20low%20visibility%2C%20essential%20image%0Ainformation%20can%20be%20obscured%2C%20rendering%20standard%20extraction%20methods%20ineffective.%0ATraditional%20digital%20processing%20techniques%2C%20such%20as%20histogram%20stretching%2C%20aim%20to%0Amitigate%20fog%20effects%20by%20enhancing%20object%20light%20contrast%20diminished%20by%0Aatmospheric%20scattering.%20However%2C%20these%20methods%20often%20experience%20reduce%0Aeffectiveness%20under%20inhomogeneous%20illumination.%20This%20paper%20introduces%20a%20novel%0Aapproach%20that%20adaptively%20filters%20background%20illumination%20under%20extremely%20low%0Avisibility%20and%20preserve%20only%20the%20essential%20signal%20information.%20Additionally%2C%20we%0Aemploy%20a%20visual%20optimization%20strategy%20based%20on%20image%20gradients%20to%20eliminate%0Agrayscale%20banding.%20Finally%2C%20the%20image%20is%20transformed%20to%20achieve%20high%20contrast%0Aand%20maintain%20fidelity%20to%20the%20original%20information%20through%20maximum%20histogram%0Aequalization.%20Our%20proposed%20method%20significantly%20enhances%20signal%20clarity%20in%0Aconditions%20of%20extremely%20low%20visibility%20and%20outperforms%20existing%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17503v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inhomogeneous%20illuminated%20image%20enhancement%20under%20extremely%20low%0A%20%20visibility%20condition&entry.906535625=Libang%20Chen%20and%20Yikun%20Liu%20and%20Jianying%20Zhou&entry.1292438233=%20%20Imaging%20through%20fog%20significantly%20impacts%20fields%20such%20as%20object%20detection%20and%0Arecognition.%20In%20conditions%20of%20extremely%20low%20visibility%2C%20essential%20image%0Ainformation%20can%20be%20obscured%2C%20rendering%20standard%20extraction%20methods%20ineffective.%0ATraditional%20digital%20processing%20techniques%2C%20such%20as%20histogram%20stretching%2C%20aim%20to%0Amitigate%20fog%20effects%20by%20enhancing%20object%20light%20contrast%20diminished%20by%0Aatmospheric%20scattering.%20However%2C%20these%20methods%20often%20experience%20reduce%0Aeffectiveness%20under%20inhomogeneous%20illumination.%20This%20paper%20introduces%20a%20novel%0Aapproach%20that%20adaptively%20filters%20background%20illumination%20under%20extremely%20low%0Avisibility%20and%20preserve%20only%20the%20essential%20signal%20information.%20Additionally%2C%20we%0Aemploy%20a%20visual%20optimization%20strategy%20based%20on%20image%20gradients%20to%20eliminate%0Agrayscale%20banding.%20Finally%2C%20the%20image%20is%20transformed%20to%20achieve%20high%20contrast%0Aand%20maintain%20fidelity%20to%20the%20original%20information%20through%20maximum%20histogram%0Aequalization.%20Our%20proposed%20method%20significantly%20enhances%20signal%20clarity%20in%0Aconditions%20of%20extremely%20low%20visibility%20and%20outperforms%20existing%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17503v1&entry.124074799=Read"},
{"title": "Learning to Visually Connect Actions and their Effects", "author": "Eric Peh and Paritosh Parmar and Basura Fernando", "abstract": "  In this work, we introduce the novel concept of visually Connecting Actions\nand Their Effects (CATE) in video understanding. CATE can have applications in\nareas like task planning and learning from demonstration. We identify and\nexplore two different aspects of the concept of CATE: Action Selection and\nEffect-Affinity Assessment, where video understanding models connect actions\nand effects at semantic and fine-grained levels, respectively. We observe that\ndifferent formulations produce representations capturing intuitive action\nproperties. We also design various baseline models for Action Selection and\nEffect-Affinity Assessment. Despite the intuitive nature of the task, we\nobserve that models struggle, and humans outperform them by a large margin. The\nstudy aims to establish a foundation for future efforts, showcasing the\nflexibility and versatility of connecting actions and effects in video\nunderstanding, with the hope of inspiring advanced formulations and models.\n", "link": "http://arxiv.org/abs/2401.10805v2", "date": "2024-04-26", "relevancy": 1.605, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5471}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5362}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5297}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Visually%20Connect%20Actions%20and%20their%20Effects&body=Title%3A%20Learning%20to%20Visually%20Connect%20Actions%20and%20their%20Effects%0AAuthor%3A%20Eric%20Peh%20and%20Paritosh%20Parmar%20and%20Basura%20Fernando%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20the%20novel%20concept%20of%20visually%20Connecting%20Actions%0Aand%20Their%20Effects%20%28CATE%29%20in%20video%20understanding.%20CATE%20can%20have%20applications%20in%0Aareas%20like%20task%20planning%20and%20learning%20from%20demonstration.%20We%20identify%20and%0Aexplore%20two%20different%20aspects%20of%20the%20concept%20of%20CATE%3A%20Action%20Selection%20and%0AEffect-Affinity%20Assessment%2C%20where%20video%20understanding%20models%20connect%20actions%0Aand%20effects%20at%20semantic%20and%20fine-grained%20levels%2C%20respectively.%20We%20observe%20that%0Adifferent%20formulations%20produce%20representations%20capturing%20intuitive%20action%0Aproperties.%20We%20also%20design%20various%20baseline%20models%20for%20Action%20Selection%20and%0AEffect-Affinity%20Assessment.%20Despite%20the%20intuitive%20nature%20of%20the%20task%2C%20we%0Aobserve%20that%20models%20struggle%2C%20and%20humans%20outperform%20them%20by%20a%20large%20margin.%20The%0Astudy%20aims%20to%20establish%20a%20foundation%20for%20future%20efforts%2C%20showcasing%20the%0Aflexibility%20and%20versatility%20of%20connecting%20actions%20and%20effects%20in%20video%0Aunderstanding%2C%20with%20the%20hope%20of%20inspiring%20advanced%20formulations%20and%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10805v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Visually%20Connect%20Actions%20and%20their%20Effects&entry.906535625=Eric%20Peh%20and%20Paritosh%20Parmar%20and%20Basura%20Fernando&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20the%20novel%20concept%20of%20visually%20Connecting%20Actions%0Aand%20Their%20Effects%20%28CATE%29%20in%20video%20understanding.%20CATE%20can%20have%20applications%20in%0Aareas%20like%20task%20planning%20and%20learning%20from%20demonstration.%20We%20identify%20and%0Aexplore%20two%20different%20aspects%20of%20the%20concept%20of%20CATE%3A%20Action%20Selection%20and%0AEffect-Affinity%20Assessment%2C%20where%20video%20understanding%20models%20connect%20actions%0Aand%20effects%20at%20semantic%20and%20fine-grained%20levels%2C%20respectively.%20We%20observe%20that%0Adifferent%20formulations%20produce%20representations%20capturing%20intuitive%20action%0Aproperties.%20We%20also%20design%20various%20baseline%20models%20for%20Action%20Selection%20and%0AEffect-Affinity%20Assessment.%20Despite%20the%20intuitive%20nature%20of%20the%20task%2C%20we%0Aobserve%20that%20models%20struggle%2C%20and%20humans%20outperform%20them%20by%20a%20large%20margin.%20The%0Astudy%20aims%20to%20establish%20a%20foundation%20for%20future%20efforts%2C%20showcasing%20the%0Aflexibility%20and%20versatility%20of%20connecting%20actions%20and%20effects%20in%20video%0Aunderstanding%2C%20with%20the%20hope%20of%20inspiring%20advanced%20formulations%20and%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10805v2&entry.124074799=Read"},
{"title": "Large Language Model Agent as a Mechanical Designer", "author": "Yayati Jadhav and Amir Barati Farimani", "abstract": "  Conventional mechanical design paradigms rely on experts systematically\nrefining concepts through experience-guided modification and FEA to meet\nspecific requirements. However, this approach can be time-consuming and heavily\ndependent on prior knowledge and experience. While numerous machine learning\nmodels have been developed to streamline this intensive and expert-driven\niterative process, these methods typically demand extensive training data and\nconsiderable computational resources. Furthermore, methods based on deep\nlearning are usually restricted to the specific domains and tasks for which\nthey were trained, limiting their applicability across different tasks. This\ncreates a trade-off between the efficiency of automation and the demand for\nresources. In this study, we present a novel approach that integrates\npre-trained LLMs with a FEM module. The FEM module evaluates each design and\nprovides essential feedback, guiding the LLMs to continuously learn, plan,\ngenerate, and optimize designs without the need for domain-specific training.\nWe demonstrate the effectiveness of our proposed framework in managing the\niterative optimization of truss structures, showcasing its capability to reason\nabout and refine designs according to structured feedback and criteria. Our\nresults reveal that these LLM-based agents can successfully generate truss\ndesigns that comply with natural language specifications with a success rate of\nup to 90%, which varies according to the applied constraints. By employing\nprompt-based optimization techniques we show that LLM based agents exhibit\noptimization behavior when provided with solution-score pairs to iteratively\nrefine designs to meet specifications. This ability of LLM agents to produce\nviable designs and optimize them based on their inherent reasoning capabilities\nhighlights their potential to develop and implement effective design strategies\nautonomously.\n", "link": "http://arxiv.org/abs/2404.17525v1", "date": "2024-04-26", "relevancy": 1.5583, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5284}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5173}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5158}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model%20Agent%20as%20a%20Mechanical%20Designer&body=Title%3A%20Large%20Language%20Model%20Agent%20as%20a%20Mechanical%20Designer%0AAuthor%3A%20Yayati%20Jadhav%20and%20Amir%20Barati%20Farimani%0AAbstract%3A%20%20%20Conventional%20mechanical%20design%20paradigms%20rely%20on%20experts%20systematically%0Arefining%20concepts%20through%20experience-guided%20modification%20and%20FEA%20to%20meet%0Aspecific%20requirements.%20However%2C%20this%20approach%20can%20be%20time-consuming%20and%20heavily%0Adependent%20on%20prior%20knowledge%20and%20experience.%20While%20numerous%20machine%20learning%0Amodels%20have%20been%20developed%20to%20streamline%20this%20intensive%20and%20expert-driven%0Aiterative%20process%2C%20these%20methods%20typically%20demand%20extensive%20training%20data%20and%0Aconsiderable%20computational%20resources.%20Furthermore%2C%20methods%20based%20on%20deep%0Alearning%20are%20usually%20restricted%20to%20the%20specific%20domains%20and%20tasks%20for%20which%0Athey%20were%20trained%2C%20limiting%20their%20applicability%20across%20different%20tasks.%20This%0Acreates%20a%20trade-off%20between%20the%20efficiency%20of%20automation%20and%20the%20demand%20for%0Aresources.%20In%20this%20study%2C%20we%20present%20a%20novel%20approach%20that%20integrates%0Apre-trained%20LLMs%20with%20a%20FEM%20module.%20The%20FEM%20module%20evaluates%20each%20design%20and%0Aprovides%20essential%20feedback%2C%20guiding%20the%20LLMs%20to%20continuously%20learn%2C%20plan%2C%0Agenerate%2C%20and%20optimize%20designs%20without%20the%20need%20for%20domain-specific%20training.%0AWe%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20framework%20in%20managing%20the%0Aiterative%20optimization%20of%20truss%20structures%2C%20showcasing%20its%20capability%20to%20reason%0Aabout%20and%20refine%20designs%20according%20to%20structured%20feedback%20and%20criteria.%20Our%0Aresults%20reveal%20that%20these%20LLM-based%20agents%20can%20successfully%20generate%20truss%0Adesigns%20that%20comply%20with%20natural%20language%20specifications%20with%20a%20success%20rate%20of%0Aup%20to%2090%25%2C%20which%20varies%20according%20to%20the%20applied%20constraints.%20By%20employing%0Aprompt-based%20optimization%20techniques%20we%20show%20that%20LLM%20based%20agents%20exhibit%0Aoptimization%20behavior%20when%20provided%20with%20solution-score%20pairs%20to%20iteratively%0Arefine%20designs%20to%20meet%20specifications.%20This%20ability%20of%20LLM%20agents%20to%20produce%0Aviable%20designs%20and%20optimize%20them%20based%20on%20their%20inherent%20reasoning%20capabilities%0Ahighlights%20their%20potential%20to%20develop%20and%20implement%20effective%20design%20strategies%0Aautonomously.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17525v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model%20Agent%20as%20a%20Mechanical%20Designer&entry.906535625=Yayati%20Jadhav%20and%20Amir%20Barati%20Farimani&entry.1292438233=%20%20Conventional%20mechanical%20design%20paradigms%20rely%20on%20experts%20systematically%0Arefining%20concepts%20through%20experience-guided%20modification%20and%20FEA%20to%20meet%0Aspecific%20requirements.%20However%2C%20this%20approach%20can%20be%20time-consuming%20and%20heavily%0Adependent%20on%20prior%20knowledge%20and%20experience.%20While%20numerous%20machine%20learning%0Amodels%20have%20been%20developed%20to%20streamline%20this%20intensive%20and%20expert-driven%0Aiterative%20process%2C%20these%20methods%20typically%20demand%20extensive%20training%20data%20and%0Aconsiderable%20computational%20resources.%20Furthermore%2C%20methods%20based%20on%20deep%0Alearning%20are%20usually%20restricted%20to%20the%20specific%20domains%20and%20tasks%20for%20which%0Athey%20were%20trained%2C%20limiting%20their%20applicability%20across%20different%20tasks.%20This%0Acreates%20a%20trade-off%20between%20the%20efficiency%20of%20automation%20and%20the%20demand%20for%0Aresources.%20In%20this%20study%2C%20we%20present%20a%20novel%20approach%20that%20integrates%0Apre-trained%20LLMs%20with%20a%20FEM%20module.%20The%20FEM%20module%20evaluates%20each%20design%20and%0Aprovides%20essential%20feedback%2C%20guiding%20the%20LLMs%20to%20continuously%20learn%2C%20plan%2C%0Agenerate%2C%20and%20optimize%20designs%20without%20the%20need%20for%20domain-specific%20training.%0AWe%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20framework%20in%20managing%20the%0Aiterative%20optimization%20of%20truss%20structures%2C%20showcasing%20its%20capability%20to%20reason%0Aabout%20and%20refine%20designs%20according%20to%20structured%20feedback%20and%20criteria.%20Our%0Aresults%20reveal%20that%20these%20LLM-based%20agents%20can%20successfully%20generate%20truss%0Adesigns%20that%20comply%20with%20natural%20language%20specifications%20with%20a%20success%20rate%20of%0Aup%20to%2090%25%2C%20which%20varies%20according%20to%20the%20applied%20constraints.%20By%20employing%0Aprompt-based%20optimization%20techniques%20we%20show%20that%20LLM%20based%20agents%20exhibit%0Aoptimization%20behavior%20when%20provided%20with%20solution-score%20pairs%20to%20iteratively%0Arefine%20designs%20to%20meet%20specifications.%20This%20ability%20of%20LLM%20agents%20to%20produce%0Aviable%20designs%20and%20optimize%20them%20based%20on%20their%20inherent%20reasoning%20capabilities%0Ahighlights%20their%20potential%20to%20develop%20and%20implement%20effective%20design%20strategies%0Aautonomously.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17525v1&entry.124074799=Read"},
{"title": "Narrative Action Evaluation with Prompt-Guided Multimodal Interaction", "author": "Shiyi Zhang and Sule Bai and Guangyi Chen and Lei Chen and Jiwen Lu and Junle Wang and Yansong Tang", "abstract": "  In this paper, we investigate a new problem called narrative action\nevaluation (NAE). NAE aims to generate professional commentary that evaluates\nthe execution of an action. Unlike traditional tasks such as score-based action\nquality assessment and video captioning involving superficial sentences, NAE\nfocuses on creating detailed narratives in natural language. These narratives\nprovide intricate descriptions of actions along with objective evaluations. NAE\nis a more challenging task because it requires both narrative flexibility and\nevaluation rigor. One existing possible solution is to use multi-task learning,\nwhere narrative language and evaluative information are predicted separately.\nHowever, this approach results in reduced performance for individual tasks\nbecause of variations between tasks and differences in modality between\nlanguage information and evaluation information. To address this, we propose a\nprompt-guided multimodal interaction framework. This framework utilizes a pair\nof transformers to facilitate the interaction between different modalities of\ninformation. It also uses prompts to transform the score regression task into a\nvideo-text matching task, thus enabling task interactivity. To support further\nresearch in this field, we re-annotate the MTL-AQA and FineGym datasets with\nhigh-quality and comprehensive action narration. Additionally, we establish\nbenchmarks for NAE. Extensive experiment results prove that our method\noutperforms separate learning methods and naive multi-task learning methods.\nData and code are released at https://github.com/shiyi-zh0408/NAE_CVPR2024.\n", "link": "http://arxiv.org/abs/2404.14471v2", "date": "2024-04-26", "relevancy": 1.5576, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5242}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.52}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5169}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Narrative%20Action%20Evaluation%20with%20Prompt-Guided%20Multimodal%20Interaction&body=Title%3A%20Narrative%20Action%20Evaluation%20with%20Prompt-Guided%20Multimodal%20Interaction%0AAuthor%3A%20Shiyi%20Zhang%20and%20Sule%20Bai%20and%20Guangyi%20Chen%20and%20Lei%20Chen%20and%20Jiwen%20Lu%20and%20Junle%20Wang%20and%20Yansong%20Tang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20a%20new%20problem%20called%20narrative%20action%0Aevaluation%20%28NAE%29.%20NAE%20aims%20to%20generate%20professional%20commentary%20that%20evaluates%0Athe%20execution%20of%20an%20action.%20Unlike%20traditional%20tasks%20such%20as%20score-based%20action%0Aquality%20assessment%20and%20video%20captioning%20involving%20superficial%20sentences%2C%20NAE%0Afocuses%20on%20creating%20detailed%20narratives%20in%20natural%20language.%20These%20narratives%0Aprovide%20intricate%20descriptions%20of%20actions%20along%20with%20objective%20evaluations.%20NAE%0Ais%20a%20more%20challenging%20task%20because%20it%20requires%20both%20narrative%20flexibility%20and%0Aevaluation%20rigor.%20One%20existing%20possible%20solution%20is%20to%20use%20multi-task%20learning%2C%0Awhere%20narrative%20language%20and%20evaluative%20information%20are%20predicted%20separately.%0AHowever%2C%20this%20approach%20results%20in%20reduced%20performance%20for%20individual%20tasks%0Abecause%20of%20variations%20between%20tasks%20and%20differences%20in%20modality%20between%0Alanguage%20information%20and%20evaluation%20information.%20To%20address%20this%2C%20we%20propose%20a%0Aprompt-guided%20multimodal%20interaction%20framework.%20This%20framework%20utilizes%20a%20pair%0Aof%20transformers%20to%20facilitate%20the%20interaction%20between%20different%20modalities%20of%0Ainformation.%20It%20also%20uses%20prompts%20to%20transform%20the%20score%20regression%20task%20into%20a%0Avideo-text%20matching%20task%2C%20thus%20enabling%20task%20interactivity.%20To%20support%20further%0Aresearch%20in%20this%20field%2C%20we%20re-annotate%20the%20MTL-AQA%20and%20FineGym%20datasets%20with%0Ahigh-quality%20and%20comprehensive%20action%20narration.%20Additionally%2C%20we%20establish%0Abenchmarks%20for%20NAE.%20Extensive%20experiment%20results%20prove%20that%20our%20method%0Aoutperforms%20separate%20learning%20methods%20and%20naive%20multi-task%20learning%20methods.%0AData%20and%20code%20are%20released%20at%20https%3A//github.com/shiyi-zh0408/NAE_CVPR2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14471v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Narrative%20Action%20Evaluation%20with%20Prompt-Guided%20Multimodal%20Interaction&entry.906535625=Shiyi%20Zhang%20and%20Sule%20Bai%20and%20Guangyi%20Chen%20and%20Lei%20Chen%20and%20Jiwen%20Lu%20and%20Junle%20Wang%20and%20Yansong%20Tang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20a%20new%20problem%20called%20narrative%20action%0Aevaluation%20%28NAE%29.%20NAE%20aims%20to%20generate%20professional%20commentary%20that%20evaluates%0Athe%20execution%20of%20an%20action.%20Unlike%20traditional%20tasks%20such%20as%20score-based%20action%0Aquality%20assessment%20and%20video%20captioning%20involving%20superficial%20sentences%2C%20NAE%0Afocuses%20on%20creating%20detailed%20narratives%20in%20natural%20language.%20These%20narratives%0Aprovide%20intricate%20descriptions%20of%20actions%20along%20with%20objective%20evaluations.%20NAE%0Ais%20a%20more%20challenging%20task%20because%20it%20requires%20both%20narrative%20flexibility%20and%0Aevaluation%20rigor.%20One%20existing%20possible%20solution%20is%20to%20use%20multi-task%20learning%2C%0Awhere%20narrative%20language%20and%20evaluative%20information%20are%20predicted%20separately.%0AHowever%2C%20this%20approach%20results%20in%20reduced%20performance%20for%20individual%20tasks%0Abecause%20of%20variations%20between%20tasks%20and%20differences%20in%20modality%20between%0Alanguage%20information%20and%20evaluation%20information.%20To%20address%20this%2C%20we%20propose%20a%0Aprompt-guided%20multimodal%20interaction%20framework.%20This%20framework%20utilizes%20a%20pair%0Aof%20transformers%20to%20facilitate%20the%20interaction%20between%20different%20modalities%20of%0Ainformation.%20It%20also%20uses%20prompts%20to%20transform%20the%20score%20regression%20task%20into%20a%0Avideo-text%20matching%20task%2C%20thus%20enabling%20task%20interactivity.%20To%20support%20further%0Aresearch%20in%20this%20field%2C%20we%20re-annotate%20the%20MTL-AQA%20and%20FineGym%20datasets%20with%0Ahigh-quality%20and%20comprehensive%20action%20narration.%20Additionally%2C%20we%20establish%0Abenchmarks%20for%20NAE.%20Extensive%20experiment%20results%20prove%20that%20our%20method%0Aoutperforms%20separate%20learning%20methods%20and%20naive%20multi-task%20learning%20methods.%0AData%20and%20code%20are%20released%20at%20https%3A//github.com/shiyi-zh0408/NAE_CVPR2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14471v2&entry.124074799=Read"},
{"title": "Lazy Data Practices Harm Fairness Research", "author": "Jan Simson and Alessandro Fabris and Christoph Kern", "abstract": "  Data practices shape research and practice on fairness in machine learning\n(fair ML). Critical data studies offer important reflections and critiques for\nthe responsible advancement of the field by highlighting shortcomings and\nproposing recommendations for improvement. In this work, we present a\ncomprehensive analysis of fair ML datasets, demonstrating how unreflective yet\ncommon practices hinder the reach and reliability of algorithmic fairness\nfindings. We systematically study protected information encoded in tabular\ndatasets and their usage in 280 experiments across 142 publications.\n  Our analyses identify three main areas of concern: (1) a \\textbf{lack of\nrepresentation for certain protected attributes} in both data and evaluations;\n(2) the widespread \\textbf{exclusion of minorities} during data preprocessing;\nand (3) \\textbf{opaque data processing} threatening the generalization of\nfairness research. By conducting exemplary analyses on the utilization of\nprominent datasets, we demonstrate how unreflective data decisions\ndisproportionately affect minority groups, fairness metrics, and resultant\nmodel comparisons. Additionally, we identify supplementary factors such as\nlimitations in publicly available data, privacy considerations, and a general\nlack of awareness, which exacerbate these challenges. To address these issues,\nwe propose a set of recommendations for data usage in fairness research\ncentered on transparency and responsible inclusion. This study underscores the\nneed for a critical reevaluation of data practices in fair ML and offers\ndirections to improve both the sourcing and usage of datasets.\n", "link": "http://arxiv.org/abs/2404.17293v1", "date": "2024-04-26", "relevancy": 1.2524, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4351}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4233}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4081}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Lazy%20Data%20Practices%20Harm%20Fairness%20Research&body=Title%3A%20Lazy%20Data%20Practices%20Harm%20Fairness%20Research%0AAuthor%3A%20Jan%20Simson%20and%20Alessandro%20Fabris%20and%20Christoph%20Kern%0AAbstract%3A%20%20%20Data%20practices%20shape%20research%20and%20practice%20on%20fairness%20in%20machine%20learning%0A%28fair%20ML%29.%20Critical%20data%20studies%20offer%20important%20reflections%20and%20critiques%20for%0Athe%20responsible%20advancement%20of%20the%20field%20by%20highlighting%20shortcomings%20and%0Aproposing%20recommendations%20for%20improvement.%20In%20this%20work%2C%20we%20present%20a%0Acomprehensive%20analysis%20of%20fair%20ML%20datasets%2C%20demonstrating%20how%20unreflective%20yet%0Acommon%20practices%20hinder%20the%20reach%20and%20reliability%20of%20algorithmic%20fairness%0Afindings.%20We%20systematically%20study%20protected%20information%20encoded%20in%20tabular%0Adatasets%20and%20their%20usage%20in%20280%20experiments%20across%20142%20publications.%0A%20%20Our%20analyses%20identify%20three%20main%20areas%20of%20concern%3A%20%281%29%20a%20%5Ctextbf%7Black%20of%0Arepresentation%20for%20certain%20protected%20attributes%7D%20in%20both%20data%20and%20evaluations%3B%0A%282%29%20the%20widespread%20%5Ctextbf%7Bexclusion%20of%20minorities%7D%20during%20data%20preprocessing%3B%0Aand%20%283%29%20%5Ctextbf%7Bopaque%20data%20processing%7D%20threatening%20the%20generalization%20of%0Afairness%20research.%20By%20conducting%20exemplary%20analyses%20on%20the%20utilization%20of%0Aprominent%20datasets%2C%20we%20demonstrate%20how%20unreflective%20data%20decisions%0Adisproportionately%20affect%20minority%20groups%2C%20fairness%20metrics%2C%20and%20resultant%0Amodel%20comparisons.%20Additionally%2C%20we%20identify%20supplementary%20factors%20such%20as%0Alimitations%20in%20publicly%20available%20data%2C%20privacy%20considerations%2C%20and%20a%20general%0Alack%20of%20awareness%2C%20which%20exacerbate%20these%20challenges.%20To%20address%20these%20issues%2C%0Awe%20propose%20a%20set%20of%20recommendations%20for%20data%20usage%20in%20fairness%20research%0Acentered%20on%20transparency%20and%20responsible%20inclusion.%20This%20study%20underscores%20the%0Aneed%20for%20a%20critical%20reevaluation%20of%20data%20practices%20in%20fair%20ML%20and%20offers%0Adirections%20to%20improve%20both%20the%20sourcing%20and%20usage%20of%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17293v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lazy%20Data%20Practices%20Harm%20Fairness%20Research&entry.906535625=Jan%20Simson%20and%20Alessandro%20Fabris%20and%20Christoph%20Kern&entry.1292438233=%20%20Data%20practices%20shape%20research%20and%20practice%20on%20fairness%20in%20machine%20learning%0A%28fair%20ML%29.%20Critical%20data%20studies%20offer%20important%20reflections%20and%20critiques%20for%0Athe%20responsible%20advancement%20of%20the%20field%20by%20highlighting%20shortcomings%20and%0Aproposing%20recommendations%20for%20improvement.%20In%20this%20work%2C%20we%20present%20a%0Acomprehensive%20analysis%20of%20fair%20ML%20datasets%2C%20demonstrating%20how%20unreflective%20yet%0Acommon%20practices%20hinder%20the%20reach%20and%20reliability%20of%20algorithmic%20fairness%0Afindings.%20We%20systematically%20study%20protected%20information%20encoded%20in%20tabular%0Adatasets%20and%20their%20usage%20in%20280%20experiments%20across%20142%20publications.%0A%20%20Our%20analyses%20identify%20three%20main%20areas%20of%20concern%3A%20%281%29%20a%20%5Ctextbf%7Black%20of%0Arepresentation%20for%20certain%20protected%20attributes%7D%20in%20both%20data%20and%20evaluations%3B%0A%282%29%20the%20widespread%20%5Ctextbf%7Bexclusion%20of%20minorities%7D%20during%20data%20preprocessing%3B%0Aand%20%283%29%20%5Ctextbf%7Bopaque%20data%20processing%7D%20threatening%20the%20generalization%20of%0Afairness%20research.%20By%20conducting%20exemplary%20analyses%20on%20the%20utilization%20of%0Aprominent%20datasets%2C%20we%20demonstrate%20how%20unreflective%20data%20decisions%0Adisproportionately%20affect%20minority%20groups%2C%20fairness%20metrics%2C%20and%20resultant%0Amodel%20comparisons.%20Additionally%2C%20we%20identify%20supplementary%20factors%20such%20as%0Alimitations%20in%20publicly%20available%20data%2C%20privacy%20considerations%2C%20and%20a%20general%0Alack%20of%20awareness%2C%20which%20exacerbate%20these%20challenges.%20To%20address%20these%20issues%2C%0Awe%20propose%20a%20set%20of%20recommendations%20for%20data%20usage%20in%20fairness%20research%0Acentered%20on%20transparency%20and%20responsible%20inclusion.%20This%20study%20underscores%20the%0Aneed%20for%20a%20critical%20reevaluation%20of%20data%20practices%20in%20fair%20ML%20and%20offers%0Adirections%20to%20improve%20both%20the%20sourcing%20and%20usage%20of%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17293v1&entry.124074799=Read"},
{"title": "Fast Abstracts and Student Forum Proceedings -- EDCC 2024 -- 19th\n  European Dependable Computing Conference", "author": "Simona Bernardi and Tommaso Zoppi", "abstract": "  The goal of the Fast Abstracts track is to bring together researchers and\npractitioners working on dependable computing to discuss work in progress or\nopinion pieces. Contributions are welcome from academia and industry. Fast\nAbstracts aim to serve as a rapid and flexible mechanism to: (i) Report on\ncurrent work that may or may not be complete; (ii) Introduce new ideas to the\ncommunity; (iii) State positions on controversial issues or open problems; (iv)\nShare lessons learnt from real-word dependability engineering; and (v) Debunk\nor question results from other papers based on contra-indications. The Student\nForum aims at creating a vibrant and friendly environment where students can\npresent and discuss their work, and exchange ideas and experiences with other\nstudents, researchers and industry. One of the key goals of the Forum is to\nprovide students with feedback on their preliminary results that might help\nwith their future research directions.\n", "link": "http://arxiv.org/abs/2404.17465v1", "date": "2024-04-26", "relevancy": 1.4324, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3755}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3622}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3391}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fast%20Abstracts%20and%20Student%20Forum%20Proceedings%20--%20EDCC%202024%20--%2019th%0A%20%20European%20Dependable%20Computing%20Conference&body=Title%3A%20Fast%20Abstracts%20and%20Student%20Forum%20Proceedings%20--%20EDCC%202024%20--%2019th%0A%20%20European%20Dependable%20Computing%20Conference%0AAuthor%3A%20Simona%20Bernardi%20and%20Tommaso%20Zoppi%0AAbstract%3A%20%20%20The%20goal%20of%20the%20Fast%20Abstracts%20track%20is%20to%20bring%20together%20researchers%20and%0Apractitioners%20working%20on%20dependable%20computing%20to%20discuss%20work%20in%20progress%20or%0Aopinion%20pieces.%20Contributions%20are%20welcome%20from%20academia%20and%20industry.%20Fast%0AAbstracts%20aim%20to%20serve%20as%20a%20rapid%20and%20flexible%20mechanism%20to%3A%20%28i%29%20Report%20on%0Acurrent%20work%20that%20may%20or%20may%20not%20be%20complete%3B%20%28ii%29%20Introduce%20new%20ideas%20to%20the%0Acommunity%3B%20%28iii%29%20State%20positions%20on%20controversial%20issues%20or%20open%20problems%3B%20%28iv%29%0AShare%20lessons%20learnt%20from%20real-word%20dependability%20engineering%3B%20and%20%28v%29%20Debunk%0Aor%20question%20results%20from%20other%20papers%20based%20on%20contra-indications.%20The%20Student%0AForum%20aims%20at%20creating%20a%20vibrant%20and%20friendly%20environment%20where%20students%20can%0Apresent%20and%20discuss%20their%20work%2C%20and%20exchange%20ideas%20and%20experiences%20with%20other%0Astudents%2C%20researchers%20and%20industry.%20One%20of%20the%20key%20goals%20of%20the%20Forum%20is%20to%0Aprovide%20students%20with%20feedback%20on%20their%20preliminary%20results%20that%20might%20help%0Awith%20their%20future%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17465v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Abstracts%20and%20Student%20Forum%20Proceedings%20--%20EDCC%202024%20--%2019th%0A%20%20European%20Dependable%20Computing%20Conference&entry.906535625=Simona%20Bernardi%20and%20Tommaso%20Zoppi&entry.1292438233=%20%20The%20goal%20of%20the%20Fast%20Abstracts%20track%20is%20to%20bring%20together%20researchers%20and%0Apractitioners%20working%20on%20dependable%20computing%20to%20discuss%20work%20in%20progress%20or%0Aopinion%20pieces.%20Contributions%20are%20welcome%20from%20academia%20and%20industry.%20Fast%0AAbstracts%20aim%20to%20serve%20as%20a%20rapid%20and%20flexible%20mechanism%20to%3A%20%28i%29%20Report%20on%0Acurrent%20work%20that%20may%20or%20may%20not%20be%20complete%3B%20%28ii%29%20Introduce%20new%20ideas%20to%20the%0Acommunity%3B%20%28iii%29%20State%20positions%20on%20controversial%20issues%20or%20open%20problems%3B%20%28iv%29%0AShare%20lessons%20learnt%20from%20real-word%20dependability%20engineering%3B%20and%20%28v%29%20Debunk%0Aor%20question%20results%20from%20other%20papers%20based%20on%20contra-indications.%20The%20Student%0AForum%20aims%20at%20creating%20a%20vibrant%20and%20friendly%20environment%20where%20students%20can%0Apresent%20and%20discuss%20their%20work%2C%20and%20exchange%20ideas%20and%20experiences%20with%20other%0Astudents%2C%20researchers%20and%20industry.%20One%20of%20the%20key%20goals%20of%20the%20Forum%20is%20to%0Aprovide%20students%20with%20feedback%20on%20their%20preliminary%20results%20that%20might%20help%0Awith%20their%20future%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17465v1&entry.124074799=Read"},
{"title": "The Power of Resets in Online Reinforcement Learning", "author": "Zakaria Mhammedi and Dylan J. Foster and Alexander Rakhlin", "abstract": "  Simulators are a pervasive tool in reinforcement learning, but most existing\nalgorithms cannot efficiently exploit simulator access -- particularly in\nhigh-dimensional domains that require general function approximation. We\nexplore the power of simulators through online reinforcement learning with\n{local simulator access} (or, local planning), an RL protocol where the agent\nis allowed to reset to previously observed states and follow their dynamics\nduring training. We use local simulator access to unlock new statistical\nguarantees that were previously out of reach:\n  - We show that MDPs with low coverability (Xie et al. 2023) -- a general\nstructural condition that subsumes Block MDPs and Low-Rank MDPs -- can be\nlearned in a sample-efficient fashion with only $Q^{\\star}$-realizability\n(realizability of the optimal state-value function); existing online RL\nalgorithms require significantly stronger representation conditions.\n  - As a consequence, we show that the notorious Exogenous Block MDP problem\n(Efroni et al. 2022) is tractable under local simulator access.\n  The results above are achieved through a computationally inefficient\nalgorithm. We complement them with a more computationally efficient algorithm,\nRVFS (Recursive Value Function Search), which achieves provable sample\ncomplexity guarantees under a strengthened statistical assumption known as\npushforward coverability. RVFS can be viewed as a principled, provable\ncounterpart to a successful empirical paradigm that combines recursive search\n(e.g., MCTS) with value function approximation.\n", "link": "http://arxiv.org/abs/2404.15417v2", "date": "2024-04-26", "relevancy": 1.3975, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4711}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4652}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4532}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Power%20of%20Resets%20in%20Online%20Reinforcement%20Learning&body=Title%3A%20The%20Power%20of%20Resets%20in%20Online%20Reinforcement%20Learning%0AAuthor%3A%20Zakaria%20Mhammedi%20and%20Dylan%20J.%20Foster%20and%20Alexander%20Rakhlin%0AAbstract%3A%20%20%20Simulators%20are%20a%20pervasive%20tool%20in%20reinforcement%20learning%2C%20but%20most%20existing%0Aalgorithms%20cannot%20efficiently%20exploit%20simulator%20access%20--%20particularly%20in%0Ahigh-dimensional%20domains%20that%20require%20general%20function%20approximation.%20We%0Aexplore%20the%20power%20of%20simulators%20through%20online%20reinforcement%20learning%20with%0A%7Blocal%20simulator%20access%7D%20%28or%2C%20local%20planning%29%2C%20an%20RL%20protocol%20where%20the%20agent%0Ais%20allowed%20to%20reset%20to%20previously%20observed%20states%20and%20follow%20their%20dynamics%0Aduring%20training.%20We%20use%20local%20simulator%20access%20to%20unlock%20new%20statistical%0Aguarantees%20that%20were%20previously%20out%20of%20reach%3A%0A%20%20-%20We%20show%20that%20MDPs%20with%20low%20coverability%20%28Xie%20et%20al.%202023%29%20--%20a%20general%0Astructural%20condition%20that%20subsumes%20Block%20MDPs%20and%20Low-Rank%20MDPs%20--%20can%20be%0Alearned%20in%20a%20sample-efficient%20fashion%20with%20only%20%24Q%5E%7B%5Cstar%7D%24-realizability%0A%28realizability%20of%20the%20optimal%20state-value%20function%29%3B%20existing%20online%20RL%0Aalgorithms%20require%20significantly%20stronger%20representation%20conditions.%0A%20%20-%20As%20a%20consequence%2C%20we%20show%20that%20the%20notorious%20Exogenous%20Block%20MDP%20problem%0A%28Efroni%20et%20al.%202022%29%20is%20tractable%20under%20local%20simulator%20access.%0A%20%20The%20results%20above%20are%20achieved%20through%20a%20computationally%20inefficient%0Aalgorithm.%20We%20complement%20them%20with%20a%20more%20computationally%20efficient%20algorithm%2C%0ARVFS%20%28Recursive%20Value%20Function%20Search%29%2C%20which%20achieves%20provable%20sample%0Acomplexity%20guarantees%20under%20a%20strengthened%20statistical%20assumption%20known%20as%0Apushforward%20coverability.%20RVFS%20can%20be%20viewed%20as%20a%20principled%2C%20provable%0Acounterpart%20to%20a%20successful%20empirical%20paradigm%20that%20combines%20recursive%20search%0A%28e.g.%2C%20MCTS%29%20with%20value%20function%20approximation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15417v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Power%20of%20Resets%20in%20Online%20Reinforcement%20Learning&entry.906535625=Zakaria%20Mhammedi%20and%20Dylan%20J.%20Foster%20and%20Alexander%20Rakhlin&entry.1292438233=%20%20Simulators%20are%20a%20pervasive%20tool%20in%20reinforcement%20learning%2C%20but%20most%20existing%0Aalgorithms%20cannot%20efficiently%20exploit%20simulator%20access%20--%20particularly%20in%0Ahigh-dimensional%20domains%20that%20require%20general%20function%20approximation.%20We%0Aexplore%20the%20power%20of%20simulators%20through%20online%20reinforcement%20learning%20with%0A%7Blocal%20simulator%20access%7D%20%28or%2C%20local%20planning%29%2C%20an%20RL%20protocol%20where%20the%20agent%0Ais%20allowed%20to%20reset%20to%20previously%20observed%20states%20and%20follow%20their%20dynamics%0Aduring%20training.%20We%20use%20local%20simulator%20access%20to%20unlock%20new%20statistical%0Aguarantees%20that%20were%20previously%20out%20of%20reach%3A%0A%20%20-%20We%20show%20that%20MDPs%20with%20low%20coverability%20%28Xie%20et%20al.%202023%29%20--%20a%20general%0Astructural%20condition%20that%20subsumes%20Block%20MDPs%20and%20Low-Rank%20MDPs%20--%20can%20be%0Alearned%20in%20a%20sample-efficient%20fashion%20with%20only%20%24Q%5E%7B%5Cstar%7D%24-realizability%0A%28realizability%20of%20the%20optimal%20state-value%20function%29%3B%20existing%20online%20RL%0Aalgorithms%20require%20significantly%20stronger%20representation%20conditions.%0A%20%20-%20As%20a%20consequence%2C%20we%20show%20that%20the%20notorious%20Exogenous%20Block%20MDP%20problem%0A%28Efroni%20et%20al.%202022%29%20is%20tractable%20under%20local%20simulator%20access.%0A%20%20The%20results%20above%20are%20achieved%20through%20a%20computationally%20inefficient%0Aalgorithm.%20We%20complement%20them%20with%20a%20more%20computationally%20efficient%20algorithm%2C%0ARVFS%20%28Recursive%20Value%20Function%20Search%29%2C%20which%20achieves%20provable%20sample%0Acomplexity%20guarantees%20under%20a%20strengthened%20statistical%20assumption%20known%20as%0Apushforward%20coverability.%20RVFS%20can%20be%20viewed%20as%20a%20principled%2C%20provable%0Acounterpart%20to%20a%20successful%20empirical%20paradigm%20that%20combines%20recursive%20search%0A%28e.g.%2C%20MCTS%29%20with%20value%20function%20approximation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15417v2&entry.124074799=Read"},
{"title": "Enhancing Legal Compliance and Regulation Analysis with Large Language\n  Models", "author": "Shabnam Hassani", "abstract": "  This research explores the application of Large Language Models (LLMs) for\nautomating the extraction of requirement-related legal content in the food\nsafety domain and checking legal compliance of regulatory artifacts. With\nIndustry 4.0 revolutionizing the food industry and with the General Data\nProtection Regulation (GDPR) reshaping privacy policies and data processing\nagreements, there is a growing gap between regulatory analysis and recent\ntechnological advancements. This study aims to bridge this gap by leveraging\nLLMs, namely BERT and GPT models, to accurately classify legal provisions and\nautomate compliance checks. Our findings demonstrate promising results,\nindicating LLMs' significant potential to enhance legal compliance and\nregulatory analysis efficiency, notably by reducing manual workload and\nimproving accuracy within reasonable time and financial constraints.\n", "link": "http://arxiv.org/abs/2404.17522v1", "date": "2024-04-26", "relevancy": 1.3636, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.495}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4539}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4386}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Legal%20Compliance%20and%20Regulation%20Analysis%20with%20Large%20Language%0A%20%20Models&body=Title%3A%20Enhancing%20Legal%20Compliance%20and%20Regulation%20Analysis%20with%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Shabnam%20Hassani%0AAbstract%3A%20%20%20This%20research%20explores%20the%20application%20of%20Large%20Language%20Models%20%28LLMs%29%20for%0Aautomating%20the%20extraction%20of%20requirement-related%20legal%20content%20in%20the%20food%0Asafety%20domain%20and%20checking%20legal%20compliance%20of%20regulatory%20artifacts.%20With%0AIndustry%204.0%20revolutionizing%20the%20food%20industry%20and%20with%20the%20General%20Data%0AProtection%20Regulation%20%28GDPR%29%20reshaping%20privacy%20policies%20and%20data%20processing%0Aagreements%2C%20there%20is%20a%20growing%20gap%20between%20regulatory%20analysis%20and%20recent%0Atechnological%20advancements.%20This%20study%20aims%20to%20bridge%20this%20gap%20by%20leveraging%0ALLMs%2C%20namely%20BERT%20and%20GPT%20models%2C%20to%20accurately%20classify%20legal%20provisions%20and%0Aautomate%20compliance%20checks.%20Our%20findings%20demonstrate%20promising%20results%2C%0Aindicating%20LLMs%27%20significant%20potential%20to%20enhance%20legal%20compliance%20and%0Aregulatory%20analysis%20efficiency%2C%20notably%20by%20reducing%20manual%20workload%20and%0Aimproving%20accuracy%20within%20reasonable%20time%20and%20financial%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17522v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Legal%20Compliance%20and%20Regulation%20Analysis%20with%20Large%20Language%0A%20%20Models&entry.906535625=Shabnam%20Hassani&entry.1292438233=%20%20This%20research%20explores%20the%20application%20of%20Large%20Language%20Models%20%28LLMs%29%20for%0Aautomating%20the%20extraction%20of%20requirement-related%20legal%20content%20in%20the%20food%0Asafety%20domain%20and%20checking%20legal%20compliance%20of%20regulatory%20artifacts.%20With%0AIndustry%204.0%20revolutionizing%20the%20food%20industry%20and%20with%20the%20General%20Data%0AProtection%20Regulation%20%28GDPR%29%20reshaping%20privacy%20policies%20and%20data%20processing%0Aagreements%2C%20there%20is%20a%20growing%20gap%20between%20regulatory%20analysis%20and%20recent%0Atechnological%20advancements.%20This%20study%20aims%20to%20bridge%20this%20gap%20by%20leveraging%0ALLMs%2C%20namely%20BERT%20and%20GPT%20models%2C%20to%20accurately%20classify%20legal%20provisions%20and%0Aautomate%20compliance%20checks.%20Our%20findings%20demonstrate%20promising%20results%2C%0Aindicating%20LLMs%27%20significant%20potential%20to%20enhance%20legal%20compliance%20and%0Aregulatory%20analysis%20efficiency%2C%20notably%20by%20reducing%20manual%20workload%20and%0Aimproving%20accuracy%20within%20reasonable%20time%20and%20financial%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17522v1&entry.124074799=Read"},
{"title": "Learning to Defer in Content Moderation: The Human-AI Interplay", "author": "Thodoris Lykouris and Wentao Weng", "abstract": "  Successful content moderation in online platforms relies on a human-AI\ncollaboration approach. A typical heuristic estimates the expected harmfulness\nof a post and uses fixed thresholds to decide whether to remove it and whether\nto send it for human review. This disregards the prediction uncertainty, the\ntime-varying element of human review capacity and post arrivals, and the\nselective sampling in the dataset (humans only review posts filtered by the\nadmission algorithm).\n  In this paper, we introduce a model to capture the human-AI interplay in\ncontent moderation. The algorithm observes contextual information for incoming\nposts, makes classification and admission decisions, and schedules posts for\nhuman review. Only admitted posts receive human reviews on their harmfulness.\nThese reviews help educate the machine-learning algorithms but are delayed due\nto congestion in the human review system. The classical learning-theoretic way\nto capture this human-AI interplay is via the framework of learning to defer,\nwhere the algorithm has the option to defer a classification task to humans for\na fixed cost and immediately receive feedback. Our model contributes to this\nliterature by introducing congestion in the human review system. Moreover,\nunlike work on online learning with delayed feedback where the delay in the\nfeedback is exogenous to the algorithm's decisions, the delay in our model is\nendogenous to both the admission and the scheduling decisions.\n  We propose a near-optimal learning algorithm that carefully balances the\nclassification loss from a selectively sampled dataset, the idiosyncratic loss\nof non-reviewed posts, and the delay loss of having congestion in the human\nreview system. To the best of our knowledge, this is the first result for\nonline learning in contextual queueing systems and hence our analytical\nframework may be of independent interest.\n", "link": "http://arxiv.org/abs/2402.12237v2", "date": "2024-04-26", "relevancy": 0.9834, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5002}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.492}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4829}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Defer%20in%20Content%20Moderation%3A%20The%20Human-AI%20Interplay&body=Title%3A%20Learning%20to%20Defer%20in%20Content%20Moderation%3A%20The%20Human-AI%20Interplay%0AAuthor%3A%20Thodoris%20Lykouris%20and%20Wentao%20Weng%0AAbstract%3A%20%20%20Successful%20content%20moderation%20in%20online%20platforms%20relies%20on%20a%20human-AI%0Acollaboration%20approach.%20A%20typical%20heuristic%20estimates%20the%20expected%20harmfulness%0Aof%20a%20post%20and%20uses%20fixed%20thresholds%20to%20decide%20whether%20to%20remove%20it%20and%20whether%0Ato%20send%20it%20for%20human%20review.%20This%20disregards%20the%20prediction%20uncertainty%2C%20the%0Atime-varying%20element%20of%20human%20review%20capacity%20and%20post%20arrivals%2C%20and%20the%0Aselective%20sampling%20in%20the%20dataset%20%28humans%20only%20review%20posts%20filtered%20by%20the%0Aadmission%20algorithm%29.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20model%20to%20capture%20the%20human-AI%20interplay%20in%0Acontent%20moderation.%20The%20algorithm%20observes%20contextual%20information%20for%20incoming%0Aposts%2C%20makes%20classification%20and%20admission%20decisions%2C%20and%20schedules%20posts%20for%0Ahuman%20review.%20Only%20admitted%20posts%20receive%20human%20reviews%20on%20their%20harmfulness.%0AThese%20reviews%20help%20educate%20the%20machine-learning%20algorithms%20but%20are%20delayed%20due%0Ato%20congestion%20in%20the%20human%20review%20system.%20The%20classical%20learning-theoretic%20way%0Ato%20capture%20this%20human-AI%20interplay%20is%20via%20the%20framework%20of%20learning%20to%20defer%2C%0Awhere%20the%20algorithm%20has%20the%20option%20to%20defer%20a%20classification%20task%20to%20humans%20for%0Aa%20fixed%20cost%20and%20immediately%20receive%20feedback.%20Our%20model%20contributes%20to%20this%0Aliterature%20by%20introducing%20congestion%20in%20the%20human%20review%20system.%20Moreover%2C%0Aunlike%20work%20on%20online%20learning%20with%20delayed%20feedback%20where%20the%20delay%20in%20the%0Afeedback%20is%20exogenous%20to%20the%20algorithm%27s%20decisions%2C%20the%20delay%20in%20our%20model%20is%0Aendogenous%20to%20both%20the%20admission%20and%20the%20scheduling%20decisions.%0A%20%20We%20propose%20a%20near-optimal%20learning%20algorithm%20that%20carefully%20balances%20the%0Aclassification%20loss%20from%20a%20selectively%20sampled%20dataset%2C%20the%20idiosyncratic%20loss%0Aof%20non-reviewed%20posts%2C%20and%20the%20delay%20loss%20of%20having%20congestion%20in%20the%20human%0Areview%20system.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20result%20for%0Aonline%20learning%20in%20contextual%20queueing%20systems%20and%20hence%20our%20analytical%0Aframework%20may%20be%20of%20independent%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12237v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Defer%20in%20Content%20Moderation%3A%20The%20Human-AI%20Interplay&entry.906535625=Thodoris%20Lykouris%20and%20Wentao%20Weng&entry.1292438233=%20%20Successful%20content%20moderation%20in%20online%20platforms%20relies%20on%20a%20human-AI%0Acollaboration%20approach.%20A%20typical%20heuristic%20estimates%20the%20expected%20harmfulness%0Aof%20a%20post%20and%20uses%20fixed%20thresholds%20to%20decide%20whether%20to%20remove%20it%20and%20whether%0Ato%20send%20it%20for%20human%20review.%20This%20disregards%20the%20prediction%20uncertainty%2C%20the%0Atime-varying%20element%20of%20human%20review%20capacity%20and%20post%20arrivals%2C%20and%20the%0Aselective%20sampling%20in%20the%20dataset%20%28humans%20only%20review%20posts%20filtered%20by%20the%0Aadmission%20algorithm%29.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20model%20to%20capture%20the%20human-AI%20interplay%20in%0Acontent%20moderation.%20The%20algorithm%20observes%20contextual%20information%20for%20incoming%0Aposts%2C%20makes%20classification%20and%20admission%20decisions%2C%20and%20schedules%20posts%20for%0Ahuman%20review.%20Only%20admitted%20posts%20receive%20human%20reviews%20on%20their%20harmfulness.%0AThese%20reviews%20help%20educate%20the%20machine-learning%20algorithms%20but%20are%20delayed%20due%0Ato%20congestion%20in%20the%20human%20review%20system.%20The%20classical%20learning-theoretic%20way%0Ato%20capture%20this%20human-AI%20interplay%20is%20via%20the%20framework%20of%20learning%20to%20defer%2C%0Awhere%20the%20algorithm%20has%20the%20option%20to%20defer%20a%20classification%20task%20to%20humans%20for%0Aa%20fixed%20cost%20and%20immediately%20receive%20feedback.%20Our%20model%20contributes%20to%20this%0Aliterature%20by%20introducing%20congestion%20in%20the%20human%20review%20system.%20Moreover%2C%0Aunlike%20work%20on%20online%20learning%20with%20delayed%20feedback%20where%20the%20delay%20in%20the%0Afeedback%20is%20exogenous%20to%20the%20algorithm%27s%20decisions%2C%20the%20delay%20in%20our%20model%20is%0Aendogenous%20to%20both%20the%20admission%20and%20the%20scheduling%20decisions.%0A%20%20We%20propose%20a%20near-optimal%20learning%20algorithm%20that%20carefully%20balances%20the%0Aclassification%20loss%20from%20a%20selectively%20sampled%20dataset%2C%20the%20idiosyncratic%20loss%0Aof%20non-reviewed%20posts%2C%20and%20the%20delay%20loss%20of%20having%20congestion%20in%20the%20human%0Areview%20system.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20result%20for%0Aonline%20learning%20in%20contextual%20queueing%20systems%20and%20hence%20our%20analytical%0Aframework%20may%20be%20of%20independent%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12237v2&entry.124074799=Read"},
{"title": "Assessing the Potential of AI for Spatially Sensitive Nature-Related\n  Financial Risks", "author": "Steven Reece and Emma O donnell and Felicia Liu and Joanna Wolstenholme and Frida Arriaga and Giacomo Ascenzi and Richard Pywell", "abstract": "  There is growing recognition among financial institutions, financial\nregulators and policy makers of the importance of addressing nature-related\nrisks and opportunities. Evaluating and assessing nature-related risks for\nfinancial institutions is challenging due to the large volume of heterogeneous\ndata available on nature and the complexity of investment value chains and the\nvarious components' relationship to nature. The dual problem of scaling data\nanalytics and analysing complex systems can be addressed using Artificial\nIntelligence (AI). We address issues such as plugging existing data gaps with\ndiscovered data, data estimation under uncertainty, time series analysis and\n(near) real-time updates. This report presents potential AI solutions for\nmodels of two distinct use cases, the Brazil Beef Supply Use Case and the Water\nUtility Use Case. Our two use cases cover a broad perspective within\nsustainable finance. The Brazilian cattle farming use case is an example of\ngreening finance - integrating nature-related considerations into mainstream\nfinancial decision-making to transition investments away from sectors with poor\nhistorical track records and unsustainable operations. The deployment of\nnature-based solutions in the UK water utility use case is an example of\nfinancing green - driving investment to nature-positive outcomes. The two use\ncases also cover different sectors, geographies, financial assets and AI\nmodelling techniques, providing an overview on how AI could be applied to\ndifferent challenges relating to nature's integration into finance. This report\nis primarily aimed at financial institutions but is also of interest to ESG\ndata providers, TNFD, systems modellers, and, of course, AI practitioners.\n", "link": "http://arxiv.org/abs/2404.17369v1", "date": "2024-04-26", "relevancy": 1.2334, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4309}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4128}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3873}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Assessing%20the%20Potential%20of%20AI%20for%20Spatially%20Sensitive%20Nature-Related%0A%20%20Financial%20Risks&body=Title%3A%20Assessing%20the%20Potential%20of%20AI%20for%20Spatially%20Sensitive%20Nature-Related%0A%20%20Financial%20Risks%0AAuthor%3A%20Steven%20Reece%20and%20Emma%20O%20donnell%20and%20Felicia%20Liu%20and%20Joanna%20Wolstenholme%20and%20Frida%20Arriaga%20and%20Giacomo%20Ascenzi%20and%20Richard%20Pywell%0AAbstract%3A%20%20%20There%20is%20growing%20recognition%20among%20financial%20institutions%2C%20financial%0Aregulators%20and%20policy%20makers%20of%20the%20importance%20of%20addressing%20nature-related%0Arisks%20and%20opportunities.%20Evaluating%20and%20assessing%20nature-related%20risks%20for%0Afinancial%20institutions%20is%20challenging%20due%20to%20the%20large%20volume%20of%20heterogeneous%0Adata%20available%20on%20nature%20and%20the%20complexity%20of%20investment%20value%20chains%20and%20the%0Avarious%20components%27%20relationship%20to%20nature.%20The%20dual%20problem%20of%20scaling%20data%0Aanalytics%20and%20analysing%20complex%20systems%20can%20be%20addressed%20using%20Artificial%0AIntelligence%20%28AI%29.%20We%20address%20issues%20such%20as%20plugging%20existing%20data%20gaps%20with%0Adiscovered%20data%2C%20data%20estimation%20under%20uncertainty%2C%20time%20series%20analysis%20and%0A%28near%29%20real-time%20updates.%20This%20report%20presents%20potential%20AI%20solutions%20for%0Amodels%20of%20two%20distinct%20use%20cases%2C%20the%20Brazil%20Beef%20Supply%20Use%20Case%20and%20the%20Water%0AUtility%20Use%20Case.%20Our%20two%20use%20cases%20cover%20a%20broad%20perspective%20within%0Asustainable%20finance.%20The%20Brazilian%20cattle%20farming%20use%20case%20is%20an%20example%20of%0Agreening%20finance%20-%20integrating%20nature-related%20considerations%20into%20mainstream%0Afinancial%20decision-making%20to%20transition%20investments%20away%20from%20sectors%20with%20poor%0Ahistorical%20track%20records%20and%20unsustainable%20operations.%20The%20deployment%20of%0Anature-based%20solutions%20in%20the%20UK%20water%20utility%20use%20case%20is%20an%20example%20of%0Afinancing%20green%20-%20driving%20investment%20to%20nature-positive%20outcomes.%20The%20two%20use%0Acases%20also%20cover%20different%20sectors%2C%20geographies%2C%20financial%20assets%20and%20AI%0Amodelling%20techniques%2C%20providing%20an%20overview%20on%20how%20AI%20could%20be%20applied%20to%0Adifferent%20challenges%20relating%20to%20nature%27s%20integration%20into%20finance.%20This%20report%0Ais%20primarily%20aimed%20at%20financial%20institutions%20but%20is%20also%20of%20interest%20to%20ESG%0Adata%20providers%2C%20TNFD%2C%20systems%20modellers%2C%20and%2C%20of%20course%2C%20AI%20practitioners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17369v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20the%20Potential%20of%20AI%20for%20Spatially%20Sensitive%20Nature-Related%0A%20%20Financial%20Risks&entry.906535625=Steven%20Reece%20and%20Emma%20O%20donnell%20and%20Felicia%20Liu%20and%20Joanna%20Wolstenholme%20and%20Frida%20Arriaga%20and%20Giacomo%20Ascenzi%20and%20Richard%20Pywell&entry.1292438233=%20%20There%20is%20growing%20recognition%20among%20financial%20institutions%2C%20financial%0Aregulators%20and%20policy%20makers%20of%20the%20importance%20of%20addressing%20nature-related%0Arisks%20and%20opportunities.%20Evaluating%20and%20assessing%20nature-related%20risks%20for%0Afinancial%20institutions%20is%20challenging%20due%20to%20the%20large%20volume%20of%20heterogeneous%0Adata%20available%20on%20nature%20and%20the%20complexity%20of%20investment%20value%20chains%20and%20the%0Avarious%20components%27%20relationship%20to%20nature.%20The%20dual%20problem%20of%20scaling%20data%0Aanalytics%20and%20analysing%20complex%20systems%20can%20be%20addressed%20using%20Artificial%0AIntelligence%20%28AI%29.%20We%20address%20issues%20such%20as%20plugging%20existing%20data%20gaps%20with%0Adiscovered%20data%2C%20data%20estimation%20under%20uncertainty%2C%20time%20series%20analysis%20and%0A%28near%29%20real-time%20updates.%20This%20report%20presents%20potential%20AI%20solutions%20for%0Amodels%20of%20two%20distinct%20use%20cases%2C%20the%20Brazil%20Beef%20Supply%20Use%20Case%20and%20the%20Water%0AUtility%20Use%20Case.%20Our%20two%20use%20cases%20cover%20a%20broad%20perspective%20within%0Asustainable%20finance.%20The%20Brazilian%20cattle%20farming%20use%20case%20is%20an%20example%20of%0Agreening%20finance%20-%20integrating%20nature-related%20considerations%20into%20mainstream%0Afinancial%20decision-making%20to%20transition%20investments%20away%20from%20sectors%20with%20poor%0Ahistorical%20track%20records%20and%20unsustainable%20operations.%20The%20deployment%20of%0Anature-based%20solutions%20in%20the%20UK%20water%20utility%20use%20case%20is%20an%20example%20of%0Afinancing%20green%20-%20driving%20investment%20to%20nature-positive%20outcomes.%20The%20two%20use%0Acases%20also%20cover%20different%20sectors%2C%20geographies%2C%20financial%20assets%20and%20AI%0Amodelling%20techniques%2C%20providing%20an%20overview%20on%20how%20AI%20could%20be%20applied%20to%0Adifferent%20challenges%20relating%20to%20nature%27s%20integration%20into%20finance.%20This%20report%0Ais%20primarily%20aimed%20at%20financial%20institutions%20but%20is%20also%20of%20interest%20to%20ESG%0Adata%20providers%2C%20TNFD%2C%20systems%20modellers%2C%20and%2C%20of%20course%2C%20AI%20practitioners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17369v1&entry.124074799=Read"},
{"title": "Using Neural Implicit Flow To Represent Latent Dynamics Of Canonical\n  Systems", "author": "Imran Nasim and Joa\u00f5 Lucas de Sousa Almeida", "abstract": "  The recently introduced class of architectures known as Neural Operators has\nemerged as highly versatile tools applicable to a wide range of tasks in the\nfield of Scientific Machine Learning (SciML), including data representation and\nforecasting. In this study, we investigate the capabilities of Neural Implicit\nFlow (NIF), a recently developed mesh-agnostic neural operator, for\nrepresenting the latent dynamics of canonical systems such as the\nKuramoto-Sivashinsky (KS), forced Korteweg-de Vries (fKdV), and Sine-Gordon\n(SG) equations, as well as for extracting dynamically relevant information from\nthem. Finally we assess the applicability of NIF as a dimensionality reduction\nalgorithm and conduct a comparative analysis with another widely recognized\nfamily of neural operators, known as Deep Operator Networks (DeepONets).\n", "link": "http://arxiv.org/abs/2404.17535v1", "date": "2024-04-26", "relevancy": 1.0573, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5717}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5095}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5048}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Using%20Neural%20Implicit%20Flow%20To%20Represent%20Latent%20Dynamics%20Of%20Canonical%0A%20%20Systems&body=Title%3A%20Using%20Neural%20Implicit%20Flow%20To%20Represent%20Latent%20Dynamics%20Of%20Canonical%0A%20%20Systems%0AAuthor%3A%20Imran%20Nasim%20and%20Joa%C3%B5%20Lucas%20de%20Sousa%20Almeida%0AAbstract%3A%20%20%20The%20recently%20introduced%20class%20of%20architectures%20known%20as%20Neural%20Operators%20has%0Aemerged%20as%20highly%20versatile%20tools%20applicable%20to%20a%20wide%20range%20of%20tasks%20in%20the%0Afield%20of%20Scientific%20Machine%20Learning%20%28SciML%29%2C%20including%20data%20representation%20and%0Aforecasting.%20In%20this%20study%2C%20we%20investigate%20the%20capabilities%20of%20Neural%20Implicit%0AFlow%20%28NIF%29%2C%20a%20recently%20developed%20mesh-agnostic%20neural%20operator%2C%20for%0Arepresenting%20the%20latent%20dynamics%20of%20canonical%20systems%20such%20as%20the%0AKuramoto-Sivashinsky%20%28KS%29%2C%20forced%20Korteweg-de%20Vries%20%28fKdV%29%2C%20and%20Sine-Gordon%0A%28SG%29%20equations%2C%20as%20well%20as%20for%20extracting%20dynamically%20relevant%20information%20from%0Athem.%20Finally%20we%20assess%20the%20applicability%20of%20NIF%20as%20a%20dimensionality%20reduction%0Aalgorithm%20and%20conduct%20a%20comparative%20analysis%20with%20another%20widely%20recognized%0Afamily%20of%20neural%20operators%2C%20known%20as%20Deep%20Operator%20Networks%20%28DeepONets%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17535v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Neural%20Implicit%20Flow%20To%20Represent%20Latent%20Dynamics%20Of%20Canonical%0A%20%20Systems&entry.906535625=Imran%20Nasim%20and%20Joa%C3%B5%20Lucas%20de%20Sousa%20Almeida&entry.1292438233=%20%20The%20recently%20introduced%20class%20of%20architectures%20known%20as%20Neural%20Operators%20has%0Aemerged%20as%20highly%20versatile%20tools%20applicable%20to%20a%20wide%20range%20of%20tasks%20in%20the%0Afield%20of%20Scientific%20Machine%20Learning%20%28SciML%29%2C%20including%20data%20representation%20and%0Aforecasting.%20In%20this%20study%2C%20we%20investigate%20the%20capabilities%20of%20Neural%20Implicit%0AFlow%20%28NIF%29%2C%20a%20recently%20developed%20mesh-agnostic%20neural%20operator%2C%20for%0Arepresenting%20the%20latent%20dynamics%20of%20canonical%20systems%20such%20as%20the%0AKuramoto-Sivashinsky%20%28KS%29%2C%20forced%20Korteweg-de%20Vries%20%28fKdV%29%2C%20and%20Sine-Gordon%0A%28SG%29%20equations%2C%20as%20well%20as%20for%20extracting%20dynamically%20relevant%20information%20from%0Athem.%20Finally%20we%20assess%20the%20applicability%20of%20NIF%20as%20a%20dimensionality%20reduction%0Aalgorithm%20and%20conduct%20a%20comparative%20analysis%20with%20another%20widely%20recognized%0Afamily%20of%20neural%20operators%2C%20known%20as%20Deep%20Operator%20Networks%20%28DeepONets%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17535v1&entry.124074799=Read"},
{"title": "Transformers in the Service of Description Logic-based Contexts", "author": "Angelos Poulis and Eleni Tsalapati and Manolis Koubarakis", "abstract": "  Recent advancements in transformer-based models have initiated research\ninterests in investigating their ability to learn to perform reasoning tasks.\nHowever, most of the contexts used for this purpose are in practice very\nsimple: generated from short (fragments of) first-order logic sentences with\nonly a few logical operators and quantifiers. In this work, we construct the\nnatural language dataset, DELTA$_D$, using the description logic language\n$\\mathcal{ALCQ}$. DELTA$_D$ contains 384K examples, and increases in two\ndimensions: i) reasoning depth, and ii) linguistic complexity. In this way, we\nsystematically investigate the reasoning ability of a supervised fine-tuned\nDeBERTa-based model and of two large language models (GPT-3.5, GPT-4) with\nfew-shot prompting. Our results demonstrate that the DeBERTa-based model can\nmaster the reasoning task and that the performance of GPTs can improve\nsignificantly even when a small number of samples is provided (9 shots). We\nopen-source our code and datasets.\n", "link": "http://arxiv.org/abs/2311.08941v3", "date": "2024-04-26", "relevancy": 1.4532, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5113}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4894}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4717}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Transformers%20in%20the%20Service%20of%20Description%20Logic-based%20Contexts&body=Title%3A%20Transformers%20in%20the%20Service%20of%20Description%20Logic-based%20Contexts%0AAuthor%3A%20Angelos%20Poulis%20and%20Eleni%20Tsalapati%20and%20Manolis%20Koubarakis%0AAbstract%3A%20%20%20Recent%20advancements%20in%20transformer-based%20models%20have%20initiated%20research%0Ainterests%20in%20investigating%20their%20ability%20to%20learn%20to%20perform%20reasoning%20tasks.%0AHowever%2C%20most%20of%20the%20contexts%20used%20for%20this%20purpose%20are%20in%20practice%20very%0Asimple%3A%20generated%20from%20short%20%28fragments%20of%29%20first-order%20logic%20sentences%20with%0Aonly%20a%20few%20logical%20operators%20and%20quantifiers.%20In%20this%20work%2C%20we%20construct%20the%0Anatural%20language%20dataset%2C%20DELTA%24_D%24%2C%20using%20the%20description%20logic%20language%0A%24%5Cmathcal%7BALCQ%7D%24.%20DELTA%24_D%24%20contains%20384K%20examples%2C%20and%20increases%20in%20two%0Adimensions%3A%20i%29%20reasoning%20depth%2C%20and%20ii%29%20linguistic%20complexity.%20In%20this%20way%2C%20we%0Asystematically%20investigate%20the%20reasoning%20ability%20of%20a%20supervised%20fine-tuned%0ADeBERTa-based%20model%20and%20of%20two%20large%20language%20models%20%28GPT-3.5%2C%20GPT-4%29%20with%0Afew-shot%20prompting.%20Our%20results%20demonstrate%20that%20the%20DeBERTa-based%20model%20can%0Amaster%20the%20reasoning%20task%20and%20that%20the%20performance%20of%20GPTs%20can%20improve%0Asignificantly%20even%20when%20a%20small%20number%20of%20samples%20is%20provided%20%289%20shots%29.%20We%0Aopen-source%20our%20code%20and%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.08941v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformers%20in%20the%20Service%20of%20Description%20Logic-based%20Contexts&entry.906535625=Angelos%20Poulis%20and%20Eleni%20Tsalapati%20and%20Manolis%20Koubarakis&entry.1292438233=%20%20Recent%20advancements%20in%20transformer-based%20models%20have%20initiated%20research%0Ainterests%20in%20investigating%20their%20ability%20to%20learn%20to%20perform%20reasoning%20tasks.%0AHowever%2C%20most%20of%20the%20contexts%20used%20for%20this%20purpose%20are%20in%20practice%20very%0Asimple%3A%20generated%20from%20short%20%28fragments%20of%29%20first-order%20logic%20sentences%20with%0Aonly%20a%20few%20logical%20operators%20and%20quantifiers.%20In%20this%20work%2C%20we%20construct%20the%0Anatural%20language%20dataset%2C%20DELTA%24_D%24%2C%20using%20the%20description%20logic%20language%0A%24%5Cmathcal%7BALCQ%7D%24.%20DELTA%24_D%24%20contains%20384K%20examples%2C%20and%20increases%20in%20two%0Adimensions%3A%20i%29%20reasoning%20depth%2C%20and%20ii%29%20linguistic%20complexity.%20In%20this%20way%2C%20we%0Asystematically%20investigate%20the%20reasoning%20ability%20of%20a%20supervised%20fine-tuned%0ADeBERTa-based%20model%20and%20of%20two%20large%20language%20models%20%28GPT-3.5%2C%20GPT-4%29%20with%0Afew-shot%20prompting.%20Our%20results%20demonstrate%20that%20the%20DeBERTa-based%20model%20can%0Amaster%20the%20reasoning%20task%20and%20that%20the%20performance%20of%20GPTs%20can%20improve%0Asignificantly%20even%20when%20a%20small%20number%20of%20samples%20is%20provided%20%289%20shots%29.%20We%0Aopen-source%20our%20code%20and%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.08941v3&entry.124074799=Read"},
{"title": "The Promise and Challenges of Using LLMs to Accelerate the Screening\n  Process of Systematic Reviews", "author": "Aleksi Huotala and Miikka Kuutila and Paul Ralph and Mika M\u00e4ntyl\u00e4", "abstract": "  Systematic review (SR) is a popular research method in software engineering\n(SE). However, conducting an SR takes an average of 67 weeks. Thus, automating\nany step of the SR process could reduce the effort associated with SRs. Our\nobjective is to investigate if Large Language Models (LLMs) can accelerate\ntitle-abstract screening by simplifying abstracts for human screeners, and\nautomating title-abstract screening. We performed an experiment where humans\nscreened titles and abstracts for 20 papers with both original and simplified\nabstracts from a prior SR. The experiment with human screeners was reproduced\nwith GPT-3.5 and GPT-4 LLMs to perform the same screening tasks. We also\nstudied if different prompting techniques (Zero-shot (ZS), One-shot (OS),\nFew-shot (FS), and Few-shot with Chain-of-Thought (FS-CoT)) improve the\nscreening performance of LLMs. Lastly, we studied if redesigning the prompt\nused in the LLM reproduction of screening leads to improved performance. Text\nsimplification did not increase the screeners' screening performance, but\nreduced the time used in screening. Screeners' scientific literacy skills and\nresearcher status predict screening performance. Some LLM and prompt\ncombinations perform as well as human screeners in the screening tasks. Our\nresults indicate that the GPT-4 LLM is better than its predecessor, GPT-3.5.\nAdditionally, Few-shot and One-shot prompting outperforms Zero-shot prompting.\nUsing LLMs for text simplification in the screening process does not\nsignificantly improve human performance. Using LLMs to automate title-abstract\nscreening seems promising, but current LLMs are not significantly more accurate\nthan human screeners. To recommend the use of LLMs in the screening process of\nSRs, more research is needed. We recommend future SR studies publish\nreplication packages with screening data to enable more conclusive\nexperimenting with LLM screening.\n", "link": "http://arxiv.org/abs/2404.15667v3", "date": "2024-04-26", "relevancy": 1.3511, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4649}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4544}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4258}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Promise%20and%20Challenges%20of%20Using%20LLMs%20to%20Accelerate%20the%20Screening%0A%20%20Process%20of%20Systematic%20Reviews&body=Title%3A%20The%20Promise%20and%20Challenges%20of%20Using%20LLMs%20to%20Accelerate%20the%20Screening%0A%20%20Process%20of%20Systematic%20Reviews%0AAuthor%3A%20Aleksi%20Huotala%20and%20Miikka%20Kuutila%20and%20Paul%20Ralph%20and%20Mika%20M%C3%A4ntyl%C3%A4%0AAbstract%3A%20%20%20Systematic%20review%20%28SR%29%20is%20a%20popular%20research%20method%20in%20software%20engineering%0A%28SE%29.%20However%2C%20conducting%20an%20SR%20takes%20an%20average%20of%2067%20weeks.%20Thus%2C%20automating%0Aany%20step%20of%20the%20SR%20process%20could%20reduce%20the%20effort%20associated%20with%20SRs.%20Our%0Aobjective%20is%20to%20investigate%20if%20Large%20Language%20Models%20%28LLMs%29%20can%20accelerate%0Atitle-abstract%20screening%20by%20simplifying%20abstracts%20for%20human%20screeners%2C%20and%0Aautomating%20title-abstract%20screening.%20We%20performed%20an%20experiment%20where%20humans%0Ascreened%20titles%20and%20abstracts%20for%2020%20papers%20with%20both%20original%20and%20simplified%0Aabstracts%20from%20a%20prior%20SR.%20The%20experiment%20with%20human%20screeners%20was%20reproduced%0Awith%20GPT-3.5%20and%20GPT-4%20LLMs%20to%20perform%20the%20same%20screening%20tasks.%20We%20also%0Astudied%20if%20different%20prompting%20techniques%20%28Zero-shot%20%28ZS%29%2C%20One-shot%20%28OS%29%2C%0AFew-shot%20%28FS%29%2C%20and%20Few-shot%20with%20Chain-of-Thought%20%28FS-CoT%29%29%20improve%20the%0Ascreening%20performance%20of%20LLMs.%20Lastly%2C%20we%20studied%20if%20redesigning%20the%20prompt%0Aused%20in%20the%20LLM%20reproduction%20of%20screening%20leads%20to%20improved%20performance.%20Text%0Asimplification%20did%20not%20increase%20the%20screeners%27%20screening%20performance%2C%20but%0Areduced%20the%20time%20used%20in%20screening.%20Screeners%27%20scientific%20literacy%20skills%20and%0Aresearcher%20status%20predict%20screening%20performance.%20Some%20LLM%20and%20prompt%0Acombinations%20perform%20as%20well%20as%20human%20screeners%20in%20the%20screening%20tasks.%20Our%0Aresults%20indicate%20that%20the%20GPT-4%20LLM%20is%20better%20than%20its%20predecessor%2C%20GPT-3.5.%0AAdditionally%2C%20Few-shot%20and%20One-shot%20prompting%20outperforms%20Zero-shot%20prompting.%0AUsing%20LLMs%20for%20text%20simplification%20in%20the%20screening%20process%20does%20not%0Asignificantly%20improve%20human%20performance.%20Using%20LLMs%20to%20automate%20title-abstract%0Ascreening%20seems%20promising%2C%20but%20current%20LLMs%20are%20not%20significantly%20more%20accurate%0Athan%20human%20screeners.%20To%20recommend%20the%20use%20of%20LLMs%20in%20the%20screening%20process%20of%0ASRs%2C%20more%20research%20is%20needed.%20We%20recommend%20future%20SR%20studies%20publish%0Areplication%20packages%20with%20screening%20data%20to%20enable%20more%20conclusive%0Aexperimenting%20with%20LLM%20screening.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15667v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Promise%20and%20Challenges%20of%20Using%20LLMs%20to%20Accelerate%20the%20Screening%0A%20%20Process%20of%20Systematic%20Reviews&entry.906535625=Aleksi%20Huotala%20and%20Miikka%20Kuutila%20and%20Paul%20Ralph%20and%20Mika%20M%C3%A4ntyl%C3%A4&entry.1292438233=%20%20Systematic%20review%20%28SR%29%20is%20a%20popular%20research%20method%20in%20software%20engineering%0A%28SE%29.%20However%2C%20conducting%20an%20SR%20takes%20an%20average%20of%2067%20weeks.%20Thus%2C%20automating%0Aany%20step%20of%20the%20SR%20process%20could%20reduce%20the%20effort%20associated%20with%20SRs.%20Our%0Aobjective%20is%20to%20investigate%20if%20Large%20Language%20Models%20%28LLMs%29%20can%20accelerate%0Atitle-abstract%20screening%20by%20simplifying%20abstracts%20for%20human%20screeners%2C%20and%0Aautomating%20title-abstract%20screening.%20We%20performed%20an%20experiment%20where%20humans%0Ascreened%20titles%20and%20abstracts%20for%2020%20papers%20with%20both%20original%20and%20simplified%0Aabstracts%20from%20a%20prior%20SR.%20The%20experiment%20with%20human%20screeners%20was%20reproduced%0Awith%20GPT-3.5%20and%20GPT-4%20LLMs%20to%20perform%20the%20same%20screening%20tasks.%20We%20also%0Astudied%20if%20different%20prompting%20techniques%20%28Zero-shot%20%28ZS%29%2C%20One-shot%20%28OS%29%2C%0AFew-shot%20%28FS%29%2C%20and%20Few-shot%20with%20Chain-of-Thought%20%28FS-CoT%29%29%20improve%20the%0Ascreening%20performance%20of%20LLMs.%20Lastly%2C%20we%20studied%20if%20redesigning%20the%20prompt%0Aused%20in%20the%20LLM%20reproduction%20of%20screening%20leads%20to%20improved%20performance.%20Text%0Asimplification%20did%20not%20increase%20the%20screeners%27%20screening%20performance%2C%20but%0Areduced%20the%20time%20used%20in%20screening.%20Screeners%27%20scientific%20literacy%20skills%20and%0Aresearcher%20status%20predict%20screening%20performance.%20Some%20LLM%20and%20prompt%0Acombinations%20perform%20as%20well%20as%20human%20screeners%20in%20the%20screening%20tasks.%20Our%0Aresults%20indicate%20that%20the%20GPT-4%20LLM%20is%20better%20than%20its%20predecessor%2C%20GPT-3.5.%0AAdditionally%2C%20Few-shot%20and%20One-shot%20prompting%20outperforms%20Zero-shot%20prompting.%0AUsing%20LLMs%20for%20text%20simplification%20in%20the%20screening%20process%20does%20not%0Asignificantly%20improve%20human%20performance.%20Using%20LLMs%20to%20automate%20title-abstract%0Ascreening%20seems%20promising%2C%20but%20current%20LLMs%20are%20not%20significantly%20more%20accurate%0Athan%20human%20screeners.%20To%20recommend%20the%20use%20of%20LLMs%20in%20the%20screening%20process%20of%0ASRs%2C%20more%20research%20is%20needed.%20We%20recommend%20future%20SR%20studies%20publish%0Areplication%20packages%20with%20screening%20data%20to%20enable%20more%20conclusive%0Aexperimenting%20with%20LLM%20screening.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15667v3&entry.124074799=Read"},
{"title": "Constrained Neural Networks for Interpretable Heuristic Creation to\n  Optimise Computer Algebra Systems", "author": "Dorian Florescu and Matthew England", "abstract": "  We present a new methodology for utilising machine learning technology in\nsymbolic computation research. We explain how a well known human-designed\nheuristic to make the choice of variable ordering in cylindrical algebraic\ndecomposition may be represented as a constrained neural network. This allows\nus to then use machine learning methods to further optimise the heuristic,\nleading to new networks of similar size, representing new heuristics of similar\ncomplexity as the original human-designed one. We present this as a form of\nante-hoc explainability for use in computer algebra development.\n", "link": "http://arxiv.org/abs/2404.17508v1", "date": "2024-04-26", "relevancy": 1.3709, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4818}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4705}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4416}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Constrained%20Neural%20Networks%20for%20Interpretable%20Heuristic%20Creation%20to%0A%20%20Optimise%20Computer%20Algebra%20Systems&body=Title%3A%20Constrained%20Neural%20Networks%20for%20Interpretable%20Heuristic%20Creation%20to%0A%20%20Optimise%20Computer%20Algebra%20Systems%0AAuthor%3A%20Dorian%20Florescu%20and%20Matthew%20England%0AAbstract%3A%20%20%20We%20present%20a%20new%20methodology%20for%20utilising%20machine%20learning%20technology%20in%0Asymbolic%20computation%20research.%20We%20explain%20how%20a%20well%20known%20human-designed%0Aheuristic%20to%20make%20the%20choice%20of%20variable%20ordering%20in%20cylindrical%20algebraic%0Adecomposition%20may%20be%20represented%20as%20a%20constrained%20neural%20network.%20This%20allows%0Aus%20to%20then%20use%20machine%20learning%20methods%20to%20further%20optimise%20the%20heuristic%2C%0Aleading%20to%20new%20networks%20of%20similar%20size%2C%20representing%20new%20heuristics%20of%20similar%0Acomplexity%20as%20the%20original%20human-designed%20one.%20We%20present%20this%20as%20a%20form%20of%0Aante-hoc%20explainability%20for%20use%20in%20computer%20algebra%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17508v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constrained%20Neural%20Networks%20for%20Interpretable%20Heuristic%20Creation%20to%0A%20%20Optimise%20Computer%20Algebra%20Systems&entry.906535625=Dorian%20Florescu%20and%20Matthew%20England&entry.1292438233=%20%20We%20present%20a%20new%20methodology%20for%20utilising%20machine%20learning%20technology%20in%0Asymbolic%20computation%20research.%20We%20explain%20how%20a%20well%20known%20human-designed%0Aheuristic%20to%20make%20the%20choice%20of%20variable%20ordering%20in%20cylindrical%20algebraic%0Adecomposition%20may%20be%20represented%20as%20a%20constrained%20neural%20network.%20This%20allows%0Aus%20to%20then%20use%20machine%20learning%20methods%20to%20further%20optimise%20the%20heuristic%2C%0Aleading%20to%20new%20networks%20of%20similar%20size%2C%20representing%20new%20heuristics%20of%20similar%0Acomplexity%20as%20the%20original%20human-designed%20one.%20We%20present%20this%20as%20a%20form%20of%0Aante-hoc%20explainability%20for%20use%20in%20computer%20algebra%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17508v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


