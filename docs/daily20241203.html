<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241202.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Beyond Gaussians: Fast and High-Fidelity 3D Splatting with Linear\n  Kernels", "author": "Haodong Chen and Runnan Chen and Qiang Qu and Zhaoqing Wang and Tongliang Liu and Xiaoming Chen and Yuk Ying Chung", "abstract": "  Recent advancements in 3D Gaussian Splatting (3DGS) have substantially\nimproved novel view synthesis, enabling high-quality reconstruction and\nreal-time rendering. However, blurring artifacts, such as floating primitives\nand over-reconstruction, remain challenging. Current methods address these\nissues by refining scene structure, enhancing geometric representations,\naddressing blur in training images, improving rendering consistency, and\noptimizing density control, yet the role of kernel design remains\nunderexplored. We identify the soft boundaries of Gaussian ellipsoids as one of\nthe causes of these artifacts, limiting detail capture in high-frequency\nregions. To bridge this gap, we introduce 3D Linear Splatting (3DLS), which\nreplaces Gaussian kernels with linear kernels to achieve sharper and more\nprecise results, particularly in high-frequency regions. Through evaluations on\nthree datasets, 3DLS demonstrates state-of-the-art fidelity and accuracy, along\nwith a 30% FPS improvement over baseline 3DGS. The implementation will be made\npublicly available upon acceptance.\n", "link": "http://arxiv.org/abs/2411.12440v3", "date": "2024-12-02", "relevancy": 3.3169, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6905}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6689}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Gaussians%3A%20Fast%20and%20High-Fidelity%203D%20Splatting%20with%20Linear%0A%20%20Kernels&body=Title%3A%20Beyond%20Gaussians%3A%20Fast%20and%20High-Fidelity%203D%20Splatting%20with%20Linear%0A%20%20Kernels%0AAuthor%3A%20Haodong%20Chen%20and%20Runnan%20Chen%20and%20Qiang%20Qu%20and%20Zhaoqing%20Wang%20and%20Tongliang%20Liu%20and%20Xiaoming%20Chen%20and%20Yuk%20Ying%20Chung%0AAbstract%3A%20%20%20Recent%20advancements%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20substantially%0Aimproved%20novel%20view%20synthesis%2C%20enabling%20high-quality%20reconstruction%20and%0Areal-time%20rendering.%20However%2C%20blurring%20artifacts%2C%20such%20as%20floating%20primitives%0Aand%20over-reconstruction%2C%20remain%20challenging.%20Current%20methods%20address%20these%0Aissues%20by%20refining%20scene%20structure%2C%20enhancing%20geometric%20representations%2C%0Aaddressing%20blur%20in%20training%20images%2C%20improving%20rendering%20consistency%2C%20and%0Aoptimizing%20density%20control%2C%20yet%20the%20role%20of%20kernel%20design%20remains%0Aunderexplored.%20We%20identify%20the%20soft%20boundaries%20of%20Gaussian%20ellipsoids%20as%20one%20of%0Athe%20causes%20of%20these%20artifacts%2C%20limiting%20detail%20capture%20in%20high-frequency%0Aregions.%20To%20bridge%20this%20gap%2C%20we%20introduce%203D%20Linear%20Splatting%20%283DLS%29%2C%20which%0Areplaces%20Gaussian%20kernels%20with%20linear%20kernels%20to%20achieve%20sharper%20and%20more%0Aprecise%20results%2C%20particularly%20in%20high-frequency%20regions.%20Through%20evaluations%20on%0Athree%20datasets%2C%203DLS%20demonstrates%20state-of-the-art%20fidelity%20and%20accuracy%2C%20along%0Awith%20a%2030%25%20FPS%20improvement%20over%20baseline%203DGS.%20The%20implementation%20will%20be%20made%0Apublicly%20available%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12440v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Gaussians%253A%2520Fast%2520and%2520High-Fidelity%25203D%2520Splatting%2520with%2520Linear%250A%2520%2520Kernels%26entry.906535625%3DHaodong%2520Chen%2520and%2520Runnan%2520Chen%2520and%2520Qiang%2520Qu%2520and%2520Zhaoqing%2520Wang%2520and%2520Tongliang%2520Liu%2520and%2520Xiaoming%2520Chen%2520and%2520Yuk%2520Ying%2520Chung%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520have%2520substantially%250Aimproved%2520novel%2520view%2520synthesis%252C%2520enabling%2520high-quality%2520reconstruction%2520and%250Areal-time%2520rendering.%2520However%252C%2520blurring%2520artifacts%252C%2520such%2520as%2520floating%2520primitives%250Aand%2520over-reconstruction%252C%2520remain%2520challenging.%2520Current%2520methods%2520address%2520these%250Aissues%2520by%2520refining%2520scene%2520structure%252C%2520enhancing%2520geometric%2520representations%252C%250Aaddressing%2520blur%2520in%2520training%2520images%252C%2520improving%2520rendering%2520consistency%252C%2520and%250Aoptimizing%2520density%2520control%252C%2520yet%2520the%2520role%2520of%2520kernel%2520design%2520remains%250Aunderexplored.%2520We%2520identify%2520the%2520soft%2520boundaries%2520of%2520Gaussian%2520ellipsoids%2520as%2520one%2520of%250Athe%2520causes%2520of%2520these%2520artifacts%252C%2520limiting%2520detail%2520capture%2520in%2520high-frequency%250Aregions.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%25203D%2520Linear%2520Splatting%2520%25283DLS%2529%252C%2520which%250Areplaces%2520Gaussian%2520kernels%2520with%2520linear%2520kernels%2520to%2520achieve%2520sharper%2520and%2520more%250Aprecise%2520results%252C%2520particularly%2520in%2520high-frequency%2520regions.%2520Through%2520evaluations%2520on%250Athree%2520datasets%252C%25203DLS%2520demonstrates%2520state-of-the-art%2520fidelity%2520and%2520accuracy%252C%2520along%250Awith%2520a%252030%2525%2520FPS%2520improvement%2520over%2520baseline%25203DGS.%2520The%2520implementation%2520will%2520be%2520made%250Apublicly%2520available%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12440v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Gaussians%3A%20Fast%20and%20High-Fidelity%203D%20Splatting%20with%20Linear%0A%20%20Kernels&entry.906535625=Haodong%20Chen%20and%20Runnan%20Chen%20and%20Qiang%20Qu%20and%20Zhaoqing%20Wang%20and%20Tongliang%20Liu%20and%20Xiaoming%20Chen%20and%20Yuk%20Ying%20Chung&entry.1292438233=%20%20Recent%20advancements%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20substantially%0Aimproved%20novel%20view%20synthesis%2C%20enabling%20high-quality%20reconstruction%20and%0Areal-time%20rendering.%20However%2C%20blurring%20artifacts%2C%20such%20as%20floating%20primitives%0Aand%20over-reconstruction%2C%20remain%20challenging.%20Current%20methods%20address%20these%0Aissues%20by%20refining%20scene%20structure%2C%20enhancing%20geometric%20representations%2C%0Aaddressing%20blur%20in%20training%20images%2C%20improving%20rendering%20consistency%2C%20and%0Aoptimizing%20density%20control%2C%20yet%20the%20role%20of%20kernel%20design%20remains%0Aunderexplored.%20We%20identify%20the%20soft%20boundaries%20of%20Gaussian%20ellipsoids%20as%20one%20of%0Athe%20causes%20of%20these%20artifacts%2C%20limiting%20detail%20capture%20in%20high-frequency%0Aregions.%20To%20bridge%20this%20gap%2C%20we%20introduce%203D%20Linear%20Splatting%20%283DLS%29%2C%20which%0Areplaces%20Gaussian%20kernels%20with%20linear%20kernels%20to%20achieve%20sharper%20and%20more%0Aprecise%20results%2C%20particularly%20in%20high-frequency%20regions.%20Through%20evaluations%20on%0Athree%20datasets%2C%203DLS%20demonstrates%20state-of-the-art%20fidelity%20and%20accuracy%2C%20along%0Awith%20a%2030%25%20FPS%20improvement%20over%20baseline%203DGS.%20The%20implementation%20will%20be%20made%0Apublicly%20available%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12440v3&entry.124074799=Read"},
{"title": "PartGS:Learning Part-aware 3D Representations by Fusing 2D Gaussians and\n  Superquadrics", "author": "Zhirui Gao and Renjiao Yi and Yuhang Huang and Wei Chen and Chenyang Zhu and Kai Xu", "abstract": "  Low-level 3D representations, such as point clouds, meshes, NeRFs, and 3D\nGaussians, are commonly used to represent 3D objects or scenes. However, human\nperception typically understands 3D objects at a higher level as a composition\nof parts or structures rather than points or voxels. Representing 3D objects or\nscenes as semantic parts can benefit further understanding and applications. In\nthis paper, we introduce $\\textbf{PartGS}$, $\\textbf{part}$-aware 3D\nreconstruction by a hybrid representation of 2D $\\textbf{G}$aussians and\n$\\textbf{S}$uperquadrics, which parses objects or scenes into semantic parts,\ndigging 3D structural clues from multi-view image inputs. Accurate structured\ngeometry reconstruction and high-quality rendering are achieved at the same\ntime. Our method simultaneously optimizes superquadric meshes and Gaussians by\ncoupling their parameters within our hybrid representation. On one hand, this\nhybrid representation inherits the advantage of superquadrics to represent\ndifferent shape primitives, supporting flexible part decomposition of scenes.\nOn the other hand, 2D Gaussians capture complex texture and geometry details,\nensuring high-quality appearance and geometry reconstruction. Our method is\nfully unsupervised and outperforms existing state-of-the-art approaches in\nextensive experiments on DTU, ShapeNet, and real-life datasets.\n", "link": "http://arxiv.org/abs/2408.10789v2", "date": "2024-12-02", "relevancy": 3.2683, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6631}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6523}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PartGS%3ALearning%20Part-aware%203D%20Representations%20by%20Fusing%202D%20Gaussians%20and%0A%20%20Superquadrics&body=Title%3A%20PartGS%3ALearning%20Part-aware%203D%20Representations%20by%20Fusing%202D%20Gaussians%20and%0A%20%20Superquadrics%0AAuthor%3A%20Zhirui%20Gao%20and%20Renjiao%20Yi%20and%20Yuhang%20Huang%20and%20Wei%20Chen%20and%20Chenyang%20Zhu%20and%20Kai%20Xu%0AAbstract%3A%20%20%20Low-level%203D%20representations%2C%20such%20as%20point%20clouds%2C%20meshes%2C%20NeRFs%2C%20and%203D%0AGaussians%2C%20are%20commonly%20used%20to%20represent%203D%20objects%20or%20scenes.%20However%2C%20human%0Aperception%20typically%20understands%203D%20objects%20at%20a%20higher%20level%20as%20a%20composition%0Aof%20parts%20or%20structures%20rather%20than%20points%20or%20voxels.%20Representing%203D%20objects%20or%0Ascenes%20as%20semantic%20parts%20can%20benefit%20further%20understanding%20and%20applications.%20In%0Athis%20paper%2C%20we%20introduce%20%24%5Ctextbf%7BPartGS%7D%24%2C%20%24%5Ctextbf%7Bpart%7D%24-aware%203D%0Areconstruction%20by%20a%20hybrid%20representation%20of%202D%20%24%5Ctextbf%7BG%7D%24aussians%20and%0A%24%5Ctextbf%7BS%7D%24uperquadrics%2C%20which%20parses%20objects%20or%20scenes%20into%20semantic%20parts%2C%0Adigging%203D%20structural%20clues%20from%20multi-view%20image%20inputs.%20Accurate%20structured%0Ageometry%20reconstruction%20and%20high-quality%20rendering%20are%20achieved%20at%20the%20same%0Atime.%20Our%20method%20simultaneously%20optimizes%20superquadric%20meshes%20and%20Gaussians%20by%0Acoupling%20their%20parameters%20within%20our%20hybrid%20representation.%20On%20one%20hand%2C%20this%0Ahybrid%20representation%20inherits%20the%20advantage%20of%20superquadrics%20to%20represent%0Adifferent%20shape%20primitives%2C%20supporting%20flexible%20part%20decomposition%20of%20scenes.%0AOn%20the%20other%20hand%2C%202D%20Gaussians%20capture%20complex%20texture%20and%20geometry%20details%2C%0Aensuring%20high-quality%20appearance%20and%20geometry%20reconstruction.%20Our%20method%20is%0Afully%20unsupervised%20and%20outperforms%20existing%20state-of-the-art%20approaches%20in%0Aextensive%20experiments%20on%20DTU%2C%20ShapeNet%2C%20and%20real-life%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10789v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartGS%253ALearning%2520Part-aware%25203D%2520Representations%2520by%2520Fusing%25202D%2520Gaussians%2520and%250A%2520%2520Superquadrics%26entry.906535625%3DZhirui%2520Gao%2520and%2520Renjiao%2520Yi%2520and%2520Yuhang%2520Huang%2520and%2520Wei%2520Chen%2520and%2520Chenyang%2520Zhu%2520and%2520Kai%2520Xu%26entry.1292438233%3D%2520%2520Low-level%25203D%2520representations%252C%2520such%2520as%2520point%2520clouds%252C%2520meshes%252C%2520NeRFs%252C%2520and%25203D%250AGaussians%252C%2520are%2520commonly%2520used%2520to%2520represent%25203D%2520objects%2520or%2520scenes.%2520However%252C%2520human%250Aperception%2520typically%2520understands%25203D%2520objects%2520at%2520a%2520higher%2520level%2520as%2520a%2520composition%250Aof%2520parts%2520or%2520structures%2520rather%2520than%2520points%2520or%2520voxels.%2520Representing%25203D%2520objects%2520or%250Ascenes%2520as%2520semantic%2520parts%2520can%2520benefit%2520further%2520understanding%2520and%2520applications.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520%2524%255Ctextbf%257BPartGS%257D%2524%252C%2520%2524%255Ctextbf%257Bpart%257D%2524-aware%25203D%250Areconstruction%2520by%2520a%2520hybrid%2520representation%2520of%25202D%2520%2524%255Ctextbf%257BG%257D%2524aussians%2520and%250A%2524%255Ctextbf%257BS%257D%2524uperquadrics%252C%2520which%2520parses%2520objects%2520or%2520scenes%2520into%2520semantic%2520parts%252C%250Adigging%25203D%2520structural%2520clues%2520from%2520multi-view%2520image%2520inputs.%2520Accurate%2520structured%250Ageometry%2520reconstruction%2520and%2520high-quality%2520rendering%2520are%2520achieved%2520at%2520the%2520same%250Atime.%2520Our%2520method%2520simultaneously%2520optimizes%2520superquadric%2520meshes%2520and%2520Gaussians%2520by%250Acoupling%2520their%2520parameters%2520within%2520our%2520hybrid%2520representation.%2520On%2520one%2520hand%252C%2520this%250Ahybrid%2520representation%2520inherits%2520the%2520advantage%2520of%2520superquadrics%2520to%2520represent%250Adifferent%2520shape%2520primitives%252C%2520supporting%2520flexible%2520part%2520decomposition%2520of%2520scenes.%250AOn%2520the%2520other%2520hand%252C%25202D%2520Gaussians%2520capture%2520complex%2520texture%2520and%2520geometry%2520details%252C%250Aensuring%2520high-quality%2520appearance%2520and%2520geometry%2520reconstruction.%2520Our%2520method%2520is%250Afully%2520unsupervised%2520and%2520outperforms%2520existing%2520state-of-the-art%2520approaches%2520in%250Aextensive%2520experiments%2520on%2520DTU%252C%2520ShapeNet%252C%2520and%2520real-life%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10789v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PartGS%3ALearning%20Part-aware%203D%20Representations%20by%20Fusing%202D%20Gaussians%20and%0A%20%20Superquadrics&entry.906535625=Zhirui%20Gao%20and%20Renjiao%20Yi%20and%20Yuhang%20Huang%20and%20Wei%20Chen%20and%20Chenyang%20Zhu%20and%20Kai%20Xu&entry.1292438233=%20%20Low-level%203D%20representations%2C%20such%20as%20point%20clouds%2C%20meshes%2C%20NeRFs%2C%20and%203D%0AGaussians%2C%20are%20commonly%20used%20to%20represent%203D%20objects%20or%20scenes.%20However%2C%20human%0Aperception%20typically%20understands%203D%20objects%20at%20a%20higher%20level%20as%20a%20composition%0Aof%20parts%20or%20structures%20rather%20than%20points%20or%20voxels.%20Representing%203D%20objects%20or%0Ascenes%20as%20semantic%20parts%20can%20benefit%20further%20understanding%20and%20applications.%20In%0Athis%20paper%2C%20we%20introduce%20%24%5Ctextbf%7BPartGS%7D%24%2C%20%24%5Ctextbf%7Bpart%7D%24-aware%203D%0Areconstruction%20by%20a%20hybrid%20representation%20of%202D%20%24%5Ctextbf%7BG%7D%24aussians%20and%0A%24%5Ctextbf%7BS%7D%24uperquadrics%2C%20which%20parses%20objects%20or%20scenes%20into%20semantic%20parts%2C%0Adigging%203D%20structural%20clues%20from%20multi-view%20image%20inputs.%20Accurate%20structured%0Ageometry%20reconstruction%20and%20high-quality%20rendering%20are%20achieved%20at%20the%20same%0Atime.%20Our%20method%20simultaneously%20optimizes%20superquadric%20meshes%20and%20Gaussians%20by%0Acoupling%20their%20parameters%20within%20our%20hybrid%20representation.%20On%20one%20hand%2C%20this%0Ahybrid%20representation%20inherits%20the%20advantage%20of%20superquadrics%20to%20represent%0Adifferent%20shape%20primitives%2C%20supporting%20flexible%20part%20decomposition%20of%20scenes.%0AOn%20the%20other%20hand%2C%202D%20Gaussians%20capture%20complex%20texture%20and%20geometry%20details%2C%0Aensuring%20high-quality%20appearance%20and%20geometry%20reconstruction.%20Our%20method%20is%0Afully%20unsupervised%20and%20outperforms%20existing%20state-of-the-art%20approaches%20in%0Aextensive%20experiments%20on%20DTU%2C%20ShapeNet%2C%20and%20real-life%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10789v2&entry.124074799=Read"},
{"title": "DUSt3R: Geometric 3D Vision Made Easy", "author": "Shuzhe Wang and Vincent Leroy and Yohann Cabon and Boris Chidlovskii and Jerome Revaud", "abstract": "  Multi-view stereo reconstruction (MVS) in the wild requires to first estimate\nthe camera parameters e.g. intrinsic and extrinsic parameters. These are\nusually tedious and cumbersome to obtain, yet they are mandatory to triangulate\ncorresponding pixels in 3D space, which is the core of all best performing MVS\nalgorithms. In this work, we take an opposite stance and introduce DUSt3R, a\nradically novel paradigm for Dense and Unconstrained Stereo 3D Reconstruction\nof arbitrary image collections, i.e. operating without prior information about\ncamera calibration nor viewpoint poses. We cast the pairwise reconstruction\nproblem as a regression of pointmaps, relaxing the hard constraints of usual\nprojective camera models. We show that this formulation smoothly unifies the\nmonocular and binocular reconstruction cases. In the case where more than two\nimages are provided, we further propose a simple yet effective global alignment\nstrategy that expresses all pairwise pointmaps in a common reference frame. We\nbase our network architecture on standard Transformer encoders and decoders,\nallowing us to leverage powerful pretrained models. Our formulation directly\nprovides a 3D model of the scene as well as depth information, but\ninterestingly, we can seamlessly recover from it, pixel matches, relative and\nabsolute camera. Exhaustive experiments on all these tasks showcase that the\nproposed DUSt3R can unify various 3D vision tasks and set new SoTAs on\nmonocular/multi-view depth estimation as well as relative pose estimation. In\nsummary, DUSt3R makes many geometric 3D vision tasks easy.\n", "link": "http://arxiv.org/abs/2312.14132v3", "date": "2024-12-02", "relevancy": 3.2325, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6523}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6523}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DUSt3R%3A%20Geometric%203D%20Vision%20Made%20Easy&body=Title%3A%20DUSt3R%3A%20Geometric%203D%20Vision%20Made%20Easy%0AAuthor%3A%20Shuzhe%20Wang%20and%20Vincent%20Leroy%20and%20Yohann%20Cabon%20and%20Boris%20Chidlovskii%20and%20Jerome%20Revaud%0AAbstract%3A%20%20%20Multi-view%20stereo%20reconstruction%20%28MVS%29%20in%20the%20wild%20requires%20to%20first%20estimate%0Athe%20camera%20parameters%20e.g.%20intrinsic%20and%20extrinsic%20parameters.%20These%20are%0Ausually%20tedious%20and%20cumbersome%20to%20obtain%2C%20yet%20they%20are%20mandatory%20to%20triangulate%0Acorresponding%20pixels%20in%203D%20space%2C%20which%20is%20the%20core%20of%20all%20best%20performing%20MVS%0Aalgorithms.%20In%20this%20work%2C%20we%20take%20an%20opposite%20stance%20and%20introduce%20DUSt3R%2C%20a%0Aradically%20novel%20paradigm%20for%20Dense%20and%20Unconstrained%20Stereo%203D%20Reconstruction%0Aof%20arbitrary%20image%20collections%2C%20i.e.%20operating%20without%20prior%20information%20about%0Acamera%20calibration%20nor%20viewpoint%20poses.%20We%20cast%20the%20pairwise%20reconstruction%0Aproblem%20as%20a%20regression%20of%20pointmaps%2C%20relaxing%20the%20hard%20constraints%20of%20usual%0Aprojective%20camera%20models.%20We%20show%20that%20this%20formulation%20smoothly%20unifies%20the%0Amonocular%20and%20binocular%20reconstruction%20cases.%20In%20the%20case%20where%20more%20than%20two%0Aimages%20are%20provided%2C%20we%20further%20propose%20a%20simple%20yet%20effective%20global%20alignment%0Astrategy%20that%20expresses%20all%20pairwise%20pointmaps%20in%20a%20common%20reference%20frame.%20We%0Abase%20our%20network%20architecture%20on%20standard%20Transformer%20encoders%20and%20decoders%2C%0Aallowing%20us%20to%20leverage%20powerful%20pretrained%20models.%20Our%20formulation%20directly%0Aprovides%20a%203D%20model%20of%20the%20scene%20as%20well%20as%20depth%20information%2C%20but%0Ainterestingly%2C%20we%20can%20seamlessly%20recover%20from%20it%2C%20pixel%20matches%2C%20relative%20and%0Aabsolute%20camera.%20Exhaustive%20experiments%20on%20all%20these%20tasks%20showcase%20that%20the%0Aproposed%20DUSt3R%20can%20unify%20various%203D%20vision%20tasks%20and%20set%20new%20SoTAs%20on%0Amonocular/multi-view%20depth%20estimation%20as%20well%20as%20relative%20pose%20estimation.%20In%0Asummary%2C%20DUSt3R%20makes%20many%20geometric%203D%20vision%20tasks%20easy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14132v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDUSt3R%253A%2520Geometric%25203D%2520Vision%2520Made%2520Easy%26entry.906535625%3DShuzhe%2520Wang%2520and%2520Vincent%2520Leroy%2520and%2520Yohann%2520Cabon%2520and%2520Boris%2520Chidlovskii%2520and%2520Jerome%2520Revaud%26entry.1292438233%3D%2520%2520Multi-view%2520stereo%2520reconstruction%2520%2528MVS%2529%2520in%2520the%2520wild%2520requires%2520to%2520first%2520estimate%250Athe%2520camera%2520parameters%2520e.g.%2520intrinsic%2520and%2520extrinsic%2520parameters.%2520These%2520are%250Ausually%2520tedious%2520and%2520cumbersome%2520to%2520obtain%252C%2520yet%2520they%2520are%2520mandatory%2520to%2520triangulate%250Acorresponding%2520pixels%2520in%25203D%2520space%252C%2520which%2520is%2520the%2520core%2520of%2520all%2520best%2520performing%2520MVS%250Aalgorithms.%2520In%2520this%2520work%252C%2520we%2520take%2520an%2520opposite%2520stance%2520and%2520introduce%2520DUSt3R%252C%2520a%250Aradically%2520novel%2520paradigm%2520for%2520Dense%2520and%2520Unconstrained%2520Stereo%25203D%2520Reconstruction%250Aof%2520arbitrary%2520image%2520collections%252C%2520i.e.%2520operating%2520without%2520prior%2520information%2520about%250Acamera%2520calibration%2520nor%2520viewpoint%2520poses.%2520We%2520cast%2520the%2520pairwise%2520reconstruction%250Aproblem%2520as%2520a%2520regression%2520of%2520pointmaps%252C%2520relaxing%2520the%2520hard%2520constraints%2520of%2520usual%250Aprojective%2520camera%2520models.%2520We%2520show%2520that%2520this%2520formulation%2520smoothly%2520unifies%2520the%250Amonocular%2520and%2520binocular%2520reconstruction%2520cases.%2520In%2520the%2520case%2520where%2520more%2520than%2520two%250Aimages%2520are%2520provided%252C%2520we%2520further%2520propose%2520a%2520simple%2520yet%2520effective%2520global%2520alignment%250Astrategy%2520that%2520expresses%2520all%2520pairwise%2520pointmaps%2520in%2520a%2520common%2520reference%2520frame.%2520We%250Abase%2520our%2520network%2520architecture%2520on%2520standard%2520Transformer%2520encoders%2520and%2520decoders%252C%250Aallowing%2520us%2520to%2520leverage%2520powerful%2520pretrained%2520models.%2520Our%2520formulation%2520directly%250Aprovides%2520a%25203D%2520model%2520of%2520the%2520scene%2520as%2520well%2520as%2520depth%2520information%252C%2520but%250Ainterestingly%252C%2520we%2520can%2520seamlessly%2520recover%2520from%2520it%252C%2520pixel%2520matches%252C%2520relative%2520and%250Aabsolute%2520camera.%2520Exhaustive%2520experiments%2520on%2520all%2520these%2520tasks%2520showcase%2520that%2520the%250Aproposed%2520DUSt3R%2520can%2520unify%2520various%25203D%2520vision%2520tasks%2520and%2520set%2520new%2520SoTAs%2520on%250Amonocular/multi-view%2520depth%2520estimation%2520as%2520well%2520as%2520relative%2520pose%2520estimation.%2520In%250Asummary%252C%2520DUSt3R%2520makes%2520many%2520geometric%25203D%2520vision%2520tasks%2520easy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14132v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DUSt3R%3A%20Geometric%203D%20Vision%20Made%20Easy&entry.906535625=Shuzhe%20Wang%20and%20Vincent%20Leroy%20and%20Yohann%20Cabon%20and%20Boris%20Chidlovskii%20and%20Jerome%20Revaud&entry.1292438233=%20%20Multi-view%20stereo%20reconstruction%20%28MVS%29%20in%20the%20wild%20requires%20to%20first%20estimate%0Athe%20camera%20parameters%20e.g.%20intrinsic%20and%20extrinsic%20parameters.%20These%20are%0Ausually%20tedious%20and%20cumbersome%20to%20obtain%2C%20yet%20they%20are%20mandatory%20to%20triangulate%0Acorresponding%20pixels%20in%203D%20space%2C%20which%20is%20the%20core%20of%20all%20best%20performing%20MVS%0Aalgorithms.%20In%20this%20work%2C%20we%20take%20an%20opposite%20stance%20and%20introduce%20DUSt3R%2C%20a%0Aradically%20novel%20paradigm%20for%20Dense%20and%20Unconstrained%20Stereo%203D%20Reconstruction%0Aof%20arbitrary%20image%20collections%2C%20i.e.%20operating%20without%20prior%20information%20about%0Acamera%20calibration%20nor%20viewpoint%20poses.%20We%20cast%20the%20pairwise%20reconstruction%0Aproblem%20as%20a%20regression%20of%20pointmaps%2C%20relaxing%20the%20hard%20constraints%20of%20usual%0Aprojective%20camera%20models.%20We%20show%20that%20this%20formulation%20smoothly%20unifies%20the%0Amonocular%20and%20binocular%20reconstruction%20cases.%20In%20the%20case%20where%20more%20than%20two%0Aimages%20are%20provided%2C%20we%20further%20propose%20a%20simple%20yet%20effective%20global%20alignment%0Astrategy%20that%20expresses%20all%20pairwise%20pointmaps%20in%20a%20common%20reference%20frame.%20We%0Abase%20our%20network%20architecture%20on%20standard%20Transformer%20encoders%20and%20decoders%2C%0Aallowing%20us%20to%20leverage%20powerful%20pretrained%20models.%20Our%20formulation%20directly%0Aprovides%20a%203D%20model%20of%20the%20scene%20as%20well%20as%20depth%20information%2C%20but%0Ainterestingly%2C%20we%20can%20seamlessly%20recover%20from%20it%2C%20pixel%20matches%2C%20relative%20and%0Aabsolute%20camera.%20Exhaustive%20experiments%20on%20all%20these%20tasks%20showcase%20that%20the%0Aproposed%20DUSt3R%20can%20unify%20various%203D%20vision%20tasks%20and%20set%20new%20SoTAs%20on%0Amonocular/multi-view%20depth%20estimation%20as%20well%20as%20relative%20pose%20estimation.%20In%0Asummary%2C%20DUSt3R%20makes%20many%20geometric%203D%20vision%20tasks%20easy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14132v3&entry.124074799=Read"},
{"title": "AniFaceDiff: Animating Stylized Avatars via Parametric Conditioned\n  Diffusion Models", "author": "Ken Chen and Sachith Seneviratne and Wei Wang and Dongting Hu and Sanjay Saha and Md. Tarek Hasan and Sanka Rasnayaka and Tamasha Malepathirana and Mingming Gong and Saman Halgamuge", "abstract": "  Animating stylized avatars with dynamic poses and expressions has attracted\nincreasing attention for its broad range of applications. Previous research has\nmade significant progress by training controllable generative models to\nsynthesize animations based on reference characteristics, pose, and expression\nconditions. However, the mechanisms used in these methods to control pose and\nexpression often inadvertently introduce unintended features from the target\nmotion, while also causing a loss of expression-related details, particularly\nwhen applied to stylized animation. This paper proposes a new method based on\nStable Diffusion, called AniFaceDiff, incorporating a new conditioning module\nfor animating stylized avatars. First, we propose a refined spatial\nconditioning approach by Facial Alignment to prevent the inclusion of identity\ncharacteristics from the target motion. Then, we introduce an Expression\nAdapter that incorporates additional cross-attention layers to address the\npotential loss of expression-related information. Our approach effectively\npreserves pose and expression from the target video while maintaining input\nimage consistency. Extensive experiments demonstrate that our method achieves\nstate-of-the-art results, showcasing superior image quality, preservation of\nreference features, and expression accuracy, particularly for out-of-domain\nanimation across diverse styles, highlighting its versatility and strong\ngeneralization capabilities. This work aims to enhance the quality of virtual\nstylized animation for positive applications. To promote responsible use in\nvirtual environments, we contribute to the advancement of detection for\ngenerative content by evaluating state-of-the-art detectors, highlighting\npotential areas for improvement, and suggesting solutions.\n", "link": "http://arxiv.org/abs/2406.13272v2", "date": "2024-12-02", "relevancy": 3.1943, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6474}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6357}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AniFaceDiff%3A%20Animating%20Stylized%20Avatars%20via%20Parametric%20Conditioned%0A%20%20Diffusion%20Models&body=Title%3A%20AniFaceDiff%3A%20Animating%20Stylized%20Avatars%20via%20Parametric%20Conditioned%0A%20%20Diffusion%20Models%0AAuthor%3A%20Ken%20Chen%20and%20Sachith%20Seneviratne%20and%20Wei%20Wang%20and%20Dongting%20Hu%20and%20Sanjay%20Saha%20and%20Md.%20Tarek%20Hasan%20and%20Sanka%20Rasnayaka%20and%20Tamasha%20Malepathirana%20and%20Mingming%20Gong%20and%20Saman%20Halgamuge%0AAbstract%3A%20%20%20Animating%20stylized%20avatars%20with%20dynamic%20poses%20and%20expressions%20has%20attracted%0Aincreasing%20attention%20for%20its%20broad%20range%20of%20applications.%20Previous%20research%20has%0Amade%20significant%20progress%20by%20training%20controllable%20generative%20models%20to%0Asynthesize%20animations%20based%20on%20reference%20characteristics%2C%20pose%2C%20and%20expression%0Aconditions.%20However%2C%20the%20mechanisms%20used%20in%20these%20methods%20to%20control%20pose%20and%0Aexpression%20often%20inadvertently%20introduce%20unintended%20features%20from%20the%20target%0Amotion%2C%20while%20also%20causing%20a%20loss%20of%20expression-related%20details%2C%20particularly%0Awhen%20applied%20to%20stylized%20animation.%20This%20paper%20proposes%20a%20new%20method%20based%20on%0AStable%20Diffusion%2C%20called%20AniFaceDiff%2C%20incorporating%20a%20new%20conditioning%20module%0Afor%20animating%20stylized%20avatars.%20First%2C%20we%20propose%20a%20refined%20spatial%0Aconditioning%20approach%20by%20Facial%20Alignment%20to%20prevent%20the%20inclusion%20of%20identity%0Acharacteristics%20from%20the%20target%20motion.%20Then%2C%20we%20introduce%20an%20Expression%0AAdapter%20that%20incorporates%20additional%20cross-attention%20layers%20to%20address%20the%0Apotential%20loss%20of%20expression-related%20information.%20Our%20approach%20effectively%0Apreserves%20pose%20and%20expression%20from%20the%20target%20video%20while%20maintaining%20input%0Aimage%20consistency.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20results%2C%20showcasing%20superior%20image%20quality%2C%20preservation%20of%0Areference%20features%2C%20and%20expression%20accuracy%2C%20particularly%20for%20out-of-domain%0Aanimation%20across%20diverse%20styles%2C%20highlighting%20its%20versatility%20and%20strong%0Ageneralization%20capabilities.%20This%20work%20aims%20to%20enhance%20the%20quality%20of%20virtual%0Astylized%20animation%20for%20positive%20applications.%20To%20promote%20responsible%20use%20in%0Avirtual%20environments%2C%20we%20contribute%20to%20the%20advancement%20of%20detection%20for%0Agenerative%20content%20by%20evaluating%20state-of-the-art%20detectors%2C%20highlighting%0Apotential%20areas%20for%20improvement%2C%20and%20suggesting%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13272v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAniFaceDiff%253A%2520Animating%2520Stylized%2520Avatars%2520via%2520Parametric%2520Conditioned%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DKen%2520Chen%2520and%2520Sachith%2520Seneviratne%2520and%2520Wei%2520Wang%2520and%2520Dongting%2520Hu%2520and%2520Sanjay%2520Saha%2520and%2520Md.%2520Tarek%2520Hasan%2520and%2520Sanka%2520Rasnayaka%2520and%2520Tamasha%2520Malepathirana%2520and%2520Mingming%2520Gong%2520and%2520Saman%2520Halgamuge%26entry.1292438233%3D%2520%2520Animating%2520stylized%2520avatars%2520with%2520dynamic%2520poses%2520and%2520expressions%2520has%2520attracted%250Aincreasing%2520attention%2520for%2520its%2520broad%2520range%2520of%2520applications.%2520Previous%2520research%2520has%250Amade%2520significant%2520progress%2520by%2520training%2520controllable%2520generative%2520models%2520to%250Asynthesize%2520animations%2520based%2520on%2520reference%2520characteristics%252C%2520pose%252C%2520and%2520expression%250Aconditions.%2520However%252C%2520the%2520mechanisms%2520used%2520in%2520these%2520methods%2520to%2520control%2520pose%2520and%250Aexpression%2520often%2520inadvertently%2520introduce%2520unintended%2520features%2520from%2520the%2520target%250Amotion%252C%2520while%2520also%2520causing%2520a%2520loss%2520of%2520expression-related%2520details%252C%2520particularly%250Awhen%2520applied%2520to%2520stylized%2520animation.%2520This%2520paper%2520proposes%2520a%2520new%2520method%2520based%2520on%250AStable%2520Diffusion%252C%2520called%2520AniFaceDiff%252C%2520incorporating%2520a%2520new%2520conditioning%2520module%250Afor%2520animating%2520stylized%2520avatars.%2520First%252C%2520we%2520propose%2520a%2520refined%2520spatial%250Aconditioning%2520approach%2520by%2520Facial%2520Alignment%2520to%2520prevent%2520the%2520inclusion%2520of%2520identity%250Acharacteristics%2520from%2520the%2520target%2520motion.%2520Then%252C%2520we%2520introduce%2520an%2520Expression%250AAdapter%2520that%2520incorporates%2520additional%2520cross-attention%2520layers%2520to%2520address%2520the%250Apotential%2520loss%2520of%2520expression-related%2520information.%2520Our%2520approach%2520effectively%250Apreserves%2520pose%2520and%2520expression%2520from%2520the%2520target%2520video%2520while%2520maintaining%2520input%250Aimage%2520consistency.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520results%252C%2520showcasing%2520superior%2520image%2520quality%252C%2520preservation%2520of%250Areference%2520features%252C%2520and%2520expression%2520accuracy%252C%2520particularly%2520for%2520out-of-domain%250Aanimation%2520across%2520diverse%2520styles%252C%2520highlighting%2520its%2520versatility%2520and%2520strong%250Ageneralization%2520capabilities.%2520This%2520work%2520aims%2520to%2520enhance%2520the%2520quality%2520of%2520virtual%250Astylized%2520animation%2520for%2520positive%2520applications.%2520To%2520promote%2520responsible%2520use%2520in%250Avirtual%2520environments%252C%2520we%2520contribute%2520to%2520the%2520advancement%2520of%2520detection%2520for%250Agenerative%2520content%2520by%2520evaluating%2520state-of-the-art%2520detectors%252C%2520highlighting%250Apotential%2520areas%2520for%2520improvement%252C%2520and%2520suggesting%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13272v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AniFaceDiff%3A%20Animating%20Stylized%20Avatars%20via%20Parametric%20Conditioned%0A%20%20Diffusion%20Models&entry.906535625=Ken%20Chen%20and%20Sachith%20Seneviratne%20and%20Wei%20Wang%20and%20Dongting%20Hu%20and%20Sanjay%20Saha%20and%20Md.%20Tarek%20Hasan%20and%20Sanka%20Rasnayaka%20and%20Tamasha%20Malepathirana%20and%20Mingming%20Gong%20and%20Saman%20Halgamuge&entry.1292438233=%20%20Animating%20stylized%20avatars%20with%20dynamic%20poses%20and%20expressions%20has%20attracted%0Aincreasing%20attention%20for%20its%20broad%20range%20of%20applications.%20Previous%20research%20has%0Amade%20significant%20progress%20by%20training%20controllable%20generative%20models%20to%0Asynthesize%20animations%20based%20on%20reference%20characteristics%2C%20pose%2C%20and%20expression%0Aconditions.%20However%2C%20the%20mechanisms%20used%20in%20these%20methods%20to%20control%20pose%20and%0Aexpression%20often%20inadvertently%20introduce%20unintended%20features%20from%20the%20target%0Amotion%2C%20while%20also%20causing%20a%20loss%20of%20expression-related%20details%2C%20particularly%0Awhen%20applied%20to%20stylized%20animation.%20This%20paper%20proposes%20a%20new%20method%20based%20on%0AStable%20Diffusion%2C%20called%20AniFaceDiff%2C%20incorporating%20a%20new%20conditioning%20module%0Afor%20animating%20stylized%20avatars.%20First%2C%20we%20propose%20a%20refined%20spatial%0Aconditioning%20approach%20by%20Facial%20Alignment%20to%20prevent%20the%20inclusion%20of%20identity%0Acharacteristics%20from%20the%20target%20motion.%20Then%2C%20we%20introduce%20an%20Expression%0AAdapter%20that%20incorporates%20additional%20cross-attention%20layers%20to%20address%20the%0Apotential%20loss%20of%20expression-related%20information.%20Our%20approach%20effectively%0Apreserves%20pose%20and%20expression%20from%20the%20target%20video%20while%20maintaining%20input%0Aimage%20consistency.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20results%2C%20showcasing%20superior%20image%20quality%2C%20preservation%20of%0Areference%20features%2C%20and%20expression%20accuracy%2C%20particularly%20for%20out-of-domain%0Aanimation%20across%20diverse%20styles%2C%20highlighting%20its%20versatility%20and%20strong%0Ageneralization%20capabilities.%20This%20work%20aims%20to%20enhance%20the%20quality%20of%20virtual%0Astylized%20animation%20for%20positive%20applications.%20To%20promote%20responsible%20use%20in%0Avirtual%20environments%2C%20we%20contribute%20to%20the%20advancement%20of%20detection%20for%0Agenerative%20content%20by%20evaluating%20state-of-the-art%20detectors%2C%20highlighting%0Apotential%20areas%20for%20improvement%2C%20and%20suggesting%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13272v2&entry.124074799=Read"},
{"title": "Multi-View Large Reconstruction Model via Geometry-Aware Positional\n  Encoding and Attention", "author": "Mengfei Li and Xiaoxiao Long and Yixun Liang and Weiyu Li and Yuan Liu and Peng Li and Wenhan Luo and Wenping Wang and Yike Guo", "abstract": "  Despite recent advancements in the Large Reconstruction Model (LRM)\ndemonstrating impressive results, when extending its input from single image to\nmultiple images, it exhibits inefficiencies, subpar geometric and texture\nquality, as well as slower convergence speed than expected. It is attributed to\nthat, LRM formulates 3D reconstruction as a naive images-to-3D translation\nproblem, ignoring the strong 3D coherence among the input images. In this\npaper, we propose a Multi-view Large Reconstruction Model (M-LRM) designed to\nreconstruct high-quality 3D shapes from multi-views in a 3D-aware manner.\nSpecifically, we introduce a multi-view consistent cross-attention scheme to\nenable M-LRM to accurately query information from the input images. Moreover,\nwe employ the 3D priors of the input multi-view images to initialize the\ntriplane tokens. Compared to previous methods, the proposed M-LRM can generate\n3D shapes of high fidelity. Experimental studies demonstrate that our model\nachieves a significant performance gain and faster training convergence.\nProject page: \\url{https://murphylmf.github.io/M-LRM/}.\n", "link": "http://arxiv.org/abs/2406.07648v2", "date": "2024-12-02", "relevancy": 3.101, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6477}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6064}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-View%20Large%20Reconstruction%20Model%20via%20Geometry-Aware%20Positional%0A%20%20Encoding%20and%20Attention&body=Title%3A%20Multi-View%20Large%20Reconstruction%20Model%20via%20Geometry-Aware%20Positional%0A%20%20Encoding%20and%20Attention%0AAuthor%3A%20Mengfei%20Li%20and%20Xiaoxiao%20Long%20and%20Yixun%20Liang%20and%20Weiyu%20Li%20and%20Yuan%20Liu%20and%20Peng%20Li%20and%20Wenhan%20Luo%20and%20Wenping%20Wang%20and%20Yike%20Guo%0AAbstract%3A%20%20%20Despite%20recent%20advancements%20in%20the%20Large%20Reconstruction%20Model%20%28LRM%29%0Ademonstrating%20impressive%20results%2C%20when%20extending%20its%20input%20from%20single%20image%20to%0Amultiple%20images%2C%20it%20exhibits%20inefficiencies%2C%20subpar%20geometric%20and%20texture%0Aquality%2C%20as%20well%20as%20slower%20convergence%20speed%20than%20expected.%20It%20is%20attributed%20to%0Athat%2C%20LRM%20formulates%203D%20reconstruction%20as%20a%20naive%20images-to-3D%20translation%0Aproblem%2C%20ignoring%20the%20strong%203D%20coherence%20among%20the%20input%20images.%20In%20this%0Apaper%2C%20we%20propose%20a%20Multi-view%20Large%20Reconstruction%20Model%20%28M-LRM%29%20designed%20to%0Areconstruct%20high-quality%203D%20shapes%20from%20multi-views%20in%20a%203D-aware%20manner.%0ASpecifically%2C%20we%20introduce%20a%20multi-view%20consistent%20cross-attention%20scheme%20to%0Aenable%20M-LRM%20to%20accurately%20query%20information%20from%20the%20input%20images.%20Moreover%2C%0Awe%20employ%20the%203D%20priors%20of%20the%20input%20multi-view%20images%20to%20initialize%20the%0Atriplane%20tokens.%20Compared%20to%20previous%20methods%2C%20the%20proposed%20M-LRM%20can%20generate%0A3D%20shapes%20of%20high%20fidelity.%20Experimental%20studies%20demonstrate%20that%20our%20model%0Aachieves%20a%20significant%20performance%20gain%20and%20faster%20training%20convergence.%0AProject%20page%3A%20%5Curl%7Bhttps%3A//murphylmf.github.io/M-LRM/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07648v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-View%2520Large%2520Reconstruction%2520Model%2520via%2520Geometry-Aware%2520Positional%250A%2520%2520Encoding%2520and%2520Attention%26entry.906535625%3DMengfei%2520Li%2520and%2520Xiaoxiao%2520Long%2520and%2520Yixun%2520Liang%2520and%2520Weiyu%2520Li%2520and%2520Yuan%2520Liu%2520and%2520Peng%2520Li%2520and%2520Wenhan%2520Luo%2520and%2520Wenping%2520Wang%2520and%2520Yike%2520Guo%26entry.1292438233%3D%2520%2520Despite%2520recent%2520advancements%2520in%2520the%2520Large%2520Reconstruction%2520Model%2520%2528LRM%2529%250Ademonstrating%2520impressive%2520results%252C%2520when%2520extending%2520its%2520input%2520from%2520single%2520image%2520to%250Amultiple%2520images%252C%2520it%2520exhibits%2520inefficiencies%252C%2520subpar%2520geometric%2520and%2520texture%250Aquality%252C%2520as%2520well%2520as%2520slower%2520convergence%2520speed%2520than%2520expected.%2520It%2520is%2520attributed%2520to%250Athat%252C%2520LRM%2520formulates%25203D%2520reconstruction%2520as%2520a%2520naive%2520images-to-3D%2520translation%250Aproblem%252C%2520ignoring%2520the%2520strong%25203D%2520coherence%2520among%2520the%2520input%2520images.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520Multi-view%2520Large%2520Reconstruction%2520Model%2520%2528M-LRM%2529%2520designed%2520to%250Areconstruct%2520high-quality%25203D%2520shapes%2520from%2520multi-views%2520in%2520a%25203D-aware%2520manner.%250ASpecifically%252C%2520we%2520introduce%2520a%2520multi-view%2520consistent%2520cross-attention%2520scheme%2520to%250Aenable%2520M-LRM%2520to%2520accurately%2520query%2520information%2520from%2520the%2520input%2520images.%2520Moreover%252C%250Awe%2520employ%2520the%25203D%2520priors%2520of%2520the%2520input%2520multi-view%2520images%2520to%2520initialize%2520the%250Atriplane%2520tokens.%2520Compared%2520to%2520previous%2520methods%252C%2520the%2520proposed%2520M-LRM%2520can%2520generate%250A3D%2520shapes%2520of%2520high%2520fidelity.%2520Experimental%2520studies%2520demonstrate%2520that%2520our%2520model%250Aachieves%2520a%2520significant%2520performance%2520gain%2520and%2520faster%2520training%2520convergence.%250AProject%2520page%253A%2520%255Curl%257Bhttps%253A//murphylmf.github.io/M-LRM/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07648v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-View%20Large%20Reconstruction%20Model%20via%20Geometry-Aware%20Positional%0A%20%20Encoding%20and%20Attention&entry.906535625=Mengfei%20Li%20and%20Xiaoxiao%20Long%20and%20Yixun%20Liang%20and%20Weiyu%20Li%20and%20Yuan%20Liu%20and%20Peng%20Li%20and%20Wenhan%20Luo%20and%20Wenping%20Wang%20and%20Yike%20Guo&entry.1292438233=%20%20Despite%20recent%20advancements%20in%20the%20Large%20Reconstruction%20Model%20%28LRM%29%0Ademonstrating%20impressive%20results%2C%20when%20extending%20its%20input%20from%20single%20image%20to%0Amultiple%20images%2C%20it%20exhibits%20inefficiencies%2C%20subpar%20geometric%20and%20texture%0Aquality%2C%20as%20well%20as%20slower%20convergence%20speed%20than%20expected.%20It%20is%20attributed%20to%0Athat%2C%20LRM%20formulates%203D%20reconstruction%20as%20a%20naive%20images-to-3D%20translation%0Aproblem%2C%20ignoring%20the%20strong%203D%20coherence%20among%20the%20input%20images.%20In%20this%0Apaper%2C%20we%20propose%20a%20Multi-view%20Large%20Reconstruction%20Model%20%28M-LRM%29%20designed%20to%0Areconstruct%20high-quality%203D%20shapes%20from%20multi-views%20in%20a%203D-aware%20manner.%0ASpecifically%2C%20we%20introduce%20a%20multi-view%20consistent%20cross-attention%20scheme%20to%0Aenable%20M-LRM%20to%20accurately%20query%20information%20from%20the%20input%20images.%20Moreover%2C%0Awe%20employ%20the%203D%20priors%20of%20the%20input%20multi-view%20images%20to%20initialize%20the%0Atriplane%20tokens.%20Compared%20to%20previous%20methods%2C%20the%20proposed%20M-LRM%20can%20generate%0A3D%20shapes%20of%20high%20fidelity.%20Experimental%20studies%20demonstrate%20that%20our%20model%0Aachieves%20a%20significant%20performance%20gain%20and%20faster%20training%20convergence.%0AProject%20page%3A%20%5Curl%7Bhttps%3A//murphylmf.github.io/M-LRM/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07648v2&entry.124074799=Read"},
{"title": "GuardSplat: Efficient and Robust Watermarking for 3D Gaussian Splatting", "author": "Zixuan Chen and Guangcong Wang and Jiahao Zhu and Jianhuang Lai and Xiaohua Xie", "abstract": "  3D Gaussian Splatting (3DGS) has recently created impressive assets for\nvarious applications. However, the copyright of these assets is not well\nprotected as existing watermarking methods are not suited for 3DGS considering\nsecurity, capacity, and invisibility. Besides, these methods often require\nhours or even days for optimization, limiting the application scenarios. In\nthis paper, we propose GuardSplat, an innovative and efficient framework that\neffectively protects the copyright of 3DGS assets. Specifically, 1) We first\npropose a CLIP-guided Message Decoupling Optimization module for training the\nmessage decoder, leveraging CLIP's aligning capability and rich representations\nto achieve a high extraction accuracy with minimal optimization costs,\npresenting exceptional capability and efficiency. 2) Then, we propose a\nSpherical-harmonic-aware (SH-aware) Message Embedding module tailored for 3DGS,\nwhich employs a set of SH offsets to seamlessly embed the message into the SH\nfeatures of each 3D Gaussian while maintaining the original 3D structure. It\nenables the 3DGS assets to be watermarked with minimal fidelity trade-offs and\nprevents malicious users from removing the messages from the model files,\nmeeting the demands for invisibility and security. 3) We further propose an\nAnti-distortion Message Extraction module to improve robustness against various\nvisual distortions. Extensive experiments demonstrate that GuardSplat\noutperforms the state-of-the-art methods and achieves fast optimization speed.\n", "link": "http://arxiv.org/abs/2411.19895v2", "date": "2024-12-02", "relevancy": 3.0235, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.615}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6132}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GuardSplat%3A%20Efficient%20and%20Robust%20Watermarking%20for%203D%20Gaussian%20Splatting&body=Title%3A%20GuardSplat%3A%20Efficient%20and%20Robust%20Watermarking%20for%203D%20Gaussian%20Splatting%0AAuthor%3A%20Zixuan%20Chen%20and%20Guangcong%20Wang%20and%20Jiahao%20Zhu%20and%20Jianhuang%20Lai%20and%20Xiaohua%20Xie%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20created%20impressive%20assets%20for%0Avarious%20applications.%20However%2C%20the%20copyright%20of%20these%20assets%20is%20not%20well%0Aprotected%20as%20existing%20watermarking%20methods%20are%20not%20suited%20for%203DGS%20considering%0Asecurity%2C%20capacity%2C%20and%20invisibility.%20Besides%2C%20these%20methods%20often%20require%0Ahours%20or%20even%20days%20for%20optimization%2C%20limiting%20the%20application%20scenarios.%20In%0Athis%20paper%2C%20we%20propose%20GuardSplat%2C%20an%20innovative%20and%20efficient%20framework%20that%0Aeffectively%20protects%20the%20copyright%20of%203DGS%20assets.%20Specifically%2C%201%29%20We%20first%0Apropose%20a%20CLIP-guided%20Message%20Decoupling%20Optimization%20module%20for%20training%20the%0Amessage%20decoder%2C%20leveraging%20CLIP%27s%20aligning%20capability%20and%20rich%20representations%0Ato%20achieve%20a%20high%20extraction%20accuracy%20with%20minimal%20optimization%20costs%2C%0Apresenting%20exceptional%20capability%20and%20efficiency.%202%29%20Then%2C%20we%20propose%20a%0ASpherical-harmonic-aware%20%28SH-aware%29%20Message%20Embedding%20module%20tailored%20for%203DGS%2C%0Awhich%20employs%20a%20set%20of%20SH%20offsets%20to%20seamlessly%20embed%20the%20message%20into%20the%20SH%0Afeatures%20of%20each%203D%20Gaussian%20while%20maintaining%20the%20original%203D%20structure.%20It%0Aenables%20the%203DGS%20assets%20to%20be%20watermarked%20with%20minimal%20fidelity%20trade-offs%20and%0Aprevents%20malicious%20users%20from%20removing%20the%20messages%20from%20the%20model%20files%2C%0Ameeting%20the%20demands%20for%20invisibility%20and%20security.%203%29%20We%20further%20propose%20an%0AAnti-distortion%20Message%20Extraction%20module%20to%20improve%20robustness%20against%20various%0Avisual%20distortions.%20Extensive%20experiments%20demonstrate%20that%20GuardSplat%0Aoutperforms%20the%20state-of-the-art%20methods%20and%20achieves%20fast%20optimization%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19895v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuardSplat%253A%2520Efficient%2520and%2520Robust%2520Watermarking%2520for%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DZixuan%2520Chen%2520and%2520Guangcong%2520Wang%2520and%2520Jiahao%2520Zhu%2520and%2520Jianhuang%2520Lai%2520and%2520Xiaohua%2520Xie%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520recently%2520created%2520impressive%2520assets%2520for%250Avarious%2520applications.%2520However%252C%2520the%2520copyright%2520of%2520these%2520assets%2520is%2520not%2520well%250Aprotected%2520as%2520existing%2520watermarking%2520methods%2520are%2520not%2520suited%2520for%25203DGS%2520considering%250Asecurity%252C%2520capacity%252C%2520and%2520invisibility.%2520Besides%252C%2520these%2520methods%2520often%2520require%250Ahours%2520or%2520even%2520days%2520for%2520optimization%252C%2520limiting%2520the%2520application%2520scenarios.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520GuardSplat%252C%2520an%2520innovative%2520and%2520efficient%2520framework%2520that%250Aeffectively%2520protects%2520the%2520copyright%2520of%25203DGS%2520assets.%2520Specifically%252C%25201%2529%2520We%2520first%250Apropose%2520a%2520CLIP-guided%2520Message%2520Decoupling%2520Optimization%2520module%2520for%2520training%2520the%250Amessage%2520decoder%252C%2520leveraging%2520CLIP%2527s%2520aligning%2520capability%2520and%2520rich%2520representations%250Ato%2520achieve%2520a%2520high%2520extraction%2520accuracy%2520with%2520minimal%2520optimization%2520costs%252C%250Apresenting%2520exceptional%2520capability%2520and%2520efficiency.%25202%2529%2520Then%252C%2520we%2520propose%2520a%250ASpherical-harmonic-aware%2520%2528SH-aware%2529%2520Message%2520Embedding%2520module%2520tailored%2520for%25203DGS%252C%250Awhich%2520employs%2520a%2520set%2520of%2520SH%2520offsets%2520to%2520seamlessly%2520embed%2520the%2520message%2520into%2520the%2520SH%250Afeatures%2520of%2520each%25203D%2520Gaussian%2520while%2520maintaining%2520the%2520original%25203D%2520structure.%2520It%250Aenables%2520the%25203DGS%2520assets%2520to%2520be%2520watermarked%2520with%2520minimal%2520fidelity%2520trade-offs%2520and%250Aprevents%2520malicious%2520users%2520from%2520removing%2520the%2520messages%2520from%2520the%2520model%2520files%252C%250Ameeting%2520the%2520demands%2520for%2520invisibility%2520and%2520security.%25203%2529%2520We%2520further%2520propose%2520an%250AAnti-distortion%2520Message%2520Extraction%2520module%2520to%2520improve%2520robustness%2520against%2520various%250Avisual%2520distortions.%2520Extensive%2520experiments%2520demonstrate%2520that%2520GuardSplat%250Aoutperforms%2520the%2520state-of-the-art%2520methods%2520and%2520achieves%2520fast%2520optimization%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19895v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GuardSplat%3A%20Efficient%20and%20Robust%20Watermarking%20for%203D%20Gaussian%20Splatting&entry.906535625=Zixuan%20Chen%20and%20Guangcong%20Wang%20and%20Jiahao%20Zhu%20and%20Jianhuang%20Lai%20and%20Xiaohua%20Xie&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20created%20impressive%20assets%20for%0Avarious%20applications.%20However%2C%20the%20copyright%20of%20these%20assets%20is%20not%20well%0Aprotected%20as%20existing%20watermarking%20methods%20are%20not%20suited%20for%203DGS%20considering%0Asecurity%2C%20capacity%2C%20and%20invisibility.%20Besides%2C%20these%20methods%20often%20require%0Ahours%20or%20even%20days%20for%20optimization%2C%20limiting%20the%20application%20scenarios.%20In%0Athis%20paper%2C%20we%20propose%20GuardSplat%2C%20an%20innovative%20and%20efficient%20framework%20that%0Aeffectively%20protects%20the%20copyright%20of%203DGS%20assets.%20Specifically%2C%201%29%20We%20first%0Apropose%20a%20CLIP-guided%20Message%20Decoupling%20Optimization%20module%20for%20training%20the%0Amessage%20decoder%2C%20leveraging%20CLIP%27s%20aligning%20capability%20and%20rich%20representations%0Ato%20achieve%20a%20high%20extraction%20accuracy%20with%20minimal%20optimization%20costs%2C%0Apresenting%20exceptional%20capability%20and%20efficiency.%202%29%20Then%2C%20we%20propose%20a%0ASpherical-harmonic-aware%20%28SH-aware%29%20Message%20Embedding%20module%20tailored%20for%203DGS%2C%0Awhich%20employs%20a%20set%20of%20SH%20offsets%20to%20seamlessly%20embed%20the%20message%20into%20the%20SH%0Afeatures%20of%20each%203D%20Gaussian%20while%20maintaining%20the%20original%203D%20structure.%20It%0Aenables%20the%203DGS%20assets%20to%20be%20watermarked%20with%20minimal%20fidelity%20trade-offs%20and%0Aprevents%20malicious%20users%20from%20removing%20the%20messages%20from%20the%20model%20files%2C%0Ameeting%20the%20demands%20for%20invisibility%20and%20security.%203%29%20We%20further%20propose%20an%0AAnti-distortion%20Message%20Extraction%20module%20to%20improve%20robustness%20against%20various%0Avisual%20distortions.%20Extensive%20experiments%20demonstrate%20that%20GuardSplat%0Aoutperforms%20the%20state-of-the-art%20methods%20and%20achieves%20fast%20optimization%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19895v2&entry.124074799=Read"},
{"title": "Efficient Multi-modal Large Language Models via Visual Token Grouping", "author": "Minbin Huang and Runhui Huang and Han Shi and Yimeng Chen and Chuanyang Zheng and Xiangguo Sun and Xin Jiang and Zhenguo Li and Hong Cheng", "abstract": "  The development of Multi-modal Large Language Models (MLLMs) enhances Large\nLanguage Models (LLMs) with the ability to perceive data formats beyond text,\nsignificantly advancing a range of downstream applications, such as visual\nquestion answering and image captioning. However, the substantial computational\ncosts associated with processing high-resolution images and videos pose a\nbarrier to their broader adoption. To address this challenge, compressing\nvision tokens in MLLMs has emerged as a promising approach to reduce inference\ncosts. While existing methods conduct token reduction in the feature alignment\nphase. In this paper, we introduce VisToG, a novel grouping mechanism that\nleverages the capabilities of pre-trained vision encoders to group similar\nimage segments without the need for segmentation masks. Specifically, we\nconcatenate semantic tokens to represent image semantic segments after the\nlinear projection layer before feeding into the vision encoder. Besides, with\nthe isolated attention we adopt, VisToG can identify and eliminate redundant\nvisual tokens utilizing the prior knowledge in the pre-trained vision encoder,\nwhich effectively reduces computational demands. Extensive experiments\ndemonstrate the effectiveness of VisToG, maintaining 98.1% of the original\nperformance while achieving a reduction of over 27\\% inference time.\n", "link": "http://arxiv.org/abs/2411.17773v2", "date": "2024-12-02", "relevancy": 2.8681, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5753}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5738}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Multi-modal%20Large%20Language%20Models%20via%20Visual%20Token%20Grouping&body=Title%3A%20Efficient%20Multi-modal%20Large%20Language%20Models%20via%20Visual%20Token%20Grouping%0AAuthor%3A%20Minbin%20Huang%20and%20Runhui%20Huang%20and%20Han%20Shi%20and%20Yimeng%20Chen%20and%20Chuanyang%20Zheng%20and%20Xiangguo%20Sun%20and%20Xin%20Jiang%20and%20Zhenguo%20Li%20and%20Hong%20Cheng%0AAbstract%3A%20%20%20The%20development%20of%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20enhances%20Large%0ALanguage%20Models%20%28LLMs%29%20with%20the%20ability%20to%20perceive%20data%20formats%20beyond%20text%2C%0Asignificantly%20advancing%20a%20range%20of%20downstream%20applications%2C%20such%20as%20visual%0Aquestion%20answering%20and%20image%20captioning.%20However%2C%20the%20substantial%20computational%0Acosts%20associated%20with%20processing%20high-resolution%20images%20and%20videos%20pose%20a%0Abarrier%20to%20their%20broader%20adoption.%20To%20address%20this%20challenge%2C%20compressing%0Avision%20tokens%20in%20MLLMs%20has%20emerged%20as%20a%20promising%20approach%20to%20reduce%20inference%0Acosts.%20While%20existing%20methods%20conduct%20token%20reduction%20in%20the%20feature%20alignment%0Aphase.%20In%20this%20paper%2C%20we%20introduce%20VisToG%2C%20a%20novel%20grouping%20mechanism%20that%0Aleverages%20the%20capabilities%20of%20pre-trained%20vision%20encoders%20to%20group%20similar%0Aimage%20segments%20without%20the%20need%20for%20segmentation%20masks.%20Specifically%2C%20we%0Aconcatenate%20semantic%20tokens%20to%20represent%20image%20semantic%20segments%20after%20the%0Alinear%20projection%20layer%20before%20feeding%20into%20the%20vision%20encoder.%20Besides%2C%20with%0Athe%20isolated%20attention%20we%20adopt%2C%20VisToG%20can%20identify%20and%20eliminate%20redundant%0Avisual%20tokens%20utilizing%20the%20prior%20knowledge%20in%20the%20pre-trained%20vision%20encoder%2C%0Awhich%20effectively%20reduces%20computational%20demands.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20VisToG%2C%20maintaining%2098.1%25%20of%20the%20original%0Aperformance%20while%20achieving%20a%20reduction%20of%20over%2027%5C%25%20inference%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17773v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Multi-modal%2520Large%2520Language%2520Models%2520via%2520Visual%2520Token%2520Grouping%26entry.906535625%3DMinbin%2520Huang%2520and%2520Runhui%2520Huang%2520and%2520Han%2520Shi%2520and%2520Yimeng%2520Chen%2520and%2520Chuanyang%2520Zheng%2520and%2520Xiangguo%2520Sun%2520and%2520Xin%2520Jiang%2520and%2520Zhenguo%2520Li%2520and%2520Hong%2520Cheng%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520enhances%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520with%2520the%2520ability%2520to%2520perceive%2520data%2520formats%2520beyond%2520text%252C%250Asignificantly%2520advancing%2520a%2520range%2520of%2520downstream%2520applications%252C%2520such%2520as%2520visual%250Aquestion%2520answering%2520and%2520image%2520captioning.%2520However%252C%2520the%2520substantial%2520computational%250Acosts%2520associated%2520with%2520processing%2520high-resolution%2520images%2520and%2520videos%2520pose%2520a%250Abarrier%2520to%2520their%2520broader%2520adoption.%2520To%2520address%2520this%2520challenge%252C%2520compressing%250Avision%2520tokens%2520in%2520MLLMs%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520to%2520reduce%2520inference%250Acosts.%2520While%2520existing%2520methods%2520conduct%2520token%2520reduction%2520in%2520the%2520feature%2520alignment%250Aphase.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520VisToG%252C%2520a%2520novel%2520grouping%2520mechanism%2520that%250Aleverages%2520the%2520capabilities%2520of%2520pre-trained%2520vision%2520encoders%2520to%2520group%2520similar%250Aimage%2520segments%2520without%2520the%2520need%2520for%2520segmentation%2520masks.%2520Specifically%252C%2520we%250Aconcatenate%2520semantic%2520tokens%2520to%2520represent%2520image%2520semantic%2520segments%2520after%2520the%250Alinear%2520projection%2520layer%2520before%2520feeding%2520into%2520the%2520vision%2520encoder.%2520Besides%252C%2520with%250Athe%2520isolated%2520attention%2520we%2520adopt%252C%2520VisToG%2520can%2520identify%2520and%2520eliminate%2520redundant%250Avisual%2520tokens%2520utilizing%2520the%2520prior%2520knowledge%2520in%2520the%2520pre-trained%2520vision%2520encoder%252C%250Awhich%2520effectively%2520reduces%2520computational%2520demands.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520effectiveness%2520of%2520VisToG%252C%2520maintaining%252098.1%2525%2520of%2520the%2520original%250Aperformance%2520while%2520achieving%2520a%2520reduction%2520of%2520over%252027%255C%2525%2520inference%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17773v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Multi-modal%20Large%20Language%20Models%20via%20Visual%20Token%20Grouping&entry.906535625=Minbin%20Huang%20and%20Runhui%20Huang%20and%20Han%20Shi%20and%20Yimeng%20Chen%20and%20Chuanyang%20Zheng%20and%20Xiangguo%20Sun%20and%20Xin%20Jiang%20and%20Zhenguo%20Li%20and%20Hong%20Cheng&entry.1292438233=%20%20The%20development%20of%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20enhances%20Large%0ALanguage%20Models%20%28LLMs%29%20with%20the%20ability%20to%20perceive%20data%20formats%20beyond%20text%2C%0Asignificantly%20advancing%20a%20range%20of%20downstream%20applications%2C%20such%20as%20visual%0Aquestion%20answering%20and%20image%20captioning.%20However%2C%20the%20substantial%20computational%0Acosts%20associated%20with%20processing%20high-resolution%20images%20and%20videos%20pose%20a%0Abarrier%20to%20their%20broader%20adoption.%20To%20address%20this%20challenge%2C%20compressing%0Avision%20tokens%20in%20MLLMs%20has%20emerged%20as%20a%20promising%20approach%20to%20reduce%20inference%0Acosts.%20While%20existing%20methods%20conduct%20token%20reduction%20in%20the%20feature%20alignment%0Aphase.%20In%20this%20paper%2C%20we%20introduce%20VisToG%2C%20a%20novel%20grouping%20mechanism%20that%0Aleverages%20the%20capabilities%20of%20pre-trained%20vision%20encoders%20to%20group%20similar%0Aimage%20segments%20without%20the%20need%20for%20segmentation%20masks.%20Specifically%2C%20we%0Aconcatenate%20semantic%20tokens%20to%20represent%20image%20semantic%20segments%20after%20the%0Alinear%20projection%20layer%20before%20feeding%20into%20the%20vision%20encoder.%20Besides%2C%20with%0Athe%20isolated%20attention%20we%20adopt%2C%20VisToG%20can%20identify%20and%20eliminate%20redundant%0Avisual%20tokens%20utilizing%20the%20prior%20knowledge%20in%20the%20pre-trained%20vision%20encoder%2C%0Awhich%20effectively%20reduces%20computational%20demands.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20VisToG%2C%20maintaining%2098.1%25%20of%20the%20original%0Aperformance%20while%20achieving%20a%20reduction%20of%20over%2027%5C%25%20inference%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17773v2&entry.124074799=Read"},
{"title": "An Architectural Approach to Enhance Deep Long-Tailed Learning", "author": "Yuhan Pan and Yanan Sun and Wei Gong", "abstract": "  Deep long-tailed recognition has been widely studied to address the issue of\nimbalanced data distributions in real-world scenarios. However, there has been\ninsufficient focus on the design of neural architectures, despite empirical\nevidence suggesting that architecture can significantly impact performance. In\nthis paper, we attempt to mitigate long-tailed issues through architectural\nimprovements. To simplify the design process, we utilize Differential\nArchitecture Search (DARTS) to achieve this goal. Unfortunately, existing DARTS\nmethods struggle to perform well in long-tailed scenarios. To tackle this\nchallenge, we introduce Long-Tailed Differential Architecture Search (LTDAS).\nSpecifically, we conduct extensive experiments to explore architectural\ncomponents that demonstrate better performance on long-tailed data and propose\na new search space based on our observations. This ensures that the\narchitecture obtained through our search process incorporates superior\ncomponents. Additionally, we propose replacing the learnable linear classifier\nwith an Equiangular Tight Frame (ETF) classifier to further enhance our method.\nThis classifier effectively alleviates the biased search process and prevents\nperformance collapse. Extensive experimental evaluations demonstrate that our\napproach consistently improves upon existing methods from an orthogonal\nperspective and achieves state-of-the-art results with simple enhancements.\n", "link": "http://arxiv.org/abs/2411.06098v3", "date": "2024-12-02", "relevancy": 2.8658, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6093}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5706}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Architectural%20Approach%20to%20Enhance%20Deep%20Long-Tailed%20Learning&body=Title%3A%20An%20Architectural%20Approach%20to%20Enhance%20Deep%20Long-Tailed%20Learning%0AAuthor%3A%20Yuhan%20Pan%20and%20Yanan%20Sun%20and%20Wei%20Gong%0AAbstract%3A%20%20%20Deep%20long-tailed%20recognition%20has%20been%20widely%20studied%20to%20address%20the%20issue%20of%0Aimbalanced%20data%20distributions%20in%20real-world%20scenarios.%20However%2C%20there%20has%20been%0Ainsufficient%20focus%20on%20the%20design%20of%20neural%20architectures%2C%20despite%20empirical%0Aevidence%20suggesting%20that%20architecture%20can%20significantly%20impact%20performance.%20In%0Athis%20paper%2C%20we%20attempt%20to%20mitigate%20long-tailed%20issues%20through%20architectural%0Aimprovements.%20To%20simplify%20the%20design%20process%2C%20we%20utilize%20Differential%0AArchitecture%20Search%20%28DARTS%29%20to%20achieve%20this%20goal.%20Unfortunately%2C%20existing%20DARTS%0Amethods%20struggle%20to%20perform%20well%20in%20long-tailed%20scenarios.%20To%20tackle%20this%0Achallenge%2C%20we%20introduce%20Long-Tailed%20Differential%20Architecture%20Search%20%28LTDAS%29.%0ASpecifically%2C%20we%20conduct%20extensive%20experiments%20to%20explore%20architectural%0Acomponents%20that%20demonstrate%20better%20performance%20on%20long-tailed%20data%20and%20propose%0Aa%20new%20search%20space%20based%20on%20our%20observations.%20This%20ensures%20that%20the%0Aarchitecture%20obtained%20through%20our%20search%20process%20incorporates%20superior%0Acomponents.%20Additionally%2C%20we%20propose%20replacing%20the%20learnable%20linear%20classifier%0Awith%20an%20Equiangular%20Tight%20Frame%20%28ETF%29%20classifier%20to%20further%20enhance%20our%20method.%0AThis%20classifier%20effectively%20alleviates%20the%20biased%20search%20process%20and%20prevents%0Aperformance%20collapse.%20Extensive%20experimental%20evaluations%20demonstrate%20that%20our%0Aapproach%20consistently%20improves%20upon%20existing%20methods%20from%20an%20orthogonal%0Aperspective%20and%20achieves%20state-of-the-art%20results%20with%20simple%20enhancements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06098v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Architectural%2520Approach%2520to%2520Enhance%2520Deep%2520Long-Tailed%2520Learning%26entry.906535625%3DYuhan%2520Pan%2520and%2520Yanan%2520Sun%2520and%2520Wei%2520Gong%26entry.1292438233%3D%2520%2520Deep%2520long-tailed%2520recognition%2520has%2520been%2520widely%2520studied%2520to%2520address%2520the%2520issue%2520of%250Aimbalanced%2520data%2520distributions%2520in%2520real-world%2520scenarios.%2520However%252C%2520there%2520has%2520been%250Ainsufficient%2520focus%2520on%2520the%2520design%2520of%2520neural%2520architectures%252C%2520despite%2520empirical%250Aevidence%2520suggesting%2520that%2520architecture%2520can%2520significantly%2520impact%2520performance.%2520In%250Athis%2520paper%252C%2520we%2520attempt%2520to%2520mitigate%2520long-tailed%2520issues%2520through%2520architectural%250Aimprovements.%2520To%2520simplify%2520the%2520design%2520process%252C%2520we%2520utilize%2520Differential%250AArchitecture%2520Search%2520%2528DARTS%2529%2520to%2520achieve%2520this%2520goal.%2520Unfortunately%252C%2520existing%2520DARTS%250Amethods%2520struggle%2520to%2520perform%2520well%2520in%2520long-tailed%2520scenarios.%2520To%2520tackle%2520this%250Achallenge%252C%2520we%2520introduce%2520Long-Tailed%2520Differential%2520Architecture%2520Search%2520%2528LTDAS%2529.%250ASpecifically%252C%2520we%2520conduct%2520extensive%2520experiments%2520to%2520explore%2520architectural%250Acomponents%2520that%2520demonstrate%2520better%2520performance%2520on%2520long-tailed%2520data%2520and%2520propose%250Aa%2520new%2520search%2520space%2520based%2520on%2520our%2520observations.%2520This%2520ensures%2520that%2520the%250Aarchitecture%2520obtained%2520through%2520our%2520search%2520process%2520incorporates%2520superior%250Acomponents.%2520Additionally%252C%2520we%2520propose%2520replacing%2520the%2520learnable%2520linear%2520classifier%250Awith%2520an%2520Equiangular%2520Tight%2520Frame%2520%2528ETF%2529%2520classifier%2520to%2520further%2520enhance%2520our%2520method.%250AThis%2520classifier%2520effectively%2520alleviates%2520the%2520biased%2520search%2520process%2520and%2520prevents%250Aperformance%2520collapse.%2520Extensive%2520experimental%2520evaluations%2520demonstrate%2520that%2520our%250Aapproach%2520consistently%2520improves%2520upon%2520existing%2520methods%2520from%2520an%2520orthogonal%250Aperspective%2520and%2520achieves%2520state-of-the-art%2520results%2520with%2520simple%2520enhancements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06098v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Architectural%20Approach%20to%20Enhance%20Deep%20Long-Tailed%20Learning&entry.906535625=Yuhan%20Pan%20and%20Yanan%20Sun%20and%20Wei%20Gong&entry.1292438233=%20%20Deep%20long-tailed%20recognition%20has%20been%20widely%20studied%20to%20address%20the%20issue%20of%0Aimbalanced%20data%20distributions%20in%20real-world%20scenarios.%20However%2C%20there%20has%20been%0Ainsufficient%20focus%20on%20the%20design%20of%20neural%20architectures%2C%20despite%20empirical%0Aevidence%20suggesting%20that%20architecture%20can%20significantly%20impact%20performance.%20In%0Athis%20paper%2C%20we%20attempt%20to%20mitigate%20long-tailed%20issues%20through%20architectural%0Aimprovements.%20To%20simplify%20the%20design%20process%2C%20we%20utilize%20Differential%0AArchitecture%20Search%20%28DARTS%29%20to%20achieve%20this%20goal.%20Unfortunately%2C%20existing%20DARTS%0Amethods%20struggle%20to%20perform%20well%20in%20long-tailed%20scenarios.%20To%20tackle%20this%0Achallenge%2C%20we%20introduce%20Long-Tailed%20Differential%20Architecture%20Search%20%28LTDAS%29.%0ASpecifically%2C%20we%20conduct%20extensive%20experiments%20to%20explore%20architectural%0Acomponents%20that%20demonstrate%20better%20performance%20on%20long-tailed%20data%20and%20propose%0Aa%20new%20search%20space%20based%20on%20our%20observations.%20This%20ensures%20that%20the%0Aarchitecture%20obtained%20through%20our%20search%20process%20incorporates%20superior%0Acomponents.%20Additionally%2C%20we%20propose%20replacing%20the%20learnable%20linear%20classifier%0Awith%20an%20Equiangular%20Tight%20Frame%20%28ETF%29%20classifier%20to%20further%20enhance%20our%20method.%0AThis%20classifier%20effectively%20alleviates%20the%20biased%20search%20process%20and%20prevents%0Aperformance%20collapse.%20Extensive%20experimental%20evaluations%20demonstrate%20that%20our%0Aapproach%20consistently%20improves%20upon%20existing%20methods%20from%20an%20orthogonal%0Aperspective%20and%20achieves%20state-of-the-art%20results%20with%20simple%20enhancements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06098v3&entry.124074799=Read"},
{"title": "MathGLM-Vision: Solving Mathematical Problems with Multi-Modal Large\n  Language Model", "author": "Zhen Yang and Jinhao Chen and Zhengxiao Du and Wenmeng Yu and Weihan Wang and Wenyi Hong and Zhihuan Jiang and Bin Xu and Jie Tang", "abstract": "  Large language models (LLMs) have demonstrated significant capabilities in\nmathematical reasoning, particularly with text-based mathematical problems.\nHowever, current multi-modal large language models (MLLMs), especially those\nspecialized in mathematics, tend to focus predominantly on solving geometric\nproblems but ignore the diversity of visual information available in other\nareas of mathematics. Moreover, the geometric information for these specialized\nmathematical MLLMs is derived from several public datasets, which are typically\nlimited in diversity and complexity. To address these limitations, we aim to\nconstruct a fine-tuning dataset named MathVL, and develop a series of\nspecialized mathematical MLLMs termed MathGLM-Vision by conducting Supervised\nFine-Tuning (SFT) on MathVL with various parameter-scale backbones. To\nextensively evaluate the effectiveness of MathGLM-Vision, we conduct\nexperiments on several public benchmarks and our curated MathVL-test consisting\nof 2,000 problems. Experimental results demonstrate that MathGLM-Vision\nachieves significant improvements compared with some existing models, including\nbackbone models and open-source mathematical MLLMs. These findings indicate the\nimportance of diversity dataset in enhancing the mathematical reasoning\nabilities of MLLMs.\n", "link": "http://arxiv.org/abs/2409.13729v2", "date": "2024-12-02", "relevancy": 2.8444, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5711}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5711}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MathGLM-Vision%3A%20Solving%20Mathematical%20Problems%20with%20Multi-Modal%20Large%0A%20%20Language%20Model&body=Title%3A%20MathGLM-Vision%3A%20Solving%20Mathematical%20Problems%20with%20Multi-Modal%20Large%0A%20%20Language%20Model%0AAuthor%3A%20Zhen%20Yang%20and%20Jinhao%20Chen%20and%20Zhengxiao%20Du%20and%20Wenmeng%20Yu%20and%20Weihan%20Wang%20and%20Wenyi%20Hong%20and%20Zhihuan%20Jiang%20and%20Bin%20Xu%20and%20Jie%20Tang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20significant%20capabilities%20in%0Amathematical%20reasoning%2C%20particularly%20with%20text-based%20mathematical%20problems.%0AHowever%2C%20current%20multi-modal%20large%20language%20models%20%28MLLMs%29%2C%20especially%20those%0Aspecialized%20in%20mathematics%2C%20tend%20to%20focus%20predominantly%20on%20solving%20geometric%0Aproblems%20but%20ignore%20the%20diversity%20of%20visual%20information%20available%20in%20other%0Aareas%20of%20mathematics.%20Moreover%2C%20the%20geometric%20information%20for%20these%20specialized%0Amathematical%20MLLMs%20is%20derived%20from%20several%20public%20datasets%2C%20which%20are%20typically%0Alimited%20in%20diversity%20and%20complexity.%20To%20address%20these%20limitations%2C%20we%20aim%20to%0Aconstruct%20a%20fine-tuning%20dataset%20named%20MathVL%2C%20and%20develop%20a%20series%20of%0Aspecialized%20mathematical%20MLLMs%20termed%20MathGLM-Vision%20by%20conducting%20Supervised%0AFine-Tuning%20%28SFT%29%20on%20MathVL%20with%20various%20parameter-scale%20backbones.%20To%0Aextensively%20evaluate%20the%20effectiveness%20of%20MathGLM-Vision%2C%20we%20conduct%0Aexperiments%20on%20several%20public%20benchmarks%20and%20our%20curated%20MathVL-test%20consisting%0Aof%202%2C000%20problems.%20Experimental%20results%20demonstrate%20that%20MathGLM-Vision%0Aachieves%20significant%20improvements%20compared%20with%20some%20existing%20models%2C%20including%0Abackbone%20models%20and%20open-source%20mathematical%20MLLMs.%20These%20findings%20indicate%20the%0Aimportance%20of%20diversity%20dataset%20in%20enhancing%20the%20mathematical%20reasoning%0Aabilities%20of%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13729v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMathGLM-Vision%253A%2520Solving%2520Mathematical%2520Problems%2520with%2520Multi-Modal%2520Large%250A%2520%2520Language%2520Model%26entry.906535625%3DZhen%2520Yang%2520and%2520Jinhao%2520Chen%2520and%2520Zhengxiao%2520Du%2520and%2520Wenmeng%2520Yu%2520and%2520Weihan%2520Wang%2520and%2520Wenyi%2520Hong%2520and%2520Zhihuan%2520Jiang%2520and%2520Bin%2520Xu%2520and%2520Jie%2520Tang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520significant%2520capabilities%2520in%250Amathematical%2520reasoning%252C%2520particularly%2520with%2520text-based%2520mathematical%2520problems.%250AHowever%252C%2520current%2520multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520especially%2520those%250Aspecialized%2520in%2520mathematics%252C%2520tend%2520to%2520focus%2520predominantly%2520on%2520solving%2520geometric%250Aproblems%2520but%2520ignore%2520the%2520diversity%2520of%2520visual%2520information%2520available%2520in%2520other%250Aareas%2520of%2520mathematics.%2520Moreover%252C%2520the%2520geometric%2520information%2520for%2520these%2520specialized%250Amathematical%2520MLLMs%2520is%2520derived%2520from%2520several%2520public%2520datasets%252C%2520which%2520are%2520typically%250Alimited%2520in%2520diversity%2520and%2520complexity.%2520To%2520address%2520these%2520limitations%252C%2520we%2520aim%2520to%250Aconstruct%2520a%2520fine-tuning%2520dataset%2520named%2520MathVL%252C%2520and%2520develop%2520a%2520series%2520of%250Aspecialized%2520mathematical%2520MLLMs%2520termed%2520MathGLM-Vision%2520by%2520conducting%2520Supervised%250AFine-Tuning%2520%2528SFT%2529%2520on%2520MathVL%2520with%2520various%2520parameter-scale%2520backbones.%2520To%250Aextensively%2520evaluate%2520the%2520effectiveness%2520of%2520MathGLM-Vision%252C%2520we%2520conduct%250Aexperiments%2520on%2520several%2520public%2520benchmarks%2520and%2520our%2520curated%2520MathVL-test%2520consisting%250Aof%25202%252C000%2520problems.%2520Experimental%2520results%2520demonstrate%2520that%2520MathGLM-Vision%250Aachieves%2520significant%2520improvements%2520compared%2520with%2520some%2520existing%2520models%252C%2520including%250Abackbone%2520models%2520and%2520open-source%2520mathematical%2520MLLMs.%2520These%2520findings%2520indicate%2520the%250Aimportance%2520of%2520diversity%2520dataset%2520in%2520enhancing%2520the%2520mathematical%2520reasoning%250Aabilities%2520of%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13729v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MathGLM-Vision%3A%20Solving%20Mathematical%20Problems%20with%20Multi-Modal%20Large%0A%20%20Language%20Model&entry.906535625=Zhen%20Yang%20and%20Jinhao%20Chen%20and%20Zhengxiao%20Du%20and%20Wenmeng%20Yu%20and%20Weihan%20Wang%20and%20Wenyi%20Hong%20and%20Zhihuan%20Jiang%20and%20Bin%20Xu%20and%20Jie%20Tang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20significant%20capabilities%20in%0Amathematical%20reasoning%2C%20particularly%20with%20text-based%20mathematical%20problems.%0AHowever%2C%20current%20multi-modal%20large%20language%20models%20%28MLLMs%29%2C%20especially%20those%0Aspecialized%20in%20mathematics%2C%20tend%20to%20focus%20predominantly%20on%20solving%20geometric%0Aproblems%20but%20ignore%20the%20diversity%20of%20visual%20information%20available%20in%20other%0Aareas%20of%20mathematics.%20Moreover%2C%20the%20geometric%20information%20for%20these%20specialized%0Amathematical%20MLLMs%20is%20derived%20from%20several%20public%20datasets%2C%20which%20are%20typically%0Alimited%20in%20diversity%20and%20complexity.%20To%20address%20these%20limitations%2C%20we%20aim%20to%0Aconstruct%20a%20fine-tuning%20dataset%20named%20MathVL%2C%20and%20develop%20a%20series%20of%0Aspecialized%20mathematical%20MLLMs%20termed%20MathGLM-Vision%20by%20conducting%20Supervised%0AFine-Tuning%20%28SFT%29%20on%20MathVL%20with%20various%20parameter-scale%20backbones.%20To%0Aextensively%20evaluate%20the%20effectiveness%20of%20MathGLM-Vision%2C%20we%20conduct%0Aexperiments%20on%20several%20public%20benchmarks%20and%20our%20curated%20MathVL-test%20consisting%0Aof%202%2C000%20problems.%20Experimental%20results%20demonstrate%20that%20MathGLM-Vision%0Aachieves%20significant%20improvements%20compared%20with%20some%20existing%20models%2C%20including%0Abackbone%20models%20and%20open-source%20mathematical%20MLLMs.%20These%20findings%20indicate%20the%0Aimportance%20of%20diversity%20dataset%20in%20enhancing%20the%20mathematical%20reasoning%0Aabilities%20of%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13729v2&entry.124074799=Read"},
{"title": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision\n  Language Models", "author": "M. Jehanzeb Mirza and Mengjie Zhao and Zhuoyuan Mao and Sivan Doveh and Wei Lin and Paul Gavrikov and Michael Dorkenwald and Shiqi Yang and Saurav Jha and Hiromi Wakaki and Yuki Mitsufuji and Horst Possegger and Rogerio Feris and Leonid Karlinsky and James Glass", "abstract": "  In this work, we propose a novel method (GLOV) enabling Large Language Models\n(LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to\nenhance downstream vision tasks. Our GLOV meta-prompts an LLM with the\ndownstream task description, querying it for suitable VLM prompts (e.g., for\nzero-shot classification with CLIP). These prompts are ranked according to a\npurity measure obtained through a fitness function. In each respective\noptimization step, the ranked prompts are fed as in-context examples (with\ntheir accuracies) to equip the LLM with the knowledge of the type of text\nprompts preferred by the downstream VLM. Furthermore, we also explicitly steer\nthe LLM generation process in each optimization step by specifically adding an\noffset difference vector of the embeddings from the positive and negative\nsolutions found by the LLM, in previous optimization steps, to the intermediate\nlayer of the network for the next generation step. This offset vector steers\nthe LLM generation toward the type of language preferred by the downstream VLM,\nresulting in enhanced performance on the downstream vision tasks. We\ncomprehensively evaluate our GLOV on 16 diverse datasets using two families of\nVLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models\n-- showing that the discovered solutions can enhance the recognition\nperformance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for these\nmodels.\n", "link": "http://arxiv.org/abs/2410.06154v2", "date": "2024-12-02", "relevancy": 2.8306, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5724}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5724}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLOV%3A%20Guided%20Large%20Language%20Models%20as%20Implicit%20Optimizers%20for%20Vision%0A%20%20Language%20Models&body=Title%3A%20GLOV%3A%20Guided%20Large%20Language%20Models%20as%20Implicit%20Optimizers%20for%20Vision%0A%20%20Language%20Models%0AAuthor%3A%20M.%20Jehanzeb%20Mirza%20and%20Mengjie%20Zhao%20and%20Zhuoyuan%20Mao%20and%20Sivan%20Doveh%20and%20Wei%20Lin%20and%20Paul%20Gavrikov%20and%20Michael%20Dorkenwald%20and%20Shiqi%20Yang%20and%20Saurav%20Jha%20and%20Hiromi%20Wakaki%20and%20Yuki%20Mitsufuji%20and%20Horst%20Possegger%20and%20Rogerio%20Feris%20and%20Leonid%20Karlinsky%20and%20James%20Glass%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20method%20%28GLOV%29%20enabling%20Large%20Language%20Models%0A%28LLMs%29%20to%20act%20as%20implicit%20Optimizers%20for%20Vision-Langugage%20Models%20%28VLMs%29%20to%0Aenhance%20downstream%20vision%20tasks.%20Our%20GLOV%20meta-prompts%20an%20LLM%20with%20the%0Adownstream%20task%20description%2C%20querying%20it%20for%20suitable%20VLM%20prompts%20%28e.g.%2C%20for%0Azero-shot%20classification%20with%20CLIP%29.%20These%20prompts%20are%20ranked%20according%20to%20a%0Apurity%20measure%20obtained%20through%20a%20fitness%20function.%20In%20each%20respective%0Aoptimization%20step%2C%20the%20ranked%20prompts%20are%20fed%20as%20in-context%20examples%20%28with%0Atheir%20accuracies%29%20to%20equip%20the%20LLM%20with%20the%20knowledge%20of%20the%20type%20of%20text%0Aprompts%20preferred%20by%20the%20downstream%20VLM.%20Furthermore%2C%20we%20also%20explicitly%20steer%0Athe%20LLM%20generation%20process%20in%20each%20optimization%20step%20by%20specifically%20adding%20an%0Aoffset%20difference%20vector%20of%20the%20embeddings%20from%20the%20positive%20and%20negative%0Asolutions%20found%20by%20the%20LLM%2C%20in%20previous%20optimization%20steps%2C%20to%20the%20intermediate%0Alayer%20of%20the%20network%20for%20the%20next%20generation%20step.%20This%20offset%20vector%20steers%0Athe%20LLM%20generation%20toward%20the%20type%20of%20language%20preferred%20by%20the%20downstream%20VLM%2C%0Aresulting%20in%20enhanced%20performance%20on%20the%20downstream%20vision%20tasks.%20We%0Acomprehensively%20evaluate%20our%20GLOV%20on%2016%20diverse%20datasets%20using%20two%20families%20of%0AVLMs%2C%20i.e.%2C%20dual-encoder%20%28e.g.%2C%20CLIP%29%20and%20encoder-decoder%20%28e.g.%2C%20LLaVa%29%20models%0A--%20showing%20that%20the%20discovered%20solutions%20can%20enhance%20the%20recognition%0Aperformance%20by%20up%20to%2015.0%25%20and%2057.5%25%20%283.8%25%20and%2021.6%25%20on%20average%29%20for%20these%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06154v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLOV%253A%2520Guided%2520Large%2520Language%2520Models%2520as%2520Implicit%2520Optimizers%2520for%2520Vision%250A%2520%2520Language%2520Models%26entry.906535625%3DM.%2520Jehanzeb%2520Mirza%2520and%2520Mengjie%2520Zhao%2520and%2520Zhuoyuan%2520Mao%2520and%2520Sivan%2520Doveh%2520and%2520Wei%2520Lin%2520and%2520Paul%2520Gavrikov%2520and%2520Michael%2520Dorkenwald%2520and%2520Shiqi%2520Yang%2520and%2520Saurav%2520Jha%2520and%2520Hiromi%2520Wakaki%2520and%2520Yuki%2520Mitsufuji%2520and%2520Horst%2520Possegger%2520and%2520Rogerio%2520Feris%2520and%2520Leonid%2520Karlinsky%2520and%2520James%2520Glass%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520method%2520%2528GLOV%2529%2520enabling%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520to%2520act%2520as%2520implicit%2520Optimizers%2520for%2520Vision-Langugage%2520Models%2520%2528VLMs%2529%2520to%250Aenhance%2520downstream%2520vision%2520tasks.%2520Our%2520GLOV%2520meta-prompts%2520an%2520LLM%2520with%2520the%250Adownstream%2520task%2520description%252C%2520querying%2520it%2520for%2520suitable%2520VLM%2520prompts%2520%2528e.g.%252C%2520for%250Azero-shot%2520classification%2520with%2520CLIP%2529.%2520These%2520prompts%2520are%2520ranked%2520according%2520to%2520a%250Apurity%2520measure%2520obtained%2520through%2520a%2520fitness%2520function.%2520In%2520each%2520respective%250Aoptimization%2520step%252C%2520the%2520ranked%2520prompts%2520are%2520fed%2520as%2520in-context%2520examples%2520%2528with%250Atheir%2520accuracies%2529%2520to%2520equip%2520the%2520LLM%2520with%2520the%2520knowledge%2520of%2520the%2520type%2520of%2520text%250Aprompts%2520preferred%2520by%2520the%2520downstream%2520VLM.%2520Furthermore%252C%2520we%2520also%2520explicitly%2520steer%250Athe%2520LLM%2520generation%2520process%2520in%2520each%2520optimization%2520step%2520by%2520specifically%2520adding%2520an%250Aoffset%2520difference%2520vector%2520of%2520the%2520embeddings%2520from%2520the%2520positive%2520and%2520negative%250Asolutions%2520found%2520by%2520the%2520LLM%252C%2520in%2520previous%2520optimization%2520steps%252C%2520to%2520the%2520intermediate%250Alayer%2520of%2520the%2520network%2520for%2520the%2520next%2520generation%2520step.%2520This%2520offset%2520vector%2520steers%250Athe%2520LLM%2520generation%2520toward%2520the%2520type%2520of%2520language%2520preferred%2520by%2520the%2520downstream%2520VLM%252C%250Aresulting%2520in%2520enhanced%2520performance%2520on%2520the%2520downstream%2520vision%2520tasks.%2520We%250Acomprehensively%2520evaluate%2520our%2520GLOV%2520on%252016%2520diverse%2520datasets%2520using%2520two%2520families%2520of%250AVLMs%252C%2520i.e.%252C%2520dual-encoder%2520%2528e.g.%252C%2520CLIP%2529%2520and%2520encoder-decoder%2520%2528e.g.%252C%2520LLaVa%2529%2520models%250A--%2520showing%2520that%2520the%2520discovered%2520solutions%2520can%2520enhance%2520the%2520recognition%250Aperformance%2520by%2520up%2520to%252015.0%2525%2520and%252057.5%2525%2520%25283.8%2525%2520and%252021.6%2525%2520on%2520average%2529%2520for%2520these%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06154v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLOV%3A%20Guided%20Large%20Language%20Models%20as%20Implicit%20Optimizers%20for%20Vision%0A%20%20Language%20Models&entry.906535625=M.%20Jehanzeb%20Mirza%20and%20Mengjie%20Zhao%20and%20Zhuoyuan%20Mao%20and%20Sivan%20Doveh%20and%20Wei%20Lin%20and%20Paul%20Gavrikov%20and%20Michael%20Dorkenwald%20and%20Shiqi%20Yang%20and%20Saurav%20Jha%20and%20Hiromi%20Wakaki%20and%20Yuki%20Mitsufuji%20and%20Horst%20Possegger%20and%20Rogerio%20Feris%20and%20Leonid%20Karlinsky%20and%20James%20Glass&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20method%20%28GLOV%29%20enabling%20Large%20Language%20Models%0A%28LLMs%29%20to%20act%20as%20implicit%20Optimizers%20for%20Vision-Langugage%20Models%20%28VLMs%29%20to%0Aenhance%20downstream%20vision%20tasks.%20Our%20GLOV%20meta-prompts%20an%20LLM%20with%20the%0Adownstream%20task%20description%2C%20querying%20it%20for%20suitable%20VLM%20prompts%20%28e.g.%2C%20for%0Azero-shot%20classification%20with%20CLIP%29.%20These%20prompts%20are%20ranked%20according%20to%20a%0Apurity%20measure%20obtained%20through%20a%20fitness%20function.%20In%20each%20respective%0Aoptimization%20step%2C%20the%20ranked%20prompts%20are%20fed%20as%20in-context%20examples%20%28with%0Atheir%20accuracies%29%20to%20equip%20the%20LLM%20with%20the%20knowledge%20of%20the%20type%20of%20text%0Aprompts%20preferred%20by%20the%20downstream%20VLM.%20Furthermore%2C%20we%20also%20explicitly%20steer%0Athe%20LLM%20generation%20process%20in%20each%20optimization%20step%20by%20specifically%20adding%20an%0Aoffset%20difference%20vector%20of%20the%20embeddings%20from%20the%20positive%20and%20negative%0Asolutions%20found%20by%20the%20LLM%2C%20in%20previous%20optimization%20steps%2C%20to%20the%20intermediate%0Alayer%20of%20the%20network%20for%20the%20next%20generation%20step.%20This%20offset%20vector%20steers%0Athe%20LLM%20generation%20toward%20the%20type%20of%20language%20preferred%20by%20the%20downstream%20VLM%2C%0Aresulting%20in%20enhanced%20performance%20on%20the%20downstream%20vision%20tasks.%20We%0Acomprehensively%20evaluate%20our%20GLOV%20on%2016%20diverse%20datasets%20using%20two%20families%20of%0AVLMs%2C%20i.e.%2C%20dual-encoder%20%28e.g.%2C%20CLIP%29%20and%20encoder-decoder%20%28e.g.%2C%20LLaVa%29%20models%0A--%20showing%20that%20the%20discovered%20solutions%20can%20enhance%20the%20recognition%0Aperformance%20by%20up%20to%2015.0%25%20and%2057.5%25%20%283.8%25%20and%2021.6%25%20on%20average%29%20for%20these%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06154v2&entry.124074799=Read"},
{"title": "Advances in 3D Neural Stylization: A Survey", "author": "Yingshu Chen and Guocheng Shao and Ka Chun Shum and Binh-Son Hua and Sai-Kit Yeung", "abstract": "  Modern artificial intelligence offers a novel and transformative approach to\ncreating digital art across diverse styles and modalities like images, videos\nand 3D data, unleashing the power of creativity and revolutionizing the way\nthat we perceive and interact with visual content. This paper reports on recent\nadvances in stylized 3D asset creation and manipulation with the expressive\npower of neural networks. We establish a taxonomy for neural stylization,\nconsidering crucial design choices such as scene representation, guidance data,\noptimization strategies, and output styles. Building on such taxonomy, our\nsurvey first revisits the background of neural stylization on 2D images, and\nthen presents in-depth discussions on recent neural stylization methods for 3D\ndata, accompanied by a benchmark evaluating selected mesh and neural field\nstylization methods. Based on the insights gained from the survey, we highlight\nthe practical significance, open challenges, future research, and potential\nimpacts of neural stylization, which facilitates researchers and practitioners\nto navigate the rapidly evolving landscape of 3D content creation using modern\nartificial intelligence.\n", "link": "http://arxiv.org/abs/2311.18328v3", "date": "2024-12-02", "relevancy": 2.7982, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5709}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5709}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advances%20in%203D%20Neural%20Stylization%3A%20A%20Survey&body=Title%3A%20Advances%20in%203D%20Neural%20Stylization%3A%20A%20Survey%0AAuthor%3A%20Yingshu%20Chen%20and%20Guocheng%20Shao%20and%20Ka%20Chun%20Shum%20and%20Binh-Son%20Hua%20and%20Sai-Kit%20Yeung%0AAbstract%3A%20%20%20Modern%20artificial%20intelligence%20offers%20a%20novel%20and%20transformative%20approach%20to%0Acreating%20digital%20art%20across%20diverse%20styles%20and%20modalities%20like%20images%2C%20videos%0Aand%203D%20data%2C%20unleashing%20the%20power%20of%20creativity%20and%20revolutionizing%20the%20way%0Athat%20we%20perceive%20and%20interact%20with%20visual%20content.%20This%20paper%20reports%20on%20recent%0Aadvances%20in%20stylized%203D%20asset%20creation%20and%20manipulation%20with%20the%20expressive%0Apower%20of%20neural%20networks.%20We%20establish%20a%20taxonomy%20for%20neural%20stylization%2C%0Aconsidering%20crucial%20design%20choices%20such%20as%20scene%20representation%2C%20guidance%20data%2C%0Aoptimization%20strategies%2C%20and%20output%20styles.%20Building%20on%20such%20taxonomy%2C%20our%0Asurvey%20first%20revisits%20the%20background%20of%20neural%20stylization%20on%202D%20images%2C%20and%0Athen%20presents%20in-depth%20discussions%20on%20recent%20neural%20stylization%20methods%20for%203D%0Adata%2C%20accompanied%20by%20a%20benchmark%20evaluating%20selected%20mesh%20and%20neural%20field%0Astylization%20methods.%20Based%20on%20the%20insights%20gained%20from%20the%20survey%2C%20we%20highlight%0Athe%20practical%20significance%2C%20open%20challenges%2C%20future%20research%2C%20and%20potential%0Aimpacts%20of%20neural%20stylization%2C%20which%20facilitates%20researchers%20and%20practitioners%0Ato%20navigate%20the%20rapidly%20evolving%20landscape%20of%203D%20content%20creation%20using%20modern%0Aartificial%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18328v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvances%2520in%25203D%2520Neural%2520Stylization%253A%2520A%2520Survey%26entry.906535625%3DYingshu%2520Chen%2520and%2520Guocheng%2520Shao%2520and%2520Ka%2520Chun%2520Shum%2520and%2520Binh-Son%2520Hua%2520and%2520Sai-Kit%2520Yeung%26entry.1292438233%3D%2520%2520Modern%2520artificial%2520intelligence%2520offers%2520a%2520novel%2520and%2520transformative%2520approach%2520to%250Acreating%2520digital%2520art%2520across%2520diverse%2520styles%2520and%2520modalities%2520like%2520images%252C%2520videos%250Aand%25203D%2520data%252C%2520unleashing%2520the%2520power%2520of%2520creativity%2520and%2520revolutionizing%2520the%2520way%250Athat%2520we%2520perceive%2520and%2520interact%2520with%2520visual%2520content.%2520This%2520paper%2520reports%2520on%2520recent%250Aadvances%2520in%2520stylized%25203D%2520asset%2520creation%2520and%2520manipulation%2520with%2520the%2520expressive%250Apower%2520of%2520neural%2520networks.%2520We%2520establish%2520a%2520taxonomy%2520for%2520neural%2520stylization%252C%250Aconsidering%2520crucial%2520design%2520choices%2520such%2520as%2520scene%2520representation%252C%2520guidance%2520data%252C%250Aoptimization%2520strategies%252C%2520and%2520output%2520styles.%2520Building%2520on%2520such%2520taxonomy%252C%2520our%250Asurvey%2520first%2520revisits%2520the%2520background%2520of%2520neural%2520stylization%2520on%25202D%2520images%252C%2520and%250Athen%2520presents%2520in-depth%2520discussions%2520on%2520recent%2520neural%2520stylization%2520methods%2520for%25203D%250Adata%252C%2520accompanied%2520by%2520a%2520benchmark%2520evaluating%2520selected%2520mesh%2520and%2520neural%2520field%250Astylization%2520methods.%2520Based%2520on%2520the%2520insights%2520gained%2520from%2520the%2520survey%252C%2520we%2520highlight%250Athe%2520practical%2520significance%252C%2520open%2520challenges%252C%2520future%2520research%252C%2520and%2520potential%250Aimpacts%2520of%2520neural%2520stylization%252C%2520which%2520facilitates%2520researchers%2520and%2520practitioners%250Ato%2520navigate%2520the%2520rapidly%2520evolving%2520landscape%2520of%25203D%2520content%2520creation%2520using%2520modern%250Aartificial%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.18328v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advances%20in%203D%20Neural%20Stylization%3A%20A%20Survey&entry.906535625=Yingshu%20Chen%20and%20Guocheng%20Shao%20and%20Ka%20Chun%20Shum%20and%20Binh-Son%20Hua%20and%20Sai-Kit%20Yeung&entry.1292438233=%20%20Modern%20artificial%20intelligence%20offers%20a%20novel%20and%20transformative%20approach%20to%0Acreating%20digital%20art%20across%20diverse%20styles%20and%20modalities%20like%20images%2C%20videos%0Aand%203D%20data%2C%20unleashing%20the%20power%20of%20creativity%20and%20revolutionizing%20the%20way%0Athat%20we%20perceive%20and%20interact%20with%20visual%20content.%20This%20paper%20reports%20on%20recent%0Aadvances%20in%20stylized%203D%20asset%20creation%20and%20manipulation%20with%20the%20expressive%0Apower%20of%20neural%20networks.%20We%20establish%20a%20taxonomy%20for%20neural%20stylization%2C%0Aconsidering%20crucial%20design%20choices%20such%20as%20scene%20representation%2C%20guidance%20data%2C%0Aoptimization%20strategies%2C%20and%20output%20styles.%20Building%20on%20such%20taxonomy%2C%20our%0Asurvey%20first%20revisits%20the%20background%20of%20neural%20stylization%20on%202D%20images%2C%20and%0Athen%20presents%20in-depth%20discussions%20on%20recent%20neural%20stylization%20methods%20for%203D%0Adata%2C%20accompanied%20by%20a%20benchmark%20evaluating%20selected%20mesh%20and%20neural%20field%0Astylization%20methods.%20Based%20on%20the%20insights%20gained%20from%20the%20survey%2C%20we%20highlight%0Athe%20practical%20significance%2C%20open%20challenges%2C%20future%20research%2C%20and%20potential%0Aimpacts%20of%20neural%20stylization%2C%20which%20facilitates%20researchers%20and%20practitioners%0Ato%20navigate%20the%20rapidly%20evolving%20landscape%20of%203D%20content%20creation%20using%20modern%0Aartificial%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18328v3&entry.124074799=Read"},
{"title": "A Survey and Benchmark of Automatic Surface Reconstruction from Point\n  Clouds", "author": "Raphael Sulzer and Renaud Marlet and Bruno Vallet and Loic Landrieu", "abstract": "  We present a comprehensive survey and benchmark of both traditional and\nlearning-based methods for surface reconstruction from point clouds. This task\nis particularly challenging for real-world acquisitions due to factors such as\nnoise, outliers, non-uniform sampling, and missing data. Traditional approaches\noften simplify the problem by imposing handcrafted priors on either the input\npoint clouds or the resulting surface, a process that can require tedious\nhyperparameter tuning. In contrast, deep learning models have the capability to\ndirectly learn the properties of input point clouds and desired surfaces from\ndata. We study the influence of handcrafted and learned priors on the precision\nand robustness of surface reconstruction techniques. We evaluate various\ntime-tested and contemporary methods in a standardized manner. When both\ntrained and evaluated on point clouds with identical characteristics, the\nlearning-based models consistently produce higher-quality surfaces compared to\ntheir traditional counterparts -- even in scenarios involving novel shape\ncategories. However, traditional methods demonstrate greater resilience to the\ndiverse anomalies commonly found in real-world 3D acquisitions. For the benefit\nof the research community, we make our code and datasets available, inviting\nfurther enhancements to learning-based surface reconstruction. This can be\naccessed at https://github.com/raphaelsulzer/dsr-benchmark .\n", "link": "http://arxiv.org/abs/2301.13656v4", "date": "2024-12-02", "relevancy": 2.7895, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5802}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5477}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20and%20Benchmark%20of%20Automatic%20Surface%20Reconstruction%20from%20Point%0A%20%20Clouds&body=Title%3A%20A%20Survey%20and%20Benchmark%20of%20Automatic%20Surface%20Reconstruction%20from%20Point%0A%20%20Clouds%0AAuthor%3A%20Raphael%20Sulzer%20and%20Renaud%20Marlet%20and%20Bruno%20Vallet%20and%20Loic%20Landrieu%0AAbstract%3A%20%20%20We%20present%20a%20comprehensive%20survey%20and%20benchmark%20of%20both%20traditional%20and%0Alearning-based%20methods%20for%20surface%20reconstruction%20from%20point%20clouds.%20This%20task%0Ais%20particularly%20challenging%20for%20real-world%20acquisitions%20due%20to%20factors%20such%20as%0Anoise%2C%20outliers%2C%20non-uniform%20sampling%2C%20and%20missing%20data.%20Traditional%20approaches%0Aoften%20simplify%20the%20problem%20by%20imposing%20handcrafted%20priors%20on%20either%20the%20input%0Apoint%20clouds%20or%20the%20resulting%20surface%2C%20a%20process%20that%20can%20require%20tedious%0Ahyperparameter%20tuning.%20In%20contrast%2C%20deep%20learning%20models%20have%20the%20capability%20to%0Adirectly%20learn%20the%20properties%20of%20input%20point%20clouds%20and%20desired%20surfaces%20from%0Adata.%20We%20study%20the%20influence%20of%20handcrafted%20and%20learned%20priors%20on%20the%20precision%0Aand%20robustness%20of%20surface%20reconstruction%20techniques.%20We%20evaluate%20various%0Atime-tested%20and%20contemporary%20methods%20in%20a%20standardized%20manner.%20When%20both%0Atrained%20and%20evaluated%20on%20point%20clouds%20with%20identical%20characteristics%2C%20the%0Alearning-based%20models%20consistently%20produce%20higher-quality%20surfaces%20compared%20to%0Atheir%20traditional%20counterparts%20--%20even%20in%20scenarios%20involving%20novel%20shape%0Acategories.%20However%2C%20traditional%20methods%20demonstrate%20greater%20resilience%20to%20the%0Adiverse%20anomalies%20commonly%20found%20in%20real-world%203D%20acquisitions.%20For%20the%20benefit%0Aof%20the%20research%20community%2C%20we%20make%20our%20code%20and%20datasets%20available%2C%20inviting%0Afurther%20enhancements%20to%20learning-based%20surface%20reconstruction.%20This%20can%20be%0Aaccessed%20at%20https%3A//github.com/raphaelsulzer/dsr-benchmark%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.13656v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520and%2520Benchmark%2520of%2520Automatic%2520Surface%2520Reconstruction%2520from%2520Point%250A%2520%2520Clouds%26entry.906535625%3DRaphael%2520Sulzer%2520and%2520Renaud%2520Marlet%2520and%2520Bruno%2520Vallet%2520and%2520Loic%2520Landrieu%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520comprehensive%2520survey%2520and%2520benchmark%2520of%2520both%2520traditional%2520and%250Alearning-based%2520methods%2520for%2520surface%2520reconstruction%2520from%2520point%2520clouds.%2520This%2520task%250Ais%2520particularly%2520challenging%2520for%2520real-world%2520acquisitions%2520due%2520to%2520factors%2520such%2520as%250Anoise%252C%2520outliers%252C%2520non-uniform%2520sampling%252C%2520and%2520missing%2520data.%2520Traditional%2520approaches%250Aoften%2520simplify%2520the%2520problem%2520by%2520imposing%2520handcrafted%2520priors%2520on%2520either%2520the%2520input%250Apoint%2520clouds%2520or%2520the%2520resulting%2520surface%252C%2520a%2520process%2520that%2520can%2520require%2520tedious%250Ahyperparameter%2520tuning.%2520In%2520contrast%252C%2520deep%2520learning%2520models%2520have%2520the%2520capability%2520to%250Adirectly%2520learn%2520the%2520properties%2520of%2520input%2520point%2520clouds%2520and%2520desired%2520surfaces%2520from%250Adata.%2520We%2520study%2520the%2520influence%2520of%2520handcrafted%2520and%2520learned%2520priors%2520on%2520the%2520precision%250Aand%2520robustness%2520of%2520surface%2520reconstruction%2520techniques.%2520We%2520evaluate%2520various%250Atime-tested%2520and%2520contemporary%2520methods%2520in%2520a%2520standardized%2520manner.%2520When%2520both%250Atrained%2520and%2520evaluated%2520on%2520point%2520clouds%2520with%2520identical%2520characteristics%252C%2520the%250Alearning-based%2520models%2520consistently%2520produce%2520higher-quality%2520surfaces%2520compared%2520to%250Atheir%2520traditional%2520counterparts%2520--%2520even%2520in%2520scenarios%2520involving%2520novel%2520shape%250Acategories.%2520However%252C%2520traditional%2520methods%2520demonstrate%2520greater%2520resilience%2520to%2520the%250Adiverse%2520anomalies%2520commonly%2520found%2520in%2520real-world%25203D%2520acquisitions.%2520For%2520the%2520benefit%250Aof%2520the%2520research%2520community%252C%2520we%2520make%2520our%2520code%2520and%2520datasets%2520available%252C%2520inviting%250Afurther%2520enhancements%2520to%2520learning-based%2520surface%2520reconstruction.%2520This%2520can%2520be%250Aaccessed%2520at%2520https%253A//github.com/raphaelsulzer/dsr-benchmark%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.13656v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20and%20Benchmark%20of%20Automatic%20Surface%20Reconstruction%20from%20Point%0A%20%20Clouds&entry.906535625=Raphael%20Sulzer%20and%20Renaud%20Marlet%20and%20Bruno%20Vallet%20and%20Loic%20Landrieu&entry.1292438233=%20%20We%20present%20a%20comprehensive%20survey%20and%20benchmark%20of%20both%20traditional%20and%0Alearning-based%20methods%20for%20surface%20reconstruction%20from%20point%20clouds.%20This%20task%0Ais%20particularly%20challenging%20for%20real-world%20acquisitions%20due%20to%20factors%20such%20as%0Anoise%2C%20outliers%2C%20non-uniform%20sampling%2C%20and%20missing%20data.%20Traditional%20approaches%0Aoften%20simplify%20the%20problem%20by%20imposing%20handcrafted%20priors%20on%20either%20the%20input%0Apoint%20clouds%20or%20the%20resulting%20surface%2C%20a%20process%20that%20can%20require%20tedious%0Ahyperparameter%20tuning.%20In%20contrast%2C%20deep%20learning%20models%20have%20the%20capability%20to%0Adirectly%20learn%20the%20properties%20of%20input%20point%20clouds%20and%20desired%20surfaces%20from%0Adata.%20We%20study%20the%20influence%20of%20handcrafted%20and%20learned%20priors%20on%20the%20precision%0Aand%20robustness%20of%20surface%20reconstruction%20techniques.%20We%20evaluate%20various%0Atime-tested%20and%20contemporary%20methods%20in%20a%20standardized%20manner.%20When%20both%0Atrained%20and%20evaluated%20on%20point%20clouds%20with%20identical%20characteristics%2C%20the%0Alearning-based%20models%20consistently%20produce%20higher-quality%20surfaces%20compared%20to%0Atheir%20traditional%20counterparts%20--%20even%20in%20scenarios%20involving%20novel%20shape%0Acategories.%20However%2C%20traditional%20methods%20demonstrate%20greater%20resilience%20to%20the%0Adiverse%20anomalies%20commonly%20found%20in%20real-world%203D%20acquisitions.%20For%20the%20benefit%0Aof%20the%20research%20community%2C%20we%20make%20our%20code%20and%20datasets%20available%2C%20inviting%0Afurther%20enhancements%20to%20learning-based%20surface%20reconstruction.%20This%20can%20be%0Aaccessed%20at%20https%3A//github.com/raphaelsulzer/dsr-benchmark%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.13656v4&entry.124074799=Read"},
{"title": "Multi-task Learning To Improve Semantic Segmentation Of CBCT Scans Using\n  Image Reconstruction", "author": "Maximilian Ernst Tschuchnig and Julia Coste-Marin and Philipp Steininger and Michael Gadermayr", "abstract": "  Semantic segmentation is a crucial task in medical image processing,\nessential for segmenting organs or lesions such as tumors. In this study we aim\nto improve automated segmentation in CBCTs through multi-task learning. To\nevaluate effects on different volume qualities, a CBCT dataset is synthesised\nfrom the CT Liver Tumor Segmentation Benchmark (LiTS) dataset. To improve\nsegmentation, two approaches are investigated. First, we perform multi-task\nlearning to add morphology based regularization through a volume reconstruction\ntask. Second, we use this reconstruction task to reconstruct the best quality\nCBCT (most similar to the original CT), facilitating denoising effects. We\nexplore both holistic and patch-based approaches. Our findings reveal that,\nespecially using a patch-based approach, multi-task learning improves\nsegmentation in most cases and that these results can further be improved by\nour denoising approach.\n", "link": "http://arxiv.org/abs/2312.12990v2", "date": "2024-12-02", "relevancy": 2.7205, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5772}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5276}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-task%20Learning%20To%20Improve%20Semantic%20Segmentation%20Of%20CBCT%20Scans%20Using%0A%20%20Image%20Reconstruction&body=Title%3A%20Multi-task%20Learning%20To%20Improve%20Semantic%20Segmentation%20Of%20CBCT%20Scans%20Using%0A%20%20Image%20Reconstruction%0AAuthor%3A%20Maximilian%20Ernst%20Tschuchnig%20and%20Julia%20Coste-Marin%20and%20Philipp%20Steininger%20and%20Michael%20Gadermayr%0AAbstract%3A%20%20%20Semantic%20segmentation%20is%20a%20crucial%20task%20in%20medical%20image%20processing%2C%0Aessential%20for%20segmenting%20organs%20or%20lesions%20such%20as%20tumors.%20In%20this%20study%20we%20aim%0Ato%20improve%20automated%20segmentation%20in%20CBCTs%20through%20multi-task%20learning.%20To%0Aevaluate%20effects%20on%20different%20volume%20qualities%2C%20a%20CBCT%20dataset%20is%20synthesised%0Afrom%20the%20CT%20Liver%20Tumor%20Segmentation%20Benchmark%20%28LiTS%29%20dataset.%20To%20improve%0Asegmentation%2C%20two%20approaches%20are%20investigated.%20First%2C%20we%20perform%20multi-task%0Alearning%20to%20add%20morphology%20based%20regularization%20through%20a%20volume%20reconstruction%0Atask.%20Second%2C%20we%20use%20this%20reconstruction%20task%20to%20reconstruct%20the%20best%20quality%0ACBCT%20%28most%20similar%20to%20the%20original%20CT%29%2C%20facilitating%20denoising%20effects.%20We%0Aexplore%20both%20holistic%20and%20patch-based%20approaches.%20Our%20findings%20reveal%20that%2C%0Aespecially%20using%20a%20patch-based%20approach%2C%20multi-task%20learning%20improves%0Asegmentation%20in%20most%20cases%20and%20that%20these%20results%20can%20further%20be%20improved%20by%0Aour%20denoising%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12990v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-task%2520Learning%2520To%2520Improve%2520Semantic%2520Segmentation%2520Of%2520CBCT%2520Scans%2520Using%250A%2520%2520Image%2520Reconstruction%26entry.906535625%3DMaximilian%2520Ernst%2520Tschuchnig%2520and%2520Julia%2520Coste-Marin%2520and%2520Philipp%2520Steininger%2520and%2520Michael%2520Gadermayr%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520is%2520a%2520crucial%2520task%2520in%2520medical%2520image%2520processing%252C%250Aessential%2520for%2520segmenting%2520organs%2520or%2520lesions%2520such%2520as%2520tumors.%2520In%2520this%2520study%2520we%2520aim%250Ato%2520improve%2520automated%2520segmentation%2520in%2520CBCTs%2520through%2520multi-task%2520learning.%2520To%250Aevaluate%2520effects%2520on%2520different%2520volume%2520qualities%252C%2520a%2520CBCT%2520dataset%2520is%2520synthesised%250Afrom%2520the%2520CT%2520Liver%2520Tumor%2520Segmentation%2520Benchmark%2520%2528LiTS%2529%2520dataset.%2520To%2520improve%250Asegmentation%252C%2520two%2520approaches%2520are%2520investigated.%2520First%252C%2520we%2520perform%2520multi-task%250Alearning%2520to%2520add%2520morphology%2520based%2520regularization%2520through%2520a%2520volume%2520reconstruction%250Atask.%2520Second%252C%2520we%2520use%2520this%2520reconstruction%2520task%2520to%2520reconstruct%2520the%2520best%2520quality%250ACBCT%2520%2528most%2520similar%2520to%2520the%2520original%2520CT%2529%252C%2520facilitating%2520denoising%2520effects.%2520We%250Aexplore%2520both%2520holistic%2520and%2520patch-based%2520approaches.%2520Our%2520findings%2520reveal%2520that%252C%250Aespecially%2520using%2520a%2520patch-based%2520approach%252C%2520multi-task%2520learning%2520improves%250Asegmentation%2520in%2520most%2520cases%2520and%2520that%2520these%2520results%2520can%2520further%2520be%2520improved%2520by%250Aour%2520denoising%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.12990v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-task%20Learning%20To%20Improve%20Semantic%20Segmentation%20Of%20CBCT%20Scans%20Using%0A%20%20Image%20Reconstruction&entry.906535625=Maximilian%20Ernst%20Tschuchnig%20and%20Julia%20Coste-Marin%20and%20Philipp%20Steininger%20and%20Michael%20Gadermayr&entry.1292438233=%20%20Semantic%20segmentation%20is%20a%20crucial%20task%20in%20medical%20image%20processing%2C%0Aessential%20for%20segmenting%20organs%20or%20lesions%20such%20as%20tumors.%20In%20this%20study%20we%20aim%0Ato%20improve%20automated%20segmentation%20in%20CBCTs%20through%20multi-task%20learning.%20To%0Aevaluate%20effects%20on%20different%20volume%20qualities%2C%20a%20CBCT%20dataset%20is%20synthesised%0Afrom%20the%20CT%20Liver%20Tumor%20Segmentation%20Benchmark%20%28LiTS%29%20dataset.%20To%20improve%0Asegmentation%2C%20two%20approaches%20are%20investigated.%20First%2C%20we%20perform%20multi-task%0Alearning%20to%20add%20morphology%20based%20regularization%20through%20a%20volume%20reconstruction%0Atask.%20Second%2C%20we%20use%20this%20reconstruction%20task%20to%20reconstruct%20the%20best%20quality%0ACBCT%20%28most%20similar%20to%20the%20original%20CT%29%2C%20facilitating%20denoising%20effects.%20We%0Aexplore%20both%20holistic%20and%20patch-based%20approaches.%20Our%20findings%20reveal%20that%2C%0Aespecially%20using%20a%20patch-based%20approach%2C%20multi-task%20learning%20improves%0Asegmentation%20in%20most%20cases%20and%20that%20these%20results%20can%20further%20be%20improved%20by%0Aour%20denoising%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12990v2&entry.124074799=Read"},
{"title": "DGNN-YOLO: Dynamic Graph Neural Networks with YOLO11 for Small Object\n  Detection and Tracking in Traffic Surveillance", "author": "Shahriar Soudeep and M. F. Mridha and Md Abrar Jahin and Nilanjan Dey", "abstract": "  Accurate detection and tracking of small objects such as pedestrians,\ncyclists, and motorbikes are critical for traffic surveillance systems, which\nare crucial in improving road safety and decision-making in intelligent\ntransportation systems. However, traditional methods struggle with challenges\nsuch as occlusion, low resolution, and dynamic traffic conditions,\nnecessitating innovative approaches to address these limitations. This paper\nintroduces DGNN-YOLO, a novel framework integrating dynamic graph neural\nnetworks (DGNN) with YOLO11 to enhance small object detection and tracking in\ntraffic surveillance systems. The framework leverages YOLO11's advanced spatial\nfeature extraction capabilities for precise object detection and incorporates\nDGNN to model spatial-temporal relationships for robust real-time tracking\ndynamically. By constructing and updating graph structures, DGNN-YOLO\neffectively represents objects as nodes and their interactions as edges,\nensuring adaptive and accurate tracking in complex and dynamic environments.\nExtensive experiments demonstrate that DGNN-YOLO consistently outperforms\nstate-of-the-art methods in detecting and tracking small objects under diverse\ntraffic conditions, achieving the highest precision (0.8382), recall (0.6875),\nand mAP@0.5:0.95 (0.6476), showcasing its robustness and scalability,\nparticularly in challenging scenarios involving small and occluded objects.\nThis work provides a scalable, real-time traffic surveillance and analysis\nsolution, significantly contributing to intelligent transportation systems.\n", "link": "http://arxiv.org/abs/2411.17251v2", "date": "2024-12-02", "relevancy": 2.7126, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5564}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5461}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DGNN-YOLO%3A%20Dynamic%20Graph%20Neural%20Networks%20with%20YOLO11%20for%20Small%20Object%0A%20%20Detection%20and%20Tracking%20in%20Traffic%20Surveillance&body=Title%3A%20DGNN-YOLO%3A%20Dynamic%20Graph%20Neural%20Networks%20with%20YOLO11%20for%20Small%20Object%0A%20%20Detection%20and%20Tracking%20in%20Traffic%20Surveillance%0AAuthor%3A%20Shahriar%20Soudeep%20and%20M.%20F.%20Mridha%20and%20Md%20Abrar%20Jahin%20and%20Nilanjan%20Dey%0AAbstract%3A%20%20%20Accurate%20detection%20and%20tracking%20of%20small%20objects%20such%20as%20pedestrians%2C%0Acyclists%2C%20and%20motorbikes%20are%20critical%20for%20traffic%20surveillance%20systems%2C%20which%0Aare%20crucial%20in%20improving%20road%20safety%20and%20decision-making%20in%20intelligent%0Atransportation%20systems.%20However%2C%20traditional%20methods%20struggle%20with%20challenges%0Asuch%20as%20occlusion%2C%20low%20resolution%2C%20and%20dynamic%20traffic%20conditions%2C%0Anecessitating%20innovative%20approaches%20to%20address%20these%20limitations.%20This%20paper%0Aintroduces%20DGNN-YOLO%2C%20a%20novel%20framework%20integrating%20dynamic%20graph%20neural%0Anetworks%20%28DGNN%29%20with%20YOLO11%20to%20enhance%20small%20object%20detection%20and%20tracking%20in%0Atraffic%20surveillance%20systems.%20The%20framework%20leverages%20YOLO11%27s%20advanced%20spatial%0Afeature%20extraction%20capabilities%20for%20precise%20object%20detection%20and%20incorporates%0ADGNN%20to%20model%20spatial-temporal%20relationships%20for%20robust%20real-time%20tracking%0Adynamically.%20By%20constructing%20and%20updating%20graph%20structures%2C%20DGNN-YOLO%0Aeffectively%20represents%20objects%20as%20nodes%20and%20their%20interactions%20as%20edges%2C%0Aensuring%20adaptive%20and%20accurate%20tracking%20in%20complex%20and%20dynamic%20environments.%0AExtensive%20experiments%20demonstrate%20that%20DGNN-YOLO%20consistently%20outperforms%0Astate-of-the-art%20methods%20in%20detecting%20and%20tracking%20small%20objects%20under%20diverse%0Atraffic%20conditions%2C%20achieving%20the%20highest%20precision%20%280.8382%29%2C%20recall%20%280.6875%29%2C%0Aand%20mAP%400.5%3A0.95%20%280.6476%29%2C%20showcasing%20its%20robustness%20and%20scalability%2C%0Aparticularly%20in%20challenging%20scenarios%20involving%20small%20and%20occluded%20objects.%0AThis%20work%20provides%20a%20scalable%2C%20real-time%20traffic%20surveillance%20and%20analysis%0Asolution%2C%20significantly%20contributing%20to%20intelligent%20transportation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17251v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDGNN-YOLO%253A%2520Dynamic%2520Graph%2520Neural%2520Networks%2520with%2520YOLO11%2520for%2520Small%2520Object%250A%2520%2520Detection%2520and%2520Tracking%2520in%2520Traffic%2520Surveillance%26entry.906535625%3DShahriar%2520Soudeep%2520and%2520M.%2520F.%2520Mridha%2520and%2520Md%2520Abrar%2520Jahin%2520and%2520Nilanjan%2520Dey%26entry.1292438233%3D%2520%2520Accurate%2520detection%2520and%2520tracking%2520of%2520small%2520objects%2520such%2520as%2520pedestrians%252C%250Acyclists%252C%2520and%2520motorbikes%2520are%2520critical%2520for%2520traffic%2520surveillance%2520systems%252C%2520which%250Aare%2520crucial%2520in%2520improving%2520road%2520safety%2520and%2520decision-making%2520in%2520intelligent%250Atransportation%2520systems.%2520However%252C%2520traditional%2520methods%2520struggle%2520with%2520challenges%250Asuch%2520as%2520occlusion%252C%2520low%2520resolution%252C%2520and%2520dynamic%2520traffic%2520conditions%252C%250Anecessitating%2520innovative%2520approaches%2520to%2520address%2520these%2520limitations.%2520This%2520paper%250Aintroduces%2520DGNN-YOLO%252C%2520a%2520novel%2520framework%2520integrating%2520dynamic%2520graph%2520neural%250Anetworks%2520%2528DGNN%2529%2520with%2520YOLO11%2520to%2520enhance%2520small%2520object%2520detection%2520and%2520tracking%2520in%250Atraffic%2520surveillance%2520systems.%2520The%2520framework%2520leverages%2520YOLO11%2527s%2520advanced%2520spatial%250Afeature%2520extraction%2520capabilities%2520for%2520precise%2520object%2520detection%2520and%2520incorporates%250ADGNN%2520to%2520model%2520spatial-temporal%2520relationships%2520for%2520robust%2520real-time%2520tracking%250Adynamically.%2520By%2520constructing%2520and%2520updating%2520graph%2520structures%252C%2520DGNN-YOLO%250Aeffectively%2520represents%2520objects%2520as%2520nodes%2520and%2520their%2520interactions%2520as%2520edges%252C%250Aensuring%2520adaptive%2520and%2520accurate%2520tracking%2520in%2520complex%2520and%2520dynamic%2520environments.%250AExtensive%2520experiments%2520demonstrate%2520that%2520DGNN-YOLO%2520consistently%2520outperforms%250Astate-of-the-art%2520methods%2520in%2520detecting%2520and%2520tracking%2520small%2520objects%2520under%2520diverse%250Atraffic%2520conditions%252C%2520achieving%2520the%2520highest%2520precision%2520%25280.8382%2529%252C%2520recall%2520%25280.6875%2529%252C%250Aand%2520mAP%25400.5%253A0.95%2520%25280.6476%2529%252C%2520showcasing%2520its%2520robustness%2520and%2520scalability%252C%250Aparticularly%2520in%2520challenging%2520scenarios%2520involving%2520small%2520and%2520occluded%2520objects.%250AThis%2520work%2520provides%2520a%2520scalable%252C%2520real-time%2520traffic%2520surveillance%2520and%2520analysis%250Asolution%252C%2520significantly%2520contributing%2520to%2520intelligent%2520transportation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17251v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DGNN-YOLO%3A%20Dynamic%20Graph%20Neural%20Networks%20with%20YOLO11%20for%20Small%20Object%0A%20%20Detection%20and%20Tracking%20in%20Traffic%20Surveillance&entry.906535625=Shahriar%20Soudeep%20and%20M.%20F.%20Mridha%20and%20Md%20Abrar%20Jahin%20and%20Nilanjan%20Dey&entry.1292438233=%20%20Accurate%20detection%20and%20tracking%20of%20small%20objects%20such%20as%20pedestrians%2C%0Acyclists%2C%20and%20motorbikes%20are%20critical%20for%20traffic%20surveillance%20systems%2C%20which%0Aare%20crucial%20in%20improving%20road%20safety%20and%20decision-making%20in%20intelligent%0Atransportation%20systems.%20However%2C%20traditional%20methods%20struggle%20with%20challenges%0Asuch%20as%20occlusion%2C%20low%20resolution%2C%20and%20dynamic%20traffic%20conditions%2C%0Anecessitating%20innovative%20approaches%20to%20address%20these%20limitations.%20This%20paper%0Aintroduces%20DGNN-YOLO%2C%20a%20novel%20framework%20integrating%20dynamic%20graph%20neural%0Anetworks%20%28DGNN%29%20with%20YOLO11%20to%20enhance%20small%20object%20detection%20and%20tracking%20in%0Atraffic%20surveillance%20systems.%20The%20framework%20leverages%20YOLO11%27s%20advanced%20spatial%0Afeature%20extraction%20capabilities%20for%20precise%20object%20detection%20and%20incorporates%0ADGNN%20to%20model%20spatial-temporal%20relationships%20for%20robust%20real-time%20tracking%0Adynamically.%20By%20constructing%20and%20updating%20graph%20structures%2C%20DGNN-YOLO%0Aeffectively%20represents%20objects%20as%20nodes%20and%20their%20interactions%20as%20edges%2C%0Aensuring%20adaptive%20and%20accurate%20tracking%20in%20complex%20and%20dynamic%20environments.%0AExtensive%20experiments%20demonstrate%20that%20DGNN-YOLO%20consistently%20outperforms%0Astate-of-the-art%20methods%20in%20detecting%20and%20tracking%20small%20objects%20under%20diverse%0Atraffic%20conditions%2C%20achieving%20the%20highest%20precision%20%280.8382%29%2C%20recall%20%280.6875%29%2C%0Aand%20mAP%400.5%3A0.95%20%280.6476%29%2C%20showcasing%20its%20robustness%20and%20scalability%2C%0Aparticularly%20in%20challenging%20scenarios%20involving%20small%20and%20occluded%20objects.%0AThis%20work%20provides%20a%20scalable%2C%20real-time%20traffic%20surveillance%20and%20analysis%0Asolution%2C%20significantly%20contributing%20to%20intelligent%20transportation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17251v2&entry.124074799=Read"},
{"title": "Moner: Motion Correction in Undersampled Radial MRI with Unsupervised\n  Neural Representation", "author": "Qing Wu and Chenhe Du and XuanYu Tian and Jingyi Yu and Yuyao Zhang and Hongjiang Wei", "abstract": "  Motion correction (MoCo) in radial MRI is a challenging problem due to the\nunpredictability of subject's motion. Current state-of-the-art (SOTA) MoCo\nalgorithms often use extensive high-quality MR images to pre-train neural\nnetworks, obtaining excellent reconstructions. However, the need for\nlarge-scale datasets significantly increases costs and limits model\ngeneralization. In this work, we propose Moner, an unsupervised MoCo method\nthat jointly solves artifact-free MR images and accurate motion from\nundersampled, rigid motion-corrupted k-space data, without requiring training\ndata. Our core idea is to leverage the continuous prior of implicit neural\nrepresentation (INR) to constrain this ill-posed inverse problem, enabling\nideal solutions. Specifically, we incorporate a quasi-static motion model into\nthe INR, granting its ability to correct subject's motion. To stabilize model\noptimization, we reformulate radial MRI as a back-projection problem using the\nFourier-slice theorem. Additionally, we propose a novel coarse-to-fine hash\nencoding strategy, significantly enhancing MoCo accuracy. Experiments on\nmultiple MRI datasets show our Moner achieves performance comparable to SOTA\nMoCo techniques on in-domain data, while demonstrating significant improvements\non out-of-domain data.\n", "link": "http://arxiv.org/abs/2409.16921v2", "date": "2024-12-02", "relevancy": 2.6134, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5331}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5226}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moner%3A%20Motion%20Correction%20in%20Undersampled%20Radial%20MRI%20with%20Unsupervised%0A%20%20Neural%20Representation&body=Title%3A%20Moner%3A%20Motion%20Correction%20in%20Undersampled%20Radial%20MRI%20with%20Unsupervised%0A%20%20Neural%20Representation%0AAuthor%3A%20Qing%20Wu%20and%20Chenhe%20Du%20and%20XuanYu%20Tian%20and%20Jingyi%20Yu%20and%20Yuyao%20Zhang%20and%20Hongjiang%20Wei%0AAbstract%3A%20%20%20Motion%20correction%20%28MoCo%29%20in%20radial%20MRI%20is%20a%20challenging%20problem%20due%20to%20the%0Aunpredictability%20of%20subject%27s%20motion.%20Current%20state-of-the-art%20%28SOTA%29%20MoCo%0Aalgorithms%20often%20use%20extensive%20high-quality%20MR%20images%20to%20pre-train%20neural%0Anetworks%2C%20obtaining%20excellent%20reconstructions.%20However%2C%20the%20need%20for%0Alarge-scale%20datasets%20significantly%20increases%20costs%20and%20limits%20model%0Ageneralization.%20In%20this%20work%2C%20we%20propose%20Moner%2C%20an%20unsupervised%20MoCo%20method%0Athat%20jointly%20solves%20artifact-free%20MR%20images%20and%20accurate%20motion%20from%0Aundersampled%2C%20rigid%20motion-corrupted%20k-space%20data%2C%20without%20requiring%20training%0Adata.%20Our%20core%20idea%20is%20to%20leverage%20the%20continuous%20prior%20of%20implicit%20neural%0Arepresentation%20%28INR%29%20to%20constrain%20this%20ill-posed%20inverse%20problem%2C%20enabling%0Aideal%20solutions.%20Specifically%2C%20we%20incorporate%20a%20quasi-static%20motion%20model%20into%0Athe%20INR%2C%20granting%20its%20ability%20to%20correct%20subject%27s%20motion.%20To%20stabilize%20model%0Aoptimization%2C%20we%20reformulate%20radial%20MRI%20as%20a%20back-projection%20problem%20using%20the%0AFourier-slice%20theorem.%20Additionally%2C%20we%20propose%20a%20novel%20coarse-to-fine%20hash%0Aencoding%20strategy%2C%20significantly%20enhancing%20MoCo%20accuracy.%20Experiments%20on%0Amultiple%20MRI%20datasets%20show%20our%20Moner%20achieves%20performance%20comparable%20to%20SOTA%0AMoCo%20techniques%20on%20in-domain%20data%2C%20while%20demonstrating%20significant%20improvements%0Aon%20out-of-domain%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16921v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoner%253A%2520Motion%2520Correction%2520in%2520Undersampled%2520Radial%2520MRI%2520with%2520Unsupervised%250A%2520%2520Neural%2520Representation%26entry.906535625%3DQing%2520Wu%2520and%2520Chenhe%2520Du%2520and%2520XuanYu%2520Tian%2520and%2520Jingyi%2520Yu%2520and%2520Yuyao%2520Zhang%2520and%2520Hongjiang%2520Wei%26entry.1292438233%3D%2520%2520Motion%2520correction%2520%2528MoCo%2529%2520in%2520radial%2520MRI%2520is%2520a%2520challenging%2520problem%2520due%2520to%2520the%250Aunpredictability%2520of%2520subject%2527s%2520motion.%2520Current%2520state-of-the-art%2520%2528SOTA%2529%2520MoCo%250Aalgorithms%2520often%2520use%2520extensive%2520high-quality%2520MR%2520images%2520to%2520pre-train%2520neural%250Anetworks%252C%2520obtaining%2520excellent%2520reconstructions.%2520However%252C%2520the%2520need%2520for%250Alarge-scale%2520datasets%2520significantly%2520increases%2520costs%2520and%2520limits%2520model%250Ageneralization.%2520In%2520this%2520work%252C%2520we%2520propose%2520Moner%252C%2520an%2520unsupervised%2520MoCo%2520method%250Athat%2520jointly%2520solves%2520artifact-free%2520MR%2520images%2520and%2520accurate%2520motion%2520from%250Aundersampled%252C%2520rigid%2520motion-corrupted%2520k-space%2520data%252C%2520without%2520requiring%2520training%250Adata.%2520Our%2520core%2520idea%2520is%2520to%2520leverage%2520the%2520continuous%2520prior%2520of%2520implicit%2520neural%250Arepresentation%2520%2528INR%2529%2520to%2520constrain%2520this%2520ill-posed%2520inverse%2520problem%252C%2520enabling%250Aideal%2520solutions.%2520Specifically%252C%2520we%2520incorporate%2520a%2520quasi-static%2520motion%2520model%2520into%250Athe%2520INR%252C%2520granting%2520its%2520ability%2520to%2520correct%2520subject%2527s%2520motion.%2520To%2520stabilize%2520model%250Aoptimization%252C%2520we%2520reformulate%2520radial%2520MRI%2520as%2520a%2520back-projection%2520problem%2520using%2520the%250AFourier-slice%2520theorem.%2520Additionally%252C%2520we%2520propose%2520a%2520novel%2520coarse-to-fine%2520hash%250Aencoding%2520strategy%252C%2520significantly%2520enhancing%2520MoCo%2520accuracy.%2520Experiments%2520on%250Amultiple%2520MRI%2520datasets%2520show%2520our%2520Moner%2520achieves%2520performance%2520comparable%2520to%2520SOTA%250AMoCo%2520techniques%2520on%2520in-domain%2520data%252C%2520while%2520demonstrating%2520significant%2520improvements%250Aon%2520out-of-domain%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16921v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moner%3A%20Motion%20Correction%20in%20Undersampled%20Radial%20MRI%20with%20Unsupervised%0A%20%20Neural%20Representation&entry.906535625=Qing%20Wu%20and%20Chenhe%20Du%20and%20XuanYu%20Tian%20and%20Jingyi%20Yu%20and%20Yuyao%20Zhang%20and%20Hongjiang%20Wei&entry.1292438233=%20%20Motion%20correction%20%28MoCo%29%20in%20radial%20MRI%20is%20a%20challenging%20problem%20due%20to%20the%0Aunpredictability%20of%20subject%27s%20motion.%20Current%20state-of-the-art%20%28SOTA%29%20MoCo%0Aalgorithms%20often%20use%20extensive%20high-quality%20MR%20images%20to%20pre-train%20neural%0Anetworks%2C%20obtaining%20excellent%20reconstructions.%20However%2C%20the%20need%20for%0Alarge-scale%20datasets%20significantly%20increases%20costs%20and%20limits%20model%0Ageneralization.%20In%20this%20work%2C%20we%20propose%20Moner%2C%20an%20unsupervised%20MoCo%20method%0Athat%20jointly%20solves%20artifact-free%20MR%20images%20and%20accurate%20motion%20from%0Aundersampled%2C%20rigid%20motion-corrupted%20k-space%20data%2C%20without%20requiring%20training%0Adata.%20Our%20core%20idea%20is%20to%20leverage%20the%20continuous%20prior%20of%20implicit%20neural%0Arepresentation%20%28INR%29%20to%20constrain%20this%20ill-posed%20inverse%20problem%2C%20enabling%0Aideal%20solutions.%20Specifically%2C%20we%20incorporate%20a%20quasi-static%20motion%20model%20into%0Athe%20INR%2C%20granting%20its%20ability%20to%20correct%20subject%27s%20motion.%20To%20stabilize%20model%0Aoptimization%2C%20we%20reformulate%20radial%20MRI%20as%20a%20back-projection%20problem%20using%20the%0AFourier-slice%20theorem.%20Additionally%2C%20we%20propose%20a%20novel%20coarse-to-fine%20hash%0Aencoding%20strategy%2C%20significantly%20enhancing%20MoCo%20accuracy.%20Experiments%20on%0Amultiple%20MRI%20datasets%20show%20our%20Moner%20achieves%20performance%20comparable%20to%20SOTA%0AMoCo%20techniques%20on%20in-domain%20data%2C%20while%20demonstrating%20significant%20improvements%0Aon%20out-of-domain%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16921v2&entry.124074799=Read"},
{"title": "Continual Learning in the Presence of Repetition", "author": "Hamed Hemati and Lorenzo Pellegrini and Xiaotian Duan and Zixuan Zhao and Fangfang Xia and Marc Masana and Benedikt Tscheschner and Eduardo Veas and Yuxiang Zheng and Shiji Zhao and Shao-Yuan Li and Sheng-Jun Huang and Vincenzo Lomonaco and Gido M. van de Ven", "abstract": "  Continual learning (CL) provides a framework for training models in\never-evolving environments. Although re-occurrence of previously seen objects\nor tasks is common in real-world problems, the concept of repetition in the\ndata stream is not often considered in standard benchmarks for CL. Unlike with\nthe rehearsal mechanism in buffer-based strategies, where sample repetition is\ncontrolled by the strategy, repetition in the data stream naturally stems from\nthe environment. This report provides a summary of the CLVision challenge at\nCVPR 2023, which focused on the topic of repetition in class-incremental\nlearning. The report initially outlines the challenge objective and then\ndescribes three solutions proposed by finalist teams that aim to effectively\nexploit the repetition in the stream to learn continually. The experimental\nresults from the challenge highlight the effectiveness of ensemble-based\nsolutions that employ multiple versions of similar modules, each trained on\ndifferent but overlapping subsets of classes. This report underscores the\ntransformative potential of taking a different perspective in CL by employing\nrepetition in the data stream to foster innovative strategy design.\n", "link": "http://arxiv.org/abs/2405.04101v2", "date": "2024-12-02", "relevancy": 2.5468, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5141}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5141}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%20in%20the%20Presence%20of%20Repetition&body=Title%3A%20Continual%20Learning%20in%20the%20Presence%20of%20Repetition%0AAuthor%3A%20Hamed%20Hemati%20and%20Lorenzo%20Pellegrini%20and%20Xiaotian%20Duan%20and%20Zixuan%20Zhao%20and%20Fangfang%20Xia%20and%20Marc%20Masana%20and%20Benedikt%20Tscheschner%20and%20Eduardo%20Veas%20and%20Yuxiang%20Zheng%20and%20Shiji%20Zhao%20and%20Shao-Yuan%20Li%20and%20Sheng-Jun%20Huang%20and%20Vincenzo%20Lomonaco%20and%20Gido%20M.%20van%20de%20Ven%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20provides%20a%20framework%20for%20training%20models%20in%0Aever-evolving%20environments.%20Although%20re-occurrence%20of%20previously%20seen%20objects%0Aor%20tasks%20is%20common%20in%20real-world%20problems%2C%20the%20concept%20of%20repetition%20in%20the%0Adata%20stream%20is%20not%20often%20considered%20in%20standard%20benchmarks%20for%20CL.%20Unlike%20with%0Athe%20rehearsal%20mechanism%20in%20buffer-based%20strategies%2C%20where%20sample%20repetition%20is%0Acontrolled%20by%20the%20strategy%2C%20repetition%20in%20the%20data%20stream%20naturally%20stems%20from%0Athe%20environment.%20This%20report%20provides%20a%20summary%20of%20the%20CLVision%20challenge%20at%0ACVPR%202023%2C%20which%20focused%20on%20the%20topic%20of%20repetition%20in%20class-incremental%0Alearning.%20The%20report%20initially%20outlines%20the%20challenge%20objective%20and%20then%0Adescribes%20three%20solutions%20proposed%20by%20finalist%20teams%20that%20aim%20to%20effectively%0Aexploit%20the%20repetition%20in%20the%20stream%20to%20learn%20continually.%20The%20experimental%0Aresults%20from%20the%20challenge%20highlight%20the%20effectiveness%20of%20ensemble-based%0Asolutions%20that%20employ%20multiple%20versions%20of%20similar%20modules%2C%20each%20trained%20on%0Adifferent%20but%20overlapping%20subsets%20of%20classes.%20This%20report%20underscores%20the%0Atransformative%20potential%20of%20taking%20a%20different%20perspective%20in%20CL%20by%20employing%0Arepetition%20in%20the%20data%20stream%20to%20foster%20innovative%20strategy%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04101v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Learning%2520in%2520the%2520Presence%2520of%2520Repetition%26entry.906535625%3DHamed%2520Hemati%2520and%2520Lorenzo%2520Pellegrini%2520and%2520Xiaotian%2520Duan%2520and%2520Zixuan%2520Zhao%2520and%2520Fangfang%2520Xia%2520and%2520Marc%2520Masana%2520and%2520Benedikt%2520Tscheschner%2520and%2520Eduardo%2520Veas%2520and%2520Yuxiang%2520Zheng%2520and%2520Shiji%2520Zhao%2520and%2520Shao-Yuan%2520Li%2520and%2520Sheng-Jun%2520Huang%2520and%2520Vincenzo%2520Lomonaco%2520and%2520Gido%2520M.%2520van%2520de%2520Ven%26entry.1292438233%3D%2520%2520Continual%2520learning%2520%2528CL%2529%2520provides%2520a%2520framework%2520for%2520training%2520models%2520in%250Aever-evolving%2520environments.%2520Although%2520re-occurrence%2520of%2520previously%2520seen%2520objects%250Aor%2520tasks%2520is%2520common%2520in%2520real-world%2520problems%252C%2520the%2520concept%2520of%2520repetition%2520in%2520the%250Adata%2520stream%2520is%2520not%2520often%2520considered%2520in%2520standard%2520benchmarks%2520for%2520CL.%2520Unlike%2520with%250Athe%2520rehearsal%2520mechanism%2520in%2520buffer-based%2520strategies%252C%2520where%2520sample%2520repetition%2520is%250Acontrolled%2520by%2520the%2520strategy%252C%2520repetition%2520in%2520the%2520data%2520stream%2520naturally%2520stems%2520from%250Athe%2520environment.%2520This%2520report%2520provides%2520a%2520summary%2520of%2520the%2520CLVision%2520challenge%2520at%250ACVPR%25202023%252C%2520which%2520focused%2520on%2520the%2520topic%2520of%2520repetition%2520in%2520class-incremental%250Alearning.%2520The%2520report%2520initially%2520outlines%2520the%2520challenge%2520objective%2520and%2520then%250Adescribes%2520three%2520solutions%2520proposed%2520by%2520finalist%2520teams%2520that%2520aim%2520to%2520effectively%250Aexploit%2520the%2520repetition%2520in%2520the%2520stream%2520to%2520learn%2520continually.%2520The%2520experimental%250Aresults%2520from%2520the%2520challenge%2520highlight%2520the%2520effectiveness%2520of%2520ensemble-based%250Asolutions%2520that%2520employ%2520multiple%2520versions%2520of%2520similar%2520modules%252C%2520each%2520trained%2520on%250Adifferent%2520but%2520overlapping%2520subsets%2520of%2520classes.%2520This%2520report%2520underscores%2520the%250Atransformative%2520potential%2520of%2520taking%2520a%2520different%2520perspective%2520in%2520CL%2520by%2520employing%250Arepetition%2520in%2520the%2520data%2520stream%2520to%2520foster%2520innovative%2520strategy%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04101v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%20in%20the%20Presence%20of%20Repetition&entry.906535625=Hamed%20Hemati%20and%20Lorenzo%20Pellegrini%20and%20Xiaotian%20Duan%20and%20Zixuan%20Zhao%20and%20Fangfang%20Xia%20and%20Marc%20Masana%20and%20Benedikt%20Tscheschner%20and%20Eduardo%20Veas%20and%20Yuxiang%20Zheng%20and%20Shiji%20Zhao%20and%20Shao-Yuan%20Li%20and%20Sheng-Jun%20Huang%20and%20Vincenzo%20Lomonaco%20and%20Gido%20M.%20van%20de%20Ven&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20provides%20a%20framework%20for%20training%20models%20in%0Aever-evolving%20environments.%20Although%20re-occurrence%20of%20previously%20seen%20objects%0Aor%20tasks%20is%20common%20in%20real-world%20problems%2C%20the%20concept%20of%20repetition%20in%20the%0Adata%20stream%20is%20not%20often%20considered%20in%20standard%20benchmarks%20for%20CL.%20Unlike%20with%0Athe%20rehearsal%20mechanism%20in%20buffer-based%20strategies%2C%20where%20sample%20repetition%20is%0Acontrolled%20by%20the%20strategy%2C%20repetition%20in%20the%20data%20stream%20naturally%20stems%20from%0Athe%20environment.%20This%20report%20provides%20a%20summary%20of%20the%20CLVision%20challenge%20at%0ACVPR%202023%2C%20which%20focused%20on%20the%20topic%20of%20repetition%20in%20class-incremental%0Alearning.%20The%20report%20initially%20outlines%20the%20challenge%20objective%20and%20then%0Adescribes%20three%20solutions%20proposed%20by%20finalist%20teams%20that%20aim%20to%20effectively%0Aexploit%20the%20repetition%20in%20the%20stream%20to%20learn%20continually.%20The%20experimental%0Aresults%20from%20the%20challenge%20highlight%20the%20effectiveness%20of%20ensemble-based%0Asolutions%20that%20employ%20multiple%20versions%20of%20similar%20modules%2C%20each%20trained%20on%0Adifferent%20but%20overlapping%20subsets%20of%20classes.%20This%20report%20underscores%20the%0Atransformative%20potential%20of%20taking%20a%20different%20perspective%20in%20CL%20by%20employing%0Arepetition%20in%20the%20data%20stream%20to%20foster%20innovative%20strategy%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04101v2&entry.124074799=Read"},
{"title": "Improved Multi-Task Brain Tumour Segmentation with Synthetic Data\n  Augmentation", "author": "Andr\u00e9 Ferreira and Tiago Jesus and Behrus Puladi and Jens Kleesiek and Victor Alves and Jan Egger", "abstract": "  This paper presents the winning solution of task 1 and the third-placed\nsolution of task 3 of the BraTS challenge. The use of automated tools in\nclinical practice has increased due to the development of more and more\nsophisticated and reliable algorithms. However, achieving clinical standards\nand developing tools for real-life scenarios is a major challenge. To this end,\nBraTS has organised tasks to find the most advanced solutions for specific\npurposes. In this paper, we propose the use of synthetic data to train\nstate-of-the-art frameworks in order to improve the segmentation of adult\ngliomas in a post-treatment scenario, and the segmentation of meningioma for\nradiotherapy planning. Our results suggest that the use of synthetic data leads\nto more robust algorithms, although the synthetic data generation pipeline is\nnot directly suited to the meningioma task. In task 1, we achieved a DSC of\n0.7900, 0.8076, 0.7760, 0.8926, 0.7874, 0.8938 and a HD95 of 35.63, 30.35,\n44.58, 16.87, 38.19, 17.95 for ET, NETC, RC, SNFH, TC and WT, respectively and,\nin task 3, we achieved a DSC of 0.801 and HD95 of 38.26, in the testing phase.\nThe code for these tasks is available at\nhttps://github.com/ShadowTwin41/BraTS_2023_2024_solutions.\n", "link": "http://arxiv.org/abs/2411.04632v2", "date": "2024-12-02", "relevancy": 2.5122, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5319}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4929}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Multi-Task%20Brain%20Tumour%20Segmentation%20with%20Synthetic%20Data%0A%20%20Augmentation&body=Title%3A%20Improved%20Multi-Task%20Brain%20Tumour%20Segmentation%20with%20Synthetic%20Data%0A%20%20Augmentation%0AAuthor%3A%20Andr%C3%A9%20Ferreira%20and%20Tiago%20Jesus%20and%20Behrus%20Puladi%20and%20Jens%20Kleesiek%20and%20Victor%20Alves%20and%20Jan%20Egger%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20winning%20solution%20of%20task%201%20and%20the%20third-placed%0Asolution%20of%20task%203%20of%20the%20BraTS%20challenge.%20The%20use%20of%20automated%20tools%20in%0Aclinical%20practice%20has%20increased%20due%20to%20the%20development%20of%20more%20and%20more%0Asophisticated%20and%20reliable%20algorithms.%20However%2C%20achieving%20clinical%20standards%0Aand%20developing%20tools%20for%20real-life%20scenarios%20is%20a%20major%20challenge.%20To%20this%20end%2C%0ABraTS%20has%20organised%20tasks%20to%20find%20the%20most%20advanced%20solutions%20for%20specific%0Apurposes.%20In%20this%20paper%2C%20we%20propose%20the%20use%20of%20synthetic%20data%20to%20train%0Astate-of-the-art%20frameworks%20in%20order%20to%20improve%20the%20segmentation%20of%20adult%0Agliomas%20in%20a%20post-treatment%20scenario%2C%20and%20the%20segmentation%20of%20meningioma%20for%0Aradiotherapy%20planning.%20Our%20results%20suggest%20that%20the%20use%20of%20synthetic%20data%20leads%0Ato%20more%20robust%20algorithms%2C%20although%20the%20synthetic%20data%20generation%20pipeline%20is%0Anot%20directly%20suited%20to%20the%20meningioma%20task.%20In%20task%201%2C%20we%20achieved%20a%20DSC%20of%0A0.7900%2C%200.8076%2C%200.7760%2C%200.8926%2C%200.7874%2C%200.8938%20and%20a%20HD95%20of%2035.63%2C%2030.35%2C%0A44.58%2C%2016.87%2C%2038.19%2C%2017.95%20for%20ET%2C%20NETC%2C%20RC%2C%20SNFH%2C%20TC%20and%20WT%2C%20respectively%20and%2C%0Ain%20task%203%2C%20we%20achieved%20a%20DSC%20of%200.801%20and%20HD95%20of%2038.26%2C%20in%20the%20testing%20phase.%0AThe%20code%20for%20these%20tasks%20is%20available%20at%0Ahttps%3A//github.com/ShadowTwin41/BraTS_2023_2024_solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04632v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Multi-Task%2520Brain%2520Tumour%2520Segmentation%2520with%2520Synthetic%2520Data%250A%2520%2520Augmentation%26entry.906535625%3DAndr%25C3%25A9%2520Ferreira%2520and%2520Tiago%2520Jesus%2520and%2520Behrus%2520Puladi%2520and%2520Jens%2520Kleesiek%2520and%2520Victor%2520Alves%2520and%2520Jan%2520Egger%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520winning%2520solution%2520of%2520task%25201%2520and%2520the%2520third-placed%250Asolution%2520of%2520task%25203%2520of%2520the%2520BraTS%2520challenge.%2520The%2520use%2520of%2520automated%2520tools%2520in%250Aclinical%2520practice%2520has%2520increased%2520due%2520to%2520the%2520development%2520of%2520more%2520and%2520more%250Asophisticated%2520and%2520reliable%2520algorithms.%2520However%252C%2520achieving%2520clinical%2520standards%250Aand%2520developing%2520tools%2520for%2520real-life%2520scenarios%2520is%2520a%2520major%2520challenge.%2520To%2520this%2520end%252C%250ABraTS%2520has%2520organised%2520tasks%2520to%2520find%2520the%2520most%2520advanced%2520solutions%2520for%2520specific%250Apurposes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520use%2520of%2520synthetic%2520data%2520to%2520train%250Astate-of-the-art%2520frameworks%2520in%2520order%2520to%2520improve%2520the%2520segmentation%2520of%2520adult%250Agliomas%2520in%2520a%2520post-treatment%2520scenario%252C%2520and%2520the%2520segmentation%2520of%2520meningioma%2520for%250Aradiotherapy%2520planning.%2520Our%2520results%2520suggest%2520that%2520the%2520use%2520of%2520synthetic%2520data%2520leads%250Ato%2520more%2520robust%2520algorithms%252C%2520although%2520the%2520synthetic%2520data%2520generation%2520pipeline%2520is%250Anot%2520directly%2520suited%2520to%2520the%2520meningioma%2520task.%2520In%2520task%25201%252C%2520we%2520achieved%2520a%2520DSC%2520of%250A0.7900%252C%25200.8076%252C%25200.7760%252C%25200.8926%252C%25200.7874%252C%25200.8938%2520and%2520a%2520HD95%2520of%252035.63%252C%252030.35%252C%250A44.58%252C%252016.87%252C%252038.19%252C%252017.95%2520for%2520ET%252C%2520NETC%252C%2520RC%252C%2520SNFH%252C%2520TC%2520and%2520WT%252C%2520respectively%2520and%252C%250Ain%2520task%25203%252C%2520we%2520achieved%2520a%2520DSC%2520of%25200.801%2520and%2520HD95%2520of%252038.26%252C%2520in%2520the%2520testing%2520phase.%250AThe%2520code%2520for%2520these%2520tasks%2520is%2520available%2520at%250Ahttps%253A//github.com/ShadowTwin41/BraTS_2023_2024_solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04632v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Multi-Task%20Brain%20Tumour%20Segmentation%20with%20Synthetic%20Data%0A%20%20Augmentation&entry.906535625=Andr%C3%A9%20Ferreira%20and%20Tiago%20Jesus%20and%20Behrus%20Puladi%20and%20Jens%20Kleesiek%20and%20Victor%20Alves%20and%20Jan%20Egger&entry.1292438233=%20%20This%20paper%20presents%20the%20winning%20solution%20of%20task%201%20and%20the%20third-placed%0Asolution%20of%20task%203%20of%20the%20BraTS%20challenge.%20The%20use%20of%20automated%20tools%20in%0Aclinical%20practice%20has%20increased%20due%20to%20the%20development%20of%20more%20and%20more%0Asophisticated%20and%20reliable%20algorithms.%20However%2C%20achieving%20clinical%20standards%0Aand%20developing%20tools%20for%20real-life%20scenarios%20is%20a%20major%20challenge.%20To%20this%20end%2C%0ABraTS%20has%20organised%20tasks%20to%20find%20the%20most%20advanced%20solutions%20for%20specific%0Apurposes.%20In%20this%20paper%2C%20we%20propose%20the%20use%20of%20synthetic%20data%20to%20train%0Astate-of-the-art%20frameworks%20in%20order%20to%20improve%20the%20segmentation%20of%20adult%0Agliomas%20in%20a%20post-treatment%20scenario%2C%20and%20the%20segmentation%20of%20meningioma%20for%0Aradiotherapy%20planning.%20Our%20results%20suggest%20that%20the%20use%20of%20synthetic%20data%20leads%0Ato%20more%20robust%20algorithms%2C%20although%20the%20synthetic%20data%20generation%20pipeline%20is%0Anot%20directly%20suited%20to%20the%20meningioma%20task.%20In%20task%201%2C%20we%20achieved%20a%20DSC%20of%0A0.7900%2C%200.8076%2C%200.7760%2C%200.8926%2C%200.7874%2C%200.8938%20and%20a%20HD95%20of%2035.63%2C%2030.35%2C%0A44.58%2C%2016.87%2C%2038.19%2C%2017.95%20for%20ET%2C%20NETC%2C%20RC%2C%20SNFH%2C%20TC%20and%20WT%2C%20respectively%20and%2C%0Ain%20task%203%2C%20we%20achieved%20a%20DSC%20of%200.801%20and%20HD95%20of%2038.26%2C%20in%20the%20testing%20phase.%0AThe%20code%20for%20these%20tasks%20is%20available%20at%0Ahttps%3A//github.com/ShadowTwin41/BraTS_2023_2024_solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04632v2&entry.124074799=Read"},
{"title": "From Text to Insight: Large Language Models for Materials Science Data\n  Extraction", "author": "Mara Schilling-Wilhelmi and Marti\u00f1o R\u00edos-Garc\u00eda and Sherjeel Shabih and Mar\u00eda Victoria Gil and Santiago Miret and Christoph T. Koch and Jos\u00e9 A. M\u00e1rquez and Kevin Maik Jablonka", "abstract": "  The vast majority of materials science knowledge exists in unstructured\nnatural language, yet structured data is crucial for innovative and systematic\nmaterials design. Traditionally, the field has relied on manual curation and\npartial automation for data extraction for specific use cases. The advent of\nlarge language models (LLMs) represents a significant shift, potentially\nenabling efficient extraction of structured, actionable data from unstructured\ntext by non-experts. While applying LLMs to materials science data extraction\npresents unique challenges, domain knowledge offers opportunities to guide and\nvalidate LLM outputs. This review provides a comprehensive overview of\nLLM-based structured data extraction in materials science, synthesizing current\nknowledge and outlining future directions. We address the lack of standardized\nguidelines and present frameworks for leveraging the synergy between LLMs and\nmaterials science expertise. This work serves as a foundational resource for\nresearchers aiming to harness LLMs for data-driven materials research. The\ninsights presented here could significantly enhance how researchers across\ndisciplines access and utilize scientific information, potentially accelerating\nthe development of novel materials for critical societal needs.\n", "link": "http://arxiv.org/abs/2407.16867v2", "date": "2024-12-02", "relevancy": 2.4581, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5019}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5019}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Text%20to%20Insight%3A%20Large%20Language%20Models%20for%20Materials%20Science%20Data%0A%20%20Extraction&body=Title%3A%20From%20Text%20to%20Insight%3A%20Large%20Language%20Models%20for%20Materials%20Science%20Data%0A%20%20Extraction%0AAuthor%3A%20Mara%20Schilling-Wilhelmi%20and%20Marti%C3%B1o%20R%C3%ADos-Garc%C3%ADa%20and%20Sherjeel%20Shabih%20and%20Mar%C3%ADa%20Victoria%20Gil%20and%20Santiago%20Miret%20and%20Christoph%20T.%20Koch%20and%20Jos%C3%A9%20A.%20M%C3%A1rquez%20and%20Kevin%20Maik%20Jablonka%0AAbstract%3A%20%20%20The%20vast%20majority%20of%20materials%20science%20knowledge%20exists%20in%20unstructured%0Anatural%20language%2C%20yet%20structured%20data%20is%20crucial%20for%20innovative%20and%20systematic%0Amaterials%20design.%20Traditionally%2C%20the%20field%20has%20relied%20on%20manual%20curation%20and%0Apartial%20automation%20for%20data%20extraction%20for%20specific%20use%20cases.%20The%20advent%20of%0Alarge%20language%20models%20%28LLMs%29%20represents%20a%20significant%20shift%2C%20potentially%0Aenabling%20efficient%20extraction%20of%20structured%2C%20actionable%20data%20from%20unstructured%0Atext%20by%20non-experts.%20While%20applying%20LLMs%20to%20materials%20science%20data%20extraction%0Apresents%20unique%20challenges%2C%20domain%20knowledge%20offers%20opportunities%20to%20guide%20and%0Avalidate%20LLM%20outputs.%20This%20review%20provides%20a%20comprehensive%20overview%20of%0ALLM-based%20structured%20data%20extraction%20in%20materials%20science%2C%20synthesizing%20current%0Aknowledge%20and%20outlining%20future%20directions.%20We%20address%20the%20lack%20of%20standardized%0Aguidelines%20and%20present%20frameworks%20for%20leveraging%20the%20synergy%20between%20LLMs%20and%0Amaterials%20science%20expertise.%20This%20work%20serves%20as%20a%20foundational%20resource%20for%0Aresearchers%20aiming%20to%20harness%20LLMs%20for%20data-driven%20materials%20research.%20The%0Ainsights%20presented%20here%20could%20significantly%20enhance%20how%20researchers%20across%0Adisciplines%20access%20and%20utilize%20scientific%20information%2C%20potentially%20accelerating%0Athe%20development%20of%20novel%20materials%20for%20critical%20societal%20needs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16867v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Text%2520to%2520Insight%253A%2520Large%2520Language%2520Models%2520for%2520Materials%2520Science%2520Data%250A%2520%2520Extraction%26entry.906535625%3DMara%2520Schilling-Wilhelmi%2520and%2520Marti%25C3%25B1o%2520R%25C3%25ADos-Garc%25C3%25ADa%2520and%2520Sherjeel%2520Shabih%2520and%2520Mar%25C3%25ADa%2520Victoria%2520Gil%2520and%2520Santiago%2520Miret%2520and%2520Christoph%2520T.%2520Koch%2520and%2520Jos%25C3%25A9%2520A.%2520M%25C3%25A1rquez%2520and%2520Kevin%2520Maik%2520Jablonka%26entry.1292438233%3D%2520%2520The%2520vast%2520majority%2520of%2520materials%2520science%2520knowledge%2520exists%2520in%2520unstructured%250Anatural%2520language%252C%2520yet%2520structured%2520data%2520is%2520crucial%2520for%2520innovative%2520and%2520systematic%250Amaterials%2520design.%2520Traditionally%252C%2520the%2520field%2520has%2520relied%2520on%2520manual%2520curation%2520and%250Apartial%2520automation%2520for%2520data%2520extraction%2520for%2520specific%2520use%2520cases.%2520The%2520advent%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520represents%2520a%2520significant%2520shift%252C%2520potentially%250Aenabling%2520efficient%2520extraction%2520of%2520structured%252C%2520actionable%2520data%2520from%2520unstructured%250Atext%2520by%2520non-experts.%2520While%2520applying%2520LLMs%2520to%2520materials%2520science%2520data%2520extraction%250Apresents%2520unique%2520challenges%252C%2520domain%2520knowledge%2520offers%2520opportunities%2520to%2520guide%2520and%250Avalidate%2520LLM%2520outputs.%2520This%2520review%2520provides%2520a%2520comprehensive%2520overview%2520of%250ALLM-based%2520structured%2520data%2520extraction%2520in%2520materials%2520science%252C%2520synthesizing%2520current%250Aknowledge%2520and%2520outlining%2520future%2520directions.%2520We%2520address%2520the%2520lack%2520of%2520standardized%250Aguidelines%2520and%2520present%2520frameworks%2520for%2520leveraging%2520the%2520synergy%2520between%2520LLMs%2520and%250Amaterials%2520science%2520expertise.%2520This%2520work%2520serves%2520as%2520a%2520foundational%2520resource%2520for%250Aresearchers%2520aiming%2520to%2520harness%2520LLMs%2520for%2520data-driven%2520materials%2520research.%2520The%250Ainsights%2520presented%2520here%2520could%2520significantly%2520enhance%2520how%2520researchers%2520across%250Adisciplines%2520access%2520and%2520utilize%2520scientific%2520information%252C%2520potentially%2520accelerating%250Athe%2520development%2520of%2520novel%2520materials%2520for%2520critical%2520societal%2520needs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16867v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Text%20to%20Insight%3A%20Large%20Language%20Models%20for%20Materials%20Science%20Data%0A%20%20Extraction&entry.906535625=Mara%20Schilling-Wilhelmi%20and%20Marti%C3%B1o%20R%C3%ADos-Garc%C3%ADa%20and%20Sherjeel%20Shabih%20and%20Mar%C3%ADa%20Victoria%20Gil%20and%20Santiago%20Miret%20and%20Christoph%20T.%20Koch%20and%20Jos%C3%A9%20A.%20M%C3%A1rquez%20and%20Kevin%20Maik%20Jablonka&entry.1292438233=%20%20The%20vast%20majority%20of%20materials%20science%20knowledge%20exists%20in%20unstructured%0Anatural%20language%2C%20yet%20structured%20data%20is%20crucial%20for%20innovative%20and%20systematic%0Amaterials%20design.%20Traditionally%2C%20the%20field%20has%20relied%20on%20manual%20curation%20and%0Apartial%20automation%20for%20data%20extraction%20for%20specific%20use%20cases.%20The%20advent%20of%0Alarge%20language%20models%20%28LLMs%29%20represents%20a%20significant%20shift%2C%20potentially%0Aenabling%20efficient%20extraction%20of%20structured%2C%20actionable%20data%20from%20unstructured%0Atext%20by%20non-experts.%20While%20applying%20LLMs%20to%20materials%20science%20data%20extraction%0Apresents%20unique%20challenges%2C%20domain%20knowledge%20offers%20opportunities%20to%20guide%20and%0Avalidate%20LLM%20outputs.%20This%20review%20provides%20a%20comprehensive%20overview%20of%0ALLM-based%20structured%20data%20extraction%20in%20materials%20science%2C%20synthesizing%20current%0Aknowledge%20and%20outlining%20future%20directions.%20We%20address%20the%20lack%20of%20standardized%0Aguidelines%20and%20present%20frameworks%20for%20leveraging%20the%20synergy%20between%20LLMs%20and%0Amaterials%20science%20expertise.%20This%20work%20serves%20as%20a%20foundational%20resource%20for%0Aresearchers%20aiming%20to%20harness%20LLMs%20for%20data-driven%20materials%20research.%20The%0Ainsights%20presented%20here%20could%20significantly%20enhance%20how%20researchers%20across%0Adisciplines%20access%20and%20utilize%20scientific%20information%2C%20potentially%20accelerating%0Athe%20development%20of%20novel%20materials%20for%20critical%20societal%20needs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16867v2&entry.124074799=Read"},
{"title": "Deepfake for the Good: Generating Avatars through Face-Swapping with\n  Implicit Deepfake Generation", "author": "Georgii Stanishevskii and Jakub Steczkiewicz and Tomasz Szczepanik and S\u0142awomir Tadeja and Jacek Tabor and Przemys\u0142aw Spurek", "abstract": "  Numerous emerging deep-learning techniques have had a substantial impact on\ncomputer graphics. Among the most promising breakthroughs are the rise of\nNeural Radiance Fields (NeRFs) and Gaussian Splatting (GS). NeRFs encode the\nobject's shape and color in neural network weights using a handful of images\nwith known camera positions to generate novel views. In contrast, GS provides\naccelerated training and inference without a decrease in rendering quality by\nencoding the object's characteristics in a collection of Gaussian\ndistributions. These two techniques have found many use cases in spatial\ncomputing and other domains. On the other hand, the emergence of deepfake\nmethods has sparked considerable controversy. Deepfakes refers to artificial\nintelligence-generated videos that closely mimic authentic footage. Using\ngenerative models, they can modify facial features, enabling the creation of\naltered identities or expressions that exhibit a remarkably realistic\nappearance to a real person. Despite these controversies, deepfake can offer a\nnext-generation solution for avatar creation and gaming when of desirable\nquality. To that end, we show how to combine all these emerging technologies to\nobtain a more plausible outcome. Our ImplicitDeepfake uses the classical\ndeepfake algorithm to modify all training images separately and then train NeRF\nand GS on modified faces. Such simple strategies can produce plausible 3D\ndeepfake-based avatars.\n", "link": "http://arxiv.org/abs/2402.06390v2", "date": "2024-12-02", "relevancy": 2.4572, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6202}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6202}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deepfake%20for%20the%20Good%3A%20Generating%20Avatars%20through%20Face-Swapping%20with%0A%20%20Implicit%20Deepfake%20Generation&body=Title%3A%20Deepfake%20for%20the%20Good%3A%20Generating%20Avatars%20through%20Face-Swapping%20with%0A%20%20Implicit%20Deepfake%20Generation%0AAuthor%3A%20Georgii%20Stanishevskii%20and%20Jakub%20Steczkiewicz%20and%20Tomasz%20Szczepanik%20and%20S%C5%82awomir%20Tadeja%20and%20Jacek%20Tabor%20and%20Przemys%C5%82aw%20Spurek%0AAbstract%3A%20%20%20Numerous%20emerging%20deep-learning%20techniques%20have%20had%20a%20substantial%20impact%20on%0Acomputer%20graphics.%20Among%20the%20most%20promising%20breakthroughs%20are%20the%20rise%20of%0ANeural%20Radiance%20Fields%20%28NeRFs%29%20and%20Gaussian%20Splatting%20%28GS%29.%20NeRFs%20encode%20the%0Aobject%27s%20shape%20and%20color%20in%20neural%20network%20weights%20using%20a%20handful%20of%20images%0Awith%20known%20camera%20positions%20to%20generate%20novel%20views.%20In%20contrast%2C%20GS%20provides%0Aaccelerated%20training%20and%20inference%20without%20a%20decrease%20in%20rendering%20quality%20by%0Aencoding%20the%20object%27s%20characteristics%20in%20a%20collection%20of%20Gaussian%0Adistributions.%20These%20two%20techniques%20have%20found%20many%20use%20cases%20in%20spatial%0Acomputing%20and%20other%20domains.%20On%20the%20other%20hand%2C%20the%20emergence%20of%20deepfake%0Amethods%20has%20sparked%20considerable%20controversy.%20Deepfakes%20refers%20to%20artificial%0Aintelligence-generated%20videos%20that%20closely%20mimic%20authentic%20footage.%20Using%0Agenerative%20models%2C%20they%20can%20modify%20facial%20features%2C%20enabling%20the%20creation%20of%0Aaltered%20identities%20or%20expressions%20that%20exhibit%20a%20remarkably%20realistic%0Aappearance%20to%20a%20real%20person.%20Despite%20these%20controversies%2C%20deepfake%20can%20offer%20a%0Anext-generation%20solution%20for%20avatar%20creation%20and%20gaming%20when%20of%20desirable%0Aquality.%20To%20that%20end%2C%20we%20show%20how%20to%20combine%20all%20these%20emerging%20technologies%20to%0Aobtain%20a%20more%20plausible%20outcome.%20Our%20ImplicitDeepfake%20uses%20the%20classical%0Adeepfake%20algorithm%20to%20modify%20all%20training%20images%20separately%20and%20then%20train%20NeRF%0Aand%20GS%20on%20modified%20faces.%20Such%20simple%20strategies%20can%20produce%20plausible%203D%0Adeepfake-based%20avatars.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06390v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepfake%2520for%2520the%2520Good%253A%2520Generating%2520Avatars%2520through%2520Face-Swapping%2520with%250A%2520%2520Implicit%2520Deepfake%2520Generation%26entry.906535625%3DGeorgii%2520Stanishevskii%2520and%2520Jakub%2520Steczkiewicz%2520and%2520Tomasz%2520Szczepanik%2520and%2520S%25C5%2582awomir%2520Tadeja%2520and%2520Jacek%2520Tabor%2520and%2520Przemys%25C5%2582aw%2520Spurek%26entry.1292438233%3D%2520%2520Numerous%2520emerging%2520deep-learning%2520techniques%2520have%2520had%2520a%2520substantial%2520impact%2520on%250Acomputer%2520graphics.%2520Among%2520the%2520most%2520promising%2520breakthroughs%2520are%2520the%2520rise%2520of%250ANeural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520and%2520Gaussian%2520Splatting%2520%2528GS%2529.%2520NeRFs%2520encode%2520the%250Aobject%2527s%2520shape%2520and%2520color%2520in%2520neural%2520network%2520weights%2520using%2520a%2520handful%2520of%2520images%250Awith%2520known%2520camera%2520positions%2520to%2520generate%2520novel%2520views.%2520In%2520contrast%252C%2520GS%2520provides%250Aaccelerated%2520training%2520and%2520inference%2520without%2520a%2520decrease%2520in%2520rendering%2520quality%2520by%250Aencoding%2520the%2520object%2527s%2520characteristics%2520in%2520a%2520collection%2520of%2520Gaussian%250Adistributions.%2520These%2520two%2520techniques%2520have%2520found%2520many%2520use%2520cases%2520in%2520spatial%250Acomputing%2520and%2520other%2520domains.%2520On%2520the%2520other%2520hand%252C%2520the%2520emergence%2520of%2520deepfake%250Amethods%2520has%2520sparked%2520considerable%2520controversy.%2520Deepfakes%2520refers%2520to%2520artificial%250Aintelligence-generated%2520videos%2520that%2520closely%2520mimic%2520authentic%2520footage.%2520Using%250Agenerative%2520models%252C%2520they%2520can%2520modify%2520facial%2520features%252C%2520enabling%2520the%2520creation%2520of%250Aaltered%2520identities%2520or%2520expressions%2520that%2520exhibit%2520a%2520remarkably%2520realistic%250Aappearance%2520to%2520a%2520real%2520person.%2520Despite%2520these%2520controversies%252C%2520deepfake%2520can%2520offer%2520a%250Anext-generation%2520solution%2520for%2520avatar%2520creation%2520and%2520gaming%2520when%2520of%2520desirable%250Aquality.%2520To%2520that%2520end%252C%2520we%2520show%2520how%2520to%2520combine%2520all%2520these%2520emerging%2520technologies%2520to%250Aobtain%2520a%2520more%2520plausible%2520outcome.%2520Our%2520ImplicitDeepfake%2520uses%2520the%2520classical%250Adeepfake%2520algorithm%2520to%2520modify%2520all%2520training%2520images%2520separately%2520and%2520then%2520train%2520NeRF%250Aand%2520GS%2520on%2520modified%2520faces.%2520Such%2520simple%2520strategies%2520can%2520produce%2520plausible%25203D%250Adeepfake-based%2520avatars.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.06390v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deepfake%20for%20the%20Good%3A%20Generating%20Avatars%20through%20Face-Swapping%20with%0A%20%20Implicit%20Deepfake%20Generation&entry.906535625=Georgii%20Stanishevskii%20and%20Jakub%20Steczkiewicz%20and%20Tomasz%20Szczepanik%20and%20S%C5%82awomir%20Tadeja%20and%20Jacek%20Tabor%20and%20Przemys%C5%82aw%20Spurek&entry.1292438233=%20%20Numerous%20emerging%20deep-learning%20techniques%20have%20had%20a%20substantial%20impact%20on%0Acomputer%20graphics.%20Among%20the%20most%20promising%20breakthroughs%20are%20the%20rise%20of%0ANeural%20Radiance%20Fields%20%28NeRFs%29%20and%20Gaussian%20Splatting%20%28GS%29.%20NeRFs%20encode%20the%0Aobject%27s%20shape%20and%20color%20in%20neural%20network%20weights%20using%20a%20handful%20of%20images%0Awith%20known%20camera%20positions%20to%20generate%20novel%20views.%20In%20contrast%2C%20GS%20provides%0Aaccelerated%20training%20and%20inference%20without%20a%20decrease%20in%20rendering%20quality%20by%0Aencoding%20the%20object%27s%20characteristics%20in%20a%20collection%20of%20Gaussian%0Adistributions.%20These%20two%20techniques%20have%20found%20many%20use%20cases%20in%20spatial%0Acomputing%20and%20other%20domains.%20On%20the%20other%20hand%2C%20the%20emergence%20of%20deepfake%0Amethods%20has%20sparked%20considerable%20controversy.%20Deepfakes%20refers%20to%20artificial%0Aintelligence-generated%20videos%20that%20closely%20mimic%20authentic%20footage.%20Using%0Agenerative%20models%2C%20they%20can%20modify%20facial%20features%2C%20enabling%20the%20creation%20of%0Aaltered%20identities%20or%20expressions%20that%20exhibit%20a%20remarkably%20realistic%0Aappearance%20to%20a%20real%20person.%20Despite%20these%20controversies%2C%20deepfake%20can%20offer%20a%0Anext-generation%20solution%20for%20avatar%20creation%20and%20gaming%20when%20of%20desirable%0Aquality.%20To%20that%20end%2C%20we%20show%20how%20to%20combine%20all%20these%20emerging%20technologies%20to%0Aobtain%20a%20more%20plausible%20outcome.%20Our%20ImplicitDeepfake%20uses%20the%20classical%0Adeepfake%20algorithm%20to%20modify%20all%20training%20images%20separately%20and%20then%20train%20NeRF%0Aand%20GS%20on%20modified%20faces.%20Such%20simple%20strategies%20can%20produce%20plausible%203D%0Adeepfake-based%20avatars.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06390v2&entry.124074799=Read"},
{"title": "Multimodal Perception System for Real Open Environment", "author": "Yuyang Sha", "abstract": "  This paper presents a novel multimodal perception system for a real open\nenvironment. The proposed system includes an embedded computation platform,\ncameras, ultrasonic sensors, GPS, and IMU devices. Unlike the traditional\nframeworks, our system integrates multiple sensors with advanced computer\nvision algorithms to help users walk outside reliably. The system can\nefficiently complete various tasks, including navigating to specific locations,\npassing through obstacle regions, and crossing intersections. Specifically, we\nalso use ultrasonic sensors and depth cameras to enhance obstacle avoidance\nperformance. The path planning module is designed to find the locally optimal\nroute based on various feedback and the user's current state. To evaluate the\nperformance of the proposed system, we design several experiments under\ndifferent scenarios. The results show that the system can help users walk\nefficiently and independently in complex situations.\n", "link": "http://arxiv.org/abs/2410.07926v2", "date": "2024-12-02", "relevancy": 2.4071, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6375}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5984}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Perception%20System%20for%20Real%20Open%20Environment&body=Title%3A%20Multimodal%20Perception%20System%20for%20Real%20Open%20Environment%0AAuthor%3A%20Yuyang%20Sha%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20multimodal%20perception%20system%20for%20a%20real%20open%0Aenvironment.%20The%20proposed%20system%20includes%20an%20embedded%20computation%20platform%2C%0Acameras%2C%20ultrasonic%20sensors%2C%20GPS%2C%20and%20IMU%20devices.%20Unlike%20the%20traditional%0Aframeworks%2C%20our%20system%20integrates%20multiple%20sensors%20with%20advanced%20computer%0Avision%20algorithms%20to%20help%20users%20walk%20outside%20reliably.%20The%20system%20can%0Aefficiently%20complete%20various%20tasks%2C%20including%20navigating%20to%20specific%20locations%2C%0Apassing%20through%20obstacle%20regions%2C%20and%20crossing%20intersections.%20Specifically%2C%20we%0Aalso%20use%20ultrasonic%20sensors%20and%20depth%20cameras%20to%20enhance%20obstacle%20avoidance%0Aperformance.%20The%20path%20planning%20module%20is%20designed%20to%20find%20the%20locally%20optimal%0Aroute%20based%20on%20various%20feedback%20and%20the%20user%27s%20current%20state.%20To%20evaluate%20the%0Aperformance%20of%20the%20proposed%20system%2C%20we%20design%20several%20experiments%20under%0Adifferent%20scenarios.%20The%20results%20show%20that%20the%20system%20can%20help%20users%20walk%0Aefficiently%20and%20independently%20in%20complex%20situations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07926v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Perception%2520System%2520for%2520Real%2520Open%2520Environment%26entry.906535625%3DYuyang%2520Sha%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520multimodal%2520perception%2520system%2520for%2520a%2520real%2520open%250Aenvironment.%2520The%2520proposed%2520system%2520includes%2520an%2520embedded%2520computation%2520platform%252C%250Acameras%252C%2520ultrasonic%2520sensors%252C%2520GPS%252C%2520and%2520IMU%2520devices.%2520Unlike%2520the%2520traditional%250Aframeworks%252C%2520our%2520system%2520integrates%2520multiple%2520sensors%2520with%2520advanced%2520computer%250Avision%2520algorithms%2520to%2520help%2520users%2520walk%2520outside%2520reliably.%2520The%2520system%2520can%250Aefficiently%2520complete%2520various%2520tasks%252C%2520including%2520navigating%2520to%2520specific%2520locations%252C%250Apassing%2520through%2520obstacle%2520regions%252C%2520and%2520crossing%2520intersections.%2520Specifically%252C%2520we%250Aalso%2520use%2520ultrasonic%2520sensors%2520and%2520depth%2520cameras%2520to%2520enhance%2520obstacle%2520avoidance%250Aperformance.%2520The%2520path%2520planning%2520module%2520is%2520designed%2520to%2520find%2520the%2520locally%2520optimal%250Aroute%2520based%2520on%2520various%2520feedback%2520and%2520the%2520user%2527s%2520current%2520state.%2520To%2520evaluate%2520the%250Aperformance%2520of%2520the%2520proposed%2520system%252C%2520we%2520design%2520several%2520experiments%2520under%250Adifferent%2520scenarios.%2520The%2520results%2520show%2520that%2520the%2520system%2520can%2520help%2520users%2520walk%250Aefficiently%2520and%2520independently%2520in%2520complex%2520situations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07926v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Perception%20System%20for%20Real%20Open%20Environment&entry.906535625=Yuyang%20Sha&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20multimodal%20perception%20system%20for%20a%20real%20open%0Aenvironment.%20The%20proposed%20system%20includes%20an%20embedded%20computation%20platform%2C%0Acameras%2C%20ultrasonic%20sensors%2C%20GPS%2C%20and%20IMU%20devices.%20Unlike%20the%20traditional%0Aframeworks%2C%20our%20system%20integrates%20multiple%20sensors%20with%20advanced%20computer%0Avision%20algorithms%20to%20help%20users%20walk%20outside%20reliably.%20The%20system%20can%0Aefficiently%20complete%20various%20tasks%2C%20including%20navigating%20to%20specific%20locations%2C%0Apassing%20through%20obstacle%20regions%2C%20and%20crossing%20intersections.%20Specifically%2C%20we%0Aalso%20use%20ultrasonic%20sensors%20and%20depth%20cameras%20to%20enhance%20obstacle%20avoidance%0Aperformance.%20The%20path%20planning%20module%20is%20designed%20to%20find%20the%20locally%20optimal%0Aroute%20based%20on%20various%20feedback%20and%20the%20user%27s%20current%20state.%20To%20evaluate%20the%0Aperformance%20of%20the%20proposed%20system%2C%20we%20design%20several%20experiments%20under%0Adifferent%20scenarios.%20The%20results%20show%20that%20the%20system%20can%20help%20users%20walk%0Aefficiently%20and%20independently%20in%20complex%20situations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07926v2&entry.124074799=Read"},
{"title": "DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with\n  Diffusion Autoencoder", "author": "Chenpeng Du and Qi Chen and Tianyu He and Xu Tan and Xie Chen and Kai Yu and Sheng Zhao and Jiang Bian", "abstract": "  While recent research has made significant progress in speech-driven talking\nface generation, the quality of the generated video still lags behind that of\nreal recordings. One reason for this is the use of handcrafted intermediate\nrepresentations like facial landmarks and 3DMM coefficients, which are designed\nbased on human knowledge and are insufficient to precisely describe facial\nmovements. Additionally, these methods require an external pretrained model for\nextracting these representations, whose performance sets an upper bound on\ntalking face generation. To address these limitations, we propose a novel\nmethod called DAE-Talker that leverages data-driven latent representations\nobtained from a diffusion autoencoder (DAE). DAE contains an image encoder that\nencodes an image into a latent vector and a DDIM image decoder that\nreconstructs the image from it. We train our DAE on talking face video frames\nand then extract their latent representations as the training target for a\nConformer-based speech2latent model. This allows DAE-Talker to synthesize full\nvideo frames and produce natural head movements that align with the content of\nspeech, rather than relying on a predetermined head pose from a template video.\nWe also introduce pose modelling in speech2latent for pose controllability.\nAdditionally, we propose a novel method for generating continuous video frames\nwith the DDIM image decoder trained on individual frames, eliminating the need\nfor modelling the joint distribution of consecutive frames directly. Our\nexperiments show that DAE-Talker outperforms existing popular methods in\nlip-sync, video fidelity, and pose naturalness. We also conduct ablation\nstudies to analyze the effectiveness of the proposed techniques and demonstrate\nthe pose controllability of DAE-Talker.\n", "link": "http://arxiv.org/abs/2303.17550v6", "date": "2024-12-02", "relevancy": 2.3973, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6196}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6066}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DAE-Talker%3A%20High%20Fidelity%20Speech-Driven%20Talking%20Face%20Generation%20with%0A%20%20Diffusion%20Autoencoder&body=Title%3A%20DAE-Talker%3A%20High%20Fidelity%20Speech-Driven%20Talking%20Face%20Generation%20with%0A%20%20Diffusion%20Autoencoder%0AAuthor%3A%20Chenpeng%20Du%20and%20Qi%20Chen%20and%20Tianyu%20He%20and%20Xu%20Tan%20and%20Xie%20Chen%20and%20Kai%20Yu%20and%20Sheng%20Zhao%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20While%20recent%20research%20has%20made%20significant%20progress%20in%20speech-driven%20talking%0Aface%20generation%2C%20the%20quality%20of%20the%20generated%20video%20still%20lags%20behind%20that%20of%0Areal%20recordings.%20One%20reason%20for%20this%20is%20the%20use%20of%20handcrafted%20intermediate%0Arepresentations%20like%20facial%20landmarks%20and%203DMM%20coefficients%2C%20which%20are%20designed%0Abased%20on%20human%20knowledge%20and%20are%20insufficient%20to%20precisely%20describe%20facial%0Amovements.%20Additionally%2C%20these%20methods%20require%20an%20external%20pretrained%20model%20for%0Aextracting%20these%20representations%2C%20whose%20performance%20sets%20an%20upper%20bound%20on%0Atalking%20face%20generation.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0Amethod%20called%20DAE-Talker%20that%20leverages%20data-driven%20latent%20representations%0Aobtained%20from%20a%20diffusion%20autoencoder%20%28DAE%29.%20DAE%20contains%20an%20image%20encoder%20that%0Aencodes%20an%20image%20into%20a%20latent%20vector%20and%20a%20DDIM%20image%20decoder%20that%0Areconstructs%20the%20image%20from%20it.%20We%20train%20our%20DAE%20on%20talking%20face%20video%20frames%0Aand%20then%20extract%20their%20latent%20representations%20as%20the%20training%20target%20for%20a%0AConformer-based%20speech2latent%20model.%20This%20allows%20DAE-Talker%20to%20synthesize%20full%0Avideo%20frames%20and%20produce%20natural%20head%20movements%20that%20align%20with%20the%20content%20of%0Aspeech%2C%20rather%20than%20relying%20on%20a%20predetermined%20head%20pose%20from%20a%20template%20video.%0AWe%20also%20introduce%20pose%20modelling%20in%20speech2latent%20for%20pose%20controllability.%0AAdditionally%2C%20we%20propose%20a%20novel%20method%20for%20generating%20continuous%20video%20frames%0Awith%20the%20DDIM%20image%20decoder%20trained%20on%20individual%20frames%2C%20eliminating%20the%20need%0Afor%20modelling%20the%20joint%20distribution%20of%20consecutive%20frames%20directly.%20Our%0Aexperiments%20show%20that%20DAE-Talker%20outperforms%20existing%20popular%20methods%20in%0Alip-sync%2C%20video%20fidelity%2C%20and%20pose%20naturalness.%20We%20also%20conduct%20ablation%0Astudies%20to%20analyze%20the%20effectiveness%20of%20the%20proposed%20techniques%20and%20demonstrate%0Athe%20pose%20controllability%20of%20DAE-Talker.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.17550v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDAE-Talker%253A%2520High%2520Fidelity%2520Speech-Driven%2520Talking%2520Face%2520Generation%2520with%250A%2520%2520Diffusion%2520Autoencoder%26entry.906535625%3DChenpeng%2520Du%2520and%2520Qi%2520Chen%2520and%2520Tianyu%2520He%2520and%2520Xu%2520Tan%2520and%2520Xie%2520Chen%2520and%2520Kai%2520Yu%2520and%2520Sheng%2520Zhao%2520and%2520Jiang%2520Bian%26entry.1292438233%3D%2520%2520While%2520recent%2520research%2520has%2520made%2520significant%2520progress%2520in%2520speech-driven%2520talking%250Aface%2520generation%252C%2520the%2520quality%2520of%2520the%2520generated%2520video%2520still%2520lags%2520behind%2520that%2520of%250Areal%2520recordings.%2520One%2520reason%2520for%2520this%2520is%2520the%2520use%2520of%2520handcrafted%2520intermediate%250Arepresentations%2520like%2520facial%2520landmarks%2520and%25203DMM%2520coefficients%252C%2520which%2520are%2520designed%250Abased%2520on%2520human%2520knowledge%2520and%2520are%2520insufficient%2520to%2520precisely%2520describe%2520facial%250Amovements.%2520Additionally%252C%2520these%2520methods%2520require%2520an%2520external%2520pretrained%2520model%2520for%250Aextracting%2520these%2520representations%252C%2520whose%2520performance%2520sets%2520an%2520upper%2520bound%2520on%250Atalking%2520face%2520generation.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%250Amethod%2520called%2520DAE-Talker%2520that%2520leverages%2520data-driven%2520latent%2520representations%250Aobtained%2520from%2520a%2520diffusion%2520autoencoder%2520%2528DAE%2529.%2520DAE%2520contains%2520an%2520image%2520encoder%2520that%250Aencodes%2520an%2520image%2520into%2520a%2520latent%2520vector%2520and%2520a%2520DDIM%2520image%2520decoder%2520that%250Areconstructs%2520the%2520image%2520from%2520it.%2520We%2520train%2520our%2520DAE%2520on%2520talking%2520face%2520video%2520frames%250Aand%2520then%2520extract%2520their%2520latent%2520representations%2520as%2520the%2520training%2520target%2520for%2520a%250AConformer-based%2520speech2latent%2520model.%2520This%2520allows%2520DAE-Talker%2520to%2520synthesize%2520full%250Avideo%2520frames%2520and%2520produce%2520natural%2520head%2520movements%2520that%2520align%2520with%2520the%2520content%2520of%250Aspeech%252C%2520rather%2520than%2520relying%2520on%2520a%2520predetermined%2520head%2520pose%2520from%2520a%2520template%2520video.%250AWe%2520also%2520introduce%2520pose%2520modelling%2520in%2520speech2latent%2520for%2520pose%2520controllability.%250AAdditionally%252C%2520we%2520propose%2520a%2520novel%2520method%2520for%2520generating%2520continuous%2520video%2520frames%250Awith%2520the%2520DDIM%2520image%2520decoder%2520trained%2520on%2520individual%2520frames%252C%2520eliminating%2520the%2520need%250Afor%2520modelling%2520the%2520joint%2520distribution%2520of%2520consecutive%2520frames%2520directly.%2520Our%250Aexperiments%2520show%2520that%2520DAE-Talker%2520outperforms%2520existing%2520popular%2520methods%2520in%250Alip-sync%252C%2520video%2520fidelity%252C%2520and%2520pose%2520naturalness.%2520We%2520also%2520conduct%2520ablation%250Astudies%2520to%2520analyze%2520the%2520effectiveness%2520of%2520the%2520proposed%2520techniques%2520and%2520demonstrate%250Athe%2520pose%2520controllability%2520of%2520DAE-Talker.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.17550v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAE-Talker%3A%20High%20Fidelity%20Speech-Driven%20Talking%20Face%20Generation%20with%0A%20%20Diffusion%20Autoencoder&entry.906535625=Chenpeng%20Du%20and%20Qi%20Chen%20and%20Tianyu%20He%20and%20Xu%20Tan%20and%20Xie%20Chen%20and%20Kai%20Yu%20and%20Sheng%20Zhao%20and%20Jiang%20Bian&entry.1292438233=%20%20While%20recent%20research%20has%20made%20significant%20progress%20in%20speech-driven%20talking%0Aface%20generation%2C%20the%20quality%20of%20the%20generated%20video%20still%20lags%20behind%20that%20of%0Areal%20recordings.%20One%20reason%20for%20this%20is%20the%20use%20of%20handcrafted%20intermediate%0Arepresentations%20like%20facial%20landmarks%20and%203DMM%20coefficients%2C%20which%20are%20designed%0Abased%20on%20human%20knowledge%20and%20are%20insufficient%20to%20precisely%20describe%20facial%0Amovements.%20Additionally%2C%20these%20methods%20require%20an%20external%20pretrained%20model%20for%0Aextracting%20these%20representations%2C%20whose%20performance%20sets%20an%20upper%20bound%20on%0Atalking%20face%20generation.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0Amethod%20called%20DAE-Talker%20that%20leverages%20data-driven%20latent%20representations%0Aobtained%20from%20a%20diffusion%20autoencoder%20%28DAE%29.%20DAE%20contains%20an%20image%20encoder%20that%0Aencodes%20an%20image%20into%20a%20latent%20vector%20and%20a%20DDIM%20image%20decoder%20that%0Areconstructs%20the%20image%20from%20it.%20We%20train%20our%20DAE%20on%20talking%20face%20video%20frames%0Aand%20then%20extract%20their%20latent%20representations%20as%20the%20training%20target%20for%20a%0AConformer-based%20speech2latent%20model.%20This%20allows%20DAE-Talker%20to%20synthesize%20full%0Avideo%20frames%20and%20produce%20natural%20head%20movements%20that%20align%20with%20the%20content%20of%0Aspeech%2C%20rather%20than%20relying%20on%20a%20predetermined%20head%20pose%20from%20a%20template%20video.%0AWe%20also%20introduce%20pose%20modelling%20in%20speech2latent%20for%20pose%20controllability.%0AAdditionally%2C%20we%20propose%20a%20novel%20method%20for%20generating%20continuous%20video%20frames%0Awith%20the%20DDIM%20image%20decoder%20trained%20on%20individual%20frames%2C%20eliminating%20the%20need%0Afor%20modelling%20the%20joint%20distribution%20of%20consecutive%20frames%20directly.%20Our%0Aexperiments%20show%20that%20DAE-Talker%20outperforms%20existing%20popular%20methods%20in%0Alip-sync%2C%20video%20fidelity%2C%20and%20pose%20naturalness.%20We%20also%20conduct%20ablation%0Astudies%20to%20analyze%20the%20effectiveness%20of%20the%20proposed%20techniques%20and%20demonstrate%0Athe%20pose%20controllability%20of%20DAE-Talker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.17550v6&entry.124074799=Read"},
{"title": "RO-SVD: A Reconfigurable Hardware Copyright Protection Framework for\n  AIGC Applications", "author": "Zhuoheng Ran and Muhammad A. A. Abdelgawad and Zekai Zhang and Ray C. C. Cheung and Hong Yan", "abstract": "  The dramatic surge in the utilisation of generative artificial intelligence\n(GenAI) underscores the need for a secure and efficient mechanism to\nresponsibly manage, use and disseminate multi-dimensional data generated by\nartificial intelligence (AI). In this paper, we propose a blockchain-based\ncopyright traceability framework called ring oscillator-singular value\ndecomposition (RO-SVD), which introduces decomposition computing to approximate\nlow-rank matrices generated from hardware entropy sources and establishes an\nAI-generated content (AIGC) copyright traceability mechanism at the device\nlevel. By leveraging the parallelism and reconfigurability of\nfield-programmable gate arrays (FPGAs), our framework can be easily constructed\non existing AI-accelerated devices and provide a low-cost solution to emerging\ncopyright issues of AIGC. We developed a hardware-software (HW/SW) co-design\nprototype based on comprehensive analysis and on-board experiments with\nmultiple AI-applicable FPGAs. Using AI-generated images as a case study, our\nframework demonstrated effectiveness and emphasised customisation,\nunpredictability, efficiency, management and reconfigurability. To the best of\nour knowledge, this is the first practical hardware study discussing and\nimplementing copyright traceability specifically for AI-generated content.\n", "link": "http://arxiv.org/abs/2406.11536v2", "date": "2024-12-02", "relevancy": 2.3904, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.489}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4794}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RO-SVD%3A%20A%20Reconfigurable%20Hardware%20Copyright%20Protection%20Framework%20for%0A%20%20AIGC%20Applications&body=Title%3A%20RO-SVD%3A%20A%20Reconfigurable%20Hardware%20Copyright%20Protection%20Framework%20for%0A%20%20AIGC%20Applications%0AAuthor%3A%20Zhuoheng%20Ran%20and%20Muhammad%20A.%20A.%20Abdelgawad%20and%20Zekai%20Zhang%20and%20Ray%20C.%20C.%20Cheung%20and%20Hong%20Yan%0AAbstract%3A%20%20%20The%20dramatic%20surge%20in%20the%20utilisation%20of%20generative%20artificial%20intelligence%0A%28GenAI%29%20underscores%20the%20need%20for%20a%20secure%20and%20efficient%20mechanism%20to%0Aresponsibly%20manage%2C%20use%20and%20disseminate%20multi-dimensional%20data%20generated%20by%0Aartificial%20intelligence%20%28AI%29.%20In%20this%20paper%2C%20we%20propose%20a%20blockchain-based%0Acopyright%20traceability%20framework%20called%20ring%20oscillator-singular%20value%0Adecomposition%20%28RO-SVD%29%2C%20which%20introduces%20decomposition%20computing%20to%20approximate%0Alow-rank%20matrices%20generated%20from%20hardware%20entropy%20sources%20and%20establishes%20an%0AAI-generated%20content%20%28AIGC%29%20copyright%20traceability%20mechanism%20at%20the%20device%0Alevel.%20By%20leveraging%20the%20parallelism%20and%20reconfigurability%20of%0Afield-programmable%20gate%20arrays%20%28FPGAs%29%2C%20our%20framework%20can%20be%20easily%20constructed%0Aon%20existing%20AI-accelerated%20devices%20and%20provide%20a%20low-cost%20solution%20to%20emerging%0Acopyright%20issues%20of%20AIGC.%20We%20developed%20a%20hardware-software%20%28HW/SW%29%20co-design%0Aprototype%20based%20on%20comprehensive%20analysis%20and%20on-board%20experiments%20with%0Amultiple%20AI-applicable%20FPGAs.%20Using%20AI-generated%20images%20as%20a%20case%20study%2C%20our%0Aframework%20demonstrated%20effectiveness%20and%20emphasised%20customisation%2C%0Aunpredictability%2C%20efficiency%2C%20management%20and%20reconfigurability.%20To%20the%20best%20of%0Aour%20knowledge%2C%20this%20is%20the%20first%20practical%20hardware%20study%20discussing%20and%0Aimplementing%20copyright%20traceability%20specifically%20for%20AI-generated%20content.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11536v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRO-SVD%253A%2520A%2520Reconfigurable%2520Hardware%2520Copyright%2520Protection%2520Framework%2520for%250A%2520%2520AIGC%2520Applications%26entry.906535625%3DZhuoheng%2520Ran%2520and%2520Muhammad%2520A.%2520A.%2520Abdelgawad%2520and%2520Zekai%2520Zhang%2520and%2520Ray%2520C.%2520C.%2520Cheung%2520and%2520Hong%2520Yan%26entry.1292438233%3D%2520%2520The%2520dramatic%2520surge%2520in%2520the%2520utilisation%2520of%2520generative%2520artificial%2520intelligence%250A%2528GenAI%2529%2520underscores%2520the%2520need%2520for%2520a%2520secure%2520and%2520efficient%2520mechanism%2520to%250Aresponsibly%2520manage%252C%2520use%2520and%2520disseminate%2520multi-dimensional%2520data%2520generated%2520by%250Aartificial%2520intelligence%2520%2528AI%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520blockchain-based%250Acopyright%2520traceability%2520framework%2520called%2520ring%2520oscillator-singular%2520value%250Adecomposition%2520%2528RO-SVD%2529%252C%2520which%2520introduces%2520decomposition%2520computing%2520to%2520approximate%250Alow-rank%2520matrices%2520generated%2520from%2520hardware%2520entropy%2520sources%2520and%2520establishes%2520an%250AAI-generated%2520content%2520%2528AIGC%2529%2520copyright%2520traceability%2520mechanism%2520at%2520the%2520device%250Alevel.%2520By%2520leveraging%2520the%2520parallelism%2520and%2520reconfigurability%2520of%250Afield-programmable%2520gate%2520arrays%2520%2528FPGAs%2529%252C%2520our%2520framework%2520can%2520be%2520easily%2520constructed%250Aon%2520existing%2520AI-accelerated%2520devices%2520and%2520provide%2520a%2520low-cost%2520solution%2520to%2520emerging%250Acopyright%2520issues%2520of%2520AIGC.%2520We%2520developed%2520a%2520hardware-software%2520%2528HW/SW%2529%2520co-design%250Aprototype%2520based%2520on%2520comprehensive%2520analysis%2520and%2520on-board%2520experiments%2520with%250Amultiple%2520AI-applicable%2520FPGAs.%2520Using%2520AI-generated%2520images%2520as%2520a%2520case%2520study%252C%2520our%250Aframework%2520demonstrated%2520effectiveness%2520and%2520emphasised%2520customisation%252C%250Aunpredictability%252C%2520efficiency%252C%2520management%2520and%2520reconfigurability.%2520To%2520the%2520best%2520of%250Aour%2520knowledge%252C%2520this%2520is%2520the%2520first%2520practical%2520hardware%2520study%2520discussing%2520and%250Aimplementing%2520copyright%2520traceability%2520specifically%2520for%2520AI-generated%2520content.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11536v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RO-SVD%3A%20A%20Reconfigurable%20Hardware%20Copyright%20Protection%20Framework%20for%0A%20%20AIGC%20Applications&entry.906535625=Zhuoheng%20Ran%20and%20Muhammad%20A.%20A.%20Abdelgawad%20and%20Zekai%20Zhang%20and%20Ray%20C.%20C.%20Cheung%20and%20Hong%20Yan&entry.1292438233=%20%20The%20dramatic%20surge%20in%20the%20utilisation%20of%20generative%20artificial%20intelligence%0A%28GenAI%29%20underscores%20the%20need%20for%20a%20secure%20and%20efficient%20mechanism%20to%0Aresponsibly%20manage%2C%20use%20and%20disseminate%20multi-dimensional%20data%20generated%20by%0Aartificial%20intelligence%20%28AI%29.%20In%20this%20paper%2C%20we%20propose%20a%20blockchain-based%0Acopyright%20traceability%20framework%20called%20ring%20oscillator-singular%20value%0Adecomposition%20%28RO-SVD%29%2C%20which%20introduces%20decomposition%20computing%20to%20approximate%0Alow-rank%20matrices%20generated%20from%20hardware%20entropy%20sources%20and%20establishes%20an%0AAI-generated%20content%20%28AIGC%29%20copyright%20traceability%20mechanism%20at%20the%20device%0Alevel.%20By%20leveraging%20the%20parallelism%20and%20reconfigurability%20of%0Afield-programmable%20gate%20arrays%20%28FPGAs%29%2C%20our%20framework%20can%20be%20easily%20constructed%0Aon%20existing%20AI-accelerated%20devices%20and%20provide%20a%20low-cost%20solution%20to%20emerging%0Acopyright%20issues%20of%20AIGC.%20We%20developed%20a%20hardware-software%20%28HW/SW%29%20co-design%0Aprototype%20based%20on%20comprehensive%20analysis%20and%20on-board%20experiments%20with%0Amultiple%20AI-applicable%20FPGAs.%20Using%20AI-generated%20images%20as%20a%20case%20study%2C%20our%0Aframework%20demonstrated%20effectiveness%20and%20emphasised%20customisation%2C%0Aunpredictability%2C%20efficiency%2C%20management%20and%20reconfigurability.%20To%20the%20best%20of%0Aour%20knowledge%2C%20this%20is%20the%20first%20practical%20hardware%20study%20discussing%20and%0Aimplementing%20copyright%20traceability%20specifically%20for%20AI-generated%20content.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11536v2&entry.124074799=Read"},
{"title": "VisScience: An Extensive Benchmark for Evaluating K12 Educational\n  Multi-modal Scientific Reasoning", "author": "Zhihuan Jiang and Zhen Yang and Jinhao Chen and Zhengxiao Du and Weihan Wang and Bin Xu and Jie Tang", "abstract": "  Multi-modal large language models (MLLMs) have demonstrated promising\ncapabilities across various tasks by integrating textual and visual information\nto achieve visual understanding in complex scenarios. Despite the availability\nof several benchmarks aims to evaluating MLLMs in tasks from visual question\nanswering to complex problem-solving, most focus predominantly on mathematics\nor general visual understanding tasks. This reveals a critical gap in current\nbenchmarks, which often overlook the inclusion of other key scientific\ndisciplines such as physics and chemistry. To address this gap, we meticulously\nconstruct a comprehensive benchmark, named VisScience, which is utilized to\nassess the multi-modal scientific reasoning across the three disciplines of\nmathematics, physics, and chemistry. This benchmark comprises 3,000 questions\ndrawn from K12 education - spanning elementary school through high school -\nequally distributed across three disciplines, with 1,000 questions per\ndiscipline. The questions within VisScience span 21 distinct subjects and are\ncategorized into five difficulty levels, offering a broad spectrum of topics\nwithin each discipline. With VisScience, we present a detailed evaluation of\nthe performance of 25 representative MLLMs in scientific reasoning.\nExperimental results demonstrate that closed-source MLLMs generally outperform\nopen-source models. The best performance observed include a 53.4\\% accuracy in\nmathematics by Claude3.5-Sonnet, 38.2\\% in physics by GPT-4o, and 47.0\\% in\nchemistry by Gemini-1.5-Pro. These results underscore the strengths and\nlimitations of MLLMs, suggesting areas for future improvement and highlighting\nthe importance of developing models that can effectively handle the diverse\ndemands of multi-modal scientific reasoning.\n", "link": "http://arxiv.org/abs/2409.13730v2", "date": "2024-12-02", "relevancy": 2.3683, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6023}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6023}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisScience%3A%20An%20Extensive%20Benchmark%20for%20Evaluating%20K12%20Educational%0A%20%20Multi-modal%20Scientific%20Reasoning&body=Title%3A%20VisScience%3A%20An%20Extensive%20Benchmark%20for%20Evaluating%20K12%20Educational%0A%20%20Multi-modal%20Scientific%20Reasoning%0AAuthor%3A%20Zhihuan%20Jiang%20and%20Zhen%20Yang%20and%20Jinhao%20Chen%20and%20Zhengxiao%20Du%20and%20Weihan%20Wang%20and%20Bin%20Xu%20and%20Jie%20Tang%0AAbstract%3A%20%20%20Multi-modal%20large%20language%20models%20%28MLLMs%29%20have%20demonstrated%20promising%0Acapabilities%20across%20various%20tasks%20by%20integrating%20textual%20and%20visual%20information%0Ato%20achieve%20visual%20understanding%20in%20complex%20scenarios.%20Despite%20the%20availability%0Aof%20several%20benchmarks%20aims%20to%20evaluating%20MLLMs%20in%20tasks%20from%20visual%20question%0Aanswering%20to%20complex%20problem-solving%2C%20most%20focus%20predominantly%20on%20mathematics%0Aor%20general%20visual%20understanding%20tasks.%20This%20reveals%20a%20critical%20gap%20in%20current%0Abenchmarks%2C%20which%20often%20overlook%20the%20inclusion%20of%20other%20key%20scientific%0Adisciplines%20such%20as%20physics%20and%20chemistry.%20To%20address%20this%20gap%2C%20we%20meticulously%0Aconstruct%20a%20comprehensive%20benchmark%2C%20named%20VisScience%2C%20which%20is%20utilized%20to%0Aassess%20the%20multi-modal%20scientific%20reasoning%20across%20the%20three%20disciplines%20of%0Amathematics%2C%20physics%2C%20and%20chemistry.%20This%20benchmark%20comprises%203%2C000%20questions%0Adrawn%20from%20K12%20education%20-%20spanning%20elementary%20school%20through%20high%20school%20-%0Aequally%20distributed%20across%20three%20disciplines%2C%20with%201%2C000%20questions%20per%0Adiscipline.%20The%20questions%20within%20VisScience%20span%2021%20distinct%20subjects%20and%20are%0Acategorized%20into%20five%20difficulty%20levels%2C%20offering%20a%20broad%20spectrum%20of%20topics%0Awithin%20each%20discipline.%20With%20VisScience%2C%20we%20present%20a%20detailed%20evaluation%20of%0Athe%20performance%20of%2025%20representative%20MLLMs%20in%20scientific%20reasoning.%0AExperimental%20results%20demonstrate%20that%20closed-source%20MLLMs%20generally%20outperform%0Aopen-source%20models.%20The%20best%20performance%20observed%20include%20a%2053.4%5C%25%20accuracy%20in%0Amathematics%20by%20Claude3.5-Sonnet%2C%2038.2%5C%25%20in%20physics%20by%20GPT-4o%2C%20and%2047.0%5C%25%20in%0Achemistry%20by%20Gemini-1.5-Pro.%20These%20results%20underscore%20the%20strengths%20and%0Alimitations%20of%20MLLMs%2C%20suggesting%20areas%20for%20future%20improvement%20and%20highlighting%0Athe%20importance%20of%20developing%20models%20that%20can%20effectively%20handle%20the%20diverse%0Ademands%20of%20multi-modal%20scientific%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13730v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisScience%253A%2520An%2520Extensive%2520Benchmark%2520for%2520Evaluating%2520K12%2520Educational%250A%2520%2520Multi-modal%2520Scientific%2520Reasoning%26entry.906535625%3DZhihuan%2520Jiang%2520and%2520Zhen%2520Yang%2520and%2520Jinhao%2520Chen%2520and%2520Zhengxiao%2520Du%2520and%2520Weihan%2520Wang%2520and%2520Bin%2520Xu%2520and%2520Jie%2520Tang%26entry.1292438233%3D%2520%2520Multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520promising%250Acapabilities%2520across%2520various%2520tasks%2520by%2520integrating%2520textual%2520and%2520visual%2520information%250Ato%2520achieve%2520visual%2520understanding%2520in%2520complex%2520scenarios.%2520Despite%2520the%2520availability%250Aof%2520several%2520benchmarks%2520aims%2520to%2520evaluating%2520MLLMs%2520in%2520tasks%2520from%2520visual%2520question%250Aanswering%2520to%2520complex%2520problem-solving%252C%2520most%2520focus%2520predominantly%2520on%2520mathematics%250Aor%2520general%2520visual%2520understanding%2520tasks.%2520This%2520reveals%2520a%2520critical%2520gap%2520in%2520current%250Abenchmarks%252C%2520which%2520often%2520overlook%2520the%2520inclusion%2520of%2520other%2520key%2520scientific%250Adisciplines%2520such%2520as%2520physics%2520and%2520chemistry.%2520To%2520address%2520this%2520gap%252C%2520we%2520meticulously%250Aconstruct%2520a%2520comprehensive%2520benchmark%252C%2520named%2520VisScience%252C%2520which%2520is%2520utilized%2520to%250Aassess%2520the%2520multi-modal%2520scientific%2520reasoning%2520across%2520the%2520three%2520disciplines%2520of%250Amathematics%252C%2520physics%252C%2520and%2520chemistry.%2520This%2520benchmark%2520comprises%25203%252C000%2520questions%250Adrawn%2520from%2520K12%2520education%2520-%2520spanning%2520elementary%2520school%2520through%2520high%2520school%2520-%250Aequally%2520distributed%2520across%2520three%2520disciplines%252C%2520with%25201%252C000%2520questions%2520per%250Adiscipline.%2520The%2520questions%2520within%2520VisScience%2520span%252021%2520distinct%2520subjects%2520and%2520are%250Acategorized%2520into%2520five%2520difficulty%2520levels%252C%2520offering%2520a%2520broad%2520spectrum%2520of%2520topics%250Awithin%2520each%2520discipline.%2520With%2520VisScience%252C%2520we%2520present%2520a%2520detailed%2520evaluation%2520of%250Athe%2520performance%2520of%252025%2520representative%2520MLLMs%2520in%2520scientific%2520reasoning.%250AExperimental%2520results%2520demonstrate%2520that%2520closed-source%2520MLLMs%2520generally%2520outperform%250Aopen-source%2520models.%2520The%2520best%2520performance%2520observed%2520include%2520a%252053.4%255C%2525%2520accuracy%2520in%250Amathematics%2520by%2520Claude3.5-Sonnet%252C%252038.2%255C%2525%2520in%2520physics%2520by%2520GPT-4o%252C%2520and%252047.0%255C%2525%2520in%250Achemistry%2520by%2520Gemini-1.5-Pro.%2520These%2520results%2520underscore%2520the%2520strengths%2520and%250Alimitations%2520of%2520MLLMs%252C%2520suggesting%2520areas%2520for%2520future%2520improvement%2520and%2520highlighting%250Athe%2520importance%2520of%2520developing%2520models%2520that%2520can%2520effectively%2520handle%2520the%2520diverse%250Ademands%2520of%2520multi-modal%2520scientific%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13730v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisScience%3A%20An%20Extensive%20Benchmark%20for%20Evaluating%20K12%20Educational%0A%20%20Multi-modal%20Scientific%20Reasoning&entry.906535625=Zhihuan%20Jiang%20and%20Zhen%20Yang%20and%20Jinhao%20Chen%20and%20Zhengxiao%20Du%20and%20Weihan%20Wang%20and%20Bin%20Xu%20and%20Jie%20Tang&entry.1292438233=%20%20Multi-modal%20large%20language%20models%20%28MLLMs%29%20have%20demonstrated%20promising%0Acapabilities%20across%20various%20tasks%20by%20integrating%20textual%20and%20visual%20information%0Ato%20achieve%20visual%20understanding%20in%20complex%20scenarios.%20Despite%20the%20availability%0Aof%20several%20benchmarks%20aims%20to%20evaluating%20MLLMs%20in%20tasks%20from%20visual%20question%0Aanswering%20to%20complex%20problem-solving%2C%20most%20focus%20predominantly%20on%20mathematics%0Aor%20general%20visual%20understanding%20tasks.%20This%20reveals%20a%20critical%20gap%20in%20current%0Abenchmarks%2C%20which%20often%20overlook%20the%20inclusion%20of%20other%20key%20scientific%0Adisciplines%20such%20as%20physics%20and%20chemistry.%20To%20address%20this%20gap%2C%20we%20meticulously%0Aconstruct%20a%20comprehensive%20benchmark%2C%20named%20VisScience%2C%20which%20is%20utilized%20to%0Aassess%20the%20multi-modal%20scientific%20reasoning%20across%20the%20three%20disciplines%20of%0Amathematics%2C%20physics%2C%20and%20chemistry.%20This%20benchmark%20comprises%203%2C000%20questions%0Adrawn%20from%20K12%20education%20-%20spanning%20elementary%20school%20through%20high%20school%20-%0Aequally%20distributed%20across%20three%20disciplines%2C%20with%201%2C000%20questions%20per%0Adiscipline.%20The%20questions%20within%20VisScience%20span%2021%20distinct%20subjects%20and%20are%0Acategorized%20into%20five%20difficulty%20levels%2C%20offering%20a%20broad%20spectrum%20of%20topics%0Awithin%20each%20discipline.%20With%20VisScience%2C%20we%20present%20a%20detailed%20evaluation%20of%0Athe%20performance%20of%2025%20representative%20MLLMs%20in%20scientific%20reasoning.%0AExperimental%20results%20demonstrate%20that%20closed-source%20MLLMs%20generally%20outperform%0Aopen-source%20models.%20The%20best%20performance%20observed%20include%20a%2053.4%5C%25%20accuracy%20in%0Amathematics%20by%20Claude3.5-Sonnet%2C%2038.2%5C%25%20in%20physics%20by%20GPT-4o%2C%20and%2047.0%5C%25%20in%0Achemistry%20by%20Gemini-1.5-Pro.%20These%20results%20underscore%20the%20strengths%20and%0Alimitations%20of%20MLLMs%2C%20suggesting%20areas%20for%20future%20improvement%20and%20highlighting%0Athe%20importance%20of%20developing%20models%20that%20can%20effectively%20handle%20the%20diverse%0Ademands%20of%20multi-modal%20scientific%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13730v2&entry.124074799=Read"},
{"title": "Morph: A Motion-free Physics Optimization Framework for Human Motion\n  Generation", "author": "Zhuo Li and Mingshuang Luo and Ruibing Hou and Xin Zhao and Hao Liu and Hong Chang and Zimo Liu and Chen Li", "abstract": "  Human motion generation plays a vital role in applications such as digital\nhumans and humanoid robot control. However, most existing approaches disregard\nphysics constraints, leading to the frequent production of physically\nimplausible motions with pronounced artifacts such as floating and foot\nsliding. In this paper, we propose \\textbf{Morph}, a\n\\textbf{Mo}tion-f\\textbf{r}ee \\textbf{ph}ysics optimization framework,\ncomprising a Motion Generator and a Motion Physics Refinement module, for\nenhancing physical plausibility without relying on costly real-world motion\ndata. Specifically, the Motion Generator is responsible for providing\nlarge-scale synthetic motion data, while the Motion Physics Refinement Module\nutilizes these synthetic data to train a motion imitator within a physics\nsimulator, enforcing physical constraints to project the noisy motions into a\nphysically-plausible space. These physically refined motions, in turn, are used\nto fine-tune the Motion Generator, further enhancing its capability.\nExperiments on both text-to-motion and music-to-dance generation tasks\ndemonstrate that our framework achieves state-of-the-art motion generation\nquality while improving physical plausibility drastically.\n", "link": "http://arxiv.org/abs/2411.14951v2", "date": "2024-12-02", "relevancy": 2.357, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6584}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5416}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Morph%3A%20A%20Motion-free%20Physics%20Optimization%20Framework%20for%20Human%20Motion%0A%20%20Generation&body=Title%3A%20Morph%3A%20A%20Motion-free%20Physics%20Optimization%20Framework%20for%20Human%20Motion%0A%20%20Generation%0AAuthor%3A%20Zhuo%20Li%20and%20Mingshuang%20Luo%20and%20Ruibing%20Hou%20and%20Xin%20Zhao%20and%20Hao%20Liu%20and%20Hong%20Chang%20and%20Zimo%20Liu%20and%20Chen%20Li%0AAbstract%3A%20%20%20Human%20motion%20generation%20plays%20a%20vital%20role%20in%20applications%20such%20as%20digital%0Ahumans%20and%20humanoid%20robot%20control.%20However%2C%20most%20existing%20approaches%20disregard%0Aphysics%20constraints%2C%20leading%20to%20the%20frequent%20production%20of%20physically%0Aimplausible%20motions%20with%20pronounced%20artifacts%20such%20as%20floating%20and%20foot%0Asliding.%20In%20this%20paper%2C%20we%20propose%20%5Ctextbf%7BMorph%7D%2C%20a%0A%5Ctextbf%7BMo%7Dtion-f%5Ctextbf%7Br%7Dee%20%5Ctextbf%7Bph%7Dysics%20optimization%20framework%2C%0Acomprising%20a%20Motion%20Generator%20and%20a%20Motion%20Physics%20Refinement%20module%2C%20for%0Aenhancing%20physical%20plausibility%20without%20relying%20on%20costly%20real-world%20motion%0Adata.%20Specifically%2C%20the%20Motion%20Generator%20is%20responsible%20for%20providing%0Alarge-scale%20synthetic%20motion%20data%2C%20while%20the%20Motion%20Physics%20Refinement%20Module%0Autilizes%20these%20synthetic%20data%20to%20train%20a%20motion%20imitator%20within%20a%20physics%0Asimulator%2C%20enforcing%20physical%20constraints%20to%20project%20the%20noisy%20motions%20into%20a%0Aphysically-plausible%20space.%20These%20physically%20refined%20motions%2C%20in%20turn%2C%20are%20used%0Ato%20fine-tune%20the%20Motion%20Generator%2C%20further%20enhancing%20its%20capability.%0AExperiments%20on%20both%20text-to-motion%20and%20music-to-dance%20generation%20tasks%0Ademonstrate%20that%20our%20framework%20achieves%20state-of-the-art%20motion%20generation%0Aquality%20while%20improving%20physical%20plausibility%20drastically.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14951v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMorph%253A%2520A%2520Motion-free%2520Physics%2520Optimization%2520Framework%2520for%2520Human%2520Motion%250A%2520%2520Generation%26entry.906535625%3DZhuo%2520Li%2520and%2520Mingshuang%2520Luo%2520and%2520Ruibing%2520Hou%2520and%2520Xin%2520Zhao%2520and%2520Hao%2520Liu%2520and%2520Hong%2520Chang%2520and%2520Zimo%2520Liu%2520and%2520Chen%2520Li%26entry.1292438233%3D%2520%2520Human%2520motion%2520generation%2520plays%2520a%2520vital%2520role%2520in%2520applications%2520such%2520as%2520digital%250Ahumans%2520and%2520humanoid%2520robot%2520control.%2520However%252C%2520most%2520existing%2520approaches%2520disregard%250Aphysics%2520constraints%252C%2520leading%2520to%2520the%2520frequent%2520production%2520of%2520physically%250Aimplausible%2520motions%2520with%2520pronounced%2520artifacts%2520such%2520as%2520floating%2520and%2520foot%250Asliding.%2520In%2520this%2520paper%252C%2520we%2520propose%2520%255Ctextbf%257BMorph%257D%252C%2520a%250A%255Ctextbf%257BMo%257Dtion-f%255Ctextbf%257Br%257Dee%2520%255Ctextbf%257Bph%257Dysics%2520optimization%2520framework%252C%250Acomprising%2520a%2520Motion%2520Generator%2520and%2520a%2520Motion%2520Physics%2520Refinement%2520module%252C%2520for%250Aenhancing%2520physical%2520plausibility%2520without%2520relying%2520on%2520costly%2520real-world%2520motion%250Adata.%2520Specifically%252C%2520the%2520Motion%2520Generator%2520is%2520responsible%2520for%2520providing%250Alarge-scale%2520synthetic%2520motion%2520data%252C%2520while%2520the%2520Motion%2520Physics%2520Refinement%2520Module%250Autilizes%2520these%2520synthetic%2520data%2520to%2520train%2520a%2520motion%2520imitator%2520within%2520a%2520physics%250Asimulator%252C%2520enforcing%2520physical%2520constraints%2520to%2520project%2520the%2520noisy%2520motions%2520into%2520a%250Aphysically-plausible%2520space.%2520These%2520physically%2520refined%2520motions%252C%2520in%2520turn%252C%2520are%2520used%250Ato%2520fine-tune%2520the%2520Motion%2520Generator%252C%2520further%2520enhancing%2520its%2520capability.%250AExperiments%2520on%2520both%2520text-to-motion%2520and%2520music-to-dance%2520generation%2520tasks%250Ademonstrate%2520that%2520our%2520framework%2520achieves%2520state-of-the-art%2520motion%2520generation%250Aquality%2520while%2520improving%2520physical%2520plausibility%2520drastically.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14951v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Morph%3A%20A%20Motion-free%20Physics%20Optimization%20Framework%20for%20Human%20Motion%0A%20%20Generation&entry.906535625=Zhuo%20Li%20and%20Mingshuang%20Luo%20and%20Ruibing%20Hou%20and%20Xin%20Zhao%20and%20Hao%20Liu%20and%20Hong%20Chang%20and%20Zimo%20Liu%20and%20Chen%20Li&entry.1292438233=%20%20Human%20motion%20generation%20plays%20a%20vital%20role%20in%20applications%20such%20as%20digital%0Ahumans%20and%20humanoid%20robot%20control.%20However%2C%20most%20existing%20approaches%20disregard%0Aphysics%20constraints%2C%20leading%20to%20the%20frequent%20production%20of%20physically%0Aimplausible%20motions%20with%20pronounced%20artifacts%20such%20as%20floating%20and%20foot%0Asliding.%20In%20this%20paper%2C%20we%20propose%20%5Ctextbf%7BMorph%7D%2C%20a%0A%5Ctextbf%7BMo%7Dtion-f%5Ctextbf%7Br%7Dee%20%5Ctextbf%7Bph%7Dysics%20optimization%20framework%2C%0Acomprising%20a%20Motion%20Generator%20and%20a%20Motion%20Physics%20Refinement%20module%2C%20for%0Aenhancing%20physical%20plausibility%20without%20relying%20on%20costly%20real-world%20motion%0Adata.%20Specifically%2C%20the%20Motion%20Generator%20is%20responsible%20for%20providing%0Alarge-scale%20synthetic%20motion%20data%2C%20while%20the%20Motion%20Physics%20Refinement%20Module%0Autilizes%20these%20synthetic%20data%20to%20train%20a%20motion%20imitator%20within%20a%20physics%0Asimulator%2C%20enforcing%20physical%20constraints%20to%20project%20the%20noisy%20motions%20into%20a%0Aphysically-plausible%20space.%20These%20physically%20refined%20motions%2C%20in%20turn%2C%20are%20used%0Ato%20fine-tune%20the%20Motion%20Generator%2C%20further%20enhancing%20its%20capability.%0AExperiments%20on%20both%20text-to-motion%20and%20music-to-dance%20generation%20tasks%0Ademonstrate%20that%20our%20framework%20achieves%20state-of-the-art%20motion%20generation%0Aquality%20while%20improving%20physical%20plausibility%20drastically.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14951v2&entry.124074799=Read"},
{"title": "Evaluation of Multi-Scale Multiple Instance Learning to Improve Thyroid\n  Cancer Classification", "author": "Maximilian E. Tschuchnig and Philipp Grubm\u00fcller and Lea M. Stangassinger and Christina Kreutzer and S\u00e9bastien Couillard-Despr\u00e9s and Gertie J. Oostingh and Anton Hittmair and Michael Gadermayr", "abstract": "  Thyroid cancer is currently the fifth most common malignancy diagnosed in\nwomen. Since differentiation of cancer sub-types is important for treatment and\ncurrent, manual methods are time consuming and subjective, automatic\ncomputer-aided differentiation of cancer types is crucial. Manual\ndifferentiation of thyroid cancer is based on tissue sections, analysed by\npathologists using histological features. Due to the enormous size of gigapixel\nwhole slide images, holistic classification using deep learning methods is not\nfeasible. Patch based multiple instance learning approaches, combined with\naggregations such as bag-of-words, is a common approach. This work's\ncontribution is to extend a patch based state-of-the-art method by generating\nand combining feature vectors of three different patch resolutions and\nanalysing three distinct ways of combining them. The results showed\nimprovements in one of the three multi-scale approaches, while the others led\nto decreased scores. This provides motivation for analysis and discussion of\nthe individual approaches.\n", "link": "http://arxiv.org/abs/2204.10942v2", "date": "2024-12-02", "relevancy": 2.3315, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4958}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.452}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20of%20Multi-Scale%20Multiple%20Instance%20Learning%20to%20Improve%20Thyroid%0A%20%20Cancer%20Classification&body=Title%3A%20Evaluation%20of%20Multi-Scale%20Multiple%20Instance%20Learning%20to%20Improve%20Thyroid%0A%20%20Cancer%20Classification%0AAuthor%3A%20Maximilian%20E.%20Tschuchnig%20and%20Philipp%20Grubm%C3%BCller%20and%20Lea%20M.%20Stangassinger%20and%20Christina%20Kreutzer%20and%20S%C3%A9bastien%20Couillard-Despr%C3%A9s%20and%20Gertie%20J.%20Oostingh%20and%20Anton%20Hittmair%20and%20Michael%20Gadermayr%0AAbstract%3A%20%20%20Thyroid%20cancer%20is%20currently%20the%20fifth%20most%20common%20malignancy%20diagnosed%20in%0Awomen.%20Since%20differentiation%20of%20cancer%20sub-types%20is%20important%20for%20treatment%20and%0Acurrent%2C%20manual%20methods%20are%20time%20consuming%20and%20subjective%2C%20automatic%0Acomputer-aided%20differentiation%20of%20cancer%20types%20is%20crucial.%20Manual%0Adifferentiation%20of%20thyroid%20cancer%20is%20based%20on%20tissue%20sections%2C%20analysed%20by%0Apathologists%20using%20histological%20features.%20Due%20to%20the%20enormous%20size%20of%20gigapixel%0Awhole%20slide%20images%2C%20holistic%20classification%20using%20deep%20learning%20methods%20is%20not%0Afeasible.%20Patch%20based%20multiple%20instance%20learning%20approaches%2C%20combined%20with%0Aaggregations%20such%20as%20bag-of-words%2C%20is%20a%20common%20approach.%20This%20work%27s%0Acontribution%20is%20to%20extend%20a%20patch%20based%20state-of-the-art%20method%20by%20generating%0Aand%20combining%20feature%20vectors%20of%20three%20different%20patch%20resolutions%20and%0Aanalysing%20three%20distinct%20ways%20of%20combining%20them.%20The%20results%20showed%0Aimprovements%20in%20one%20of%20the%20three%20multi-scale%20approaches%2C%20while%20the%20others%20led%0Ato%20decreased%20scores.%20This%20provides%20motivation%20for%20analysis%20and%20discussion%20of%0Athe%20individual%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2204.10942v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520of%2520Multi-Scale%2520Multiple%2520Instance%2520Learning%2520to%2520Improve%2520Thyroid%250A%2520%2520Cancer%2520Classification%26entry.906535625%3DMaximilian%2520E.%2520Tschuchnig%2520and%2520Philipp%2520Grubm%25C3%25BCller%2520and%2520Lea%2520M.%2520Stangassinger%2520and%2520Christina%2520Kreutzer%2520and%2520S%25C3%25A9bastien%2520Couillard-Despr%25C3%25A9s%2520and%2520Gertie%2520J.%2520Oostingh%2520and%2520Anton%2520Hittmair%2520and%2520Michael%2520Gadermayr%26entry.1292438233%3D%2520%2520Thyroid%2520cancer%2520is%2520currently%2520the%2520fifth%2520most%2520common%2520malignancy%2520diagnosed%2520in%250Awomen.%2520Since%2520differentiation%2520of%2520cancer%2520sub-types%2520is%2520important%2520for%2520treatment%2520and%250Acurrent%252C%2520manual%2520methods%2520are%2520time%2520consuming%2520and%2520subjective%252C%2520automatic%250Acomputer-aided%2520differentiation%2520of%2520cancer%2520types%2520is%2520crucial.%2520Manual%250Adifferentiation%2520of%2520thyroid%2520cancer%2520is%2520based%2520on%2520tissue%2520sections%252C%2520analysed%2520by%250Apathologists%2520using%2520histological%2520features.%2520Due%2520to%2520the%2520enormous%2520size%2520of%2520gigapixel%250Awhole%2520slide%2520images%252C%2520holistic%2520classification%2520using%2520deep%2520learning%2520methods%2520is%2520not%250Afeasible.%2520Patch%2520based%2520multiple%2520instance%2520learning%2520approaches%252C%2520combined%2520with%250Aaggregations%2520such%2520as%2520bag-of-words%252C%2520is%2520a%2520common%2520approach.%2520This%2520work%2527s%250Acontribution%2520is%2520to%2520extend%2520a%2520patch%2520based%2520state-of-the-art%2520method%2520by%2520generating%250Aand%2520combining%2520feature%2520vectors%2520of%2520three%2520different%2520patch%2520resolutions%2520and%250Aanalysing%2520three%2520distinct%2520ways%2520of%2520combining%2520them.%2520The%2520results%2520showed%250Aimprovements%2520in%2520one%2520of%2520the%2520three%2520multi-scale%2520approaches%252C%2520while%2520the%2520others%2520led%250Ato%2520decreased%2520scores.%2520This%2520provides%2520motivation%2520for%2520analysis%2520and%2520discussion%2520of%250Athe%2520individual%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2204.10942v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20of%20Multi-Scale%20Multiple%20Instance%20Learning%20to%20Improve%20Thyroid%0A%20%20Cancer%20Classification&entry.906535625=Maximilian%20E.%20Tschuchnig%20and%20Philipp%20Grubm%C3%BCller%20and%20Lea%20M.%20Stangassinger%20and%20Christina%20Kreutzer%20and%20S%C3%A9bastien%20Couillard-Despr%C3%A9s%20and%20Gertie%20J.%20Oostingh%20and%20Anton%20Hittmair%20and%20Michael%20Gadermayr&entry.1292438233=%20%20Thyroid%20cancer%20is%20currently%20the%20fifth%20most%20common%20malignancy%20diagnosed%20in%0Awomen.%20Since%20differentiation%20of%20cancer%20sub-types%20is%20important%20for%20treatment%20and%0Acurrent%2C%20manual%20methods%20are%20time%20consuming%20and%20subjective%2C%20automatic%0Acomputer-aided%20differentiation%20of%20cancer%20types%20is%20crucial.%20Manual%0Adifferentiation%20of%20thyroid%20cancer%20is%20based%20on%20tissue%20sections%2C%20analysed%20by%0Apathologists%20using%20histological%20features.%20Due%20to%20the%20enormous%20size%20of%20gigapixel%0Awhole%20slide%20images%2C%20holistic%20classification%20using%20deep%20learning%20methods%20is%20not%0Afeasible.%20Patch%20based%20multiple%20instance%20learning%20approaches%2C%20combined%20with%0Aaggregations%20such%20as%20bag-of-words%2C%20is%20a%20common%20approach.%20This%20work%27s%0Acontribution%20is%20to%20extend%20a%20patch%20based%20state-of-the-art%20method%20by%20generating%0Aand%20combining%20feature%20vectors%20of%20three%20different%20patch%20resolutions%20and%0Aanalysing%20three%20distinct%20ways%20of%20combining%20them.%20The%20results%20showed%0Aimprovements%20in%20one%20of%20the%20three%20multi-scale%20approaches%2C%20while%20the%20others%20led%0Ato%20decreased%20scores.%20This%20provides%20motivation%20for%20analysis%20and%20discussion%20of%0Athe%20individual%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2204.10942v2&entry.124074799=Read"},
{"title": "VQA$^2$: Visual Question Answering for Video Quality Assessment", "author": "Ziheng Jia and Zicheng Zhang and Jiaying Qian and Haoning Wu and Wei Sun and Chunyi Li and Xiaohong Liu and Weisi Lin and Guangtao Zhai and Xiongkuo Min", "abstract": "  The advent and proliferation of large multi-modal models (LMMs) have\nintroduced new paradigms to computer vision, transforming various tasks into a\nunified visual question answering framework. Video Quality Assessment (VQA), a\nclassic field in low-level visual perception, focused initially on quantitative\nvideo quality scoring. However, driven by advances in LMMs, it is now\nprogressing toward more holistic visual quality understanding tasks. Recent\nstudies in the image domain have demonstrated that Visual Question Answering\n(VQA) can markedly enhance low-level visual quality evaluation. Nevertheless,\nrelated work has not been explored in the video domain, leaving substantial\nroom for improvement. To address this gap, we introduce the VQA2 Instruction\nDataset - the first visual question answering instruction dataset that focuses\non video quality assessment. This dataset consists of 3 subsets and covers\nvarious video types, containing 157,755 instruction question-answer pairs.\nThen, leveraging this foundation, we present the VQA2 series models. The VQA2\nseries models interleave visual and motion tokens to enhance the perception of\nspatial-temporal quality details in videos. We conduct extensive experiments on\nvideo quality scoring and understanding tasks, and results demonstrate that the\nVQA2series models achieve excellent performance in both tasks. Notably, our\nfinal model, the VQA2-Assistant, exceeds the renowned GPT-4o in visual quality\nunderstanding tasks while maintaining strong competitiveness in quality scoring\ntasks. Our work provides a foundation and feasible approach for integrating\nlow-level video quality assessment and understanding with LMMs.\n", "link": "http://arxiv.org/abs/2411.03795v4", "date": "2024-12-02", "relevancy": 2.3234, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5862}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5862}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VQA%24%5E2%24%3A%20Visual%20Question%20Answering%20for%20Video%20Quality%20Assessment&body=Title%3A%20VQA%24%5E2%24%3A%20Visual%20Question%20Answering%20for%20Video%20Quality%20Assessment%0AAuthor%3A%20Ziheng%20Jia%20and%20Zicheng%20Zhang%20and%20Jiaying%20Qian%20and%20Haoning%20Wu%20and%20Wei%20Sun%20and%20Chunyi%20Li%20and%20Xiaohong%20Liu%20and%20Weisi%20Lin%20and%20Guangtao%20Zhai%20and%20Xiongkuo%20Min%0AAbstract%3A%20%20%20The%20advent%20and%20proliferation%20of%20large%20multi-modal%20models%20%28LMMs%29%20have%0Aintroduced%20new%20paradigms%20to%20computer%20vision%2C%20transforming%20various%20tasks%20into%20a%0Aunified%20visual%20question%20answering%20framework.%20Video%20Quality%20Assessment%20%28VQA%29%2C%20a%0Aclassic%20field%20in%20low-level%20visual%20perception%2C%20focused%20initially%20on%20quantitative%0Avideo%20quality%20scoring.%20However%2C%20driven%20by%20advances%20in%20LMMs%2C%20it%20is%20now%0Aprogressing%20toward%20more%20holistic%20visual%20quality%20understanding%20tasks.%20Recent%0Astudies%20in%20the%20image%20domain%20have%20demonstrated%20that%20Visual%20Question%20Answering%0A%28VQA%29%20can%20markedly%20enhance%20low-level%20visual%20quality%20evaluation.%20Nevertheless%2C%0Arelated%20work%20has%20not%20been%20explored%20in%20the%20video%20domain%2C%20leaving%20substantial%0Aroom%20for%20improvement.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20VQA2%20Instruction%0ADataset%20-%20the%20first%20visual%20question%20answering%20instruction%20dataset%20that%20focuses%0Aon%20video%20quality%20assessment.%20This%20dataset%20consists%20of%203%20subsets%20and%20covers%0Avarious%20video%20types%2C%20containing%20157%2C755%20instruction%20question-answer%20pairs.%0AThen%2C%20leveraging%20this%20foundation%2C%20we%20present%20the%20VQA2%20series%20models.%20The%20VQA2%0Aseries%20models%20interleave%20visual%20and%20motion%20tokens%20to%20enhance%20the%20perception%20of%0Aspatial-temporal%20quality%20details%20in%20videos.%20We%20conduct%20extensive%20experiments%20on%0Avideo%20quality%20scoring%20and%20understanding%20tasks%2C%20and%20results%20demonstrate%20that%20the%0AVQA2series%20models%20achieve%20excellent%20performance%20in%20both%20tasks.%20Notably%2C%20our%0Afinal%20model%2C%20the%20VQA2-Assistant%2C%20exceeds%20the%20renowned%20GPT-4o%20in%20visual%20quality%0Aunderstanding%20tasks%20while%20maintaining%20strong%20competitiveness%20in%20quality%20scoring%0Atasks.%20Our%20work%20provides%20a%20foundation%20and%20feasible%20approach%20for%20integrating%0Alow-level%20video%20quality%20assessment%20and%20understanding%20with%20LMMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03795v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVQA%2524%255E2%2524%253A%2520Visual%2520Question%2520Answering%2520for%2520Video%2520Quality%2520Assessment%26entry.906535625%3DZiheng%2520Jia%2520and%2520Zicheng%2520Zhang%2520and%2520Jiaying%2520Qian%2520and%2520Haoning%2520Wu%2520and%2520Wei%2520Sun%2520and%2520Chunyi%2520Li%2520and%2520Xiaohong%2520Liu%2520and%2520Weisi%2520Lin%2520and%2520Guangtao%2520Zhai%2520and%2520Xiongkuo%2520Min%26entry.1292438233%3D%2520%2520The%2520advent%2520and%2520proliferation%2520of%2520large%2520multi-modal%2520models%2520%2528LMMs%2529%2520have%250Aintroduced%2520new%2520paradigms%2520to%2520computer%2520vision%252C%2520transforming%2520various%2520tasks%2520into%2520a%250Aunified%2520visual%2520question%2520answering%2520framework.%2520Video%2520Quality%2520Assessment%2520%2528VQA%2529%252C%2520a%250Aclassic%2520field%2520in%2520low-level%2520visual%2520perception%252C%2520focused%2520initially%2520on%2520quantitative%250Avideo%2520quality%2520scoring.%2520However%252C%2520driven%2520by%2520advances%2520in%2520LMMs%252C%2520it%2520is%2520now%250Aprogressing%2520toward%2520more%2520holistic%2520visual%2520quality%2520understanding%2520tasks.%2520Recent%250Astudies%2520in%2520the%2520image%2520domain%2520have%2520demonstrated%2520that%2520Visual%2520Question%2520Answering%250A%2528VQA%2529%2520can%2520markedly%2520enhance%2520low-level%2520visual%2520quality%2520evaluation.%2520Nevertheless%252C%250Arelated%2520work%2520has%2520not%2520been%2520explored%2520in%2520the%2520video%2520domain%252C%2520leaving%2520substantial%250Aroom%2520for%2520improvement.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520the%2520VQA2%2520Instruction%250ADataset%2520-%2520the%2520first%2520visual%2520question%2520answering%2520instruction%2520dataset%2520that%2520focuses%250Aon%2520video%2520quality%2520assessment.%2520This%2520dataset%2520consists%2520of%25203%2520subsets%2520and%2520covers%250Avarious%2520video%2520types%252C%2520containing%2520157%252C755%2520instruction%2520question-answer%2520pairs.%250AThen%252C%2520leveraging%2520this%2520foundation%252C%2520we%2520present%2520the%2520VQA2%2520series%2520models.%2520The%2520VQA2%250Aseries%2520models%2520interleave%2520visual%2520and%2520motion%2520tokens%2520to%2520enhance%2520the%2520perception%2520of%250Aspatial-temporal%2520quality%2520details%2520in%2520videos.%2520We%2520conduct%2520extensive%2520experiments%2520on%250Avideo%2520quality%2520scoring%2520and%2520understanding%2520tasks%252C%2520and%2520results%2520demonstrate%2520that%2520the%250AVQA2series%2520models%2520achieve%2520excellent%2520performance%2520in%2520both%2520tasks.%2520Notably%252C%2520our%250Afinal%2520model%252C%2520the%2520VQA2-Assistant%252C%2520exceeds%2520the%2520renowned%2520GPT-4o%2520in%2520visual%2520quality%250Aunderstanding%2520tasks%2520while%2520maintaining%2520strong%2520competitiveness%2520in%2520quality%2520scoring%250Atasks.%2520Our%2520work%2520provides%2520a%2520foundation%2520and%2520feasible%2520approach%2520for%2520integrating%250Alow-level%2520video%2520quality%2520assessment%2520and%2520understanding%2520with%2520LMMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03795v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VQA%24%5E2%24%3A%20Visual%20Question%20Answering%20for%20Video%20Quality%20Assessment&entry.906535625=Ziheng%20Jia%20and%20Zicheng%20Zhang%20and%20Jiaying%20Qian%20and%20Haoning%20Wu%20and%20Wei%20Sun%20and%20Chunyi%20Li%20and%20Xiaohong%20Liu%20and%20Weisi%20Lin%20and%20Guangtao%20Zhai%20and%20Xiongkuo%20Min&entry.1292438233=%20%20The%20advent%20and%20proliferation%20of%20large%20multi-modal%20models%20%28LMMs%29%20have%0Aintroduced%20new%20paradigms%20to%20computer%20vision%2C%20transforming%20various%20tasks%20into%20a%0Aunified%20visual%20question%20answering%20framework.%20Video%20Quality%20Assessment%20%28VQA%29%2C%20a%0Aclassic%20field%20in%20low-level%20visual%20perception%2C%20focused%20initially%20on%20quantitative%0Avideo%20quality%20scoring.%20However%2C%20driven%20by%20advances%20in%20LMMs%2C%20it%20is%20now%0Aprogressing%20toward%20more%20holistic%20visual%20quality%20understanding%20tasks.%20Recent%0Astudies%20in%20the%20image%20domain%20have%20demonstrated%20that%20Visual%20Question%20Answering%0A%28VQA%29%20can%20markedly%20enhance%20low-level%20visual%20quality%20evaluation.%20Nevertheless%2C%0Arelated%20work%20has%20not%20been%20explored%20in%20the%20video%20domain%2C%20leaving%20substantial%0Aroom%20for%20improvement.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20VQA2%20Instruction%0ADataset%20-%20the%20first%20visual%20question%20answering%20instruction%20dataset%20that%20focuses%0Aon%20video%20quality%20assessment.%20This%20dataset%20consists%20of%203%20subsets%20and%20covers%0Avarious%20video%20types%2C%20containing%20157%2C755%20instruction%20question-answer%20pairs.%0AThen%2C%20leveraging%20this%20foundation%2C%20we%20present%20the%20VQA2%20series%20models.%20The%20VQA2%0Aseries%20models%20interleave%20visual%20and%20motion%20tokens%20to%20enhance%20the%20perception%20of%0Aspatial-temporal%20quality%20details%20in%20videos.%20We%20conduct%20extensive%20experiments%20on%0Avideo%20quality%20scoring%20and%20understanding%20tasks%2C%20and%20results%20demonstrate%20that%20the%0AVQA2series%20models%20achieve%20excellent%20performance%20in%20both%20tasks.%20Notably%2C%20our%0Afinal%20model%2C%20the%20VQA2-Assistant%2C%20exceeds%20the%20renowned%20GPT-4o%20in%20visual%20quality%0Aunderstanding%20tasks%20while%20maintaining%20strong%20competitiveness%20in%20quality%20scoring%0Atasks.%20Our%20work%20provides%20a%20foundation%20and%20feasible%20approach%20for%20integrating%0Alow-level%20video%20quality%20assessment%20and%20understanding%20with%20LMMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03795v4&entry.124074799=Read"},
{"title": "Revisiting MAE pre-training for 3D medical image segmentation", "author": "Tassilo Wald and Constantin Ulrich and Stanislav Lukyanenko and Andrei Goncharov and Alberto Paderno and Leander Maerkisch and Paul F. J\u00e4ger and Klaus Maier-Hein", "abstract": "  Self-Supervised Learning (SSL) presents an exciting opportunity to unlock the\npotential of vast, untapped clinical datasets, for various downstream\napplications that suffer from the scarcity of labeled data. While SSL has\nrevolutionized fields like natural language processing and computer vision, its\nadoption in 3D medical image computing has been limited by three key pitfalls:\nSmall pre-training dataset sizes, architectures inadequate for 3D medical image\nanalysis, and insufficient evaluation practices. In this paper, we address\nthese issues by i) leveraging a large-scale dataset of 39k 3D brain MRI volumes\nand ii) using a Residual Encoder U-Net architecture within the state-of-the-art\nnnU-Net framework. iii) A robust development framework, incorporating 5\ndevelopment and 8 testing brain MRI segmentation datasets, allowed\nperformance-driven design decisions to optimize the simple concept of Masked\nAuto Encoders (MAEs) for 3D CNNs. The resulting model not only surpasses\nprevious SSL methods but also outperforms the strong nnU-Net baseline by an\naverage of approximately 3 Dice points setting a new state-of-the-art. Our code\nand models are made available here.\n", "link": "http://arxiv.org/abs/2410.23132v2", "date": "2024-12-02", "relevancy": 2.2664, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6167}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5606}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20MAE%20pre-training%20for%203D%20medical%20image%20segmentation&body=Title%3A%20Revisiting%20MAE%20pre-training%20for%203D%20medical%20image%20segmentation%0AAuthor%3A%20Tassilo%20Wald%20and%20Constantin%20Ulrich%20and%20Stanislav%20Lukyanenko%20and%20Andrei%20Goncharov%20and%20Alberto%20Paderno%20and%20Leander%20Maerkisch%20and%20Paul%20F.%20J%C3%A4ger%20and%20Klaus%20Maier-Hein%0AAbstract%3A%20%20%20Self-Supervised%20Learning%20%28SSL%29%20presents%20an%20exciting%20opportunity%20to%20unlock%20the%0Apotential%20of%20vast%2C%20untapped%20clinical%20datasets%2C%20for%20various%20downstream%0Aapplications%20that%20suffer%20from%20the%20scarcity%20of%20labeled%20data.%20While%20SSL%20has%0Arevolutionized%20fields%20like%20natural%20language%20processing%20and%20computer%20vision%2C%20its%0Aadoption%20in%203D%20medical%20image%20computing%20has%20been%20limited%20by%20three%20key%20pitfalls%3A%0ASmall%20pre-training%20dataset%20sizes%2C%20architectures%20inadequate%20for%203D%20medical%20image%0Aanalysis%2C%20and%20insufficient%20evaluation%20practices.%20In%20this%20paper%2C%20we%20address%0Athese%20issues%20by%20i%29%20leveraging%20a%20large-scale%20dataset%20of%2039k%203D%20brain%20MRI%20volumes%0Aand%20ii%29%20using%20a%20Residual%20Encoder%20U-Net%20architecture%20within%20the%20state-of-the-art%0AnnU-Net%20framework.%20iii%29%20A%20robust%20development%20framework%2C%20incorporating%205%0Adevelopment%20and%208%20testing%20brain%20MRI%20segmentation%20datasets%2C%20allowed%0Aperformance-driven%20design%20decisions%20to%20optimize%20the%20simple%20concept%20of%20Masked%0AAuto%20Encoders%20%28MAEs%29%20for%203D%20CNNs.%20The%20resulting%20model%20not%20only%20surpasses%0Aprevious%20SSL%20methods%20but%20also%20outperforms%20the%20strong%20nnU-Net%20baseline%20by%20an%0Aaverage%20of%20approximately%203%20Dice%20points%20setting%20a%20new%20state-of-the-art.%20Our%20code%0Aand%20models%20are%20made%20available%20here.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23132v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520MAE%2520pre-training%2520for%25203D%2520medical%2520image%2520segmentation%26entry.906535625%3DTassilo%2520Wald%2520and%2520Constantin%2520Ulrich%2520and%2520Stanislav%2520Lukyanenko%2520and%2520Andrei%2520Goncharov%2520and%2520Alberto%2520Paderno%2520and%2520Leander%2520Maerkisch%2520and%2520Paul%2520F.%2520J%25C3%25A4ger%2520and%2520Klaus%2520Maier-Hein%26entry.1292438233%3D%2520%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520presents%2520an%2520exciting%2520opportunity%2520to%2520unlock%2520the%250Apotential%2520of%2520vast%252C%2520untapped%2520clinical%2520datasets%252C%2520for%2520various%2520downstream%250Aapplications%2520that%2520suffer%2520from%2520the%2520scarcity%2520of%2520labeled%2520data.%2520While%2520SSL%2520has%250Arevolutionized%2520fields%2520like%2520natural%2520language%2520processing%2520and%2520computer%2520vision%252C%2520its%250Aadoption%2520in%25203D%2520medical%2520image%2520computing%2520has%2520been%2520limited%2520by%2520three%2520key%2520pitfalls%253A%250ASmall%2520pre-training%2520dataset%2520sizes%252C%2520architectures%2520inadequate%2520for%25203D%2520medical%2520image%250Aanalysis%252C%2520and%2520insufficient%2520evaluation%2520practices.%2520In%2520this%2520paper%252C%2520we%2520address%250Athese%2520issues%2520by%2520i%2529%2520leveraging%2520a%2520large-scale%2520dataset%2520of%252039k%25203D%2520brain%2520MRI%2520volumes%250Aand%2520ii%2529%2520using%2520a%2520Residual%2520Encoder%2520U-Net%2520architecture%2520within%2520the%2520state-of-the-art%250AnnU-Net%2520framework.%2520iii%2529%2520A%2520robust%2520development%2520framework%252C%2520incorporating%25205%250Adevelopment%2520and%25208%2520testing%2520brain%2520MRI%2520segmentation%2520datasets%252C%2520allowed%250Aperformance-driven%2520design%2520decisions%2520to%2520optimize%2520the%2520simple%2520concept%2520of%2520Masked%250AAuto%2520Encoders%2520%2528MAEs%2529%2520for%25203D%2520CNNs.%2520The%2520resulting%2520model%2520not%2520only%2520surpasses%250Aprevious%2520SSL%2520methods%2520but%2520also%2520outperforms%2520the%2520strong%2520nnU-Net%2520baseline%2520by%2520an%250Aaverage%2520of%2520approximately%25203%2520Dice%2520points%2520setting%2520a%2520new%2520state-of-the-art.%2520Our%2520code%250Aand%2520models%2520are%2520made%2520available%2520here.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23132v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20MAE%20pre-training%20for%203D%20medical%20image%20segmentation&entry.906535625=Tassilo%20Wald%20and%20Constantin%20Ulrich%20and%20Stanislav%20Lukyanenko%20and%20Andrei%20Goncharov%20and%20Alberto%20Paderno%20and%20Leander%20Maerkisch%20and%20Paul%20F.%20J%C3%A4ger%20and%20Klaus%20Maier-Hein&entry.1292438233=%20%20Self-Supervised%20Learning%20%28SSL%29%20presents%20an%20exciting%20opportunity%20to%20unlock%20the%0Apotential%20of%20vast%2C%20untapped%20clinical%20datasets%2C%20for%20various%20downstream%0Aapplications%20that%20suffer%20from%20the%20scarcity%20of%20labeled%20data.%20While%20SSL%20has%0Arevolutionized%20fields%20like%20natural%20language%20processing%20and%20computer%20vision%2C%20its%0Aadoption%20in%203D%20medical%20image%20computing%20has%20been%20limited%20by%20three%20key%20pitfalls%3A%0ASmall%20pre-training%20dataset%20sizes%2C%20architectures%20inadequate%20for%203D%20medical%20image%0Aanalysis%2C%20and%20insufficient%20evaluation%20practices.%20In%20this%20paper%2C%20we%20address%0Athese%20issues%20by%20i%29%20leveraging%20a%20large-scale%20dataset%20of%2039k%203D%20brain%20MRI%20volumes%0Aand%20ii%29%20using%20a%20Residual%20Encoder%20U-Net%20architecture%20within%20the%20state-of-the-art%0AnnU-Net%20framework.%20iii%29%20A%20robust%20development%20framework%2C%20incorporating%205%0Adevelopment%20and%208%20testing%20brain%20MRI%20segmentation%20datasets%2C%20allowed%0Aperformance-driven%20design%20decisions%20to%20optimize%20the%20simple%20concept%20of%20Masked%0AAuto%20Encoders%20%28MAEs%29%20for%203D%20CNNs.%20The%20resulting%20model%20not%20only%20surpasses%0Aprevious%20SSL%20methods%20but%20also%20outperforms%20the%20strong%20nnU-Net%20baseline%20by%20an%0Aaverage%20of%20approximately%203%20Dice%20points%20setting%20a%20new%20state-of-the-art.%20Our%20code%0Aand%20models%20are%20made%20available%20here.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23132v2&entry.124074799=Read"},
{"title": "Pixel-aligned RGB-NIR Stereo Imaging and Dataset for Robot Vision", "author": "Jinnyeong Kim and Seung-Hwan Baek", "abstract": "  Integrating RGB and NIR stereo imaging provides complementary spectral\ninformation, potentially enhancing robotic 3D vision in challenging lighting\nconditions. However, existing datasets and imaging systems lack pixel-level\nalignment between RGB and NIR images, posing challenges for downstream vision\ntasks. In this paper, we introduce a robotic vision system equipped with\npixel-aligned RGB-NIR stereo cameras and a LiDAR sensor mounted on a mobile\nrobot. The system simultaneously captures pixel-aligned pairs of RGB stereo\nimages, NIR stereo images, and temporally synchronized LiDAR points. Utilizing\nthe mobility of the robot, we present a dataset containing continuous video\nframes under diverse lighting conditions. We then introduce two methods that\nutilize the pixel-aligned RGB-NIR images: an RGB-NIR image fusion method and a\nfeature fusion method. The first approach enables existing RGB-pretrained\nvision models to directly utilize RGB-NIR information without fine-tuning. The\nsecond approach fine-tunes existing vision models to more effectively utilize\nRGB-NIR information. Experimental results demonstrate the effectiveness of\nusing pixel-aligned RGB-NIR images across diverse lighting conditions.\n", "link": "http://arxiv.org/abs/2411.18025v2", "date": "2024-12-02", "relevancy": 2.2485, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5744}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.555}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pixel-aligned%20RGB-NIR%20Stereo%20Imaging%20and%20Dataset%20for%20Robot%20Vision&body=Title%3A%20Pixel-aligned%20RGB-NIR%20Stereo%20Imaging%20and%20Dataset%20for%20Robot%20Vision%0AAuthor%3A%20Jinnyeong%20Kim%20and%20Seung-Hwan%20Baek%0AAbstract%3A%20%20%20Integrating%20RGB%20and%20NIR%20stereo%20imaging%20provides%20complementary%20spectral%0Ainformation%2C%20potentially%20enhancing%20robotic%203D%20vision%20in%20challenging%20lighting%0Aconditions.%20However%2C%20existing%20datasets%20and%20imaging%20systems%20lack%20pixel-level%0Aalignment%20between%20RGB%20and%20NIR%20images%2C%20posing%20challenges%20for%20downstream%20vision%0Atasks.%20In%20this%20paper%2C%20we%20introduce%20a%20robotic%20vision%20system%20equipped%20with%0Apixel-aligned%20RGB-NIR%20stereo%20cameras%20and%20a%20LiDAR%20sensor%20mounted%20on%20a%20mobile%0Arobot.%20The%20system%20simultaneously%20captures%20pixel-aligned%20pairs%20of%20RGB%20stereo%0Aimages%2C%20NIR%20stereo%20images%2C%20and%20temporally%20synchronized%20LiDAR%20points.%20Utilizing%0Athe%20mobility%20of%20the%20robot%2C%20we%20present%20a%20dataset%20containing%20continuous%20video%0Aframes%20under%20diverse%20lighting%20conditions.%20We%20then%20introduce%20two%20methods%20that%0Autilize%20the%20pixel-aligned%20RGB-NIR%20images%3A%20an%20RGB-NIR%20image%20fusion%20method%20and%20a%0Afeature%20fusion%20method.%20The%20first%20approach%20enables%20existing%20RGB-pretrained%0Avision%20models%20to%20directly%20utilize%20RGB-NIR%20information%20without%20fine-tuning.%20The%0Asecond%20approach%20fine-tunes%20existing%20vision%20models%20to%20more%20effectively%20utilize%0ARGB-NIR%20information.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%0Ausing%20pixel-aligned%20RGB-NIR%20images%20across%20diverse%20lighting%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18025v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixel-aligned%2520RGB-NIR%2520Stereo%2520Imaging%2520and%2520Dataset%2520for%2520Robot%2520Vision%26entry.906535625%3DJinnyeong%2520Kim%2520and%2520Seung-Hwan%2520Baek%26entry.1292438233%3D%2520%2520Integrating%2520RGB%2520and%2520NIR%2520stereo%2520imaging%2520provides%2520complementary%2520spectral%250Ainformation%252C%2520potentially%2520enhancing%2520robotic%25203D%2520vision%2520in%2520challenging%2520lighting%250Aconditions.%2520However%252C%2520existing%2520datasets%2520and%2520imaging%2520systems%2520lack%2520pixel-level%250Aalignment%2520between%2520RGB%2520and%2520NIR%2520images%252C%2520posing%2520challenges%2520for%2520downstream%2520vision%250Atasks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520robotic%2520vision%2520system%2520equipped%2520with%250Apixel-aligned%2520RGB-NIR%2520stereo%2520cameras%2520and%2520a%2520LiDAR%2520sensor%2520mounted%2520on%2520a%2520mobile%250Arobot.%2520The%2520system%2520simultaneously%2520captures%2520pixel-aligned%2520pairs%2520of%2520RGB%2520stereo%250Aimages%252C%2520NIR%2520stereo%2520images%252C%2520and%2520temporally%2520synchronized%2520LiDAR%2520points.%2520Utilizing%250Athe%2520mobility%2520of%2520the%2520robot%252C%2520we%2520present%2520a%2520dataset%2520containing%2520continuous%2520video%250Aframes%2520under%2520diverse%2520lighting%2520conditions.%2520We%2520then%2520introduce%2520two%2520methods%2520that%250Autilize%2520the%2520pixel-aligned%2520RGB-NIR%2520images%253A%2520an%2520RGB-NIR%2520image%2520fusion%2520method%2520and%2520a%250Afeature%2520fusion%2520method.%2520The%2520first%2520approach%2520enables%2520existing%2520RGB-pretrained%250Avision%2520models%2520to%2520directly%2520utilize%2520RGB-NIR%2520information%2520without%2520fine-tuning.%2520The%250Asecond%2520approach%2520fine-tunes%2520existing%2520vision%2520models%2520to%2520more%2520effectively%2520utilize%250ARGB-NIR%2520information.%2520Experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%250Ausing%2520pixel-aligned%2520RGB-NIR%2520images%2520across%2520diverse%2520lighting%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18025v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixel-aligned%20RGB-NIR%20Stereo%20Imaging%20and%20Dataset%20for%20Robot%20Vision&entry.906535625=Jinnyeong%20Kim%20and%20Seung-Hwan%20Baek&entry.1292438233=%20%20Integrating%20RGB%20and%20NIR%20stereo%20imaging%20provides%20complementary%20spectral%0Ainformation%2C%20potentially%20enhancing%20robotic%203D%20vision%20in%20challenging%20lighting%0Aconditions.%20However%2C%20existing%20datasets%20and%20imaging%20systems%20lack%20pixel-level%0Aalignment%20between%20RGB%20and%20NIR%20images%2C%20posing%20challenges%20for%20downstream%20vision%0Atasks.%20In%20this%20paper%2C%20we%20introduce%20a%20robotic%20vision%20system%20equipped%20with%0Apixel-aligned%20RGB-NIR%20stereo%20cameras%20and%20a%20LiDAR%20sensor%20mounted%20on%20a%20mobile%0Arobot.%20The%20system%20simultaneously%20captures%20pixel-aligned%20pairs%20of%20RGB%20stereo%0Aimages%2C%20NIR%20stereo%20images%2C%20and%20temporally%20synchronized%20LiDAR%20points.%20Utilizing%0Athe%20mobility%20of%20the%20robot%2C%20we%20present%20a%20dataset%20containing%20continuous%20video%0Aframes%20under%20diverse%20lighting%20conditions.%20We%20then%20introduce%20two%20methods%20that%0Autilize%20the%20pixel-aligned%20RGB-NIR%20images%3A%20an%20RGB-NIR%20image%20fusion%20method%20and%20a%0Afeature%20fusion%20method.%20The%20first%20approach%20enables%20existing%20RGB-pretrained%0Avision%20models%20to%20directly%20utilize%20RGB-NIR%20information%20without%20fine-tuning.%20The%0Asecond%20approach%20fine-tunes%20existing%20vision%20models%20to%20more%20effectively%20utilize%0ARGB-NIR%20information.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%0Ausing%20pixel-aligned%20RGB-NIR%20images%20across%20diverse%20lighting%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18025v2&entry.124074799=Read"},
{"title": "Constraining Generative Models for Engineering Design with Negative Data", "author": "Lyle Regenwetter and Giorgio Giannone and Akash Srivastava and Dan Gutfreund and Faez Ahmed", "abstract": "  Generative models have recently achieved remarkable success and widespread\nadoption in society, yet they often struggle to generate realistic and accurate\noutputs. This challenge extends beyond language and vision into fields like\nengineering design, where safety-critical engineering standards and\nnon-negotiable physical laws tightly constrain what outputs are considered\nacceptable. In this work, we introduce a novel training method to guide a\ngenerative model toward constraint-satisfying outputs using `negative data' --\nexamples of what to avoid. Our negative-data generative model (NDGM)\nformulation easily outperforms classic models, generating 1/6 as many\nconstraint-violating samples using 1/8 as much data in certain problems. It\nalso consistently outperforms other baselines, achieving a balance between\nconstraint satisfaction and distributional similarity that is unsurpassed by\nany other model in 12 of the 14 problems tested. This widespread superiority is\nrigorously demonstrated across numerous synthetic tests and real engineering\nproblems, such as ship hull synthesis with hydrodynamic constraints and vehicle\ndesign with impact safety constraints. Our benchmarks showcase both the\nbest-in-class performance of our new NDGM formulation and the overall dominance\nof NDGMs versus classic generative models. We publicly release the code and\nbenchmarks at https://github.com/Lyleregenwetter/NDGMs.\n", "link": "http://arxiv.org/abs/2306.15166v2", "date": "2024-12-02", "relevancy": 2.1989, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5761}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5449}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constraining%20Generative%20Models%20for%20Engineering%20Design%20with%20Negative%20Data&body=Title%3A%20Constraining%20Generative%20Models%20for%20Engineering%20Design%20with%20Negative%20Data%0AAuthor%3A%20Lyle%20Regenwetter%20and%20Giorgio%20Giannone%20and%20Akash%20Srivastava%20and%20Dan%20Gutfreund%20and%20Faez%20Ahmed%0AAbstract%3A%20%20%20Generative%20models%20have%20recently%20achieved%20remarkable%20success%20and%20widespread%0Aadoption%20in%20society%2C%20yet%20they%20often%20struggle%20to%20generate%20realistic%20and%20accurate%0Aoutputs.%20This%20challenge%20extends%20beyond%20language%20and%20vision%20into%20fields%20like%0Aengineering%20design%2C%20where%20safety-critical%20engineering%20standards%20and%0Anon-negotiable%20physical%20laws%20tightly%20constrain%20what%20outputs%20are%20considered%0Aacceptable.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20training%20method%20to%20guide%20a%0Agenerative%20model%20toward%20constraint-satisfying%20outputs%20using%20%60negative%20data%27%20--%0Aexamples%20of%20what%20to%20avoid.%20Our%20negative-data%20generative%20model%20%28NDGM%29%0Aformulation%20easily%20outperforms%20classic%20models%2C%20generating%201/6%20as%20many%0Aconstraint-violating%20samples%20using%201/8%20as%20much%20data%20in%20certain%20problems.%20It%0Aalso%20consistently%20outperforms%20other%20baselines%2C%20achieving%20a%20balance%20between%0Aconstraint%20satisfaction%20and%20distributional%20similarity%20that%20is%20unsurpassed%20by%0Aany%20other%20model%20in%2012%20of%20the%2014%20problems%20tested.%20This%20widespread%20superiority%20is%0Arigorously%20demonstrated%20across%20numerous%20synthetic%20tests%20and%20real%20engineering%0Aproblems%2C%20such%20as%20ship%20hull%20synthesis%20with%20hydrodynamic%20constraints%20and%20vehicle%0Adesign%20with%20impact%20safety%20constraints.%20Our%20benchmarks%20showcase%20both%20the%0Abest-in-class%20performance%20of%20our%20new%20NDGM%20formulation%20and%20the%20overall%20dominance%0Aof%20NDGMs%20versus%20classic%20generative%20models.%20We%20publicly%20release%20the%20code%20and%0Abenchmarks%20at%20https%3A//github.com/Lyleregenwetter/NDGMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.15166v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstraining%2520Generative%2520Models%2520for%2520Engineering%2520Design%2520with%2520Negative%2520Data%26entry.906535625%3DLyle%2520Regenwetter%2520and%2520Giorgio%2520Giannone%2520and%2520Akash%2520Srivastava%2520and%2520Dan%2520Gutfreund%2520and%2520Faez%2520Ahmed%26entry.1292438233%3D%2520%2520Generative%2520models%2520have%2520recently%2520achieved%2520remarkable%2520success%2520and%2520widespread%250Aadoption%2520in%2520society%252C%2520yet%2520they%2520often%2520struggle%2520to%2520generate%2520realistic%2520and%2520accurate%250Aoutputs.%2520This%2520challenge%2520extends%2520beyond%2520language%2520and%2520vision%2520into%2520fields%2520like%250Aengineering%2520design%252C%2520where%2520safety-critical%2520engineering%2520standards%2520and%250Anon-negotiable%2520physical%2520laws%2520tightly%2520constrain%2520what%2520outputs%2520are%2520considered%250Aacceptable.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520training%2520method%2520to%2520guide%2520a%250Agenerative%2520model%2520toward%2520constraint-satisfying%2520outputs%2520using%2520%2560negative%2520data%2527%2520--%250Aexamples%2520of%2520what%2520to%2520avoid.%2520Our%2520negative-data%2520generative%2520model%2520%2528NDGM%2529%250Aformulation%2520easily%2520outperforms%2520classic%2520models%252C%2520generating%25201/6%2520as%2520many%250Aconstraint-violating%2520samples%2520using%25201/8%2520as%2520much%2520data%2520in%2520certain%2520problems.%2520It%250Aalso%2520consistently%2520outperforms%2520other%2520baselines%252C%2520achieving%2520a%2520balance%2520between%250Aconstraint%2520satisfaction%2520and%2520distributional%2520similarity%2520that%2520is%2520unsurpassed%2520by%250Aany%2520other%2520model%2520in%252012%2520of%2520the%252014%2520problems%2520tested.%2520This%2520widespread%2520superiority%2520is%250Arigorously%2520demonstrated%2520across%2520numerous%2520synthetic%2520tests%2520and%2520real%2520engineering%250Aproblems%252C%2520such%2520as%2520ship%2520hull%2520synthesis%2520with%2520hydrodynamic%2520constraints%2520and%2520vehicle%250Adesign%2520with%2520impact%2520safety%2520constraints.%2520Our%2520benchmarks%2520showcase%2520both%2520the%250Abest-in-class%2520performance%2520of%2520our%2520new%2520NDGM%2520formulation%2520and%2520the%2520overall%2520dominance%250Aof%2520NDGMs%2520versus%2520classic%2520generative%2520models.%2520We%2520publicly%2520release%2520the%2520code%2520and%250Abenchmarks%2520at%2520https%253A//github.com/Lyleregenwetter/NDGMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.15166v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constraining%20Generative%20Models%20for%20Engineering%20Design%20with%20Negative%20Data&entry.906535625=Lyle%20Regenwetter%20and%20Giorgio%20Giannone%20and%20Akash%20Srivastava%20and%20Dan%20Gutfreund%20and%20Faez%20Ahmed&entry.1292438233=%20%20Generative%20models%20have%20recently%20achieved%20remarkable%20success%20and%20widespread%0Aadoption%20in%20society%2C%20yet%20they%20often%20struggle%20to%20generate%20realistic%20and%20accurate%0Aoutputs.%20This%20challenge%20extends%20beyond%20language%20and%20vision%20into%20fields%20like%0Aengineering%20design%2C%20where%20safety-critical%20engineering%20standards%20and%0Anon-negotiable%20physical%20laws%20tightly%20constrain%20what%20outputs%20are%20considered%0Aacceptable.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20training%20method%20to%20guide%20a%0Agenerative%20model%20toward%20constraint-satisfying%20outputs%20using%20%60negative%20data%27%20--%0Aexamples%20of%20what%20to%20avoid.%20Our%20negative-data%20generative%20model%20%28NDGM%29%0Aformulation%20easily%20outperforms%20classic%20models%2C%20generating%201/6%20as%20many%0Aconstraint-violating%20samples%20using%201/8%20as%20much%20data%20in%20certain%20problems.%20It%0Aalso%20consistently%20outperforms%20other%20baselines%2C%20achieving%20a%20balance%20between%0Aconstraint%20satisfaction%20and%20distributional%20similarity%20that%20is%20unsurpassed%20by%0Aany%20other%20model%20in%2012%20of%20the%2014%20problems%20tested.%20This%20widespread%20superiority%20is%0Arigorously%20demonstrated%20across%20numerous%20synthetic%20tests%20and%20real%20engineering%0Aproblems%2C%20such%20as%20ship%20hull%20synthesis%20with%20hydrodynamic%20constraints%20and%20vehicle%0Adesign%20with%20impact%20safety%20constraints.%20Our%20benchmarks%20showcase%20both%20the%0Abest-in-class%20performance%20of%20our%20new%20NDGM%20formulation%20and%20the%20overall%20dominance%0Aof%20NDGMs%20versus%20classic%20generative%20models.%20We%20publicly%20release%20the%20code%20and%0Abenchmarks%20at%20https%3A//github.com/Lyleregenwetter/NDGMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.15166v2&entry.124074799=Read"},
{"title": "End-to-End Supervised Hierarchical Graph Clustering for Speaker\n  Diarization", "author": "Prachi Singh and Sriram Ganapathy", "abstract": "  Speaker diarization, the task of segmenting an audio recording based on\nspeaker identity, constitutes an important speech pre-processing step for\nseveral downstream applications.The conventional approach to diarization\ninvolves multiple steps of embedding extraction and clustering, which are often\noptimized in an isolated fashion. While end-to-end diarization systems attempt\nto learn a single model for the task, they are often cumbersome to train and\nrequire large supervised datasets. In this paper, we propose an end-to-end\nsupervised hierarchical clustering algorithm based on graph neural networks\n(GNN), called End-to-end Supervised HierARchical Clustering (E-SHARC). The\nembedding extractor is initialized using a pre-trained x-vector model while the\nGNN model is trained initially using the x-vector embeddings from the\npre-trained model. Finally, the E-SHARC model uses the front-end mel-filterbank\nfeatures as input and jointly optimizes the embedding extractor and the GNN\nclustering module, performing representation learning, metric learning, and\nclustering with end-to-end optimization. Further, with additional inputs from\nan external overlap detector, the E-SHARC approach is capable of predicting the\nspeakers in the overlapping speech regions. The experimental evaluation on\nbenchmark datasets like AMI, Voxconverse and DISPLACE, illustrates that the\nproposed E-SHARC framework provides competitive diarization results using graph\nbased clustering methods.\n", "link": "http://arxiv.org/abs/2401.12850v2", "date": "2024-12-02", "relevancy": 2.1929, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.446}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4349}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-End%20Supervised%20Hierarchical%20Graph%20Clustering%20for%20Speaker%0A%20%20Diarization&body=Title%3A%20End-to-End%20Supervised%20Hierarchical%20Graph%20Clustering%20for%20Speaker%0A%20%20Diarization%0AAuthor%3A%20Prachi%20Singh%20and%20Sriram%20Ganapathy%0AAbstract%3A%20%20%20Speaker%20diarization%2C%20the%20task%20of%20segmenting%20an%20audio%20recording%20based%20on%0Aspeaker%20identity%2C%20constitutes%20an%20important%20speech%20pre-processing%20step%20for%0Aseveral%20downstream%20applications.The%20conventional%20approach%20to%20diarization%0Ainvolves%20multiple%20steps%20of%20embedding%20extraction%20and%20clustering%2C%20which%20are%20often%0Aoptimized%20in%20an%20isolated%20fashion.%20While%20end-to-end%20diarization%20systems%20attempt%0Ato%20learn%20a%20single%20model%20for%20the%20task%2C%20they%20are%20often%20cumbersome%20to%20train%20and%0Arequire%20large%20supervised%20datasets.%20In%20this%20paper%2C%20we%20propose%20an%20end-to-end%0Asupervised%20hierarchical%20clustering%20algorithm%20based%20on%20graph%20neural%20networks%0A%28GNN%29%2C%20called%20End-to-end%20Supervised%20HierARchical%20Clustering%20%28E-SHARC%29.%20The%0Aembedding%20extractor%20is%20initialized%20using%20a%20pre-trained%20x-vector%20model%20while%20the%0AGNN%20model%20is%20trained%20initially%20using%20the%20x-vector%20embeddings%20from%20the%0Apre-trained%20model.%20Finally%2C%20the%20E-SHARC%20model%20uses%20the%20front-end%20mel-filterbank%0Afeatures%20as%20input%20and%20jointly%20optimizes%20the%20embedding%20extractor%20and%20the%20GNN%0Aclustering%20module%2C%20performing%20representation%20learning%2C%20metric%20learning%2C%20and%0Aclustering%20with%20end-to-end%20optimization.%20Further%2C%20with%20additional%20inputs%20from%0Aan%20external%20overlap%20detector%2C%20the%20E-SHARC%20approach%20is%20capable%20of%20predicting%20the%0Aspeakers%20in%20the%20overlapping%20speech%20regions.%20The%20experimental%20evaluation%20on%0Abenchmark%20datasets%20like%20AMI%2C%20Voxconverse%20and%20DISPLACE%2C%20illustrates%20that%20the%0Aproposed%20E-SHARC%20framework%20provides%20competitive%20diarization%20results%20using%20graph%0Abased%20clustering%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.12850v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-End%2520Supervised%2520Hierarchical%2520Graph%2520Clustering%2520for%2520Speaker%250A%2520%2520Diarization%26entry.906535625%3DPrachi%2520Singh%2520and%2520Sriram%2520Ganapathy%26entry.1292438233%3D%2520%2520Speaker%2520diarization%252C%2520the%2520task%2520of%2520segmenting%2520an%2520audio%2520recording%2520based%2520on%250Aspeaker%2520identity%252C%2520constitutes%2520an%2520important%2520speech%2520pre-processing%2520step%2520for%250Aseveral%2520downstream%2520applications.The%2520conventional%2520approach%2520to%2520diarization%250Ainvolves%2520multiple%2520steps%2520of%2520embedding%2520extraction%2520and%2520clustering%252C%2520which%2520are%2520often%250Aoptimized%2520in%2520an%2520isolated%2520fashion.%2520While%2520end-to-end%2520diarization%2520systems%2520attempt%250Ato%2520learn%2520a%2520single%2520model%2520for%2520the%2520task%252C%2520they%2520are%2520often%2520cumbersome%2520to%2520train%2520and%250Arequire%2520large%2520supervised%2520datasets.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520end-to-end%250Asupervised%2520hierarchical%2520clustering%2520algorithm%2520based%2520on%2520graph%2520neural%2520networks%250A%2528GNN%2529%252C%2520called%2520End-to-end%2520Supervised%2520HierARchical%2520Clustering%2520%2528E-SHARC%2529.%2520The%250Aembedding%2520extractor%2520is%2520initialized%2520using%2520a%2520pre-trained%2520x-vector%2520model%2520while%2520the%250AGNN%2520model%2520is%2520trained%2520initially%2520using%2520the%2520x-vector%2520embeddings%2520from%2520the%250Apre-trained%2520model.%2520Finally%252C%2520the%2520E-SHARC%2520model%2520uses%2520the%2520front-end%2520mel-filterbank%250Afeatures%2520as%2520input%2520and%2520jointly%2520optimizes%2520the%2520embedding%2520extractor%2520and%2520the%2520GNN%250Aclustering%2520module%252C%2520performing%2520representation%2520learning%252C%2520metric%2520learning%252C%2520and%250Aclustering%2520with%2520end-to-end%2520optimization.%2520Further%252C%2520with%2520additional%2520inputs%2520from%250Aan%2520external%2520overlap%2520detector%252C%2520the%2520E-SHARC%2520approach%2520is%2520capable%2520of%2520predicting%2520the%250Aspeakers%2520in%2520the%2520overlapping%2520speech%2520regions.%2520The%2520experimental%2520evaluation%2520on%250Abenchmark%2520datasets%2520like%2520AMI%252C%2520Voxconverse%2520and%2520DISPLACE%252C%2520illustrates%2520that%2520the%250Aproposed%2520E-SHARC%2520framework%2520provides%2520competitive%2520diarization%2520results%2520using%2520graph%250Abased%2520clustering%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.12850v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-End%20Supervised%20Hierarchical%20Graph%20Clustering%20for%20Speaker%0A%20%20Diarization&entry.906535625=Prachi%20Singh%20and%20Sriram%20Ganapathy&entry.1292438233=%20%20Speaker%20diarization%2C%20the%20task%20of%20segmenting%20an%20audio%20recording%20based%20on%0Aspeaker%20identity%2C%20constitutes%20an%20important%20speech%20pre-processing%20step%20for%0Aseveral%20downstream%20applications.The%20conventional%20approach%20to%20diarization%0Ainvolves%20multiple%20steps%20of%20embedding%20extraction%20and%20clustering%2C%20which%20are%20often%0Aoptimized%20in%20an%20isolated%20fashion.%20While%20end-to-end%20diarization%20systems%20attempt%0Ato%20learn%20a%20single%20model%20for%20the%20task%2C%20they%20are%20often%20cumbersome%20to%20train%20and%0Arequire%20large%20supervised%20datasets.%20In%20this%20paper%2C%20we%20propose%20an%20end-to-end%0Asupervised%20hierarchical%20clustering%20algorithm%20based%20on%20graph%20neural%20networks%0A%28GNN%29%2C%20called%20End-to-end%20Supervised%20HierARchical%20Clustering%20%28E-SHARC%29.%20The%0Aembedding%20extractor%20is%20initialized%20using%20a%20pre-trained%20x-vector%20model%20while%20the%0AGNN%20model%20is%20trained%20initially%20using%20the%20x-vector%20embeddings%20from%20the%0Apre-trained%20model.%20Finally%2C%20the%20E-SHARC%20model%20uses%20the%20front-end%20mel-filterbank%0Afeatures%20as%20input%20and%20jointly%20optimizes%20the%20embedding%20extractor%20and%20the%20GNN%0Aclustering%20module%2C%20performing%20representation%20learning%2C%20metric%20learning%2C%20and%0Aclustering%20with%20end-to-end%20optimization.%20Further%2C%20with%20additional%20inputs%20from%0Aan%20external%20overlap%20detector%2C%20the%20E-SHARC%20approach%20is%20capable%20of%20predicting%20the%0Aspeakers%20in%20the%20overlapping%20speech%20regions.%20The%20experimental%20evaluation%20on%0Abenchmark%20datasets%20like%20AMI%2C%20Voxconverse%20and%20DISPLACE%2C%20illustrates%20that%20the%0Aproposed%20E-SHARC%20framework%20provides%20competitive%20diarization%20results%20using%20graph%0Abased%20clustering%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.12850v2&entry.124074799=Read"},
{"title": "ConvMixFormer- A Resource-efficient Convolution Mixer for\n  Transformer-based Dynamic Hand Gesture Recognition", "author": "Mallika Garg and Debashis Ghosh and Pyari Mohan Pradhan", "abstract": "  Transformer models have demonstrated remarkable success in many domains such\nas natural language processing (NLP) and computer vision. With the growing\ninterest in transformer-based architectures, they are now utilized for gesture\nrecognition. So, we also explore and devise a novel ConvMixFormer architecture\nfor dynamic hand gestures. The transformers use quadratic scaling of the\nattention features with the sequential data, due to which these models are\ncomputationally complex and heavy. We have considered this drawback of the\ntransformer and designed a resource-efficient model that replaces the\nself-attention in the transformer with the simple convolutional layer-based\ntoken mixer. The computational cost and the parameters used for the\nconvolution-based mixer are comparatively less than the quadratic\nself-attention. Convolution-mixer helps the model capture the local spatial\nfeatures that self-attention struggles to capture due to their sequential\nprocessing nature. Further, an efficient gate mechanism is employed instead of\na conventional feed-forward network in the transformer to help the model\ncontrol the flow of features within different stages of the proposed model.\nThis design uses fewer learnable parameters which is nearly half the vanilla\ntransformer that helps in fast and efficient training. The proposed method is\nevaluated on NVidia Dynamic Hand Gesture and Briareo datasets and our model has\nachieved state-of-the-art results on single and multimodal inputs. We have also\nshown the parameter efficiency of the proposed ConvMixFormer model compared to\nother methods. The source code is available at\nhttps://github.com/mallikagarg/ConvMixFormer.\n", "link": "http://arxiv.org/abs/2411.07118v3", "date": "2024-12-02", "relevancy": 2.1553, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5562}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.539}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConvMixFormer-%20A%20Resource-efficient%20Convolution%20Mixer%20for%0A%20%20Transformer-based%20Dynamic%20Hand%20Gesture%20Recognition&body=Title%3A%20ConvMixFormer-%20A%20Resource-efficient%20Convolution%20Mixer%20for%0A%20%20Transformer-based%20Dynamic%20Hand%20Gesture%20Recognition%0AAuthor%3A%20Mallika%20Garg%20and%20Debashis%20Ghosh%20and%20Pyari%20Mohan%20Pradhan%0AAbstract%3A%20%20%20Transformer%20models%20have%20demonstrated%20remarkable%20success%20in%20many%20domains%20such%0Aas%20natural%20language%20processing%20%28NLP%29%20and%20computer%20vision.%20With%20the%20growing%0Ainterest%20in%20transformer-based%20architectures%2C%20they%20are%20now%20utilized%20for%20gesture%0Arecognition.%20So%2C%20we%20also%20explore%20and%20devise%20a%20novel%20ConvMixFormer%20architecture%0Afor%20dynamic%20hand%20gestures.%20The%20transformers%20use%20quadratic%20scaling%20of%20the%0Aattention%20features%20with%20the%20sequential%20data%2C%20due%20to%20which%20these%20models%20are%0Acomputationally%20complex%20and%20heavy.%20We%20have%20considered%20this%20drawback%20of%20the%0Atransformer%20and%20designed%20a%20resource-efficient%20model%20that%20replaces%20the%0Aself-attention%20in%20the%20transformer%20with%20the%20simple%20convolutional%20layer-based%0Atoken%20mixer.%20The%20computational%20cost%20and%20the%20parameters%20used%20for%20the%0Aconvolution-based%20mixer%20are%20comparatively%20less%20than%20the%20quadratic%0Aself-attention.%20Convolution-mixer%20helps%20the%20model%20capture%20the%20local%20spatial%0Afeatures%20that%20self-attention%20struggles%20to%20capture%20due%20to%20their%20sequential%0Aprocessing%20nature.%20Further%2C%20an%20efficient%20gate%20mechanism%20is%20employed%20instead%20of%0Aa%20conventional%20feed-forward%20network%20in%20the%20transformer%20to%20help%20the%20model%0Acontrol%20the%20flow%20of%20features%20within%20different%20stages%20of%20the%20proposed%20model.%0AThis%20design%20uses%20fewer%20learnable%20parameters%20which%20is%20nearly%20half%20the%20vanilla%0Atransformer%20that%20helps%20in%20fast%20and%20efficient%20training.%20The%20proposed%20method%20is%0Aevaluated%20on%20NVidia%20Dynamic%20Hand%20Gesture%20and%20Briareo%20datasets%20and%20our%20model%20has%0Aachieved%20state-of-the-art%20results%20on%20single%20and%20multimodal%20inputs.%20We%20have%20also%0Ashown%20the%20parameter%20efficiency%20of%20the%20proposed%20ConvMixFormer%20model%20compared%20to%0Aother%20methods.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/mallikagarg/ConvMixFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07118v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvMixFormer-%2520A%2520Resource-efficient%2520Convolution%2520Mixer%2520for%250A%2520%2520Transformer-based%2520Dynamic%2520Hand%2520Gesture%2520Recognition%26entry.906535625%3DMallika%2520Garg%2520and%2520Debashis%2520Ghosh%2520and%2520Pyari%2520Mohan%2520Pradhan%26entry.1292438233%3D%2520%2520Transformer%2520models%2520have%2520demonstrated%2520remarkable%2520success%2520in%2520many%2520domains%2520such%250Aas%2520natural%2520language%2520processing%2520%2528NLP%2529%2520and%2520computer%2520vision.%2520With%2520the%2520growing%250Ainterest%2520in%2520transformer-based%2520architectures%252C%2520they%2520are%2520now%2520utilized%2520for%2520gesture%250Arecognition.%2520So%252C%2520we%2520also%2520explore%2520and%2520devise%2520a%2520novel%2520ConvMixFormer%2520architecture%250Afor%2520dynamic%2520hand%2520gestures.%2520The%2520transformers%2520use%2520quadratic%2520scaling%2520of%2520the%250Aattention%2520features%2520with%2520the%2520sequential%2520data%252C%2520due%2520to%2520which%2520these%2520models%2520are%250Acomputationally%2520complex%2520and%2520heavy.%2520We%2520have%2520considered%2520this%2520drawback%2520of%2520the%250Atransformer%2520and%2520designed%2520a%2520resource-efficient%2520model%2520that%2520replaces%2520the%250Aself-attention%2520in%2520the%2520transformer%2520with%2520the%2520simple%2520convolutional%2520layer-based%250Atoken%2520mixer.%2520The%2520computational%2520cost%2520and%2520the%2520parameters%2520used%2520for%2520the%250Aconvolution-based%2520mixer%2520are%2520comparatively%2520less%2520than%2520the%2520quadratic%250Aself-attention.%2520Convolution-mixer%2520helps%2520the%2520model%2520capture%2520the%2520local%2520spatial%250Afeatures%2520that%2520self-attention%2520struggles%2520to%2520capture%2520due%2520to%2520their%2520sequential%250Aprocessing%2520nature.%2520Further%252C%2520an%2520efficient%2520gate%2520mechanism%2520is%2520employed%2520instead%2520of%250Aa%2520conventional%2520feed-forward%2520network%2520in%2520the%2520transformer%2520to%2520help%2520the%2520model%250Acontrol%2520the%2520flow%2520of%2520features%2520within%2520different%2520stages%2520of%2520the%2520proposed%2520model.%250AThis%2520design%2520uses%2520fewer%2520learnable%2520parameters%2520which%2520is%2520nearly%2520half%2520the%2520vanilla%250Atransformer%2520that%2520helps%2520in%2520fast%2520and%2520efficient%2520training.%2520The%2520proposed%2520method%2520is%250Aevaluated%2520on%2520NVidia%2520Dynamic%2520Hand%2520Gesture%2520and%2520Briareo%2520datasets%2520and%2520our%2520model%2520has%250Aachieved%2520state-of-the-art%2520results%2520on%2520single%2520and%2520multimodal%2520inputs.%2520We%2520have%2520also%250Ashown%2520the%2520parameter%2520efficiency%2520of%2520the%2520proposed%2520ConvMixFormer%2520model%2520compared%2520to%250Aother%2520methods.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/mallikagarg/ConvMixFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07118v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConvMixFormer-%20A%20Resource-efficient%20Convolution%20Mixer%20for%0A%20%20Transformer-based%20Dynamic%20Hand%20Gesture%20Recognition&entry.906535625=Mallika%20Garg%20and%20Debashis%20Ghosh%20and%20Pyari%20Mohan%20Pradhan&entry.1292438233=%20%20Transformer%20models%20have%20demonstrated%20remarkable%20success%20in%20many%20domains%20such%0Aas%20natural%20language%20processing%20%28NLP%29%20and%20computer%20vision.%20With%20the%20growing%0Ainterest%20in%20transformer-based%20architectures%2C%20they%20are%20now%20utilized%20for%20gesture%0Arecognition.%20So%2C%20we%20also%20explore%20and%20devise%20a%20novel%20ConvMixFormer%20architecture%0Afor%20dynamic%20hand%20gestures.%20The%20transformers%20use%20quadratic%20scaling%20of%20the%0Aattention%20features%20with%20the%20sequential%20data%2C%20due%20to%20which%20these%20models%20are%0Acomputationally%20complex%20and%20heavy.%20We%20have%20considered%20this%20drawback%20of%20the%0Atransformer%20and%20designed%20a%20resource-efficient%20model%20that%20replaces%20the%0Aself-attention%20in%20the%20transformer%20with%20the%20simple%20convolutional%20layer-based%0Atoken%20mixer.%20The%20computational%20cost%20and%20the%20parameters%20used%20for%20the%0Aconvolution-based%20mixer%20are%20comparatively%20less%20than%20the%20quadratic%0Aself-attention.%20Convolution-mixer%20helps%20the%20model%20capture%20the%20local%20spatial%0Afeatures%20that%20self-attention%20struggles%20to%20capture%20due%20to%20their%20sequential%0Aprocessing%20nature.%20Further%2C%20an%20efficient%20gate%20mechanism%20is%20employed%20instead%20of%0Aa%20conventional%20feed-forward%20network%20in%20the%20transformer%20to%20help%20the%20model%0Acontrol%20the%20flow%20of%20features%20within%20different%20stages%20of%20the%20proposed%20model.%0AThis%20design%20uses%20fewer%20learnable%20parameters%20which%20is%20nearly%20half%20the%20vanilla%0Atransformer%20that%20helps%20in%20fast%20and%20efficient%20training.%20The%20proposed%20method%20is%0Aevaluated%20on%20NVidia%20Dynamic%20Hand%20Gesture%20and%20Briareo%20datasets%20and%20our%20model%20has%0Aachieved%20state-of-the-art%20results%20on%20single%20and%20multimodal%20inputs.%20We%20have%20also%0Ashown%20the%20parameter%20efficiency%20of%20the%20proposed%20ConvMixFormer%20model%20compared%20to%0Aother%20methods.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/mallikagarg/ConvMixFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07118v3&entry.124074799=Read"},
{"title": "QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection", "author": "Nouhaila Innan and Alberto Marchisio and Mohamed Bennai and Muhammad Shafique", "abstract": "  This study introduces the Quantum Federated Neural Network for Financial\nFraud Detection (QFNN-FFD), a cutting-edge framework merging Quantum Machine\nLearning (QML) and quantum computing with Federated Learning (FL) for financial\nfraud detection. Using quantum technologies' computational power and the robust\ndata privacy protections offered by FL, QFNN-FFD emerges as a secure and\nefficient method for identifying fraudulent transactions within the financial\nsector. Implementing a dual-phase training model across distributed clients\nenhances data integrity and enables superior performance metrics, achieving\nprecision rates consistently above 95%. Additionally, QFNN-FFD demonstrates\nexceptional resilience by maintaining an impressive 80% accuracy, highlighting\nits robustness and readiness for real-world applications. This combination of\nhigh performance, security, and robustness against noise positions QFNN-FFD as\na transformative advancement in financial technology solutions and establishes\nit as a new benchmark for privacy-focused fraud detection systems. This\nframework facilitates the broader adoption of secure, quantum-enhanced\nfinancial services and inspires future innovations that could use QML to tackle\ncomplex challenges in other areas requiring high confidentiality and accuracy.\n", "link": "http://arxiv.org/abs/2404.02595v3", "date": "2024-12-02", "relevancy": 2.1516, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.437}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4331}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QFNN-FFD%3A%20Quantum%20Federated%20Neural%20Network%20for%20Financial%20Fraud%20Detection&body=Title%3A%20QFNN-FFD%3A%20Quantum%20Federated%20Neural%20Network%20for%20Financial%20Fraud%20Detection%0AAuthor%3A%20Nouhaila%20Innan%20and%20Alberto%20Marchisio%20and%20Mohamed%20Bennai%20and%20Muhammad%20Shafique%0AAbstract%3A%20%20%20This%20study%20introduces%20the%20Quantum%20Federated%20Neural%20Network%20for%20Financial%0AFraud%20Detection%20%28QFNN-FFD%29%2C%20a%20cutting-edge%20framework%20merging%20Quantum%20Machine%0ALearning%20%28QML%29%20and%20quantum%20computing%20with%20Federated%20Learning%20%28FL%29%20for%20financial%0Afraud%20detection.%20Using%20quantum%20technologies%27%20computational%20power%20and%20the%20robust%0Adata%20privacy%20protections%20offered%20by%20FL%2C%20QFNN-FFD%20emerges%20as%20a%20secure%20and%0Aefficient%20method%20for%20identifying%20fraudulent%20transactions%20within%20the%20financial%0Asector.%20Implementing%20a%20dual-phase%20training%20model%20across%20distributed%20clients%0Aenhances%20data%20integrity%20and%20enables%20superior%20performance%20metrics%2C%20achieving%0Aprecision%20rates%20consistently%20above%2095%25.%20Additionally%2C%20QFNN-FFD%20demonstrates%0Aexceptional%20resilience%20by%20maintaining%20an%20impressive%2080%25%20accuracy%2C%20highlighting%0Aits%20robustness%20and%20readiness%20for%20real-world%20applications.%20This%20combination%20of%0Ahigh%20performance%2C%20security%2C%20and%20robustness%20against%20noise%20positions%20QFNN-FFD%20as%0Aa%20transformative%20advancement%20in%20financial%20technology%20solutions%20and%20establishes%0Ait%20as%20a%20new%20benchmark%20for%20privacy-focused%20fraud%20detection%20systems.%20This%0Aframework%20facilitates%20the%20broader%20adoption%20of%20secure%2C%20quantum-enhanced%0Afinancial%20services%20and%20inspires%20future%20innovations%20that%20could%20use%20QML%20to%20tackle%0Acomplex%20challenges%20in%20other%20areas%20requiring%20high%20confidentiality%20and%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02595v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQFNN-FFD%253A%2520Quantum%2520Federated%2520Neural%2520Network%2520for%2520Financial%2520Fraud%2520Detection%26entry.906535625%3DNouhaila%2520Innan%2520and%2520Alberto%2520Marchisio%2520and%2520Mohamed%2520Bennai%2520and%2520Muhammad%2520Shafique%26entry.1292438233%3D%2520%2520This%2520study%2520introduces%2520the%2520Quantum%2520Federated%2520Neural%2520Network%2520for%2520Financial%250AFraud%2520Detection%2520%2528QFNN-FFD%2529%252C%2520a%2520cutting-edge%2520framework%2520merging%2520Quantum%2520Machine%250ALearning%2520%2528QML%2529%2520and%2520quantum%2520computing%2520with%2520Federated%2520Learning%2520%2528FL%2529%2520for%2520financial%250Afraud%2520detection.%2520Using%2520quantum%2520technologies%2527%2520computational%2520power%2520and%2520the%2520robust%250Adata%2520privacy%2520protections%2520offered%2520by%2520FL%252C%2520QFNN-FFD%2520emerges%2520as%2520a%2520secure%2520and%250Aefficient%2520method%2520for%2520identifying%2520fraudulent%2520transactions%2520within%2520the%2520financial%250Asector.%2520Implementing%2520a%2520dual-phase%2520training%2520model%2520across%2520distributed%2520clients%250Aenhances%2520data%2520integrity%2520and%2520enables%2520superior%2520performance%2520metrics%252C%2520achieving%250Aprecision%2520rates%2520consistently%2520above%252095%2525.%2520Additionally%252C%2520QFNN-FFD%2520demonstrates%250Aexceptional%2520resilience%2520by%2520maintaining%2520an%2520impressive%252080%2525%2520accuracy%252C%2520highlighting%250Aits%2520robustness%2520and%2520readiness%2520for%2520real-world%2520applications.%2520This%2520combination%2520of%250Ahigh%2520performance%252C%2520security%252C%2520and%2520robustness%2520against%2520noise%2520positions%2520QFNN-FFD%2520as%250Aa%2520transformative%2520advancement%2520in%2520financial%2520technology%2520solutions%2520and%2520establishes%250Ait%2520as%2520a%2520new%2520benchmark%2520for%2520privacy-focused%2520fraud%2520detection%2520systems.%2520This%250Aframework%2520facilitates%2520the%2520broader%2520adoption%2520of%2520secure%252C%2520quantum-enhanced%250Afinancial%2520services%2520and%2520inspires%2520future%2520innovations%2520that%2520could%2520use%2520QML%2520to%2520tackle%250Acomplex%2520challenges%2520in%2520other%2520areas%2520requiring%2520high%2520confidentiality%2520and%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02595v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QFNN-FFD%3A%20Quantum%20Federated%20Neural%20Network%20for%20Financial%20Fraud%20Detection&entry.906535625=Nouhaila%20Innan%20and%20Alberto%20Marchisio%20and%20Mohamed%20Bennai%20and%20Muhammad%20Shafique&entry.1292438233=%20%20This%20study%20introduces%20the%20Quantum%20Federated%20Neural%20Network%20for%20Financial%0AFraud%20Detection%20%28QFNN-FFD%29%2C%20a%20cutting-edge%20framework%20merging%20Quantum%20Machine%0ALearning%20%28QML%29%20and%20quantum%20computing%20with%20Federated%20Learning%20%28FL%29%20for%20financial%0Afraud%20detection.%20Using%20quantum%20technologies%27%20computational%20power%20and%20the%20robust%0Adata%20privacy%20protections%20offered%20by%20FL%2C%20QFNN-FFD%20emerges%20as%20a%20secure%20and%0Aefficient%20method%20for%20identifying%20fraudulent%20transactions%20within%20the%20financial%0Asector.%20Implementing%20a%20dual-phase%20training%20model%20across%20distributed%20clients%0Aenhances%20data%20integrity%20and%20enables%20superior%20performance%20metrics%2C%20achieving%0Aprecision%20rates%20consistently%20above%2095%25.%20Additionally%2C%20QFNN-FFD%20demonstrates%0Aexceptional%20resilience%20by%20maintaining%20an%20impressive%2080%25%20accuracy%2C%20highlighting%0Aits%20robustness%20and%20readiness%20for%20real-world%20applications.%20This%20combination%20of%0Ahigh%20performance%2C%20security%2C%20and%20robustness%20against%20noise%20positions%20QFNN-FFD%20as%0Aa%20transformative%20advancement%20in%20financial%20technology%20solutions%20and%20establishes%0Ait%20as%20a%20new%20benchmark%20for%20privacy-focused%20fraud%20detection%20systems.%20This%0Aframework%20facilitates%20the%20broader%20adoption%20of%20secure%2C%20quantum-enhanced%0Afinancial%20services%20and%20inspires%20future%20innovations%20that%20could%20use%20QML%20to%20tackle%0Acomplex%20challenges%20in%20other%20areas%20requiring%20high%20confidentiality%20and%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02595v3&entry.124074799=Read"},
{"title": "Topology-Based Reconstruction Prevention for Decentralised Learning", "author": "Florine W. Dekker and Zekeriya Erkin and Mauro Conti", "abstract": "  Decentralised learning has recently gained traction as an alternative to\nfederated learning in which both data and coordination are distributed. To\npreserve the confidentiality of users' data, decentralised learning relies on\ndifferential privacy, multi-party computation, or both. However, running\nmultiple privacy-preserving summations in sequence may allow adversaries to\nperform reconstruction attacks. Current reconstruction countermeasures either\ncannot trivially be adapted to the distributed setting, or add excessive\namounts of noise.\n  In this work, we first show that passive honest-but-curious adversaries can\ninfer other users' private data after several privacy-preserving summations.\nFor example, in subgraphs with 18 users, we show that only three passive\nhonest-but-curious adversaries succeed at reconstructing private data 11.0% of\nthe time, requiring an average of 8.8 summations per adversary. The success\nrate depends only on the adversaries' direct neighbourhood, and is independent\nof the size of the full network. We consider weak adversaries that do not\ncontrol the graph topology, cannot exploit the summation's inner workings, and\ndo not have auxiliary knowledge; and show that these adversaries can still\ninfer private data.\n  We analyse how reconstruction relates to topology and propose the first\ntopology-based decentralised defence against reconstruction attacks. We show\nthat reconstruction requires a number of adversaries linear in the length of\nthe network's shortest cycle. Consequently, exact attacks over\nprivacy-preserving summations are impossible in acyclic networks.\n  Our work is a stepping stone for a formal theory of topology-based\ndecentralised reconstruction defences. Such a theory would generalise our\ncountermeasure beyond summation, define confidentiality in terms of entropy,\nand describe the interactions with (topology-aware) differential privacy.\n", "link": "http://arxiv.org/abs/2312.05248v3", "date": "2024-12-02", "relevancy": 2.1487, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4386}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4269}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topology-Based%20Reconstruction%20Prevention%20for%20Decentralised%20Learning&body=Title%3A%20Topology-Based%20Reconstruction%20Prevention%20for%20Decentralised%20Learning%0AAuthor%3A%20Florine%20W.%20Dekker%20and%20Zekeriya%20Erkin%20and%20Mauro%20Conti%0AAbstract%3A%20%20%20Decentralised%20learning%20has%20recently%20gained%20traction%20as%20an%20alternative%20to%0Afederated%20learning%20in%20which%20both%20data%20and%20coordination%20are%20distributed.%20To%0Apreserve%20the%20confidentiality%20of%20users%27%20data%2C%20decentralised%20learning%20relies%20on%0Adifferential%20privacy%2C%20multi-party%20computation%2C%20or%20both.%20However%2C%20running%0Amultiple%20privacy-preserving%20summations%20in%20sequence%20may%20allow%20adversaries%20to%0Aperform%20reconstruction%20attacks.%20Current%20reconstruction%20countermeasures%20either%0Acannot%20trivially%20be%20adapted%20to%20the%20distributed%20setting%2C%20or%20add%20excessive%0Aamounts%20of%20noise.%0A%20%20In%20this%20work%2C%20we%20first%20show%20that%20passive%20honest-but-curious%20adversaries%20can%0Ainfer%20other%20users%27%20private%20data%20after%20several%20privacy-preserving%20summations.%0AFor%20example%2C%20in%20subgraphs%20with%2018%20users%2C%20we%20show%20that%20only%20three%20passive%0Ahonest-but-curious%20adversaries%20succeed%20at%20reconstructing%20private%20data%2011.0%25%20of%0Athe%20time%2C%20requiring%20an%20average%20of%208.8%20summations%20per%20adversary.%20The%20success%0Arate%20depends%20only%20on%20the%20adversaries%27%20direct%20neighbourhood%2C%20and%20is%20independent%0Aof%20the%20size%20of%20the%20full%20network.%20We%20consider%20weak%20adversaries%20that%20do%20not%0Acontrol%20the%20graph%20topology%2C%20cannot%20exploit%20the%20summation%27s%20inner%20workings%2C%20and%0Ado%20not%20have%20auxiliary%20knowledge%3B%20and%20show%20that%20these%20adversaries%20can%20still%0Ainfer%20private%20data.%0A%20%20We%20analyse%20how%20reconstruction%20relates%20to%20topology%20and%20propose%20the%20first%0Atopology-based%20decentralised%20defence%20against%20reconstruction%20attacks.%20We%20show%0Athat%20reconstruction%20requires%20a%20number%20of%20adversaries%20linear%20in%20the%20length%20of%0Athe%20network%27s%20shortest%20cycle.%20Consequently%2C%20exact%20attacks%20over%0Aprivacy-preserving%20summations%20are%20impossible%20in%20acyclic%20networks.%0A%20%20Our%20work%20is%20a%20stepping%20stone%20for%20a%20formal%20theory%20of%20topology-based%0Adecentralised%20reconstruction%20defences.%20Such%20a%20theory%20would%20generalise%20our%0Acountermeasure%20beyond%20summation%2C%20define%20confidentiality%20in%20terms%20of%20entropy%2C%0Aand%20describe%20the%20interactions%20with%20%28topology-aware%29%20differential%20privacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05248v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopology-Based%2520Reconstruction%2520Prevention%2520for%2520Decentralised%2520Learning%26entry.906535625%3DFlorine%2520W.%2520Dekker%2520and%2520Zekeriya%2520Erkin%2520and%2520Mauro%2520Conti%26entry.1292438233%3D%2520%2520Decentralised%2520learning%2520has%2520recently%2520gained%2520traction%2520as%2520an%2520alternative%2520to%250Afederated%2520learning%2520in%2520which%2520both%2520data%2520and%2520coordination%2520are%2520distributed.%2520To%250Apreserve%2520the%2520confidentiality%2520of%2520users%2527%2520data%252C%2520decentralised%2520learning%2520relies%2520on%250Adifferential%2520privacy%252C%2520multi-party%2520computation%252C%2520or%2520both.%2520However%252C%2520running%250Amultiple%2520privacy-preserving%2520summations%2520in%2520sequence%2520may%2520allow%2520adversaries%2520to%250Aperform%2520reconstruction%2520attacks.%2520Current%2520reconstruction%2520countermeasures%2520either%250Acannot%2520trivially%2520be%2520adapted%2520to%2520the%2520distributed%2520setting%252C%2520or%2520add%2520excessive%250Aamounts%2520of%2520noise.%250A%2520%2520In%2520this%2520work%252C%2520we%2520first%2520show%2520that%2520passive%2520honest-but-curious%2520adversaries%2520can%250Ainfer%2520other%2520users%2527%2520private%2520data%2520after%2520several%2520privacy-preserving%2520summations.%250AFor%2520example%252C%2520in%2520subgraphs%2520with%252018%2520users%252C%2520we%2520show%2520that%2520only%2520three%2520passive%250Ahonest-but-curious%2520adversaries%2520succeed%2520at%2520reconstructing%2520private%2520data%252011.0%2525%2520of%250Athe%2520time%252C%2520requiring%2520an%2520average%2520of%25208.8%2520summations%2520per%2520adversary.%2520The%2520success%250Arate%2520depends%2520only%2520on%2520the%2520adversaries%2527%2520direct%2520neighbourhood%252C%2520and%2520is%2520independent%250Aof%2520the%2520size%2520of%2520the%2520full%2520network.%2520We%2520consider%2520weak%2520adversaries%2520that%2520do%2520not%250Acontrol%2520the%2520graph%2520topology%252C%2520cannot%2520exploit%2520the%2520summation%2527s%2520inner%2520workings%252C%2520and%250Ado%2520not%2520have%2520auxiliary%2520knowledge%253B%2520and%2520show%2520that%2520these%2520adversaries%2520can%2520still%250Ainfer%2520private%2520data.%250A%2520%2520We%2520analyse%2520how%2520reconstruction%2520relates%2520to%2520topology%2520and%2520propose%2520the%2520first%250Atopology-based%2520decentralised%2520defence%2520against%2520reconstruction%2520attacks.%2520We%2520show%250Athat%2520reconstruction%2520requires%2520a%2520number%2520of%2520adversaries%2520linear%2520in%2520the%2520length%2520of%250Athe%2520network%2527s%2520shortest%2520cycle.%2520Consequently%252C%2520exact%2520attacks%2520over%250Aprivacy-preserving%2520summations%2520are%2520impossible%2520in%2520acyclic%2520networks.%250A%2520%2520Our%2520work%2520is%2520a%2520stepping%2520stone%2520for%2520a%2520formal%2520theory%2520of%2520topology-based%250Adecentralised%2520reconstruction%2520defences.%2520Such%2520a%2520theory%2520would%2520generalise%2520our%250Acountermeasure%2520beyond%2520summation%252C%2520define%2520confidentiality%2520in%2520terms%2520of%2520entropy%252C%250Aand%2520describe%2520the%2520interactions%2520with%2520%2528topology-aware%2529%2520differential%2520privacy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.05248v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topology-Based%20Reconstruction%20Prevention%20for%20Decentralised%20Learning&entry.906535625=Florine%20W.%20Dekker%20and%20Zekeriya%20Erkin%20and%20Mauro%20Conti&entry.1292438233=%20%20Decentralised%20learning%20has%20recently%20gained%20traction%20as%20an%20alternative%20to%0Afederated%20learning%20in%20which%20both%20data%20and%20coordination%20are%20distributed.%20To%0Apreserve%20the%20confidentiality%20of%20users%27%20data%2C%20decentralised%20learning%20relies%20on%0Adifferential%20privacy%2C%20multi-party%20computation%2C%20or%20both.%20However%2C%20running%0Amultiple%20privacy-preserving%20summations%20in%20sequence%20may%20allow%20adversaries%20to%0Aperform%20reconstruction%20attacks.%20Current%20reconstruction%20countermeasures%20either%0Acannot%20trivially%20be%20adapted%20to%20the%20distributed%20setting%2C%20or%20add%20excessive%0Aamounts%20of%20noise.%0A%20%20In%20this%20work%2C%20we%20first%20show%20that%20passive%20honest-but-curious%20adversaries%20can%0Ainfer%20other%20users%27%20private%20data%20after%20several%20privacy-preserving%20summations.%0AFor%20example%2C%20in%20subgraphs%20with%2018%20users%2C%20we%20show%20that%20only%20three%20passive%0Ahonest-but-curious%20adversaries%20succeed%20at%20reconstructing%20private%20data%2011.0%25%20of%0Athe%20time%2C%20requiring%20an%20average%20of%208.8%20summations%20per%20adversary.%20The%20success%0Arate%20depends%20only%20on%20the%20adversaries%27%20direct%20neighbourhood%2C%20and%20is%20independent%0Aof%20the%20size%20of%20the%20full%20network.%20We%20consider%20weak%20adversaries%20that%20do%20not%0Acontrol%20the%20graph%20topology%2C%20cannot%20exploit%20the%20summation%27s%20inner%20workings%2C%20and%0Ado%20not%20have%20auxiliary%20knowledge%3B%20and%20show%20that%20these%20adversaries%20can%20still%0Ainfer%20private%20data.%0A%20%20We%20analyse%20how%20reconstruction%20relates%20to%20topology%20and%20propose%20the%20first%0Atopology-based%20decentralised%20defence%20against%20reconstruction%20attacks.%20We%20show%0Athat%20reconstruction%20requires%20a%20number%20of%20adversaries%20linear%20in%20the%20length%20of%0Athe%20network%27s%20shortest%20cycle.%20Consequently%2C%20exact%20attacks%20over%0Aprivacy-preserving%20summations%20are%20impossible%20in%20acyclic%20networks.%0A%20%20Our%20work%20is%20a%20stepping%20stone%20for%20a%20formal%20theory%20of%20topology-based%0Adecentralised%20reconstruction%20defences.%20Such%20a%20theory%20would%20generalise%20our%0Acountermeasure%20beyond%20summation%2C%20define%20confidentiality%20in%20terms%20of%20entropy%2C%0Aand%20describe%20the%20interactions%20with%20%28topology-aware%29%20differential%20privacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05248v3&entry.124074799=Read"},
{"title": "A Self-Supervised Task for Fault Detection in Satellite Multivariate\n  Time Series", "author": "Carlo Cena and Silvia Bucci and Alessandro Balossino and Marcello Chiaberge", "abstract": "  In the space sector, due to environmental conditions and restricted\naccessibility, robust fault detection methods are imperative for ensuring\nmission success and safeguarding valuable assets. This work proposes a novel\napproach leveraging Physics-Informed Real NVP neural networks, renowned for\ntheir ability to model complex and high-dimensional distributions, augmented\nwith a self-supervised task based on sensors' data permutation. It focuses on\nenhancing fault detection within the satellite multivariate time series. The\nexperiments involve various configurations, including pre-training with\nself-supervision, multi-task learning, and standalone self-supervised training.\nResults indicate significant performance improvements across all settings. In\nparticular, employing only the self-supervised loss yields the best overall\nresults, suggesting its efficacy in guiding the network to extract relevant\nfeatures for fault detection. This study presents a promising direction for\nimproving fault detection in space systems and warrants further exploration in\nother datasets and applications.\n", "link": "http://arxiv.org/abs/2407.02861v2", "date": "2024-12-02", "relevancy": 2.1273, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5451}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5243}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Self-Supervised%20Task%20for%20Fault%20Detection%20in%20Satellite%20Multivariate%0A%20%20Time%20Series&body=Title%3A%20A%20Self-Supervised%20Task%20for%20Fault%20Detection%20in%20Satellite%20Multivariate%0A%20%20Time%20Series%0AAuthor%3A%20Carlo%20Cena%20and%20Silvia%20Bucci%20and%20Alessandro%20Balossino%20and%20Marcello%20Chiaberge%0AAbstract%3A%20%20%20In%20the%20space%20sector%2C%20due%20to%20environmental%20conditions%20and%20restricted%0Aaccessibility%2C%20robust%20fault%20detection%20methods%20are%20imperative%20for%20ensuring%0Amission%20success%20and%20safeguarding%20valuable%20assets.%20This%20work%20proposes%20a%20novel%0Aapproach%20leveraging%20Physics-Informed%20Real%20NVP%20neural%20networks%2C%20renowned%20for%0Atheir%20ability%20to%20model%20complex%20and%20high-dimensional%20distributions%2C%20augmented%0Awith%20a%20self-supervised%20task%20based%20on%20sensors%27%20data%20permutation.%20It%20focuses%20on%0Aenhancing%20fault%20detection%20within%20the%20satellite%20multivariate%20time%20series.%20The%0Aexperiments%20involve%20various%20configurations%2C%20including%20pre-training%20with%0Aself-supervision%2C%20multi-task%20learning%2C%20and%20standalone%20self-supervised%20training.%0AResults%20indicate%20significant%20performance%20improvements%20across%20all%20settings.%20In%0Aparticular%2C%20employing%20only%20the%20self-supervised%20loss%20yields%20the%20best%20overall%0Aresults%2C%20suggesting%20its%20efficacy%20in%20guiding%20the%20network%20to%20extract%20relevant%0Afeatures%20for%20fault%20detection.%20This%20study%20presents%20a%20promising%20direction%20for%0Aimproving%20fault%20detection%20in%20space%20systems%20and%20warrants%20further%20exploration%20in%0Aother%20datasets%20and%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02861v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Self-Supervised%2520Task%2520for%2520Fault%2520Detection%2520in%2520Satellite%2520Multivariate%250A%2520%2520Time%2520Series%26entry.906535625%3DCarlo%2520Cena%2520and%2520Silvia%2520Bucci%2520and%2520Alessandro%2520Balossino%2520and%2520Marcello%2520Chiaberge%26entry.1292438233%3D%2520%2520In%2520the%2520space%2520sector%252C%2520due%2520to%2520environmental%2520conditions%2520and%2520restricted%250Aaccessibility%252C%2520robust%2520fault%2520detection%2520methods%2520are%2520imperative%2520for%2520ensuring%250Amission%2520success%2520and%2520safeguarding%2520valuable%2520assets.%2520This%2520work%2520proposes%2520a%2520novel%250Aapproach%2520leveraging%2520Physics-Informed%2520Real%2520NVP%2520neural%2520networks%252C%2520renowned%2520for%250Atheir%2520ability%2520to%2520model%2520complex%2520and%2520high-dimensional%2520distributions%252C%2520augmented%250Awith%2520a%2520self-supervised%2520task%2520based%2520on%2520sensors%2527%2520data%2520permutation.%2520It%2520focuses%2520on%250Aenhancing%2520fault%2520detection%2520within%2520the%2520satellite%2520multivariate%2520time%2520series.%2520The%250Aexperiments%2520involve%2520various%2520configurations%252C%2520including%2520pre-training%2520with%250Aself-supervision%252C%2520multi-task%2520learning%252C%2520and%2520standalone%2520self-supervised%2520training.%250AResults%2520indicate%2520significant%2520performance%2520improvements%2520across%2520all%2520settings.%2520In%250Aparticular%252C%2520employing%2520only%2520the%2520self-supervised%2520loss%2520yields%2520the%2520best%2520overall%250Aresults%252C%2520suggesting%2520its%2520efficacy%2520in%2520guiding%2520the%2520network%2520to%2520extract%2520relevant%250Afeatures%2520for%2520fault%2520detection.%2520This%2520study%2520presents%2520a%2520promising%2520direction%2520for%250Aimproving%2520fault%2520detection%2520in%2520space%2520systems%2520and%2520warrants%2520further%2520exploration%2520in%250Aother%2520datasets%2520and%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02861v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Self-Supervised%20Task%20for%20Fault%20Detection%20in%20Satellite%20Multivariate%0A%20%20Time%20Series&entry.906535625=Carlo%20Cena%20and%20Silvia%20Bucci%20and%20Alessandro%20Balossino%20and%20Marcello%20Chiaberge&entry.1292438233=%20%20In%20the%20space%20sector%2C%20due%20to%20environmental%20conditions%20and%20restricted%0Aaccessibility%2C%20robust%20fault%20detection%20methods%20are%20imperative%20for%20ensuring%0Amission%20success%20and%20safeguarding%20valuable%20assets.%20This%20work%20proposes%20a%20novel%0Aapproach%20leveraging%20Physics-Informed%20Real%20NVP%20neural%20networks%2C%20renowned%20for%0Atheir%20ability%20to%20model%20complex%20and%20high-dimensional%20distributions%2C%20augmented%0Awith%20a%20self-supervised%20task%20based%20on%20sensors%27%20data%20permutation.%20It%20focuses%20on%0Aenhancing%20fault%20detection%20within%20the%20satellite%20multivariate%20time%20series.%20The%0Aexperiments%20involve%20various%20configurations%2C%20including%20pre-training%20with%0Aself-supervision%2C%20multi-task%20learning%2C%20and%20standalone%20self-supervised%20training.%0AResults%20indicate%20significant%20performance%20improvements%20across%20all%20settings.%20In%0Aparticular%2C%20employing%20only%20the%20self-supervised%20loss%20yields%20the%20best%20overall%0Aresults%2C%20suggesting%20its%20efficacy%20in%20guiding%20the%20network%20to%20extract%20relevant%0Afeatures%20for%20fault%20detection.%20This%20study%20presents%20a%20promising%20direction%20for%0Aimproving%20fault%20detection%20in%20space%20systems%20and%20warrants%20further%20exploration%20in%0Aother%20datasets%20and%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02861v2&entry.124074799=Read"},
{"title": "Learning General Representation of 12-Lead Electrocardiogram with a\n  Joint-Embedding Predictive Architecture", "author": "Sehun Kim", "abstract": "  Electrocardiogram (ECG) captures the heart's electrical signals, offering\nvaluable information for diagnosing cardiac conditions. However, the scarcity\nof labeled data makes it challenging to fully leverage supervised learning in\nmedical domain. Self-supervised learning (SSL) offers a promising solution,\nenabling models to learn from unlabeled data and uncover meaningful patterns.\nIn this paper, we show that masked modeling in the latent space can be a\npowerful alternative to existing self-supervised methods in the ECG domain. We\nintroduce ECG-JEPA, a SSL model for 12-lead ECG analysis that learns semantic\nrepresentations of ECG data by predicting in the hidden latent space, bypassing\nthe need to reconstruct raw signals. This approach offers several advantages in\nthe ECG domain: (1) it avoids producing unnecessary details, such as noise,\nwhich is common in ECG; and (2) it addresses the limitations of na\\\"ive L2 loss\nbetween raw signals. Another key contribution is the introduction of\nCross-Pattern Attention (CroPA), a specialized masked attention mechanism\ntailored for 12-lead ECG data. ECG-JEPA is trained on the union of several open\nECG datasets, totaling approximately 180,000 samples, and achieves\nstate-of-the-art performance in various downstream tasks including ECG\nclassification and feature prediction. Our code is openly available at\nhttps://github.com/sehunfromdaegu/ECG_JEPA.\n", "link": "http://arxiv.org/abs/2410.08559v3", "date": "2024-12-02", "relevancy": 2.1208, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5434}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5283}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20General%20Representation%20of%2012-Lead%20Electrocardiogram%20with%20a%0A%20%20Joint-Embedding%20Predictive%20Architecture&body=Title%3A%20Learning%20General%20Representation%20of%2012-Lead%20Electrocardiogram%20with%20a%0A%20%20Joint-Embedding%20Predictive%20Architecture%0AAuthor%3A%20Sehun%20Kim%0AAbstract%3A%20%20%20Electrocardiogram%20%28ECG%29%20captures%20the%20heart%27s%20electrical%20signals%2C%20offering%0Avaluable%20information%20for%20diagnosing%20cardiac%20conditions.%20However%2C%20the%20scarcity%0Aof%20labeled%20data%20makes%20it%20challenging%20to%20fully%20leverage%20supervised%20learning%20in%0Amedical%20domain.%20Self-supervised%20learning%20%28SSL%29%20offers%20a%20promising%20solution%2C%0Aenabling%20models%20to%20learn%20from%20unlabeled%20data%20and%20uncover%20meaningful%20patterns.%0AIn%20this%20paper%2C%20we%20show%20that%20masked%20modeling%20in%20the%20latent%20space%20can%20be%20a%0Apowerful%20alternative%20to%20existing%20self-supervised%20methods%20in%20the%20ECG%20domain.%20We%0Aintroduce%20ECG-JEPA%2C%20a%20SSL%20model%20for%2012-lead%20ECG%20analysis%20that%20learns%20semantic%0Arepresentations%20of%20ECG%20data%20by%20predicting%20in%20the%20hidden%20latent%20space%2C%20bypassing%0Athe%20need%20to%20reconstruct%20raw%20signals.%20This%20approach%20offers%20several%20advantages%20in%0Athe%20ECG%20domain%3A%20%281%29%20it%20avoids%20producing%20unnecessary%20details%2C%20such%20as%20noise%2C%0Awhich%20is%20common%20in%20ECG%3B%20and%20%282%29%20it%20addresses%20the%20limitations%20of%20na%5C%22ive%20L2%20loss%0Abetween%20raw%20signals.%20Another%20key%20contribution%20is%20the%20introduction%20of%0ACross-Pattern%20Attention%20%28CroPA%29%2C%20a%20specialized%20masked%20attention%20mechanism%0Atailored%20for%2012-lead%20ECG%20data.%20ECG-JEPA%20is%20trained%20on%20the%20union%20of%20several%20open%0AECG%20datasets%2C%20totaling%20approximately%20180%2C000%20samples%2C%20and%20achieves%0Astate-of-the-art%20performance%20in%20various%20downstream%20tasks%20including%20ECG%0Aclassification%20and%20feature%20prediction.%20Our%20code%20is%20openly%20available%20at%0Ahttps%3A//github.com/sehunfromdaegu/ECG_JEPA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08559v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520General%2520Representation%2520of%252012-Lead%2520Electrocardiogram%2520with%2520a%250A%2520%2520Joint-Embedding%2520Predictive%2520Architecture%26entry.906535625%3DSehun%2520Kim%26entry.1292438233%3D%2520%2520Electrocardiogram%2520%2528ECG%2529%2520captures%2520the%2520heart%2527s%2520electrical%2520signals%252C%2520offering%250Avaluable%2520information%2520for%2520diagnosing%2520cardiac%2520conditions.%2520However%252C%2520the%2520scarcity%250Aof%2520labeled%2520data%2520makes%2520it%2520challenging%2520to%2520fully%2520leverage%2520supervised%2520learning%2520in%250Amedical%2520domain.%2520Self-supervised%2520learning%2520%2528SSL%2529%2520offers%2520a%2520promising%2520solution%252C%250Aenabling%2520models%2520to%2520learn%2520from%2520unlabeled%2520data%2520and%2520uncover%2520meaningful%2520patterns.%250AIn%2520this%2520paper%252C%2520we%2520show%2520that%2520masked%2520modeling%2520in%2520the%2520latent%2520space%2520can%2520be%2520a%250Apowerful%2520alternative%2520to%2520existing%2520self-supervised%2520methods%2520in%2520the%2520ECG%2520domain.%2520We%250Aintroduce%2520ECG-JEPA%252C%2520a%2520SSL%2520model%2520for%252012-lead%2520ECG%2520analysis%2520that%2520learns%2520semantic%250Arepresentations%2520of%2520ECG%2520data%2520by%2520predicting%2520in%2520the%2520hidden%2520latent%2520space%252C%2520bypassing%250Athe%2520need%2520to%2520reconstruct%2520raw%2520signals.%2520This%2520approach%2520offers%2520several%2520advantages%2520in%250Athe%2520ECG%2520domain%253A%2520%25281%2529%2520it%2520avoids%2520producing%2520unnecessary%2520details%252C%2520such%2520as%2520noise%252C%250Awhich%2520is%2520common%2520in%2520ECG%253B%2520and%2520%25282%2529%2520it%2520addresses%2520the%2520limitations%2520of%2520na%255C%2522ive%2520L2%2520loss%250Abetween%2520raw%2520signals.%2520Another%2520key%2520contribution%2520is%2520the%2520introduction%2520of%250ACross-Pattern%2520Attention%2520%2528CroPA%2529%252C%2520a%2520specialized%2520masked%2520attention%2520mechanism%250Atailored%2520for%252012-lead%2520ECG%2520data.%2520ECG-JEPA%2520is%2520trained%2520on%2520the%2520union%2520of%2520several%2520open%250AECG%2520datasets%252C%2520totaling%2520approximately%2520180%252C000%2520samples%252C%2520and%2520achieves%250Astate-of-the-art%2520performance%2520in%2520various%2520downstream%2520tasks%2520including%2520ECG%250Aclassification%2520and%2520feature%2520prediction.%2520Our%2520code%2520is%2520openly%2520available%2520at%250Ahttps%253A//github.com/sehunfromdaegu/ECG_JEPA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08559v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20General%20Representation%20of%2012-Lead%20Electrocardiogram%20with%20a%0A%20%20Joint-Embedding%20Predictive%20Architecture&entry.906535625=Sehun%20Kim&entry.1292438233=%20%20Electrocardiogram%20%28ECG%29%20captures%20the%20heart%27s%20electrical%20signals%2C%20offering%0Avaluable%20information%20for%20diagnosing%20cardiac%20conditions.%20However%2C%20the%20scarcity%0Aof%20labeled%20data%20makes%20it%20challenging%20to%20fully%20leverage%20supervised%20learning%20in%0Amedical%20domain.%20Self-supervised%20learning%20%28SSL%29%20offers%20a%20promising%20solution%2C%0Aenabling%20models%20to%20learn%20from%20unlabeled%20data%20and%20uncover%20meaningful%20patterns.%0AIn%20this%20paper%2C%20we%20show%20that%20masked%20modeling%20in%20the%20latent%20space%20can%20be%20a%0Apowerful%20alternative%20to%20existing%20self-supervised%20methods%20in%20the%20ECG%20domain.%20We%0Aintroduce%20ECG-JEPA%2C%20a%20SSL%20model%20for%2012-lead%20ECG%20analysis%20that%20learns%20semantic%0Arepresentations%20of%20ECG%20data%20by%20predicting%20in%20the%20hidden%20latent%20space%2C%20bypassing%0Athe%20need%20to%20reconstruct%20raw%20signals.%20This%20approach%20offers%20several%20advantages%20in%0Athe%20ECG%20domain%3A%20%281%29%20it%20avoids%20producing%20unnecessary%20details%2C%20such%20as%20noise%2C%0Awhich%20is%20common%20in%20ECG%3B%20and%20%282%29%20it%20addresses%20the%20limitations%20of%20na%5C%22ive%20L2%20loss%0Abetween%20raw%20signals.%20Another%20key%20contribution%20is%20the%20introduction%20of%0ACross-Pattern%20Attention%20%28CroPA%29%2C%20a%20specialized%20masked%20attention%20mechanism%0Atailored%20for%2012-lead%20ECG%20data.%20ECG-JEPA%20is%20trained%20on%20the%20union%20of%20several%20open%0AECG%20datasets%2C%20totaling%20approximately%20180%2C000%20samples%2C%20and%20achieves%0Astate-of-the-art%20performance%20in%20various%20downstream%20tasks%20including%20ECG%0Aclassification%20and%20feature%20prediction.%20Our%20code%20is%20openly%20available%20at%0Ahttps%3A//github.com/sehunfromdaegu/ECG_JEPA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08559v3&entry.124074799=Read"},
{"title": "Brain Tumour Removing and Missing Modality Generation using 3D WDM", "author": "Andr\u00e9 Ferreira and Gijs Luijten and Behrus Puladi and Jens Kleesiek and Victor Alves and Jan Egger", "abstract": "  This paper presents the second-placed solution for task 8 and the\nparticipation solution for task 7 of BraTS 2024. The adoption of automated\nbrain analysis algorithms to support clinical practice is increasing. However,\nmany of these algorithms struggle with the presence of brain lesions or the\nabsence of certain MRI modalities. The alterations in the brain's morphology\nleads to high variability and thus poor performance of predictive models that\nwere trained only on healthy brains. The lack of information that is usually\nprovided by some of the missing MRI modalities also reduces the reliability of\nthe prediction models trained with all modalities. In order to improve the\nperformance of these models, we propose the use of conditional 3D wavelet\ndiffusion models. The wavelet transform enabled full-resolution image training\nand prediction on a GPU with 48 GB VRAM, without patching or downsampling,\npreserving all information for prediction. The code for these tasks is\navailable at https://github.com/ShadowTwin41/BraTS_2023_2024_solutions.\n", "link": "http://arxiv.org/abs/2411.04630v2", "date": "2024-12-02", "relevancy": 2.1038, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5262}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5259}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain%20Tumour%20Removing%20and%20Missing%20Modality%20Generation%20using%203D%20WDM&body=Title%3A%20Brain%20Tumour%20Removing%20and%20Missing%20Modality%20Generation%20using%203D%20WDM%0AAuthor%3A%20Andr%C3%A9%20Ferreira%20and%20Gijs%20Luijten%20and%20Behrus%20Puladi%20and%20Jens%20Kleesiek%20and%20Victor%20Alves%20and%20Jan%20Egger%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20second-placed%20solution%20for%20task%208%20and%20the%0Aparticipation%20solution%20for%20task%207%20of%20BraTS%202024.%20The%20adoption%20of%20automated%0Abrain%20analysis%20algorithms%20to%20support%20clinical%20practice%20is%20increasing.%20However%2C%0Amany%20of%20these%20algorithms%20struggle%20with%20the%20presence%20of%20brain%20lesions%20or%20the%0Aabsence%20of%20certain%20MRI%20modalities.%20The%20alterations%20in%20the%20brain%27s%20morphology%0Aleads%20to%20high%20variability%20and%20thus%20poor%20performance%20of%20predictive%20models%20that%0Awere%20trained%20only%20on%20healthy%20brains.%20The%20lack%20of%20information%20that%20is%20usually%0Aprovided%20by%20some%20of%20the%20missing%20MRI%20modalities%20also%20reduces%20the%20reliability%20of%0Athe%20prediction%20models%20trained%20with%20all%20modalities.%20In%20order%20to%20improve%20the%0Aperformance%20of%20these%20models%2C%20we%20propose%20the%20use%20of%20conditional%203D%20wavelet%0Adiffusion%20models.%20The%20wavelet%20transform%20enabled%20full-resolution%20image%20training%0Aand%20prediction%20on%20a%20GPU%20with%2048%20GB%20VRAM%2C%20without%20patching%20or%20downsampling%2C%0Apreserving%20all%20information%20for%20prediction.%20The%20code%20for%20these%20tasks%20is%0Aavailable%20at%20https%3A//github.com/ShadowTwin41/BraTS_2023_2024_solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04630v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain%2520Tumour%2520Removing%2520and%2520Missing%2520Modality%2520Generation%2520using%25203D%2520WDM%26entry.906535625%3DAndr%25C3%25A9%2520Ferreira%2520and%2520Gijs%2520Luijten%2520and%2520Behrus%2520Puladi%2520and%2520Jens%2520Kleesiek%2520and%2520Victor%2520Alves%2520and%2520Jan%2520Egger%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520second-placed%2520solution%2520for%2520task%25208%2520and%2520the%250Aparticipation%2520solution%2520for%2520task%25207%2520of%2520BraTS%25202024.%2520The%2520adoption%2520of%2520automated%250Abrain%2520analysis%2520algorithms%2520to%2520support%2520clinical%2520practice%2520is%2520increasing.%2520However%252C%250Amany%2520of%2520these%2520algorithms%2520struggle%2520with%2520the%2520presence%2520of%2520brain%2520lesions%2520or%2520the%250Aabsence%2520of%2520certain%2520MRI%2520modalities.%2520The%2520alterations%2520in%2520the%2520brain%2527s%2520morphology%250Aleads%2520to%2520high%2520variability%2520and%2520thus%2520poor%2520performance%2520of%2520predictive%2520models%2520that%250Awere%2520trained%2520only%2520on%2520healthy%2520brains.%2520The%2520lack%2520of%2520information%2520that%2520is%2520usually%250Aprovided%2520by%2520some%2520of%2520the%2520missing%2520MRI%2520modalities%2520also%2520reduces%2520the%2520reliability%2520of%250Athe%2520prediction%2520models%2520trained%2520with%2520all%2520modalities.%2520In%2520order%2520to%2520improve%2520the%250Aperformance%2520of%2520these%2520models%252C%2520we%2520propose%2520the%2520use%2520of%2520conditional%25203D%2520wavelet%250Adiffusion%2520models.%2520The%2520wavelet%2520transform%2520enabled%2520full-resolution%2520image%2520training%250Aand%2520prediction%2520on%2520a%2520GPU%2520with%252048%2520GB%2520VRAM%252C%2520without%2520patching%2520or%2520downsampling%252C%250Apreserving%2520all%2520information%2520for%2520prediction.%2520The%2520code%2520for%2520these%2520tasks%2520is%250Aavailable%2520at%2520https%253A//github.com/ShadowTwin41/BraTS_2023_2024_solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04630v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain%20Tumour%20Removing%20and%20Missing%20Modality%20Generation%20using%203D%20WDM&entry.906535625=Andr%C3%A9%20Ferreira%20and%20Gijs%20Luijten%20and%20Behrus%20Puladi%20and%20Jens%20Kleesiek%20and%20Victor%20Alves%20and%20Jan%20Egger&entry.1292438233=%20%20This%20paper%20presents%20the%20second-placed%20solution%20for%20task%208%20and%20the%0Aparticipation%20solution%20for%20task%207%20of%20BraTS%202024.%20The%20adoption%20of%20automated%0Abrain%20analysis%20algorithms%20to%20support%20clinical%20practice%20is%20increasing.%20However%2C%0Amany%20of%20these%20algorithms%20struggle%20with%20the%20presence%20of%20brain%20lesions%20or%20the%0Aabsence%20of%20certain%20MRI%20modalities.%20The%20alterations%20in%20the%20brain%27s%20morphology%0Aleads%20to%20high%20variability%20and%20thus%20poor%20performance%20of%20predictive%20models%20that%0Awere%20trained%20only%20on%20healthy%20brains.%20The%20lack%20of%20information%20that%20is%20usually%0Aprovided%20by%20some%20of%20the%20missing%20MRI%20modalities%20also%20reduces%20the%20reliability%20of%0Athe%20prediction%20models%20trained%20with%20all%20modalities.%20In%20order%20to%20improve%20the%0Aperformance%20of%20these%20models%2C%20we%20propose%20the%20use%20of%20conditional%203D%20wavelet%0Adiffusion%20models.%20The%20wavelet%20transform%20enabled%20full-resolution%20image%20training%0Aand%20prediction%20on%20a%20GPU%20with%2048%20GB%20VRAM%2C%20without%20patching%20or%20downsampling%2C%0Apreserving%20all%20information%20for%20prediction.%20The%20code%20for%20these%20tasks%20is%0Aavailable%20at%20https%3A//github.com/ShadowTwin41/BraTS_2023_2024_solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04630v2&entry.124074799=Read"},
{"title": "On Meta-Prompting", "author": "Adrian de Wynter and Xun Wang and Qilong Gu and Si-Qing Chen", "abstract": "  Modern generative language models are capable of interpreting input strings\nas instructions, or prompts, and carry out tasks based on them. Many approaches\nto prompting and pre-training these models involve the automated generation of\nthese prompts: meta-prompting, or prompting to obtain prompts. We propose a\ntheoretical framework based on category theory to generalize and describe them.\nThis framework is flexible enough to account for stochasticity, and allows us\nto obtain formal results around task agnosticity and equivalence of various\nmeta-prompting approaches. Experimentally, we test our framework in two active\nareas of model research: creativity and ideation. We find that user preference\nstrongly favors (p < 0.01) the prompts generated under meta-prompting, as well\nas their corresponding outputs, over a series of hardcoded baseline prompts\nthat include the original task definition. Using our framework, we argue that\nmeta-prompting is more effective than basic prompting at generating desirable\noutputs.\n", "link": "http://arxiv.org/abs/2312.06562v2", "date": "2024-12-02", "relevancy": 2.1014, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5642}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5264}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Meta-Prompting&body=Title%3A%20On%20Meta-Prompting%0AAuthor%3A%20Adrian%20de%20Wynter%20and%20Xun%20Wang%20and%20Qilong%20Gu%20and%20Si-Qing%20Chen%0AAbstract%3A%20%20%20Modern%20generative%20language%20models%20are%20capable%20of%20interpreting%20input%20strings%0Aas%20instructions%2C%20or%20prompts%2C%20and%20carry%20out%20tasks%20based%20on%20them.%20Many%20approaches%0Ato%20prompting%20and%20pre-training%20these%20models%20involve%20the%20automated%20generation%20of%0Athese%20prompts%3A%20meta-prompting%2C%20or%20prompting%20to%20obtain%20prompts.%20We%20propose%20a%0Atheoretical%20framework%20based%20on%20category%20theory%20to%20generalize%20and%20describe%20them.%0AThis%20framework%20is%20flexible%20enough%20to%20account%20for%20stochasticity%2C%20and%20allows%20us%0Ato%20obtain%20formal%20results%20around%20task%20agnosticity%20and%20equivalence%20of%20various%0Ameta-prompting%20approaches.%20Experimentally%2C%20we%20test%20our%20framework%20in%20two%20active%0Aareas%20of%20model%20research%3A%20creativity%20and%20ideation.%20We%20find%20that%20user%20preference%0Astrongly%20favors%20%28p%20%3C%200.01%29%20the%20prompts%20generated%20under%20meta-prompting%2C%20as%20well%0Aas%20their%20corresponding%20outputs%2C%20over%20a%20series%20of%20hardcoded%20baseline%20prompts%0Athat%20include%20the%20original%20task%20definition.%20Using%20our%20framework%2C%20we%20argue%20that%0Ameta-prompting%20is%20more%20effective%20than%20basic%20prompting%20at%20generating%20desirable%0Aoutputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06562v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Meta-Prompting%26entry.906535625%3DAdrian%2520de%2520Wynter%2520and%2520Xun%2520Wang%2520and%2520Qilong%2520Gu%2520and%2520Si-Qing%2520Chen%26entry.1292438233%3D%2520%2520Modern%2520generative%2520language%2520models%2520are%2520capable%2520of%2520interpreting%2520input%2520strings%250Aas%2520instructions%252C%2520or%2520prompts%252C%2520and%2520carry%2520out%2520tasks%2520based%2520on%2520them.%2520Many%2520approaches%250Ato%2520prompting%2520and%2520pre-training%2520these%2520models%2520involve%2520the%2520automated%2520generation%2520of%250Athese%2520prompts%253A%2520meta-prompting%252C%2520or%2520prompting%2520to%2520obtain%2520prompts.%2520We%2520propose%2520a%250Atheoretical%2520framework%2520based%2520on%2520category%2520theory%2520to%2520generalize%2520and%2520describe%2520them.%250AThis%2520framework%2520is%2520flexible%2520enough%2520to%2520account%2520for%2520stochasticity%252C%2520and%2520allows%2520us%250Ato%2520obtain%2520formal%2520results%2520around%2520task%2520agnosticity%2520and%2520equivalence%2520of%2520various%250Ameta-prompting%2520approaches.%2520Experimentally%252C%2520we%2520test%2520our%2520framework%2520in%2520two%2520active%250Aareas%2520of%2520model%2520research%253A%2520creativity%2520and%2520ideation.%2520We%2520find%2520that%2520user%2520preference%250Astrongly%2520favors%2520%2528p%2520%253C%25200.01%2529%2520the%2520prompts%2520generated%2520under%2520meta-prompting%252C%2520as%2520well%250Aas%2520their%2520corresponding%2520outputs%252C%2520over%2520a%2520series%2520of%2520hardcoded%2520baseline%2520prompts%250Athat%2520include%2520the%2520original%2520task%2520definition.%2520Using%2520our%2520framework%252C%2520we%2520argue%2520that%250Ameta-prompting%2520is%2520more%2520effective%2520than%2520basic%2520prompting%2520at%2520generating%2520desirable%250Aoutputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.06562v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Meta-Prompting&entry.906535625=Adrian%20de%20Wynter%20and%20Xun%20Wang%20and%20Qilong%20Gu%20and%20Si-Qing%20Chen&entry.1292438233=%20%20Modern%20generative%20language%20models%20are%20capable%20of%20interpreting%20input%20strings%0Aas%20instructions%2C%20or%20prompts%2C%20and%20carry%20out%20tasks%20based%20on%20them.%20Many%20approaches%0Ato%20prompting%20and%20pre-training%20these%20models%20involve%20the%20automated%20generation%20of%0Athese%20prompts%3A%20meta-prompting%2C%20or%20prompting%20to%20obtain%20prompts.%20We%20propose%20a%0Atheoretical%20framework%20based%20on%20category%20theory%20to%20generalize%20and%20describe%20them.%0AThis%20framework%20is%20flexible%20enough%20to%20account%20for%20stochasticity%2C%20and%20allows%20us%0Ato%20obtain%20formal%20results%20around%20task%20agnosticity%20and%20equivalence%20of%20various%0Ameta-prompting%20approaches.%20Experimentally%2C%20we%20test%20our%20framework%20in%20two%20active%0Aareas%20of%20model%20research%3A%20creativity%20and%20ideation.%20We%20find%20that%20user%20preference%0Astrongly%20favors%20%28p%20%3C%200.01%29%20the%20prompts%20generated%20under%20meta-prompting%2C%20as%20well%0Aas%20their%20corresponding%20outputs%2C%20over%20a%20series%20of%20hardcoded%20baseline%20prompts%0Athat%20include%20the%20original%20task%20definition.%20Using%20our%20framework%2C%20we%20argue%20that%0Ameta-prompting%20is%20more%20effective%20than%20basic%20prompting%20at%20generating%20desirable%0Aoutputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06562v2&entry.124074799=Read"},
{"title": "ProtFAD: Introducing function-aware domains as implicit modality towards\n  protein function prediction", "author": "Mingqing Wang and Zhiwei Nie and Yonghong He and Athanasios V. Vasilakos and Zhixiang Ren", "abstract": "  Protein function prediction is currently achieved by encoding its sequence or\nstructure, where the sequence-to-function transcendence and high-quality\nstructural data scarcity lead to obvious performance bottlenecks. Protein\ndomains are \"building blocks\" of proteins that are functionally independent,\nand their combinations determine the diverse biological functions. However,\nmost existing studies have yet to thoroughly explore the intricate functional\ninformation contained in the protein domains. To fill this gap, we propose a\nsynergistic integration approach for a function-aware domain representation,\nand a domain-joint contrastive learning strategy to distinguish different\nprotein functions while aligning the modalities. Specifically, we align the\ndomain semantics with GO terms and text description to pre-train domain\nembeddings. Furthermore, we partition proteins into multiple sub-views based on\ncontinuous joint domains for contrastive training under the supervision of a\nnovel triplet InfoNCE loss. Our approach significantly and comprehensively\noutperforms the state-of-the-art methods on various benchmarks, and clearly\ndifferentiates proteins carrying distinct functions compared to the competitor.\nOur implementation is available at\nhttps://github.com/AI-HPC-Research-Team/ProtFAD.\n", "link": "http://arxiv.org/abs/2405.15158v2", "date": "2024-12-02", "relevancy": 2.096, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5409}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProtFAD%3A%20Introducing%20function-aware%20domains%20as%20implicit%20modality%20towards%0A%20%20protein%20function%20prediction&body=Title%3A%20ProtFAD%3A%20Introducing%20function-aware%20domains%20as%20implicit%20modality%20towards%0A%20%20protein%20function%20prediction%0AAuthor%3A%20Mingqing%20Wang%20and%20Zhiwei%20Nie%20and%20Yonghong%20He%20and%20Athanasios%20V.%20Vasilakos%20and%20Zhixiang%20Ren%0AAbstract%3A%20%20%20Protein%20function%20prediction%20is%20currently%20achieved%20by%20encoding%20its%20sequence%20or%0Astructure%2C%20where%20the%20sequence-to-function%20transcendence%20and%20high-quality%0Astructural%20data%20scarcity%20lead%20to%20obvious%20performance%20bottlenecks.%20Protein%0Adomains%20are%20%22building%20blocks%22%20of%20proteins%20that%20are%20functionally%20independent%2C%0Aand%20their%20combinations%20determine%20the%20diverse%20biological%20functions.%20However%2C%0Amost%20existing%20studies%20have%20yet%20to%20thoroughly%20explore%20the%20intricate%20functional%0Ainformation%20contained%20in%20the%20protein%20domains.%20To%20fill%20this%20gap%2C%20we%20propose%20a%0Asynergistic%20integration%20approach%20for%20a%20function-aware%20domain%20representation%2C%0Aand%20a%20domain-joint%20contrastive%20learning%20strategy%20to%20distinguish%20different%0Aprotein%20functions%20while%20aligning%20the%20modalities.%20Specifically%2C%20we%20align%20the%0Adomain%20semantics%20with%20GO%20terms%20and%20text%20description%20to%20pre-train%20domain%0Aembeddings.%20Furthermore%2C%20we%20partition%20proteins%20into%20multiple%20sub-views%20based%20on%0Acontinuous%20joint%20domains%20for%20contrastive%20training%20under%20the%20supervision%20of%20a%0Anovel%20triplet%20InfoNCE%20loss.%20Our%20approach%20significantly%20and%20comprehensively%0Aoutperforms%20the%20state-of-the-art%20methods%20on%20various%20benchmarks%2C%20and%20clearly%0Adifferentiates%20proteins%20carrying%20distinct%20functions%20compared%20to%20the%20competitor.%0AOur%20implementation%20is%20available%20at%0Ahttps%3A//github.com/AI-HPC-Research-Team/ProtFAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15158v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProtFAD%253A%2520Introducing%2520function-aware%2520domains%2520as%2520implicit%2520modality%2520towards%250A%2520%2520protein%2520function%2520prediction%26entry.906535625%3DMingqing%2520Wang%2520and%2520Zhiwei%2520Nie%2520and%2520Yonghong%2520He%2520and%2520Athanasios%2520V.%2520Vasilakos%2520and%2520Zhixiang%2520Ren%26entry.1292438233%3D%2520%2520Protein%2520function%2520prediction%2520is%2520currently%2520achieved%2520by%2520encoding%2520its%2520sequence%2520or%250Astructure%252C%2520where%2520the%2520sequence-to-function%2520transcendence%2520and%2520high-quality%250Astructural%2520data%2520scarcity%2520lead%2520to%2520obvious%2520performance%2520bottlenecks.%2520Protein%250Adomains%2520are%2520%2522building%2520blocks%2522%2520of%2520proteins%2520that%2520are%2520functionally%2520independent%252C%250Aand%2520their%2520combinations%2520determine%2520the%2520diverse%2520biological%2520functions.%2520However%252C%250Amost%2520existing%2520studies%2520have%2520yet%2520to%2520thoroughly%2520explore%2520the%2520intricate%2520functional%250Ainformation%2520contained%2520in%2520the%2520protein%2520domains.%2520To%2520fill%2520this%2520gap%252C%2520we%2520propose%2520a%250Asynergistic%2520integration%2520approach%2520for%2520a%2520function-aware%2520domain%2520representation%252C%250Aand%2520a%2520domain-joint%2520contrastive%2520learning%2520strategy%2520to%2520distinguish%2520different%250Aprotein%2520functions%2520while%2520aligning%2520the%2520modalities.%2520Specifically%252C%2520we%2520align%2520the%250Adomain%2520semantics%2520with%2520GO%2520terms%2520and%2520text%2520description%2520to%2520pre-train%2520domain%250Aembeddings.%2520Furthermore%252C%2520we%2520partition%2520proteins%2520into%2520multiple%2520sub-views%2520based%2520on%250Acontinuous%2520joint%2520domains%2520for%2520contrastive%2520training%2520under%2520the%2520supervision%2520of%2520a%250Anovel%2520triplet%2520InfoNCE%2520loss.%2520Our%2520approach%2520significantly%2520and%2520comprehensively%250Aoutperforms%2520the%2520state-of-the-art%2520methods%2520on%2520various%2520benchmarks%252C%2520and%2520clearly%250Adifferentiates%2520proteins%2520carrying%2520distinct%2520functions%2520compared%2520to%2520the%2520competitor.%250AOur%2520implementation%2520is%2520available%2520at%250Ahttps%253A//github.com/AI-HPC-Research-Team/ProtFAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15158v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProtFAD%3A%20Introducing%20function-aware%20domains%20as%20implicit%20modality%20towards%0A%20%20protein%20function%20prediction&entry.906535625=Mingqing%20Wang%20and%20Zhiwei%20Nie%20and%20Yonghong%20He%20and%20Athanasios%20V.%20Vasilakos%20and%20Zhixiang%20Ren&entry.1292438233=%20%20Protein%20function%20prediction%20is%20currently%20achieved%20by%20encoding%20its%20sequence%20or%0Astructure%2C%20where%20the%20sequence-to-function%20transcendence%20and%20high-quality%0Astructural%20data%20scarcity%20lead%20to%20obvious%20performance%20bottlenecks.%20Protein%0Adomains%20are%20%22building%20blocks%22%20of%20proteins%20that%20are%20functionally%20independent%2C%0Aand%20their%20combinations%20determine%20the%20diverse%20biological%20functions.%20However%2C%0Amost%20existing%20studies%20have%20yet%20to%20thoroughly%20explore%20the%20intricate%20functional%0Ainformation%20contained%20in%20the%20protein%20domains.%20To%20fill%20this%20gap%2C%20we%20propose%20a%0Asynergistic%20integration%20approach%20for%20a%20function-aware%20domain%20representation%2C%0Aand%20a%20domain-joint%20contrastive%20learning%20strategy%20to%20distinguish%20different%0Aprotein%20functions%20while%20aligning%20the%20modalities.%20Specifically%2C%20we%20align%20the%0Adomain%20semantics%20with%20GO%20terms%20and%20text%20description%20to%20pre-train%20domain%0Aembeddings.%20Furthermore%2C%20we%20partition%20proteins%20into%20multiple%20sub-views%20based%20on%0Acontinuous%20joint%20domains%20for%20contrastive%20training%20under%20the%20supervision%20of%20a%0Anovel%20triplet%20InfoNCE%20loss.%20Our%20approach%20significantly%20and%20comprehensively%0Aoutperforms%20the%20state-of-the-art%20methods%20on%20various%20benchmarks%2C%20and%20clearly%0Adifferentiates%20proteins%20carrying%20distinct%20functions%20compared%20to%20the%20competitor.%0AOur%20implementation%20is%20available%20at%0Ahttps%3A//github.com/AI-HPC-Research-Team/ProtFAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15158v2&entry.124074799=Read"},
{"title": "MLLM-LLaVA-FL: Multimodal Large Language Model Assisted Federated\n  Learning", "author": "Jianyi Zhang and Hao Frank Yang and Ang Li and Xin Guo and Pu Wang and Haiming Wang and Yiran Chen and Hai Li", "abstract": "  Previous studies on federated learning (FL) often encounter performance\ndegradation due to data heterogeneity among different clients. In light of the\nrecent advances in multimodal large language models (MLLMs), such as GPT-4v and\nLLaVA, which demonstrate their exceptional proficiency in multimodal tasks,\nsuch as image captioning and multimodal question answering. We introduce a\nnovel federated learning framework, named Multimodal Large Language Model\nAssisted Federated Learning (MLLM-LLaVA-FL), which employs powerful MLLMs at\nthe server end to address the heterogeneous and long-tailed challenges. Owing\nto the advanced cross-modality representation capabilities and the extensive\nopen-vocabulary prior knowledge of MLLMs, our framework is adept at harnessing\nthe extensive, yet previously underexploited, open-source data accessible from\nwebsites and powerful server-side computational resources. Hence, the\nMLLM-LLaVA-FL not only enhances the performance but also avoids increasing the\nrisk of privacy leakage and the computational burden on local devices,\ndistinguishing it from prior methodologies. Our framework has three key stages.\nInitially, we conduct global visual-text pretraining of the model. This\npretraining is facilitated by utilizing the extensive open-source data\navailable online, with the assistance of MLLMs. Subsequently, the pretrained\nmodel is distributed among various clients for local training. Finally, once\nthe locally trained models are transmitted back to the server, a global\nalignment is carried out under the supervision of MLLMs to further enhance the\nperformance. Experimental evaluations on established benchmarks, show that our\nframework delivers promising performance in the typical scenarios with data\nheterogeneity and long-tail distribution across different clients in FL.\n", "link": "http://arxiv.org/abs/2409.06067v2", "date": "2024-12-02", "relevancy": 2.0949, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5583}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5102}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLLM-LLaVA-FL%3A%20Multimodal%20Large%20Language%20Model%20Assisted%20Federated%0A%20%20Learning&body=Title%3A%20MLLM-LLaVA-FL%3A%20Multimodal%20Large%20Language%20Model%20Assisted%20Federated%0A%20%20Learning%0AAuthor%3A%20Jianyi%20Zhang%20and%20Hao%20Frank%20Yang%20and%20Ang%20Li%20and%20Xin%20Guo%20and%20Pu%20Wang%20and%20Haiming%20Wang%20and%20Yiran%20Chen%20and%20Hai%20Li%0AAbstract%3A%20%20%20Previous%20studies%20on%20federated%20learning%20%28FL%29%20often%20encounter%20performance%0Adegradation%20due%20to%20data%20heterogeneity%20among%20different%20clients.%20In%20light%20of%20the%0Arecent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20such%20as%20GPT-4v%20and%0ALLaVA%2C%20which%20demonstrate%20their%20exceptional%20proficiency%20in%20multimodal%20tasks%2C%0Asuch%20as%20image%20captioning%20and%20multimodal%20question%20answering.%20We%20introduce%20a%0Anovel%20federated%20learning%20framework%2C%20named%20Multimodal%20Large%20Language%20Model%0AAssisted%20Federated%20Learning%20%28MLLM-LLaVA-FL%29%2C%20which%20employs%20powerful%20MLLMs%20at%0Athe%20server%20end%20to%20address%20the%20heterogeneous%20and%20long-tailed%20challenges.%20Owing%0Ato%20the%20advanced%20cross-modality%20representation%20capabilities%20and%20the%20extensive%0Aopen-vocabulary%20prior%20knowledge%20of%20MLLMs%2C%20our%20framework%20is%20adept%20at%20harnessing%0Athe%20extensive%2C%20yet%20previously%20underexploited%2C%20open-source%20data%20accessible%20from%0Awebsites%20and%20powerful%20server-side%20computational%20resources.%20Hence%2C%20the%0AMLLM-LLaVA-FL%20not%20only%20enhances%20the%20performance%20but%20also%20avoids%20increasing%20the%0Arisk%20of%20privacy%20leakage%20and%20the%20computational%20burden%20on%20local%20devices%2C%0Adistinguishing%20it%20from%20prior%20methodologies.%20Our%20framework%20has%20three%20key%20stages.%0AInitially%2C%20we%20conduct%20global%20visual-text%20pretraining%20of%20the%20model.%20This%0Apretraining%20is%20facilitated%20by%20utilizing%20the%20extensive%20open-source%20data%0Aavailable%20online%2C%20with%20the%20assistance%20of%20MLLMs.%20Subsequently%2C%20the%20pretrained%0Amodel%20is%20distributed%20among%20various%20clients%20for%20local%20training.%20Finally%2C%20once%0Athe%20locally%20trained%20models%20are%20transmitted%20back%20to%20the%20server%2C%20a%20global%0Aalignment%20is%20carried%20out%20under%20the%20supervision%20of%20MLLMs%20to%20further%20enhance%20the%0Aperformance.%20Experimental%20evaluations%20on%20established%20benchmarks%2C%20show%20that%20our%0Aframework%20delivers%20promising%20performance%20in%20the%20typical%20scenarios%20with%20data%0Aheterogeneity%20and%20long-tail%20distribution%20across%20different%20clients%20in%20FL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06067v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLLM-LLaVA-FL%253A%2520Multimodal%2520Large%2520Language%2520Model%2520Assisted%2520Federated%250A%2520%2520Learning%26entry.906535625%3DJianyi%2520Zhang%2520and%2520Hao%2520Frank%2520Yang%2520and%2520Ang%2520Li%2520and%2520Xin%2520Guo%2520and%2520Pu%2520Wang%2520and%2520Haiming%2520Wang%2520and%2520Yiran%2520Chen%2520and%2520Hai%2520Li%26entry.1292438233%3D%2520%2520Previous%2520studies%2520on%2520federated%2520learning%2520%2528FL%2529%2520often%2520encounter%2520performance%250Adegradation%2520due%2520to%2520data%2520heterogeneity%2520among%2520different%2520clients.%2520In%2520light%2520of%2520the%250Arecent%2520advances%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520such%2520as%2520GPT-4v%2520and%250ALLaVA%252C%2520which%2520demonstrate%2520their%2520exceptional%2520proficiency%2520in%2520multimodal%2520tasks%252C%250Asuch%2520as%2520image%2520captioning%2520and%2520multimodal%2520question%2520answering.%2520We%2520introduce%2520a%250Anovel%2520federated%2520learning%2520framework%252C%2520named%2520Multimodal%2520Large%2520Language%2520Model%250AAssisted%2520Federated%2520Learning%2520%2528MLLM-LLaVA-FL%2529%252C%2520which%2520employs%2520powerful%2520MLLMs%2520at%250Athe%2520server%2520end%2520to%2520address%2520the%2520heterogeneous%2520and%2520long-tailed%2520challenges.%2520Owing%250Ato%2520the%2520advanced%2520cross-modality%2520representation%2520capabilities%2520and%2520the%2520extensive%250Aopen-vocabulary%2520prior%2520knowledge%2520of%2520MLLMs%252C%2520our%2520framework%2520is%2520adept%2520at%2520harnessing%250Athe%2520extensive%252C%2520yet%2520previously%2520underexploited%252C%2520open-source%2520data%2520accessible%2520from%250Awebsites%2520and%2520powerful%2520server-side%2520computational%2520resources.%2520Hence%252C%2520the%250AMLLM-LLaVA-FL%2520not%2520only%2520enhances%2520the%2520performance%2520but%2520also%2520avoids%2520increasing%2520the%250Arisk%2520of%2520privacy%2520leakage%2520and%2520the%2520computational%2520burden%2520on%2520local%2520devices%252C%250Adistinguishing%2520it%2520from%2520prior%2520methodologies.%2520Our%2520framework%2520has%2520three%2520key%2520stages.%250AInitially%252C%2520we%2520conduct%2520global%2520visual-text%2520pretraining%2520of%2520the%2520model.%2520This%250Apretraining%2520is%2520facilitated%2520by%2520utilizing%2520the%2520extensive%2520open-source%2520data%250Aavailable%2520online%252C%2520with%2520the%2520assistance%2520of%2520MLLMs.%2520Subsequently%252C%2520the%2520pretrained%250Amodel%2520is%2520distributed%2520among%2520various%2520clients%2520for%2520local%2520training.%2520Finally%252C%2520once%250Athe%2520locally%2520trained%2520models%2520are%2520transmitted%2520back%2520to%2520the%2520server%252C%2520a%2520global%250Aalignment%2520is%2520carried%2520out%2520under%2520the%2520supervision%2520of%2520MLLMs%2520to%2520further%2520enhance%2520the%250Aperformance.%2520Experimental%2520evaluations%2520on%2520established%2520benchmarks%252C%2520show%2520that%2520our%250Aframework%2520delivers%2520promising%2520performance%2520in%2520the%2520typical%2520scenarios%2520with%2520data%250Aheterogeneity%2520and%2520long-tail%2520distribution%2520across%2520different%2520clients%2520in%2520FL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06067v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLLM-LLaVA-FL%3A%20Multimodal%20Large%20Language%20Model%20Assisted%20Federated%0A%20%20Learning&entry.906535625=Jianyi%20Zhang%20and%20Hao%20Frank%20Yang%20and%20Ang%20Li%20and%20Xin%20Guo%20and%20Pu%20Wang%20and%20Haiming%20Wang%20and%20Yiran%20Chen%20and%20Hai%20Li&entry.1292438233=%20%20Previous%20studies%20on%20federated%20learning%20%28FL%29%20often%20encounter%20performance%0Adegradation%20due%20to%20data%20heterogeneity%20among%20different%20clients.%20In%20light%20of%20the%0Arecent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20such%20as%20GPT-4v%20and%0ALLaVA%2C%20which%20demonstrate%20their%20exceptional%20proficiency%20in%20multimodal%20tasks%2C%0Asuch%20as%20image%20captioning%20and%20multimodal%20question%20answering.%20We%20introduce%20a%0Anovel%20federated%20learning%20framework%2C%20named%20Multimodal%20Large%20Language%20Model%0AAssisted%20Federated%20Learning%20%28MLLM-LLaVA-FL%29%2C%20which%20employs%20powerful%20MLLMs%20at%0Athe%20server%20end%20to%20address%20the%20heterogeneous%20and%20long-tailed%20challenges.%20Owing%0Ato%20the%20advanced%20cross-modality%20representation%20capabilities%20and%20the%20extensive%0Aopen-vocabulary%20prior%20knowledge%20of%20MLLMs%2C%20our%20framework%20is%20adept%20at%20harnessing%0Athe%20extensive%2C%20yet%20previously%20underexploited%2C%20open-source%20data%20accessible%20from%0Awebsites%20and%20powerful%20server-side%20computational%20resources.%20Hence%2C%20the%0AMLLM-LLaVA-FL%20not%20only%20enhances%20the%20performance%20but%20also%20avoids%20increasing%20the%0Arisk%20of%20privacy%20leakage%20and%20the%20computational%20burden%20on%20local%20devices%2C%0Adistinguishing%20it%20from%20prior%20methodologies.%20Our%20framework%20has%20three%20key%20stages.%0AInitially%2C%20we%20conduct%20global%20visual-text%20pretraining%20of%20the%20model.%20This%0Apretraining%20is%20facilitated%20by%20utilizing%20the%20extensive%20open-source%20data%0Aavailable%20online%2C%20with%20the%20assistance%20of%20MLLMs.%20Subsequently%2C%20the%20pretrained%0Amodel%20is%20distributed%20among%20various%20clients%20for%20local%20training.%20Finally%2C%20once%0Athe%20locally%20trained%20models%20are%20transmitted%20back%20to%20the%20server%2C%20a%20global%0Aalignment%20is%20carried%20out%20under%20the%20supervision%20of%20MLLMs%20to%20further%20enhance%20the%0Aperformance.%20Experimental%20evaluations%20on%20established%20benchmarks%2C%20show%20that%20our%0Aframework%20delivers%20promising%20performance%20in%20the%20typical%20scenarios%20with%20data%0Aheterogeneity%20and%20long-tail%20distribution%20across%20different%20clients%20in%20FL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06067v2&entry.124074799=Read"},
{"title": "BK-SDM: A Lightweight, Fast, and Cheap Version of Stable Diffusion", "author": "Bo-Kyeong Kim and Hyoung-Kyu Song and Thibault Castells and Shinkook Choi", "abstract": "  Text-to-image (T2I) generation with Stable Diffusion models (SDMs) involves\nhigh computing demands due to billion-scale parameters. To enhance efficiency,\nrecent studies have reduced sampling steps and applied network quantization\nwhile retaining the original architectures. The lack of architectural reduction\nattempts may stem from worries over expensive retraining for such massive\nmodels. In this work, we uncover the surprising potential of block pruning and\nfeature distillation for low-cost general-purpose T2I. By removing several\nresidual and attention blocks from the U-Net of SDMs, we achieve 30%~50%\nreduction in model size, MACs, and latency. We show that distillation\nretraining is effective even under limited resources: using only 13 A100 days\nand a tiny dataset, our compact models can imitate the original SDMs (v1.4 and\nv2.1-base with over 6,000 A100 days). Benefiting from the transferred\nknowledge, our BK-SDMs deliver competitive results on zero-shot MS-COCO against\nlarger multi-billion parameter models. We further demonstrate the applicability\nof our lightweight backbones in personalized generation and image-to-image\ntranslation. Deployment of our models on edge devices attains 4-second\ninference. Code and models can be found at:\nhttps://github.com/Nota-NetsPresso/BK-SDM\n", "link": "http://arxiv.org/abs/2305.15798v4", "date": "2024-12-02", "relevancy": 2.0802, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.7607}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6976}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BK-SDM%3A%20A%20Lightweight%2C%20Fast%2C%20and%20Cheap%20Version%20of%20Stable%20Diffusion&body=Title%3A%20BK-SDM%3A%20A%20Lightweight%2C%20Fast%2C%20and%20Cheap%20Version%20of%20Stable%20Diffusion%0AAuthor%3A%20Bo-Kyeong%20Kim%20and%20Hyoung-Kyu%20Song%20and%20Thibault%20Castells%20and%20Shinkook%20Choi%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20generation%20with%20Stable%20Diffusion%20models%20%28SDMs%29%20involves%0Ahigh%20computing%20demands%20due%20to%20billion-scale%20parameters.%20To%20enhance%20efficiency%2C%0Arecent%20studies%20have%20reduced%20sampling%20steps%20and%20applied%20network%20quantization%0Awhile%20retaining%20the%20original%20architectures.%20The%20lack%20of%20architectural%20reduction%0Aattempts%20may%20stem%20from%20worries%20over%20expensive%20retraining%20for%20such%20massive%0Amodels.%20In%20this%20work%2C%20we%20uncover%20the%20surprising%20potential%20of%20block%20pruning%20and%0Afeature%20distillation%20for%20low-cost%20general-purpose%20T2I.%20By%20removing%20several%0Aresidual%20and%20attention%20blocks%20from%20the%20U-Net%20of%20SDMs%2C%20we%20achieve%2030%25~50%25%0Areduction%20in%20model%20size%2C%20MACs%2C%20and%20latency.%20We%20show%20that%20distillation%0Aretraining%20is%20effective%20even%20under%20limited%20resources%3A%20using%20only%2013%20A100%20days%0Aand%20a%20tiny%20dataset%2C%20our%20compact%20models%20can%20imitate%20the%20original%20SDMs%20%28v1.4%20and%0Av2.1-base%20with%20over%206%2C000%20A100%20days%29.%20Benefiting%20from%20the%20transferred%0Aknowledge%2C%20our%20BK-SDMs%20deliver%20competitive%20results%20on%20zero-shot%20MS-COCO%20against%0Alarger%20multi-billion%20parameter%20models.%20We%20further%20demonstrate%20the%20applicability%0Aof%20our%20lightweight%20backbones%20in%20personalized%20generation%20and%20image-to-image%0Atranslation.%20Deployment%20of%20our%20models%20on%20edge%20devices%20attains%204-second%0Ainference.%20Code%20and%20models%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/Nota-NetsPresso/BK-SDM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.15798v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBK-SDM%253A%2520A%2520Lightweight%252C%2520Fast%252C%2520and%2520Cheap%2520Version%2520of%2520Stable%2520Diffusion%26entry.906535625%3DBo-Kyeong%2520Kim%2520and%2520Hyoung-Kyu%2520Song%2520and%2520Thibault%2520Castells%2520and%2520Shinkook%2520Choi%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520generation%2520with%2520Stable%2520Diffusion%2520models%2520%2528SDMs%2529%2520involves%250Ahigh%2520computing%2520demands%2520due%2520to%2520billion-scale%2520parameters.%2520To%2520enhance%2520efficiency%252C%250Arecent%2520studies%2520have%2520reduced%2520sampling%2520steps%2520and%2520applied%2520network%2520quantization%250Awhile%2520retaining%2520the%2520original%2520architectures.%2520The%2520lack%2520of%2520architectural%2520reduction%250Aattempts%2520may%2520stem%2520from%2520worries%2520over%2520expensive%2520retraining%2520for%2520such%2520massive%250Amodels.%2520In%2520this%2520work%252C%2520we%2520uncover%2520the%2520surprising%2520potential%2520of%2520block%2520pruning%2520and%250Afeature%2520distillation%2520for%2520low-cost%2520general-purpose%2520T2I.%2520By%2520removing%2520several%250Aresidual%2520and%2520attention%2520blocks%2520from%2520the%2520U-Net%2520of%2520SDMs%252C%2520we%2520achieve%252030%2525~50%2525%250Areduction%2520in%2520model%2520size%252C%2520MACs%252C%2520and%2520latency.%2520We%2520show%2520that%2520distillation%250Aretraining%2520is%2520effective%2520even%2520under%2520limited%2520resources%253A%2520using%2520only%252013%2520A100%2520days%250Aand%2520a%2520tiny%2520dataset%252C%2520our%2520compact%2520models%2520can%2520imitate%2520the%2520original%2520SDMs%2520%2528v1.4%2520and%250Av2.1-base%2520with%2520over%25206%252C000%2520A100%2520days%2529.%2520Benefiting%2520from%2520the%2520transferred%250Aknowledge%252C%2520our%2520BK-SDMs%2520deliver%2520competitive%2520results%2520on%2520zero-shot%2520MS-COCO%2520against%250Alarger%2520multi-billion%2520parameter%2520models.%2520We%2520further%2520demonstrate%2520the%2520applicability%250Aof%2520our%2520lightweight%2520backbones%2520in%2520personalized%2520generation%2520and%2520image-to-image%250Atranslation.%2520Deployment%2520of%2520our%2520models%2520on%2520edge%2520devices%2520attains%25204-second%250Ainference.%2520Code%2520and%2520models%2520can%2520be%2520found%2520at%253A%250Ahttps%253A//github.com/Nota-NetsPresso/BK-SDM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.15798v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BK-SDM%3A%20A%20Lightweight%2C%20Fast%2C%20and%20Cheap%20Version%20of%20Stable%20Diffusion&entry.906535625=Bo-Kyeong%20Kim%20and%20Hyoung-Kyu%20Song%20and%20Thibault%20Castells%20and%20Shinkook%20Choi&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20generation%20with%20Stable%20Diffusion%20models%20%28SDMs%29%20involves%0Ahigh%20computing%20demands%20due%20to%20billion-scale%20parameters.%20To%20enhance%20efficiency%2C%0Arecent%20studies%20have%20reduced%20sampling%20steps%20and%20applied%20network%20quantization%0Awhile%20retaining%20the%20original%20architectures.%20The%20lack%20of%20architectural%20reduction%0Aattempts%20may%20stem%20from%20worries%20over%20expensive%20retraining%20for%20such%20massive%0Amodels.%20In%20this%20work%2C%20we%20uncover%20the%20surprising%20potential%20of%20block%20pruning%20and%0Afeature%20distillation%20for%20low-cost%20general-purpose%20T2I.%20By%20removing%20several%0Aresidual%20and%20attention%20blocks%20from%20the%20U-Net%20of%20SDMs%2C%20we%20achieve%2030%25~50%25%0Areduction%20in%20model%20size%2C%20MACs%2C%20and%20latency.%20We%20show%20that%20distillation%0Aretraining%20is%20effective%20even%20under%20limited%20resources%3A%20using%20only%2013%20A100%20days%0Aand%20a%20tiny%20dataset%2C%20our%20compact%20models%20can%20imitate%20the%20original%20SDMs%20%28v1.4%20and%0Av2.1-base%20with%20over%206%2C000%20A100%20days%29.%20Benefiting%20from%20the%20transferred%0Aknowledge%2C%20our%20BK-SDMs%20deliver%20competitive%20results%20on%20zero-shot%20MS-COCO%20against%0Alarger%20multi-billion%20parameter%20models.%20We%20further%20demonstrate%20the%20applicability%0Aof%20our%20lightweight%20backbones%20in%20personalized%20generation%20and%20image-to-image%0Atranslation.%20Deployment%20of%20our%20models%20on%20edge%20devices%20attains%204-second%0Ainference.%20Code%20and%20models%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/Nota-NetsPresso/BK-SDM%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.15798v4&entry.124074799=Read"},
{"title": "Depression Detection and Analysis using Large Language Models on Textual\n  and Audio-Visual Modalities", "author": "Chayan Tank and Sarthak Pol and Vinayak Katoch and Shaina Mehta and Avinash Anand and Rajiv Ratn Shah", "abstract": "  Depression has proven to be a significant public health issue, profoundly\naffecting the psychological well-being of individuals. If it remains\nundiagnosed, depression can lead to severe health issues, which can manifest\nphysically and even lead to suicide. Generally, Diagnosing depression or any\nother mental disorder involves conducting semi-structured interviews alongside\nsupplementary questionnaires, including variants of the Patient Health\nQuestionnaire (PHQ) by Clinicians and mental health professionals. This\napproach places significant reliance on the experience and judgment of trained\nphysicians, making the diagnosis susceptible to personal biases. Given that the\nunderlying mechanisms causing depression are still being actively researched,\nphysicians often face challenges in diagnosing and treating the condition,\nparticularly in its early stages of clinical presentation. Recently,\nsignificant strides have been made in Artificial neural computing to solve\nproblems involving text, image, and speech in various domains. Our analysis has\naimed to leverage these state-of-the-art (SOTA) models in our experiments to\nachieve optimal outcomes leveraging multiple modalities. The experiments were\nperformed on the Extended Distress Analysis Interview Corpus Wizard of Oz\ndataset (E-DAIC) corpus presented in the Audio/Visual Emotion Challenge (AVEC)\n2019 Challenge. The proposed solutions demonstrate better results achieved by\nProprietary and Open-source Large Language Models (LLMs), which achieved a Root\nMean Square Error (RMSE) score of 3.98 on Textual Modality, beating the AVEC\n2019 challenge baseline results and current SOTA regression analysis\narchitectures. Additionally, the proposed solution achieved an accuracy of\n71.43% in the classification task. The paper also includes a novel audio-visual\nmulti-modal network that predicts PHQ-8 scores with an RMSE of 6.51.\n", "link": "http://arxiv.org/abs/2407.06125v2", "date": "2024-12-02", "relevancy": 2.0515, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5127}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depression%20Detection%20and%20Analysis%20using%20Large%20Language%20Models%20on%20Textual%0A%20%20and%20Audio-Visual%20Modalities&body=Title%3A%20Depression%20Detection%20and%20Analysis%20using%20Large%20Language%20Models%20on%20Textual%0A%20%20and%20Audio-Visual%20Modalities%0AAuthor%3A%20Chayan%20Tank%20and%20Sarthak%20Pol%20and%20Vinayak%20Katoch%20and%20Shaina%20Mehta%20and%20Avinash%20Anand%20and%20Rajiv%20Ratn%20Shah%0AAbstract%3A%20%20%20Depression%20has%20proven%20to%20be%20a%20significant%20public%20health%20issue%2C%20profoundly%0Aaffecting%20the%20psychological%20well-being%20of%20individuals.%20If%20it%20remains%0Aundiagnosed%2C%20depression%20can%20lead%20to%20severe%20health%20issues%2C%20which%20can%20manifest%0Aphysically%20and%20even%20lead%20to%20suicide.%20Generally%2C%20Diagnosing%20depression%20or%20any%0Aother%20mental%20disorder%20involves%20conducting%20semi-structured%20interviews%20alongside%0Asupplementary%20questionnaires%2C%20including%20variants%20of%20the%20Patient%20Health%0AQuestionnaire%20%28PHQ%29%20by%20Clinicians%20and%20mental%20health%20professionals.%20This%0Aapproach%20places%20significant%20reliance%20on%20the%20experience%20and%20judgment%20of%20trained%0Aphysicians%2C%20making%20the%20diagnosis%20susceptible%20to%20personal%20biases.%20Given%20that%20the%0Aunderlying%20mechanisms%20causing%20depression%20are%20still%20being%20actively%20researched%2C%0Aphysicians%20often%20face%20challenges%20in%20diagnosing%20and%20treating%20the%20condition%2C%0Aparticularly%20in%20its%20early%20stages%20of%20clinical%20presentation.%20Recently%2C%0Asignificant%20strides%20have%20been%20made%20in%20Artificial%20neural%20computing%20to%20solve%0Aproblems%20involving%20text%2C%20image%2C%20and%20speech%20in%20various%20domains.%20Our%20analysis%20has%0Aaimed%20to%20leverage%20these%20state-of-the-art%20%28SOTA%29%20models%20in%20our%20experiments%20to%0Aachieve%20optimal%20outcomes%20leveraging%20multiple%20modalities.%20The%20experiments%20were%0Aperformed%20on%20the%20Extended%20Distress%20Analysis%20Interview%20Corpus%20Wizard%20of%20Oz%0Adataset%20%28E-DAIC%29%20corpus%20presented%20in%20the%20Audio/Visual%20Emotion%20Challenge%20%28AVEC%29%0A2019%20Challenge.%20The%20proposed%20solutions%20demonstrate%20better%20results%20achieved%20by%0AProprietary%20and%20Open-source%20Large%20Language%20Models%20%28LLMs%29%2C%20which%20achieved%20a%20Root%0AMean%20Square%20Error%20%28RMSE%29%20score%20of%203.98%20on%20Textual%20Modality%2C%20beating%20the%20AVEC%0A2019%20challenge%20baseline%20results%20and%20current%20SOTA%20regression%20analysis%0Aarchitectures.%20Additionally%2C%20the%20proposed%20solution%20achieved%20an%20accuracy%20of%0A71.43%25%20in%20the%20classification%20task.%20The%20paper%20also%20includes%20a%20novel%20audio-visual%0Amulti-modal%20network%20that%20predicts%20PHQ-8%20scores%20with%20an%20RMSE%20of%206.51.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06125v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepression%2520Detection%2520and%2520Analysis%2520using%2520Large%2520Language%2520Models%2520on%2520Textual%250A%2520%2520and%2520Audio-Visual%2520Modalities%26entry.906535625%3DChayan%2520Tank%2520and%2520Sarthak%2520Pol%2520and%2520Vinayak%2520Katoch%2520and%2520Shaina%2520Mehta%2520and%2520Avinash%2520Anand%2520and%2520Rajiv%2520Ratn%2520Shah%26entry.1292438233%3D%2520%2520Depression%2520has%2520proven%2520to%2520be%2520a%2520significant%2520public%2520health%2520issue%252C%2520profoundly%250Aaffecting%2520the%2520psychological%2520well-being%2520of%2520individuals.%2520If%2520it%2520remains%250Aundiagnosed%252C%2520depression%2520can%2520lead%2520to%2520severe%2520health%2520issues%252C%2520which%2520can%2520manifest%250Aphysically%2520and%2520even%2520lead%2520to%2520suicide.%2520Generally%252C%2520Diagnosing%2520depression%2520or%2520any%250Aother%2520mental%2520disorder%2520involves%2520conducting%2520semi-structured%2520interviews%2520alongside%250Asupplementary%2520questionnaires%252C%2520including%2520variants%2520of%2520the%2520Patient%2520Health%250AQuestionnaire%2520%2528PHQ%2529%2520by%2520Clinicians%2520and%2520mental%2520health%2520professionals.%2520This%250Aapproach%2520places%2520significant%2520reliance%2520on%2520the%2520experience%2520and%2520judgment%2520of%2520trained%250Aphysicians%252C%2520making%2520the%2520diagnosis%2520susceptible%2520to%2520personal%2520biases.%2520Given%2520that%2520the%250Aunderlying%2520mechanisms%2520causing%2520depression%2520are%2520still%2520being%2520actively%2520researched%252C%250Aphysicians%2520often%2520face%2520challenges%2520in%2520diagnosing%2520and%2520treating%2520the%2520condition%252C%250Aparticularly%2520in%2520its%2520early%2520stages%2520of%2520clinical%2520presentation.%2520Recently%252C%250Asignificant%2520strides%2520have%2520been%2520made%2520in%2520Artificial%2520neural%2520computing%2520to%2520solve%250Aproblems%2520involving%2520text%252C%2520image%252C%2520and%2520speech%2520in%2520various%2520domains.%2520Our%2520analysis%2520has%250Aaimed%2520to%2520leverage%2520these%2520state-of-the-art%2520%2528SOTA%2529%2520models%2520in%2520our%2520experiments%2520to%250Aachieve%2520optimal%2520outcomes%2520leveraging%2520multiple%2520modalities.%2520The%2520experiments%2520were%250Aperformed%2520on%2520the%2520Extended%2520Distress%2520Analysis%2520Interview%2520Corpus%2520Wizard%2520of%2520Oz%250Adataset%2520%2528E-DAIC%2529%2520corpus%2520presented%2520in%2520the%2520Audio/Visual%2520Emotion%2520Challenge%2520%2528AVEC%2529%250A2019%2520Challenge.%2520The%2520proposed%2520solutions%2520demonstrate%2520better%2520results%2520achieved%2520by%250AProprietary%2520and%2520Open-source%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520which%2520achieved%2520a%2520Root%250AMean%2520Square%2520Error%2520%2528RMSE%2529%2520score%2520of%25203.98%2520on%2520Textual%2520Modality%252C%2520beating%2520the%2520AVEC%250A2019%2520challenge%2520baseline%2520results%2520and%2520current%2520SOTA%2520regression%2520analysis%250Aarchitectures.%2520Additionally%252C%2520the%2520proposed%2520solution%2520achieved%2520an%2520accuracy%2520of%250A71.43%2525%2520in%2520the%2520classification%2520task.%2520The%2520paper%2520also%2520includes%2520a%2520novel%2520audio-visual%250Amulti-modal%2520network%2520that%2520predicts%2520PHQ-8%2520scores%2520with%2520an%2520RMSE%2520of%25206.51.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06125v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depression%20Detection%20and%20Analysis%20using%20Large%20Language%20Models%20on%20Textual%0A%20%20and%20Audio-Visual%20Modalities&entry.906535625=Chayan%20Tank%20and%20Sarthak%20Pol%20and%20Vinayak%20Katoch%20and%20Shaina%20Mehta%20and%20Avinash%20Anand%20and%20Rajiv%20Ratn%20Shah&entry.1292438233=%20%20Depression%20has%20proven%20to%20be%20a%20significant%20public%20health%20issue%2C%20profoundly%0Aaffecting%20the%20psychological%20well-being%20of%20individuals.%20If%20it%20remains%0Aundiagnosed%2C%20depression%20can%20lead%20to%20severe%20health%20issues%2C%20which%20can%20manifest%0Aphysically%20and%20even%20lead%20to%20suicide.%20Generally%2C%20Diagnosing%20depression%20or%20any%0Aother%20mental%20disorder%20involves%20conducting%20semi-structured%20interviews%20alongside%0Asupplementary%20questionnaires%2C%20including%20variants%20of%20the%20Patient%20Health%0AQuestionnaire%20%28PHQ%29%20by%20Clinicians%20and%20mental%20health%20professionals.%20This%0Aapproach%20places%20significant%20reliance%20on%20the%20experience%20and%20judgment%20of%20trained%0Aphysicians%2C%20making%20the%20diagnosis%20susceptible%20to%20personal%20biases.%20Given%20that%20the%0Aunderlying%20mechanisms%20causing%20depression%20are%20still%20being%20actively%20researched%2C%0Aphysicians%20often%20face%20challenges%20in%20diagnosing%20and%20treating%20the%20condition%2C%0Aparticularly%20in%20its%20early%20stages%20of%20clinical%20presentation.%20Recently%2C%0Asignificant%20strides%20have%20been%20made%20in%20Artificial%20neural%20computing%20to%20solve%0Aproblems%20involving%20text%2C%20image%2C%20and%20speech%20in%20various%20domains.%20Our%20analysis%20has%0Aaimed%20to%20leverage%20these%20state-of-the-art%20%28SOTA%29%20models%20in%20our%20experiments%20to%0Aachieve%20optimal%20outcomes%20leveraging%20multiple%20modalities.%20The%20experiments%20were%0Aperformed%20on%20the%20Extended%20Distress%20Analysis%20Interview%20Corpus%20Wizard%20of%20Oz%0Adataset%20%28E-DAIC%29%20corpus%20presented%20in%20the%20Audio/Visual%20Emotion%20Challenge%20%28AVEC%29%0A2019%20Challenge.%20The%20proposed%20solutions%20demonstrate%20better%20results%20achieved%20by%0AProprietary%20and%20Open-source%20Large%20Language%20Models%20%28LLMs%29%2C%20which%20achieved%20a%20Root%0AMean%20Square%20Error%20%28RMSE%29%20score%20of%203.98%20on%20Textual%20Modality%2C%20beating%20the%20AVEC%0A2019%20challenge%20baseline%20results%20and%20current%20SOTA%20regression%20analysis%0Aarchitectures.%20Additionally%2C%20the%20proposed%20solution%20achieved%20an%20accuracy%20of%0A71.43%25%20in%20the%20classification%20task.%20The%20paper%20also%20includes%20a%20novel%20audio-visual%0Amulti-modal%20network%20that%20predicts%20PHQ-8%20scores%20with%20an%20RMSE%20of%206.51.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06125v2&entry.124074799=Read"},
{"title": "Object-Size-Driven Design of Convolutional Neural Networks: Virtual Axle\n  Detection based on Raw Data", "author": "Henik Riedel and Robert Steven Lorenzen and Clemens H\u00fcbler", "abstract": "  As infrastructure ages, the need for efficient monitoring methods becomes\nincreasingly critical. Bridge Weigh-In-Motion (BWIM) systems are crucial for\ncost-effective determination of loads and, consequently, the residual service\nlife of road and railway infrastructure. However, conventional BWIM systems\nrequire additional sensors for axle detection, which must be installed in\npotentially inaccessible locations or places that interfere with bridge\noperation.\n  This study presents a novel approach for real-time detection of train axles\nusing sensors arbitrarily placed on bridges, providing an alternative to\ndedicated axle detectors. The developed Virtual Axle Detector with Enhanced\nReceptive Field (VADER) has been validated on a single-track railway bridge\nusing only acceleration measurements, detecting 99.9% of axles with a spatial\nerror of 3.69cm. Using raw data as input outperformed the state-of-the-art\nspectrogram-based method in both speed and memory usage by 99%, thereby making\nreal-time application feasible for the first time.\n  Additionally, we introduce the Maximum Receptive Field (MRF) rule, a novel\napproach to optimise hyperparameters of Convolutional Neural Networks (CNNs)\nbased on the size of objects. In this context, the object size relates to the\nfundamental frequency of a bridge. The MRF rule effectively narrows the\nhyperparameter search space, overcoming the need for extensive hyperparameter\ntuning. Since the MRF rule can theoretically be applied to all unstructured\ndata, it could have implications for a wide range of deep learning problems,\nfrom earthquake prediction to object recognition.\n", "link": "http://arxiv.org/abs/2309.01574v4", "date": "2024-12-02", "relevancy": 2.041, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5327}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5057}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object-Size-Driven%20Design%20of%20Convolutional%20Neural%20Networks%3A%20Virtual%20Axle%0A%20%20Detection%20based%20on%20Raw%20Data&body=Title%3A%20Object-Size-Driven%20Design%20of%20Convolutional%20Neural%20Networks%3A%20Virtual%20Axle%0A%20%20Detection%20based%20on%20Raw%20Data%0AAuthor%3A%20Henik%20Riedel%20and%20Robert%20Steven%20Lorenzen%20and%20Clemens%20H%C3%BCbler%0AAbstract%3A%20%20%20As%20infrastructure%20ages%2C%20the%20need%20for%20efficient%20monitoring%20methods%20becomes%0Aincreasingly%20critical.%20Bridge%20Weigh-In-Motion%20%28BWIM%29%20systems%20are%20crucial%20for%0Acost-effective%20determination%20of%20loads%20and%2C%20consequently%2C%20the%20residual%20service%0Alife%20of%20road%20and%20railway%20infrastructure.%20However%2C%20conventional%20BWIM%20systems%0Arequire%20additional%20sensors%20for%20axle%20detection%2C%20which%20must%20be%20installed%20in%0Apotentially%20inaccessible%20locations%20or%20places%20that%20interfere%20with%20bridge%0Aoperation.%0A%20%20This%20study%20presents%20a%20novel%20approach%20for%20real-time%20detection%20of%20train%20axles%0Ausing%20sensors%20arbitrarily%20placed%20on%20bridges%2C%20providing%20an%20alternative%20to%0Adedicated%20axle%20detectors.%20The%20developed%20Virtual%20Axle%20Detector%20with%20Enhanced%0AReceptive%20Field%20%28VADER%29%20has%20been%20validated%20on%20a%20single-track%20railway%20bridge%0Ausing%20only%20acceleration%20measurements%2C%20detecting%2099.9%25%20of%20axles%20with%20a%20spatial%0Aerror%20of%203.69cm.%20Using%20raw%20data%20as%20input%20outperformed%20the%20state-of-the-art%0Aspectrogram-based%20method%20in%20both%20speed%20and%20memory%20usage%20by%2099%25%2C%20thereby%20making%0Areal-time%20application%20feasible%20for%20the%20first%20time.%0A%20%20Additionally%2C%20we%20introduce%20the%20Maximum%20Receptive%20Field%20%28MRF%29%20rule%2C%20a%20novel%0Aapproach%20to%20optimise%20hyperparameters%20of%20Convolutional%20Neural%20Networks%20%28CNNs%29%0Abased%20on%20the%20size%20of%20objects.%20In%20this%20context%2C%20the%20object%20size%20relates%20to%20the%0Afundamental%20frequency%20of%20a%20bridge.%20The%20MRF%20rule%20effectively%20narrows%20the%0Ahyperparameter%20search%20space%2C%20overcoming%20the%20need%20for%20extensive%20hyperparameter%0Atuning.%20Since%20the%20MRF%20rule%20can%20theoretically%20be%20applied%20to%20all%20unstructured%0Adata%2C%20it%20could%20have%20implications%20for%20a%20wide%20range%20of%20deep%20learning%20problems%2C%0Afrom%20earthquake%20prediction%20to%20object%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.01574v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject-Size-Driven%2520Design%2520of%2520Convolutional%2520Neural%2520Networks%253A%2520Virtual%2520Axle%250A%2520%2520Detection%2520based%2520on%2520Raw%2520Data%26entry.906535625%3DHenik%2520Riedel%2520and%2520Robert%2520Steven%2520Lorenzen%2520and%2520Clemens%2520H%25C3%25BCbler%26entry.1292438233%3D%2520%2520As%2520infrastructure%2520ages%252C%2520the%2520need%2520for%2520efficient%2520monitoring%2520methods%2520becomes%250Aincreasingly%2520critical.%2520Bridge%2520Weigh-In-Motion%2520%2528BWIM%2529%2520systems%2520are%2520crucial%2520for%250Acost-effective%2520determination%2520of%2520loads%2520and%252C%2520consequently%252C%2520the%2520residual%2520service%250Alife%2520of%2520road%2520and%2520railway%2520infrastructure.%2520However%252C%2520conventional%2520BWIM%2520systems%250Arequire%2520additional%2520sensors%2520for%2520axle%2520detection%252C%2520which%2520must%2520be%2520installed%2520in%250Apotentially%2520inaccessible%2520locations%2520or%2520places%2520that%2520interfere%2520with%2520bridge%250Aoperation.%250A%2520%2520This%2520study%2520presents%2520a%2520novel%2520approach%2520for%2520real-time%2520detection%2520of%2520train%2520axles%250Ausing%2520sensors%2520arbitrarily%2520placed%2520on%2520bridges%252C%2520providing%2520an%2520alternative%2520to%250Adedicated%2520axle%2520detectors.%2520The%2520developed%2520Virtual%2520Axle%2520Detector%2520with%2520Enhanced%250AReceptive%2520Field%2520%2528VADER%2529%2520has%2520been%2520validated%2520on%2520a%2520single-track%2520railway%2520bridge%250Ausing%2520only%2520acceleration%2520measurements%252C%2520detecting%252099.9%2525%2520of%2520axles%2520with%2520a%2520spatial%250Aerror%2520of%25203.69cm.%2520Using%2520raw%2520data%2520as%2520input%2520outperformed%2520the%2520state-of-the-art%250Aspectrogram-based%2520method%2520in%2520both%2520speed%2520and%2520memory%2520usage%2520by%252099%2525%252C%2520thereby%2520making%250Areal-time%2520application%2520feasible%2520for%2520the%2520first%2520time.%250A%2520%2520Additionally%252C%2520we%2520introduce%2520the%2520Maximum%2520Receptive%2520Field%2520%2528MRF%2529%2520rule%252C%2520a%2520novel%250Aapproach%2520to%2520optimise%2520hyperparameters%2520of%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%250Abased%2520on%2520the%2520size%2520of%2520objects.%2520In%2520this%2520context%252C%2520the%2520object%2520size%2520relates%2520to%2520the%250Afundamental%2520frequency%2520of%2520a%2520bridge.%2520The%2520MRF%2520rule%2520effectively%2520narrows%2520the%250Ahyperparameter%2520search%2520space%252C%2520overcoming%2520the%2520need%2520for%2520extensive%2520hyperparameter%250Atuning.%2520Since%2520the%2520MRF%2520rule%2520can%2520theoretically%2520be%2520applied%2520to%2520all%2520unstructured%250Adata%252C%2520it%2520could%2520have%2520implications%2520for%2520a%2520wide%2520range%2520of%2520deep%2520learning%2520problems%252C%250Afrom%2520earthquake%2520prediction%2520to%2520object%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.01574v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object-Size-Driven%20Design%20of%20Convolutional%20Neural%20Networks%3A%20Virtual%20Axle%0A%20%20Detection%20based%20on%20Raw%20Data&entry.906535625=Henik%20Riedel%20and%20Robert%20Steven%20Lorenzen%20and%20Clemens%20H%C3%BCbler&entry.1292438233=%20%20As%20infrastructure%20ages%2C%20the%20need%20for%20efficient%20monitoring%20methods%20becomes%0Aincreasingly%20critical.%20Bridge%20Weigh-In-Motion%20%28BWIM%29%20systems%20are%20crucial%20for%0Acost-effective%20determination%20of%20loads%20and%2C%20consequently%2C%20the%20residual%20service%0Alife%20of%20road%20and%20railway%20infrastructure.%20However%2C%20conventional%20BWIM%20systems%0Arequire%20additional%20sensors%20for%20axle%20detection%2C%20which%20must%20be%20installed%20in%0Apotentially%20inaccessible%20locations%20or%20places%20that%20interfere%20with%20bridge%0Aoperation.%0A%20%20This%20study%20presents%20a%20novel%20approach%20for%20real-time%20detection%20of%20train%20axles%0Ausing%20sensors%20arbitrarily%20placed%20on%20bridges%2C%20providing%20an%20alternative%20to%0Adedicated%20axle%20detectors.%20The%20developed%20Virtual%20Axle%20Detector%20with%20Enhanced%0AReceptive%20Field%20%28VADER%29%20has%20been%20validated%20on%20a%20single-track%20railway%20bridge%0Ausing%20only%20acceleration%20measurements%2C%20detecting%2099.9%25%20of%20axles%20with%20a%20spatial%0Aerror%20of%203.69cm.%20Using%20raw%20data%20as%20input%20outperformed%20the%20state-of-the-art%0Aspectrogram-based%20method%20in%20both%20speed%20and%20memory%20usage%20by%2099%25%2C%20thereby%20making%0Areal-time%20application%20feasible%20for%20the%20first%20time.%0A%20%20Additionally%2C%20we%20introduce%20the%20Maximum%20Receptive%20Field%20%28MRF%29%20rule%2C%20a%20novel%0Aapproach%20to%20optimise%20hyperparameters%20of%20Convolutional%20Neural%20Networks%20%28CNNs%29%0Abased%20on%20the%20size%20of%20objects.%20In%20this%20context%2C%20the%20object%20size%20relates%20to%20the%0Afundamental%20frequency%20of%20a%20bridge.%20The%20MRF%20rule%20effectively%20narrows%20the%0Ahyperparameter%20search%20space%2C%20overcoming%20the%20need%20for%20extensive%20hyperparameter%0Atuning.%20Since%20the%20MRF%20rule%20can%20theoretically%20be%20applied%20to%20all%20unstructured%0Adata%2C%20it%20could%20have%20implications%20for%20a%20wide%20range%20of%20deep%20learning%20problems%2C%0Afrom%20earthquake%20prediction%20to%20object%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.01574v4&entry.124074799=Read"},
{"title": "Combining Induction and Transduction for Abstract Reasoning", "author": "Wen-Ding Li and Keya Hu and Carter Larsen and Yuqing Wu and Simon Alford and Caleb Woo and Spencer M. Dunn and Hao Tang and Michelangelo Naim and Dat Nguyen and Wei-Long Zheng and Zenna Tavares and Yewen Pu and Kevin Ellis", "abstract": "  When learning an input-output mapping from very few examples, is it better to\nfirst infer a latent function that explains the examples, or is it better to\ndirectly predict new test outputs, e.g. using a neural network? We study this\nquestion on ARC by training neural models for induction (inferring latent\nfunctions) and transduction (directly predicting the test output for a given\ntest input). We train on synthetically generated variations of Python programs\nthat solve ARC training tasks. We find inductive and transductive models solve\ndifferent kinds of test problems, despite having the same training problems and\nsharing the same neural architecture: Inductive program synthesis excels at\nprecise computations, and at composing multiple concepts, while transduction\nsucceeds on fuzzier perceptual concepts. Ensembling them approaches human-level\nperformance on ARC.\n", "link": "http://arxiv.org/abs/2411.02272v4", "date": "2024-12-02", "relevancy": 2.0298, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Combining%20Induction%20and%20Transduction%20for%20Abstract%20Reasoning&body=Title%3A%20Combining%20Induction%20and%20Transduction%20for%20Abstract%20Reasoning%0AAuthor%3A%20Wen-Ding%20Li%20and%20Keya%20Hu%20and%20Carter%20Larsen%20and%20Yuqing%20Wu%20and%20Simon%20Alford%20and%20Caleb%20Woo%20and%20Spencer%20M.%20Dunn%20and%20Hao%20Tang%20and%20Michelangelo%20Naim%20and%20Dat%20Nguyen%20and%20Wei-Long%20Zheng%20and%20Zenna%20Tavares%20and%20Yewen%20Pu%20and%20Kevin%20Ellis%0AAbstract%3A%20%20%20When%20learning%20an%20input-output%20mapping%20from%20very%20few%20examples%2C%20is%20it%20better%20to%0Afirst%20infer%20a%20latent%20function%20that%20explains%20the%20examples%2C%20or%20is%20it%20better%20to%0Adirectly%20predict%20new%20test%20outputs%2C%20e.g.%20using%20a%20neural%20network%3F%20We%20study%20this%0Aquestion%20on%20ARC%20by%20training%20neural%20models%20for%20induction%20%28inferring%20latent%0Afunctions%29%20and%20transduction%20%28directly%20predicting%20the%20test%20output%20for%20a%20given%0Atest%20input%29.%20We%20train%20on%20synthetically%20generated%20variations%20of%20Python%20programs%0Athat%20solve%20ARC%20training%20tasks.%20We%20find%20inductive%20and%20transductive%20models%20solve%0Adifferent%20kinds%20of%20test%20problems%2C%20despite%20having%20the%20same%20training%20problems%20and%0Asharing%20the%20same%20neural%20architecture%3A%20Inductive%20program%20synthesis%20excels%20at%0Aprecise%20computations%2C%20and%20at%20composing%20multiple%20concepts%2C%20while%20transduction%0Asucceeds%20on%20fuzzier%20perceptual%20concepts.%20Ensembling%20them%20approaches%20human-level%0Aperformance%20on%20ARC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02272v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombining%2520Induction%2520and%2520Transduction%2520for%2520Abstract%2520Reasoning%26entry.906535625%3DWen-Ding%2520Li%2520and%2520Keya%2520Hu%2520and%2520Carter%2520Larsen%2520and%2520Yuqing%2520Wu%2520and%2520Simon%2520Alford%2520and%2520Caleb%2520Woo%2520and%2520Spencer%2520M.%2520Dunn%2520and%2520Hao%2520Tang%2520and%2520Michelangelo%2520Naim%2520and%2520Dat%2520Nguyen%2520and%2520Wei-Long%2520Zheng%2520and%2520Zenna%2520Tavares%2520and%2520Yewen%2520Pu%2520and%2520Kevin%2520Ellis%26entry.1292438233%3D%2520%2520When%2520learning%2520an%2520input-output%2520mapping%2520from%2520very%2520few%2520examples%252C%2520is%2520it%2520better%2520to%250Afirst%2520infer%2520a%2520latent%2520function%2520that%2520explains%2520the%2520examples%252C%2520or%2520is%2520it%2520better%2520to%250Adirectly%2520predict%2520new%2520test%2520outputs%252C%2520e.g.%2520using%2520a%2520neural%2520network%253F%2520We%2520study%2520this%250Aquestion%2520on%2520ARC%2520by%2520training%2520neural%2520models%2520for%2520induction%2520%2528inferring%2520latent%250Afunctions%2529%2520and%2520transduction%2520%2528directly%2520predicting%2520the%2520test%2520output%2520for%2520a%2520given%250Atest%2520input%2529.%2520We%2520train%2520on%2520synthetically%2520generated%2520variations%2520of%2520Python%2520programs%250Athat%2520solve%2520ARC%2520training%2520tasks.%2520We%2520find%2520inductive%2520and%2520transductive%2520models%2520solve%250Adifferent%2520kinds%2520of%2520test%2520problems%252C%2520despite%2520having%2520the%2520same%2520training%2520problems%2520and%250Asharing%2520the%2520same%2520neural%2520architecture%253A%2520Inductive%2520program%2520synthesis%2520excels%2520at%250Aprecise%2520computations%252C%2520and%2520at%2520composing%2520multiple%2520concepts%252C%2520while%2520transduction%250Asucceeds%2520on%2520fuzzier%2520perceptual%2520concepts.%2520Ensembling%2520them%2520approaches%2520human-level%250Aperformance%2520on%2520ARC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02272v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Combining%20Induction%20and%20Transduction%20for%20Abstract%20Reasoning&entry.906535625=Wen-Ding%20Li%20and%20Keya%20Hu%20and%20Carter%20Larsen%20and%20Yuqing%20Wu%20and%20Simon%20Alford%20and%20Caleb%20Woo%20and%20Spencer%20M.%20Dunn%20and%20Hao%20Tang%20and%20Michelangelo%20Naim%20and%20Dat%20Nguyen%20and%20Wei-Long%20Zheng%20and%20Zenna%20Tavares%20and%20Yewen%20Pu%20and%20Kevin%20Ellis&entry.1292438233=%20%20When%20learning%20an%20input-output%20mapping%20from%20very%20few%20examples%2C%20is%20it%20better%20to%0Afirst%20infer%20a%20latent%20function%20that%20explains%20the%20examples%2C%20or%20is%20it%20better%20to%0Adirectly%20predict%20new%20test%20outputs%2C%20e.g.%20using%20a%20neural%20network%3F%20We%20study%20this%0Aquestion%20on%20ARC%20by%20training%20neural%20models%20for%20induction%20%28inferring%20latent%0Afunctions%29%20and%20transduction%20%28directly%20predicting%20the%20test%20output%20for%20a%20given%0Atest%20input%29.%20We%20train%20on%20synthetically%20generated%20variations%20of%20Python%20programs%0Athat%20solve%20ARC%20training%20tasks.%20We%20find%20inductive%20and%20transductive%20models%20solve%0Adifferent%20kinds%20of%20test%20problems%2C%20despite%20having%20the%20same%20training%20problems%20and%0Asharing%20the%20same%20neural%20architecture%3A%20Inductive%20program%20synthesis%20excels%20at%0Aprecise%20computations%2C%20and%20at%20composing%20multiple%20concepts%2C%20while%20transduction%0Asucceeds%20on%20fuzzier%20perceptual%20concepts.%20Ensembling%20them%20approaches%20human-level%0Aperformance%20on%20ARC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02272v4&entry.124074799=Read"},
{"title": "Topology Only Pre-Training: Towards Generalised Multi-Domain Graph\n  Models", "author": "Alex O. Davies and Riku W. Green and Nirav S. Ajmeri and Telmo M. Silva Filho", "abstract": "  The principal benefit of unsupervised representation learning is that a\npre-trained model can be fine-tuned where data or labels are scarce. Existing\napproaches for graph representation learning are domain specific, maintaining\nconsistent node and edge features across the pre-training and target datasets.\nThis has precluded transfer to multiple domains. We present Topology Only\nPre-Training (ToP), a graph pre-training method based on node and edge feature\nexclusion. We show positive transfer on evaluation datasets from multiple\ndomains, including domains not present in pre-training data, running directly\ncontrary to assumptions made in contemporary works. On 75% of experiments, ToP\nmodels perform significantly $p \\leq 0.01$ better than a supervised baseline.\nPerformance is significantly positive on 85.7% of tasks when node and edge\nfeatures are used in fine-tuning. We further show that out-of-domain topologies\ncan produce more useful pre-training than in-domain. Under ToP we show better\ntransfer from non-molecule pre-training, compared to molecule pre-training, on\n79% of molecular benchmarks. Against the limited set of other generalist graph\nmodels ToP performs strongly, including against models with many orders of\nmagnitude larger. These findings show that ToP opens broad areas of research in\nboth transfer learning on scarcely populated graph domains and in graph\nfoundation models.\n", "link": "http://arxiv.org/abs/2311.03976v4", "date": "2024-12-02", "relevancy": 1.9866, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5035}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5022}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topology%20Only%20Pre-Training%3A%20Towards%20Generalised%20Multi-Domain%20Graph%0A%20%20Models&body=Title%3A%20Topology%20Only%20Pre-Training%3A%20Towards%20Generalised%20Multi-Domain%20Graph%0A%20%20Models%0AAuthor%3A%20Alex%20O.%20Davies%20and%20Riku%20W.%20Green%20and%20Nirav%20S.%20Ajmeri%20and%20Telmo%20M.%20Silva%20Filho%0AAbstract%3A%20%20%20The%20principal%20benefit%20of%20unsupervised%20representation%20learning%20is%20that%20a%0Apre-trained%20model%20can%20be%20fine-tuned%20where%20data%20or%20labels%20are%20scarce.%20Existing%0Aapproaches%20for%20graph%20representation%20learning%20are%20domain%20specific%2C%20maintaining%0Aconsistent%20node%20and%20edge%20features%20across%20the%20pre-training%20and%20target%20datasets.%0AThis%20has%20precluded%20transfer%20to%20multiple%20domains.%20We%20present%20Topology%20Only%0APre-Training%20%28ToP%29%2C%20a%20graph%20pre-training%20method%20based%20on%20node%20and%20edge%20feature%0Aexclusion.%20We%20show%20positive%20transfer%20on%20evaluation%20datasets%20from%20multiple%0Adomains%2C%20including%20domains%20not%20present%20in%20pre-training%20data%2C%20running%20directly%0Acontrary%20to%20assumptions%20made%20in%20contemporary%20works.%20On%2075%25%20of%20experiments%2C%20ToP%0Amodels%20perform%20significantly%20%24p%20%5Cleq%200.01%24%20better%20than%20a%20supervised%20baseline.%0APerformance%20is%20significantly%20positive%20on%2085.7%25%20of%20tasks%20when%20node%20and%20edge%0Afeatures%20are%20used%20in%20fine-tuning.%20We%20further%20show%20that%20out-of-domain%20topologies%0Acan%20produce%20more%20useful%20pre-training%20than%20in-domain.%20Under%20ToP%20we%20show%20better%0Atransfer%20from%20non-molecule%20pre-training%2C%20compared%20to%20molecule%20pre-training%2C%20on%0A79%25%20of%20molecular%20benchmarks.%20Against%20the%20limited%20set%20of%20other%20generalist%20graph%0Amodels%20ToP%20performs%20strongly%2C%20including%20against%20models%20with%20many%20orders%20of%0Amagnitude%20larger.%20These%20findings%20show%20that%20ToP%20opens%20broad%20areas%20of%20research%20in%0Aboth%20transfer%20learning%20on%20scarcely%20populated%20graph%20domains%20and%20in%20graph%0Afoundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.03976v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopology%2520Only%2520Pre-Training%253A%2520Towards%2520Generalised%2520Multi-Domain%2520Graph%250A%2520%2520Models%26entry.906535625%3DAlex%2520O.%2520Davies%2520and%2520Riku%2520W.%2520Green%2520and%2520Nirav%2520S.%2520Ajmeri%2520and%2520Telmo%2520M.%2520Silva%2520Filho%26entry.1292438233%3D%2520%2520The%2520principal%2520benefit%2520of%2520unsupervised%2520representation%2520learning%2520is%2520that%2520a%250Apre-trained%2520model%2520can%2520be%2520fine-tuned%2520where%2520data%2520or%2520labels%2520are%2520scarce.%2520Existing%250Aapproaches%2520for%2520graph%2520representation%2520learning%2520are%2520domain%2520specific%252C%2520maintaining%250Aconsistent%2520node%2520and%2520edge%2520features%2520across%2520the%2520pre-training%2520and%2520target%2520datasets.%250AThis%2520has%2520precluded%2520transfer%2520to%2520multiple%2520domains.%2520We%2520present%2520Topology%2520Only%250APre-Training%2520%2528ToP%2529%252C%2520a%2520graph%2520pre-training%2520method%2520based%2520on%2520node%2520and%2520edge%2520feature%250Aexclusion.%2520We%2520show%2520positive%2520transfer%2520on%2520evaluation%2520datasets%2520from%2520multiple%250Adomains%252C%2520including%2520domains%2520not%2520present%2520in%2520pre-training%2520data%252C%2520running%2520directly%250Acontrary%2520to%2520assumptions%2520made%2520in%2520contemporary%2520works.%2520On%252075%2525%2520of%2520experiments%252C%2520ToP%250Amodels%2520perform%2520significantly%2520%2524p%2520%255Cleq%25200.01%2524%2520better%2520than%2520a%2520supervised%2520baseline.%250APerformance%2520is%2520significantly%2520positive%2520on%252085.7%2525%2520of%2520tasks%2520when%2520node%2520and%2520edge%250Afeatures%2520are%2520used%2520in%2520fine-tuning.%2520We%2520further%2520show%2520that%2520out-of-domain%2520topologies%250Acan%2520produce%2520more%2520useful%2520pre-training%2520than%2520in-domain.%2520Under%2520ToP%2520we%2520show%2520better%250Atransfer%2520from%2520non-molecule%2520pre-training%252C%2520compared%2520to%2520molecule%2520pre-training%252C%2520on%250A79%2525%2520of%2520molecular%2520benchmarks.%2520Against%2520the%2520limited%2520set%2520of%2520other%2520generalist%2520graph%250Amodels%2520ToP%2520performs%2520strongly%252C%2520including%2520against%2520models%2520with%2520many%2520orders%2520of%250Amagnitude%2520larger.%2520These%2520findings%2520show%2520that%2520ToP%2520opens%2520broad%2520areas%2520of%2520research%2520in%250Aboth%2520transfer%2520learning%2520on%2520scarcely%2520populated%2520graph%2520domains%2520and%2520in%2520graph%250Afoundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.03976v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topology%20Only%20Pre-Training%3A%20Towards%20Generalised%20Multi-Domain%20Graph%0A%20%20Models&entry.906535625=Alex%20O.%20Davies%20and%20Riku%20W.%20Green%20and%20Nirav%20S.%20Ajmeri%20and%20Telmo%20M.%20Silva%20Filho&entry.1292438233=%20%20The%20principal%20benefit%20of%20unsupervised%20representation%20learning%20is%20that%20a%0Apre-trained%20model%20can%20be%20fine-tuned%20where%20data%20or%20labels%20are%20scarce.%20Existing%0Aapproaches%20for%20graph%20representation%20learning%20are%20domain%20specific%2C%20maintaining%0Aconsistent%20node%20and%20edge%20features%20across%20the%20pre-training%20and%20target%20datasets.%0AThis%20has%20precluded%20transfer%20to%20multiple%20domains.%20We%20present%20Topology%20Only%0APre-Training%20%28ToP%29%2C%20a%20graph%20pre-training%20method%20based%20on%20node%20and%20edge%20feature%0Aexclusion.%20We%20show%20positive%20transfer%20on%20evaluation%20datasets%20from%20multiple%0Adomains%2C%20including%20domains%20not%20present%20in%20pre-training%20data%2C%20running%20directly%0Acontrary%20to%20assumptions%20made%20in%20contemporary%20works.%20On%2075%25%20of%20experiments%2C%20ToP%0Amodels%20perform%20significantly%20%24p%20%5Cleq%200.01%24%20better%20than%20a%20supervised%20baseline.%0APerformance%20is%20significantly%20positive%20on%2085.7%25%20of%20tasks%20when%20node%20and%20edge%0Afeatures%20are%20used%20in%20fine-tuning.%20We%20further%20show%20that%20out-of-domain%20topologies%0Acan%20produce%20more%20useful%20pre-training%20than%20in-domain.%20Under%20ToP%20we%20show%20better%0Atransfer%20from%20non-molecule%20pre-training%2C%20compared%20to%20molecule%20pre-training%2C%20on%0A79%25%20of%20molecular%20benchmarks.%20Against%20the%20limited%20set%20of%20other%20generalist%20graph%0Amodels%20ToP%20performs%20strongly%2C%20including%20against%20models%20with%20many%20orders%20of%0Amagnitude%20larger.%20These%20findings%20show%20that%20ToP%20opens%20broad%20areas%20of%20research%20in%0Aboth%20transfer%20learning%20on%20scarcely%20populated%20graph%20domains%20and%20in%20graph%0Afoundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.03976v4&entry.124074799=Read"},
{"title": "Enriching Ontologies with Disjointness Axioms using Large Language\n  Models", "author": "Elias Crum and Antonio De Santis and Manon Ovide and Jiaxin Pan and Alessia Pisu and Nicolas Lazzari and Sebastian Rudolph", "abstract": "  Ontologies often lack explicit disjointness declarations between classes,\ndespite their usefulness for sophisticated reasoning and consistency checking\nin Knowledge Graphs. In this study, we explore the potential of Large Language\nModels (LLMs) to enrich ontologies by identifying and asserting class\ndisjointness axioms. Our approach aims at leveraging the implicit knowledge\nembedded in LLMs, using prompt engineering to elicit this knowledge for\nclassifying ontological disjointness. We validate our methodology on the\nDBpedia ontology, focusing on open-source LLMs. Our findings suggest that LLMs,\nwhen guided by effective prompt strategies, can reliably identify disjoint\nclass relationships, thus streamlining the process of ontology completion\nwithout extensive manual input. For comprehensive disjointness enrichment, we\npropose a process that takes logical relationships between disjointness and\nsubclass statements into account in order to maintain satisfiability and reduce\nthe number of calls to the LLM. This work provides a foundation for future\napplications of LLMs in automated ontology enhancement and offers insights into\noptimizing LLM performance through strategic prompt design. Our code is\npublicly available on GitHub at https://github.com/n28div/llm-disjointness.\n", "link": "http://arxiv.org/abs/2410.03235v2", "date": "2024-12-02", "relevancy": 1.9836, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4998}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4951}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enriching%20Ontologies%20with%20Disjointness%20Axioms%20using%20Large%20Language%0A%20%20Models&body=Title%3A%20Enriching%20Ontologies%20with%20Disjointness%20Axioms%20using%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Elias%20Crum%20and%20Antonio%20De%20Santis%20and%20Manon%20Ovide%20and%20Jiaxin%20Pan%20and%20Alessia%20Pisu%20and%20Nicolas%20Lazzari%20and%20Sebastian%20Rudolph%0AAbstract%3A%20%20%20Ontologies%20often%20lack%20explicit%20disjointness%20declarations%20between%20classes%2C%0Adespite%20their%20usefulness%20for%20sophisticated%20reasoning%20and%20consistency%20checking%0Ain%20Knowledge%20Graphs.%20In%20this%20study%2C%20we%20explore%20the%20potential%20of%20Large%20Language%0AModels%20%28LLMs%29%20to%20enrich%20ontologies%20by%20identifying%20and%20asserting%20class%0Adisjointness%20axioms.%20Our%20approach%20aims%20at%20leveraging%20the%20implicit%20knowledge%0Aembedded%20in%20LLMs%2C%20using%20prompt%20engineering%20to%20elicit%20this%20knowledge%20for%0Aclassifying%20ontological%20disjointness.%20We%20validate%20our%20methodology%20on%20the%0ADBpedia%20ontology%2C%20focusing%20on%20open-source%20LLMs.%20Our%20findings%20suggest%20that%20LLMs%2C%0Awhen%20guided%20by%20effective%20prompt%20strategies%2C%20can%20reliably%20identify%20disjoint%0Aclass%20relationships%2C%20thus%20streamlining%20the%20process%20of%20ontology%20completion%0Awithout%20extensive%20manual%20input.%20For%20comprehensive%20disjointness%20enrichment%2C%20we%0Apropose%20a%20process%20that%20takes%20logical%20relationships%20between%20disjointness%20and%0Asubclass%20statements%20into%20account%20in%20order%20to%20maintain%20satisfiability%20and%20reduce%0Athe%20number%20of%20calls%20to%20the%20LLM.%20This%20work%20provides%20a%20foundation%20for%20future%0Aapplications%20of%20LLMs%20in%20automated%20ontology%20enhancement%20and%20offers%20insights%20into%0Aoptimizing%20LLM%20performance%20through%20strategic%20prompt%20design.%20Our%20code%20is%0Apublicly%20available%20on%20GitHub%20at%20https%3A//github.com/n28div/llm-disjointness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03235v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnriching%2520Ontologies%2520with%2520Disjointness%2520Axioms%2520using%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DElias%2520Crum%2520and%2520Antonio%2520De%2520Santis%2520and%2520Manon%2520Ovide%2520and%2520Jiaxin%2520Pan%2520and%2520Alessia%2520Pisu%2520and%2520Nicolas%2520Lazzari%2520and%2520Sebastian%2520Rudolph%26entry.1292438233%3D%2520%2520Ontologies%2520often%2520lack%2520explicit%2520disjointness%2520declarations%2520between%2520classes%252C%250Adespite%2520their%2520usefulness%2520for%2520sophisticated%2520reasoning%2520and%2520consistency%2520checking%250Ain%2520Knowledge%2520Graphs.%2520In%2520this%2520study%252C%2520we%2520explore%2520the%2520potential%2520of%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520to%2520enrich%2520ontologies%2520by%2520identifying%2520and%2520asserting%2520class%250Adisjointness%2520axioms.%2520Our%2520approach%2520aims%2520at%2520leveraging%2520the%2520implicit%2520knowledge%250Aembedded%2520in%2520LLMs%252C%2520using%2520prompt%2520engineering%2520to%2520elicit%2520this%2520knowledge%2520for%250Aclassifying%2520ontological%2520disjointness.%2520We%2520validate%2520our%2520methodology%2520on%2520the%250ADBpedia%2520ontology%252C%2520focusing%2520on%2520open-source%2520LLMs.%2520Our%2520findings%2520suggest%2520that%2520LLMs%252C%250Awhen%2520guided%2520by%2520effective%2520prompt%2520strategies%252C%2520can%2520reliably%2520identify%2520disjoint%250Aclass%2520relationships%252C%2520thus%2520streamlining%2520the%2520process%2520of%2520ontology%2520completion%250Awithout%2520extensive%2520manual%2520input.%2520For%2520comprehensive%2520disjointness%2520enrichment%252C%2520we%250Apropose%2520a%2520process%2520that%2520takes%2520logical%2520relationships%2520between%2520disjointness%2520and%250Asubclass%2520statements%2520into%2520account%2520in%2520order%2520to%2520maintain%2520satisfiability%2520and%2520reduce%250Athe%2520number%2520of%2520calls%2520to%2520the%2520LLM.%2520This%2520work%2520provides%2520a%2520foundation%2520for%2520future%250Aapplications%2520of%2520LLMs%2520in%2520automated%2520ontology%2520enhancement%2520and%2520offers%2520insights%2520into%250Aoptimizing%2520LLM%2520performance%2520through%2520strategic%2520prompt%2520design.%2520Our%2520code%2520is%250Apublicly%2520available%2520on%2520GitHub%2520at%2520https%253A//github.com/n28div/llm-disjointness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03235v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enriching%20Ontologies%20with%20Disjointness%20Axioms%20using%20Large%20Language%0A%20%20Models&entry.906535625=Elias%20Crum%20and%20Antonio%20De%20Santis%20and%20Manon%20Ovide%20and%20Jiaxin%20Pan%20and%20Alessia%20Pisu%20and%20Nicolas%20Lazzari%20and%20Sebastian%20Rudolph&entry.1292438233=%20%20Ontologies%20often%20lack%20explicit%20disjointness%20declarations%20between%20classes%2C%0Adespite%20their%20usefulness%20for%20sophisticated%20reasoning%20and%20consistency%20checking%0Ain%20Knowledge%20Graphs.%20In%20this%20study%2C%20we%20explore%20the%20potential%20of%20Large%20Language%0AModels%20%28LLMs%29%20to%20enrich%20ontologies%20by%20identifying%20and%20asserting%20class%0Adisjointness%20axioms.%20Our%20approach%20aims%20at%20leveraging%20the%20implicit%20knowledge%0Aembedded%20in%20LLMs%2C%20using%20prompt%20engineering%20to%20elicit%20this%20knowledge%20for%0Aclassifying%20ontological%20disjointness.%20We%20validate%20our%20methodology%20on%20the%0ADBpedia%20ontology%2C%20focusing%20on%20open-source%20LLMs.%20Our%20findings%20suggest%20that%20LLMs%2C%0Awhen%20guided%20by%20effective%20prompt%20strategies%2C%20can%20reliably%20identify%20disjoint%0Aclass%20relationships%2C%20thus%20streamlining%20the%20process%20of%20ontology%20completion%0Awithout%20extensive%20manual%20input.%20For%20comprehensive%20disjointness%20enrichment%2C%20we%0Apropose%20a%20process%20that%20takes%20logical%20relationships%20between%20disjointness%20and%0Asubclass%20statements%20into%20account%20in%20order%20to%20maintain%20satisfiability%20and%20reduce%0Athe%20number%20of%20calls%20to%20the%20LLM.%20This%20work%20provides%20a%20foundation%20for%20future%0Aapplications%20of%20LLMs%20in%20automated%20ontology%20enhancement%20and%20offers%20insights%20into%0Aoptimizing%20LLM%20performance%20through%20strategic%20prompt%20design.%20Our%20code%20is%0Apublicly%20available%20on%20GitHub%20at%20https%3A//github.com/n28div/llm-disjointness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03235v2&entry.124074799=Read"},
{"title": "WOMD-Reasoning: A Large-Scale Dataset and Benchmark for Interaction and\n  Intention Reasoning in Driving", "author": "Yiheng Li and Cunxin Fan and Chongjian Ge and Zhihao Zhao and Chenran Li and Chenfeng Xu and Huaxiu Yao and Masayoshi Tomizuka and Bolei Zhou and Chen Tang and Mingyu Ding and Wei Zhan", "abstract": "  We propose Waymo Open Motion Dataset-Reasoning (WOMD-Reasoning), a\ncomprehensive large-scale dataset with 3 million Q&As built on WOMD focusing on\ndescribing and reasoning interactions and intentions in driving scenarios.\nExisting language datasets for driving primarily capture interactions caused by\nclose distances. However, interactions induced by traffic rules and human\nintentions, which can occur over long distances, are yet sufficiently covered.\nTo address this, WOMD-Reasoning presents by far the largest multi-modal Q&A\ndataset on real-world driving scenarios, covering a wide range of driving\ntopics from map descriptions and motion status descriptions to narratives and\nanalyses of agents' interactions, behaviors, and intentions. We further\nintroduce Motion-LLaVA, a motion-language model fine-tuned on the proposed\ndataset with robust interaction reasoning capabilities. We benchmark its\nperformance across various configurations including different input modalities,\nreasoning techniques, and network architectures. The robust, diverse, and\nmulti-modal nature of WOMD-Reasoning highlights its potential to advance future\nautonomous driving research and enable a broad range of applications. The\ndataset and its vision modal extension are available at\nhttps://waymo.com/open/download, and the codes & prompts to build it are\navailable at https://github.com/yhli123/WOMD-Reasoning.\n", "link": "http://arxiv.org/abs/2407.04281v2", "date": "2024-12-02", "relevancy": 1.9809, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5104}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WOMD-Reasoning%3A%20A%20Large-Scale%20Dataset%20and%20Benchmark%20for%20Interaction%20and%0A%20%20Intention%20Reasoning%20in%20Driving&body=Title%3A%20WOMD-Reasoning%3A%20A%20Large-Scale%20Dataset%20and%20Benchmark%20for%20Interaction%20and%0A%20%20Intention%20Reasoning%20in%20Driving%0AAuthor%3A%20Yiheng%20Li%20and%20Cunxin%20Fan%20and%20Chongjian%20Ge%20and%20Zhihao%20Zhao%20and%20Chenran%20Li%20and%20Chenfeng%20Xu%20and%20Huaxiu%20Yao%20and%20Masayoshi%20Tomizuka%20and%20Bolei%20Zhou%20and%20Chen%20Tang%20and%20Mingyu%20Ding%20and%20Wei%20Zhan%0AAbstract%3A%20%20%20We%20propose%20Waymo%20Open%20Motion%20Dataset-Reasoning%20%28WOMD-Reasoning%29%2C%20a%0Acomprehensive%20large-scale%20dataset%20with%203%20million%20Q%26As%20built%20on%20WOMD%20focusing%20on%0Adescribing%20and%20reasoning%20interactions%20and%20intentions%20in%20driving%20scenarios.%0AExisting%20language%20datasets%20for%20driving%20primarily%20capture%20interactions%20caused%20by%0Aclose%20distances.%20However%2C%20interactions%20induced%20by%20traffic%20rules%20and%20human%0Aintentions%2C%20which%20can%20occur%20over%20long%20distances%2C%20are%20yet%20sufficiently%20covered.%0ATo%20address%20this%2C%20WOMD-Reasoning%20presents%20by%20far%20the%20largest%20multi-modal%20Q%26A%0Adataset%20on%20real-world%20driving%20scenarios%2C%20covering%20a%20wide%20range%20of%20driving%0Atopics%20from%20map%20descriptions%20and%20motion%20status%20descriptions%20to%20narratives%20and%0Aanalyses%20of%20agents%27%20interactions%2C%20behaviors%2C%20and%20intentions.%20We%20further%0Aintroduce%20Motion-LLaVA%2C%20a%20motion-language%20model%20fine-tuned%20on%20the%20proposed%0Adataset%20with%20robust%20interaction%20reasoning%20capabilities.%20We%20benchmark%20its%0Aperformance%20across%20various%20configurations%20including%20different%20input%20modalities%2C%0Areasoning%20techniques%2C%20and%20network%20architectures.%20The%20robust%2C%20diverse%2C%20and%0Amulti-modal%20nature%20of%20WOMD-Reasoning%20highlights%20its%20potential%20to%20advance%20future%0Aautonomous%20driving%20research%20and%20enable%20a%20broad%20range%20of%20applications.%20The%0Adataset%20and%20its%20vision%20modal%20extension%20are%20available%20at%0Ahttps%3A//waymo.com/open/download%2C%20and%20the%20codes%20%26%20prompts%20to%20build%20it%20are%0Aavailable%20at%20https%3A//github.com/yhli123/WOMD-Reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04281v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWOMD-Reasoning%253A%2520A%2520Large-Scale%2520Dataset%2520and%2520Benchmark%2520for%2520Interaction%2520and%250A%2520%2520Intention%2520Reasoning%2520in%2520Driving%26entry.906535625%3DYiheng%2520Li%2520and%2520Cunxin%2520Fan%2520and%2520Chongjian%2520Ge%2520and%2520Zhihao%2520Zhao%2520and%2520Chenran%2520Li%2520and%2520Chenfeng%2520Xu%2520and%2520Huaxiu%2520Yao%2520and%2520Masayoshi%2520Tomizuka%2520and%2520Bolei%2520Zhou%2520and%2520Chen%2520Tang%2520and%2520Mingyu%2520Ding%2520and%2520Wei%2520Zhan%26entry.1292438233%3D%2520%2520We%2520propose%2520Waymo%2520Open%2520Motion%2520Dataset-Reasoning%2520%2528WOMD-Reasoning%2529%252C%2520a%250Acomprehensive%2520large-scale%2520dataset%2520with%25203%2520million%2520Q%2526As%2520built%2520on%2520WOMD%2520focusing%2520on%250Adescribing%2520and%2520reasoning%2520interactions%2520and%2520intentions%2520in%2520driving%2520scenarios.%250AExisting%2520language%2520datasets%2520for%2520driving%2520primarily%2520capture%2520interactions%2520caused%2520by%250Aclose%2520distances.%2520However%252C%2520interactions%2520induced%2520by%2520traffic%2520rules%2520and%2520human%250Aintentions%252C%2520which%2520can%2520occur%2520over%2520long%2520distances%252C%2520are%2520yet%2520sufficiently%2520covered.%250ATo%2520address%2520this%252C%2520WOMD-Reasoning%2520presents%2520by%2520far%2520the%2520largest%2520multi-modal%2520Q%2526A%250Adataset%2520on%2520real-world%2520driving%2520scenarios%252C%2520covering%2520a%2520wide%2520range%2520of%2520driving%250Atopics%2520from%2520map%2520descriptions%2520and%2520motion%2520status%2520descriptions%2520to%2520narratives%2520and%250Aanalyses%2520of%2520agents%2527%2520interactions%252C%2520behaviors%252C%2520and%2520intentions.%2520We%2520further%250Aintroduce%2520Motion-LLaVA%252C%2520a%2520motion-language%2520model%2520fine-tuned%2520on%2520the%2520proposed%250Adataset%2520with%2520robust%2520interaction%2520reasoning%2520capabilities.%2520We%2520benchmark%2520its%250Aperformance%2520across%2520various%2520configurations%2520including%2520different%2520input%2520modalities%252C%250Areasoning%2520techniques%252C%2520and%2520network%2520architectures.%2520The%2520robust%252C%2520diverse%252C%2520and%250Amulti-modal%2520nature%2520of%2520WOMD-Reasoning%2520highlights%2520its%2520potential%2520to%2520advance%2520future%250Aautonomous%2520driving%2520research%2520and%2520enable%2520a%2520broad%2520range%2520of%2520applications.%2520The%250Adataset%2520and%2520its%2520vision%2520modal%2520extension%2520are%2520available%2520at%250Ahttps%253A//waymo.com/open/download%252C%2520and%2520the%2520codes%2520%2526%2520prompts%2520to%2520build%2520it%2520are%250Aavailable%2520at%2520https%253A//github.com/yhli123/WOMD-Reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04281v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WOMD-Reasoning%3A%20A%20Large-Scale%20Dataset%20and%20Benchmark%20for%20Interaction%20and%0A%20%20Intention%20Reasoning%20in%20Driving&entry.906535625=Yiheng%20Li%20and%20Cunxin%20Fan%20and%20Chongjian%20Ge%20and%20Zhihao%20Zhao%20and%20Chenran%20Li%20and%20Chenfeng%20Xu%20and%20Huaxiu%20Yao%20and%20Masayoshi%20Tomizuka%20and%20Bolei%20Zhou%20and%20Chen%20Tang%20and%20Mingyu%20Ding%20and%20Wei%20Zhan&entry.1292438233=%20%20We%20propose%20Waymo%20Open%20Motion%20Dataset-Reasoning%20%28WOMD-Reasoning%29%2C%20a%0Acomprehensive%20large-scale%20dataset%20with%203%20million%20Q%26As%20built%20on%20WOMD%20focusing%20on%0Adescribing%20and%20reasoning%20interactions%20and%20intentions%20in%20driving%20scenarios.%0AExisting%20language%20datasets%20for%20driving%20primarily%20capture%20interactions%20caused%20by%0Aclose%20distances.%20However%2C%20interactions%20induced%20by%20traffic%20rules%20and%20human%0Aintentions%2C%20which%20can%20occur%20over%20long%20distances%2C%20are%20yet%20sufficiently%20covered.%0ATo%20address%20this%2C%20WOMD-Reasoning%20presents%20by%20far%20the%20largest%20multi-modal%20Q%26A%0Adataset%20on%20real-world%20driving%20scenarios%2C%20covering%20a%20wide%20range%20of%20driving%0Atopics%20from%20map%20descriptions%20and%20motion%20status%20descriptions%20to%20narratives%20and%0Aanalyses%20of%20agents%27%20interactions%2C%20behaviors%2C%20and%20intentions.%20We%20further%0Aintroduce%20Motion-LLaVA%2C%20a%20motion-language%20model%20fine-tuned%20on%20the%20proposed%0Adataset%20with%20robust%20interaction%20reasoning%20capabilities.%20We%20benchmark%20its%0Aperformance%20across%20various%20configurations%20including%20different%20input%20modalities%2C%0Areasoning%20techniques%2C%20and%20network%20architectures.%20The%20robust%2C%20diverse%2C%20and%0Amulti-modal%20nature%20of%20WOMD-Reasoning%20highlights%20its%20potential%20to%20advance%20future%0Aautonomous%20driving%20research%20and%20enable%20a%20broad%20range%20of%20applications.%20The%0Adataset%20and%20its%20vision%20modal%20extension%20are%20available%20at%0Ahttps%3A//waymo.com/open/download%2C%20and%20the%20codes%20%26%20prompts%20to%20build%20it%20are%0Aavailable%20at%20https%3A//github.com/yhli123/WOMD-Reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04281v2&entry.124074799=Read"},
{"title": "Reliable Generation of Privacy-preserving Synthetic Electronic Health\n  Record Time Series via Diffusion Models", "author": "Muhang Tian and Bernie Chen and Allan Guo and Shiyi Jiang and Anru R. Zhang", "abstract": "  Electronic Health Records (EHRs) are rich sources of patient-level data,\noffering valuable resources for medical data analysis. However, privacy\nconcerns often restrict access to EHRs, hindering downstream analysis. Current\nEHR de-identification methods are flawed and can lead to potential privacy\nleakage. Additionally, existing publicly available EHR databases are limited,\npreventing the advancement of medical research using EHR. This study aims to\novercome these challenges by generating realistic and privacy-preserving\nsynthetic electronic health records (EHRs) time series efficiently. We\nintroduce a new method for generating diverse and realistic synthetic EHR time\nseries data using Denoising Diffusion Probabilistic Models (DDPM). We conducted\nexperiments on six databases: Medical Information Mart for Intensive Care III\nand IV (MIMIC-III/IV), the eICU Collaborative Research Database (eICU), and\nnon-EHR datasets on Stocks and Energy. We compared our proposed method with\neight existing methods. Our results demonstrate that our approach significantly\noutperforms all existing methods in terms of data fidelity while requiring less\ntraining effort. Additionally, data generated by our method yields a lower\ndiscriminative accuracy compared to other baseline methods, indicating the\nproposed method can generate data with less privacy risk. The proposed\ndiffusion-model-based method can reliably and efficiently generate synthetic\nEHR time series, which facilitates the downstream medical data analysis. Our\nnumerical results show the superiority of the proposed method over all other\nexisting methods.\n", "link": "http://arxiv.org/abs/2310.15290v6", "date": "2024-12-02", "relevancy": 1.9762, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.497}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4921}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reliable%20Generation%20of%20Privacy-preserving%20Synthetic%20Electronic%20Health%0A%20%20Record%20Time%20Series%20via%20Diffusion%20Models&body=Title%3A%20Reliable%20Generation%20of%20Privacy-preserving%20Synthetic%20Electronic%20Health%0A%20%20Record%20Time%20Series%20via%20Diffusion%20Models%0AAuthor%3A%20Muhang%20Tian%20and%20Bernie%20Chen%20and%20Allan%20Guo%20and%20Shiyi%20Jiang%20and%20Anru%20R.%20Zhang%0AAbstract%3A%20%20%20Electronic%20Health%20Records%20%28EHRs%29%20are%20rich%20sources%20of%20patient-level%20data%2C%0Aoffering%20valuable%20resources%20for%20medical%20data%20analysis.%20However%2C%20privacy%0Aconcerns%20often%20restrict%20access%20to%20EHRs%2C%20hindering%20downstream%20analysis.%20Current%0AEHR%20de-identification%20methods%20are%20flawed%20and%20can%20lead%20to%20potential%20privacy%0Aleakage.%20Additionally%2C%20existing%20publicly%20available%20EHR%20databases%20are%20limited%2C%0Apreventing%20the%20advancement%20of%20medical%20research%20using%20EHR.%20This%20study%20aims%20to%0Aovercome%20these%20challenges%20by%20generating%20realistic%20and%20privacy-preserving%0Asynthetic%20electronic%20health%20records%20%28EHRs%29%20time%20series%20efficiently.%20We%0Aintroduce%20a%20new%20method%20for%20generating%20diverse%20and%20realistic%20synthetic%20EHR%20time%0Aseries%20data%20using%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPM%29.%20We%20conducted%0Aexperiments%20on%20six%20databases%3A%20Medical%20Information%20Mart%20for%20Intensive%20Care%20III%0Aand%20IV%20%28MIMIC-III/IV%29%2C%20the%20eICU%20Collaborative%20Research%20Database%20%28eICU%29%2C%20and%0Anon-EHR%20datasets%20on%20Stocks%20and%20Energy.%20We%20compared%20our%20proposed%20method%20with%0Aeight%20existing%20methods.%20Our%20results%20demonstrate%20that%20our%20approach%20significantly%0Aoutperforms%20all%20existing%20methods%20in%20terms%20of%20data%20fidelity%20while%20requiring%20less%0Atraining%20effort.%20Additionally%2C%20data%20generated%20by%20our%20method%20yields%20a%20lower%0Adiscriminative%20accuracy%20compared%20to%20other%20baseline%20methods%2C%20indicating%20the%0Aproposed%20method%20can%20generate%20data%20with%20less%20privacy%20risk.%20The%20proposed%0Adiffusion-model-based%20method%20can%20reliably%20and%20efficiently%20generate%20synthetic%0AEHR%20time%20series%2C%20which%20facilitates%20the%20downstream%20medical%20data%20analysis.%20Our%0Anumerical%20results%20show%20the%20superiority%20of%20the%20proposed%20method%20over%20all%20other%0Aexisting%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.15290v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReliable%2520Generation%2520of%2520Privacy-preserving%2520Synthetic%2520Electronic%2520Health%250A%2520%2520Record%2520Time%2520Series%2520via%2520Diffusion%2520Models%26entry.906535625%3DMuhang%2520Tian%2520and%2520Bernie%2520Chen%2520and%2520Allan%2520Guo%2520and%2520Shiyi%2520Jiang%2520and%2520Anru%2520R.%2520Zhang%26entry.1292438233%3D%2520%2520Electronic%2520Health%2520Records%2520%2528EHRs%2529%2520are%2520rich%2520sources%2520of%2520patient-level%2520data%252C%250Aoffering%2520valuable%2520resources%2520for%2520medical%2520data%2520analysis.%2520However%252C%2520privacy%250Aconcerns%2520often%2520restrict%2520access%2520to%2520EHRs%252C%2520hindering%2520downstream%2520analysis.%2520Current%250AEHR%2520de-identification%2520methods%2520are%2520flawed%2520and%2520can%2520lead%2520to%2520potential%2520privacy%250Aleakage.%2520Additionally%252C%2520existing%2520publicly%2520available%2520EHR%2520databases%2520are%2520limited%252C%250Apreventing%2520the%2520advancement%2520of%2520medical%2520research%2520using%2520EHR.%2520This%2520study%2520aims%2520to%250Aovercome%2520these%2520challenges%2520by%2520generating%2520realistic%2520and%2520privacy-preserving%250Asynthetic%2520electronic%2520health%2520records%2520%2528EHRs%2529%2520time%2520series%2520efficiently.%2520We%250Aintroduce%2520a%2520new%2520method%2520for%2520generating%2520diverse%2520and%2520realistic%2520synthetic%2520EHR%2520time%250Aseries%2520data%2520using%2520Denoising%2520Diffusion%2520Probabilistic%2520Models%2520%2528DDPM%2529.%2520We%2520conducted%250Aexperiments%2520on%2520six%2520databases%253A%2520Medical%2520Information%2520Mart%2520for%2520Intensive%2520Care%2520III%250Aand%2520IV%2520%2528MIMIC-III/IV%2529%252C%2520the%2520eICU%2520Collaborative%2520Research%2520Database%2520%2528eICU%2529%252C%2520and%250Anon-EHR%2520datasets%2520on%2520Stocks%2520and%2520Energy.%2520We%2520compared%2520our%2520proposed%2520method%2520with%250Aeight%2520existing%2520methods.%2520Our%2520results%2520demonstrate%2520that%2520our%2520approach%2520significantly%250Aoutperforms%2520all%2520existing%2520methods%2520in%2520terms%2520of%2520data%2520fidelity%2520while%2520requiring%2520less%250Atraining%2520effort.%2520Additionally%252C%2520data%2520generated%2520by%2520our%2520method%2520yields%2520a%2520lower%250Adiscriminative%2520accuracy%2520compared%2520to%2520other%2520baseline%2520methods%252C%2520indicating%2520the%250Aproposed%2520method%2520can%2520generate%2520data%2520with%2520less%2520privacy%2520risk.%2520The%2520proposed%250Adiffusion-model-based%2520method%2520can%2520reliably%2520and%2520efficiently%2520generate%2520synthetic%250AEHR%2520time%2520series%252C%2520which%2520facilitates%2520the%2520downstream%2520medical%2520data%2520analysis.%2520Our%250Anumerical%2520results%2520show%2520the%2520superiority%2520of%2520the%2520proposed%2520method%2520over%2520all%2520other%250Aexisting%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.15290v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reliable%20Generation%20of%20Privacy-preserving%20Synthetic%20Electronic%20Health%0A%20%20Record%20Time%20Series%20via%20Diffusion%20Models&entry.906535625=Muhang%20Tian%20and%20Bernie%20Chen%20and%20Allan%20Guo%20and%20Shiyi%20Jiang%20and%20Anru%20R.%20Zhang&entry.1292438233=%20%20Electronic%20Health%20Records%20%28EHRs%29%20are%20rich%20sources%20of%20patient-level%20data%2C%0Aoffering%20valuable%20resources%20for%20medical%20data%20analysis.%20However%2C%20privacy%0Aconcerns%20often%20restrict%20access%20to%20EHRs%2C%20hindering%20downstream%20analysis.%20Current%0AEHR%20de-identification%20methods%20are%20flawed%20and%20can%20lead%20to%20potential%20privacy%0Aleakage.%20Additionally%2C%20existing%20publicly%20available%20EHR%20databases%20are%20limited%2C%0Apreventing%20the%20advancement%20of%20medical%20research%20using%20EHR.%20This%20study%20aims%20to%0Aovercome%20these%20challenges%20by%20generating%20realistic%20and%20privacy-preserving%0Asynthetic%20electronic%20health%20records%20%28EHRs%29%20time%20series%20efficiently.%20We%0Aintroduce%20a%20new%20method%20for%20generating%20diverse%20and%20realistic%20synthetic%20EHR%20time%0Aseries%20data%20using%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPM%29.%20We%20conducted%0Aexperiments%20on%20six%20databases%3A%20Medical%20Information%20Mart%20for%20Intensive%20Care%20III%0Aand%20IV%20%28MIMIC-III/IV%29%2C%20the%20eICU%20Collaborative%20Research%20Database%20%28eICU%29%2C%20and%0Anon-EHR%20datasets%20on%20Stocks%20and%20Energy.%20We%20compared%20our%20proposed%20method%20with%0Aeight%20existing%20methods.%20Our%20results%20demonstrate%20that%20our%20approach%20significantly%0Aoutperforms%20all%20existing%20methods%20in%20terms%20of%20data%20fidelity%20while%20requiring%20less%0Atraining%20effort.%20Additionally%2C%20data%20generated%20by%20our%20method%20yields%20a%20lower%0Adiscriminative%20accuracy%20compared%20to%20other%20baseline%20methods%2C%20indicating%20the%0Aproposed%20method%20can%20generate%20data%20with%20less%20privacy%20risk.%20The%20proposed%0Adiffusion-model-based%20method%20can%20reliably%20and%20efficiently%20generate%20synthetic%0AEHR%20time%20series%2C%20which%20facilitates%20the%20downstream%20medical%20data%20analysis.%20Our%0Anumerical%20results%20show%20the%20superiority%20of%20the%20proposed%20method%20over%20all%20other%0Aexisting%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.15290v6&entry.124074799=Read"},
{"title": "Understanding LLM Embeddings for Regression", "author": "Eric Tang and Bangding Yang and Xingyou Song", "abstract": "  With the rise of large language models (LLMs) for flexibly processing\ninformation as strings, a natural application is regression, specifically by\npreprocessing string representations into LLM embeddings as downstream features\nfor metric prediction. In this paper, we provide one of the first comprehensive\ninvestigations into embedding-based regression and demonstrate that LLM\nembeddings as features can be better for high-dimensional regression tasks than\nusing traditional feature engineering. This regression performance can be\nexplained in part due to LLM embeddings over numeric data inherently preserving\nLipschitz continuity over the feature space. Furthermore, we quantify the\ncontribution of different model effects, most notably model size and language\nunderstanding, which we find surprisingly do not always improve regression\nperformance.\n", "link": "http://arxiv.org/abs/2411.14708v2", "date": "2024-12-02", "relevancy": 1.9694, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5011}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5011}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20LLM%20Embeddings%20for%20Regression&body=Title%3A%20Understanding%20LLM%20Embeddings%20for%20Regression%0AAuthor%3A%20Eric%20Tang%20and%20Bangding%20Yang%20and%20Xingyou%20Song%0AAbstract%3A%20%20%20With%20the%20rise%20of%20large%20language%20models%20%28LLMs%29%20for%20flexibly%20processing%0Ainformation%20as%20strings%2C%20a%20natural%20application%20is%20regression%2C%20specifically%20by%0Apreprocessing%20string%20representations%20into%20LLM%20embeddings%20as%20downstream%20features%0Afor%20metric%20prediction.%20In%20this%20paper%2C%20we%20provide%20one%20of%20the%20first%20comprehensive%0Ainvestigations%20into%20embedding-based%20regression%20and%20demonstrate%20that%20LLM%0Aembeddings%20as%20features%20can%20be%20better%20for%20high-dimensional%20regression%20tasks%20than%0Ausing%20traditional%20feature%20engineering.%20This%20regression%20performance%20can%20be%0Aexplained%20in%20part%20due%20to%20LLM%20embeddings%20over%20numeric%20data%20inherently%20preserving%0ALipschitz%20continuity%20over%20the%20feature%20space.%20Furthermore%2C%20we%20quantify%20the%0Acontribution%20of%20different%20model%20effects%2C%20most%20notably%20model%20size%20and%20language%0Aunderstanding%2C%20which%20we%20find%20surprisingly%20do%20not%20always%20improve%20regression%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14708v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520LLM%2520Embeddings%2520for%2520Regression%26entry.906535625%3DEric%2520Tang%2520and%2520Bangding%2520Yang%2520and%2520Xingyou%2520Song%26entry.1292438233%3D%2520%2520With%2520the%2520rise%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520flexibly%2520processing%250Ainformation%2520as%2520strings%252C%2520a%2520natural%2520application%2520is%2520regression%252C%2520specifically%2520by%250Apreprocessing%2520string%2520representations%2520into%2520LLM%2520embeddings%2520as%2520downstream%2520features%250Afor%2520metric%2520prediction.%2520In%2520this%2520paper%252C%2520we%2520provide%2520one%2520of%2520the%2520first%2520comprehensive%250Ainvestigations%2520into%2520embedding-based%2520regression%2520and%2520demonstrate%2520that%2520LLM%250Aembeddings%2520as%2520features%2520can%2520be%2520better%2520for%2520high-dimensional%2520regression%2520tasks%2520than%250Ausing%2520traditional%2520feature%2520engineering.%2520This%2520regression%2520performance%2520can%2520be%250Aexplained%2520in%2520part%2520due%2520to%2520LLM%2520embeddings%2520over%2520numeric%2520data%2520inherently%2520preserving%250ALipschitz%2520continuity%2520over%2520the%2520feature%2520space.%2520Furthermore%252C%2520we%2520quantify%2520the%250Acontribution%2520of%2520different%2520model%2520effects%252C%2520most%2520notably%2520model%2520size%2520and%2520language%250Aunderstanding%252C%2520which%2520we%2520find%2520surprisingly%2520do%2520not%2520always%2520improve%2520regression%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14708v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20LLM%20Embeddings%20for%20Regression&entry.906535625=Eric%20Tang%20and%20Bangding%20Yang%20and%20Xingyou%20Song&entry.1292438233=%20%20With%20the%20rise%20of%20large%20language%20models%20%28LLMs%29%20for%20flexibly%20processing%0Ainformation%20as%20strings%2C%20a%20natural%20application%20is%20regression%2C%20specifically%20by%0Apreprocessing%20string%20representations%20into%20LLM%20embeddings%20as%20downstream%20features%0Afor%20metric%20prediction.%20In%20this%20paper%2C%20we%20provide%20one%20of%20the%20first%20comprehensive%0Ainvestigations%20into%20embedding-based%20regression%20and%20demonstrate%20that%20LLM%0Aembeddings%20as%20features%20can%20be%20better%20for%20high-dimensional%20regression%20tasks%20than%0Ausing%20traditional%20feature%20engineering.%20This%20regression%20performance%20can%20be%0Aexplained%20in%20part%20due%20to%20LLM%20embeddings%20over%20numeric%20data%20inherently%20preserving%0ALipschitz%20continuity%20over%20the%20feature%20space.%20Furthermore%2C%20we%20quantify%20the%0Acontribution%20of%20different%20model%20effects%2C%20most%20notably%20model%20size%20and%20language%0Aunderstanding%2C%20which%20we%20find%20surprisingly%20do%20not%20always%20improve%20regression%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14708v2&entry.124074799=Read"},
{"title": "What Differentiates Educational Literature? A Multimodal Fusion Approach\n  of Transformers and Computational Linguistics", "author": "Jordan J. Bird", "abstract": "  The integration of new literature into the English curriculum remains a\nchallenge since educators often lack scalable tools to rapidly evaluate\nreadability and adapt texts for diverse classroom needs. This study proposes to\naddress this gap through a multimodal approach that combines transformer-based\ntext classification with linguistic feature analysis to align texts with UK Key\nStages. Eight state-of-the-art Transformers were fine-tuned on segmented text\ndata, with BERT achieving the highest unimodal F1 score of 0.75. In parallel,\n500 deep neural network topologies were searched for the classification of\nlinguistic characteristics, achieving an F1 score of 0.392. The fusion of these\nmodalities shows a significant improvement, with every multimodal approach\noutperforming all unimodal models. In particular, the ELECTRA Transformer fused\nwith the neural network achieved an F1 score of 0.996. Unimodal and multimodal\napproaches are shown to have statistically significant differences in all\nvalidation metrics (accuracy, precision, recall, F1 score) except for inference\ntime. The proposed approach is finally encapsulated in a stakeholder-facing web\napplication, providing non-technical stakeholder access to real-time insights\non text complexity, reading difficulty, curriculum alignment, and\nrecommendations for learning age range. The application empowers data-driven\ndecision making and reduces manual workload by integrating AI-based\nrecommendations into lesson planning for English literature.\n", "link": "http://arxiv.org/abs/2411.17593v3", "date": "2024-12-02", "relevancy": 1.9531, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5012}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4939}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Differentiates%20Educational%20Literature%3F%20A%20Multimodal%20Fusion%20Approach%0A%20%20of%20Transformers%20and%20Computational%20Linguistics&body=Title%3A%20What%20Differentiates%20Educational%20Literature%3F%20A%20Multimodal%20Fusion%20Approach%0A%20%20of%20Transformers%20and%20Computational%20Linguistics%0AAuthor%3A%20Jordan%20J.%20Bird%0AAbstract%3A%20%20%20The%20integration%20of%20new%20literature%20into%20the%20English%20curriculum%20remains%20a%0Achallenge%20since%20educators%20often%20lack%20scalable%20tools%20to%20rapidly%20evaluate%0Areadability%20and%20adapt%20texts%20for%20diverse%20classroom%20needs.%20This%20study%20proposes%20to%0Aaddress%20this%20gap%20through%20a%20multimodal%20approach%20that%20combines%20transformer-based%0Atext%20classification%20with%20linguistic%20feature%20analysis%20to%20align%20texts%20with%20UK%20Key%0AStages.%20Eight%20state-of-the-art%20Transformers%20were%20fine-tuned%20on%20segmented%20text%0Adata%2C%20with%20BERT%20achieving%20the%20highest%20unimodal%20F1%20score%20of%200.75.%20In%20parallel%2C%0A500%20deep%20neural%20network%20topologies%20were%20searched%20for%20the%20classification%20of%0Alinguistic%20characteristics%2C%20achieving%20an%20F1%20score%20of%200.392.%20The%20fusion%20of%20these%0Amodalities%20shows%20a%20significant%20improvement%2C%20with%20every%20multimodal%20approach%0Aoutperforming%20all%20unimodal%20models.%20In%20particular%2C%20the%20ELECTRA%20Transformer%20fused%0Awith%20the%20neural%20network%20achieved%20an%20F1%20score%20of%200.996.%20Unimodal%20and%20multimodal%0Aapproaches%20are%20shown%20to%20have%20statistically%20significant%20differences%20in%20all%0Avalidation%20metrics%20%28accuracy%2C%20precision%2C%20recall%2C%20F1%20score%29%20except%20for%20inference%0Atime.%20The%20proposed%20approach%20is%20finally%20encapsulated%20in%20a%20stakeholder-facing%20web%0Aapplication%2C%20providing%20non-technical%20stakeholder%20access%20to%20real-time%20insights%0Aon%20text%20complexity%2C%20reading%20difficulty%2C%20curriculum%20alignment%2C%20and%0Arecommendations%20for%20learning%20age%20range.%20The%20application%20empowers%20data-driven%0Adecision%20making%20and%20reduces%20manual%20workload%20by%20integrating%20AI-based%0Arecommendations%20into%20lesson%20planning%20for%20English%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17593v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Differentiates%2520Educational%2520Literature%253F%2520A%2520Multimodal%2520Fusion%2520Approach%250A%2520%2520of%2520Transformers%2520and%2520Computational%2520Linguistics%26entry.906535625%3DJordan%2520J.%2520Bird%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520new%2520literature%2520into%2520the%2520English%2520curriculum%2520remains%2520a%250Achallenge%2520since%2520educators%2520often%2520lack%2520scalable%2520tools%2520to%2520rapidly%2520evaluate%250Areadability%2520and%2520adapt%2520texts%2520for%2520diverse%2520classroom%2520needs.%2520This%2520study%2520proposes%2520to%250Aaddress%2520this%2520gap%2520through%2520a%2520multimodal%2520approach%2520that%2520combines%2520transformer-based%250Atext%2520classification%2520with%2520linguistic%2520feature%2520analysis%2520to%2520align%2520texts%2520with%2520UK%2520Key%250AStages.%2520Eight%2520state-of-the-art%2520Transformers%2520were%2520fine-tuned%2520on%2520segmented%2520text%250Adata%252C%2520with%2520BERT%2520achieving%2520the%2520highest%2520unimodal%2520F1%2520score%2520of%25200.75.%2520In%2520parallel%252C%250A500%2520deep%2520neural%2520network%2520topologies%2520were%2520searched%2520for%2520the%2520classification%2520of%250Alinguistic%2520characteristics%252C%2520achieving%2520an%2520F1%2520score%2520of%25200.392.%2520The%2520fusion%2520of%2520these%250Amodalities%2520shows%2520a%2520significant%2520improvement%252C%2520with%2520every%2520multimodal%2520approach%250Aoutperforming%2520all%2520unimodal%2520models.%2520In%2520particular%252C%2520the%2520ELECTRA%2520Transformer%2520fused%250Awith%2520the%2520neural%2520network%2520achieved%2520an%2520F1%2520score%2520of%25200.996.%2520Unimodal%2520and%2520multimodal%250Aapproaches%2520are%2520shown%2520to%2520have%2520statistically%2520significant%2520differences%2520in%2520all%250Avalidation%2520metrics%2520%2528accuracy%252C%2520precision%252C%2520recall%252C%2520F1%2520score%2529%2520except%2520for%2520inference%250Atime.%2520The%2520proposed%2520approach%2520is%2520finally%2520encapsulated%2520in%2520a%2520stakeholder-facing%2520web%250Aapplication%252C%2520providing%2520non-technical%2520stakeholder%2520access%2520to%2520real-time%2520insights%250Aon%2520text%2520complexity%252C%2520reading%2520difficulty%252C%2520curriculum%2520alignment%252C%2520and%250Arecommendations%2520for%2520learning%2520age%2520range.%2520The%2520application%2520empowers%2520data-driven%250Adecision%2520making%2520and%2520reduces%2520manual%2520workload%2520by%2520integrating%2520AI-based%250Arecommendations%2520into%2520lesson%2520planning%2520for%2520English%2520literature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17593v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Differentiates%20Educational%20Literature%3F%20A%20Multimodal%20Fusion%20Approach%0A%20%20of%20Transformers%20and%20Computational%20Linguistics&entry.906535625=Jordan%20J.%20Bird&entry.1292438233=%20%20The%20integration%20of%20new%20literature%20into%20the%20English%20curriculum%20remains%20a%0Achallenge%20since%20educators%20often%20lack%20scalable%20tools%20to%20rapidly%20evaluate%0Areadability%20and%20adapt%20texts%20for%20diverse%20classroom%20needs.%20This%20study%20proposes%20to%0Aaddress%20this%20gap%20through%20a%20multimodal%20approach%20that%20combines%20transformer-based%0Atext%20classification%20with%20linguistic%20feature%20analysis%20to%20align%20texts%20with%20UK%20Key%0AStages.%20Eight%20state-of-the-art%20Transformers%20were%20fine-tuned%20on%20segmented%20text%0Adata%2C%20with%20BERT%20achieving%20the%20highest%20unimodal%20F1%20score%20of%200.75.%20In%20parallel%2C%0A500%20deep%20neural%20network%20topologies%20were%20searched%20for%20the%20classification%20of%0Alinguistic%20characteristics%2C%20achieving%20an%20F1%20score%20of%200.392.%20The%20fusion%20of%20these%0Amodalities%20shows%20a%20significant%20improvement%2C%20with%20every%20multimodal%20approach%0Aoutperforming%20all%20unimodal%20models.%20In%20particular%2C%20the%20ELECTRA%20Transformer%20fused%0Awith%20the%20neural%20network%20achieved%20an%20F1%20score%20of%200.996.%20Unimodal%20and%20multimodal%0Aapproaches%20are%20shown%20to%20have%20statistically%20significant%20differences%20in%20all%0Avalidation%20metrics%20%28accuracy%2C%20precision%2C%20recall%2C%20F1%20score%29%20except%20for%20inference%0Atime.%20The%20proposed%20approach%20is%20finally%20encapsulated%20in%20a%20stakeholder-facing%20web%0Aapplication%2C%20providing%20non-technical%20stakeholder%20access%20to%20real-time%20insights%0Aon%20text%20complexity%2C%20reading%20difficulty%2C%20curriculum%20alignment%2C%20and%0Arecommendations%20for%20learning%20age%20range.%20The%20application%20empowers%20data-driven%0Adecision%20making%20and%20reduces%20manual%20workload%20by%20integrating%20AI-based%0Arecommendations%20into%20lesson%20planning%20for%20English%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17593v3&entry.124074799=Read"},
{"title": "Cross-Refine: Improving Natural Language Explanation Generation by\n  Learning in Tandem", "author": "Qianli Wang and Tatiana Anikina and Nils Feldhus and Simon Ostermann and Sebastian M\u00f6ller and Vera Schmitt", "abstract": "  Natural language explanations (NLEs) are vital for elucidating the reasoning\nbehind large language model (LLM) decisions. Many techniques have been\ndeveloped to generate NLEs using LLMs. However, like humans, LLMs might not\nalways produce optimal NLEs on first attempt. Inspired by human learning\nprocesses, we introduce Cross-Refine, which employs role modeling by deploying\ntwo LLMs as generator and critic, respectively. The generator outputs a first\nNLE and then refines this initial explanation using feedback and suggestions\nprovided by the critic. Cross-Refine does not require any supervised training\ndata or additional training. We validate Cross-Refine across three NLP tasks\nusing three state-of-the-art open-source LLMs through automatic and human\nevaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which\nonly utilizes self-feedback to refine the explanations. Our findings from\nautomatic evaluation and a user study indicate that Cross-Refine outperforms\nSelf-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful\nLLMs, whereas Self-Refine only yields strong results with ChatGPT.\nAdditionally, we conduct an ablation study to assess the importance of feedback\nand suggestions. Both of them play an important role in refining explanations.\nWe further evaluate Cross-Refine on a bilingual dataset in English and German.\n", "link": "http://arxiv.org/abs/2409.07123v2", "date": "2024-12-02", "relevancy": 1.9462, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4896}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4896}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Refine%3A%20Improving%20Natural%20Language%20Explanation%20Generation%20by%0A%20%20Learning%20in%20Tandem&body=Title%3A%20Cross-Refine%3A%20Improving%20Natural%20Language%20Explanation%20Generation%20by%0A%20%20Learning%20in%20Tandem%0AAuthor%3A%20Qianli%20Wang%20and%20Tatiana%20Anikina%20and%20Nils%20Feldhus%20and%20Simon%20Ostermann%20and%20Sebastian%20M%C3%B6ller%20and%20Vera%20Schmitt%0AAbstract%3A%20%20%20Natural%20language%20explanations%20%28NLEs%29%20are%20vital%20for%20elucidating%20the%20reasoning%0Abehind%20large%20language%20model%20%28LLM%29%20decisions.%20Many%20techniques%20have%20been%0Adeveloped%20to%20generate%20NLEs%20using%20LLMs.%20However%2C%20like%20humans%2C%20LLMs%20might%20not%0Aalways%20produce%20optimal%20NLEs%20on%20first%20attempt.%20Inspired%20by%20human%20learning%0Aprocesses%2C%20we%20introduce%20Cross-Refine%2C%20which%20employs%20role%20modeling%20by%20deploying%0Atwo%20LLMs%20as%20generator%20and%20critic%2C%20respectively.%20The%20generator%20outputs%20a%20first%0ANLE%20and%20then%20refines%20this%20initial%20explanation%20using%20feedback%20and%20suggestions%0Aprovided%20by%20the%20critic.%20Cross-Refine%20does%20not%20require%20any%20supervised%20training%0Adata%20or%20additional%20training.%20We%20validate%20Cross-Refine%20across%20three%20NLP%20tasks%0Ausing%20three%20state-of-the-art%20open-source%20LLMs%20through%20automatic%20and%20human%0Aevaluation.%20We%20select%20Self-Refine%20%28Madaan%20et%20al.%2C%202023%29%20as%20the%20baseline%2C%20which%0Aonly%20utilizes%20self-feedback%20to%20refine%20the%20explanations.%20Our%20findings%20from%0Aautomatic%20evaluation%20and%20a%20user%20study%20indicate%20that%20Cross-Refine%20outperforms%0ASelf-Refine.%20Meanwhile%2C%20Cross-Refine%20can%20perform%20effectively%20with%20less%20powerful%0ALLMs%2C%20whereas%20Self-Refine%20only%20yields%20strong%20results%20with%20ChatGPT.%0AAdditionally%2C%20we%20conduct%20an%20ablation%20study%20to%20assess%20the%20importance%20of%20feedback%0Aand%20suggestions.%20Both%20of%20them%20play%20an%20important%20role%20in%20refining%20explanations.%0AWe%20further%20evaluate%20Cross-Refine%20on%20a%20bilingual%20dataset%20in%20English%20and%20German.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07123v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Refine%253A%2520Improving%2520Natural%2520Language%2520Explanation%2520Generation%2520by%250A%2520%2520Learning%2520in%2520Tandem%26entry.906535625%3DQianli%2520Wang%2520and%2520Tatiana%2520Anikina%2520and%2520Nils%2520Feldhus%2520and%2520Simon%2520Ostermann%2520and%2520Sebastian%2520M%25C3%25B6ller%2520and%2520Vera%2520Schmitt%26entry.1292438233%3D%2520%2520Natural%2520language%2520explanations%2520%2528NLEs%2529%2520are%2520vital%2520for%2520elucidating%2520the%2520reasoning%250Abehind%2520large%2520language%2520model%2520%2528LLM%2529%2520decisions.%2520Many%2520techniques%2520have%2520been%250Adeveloped%2520to%2520generate%2520NLEs%2520using%2520LLMs.%2520However%252C%2520like%2520humans%252C%2520LLMs%2520might%2520not%250Aalways%2520produce%2520optimal%2520NLEs%2520on%2520first%2520attempt.%2520Inspired%2520by%2520human%2520learning%250Aprocesses%252C%2520we%2520introduce%2520Cross-Refine%252C%2520which%2520employs%2520role%2520modeling%2520by%2520deploying%250Atwo%2520LLMs%2520as%2520generator%2520and%2520critic%252C%2520respectively.%2520The%2520generator%2520outputs%2520a%2520first%250ANLE%2520and%2520then%2520refines%2520this%2520initial%2520explanation%2520using%2520feedback%2520and%2520suggestions%250Aprovided%2520by%2520the%2520critic.%2520Cross-Refine%2520does%2520not%2520require%2520any%2520supervised%2520training%250Adata%2520or%2520additional%2520training.%2520We%2520validate%2520Cross-Refine%2520across%2520three%2520NLP%2520tasks%250Ausing%2520three%2520state-of-the-art%2520open-source%2520LLMs%2520through%2520automatic%2520and%2520human%250Aevaluation.%2520We%2520select%2520Self-Refine%2520%2528Madaan%2520et%2520al.%252C%25202023%2529%2520as%2520the%2520baseline%252C%2520which%250Aonly%2520utilizes%2520self-feedback%2520to%2520refine%2520the%2520explanations.%2520Our%2520findings%2520from%250Aautomatic%2520evaluation%2520and%2520a%2520user%2520study%2520indicate%2520that%2520Cross-Refine%2520outperforms%250ASelf-Refine.%2520Meanwhile%252C%2520Cross-Refine%2520can%2520perform%2520effectively%2520with%2520less%2520powerful%250ALLMs%252C%2520whereas%2520Self-Refine%2520only%2520yields%2520strong%2520results%2520with%2520ChatGPT.%250AAdditionally%252C%2520we%2520conduct%2520an%2520ablation%2520study%2520to%2520assess%2520the%2520importance%2520of%2520feedback%250Aand%2520suggestions.%2520Both%2520of%2520them%2520play%2520an%2520important%2520role%2520in%2520refining%2520explanations.%250AWe%2520further%2520evaluate%2520Cross-Refine%2520on%2520a%2520bilingual%2520dataset%2520in%2520English%2520and%2520German.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07123v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Refine%3A%20Improving%20Natural%20Language%20Explanation%20Generation%20by%0A%20%20Learning%20in%20Tandem&entry.906535625=Qianli%20Wang%20and%20Tatiana%20Anikina%20and%20Nils%20Feldhus%20and%20Simon%20Ostermann%20and%20Sebastian%20M%C3%B6ller%20and%20Vera%20Schmitt&entry.1292438233=%20%20Natural%20language%20explanations%20%28NLEs%29%20are%20vital%20for%20elucidating%20the%20reasoning%0Abehind%20large%20language%20model%20%28LLM%29%20decisions.%20Many%20techniques%20have%20been%0Adeveloped%20to%20generate%20NLEs%20using%20LLMs.%20However%2C%20like%20humans%2C%20LLMs%20might%20not%0Aalways%20produce%20optimal%20NLEs%20on%20first%20attempt.%20Inspired%20by%20human%20learning%0Aprocesses%2C%20we%20introduce%20Cross-Refine%2C%20which%20employs%20role%20modeling%20by%20deploying%0Atwo%20LLMs%20as%20generator%20and%20critic%2C%20respectively.%20The%20generator%20outputs%20a%20first%0ANLE%20and%20then%20refines%20this%20initial%20explanation%20using%20feedback%20and%20suggestions%0Aprovided%20by%20the%20critic.%20Cross-Refine%20does%20not%20require%20any%20supervised%20training%0Adata%20or%20additional%20training.%20We%20validate%20Cross-Refine%20across%20three%20NLP%20tasks%0Ausing%20three%20state-of-the-art%20open-source%20LLMs%20through%20automatic%20and%20human%0Aevaluation.%20We%20select%20Self-Refine%20%28Madaan%20et%20al.%2C%202023%29%20as%20the%20baseline%2C%20which%0Aonly%20utilizes%20self-feedback%20to%20refine%20the%20explanations.%20Our%20findings%20from%0Aautomatic%20evaluation%20and%20a%20user%20study%20indicate%20that%20Cross-Refine%20outperforms%0ASelf-Refine.%20Meanwhile%2C%20Cross-Refine%20can%20perform%20effectively%20with%20less%20powerful%0ALLMs%2C%20whereas%20Self-Refine%20only%20yields%20strong%20results%20with%20ChatGPT.%0AAdditionally%2C%20we%20conduct%20an%20ablation%20study%20to%20assess%20the%20importance%20of%20feedback%0Aand%20suggestions.%20Both%20of%20them%20play%20an%20important%20role%20in%20refining%20explanations.%0AWe%20further%20evaluate%20Cross-Refine%20on%20a%20bilingual%20dataset%20in%20English%20and%20German.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07123v2&entry.124074799=Read"},
{"title": "Uncertainty quantification for fast reconstruction methods using\n  augmented equivariant bootstrap: Application to radio interferometry", "author": "Mostafa Cherif and Tob\u00edas I. Liaudat and Jonathan Kern and Christophe Kervazo and J\u00e9r\u00f4me Bobin", "abstract": "  The advent of next-generation radio interferometers like the Square Kilometer\nArray promises to revolutionise our radio astronomy observational capabilities.\nThe unprecedented volume of data these devices generate requires fast and\naccurate image reconstruction algorithms to solve the ill-posed radio\ninterferometric imaging problem. Most state-of-the-art reconstruction methods\nlack trustworthy and scalable uncertainty quantification, which is critical for\nthe rigorous scientific interpretation of radio observations. We propose an\nunsupervised technique based on a conformalized version of a radio-augmented\nequivariant bootstrapping method, which allows us to quantify uncertainties for\nfast reconstruction methods. Noticeably, we rely on reconstructions from\nultra-fast unrolled algorithms. The proposed method brings more reliable\nuncertainty estimations to our problem than existing alternatives.\n", "link": "http://arxiv.org/abs/2410.23178v2", "date": "2024-12-02", "relevancy": 1.9302, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4952}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4798}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20quantification%20for%20fast%20reconstruction%20methods%20using%0A%20%20augmented%20equivariant%20bootstrap%3A%20Application%20to%20radio%20interferometry&body=Title%3A%20Uncertainty%20quantification%20for%20fast%20reconstruction%20methods%20using%0A%20%20augmented%20equivariant%20bootstrap%3A%20Application%20to%20radio%20interferometry%0AAuthor%3A%20Mostafa%20Cherif%20and%20Tob%C3%ADas%20I.%20Liaudat%20and%20Jonathan%20Kern%20and%20Christophe%20Kervazo%20and%20J%C3%A9r%C3%B4me%20Bobin%0AAbstract%3A%20%20%20The%20advent%20of%20next-generation%20radio%20interferometers%20like%20the%20Square%20Kilometer%0AArray%20promises%20to%20revolutionise%20our%20radio%20astronomy%20observational%20capabilities.%0AThe%20unprecedented%20volume%20of%20data%20these%20devices%20generate%20requires%20fast%20and%0Aaccurate%20image%20reconstruction%20algorithms%20to%20solve%20the%20ill-posed%20radio%0Ainterferometric%20imaging%20problem.%20Most%20state-of-the-art%20reconstruction%20methods%0Alack%20trustworthy%20and%20scalable%20uncertainty%20quantification%2C%20which%20is%20critical%20for%0Athe%20rigorous%20scientific%20interpretation%20of%20radio%20observations.%20We%20propose%20an%0Aunsupervised%20technique%20based%20on%20a%20conformalized%20version%20of%20a%20radio-augmented%0Aequivariant%20bootstrapping%20method%2C%20which%20allows%20us%20to%20quantify%20uncertainties%20for%0Afast%20reconstruction%20methods.%20Noticeably%2C%20we%20rely%20on%20reconstructions%20from%0Aultra-fast%20unrolled%20algorithms.%20The%20proposed%20method%20brings%20more%20reliable%0Auncertainty%20estimations%20to%20our%20problem%20than%20existing%20alternatives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23178v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520quantification%2520for%2520fast%2520reconstruction%2520methods%2520using%250A%2520%2520augmented%2520equivariant%2520bootstrap%253A%2520Application%2520to%2520radio%2520interferometry%26entry.906535625%3DMostafa%2520Cherif%2520and%2520Tob%25C3%25ADas%2520I.%2520Liaudat%2520and%2520Jonathan%2520Kern%2520and%2520Christophe%2520Kervazo%2520and%2520J%25C3%25A9r%25C3%25B4me%2520Bobin%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520next-generation%2520radio%2520interferometers%2520like%2520the%2520Square%2520Kilometer%250AArray%2520promises%2520to%2520revolutionise%2520our%2520radio%2520astronomy%2520observational%2520capabilities.%250AThe%2520unprecedented%2520volume%2520of%2520data%2520these%2520devices%2520generate%2520requires%2520fast%2520and%250Aaccurate%2520image%2520reconstruction%2520algorithms%2520to%2520solve%2520the%2520ill-posed%2520radio%250Ainterferometric%2520imaging%2520problem.%2520Most%2520state-of-the-art%2520reconstruction%2520methods%250Alack%2520trustworthy%2520and%2520scalable%2520uncertainty%2520quantification%252C%2520which%2520is%2520critical%2520for%250Athe%2520rigorous%2520scientific%2520interpretation%2520of%2520radio%2520observations.%2520We%2520propose%2520an%250Aunsupervised%2520technique%2520based%2520on%2520a%2520conformalized%2520version%2520of%2520a%2520radio-augmented%250Aequivariant%2520bootstrapping%2520method%252C%2520which%2520allows%2520us%2520to%2520quantify%2520uncertainties%2520for%250Afast%2520reconstruction%2520methods.%2520Noticeably%252C%2520we%2520rely%2520on%2520reconstructions%2520from%250Aultra-fast%2520unrolled%2520algorithms.%2520The%2520proposed%2520method%2520brings%2520more%2520reliable%250Auncertainty%2520estimations%2520to%2520our%2520problem%2520than%2520existing%2520alternatives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23178v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20quantification%20for%20fast%20reconstruction%20methods%20using%0A%20%20augmented%20equivariant%20bootstrap%3A%20Application%20to%20radio%20interferometry&entry.906535625=Mostafa%20Cherif%20and%20Tob%C3%ADas%20I.%20Liaudat%20and%20Jonathan%20Kern%20and%20Christophe%20Kervazo%20and%20J%C3%A9r%C3%B4me%20Bobin&entry.1292438233=%20%20The%20advent%20of%20next-generation%20radio%20interferometers%20like%20the%20Square%20Kilometer%0AArray%20promises%20to%20revolutionise%20our%20radio%20astronomy%20observational%20capabilities.%0AThe%20unprecedented%20volume%20of%20data%20these%20devices%20generate%20requires%20fast%20and%0Aaccurate%20image%20reconstruction%20algorithms%20to%20solve%20the%20ill-posed%20radio%0Ainterferometric%20imaging%20problem.%20Most%20state-of-the-art%20reconstruction%20methods%0Alack%20trustworthy%20and%20scalable%20uncertainty%20quantification%2C%20which%20is%20critical%20for%0Athe%20rigorous%20scientific%20interpretation%20of%20radio%20observations.%20We%20propose%20an%0Aunsupervised%20technique%20based%20on%20a%20conformalized%20version%20of%20a%20radio-augmented%0Aequivariant%20bootstrapping%20method%2C%20which%20allows%20us%20to%20quantify%20uncertainties%20for%0Afast%20reconstruction%20methods.%20Noticeably%2C%20we%20rely%20on%20reconstructions%20from%0Aultra-fast%20unrolled%20algorithms.%20The%20proposed%20method%20brings%20more%20reliable%0Auncertainty%20estimations%20to%20our%20problem%20than%20existing%20alternatives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23178v2&entry.124074799=Read"},
{"title": "ARTIST: Improving the Generation of Text-rich Images with Disentangled\n  Diffusion Models and Large Language Models", "author": "Jianyi Zhang and Yufan Zhou and Jiuxiang Gu and Curtis Wigington and Tong Yu and Yiran Chen and Tong Sun and Ruiyi Zhang", "abstract": "  Diffusion models have demonstrated exceptional capabilities in generating a\nbroad spectrum of visual content, yet their proficiency in rendering text is\nstill limited: they often generate inaccurate characters or words that fail to\nblend well with the underlying image. To address these shortcomings, we\nintroduce a novel framework named, ARTIST, which incorporates a dedicated\ntextual diffusion model to focus on the learning of text structures\nspecifically. Initially, we pretrain this textual model to capture the\nintricacies of text representation. Subsequently, we finetune a visual\ndiffusion model, enabling it to assimilate textual structure information from\nthe pretrained textual model. This disentangled architecture design and\ntraining strategy significantly enhance the text rendering ability of the\ndiffusion models for text-rich image generation. Additionally, we leverage the\ncapabilities of pretrained large language models to interpret user intentions\nbetter, contributing to improved generation quality. Empirical results on the\nMARIO-Eval benchmark underscore the effectiveness of the proposed method,\nshowing an improvement of up to 15% in various metrics.\n", "link": "http://arxiv.org/abs/2406.12044v3", "date": "2024-12-02", "relevancy": 1.9138, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.676}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6482}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARTIST%3A%20Improving%20the%20Generation%20of%20Text-rich%20Images%20with%20Disentangled%0A%20%20Diffusion%20Models%20and%20Large%20Language%20Models&body=Title%3A%20ARTIST%3A%20Improving%20the%20Generation%20of%20Text-rich%20Images%20with%20Disentangled%0A%20%20Diffusion%20Models%20and%20Large%20Language%20Models%0AAuthor%3A%20Jianyi%20Zhang%20and%20Yufan%20Zhou%20and%20Jiuxiang%20Gu%20and%20Curtis%20Wigington%20and%20Tong%20Yu%20and%20Yiran%20Chen%20and%20Tong%20Sun%20and%20Ruiyi%20Zhang%0AAbstract%3A%20%20%20Diffusion%20models%20have%20demonstrated%20exceptional%20capabilities%20in%20generating%20a%0Abroad%20spectrum%20of%20visual%20content%2C%20yet%20their%20proficiency%20in%20rendering%20text%20is%0Astill%20limited%3A%20they%20often%20generate%20inaccurate%20characters%20or%20words%20that%20fail%20to%0Ablend%20well%20with%20the%20underlying%20image.%20To%20address%20these%20shortcomings%2C%20we%0Aintroduce%20a%20novel%20framework%20named%2C%20ARTIST%2C%20which%20incorporates%20a%20dedicated%0Atextual%20diffusion%20model%20to%20focus%20on%20the%20learning%20of%20text%20structures%0Aspecifically.%20Initially%2C%20we%20pretrain%20this%20textual%20model%20to%20capture%20the%0Aintricacies%20of%20text%20representation.%20Subsequently%2C%20we%20finetune%20a%20visual%0Adiffusion%20model%2C%20enabling%20it%20to%20assimilate%20textual%20structure%20information%20from%0Athe%20pretrained%20textual%20model.%20This%20disentangled%20architecture%20design%20and%0Atraining%20strategy%20significantly%20enhance%20the%20text%20rendering%20ability%20of%20the%0Adiffusion%20models%20for%20text-rich%20image%20generation.%20Additionally%2C%20we%20leverage%20the%0Acapabilities%20of%20pretrained%20large%20language%20models%20to%20interpret%20user%20intentions%0Abetter%2C%20contributing%20to%20improved%20generation%20quality.%20Empirical%20results%20on%20the%0AMARIO-Eval%20benchmark%20underscore%20the%20effectiveness%20of%20the%20proposed%20method%2C%0Ashowing%20an%20improvement%20of%20up%20to%2015%25%20in%20various%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12044v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARTIST%253A%2520Improving%2520the%2520Generation%2520of%2520Text-rich%2520Images%2520with%2520Disentangled%250A%2520%2520Diffusion%2520Models%2520and%2520Large%2520Language%2520Models%26entry.906535625%3DJianyi%2520Zhang%2520and%2520Yufan%2520Zhou%2520and%2520Jiuxiang%2520Gu%2520and%2520Curtis%2520Wigington%2520and%2520Tong%2520Yu%2520and%2520Yiran%2520Chen%2520and%2520Tong%2520Sun%2520and%2520Ruiyi%2520Zhang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520demonstrated%2520exceptional%2520capabilities%2520in%2520generating%2520a%250Abroad%2520spectrum%2520of%2520visual%2520content%252C%2520yet%2520their%2520proficiency%2520in%2520rendering%2520text%2520is%250Astill%2520limited%253A%2520they%2520often%2520generate%2520inaccurate%2520characters%2520or%2520words%2520that%2520fail%2520to%250Ablend%2520well%2520with%2520the%2520underlying%2520image.%2520To%2520address%2520these%2520shortcomings%252C%2520we%250Aintroduce%2520a%2520novel%2520framework%2520named%252C%2520ARTIST%252C%2520which%2520incorporates%2520a%2520dedicated%250Atextual%2520diffusion%2520model%2520to%2520focus%2520on%2520the%2520learning%2520of%2520text%2520structures%250Aspecifically.%2520Initially%252C%2520we%2520pretrain%2520this%2520textual%2520model%2520to%2520capture%2520the%250Aintricacies%2520of%2520text%2520representation.%2520Subsequently%252C%2520we%2520finetune%2520a%2520visual%250Adiffusion%2520model%252C%2520enabling%2520it%2520to%2520assimilate%2520textual%2520structure%2520information%2520from%250Athe%2520pretrained%2520textual%2520model.%2520This%2520disentangled%2520architecture%2520design%2520and%250Atraining%2520strategy%2520significantly%2520enhance%2520the%2520text%2520rendering%2520ability%2520of%2520the%250Adiffusion%2520models%2520for%2520text-rich%2520image%2520generation.%2520Additionally%252C%2520we%2520leverage%2520the%250Acapabilities%2520of%2520pretrained%2520large%2520language%2520models%2520to%2520interpret%2520user%2520intentions%250Abetter%252C%2520contributing%2520to%2520improved%2520generation%2520quality.%2520Empirical%2520results%2520on%2520the%250AMARIO-Eval%2520benchmark%2520underscore%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%252C%250Ashowing%2520an%2520improvement%2520of%2520up%2520to%252015%2525%2520in%2520various%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12044v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARTIST%3A%20Improving%20the%20Generation%20of%20Text-rich%20Images%20with%20Disentangled%0A%20%20Diffusion%20Models%20and%20Large%20Language%20Models&entry.906535625=Jianyi%20Zhang%20and%20Yufan%20Zhou%20and%20Jiuxiang%20Gu%20and%20Curtis%20Wigington%20and%20Tong%20Yu%20and%20Yiran%20Chen%20and%20Tong%20Sun%20and%20Ruiyi%20Zhang&entry.1292438233=%20%20Diffusion%20models%20have%20demonstrated%20exceptional%20capabilities%20in%20generating%20a%0Abroad%20spectrum%20of%20visual%20content%2C%20yet%20their%20proficiency%20in%20rendering%20text%20is%0Astill%20limited%3A%20they%20often%20generate%20inaccurate%20characters%20or%20words%20that%20fail%20to%0Ablend%20well%20with%20the%20underlying%20image.%20To%20address%20these%20shortcomings%2C%20we%0Aintroduce%20a%20novel%20framework%20named%2C%20ARTIST%2C%20which%20incorporates%20a%20dedicated%0Atextual%20diffusion%20model%20to%20focus%20on%20the%20learning%20of%20text%20structures%0Aspecifically.%20Initially%2C%20we%20pretrain%20this%20textual%20model%20to%20capture%20the%0Aintricacies%20of%20text%20representation.%20Subsequently%2C%20we%20finetune%20a%20visual%0Adiffusion%20model%2C%20enabling%20it%20to%20assimilate%20textual%20structure%20information%20from%0Athe%20pretrained%20textual%20model.%20This%20disentangled%20architecture%20design%20and%0Atraining%20strategy%20significantly%20enhance%20the%20text%20rendering%20ability%20of%20the%0Adiffusion%20models%20for%20text-rich%20image%20generation.%20Additionally%2C%20we%20leverage%20the%0Acapabilities%20of%20pretrained%20large%20language%20models%20to%20interpret%20user%20intentions%0Abetter%2C%20contributing%20to%20improved%20generation%20quality.%20Empirical%20results%20on%20the%0AMARIO-Eval%20benchmark%20underscore%20the%20effectiveness%20of%20the%20proposed%20method%2C%0Ashowing%20an%20improvement%20of%20up%20to%2015%25%20in%20various%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12044v3&entry.124074799=Read"},
{"title": "Multi-turn Reinforcement Learning from Preference Human Feedback", "author": "Lior Shani and Aviv Rosenberg and Asaf Cassel and Oran Lang and Daniele Calandriello and Avital Zipori and Hila Noga and Orgad Keller and Bilal Piot and Idan Szpektor and Avinatan Hassidim and Yossi Matias and R\u00e9mi Munos", "abstract": "  Reinforcement Learning from Human Feedback (RLHF) has become the standard\napproach for aligning Large Language Models (LLMs) with human preferences,\nallowing LLMs to demonstrate remarkable abilities in various tasks. Existing\nmethods work by emulating the preferences at the single decision (turn) level,\nlimiting their capabilities in settings that require planning or multi-turn\ninteractions to achieve a long-term goal. In this paper, we address this issue\nby developing novel methods for Reinforcement Learning (RL) from preference\nfeedback between two full multi-turn conversations. In the tabular setting, we\npresent a novel mirror-descent-based policy optimization algorithm for the\ngeneral multi-turn preference-based RL problem, and prove its convergence to\nNash equilibrium. To evaluate performance, we create a new environment,\nEducation Dialogue, where a teacher agent guides a student in learning a random\ntopic, and show that a deep RL variant of our algorithm outperforms RLHF\nbaselines. Finally, we show that in an environment with explicit rewards, our\nalgorithm recovers the same performance as a reward-based RL baseline, despite\nrelying solely on a weaker preference signal.\n", "link": "http://arxiv.org/abs/2405.14655v2", "date": "2024-12-02", "relevancy": 1.9032, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.479}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4771}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-turn%20Reinforcement%20Learning%20from%20Preference%20Human%20Feedback&body=Title%3A%20Multi-turn%20Reinforcement%20Learning%20from%20Preference%20Human%20Feedback%0AAuthor%3A%20Lior%20Shani%20and%20Aviv%20Rosenberg%20and%20Asaf%20Cassel%20and%20Oran%20Lang%20and%20Daniele%20Calandriello%20and%20Avital%20Zipori%20and%20Hila%20Noga%20and%20Orgad%20Keller%20and%20Bilal%20Piot%20and%20Idan%20Szpektor%20and%20Avinatan%20Hassidim%20and%20Yossi%20Matias%20and%20R%C3%A9mi%20Munos%0AAbstract%3A%20%20%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20has%20become%20the%20standard%0Aapproach%20for%20aligning%20Large%20Language%20Models%20%28LLMs%29%20with%20human%20preferences%2C%0Aallowing%20LLMs%20to%20demonstrate%20remarkable%20abilities%20in%20various%20tasks.%20Existing%0Amethods%20work%20by%20emulating%20the%20preferences%20at%20the%20single%20decision%20%28turn%29%20level%2C%0Alimiting%20their%20capabilities%20in%20settings%20that%20require%20planning%20or%20multi-turn%0Ainteractions%20to%20achieve%20a%20long-term%20goal.%20In%20this%20paper%2C%20we%20address%20this%20issue%0Aby%20developing%20novel%20methods%20for%20Reinforcement%20Learning%20%28RL%29%20from%20preference%0Afeedback%20between%20two%20full%20multi-turn%20conversations.%20In%20the%20tabular%20setting%2C%20we%0Apresent%20a%20novel%20mirror-descent-based%20policy%20optimization%20algorithm%20for%20the%0Ageneral%20multi-turn%20preference-based%20RL%20problem%2C%20and%20prove%20its%20convergence%20to%0ANash%20equilibrium.%20To%20evaluate%20performance%2C%20we%20create%20a%20new%20environment%2C%0AEducation%20Dialogue%2C%20where%20a%20teacher%20agent%20guides%20a%20student%20in%20learning%20a%20random%0Atopic%2C%20and%20show%20that%20a%20deep%20RL%20variant%20of%20our%20algorithm%20outperforms%20RLHF%0Abaselines.%20Finally%2C%20we%20show%20that%20in%20an%20environment%20with%20explicit%20rewards%2C%20our%0Aalgorithm%20recovers%20the%20same%20performance%20as%20a%20reward-based%20RL%20baseline%2C%20despite%0Arelying%20solely%20on%20a%20weaker%20preference%20signal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14655v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-turn%2520Reinforcement%2520Learning%2520from%2520Preference%2520Human%2520Feedback%26entry.906535625%3DLior%2520Shani%2520and%2520Aviv%2520Rosenberg%2520and%2520Asaf%2520Cassel%2520and%2520Oran%2520Lang%2520and%2520Daniele%2520Calandriello%2520and%2520Avital%2520Zipori%2520and%2520Hila%2520Noga%2520and%2520Orgad%2520Keller%2520and%2520Bilal%2520Piot%2520and%2520Idan%2520Szpektor%2520and%2520Avinatan%2520Hassidim%2520and%2520Yossi%2520Matias%2520and%2520R%25C3%25A9mi%2520Munos%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520has%2520become%2520the%2520standard%250Aapproach%2520for%2520aligning%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520human%2520preferences%252C%250Aallowing%2520LLMs%2520to%2520demonstrate%2520remarkable%2520abilities%2520in%2520various%2520tasks.%2520Existing%250Amethods%2520work%2520by%2520emulating%2520the%2520preferences%2520at%2520the%2520single%2520decision%2520%2528turn%2529%2520level%252C%250Alimiting%2520their%2520capabilities%2520in%2520settings%2520that%2520require%2520planning%2520or%2520multi-turn%250Ainteractions%2520to%2520achieve%2520a%2520long-term%2520goal.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520issue%250Aby%2520developing%2520novel%2520methods%2520for%2520Reinforcement%2520Learning%2520%2528RL%2529%2520from%2520preference%250Afeedback%2520between%2520two%2520full%2520multi-turn%2520conversations.%2520In%2520the%2520tabular%2520setting%252C%2520we%250Apresent%2520a%2520novel%2520mirror-descent-based%2520policy%2520optimization%2520algorithm%2520for%2520the%250Ageneral%2520multi-turn%2520preference-based%2520RL%2520problem%252C%2520and%2520prove%2520its%2520convergence%2520to%250ANash%2520equilibrium.%2520To%2520evaluate%2520performance%252C%2520we%2520create%2520a%2520new%2520environment%252C%250AEducation%2520Dialogue%252C%2520where%2520a%2520teacher%2520agent%2520guides%2520a%2520student%2520in%2520learning%2520a%2520random%250Atopic%252C%2520and%2520show%2520that%2520a%2520deep%2520RL%2520variant%2520of%2520our%2520algorithm%2520outperforms%2520RLHF%250Abaselines.%2520Finally%252C%2520we%2520show%2520that%2520in%2520an%2520environment%2520with%2520explicit%2520rewards%252C%2520our%250Aalgorithm%2520recovers%2520the%2520same%2520performance%2520as%2520a%2520reward-based%2520RL%2520baseline%252C%2520despite%250Arelying%2520solely%2520on%2520a%2520weaker%2520preference%2520signal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14655v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-turn%20Reinforcement%20Learning%20from%20Preference%20Human%20Feedback&entry.906535625=Lior%20Shani%20and%20Aviv%20Rosenberg%20and%20Asaf%20Cassel%20and%20Oran%20Lang%20and%20Daniele%20Calandriello%20and%20Avital%20Zipori%20and%20Hila%20Noga%20and%20Orgad%20Keller%20and%20Bilal%20Piot%20and%20Idan%20Szpektor%20and%20Avinatan%20Hassidim%20and%20Yossi%20Matias%20and%20R%C3%A9mi%20Munos&entry.1292438233=%20%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20has%20become%20the%20standard%0Aapproach%20for%20aligning%20Large%20Language%20Models%20%28LLMs%29%20with%20human%20preferences%2C%0Aallowing%20LLMs%20to%20demonstrate%20remarkable%20abilities%20in%20various%20tasks.%20Existing%0Amethods%20work%20by%20emulating%20the%20preferences%20at%20the%20single%20decision%20%28turn%29%20level%2C%0Alimiting%20their%20capabilities%20in%20settings%20that%20require%20planning%20or%20multi-turn%0Ainteractions%20to%20achieve%20a%20long-term%20goal.%20In%20this%20paper%2C%20we%20address%20this%20issue%0Aby%20developing%20novel%20methods%20for%20Reinforcement%20Learning%20%28RL%29%20from%20preference%0Afeedback%20between%20two%20full%20multi-turn%20conversations.%20In%20the%20tabular%20setting%2C%20we%0Apresent%20a%20novel%20mirror-descent-based%20policy%20optimization%20algorithm%20for%20the%0Ageneral%20multi-turn%20preference-based%20RL%20problem%2C%20and%20prove%20its%20convergence%20to%0ANash%20equilibrium.%20To%20evaluate%20performance%2C%20we%20create%20a%20new%20environment%2C%0AEducation%20Dialogue%2C%20where%20a%20teacher%20agent%20guides%20a%20student%20in%20learning%20a%20random%0Atopic%2C%20and%20show%20that%20a%20deep%20RL%20variant%20of%20our%20algorithm%20outperforms%20RLHF%0Abaselines.%20Finally%2C%20we%20show%20that%20in%20an%20environment%20with%20explicit%20rewards%2C%20our%0Aalgorithm%20recovers%20the%20same%20performance%20as%20a%20reward-based%20RL%20baseline%2C%20despite%0Arelying%20solely%20on%20a%20weaker%20preference%20signal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14655v2&entry.124074799=Read"},
{"title": "Asynchronous Message-Passing and Zeroth-Order Optimization Based\n  Distributed Learning with a Use-Case in Resource Allocation in Communication\n  Networks", "author": "Pourya Behmandpoor and Marc Moonen and Panagiotis Patrinos", "abstract": "  Distributed learning and adaptation have received significant interest and\nfound wide-ranging applications in machine learning and signal processing.\nWhile various approaches, such as shared-memory optimization, multi-task\nlearning, and consensus-based learning (e.g., federated learning and learning\nover graphs), focus on optimizing either local costs or a global cost, there\nremains a need for further exploration of their interconnections. This paper\nspecifically focuses on a scenario where agents collaborate towards a common\ntask (i.e., optimizing a global cost equal to aggregated local costs) while\neffectively having distinct individual tasks (i.e., optimizing individual local\nparameters in a local cost). Each agent's actions can potentially impact other\nagents' performance through interactions. Notably, each agent has access to\nonly its local zeroth-order oracle (i.e., cost function value) and shares\nscalar values, rather than gradient vectors, with other agents, leading to\ncommunication bandwidth efficiency and agent privacy. Agents employ\nzeroth-order optimization to update their parameters, and the asynchronous\nmessage-passing between them is subject to bounded but possibly random\ncommunication delays. This paper presents theoretical convergence analyses and\nestablishes a convergence rate for nonconvex problems. Furthermore, it\naddresses the relevant use-case of deep learning-based resource allocation in\ncommunication networks and conducts numerical experiments in which agents,\nacting as transmitters, collaboratively train their individual policies to\nmaximize a global reward, e.g., a sum of data rates.\n", "link": "http://arxiv.org/abs/2311.04604v3", "date": "2024-12-02", "relevancy": 1.8991, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4985}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4705}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asynchronous%20Message-Passing%20and%20Zeroth-Order%20Optimization%20Based%0A%20%20Distributed%20Learning%20with%20a%20Use-Case%20in%20Resource%20Allocation%20in%20Communication%0A%20%20Networks&body=Title%3A%20Asynchronous%20Message-Passing%20and%20Zeroth-Order%20Optimization%20Based%0A%20%20Distributed%20Learning%20with%20a%20Use-Case%20in%20Resource%20Allocation%20in%20Communication%0A%20%20Networks%0AAuthor%3A%20Pourya%20Behmandpoor%20and%20Marc%20Moonen%20and%20Panagiotis%20Patrinos%0AAbstract%3A%20%20%20Distributed%20learning%20and%20adaptation%20have%20received%20significant%20interest%20and%0Afound%20wide-ranging%20applications%20in%20machine%20learning%20and%20signal%20processing.%0AWhile%20various%20approaches%2C%20such%20as%20shared-memory%20optimization%2C%20multi-task%0Alearning%2C%20and%20consensus-based%20learning%20%28e.g.%2C%20federated%20learning%20and%20learning%0Aover%20graphs%29%2C%20focus%20on%20optimizing%20either%20local%20costs%20or%20a%20global%20cost%2C%20there%0Aremains%20a%20need%20for%20further%20exploration%20of%20their%20interconnections.%20This%20paper%0Aspecifically%20focuses%20on%20a%20scenario%20where%20agents%20collaborate%20towards%20a%20common%0Atask%20%28i.e.%2C%20optimizing%20a%20global%20cost%20equal%20to%20aggregated%20local%20costs%29%20while%0Aeffectively%20having%20distinct%20individual%20tasks%20%28i.e.%2C%20optimizing%20individual%20local%0Aparameters%20in%20a%20local%20cost%29.%20Each%20agent%27s%20actions%20can%20potentially%20impact%20other%0Aagents%27%20performance%20through%20interactions.%20Notably%2C%20each%20agent%20has%20access%20to%0Aonly%20its%20local%20zeroth-order%20oracle%20%28i.e.%2C%20cost%20function%20value%29%20and%20shares%0Ascalar%20values%2C%20rather%20than%20gradient%20vectors%2C%20with%20other%20agents%2C%20leading%20to%0Acommunication%20bandwidth%20efficiency%20and%20agent%20privacy.%20Agents%20employ%0Azeroth-order%20optimization%20to%20update%20their%20parameters%2C%20and%20the%20asynchronous%0Amessage-passing%20between%20them%20is%20subject%20to%20bounded%20but%20possibly%20random%0Acommunication%20delays.%20This%20paper%20presents%20theoretical%20convergence%20analyses%20and%0Aestablishes%20a%20convergence%20rate%20for%20nonconvex%20problems.%20Furthermore%2C%20it%0Aaddresses%20the%20relevant%20use-case%20of%20deep%20learning-based%20resource%20allocation%20in%0Acommunication%20networks%20and%20conducts%20numerical%20experiments%20in%20which%20agents%2C%0Aacting%20as%20transmitters%2C%20collaboratively%20train%20their%20individual%20policies%20to%0Amaximize%20a%20global%20reward%2C%20e.g.%2C%20a%20sum%20of%20data%20rates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04604v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsynchronous%2520Message-Passing%2520and%2520Zeroth-Order%2520Optimization%2520Based%250A%2520%2520Distributed%2520Learning%2520with%2520a%2520Use-Case%2520in%2520Resource%2520Allocation%2520in%2520Communication%250A%2520%2520Networks%26entry.906535625%3DPourya%2520Behmandpoor%2520and%2520Marc%2520Moonen%2520and%2520Panagiotis%2520Patrinos%26entry.1292438233%3D%2520%2520Distributed%2520learning%2520and%2520adaptation%2520have%2520received%2520significant%2520interest%2520and%250Afound%2520wide-ranging%2520applications%2520in%2520machine%2520learning%2520and%2520signal%2520processing.%250AWhile%2520various%2520approaches%252C%2520such%2520as%2520shared-memory%2520optimization%252C%2520multi-task%250Alearning%252C%2520and%2520consensus-based%2520learning%2520%2528e.g.%252C%2520federated%2520learning%2520and%2520learning%250Aover%2520graphs%2529%252C%2520focus%2520on%2520optimizing%2520either%2520local%2520costs%2520or%2520a%2520global%2520cost%252C%2520there%250Aremains%2520a%2520need%2520for%2520further%2520exploration%2520of%2520their%2520interconnections.%2520This%2520paper%250Aspecifically%2520focuses%2520on%2520a%2520scenario%2520where%2520agents%2520collaborate%2520towards%2520a%2520common%250Atask%2520%2528i.e.%252C%2520optimizing%2520a%2520global%2520cost%2520equal%2520to%2520aggregated%2520local%2520costs%2529%2520while%250Aeffectively%2520having%2520distinct%2520individual%2520tasks%2520%2528i.e.%252C%2520optimizing%2520individual%2520local%250Aparameters%2520in%2520a%2520local%2520cost%2529.%2520Each%2520agent%2527s%2520actions%2520can%2520potentially%2520impact%2520other%250Aagents%2527%2520performance%2520through%2520interactions.%2520Notably%252C%2520each%2520agent%2520has%2520access%2520to%250Aonly%2520its%2520local%2520zeroth-order%2520oracle%2520%2528i.e.%252C%2520cost%2520function%2520value%2529%2520and%2520shares%250Ascalar%2520values%252C%2520rather%2520than%2520gradient%2520vectors%252C%2520with%2520other%2520agents%252C%2520leading%2520to%250Acommunication%2520bandwidth%2520efficiency%2520and%2520agent%2520privacy.%2520Agents%2520employ%250Azeroth-order%2520optimization%2520to%2520update%2520their%2520parameters%252C%2520and%2520the%2520asynchronous%250Amessage-passing%2520between%2520them%2520is%2520subject%2520to%2520bounded%2520but%2520possibly%2520random%250Acommunication%2520delays.%2520This%2520paper%2520presents%2520theoretical%2520convergence%2520analyses%2520and%250Aestablishes%2520a%2520convergence%2520rate%2520for%2520nonconvex%2520problems.%2520Furthermore%252C%2520it%250Aaddresses%2520the%2520relevant%2520use-case%2520of%2520deep%2520learning-based%2520resource%2520allocation%2520in%250Acommunication%2520networks%2520and%2520conducts%2520numerical%2520experiments%2520in%2520which%2520agents%252C%250Aacting%2520as%2520transmitters%252C%2520collaboratively%2520train%2520their%2520individual%2520policies%2520to%250Amaximize%2520a%2520global%2520reward%252C%2520e.g.%252C%2520a%2520sum%2520of%2520data%2520rates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.04604v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asynchronous%20Message-Passing%20and%20Zeroth-Order%20Optimization%20Based%0A%20%20Distributed%20Learning%20with%20a%20Use-Case%20in%20Resource%20Allocation%20in%20Communication%0A%20%20Networks&entry.906535625=Pourya%20Behmandpoor%20and%20Marc%20Moonen%20and%20Panagiotis%20Patrinos&entry.1292438233=%20%20Distributed%20learning%20and%20adaptation%20have%20received%20significant%20interest%20and%0Afound%20wide-ranging%20applications%20in%20machine%20learning%20and%20signal%20processing.%0AWhile%20various%20approaches%2C%20such%20as%20shared-memory%20optimization%2C%20multi-task%0Alearning%2C%20and%20consensus-based%20learning%20%28e.g.%2C%20federated%20learning%20and%20learning%0Aover%20graphs%29%2C%20focus%20on%20optimizing%20either%20local%20costs%20or%20a%20global%20cost%2C%20there%0Aremains%20a%20need%20for%20further%20exploration%20of%20their%20interconnections.%20This%20paper%0Aspecifically%20focuses%20on%20a%20scenario%20where%20agents%20collaborate%20towards%20a%20common%0Atask%20%28i.e.%2C%20optimizing%20a%20global%20cost%20equal%20to%20aggregated%20local%20costs%29%20while%0Aeffectively%20having%20distinct%20individual%20tasks%20%28i.e.%2C%20optimizing%20individual%20local%0Aparameters%20in%20a%20local%20cost%29.%20Each%20agent%27s%20actions%20can%20potentially%20impact%20other%0Aagents%27%20performance%20through%20interactions.%20Notably%2C%20each%20agent%20has%20access%20to%0Aonly%20its%20local%20zeroth-order%20oracle%20%28i.e.%2C%20cost%20function%20value%29%20and%20shares%0Ascalar%20values%2C%20rather%20than%20gradient%20vectors%2C%20with%20other%20agents%2C%20leading%20to%0Acommunication%20bandwidth%20efficiency%20and%20agent%20privacy.%20Agents%20employ%0Azeroth-order%20optimization%20to%20update%20their%20parameters%2C%20and%20the%20asynchronous%0Amessage-passing%20between%20them%20is%20subject%20to%20bounded%20but%20possibly%20random%0Acommunication%20delays.%20This%20paper%20presents%20theoretical%20convergence%20analyses%20and%0Aestablishes%20a%20convergence%20rate%20for%20nonconvex%20problems.%20Furthermore%2C%20it%0Aaddresses%20the%20relevant%20use-case%20of%20deep%20learning-based%20resource%20allocation%20in%0Acommunication%20networks%20and%20conducts%20numerical%20experiments%20in%20which%20agents%2C%0Aacting%20as%20transmitters%2C%20collaboratively%20train%20their%20individual%20policies%20to%0Amaximize%20a%20global%20reward%2C%20e.g.%2C%20a%20sum%20of%20data%20rates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04604v3&entry.124074799=Read"},
{"title": "Physics-Informed Real NVP for Satellite Power System Fault Detection", "author": "Carlo Cena and Umberto Albertin and Mauro Martini and Silvia Bucci and Marcello Chiaberge", "abstract": "  The unique challenges posed by the space environment, characterized by\nextreme conditions and limited accessibility, raise the need for robust and\nreliable techniques to identify and prevent satellite faults. Fault detection\nmethods in the space sector are required to ensure mission success and to\nprotect valuable assets. In this context, this paper proposes an Artificial\nIntelligence (AI) based fault detection methodology and evaluates its\nperformance on ADAPT (Advanced Diagnostics and Prognostics Testbed), an\nElectrical Power System (EPS) dataset, crafted in laboratory by NASA. Our study\nfocuses on the application of a physics-informed (PI) real-valued non-volume\npreserving (Real NVP) model for fault detection in space systems. The efficacy\nof this method is systematically compared against other AI approaches such as\nGated Recurrent Unit (GRU) and Autoencoder-based techniques. Results show that\nour physics-informed approach outperforms existing methods of fault detection,\ndemonstrating its suitability for addressing the unique challenges of satellite\nEPS sub-system faults. Furthermore, we unveil the competitive advantage of\nphysics-informed loss in AI models to address specific space needs, namely\nrobustness, reliability, and power constraints, crucial for space exploration\nand satellite missions.\n", "link": "http://arxiv.org/abs/2405.17339v2", "date": "2024-12-02", "relevancy": 1.8952, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4889}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4719}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Informed%20Real%20NVP%20for%20Satellite%20Power%20System%20Fault%20Detection&body=Title%3A%20Physics-Informed%20Real%20NVP%20for%20Satellite%20Power%20System%20Fault%20Detection%0AAuthor%3A%20Carlo%20Cena%20and%20Umberto%20Albertin%20and%20Mauro%20Martini%20and%20Silvia%20Bucci%20and%20Marcello%20Chiaberge%0AAbstract%3A%20%20%20The%20unique%20challenges%20posed%20by%20the%20space%20environment%2C%20characterized%20by%0Aextreme%20conditions%20and%20limited%20accessibility%2C%20raise%20the%20need%20for%20robust%20and%0Areliable%20techniques%20to%20identify%20and%20prevent%20satellite%20faults.%20Fault%20detection%0Amethods%20in%20the%20space%20sector%20are%20required%20to%20ensure%20mission%20success%20and%20to%0Aprotect%20valuable%20assets.%20In%20this%20context%2C%20this%20paper%20proposes%20an%20Artificial%0AIntelligence%20%28AI%29%20based%20fault%20detection%20methodology%20and%20evaluates%20its%0Aperformance%20on%20ADAPT%20%28Advanced%20Diagnostics%20and%20Prognostics%20Testbed%29%2C%20an%0AElectrical%20Power%20System%20%28EPS%29%20dataset%2C%20crafted%20in%20laboratory%20by%20NASA.%20Our%20study%0Afocuses%20on%20the%20application%20of%20a%20physics-informed%20%28PI%29%20real-valued%20non-volume%0Apreserving%20%28Real%20NVP%29%20model%20for%20fault%20detection%20in%20space%20systems.%20The%20efficacy%0Aof%20this%20method%20is%20systematically%20compared%20against%20other%20AI%20approaches%20such%20as%0AGated%20Recurrent%20Unit%20%28GRU%29%20and%20Autoencoder-based%20techniques.%20Results%20show%20that%0Aour%20physics-informed%20approach%20outperforms%20existing%20methods%20of%20fault%20detection%2C%0Ademonstrating%20its%20suitability%20for%20addressing%20the%20unique%20challenges%20of%20satellite%0AEPS%20sub-system%20faults.%20Furthermore%2C%20we%20unveil%20the%20competitive%20advantage%20of%0Aphysics-informed%20loss%20in%20AI%20models%20to%20address%20specific%20space%20needs%2C%20namely%0Arobustness%2C%20reliability%2C%20and%20power%20constraints%2C%20crucial%20for%20space%20exploration%0Aand%20satellite%20missions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17339v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Informed%2520Real%2520NVP%2520for%2520Satellite%2520Power%2520System%2520Fault%2520Detection%26entry.906535625%3DCarlo%2520Cena%2520and%2520Umberto%2520Albertin%2520and%2520Mauro%2520Martini%2520and%2520Silvia%2520Bucci%2520and%2520Marcello%2520Chiaberge%26entry.1292438233%3D%2520%2520The%2520unique%2520challenges%2520posed%2520by%2520the%2520space%2520environment%252C%2520characterized%2520by%250Aextreme%2520conditions%2520and%2520limited%2520accessibility%252C%2520raise%2520the%2520need%2520for%2520robust%2520and%250Areliable%2520techniques%2520to%2520identify%2520and%2520prevent%2520satellite%2520faults.%2520Fault%2520detection%250Amethods%2520in%2520the%2520space%2520sector%2520are%2520required%2520to%2520ensure%2520mission%2520success%2520and%2520to%250Aprotect%2520valuable%2520assets.%2520In%2520this%2520context%252C%2520this%2520paper%2520proposes%2520an%2520Artificial%250AIntelligence%2520%2528AI%2529%2520based%2520fault%2520detection%2520methodology%2520and%2520evaluates%2520its%250Aperformance%2520on%2520ADAPT%2520%2528Advanced%2520Diagnostics%2520and%2520Prognostics%2520Testbed%2529%252C%2520an%250AElectrical%2520Power%2520System%2520%2528EPS%2529%2520dataset%252C%2520crafted%2520in%2520laboratory%2520by%2520NASA.%2520Our%2520study%250Afocuses%2520on%2520the%2520application%2520of%2520a%2520physics-informed%2520%2528PI%2529%2520real-valued%2520non-volume%250Apreserving%2520%2528Real%2520NVP%2529%2520model%2520for%2520fault%2520detection%2520in%2520space%2520systems.%2520The%2520efficacy%250Aof%2520this%2520method%2520is%2520systematically%2520compared%2520against%2520other%2520AI%2520approaches%2520such%2520as%250AGated%2520Recurrent%2520Unit%2520%2528GRU%2529%2520and%2520Autoencoder-based%2520techniques.%2520Results%2520show%2520that%250Aour%2520physics-informed%2520approach%2520outperforms%2520existing%2520methods%2520of%2520fault%2520detection%252C%250Ademonstrating%2520its%2520suitability%2520for%2520addressing%2520the%2520unique%2520challenges%2520of%2520satellite%250AEPS%2520sub-system%2520faults.%2520Furthermore%252C%2520we%2520unveil%2520the%2520competitive%2520advantage%2520of%250Aphysics-informed%2520loss%2520in%2520AI%2520models%2520to%2520address%2520specific%2520space%2520needs%252C%2520namely%250Arobustness%252C%2520reliability%252C%2520and%2520power%2520constraints%252C%2520crucial%2520for%2520space%2520exploration%250Aand%2520satellite%2520missions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17339v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Informed%20Real%20NVP%20for%20Satellite%20Power%20System%20Fault%20Detection&entry.906535625=Carlo%20Cena%20and%20Umberto%20Albertin%20and%20Mauro%20Martini%20and%20Silvia%20Bucci%20and%20Marcello%20Chiaberge&entry.1292438233=%20%20The%20unique%20challenges%20posed%20by%20the%20space%20environment%2C%20characterized%20by%0Aextreme%20conditions%20and%20limited%20accessibility%2C%20raise%20the%20need%20for%20robust%20and%0Areliable%20techniques%20to%20identify%20and%20prevent%20satellite%20faults.%20Fault%20detection%0Amethods%20in%20the%20space%20sector%20are%20required%20to%20ensure%20mission%20success%20and%20to%0Aprotect%20valuable%20assets.%20In%20this%20context%2C%20this%20paper%20proposes%20an%20Artificial%0AIntelligence%20%28AI%29%20based%20fault%20detection%20methodology%20and%20evaluates%20its%0Aperformance%20on%20ADAPT%20%28Advanced%20Diagnostics%20and%20Prognostics%20Testbed%29%2C%20an%0AElectrical%20Power%20System%20%28EPS%29%20dataset%2C%20crafted%20in%20laboratory%20by%20NASA.%20Our%20study%0Afocuses%20on%20the%20application%20of%20a%20physics-informed%20%28PI%29%20real-valued%20non-volume%0Apreserving%20%28Real%20NVP%29%20model%20for%20fault%20detection%20in%20space%20systems.%20The%20efficacy%0Aof%20this%20method%20is%20systematically%20compared%20against%20other%20AI%20approaches%20such%20as%0AGated%20Recurrent%20Unit%20%28GRU%29%20and%20Autoencoder-based%20techniques.%20Results%20show%20that%0Aour%20physics-informed%20approach%20outperforms%20existing%20methods%20of%20fault%20detection%2C%0Ademonstrating%20its%20suitability%20for%20addressing%20the%20unique%20challenges%20of%20satellite%0AEPS%20sub-system%20faults.%20Furthermore%2C%20we%20unveil%20the%20competitive%20advantage%20of%0Aphysics-informed%20loss%20in%20AI%20models%20to%20address%20specific%20space%20needs%2C%20namely%0Arobustness%2C%20reliability%2C%20and%20power%20constraints%2C%20crucial%20for%20space%20exploration%0Aand%20satellite%20missions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17339v2&entry.124074799=Read"},
{"title": "Invertible Consistency Distillation for Text-Guided Image Editing in\n  Around 7 Steps", "author": "Nikita Starodubcev and Mikhail Khoroshikh and Artem Babenko and Dmitry Baranchuk", "abstract": "  Diffusion distillation represents a highly promising direction for achieving\nfaithful text-to-image generation in a few sampling steps. However, despite\nrecent successes, existing distilled models still do not provide the full\nspectrum of diffusion abilities, such as real image inversion, which enables\nmany precise image manipulation methods. This work aims to enrich distilled\ntext-to-image diffusion models with the ability to effectively encode real\nimages into their latent space. To this end, we introduce invertible\nConsistency Distillation (iCD), a generalized consistency distillation\nframework that facilitates both high-quality image synthesis and accurate image\nencoding in only 3-4 inference steps. Though the inversion problem for\ntext-to-image diffusion models gets exacerbated by high classifier-free\nguidance scales, we notice that dynamic guidance significantly reduces\nreconstruction errors without noticeable degradation in generation performance.\nAs a result, we demonstrate that iCD equipped with dynamic guidance may serve\nas a highly effective tool for zero-shot text-guided image editing, competing\nwith more expensive state-of-the-art alternatives.\n", "link": "http://arxiv.org/abs/2406.14539v3", "date": "2024-12-02", "relevancy": 1.8629, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6524}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6189}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Invertible%20Consistency%20Distillation%20for%20Text-Guided%20Image%20Editing%20in%0A%20%20Around%207%20Steps&body=Title%3A%20Invertible%20Consistency%20Distillation%20for%20Text-Guided%20Image%20Editing%20in%0A%20%20Around%207%20Steps%0AAuthor%3A%20Nikita%20Starodubcev%20and%20Mikhail%20Khoroshikh%20and%20Artem%20Babenko%20and%20Dmitry%20Baranchuk%0AAbstract%3A%20%20%20Diffusion%20distillation%20represents%20a%20highly%20promising%20direction%20for%20achieving%0Afaithful%20text-to-image%20generation%20in%20a%20few%20sampling%20steps.%20However%2C%20despite%0Arecent%20successes%2C%20existing%20distilled%20models%20still%20do%20not%20provide%20the%20full%0Aspectrum%20of%20diffusion%20abilities%2C%20such%20as%20real%20image%20inversion%2C%20which%20enables%0Amany%20precise%20image%20manipulation%20methods.%20This%20work%20aims%20to%20enrich%20distilled%0Atext-to-image%20diffusion%20models%20with%20the%20ability%20to%20effectively%20encode%20real%0Aimages%20into%20their%20latent%20space.%20To%20this%20end%2C%20we%20introduce%20invertible%0AConsistency%20Distillation%20%28iCD%29%2C%20a%20generalized%20consistency%20distillation%0Aframework%20that%20facilitates%20both%20high-quality%20image%20synthesis%20and%20accurate%20image%0Aencoding%20in%20only%203-4%20inference%20steps.%20Though%20the%20inversion%20problem%20for%0Atext-to-image%20diffusion%20models%20gets%20exacerbated%20by%20high%20classifier-free%0Aguidance%20scales%2C%20we%20notice%20that%20dynamic%20guidance%20significantly%20reduces%0Areconstruction%20errors%20without%20noticeable%20degradation%20in%20generation%20performance.%0AAs%20a%20result%2C%20we%20demonstrate%20that%20iCD%20equipped%20with%20dynamic%20guidance%20may%20serve%0Aas%20a%20highly%20effective%20tool%20for%20zero-shot%20text-guided%20image%20editing%2C%20competing%0Awith%20more%20expensive%20state-of-the-art%20alternatives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14539v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvertible%2520Consistency%2520Distillation%2520for%2520Text-Guided%2520Image%2520Editing%2520in%250A%2520%2520Around%25207%2520Steps%26entry.906535625%3DNikita%2520Starodubcev%2520and%2520Mikhail%2520Khoroshikh%2520and%2520Artem%2520Babenko%2520and%2520Dmitry%2520Baranchuk%26entry.1292438233%3D%2520%2520Diffusion%2520distillation%2520represents%2520a%2520highly%2520promising%2520direction%2520for%2520achieving%250Afaithful%2520text-to-image%2520generation%2520in%2520a%2520few%2520sampling%2520steps.%2520However%252C%2520despite%250Arecent%2520successes%252C%2520existing%2520distilled%2520models%2520still%2520do%2520not%2520provide%2520the%2520full%250Aspectrum%2520of%2520diffusion%2520abilities%252C%2520such%2520as%2520real%2520image%2520inversion%252C%2520which%2520enables%250Amany%2520precise%2520image%2520manipulation%2520methods.%2520This%2520work%2520aims%2520to%2520enrich%2520distilled%250Atext-to-image%2520diffusion%2520models%2520with%2520the%2520ability%2520to%2520effectively%2520encode%2520real%250Aimages%2520into%2520their%2520latent%2520space.%2520To%2520this%2520end%252C%2520we%2520introduce%2520invertible%250AConsistency%2520Distillation%2520%2528iCD%2529%252C%2520a%2520generalized%2520consistency%2520distillation%250Aframework%2520that%2520facilitates%2520both%2520high-quality%2520image%2520synthesis%2520and%2520accurate%2520image%250Aencoding%2520in%2520only%25203-4%2520inference%2520steps.%2520Though%2520the%2520inversion%2520problem%2520for%250Atext-to-image%2520diffusion%2520models%2520gets%2520exacerbated%2520by%2520high%2520classifier-free%250Aguidance%2520scales%252C%2520we%2520notice%2520that%2520dynamic%2520guidance%2520significantly%2520reduces%250Areconstruction%2520errors%2520without%2520noticeable%2520degradation%2520in%2520generation%2520performance.%250AAs%2520a%2520result%252C%2520we%2520demonstrate%2520that%2520iCD%2520equipped%2520with%2520dynamic%2520guidance%2520may%2520serve%250Aas%2520a%2520highly%2520effective%2520tool%2520for%2520zero-shot%2520text-guided%2520image%2520editing%252C%2520competing%250Awith%2520more%2520expensive%2520state-of-the-art%2520alternatives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14539v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Invertible%20Consistency%20Distillation%20for%20Text-Guided%20Image%20Editing%20in%0A%20%20Around%207%20Steps&entry.906535625=Nikita%20Starodubcev%20and%20Mikhail%20Khoroshikh%20and%20Artem%20Babenko%20and%20Dmitry%20Baranchuk&entry.1292438233=%20%20Diffusion%20distillation%20represents%20a%20highly%20promising%20direction%20for%20achieving%0Afaithful%20text-to-image%20generation%20in%20a%20few%20sampling%20steps.%20However%2C%20despite%0Arecent%20successes%2C%20existing%20distilled%20models%20still%20do%20not%20provide%20the%20full%0Aspectrum%20of%20diffusion%20abilities%2C%20such%20as%20real%20image%20inversion%2C%20which%20enables%0Amany%20precise%20image%20manipulation%20methods.%20This%20work%20aims%20to%20enrich%20distilled%0Atext-to-image%20diffusion%20models%20with%20the%20ability%20to%20effectively%20encode%20real%0Aimages%20into%20their%20latent%20space.%20To%20this%20end%2C%20we%20introduce%20invertible%0AConsistency%20Distillation%20%28iCD%29%2C%20a%20generalized%20consistency%20distillation%0Aframework%20that%20facilitates%20both%20high-quality%20image%20synthesis%20and%20accurate%20image%0Aencoding%20in%20only%203-4%20inference%20steps.%20Though%20the%20inversion%20problem%20for%0Atext-to-image%20diffusion%20models%20gets%20exacerbated%20by%20high%20classifier-free%0Aguidance%20scales%2C%20we%20notice%20that%20dynamic%20guidance%20significantly%20reduces%0Areconstruction%20errors%20without%20noticeable%20degradation%20in%20generation%20performance.%0AAs%20a%20result%2C%20we%20demonstrate%20that%20iCD%20equipped%20with%20dynamic%20guidance%20may%20serve%0Aas%20a%20highly%20effective%20tool%20for%20zero-shot%20text-guided%20image%20editing%2C%20competing%0Awith%20more%20expensive%20state-of-the-art%20alternatives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14539v3&entry.124074799=Read"},
{"title": "Deep Learning 2.0: Artificial Neurons That Matter -- Reject Correlation,\n  Embrace Orthogonality", "author": "Taha Bouhsine", "abstract": "  We introduce a yat-product-powered neural network, the Neural Matter Network\n(NMN), a breakthrough in deep learning that achieves non-linear pattern\nrecognition without activation functions. Our key innovation relies on the\nyat-product and yat-product, which naturally induces non-linearity by\nprojecting inputs into a pseudo-metric space, eliminating the need for\ntraditional activation functions while maintaining only a softmax layer for\nfinal class probability distribution. This approach simplifies network\narchitecture and provides unprecedented transparency into the network's\ndecision-making process. Our comprehensive empirical evaluation across\ndifferent datasets demonstrates that NMN consistently outperforms traditional\nMLPs. The results challenge the assumption that separate activation functions\nare necessary for effective deep-learning models. The implications of this work\nextend beyond immediate architectural benefits, by eliminating intermediate\nactivation functions while preserving non-linear capabilities, yat-MLP\nestablishes a new paradigm for neural network design that combines simplicity\nwith effectiveness. Most importantly, our approach provides unprecedented\ninsights into the traditionally opaque \"black-box\" nature of neural networks,\noffering a clearer understanding of how these models process and classify\ninformation.\n", "link": "http://arxiv.org/abs/2411.08085v2", "date": "2024-12-02", "relevancy": 1.8461, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.484}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4597}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%202.0%3A%20Artificial%20Neurons%20That%20Matter%20--%20Reject%20Correlation%2C%0A%20%20Embrace%20Orthogonality&body=Title%3A%20Deep%20Learning%202.0%3A%20Artificial%20Neurons%20That%20Matter%20--%20Reject%20Correlation%2C%0A%20%20Embrace%20Orthogonality%0AAuthor%3A%20Taha%20Bouhsine%0AAbstract%3A%20%20%20We%20introduce%20a%20yat-product-powered%20neural%20network%2C%20the%20Neural%20Matter%20Network%0A%28NMN%29%2C%20a%20breakthrough%20in%20deep%20learning%20that%20achieves%20non-linear%20pattern%0Arecognition%20without%20activation%20functions.%20Our%20key%20innovation%20relies%20on%20the%0Ayat-product%20and%20yat-product%2C%20which%20naturally%20induces%20non-linearity%20by%0Aprojecting%20inputs%20into%20a%20pseudo-metric%20space%2C%20eliminating%20the%20need%20for%0Atraditional%20activation%20functions%20while%20maintaining%20only%20a%20softmax%20layer%20for%0Afinal%20class%20probability%20distribution.%20This%20approach%20simplifies%20network%0Aarchitecture%20and%20provides%20unprecedented%20transparency%20into%20the%20network%27s%0Adecision-making%20process.%20Our%20comprehensive%20empirical%20evaluation%20across%0Adifferent%20datasets%20demonstrates%20that%20NMN%20consistently%20outperforms%20traditional%0AMLPs.%20The%20results%20challenge%20the%20assumption%20that%20separate%20activation%20functions%0Aare%20necessary%20for%20effective%20deep-learning%20models.%20The%20implications%20of%20this%20work%0Aextend%20beyond%20immediate%20architectural%20benefits%2C%20by%20eliminating%20intermediate%0Aactivation%20functions%20while%20preserving%20non-linear%20capabilities%2C%20yat-MLP%0Aestablishes%20a%20new%20paradigm%20for%20neural%20network%20design%20that%20combines%20simplicity%0Awith%20effectiveness.%20Most%20importantly%2C%20our%20approach%20provides%20unprecedented%0Ainsights%20into%20the%20traditionally%20opaque%20%22black-box%22%20nature%20of%20neural%20networks%2C%0Aoffering%20a%20clearer%20understanding%20of%20how%20these%20models%20process%20and%20classify%0Ainformation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08085v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%25202.0%253A%2520Artificial%2520Neurons%2520That%2520Matter%2520--%2520Reject%2520Correlation%252C%250A%2520%2520Embrace%2520Orthogonality%26entry.906535625%3DTaha%2520Bouhsine%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520yat-product-powered%2520neural%2520network%252C%2520the%2520Neural%2520Matter%2520Network%250A%2528NMN%2529%252C%2520a%2520breakthrough%2520in%2520deep%2520learning%2520that%2520achieves%2520non-linear%2520pattern%250Arecognition%2520without%2520activation%2520functions.%2520Our%2520key%2520innovation%2520relies%2520on%2520the%250Ayat-product%2520and%2520yat-product%252C%2520which%2520naturally%2520induces%2520non-linearity%2520by%250Aprojecting%2520inputs%2520into%2520a%2520pseudo-metric%2520space%252C%2520eliminating%2520the%2520need%2520for%250Atraditional%2520activation%2520functions%2520while%2520maintaining%2520only%2520a%2520softmax%2520layer%2520for%250Afinal%2520class%2520probability%2520distribution.%2520This%2520approach%2520simplifies%2520network%250Aarchitecture%2520and%2520provides%2520unprecedented%2520transparency%2520into%2520the%2520network%2527s%250Adecision-making%2520process.%2520Our%2520comprehensive%2520empirical%2520evaluation%2520across%250Adifferent%2520datasets%2520demonstrates%2520that%2520NMN%2520consistently%2520outperforms%2520traditional%250AMLPs.%2520The%2520results%2520challenge%2520the%2520assumption%2520that%2520separate%2520activation%2520functions%250Aare%2520necessary%2520for%2520effective%2520deep-learning%2520models.%2520The%2520implications%2520of%2520this%2520work%250Aextend%2520beyond%2520immediate%2520architectural%2520benefits%252C%2520by%2520eliminating%2520intermediate%250Aactivation%2520functions%2520while%2520preserving%2520non-linear%2520capabilities%252C%2520yat-MLP%250Aestablishes%2520a%2520new%2520paradigm%2520for%2520neural%2520network%2520design%2520that%2520combines%2520simplicity%250Awith%2520effectiveness.%2520Most%2520importantly%252C%2520our%2520approach%2520provides%2520unprecedented%250Ainsights%2520into%2520the%2520traditionally%2520opaque%2520%2522black-box%2522%2520nature%2520of%2520neural%2520networks%252C%250Aoffering%2520a%2520clearer%2520understanding%2520of%2520how%2520these%2520models%2520process%2520and%2520classify%250Ainformation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08085v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%202.0%3A%20Artificial%20Neurons%20That%20Matter%20--%20Reject%20Correlation%2C%0A%20%20Embrace%20Orthogonality&entry.906535625=Taha%20Bouhsine&entry.1292438233=%20%20We%20introduce%20a%20yat-product-powered%20neural%20network%2C%20the%20Neural%20Matter%20Network%0A%28NMN%29%2C%20a%20breakthrough%20in%20deep%20learning%20that%20achieves%20non-linear%20pattern%0Arecognition%20without%20activation%20functions.%20Our%20key%20innovation%20relies%20on%20the%0Ayat-product%20and%20yat-product%2C%20which%20naturally%20induces%20non-linearity%20by%0Aprojecting%20inputs%20into%20a%20pseudo-metric%20space%2C%20eliminating%20the%20need%20for%0Atraditional%20activation%20functions%20while%20maintaining%20only%20a%20softmax%20layer%20for%0Afinal%20class%20probability%20distribution.%20This%20approach%20simplifies%20network%0Aarchitecture%20and%20provides%20unprecedented%20transparency%20into%20the%20network%27s%0Adecision-making%20process.%20Our%20comprehensive%20empirical%20evaluation%20across%0Adifferent%20datasets%20demonstrates%20that%20NMN%20consistently%20outperforms%20traditional%0AMLPs.%20The%20results%20challenge%20the%20assumption%20that%20separate%20activation%20functions%0Aare%20necessary%20for%20effective%20deep-learning%20models.%20The%20implications%20of%20this%20work%0Aextend%20beyond%20immediate%20architectural%20benefits%2C%20by%20eliminating%20intermediate%0Aactivation%20functions%20while%20preserving%20non-linear%20capabilities%2C%20yat-MLP%0Aestablishes%20a%20new%20paradigm%20for%20neural%20network%20design%20that%20combines%20simplicity%0Awith%20effectiveness.%20Most%20importantly%2C%20our%20approach%20provides%20unprecedented%0Ainsights%20into%20the%20traditionally%20opaque%20%22black-box%22%20nature%20of%20neural%20networks%2C%0Aoffering%20a%20clearer%20understanding%20of%20how%20these%20models%20process%20and%20classify%0Ainformation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08085v2&entry.124074799=Read"},
{"title": "Masked Generative Priors Improve World Models Sequence Modelling\n  Capabilities", "author": "Cristian Meo and Mircea Lica and Zarif Ikram and Akihiro Nakano and Vedant Shah and Aniket Rajiv Didolkar and Dianbo Liu and Anirudh Goyal and Justin Dauwels", "abstract": "  Deep Reinforcement Learning (RL) has become the leading approach for creating\nartificial agents in complex environments. Model-based approaches, which are RL\nmethods with world models that predict environment dynamics, are among the most\npromising directions for improving data efficiency, forming a critical step\ntoward bridging the gap between research and real-world deployment. In\nparticular, world models enhance sample efficiency by learning in imagination,\nwhich involves training a generative sequence model of the environment in a\nself-supervised manner. Recently, Masked Generative Modelling has emerged as a\nmore efficient and superior inductive bias for modelling and generating token\nsequences. Building on the Efficient Stochastic Transformer-based World Models\n(STORM) architecture, we replace the traditional MLP prior with a Masked\nGenerative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our\nmodel on two downstream tasks: reinforcement learning and video prediction.\nGIT-STORM demonstrates substantial performance gains in RL tasks on the Atari\n100k benchmark. Moreover, we apply Transformer-based World Models to continuous\naction environments for the first time, addressing a significant gap in prior\nresearch. To achieve this, we employ a state mixer function that integrates\nlatent state representations with actions, enabling our model to handle\ncontinuous control tasks. We validate this approach through qualitative and\nquantitative analyses on the DeepMind Control Suite, showcasing the\neffectiveness of Transformer-based World Models in this new domain. Our results\nhighlight the versatility and efficacy of the MaskGIT dynamics prior, paving\nthe way for more accurate world models and effective RL policies.\n", "link": "http://arxiv.org/abs/2410.07836v4", "date": "2024-12-02", "relevancy": 1.8383, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6453}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5726}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Masked%20Generative%20Priors%20Improve%20World%20Models%20Sequence%20Modelling%0A%20%20Capabilities&body=Title%3A%20Masked%20Generative%20Priors%20Improve%20World%20Models%20Sequence%20Modelling%0A%20%20Capabilities%0AAuthor%3A%20Cristian%20Meo%20and%20Mircea%20Lica%20and%20Zarif%20Ikram%20and%20Akihiro%20Nakano%20and%20Vedant%20Shah%20and%20Aniket%20Rajiv%20Didolkar%20and%20Dianbo%20Liu%20and%20Anirudh%20Goyal%20and%20Justin%20Dauwels%0AAbstract%3A%20%20%20Deep%20Reinforcement%20Learning%20%28RL%29%20has%20become%20the%20leading%20approach%20for%20creating%0Aartificial%20agents%20in%20complex%20environments.%20Model-based%20approaches%2C%20which%20are%20RL%0Amethods%20with%20world%20models%20that%20predict%20environment%20dynamics%2C%20are%20among%20the%20most%0Apromising%20directions%20for%20improving%20data%20efficiency%2C%20forming%20a%20critical%20step%0Atoward%20bridging%20the%20gap%20between%20research%20and%20real-world%20deployment.%20In%0Aparticular%2C%20world%20models%20enhance%20sample%20efficiency%20by%20learning%20in%20imagination%2C%0Awhich%20involves%20training%20a%20generative%20sequence%20model%20of%20the%20environment%20in%20a%0Aself-supervised%20manner.%20Recently%2C%20Masked%20Generative%20Modelling%20has%20emerged%20as%20a%0Amore%20efficient%20and%20superior%20inductive%20bias%20for%20modelling%20and%20generating%20token%0Asequences.%20Building%20on%20the%20Efficient%20Stochastic%20Transformer-based%20World%20Models%0A%28STORM%29%20architecture%2C%20we%20replace%20the%20traditional%20MLP%20prior%20with%20a%20Masked%0AGenerative%20Prior%20%28e.g.%2C%20MaskGIT%20Prior%29%20and%20introduce%20GIT-STORM.%20We%20evaluate%20our%0Amodel%20on%20two%20downstream%20tasks%3A%20reinforcement%20learning%20and%20video%20prediction.%0AGIT-STORM%20demonstrates%20substantial%20performance%20gains%20in%20RL%20tasks%20on%20the%20Atari%0A100k%20benchmark.%20Moreover%2C%20we%20apply%20Transformer-based%20World%20Models%20to%20continuous%0Aaction%20environments%20for%20the%20first%20time%2C%20addressing%20a%20significant%20gap%20in%20prior%0Aresearch.%20To%20achieve%20this%2C%20we%20employ%20a%20state%20mixer%20function%20that%20integrates%0Alatent%20state%20representations%20with%20actions%2C%20enabling%20our%20model%20to%20handle%0Acontinuous%20control%20tasks.%20We%20validate%20this%20approach%20through%20qualitative%20and%0Aquantitative%20analyses%20on%20the%20DeepMind%20Control%20Suite%2C%20showcasing%20the%0Aeffectiveness%20of%20Transformer-based%20World%20Models%20in%20this%20new%20domain.%20Our%20results%0Ahighlight%20the%20versatility%20and%20efficacy%20of%20the%20MaskGIT%20dynamics%20prior%2C%20paving%0Athe%20way%20for%20more%20accurate%20world%20models%20and%20effective%20RL%20policies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07836v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasked%2520Generative%2520Priors%2520Improve%2520World%2520Models%2520Sequence%2520Modelling%250A%2520%2520Capabilities%26entry.906535625%3DCristian%2520Meo%2520and%2520Mircea%2520Lica%2520and%2520Zarif%2520Ikram%2520and%2520Akihiro%2520Nakano%2520and%2520Vedant%2520Shah%2520and%2520Aniket%2520Rajiv%2520Didolkar%2520and%2520Dianbo%2520Liu%2520and%2520Anirudh%2520Goyal%2520and%2520Justin%2520Dauwels%26entry.1292438233%3D%2520%2520Deep%2520Reinforcement%2520Learning%2520%2528RL%2529%2520has%2520become%2520the%2520leading%2520approach%2520for%2520creating%250Aartificial%2520agents%2520in%2520complex%2520environments.%2520Model-based%2520approaches%252C%2520which%2520are%2520RL%250Amethods%2520with%2520world%2520models%2520that%2520predict%2520environment%2520dynamics%252C%2520are%2520among%2520the%2520most%250Apromising%2520directions%2520for%2520improving%2520data%2520efficiency%252C%2520forming%2520a%2520critical%2520step%250Atoward%2520bridging%2520the%2520gap%2520between%2520research%2520and%2520real-world%2520deployment.%2520In%250Aparticular%252C%2520world%2520models%2520enhance%2520sample%2520efficiency%2520by%2520learning%2520in%2520imagination%252C%250Awhich%2520involves%2520training%2520a%2520generative%2520sequence%2520model%2520of%2520the%2520environment%2520in%2520a%250Aself-supervised%2520manner.%2520Recently%252C%2520Masked%2520Generative%2520Modelling%2520has%2520emerged%2520as%2520a%250Amore%2520efficient%2520and%2520superior%2520inductive%2520bias%2520for%2520modelling%2520and%2520generating%2520token%250Asequences.%2520Building%2520on%2520the%2520Efficient%2520Stochastic%2520Transformer-based%2520World%2520Models%250A%2528STORM%2529%2520architecture%252C%2520we%2520replace%2520the%2520traditional%2520MLP%2520prior%2520with%2520a%2520Masked%250AGenerative%2520Prior%2520%2528e.g.%252C%2520MaskGIT%2520Prior%2529%2520and%2520introduce%2520GIT-STORM.%2520We%2520evaluate%2520our%250Amodel%2520on%2520two%2520downstream%2520tasks%253A%2520reinforcement%2520learning%2520and%2520video%2520prediction.%250AGIT-STORM%2520demonstrates%2520substantial%2520performance%2520gains%2520in%2520RL%2520tasks%2520on%2520the%2520Atari%250A100k%2520benchmark.%2520Moreover%252C%2520we%2520apply%2520Transformer-based%2520World%2520Models%2520to%2520continuous%250Aaction%2520environments%2520for%2520the%2520first%2520time%252C%2520addressing%2520a%2520significant%2520gap%2520in%2520prior%250Aresearch.%2520To%2520achieve%2520this%252C%2520we%2520employ%2520a%2520state%2520mixer%2520function%2520that%2520integrates%250Alatent%2520state%2520representations%2520with%2520actions%252C%2520enabling%2520our%2520model%2520to%2520handle%250Acontinuous%2520control%2520tasks.%2520We%2520validate%2520this%2520approach%2520through%2520qualitative%2520and%250Aquantitative%2520analyses%2520on%2520the%2520DeepMind%2520Control%2520Suite%252C%2520showcasing%2520the%250Aeffectiveness%2520of%2520Transformer-based%2520World%2520Models%2520in%2520this%2520new%2520domain.%2520Our%2520results%250Ahighlight%2520the%2520versatility%2520and%2520efficacy%2520of%2520the%2520MaskGIT%2520dynamics%2520prior%252C%2520paving%250Athe%2520way%2520for%2520more%2520accurate%2520world%2520models%2520and%2520effective%2520RL%2520policies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07836v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Generative%20Priors%20Improve%20World%20Models%20Sequence%20Modelling%0A%20%20Capabilities&entry.906535625=Cristian%20Meo%20and%20Mircea%20Lica%20and%20Zarif%20Ikram%20and%20Akihiro%20Nakano%20and%20Vedant%20Shah%20and%20Aniket%20Rajiv%20Didolkar%20and%20Dianbo%20Liu%20and%20Anirudh%20Goyal%20and%20Justin%20Dauwels&entry.1292438233=%20%20Deep%20Reinforcement%20Learning%20%28RL%29%20has%20become%20the%20leading%20approach%20for%20creating%0Aartificial%20agents%20in%20complex%20environments.%20Model-based%20approaches%2C%20which%20are%20RL%0Amethods%20with%20world%20models%20that%20predict%20environment%20dynamics%2C%20are%20among%20the%20most%0Apromising%20directions%20for%20improving%20data%20efficiency%2C%20forming%20a%20critical%20step%0Atoward%20bridging%20the%20gap%20between%20research%20and%20real-world%20deployment.%20In%0Aparticular%2C%20world%20models%20enhance%20sample%20efficiency%20by%20learning%20in%20imagination%2C%0Awhich%20involves%20training%20a%20generative%20sequence%20model%20of%20the%20environment%20in%20a%0Aself-supervised%20manner.%20Recently%2C%20Masked%20Generative%20Modelling%20has%20emerged%20as%20a%0Amore%20efficient%20and%20superior%20inductive%20bias%20for%20modelling%20and%20generating%20token%0Asequences.%20Building%20on%20the%20Efficient%20Stochastic%20Transformer-based%20World%20Models%0A%28STORM%29%20architecture%2C%20we%20replace%20the%20traditional%20MLP%20prior%20with%20a%20Masked%0AGenerative%20Prior%20%28e.g.%2C%20MaskGIT%20Prior%29%20and%20introduce%20GIT-STORM.%20We%20evaluate%20our%0Amodel%20on%20two%20downstream%20tasks%3A%20reinforcement%20learning%20and%20video%20prediction.%0AGIT-STORM%20demonstrates%20substantial%20performance%20gains%20in%20RL%20tasks%20on%20the%20Atari%0A100k%20benchmark.%20Moreover%2C%20we%20apply%20Transformer-based%20World%20Models%20to%20continuous%0Aaction%20environments%20for%20the%20first%20time%2C%20addressing%20a%20significant%20gap%20in%20prior%0Aresearch.%20To%20achieve%20this%2C%20we%20employ%20a%20state%20mixer%20function%20that%20integrates%0Alatent%20state%20representations%20with%20actions%2C%20enabling%20our%20model%20to%20handle%0Acontinuous%20control%20tasks.%20We%20validate%20this%20approach%20through%20qualitative%20and%0Aquantitative%20analyses%20on%20the%20DeepMind%20Control%20Suite%2C%20showcasing%20the%0Aeffectiveness%20of%20Transformer-based%20World%20Models%20in%20this%20new%20domain.%20Our%20results%0Ahighlight%20the%20versatility%20and%20efficacy%20of%20the%20MaskGIT%20dynamics%20prior%2C%20paving%0Athe%20way%20for%20more%20accurate%20world%20models%20and%20effective%20RL%20policies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07836v4&entry.124074799=Read"},
{"title": "Fair Generalized Linear Mixed Models", "author": "Jan Pablo Burgard and Jo\u00e3o Vitor Pamplona", "abstract": "  When using machine learning for automated prediction, it is important to\naccount for fairness in the prediction. Fairness in machine learning aims to\nensure that biases in the data and model inaccuracies do not lead to\ndiscriminatory decisions. E.g., predictions from fair machine learning models\nshould not discriminate against sensitive variables such as sexual orientation\nand ethnicity. The training data often in obtained from social surveys. In\nsocial surveys, oftentimes the data collection process is a strata sampling,\ne.g. due to cost restrictions. In strata samples, the assumption of\nindependence between the observation is not fulfilled. Hence, if the machine\nlearning models do not account for the strata correlations, the results may be\nbiased. Especially high is the bias in cases where the strata assignment is\ncorrelated to the variable of interest. We present in this paper an algorithm\nthat can handle both problems simultaneously, and we demonstrate the impact of\nstratified sampling on the quality of fair machine learning predictions in a\nreproducible simulation study.\n", "link": "http://arxiv.org/abs/2405.09273v7", "date": "2024-12-02", "relevancy": 1.8378, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4884}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4622}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fair%20Generalized%20Linear%20Mixed%20Models&body=Title%3A%20Fair%20Generalized%20Linear%20Mixed%20Models%0AAuthor%3A%20Jan%20Pablo%20Burgard%20and%20Jo%C3%A3o%20Vitor%20Pamplona%0AAbstract%3A%20%20%20When%20using%20machine%20learning%20for%20automated%20prediction%2C%20it%20is%20important%20to%0Aaccount%20for%20fairness%20in%20the%20prediction.%20Fairness%20in%20machine%20learning%20aims%20to%0Aensure%20that%20biases%20in%20the%20data%20and%20model%20inaccuracies%20do%20not%20lead%20to%0Adiscriminatory%20decisions.%20E.g.%2C%20predictions%20from%20fair%20machine%20learning%20models%0Ashould%20not%20discriminate%20against%20sensitive%20variables%20such%20as%20sexual%20orientation%0Aand%20ethnicity.%20The%20training%20data%20often%20in%20obtained%20from%20social%20surveys.%20In%0Asocial%20surveys%2C%20oftentimes%20the%20data%20collection%20process%20is%20a%20strata%20sampling%2C%0Ae.g.%20due%20to%20cost%20restrictions.%20In%20strata%20samples%2C%20the%20assumption%20of%0Aindependence%20between%20the%20observation%20is%20not%20fulfilled.%20Hence%2C%20if%20the%20machine%0Alearning%20models%20do%20not%20account%20for%20the%20strata%20correlations%2C%20the%20results%20may%20be%0Abiased.%20Especially%20high%20is%20the%20bias%20in%20cases%20where%20the%20strata%20assignment%20is%0Acorrelated%20to%20the%20variable%20of%20interest.%20We%20present%20in%20this%20paper%20an%20algorithm%0Athat%20can%20handle%20both%20problems%20simultaneously%2C%20and%20we%20demonstrate%20the%20impact%20of%0Astratified%20sampling%20on%20the%20quality%20of%20fair%20machine%20learning%20predictions%20in%20a%0Areproducible%20simulation%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09273v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFair%2520Generalized%2520Linear%2520Mixed%2520Models%26entry.906535625%3DJan%2520Pablo%2520Burgard%2520and%2520Jo%25C3%25A3o%2520Vitor%2520Pamplona%26entry.1292438233%3D%2520%2520When%2520using%2520machine%2520learning%2520for%2520automated%2520prediction%252C%2520it%2520is%2520important%2520to%250Aaccount%2520for%2520fairness%2520in%2520the%2520prediction.%2520Fairness%2520in%2520machine%2520learning%2520aims%2520to%250Aensure%2520that%2520biases%2520in%2520the%2520data%2520and%2520model%2520inaccuracies%2520do%2520not%2520lead%2520to%250Adiscriminatory%2520decisions.%2520E.g.%252C%2520predictions%2520from%2520fair%2520machine%2520learning%2520models%250Ashould%2520not%2520discriminate%2520against%2520sensitive%2520variables%2520such%2520as%2520sexual%2520orientation%250Aand%2520ethnicity.%2520The%2520training%2520data%2520often%2520in%2520obtained%2520from%2520social%2520surveys.%2520In%250Asocial%2520surveys%252C%2520oftentimes%2520the%2520data%2520collection%2520process%2520is%2520a%2520strata%2520sampling%252C%250Ae.g.%2520due%2520to%2520cost%2520restrictions.%2520In%2520strata%2520samples%252C%2520the%2520assumption%2520of%250Aindependence%2520between%2520the%2520observation%2520is%2520not%2520fulfilled.%2520Hence%252C%2520if%2520the%2520machine%250Alearning%2520models%2520do%2520not%2520account%2520for%2520the%2520strata%2520correlations%252C%2520the%2520results%2520may%2520be%250Abiased.%2520Especially%2520high%2520is%2520the%2520bias%2520in%2520cases%2520where%2520the%2520strata%2520assignment%2520is%250Acorrelated%2520to%2520the%2520variable%2520of%2520interest.%2520We%2520present%2520in%2520this%2520paper%2520an%2520algorithm%250Athat%2520can%2520handle%2520both%2520problems%2520simultaneously%252C%2520and%2520we%2520demonstrate%2520the%2520impact%2520of%250Astratified%2520sampling%2520on%2520the%2520quality%2520of%2520fair%2520machine%2520learning%2520predictions%2520in%2520a%250Areproducible%2520simulation%2520study.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09273v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fair%20Generalized%20Linear%20Mixed%20Models&entry.906535625=Jan%20Pablo%20Burgard%20and%20Jo%C3%A3o%20Vitor%20Pamplona&entry.1292438233=%20%20When%20using%20machine%20learning%20for%20automated%20prediction%2C%20it%20is%20important%20to%0Aaccount%20for%20fairness%20in%20the%20prediction.%20Fairness%20in%20machine%20learning%20aims%20to%0Aensure%20that%20biases%20in%20the%20data%20and%20model%20inaccuracies%20do%20not%20lead%20to%0Adiscriminatory%20decisions.%20E.g.%2C%20predictions%20from%20fair%20machine%20learning%20models%0Ashould%20not%20discriminate%20against%20sensitive%20variables%20such%20as%20sexual%20orientation%0Aand%20ethnicity.%20The%20training%20data%20often%20in%20obtained%20from%20social%20surveys.%20In%0Asocial%20surveys%2C%20oftentimes%20the%20data%20collection%20process%20is%20a%20strata%20sampling%2C%0Ae.g.%20due%20to%20cost%20restrictions.%20In%20strata%20samples%2C%20the%20assumption%20of%0Aindependence%20between%20the%20observation%20is%20not%20fulfilled.%20Hence%2C%20if%20the%20machine%0Alearning%20models%20do%20not%20account%20for%20the%20strata%20correlations%2C%20the%20results%20may%20be%0Abiased.%20Especially%20high%20is%20the%20bias%20in%20cases%20where%20the%20strata%20assignment%20is%0Acorrelated%20to%20the%20variable%20of%20interest.%20We%20present%20in%20this%20paper%20an%20algorithm%0Athat%20can%20handle%20both%20problems%20simultaneously%2C%20and%20we%20demonstrate%20the%20impact%20of%0Astratified%20sampling%20on%20the%20quality%20of%20fair%20machine%20learning%20predictions%20in%20a%0Areproducible%20simulation%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09273v7&entry.124074799=Read"},
{"title": "Fair Mixed Effects Support Vector Machine", "author": "Jan Pablo Burgard and Jo\u00e3o Vitor Pamplona", "abstract": "  To ensure unbiased and ethical automated predictions, fairness must be a core\nprinciple in machine learning applications. Fairness in machine learning aims\nto mitigate biases present in the training data and model imperfections that\ncould lead to discriminatory outcomes. This is achieved by preventing the model\nfrom making decisions based on sensitive characteristics like ethnicity or\nsexual orientation. A fundamental assumption in machine learning is the\nindependence of observations. However, this assumption often does not hold true\nfor data describing social phenomena, where data points are often clustered\nbased. Hence, if the machine learning models do not account for the cluster\ncorrelations, the results may be biased. Especially high is the bias in cases\nwhere the cluster assignment is correlated to the variable of interest. We\npresent a fair mixed effects support vector machine algorithm that can handle\nboth problems simultaneously. With a reproducible simulation study we\ndemonstrate the impact of clustered data on the quality of fair machine\nlearning predictions.\n", "link": "http://arxiv.org/abs/2405.06433v6", "date": "2024-12-02", "relevancy": 1.8305, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4784}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4535}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fair%20Mixed%20Effects%20Support%20Vector%20Machine&body=Title%3A%20Fair%20Mixed%20Effects%20Support%20Vector%20Machine%0AAuthor%3A%20Jan%20Pablo%20Burgard%20and%20Jo%C3%A3o%20Vitor%20Pamplona%0AAbstract%3A%20%20%20To%20ensure%20unbiased%20and%20ethical%20automated%20predictions%2C%20fairness%20must%20be%20a%20core%0Aprinciple%20in%20machine%20learning%20applications.%20Fairness%20in%20machine%20learning%20aims%0Ato%20mitigate%20biases%20present%20in%20the%20training%20data%20and%20model%20imperfections%20that%0Acould%20lead%20to%20discriminatory%20outcomes.%20This%20is%20achieved%20by%20preventing%20the%20model%0Afrom%20making%20decisions%20based%20on%20sensitive%20characteristics%20like%20ethnicity%20or%0Asexual%20orientation.%20A%20fundamental%20assumption%20in%20machine%20learning%20is%20the%0Aindependence%20of%20observations.%20However%2C%20this%20assumption%20often%20does%20not%20hold%20true%0Afor%20data%20describing%20social%20phenomena%2C%20where%20data%20points%20are%20often%20clustered%0Abased.%20Hence%2C%20if%20the%20machine%20learning%20models%20do%20not%20account%20for%20the%20cluster%0Acorrelations%2C%20the%20results%20may%20be%20biased.%20Especially%20high%20is%20the%20bias%20in%20cases%0Awhere%20the%20cluster%20assignment%20is%20correlated%20to%20the%20variable%20of%20interest.%20We%0Apresent%20a%20fair%20mixed%20effects%20support%20vector%20machine%20algorithm%20that%20can%20handle%0Aboth%20problems%20simultaneously.%20With%20a%20reproducible%20simulation%20study%20we%0Ademonstrate%20the%20impact%20of%20clustered%20data%20on%20the%20quality%20of%20fair%20machine%0Alearning%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06433v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFair%2520Mixed%2520Effects%2520Support%2520Vector%2520Machine%26entry.906535625%3DJan%2520Pablo%2520Burgard%2520and%2520Jo%25C3%25A3o%2520Vitor%2520Pamplona%26entry.1292438233%3D%2520%2520To%2520ensure%2520unbiased%2520and%2520ethical%2520automated%2520predictions%252C%2520fairness%2520must%2520be%2520a%2520core%250Aprinciple%2520in%2520machine%2520learning%2520applications.%2520Fairness%2520in%2520machine%2520learning%2520aims%250Ato%2520mitigate%2520biases%2520present%2520in%2520the%2520training%2520data%2520and%2520model%2520imperfections%2520that%250Acould%2520lead%2520to%2520discriminatory%2520outcomes.%2520This%2520is%2520achieved%2520by%2520preventing%2520the%2520model%250Afrom%2520making%2520decisions%2520based%2520on%2520sensitive%2520characteristics%2520like%2520ethnicity%2520or%250Asexual%2520orientation.%2520A%2520fundamental%2520assumption%2520in%2520machine%2520learning%2520is%2520the%250Aindependence%2520of%2520observations.%2520However%252C%2520this%2520assumption%2520often%2520does%2520not%2520hold%2520true%250Afor%2520data%2520describing%2520social%2520phenomena%252C%2520where%2520data%2520points%2520are%2520often%2520clustered%250Abased.%2520Hence%252C%2520if%2520the%2520machine%2520learning%2520models%2520do%2520not%2520account%2520for%2520the%2520cluster%250Acorrelations%252C%2520the%2520results%2520may%2520be%2520biased.%2520Especially%2520high%2520is%2520the%2520bias%2520in%2520cases%250Awhere%2520the%2520cluster%2520assignment%2520is%2520correlated%2520to%2520the%2520variable%2520of%2520interest.%2520We%250Apresent%2520a%2520fair%2520mixed%2520effects%2520support%2520vector%2520machine%2520algorithm%2520that%2520can%2520handle%250Aboth%2520problems%2520simultaneously.%2520With%2520a%2520reproducible%2520simulation%2520study%2520we%250Ademonstrate%2520the%2520impact%2520of%2520clustered%2520data%2520on%2520the%2520quality%2520of%2520fair%2520machine%250Alearning%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06433v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fair%20Mixed%20Effects%20Support%20Vector%20Machine&entry.906535625=Jan%20Pablo%20Burgard%20and%20Jo%C3%A3o%20Vitor%20Pamplona&entry.1292438233=%20%20To%20ensure%20unbiased%20and%20ethical%20automated%20predictions%2C%20fairness%20must%20be%20a%20core%0Aprinciple%20in%20machine%20learning%20applications.%20Fairness%20in%20machine%20learning%20aims%0Ato%20mitigate%20biases%20present%20in%20the%20training%20data%20and%20model%20imperfections%20that%0Acould%20lead%20to%20discriminatory%20outcomes.%20This%20is%20achieved%20by%20preventing%20the%20model%0Afrom%20making%20decisions%20based%20on%20sensitive%20characteristics%20like%20ethnicity%20or%0Asexual%20orientation.%20A%20fundamental%20assumption%20in%20machine%20learning%20is%20the%0Aindependence%20of%20observations.%20However%2C%20this%20assumption%20often%20does%20not%20hold%20true%0Afor%20data%20describing%20social%20phenomena%2C%20where%20data%20points%20are%20often%20clustered%0Abased.%20Hence%2C%20if%20the%20machine%20learning%20models%20do%20not%20account%20for%20the%20cluster%0Acorrelations%2C%20the%20results%20may%20be%20biased.%20Especially%20high%20is%20the%20bias%20in%20cases%0Awhere%20the%20cluster%20assignment%20is%20correlated%20to%20the%20variable%20of%20interest.%20We%0Apresent%20a%20fair%20mixed%20effects%20support%20vector%20machine%20algorithm%20that%20can%20handle%0Aboth%20problems%20simultaneously.%20With%20a%20reproducible%20simulation%20study%20we%0Ademonstrate%20the%20impact%20of%20clustered%20data%20on%20the%20quality%20of%20fair%20machine%0Alearning%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06433v6&entry.124074799=Read"},
{"title": "Anomaly Detection in Medical Imaging -- A Mini Review", "author": "Maximilian E. Tschuchnig and Michael Gadermayr", "abstract": "  The increasing digitization of medical imaging enables machine learning based\nimprovements in detecting, visualizing and segmenting lesions, easing the\nworkload for medical experts. However, supervised machine learning requires\nreliable labelled data, which is is often difficult or impossible to collect or\nat least time consuming and thereby costly. Therefore methods requiring only\npartly labeled data (semi-supervised) or no labeling at all (unsupervised\nmethods) have been applied more regularly. Anomaly detection is one possible\nmethodology that is able to leverage semi-supervised and unsupervised methods\nto handle medical imaging tasks like classification and segmentation. This\npaper uses a semi-exhaustive literature review of relevant anomaly detection\npapers in medical imaging to cluster into applications, highlight important\nresults, establish lessons learned and give further advice on how to approach\nanomaly detection in medical imaging. The qualitative analysis is based on\ngoogle scholar and 4 different search terms, resulting in 120 different\nanalysed papers. The main results showed that the current research is mostly\nmotivated by reducing the need for labelled data. Also, the successful and\nsubstantial amount of research in the brain MRI domain shows the potential for\napplications in further domains like OCT and chest X-ray.\n", "link": "http://arxiv.org/abs/2108.11986v2", "date": "2024-12-02", "relevancy": 1.829, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4885}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4522}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anomaly%20Detection%20in%20Medical%20Imaging%20--%20A%20Mini%20Review&body=Title%3A%20Anomaly%20Detection%20in%20Medical%20Imaging%20--%20A%20Mini%20Review%0AAuthor%3A%20Maximilian%20E.%20Tschuchnig%20and%20Michael%20Gadermayr%0AAbstract%3A%20%20%20The%20increasing%20digitization%20of%20medical%20imaging%20enables%20machine%20learning%20based%0Aimprovements%20in%20detecting%2C%20visualizing%20and%20segmenting%20lesions%2C%20easing%20the%0Aworkload%20for%20medical%20experts.%20However%2C%20supervised%20machine%20learning%20requires%0Areliable%20labelled%20data%2C%20which%20is%20is%20often%20difficult%20or%20impossible%20to%20collect%20or%0Aat%20least%20time%20consuming%20and%20thereby%20costly.%20Therefore%20methods%20requiring%20only%0Apartly%20labeled%20data%20%28semi-supervised%29%20or%20no%20labeling%20at%20all%20%28unsupervised%0Amethods%29%20have%20been%20applied%20more%20regularly.%20Anomaly%20detection%20is%20one%20possible%0Amethodology%20that%20is%20able%20to%20leverage%20semi-supervised%20and%20unsupervised%20methods%0Ato%20handle%20medical%20imaging%20tasks%20like%20classification%20and%20segmentation.%20This%0Apaper%20uses%20a%20semi-exhaustive%20literature%20review%20of%20relevant%20anomaly%20detection%0Apapers%20in%20medical%20imaging%20to%20cluster%20into%20applications%2C%20highlight%20important%0Aresults%2C%20establish%20lessons%20learned%20and%20give%20further%20advice%20on%20how%20to%20approach%0Aanomaly%20detection%20in%20medical%20imaging.%20The%20qualitative%20analysis%20is%20based%20on%0Agoogle%20scholar%20and%204%20different%20search%20terms%2C%20resulting%20in%20120%20different%0Aanalysed%20papers.%20The%20main%20results%20showed%20that%20the%20current%20research%20is%20mostly%0Amotivated%20by%20reducing%20the%20need%20for%20labelled%20data.%20Also%2C%20the%20successful%20and%0Asubstantial%20amount%20of%20research%20in%20the%20brain%20MRI%20domain%20shows%20the%20potential%20for%0Aapplications%20in%20further%20domains%20like%20OCT%20and%20chest%20X-ray.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2108.11986v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnomaly%2520Detection%2520in%2520Medical%2520Imaging%2520--%2520A%2520Mini%2520Review%26entry.906535625%3DMaximilian%2520E.%2520Tschuchnig%2520and%2520Michael%2520Gadermayr%26entry.1292438233%3D%2520%2520The%2520increasing%2520digitization%2520of%2520medical%2520imaging%2520enables%2520machine%2520learning%2520based%250Aimprovements%2520in%2520detecting%252C%2520visualizing%2520and%2520segmenting%2520lesions%252C%2520easing%2520the%250Aworkload%2520for%2520medical%2520experts.%2520However%252C%2520supervised%2520machine%2520learning%2520requires%250Areliable%2520labelled%2520data%252C%2520which%2520is%2520is%2520often%2520difficult%2520or%2520impossible%2520to%2520collect%2520or%250Aat%2520least%2520time%2520consuming%2520and%2520thereby%2520costly.%2520Therefore%2520methods%2520requiring%2520only%250Apartly%2520labeled%2520data%2520%2528semi-supervised%2529%2520or%2520no%2520labeling%2520at%2520all%2520%2528unsupervised%250Amethods%2529%2520have%2520been%2520applied%2520more%2520regularly.%2520Anomaly%2520detection%2520is%2520one%2520possible%250Amethodology%2520that%2520is%2520able%2520to%2520leverage%2520semi-supervised%2520and%2520unsupervised%2520methods%250Ato%2520handle%2520medical%2520imaging%2520tasks%2520like%2520classification%2520and%2520segmentation.%2520This%250Apaper%2520uses%2520a%2520semi-exhaustive%2520literature%2520review%2520of%2520relevant%2520anomaly%2520detection%250Apapers%2520in%2520medical%2520imaging%2520to%2520cluster%2520into%2520applications%252C%2520highlight%2520important%250Aresults%252C%2520establish%2520lessons%2520learned%2520and%2520give%2520further%2520advice%2520on%2520how%2520to%2520approach%250Aanomaly%2520detection%2520in%2520medical%2520imaging.%2520The%2520qualitative%2520analysis%2520is%2520based%2520on%250Agoogle%2520scholar%2520and%25204%2520different%2520search%2520terms%252C%2520resulting%2520in%2520120%2520different%250Aanalysed%2520papers.%2520The%2520main%2520results%2520showed%2520that%2520the%2520current%2520research%2520is%2520mostly%250Amotivated%2520by%2520reducing%2520the%2520need%2520for%2520labelled%2520data.%2520Also%252C%2520the%2520successful%2520and%250Asubstantial%2520amount%2520of%2520research%2520in%2520the%2520brain%2520MRI%2520domain%2520shows%2520the%2520potential%2520for%250Aapplications%2520in%2520further%2520domains%2520like%2520OCT%2520and%2520chest%2520X-ray.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2108.11986v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anomaly%20Detection%20in%20Medical%20Imaging%20--%20A%20Mini%20Review&entry.906535625=Maximilian%20E.%20Tschuchnig%20and%20Michael%20Gadermayr&entry.1292438233=%20%20The%20increasing%20digitization%20of%20medical%20imaging%20enables%20machine%20learning%20based%0Aimprovements%20in%20detecting%2C%20visualizing%20and%20segmenting%20lesions%2C%20easing%20the%0Aworkload%20for%20medical%20experts.%20However%2C%20supervised%20machine%20learning%20requires%0Areliable%20labelled%20data%2C%20which%20is%20is%20often%20difficult%20or%20impossible%20to%20collect%20or%0Aat%20least%20time%20consuming%20and%20thereby%20costly.%20Therefore%20methods%20requiring%20only%0Apartly%20labeled%20data%20%28semi-supervised%29%20or%20no%20labeling%20at%20all%20%28unsupervised%0Amethods%29%20have%20been%20applied%20more%20regularly.%20Anomaly%20detection%20is%20one%20possible%0Amethodology%20that%20is%20able%20to%20leverage%20semi-supervised%20and%20unsupervised%20methods%0Ato%20handle%20medical%20imaging%20tasks%20like%20classification%20and%20segmentation.%20This%0Apaper%20uses%20a%20semi-exhaustive%20literature%20review%20of%20relevant%20anomaly%20detection%0Apapers%20in%20medical%20imaging%20to%20cluster%20into%20applications%2C%20highlight%20important%0Aresults%2C%20establish%20lessons%20learned%20and%20give%20further%20advice%20on%20how%20to%20approach%0Aanomaly%20detection%20in%20medical%20imaging.%20The%20qualitative%20analysis%20is%20based%20on%0Agoogle%20scholar%20and%204%20different%20search%20terms%2C%20resulting%20in%20120%20different%0Aanalysed%20papers.%20The%20main%20results%20showed%20that%20the%20current%20research%20is%20mostly%0Amotivated%20by%20reducing%20the%20need%20for%20labelled%20data.%20Also%2C%20the%20successful%20and%0Asubstantial%20amount%20of%20research%20in%20the%20brain%20MRI%20domain%20shows%20the%20potential%20for%0Aapplications%20in%20further%20domains%20like%20OCT%20and%20chest%20X-ray.%0A&entry.1838667208=http%3A//arxiv.org/abs/2108.11986v2&entry.124074799=Read"},
{"title": "Structure-Aware Human Body Reshaping with Adaptive Affinity-Graph\n  Network", "author": "Qiwen Deng and Yangcen Liu and Wen Li and Guoqing Wang", "abstract": "  Given a source portrait, the automatic human body reshaping task aims at\nediting it to an aesthetic body shape. As the technology has been widely used\nin media, several methods have been proposed mainly focusing on generating\noptical flow to warp the body shape. However, those previous works only\nconsider the local transformation of different body parts (arms, torso, and\nlegs), ignoring the global affinity, and limiting the capacity to ensure\nconsistency and quality across the entire body. In this paper, we propose a\nnovel Adaptive Affinity-Graph Network (AAGN), which extracts the global\naffinity between different body parts to enhance the quality of the generated\noptical flow. Specifically, our AAGN primarily introduces the following\ndesigns: (1) we propose an Adaptive Affinity-Graph (AAG) Block that leverages\nthe characteristic of a fully connected graph. AAG represents different body\nparts as nodes in an adaptive fully connected graph and captures all the\naffinities between nodes to obtain a global affinity map. The design could\nbetter improve the consistency between body parts. (2) Besides, for\nhigh-frequency details are crucial for photo aesthetics, a Body Shape\nDiscriminator (BSD) is designed to extract information from both high-frequency\nand spatial domain. Particularly, an SRM filter is utilized to extract\nhigh-frequency details, which are combined with spatial features as input to\nthe BSD. With this design, BSD guides the Flow Generator (FG) to pay attention\nto various fine details rather than rigid pixel-level fitting. Extensive\nexperiments conducted on the BR-5K dataset demonstrate that our framework\nsignificantly enhances the aesthetic appeal of reshaped photos, surpassing all\nprevious work to achieve state-of-the-art in all evaluation metrics.\n", "link": "http://arxiv.org/abs/2404.13983v2", "date": "2024-12-02", "relevancy": 1.8126, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6273}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5934}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure-Aware%20Human%20Body%20Reshaping%20with%20Adaptive%20Affinity-Graph%0A%20%20Network&body=Title%3A%20Structure-Aware%20Human%20Body%20Reshaping%20with%20Adaptive%20Affinity-Graph%0A%20%20Network%0AAuthor%3A%20Qiwen%20Deng%20and%20Yangcen%20Liu%20and%20Wen%20Li%20and%20Guoqing%20Wang%0AAbstract%3A%20%20%20Given%20a%20source%20portrait%2C%20the%20automatic%20human%20body%20reshaping%20task%20aims%20at%0Aediting%20it%20to%20an%20aesthetic%20body%20shape.%20As%20the%20technology%20has%20been%20widely%20used%0Ain%20media%2C%20several%20methods%20have%20been%20proposed%20mainly%20focusing%20on%20generating%0Aoptical%20flow%20to%20warp%20the%20body%20shape.%20However%2C%20those%20previous%20works%20only%0Aconsider%20the%20local%20transformation%20of%20different%20body%20parts%20%28arms%2C%20torso%2C%20and%0Alegs%29%2C%20ignoring%20the%20global%20affinity%2C%20and%20limiting%20the%20capacity%20to%20ensure%0Aconsistency%20and%20quality%20across%20the%20entire%20body.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20Adaptive%20Affinity-Graph%20Network%20%28AAGN%29%2C%20which%20extracts%20the%20global%0Aaffinity%20between%20different%20body%20parts%20to%20enhance%20the%20quality%20of%20the%20generated%0Aoptical%20flow.%20Specifically%2C%20our%20AAGN%20primarily%20introduces%20the%20following%0Adesigns%3A%20%281%29%20we%20propose%20an%20Adaptive%20Affinity-Graph%20%28AAG%29%20Block%20that%20leverages%0Athe%20characteristic%20of%20a%20fully%20connected%20graph.%20AAG%20represents%20different%20body%0Aparts%20as%20nodes%20in%20an%20adaptive%20fully%20connected%20graph%20and%20captures%20all%20the%0Aaffinities%20between%20nodes%20to%20obtain%20a%20global%20affinity%20map.%20The%20design%20could%0Abetter%20improve%20the%20consistency%20between%20body%20parts.%20%282%29%20Besides%2C%20for%0Ahigh-frequency%20details%20are%20crucial%20for%20photo%20aesthetics%2C%20a%20Body%20Shape%0ADiscriminator%20%28BSD%29%20is%20designed%20to%20extract%20information%20from%20both%20high-frequency%0Aand%20spatial%20domain.%20Particularly%2C%20an%20SRM%20filter%20is%20utilized%20to%20extract%0Ahigh-frequency%20details%2C%20which%20are%20combined%20with%20spatial%20features%20as%20input%20to%0Athe%20BSD.%20With%20this%20design%2C%20BSD%20guides%20the%20Flow%20Generator%20%28FG%29%20to%20pay%20attention%0Ato%20various%20fine%20details%20rather%20than%20rigid%20pixel-level%20fitting.%20Extensive%0Aexperiments%20conducted%20on%20the%20BR-5K%20dataset%20demonstrate%20that%20our%20framework%0Asignificantly%20enhances%20the%20aesthetic%20appeal%20of%20reshaped%20photos%2C%20surpassing%20all%0Aprevious%20work%20to%20achieve%20state-of-the-art%20in%20all%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13983v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure-Aware%2520Human%2520Body%2520Reshaping%2520with%2520Adaptive%2520Affinity-Graph%250A%2520%2520Network%26entry.906535625%3DQiwen%2520Deng%2520and%2520Yangcen%2520Liu%2520and%2520Wen%2520Li%2520and%2520Guoqing%2520Wang%26entry.1292438233%3D%2520%2520Given%2520a%2520source%2520portrait%252C%2520the%2520automatic%2520human%2520body%2520reshaping%2520task%2520aims%2520at%250Aediting%2520it%2520to%2520an%2520aesthetic%2520body%2520shape.%2520As%2520the%2520technology%2520has%2520been%2520widely%2520used%250Ain%2520media%252C%2520several%2520methods%2520have%2520been%2520proposed%2520mainly%2520focusing%2520on%2520generating%250Aoptical%2520flow%2520to%2520warp%2520the%2520body%2520shape.%2520However%252C%2520those%2520previous%2520works%2520only%250Aconsider%2520the%2520local%2520transformation%2520of%2520different%2520body%2520parts%2520%2528arms%252C%2520torso%252C%2520and%250Alegs%2529%252C%2520ignoring%2520the%2520global%2520affinity%252C%2520and%2520limiting%2520the%2520capacity%2520to%2520ensure%250Aconsistency%2520and%2520quality%2520across%2520the%2520entire%2520body.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520Adaptive%2520Affinity-Graph%2520Network%2520%2528AAGN%2529%252C%2520which%2520extracts%2520the%2520global%250Aaffinity%2520between%2520different%2520body%2520parts%2520to%2520enhance%2520the%2520quality%2520of%2520the%2520generated%250Aoptical%2520flow.%2520Specifically%252C%2520our%2520AAGN%2520primarily%2520introduces%2520the%2520following%250Adesigns%253A%2520%25281%2529%2520we%2520propose%2520an%2520Adaptive%2520Affinity-Graph%2520%2528AAG%2529%2520Block%2520that%2520leverages%250Athe%2520characteristic%2520of%2520a%2520fully%2520connected%2520graph.%2520AAG%2520represents%2520different%2520body%250Aparts%2520as%2520nodes%2520in%2520an%2520adaptive%2520fully%2520connected%2520graph%2520and%2520captures%2520all%2520the%250Aaffinities%2520between%2520nodes%2520to%2520obtain%2520a%2520global%2520affinity%2520map.%2520The%2520design%2520could%250Abetter%2520improve%2520the%2520consistency%2520between%2520body%2520parts.%2520%25282%2529%2520Besides%252C%2520for%250Ahigh-frequency%2520details%2520are%2520crucial%2520for%2520photo%2520aesthetics%252C%2520a%2520Body%2520Shape%250ADiscriminator%2520%2528BSD%2529%2520is%2520designed%2520to%2520extract%2520information%2520from%2520both%2520high-frequency%250Aand%2520spatial%2520domain.%2520Particularly%252C%2520an%2520SRM%2520filter%2520is%2520utilized%2520to%2520extract%250Ahigh-frequency%2520details%252C%2520which%2520are%2520combined%2520with%2520spatial%2520features%2520as%2520input%2520to%250Athe%2520BSD.%2520With%2520this%2520design%252C%2520BSD%2520guides%2520the%2520Flow%2520Generator%2520%2528FG%2529%2520to%2520pay%2520attention%250Ato%2520various%2520fine%2520details%2520rather%2520than%2520rigid%2520pixel-level%2520fitting.%2520Extensive%250Aexperiments%2520conducted%2520on%2520the%2520BR-5K%2520dataset%2520demonstrate%2520that%2520our%2520framework%250Asignificantly%2520enhances%2520the%2520aesthetic%2520appeal%2520of%2520reshaped%2520photos%252C%2520surpassing%2520all%250Aprevious%2520work%2520to%2520achieve%2520state-of-the-art%2520in%2520all%2520evaluation%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.13983v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-Aware%20Human%20Body%20Reshaping%20with%20Adaptive%20Affinity-Graph%0A%20%20Network&entry.906535625=Qiwen%20Deng%20and%20Yangcen%20Liu%20and%20Wen%20Li%20and%20Guoqing%20Wang&entry.1292438233=%20%20Given%20a%20source%20portrait%2C%20the%20automatic%20human%20body%20reshaping%20task%20aims%20at%0Aediting%20it%20to%20an%20aesthetic%20body%20shape.%20As%20the%20technology%20has%20been%20widely%20used%0Ain%20media%2C%20several%20methods%20have%20been%20proposed%20mainly%20focusing%20on%20generating%0Aoptical%20flow%20to%20warp%20the%20body%20shape.%20However%2C%20those%20previous%20works%20only%0Aconsider%20the%20local%20transformation%20of%20different%20body%20parts%20%28arms%2C%20torso%2C%20and%0Alegs%29%2C%20ignoring%20the%20global%20affinity%2C%20and%20limiting%20the%20capacity%20to%20ensure%0Aconsistency%20and%20quality%20across%20the%20entire%20body.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20Adaptive%20Affinity-Graph%20Network%20%28AAGN%29%2C%20which%20extracts%20the%20global%0Aaffinity%20between%20different%20body%20parts%20to%20enhance%20the%20quality%20of%20the%20generated%0Aoptical%20flow.%20Specifically%2C%20our%20AAGN%20primarily%20introduces%20the%20following%0Adesigns%3A%20%281%29%20we%20propose%20an%20Adaptive%20Affinity-Graph%20%28AAG%29%20Block%20that%20leverages%0Athe%20characteristic%20of%20a%20fully%20connected%20graph.%20AAG%20represents%20different%20body%0Aparts%20as%20nodes%20in%20an%20adaptive%20fully%20connected%20graph%20and%20captures%20all%20the%0Aaffinities%20between%20nodes%20to%20obtain%20a%20global%20affinity%20map.%20The%20design%20could%0Abetter%20improve%20the%20consistency%20between%20body%20parts.%20%282%29%20Besides%2C%20for%0Ahigh-frequency%20details%20are%20crucial%20for%20photo%20aesthetics%2C%20a%20Body%20Shape%0ADiscriminator%20%28BSD%29%20is%20designed%20to%20extract%20information%20from%20both%20high-frequency%0Aand%20spatial%20domain.%20Particularly%2C%20an%20SRM%20filter%20is%20utilized%20to%20extract%0Ahigh-frequency%20details%2C%20which%20are%20combined%20with%20spatial%20features%20as%20input%20to%0Athe%20BSD.%20With%20this%20design%2C%20BSD%20guides%20the%20Flow%20Generator%20%28FG%29%20to%20pay%20attention%0Ato%20various%20fine%20details%20rather%20than%20rigid%20pixel-level%20fitting.%20Extensive%0Aexperiments%20conducted%20on%20the%20BR-5K%20dataset%20demonstrate%20that%20our%20framework%0Asignificantly%20enhances%20the%20aesthetic%20appeal%20of%20reshaped%20photos%2C%20surpassing%20all%0Aprevious%20work%20to%20achieve%20state-of-the-art%20in%20all%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13983v2&entry.124074799=Read"},
{"title": "Limits to Predicting Online Speech Using Large Language Models", "author": "Mina Remeli and Moritz Hardt and Robert C. Williamson", "abstract": "  We study the predictability of online speech on social media, and whether\npredictability improves with information outside a user's own posts. Recent\ntheoretical results suggest that posts from a user's social circle are as\npredictive of the user's future posts as that of the user's past posts.\nMotivated by the success of large language models, we empirically test this\nhypothesis. We define predictability as a measure of the model's uncertainty,\ni.e., its negative log-likelihood on future tokens given context. As the basis\nof our study, we collect 10M tweets for ``tweet-tuning'' base models and a\nfurther 6.25M posts from more than five thousand X (previously Twitter) users\nand their peers. Across four large language models ranging in size from 1.5\nbillion to 70 billion parameters, we find that predicting a user's posts from\ntheir peers' posts performs poorly. Moreover, the value of the user's own posts\nfor prediction is consistently higher than that of their peers'. We extend our\ninvestigation with a detailed analysis on what's learned in-context and the\nrobustness of our findings. From context, base models learn to correctly\npredict @-mentions and hashtags. Moreover, our results replicate if instead of\nprompting the model with additional context, we finetune on it. Across the\nboard, we find that predicting the posts of individual users remains hard.\n", "link": "http://arxiv.org/abs/2407.12850v2", "date": "2024-12-02", "relevancy": 1.7912, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4531}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4468}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Limits%20to%20Predicting%20Online%20Speech%20Using%20Large%20Language%20Models&body=Title%3A%20Limits%20to%20Predicting%20Online%20Speech%20Using%20Large%20Language%20Models%0AAuthor%3A%20Mina%20Remeli%20and%20Moritz%20Hardt%20and%20Robert%20C.%20Williamson%0AAbstract%3A%20%20%20We%20study%20the%20predictability%20of%20online%20speech%20on%20social%20media%2C%20and%20whether%0Apredictability%20improves%20with%20information%20outside%20a%20user%27s%20own%20posts.%20Recent%0Atheoretical%20results%20suggest%20that%20posts%20from%20a%20user%27s%20social%20circle%20are%20as%0Apredictive%20of%20the%20user%27s%20future%20posts%20as%20that%20of%20the%20user%27s%20past%20posts.%0AMotivated%20by%20the%20success%20of%20large%20language%20models%2C%20we%20empirically%20test%20this%0Ahypothesis.%20We%20define%20predictability%20as%20a%20measure%20of%20the%20model%27s%20uncertainty%2C%0Ai.e.%2C%20its%20negative%20log-likelihood%20on%20future%20tokens%20given%20context.%20As%20the%20basis%0Aof%20our%20study%2C%20we%20collect%2010M%20tweets%20for%20%60%60tweet-tuning%27%27%20base%20models%20and%20a%0Afurther%206.25M%20posts%20from%20more%20than%20five%20thousand%20X%20%28previously%20Twitter%29%20users%0Aand%20their%20peers.%20Across%20four%20large%20language%20models%20ranging%20in%20size%20from%201.5%0Abillion%20to%2070%20billion%20parameters%2C%20we%20find%20that%20predicting%20a%20user%27s%20posts%20from%0Atheir%20peers%27%20posts%20performs%20poorly.%20Moreover%2C%20the%20value%20of%20the%20user%27s%20own%20posts%0Afor%20prediction%20is%20consistently%20higher%20than%20that%20of%20their%20peers%27.%20We%20extend%20our%0Ainvestigation%20with%20a%20detailed%20analysis%20on%20what%27s%20learned%20in-context%20and%20the%0Arobustness%20of%20our%20findings.%20From%20context%2C%20base%20models%20learn%20to%20correctly%0Apredict%20%40-mentions%20and%20hashtags.%20Moreover%2C%20our%20results%20replicate%20if%20instead%20of%0Aprompting%20the%20model%20with%20additional%20context%2C%20we%20finetune%20on%20it.%20Across%20the%0Aboard%2C%20we%20find%20that%20predicting%20the%20posts%20of%20individual%20users%20remains%20hard.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12850v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLimits%2520to%2520Predicting%2520Online%2520Speech%2520Using%2520Large%2520Language%2520Models%26entry.906535625%3DMina%2520Remeli%2520and%2520Moritz%2520Hardt%2520and%2520Robert%2520C.%2520Williamson%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520predictability%2520of%2520online%2520speech%2520on%2520social%2520media%252C%2520and%2520whether%250Apredictability%2520improves%2520with%2520information%2520outside%2520a%2520user%2527s%2520own%2520posts.%2520Recent%250Atheoretical%2520results%2520suggest%2520that%2520posts%2520from%2520a%2520user%2527s%2520social%2520circle%2520are%2520as%250Apredictive%2520of%2520the%2520user%2527s%2520future%2520posts%2520as%2520that%2520of%2520the%2520user%2527s%2520past%2520posts.%250AMotivated%2520by%2520the%2520success%2520of%2520large%2520language%2520models%252C%2520we%2520empirically%2520test%2520this%250Ahypothesis.%2520We%2520define%2520predictability%2520as%2520a%2520measure%2520of%2520the%2520model%2527s%2520uncertainty%252C%250Ai.e.%252C%2520its%2520negative%2520log-likelihood%2520on%2520future%2520tokens%2520given%2520context.%2520As%2520the%2520basis%250Aof%2520our%2520study%252C%2520we%2520collect%252010M%2520tweets%2520for%2520%2560%2560tweet-tuning%2527%2527%2520base%2520models%2520and%2520a%250Afurther%25206.25M%2520posts%2520from%2520more%2520than%2520five%2520thousand%2520X%2520%2528previously%2520Twitter%2529%2520users%250Aand%2520their%2520peers.%2520Across%2520four%2520large%2520language%2520models%2520ranging%2520in%2520size%2520from%25201.5%250Abillion%2520to%252070%2520billion%2520parameters%252C%2520we%2520find%2520that%2520predicting%2520a%2520user%2527s%2520posts%2520from%250Atheir%2520peers%2527%2520posts%2520performs%2520poorly.%2520Moreover%252C%2520the%2520value%2520of%2520the%2520user%2527s%2520own%2520posts%250Afor%2520prediction%2520is%2520consistently%2520higher%2520than%2520that%2520of%2520their%2520peers%2527.%2520We%2520extend%2520our%250Ainvestigation%2520with%2520a%2520detailed%2520analysis%2520on%2520what%2527s%2520learned%2520in-context%2520and%2520the%250Arobustness%2520of%2520our%2520findings.%2520From%2520context%252C%2520base%2520models%2520learn%2520to%2520correctly%250Apredict%2520%2540-mentions%2520and%2520hashtags.%2520Moreover%252C%2520our%2520results%2520replicate%2520if%2520instead%2520of%250Aprompting%2520the%2520model%2520with%2520additional%2520context%252C%2520we%2520finetune%2520on%2520it.%2520Across%2520the%250Aboard%252C%2520we%2520find%2520that%2520predicting%2520the%2520posts%2520of%2520individual%2520users%2520remains%2520hard.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12850v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Limits%20to%20Predicting%20Online%20Speech%20Using%20Large%20Language%20Models&entry.906535625=Mina%20Remeli%20and%20Moritz%20Hardt%20and%20Robert%20C.%20Williamson&entry.1292438233=%20%20We%20study%20the%20predictability%20of%20online%20speech%20on%20social%20media%2C%20and%20whether%0Apredictability%20improves%20with%20information%20outside%20a%20user%27s%20own%20posts.%20Recent%0Atheoretical%20results%20suggest%20that%20posts%20from%20a%20user%27s%20social%20circle%20are%20as%0Apredictive%20of%20the%20user%27s%20future%20posts%20as%20that%20of%20the%20user%27s%20past%20posts.%0AMotivated%20by%20the%20success%20of%20large%20language%20models%2C%20we%20empirically%20test%20this%0Ahypothesis.%20We%20define%20predictability%20as%20a%20measure%20of%20the%20model%27s%20uncertainty%2C%0Ai.e.%2C%20its%20negative%20log-likelihood%20on%20future%20tokens%20given%20context.%20As%20the%20basis%0Aof%20our%20study%2C%20we%20collect%2010M%20tweets%20for%20%60%60tweet-tuning%27%27%20base%20models%20and%20a%0Afurther%206.25M%20posts%20from%20more%20than%20five%20thousand%20X%20%28previously%20Twitter%29%20users%0Aand%20their%20peers.%20Across%20four%20large%20language%20models%20ranging%20in%20size%20from%201.5%0Abillion%20to%2070%20billion%20parameters%2C%20we%20find%20that%20predicting%20a%20user%27s%20posts%20from%0Atheir%20peers%27%20posts%20performs%20poorly.%20Moreover%2C%20the%20value%20of%20the%20user%27s%20own%20posts%0Afor%20prediction%20is%20consistently%20higher%20than%20that%20of%20their%20peers%27.%20We%20extend%20our%0Ainvestigation%20with%20a%20detailed%20analysis%20on%20what%27s%20learned%20in-context%20and%20the%0Arobustness%20of%20our%20findings.%20From%20context%2C%20base%20models%20learn%20to%20correctly%0Apredict%20%40-mentions%20and%20hashtags.%20Moreover%2C%20our%20results%20replicate%20if%20instead%20of%0Aprompting%20the%20model%20with%20additional%20context%2C%20we%20finetune%20on%20it.%20Across%20the%0Aboard%2C%20we%20find%20that%20predicting%20the%20posts%20of%20individual%20users%20remains%20hard.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12850v2&entry.124074799=Read"},
{"title": "OminiControl: Minimal and Universal Control for Diffusion Transformer", "author": "Zhenxiong Tan and Songhua Liu and Xingyi Yang and Qiaochu Xue and Xinchao Wang", "abstract": "  In this paper, we introduce OminiControl, a highly versatile and\nparameter-efficient framework that integrates image conditions into pre-trained\nDiffusion Transformer (DiT) models. At its core, OminiControl leverages a\nparameter reuse mechanism, enabling the DiT to encode image conditions using\nitself as a powerful backbone and process them with its flexible multi-modal\nattention processors. Unlike existing methods, which rely heavily on additional\nencoder modules with complex architectures, OminiControl (1) effectively and\nefficiently incorporates injected image conditions with only ~0.1% additional\nparameters, and (2) addresses a wide range of image conditioning tasks in a\nunified manner, including subject-driven generation and spatially-aligned\nconditions such as edges, depth, and more. Remarkably, these capabilities are\nachieved by training on images generated by the DiT itself, which is\nparticularly beneficial for subject-driven generation. Extensive evaluations\ndemonstrate that OminiControl outperforms existing UNet-based and DiT-adapted\nmodels in both subject-driven and spatially-aligned conditional generation.\nAdditionally, we release our training dataset, Subjects200K, a diverse\ncollection of over 200,000 identity-consistent images, along with an efficient\ndata synthesis pipeline to advance research in subject-consistent generation.\n", "link": "http://arxiv.org/abs/2411.15098v3", "date": "2024-12-02", "relevancy": 1.7639, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6327}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.577}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OminiControl%3A%20Minimal%20and%20Universal%20Control%20for%20Diffusion%20Transformer&body=Title%3A%20OminiControl%3A%20Minimal%20and%20Universal%20Control%20for%20Diffusion%20Transformer%0AAuthor%3A%20Zhenxiong%20Tan%20and%20Songhua%20Liu%20and%20Xingyi%20Yang%20and%20Qiaochu%20Xue%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20OminiControl%2C%20a%20highly%20versatile%20and%0Aparameter-efficient%20framework%20that%20integrates%20image%20conditions%20into%20pre-trained%0ADiffusion%20Transformer%20%28DiT%29%20models.%20At%20its%20core%2C%20OminiControl%20leverages%20a%0Aparameter%20reuse%20mechanism%2C%20enabling%20the%20DiT%20to%20encode%20image%20conditions%20using%0Aitself%20as%20a%20powerful%20backbone%20and%20process%20them%20with%20its%20flexible%20multi-modal%0Aattention%20processors.%20Unlike%20existing%20methods%2C%20which%20rely%20heavily%20on%20additional%0Aencoder%20modules%20with%20complex%20architectures%2C%20OminiControl%20%281%29%20effectively%20and%0Aefficiently%20incorporates%20injected%20image%20conditions%20with%20only%20~0.1%25%20additional%0Aparameters%2C%20and%20%282%29%20addresses%20a%20wide%20range%20of%20image%20conditioning%20tasks%20in%20a%0Aunified%20manner%2C%20including%20subject-driven%20generation%20and%20spatially-aligned%0Aconditions%20such%20as%20edges%2C%20depth%2C%20and%20more.%20Remarkably%2C%20these%20capabilities%20are%0Aachieved%20by%20training%20on%20images%20generated%20by%20the%20DiT%20itself%2C%20which%20is%0Aparticularly%20beneficial%20for%20subject-driven%20generation.%20Extensive%20evaluations%0Ademonstrate%20that%20OminiControl%20outperforms%20existing%20UNet-based%20and%20DiT-adapted%0Amodels%20in%20both%20subject-driven%20and%20spatially-aligned%20conditional%20generation.%0AAdditionally%2C%20we%20release%20our%20training%20dataset%2C%20Subjects200K%2C%20a%20diverse%0Acollection%20of%20over%20200%2C000%20identity-consistent%20images%2C%20along%20with%20an%20efficient%0Adata%20synthesis%20pipeline%20to%20advance%20research%20in%20subject-consistent%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15098v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOminiControl%253A%2520Minimal%2520and%2520Universal%2520Control%2520for%2520Diffusion%2520Transformer%26entry.906535625%3DZhenxiong%2520Tan%2520and%2520Songhua%2520Liu%2520and%2520Xingyi%2520Yang%2520and%2520Qiaochu%2520Xue%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520OminiControl%252C%2520a%2520highly%2520versatile%2520and%250Aparameter-efficient%2520framework%2520that%2520integrates%2520image%2520conditions%2520into%2520pre-trained%250ADiffusion%2520Transformer%2520%2528DiT%2529%2520models.%2520At%2520its%2520core%252C%2520OminiControl%2520leverages%2520a%250Aparameter%2520reuse%2520mechanism%252C%2520enabling%2520the%2520DiT%2520to%2520encode%2520image%2520conditions%2520using%250Aitself%2520as%2520a%2520powerful%2520backbone%2520and%2520process%2520them%2520with%2520its%2520flexible%2520multi-modal%250Aattention%2520processors.%2520Unlike%2520existing%2520methods%252C%2520which%2520rely%2520heavily%2520on%2520additional%250Aencoder%2520modules%2520with%2520complex%2520architectures%252C%2520OminiControl%2520%25281%2529%2520effectively%2520and%250Aefficiently%2520incorporates%2520injected%2520image%2520conditions%2520with%2520only%2520~0.1%2525%2520additional%250Aparameters%252C%2520and%2520%25282%2529%2520addresses%2520a%2520wide%2520range%2520of%2520image%2520conditioning%2520tasks%2520in%2520a%250Aunified%2520manner%252C%2520including%2520subject-driven%2520generation%2520and%2520spatially-aligned%250Aconditions%2520such%2520as%2520edges%252C%2520depth%252C%2520and%2520more.%2520Remarkably%252C%2520these%2520capabilities%2520are%250Aachieved%2520by%2520training%2520on%2520images%2520generated%2520by%2520the%2520DiT%2520itself%252C%2520which%2520is%250Aparticularly%2520beneficial%2520for%2520subject-driven%2520generation.%2520Extensive%2520evaluations%250Ademonstrate%2520that%2520OminiControl%2520outperforms%2520existing%2520UNet-based%2520and%2520DiT-adapted%250Amodels%2520in%2520both%2520subject-driven%2520and%2520spatially-aligned%2520conditional%2520generation.%250AAdditionally%252C%2520we%2520release%2520our%2520training%2520dataset%252C%2520Subjects200K%252C%2520a%2520diverse%250Acollection%2520of%2520over%2520200%252C000%2520identity-consistent%2520images%252C%2520along%2520with%2520an%2520efficient%250Adata%2520synthesis%2520pipeline%2520to%2520advance%2520research%2520in%2520subject-consistent%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15098v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OminiControl%3A%20Minimal%20and%20Universal%20Control%20for%20Diffusion%20Transformer&entry.906535625=Zhenxiong%20Tan%20and%20Songhua%20Liu%20and%20Xingyi%20Yang%20and%20Qiaochu%20Xue%20and%20Xinchao%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20OminiControl%2C%20a%20highly%20versatile%20and%0Aparameter-efficient%20framework%20that%20integrates%20image%20conditions%20into%20pre-trained%0ADiffusion%20Transformer%20%28DiT%29%20models.%20At%20its%20core%2C%20OminiControl%20leverages%20a%0Aparameter%20reuse%20mechanism%2C%20enabling%20the%20DiT%20to%20encode%20image%20conditions%20using%0Aitself%20as%20a%20powerful%20backbone%20and%20process%20them%20with%20its%20flexible%20multi-modal%0Aattention%20processors.%20Unlike%20existing%20methods%2C%20which%20rely%20heavily%20on%20additional%0Aencoder%20modules%20with%20complex%20architectures%2C%20OminiControl%20%281%29%20effectively%20and%0Aefficiently%20incorporates%20injected%20image%20conditions%20with%20only%20~0.1%25%20additional%0Aparameters%2C%20and%20%282%29%20addresses%20a%20wide%20range%20of%20image%20conditioning%20tasks%20in%20a%0Aunified%20manner%2C%20including%20subject-driven%20generation%20and%20spatially-aligned%0Aconditions%20such%20as%20edges%2C%20depth%2C%20and%20more.%20Remarkably%2C%20these%20capabilities%20are%0Aachieved%20by%20training%20on%20images%20generated%20by%20the%20DiT%20itself%2C%20which%20is%0Aparticularly%20beneficial%20for%20subject-driven%20generation.%20Extensive%20evaluations%0Ademonstrate%20that%20OminiControl%20outperforms%20existing%20UNet-based%20and%20DiT-adapted%0Amodels%20in%20both%20subject-driven%20and%20spatially-aligned%20conditional%20generation.%0AAdditionally%2C%20we%20release%20our%20training%20dataset%2C%20Subjects200K%2C%20a%20diverse%0Acollection%20of%20over%20200%2C000%20identity-consistent%20images%2C%20along%20with%20an%20efficient%0Adata%20synthesis%20pipeline%20to%20advance%20research%20in%20subject-consistent%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15098v3&entry.124074799=Read"},
{"title": "Anticipating Object State Changes in Long Procedural Videos", "author": "Victoria Manousaki and Konstantinos Bacharidis and Filippos Gouidis and Konstantinos Papoutsakis and Dimitris Plexousakis and Antonis Argyros", "abstract": "  In this work, we introduce (a) the new problem of anticipating object state\nchanges in images and videos during procedural activities, (b) new curated\nannotation data for object state change classification based on the Ego4D\ndataset, and (c) the first method for addressing this challenging problem.\nSolutions to this new task have important implications in vision-based scene\nunderstanding, automated monitoring systems, and action planning. The proposed\nnovel framework predicts object state changes that will occur in the near\nfuture due to yet unseen human actions by integrating learned visual features\nthat represent recent visual information with natural language (NLP) features\nthat represent past object state changes and actions. Leveraging the extensive\nand challenging Ego4D dataset which provides a large-scale collection of\nfirst-person perspective videos across numerous interaction scenarios, we\nintroduce an extension noted Ego4D-OSCA that provides new curated annotation\ndata for the object state change anticipation task (OSCA). An extensive\nexperimental evaluation is presented demonstrating the proposed method's\nefficacy in predicting object state changes in dynamic scenarios. The\nperformance of the proposed approach also underscores the potential of\nintegrating video and linguistic cues to enhance the predictive performance of\nvideo understanding systems and lays the groundwork for future research on the\nnew task of object state change anticipation. The source code and the new\nannotation data (Ego4D-OSCA) will be made publicly available.\n", "link": "http://arxiv.org/abs/2405.12789v3", "date": "2024-12-02", "relevancy": 1.7543, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6022}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5899}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anticipating%20Object%20State%20Changes%20in%20Long%20Procedural%20Videos&body=Title%3A%20Anticipating%20Object%20State%20Changes%20in%20Long%20Procedural%20Videos%0AAuthor%3A%20Victoria%20Manousaki%20and%20Konstantinos%20Bacharidis%20and%20Filippos%20Gouidis%20and%20Konstantinos%20Papoutsakis%20and%20Dimitris%20Plexousakis%20and%20Antonis%20Argyros%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20%28a%29%20the%20new%20problem%20of%20anticipating%20object%20state%0Achanges%20in%20images%20and%20videos%20during%20procedural%20activities%2C%20%28b%29%20new%20curated%0Aannotation%20data%20for%20object%20state%20change%20classification%20based%20on%20the%20Ego4D%0Adataset%2C%20and%20%28c%29%20the%20first%20method%20for%20addressing%20this%20challenging%20problem.%0ASolutions%20to%20this%20new%20task%20have%20important%20implications%20in%20vision-based%20scene%0Aunderstanding%2C%20automated%20monitoring%20systems%2C%20and%20action%20planning.%20The%20proposed%0Anovel%20framework%20predicts%20object%20state%20changes%20that%20will%20occur%20in%20the%20near%0Afuture%20due%20to%20yet%20unseen%20human%20actions%20by%20integrating%20learned%20visual%20features%0Athat%20represent%20recent%20visual%20information%20with%20natural%20language%20%28NLP%29%20features%0Athat%20represent%20past%20object%20state%20changes%20and%20actions.%20Leveraging%20the%20extensive%0Aand%20challenging%20Ego4D%20dataset%20which%20provides%20a%20large-scale%20collection%20of%0Afirst-person%20perspective%20videos%20across%20numerous%20interaction%20scenarios%2C%20we%0Aintroduce%20an%20extension%20noted%20Ego4D-OSCA%20that%20provides%20new%20curated%20annotation%0Adata%20for%20the%20object%20state%20change%20anticipation%20task%20%28OSCA%29.%20An%20extensive%0Aexperimental%20evaluation%20is%20presented%20demonstrating%20the%20proposed%20method%27s%0Aefficacy%20in%20predicting%20object%20state%20changes%20in%20dynamic%20scenarios.%20The%0Aperformance%20of%20the%20proposed%20approach%20also%20underscores%20the%20potential%20of%0Aintegrating%20video%20and%20linguistic%20cues%20to%20enhance%20the%20predictive%20performance%20of%0Avideo%20understanding%20systems%20and%20lays%20the%20groundwork%20for%20future%20research%20on%20the%0Anew%20task%20of%20object%20state%20change%20anticipation.%20The%20source%20code%20and%20the%20new%0Aannotation%20data%20%28Ego4D-OSCA%29%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12789v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnticipating%2520Object%2520State%2520Changes%2520in%2520Long%2520Procedural%2520Videos%26entry.906535625%3DVictoria%2520Manousaki%2520and%2520Konstantinos%2520Bacharidis%2520and%2520Filippos%2520Gouidis%2520and%2520Konstantinos%2520Papoutsakis%2520and%2520Dimitris%2520Plexousakis%2520and%2520Antonis%2520Argyros%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520%2528a%2529%2520the%2520new%2520problem%2520of%2520anticipating%2520object%2520state%250Achanges%2520in%2520images%2520and%2520videos%2520during%2520procedural%2520activities%252C%2520%2528b%2529%2520new%2520curated%250Aannotation%2520data%2520for%2520object%2520state%2520change%2520classification%2520based%2520on%2520the%2520Ego4D%250Adataset%252C%2520and%2520%2528c%2529%2520the%2520first%2520method%2520for%2520addressing%2520this%2520challenging%2520problem.%250ASolutions%2520to%2520this%2520new%2520task%2520have%2520important%2520implications%2520in%2520vision-based%2520scene%250Aunderstanding%252C%2520automated%2520monitoring%2520systems%252C%2520and%2520action%2520planning.%2520The%2520proposed%250Anovel%2520framework%2520predicts%2520object%2520state%2520changes%2520that%2520will%2520occur%2520in%2520the%2520near%250Afuture%2520due%2520to%2520yet%2520unseen%2520human%2520actions%2520by%2520integrating%2520learned%2520visual%2520features%250Athat%2520represent%2520recent%2520visual%2520information%2520with%2520natural%2520language%2520%2528NLP%2529%2520features%250Athat%2520represent%2520past%2520object%2520state%2520changes%2520and%2520actions.%2520Leveraging%2520the%2520extensive%250Aand%2520challenging%2520Ego4D%2520dataset%2520which%2520provides%2520a%2520large-scale%2520collection%2520of%250Afirst-person%2520perspective%2520videos%2520across%2520numerous%2520interaction%2520scenarios%252C%2520we%250Aintroduce%2520an%2520extension%2520noted%2520Ego4D-OSCA%2520that%2520provides%2520new%2520curated%2520annotation%250Adata%2520for%2520the%2520object%2520state%2520change%2520anticipation%2520task%2520%2528OSCA%2529.%2520An%2520extensive%250Aexperimental%2520evaluation%2520is%2520presented%2520demonstrating%2520the%2520proposed%2520method%2527s%250Aefficacy%2520in%2520predicting%2520object%2520state%2520changes%2520in%2520dynamic%2520scenarios.%2520The%250Aperformance%2520of%2520the%2520proposed%2520approach%2520also%2520underscores%2520the%2520potential%2520of%250Aintegrating%2520video%2520and%2520linguistic%2520cues%2520to%2520enhance%2520the%2520predictive%2520performance%2520of%250Avideo%2520understanding%2520systems%2520and%2520lays%2520the%2520groundwork%2520for%2520future%2520research%2520on%2520the%250Anew%2520task%2520of%2520object%2520state%2520change%2520anticipation.%2520The%2520source%2520code%2520and%2520the%2520new%250Aannotation%2520data%2520%2528Ego4D-OSCA%2529%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12789v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anticipating%20Object%20State%20Changes%20in%20Long%20Procedural%20Videos&entry.906535625=Victoria%20Manousaki%20and%20Konstantinos%20Bacharidis%20and%20Filippos%20Gouidis%20and%20Konstantinos%20Papoutsakis%20and%20Dimitris%20Plexousakis%20and%20Antonis%20Argyros&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20%28a%29%20the%20new%20problem%20of%20anticipating%20object%20state%0Achanges%20in%20images%20and%20videos%20during%20procedural%20activities%2C%20%28b%29%20new%20curated%0Aannotation%20data%20for%20object%20state%20change%20classification%20based%20on%20the%20Ego4D%0Adataset%2C%20and%20%28c%29%20the%20first%20method%20for%20addressing%20this%20challenging%20problem.%0ASolutions%20to%20this%20new%20task%20have%20important%20implications%20in%20vision-based%20scene%0Aunderstanding%2C%20automated%20monitoring%20systems%2C%20and%20action%20planning.%20The%20proposed%0Anovel%20framework%20predicts%20object%20state%20changes%20that%20will%20occur%20in%20the%20near%0Afuture%20due%20to%20yet%20unseen%20human%20actions%20by%20integrating%20learned%20visual%20features%0Athat%20represent%20recent%20visual%20information%20with%20natural%20language%20%28NLP%29%20features%0Athat%20represent%20past%20object%20state%20changes%20and%20actions.%20Leveraging%20the%20extensive%0Aand%20challenging%20Ego4D%20dataset%20which%20provides%20a%20large-scale%20collection%20of%0Afirst-person%20perspective%20videos%20across%20numerous%20interaction%20scenarios%2C%20we%0Aintroduce%20an%20extension%20noted%20Ego4D-OSCA%20that%20provides%20new%20curated%20annotation%0Adata%20for%20the%20object%20state%20change%20anticipation%20task%20%28OSCA%29.%20An%20extensive%0Aexperimental%20evaluation%20is%20presented%20demonstrating%20the%20proposed%20method%27s%0Aefficacy%20in%20predicting%20object%20state%20changes%20in%20dynamic%20scenarios.%20The%0Aperformance%20of%20the%20proposed%20approach%20also%20underscores%20the%20potential%20of%0Aintegrating%20video%20and%20linguistic%20cues%20to%20enhance%20the%20predictive%20performance%20of%0Avideo%20understanding%20systems%20and%20lays%20the%20groundwork%20for%20future%20research%20on%20the%0Anew%20task%20of%20object%20state%20change%20anticipation.%20The%20source%20code%20and%20the%20new%0Aannotation%20data%20%28Ego4D-OSCA%29%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12789v3&entry.124074799=Read"},
{"title": "Self-Adaptive Quantum Kernel Principal Components Analysis for Compact\n  Readout of Chemiresistive Sensor Arrays", "author": "Zeheng Wang and Timothy van der Laan and Muhammad Usman", "abstract": "  The rapid growth of Internet of Things (IoT) devices necessitates efficient\ndata compression techniques to handle the vast amounts of data generated by\nthese devices. Chemiresistive sensor arrays (CSAs), a simple-to-fabricate but\ncrucial component in IoT systems, generate large volumes of data due to their\nsimultaneous multi-sensor operations. Classical principal component analysis\n(cPCA) methods, a common solution to the data compression challenge, face\nlimitations in preserving critical information during dimensionality reduction.\nIn this study, we present self-adaptive quantum kernel (SAQK) PCA as a superior\nalternative to enhance information retention. Our findings demonstrate that\nSAQK PCA outperforms cPCA in various back-end machine-learning tasks,\nespecially in low-dimensional scenarios where access to quantum bits is\nlimited. These results highlight the potential of noisy intermediate-scale\nquantum (NISQ) computers to revolutionize data processing in real-world IoT\napplications by improving the efficiency and reliability of CSA data\ncompression and readout, despite the current constraints on qubit availability.\n", "link": "http://arxiv.org/abs/2409.00115v2", "date": "2024-12-02", "relevancy": 1.7463, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4685}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.434}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Adaptive%20Quantum%20Kernel%20Principal%20Components%20Analysis%20for%20Compact%0A%20%20Readout%20of%20Chemiresistive%20Sensor%20Arrays&body=Title%3A%20Self-Adaptive%20Quantum%20Kernel%20Principal%20Components%20Analysis%20for%20Compact%0A%20%20Readout%20of%20Chemiresistive%20Sensor%20Arrays%0AAuthor%3A%20Zeheng%20Wang%20and%20Timothy%20van%20der%20Laan%20and%20Muhammad%20Usman%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20Internet%20of%20Things%20%28IoT%29%20devices%20necessitates%20efficient%0Adata%20compression%20techniques%20to%20handle%20the%20vast%20amounts%20of%20data%20generated%20by%0Athese%20devices.%20Chemiresistive%20sensor%20arrays%20%28CSAs%29%2C%20a%20simple-to-fabricate%20but%0Acrucial%20component%20in%20IoT%20systems%2C%20generate%20large%20volumes%20of%20data%20due%20to%20their%0Asimultaneous%20multi-sensor%20operations.%20Classical%20principal%20component%20analysis%0A%28cPCA%29%20methods%2C%20a%20common%20solution%20to%20the%20data%20compression%20challenge%2C%20face%0Alimitations%20in%20preserving%20critical%20information%20during%20dimensionality%20reduction.%0AIn%20this%20study%2C%20we%20present%20self-adaptive%20quantum%20kernel%20%28SAQK%29%20PCA%20as%20a%20superior%0Aalternative%20to%20enhance%20information%20retention.%20Our%20findings%20demonstrate%20that%0ASAQK%20PCA%20outperforms%20cPCA%20in%20various%20back-end%20machine-learning%20tasks%2C%0Aespecially%20in%20low-dimensional%20scenarios%20where%20access%20to%20quantum%20bits%20is%0Alimited.%20These%20results%20highlight%20the%20potential%20of%20noisy%20intermediate-scale%0Aquantum%20%28NISQ%29%20computers%20to%20revolutionize%20data%20processing%20in%20real-world%20IoT%0Aapplications%20by%20improving%20the%20efficiency%20and%20reliability%20of%20CSA%20data%0Acompression%20and%20readout%2C%20despite%20the%20current%20constraints%20on%20qubit%20availability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00115v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Adaptive%2520Quantum%2520Kernel%2520Principal%2520Components%2520Analysis%2520for%2520Compact%250A%2520%2520Readout%2520of%2520Chemiresistive%2520Sensor%2520Arrays%26entry.906535625%3DZeheng%2520Wang%2520and%2520Timothy%2520van%2520der%2520Laan%2520and%2520Muhammad%2520Usman%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520devices%2520necessitates%2520efficient%250Adata%2520compression%2520techniques%2520to%2520handle%2520the%2520vast%2520amounts%2520of%2520data%2520generated%2520by%250Athese%2520devices.%2520Chemiresistive%2520sensor%2520arrays%2520%2528CSAs%2529%252C%2520a%2520simple-to-fabricate%2520but%250Acrucial%2520component%2520in%2520IoT%2520systems%252C%2520generate%2520large%2520volumes%2520of%2520data%2520due%2520to%2520their%250Asimultaneous%2520multi-sensor%2520operations.%2520Classical%2520principal%2520component%2520analysis%250A%2528cPCA%2529%2520methods%252C%2520a%2520common%2520solution%2520to%2520the%2520data%2520compression%2520challenge%252C%2520face%250Alimitations%2520in%2520preserving%2520critical%2520information%2520during%2520dimensionality%2520reduction.%250AIn%2520this%2520study%252C%2520we%2520present%2520self-adaptive%2520quantum%2520kernel%2520%2528SAQK%2529%2520PCA%2520as%2520a%2520superior%250Aalternative%2520to%2520enhance%2520information%2520retention.%2520Our%2520findings%2520demonstrate%2520that%250ASAQK%2520PCA%2520outperforms%2520cPCA%2520in%2520various%2520back-end%2520machine-learning%2520tasks%252C%250Aespecially%2520in%2520low-dimensional%2520scenarios%2520where%2520access%2520to%2520quantum%2520bits%2520is%250Alimited.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520noisy%2520intermediate-scale%250Aquantum%2520%2528NISQ%2529%2520computers%2520to%2520revolutionize%2520data%2520processing%2520in%2520real-world%2520IoT%250Aapplications%2520by%2520improving%2520the%2520efficiency%2520and%2520reliability%2520of%2520CSA%2520data%250Acompression%2520and%2520readout%252C%2520despite%2520the%2520current%2520constraints%2520on%2520qubit%2520availability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00115v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Adaptive%20Quantum%20Kernel%20Principal%20Components%20Analysis%20for%20Compact%0A%20%20Readout%20of%20Chemiresistive%20Sensor%20Arrays&entry.906535625=Zeheng%20Wang%20and%20Timothy%20van%20der%20Laan%20and%20Muhammad%20Usman&entry.1292438233=%20%20The%20rapid%20growth%20of%20Internet%20of%20Things%20%28IoT%29%20devices%20necessitates%20efficient%0Adata%20compression%20techniques%20to%20handle%20the%20vast%20amounts%20of%20data%20generated%20by%0Athese%20devices.%20Chemiresistive%20sensor%20arrays%20%28CSAs%29%2C%20a%20simple-to-fabricate%20but%0Acrucial%20component%20in%20IoT%20systems%2C%20generate%20large%20volumes%20of%20data%20due%20to%20their%0Asimultaneous%20multi-sensor%20operations.%20Classical%20principal%20component%20analysis%0A%28cPCA%29%20methods%2C%20a%20common%20solution%20to%20the%20data%20compression%20challenge%2C%20face%0Alimitations%20in%20preserving%20critical%20information%20during%20dimensionality%20reduction.%0AIn%20this%20study%2C%20we%20present%20self-adaptive%20quantum%20kernel%20%28SAQK%29%20PCA%20as%20a%20superior%0Aalternative%20to%20enhance%20information%20retention.%20Our%20findings%20demonstrate%20that%0ASAQK%20PCA%20outperforms%20cPCA%20in%20various%20back-end%20machine-learning%20tasks%2C%0Aespecially%20in%20low-dimensional%20scenarios%20where%20access%20to%20quantum%20bits%20is%0Alimited.%20These%20results%20highlight%20the%20potential%20of%20noisy%20intermediate-scale%0Aquantum%20%28NISQ%29%20computers%20to%20revolutionize%20data%20processing%20in%20real-world%20IoT%0Aapplications%20by%20improving%20the%20efficiency%20and%20reliability%20of%20CSA%20data%0Acompression%20and%20readout%2C%20despite%20the%20current%20constraints%20on%20qubit%20availability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00115v2&entry.124074799=Read"},
{"title": "Autobidders with Budget and ROI Constraints: Efficiency, Regret, and\n  Pacing Dynamics", "author": "Brendan Lucier and Sarath Pattathil and Aleksandrs Slivkins and Mengxiao Zhang", "abstract": "  We study a game between autobidding algorithms that compete in an online\nadvertising platform. Each autobidder is tasked with maximizing its\nadvertiser's total value over multiple rounds of a repeated auction, subject to\nbudget and return-on-investment constraints. We propose a gradient-based\nlearning algorithm that is guaranteed to satisfy all constraints and achieves\nvanishing individual regret. Our algorithm uses only bandit feedback and can be\nused with the first- or second-price auction, as well as with any\n\"intermediate\" auction format. Our main result is that when these autobidders\nplay against each other, the resulting expected liquid welfare over all rounds\nis at least half of the expected optimal liquid welfare achieved by any\nallocation. This holds whether or not the bidding dynamics converges to an\nequilibrium.\n", "link": "http://arxiv.org/abs/2301.13306v6", "date": "2024-12-02", "relevancy": 1.742, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4481}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4313}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autobidders%20with%20Budget%20and%20ROI%20Constraints%3A%20Efficiency%2C%20Regret%2C%20and%0A%20%20Pacing%20Dynamics&body=Title%3A%20Autobidders%20with%20Budget%20and%20ROI%20Constraints%3A%20Efficiency%2C%20Regret%2C%20and%0A%20%20Pacing%20Dynamics%0AAuthor%3A%20Brendan%20Lucier%20and%20Sarath%20Pattathil%20and%20Aleksandrs%20Slivkins%20and%20Mengxiao%20Zhang%0AAbstract%3A%20%20%20We%20study%20a%20game%20between%20autobidding%20algorithms%20that%20compete%20in%20an%20online%0Aadvertising%20platform.%20Each%20autobidder%20is%20tasked%20with%20maximizing%20its%0Aadvertiser%27s%20total%20value%20over%20multiple%20rounds%20of%20a%20repeated%20auction%2C%20subject%20to%0Abudget%20and%20return-on-investment%20constraints.%20We%20propose%20a%20gradient-based%0Alearning%20algorithm%20that%20is%20guaranteed%20to%20satisfy%20all%20constraints%20and%20achieves%0Avanishing%20individual%20regret.%20Our%20algorithm%20uses%20only%20bandit%20feedback%20and%20can%20be%0Aused%20with%20the%20first-%20or%20second-price%20auction%2C%20as%20well%20as%20with%20any%0A%22intermediate%22%20auction%20format.%20Our%20main%20result%20is%20that%20when%20these%20autobidders%0Aplay%20against%20each%20other%2C%20the%20resulting%20expected%20liquid%20welfare%20over%20all%20rounds%0Ais%20at%20least%20half%20of%20the%20expected%20optimal%20liquid%20welfare%20achieved%20by%20any%0Aallocation.%20This%20holds%20whether%20or%20not%20the%20bidding%20dynamics%20converges%20to%20an%0Aequilibrium.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.13306v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutobidders%2520with%2520Budget%2520and%2520ROI%2520Constraints%253A%2520Efficiency%252C%2520Regret%252C%2520and%250A%2520%2520Pacing%2520Dynamics%26entry.906535625%3DBrendan%2520Lucier%2520and%2520Sarath%2520Pattathil%2520and%2520Aleksandrs%2520Slivkins%2520and%2520Mengxiao%2520Zhang%26entry.1292438233%3D%2520%2520We%2520study%2520a%2520game%2520between%2520autobidding%2520algorithms%2520that%2520compete%2520in%2520an%2520online%250Aadvertising%2520platform.%2520Each%2520autobidder%2520is%2520tasked%2520with%2520maximizing%2520its%250Aadvertiser%2527s%2520total%2520value%2520over%2520multiple%2520rounds%2520of%2520a%2520repeated%2520auction%252C%2520subject%2520to%250Abudget%2520and%2520return-on-investment%2520constraints.%2520We%2520propose%2520a%2520gradient-based%250Alearning%2520algorithm%2520that%2520is%2520guaranteed%2520to%2520satisfy%2520all%2520constraints%2520and%2520achieves%250Avanishing%2520individual%2520regret.%2520Our%2520algorithm%2520uses%2520only%2520bandit%2520feedback%2520and%2520can%2520be%250Aused%2520with%2520the%2520first-%2520or%2520second-price%2520auction%252C%2520as%2520well%2520as%2520with%2520any%250A%2522intermediate%2522%2520auction%2520format.%2520Our%2520main%2520result%2520is%2520that%2520when%2520these%2520autobidders%250Aplay%2520against%2520each%2520other%252C%2520the%2520resulting%2520expected%2520liquid%2520welfare%2520over%2520all%2520rounds%250Ais%2520at%2520least%2520half%2520of%2520the%2520expected%2520optimal%2520liquid%2520welfare%2520achieved%2520by%2520any%250Aallocation.%2520This%2520holds%2520whether%2520or%2520not%2520the%2520bidding%2520dynamics%2520converges%2520to%2520an%250Aequilibrium.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.13306v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autobidders%20with%20Budget%20and%20ROI%20Constraints%3A%20Efficiency%2C%20Regret%2C%20and%0A%20%20Pacing%20Dynamics&entry.906535625=Brendan%20Lucier%20and%20Sarath%20Pattathil%20and%20Aleksandrs%20Slivkins%20and%20Mengxiao%20Zhang&entry.1292438233=%20%20We%20study%20a%20game%20between%20autobidding%20algorithms%20that%20compete%20in%20an%20online%0Aadvertising%20platform.%20Each%20autobidder%20is%20tasked%20with%20maximizing%20its%0Aadvertiser%27s%20total%20value%20over%20multiple%20rounds%20of%20a%20repeated%20auction%2C%20subject%20to%0Abudget%20and%20return-on-investment%20constraints.%20We%20propose%20a%20gradient-based%0Alearning%20algorithm%20that%20is%20guaranteed%20to%20satisfy%20all%20constraints%20and%20achieves%0Avanishing%20individual%20regret.%20Our%20algorithm%20uses%20only%20bandit%20feedback%20and%20can%20be%0Aused%20with%20the%20first-%20or%20second-price%20auction%2C%20as%20well%20as%20with%20any%0A%22intermediate%22%20auction%20format.%20Our%20main%20result%20is%20that%20when%20these%20autobidders%0Aplay%20against%20each%20other%2C%20the%20resulting%20expected%20liquid%20welfare%20over%20all%20rounds%0Ais%20at%20least%20half%20of%20the%20expected%20optimal%20liquid%20welfare%20achieved%20by%20any%0Aallocation.%20This%20holds%20whether%20or%20not%20the%20bidding%20dynamics%20converges%20to%20an%0Aequilibrium.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.13306v6&entry.124074799=Read"},
{"title": "Bridging Language and Action: A Survey of Language-Conditioned Robot\n  Manipulation", "author": "Hongkuan Zhou and Xiangtong Yao and Oier Mees and Yuan Meng and Ted Xiao and Yonatan Bisk and Jean Oh and Edward Johns and Mohit Shridhar and Dhruv Shah and Jesse Thomason and Kai Huang and Joyce Chai and Zhenshan Bing and Alois Knoll", "abstract": "  Language-conditioned robot manipulation is an emerging field aimed at\nenabling seamless communication and cooperation between humans and robotic\nagents by teaching robots to comprehend and execute instructions conveyed in\nnatural language. This interdisciplinary area integrates scene understanding,\nlanguage processing, and policy learning to bridge the gap between human\ninstructions and robotic actions. In this comprehensive survey, we\nsystematically explore recent advancements in language-conditioned robotic\nmanipulation. We categorize existing methods into language-conditioned reward\nshaping, language-conditioned policy learning, neuro-symbolic artificial\nintelligence, and the utilization of foundational models (FMs) such as large\nlanguage models (LLMs) and vision-language models (VLMs). Specifically, we\nanalyze state-of-the-art techniques concerning semantic information extraction,\nenvironment and evaluation, auxiliary tasks, and task representation\nstrategies. By conducting a comparative analysis, we highlight the strengths\nand limitations of current approaches in bridging language instructions with\nrobot actions. Finally, we discuss open challenges and future research\ndirections, focusing on potentially enhancing generalization capabilities and\naddressing safety issues in language-conditioned robot manipulators. The GitHub\nrepository of this paper can be found at\nhttps://github.com/hk-zh/language-conditioned-robot-manipulation-models.\n", "link": "http://arxiv.org/abs/2312.10807v3", "date": "2024-12-02", "relevancy": 1.728, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.642}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.587}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Language%20and%20Action%3A%20A%20Survey%20of%20Language-Conditioned%20Robot%0A%20%20Manipulation&body=Title%3A%20Bridging%20Language%20and%20Action%3A%20A%20Survey%20of%20Language-Conditioned%20Robot%0A%20%20Manipulation%0AAuthor%3A%20Hongkuan%20Zhou%20and%20Xiangtong%20Yao%20and%20Oier%20Mees%20and%20Yuan%20Meng%20and%20Ted%20Xiao%20and%20Yonatan%20Bisk%20and%20Jean%20Oh%20and%20Edward%20Johns%20and%20Mohit%20Shridhar%20and%20Dhruv%20Shah%20and%20Jesse%20Thomason%20and%20Kai%20Huang%20and%20Joyce%20Chai%20and%20Zhenshan%20Bing%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20Language-conditioned%20robot%20manipulation%20is%20an%20emerging%20field%20aimed%20at%0Aenabling%20seamless%20communication%20and%20cooperation%20between%20humans%20and%20robotic%0Aagents%20by%20teaching%20robots%20to%20comprehend%20and%20execute%20instructions%20conveyed%20in%0Anatural%20language.%20This%20interdisciplinary%20area%20integrates%20scene%20understanding%2C%0Alanguage%20processing%2C%20and%20policy%20learning%20to%20bridge%20the%20gap%20between%20human%0Ainstructions%20and%20robotic%20actions.%20In%20this%20comprehensive%20survey%2C%20we%0Asystematically%20explore%20recent%20advancements%20in%20language-conditioned%20robotic%0Amanipulation.%20We%20categorize%20existing%20methods%20into%20language-conditioned%20reward%0Ashaping%2C%20language-conditioned%20policy%20learning%2C%20neuro-symbolic%20artificial%0Aintelligence%2C%20and%20the%20utilization%20of%20foundational%20models%20%28FMs%29%20such%20as%20large%0Alanguage%20models%20%28LLMs%29%20and%20vision-language%20models%20%28VLMs%29.%20Specifically%2C%20we%0Aanalyze%20state-of-the-art%20techniques%20concerning%20semantic%20information%20extraction%2C%0Aenvironment%20and%20evaluation%2C%20auxiliary%20tasks%2C%20and%20task%20representation%0Astrategies.%20By%20conducting%20a%20comparative%20analysis%2C%20we%20highlight%20the%20strengths%0Aand%20limitations%20of%20current%20approaches%20in%20bridging%20language%20instructions%20with%0Arobot%20actions.%20Finally%2C%20we%20discuss%20open%20challenges%20and%20future%20research%0Adirections%2C%20focusing%20on%20potentially%20enhancing%20generalization%20capabilities%20and%0Aaddressing%20safety%20issues%20in%20language-conditioned%20robot%20manipulators.%20The%20GitHub%0Arepository%20of%20this%20paper%20can%20be%20found%20at%0Ahttps%3A//github.com/hk-zh/language-conditioned-robot-manipulation-models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10807v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Language%2520and%2520Action%253A%2520A%2520Survey%2520of%2520Language-Conditioned%2520Robot%250A%2520%2520Manipulation%26entry.906535625%3DHongkuan%2520Zhou%2520and%2520Xiangtong%2520Yao%2520and%2520Oier%2520Mees%2520and%2520Yuan%2520Meng%2520and%2520Ted%2520Xiao%2520and%2520Yonatan%2520Bisk%2520and%2520Jean%2520Oh%2520and%2520Edward%2520Johns%2520and%2520Mohit%2520Shridhar%2520and%2520Dhruv%2520Shah%2520and%2520Jesse%2520Thomason%2520and%2520Kai%2520Huang%2520and%2520Joyce%2520Chai%2520and%2520Zhenshan%2520Bing%2520and%2520Alois%2520Knoll%26entry.1292438233%3D%2520%2520Language-conditioned%2520robot%2520manipulation%2520is%2520an%2520emerging%2520field%2520aimed%2520at%250Aenabling%2520seamless%2520communication%2520and%2520cooperation%2520between%2520humans%2520and%2520robotic%250Aagents%2520by%2520teaching%2520robots%2520to%2520comprehend%2520and%2520execute%2520instructions%2520conveyed%2520in%250Anatural%2520language.%2520This%2520interdisciplinary%2520area%2520integrates%2520scene%2520understanding%252C%250Alanguage%2520processing%252C%2520and%2520policy%2520learning%2520to%2520bridge%2520the%2520gap%2520between%2520human%250Ainstructions%2520and%2520robotic%2520actions.%2520In%2520this%2520comprehensive%2520survey%252C%2520we%250Asystematically%2520explore%2520recent%2520advancements%2520in%2520language-conditioned%2520robotic%250Amanipulation.%2520We%2520categorize%2520existing%2520methods%2520into%2520language-conditioned%2520reward%250Ashaping%252C%2520language-conditioned%2520policy%2520learning%252C%2520neuro-symbolic%2520artificial%250Aintelligence%252C%2520and%2520the%2520utilization%2520of%2520foundational%2520models%2520%2528FMs%2529%2520such%2520as%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520and%2520vision-language%2520models%2520%2528VLMs%2529.%2520Specifically%252C%2520we%250Aanalyze%2520state-of-the-art%2520techniques%2520concerning%2520semantic%2520information%2520extraction%252C%250Aenvironment%2520and%2520evaluation%252C%2520auxiliary%2520tasks%252C%2520and%2520task%2520representation%250Astrategies.%2520By%2520conducting%2520a%2520comparative%2520analysis%252C%2520we%2520highlight%2520the%2520strengths%250Aand%2520limitations%2520of%2520current%2520approaches%2520in%2520bridging%2520language%2520instructions%2520with%250Arobot%2520actions.%2520Finally%252C%2520we%2520discuss%2520open%2520challenges%2520and%2520future%2520research%250Adirections%252C%2520focusing%2520on%2520potentially%2520enhancing%2520generalization%2520capabilities%2520and%250Aaddressing%2520safety%2520issues%2520in%2520language-conditioned%2520robot%2520manipulators.%2520The%2520GitHub%250Arepository%2520of%2520this%2520paper%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/hk-zh/language-conditioned-robot-manipulation-models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.10807v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Language%20and%20Action%3A%20A%20Survey%20of%20Language-Conditioned%20Robot%0A%20%20Manipulation&entry.906535625=Hongkuan%20Zhou%20and%20Xiangtong%20Yao%20and%20Oier%20Mees%20and%20Yuan%20Meng%20and%20Ted%20Xiao%20and%20Yonatan%20Bisk%20and%20Jean%20Oh%20and%20Edward%20Johns%20and%20Mohit%20Shridhar%20and%20Dhruv%20Shah%20and%20Jesse%20Thomason%20and%20Kai%20Huang%20and%20Joyce%20Chai%20and%20Zhenshan%20Bing%20and%20Alois%20Knoll&entry.1292438233=%20%20Language-conditioned%20robot%20manipulation%20is%20an%20emerging%20field%20aimed%20at%0Aenabling%20seamless%20communication%20and%20cooperation%20between%20humans%20and%20robotic%0Aagents%20by%20teaching%20robots%20to%20comprehend%20and%20execute%20instructions%20conveyed%20in%0Anatural%20language.%20This%20interdisciplinary%20area%20integrates%20scene%20understanding%2C%0Alanguage%20processing%2C%20and%20policy%20learning%20to%20bridge%20the%20gap%20between%20human%0Ainstructions%20and%20robotic%20actions.%20In%20this%20comprehensive%20survey%2C%20we%0Asystematically%20explore%20recent%20advancements%20in%20language-conditioned%20robotic%0Amanipulation.%20We%20categorize%20existing%20methods%20into%20language-conditioned%20reward%0Ashaping%2C%20language-conditioned%20policy%20learning%2C%20neuro-symbolic%20artificial%0Aintelligence%2C%20and%20the%20utilization%20of%20foundational%20models%20%28FMs%29%20such%20as%20large%0Alanguage%20models%20%28LLMs%29%20and%20vision-language%20models%20%28VLMs%29.%20Specifically%2C%20we%0Aanalyze%20state-of-the-art%20techniques%20concerning%20semantic%20information%20extraction%2C%0Aenvironment%20and%20evaluation%2C%20auxiliary%20tasks%2C%20and%20task%20representation%0Astrategies.%20By%20conducting%20a%20comparative%20analysis%2C%20we%20highlight%20the%20strengths%0Aand%20limitations%20of%20current%20approaches%20in%20bridging%20language%20instructions%20with%0Arobot%20actions.%20Finally%2C%20we%20discuss%20open%20challenges%20and%20future%20research%0Adirections%2C%20focusing%20on%20potentially%20enhancing%20generalization%20capabilities%20and%0Aaddressing%20safety%20issues%20in%20language-conditioned%20robot%20manipulators.%20The%20GitHub%0Arepository%20of%20this%20paper%20can%20be%20found%20at%0Ahttps%3A//github.com/hk-zh/language-conditioned-robot-manipulation-models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10807v3&entry.124074799=Read"},
{"title": "Strongly-polynomial time and validation analysis of policy gradient\n  methods", "author": "Caleb Ju and Guanghui Lan", "abstract": "  This paper proposes a novel termination criterion, termed the advantage gap\nfunction, for finite state and action Markov decision processes (MDP) and\nreinforcement learning (RL). By incorporating this advantage gap function into\nthe design of step size rules and deriving a new linear rate of convergence\nthat is independent of the stationary state distribution of the optimal policy,\nwe demonstrate that policy gradient methods can solve MDPs in\nstrongly-polynomial time. To the best of our knowledge, this is the first time\nthat such strong convergence properties have been established for policy\ngradient methods. Moreover, in the stochastic setting, where only stochastic\nestimates of policy gradients are available, we show that the advantage gap\nfunction provides close approximations of the optimality gap for each\nindividual state and exhibits a sublinear rate of convergence at every state.\nThe advantage gap function can be easily estimated in the stochastic case, and\nwhen coupled with easily computable upper bounds on policy values, they provide\na convenient way to validate the solutions generated by policy gradient\nmethods. Therefore, our developments offer a principled and computable measure\nof optimality for RL, whereas current practice tends to rely on\nalgorithm-to-algorithm or baselines comparisons with no certificate of\noptimality.\n", "link": "http://arxiv.org/abs/2409.19437v3", "date": "2024-12-02", "relevancy": 1.7088, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4754}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4229}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Strongly-polynomial%20time%20and%20validation%20analysis%20of%20policy%20gradient%0A%20%20methods&body=Title%3A%20Strongly-polynomial%20time%20and%20validation%20analysis%20of%20policy%20gradient%0A%20%20methods%0AAuthor%3A%20Caleb%20Ju%20and%20Guanghui%20Lan%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20novel%20termination%20criterion%2C%20termed%20the%20advantage%20gap%0Afunction%2C%20for%20finite%20state%20and%20action%20Markov%20decision%20processes%20%28MDP%29%20and%0Areinforcement%20learning%20%28RL%29.%20By%20incorporating%20this%20advantage%20gap%20function%20into%0Athe%20design%20of%20step%20size%20rules%20and%20deriving%20a%20new%20linear%20rate%20of%20convergence%0Athat%20is%20independent%20of%20the%20stationary%20state%20distribution%20of%20the%20optimal%20policy%2C%0Awe%20demonstrate%20that%20policy%20gradient%20methods%20can%20solve%20MDPs%20in%0Astrongly-polynomial%20time.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20time%0Athat%20such%20strong%20convergence%20properties%20have%20been%20established%20for%20policy%0Agradient%20methods.%20Moreover%2C%20in%20the%20stochastic%20setting%2C%20where%20only%20stochastic%0Aestimates%20of%20policy%20gradients%20are%20available%2C%20we%20show%20that%20the%20advantage%20gap%0Afunction%20provides%20close%20approximations%20of%20the%20optimality%20gap%20for%20each%0Aindividual%20state%20and%20exhibits%20a%20sublinear%20rate%20of%20convergence%20at%20every%20state.%0AThe%20advantage%20gap%20function%20can%20be%20easily%20estimated%20in%20the%20stochastic%20case%2C%20and%0Awhen%20coupled%20with%20easily%20computable%20upper%20bounds%20on%20policy%20values%2C%20they%20provide%0Aa%20convenient%20way%20to%20validate%20the%20solutions%20generated%20by%20policy%20gradient%0Amethods.%20Therefore%2C%20our%20developments%20offer%20a%20principled%20and%20computable%20measure%0Aof%20optimality%20for%20RL%2C%20whereas%20current%20practice%20tends%20to%20rely%20on%0Aalgorithm-to-algorithm%20or%20baselines%20comparisons%20with%20no%20certificate%20of%0Aoptimality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.19437v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStrongly-polynomial%2520time%2520and%2520validation%2520analysis%2520of%2520policy%2520gradient%250A%2520%2520methods%26entry.906535625%3DCaleb%2520Ju%2520and%2520Guanghui%2520Lan%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520novel%2520termination%2520criterion%252C%2520termed%2520the%2520advantage%2520gap%250Afunction%252C%2520for%2520finite%2520state%2520and%2520action%2520Markov%2520decision%2520processes%2520%2528MDP%2529%2520and%250Areinforcement%2520learning%2520%2528RL%2529.%2520By%2520incorporating%2520this%2520advantage%2520gap%2520function%2520into%250Athe%2520design%2520of%2520step%2520size%2520rules%2520and%2520deriving%2520a%2520new%2520linear%2520rate%2520of%2520convergence%250Athat%2520is%2520independent%2520of%2520the%2520stationary%2520state%2520distribution%2520of%2520the%2520optimal%2520policy%252C%250Awe%2520demonstrate%2520that%2520policy%2520gradient%2520methods%2520can%2520solve%2520MDPs%2520in%250Astrongly-polynomial%2520time.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520time%250Athat%2520such%2520strong%2520convergence%2520properties%2520have%2520been%2520established%2520for%2520policy%250Agradient%2520methods.%2520Moreover%252C%2520in%2520the%2520stochastic%2520setting%252C%2520where%2520only%2520stochastic%250Aestimates%2520of%2520policy%2520gradients%2520are%2520available%252C%2520we%2520show%2520that%2520the%2520advantage%2520gap%250Afunction%2520provides%2520close%2520approximations%2520of%2520the%2520optimality%2520gap%2520for%2520each%250Aindividual%2520state%2520and%2520exhibits%2520a%2520sublinear%2520rate%2520of%2520convergence%2520at%2520every%2520state.%250AThe%2520advantage%2520gap%2520function%2520can%2520be%2520easily%2520estimated%2520in%2520the%2520stochastic%2520case%252C%2520and%250Awhen%2520coupled%2520with%2520easily%2520computable%2520upper%2520bounds%2520on%2520policy%2520values%252C%2520they%2520provide%250Aa%2520convenient%2520way%2520to%2520validate%2520the%2520solutions%2520generated%2520by%2520policy%2520gradient%250Amethods.%2520Therefore%252C%2520our%2520developments%2520offer%2520a%2520principled%2520and%2520computable%2520measure%250Aof%2520optimality%2520for%2520RL%252C%2520whereas%2520current%2520practice%2520tends%2520to%2520rely%2520on%250Aalgorithm-to-algorithm%2520or%2520baselines%2520comparisons%2520with%2520no%2520certificate%2520of%250Aoptimality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.19437v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Strongly-polynomial%20time%20and%20validation%20analysis%20of%20policy%20gradient%0A%20%20methods&entry.906535625=Caleb%20Ju%20and%20Guanghui%20Lan&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20termination%20criterion%2C%20termed%20the%20advantage%20gap%0Afunction%2C%20for%20finite%20state%20and%20action%20Markov%20decision%20processes%20%28MDP%29%20and%0Areinforcement%20learning%20%28RL%29.%20By%20incorporating%20this%20advantage%20gap%20function%20into%0Athe%20design%20of%20step%20size%20rules%20and%20deriving%20a%20new%20linear%20rate%20of%20convergence%0Athat%20is%20independent%20of%20the%20stationary%20state%20distribution%20of%20the%20optimal%20policy%2C%0Awe%20demonstrate%20that%20policy%20gradient%20methods%20can%20solve%20MDPs%20in%0Astrongly-polynomial%20time.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20time%0Athat%20such%20strong%20convergence%20properties%20have%20been%20established%20for%20policy%0Agradient%20methods.%20Moreover%2C%20in%20the%20stochastic%20setting%2C%20where%20only%20stochastic%0Aestimates%20of%20policy%20gradients%20are%20available%2C%20we%20show%20that%20the%20advantage%20gap%0Afunction%20provides%20close%20approximations%20of%20the%20optimality%20gap%20for%20each%0Aindividual%20state%20and%20exhibits%20a%20sublinear%20rate%20of%20convergence%20at%20every%20state.%0AThe%20advantage%20gap%20function%20can%20be%20easily%20estimated%20in%20the%20stochastic%20case%2C%20and%0Awhen%20coupled%20with%20easily%20computable%20upper%20bounds%20on%20policy%20values%2C%20they%20provide%0Aa%20convenient%20way%20to%20validate%20the%20solutions%20generated%20by%20policy%20gradient%0Amethods.%20Therefore%2C%20our%20developments%20offer%20a%20principled%20and%20computable%20measure%0Aof%20optimality%20for%20RL%2C%20whereas%20current%20practice%20tends%20to%20rely%20on%0Aalgorithm-to-algorithm%20or%20baselines%20comparisons%20with%20no%20certificate%20of%0Aoptimality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.19437v3&entry.124074799=Read"},
{"title": "Learning Temporally Consistent Video Depth from Video Diffusion Priors", "author": "Jiahao Shao and Yuanbo Yang and Hongyu Zhou and Youmin Zhang and Yujun Shen and Vitor Guizilini and Yue Wang and Matteo Poggi and Yiyi Liao", "abstract": "  This work addresses the challenge of streamed video depth estimation, which\nexpects not only per-frame accuracy but, more importantly, cross-frame\nconsistency. We argue that sharing contextual information between frames or\nclips is pivotal in fostering temporal consistency. Thus, instead of directly\ndeveloping a depth estimator from scratch, we reformulate this predictive task\ninto a conditional generation problem to provide contextual information within\na clip and across clips. Specifically, we propose a consistent context-aware\ntraining and inference strategy for arbitrarily long videos to provide\ncross-clip context. We sample independent noise levels for each frame within a\nclip during training while using a sliding window strategy and initializing\noverlapping frames with previously predicted frames without adding noise.\nMoreover, we design an effective training strategy to provide context within a\nclip. Extensive experimental results validate our design choices and\ndemonstrate the superiority of our approach, dubbed ChronoDepth. Project page:\nhttps://xdimlab.github.io/ChronoDepth/.\n", "link": "http://arxiv.org/abs/2406.01493v3", "date": "2024-12-02", "relevancy": 1.7084, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5932}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5669}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Temporally%20Consistent%20Video%20Depth%20from%20Video%20Diffusion%20Priors&body=Title%3A%20Learning%20Temporally%20Consistent%20Video%20Depth%20from%20Video%20Diffusion%20Priors%0AAuthor%3A%20Jiahao%20Shao%20and%20Yuanbo%20Yang%20and%20Hongyu%20Zhou%20and%20Youmin%20Zhang%20and%20Yujun%20Shen%20and%20Vitor%20Guizilini%20and%20Yue%20Wang%20and%20Matteo%20Poggi%20and%20Yiyi%20Liao%0AAbstract%3A%20%20%20This%20work%20addresses%20the%20challenge%20of%20streamed%20video%20depth%20estimation%2C%20which%0Aexpects%20not%20only%20per-frame%20accuracy%20but%2C%20more%20importantly%2C%20cross-frame%0Aconsistency.%20We%20argue%20that%20sharing%20contextual%20information%20between%20frames%20or%0Aclips%20is%20pivotal%20in%20fostering%20temporal%20consistency.%20Thus%2C%20instead%20of%20directly%0Adeveloping%20a%20depth%20estimator%20from%20scratch%2C%20we%20reformulate%20this%20predictive%20task%0Ainto%20a%20conditional%20generation%20problem%20to%20provide%20contextual%20information%20within%0Aa%20clip%20and%20across%20clips.%20Specifically%2C%20we%20propose%20a%20consistent%20context-aware%0Atraining%20and%20inference%20strategy%20for%20arbitrarily%20long%20videos%20to%20provide%0Across-clip%20context.%20We%20sample%20independent%20noise%20levels%20for%20each%20frame%20within%20a%0Aclip%20during%20training%20while%20using%20a%20sliding%20window%20strategy%20and%20initializing%0Aoverlapping%20frames%20with%20previously%20predicted%20frames%20without%20adding%20noise.%0AMoreover%2C%20we%20design%20an%20effective%20training%20strategy%20to%20provide%20context%20within%20a%0Aclip.%20Extensive%20experimental%20results%20validate%20our%20design%20choices%20and%0Ademonstrate%20the%20superiority%20of%20our%20approach%2C%20dubbed%20ChronoDepth.%20Project%20page%3A%0Ahttps%3A//xdimlab.github.io/ChronoDepth/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01493v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Temporally%2520Consistent%2520Video%2520Depth%2520from%2520Video%2520Diffusion%2520Priors%26entry.906535625%3DJiahao%2520Shao%2520and%2520Yuanbo%2520Yang%2520and%2520Hongyu%2520Zhou%2520and%2520Youmin%2520Zhang%2520and%2520Yujun%2520Shen%2520and%2520Vitor%2520Guizilini%2520and%2520Yue%2520Wang%2520and%2520Matteo%2520Poggi%2520and%2520Yiyi%2520Liao%26entry.1292438233%3D%2520%2520This%2520work%2520addresses%2520the%2520challenge%2520of%2520streamed%2520video%2520depth%2520estimation%252C%2520which%250Aexpects%2520not%2520only%2520per-frame%2520accuracy%2520but%252C%2520more%2520importantly%252C%2520cross-frame%250Aconsistency.%2520We%2520argue%2520that%2520sharing%2520contextual%2520information%2520between%2520frames%2520or%250Aclips%2520is%2520pivotal%2520in%2520fostering%2520temporal%2520consistency.%2520Thus%252C%2520instead%2520of%2520directly%250Adeveloping%2520a%2520depth%2520estimator%2520from%2520scratch%252C%2520we%2520reformulate%2520this%2520predictive%2520task%250Ainto%2520a%2520conditional%2520generation%2520problem%2520to%2520provide%2520contextual%2520information%2520within%250Aa%2520clip%2520and%2520across%2520clips.%2520Specifically%252C%2520we%2520propose%2520a%2520consistent%2520context-aware%250Atraining%2520and%2520inference%2520strategy%2520for%2520arbitrarily%2520long%2520videos%2520to%2520provide%250Across-clip%2520context.%2520We%2520sample%2520independent%2520noise%2520levels%2520for%2520each%2520frame%2520within%2520a%250Aclip%2520during%2520training%2520while%2520using%2520a%2520sliding%2520window%2520strategy%2520and%2520initializing%250Aoverlapping%2520frames%2520with%2520previously%2520predicted%2520frames%2520without%2520adding%2520noise.%250AMoreover%252C%2520we%2520design%2520an%2520effective%2520training%2520strategy%2520to%2520provide%2520context%2520within%2520a%250Aclip.%2520Extensive%2520experimental%2520results%2520validate%2520our%2520design%2520choices%2520and%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520approach%252C%2520dubbed%2520ChronoDepth.%2520Project%2520page%253A%250Ahttps%253A//xdimlab.github.io/ChronoDepth/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01493v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Temporally%20Consistent%20Video%20Depth%20from%20Video%20Diffusion%20Priors&entry.906535625=Jiahao%20Shao%20and%20Yuanbo%20Yang%20and%20Hongyu%20Zhou%20and%20Youmin%20Zhang%20and%20Yujun%20Shen%20and%20Vitor%20Guizilini%20and%20Yue%20Wang%20and%20Matteo%20Poggi%20and%20Yiyi%20Liao&entry.1292438233=%20%20This%20work%20addresses%20the%20challenge%20of%20streamed%20video%20depth%20estimation%2C%20which%0Aexpects%20not%20only%20per-frame%20accuracy%20but%2C%20more%20importantly%2C%20cross-frame%0Aconsistency.%20We%20argue%20that%20sharing%20contextual%20information%20between%20frames%20or%0Aclips%20is%20pivotal%20in%20fostering%20temporal%20consistency.%20Thus%2C%20instead%20of%20directly%0Adeveloping%20a%20depth%20estimator%20from%20scratch%2C%20we%20reformulate%20this%20predictive%20task%0Ainto%20a%20conditional%20generation%20problem%20to%20provide%20contextual%20information%20within%0Aa%20clip%20and%20across%20clips.%20Specifically%2C%20we%20propose%20a%20consistent%20context-aware%0Atraining%20and%20inference%20strategy%20for%20arbitrarily%20long%20videos%20to%20provide%0Across-clip%20context.%20We%20sample%20independent%20noise%20levels%20for%20each%20frame%20within%20a%0Aclip%20during%20training%20while%20using%20a%20sliding%20window%20strategy%20and%20initializing%0Aoverlapping%20frames%20with%20previously%20predicted%20frames%20without%20adding%20noise.%0AMoreover%2C%20we%20design%20an%20effective%20training%20strategy%20to%20provide%20context%20within%20a%0Aclip.%20Extensive%20experimental%20results%20validate%20our%20design%20choices%20and%0Ademonstrate%20the%20superiority%20of%20our%20approach%2C%20dubbed%20ChronoDepth.%20Project%20page%3A%0Ahttps%3A//xdimlab.github.io/ChronoDepth/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01493v3&entry.124074799=Read"},
{"title": "RIRAG: Regulatory Information Retrieval and Answer Generation", "author": "Tuba Gokhan and Kexin Wang and Iryna Gurevych and Ted Briscoe", "abstract": "  Regulatory documents, issued by governmental regulatory bodies, establish\nrules, guidelines, and standards that organizations must adhere to for legal\ncompliance. These documents, characterized by their length, complexity and\nfrequent updates, are challenging to interpret, requiring significant\nallocation of time and expertise on the part of organizations to ensure ongoing\ncompliance. Regulatory Natural Language Processing (RegNLP) is a\nmultidisciplinary field aimed at simplifying access to and interpretation of\nregulatory rules and obligations. We introduce a task of generating\nquestion-passages pairs, where questions are automatically created and paired\nwith relevant regulatory passages, facilitating the development of regulatory\nquestion-answering systems. We create the ObliQA dataset, containing 27,869\nquestions derived from the collection of Abu Dhabi Global Markets (ADGM)\nfinancial regulation documents, design a baseline Regulatory Information\nRetrieval and Answer Generation (RIRAG) system and evaluate it with RePASs, a\nnovel evaluation metric that tests whether generated answers accurately capture\nall relevant obligations while avoiding contradictions.\n", "link": "http://arxiv.org/abs/2409.05677v2", "date": "2024-12-02", "relevancy": 1.6425, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4541}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4098}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RIRAG%3A%20Regulatory%20Information%20Retrieval%20and%20Answer%20Generation&body=Title%3A%20RIRAG%3A%20Regulatory%20Information%20Retrieval%20and%20Answer%20Generation%0AAuthor%3A%20Tuba%20Gokhan%20and%20Kexin%20Wang%20and%20Iryna%20Gurevych%20and%20Ted%20Briscoe%0AAbstract%3A%20%20%20Regulatory%20documents%2C%20issued%20by%20governmental%20regulatory%20bodies%2C%20establish%0Arules%2C%20guidelines%2C%20and%20standards%20that%20organizations%20must%20adhere%20to%20for%20legal%0Acompliance.%20These%20documents%2C%20characterized%20by%20their%20length%2C%20complexity%20and%0Afrequent%20updates%2C%20are%20challenging%20to%20interpret%2C%20requiring%20significant%0Aallocation%20of%20time%20and%20expertise%20on%20the%20part%20of%20organizations%20to%20ensure%20ongoing%0Acompliance.%20Regulatory%20Natural%20Language%20Processing%20%28RegNLP%29%20is%20a%0Amultidisciplinary%20field%20aimed%20at%20simplifying%20access%20to%20and%20interpretation%20of%0Aregulatory%20rules%20and%20obligations.%20We%20introduce%20a%20task%20of%20generating%0Aquestion-passages%20pairs%2C%20where%20questions%20are%20automatically%20created%20and%20paired%0Awith%20relevant%20regulatory%20passages%2C%20facilitating%20the%20development%20of%20regulatory%0Aquestion-answering%20systems.%20We%20create%20the%20ObliQA%20dataset%2C%20containing%2027%2C869%0Aquestions%20derived%20from%20the%20collection%20of%20Abu%20Dhabi%20Global%20Markets%20%28ADGM%29%0Afinancial%20regulation%20documents%2C%20design%20a%20baseline%20Regulatory%20Information%0ARetrieval%20and%20Answer%20Generation%20%28RIRAG%29%20system%20and%20evaluate%20it%20with%20RePASs%2C%20a%0Anovel%20evaluation%20metric%20that%20tests%20whether%20generated%20answers%20accurately%20capture%0Aall%20relevant%20obligations%20while%20avoiding%20contradictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05677v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRIRAG%253A%2520Regulatory%2520Information%2520Retrieval%2520and%2520Answer%2520Generation%26entry.906535625%3DTuba%2520Gokhan%2520and%2520Kexin%2520Wang%2520and%2520Iryna%2520Gurevych%2520and%2520Ted%2520Briscoe%26entry.1292438233%3D%2520%2520Regulatory%2520documents%252C%2520issued%2520by%2520governmental%2520regulatory%2520bodies%252C%2520establish%250Arules%252C%2520guidelines%252C%2520and%2520standards%2520that%2520organizations%2520must%2520adhere%2520to%2520for%2520legal%250Acompliance.%2520These%2520documents%252C%2520characterized%2520by%2520their%2520length%252C%2520complexity%2520and%250Afrequent%2520updates%252C%2520are%2520challenging%2520to%2520interpret%252C%2520requiring%2520significant%250Aallocation%2520of%2520time%2520and%2520expertise%2520on%2520the%2520part%2520of%2520organizations%2520to%2520ensure%2520ongoing%250Acompliance.%2520Regulatory%2520Natural%2520Language%2520Processing%2520%2528RegNLP%2529%2520is%2520a%250Amultidisciplinary%2520field%2520aimed%2520at%2520simplifying%2520access%2520to%2520and%2520interpretation%2520of%250Aregulatory%2520rules%2520and%2520obligations.%2520We%2520introduce%2520a%2520task%2520of%2520generating%250Aquestion-passages%2520pairs%252C%2520where%2520questions%2520are%2520automatically%2520created%2520and%2520paired%250Awith%2520relevant%2520regulatory%2520passages%252C%2520facilitating%2520the%2520development%2520of%2520regulatory%250Aquestion-answering%2520systems.%2520We%2520create%2520the%2520ObliQA%2520dataset%252C%2520containing%252027%252C869%250Aquestions%2520derived%2520from%2520the%2520collection%2520of%2520Abu%2520Dhabi%2520Global%2520Markets%2520%2528ADGM%2529%250Afinancial%2520regulation%2520documents%252C%2520design%2520a%2520baseline%2520Regulatory%2520Information%250ARetrieval%2520and%2520Answer%2520Generation%2520%2528RIRAG%2529%2520system%2520and%2520evaluate%2520it%2520with%2520RePASs%252C%2520a%250Anovel%2520evaluation%2520metric%2520that%2520tests%2520whether%2520generated%2520answers%2520accurately%2520capture%250Aall%2520relevant%2520obligations%2520while%2520avoiding%2520contradictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05677v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RIRAG%3A%20Regulatory%20Information%20Retrieval%20and%20Answer%20Generation&entry.906535625=Tuba%20Gokhan%20and%20Kexin%20Wang%20and%20Iryna%20Gurevych%20and%20Ted%20Briscoe&entry.1292438233=%20%20Regulatory%20documents%2C%20issued%20by%20governmental%20regulatory%20bodies%2C%20establish%0Arules%2C%20guidelines%2C%20and%20standards%20that%20organizations%20must%20adhere%20to%20for%20legal%0Acompliance.%20These%20documents%2C%20characterized%20by%20their%20length%2C%20complexity%20and%0Afrequent%20updates%2C%20are%20challenging%20to%20interpret%2C%20requiring%20significant%0Aallocation%20of%20time%20and%20expertise%20on%20the%20part%20of%20organizations%20to%20ensure%20ongoing%0Acompliance.%20Regulatory%20Natural%20Language%20Processing%20%28RegNLP%29%20is%20a%0Amultidisciplinary%20field%20aimed%20at%20simplifying%20access%20to%20and%20interpretation%20of%0Aregulatory%20rules%20and%20obligations.%20We%20introduce%20a%20task%20of%20generating%0Aquestion-passages%20pairs%2C%20where%20questions%20are%20automatically%20created%20and%20paired%0Awith%20relevant%20regulatory%20passages%2C%20facilitating%20the%20development%20of%20regulatory%0Aquestion-answering%20systems.%20We%20create%20the%20ObliQA%20dataset%2C%20containing%2027%2C869%0Aquestions%20derived%20from%20the%20collection%20of%20Abu%20Dhabi%20Global%20Markets%20%28ADGM%29%0Afinancial%20regulation%20documents%2C%20design%20a%20baseline%20Regulatory%20Information%0ARetrieval%20and%20Answer%20Generation%20%28RIRAG%29%20system%20and%20evaluate%20it%20with%20RePASs%2C%20a%0Anovel%20evaluation%20metric%20that%20tests%20whether%20generated%20answers%20accurately%20capture%0Aall%20relevant%20obligations%20while%20avoiding%20contradictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05677v2&entry.124074799=Read"},
{"title": "Nonequilbrium physics of generative diffusion models", "author": "Zhendong Yu and Haiping Huang", "abstract": "  Generative diffusion models apply the concept of Langevin dynamics in physics\nto machine leaning, attracting a lot of interests from engineering, statistics\nand physics, but a complete picture about inherent mechanisms is still lacking.\nIn this paper, we provide a transparent physics analysis of diffusion models,\nformulating the fluctuation theorem, entropy production, equilibrium measure,\nand Franz-Parisi potential to understand the dynamic process and intrinsic\nphase transitions. Our analysis is rooted in a path integral representation of\nboth forward and backward dynamics, and in treating the reverse diffusion\ngenerative process as a statistical inference, where the time-dependent state\nvariables serve as quenched disorder akin to that in spin glass theory. Our\nstudy thus links stochastic thermodynamics, statistical inference and geometry\nbased analysis together to yield a coherent picture about how the generative\ndiffusion models work.\n", "link": "http://arxiv.org/abs/2405.11932v3", "date": "2024-12-02", "relevancy": 1.637, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5607}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.54}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonequilbrium%20physics%20of%20generative%20diffusion%20models&body=Title%3A%20Nonequilbrium%20physics%20of%20generative%20diffusion%20models%0AAuthor%3A%20Zhendong%20Yu%20and%20Haiping%20Huang%0AAbstract%3A%20%20%20Generative%20diffusion%20models%20apply%20the%20concept%20of%20Langevin%20dynamics%20in%20physics%0Ato%20machine%20leaning%2C%20attracting%20a%20lot%20of%20interests%20from%20engineering%2C%20statistics%0Aand%20physics%2C%20but%20a%20complete%20picture%20about%20inherent%20mechanisms%20is%20still%20lacking.%0AIn%20this%20paper%2C%20we%20provide%20a%20transparent%20physics%20analysis%20of%20diffusion%20models%2C%0Aformulating%20the%20fluctuation%20theorem%2C%20entropy%20production%2C%20equilibrium%20measure%2C%0Aand%20Franz-Parisi%20potential%20to%20understand%20the%20dynamic%20process%20and%20intrinsic%0Aphase%20transitions.%20Our%20analysis%20is%20rooted%20in%20a%20path%20integral%20representation%20of%0Aboth%20forward%20and%20backward%20dynamics%2C%20and%20in%20treating%20the%20reverse%20diffusion%0Agenerative%20process%20as%20a%20statistical%20inference%2C%20where%20the%20time-dependent%20state%0Avariables%20serve%20as%20quenched%20disorder%20akin%20to%20that%20in%20spin%20glass%20theory.%20Our%0Astudy%20thus%20links%20stochastic%20thermodynamics%2C%20statistical%20inference%20and%20geometry%0Abased%20analysis%20together%20to%20yield%20a%20coherent%20picture%20about%20how%20the%20generative%0Adiffusion%20models%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11932v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonequilbrium%2520physics%2520of%2520generative%2520diffusion%2520models%26entry.906535625%3DZhendong%2520Yu%2520and%2520Haiping%2520Huang%26entry.1292438233%3D%2520%2520Generative%2520diffusion%2520models%2520apply%2520the%2520concept%2520of%2520Langevin%2520dynamics%2520in%2520physics%250Ato%2520machine%2520leaning%252C%2520attracting%2520a%2520lot%2520of%2520interests%2520from%2520engineering%252C%2520statistics%250Aand%2520physics%252C%2520but%2520a%2520complete%2520picture%2520about%2520inherent%2520mechanisms%2520is%2520still%2520lacking.%250AIn%2520this%2520paper%252C%2520we%2520provide%2520a%2520transparent%2520physics%2520analysis%2520of%2520diffusion%2520models%252C%250Aformulating%2520the%2520fluctuation%2520theorem%252C%2520entropy%2520production%252C%2520equilibrium%2520measure%252C%250Aand%2520Franz-Parisi%2520potential%2520to%2520understand%2520the%2520dynamic%2520process%2520and%2520intrinsic%250Aphase%2520transitions.%2520Our%2520analysis%2520is%2520rooted%2520in%2520a%2520path%2520integral%2520representation%2520of%250Aboth%2520forward%2520and%2520backward%2520dynamics%252C%2520and%2520in%2520treating%2520the%2520reverse%2520diffusion%250Agenerative%2520process%2520as%2520a%2520statistical%2520inference%252C%2520where%2520the%2520time-dependent%2520state%250Avariables%2520serve%2520as%2520quenched%2520disorder%2520akin%2520to%2520that%2520in%2520spin%2520glass%2520theory.%2520Our%250Astudy%2520thus%2520links%2520stochastic%2520thermodynamics%252C%2520statistical%2520inference%2520and%2520geometry%250Abased%2520analysis%2520together%2520to%2520yield%2520a%2520coherent%2520picture%2520about%2520how%2520the%2520generative%250Adiffusion%2520models%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11932v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonequilbrium%20physics%20of%20generative%20diffusion%20models&entry.906535625=Zhendong%20Yu%20and%20Haiping%20Huang&entry.1292438233=%20%20Generative%20diffusion%20models%20apply%20the%20concept%20of%20Langevin%20dynamics%20in%20physics%0Ato%20machine%20leaning%2C%20attracting%20a%20lot%20of%20interests%20from%20engineering%2C%20statistics%0Aand%20physics%2C%20but%20a%20complete%20picture%20about%20inherent%20mechanisms%20is%20still%20lacking.%0AIn%20this%20paper%2C%20we%20provide%20a%20transparent%20physics%20analysis%20of%20diffusion%20models%2C%0Aformulating%20the%20fluctuation%20theorem%2C%20entropy%20production%2C%20equilibrium%20measure%2C%0Aand%20Franz-Parisi%20potential%20to%20understand%20the%20dynamic%20process%20and%20intrinsic%0Aphase%20transitions.%20Our%20analysis%20is%20rooted%20in%20a%20path%20integral%20representation%20of%0Aboth%20forward%20and%20backward%20dynamics%2C%20and%20in%20treating%20the%20reverse%20diffusion%0Agenerative%20process%20as%20a%20statistical%20inference%2C%20where%20the%20time-dependent%20state%0Avariables%20serve%20as%20quenched%20disorder%20akin%20to%20that%20in%20spin%20glass%20theory.%20Our%0Astudy%20thus%20links%20stochastic%20thermodynamics%2C%20statistical%20inference%20and%20geometry%0Abased%20analysis%20together%20to%20yield%20a%20coherent%20picture%20about%20how%20the%20generative%0Adiffusion%20models%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11932v3&entry.124074799=Read"},
{"title": "A Note on Doubly Robust Estimator in Regression Continuity Designs", "author": "Masahiro Kato", "abstract": "  This note introduces a doubly robust (DR) estimator for regression\ndiscontinuity (RD) designs. RD designs provide a quasi-experimental framework\nfor estimating treatment effects, where treatment assignment depends on whether\na running variable surpasses a predefined cutoff. A common approach in RD\nestimation is the use of nonparametric regression methods, such as local linear\nregression. However, the validity of these methods still relies on the\nconsistency of the nonparametric estimators. In this study, we propose the\nDR-RD estimator, which combines two distinct estimators for the conditional\nexpected outcomes. The primary advantage of the DR-RD estimator lies in its\nability to ensure the consistency of the treatment effect estimation as long as\nat least one of the two estimators is consistent. Consequently, our DR-RD\nestimator enhances robustness of treatment effect estimators in RD designs.\n", "link": "http://arxiv.org/abs/2411.07978v3", "date": "2024-12-02", "relevancy": 1.6343, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4348}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4006}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Note%20on%20Doubly%20Robust%20Estimator%20in%20Regression%20Continuity%20Designs&body=Title%3A%20A%20Note%20on%20Doubly%20Robust%20Estimator%20in%20Regression%20Continuity%20Designs%0AAuthor%3A%20Masahiro%20Kato%0AAbstract%3A%20%20%20This%20note%20introduces%20a%20doubly%20robust%20%28DR%29%20estimator%20for%20regression%0Adiscontinuity%20%28RD%29%20designs.%20RD%20designs%20provide%20a%20quasi-experimental%20framework%0Afor%20estimating%20treatment%20effects%2C%20where%20treatment%20assignment%20depends%20on%20whether%0Aa%20running%20variable%20surpasses%20a%20predefined%20cutoff.%20A%20common%20approach%20in%20RD%0Aestimation%20is%20the%20use%20of%20nonparametric%20regression%20methods%2C%20such%20as%20local%20linear%0Aregression.%20However%2C%20the%20validity%20of%20these%20methods%20still%20relies%20on%20the%0Aconsistency%20of%20the%20nonparametric%20estimators.%20In%20this%20study%2C%20we%20propose%20the%0ADR-RD%20estimator%2C%20which%20combines%20two%20distinct%20estimators%20for%20the%20conditional%0Aexpected%20outcomes.%20The%20primary%20advantage%20of%20the%20DR-RD%20estimator%20lies%20in%20its%0Aability%20to%20ensure%20the%20consistency%20of%20the%20treatment%20effect%20estimation%20as%20long%20as%0Aat%20least%20one%20of%20the%20two%20estimators%20is%20consistent.%20Consequently%2C%20our%20DR-RD%0Aestimator%20enhances%20robustness%20of%20treatment%20effect%20estimators%20in%20RD%20designs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07978v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Note%2520on%2520Doubly%2520Robust%2520Estimator%2520in%2520Regression%2520Continuity%2520Designs%26entry.906535625%3DMasahiro%2520Kato%26entry.1292438233%3D%2520%2520This%2520note%2520introduces%2520a%2520doubly%2520robust%2520%2528DR%2529%2520estimator%2520for%2520regression%250Adiscontinuity%2520%2528RD%2529%2520designs.%2520RD%2520designs%2520provide%2520a%2520quasi-experimental%2520framework%250Afor%2520estimating%2520treatment%2520effects%252C%2520where%2520treatment%2520assignment%2520depends%2520on%2520whether%250Aa%2520running%2520variable%2520surpasses%2520a%2520predefined%2520cutoff.%2520A%2520common%2520approach%2520in%2520RD%250Aestimation%2520is%2520the%2520use%2520of%2520nonparametric%2520regression%2520methods%252C%2520such%2520as%2520local%2520linear%250Aregression.%2520However%252C%2520the%2520validity%2520of%2520these%2520methods%2520still%2520relies%2520on%2520the%250Aconsistency%2520of%2520the%2520nonparametric%2520estimators.%2520In%2520this%2520study%252C%2520we%2520propose%2520the%250ADR-RD%2520estimator%252C%2520which%2520combines%2520two%2520distinct%2520estimators%2520for%2520the%2520conditional%250Aexpected%2520outcomes.%2520The%2520primary%2520advantage%2520of%2520the%2520DR-RD%2520estimator%2520lies%2520in%2520its%250Aability%2520to%2520ensure%2520the%2520consistency%2520of%2520the%2520treatment%2520effect%2520estimation%2520as%2520long%2520as%250Aat%2520least%2520one%2520of%2520the%2520two%2520estimators%2520is%2520consistent.%2520Consequently%252C%2520our%2520DR-RD%250Aestimator%2520enhances%2520robustness%2520of%2520treatment%2520effect%2520estimators%2520in%2520RD%2520designs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07978v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Note%20on%20Doubly%20Robust%20Estimator%20in%20Regression%20Continuity%20Designs&entry.906535625=Masahiro%20Kato&entry.1292438233=%20%20This%20note%20introduces%20a%20doubly%20robust%20%28DR%29%20estimator%20for%20regression%0Adiscontinuity%20%28RD%29%20designs.%20RD%20designs%20provide%20a%20quasi-experimental%20framework%0Afor%20estimating%20treatment%20effects%2C%20where%20treatment%20assignment%20depends%20on%20whether%0Aa%20running%20variable%20surpasses%20a%20predefined%20cutoff.%20A%20common%20approach%20in%20RD%0Aestimation%20is%20the%20use%20of%20nonparametric%20regression%20methods%2C%20such%20as%20local%20linear%0Aregression.%20However%2C%20the%20validity%20of%20these%20methods%20still%20relies%20on%20the%0Aconsistency%20of%20the%20nonparametric%20estimators.%20In%20this%20study%2C%20we%20propose%20the%0ADR-RD%20estimator%2C%20which%20combines%20two%20distinct%20estimators%20for%20the%20conditional%0Aexpected%20outcomes.%20The%20primary%20advantage%20of%20the%20DR-RD%20estimator%20lies%20in%20its%0Aability%20to%20ensure%20the%20consistency%20of%20the%20treatment%20effect%20estimation%20as%20long%20as%0Aat%20least%20one%20of%20the%20two%20estimators%20is%20consistent.%20Consequently%2C%20our%20DR-RD%0Aestimator%20enhances%20robustness%20of%20treatment%20effect%20estimators%20in%20RD%20designs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07978v3&entry.124074799=Read"},
{"title": "LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation", "author": "Juntao Jiang and Mengmeng Wang and Huizhong Tian and Lingbo Cheng and Yong Liu", "abstract": "  While large models have achieved significant progress in computer vision,\nchallenges such as optimization complexity, the intricacy of transformer\narchitectures, computational constraints, and practical application demands\nhighlight the importance of simpler model designs in medical image\nsegmentation. This need is particularly pronounced in mobile medical devices,\nwhich require lightweight, deployable models with real-time performance.\nHowever, existing lightweight models often suffer from poor robustness across\ndatasets, limiting their widespread adoption. To address these challenges, this\npaper introduces LV-UNet, a lightweight and vanilla model that leverages\npre-trained MobileNetv3-Large backbones and incorporates fusible modules.\nLV-UNet employs an enhanced deep training strategy and switches to a deployment\nmode during inference by re-parametrization, significantly reducing parameter\ncount and computational overhead. Experimental results on ISIC 2016, BUSI,\nCVC-ClinicDB, CVC-ColonDB, and Kvair-SEG datasets demonstrate a better\ntrade-off between performance and the computational load. The code will be\nreleased at \\url{https://github.com/juntaoJianggavin/LV-UNet}.\n", "link": "http://arxiv.org/abs/2408.16886v3", "date": "2024-12-02", "relevancy": 1.6284, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5668}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5422}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LV-UNet%3A%20A%20Lightweight%20and%20Vanilla%20Model%20for%20Medical%20Image%20Segmentation&body=Title%3A%20LV-UNet%3A%20A%20Lightweight%20and%20Vanilla%20Model%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Juntao%20Jiang%20and%20Mengmeng%20Wang%20and%20Huizhong%20Tian%20and%20Lingbo%20Cheng%20and%20Yong%20Liu%0AAbstract%3A%20%20%20While%20large%20models%20have%20achieved%20significant%20progress%20in%20computer%20vision%2C%0Achallenges%20such%20as%20optimization%20complexity%2C%20the%20intricacy%20of%20transformer%0Aarchitectures%2C%20computational%20constraints%2C%20and%20practical%20application%20demands%0Ahighlight%20the%20importance%20of%20simpler%20model%20designs%20in%20medical%20image%0Asegmentation.%20This%20need%20is%20particularly%20pronounced%20in%20mobile%20medical%20devices%2C%0Awhich%20require%20lightweight%2C%20deployable%20models%20with%20real-time%20performance.%0AHowever%2C%20existing%20lightweight%20models%20often%20suffer%20from%20poor%20robustness%20across%0Adatasets%2C%20limiting%20their%20widespread%20adoption.%20To%20address%20these%20challenges%2C%20this%0Apaper%20introduces%20LV-UNet%2C%20a%20lightweight%20and%20vanilla%20model%20that%20leverages%0Apre-trained%20MobileNetv3-Large%20backbones%20and%20incorporates%20fusible%20modules.%0ALV-UNet%20employs%20an%20enhanced%20deep%20training%20strategy%20and%20switches%20to%20a%20deployment%0Amode%20during%20inference%20by%20re-parametrization%2C%20significantly%20reducing%20parameter%0Acount%20and%20computational%20overhead.%20Experimental%20results%20on%20ISIC%202016%2C%20BUSI%2C%0ACVC-ClinicDB%2C%20CVC-ColonDB%2C%20and%20Kvair-SEG%20datasets%20demonstrate%20a%20better%0Atrade-off%20between%20performance%20and%20the%20computational%20load.%20The%20code%20will%20be%0Areleased%20at%20%5Curl%7Bhttps%3A//github.com/juntaoJianggavin/LV-UNet%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16886v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLV-UNet%253A%2520A%2520Lightweight%2520and%2520Vanilla%2520Model%2520for%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DJuntao%2520Jiang%2520and%2520Mengmeng%2520Wang%2520and%2520Huizhong%2520Tian%2520and%2520Lingbo%2520Cheng%2520and%2520Yong%2520Liu%26entry.1292438233%3D%2520%2520While%2520large%2520models%2520have%2520achieved%2520significant%2520progress%2520in%2520computer%2520vision%252C%250Achallenges%2520such%2520as%2520optimization%2520complexity%252C%2520the%2520intricacy%2520of%2520transformer%250Aarchitectures%252C%2520computational%2520constraints%252C%2520and%2520practical%2520application%2520demands%250Ahighlight%2520the%2520importance%2520of%2520simpler%2520model%2520designs%2520in%2520medical%2520image%250Asegmentation.%2520This%2520need%2520is%2520particularly%2520pronounced%2520in%2520mobile%2520medical%2520devices%252C%250Awhich%2520require%2520lightweight%252C%2520deployable%2520models%2520with%2520real-time%2520performance.%250AHowever%252C%2520existing%2520lightweight%2520models%2520often%2520suffer%2520from%2520poor%2520robustness%2520across%250Adatasets%252C%2520limiting%2520their%2520widespread%2520adoption.%2520To%2520address%2520these%2520challenges%252C%2520this%250Apaper%2520introduces%2520LV-UNet%252C%2520a%2520lightweight%2520and%2520vanilla%2520model%2520that%2520leverages%250Apre-trained%2520MobileNetv3-Large%2520backbones%2520and%2520incorporates%2520fusible%2520modules.%250ALV-UNet%2520employs%2520an%2520enhanced%2520deep%2520training%2520strategy%2520and%2520switches%2520to%2520a%2520deployment%250Amode%2520during%2520inference%2520by%2520re-parametrization%252C%2520significantly%2520reducing%2520parameter%250Acount%2520and%2520computational%2520overhead.%2520Experimental%2520results%2520on%2520ISIC%25202016%252C%2520BUSI%252C%250ACVC-ClinicDB%252C%2520CVC-ColonDB%252C%2520and%2520Kvair-SEG%2520datasets%2520demonstrate%2520a%2520better%250Atrade-off%2520between%2520performance%2520and%2520the%2520computational%2520load.%2520The%2520code%2520will%2520be%250Areleased%2520at%2520%255Curl%257Bhttps%253A//github.com/juntaoJianggavin/LV-UNet%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16886v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LV-UNet%3A%20A%20Lightweight%20and%20Vanilla%20Model%20for%20Medical%20Image%20Segmentation&entry.906535625=Juntao%20Jiang%20and%20Mengmeng%20Wang%20and%20Huizhong%20Tian%20and%20Lingbo%20Cheng%20and%20Yong%20Liu&entry.1292438233=%20%20While%20large%20models%20have%20achieved%20significant%20progress%20in%20computer%20vision%2C%0Achallenges%20such%20as%20optimization%20complexity%2C%20the%20intricacy%20of%20transformer%0Aarchitectures%2C%20computational%20constraints%2C%20and%20practical%20application%20demands%0Ahighlight%20the%20importance%20of%20simpler%20model%20designs%20in%20medical%20image%0Asegmentation.%20This%20need%20is%20particularly%20pronounced%20in%20mobile%20medical%20devices%2C%0Awhich%20require%20lightweight%2C%20deployable%20models%20with%20real-time%20performance.%0AHowever%2C%20existing%20lightweight%20models%20often%20suffer%20from%20poor%20robustness%20across%0Adatasets%2C%20limiting%20their%20widespread%20adoption.%20To%20address%20these%20challenges%2C%20this%0Apaper%20introduces%20LV-UNet%2C%20a%20lightweight%20and%20vanilla%20model%20that%20leverages%0Apre-trained%20MobileNetv3-Large%20backbones%20and%20incorporates%20fusible%20modules.%0ALV-UNet%20employs%20an%20enhanced%20deep%20training%20strategy%20and%20switches%20to%20a%20deployment%0Amode%20during%20inference%20by%20re-parametrization%2C%20significantly%20reducing%20parameter%0Acount%20and%20computational%20overhead.%20Experimental%20results%20on%20ISIC%202016%2C%20BUSI%2C%0ACVC-ClinicDB%2C%20CVC-ColonDB%2C%20and%20Kvair-SEG%20datasets%20demonstrate%20a%20better%0Atrade-off%20between%20performance%20and%20the%20computational%20load.%20The%20code%20will%20be%0Areleased%20at%20%5Curl%7Bhttps%3A//github.com/juntaoJianggavin/LV-UNet%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16886v3&entry.124074799=Read"},
{"title": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head", "author": "Tiancheng Zhao and Peng Liu and Xuan He and Lu Zhang and Kyusong Lee", "abstract": "  End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}\n", "link": "http://arxiv.org/abs/2403.06892v2", "date": "2024-12-02", "relevancy": 1.6228, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5492}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5405}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-time%20Transformer-based%20Open-Vocabulary%20Detection%20with%20Efficient%0A%20%20Fusion%20Head&body=Title%3A%20Real-time%20Transformer-based%20Open-Vocabulary%20Detection%20with%20Efficient%0A%20%20Fusion%20Head%0AAuthor%3A%20Tiancheng%20Zhao%20and%20Peng%20Liu%20and%20Xuan%20He%20and%20Lu%20Zhang%20and%20Kyusong%20Lee%0AAbstract%3A%20%20%20End-to-end%20transformer-based%20detectors%20%28DETRs%29%20have%20shown%20exceptional%0Aperformance%20in%20both%20closed-set%20and%20open-vocabulary%20object%20detection%20%28OVD%29%20tasks%0Athrough%20the%20integration%20of%20language%20modalities.%20However%2C%20their%20demanding%0Acomputational%20requirements%20have%20hindered%20their%20practical%20application%20in%0Areal-time%20object%20detection%20%28OD%29%20scenarios.%20In%20this%20paper%2C%20we%20scrutinize%20the%0Alimitations%20of%20two%20leading%20models%20in%20the%20OVDEval%20benchmark%2C%20OmDet%20and%0AGrounding-DINO%2C%20and%20introduce%20OmDet-Turbo.%20This%20novel%20transformer-based%0Areal-time%20OVD%20model%20features%20an%20innovative%20Efficient%20Fusion%20Head%20%28EFH%29%20module%0Adesigned%20to%20alleviate%20the%20bottlenecks%20observed%20in%20OmDet%20and%20Grounding-DINO.%0ANotably%2C%20OmDet-Turbo-Base%20achieves%20a%20100.2%20frames%20per%20second%20%28FPS%29%20with%0ATensorRT%20and%20language%20cache%20techniques%20applied.%20Notably%2C%20in%20zero-shot%20scenarios%0Aon%20COCO%20and%20LVIS%20datasets%2C%20OmDet-Turbo%20achieves%20performance%20levels%20nearly%20on%0Apar%20with%20current%20state-of-the-art%20supervised%20models.%20Furthermore%2C%20it%0Aestablishes%20new%20state-of-the-art%20benchmarks%20on%20ODinW%20and%20OVDEval%2C%20boasting%20an%0AAP%20of%2030.1%20and%20an%20NMS-AP%20of%2026.86%2C%20respectively.%20The%20practicality%20of%0AOmDet-Turbo%20in%20industrial%20applications%20is%20underscored%20by%20its%20exceptional%0Aperformance%20on%20benchmark%20datasets%20and%20superior%20inference%20speed%2C%20positioning%20it%0Aas%20a%20compelling%20choice%20for%20real-time%20object%20detection%20tasks.%20Code%3A%0A%5Curl%7Bhttps%3A//github.com/om-ai-lab/OmDet%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06892v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-time%2520Transformer-based%2520Open-Vocabulary%2520Detection%2520with%2520Efficient%250A%2520%2520Fusion%2520Head%26entry.906535625%3DTiancheng%2520Zhao%2520and%2520Peng%2520Liu%2520and%2520Xuan%2520He%2520and%2520Lu%2520Zhang%2520and%2520Kyusong%2520Lee%26entry.1292438233%3D%2520%2520End-to-end%2520transformer-based%2520detectors%2520%2528DETRs%2529%2520have%2520shown%2520exceptional%250Aperformance%2520in%2520both%2520closed-set%2520and%2520open-vocabulary%2520object%2520detection%2520%2528OVD%2529%2520tasks%250Athrough%2520the%2520integration%2520of%2520language%2520modalities.%2520However%252C%2520their%2520demanding%250Acomputational%2520requirements%2520have%2520hindered%2520their%2520practical%2520application%2520in%250Areal-time%2520object%2520detection%2520%2528OD%2529%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520scrutinize%2520the%250Alimitations%2520of%2520two%2520leading%2520models%2520in%2520the%2520OVDEval%2520benchmark%252C%2520OmDet%2520and%250AGrounding-DINO%252C%2520and%2520introduce%2520OmDet-Turbo.%2520This%2520novel%2520transformer-based%250Areal-time%2520OVD%2520model%2520features%2520an%2520innovative%2520Efficient%2520Fusion%2520Head%2520%2528EFH%2529%2520module%250Adesigned%2520to%2520alleviate%2520the%2520bottlenecks%2520observed%2520in%2520OmDet%2520and%2520Grounding-DINO.%250ANotably%252C%2520OmDet-Turbo-Base%2520achieves%2520a%2520100.2%2520frames%2520per%2520second%2520%2528FPS%2529%2520with%250ATensorRT%2520and%2520language%2520cache%2520techniques%2520applied.%2520Notably%252C%2520in%2520zero-shot%2520scenarios%250Aon%2520COCO%2520and%2520LVIS%2520datasets%252C%2520OmDet-Turbo%2520achieves%2520performance%2520levels%2520nearly%2520on%250Apar%2520with%2520current%2520state-of-the-art%2520supervised%2520models.%2520Furthermore%252C%2520it%250Aestablishes%2520new%2520state-of-the-art%2520benchmarks%2520on%2520ODinW%2520and%2520OVDEval%252C%2520boasting%2520an%250AAP%2520of%252030.1%2520and%2520an%2520NMS-AP%2520of%252026.86%252C%2520respectively.%2520The%2520practicality%2520of%250AOmDet-Turbo%2520in%2520industrial%2520applications%2520is%2520underscored%2520by%2520its%2520exceptional%250Aperformance%2520on%2520benchmark%2520datasets%2520and%2520superior%2520inference%2520speed%252C%2520positioning%2520it%250Aas%2520a%2520compelling%2520choice%2520for%2520real-time%2520object%2520detection%2520tasks.%2520Code%253A%250A%255Curl%257Bhttps%253A//github.com/om-ai-lab/OmDet%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06892v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%20Transformer-based%20Open-Vocabulary%20Detection%20with%20Efficient%0A%20%20Fusion%20Head&entry.906535625=Tiancheng%20Zhao%20and%20Peng%20Liu%20and%20Xuan%20He%20and%20Lu%20Zhang%20and%20Kyusong%20Lee&entry.1292438233=%20%20End-to-end%20transformer-based%20detectors%20%28DETRs%29%20have%20shown%20exceptional%0Aperformance%20in%20both%20closed-set%20and%20open-vocabulary%20object%20detection%20%28OVD%29%20tasks%0Athrough%20the%20integration%20of%20language%20modalities.%20However%2C%20their%20demanding%0Acomputational%20requirements%20have%20hindered%20their%20practical%20application%20in%0Areal-time%20object%20detection%20%28OD%29%20scenarios.%20In%20this%20paper%2C%20we%20scrutinize%20the%0Alimitations%20of%20two%20leading%20models%20in%20the%20OVDEval%20benchmark%2C%20OmDet%20and%0AGrounding-DINO%2C%20and%20introduce%20OmDet-Turbo.%20This%20novel%20transformer-based%0Areal-time%20OVD%20model%20features%20an%20innovative%20Efficient%20Fusion%20Head%20%28EFH%29%20module%0Adesigned%20to%20alleviate%20the%20bottlenecks%20observed%20in%20OmDet%20and%20Grounding-DINO.%0ANotably%2C%20OmDet-Turbo-Base%20achieves%20a%20100.2%20frames%20per%20second%20%28FPS%29%20with%0ATensorRT%20and%20language%20cache%20techniques%20applied.%20Notably%2C%20in%20zero-shot%20scenarios%0Aon%20COCO%20and%20LVIS%20datasets%2C%20OmDet-Turbo%20achieves%20performance%20levels%20nearly%20on%0Apar%20with%20current%20state-of-the-art%20supervised%20models.%20Furthermore%2C%20it%0Aestablishes%20new%20state-of-the-art%20benchmarks%20on%20ODinW%20and%20OVDEval%2C%20boasting%20an%0AAP%20of%2030.1%20and%20an%20NMS-AP%20of%2026.86%2C%20respectively.%20The%20practicality%20of%0AOmDet-Turbo%20in%20industrial%20applications%20is%20underscored%20by%20its%20exceptional%0Aperformance%20on%20benchmark%20datasets%20and%20superior%20inference%20speed%2C%20positioning%20it%0Aas%20a%20compelling%20choice%20for%20real-time%20object%20detection%20tasks.%20Code%3A%0A%5Curl%7Bhttps%3A//github.com/om-ai-lab/OmDet%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06892v2&entry.124074799=Read"},
{"title": "State Estimation for Continuum Multi-Robot Systems on SE(3)", "author": "Sven Lilge and Timothy D. Barfoot and Jessica Burgner-Kahrs", "abstract": "  In contrast to conventional robots, accurately modeling the kinematics and\nstatics of continuum robots is challenging due to partially unknown material\nproperties, parasitic effects, or unknown forces acting on the continuous body.\nConsequentially, state estimation approaches that utilize additional sensor\ninformation to predict the shape of continuum robots have garnered significant\ninterest. This paper presents a novel approach to state estimation for systems\nwith multiple coupled continuum robots, which allows estimating the shape and\nstrain variables of multiple continuum robots in an arbitrary coupled topology.\nSimulations and experiments demonstrate the capabilities and versatility of the\nproposed method, while achieving accurate and continuous estimates for the\nstate of such systems, resulting in average end-effector errors of 3.3 mm and\n5.02{\\deg} depending on the sensor setup. It is further shown, that the\napproach offers fast computation times of below 10 ms, enabling its utilization\nin quasi-static real-time scenarios with average update rates of 100-200 Hz. An\nopen-source C++ implementation of the proposed state estimation method is made\npublicly available to the community.\n", "link": "http://arxiv.org/abs/2401.13540v2", "date": "2024-12-02", "relevancy": 1.5901, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6382}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5254}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20State%20Estimation%20for%20Continuum%20Multi-Robot%20Systems%20on%20SE%283%29&body=Title%3A%20State%20Estimation%20for%20Continuum%20Multi-Robot%20Systems%20on%20SE%283%29%0AAuthor%3A%20Sven%20Lilge%20and%20Timothy%20D.%20Barfoot%20and%20Jessica%20Burgner-Kahrs%0AAbstract%3A%20%20%20In%20contrast%20to%20conventional%20robots%2C%20accurately%20modeling%20the%20kinematics%20and%0Astatics%20of%20continuum%20robots%20is%20challenging%20due%20to%20partially%20unknown%20material%0Aproperties%2C%20parasitic%20effects%2C%20or%20unknown%20forces%20acting%20on%20the%20continuous%20body.%0AConsequentially%2C%20state%20estimation%20approaches%20that%20utilize%20additional%20sensor%0Ainformation%20to%20predict%20the%20shape%20of%20continuum%20robots%20have%20garnered%20significant%0Ainterest.%20This%20paper%20presents%20a%20novel%20approach%20to%20state%20estimation%20for%20systems%0Awith%20multiple%20coupled%20continuum%20robots%2C%20which%20allows%20estimating%20the%20shape%20and%0Astrain%20variables%20of%20multiple%20continuum%20robots%20in%20an%20arbitrary%20coupled%20topology.%0ASimulations%20and%20experiments%20demonstrate%20the%20capabilities%20and%20versatility%20of%20the%0Aproposed%20method%2C%20while%20achieving%20accurate%20and%20continuous%20estimates%20for%20the%0Astate%20of%20such%20systems%2C%20resulting%20in%20average%20end-effector%20errors%20of%203.3%20mm%20and%0A5.02%7B%5Cdeg%7D%20depending%20on%20the%20sensor%20setup.%20It%20is%20further%20shown%2C%20that%20the%0Aapproach%20offers%20fast%20computation%20times%20of%20below%2010%20ms%2C%20enabling%20its%20utilization%0Ain%20quasi-static%20real-time%20scenarios%20with%20average%20update%20rates%20of%20100-200%20Hz.%20An%0Aopen-source%20C%2B%2B%20implementation%20of%20the%20proposed%20state%20estimation%20method%20is%20made%0Apublicly%20available%20to%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.13540v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DState%2520Estimation%2520for%2520Continuum%2520Multi-Robot%2520Systems%2520on%2520SE%25283%2529%26entry.906535625%3DSven%2520Lilge%2520and%2520Timothy%2520D.%2520Barfoot%2520and%2520Jessica%2520Burgner-Kahrs%26entry.1292438233%3D%2520%2520In%2520contrast%2520to%2520conventional%2520robots%252C%2520accurately%2520modeling%2520the%2520kinematics%2520and%250Astatics%2520of%2520continuum%2520robots%2520is%2520challenging%2520due%2520to%2520partially%2520unknown%2520material%250Aproperties%252C%2520parasitic%2520effects%252C%2520or%2520unknown%2520forces%2520acting%2520on%2520the%2520continuous%2520body.%250AConsequentially%252C%2520state%2520estimation%2520approaches%2520that%2520utilize%2520additional%2520sensor%250Ainformation%2520to%2520predict%2520the%2520shape%2520of%2520continuum%2520robots%2520have%2520garnered%2520significant%250Ainterest.%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520to%2520state%2520estimation%2520for%2520systems%250Awith%2520multiple%2520coupled%2520continuum%2520robots%252C%2520which%2520allows%2520estimating%2520the%2520shape%2520and%250Astrain%2520variables%2520of%2520multiple%2520continuum%2520robots%2520in%2520an%2520arbitrary%2520coupled%2520topology.%250ASimulations%2520and%2520experiments%2520demonstrate%2520the%2520capabilities%2520and%2520versatility%2520of%2520the%250Aproposed%2520method%252C%2520while%2520achieving%2520accurate%2520and%2520continuous%2520estimates%2520for%2520the%250Astate%2520of%2520such%2520systems%252C%2520resulting%2520in%2520average%2520end-effector%2520errors%2520of%25203.3%2520mm%2520and%250A5.02%257B%255Cdeg%257D%2520depending%2520on%2520the%2520sensor%2520setup.%2520It%2520is%2520further%2520shown%252C%2520that%2520the%250Aapproach%2520offers%2520fast%2520computation%2520times%2520of%2520below%252010%2520ms%252C%2520enabling%2520its%2520utilization%250Ain%2520quasi-static%2520real-time%2520scenarios%2520with%2520average%2520update%2520rates%2520of%2520100-200%2520Hz.%2520An%250Aopen-source%2520C%252B%252B%2520implementation%2520of%2520the%2520proposed%2520state%2520estimation%2520method%2520is%2520made%250Apublicly%2520available%2520to%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.13540v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=State%20Estimation%20for%20Continuum%20Multi-Robot%20Systems%20on%20SE%283%29&entry.906535625=Sven%20Lilge%20and%20Timothy%20D.%20Barfoot%20and%20Jessica%20Burgner-Kahrs&entry.1292438233=%20%20In%20contrast%20to%20conventional%20robots%2C%20accurately%20modeling%20the%20kinematics%20and%0Astatics%20of%20continuum%20robots%20is%20challenging%20due%20to%20partially%20unknown%20material%0Aproperties%2C%20parasitic%20effects%2C%20or%20unknown%20forces%20acting%20on%20the%20continuous%20body.%0AConsequentially%2C%20state%20estimation%20approaches%20that%20utilize%20additional%20sensor%0Ainformation%20to%20predict%20the%20shape%20of%20continuum%20robots%20have%20garnered%20significant%0Ainterest.%20This%20paper%20presents%20a%20novel%20approach%20to%20state%20estimation%20for%20systems%0Awith%20multiple%20coupled%20continuum%20robots%2C%20which%20allows%20estimating%20the%20shape%20and%0Astrain%20variables%20of%20multiple%20continuum%20robots%20in%20an%20arbitrary%20coupled%20topology.%0ASimulations%20and%20experiments%20demonstrate%20the%20capabilities%20and%20versatility%20of%20the%0Aproposed%20method%2C%20while%20achieving%20accurate%20and%20continuous%20estimates%20for%20the%0Astate%20of%20such%20systems%2C%20resulting%20in%20average%20end-effector%20errors%20of%203.3%20mm%20and%0A5.02%7B%5Cdeg%7D%20depending%20on%20the%20sensor%20setup.%20It%20is%20further%20shown%2C%20that%20the%0Aapproach%20offers%20fast%20computation%20times%20of%20below%2010%20ms%2C%20enabling%20its%20utilization%0Ain%20quasi-static%20real-time%20scenarios%20with%20average%20update%20rates%20of%20100-200%20Hz.%20An%0Aopen-source%20C%2B%2B%20implementation%20of%20the%20proposed%20state%20estimation%20method%20is%20made%0Apublicly%20available%20to%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13540v2&entry.124074799=Read"},
{"title": "MASP: Scalable GNN-based Planning for Multi-Agent Navigation", "author": "Xinyi Yang and Xinting Yang and Chao Yu and Jiayu Chen and Wenbo Ding and Huazhong Yang and Yu Wang", "abstract": "  We investigate multi-agent navigation tasks, where multiple agents need to\nreach initially unassigned goals in a limited time. Classical planning-based\nmethods suffer from expensive computation overhead at each step and offer\nlimited expressiveness for complex cooperation strategies. In contrast,\nreinforcement learning (RL) has recently become a popular approach for\naddressing this issue. However, RL struggles with low data efficiency and\ncooperation when directly exploring (nearly) optimal policies in a large\nexploration space, especially with an increased number of agents(e.g., 10+\nagents) or in complex environments (e.g., 3-D simulators). In this paper, we\npropose the Multi-Agent Scalable Graph-based Planner (MASP), a goal-conditioned\nhierarchical planner for navigation tasks with a substantial number of agents\nin the decentralized setting. MASP employs a hierarchical framework to reduce\nspace complexity by decomposing a large exploration space into multiple\ngoal-conditioned subspaces, where a high-level policy assigns agents goals, and\na low-level policy navigates agents toward designated goals. For agent\ncooperation and the adaptation to varying team sizes, we model agents and goals\nas graphs to better capture their relationship. The high-level policy, the Goal\nMatcher, leverages a graph-based Self-Encoder and Cross-Encoder to optimize\ngoal assignment by updating the agent and the goal graphs. The low-level\npolicy, the Coordinated Action Executor, introduces the Group Information\nFusion to facilitate group division and extract agent relationships across\ngroups, enhancing training efficiency for agent cooperation. The results\ndemonstrate that MASP outperforms RL and planning-based baselines in task\nefficiency.\n", "link": "http://arxiv.org/abs/2312.02522v2", "date": "2024-12-02", "relevancy": 1.588, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5847}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5261}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MASP%3A%20Scalable%20GNN-based%20Planning%20for%20Multi-Agent%20Navigation&body=Title%3A%20MASP%3A%20Scalable%20GNN-based%20Planning%20for%20Multi-Agent%20Navigation%0AAuthor%3A%20Xinyi%20Yang%20and%20Xinting%20Yang%20and%20Chao%20Yu%20and%20Jiayu%20Chen%20and%20Wenbo%20Ding%20and%20Huazhong%20Yang%20and%20Yu%20Wang%0AAbstract%3A%20%20%20We%20investigate%20multi-agent%20navigation%20tasks%2C%20where%20multiple%20agents%20need%20to%0Areach%20initially%20unassigned%20goals%20in%20a%20limited%20time.%20Classical%20planning-based%0Amethods%20suffer%20from%20expensive%20computation%20overhead%20at%20each%20step%20and%20offer%0Alimited%20expressiveness%20for%20complex%20cooperation%20strategies.%20In%20contrast%2C%0Areinforcement%20learning%20%28RL%29%20has%20recently%20become%20a%20popular%20approach%20for%0Aaddressing%20this%20issue.%20However%2C%20RL%20struggles%20with%20low%20data%20efficiency%20and%0Acooperation%20when%20directly%20exploring%20%28nearly%29%20optimal%20policies%20in%20a%20large%0Aexploration%20space%2C%20especially%20with%20an%20increased%20number%20of%20agents%28e.g.%2C%2010%2B%0Aagents%29%20or%20in%20complex%20environments%20%28e.g.%2C%203-D%20simulators%29.%20In%20this%20paper%2C%20we%0Apropose%20the%20Multi-Agent%20Scalable%20Graph-based%20Planner%20%28MASP%29%2C%20a%20goal-conditioned%0Ahierarchical%20planner%20for%20navigation%20tasks%20with%20a%20substantial%20number%20of%20agents%0Ain%20the%20decentralized%20setting.%20MASP%20employs%20a%20hierarchical%20framework%20to%20reduce%0Aspace%20complexity%20by%20decomposing%20a%20large%20exploration%20space%20into%20multiple%0Agoal-conditioned%20subspaces%2C%20where%20a%20high-level%20policy%20assigns%20agents%20goals%2C%20and%0Aa%20low-level%20policy%20navigates%20agents%20toward%20designated%20goals.%20For%20agent%0Acooperation%20and%20the%20adaptation%20to%20varying%20team%20sizes%2C%20we%20model%20agents%20and%20goals%0Aas%20graphs%20to%20better%20capture%20their%20relationship.%20The%20high-level%20policy%2C%20the%20Goal%0AMatcher%2C%20leverages%20a%20graph-based%20Self-Encoder%20and%20Cross-Encoder%20to%20optimize%0Agoal%20assignment%20by%20updating%20the%20agent%20and%20the%20goal%20graphs.%20The%20low-level%0Apolicy%2C%20the%20Coordinated%20Action%20Executor%2C%20introduces%20the%20Group%20Information%0AFusion%20to%20facilitate%20group%20division%20and%20extract%20agent%20relationships%20across%0Agroups%2C%20enhancing%20training%20efficiency%20for%20agent%20cooperation.%20The%20results%0Ademonstrate%20that%20MASP%20outperforms%20RL%20and%20planning-based%20baselines%20in%20task%0Aefficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02522v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMASP%253A%2520Scalable%2520GNN-based%2520Planning%2520for%2520Multi-Agent%2520Navigation%26entry.906535625%3DXinyi%2520Yang%2520and%2520Xinting%2520Yang%2520and%2520Chao%2520Yu%2520and%2520Jiayu%2520Chen%2520and%2520Wenbo%2520Ding%2520and%2520Huazhong%2520Yang%2520and%2520Yu%2520Wang%26entry.1292438233%3D%2520%2520We%2520investigate%2520multi-agent%2520navigation%2520tasks%252C%2520where%2520multiple%2520agents%2520need%2520to%250Areach%2520initially%2520unassigned%2520goals%2520in%2520a%2520limited%2520time.%2520Classical%2520planning-based%250Amethods%2520suffer%2520from%2520expensive%2520computation%2520overhead%2520at%2520each%2520step%2520and%2520offer%250Alimited%2520expressiveness%2520for%2520complex%2520cooperation%2520strategies.%2520In%2520contrast%252C%250Areinforcement%2520learning%2520%2528RL%2529%2520has%2520recently%2520become%2520a%2520popular%2520approach%2520for%250Aaddressing%2520this%2520issue.%2520However%252C%2520RL%2520struggles%2520with%2520low%2520data%2520efficiency%2520and%250Acooperation%2520when%2520directly%2520exploring%2520%2528nearly%2529%2520optimal%2520policies%2520in%2520a%2520large%250Aexploration%2520space%252C%2520especially%2520with%2520an%2520increased%2520number%2520of%2520agents%2528e.g.%252C%252010%252B%250Aagents%2529%2520or%2520in%2520complex%2520environments%2520%2528e.g.%252C%25203-D%2520simulators%2529.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520the%2520Multi-Agent%2520Scalable%2520Graph-based%2520Planner%2520%2528MASP%2529%252C%2520a%2520goal-conditioned%250Ahierarchical%2520planner%2520for%2520navigation%2520tasks%2520with%2520a%2520substantial%2520number%2520of%2520agents%250Ain%2520the%2520decentralized%2520setting.%2520MASP%2520employs%2520a%2520hierarchical%2520framework%2520to%2520reduce%250Aspace%2520complexity%2520by%2520decomposing%2520a%2520large%2520exploration%2520space%2520into%2520multiple%250Agoal-conditioned%2520subspaces%252C%2520where%2520a%2520high-level%2520policy%2520assigns%2520agents%2520goals%252C%2520and%250Aa%2520low-level%2520policy%2520navigates%2520agents%2520toward%2520designated%2520goals.%2520For%2520agent%250Acooperation%2520and%2520the%2520adaptation%2520to%2520varying%2520team%2520sizes%252C%2520we%2520model%2520agents%2520and%2520goals%250Aas%2520graphs%2520to%2520better%2520capture%2520their%2520relationship.%2520The%2520high-level%2520policy%252C%2520the%2520Goal%250AMatcher%252C%2520leverages%2520a%2520graph-based%2520Self-Encoder%2520and%2520Cross-Encoder%2520to%2520optimize%250Agoal%2520assignment%2520by%2520updating%2520the%2520agent%2520and%2520the%2520goal%2520graphs.%2520The%2520low-level%250Apolicy%252C%2520the%2520Coordinated%2520Action%2520Executor%252C%2520introduces%2520the%2520Group%2520Information%250AFusion%2520to%2520facilitate%2520group%2520division%2520and%2520extract%2520agent%2520relationships%2520across%250Agroups%252C%2520enhancing%2520training%2520efficiency%2520for%2520agent%2520cooperation.%2520The%2520results%250Ademonstrate%2520that%2520MASP%2520outperforms%2520RL%2520and%2520planning-based%2520baselines%2520in%2520task%250Aefficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02522v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MASP%3A%20Scalable%20GNN-based%20Planning%20for%20Multi-Agent%20Navigation&entry.906535625=Xinyi%20Yang%20and%20Xinting%20Yang%20and%20Chao%20Yu%20and%20Jiayu%20Chen%20and%20Wenbo%20Ding%20and%20Huazhong%20Yang%20and%20Yu%20Wang&entry.1292438233=%20%20We%20investigate%20multi-agent%20navigation%20tasks%2C%20where%20multiple%20agents%20need%20to%0Areach%20initially%20unassigned%20goals%20in%20a%20limited%20time.%20Classical%20planning-based%0Amethods%20suffer%20from%20expensive%20computation%20overhead%20at%20each%20step%20and%20offer%0Alimited%20expressiveness%20for%20complex%20cooperation%20strategies.%20In%20contrast%2C%0Areinforcement%20learning%20%28RL%29%20has%20recently%20become%20a%20popular%20approach%20for%0Aaddressing%20this%20issue.%20However%2C%20RL%20struggles%20with%20low%20data%20efficiency%20and%0Acooperation%20when%20directly%20exploring%20%28nearly%29%20optimal%20policies%20in%20a%20large%0Aexploration%20space%2C%20especially%20with%20an%20increased%20number%20of%20agents%28e.g.%2C%2010%2B%0Aagents%29%20or%20in%20complex%20environments%20%28e.g.%2C%203-D%20simulators%29.%20In%20this%20paper%2C%20we%0Apropose%20the%20Multi-Agent%20Scalable%20Graph-based%20Planner%20%28MASP%29%2C%20a%20goal-conditioned%0Ahierarchical%20planner%20for%20navigation%20tasks%20with%20a%20substantial%20number%20of%20agents%0Ain%20the%20decentralized%20setting.%20MASP%20employs%20a%20hierarchical%20framework%20to%20reduce%0Aspace%20complexity%20by%20decomposing%20a%20large%20exploration%20space%20into%20multiple%0Agoal-conditioned%20subspaces%2C%20where%20a%20high-level%20policy%20assigns%20agents%20goals%2C%20and%0Aa%20low-level%20policy%20navigates%20agents%20toward%20designated%20goals.%20For%20agent%0Acooperation%20and%20the%20adaptation%20to%20varying%20team%20sizes%2C%20we%20model%20agents%20and%20goals%0Aas%20graphs%20to%20better%20capture%20their%20relationship.%20The%20high-level%20policy%2C%20the%20Goal%0AMatcher%2C%20leverages%20a%20graph-based%20Self-Encoder%20and%20Cross-Encoder%20to%20optimize%0Agoal%20assignment%20by%20updating%20the%20agent%20and%20the%20goal%20graphs.%20The%20low-level%0Apolicy%2C%20the%20Coordinated%20Action%20Executor%2C%20introduces%20the%20Group%20Information%0AFusion%20to%20facilitate%20group%20division%20and%20extract%20agent%20relationships%20across%0Agroups%2C%20enhancing%20training%20efficiency%20for%20agent%20cooperation.%20The%20results%0Ademonstrate%20that%20MASP%20outperforms%20RL%20and%20planning-based%20baselines%20in%20task%0Aefficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02522v2&entry.124074799=Read"},
{"title": "Two Tales of Single-Phase Contrastive Hebbian Learning", "author": "Rasmus Kj\u00e6r H\u00f8ier and Christopher Zach", "abstract": "  The search for ``biologically plausible'' learning algorithms has converged\non the idea of representing gradients as activity differences. However, most\napproaches require a high degree of synchronization (distinct phases during\nlearning) and introduce substantial computational overhead, which raises doubts\nregarding their biological plausibility as well as their potential utility for\nneuromorphic computing. Furthermore, they commonly rely on applying\ninfinitesimal perturbations (nudges) to output units, which is impractical in\nnoisy environments. Recently it has been shown that by modelling artificial\nneurons as dyads with two oppositely nudged compartments, it is possible for a\nfully local learning algorithm named ``dual propagation'' to bridge the\nperformance gap to backpropagation, without requiring separate learning phases\nor infinitesimal nudging. However, the algorithm has the drawback that its\nnumerical stability relies on symmetric nudging, which may be restrictive in\nbiological and analog implementations. In this work we first provide a solid\nfoundation for the objective underlying the dual propagation method, which also\nreveals a surprising connection with adversarial robustness. Second, we\ndemonstrate how dual propagation is related to a particular adjoint state\nmethod, which is stable regardless of asymmetric nudging.\n", "link": "http://arxiv.org/abs/2402.08573v3", "date": "2024-12-02", "relevancy": 1.5522, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5234}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5223}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two%20Tales%20of%20Single-Phase%20Contrastive%20Hebbian%20Learning&body=Title%3A%20Two%20Tales%20of%20Single-Phase%20Contrastive%20Hebbian%20Learning%0AAuthor%3A%20Rasmus%20Kj%C3%A6r%20H%C3%B8ier%20and%20Christopher%20Zach%0AAbstract%3A%20%20%20The%20search%20for%20%60%60biologically%20plausible%27%27%20learning%20algorithms%20has%20converged%0Aon%20the%20idea%20of%20representing%20gradients%20as%20activity%20differences.%20However%2C%20most%0Aapproaches%20require%20a%20high%20degree%20of%20synchronization%20%28distinct%20phases%20during%0Alearning%29%20and%20introduce%20substantial%20computational%20overhead%2C%20which%20raises%20doubts%0Aregarding%20their%20biological%20plausibility%20as%20well%20as%20their%20potential%20utility%20for%0Aneuromorphic%20computing.%20Furthermore%2C%20they%20commonly%20rely%20on%20applying%0Ainfinitesimal%20perturbations%20%28nudges%29%20to%20output%20units%2C%20which%20is%20impractical%20in%0Anoisy%20environments.%20Recently%20it%20has%20been%20shown%20that%20by%20modelling%20artificial%0Aneurons%20as%20dyads%20with%20two%20oppositely%20nudged%20compartments%2C%20it%20is%20possible%20for%20a%0Afully%20local%20learning%20algorithm%20named%20%60%60dual%20propagation%27%27%20to%20bridge%20the%0Aperformance%20gap%20to%20backpropagation%2C%20without%20requiring%20separate%20learning%20phases%0Aor%20infinitesimal%20nudging.%20However%2C%20the%20algorithm%20has%20the%20drawback%20that%20its%0Anumerical%20stability%20relies%20on%20symmetric%20nudging%2C%20which%20may%20be%20restrictive%20in%0Abiological%20and%20analog%20implementations.%20In%20this%20work%20we%20first%20provide%20a%20solid%0Afoundation%20for%20the%20objective%20underlying%20the%20dual%20propagation%20method%2C%20which%20also%0Areveals%20a%20surprising%20connection%20with%20adversarial%20robustness.%20Second%2C%20we%0Ademonstrate%20how%20dual%20propagation%20is%20related%20to%20a%20particular%20adjoint%20state%0Amethod%2C%20which%20is%20stable%20regardless%20of%20asymmetric%20nudging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08573v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo%2520Tales%2520of%2520Single-Phase%2520Contrastive%2520Hebbian%2520Learning%26entry.906535625%3DRasmus%2520Kj%25C3%25A6r%2520H%25C3%25B8ier%2520and%2520Christopher%2520Zach%26entry.1292438233%3D%2520%2520The%2520search%2520for%2520%2560%2560biologically%2520plausible%2527%2527%2520learning%2520algorithms%2520has%2520converged%250Aon%2520the%2520idea%2520of%2520representing%2520gradients%2520as%2520activity%2520differences.%2520However%252C%2520most%250Aapproaches%2520require%2520a%2520high%2520degree%2520of%2520synchronization%2520%2528distinct%2520phases%2520during%250Alearning%2529%2520and%2520introduce%2520substantial%2520computational%2520overhead%252C%2520which%2520raises%2520doubts%250Aregarding%2520their%2520biological%2520plausibility%2520as%2520well%2520as%2520their%2520potential%2520utility%2520for%250Aneuromorphic%2520computing.%2520Furthermore%252C%2520they%2520commonly%2520rely%2520on%2520applying%250Ainfinitesimal%2520perturbations%2520%2528nudges%2529%2520to%2520output%2520units%252C%2520which%2520is%2520impractical%2520in%250Anoisy%2520environments.%2520Recently%2520it%2520has%2520been%2520shown%2520that%2520by%2520modelling%2520artificial%250Aneurons%2520as%2520dyads%2520with%2520two%2520oppositely%2520nudged%2520compartments%252C%2520it%2520is%2520possible%2520for%2520a%250Afully%2520local%2520learning%2520algorithm%2520named%2520%2560%2560dual%2520propagation%2527%2527%2520to%2520bridge%2520the%250Aperformance%2520gap%2520to%2520backpropagation%252C%2520without%2520requiring%2520separate%2520learning%2520phases%250Aor%2520infinitesimal%2520nudging.%2520However%252C%2520the%2520algorithm%2520has%2520the%2520drawback%2520that%2520its%250Anumerical%2520stability%2520relies%2520on%2520symmetric%2520nudging%252C%2520which%2520may%2520be%2520restrictive%2520in%250Abiological%2520and%2520analog%2520implementations.%2520In%2520this%2520work%2520we%2520first%2520provide%2520a%2520solid%250Afoundation%2520for%2520the%2520objective%2520underlying%2520the%2520dual%2520propagation%2520method%252C%2520which%2520also%250Areveals%2520a%2520surprising%2520connection%2520with%2520adversarial%2520robustness.%2520Second%252C%2520we%250Ademonstrate%2520how%2520dual%2520propagation%2520is%2520related%2520to%2520a%2520particular%2520adjoint%2520state%250Amethod%252C%2520which%2520is%2520stable%2520regardless%2520of%2520asymmetric%2520nudging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08573v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two%20Tales%20of%20Single-Phase%20Contrastive%20Hebbian%20Learning&entry.906535625=Rasmus%20Kj%C3%A6r%20H%C3%B8ier%20and%20Christopher%20Zach&entry.1292438233=%20%20The%20search%20for%20%60%60biologically%20plausible%27%27%20learning%20algorithms%20has%20converged%0Aon%20the%20idea%20of%20representing%20gradients%20as%20activity%20differences.%20However%2C%20most%0Aapproaches%20require%20a%20high%20degree%20of%20synchronization%20%28distinct%20phases%20during%0Alearning%29%20and%20introduce%20substantial%20computational%20overhead%2C%20which%20raises%20doubts%0Aregarding%20their%20biological%20plausibility%20as%20well%20as%20their%20potential%20utility%20for%0Aneuromorphic%20computing.%20Furthermore%2C%20they%20commonly%20rely%20on%20applying%0Ainfinitesimal%20perturbations%20%28nudges%29%20to%20output%20units%2C%20which%20is%20impractical%20in%0Anoisy%20environments.%20Recently%20it%20has%20been%20shown%20that%20by%20modelling%20artificial%0Aneurons%20as%20dyads%20with%20two%20oppositely%20nudged%20compartments%2C%20it%20is%20possible%20for%20a%0Afully%20local%20learning%20algorithm%20named%20%60%60dual%20propagation%27%27%20to%20bridge%20the%0Aperformance%20gap%20to%20backpropagation%2C%20without%20requiring%20separate%20learning%20phases%0Aor%20infinitesimal%20nudging.%20However%2C%20the%20algorithm%20has%20the%20drawback%20that%20its%0Anumerical%20stability%20relies%20on%20symmetric%20nudging%2C%20which%20may%20be%20restrictive%20in%0Abiological%20and%20analog%20implementations.%20In%20this%20work%20we%20first%20provide%20a%20solid%0Afoundation%20for%20the%20objective%20underlying%20the%20dual%20propagation%20method%2C%20which%20also%0Areveals%20a%20surprising%20connection%20with%20adversarial%20robustness.%20Second%2C%20we%0Ademonstrate%20how%20dual%20propagation%20is%20related%20to%20a%20particular%20adjoint%20state%0Amethod%2C%20which%20is%20stable%20regardless%20of%20asymmetric%20nudging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08573v3&entry.124074799=Read"},
{"title": "Image Statistics Predict the Sensitivity of Perceptual Quality Metrics", "author": "Alexander Hepburn and Valero Laparra and Ra\u00fal Santos-Rodriguez and Jes\u00fas Malo", "abstract": "  Previously, Barlow and Attneave hypothesised a link between biological vision\nand information maximisation. Following Shannon, information was defined using\nthe probability of natural images. Several physiological and psychophysical\nphenomena have been derived from principles like info-max, efficient coding, or\noptimal denoising. However, it remains unclear how this link is expressed in\nmathematical terms from image probability. Classical derivations were subjected\nto strong assumptions on the probability models and on the behaviour of the\nsensors. Moreover, the direct evaluation of the hypothesis was limited by the\ninability of classical image models to deliver accurate estimates of the\nprobability. Here, we directly evaluate image probabilities using a generative\nmodel for natural images, and analyse how probability-related factors can be\ncombined to predict the sensitivity of state-of-the-art subjective image\nquality metrics, a proxy for human perception. We use information theory and\nregression analysis to find a simple model that when combining just two\nprobability-related factors achieves 0.77 correlation with subjective metrics.\nThis probability-based model is validated in two ways: through direct\ncomparison with the opinion of real observers in a subjective quality\nexperiment, and by reproducing basic trends of classical psychophysical facts\nsuch as the Contrast Sensitivity Function, the Weber-law, and contrast masking.\n", "link": "http://arxiv.org/abs/2303.09874v4", "date": "2024-12-02", "relevancy": 1.5462, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5265}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5146}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Statistics%20Predict%20the%20Sensitivity%20of%20Perceptual%20Quality%20Metrics&body=Title%3A%20Image%20Statistics%20Predict%20the%20Sensitivity%20of%20Perceptual%20Quality%20Metrics%0AAuthor%3A%20Alexander%20Hepburn%20and%20Valero%20Laparra%20and%20Ra%C3%BAl%20Santos-Rodriguez%20and%20Jes%C3%BAs%20Malo%0AAbstract%3A%20%20%20Previously%2C%20Barlow%20and%20Attneave%20hypothesised%20a%20link%20between%20biological%20vision%0Aand%20information%20maximisation.%20Following%20Shannon%2C%20information%20was%20defined%20using%0Athe%20probability%20of%20natural%20images.%20Several%20physiological%20and%20psychophysical%0Aphenomena%20have%20been%20derived%20from%20principles%20like%20info-max%2C%20efficient%20coding%2C%20or%0Aoptimal%20denoising.%20However%2C%20it%20remains%20unclear%20how%20this%20link%20is%20expressed%20in%0Amathematical%20terms%20from%20image%20probability.%20Classical%20derivations%20were%20subjected%0Ato%20strong%20assumptions%20on%20the%20probability%20models%20and%20on%20the%20behaviour%20of%20the%0Asensors.%20Moreover%2C%20the%20direct%20evaluation%20of%20the%20hypothesis%20was%20limited%20by%20the%0Ainability%20of%20classical%20image%20models%20to%20deliver%20accurate%20estimates%20of%20the%0Aprobability.%20Here%2C%20we%20directly%20evaluate%20image%20probabilities%20using%20a%20generative%0Amodel%20for%20natural%20images%2C%20and%20analyse%20how%20probability-related%20factors%20can%20be%0Acombined%20to%20predict%20the%20sensitivity%20of%20state-of-the-art%20subjective%20image%0Aquality%20metrics%2C%20a%20proxy%20for%20human%20perception.%20We%20use%20information%20theory%20and%0Aregression%20analysis%20to%20find%20a%20simple%20model%20that%20when%20combining%20just%20two%0Aprobability-related%20factors%20achieves%200.77%20correlation%20with%20subjective%20metrics.%0AThis%20probability-based%20model%20is%20validated%20in%20two%20ways%3A%20through%20direct%0Acomparison%20with%20the%20opinion%20of%20real%20observers%20in%20a%20subjective%20quality%0Aexperiment%2C%20and%20by%20reproducing%20basic%20trends%20of%20classical%20psychophysical%20facts%0Asuch%20as%20the%20Contrast%20Sensitivity%20Function%2C%20the%20Weber-law%2C%20and%20contrast%20masking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.09874v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Statistics%2520Predict%2520the%2520Sensitivity%2520of%2520Perceptual%2520Quality%2520Metrics%26entry.906535625%3DAlexander%2520Hepburn%2520and%2520Valero%2520Laparra%2520and%2520Ra%25C3%25BAl%2520Santos-Rodriguez%2520and%2520Jes%25C3%25BAs%2520Malo%26entry.1292438233%3D%2520%2520Previously%252C%2520Barlow%2520and%2520Attneave%2520hypothesised%2520a%2520link%2520between%2520biological%2520vision%250Aand%2520information%2520maximisation.%2520Following%2520Shannon%252C%2520information%2520was%2520defined%2520using%250Athe%2520probability%2520of%2520natural%2520images.%2520Several%2520physiological%2520and%2520psychophysical%250Aphenomena%2520have%2520been%2520derived%2520from%2520principles%2520like%2520info-max%252C%2520efficient%2520coding%252C%2520or%250Aoptimal%2520denoising.%2520However%252C%2520it%2520remains%2520unclear%2520how%2520this%2520link%2520is%2520expressed%2520in%250Amathematical%2520terms%2520from%2520image%2520probability.%2520Classical%2520derivations%2520were%2520subjected%250Ato%2520strong%2520assumptions%2520on%2520the%2520probability%2520models%2520and%2520on%2520the%2520behaviour%2520of%2520the%250Asensors.%2520Moreover%252C%2520the%2520direct%2520evaluation%2520of%2520the%2520hypothesis%2520was%2520limited%2520by%2520the%250Ainability%2520of%2520classical%2520image%2520models%2520to%2520deliver%2520accurate%2520estimates%2520of%2520the%250Aprobability.%2520Here%252C%2520we%2520directly%2520evaluate%2520image%2520probabilities%2520using%2520a%2520generative%250Amodel%2520for%2520natural%2520images%252C%2520and%2520analyse%2520how%2520probability-related%2520factors%2520can%2520be%250Acombined%2520to%2520predict%2520the%2520sensitivity%2520of%2520state-of-the-art%2520subjective%2520image%250Aquality%2520metrics%252C%2520a%2520proxy%2520for%2520human%2520perception.%2520We%2520use%2520information%2520theory%2520and%250Aregression%2520analysis%2520to%2520find%2520a%2520simple%2520model%2520that%2520when%2520combining%2520just%2520two%250Aprobability-related%2520factors%2520achieves%25200.77%2520correlation%2520with%2520subjective%2520metrics.%250AThis%2520probability-based%2520model%2520is%2520validated%2520in%2520two%2520ways%253A%2520through%2520direct%250Acomparison%2520with%2520the%2520opinion%2520of%2520real%2520observers%2520in%2520a%2520subjective%2520quality%250Aexperiment%252C%2520and%2520by%2520reproducing%2520basic%2520trends%2520of%2520classical%2520psychophysical%2520facts%250Asuch%2520as%2520the%2520Contrast%2520Sensitivity%2520Function%252C%2520the%2520Weber-law%252C%2520and%2520contrast%2520masking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.09874v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Statistics%20Predict%20the%20Sensitivity%20of%20Perceptual%20Quality%20Metrics&entry.906535625=Alexander%20Hepburn%20and%20Valero%20Laparra%20and%20Ra%C3%BAl%20Santos-Rodriguez%20and%20Jes%C3%BAs%20Malo&entry.1292438233=%20%20Previously%2C%20Barlow%20and%20Attneave%20hypothesised%20a%20link%20between%20biological%20vision%0Aand%20information%20maximisation.%20Following%20Shannon%2C%20information%20was%20defined%20using%0Athe%20probability%20of%20natural%20images.%20Several%20physiological%20and%20psychophysical%0Aphenomena%20have%20been%20derived%20from%20principles%20like%20info-max%2C%20efficient%20coding%2C%20or%0Aoptimal%20denoising.%20However%2C%20it%20remains%20unclear%20how%20this%20link%20is%20expressed%20in%0Amathematical%20terms%20from%20image%20probability.%20Classical%20derivations%20were%20subjected%0Ato%20strong%20assumptions%20on%20the%20probability%20models%20and%20on%20the%20behaviour%20of%20the%0Asensors.%20Moreover%2C%20the%20direct%20evaluation%20of%20the%20hypothesis%20was%20limited%20by%20the%0Ainability%20of%20classical%20image%20models%20to%20deliver%20accurate%20estimates%20of%20the%0Aprobability.%20Here%2C%20we%20directly%20evaluate%20image%20probabilities%20using%20a%20generative%0Amodel%20for%20natural%20images%2C%20and%20analyse%20how%20probability-related%20factors%20can%20be%0Acombined%20to%20predict%20the%20sensitivity%20of%20state-of-the-art%20subjective%20image%0Aquality%20metrics%2C%20a%20proxy%20for%20human%20perception.%20We%20use%20information%20theory%20and%0Aregression%20analysis%20to%20find%20a%20simple%20model%20that%20when%20combining%20just%20two%0Aprobability-related%20factors%20achieves%200.77%20correlation%20with%20subjective%20metrics.%0AThis%20probability-based%20model%20is%20validated%20in%20two%20ways%3A%20through%20direct%0Acomparison%20with%20the%20opinion%20of%20real%20observers%20in%20a%20subjective%20quality%0Aexperiment%2C%20and%20by%20reproducing%20basic%20trends%20of%20classical%20psychophysical%20facts%0Asuch%20as%20the%20Contrast%20Sensitivity%20Function%2C%20the%20Weber-law%2C%20and%20contrast%20masking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.09874v4&entry.124074799=Read"},
{"title": "Dual-Personalizing Adapter for Federated Foundation Models", "author": "Yiyuan Yang and Guodong Long and Tao Shen and Jing Jiang and Michael Blumenstein", "abstract": "  Recently, foundation models, particularly large language models (LLMs), have\ndemonstrated an impressive ability to adapt to various tasks by fine-tuning\ndiverse instruction data. Notably, federated foundation models (FedFM) emerge\nas a privacy preservation method to fine-tune models collaboratively under\nfederated learning (FL) settings by leveraging many distributed datasets with\nnon-IID data. To alleviate communication and computation overhead,\nparameter-efficient methods are introduced for efficiency, and some research\nadapted personalization methods to FedFM for better user preferences alignment.\nHowever, a critical gap in existing research is the neglect of test-time\ndistribution shifts in real-world applications, and conventional methods for\ntest-time distribution shifts in personalized FL are less effective for FedFM\ndue to their failure to adapt to complex distribution shift scenarios and the\nrequirement to train all parameters. To bridge this gap, we refine the setting\nin FedFM, termed test-time personalization, which aims to learn personalized\nfederated foundation models on clients while effectively handling test-time\ndistribution shifts simultaneously. To address challenges in this setting, we\nexplore a simple yet effective solution, a Federated Dual-Personalizing Adapter\n(FedDPA) architecture. By co-working with a foundation model, a global adapter\nand a local adapter jointly tackle the test-time distribution shifts and\nclient-specific personalization. Additionally, we introduce an instance-wise\ndynamic weighting mechanism that dynamically integrates the global and local\nadapters for each test instance during inference, facilitating effective\ntest-time personalization. The effectiveness of the proposed method has been\nevaluated on benchmark datasets across different NLP tasks.\n", "link": "http://arxiv.org/abs/2403.19211v2", "date": "2024-12-02", "relevancy": 1.5429, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5361}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.495}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-Personalizing%20Adapter%20for%20Federated%20Foundation%20Models&body=Title%3A%20Dual-Personalizing%20Adapter%20for%20Federated%20Foundation%20Models%0AAuthor%3A%20Yiyuan%20Yang%20and%20Guodong%20Long%20and%20Tao%20Shen%20and%20Jing%20Jiang%20and%20Michael%20Blumenstein%0AAbstract%3A%20%20%20Recently%2C%20foundation%20models%2C%20particularly%20large%20language%20models%20%28LLMs%29%2C%20have%0Ademonstrated%20an%20impressive%20ability%20to%20adapt%20to%20various%20tasks%20by%20fine-tuning%0Adiverse%20instruction%20data.%20Notably%2C%20federated%20foundation%20models%20%28FedFM%29%20emerge%0Aas%20a%20privacy%20preservation%20method%20to%20fine-tune%20models%20collaboratively%20under%0Afederated%20learning%20%28FL%29%20settings%20by%20leveraging%20many%20distributed%20datasets%20with%0Anon-IID%20data.%20To%20alleviate%20communication%20and%20computation%20overhead%2C%0Aparameter-efficient%20methods%20are%20introduced%20for%20efficiency%2C%20and%20some%20research%0Aadapted%20personalization%20methods%20to%20FedFM%20for%20better%20user%20preferences%20alignment.%0AHowever%2C%20a%20critical%20gap%20in%20existing%20research%20is%20the%20neglect%20of%20test-time%0Adistribution%20shifts%20in%20real-world%20applications%2C%20and%20conventional%20methods%20for%0Atest-time%20distribution%20shifts%20in%20personalized%20FL%20are%20less%20effective%20for%20FedFM%0Adue%20to%20their%20failure%20to%20adapt%20to%20complex%20distribution%20shift%20scenarios%20and%20the%0Arequirement%20to%20train%20all%20parameters.%20To%20bridge%20this%20gap%2C%20we%20refine%20the%20setting%0Ain%20FedFM%2C%20termed%20test-time%20personalization%2C%20which%20aims%20to%20learn%20personalized%0Afederated%20foundation%20models%20on%20clients%20while%20effectively%20handling%20test-time%0Adistribution%20shifts%20simultaneously.%20To%20address%20challenges%20in%20this%20setting%2C%20we%0Aexplore%20a%20simple%20yet%20effective%20solution%2C%20a%20Federated%20Dual-Personalizing%20Adapter%0A%28FedDPA%29%20architecture.%20By%20co-working%20with%20a%20foundation%20model%2C%20a%20global%20adapter%0Aand%20a%20local%20adapter%20jointly%20tackle%20the%20test-time%20distribution%20shifts%20and%0Aclient-specific%20personalization.%20Additionally%2C%20we%20introduce%20an%20instance-wise%0Adynamic%20weighting%20mechanism%20that%20dynamically%20integrates%20the%20global%20and%20local%0Aadapters%20for%20each%20test%20instance%20during%20inference%2C%20facilitating%20effective%0Atest-time%20personalization.%20The%20effectiveness%20of%20the%20proposed%20method%20has%20been%0Aevaluated%20on%20benchmark%20datasets%20across%20different%20NLP%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19211v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-Personalizing%2520Adapter%2520for%2520Federated%2520Foundation%2520Models%26entry.906535625%3DYiyuan%2520Yang%2520and%2520Guodong%2520Long%2520and%2520Tao%2520Shen%2520and%2520Jing%2520Jiang%2520and%2520Michael%2520Blumenstein%26entry.1292438233%3D%2520%2520Recently%252C%2520foundation%2520models%252C%2520particularly%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520have%250Ademonstrated%2520an%2520impressive%2520ability%2520to%2520adapt%2520to%2520various%2520tasks%2520by%2520fine-tuning%250Adiverse%2520instruction%2520data.%2520Notably%252C%2520federated%2520foundation%2520models%2520%2528FedFM%2529%2520emerge%250Aas%2520a%2520privacy%2520preservation%2520method%2520to%2520fine-tune%2520models%2520collaboratively%2520under%250Afederated%2520learning%2520%2528FL%2529%2520settings%2520by%2520leveraging%2520many%2520distributed%2520datasets%2520with%250Anon-IID%2520data.%2520To%2520alleviate%2520communication%2520and%2520computation%2520overhead%252C%250Aparameter-efficient%2520methods%2520are%2520introduced%2520for%2520efficiency%252C%2520and%2520some%2520research%250Aadapted%2520personalization%2520methods%2520to%2520FedFM%2520for%2520better%2520user%2520preferences%2520alignment.%250AHowever%252C%2520a%2520critical%2520gap%2520in%2520existing%2520research%2520is%2520the%2520neglect%2520of%2520test-time%250Adistribution%2520shifts%2520in%2520real-world%2520applications%252C%2520and%2520conventional%2520methods%2520for%250Atest-time%2520distribution%2520shifts%2520in%2520personalized%2520FL%2520are%2520less%2520effective%2520for%2520FedFM%250Adue%2520to%2520their%2520failure%2520to%2520adapt%2520to%2520complex%2520distribution%2520shift%2520scenarios%2520and%2520the%250Arequirement%2520to%2520train%2520all%2520parameters.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520refine%2520the%2520setting%250Ain%2520FedFM%252C%2520termed%2520test-time%2520personalization%252C%2520which%2520aims%2520to%2520learn%2520personalized%250Afederated%2520foundation%2520models%2520on%2520clients%2520while%2520effectively%2520handling%2520test-time%250Adistribution%2520shifts%2520simultaneously.%2520To%2520address%2520challenges%2520in%2520this%2520setting%252C%2520we%250Aexplore%2520a%2520simple%2520yet%2520effective%2520solution%252C%2520a%2520Federated%2520Dual-Personalizing%2520Adapter%250A%2528FedDPA%2529%2520architecture.%2520By%2520co-working%2520with%2520a%2520foundation%2520model%252C%2520a%2520global%2520adapter%250Aand%2520a%2520local%2520adapter%2520jointly%2520tackle%2520the%2520test-time%2520distribution%2520shifts%2520and%250Aclient-specific%2520personalization.%2520Additionally%252C%2520we%2520introduce%2520an%2520instance-wise%250Adynamic%2520weighting%2520mechanism%2520that%2520dynamically%2520integrates%2520the%2520global%2520and%2520local%250Aadapters%2520for%2520each%2520test%2520instance%2520during%2520inference%252C%2520facilitating%2520effective%250Atest-time%2520personalization.%2520The%2520effectiveness%2520of%2520the%2520proposed%2520method%2520has%2520been%250Aevaluated%2520on%2520benchmark%2520datasets%2520across%2520different%2520NLP%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19211v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Personalizing%20Adapter%20for%20Federated%20Foundation%20Models&entry.906535625=Yiyuan%20Yang%20and%20Guodong%20Long%20and%20Tao%20Shen%20and%20Jing%20Jiang%20and%20Michael%20Blumenstein&entry.1292438233=%20%20Recently%2C%20foundation%20models%2C%20particularly%20large%20language%20models%20%28LLMs%29%2C%20have%0Ademonstrated%20an%20impressive%20ability%20to%20adapt%20to%20various%20tasks%20by%20fine-tuning%0Adiverse%20instruction%20data.%20Notably%2C%20federated%20foundation%20models%20%28FedFM%29%20emerge%0Aas%20a%20privacy%20preservation%20method%20to%20fine-tune%20models%20collaboratively%20under%0Afederated%20learning%20%28FL%29%20settings%20by%20leveraging%20many%20distributed%20datasets%20with%0Anon-IID%20data.%20To%20alleviate%20communication%20and%20computation%20overhead%2C%0Aparameter-efficient%20methods%20are%20introduced%20for%20efficiency%2C%20and%20some%20research%0Aadapted%20personalization%20methods%20to%20FedFM%20for%20better%20user%20preferences%20alignment.%0AHowever%2C%20a%20critical%20gap%20in%20existing%20research%20is%20the%20neglect%20of%20test-time%0Adistribution%20shifts%20in%20real-world%20applications%2C%20and%20conventional%20methods%20for%0Atest-time%20distribution%20shifts%20in%20personalized%20FL%20are%20less%20effective%20for%20FedFM%0Adue%20to%20their%20failure%20to%20adapt%20to%20complex%20distribution%20shift%20scenarios%20and%20the%0Arequirement%20to%20train%20all%20parameters.%20To%20bridge%20this%20gap%2C%20we%20refine%20the%20setting%0Ain%20FedFM%2C%20termed%20test-time%20personalization%2C%20which%20aims%20to%20learn%20personalized%0Afederated%20foundation%20models%20on%20clients%20while%20effectively%20handling%20test-time%0Adistribution%20shifts%20simultaneously.%20To%20address%20challenges%20in%20this%20setting%2C%20we%0Aexplore%20a%20simple%20yet%20effective%20solution%2C%20a%20Federated%20Dual-Personalizing%20Adapter%0A%28FedDPA%29%20architecture.%20By%20co-working%20with%20a%20foundation%20model%2C%20a%20global%20adapter%0Aand%20a%20local%20adapter%20jointly%20tackle%20the%20test-time%20distribution%20shifts%20and%0Aclient-specific%20personalization.%20Additionally%2C%20we%20introduce%20an%20instance-wise%0Adynamic%20weighting%20mechanism%20that%20dynamically%20integrates%20the%20global%20and%20local%0Aadapters%20for%20each%20test%20instance%20during%20inference%2C%20facilitating%20effective%0Atest-time%20personalization.%20The%20effectiveness%20of%20the%20proposed%20method%20has%20been%0Aevaluated%20on%20benchmark%20datasets%20across%20different%20NLP%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19211v2&entry.124074799=Read"},
{"title": "Dynamic Estimation of Learning Rates Using a Non-Linear Autoregressive\n  Model", "author": "Ramin Okhrati", "abstract": "  We introduce a new class of adaptive non-linear autoregressive (Nlar) models\nincorporating the concept of momentum, which dynamically estimate both the\nlearning rates and momentum as the number of iterations increases. In our\nmethod, the growth of the gradients is controlled using a scaling (clipping)\nfunction, leading to stable convergence. Within this framework, we propose\nthree distinct estimators for learning rates and provide theoretical proof of\ntheir convergence. We further demonstrate how these estimators underpin the\ndevelopment of effective Nlar optimizers. The performance of the proposed\nestimators and optimizers is rigorously evaluated through extensive experiments\nacross several datasets and a reinforcement learning environment. The results\nhighlight two key features of the Nlar optimizers: robust convergence despite\nvariations in underlying parameters, including large initial learning rates,\nand strong adaptability with rapid convergence during the initial epochs.\n", "link": "http://arxiv.org/abs/2410.09943v2", "date": "2024-12-02", "relevancy": 1.5409, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5417}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4962}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Estimation%20of%20Learning%20Rates%20Using%20a%20Non-Linear%20Autoregressive%0A%20%20Model&body=Title%3A%20Dynamic%20Estimation%20of%20Learning%20Rates%20Using%20a%20Non-Linear%20Autoregressive%0A%20%20Model%0AAuthor%3A%20Ramin%20Okhrati%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20class%20of%20adaptive%20non-linear%20autoregressive%20%28Nlar%29%20models%0Aincorporating%20the%20concept%20of%20momentum%2C%20which%20dynamically%20estimate%20both%20the%0Alearning%20rates%20and%20momentum%20as%20the%20number%20of%20iterations%20increases.%20In%20our%0Amethod%2C%20the%20growth%20of%20the%20gradients%20is%20controlled%20using%20a%20scaling%20%28clipping%29%0Afunction%2C%20leading%20to%20stable%20convergence.%20Within%20this%20framework%2C%20we%20propose%0Athree%20distinct%20estimators%20for%20learning%20rates%20and%20provide%20theoretical%20proof%20of%0Atheir%20convergence.%20We%20further%20demonstrate%20how%20these%20estimators%20underpin%20the%0Adevelopment%20of%20effective%20Nlar%20optimizers.%20The%20performance%20of%20the%20proposed%0Aestimators%20and%20optimizers%20is%20rigorously%20evaluated%20through%20extensive%20experiments%0Aacross%20several%20datasets%20and%20a%20reinforcement%20learning%20environment.%20The%20results%0Ahighlight%20two%20key%20features%20of%20the%20Nlar%20optimizers%3A%20robust%20convergence%20despite%0Avariations%20in%20underlying%20parameters%2C%20including%20large%20initial%20learning%20rates%2C%0Aand%20strong%20adaptability%20with%20rapid%20convergence%20during%20the%20initial%20epochs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09943v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Estimation%2520of%2520Learning%2520Rates%2520Using%2520a%2520Non-Linear%2520Autoregressive%250A%2520%2520Model%26entry.906535625%3DRamin%2520Okhrati%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520class%2520of%2520adaptive%2520non-linear%2520autoregressive%2520%2528Nlar%2529%2520models%250Aincorporating%2520the%2520concept%2520of%2520momentum%252C%2520which%2520dynamically%2520estimate%2520both%2520the%250Alearning%2520rates%2520and%2520momentum%2520as%2520the%2520number%2520of%2520iterations%2520increases.%2520In%2520our%250Amethod%252C%2520the%2520growth%2520of%2520the%2520gradients%2520is%2520controlled%2520using%2520a%2520scaling%2520%2528clipping%2529%250Afunction%252C%2520leading%2520to%2520stable%2520convergence.%2520Within%2520this%2520framework%252C%2520we%2520propose%250Athree%2520distinct%2520estimators%2520for%2520learning%2520rates%2520and%2520provide%2520theoretical%2520proof%2520of%250Atheir%2520convergence.%2520We%2520further%2520demonstrate%2520how%2520these%2520estimators%2520underpin%2520the%250Adevelopment%2520of%2520effective%2520Nlar%2520optimizers.%2520The%2520performance%2520of%2520the%2520proposed%250Aestimators%2520and%2520optimizers%2520is%2520rigorously%2520evaluated%2520through%2520extensive%2520experiments%250Aacross%2520several%2520datasets%2520and%2520a%2520reinforcement%2520learning%2520environment.%2520The%2520results%250Ahighlight%2520two%2520key%2520features%2520of%2520the%2520Nlar%2520optimizers%253A%2520robust%2520convergence%2520despite%250Avariations%2520in%2520underlying%2520parameters%252C%2520including%2520large%2520initial%2520learning%2520rates%252C%250Aand%2520strong%2520adaptability%2520with%2520rapid%2520convergence%2520during%2520the%2520initial%2520epochs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09943v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Estimation%20of%20Learning%20Rates%20Using%20a%20Non-Linear%20Autoregressive%0A%20%20Model&entry.906535625=Ramin%20Okhrati&entry.1292438233=%20%20We%20introduce%20a%20new%20class%20of%20adaptive%20non-linear%20autoregressive%20%28Nlar%29%20models%0Aincorporating%20the%20concept%20of%20momentum%2C%20which%20dynamically%20estimate%20both%20the%0Alearning%20rates%20and%20momentum%20as%20the%20number%20of%20iterations%20increases.%20In%20our%0Amethod%2C%20the%20growth%20of%20the%20gradients%20is%20controlled%20using%20a%20scaling%20%28clipping%29%0Afunction%2C%20leading%20to%20stable%20convergence.%20Within%20this%20framework%2C%20we%20propose%0Athree%20distinct%20estimators%20for%20learning%20rates%20and%20provide%20theoretical%20proof%20of%0Atheir%20convergence.%20We%20further%20demonstrate%20how%20these%20estimators%20underpin%20the%0Adevelopment%20of%20effective%20Nlar%20optimizers.%20The%20performance%20of%20the%20proposed%0Aestimators%20and%20optimizers%20is%20rigorously%20evaluated%20through%20extensive%20experiments%0Aacross%20several%20datasets%20and%20a%20reinforcement%20learning%20environment.%20The%20results%0Ahighlight%20two%20key%20features%20of%20the%20Nlar%20optimizers%3A%20robust%20convergence%20despite%0Avariations%20in%20underlying%20parameters%2C%20including%20large%20initial%20learning%20rates%2C%0Aand%20strong%20adaptability%20with%20rapid%20convergence%20during%20the%20initial%20epochs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09943v2&entry.124074799=Read"},
{"title": "Sample Complexity Bounds for Linear System Identification from a Finite\n  Set", "author": "Nicolas Chatzikiriakos and Andrea Iannelli", "abstract": "  This paper considers a finite sample perspective on the problem of\nidentifying an LTI system from a finite set of possible systems using\ntrajectory data. To this end, we use the maximum likelihood estimator to\nidentify the true system and provide an upper bound for its sample complexity.\nCrucially, the derived bound does not rely on a potentially restrictive\nstability assumption. Additionally, we leverage tools from information theory\nto provide a lower bound to the sample complexity that holds independently of\nthe used estimator. The derived sample complexity bounds are analyzed\nanalytically and numerically.\n", "link": "http://arxiv.org/abs/2409.11141v2", "date": "2024-12-02", "relevancy": 1.5409, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.394}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.38}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sample%20Complexity%20Bounds%20for%20Linear%20System%20Identification%20from%20a%20Finite%0A%20%20Set&body=Title%3A%20Sample%20Complexity%20Bounds%20for%20Linear%20System%20Identification%20from%20a%20Finite%0A%20%20Set%0AAuthor%3A%20Nicolas%20Chatzikiriakos%20and%20Andrea%20Iannelli%0AAbstract%3A%20%20%20This%20paper%20considers%20a%20finite%20sample%20perspective%20on%20the%20problem%20of%0Aidentifying%20an%20LTI%20system%20from%20a%20finite%20set%20of%20possible%20systems%20using%0Atrajectory%20data.%20To%20this%20end%2C%20we%20use%20the%20maximum%20likelihood%20estimator%20to%0Aidentify%20the%20true%20system%20and%20provide%20an%20upper%20bound%20for%20its%20sample%20complexity.%0ACrucially%2C%20the%20derived%20bound%20does%20not%20rely%20on%20a%20potentially%20restrictive%0Astability%20assumption.%20Additionally%2C%20we%20leverage%20tools%20from%20information%20theory%0Ato%20provide%20a%20lower%20bound%20to%20the%20sample%20complexity%20that%20holds%20independently%20of%0Athe%20used%20estimator.%20The%20derived%20sample%20complexity%20bounds%20are%20analyzed%0Aanalytically%20and%20numerically.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11141v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSample%2520Complexity%2520Bounds%2520for%2520Linear%2520System%2520Identification%2520from%2520a%2520Finite%250A%2520%2520Set%26entry.906535625%3DNicolas%2520Chatzikiriakos%2520and%2520Andrea%2520Iannelli%26entry.1292438233%3D%2520%2520This%2520paper%2520considers%2520a%2520finite%2520sample%2520perspective%2520on%2520the%2520problem%2520of%250Aidentifying%2520an%2520LTI%2520system%2520from%2520a%2520finite%2520set%2520of%2520possible%2520systems%2520using%250Atrajectory%2520data.%2520To%2520this%2520end%252C%2520we%2520use%2520the%2520maximum%2520likelihood%2520estimator%2520to%250Aidentify%2520the%2520true%2520system%2520and%2520provide%2520an%2520upper%2520bound%2520for%2520its%2520sample%2520complexity.%250ACrucially%252C%2520the%2520derived%2520bound%2520does%2520not%2520rely%2520on%2520a%2520potentially%2520restrictive%250Astability%2520assumption.%2520Additionally%252C%2520we%2520leverage%2520tools%2520from%2520information%2520theory%250Ato%2520provide%2520a%2520lower%2520bound%2520to%2520the%2520sample%2520complexity%2520that%2520holds%2520independently%2520of%250Athe%2520used%2520estimator.%2520The%2520derived%2520sample%2520complexity%2520bounds%2520are%2520analyzed%250Aanalytically%2520and%2520numerically.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11141v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sample%20Complexity%20Bounds%20for%20Linear%20System%20Identification%20from%20a%20Finite%0A%20%20Set&entry.906535625=Nicolas%20Chatzikiriakos%20and%20Andrea%20Iannelli&entry.1292438233=%20%20This%20paper%20considers%20a%20finite%20sample%20perspective%20on%20the%20problem%20of%0Aidentifying%20an%20LTI%20system%20from%20a%20finite%20set%20of%20possible%20systems%20using%0Atrajectory%20data.%20To%20this%20end%2C%20we%20use%20the%20maximum%20likelihood%20estimator%20to%0Aidentify%20the%20true%20system%20and%20provide%20an%20upper%20bound%20for%20its%20sample%20complexity.%0ACrucially%2C%20the%20derived%20bound%20does%20not%20rely%20on%20a%20potentially%20restrictive%0Astability%20assumption.%20Additionally%2C%20we%20leverage%20tools%20from%20information%20theory%0Ato%20provide%20a%20lower%20bound%20to%20the%20sample%20complexity%20that%20holds%20independently%20of%0Athe%20used%20estimator.%20The%20derived%20sample%20complexity%20bounds%20are%20analyzed%0Aanalytically%20and%20numerically.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11141v2&entry.124074799=Read"},
{"title": "Differentially Private Zeroth-Order Methods for Scalable Large Language\n  Model Finetuning", "author": "Z Liu and J Lou and W Bao and Y Hu and B Li and Z Qin and K Ren", "abstract": "  Fine-tuning on task-specific datasets is a widely-embraced paradigm of\nharnessing the powerful capability of pretrained LLMs for various downstream\ntasks. Due to the popularity of LLMs fine-tuning and its accompanying privacy\nconcerns, differentially private (DP) fine-tuning of pretrained LLMs has been\nwidely used to safeguarding the privacy of task-specific datasets. Lying at the\ndesign core of DP LLM fine-tuning methods is the satisfactory tradeoff among\nprivacy, utility, and scalability. Most existing methods build upon the seminal\nwork of DP-SGD. Despite pushing the scalability of DP-SGD to its limit,\nDP-SGD-based fine-tuning methods are unfortunately limited by the inherent\ninefficiency of SGD.\n  In this paper, we investigate the potential of DP zeroth-order methods for\nLLM pretraining, which avoids the scalability bottleneck of SGD by\napproximating the gradient with the more efficient zeroth-order gradient.\nRather than treating the zeroth-order method as a drop-in replacement for SGD,\nthis paper presents a comprehensive study both theoretically and empirically.\nFirst, we propose the stagewise DP zeroth-order method (DP-ZOSO) that\ndynamically schedules key hyperparameters. This design is grounded on the\nsynergy between DP random perturbation and the gradient approximation error of\nthe zeroth-order method, and its effect on fine-tuning trajectory.\n  We provide theoretical analysis for both proposed methods. We conduct\nextensive empirical analysis on both encoder-only masked language model and\ndecoder-only autoregressive language model, achieving impressive results in\nterms of scalability and utility regardless of the class of tasks (compared\nwith DPZero, DP-ZOPO improves $4.5\\%$ on SST-5, $5.5\\%$ on MNLI with\nRoBERTa-Large and 9.2\\% on CB, 3.9\\% on BoolQ with OPT-2.7b when $\\epsilon=4$,\ndemonstrates more significant enhancement in performance on more complicated\ntasks).\n", "link": "http://arxiv.org/abs/2402.07818v5", "date": "2024-12-02", "relevancy": 1.5331, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5259}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5096}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentially%20Private%20Zeroth-Order%20Methods%20for%20Scalable%20Large%20Language%0A%20%20Model%20Finetuning&body=Title%3A%20Differentially%20Private%20Zeroth-Order%20Methods%20for%20Scalable%20Large%20Language%0A%20%20Model%20Finetuning%0AAuthor%3A%20Z%20Liu%20and%20J%20Lou%20and%20W%20Bao%20and%20Y%20Hu%20and%20B%20Li%20and%20Z%20Qin%20and%20K%20Ren%0AAbstract%3A%20%20%20Fine-tuning%20on%20task-specific%20datasets%20is%20a%20widely-embraced%20paradigm%20of%0Aharnessing%20the%20powerful%20capability%20of%20pretrained%20LLMs%20for%20various%20downstream%0Atasks.%20Due%20to%20the%20popularity%20of%20LLMs%20fine-tuning%20and%20its%20accompanying%20privacy%0Aconcerns%2C%20differentially%20private%20%28DP%29%20fine-tuning%20of%20pretrained%20LLMs%20has%20been%0Awidely%20used%20to%20safeguarding%20the%20privacy%20of%20task-specific%20datasets.%20Lying%20at%20the%0Adesign%20core%20of%20DP%20LLM%20fine-tuning%20methods%20is%20the%20satisfactory%20tradeoff%20among%0Aprivacy%2C%20utility%2C%20and%20scalability.%20Most%20existing%20methods%20build%20upon%20the%20seminal%0Awork%20of%20DP-SGD.%20Despite%20pushing%20the%20scalability%20of%20DP-SGD%20to%20its%20limit%2C%0ADP-SGD-based%20fine-tuning%20methods%20are%20unfortunately%20limited%20by%20the%20inherent%0Ainefficiency%20of%20SGD.%0A%20%20In%20this%20paper%2C%20we%20investigate%20the%20potential%20of%20DP%20zeroth-order%20methods%20for%0ALLM%20pretraining%2C%20which%20avoids%20the%20scalability%20bottleneck%20of%20SGD%20by%0Aapproximating%20the%20gradient%20with%20the%20more%20efficient%20zeroth-order%20gradient.%0ARather%20than%20treating%20the%20zeroth-order%20method%20as%20a%20drop-in%20replacement%20for%20SGD%2C%0Athis%20paper%20presents%20a%20comprehensive%20study%20both%20theoretically%20and%20empirically.%0AFirst%2C%20we%20propose%20the%20stagewise%20DP%20zeroth-order%20method%20%28DP-ZOSO%29%20that%0Adynamically%20schedules%20key%20hyperparameters.%20This%20design%20is%20grounded%20on%20the%0Asynergy%20between%20DP%20random%20perturbation%20and%20the%20gradient%20approximation%20error%20of%0Athe%20zeroth-order%20method%2C%20and%20its%20effect%20on%20fine-tuning%20trajectory.%0A%20%20We%20provide%20theoretical%20analysis%20for%20both%20proposed%20methods.%20We%20conduct%0Aextensive%20empirical%20analysis%20on%20both%20encoder-only%20masked%20language%20model%20and%0Adecoder-only%20autoregressive%20language%20model%2C%20achieving%20impressive%20results%20in%0Aterms%20of%20scalability%20and%20utility%20regardless%20of%20the%20class%20of%20tasks%20%28compared%0Awith%20DPZero%2C%20DP-ZOPO%20improves%20%244.5%5C%25%24%20on%20SST-5%2C%20%245.5%5C%25%24%20on%20MNLI%20with%0ARoBERTa-Large%20and%209.2%5C%25%20on%20CB%2C%203.9%5C%25%20on%20BoolQ%20with%20OPT-2.7b%20when%20%24%5Cepsilon%3D4%24%2C%0Ademonstrates%20more%20significant%20enhancement%20in%20performance%20on%20more%20complicated%0Atasks%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07818v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentially%2520Private%2520Zeroth-Order%2520Methods%2520for%2520Scalable%2520Large%2520Language%250A%2520%2520Model%2520Finetuning%26entry.906535625%3DZ%2520Liu%2520and%2520J%2520Lou%2520and%2520W%2520Bao%2520and%2520Y%2520Hu%2520and%2520B%2520Li%2520and%2520Z%2520Qin%2520and%2520K%2520Ren%26entry.1292438233%3D%2520%2520Fine-tuning%2520on%2520task-specific%2520datasets%2520is%2520a%2520widely-embraced%2520paradigm%2520of%250Aharnessing%2520the%2520powerful%2520capability%2520of%2520pretrained%2520LLMs%2520for%2520various%2520downstream%250Atasks.%2520Due%2520to%2520the%2520popularity%2520of%2520LLMs%2520fine-tuning%2520and%2520its%2520accompanying%2520privacy%250Aconcerns%252C%2520differentially%2520private%2520%2528DP%2529%2520fine-tuning%2520of%2520pretrained%2520LLMs%2520has%2520been%250Awidely%2520used%2520to%2520safeguarding%2520the%2520privacy%2520of%2520task-specific%2520datasets.%2520Lying%2520at%2520the%250Adesign%2520core%2520of%2520DP%2520LLM%2520fine-tuning%2520methods%2520is%2520the%2520satisfactory%2520tradeoff%2520among%250Aprivacy%252C%2520utility%252C%2520and%2520scalability.%2520Most%2520existing%2520methods%2520build%2520upon%2520the%2520seminal%250Awork%2520of%2520DP-SGD.%2520Despite%2520pushing%2520the%2520scalability%2520of%2520DP-SGD%2520to%2520its%2520limit%252C%250ADP-SGD-based%2520fine-tuning%2520methods%2520are%2520unfortunately%2520limited%2520by%2520the%2520inherent%250Ainefficiency%2520of%2520SGD.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520potential%2520of%2520DP%2520zeroth-order%2520methods%2520for%250ALLM%2520pretraining%252C%2520which%2520avoids%2520the%2520scalability%2520bottleneck%2520of%2520SGD%2520by%250Aapproximating%2520the%2520gradient%2520with%2520the%2520more%2520efficient%2520zeroth-order%2520gradient.%250ARather%2520than%2520treating%2520the%2520zeroth-order%2520method%2520as%2520a%2520drop-in%2520replacement%2520for%2520SGD%252C%250Athis%2520paper%2520presents%2520a%2520comprehensive%2520study%2520both%2520theoretically%2520and%2520empirically.%250AFirst%252C%2520we%2520propose%2520the%2520stagewise%2520DP%2520zeroth-order%2520method%2520%2528DP-ZOSO%2529%2520that%250Adynamically%2520schedules%2520key%2520hyperparameters.%2520This%2520design%2520is%2520grounded%2520on%2520the%250Asynergy%2520between%2520DP%2520random%2520perturbation%2520and%2520the%2520gradient%2520approximation%2520error%2520of%250Athe%2520zeroth-order%2520method%252C%2520and%2520its%2520effect%2520on%2520fine-tuning%2520trajectory.%250A%2520%2520We%2520provide%2520theoretical%2520analysis%2520for%2520both%2520proposed%2520methods.%2520We%2520conduct%250Aextensive%2520empirical%2520analysis%2520on%2520both%2520encoder-only%2520masked%2520language%2520model%2520and%250Adecoder-only%2520autoregressive%2520language%2520model%252C%2520achieving%2520impressive%2520results%2520in%250Aterms%2520of%2520scalability%2520and%2520utility%2520regardless%2520of%2520the%2520class%2520of%2520tasks%2520%2528compared%250Awith%2520DPZero%252C%2520DP-ZOPO%2520improves%2520%25244.5%255C%2525%2524%2520on%2520SST-5%252C%2520%25245.5%255C%2525%2524%2520on%2520MNLI%2520with%250ARoBERTa-Large%2520and%25209.2%255C%2525%2520on%2520CB%252C%25203.9%255C%2525%2520on%2520BoolQ%2520with%2520OPT-2.7b%2520when%2520%2524%255Cepsilon%253D4%2524%252C%250Ademonstrates%2520more%2520significant%2520enhancement%2520in%2520performance%2520on%2520more%2520complicated%250Atasks%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07818v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentially%20Private%20Zeroth-Order%20Methods%20for%20Scalable%20Large%20Language%0A%20%20Model%20Finetuning&entry.906535625=Z%20Liu%20and%20J%20Lou%20and%20W%20Bao%20and%20Y%20Hu%20and%20B%20Li%20and%20Z%20Qin%20and%20K%20Ren&entry.1292438233=%20%20Fine-tuning%20on%20task-specific%20datasets%20is%20a%20widely-embraced%20paradigm%20of%0Aharnessing%20the%20powerful%20capability%20of%20pretrained%20LLMs%20for%20various%20downstream%0Atasks.%20Due%20to%20the%20popularity%20of%20LLMs%20fine-tuning%20and%20its%20accompanying%20privacy%0Aconcerns%2C%20differentially%20private%20%28DP%29%20fine-tuning%20of%20pretrained%20LLMs%20has%20been%0Awidely%20used%20to%20safeguarding%20the%20privacy%20of%20task-specific%20datasets.%20Lying%20at%20the%0Adesign%20core%20of%20DP%20LLM%20fine-tuning%20methods%20is%20the%20satisfactory%20tradeoff%20among%0Aprivacy%2C%20utility%2C%20and%20scalability.%20Most%20existing%20methods%20build%20upon%20the%20seminal%0Awork%20of%20DP-SGD.%20Despite%20pushing%20the%20scalability%20of%20DP-SGD%20to%20its%20limit%2C%0ADP-SGD-based%20fine-tuning%20methods%20are%20unfortunately%20limited%20by%20the%20inherent%0Ainefficiency%20of%20SGD.%0A%20%20In%20this%20paper%2C%20we%20investigate%20the%20potential%20of%20DP%20zeroth-order%20methods%20for%0ALLM%20pretraining%2C%20which%20avoids%20the%20scalability%20bottleneck%20of%20SGD%20by%0Aapproximating%20the%20gradient%20with%20the%20more%20efficient%20zeroth-order%20gradient.%0ARather%20than%20treating%20the%20zeroth-order%20method%20as%20a%20drop-in%20replacement%20for%20SGD%2C%0Athis%20paper%20presents%20a%20comprehensive%20study%20both%20theoretically%20and%20empirically.%0AFirst%2C%20we%20propose%20the%20stagewise%20DP%20zeroth-order%20method%20%28DP-ZOSO%29%20that%0Adynamically%20schedules%20key%20hyperparameters.%20This%20design%20is%20grounded%20on%20the%0Asynergy%20between%20DP%20random%20perturbation%20and%20the%20gradient%20approximation%20error%20of%0Athe%20zeroth-order%20method%2C%20and%20its%20effect%20on%20fine-tuning%20trajectory.%0A%20%20We%20provide%20theoretical%20analysis%20for%20both%20proposed%20methods.%20We%20conduct%0Aextensive%20empirical%20analysis%20on%20both%20encoder-only%20masked%20language%20model%20and%0Adecoder-only%20autoregressive%20language%20model%2C%20achieving%20impressive%20results%20in%0Aterms%20of%20scalability%20and%20utility%20regardless%20of%20the%20class%20of%20tasks%20%28compared%0Awith%20DPZero%2C%20DP-ZOPO%20improves%20%244.5%5C%25%24%20on%20SST-5%2C%20%245.5%5C%25%24%20on%20MNLI%20with%0ARoBERTa-Large%20and%209.2%5C%25%20on%20CB%2C%203.9%5C%25%20on%20BoolQ%20with%20OPT-2.7b%20when%20%24%5Cepsilon%3D4%24%2C%0Ademonstrates%20more%20significant%20enhancement%20in%20performance%20on%20more%20complicated%0Atasks%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07818v5&entry.124074799=Read"},
{"title": "CREW: Facilitating Human-AI Teaming Research", "author": "Lingyu Zhang and Zhengran Ji and Boyuan Chen", "abstract": "  With the increasing deployment of artificial intelligence (AI) technologies,\nthe potential of humans working with AI agents has been growing at a great\nspeed. Human-AI teaming is an important paradigm for studying various aspects\nwhen humans and AI agents work together. The unique aspect of Human-AI teaming\nresearch is the need to jointly study humans and AI agents, demanding\nmultidisciplinary research efforts from machine learning to human-computer\ninteraction, robotics, cognitive science, neuroscience, psychology, social\nscience, and complex systems. However, existing platforms for Human-AI teaming\nresearch are limited, often supporting oversimplified scenarios and a single\ntask, or specifically focusing on either human-teaming research or multi-agent\nAI algorithms. We introduce CREW, a platform to facilitate Human-AI teaming\nresearch in real-time decision-making scenarios and engage collaborations from\nmultiple scientific disciplines, with a strong emphasis on human involvement.\nIt includes pre-built tasks for cognitive studies and Human-AI teaming with\nexpandable potentials from our modular design. Following conventional cognitive\nneuroscience research, CREW also supports multimodal human physiological signal\nrecording for behavior analysis. Moreover, CREW benchmarks real-time\nhuman-guided reinforcement learning agents using state-of-the-art algorithms\nand well-tuned baselines. With CREW, we were able to conduct 50 human subject\nstudies within a week to verify the effectiveness of our benchmark.\n", "link": "http://arxiv.org/abs/2408.00170v2", "date": "2024-12-02", "relevancy": 1.498, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5137}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5053}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CREW%3A%20Facilitating%20Human-AI%20Teaming%20Research&body=Title%3A%20CREW%3A%20Facilitating%20Human-AI%20Teaming%20Research%0AAuthor%3A%20Lingyu%20Zhang%20and%20Zhengran%20Ji%20and%20Boyuan%20Chen%0AAbstract%3A%20%20%20With%20the%20increasing%20deployment%20of%20artificial%20intelligence%20%28AI%29%20technologies%2C%0Athe%20potential%20of%20humans%20working%20with%20AI%20agents%20has%20been%20growing%20at%20a%20great%0Aspeed.%20Human-AI%20teaming%20is%20an%20important%20paradigm%20for%20studying%20various%20aspects%0Awhen%20humans%20and%20AI%20agents%20work%20together.%20The%20unique%20aspect%20of%20Human-AI%20teaming%0Aresearch%20is%20the%20need%20to%20jointly%20study%20humans%20and%20AI%20agents%2C%20demanding%0Amultidisciplinary%20research%20efforts%20from%20machine%20learning%20to%20human-computer%0Ainteraction%2C%20robotics%2C%20cognitive%20science%2C%20neuroscience%2C%20psychology%2C%20social%0Ascience%2C%20and%20complex%20systems.%20However%2C%20existing%20platforms%20for%20Human-AI%20teaming%0Aresearch%20are%20limited%2C%20often%20supporting%20oversimplified%20scenarios%20and%20a%20single%0Atask%2C%20or%20specifically%20focusing%20on%20either%20human-teaming%20research%20or%20multi-agent%0AAI%20algorithms.%20We%20introduce%20CREW%2C%20a%20platform%20to%20facilitate%20Human-AI%20teaming%0Aresearch%20in%20real-time%20decision-making%20scenarios%20and%20engage%20collaborations%20from%0Amultiple%20scientific%20disciplines%2C%20with%20a%20strong%20emphasis%20on%20human%20involvement.%0AIt%20includes%20pre-built%20tasks%20for%20cognitive%20studies%20and%20Human-AI%20teaming%20with%0Aexpandable%20potentials%20from%20our%20modular%20design.%20Following%20conventional%20cognitive%0Aneuroscience%20research%2C%20CREW%20also%20supports%20multimodal%20human%20physiological%20signal%0Arecording%20for%20behavior%20analysis.%20Moreover%2C%20CREW%20benchmarks%20real-time%0Ahuman-guided%20reinforcement%20learning%20agents%20using%20state-of-the-art%20algorithms%0Aand%20well-tuned%20baselines.%20With%20CREW%2C%20we%20were%20able%20to%20conduct%2050%20human%20subject%0Astudies%20within%20a%20week%20to%20verify%20the%20effectiveness%20of%20our%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00170v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCREW%253A%2520Facilitating%2520Human-AI%2520Teaming%2520Research%26entry.906535625%3DLingyu%2520Zhang%2520and%2520Zhengran%2520Ji%2520and%2520Boyuan%2520Chen%26entry.1292438233%3D%2520%2520With%2520the%2520increasing%2520deployment%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520technologies%252C%250Athe%2520potential%2520of%2520humans%2520working%2520with%2520AI%2520agents%2520has%2520been%2520growing%2520at%2520a%2520great%250Aspeed.%2520Human-AI%2520teaming%2520is%2520an%2520important%2520paradigm%2520for%2520studying%2520various%2520aspects%250Awhen%2520humans%2520and%2520AI%2520agents%2520work%2520together.%2520The%2520unique%2520aspect%2520of%2520Human-AI%2520teaming%250Aresearch%2520is%2520the%2520need%2520to%2520jointly%2520study%2520humans%2520and%2520AI%2520agents%252C%2520demanding%250Amultidisciplinary%2520research%2520efforts%2520from%2520machine%2520learning%2520to%2520human-computer%250Ainteraction%252C%2520robotics%252C%2520cognitive%2520science%252C%2520neuroscience%252C%2520psychology%252C%2520social%250Ascience%252C%2520and%2520complex%2520systems.%2520However%252C%2520existing%2520platforms%2520for%2520Human-AI%2520teaming%250Aresearch%2520are%2520limited%252C%2520often%2520supporting%2520oversimplified%2520scenarios%2520and%2520a%2520single%250Atask%252C%2520or%2520specifically%2520focusing%2520on%2520either%2520human-teaming%2520research%2520or%2520multi-agent%250AAI%2520algorithms.%2520We%2520introduce%2520CREW%252C%2520a%2520platform%2520to%2520facilitate%2520Human-AI%2520teaming%250Aresearch%2520in%2520real-time%2520decision-making%2520scenarios%2520and%2520engage%2520collaborations%2520from%250Amultiple%2520scientific%2520disciplines%252C%2520with%2520a%2520strong%2520emphasis%2520on%2520human%2520involvement.%250AIt%2520includes%2520pre-built%2520tasks%2520for%2520cognitive%2520studies%2520and%2520Human-AI%2520teaming%2520with%250Aexpandable%2520potentials%2520from%2520our%2520modular%2520design.%2520Following%2520conventional%2520cognitive%250Aneuroscience%2520research%252C%2520CREW%2520also%2520supports%2520multimodal%2520human%2520physiological%2520signal%250Arecording%2520for%2520behavior%2520analysis.%2520Moreover%252C%2520CREW%2520benchmarks%2520real-time%250Ahuman-guided%2520reinforcement%2520learning%2520agents%2520using%2520state-of-the-art%2520algorithms%250Aand%2520well-tuned%2520baselines.%2520With%2520CREW%252C%2520we%2520were%2520able%2520to%2520conduct%252050%2520human%2520subject%250Astudies%2520within%2520a%2520week%2520to%2520verify%2520the%2520effectiveness%2520of%2520our%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00170v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CREW%3A%20Facilitating%20Human-AI%20Teaming%20Research&entry.906535625=Lingyu%20Zhang%20and%20Zhengran%20Ji%20and%20Boyuan%20Chen&entry.1292438233=%20%20With%20the%20increasing%20deployment%20of%20artificial%20intelligence%20%28AI%29%20technologies%2C%0Athe%20potential%20of%20humans%20working%20with%20AI%20agents%20has%20been%20growing%20at%20a%20great%0Aspeed.%20Human-AI%20teaming%20is%20an%20important%20paradigm%20for%20studying%20various%20aspects%0Awhen%20humans%20and%20AI%20agents%20work%20together.%20The%20unique%20aspect%20of%20Human-AI%20teaming%0Aresearch%20is%20the%20need%20to%20jointly%20study%20humans%20and%20AI%20agents%2C%20demanding%0Amultidisciplinary%20research%20efforts%20from%20machine%20learning%20to%20human-computer%0Ainteraction%2C%20robotics%2C%20cognitive%20science%2C%20neuroscience%2C%20psychology%2C%20social%0Ascience%2C%20and%20complex%20systems.%20However%2C%20existing%20platforms%20for%20Human-AI%20teaming%0Aresearch%20are%20limited%2C%20often%20supporting%20oversimplified%20scenarios%20and%20a%20single%0Atask%2C%20or%20specifically%20focusing%20on%20either%20human-teaming%20research%20or%20multi-agent%0AAI%20algorithms.%20We%20introduce%20CREW%2C%20a%20platform%20to%20facilitate%20Human-AI%20teaming%0Aresearch%20in%20real-time%20decision-making%20scenarios%20and%20engage%20collaborations%20from%0Amultiple%20scientific%20disciplines%2C%20with%20a%20strong%20emphasis%20on%20human%20involvement.%0AIt%20includes%20pre-built%20tasks%20for%20cognitive%20studies%20and%20Human-AI%20teaming%20with%0Aexpandable%20potentials%20from%20our%20modular%20design.%20Following%20conventional%20cognitive%0Aneuroscience%20research%2C%20CREW%20also%20supports%20multimodal%20human%20physiological%20signal%0Arecording%20for%20behavior%20analysis.%20Moreover%2C%20CREW%20benchmarks%20real-time%0Ahuman-guided%20reinforcement%20learning%20agents%20using%20state-of-the-art%20algorithms%0Aand%20well-tuned%20baselines.%20With%20CREW%2C%20we%20were%20able%20to%20conduct%2050%20human%20subject%0Astudies%20within%20a%20week%20to%20verify%20the%20effectiveness%20of%20our%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00170v2&entry.124074799=Read"},
{"title": "Methods for generating and evaluating synthetic longitudinal patient\n  data: a systematic review", "author": "Katariina Perkonoja and Kari Auranen and Joni Virta", "abstract": "  The rapid growth in data availability has facilitated research and\ndevelopment, yet not all industries have benefited equally due to legal and\nprivacy constraints. The healthcare sector faces significant challenges in\nutilizing patient data because of concerns about data security and\nconfidentiality. To address this, various privacy-preserving methods, including\nsynthetic data generation, have been proposed. Synthetic data replicate\nexisting data as closely as possible, acting as a proxy for sensitive\ninformation. While patient data are often longitudinal, this aspect remains\nunderrepresented in existing reviews of synthetic data generation in\nhealthcare. This paper maps and describes methods for generating and evaluating\nsynthetic longitudinal patient data in real-life settings through a systematic\nliterature review, conducted following the PRISMA guidelines and incorporating\ndata from five databases up to May 2024. Thirty-nine methods were identified,\nwith four addressing all challenges of longitudinal data generation, though\nnone included privacy-preserving mechanisms. Resemblance was evaluated in most\nstudies, utility in the majority, and privacy in just over half. Only a small\nfraction of studies assessed all three aspects. Our findings highlight the need\nfor further research in this area.\n", "link": "http://arxiv.org/abs/2309.12380v3", "date": "2024-12-02", "relevancy": 1.497, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.3788}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.3723}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.3678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Methods%20for%20generating%20and%20evaluating%20synthetic%20longitudinal%20patient%0A%20%20data%3A%20a%20systematic%20review&body=Title%3A%20Methods%20for%20generating%20and%20evaluating%20synthetic%20longitudinal%20patient%0A%20%20data%3A%20a%20systematic%20review%0AAuthor%3A%20Katariina%20Perkonoja%20and%20Kari%20Auranen%20and%20Joni%20Virta%0AAbstract%3A%20%20%20The%20rapid%20growth%20in%20data%20availability%20has%20facilitated%20research%20and%0Adevelopment%2C%20yet%20not%20all%20industries%20have%20benefited%20equally%20due%20to%20legal%20and%0Aprivacy%20constraints.%20The%20healthcare%20sector%20faces%20significant%20challenges%20in%0Autilizing%20patient%20data%20because%20of%20concerns%20about%20data%20security%20and%0Aconfidentiality.%20To%20address%20this%2C%20various%20privacy-preserving%20methods%2C%20including%0Asynthetic%20data%20generation%2C%20have%20been%20proposed.%20Synthetic%20data%20replicate%0Aexisting%20data%20as%20closely%20as%20possible%2C%20acting%20as%20a%20proxy%20for%20sensitive%0Ainformation.%20While%20patient%20data%20are%20often%20longitudinal%2C%20this%20aspect%20remains%0Aunderrepresented%20in%20existing%20reviews%20of%20synthetic%20data%20generation%20in%0Ahealthcare.%20This%20paper%20maps%20and%20describes%20methods%20for%20generating%20and%20evaluating%0Asynthetic%20longitudinal%20patient%20data%20in%20real-life%20settings%20through%20a%20systematic%0Aliterature%20review%2C%20conducted%20following%20the%20PRISMA%20guidelines%20and%20incorporating%0Adata%20from%20five%20databases%20up%20to%20May%202024.%20Thirty-nine%20methods%20were%20identified%2C%0Awith%20four%20addressing%20all%20challenges%20of%20longitudinal%20data%20generation%2C%20though%0Anone%20included%20privacy-preserving%20mechanisms.%20Resemblance%20was%20evaluated%20in%20most%0Astudies%2C%20utility%20in%20the%20majority%2C%20and%20privacy%20in%20just%20over%20half.%20Only%20a%20small%0Afraction%20of%20studies%20assessed%20all%20three%20aspects.%20Our%20findings%20highlight%20the%20need%0Afor%20further%20research%20in%20this%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.12380v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMethods%2520for%2520generating%2520and%2520evaluating%2520synthetic%2520longitudinal%2520patient%250A%2520%2520data%253A%2520a%2520systematic%2520review%26entry.906535625%3DKatariina%2520Perkonoja%2520and%2520Kari%2520Auranen%2520and%2520Joni%2520Virta%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520in%2520data%2520availability%2520has%2520facilitated%2520research%2520and%250Adevelopment%252C%2520yet%2520not%2520all%2520industries%2520have%2520benefited%2520equally%2520due%2520to%2520legal%2520and%250Aprivacy%2520constraints.%2520The%2520healthcare%2520sector%2520faces%2520significant%2520challenges%2520in%250Autilizing%2520patient%2520data%2520because%2520of%2520concerns%2520about%2520data%2520security%2520and%250Aconfidentiality.%2520To%2520address%2520this%252C%2520various%2520privacy-preserving%2520methods%252C%2520including%250Asynthetic%2520data%2520generation%252C%2520have%2520been%2520proposed.%2520Synthetic%2520data%2520replicate%250Aexisting%2520data%2520as%2520closely%2520as%2520possible%252C%2520acting%2520as%2520a%2520proxy%2520for%2520sensitive%250Ainformation.%2520While%2520patient%2520data%2520are%2520often%2520longitudinal%252C%2520this%2520aspect%2520remains%250Aunderrepresented%2520in%2520existing%2520reviews%2520of%2520synthetic%2520data%2520generation%2520in%250Ahealthcare.%2520This%2520paper%2520maps%2520and%2520describes%2520methods%2520for%2520generating%2520and%2520evaluating%250Asynthetic%2520longitudinal%2520patient%2520data%2520in%2520real-life%2520settings%2520through%2520a%2520systematic%250Aliterature%2520review%252C%2520conducted%2520following%2520the%2520PRISMA%2520guidelines%2520and%2520incorporating%250Adata%2520from%2520five%2520databases%2520up%2520to%2520May%25202024.%2520Thirty-nine%2520methods%2520were%2520identified%252C%250Awith%2520four%2520addressing%2520all%2520challenges%2520of%2520longitudinal%2520data%2520generation%252C%2520though%250Anone%2520included%2520privacy-preserving%2520mechanisms.%2520Resemblance%2520was%2520evaluated%2520in%2520most%250Astudies%252C%2520utility%2520in%2520the%2520majority%252C%2520and%2520privacy%2520in%2520just%2520over%2520half.%2520Only%2520a%2520small%250Afraction%2520of%2520studies%2520assessed%2520all%2520three%2520aspects.%2520Our%2520findings%2520highlight%2520the%2520need%250Afor%2520further%2520research%2520in%2520this%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.12380v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Methods%20for%20generating%20and%20evaluating%20synthetic%20longitudinal%20patient%0A%20%20data%3A%20a%20systematic%20review&entry.906535625=Katariina%20Perkonoja%20and%20Kari%20Auranen%20and%20Joni%20Virta&entry.1292438233=%20%20The%20rapid%20growth%20in%20data%20availability%20has%20facilitated%20research%20and%0Adevelopment%2C%20yet%20not%20all%20industries%20have%20benefited%20equally%20due%20to%20legal%20and%0Aprivacy%20constraints.%20The%20healthcare%20sector%20faces%20significant%20challenges%20in%0Autilizing%20patient%20data%20because%20of%20concerns%20about%20data%20security%20and%0Aconfidentiality.%20To%20address%20this%2C%20various%20privacy-preserving%20methods%2C%20including%0Asynthetic%20data%20generation%2C%20have%20been%20proposed.%20Synthetic%20data%20replicate%0Aexisting%20data%20as%20closely%20as%20possible%2C%20acting%20as%20a%20proxy%20for%20sensitive%0Ainformation.%20While%20patient%20data%20are%20often%20longitudinal%2C%20this%20aspect%20remains%0Aunderrepresented%20in%20existing%20reviews%20of%20synthetic%20data%20generation%20in%0Ahealthcare.%20This%20paper%20maps%20and%20describes%20methods%20for%20generating%20and%20evaluating%0Asynthetic%20longitudinal%20patient%20data%20in%20real-life%20settings%20through%20a%20systematic%0Aliterature%20review%2C%20conducted%20following%20the%20PRISMA%20guidelines%20and%20incorporating%0Adata%20from%20five%20databases%20up%20to%20May%202024.%20Thirty-nine%20methods%20were%20identified%2C%0Awith%20four%20addressing%20all%20challenges%20of%20longitudinal%20data%20generation%2C%20though%0Anone%20included%20privacy-preserving%20mechanisms.%20Resemblance%20was%20evaluated%20in%20most%0Astudies%2C%20utility%20in%20the%20majority%2C%20and%20privacy%20in%20just%20over%20half.%20Only%20a%20small%0Afraction%20of%20studies%20assessed%20all%20three%20aspects.%20Our%20findings%20highlight%20the%20need%0Afor%20further%20research%20in%20this%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.12380v3&entry.124074799=Read"},
{"title": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities", "author": "Ezra Karger and Houtan Bastani and Chen Yueh-Han and Zachary Jacobs and Danny Halawi and Fred Zhang and Philip E. Tetlock", "abstract": "  Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM (p-value\n$<0.01$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org.\n", "link": "http://arxiv.org/abs/2409.19839v3", "date": "2024-12-02", "relevancy": 1.4595, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5042}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4661}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ForecastBench%3A%20A%20Dynamic%20Benchmark%20of%20AI%20Forecasting%20Capabilities&body=Title%3A%20ForecastBench%3A%20A%20Dynamic%20Benchmark%20of%20AI%20Forecasting%20Capabilities%0AAuthor%3A%20Ezra%20Karger%20and%20Houtan%20Bastani%20and%20Chen%20Yueh-Han%20and%20Zachary%20Jacobs%20and%20Danny%20Halawi%20and%20Fred%20Zhang%20and%20Philip%20E.%20Tetlock%0AAbstract%3A%20%20%20Forecasts%20of%20future%20events%20are%20essential%20inputs%20into%20informed%0Adecision-making.%20Machine%20learning%20%28ML%29%20systems%20have%20the%20potential%20to%20deliver%0Aforecasts%20at%20scale%2C%20but%20there%20is%20no%20framework%20for%20evaluating%20the%20accuracy%20of%20ML%0Asystems%20on%20a%20standardized%20set%20of%20forecasting%20questions.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20ForecastBench%3A%20a%20dynamic%20benchmark%20that%20evaluates%20the%20accuracy%20of%20ML%0Asystems%20on%20an%20automatically%20generated%20and%20regularly%20updated%20set%20of%201%2C000%0Aforecasting%20questions.%20To%20avoid%20any%20possibility%20of%20data%20leakage%2C%20ForecastBench%0Ais%20comprised%20solely%20of%20questions%20about%20future%20events%20that%20have%20no%20known%20answer%0Aat%20the%20time%20of%20submission.%20We%20quantify%20the%20capabilities%20of%20current%20ML%20systems%0Aby%20collecting%20forecasts%20from%20expert%20%28human%29%20forecasters%2C%20the%20general%20public%2C%0Aand%20LLMs%20on%20a%20random%20subset%20of%20questions%20from%20the%20benchmark%20%28%24N%3D200%24%29.%20While%0ALLMs%20have%20achieved%20super-human%20performance%20on%20many%20benchmarks%2C%20they%20perform%0Aless%20well%20here%3A%20expert%20forecasters%20outperform%20the%20top-performing%20LLM%20%28p-value%0A%24%3C0.01%24%29.%20We%20display%20system%20and%20human%20scores%20in%20a%20public%20leaderboard%20at%0Awww.forecastbench.org.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.19839v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForecastBench%253A%2520A%2520Dynamic%2520Benchmark%2520of%2520AI%2520Forecasting%2520Capabilities%26entry.906535625%3DEzra%2520Karger%2520and%2520Houtan%2520Bastani%2520and%2520Chen%2520Yueh-Han%2520and%2520Zachary%2520Jacobs%2520and%2520Danny%2520Halawi%2520and%2520Fred%2520Zhang%2520and%2520Philip%2520E.%2520Tetlock%26entry.1292438233%3D%2520%2520Forecasts%2520of%2520future%2520events%2520are%2520essential%2520inputs%2520into%2520informed%250Adecision-making.%2520Machine%2520learning%2520%2528ML%2529%2520systems%2520have%2520the%2520potential%2520to%2520deliver%250Aforecasts%2520at%2520scale%252C%2520but%2520there%2520is%2520no%2520framework%2520for%2520evaluating%2520the%2520accuracy%2520of%2520ML%250Asystems%2520on%2520a%2520standardized%2520set%2520of%2520forecasting%2520questions.%2520To%2520address%2520this%2520gap%252C%2520we%250Aintroduce%2520ForecastBench%253A%2520a%2520dynamic%2520benchmark%2520that%2520evaluates%2520the%2520accuracy%2520of%2520ML%250Asystems%2520on%2520an%2520automatically%2520generated%2520and%2520regularly%2520updated%2520set%2520of%25201%252C000%250Aforecasting%2520questions.%2520To%2520avoid%2520any%2520possibility%2520of%2520data%2520leakage%252C%2520ForecastBench%250Ais%2520comprised%2520solely%2520of%2520questions%2520about%2520future%2520events%2520that%2520have%2520no%2520known%2520answer%250Aat%2520the%2520time%2520of%2520submission.%2520We%2520quantify%2520the%2520capabilities%2520of%2520current%2520ML%2520systems%250Aby%2520collecting%2520forecasts%2520from%2520expert%2520%2528human%2529%2520forecasters%252C%2520the%2520general%2520public%252C%250Aand%2520LLMs%2520on%2520a%2520random%2520subset%2520of%2520questions%2520from%2520the%2520benchmark%2520%2528%2524N%253D200%2524%2529.%2520While%250ALLMs%2520have%2520achieved%2520super-human%2520performance%2520on%2520many%2520benchmarks%252C%2520they%2520perform%250Aless%2520well%2520here%253A%2520expert%2520forecasters%2520outperform%2520the%2520top-performing%2520LLM%2520%2528p-value%250A%2524%253C0.01%2524%2529.%2520We%2520display%2520system%2520and%2520human%2520scores%2520in%2520a%2520public%2520leaderboard%2520at%250Awww.forecastbench.org.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.19839v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ForecastBench%3A%20A%20Dynamic%20Benchmark%20of%20AI%20Forecasting%20Capabilities&entry.906535625=Ezra%20Karger%20and%20Houtan%20Bastani%20and%20Chen%20Yueh-Han%20and%20Zachary%20Jacobs%20and%20Danny%20Halawi%20and%20Fred%20Zhang%20and%20Philip%20E.%20Tetlock&entry.1292438233=%20%20Forecasts%20of%20future%20events%20are%20essential%20inputs%20into%20informed%0Adecision-making.%20Machine%20learning%20%28ML%29%20systems%20have%20the%20potential%20to%20deliver%0Aforecasts%20at%20scale%2C%20but%20there%20is%20no%20framework%20for%20evaluating%20the%20accuracy%20of%20ML%0Asystems%20on%20a%20standardized%20set%20of%20forecasting%20questions.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20ForecastBench%3A%20a%20dynamic%20benchmark%20that%20evaluates%20the%20accuracy%20of%20ML%0Asystems%20on%20an%20automatically%20generated%20and%20regularly%20updated%20set%20of%201%2C000%0Aforecasting%20questions.%20To%20avoid%20any%20possibility%20of%20data%20leakage%2C%20ForecastBench%0Ais%20comprised%20solely%20of%20questions%20about%20future%20events%20that%20have%20no%20known%20answer%0Aat%20the%20time%20of%20submission.%20We%20quantify%20the%20capabilities%20of%20current%20ML%20systems%0Aby%20collecting%20forecasts%20from%20expert%20%28human%29%20forecasters%2C%20the%20general%20public%2C%0Aand%20LLMs%20on%20a%20random%20subset%20of%20questions%20from%20the%20benchmark%20%28%24N%3D200%24%29.%20While%0ALLMs%20have%20achieved%20super-human%20performance%20on%20many%20benchmarks%2C%20they%20perform%0Aless%20well%20here%3A%20expert%20forecasters%20outperform%20the%20top-performing%20LLM%20%28p-value%0A%24%3C0.01%24%29.%20We%20display%20system%20and%20human%20scores%20in%20a%20public%20leaderboard%20at%0Awww.forecastbench.org.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.19839v3&entry.124074799=Read"},
{"title": "Simulation-based inference with scattering representations: scattering\n  is all you need", "author": "Kiyam Lin and Benjamin Joachimi and Jason D. McEwen", "abstract": "  We demonstrate the successful use of scattering representations without\nfurther compression for simulation-based inference (SBI) with images (i.e.\nfield-level), illustrated with a cosmological case study. Scattering\nrepresentations provide a highly effective representational space for\nsubsequent learning tasks, although the higher dimensional compressed space\nintroduces challenges. We overcome these through spatial averaging, coupled\nwith more expressive density estimators. Compared to alternative methods, such\nan approach does not require additional simulations for either training or\ncomputing derivatives, is interpretable, and resilient to covariate shift. As\nexpected, we show that a scattering only approach extracts more information\nthan traditional second order summary statistics.\n", "link": "http://arxiv.org/abs/2410.11883v3", "date": "2024-12-02", "relevancy": 1.4577, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4925}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4844}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simulation-based%20inference%20with%20scattering%20representations%3A%20scattering%0A%20%20is%20all%20you%20need&body=Title%3A%20Simulation-based%20inference%20with%20scattering%20representations%3A%20scattering%0A%20%20is%20all%20you%20need%0AAuthor%3A%20Kiyam%20Lin%20and%20Benjamin%20Joachimi%20and%20Jason%20D.%20McEwen%0AAbstract%3A%20%20%20We%20demonstrate%20the%20successful%20use%20of%20scattering%20representations%20without%0Afurther%20compression%20for%20simulation-based%20inference%20%28SBI%29%20with%20images%20%28i.e.%0Afield-level%29%2C%20illustrated%20with%20a%20cosmological%20case%20study.%20Scattering%0Arepresentations%20provide%20a%20highly%20effective%20representational%20space%20for%0Asubsequent%20learning%20tasks%2C%20although%20the%20higher%20dimensional%20compressed%20space%0Aintroduces%20challenges.%20We%20overcome%20these%20through%20spatial%20averaging%2C%20coupled%0Awith%20more%20expressive%20density%20estimators.%20Compared%20to%20alternative%20methods%2C%20such%0Aan%20approach%20does%20not%20require%20additional%20simulations%20for%20either%20training%20or%0Acomputing%20derivatives%2C%20is%20interpretable%2C%20and%20resilient%20to%20covariate%20shift.%20As%0Aexpected%2C%20we%20show%20that%20a%20scattering%20only%20approach%20extracts%20more%20information%0Athan%20traditional%20second%20order%20summary%20statistics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11883v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimulation-based%2520inference%2520with%2520scattering%2520representations%253A%2520scattering%250A%2520%2520is%2520all%2520you%2520need%26entry.906535625%3DKiyam%2520Lin%2520and%2520Benjamin%2520Joachimi%2520and%2520Jason%2520D.%2520McEwen%26entry.1292438233%3D%2520%2520We%2520demonstrate%2520the%2520successful%2520use%2520of%2520scattering%2520representations%2520without%250Afurther%2520compression%2520for%2520simulation-based%2520inference%2520%2528SBI%2529%2520with%2520images%2520%2528i.e.%250Afield-level%2529%252C%2520illustrated%2520with%2520a%2520cosmological%2520case%2520study.%2520Scattering%250Arepresentations%2520provide%2520a%2520highly%2520effective%2520representational%2520space%2520for%250Asubsequent%2520learning%2520tasks%252C%2520although%2520the%2520higher%2520dimensional%2520compressed%2520space%250Aintroduces%2520challenges.%2520We%2520overcome%2520these%2520through%2520spatial%2520averaging%252C%2520coupled%250Awith%2520more%2520expressive%2520density%2520estimators.%2520Compared%2520to%2520alternative%2520methods%252C%2520such%250Aan%2520approach%2520does%2520not%2520require%2520additional%2520simulations%2520for%2520either%2520training%2520or%250Acomputing%2520derivatives%252C%2520is%2520interpretable%252C%2520and%2520resilient%2520to%2520covariate%2520shift.%2520As%250Aexpected%252C%2520we%2520show%2520that%2520a%2520scattering%2520only%2520approach%2520extracts%2520more%2520information%250Athan%2520traditional%2520second%2520order%2520summary%2520statistics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11883v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simulation-based%20inference%20with%20scattering%20representations%3A%20scattering%0A%20%20is%20all%20you%20need&entry.906535625=Kiyam%20Lin%20and%20Benjamin%20Joachimi%20and%20Jason%20D.%20McEwen&entry.1292438233=%20%20We%20demonstrate%20the%20successful%20use%20of%20scattering%20representations%20without%0Afurther%20compression%20for%20simulation-based%20inference%20%28SBI%29%20with%20images%20%28i.e.%0Afield-level%29%2C%20illustrated%20with%20a%20cosmological%20case%20study.%20Scattering%0Arepresentations%20provide%20a%20highly%20effective%20representational%20space%20for%0Asubsequent%20learning%20tasks%2C%20although%20the%20higher%20dimensional%20compressed%20space%0Aintroduces%20challenges.%20We%20overcome%20these%20through%20spatial%20averaging%2C%20coupled%0Awith%20more%20expressive%20density%20estimators.%20Compared%20to%20alternative%20methods%2C%20such%0Aan%20approach%20does%20not%20require%20additional%20simulations%20for%20either%20training%20or%0Acomputing%20derivatives%2C%20is%20interpretable%2C%20and%20resilient%20to%20covariate%20shift.%20As%0Aexpected%2C%20we%20show%20that%20a%20scattering%20only%20approach%20extracts%20more%20information%0Athan%20traditional%20second%20order%20summary%20statistics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11883v3&entry.124074799=Read"},
{"title": "Moral Alignment for LLM Agents", "author": "Elizaveta Tennant and Stephen Hailes and Mirco Musolesi", "abstract": "  Decision-making agents based on pre-trained Large Language Models (LLMs) are\nincreasingly being deployed across various domains of human activity. While\ntheir applications are currently rather specialized, several research efforts\nare under way to develop more generalist agents. As LLM-based systems become\nmore agentic, their influence on human activity will grow and the transparency\nof this will decrease. Consequently, developing effective methods for aligning\nthem to human values is vital.\n  The prevailing practice in alignment often relies on human preference data\n(e.g., in RLHF or DPO), in which values are implicit and are essentially\ndeduced from relative preferences over different model outputs. In this work,\ninstead of relying on human feedback, we introduce the design of reward\nfunctions that explicitly encode core human values for Reinforcement\nLearning-based fine-tuning of foundation agent models. Specifically, we use\nintrinsic rewards for the moral alignment of LLM agents.\n  We evaluate our approach using the traditional philosophical frameworks of\nDeontological Ethics and Utilitarianism, quantifying moral rewards for agents\nin terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD)\nenvironment. We also show how moral fine-tuning can be deployed to enable an\nagent to unlearn a previously developed selfish strategy. Finally, we find that\ncertain moral strategies learned on the IPD game generalize to several other\nmatrix game environments. In summary, we demonstrate that fine-tuning with\nintrinsic rewards is a promising general solution for aligning LLM agents to\nhuman values, and it might represent a more transparent and cost-effective\nalternative to currently predominant alignment techniques.\n", "link": "http://arxiv.org/abs/2410.01639v2", "date": "2024-12-02", "relevancy": 1.4169, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4748}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4713}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moral%20Alignment%20for%20LLM%20Agents&body=Title%3A%20Moral%20Alignment%20for%20LLM%20Agents%0AAuthor%3A%20Elizaveta%20Tennant%20and%20Stephen%20Hailes%20and%20Mirco%20Musolesi%0AAbstract%3A%20%20%20Decision-making%20agents%20based%20on%20pre-trained%20Large%20Language%20Models%20%28LLMs%29%20are%0Aincreasingly%20being%20deployed%20across%20various%20domains%20of%20human%20activity.%20While%0Atheir%20applications%20are%20currently%20rather%20specialized%2C%20several%20research%20efforts%0Aare%20under%20way%20to%20develop%20more%20generalist%20agents.%20As%20LLM-based%20systems%20become%0Amore%20agentic%2C%20their%20influence%20on%20human%20activity%20will%20grow%20and%20the%20transparency%0Aof%20this%20will%20decrease.%20Consequently%2C%20developing%20effective%20methods%20for%20aligning%0Athem%20to%20human%20values%20is%20vital.%0A%20%20The%20prevailing%20practice%20in%20alignment%20often%20relies%20on%20human%20preference%20data%0A%28e.g.%2C%20in%20RLHF%20or%20DPO%29%2C%20in%20which%20values%20are%20implicit%20and%20are%20essentially%0Adeduced%20from%20relative%20preferences%20over%20different%20model%20outputs.%20In%20this%20work%2C%0Ainstead%20of%20relying%20on%20human%20feedback%2C%20we%20introduce%20the%20design%20of%20reward%0Afunctions%20that%20explicitly%20encode%20core%20human%20values%20for%20Reinforcement%0ALearning-based%20fine-tuning%20of%20foundation%20agent%20models.%20Specifically%2C%20we%20use%0Aintrinsic%20rewards%20for%20the%20moral%20alignment%20of%20LLM%20agents.%0A%20%20We%20evaluate%20our%20approach%20using%20the%20traditional%20philosophical%20frameworks%20of%0ADeontological%20Ethics%20and%20Utilitarianism%2C%20quantifying%20moral%20rewards%20for%20agents%0Ain%20terms%20of%20actions%20and%20consequences%20on%20the%20Iterated%20Prisoner%27s%20Dilemma%20%28IPD%29%0Aenvironment.%20We%20also%20show%20how%20moral%20fine-tuning%20can%20be%20deployed%20to%20enable%20an%0Aagent%20to%20unlearn%20a%20previously%20developed%20selfish%20strategy.%20Finally%2C%20we%20find%20that%0Acertain%20moral%20strategies%20learned%20on%20the%20IPD%20game%20generalize%20to%20several%20other%0Amatrix%20game%20environments.%20In%20summary%2C%20we%20demonstrate%20that%20fine-tuning%20with%0Aintrinsic%20rewards%20is%20a%20promising%20general%20solution%20for%20aligning%20LLM%20agents%20to%0Ahuman%20values%2C%20and%20it%20might%20represent%20a%20more%20transparent%20and%20cost-effective%0Aalternative%20to%20currently%20predominant%20alignment%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01639v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoral%2520Alignment%2520for%2520LLM%2520Agents%26entry.906535625%3DElizaveta%2520Tennant%2520and%2520Stephen%2520Hailes%2520and%2520Mirco%2520Musolesi%26entry.1292438233%3D%2520%2520Decision-making%2520agents%2520based%2520on%2520pre-trained%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%250Aincreasingly%2520being%2520deployed%2520across%2520various%2520domains%2520of%2520human%2520activity.%2520While%250Atheir%2520applications%2520are%2520currently%2520rather%2520specialized%252C%2520several%2520research%2520efforts%250Aare%2520under%2520way%2520to%2520develop%2520more%2520generalist%2520agents.%2520As%2520LLM-based%2520systems%2520become%250Amore%2520agentic%252C%2520their%2520influence%2520on%2520human%2520activity%2520will%2520grow%2520and%2520the%2520transparency%250Aof%2520this%2520will%2520decrease.%2520Consequently%252C%2520developing%2520effective%2520methods%2520for%2520aligning%250Athem%2520to%2520human%2520values%2520is%2520vital.%250A%2520%2520The%2520prevailing%2520practice%2520in%2520alignment%2520often%2520relies%2520on%2520human%2520preference%2520data%250A%2528e.g.%252C%2520in%2520RLHF%2520or%2520DPO%2529%252C%2520in%2520which%2520values%2520are%2520implicit%2520and%2520are%2520essentially%250Adeduced%2520from%2520relative%2520preferences%2520over%2520different%2520model%2520outputs.%2520In%2520this%2520work%252C%250Ainstead%2520of%2520relying%2520on%2520human%2520feedback%252C%2520we%2520introduce%2520the%2520design%2520of%2520reward%250Afunctions%2520that%2520explicitly%2520encode%2520core%2520human%2520values%2520for%2520Reinforcement%250ALearning-based%2520fine-tuning%2520of%2520foundation%2520agent%2520models.%2520Specifically%252C%2520we%2520use%250Aintrinsic%2520rewards%2520for%2520the%2520moral%2520alignment%2520of%2520LLM%2520agents.%250A%2520%2520We%2520evaluate%2520our%2520approach%2520using%2520the%2520traditional%2520philosophical%2520frameworks%2520of%250ADeontological%2520Ethics%2520and%2520Utilitarianism%252C%2520quantifying%2520moral%2520rewards%2520for%2520agents%250Ain%2520terms%2520of%2520actions%2520and%2520consequences%2520on%2520the%2520Iterated%2520Prisoner%2527s%2520Dilemma%2520%2528IPD%2529%250Aenvironment.%2520We%2520also%2520show%2520how%2520moral%2520fine-tuning%2520can%2520be%2520deployed%2520to%2520enable%2520an%250Aagent%2520to%2520unlearn%2520a%2520previously%2520developed%2520selfish%2520strategy.%2520Finally%252C%2520we%2520find%2520that%250Acertain%2520moral%2520strategies%2520learned%2520on%2520the%2520IPD%2520game%2520generalize%2520to%2520several%2520other%250Amatrix%2520game%2520environments.%2520In%2520summary%252C%2520we%2520demonstrate%2520that%2520fine-tuning%2520with%250Aintrinsic%2520rewards%2520is%2520a%2520promising%2520general%2520solution%2520for%2520aligning%2520LLM%2520agents%2520to%250Ahuman%2520values%252C%2520and%2520it%2520might%2520represent%2520a%2520more%2520transparent%2520and%2520cost-effective%250Aalternative%2520to%2520currently%2520predominant%2520alignment%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01639v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moral%20Alignment%20for%20LLM%20Agents&entry.906535625=Elizaveta%20Tennant%20and%20Stephen%20Hailes%20and%20Mirco%20Musolesi&entry.1292438233=%20%20Decision-making%20agents%20based%20on%20pre-trained%20Large%20Language%20Models%20%28LLMs%29%20are%0Aincreasingly%20being%20deployed%20across%20various%20domains%20of%20human%20activity.%20While%0Atheir%20applications%20are%20currently%20rather%20specialized%2C%20several%20research%20efforts%0Aare%20under%20way%20to%20develop%20more%20generalist%20agents.%20As%20LLM-based%20systems%20become%0Amore%20agentic%2C%20their%20influence%20on%20human%20activity%20will%20grow%20and%20the%20transparency%0Aof%20this%20will%20decrease.%20Consequently%2C%20developing%20effective%20methods%20for%20aligning%0Athem%20to%20human%20values%20is%20vital.%0A%20%20The%20prevailing%20practice%20in%20alignment%20often%20relies%20on%20human%20preference%20data%0A%28e.g.%2C%20in%20RLHF%20or%20DPO%29%2C%20in%20which%20values%20are%20implicit%20and%20are%20essentially%0Adeduced%20from%20relative%20preferences%20over%20different%20model%20outputs.%20In%20this%20work%2C%0Ainstead%20of%20relying%20on%20human%20feedback%2C%20we%20introduce%20the%20design%20of%20reward%0Afunctions%20that%20explicitly%20encode%20core%20human%20values%20for%20Reinforcement%0ALearning-based%20fine-tuning%20of%20foundation%20agent%20models.%20Specifically%2C%20we%20use%0Aintrinsic%20rewards%20for%20the%20moral%20alignment%20of%20LLM%20agents.%0A%20%20We%20evaluate%20our%20approach%20using%20the%20traditional%20philosophical%20frameworks%20of%0ADeontological%20Ethics%20and%20Utilitarianism%2C%20quantifying%20moral%20rewards%20for%20agents%0Ain%20terms%20of%20actions%20and%20consequences%20on%20the%20Iterated%20Prisoner%27s%20Dilemma%20%28IPD%29%0Aenvironment.%20We%20also%20show%20how%20moral%20fine-tuning%20can%20be%20deployed%20to%20enable%20an%0Aagent%20to%20unlearn%20a%20previously%20developed%20selfish%20strategy.%20Finally%2C%20we%20find%20that%0Acertain%20moral%20strategies%20learned%20on%20the%20IPD%20game%20generalize%20to%20several%20other%0Amatrix%20game%20environments.%20In%20summary%2C%20we%20demonstrate%20that%20fine-tuning%20with%0Aintrinsic%20rewards%20is%20a%20promising%20general%20solution%20for%20aligning%20LLM%20agents%20to%0Ahuman%20values%2C%20and%20it%20might%20represent%20a%20more%20transparent%20and%20cost-effective%0Aalternative%20to%20currently%20predominant%20alignment%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01639v2&entry.124074799=Read"},
{"title": "Probabilistic Graph Rewiring via Virtual Nodes", "author": "Chendi Qian and Andrei Manolache and Christopher Morris and Mathias Niepert", "abstract": "  Message-passing graph neural networks (MPNNs) have emerged as a powerful\nparadigm for graph-based machine learning. Despite their effectiveness, MPNNs\nface challenges such as under-reaching and over-squashing, where limited\nreceptive fields and structural bottlenecks hinder information flow in the\ngraph. While graph transformers hold promise in addressing these issues, their\nscalability is limited due to quadratic complexity regarding the number of\nnodes, rendering them impractical for larger graphs. Here, we propose\nimplicitly rewired message-passing neural networks (IPR-MPNNs), a novel\napproach that integrates implicit probabilistic graph rewiring into MPNNs. By\nintroducing a small number of virtual nodes, i.e., adding additional nodes to a\ngiven graph and connecting them to existing nodes, in a differentiable,\nend-to-end manner, IPR-MPNNs enable long-distance message propagation,\ncircumventing quadratic complexity. Theoretically, we demonstrate that\nIPR-MPNNs surpass the expressiveness of traditional MPNNs. Empirically, we\nvalidate our approach by showcasing its ability to mitigate under-reaching and\nover-squashing effects, achieving state-of-the-art performance across multiple\ngraph datasets. Notably, IPR-MPNNs outperform graph transformers while\nmaintaining significantly faster computational efficiency.\n", "link": "http://arxiv.org/abs/2405.17311v3", "date": "2024-12-02", "relevancy": 1.4144, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4978}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4785}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Graph%20Rewiring%20via%20Virtual%20Nodes&body=Title%3A%20Probabilistic%20Graph%20Rewiring%20via%20Virtual%20Nodes%0AAuthor%3A%20Chendi%20Qian%20and%20Andrei%20Manolache%20and%20Christopher%20Morris%20and%20Mathias%20Niepert%0AAbstract%3A%20%20%20Message-passing%20graph%20neural%20networks%20%28MPNNs%29%20have%20emerged%20as%20a%20powerful%0Aparadigm%20for%20graph-based%20machine%20learning.%20Despite%20their%20effectiveness%2C%20MPNNs%0Aface%20challenges%20such%20as%20under-reaching%20and%20over-squashing%2C%20where%20limited%0Areceptive%20fields%20and%20structural%20bottlenecks%20hinder%20information%20flow%20in%20the%0Agraph.%20While%20graph%20transformers%20hold%20promise%20in%20addressing%20these%20issues%2C%20their%0Ascalability%20is%20limited%20due%20to%20quadratic%20complexity%20regarding%20the%20number%20of%0Anodes%2C%20rendering%20them%20impractical%20for%20larger%20graphs.%20Here%2C%20we%20propose%0Aimplicitly%20rewired%20message-passing%20neural%20networks%20%28IPR-MPNNs%29%2C%20a%20novel%0Aapproach%20that%20integrates%20implicit%20probabilistic%20graph%20rewiring%20into%20MPNNs.%20By%0Aintroducing%20a%20small%20number%20of%20virtual%20nodes%2C%20i.e.%2C%20adding%20additional%20nodes%20to%20a%0Agiven%20graph%20and%20connecting%20them%20to%20existing%20nodes%2C%20in%20a%20differentiable%2C%0Aend-to-end%20manner%2C%20IPR-MPNNs%20enable%20long-distance%20message%20propagation%2C%0Acircumventing%20quadratic%20complexity.%20Theoretically%2C%20we%20demonstrate%20that%0AIPR-MPNNs%20surpass%20the%20expressiveness%20of%20traditional%20MPNNs.%20Empirically%2C%20we%0Avalidate%20our%20approach%20by%20showcasing%20its%20ability%20to%20mitigate%20under-reaching%20and%0Aover-squashing%20effects%2C%20achieving%20state-of-the-art%20performance%20across%20multiple%0Agraph%20datasets.%20Notably%2C%20IPR-MPNNs%20outperform%20graph%20transformers%20while%0Amaintaining%20significantly%20faster%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17311v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Graph%2520Rewiring%2520via%2520Virtual%2520Nodes%26entry.906535625%3DChendi%2520Qian%2520and%2520Andrei%2520Manolache%2520and%2520Christopher%2520Morris%2520and%2520Mathias%2520Niepert%26entry.1292438233%3D%2520%2520Message-passing%2520graph%2520neural%2520networks%2520%2528MPNNs%2529%2520have%2520emerged%2520as%2520a%2520powerful%250Aparadigm%2520for%2520graph-based%2520machine%2520learning.%2520Despite%2520their%2520effectiveness%252C%2520MPNNs%250Aface%2520challenges%2520such%2520as%2520under-reaching%2520and%2520over-squashing%252C%2520where%2520limited%250Areceptive%2520fields%2520and%2520structural%2520bottlenecks%2520hinder%2520information%2520flow%2520in%2520the%250Agraph.%2520While%2520graph%2520transformers%2520hold%2520promise%2520in%2520addressing%2520these%2520issues%252C%2520their%250Ascalability%2520is%2520limited%2520due%2520to%2520quadratic%2520complexity%2520regarding%2520the%2520number%2520of%250Anodes%252C%2520rendering%2520them%2520impractical%2520for%2520larger%2520graphs.%2520Here%252C%2520we%2520propose%250Aimplicitly%2520rewired%2520message-passing%2520neural%2520networks%2520%2528IPR-MPNNs%2529%252C%2520a%2520novel%250Aapproach%2520that%2520integrates%2520implicit%2520probabilistic%2520graph%2520rewiring%2520into%2520MPNNs.%2520By%250Aintroducing%2520a%2520small%2520number%2520of%2520virtual%2520nodes%252C%2520i.e.%252C%2520adding%2520additional%2520nodes%2520to%2520a%250Agiven%2520graph%2520and%2520connecting%2520them%2520to%2520existing%2520nodes%252C%2520in%2520a%2520differentiable%252C%250Aend-to-end%2520manner%252C%2520IPR-MPNNs%2520enable%2520long-distance%2520message%2520propagation%252C%250Acircumventing%2520quadratic%2520complexity.%2520Theoretically%252C%2520we%2520demonstrate%2520that%250AIPR-MPNNs%2520surpass%2520the%2520expressiveness%2520of%2520traditional%2520MPNNs.%2520Empirically%252C%2520we%250Avalidate%2520our%2520approach%2520by%2520showcasing%2520its%2520ability%2520to%2520mitigate%2520under-reaching%2520and%250Aover-squashing%2520effects%252C%2520achieving%2520state-of-the-art%2520performance%2520across%2520multiple%250Agraph%2520datasets.%2520Notably%252C%2520IPR-MPNNs%2520outperform%2520graph%2520transformers%2520while%250Amaintaining%2520significantly%2520faster%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17311v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Graph%20Rewiring%20via%20Virtual%20Nodes&entry.906535625=Chendi%20Qian%20and%20Andrei%20Manolache%20and%20Christopher%20Morris%20and%20Mathias%20Niepert&entry.1292438233=%20%20Message-passing%20graph%20neural%20networks%20%28MPNNs%29%20have%20emerged%20as%20a%20powerful%0Aparadigm%20for%20graph-based%20machine%20learning.%20Despite%20their%20effectiveness%2C%20MPNNs%0Aface%20challenges%20such%20as%20under-reaching%20and%20over-squashing%2C%20where%20limited%0Areceptive%20fields%20and%20structural%20bottlenecks%20hinder%20information%20flow%20in%20the%0Agraph.%20While%20graph%20transformers%20hold%20promise%20in%20addressing%20these%20issues%2C%20their%0Ascalability%20is%20limited%20due%20to%20quadratic%20complexity%20regarding%20the%20number%20of%0Anodes%2C%20rendering%20them%20impractical%20for%20larger%20graphs.%20Here%2C%20we%20propose%0Aimplicitly%20rewired%20message-passing%20neural%20networks%20%28IPR-MPNNs%29%2C%20a%20novel%0Aapproach%20that%20integrates%20implicit%20probabilistic%20graph%20rewiring%20into%20MPNNs.%20By%0Aintroducing%20a%20small%20number%20of%20virtual%20nodes%2C%20i.e.%2C%20adding%20additional%20nodes%20to%20a%0Agiven%20graph%20and%20connecting%20them%20to%20existing%20nodes%2C%20in%20a%20differentiable%2C%0Aend-to-end%20manner%2C%20IPR-MPNNs%20enable%20long-distance%20message%20propagation%2C%0Acircumventing%20quadratic%20complexity.%20Theoretically%2C%20we%20demonstrate%20that%0AIPR-MPNNs%20surpass%20the%20expressiveness%20of%20traditional%20MPNNs.%20Empirically%2C%20we%0Avalidate%20our%20approach%20by%20showcasing%20its%20ability%20to%20mitigate%20under-reaching%20and%0Aover-squashing%20effects%2C%20achieving%20state-of-the-art%20performance%20across%20multiple%0Agraph%20datasets.%20Notably%2C%20IPR-MPNNs%20outperform%20graph%20transformers%20while%0Amaintaining%20significantly%20faster%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17311v3&entry.124074799=Read"},
{"title": "A Survey on Task Allocation and Scheduling in Robotic Network Systems", "author": "Saeid Alirezazadeh and Lu\u00eds A. Alexandre", "abstract": "  Cloud Robotics is helping to create a new generation of robots that leverage\nthe nearly unlimited resources of large data centers (i.e., the cloud),\novercoming the limitations imposed by on-board resources. Different processing\npower, capabilities, resource sizes, energy consumption, and so forth, make\nscheduling and task allocation critical components. The basic idea of task\nallocation and scheduling is to optimize performance by minimizing completion\ntime, energy consumption, delays between two consecutive tasks, along with\nothers, and maximizing resource utilization, number of completed tasks in a\ngiven time interval, and suchlike. In the past, several works have addressed\nvarious aspects of task allocation and scheduling. In this paper, we provide a\ncomprehensive overview of task allocation and scheduling strategies and related\nmetrics suitable for robotic network cloud systems. We discuss the issues\nrelated to allocation and scheduling methods and the limitations that need to\nbe overcome. The literature review is organized according to three different\nviewpoints: Architectures and Applications, Methods and Parameters. In\naddition, the limitations of each method are highlighted for future research.\n", "link": "http://arxiv.org/abs/2303.12876v3", "date": "2024-12-02", "relevancy": 1.4028, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5201}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4687}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Task%20Allocation%20and%20Scheduling%20in%20Robotic%20Network%20Systems&body=Title%3A%20A%20Survey%20on%20Task%20Allocation%20and%20Scheduling%20in%20Robotic%20Network%20Systems%0AAuthor%3A%20Saeid%20Alirezazadeh%20and%20Lu%C3%ADs%20A.%20Alexandre%0AAbstract%3A%20%20%20Cloud%20Robotics%20is%20helping%20to%20create%20a%20new%20generation%20of%20robots%20that%20leverage%0Athe%20nearly%20unlimited%20resources%20of%20large%20data%20centers%20%28i.e.%2C%20the%20cloud%29%2C%0Aovercoming%20the%20limitations%20imposed%20by%20on-board%20resources.%20Different%20processing%0Apower%2C%20capabilities%2C%20resource%20sizes%2C%20energy%20consumption%2C%20and%20so%20forth%2C%20make%0Ascheduling%20and%20task%20allocation%20critical%20components.%20The%20basic%20idea%20of%20task%0Aallocation%20and%20scheduling%20is%20to%20optimize%20performance%20by%20minimizing%20completion%0Atime%2C%20energy%20consumption%2C%20delays%20between%20two%20consecutive%20tasks%2C%20along%20with%0Aothers%2C%20and%20maximizing%20resource%20utilization%2C%20number%20of%20completed%20tasks%20in%20a%0Agiven%20time%20interval%2C%20and%20suchlike.%20In%20the%20past%2C%20several%20works%20have%20addressed%0Avarious%20aspects%20of%20task%20allocation%20and%20scheduling.%20In%20this%20paper%2C%20we%20provide%20a%0Acomprehensive%20overview%20of%20task%20allocation%20and%20scheduling%20strategies%20and%20related%0Ametrics%20suitable%20for%20robotic%20network%20cloud%20systems.%20We%20discuss%20the%20issues%0Arelated%20to%20allocation%20and%20scheduling%20methods%20and%20the%20limitations%20that%20need%20to%0Abe%20overcome.%20The%20literature%20review%20is%20organized%20according%20to%20three%20different%0Aviewpoints%3A%20Architectures%20and%20Applications%2C%20Methods%20and%20Parameters.%20In%0Aaddition%2C%20the%20limitations%20of%20each%20method%20are%20highlighted%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.12876v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Task%2520Allocation%2520and%2520Scheduling%2520in%2520Robotic%2520Network%2520Systems%26entry.906535625%3DSaeid%2520Alirezazadeh%2520and%2520Lu%25C3%25ADs%2520A.%2520Alexandre%26entry.1292438233%3D%2520%2520Cloud%2520Robotics%2520is%2520helping%2520to%2520create%2520a%2520new%2520generation%2520of%2520robots%2520that%2520leverage%250Athe%2520nearly%2520unlimited%2520resources%2520of%2520large%2520data%2520centers%2520%2528i.e.%252C%2520the%2520cloud%2529%252C%250Aovercoming%2520the%2520limitations%2520imposed%2520by%2520on-board%2520resources.%2520Different%2520processing%250Apower%252C%2520capabilities%252C%2520resource%2520sizes%252C%2520energy%2520consumption%252C%2520and%2520so%2520forth%252C%2520make%250Ascheduling%2520and%2520task%2520allocation%2520critical%2520components.%2520The%2520basic%2520idea%2520of%2520task%250Aallocation%2520and%2520scheduling%2520is%2520to%2520optimize%2520performance%2520by%2520minimizing%2520completion%250Atime%252C%2520energy%2520consumption%252C%2520delays%2520between%2520two%2520consecutive%2520tasks%252C%2520along%2520with%250Aothers%252C%2520and%2520maximizing%2520resource%2520utilization%252C%2520number%2520of%2520completed%2520tasks%2520in%2520a%250Agiven%2520time%2520interval%252C%2520and%2520suchlike.%2520In%2520the%2520past%252C%2520several%2520works%2520have%2520addressed%250Avarious%2520aspects%2520of%2520task%2520allocation%2520and%2520scheduling.%2520In%2520this%2520paper%252C%2520we%2520provide%2520a%250Acomprehensive%2520overview%2520of%2520task%2520allocation%2520and%2520scheduling%2520strategies%2520and%2520related%250Ametrics%2520suitable%2520for%2520robotic%2520network%2520cloud%2520systems.%2520We%2520discuss%2520the%2520issues%250Arelated%2520to%2520allocation%2520and%2520scheduling%2520methods%2520and%2520the%2520limitations%2520that%2520need%2520to%250Abe%2520overcome.%2520The%2520literature%2520review%2520is%2520organized%2520according%2520to%2520three%2520different%250Aviewpoints%253A%2520Architectures%2520and%2520Applications%252C%2520Methods%2520and%2520Parameters.%2520In%250Aaddition%252C%2520the%2520limitations%2520of%2520each%2520method%2520are%2520highlighted%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.12876v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Task%20Allocation%20and%20Scheduling%20in%20Robotic%20Network%20Systems&entry.906535625=Saeid%20Alirezazadeh%20and%20Lu%C3%ADs%20A.%20Alexandre&entry.1292438233=%20%20Cloud%20Robotics%20is%20helping%20to%20create%20a%20new%20generation%20of%20robots%20that%20leverage%0Athe%20nearly%20unlimited%20resources%20of%20large%20data%20centers%20%28i.e.%2C%20the%20cloud%29%2C%0Aovercoming%20the%20limitations%20imposed%20by%20on-board%20resources.%20Different%20processing%0Apower%2C%20capabilities%2C%20resource%20sizes%2C%20energy%20consumption%2C%20and%20so%20forth%2C%20make%0Ascheduling%20and%20task%20allocation%20critical%20components.%20The%20basic%20idea%20of%20task%0Aallocation%20and%20scheduling%20is%20to%20optimize%20performance%20by%20minimizing%20completion%0Atime%2C%20energy%20consumption%2C%20delays%20between%20two%20consecutive%20tasks%2C%20along%20with%0Aothers%2C%20and%20maximizing%20resource%20utilization%2C%20number%20of%20completed%20tasks%20in%20a%0Agiven%20time%20interval%2C%20and%20suchlike.%20In%20the%20past%2C%20several%20works%20have%20addressed%0Avarious%20aspects%20of%20task%20allocation%20and%20scheduling.%20In%20this%20paper%2C%20we%20provide%20a%0Acomprehensive%20overview%20of%20task%20allocation%20and%20scheduling%20strategies%20and%20related%0Ametrics%20suitable%20for%20robotic%20network%20cloud%20systems.%20We%20discuss%20the%20issues%0Arelated%20to%20allocation%20and%20scheduling%20methods%20and%20the%20limitations%20that%20need%20to%0Abe%20overcome.%20The%20literature%20review%20is%20organized%20according%20to%20three%20different%0Aviewpoints%3A%20Architectures%20and%20Applications%2C%20Methods%20and%20Parameters.%20In%0Aaddition%2C%20the%20limitations%20of%20each%20method%20are%20highlighted%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.12876v3&entry.124074799=Read"},
{"title": "Correction to \"Wasserstein distance estimates for the distributions of\n  numerical approximations to ergodic stochastic differential equations\"", "author": "Daniel Paulin and Peter A. Whalley", "abstract": "  A method for analyzing non-asymptotic guarantees of numerical discretizations\nof ergodic SDEs in Wasserstein-2 distance is presented by Sanz-Serna and\nZygalakis in ``Wasserstein distance estimates for the distributions of\nnumerical approximations to ergodic stochastic differential equations\". They\nanalyze the UBU integrator which is strong order two and only requires one\ngradient evaluation per step, resulting in desirable non-asymptotic guarantees,\nin particular $\\mathcal{O}(d^{1/4}\\epsilon^{-1/2})$ steps to reach a distance\nof $\\epsilon > 0$ in Wasserstein-2 distance away from the target distribution.\nHowever, there is a mistake in the local error estimates in Sanz-Serna and\nZygalakis (2021), in particular, a stronger assumption is needed to achieve\nthese complexity estimates. This note reconciles the theory with the dimension\ndependence observed in practice in many applications of interest.\n", "link": "http://arxiv.org/abs/2402.08711v3", "date": "2024-12-02", "relevancy": 0.7964, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4064}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3981}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Correction%20to%20%22Wasserstein%20distance%20estimates%20for%20the%20distributions%20of%0A%20%20numerical%20approximations%20to%20ergodic%20stochastic%20differential%20equations%22&body=Title%3A%20Correction%20to%20%22Wasserstein%20distance%20estimates%20for%20the%20distributions%20of%0A%20%20numerical%20approximations%20to%20ergodic%20stochastic%20differential%20equations%22%0AAuthor%3A%20Daniel%20Paulin%20and%20Peter%20A.%20Whalley%0AAbstract%3A%20%20%20A%20method%20for%20analyzing%20non-asymptotic%20guarantees%20of%20numerical%20discretizations%0Aof%20ergodic%20SDEs%20in%20Wasserstein-2%20distance%20is%20presented%20by%20Sanz-Serna%20and%0AZygalakis%20in%20%60%60Wasserstein%20distance%20estimates%20for%20the%20distributions%20of%0Anumerical%20approximations%20to%20ergodic%20stochastic%20differential%20equations%22.%20They%0Aanalyze%20the%20UBU%20integrator%20which%20is%20strong%20order%20two%20and%20only%20requires%20one%0Agradient%20evaluation%20per%20step%2C%20resulting%20in%20desirable%20non-asymptotic%20guarantees%2C%0Ain%20particular%20%24%5Cmathcal%7BO%7D%28d%5E%7B1/4%7D%5Cepsilon%5E%7B-1/2%7D%29%24%20steps%20to%20reach%20a%20distance%0Aof%20%24%5Cepsilon%20%3E%200%24%20in%20Wasserstein-2%20distance%20away%20from%20the%20target%20distribution.%0AHowever%2C%20there%20is%20a%20mistake%20in%20the%20local%20error%20estimates%20in%20Sanz-Serna%20and%0AZygalakis%20%282021%29%2C%20in%20particular%2C%20a%20stronger%20assumption%20is%20needed%20to%20achieve%0Athese%20complexity%20estimates.%20This%20note%20reconciles%20the%20theory%20with%20the%20dimension%0Adependence%20observed%20in%20practice%20in%20many%20applications%20of%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08711v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorrection%2520to%2520%2522Wasserstein%2520distance%2520estimates%2520for%2520the%2520distributions%2520of%250A%2520%2520numerical%2520approximations%2520to%2520ergodic%2520stochastic%2520differential%2520equations%2522%26entry.906535625%3DDaniel%2520Paulin%2520and%2520Peter%2520A.%2520Whalley%26entry.1292438233%3D%2520%2520A%2520method%2520for%2520analyzing%2520non-asymptotic%2520guarantees%2520of%2520numerical%2520discretizations%250Aof%2520ergodic%2520SDEs%2520in%2520Wasserstein-2%2520distance%2520is%2520presented%2520by%2520Sanz-Serna%2520and%250AZygalakis%2520in%2520%2560%2560Wasserstein%2520distance%2520estimates%2520for%2520the%2520distributions%2520of%250Anumerical%2520approximations%2520to%2520ergodic%2520stochastic%2520differential%2520equations%2522.%2520They%250Aanalyze%2520the%2520UBU%2520integrator%2520which%2520is%2520strong%2520order%2520two%2520and%2520only%2520requires%2520one%250Agradient%2520evaluation%2520per%2520step%252C%2520resulting%2520in%2520desirable%2520non-asymptotic%2520guarantees%252C%250Ain%2520particular%2520%2524%255Cmathcal%257BO%257D%2528d%255E%257B1/4%257D%255Cepsilon%255E%257B-1/2%257D%2529%2524%2520steps%2520to%2520reach%2520a%2520distance%250Aof%2520%2524%255Cepsilon%2520%253E%25200%2524%2520in%2520Wasserstein-2%2520distance%2520away%2520from%2520the%2520target%2520distribution.%250AHowever%252C%2520there%2520is%2520a%2520mistake%2520in%2520the%2520local%2520error%2520estimates%2520in%2520Sanz-Serna%2520and%250AZygalakis%2520%25282021%2529%252C%2520in%2520particular%252C%2520a%2520stronger%2520assumption%2520is%2520needed%2520to%2520achieve%250Athese%2520complexity%2520estimates.%2520This%2520note%2520reconciles%2520the%2520theory%2520with%2520the%2520dimension%250Adependence%2520observed%2520in%2520practice%2520in%2520many%2520applications%2520of%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08711v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Correction%20to%20%22Wasserstein%20distance%20estimates%20for%20the%20distributions%20of%0A%20%20numerical%20approximations%20to%20ergodic%20stochastic%20differential%20equations%22&entry.906535625=Daniel%20Paulin%20and%20Peter%20A.%20Whalley&entry.1292438233=%20%20A%20method%20for%20analyzing%20non-asymptotic%20guarantees%20of%20numerical%20discretizations%0Aof%20ergodic%20SDEs%20in%20Wasserstein-2%20distance%20is%20presented%20by%20Sanz-Serna%20and%0AZygalakis%20in%20%60%60Wasserstein%20distance%20estimates%20for%20the%20distributions%20of%0Anumerical%20approximations%20to%20ergodic%20stochastic%20differential%20equations%22.%20They%0Aanalyze%20the%20UBU%20integrator%20which%20is%20strong%20order%20two%20and%20only%20requires%20one%0Agradient%20evaluation%20per%20step%2C%20resulting%20in%20desirable%20non-asymptotic%20guarantees%2C%0Ain%20particular%20%24%5Cmathcal%7BO%7D%28d%5E%7B1/4%7D%5Cepsilon%5E%7B-1/2%7D%29%24%20steps%20to%20reach%20a%20distance%0Aof%20%24%5Cepsilon%20%3E%200%24%20in%20Wasserstein-2%20distance%20away%20from%20the%20target%20distribution.%0AHowever%2C%20there%20is%20a%20mistake%20in%20the%20local%20error%20estimates%20in%20Sanz-Serna%20and%0AZygalakis%20%282021%29%2C%20in%20particular%2C%20a%20stronger%20assumption%20is%20needed%20to%20achieve%0Athese%20complexity%20estimates.%20This%20note%20reconciles%20the%20theory%20with%20the%20dimension%0Adependence%20observed%20in%20practice%20in%20many%20applications%20of%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08711v3&entry.124074799=Read"},
{"title": "Inference Scaling fLaws: The Limits of LLM Resampling with Imperfect\n  Verifiers", "author": "Benedikt Stroebl and Sayash Kapoor and Arvind Narayanan", "abstract": "  Recent research has generated hope that inference scaling could allow weaker\nlanguage models to match or exceed the accuracy of stronger models, such as by\nrepeatedly sampling solutions to a coding problem until it passes unit tests.\nThe central thesis of this paper is that there is no free lunch for inference\nscaling: indefinite accuracy improvement through resampling can only be\nrealized if the \"verifier\" (in this case, a set of unit tests) is perfect. When\nthe verifier is imperfect, as it almost always is in domains such as reasoning\nor coding (for example, unit tests have imperfect coverage), there is a nonzero\nprobability of false positives: incorrect solutions that pass the verifier.\nResampling cannot decrease this probability, so it imposes an upper bound to\nthe accuracy of resampling-based inference scaling even with an infinite\ncompute budget. We find that there is a very strong correlation between the\nmodel's single-sample accuracy (i.e. accuracy without unit tests) and its false\npositive rate on coding benchmarks HumanEval and MBPP, whose unit tests have\nlimited coverage. Therefore, no amount of inference scaling of weaker models\ncan enable them to match the single-sample accuracy of a sufficiently strong\nmodel (Fig. 1a). When we consider that false positives have a negative utility\ncompared to abstaining from producing a solution, it bends the inference\nscaling curve further downward. Empirically, we find that the optimal number of\nsamples can be less than 10 under realistic assumptions (Fig. 1b). Finally, we\nshow that beyond accuracy, false positives may have other undesirable\nqualities, such as poor adherence to coding style conventions.\n", "link": "http://arxiv.org/abs/2411.17501v2", "date": "2024-12-02", "relevancy": 1.3706, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4933}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4551}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inference%20Scaling%20fLaws%3A%20The%20Limits%20of%20LLM%20Resampling%20with%20Imperfect%0A%20%20Verifiers&body=Title%3A%20Inference%20Scaling%20fLaws%3A%20The%20Limits%20of%20LLM%20Resampling%20with%20Imperfect%0A%20%20Verifiers%0AAuthor%3A%20Benedikt%20Stroebl%20and%20Sayash%20Kapoor%20and%20Arvind%20Narayanan%0AAbstract%3A%20%20%20Recent%20research%20has%20generated%20hope%20that%20inference%20scaling%20could%20allow%20weaker%0Alanguage%20models%20to%20match%20or%20exceed%20the%20accuracy%20of%20stronger%20models%2C%20such%20as%20by%0Arepeatedly%20sampling%20solutions%20to%20a%20coding%20problem%20until%20it%20passes%20unit%20tests.%0AThe%20central%20thesis%20of%20this%20paper%20is%20that%20there%20is%20no%20free%20lunch%20for%20inference%0Ascaling%3A%20indefinite%20accuracy%20improvement%20through%20resampling%20can%20only%20be%0Arealized%20if%20the%20%22verifier%22%20%28in%20this%20case%2C%20a%20set%20of%20unit%20tests%29%20is%20perfect.%20When%0Athe%20verifier%20is%20imperfect%2C%20as%20it%20almost%20always%20is%20in%20domains%20such%20as%20reasoning%0Aor%20coding%20%28for%20example%2C%20unit%20tests%20have%20imperfect%20coverage%29%2C%20there%20is%20a%20nonzero%0Aprobability%20of%20false%20positives%3A%20incorrect%20solutions%20that%20pass%20the%20verifier.%0AResampling%20cannot%20decrease%20this%20probability%2C%20so%20it%20imposes%20an%20upper%20bound%20to%0Athe%20accuracy%20of%20resampling-based%20inference%20scaling%20even%20with%20an%20infinite%0Acompute%20budget.%20We%20find%20that%20there%20is%20a%20very%20strong%20correlation%20between%20the%0Amodel%27s%20single-sample%20accuracy%20%28i.e.%20accuracy%20without%20unit%20tests%29%20and%20its%20false%0Apositive%20rate%20on%20coding%20benchmarks%20HumanEval%20and%20MBPP%2C%20whose%20unit%20tests%20have%0Alimited%20coverage.%20Therefore%2C%20no%20amount%20of%20inference%20scaling%20of%20weaker%20models%0Acan%20enable%20them%20to%20match%20the%20single-sample%20accuracy%20of%20a%20sufficiently%20strong%0Amodel%20%28Fig.%201a%29.%20When%20we%20consider%20that%20false%20positives%20have%20a%20negative%20utility%0Acompared%20to%20abstaining%20from%20producing%20a%20solution%2C%20it%20bends%20the%20inference%0Ascaling%20curve%20further%20downward.%20Empirically%2C%20we%20find%20that%20the%20optimal%20number%20of%0Asamples%20can%20be%20less%20than%2010%20under%20realistic%20assumptions%20%28Fig.%201b%29.%20Finally%2C%20we%0Ashow%20that%20beyond%20accuracy%2C%20false%20positives%20may%20have%20other%20undesirable%0Aqualities%2C%20such%20as%20poor%20adherence%20to%20coding%20style%20conventions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17501v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInference%2520Scaling%2520fLaws%253A%2520The%2520Limits%2520of%2520LLM%2520Resampling%2520with%2520Imperfect%250A%2520%2520Verifiers%26entry.906535625%3DBenedikt%2520Stroebl%2520and%2520Sayash%2520Kapoor%2520and%2520Arvind%2520Narayanan%26entry.1292438233%3D%2520%2520Recent%2520research%2520has%2520generated%2520hope%2520that%2520inference%2520scaling%2520could%2520allow%2520weaker%250Alanguage%2520models%2520to%2520match%2520or%2520exceed%2520the%2520accuracy%2520of%2520stronger%2520models%252C%2520such%2520as%2520by%250Arepeatedly%2520sampling%2520solutions%2520to%2520a%2520coding%2520problem%2520until%2520it%2520passes%2520unit%2520tests.%250AThe%2520central%2520thesis%2520of%2520this%2520paper%2520is%2520that%2520there%2520is%2520no%2520free%2520lunch%2520for%2520inference%250Ascaling%253A%2520indefinite%2520accuracy%2520improvement%2520through%2520resampling%2520can%2520only%2520be%250Arealized%2520if%2520the%2520%2522verifier%2522%2520%2528in%2520this%2520case%252C%2520a%2520set%2520of%2520unit%2520tests%2529%2520is%2520perfect.%2520When%250Athe%2520verifier%2520is%2520imperfect%252C%2520as%2520it%2520almost%2520always%2520is%2520in%2520domains%2520such%2520as%2520reasoning%250Aor%2520coding%2520%2528for%2520example%252C%2520unit%2520tests%2520have%2520imperfect%2520coverage%2529%252C%2520there%2520is%2520a%2520nonzero%250Aprobability%2520of%2520false%2520positives%253A%2520incorrect%2520solutions%2520that%2520pass%2520the%2520verifier.%250AResampling%2520cannot%2520decrease%2520this%2520probability%252C%2520so%2520it%2520imposes%2520an%2520upper%2520bound%2520to%250Athe%2520accuracy%2520of%2520resampling-based%2520inference%2520scaling%2520even%2520with%2520an%2520infinite%250Acompute%2520budget.%2520We%2520find%2520that%2520there%2520is%2520a%2520very%2520strong%2520correlation%2520between%2520the%250Amodel%2527s%2520single-sample%2520accuracy%2520%2528i.e.%2520accuracy%2520without%2520unit%2520tests%2529%2520and%2520its%2520false%250Apositive%2520rate%2520on%2520coding%2520benchmarks%2520HumanEval%2520and%2520MBPP%252C%2520whose%2520unit%2520tests%2520have%250Alimited%2520coverage.%2520Therefore%252C%2520no%2520amount%2520of%2520inference%2520scaling%2520of%2520weaker%2520models%250Acan%2520enable%2520them%2520to%2520match%2520the%2520single-sample%2520accuracy%2520of%2520a%2520sufficiently%2520strong%250Amodel%2520%2528Fig.%25201a%2529.%2520When%2520we%2520consider%2520that%2520false%2520positives%2520have%2520a%2520negative%2520utility%250Acompared%2520to%2520abstaining%2520from%2520producing%2520a%2520solution%252C%2520it%2520bends%2520the%2520inference%250Ascaling%2520curve%2520further%2520downward.%2520Empirically%252C%2520we%2520find%2520that%2520the%2520optimal%2520number%2520of%250Asamples%2520can%2520be%2520less%2520than%252010%2520under%2520realistic%2520assumptions%2520%2528Fig.%25201b%2529.%2520Finally%252C%2520we%250Ashow%2520that%2520beyond%2520accuracy%252C%2520false%2520positives%2520may%2520have%2520other%2520undesirable%250Aqualities%252C%2520such%2520as%2520poor%2520adherence%2520to%2520coding%2520style%2520conventions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17501v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inference%20Scaling%20fLaws%3A%20The%20Limits%20of%20LLM%20Resampling%20with%20Imperfect%0A%20%20Verifiers&entry.906535625=Benedikt%20Stroebl%20and%20Sayash%20Kapoor%20and%20Arvind%20Narayanan&entry.1292438233=%20%20Recent%20research%20has%20generated%20hope%20that%20inference%20scaling%20could%20allow%20weaker%0Alanguage%20models%20to%20match%20or%20exceed%20the%20accuracy%20of%20stronger%20models%2C%20such%20as%20by%0Arepeatedly%20sampling%20solutions%20to%20a%20coding%20problem%20until%20it%20passes%20unit%20tests.%0AThe%20central%20thesis%20of%20this%20paper%20is%20that%20there%20is%20no%20free%20lunch%20for%20inference%0Ascaling%3A%20indefinite%20accuracy%20improvement%20through%20resampling%20can%20only%20be%0Arealized%20if%20the%20%22verifier%22%20%28in%20this%20case%2C%20a%20set%20of%20unit%20tests%29%20is%20perfect.%20When%0Athe%20verifier%20is%20imperfect%2C%20as%20it%20almost%20always%20is%20in%20domains%20such%20as%20reasoning%0Aor%20coding%20%28for%20example%2C%20unit%20tests%20have%20imperfect%20coverage%29%2C%20there%20is%20a%20nonzero%0Aprobability%20of%20false%20positives%3A%20incorrect%20solutions%20that%20pass%20the%20verifier.%0AResampling%20cannot%20decrease%20this%20probability%2C%20so%20it%20imposes%20an%20upper%20bound%20to%0Athe%20accuracy%20of%20resampling-based%20inference%20scaling%20even%20with%20an%20infinite%0Acompute%20budget.%20We%20find%20that%20there%20is%20a%20very%20strong%20correlation%20between%20the%0Amodel%27s%20single-sample%20accuracy%20%28i.e.%20accuracy%20without%20unit%20tests%29%20and%20its%20false%0Apositive%20rate%20on%20coding%20benchmarks%20HumanEval%20and%20MBPP%2C%20whose%20unit%20tests%20have%0Alimited%20coverage.%20Therefore%2C%20no%20amount%20of%20inference%20scaling%20of%20weaker%20models%0Acan%20enable%20them%20to%20match%20the%20single-sample%20accuracy%20of%20a%20sufficiently%20strong%0Amodel%20%28Fig.%201a%29.%20When%20we%20consider%20that%20false%20positives%20have%20a%20negative%20utility%0Acompared%20to%20abstaining%20from%20producing%20a%20solution%2C%20it%20bends%20the%20inference%0Ascaling%20curve%20further%20downward.%20Empirically%2C%20we%20find%20that%20the%20optimal%20number%20of%0Asamples%20can%20be%20less%20than%2010%20under%20realistic%20assumptions%20%28Fig.%201b%29.%20Finally%2C%20we%0Ashow%20that%20beyond%20accuracy%2C%20false%20positives%20may%20have%20other%20undesirable%0Aqualities%2C%20such%20as%20poor%20adherence%20to%20coding%20style%20conventions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17501v2&entry.124074799=Read"},
{"title": "Versatile Behavior Diffusion for Generalized Traffic Agent Simulation", "author": "Zhiyu Huang and Zixu Zhang and Ameya Vaidya and Yuxiao Chen and Chen Lv and Jaime Fern\u00e1ndez Fisac", "abstract": "  Existing traffic simulation models often fail to capture the complexities of\nreal-world scenarios, limiting the effective evaluation of autonomous driving\nsystems. We introduce Versatile Behavior Diffusion (VBD), a novel traffic\nscenario generation framework that utilizes diffusion generative models to\npredict scene-consistent and controllable multi-agent interactions in\nclosed-loop settings. VBD achieves state-of-the-art performance on the Waymo\nSim Agents Benchmark and can effectively produce realistic and coherent traffic\nbehaviors with complex agent interactions under diverse environmental\nconditions. Furthermore, VBD offers inference-time scenario editing through\nmulti-step refinement guided by behavior priors and model-based optimization\nobjectives. This capability allows for controllable multi-agent behavior\ngeneration, accommodating a wide range of user requirements across various\ntraffic simulation applications. Despite being trained solely on publicly\navailable datasets representing typical traffic conditions, we introduce\nconflict-prior and game-theoretic guidance approaches that enable the creation\nof interactive, long-tail safety-critical scenarios, which is essential for\ncomprehensive testing and validation of autonomous vehicles. Lastly, we provide\nin-depth insights into effective training and inference strategies for\ndiffusion-based traffic scenario generation models, highlighting best practices\nand common pitfalls. Our work significantly advances the ability to simulate\ncomplex traffic environments, offering a powerful tool for the development and\nassessment of autonomous driving technologies.\n", "link": "http://arxiv.org/abs/2404.02524v2", "date": "2024-12-02", "relevancy": 1.0754, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.559}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5322}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Versatile%20Behavior%20Diffusion%20for%20Generalized%20Traffic%20Agent%20Simulation&body=Title%3A%20Versatile%20Behavior%20Diffusion%20for%20Generalized%20Traffic%20Agent%20Simulation%0AAuthor%3A%20Zhiyu%20Huang%20and%20Zixu%20Zhang%20and%20Ameya%20Vaidya%20and%20Yuxiao%20Chen%20and%20Chen%20Lv%20and%20Jaime%20Fern%C3%A1ndez%20Fisac%0AAbstract%3A%20%20%20Existing%20traffic%20simulation%20models%20often%20fail%20to%20capture%20the%20complexities%20of%0Areal-world%20scenarios%2C%20limiting%20the%20effective%20evaluation%20of%20autonomous%20driving%0Asystems.%20We%20introduce%20Versatile%20Behavior%20Diffusion%20%28VBD%29%2C%20a%20novel%20traffic%0Ascenario%20generation%20framework%20that%20utilizes%20diffusion%20generative%20models%20to%0Apredict%20scene-consistent%20and%20controllable%20multi-agent%20interactions%20in%0Aclosed-loop%20settings.%20VBD%20achieves%20state-of-the-art%20performance%20on%20the%20Waymo%0ASim%20Agents%20Benchmark%20and%20can%20effectively%20produce%20realistic%20and%20coherent%20traffic%0Abehaviors%20with%20complex%20agent%20interactions%20under%20diverse%20environmental%0Aconditions.%20Furthermore%2C%20VBD%20offers%20inference-time%20scenario%20editing%20through%0Amulti-step%20refinement%20guided%20by%20behavior%20priors%20and%20model-based%20optimization%0Aobjectives.%20This%20capability%20allows%20for%20controllable%20multi-agent%20behavior%0Ageneration%2C%20accommodating%20a%20wide%20range%20of%20user%20requirements%20across%20various%0Atraffic%20simulation%20applications.%20Despite%20being%20trained%20solely%20on%20publicly%0Aavailable%20datasets%20representing%20typical%20traffic%20conditions%2C%20we%20introduce%0Aconflict-prior%20and%20game-theoretic%20guidance%20approaches%20that%20enable%20the%20creation%0Aof%20interactive%2C%20long-tail%20safety-critical%20scenarios%2C%20which%20is%20essential%20for%0Acomprehensive%20testing%20and%20validation%20of%20autonomous%20vehicles.%20Lastly%2C%20we%20provide%0Ain-depth%20insights%20into%20effective%20training%20and%20inference%20strategies%20for%0Adiffusion-based%20traffic%20scenario%20generation%20models%2C%20highlighting%20best%20practices%0Aand%20common%20pitfalls.%20Our%20work%20significantly%20advances%20the%20ability%20to%20simulate%0Acomplex%20traffic%20environments%2C%20offering%20a%20powerful%20tool%20for%20the%20development%20and%0Aassessment%20of%20autonomous%20driving%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02524v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVersatile%2520Behavior%2520Diffusion%2520for%2520Generalized%2520Traffic%2520Agent%2520Simulation%26entry.906535625%3DZhiyu%2520Huang%2520and%2520Zixu%2520Zhang%2520and%2520Ameya%2520Vaidya%2520and%2520Yuxiao%2520Chen%2520and%2520Chen%2520Lv%2520and%2520Jaime%2520Fern%25C3%25A1ndez%2520Fisac%26entry.1292438233%3D%2520%2520Existing%2520traffic%2520simulation%2520models%2520often%2520fail%2520to%2520capture%2520the%2520complexities%2520of%250Areal-world%2520scenarios%252C%2520limiting%2520the%2520effective%2520evaluation%2520of%2520autonomous%2520driving%250Asystems.%2520We%2520introduce%2520Versatile%2520Behavior%2520Diffusion%2520%2528VBD%2529%252C%2520a%2520novel%2520traffic%250Ascenario%2520generation%2520framework%2520that%2520utilizes%2520diffusion%2520generative%2520models%2520to%250Apredict%2520scene-consistent%2520and%2520controllable%2520multi-agent%2520interactions%2520in%250Aclosed-loop%2520settings.%2520VBD%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%2520Waymo%250ASim%2520Agents%2520Benchmark%2520and%2520can%2520effectively%2520produce%2520realistic%2520and%2520coherent%2520traffic%250Abehaviors%2520with%2520complex%2520agent%2520interactions%2520under%2520diverse%2520environmental%250Aconditions.%2520Furthermore%252C%2520VBD%2520offers%2520inference-time%2520scenario%2520editing%2520through%250Amulti-step%2520refinement%2520guided%2520by%2520behavior%2520priors%2520and%2520model-based%2520optimization%250Aobjectives.%2520This%2520capability%2520allows%2520for%2520controllable%2520multi-agent%2520behavior%250Ageneration%252C%2520accommodating%2520a%2520wide%2520range%2520of%2520user%2520requirements%2520across%2520various%250Atraffic%2520simulation%2520applications.%2520Despite%2520being%2520trained%2520solely%2520on%2520publicly%250Aavailable%2520datasets%2520representing%2520typical%2520traffic%2520conditions%252C%2520we%2520introduce%250Aconflict-prior%2520and%2520game-theoretic%2520guidance%2520approaches%2520that%2520enable%2520the%2520creation%250Aof%2520interactive%252C%2520long-tail%2520safety-critical%2520scenarios%252C%2520which%2520is%2520essential%2520for%250Acomprehensive%2520testing%2520and%2520validation%2520of%2520autonomous%2520vehicles.%2520Lastly%252C%2520we%2520provide%250Ain-depth%2520insights%2520into%2520effective%2520training%2520and%2520inference%2520strategies%2520for%250Adiffusion-based%2520traffic%2520scenario%2520generation%2520models%252C%2520highlighting%2520best%2520practices%250Aand%2520common%2520pitfalls.%2520Our%2520work%2520significantly%2520advances%2520the%2520ability%2520to%2520simulate%250Acomplex%2520traffic%2520environments%252C%2520offering%2520a%2520powerful%2520tool%2520for%2520the%2520development%2520and%250Aassessment%2520of%2520autonomous%2520driving%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02524v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Versatile%20Behavior%20Diffusion%20for%20Generalized%20Traffic%20Agent%20Simulation&entry.906535625=Zhiyu%20Huang%20and%20Zixu%20Zhang%20and%20Ameya%20Vaidya%20and%20Yuxiao%20Chen%20and%20Chen%20Lv%20and%20Jaime%20Fern%C3%A1ndez%20Fisac&entry.1292438233=%20%20Existing%20traffic%20simulation%20models%20often%20fail%20to%20capture%20the%20complexities%20of%0Areal-world%20scenarios%2C%20limiting%20the%20effective%20evaluation%20of%20autonomous%20driving%0Asystems.%20We%20introduce%20Versatile%20Behavior%20Diffusion%20%28VBD%29%2C%20a%20novel%20traffic%0Ascenario%20generation%20framework%20that%20utilizes%20diffusion%20generative%20models%20to%0Apredict%20scene-consistent%20and%20controllable%20multi-agent%20interactions%20in%0Aclosed-loop%20settings.%20VBD%20achieves%20state-of-the-art%20performance%20on%20the%20Waymo%0ASim%20Agents%20Benchmark%20and%20can%20effectively%20produce%20realistic%20and%20coherent%20traffic%0Abehaviors%20with%20complex%20agent%20interactions%20under%20diverse%20environmental%0Aconditions.%20Furthermore%2C%20VBD%20offers%20inference-time%20scenario%20editing%20through%0Amulti-step%20refinement%20guided%20by%20behavior%20priors%20and%20model-based%20optimization%0Aobjectives.%20This%20capability%20allows%20for%20controllable%20multi-agent%20behavior%0Ageneration%2C%20accommodating%20a%20wide%20range%20of%20user%20requirements%20across%20various%0Atraffic%20simulation%20applications.%20Despite%20being%20trained%20solely%20on%20publicly%0Aavailable%20datasets%20representing%20typical%20traffic%20conditions%2C%20we%20introduce%0Aconflict-prior%20and%20game-theoretic%20guidance%20approaches%20that%20enable%20the%20creation%0Aof%20interactive%2C%20long-tail%20safety-critical%20scenarios%2C%20which%20is%20essential%20for%0Acomprehensive%20testing%20and%20validation%20of%20autonomous%20vehicles.%20Lastly%2C%20we%20provide%0Ain-depth%20insights%20into%20effective%20training%20and%20inference%20strategies%20for%0Adiffusion-based%20traffic%20scenario%20generation%20models%2C%20highlighting%20best%20practices%0Aand%20common%20pitfalls.%20Our%20work%20significantly%20advances%20the%20ability%20to%20simulate%0Acomplex%20traffic%20environments%2C%20offering%20a%20powerful%20tool%20for%20the%20development%20and%0Aassessment%20of%20autonomous%20driving%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02524v2&entry.124074799=Read"},
{"title": "Free-Mask: A Novel Paradigm of Integration Between the Segmentation\n  Diffusion Model and Image Editing to Improve Segmentation Ability", "author": "Bo Gao and Fangxu Xing and Daniel Tang", "abstract": "  Current semantic segmentation models typically require a substantial amount\nof manually annotated data, a process that is both time-consuming and\nresource-intensive. Alternatively, leveraging advanced text-to-image models\nsuch as Midjourney and Stable Diffusion has emerged as an efficient strategy,\nenabling the automatic generation of synthetic data in place of manual\nannotations. However, previous methods have been limited to generating\nsingle-instance images, as the generation of multiple instances with Stable\nDiffusion has proven unstable. To address this limitation and expand the scope\nand diversity of synthetic datasets, we propose a framework \\textbf{Free-Mask}\nthat combines a Diffusion Model for segmentation with advanced image editing\ncapabilities, allowing for the integration of multiple objects into images via\ntext-to-image models. Our method facilitates the creation of highly realistic\ndatasets that closely emulate open-world environments while generating accurate\nsegmentation masks. It reduces the labor associated with manual annotation and\nalso ensures precise mask generation. Experimental results demonstrate that\nsynthetic data generated by \\textbf{Free-Mask} enables segmentation models to\noutperform those trained on real data, especially in zero-shot settings.\nNotably, \\textbf{Free-Mask} achieves new state-of-the-art results on previously\nunseen classes in the VOC 2012 benchmark.\n", "link": "http://arxiv.org/abs/2411.01819v2", "date": "2024-12-02", "relevancy": 1.264, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6583}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.633}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Free-Mask%3A%20A%20Novel%20Paradigm%20of%20Integration%20Between%20the%20Segmentation%0A%20%20Diffusion%20Model%20and%20Image%20Editing%20to%20Improve%20Segmentation%20Ability&body=Title%3A%20Free-Mask%3A%20A%20Novel%20Paradigm%20of%20Integration%20Between%20the%20Segmentation%0A%20%20Diffusion%20Model%20and%20Image%20Editing%20to%20Improve%20Segmentation%20Ability%0AAuthor%3A%20Bo%20Gao%20and%20Fangxu%20Xing%20and%20Daniel%20Tang%0AAbstract%3A%20%20%20Current%20semantic%20segmentation%20models%20typically%20require%20a%20substantial%20amount%0Aof%20manually%20annotated%20data%2C%20a%20process%20that%20is%20both%20time-consuming%20and%0Aresource-intensive.%20Alternatively%2C%20leveraging%20advanced%20text-to-image%20models%0Asuch%20as%20Midjourney%20and%20Stable%20Diffusion%20has%20emerged%20as%20an%20efficient%20strategy%2C%0Aenabling%20the%20automatic%20generation%20of%20synthetic%20data%20in%20place%20of%20manual%0Aannotations.%20However%2C%20previous%20methods%20have%20been%20limited%20to%20generating%0Asingle-instance%20images%2C%20as%20the%20generation%20of%20multiple%20instances%20with%20Stable%0ADiffusion%20has%20proven%20unstable.%20To%20address%20this%20limitation%20and%20expand%20the%20scope%0Aand%20diversity%20of%20synthetic%20datasets%2C%20we%20propose%20a%20framework%20%5Ctextbf%7BFree-Mask%7D%0Athat%20combines%20a%20Diffusion%20Model%20for%20segmentation%20with%20advanced%20image%20editing%0Acapabilities%2C%20allowing%20for%20the%20integration%20of%20multiple%20objects%20into%20images%20via%0Atext-to-image%20models.%20Our%20method%20facilitates%20the%20creation%20of%20highly%20realistic%0Adatasets%20that%20closely%20emulate%20open-world%20environments%20while%20generating%20accurate%0Asegmentation%20masks.%20It%20reduces%20the%20labor%20associated%20with%20manual%20annotation%20and%0Aalso%20ensures%20precise%20mask%20generation.%20Experimental%20results%20demonstrate%20that%0Asynthetic%20data%20generated%20by%20%5Ctextbf%7BFree-Mask%7D%20enables%20segmentation%20models%20to%0Aoutperform%20those%20trained%20on%20real%20data%2C%20especially%20in%20zero-shot%20settings.%0ANotably%2C%20%5Ctextbf%7BFree-Mask%7D%20achieves%20new%20state-of-the-art%20results%20on%20previously%0Aunseen%20classes%20in%20the%20VOC%202012%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01819v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFree-Mask%253A%2520A%2520Novel%2520Paradigm%2520of%2520Integration%2520Between%2520the%2520Segmentation%250A%2520%2520Diffusion%2520Model%2520and%2520Image%2520Editing%2520to%2520Improve%2520Segmentation%2520Ability%26entry.906535625%3DBo%2520Gao%2520and%2520Fangxu%2520Xing%2520and%2520Daniel%2520Tang%26entry.1292438233%3D%2520%2520Current%2520semantic%2520segmentation%2520models%2520typically%2520require%2520a%2520substantial%2520amount%250Aof%2520manually%2520annotated%2520data%252C%2520a%2520process%2520that%2520is%2520both%2520time-consuming%2520and%250Aresource-intensive.%2520Alternatively%252C%2520leveraging%2520advanced%2520text-to-image%2520models%250Asuch%2520as%2520Midjourney%2520and%2520Stable%2520Diffusion%2520has%2520emerged%2520as%2520an%2520efficient%2520strategy%252C%250Aenabling%2520the%2520automatic%2520generation%2520of%2520synthetic%2520data%2520in%2520place%2520of%2520manual%250Aannotations.%2520However%252C%2520previous%2520methods%2520have%2520been%2520limited%2520to%2520generating%250Asingle-instance%2520images%252C%2520as%2520the%2520generation%2520of%2520multiple%2520instances%2520with%2520Stable%250ADiffusion%2520has%2520proven%2520unstable.%2520To%2520address%2520this%2520limitation%2520and%2520expand%2520the%2520scope%250Aand%2520diversity%2520of%2520synthetic%2520datasets%252C%2520we%2520propose%2520a%2520framework%2520%255Ctextbf%257BFree-Mask%257D%250Athat%2520combines%2520a%2520Diffusion%2520Model%2520for%2520segmentation%2520with%2520advanced%2520image%2520editing%250Acapabilities%252C%2520allowing%2520for%2520the%2520integration%2520of%2520multiple%2520objects%2520into%2520images%2520via%250Atext-to-image%2520models.%2520Our%2520method%2520facilitates%2520the%2520creation%2520of%2520highly%2520realistic%250Adatasets%2520that%2520closely%2520emulate%2520open-world%2520environments%2520while%2520generating%2520accurate%250Asegmentation%2520masks.%2520It%2520reduces%2520the%2520labor%2520associated%2520with%2520manual%2520annotation%2520and%250Aalso%2520ensures%2520precise%2520mask%2520generation.%2520Experimental%2520results%2520demonstrate%2520that%250Asynthetic%2520data%2520generated%2520by%2520%255Ctextbf%257BFree-Mask%257D%2520enables%2520segmentation%2520models%2520to%250Aoutperform%2520those%2520trained%2520on%2520real%2520data%252C%2520especially%2520in%2520zero-shot%2520settings.%250ANotably%252C%2520%255Ctextbf%257BFree-Mask%257D%2520achieves%2520new%2520state-of-the-art%2520results%2520on%2520previously%250Aunseen%2520classes%2520in%2520the%2520VOC%25202012%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01819v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Free-Mask%3A%20A%20Novel%20Paradigm%20of%20Integration%20Between%20the%20Segmentation%0A%20%20Diffusion%20Model%20and%20Image%20Editing%20to%20Improve%20Segmentation%20Ability&entry.906535625=Bo%20Gao%20and%20Fangxu%20Xing%20and%20Daniel%20Tang&entry.1292438233=%20%20Current%20semantic%20segmentation%20models%20typically%20require%20a%20substantial%20amount%0Aof%20manually%20annotated%20data%2C%20a%20process%20that%20is%20both%20time-consuming%20and%0Aresource-intensive.%20Alternatively%2C%20leveraging%20advanced%20text-to-image%20models%0Asuch%20as%20Midjourney%20and%20Stable%20Diffusion%20has%20emerged%20as%20an%20efficient%20strategy%2C%0Aenabling%20the%20automatic%20generation%20of%20synthetic%20data%20in%20place%20of%20manual%0Aannotations.%20However%2C%20previous%20methods%20have%20been%20limited%20to%20generating%0Asingle-instance%20images%2C%20as%20the%20generation%20of%20multiple%20instances%20with%20Stable%0ADiffusion%20has%20proven%20unstable.%20To%20address%20this%20limitation%20and%20expand%20the%20scope%0Aand%20diversity%20of%20synthetic%20datasets%2C%20we%20propose%20a%20framework%20%5Ctextbf%7BFree-Mask%7D%0Athat%20combines%20a%20Diffusion%20Model%20for%20segmentation%20with%20advanced%20image%20editing%0Acapabilities%2C%20allowing%20for%20the%20integration%20of%20multiple%20objects%20into%20images%20via%0Atext-to-image%20models.%20Our%20method%20facilitates%20the%20creation%20of%20highly%20realistic%0Adatasets%20that%20closely%20emulate%20open-world%20environments%20while%20generating%20accurate%0Asegmentation%20masks.%20It%20reduces%20the%20labor%20associated%20with%20manual%20annotation%20and%0Aalso%20ensures%20precise%20mask%20generation.%20Experimental%20results%20demonstrate%20that%0Asynthetic%20data%20generated%20by%20%5Ctextbf%7BFree-Mask%7D%20enables%20segmentation%20models%20to%0Aoutperform%20those%20trained%20on%20real%20data%2C%20especially%20in%20zero-shot%20settings.%0ANotably%2C%20%5Ctextbf%7BFree-Mask%7D%20achieves%20new%20state-of-the-art%20results%20on%20previously%0Aunseen%20classes%20in%20the%20VOC%202012%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01819v2&entry.124074799=Read"},
{"title": "Latent Diffusion for Neural Spiking Data", "author": "Jaivardhan Kapoor and Auguste Schulz and Julius Vetter and Felix Pei and Richard Gao and Jakob H. Macke", "abstract": "  Modern datasets in neuroscience enable unprecedented inquiries into the\nrelationship between complex behaviors and the activity of many simultaneously\nrecorded neurons. While latent variable models can successfully extract\nlow-dimensional embeddings from such recordings, using them to generate\nrealistic spiking data, especially in a behavior-dependent manner, still poses\na challenge. Here, we present Latent Diffusion for Neural Spiking data (LDNS),\na diffusion-based generative model with a low-dimensional latent space: LDNS\nemploys an autoencoder with structured state-space (S4) layers to project\ndiscrete high-dimensional spiking data into continuous time-aligned latents. On\nthese inferred latents, we train expressive (conditional) diffusion models,\nenabling us to sample neural activity with realistic single-neuron and\npopulation spiking statistics. We validate LDNS on synthetic data, accurately\nrecovering latent structure, firing rates, and spiking statistics. Next, we\ndemonstrate its flexibility by generating variable-length data that mimics\nhuman cortical activity during attempted speech. We show how to equip LDNS with\nan expressive observation model that accounts for single-neuron dynamics not\nmediated by the latent state, further increasing the realism of generated\nsamples. Finally, conditional LDNS trained on motor cortical activity during\ndiverse reaching behaviors can generate realistic spiking data given reach\ndirection or unseen reach trajectories. In summary, LDNS simultaneously enables\ninference of low-dimensional latents and realistic conditional generation of\nneural spiking datasets, opening up further possibilities for simulating\nexperimentally testable hypotheses.\n", "link": "http://arxiv.org/abs/2407.08751v2", "date": "2024-12-02", "relevancy": 1.1343, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6423}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.532}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Diffusion%20for%20Neural%20Spiking%20Data&body=Title%3A%20Latent%20Diffusion%20for%20Neural%20Spiking%20Data%0AAuthor%3A%20Jaivardhan%20Kapoor%20and%20Auguste%20Schulz%20and%20Julius%20Vetter%20and%20Felix%20Pei%20and%20Richard%20Gao%20and%20Jakob%20H.%20Macke%0AAbstract%3A%20%20%20Modern%20datasets%20in%20neuroscience%20enable%20unprecedented%20inquiries%20into%20the%0Arelationship%20between%20complex%20behaviors%20and%20the%20activity%20of%20many%20simultaneously%0Arecorded%20neurons.%20While%20latent%20variable%20models%20can%20successfully%20extract%0Alow-dimensional%20embeddings%20from%20such%20recordings%2C%20using%20them%20to%20generate%0Arealistic%20spiking%20data%2C%20especially%20in%20a%20behavior-dependent%20manner%2C%20still%20poses%0Aa%20challenge.%20Here%2C%20we%20present%20Latent%20Diffusion%20for%20Neural%20Spiking%20data%20%28LDNS%29%2C%0Aa%20diffusion-based%20generative%20model%20with%20a%20low-dimensional%20latent%20space%3A%20LDNS%0Aemploys%20an%20autoencoder%20with%20structured%20state-space%20%28S4%29%20layers%20to%20project%0Adiscrete%20high-dimensional%20spiking%20data%20into%20continuous%20time-aligned%20latents.%20On%0Athese%20inferred%20latents%2C%20we%20train%20expressive%20%28conditional%29%20diffusion%20models%2C%0Aenabling%20us%20to%20sample%20neural%20activity%20with%20realistic%20single-neuron%20and%0Apopulation%20spiking%20statistics.%20We%20validate%20LDNS%20on%20synthetic%20data%2C%20accurately%0Arecovering%20latent%20structure%2C%20firing%20rates%2C%20and%20spiking%20statistics.%20Next%2C%20we%0Ademonstrate%20its%20flexibility%20by%20generating%20variable-length%20data%20that%20mimics%0Ahuman%20cortical%20activity%20during%20attempted%20speech.%20We%20show%20how%20to%20equip%20LDNS%20with%0Aan%20expressive%20observation%20model%20that%20accounts%20for%20single-neuron%20dynamics%20not%0Amediated%20by%20the%20latent%20state%2C%20further%20increasing%20the%20realism%20of%20generated%0Asamples.%20Finally%2C%20conditional%20LDNS%20trained%20on%20motor%20cortical%20activity%20during%0Adiverse%20reaching%20behaviors%20can%20generate%20realistic%20spiking%20data%20given%20reach%0Adirection%20or%20unseen%20reach%20trajectories.%20In%20summary%2C%20LDNS%20simultaneously%20enables%0Ainference%20of%20low-dimensional%20latents%20and%20realistic%20conditional%20generation%20of%0Aneural%20spiking%20datasets%2C%20opening%20up%20further%20possibilities%20for%20simulating%0Aexperimentally%20testable%20hypotheses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08751v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Diffusion%2520for%2520Neural%2520Spiking%2520Data%26entry.906535625%3DJaivardhan%2520Kapoor%2520and%2520Auguste%2520Schulz%2520and%2520Julius%2520Vetter%2520and%2520Felix%2520Pei%2520and%2520Richard%2520Gao%2520and%2520Jakob%2520H.%2520Macke%26entry.1292438233%3D%2520%2520Modern%2520datasets%2520in%2520neuroscience%2520enable%2520unprecedented%2520inquiries%2520into%2520the%250Arelationship%2520between%2520complex%2520behaviors%2520and%2520the%2520activity%2520of%2520many%2520simultaneously%250Arecorded%2520neurons.%2520While%2520latent%2520variable%2520models%2520can%2520successfully%2520extract%250Alow-dimensional%2520embeddings%2520from%2520such%2520recordings%252C%2520using%2520them%2520to%2520generate%250Arealistic%2520spiking%2520data%252C%2520especially%2520in%2520a%2520behavior-dependent%2520manner%252C%2520still%2520poses%250Aa%2520challenge.%2520Here%252C%2520we%2520present%2520Latent%2520Diffusion%2520for%2520Neural%2520Spiking%2520data%2520%2528LDNS%2529%252C%250Aa%2520diffusion-based%2520generative%2520model%2520with%2520a%2520low-dimensional%2520latent%2520space%253A%2520LDNS%250Aemploys%2520an%2520autoencoder%2520with%2520structured%2520state-space%2520%2528S4%2529%2520layers%2520to%2520project%250Adiscrete%2520high-dimensional%2520spiking%2520data%2520into%2520continuous%2520time-aligned%2520latents.%2520On%250Athese%2520inferred%2520latents%252C%2520we%2520train%2520expressive%2520%2528conditional%2529%2520diffusion%2520models%252C%250Aenabling%2520us%2520to%2520sample%2520neural%2520activity%2520with%2520realistic%2520single-neuron%2520and%250Apopulation%2520spiking%2520statistics.%2520We%2520validate%2520LDNS%2520on%2520synthetic%2520data%252C%2520accurately%250Arecovering%2520latent%2520structure%252C%2520firing%2520rates%252C%2520and%2520spiking%2520statistics.%2520Next%252C%2520we%250Ademonstrate%2520its%2520flexibility%2520by%2520generating%2520variable-length%2520data%2520that%2520mimics%250Ahuman%2520cortical%2520activity%2520during%2520attempted%2520speech.%2520We%2520show%2520how%2520to%2520equip%2520LDNS%2520with%250Aan%2520expressive%2520observation%2520model%2520that%2520accounts%2520for%2520single-neuron%2520dynamics%2520not%250Amediated%2520by%2520the%2520latent%2520state%252C%2520further%2520increasing%2520the%2520realism%2520of%2520generated%250Asamples.%2520Finally%252C%2520conditional%2520LDNS%2520trained%2520on%2520motor%2520cortical%2520activity%2520during%250Adiverse%2520reaching%2520behaviors%2520can%2520generate%2520realistic%2520spiking%2520data%2520given%2520reach%250Adirection%2520or%2520unseen%2520reach%2520trajectories.%2520In%2520summary%252C%2520LDNS%2520simultaneously%2520enables%250Ainference%2520of%2520low-dimensional%2520latents%2520and%2520realistic%2520conditional%2520generation%2520of%250Aneural%2520spiking%2520datasets%252C%2520opening%2520up%2520further%2520possibilities%2520for%2520simulating%250Aexperimentally%2520testable%2520hypotheses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08751v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Diffusion%20for%20Neural%20Spiking%20Data&entry.906535625=Jaivardhan%20Kapoor%20and%20Auguste%20Schulz%20and%20Julius%20Vetter%20and%20Felix%20Pei%20and%20Richard%20Gao%20and%20Jakob%20H.%20Macke&entry.1292438233=%20%20Modern%20datasets%20in%20neuroscience%20enable%20unprecedented%20inquiries%20into%20the%0Arelationship%20between%20complex%20behaviors%20and%20the%20activity%20of%20many%20simultaneously%0Arecorded%20neurons.%20While%20latent%20variable%20models%20can%20successfully%20extract%0Alow-dimensional%20embeddings%20from%20such%20recordings%2C%20using%20them%20to%20generate%0Arealistic%20spiking%20data%2C%20especially%20in%20a%20behavior-dependent%20manner%2C%20still%20poses%0Aa%20challenge.%20Here%2C%20we%20present%20Latent%20Diffusion%20for%20Neural%20Spiking%20data%20%28LDNS%29%2C%0Aa%20diffusion-based%20generative%20model%20with%20a%20low-dimensional%20latent%20space%3A%20LDNS%0Aemploys%20an%20autoencoder%20with%20structured%20state-space%20%28S4%29%20layers%20to%20project%0Adiscrete%20high-dimensional%20spiking%20data%20into%20continuous%20time-aligned%20latents.%20On%0Athese%20inferred%20latents%2C%20we%20train%20expressive%20%28conditional%29%20diffusion%20models%2C%0Aenabling%20us%20to%20sample%20neural%20activity%20with%20realistic%20single-neuron%20and%0Apopulation%20spiking%20statistics.%20We%20validate%20LDNS%20on%20synthetic%20data%2C%20accurately%0Arecovering%20latent%20structure%2C%20firing%20rates%2C%20and%20spiking%20statistics.%20Next%2C%20we%0Ademonstrate%20its%20flexibility%20by%20generating%20variable-length%20data%20that%20mimics%0Ahuman%20cortical%20activity%20during%20attempted%20speech.%20We%20show%20how%20to%20equip%20LDNS%20with%0Aan%20expressive%20observation%20model%20that%20accounts%20for%20single-neuron%20dynamics%20not%0Amediated%20by%20the%20latent%20state%2C%20further%20increasing%20the%20realism%20of%20generated%0Asamples.%20Finally%2C%20conditional%20LDNS%20trained%20on%20motor%20cortical%20activity%20during%0Adiverse%20reaching%20behaviors%20can%20generate%20realistic%20spiking%20data%20given%20reach%0Adirection%20or%20unseen%20reach%20trajectories.%20In%20summary%2C%20LDNS%20simultaneously%20enables%0Ainference%20of%20low-dimensional%20latents%20and%20realistic%20conditional%20generation%20of%0Aneural%20spiking%20datasets%2C%20opening%20up%20further%20possibilities%20for%20simulating%0Aexperimentally%20testable%20hypotheses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08751v2&entry.124074799=Read"},
{"title": "Discovering group dynamics in coordinated time series via hierarchical\n  recurrent switching-state models", "author": "Michael T. Wojnowicz and Kaitlin Gili and Preetish Rath and Eric Miller and Jeffrey Miller and Clifford Hancock and Meghan O'Donovan and Seth Elkin-Frankston and Tad T. Bruny\u00e9 and Michael C. Hughes", "abstract": "  We seek a computationally efficient model for a collection of time series\narising from multiple interacting entities (a.k.a. \"agents\"). Recent models of\nspatiotemporal patterns across individuals fail to incorporate explicit\nsystem-level collective behavior that can influence the trajectories of\nindividual entities. To address this gap in the literature, we present a new\nhierarchical switching-state model that can be trained in an unsupervised\nfashion to simultaneously learn both system-level and individual-level\ndynamics. We employ a latent system-level discrete state Markov chain that\nprovides top-down influence on latent entity-level chains which in turn govern\nthe emission of each observed time series. Recurrent feedback from the\nobservations to the latent chains at both entity and system levels allows\nrecent situational context to inform how dynamics unfold at all levels in\nbottom-up fashion. We hypothesize that including both top-down and bottom-up\ninfluences on group dynamics will improve interpretability of the learned\ndynamics and reduce error when forecasting. Our hierarchical switching\nrecurrent dynamical model can be learned via closed-form variational coordinate\nascent updates to all latent chains that scale linearly in the number of\nentities. This is asymptotically no more costly than fitting a separate model\nfor each entity. Analysis of both synthetic data and real basketball team\nmovements suggests our lean parametric model can achieve competitive forecasts\ncompared to larger neural network models that require far more computational\nresources. Further experiments on soldier data as well as a synthetic task with\n64 cooperating entities show how our approach can yield interpretable insights\nabout team dynamics over time.\n", "link": "http://arxiv.org/abs/2401.14973v2", "date": "2024-12-02", "relevancy": 0.9803, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5058}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4824}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovering%20group%20dynamics%20in%20coordinated%20time%20series%20via%20hierarchical%0A%20%20recurrent%20switching-state%20models&body=Title%3A%20Discovering%20group%20dynamics%20in%20coordinated%20time%20series%20via%20hierarchical%0A%20%20recurrent%20switching-state%20models%0AAuthor%3A%20Michael%20T.%20Wojnowicz%20and%20Kaitlin%20Gili%20and%20Preetish%20Rath%20and%20Eric%20Miller%20and%20Jeffrey%20Miller%20and%20Clifford%20Hancock%20and%20Meghan%20O%27Donovan%20and%20Seth%20Elkin-Frankston%20and%20Tad%20T.%20Bruny%C3%A9%20and%20Michael%20C.%20Hughes%0AAbstract%3A%20%20%20We%20seek%20a%20computationally%20efficient%20model%20for%20a%20collection%20of%20time%20series%0Aarising%20from%20multiple%20interacting%20entities%20%28a.k.a.%20%22agents%22%29.%20Recent%20models%20of%0Aspatiotemporal%20patterns%20across%20individuals%20fail%20to%20incorporate%20explicit%0Asystem-level%20collective%20behavior%20that%20can%20influence%20the%20trajectories%20of%0Aindividual%20entities.%20To%20address%20this%20gap%20in%20the%20literature%2C%20we%20present%20a%20new%0Ahierarchical%20switching-state%20model%20that%20can%20be%20trained%20in%20an%20unsupervised%0Afashion%20to%20simultaneously%20learn%20both%20system-level%20and%20individual-level%0Adynamics.%20We%20employ%20a%20latent%20system-level%20discrete%20state%20Markov%20chain%20that%0Aprovides%20top-down%20influence%20on%20latent%20entity-level%20chains%20which%20in%20turn%20govern%0Athe%20emission%20of%20each%20observed%20time%20series.%20Recurrent%20feedback%20from%20the%0Aobservations%20to%20the%20latent%20chains%20at%20both%20entity%20and%20system%20levels%20allows%0Arecent%20situational%20context%20to%20inform%20how%20dynamics%20unfold%20at%20all%20levels%20in%0Abottom-up%20fashion.%20We%20hypothesize%20that%20including%20both%20top-down%20and%20bottom-up%0Ainfluences%20on%20group%20dynamics%20will%20improve%20interpretability%20of%20the%20learned%0Adynamics%20and%20reduce%20error%20when%20forecasting.%20Our%20hierarchical%20switching%0Arecurrent%20dynamical%20model%20can%20be%20learned%20via%20closed-form%20variational%20coordinate%0Aascent%20updates%20to%20all%20latent%20chains%20that%20scale%20linearly%20in%20the%20number%20of%0Aentities.%20This%20is%20asymptotically%20no%20more%20costly%20than%20fitting%20a%20separate%20model%0Afor%20each%20entity.%20Analysis%20of%20both%20synthetic%20data%20and%20real%20basketball%20team%0Amovements%20suggests%20our%20lean%20parametric%20model%20can%20achieve%20competitive%20forecasts%0Acompared%20to%20larger%20neural%20network%20models%20that%20require%20far%20more%20computational%0Aresources.%20Further%20experiments%20on%20soldier%20data%20as%20well%20as%20a%20synthetic%20task%20with%0A64%20cooperating%20entities%20show%20how%20our%20approach%20can%20yield%20interpretable%20insights%0Aabout%20team%20dynamics%20over%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.14973v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovering%2520group%2520dynamics%2520in%2520coordinated%2520time%2520series%2520via%2520hierarchical%250A%2520%2520recurrent%2520switching-state%2520models%26entry.906535625%3DMichael%2520T.%2520Wojnowicz%2520and%2520Kaitlin%2520Gili%2520and%2520Preetish%2520Rath%2520and%2520Eric%2520Miller%2520and%2520Jeffrey%2520Miller%2520and%2520Clifford%2520Hancock%2520and%2520Meghan%2520O%2527Donovan%2520and%2520Seth%2520Elkin-Frankston%2520and%2520Tad%2520T.%2520Bruny%25C3%25A9%2520and%2520Michael%2520C.%2520Hughes%26entry.1292438233%3D%2520%2520We%2520seek%2520a%2520computationally%2520efficient%2520model%2520for%2520a%2520collection%2520of%2520time%2520series%250Aarising%2520from%2520multiple%2520interacting%2520entities%2520%2528a.k.a.%2520%2522agents%2522%2529.%2520Recent%2520models%2520of%250Aspatiotemporal%2520patterns%2520across%2520individuals%2520fail%2520to%2520incorporate%2520explicit%250Asystem-level%2520collective%2520behavior%2520that%2520can%2520influence%2520the%2520trajectories%2520of%250Aindividual%2520entities.%2520To%2520address%2520this%2520gap%2520in%2520the%2520literature%252C%2520we%2520present%2520a%2520new%250Ahierarchical%2520switching-state%2520model%2520that%2520can%2520be%2520trained%2520in%2520an%2520unsupervised%250Afashion%2520to%2520simultaneously%2520learn%2520both%2520system-level%2520and%2520individual-level%250Adynamics.%2520We%2520employ%2520a%2520latent%2520system-level%2520discrete%2520state%2520Markov%2520chain%2520that%250Aprovides%2520top-down%2520influence%2520on%2520latent%2520entity-level%2520chains%2520which%2520in%2520turn%2520govern%250Athe%2520emission%2520of%2520each%2520observed%2520time%2520series.%2520Recurrent%2520feedback%2520from%2520the%250Aobservations%2520to%2520the%2520latent%2520chains%2520at%2520both%2520entity%2520and%2520system%2520levels%2520allows%250Arecent%2520situational%2520context%2520to%2520inform%2520how%2520dynamics%2520unfold%2520at%2520all%2520levels%2520in%250Abottom-up%2520fashion.%2520We%2520hypothesize%2520that%2520including%2520both%2520top-down%2520and%2520bottom-up%250Ainfluences%2520on%2520group%2520dynamics%2520will%2520improve%2520interpretability%2520of%2520the%2520learned%250Adynamics%2520and%2520reduce%2520error%2520when%2520forecasting.%2520Our%2520hierarchical%2520switching%250Arecurrent%2520dynamical%2520model%2520can%2520be%2520learned%2520via%2520closed-form%2520variational%2520coordinate%250Aascent%2520updates%2520to%2520all%2520latent%2520chains%2520that%2520scale%2520linearly%2520in%2520the%2520number%2520of%250Aentities.%2520This%2520is%2520asymptotically%2520no%2520more%2520costly%2520than%2520fitting%2520a%2520separate%2520model%250Afor%2520each%2520entity.%2520Analysis%2520of%2520both%2520synthetic%2520data%2520and%2520real%2520basketball%2520team%250Amovements%2520suggests%2520our%2520lean%2520parametric%2520model%2520can%2520achieve%2520competitive%2520forecasts%250Acompared%2520to%2520larger%2520neural%2520network%2520models%2520that%2520require%2520far%2520more%2520computational%250Aresources.%2520Further%2520experiments%2520on%2520soldier%2520data%2520as%2520well%2520as%2520a%2520synthetic%2520task%2520with%250A64%2520cooperating%2520entities%2520show%2520how%2520our%2520approach%2520can%2520yield%2520interpretable%2520insights%250Aabout%2520team%2520dynamics%2520over%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.14973v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovering%20group%20dynamics%20in%20coordinated%20time%20series%20via%20hierarchical%0A%20%20recurrent%20switching-state%20models&entry.906535625=Michael%20T.%20Wojnowicz%20and%20Kaitlin%20Gili%20and%20Preetish%20Rath%20and%20Eric%20Miller%20and%20Jeffrey%20Miller%20and%20Clifford%20Hancock%20and%20Meghan%20O%27Donovan%20and%20Seth%20Elkin-Frankston%20and%20Tad%20T.%20Bruny%C3%A9%20and%20Michael%20C.%20Hughes&entry.1292438233=%20%20We%20seek%20a%20computationally%20efficient%20model%20for%20a%20collection%20of%20time%20series%0Aarising%20from%20multiple%20interacting%20entities%20%28a.k.a.%20%22agents%22%29.%20Recent%20models%20of%0Aspatiotemporal%20patterns%20across%20individuals%20fail%20to%20incorporate%20explicit%0Asystem-level%20collective%20behavior%20that%20can%20influence%20the%20trajectories%20of%0Aindividual%20entities.%20To%20address%20this%20gap%20in%20the%20literature%2C%20we%20present%20a%20new%0Ahierarchical%20switching-state%20model%20that%20can%20be%20trained%20in%20an%20unsupervised%0Afashion%20to%20simultaneously%20learn%20both%20system-level%20and%20individual-level%0Adynamics.%20We%20employ%20a%20latent%20system-level%20discrete%20state%20Markov%20chain%20that%0Aprovides%20top-down%20influence%20on%20latent%20entity-level%20chains%20which%20in%20turn%20govern%0Athe%20emission%20of%20each%20observed%20time%20series.%20Recurrent%20feedback%20from%20the%0Aobservations%20to%20the%20latent%20chains%20at%20both%20entity%20and%20system%20levels%20allows%0Arecent%20situational%20context%20to%20inform%20how%20dynamics%20unfold%20at%20all%20levels%20in%0Abottom-up%20fashion.%20We%20hypothesize%20that%20including%20both%20top-down%20and%20bottom-up%0Ainfluences%20on%20group%20dynamics%20will%20improve%20interpretability%20of%20the%20learned%0Adynamics%20and%20reduce%20error%20when%20forecasting.%20Our%20hierarchical%20switching%0Arecurrent%20dynamical%20model%20can%20be%20learned%20via%20closed-form%20variational%20coordinate%0Aascent%20updates%20to%20all%20latent%20chains%20that%20scale%20linearly%20in%20the%20number%20of%0Aentities.%20This%20is%20asymptotically%20no%20more%20costly%20than%20fitting%20a%20separate%20model%0Afor%20each%20entity.%20Analysis%20of%20both%20synthetic%20data%20and%20real%20basketball%20team%0Amovements%20suggests%20our%20lean%20parametric%20model%20can%20achieve%20competitive%20forecasts%0Acompared%20to%20larger%20neural%20network%20models%20that%20require%20far%20more%20computational%0Aresources.%20Further%20experiments%20on%20soldier%20data%20as%20well%20as%20a%20synthetic%20task%20with%0A64%20cooperating%20entities%20show%20how%20our%20approach%20can%20yield%20interpretable%20insights%0Aabout%20team%20dynamics%20over%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14973v2&entry.124074799=Read"},
{"title": "Regression Trees Know Calculus", "author": "Nathan Wycoff", "abstract": "  Regression trees have emerged as a preeminent tool for solving real-world\nregression problems due to their ability to deal with nonlinearities,\ninteraction effects and sharp discontinuities. In this article, we rather study\nregression trees applied to well-behaved, differentiable functions, and\ndetermine the relationship between node parameters and the local gradient of\nthe function being approximated. We find a simple estimate of the gradient\nwhich can be efficiently computed using quantities exposed by popular tree\nlearning libraries. This allows the tools developed in the context of\ndifferentiable algorithms, like neural nets and Gaussian processes, to be\ndeployed to tree-based models. To demonstrate this, we study measures of model\nsensitivity defined in terms of integrals of gradients and demonstrate how to\ncompute them for regression trees using the proposed gradient estimates.\nQuantitative and qualitative numerical experiments reveal the capability of\ngradients estimated by regression trees to improve predictive analysis, solve\ntasks in uncertainty quantification, and provide interpretation of model\nbehavior.\n", "link": "http://arxiv.org/abs/2405.13846v2", "date": "2024-12-02", "relevancy": 1.2911, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.435}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4344}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regression%20Trees%20Know%20Calculus&body=Title%3A%20Regression%20Trees%20Know%20Calculus%0AAuthor%3A%20Nathan%20Wycoff%0AAbstract%3A%20%20%20Regression%20trees%20have%20emerged%20as%20a%20preeminent%20tool%20for%20solving%20real-world%0Aregression%20problems%20due%20to%20their%20ability%20to%20deal%20with%20nonlinearities%2C%0Ainteraction%20effects%20and%20sharp%20discontinuities.%20In%20this%20article%2C%20we%20rather%20study%0Aregression%20trees%20applied%20to%20well-behaved%2C%20differentiable%20functions%2C%20and%0Adetermine%20the%20relationship%20between%20node%20parameters%20and%20the%20local%20gradient%20of%0Athe%20function%20being%20approximated.%20We%20find%20a%20simple%20estimate%20of%20the%20gradient%0Awhich%20can%20be%20efficiently%20computed%20using%20quantities%20exposed%20by%20popular%20tree%0Alearning%20libraries.%20This%20allows%20the%20tools%20developed%20in%20the%20context%20of%0Adifferentiable%20algorithms%2C%20like%20neural%20nets%20and%20Gaussian%20processes%2C%20to%20be%0Adeployed%20to%20tree-based%20models.%20To%20demonstrate%20this%2C%20we%20study%20measures%20of%20model%0Asensitivity%20defined%20in%20terms%20of%20integrals%20of%20gradients%20and%20demonstrate%20how%20to%0Acompute%20them%20for%20regression%20trees%20using%20the%20proposed%20gradient%20estimates.%0AQuantitative%20and%20qualitative%20numerical%20experiments%20reveal%20the%20capability%20of%0Agradients%20estimated%20by%20regression%20trees%20to%20improve%20predictive%20analysis%2C%20solve%0Atasks%20in%20uncertainty%20quantification%2C%20and%20provide%20interpretation%20of%20model%0Abehavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13846v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegression%2520Trees%2520Know%2520Calculus%26entry.906535625%3DNathan%2520Wycoff%26entry.1292438233%3D%2520%2520Regression%2520trees%2520have%2520emerged%2520as%2520a%2520preeminent%2520tool%2520for%2520solving%2520real-world%250Aregression%2520problems%2520due%2520to%2520their%2520ability%2520to%2520deal%2520with%2520nonlinearities%252C%250Ainteraction%2520effects%2520and%2520sharp%2520discontinuities.%2520In%2520this%2520article%252C%2520we%2520rather%2520study%250Aregression%2520trees%2520applied%2520to%2520well-behaved%252C%2520differentiable%2520functions%252C%2520and%250Adetermine%2520the%2520relationship%2520between%2520node%2520parameters%2520and%2520the%2520local%2520gradient%2520of%250Athe%2520function%2520being%2520approximated.%2520We%2520find%2520a%2520simple%2520estimate%2520of%2520the%2520gradient%250Awhich%2520can%2520be%2520efficiently%2520computed%2520using%2520quantities%2520exposed%2520by%2520popular%2520tree%250Alearning%2520libraries.%2520This%2520allows%2520the%2520tools%2520developed%2520in%2520the%2520context%2520of%250Adifferentiable%2520algorithms%252C%2520like%2520neural%2520nets%2520and%2520Gaussian%2520processes%252C%2520to%2520be%250Adeployed%2520to%2520tree-based%2520models.%2520To%2520demonstrate%2520this%252C%2520we%2520study%2520measures%2520of%2520model%250Asensitivity%2520defined%2520in%2520terms%2520of%2520integrals%2520of%2520gradients%2520and%2520demonstrate%2520how%2520to%250Acompute%2520them%2520for%2520regression%2520trees%2520using%2520the%2520proposed%2520gradient%2520estimates.%250AQuantitative%2520and%2520qualitative%2520numerical%2520experiments%2520reveal%2520the%2520capability%2520of%250Agradients%2520estimated%2520by%2520regression%2520trees%2520to%2520improve%2520predictive%2520analysis%252C%2520solve%250Atasks%2520in%2520uncertainty%2520quantification%252C%2520and%2520provide%2520interpretation%2520of%2520model%250Abehavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13846v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regression%20Trees%20Know%20Calculus&entry.906535625=Nathan%20Wycoff&entry.1292438233=%20%20Regression%20trees%20have%20emerged%20as%20a%20preeminent%20tool%20for%20solving%20real-world%0Aregression%20problems%20due%20to%20their%20ability%20to%20deal%20with%20nonlinearities%2C%0Ainteraction%20effects%20and%20sharp%20discontinuities.%20In%20this%20article%2C%20we%20rather%20study%0Aregression%20trees%20applied%20to%20well-behaved%2C%20differentiable%20functions%2C%20and%0Adetermine%20the%20relationship%20between%20node%20parameters%20and%20the%20local%20gradient%20of%0Athe%20function%20being%20approximated.%20We%20find%20a%20simple%20estimate%20of%20the%20gradient%0Awhich%20can%20be%20efficiently%20computed%20using%20quantities%20exposed%20by%20popular%20tree%0Alearning%20libraries.%20This%20allows%20the%20tools%20developed%20in%20the%20context%20of%0Adifferentiable%20algorithms%2C%20like%20neural%20nets%20and%20Gaussian%20processes%2C%20to%20be%0Adeployed%20to%20tree-based%20models.%20To%20demonstrate%20this%2C%20we%20study%20measures%20of%20model%0Asensitivity%20defined%20in%20terms%20of%20integrals%20of%20gradients%20and%20demonstrate%20how%20to%0Acompute%20them%20for%20regression%20trees%20using%20the%20proposed%20gradient%20estimates.%0AQuantitative%20and%20qualitative%20numerical%20experiments%20reveal%20the%20capability%20of%0Agradients%20estimated%20by%20regression%20trees%20to%20improve%20predictive%20analysis%2C%20solve%0Atasks%20in%20uncertainty%20quantification%2C%20and%20provide%20interpretation%20of%20model%0Abehavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13846v2&entry.124074799=Read"},
{"title": "Understanding Generalizability of Diffusion Models Requires Rethinking\n  the Hidden Gaussian Structure", "author": "Xiang Li and Yixiang Dai and Qing Qu", "abstract": "  In this work, we study the generalizability of diffusion models by looking\ninto the hidden properties of the learned score functions, which are\nessentially a series of deep denoisers trained on various noise levels. We\nobserve that as diffusion models transition from memorization to\ngeneralization, their corresponding nonlinear diffusion denoisers exhibit\nincreasing linearity. This discovery leads us to investigate the linear\ncounterparts of the nonlinear diffusion models, which are a series of linear\nmodels trained to match the function mappings of the nonlinear diffusion\ndenoisers. Surprisingly, these linear denoisers are approximately the optimal\ndenoisers for a multivariate Gaussian distribution characterized by the\nempirical mean and covariance of the training dataset. This finding implies\nthat diffusion models have the inductive bias towards capturing and utilizing\nthe Gaussian structure (covariance information) of the training dataset for\ndata generation. We empirically demonstrate that this inductive bias is a\nunique property of diffusion models in the generalization regime, which becomes\nincreasingly evident when the model's capacity is relatively small compared to\nthe training dataset size. In the case that the model is highly\noverparameterized, this inductive bias emerges during the initial training\nphases before the model fully memorizes its training data. Our study provides\ncrucial insights into understanding the notable strong generalization\nphenomenon recently observed in real-world diffusion models.\n", "link": "http://arxiv.org/abs/2410.24060v5", "date": "2024-12-02", "relevancy": 1.0533, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5394}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5351}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Generalizability%20of%20Diffusion%20Models%20Requires%20Rethinking%0A%20%20the%20Hidden%20Gaussian%20Structure&body=Title%3A%20Understanding%20Generalizability%20of%20Diffusion%20Models%20Requires%20Rethinking%0A%20%20the%20Hidden%20Gaussian%20Structure%0AAuthor%3A%20Xiang%20Li%20and%20Yixiang%20Dai%20and%20Qing%20Qu%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20study%20the%20generalizability%20of%20diffusion%20models%20by%20looking%0Ainto%20the%20hidden%20properties%20of%20the%20learned%20score%20functions%2C%20which%20are%0Aessentially%20a%20series%20of%20deep%20denoisers%20trained%20on%20various%20noise%20levels.%20We%0Aobserve%20that%20as%20diffusion%20models%20transition%20from%20memorization%20to%0Ageneralization%2C%20their%20corresponding%20nonlinear%20diffusion%20denoisers%20exhibit%0Aincreasing%20linearity.%20This%20discovery%20leads%20us%20to%20investigate%20the%20linear%0Acounterparts%20of%20the%20nonlinear%20diffusion%20models%2C%20which%20are%20a%20series%20of%20linear%0Amodels%20trained%20to%20match%20the%20function%20mappings%20of%20the%20nonlinear%20diffusion%0Adenoisers.%20Surprisingly%2C%20these%20linear%20denoisers%20are%20approximately%20the%20optimal%0Adenoisers%20for%20a%20multivariate%20Gaussian%20distribution%20characterized%20by%20the%0Aempirical%20mean%20and%20covariance%20of%20the%20training%20dataset.%20This%20finding%20implies%0Athat%20diffusion%20models%20have%20the%20inductive%20bias%20towards%20capturing%20and%20utilizing%0Athe%20Gaussian%20structure%20%28covariance%20information%29%20of%20the%20training%20dataset%20for%0Adata%20generation.%20We%20empirically%20demonstrate%20that%20this%20inductive%20bias%20is%20a%0Aunique%20property%20of%20diffusion%20models%20in%20the%20generalization%20regime%2C%20which%20becomes%0Aincreasingly%20evident%20when%20the%20model%27s%20capacity%20is%20relatively%20small%20compared%20to%0Athe%20training%20dataset%20size.%20In%20the%20case%20that%20the%20model%20is%20highly%0Aoverparameterized%2C%20this%20inductive%20bias%20emerges%20during%20the%20initial%20training%0Aphases%20before%20the%20model%20fully%20memorizes%20its%20training%20data.%20Our%20study%20provides%0Acrucial%20insights%20into%20understanding%20the%20notable%20strong%20generalization%0Aphenomenon%20recently%20observed%20in%20real-world%20diffusion%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24060v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Generalizability%2520of%2520Diffusion%2520Models%2520Requires%2520Rethinking%250A%2520%2520the%2520Hidden%2520Gaussian%2520Structure%26entry.906535625%3DXiang%2520Li%2520and%2520Yixiang%2520Dai%2520and%2520Qing%2520Qu%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520generalizability%2520of%2520diffusion%2520models%2520by%2520looking%250Ainto%2520the%2520hidden%2520properties%2520of%2520the%2520learned%2520score%2520functions%252C%2520which%2520are%250Aessentially%2520a%2520series%2520of%2520deep%2520denoisers%2520trained%2520on%2520various%2520noise%2520levels.%2520We%250Aobserve%2520that%2520as%2520diffusion%2520models%2520transition%2520from%2520memorization%2520to%250Ageneralization%252C%2520their%2520corresponding%2520nonlinear%2520diffusion%2520denoisers%2520exhibit%250Aincreasing%2520linearity.%2520This%2520discovery%2520leads%2520us%2520to%2520investigate%2520the%2520linear%250Acounterparts%2520of%2520the%2520nonlinear%2520diffusion%2520models%252C%2520which%2520are%2520a%2520series%2520of%2520linear%250Amodels%2520trained%2520to%2520match%2520the%2520function%2520mappings%2520of%2520the%2520nonlinear%2520diffusion%250Adenoisers.%2520Surprisingly%252C%2520these%2520linear%2520denoisers%2520are%2520approximately%2520the%2520optimal%250Adenoisers%2520for%2520a%2520multivariate%2520Gaussian%2520distribution%2520characterized%2520by%2520the%250Aempirical%2520mean%2520and%2520covariance%2520of%2520the%2520training%2520dataset.%2520This%2520finding%2520implies%250Athat%2520diffusion%2520models%2520have%2520the%2520inductive%2520bias%2520towards%2520capturing%2520and%2520utilizing%250Athe%2520Gaussian%2520structure%2520%2528covariance%2520information%2529%2520of%2520the%2520training%2520dataset%2520for%250Adata%2520generation.%2520We%2520empirically%2520demonstrate%2520that%2520this%2520inductive%2520bias%2520is%2520a%250Aunique%2520property%2520of%2520diffusion%2520models%2520in%2520the%2520generalization%2520regime%252C%2520which%2520becomes%250Aincreasingly%2520evident%2520when%2520the%2520model%2527s%2520capacity%2520is%2520relatively%2520small%2520compared%2520to%250Athe%2520training%2520dataset%2520size.%2520In%2520the%2520case%2520that%2520the%2520model%2520is%2520highly%250Aoverparameterized%252C%2520this%2520inductive%2520bias%2520emerges%2520during%2520the%2520initial%2520training%250Aphases%2520before%2520the%2520model%2520fully%2520memorizes%2520its%2520training%2520data.%2520Our%2520study%2520provides%250Acrucial%2520insights%2520into%2520understanding%2520the%2520notable%2520strong%2520generalization%250Aphenomenon%2520recently%2520observed%2520in%2520real-world%2520diffusion%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24060v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Generalizability%20of%20Diffusion%20Models%20Requires%20Rethinking%0A%20%20the%20Hidden%20Gaussian%20Structure&entry.906535625=Xiang%20Li%20and%20Yixiang%20Dai%20and%20Qing%20Qu&entry.1292438233=%20%20In%20this%20work%2C%20we%20study%20the%20generalizability%20of%20diffusion%20models%20by%20looking%0Ainto%20the%20hidden%20properties%20of%20the%20learned%20score%20functions%2C%20which%20are%0Aessentially%20a%20series%20of%20deep%20denoisers%20trained%20on%20various%20noise%20levels.%20We%0Aobserve%20that%20as%20diffusion%20models%20transition%20from%20memorization%20to%0Ageneralization%2C%20their%20corresponding%20nonlinear%20diffusion%20denoisers%20exhibit%0Aincreasing%20linearity.%20This%20discovery%20leads%20us%20to%20investigate%20the%20linear%0Acounterparts%20of%20the%20nonlinear%20diffusion%20models%2C%20which%20are%20a%20series%20of%20linear%0Amodels%20trained%20to%20match%20the%20function%20mappings%20of%20the%20nonlinear%20diffusion%0Adenoisers.%20Surprisingly%2C%20these%20linear%20denoisers%20are%20approximately%20the%20optimal%0Adenoisers%20for%20a%20multivariate%20Gaussian%20distribution%20characterized%20by%20the%0Aempirical%20mean%20and%20covariance%20of%20the%20training%20dataset.%20This%20finding%20implies%0Athat%20diffusion%20models%20have%20the%20inductive%20bias%20towards%20capturing%20and%20utilizing%0Athe%20Gaussian%20structure%20%28covariance%20information%29%20of%20the%20training%20dataset%20for%0Adata%20generation.%20We%20empirically%20demonstrate%20that%20this%20inductive%20bias%20is%20a%0Aunique%20property%20of%20diffusion%20models%20in%20the%20generalization%20regime%2C%20which%20becomes%0Aincreasingly%20evident%20when%20the%20model%27s%20capacity%20is%20relatively%20small%20compared%20to%0Athe%20training%20dataset%20size.%20In%20the%20case%20that%20the%20model%20is%20highly%0Aoverparameterized%2C%20this%20inductive%20bias%20emerges%20during%20the%20initial%20training%0Aphases%20before%20the%20model%20fully%20memorizes%20its%20training%20data.%20Our%20study%20provides%0Acrucial%20insights%20into%20understanding%20the%20notable%20strong%20generalization%0Aphenomenon%20recently%20observed%20in%20real-world%20diffusion%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24060v5&entry.124074799=Read"},
{"title": "Inducing Group Fairness in Prompt-Based Language Model Decisions", "author": "James Atwood and Nino Scherrer and Preethi Lahoti and Ananth Balashankar and Flavien Prost and Ahmad Beirami", "abstract": "  Classifiers are used throughout industry to enforce policies, ranging from\nthe detection of toxic content to age-appropriate content filtering. While\nthese classifiers serve important functions, it is also essential that they are\nbuilt in ways that minimize unfair biases for users.\n  One such fairness consideration is called group fairness, which desires that\ndifferent sub-population of users receive equal treatment. This is a\nwell-studied problem in the context of 'classical' classifiers. However, the\nemergence of prompt-based language model (LM) decision making has created new\nopportunities to solve text-based classification tasks, and the fairness\nproperties of these new classifiers are not yet well understood. Further, the\n`remediation toolkit' is incomplete for LM-based decision makers and little is\nunderstood about how to improve decision maker group fairness while maintaining\nclassifier performance.\n  This work sets out to add more tools to that toolbox. We introduce\nadaptations of existing effective approaches from the classical classifier\nfairness to the prompt-based classifier space. We also devise simple methods\nthat take advantage of the new structure of prompt-based decision makers and\noperate at the prompt level. We compare these approaches empirically on real\ndata. Our results suggest that adaptations of approaches that are effective for\nclassical classifiers remain effective in the LM-based classifier environment.\nHowever, there is room for further exploration of prompt-based remediation\nmethods (and other remediation methods that take advantage of LM structure).\n", "link": "http://arxiv.org/abs/2406.16738v2", "date": "2024-12-02", "relevancy": 1.3526, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4626}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4548}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inducing%20Group%20Fairness%20in%20Prompt-Based%20Language%20Model%20Decisions&body=Title%3A%20Inducing%20Group%20Fairness%20in%20Prompt-Based%20Language%20Model%20Decisions%0AAuthor%3A%20James%20Atwood%20and%20Nino%20Scherrer%20and%20Preethi%20Lahoti%20and%20Ananth%20Balashankar%20and%20Flavien%20Prost%20and%20Ahmad%20Beirami%0AAbstract%3A%20%20%20Classifiers%20are%20used%20throughout%20industry%20to%20enforce%20policies%2C%20ranging%20from%0Athe%20detection%20of%20toxic%20content%20to%20age-appropriate%20content%20filtering.%20While%0Athese%20classifiers%20serve%20important%20functions%2C%20it%20is%20also%20essential%20that%20they%20are%0Abuilt%20in%20ways%20that%20minimize%20unfair%20biases%20for%20users.%0A%20%20One%20such%20fairness%20consideration%20is%20called%20group%20fairness%2C%20which%20desires%20that%0Adifferent%20sub-population%20of%20users%20receive%20equal%20treatment.%20This%20is%20a%0Awell-studied%20problem%20in%20the%20context%20of%20%27classical%27%20classifiers.%20However%2C%20the%0Aemergence%20of%20prompt-based%20language%20model%20%28LM%29%20decision%20making%20has%20created%20new%0Aopportunities%20to%20solve%20text-based%20classification%20tasks%2C%20and%20the%20fairness%0Aproperties%20of%20these%20new%20classifiers%20are%20not%20yet%20well%20understood.%20Further%2C%20the%0A%60remediation%20toolkit%27%20is%20incomplete%20for%20LM-based%20decision%20makers%20and%20little%20is%0Aunderstood%20about%20how%20to%20improve%20decision%20maker%20group%20fairness%20while%20maintaining%0Aclassifier%20performance.%0A%20%20This%20work%20sets%20out%20to%20add%20more%20tools%20to%20that%20toolbox.%20We%20introduce%0Aadaptations%20of%20existing%20effective%20approaches%20from%20the%20classical%20classifier%0Afairness%20to%20the%20prompt-based%20classifier%20space.%20We%20also%20devise%20simple%20methods%0Athat%20take%20advantage%20of%20the%20new%20structure%20of%20prompt-based%20decision%20makers%20and%0Aoperate%20at%20the%20prompt%20level.%20We%20compare%20these%20approaches%20empirically%20on%20real%0Adata.%20Our%20results%20suggest%20that%20adaptations%20of%20approaches%20that%20are%20effective%20for%0Aclassical%20classifiers%20remain%20effective%20in%20the%20LM-based%20classifier%20environment.%0AHowever%2C%20there%20is%20room%20for%20further%20exploration%20of%20prompt-based%20remediation%0Amethods%20%28and%20other%20remediation%20methods%20that%20take%20advantage%20of%20LM%20structure%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16738v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInducing%2520Group%2520Fairness%2520in%2520Prompt-Based%2520Language%2520Model%2520Decisions%26entry.906535625%3DJames%2520Atwood%2520and%2520Nino%2520Scherrer%2520and%2520Preethi%2520Lahoti%2520and%2520Ananth%2520Balashankar%2520and%2520Flavien%2520Prost%2520and%2520Ahmad%2520Beirami%26entry.1292438233%3D%2520%2520Classifiers%2520are%2520used%2520throughout%2520industry%2520to%2520enforce%2520policies%252C%2520ranging%2520from%250Athe%2520detection%2520of%2520toxic%2520content%2520to%2520age-appropriate%2520content%2520filtering.%2520While%250Athese%2520classifiers%2520serve%2520important%2520functions%252C%2520it%2520is%2520also%2520essential%2520that%2520they%2520are%250Abuilt%2520in%2520ways%2520that%2520minimize%2520unfair%2520biases%2520for%2520users.%250A%2520%2520One%2520such%2520fairness%2520consideration%2520is%2520called%2520group%2520fairness%252C%2520which%2520desires%2520that%250Adifferent%2520sub-population%2520of%2520users%2520receive%2520equal%2520treatment.%2520This%2520is%2520a%250Awell-studied%2520problem%2520in%2520the%2520context%2520of%2520%2527classical%2527%2520classifiers.%2520However%252C%2520the%250Aemergence%2520of%2520prompt-based%2520language%2520model%2520%2528LM%2529%2520decision%2520making%2520has%2520created%2520new%250Aopportunities%2520to%2520solve%2520text-based%2520classification%2520tasks%252C%2520and%2520the%2520fairness%250Aproperties%2520of%2520these%2520new%2520classifiers%2520are%2520not%2520yet%2520well%2520understood.%2520Further%252C%2520the%250A%2560remediation%2520toolkit%2527%2520is%2520incomplete%2520for%2520LM-based%2520decision%2520makers%2520and%2520little%2520is%250Aunderstood%2520about%2520how%2520to%2520improve%2520decision%2520maker%2520group%2520fairness%2520while%2520maintaining%250Aclassifier%2520performance.%250A%2520%2520This%2520work%2520sets%2520out%2520to%2520add%2520more%2520tools%2520to%2520that%2520toolbox.%2520We%2520introduce%250Aadaptations%2520of%2520existing%2520effective%2520approaches%2520from%2520the%2520classical%2520classifier%250Afairness%2520to%2520the%2520prompt-based%2520classifier%2520space.%2520We%2520also%2520devise%2520simple%2520methods%250Athat%2520take%2520advantage%2520of%2520the%2520new%2520structure%2520of%2520prompt-based%2520decision%2520makers%2520and%250Aoperate%2520at%2520the%2520prompt%2520level.%2520We%2520compare%2520these%2520approaches%2520empirically%2520on%2520real%250Adata.%2520Our%2520results%2520suggest%2520that%2520adaptations%2520of%2520approaches%2520that%2520are%2520effective%2520for%250Aclassical%2520classifiers%2520remain%2520effective%2520in%2520the%2520LM-based%2520classifier%2520environment.%250AHowever%252C%2520there%2520is%2520room%2520for%2520further%2520exploration%2520of%2520prompt-based%2520remediation%250Amethods%2520%2528and%2520other%2520remediation%2520methods%2520that%2520take%2520advantage%2520of%2520LM%2520structure%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16738v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inducing%20Group%20Fairness%20in%20Prompt-Based%20Language%20Model%20Decisions&entry.906535625=James%20Atwood%20and%20Nino%20Scherrer%20and%20Preethi%20Lahoti%20and%20Ananth%20Balashankar%20and%20Flavien%20Prost%20and%20Ahmad%20Beirami&entry.1292438233=%20%20Classifiers%20are%20used%20throughout%20industry%20to%20enforce%20policies%2C%20ranging%20from%0Athe%20detection%20of%20toxic%20content%20to%20age-appropriate%20content%20filtering.%20While%0Athese%20classifiers%20serve%20important%20functions%2C%20it%20is%20also%20essential%20that%20they%20are%0Abuilt%20in%20ways%20that%20minimize%20unfair%20biases%20for%20users.%0A%20%20One%20such%20fairness%20consideration%20is%20called%20group%20fairness%2C%20which%20desires%20that%0Adifferent%20sub-population%20of%20users%20receive%20equal%20treatment.%20This%20is%20a%0Awell-studied%20problem%20in%20the%20context%20of%20%27classical%27%20classifiers.%20However%2C%20the%0Aemergence%20of%20prompt-based%20language%20model%20%28LM%29%20decision%20making%20has%20created%20new%0Aopportunities%20to%20solve%20text-based%20classification%20tasks%2C%20and%20the%20fairness%0Aproperties%20of%20these%20new%20classifiers%20are%20not%20yet%20well%20understood.%20Further%2C%20the%0A%60remediation%20toolkit%27%20is%20incomplete%20for%20LM-based%20decision%20makers%20and%20little%20is%0Aunderstood%20about%20how%20to%20improve%20decision%20maker%20group%20fairness%20while%20maintaining%0Aclassifier%20performance.%0A%20%20This%20work%20sets%20out%20to%20add%20more%20tools%20to%20that%20toolbox.%20We%20introduce%0Aadaptations%20of%20existing%20effective%20approaches%20from%20the%20classical%20classifier%0Afairness%20to%20the%20prompt-based%20classifier%20space.%20We%20also%20devise%20simple%20methods%0Athat%20take%20advantage%20of%20the%20new%20structure%20of%20prompt-based%20decision%20makers%20and%0Aoperate%20at%20the%20prompt%20level.%20We%20compare%20these%20approaches%20empirically%20on%20real%0Adata.%20Our%20results%20suggest%20that%20adaptations%20of%20approaches%20that%20are%20effective%20for%0Aclassical%20classifiers%20remain%20effective%20in%20the%20LM-based%20classifier%20environment.%0AHowever%2C%20there%20is%20room%20for%20further%20exploration%20of%20prompt-based%20remediation%0Amethods%20%28and%20other%20remediation%20methods%20that%20take%20advantage%20of%20LM%20structure%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16738v2&entry.124074799=Read"},
{"title": "Protecting Federated Learning from Extreme Model Poisoning Attacks via\n  Multidimensional Time Series Anomaly Detection", "author": "Edoardo Gabrielli and Dimitri Belli and Zoe Matrullo and Vittorio Miori and Gabriele Tolomei", "abstract": "  Current defense mechanisms against model poisoning attacks in federated\nlearning (FL) systems have proven effective up to a certain threshold of\nmalicious clients. In this work, we introduce FLANDERS, a novel pre-aggregation\nfilter for FL resilient to large-scale model poisoning attacks, i.e., when\nmalicious clients far exceed legitimate participants. FLANDERS treats the\nsequence of local models sent by clients in each FL round as a matrix-valued\ntime series. Then, it identifies malicious client updates as outliers in this\ntime series by comparing actual observations with estimates generated by a\nmatrix autoregressive forecasting model maintained by the server. Experiments\nconducted in several non-iid FL setups show that FLANDERS significantly\nimproves robustness across a wide spectrum of attacks when paired with standard\nand robust existing aggregation methods.\n", "link": "http://arxiv.org/abs/2303.16668v3", "date": "2024-12-02", "relevancy": 1.3377, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4616}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4285}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Protecting%20Federated%20Learning%20from%20Extreme%20Model%20Poisoning%20Attacks%20via%0A%20%20Multidimensional%20Time%20Series%20Anomaly%20Detection&body=Title%3A%20Protecting%20Federated%20Learning%20from%20Extreme%20Model%20Poisoning%20Attacks%20via%0A%20%20Multidimensional%20Time%20Series%20Anomaly%20Detection%0AAuthor%3A%20Edoardo%20Gabrielli%20and%20Dimitri%20Belli%20and%20Zoe%20Matrullo%20and%20Vittorio%20Miori%20and%20Gabriele%20Tolomei%0AAbstract%3A%20%20%20Current%20defense%20mechanisms%20against%20model%20poisoning%20attacks%20in%20federated%0Alearning%20%28FL%29%20systems%20have%20proven%20effective%20up%20to%20a%20certain%20threshold%20of%0Amalicious%20clients.%20In%20this%20work%2C%20we%20introduce%20FLANDERS%2C%20a%20novel%20pre-aggregation%0Afilter%20for%20FL%20resilient%20to%20large-scale%20model%20poisoning%20attacks%2C%20i.e.%2C%20when%0Amalicious%20clients%20far%20exceed%20legitimate%20participants.%20FLANDERS%20treats%20the%0Asequence%20of%20local%20models%20sent%20by%20clients%20in%20each%20FL%20round%20as%20a%20matrix-valued%0Atime%20series.%20Then%2C%20it%20identifies%20malicious%20client%20updates%20as%20outliers%20in%20this%0Atime%20series%20by%20comparing%20actual%20observations%20with%20estimates%20generated%20by%20a%0Amatrix%20autoregressive%20forecasting%20model%20maintained%20by%20the%20server.%20Experiments%0Aconducted%20in%20several%20non-iid%20FL%20setups%20show%20that%20FLANDERS%20significantly%0Aimproves%20robustness%20across%20a%20wide%20spectrum%20of%20attacks%20when%20paired%20with%20standard%0Aand%20robust%20existing%20aggregation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.16668v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProtecting%2520Federated%2520Learning%2520from%2520Extreme%2520Model%2520Poisoning%2520Attacks%2520via%250A%2520%2520Multidimensional%2520Time%2520Series%2520Anomaly%2520Detection%26entry.906535625%3DEdoardo%2520Gabrielli%2520and%2520Dimitri%2520Belli%2520and%2520Zoe%2520Matrullo%2520and%2520Vittorio%2520Miori%2520and%2520Gabriele%2520Tolomei%26entry.1292438233%3D%2520%2520Current%2520defense%2520mechanisms%2520against%2520model%2520poisoning%2520attacks%2520in%2520federated%250Alearning%2520%2528FL%2529%2520systems%2520have%2520proven%2520effective%2520up%2520to%2520a%2520certain%2520threshold%2520of%250Amalicious%2520clients.%2520In%2520this%2520work%252C%2520we%2520introduce%2520FLANDERS%252C%2520a%2520novel%2520pre-aggregation%250Afilter%2520for%2520FL%2520resilient%2520to%2520large-scale%2520model%2520poisoning%2520attacks%252C%2520i.e.%252C%2520when%250Amalicious%2520clients%2520far%2520exceed%2520legitimate%2520participants.%2520FLANDERS%2520treats%2520the%250Asequence%2520of%2520local%2520models%2520sent%2520by%2520clients%2520in%2520each%2520FL%2520round%2520as%2520a%2520matrix-valued%250Atime%2520series.%2520Then%252C%2520it%2520identifies%2520malicious%2520client%2520updates%2520as%2520outliers%2520in%2520this%250Atime%2520series%2520by%2520comparing%2520actual%2520observations%2520with%2520estimates%2520generated%2520by%2520a%250Amatrix%2520autoregressive%2520forecasting%2520model%2520maintained%2520by%2520the%2520server.%2520Experiments%250Aconducted%2520in%2520several%2520non-iid%2520FL%2520setups%2520show%2520that%2520FLANDERS%2520significantly%250Aimproves%2520robustness%2520across%2520a%2520wide%2520spectrum%2520of%2520attacks%2520when%2520paired%2520with%2520standard%250Aand%2520robust%2520existing%2520aggregation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.16668v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Protecting%20Federated%20Learning%20from%20Extreme%20Model%20Poisoning%20Attacks%20via%0A%20%20Multidimensional%20Time%20Series%20Anomaly%20Detection&entry.906535625=Edoardo%20Gabrielli%20and%20Dimitri%20Belli%20and%20Zoe%20Matrullo%20and%20Vittorio%20Miori%20and%20Gabriele%20Tolomei&entry.1292438233=%20%20Current%20defense%20mechanisms%20against%20model%20poisoning%20attacks%20in%20federated%0Alearning%20%28FL%29%20systems%20have%20proven%20effective%20up%20to%20a%20certain%20threshold%20of%0Amalicious%20clients.%20In%20this%20work%2C%20we%20introduce%20FLANDERS%2C%20a%20novel%20pre-aggregation%0Afilter%20for%20FL%20resilient%20to%20large-scale%20model%20poisoning%20attacks%2C%20i.e.%2C%20when%0Amalicious%20clients%20far%20exceed%20legitimate%20participants.%20FLANDERS%20treats%20the%0Asequence%20of%20local%20models%20sent%20by%20clients%20in%20each%20FL%20round%20as%20a%20matrix-valued%0Atime%20series.%20Then%2C%20it%20identifies%20malicious%20client%20updates%20as%20outliers%20in%20this%0Atime%20series%20by%20comparing%20actual%20observations%20with%20estimates%20generated%20by%20a%0Amatrix%20autoregressive%20forecasting%20model%20maintained%20by%20the%20server.%20Experiments%0Aconducted%20in%20several%20non-iid%20FL%20setups%20show%20that%20FLANDERS%20significantly%0Aimproves%20robustness%20across%20a%20wide%20spectrum%20of%20attacks%20when%20paired%20with%20standard%0Aand%20robust%20existing%20aggregation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.16668v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


